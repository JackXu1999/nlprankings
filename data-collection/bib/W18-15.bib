@proceedings{ws-2018-storytelling,
    title = "Proceedings of the First Workshop on Storytelling",
    author = "Mitchell, Margaret  and
      Huang, Ting-Hao {`}Kenneth{'}  and
      Ferraro, Francis  and
      Misra, Ishan",
    month = jun,
    year = "2018",
    address = "New Orleans, Louisiana",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/W18-1500",
    doi = "10.18653/v1/W18-15",
}
@inproceedings{kasunic-kaufman-2018-learning,
    title = "Learning to Listen: Critically Considering the Role of {AI} in Human Storytelling and Character Creation",
    author = "Kasunic, Anna  and
      Kaufman, Geoff",
    booktitle = "Proceedings of the First Workshop on Storytelling",
    month = jun,
    year = "2018",
    address = "New Orleans, Louisiana",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/W18-1501",
    doi = "10.18653/v1/W18-1501",
    pages = "1--13",
    abstract = "In this opinion piece, we argue that there is a need for alternative design directions to complement existing AI efforts in narrative and character generation and algorithm development. To make our argument, we a) outline the predominant roles and goals of AI research in storytelling; b) present existing discourse on the benefits and harms of narratives; and c) highlight the pain points in character creation revealed by semi-structured interviews we conducted with 14 individuals deeply involved in some form of character creation. We conclude by proffering several specific design avenues that we believe can seed fruitful research collaborations. In our vision, AI collaborates with humans during creative processes and narrative generation, helps amplify voices and perspectives that are currently marginalized or misrepresented, and engenders experiences of narrative that support spectatorship and listening roles.",
}
@inproceedings{roemmele-gordon-2018-linguistic,
    title = "Linguistic Features of Helpfulness in Automated Support for Creative Writing",
    author = "Roemmele, Melissa  and
      Gordon, Andrew",
    booktitle = "Proceedings of the First Workshop on Storytelling",
    month = jun,
    year = "2018",
    address = "New Orleans, Louisiana",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/W18-1502",
    doi = "10.18653/v1/W18-1502",
    pages = "14--19",
    abstract = "We examine an emerging NLP application that supports creative writing by automatically suggesting continuing sentences in a story. The application tracks users{'} modifications to generated sentences, which can be used to quantify their {``}helpfulness{''} in advancing the story. We explore the task of predicting helpfulness based on automatically detected linguistic features of the suggestions. We illustrate this analysis on a set of user interactions with the application using an initial selection of features relevant to story generation.",
}
@inproceedings{lukin-etal-2018-pipeline,
    title = "A Pipeline for Creative Visual Storytelling",
    author = "Lukin, Stephanie  and
      Hobbs, Reginald  and
      Voss, Clare",
    booktitle = "Proceedings of the First Workshop on Storytelling",
    month = jun,
    year = "2018",
    address = "New Orleans, Louisiana",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/W18-1503",
    doi = "10.18653/v1/W18-1503",
    pages = "20--32",
    abstract = "Computational visual storytelling produces a textual description of events and interpretations depicted in a sequence of images. These texts are made possible by advances and cross-disciplinary approaches in natural language processing, generation, and computer vision. We define a computational creative visual storytelling as one with the ability to alter the telling of a story along three aspects: to speak about different environments, to produce variations based on narrative goals, and to adapt the narrative to the audience. These aspects of creative storytelling and their effect on the narrative have yet to be explored in visual storytelling. This paper presents a pipeline of task-modules, Object Identification, Single-Image Inferencing, and Multi-Image Narration, that serve as a preliminary design for building a creative visual storyteller. We have piloted this design for a sequence of images in an annotation task. We present and analyze the collected corpus and describe plans towards automation.",
}
@inproceedings{gillick-bamman-2018-telling,
    title = "Telling Stories with Soundtracks: An Empirical Analysis of Music in Film",
    author = "Gillick, Jon  and
      Bamman, David",
    booktitle = "Proceedings of the First Workshop on Storytelling",
    month = jun,
    year = "2018",
    address = "New Orleans, Louisiana",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/W18-1504",
    doi = "10.18653/v1/W18-1504",
    pages = "33--42",
    abstract = "Soundtracks play an important role in carrying the story of a film. In this work, we collect a corpus of movies and television shows matched with subtitles and soundtracks and analyze the relationship between story, song, and audience reception. We look at the content of a film through the lens of its latent topics and at the content of a song through descriptors of its musical attributes. In two experiments, we find first that individual topics are strongly associated with musical attributes, and second, that musical attributes of soundtracks are predictive of film ratings, even after controlling for topic and genre.",
}
@inproceedings{peng-etal-2018-towards,
    title = "Towards Controllable Story Generation",
    author = "Peng, Nanyun  and
      Ghazvininejad, Marjan  and
      May, Jonathan  and
      Knight, Kevin",
    booktitle = "Proceedings of the First Workshop on Storytelling",
    month = jun,
    year = "2018",
    address = "New Orleans, Louisiana",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/W18-1505",
    doi = "10.18653/v1/W18-1505",
    pages = "43--49",
    abstract = "We present a general framework of analyzing existing story corpora to generate controllable and creative new stories. The proposed framework needs little manual annotation to achieve controllable story generation. It creates a new interface for humans to interact with computers to generate personalized stories. We apply the framework to build recurrent neural network (RNN)-based generation models to control story ending valence and storyline. Experiments show that our methods successfully achieve the control and enhance the coherence of stories through introducing storylines. with additional control factors, the generation model gets lower perplexity, and yields more coherent stories that are faithful to the control factors according to human evaluation.",
}
@inproceedings{roemmele-gordon-2018-encoder,
    title = "An Encoder-decoder Approach to Predicting Causal Relations in Stories",
    author = "Roemmele, Melissa  and
      Gordon, Andrew",
    booktitle = "Proceedings of the First Workshop on Storytelling",
    month = jun,
    year = "2018",
    address = "New Orleans, Louisiana",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/W18-1506",
    doi = "10.18653/v1/W18-1506",
    pages = "50--59",
    abstract = "We address the task of predicting causally related events in stories according to a standard evaluation framework, the Choice of Plausible Alternatives (COPA). We present a neural encoder-decoder model that learns to predict relations between adjacent sequences in stories as a means of modeling causality. We explore this approach using different methods for extracting and representing sequence pairs as well as different model architectures. We also compare the impact of different training datasets on our model. In particular, we demonstrate the usefulness of a corpus not previously applied to COPA, the ROCStories corpus. While not state-of-the-art, our results establish a new reference point for systems evaluated on COPA, and one that is particularly informative for future neural-based approaches.",
}
@inproceedings{tozzo-etal-2018-neural,
    title = "Neural Event Extraction from Movies Description",
    author = "Tozzo, Alex  and
      Jovanovi{\'c}, Dejan  and
      Amer, Mohamed",
    booktitle = "Proceedings of the First Workshop on Storytelling",
    month = jun,
    year = "2018",
    address = "New Orleans, Louisiana",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/W18-1507",
    doi = "10.18653/v1/W18-1507",
    pages = "60--66",
    abstract = "We present a novel approach for event extraction and abstraction from movie descriptions. Our event frame consists of {``}who{''}, {``}did what{''} {``}to whom{''}, {``}where{''}, and {``}when{''}. We formulate our problem using a recurrent neural network, enhanced with structural features extracted from syntactic parser, and trained using curriculum learning by progressively increasing the difficulty of the sentences. Our model serves as an intermediate step towards question answering systems, visual storytelling, and story completion tasks. We evaluate our approach on MovieQA dataset.",
}
