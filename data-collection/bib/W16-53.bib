@proceedings{ws-2016-cognitive-aspects,
    title = "Proceedings of the 5th Workshop on Cognitive Aspects of the Lexicon ({C}og{AL}ex - V)",
    author = "Zock, Michael  and
      Lenci, Alessandro  and
      Evert, Stefan",
    month = dec,
    year = "2016",
    address = "Osaka, Japan",
    publisher = "The COLING 2016 Organizing Committee",
    url = "https://www.aclweb.org/anthology/W16-5300",
}
@inproceedings{biemann-2016-vectors,
    title = "Vectors or Graphs? On Differences of Representations for Distributional Semantic Models",
    author = "Biemann, Chris",
    booktitle = "Proceedings of the 5th Workshop on Cognitive Aspects of the Lexicon ({C}og{AL}ex - V)",
    month = dec,
    year = "2016",
    address = "Osaka, Japan",
    publisher = "The COLING 2016 Organizing Committee",
    url = "https://www.aclweb.org/anthology/W16-5301",
    pages = "1--7",
    abstract = "Distributional Semantic Models (DSMs) have recently received increased attention, together with the rise of neural architectures for scalable training of dense vector embeddings. While some of the literature even includes terms like {`}vectors{'} and {`}dimensionality{'} in the definition of DSMs, there are some good reasons why we should consider alternative formulations of distributional models. As an instance, I present a scalable graph-based solution to distributional semantics. The model belongs to the family of {`}count-based{'} DSMs, keeps its representation sparse and explicit, and thus fully interpretable. I will highlight some important differences between sparse graph-based and dense vector approaches to DSMs: while dense vector-based models are computationally easier to handle and provide a nice uniform representation that can be compared and combined in many ways, they lack interpretability, provenance and robustness. On the other hand, graph-based sparse models have a more straightforward interpretation, handle sense distinctions more naturally and can straightforwardly be linked to knowledge bases, while lacking the ability to compare arbitrary lexical units and a compositionality operation. Since both representations have their merits, I opt for exploring their combination in the outlook.",
}
@inproceedings{lebani-lenci-2016-beware,
    title = "{``}Beware the Jabberwock, dear reader!{''} Testing the distributional reality of construction semantics",
    author = "Lebani, Gianluca  and
      Lenci, Alessandro",
    booktitle = "Proceedings of the 5th Workshop on Cognitive Aspects of the Lexicon ({C}og{AL}ex - V)",
    month = dec,
    year = "2016",
    address = "Osaka, Japan",
    publisher = "The COLING 2016 Organizing Committee",
    url = "https://www.aclweb.org/anthology/W16-5302",
    pages = "8--18",
    abstract = "Notwithstanding the success of the notion of construction, the computational tradition still lacks a way to represent the semantic content of these linguistic entities. Here we present a simple corpus-based model implementing the idea that the meaning of a syntactic construction is intimately related to the semantics of its typical verbs. It is a two-step process, that starts by identifying the typical verbs occurring with a given syntactic construction and building their distributional vectors. We then calculated the weighted centroid of these vectors in order to derive the distributional signature of a construction. In order to assess the goodness of our approach, we replicated the priming effect described by Johnson and Golberg (2013) as a function of the semantic distance between a construction and its prototypical verbs. Additional support for our view comes from a regression analysis showing that our distributional information can be used to model behavioral data collected with a crowdsourced elicitation experiment.",
}
@inproceedings{lopukhina-lopukhin-2016-regular,
    title = "Regular polysemy: from sense vectors to sense patterns",
    author = "Lopukhina, Anastasiya  and
      Lopukhin, Konstantin",
    booktitle = "Proceedings of the 5th Workshop on Cognitive Aspects of the Lexicon ({C}og{AL}ex - V)",
    month = dec,
    year = "2016",
    address = "Osaka, Japan",
    publisher = "The COLING 2016 Organizing Committee",
    url = "https://www.aclweb.org/anthology/W16-5303",
    pages = "19--23",
    abstract = "Regular polysemy was extensively investigated in lexical semantics, but this phenomenon has been very little studied in distributional semantics. We propose a model for regular polysemy detection that is based on sense vectors and allows to work directly with senses in semantic vector space. Our method is able to detect polysemous words that have the same regular sense alternation as in a given example (a word with two automatically induced senses that represent one polysemy pattern, such as ANIMAL / FOOD). The method works equally well for nouns, verbs and adjectives and achieves average recall of 0.55 and average precision of 0.59 for ten different polysemy patterns.",
}
@inproceedings{shwartz-dagan-2016-path,
    title = "Path-based vs. Distributional Information in Recognizing Lexical Semantic Relations",
    author = "Shwartz, Vered  and
      Dagan, Ido",
    booktitle = "Proceedings of the 5th Workshop on Cognitive Aspects of the Lexicon ({C}og{AL}ex - V)",
    month = dec,
    year = "2016",
    address = "Osaka, Japan",
    publisher = "The COLING 2016 Organizing Committee",
    url = "https://www.aclweb.org/anthology/W16-5304",
    pages = "24--29",
    abstract = "Recognizing various semantic relations between terms is beneficial for many NLP tasks. While path-based and distributional information sources are considered complementary for this task, the superior results the latter showed recently suggested that the former{'}s contribution might have become obsolete. We follow the recent success of an integrated neural method for hypernymy detection (Shwartz et al., 2016) and extend it to recognize multiple relations. The empirical results show that this method is effective in the multiclass setting as well. We further show that the path-based information source always contributes to the classification, and analyze the cases in which it mostly complements the distributional information.",
}
@inproceedings{santos-etal-2016-semantic,
    title = "Semantic Relation Classification: Task Formalisation and Refinement",
    author = "Santos, Vivian  and
      Huerliman, Manuela  and
      Davis, Brian  and
      Handschuh, Siegfried  and
      Freitas, Andr{\'e}",
    booktitle = "Proceedings of the 5th Workshop on Cognitive Aspects of the Lexicon ({C}og{AL}ex - V)",
    month = dec,
    year = "2016",
    address = "Osaka, Japan",
    publisher = "The COLING 2016 Organizing Committee",
    url = "https://www.aclweb.org/anthology/W16-5305",
    pages = "30--39",
    abstract = "The identification of semantic relations between terms within texts is a fundamental task in Natural Language Processing which can support applications requiring a lightweight semantic interpretation model. Currently, semantic relation classification concentrates on relations which are evaluated over open-domain data. This work provides a critique on the set of abstract relations used for semantic relation classification with regard to their ability to express relationships between terms which are found in a domain-specific corpora. Based on this analysis, this work proposes an alternative semantic relation model based on reusing and extending the set of abstract relations present in the DOLCE ontology. The resulting set of relations is well grounded, allows to capture a wide range of relations and could thus be used as a foundation for automatic classification of semantic relations.",
}
@inproceedings{attia-etal-2016-power,
    title = "The Power of Language Music: {A}rabic Lemmatization through Patterns",
    author = "Attia, Mohammed  and
      Zirikly, Ayah  and
      Diab, Mona",
    booktitle = "Proceedings of the 5th Workshop on Cognitive Aspects of the Lexicon ({C}og{AL}ex - V)",
    month = dec,
    year = "2016",
    address = "Osaka, Japan",
    publisher = "The COLING 2016 Organizing Committee",
    url = "https://www.aclweb.org/anthology/W16-5306",
    pages = "40--50",
    abstract = "The interaction between roots and patterns in Arabic has intrigued lexicographers and morphologists for centuries. While roots provide the consonantal building blocks, patterns provide the syllabic vocalic moulds. While roots provide abstract semantic classes, patterns realize these classes in specific instances. In this way both roots and patterns are indispensable for understanding the derivational, morphological and, to some extent, the cognitive aspects of the Arabic language. In this paper we perform lemmatization (a high-level lexical processing) without relying on a lookup dictionary. We use a hybrid approach that consists of a machine learning classifier to predict the lemma pattern for a given stem, and mapping rules to convert stems to their respective lemmas with the vocalization defined by the pattern.",
}
@inproceedings{kageback-salomonsson-2016-word,
    title = "Word Sense Disambiguation using a Bidirectional {LSTM}",
    author = {K{\aa}geb{\"a}ck, Mikael  and
      Salomonsson, Hans},
    booktitle = "Proceedings of the 5th Workshop on Cognitive Aspects of the Lexicon ({C}og{AL}ex - V)",
    month = dec,
    year = "2016",
    address = "Osaka, Japan",
    publisher = "The COLING 2016 Organizing Committee",
    url = "https://www.aclweb.org/anthology/W16-5307",
    pages = "51--56",
    abstract = "In this paper we present a clean, yet effective, model for word sense disambiguation. Our approach leverage a bidirectional long short-term memory network which is shared between all words. This enables the model to share statistical strength and to scale well with vocabulary size. The model is trained end-to-end, directly from the raw text to sense labels, and makes effective use of word order. We evaluate our approach on two standard datasets, using identical hyperparameter settings, which are in turn tuned on a third set of held out data. We employ no external resources (e.g. knowledge graphs, part-of-speech tagging, etc), language specific features, or hand crafted rules, but still achieve statistically equivalent results to the best state-of-the-art systems, that employ no such limitations.",
}
@inproceedings{zock-biemann-2016-towards,
    title = "Towards a resource based on users{'} knowledge to overcome the Tip of the Tongue problem.",
    author = "Zock, Michael  and
      Biemann, Chris",
    booktitle = "Proceedings of the 5th Workshop on Cognitive Aspects of the Lexicon ({C}og{AL}ex - V)",
    month = dec,
    year = "2016",
    address = "Osaka, Japan",
    publisher = "The COLING 2016 Organizing Committee",
    url = "https://www.aclweb.org/anthology/W16-5308",
    pages = "57--68",
    abstract = "Language production is largely a matter of words which, in the case of access problems, can be searched for in an external resource (lexicon, thesaurus). In this kind of dialogue the user provides the momentarily available knowledge concerning the target and the system responds with the best guess(es) it can make given this input. As tip-of-the-tongue (ToT)-studies have shown, people always have some knowledge concerning the target (meaning fragments, number of syllables, ...) even if its complete form is eluding them. We will show here how to tap on this knowledge to build a resource likely to help authors (speakers/writers) to overcome the ToT-problem. Yet, before doing so we need a better understanding of the various kinds of knowledge people have when looking for a word. To this end, we asked crowdworkers to provide some cues to describe a given target and to specify then how each one of them relates to the target, in the hope that this could help others to find the elusive word. Next, we checked how well a given search strategy worked when being applied to differently built lexical networks. The results showed quite dramatic differences, which is not really surprising. After all, different networks are built for different purposes; hence each one of them is more or less suited for a given task. What was more surprising though is the fact that the relational information given by the users did not allow us to find the elusive word in WordNet better than without it.",
}
@inproceedings{santus-etal-2016-cogalex,
    title = "The {C}og{AL}ex-V Shared Task on the Corpus-Based Identification of Semantic Relations",
    author = "Santus, Enrico  and
      Gladkova, Anna  and
      Evert, Stefan  and
      Lenci, Alessandro",
    booktitle = "Proceedings of the 5th Workshop on Cognitive Aspects of the Lexicon ({C}og{AL}ex - V)",
    month = dec,
    year = "2016",
    address = "Osaka, Japan",
    publisher = "The COLING 2016 Organizing Committee",
    url = "https://www.aclweb.org/anthology/W16-5309",
    pages = "69--79",
    abstract = "The shared task of the 5th Workshop on Cognitive Aspects of the Lexicon (CogALex-V) aims at providing a common benchmark for testing current corpus-based methods for the identification of lexical semantic relations (synonymy, antonymy, hypernymy, part-whole meronymy) and at gaining a better understanding of their respective strengths and weaknesses. The shared task uses a challenging dataset extracted from EVALution 1.0, which contains word pairs holding the above-mentioned relations as well as semantically unrelated control items (random). The task is split into two subtasks: (i) identification of related word pairs vs. unrelated ones; (ii) classification of the word pairs according to their semantic relation. This paper describes the subtasks, the dataset, the evaluation metrics, the seven participating systems and their results. The best performing system in subtask 1 is GHHH (F1 = 0.790), while the best system in subtask 2 is LexNet (F1 = 0.445). The dataset and the task description are available at \url{https://sites.google.com/site/cogalex2016/home/shared-task}.",
}
@inproceedings{shwartz-dagan-2016-cogalex,
    title = "{C}og{AL}ex-V Shared Task: {L}ex{NET} - Integrated Path-based and Distributional Method for the Identification of Semantic Relations",
    author = "Shwartz, Vered  and
      Dagan, Ido",
    booktitle = "Proceedings of the 5th Workshop on Cognitive Aspects of the Lexicon ({C}og{AL}ex - V)",
    month = dec,
    year = "2016",
    address = "Osaka, Japan",
    publisher = "The COLING 2016 Organizing Committee",
    url = "https://www.aclweb.org/anthology/W16-5310",
    pages = "80--85",
    abstract = "We present a submission to the CogALex 2016 shared task on the corpus-based identification of semantic relations, using LexNET (Shwartz and Dagan, 2016), an integrated path-based and distributional method for semantic relation classification. The reported results in the shared task bring this submission to the third place on subtask 1 (word relatedness), and the first place on subtask 2 (semantic relation classification), demonstrating the utility of integrating the complementary path-based and distributional information sources in recognizing concrete semantic relations. Combined with a common similarity measure, LexNET performs fairly good on the word relatedness task (subtask 1). The relatively low performance of LexNET and all other systems on subtask 2, however, confirms the difficulty of the semantic relation classification task, and stresses the need to develop additional methods for this task.",
}
@inproceedings{attia-etal-2016-cogalex,
    title = "{C}og{AL}ex-V Shared Task: {GHHH} - Detecting Semantic Relations via Word Embeddings",
    author = "Attia, Mohammed  and
      Maharjan, Suraj  and
      Samih, Younes  and
      Kallmeyer, Laura  and
      Solorio, Thamar",
    booktitle = "Proceedings of the 5th Workshop on Cognitive Aspects of the Lexicon ({C}og{AL}ex - V)",
    month = dec,
    year = "2016",
    address = "Osaka, Japan",
    publisher = "The COLING 2016 Organizing Committee",
    url = "https://www.aclweb.org/anthology/W16-5311",
    pages = "86--91",
    abstract = "This paper describes our system submission to the CogALex-2016 Shared Task on Corpus-Based Identification of Semantic Relations. Our system won first place for Task-1 and second place for Task-2. The evaluation results of our system on the test set is 88.1{\%} (79.0{\%} for TRUE only) f-measure for Task-1 on detecting semantic similarity, and 76.0{\%} (42.3{\%} when excluding RANDOM) for Task-2 on identifying finer-grained semantic relations. In our experiments, we try word analogy, linear regression, and multi-task Convolutional Neural Networks (CNNs) with word embeddings from publicly available word vectors. We found that linear regression performs better in the binary classification (Task-1), while CNNs have better performance in the multi-class semantic classification (Task-2). We assume that word analogy is more suited for deterministic answers rather than handling the ambiguity of one-to-many and many-to-many relationships. We also show that classifier performance could benefit from balancing the distribution of labels in the training data.",
}
@inproceedings{evert-2016-cogalex,
    title = "{C}og{AL}ex-V Shared Task: Mach5 {--} A traditional {DSM} approach to semantic relatedness",
    author = "Evert, Stefan",
    booktitle = "Proceedings of the 5th Workshop on Cognitive Aspects of the Lexicon ({C}og{AL}ex - V)",
    month = dec,
    year = "2016",
    address = "Osaka, Japan",
    publisher = "The COLING 2016 Organizing Committee",
    url = "https://www.aclweb.org/anthology/W16-5312",
    pages = "92--97",
    abstract = "This contribution provides a strong baseline result for the CogALex-V shared task using a traditional {``}count{''}-type DSM (placed in rank 2 out of 7 in subtask 1 and rank 3 out of 6 in subtask 2). Parameter tuning experiments reveal some surprising effects and suggest that the use of random word pairs as negative examples may be problematic, guiding the parameter optimization in an undesirable direction.",
}
@inproceedings{chersoni-etal-2016-cogalex,
    title = "{C}og{AL}ex-V Shared Task: {ROOT}18",
    author = "Chersoni, Emmanuele  and
      Rambelli, Giulia  and
      Santus, Enrico",
    booktitle = "Proceedings of the 5th Workshop on Cognitive Aspects of the Lexicon ({C}og{AL}ex - V)",
    month = dec,
    year = "2016",
    address = "Osaka, Japan",
    publisher = "The COLING 2016 Organizing Committee",
    url = "https://www.aclweb.org/anthology/W16-5313",
    pages = "98--103",
    abstract = "In this paper, we describe ROOT 18, a classifier using the scores of several unsupervised distributional measures as features to discriminate between semantically related and unrelated words, and then to classify the related pairs according to their semantic relation (i.e. synonymy, antonymy, hypernymy, part-whole meronymy). Our classifier participated in the CogALex-V Shared Task, showing a solid performance on the first subtask, but a poor performance on the second subtask. The low scores reported on the second subtask suggest that distributional measures are not sufficient to discriminate between multiple semantic relations at once.",
}
@inproceedings{guggilla-2016-cogalex,
    title = "{C}og{AL}ex-V Shared Task: {CGSRC} - Classifying Semantic Relations using Convolutional Neural Networks",
    author = "Guggilla, Chinnappa",
    booktitle = "Proceedings of the 5th Workshop on Cognitive Aspects of the Lexicon ({C}og{AL}ex - V)",
    month = dec,
    year = "2016",
    address = "Osaka, Japan",
    publisher = "The COLING 2016 Organizing Committee",
    url = "https://www.aclweb.org/anthology/W16-5314",
    pages = "104--109",
    abstract = "In this paper, we describe a system (CGSRC) for classifying four semantic relations: synonym, hypernym, antonym and meronym using convolutional neural networks (CNN). We have participated in CogALex-V semantic shared task of corpus-based identification of semantic relations. Proposed approach using CNN-based deep neural networks leveraging pre-compiled word2vec distributional neural embeddings achieved 43.15{\%} weighted-F1 accuracy on subtask-1 (checking existence of a relation between two terms) and 25.24{\%} weighted-F1 accuracy on subtask-2 (classifying relation types).",
}
@inproceedings{luce-etal-2016-cogalex,
    title = "{C}og{AL}ex-V Shared Task: {LOPE}",
    author = "Luce, Kanan  and
      Yu, Jiaxing  and
      Hsieh, Shu-Kai",
    booktitle = "Proceedings of the 5th Workshop on Cognitive Aspects of the Lexicon ({C}og{AL}ex - V)",
    month = dec,
    year = "2016",
    address = "Osaka, Japan",
    publisher = "The COLING 2016 Organizing Committee",
    url = "https://www.aclweb.org/anthology/W16-5315",
    pages = "110--113",
    abstract = "Automatic discovery of semantically-related words is one of the most important NLP tasks, and has great impact on the theoretical psycholinguistic modeling of the mental lexicon. In this shared task, we employ the word embeddings model to testify two thoughts explicitly or implicitly assumed by the NLP community: (1). Word embedding models can reflect syntagmatic similarities in usage between words to distances in projected vector space. (2). Word embedding models can reflect paradigmatic relationships between words.",
}
@inproceedings{wartena-aga-2016-cogalex,
    title = "{C}og{AL}ex-V Shared Task: {H}s{H}-Supervised {--} Supervised similarity learning using entry wise product of context vectors",
    author = "Wartena, Christian  and
      Aga, Rosa Tsegaye",
    booktitle = "Proceedings of the 5th Workshop on Cognitive Aspects of the Lexicon ({C}og{AL}ex - V)",
    month = dec,
    year = "2016",
    address = "Osaka, Japan",
    publisher = "The COLING 2016 Organizing Committee",
    url = "https://www.aclweb.org/anthology/W16-5316",
    pages = "114--118",
    abstract = "The CogALex-V Shared Task provides two datasets that consists of pairs of words along with a classification of their semantic relation. The dataset for the first task distinguishes only between related and unrelated, while the second data set distinguishes several types of semantic relations. A number of recent papers propose to construct a feature vector that represents a pair of words by applying a pairwise simple operation to all elements of the feature vector. Subsequently, the pairs can be classified by training any classification algorithm on these vectors. In the present paper we apply this method to the provided datasets. We see that the results are not better than from the given simple baseline. We conclude that the results of the investigated method are strongly depended on the type of data to which it is applied.",
}
@inproceedings{aoki-nakatani-2016-study,
    title = "A Study of the Bump Alternation in {J}apanese from the Perspective of Extended/Onset Causation",
    author = "Aoki, Natsuno  and
      Nakatani, Kentaro",
    booktitle = "Proceedings of the 5th Workshop on Cognitive Aspects of the Lexicon ({C}og{AL}ex - V)",
    month = dec,
    year = "2016",
    address = "Osaka, Japan",
    publisher = "The COLING 2016 Organizing Committee",
    url = "https://www.aclweb.org/anthology/W16-5317",
    pages = "119--124",
    abstract = "This paper deals with a seldom studied object/oblique alternation phenomenon in Japanese, which. We call this the bump alternation. This phenomenon, first discussed by Sadanobu (1990), is similar to the English with/against alternation. For example, compare hit the wall with the bat [=immobile-as-direct-object frame] to hit the bat against the wall [=mobile-as-direct-object frame]). However, in the Japanese version, the case frame remains constant. Although we fundamentally question Sadanobu{'}s acceptability judgment, we also claim that the causation type (i.e., whether the event is an instance of onset or extended causation; Talmy, 1988; 2000) could make an improvement. An extended causative interpretation could improve the acceptability of the otherwise awkward immobile-as-direct-object frame. We examined this claim through a rating study, and the results showed an interaction between the Causation type (extended/onset) and the Object type (mobile/immobile) in the direction we predicted. We propose that a perspective shift on what is moving causes the {``}extended causation{''} advantage.",
}
@inproceedings{bott-etal-2016-ghost,
    title = "{G}ho{S}t-{PV}: A Representative Gold Standard of {G}erman Particle Verbs",
    author = "Bott, Stefan  and
      Khvtisavrishvili, Nana  and
      Kisselew, Max  and
      Schulte im Walde, Sabine",
    booktitle = "Proceedings of the 5th Workshop on Cognitive Aspects of the Lexicon ({C}og{AL}ex - V)",
    month = dec,
    year = "2016",
    address = "Osaka, Japan",
    publisher = "The COLING 2016 Organizing Committee",
    url = "https://www.aclweb.org/anthology/W16-5318",
    pages = "125--133",
    abstract = "German particle verbs represent a frequent type of multi-word-expression that forms a highly productive paradigm in the lexicon. Similarly to other multi-word expressions, particle verbs exhibit various levels of compositionality. One of the major obstacles for the study of compositionality is the lack of representative gold standards of human ratings. In order to address this bottleneck, this paper presents such a gold standard data set containing 400 randomly selected German particle verbs. It is balanced across several particle types and three frequency bands, and accomplished by human ratings on the degree of semantic compositionality.",
}
@inproceedings{daoud-daoud-2016-discovering,
    title = "Discovering Potential Terminological Relationships from Twitter{'}s Timed Content",
    author = "Daoud, Mohammad  and
      Daoud, Daoud",
    booktitle = "Proceedings of the 5th Workshop on Cognitive Aspects of the Lexicon ({C}og{AL}ex - V)",
    month = dec,
    year = "2016",
    address = "Osaka, Japan",
    publisher = "The COLING 2016 Organizing Committee",
    url = "https://www.aclweb.org/anthology/W16-5319",
    pages = "134--144",
    abstract = "This paper presents a method to discover possible terminological relationships from tweets. We match the histories of terms (frequency patterns). Similar history indicates a possible relationship between terms. For example, if two terms (t1, t2) appeared frequently in Twitter at particular days, and there is a {`}similarity{'} in the frequencies over a period of time, then t1 and t2 can be related. Maintaining standard terminological repository with updated relationships can be difficult; especially in a dynamic domain such as social media where thousands of new terms (neology) are coined every day. So we propose to construct a raw repository of lexical units with unconfirmed relationships. We have experimented our method on time-sensitive Arabic terms used by the online Arabic community of Twitter. We draw relationships between these terms by matching their similar frequency patterns (timelines). We use dynamic time warping as a similarity measure. For evaluation, we have selected 630 possible terms (we call them preterms) and we matched the similarity of these terms over a period of 30 days. Around 270 correct relationships were discovered with a precision of 0.61. These relationships were extracted without considering the textual context of the term.",
}
@inproceedings{fonseca-etal-2016-lexfom,
    title = "{L}exfom: a lexical functions ontology model",
    author = "Fonseca, Alexsandro  and
      Sadat, Fatiha  and
      Lareau, Fran{\c{c}}ois",
    booktitle = "Proceedings of the 5th Workshop on Cognitive Aspects of the Lexicon ({C}og{AL}ex - V)",
    month = dec,
    year = "2016",
    address = "Osaka, Japan",
    publisher = "The COLING 2016 Organizing Committee",
    url = "https://www.aclweb.org/anthology/W16-5320",
    pages = "145--155",
    abstract = "A lexical function represents a type of relation that exists between lexical units (words or expressions) in any language. For example, the antonymy is a type of relation that is represented by the lexical function Anti: Anti(big) = small. Those relations include both paradigmatic relations, i.e. vertical relations, such as synonymy, antonymy and meronymy and syntagmatic relations, i.e. horizontal relations, such as objective qualification (legitimate demand), subjective qualification (fruitful analysis), positive evaluation (good review) and support verbs (pay a visit, subject to an interrogation). In this paper, we present the Lexical Functions Ontology Model (lexfom) to represent lexical functions and the relation among lexical units. Lexfom is divided in four modules: lexical function representation (lfrep), lexical function family (lffam), lexical function semantic perspective (lfsem) and lexical function relations (lfrel). Moreover, we show how it combines to Lexical Model for Ontologies (lemon), for the transformation of lexical networks into the semantic web formats. So far, we have implemented 100 simple and 500 complex lexical functions, and encoded about 8,000 syntagmatic and 46,000 paradigmatic relations, for the French language.",
}
@inproceedings{l-homme-etal-2016-proposal,
    title = "A Proposal for combining {``}general{''} and specialized frames",
    author = "L{'} Homme, Marie-Claude  and
      Subirats, Carlos  and
      Robichaud, Beno{\^\i}t",
    booktitle = "Proceedings of the 5th Workshop on Cognitive Aspects of the Lexicon ({C}og{AL}ex - V)",
    month = dec,
    year = "2016",
    address = "Osaka, Japan",
    publisher = "The COLING 2016 Organizing Committee",
    url = "https://www.aclweb.org/anthology/W16-5321",
    pages = "156--165",
    abstract = "The objectives of the work described in this paper are: 1. To list the differences between a general language resource (namely FrameNet) and a domain-specific resource; 2. To devise solutions to merge their contents in order to increase the coverage of the general resource. Both resources are based on Frame Semantics (Fillmore 1985; Fillmore and Baker 2010) and this raises specific challenges since the theoretical framework and the methodology derived from it provide for both a lexical description and a conceptual representation. We propose a series of strategies that handle both lexical and conceptual (frame) differences and implemented them in the specialized resource. We also show that most differences can be handled in a straightforward manner. However, some more domain specific differences (such as frames defined exclusively for the specialized domain or relations between these frames) are likely to be much more difficult to take into account since some are domain-specific.",
}
@inproceedings{pastena-lenci-2016-antonymy,
    title = "Antonymy and Canonicity: Experimental and Distributional Evidence",
    author = "Pastena, Andreana  and
      Lenci, Alessandro",
    booktitle = "Proceedings of the 5th Workshop on Cognitive Aspects of the Lexicon ({C}og{AL}ex - V)",
    month = dec,
    year = "2016",
    address = "Osaka, Japan",
    publisher = "The COLING 2016 Organizing Committee",
    url = "https://www.aclweb.org/anthology/W16-5322",
    pages = "166--175",
    abstract = "The present paper investigates the phenomenon of antonym canonicity by providing new behavioural and distributional evidence on Italian adjectives. Previous studies have showed that some pairs of antonyms are perceived to be better examples of opposition than others, and are so considered representative of the whole category (e.g., Deese, 1964; Murphy, 2003; Paradis et al., 2009). Our goal is to further investigate why such canonical pairs (Murphy, 2003) exist and how they come to be associated. In the literature, two different approaches have dealt with this issue. The lexical-categorical approach (Charles and Miller, 1989; Justeson and Katz, 1991) finds the cause of canonicity in the high co-occurrence frequency of the two adjectives. The cognitive-prototype approach (Paradis et al., 2009; Jones et al., 2012) instead claims that two adjectives form a canonical pair because they are aligned along a simple and salient dimension. Our empirical evidence, while supporting the latter view, shows that the paradigmatic distributional properties of adjectives can also contribute to explain the phenomenon of canonicity, providing a corpus-based correlate of the cognitive notion of salience.",
}
@inproceedings{silva-etal-2016-categorization,
    title = "Categorization of Semantic Roles for Dictionary Definitions",
    author = "Silva, Vivian  and
      Handschuh, Siegfried  and
      Freitas, Andr{\'e}",
    booktitle = "Proceedings of the 5th Workshop on Cognitive Aspects of the Lexicon ({C}og{AL}ex - V)",
    month = dec,
    year = "2016",
    address = "Osaka, Japan",
    publisher = "The COLING 2016 Organizing Committee",
    url = "https://www.aclweb.org/anthology/W16-5323",
    pages = "176--184",
    abstract = "Understanding the semantic relationships between terms is a fundamental task in natural language processing applications. While structured resources that can express those relationships in a formal way, such as ontologies, are still scarce, a large number of linguistic resources gathering dictionary definitions is becoming available, but understanding the semantic structure of natural language definitions is fundamental to make them useful in semantic interpretation tasks. Based on an analysis of a subset of WordNet{'}s glosses, we propose a set of semantic roles that compose the semantic structure of a dictionary definition, and show how they are related to the definition{'}s syntactic configuration, identifying patterns that can be used in the development of information extraction frameworks and semantic models.",
}
@inproceedings{tomokiyo-boitet-2016-corpus,
    title = "Corpus and dictionary development for classifiers/quantifiers towards a {F}rench-{J}apanese machine translation",
    author = "Tomokiyo, Mutsuko  and
      Boitet, Christian",
    booktitle = "Proceedings of the 5th Workshop on Cognitive Aspects of the Lexicon ({C}og{AL}ex - V)",
    month = dec,
    year = "2016",
    address = "Osaka, Japan",
    publisher = "The COLING 2016 Organizing Committee",
    url = "https://www.aclweb.org/anthology/W16-5324",
    pages = "185--192",
    abstract = "Although quantifiers/classifiers expressions occur frequently in everyday communications or written documents, there is no description for them in classical bilingual paper dictionaries, nor in machine-readable dictionaries. The paper describes a corpus and dictionary development for quantifiers/classifiers, and their usage in the framework of French-Japanese machine translation (MT). They often cause problems of lexical ambiguity and of set phrase recognition during analysis, in particular for a long-distance language pair like French and Japanese. For the development of a dictionary aiming at ambiguity resolution for expressions including quantifiers and classifiers which may be ambiguous with common nouns, we have annotated our corpus with UWs (interlingual lexemes) of UNL (Universal Networking Language) found on the UNL-jp dictionary. The extraction of potential classifiers/quantifiers from corpus is made by UNLexplorer web service. Keywords : classifiers, quantifiers, phraseology study, corpus annotation, UNL (Universal Networking Language), UWs dictionary, Tori Bank, French-Japanese machine translation (MT).",
}
