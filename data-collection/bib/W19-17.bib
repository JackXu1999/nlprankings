@proceedings{ws-2019-speech,
    title = "Proceedings of the Eighth Workshop on Speech and Language Processing for Assistive Technologies",
    author = "Heidi Christensen, University of Sheffield  and
      Kristy Hollingshead, Florida Institute for Human  and
      Cognition, Machine  and
      Emily Prud{'}hommeaux, Boston College  and
      Frank Rudzicz, University of Toronto  and
      Keith Vertanen, Michigan Technological University",
    month = jun,
    year = "2019",
    address = "Minneapolis, Minnesota",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/W19-1700",
}
@inproceedings{virkkunen-etal-2019-user,
    title = "A user study to compare two conversational assistants designed for people with hearing impairments",
    author = {Virkkunen, Anja  and
      Lukkarila, Juri  and
      Palom{\"a}ki, Kalle  and
      Kurimo, Mikko},
    booktitle = "Proceedings of the Eighth Workshop on Speech and Language Processing for Assistive Technologies",
    month = jun,
    year = "2019",
    address = "Minneapolis, Minnesota",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/W19-1701",
    doi = "10.18653/v1/W19-1701",
    pages = "1--8",
    abstract = "Participating in conversations can be difficult for people with hearing loss, especially in acoustically challenging environments. We studied the preferences the hearing impaired have for a personal conversation assistant based on automatic speech recognition (ASR) technology. We created two prototypes which were evaluated by hearing impaired test users. This paper qualitatively compares the two based on the feedback obtained from the tests. The first prototype was a proof-of-concept system running real-time ASR on a laptop. The second prototype was developed for a mobile device with the recognizer running on a separate server. In the mobile device, augmented reality (AR) was used to help the hearing impaired observe gestures and lip movements of the speaker simultaneously with the transcriptions. Several testers found the systems useful enough to use in their daily lives, with majority preferring the mobile AR version. The biggest concern of the testers was the accuracy of the transcriptions and the lack of speaker identification.",
}
@inproceedings{kafle-etal-2019-modeling,
    title = "Modeling Acoustic-Prosodic Cues for Word Importance Prediction in Spoken Dialogues",
    author = "Kafle, Sushant  and
      Alm, Cissi Ovesdotter  and
      Huenerfauth, Matt",
    booktitle = "Proceedings of the Eighth Workshop on Speech and Language Processing for Assistive Technologies",
    month = jun,
    year = "2019",
    address = "Minneapolis, Minnesota",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/W19-1702",
    doi = "10.18653/v1/W19-1702",
    pages = "9--16",
    abstract = "Prosodic cues in conversational speech aid listeners in discerning a message. We investigate whether acoustic cues in spoken dialogue can be used to identify the importance of individual words to the meaning of a conversation turn. Individuals who are Deaf and Hard of Hearing often rely on real-time captions in live meetings. Word error rate, a traditional metric for evaluating automatic speech recognition (ASR), fails to capture that some words are more important for a system to transcribe correctly than others. We present and evaluate neural architectures that use acoustic features for 3-class word importance prediction. Our model performs competitively against state-of-the-art text-based word-importance prediction models, and it demonstrates particular benefits when operating on imperfect ASR output.",
}
@inproceedings{cao-etal-2019-permanent,
    title = "Permanent Magnetic Articulograph ({PMA}) vs Electromagnetic Articulograph ({EMA}) in Articulation-to-Speech Synthesis for Silent Speech Interface",
    author = "Cao, Beiming  and
      Sebkhi, Nordine  and
      Mau, Ted  and
      Inan, Omer T.  and
      Wang, Jun",
    booktitle = "Proceedings of the Eighth Workshop on Speech and Language Processing for Assistive Technologies",
    month = jun,
    year = "2019",
    address = "Minneapolis, Minnesota",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/W19-1703",
    doi = "10.18653/v1/W19-1703",
    pages = "17--23",
    abstract = "Silent speech interfaces (SSIs) are devices that enable speech communication when audible speech is unavailable. Articulation-to-speech (ATS) synthesis is a software design in SSI that directly converts articulatory movement information into audible speech signals. Permanent magnetic articulograph (PMA) is a wireless articulator motion tracking technology that is similar to commercial, wired Electromagnetic Articulograph (EMA). PMA has shown great potential for practical SSI applications, because it is wireless. The ATS performance of PMA, however, is unknown when compared with current EMA. In this study, we compared the performance of ATS using a PMA we recently developed and a commercially available EMA (NDI Wave system). Datasets with same stimuli and size that were collected from tongue tip were used in the comparison. The experimental results indicated the performance of PMA was close to, although not as equally good as that of EMA. Furthermore, in PMA, converting the raw magnetic signals to positional signals did not significantly affect the performance of ATS, which support the future direction in PMA-based ATS can be focused on the use of positional signals to maximize the benefit of spatial analysis.",
}
@inproceedings{wisler-etal-2019-speech,
    title = "Speech-based Estimation of Bulbar Regression in Amyotrophic Lateral Sclerosis",
    author = "Wisler, Alan  and
      Teplansky, Kristin  and
      Green, Jordan  and
      Yunusova, Yana  and
      Campbell, Thomas  and
      Heitzman, Daragh  and
      Wang, Jun",
    booktitle = "Proceedings of the Eighth Workshop on Speech and Language Processing for Assistive Technologies",
    month = jun,
    year = "2019",
    address = "Minneapolis, Minnesota",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/W19-1704",
    doi = "10.18653/v1/W19-1704",
    pages = "24--31",
    abstract = "Amyotrophic Lateral Sclerosis (ALS) is a progressive neurological disease that leads to degeneration of motor neurons and, as a result, inhibits the ability of the brain to control muscle movements. Monitoring the progression of ALS is of fundamental importance due to the wide variability in disease outlook that exists across patients. This progression is typically tracked using the ALS functional rating scale - revised (ALSFRS-R), which is the current clinical assessment of a patient{'}s level of functional impairment including speech and other motor tasks. In this paper, we investigated automatic estimation of the ALSFRS-R bulbar subscore from acoustic and articulatory movement samples. Experimental results demonstrated the AFSFRS-R bulbar subscore can be predicted from speech samples, which has clinical implication for automatic monitoring of the disease progression of ALS using speech information.",
}
@inproceedings{sohail-traum-2019-blissymbolics,
    title = "A Blissymbolics Translation System",
    author = "Sohail, Usman  and
      Traum, David",
    booktitle = "Proceedings of the Eighth Workshop on Speech and Language Processing for Assistive Technologies",
    month = jun,
    year = "2019",
    address = "Minneapolis, Minnesota",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/W19-1705",
    doi = "10.18653/v1/W19-1705",
    pages = "32--36",
    abstract = "Blissymbolics (Bliss) is a pictographic writing system that is used by people with communication disorders. Bliss attempts to create a writing system that makes words easier to distinguish by using pictographic symbols that encapsulate meaning rather than sound, as the English alphabet does for example. Users of Bliss rely on human interpreters to use Bliss. We created a translation system from Bliss to natural English with the hopes of decreasing the reliance on human interpreters by the Bliss community. We first discuss the basic rules of Blissymbolics. Then we point out some of the challenges associated with developing computer assisted tools for Blissymbolics. Next we talk about our ongoing work in developing a translation system, including current limitations, and future work. We conclude with a set of examples showing the current capabilities of our translation system.",
}
@inproceedings{adhikary-etal-2019-investigating,
    title = "Investigating Speech Recognition for Improving Predictive {AAC}",
    author = "Adhikary, Jiban  and
      Watling, Robbie  and
      Fletcher, Crystal  and
      Stanage, Alex  and
      Vertanen, Keith",
    booktitle = "Proceedings of the Eighth Workshop on Speech and Language Processing for Assistive Technologies",
    month = jun,
    year = "2019",
    address = "Minneapolis, Minnesota",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/W19-1706",
    doi = "10.18653/v1/W19-1706",
    pages = "37--43",
    abstract = "Making good letter or word predictions can help accelerate the communication of users of high-tech AAC devices. This is particularly important for real-time person-to-person conversations. We investigate whether per forming speech recognition on the speaking-side of a conversation can improve language model based predictions. We compare the accuracy of three plausible microphone deployment options and the accuracy of two commercial speech recognition engines (Google and IBM Watson). We found that despite recognition word error rates of 7-16{\%}, our ensemble of N-gram and recurrent neural network language models made predictions nearly as good as when they used the reference transcripts.",
}
@inproceedings{dong-etal-2019-noisy,
    title = "Noisy Neural Language Modeling for Typing Prediction in {BCI} Communication",
    author = "Dong, Rui  and
      Smith, David  and
      Dudy, Shiran  and
      Bedrick, Steven",
    booktitle = "Proceedings of the Eighth Workshop on Speech and Language Processing for Assistive Technologies",
    month = jun,
    year = "2019",
    address = "Minneapolis, Minnesota",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/W19-1707",
    doi = "10.18653/v1/W19-1707",
    pages = "44--51",
    abstract = "Language models have broad adoption in predictive typing tasks. When the typing history contains numerous errors, as in open-vocabulary predictive typing with brain-computer interface (BCI) systems, we observe significant performance degradation in both n-gram and recurrent neural network language models trained on clean text. In evaluations of ranking character predictions, training recurrent LMs on noisy text makes them much more robust to noisy histories, even when the error model is misspecified. We also propose an effective strategy for combining evidence from multiple ambiguous histories of BCI electroencephalogram measurements.",
}
