@proceedings{ws-2018-semantic,
    title = "Proceedings of the Third Workshop on Semantic Deep Learning",
    author = "Anke, Luis Espinosa  and
      Gromann, Dagmar  and
      Declerck, Thierry",
    month = aug,
    year = "2018",
    address = "Santa Fe, New Mexico",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/W18-4000",
}
@inproceedings{gupta-etal-2018-replicated,
    title = "Replicated {S}iamese {LSTM} in Ticketing System for Similarity Learning and Retrieval in Asymmetric Texts",
    author = {Gupta, Pankaj  and
      Andrassy, Bernt  and
      Sch{\"u}tze, Hinrich},
    booktitle = "Proceedings of the Third Workshop on Semantic Deep Learning",
    month = aug,
    year = "2018",
    address = "Santa Fe, New Mexico",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/W18-4001",
    pages = "1--11",
    abstract = "The goal of our industrial ticketing system is to retrieve a relevant solution for an input query, by matching with historical tickets stored in knowledge base. A query is comprised of subject and description, while a historical ticket consists of subject, description and solution. To retrieve a relevant solution, we use textual similarity paradigm to learn similarity in the query and historical tickets. The task is challenging due to significant term mismatch in the query and ticket pairs of asymmetric lengths, where subject is a short text but description and solution are multi-sentence texts. We present a novel Replicated Siamese LSTM model to learn similarity in asymmetric text pairs, that gives 22{\%} and 7{\%} gain (Accuracy@10) for retrieval task, respectively over unsupervised and supervised baselines. We also show that the topic and distributed semantic features for short and long texts improved both similarity learning and retrieval.",
}
@inproceedings{yoon-etal-2018-word,
    title = "Word-Embedding based Content Features for Automated Oral Proficiency Scoring",
    author = "Yoon, Su-Youn  and
      Loukina, Anastassia  and
      Lee, Chong Min  and
      Mulholland, Matthew  and
      Wang, Xinhao  and
      Choi, Ikkyu",
    booktitle = "Proceedings of the Third Workshop on Semantic Deep Learning",
    month = aug,
    year = "2018",
    address = "Santa Fe, New Mexico",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/W18-4002",
    pages = "12--22",
    abstract = "In this study, we develop content features for an automated scoring system of non-native English speakers{'} spontaneous speech. The features calculate the lexical similarity between the question text and the ASR word hypothesis of the spoken response, based on traditional word vector models or word embeddings. The proposed features do not require any sample training responses for each question, and this is a strong advantage since collecting question-specific data is an expensive task, and sometimes even impossible due to concerns about question exposure. We explore the impact of these new features on the automated scoring of two different question types: (a) providing opinions on familiar topics and (b) answering a question about a stimulus material. The proposed features showed statistically significant correlations with the oral proficiency scores, and the combination of new features with the speech-driven features achieved a small but significant further improvement for the latter question type. Further analyses suggested that the new features were effective in assigning more accurate scores for responses with serious content issues.",
}
@inproceedings{nieto-pina-johansson-2018-automatically,
    title = "Automatically Linking Lexical Resources with Word Sense Embedding Models",
    author = "Nieto-Pi{\~n}a, Luis  and
      Johansson, Richard",
    booktitle = "Proceedings of the Third Workshop on Semantic Deep Learning",
    month = aug,
    year = "2018",
    address = "Santa Fe, New Mexico",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/W18-4003",
    pages = "23--29",
    abstract = "Automatically learnt word sense embeddings are developed as an attempt to refine the capabilities of coarse word embeddings. The word sense representations obtained this way are, however, sensitive to underlying corpora and parameterizations, and they might be difficult to relate to formally defined word senses. We propose to tackle this problem by devising a mechanism to establish links between word sense embeddings and lexical resources created by experts. We evaluate the applicability of these links in a task to retrieve instances of word sense unlisted in the lexicon.",
}
@inproceedings{ezeani-etal-2018-transferred,
    title = "Transferred Embeddings for {I}gbo Similarity, Analogy, and Diacritic Restoration Tasks",
    author = "Ezeani, Ignatius  and
      Onyenwe, Ikechukwu  and
      Hepple, Mark",
    booktitle = "Proceedings of the Third Workshop on Semantic Deep Learning",
    month = aug,
    year = "2018",
    address = "Santa Fe, New Mexico",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/W18-4004",
    pages = "30--38",
    abstract = "Existing NLP models are mostly trained with data from well-resourced languages. Most minority languages face the challenge of lack of resources - data and technologies - for NLP research. Building these resources from scratch for each minority language will be very expensive, time-consuming and amount largely to unnecessarily re-inventing the wheel. In this paper, we applied transfer learning techniques to create Igbo word embeddings from a variety of existing English trained embeddings. Transfer learning methods were also used to build standard datasets for Igbo word similarity and analogy tasks for intrinsic evaluation of embeddings. These projected embeddings were also applied to diacritic restoration task. Our results indicate that the projected models not only outperform the trained ones on the semantic-based tasks of analogy, word-similarity, and odd-word identifying, but they also achieve enhanced performance on the diacritic restoration with learned diacritic embeddings.",
}
@inproceedings{parupalli-etal-2018-towards-enhancing,
    title = "Towards Enhancing Lexical Resource and Using Sense-annotations of {O}nto{S}ense{N}et for Sentiment Analysis",
    author = "Parupalli, Sreekavitha  and
      Anvesh Rao, Vijjini  and
      Mamidi, Radhika",
    booktitle = "Proceedings of the Third Workshop on Semantic Deep Learning",
    month = aug,
    year = "2018",
    address = "Santa Fe, New Mexico",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/W18-4005",
    pages = "39--44",
    abstract = "This paper illustrates the interface of the tool we developed for crowd sourcing and we explain the annotation procedure in detail. Our tool is named as {`}పారుపల్లి పదజాలం{'} (Parupalli Padajaalam) which means web of words by Parupalli. The aim of this tool is to populate the OntoSenseNet, sentiment polarity annotated Telugu resource. Recent works have shown the importance of word-level annotations on sentiment analysis. With this as basis, we aim to analyze the importance of sense-annotations obtained from OntoSenseNet in performing the task of sentiment analysis. We explain the features extracted from OntoSenseNet (Telugu). Furthermore we compute and explain the adverbial class distribution of verbs in OntoSenseNet. This task is known to aid in disambiguating word-senses which helps in enhancing the performance of word-sense disambiguation (WSD) task(s).",
}
@inproceedings{schockaert-2018-knowledge,
    title = "Knowledge Representation with Conceptual Spaces",
    author = "Schockaert, Steven",
    booktitle = "Proceedings of the Third Workshop on Semantic Deep Learning",
    month = aug,
    year = "2018",
    address = "Santa Fe, New Mexico",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/W18-4006",
    pages = "45",
    abstract = "Entity embeddings are vector space representations of a given domain of interest. They are typically learned from text corpora (possibly in combination with any available structured knowledge), based on the intuition that similar entities should be represented by similar vectors. The usefulness of such entity embeddings largely stems from the fact that they implicitly encode a rich amount of knowledge about the considered domain, beyond mere similarity. In an embedding of movies, for instance, we may expect all movies from a given genre to be located in some low-dimensional manifold. This is particularly useful in supervised learning settings, where it may e.g. allow neural movie recommenders to base predictions on the genre of a movie, without that genre having to be specified explicitly for each movie, or without even the need to specify that the genre of a movie is a property that may have predictive value for the considered task. In unsupervised settings, however, such implicitly encoded knowledge cannot be leveraged. Conceptual spaces, as proposed by Grdenfors, are similar to entity embeddings, but provide more structure. In conceptual spaces, among others, dimensions are interpretable and grouped into facets, and properties and concepts are explicitly modelled as (vague) regions. Thanks to this additional structure, conceptual spaces can be used as a knowledge representation framework, which can also be effectively exploited in unsupervised settings. Given a conceptual space of movies, for instance, we are able to answer queries that ask about similarity w.r.t. a particular facet (e.g. movies which are cinematographically similar to Jurassic Park), that refer to a given feature (e.g. movies which are scarier than Jurassic Park but otherwise similar), or that refer to particular properties or concepts (e.g. thriller from the 1990s with a dinosaur theme). Compared to standard entity embeddings, however, conceptual spaces are more challenging to learn in a purely data-driven fashion. In this talk, I will give an overview of some approaches for learning such representations that have recently been developed within the context of the FLEXILOG project.",
}
@inproceedings{christodoulopoulos-2018-knowledge,
    title = "Knowledge Representation and Extraction at Scale",
    author = "Christodoulopoulos, Christos",
    booktitle = "Proceedings of the Third Workshop on Semantic Deep Learning",
    month = aug,
    year = "2018",
    address = "Santa Fe, New Mexico",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/W18-4007",
    pages = "46",
    abstract = "These days, most general knowledge question-answering systems rely on large-scale knowledge bases comprising billions of facts about millions of entities. Having a structured source of semantic knowledge means that we can answer questions involving single static facts (e.g. {``}Who was the 8th president of the US?{''}) or dynamically generated ones (e.g. {``}How old is Donald Trump?{''}). More importantly, we can answer questions involving multiple inference steps ({``}Is the queen older than the president of the US?{''}). In this talk, I{'}m going to be discussing some of the unique challenges that are involved with building and maintaining a consistent knowledge base for Alexa, extending it with new facts and using it to serve answers in multiple languages. I will focus on three recent projects from our group. First, a way of measuring the completeness of a knowledge base, that is based on usage patterns. The definition of the usage of the KB is done in terms of the relation distribution of entities seen in question-answer logs. Instead of directly estimating the relation distribution of individual entities, it is generalized to the {``}class signature{''} of each entity. For example, users ask for baseball players{'} height, age, and batting average, so a knowledge base is complete (with respect to baseball players) if every entity has facts for those three relations. Second, an investigation into fact extraction from unstructured text. I will present a method for creating distant (weak) supervision labels for training a large-scale relation extraction system. I will also discuss the effectiveness of neural network approaches by decoupling the model architecture from the feature design of a state-of-the-art neural network system. Surprisingly, a much simpler classifier trained on similar features performs on par with the highly complex neural network system (at 75x reduction to the training time), suggesting that the features are a bigger contributor to the final performance. Finally, I will present the Fact Extraction and VERification (FEVER) dataset and challenge. The dataset comprises more than 185,000 human-generated claims extracted from Wikipedia pages. False claims were generated by mutating true claims in a variety of ways, some of which were meaningaltering. During the verification step, annotators were required to label a claim for its validity and also supply full-sentence textual evidence from (potentially multiple) Wikipedia articles for the label. With FEVER, we aim to help create a new generation of transparent and interprable knowledge extraction systems.",
}
