[{"id":"W18-2501","title":"{A}llen{NLP}: A Deep Semantic Natural Language Processing Platform","authors":["Gardner, Matt","Grus, Joel","Neumann, Mark","Tafjord, Oyvind","Dasigi, Pradeep","Liu, Nelson F.","Peters, Matthew","Schmitz, Michael","Zettlemoyer, Luke"],"emails":["","","","","","","","",""],"author_id":["matt-gardner","joel-grus","mark-neumann","oyvind-tafjord","pradeep-dasigi","nelson-f-liu","matthew-peters","michael-schmitz","luke-zettlemoyer"],"abstract":"Modern natural language processing (NLP) research requires writing code. Ideally this code would provide a precise definition of the approach, easy repeatability of results, and a basis for extending the research. However, many research codebases bury high-level parameters under implementation details, are challenging to run and debug, and are difficult enough to extend that they are more likely to be rewritten. This paper describes AllenNLP, a library for applying deep learning methods to NLP research that addresses these issues with easy-to-use command-line tools, declarative configuration-driven experiments, and modular NLP abstractions. AllenNLP has already increased the rate of research experimentation and the sharing of NLP components at the Allen Institute for Artificial Intelligence, and we are working to have the same impact across the field.","pages":"1--6","doi":"10.18653\/v1\/W18-2501","url":"https:\/\/www.aclweb.org\/anthology\/W18-2501","publisher":"Association for Computational Linguistics","address":"Melbourne, Australia","year":"2018","month":"July","booktitle":"Proceedings of Workshop for {NLP} Open Source Software ({NLP}-{OSS})"},{"id":"W18-2502","title":"Stop Word Lists in Free Open-source Software Packages","authors":["Nothman, Joel","Qin, Hanmin","Yurchak, Roman"],"emails":["joel.nothman@gmail.com","qinhanmin2005@sina.com","rth.yurchak@gmail.com"],"author_id":["joel-nothman","hanmin-qin","roman-yurchak"],"abstract":"Open-source software packages for language processing often include stop word lists. Users may apply them without awareness of their surprising omissions (e.g. {``}hasn{'}t{''} but not {``}hadn{'}t{''}) and inclusions ({``}computer{''}), or their incompatibility with a particular tokenizer. Motivated by issues raised about the Scikit-learn stop list, we investigate variation among and consistency within 52 popular English-language stop lists, and propose strategies for mitigating these issues.","pages":"7--12","doi":"10.18653\/v1\/W18-2502","url":"https:\/\/www.aclweb.org\/anthology\/W18-2502","publisher":"Association for Computational Linguistics","address":"Melbourne, Australia","year":"2018","month":"July","booktitle":"Proceedings of Workshop for {NLP} Open Source Software ({NLP}-{OSS})"},{"id":"W18-2503","title":"{T}exar: A Modularized, Versatile, and Extensible Toolbox for Text Generation","authors":["Hu, Zhiting","Yang, Zichao","Zhao, Tiancheng","Shi, Haoran","He, Junxian","Wang, Di","Ma, Xuezhe","Liu, Zhengzhong","Liang, Xiaodan","Qin, Lianhui","Chaplot, Devendra Singh","Tan, Bowen","Yu, Xingjiang","Xing, Eric"],"emails":["zhitingh@cs.cmu.edu","","","","","","","","","","","","",""],"author_id":["zhiting-hu","zichao-yang","tiancheng-zhao","haoran-shi","junxian-he","di-wang","xuezhe-ma","zhengzhong-liu","xiaodan-liang","lianhui-qin","devendra-singh-chaplot","bowen-tan","xingjiang-yu","eric-xing"],"abstract":"We introduce Texar, an open-source toolkit aiming to support the broad set of text generation tasks. Different from many existing toolkits that are specialized for specific applications (e.g., neural machine translation), Texar is designed to be highly flexible and versatile. This is achieved by abstracting the common patterns underlying the diverse tasks and methodologies, creating a library of highly reusable modules and functionalities, and enabling arbitrary model architectures and various algorithmic paradigms. The features make Texar particularly suitable for technique sharing and generalization across different text generation applications. The toolkit emphasizes heavily on extensibility and modularized system design, so that components can be freely plugged in or swapped out. We conduct extensive experiments and case studies to demonstrate the use and advantage of the toolkit.","pages":"13--22","doi":"10.18653\/v1\/W18-2503","url":"https:\/\/www.aclweb.org\/anthology\/W18-2503","publisher":"Association for Computational Linguistics","address":"Melbourne, Australia","year":"2018","month":"July","booktitle":"Proceedings of Workshop for {NLP} Open Source Software ({NLP}-{OSS})"},{"id":"W18-2504","title":"The {ACL} Anthology: Current State and Future Directions","authors":["Gildea, Daniel","Kan, Min-Yen","Madnani, Nitin","Teichmann, Christoph","Villalba, Mart{\\'\\i}n"],"emails":["gildea@cs.rochester.edu","kanmy@comp.nus.edu.sg","nmadnani@ets.org","","villalba@coli.uni-saarland.de"],"author_id":["daniel-gildea","min-yen-kan","nitin-madnani","christoph-teichmann","martin-villalba"],"abstract":"The Association of Computational Linguistic{'}s Anthology is the open source archive, and the main source for computational linguistics and natural language processing{'}s scientific literature. The ACL Anthology is currently maintained exclusively by community volunteers and has to be available and up-to-date at all times. We first discuss the current, open source approach used to achieve this, and then discuss how the planned use of Docker images will improve the Anthology{'}s long-term stability. This change will make it easier for researchers to utilize Anthology data for experimentation. We believe the ACL community can directly benefit from the extension-friendly architecture of the Anthology. We end by issuing an open challenge of reviewer matching we encourage the community to rally towards.","pages":"23--28","doi":"10.18653\/v1\/W18-2504","url":"https:\/\/www.aclweb.org\/anthology\/W18-2504","publisher":"Association for Computational Linguistics","address":"Melbourne, Australia","year":"2018","month":"July","booktitle":"Proceedings of Workshop for {NLP} Open Source Software ({NLP}-{OSS})"},{"id":"W18-2505","title":"The risk of sub-optimal use of Open Source {NLP} Software: {UKB} is inadvertently state-of-the-art in knowledge-based {WSD}","authors":["Agirre, Eneko","L{\\'o}pez de Lacalle, Oier","Soroa, Aitor"],"emails":["e.agirre@ehu.eus","oier.lopezdelacalle@ehu.eus","a.soroa@ehu.eus"],"author_id":["eneko-agirre","oier-lopez-de-lacalle","aitor-soroa"],"abstract":"UKB is an open source collection of programs for performing, among other tasks, Knowledge-Based Word Sense Disambiguation (WSD). Since it was released in 2009 it has been often used out-of-the-box in sub-optimal settings. We show that nine years later it is the state-of-the-art on knowledge-based WSD. This case shows the pitfalls of releasing open source NLP software without optimal default settings and precise instructions for reproducibility.","pages":"29--33","doi":"10.18653\/v1\/W18-2505","url":"https:\/\/www.aclweb.org\/anthology\/W18-2505","publisher":"Association for Computational Linguistics","address":"Melbourne, Australia","year":"2018","month":"July","booktitle":"Proceedings of Workshop for {NLP} Open Source Software ({NLP}-{OSS})"},{"id":"W18-2506","title":"{B}aseline: A Library for Rapid Modeling, Experimentation and Development of Deep Learning Algorithms targeting {NLP}","authors":["Pressel, Daniel","Ray Choudhury, Sagnik","Lester, Brian","Zhao, Yanjie","Barta, Matt"],"emails":["dpressel@interactions.com","schoudhury@interactions.com","blester@interactions.com","yzhao@interactions.com","mbarta@interactions.com"],"author_id":["daniel-pressel","sagnik-ray-choudhury","brian-lester","yanjie-zhao","matt-barta"],"abstract":"We introduce Baseline: a library for reproducible deep learning research and fast model development for NLP. The library provides easily extensible abstractions and implementations for data loading, model development, training and export of deep learning architectures. It also provides implementations for simple, high-performance, deep learning models for various NLP tasks, against which newly developed models can be compared. Deep learning experiments are hard to reproduce, Baseline provides functionalities to track them. The goal is to allow a researcher to focus on model development, delegating the repetitive tasks to the library.","pages":"34--40","doi":"10.18653\/v1\/W18-2506","url":"https:\/\/www.aclweb.org\/anthology\/W18-2506","publisher":"Association for Computational Linguistics","address":"Melbourne, Australia","year":"2018","month":"July","booktitle":"Proceedings of Workshop for {NLP} Open Source Software ({NLP}-{OSS})"},{"id":"W18-2507","title":"{O}pen{S}eq2{S}eq: Extensible Toolkit for Distributed and Mixed Precision Training of Sequence-to-Sequence Models","authors":["Kuchaiev, Oleksii","Ginsburg, Boris","Gitman, Igor","Lavrukhin, Vitaly","Case, Carl","Micikevicius, Paulius"],"emails":["okuchaiev@nvidia.com","bginsburg@nvidia.com","igitman@nvidia.com","vlavrukhin@nvidia.com","carlc@nvidia.com","pauliusm@nvidia.com"],"author_id":["oleksii-kuchaiev","boris-ginsburg","igor-gitman","vitaly-lavrukhin","carl-case","paulius-micikevicius"],"abstract":"We present OpenSeq2Seq {--} an open-source toolkit for training sequence-to-sequence models. The main goal of our toolkit is to allow researchers to most effectively explore different sequence-to-sequence architectures. The efficiency is achieved by fully supporting distributed and mixed-precision training. OpenSeq2Seq provides building blocks for training encoder-decoder models for neural machine translation and automatic speech recognition. We plan to extend it with other modalities in the future.","pages":"41--46","doi":"10.18653\/v1\/W18-2507","url":"https:\/\/www.aclweb.org\/anthology\/W18-2507","publisher":"Association for Computational Linguistics","address":"Melbourne, Australia","year":"2018","month":"July","booktitle":"Proceedings of Workshop for {NLP} Open Source Software ({NLP}-{OSS})"},{"id":"W18-2508","title":"Integrating Multiple {NLP} Technologies into an Open-source Platform for Multilingual Media Monitoring","authors":["Germann, Ulrich","Liepins, Ren{\\=a}rs","Gosko, Didzis","Barzdins, Guntis"],"emails":["ugermann@ed.ac.uk","renars.liepins@leta.lv","didzis.gosko@leta.lv","guntis.barzdins@lu.lv"],"author_id":["ulrich-germann","renars-liepins","didzis-gosko","guntis-barzdins"],"abstract":"The open-source SUMMA Platform is a highly scalable distributed architecture for monitoring a large number of media broadcasts in parallel, with a lag behind actual broadcast time of at most a few minutes. It assembles numerous state-of-the-art NLP technologies into a fully automated media ingestion pipeline that can record live broadcasts, detect and transcribe spoken content, translate from several languages (original text or transcribed speech) into English, recognize Named Entities, detect topics, cluster and summarize documents across language barriers, and extract and store factual claims in these news items. This paper describes the intended use cases and discusses the system design decisions that allowed us to integrate state-of-the-art NLP modules into an effective workflow with comparatively little effort.","pages":"47--51","doi":"10.18653\/v1\/W18-2508","url":"https:\/\/www.aclweb.org\/anthology\/W18-2508","publisher":"Association for Computational Linguistics","address":"Melbourne, Australia","year":"2018","month":"July","booktitle":"Proceedings of Workshop for {NLP} Open Source Software ({NLP}-{OSS})"},{"id":"W18-2509","title":"The Annotated Transformer","authors":["Rush, Alexander"],"emails":["srush@seas.harvard.edu"],"author_id":["alexander-m-rush"],"abstract":"(Note this is not being submitted blind. The chair of the workshop requested this submission unblinded from me on twitter, so assuming that is okay.) A major goal of open-source NLP is to quickly and accurately reproduce the results of new work, in a manner that the community can easily use and modify. While most papers publish enough detail for replication, it still may be difficult to achieve good results in practice. This paper presents a worked exercise of paper reproduction with the goal of implementing the results of the recent Transformer model. The replication exercise aims at simple code structure that follows closely with the original work, while achieving an efficient usable system.","pages":"52--60","doi":"10.18653\/v1\/W18-2509","url":"https:\/\/www.aclweb.org\/anthology\/W18-2509","publisher":"Association for Computational Linguistics","address":"Melbourne, Australia","year":"2018","month":"July","booktitle":"Proceedings of Workshop for {NLP} Open Source Software ({NLP}-{OSS})"}]