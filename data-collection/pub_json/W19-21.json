[{"id":"W19-2101","title":"Not My President: How Names and Titles Frame Political Figures","authors":["van den Berg, Esther","Korfhage, Katharina","Ruppenhofer, Josef","Wiegand, Michael","Markert, Katja"],"emails":["vdberg@cl.uni-heidelberg.de","korfhage@cl.uni-heidelberg.de","ruppenhofer@ids-mannheim.de","wiegand@ids-mannheim.de","markert@cl.uni-heidelberg.de"],"author_id":["esther-van-den-berg","katharina-korfhage","josef-ruppenhofer","michael-wiegand","katja-markert"],"abstract":"Naming and titling have been discussed in sociolinguistics as markers of status or solidarity. However, these functions have not been studied on a larger scale or for social media data. We collect a corpus of tweets mentioning presidents of six G20 countries by various naming forms. We show that naming variation relates to stance towards the president in a way that is suggestive of a framing effect mediated by respectfulness. This confirms sociolinguistic theory of naming and titling as markers of status.","pages":"1--6","doi":"10.18653\/v1\/W19-2101","url":"https:\/\/www.aclweb.org\/anthology\/W19-2101","publisher":"Association for Computational Linguistics","address":"Minneapolis, Minnesota","year":"2019","month":"June","booktitle":"Proceedings of the Third Workshop on Natural Language Processing and Computational Social Science"},{"id":"W19-2102","title":"Identification, Interpretability, and {B}ayesian Word Embeddings","authors":["Lauretig, Adam"],"emails":["lauretig.1@osu.edu"],"author_id":["adam-lauretig"],"abstract":"Social scientists have recently turned to analyzing text using tools from natural language processing like word embeddings to measure concepts like ideology, bias, and affinity. However, word embeddings are difficult to use in the regression framework familiar to social scientists: embeddings are are neither identified, nor directly interpretable. I offer two advances on standard embedding models to remedy these problems. First, I develop Bayesian Word Embeddings with Automatic Relevance Determination priors, relaxing the assumption that all embedding dimensions have equal weight. Second, I apply work identifying latent variable models to anchor embeddings, identifying them, and making them interpretable and usable in a regression. I then apply this model and anchoring approach to two cases, the shift in internationalist rhetoric in the American presidents{'} inaugural addresses, and the relationship between bellicosity in American foreign policy decision-makers{'} deliberations. I find that inaugural addresses became less internationalist after 1945, which goes against the conventional wisdom, and that an increase in bellicosity is associated with an increase in hostile actions by the United States, showing that elite deliberations are not cheap talk, and helping confirm the validity of the model.","pages":"7--17","doi":"10.18653\/v1\/W19-2102","url":"https:\/\/www.aclweb.org\/anthology\/W19-2102","publisher":"Association for Computational Linguistics","address":"Minneapolis, Minnesota","year":"2019","month":"June","booktitle":"Proceedings of the Third Workshop on Natural Language Processing and Computational Social Science"},{"id":"W19-2103","title":"Tweet Classification without the Tweet: An Empirical Examination of User versus Document Attributes","authors":["Lynn, Veronica","Giorgi, Salvatore","Balasubramanian, Niranjan","Schwartz, H. Andrew"],"emails":["velynn@cs.stonybrook.edu","sgiorgi@sas.upenn.edu","niranjan@cs.stonybrook.edu","has@cs.stonybrook.edu"],"author_id":["veronica-lynn","salvatore-giorgi","niranjan-balasubramanian","h-andrew-schwartz"],"abstract":"NLP naturally puts a primary focus on leveraging document language, occasionally considering user attributes as supplemental. However, as we tackle more social scientific tasks, it is possible user attributes might be of primary importance and the document supplemental. Here, we systematically investigate the predictive power of user-level features alone versus document-level features for document-level tasks. We first show user attributes can sometimes carry more task-related information than the document itself. For example, a tweet-level stance detection model using only 13 user-level attributes (i.e. features that did not depend on the specific tweet) was able to obtain a higher F1 than the top-performing SemEval participant. We then consider multiple tasks and a wider range of user attributes, showing the performance of strong document-only models can often be improved (as in stance, sentiment, and sarcasm) with user attributes, particularly benefiting tasks with stable {``}trait-like{''} outcomes (e.g. stance) most relative to frequently changing {``}state-like{''} outcomes (e.g. sentiment). These results not only support the growing work on integrating user factors into predictive systems, but that some of our NLP tasks might be better cast primarily as user-level (or human) tasks.","pages":"18--28","doi":"10.18653\/v1\/W19-2103","url":"https:\/\/www.aclweb.org\/anthology\/W19-2103","publisher":"Association for Computational Linguistics","address":"Minneapolis, Minnesota","year":"2019","month":"June","booktitle":"Proceedings of the Third Workshop on Natural Language Processing and Computational Social Science"},{"id":"W19-2104","title":"Geolocating Political Events in Text","authors":["Halterman, Andrew"],"emails":["ahalt@mit.edu"],"author_id":["andrew-halterman"],"abstract":"This work introduces a general method for automatically finding the locations where political events in text occurred. Using a novel set of 8,000 labeled sentences, I create a method to link automatically extracted events and locations in text. The model achieves human level performance on the annotation task and outperforms previous event geolocation systems. It can be applied to most event extraction systems across geographic contexts. I formalize the event{--}location linking task, describe the neural network model, describe the potential uses of such a system in political science, and demonstrate a workflow to answer an open question on the role of conventional military offensives in causing civilian casualties in the Syrian civil war.","pages":"29--39","doi":"10.18653\/v1\/W19-2104","url":"https:\/\/www.aclweb.org\/anthology\/W19-2104","publisher":"Association for Computational Linguistics","address":"Minneapolis, Minnesota","year":"2019","month":"June","booktitle":"Proceedings of the Third Workshop on Natural Language Processing and Computational Social Science"},{"id":"W19-2105","title":"Neural Network Prediction of Censorable Language","authors":["Ng, Kei Yin","Feldman, Anna","Peng, Jing","Leberknight, Chris"],"emails":["ngk2@montclair.edu","feldmana@montclair.edu","pengj@montclair.edu","leberknightc@montclair.edu"],"author_id":["kei-yin-ng","anna-feldman","jing-peng","chris-leberknight"],"abstract":"Internet censorship imposes restrictions on what information can be publicized or viewed on the Internet. According to Freedom House{'}s annual Freedom on the Net report, more than half the world{'}s Internet users now live in a place where the Internet is censored or restricted. China has built the world{'}s most extensive and sophisticated online censorship system. In this paper, we describe a new corpus of censored and uncensored social media tweets from a Chinese microblogging website, Sina Weibo, collected by tracking posts that mention {`}sensitive{'} topics or authored by {`}sensitive{'} users. We use this corpus to build a neural network classifier to predict censorship. Our model performs with a 88.50{\\%} accuracy using only linguistic features. We discuss these features in detail and hypothesize that they could potentially be used for censorship circumvention.","pages":"40--46","doi":"10.18653\/v1\/W19-2105","url":"https:\/\/www.aclweb.org\/anthology\/W19-2105","publisher":"Association for Computational Linguistics","address":"Minneapolis, Minnesota","year":"2019","month":"June","booktitle":"Proceedings of the Third Workshop on Natural Language Processing and Computational Social Science"},{"id":"W19-2106","title":"Modeling performance differences on cognitive tests using {LSTM}s and skip-thought vectors trained on reported media consumption.","authors":["Courtland, Maury","Davani, Aida","Reyes, Melissa","Yeh, Leigh","Leung, Jun","Kennedy, Brendan","Dehghani, Morteza","Zevin, Jason"],"emails":["landerpo@usc.edu","mostafaz@usc.edu","reyesmel@usc.edu","leighyeh@usc.edu","junyenle@usc.edu","btkenned@usc.edu","mdehghan@usc.edu","zevin@usc.edu"],"author_id":["maury-courtland","aida-davani","melissa-reyes","leigh-yeh","jun-leung","brendan-kennedy","morteza-dehghani","jason-zevin"],"abstract":"Cognitive tests have traditionally resorted to standardizing testing materials in the name of equality and because of the onerous nature of creating test items. This approach ignores participants{'} diverse language experiences that potentially significantly affect testing outcomes. Here, we seek to explain our prior finding of significant performance differences on two cognitive tests (reading span and SPiN) between clusters of participants based on their media consumption. Here, we model the language contained in these media sources using an LSTM trained on corpora of each cluster{'}s media sources to predict target words. We also model semantic similarity of test items with each cluster{'}s corpus using skip-thought vectors. We find robust, significant correlations between performance on the SPiN test and the LSTMs and skip-thought models we present here, but not the reading span test.","pages":"47--53","doi":"10.18653\/v1\/W19-2106","url":"https:\/\/www.aclweb.org\/anthology\/W19-2106","publisher":"Association for Computational Linguistics","address":"Minneapolis, Minnesota","year":"2019","month":"June","booktitle":"Proceedings of the Third Workshop on Natural Language Processing and Computational Social Science"},{"id":"W19-2107","title":"Using time series and natural language processing to identify viral moments in the 2016 {U}.{S}. Presidential Debate","authors":["Lukito, Josephine","K Sarma, Prathusha","Foley, Jordan","Abhishek, Aman"],"emails":["jlukito@wisc.edu","kameswarasar@wisc.edu","jfoley5@wisc.edu","aabhishek@wisc.edu"],"author_id":["josephine-lukito","prathusha-kameswara-sarma","jordan-foley","aman-abhishek"],"abstract":"This paper proposes a method for identifying and studying viral moments or highlights during a political debate. Using a combined strategy of time series analysis and domain adapted word embeddings, this study provides an in-depth analysis of several key moments during the 2016 U.S. Presidential election. First, a time series outlier analysis is used to identify key moments during the debate. These moments had to result in a long-term shift in attention towards either Hillary Clinton or Donald Trump (i.e., a transient change outlier or an intervention, resulting in a permanent change in the time series). To assess whether these moments also resulted in a discursive shift, two corpora are produced for each potential viral moment (a pre-viral corpus and post-viral corpus). A domain adaptation layer learns weights to combine a generic and domain-specific (DS) word embedding into a domain adapted (DA) embedding. Words are then classified using a generic encoder+ classifier framework that relies on these word embeddings as inputs. Results suggest that both Clinton and Trump were able to induce discourse-shifting viral moments, though the former is much better at producing a topically-specific discursive shift.","pages":"54--64","doi":"10.18653\/v1\/W19-2107","url":"https:\/\/www.aclweb.org\/anthology\/W19-2107","publisher":"Association for Computational Linguistics","address":"Minneapolis, Minnesota","year":"2019","month":"June","booktitle":"Proceedings of the Third Workshop on Natural Language Processing and Computational Social Science"},{"id":"W19-2108","title":"Stance Classification, Outcome Prediction, and Impact Assessment: {NLP} Tasks for Studying Group Decision-Making","authors":["Mayfield, Elijah","Black, Alan"],"emails":["elijah@cmu.edu","awb@cs.cmu.edu"],"author_id":["elijah-mayfield","alan-w-black"],"abstract":"In group decision-making, the nuanced process of conflict and resolution that leads to consensus formation is closely tied to the quality of decisions made. Behavioral scientists rarely have rich access to process variables, though, as unstructured discussion transcripts are difficult to analyze. Here, we define ways for NLP researchers to contribute to the study of groups and teams. We introduce three tasks alongside a large new corpus of over 400,000 group debates on Wikipedia. We describe the tasks and their importance, then provide baselines showing that BERT contextualized word embeddings consistently outperform other language representations.","pages":"65--77","doi":"10.18653\/v1\/W19-2108","url":"https:\/\/www.aclweb.org\/anthology\/W19-2108","publisher":"Association for Computational Linguistics","address":"Minneapolis, Minnesota","year":"2019","month":"June","booktitle":"Proceedings of the Third Workshop on Natural Language Processing and Computational Social Science"},{"id":"W19-2109","title":"A Sociolinguistic Study of Online Echo Chambers on Twitter","authors":["Duseja, Nikita","Jhamtani, Harsh"],"emails":["nduseja@tamu.edu","jharsh@cmu.edu"],"author_id":["nikita-duseja","harsh-jhamtani"],"abstract":"Online social media platforms such as Facebook and Twitter are increasingly facing criticism for polarization of users. One particular aspect which has caught the attention of various critics is presence of users in echo chambers - a situation wherein users are exposed mostly to the opinions which are in sync with their own views. In this paper, we perform a sociolinguistic study by comparing the tweets of users in echo chambers with the tweets of users not in echo chambers with similar levels of polarity on a broad topic. Specifically, we carry out a comparative analysis of tweet structure, lexical choices, and focus issues, and provide possible explanations for the results.","pages":"78--83","doi":"10.18653\/v1\/W19-2109","url":"https:\/\/www.aclweb.org\/anthology\/W19-2109","publisher":"Association for Computational Linguistics","address":"Minneapolis, Minnesota","year":"2019","month":"June","booktitle":"Proceedings of the Third Workshop on Natural Language Processing and Computational Social Science"},{"id":"W19-2110","title":"Uphill from here: Sentiment patterns in videos from left- and right-wing {Y}ou{T}ube news channels","authors":["Soldner, Felix","Ho, Justin Chun-ting","Makhortykh, Mykola","van der Vegt, Isabelle W.J.","Mozes, Maximilian","Kleinberg, Bennett"],"emails":["felix.soldner@ucl.ac.uk","2justin.ho@ed.ac.uk","3m.makhortykh@uva.nl","isabelle.vandervegt@ucl.ac.uk","4mozes@cs.tum.edu","bennett.kleinberg@ucl.ac.uk"],"author_id":["felix-soldner","justin-chun-ting-ho","mykola-makhortykh","isabelle-w-j-van-der-vegt","maximilian-mozes","bennett-kleinberg"],"abstract":"News consumption exhibits an increasing shift towards online sources, which bring platforms such as YouTube more into focus. Thus, the distribution of politically loaded news is easier, receives more attention, but also raises the concern of forming isolated ideological communities. Understanding how such news is communicated and received is becoming increasingly important. To expand our understanding in this domain, we apply a linguistic temporal trajectory analysis to analyze sentiment patterns in English-language videos from news channels on YouTube. We examine transcripts from videos distributed through eight channels with pro-left and pro-right political leanings. Using unsupervised clustering, we identify seven different sentiment patterns in the transcripts. We found that the use of two sentiment patterns differed significantly depending on political leaning. Furthermore, we used predictive models to examine how different sentiment patterns relate to video popularity and if they differ depending on the channel{'}s political leaning. No clear relations between sentiment patterns and popularity were found. However, results indicate, that videos from pro-right news channels are more popular and that a negative sentiment further increases that popularity, when sentiments are averaged for each video.","pages":"84--93","doi":"10.18653\/v1\/W19-2110","url":"https:\/\/www.aclweb.org\/anthology\/W19-2110","publisher":"Association for Computational Linguistics","address":"Minneapolis, Minnesota","year":"2019","month":"June","booktitle":"Proceedings of the Third Workshop on Natural Language Processing and Computational Social Science"},{"id":"W19-2111","title":"Simple dynamic word embeddings for mapping perceptions in the public sphere","authors":["Gillani, Nabeel","Levy, Roger"],"emails":["ngillani@mit.edu","rplevy@mit.edu"],"author_id":["nabeel-gillani","roger-levy"],"abstract":"Word embeddings trained on large-scale historical corpora can illuminate human biases and stereotypes that perpetuate social inequalities. These embeddings are often trained in separate vector space models defined according to different attributes of interest. In this paper, we introduce a single, unified dynamic embedding model that learns attribute-specific word embeddings and apply it to a novel dataset{---}talk radio shows from around the US{---}to analyze perceptions about refugees. We validate our model on a benchmark dataset and apply it to two corpora of talk radio shows averaging 117 million words produced over one month across 83 stations and 64 cities. Our findings suggest that dynamic word embeddings are capable of identifying nuanced differences in public discourse about contentious topics, suggesting their usefulness as a tool for better understanding how the public perceives and engages with different issues across time, geography, and other dimensions.","pages":"94--99","doi":"10.18653\/v1\/W19-2111","url":"https:\/\/www.aclweb.org\/anthology\/W19-2111","publisher":"Association for Computational Linguistics","address":"Minneapolis, Minnesota","year":"2019","month":"June","booktitle":"Proceedings of the Third Workshop on Natural Language Processing and Computational Social Science"},{"id":"W19-2112","title":"Modeling Behavioral Aspects of Social Media Discourse for Moral Classification","authors":["Johnson, Kristen","Goldwasser, Dan"],"emails":["john1187@purdue.edu","dgoldwas@purdue.edu"],"author_id":["kristen-johnson","dan-goldwasser"],"abstract":"Political discourse on social media microblogs, specifically Twitter, has become an undeniable part of mainstream U.S. politics. Given the length constraint of tweets, politicians must carefully word their statements to ensure their message is understood by their intended audience. This constraint often eliminates the context of the tweet, making automatic analysis of social media political discourse a difficult task. To overcome this challenge, we propose simultaneous modeling of high-level abstractions of political language, such as political slogans and framing strategies, with abstractions of how politicians behave on Twitter. These behavioral abstractions can be further leveraged as forms of supervision in order to increase prediction accuracy, while reducing the burden of annotation. In this work, we use Probabilistic Soft Logic (PSL) to build relational models to capture the similarities in language and behavior that obfuscate political messages on Twitter. When combined, these descriptors reveal the moral foundations underlying the discourse of U.S. politicians online, \\textit{across} differing governing administrations, showing how party talking points remain cohesive or change over time.","pages":"100--109","doi":"10.18653\/v1\/W19-2112","url":"https:\/\/www.aclweb.org\/anthology\/W19-2112","publisher":"Association for Computational Linguistics","address":"Minneapolis, Minnesota","year":"2019","month":"June","booktitle":"Proceedings of the Third Workshop on Natural Language Processing and Computational Social Science"}]