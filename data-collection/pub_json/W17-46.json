[{"id":"W17-4601","title":"Functions of Silences towards Information Flow in Spoken Conversation","authors":["Chowdhury, Shammur Absar","Stepanov, Evgeny","Danieli, Morena","Riccardi, Giuseppe"],"emails":["","","",""],"author_id":["shammur-absar-chowdhury","evgeny-stepanov","morena-danieli","giuseppe-riccardi"],"abstract":"Silence is an integral part of the most frequent turn-taking phenomena in spoken conversations. Silence is sized and placed within the conversation flow and it is coordinated by the speakers along with the other speech acts. The objective of this analytical study is twofold: to explore the functions of silence with duration of one second and above, towards information flow in a dyadic conversation utilizing the sequences of dialog acts present in the turns surrounding the silence itself; and to design a feature space useful for clustering the silences using a hierarchical concept formation algorithm. The resulting clusters are manually grouped into functional categories based on their similarities. It is observed that the silence plays an important role in response preparation, also can indicate speakers{'} hesitation or indecisiveness. It is also observed that sometimes long silences can be used deliberately to get a forced response from another speaker thus making silence a multi-functional and an important catalyst towards information flow.","pages":"1--9","doi":"10.18653\/v1\/W17-4601","url":"https:\/\/www.aclweb.org\/anthology\/W17-4601","publisher":"Association for Computational Linguistics","address":"Copenhagen, Denmark","year":"2017","month":"September","booktitle":"Proceedings of the Workshop on Speech-Centric Natural Language Processing"},{"id":"W17-4602","title":"Encoding Word Confusion Networks with Recurrent Neural Networks for Dialog State Tracking","authors":["Jagfeld, Glorianna","Vu, Ngoc Thang"],"emails":["glorianna.jagfeld@ims.uni-stuttgart.de","thang.vu@ims.uni-stuttgart.de"],"author_id":["glorianna-jagfeld","ngoc-thang-vu"],"abstract":"This paper presents our novel method to encode word confusion networks, which can represent a rich hypothesis space of automatic speech recognition systems, via recurrent neural networks. We demonstrate the utility of our approach for the task of dialog state tracking in spoken dialog systems that relies on automatic speech recognition output. Encoding confusion networks outperforms encoding the best hypothesis of the automatic speech recognition in a neural system for dialog state tracking on the well-known second Dialog State Tracking Challenge dataset.","pages":"10--17","doi":"10.18653\/v1\/W17-4602","url":"https:\/\/www.aclweb.org\/anthology\/W17-4602","publisher":"Association for Computational Linguistics","address":"Copenhagen, Denmark","year":"2017","month":"September","booktitle":"Proceedings of the Workshop on Speech-Centric Natural Language Processing"},{"id":"W17-4603","title":"Analyzing Human and Machine Performance In Resolving Ambiguous Spoken Sentences","authors":["Ghaly, Hussein","Mandel, Michael"],"emails":["hghaly@gc.cuny.edu","mmandel@gc.cuny.edu"],"author_id":["hussein-ghaly","michael-mandel"],"abstract":"Written sentences can be more ambiguous than spoken sentences. We investigate this difference for two different types of ambiguity: prepositional phrase (PP) attachment and sentences where the addition of commas changes the meaning. We recorded a native English speaker saying several of each type of sentence both with and without disambiguating contextual information. These sentences were then presented either as text or audio and either with or without context to subjects who were asked to select the proper interpretation of the sentence. Results suggest that comma-ambiguous sentences are easier to disambiguate than PP-attachment-ambiguous sentences, possibly due to the presence of clear prosodic boundaries, namely silent pauses. Subject performance for sentences with PP-attachment ambiguity without context was 52{\\%} for text only while it was 72.4{\\%} for audio only, suggesting that audio has more disambiguating information than text. Using an analysis of acoustic features of two PP-attachment sentences, a simple classifier was implemented to resolve the PP-attachment ambiguity being early or late closure with a mean accuracy of 80{\\%}.","pages":"18--26","doi":"10.18653\/v1\/W17-4603","url":"https:\/\/www.aclweb.org\/anthology\/W17-4603","publisher":"Association for Computational Linguistics","address":"Copenhagen, Denmark","year":"2017","month":"September","booktitle":"Proceedings of the Workshop on Speech-Centric Natural Language Processing"},{"id":"W17-4604","title":"Parsing transcripts of speech","authors":["Caines, Andrew","McCarthy, Michael","Buttery, Paula"],"emails":["mactoft@cantab.net","pjb48@cam.ac.uk","apc38@cam.ac.uk"],"author_id":["andrew-caines","michael-mccarthy","paula-buttery"],"abstract":"We present an analysis of parser performance on speech data, comparing word type and token frequency distributions with written data, and evaluating parse accuracy by length of input string. We find that parser performance tends to deteriorate with increasing length of string, more so for spoken than for written texts. We train an alternative parsing model with added speech data and demonstrate improvements in accuracy on speech-units, with no deterioration in performance on written text.","pages":"27--36","doi":"10.18653\/v1\/W17-4604","url":"https:\/\/www.aclweb.org\/anthology\/W17-4604","publisher":"Association for Computational Linguistics","address":"Copenhagen, Denmark","year":"2017","month":"September","booktitle":"Proceedings of the Workshop on Speech-Centric Natural Language Processing"},{"id":"W17-4605","title":"Enriching {ASR} Lattices with {POS} Tags for Dependency Parsing","authors":["Stiefel, Moritz","Vu, Ngoc Thang"],"emails":["moritz.stiefel@ims.uni-stuttgart.de","thang.vu@ims.uni-stuttgart.de"],"author_id":["moritz-stiefel","ngoc-thang-vu"],"abstract":"Parsing speech requires a richer representation than 1-best or n-best hypotheses, e.g. lattices. Moreover, previous work shows that part-of-speech (POS) tags are a valuable resource for parsing. In this paper, we therefore explore a joint modeling approach of automatic speech recognition (ASR) and POS tagging to enrich ASR word lattices. To that end, we manipulate the ASR process from the pronouncing dictionary onward to use word-POS pairs instead of words. We evaluate ASR, POS tagging and dependency parsing (DP) performance demonstrating a successful lattice-based integration of ASR and POS tagging.","pages":"37--47","doi":"10.18653\/v1\/W17-4605","url":"https:\/\/www.aclweb.org\/anthology\/W17-4605","publisher":"Association for Computational Linguistics","address":"Copenhagen, Denmark","year":"2017","month":"September","booktitle":"Proceedings of the Workshop on Speech-Centric Natural Language Processing"},{"id":"W17-4606","title":"End-to-End Information Extraction without Token-Level Supervision","authors":["Palm, Rasmus Berg","Hovy, Dirk","Laws, Florian","Winther, Ole"],"emails":["rapal@dtu.dk","dirk.hovy@di.ku.dk","fla@tradeshift.com","olwi@dtu.dk"],"author_id":["rasmus-berg-palm","dirk-hovy","florian-laws","ole-winther"],"abstract":"Most state-of-the-art information extraction approaches rely on token-level labels to find the areas of interest in text. Unfortunately, these labels are time-consuming and costly to create, and consequently, not available for many real-life IE tasks. To make matters worse, token-level labels are usually not the desired output, but just an intermediary step. End-to-end (E2E) models, which take raw text as input and produce the desired output directly, need not depend on token-level labels. We propose an E2E model based on pointer networks, which can be trained directly on pairs of raw input and output text. We evaluate our model on the ATIS data set, MIT restaurant corpus and the MIT movie corpus and compare to neural baselines that do use token-level labels. We achieve competitive results, within a few percentage points of the baselines, showing the feasibility of E2E information extraction without the need for token-level labels. This opens up new possibilities, as for many tasks currently addressed by human extractors, raw input and output data are available, but not token-level labels.","pages":"48--52","doi":"10.18653\/v1\/W17-4606","url":"https:\/\/www.aclweb.org\/anthology\/W17-4606","publisher":"Association for Computational Linguistics","address":"Copenhagen, Denmark","year":"2017","month":"September","booktitle":"Proceedings of the Workshop on Speech-Centric Natural Language Processing"},{"id":"W17-4607","title":"Spoken Term Discovery for Language Documentation using Translations","authors":["Anastasopoulos, Antonios","Bansal, Sameer","Chiang, David","Goldwater, Sharon","Lopez, Adam"],"emails":["","","","",""],"author_id":["antonios-anastasopoulos","sameer-bansal","david-chiang","sharon-goldwater","adam-lopez"],"abstract":"Vast amounts of speech data collected for language documentation and research remain untranscribed and unsearchable, but often a small amount of speech may have text translations available. We present a method for partially labeling additional speech with translations in this scenario. We modify an unsupervised speech-to-translation alignment model and obtain prototype speech segments that match the translation words, which are in turn used to discover terms in the unlabelled data. We evaluate our method on a Spanish-English speech translation corpus and on two corpora of endangered languages, Arapaho and Ainu, demonstrating its appropriateness and applicability in an actual very-low-resource scenario.","pages":"53--58","doi":"10.18653\/v1\/W17-4607","url":"https:\/\/www.aclweb.org\/anthology\/W17-4607","publisher":"Association for Computational Linguistics","address":"Copenhagen, Denmark","year":"2017","month":"September","booktitle":"Proceedings of the Workshop on Speech-Centric Natural Language Processing"},{"id":"W17-4608","title":"{A}mharic-{E}nglish Speech Translation in Tourism Domain","authors":["Melese, Michael","Besacier, Laurent","Meshesha, Million"],"emails":["michael.melese@aau.edu.et","laurent.besacier@imag.fr","michael.melese@aau.edu.et"],"author_id":["michael-melese","laurent-besacier","million-meshesha"],"abstract":"This paper describes speech translation from Amharic-to-English, particularly Automatic Speech Recognition (ASR) with post-editing feature and Amharic-English Statistical Machine Translation (SMT). ASR experiment is conducted using morpheme language model (LM) and phoneme acoustic model(AM). Likewise,SMT conducted using word and morpheme as unit. Morpheme based translation shows a 6.29 BLEU score at a 76.4{\\%} of recognition accuracy while word based translation shows a 12.83 BLEU score using 77.4{\\%} word recognition accuracy. Further, after post-edit on Amharic ASR using corpus based n-gram, the word recognition accuracy increased by 1.42{\\%}. Since post-edit approach reduces error propagation, the word based translation accuracy improved by 0.25 (1.95{\\%}) BLEU score. We are now working towards further improving propagated errors through different algorithms at each unit of speech translation cascading component.","pages":"59--66","doi":"10.18653\/v1\/W17-4608","url":"https:\/\/www.aclweb.org\/anthology\/W17-4608","publisher":"Association for Computational Linguistics","address":"Copenhagen, Denmark","year":"2017","month":"September","booktitle":"Proceedings of the Workshop on Speech-Centric Natural Language Processing"},{"id":"W17-4609","title":"Speech- and Text-driven Features for Automated Scoring of {E}nglish Speaking Tasks","authors":["Loukina, Anastassia","Madnani, Nitin","Cahill, Aoife"],"emails":["aloukina@ets.org","nmadnani@ets.org","acahill@ets.org"],"author_id":["anastassia-loukina","nitin-madnani","aoife-cahill"],"abstract":"We consider the automatic scoring of a task for which both the content of the response as well its spoken fluency are important. We combine features from a text-only content scoring system originally designed for written responses with several categories of acoustic features. Although adding any single category of acoustic features to the text-only system on its own does not significantly improve performance, adding all acoustic features together does yield a small but significant improvement. These results are consistent for responses to open-ended questions and to questions focused on some given source material.","pages":"67--77","doi":"10.18653\/v1\/W17-4609","url":"https:\/\/www.aclweb.org\/anthology\/W17-4609","publisher":"Association for Computational Linguistics","address":"Copenhagen, Denmark","year":"2017","month":"September","booktitle":"Proceedings of the Workshop on Speech-Centric Natural Language Processing"},{"id":"W17-4610","title":"Improving coreference resolution with automatically predicted prosodic information","authors":["Roesiger, Ina","Stehwien, Sabrina","Riester, Arndt","Vu, Ngoc Thang"],"emails":["roesigia@ims.uni-stuttgart.de","stehwisa@ims.uni-stuttgart.de","arndt@ims.uni-stuttgart.de","thangvu@ims.uni-stuttgart.de"],"author_id":["ina-roesiger","sabrina-stehwien","arndt-riester","ngoc-thang-vu"],"abstract":"Adding manually annotated prosodic information, specifically pitch accents and phrasing, to the typical text-based feature set for coreference resolution has previously been shown to have a positive effect on German data. Practical applications on spoken language, however, would rely on automatically predicted prosodic information. In this paper we predict pitch accents (and phrase boundaries) using a convolutional neural network (CNN) model from acoustic features extracted from the speech signal. After an assessment of the quality of these automatic prosodic annotations, we show that they also significantly improve coreference resolution.","pages":"78--83","doi":"10.18653\/v1\/W17-4610","url":"https:\/\/www.aclweb.org\/anthology\/W17-4610","publisher":"Association for Computational Linguistics","address":"Copenhagen, Denmark","year":"2017","month":"September","booktitle":"Proceedings of the Workshop on Speech-Centric Natural Language Processing"}]