[{"id":"W19-2301","title":"An Adversarial Learning Framework For A Persona-Based Multi-Turn Dialogue Model","authors":["Olabiyi, Oluwatobi","Khazane, Anish","Salimov, Alan","Mueller, Erik"],"emails":["oluwatobi.olabiyi@capitalone.com","anish.khazane@capitalone.com","alan.salimov@capitalone.com","erik.mueller@capitalone.com"],"author_id":["oluwatobi-olabiyi","anish-khazane","alan-salimov","erik-mueller"],"abstract":"In this paper, we extend the persona-based sequence-to-sequence (Seq2Seq) neural network conversation model to a multi-turn dialogue scenario by modifying the state-of-the- art hredGAN architecture to simultaneously capture utterance attributes such as speaker identity, dialogue topic, speaker sentiments and so on. The proposed system, phredGAN has a persona-based HRED generator (PHRED) and a conditional discriminator. We also explore two approaches to accomplish the conditional discriminator: (1) phredGANa, a system that passes the attribute representation as an additional input into a traditional adversarial discriminator, and (2) phredGANd, a dual discriminator system which in addition to the adversarial discriminator, collaboratively predicts the attribute(s) that generated the input utterance. To demonstrate the superior performance of phredGAN over the persona Seq2Seq model, we experiment with two conversational datasets, the Ubuntu Dialogue Corpus (UDC) and TV series transcripts from the Big Bang Theory and Friends. Performance comparison is made with respect to a variety of quantitative measures as well as crowd-sourced human evaluation. We also explore the trade-offs from using either variant of phredGAN on datasets with many but weak attribute modalities (such as with Big Bang Theory and Friends) and ones with few but strong attribute modalities (customer-agent interactions in Ubuntu dataset).","pages":"1--10","doi":"10.18653\/v1\/W19-2301","url":"https:\/\/www.aclweb.org\/anthology\/W19-2301","publisher":"Association for Computational Linguistics","address":"Minneapolis, Minnesota","year":"2019","month":"June","booktitle":"Proceedings of the Workshop on Methods for Optimizing and Evaluating Neural Language Generation"},{"id":"W19-2302","title":"{DAL}: Dual Adversarial Learning for Dialogue Generation","authors":["Cui, Shaobo","Lian, Rongzhong","Jiang, Di","Song, Yuanfeng","Bao, Siqi","Jiang, Yong"],"emails":["cuishaobo16@mails.tsinghua.edu.cn","lianrongzhong@baidu.com","jiangdi@baidu.com","songyuanfeng@baidu.com","baosiqi@baidu.com","jiangy@sz.tsinghua.edu.cn"],"author_id":["shaobo-cui","rongzhong-lian","di-jiang","yuanfeng-song","siqi-bao","yong-jiang"],"abstract":"In open-domain dialogue systems, generative approaches have attracted much attention for response generation. However, existing methods are heavily plagued by generating safe responses and unnatural responses. To alleviate these two problems, we propose a novel framework named Dual Adversarial Learning(DAL) for high-quality response generation. DAL innovatively utilizes the duality between query generation and response generation to avoid safe responses and increase the diversity of the generated responses. Additionally, DAL uses adversarial learning to mimic human judges and guides the system to generate natural responses. Experimental results demonstrate that DAL effectively improves both diversity and overall quality of the generated responses. DAL outperforms state-of-the-art methods regarding automatic metrics and human evaluations.","pages":"11--20","doi":"10.18653\/v1\/W19-2302","url":"https:\/\/www.aclweb.org\/anthology\/W19-2302","publisher":"Association for Computational Linguistics","address":"Minneapolis, Minnesota","year":"2019","month":"June","booktitle":"Proceedings of the Workshop on Methods for Optimizing and Evaluating Neural Language Generation"},{"id":"W19-2303","title":"How to Compare Summarizers without Target Length? Pitfalls, Solutions and Re-Examination of the Neural Summarization Literature","authors":["Sun, Simeng","Shapira, Ori","Dagan, Ido","Nenkova, Ani"],"emails":["simsun@seas.upenn.edu","obspp18@gmail.com","dagan@cs.biu.ac.il","nenkova@seas.upenn.edu"],"author_id":["simeng-sun","ori-shapira","ido-dagan","ani-nenkova"],"abstract":"We show that plain ROUGE F1 scores are not ideal for comparing current neural systems which on average produce different lengths. This is due to a non-linear pattern between ROUGE F1 and summary length. To alleviate the effect of length during evaluation, we have proposed a new method which normalizes the ROUGE F1 scores of a system by that of a random system with same average output length. A pilot human evaluation has shown that humans prefer short summaries in terms of the verbosity of a summary but overall consider longer summaries to be of higher quality. While human evaluations are more expensive in time and resources, it is clear that normalization, such as the one we proposed for automatic evaluation, will make human evaluations more meaningful.","pages":"21--29","doi":"10.18653\/v1\/W19-2303","url":"https:\/\/www.aclweb.org\/anthology\/W19-2303","publisher":"Association for Computational Linguistics","address":"Minneapolis, Minnesota","year":"2019","month":"June","booktitle":"Proceedings of the Workshop on Methods for Optimizing and Evaluating Neural Language Generation"},{"id":"W19-2304","title":"{BERT} has a Mouth, and It Must Speak: {BERT} as a {M}arkov Random Field Language Model","authors":["Wang, Alex","Cho, Kyunghyun"],"emails":["alexwang@nyu.edu","kyunghyun.cho@nyu.edu"],"author_id":["alex-wang","kyunghyun-cho"],"abstract":"We show that BERT (Devlin et al., 2018) is a Markov random field language model. This formulation gives way to a natural procedure to sample sentences from BERT. We generate from BERT and find that it can produce high quality, fluent generations. Compared to the generations of a traditional left-to-right language model, BERT generates sentences that are more diverse but of slightly worse quality.","pages":"30--36","doi":"10.18653\/v1\/W19-2304","url":"https:\/\/www.aclweb.org\/anthology\/W19-2304","publisher":"Association for Computational Linguistics","address":"Minneapolis, Minnesota","year":"2019","month":"June","booktitle":"Proceedings of the Workshop on Methods for Optimizing and Evaluating Neural Language Generation"},{"id":"W19-2305","title":"Neural Text Simplification in Low-Resource Conditions Using Weak Supervision","authors":["Palmero Aprosio, Alessio","Tonelli, Sara","Turchi, Marco","Negri, Matteo","Di Gangi, Mattia A."],"emails":["aprosio@fbk.eu","satonelli@fbk.eu","turchi@fbk.eu","negri@fbk.eu","digangi@fbk.eu"],"author_id":["alessio-palmero-aprosio","sara-tonelli","marco-turchi","matteo-negri","mattia-a-di-gangi"],"abstract":"Neural text simplification has gained increasing attention in the NLP community thanks to recent advancements in deep sequence-to-sequence learning. Most recent efforts with such a data-demanding paradigm have dealt with the English language, for which sizeable training datasets are currently available to deploy competitive models. Similar improvements on less resource-rich languages are conditioned either to intensive manual work to create training data, or to the design of effective automatic generation techniques to bypass the data acquisition bottleneck. Inspired by the machine translation field, in which synthetic parallel pairs generated from monolingual data yield significant improvements to neural models, in this paper we exploit large amounts of heterogeneous data to automatically select simple sentences, which are then used to create synthetic simplification pairs. We also evaluate other solutions, such as oversampling and the use of external word embeddings to be fed to the neural simplification system. Our approach is evaluated on Italian and Spanish, for which few thousand gold sentence pairs are available. The results show that these techniques yield performance improvements over a baseline sequence-to-sequence configuration.","pages":"37--44","doi":"10.18653\/v1\/W19-2305","url":"https:\/\/www.aclweb.org\/anthology\/W19-2305","publisher":"Association for Computational Linguistics","address":"Minneapolis, Minnesota","year":"2019","month":"June","booktitle":"Proceedings of the Workshop on Methods for Optimizing and Evaluating Neural Language Generation"},{"id":"W19-2306","title":"Paraphrase Generation for Semi-Supervised Learning in {NLU}","authors":["Cho, Eunah","Xie, He","Campbell, William M."],"emails":["eunahch@amazon.com","hexie@amazon.com","cmpw@amazon.com"],"author_id":["eunah-cho","he-xie","william-m-campbell"],"abstract":"Semi-supervised learning is an efficient way to improve performance for natural language processing systems. In this work, we propose Para-SSL, a scheme to generate candidate utterances using paraphrasing and methods from semi-supervised learning. In order to perform paraphrase generation in the context of a dialog system, we automatically extract paraphrase pairs to create a paraphrase corpus. Using this data, we build a paraphrase generation system and perform one-to-many generation, followed by a validation step to select only the utterances with good quality. The paraphrase-based semi-supervised learning is applied to five functionalities in a natural language understanding system. Our proposed method for semi-supervised learning using paraphrase generation does not require user utterances and can be applied prior to releasing a new functionality to a system. Experiments show that we can achieve up to 19{\\%} of relative slot error reduction without an access to user utterances, and up to 35{\\%} when leveraging live traffic utterances.","pages":"45--54","doi":"10.18653\/v1\/W19-2306","url":"https:\/\/www.aclweb.org\/anthology\/W19-2306","publisher":"Association for Computational Linguistics","address":"Minneapolis, Minnesota","year":"2019","month":"June","booktitle":"Proceedings of the Workshop on Methods for Optimizing and Evaluating Neural Language Generation"},{"id":"W19-2307","title":"Bilingual-{GAN}: A Step Towards Parallel Text Generation","authors":["Rashid, Ahmad","Do-Omri, Alan","Haidar, Md. Akmal","Liu, Qun","Rezagholizadeh, Mehdi"],"emails":["ahmad.rashid@huawei.com","alan.do.omri@huawei.com","md.akmal.haidar@huawei.com","qun.liu@huawei.com","mehdi.rezagholizadeh@huawei.com"],"author_id":["ahmad-rashid","alan-do-omri","md-akmal-haidar","qun-liu","mehdi-rezagholizadeh"],"abstract":"Latent space based GAN methods and attention based sequence to sequence models have achieved impressive results in text generation and unsupervised machine translation respectively. Leveraging the two domains, we propose an adversarial latent space based model capable of generating parallel sentences in two languages concurrently and translating bidirectionally. The bilingual generation goal is achieved by sampling from the latent space that is shared between both languages. First two denoising autoencoders are trained, with shared encoders and back-translation to enforce a shared latent state between the two languages. The decoder is shared for the two translation directions. Next, a GAN is trained to generate synthetic {`}code{'} mimicking the languages{'} shared latent space. This code is then fed into the decoder to generate text in either language. We perform our experiments on Europarl and Multi30k datasets, on the English-French language pair, and document our performance using both supervised and unsupervised machine translation.","pages":"55--64","doi":"10.18653\/v1\/W19-2307","url":"https:\/\/www.aclweb.org\/anthology\/W19-2307","publisher":"Association for Computational Linguistics","address":"Minneapolis, Minnesota","year":"2019","month":"June","booktitle":"Proceedings of the Workshop on Methods for Optimizing and Evaluating Neural Language Generation"},{"id":"W19-2308","title":"Designing a Symbolic Intermediate Representation for Neural Surface Realization","authors":["Elder, Henry","Foster, Jennifer","Barry, James","O{'}Connor, Alexander"],"emails":["henry.elder@adaptcentre.ie","jennifer.foster@dcu.ie","james.barry@adaptcentre.ie","alex.oconnor@autodesk.com"],"author_id":["henry-elder","jennifer-foster","james-barry","alexander-oconnor"],"abstract":"Generated output from neural NLG systems often contain errors such as hallucination, repetition or contradiction. This work focuses on designing a symbolic intermediate representation to be used in multi-stage neural generation with the intention of reducing the frequency of failed outputs. We show that surface realization from this intermediate representation is of high quality and when the full system is applied to the E2E dataset it outperforms the winner of the E2E challenge. Furthermore, by breaking out the surface realization step from typically end-to-end neural systems, we also provide a framework for non-neural based content selection and planning systems to potentially take advantage of semi-supervised pretraining of neural surface realization models.","pages":"65--73","doi":"10.18653\/v1\/W19-2308","url":"https:\/\/www.aclweb.org\/anthology\/W19-2308","publisher":"Association for Computational Linguistics","address":"Minneapolis, Minnesota","year":"2019","month":"June","booktitle":"Proceedings of the Workshop on Methods for Optimizing and Evaluating Neural Language Generation"},{"id":"W19-2309","title":"Neural Text Style Transfer via Denoising and Reranking","authors":["Lee, Joseph","Xie, Ziang","Wang, Cindy","Drach, Max","Jurafsky, Dan","Ng, Andrew"],"emails":["joseph.lee@cs.stanford.edu","zxie@cs.stanford.edu","cindyw@cs.stanford.edu","mdrach@cs.stanford.edu","jurafsky@stanford.edu","ang@cs.stanford.edu"],"author_id":["joseph-lee","ziang-xie","cindy-wang","max-drach","dan-jurafsky","andrew-y-ng"],"abstract":"We introduce a simple method for text style transfer that frames style transfer as denoising: we synthesize a noisy corpus and treat the source style as a noisy version of the target style. To control for aspects such as preserving meaning while modifying style, we propose a reranking approach in the data synthesis phase. We evaluate our method on three novel style transfer tasks: transferring between British and American varieties, text genres (formal vs. casual), and lyrics from different musical genres. By measuring style transfer quality, meaning preservation, and the fluency of generated outputs, we demonstrate that our method is able both to produce high-quality output while maintaining the flexibility to suggest syntactically rich stylistic edits.","pages":"74--81","doi":"10.18653\/v1\/W19-2309","url":"https:\/\/www.aclweb.org\/anthology\/W19-2309","publisher":"Association for Computational Linguistics","address":"Minneapolis, Minnesota","year":"2019","month":"June","booktitle":"Proceedings of the Workshop on Methods for Optimizing and Evaluating Neural Language Generation"},{"id":"W19-2310","title":"Better Automatic Evaluation of Open-Domain Dialogue Systems with Contextualized Embeddings","authors":["Ghazarian, Sarik","Wei, Johnny","Galstyan, Aram","Peng, Nanyun"],"emails":["sarik@isi.edu","jwei@umass.edu","galstyan@isi.edu","npeng@isi.edu"],"author_id":["sarik-ghazarian","johnny-wei","aram-galstyan","nanyun-peng"],"abstract":"Despite advances in open-domain dialogue systems, automatic evaluation of such systems is still a challenging problem. Traditional reference-based metrics such as BLEU are ineffective because there could be many valid responses for a given context that share no common words with reference responses. A recent work proposed Referenced metric and Unreferenced metric Blended Evaluation Routine (RUBER) to combine a learning-based metric, which predicts relatedness between a generated response and a given query, with reference-based metric; it showed high correlation with human judgments. In this paper, we explore using contextualized word embeddings to compute more accurate relatedness scores, thus better evaluation metrics. Experiments show that our evaluation metrics outperform RUBER, which is trained on static embeddings.","pages":"82--89","doi":"10.18653\/v1\/W19-2310","url":"https:\/\/www.aclweb.org\/anthology\/W19-2310","publisher":"Association for Computational Linguistics","address":"Minneapolis, Minnesota","year":"2019","month":"June","booktitle":"Proceedings of the Workshop on Methods for Optimizing and Evaluating Neural Language Generation"},{"id":"W19-2311","title":"Jointly Measuring Diversity and Quality in Text Generation Models","authors":["Alihosseini, Danial","Montahaei, Ehsan","Soleymani Baghshah, Mahdieh"],"emails":["dalihosseini@ce.sharif.edu","ehsan.montahaei@gmail.com","soleymani@sharif.edu"],"author_id":["danial-alihosseini","ehsan-montahaei","mahdieh-soleymani-baghshah"],"abstract":"Text generation is an important Natural Language Processing task with various applications. Although several metrics have already been introduced to evaluate the text generation methods, each of them has its own shortcomings. The most widely used metrics such as BLEU only consider the quality of generated sentences and neglecting their diversity. For example, repeatedly generation of only one high quality sentence would result in a high BLEU score. On the other hand, the more recent metric introduced to evaluate the diversity of generated texts known as Self-BLEU ignores the quality of generated texts. In this paper, we propose metrics to evaluate both the quality and diversity simultaneously by approximating the distance of the learned generative model and the real data distribution. For this purpose, we first introduce a metric that approximates this distance using n-gram based measures. Then, a feature-based measure which is based on a recent highly deep model trained on a large text corpus called BERT is introduced. Finally, for oracle training mode in which the generator\u02bcs density can also be calculated, we propose to use the distance measures between the corresponding explicit distributions. Eventually, the most popular and recent text generation models are evaluated using both the existing and the proposed metrics and the preferences of the proposed metrics are determined.","pages":"90--98","doi":"10.18653\/v1\/W19-2311","url":"https:\/\/www.aclweb.org\/anthology\/W19-2311","publisher":"Association for Computational Linguistics","address":"Minneapolis, Minnesota","year":"2019","month":"June","booktitle":"Proceedings of the Workshop on Methods for Optimizing and Evaluating Neural Language Generation"}]