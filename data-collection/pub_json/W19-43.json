[{"id":"W19-4301","title":"Deep Generalized Canonical Correlation Analysis","authors":["Benton, Adrian","Khayrallah, Huda","Gujral, Biman","Reisinger, Dee Ann","Zhang, Sheng","Arora, Raman"],"emails":["abenton10@bloomberg.net","","","","",""],"author_id":["adrian-benton","huda-khayrallah","biman-gujral","dee-ann-reisinger","sheng-zhang","raman-arora"],"abstract":"We present Deep Generalized Canonical Correlation Analysis (DGCCA) {--} a method for learning nonlinear transformations of arbitrarily many views of data, such that the resulting transformations are maximally informative of each other. While methods for nonlinear two view representation learning (Deep CCA, (Andrew et al., 2013)) and linear many-view representation learning (Generalized CCA (Horst, 1961)) exist, DGCCA combines the flexibility of nonlinear (deep) representation learning with the statistical power of incorporating information from many sources, or views. We present the DGCCA formulation as well as an efficient stochastic optimization algorithm for solving it. We learn and evaluate DGCCA representations for three downstream tasks: phonetic transcription from acoustic {\\&} articulatory measurements, recommending hashtags and recommending friends on a dataset of Twitter users.","pages":"1--6","doi":"10.18653\/v1\/W19-4301","url":"https:\/\/www.aclweb.org\/anthology\/W19-4301","publisher":"Association for Computational Linguistics","address":"Florence, Italy","year":"2019","month":"August","booktitle":"Proceedings of the 4th Workshop on Representation Learning for NLP (RepL4NLP-2019)"},{"id":"W19-4302","title":"To Tune or Not to Tune? Adapting Pretrained Representations to Diverse Tasks","authors":["Peters, Matthew E.","Ruder, Sebastian","Smith, Noah A."],"emails":["matthewp@allenai.org","sebastian@ruder.io","noah@allenai.org"],"author_id":["matthew-e-peters","sebastian-ruder","noah-a-smith"],"abstract":"While most previous work has focused on different pretraining objectives and architectures for transfer learning, we ask how to best adapt the pretrained model to a given target task. We focus on the two most common forms of adaptation, feature extraction (where the pretrained weights are frozen), and directly fine-tuning the pretrained model. Our empirical results across diverse NLP tasks with two state-of-the-art models show that the relative performance of fine-tuning vs. feature extraction depends on the similarity of the pretraining and target tasks. We explore possible explanations for this finding and provide a set of adaptation guidelines for the NLP practitioner.","pages":"7--14","doi":"10.18653\/v1\/W19-4302","url":"https:\/\/www.aclweb.org\/anthology\/W19-4302","publisher":"Association for Computational Linguistics","address":"Florence, Italy","year":"2019","month":"August","booktitle":"Proceedings of the 4th Workshop on Representation Learning for NLP (RepL4NLP-2019)"},{"id":"W19-4303","title":"Generative Adversarial Networks for Text Using Word2vec Intermediaries","authors":["Budhkar, Akshay","Vishnubhotla, Krishnapriya","Hossain, Safwan","Rudzicz, Frank"],"emails":["abudhkar@cs.toronto.edu","vkpriya@cs.toronto.edu","safwan.hossain@mail.utoronto.ca","frank@cs.toronto.edu"],"author_id":["akshay-budhkar","krishnapriya-vishnubhotla","safwan-hossain","frank-rudzicz"],"abstract":"Generative adversarial networks (GANs) have shown considerable success, especially in the realistic generation of images. In this work, we apply similar techniques for the generation of text. We propose a novel approach to handle the discrete nature of text, during training, using word embeddings. Our method is agnostic to vocabulary size and achieves competitive results relative to methods with various discrete gradient estimators.","pages":"15--26","doi":"10.18653\/v1\/W19-4303","url":"https:\/\/www.aclweb.org\/anthology\/W19-4303","publisher":"Association for Computational Linguistics","address":"Florence, Italy","year":"2019","month":"August","booktitle":"Proceedings of the 4th Workshop on Representation Learning for NLP (RepL4NLP-2019)"},{"id":"W19-4304","title":"An Evaluation of Language-Agnostic Inner-Attention-Based Representations in Machine Translation","authors":["Raganato, Alessandro","V{\\'a}zquez, Ra{\\'u}l","Creutz, Mathias","Tiedemann, J{\\\"o}rg"],"emails":["alessandro.raganato@helsinki.fi","ra{\\'u}l.v{\\'a}zquez@helsinki.fi","mathias.creutz@helsinki.fi","j{\\\"o}rg.tiedemann@helsinki.fi"],"author_id":["alessandro-raganato","raul-vazquez","mathias-creutz","jorg-tiedemann"],"abstract":"In this paper, we explore a multilingual translation model with a cross-lingually shared layer that can be used as fixed-size sentence representation in different downstream tasks. We systematically study the impact of the size of the shared layer and the effect of including additional languages in the model. In contrast to related previous work, we demonstrate that the performance in translation does correlate with trainable downstream tasks. In particular, we show that larger intermediate layers not only improve translation quality, especially for long sentences, but also push the accuracy of trainable classification tasks. On the other hand, shorter representations lead to increased compression that is beneficial in non-trainable similarity tasks. We hypothesize that the training procedure on the downstream task enables the model to identify the encoded information that is useful for the specific task whereas non-trainable benchmarks can be confused by other types of information also encoded in the representation of a sentence.","pages":"27--32","doi":"10.18653\/v1\/W19-4304","url":"https:\/\/www.aclweb.org\/anthology\/W19-4304","publisher":"Association for Computational Linguistics","address":"Florence, Italy","year":"2019","month":"August","booktitle":"Proceedings of the 4th Workshop on Representation Learning for NLP (RepL4NLP-2019)"},{"id":"W19-4305","title":"Multilingual {NMT} with a Language-Independent Attention Bridge","authors":["V{\\'a}zquez, Ra{\\'u}l","Raganato, Alessandro","Tiedemann, J{\\\"o}rg","Creutz, Mathias"],"emails":["ra{\\'u}l.v{\\'a}zquez@helsinki.fi","alessandro.raganato@helsinki.fi","j{\\\"o}rg.tiedemann@helsinki.fi","mathias.creutz@helsinki.fi"],"author_id":["raul-vazquez","alessandro-raganato","jorg-tiedemann","mathias-creutz"],"abstract":"In this paper, we propose an architecture for machine translation (MT) capable of obtaining multilingual sentence representations by incorporating an intermediate attention bridge that is shared across all languages. We train the model with language-specific encoders and decoders that are connected through an inner-attention layer on the encoder side. The attention bridge exploits the semantics from each language for translation and develops into a language-agnostic meaning representation that can efficiently be used for transfer learning. We present a new framework for the efficient development of multilingual neural machine translation (NMT) using this model and scheduled training. We have tested the approach in a systematic way with a multi-parallel data set. The model achieves substantial improvements over strong bilingual models and performs well for zero-shot translation, which demonstrates its ability of abstraction and transfer learning.","pages":"33--39","doi":"10.18653\/v1\/W19-4305","url":"https:\/\/www.aclweb.org\/anthology\/W19-4305","publisher":"Association for Computational Linguistics","address":"Florence, Italy","year":"2019","month":"August","booktitle":"Proceedings of the 4th Workshop on Representation Learning for NLP (RepL4NLP-2019)"},{"id":"W19-4306","title":"Efficient Language Modeling with Automatic Relevance Determination in Recurrent Neural Networks","authors":["Kodryan, Maxim","Grachev, Artem","Ignatov, Dmitry","Vetrov, Dmitry"],"emails":["maxkordn54@gmail.com","grachev.art@gmail.com","",""],"author_id":["maxim-kodryan","artem-grachev","dmitry-ignatov","dmitry-vetrov"],"abstract":"Reduction of the number of parameters is one of the most important goals in Deep Learning. In this article we propose an adaptation of Doubly Stochastic Variational Inference for Automatic Relevance Determination (DSVI-ARD) for neural networks compression. We find this method to be especially useful in language modeling tasks, where large number of parameters in the input and output layers is often excessive. We also show that DSVI-ARD can be applied together with encoder-decoder weight tying allowing to achieve even better sparsity and performance. Our experiments demonstrate that more than 90{\\%} of the weights in both encoder and decoder layers can be removed with a minimal quality loss.","pages":"40--48","doi":"10.18653\/v1\/W19-4306","url":"https:\/\/www.aclweb.org\/anthology\/W19-4306","publisher":"Association for Computational Linguistics","address":"Florence, Italy","year":"2019","month":"August","booktitle":"Proceedings of the 4th Workshop on Representation Learning for NLP (RepL4NLP-2019)"},{"id":"W19-4307","title":"{M}o{RT}y: Unsupervised Learning of Task-specialized Word Embeddings by Autoencoding","authors":["Rethmeier, Nils","Plank, Barbara"],"emails":["nils.rethmeier@dfki.de","bplank@itu.dk"],"author_id":["nils-rethmeier","barbara-plank"],"abstract":"Word embeddings have undoubtedly revolutionized NLP. However, pretrained embeddings do not always work for a specific task (or set of tasks), particularly in limited resource setups. We introduce a simple yet effective, self-supervised post-processing method that constructs task-specialized word representations by picking from a menu of reconstructing transformations to yield improved end-task performance (MORTY). The method is complementary to recent state-of-the-art approaches to inductive transfer via fine-tuning, and forgoes costly model architectures and annotation. We evaluate MORTY on a broad range of setups, including different word embedding methods, corpus sizes and end-task semantics. Finally, we provide a surprisingly simple recipe to obtain specialized embeddings that better fit end-tasks.","pages":"49--54","doi":"10.18653\/v1\/W19-4307","url":"https:\/\/www.aclweb.org\/anthology\/W19-4307","publisher":"Association for Computational Linguistics","address":"Florence, Italy","year":"2019","month":"August","booktitle":"Proceedings of the 4th Workshop on Representation Learning for NLP (RepL4NLP-2019)"},{"id":"W19-4308","title":"Pitfalls in the Evaluation of Sentence Embeddings","authors":["Eger, Steffen","R{\\\"u}ckl{\\'e}, Andreas","Gurevych, Iryna"],"emails":["","",""],"author_id":["steffen-eger","andreas-ruckle","iryna-gurevych"],"abstract":"Deep learning models continuously break new records across different NLP tasks. At the same time, their success exposes weaknesses of model evaluation. Here, we compile several key pitfalls of evaluation of sentence embeddings, a currently very popular NLP paradigm. These pitfalls include the comparison of embeddings of different sizes, normalization of embeddings, and the low (and diverging) correlations between transfer and probing tasks. Our motivation is to challenge the current evaluation of sentence embeddings and to provide an easy-to-access reference for future research. Based on our insights, we also recommend better practices for better future evaluations of sentence embeddings.","pages":"55--60","doi":"10.18653\/v1\/W19-4308","url":"https:\/\/www.aclweb.org\/anthology\/W19-4308","publisher":"Association for Computational Linguistics","address":"Florence, Italy","year":"2019","month":"August","booktitle":"Proceedings of the 4th Workshop on Representation Learning for NLP (RepL4NLP-2019)"},{"id":"W19-4309","title":"Learning Bilingual Sentence Embeddings via Autoencoding and Computing Similarities with a Multilayer Perceptron","authors":["Kim, Yunsu","Rosendahl, Hendrik","Rossenbach, Nick","Rosendahl, Jan","Khadivi, Shahram","Ney, Hermann"],"emails":["","hrosendahl@ebay.com","surname@cs.rwth-aachen.de","","skhadivi@ebay.com",""],"author_id":["yunsu-kim","hendrik-rosendahl","nick-rossenbach","jan-rosendahl","shahram-khadivi","hermann-ney"],"abstract":"We propose a novel model architecture and training algorithm to learn bilingual sentence embeddings from a combination of parallel and monolingual data. Our method connects autoencoding and neural machine translation to force the source and target sentence embeddings to share the same space without the help of a pivot language or an additional transformation. We train a multilayer perceptron on top of the sentence embeddings to extract good bilingual sentence pairs from nonparallel or noisy parallel data. Our approach shows promising performance on sentence alignment recovery and the WMT 2018 parallel corpus filtering tasks with only a single model.","pages":"61--71","doi":"10.18653\/v1\/W19-4309","url":"https:\/\/www.aclweb.org\/anthology\/W19-4309","publisher":"Association for Computational Linguistics","address":"Florence, Italy","year":"2019","month":"August","booktitle":"Proceedings of the 4th Workshop on Representation Learning for NLP (RepL4NLP-2019)"},{"id":"W19-4310","title":"Specializing Distributional Vectors of All Words for Lexical Entailment","authors":["Kamath, Aishwarya","Pfeiffer, Jonas","Ponti, Edoardo Maria","Glava{\\v{s}}, Goran","Vuli{\\'c}, Ivan"],"emails":["1aishwarya.kamath@oracle.com","2pfeiffer@ukp.informatik.tu-darmstadt.de","ep490@cam.ac.uk","4goran@informatik.uni-mannheim.de","iv250@cam.ac.uk"],"author_id":["aishwarya-kamath","jonas-pfeiffer","edoardo-maria-ponti","goran-glavas","ivan-vulic"],"abstract":"Semantic specialization methods fine-tune distributional word vectors using lexical knowledge from external resources (e.g. WordNet) to accentuate a particular relation between words. However, such post-processing methods suffer from limited coverage as they affect only vectors of words seen in the external resources. We present the first post-processing method that specializes vectors of all vocabulary words {--} including those unseen in the resources {--} for the asymmetric relation of lexical entailment (LE) (i.e., hyponymy-hypernymy relation). Leveraging a partially LE-specialized distributional space, our POSTLE (i.e., post-specialization for LE) model learns an explicit global specialization function, allowing for specialization of vectors of unseen words, as well as word vectors from other languages via cross-lingual transfer. We capture the function as a deep feed-forward neural network: its objective re-scales vector norms to reflect the concept hierarchy while simultaneously attracting hyponymy-hypernymy pairs to better reflect semantic similarity. An extended model variant augments the basic architecture with an adversarial discriminator. We demonstrate the usefulness and versatility of POSTLE models with different input distributional spaces in different scenarios (monolingual LE and zero-shot cross-lingual LE transfer) and tasks (binary and graded LE). We report consistent gains over state-of-the-art LE-specialization methods, and successfully LE-specialize word vectors for languages without any external lexical knowledge.","pages":"72--83","doi":"10.18653\/v1\/W19-4310","url":"https:\/\/www.aclweb.org\/anthology\/W19-4310","publisher":"Association for Computational Linguistics","address":"Florence, Italy","year":"2019","month":"August","booktitle":"Proceedings of the 4th Workshop on Representation Learning for NLP (RepL4NLP-2019)"},{"id":"W19-4311","title":"Composing Noun Phrase Vector Representations","authors":["Kalouli, Aikaterini-Lida","dePaiva, Valeria","Crouch, Richard"],"emails":["aikaterini-lida.kalouli@uni-konstanz.de","valeria.depaiva@gmail.com","dick.crouch@gmail.com"],"author_id":["aikaterini-lida-kalouli","valeria-depaiva","richard-crouch"],"abstract":"Vector representations of words have seen an increasing success over the past years in a variety of NLP tasks. While there seems to be a consensus about the usefulness of word embeddings and how to learn them, it is still unclear which representations can capture the meaning of phrases or even whole sentences. Recent work has shown that simple operations outperform more complex deep architectures. In this work, we propose two novel constraints for computing noun phrase vector representations. First, we propose that the semantic and not the syntactic contribution of each component of a noun phrase should be considered, so that the resulting composed vectors express more of the phrase meaning. Second, the composition process of the two phrase vectors should apply suitable dimensions{'} selection in a way that specific semantic features captured by the phrase{'}s meaning become more salient. Our proposed methods are compared to 11 other approaches, including popular baselines and a neural net architecture, and are evaluated across 6 tasks and 2 datasets. Our results show that these constraints lead to more expressive phrase representations and can be applied to other state-of-the-art methods to improve their performance.","pages":"84--95","doi":"10.18653\/v1\/W19-4311","url":"https:\/\/www.aclweb.org\/anthology\/W19-4311","publisher":"Association for Computational Linguistics","address":"Florence, Italy","year":"2019","month":"August","booktitle":"Proceedings of the 4th Workshop on Representation Learning for NLP (RepL4NLP-2019)"},{"id":"W19-4312","title":"Towards Robust Named Entity Recognition for Historic {G}erman","authors":["Schweter, Stefan","Baiter, Johannes"],"emails":["stefan.schweter@bsb-muenchen.de","johannes.baiter@bsb-muenchen.de"],"author_id":["stefan-schweter","johannes-baiter"],"abstract":"In this paper we study the influence of using language model pre-training for named entity recognition for Historic German. We achieve new state-of-the-art results using carefully chosen training data for language models. For a low-resource domain like named entity recognition for Historic German, language model pre-training can be a strong competitor to CRF-only methods. We show that language model pre-training can be more effective than using transfer-learning with labeled datasets. Furthermore, we introduce a new language model pre-training objective, synthetic masked language model pre-training (SMLM), that allows a transfer from one domain (contemporary texts) to another domain (historical texts) by using only the same (character) vocabulary. Results show that using SMLM can achieve comparable results for Historic named entity recognition, even when they are only trained on contemporary texts. Our pre-trained character-based language models improve upon classical CRF-based methods and previous work on Bi-LSTMs by boosting F1 score performance by up to 6{\\%}.","pages":"96--103","doi":"10.18653\/v1\/W19-4312","url":"https:\/\/www.aclweb.org\/anthology\/W19-4312","publisher":"Association for Computational Linguistics","address":"Florence, Italy","year":"2019","month":"August","booktitle":"Proceedings of the 4th Workshop on Representation Learning for NLP (RepL4NLP-2019)"},{"id":"W19-4313","title":"On Evaluating Embedding Models for Knowledge Base Completion","authors":["Wang, Yanjie","Ruffinelli, Daniel","Gemulla, Rainer","Broscheit, Samuel","Meilicke, Christian"],"emails":["ywang@uni-mannheim.de","daniel@informatik.uni-mannheim.de","rgemulla@uni-mannheim.de","broscheit@informatik.uni-mannheim.de","christian@informatik.uni-mannheim.de"],"author_id":["yanjie-wang","daniel-ruffinelli","rainer-gemulla","samuel-broscheit","christian-meilicke"],"abstract":"Knowledge graph embedding models have recently received significant attention in the literature. These models learn latent semantic representations for the entities and relations in a given knowledge base; the representations can be used to infer missing knowledge. In this paper, we study the question of how well recent embedding models perform for the task of knowledge base completion, i.e., the task of inferring new facts from an incomplete knowledge base. We argue that the entity ranking protocol, which is currently used to evaluate knowledge graph embedding models, is not suitable to answer this question since only a subset of the model predictions are evaluated. We propose an alternative entity-pair ranking protocol that considers all model predictions as a whole and is thus more suitable to the task. We conducted an experimental study on standard datasets and found that the performance of popular embeddings models was unsatisfactory under the new protocol, even on datasets that are generally considered to be too easy. Moreover, we found that a simple rule-based model often provided superior performance. Our findings suggest that there is a need for more research into embedding models as well as their training strategies for the task of knowledge base completion.","pages":"104--112","doi":"10.18653\/v1\/W19-4313","url":"https:\/\/www.aclweb.org\/anthology\/W19-4313","publisher":"Association for Computational Linguistics","address":"Florence, Italy","year":"2019","month":"August","booktitle":"Proceedings of the 4th Workshop on Representation Learning for NLP (RepL4NLP-2019)"},{"id":"W19-4314","title":"Constructive Type-Logical Supertagging With Self-Attention Networks","authors":["Kogkalidis, Konstantinos","Moortgat, Michael","Deoskar, Tejaswini"],"emails":["k.kogkalidis@uu.nl","m.j.moortgat@uu.nl","t.deoskar@uu.nl"],"author_id":["konstantinos-kogkalidis","michael-moortgat","tejaswini-deoskar"],"abstract":"We propose a novel application of self-attention networks towards grammar induction. We present an attention-based supertagger for a refined type-logical grammar, trained on constructing types inductively. In addition to achieving a high overall type accuracy, our model is able to learn the syntax of the grammar{'}s type system along with its denotational semantics. This lifts the closed world assumption commonly made by lexicalized grammar supertaggers, greatly enhancing its generalization potential. This is evidenced both by its adequate accuracy over sparse word types and its ability to correctly construct complex types never seen during training, which, to the best of our knowledge, was as of yet unaccomplished.","pages":"113--123","doi":"10.18653\/v1\/W19-4314","url":"https:\/\/www.aclweb.org\/anthology\/W19-4314","publisher":"Association for Computational Linguistics","address":"Florence, Italy","year":"2019","month":"August","booktitle":"Proceedings of the 4th Workshop on Representation Learning for NLP (RepL4NLP-2019)"},{"id":"W19-4315","title":"Auto-Encoding Variational Neural Machine Translation","authors":["Eikema, Bryan","Aziz, Wilker"],"emails":["b.eikema@uva.nl","w.aziz@uva.nl"],"author_id":["bryan-eikema","wilker-aziz"],"abstract":"We present a deep generative model of bilingual sentence pairs for machine translation. The model generates source and target sentences jointly from a shared latent representation and is parameterised by neural networks. We perform efficient training using amortised variational inference and reparameterised gradients. Additionally, we discuss the statistical implications of joint modelling and propose an efficient approximation to maximum a posteriori decoding for fast test-time predictions. We demonstrate the effectiveness of our model in three machine translation scenarios: in-domain training, mixed-domain training, and learning from a mix of gold-standard and synthetic data. Our experiments show consistently that our joint formulation outperforms conditional modelling (i.e. standard neural machine translation) in all such scenarios.","pages":"124--141","doi":"10.18653\/v1\/W19-4315","url":"https:\/\/www.aclweb.org\/anthology\/W19-4315","publisher":"Association for Computational Linguistics","address":"Florence, Italy","year":"2019","month":"August","booktitle":"Proceedings of the 4th Workshop on Representation Learning for NLP (RepL4NLP-2019)"},{"id":"W19-4316","title":"Learning Bilingual Word Embeddings Using Lexical Definitions","authors":["Shi, Weijia","Chen, Muhao","Tian, Yingtao","Chang, Kai-Wei"],"emails":["swj0419@cs.ucla.edu","muhaochen@cs.ucla.edu","yittian@cs.stonybrook.edu","kw2c@cs.ucla.edu"],"author_id":["weijia-shi","muhao-chen","yingtao-tian","kai-wei-chang"],"abstract":"Bilingual word embeddings, which represent lexicons of different languages in a shared embedding space, are essential for supporting semantic and knowledge transfers in a variety of cross-lingual NLP tasks. Existing approaches to training bilingual word embeddings require either large collections of pre-defined seed lexicons that are expensive to obtain, or parallel sentences that comprise coarse and noisy alignment. In contrast, we propose BiLex that leverages publicly available lexical definitions for bilingual word embedding learning. Without the need of predefined seed lexicons, BiLex comprises a novel word pairing strategy to automatically identify and propagate the precise fine-grain word alignment from lexical definitions. We evaluate BiLex in word-level and sentence-level translation tasks, which seek to find the cross-lingual counterparts of words and sentences respectively. BiLex significantly outperforms previous embedding methods on both tasks.","pages":"142--147","doi":"10.18653\/v1\/W19-4316","url":"https:\/\/www.aclweb.org\/anthology\/W19-4316","publisher":"Association for Computational Linguistics","address":"Florence, Italy","year":"2019","month":"August","booktitle":"Proceedings of the 4th Workshop on Representation Learning for NLP (RepL4NLP-2019)"},{"id":"W19-4317","title":"An Empirical Study on Pre-trained Embeddings and Language Models for Bot Detection","authors":["Garcia-Silva, Andres","Berrio, Cristian","G{\\'o}mez-P{\\'e}rez, Jos{\\'e} Manuel"],"emails":["agarcia@expertsystem.com","cberrio@expertsystem.com","jmgomez@expertsystem.com"],"author_id":["andres-garcia-silva","cristian-berrio","jose-manuel-gomez-perez"],"abstract":"Fine-tuning pre-trained language models has significantly advanced the state of art in a wide range of NLP downstream tasks. Usually, such language models are learned from large and well-formed text corpora from e.g. encyclopedic resources, books or news. However, a significant amount of the text to be analyzed nowadays is Web data, often from social media. In this paper we consider the research question: How do standard pre-trained language models generalize and capture the peculiarities of rather short, informal and frequently automatically generated text found in social media? To answer this question, we focus on bot detection in Twitter as our evaluation task and test the performance of fine-tuning approaches based on language models against popular neural architectures such as LSTM and CNN combined with pre-trained and contextualized embeddings. Our results also show strong performance variations among the different language model approaches, which suggest further research.","pages":"148--155","doi":"10.18653\/v1\/W19-4317","url":"https:\/\/www.aclweb.org\/anthology\/W19-4317","publisher":"Association for Computational Linguistics","address":"Florence, Italy","year":"2019","month":"August","booktitle":"Proceedings of the 4th Workshop on Representation Learning for NLP (RepL4NLP-2019)"},{"id":"W19-4318","title":"Probing Multilingual Sentence Representations With X-Probe","authors":["Ravishankar, Vinit","{\\O}vrelid, Lilja","Velldal, Erik"],"emails":["vinitr@ifi.uio.no","liljao@ifi.uio.no","erikve@ifi.uio.no"],"author_id":["vinit-ravishankar","lilja-ovrelid","erik-velldal"],"abstract":"This paper extends the task of probing sentence representations for linguistic insight in a multilingual domain. In doing so, we make two contributions: first, we provide datasets for multilingual probing, derived from Wikipedia, in five languages, viz. English, French, German, Spanish and Russian. Second, we evaluate six sentence encoders for each language, each trained by mapping sentence representations to English sentence representations, using sentences in a parallel corpus. We discover that cross-lingually mapped representations are often better at retaining certain linguistic information than representations derived from English encoders trained on natural language inference (NLI) as a downstream task.","pages":"156--168","doi":"10.18653\/v1\/W19-4318","url":"https:\/\/www.aclweb.org\/anthology\/W19-4318","publisher":"Association for Computational Linguistics","address":"Florence, Italy","year":"2019","month":"August","booktitle":"Proceedings of the 4th Workshop on Representation Learning for NLP (RepL4NLP-2019)"},{"id":"W19-4319","title":"Fine-Grained Entity Typing in Hyperbolic Space","authors":["L{\\'o}pez, Federico","Heinzerling, Benjamin","Strube, Michael"],"emails":["federico.l{\\'o}pez@h-its.org","benjamin.heinzerling@h-its.org","michael.strube@h-its.org"],"author_id":["federico-lopez","benjamin-heinzerling","michael-strube"],"abstract":"How can we represent hierarchical information present in large type inventories for entity typing? We study the suitability of hyperbolic embeddings to capture hierarchical relations between mentions in context and their target types in a shared vector space. We evaluate on two datasets and propose two different techniques to extract hierarchical information from the type inventory: from an expert-generated ontology and by automatically mining the dataset. The hyperbolic model shows improvements in some but not all cases over its Euclidean counterpart. Our analysis suggests that the adequacy of this geometry depends on the granularity of the type inventory and the representation of its distribution.","pages":"169--180","doi":"10.18653\/v1\/W19-4319","url":"https:\/\/www.aclweb.org\/anthology\/W19-4319","publisher":"Association for Computational Linguistics","address":"Florence, Italy","year":"2019","month":"August","booktitle":"Proceedings of the 4th Workshop on Representation Learning for NLP (RepL4NLP-2019)"},{"id":"W19-4320","title":"Learning Multilingual Meta-Embeddings for Code-Switching Named Entity Recognition","authors":["Winata, Genta Indra","Lin, Zhaojiang","Fung, Pascale"],"emails":["giwinata@connect.ust.hk","zlinao@connect.ust.hk","pascale@ece.ust.hk"],"author_id":["genta-indra-winata","zhaojiang-lin","pascale-fung"],"abstract":"In this paper, we propose Multilingual Meta-Embeddings (MME), an effective method to learn multilingual representations by leveraging monolingual pre-trained embeddings. MME learns to utilize information from these embeddings via a self-attention mechanism without explicit language identification. We evaluate the proposed embedding method on the code-switching English-Spanish Named Entity Recognition dataset in a multilingual and cross-lingual setting. The experimental results show that our proposed method achieves state-of-the-art performance on the multilingual setting, and it has the ability to generalize to an unseen language task.","pages":"181--186","doi":"10.18653\/v1\/W19-4320","url":"https:\/\/www.aclweb.org\/anthology\/W19-4320","publisher":"Association for Computational Linguistics","address":"Florence, Italy","year":"2019","month":"August","booktitle":"Proceedings of the 4th Workshop on Representation Learning for NLP (RepL4NLP-2019)"},{"id":"W19-4321","title":"Investigating Sub-Word Embedding Strategies for the Morphologically Rich and Free Phrase-Order {H}ungarian","authors":["D{\\\"o}br{\\\"o}ssy, B{\\'a}lint","Makrai, M{\\'a}rton","Tarj{\\'a}n, Bal{\\'a}zs","Szasz{\\'a}k, Gy{\\\"o}rgy"],"emails":["balint.dobrossy@gmail.com","2makrai.marton@nytud.mta.hu","tarjanb@tmit.bme.hu","szaszak@tmit.bme.hu"],"author_id":["balint-dobrossy","marton-makrai","balazs-tarjan","gyorgy-szaszak1"],"abstract":"For morphologically rich languages, word embeddings provide less consistent semantic representations due to higher variance in word forms. Moreover, these languages often allow for less constrained word order, which further increases variance. For the highly agglutinative Hungarian, semantic accuracy of word embeddings measured on word analogy tasks drops by 50-75{\\%} compared to English. We observed that embeddings learn morphosyntax quite well instead. Therefore, we explore and evaluate several sub-word unit based embedding strategies {--} character n-grams, lemmatization provided by an NLP-pipeline, and segments obtained in unsupervised learning (morfessor) {--} to boost semantic consistency in Hungarian word vectors. The effect of changing embedding dimension and context window size have also been considered. Morphological analysis based lemmatization was found to be the best strategy to improve embeddings{'} semantic accuracy, whereas adding character n-grams was found consistently counterproductive in this regard.","pages":"187--193","doi":"10.18653\/v1\/W19-4321","url":"https:\/\/www.aclweb.org\/anthology\/W19-4321","publisher":"Association for Computational Linguistics","address":"Florence, Italy","year":"2019","month":"August","booktitle":"Proceedings of the 4th Workshop on Representation Learning for NLP (RepL4NLP-2019)"},{"id":"W19-4322","title":"A Self-Training Approach for Short Text Clustering","authors":["Hadifar, Amir","Sterckx, Lucas","Demeester, Thomas","Develder, Chris"],"emails":["amir.hadifar@ugent.be","lucas.sterckx@ugent.be","thomas.demeester@ugent.be","chris.develder@ugent.be"],"author_id":["amir-hadifar","lucas-sterckx","thomas-demeester","chris-develder"],"abstract":"Short text clustering is a challenging problem when adopting traditional bag-of-words or TF-IDF representations, since these lead to sparse vector representations of the short texts. Low-dimensional continuous representations or embeddings can counter that sparseness problem: their high representational power is exploited in deep clustering algorithms. While deep clustering has been studied extensively in computer vision, relatively little work has focused on NLP. The method we propose, learns discriminative features from both an autoencoder and a sentence embedding, then uses assignments from a clustering algorithm as supervision to update weights of the encoder network. Experiments on three short text datasets empirically validate the effectiveness of our method.","pages":"194--199","doi":"10.18653\/v1\/W19-4322","url":"https:\/\/www.aclweb.org\/anthology\/W19-4322","publisher":"Association for Computational Linguistics","address":"Florence, Italy","year":"2019","month":"August","booktitle":"Proceedings of the 4th Workshop on Representation Learning for NLP (RepL4NLP-2019)"},{"id":"W19-4323","title":"Improving Word Embeddings Using Kernel {PCA}","authors":["Gupta, Vishwani","Giesselbach, Sven","R{\\\"u}ping, Stefan","Bauckhage, Christian"],"emails":["vishwani.gupta@iais.fraunhofer.de","sven.giesselbach@iais.fraunhofer.de","stefan.r{\\\"u}ping@iais.fraunhofer.de","christian.bauckhage@iais.fraunhofer.de"],"author_id":["vishwani-gupta","sven-giesselbach","stefan-ruping","christian-bauckhage"],"abstract":"Word-based embedding approaches such as Word2Vec capture the meaning of words and relations between them, particularly well when trained with large text collections; however, they fail to do so with small datasets. Extensions such as fastText reduce the amount of data needed slightly, however, the joint task of learning meaningful morphology, syntactic and semantic representations still requires a lot of data. In this paper, we introduce a new approach to warm-start embedding models with morphological information, in order to reduce training time and enhance their performance. We use word embeddings generated using both word2vec and fastText models and enrich them with morphological information of words, derived from kernel principal component analysis (KPCA) of word similarity matrices. This can be seen as explicitly feeding the network morphological similarities and letting it learn semantic and syntactic similarities. Evaluating our models on word similarity and analogy tasks in English and German, we find that they not only achieve higher accuracies than the original skip-gram and fastText models but also require significantly less training data and time. Another benefit of our approach is that it is capable of generating a high-quality representation of infrequent words as, for example, found in very recent news articles with rapidly changing vocabularies. Lastly, we evaluate the different models on a downstream sentence classification task in which a CNN model is initialized with our embeddings and find promising results.","pages":"200--208","doi":"10.18653\/v1\/W19-4323","url":"https:\/\/www.aclweb.org\/anthology\/W19-4323","publisher":"Association for Computational Linguistics","address":"Florence, Italy","year":"2019","month":"August","booktitle":"Proceedings of the 4th Workshop on Representation Learning for NLP (RepL4NLP-2019)"},{"id":"W19-4324","title":"Assessing Incrementality in Sequence-to-Sequence Models","authors":["Ulmer, Dennis","Hupkes, Dieuwke","Bruni, Elia"],"emails":["dennis.ulmer@gmx.de","d.hupkes@uva.nl","elia.bruni@gmail.com"],"author_id":["dennis-ulmer","dieuwke-hupkes","elia-bruni"],"abstract":"Since their inception, encoder-decoder models have successfully been applied to a wide array of problems in computational linguistics. The most recent successes are predominantly due to the use of different variations of attention mechanisms, but their cognitive plausibility is questionable. In particular, because past representations can be revisited at any point in time, attention-centric methods seem to lack an incentive to build up incrementally more informative representations of incoming sentences. This way of processing stands in stark contrast with the way in which humans are believed to process language: continuously and rapidly integrating new information as it is encountered. In this work, we propose three novel metrics to assess the behavior of RNNs with and without an attention mechanism and identify key differences in the way the different model types process sentences.","pages":"209--217","doi":"10.18653\/v1\/W19-4324","url":"https:\/\/www.aclweb.org\/anthology\/W19-4324","publisher":"Association for Computational Linguistics","address":"Florence, Italy","year":"2019","month":"August","booktitle":"Proceedings of the 4th Workshop on Representation Learning for NLP (RepL4NLP-2019)"},{"id":"W19-4325","title":"On Committee Representations of Adversarial Learning Models for Question-Answer Ranking","authors":["Gupta, Sparsh","Carvalho, Vitor"],"emails":["spg005@ucsd.edu","carvalho@intuit.com"],"author_id":["sparsh-gupta","vitor-carvalho"],"abstract":"Adversarial training is a process in Machine Learning that explicitly trains models on adversarial inputs (inputs designed to deceive or trick the learning process) in order to make it more robust or accurate. In this paper we investigate how representing adversarial training models as committees can be used to effectively improve the performance of Question-Answer (QA) Ranking. We start by empirically probing the effects of adversarial training over multiple QA ranking algorithms, including the state-of-the-art Multihop Attention Network model. We evaluate these algorithms on several benchmark datasets and observe that, while adversarial training is beneficial to most baseline algorithms, there are cases where it may lead to overfitting and performance degradation. We investigate the causes of such degradation, and then propose a new representation procedure for this adversarial learning problem, based on committee learning, that not only is capable of consistently improving all baseline algorithms, but also outperforms the previous state-of-the-art algorithm by as much as 6{\\%} in NDCG.","pages":"218--223","doi":"10.18653\/v1\/W19-4325","url":"https:\/\/www.aclweb.org\/anthology\/W19-4325","publisher":"Association for Computational Linguistics","address":"Florence, Italy","year":"2019","month":"August","booktitle":"Proceedings of the 4th Workshop on Representation Learning for NLP (RepL4NLP-2019)"},{"id":"W19-4326","title":"Meta-Learning Improves Lifelong Relation Extraction","authors":["Obamuyide, Abiola","Vlachos, Andreas"],"emails":["avobamuyide1@sheffield.ac.uk","andreas.vlachos@cst.cam.ac.uk"],"author_id":["abiola-obamuyide","andreas-vlachos"],"abstract":"Most existing relation extraction models assume a fixed set of relations and are unable to adapt to exploit newly available supervision data to extract new relations. In order to alleviate such problems, there is the need to develop approaches that make relation extraction models capable of continuous adaptation and learning. We investigate and present results for such an approach, based on a combination of ideas from lifelong learning and optimization-based meta-learning. We evaluate the proposed approach on two recent lifelong relation extraction benchmarks, and demonstrate that it markedly outperforms current state-of-the-art approaches.","pages":"224--229","doi":"10.18653\/v1\/W19-4326","url":"https:\/\/www.aclweb.org\/anthology\/W19-4326","publisher":"Association for Computational Linguistics","address":"Florence, Italy","year":"2019","month":"August","booktitle":"Proceedings of the 4th Workshop on Representation Learning for NLP (RepL4NLP-2019)"},{"id":"W19-4327","title":"Best Practices for Learning Domain-Specific Cross-Lingual Embeddings","authors":["Shakurova, Lena","Nyari, Beata","Li, Chao","Rotaru, Mihai"],"emails":["shakurova@textkernel.nl","nyari@textkernel.nl","chaoli@textkernel.nl","rotaru@textkernel.nl"],"author_id":["lena-shakurova","beata-nyari","chao-li","mihai-rotaru"],"abstract":"Cross-lingual embeddings aim to represent words in multiple languages in a shared vector space by capturing semantic similarities across languages. They are a crucial component for scaling tasks to multiple languages by transferring knowledge from languages with rich resources to low-resource languages. A common approach to learning cross-lingual embeddings is to train monolingual embeddings separately for each language and learn a linear projection from the monolingual spaces into a shared space, where the mapping relies on a small seed dictionary. While there are high-quality generic seed dictionaries and pre-trained cross-lingual embeddings available for many language pairs, there is little research on how they perform on specialised tasks. In this paper, we investigate the best practices for constructing the seed dictionary for a specific domain. We evaluate the embeddings on the sequence labelling task of Curriculum Vitae parsing and show that the size of a bilingual dictionary, the frequency of the dictionary words in the domain corpora and the source of data (task-specific vs generic) influence performance. We also show that the less training data is available in the low-resource language, the more the construction of the bilingual dictionary matters, and demonstrate that some of the choices are crucial in the zero-shot transfer learning case.","pages":"230--234","doi":"10.18653\/v1\/W19-4327","url":"https:\/\/www.aclweb.org\/anthology\/W19-4327","publisher":"Association for Computational Linguistics","address":"Florence, Italy","year":"2019","month":"August","booktitle":"Proceedings of the 4th Workshop on Representation Learning for NLP (RepL4NLP-2019)"},{"id":"W19-4328","title":"Effective Dimensionality Reduction for Word Embeddings","authors":["Raunak, Vikas","Gupta, Vivek","Metze, Florian"],"emails":["vraunak@cs.cmu.edu","vgupta@cs.utah.edu","fmetze@cs.cmu.edu"],"author_id":["vikas-raunak","vivek-gupta","florian-metze"],"abstract":"Pre-trained word embeddings are used in several downstream applications as well as for constructing representations for sentences, paragraphs and documents. Recently, there has been an emphasis on improving the pretrained word vectors through post-processing algorithms. One improvement area is reducing the dimensionality of word embeddings. Reducing the size of word embeddings can improve their utility in memory constrained devices, benefiting several real world applications. In this work, we present a novel technique that efficiently combines PCA based dimensionality reduction with a recently proposed post-processing algorithm (Mu and Viswanath, 2018), to construct effective word embeddings of lower dimensions. Empirical evaluations on several benchmarks show that our algorithm efficiently reduces the embedding size while achieving similar or (more often) better performance than original embeddings. We have released the source code along with this paper.","pages":"235--243","doi":"10.18653\/v1\/W19-4328","url":"https:\/\/www.aclweb.org\/anthology\/W19-4328","publisher":"Association for Computational Linguistics","address":"Florence, Italy","year":"2019","month":"August","booktitle":"Proceedings of the 4th Workshop on Representation Learning for NLP (RepL4NLP-2019)"},{"id":"W19-4329","title":"Learning Word Embeddings without Context Vectors","authors":["Zobnin, Alexey","Elistratova, Evgenia"],"emails":["azobnin@hse.ru","evg3307@yandex.ru"],"author_id":["alexey-zobnin","evgenia-elistratova"],"abstract":"Most word embedding algorithms such as word2vec or fastText construct two sort of vectors: for words and for contexts. Naive use of vectors of only one sort leads to poor results. We suggest using indefinite inner product in skip-gram negative sampling algorithm. This allows us to use only one sort of vectors without loss of quality. Our {``}context-free{''} cf algorithm performs on par with SGNS on word similarity datasets","pages":"244--249","doi":"10.18653\/v1\/W19-4329","url":"https:\/\/www.aclweb.org\/anthology\/W19-4329","publisher":"Association for Computational Linguistics","address":"Florence, Italy","year":"2019","month":"August","booktitle":"Proceedings of the 4th Workshop on Representation Learning for NLP (RepL4NLP-2019)"},{"id":"W19-4330","title":"Learning Cross-Lingual Sentence Representations via a Multi-task Dual-Encoder Model","authors":["Chidambaram, Muthu","Yang, Yinfei","Cer, Daniel","Yuan, Steve","Sung, Yunhsuan","Strope, Brian","Kurzweil, Ray"],"emails":["mutty@google.com","yinfeiy@google.com","cer@google.com","","","",""],"author_id":["muthu-chidambaram","yinfei-yang","daniel-cer","steve-yuan","yunhsuan-sung","brian-strope","ray-kurzweil"],"abstract":"The scarcity of labeled training data across many languages is a significant roadblock for multilingual neural language processing. We approach the lack of in-language training data using sentence embeddings that map text written in different languages, but with similar meanings, to nearby embedding space representations. The representations are produced using a dual-encoder based model trained to maximize the representational similarity between sentence pairs drawn from parallel data. The representations are enhanced using multitask training and unsupervised monolingual corpora. The effectiveness of our multilingual sentence embeddings are assessed on a comprehensive collection of monolingual, cross-lingual, and zero-shot\/few-shot learning tasks.","pages":"250--259","doi":"10.18653\/v1\/W19-4330","url":"https:\/\/www.aclweb.org\/anthology\/W19-4330","publisher":"Association for Computational Linguistics","address":"Florence, Italy","year":"2019","month":"August","booktitle":"Proceedings of the 4th Workshop on Representation Learning for NLP (RepL4NLP-2019)"},{"id":"W19-4331","title":"Modality-based Factorization for Multimodal Fusion","authors":["Barezi, Elham J.","Fung, Pascale"],"emails":["ejs@cse.ust.hk","pascale@ust.hk"],"author_id":["elham-j-barezi","pascale-fung"],"abstract":"We propose a novel method, Modality-based Redundancy Reduction Fusion (MRRF), for understanding and modulating the relative contribution of each modality in multimodal inference tasks. This is achieved by obtaining an $(M+1)$-way tensor to consider the high-order relationships between $M$ modalities and the output layer of a neural network model. Applying a modality-based tensor factorization method, which adopts different factors for different modalities, results in removing information present in a modality that can be compensated by other modalities, with respect to model outputs. This helps to understand the relative utility of information in each modality. In addition it leads to a less complicated model with less parameters and therefore could be applied as a regularizer avoiding overfitting. We have applied this method to three different multimodal datasets in sentiment analysis, personality trait recognition, and emotion recognition. We are able to recognize relationships and relative importance of different modalities in these tasks and achieves a 1{\\%} to 4{\\%} improvement on several evaluation measures compared to the state-of-the-art for all three tasks.","pages":"260--269","doi":"10.18653\/v1\/W19-4331","url":"https:\/\/www.aclweb.org\/anthology\/W19-4331","publisher":"Association for Computational Linguistics","address":"Florence, Italy","year":"2019","month":"August","booktitle":"Proceedings of the 4th Workshop on Representation Learning for NLP (RepL4NLP-2019)"},{"id":"W19-4332","title":"Leveraging Pre-Trained Embeddings for Welsh Taggers","authors":["Ezeani, Ignatius","Piao, Scott","Neale, Steven","Rayson, Paul","Knight, Dawn"],"emails":["i.ezeani@lancaster.ac.uk","s.piao@lancaster.ac.uk","2@cardiff.ac.uk","p.rayson@lancaster.ac.uk","knightd5@cardiff.ac.uk"],"author_id":["ignatius-ezeani","scott-s-l-piao","steven-neale","paul-rayson","dawn-knight"],"abstract":"While the application of word embedding models to downstream Natural Language Processing (NLP) tasks has been shown to be successful, the benefits for low-resource languages is somewhat limited due to lack of adequate data for training the models. However, NLP research efforts for low-resource languages have focused on constantly seeking ways to harness pre-trained models to improve the performance of NLP systems built to process these languages without the need to re-invent the wheel. One such language is Welsh and therefore, in this paper, we present the results of our experiments on learning a simple multi-task neural network model for part-of-speech and semantic tagging for Welsh using a pre-trained embedding model from FastText. Our model{'}s performance was compared with those of the existing rule-based stand-alone taggers for part-of-speech and semantic taggers. Despite its simplicity and capacity to perform both tasks simultaneously, our tagger compared very well with the existing taggers.","pages":"270--280","doi":"10.18653\/v1\/W19-4332","url":"https:\/\/www.aclweb.org\/anthology\/W19-4332","publisher":"Association for Computational Linguistics","address":"Florence, Italy","year":"2019","month":"August","booktitle":"Proceedings of the 4th Workshop on Representation Learning for NLP (RepL4NLP-2019)"}]