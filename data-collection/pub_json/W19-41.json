[{"id":"W19-4101","title":"A Repository of Conversational Datasets","authors":["Henderson, Matthew","Budzianowski, Pawe{\\l}","Casanueva, I{\\~n}igo","Coope, Sam","Gerz, Daniela","Kumar, Girish","Mrk{\\v{s}}i{\\'c}, Nikola","Spithourakis, Georgios","Su, Pei-Hao","Vuli{\\'c}, Ivan","Wen, Tsung-Hsien"],"emails":["matt@poly-ai.com","","","","","","","","","",""],"author_id":["matthew-henderson","pawel-budzianowski","inigo-casanueva","sam-coope","daniela-gerz","girish-kumar","nikola-mrksic","georgios-spithourakis","pei-hao-su","ivan-vulic","tsung-hsien-wen"],"abstract":"Progress in Machine Learning is often driven by the availability of large datasets, and consistent evaluation metrics for comparing modeling approaches. To this end, we present a repository of conversational datasets consisting of hundreds of millions of examples, and a standardised evaluation procedure for conversational response selection models using 1-of-100 accuracy. The repository contains scripts that allow researchers to reproduce the standard datasets, or to adapt the pre-processing and data filtering steps to their needs. We introduce and evaluate several competitive baselines for conversational response selection, whose implementations are shared in the repository, as well as a neural encoder model that is trained on the entire training set.","pages":"1--10","doi":"10.18653\/v1\/W19-4101","url":"https:\/\/www.aclweb.org\/anthology\/W19-4101","publisher":"Association for Computational Linguistics","address":"Florence, Italy","year":"2019","month":"August","booktitle":"Proceedings of the First Workshop on NLP for Conversational AI"},{"id":"W19-4102","title":"A Simple but Effective Method to Incorporate Multi-turn Context with {BERT} for Conversational Machine Comprehension","authors":["Ohsugi, Yasuhito","Saito, Itsumi","Nishida, Kyosuke","Asano, Hisako","Tomita, Junji"],"emails":["yasuhito.ohsugi.va@hco.ntt.co.jp","","","",""],"author_id":["yasuhito-ohsugi","itsumi-saito","kyosuke-nishida","hisako-asano","junji-tomita"],"abstract":"Conversational machine comprehension (CMC) requires understanding the context of multi-turn dialogue. Using BERT, a pretraining language model, has been successful for single-turn machine comprehension, while modeling multiple turns of question answering with BERT has not been established because BERT has a limit on the number and the length of input sequences. In this paper, we propose a simple but effective method with BERT for CMC. Our method uses BERT to encode a paragraph independently conditioned with each question and each answer in a multi-turn context. Then, the method predicts an answer on the basis of the paragraph representations encoded with BERT. The experiments with representative CMC datasets, QuAC and CoQA, show that our method outperformed recently published methods (+0.8 F1 on QuAC and +2.1 F1 on CoQA). In addition, we conducted a detailed analysis of the effects of the number and types of dialogue history on the accuracy of CMC, and we found that the gold answer history, which may not be given in an actual conversation, contributed to the model performance most on both datasets.","pages":"11--17","doi":"10.18653\/v1\/W19-4102","url":"https:\/\/www.aclweb.org\/anthology\/W19-4102","publisher":"Association for Computational Linguistics","address":"Florence, Italy","year":"2019","month":"August","booktitle":"Proceedings of the First Workshop on NLP for Conversational AI"},{"id":"W19-4103","title":"Augmenting Neural Response Generation with Context-Aware Topical Attention","authors":["Dziri, Nouha","Kamalloo, Ehsan","Mathewson, Kory","Zaiane, Osmar"],"emails":["dziri@cs.ualberta.ca","kamalloo@cs.ualberta.ca","korym@cs.ualberta.ca","zaiane@cs.ualberta.ca"],"author_id":["nouha-dziri","ehsan-kamalloo","kory-mathewson","osmar-r-zaiane"],"abstract":"Sequence-to-Sequence (Seq2Seq) models have witnessed a notable success in generating natural conversational exchanges. Notwithstanding the syntactically well-formed responses generated by these neural network models, they are prone to be acontextual, short and generic. In this work, we introduce a Topical Hierarchical Recurrent Encoder Decoder (THRED), a novel, fully data-driven, multi-turn response generation system intended to produce contextual and topic-aware responses. Our model is built upon the basic Seq2Seq model by augmenting it with a hierarchical joint attention mechanism that incorporates topical concepts and previous interactions into the response generation. To train our model, we provide a clean and high-quality conversational dataset mined from Reddit comments. We evaluate THRED on two novel automated metrics, dubbed Semantic Similarity and Response Echo Index, as well as with human evaluation. Our experiments demonstrate that the proposed model is able to generate more diverse and contextually relevant responses compared to the strong baselines.","pages":"18--31","doi":"10.18653\/v1\/W19-4103","url":"https:\/\/www.aclweb.org\/anthology\/W19-4103","publisher":"Association for Computational Linguistics","address":"Florence, Italy","year":"2019","month":"August","booktitle":"Proceedings of the First Workshop on NLP for Conversational AI"},{"id":"W19-4104","title":"Building a Production Model for Retrieval-Based Chatbots","authors":["Swanson, Kyle","Yu, Lili","Fox, Christopher","Wohlwend, Jeremy","Lei, Tao"],"emails":["swansonk@mit.edu","liliyu@asapp.com","cdfox@asapp.com","jeremy@asapp.com","tao@asapp.com"],"author_id":["kyle-swanson","lili-yu","christopher-fox","jeremy-wohlwend","tao-lei"],"abstract":"Response suggestion is an important task for building human-computer conversation systems. Recent approaches to conversation modeling have introduced new model architectures with impressive results, but relatively little attention has been paid to whether these models would be practical in a production setting. In this paper, we describe the unique challenges of building a production retrieval-based conversation system, which selects outputs from a whitelist of candidate responses. To address these challenges, we propose a dual encoder architecture which performs rapid inference and scales well with the size of the whitelist. We also introduce and compare two methods for generating whitelists, and we carry out a comprehensive analysis of the model and whitelists. Experimental results on a large, proprietary help desk chat dataset, including both offline metrics and a human evaluation, indicate production-quality performance and illustrate key lessons about conversation modeling in practice.","pages":"32--41","doi":"10.18653\/v1\/W19-4104","url":"https:\/\/www.aclweb.org\/anthology\/W19-4104","publisher":"Association for Computational Linguistics","address":"Florence, Italy","year":"2019","month":"August","booktitle":"Proceedings of the First Workshop on NLP for Conversational AI"},{"id":"W19-4105","title":"Co-Operation as an Asymmetric Form of Human-Computer Creativity. Case: Peace Machine","authors":["H{\\\"a}m{\\\"a}l{\\\"a}inen, Mika","Honkela, Timo"],"emails":["mika.hamalainen@helsinki.fi","timo.honkela@helsinki.fi"],"author_id":["mika-hamalainen","timo-honkela"],"abstract":"This theoretical paper identifies a need for a definition of asymmetric co-creativity where creativity is expected from the computational agent but not from the human user. Our co-operative creativity framework takes into account that the computational agent has a message to convey in a co-operative fashion, which introduces a trade-off on how creative the computer can be. The requirements of co-operation are identified from an interdisciplinary point of view. We divide co-operative creativity in message creativity, contextual creativity and communicative creativity. Finally these notions are applied in the context of the Peace Machine system concept.","pages":"42--50","doi":"10.18653\/v1\/W19-4105","url":"https:\/\/www.aclweb.org\/anthology\/W19-4105","publisher":"Association for Computational Linguistics","address":"Florence, Italy","year":"2019","month":"August","booktitle":"Proceedings of the First Workshop on NLP for Conversational AI"},{"id":"W19-4106","title":"Conversational Response Re-ranking Based on Event Causality and Role Factored Tensor Event Embedding","authors":["Tanaka, Shohei","Yoshino, Koichiro","Sudoh, Katsuhito","Nakamura, Satoshi"],"emails":["takana.shohei.tj7@is.naist.jp","koichiro@is.naist.jp","sudoh@is.naist.jp","s-nakamura@is.naist.jp"],"author_id":["shohei-tanaka","koichiro-yoshino","katsuhito-sudoh","satoshi-nakamura"],"abstract":"We propose a novel method for selecting coherent and diverse responses for a given dialogue context. The proposed method re-ranks response candidates generated from conversational models by using event causality relations between events in a dialogue history and response candidates (e.g., {``}be stressed out{''} precedes {``}relieve stress{''}). We use distributed event representation based on the Role Factored Tensor Model for a robust matching of event causality relations due to limited event causality knowledge of the system. Experimental results showed that the proposed method improved coherency and dialogue continuity of system responses.","pages":"51--59","doi":"10.18653\/v1\/W19-4106","url":"https:\/\/www.aclweb.org\/anthology\/W19-4106","publisher":"Association for Computational Linguistics","address":"Florence, Italy","year":"2019","month":"August","booktitle":"Proceedings of the First Workshop on NLP for Conversational AI"},{"id":"W19-4107","title":"{DSTC}7 Task 1: Noetic End-to-End Response Selection","authors":["Gunasekara, Chulaka","Kummerfeld, Jonathan K.","Polymenakos, Lazaros","Lasecki, Walter"],"emails":["chulaka.gunasekara@ibm.com","jkummerf@umich.edu","","wlasecki@umich.edu"],"author_id":["chulaka-gunasekara","jonathan-k-kummerfeld","lazaros-polymenakos","walter-lasecki"],"abstract":"Goal-oriented dialogue in complex domains is an extremely challenging problem and there are relatively few datasets. This task provided two new resources that presented different challenges: one was focused but small, while the other was large but diverse. We also considered several new variations on the next utterance selection problem: (1) increasing the number of candidates, (2) including paraphrases, and (3) not including a correct option in the candidate set. Twenty teams participated, developing a range of neural network models, including some that successfully incorporated external data to boost performance. Both datasets have been publicly released, enabling future work to build on these results, working towards robust goal-oriented dialogue systems.","pages":"60--67","doi":"10.18653\/v1\/W19-4107","url":"https:\/\/www.aclweb.org\/anthology\/W19-4107","publisher":"Association for Computational Linguistics","address":"Florence, Italy","year":"2019","month":"August","booktitle":"Proceedings of the First Workshop on NLP for Conversational AI"},{"id":"W19-4108","title":"End-to-End Neural Context Reconstruction in {C}hinese Dialogue","authors":["Yang, Wei","Qiao, Rui","Qin, Haocheng","Sun, Amy","Tan, Luchen","Xiong, Kun","Li, Ming"],"emails":["","","","","","",""],"author_id":["wei-yang","rui-qiao","haocheng-qin","amy-sun","luchen-tan","kun-xiong","ming-li"],"abstract":"We tackle the problem of context reconstruction in Chinese dialogue, where the task is to replace pronouns, zero pronouns, and other referring expressions with their referent nouns so that sentences can be processed in isolation without context. Following a standard decomposition of the context reconstruction task into referring expression detection and coreference resolution, we propose a novel end-to-end architecture for separately and jointly accomplishing this task. Key features of this model include POS and position encoding using CNNs and a novel pronoun masking mechanism. One perennial problem in building such models is the paucity of training data, which we address by augmenting previously-proposed methods to generate a large amount of realistic training data. The combination of more data and better models yields accuracy higher than the state-of-the-art method in coreference resolution and end-to-end context reconstruction.","pages":"68--76","doi":"10.18653\/v1\/W19-4108","url":"https:\/\/www.aclweb.org\/anthology\/W19-4108","publisher":"Association for Computational Linguistics","address":"Florence, Italy","year":"2019","month":"August","booktitle":"Proceedings of the First Workshop on NLP for Conversational AI"},{"id":"W19-4109","title":"Energy-Based Modelling for Dialogue State Tracking","authors":["Trinh, Anh Duong","Ross, Robert","Kelleher, John"],"emails":["anhduong.trinh@mydit.ie","robert.ross@dit.ie","john.d.kelleher@dit.ie"],"author_id":["anh-duong-trinh","robert-ross","john-kelleher"],"abstract":"The uncertainties of language and the complexity of dialogue contexts make accurate dialogue state tracking one of the more challenging aspects of dialogue processing. To improve state tracking quality, we argue that relationships between different aspects of dialogue state must be taken into account as they can often guide a more accurate interpretation process. To this end, we present an energy-based approach to dialogue state tracking as a structured classification task. The novelty of our approach lies in the use of an energy network on top of a deep learning architecture to explore more signal correlations between network variables including input features and output labels. We demonstrate that the energy-based approach improves the performance of a deep learning dialogue state tracker towards state-of-the-art results without the need for many of the other steps required by current state-of-the-art methods.","pages":"77--86","doi":"10.18653\/v1\/W19-4109","url":"https:\/\/www.aclweb.org\/anthology\/W19-4109","publisher":"Association for Computational Linguistics","address":"Florence, Italy","year":"2019","month":"August","booktitle":"Proceedings of the First Workshop on NLP for Conversational AI"},{"id":"W19-4110","title":"Evaluation and Improvement of Chatbot Text Classification Data Quality Using Plausible Negative Examples","authors":["Kuksenok, Kit","Martyniv, Andriy"],"emails":["kit@jobpal.ai","andriy@jobpal.ai"],"author_id":["kit-kuksenok","andriy-martyniv"],"abstract":"We describe and validate a metric for estimating multi-class classifier performance based on cross-validation and adapted for improvement of small, unbalanced natural-language datasets used in chatbot design. Our experiences draw upon building recruitment chatbots that mediate communication between job-seekers and recruiters by exposing the ML\/NLP dataset to the recruiting team. Evaluation approaches must be understandable to various stakeholders, and useful for improving chatbot performance. The metric, nex-cv, uses negative examples in the evaluation of text classification, and fulfils three requirements. First, it is actionable: it can be used by non-developer staff. Second, it is not overly optimistic compared to human ratings, making it a fast method for comparing classifiers. Third, it allows model-agnostic comparison, making it useful for comparing systems despite implementation differences. We validate the metric based on seven recruitment-domain datasets in English and German over the course of one year.","pages":"87--95","doi":"10.18653\/v1\/W19-4110","url":"https:\/\/www.aclweb.org\/anthology\/W19-4110","publisher":"Association for Computational Linguistics","address":"Florence, Italy","year":"2019","month":"August","booktitle":"Proceedings of the First Workshop on NLP for Conversational AI"},{"id":"W19-4111","title":"Improving Long Distance Slot Carryover in Spoken Dialogue Systems","authors":["Chen, Tongfei","Naik, Chetan","He, Hua","Rastogi, Pushpendre","Mathias, Lambert"],"emails":["tongfei@jhu.edu","chetnaik@amazon.com","huhe@amazon.com","prastogi@amazon.com","mathiasl@amazon.com"],"author_id":["tongfei-chen","chetan-naik","hua-he","pushpendre-rastogi","lambert-mathias"],"abstract":"Tracking the state of the conversation is a central component in task-oriented spoken dialogue systems. One such approach for tracking the dialogue state is slot carryover, where a model makes a binary decision if a slot from the context is relevant to the current turn. Previous work on the slot carryover task used models that made independent decisions for each slot. A close analysis of the results show that this approach results in poor performance over longer context dialogues. In this paper, we propose to jointly model the slots. We propose two neural network architectures, one based on pointer networks that incorporate slot ordering information, and the other based on transformer networks that uses self attention mechanism to model the slot interdependencies. Our experiments on an internal dialogue benchmark dataset and on the public DSTC2 dataset demonstrate that our proposed models are able to resolve longer distance slot references and are able to achieve competitive performance.","pages":"96--105","doi":"10.18653\/v1\/W19-4111","url":"https:\/\/www.aclweb.org\/anthology\/W19-4111","publisher":"Association for Computational Linguistics","address":"Florence, Italy","year":"2019","month":"August","booktitle":"Proceedings of the First Workshop on NLP for Conversational AI"},{"id":"W19-4112","title":"Insights from Building an Open-Ended Conversational Agent","authors":["Gupta, Khyatti","Joshi, Meghana","Chatterjee, Ankush","Damani, Sonam","Narahari, Kedhar Nath","Agrawal, Puneet"],"emails":["khgupt@microsoft.com","mejoshi@microsoft.com","anchatte@microsoft.com","sodamani@microsoft.com","kedharn@microsoft.com","punagr@microsoft.com"],"author_id":["khyatti-gupta","meghana-joshi","ankush-chatterjee","sonam-damani","kedhar-nath-narahari","puneet-agrawal"],"abstract":"Dialogue systems and conversational agents are becoming increasingly popular in modern society. We conceptualized one such conversational agent, Microsoft{'}s {``}Ruuh{''} with the promise to be able to talk to its users on any subject they choose. Building an open-ended conversational agent like Ruuh at onset seems like a daunting task, since the agent needs to think beyond the utilitarian notion of merely generating {``}relevant{''} responses and meet a wider range of user social needs, like expressing happiness when user{'}s favourite sports team wins, sharing a cute comment on showing the pictures of the user{'}s pet and so on. The agent also needs to detect and respond to abusive language, sensitive topics and trolling behaviour of the users. Many of these problems pose significant research challenges as well as product design limitations as one needs to circumnavigate the technical limitations to create an acceptable user experience. However, as the product reaches the real users the true test begins, and one realizes the challenges and opportunities that lie in the vast domain of conversations. With over 2.5 million real-world users till date who have generated over 300 million user conversations with Ruuh, there is a plethora of learning, insights and opportunities that we will talk about in this paper.","pages":"106--112","doi":"10.18653\/v1\/W19-4112","url":"https:\/\/www.aclweb.org\/anthology\/W19-4112","publisher":"Association for Computational Linguistics","address":"Florence, Italy","year":"2019","month":"August","booktitle":"Proceedings of the First Workshop on NLP for Conversational AI"},{"id":"W19-4113","title":"Learning to Explain: Answering Why-Questions via Rephrasing","authors":["Nie, Allen","Bennett, Erin","Goodman, Noah"],"emails":["anie@cs.stanford.edu","erindb@stanford.edu","ngoodman@stanford.edu"],"author_id":["allen-nie","erin-bennett","noah-goodman"],"abstract":"Providing plausible responses to why questions is a challenging but critical goal for language based human-machine interaction. Explanations are challenging in that they require many different forms of abstract knowledge and reasoning. Previous work has either relied on human-curated structured knowledge bases or detailed domain representation to generate satisfactory explanations. They are also often limited to ranking pre-existing explanation choices. In our work, we contribute to the under-explored area of generating natural language explanations for general phenomena. We automatically collect large datasets of explanation-phenomenon pairs which allow us to train sequence-to-sequence models to generate natural language explanations. We compare different training strategies and evaluate their performance using both automatic scores and human ratings. We demonstrate that our strategy is sufficient to generate highly plausible explanations for general open-domain phenomena compared to other models trained on different datasets.","pages":"113--120","doi":"10.18653\/v1\/W19-4113","url":"https:\/\/www.aclweb.org\/anthology\/W19-4113","publisher":"Association for Computational Linguistics","address":"Florence, Italy","year":"2019","month":"August","booktitle":"Proceedings of the First Workshop on NLP for Conversational AI"},{"id":"W19-4114","title":"Multi-turn Dialogue Response Generation in an Adversarial Learning Framework","authors":["Olabiyi, Oluwatobi","Salimov, Alan O","Khazane, Anish","Mueller, Erik"],"emails":["oluwatobi.olabiyi@capitalone.com","alan.salimov@capitalone.com","anish.khazane@capitalone.com","erik.mueller@capitalone.com"],"author_id":["oluwatobi-olabiyi","alan-o-salimov","anish-khazane","erik-mueller"],"abstract":"We propose an adversarial learning approach for generating multi-turn dialogue responses. Our proposed framework, hredGAN, is based on conditional generative adversarial networks (GANs). The GAN{'}s generator is a modified hierarchical recurrent encoder-decoder network (HRED) and the discriminator is a word-level bidirectional RNN that shares context and word embeddings with the generator. During inference, noise samples conditioned on the dialogue history are used to perturb the generator{'}s latent space to generate several possible responses. The final response is the one ranked best by the discriminator. The hredGAN shows improved performance over existing methods: (1) it generalizes better than networks trained using only the log-likelihood criterion, and (2) it generates longer, more informative and more diverse responses with high utterance and topic relevance even with limited training data. This performance improvement is demonstrated on the Movie triples and Ubuntu dialogue datasets with both the automatic and human evaluations.","pages":"121--132","doi":"10.18653\/v1\/W19-4114","url":"https:\/\/www.aclweb.org\/anthology\/W19-4114","publisher":"Association for Computational Linguistics","address":"Florence, Italy","year":"2019","month":"August","booktitle":"Proceedings of the First Workshop on NLP for Conversational AI"},{"id":"W19-4115","title":"Relevant and Informative Response Generation using Pointwise Mutual Information","authors":["Takayama, Junya","Arase, Yuki"],"emails":["takayama.junya@ist.osaka-u.ac.jp","arase@ist.osaka-u.ac.jp"],"author_id":["junya-takayama","yuki-arase"],"abstract":"A sequence-to-sequence model tends to generate generic responses with little information for input utterances. To solve this problem, we propose a neural model that generates relevant and informative responses. Our model has simple architecture to enable easy application to existing neural dialogue models. Specifically, using positive pointwise mutual information, it first identifies keywords that frequently co-occur in responses given an utterance. Then, the model encourages the decoder to use the keywords for response generation. Experiment results demonstrate that our model successfully diversifies responses relative to previous models.","pages":"133--138","doi":"10.18653\/v1\/W19-4115","url":"https:\/\/www.aclweb.org\/anthology\/W19-4115","publisher":"Association for Computational Linguistics","address":"Florence, Italy","year":"2019","month":"August","booktitle":"Proceedings of the First Workshop on NLP for Conversational AI"},{"id":"W19-4116","title":"Responsive and Self-Expressive Dialogue Generation","authors":["Chikai, Kozo","Takayama, Junya","Arase, Yuki"],"emails":["tokoroten0401@gmail.com","takayama.junya@ist.osaka-u.ac.jp","arase@ist.osaka-u.ac.jp"],"author_id":["kozo-chikai","junya-takayama","yuki-arase"],"abstract":"A neural conversation model is a promising approach to develop dialogue systems with the ability of chit-chat. It allows training a model in an end-to-end manner without complex rule design nor feature engineering. However, as a side effect, the neural model tends to generate safe but uninformative and insensitive responses like {``}OK{''} and {``}I don{'}t know.{''} Such replies are called generic responses and regarded as a critical problem for user-engagement of dialogue systems. For a more engaging chit-chat experience, we propose a neural conversation model that generates responsive and self-expressive replies. Specifically, our model generates domain-aware and sentiment-rich responses. Experiments empirically confirmed that our model outperformed the sequence-to-sequence model; 68.1{\\%} of our responses were domain-aware with sentiment polarities, which was only 2.7{\\%} for responses generated by the sequence-to-sequence model.","pages":"139--149","doi":"10.18653\/v1\/W19-4116","url":"https:\/\/www.aclweb.org\/anthology\/W19-4116","publisher":"Association for Computational Linguistics","address":"Florence, Italy","year":"2019","month":"August","booktitle":"Proceedings of the First Workshop on NLP for Conversational AI"}]