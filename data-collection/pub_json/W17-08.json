[{"id":"W17-0801","title":"Readers vs. Writers vs. Texts: Coping with Different Perspectives of Text Understanding in Emotion Annotation","authors":["Buechel, Sven","Hahn, Udo"],"emails":["sven.buechel@uni-jena.de","udo.hahn@uni-jena.de"],"author_id":["sven-buechel","udo-hahn"],"abstract":"We here examine how different perspectives of understanding written discourse, like the reader{'}s, the writer{'}s or the text{'}s point of view, affect the quality of emotion annotations. We conducted a series of annotation experiments on two corpora, a popular movie review corpus and a genre- and domain-balanced corpus of standard English. We found statistical evidence that the writer{'}s perspective yields superior annotation quality overall. However, the quality one perspective yields compared to the other(s) seems to depend on the domain the utterance originates from. Our data further suggest that the popular movie review data set suffers from an atypical bimodal distribution which may decrease model performance when used as a training resource.","pages":"1--12","doi":"10.18653\/v1\/W17-0801","url":"https:\/\/www.aclweb.org\/anthology\/W17-0801","publisher":"Association for Computational Linguistics","address":"Valencia, Spain","year":"2017","month":"April","booktitle":"Proceedings of the 11th Linguistic Annotation Workshop"},{"id":"W17-0802","title":"Finding Good Conversations Online: The Yahoo News Annotated Comments Corpus","authors":["Napoles, Courtney","Tetreault, Joel","Pappu, Aasish","Rosato, Enrica","Provenzale, Brian"],"emails":["napoles@cs.jhu.edu","joel.tetreault@grammarly.com","aasishkp@yahoo-inc.com","niversity@cs.jhu.edu","bprovenzale@gmail.com"],"author_id":["courtney-napoles","joel-tetreault","aasish-pappu","enrica-rosato","brian-provenzale"],"abstract":"This work presents a dataset and annotation scheme for the new task of identifying {``}good{''} conversations that occur online, which we call ERICs: Engaging, Respectful, and\/or Informative Conversations. We develop a taxonomy to reflect features of entire threads and individual comments which we believe contribute to identifying ERICs; code a novel dataset of Yahoo News comment threads (2.4k threads and 10k comments) and 1k threads from the Internet Argument Corpus; and analyze the features characteristic of ERICs. This is one of the largest annotated corpora of online human dialogues, with the most detailed set of annotations. It will be valuable for identifying ERICs and other aspects of argumentation, dialogue, and discourse.","pages":"13--23","doi":"10.18653\/v1\/W17-0802","url":"https:\/\/www.aclweb.org\/anthology\/W17-0802","publisher":"Association for Computational Linguistics","address":"Valencia, Spain","year":"2017","month":"April","booktitle":"Proceedings of the 11th Linguistic Annotation Workshop"},{"id":"W17-0803","title":"Crowdsourcing discourse interpretations: On the influence of context and the reliability of a connective insertion task","authors":["Scholman, Merel","Demberg, Vera"],"emails":["m.c.j.scholman@coli.uni-saarland.de","vera@coli.uni-saarland.de"],"author_id":["merel-scholman","vera-demberg"],"abstract":"Traditional discourse annotation tasks are considered costly and time-consuming, and the reliability and validity of these tasks is in question. In this paper, we investigate whether crowdsourcing can be used to obtain reliable discourse relation annotations. We also examine the influence of context on the reliability of the data. The results of a crowdsourced connective insertion task showed that the method can be used to obtain reliable annotations: The majority of the inserted connectives converged with the original label. Further, the method is sensitive to the fact that multiple senses can often be inferred for a single relation. Regarding the presence of context, the results show no significant difference in distributions of insertions between conditions overall. However, a by-item comparison revealed several characteristics of segments that determine whether the presence of context makes a difference in annotations. The findings discussed in this paper can be taken as evidence that crowdsourcing can be used as a valuable method to obtain insights into the sense(s) of relations.","pages":"24--33","doi":"10.18653\/v1\/W17-0803","url":"https:\/\/www.aclweb.org\/anthology\/W17-0803","publisher":"Association for Computational Linguistics","address":"Valencia, Spain","year":"2017","month":"April","booktitle":"Proceedings of the 11th Linguistic Annotation Workshop"},{"id":"W17-0804","title":"A Code-Switching Corpus of {T}urkish-{G}erman Conversations","authors":["{\\c{C}}etino{\\u{g}}lu, {\\\"O}zlem"],"emails":["ozlem@ims.uni-stuttgart.de"],"author_id":["ozlem-cetinoglu"],"abstract":"We present a code-switching corpus of Turkish-German that is collected by recording conversations of bilinguals. The recordings are then transcribed in two layers following speech and orthography conventions, and annotated with sentence boundaries and intersentential, intrasentential, and intra-word switch points. The total amount of data is 5 hours of speech which corresponds to 3614 sentences. The corpus aims at serving as a resource for speech or text analysis, as well as a collection for linguistic inquiries.","pages":"34--40","doi":"10.18653\/v1\/W17-0804","url":"https:\/\/www.aclweb.org\/anthology\/W17-0804","publisher":"Association for Computational Linguistics","address":"Valencia, Spain","year":"2017","month":"April","booktitle":"Proceedings of the 11th Linguistic Annotation Workshop"},{"id":"W17-0805","title":"Annotating omission in statement pairs","authors":["Mart{\\'\\i}nez Alonso, H{\\'e}ctor","Delamaire, Amaury","Sagot, Beno{\\^\\i}t"],"emails":["hector.martinez-alonso@inria.fr","amaury.delamaire@trooclick.com","benoit.sagot@inria.fr"],"author_id":["hector-martinez-alonso","amaury-delamaire","benoit-sagot"],"abstract":"We focus on the identification of omission in statement pairs. We compare three annotation schemes, namely two different crowdsourcing schemes and manual expert annotation. We show that the simplest of the two crowdsourcing approaches yields a better annotation quality than the more complex one. We use a dedicated classifier to assess whether the annotators{'} behavior can be explained by straightforward linguistic features. The classifier benefits from a modeling that uses lexical information beyond length and overlap measures. However, for our task, we argue that expert and not crowdsourcing-based annotation is the best compromise between annotation cost and quality.","pages":"41--45","doi":"10.18653\/v1\/W17-0805","url":"https:\/\/www.aclweb.org\/anthology\/W17-0805","publisher":"Association for Computational Linguistics","address":"Valencia, Spain","year":"2017","month":"April","booktitle":"Proceedings of the 11th Linguistic Annotation Workshop"},{"id":"W17-0806","title":"Annotating Speech, Attitude and Perception Reports","authors":["Bary, Corien","Hess, Leopold","Thijs, Kees","Berck, Peter","Hendrickx, Iris"],"emails":["c.bary@ftr.ru.nl","l.hess@ftr.ru.nl","k.thijs@ftr.ru.nl","p.berck@let.ru.nl","i.hendrickx@let.ru.nl"],"author_id":["corien-bary","leopold-hess","kees-thijs","peter-berck","iris-hendrickx"],"abstract":"We present REPORTS, an annotation scheme for the annotation of speech, attitude and perception reports. Such a scheme makes it possible to annotate the various text elements involved in such reports (e.g. embedding entity, complement, complement head) and their relations in a uniform way, which in turn facilitates the automatic extraction of information on, for example, complementation and vocabulary distribution. We also present the Ancient Greek corpus RAG (Thucydides{'} History of the Peloponnesian War), to which we have applied this scheme using the annotation tool BRAT. We discuss some of the issues, both theoretical and practical, that we encountered, show how the corpus helps in answering specific questions, and conclude that REPORTS fitted in well with our needs.","pages":"46--56","doi":"10.18653\/v1\/W17-0806","url":"https:\/\/www.aclweb.org\/anthology\/W17-0806","publisher":"Association for Computational Linguistics","address":"Valencia, Spain","year":"2017","month":"April","booktitle":"Proceedings of the 11th Linguistic Annotation Workshop"},{"id":"W17-0807","title":"Consistent Classification of Translation Revisions: A Case Study of {E}nglish-{J}apanese Student Translations","authors":["Fujita, Atsushi","Tanabe, Kikuko","Toyoshima, Chiho","Yamamoto, Mayuka","Kageura, Kyo","Hartley, Anthony"],"emails":["atsushi.fujita@nict.go.jp","kikukotanabe@gmail.com","c.toyoshima1113@gmail.com","yamamoto.mayuka@honyakuctr.co.jp","kyo@p.u-tokyo.ac.jp","artley@rikkyo.ac.jp"],"author_id":["atsushi-fujita","kikuko-tanabe","chiho-toyoshima","mayuka-yamamoto","kyo-kageura","anthony-hartley"],"abstract":"Consistency is a crucial requirement in text annotation. It is especially important in educational applications, as lack of consistency directly affects learners{'} motivation and learning performance. This paper presents a quality assessment scheme for English-to-Japanese translations produced by learner translators at university. We constructed a revision typology and a decision tree manually through an application of the OntoNotes method, i.e., an iteration of assessing learners{'} translations and hypothesizing the conditions for consistent decision making, as well as re-organizing the typology. Intrinsic evaluation of the created scheme confirmed its potential contribution to the consistent classification of identified erroneous text spans, achieving visibly higher Cohen{'}s kappa values, up to 0.831, than previous work. This paper also describes an application of our scheme to an English-to-Japanese translation exercise course for undergraduate students at a university in Japan.","pages":"57--66","doi":"10.18653\/v1\/W17-0807","url":"https:\/\/www.aclweb.org\/anthology\/W17-0807","publisher":"Association for Computational Linguistics","address":"Valencia, Spain","year":"2017","month":"April","booktitle":"Proceedings of the 11th Linguistic Annotation Workshop"},{"id":"W17-0808","title":"Representation and Interchange of Linguistic Annotation. An In-Depth, Side-by-Side Comparison of Three Designs","authors":["Eckart de Castilho, Richard","Ide, Nancy","Lapponi, Emanuele","Oepen, Stephan","Suderman, Keith","Velldal, Erik","Verhagen, Marc"],"emails":["","","","","","",""],"author_id":["richard-eckart-de-castilho","nancy-ide","emanuele-lapponi","stephan-oepen","keith-suderman","erik-velldal","marc-verhagen"],"abstract":"For decades, most self-respecting linguistic engineering initiatives have designed and implemented custom representations for various layers of, for example, morphological, syntactic, and semantic analysis. Despite occasional efforts at harmonization or even standardization, our field today is blessed with a multitude of ways of encoding and exchanging linguistic annotations of these types, both at the levels of {`}abstract syntax{'}, naming choices, and of course file formats. To a large degree, it is possible to work within and across design plurality by conversion, and often there may be good reasons for divergent design reflecting differences in use. However, it is likely that some abstract commonalities across choices of representation are obscured by more superficial differences, and conversely there is no obvious procedure to tease apart what actually constitute contentful vs. mere technical divergences. In this study, we seek to conceptually align three representations for common types of morpho-syntactic analysis, pinpoint what in our view constitute contentful differences, and reflect on the underlying principles and specific requirements that led to individual choices. We expect that a more in-depth understanding of these choices across designs may led to increased harmonization, or at least to more informed design of future representations.","pages":"67--75","doi":"10.18653\/v1\/W17-0808","url":"https:\/\/www.aclweb.org\/anthology\/W17-0808","publisher":"Association for Computational Linguistics","address":"Valencia, Spain","year":"2017","month":"April","booktitle":"Proceedings of the 11th Linguistic Annotation Workshop"},{"id":"W17-0809","title":"{TDB} 1.1: Extensions on {T}urkish Discourse Bank","authors":["Zeyrek, Deniz","Kurfal{\\i}, Murathan"],"emails":["dezeyrek@metu.edu.tr","kurfali@metu.edu.tr"],"author_id":["deniz-zeyrek","murathan-kurfali"],"abstract":"This paper presents the recent developments on Turkish Discourse Bank (TDB). First, the resource is summarized and an evaluation is presented. Then, TDB 1.1, i.e. enrichments on 10{\\%} of the corpus are described (namely, senses for explicit discourse connectives, and new annotations for three discourse relation types - implicit relations, entity relations and alternative lexicalizations). The method of annotation is explained and the data are evaluated.","pages":"76--81","doi":"10.18653\/v1\/W17-0809","url":"https:\/\/www.aclweb.org\/anthology\/W17-0809","publisher":"Association for Computational Linguistics","address":"Valencia, Spain","year":"2017","month":"April","booktitle":"Proceedings of the 11th Linguistic Annotation Workshop"},{"id":"W17-0810","title":"Two Layers of Annotation for Representing Event Mentions in News Stories","authors":["di Buono, Maria Pia","Tutek, Martin","{\\v{S}}najder, Jan","Glava{\\v{s}}, Goran","Dalbelo Ba{\\v{s}}i{\\'c}, Bojana","Mili{\\'c}-Frayling, Nata{\\v{s}}a"],"emails":["maria.di buono@fer.hr","martin.tutek@fer.hr","jan.{\\v{s}}najder@fer.hr","goran@informatik.uni-mannheim.de","bojana.dalbelo ba{\\v{s}}i{\\'c}@fer.hr","nata{\\v{s}}a.mili{\\'c}-frayling@fer.hr"],"author_id":["maria-pia-di-buono","martin-tutek","jan-snajder","goran-glavas","bojana-dalbelo-basic","natasa-milic-frayling"],"abstract":"In this paper, we describe our preliminary study on annotating event mention as a part of our research on high-precision news event extraction models. To this end, we propose a two-layer annotation scheme, designed to separately capture the functional and conceptual aspects of event mentions. We hypothesize that the precision of models can be improved by modeling and extracting separately the different aspects of news events, and then combining the extracted information by leveraging the complementarities of the models. In addition, we carry out a preliminary annotation using the proposed scheme and analyze the annotation quality in terms of inter-annotator agreement.","pages":"82--90","doi":"10.18653\/v1\/W17-0810","url":"https:\/\/www.aclweb.org\/anthology\/W17-0810","publisher":"Association for Computational Linguistics","address":"Valencia, Spain","year":"2017","month":"April","booktitle":"Proceedings of the 11th Linguistic Annotation Workshop"},{"id":"W17-0811","title":"Word Similarity Datasets for {I}ndian Languages: Annotation and Baseline Systems","authors":["Akhtar, Syed Sarfaraz","Gupta, Arihant","Vajpayee, Avijit","Srivastava, Arjit","Shrivastava, Manish"],"emails":["syed.akhtar@research.iiit.ac.in","arihant.gupta@research.iiit.ac.in","avijit.vajpayee@students.iiit.ac.in","arjit.srivastava@research.iiit.ac.in","manish.shrivastava@iiit.ac.in"],"author_id":["syed-sarfaraz-akhtar","arihant-gupta","avijit-vajpayee","arjit-srivastava","manish-shrivastava"],"abstract":"With the advent of word representations, word similarity tasks are becoming increasing popular as an evaluation metric for the quality of the representations. In this paper, we present manually annotated monolingual word similarity datasets of six Indian languages - Urdu, Telugu, Marathi, Punjabi, Tamil and Gujarati. These languages are most spoken Indian languages worldwide after Hindi and Bengali. For the construction of these datasets, our approach relies on translation and re-annotation of word similarity datasets of English. We also present baseline scores for word representation models using state-of-the-art techniques for Urdu, Telugu and Marathi by evaluating them on newly created word similarity datasets.","pages":"91--94","doi":"10.18653\/v1\/W17-0811","url":"https:\/\/www.aclweb.org\/anthology\/W17-0811","publisher":"Association for Computational Linguistics","address":"Valencia, Spain","year":"2017","month":"April","booktitle":"Proceedings of the 11th Linguistic Annotation Workshop"},{"id":"W17-0812","title":"The {BEC}au{SE} Corpus 2.0: Annotating Causality and Overlapping Relations","authors":["Dunietz, Jesse","Levin, Lori","Carbonell, Jaime"],"emails":["jdunietz@cs.cmu.edu","jgc@cs.cmu.edu","lsl@cs.cmu.edu"],"author_id":["jesse-dunietz","lori-levin","jaime-g-carbonell"],"abstract":"Language of cause and effect captures an essential component of the semantics of a text. However, causal language is also intertwined with other semantic relations, such as temporal precedence and correlation. This makes it difficult to determine when causation is the primary intended meaning. This paper presents BECauSE 2.0, a new version of the BECauSE corpus with exhaustively annotated expressions of causal language, but also seven semantic relations that are frequently co-present with causation. The new corpus shows high inter-annotator agreement, and yields insights both about the linguistic expressions of causation and about the process of annotating co-present semantic relations.","pages":"95--104","doi":"10.18653\/v1\/W17-0812","url":"https:\/\/www.aclweb.org\/anthology\/W17-0812","publisher":"Association for Computational Linguistics","address":"Valencia, Spain","year":"2017","month":"April","booktitle":"Proceedings of the 11th Linguistic Annotation Workshop"},{"id":"W17-0813","title":"Catching the Common Cause: Extraction and Annotation of Causal Relations and their Participants","authors":["Rehbein, Ines","Ruppenhofer, Josef"],"emails":["rehbein@cl.uni-heidelberg.de","ruppenhofer@cl.uni-heidelberg.de"],"author_id":["ines-rehbein","josef-ruppenhofer"],"abstract":"In this paper, we present a simple, yet effective method for the automatic identification and extraction of causal relations from text, based on a large English-German parallel corpus. The goal of this effort is to create a lexical resource for German causal relations. The resource will consist of a lexicon that describes constructions that trigger causality as well as the participants of the causal event, and will be augmented by a corpus with annotated instances for each entry, that can be used as training data to develop a system for automatic classification of causal relations. Focusing on verbs, our method harvested a set of 100 different lexical triggers of causality, including support verb constructions. At the moment, our corpus includes over 1,000 annotated instances. The lexicon and the annotated data will be made available to the research community.","pages":"105--114","doi":"10.18653\/v1\/W17-0813","url":"https:\/\/www.aclweb.org\/anthology\/W17-0813","publisher":"Association for Computational Linguistics","address":"Valencia, Spain","year":"2017","month":"April","booktitle":"Proceedings of the 11th Linguistic Annotation Workshop"},{"id":"W17-0814","title":"Assessing {SRL} Frameworks with Automatic Training Data Expansion","authors":["Hartmann, Silvana","M{\\'u}jdricza-Maydt, {\\'E}va","Kuznetsov, Ilia","Gurevych, Iryna","Frank, Anette"],"emails":["","mujdricza@ukp.informatik.tu-darmstadt.de","","","frank@ukp.informatik.tu-darmstadt.de"],"author_id":["silvana-hartmann","eva-mujdricza-maydt","ilia-kuznetsov","iryna-gurevych","anette-frank"],"abstract":"We present the first experiment-based study that explicitly contrasts the three major semantic role labeling frameworks. As a prerequisite, we create a dataset labeled with parallel FrameNet-, PropBank-, and VerbNet-style labels for German. We train a state-of-the-art SRL tool for German for the different annotation styles and provide a comparative analysis across frameworks. We further explore the behavior of the frameworks with automatic training data generation. VerbNet provides larger semantic expressivity than PropBank, and we find that its generalization capacity approaches PropBank in SRL training, but it benefits less from training data expansion than the sparse-data affected FrameNet.","pages":"115--121","doi":"10.18653\/v1\/W17-0814","url":"https:\/\/www.aclweb.org\/anthology\/W17-0814","publisher":"Association for Computational Linguistics","address":"Valencia, Spain","year":"2017","month":"April","booktitle":"Proceedings of the 11th Linguistic Annotation Workshop"}]