[{"id":"Q19-1001","title":"Grammar Error Correction in Morphologically Rich Languages: The Case of {R}ussian","authors":["Rozovskaya, Alla","Roth, Dan"],"emails":["arozovskaya@qc.cuny.edu","danroth@seas.upenn.edu"],"author_id":["alla-rozovskaya","dan-roth"],"abstract":"Until now, most of the research in grammar error correction focused on English, and the problem has hardly been explored for other languages. We address the task of correcting writing mistakes in morphologically rich languages, with a focus on Russian. We present a corrected and error-tagged corpus of Russian learner writing and develop models that make use of existing state-of-the-art methods that have been well studied for English. Although impressive results have recently been achieved for grammar error correction of non-native English writing, these results are limited to domains where plentiful training data are available. Because annotation is extremely costly, these approaches are not suitable for the majority of domains and languages. We thus focus on methods that use {``}minimal supervision{''}; that is, those that do not rely on large amounts of annotated training data, and show how existing minimal-supervision approaches extend to a highly inflectional language such as Russian. The results demonstrate that these methods are particularly useful for correcting mistakes in grammatical phenomena that involve rich morphology.","pages":"1--17","doi":"10.1162\/tacl_a_00251","url":"https:\/\/www.aclweb.org\/anthology\/Q19-1001","year":"2019","month":"March","volume":"7","journal":"Transactions of the Association for Computational Linguistics"},{"id":"Q19-1002","title":"Semantic Neural Machine Translation Using {AMR}","authors":["Song, Linfeng","Gildea, Daniel","Zhang, Yue","Wang, Zhiguo","Su, Jinsong"],"emails":["lsong10@cs.rochester.edu","gildea@cs.rochester.edu","2yue.zhang@wias.org.cn","3zgw.tomorrow@gmail.com","4jssu@xmu.edu.cn"],"author_id":["linfeng-song","daniel-gildea","yue-zhang","zhiguo-wang","jinsong-su"],"abstract":"It is intuitive that semantic representations can be useful for machine translation, mainly because they can help in enforcing meaning preservation and handling data sparsity (many sentences correspond to one meaning) of machine translation models. On the other hand, little work has been done on leveraging semantics for neural machine translation (NMT). In this work, we study the usefulness of AMR (abstract meaning representation) on NMT. Experiments on a standard English-to-German dataset show that incorporating AMR as additional knowledge can significantly improve a strong attention-based sequence-to-sequence neural translation model.","pages":"19--31","doi":"10.1162\/tacl_a_00252","url":"https:\/\/www.aclweb.org\/anthology\/Q19-1002","year":"2019","month":"March","volume":"7","journal":"Transactions of the Association for Computational Linguistics"},{"id":"Q19-1003","title":"Joint Transition-Based Models for Morpho-Syntactic Parsing: Parsing Strategies for {MRL}s and a Case Study from Modern {H}ebrew","authors":["More, Amir","Seker, Amit","Basmova, Victoria","Tsarfaty, Reut"],"emails":["habeanf@gmail.com","amitse@openu.ac.il","vicbas@openu.ac.il","reutts@openu.ac.il"],"author_id":["amir-more","amit-seker","victoria-basmova","reut-tsarfaty"],"abstract":"In standard NLP pipelines, morphological analysis and disambiguation (MA{\\&}D) precedes syntactic and semantic downstream tasks. However, for languages with complex and ambiguous word-internal structure, known as morphologically rich languages (MRLs), it has been hypothesized that syntactic context may be crucial for accurate MA{\\&}D, and vice versa. In this work we empirically confirm this hypothesis for Modern Hebrew, an MRL with complex morphology and severe word-level ambiguity, in a novel transition-based framework. Specifically, we propose a joint morphosyntactic transition-based framework which formally unifies two distinct transition systems, morphological and syntactic, into a single transition-based system with joint training and joint inference. We empirically show that MA{\\&}D results obtained in the joint settings outperform MA{\\&}D results obtained by the respective standalone components, and that end-to-end parsing results obtained by our joint system present a new state of the art for Hebrew dependency parsing.","pages":"33--48","doi":"10.1162\/tacl_a_00253","url":"https:\/\/www.aclweb.org\/anthology\/Q19-1003","year":"2019","month":"March","volume":"7","journal":"Transactions of the Association for Computational Linguistics"},{"id":"Q19-1004","title":"Analysis Methods in Neural Language Processing: A Survey","authors":["Belinkov, Yonatan","Glass, James"],"emails":["belinkov@mit.edu","glass@mit.edu"],"author_id":["yonatan-belinkov","james-glass"],"abstract":"The field of natural language processing has seen impressive progress in recent years, with neural network models replacing many of the traditional systems. A plethora of new models have been proposed, many of which are thought to be opaque compared to their feature-rich counterparts. This has led researchers to analyze, interpret, and evaluate neural networks in novel and more fine-grained ways. In this survey paper, we review analysis methods in neural language processing, categorize them according to prominent research trends, highlight existing limitations, and point to potential directions for future work.","pages":"49--72","doi":"10.1162\/tacl_a_00254","url":"https:\/\/www.aclweb.org\/anthology\/Q19-1004","year":"2019","month":"March","volume":"7","journal":"Transactions of the Association for Computational Linguistics"},{"id":"Q19-1005","title":"Unlexicalized Transition-based Discontinuous Constituency Parsing","authors":["Coavoux, Maximin","Crabb{\\'e}, Beno{\\^\\i}t","Cohen, Shay B."],"emails":["mcoavoux@inf.ed.ac.uk","bcrabbe@linguist.univ-paris-diderot.fr","scohen@inf.ed.ac.uk"],"author_id":["maximin-coavoux","benoit-crabbe","shay-b-cohen"],"abstract":"Lexicalized parsing models are based on the assumptions that (i) constituents are organized around a lexical head and (ii) bilexical statistics are crucial to solve ambiguities. In this paper, we introduce an unlexicalized transition-based parser for discontinuous constituency structures, based on a structure-label transition system and a bi-LSTM scoring system. We compare it with lexicalized parsing models in order to address the question of lexicalization in the context of discontinuous constituency parsing. Our experiments show that unlexicalized models systematically achieve higher results than lexicalized models, and provide additional empirical evidence that lexicalization is not necessary to achieve strong parsing results. Our best unlexicalized model sets a new state of the art on English and German discontinuous constituency treebanks. We further provide a per-phenomenon analysis of its errors on discontinuous constituents.","pages":"73--89","doi":"10.1162\/tacl_a_00255","url":"https:\/\/www.aclweb.org\/anthology\/Q19-1005","year":"2019","month":"March","volume":"7","journal":"Transactions of the Association for Computational Linguistics"},{"id":"Q19-1006","title":"Synchronous Bidirectional Neural Machine Translation","authors":["Zhou, Long","Zhang, Jiajun","Zong, Chengqing"],"emails":["long.zhou@nlpr.ia.ac.cn","jjzhang@nlpr.ia.ac.cn","cqzong@nlpr.ia.ac.cn"],"author_id":["long-zhou","jiajun-zhang","chengqing-zong"],"abstract":"Existing approaches to neural machine translation (NMT) generate the target language sequence token-by-token from left to right. However, this kind of unidirectional decoding framework cannot make full use of the target-side future contexts which can be produced in a right-to-left decoding direction, and thus suffers from the issue of unbalanced outputs. In this paper, we introduce a synchronous bidirectional{--}neural machine translation (SB-NMT) that predicts its outputs using left-to-right and right-to-left decoding simultaneously and interactively, in order to leverage both of the history and future information at the same time. Specifically, we first propose a new algorithm that enables synchronous bidirectional decoding in a single model. Then, we present an interactive decoding model in which left-to-right (right-to-left) generation does not only depend on its previously generated outputs, but also relies on future contexts predicted by right-to-left (left-to-right) decoding. We extensively evaluate the proposed SB-NMT model on large-scale NIST Chinese{--}English, WMT14 English{--}German, and WMT18 Russian{--}English translation tasks. Experimental results demonstrate that our model achieves significant improvements over the strong Transformer model by 3.92, 1.49, and 1.04 BLEU points, respectively, and obtains the state-of-the-art performance on Chinese{--}English and English{--}German translation tasks.","pages":"91--105","doi":"10.1162\/tacl_a_00256","url":"https:\/\/www.aclweb.org\/anthology\/Q19-1006","year":"2019","month":"March","volume":"7","journal":"Transactions of the Association for Computational Linguistics"},{"id":"Q19-1007","title":"Learning Multilingual Word Embeddings in Latent Metric Space: A Geometric Approach","authors":["Jawanpuria, Pratik","Balgovind, Arjun","Kunchukuttan, Anoop","Mishra, Bamdev"],"emails":["pratik.jawanpuria@microsoft.com","2barjun@cse.iitm.ac.in","ankunchu@microsoft.com","bamdevm@microsoft.com"],"author_id":["pratik-jawanpuria","arjun-balgovind","anoop-kunchukuttan","bamdev-mishra"],"abstract":"We propose a novel geometric approach for learning bilingual mappings given monolingual embeddings and a bilingual dictionary. Our approach decouples the source-to-target language transformation into (a) language-specific rotations on the original embeddings to align them in a common, latent space, and (b) a language-independent similarity metric in this common space to better model the similarity between the embeddings. Overall, we pose the bilingual mapping problem as a classification problem on smooth Riemannian manifolds. Empirically, our approach outperforms previous approaches on the bilingual lexicon induction and cross-lingual word similarity tasks. We next generalize our framework to represent multiple languages in a common latent space. Language-specific rotations for all the languages and a common similarity metric in the latent space are learned jointly from bilingual dictionaries for multiple language pairs. We illustrate the effectiveness of joint learning for multiple languages in an indirect word translation setting.","pages":"107--120","doi":"10.1162\/tacl_a_00257","url":"https:\/\/www.aclweb.org\/anthology\/Q19-1007","year":"2019","month":"March","volume":"7","journal":"Transactions of the Association for Computational Linguistics"},{"id":"Q19-1008","title":"Rotational Unit of Memory: A Novel Representation Unit for {RNN}s with Scalable Applications","authors":["Dangovski, Rumen","Jing, Li","Nakov, Preslav","Tatalovi{\\'c}, Mi{\\'c}o","Solja{\\v{c}}i{\\'c}, Marin"],"emails":["rumenrd@mit.edu","ljing@mit.edu","pnakov@qf.org.qa","mico@mit.edu","soljacic@mit.edu"],"author_id":["rumen-dangovski","li-jing","preslav-nakov","mico-tatalovic","marin-soljacic"],"abstract":"Stacking long short-term memory (LSTM) cells or gated recurrent units (GRUs) as part of a recurrent neural network (RNN) has become a standard approach to solving a number of tasks ranging from language modeling to text summarization. Although LSTMs and GRUs were designed to model long-range dependencies more accurately than conventional RNNs, they nevertheless have problems copying or recalling information from the long distant past. Here, we derive a phase-coded representation of the memory state, Rotational Unit of Memory (RUM), that unifies the concepts of unitary learning and associative memory. We show experimentally that RNNs based on RUMs can solve basic sequential tasks such as memory copying and memory recall much better than LSTMs\/GRUs. We further demonstrate that by replacing LSTM\/GRU with RUM units we can apply neural networks to real-world problems such as language modeling and text summarization, yielding results comparable to the state of the art.","pages":"121--138","doi":"10.1162\/tacl_a_00258","url":"https:\/\/www.aclweb.org\/anthology\/Q19-1008","year":"2019","month":"March","volume":"7","journal":"Transactions of the Association for Computational Linguistics"},{"id":"Q19-1009","title":"{GILE}: A Generalized Input-Label Embedding for Text Classification","authors":["Pappas, Nikolaos","Henderson, James"],"emails":["nikolaos.pappas@idiap.ch","james.henderson@idiap.ch"],"author_id":["nikolaos-pappas","james-henderson"],"abstract":"Neural text classification models typically treat output labels as categorical variables that lack description and semantics. This forces their parametrization to be dependent on the label set size, and, hence, they are unable to scale to large label sets and generalize to unseen ones. Existing joint input-label text models overcome these issues by exploiting label descriptions, but they are unable to capture complex label relationships, have rigid parametrization, and their gains on unseen labels happen often at the expense of weak performance on the labels seen during training. In this paper, we propose a new input-label model that generalizes over previous such models, addresses their limitations, and does not compromise performance on seen labels. The model consists of a joint nonlinear input-label embedding with controllable capacity and a joint-space-dependent classification unit that is trained with cross-entropy loss to optimize classification performance. We evaluate models on full-resource and low- or zero-resource text classification of multilingual news and biomedical text with a large label set. Our model outperforms monolingual and multilingual models that do not leverage label semantics and previous joint input-label space models in both scenarios.","pages":"139--155","doi":"10.1162\/tacl_a_00259","url":"https:\/\/www.aclweb.org\/anthology\/Q19-1009","year":"2019","month":"March","volume":"7","journal":"Transactions of the Association for Computational Linguistics"},{"id":"Q19-1010","title":"Autosegmental Input Strictly Local Functions","authors":["Chandlee, Jane","Jardine, Adam"],"emails":["jchandlee@haverford.edu","adam.jardine@rutgers.edu"],"author_id":["jane-chandlee","adam-jardine"],"abstract":"Autosegmental representations (ARs; Goldsmith, 1976) are claimed to enable local analyses of otherwise non-local phenomena Odden (1994). Focusing on the domain of tone, we investigate this ability of ARs using a computationally well-defined notion of locality extended from Chandlee (2014). The result is a more nuanced understanding of the way in which ARs interact with phonological locality.","pages":"157--168","doi":"10.1162\/tacl_a_00260","url":"https:\/\/www.aclweb.org\/anthology\/Q19-1010","year":"2019","month":"March","volume":"7","journal":"Transactions of the Association for Computational Linguistics"},{"id":"Q19-1011","title":"{SECTOR}: A Neural Model for Coherent Topic Segmentation and Classification","authors":["Arnold, Sebastian","Schneider, Rudolf","Cudr{\\'e}-Mauroux, Philippe","Gers, Felix A.","L{\\\"o}ser, Alexander"],"emails":["sarnold@beuth-hochschule.de","ruschneider@beuth-hochschule.de","pcm@unifr.ch","gers@beuth-hochschule.de","aloeser@beuth-hochschule.de"],"author_id":["sebastian-arnold","rudolf-schneider","philippe-cudre-mauroux","felix-a-gers","alexander-loser"],"abstract":"When searching for information, a human reader first glances over a document, spots relevant sections, and then focuses on a few sentences for resolving her intention. However, the high variance of document structure complicates the identification of the salient topic of a given section at a glance. To tackle this challenge, we present SECTOR, a model to support machine reading systems by segmenting documents into coherent sections and assigning topic labels to each section. Our deep neural network architecture learns a latent topic embedding over the course of a document. This can be leveraged to classify local topics from plain text and segment a document at topic shifts. In addition, we contribute WikiSection, a publicly available data set with 242k labeled sections in English and German from two distinct domains: diseases and cities. From our extensive evaluation of 20 architectures, we report a highest score of 71.6{\\%} F1 for the segmentation and classification of 30 topics from the English city domain, scored by our SECTOR long short-term memory model with Bloom filter embeddings and bidirectional segmentation. This is a significant improvement of 29.5 points F1 over state-of-the-art CNN classifiers with baseline segmentation.","pages":"169--184","doi":"10.1162\/tacl_a_00261","url":"https:\/\/www.aclweb.org\/anthology\/Q19-1011","year":"2019","month":"March","volume":"7","journal":"Transactions of the Association for Computational Linguistics"},{"id":"Q19-1012","title":"Complex Program Induction for Querying Knowledge Bases in the Absence of Gold Programs","authors":["Saha, Amrita","Ansari, Ghulam Ahmed","Laddha, Abhishek","Sankaranarayanan, Karthik","Chakrabarti, Soumen"],"emails":["amrsaha4@in.ibm.com","ansarigh@in.ibm.com","laddhaabhishek11@gmail.com","kartsank@in.ibm.com","soumen@cse.iitb.ac.in"],"author_id":["amrita-saha","ghulam-ahmed-ansari","abhishek-laddha","karthik-sankaranarayanan","soumen-chakrabarti"],"abstract":"Recent years have seen increasingly complex question-answering on knowledge bases (KBQA) involving logical, quantitative, and comparative reasoning over KB subgraphs. Neural Program Induction (NPI) is a pragmatic approach toward modularizing the reasoning process by translating a complex natural language query into a multi-step executable program. While NPI has been commonly trained with the {`}{`}gold{'}{'} program or its sketch, for realistic KBQA applications such gold programs are expensive to obtain. There, practically only natural language queries and the corresponding answers can be provided for training. The resulting combinatorial explosion in program space, along with extremely sparse rewards, makes NPI for KBQA ambitious and challenging. We present Complex Imperative Program Induction from Terminal Rewards (CIPITR), an advanced neural programmer that mitigates reward sparsity with auxiliary rewards, and restricts the program space to semantically correct programs using high-level constraints, KB schema, and inferred answer type. CIPITR solves complex KBQA considerably more accurately than key-value memory networks and neural symbolic machines (NSM). For moderately complex queries requiring 2- to 5-step programs, CIPITR scores at least 3{\\mbox{$\\times$}} higher F1 than the competing systems. On one of the hardest class of programs (comparative reasoning) with 5{--}10 steps, CIPITR outperforms NSM by a factor of 89 and memory networks by 9 times.","pages":"185--200","doi":"10.1162\/tacl_a_00262","url":"https:\/\/www.aclweb.org\/anthology\/Q19-1012","year":"2019","month":"March","volume":"7","journal":"Transactions of the Association for Computational Linguistics"},{"id":"Q19-1013","title":"Categorical Metadata Representation for Customized Text Classification","authors":["Kim, Jihyeok","Amplayo, Reinald Kim","Lee, Kyungjae","Sung, Sua","Seo, Minji","Hwang, Seung-won"],"emails":["reinald.kim@ed.ac.uk","zizi1532@yonsei.ac.kr","lkj0509@yonsei.ac.kr","dormouse@yonsei.ac.kr","ggatalminji@yonsei.ac.kr","seungwonh@yonsei.ac.kr"],"author_id":["jihyeok-kim","reinald-kim-amplayo","kyungjae-lee","sua-sung","minji-seo","seung-won-hwang"],"abstract":"The performance of text classification has improved tremendously using intelligently engineered neural-based models, especially those injecting categorical metadata as additional information, e.g., using user\/product information for sentiment classification. This information has been used to modify parts of the model (e.g., word embeddings, attention mechanisms) such that results can be customized according to the metadata. We observe that current representation methods for categorical metadata, which are devised for human consumption, are not as effective as claimed in popular classification methods, outperformed even by simple concatenation of categorical features in the final layer of the sentence encoder. We conjecture that categorical features are harder to represent for machine use, as available context only indirectly describes the category, and even such context is often scarce (for tail category). To this end, we propose using basis vectors to effectively incorporate categorical metadata on various parts of a neural-based model. This additionally decreases the number of parameters dramatically, especially when the number of categorical features is large. Extensive experiments on various data sets with different properties are performed and show that through our method, we can represent categorical metadata more effectively to customize parts of the model, including unexplored ones, and increase the performance of the model greatly.","pages":"201--215","doi":"10.1162\/tacl_a_00263","url":"https:\/\/www.aclweb.org\/anthology\/Q19-1013","year":"2019","month":"March","volume":"7","journal":"Transactions of the Association for Computational Linguistics"},{"id":"Q19-1014","title":"{DREAM}: A Challenge Data Set and Models for Dialogue-Based Reading Comprehension","authors":["Sun, Kai","Yu, Dian","Chen, Jianshu","Yu, Dong","Choi, Yejin","Cardie, Claire"],"emails":["ks985@cornell.edu","yudian@tencent.com","jianshuchen@tencent.com","dyu@tencent.com","yejin@cs.washington.edu","cardie@cs.cornell.edu"],"author_id":["kai-sun","dian-yu","jianshu-chen","dong-yu","yejin-choi","claire-cardie"],"abstract":"We present DREAM, the first dialogue-based multiple-choice reading comprehension data set. Collected from English as a Foreign Language examinations designed by human experts to evaluate the comprehension level of Chinese learners of English, our data set contains 10,197 multiple-choice questions for 6,444 dialogues. In contrast to existing reading comprehension data sets, DREAM is the first to focus on in-depth multi-turn multi-party dialogue understanding. DREAM is likely to present significant challenges for existing reading comprehension systems: 84{\\%} of answers are non-extractive, 85{\\%} of questions require reasoning beyond a single sentence, and 34{\\%} of questions also involve commonsense knowledge. We apply several popular neural reading comprehension models that primarily exploit surface information within the text and find them to, at best, just barely outperform a rule-based approach. We next investigate the effects of incorporating dialogue structure and different kinds of general world knowledge into both rule-based and (neural and non-neural) machine learning-based reading comprehension models. Experimental results on the DREAM data set show the effectiveness of dialogue structure and general world knowledge. DREAM is available at https:\/\/dataset.org\/dream\/.","pages":"217--231","doi":"10.1162\/tacl_a_00264","url":"https:\/\/www.aclweb.org\/anthology\/Q19-1014","year":"2019","month":"March","volume":"7","journal":"Transactions of the Association for Computational Linguistics"},{"id":"Q19-1015","title":"Learning Neural Sequence-to-Sequence Models from Weak Feedback with Bipolar Ramp Loss","authors":["Jehl, Laura","Lawrence, Carolin","Riezler, Stefan"],"emails":["jehl@cl.uni-heidelberg.de","lawrence@cl.uni-heidelberg.de","riezler@cl.uni-heidelberg.de"],"author_id":["laura-jehl","carolin-lawrence","stefan-riezler"],"abstract":"In many machine learning scenarios, supervision by gold labels is not available and conse quently neural models cannot be trained directly by maximum likelihood estimation. In a weak supervision scenario, metric-augmented objectives can be employed to assign feedback to model outputs, which can be used to extract a supervision signal for training. We present several objectives for two separate weakly supervised tasks, machine translation and semantic parsing. We show that objectives should actively discourage negative outputs in addition to promoting a surrogate gold structure. This notion of bipolarity is naturally present in ramp loss objectives, which we adapt to neural models. We show that bipolar ramp loss objectives outperform other non-bipolar ramp loss objectives and minimum risk training on both weakly supervised tasks, as well as on a supervised machine translation task. Additionally, we introduce a novel token-level ramp loss objective, which is able to outperform even the best sequence-level ramp loss on both weakly supervised tasks.","pages":"233--248","doi":"10.1162\/tacl_a_00265","url":"https:\/\/www.aclweb.org\/anthology\/Q19-1015","year":"2019","month":"March","volume":"7","journal":"Transactions of the Association for Computational Linguistics"},{"id":"Q19-1016","title":"{C}o{QA}: A Conversational Question Answering Challenge","authors":["Reddy, Siva","Chen, Danqi","Manning, Christopher D."],"emails":["sivar@cs.stanford.edu","danqi@cs.stanford.edu","manning@cs.stanford.edu"],"author_id":["siva-reddy","danqi-chen","christopher-d-manning"],"abstract":"Humans gather information through conversations involving a series of interconnected questions and answers. For machines to assist in information gathering, it is therefore essential to enable them to answer conversational questions. We introduce CoQA, a novel dataset for building Conversational Question Answering systems. Our dataset contains 127k questions with answers, obtained from 8k conversations about text passages from seven diverse domains. The questions are conversational, and the answers are free-form text with their corresponding evidence highlighted in the passage. We analyze CoQA in depth and show that conversational questions have challenging phenomena not present in existing reading comprehension datasets (e.g., coreference and pragmatic reasoning). We evaluate strong dialogue and reading comprehension models on CoQA. The best system obtains an F1 score of 65.4{\\%}, which is 23.4 points behind human performance (88.8{\\%}), indicating that there is ample room for improvement. We present CoQA as a challenge to the community at https:\/\/stanfordnlp.github.io\/coqa.","pages":"249--266","doi":"10.1162\/tacl_a_00266","url":"https:\/\/www.aclweb.org\/anthology\/Q19-1016","year":"2019","month":"March","volume":"7","journal":"Transactions of the Association for Computational Linguistics"},{"id":"Q19-1017","title":"What You Say and How You Say it: Joint Modeling of Topics and Discourse in Microblog Conversations","authors":["Zeng, Jichuan","Li, Jing","He, Yulan","Gao, Cuiyun","Lyu, Michael R.","King, Irwin"],"emails":["jczeng@cse.cuhk.edu.hk","2ameliajli@tencent.com","3yulan.he@warwick.ac.uk","cygao@cse.cuhk.edu.hk","lyu@cse.cuhk.edu.hk","king@cse.cuhk.edu.hk"],"author_id":["jichuan-zeng","jing-li","yulan-he","cuiyun-gao","michael-r-lyu","irwin-king"],"abstract":"This paper presents an unsupervised framework for jointly modeling topic content and discourse behavior in microblog conversations. Concretely, we propose a neural model to discover word clusters indicating what a conversation concerns (i.e., topics) and those reflecting how participants voice their opinions (i.e., discourse).1 Extensive experiments show that our model can yield both coherent topics and meaningful discourse behavior. Further study shows that our topic and discourse representations can benefit the classification of microblog messages, especially when they are jointly trained with the classifier.Our data sets and code are available at: http:\/\/github.com\/zengjichuan\/Topic{\\_}Disc.","pages":"267--281","doi":"10.1162\/tacl_a_00267","url":"https:\/\/www.aclweb.org\/anthology\/Q19-1017","year":"2019","month":"March","volume":"7","journal":"Transactions of the Association for Computational Linguistics"},{"id":"Q19-1018","title":"Calculating the Optimal Step in Shift-Reduce Dependency Parsing: From Cubic to Linear Time","authors":["Nederhof, Mark-Jan"],"emails":["markjan.nederhof@googlemail.com"],"author_id":["mark-jan-nederhof"],"abstract":"We present a new cubic-time algorithm to calculate the optimal next step in shift-reduce dependency parsing, relative to ground truth, commonly referred to as dynamic oracle. Unlike existing algorithms, it is applicable if the training corpus contains non-projective structures. We then show that for a projective training corpus, the time complexity can be improved from cubic to linear.","pages":"283--296","doi":"10.1162\/tacl_a_00268","url":"https:\/\/www.aclweb.org\/anthology\/Q19-1018","year":"2019","month":"March","volume":"7","journal":"Transactions of the Association for Computational Linguistics"},{"id":"Q19-1019","title":"Densely Connected Graph Convolutional Networks for Graph-to-Sequence Learning","authors":["Guo, Zhijiang","Zhang, Yan","Teng, Zhiyang","Lu, Wei"],"emails":["","tengzhiyang@westlake.edu.cn","teng@mymail.sutd.edu.sg","luwei@sutd.edu.sg"],"author_id":["zhijiang-guo","yan-zhang","zhiyang-teng","wei-lu"],"abstract":"We focus on graph-to-sequence learning, which can be framed as transducing graph structures to sequences for text generation. To capture structural information associated with graphs, we investigate the problem of encoding graphs using graph convolutional networks (GCNs). Unlike various existing approaches where shallow architectures were used for capturing local structural information only, we introduce a dense connection strategy, proposing a novel Densely Connected Graph Convolutional Network (DCGCN). Such a deep architecture is able to integrate both local and non-local features to learn a better structural representation of a graph. Our model outperforms the state-of-the-art neural models significantly on AMR-to-text generation and syntax-based neural machine translation.","pages":"297--312","doi":"10.1162\/tacl_a_00269","url":"https:\/\/www.aclweb.org\/anthology\/Q19-1019","year":"2019","month":"March","volume":"7","journal":"Transactions of the Association for Computational Linguistics"},{"id":"Q19-1020","title":"Attention-Passing Models for Robust and Data-Efficient End-to-End Speech Translation","authors":["Sperber, Matthias","Neubig, Graham","Niehues, Jan","Waibel, Alex"],"emails":["last@kit.edu","gneubig@cs.cmu.edu","",""],"author_id":["matthias-sperber","graham-neubig","jan-niehues","alex-waibel"],"abstract":"Speech translation has traditionally been approached through cascaded models consisting of a speech recognizer trained on a corpus of transcribed speech, and a machine translation system trained on parallel texts. Several recent works have shown the feasibility of collapsing the cascade into a single, direct model that can be trained in an end-to-end fashion on a corpus of translated speech. However, experiments are inconclusive on whether the cascade or the direct model is stronger, and have only been conducted under the unrealistic assumption that both are trained on equal amounts of data, ignoring other available speech recognition and machine translation corpora. In this paper, we demonstrate that direct speech translation models require more data to perform well than cascaded models, and although they allow including auxiliary data through multi-task training, they are poor at exploiting such data, putting them at a severe disadvantage. As a remedy, we propose the use of end- to-end trainable models with two attention mechanisms, the first establishing source speech to source text alignments, the second modeling source to target text alignment. We show that such models naturally decompose into multi-task{--}trainable recognition and translation tasks and propose an attention-passing technique that alleviates error propagation issues in a previous formulation of a model with two attention stages. Our proposed model outperforms all examined baselines and is able to exploit auxiliary training data much more effectively than direct attentional models.","pages":"313--325","doi":"10.1162\/tacl_a_00270","url":"https:\/\/www.aclweb.org\/anthology\/Q19-1020","year":"2019","month":"March","volume":"7","journal":"Transactions of the Association for Computational Linguistics"},{"id":"Q19-1021","title":"On the Complexity and Typology of Inflectional Morphological Systems","authors":["Cotterell, Ryan","Kirov, Christo","Hulden, Mans","Eisner, Jason"],"emails":["ryan.cotterell@jhu.edu","ckirov1@jhu.edu","mans.hulden@colorado.edu","eisner@jhu.edu"],"author_id":["ryan-cotterell","christo-kirov","mans-hulden","jason-eisner"],"abstract":"We quantify the linguistic complexity of different languages{'} morphological systems. We verify that there is a statistically significant empirical trade-off between paradigm size and irregularity: A language{'}s inflectional paradigms may be either large in size or highly irregular, but never both. We define a new measure of paradigm irregularity based on the conditional entropy of the surface realization of a paradigm{---} how hard it is to jointly predict all the word forms in a paradigm from the lemma. We estimate irregularity by training a predictive model. Our measurements are taken on large morphological paradigms from 36 typologically diverse languages.","pages":"327--342","doi":"10.1162\/tacl_a_00271","url":"https:\/\/www.aclweb.org\/anthology\/Q19-1021","year":"2019","month":"March","volume":"7","journal":"Transactions of the Association for Computational Linguistics"},{"id":"Q19-1022","title":"Syntax-aware Semantic Role Labeling without Parsing","authors":["Cai, Rui","Lapata, Mirella"],"emails":["ai@ed.ac.uk","mlap@inf.ed.ac.uk"],"author_id":["rui-cai","mirella-lapata"],"abstract":"In this paper we focus on learning dependency aware representations for semantic role labeling without recourse to an external parser. The backbone of our model is an LSTM-based semantic role labeler jointly trained with two auxiliary tasks: predicting the dependency label of a word and whether there exists an arc linking it to the predicate. The auxiliary tasks provide syntactic information that is specific to semantic role labeling and are learned from training data (dependency annotations) without relying on existing dependency parsers, which can be noisy (e.g., on out-of-domain data or infrequent constructions). Experimental results on the CoNLL-2009 benchmark dataset show that our model outperforms the state of the art in English, and consistently improves performance in other languages, including Chinese, German, and Spanish.","pages":"343--356","doi":"10.1162\/tacl_a_00272","url":"https:\/\/www.aclweb.org\/anthology\/Q19-1022","year":"2019","month":"March","volume":"7","journal":"Transactions of the Association for Computational Linguistics"},{"id":"Q19-1023","title":"A Generative Model for Punctuation in Dependency Trees","authors":["Li, Xiang Lisa","Wang, Dingquan","Eisner, Jason"],"emails":["xli150@jhu.edu","wdd@cs.jhu.edu","jason@cs.jhu.edu"],"author_id":["xiang-lisa-li","dingquan-wang","jason-eisner"],"abstract":"Treebanks traditionally treat punctuation marks as ordinary words, but linguists have suggested that a tree{'}s {``}true{''} punctuation marks are not observed (Nunberg, 1990). These latent {``}underlying{''} marks serve to delimit or separate constituents in the syntax tree. When the tree{'}s yield is rendered as a written sentence, a string rewriting mechanism transduces the underlying marks into {``}surface{''} marks, which are part of the observed (surface) string but should not be regarded as part of the tree. We formalize this idea in a generative model of punctuation that admits efficient dynamic programming. We train it without observing the underlying marks, by locally maximizing the incomplete data likelihood (similarly to the EM algorithm). When we use the trained model to reconstruct the tree{'}s underlying punctuation, the results appear plausible across 5 languages, and in particular are consistent with Nunberg{'}s analysis of English. We show that our generative model can be used to beat baselines on punctuation restoration. Also, our reconstruction of a sentence{'}s underlying punctuation lets us appropriately render the surface punctuation (via our trained underlying-to-surface mechanism) when we syntactically transform the sentence.","pages":"357--373","doi":"10.1162\/tacl_a_00273","url":"https:\/\/www.aclweb.org\/anthology\/Q19-1023","year":"2019","month":"March","volume":"7","journal":"Transactions of the Association for Computational Linguistics"},{"id":"Q19-1024","title":"Learning End-to-End Goal-Oriented Dialog with Maximal User Task Success and Minimal Human Agent Use","authors":["Rajendran, Janarthanan","Ganhotra, Jatin","Polymenakos, Lazaros C."],"emails":["rjana@umich.edu","jatinganhotra@us.ibm.com","lcpolyme@us.ibm.com"],"author_id":["janarthanan-rajendran","jatin-ganhotra","lazaros-c-polymenakos"],"abstract":"Neural end-to-end goal-oriented dialog systems showed promise to reduce the workload of human agents for customer service, as well as reduce wait time for users. However, their inability to handle new user behavior at deployment has limited their usage in real world. In this work, we propose an end-to-end trainable method for neural goal-oriented dialog systems that handles new user behaviors at deployment by transferring the dialog to a human agent intelligently. The proposed method has three goals: 1) maximize user{'}s task success by transferring to human agents, 2) minimize the load on the human agents by transferring to them only when it is essential, and 3) learn online from the human agent{'}s responses to reduce human agents{'} load further. We evaluate our proposed method on a modified-bAbI dialog task, which simulates the scenario of new user behaviors occurring at test time. Experimental results show that our proposed method is effective in achieving the desired goals.","pages":"375--386","doi":"10.1162\/tacl_a_00274","url":"https:\/\/www.aclweb.org\/anthology\/Q19-1024","year":"2019","month":"March","volume":"7","journal":"Transactions of the Association for Computational Linguistics"},{"id":"Q19-1025","title":"No Word is an Island{---}A Transformation Weighting Model for Semantic Composition","authors":["Dima, Corina","de Kok, Dani{\\\"e}l","Witte, Neele","Hinrichs, Erhard"],"emails":["corina.dima@uni-tuebingen.de","daniel.de-kok@uni-tuebingen.de","neele.witte@uni-tuebingen.de","erhard.hinrichs@uni-tuebingen.de"],"author_id":["corina-dima","daniel-de-kok","neele-witte","erhard-hinrichs"],"abstract":"Composition models of distributional semantics are used to construct phrase representations from the representations of their words. Composition models are typically situated on two ends of a spectrum. They either have a small number of parameters but compose all phrases in the same way, or they perform word-specific compositions at the cost of a far larger number of parameters. In this paper we propose transformation weighting (TransWeight), a composition model that consistently outperforms existing models on nominal compounds, adjective-noun phrases, and adverb-adjective phrases in English, German, and Dutch. TransWeight drastically reduces the number of parameters needed compared with the best model in the literature by composing similar words in the same way.","pages":"437--451","doi":"10.1162\/tacl_a_00275","url":"https:\/\/www.aclweb.org\/anthology\/Q19-1025","year":"2019","month":"March","volume":"7","journal":"Transactions of the Association for Computational Linguistics"},{"id":"Q19-1026","title":"Natural Questions: A Benchmark for Question Answering Research","authors":["Kwiatkowski, Tom","Palomaki, Jennimaria","Redfield, Olivia","Collins, Michael","Parikh, Ankur","Alberti, Chris","Epstein, Danielle","Polosukhin, Illia","Devlin, Jacob","Lee, Kenton","Toutanova, Kristina","Jones, Llion","Kelcey, Matthew","Chang, Ming-Wei","Dai, Andrew M.","Uszkoreit, Jakob","Le, Quoc","Petrov, Slav"],"emails":["","","","","","","natural-questions@google.com","","","","","","","","","","",""],"author_id":["tom-kwiatkowski","jennimaria-palomaki","olivia-redfield","michael-collins","ankur-parikh","chris-alberti","danielle-epstein","illia-polosukhin","jacob-devlin","kenton-lee","kristina-toutanova","llion-jones","matthew-kelcey","ming-wei-chang","andrew-m-dai","jakob-uszkoreit","quoc-le","slav-petrov"],"abstract":"We present the Natural Questions corpus, a question answering data set. Questions consist of real anonymized, aggregated queries issued to the Google search engine. An annotator is presented with a question along with a Wikipedia page from the top 5 search results, and annotates a long answer (typically a paragraph) and a short answer (one or more entities) if present on the page, or marks null if no long\/short answer is present. The public release consists of 307,373 training examples with single annotations; 7,830 examples with 5-way annotations for development data; and a further 7,842 examples with 5-way annotated sequestered as test data. We present experiments validating quality of the data. We also describe analysis of 25-way annotations on 302 examples, giving insights into human variability on the annotation task. We introduce robust metrics for the purposes of evaluating question answering systems; demonstrate high human upper bounds on these metrics; and establish baseline results using competitive methods drawn from related literature.","pages":"453--466","doi":"10.1162\/tacl_a_00276","url":"https:\/\/www.aclweb.org\/anthology\/Q19-1026","year":"2019","month":"March","volume":"7","journal":"Transactions of the Association for Computational Linguistics"},{"id":"Q19-1027","title":"Still a Pain in the Neck: Evaluating Text Representations on Lexical Composition","authors":["Shwartz, Vered","Dagan, Ido"],"emails":["vered1986@gmail.com","dagan@cs.biu.ac.il"],"author_id":["vered-shwartz","ido-dagan"],"abstract":"Building meaningful phrase representations is challenging because phrase meanings are not simply the sum of their constituent meanings. Lexical composition can shift the meanings of the constituent words and introduce implicit information. We tested a broad range of textual representations for their capacity to address these issues. We found that, as expected, contextualized word representations perform better than static word embeddings, more so on detecting meaning shift than in recovering implicit information, in which their performance is still far from that of humans. Our evaluation suite, consisting of six tasks related to lexical composition effects, can serve future research aiming to improve representations.","pages":"403--419","doi":"10.1162\/tacl_a_00277","url":"https:\/\/www.aclweb.org\/anthology\/Q19-1027","year":"2019","month":"March","volume":"7","journal":"Transactions of the Association for Computational Linguistics"},{"id":"Q19-1028","title":"Multiattentive Recurrent Neural Network Architecture for Multilingual Readability Assessment","authors":["Azpiazu, Ion Madrazo","Pera, Maria Soledad"],"emails":["ionmadrazo@boisestate.edu","solepera@boisestate.edu"],"author_id":["ion-madrazo-azpiazu","maria-soledad-pera"],"abstract":"We present a multiattentive recurrent neural network architecture for automatic multilingual readability assessment. This architecture considers raw words as its main input, but internally captures text structure and informs its word attention process using other syntax- and morphology-related datapoints, known to be of great importance to readability. This is achieved by a multiattentive strategy that allows the neural network to focus on specific parts of a text for predicting its reading level. We conducted an exhaustive evaluation using data sets targeting multiple languages and prediction task types, to compare the proposed model with traditional, state-of-the-art, and other neural network strategies.","pages":"421--436","doi":"10.1162\/tacl_a_00278","url":"https:\/\/www.aclweb.org\/anthology\/Q19-1028","year":"2019","month":"March","volume":"7","journal":"Transactions of the Association for Computational Linguistics"},{"id":"Q19-1029","title":"Trick Me If You Can: Human-in-the-Loop Generation of Adversarial Examples for Question Answering","authors":["Wallace, Eric","Rodriguez, Pedro","Feng, Shi","Yamada, Ikuya","Boyd-Graber, Jordan"],"emails":["ewallac2@umiacs.umd.edu","pedro@cs.umd.edu","shifeng@cs.umd.edu","ikuya@ousia.jp","jbg@umiacs.umd.edu"],"author_id":["eric-wallace","pedro-rodriguez","shi-feng","ikuya-yamada","jordan-boyd-graber"],"abstract":"Adversarial evaluation stress-tests a model{'}s understanding of natural language. Because past approaches expose superficial patterns, the resulting adversarial examples are limited in complexity and diversity. We propose human- in-the-loop adversarial generation, where human authors are guided to break models. We aid the authors with interpretations of model predictions through an interactive user interface. We apply this generation framework to a question answering task called Quizbowl, where trivia enthusiasts craft adversarial questions. The resulting questions are validated via live human{--}computer matches: Although the questions appear ordinary to humans, they systematically stump neural and information retrieval models. The adversarial questions cover diverse phenomena from multi-hop reasoning to entity type distractors, exposing open challenges in robust question answering.","pages":"387--401","doi":"10.1162\/tacl_a_00279","url":"https:\/\/www.aclweb.org\/anthology\/Q19-1029","year":"2019","month":"March","volume":"7","journal":"Transactions of the Association for Computational Linguistics"}]