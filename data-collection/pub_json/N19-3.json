[{"id":"N19-3001","title":"Is It Dish Washer Safe? Automatically Answering {``}Yes\/No{''} Questions Using Customer Reviews","authors":["Dzendzik, Daria","Vogel, Carl","Foster, Jennifer"],"emails":["daria.dzendzik@adaptcentre.ie","vogel@tcd.ie","jennifer.foster@adaptcentre.ie"],"author_id":["daria-dzendzik","carl-vogel","jennifer-foster"],"abstract":"It has become commonplace for people to share their opinions about all kinds of products by posting reviews online. It has also become commonplace for potential customers to do research about the quality and limitations of these products by posting questions online. We test the extent to which reviews are useful in question-answering by combining two Amazon datasets and focusing our attention on yes\/no questions. A manual analysis of 400 cases reveals that the reviews directly contain the answer to the question just over a third of the time. Preliminary reading comprehension experiments with this dataset prove inconclusive, with accuracy in the range 50-66{\\%}.","pages":"1--6","doi":"10.18653\/v1\/N19-3001","url":"https:\/\/www.aclweb.org\/anthology\/N19-3001","publisher":"Association for Computational Linguistics","address":"Minneapolis, Minnesota","year":"2019","month":"June","booktitle":"Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Student Research Workshop"},{"id":"N19-3002","title":"Identifying and Reducing Gender Bias in Word-Level Language Models","authors":["Bordia, Shikha","Bowman, Samuel R."],"emails":["sb6416@nyu.edu","bowman@nyu.edu"],"author_id":["shikha-bordia","samuel-bowman"],"abstract":"Many text corpora exhibit socially problematic biases, which can be propagated or amplified in the models trained on such data. For example, doctor cooccurs more frequently with male pronouns than female pronouns. In this study we (i) propose a metric to measure gender bias; (ii) measure bias in a text corpus and the text generated from a recurrent neural network language model trained on the text corpus; (iii) propose a regularization loss term for the language model that minimizes the projection of encoder-trained embeddings onto an embedding subspace that encodes gender; (iv) finally, evaluate efficacy of our proposed method on reducing gender bias. We find this regularization method to be effective in reducing gender bias up to an optimal weight assigned to the loss term, beyond which the model becomes unstable as the perplexity increases. We replicate this study on three training corpora{---}Penn Treebank, WikiText-2, and CNN\/Daily Mail{---}resulting in similar conclusions.","pages":"7--15","doi":"10.18653\/v1\/N19-3002","url":"https:\/\/www.aclweb.org\/anthology\/N19-3002","publisher":"Association for Computational Linguistics","address":"Minneapolis, Minnesota","year":"2019","month":"June","booktitle":"Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Student Research Workshop"},{"id":"N19-3003","title":"Emotion Impacts Speech Recognition Performance","authors":["Munot, Rushab","Nenkova, Ani"],"emails":["rushab@cis.upenn.edu","nenkova@cis.upenn.edu"],"author_id":["rushab-munot","ani-nenkova"],"abstract":"It has been established that the performance of speech recognition systems depends on multiple factors including the lexical content, speaker identity and dialect. Here we use three English datasets of acted emotion to demonstrate that emotional content also impacts the performance of commercial systems. On two of the corpora, emotion is a bigger contributor to recognition errors than speaker identity and on two, neutral speech is recognized considerably better than emotional speech. We further evaluate the commercial systems on spontaneous interactions that contain portions of emotional speech. We propose and validate on the acted datasets, a method that allows us to evaluate the overall impact of emotion on recognition even when manual transcripts are not available. Using this method, we show that emotion in natural spontaneous dialogue is a less prominent but still significant factor in recognition accuracy.","pages":"16--21","doi":"10.18653\/v1\/N19-3003","url":"https:\/\/www.aclweb.org\/anthology\/N19-3003","publisher":"Association for Computational Linguistics","address":"Minneapolis, Minnesota","year":"2019","month":"June","booktitle":"Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Student Research Workshop"},{"id":"N19-3004","title":"The Strength of the Weakest Supervision: Topic Classification Using Class Labels","authors":["Li, Jiatong","Zheng, Kai","Xu, Hua","Mei, Qiaozhu","Wang, Yue"],"emails":["1jiatong.li@rutgers.edu","2zhengkai@uci.edu","3hua.xu@uth.tmc.edu","4qmei@umich.edu","5wangyue@email.unc.edu"],"author_id":["jiatong-li","kai-zheng","hua-xu","qiaozhu-mei","yue-wang"],"abstract":"When developing topic classifiers for real-world applications, we begin by defining a set of meaningful topic labels. Ideally, an intelligent classifier can understand these labels right away and start classifying documents. Indeed, a human can confidently tell if an article is about science, politics, sports, or none of the above, after knowing just the class labels. We study the problem of training an initial topic classifier using only class labels. We investigate existing techniques for solving this problem and propose a simple but effective approach. Experiments on a variety of topic classification data sets show that learning from class labels can save significant initial labeling effort, essentially providing a {''}free{''} warm start to the topic classifier.","pages":"22--28","doi":"10.18653\/v1\/N19-3004","url":"https:\/\/www.aclweb.org\/anthology\/N19-3004","publisher":"Association for Computational Linguistics","address":"Minneapolis, Minnesota","year":"2019","month":"June","booktitle":"Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Student Research Workshop"},{"id":"N19-3005","title":"Handling Noisy Labels for Robustly Learning from Self-Training Data for Low-Resource Sequence Labeling","authors":["Paul, Debjit","Singh, Mittul","Hedderich, Michael A.","Klakow, Dietrich"],"emails":["paul@cl.uni-heidelberg.de","mittul.singh@aalto.fi","mhedderich@lsv.uni-saarland.de","dietrich.klakow@lsv.uni-saarland.de"],"author_id":["debjit-paul","mittul-singh","michael-a-hedderich","dietrich-klakow"],"abstract":"In this paper, we address the problem of effectively self-training neural networks in a low-resource setting. Self-training is frequently used to automatically increase the amount of training data. However, in a low-resource scenario, it is less effective due to unreliable annotations created using self-labeling of unlabeled data. We propose to combine self-training with noise handling on the self-labeled data. Directly estimating noise on the combined clean training set and self-labeled data can lead to corruption of the clean data and hence, performs worse. Thus, we propose the Clean and Noisy Label Neural Network which trains on clean and noisy self-labeled data simultaneously by explicitly modelling clean and noisy labels separately. In our experiments on Chunking and NER, this approach performs more robustly than the baselines. Complementary to this explicit approach, noise can also be handled implicitly with the help of an auxiliary learning task. To such a complementary approach, our method is more beneficial than other baseline methods and together provides the best performance overall.","pages":"29--34","doi":"10.18653\/v1\/N19-3005","url":"https:\/\/www.aclweb.org\/anthology\/N19-3005","publisher":"Association for Computational Linguistics","address":"Minneapolis, Minnesota","year":"2019","month":"June","booktitle":"Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Student Research Workshop"},{"id":"N19-3006","title":"Opinion Mining with Deep Contextualized Embeddings","authors":["Han, Wen-Bin","Kando, Noriko"],"emails":["vincent.han@nlplab.cc","kando@nii.ac.jp"],"author_id":["wen-bin-han","noriko-kando"],"abstract":"Detecting opinion expression is a potential and essential task in opinion mining that can be extended to advanced tasks. In this paper, we considered opinion expression detection as a sequence labeling task and exploited different deep contextualized embedders into the state-of-the-art architecture, composed of bidirectional long short-term memory (BiLSTM) and conditional random field (CRF). Our experimental results show that using different word embeddings can cause contrasting results, and the model can achieve remarkable scores with deep contextualized embeddings. Especially, using BERT embedder can significantly exceed using ELMo embedder.","pages":"35--42","doi":"10.18653\/v1\/N19-3006","url":"https:\/\/www.aclweb.org\/anthology\/N19-3006","publisher":"Association for Computational Linguistics","address":"Minneapolis, Minnesota","year":"2019","month":"June","booktitle":"Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Student Research Workshop"},{"id":"N19-3007","title":"A Bag-of-concepts Model Improves Relation Extraction in a Narrow Knowledge Domain with Limited Data","authors":["Chen, Jiyu","Verspoor, Karin","Zhai, Zenan"],"emails":["jiyuc@student.unimelb.edu.au","karin.verspoor@unimelb.edu.au","zenan.zhai@unimelb.edu.au"],"author_id":["jiyu-chen","karin-verspoor","zenan-zhai"],"abstract":"This paper focuses on a traditional relation extraction task in the context of limited annotated data and a narrow knowledge domain. We explore this task with a clinical corpus consisting of 200 breast cancer follow-up treatment letters in which 16 distinct types of relations are annotated. We experiment with an approach to extracting typed relations called window-bounded co-occurrence (WBC), which uses an adjustable context window around entity mentions of a relevant type, and compare its performance with a more typical intra-sentential co-occurrence baseline. We further introduce a new bag-of-concepts (BoC) approach to feature engineering based on the state-of-the-art word embeddings and word synonyms. We demonstrate the competitiveness of BoC by comparing with methods of higher complexity, and explore its effectiveness on this small dataset.","pages":"43--52","doi":"10.18653\/v1\/N19-3007","url":"https:\/\/www.aclweb.org\/anthology\/N19-3007","publisher":"Association for Computational Linguistics","address":"Minneapolis, Minnesota","year":"2019","month":"June","booktitle":"Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Student Research Workshop"},{"id":"N19-3008","title":"Generating Text through Adversarial Training Using Skip-Thought Vectors","authors":["Ahamad, Afroz"],"emails":["afroz.sahamad@gmail.com"],"author_id":["afroz-ahamad"],"abstract":"GANs have been shown to perform exceedingly well on tasks pertaining to image generation and style transfer. In the field of language modelling, word embeddings such as GLoVe and word2vec are state-of-the-art methods for applying neural network models on textual data. Attempts have been made to utilize GANs with word embeddings for text generation. This study presents an approach to text generation using Skip-Thought sentence embeddings with GANs based on gradient penalty functions and f-measures. The proposed architecture aims to reproduce writing style in the generated text by modelling the way of expression at a sentence level across all the works of an author. Extensive experiments were run in different embedding settings on a variety of tasks including conditional text generation and language generation. The model outperforms baseline text generation networks across several automated evaluation metrics like BLEU-n, METEOR and ROUGE. Further, wide applicability and effectiveness in real life tasks are demonstrated through human judgement scores.","pages":"53--60","doi":"10.18653\/v1\/N19-3008","url":"https:\/\/www.aclweb.org\/anthology\/N19-3008","publisher":"Association for Computational Linguistics","address":"Minneapolis, Minnesota","year":"2019","month":"June","booktitle":"Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Student Research Workshop"},{"id":"N19-3009","title":"A Partially Rule-Based Approach to {AMR} Generation","authors":["Manning, Emma"],"emails":["esm76@georgetown.edu"],"author_id":["emma-manning"],"abstract":"This paper presents a new approach to generating English text from Abstract Meaning Representation (AMR). In contrast to the neural and statistical MT approaches used in other AMR generation systems, this one is largely rule-based, supplemented only by a language model and simple statistical linearization models, allowing for more control over the output. We also address the difficulties of automatically evaluating AMR generation systems and the problems with BLEU for this task. We compare automatic metrics to human evaluations and show that while METEOR and TER arguably reflect human judgments better than BLEU, further research into suitable evaluation metrics is needed.","pages":"61--70","doi":"10.18653\/v1\/N19-3009","url":"https:\/\/www.aclweb.org\/anthology\/N19-3009","publisher":"Association for Computational Linguistics","address":"Minneapolis, Minnesota","year":"2019","month":"June","booktitle":"Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Student Research Workshop"},{"id":"N19-3010","title":"Computational Investigations of Pragmatic Effects in Natural Language","authors":["Kabbara, Jad"],"emails":["jad@cs.mcgill.ca"],"author_id":["jad-kabbara"],"abstract":"Semantics and pragmatics are two complimentary and intertwined aspects of meaning in language. The former is concerned with the literal (context-free) meaning of words and sentences, the latter focuses on the intended meaning, one that is context-dependent. While NLP research has focused in the past mostly on semantics, the goal of this thesis is to develop computational models that leverage this pragmatic knowledge in language that is crucial to performing many NLP tasks correctly. In this proposal, we begin by reviewing the current progress in this thesis, namely, on the tasks of definiteness prediction and adverbial presupposition triggering. Then we discuss the proposed research for the remainder of the thesis which builds on this progress towards the goal of building better and more pragmatically-aware natural language generation and understanding systems.","pages":"71--76","doi":"10.18653\/v1\/N19-3010","url":"https:\/\/www.aclweb.org\/anthology\/N19-3010","publisher":"Association for Computational Linguistics","address":"Minneapolis, Minnesota","year":"2019","month":"June","booktitle":"Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Student Research Workshop"},{"id":"N19-3011","title":"{SEDTW}ik: Segmentation-based Event Detection from Tweets Using {W}ikipedia","authors":["Morabia, Keval","Bhanu Murthy, Neti Lalita","Malapati, Aruna","Samant, Surender"],"emails":["kevalmorabia97@gmail.com","bhanu@hyderabad.bits-pilani.ac.in","arunam@hyderabad.bits-pilani.ac.in","surender.samant@hyderabad.bits-pilani.ac.in"],"author_id":["keval-morabia","neti-lalita-bhanu-murthy","aruna-malapati","surender-samant"],"abstract":"Event Detection has been one of the research areas in Text Mining that has attracted attention during this decade due to the widespread availability of social media data specifically twitter data. Twitter has become a major source for information about real-world events because of the use of hashtags and the small word limit of Twitter that ensures concise presentation of events. Previous works on event detection from tweets are either applicable to detect localized events or breaking news only or miss out on many important events. This paper presents the problems associated with event detection from tweets and a tweet-segmentation based system for event detection called SEDTWik, an extension to a previous work, that is able to detect newsworthy events occurring at different locations of the world from a wide range of categories. The main idea is to split each tweet and hash-tag into segments, extract bursty segments, cluster them, and summarize them. We evaluated our results on the well-known Events2012 corpus and achieved state-of-the-art results. Keywords: Event detection, Twitter, Social Media, Microblogging, Tweet segmentation, Text Mining, Wikipedia, Hashtag.","pages":"77--85","doi":"10.18653\/v1\/N19-3011","url":"https:\/\/www.aclweb.org\/anthology\/N19-3011","publisher":"Association for Computational Linguistics","address":"Minneapolis, Minnesota","year":"2019","month":"June","booktitle":"Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Student Research Workshop"},{"id":"N19-3012","title":"Multimodal Machine Translation with Embedding Prediction","authors":["Hirasawa, Tosho","Yamagishi, Hayahide","Matsumura, Yukio","Komachi, Mamoru"],"emails":["","","",""],"author_id":["tosho-hirasawa","hayahide-yamagishi","yukio-matsumura","mamoru-komachi"],"abstract":"Multimodal machine translation is an attractive application of neural machine translation (NMT). It helps computers to deeply understand visual objects and their relations with natural languages. However, multimodal NMT systems suffer from a shortage of available training data, resulting in poor performance for translating rare words. In NMT, pretrained word embeddings have been shown to improve NMT of low-resource domains, and a search-based approach is proposed to address the rare word problem. In this study, we effectively combine these two approaches in the context of multimodal NMT and explore how we can take full advantage of pretrained word embeddings to better translate rare words. We report overall performance improvements of 1.24 METEOR and 2.49 BLEU and achieve an improvement of 7.67 F-score for rare word translation.","pages":"86--91","doi":"10.18653\/v1\/N19-3012","url":"https:\/\/www.aclweb.org\/anthology\/N19-3012","publisher":"Association for Computational Linguistics","address":"Minneapolis, Minnesota","year":"2019","month":"June","booktitle":"Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Student Research Workshop"},{"id":"N19-3013","title":"Deep Learning and Sociophonetics: Automatic Coding of Rhoticity Using Neural Networks","authors":["Gupta, Sarah","DiPadova, Anthony"],"emails":["sarah.gupta.19@dartmouth.edu","anthony.f.dipadova.iii.19@dartmouth.edu"],"author_id":["sarah-gupta","anthony-dipadova"],"abstract":"Automated extraction methods are widely available for vowels, but automated methods for coding rhoticity have lagged far behind. R-fulness versus r-lessness (in words like park, store, etc.) is a classic and frequently cited variable, but it is still commonly coded by human analysts rather than automated methods. Human-coding requires extensive resources and lacks replicability, making it difficult to compare large datasets across research groups. Can reliable automated methods be developed to aid in coding rhoticity? In this study, we use Neural Networks\/Deep Learning, training our model on 208 Boston-area speakers.","pages":"92--96","doi":"10.18653\/v1\/N19-3013","url":"https:\/\/www.aclweb.org\/anthology\/N19-3013","publisher":"Association for Computational Linguistics","address":"Minneapolis, Minnesota","year":"2019","month":"June","booktitle":"Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Student Research Workshop"},{"id":"N19-3014","title":"Data Augmentation by Data Noising for Open-vocabulary Slots in Spoken Language Understanding","authors":["Kim, Hwa-Yeon","Roh, Yoon-Hyung","Kim, Young-Kil"],"emails":["hy.kim.aai@gmail.com","yhroh@etri.re.kr","kimyk@etri.re.kr"],"author_id":["hwa-yeon-kim","yoon-hyung-roh","young-gil-kim"],"abstract":"One of the main challenges in Spoken Language Understanding (SLU) is dealing with {`}open-vocabulary{'} slots. Recently, SLU models based on neural network were proposed, but it is still difficult to recognize the slots of unknown words or {`}open-vocabulary{'} slots because of the high cost of creating a manually tagged SLU dataset. This paper proposes data noising, which reflects the characteristics of the {`}open-vocabulary{'} slots, for data augmentation. We applied it to an attention based bi-directional recurrent neural network (Liu and Lane, 2016) and experimented with three datasets: Airline Travel Information System (ATIS), Snips, and MIT-Restaurant. We achieved performance improvements of up to 0.57{\\%} and 3.25 in intent prediction (accuracy) and slot filling (f1-score), respectively. Our method is advantageous because it does not require additional memory and it can be applied simultaneously with the training process of the model.","pages":"97--102","doi":"10.18653\/v1\/N19-3014","url":"https:\/\/www.aclweb.org\/anthology\/N19-3014","publisher":"Association for Computational Linguistics","address":"Minneapolis, Minnesota","year":"2019","month":"June","booktitle":"Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Student Research Workshop"},{"id":"N19-3015","title":"Expectation and Locality Effects in the Prediction of Disfluent Fillers and Repairs in {E}nglish Speech","authors":["Dammalapati, Samvit","Rajkumar, Rajakrishnan","Agarwal, Sumeet"],"emails":["samvit1998@gmail.com","rajak@iiserb.ac.in","sumeet@iitd.ac.in"],"author_id":["samvit-dammalapati","rajakrishnan-rajkumar","sumeet-agarwal"],"abstract":"This study examines the role of three influential theories of language processing, \\textit{viz.}, Surprisal Theory, Uniform Information Density (UID) hypothesis and Dependency Locality Theory (DLT), in predicting disfluencies in speech production. To this end, we incorporate features based on lexical surprisal, word duration and DLT integration and storage costs into logistic regression classifiers aimed to predict disfluencies in the Switchboard corpus of English conversational speech. We find that disfluencies occur in the face of upcoming difficulties and speakers tend to handle this by lessening cognitive load before disfluencies occur. Further, we see that reparandums behave differently from disfluent fillers possibly due to the lessening of the cognitive load also happening in the word choice of the reparandum, i.e., in the disfluency itself. While the UID hypothesis does not seem to play a significant role in disfluency prediction, lexical surprisal and DLT costs do give promising results in explaining language production. Further, we also find that as a means to lessen cognitive load for upcoming difficulties speakers take more time on words preceding disfluencies, making duration a key element in understanding disfluencies.","pages":"103--109","doi":"10.18653\/v1\/N19-3015","url":"https:\/\/www.aclweb.org\/anthology\/N19-3015","publisher":"Association for Computational Linguistics","address":"Minneapolis, Minnesota","year":"2019","month":"June","booktitle":"Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Student Research Workshop"},{"id":"N19-3016","title":"Gating Mechanisms for Combining Character and Word-level Word Representations: an Empirical Study","authors":["Balazs, Jorge","Matsuo, Yutaka"],"emails":["jorge@weblab.t.u-tokyo.ac.jp","matsuo@weblab.t.u-tokyo.ac.jp"],"author_id":["jorge-balazs","yutaka-matsuo"],"abstract":"In this paper we study how different ways of combining character and word-level representations affect the quality of both final word and sentence representations. We provide strong empirical evidence that modeling characters improves the learned representations at the word and sentence levels, and that doing so is particularly useful when representing less frequent words. We further show that a feature-wise sigmoid gating mechanism is a robust method for creating representations that encode semantic similarity, as it performed reasonably well in several word similarity datasets. Finally, our findings suggest that properly capturing semantic similarity at the word level does not consistently yield improved performance in downstream sentence-level tasks.","pages":"110--124","doi":"10.18653\/v1\/N19-3016","url":"https:\/\/www.aclweb.org\/anthology\/N19-3016","publisher":"Association for Computational Linguistics","address":"Minneapolis, Minnesota","year":"2019","month":"June","booktitle":"Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Student Research Workshop"},{"id":"N19-3017","title":"A Pregroup Representation of Word Order Alternation Using {H}indi Syntax","authors":["Debnath, Alok","Shrivastava, Manish"],"emails":["alok.debnath@research.iiit.ac.in","m.shrivastava@iiit.ac.in"],"author_id":["alok-debnath","manish-shrivastava"],"abstract":"Pregroup calculus has been used for the representation of free word order languages (Sanskrit and Hungarian), using a construction called precyclicity. However, restricted word order alternation has not been handled before. This paper aims at introducing and formally expressing three methods of representing word order alternation in the pregroup representation of any language. This paper describes the word order alternation patterns of Hindi, and creates a basic pregroup representation for the language. In doing so, the shortcoming of correct reductions for ungrammatical sentences due to the current apparatus is highlighted, and the aforementioned methods are invoked for a grammatically accurate representation of restricted word order alternation. The replicability of these methods is explained in the representation of adverbs and prepositional phrases in English.","pages":"125--135","doi":"10.18653\/v1\/N19-3017","url":"https:\/\/www.aclweb.org\/anthology\/N19-3017","publisher":"Association for Computational Linguistics","address":"Minneapolis, Minnesota","year":"2019","month":"June","booktitle":"Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Student Research Workshop"},{"id":"N19-3018","title":"Speak up, Fight Back! Detection of Social Media Disclosures of Sexual Harassment","authors":["Ghosh Chowdhury, Arijit","Sawhney, Ramit","Mathur, Puneet","Mahata, Debanjan","Ratn Shah, Rajiv"],"emails":["arijit10@gmail.com","ramits.co@nsit.net.in","pmathur3k6@gmail.com","dmahata@bloomberg.net","rajivratn@iiitd.ac.in"],"author_id":["arijit-ghosh-chowdhury","ramit-sawhney","puneet-mathur","debanjan-mahata","rajiv-ratn-shah"],"abstract":"The {\\#}MeToo movement is an ongoing prevalent phenomenon on social media aiming to demonstrate the frequency and widespread of sexual harassment by providing a platform to speak narrate personal experiences of such harassment. The aggregation and analysis of such disclosures pave the way to development of technology-based prevention of sexual harassment. We contend that the lack of specificity in generic sentence classification models may not be the best way to tackle text subtleties that intrinsically prevail in a classification task as complex as identifying disclosures of sexual harassment. We propose the Disclosure Language Model, a three part ULMFiT architecture, consisting of a Language model, a Medium-Specific (Twitter) model and a Task-Specific classifier to tackle this problem and create a manually annotated real-world dataset to test our technique on this, to show that using a Discourse Language Model often yields better classification performance over (i) Generic deep learning based sentence classification models (ii) existing models that rely on handcrafted stylistic features. An extensive comparison with state-of-the-art generic and specific models along with a detailed error analysis presents the case for our proposed methodology.","pages":"136--146","doi":"10.18653\/v1\/N19-3018","url":"https:\/\/www.aclweb.org\/anthology\/N19-3018","publisher":"Association for Computational Linguistics","address":"Minneapolis, Minnesota","year":"2019","month":"June","booktitle":"Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Student Research Workshop"},{"id":"N19-3019","title":"{SNAP}-{BATNET}: Cascading Author Profiling and Social Network Graphs for Suicide Ideation Detection on Social Media","authors":["Mishra, Rohan","Prakhar Sinha, Pradyumn","Sawhney, Ramit","Mahata, Debanjan","Mathur, Puneet","Ratn Shah, Rajiv"],"emails":["rohan.mishra1997@gmail.com","bt2k15@dtu.ac.in","ramits.co@nsit.net.in","dmahata@bloomberg.net","pmathur3k6@gmail.com","rajivratn@iiitd.ac.in"],"author_id":["rohan-mishra","pradyumn-prakhar-sinha","ramit-sawhney","debanjan-mahata","puneet-mathur","rajiv-ratn-shah"],"abstract":"Suicide is a leading cause of death among youth and the use of social media to detect suicidal ideation is an active line of research. While it has been established that these users share a common set of properties, the current state-of-the-art approaches utilize only text-based (stylistic and semantic) cues. We contend that the use of information from networks in the form of condensed social graph embeddings and author profiling using features from historical data can be combined with an existing set of features to improve the performance. To that end, we experiment on a manually annotated dataset of tweets created using a three-phase strategy and propose SNAP-BATNET, a deep learning based model to extract text-based features and a novel Feature Stacking approach to combine other community-based information such as historical author profiling and graph embeddings that outperform the current state-of-the-art. We conduct a comprehensive quantitative analysis with baselines, both generic and specific, that presents the case for SNAP-BATNET, along with an error analysis that highlights the limitations and challenges faced paving the way to the future of AI-based suicide ideation detection.","pages":"147--156","doi":"10.18653\/v1\/N19-3019","url":"https:\/\/www.aclweb.org\/anthology\/N19-3019","publisher":"Association for Computational Linguistics","address":"Minneapolis, Minnesota","year":"2019","month":"June","booktitle":"Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Student Research Workshop"}]