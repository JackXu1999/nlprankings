[{"id":"W19-5201","title":"Saliency-driven Word Alignment Interpretation for Neural Machine Translation","authors":["Ding, Shuoyang","Xu, Hainan","Koehn, Philipp"],"emails":["dings@jhu.edu","hxu31@jhu.edu","phi@jhu.edu"],"author_id":["shuoyang-ding","hainan-xu","philipp-koehn"],"abstract":"Despite their original goal to jointly learn to align and translate, Neural Machine Translation (NMT) models, especially Transformer, are often perceived as not learning interpretable word alignments. In this paper, we show that NMT models do learn interpretable word alignments, which could only be revealed with proper interpretation methods. We propose a series of such methods that are model-agnostic, are able to be applied either offline or online, and do not require parameter update or architectural change. We show that under the force decoding setup, the alignments induced by our interpretation method are of better quality than fast-align for some systems, and when performing free decoding, they agree well with the alignments induced by automatic alignment tools.","pages":"1--12","doi":"10.18653\/v1\/W19-5201","url":"https:\/\/www.aclweb.org\/anthology\/W19-5201","publisher":"Association for Computational Linguistics","address":"Florence, Italy","year":"2019","month":"August","booktitle":"Proceedings of the Fourth Conference on Machine Translation (Volume 1: Research Papers)"},{"id":"W19-5202","title":"Improving Zero-shot Translation with Language-Independent Constraints","authors":["Pham, Ngoc-Quan","Niehues, Jan","Ha, Thanh-Le","Waibel, Alexander"],"emails":["ngoc.pham@kit.edu","jan.niehues@maastrichtuniversity.nl","thanh-le.ha@kit.edu","alex.waibel@kit.edu"],"author_id":["ngoc-quan-pham","jan-niehues","thanh-le-ha","alex-waibel"],"abstract":"An important concern in training multilingual neural machine translation (NMT) is to translate between language pairs unseen during training, i.e zero-shot translation. Improving this ability kills two birds with one stone by providing an alternative to pivot translation which also allows us to better understand how the model captures information between languages. In this work, we carried out an investigation on this capability of the multilingual NMT models. First, we intentionally create an encoder architecture which is independent with respect to the source language. Such experiments shed light on the ability of NMT encoders to learn multilingual representations, in general. Based on such proof of concept, we were able to design regularization methods into the standard Transformer model, so that the whole architecture becomes more robust in zero-shot conditions. We investigated the behaviour of such models on the standard IWSLT 2017 multilingual dataset. We achieved an average improvement of 2.23 BLEU points across 12 language pairs compared to the zero-shot performance of a state-of-the-art multilingual system. Additionally, we carry out further experiments in which the effect is confirmed even for language pairs with multiple intermediate pivots.","pages":"13--23","doi":"10.18653\/v1\/W19-5202","url":"https:\/\/www.aclweb.org\/anthology\/W19-5202","publisher":"Association for Computational Linguistics","address":"Florence, Italy","year":"2019","month":"August","booktitle":"Proceedings of the Fourth Conference on Machine Translation (Volume 1: Research Papers)"},{"id":"W19-5203","title":"Incorporating Source Syntax into Transformer-Based Neural Machine Translation","authors":["Currey, Anna","Heafield, Kenneth"],"emails":["a.currey@sms.ed.ac.uk","kheafiel@ed.ac.uk"],"author_id":["anna-currey","kenneth-heafield"],"abstract":"Transformer-based neural machine translation (NMT) has recently achieved state-of-the-art performance on many machine translation tasks. However, recent work (Raganato and Tiedemann, 2018; Tang et al., 2018; Tran et al., 2018) has indicated that Transformer models may not learn syntactic structures as well as their recurrent neural network-based counterparts, particularly in low-resource cases. In this paper, we incorporate constituency parse information into a Transformer NMT model. We leverage linearized parses of the source training sentences in order to inject syntax into the Transformer architecture without modifying it. We introduce two methods: a multi-task machine translation and parsing model with a single encoder and decoder, and a mixed encoder model that learns to translate directly from parsed and unparsed source sentences. We evaluate our methods on low-resource translation from English into twenty target languages, showing consistent improvements of 1.3 BLEU on average across diverse target languages for the multi-task technique. We further evaluate the models on full-scale WMT tasks, finding that the multi-task model aids low- and medium-resource NMT but degenerates high-resource English-German translation.","pages":"24--33","doi":"10.18653\/v1\/W19-5203","url":"https:\/\/www.aclweb.org\/anthology\/W19-5203","publisher":"Association for Computational Linguistics","address":"Florence, Italy","year":"2019","month":"August","booktitle":"Proceedings of the Fourth Conference on Machine Translation (Volume 1: Research Papers)"},{"id":"W19-5204","title":"{APE} at Scale and Its Implications on {MT} Evaluation Biases","authors":["Freitag, Markus","Caswell, Isaac","Roy, Scott"],"emails":["freitag@google.com","icaswell@google.com","hsr@google.com"],"author_id":["markus-freitag","isaac-caswell","scott-roy"],"abstract":"In this work, we train an Automatic Post-Editing (APE) model and use it to reveal biases in standard MT evaluation procedures. The goal of our APE model is to correct typical errors introduced by the translation process, and convert the {``}translationese{''} output into natural text. Our APE model is trained entirely on monolingual data that has been round-trip translated through English, to mimic errors that are similar to the ones introduced by NMT. We apply our model to the output of existing NMT systems, and demonstrate that, while the human-judged quality improves in all cases, BLEU scores drop with forward-translated test sets. We verify these results for the WMT18 English to German, WMT15 English to French, and WMT16 English to Romanian tasks. Furthermore, we selectively apply our APE model on the output of the top submissions of the most recent WMT evaluation campaigns. We see quality improvements on all tasks of up to 2.5 BLEU points.","pages":"34--44","doi":"10.18653\/v1\/W19-5204","url":"https:\/\/www.aclweb.org\/anthology\/W19-5204","publisher":"Association for Computational Linguistics","address":"Florence, Italy","year":"2019","month":"August","booktitle":"Proceedings of the Fourth Conference on Machine Translation (Volume 1: Research Papers)"},{"id":"W19-5205","title":"Generalizing Back-Translation in Neural Machine Translation","authors":["Gra{\\c{c}}a, Miguel","Kim, Yunsu","Schamper, Julian","Khadivi, Shahram","Ney, Hermann"],"emails":["","","surname@i6.informatik.rwth-aachen.de","skhadivi@ebay.com",""],"author_id":["miguel-graca","yunsu-kim","julian-schamper","shahram-khadivi","hermann-ney"],"abstract":"Back-translation {---} data augmentation by translating target monolingual data {---} is a crucial component in modern neural machine translation (NMT). In this work, we reformulate back-translation in the scope of cross-entropy optimization of an NMT model, clarifying its underlying mathematical assumptions and approximations beyond its heuristic usage. Our formulation covers broader synthetic data generation schemes, including sampling from a target-to-source NMT model. With this formulation, we point out fundamental problems of the sampling-based approaches and propose to remedy them by (i) disabling label smoothing for the target-to-source model and (ii) sampling from a restricted search space. Our statements are investigated on the WMT 2018 German {\\textless}-{\\textgreater} English news translation task.","pages":"45--52","doi":"10.18653\/v1\/W19-5205","url":"https:\/\/www.aclweb.org\/anthology\/W19-5205","publisher":"Association for Computational Linguistics","address":"Florence, Italy","year":"2019","month":"August","booktitle":"Proceedings of the Fourth Conference on Machine Translation (Volume 1: Research Papers)"},{"id":"W19-5206","title":"Tagged Back-Translation","authors":["Caswell, Isaac","Chelba, Ciprian","Grangier, David"],"emails":["icaswell@google.com","ciprianchelba@google.com","grangier@google.com"],"author_id":["isaac-caswell","ciprian-chelba","david-grangier"],"abstract":"Recent work in Neural Machine Translation (NMT) has shown significant quality gains from noised-beam decoding during back-translation, a method to generate synthetic parallel data. We show that the main role of such synthetic noise is not to diversify the source side, as previously suggested, but simply to indicate to the model that the given source is synthetic. We propose a simpler alternative to noising techniques, consisting of tagging back-translated source sentences with an extra token. Our results on WMT outperform noised back-translation in English-Romanian and match performance on English-German, redefining the state-of-the-art on the former.","pages":"53--63","doi":"10.18653\/v1\/W19-5206","url":"https:\/\/www.aclweb.org\/anthology\/W19-5206","publisher":"Association for Computational Linguistics","address":"Florence, Italy","year":"2019","month":"August","booktitle":"Proceedings of the Fourth Conference on Machine Translation (Volume 1: Research Papers)"},{"id":"W19-5207","title":"Hierarchical Document Encoder for Parallel Corpus Mining","authors":["Guo, Mandy","Yang, Yinfei","Stevens, Keith","Cer, Daniel","Ge, Heming","Sung, Yun-hsuan","Strope, Brian","Kurzweil, Ray"],"emails":["xyguo@google.com","yinfeiy@google.com","kstevens@google.com","cer@google.com","hemingge@google.com","yhsung@google.com","","raykurzweil@google.com"],"author_id":["mandy-guo","yinfei-yang","keith-stevens","daniel-cer","heming-ge","yun-hsuan-sung1","brian-strope","ray-kurzweil"],"abstract":"We explore using multilingual document embeddings for nearest neighbor mining of parallel data. Three document-level representations are investigated: (i) document embeddings generated by simply averaging multilingual sentence embeddings; (ii) a neural bag-of-words (BoW) document encoding model; (iii) a hierarchical multilingual document encoder (HiDE) that builds on our sentence-level model. The results show document embeddings derived from sentence-level averaging are surprisingly effective for clean datasets, but suggest models trained hierarchically at the document-level are more effective on noisy data. Analysis experiments demonstrate our hierarchical models are very robust to variations in the underlying sentence embedding quality. Using document embeddings trained with HiDE achieves the state-of-the-art on United Nations (UN) parallel document mining, 94.9{\\%} P@1 for en-fr and 97.3{\\%} P@1 for en-es.","pages":"64--72","doi":"10.18653\/v1\/W19-5207","url":"https:\/\/www.aclweb.org\/anthology\/W19-5207","publisher":"Association for Computational Linguistics","address":"Florence, Italy","year":"2019","month":"August","booktitle":"Proceedings of the Fourth Conference on Machine Translation (Volume 1: Research Papers)"},{"id":"W19-5208","title":"The Effect of Translationese in Machine Translation Test Sets","authors":["Zhang, Mike","Toral, Antonio"],"emails":["j.j.zhang.1@student.rug.nl","a.toral.ruiz@rug.nl"],"author_id":["mike-zhang","antonio-toral"],"abstract":"The effect of translationese has been studied in the field of machine translation (MT), mostly with respect to training data. We study in depth the effect of translationese on test data, using the test sets from the last three editions of WMT{'}s news shared task, containing 17 translation directions. We show evidence that (i) the use of translationese in test sets results in inflated human evaluation scores for MT systems; (ii) in some cases system rankings do change and (iii) the impact translationese has on a translation direction is inversely cor- related to the translation quality attainable by state-of-the-art MT systems for that direction.","pages":"73--81","doi":"10.18653\/v1\/W19-5208","url":"https:\/\/www.aclweb.org\/anthology\/W19-5208","publisher":"Association for Computational Linguistics","address":"Florence, Italy","year":"2019","month":"August","booktitle":"Proceedings of the Fourth Conference on Machine Translation (Volume 1: Research Papers)"},{"id":"W19-5209","title":"Customizing Neural Machine Translation for Subtitling","authors":["Matusov, Evgeny","Wilken, Patrick","Georgakopoulou, Yota"],"emails":["ematusov@apptek.com","pwilken@apptek.com","yota@athenaconsultancy.eu"],"author_id":["evgeny-matusov","patrick-wilken","yota-georgakopoulou"],"abstract":"In this work, we customized a neural machine translation system for translation of subtitles in the domain of entertainment. The neural translation model was adapted to the subtitling content and style and extended by a simple, yet effective technique for utilizing inter-sentence context for short sentences such as dialog turns. The main contribution of the paper is a novel subtitle segmentation algorithm that predicts the end of a subtitle line given the previous word-level context using a recurrent neural network learned from human segmentation decisions. This model is combined with subtitle length and duration constraints established in the subtitling industry. We conducted a thorough human evaluation with two post-editors (English-to-Spanish translation of a documentary and a sitcom). It showed a notable productivity increase of up to 37{\\%} as compared to translating from scratch and significant reductions in human translation edit rate in comparison with the post-editing of the baseline non-adapted system without a learned segmentation model.","pages":"82--93","doi":"10.18653\/v1\/W19-5209","url":"https:\/\/www.aclweb.org\/anthology\/W19-5209","publisher":"Association for Computational Linguistics","address":"Florence, Italy","year":"2019","month":"August","booktitle":"Proceedings of the Fourth Conference on Machine Translation (Volume 1: Research Papers)"},{"id":"W19-5210","title":"Integration of Dubbing Constraints into Machine Translation","authors":["Saboo, Ashutosh","Baumann, Timo"],"emails":["ashutosh.saboo96@gmail.com","baumann@informatik.uni-hamburg.de"],"author_id":["ashutosh-saboo","timo-baumann"],"abstract":"Translation systems aim to perform a meaning-preserving conversion of linguistic material (typically text but also speech) from a source to a target language (and, to a lesser degree, the corresponding socio-cultural contexts). Dubbing, i.e., the lip-synchronous translation and revoicing of speech adds to this constraints about the close matching of phonetic and resulting visemic synchrony characteristics of source and target material. There is an inherent conflict between a translation{'}s meaning preservation and {`}dubbability{'} and the resulting trade-off can be controlled by weighing the synchrony constraints. We introduce our work, which to the best of our knowledge is the first of its kind, on integrating synchrony constraints into the machine translation paradigm. We present first results for the integration of synchrony constraints into encoder decoder-based neural machine translation and show that considerably more {`}dubbable{'} translations can be achieved with only a small impact on BLEU score, and dubbability improves more steeply than BLEU degrades.","pages":"94--101","doi":"10.18653\/v1\/W19-5210","url":"https:\/\/www.aclweb.org\/anthology\/W19-5210","publisher":"Association for Computational Linguistics","address":"Florence, Italy","year":"2019","month":"August","booktitle":"Proceedings of the Fourth Conference on Machine Translation (Volume 1: Research Papers)"},{"id":"W19-5211","title":"Widening the Representation Bottleneck in Neural Machine Translation with Lexical Shortcuts","authors":["Emelin, Denis","Titov, Ivan","Sennrich, Rico"],"emails":["melin@sms.ed.ac.uk","ititov@inf.ed.ac.uk","rico.sennrich@ed.ac.uk"],"author_id":["denis-emelin","ivan-titov","rico-sennrich"],"abstract":"The transformer is a state-of-the-art neural translation model that uses attention to iteratively refine lexical representations with information drawn from the surrounding context. Lexical features are fed into the first layer and propagated through a deep network of hidden layers. We argue that the need to represent and propagate lexical features in each layer limits the model{'}s capacity for learning and representing other information relevant to the task. To alleviate this bottleneck, we introduce gated shortcut connections between the embedding layer and each subsequent layer within the encoder and decoder. This enables the model to access relevant lexical content dynamically, without expending limited resources on storing it within intermediate states. We show that the proposed modification yields consistent improvements over a baseline transformer on standard WMT translation tasks in 5 translation directions (0.9 BLEU on average) and reduces the amount of lexical information passed along the hidden layers. We furthermore evaluate different ways to integrate lexical connections into the transformer architecture and present ablation experiments exploring the effect of proposed shortcuts on model behavior.","pages":"102--115","doi":"10.18653\/v1\/W19-5211","url":"https:\/\/www.aclweb.org\/anthology\/W19-5211","publisher":"Association for Computational Linguistics","address":"Florence, Italy","year":"2019","month":"August","booktitle":"Proceedings of the Fourth Conference on Machine Translation (Volume 1: Research Papers)"},{"id":"W19-5212","title":"A High-Quality Multilingual Dataset for Structured Documentation Translation","authors":["Hashimoto, Kazuma","Buschiazzo, Raffaella","Bradbury, James","Marshall, Teresa","Socher, Richard","Xiong, Caiming"],"emails":["k.hashimoto@salesforce.com","rbuschiazzo@salesforce.com","james.bradbury@salesforce.com","teresa.marshall@salesforce.com","rsocher@salesforce.com","cxiong@salesforce.com"],"author_id":["kazuma-hashimoto","raffaella-buschiazzo","james-bradbury","teresa-marshall","richard-socher","caiming-xiong"],"abstract":"This paper presents a high-quality multilingual dataset for the documentation domain to advance research on localization of structured text. Unlike widely-used datasets for translation of plain text, we collect XML-structured parallel text segments from the online documentation for an enterprise software platform. These Web pages have been professionally translated from English into 16 languages and maintained by domain experts, and around 100,000 text segments are available for each language pair. We build and evaluate translation models for seven target languages from English, with several different copy mechanisms and an XML-constrained beam search. We also experiment with a non-English pair to show that our dataset has the potential to explicitly enable $17 \\times 16$ translation settings. Our experiments show that learning to translate with the XML tags improves translation accuracy, and the beam search accurately generates XML structures. We also discuss trade-offs of using the copy mechanisms by focusing on translation of numerical words and named entities. We further provide a detailed human analysis of gaps between the model output and human translations for real-world applications, including suitability for post-editing.","pages":"116--127","doi":"10.18653\/v1\/W19-5212","url":"https:\/\/www.aclweb.org\/anthology\/W19-5212","publisher":"Association for Computational Linguistics","address":"Florence, Italy","year":"2019","month":"August","booktitle":"Proceedings of the Fourth Conference on Machine Translation (Volume 1: Research Papers)"}]