[{"id":"W19-1501","title":"Parallelizable Stack Long Short-Term Memory","authors":["Ding, Shuoyang","Koehn, Philipp"],"emails":["dings@jhu.edu","phi@jhu.edu"],"author_id":["shuoyang-ding","philipp-koehn"],"abstract":"Stack Long Short-Term Memory (StackLSTM) is useful for various applications such as parsing and string-to-tree neural machine translation, but it is also known to be notoriously difficult to parallelize for GPU training due to the fact that the computations are dependent on discrete operations. In this paper, we tackle this problem by utilizing state access patterns of StackLSTM to homogenize computations with regard to different discrete operations. Our parsing experiments show that the method scales up almost linearly with increasing batch size, and our parallelized PyTorch implementation trains significantly faster compared to the Dynet C++ implementation.","pages":"1--6","doi":"10.18653\/v1\/W19-1501","url":"https:\/\/www.aclweb.org\/anthology\/W19-1501","publisher":"Association for Computational Linguistics","address":"Minneapolis, Minnesota","year":"2019","month":"June","booktitle":"Proceedings of the Third Workshop on Structured Prediction for {NLP}"},{"id":"W19-1502","title":"Tracking Discrete and Continuous Entity State for Process Understanding","authors":["Gupta, Aditya","Durrett, Greg"],"emails":["agupta@cs.utexas.edu","gdurrett@cs.utexas.edu"],"author_id":["aditya-gupta","greg-durrett"],"abstract":"Procedural text, which describes entities and their interactions as they undergo some process, depicts entities in a uniquely nuanced way. First, each entity may have some observable discrete attributes, such as its state or location; modeling these involves imposing global structure and enforcing consistency. Second, an entity may have properties which are not made explicit but can be effectively induced and tracked by neural networks. In this paper, we propose a structured neural architecture that reflects this dual nature of entity evolution. The model tracks each entity recurrently, updating its hidden continuous representation at each step to contain relevant state information. The global discrete state structure is explicitly modelled with a neural CRF over the changing hidden representation of the entity. This CRF can explicitly capture constraints on entity states over time, enforcing that, for example, an entity cannot move to a location after it is destroyed. We evaluate the performance of our proposed model on QA tasks over process paragraphs in the ProPara dataset and find that our model achieves state-of-the-art results.","pages":"7--12","doi":"10.18653\/v1\/W19-1502","url":"https:\/\/www.aclweb.org\/anthology\/W19-1502","publisher":"Association for Computational Linguistics","address":"Minneapolis, Minnesota","year":"2019","month":"June","booktitle":"Proceedings of the Third Workshop on Structured Prediction for {NLP}"},{"id":"W19-1503","title":"{SPARSE}: Structured Prediction using Argument-Relative Structured Encoding","authors":["Bommasani, Rishi","Katiyar, Arzoo","Cardie, Claire"],"emails":["rb724@cornell.edu","arzoo@cs.cornell.edu","cardie@cs.cornell.edu"],"author_id":["rishi-bommasani","arzoo-katiyar","claire-cardie"],"abstract":"We propose structured encoding as a novel approach to learning representations for relations and events in neural structured prediction. Our approach explicitly leverages the structure of available relation and event metadata to generate these representations, which are parameterized by both the attribute structure of the metadata as well as the learned representation of the arguments of the relations and events. We consider affine, biaffine, and recurrent operators for building hierarchical representations and modelling underlying features. We apply our approach to the second-order structured prediction task studied in the 2016\/2017 Belief and Sentiment analysis evaluations (BeSt): given a document and its entities, relations, and events (including metadata and mentions), determine the sentiment of each entity towards every relation and event in the document. Without task-specific knowledge sources or domain engineering, we significantly improve over systems and baselines that neglect the available metadata or its hierarchical structure. We observe across-the-board improvements on the BeSt 2016\/2017 sentiment analysis task of at least 2.3 (absolute) and 10.6{\\%} (relative) F-measure over the previous state-of-the-art.","pages":"13--17","doi":"10.18653\/v1\/W19-1503","url":"https:\/\/www.aclweb.org\/anthology\/W19-1503","publisher":"Association for Computational Linguistics","address":"Minneapolis, Minnesota","year":"2019","month":"June","booktitle":"Proceedings of the Third Workshop on Structured Prediction for {NLP}"},{"id":"W19-1504","title":"Lightly-supervised Representation Learning with Global Interpretability","authors":["Zupon, Andrew","Alexeeva, Maria","Valenzuela-Esc{\\'a}rcega, Marco","Nagesh, Ajay","Surdeanu, Mihai"],"emails":["zupon@email.arizona.edu","alexeeva@email.arizona.edu","marcov@email.arizona.edu","ajaynagesh@email.arizona.edu","msurdeanu@email.arizona.edu"],"author_id":["andrew-zupon","maria-alexeeva","marco-valenzuela-escarcega","ajay-nagesh","mihai-surdeanu"],"abstract":"We propose a lightly-supervised approach for information extraction, in particular named entity classification, which combines the benefits of traditional bootstrapping, i.e., use of limited annotations and interpretability of extraction patterns, with the robust learning approaches proposed in representation learning. Our algorithm iteratively learns custom embeddings for both the multi-word entities to be extracted and the patterns that match them from a few example entities per category. We demonstrate that this representation-based approach outperforms three other state-of-the-art bootstrapping approaches on two datasets: CoNLL-2003 and OntoNotes. Additionally, using these embeddings, our approach outputs a globally-interpretable model consisting of a decision list, by ranking patterns based on their proximity to the average entity embedding in a given class. We show that this interpretable model performs close to our complete bootstrapping model, proving that representation learning can be used to produce interpretable models with small loss in performance. This decision list can be edited by human experts to mitigate some of that loss and in some cases outperform the original model.","pages":"18--28","doi":"10.18653\/v1\/W19-1504","url":"https:\/\/www.aclweb.org\/anthology\/W19-1504","publisher":"Association for Computational Linguistics","address":"Minneapolis, Minnesota","year":"2019","month":"June","booktitle":"Proceedings of the Third Workshop on Structured Prediction for {NLP}"},{"id":"W19-1505","title":"Semi-Supervised Teacher-Student Architecture for Relation Extraction","authors":["Luo, Fan","Nagesh, Ajay","Sharp, Rebecca","Surdeanu, Mihai"],"emails":["fanluo@email.arizona.edu","ajaynagesh@email.arizona.edu","bsharp@email.arizona.edu","msurdeanu@email.arizona.edu"],"author_id":["fan-luo","ajay-nagesh","rebecca-sharp","mihai-surdeanu"],"abstract":"Generating a large amount of training data for information extraction (IE) is either costly (if annotations are created manually), or runs the risk of introducing noisy instances (if distant supervision is used). On the other hand, semi- supervised learning (SSL) is a cost-efficient solution to combat lack of training data. In this paper, we adapt Mean Teacher (Tarvainen and Valpola, 2017), a denoising SSL framework to extract semantic relations between pairs of entities. We explore the sweet spot of amount of supervision required for good performance on this binary relation extraction task. Addition- ally, different syntax representations are incorporated into our models to enhance the learned representation of sentences. We evaluate our approach on the Google-IISc Distant Supervision (GDS) dataset, which removes test data noise present in all previous distance supervision datasets, which makes it a reliable evaluation benchmark (Jat et al., 2017). Our results show that the SSL Mean Teacher approach nears the performance of fully-supervised approaches even with only 10{\\%} of the labeled corpus. Further, the syntax-aware model out- performs other syntax-free approaches across all levels of supervision.","pages":"29--37","doi":"10.18653\/v1\/W19-1505","url":"https:\/\/www.aclweb.org\/anthology\/W19-1505","publisher":"Association for Computational Linguistics","address":"Minneapolis, Minnesota","year":"2019","month":"June","booktitle":"Proceedings of the Third Workshop on Structured Prediction for {NLP}"}]