[{"id":"Q18-1001","title":"Whodunnit? Crime Drama as a Case for Natural Language Understanding","authors":["Frermann, Lea","Cohen, Shay B.","Lapata, Mirella"],"emails":["l.frermann@ed.ac.uk","scohen@inf.ed.ac.uk","mlap@inf.ed.ac.uk"],"author_id":["lea-frermann","shay-b-cohen","mirella-lapata"],"abstract":"In this paper we argue that crime drama exemplified in television programs such as CSI: Crime Scene Investigation is an ideal testbed for approximating real-world natural language understanding and the complex inferences associated with it. We propose to treat crime drama as a new inference task, capitalizing on the fact that each episode poses the same basic question (i.e., who committed the crime) and naturally provides the answer when the perpetrator is revealed. We develop a new dataset based on CSI episodes, formalize perpetrator identification as a sequence labeling problem, and develop an LSTM-based model which learns from multi-modal data. Experimental results show that an incremental inference strategy is key to making accurate guesses as well as learning from representations fusing textual, visual, and acoustic input.","pages":"1--15","doi":"10.1162\/tacl_a_00001","url":"https:\/\/www.aclweb.org\/anthology\/Q18-1001","year":"2018","volume":"6","journal":"Transactions of the Association for Computational Linguistics"},{"id":"Q18-1002","title":"Multiple Instance Learning Networks for Fine-Grained Sentiment Analysis","authors":["Angelidis, Stefanos","Lapata, Mirella"],"emails":["s.angelidis@ed.ac.uk","mlap@inf.ed.ac.uk"],"author_id":["stefanos-angelidis","mirella-lapata"],"abstract":"We consider the task of fine-grained sentiment analysis from the perspective of multiple instance learning (MIL). Our neural model is trained on document sentiment labels, and learns to predict the sentiment of text segments, i.e. sentences or elementary discourse units (EDUs), without segment-level supervision. We introduce an attention-based polarity scoring method for identifying positive and negative text snippets and a new dataset which we call SpoT (as shorthand for Segment-level POlariTy annotations) for evaluating MIL-style sentiment models like ours. Experimental results demonstrate superior performance against multiple baselines, whereas a judgement elicitation study shows that EDU-level opinion extraction produces more informative summaries than sentence-based alternatives.","pages":"17--31","doi":"10.1162\/tacl_a_00002","url":"https:\/\/www.aclweb.org\/anthology\/Q18-1002","year":"2018","volume":"6","journal":"Transactions of the Association for Computational Linguistics"},{"id":"Q18-1003","title":"Joint Semantic Synthesis and Morphological Analysis of the Derived Word","authors":["Cotterell, Ryan","Sch{\\\"u}tze, Hinrich"],"emails":["ryan.cotterell@jhu.edu","inquiries@cislmu.org"],"author_id":["ryan-cotterell","hinrich-schutze"],"abstract":"Much like sentences are composed of words, words themselves are composed of smaller units. For example, the English word questionably can be analyzed as question+able+ly. However, this structural decomposition of the word does not directly give us a semantic representation of the word{'}s meaning. Since morphology obeys the principle of compositionality, the semantics of the word can be systematically derived from the meaning of its parts. In this work, we propose a novel probabilistic model of word formation that captures both the analysis of a word w into its constituent segments and the synthesis of the meaning of w from the meanings of those segments. Our model jointly learns to segment words into morphemes and compose distributional semantic vectors of those morphemes. We experiment with the model on English CELEX data and German DErivBase (Zeller et al., 2013) data. We show that jointly modeling semantics increases both segmentation accuracy and morpheme F1 by between 3{\\%} and 5{\\%}. Additionally, we investigate different models of vector composition, showing that recurrent neural networks yield an improvement over simple additive models. Finally, we study the degree to which the representations correspond to a linguist{'}s notion of morphological productivity.","pages":"33--48","doi":"10.1162\/tacl_a_00003","url":"https:\/\/www.aclweb.org\/anthology\/Q18-1003","year":"2018","volume":"6","journal":"Transactions of the Association for Computational Linguistics"},{"id":"Q18-1004","title":"Representation Learning for Grounded Spatial Reasoning","authors":["Janner, Michael","Narasimhan, Karthik","Barzilay, Regina"],"emails":["janner@csail.mit.edu","karthikn@csail.mit.edu","regina@csail.mit.edu"],"author_id":["michael-janner","karthik-narasimhan","regina-barzilay"],"abstract":"The interpretation of spatial references is highly contextual, requiring joint inference over both language and the environment. We consider the task of spatial reasoning in a simulated environment, where an agent can act and receive rewards. The proposed model learns a representation of the world steered by instruction text. This design allows for precise alignment of local neighborhoods with corresponding verbalizations, while also handling global references in the instructions. We train our model with reinforcement learning using a variant of generalized value iteration. The model outperforms state-of-the-art approaches on several metrics, yielding a 45{\\%} reduction in goal localization error.","pages":"49--61","doi":"10.1162\/tacl_a_00004","url":"https:\/\/www.aclweb.org\/anthology\/Q18-1004","year":"2018","volume":"6","journal":"Transactions of the Association for Computational Linguistics"},{"id":"Q18-1005","title":"Learning Structured Text Representations","authors":["Liu, Yang","Lapata, Mirella"],"emails":["yang.liu2@ed.ac.uk","mlap@inf.ed.ac.uk"],"author_id":["yang-liu-edinburgh","mirella-lapata"],"abstract":"In this paper, we focus on learning structure-aware document representations from data without recourse to a discourse parser or additional annotations. Drawing inspiration from recent efforts to empower neural networks with a structural bias (Cheng et al., 2016; Kim et al., 2017), we propose a model that can encode a document while automatically inducing rich structural dependencies. Specifically, we embed a differentiable non-projective parsing algorithm into a neural model and use attention mechanisms to incorporate the structural biases. Experimental evaluations across different tasks and datasets show that the proposed model achieves state-of-the-art results on document modeling tasks while inducing intermediate structures which are both interpretable and meaningful.","pages":"63--75","doi":"10.1162\/tacl_a_00005","url":"https:\/\/www.aclweb.org\/anthology\/Q18-1005","year":"2018","volume":"6","journal":"Transactions of the Association for Computational Linguistics"},{"id":"Q18-1006","title":"Event Time Extraction with a Decision Tree of Neural Classifiers","authors":["Reimers, Nils","Dehghani, Nazanin","Gurevych, Iryna"],"emails":["","",""],"author_id":["nils-reimers","nazanin-dehghani","iryna-gurevych"],"abstract":"Extracting the information from text when an event happened is challenging. Documents do not only report on current events, but also on past events as well as on future events. Often, the relevant time information for an event is scattered across the document. In this paper we present a novel method to automatically anchor events in time. To our knowledge it is the first approach that takes temporal information from the complete document into account. We created a decision tree that applies neural network based classifiers at its nodes. We use this tree to incrementally infer, in a stepwise manner, at which time frame an event happened. We evaluate the approach on the TimeBank-EventTime Corpus (Reimers et al., 2016) achieving an accuracy of 42.0{\\%} compared to an inter-annotator agreement (IAA) of 56.7{\\%}. For events that span over a single day we observe an accuracy improvement of 33.1 points compared to the state-of-the-art CAEVO system (Chambers et al., 2014). Without retraining, we apply this model to the SemEval-2015 Task 4 on automatic timeline generation and achieve an improvement of 4.01 points F1-score compared to the state-of-the-art. Our code is publically available.","pages":"77--89","doi":"10.1162\/tacl_a_00006","url":"https:\/\/www.aclweb.org\/anthology\/Q18-1006","year":"2018","volume":"6","journal":"Transactions of the Association for Computational Linguistics"},{"id":"Q18-1007","title":"Towards Evaluating Narrative Quality In Student Writing","authors":["Somasundaran, Swapna","Flor, Michael","Chodorow, Martin","Molloy, Hillary","Gyawali, Binod","McCulla, Laura"],"emails":["","","martin.chodorow@hunter.cuny.edu","","","ulla@ets.org"],"author_id":["swapna-somasundaran","michael-flor","martin-chodorow","hillary-molloy","binod-gyawali","laura-mcculla"],"abstract":"This work lays the foundation for automated assessments of narrative quality in student writing. We first manually score essays for narrative-relevant traits and sub-traits, and measure inter-annotator agreement. We then explore linguistic features that are indicative of good narrative writing and use them to build an automated scoring system. Experiments show that our features are more effective in scoring specific aspects of narrative quality than a state-of-the-art feature set.","pages":"91--106","doi":"10.1162\/tacl_a_00007","url":"https:\/\/www.aclweb.org\/anthology\/Q18-1007","year":"2018","volume":"6","journal":"Transactions of the Association for Computational Linguistics"},{"id":"Q18-1008","title":"Evaluating the Stability of Embedding-based Word Similarities","authors":["Antoniak, Maria","Mimno, David"],"emails":["maa343@cornell.edu","mimno@cornell.edu"],"author_id":["maria-antoniak","david-mimno"],"abstract":"Word embeddings are increasingly being used as a tool to study word associations in specific corpora. However, it is unclear whether such embeddings reflect enduring properties of language or if they are sensitive to inconsequential variations in the source documents. We find that nearest-neighbor distances are highly sensitive to small changes in the training corpus for a variety of algorithms. For all methods, including specific documents in the training set can result in substantial variations. We show that these effects are more prominent for smaller training corpora. We recommend that users never rely on single embedding models for distance calculations, but rather average over multiple bootstrap samples, especially for small corpora.","pages":"107--119","doi":"10.1162\/tacl_a_00008","url":"https:\/\/www.aclweb.org\/anthology\/Q18-1008","year":"2018","volume":"6","journal":"Transactions of the Association for Computational Linguistics"},{"id":"Q18-1009","title":"Conversation Modeling on {R}eddit Using a Graph-Structured {LSTM}","authors":["Zayats, Victoria","Ostendorf, Mari"],"emails":["vzayats@uw.edu","ostendor@uw.edu"],"author_id":["victoria-zayats","mari-ostendorf"],"abstract":"This paper presents a novel approach for modeling threaded discussions on social media using a graph-structured bidirectional LSTM (long-short term memory) which represents both hierarchical and temporal conversation structure. In experiments with a task of predicting popularity of comments in Reddit discussions, the proposed model outperforms a node-independent architecture for different sets of input features. Analyses show a benefit to the model over the full course of the discussion, improving detection in both early and late stages. Further, the use of language cues with the bidirectional tree state updates helps with identifying controversial comments.","pages":"121--132","doi":"10.1162\/tacl_a_00009","url":"https:\/\/www.aclweb.org\/anthology\/Q18-1009","year":"2018","volume":"6","journal":"Transactions of the Association for Computational Linguistics"},{"id":"Q18-1010","title":"Learning Representations Specialized in Spatial Knowledge: Leveraging Language and Vision","authors":["Collell, Guillem","Moens, Marie-Francine"],"emails":["gcollell@kuleuven.be","sien.moens@cs.kuleuven.be"],"author_id":["guillem-collell","marie-francine-moens"],"abstract":"Spatial understanding is crucial in many real-world problems, yet little progress has been made towards building representations that capture spatial knowledge. Here, we move one step forward in this direction and learn such representations by leveraging a task consisting in predicting continuous 2D spatial arrangements of objects given object-relationship-object instances (e.g., {``}cat under chair{''}) and a simple neural network model that learns the task from annotated images. We show that the model succeeds in this task and, furthermore, that it is capable of predicting correct spatial arrangements for unseen objects if either CNN features or word embeddings of the objects are provided. The differences between visual and linguistic features are discussed. Next, to evaluate the spatial representations learned in the previous task, we introduce a task and a dataset consisting in a set of crowdsourced human ratings of spatial similarity for object pairs. We find that both CNN (convolutional neural network) features and word embeddings predict human judgments of similarity well and that these vectors can be further specialized in spatial knowledge if we update them when training the model that predicts spatial arrangements of objects. Overall, this paper paves the way towards building distributed spatial representations, contributing to the understanding of spatial expressions in language.","pages":"133--144","doi":"10.1162\/tacl_a_00010","url":"https:\/\/www.aclweb.org\/anthology\/Q18-1010","year":"2018","volume":"6","journal":"Transactions of the Association for Computational Linguistics"},{"id":"Q18-1011","title":"Modeling Past and Future for Neural Machine Translation","authors":["Zheng, Zaixiang","Zhou, Hao","Huang, Shujian","Mou, Lili","Dai, Xinyu","Chen, Jiajun","Tu, Zhaopeng"],"emails":["zhengzx@nlp.nju.edu.cn","zhouhao.nlp@bytedance.com","huangsj@nlp.nju.edu.cn","doublepower.mou@gmail.com","dxy@nlp.nju.edu.cn","chenjj@nlp.nju.edu.cn","zptu@tencent.com"],"author_id":["zaixiang-zheng","hao-zhou","shujian-huang","lili-mou","xinyu-dai","jiajun-chen","zhaopeng-tu"],"abstract":"Existing neural machine translation systems do not explicitly model what has been translated and what has not during the decoding phase. To address this problem, we propose a novel mechanism that separates the source information into two parts: translated Past contents and untranslated Future contents, which are modeled by two additional recurrent layers. The Past and Future contents are fed to both the attention model and the decoder states, which provides Neural Machine Translation (NMT) systems with the knowledge of translated and untranslated contents. Experimental results show that the proposed approach significantly improves the performance in Chinese-English, German-English, and English-German translation tasks. Specifically, the proposed model outperforms the conventional coverage model in terms of both the translation quality and the alignment error rate.","pages":"145--157","doi":"10.1162\/tacl_a_00011","url":"https:\/\/www.aclweb.org\/anthology\/Q18-1011","year":"2018","volume":"6","journal":"Transactions of the Association for Computational Linguistics"},{"id":"Q18-1012","title":"Mapping to Declarative Knowledge for Word Problem Solving","authors":["Roy, Subhro","Roth, Dan"],"emails":["subhro@csail.mit.edu","danroth@seas.upenn.edu"],"author_id":["subhro-roy","dan-roth"],"abstract":"Math word problems form a natural abstraction to a range of quantitative reasoning problems, such as understanding financial news, sports results, and casualties of war. Solving such problems requires the understanding of several mathematical concepts such as dimensional analysis, subset relationships, etc. In this paper, we develop declarative rules which govern the translation of natural language description of these concepts to math expressions. We then present a framework for incorporating such declarative knowledge into word problem solving. Our method learns to map arithmetic word problem text to math expressions, by learning to select the relevant declarative knowledge for each operation of the solution expression. This provides a way to handle multiple concepts in the same problem while, at the same time, supporting interpretability of the answer expression. Our method models the mapping to declarative knowledge as a latent variable, thus removing the need for expensive annotations. Experimental evaluation suggests that our domain knowledge based solver outperforms all other systems, and that it generalizes better in the realistic case where the training data it is exposed to is biased in a different way than the test data.","pages":"159--172","doi":"10.1162\/tacl_a_00012","url":"https:\/\/www.aclweb.org\/anthology\/Q18-1012","year":"2018","volume":"6","journal":"Transactions of the Association for Computational Linguistics"},{"id":"Q18-1013","title":"Video Captioning with Multi-Faceted Attention","authors":["Long, Xiang","Gan, Chuang","de Melo, Gerard"],"emails":["longx13@mails.tsinghua.edu.cn","ganchuang1990@gmail.com","gdm@demelo.org"],"author_id":["xiang-long","chuang-gan","gerard-de-melo"],"abstract":"Video captioning has attracted an increasing amount of interest, due in part to its potential for improved accessibility and information retrieval. While existing methods rely on different kinds of visual features and model architectures, they do not make full use of pertinent semantic cues. We present a unified and extensible framework to jointly leverage multiple sorts of visual features and semantic attributes. Our novel architecture builds on LSTMs with two multi-faceted attention layers. These first learn to automatically select the most salient visual features or semantic attributes, and then yield overall representations for the input and output of the sentence generation component via custom feature scaling operations. Experimental results on the challenging MSVD and MSR-VTT datasets show that our framework outperforms previous work and performs robustly even in the presence of added noise to the features and attributes.","pages":"173--184","doi":"10.1162\/tacl_a_00013","url":"https:\/\/www.aclweb.org\/anthology\/Q18-1013","year":"2018","volume":"6","journal":"Transactions of the Association for Computational Linguistics"},{"id":"Q18-1014","title":"Unsupervised Word Mapping Using Structural Similarities in Monolingual Embeddings","authors":["Aldarmaki, Hanan","Mohan, Mahesh","Diab, Mona"],"emails":["aldarmaki@gwu.edu","mahesh_mohan@gwu.edu","mtdiab@gwu.edu"],"author_id":["hanan-aldarmaki","mahesh-mohan","mona-diab"],"abstract":"Most existing methods for automatic bilingual dictionary induction rely on prior alignments between the source and target languages, such as parallel corpora or seed dictionaries. For many language pairs, such supervised alignments are not readily available. We propose an unsupervised approach for learning a bilingual dictionary for a pair of languages given their independently-learned monolingual word embeddings. The proposed method exploits local and global structures in monolingual vector spaces to align them such that similar words are mapped to each other. We show empirically that the performance of bilingual correspondents that are learned using our proposed unsupervised method is comparable to that of using supervised bilingual correspondents from a seed dictionary.","pages":"185--196","doi":"10.1162\/tacl_a_00014","url":"https:\/\/www.aclweb.org\/anthology\/Q18-1014","year":"2018","volume":"6","journal":"Transactions of the Association for Computational Linguistics"},{"id":"Q18-1015","title":"Knowledge Completion for Generics using Guided Tensor Factorization","authors":["Sedghi, Hanie","Sabharwal, Ashish"],"emails":["hsedghi@google.com",""],"author_id":["hanie-sedghi","ashish-sabharwal"],"abstract":"Given a knowledge base or KB containing (noisy) facts about common nouns or generics, such as {``}all trees produce oxygen{''} or {``}some animals live in forests{''}, we consider the problem of inferring additional such facts at a precision similar to that of the starting KB. Such KBs capture general knowledge about the world, and are crucial for various applications such as question answering. Different from commonly studied named entity KBs such as Freebase, generics KBs involve quantification, have more complex underlying regularities, tend to be more incomplete, and violate the commonly used locally closed world assumption (LCWA). We show that existing KB completion methods struggle with this new task, and present the first approach that is successful. Our results demonstrate that external information, such as relation schemas and entity taxonomies, if used appropriately, can be a surprisingly powerful tool in this setting. First, our simple yet effective knowledge guided tensor factorization approach achieves state-of-the-art results on two generics KBs (80{\\%} precise) for science, doubling their size at 74{\\%}{--}86{\\%} precision. Second, our novel taxonomy guided, submodular, active learning method for collecting annotations about rare entities (e.g., oriole, a bird) is 6x more effective at inferring further new facts about them than multiple active learning baselines.","pages":"197--210","doi":"10.1162\/tacl_a_00015","url":"https:\/\/www.aclweb.org\/anthology\/Q18-1015","year":"2018","volume":"6","journal":"Transactions of the Association for Computational Linguistics"},{"id":"Q18-1016","title":"Unsupervised Grammar Induction with Depth-bounded {PCFG}","authors":["Jin, Lifeng","Doshi-Velez, Finale","Miller, Timothy","Schuler, William","Schwartz, Lane"],"emails":["jin.544@osu.edu","finale@seas.harvard.edu","timothy.miller@childrens.harvard.edu","schuler@ling.osu.edu","lanes@illinois.edu"],"author_id":["lifeng-jin","finale-doshi-velez","timothy-miller","william-schuler","lane-schwartz"],"abstract":"There has been recent interest in applying cognitively- or empirically-motivated bounds on recursion depth to limit the search space of grammar induction models (Ponvert et al., 2011; Noji and Johnson, 2016; Shain et al., 2016). This work extends this depth-bounding approach to probabilistic context-free grammar induction (DB-PCFG), which has a smaller parameter space than hierarchical sequence models, and therefore more fully exploits the space reductions of depth-bounding. Results for this model on grammar acquisition from transcribed child-directed speech and newswire text exceed or are competitive with those of other models when evaluated on parse accuracy. Moreover, grammars acquired from this model demonstrate a consistent use of category labels, something which has not been demonstrated by other acquisition models.","pages":"211--224","doi":"10.1162\/tacl_a_00016","url":"https:\/\/www.aclweb.org\/anthology\/Q18-1016","year":"2018","volume":"6","journal":"Transactions of the Association for Computational Linguistics"},{"id":"Q18-1017","title":"Scheduled Multi-Task Learning: From Syntax to Translation","authors":["Kiperwasser, Eliyahu","Ballesteros, Miguel"],"emails":["elikip@gmail.com","miguel.ballesteros@ibm.com"],"author_id":["eliyahu-kiperwasser","miguel-ballesteros"],"abstract":"Neural encoder-decoder models of machine translation have achieved impressive results, while learning linguistic knowledge of both the source and target languages in an implicit end-to-end manner. We propose a framework in which our model begins learning syntax and translation interleaved, gradually putting more focus on translation. Using this approach, we achieve considerable improvements in terms of BLEU score on relatively large parallel corpus (WMT14 English to German) and a low-resource (WIT German to English) setup.","pages":"225--240","doi":"10.1162\/tacl_a_00017","url":"https:\/\/www.aclweb.org\/anthology\/Q18-1017","year":"2018","volume":"6","journal":"Transactions of the Association for Computational Linguistics"},{"id":"Q18-1018","title":"Questionable Answers in Question Answering Research: Reproducibility and Variability of Published Results","authors":["Crane, Matt"],"emails":["matt.crane@uwaterloo.ca"],"author_id":["matt-crane"],"abstract":"{``}Based on theoretical reasoning it has been suggested that the reliability of findings published in the scientific literature decreases with the popularity of a research field{''} (Pfeiffer and Hoffmann, 2009). As we know, deep learning is very popular and the ability to reproduce results is an important part of science. There is growing concern within the deep learning community about the reproducibility of results that are presented. In this paper we present a number of controllable, yet unreported, effects that can substantially change the effectiveness of a sample model, and thusly the reproducibility of those results. Through these environmental effects we show that the commonly held belief that distribution of source code is all that is needed for reproducibility is not enough. Source code without a reproducible environment does not mean anything at all. In addition the range of results produced from these effects can be larger than the majority of incremental improvement reported.","pages":"241--252","doi":"10.1162\/tacl_a_00018","url":"https:\/\/www.aclweb.org\/anthology\/Q18-1018","year":"2018","volume":"6","journal":"Transactions of the Association for Computational Linguistics"},{"id":"Q18-1019","title":"Do latent tree learning models identify meaningful structure in sentences?","authors":["Williams, Adina","Drozdov, Andrew","Bowman, Samuel R."],"emails":["adinawilliams@nyu.edu","andrew.drozdov@nyu.edu","bowman@nyu.edu"],"author_id":["adina-williams","andrew-drozdov","samuel-bowman"],"abstract":"Recent work on the problem of latent tree learning has made it possible to train neural networks that learn to both parse a sentence and use the resulting parse to interpret the sentence, all without exposure to ground-truth parse trees at training time. Surprisingly, these models often perform better at sentence understanding tasks than models that use parse trees from conventional parsers. This paper aims to investigate what these latent tree learning models learn. We replicate two such models in a shared codebase and find that (i) only one of these models outperforms conventional tree-structured models on sentence classification, (ii) its parsing strategies are not especially consistent across random restarts, (iii) the parses it produces tend to be shallower than standard Penn Treebank (PTB) parses, and (iv) they do not resemble those of PTB or any other semantic or syntactic formalism that the authors are aware of.","pages":"253--267","doi":"10.1162\/tacl_a_00019","url":"https:\/\/www.aclweb.org\/anthology\/Q18-1019","year":"2018","volume":"6","journal":"Transactions of the Association for Computational Linguistics"},{"id":"Q18-1020","title":"Bootstrap Domain-Specific Sentiment Classifiers from Unlabeled Corpora","authors":["Mudinas, Andrius","Zhang, Dell","Levene, Mark"],"emails":["andrius@dcs.bbk.ac.uk","dell.z@ieee.org","mark@dcs.bbk.ac.uk"],"author_id":["andrius-mudinas","dell-zhang","mark-levene"],"abstract":"There is often the need to perform sentiment classification in a particular domain where no labeled document is available. Although we could make use of a general-purpose off-the-shelf sentiment classifier or a pre-built one for a different domain, the effectiveness would be inferior. In this paper, we explore the possibility of building domain-specific sentiment classifiers with unlabeled documents only. Our investigation indicates that in the word embeddings learned from the unlabeled corpus of a given domain, the distributed word representations (vectors) for opposite sentiments form distinct clusters, though those clusters are not transferable across domains. Exploiting such a clustering structure, we are able to utilize machine learning algorithms to induce a quality domain-specific sentiment lexicon from just a few typical sentiment words ({``}seeds{''}). An important finding is that simple linear model based supervised learning algorithms (such as linear SVM) can actually work better than more sophisticated semi-supervised\/transductive learning algorithms which represent the state-of-the-art technique for sentiment lexicon induction. The induced lexicon could be applied directly in a lexicon-based method for sentiment classification, but a higher performance could be achieved through a two-phase bootstrapping method which uses the induced lexicon to assign positive\/negative sentiment scores to unlabeled documents first, a nd t hen u ses those documents found to have clear sentiment signals as pseudo-labeled examples to train a document sentiment classifier v ia supervised learning algorithms (such as LSTM). On several benchmark datasets for document sentiment classification, our end-to-end pipelined approach which is overall unsupervised (except for a tiny set of seed words) outperforms existing unsupervised approaches and achieves an accuracy comparable to that of fully supervised approaches.","pages":"269--285","doi":"10.1162\/tacl_a_00020","url":"https:\/\/www.aclweb.org\/anthology\/Q18-1020","year":"2018","volume":"6","journal":"Transactions of the Association for Computational Linguistics"},{"id":"Q18-1021","title":"Constructing Datasets for Multi-hop Reading Comprehension Across Documents","authors":["Welbl, Johannes","Stenetorp, Pontus","Riedel, Sebastian"],"emails":["j.welbl@cs.ucl.ac.uk","p.stenetorp@cs.ucl.ac.uk","s.riedel@cs.ucl.ac.uk"],"author_id":["johannes-welbl","pontus-stenetorp","sebastian-riedel"],"abstract":"Most Reading Comprehension methods limit themselves to queries which can be answered using a single sentence, paragraph, or document. Enabling models to combine disjoint pieces of textual evidence would extend the scope of machine comprehension methods, but currently no resources exist to train and test this capability. We propose a novel task to encourage the development of models for text understanding across multiple documents and to investigate the limits of existing methods. In our task, a model learns to seek and combine evidence {---} effectively performing multihop, alias multi-step, inference. We devise a methodology to produce datasets for this task, given a collection of query-answer pairs and thematically linked documents. Two datasets from different domains are induced, and we identify potential pitfalls and devise circumvention strategies. We evaluate two previously proposed competitive models and find that one can integrate information across documents. However, both models struggle to select relevant information; and providing documents guaranteed to be relevant greatly improves their performance. While the models outperform several strong baselines, their best accuracy reaches 54.5{\\%} on an annotated test set, compared to human performance at 85.0{\\%}, leaving ample room for improvement.","pages":"287--302","doi":"10.1162\/tacl_a_00021","url":"https:\/\/www.aclweb.org\/anthology\/Q18-1021","year":"2018","volume":"6","journal":"Transactions of the Association for Computational Linguistics"},{"id":"Q18-1022","title":"Leveraging Orthographic Similarity for Multilingual Neural Transliteration","authors":["Kunchukuttan, Anoop","Khapra, Mitesh","Singh, Gurneet","Bhattacharyya, Pushpak"],"emails":["anoopk@cse.iitb.ac.in","miteshk@cse.iitm.ac.in","garry@cse.iitm.ac.in","pb@cse.iitb.ac.in"],"author_id":["anoop-kunchukuttan","mitesh-m-khapra","gurneet-singh","pushpak-bhattacharyya"],"abstract":"We address the task of joint training of transliteration models for multiple language pairs (multilingual transliteration). This is an instance of multitask learning, where individual tasks (language pairs) benefit from sharing knowledge with related tasks. We focus on transliteration involving related tasks i.e., languages sharing writing systems and phonetic properties (orthographically similar languages). We propose a modified neural encoder-decoder model that maximizes parameter sharing across language pairs in order to effectively leverage orthographic similarity. We show that multilingual transliteration significantly outperforms bilingual transliteration in different scenarios (average increase of 58{\\%} across a variety of languages we experimented with). We also show that multilingual transliteration models can generalize well to languages\/language pairs not encountered during training and hence perform well on the zeroshot transliteration task. We show that further improvements can be achieved by using phonetic feature input.","pages":"303--316","doi":"10.1162\/tacl_a_00022","url":"https:\/\/www.aclweb.org\/anthology\/Q18-1022","year":"2018","volume":"6","journal":"Transactions of the Association for Computational Linguistics"},{"id":"Q18-1023","title":"The {N}arrative{QA} Reading Comprehension Challenge","authors":["Ko{\\v{c}}isk{\\'y}, Tom{\\'a}{\\v{s}}","Schwarz, Jonathan","Blunsom, Phil","Dyer, Chris","Hermann, Karl Moritz","Melis, G{\\'a}bor","Grefenstette, Edward"],"emails":["tkocisky@google.com","schwarzjn@google.com","pblunsom@google.com","cdyer@google.com","kmh@google.com","melisgl@google.com","etg@google.com"],"author_id":["tomas-kocisky","jonathan-schwarz","phil-blunsom","chris-dyer","karl-moritz-hermann","gabor-melis","edward-grefenstette"],"abstract":"Reading comprehension (RC){---}in contrast to information retrieval{---}requires integrating information and reasoning about events, entities, and their relations across a full document. Question answering is conventionally used to assess RC ability, in both artificial agents and children learning to read. However, existing RC datasets and tasks are dominated by questions that can be solved by selecting answers using superficial information (e.g., local context similarity or global term frequency); they thus fail to test for the essential integrative aspect of RC. To encourage progress on deeper comprehension of language, we present a new dataset and set of tasks in which the reader must answer questions about stories by reading entire books or movie scripts. These tasks are designed so that successfully answering their questions requires understanding the underlying narrative rather than relying on shallow pattern matching or salience. We show that although humans solve the tasks easily, standard RC models struggle on the tasks presented here. We provide an analysis of the dataset and the challenges it presents.","pages":"317--328","doi":"10.1162\/tacl_a_00023","url":"https:\/\/www.aclweb.org\/anthology\/Q18-1023","year":"2018","volume":"6","journal":"Transactions of the Association for Computational Linguistics"},{"id":"Q18-1024","title":"Native Language Cognate Effects on Second Language Lexical Choice","authors":["Rabinovich, Ella","Tsvetkov, Yulia","Wintner, Shuly"],"emails":["ellarabi@gmail.com","ytsvetko@cs.cmu.edu","shuly@cs.haifa.ac.il"],"author_id":["ella-rabinovich","yulia-tsvetkov","shuly-wintner"],"abstract":"We present a computational analysis of cognate effects on the spontaneous linguistic productions of advanced non-native speakers. Introducing a large corpus of highly competent non-native English speakers, and using a set of carefully selected lexical items, we show that the lexical choices of non-natives are affected by cognates in their native language. This effect is so powerful that we are able to reconstruct the phylogenetic language tree of the Indo-European language family solely from the frequencies of specific lexical items in the English of authors with various native languages. We quantitatively analyze non-native lexical choice, highlighting cognate facilitation as one of the important phenomena shaping the language of non-native speakers.","pages":"329--342","doi":"10.1162\/tacl_a_00024","url":"https:\/\/www.aclweb.org\/anthology\/Q18-1024","year":"2018","volume":"6","journal":"Transactions of the Association for Computational Linguistics"},{"id":"Q18-1025","title":"From Characters to Time Intervals: New Paradigms for Evaluation and Neural Parsing of Time Normalizations","authors":["Laparra, Egoitz","Xu, Dongfang","Bethard, Steven"],"emails":["laparra@email.arizona.edu","dongfangxu9@email.arizona.edu","bethard@email.arizona.edu"],"author_id":["egoitz-laparra","dongfang-xu","steven-bethard"],"abstract":"This paper presents the first model for time normalization trained on the SCATE corpus. In the SCATE schema, time expressions are annotated as a semantic composition of time entities. This novel schema favors machine learning approaches, as it can be viewed as a semantic parsing task. In this work, we propose a character level multi-output neural network that outperforms previous state-of-the-art built on the TimeML schema. To compare predictions of systems that follow both SCATE and TimeML, we present a new scoring metric for time intervals. We also apply this new metric to carry out a comparative analysis of the annotations of both schemes in the same corpus.","pages":"343--356","doi":"10.1162\/tacl_a_00025","url":"https:\/\/www.aclweb.org\/anthology\/Q18-1025","year":"2018","volume":"6","journal":"Transactions of the Association for Computational Linguistics"},{"id":"Q18-1026","title":"Finding Convincing Arguments Using Scalable {B}ayesian Preference Learning","authors":["Simpson, Edwin","Gurevych, Iryna"],"emails":["simpson@ukp.informatik.tu-darmstadt.de","gurevych@ukp.informatik.tu-darmstadt.de"],"author_id":["edwin-simpson","iryna-gurevych"],"abstract":"We introduce a scalable Bayesian preference learning method for identifying convincing arguments in the absence of gold-standard ratings or rankings. In contrast to previous work, we avoid the need for separate methods to perform quality control on training data, predict rankings and perform pairwise classification. Bayesian approaches are an effective solution when faced with sparse or noisy training data, but have not previously been used to identify convincing arguments. One issue is scalability, which we address by developing a stochastic variational inference method for Gaussian process (GP) preference learning. We show how our method can be applied to predict argument convincingness from crowdsourced data, outperforming the previous state-of-the-art, particularly when trained with small amounts of unreliable data. We demonstrate how the Bayesian approach enables more effective active learning, thereby reducing the amount of data required to identify convincing arguments for new users and domains. While word embeddings are principally used with neural networks, our results show that word embeddings in combination with linguistic features also benefit GPs when predicting argument convincingness.","pages":"357--371","doi":"10.1162\/tacl_a_00026","url":"https:\/\/www.aclweb.org\/anthology\/Q18-1026","year":"2018","volume":"6","journal":"Transactions of the Association for Computational Linguistics"},{"id":"Q18-1027","title":"Polite Dialogue Generation Without Parallel Data","authors":["Niu, Tong","Bansal, Mohit"],"emails":["tongn@cs.unc.edu","mbansal@cs.unc.edu"],"author_id":["tong-niu","mohit-bansal"],"abstract":"Stylistic dialogue response generation, with valuable applications in personality-based conversational agents, is a challenging task because the response needs to be fluent, contextually-relevant, as well as paralinguistically accurate. Moreover, parallel datasets for regular-to-stylistic pairs are usually unavailable. We present three weakly-supervised models that can generate diverse, polite (or rude) dialogue responses without parallel data. Our late fusion model (Fusion) merges the decoder of an encoder-attention-decoder dialogue model with a language model trained on stand-alone polite utterances. Our label-finetuning (LFT) model prepends to each source sequence a politeness-score scaled label (predicted by our state-of-the-art politeness classifier) during training, and at test time is able to generate polite, neutral, and rude responses by simply scaling the label embedding by the corresponding score. Our reinforcement learning model (Polite-RL) encourages politeness generation by assigning rewards proportional to the politeness classifier score of the sampled response. We also present two retrievalbased, polite dialogue model baselines. Human evaluation validates that while the Fusion and the retrieval-based models achieve politeness with poorer context-relevance, the LFT and Polite-RL models can produce significantly more polite responses without sacrificing dialogue quality.","pages":"373--389","doi":"10.1162\/tacl_a_00027","url":"https:\/\/www.aclweb.org\/anthology\/Q18-1027","year":"2018","volume":"6","journal":"Transactions of the Association for Computational Linguistics"},{"id":"Q18-1028","title":"Measuring the Evolution of a Scientific Field through Citation Frames","authors":["Jurgens, David","Kumar, Srijan","Hoover, Raine","McFarland, Dan","Jurafsky, Dan"],"emails":["jurgens@umich.edu","srijan@stanford.edu","raine@stanford.edu","dmcfarla@stanford.edu","jurafsky@stanford.edu"],"author_id":["david-jurgens","srijan-kumar","raine-hoover","dan-mcfarland","dan-jurafsky"],"abstract":"Citations have long been used to characterize the state of a scientific field and to identify influential works. However, writers use citations for different purposes, and this varied purpose influences uptake by future scholars. Unfortunately, our understanding of how scholars use and frame citations has been limited to small-scale manual citation analysis of individual papers. We perform the largest behavioral study of citations to date, analyzing how scientific works frame their contributions through different types of citations and how this framing affects the field as a whole. We introduce a new dataset of nearly 2,000 citations annotated for their function, and use it to develop a state-of-the-art classifier and label the papers of an entire field: Natural Language Processing. We then show how differences in framing affect scientific uptake and reveal the evolution of the publication venues and the field as a whole. We demonstrate that authors are sensitive to discourse structure and publication venue when citing, and that how a paper frames its work through citations is predictive of the citation count it will receive. Finally, we use changes in citation framing to show that the field of NLP is undergoing a significant increase in consensus.","pages":"391--406","doi":"10.1162\/tacl_a_00028","url":"https:\/\/www.aclweb.org\/anthology\/Q18-1028","year":"2018","volume":"6","journal":"Transactions of the Association for Computational Linguistics"},{"id":"Q18-1029","title":"Learning to Remember Translation History with a Continuous Cache","authors":["Tu, Zhaopeng","Liu, Yang","Shi, Shuming","Zhang, Tong"],"emails":["zptu@tencent.com","liuyang2011@tsinghua.edu.cn","shumingshi@tencent.com","bradymzhang@tencent.com"],"author_id":["zhaopeng-tu","yang-liu-ict","shuming-shi","tong-zhang"],"abstract":"Existing neural machine translation (NMT) models generally translate sentences in isolation, missing the opportunity to take advantage of document-level information. In this work, we propose to augment NMT models with a very light-weight cache-like memory network, which stores recent hidden representations as translation history. The probability distribution over generated words is updated online depending on the translation history retrieved from the memory, endowing NMT models with the capability to dynamically adapt over time. Experiments on multiple domains with different topics and styles show the effectiveness of the proposed approach with negligible impact on the computational cost.","pages":"407--420","doi":"10.1162\/tacl_a_00029","url":"https:\/\/www.aclweb.org\/anthology\/Q18-1029","year":"2018","volume":"6","journal":"Transactions of the Association for Computational Linguistics"},{"id":"Q18-1030","title":"Generating Sentences by Editing Prototypes","authors":["Guu, Kelvin","Hashimoto, Tatsunori B.","Oren, Yonatan","Liang, Percy"],"emails":["","yan.shao@lingfil.uu.se","joakim.nivre@lingfil.uu.se","christian.hardmeier@lingfil.uu.se"],"author_id":["kelvin-guu","tatsunori-b-hashimoto","yonatan-oren","percy-liang"],"abstract":"We propose a new generative language model for sentences that first samples a prototype sentence from the training corpus and then edits it into a new sentence. Compared to traditional language models that generate from scratch either left-to-right or by first sampling a latent sentence vector, our prototype-then-edit model improves perplexity on language modeling and generates higher quality outputs according to human evaluation. Furthermore, the model gives rise to a latent edit vector that captures interpretable semantics such as sentence similarity and sentence-level analogies.","pages":"437--450","doi":"10.1162\/tacl_a_00030","url":"https:\/\/www.aclweb.org\/anthology\/Q18-1030","year":"2018","volume":"6","journal":"Transactions of the Association for Computational Linguistics"},{"id":"Q18-1031","title":"Detecting Institutional Dialog Acts in Police Traffic Stops","authors":["Prabhakaran, Vinodkumar","Griffiths, Camilla","Su, Hang","Verma, Prateek","Morgan, Nelson","Eberhardt, Jennifer L.","Jurafsky, Dan"],"emails":["kguu@stanford.edu","","pliang@cs.stanford.edu","","thashim@stanford.edu","","yonatano@stanford.edu"],"author_id":["vinodkumar-prabhakaran","camilla-griffiths","hang-su","prateek-verma","nelson-morgan","jennifer-l-eberhardt","dan-jurafsky"],"abstract":"We apply computational dialog methods to police body-worn camera footage to model conversations between police officers and community members in traffic stops. Relying on the theory of institutional talk, we develop a labeling scheme for police speech during traffic stops, and a tagger to detect institutional dialog acts (Reasons, Searches, Offering Help) from transcribed text at the turn (78{\\%} F-score) and stop (89{\\%} F-score) level. We then develop speech recognition and segmentation algorithms to detect these acts at the stop level from raw camera audio (81{\\%} F-score, with even higher accuracy for crucial acts like conveying the reason for the stop). We demonstrate that the dialog structures produced by our tagger could reveal whether officers follow law enforcement norms like introducing themselves, explaining the reason for the stop, and asking permission for searches. This work may therefore inform and aid efforts to ensure the procedural justice of police-community interactions.","pages":"467--481","doi":"10.1162\/tacl_a_00031","url":"https:\/\/www.aclweb.org\/anthology\/Q18-1031","year":"2018","volume":"6","journal":"Transactions of the Association for Computational Linguistics"},{"id":"Q18-1032","title":"Language Modeling for Morphologically Rich Languages: Character-Aware Modeling for Word-Level Prediction","authors":["Gerz, Daniela","Vuli{\\'c}, Ivan","Ponti, Edoardo","Naradowsky, Jason","Reichart, Roi","Korhonen, Anna"],"emails":["dsg40@cam.ac.uk","iv250@cam.ac.uk","ep490@cam.ac.uk","3narad@jhu.edu","2roiri@ie.technion.ac.il","alk23@cam.ac.uk"],"author_id":["daniela-gerz","ivan-vulic","edoardo-ponti","jason-naradowsky","roi-reichart","anna-korhonen"],"abstract":"Neural architectures are prominent in the construction of language models (LMs). However, word-level prediction is typically agnostic of subword-level information (characters and character sequences) and operates over a closed vocabulary, consisting of a limited word set. Indeed, while subword-aware models boost performance across a variety of NLP tasks, previous work did not evaluate the ability of these models to assist next-word prediction in language modeling tasks. Such subword-level informed models should be particularly effective for morphologically-rich languages (MRLs) that exhibit high type-to-token ratios. In this work, we present a large-scale LM study on 50 typologically diverse languages covering a wide variety of morphological systems, and offer new LM benchmarks to the community, while considering subword-level information. The main technical contribution of our work is a novel method for injecting subword-level information into semantic word vectors, integrated into the neural language modeling training, to facilitate word-level prediction. We conduct experiments in the LM setting where the number of infrequent words is large, and demonstrate strong perplexity gains across our 50 languages, especially for morphologically-rich languages. Our code and data sets are publicly available.","pages":"451--465","doi":"10.1162\/tacl_a_00032","url":"https:\/\/www.aclweb.org\/anthology\/Q18-1032","year":"2018","volume":"6","journal":"Transactions of the Association for Computational Linguistics"},{"id":"Q18-1033","title":"Universal Word Segmentation: Implementation and Interpretation","authors":["Shao, Yan","Hardmeier, Christian","Nivre, Joakim"],"emails":["suhang3240@gmail.com","camillag@stanford.edu","vinodkpg@stanford.edu"],"author_id":["yan-shao","christian-hardmeier","joakim-nivre"],"abstract":"Word segmentation is a low-level NLP task that is non-trivial for a considerable number of languages. In this paper, we present a sequence tagging framework and apply it to word segmentation for a wide range of languages with different writing systems and typological characteristics. Additionally, we investigate the correlations between various typological factors and word segmentation accuracy. The experimental results indicate that segmentation accuracy is positively related to word boundary markers and negatively to the number of unique non-segmental terms. Based on the analysis, we design a small set of language-specific settings and extensively evaluate the segmentation system on the Universal Dependencies datasets. Our model obtains state-of-the-art accuracies on all the UD languages. It performs substantially better on languages that are non-trivial to segment, such as Chinese, Japanese, Arabic and Hebrew, when compared to previous work.","pages":"421--435","doi":"10.1162\/tacl_a_00033","url":"https:\/\/www.aclweb.org\/anthology\/Q18-1033","year":"2018","volume":"6","journal":"Transactions of the Association for Computational Linguistics"},{"id":"Q18-1034","title":"Linear Algebraic Structure of Word Senses, with Applications to Polysemy","authors":["Arora, Sanjeev","Li, Yuanzhi","Liang, Yingyu","Ma, Tengyu","Risteski, Andrej"],"emails":["arora@cs.princeton.edu","yuanzhil@cs.princeton.edu","yingyul@cs.princeton.edu","tengyu@cs.princeton.edu","risteski@cs.princeton.edu"],"author_id":["sanjeev-arora","yuanzhi-li","yingyu-liang","tengyu-ma","andrej-risteski"],"abstract":"Word embeddings are ubiquitous in NLP and information retrieval, but it is unclear what they represent when the word is polysemous. Here it is shown that multiple word senses reside in linear superposition within the word embedding and simple sparse coding can recover vectors that approximately capture the senses. The success of our approach, which applies to several embedding methods, is mathematically explained using a variant of the random walk on discourses model (Arora et al., 2016). A novel aspect of our technique is that each extracted word sense is accompanied by one of about 2000 {``}discourse atoms{''} that gives a succinct description of which other words co-occur with that word sense. Discourse atoms can be of independent interest, and make the method potentially more useful. Empirical tests are used to verify and support the theory.","pages":"483--495","doi":"10.1162\/tacl_a_00034","url":"https:\/\/www.aclweb.org\/anthology\/Q18-1034","year":"2018","volume":"6","journal":"Transactions of the Association for Computational Linguistics"},{"id":"Q18-1035","title":"Low-Rank {RNN} Adaptation for Context-Aware Language Modeling","authors":["Jaech, Aaron","Ostendorf, Mari"],"emails":["ajaech@uw.edu","ostendor@uw.edu"],"author_id":["aaron-jaech","mari-ostendorf"],"abstract":"A context-aware language model uses location, user and\/or domain metadata (context) to adapt its predictions. In neural language models, context information is typically represented as an embedding and it is given to the RNN as an additional input, which has been shown to be useful in many applications. We introduce a more powerful mechanism for using context to adapt an RNN by letting the context vector control a low-rank transformation of the recurrent layer weight matrix. Experiments show that allowing a greater fraction of the model parameters to be adjusted has benefits in terms of perplexity and classification for several different types of context.","pages":"497--510","doi":"10.1162\/tacl_a_00035","url":"https:\/\/www.aclweb.org\/anthology\/Q18-1035","year":"2018","volume":"6","journal":"Transactions of the Association for Computational Linguistics"},{"id":"Q18-1036","title":"Neural Lattice Language Models","authors":["Buckman, Jacob","Neubig, Graham"],"emails":["jacobbuckman@gmail.com","gneubig@cs.cmu.edu"],"author_id":["jacob-buckman","graham-neubig"],"abstract":"In this work, we propose a new language modeling paradigm that has the ability to perform both prediction and moderation of information flow at multiple granularities: neural lattice language models. These models construct a lattice of possible paths through a sentence and marginalize across this lattice to calculate sequence probabilities or optimize parameters. This approach allows us to seamlessly incorporate linguistic intuitions {---} including polysemy and the existence of multiword lexical items {---} into our language model. Experiments on multiple language modeling tasks show that English neural lattice language models that utilize polysemous embeddings are able to improve perplexity by 9.95{\\%} relative to a word-level baseline, and that a Chinese model that handles multi-character tokens is able to improve perplexity by 20.94{\\%} relative to a character-level baseline.","pages":"529--541","doi":"10.1162\/tacl_a_00036","url":"https:\/\/www.aclweb.org\/anthology\/Q18-1036","year":"2018","volume":"6","journal":"Transactions of the Association for Computational Linguistics"},{"id":"Q18-1037","title":"Planning, Inference and Pragmatics in Sequential Language Games","authors":["Khani, Fereshte","Goodman, Noah D.","Liang, Percy"],"emails":["fereshte@stanford.edu","ngoodman@stanford.edu","pliang@cs.stanford.edu"],"author_id":["fereshte-khani","noah-goodman","percy-liang"],"abstract":"We study sequential language games in which two players, each with private information, communicate to achieve a common goal. In such games, a successful player must (i) infer the partner{'}s private information from the partner{'}s messages, (ii) generate messages that are most likely to help with the goal, and (iii) reason pragmatically about the partner{'}s strategy. We propose a model that captures all three characteristics and demonstrate their importance in capturing human behavior on a new goal-oriented dataset we collected using crowdsourcing.","pages":"543--555","doi":"10.1162\/tacl_a_00037","url":"https:\/\/www.aclweb.org\/anthology\/Q18-1037","year":"2018","volume":"6","journal":"Transactions of the Association for Computational Linguistics"},{"id":"Q18-1038","title":"Probabilistic Verb Selection for Data-to-Text Generation","authors":["Zhang, Dell","Yuan, Jiahao","Wang, Xiaoling","Foster, Adam"],"emails":["1dell.z@ieee.org","","2xlwang@sei.ecnu.edu.cn",""],"author_id":["dell-zhang","jiahao-yuan","xiaoling-wang","adam-foster"],"abstract":"In data-to-text Natural Language Generation (NLG) systems, computers need to find the right words to describe phenomena seen in the data. This paper focuses on the problem of choosing appropriate verbs to express the direction and magnitude of a percentage change (e.g., in stock prices). Rather than simply using the same verbs again and again, we present a principled data-driven approach to this problem based on Shannon{'}s noisy-channel model so as to bring variation and naturalness into the generated text. Our experiments on three large-scale real-world news corpora demonstrate that the proposed probabilistic model can be learned to accurately imitate human authors{'} pattern of usage around verbs, outperforming the state-of-the-art method significantly.","pages":"511--527","doi":"10.1162\/tacl_a_00038","url":"https:\/\/www.aclweb.org\/anthology\/Q18-1038","year":"2018","volume":"6","journal":"Transactions of the Association for Computational Linguistics"},{"id":"Q18-1039","title":"Adversarial Deep Averaging Networks for Cross-Lingual Sentiment Classification","authors":["Chen, Xilun","Sun, Yu","Athiwaratkun, Ben","Cardie, Claire","Weinberger, Kilian"],"emails":["xlchen@cs.cornell.edu","ys646@cornell.edu","pa338@cornell.edu","cardie@cs.cornell.edu","kqw4@cornell.edu"],"author_id":["xilun-chen","yu-sun","ben-athiwaratkun","claire-cardie","kilian-weinberger"],"abstract":"In recent years great success has been achieved in sentiment classification for English, thanks in part to the availability of copious annotated resources. Unfortunately, most languages do not enjoy such an abundance of labeled data. To tackle the sentiment classification problem in low-resource languages without adequate annotated data, we propose an Adversarial Deep Averaging Network (ADAN1) to transfer the knowledge learned from labeled data on a resource-rich source language to low-resource languages where only unlabeled data exist. ADAN has two discriminative branches: a sentiment classifier and an adversarial language discriminator. Both branches take input from a shared feature extractor to learn hidden representations that are simultaneously indicative for the classification task and invariant across languages. Experiments on Chinese and Arabic sentiment classification demonstrate that ADAN significantly outperforms state-of-the-art systems.","pages":"557--570","doi":"10.1162\/tacl_a_00039","url":"https:\/\/www.aclweb.org\/anthology\/Q18-1039","year":"2018","volume":"6","journal":"Transactions of the Association for Computational Linguistics"},{"id":"Q18-1040","title":"Comparing {B}ayesian Models of Annotation","authors":["Paun, Silviu","Carpenter, Bob","Chamberlain, Jon","Hovy, Dirk","Kruschwitz, Udo","Poesio, Massimo"],"emails":["","","","","",""],"author_id":["silviu-paun","bob-carpenter","jon-chamberlain","dirk-hovy","udo-kruschwitz","massimo-poesio"],"abstract":"The analysis of crowdsourced annotations in natural language processing is concerned with identifying (1) gold standard labels, (2) annotator accuracies and biases, and (3) item difficulties and error patterns. Traditionally, majority voting was used for 1, and coefficients of agreement for 2 and 3. Lately, model-based analysis of corpus annotations have proven better at all three tasks. But there has been relatively little work comparing them on the same datasets. This paper aims to fill this gap by analyzing six models of annotation, covering different approaches to annotator ability, item difficulty, and parameter pooling (tying) across annotators and items. We evaluate these models along four aspects: comparison to gold labels, predictive accuracy for new annotations, annotator characterization, and item difficulty, using four datasets with varying degrees of noise in the form of random (spammy) annotators. We conclude with guidelines for model selection, application, and implementation.","pages":"571--585","doi":"10.1162\/tacl_a_00040","url":"https:\/\/www.aclweb.org\/anthology\/Q18-1040","year":"2018","volume":"6","journal":"Transactions of the Association for Computational Linguistics"},{"id":"Q18-1041","title":"Data Statements for Natural Language Processing: Toward Mitigating System Bias and Enabling Better Science","authors":["Bender, Emily M.","Friedman, Batya"],"emails":["ebender@uw.edu","batya@uw.edu"],"author_id":["emily-m-bender","batya-friedman"],"abstract":"In this paper, we propose data statements as a design solution and professional practice for natural language processing technologists, in both research and development. Through the adoption and widespread use of data statements, the field can begin to address critical scientific and ethical issues that result from the use of data from certain populations in the development of technology for other populations. We present a form that data statements can take and explore the implications of adopting them as part of regular practice. We argue that data statements will help alleviate issues related to exclusion and bias in language technology, lead to better precision in claims about how natural language processing research can generalize and thus better engineering results, protect companies from public embarrassment, and ultimately lead to language technology that meets its users in their own preferred linguistic style and furthermore does not misrepresent them to others.","pages":"587--604","doi":"10.1162\/tacl_a_00041","url":"https:\/\/www.aclweb.org\/anthology\/Q18-1041","year":"2018","volume":"6","journal":"Transactions of the Association for Computational Linguistics"},{"id":"Q18-1042","title":"Mind the {GAP}: A Balanced Corpus of Gendered Ambiguous Pronouns","authors":["Webster, Kellie","Recasens, Marta","Axelrod, Vera","Baldridge, Jason"],"emails":["websterk@google.com","recasens@google.com","vaxelrod@google.com","jasonbaldridge@google.com"],"author_id":["kellie-webster","marta-recasens","vera-axelrod","jason-baldridge"],"abstract":"Coreference resolution is an important task for natural language understanding, and the resolution of ambiguous pronouns a longstanding challenge. Nonetheless, existing corpora do not capture ambiguous pronouns in sufficient volume or diversity to accurately indicate the practical utility of models. Furthermore, we find gender bias in existing corpora and systems favoring masculine entities. To address this, we present and release GAP, a gender-balanced labeled corpus of 8,908 ambiguous pronoun{--}name pairs sampled to provide diverse coverage of challenges posed by real-world text. We explore a range of baselines that demonstrate the complexity of the challenge, the best achieving just 66.9{\\%} F1. We show that syntactic structure and continuous neural models provide promising, complementary cues for approaching the challenge.","pages":"605--617","doi":"10.1162\/tacl_a_00240","url":"https:\/\/www.aclweb.org\/anthology\/Q18-1042","year":"2018","volume":"6","journal":"Transactions of the Association for Computational Linguistics"},{"id":"Q18-1043","title":"Exploring Neural Methods for Parsing Discourse Representation Structures","authors":["van Noord, Rik","Abzianidze, Lasha","Toral, Antonio","Bos, Johan"],"emails":["r.i.k.van.noord@rug.nl","l.abzianidze@rug.nl","a.toral.ruiz@rug.nl","johan.bos@rug.nl"],"author_id":["rik-van-noord","lasha-abzianidze","antonio-toral","johan-bos"],"abstract":"Neural methods have had several recent successes in semantic parsing, though they have yet to face the challenge of producing meaning representations based on formal semantics. We present a sequence-to-sequence neural semantic parser that is able to produce Discourse Representation Structures (DRSs) for English sentences with high accuracy, outperforming traditional DRS parsers. To facilitate the learning of the output, we represent DRSs as a sequence of flat clauses and introduce a method to verify that produced DRSs are well-formed and interpretable. We compare models using characters and words as input and see (somewhat surprisingly) that the former performs better than the latter. We show that eliminating variable names from the output using De Bruijn indices increases parser performance. Adding silver training data boosts performance even further.","pages":"619--633","doi":"10.1162\/tacl_a_00241","url":"https:\/\/www.aclweb.org\/anthology\/Q18-1043","year":"2018","volume":"6","journal":"Transactions of the Association for Computational Linguistics"},{"id":"Q18-1044","title":"Integrating Weakly Supervised Word Sense Disambiguation into Neural Machine Translation","authors":["Pu, Xiao","Pappas, Nikolaos","Henderson, James","Popescu-Belis, Andrei"],"emails":["xiao.pu@nuance.com","nikolaos.pappas@idiap.ch","james.henderson@idiap.ch","andrei.popescu-belis@heig-vd.ch"],"author_id":["xiao-pu","nikolaos-pappas","james-henderson","andrei-popescu-belis"],"abstract":"This paper demonstrates that word sense disambiguation (WSD) can improve neural machine translation (NMT) by widening the source context considered when modeling the senses of potentially ambiguous words. We first introduce three adaptive clustering algorithms for WSD, based on k-means, Chinese restaurant processes, and random walks, which are then applied to large word contexts represented in a low-rank space and evaluated on SemEval shared-task data. We then learn word vectors jointly with sense vectors defined by our best WSD method, within a state-of-the-art NMT system. We show that the concatenation of these vectors, and the use of a sense selection mechanism based on the weighted average of sense vectors, outperforms several baselines including sense-aware ones. This is demonstrated by translation on five language pairs. The improvements are more than 1 BLEU point over strong NMT baselines, +4{\\%} accuracy over all ambiguous nouns and verbs, or +20{\\%} when scored manually over several challenging words.","pages":"635--649","doi":"10.1162\/tacl_a_00242","url":"https:\/\/www.aclweb.org\/anthology\/Q18-1044","year":"2018","volume":"6","journal":"Transactions of the Association for Computational Linguistics"},{"id":"Q18-1045","title":"Recurrent Neural Networks in Linguistic Theory: Revisiting Pinker and Prince (1988) and the Past Tense Debate","authors":["Kirov, Christo","Cotterell, Ryan"],"emails":["ckirov1@jhu.edu","ryan.cotterell@jhu.edu"],"author_id":["christo-kirov","ryan-cotterell"],"abstract":"Can advances in NLP help advance cognitive modeling? We examine the role of artificial neural networks, the current state of the art in many common NLP tasks, by returning to a classic case study. In 1986, Rumelhart and McClelland famously introduced a neural architecture that learned to transduce English verb stems to their past tense forms. Shortly thereafter in 1988, Pinker and Prince presented a comprehensive rebuttal of many of Rumelhart and McClelland{'}s claims. Much of the force of their attack centered on the empirical inadequacy of the Rumelhart and McClelland model. Today, however, that model is severely outmoded. We show that the Encoder-Decoder network architectures used in modern NLP systems obviate most of Pinker and Prince{'}s criticisms without requiring any simplification of the past tense mapping problem. We suggest that the empirical performance of modern networks warrants a reexamination of their utility in linguistic and cognitive modeling.","pages":"651--665","doi":"10.1162\/tacl_a_00247","url":"https:\/\/www.aclweb.org\/anthology\/Q18-1045","year":"2018","volume":"6","journal":"Transactions of the Association for Computational Linguistics"},{"id":"Q18-1046","title":"Surface Statistics of an Unknown Language Indicate How to Parse It","authors":["Wang, Dingquan","Eisner, Jason"],"emails":["wdd@cs.jhu.edu","jason@cs.jhu.edu"],"author_id":["dingquan-wang","jason-eisner"],"abstract":"We introduce a novel framework for delexicalized dependency parsing in a new language. We show that useful features of the target language can be extracted automatically from an unparsed corpus, which consists only of gold part-of-speech (POS) sequences. Providing these features to our neural parser enables it to parse sequences like those in the corpus. Strikingly, our system has no supervision in the target language. Rather, it is a multilingual system that is trained end-to-end on a variety of other languages, so it learns a feature extractor that works well. We show experimentally across multiple languages: (1) Features computed from the unparsed corpus improve parsing accuracy. (2) Including thousands of synthetic languages in the training yields further improvement. (3) Despite being computed from unparsed corpora, our learned task-specific features beat previous work{'}s interpretable typological features that require parsed corpora or expert categorization of the language. Our best method improved attachment scores on held-out test languages by an average of 5.6 percentage points over past work that does not inspect the unparsed data (McDonald et al., 2011), and by 20.7 points over past {``}grammar induction{''} work that does not use training languages (Naseem et al., 2010).","pages":"667--685","doi":"10.1162\/tacl_a_00248","url":"https:\/\/www.aclweb.org\/anthology\/Q18-1046","year":"2018","volume":"6","journal":"Transactions of the Association for Computational Linguistics"},{"id":"Q18-1047","title":"Attentive Convolution: Equipping {CNN}s with {RNN}-style Attention Mechanisms","authors":["Yin, Wenpeng","Sch{\\\"u}tze, Hinrich"],"emails":["wenpeng@seas.upenn.edu","inquiries@cislmu.org"],"author_id":["wenpeng-yin","hinrich-schutze"],"abstract":"In NLP, convolutional neural networks (CNNs) have benefited less than recurrent neural networks (RNNs) from attention mechanisms. We hypothesize that this is because the attention in CNNs has been mainly implemented as attentive pooling (i.e., it is applied to pooling) rather than as attentive convolution (i.e., it is integrated into convolution). Convolution is the differentiator of CNNs in that it can powerfully model the higher-level representation of a word by taking into account its local fixed-size context in the input text tx. In this work, we propose an attentive convolution network, ATTCONV. It extends the context scope of the convolution operation, deriving higher-level features for a word not only from local context, but also from information extracted from nonlocal context by the attention mechanism commonly used in RNNs. This nonlocal context can come (i) from parts of the input text tx that are distant or (ii) from extra (i.e., external) contexts ty. Experiments on sentence modeling with zero-context (sentiment analysis), single-context (textual entailment) and multiple-context (claim verification) demonstrate the effectiveness of ATTCONV in sentence representation learning with the incorporation of context. In particular, attentive convolution outperforms attentive pooling and is a strong competitor to popular attentive RNNs.1","pages":"687--702","doi":"10.1162\/tacl_a_00249","url":"https:\/\/www.aclweb.org\/anthology\/Q18-1047","year":"2018","volume":"6","journal":"Transactions of the Association for Computational Linguistics"},{"id":"Q18-1048","title":"Learning Typed Entailment Graphs with Global Soft Constraints","authors":["Hosseini, Mohammad Javad","Chambers, Nathanael","Reddy, Siva","Holt, Xavier R.","Cohen, Shay B.","Johnson, Mark","Steedman, Mark"],"emails":["javad.hosseini@ed.ac.uk","nchamber@usna.edu","sivar@stanford.edu","xavier.ricketts-holt@mq.edu.au","scohen@inf.ed.ac.uk","mark.johnson@mq.edu.au","steedman@inf.ed.ac.uk"],"author_id":["mohammad-javad-hosseini","nathanael-chambers","siva-reddy","xavier-r-holt","shay-b-cohen","mark-johnson","mark-steedman"],"abstract":"This paper presents a new method for learning typed entailment graphs from text. We extract predicate-argument structures from multiple-source news corpora, and compute local distributional similarity scores to learn entailments between predicates with typed arguments (e.g., person contracted disease). Previous work has used transitivity constraints to improve local decisions, but these constraints are intractable on large graphs. We instead propose a scalable method that learns globally consistent similarity scores based on new soft constraints that consider both the structures across typed entailment graphs and inside each graph. Learning takes only a few hours to run over 100K predicates and our results show large improvements over local similarity scores on two entailment data sets. We further show improvements over paraphrases and entailments from the Paraphrase Database, and prior state-of-the-art entailment graphs. We show that the entailment graphs improve performance in a downstream task.","pages":"703--717","doi":"10.1162\/tacl_a_00250","url":"https:\/\/www.aclweb.org\/anthology\/Q18-1048","year":"2018","volume":"6","journal":"Transactions of the Association for Computational Linguistics"}]