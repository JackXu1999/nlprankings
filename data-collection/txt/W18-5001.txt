



















































Zero-Shot Dialog Generation with Cross-Domain Latent Actions


Proceedings of the SIGDIAL 2018 Conference, pages 1–10,
Melbourne, Australia, 12-14 July 2018. c©2018 Association for Computational Linguistics

1

Zero-Shot Dialog Generation with Cross-Domain Latent Actions

Tiancheng Zhao and Maxine Eskenazi
Language Technologies Institute

Carnegie Mellon University
Pittsburgh, Pennsylvania, USA

{tianchez, max+}@cs.cmu.edu

Abstract

This paper introduces zero-shot dialog
generation (ZSDG), as a step towards neu-
ral dialog systems that can instantly gener-
alize to new situations with minimal data.
ZSDG enables an end-to-end generative
dialog system to generalize to a new do-
main for which only a domain descrip-
tion is provided and no training dialogs are
available. Then a novel learning frame-
work, Action Matching, is proposed. This
algorithm can learn a cross-domain em-
bedding space that models the semantics
of dialog responses which, in turn, lets a
neural dialog generation model generalize
to new domains. We evaluate our meth-
ods on a new synthetic dialog dataset, and
an existing human-human dialog dataset.
Results show that our method has supe-
rior performance in learning dialog mod-
els that rapidly adapt their behavior to new
domains and suggests promising future re-
search.1

1 Introduction

The generative end-to-end dialog model (GEDM)
is one of the most powerful methods of learning
dialog agents from raw conversational data in both
chat-oriented and task-oriented domains (Serban
et al., 2016; Wen et al., 2016; Zhao et al.,
2017). Its base model is an encoder-decoder net-
work (Cho et al., 2014) that uses an encoder net-
work to encode the dialog context and generate the
next response via a decoder network. Yet prior
work in GEDMs has overlooked an important is-
sue, i.e. the data scarcity problem. In fact, the data

1Code and data are avaliable at https://github.
com/snakeztc/NeuralDialog-ZSDG

scarcity problem is extremely common in most di-
alog applications due to the wide range of poten-
tial domains that dialog systems can be applied to.
To the best of our knowledge, current GEDMs are
data-hungry and have only been successfully ap-
plied to domains with abundant training material.
This limitation prohibits the possibility of using
the GEDMs for rapid prototyping in new domains
and is only useful for domains with large datasets.

The key idea of this paper lies in developing do-
main descriptions that can capture domain-specific
information and a new type of GEDM model that
can generalize to a new domain based on the do-
main description. Humans exhibit incredible effi-
ciency in achieving this type of adaptation. Imag-
ine that a customer service agent in the shoe de-
partment is transferred to the clothing department.
After reading some relevant instructions and doc-
umentation, this agent can immediately begin to
deal with clothes-related calls without the need
for any example dialogs. We also argue that it is
more efficient and natural for domain experts to
express their knowledge in terms of domain de-
scriptions rather than example dialogs. This is
because creating example dialogs involves writ-
ing down imagined dialog exchanges that can be
shared across multiple domains and are not rele-
vant to the unique proprieties of a specific domain.
However, current state-of-the-art GEDMs are not
designed to incorporate such knowledge and are
therefore incapable of adapting its behavior to un-
seen domains.

This paper introduces the use of zero-shot dia-
log generation (ZSDG) in order to enable GEDMs
to generalize to unseen situations using minimal
dialog data. Building on zero-shot classifica-
tion (Palatucci et al., 2009), we formalize ZSDG
as a learning problem where the training data con-
tains dialog data from source domains along with
domain descriptions from both the source and tar-

https://github.com/snakeztc/NeuralDialog-ZSDG
https://github.com/snakeztc/NeuralDialog-ZSDG


2

get domains. Then at testing time, ZSDG mod-
els are evaluated on the target domain, where
no training dialogs were available. We approach
ZSDG by first discovering a dialog policy net-
work that can be shared between the source and
target domains. The output from this policy is dis-
tributed vectors which are referred to as latent ac-
tions. Then, in order to transform the latent actions
from any domain back to natural language utter-
ances, a novel Action Matching (AM) algorithm
is proposed that learns a cross-domain latent ac-
tion space that models the semantics of dialog re-
sponses. This in turns enables the GEDM to gen-
erate responses in the target domains even when it
has never observed full dialogs in them.

Finally the proposed methods and baselines are
evaluated on two dialog datasets. The first one is
a new synthetic dialog dataset generated by Sim-
Dial, which was developed for this study. Sim-
Dial enables us to easily generate task-oriented
dialogs in a large number of domains, and pro-
vides a test bed to evaluate different ZSDG ap-
proaches. We further test our methods on a re-
cently released multi-domain human-human cor-
pus (Eric and Manning, 2017b) to validate whether
performance can generalize to real-world conver-
sations. Experimental results show that our meth-
ods are effective in incorporating knowledge from
domain descriptions and achieve strong ZSDG
performance.

2 Related Work

Perhaps the most closely related topic is zero-
shot learning (ZSL) for classification (Larochelle
et al., 2008), which has focused on classifying
unseen labels. A common approach is to repre-
sent the labels as attributes instead of class in-
dexes (Palatucci et al., 2009). As a result, at
test time, the model can first predict the seman-
tic attributes in the input, then make the final
prediction by comparing the predicted attributes
with the candidate labels’ attributes. More re-
cent work (Socher et al., 2013; Romera-Paredes
and Torr, 2015) improved on this idea by learn-
ing parametric models, e.g. neural networks, to
map the label and input data into a joint embed-
ding space and then make predictions. Besides
classification, prior art has explored the notion
of task generalization in robotics, so that a robot
can execute a new task that was not mentioned
in training (Oh et al., 2017; Duan et al., 2017).

In this case, a task is described by a demonstra-
tion or a sequence of instructions, and the system
needs to learn to break down the instructions into
previously learned skills. Also generating out-
of-vocabulary (OOV) words from recurrent neu-
ral networks (RNNs) can be seen as a form of
ZSL, where the OOV words are unseen labels.
Prior work has used delexicalized tags (Zhao et al.,
2017) and copy-mechanism (Gu et al., 2016; Mer-
ity et al., 2016; Elsahar et al., 2018) to enable RNN
output words that are not in its vocabulary.

Finally, ZSL has been applied to individual
components in the dialog system pipeline. Chen
et al. (Chen et al., 2016) developed an intent clas-
sifier that can predict new intent labels that are not
included in the training data. Bapna et al. (Bapna
et al., 2017) extended that idea to the slot-filling
module to track novel slot types. Both papers
leverage a natural language description for the la-
bel (intent or slot-type) in order to learn a seman-
tic embedding of the label space. Then, given any
new labels, the model can still make predictions.
There has also been extensive work on learning
domain-adaptable dialog policy by first training
a dialog policy on previous domains and testing
the policy on a new domain. Gasic et al. (Gasic
and Young, 2014) used the Gaussian Process with
cross-domain kernel functions. The resulting pol-
icy can leverage experience from other domains to
make educated decisions in a new one.

In summary, past ZSL research in the dialog do-
main has mostly focused on the individual mod-
ules in a pipeline-based dialog system. We believe
our proposal is the first step in exploring the notion
of adapting an entire end-to-end dialog system to
new domains for domain generalization.

3 Problem Formulation

We begin by formalizing zero-shot dialog genera-
tion (ZSDG). Generative dialog models take a di-
alog context c as input and then generate the next
response x. ZSDG uses the term domain to de-
scribe the difference between training and testing
data. Let D = Ds

⋃
Dt be a set of domains,

where Ds is a set of source domains, Dt is a set of
target domains and Ds ∩Dt = ∅. During training,
we are given a set of samples {c(n),x(n), d(n)} ∼
psource(c,x, d) drawn from the source domains.
During testing, a ZSDG model will be given a di-
alog context c and a domain d drawn from the
target domains and must generate the correct re-



3

sponse x. Moreover, ZSDG assumes that every
domain d has its own domain description φ(d) that
is available at training for both source and target
domains. The primary goal is to learn a generative
dialog model F : C × D → X that can perform
well in a target domain, by relating the unseen tar-
get domain description to the seen descriptions of
the source domains. Our secondary goal is that
F should perform similarly to a model that is de-
signed to operate solely in the source domains. In
short, the problem of ZSDG can be summarized
as:

Train Data: {c,x, d} ∼ psource(c,x, d)
{φ(d)}, d ∈ D

Test Data: {c,x, d} ∼ ptarget(c,x, d)
Goal: F : C ×D → X

4 Proposed Method

4.1 Seed Responses as Domain Descriptions
The design of the domain description φ is a crucial
factor that decides whether robust performance in
the target domains is achievable. This paper pro-
poses seed response (SR) as a general-purpose do-
main description that can readily be applied to dif-
ferent dialog domains. SR needs for the develop-
ers to provide a list of example responses that the
model can generate in this domain. SR’s assump-
tion is that a dialog model can discover analogies
between responses from different domains, so that
its dialog policy trained on source domains can
be reused in the target domain. Without losing
generality, SRd defines φ(d) as {x(i),a(i), d}seed
for domain d, where x is a seed response and
a is its annotations. Annotations are salient fea-
tures that help the system in infer the relationship
amongst responses from different domains. This
may be difficult to achieve using only words in
x, e.g. two domains with distinct word distribu-
tions. For example, in a task-oriented weather
domain, a seed response can be: The weather in
New York is raining and the annotation is a se-
mantic frame that contains domain general dialog
acts and slot arguments, i.e. [Inform, loc=New
York, type=rain]. The number of seed responses
is often much smaller than the number of poten-
tial responses in the domain so it is best for SR
to cover more responses that are unique to this
domain. SRs assume that there is a discourse-
level pattern that can be shared between the source
and target domains, so that a system only needs

sentence-level knowledge to adapt to the target.
This assumption holds in many slot-filling dialog
domains and it is easy to provide utterances in the
target domain that are analogies to the ones from
the source domains.

4.2 Action Matching Encoder-Decoder

Figure 1: An overview of our Action Matching
framework that looks for a latent action space Z
shared by the response, annotation and predicted
latent action from Fe.

Figure 1 shows an overview of the model we
use to tackle ZSDG. The base model is a stan-
dard encoder-decoder F where an encoder Fe
maps c and d into a distributed representation
zc = Fe(c, d) and the decoder Fd generates
the response x given zc. We denote the embed-
ding space that zc resides in as the latent ac-
tion space. We follow the KB-as-an-environment
approach (Zhao and Eskenazi, 2016) where the
generated x include both system verbal utter-
ances and API queries that interface with back-end
databases. This base model has been proven to be
effective in human interactive evaluation for task-
oriented dialogs (Zhao et al., 2017).

We have two high-level goals: (1) learn a cross-
domain F that can be reused in all source domains
and potentially shared with target domains as well.
(2) create a mechanism to incorporate knowledge
from the domain descriptions into F so that it can
generate novel responses when tested on the target
domains. To achieve the first goal, we combine c
and d by appending d as a special word token at
the beginning of every utterance in c. This sim-
ple approach performs well and enables the con-
text encoder to take the domain into account when
processing later word tokens. Also, this context
domain integration can easily scale to dealing with
a large number of domains. Then we encourage F



4

to discover reusable dialog policy by training the
same encoder decoder on dialog data generated
from multiple source domains at the same time,
which is a form of multi-task learning (Collobert
and Weston, 2008). We achieve the second goal
by projecting the response x from all domains into
the same latent action space Z. Since x alone may
not be sufficient to infer its semantics, we rely on
their annotations a to learn meaningful semantic
representations. Let zx and za be the projected
latent actions from x and a. Our method encour-
ages zd1x1 ≈ z

d2
x2 when z

d1
a1 ≈ z

d2
a2 . Moreover, for

a given z from any domain, we ensure that the de-
coder Fd can generate the corresponding response
x by training on both SRd for d ∈ D and source
dialogs.

Specifically, we propose the Action Matching
(AM) training procedure. We first introduce a
recognition network R that can encode x and a
into zx = R(x, d) and za = R(a, d) respectively.
During training, the model receives two types of
data. The first type is domain description data in
the form of {x,a, d}seed for each domain. The
second type of data is source domain dialog data
in the form of {c,x, d}. For the first type of data,
we update the parameters in R and Fd by mini-
mizing the following loss function:

Ldd(Fd,R) =− log pFd(x|R(a, d))
+ λD[R(x, d)‖R(a, d)]

(1)

where λ is a constant hyperparameter and D is a
distance function, e.g. mean square error (MSE),
that measures the closeness of two input vectors.
The first term in Ldd trains the decoder Fd to gen-
erate the response x given za = R(a, d) from all
domains. The second term in Ldd enforces the
recognition network R to encode a response and
its annotation to nearby vectors in the latent action
space from all domains, i.e. zdx ≈ zda for d ∈ D.

Moreover, just optimizing Ldd does not ensure
that the zc predicted by the encoder Fe will be
related to the zx or za encoded by the recognition
networkR. So when we receive the second type of
data (source dialogs), we add a second term to the
standard maximum likelihood objective to train F
andR.

Ldialog(F ,R) =− log pFd(x|Fe(c, d))
+ λD(R(x, d)‖Fe(c, d))

(2)

The second term in Ldialog completes the loop
by encouraging zdc ≈ zdx, which resembles the

regularization term used in variational autoen-
coders (Kingma and Welling, 2013). Assuming
that annotation a provides a domain-agnostic se-
mantic representation of x, then F trained on
source domains can begin to operate in the tar-
get domains as well. During training, our AM
algorithm alternates between these two types of
data and optimizes Ldd or Ldialog accordingly. The
resulting models effectively learn a latent action
space that is shared by the the response annotation
a, response x and predicted latent action based on
c in all domains. AM training is summarized in
Algorithm 1.

Algorithm 1: Action Matching Training
Initialize weights of Fe, Fd,R;
Data = {c,x, d}

⋃
{x,a, d}seed

while batch ∼ Data do
if batch in the form {c,x, d} then

Backpropagate loss Ldialog
else

Backpropagate loss Ldd
end

end

4.3 Architecture Details

We implement an AMED for later experiments as
follows:

Distance Functions: In this study, we assume
that the latent actions are deterministic distributed
vectors. Thus MSE is used: D(z, ẑ) = 1L

∑L
l (zl−

ẑl)
2, where L is the dimension size of the latent

actions. Also, Ldialog and Ldd use the same dis-
tance function.

Recognition Networks: we use a bidirectional
GRU-RNN (Cho et al., 2014) as R to obtain
utterance-level embedding. Since both x and a are
sequences of word tokens, we combine them with
the domain tag by appending the domain tag in
the beginning of the original word sequence, i.e.
{x, d} or {a, d} = [d,w1, ...wJ ], where J is the
length of the word sequence. Then the R will en-
code [d,w1, ...wJ ] into hidden outputs in forward
and backward directions, [(

−→
h0,
←−
hJ), ...(

−→
hJ ,
←−
h0)].

We use the concatenation of the last hidden states
from each direction, i.e. zx or za = [

−→
hJ ,
←−
hJ ] as

utterance-level embedding for x or a respectively.
Dialog Encoders: a hierarchical recurrent en-

coder (HRE) is used to encode the dialog con-
text, which handles long contexts better than non-



5

Figure 2: Visual illustration of our AM encoder decoder with copy mechanism (Merity et al., 2016).
Note that AM can also be used with RNN decoders without the copy functionality.

hierarchical ones (Li et al., 2015). HRE first
uses an utterance encoder to encode every utter-
ance in the dialog and then uses a discourse-level
LSTM-RNN to encode the dialog context by tak-
ing output from the utterance encoder as input. In-
stead of introducing a new utterance encoder, we
reuse the recognition network R described above
as the utterance encoder, which serves the pur-
pose perfectly. Another advantage is that using
zx predicted by R as input enables the discourse-
level encoder to use knowledge from latent ac-
tions as well. Our discourse-level encoder is a 1-
layer LSTM-RNN (Hochreiter and Schmidhuber,
1997), which takes in a list of output [z1, z2..zK ]
from R and encodes them into [v1, v2, ...vK ],
where K is the number of utterances in the con-
text. The last hidden state vK is used as the pre-
dicted latent action zc.

Response Decoders: we experiment with two
types of LSTM-RNN decoders. The first is an
RNN decoder with an attention mechanism (Lu-
ong et al., 2015), enabling the decoder to dy-
namically look up information from the context.
Specifically, we flatten the dialog context into a se-
quence of words [w11, ...w1J ...wKJ ]. Using out-
put from the R and the discourse-level LSTM-
RNN, each word here is represented by mkj =
hkj +Wvvk. Let the hidden state of the decoder
at step t be st, then our attention mechanism com-
putes the Softmax output via:

αkj,t = softmax(mTkj tanh(Wαst)) (3)

s̃t =
∑
kj

αkj,tmkj (4)

pvocab(wt|st) = softmax(MLP(st, s̃t)) (5)

The second type is the LSTM-RNN with a copy

mechanism that can directly copy words from the
context as output (Gu et al., 2016). Such a mecha-
nism has already exhibited strong performance in
task-oriented dialogs (Eric and Manning, 2017a)
and is well suited for generating OOV word to-
kens (Elsahar et al., 2018). We implemented the
Pointer Sentinel Mixture Model (PSM) (Merity
et al., 2016) as our copy decoder. PSM defines the
generation of the next word as a mixture of prob-
abilities from either the Softmax output from the
decoder LSTM or the attention Softmax for words
in the context: p(wt|st) = gpvocab(wt|st) + (1 −
g)pptr(wt|st), where g is the mixture weight com-
puted from a sentinel vector u with st.

pptr(wt|st) =
∑

kj∈I(w,x)

αkj,t (6)

g = softmax(uT tanh(Wαsi)) (7)

5 Datasets for ZSDG

Two dialog datasets were used for evaluation.

5.1 SimDial Data

We developed SimDial2, which is a multi-domain
dialog generator that can generate realistic conver-
sations for slot-filling domains with configurable
complexity. See Appendix A.3 for details. Com-
pared to other synthetic dialog corpora used to test
GEDMs, e.g. bAbI (Dodge et al., 2015), SimDial
data is significantly more challenging. First since
SimDial simulates communication noise, the di-
alogs that are generated can be very long (more
than 50 turns) and the simulated agent can carry
out error recovery strategies to correctly infer the
users’ goals. This challenges end-to-end models

2https://github.com/snakeztc/SimDial

https://github.com/snakeztc/SimDial


6

to model long dialog contexts. SimDial also simu-
lates spoken language phenomena, e.g. self-repair,
hesitation. Prior work (Eshghi et al., 2017) has
shown that this type of utterance-level noise dete-
riorates end-to-end dialog system performance.

Data Details

SimDial was used to generate dialogs for 6 do-
mains: restaurant, movie, bus, restaurant-slot,
restaurant-style and weather. For each domain,
900/100/500 dialogs were generated for training,
validation and testing. On average, each dia-
log had 26 utterances and each utterance had
12.8 word tokens. The total vocabulary size was
651. We split the data such that the training
data included dialogs from the restaurant, bus and
weather domains and the test data included the
restaurant, movie, restaurant-slot and restaurant
style domains. This setup evaluates a ZSDG sys-
tem from the following perspectives:

Restaurant (in domain): evaluation on the
restaurant test data checks if a dialog model
is able to maintain its performance on the
source domains. Restaurant-slot (unseen slots):
restaurant-slot has the same slot types and natu-
ral language generation (NLG) templates as the
restaurant domain, but has a completely different
slot vocabulary, i.e. different location names and
cuisine types. Thus this is designed to evaluate a
model that can generalize to unseen slot values.
Restaurant-style (unseen NLG): restaurant-style
has the same slot type and vocabulary as restau-
rant, but its NLG templates are completely differ-
ent, e.g. “which cuisine type?” → “please tell
me what kind of food you prefer”. This part tests
whether a model can learn to adapt to generate
novel utterances with similar semantics. Movie
(new domain): movie has completely different
NLG templates and structure and shares few com-
mon traits with the source domains at the surface
level. Movie is the hardest task in the SimDial
data, which challenges a model to correctly gener-
ate next responses that are semantically different
from the ones in source domains.

Finally, we obtain SRs as domain descriptions
by randomly selecting 100 unique utterances from
each domain. The response annotation is a re-
sponse’s internal semantic frame used by the Sim-
Dial generator. For example, “I believe you said
Boston. Where are you going?” → [implicit-
confirm loc=Boston; request location].

5.2 Stanford Multi-Domain Dialog Data

The second dataset is the Stanford multi-domain
dialog (SMD) dataset (Eric and Manning, 2017b)
of 3031 human-human dialogs in three domains:
weather, navigation and scheduling. One speaker
plays the role of a driver. The other plays the
car’s AI assistant and talks to the driver to com-
plete tasks, e.g. setting directions on a GPS. Av-
erage dialog length is 5.25 utterances; vocabulary
size is 1601. We use SMD to validate whether our
proposed methods generalize to human-generated
dialogs. We generate SR by randomly selecting
150 unique utterances for each domain. An expert
annotates the seed utterances with dialog acts and
entities. For example “All right, I’ve set your next
dentist appointment for 10am. Anything else?”
→ [ack; inform goal event=dentist appointment
time=10am ; request needs]. Finally, in order to
formulate a ZSDG problem, we use a leave-one-
out approach with two domains as source domains
and the third one as the target domain, which re-
sults in 3 possible configurations.

6 Experiments and Results

The baseline models include 1. hierarchical recur-
rent encoder with attention decoder (+Attn) (Ser-
ban et al., 2016). 2. hierarchical recurrent en-
coder with copy decoder (Merity et al., 2016)
(+Copy), which has achieved very good perfor-
mance on task-oriented dialogs (Eric and Man-
ning, 2017a). We then augment both baseline
models with the proposed cross-domain AM train-
ing procedure and denote them as +Attn+AM and
+Copy+AM.

Evaluating generative dialog systems is chal-
lenging since the model can generate free-form re-
sponses. Fortunately, we have access to the inter-
nal semantic frames of the SimDial data, so we
use the automatic measures used in (Zhao et al.,
2017) that employ four metrics to quantify the per-
formance of a task-oriented dialog model. BLEU
is the corpus-level BLEU-4 between the generated
response and the reference ones (Papineni et al.,
2002). Entity F1 checks if a generated response
contains the correct entities (slots) in the reference
response. Act F1 measures whether the generated
responses reflect the dialog acts in the reference
responses, which compensates for BLEU’s limita-
tion of looking for exact word choices. A one-
vs-rest support vector machine (Scholkopf and
Smola, 2001) with bi-gram features is trained to



7

tag the dialogs in a response. KB F1 checks all
the key words in a KB query that the system is-
sues to the KB backend. Finally, we introduce
BEAK = 4

√
bleu× ent× act× kb, the geometric

mean of these four scores, to quantify a system’s
overall performance. Meanwhile, since the oracle
dialog acts and KB queries are not provided in the
SMD data (Eric and Manning, 2017b), we only re-
port BLEU and entity F1 results on SMD.

6.1 Main Results

In
domain

+Attn +Copy +Attn
+AM

+Copy
+AM

BLEU 59.1 70.4 67.7 70.1
Entity 69.2 70.5 74.1 79.9
Act 94.7 92.0 94.1 95.1
KB 94.7 96.1 95.2 97.0
BEAK 77.2 81.3 81.9 84.7
Unseen
Slot

+Attn +Copy +Attn
+AM

+Copy
+AM

BLEU 24.9 45.6 47.9 68.5
Entity 56.0 68.0 53.1 74.6
Act 90.9 91.8 86.0 94.5
KB 78.1 89.6 81.0 95.3
BEAK 56.1 71.1 64.8 82.3
Unseen
NLG

+Attn +Copy +Attn
+AM

+Copy
+AM

BLEU 15.8 36.9 43.5 70.1
Entity 61.7 68.9 63.8 72.9
Act 91.5 92.2 89.3 95.2
KB 66.2 94.6 93.1 97.0
BEAK 49.3 65.9 69.3 82.9
New
domain

+Attn +Copy +Attn
+AM

+Copy
+AM

BLEU 13.5 24.6 36.7 54.6
Entity 23.1 40.8 23.3 52.6
Act 82.3 85.5 84.8 88.5
KB 43.5 67.1 67.0 88.2
BEAK 32.5 48.8 46.8 68.8

Table 1: Evaluation results on test dialogs from
SimDial Data. Bold values indicate the best per-
formance.

Table 1 shows results on the SimDial data. Al-
though the standard +Attn model achieves good
performance in the source domains, it doesn’t gen-
eralize to target domains, especially for entity F1
in the unseen-slot domain, BLEU score in the
unseen-NLG domain, and all new domain met-
rics. The +Copy model has better, although still
limited, generalization to target domains. The
main benefit of the +Copy model is its ability
to directly copy and output words from the con-
text, reflected in its strong entity F1 in the un-
seen slot domain. However, +Copy can’t gener-
alize to new domains where utterances are novel,
e.g. the unseen NLG or the new domain. How-
ever, our AM algorithm substantially improves

performance of both decoders (Attn and Copy).
Results show that the proposed AM algorithm is
complementary to decoders with a copy mecha-
nism: HRED+Copy+AM model has the best per-
formance on all target domains. In the easier
unseen-slot and unseen-NLG domains, the result-
ing ZSDG system achieves a BEAK of about 82,
close to the in-domain BEAK performance (84.7).
Even in the new domain (movie), our model
achieves a BEAK of 67.2, 106% relative improve-
ment w.r.t +Attn and 38.8% relative improvement
w.r.t +Copy. Moreover, our AM method also im-
proves performance on in-domain dialogs, sug-
gesting that AM exploits the knowledge encoded
in the domain description and improves the mod-
els’ generalization.

Navigate Oracle +Attn +Copy +Copy
+AM

BLEU 13.4 0.9 5.4 5.9
Entity 19.3 2.6 4.7 14.3
Weather Oracle +Attn +Copy +Copy

+AM
BLEU 18.9 4.8 4.4 8.1
Entity 51.9 0.0 16.3 31.0
Schedule Oracle +Attn +Copy +Copy

+AM
BLEU 20.9 3.0 3.8 7.9
Entity 47.3 0.4 17.1 36.9

Table 2: Evaluation on SMD data. The bold do-
main title is the one that was excluded from train-
ing.

Table 2 summarizes the results on the SMD
data. We also report the oracle performance,
obtained by training +Copy on the full dataset.
The AM algorithm can significantly improve En-
tity F1 and BLEU from the two baseline mod-
els. +Copy+AM also achieves competitive perfor-
mance in terms of Entity F1 compared to the oracle
scores, despite the fact that no target domain data
was used in training.

6.2 Model Analysis

Various types of performance improvement were
also studied. Figure 3 shows the breakdown of the
BLEU score according to the dialog acts of refer-
ence responses. Models with +Copy decoder can
improve performance for all dialog acts except for
the greet act, which occurs at the beginning of a di-
alog. In this case, the +Copy decoder has no con-
text to copy and thus cannot generate any novel
responses. This is one limitation of +Copy de-
coder since in real interactive testing with humans,



8

Type Reference +Attn +Copy +Copy+AM
General Utts See you next time. Goodbye. See you next time. See you next time.
Unseen Slots Do you mean romance

movie?
Do you mean Chinese
food.

Do you mean ro-
mance food?

Do you mean ro-
mance movie?

Unseen Utts Movie 55 is a great
movie.

Bus 12 can take you
there.

Bus 55 can take you
there.

Movie 55 is a great
movie.

Table 3: Three types of responses and generation results (tested on the new movie domain). The text in
bold is the output directly copied from the context by the copy decoder.

each system utterance must be generated from the
model instead of copied from the context. How-
ever, models with AM training learn to generate
novel utterances based on knowledge from the SR,
so +Copy+AM can generate responses at the be-
ginning of a dialog.

Figure 3: Breakdown BLEU scores on the new do-
main test set from SimDial.

A qualitative analysis was conducted to summa-
rize typical responses from these models. Table 3
shows three types of typical situations in the Sim-
Dial data. The first type is general utterance ut-
terances, e.g. “See you next time” that appear in
all domains. All three models correctly generate
them in the ZSDG setting. The second type is ut-
terances with unseen slots. For example, explicit
confirm “Do you mean xx?”. +Attn fails in this
situation since the new slot values are not in its vo-
cabulary. +Copy still performs well since it learns
to copy entity-like words from the context, but the
overall sentence is often incorrect, e.g. “Do you
mean romance food”. The last one is unseen ut-
terance where both +Attn and +Copy fail. The
two baseline models can still generate responses
with correct dialog acts, but the output words are
in the source domains. Only the models trained
with AM are able to infer that “Movie xx is a great
movie” serves a function similar to “Bus xx can
take you there”, and generates responses using the
correct words from the target domain.

Finally we investigate how the the size of SR
affects AM performance. Figure 4 shows results
in the SMD schedule domain. The number of seed

Figure 4: Performance on the schedule domain
from SMD while varying the size of SR.

responses varies from 0 to 200. Performance in
the target domains is positively correlated with the
number of seed responses. We also observe that
the model achieves sufficient SR performance at
100, compared to the ones trained on all of the 200
seed responses. This suggests that the amount of
seeding needed by SR is relatively small, which
shows the practicality of using SR as a domain de-
scription.

7 Conclusion and Future Work

This paper introduces ZSDG, dealing with neu-
ral dialog systems’ domain generalization ability.
We formalize the ZSDG problem and propose an
Action Matching framework that discovers cross-
domain latent actions. We present a new simulated
multi-domain dialog dataset, SimDial, to bench-
mark the ZSDG models. Our assessment validates
the AM framework’s effectiveness and the AM en-
coder decoders perform well in the ZSDG setting.

ZSDG provides promising future research ques-
tions. How can we reduce the annotation cost
of learning the latent alignment between actions
in different domains? How can we create ZSDG
for new domains where the discourse-level pat-
terns are significantly different? What are other
potential domain description formats? In sum-
mary, solving ZSDG is an important step for fu-
ture general-purpose conversational agents.



9

References
Ankur Bapna, Gokhan Tur, Dilek Hakkani-Tur, and

Larry Heck. 2017. Towards zero-shot frame se-
mantic parsing for domain scaling. arXiv preprint
arXiv:1707.02363 .

Yun-Nung Chen, Dilek Hakkani-Tür, and Xiaodong
He. 2016. Zero-shot learning of intent embed-
dings for expansion by convolutional deep struc-
tured semantic models. In Acoustics, Speech and
Signal Processing (ICASSP), 2016 IEEE Interna-
tional Conference on. IEEE, pages 6045–6049.

Kyunghyun Cho, Bart Van Merriënboer, Caglar Gul-
cehre, Dzmitry Bahdanau, Fethi Bougares, Holger
Schwenk, and Yoshua Bengio. 2014. Learning
phrase representations using rnn encoder-decoder
for statistical machine translation. arXiv preprint
arXiv:1406.1078 .

Ronan Collobert and Jason Weston. 2008. A unified
architecture for natural language processing: Deep
neural networks with multitask learning. In Pro-
ceedings of the 25th international conference on
Machine learning. ACM, pages 160–167.

Jesse Dodge, Andreea Gane, Xiang Zhang, Antoine
Bordes, Sumit Chopra, Alexander Miller, Arthur
Szlam, and Jason Weston. 2015. Evaluating prereq-
uisite qualities for learning end-to-end dialog sys-
tems. arXiv preprint arXiv:1511.06931 .

Yan Duan, Marcin Andrychowicz, Bradly Stadie,
Jonathan Ho, Jonas Schneider, Ilya Sutskever, Pieter
Abbeel, and Wojciech Zaremba. 2017. One-shot im-
itation learning. arXiv preprint arXiv:1703.07326 .

Hady Elsahar, Christophe Gravier, and Frederique
Laforest. 2018. Zero-shot question generation from
knowledge graphs for unseen predicates and entity
types. arXiv preprint arXiv:1802.06842 .

Mihail Eric and Christopher D Manning. 2017a. A
copy-augmented sequence-to-sequence architecture
gives good performance on task-oriented dialogue.
arXiv preprint arXiv:1701.04024 .

Mihail Eric and Christopher D Manning. 2017b. Key-
value retrieval networks for task-oriented dialogue.
arXiv preprint arXiv:1705.05414 .

Arash Eshghi, Igor Shalyminov, and Oliver Lemon.
2017. Bootstrapping incremental dialogue systems
from minimal data: the generalisation power of di-
alogue grammars. arXiv preprint arXiv:1709.07858
.

Milica Gasic and Steve Young. 2014. Gaussian pro-
cesses for pomdp-based dialogue manager optimiza-
tion. IEEE/ACM Transactions on Audio, Speech,
and Language Processing 22(1):28–40.

Jiatao Gu, Zhengdong Lu, Hang Li, and Victor OK
Li. 2016. Incorporating copying mechanism in
sequence-to-sequence learning. arXiv preprint
arXiv:1603.06393 .

Sepp Hochreiter and Jürgen Schmidhuber. 1997.
Long short-term memory. Neural computation
9(8):1735–1780.

Diederik Kingma and Jimmy Ba. 2014. Adam: A
method for stochastic optimization. arXiv preprint
arXiv:1412.6980 .

Diederik P Kingma and Max Welling. 2013. Auto-
encoding variational bayes. arXiv preprint
arXiv:1312.6114 .

Hugo Larochelle, Dumitru Erhan, and Yoshua Bengio.
2008. Zero-data learning of new tasks. In AAAI. 2,
page 3.

Jiwei Li, Minh-Thang Luong, and Dan Jurafsky. 2015.
A hierarchical neural autoencoder for paragraphs
and documents. arXiv preprint arXiv:1506.01057 .

Minh-Thang Luong, Hieu Pham, and Christopher D
Manning. 2015. Effective approaches to attention-
based neural machine translation. arXiv preprint
arXiv:1508.04025 .

Stephen Merity, Caiming Xiong, James Bradbury, and
Richard Socher. 2016. Pointer sentinel mixture
models. arXiv preprint arXiv:1609.07843 .

Junhyuk Oh, Satinder Singh, Honglak Lee, and Push-
meet Kohli. 2017. Zero-shot task generalization
with multi-task deep reinforcement learning. arXiv
preprint arXiv:1706.05064 .

Mark Palatucci, Dean Pomerleau, Geoffrey E Hinton,
and Tom M Mitchell. 2009. Zero-shot learning with
semantic output codes. In Advances in neural infor-
mation processing systems. pages 1410–1418.

Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a method for automatic eval-
uation of machine translation. In Proceedings of
the 40th annual meeting on association for compu-
tational linguistics. Association for Computational
Linguistics, pages 311–318.

Bernardino Romera-Paredes and Philip Torr. 2015. An
embarrassingly simple approach to zero-shot learn-
ing. In International Conference on Machine Learn-
ing. pages 2152–2161.

Bernhard Scholkopf and Alexander J Smola. 2001.
Learning with kernels: support vector machines,
regularization, optimization, and beyond. MIT
press.

Iulian Vlad Serban, Alessandro Sordoni, Ryan Lowe,
Laurent Charlin, Joelle Pineau, Aaron Courville,
and Yoshua Bengio. 2016. A hierarchical latent
variable encoder-decoder model for generating di-
alogues. arXiv preprint arXiv:1605.06069 .

Richard Socher, Milind Ganjoo, Christopher D Man-
ning, and Andrew Ng. 2013. Zero-shot learning
through cross-modal transfer. In Advances in neu-
ral information processing systems. pages 935–943.



10

Tsung-Hsien Wen, Milica Gasic, Nikola Mrksic,
Lina M Rojas-Barahona, Pei-Hao Su, Stefan Ultes,
David Vandyke, and Steve Young. 2016. A network-
based end-to-end trainable task-oriented dialogue
system. arXiv preprint arXiv:1604.04562 .

Wojciech Zaremba, Ilya Sutskever, and Oriol Vinyals.
2014. Recurrent neural network regularization.
arXiv preprint arXiv:1409.2329 .

Tiancheng Zhao and Maxine Eskenazi. 2016. To-
wards end-to-end learning for dialog state tracking
and management using deep reinforcement learning.
arXiv preprint arXiv:1606.02560 .

Tiancheng Zhao, Allen Lu, Kyusong Lee, and
Maxine Eskenazi. 2017. Generative encoder-
decoder models for task-oriented spoken dialog
systems with chatting capability. arXiv preprint
arXiv:1706.08476 .

A Supplemental Material

A.1 Seed Response Creation Process
We follow the following process to create SR in
a new slot-filling domain. First, we collect seed
responses (including user/system utterances, KB
queries and KB responses) from each source do-
main and annotate them with dialog acts, entity
types and entity values. Then human experts with
knowledge about the target domain can write up
seed responses for the target domain by draw-
ing ideas from the sources. For example, if the
source domain is restaurants and the target do-
main is movies. The source may contain a sys-
tem utterance with its annotation: “I believed you
said Pittsburgh, what kind of food are you inter-
ested in? → [implicit-confirm, loc=Pittsburgh, re-
quest food type]”. Then the expert can come up
with a similar utterance from the target domain,
e.g. “Alright, Pittsburgh. what type of movie do
you like? → [implicit-confirm, loc=Pittsburgh,
request movie type]”. In this way, our proposed
AM training can leverage the annotations to match
these two actions as analogies in the latent ac-
tion space. Another advantage of this process is
that human experts do not have to directly label
whether two utterances from two domains are di-
rect analogies; this could be ambiguous and chal-
lenging. Instead, human experts only create do-
main shareable annotations and leave the difficult
matching problem to our models.

A.2 Model Details
For all experiments, we use a word embedding
with size 200. The recognition network uses bidi-
rectional GRU-RNN with hidden size 256 for each

direction. The discourse-level LSTM-RNN and
the decoder have hidden size 512. The models
are trained with Adam (Kingma and Ba, 2014)
with learning rate 0.001. All weights are ini-
tialized from uniform distribution between +/-
0.08. 30% dropouts are applied at the input
and output of recurrent neural networks (Zaremba
et al., 2014). For decoders with copy mech-
anisms, we also use the pointer loss proposed
in (Merity et al., 2016) which minimizes− log(g+∑

kj∈I(x,c) αkj), where I results in all positions of
output x in the attention context.

A.3 SimDial Details
To generate data, SimDial expects: a domain spec-
ification (DS) and a complexity specification (CS).
DS defines the content of the domain, e.g. restau-
rant or weather and CS defines complexity of di-
alogs for three aspects:

Environmental: the complexity level of the
communication channel, e.g. automatic speech
recognition (ASR) error rate.

Propositional: the complexity level in the
propositional content of user utterances, e.g. the
chance that a user will give multiple slot values in
one utterance.

Interaction: the complexity level in terms of
real-time interaction, e.g. the percentage of self-
repair.

The following is an example dialog generated
for the restaurant domain with all of the above
complexity turned on. The number at the end of
the speaker turn indicates ASR confidence.

Domain: Restaurant
Sys: Welcome to the restaurant recommendation system.
What do you need?
User (0.83): Hello robot. I am looking for a restaurant.
Sys: Which city are you interested in?
User (0.75): I am in San Jose. And Korean. Uhm sorry, I
am interested in Chinese food.
Sys: I believe you said San Jose. I believe you said Chi-
nese.
Sys: QUERY goal: restaurant food-pref: Chinese loca-
tion: San Jose
KB: RET restaurant: 17
Sys: Restaurant 17 is a good choice. What else can I do
for you?
User (0.6): Not done yet. Is that restaurant open now?
Sys: QUERY goal: open restaurant: 17 food-pref: Chi-
nese location: San Jose
KB: RET open: yes
Sys: Yes, it is open right now. What else?
User (0.69): No more questions. Thanks.
Sys: Bye.

Table 4: An example dialog generated from Sim-
Dial.


