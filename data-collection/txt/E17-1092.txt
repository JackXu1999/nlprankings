



















































Recognizing Insufficiently Supported Arguments in Argumentative Essays


Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics: Volume 1, Long Papers, pages 980–990,
Valencia, Spain, April 3-7, 2017. c©2017 Association for Computational Linguistics

Recognizing Insufficiently Supported Arguments in Argumentative Essays

Christian Stab† and Iryna Gurevych†‡
†Ubiquitous Knowledge Processing Lab (UKP-TUDA)

Department of Computer Science, Technische Universität Darmstadt
‡Ubiquitous Knowledge Processing Lab (UKP-DIPF)

German Institute for Educational Research
www.ukp.tu-darmstadt.de

Abstract

In this paper, we propose a new task for as-
sessing the quality of natural language ar-
guments. The premises of a well-reasoned
argument should provide enough evidence
for accepting or rejecting its claim. Al-
though this criterion, known as sufficiency,
is widely adopted in argumentation theory,
there are no empirical studies on its appli-
cability to real arguments. In this work,
we show that human annotators substan-
tially agree on the sufficiency criterion and
introduce a novel annotated corpus. Fur-
thermore, we experiment with feature-rich
SVMs and convolutional neural networks
and achieve 84% accuracy for automati-
cally identifying insufficiently supported
arguments. The final corpus as well as
the annotation guideline are freely avail-
able for encouraging future research on ar-
gument quality.1

1 Introduction

Argumentation is an omnipresent routine and an
integral part of our daily verbal communication. It
is a verbal activity that aims at increasing or de-
creasing the plausibility of a controversial stand-
point (van Eemeren et al., 1996, p. 5). Well-
reasoned arguments of high quality are not only
important for making thoughtful decisions and
persuading a particular audience but also play a
major role for drawing widely accepted conclu-
sions. Computational argumentation is a recent re-
search field in natural language processing that fo-
cuses on the analysis of arguments in natural lan-
guage texts. Novel advances have a broad applica-

1https://www.ukp.tu-darmstadt.de/data/
argumentation-mining

tion potential in various areas like debating tech-
nologies (Levy et al., 2014; Rinott et al., 2015),
policy making (Sardianos et al., 2015), informa-
tion retrieval (Carstens and Toni, 2015), and le-
gal decision support (Mochales-Palau and Moens,
2009). Recently, computational argumentation is
receiving increasing interest in intelligent writ-
ing assistance (Song et al., 2014; Stab et al.,
2014) since it enables argumentative writing sup-
port systems that provide tailored feedback about
arguments in student essays.

Most of the existing approaches in computa-
tional argumentation consider argumentation as
discourse structures and focus on the identifica-
tion of arguments in natural language texts. For
instance, existing approaches classify text units
as argumentative or non-argumentative (Moens
et al., 2007), recognize argument components
such as claims or premises at the sentence-level
(Mochales-Palau and Moens, 2009; Kwon et al.,
2007; Eckle-Kohler et al., 2015) or clause-level
(Levy et al., 2014; Sardianos et al., 2015), or iden-
tify argument structures by classifying pairs of ar-
gument components (Stab and Gurevych, 2014).
However, these approaches are of limited use for
argumentative writing support systems since they
do not recognize the weak points of arguments.

Despite the comprehensive theoretical frame-
work on argument quality in logic and argumen-
tation theory (van Eemeren et al., 1996; Damer,
2009), there are only few computational ap-
proaches that focus on the assessment of argu-
ments in natural language texts. These existing
approaches either identify undisputed arguments
in online communities (Cabrio and Villata, 2012),
assess the persuasiveness of arguments (Wei et
al., 2016), compare and rank arguments regard-
ing their convincingness (Habernal and Gurevych,
2016b), or summarize the argumentation strength

980



of an entire essay in a single holistic score (Persing
and Ng, 2015). Our approach is based on the theo-
retical framework proposed by Johnson and Blair
(2006). In particular, we focus on the sufficiency
criterion that an argument fulfills if its premises
provide enough evidence for accepting or reject-
ing the claim. The following example argument
illustrates a violation of the sufficiency criterion:

Example 1: “It is an undeniable fact
that tourism harms the natural habitats
of the destination countries. As Aus-
tralia’s Great Barrier Reef has shown,
the visitors cause immense destruction
by breaking corals as souvenirs, throw-
ing boat anchors or dropping fuel and
other sorts of pollution.”

The premise of this argument represents a particu-
lar example (second sentence) that supports a gen-
eral claim in the first sentence. The argument is
a generalization from one sample to the general
case. However, a single sample is not enough to
support the general case. Therefore, the argument
does not comply with the sufficiency criterion.

Example 2: “Cloning will be benefi-
cial for people who are in need of organ
transplants. Cloned organs will match
perfectly to the blood group and tissue
of patients since they can be raised from
cloned stem cells of the patient. In addi-
tion, it shortens the healing process.”

Example 2 illustrates a sufficiently supported ar-
gument. It is reasonable to accept that transplanta-
tion patients will benefit from cloning if it enables
a better match and an accelerated healing process.

Our primary motivation is to create an argument
analysis method for argumentative writing support
systems that classifies an argument as sufficient if
its premises provide enough evidence for accept-
ing its claim (example 2) or as insufficient if its
premises do not provide enough evidence (exam-
ple 1). Therefore, our first research question is
whether human annotators can reliably apply the
sufficiency criterion to real arguments and if it is
possible to create annotated data of high quality.
The second research question addresses the auto-
matic recognition of insufficiently supported argu-
ments. We investigate if, and how accurately, in-
sufficiently supported arguments can be identified
by computational techniques.

The contribution of this paper is threefold: first,
we investigate to what extent human annotators
agree on the sufficiency criterion. We present the
results of an annotation study with three annota-
tors and show that our annotation guideline suc-
cessfully guides annotators to substantial agree-
ment. Second, we show that insufficiently sup-
ported arguments can be identified with high accu-
racy using convolutional neural networks (CNN).
The experimental results show that a CNN signif-
icantly outperforms several challenging baselines
and manually created features. Third, we intro-
duce a novel corpus for studying the quality of ar-
guments.

2 Related Work

Previous works in computational argumentation
focused primarily on approaches for argument
mining. These include, for example, meth-
ods for the identification of arguments in legal
texts (Moens et al., 2007), news articles (Eckle-
Kohler et al., 2015; Sardianos et al., 2015),
or user-generated web discourse (Habernal and
Gurevych, 2016a). Other approaches address
the classification of argument components into
claims and premises (Mochales-Palau and Moens,
2009), supporting and opposing claims (Kwon
et al., 2007), or backings, rebuttals and refuta-
tions (Habernal and Gurevych, 2016a). Levy
et al. (2014) recognize context-dependent claims
and Rinott et al. (2015) retrieve several types
of evidence from Wikipedia. Approaches for
identifying the structure of arguments recognize
argumentative relations between argument com-
ponents using context-free grammars (Mochales-
Palau and Moens, 2009), pair classification (Stab
and Gurevych, 2014), or maximum spanning trees
(Peldszus and Stede, 2015). However, none of
these approaches consider the quality of argu-
ments.

Similarly, most existing corpora in compu-
tational argumentation are only annotated with
argument components (Habernal and Gurevych,
2016a; Aharoni et al., 2014; Mochales-Palau and
Moens, 2009) or argument structures (Reed et al.,
2008; Stab and Gurevych, 2014; Peldszus and
Stede, 2015) and do not include annotations of ar-
gumentative quality issues. Other resources in the
field contain arguments annotated with different
properties such as emotions and sarcasm (Walker
et al., 2012), the type of reasoning (Reed et al.,

981



2008) or the stance on a topic (Somasundaran and
Wiebe, 2009). However, there is no corpus of ar-
guments annotated with the sufficiency criterion.

Currently there are only few approaches that fo-
cus on the automatic assessment of argument qual-
ity. Cabrio and Villata (2012) employed textual
entailment for identifying undisputed arguments
in online discussions. They built a graph that rep-
resents attack and support relations between ar-
guments and applied the abstract argumentation
framework (Dung, 1995) for identifying accepted
arguments. Although their approach is capable of
finding undisputed arguments among a given set
of arguments, it does not answer why a specific
argument is of inferior quality than another argu-
ment. Thus, their approach is of limited use for
guiding students since it does not pinpoint partic-
ular weaknesses of arguments.

Park and Cardie (2014) proposed an approach
for classifying propositions as verifiable (experi-
ential and non-experiential) or unverifiable. Their
best approach based on a support vector machine
achieves a macro F1 score of .690. Although the
verifiability of propositions enables to determine
appropriate types of support, it does not answer if
an argument is sufficiently supported or not.

Persing and Ng (2015) introduced an approach
for recognizing the argumentation strength of an
essay. They found that pos n-grams, prompt adher-
ence features, and predicted argument components
perform best. However, their model determines a
single holistic score that summarizes the argumen-
tation quality of the entire essay. Consequently, it
does not provide formative feedback that guides
students to improve their arguments.

Recently, researchers proposed approaches for
automatically assessing the persuasiveness of ar-
guments. For instance, Wei et al. (2016) proposed
an approach for ranking user comments taken
from online fora and found that argumentation re-
lated features are effective for this task. Cano-
Basave and He (2016) ranked speakers in political
debates by using semantic frames which indicate
persuasive argumentation features, and Habernal
and Gurevych (2016b) compared the convincing-
ness of argument pairs using feature-rich SVMs
and bidirectional LSTMs. However, the persua-
siveness score of an argument is only of limited
use for argumentative writing support, since it
summarizes various quality criteria and does not
explain why an argument is weak.

3 Argument Quality: Theoretical
Background

An argument consists of several argument com-
ponents. It includes a claim and one or more
premises. The claim (also called conclusion) is
a controversial statement and the central compo-
nent of an argument. The premises constitute the
reasons for believing the claim to be true or false
(Damer, 2009, p. 14). Assessing the quality of
arguments is a complex task since arguments in
natural language are hardly ever in a standardized
form (Damer, 2009; Govier, 2010). Moreover, ar-
gument quality is a product of many different cri-
teria (Johnson and Blair, 2006). The quality of
an argument depends, for instance, on its lexical
clarity and phrasing (representation), the level of
trust that the audience has in the arguer (ethos),
and the emotions and values appealed by the argu-
ment (pathos). The logical quality of arguments
(logos) is, however, independent of all other mer-
its, defects and external influence factors (John-
son and Blair, 2006, p. 50). Certainly, external
factors or the presentation style can have a strong
influence on the persuasive power of arguments.
However, these factors can at most masquerade
an illogical argument but not improve its logical
quality. Therefore, the logical quality is most suit-
able for assessing the (intrinsic) quality of argu-
ments and for providing feedback about written
arguments respectively.

Traditionally, there are two different perspec-
tives on the logical quality of arguments: (i) the
formal logic perspective and (ii) the informal logic
perspective. The objective of formal logic ap-
proaches is to distinguish deductively valid argu-
ments from invalid arguments (van Eemeren et al.,
1996, chapter 1.2), i.e. to recognize if the claim of
an argument follows necessarily from its premises.
However, formal logic approaches cannot be ap-
plied to everyday arguments since the vast major-
ity of arguments do not follow deductive inference
rules (Damer, 2009; van Eemeren et al., 1996).

Informal logic aims at developing theoretical
frameworks for analyzing arguments in ordinary
natural language (Groarke, 2015). These include,
for example, fallacy theories which focus on deter-
mining particular argumentative mistakes that can
be observed with a marked degree of frequency.
Current theories list various forms of fallacious ar-
guments. For instance, the framework proposed
by Damer (2009) describes 61 different fallacy

982



types. However, fallacy theories are not appropri-
ate for recognizing logically good arguments (van
Eemeren et al., 1996, p. 178) since it is unknown
if all fallacies are already known. To overcome
this limitation, Johnson and Blair (2006) proposed
three binary criteria, known as RAS-criteria, that
a logically good argument needs to fulfill:

• Relevance: An argument fulfills the relevance
criterion, if all of its premises count in favor
of the truth (or falsity) of the claim.

• Acceptability: An argument fulfills the ac-
ceptability criterion if its premises represent
undisputed common knowledge or facts.

• Sufficiency: An argument complies with the
sufficiency criterion if its premises provide
enough evidence for accepting or rejecting
the claim.

The relevance criterion addresses the relation be-
tween each premise and the claim whereas the ac-
ceptability criterion focuses on the truthfulness of
each individual premise. Both need to be eval-
uated independently for each premise of the ar-
gument. The sufficiency criterion addresses the
premises of an argument together. It is fulfilled if
the relevant premises of an argument are enough
for justifying (or rejecting) the claim. The suffi-
ciency criterion presupposes a non-empty set of
relevant premises. However, an argument can
violate the relevance criterion and comply with
the sufficiency criterion at the same time. For
instance, an argument can have several relevant
premises that are sufficient for accepting the claim
and additional premises that are not relevant to the
claim. This also implies that a sufficient argument
has a non-empty set of relevant premises but it is
unknown if all premises of a sufficient argument
are relevant to the claim.

In contrast to fallacy theories, the RAS-criteria
enable to distinguish good from bad arguments
with respect to logical quality since each argument
that complies with all three criteria is a logically
good one (Govier, 2010; Johnson and Blair, 2006).
Moreover, the RAS-criteria attribute a particular
defect to the relation between individual premises
and the claim (relevance), the truthfulness of in-
dividual premises (acceptability), or the premises
considered together (sufficiency). Therefore, they
enable purposeful feedback for resolving particu-
lar defects of weak arguments and are well suited
for argumentative writing support systems.

4 Corpus Creation

We conducted our annotation on a corpus of 402
argumentative essays that has been previously an-
notated with argumentation structures (Stab and
Gurevych, 2016). By analyzing the annotated ar-
gumentation structures, we found that each body
paragraph contains at least one argument and only
4.3% of all body paragraphs include several argu-
ments, i.e. claims supported by premises. There-
fore, we considered each body paragraph as an in-
dividual argument. This approximation has ad-
ditional practical advantages for the identifica-
tion of insufficiently supported arguments since it
does not require the identification of argumenta-
tion structures in advance and prevents potential
error propagation. Following this procedure, we
extracted 1,029 arguments with an average length
of 94.6 tokens and 4.5 sentences per argument.

4.1 Annotation Study

Three non-native annotators with excellent En-
glish proficiency independently annotated the ar-
guments as sufficient or insufficient. We used 64
arguments from the corpus for elaborating the an-
notation guideline and 20 arguments for collabora-
tive training sessions with the annotators. In these
sessions, all three annotators collaboratively ana-
lyzed arguments for resolving disagreements and
obtaining a common understanding of the annota-
tion guideline. For the actual annotation task, we
used the freely available brat rapid annotation tool
(Stenetorp et al., 2012).

4.1.1 Inter-Annotator Agreement
All three annotators independently annotated an
evaluation set of 433 arguments. We evaluated
the agreement between the annotators using sev-
eral inter-annotator agreement measures imple-
mented in DKPro Agreement (Meyer et al., 2014).
We used observed agreement and the two chance-
corrected measures Fleiss’ κ (Fleiss, 1971) and
Krippendorff’s α with nominal distance function
(Krippendorff, 1980). The three annotators agreed
on 91.07% of all 433 arguments (observed agree-
ment). The chance-corrected agreement scores of
κ = .7672 and α = .7673 show substantial agree-
ment between the annotators which allows “ten-
tative conclusions” (Krippendorff, 1980). There-
fore, we conclude that human annotators can reli-
ably identify insufficiently supported arguments in
argumentative essays.

983



4.1.2 Analysis of Disagreements

In order to identify the reasons for the disagree-
ments, we manually investigated all arguments on
which the annotators disagreed. We found that a
high proportion of these arguments include modal
verbs in their claims. The following example il-
lustrates such an argument:

“Watching television too often can have
a negative effect on communication abil-
ities. For instance, children often prefer
watching cartoons or movies instead of
meeting their classmates and thus they
will not learn how to communicate prop-
erly.”

Due to the modal verb “can” in the claim of this
argument (first sentence), it is sufficient to pro-
vide one specific example as premise. However,
annotators tend to overlook modal verbs and over-
hastily annotate these arguments as insufficient.

The second most frequent cause of disagree-
ments is due to the length of the arguments. In par-
ticular, one annotator annotated remarkably fewer
arguments as insufficient. These arguments ex-
hibit a comparatively large number of premises.
This indicates that longer arguments are more
likely to be perceived as sufficient than shorter ar-
guments.

We also observed that several disagreements are
due to hard cases. For instance, assessing the
sufficiency of the following argument depends on
the subjective interpretation of the undetermined
quantification “many” in the claim:

“Living in big cities provides many op-
portunities. First of all, it will be easier
to find a job in a city. Also there are
various bars and clubs where you can
meet new people. Above all there are
shopping malls and cinemas for spend-
ing your free time.”

We also found that annotators do not agree on ar-
guments including terms such as “some”, “vari-
ous”, or “large number”. Thus, extending the an-
notation guideline with an explanation of how to
handle modal verbs, the number of premises and
undetermined qualifiers could further improve the
agreement between the annotators in future anno-
tation studies.

4.2 Creation of the Final Corpus
We merged the annotations of the three annota-
tors on the evaluation set using majority voting.
The remaining arguments have been annotated by
the two annotators with the highest pairwise agree-
ment on the evaluation set (α = .815). Disagree-
ments on the remaining arguments have been man-
ually resolved in discussions among the two anno-
tators. Table 1 shows an overview of the corpus.

size
tokens 97,370
sentences 4,593
arguments 1,029

class distribution
sufficient 681 (66.2%)
insufficient 348 (33.8%)

Table 1: Size of the final corpus and class distri-
bution of sufficiency annotations.

The class distribution is skewed towards suffi-
ciently supported arguments. However, the pro-
portion of 33.8% insufficiently supported argu-
ments indicates that students frequently do not
support their claims with sufficient evidence.

5 Experiments

We consider the identification of insufficiently
supported arguments as a binary classification task
and label each body paragraph as sufficient or in-
sufficient. For preventing errors in model assess-
ment due to a particular data splitting (Krstajic
et al., 2014), we used a repeated 5-fold cross-
validation setup and ensured that arguments from
the same essay are not distributed over the train,
test and development sets. We repeated the cross-
validation 20 times which yields a total of 100
folds. As evaluation scores, we used accuracy and
macro F1 score as well as the F1 score, precision
and recall of the class “insufficient”. Whereas the
precision indicates the performance of the model
to identify arguments that are really in need of
revision, recall shows how well the model rec-
ognizes all insufficiently supported arguments in
an essay. All evaluation scores are reported as
average including the standard deviation over the
100 folds. In order to determine the macro F1
score, we employ macro-averaging as proposed
by Sokolova and Lapalme (2009, p. 430). For
model selection and hyperparameter tuning, we
randomly sampled 10% of the training set of each

984



fold as a development set. For significance testing,
we employ Wilcoxon signed-rank test on macro
F1 scores with a significance level of α = .005.

We employ several models from the DKPro
Framework (Eckart de Castilho and Gurevych,
2014) for preprocessing. We use the language tool
segmenter2 for tokenization and sentence splitting.
We employ the Stanford parser (Klein and Man-
ning, 2003) and named entity recognizer (Finkel
et al., 2005) for constituency parsing and recog-
nizing organizations, persons and locations. Note
that only the model described in Section 5.2 re-
quires all preprocessing steps. All other models
use only the tokenization of the language tool seg-
menter.

5.1 Baselines

For our experiments, we use the following two
baselines: First, we employ a majority baseline
that classifies each argument as sufficient. Second,
we use a support vector machine with polynomial
kernel implemented in the Weka framework (Hall
et al., 2009). We employ the 4,000 most frequent
lowercased words as binary features and refer to
this model as SVM-bow.

5.2 Manually Created Features (SVM)

Our first system is based on manually created fea-
tures. As a learner, we use the same support vec-
tor machine as for SVM-bow. For feature ex-
traction and experimentation, we use the DKPro
TC text classification framework (Daxenberger et
al., 2014). We tried various features which have
been used previously for assessing the quality
or the persuasiveness of arguments (cf. Section
2). For instance, we experimented with argument
structures (Stab and Gurevych, 2014), transitional
phrases (Persing and Ng, 2015), semantic roles
(Das et al., 2014) and discourse relations (Lin et
al., 2014). However, we found that only the fol-
lowing features are effective for recognizing insuf-
ficiently supported arguments:
Lexical: To capture lexical properties, we employ
the 4,000 most frequent lowercased words as bi-
nary features analogous to SVM-bow.
Length: We use the number of tokens and the
number of sentences as features since sufficiently
supported arguments might exhibit more premises
than insufficiently supported arguments (cf. Sec-
tion 4.1.2).

2https://www.languagetool.org/

Syntax: For capturing syntactic properties, we ex-
tract binary production rules from the constituent
parse trees of each sentence of the argument as de-
scribed by Stab and Gurevych (2014).
Named Entities (ner): We assume that arguments
with insufficient support refer to particular entities
in order to justify more general claims (cf. ex-
ample 1 in Section 1). Thus, we add the num-
ber of named entities appearing in the argument
and the average occurrence of named entities per
sentence to our feature set. We consider organiza-
tions, persons and locations separately. Thus the
named entity features comprise six features in to-
tal, i.e. three binary and three numeric features.

5.3 Convolutional Neural Network (CNN)
Our second model is a convolutional neural net-
work with max-over time pooling (Collobert et al.,
2011). We use the implementation provided by
Kim (2014). The selection of this model is moti-
vated by the excellent performance that the model
achieves in many different classification tasks like
sentiment classification of question classification.
We found in our experiments that instead of using
several convolutional layers with different window
sizes, a single convolutional layer with a window
size of 2 and 250 feature maps performs best.
For representing each word of an argument, we
use word embeddings trained on the google news
data set by Mikolov et al. (2013). In order to
adapt these vectors to the identification of insuf-
ficient arguments, we use non-static word vectors
as proposed by Kim (2014). We train the net-
work with stochastic gradient descent over shuf-
fled mini-batches with the Adadelta update rule
(Zeiler, 2012), a dropout rate of .5 and a mini-
batch size of 50. For finding the best model, we
apply early stopping on the development sets.

5.4 Results
Table 2 shows the results of the model assessment
on the test sets. The SVM-bow model with un-
igram features achieves .755 macro F1 score and
.785 accuracy. It significantly outperforms the ma-
jority baseline by .357 macro F1 score which indi-
cates that lexical features are informative for iden-
tifying insufficiently supported arguments. The
support vector machine with manually created fea-
tures significantly outperforms both the majority
baseline and SVM-bow. It achieves .798 accu-
racy and .770 macro F1 score and thus outper-
forms the SVM-bow model by .015 macro F1

985



Accuracy Macro F1 F1 Insufficient Precision Recall
Human Upper Bound∗ .911±.022 .887±.026 .940±.015 .863±.058 .808±.109
Baseline Majority .662±.033 .398±.012 0 0 0
Baseline SVM-bow † .785±.029 .755±.034 .661±.051 .709±.067 .624±.067
SVM †‡ .798±.028 .770±.032 .681±.047 .731±.060 .641±.061
CNN †‡ .843±.025 .827±.027 .770±.039 .762±.054 .784±.068

Table 2: Results of model assessment on the test sets and comparison to human upper bound († significant
improvement over baseline majority; ‡ significant improvement over Baseline SVM-bow; ∗determined
on a subset of 433 arguments).

score. We obtain the best performance by using
the CNN model. It significantly outperforms all
other models with respect to all evaluation scores
and achieves .827 macro F1 score and an accuracy
of .843. The results also show that the SVM model
with manually created features achieves a consid-
erably lower recall compared to precision. Thus,
the model is less suitable for exhaustively finding
all insufficiently supported arguments. In contrast,
the CNN model is more balanced with respect to
precision and recall and considerably outperforms
the recall of the SVM model. Therefore, the CNN
model outperforms the SVM model in finding in-
sufficiently supported arguments in argumentative
essay and performs better for recognizing argu-
ments that are really in need of revision.

We determine the human upper bound by av-
eraging the evaluation scores of all three annota-
tor pairs on the 433 independently annotated argu-
ments (cf. Section 4). Human annotators achieve
an accuracy of .911. The CNN model yields only
.068 less accuracy compared to the human upper
bound and thus achieves 92.5% of human perfor-
mance.

5.5 Feature Analysis
Although the CNN model outperforms the support
vector machine with manual features, we analyzed
the features for gaining a better understanding of
insufficiently supported arguments and to inves-
tigate which linguistic properties are informative
for recognizing arguments with insufficient sup-
port. Table 3 shows the macro F1 scores of the
support vector machine using individual features
and the results of feature ablation tests on the de-
velopment sets.

The results show that lexical features are most
effective for identifying insufficiently supported
arguments. They achieve the best macro F1 score
of .749 when used individually. Removing lexical
features from the feature set also yields the highest

Macro F1 F1 Insuf. F1 Suf.
BS Majority .396±.020 0 .793±.041
only lexical .749±.048 .649±.070 .835±.040
only length .397±.023 .002±.015 .792±.040
only syntax .640±.063 .502±.101 .767±.047
only ner .681±.059 .410±.114 .823±.039
all w/o lexical .658±.059 .529±.093 .776±.045
all w/o length .766±.049 .674±.068 .847±.040
all w/o syntax .755±.049 .659±.070 .839±.040
all w/o ner .760±.050 .666±.069 .843±.041
all features .768±.049 .677±.068 .848±.040

Table 3: Results of the SVM using individual fea-
tures and feature ablation tests on the dev sets.

decrease in macro F1 score compared to the other
features. The second best features are named en-
tities. Using only named entity features yields a
macro F1 score of .681. Thus, we can confirm
our assumption that named entities are informa-
tive features for assessing the sufficiency of argu-
ments. Syntactic features are also effective for rec-
ognizing insufficiently supported arguments. They
yield .640 macro F1 score when used individu-
ally. The results also show that the length of an ar-
gument is only marginally informative for assess-
ing the sufficiency of arguments. Using the length
features individually yields only a slight improve-
ment of the macro F1 score over the majority base-
line. However, removing the length from the en-
tire feature set causes a slight decrease of .002 in
the macro F1 score compared to the system which
uses all features. We achieve the best results by
combining all features.

For gaining further insights into the character-
istics of insufficiently supported arguments, we
ranked all unigrams using information gain. The
top ten words are “example”, “my”, “was”, “in-
stance”, “i”, “for”, “me”, “friend”, “he”, and
“did”. This might be an indication that examples
(signaled by the terms “example” and “instance”)

986



or personal experiences (signaled by terms such as
“me”, “my”, “friend” or “he”) are not sufficient for
developing strong arguments.

5.6 Error Analysis

In order to analyze the most frequent errors of the
convolutional neural network, we manually inves-
tigated all arguments which are wrongly classified
in each run of the repeated cross-validation exper-
iment. In total, we found 41 sufficient arguments
which are consistently misclassified as insufficient
(false positives) and 28 insufficient arguments that
are always misclassified as sufficient (false nega-
tives).

Among the false positives, we observed that 35
arguments include examples as evidence which
are signaled by terms like “example” or “in-
stance”. Thus, the model tends to overemphasize
the presence of particular lexical indicators. Most
of these arguments either refer to an example in
addition to other premises which are already suf-
ficient to support the claim or include an example
for specifying another premise. However, we also
found several false negatives which include exam-
ples as evidence. Thus, the model does not solely
rely on these lexical clues.

Among the 28 false negatives, we found 8 ar-
guments that refer to multi-word named entities
which are not captured by word embeddings. An-
other 5 false negatives support the claim by means
of personal experience and 3 ones cite numbers,
i.e. previous studies or empirical evidence.

6 Discussion

Although the convolutional neural network
achieves promising results, the sufficiency crite-
rion is only one of three criteria that a logically
good argument needs to fulfill. Thus, our ap-
proach is not yet able to separate logically good
from illogical arguments. In our experiments,
we also analyzed arguments with respect to the
relevance and acceptability criterion. In particu-
lar, we conducted several annotation studies with
varying guidelines and two annotators on a set
of 100 arguments. For annotating the relevance
criterion, we presented the annotated structure of
each argument to the annotators and asked them to
assess the relevance of each premise for the claim
individually. In order to evaluate the acceptability
criterion, we asked the annotators to mark each
premise as acceptable if it represents undisputed

common knowledge or a fact. However, we
found that human annotators hardly agree on
these criteria. We obtained low agreement scores
of κ = .435 for the relevance criterion and
κ = .259 for the acceptability criterion, which
is not sufficient for creating a reliable corpus.
In addition, we found that the violations of the
relevance and acceptability criteria are less fre-
quent than violations of the sufficiency criterion in
argumentative essays. We observed that only 15%
of the arguments include a premise that violates
the relevance criterion and 14% of all premises
violate the acceptability criterion. Although this
imbalance explains the low agreement scores
(Artstein and Poesio, 2008, p. 573), it also poses
additional requirements for the size of the corpus
and for computational models.

Although we didn’t obtain adequate agreement
scores for the acceptability and relevance criteria,
we implemented a system that identifies insuffi-
ciently supported arguments in argumentative es-
says with a reasonable accuracy. Given that suf-
ficiency flaws are the most frequent quality de-
fects in argumentative essays, our system repre-
sents an important milestone for realizing argu-
mentative writing support systems.

7 Conclusion

We presented a novel approach for assessing the
quality of natural language arguments. In par-
ticular, we focused on the sufficiency criterion
that each logically good argument needs to ful-
fill. Previous approaches on argument quality
are of limited use for argumentative writing sup-
port systems since they are not capable of rec-
ognizing particular weaknesses in argumentative
texts. To overcome this limitation, we conducted
an empirical study on the applicability of the suf-
ficiency criterion to real arguments in argumen-
tative essays. The inter-annotator agreement of
α = .7673 shows that human annotators substan-
tially agree in this annotation task and confirms
that humans can reliably separate sufficiently sup-
ported arguments from insufficiently supported ar-
guments. We introduced a novel corpus annotated
with the sufficiency criterion for studying logical
mistakes in argumentation. This corpus is freely
available for ensuring the reproducibility of our
results and to encourage future research on ar-
gument quality. Furthermore, we presented the
results of our experiments for automatically rec-

987



ognizing if an argument is sufficiently supported
or not. We found that convolutional neural net-
works significantly outperform challenging base-
lines and manually created features with a macro
F1 score of .827 and an accuracy of .843. More-
over, we showed that insufficiently supported ar-
guments frequently exhibit particular lexical indi-
cators. In addition, the feature analysis revealed
that named entities and syntactic features are good
indicators for separating sufficiently supported ar-
guments from insufficiently supported arguments.

For future work, we plan to continue with our
experiments with the relevance and acceptability
criteria. In addition, we plan to integrate our
method in writing environments for evaluating its
effectiveness for supporting authors.

Acknowledgments

This work has been supported by the Volk-
swagen Foundation as part of the Lichtenberg-
Professorship Program under grant No. I/82806
and by the German Federal Ministry of Education
and Research (BMBF) as a part of the Software
Campus project AWS under grant No. 01|S12054.
We thank our annotators Can Diehl and Radhika
Gaonkar for their valuable contributions and the
anonymous reviewers for their helpful comments.

References
Ehud Aharoni, Anatoly Polnarov, Tamar Lavee, Daniel

Hershcovich, Ran Levy, Ruty Rinott, Dan Gutfre-
und, and Noam Slonim. 2014. A benchmark dataset
for automatic detection of claims and evidence in the
context of controversial topics. In Proceedings of
the First Workshop on Argumentation Mining, pages
64–68, Baltimore, MD, USA.

Ron Artstein and Massimo Poesio. 2008. Inter-coder
agreement for computational linguistics. Computa-
tional Linguistics, 34(4):555–596.

Elena Cabrio and Serena Villata. 2012. Combin-
ing textual entailment and argumentation theory for
supporting online debates interactions. In Proceed-
ings of the 50th Annual Meeting of the Association
for Computational Linguistics (Volume 2: Short Pa-
pers), ACL ’12, pages 208–212, Jeju Island, Korea.

Amparo Elizabeth Cano-Basave and Yulan He. 2016.
A study of the impact of persuasive argumentation in
political debates. In Proceedings of the 2016 Con-
ference of the North American Chapter of the Asso-
ciation for Computational Linguistics: Human Lan-
guage Technologies, pages 1405–1413, San Diego,
California.

Lucas Carstens and Francesca Toni. 2015. Towards
relation based argumentation mining. In Proceed-
ings of the 2nd Workshop on Argumentation Mining,
pages 29–34, Denver, CO, USA.

Ronan Collobert, Jason Weston, Léon Bottou, Michael
Karlen, Koray Kavukcuoglu, and Pavel Kuksa.
2011. Natural language processing (almost) from
scratch. Journal of Machine Learning Research,
12:2493–2537.

T. Edward Damer. 2009. Attacking Faulty Reason-
ing: A Practical Guide to Fallacy-Free Reasoning.
Wadsworth Cengage Learning, 6th edition.

Dipanjan Das, Desai Chen, André F. T. Martins,
Nathan Schneider, and Noah A. Smith. 2014.
Frame-semantic parsing. Computational Linguis-
tics, 40:1:9–56.

Johannes Daxenberger, Oliver Ferschke, Iryna
Gurevych, and Torsten Zesch. 2014. DKPro TC:
A Java-based framework for supervised learning
experiments on textual data. In Proceedings of
the 52nd Annual Meeting of the Association for
Computational Linguistics. System Demonstrations,
ACL ’14, pages 61–66, Baltimore, MD, USA.

Phan Minh Dung. 1995. On the acceptability of ar-
guments and its fundamental role in nonmonotonic
reasoning, logic programming and n-person games.
Artificial Intelligence, 77:321–357.

Richard Eckart de Castilho and Iryna Gurevych. 2014.
A broad-coverage collection of portable NLP com-
ponents for building shareable analysis pipelines.
In Proceedings of the Workshop on Open In-
frastructures and Analysis Frameworks for HLT
(OIAF4HLT) at COLING 2014, pages 1–11, Dublin,
Ireland.

Judith Eckle-Kohler, Roland Kluge, and Iryna
Gurevych. 2015. On the role of discourse markers
for discriminating claims and premises in argumen-
tative discourse. In Proceedings of the 2015 Con-
ference on Empirical Methods in Natural Language
Processing, EMNLP ’15, pages 2236–2242, Lisbon,
Portugal.

Jenny Rose Finkel, Trond Grenager, and Christopher
Manning. 2005. Incorporating non-local informa-
tion into information extraction systems by gibbs
sampling. In Proceedings of the 43rd Annual Meet-
ing of the Association for Computational Linguis-
tics, ACL ’05, pages 363–370, Ann Arbor, Michi-
gan.

Joseph L. Fleiss. 1971. Measuring nominal scale
agreement among many raters. Psychological Bul-
letin, 76(5):378–382.

Trudy Govier. 2010. A Practical Study of Argument.
Wadsworth, Cengage Learning, 7th edition.

Leo Groarke. 2015. Informal logic. In Edward N.
Zalta, editor, The Stanford Encyclopedia of Philoso-
phy. Summer 2015 edition.

988



Ivan Habernal and Iryna Gurevych. 2016a. Ar-
gumentation mining in user-generated web dis-
course. Computational Linguistics, arXiv preprint
arXiv:1601.02403v4, page (in press).

Ivan Habernal and Iryna Gurevych. 2016b. Which ar-
gument is more convincing? analyzing and predict-
ing convincingness of web arguments using bidirec-
tional lstm. In Proceedings of the 54th Annual Meet-
ing of the Association for Computational Linguistics
(Volume 1: Long Papers), pages 1589–1599, Berlin,
Germany.

Mark Hall, Eibe Frank, Geoffrey Holmes, Bernhard
Pfahringer, Peter Reutemann, and Ian H. Witten.
2009. The weka data mining software: An update.
SIGKDD Explor. Newsl., 11(1):10–18.

Ralph H. Johnson and Anthony J. Blair. 2006. Logical
Self-Defense. International Debate Education Asso-
ciation.

Yoon Kim. 2014. Convolutional neural networks for
sentence classification. In Proceedings of the 2014
Conference on Empirical Methods in Natural Lan-
guage Processing, EMNLP ’14, pages 1746–1751,
Doha, Qatar.

Dan Klein and Christopher D. Manning. 2003. Ac-
curate unlexicalized parsing. In Proceedings of the
41st Annual Meeting on Association for Computa-
tional Linguistics - Volume 1, ACL ’03, pages 423–
430, Sapporo, Japan.

Klaus Krippendorff. 1980. Content Analysis: An In-
troduction to its Methodology. Sage.

Damjan Krstajic, Ljubomir J. Buturovic, David E.
Leahy, and Simon Thomas. 2014. Cross-validation
pitfalls when selecting and assessing regression and
classification models. Journal of Cheminformatics,
6(10).

Namhee Kwon, Liang Zhou, Eduard Hovy, and Stu-
art W. Shulman. 2007. Identifying and classifying
subjective claims. In Proceedings of the 8th An-
nual International Conference on Digital Govern-
ment Research: Bridging Disciplines & Domains,
pages 76–81, Philadelphia, PA, USA.

Ran Levy, Yonatan Bilu, Daniel Hershcovich, Ehud
Aharoni, and Noam Slonim. 2014. Context depen-
dent claim detection. In Proceedings of the 25th In-
ternational Conference on Computational Linguis-
tics, COLING ’14, pages 1489–1500, Dublin, Ire-
land.

Ziheng Lin, Hwee Tou Ng, and Min-Yen Kan. 2014.
A pdtb-styled end-to-end discourse parser. Natural
Language Engineering, 20(2):151–184.

Christian M. Meyer, Margot Mieskes, Christian Stab,
and Iryna Gurevych. 2014. Dkpro agreement: An
open-source java library for measuring inter-rater
agreement. In Proceedings of the 25th International
Conference on Computational Linguistics: System

Demonstrations (COLING), pages 105–109, Dublin,
Ireland.

Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Cor-
rado, and Jeff Dean. 2013. Distributed representa-
tions of words and phrases and their compositional-
ity. In Advances in Neural Information Processing
Systems 26, pages 3111–3119. Curran Associates,
Inc.

Raquel Mochales-Palau and Marie-Francine Moens.
2009. Argumentation mining: The detection, classi-
fication and structure of arguments in text. In Pro-
ceedings of the 12th International Conference on Ar-
tificial Intelligence and Law, ICAIL ’09, pages 98–
107, Barcelona, Spain.

Marie-Francine Moens, Erik Boiy, Raquel Mochales
Palau, and Chris Reed. 2007. Automatic detection
of arguments in legal texts. In Proceedings of the
11th International Conference on Artificial Intelli-
gence and Law, ICAIL ’07, pages 225–230, Stan-
ford, CA, USA.

Joonsuk Park and Claire Cardie. 2014. Identifying
appropriate support for propositions in online user
comments. In Proceedings of the First Workshop
on Argumentation Mining, pages 29–38, Baltimore,
MA, USA.

Andreas Peldszus and Manfred Stede. 2015. Joint
prediction in mst-style discourse parsing for argu-
mentation mining. In Proceedings of the 2015 Con-
ference on Empirical Methods in Natural Language
Processing, EMNLP ’15, pages 938–948, Lisbon,
Portugal.

Isaac Persing and Vincent Ng. 2015. Modeling ar-
gument strength in student essays. In Proceedings
of the 53rd Annual Meeting of the Association for
Computational Linguistics and the 7th International
Joint Conference on Natural Language Processing
(Volume 1: Long Papers), ACL ’15, pages 543–552,
Beijing, China.

Chris Reed, Raquel Mochales-Palau, Glenn Rowe, and
Marie-Francine Moens. 2008. Language resources
for studying argument. In Proceedings of the Sixth
International Conference on Language Resources
and Evaluation, LREC ’08, pages 2613–2618, Mar-
rakech, Morocco.

Ruty Rinott, Lena Dankin, Carlos Alzate Perez,
Mitesh M. Khapra, Ehud Aharoni, and Noam
Slonim. 2015. Show me your evidence - an au-
tomatic method for context dependent evidence de-
tection. In Proceedings of the 2015 Conference on
Empirical Methods in Natural Language Process-
ing, EMNLP ’15, pages 440–450, Lisbon, Portugal.

Christos Sardianos, Ioannis Manousos Katakis, Geor-
gios Petasis, and Vangelis Karkaletsis. 2015. Argu-
ment extraction from news. In Proceedings of the
2nd Workshop on Argumentation Mining, pages 56–
66, Denver, CO, USA.

989



Marina Sokolova and Guy Lapalme. 2009. A system-
atic analysis of performance measures for classifica-
tion tasks. Information Processing & Management,
45(4):427–437.

Swapna Somasundaran and Janyce Wiebe. 2009. Rec-
ognizing stances in online debates. In Proceedings
of the 47th Annual Meeting of the ACL and the 4th
IJCNLP of the AFNLP, ACL ’09, pages 226–234,
Suntec, Singapore.

Yi Song, Michael Heilman, Beata Beigman Klebanov,
and Paul Deane. 2014. Applying argumentation
schemes for essay scoring. In Proceedings of the
First Workshop on Argumentation Mining, pages
69–78, Baltimore, MA, USA.

Christian Stab and Iryna Gurevych. 2014. Identify-
ing argumentative discourse structures in persuasive
essays. In Proceedings of the 2014 Conference on
Empirical Methods in Natural Language Process-
ing, EMNLP ’14, pages 46–56, Doha, Qatar.

Christian Stab and Iryna Gurevych. 2016. Parsing ar-
gumentation structures in persuasive essays. arXiv
preprint arXiv:1604.07370.

Christian Stab, Christian Kirschner, Judith Eckle-
Kohler, and Iryna Gurevych. 2014. Argumenta-
tion mining in persuasive essays and scientific ar-
ticles from the discourse structure perspective. In
Proceedings of the Workshop on Frontiers and Con-
nections between Argumentation Theory and Natu-
ral Language Processing, pages 40–49, Bertinoro,
Italy.

Pontus Stenetorp, Sampo Pyysalo, Goran Topić,
Tomoko Ohta, Sophia Ananiadou, and Jun’ichi Tsu-
jii. 2012. Brat: A web-based tool for nlp-assisted
text annotation. In Proceedings of the Demonstra-
tions at the 13th Conference of the European Chap-
ter of the Association for Computational Linguistics,
EACL ’12, pages 102–107, Avignon, France.

Frans H. van Eemeren, Rob Grootendorst, and Fran-
cisca Snoeck Henkemans. 1996. Fundamentals
of Argumentation Theory: A Handbook of Histori-
cal Backgrounds and Contemporary Developments.
Routledge, Taylor & Francis Group.

Marilyn Walker, Jean Fox Tree, Pranav Anand, Rob
Abbott, and Joseph King. 2012. A corpus for re-
search on deliberation and debate. In Proceedings
of the Eight International Conference on Language
Resources and Evaluation (LREC’12), pages 812–
817, Istanbul, Turkey.

Zhongyu Wei, Yang Liu, and Yi Li. 2016. Is this post
persuasive? ranking argumentative comments in on-
line forum. In Proceedings of the 54th Annual Meet-
ing of the Association for Computational Linguistics
(Volume 2: Short Papers), pages 195–200, Berlin,
Germany.

Matthew D. Zeiler. 2012. Adadelta: an adaptive learn-
ing rate method. arXiv preprint arXiv:1212.5701.

990


