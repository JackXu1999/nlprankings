



















































Learning from a Neighbor: Adapting a Japanese Parser for Korean Through Feature Transfer Learning


Language Technology for Closely Related Languages and Language Variants (LT4CloseLang), pages 2–12,
October 29, 2014, Doha, Qatar. c©2014 Association for Computational Linguistics

Learning from a Neighbor: Adapting a Japanese Parser for Korean
through Feature Transfer Learning

Hiroshi Kanayama
IBM Research - Tokyo
Koto-ku, Tokyo, Japan
hkana@jp.ibm.com

Youngja Park
IBM Research - T.J.Watson Research Center

Yorktown Heights, NY, USA
young park@us.ibm.com

Yuta Tsuboi
IBM Research - Tokyo
Koto-ku, Tokyo, Japan
yutat@jp.ibm.com

Dongmook Yi
Korea Software Solutions Laboratory, IBM Korea

Gangnam-gu, Seoul, Korea
dmyi@kr.ibm.com

Abstract

We present a new dependency parsing
method for Korean applying cross-lingual
transfer learning and domain adaptation
techniques. Unlike existing transfer learn-
ing methods relying on aligned corpora or
bilingual lexicons, we propose a feature
transfer learning method with minimal su-
pervision, which adapts an existing parser
to the target language by transferring the
features for the source language to the tar-
get language. Specifically, we utilize the
Triplet/Quadruplet Model, a hybrid pars-
ing algorithm for Japanese, and apply a
delexicalized feature transfer for Korean.
Experiments with Penn Korean Treebank
show that even using only the transferred
features from Japanese achieves a high
accuracy (81.6%) for Korean dependency
parsing. Further improvements were ob-
tained when a small annotated Korean cor-
pus was combined with the Japanese train-
ing corpus, confirming that efficient cross-
lingual transfer learning can be achieved
without expensive linguistic resources.

1 Introduction

Motivated by increasing demands for advanced
natural language processing (NLP) applications
such as sentiment analysis (Pang et al., 2002;
Nasukawa and Yi, 2003) and question answer-
ing (Kwok et al., 2001; Ferrucci et al., 2010), there
is a growing need for accurate syntactic parsing
and semantic analysis of languages, especially for
non-English languages with limited linguistic re-
sources. In this paper, we propose a new depen-
dency parsing method for Korean which requires

minimal human supervision. Dependency parsing
can handle long-distance relationships and coor-
dination phenomena very well, and has proven to
be very effective for parsing free-order languages
such as Korean and Japanese (Kübler et al., 2009).

Most statistical parsing methods rely on anno-
tated corpora labeled with phrase structures or
dependency relationships, but it is very expen-
sive to create a large number of consistent anno-
tations. Recently, treebanks have become avail-
able for many languages such as English, Ger-
man, Arabic, and Chinese. However, the pars-
ing results on these treebanks vary a lot depend-
ing on the size of annotated sentences and the type
of annotations (Levy and Manning, 2003; Mc-
Donald et al., 2013). Further, many languages
lack annotated corpus, or the size of the anno-
tated corpus is too small to develop a reliable sta-
tistical method. To address these problems, there
have been several attempts at unsupervised pars-
ing (Seginer, 2007; Spitkovsky et al., 2011), gram-
mar induction (Klein and Manning, 2004; Naseem
et al., 2010), and cross-lingual transfer learning
using annotated corpora of other languages (Mc-
Donald et al., 2011). However, the accuracies of
unsupervised methods are unacceptably low, and
results from cross-lingual transfer learning show
different outcomes for different pairs of languages,
but, in most cases, the parsing accuracy is still low
for practical purposes. A recent study by McDon-
ald et al. (2013) concludes that cross-lingual trans-
fer learning is beneficial when the source and tar-
get languages were similar. In particular, it reports
that Korean is an outlier with the lowest scores
(42% or less in UAS) when a model was trained
from European languages.

In this paper, we present a new cross-lingual

2



transfer learning method that learns a new model
for the target language by transferring the fea-
tures for the source language. Unlike other ap-
proaches which rely on aligned corpora or a
bilingual lexicon, we learn a parsing model for
Korean by reusing the features and annotated
data used in the Japanese dependency parsing,
the Triplet/Quadruplet Model (Kanayama et al.,
2000), which is a hybrid approach utilizing both
grammatical knowledge and statistics.

We exploit many similarities between the two
languages, such as the head-final structure, the
noun to verb modification via case and topic mark-
ers, and the similar word-order constraints. It was
reported that the mapping of the grammar formal-
ism in the language pair was relatively easy (Kim
et al., 2003b; Kim et al., 2003a). However, as the
two languages are classified into independent lan-
guage families (Gordon and Grimes, 2005), there
are many significant differences in their morphol-
ogy and grammar (especially in the writing sys-
tems), so it is not trivial to handle the two lan-
guages in a uniform way.

We show the Triplet/Quadruplet Model is suit-
able for bilingual transfer learning, because the
grammar rules and heuristics reduce the number
of modification candidates and can mitigate the
differences between two languages efficiently. In
addition, this model can handle the relationships
among the candidates as a richer feature space,
making the model less dependent upon the lexi-
cal features of the content words that are difficult
to align between the two languages. Similarly to
the delexicalized parsing model in (McDonald et
al., 2011), we transfer only part-of-speech infor-
mation of the features for the content words. We
create new mapping rules to extract syntactic fea-
tures for Korean parsing from the Japanese anno-
tated corpus and refine the grammar rules to get
closer modification distributions in two languages.

Our experiments with Penn Korean Tree-
bank (Han et al., 2002) confirm that the
Triplet/Quadruplet Model adapted for Korean out-
performs a distance-based dependency parsing
method, achieving 81.6% accuracy when no an-
notated Korean corpus was used. Further perfor-
mance improvements were obtained when a small
size of annotated Korean corpus was added, con-
firming that our algorithm can be applied with-
out more expensive linguistic resources such as an
aligned corpora or bilingual lexicons. Moreover,

the delexicalized feature transfer method enables
the algorithm applicable to any two languages that
have similar syntactic structures.

2 Related Work

2.1 Parsing for Korean

Since Korean is a morphologically-rich language,
many efforts for Korean parsing have focused
on automatically extracting rich lexical informa-
tion such as the use of case frame patterns for
the verbs (Lee et al., 2007), the acquisition of
case frames and nominal phrases from raw cor-
pora (Park et al., 2013), and effective features
from phrases and their neighboring contexts (Choi
and Palmer, 2011). Recently, Choi et al. (2012)
discussed the transformation of eojeol-based Ko-
rean treebank to entity-based treebank to effec-
tively train probabilistic CFG parsers. We apply
similar techniques as in (Choi et al., 2012) to miti-
gate the differences between Korean and Japanese
syntactic structures.

Chung and Rim (2003) applied the
Triplet/Quadruplet Model for Korean parsing
as done in our work. They reported that the model
performed well for long-distance dependencies,
but, in their experiments, the number of modi-
fication candidates was not effectively reduced
(only 91.5% of phrases were in one of the three
positions, while it was 98.6% in Kanayama’s
work for Japanese). In this paper, we introduce
more sophisticated grammatical knowledge and
heuristics to have similar dependency distribu-
tions in the two languages. Smith and Smith
(2004) attempted a bilingual parsing for English
and Korean by combining statistical dependency
parsers, probabilistic context-free grammars, and
word translation models into a unified framework
that jointly searches for the best English parse,
Korean parse and word alignment. However, we
utilize an existing parser and align the features
from the source language to the features for
the target language, and, thus, our method is
applicable to situations where there is no aligned
corpora or word translation models.

2.2 Transfer learning and domain adaptation

Recently, transfer learning has attracted much at-
tention, as it can overcome the lack of training
data for new languages or new domains for both
classification and regression tasks (Pan and Yang,
2010). Transfer learning has also been applied to

3



아내가
아내/NNC이/PCA

산
사/VV은/EAN

프랑스의
프랑스/NPR의/PAN

여행
여행/NNC

가방을
가방/NNC을/PCA

친구에게
친구/NNC에게/PAD

보이고
보이/VV고/ECS

싶다
싶/VX다/EFN

．
./SFN

‘wife-NOM’ ‘buy-PAST’ ‘France-GEN’ ‘travel’ ‘bag-ACC’ ‘friend-DAT’ ‘show’ ‘want’ ‘.’
e1 e2 e3 e4 e5 e6 e7 e8 e9

? ??? ?? ? ?

Figure 1: An example of dependency structures of a Korean sentence “아내가산프랑스의여행가방
을 친구에게 보이고 싶다.” (‘(I) want to show the French travel bag which (my) wife bought to (my)
friend’). Each box corresponds to a Korean phrasal unit eojeol.

妻が
妻/nが/pc

買った
買/vた/aux

フランスの
フランス/npの/pn

旅行かばんを
旅行/nかばん/nを/pc

友達に
友達/nに/pc

見せたい。
見せ/vたい/aux。/

‘wife-NOM’ ‘buy-PAST’ ‘France-GEN’ ‘travel bag-ACC’ ‘friend-DAT’ ‘want to show’
b1 b2 b3 b4 b5 b6

? ?? ??

Figure 2: A dependency structure of a Japanese sentence which corresponds to the Korean sentence in
Figure 1, “妻が買ったフランスの旅行かばんを友達に見せたい。”. Each box corresponds to a Japanese
phrasal unit bunsetsu.

syntactic parsing, where a parsing model for a tar-
get language is learned from linguistic resources
in one or more different languages (Hwa et al.,
2005; Zeman and Resnik, 2008; McDonald et al.,
2011; Durrett et al., 2012; Georgi et al., 2012;
Naseem et al., 2012). McDonald et al. (2011)
proposed a delexicalized parsing model for cross-
lingual dependency parsing and demonstrated that
a high accuracy parsing was achieved for Indo-
European languages where significant amount of
parallel texts exist. However, in more recent work,
McDonald et al. (2013) showed that, unlike trans-
fer learning within close language families, build-
ing a Korean parser from European languages was
not successful with a very low accuracy. Durrett
et al. (2012) and Georgi et al. (2012) show that
transfer parsing can be improved when additional
bilingual resources are available, such as bilingual
dictionaries and parallel corpora of glossed texts
respectively.

Our method does not require such resources and
does not have restrictions on the sentence type that
can be parsed. Instead, we use a mixture of a
small corpus in a target language (i.e., Korean) and
a larger corpus of a source language (Japanese).
This task is similar to domain adaptation, and our
objective is to outperform the training model built
on each language separately. To avoid the loss of
accuracy due to the differences between two do-
mains, we apply the domain adaptation technique
proposed by Daumé III (2007) which duplicates
the feature space into three categories with each

of the features trained by source, by target, and by
combined domains.

3 Dependency Structures of Korean and
Japanese

A dependency structure in Korean is typically an-
alyzed in terms of eojeol units, a basic phrase
that consists of a content word agglutinated with
optional functional morphemes such as postposi-
tional particles or endings for verbs. Figure 1
shows an example Korean sentence with the de-
pendencies between the eojeols indicated by ar-
rows. Figure 2 illustrates the dependency struc-
ture between the bunsetsus in Japanese that cor-
responds to the Korean structure in Figure 1. As
these figures show, the syntactic structures are
quite similar in these languages: All of the de-
pendencies are directed from left to right, and the
postpositional particles determine if the content
word modifies a verb (“가” in e1 and “が” in b1)
or a noun (“의” in e3 and “の” in b3). The eojeols
in Korean roughly correspond to the bunsetsus in
Japanese. In the remainder of this paper, we de-
note both an eojeol or a bunsetsu as a ‘PU’ (phrasal
unit) when distinction is not needed.

While Korean and Japanese have similar syn-
tactic structures, the two languages have many dif-
ferences. The eojeols in Korean are separated by
white space, while the bunsetsus in Japanese are
not. Further, the statistics show several differences
in the two languages. Table 1 compares a Korean
corpus, Penn Korean Treebank (henceforth KTB)

4



Table 1: Statistics of Korean and Japanese corpora.

KTB (Korean) EDR (Japanese)

Average number of characters (except for whitespace) per sentence 73.7 28.0
Average number of PUs per sentence 25.5 8.53
Average number of morphemes per PU 1.83 2.86
Ratio of modification to the next PU 70.0% 61.8%

Table 2: Simplified examples of Japanese grammar rules.

Rightmost morpheme of the modifier PU Conditions for the modified PUs

postpositional “を” wo (accusative) verb, adjective
postpositional “の” no (genitive, nominative) noun, verb, adjective

postpositional “と” to (conjunctive) noun, verb, adjective, adverb “一緒に” isshoni (‘together’)
adverb verb, adjective, adverb, copula

(Han et al., 2002), and a Japanese corpus, EDR
Corpus (EDR, 1996). Both corpora consist of
word-level bracketed constituents, so they are con-
verted into PU-level dependency structures using
the method described in Choi and Palmer (2011).
Though both corpora consist mainly of newspaper
or magazine articles, the sentences are not aligned
with each other, so the statistics show the compar-
isons of the two corpora, rather than the theoret-
ical comparisons of the two languages. However,
we can see that Korean sentences tend to be longer
than Japanese sentences both in terms of the num-
ber of characters and PUs. More eojeols modify an
adjacent eojeol in Korean than in Japanese. For in-
stance, e1, e4, e6, e7, and e8 modify the next eojeol
in Figure 1, but only b1, b3, and b5 modify the next
bunsetsu in Figure 2. Those differences suggest
some of the difficulties in applying the Japanese
dependency model to Korean. The Japanese pars-
ing method that will be described in the next sec-
tion exploits these characteristics, which we apply
to Korean parsing.

4 Triplet/Quadruplet Model

This section describes the Triplet/Quadruplet
Model (Kanayama et al., 2000) which was origi-
nally designed for Japanese parsing. First, we re-
view the two main ideas of the model – restriction
of modification candidates and feature selection
for probability calculation. Then, we describe how
we apply the Triplet/Quadruplet Model to Korean
parsing in Section 4.3.

4.1 Restriction of modification candidates

The Triplet/Quadruplet Model utilizes a small
number (about 50) of hand-crafted grammar rules

that determine whether a PU can modify each PU
to its right in a sentence. The main goal of the
grammar rules is to maximize the coverage, and
the rules are simple describing high-level syntac-
tic dependencies, and, thus, the rules can be eas-
ily created without worrying about the precision
or contradictory rules. The statistical information
is later used to select the right rules for a given
sentence to produce an accurate parsing result. Ta-
ble 2 shows several grammar rules for Japanese, in
which the modified PUs are determined depend-
ing on the conditions of the rightmost morpheme
in the modifier PU.

An analysis of the EDR corpus shows that
98.6% of the correct dependencies are either the
nearest PU, the second nearest PU, or the farthest
PU from the modifier (more details in Table 4(a)).
Therefore, the model can be simplified by restrict-
ing the candidates to these three candidates and
by ignoring the other PUs with a small sacrifice
(1.4%) of parsing accuracy.

4.2 Calculation of modification probabilities

Let u be a modifier PU in question, cun the u’s n-
th modification candidate PU, Φu and Ψcun the at-
tributes of u and cun, respectively. Then the prob-
ability that u modifies its n-th candidate is calcu-
lated by the triplet equation (1) or the quadruplet
equation (2) when u has two or three candidates,
respectively 1.

P (u → cun) = P (n | Φu,Ψcu1 , Ψcu2) (1)
P (u → cun) = P (n | Φu,Ψcu1 , Ψcu2 , Ψcu3) (2)

1It is trivial to show that P (u → cu1) = 1, when u has
only one candidate.

5



Table 3: Simplified examples of Korean grammar rules.

Rightmost morpheme of the modifier PU Conditions for the modified PUs

PCA,PCJ,PAD,PAU (postpositional particles) V* (verb, adjective or auxiliary), CO (copula)
EAN (nominal verb ending e.g. “은” eun) N* (noun)

ADV (adverb), ADC (conjunction) N* (noun), V* (verb, adjective or auxiliary), ADV (adverb), ADC (conjunction)
postpositional “과” gwa (conjunctive) N* (noun), V* (verb, adjective or aux), adverb “함께” hamkke (‘together’)

N* (noun) N* (noun), V* (verb, adjective or auxiliary)

Table 4: Distribution (percentage) of the position of the correct modified PU among the candidate PUs
selected by the initial grammar rules. The column ‘Sum’ shows the coverage of the 1st, 2nd and last
PUs. The EDR corpus was used for Japanese, and the KTB was used for Korean in this analysis.

(a) Japanese

# of Candidates Ratio 1st 2nd Last Sum

1 32.7 100.0 − − 100.0
2 28.1 74.3 26.7 − 100.0
3 17.5 70.6 12.6 16.8 100.0
4 9.9 70.4 11.1 13.8 95.3
≥5 11.8 70.2 11.1 10.5 91.9

Total 100 − − − 98.6

(b) Korean (with the initial grammar)

# of Candidates Ratio 1st 2nd Last Sum

1 10.5 100.0 − − 100.0
2 11.4 85.9 14.1 − 100.0
3 10.4 76.2 13.4 10.4 100.0
4 9.3 74.7 11.3 8.0 93.9
≥5 58.4 75.5 10.0 4.9 90.5

Total 100 − − − 93.9

These probabilities are estimated by the maxi-
mum entropy method with a feature set to express
Φ and Ψ. Assuming the independence of those
modifications, the probability of the dependency
tree for an entire sentence P (T ) is calculated as
the product of the probabilities of all of the depen-
dencies in the sentence using beam search to max-
imize P (T ) under the constraints of the projected
structure.

P (T ) ≃
∏
u

P (u → cun) (3)

In comparison, a traditional statistical parser
(Collins, 1997) uses Equation (4) to calculate the
probability of u modifying t.

P (u → t) = P (True | Φu, Ψt, ∆u,t) (4)
We call the model based on Equation (4) the Dis-
tance Model, since ∆u,t (the distance between u
and t) is typically used as the key feature. Though
other contextual information, in addition to the at-
tributes of u and t, can be added, the model calcu-
lates the probabilities of the dependencies between
u and t independently and thus often fails to incor-
porate appropriate contextual information.

Equations (1) and (2) have two major advan-
tages over the Distance Model: First, all the at-
tributes of the modifier and its candidates can be
handled simultaneously. The combination of those
attributes helps the model to express the context of

the modifications. Second, the probability of each
modification is calculated based on the relative po-
sitions of the candidates, instead of the distance
from the modifier PU in the surface sentence, and,
thus, the model is more robust.

4.3 Korean dependency parsing with the
Triplet/Quadruplet Model

We design the Korean parser by adapting the
Triplet/Quadruplet Model based on the analogous
characteristics of Japanese and Korean. First, we
created the Korean grammar rules for generating
candidate modified PUs by modifying the rules
for Japanese shown in Table 2 for Korean. The
rule set, containing fewer than 50 rules, is sim-
ple enough to be created manually, because the
rules simply describe possible dependencies, and
Japanese phenomena are good hints for Korean
phenomena. Table 3 shows some examples of the
rules for Korean based on the POS schema used in
the KTB corpus. We did not automatically extract
the rules from the annotated corpora so that the
rules are general and independent of the training
corpus. Nonetheless, 96.6% of the dependencies
in KTB are covered by the grammar rules. The re-
maining dependencies (3.4%) not covered by the
rule set are mainly due to rare modifications and
may indicate inconsistencies in the annotations,
so we do not seek any grammar rules to achieve
nearly 100%.

6



아내가 (‘wife-NOM’) 산 (‘buy’) 보이고 (‘show’) 싶다 (‘want’)
e1 e2 e7 e8

? ? ?

⟨1⟩ NNC (common noun)
⟨2⟩ PCA (postpositional)
⟨3⟩“이” (‘-NOM’)

⟨5⟩ VV (verb)
⟨7⟩ EAN (adnominal ending)
⟨8⟩“은” (past adnominal)

⟨5⟩ VV (verb)
⟨7⟩ ECS (ending)
⟨8⟩“고” (conjunctive)

⟨5⟩ VX (auxiliary)
⟨7⟩ EFN (final ending)
⟨8⟩“다” (predicative)

Figure 3: The features used to select the modified PU of e1 among its three candidates. The full sentence
of this example is shown in Figure 1. The numbers in brackets correspond to the feature IDs in Table 5.

Table 5: The features to express attributes of a modifier and modification candidates.

Feature set ID Description

⟨1⟩ PoS of the head morpheme of the modifier
⟨2⟩ PoS of the last morpheme of the modifier
⟨3⟩ Lex of the postpositional or endings of the modifier
⟨4⟩ Lex of the adverb of the modifier
⟨5⟩ PoS of the head morpheme of the modification candidate
⟨6⟩ Lex of the head morpheme of the modification candidate
⟨7⟩ PoS of the last morpheme of the modification candidate
⟨8⟩ Lex of the postpositional or endings of the modification candidate
⟨9⟩ Existence of a quotation expression “다고” dago or “라고” rago
⟨10⟩ Number of “은” eun (TOPIC marker) between the modifier and modification candidate
⟨11⟩ Number of commas between the modifier and modification candidate

combination ⟨1⟩ × ⟨5⟩ / ⟨2⟩ × ⟨5⟩ / ⟨2⟩ × ⟨7⟩ / ⟨3⟩ × ⟨5⟩ / ⟨3⟩ × ⟨8⟩

Table 4(a) and (b) show the distribution of the
numbers of candidate PUs and the position of the
correct modified PUs obtained from the analysis
of the EDR corpus and the KTB corpus respec-
tively. As we can see, the first candidate is pre-
ferred in both languages, but the preference of the
nearer candidate is stronger in Korean. For in-
stance, when there are more than one candidates,
the probability that the first candidate is the cor-
rect one is 78% for Korean but 71% for Japanese.
Further, when there are more than 2 candidates,
Japanese prefers the last candidate, while Korean
prefers the second candidate. Based on the analy-
sis results, the number of modification candidates
is restricted to at most three (the first, second and
last candidates) for Korean as well.

The next step is to design Φu and Ψcun , which
are required in Equations (1) and (2) to choose the
correct modified PU. We converted the feature set
from the Japanese study to get the Korean features
as listed in Table 5. For example, to find the mod-
ified PU of e1 “아내가” anae-ga (’wife-NOM’) in
the sentence shown in Figure 1, the attributes of
e1 and the attributes of the three candidates, e2,
e7, and e8, are extracted as shown in Figure 3, and
their attributes are used to estimate the probability
of each candidate in Equation (2).

5 Adaptation for Bilingual Transfer
Learning

In Section 4.3, we explained how the
Triplet/Quadruplet Model can be used for
Korean. In this section, we describe the feature
adaption techniques in more detail and investigate
if the new model with transferred features works
well when a small amount of annotated corpus for
the target language is provided. Further, we study
if we can leverage the annotated corpus for the
source language in addition to the parsing model
and train a model for the target language using the
training data for the source language.

5.1 Feature Transfer

With the assumption that Korean and Japanese
have similar syntactic dependencies, we adopt
the delexicalized parsing model presented in Mc-
Donald et al. (2011). We transfer the part-of-
speech (POS) in the Japanese features to the POS
scheme in the KTB corpus, and translate Japanese
functional words to the corresponding functional
words in Korean. This transfer process is manda-
tory because we use the language specific POS
systems to capture language-specific dependency
phenomena, unlike other works using language
universal but coarser POS systems.

We do not transfer lexical knowledge on con-

7



Table 6: Example of mapping rules for parts-of-speech and functional words.

Japanese PoS Korean PoS

common noun NNC
verb VV
adjective VJ
nominal suffix XSF

case particle “で”,“に”,“へ”,“から” PAD

others PCA

Japanese particle Korean particle

“を” wo (‘-ACC’) “을” eul
“より” yori (‘from’) “부터” buteo
“は” ha (‘-TOPIC’) “은” eun
“も” mo (‘too’) “도” do

“が” ga case particle (‘-NOM’) “이” i

conjunctive particle (‘but’) “지만” jiman

tent words and exceptional cases, so feature sets
⟨4⟩ and ⟨6⟩ are not transferred. Table 6 shows
some examples of the feature transfer which han-
dle POS tags and functional words. We note that
the Korean features shown in Figure 3 are directly
extracted from Japanese corpus using those rules.

5.2 Adaptation of parsing rules
While Japanese and Korean are similar in terms of
syntactic dependencies, there are significant dif-
ferences between the two languages in the distri-
bution of modification as shown in the Table 4(a)
and (b): In Korean, more than half of modifiers
have 5 or more candidates, while only 12% of
Japanese modifiers do. In Japanese, 98.6% of cor-
rect modified PUs are located in one of the three
positions (1st, 2nd or last), but, in Korean, the ratio
falls to 93.9% as shown in Table 4. Another ma-
jor difference of the two languages is the different
average numbers of PUs per sentence as shown in
Table 1. Korean has 25.5 PUs per sentence, while
the number is only 8.5 in Japanese. This is mainly
caused by the difference between eojeol in Korean
and bunsetsu in Japanese. In Japanese, compound
nouns and verb phrases with an auxiliary verb are
likely to form a single bunsetsu, while, in Korean,
they are split into multiple eojeols with a whites-
pace in-between.

These differences significantly reduce the ef-
fect of transfer learning. To address these prob-
lems, we further refine the grammar rules as in
the following. We added heuristic rules for the
Korean model to effectively reduce the number of
candidates in compound nouns which consist of a
noun sequence in multiple eojeols, and verbs or
adjectives followed by auxiliary verbs. Figure 4
shows an algorithm to reduce the number of mod-
ified PUs considering the structure of compound
nouns. In this example, both PUs e4 (“travel”) and
e5 (“bag-ACC”) can be candidate PUs for eojeol
e3. However, based on the rule in Figure 4, e4 and
e5 are considered as a compound noun (line 1),

and e4 is determined to modify e5 (line 3). Sub-
sequently, e3’s modifiability to e4 is rejected (line
5), and, thus, the correct modified PU of e3 is de-
termined as e5. After refining the rules for com-
pound nouns and auxiliary verbs, the probability
of the correct modified PU being the 1st, 2nd or
last candidate PU increases from 93.9% to 96.3%
as shown in Table 7, and the distribution of the
candidate’s positions for Korean became closer to
the Japanese distribution shown in Table 4(a).

5.3 Learning from heterogeneous bilingual
corpora

The feature transfer and rule adaptation methods
described in previous sections generate a very ac-
curate Korean parser using only a Japanese cor-
pus as shown in the first row in Table 8. The next
question is if we can leverage bilingual corpora to
further improve the accuracy, when annotated cor-
pus for the target language (Korean) is available.
We note that the two corpora do not need to be
aligned and can come from different domains. To
mitigate the side effects of merging heterogeneous
training data in different languages, we apply the
domain adaptation method proposed by Daumé III
(2007) and augment the feature set to a source
language-specific version, a target-specific version
and a general version. Specifically, a feature set x
in Table 5 is expanded as follows:

xK=< x,0,x > (5)
xJ=< 0, x, x > (6)

where xK and xJ denote the feature sets extracted
from the Korean corpus and the Japanese corpus
respectively. Then, the features specific to Korean
and Japanese get higher weights for the first part
or the second part respectively, and the character-
istics existing in both languages influence the last
part.

8



if PoS of ui’s last morpheme is N* and PoS of ui+1’s first morpheme is N*
then

ui must modify ui+1
if ui−1’s last morpheme is not “의” then ui−1 cannot modify ui+1

else ui−1 cannot modify ui
u1 to ui−2 cannot modify ui

프랑스의
프랑스/NPR의/PAN

여행
여행/NNC

가방을
가방/NNC을/PCA

‘France-GEN’ ‘travel’ ‘bag-ACC’
e3 e4 e5

?? ?

Figure 4: Heuristic rules to reduce the number of modification candidates surrounding compound nouns
in Korean. The example in the right figure shows that candidates in the dotted lines are removed by the
heuristics.

Table 7: Distribution of the position of correct modified PU for Korean after the refinement of the Korean
grammar rules.

# of candidates Ratio 1st 2nd Last Sum

1 46.4% 100.0% − − 100.0%
2 9.8% 79.0% 21.0% − 100.0%
3 9.2% 75.5% 12.7% 11.8% 100.0%
4 8.0% 71.0% 11.8% 9.6% 92.4%
≥ 5 26.6% 70.4% 10.1% 7.8% 88.3%
Total 100% − − − 96.3%

6 Experiments

In this section, we validate the effectiveness of
learning a Korean parser using the feature transfer
learning from the Japanese parser and compare the
Korean model with other baseline cases. We also
compare the parsing results when various sizes of
bilingual corpora were used to train the Korean
model.

6.1 Korean parsing using the
Triplet/Quadruplet Model

First, to validate the effectiveness of the
Triplet/Quadruplet Model for parsing Korean, we
built eight Korean dependency parsing models us-
ing different numbers of training sentences for Ko-
rean. The KTB corpus Version 2.0 (Han et al.,
2002) containing 5,010 annotated sentences was
used in this study. We first divide the corpus into 5
subsets by putting each sentence into its (sentence
ID mod 5)-th group. We use sentences from the
first subgroup for estimating the parameters, sen-
tences from the second subgroup for testing, and
use the remaining three subgroups for training. We
built 8 models in total, using from 0 sentence up
to 3,006 sentences selected from the training set.
The number of training sentences in each model
is shown in the first column in Table 8. The pa-
rameters were estimated by the maximum entropy
method, and the most preferable tree is selected
using each dependency probability and the beam
search. The test data set contains 1,043 sentences.

We compare the Triplet/Quadruplet Model-
based models with the Distance Model. For the
Distance Model, we used the same feature set as
in Table 5, and added the distance feature (∆u,t)
by grouping the distance between two PUs into 3
categories (1, 2 to 5, and 6 or more). The perfor-
mances are measured by UAS (unlabeled attach-
ment score), and the results of the two methods
are shown in the second column, where Japanese
Corpus Size=0, in Table 8 (a) and (b) respectively.
The top leftmost cells (80.61% and 71.63%) show
the parsing accuracies without any training cor-
pora. In these cases the nearest candidate PU is
selected as the modified PU. The difference be-
tween two models suggests the effect of restriction
of modification candidates by the grammar rules.
We note that the Triplet/Quadruplet Model pro-
duces more accurate results and outperforms the
Distance Model by more than 2 percentage points
in all cases. The results confirm that the method
for Japanese parsing is suitable for Korean pars-
ing.

6.2 Results of bilingual transfer learning

Next, we evaluate the transfer learning when anno-
tated sentences for Japanese were also added. Ta-
ble 8(a) shows the accuracies of our model when
various numbers of training sentences from Ko-
rean and Japanese are used. The first row shows
the accuracies of Korean parsing when the models
were trained only with the Japanese corpus, and

9



Table 8: The accuracy of Korean dependency parsing with various numbers of annotated sentences in the
two languages. † denotes that the mixture of bilingual corpora significantly outperformed (p < .05)
the parser trained with only the Korean corpus without Japanese corpus.

(a) Triplet/Quadruplet Model

Japanese Corpus Size

0 2,500 5,000 10,000

K
or

ea
n

Co
rp

us
Si

ze

0 80.61% 80.78% 81.23% † 81.58% †

50 82.21% 82.32% 82.40% † 82.43% †

98 82.36% 82.66% † 82.69% † 82.70% †

197 83.13% 83.18% 83.30% † 83.28%

383 83.62% 83.92% † 83.94% † 83.91% †

750 84.03% 84.00% 84.06% 84.06%

1,502 84.41% 84.34% 84.32% 84.28%

3,006 84.77% 84.64% 84.64% 84.65%

(b) Distance Model

Japanese Corpus Size

0 2,500 5,000

K
or

ea
n

Co
rp

us
Si

ze

0 71.63% 62.42% 54.92%

50 79.31% 79.55% † 79.54% †

98 80.53% 80.63% † 80.72% †

197 80.91% 80.84% 80.85%

383 81.86% 81.75% 81.76%

750 82.10% 81.92% 81.94%

1,502 82.50% 82.48% 82.50%

3,006 82.66% 82.57% 82.54%

other rows show the results when the Korean and
Japanese corpora were mixed using the method
described in Section 5.3.

As we can see from the results, the bene-
fit of transfer learning is larger when the size
of the annotated corpus for Korean (i.e., target
language) is smaller. In our experiments with
Triplet/Quadruplet Model, positive results were
obtained by the mixture of the two languages when
the Korean corpus is less than 500 sentences, that
is, the annotations in the source language success-
fully compensated the small corpus of the target
language. When the size of the Korean corpus is
relatively large (≥ 1, 500 sentences), adding the
Japanese corpus decreased the accuracy slightly,
due to syntactic differences between the two lan-
guages. Also the effect of the corpus from the
source language tends to saturate as the size of
the source corpus, when the target corpus is larger.
This is mainly because our mapping rules ignore
lexical features, so few new features found in the
larger corpus were incorrectly processed.

When merging the corpus in two languages,
if we simply concatenate the transferred features
from the source language and the features from
the target language (instead of using the dupli-
cated features shown in Equations (5) and (6)), the
accuracy dropped from 82.70% to 82.26% when
the Korean corpus size was 98 and Japanese cor-
pus size was 10,000, and from 83.91% to 83.40%
when Korean=383. These results support that
there are significant differences in the dependen-
cies between two languages even if we have im-

proved the feature mapping, and our approach
with the domain adaptation technique (Daumé III,
2007) successfully solved the difficulty.

Table 8(b) shows the results of the Distance
Model. As we can see from the first row, using
only the Japanese corpus did not help the Dis-
tance Model at all in this case. The Distance
Model was not able to mitigate the differences be-
tween the two languages, because it does not use
any grammatical rules to control the modifiability.
This demonstrates that the hybrid parsing method
with the grammar rules makes the transfer learn-
ing more effective. On the other hand, the domain
adaptation method described in (5) and (6) suc-
cessfully counteracted the contradictory phenom-
ena in the two languages and increased the accu-
racy when the size of the Korean corpus was small
(size=50 and 98). This is because the interactions
among multiple candidates which cannot be cap-
tured from the small Korean corpus were provided
by the Japanese corpus.

Some of previous work reported the parsing
accuracy with the same KTB corpus; 81% with
trained grammar (Chung et al., 2010) and 83%
with Stanford parser after corpus transformation
(Choi et al., 2012), but as Choi et al. (2012) noted
it is difficult to directly compare the accuracies.

6.3 Discussion
The analysis of e2’s dependency in Figure 1
is a good example to illustrate how the
Triplet/Quadruplet Model and the Japanese
corpus help Korean parsing. Eojeol e2 has three
modification candidates, e3, e5, and e6. In the

10



Distance Model, e3 is chosen because the distance
between the two eojeols (∆e2,e3) was 1, which
is a very strong clue for dependency. Also, in
the Triplet/Quadruplet Model trained only with
a small Korean corpus, e3 received a higher
probability than e5 and e6. However, when a
larger Japanese corpus was combined with the
Korean corpus, e5 was correctly selected as the
Japanese corpus provided more samples of the
dependency relation of “verb-PAST” (e2) and
“common noun-ACC (을)” (e5) than that of
“verb-PAST” and “proper noun-GEN (의)” (e3).

As we can notice, larger contextual information
is required to make the right decision for this case,
which may not exist sufficiently in a small cor-
pus due to data sparseness. The grammar rules in
the Triplet/Quadruplet Model can effectively cap-
ture such contextual knowledge even from a rel-
atively small corpus. Further, since the grammar
rules are based only on part-of-speech tags and a
small number of functional words, they are sim-
ilar to the delexicalized parser (McDonald et al.,
2011). These delexicalized rules are more robust
to linguistic idiosyncrasies, and, thus, are more ef-
fective for transfer learning.

7 Conclusion

We presented a new dependency parsing algo-
rithm for Korean by applying transfer learning
from an existing parser for Japanese. Unlike other
transfer learning methods relying on aligned cor-
pora or bilingual lexical resources, we proposed a
feature transfer method utilizing a small number
of hand-crafted grammar rules that exploit syn-
tactic similarities of the source and target lan-
guages. Experimental results confirm that the fea-
tures learned from the Japanese training corpus
were successfully applied for parsing Korean sen-
tences and mitigated the data sparseness problem.
The grammar rules are mostly delexicalized com-
prising only POS tags and a few functional words
(e.g., case markers), and some techniques to re-
duce the syntactic difference between two lan-
guages makes the transfer learning more effective.
This methodology is expected to be applied to any
two languages that have similar syntactic struc-
tures, and it is especially useful when the target
language is a low-resource language.

References
Jinho D. Choi and Martha Palmer. 2011. Statistical

dependency parsing in Korean: From corpus gener-
ation to automatic parsing. In Proceedings of the
Second Workshop on Statistical Parsing of Morpho-
logically Rich Languages, pages 1–11.

DongHyun Choi, Jungyeul Park, and Key-Sun Choi.
2012. Korean treebank transformation for parser
training. In Proceedings of the ACL 2012 Joint
Workshop on Statistical Parsing and Semantic Pro-
cessing of Morphologically Rich Languages, pages
78–88.

Hoojung Chung and Heechang Rim. 2003. A
new probabilistic dependency parsing model for
head-final, free word order languages. IEICE
TRANSACTIONS on Information and Systems, E86-
1(11):2490–2493.

Tagyoung Chung, Matt Post, and Daniel Gildea. 2010.
Factors affecting the accuracy of korean parsing. In
Proceedings of the NAACL HLT 2010 First Work-
shop on Statistical Parsing of Morphologically-Rich
Languages, pages 49–57.

Michael Collins. 1997. Three generative, lexicalised
models for statistical parsing. In Proceedings of the
35th Annual Meeting of the Association for Compu-
tational Linguistics and Eighth Conference of the
European Chapter of the Association for Compu-
tational Linguistics. Association for Computational
Linguistics.

Hal Daumé III. 2007. Frustratingly easy domain adap-
tation. In Proceedings of the 45th Annual Meeting of
the Association of Computational Linguistics, pages
256–263.

Greg Durrett, Adam Pauls, and Dan Klein. 2012. Syn-
tactic transfer using a bilingual lexicon. In Pro-
ceedings of the 2012 Joint Conference on Empirical
Methods in Natural Language Processing and Com-
putational Natural Language Learning, pages 1–11.

EDR. 1996. EDR (Japan Electronic Dictionary Re-
search Institute, Ltd.) electronic dictionary version
1.5 technical guide.

David A. Ferrucci, Eric W. Brown, Jennifer Chu-
Carroll, James Fan, David Gondek, Aditya Kalyan-
pur, Adam Lally, J. William Murdock, Eric Nyberg,
John M. Prager, Nico Schlaefer, and Christopher A.
Welty. 2010. Building Watson: An overview of the
DeepQA project. AI Magazine, 31(3):59–79.

Ryan Georgi, Fei Xia, and William D Lewis. 2012.
Improving dependency parsing with interlinear
glossed text and syntactic projection. In Proceed-
ings of COLING 2012, pages 371–380.

Raymond G Gordon and Barbara F Grimes. 2005. Eth-
nologue: Languages of the world, volume 15. SIL
international Dallas, TX.

11



Chung-hye Han, Na-Rae Han, Eon-Suk Ko, Heejong
Yi, and Martha Palmer. 2002. Penn Korean tree-
bank: Development and evaluation. In Proc. Pacific
Asian Conf. Language and Comp.

Rebecca Hwa, Philip Resnik, and Amy Weinberg.
2005. Breaking the resource bottleneck for multi-
lingual parsing. Technical report, DTIC Document.

Hiroshi Kanayama, Kentaro Torisawa, Yutaka Mitsu-
ishi, and Jun’ichi Tsujii. 2000. A hybrid Japanese
parser with hand-crafted grammar and statistics. In
Proceedings of the 18th International Conference on
Computational Linguistics, pages 411–417.

Roger Kim, Mary Dalrymple, Ronald M Kaplan, and
Tracy Holloway King. 2003a. Porting grammars
between typologically similar languages: Japanese
to korean. In Proceedings of the 17th Pacific Asia
Conference on Language, Information.

Roger Kim, Mary Dalrymple, Ronald M Kaplan,
Tracy Holloway King, Hiroshi Masuichi, and
Tomoko Ohkuma. 2003b. Multilingual grammar
development via grammar porting. In ESSLLI 2003
Workshop on Ideas and Strategies for Multilingual
Grammar Development, pages 49–56.

Dan Klein and Christopher D Manning. 2004. Corpus-
based induction of syntactic structure: Models of de-
pendency and constituency. In Proceedings of the
42nd Annual Meeting on Association for Computa-
tional Linguistics, pages 478–487.

Sandra Kübler, Ryan McDonald, and Joakim Nivre.
2009. Dependency parsing. Synthesis Lectures on
Human Language Technologies, 1(1):1–127.

Cody Kwok, Oren Etzioni, and Daniel S Weld. 2001.
Scaling question answering to the web. ACM Trans-
actions on Information Systems (TOIS), 19(3):242–
262.

Hyeon-Yeong Lee, Yi-Gyu Hwang, and Yong-Seok
Lee. 2007. Parsing of Korean based on CFG using
sentence pattern information. International Journal
of Computer Science and Network Security, 7(7).

Roger Levy and Christopher Manning. 2003. Is it
harder to parse chinese, or the chinese treebank? In
Proceedings of the 41st Annual Meeting on Asso-
ciation for Computational Linguistics, ACL, pages
439–446.

Ryan McDonald, Slav Petrov, and Keith Hall. 2011.
Multi-source transfer of delexicalized dependency
parsers. In Proceedings of the Conference on Em-
pirical Methods in Natural Language Processing,
pages 62–72.

Ryan McDonald, Joakim Nivre, Yvonne Quirmbach-
Brundage, Yoav Goldberg, Dipanjan Das, Kuzman
Ganchev, Keith Hall, Slav Petrov, Hao Zhang, Oscar
Täckström, et al. 2013. Universal dependency an-
notation for multilingual parsing. In Proceedings of
ACL 2013.

Tahira Naseem, Harr Chen, Regina Barzilay, and Mark
Johnson. 2010. Using universal linguistic knowl-
edge to guide grammar induction. In Proceedings of
the 2010 Conference on Empirical Methods in Nat-
ural Language Processing, pages 1234–1244.

Tahira Naseem, Regina Barzilay, and Amir Globerson.
2012. Selective sharing for multilingual dependency
parsing. In Proceedings of the 50th Annual Meet-
ing of the Association for Computational Linguis-
tics: Long Papers-Volume 1, pages 629–637.

Tetsuya Nasukawa and Jeonghee Yi. 2003. Senti-
ment analysis: Capturing favorability using natural
language processing. In Proceedings of the Second
International Conferences on Knowledge Capture,
pages 70–77.

Sinno Jialin Pan and Qiang Yang. 2010. A survey on
transfer learning. Knowledge and Data Engineer-
ing, IEEE Transactions on, 22(10):1345–1359.

Bo Pang, Lillian Lee, and Shivakumar Vaithyanathan.
2002. Thumbs up? Sentiment classification using
machine learning techniques. In Proceedings of the
ACL-02 conference on Empirical methods in nat-
ural language processing (EMNLP), pages 79–86,
Philadelphia, Pennsylvania.

Jungyeul Park, Daisuke Kawahara, Sadao Kurohashi,
and Key-Sun Choi. 2013. Towards fully lexicalized
dependency parsing for Korean. In Proceedings of
The 13th International Conference on Parsing Tech-
nologies.

Yoav Seginer. 2007. Fast unsupervised incremental
parsing. In Proceedings of the 45th Annual Meet-
ing of the Association of Computational Linguistics,
pages 384–391.

David A. Smith and Noah A. Smith. 2004. Bilingual
parsing with factored estimation: Using English to
parse Korean. In Proceedings of the 2004 Confer-
ence on Empirical Methods in Natural Language
Processing, pages 49–56.

Valentin I Spitkovsky, Hiyan Alshawi, Angel X Chang,
and Daniel Jurafsky. 2011. Unsupervised depen-
dency parsing without gold part-of-speech tags. In
Proceedings of the Conference on Empirical Meth-
ods in Natural Language Processing, pages 1281–
1290.

Daniel Zeman and Philip Resnik. 2008. Cross-
language parser adaptation between related lan-
guages. In IJCNLP, pages 35–42.

12


