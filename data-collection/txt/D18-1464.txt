


































A Neural Local Coherence Model for Text Quality Assessment


Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 4328–4339
Brussels, Belgium, October 31 - November 4, 2018. c©2018 Association for Computational Linguistics

4328

A Neural Local Coherence Model for Text Quality Assessment

Mohsen Mesgar ∗
Heidelberg Institute for

Theoretical Studies (HITS) and
Research Training Group AIPHES

Michael Strube
Heidelberg Institute for

Theoretical Studies (HITS)
michael.strube@h-its.org

Abstract

We propose a local coherence model that cap-
tures the flow of what semantically connects
adjacent sentences in a text. We represent
the semantics of a sentence by a vector and
capture its state at each word of the sen-
tence. We model what relates two adjacent
sentences based on the two most similar se-
mantic states, each of which is in one of the
sentences. We encode the perceived coherence
of a text by a vector, which represents patterns
of changes in salient information that relates
adjacent sentences. Our experiments demon-
strate that our approach is beneficial for two
downstream tasks: Readability assessment, in
which our model achieves new state-of-the-art
results; and essay scoring, in which the combi-
nation of our coherence vectors and other task-
dependent features significantly improves the
performance of a strong essay scorer.

1 Introduction

Coherence is a key factor that distinguishes
well-written texts from random collections of sen-
tences. A potential application of coherence mod-
els is text quality assessment. Examples include
readability assessment (Pitler and Nenkova, 2008;
Li and Hovy, 2014) and essay scoring (Miltsakaki
and Kukich, 2004; Burstein et al., 2010). Here,
we address the problem of local coherence model-
ing, which captures text relatedness at the level of
sentence-to-sentence transitions.

Several approaches to local coherence model-
ing have been proposed. Entity-based methods
principally relate adjacent sentences by means of
entities, which are mentioned as noun phrases,
NPs, in sentences (Barzilay and Lapata, 2008; El-
sner and Charniak, 2011; Guinaudeau and Strube,

∗This author is currently employed by the Ubiquitous
Knowledge Processing (UKP) Lab, Technische Universität
Darmstadt, https://www.ukp.tu-darmstadt.de.

2013; Tien Nguyen and Joty, 2017). Lexical mod-
els connect sentences based on semantic relations
between words in sentences (Beigman Klebanov
and Shamir, 2006; Heilman et al., 2007; Mesgar
and Strube, 2016). Both of these approaches suf-
fer from different weaknesses. The entity-based
models require an entity detection system, a coref-
erence model, and a syntactic parser. These
subsystems need to be perfect to gain the best
performance of entity-based coherence models.
The weakness of the lexical models is that they
consider words independently, i.e. regardless of
context in that words appear. More concretely,
such lexical models take sentences as a bag of
words. Recent deep learning coherence work (Li
and Hovy, 2014; Li and Jurafsky, 2017) adopts
recursive and recurrent neural networks for com-
puting semantic vectors for sentences. Coherence
models that use recursive neural networks suffer
from a severe dependence on external resources,
e.g. a syntactic parser to construct their recursion
structure. Coherence models that purely rely on
the recurrent neural networks process words se-
quentially within a text. However, in such models,
long-distance dependencies between words can-
not be captured effectively due to the limits of the
memorization capability of recurrent networks.

Our motivation is to overcome these limitations.
We use the advantages of distributional represen-
tations in order to, first, identify and represent
salient semantic information that connects sen-
tences, and second, extract patterns of changes in
such information as a text progresses. By repre-
senting words of sentences with their pre-trained
embeddings, we take lexical semantic relations be-
tween words into account. We employ a Recur-
rent Neural Network (RNN) layer to combine in-
formation in word embeddings and actual context
information of words in sentences. Our model
encodes salient information that relates two adja-



4329

cent sentences based on the two most similar RNN
states in sentences. We accumulate two identified
RNN states to represent semantic information that
connects two adjacent sentences. We encode pat-
tern of semantic information changes across sen-
tences in a text by a convolutional neural network
to represent coherence. Our end-to-end coherence
model is superior to previous work because it re-
lates sentences based on two semantic information
states in sentences that are highly similar. So it
does not need extra tools such as coreference res-
olution systems. Furthermore, our model incorpo-
rates words in their sentence context and models
(roughly) distant relations between words.

We evaluate our model on two tasks: readabil-
ity assessment and essay scoring. Both have been
frequently used for coherence evaluation (Barzilay
and Lapata, 2008; Miltsakaki and Kukich, 2004).
Readability assessment is a ranking task where we
compare the rankings given by the model against
human judgments. Essay scoring is a regression
task, in which we investigate if the combination
of coherence vectors produced by our model and
other essay scoring features proposed by Phandi
et al. (2015) improves the performance of the es-
say scorer. The experimental results show that
our model achieves the state-of-the-art result for
readability assessment on the examined dataset
(De Clercq and Hoste, 2016); and the combination
of our coherence features with other essay scoring
features significantly improves the performance of
the examined essay scorer (Phandi et al., 2015).

2 Related Work

Early work on coherence captures different
types of relations: entity-based (Grosz et al.,
1995; Barzilay and Lapata, 2008), lexical-based
(Beigman Klebanov and Flor, 2013; Somasun-
daran et al., 2014; Zhang et al., 2015), etc. Among
these models, the entity-grid model (Barzilay and
Lapata, 2005, 2008) has received a lot of attention.
In this model, entities are defined, heuristically, by
applying a string match over head nouns of all NPs
in a text. The model, then, defines all possible
changes over syntactic roles of entities in adjacent
sentences as coherence patterns. The entity-grid
model has been extended both by expanding its en-
tity extraction phase (Elsner and Charniak, 2011;
Feng and Hirst, 2012) and by defining other types
of patterns (Lin et al., 2011; Louis and Nenkova,
2012; Ji and Eisenstein, 2014; Guinaudeau and

Strube, 2013). Recently, Tien Nguyen and Joty
(2017) fed entity grid representations of texts to
a convolutional neural network (CNN) in order
to overcome the limitation of predefined coher-
ence patterns and extract patterns automatically.
However, all of these models limit relations be-
tween sentences to entities that are shared by sen-
tences. This makes the performance of these mod-
els dependent on the performance of other tools
like coreference resolution systems and syntac-
tic parsers. Our coherence model, in contrast, is
based on relations between any embedded seman-
tic information in sentences, and does not require
entity annotations. A similar approach to ours is
proposed by Mesgar and Strube (2016). Their
approach encodes lexical relations between sen-
tences in a text via a graph. Sentences are en-
coded by nodes, and lexical semantic relations be-
tween sentences are represented by edges. Coher-
ence patterns are obtained by applying a subgraph
mining method to graph representations of all texts
in a corpus. This model involves words individ-
ually and independent of their sentence context.
Our model uses a RNN layer over words in sen-
tences to incorporate context information. Our ap-
proach for extracting coherence patterns also dif-
fers from this model as we employ CNNs rather
than graph mining. Li and Hovy (2014) model
sentences as vectors derived from RNNs and train
a feed-forward neural network that takes an input
window of sentence vectors and assigns a proba-
bility which represents the coherence of the sen-
tences in the window. Text coherence is evalu-
ated by sliding the window over sentences and ag-
gregating their coherence probabilities. Similarly,
Li and Jurafsky (2017) study the same model at
a larger scale and use a sequence-to-sequence ap-
proach in which the model is trained to generate
the next sentence given the current sentence and
vice versa. Our approach differs from these meth-
ods; we represent coherence by a vector of co-
herence patterns. Moreover, our model takes dis-
tant relations between words in a text into account
by relating two semantic states of sentences that
are highly similar. Lai and Tetreault (2018) com-
pare the performance of the aforementioned co-
herence models on texts from different domains.
They conclude that the neural coherence models,
which are explained above, surpass examined non-
neural coherence models such as the entity-based
models and the lexical-based model. Unlike their



4330

evaluation method, which predicts the coherence
level of a text, we rank two texts with respect to
their coherence levels for the readability assess-
ment task. We also show that integrating our co-
herence model into an essay scorer improves its
performance.

An important task for evaluating a coherence
model is readability assessment (Li and Hovy,
2014; Petersen et al., 2015; Todirascu et al., 2016).
The more coherent a text, the faster to read and
easier to understand it is. Early readability formu-
las were based on superficial text features such as
average word lengths (Kincaid et al., 1975). These
formulas systematically ignore many important
factors that affect readability such as discourse co-
herence (Barzilay and Lapata, 2008). Schwarm
and Ostendorf (2005) and Feng et al. (2010) re-
cast readability assessment as a ranking task, and
employ different semantic (e.g. language model
perplexity scores) and syntactic (e.g. the average
number of NPs) features to solve this task. Pitler
and Nenkova (2008) show that discourse coher-
ence features are more informative than other fea-
tures for ranking texts with respect to their read-
ability. Following the related work on coherence
modeling (Barzilay and Lapata, 2008; Mesgar and
Strube, 2015), we evaluate our coherence model
on this task.

Another popular task for evaluating coherence
models is essay scoring (Beigman Klebanov and
Flor, 2013; Somasundaran et al., 2014). Milt-
sakaki and Kukich (2004) employ an essay scoring
system to examine whether local coherence fea-
tures, as defined by a measure of Centering The-
ory’s Rough-Shift transitions (Grosz et al., 1995),
might be a significant contributor to the evaluation
of essays. They show that adding such features
to their essay scorer improves its performance sig-
nificantly. Burstein et al. (2010) specifically fo-
cus on the impact of entity transition features, as
proposed by the entity-grid model for coherence
modeling, on the essay scoring task. They demon-
strate that by combining these features with other
features related to grammar errors and word usage,
the performance of their automated essay scoring
system improves. Likewise, we combine our co-
herence vectors with other features that are used
by a strong essay scorer (Phandi et al., 2015) and
show that our coherence vectors improve the per-
formance of this system significantly.

e0 e1 e2

E0

e3 e4 e5

E1

e6 e7 e8

E2

e9 e10 e12

E3

e13 e14 e15

E4

s0 s1 s2 s3 s4

LOOKUP LOOKUP LOOKUP LOOKUP LOOKUP

LSTM LSTM LSTM LSTM LSTM

h00 h
1
0 h

2
0 h

0
1 h

1
1 h

2
1 h

0
2 h

1
2 h

2
2 h

0
3 h

1
3 h

2
3 h

0
4 h

1
4 h

2
4

H0 H1 H2 H3 H4

�f1 �f2 �f3 �f4

d23

CNN

�p

Figure 1: An illustration of our model. ek is word em-
beddings associated with the kth word in an input text.
hji depicts the j

th hidden state in LSTM states of sen-
tence si. Two states in LSTM states of sentence si and
sentence si−1 that have the highest similarity are se-
lected to connect sentences. Vector �fi captures infor-
mation about the salient topic that relates sentence si
to sentence si−1. d23 represents the similarity between
�f2 and �f3 or the degree of continuity of the topic over
adjacent sentences. Different shades of gray show dif-
ferent degrees of similarity. The CNN encodes patterns
of changes as coherence vector �p.

3 Coherence Model

In this section, we describe details of our model.
First, we explain how we encode words in their
context (Section 3.1). Then we show how we re-
late sentences (Section 3.2), and finally we explain
how we represent coherence based on sentence re-
lations (Section 3.3). A general formulation of our
model is a parametric function, �p = Lθ (d), where
d is an input document, θ indicates parameters of
neural modules, and �p is a vector representation
for the coherence of d. Figure 1 illustrates our
model.

3.1 Word and Context Representations

We use a lookup table to associate all words in the
vocabulary with word embeddings. The lookup ta-
ble is initialized by existing pre-trained word em-
beddings because they capture lexical semantic re-



4331

lations between words. For sentence si, the lookup
table returns matrix Ei whose rows are embed-
dings of words in si. A weakness of former lexi-
cal coherence models (Somasundaran et al., 2014;
Mesgar and Strube, 2016) is that they only rely
on semantic relations between words in sentences,
regardless of the current context of words. In or-
der to overcome this limitation, we use a standard
unidirectional1 RNN with Long Short-Term Mem-
ory (LSTM) cells to encode the current context of
words in sentences. For embedding matrix Ei:

Hi = LSTM
�
Ei, h

n−1
i−1

�
,

where Hi is a list of LSTM states, and hn−1i−1 is
the last LSTM state of sentence si−1. Parameter n
is the number of words in a sentence. We take
state vector hji ∈ Hi as a representation of its in-
put word embedding, ej , that is combined with its
preceding word vectors in sentence si. For sake
of brevity, the details of LSTM formulations are ex-
plained in Appendix A.

3.2 Sentence Relation Representations
The relation between sentences is encoded by the
most similar semantic states of sentences. Given
two adjacent sentences, two of their LSTM states
that have the highest similarity are selected to con-
nect them. Those LSTM states refer to the salient
semantic information that is shared between sen-
tences. To model this, we follow attention compo-
nents in neural language models (Bahdanau et al.,
2014; Vaswani et al., 2017) where the similarity
between the last LSTM state and each of its pre-
ceding states is computed to measure the amount
of attention that the model should give to its pre-
ceding context for generating the next word. More
formally, for two adjacent sentences si and si−1,
one LSTM state in Hi and one LSTM state in
Hi−1 that have the maximum similarity are se-
lected to represent the relation between the sen-
tences:

(�u,�v) = argmax
(�hm∈Hi)
(�hn∈Hi−1)

(sim(�hm,�hn)),

where Hi and Hi−1 are LSTM states correspond-
ing to sentences si and si−1. The similarity func-
tion, sim, returns the absolute value of the dot

1We use unidirectional RNN to model the way that an En-
glish text is read.

product between input vectors,

sim(�hm,�hn) = |�hm · �hn|, (1)

where the function |.| computes the absolute value
of its input2. We use the dot product function be-
cause in practice it enables our model to calculate
the above equations efficiently in parallel and in
matrix-space, i.e., directly on Hi and Hi−1. Since
this is the details of implementation, we explain
matrix-based equations in Appendix B. The abso-
lute value in the similarity function is used to en-
code semantic relatedness between associated in-
formation with vectors, which is independent of
the sign of the similarity function (Manning and
Schütze, 1999).

We represent semantic information that relates
two adjacent sentences by accumulating its se-
lected LSTM states in the corresponding sen-
tences. Since averaging in the vector space is
an effective way to accumulate information rep-
resented in some vectors (Iyyer et al., 2015; Wiet-
ing et al., 2016), we compute the average of two
identified vectors among the LSTM states of two
adjacent sentences to represent semantic informa-
tion shared by the sentences. More concretely, the
vector representation of what relates sentence si to
its immediately preceding sentence is obtained by
averaging a vector of Hi and a vector of Hi−1 that
are identified as highly similar:

�fi = avg(�u,�v) =
�u+ �v

2
,

where �u and �v are selected vectors. �fi is the vector
representation of what connects si to its immedi-
ately preceding sentence.

3.3 Coherence Representations
Since sentences in a coherent text are about sim-
ilar topics and share some semantic information,
we compute semantic similarity between adjacent
information states, i.e. �fis, to capture how they are
changing through a text. We propose to encode
changes by a continuous value between 0 and 1,
where 1 shows that there is no change and 0 in-
dicates that there is a big semantic drift in a text.
Any value in between depicts how far a text is se-
mantically changing. Given two adjacent vectors
�fi and �fi+1, the degree of continuity between them
is:

2In practice, the absolute function is implemented as
g(z) = max(0, z)−min(0, z) to be differentiable.



4332

d =
sim(�fi, �fi+1)

l
,

where l is the length of input vectors, which is
used to prevent large numbers (Vaswani et al.,
2017), and sim is the similarity function (Sec-
tion 3.2). The task of this layer is to check if the
salient information that is shared by two adjacent
sentences is salient in the subsequent sentence or
not.

The last layer of our model is a convolutional
layer to automatically extract and represent pat-
terns of semantic changes in a text. CNNs have
proven useful for various NLP tasks (Collobert
et al., 2011; Kim, 2014; Kalchbrenner et al., 2014;
Cheng and Lapata, 2016) because of their effec-
tiveness in identifying patterns in their input (Xu
et al., 2015). In the case of coherence, the con-
volution layer can identify coherence patterns that
correlate with final tasks (Tien Nguyen and Joty,
2017). We use a temporal narrow convolution by
applying a kernel filter k of width h to a window
of h adjacent transitions over sentences to pro-
duce a new coherence feature. This filter is ap-
plied to each possible window of transitions in a
text to produce a feature map �p, which is a coher-
ence vector. Since we use a standard convolution
layer, we explain details of the CNN formulations
in Appendix C.

3.4 Variants of Our Model
In our experiments, we consider two variants of
our model: CohLSTM that is the full version of
our model as described above; and CohEmb that
is an ablation. CohEmb has no RNN layer, so the
model is built directly on word embeddings. In
this model, relations between sentences are made
over only content words by eliminating all stop
words.

4 Implementation Details

Model configurations. Our model is imple-
mented in PyTorch3 with CUDA 8.0 support. In
preprocessing we apply zero-padding to all sen-
tences and documents to make their length equal.
The vocabulary is limited to the 4000 most fre-
quent words in the training data and all other
words are replaced with the unknown token. We
use the pre-trained word embeddings released
by Zou et al. (2013), which are employed by

3https://pytorch.org

state-of-the-art essay scoring systems. The dimen-
sions of word embeddings and LSTM cells are 50
and 300, respectively. The convolution layer uses
one filter with size 4. However, optimizing hyper-
parameters for each task may lead to better perfor-
mance. For selecting two vectors with the highest
similarity from the LSTM states of two adjacent
sentences, we capture the similarity between any
pair of LSTM states of the sentences as an element
in a vector, and then apply a max-pooling layer to
this vector of similarities to identify the pair with
maximally similar LSTM states. Selected LSTM
states are used for representing salient information
shared by the sentences. In CohEmb, stop words
are removed by the SMART English stop word list
(Salton, 1971).

Training setup. We set the mini-batch size to
32 and train the network for 100 epochs. At each
epoch we evaluate the model on the validation set
and select the one with the best performance for
test evaluations. We optimize with Adam, with an
initial learning rate of 0.01. Word vectors are up-
dated during training. The dropout method with
rate 0.5 is employed for regularization. Loss func-
tions are specifically defined for each task.

5 Experiments

We evaluate our model on two downstream tasks:
readability assessment (Section 5.1), in which co-
herence representations of documents are mapped
to coherence scores, and then documents are
ranked based on these scores; and essay scoring
(Section 5.2), in which the coherence representa-
tion of an essay is combined with other features for
essay scoring to quantify the quality of the essay.

5.1 Readability Assessment

Readability assessment – How difficult is a text to
read and understand? – depends on many factors
one of which is coherence. Texts that are more co-
herent are supposed to be faster to read and easier
to understand. Following earlier research on local
coherence (Barzilay and Lapata, 2008; Pitler and
Nenkova, 2008; Guinaudeau and Strube, 2013),
we evaluate our coherence model on this task by
ranking texts with respect to readability, instead of
predicting readability scores. More formally, we
approach readability assessment as follows: Given
a text-pair, which text is easier to read?



4333

Compared models. We compare the two vari-
ants of our model as described in Section 3.4 with
two following state-of-the-art systems:

Mesgar and Strube (2016). This is a
graph-based coherence model, in which nodes of
a graph indicate sentences of a text, and an edge
between two sentence nodes represents the exis-
tence of a lexico-semantic relation between two
words in the sentences. Semantic relations be-
tween words are measured by the absolute value
of the cosine function over their corresponding
pre-trained word embeddings. If the similarity
value for two word vectors is below a certain
threshold4 then the connection between these two
words is omitted. Given the graph representa-
tion of a text, its coherence is encoded as a vec-
tor whose elements are frequencies of different
subgraphs in the graph. The size of subgraphs
is defined by the number of their nodes and is
set to five. Subgraphs are extracted by a ran-
dom sampling approach. We choose this model
for comparison because its intuition is similar to
our model. However, this model suffers from the
following limitations: word embeddings are con-
sidered independently, not in their current context;
and a manual threshold is used for connection fil-
tering. We overcome these two weaknesses using
the RNN and CNN layers in our model, respec-
tively.

De Clercq and Hoste (2016). This is the
state-of-the-art readability system on the exam-
ined dataset. It uses a rich set of readability fea-
tures ranging from surface to semantic text fea-
tures. The ranking is performed by LibSVM in
their model. We report their best performance that
is achieved by extensive feature engineering and
SVM’s parameter optimization.

Experimental setup. In Section 3, we formu-
lated our model as �p = Lθ (d) where θ repre-
sents parameters of the neural modules (i.e. the
CNN and RNN layers) in our model. For this task,
we use an output layer to map coherence vector �p
to score s which quantifies the degree of the per-
ceived coherence of document d. Formally, the
output layer is sd = �u · �p + b where �u and b are
the weight vector and bias, respectively. Let doc-
ument d be more readable than document d�, then
the model should ideally produce sd > s�d. We
train the parameters of the model by a pairwise

4Like Mesgar and Strube (2016), we set this threshold to
0.9.

ranking approach and define the loss function as:

loss = max {0, 1− sd + sd�} .
The parameters of the model are shared to obtain
the scores for texts in pair (d, d�).

Data. We use the readability dataset proposed
by De Clercq et al. (2014). It consists of 105
texts collected from the British National Corpus
and Wikipedia in four different genres: adminis-
trative (e.g. reports and surveys), informative (e.g.
articles of newspapers and Wikipedia entries), in-
structive (e.g. user manuals and guidelines), and
miscellaneous (e.g., very technical texts and chil-
dren’s literature). The average number of sen-
tences is about 12 per text. 10, 907 pairs of
texts are labeled with five fine-grained categories:
{−100,−50, 0, 50, 100} indicating that the first
text in a pair is respectively much easier, some-
what easier, equally difficult, somewhat difficult,
more difficult to read than the second text in the
pair. Labels of text-pairs are assigned by human
judges. Similar to De Clercq and Hoste (2016),
we evaluate on the positive and negative labels as
two sets of classes resulting in 6, 290 text-pairs in
total. The original readability dataset does not pro-
vide any standard training/validation/test sets. We
apply 5-fold cross-validation over this dataset.

Evaluation metric. The quality of a model is
measured in terms of accuracy, which is the frac-
tion of pairs that are correctly ranked by a model
divided by the total number of document-pairs.
We report the average accuracy over all runs of
cross-validation as the final result. We perform a
paired t-test to determine if improvements are sta-
tistically significant (p < .05).

Results. Table 1 summarizes the results of dif-
ferent systems for the readability assessment task.

Model Accuracy (%)
Mesgar and Strube (2016) 85.70∗

CohEmb 92.17∗

CohLSTM 97.77∗

De Clercq and Hoste (2016) 96.88∗

Table 1: Results on readability assessment. The first
system is the state-of-the-art coherence model on this
dataset. The last one is a full readability system. “∗” in-
dicates statistically significant difference with the bold
result.

CohEmb significantly outperforms the
graph-based coherence model proposed by



4334

Mesgar and Strube (2016) by a large margin
(6%), showing that our model captures coherence
better than their model. In our model, the CNN
layer automatically learns which connections are
important to be considered for coherence patterns,
whereas this is performed in Mesgar and Strube
(2016) by defining a threshold for eliminating
connections.

CohLSTM significantly outperforms both the
coherence model proposed by Mesgar and Strube
(2016) and the CohEmb model by 11% and 5%,
respectively, and defines a new state-of-the-art on
this dataset. CohLSTM, unlike Mesgar and Strube
(2016)’s model and CohEmb, considers words of
sentences in their sentence context. This sup-
ports our intuition that actual context information
of words contributes to the perceived coherence of
texts.

CohLSTM, which captures exclusively local
coherence, even outperforms the readability sys-
tem proposed by De Clercq and Hoste (2016),
which relies on a wide range of lexical, syntactic
and semantic features.

5.2 Essay Scoring
One part of the student assessment process is es-
say writing where students are asked to write an
essay about a given topic known as a prompt. An
essay scoring system assigns an essay a score re-
flecting the quality of the essay. The quality of
an essay depends on various factors including co-
herence. Following previous studies (Miltsakaki
and Kukich, 2004; Lei et al., 2014; Somasundaran
et al., 2014; Zesch et al., 2015), we approach this
task by combining the coherence vector produced
by our model and the feature vector developed by
an open-source essay scorer to represent an essay
by a vector. The final vector representation of an
essay, �x, is mapped to a score by a simple neural
regression method as follows:

s = sigmoid(�u · �x+ b),

where �u and b are the weight vector and the bias,
respectively. We exactly define vector �x for dif-
ferent examined systems, where we explain com-
pared models for essay scoring.

Compared models. We compare variations of
our model (Section 3.4) with the following mod-
els:

EASE (BLRR). As a baseline we use an
open-source essay scoring system, Enhanced AI

Scoring Engine5 (EASE) (Phandi et al., 2015).
This system was ranked third among all 154 par-
ticipating teams in the Automated Student As-
sessment Prize (ASAP) competition and is the
best among all open-source participating systems.
It employs Bayesian Linear Ridge Regression
(BLRR) as its regression method applied to a set
of linguistic features grouped in four categories:
(i) Frequency-based features: such as the number
of characters, the number of words, the number of
commas, etc; (ii) POS-based features: the number
of POS n-grams; (iii) Word overlap with prompt;
(iv) Bag of n-grams: the number of uni-grams and
bi-grams.

EASE. The difference between this system
and EASE (BLRR) is in the employed regression
method. This system uses a neural regression
method as described above. In order to have a
similar experimental settings for this task, here,
we use feature vectors generated by Phandi et al.
(2015) to train our neural regression system. The
input of the neural regression function is a nonlin-
ear transformation of the feature vector produced
by EASE, �f . Therefore �x = tanh(�w · �f + b).

EASE & CohEmb. This model combines the
feature vector computed by EASE, �f , and the co-
herence vector produced by CohEmb, �p, to have
a more reliable representation of an essay. More
concretely, the input to our regression function, �x,
is obtained as follows:

�h1 = tanh(�w1 · �f + b1),
�h2 = �h1 ⊕ �p,
�x = tanh(�w2 · �h2 + b2),

where ⊕ indicates the concatenation operatation.
EASE & CohLSTM. The structure of this

model is the same as the EASE & CohEmb struc-
ture. But the input coherence vector, �p, is pro-
duced by CohLSTM.

Dong et al. (2017). It is a sentence-document
model, which is especially designed for this task.
It first encodes each sentence by a vector, which
represents whole sentence meanings, and then use
an RNN to embed vectors of sentences into a doc-
ument vector.

Experimental setup. The size of the input vec-
tor for the regression method, �x, is fixed to 100 and
its output size is fixed to 1. Dimensions of other
parameters, �w1 and �w2, are set accordingly. The

5https://github.com/edx/ease



4335

loss function is the Mean Squared Error (MSE)
between human scores, �H , and scores predicted
by our system, �S:

MSE( �H, �S) =
1

N

�
( �H − �S)2.

The models are compared for each prompt by run-
ning 5-fold cross-validation (Dong et al., 2017).

Data. We apply our model to a dataset used in
the Automated Student Assessment Prize (ASAP)
competition run by Kaggle6. The essays are as-
sociated with scores given by humans and catego-
rized in eight prompts. Each prompt can be inter-
preted as a different essay topic along with differ-
ent genres. Table 2 summarizes some properties
of this dataset.

Prompt # Essays Genre Avg. Len.
1 1783 argumentative 350
2 1800 argumentative 350
3 1726 response 150
4 1772 response 150
5 1805 response 150
6 1800 response 150
7 1569 narrative 250
8 723 narrative 650

Table 2: Some properties of the dataset used for the
essay scoring experiment.

Evaluation metric. ASAP adopted Quadratic
Weighted Kappa (QWK) as the official evaluation
metric. This metric measures the agreement be-
tween scores predicted by a system and scores as-
signed by humans. QWK considers chance agree-
ments and penalizes large disagreements more
than small agreements. We use an implementation
of QWK that is described in Taghipour and Ng
(2016). The formulation of QWK are explained
in Appendix D. The final reported QWK is the av-
erage over QWKs of all prompts. We perform a
paired t-test to determine if improvements are sta-
tistically significant (p < .05).

Results. Table 3 shows the results of different
systems for the essay scoring task.

Both EASE & CohEmb, and
EASE & CohLSTM significantly improve EASE,
confirming that our proposed representation for
coherence is beneficial for essay scoring and

6https://www.kaggle.com/c/asap-aes/
data

improves the performance of the examined essay
scoring system. Our model does not beat the
state-of-the-art essay scoring system (Dong et al.,
2017), which is especially designed for this task
and is tuned on this dataset. This model learns a
vector representation for an input essay so that the
vector performs the best for this regression task.
In contrast, the core of our best performing essay
scoring system, i.e. EASE & CohLSTM, is the
feature vector generated by EASE, which has less
modeling capacity than a deep learning model
like the model proposed by Dong et al. (2017).
The reason that we combine our coherence model
with EASE, rather than the model proposed by
Dong et al. (2017), is that EASE has no notion of
coherence. By combining our coherence model
with it, we examine if our coherence vector
improves its performance or not.

Surprisingly, EASE & CohLSTM works on par
with EASE & CohEmb. To gain a better in-
sight, we ablate EASE feature vectors and com-
pare the performance of the coherence models,
i.e., CohLSTM, and CohEmb. Of course, coher-
ence vectors on their own are not sufficient for
predicting essay scores but this setup shows how
much each variant of our model contributes to this
task. The two last rows in Table 3 show the results.

CohLSTM outperforms CohEmb on all
prompts, which matches the results for readability
assessment. This confirms our intuition that
integrating the information of the current context
of words contributes to coherence measurement.

In terms of average QWK, CohLSTM works
similar to EASE; however they behave differently
on different prompts. The largest improvement for
CohLSTM, with respect to EASE, is obtained on
prompt 7 and 8. These two prompts ask for stories
about laughter and patience, so corresponding es-
says can be categorized in the narrative genre (see
Table 2). The guidelines of these two prompts,
which are publicly available in the Kaggle data,
ask human annotators to assign the highest score
to essays that are coherent and hold the attention of
readers through an essay. This is what our model
captures: the sequence of semantic changes in a
text, or coherence.

On prompt 5, in contrast, we see the largest de-
terioration in performance of CohLSTM in com-
parison to EASE. This prompt asks students to de-
scribe the mood created by the author of a mem-
oir. Essays are expected to contain specific infor-



4336

Model Prompts Avg QWK1 2 3 4 5 6 7 8
EASE (BLRR) 0.761 0.606 0.621 0.742 0.784 0.775 0.730 0.617 0.705∗
Dong et al. (2017) 0.822 0.682 0.672 0.814 0.803 0.811 0.801 0.705 0.764∗
EASE 0.702 0.572 0.620 0.731 0.752 0.758 0.648 0.530 0.664∗
EASE & CohEmb 0.783 0.646 0.664 0.776 0.777 0.776 0.744 0.632 0.725∗
EASE & CohLSTM 0.784 0.654 0.663 0.788 0.793 0.794 0.756 0.646 0.728∗
CohEmb 0.625 0.523 0.501 0.570 0.581 0.578 0.661 0.472 0.564∗
CohLSTM 0.669 0.634 0.591 0.710 0.639 0.716 0.729 0.641 0.666∗

Table 3: Results on essay scoring. “∗” shows significant improvements with respect to the underlined score
(p < .05). Bold numbers show the best results among different variants of our model.

mation from the memoir so that an essay with the
highest score has the highest coverage of all rel-
evant and specific information from the memoir.
Therefore, mentioning the details of the memoir
in essays of prompt 5 is more important than co-
herence for this prompt. This also shows that our
model exclusively captures the coherence of a text,
which is the goal of this paper.

6 Conclusions

We developed a local coherence model that en-
codes patterns of changes in what semantically
relates adjacent sentences. The main novelty of
our approach is that it defines sentence connec-
tions based on any semantic concept in sentences.
In this sense, our model goes beyond entity-based
coherence models, which need extra dependencies
such as coreference resolution systems. Moreover,
in contrast to lexical cohesion models, which take
words individually, our model encodes words in
their sentence context. Our model relates sen-
tences by means of distant relations between word
representations. The most similar LSTM states in
two adjacent sentences are selected to encode the
salient semantic concept that relates the sentences.
The model finally employs a convolutional layer
to extract and represent patterns of topic changes
across sentences in a text as a coherence vector.

We evaluate coherence vectors generated by our
model on the readability assessment and essay
scoring tasks. On the former, our model achieves
new state-of-the-art results. On the latter, it signif-
icantly improves the performance of a strong es-
say scorer. We believe the reason that our system
works is that it learns which semantic concepts of
sentences should be used to relate sentences, and
which information about concepts is required to
model sentence-to-sentence transitions. In future

work we intend to run qualitative experiments on
patterns that are extracted by our model to see if
they are also linguistically interpretable.

Acknowledgments

This work has been supported by the German Re-
search Foundation (DFG) as part of the Research
Training Group Adaptive Preparation of Informa-
tion from Heterogeneous Sources (AIPHES) un-
der grant No. GRK 1994/1 and the Klaus Tschira
Foundation, Heidelberg, Germany. We thank
Mohammad Taher Pilehvar, Ines Rehbein, and
Mark-Christoph Müller for their valuable feed-
back on earlier drafts of this paper. We also thank
anonymous reviewers for their useful suggestions
for improving the quality of the paper.

References
Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Ben-

gio. 2014. Neural machine translation by jointly
learning to align and translate. arXiv preprint
arXiv:1409.0473.

Regina Barzilay and Mirella Lapata. 2005. Modeling
local coherence: An entity-based approach. In Pro-
ceedings of the 43rd Annual Meeting of the Asso-
ciation for Computational Linguistics, Ann Arbor,
Mich., 25–30 June 2005, pages 141–148.

Regina Barzilay and Mirella Lapata. 2008. Modeling
local coherence: An entity-based approach. Compu-
tational Linguistics, 34(1):1–34.

Beata Beigman Klebanov and Michael Flor. 2013.
Word association profiles and their use for auto-
mated scoring of essays. In Proceedings of the 51st
Annual Meeting of the Association for Computa-
tional Linguistics (Volume 1: Long Papers), Sofia,
Bulgaria, 4–9 August 2013, pages 1148–1158.

Beata Beigman Klebanov and Eli Shamir. 2006.
Reader-based exploration of lexical cohesion. Lan-
guage Resources and Evaluation, 40(2):109–126.



4337

Jill Burstein, Joel Tetreault, and Slava Andreyev. 2010.
Using entity-based features to model coherence in
student essays. In Proceedings of Human Language
Technologies 2010: The Conference of the North
American Chapter of the Association for Compu-
tational Linguistics, Los Angeles, Cal., 2–4 June
2010, pages 681–684.

Jianpeng Cheng and Mirella Lapata. 2016. Neural
summarization by extracting sentences and words.
In Proceedings of the 54th Annual Meeting of the As-
sociation for Computational Linguistics (Volume 1:
Long Papers), Berlin, Germany, 7–12 August 2016,
pages 484–494.

Ronan Collobert, Jason Weston, Léon Bottou, Michael
Karlen, Koray Kavukcuoglu, and Pavel Kuksa.
2011. Natural language processing (almost) from
scratch. Journal of Machine Learning Research,
12(8):2493–2537.

Orphée De Clercq and Véronique Hoste. 2016. All
mixed up? Finding the optimal feature set for
general readability prediction and its application
to English and Dutch. Computational Linguistics,
42(3):457–490.

Orphée De Clercq, Véronique Hoste, Bart Desmet,
Philip Van Oosten, Martine De Cock, and Lieve
Macken. 2014. Using the crowd for readability pre-
diction. Natural Language Engineering, 20(3):293–
325.

Fei Dong, Yue Zhang, and Jie Yang. 2017.
Attention-based recurrent convolutional neural
network for automatic essay scoring. In Proceed-
ings of the 21st Conference on Computational
Natural Language Learning, Vancouver, Canada,
3–4 August 2017, pages 153–162.

Micha Elsner and Eugene Charniak. 2011. Extending
the entity grid with entity-specific features. In Pro-
ceedings of the 49th Annual Meeting of the Associa-
tion for Computational Linguistics (Volume 2: Short
Papers), Portland, Oreg., 19–24 June 2011, pages
125–129.

Lijun Feng, Martin Jansche, Matt Huenerfauth, and
Noémie Elhadad. 2010. A comparison of features
for automatic readability assessment. In Proceed-
ings of Coling 2010: Poster Volume, Beijing, China,
23–27 August 2010, pages 276–284.

Vanessa Wei Feng and Graeme Hirst. 2012. Extend-
ing the entity-based coherence model with multiple
ranks. In Proceedings of the 13th Conference of the
European Chapter of the Association for Compu-
tational Linguistics, Avignon, France, 23–27 April
2012, pages 315–324.

Barbara J. Grosz, Aravind K. Joshi, and Scott Wein-
stein. 1995. Centering: A framework for model-
ing the local coherence of discourse. Computational
Linguistics, 21(2):203–225.

Camille Guinaudeau and Michael Strube. 2013.
Graph-based local coherence modeling. In Proceed-
ings of the 51st Annual Meeting of the Association
for Computational Linguistics (Volume 1: Long Pa-
pers), Sofia, Bulgaria, 4–9 August 2013, pages 93–
103.

Michael J. Heilman, Kevyn Collins-Thompson, Jamie
Callan, and Maxine Eskenazi. 2007. Combining
lexical and grammatical features to improve read-
ability measures for first and second language texts.
In Proceedings of Human Language Technologies
2007: The Conference of the North American Chap-
ter of the Association for Computational Linguistics,
Rochester, N.Y., 22–27 April 2007, pages 460–467.

Mohit Iyyer, Varun Manjunatha, Jordan Boyd-Graber,
and Hal Daumé III. 2015. Deep unordered compo-
sition rivals syntactic methods for text classification.
In Proceedings of the 53rd Annual Meeting of the
Association for Computational Linguistics (Volume
1: Long Papers), Beijing, China, 26–31 July 2015,
pages 1681–1691.

Yangfeng Ji and Jacob Eisenstein. 2014. Represen-
tation learning for text-level discourse parsing. In
Proceedings of the 52nd Annual Meeting of the As-
sociation for Computational Linguistics (Volume 1:
Long Papers), Baltimore, Md., 22–27 June 2014,
pages 13–24.

Nal Kalchbrenner, Edward Grefenstette, and Phil Blun-
som. 2014. A convolutional neural network for
modelling sentences. In Proceedings of the 52nd
Annual Meeting of the Association for Computa-
tional Linguistics (Volume 1: Long Papers), Balti-
more, Md., 22–27 June 2014, pages 655–665.

Yoon Kim. 2014. Convolutional neural networks
for sentence classification. In Proceedings of the
2014 Conference on Empirical Methods in Natural
Language Processing, Doha, Qatar, 25–29 October
2014, pages 1746–1751.

J. Peter Kincaid, Robert P. Jr. Fishburne, Richard L.
Rogers, and Brad S. Chisson. 1975. Derivation of
new readability formulas (automated readability in-
dex, Fog count and Flesch reading ease formula)
for navy enlisted personnel. Technical Report 8-75,
Naval Technical Training Command, Naval Air Sta-
tion Memphis-Millington, Tenn.

Alice Lai and Joel Tetreault. 2018. Discourse coher-
ence in the wild: A dataset, evaluation and methods.
In Proceedings of the SIGdial 2018 Conference: The
19th Annual Meeting of the Special Interest Group
on Discourse and Dialogue, Melbourne, Australia,
12–14 July 2018, pages 214–223.

Chi-Un Lei, Ka Lok Man, and To Ting. 2014. Using
learning analytics to analyze writing skills of stu-
dents: A case study in a technological common core
curriculum course. IAENG International Journal of
Computer Science, 41(3):41–45.



4338

Jiwei Li and Eduard Hovy. 2014. A model of coher-
ence based on distributed sentence representation.
In Proceedings of the 2014 Conference on Empiri-
cal Methods in Natural Language Processing, Doha,
Qatar, 25–29 October 2014, pages 2039–2048.

Jiwei Li and Dan Jurafsky. 2017. Neural net models
of open-domain discourse coherence. In Proceed-
ings of the 2017 Conference on Empirical Methods
in Natural Language Processing, Copenhagen, Den-
mark, 7–11 September 2017, pages 198–209.

Ziheng Lin, Hwee Tou Ng, and Min-Yen Kan. 2011.
Automatically evaluating text coherence using dis-
course relations. In Proceedings of the 49th Annual
Meeting of the Association for Computational Lin-
guistics (Volume 1: Long Papers), Portland, Oreg.,
19–24 June 2011, pages 997–1006.

Annie Louis and Ani Nenkova. 2012. A coherence
model based on syntactic patterns. In Proceed-
ings of the 2012 Conference on Empirical Meth-
ods in Natural Language Processing and Natural
Language Learning, Jeju Island, Korea, 12–14 July
2012, pages 1157–1168.

Christopher D. Manning and Hinrich Schütze. 1999.
Foundations of Statistical Natural Language Pro-
cessing. MIT Press, Cambridge, Mass.

Mohsen Mesgar and Michael Strube. 2015.
Graph-based coherence modeling for assessing
readability. In Proceedings of STARSEM 2015: The
Fourth Joint Conference on Lexical and Compu-
tational Semantics, Denver, Col., 4–5 June 2015,
pages 309–318.

Mohsen Mesgar and Michael Strube. 2016. Lexical
coherence graph modeling using word embeddings.
In Proceedings of the 2016 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics: Human Language Technologies,
San Diego, Cal., 12–17 June 2016, pages 1414–
1423.

Eleni Miltsakaki and Karen Kukich. 2004. Evaluation
of text coherence for electronic essay scoring sys-
tems. Natural Language Engineering, 10(1):25–55.

Casper Petersen, Christina Lioma, Jakob Grue Simon-
sen, and Birger Larsen. 2015. Entropy and graph
based modelling of document coherence using dis-
course entities: An application to IR. In Proceed-
ings of the ACM SIGIR International Conference on
the Theory of Information Retrieval, Northhampton,
Mass., 27-30 September 2015, pages 191–200.

Peter Phandi, Kian Ming A. Chai, and Hwee Tou Ng.
2015. Flexible domain adaptation for automated es-
say scoring using correlated linear regression. In
Proceedings of the 2015 Conference on Empirical
Methods in Natural Language Processing, Lisbon,
Portugal, 17–21 September 2015, pages 431–439.

Emily Pitler and Ani Nenkova. 2008. Revisiting read-
ability: A unified framework for predicting text
quality. In Proceedings of the 2008 Conference on
Empirical Methods in Natural Language Process-
ing, Honolulu, Hawaii, 25–27 October 2008, pages
186–195.

Gerard Salton. 1971. The SMART Retrieval System
– Experiments in Automatic Document Processing.
Englewood Cliffs, N.J.: Prentice Hall.

Sarah E. Schwarm and Mari Ostendorf. 2005. Reading
level assessment using support vector machines and
statistical language models. In Proceedings of the
43rd Annual Meeting of the Association for Compu-
tational Linguistics, Ann Arbor, Mich., 25–30 June
2005, pages 523–530.

Swapna Somasundaran, Jill Burstein, and Martin
Chodorow. 2014. Lexical chaining for measur-
ing discourse coherence quality in test-taker essays.
In Proceedings of COLING 2014, the 25th Inter-
national Conference on Computational Linguistics:
Technical Papers, Dublin, Ireland, 23–29 August
2014, pages 950–961.

Kaveh Taghipour and Hwee Tou Ng. 2016. A neural
approach to automated essay scoring. In Proceed-
ings of the 2016 Conference on Empirical Methods
in Natural Language Processing, Austin, Tex., 1–5
November 2016, pages 1882–1891.

Dat Tien Nguyen and Shafiq Joty. 2017. A neural local
coherence model. In Proceedings of the 55th An-
nual Meeting of the Association for Computational
Linguistics (Volume 1: Long Papers), Vancouver,
Canada, 30 July – 4 August, 2017, pages 1320–
1330.

Amalia Todirascu, Thomas Francois, Delphine Bern-
hard, Nuria Gala, and Anne-Laure Ligozat. 2016.
Are cohesive features relevant for text readability
evaluation? In Proceedings of COLING 2016,
the 26th International Conference on Computational
Linguistics: Technical Papers, Osaka, Japan, 11–17
December 2016, pages 987–997.

Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob
Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz
Kaiser, and Illia Polosukhin. 2017. Attention is all
you need. In 31st Conference on Neural Informa-
tion Processing Systems (NIPS 2017), Long Beach,
CA., USA, 4–9 December 2017, pages 6000–6010.

John Wieting, Mohit Bansal, Kevin Gimpel, and Karen
Livescu. 2016. Charagram: Embedding words and
sentences via character n-grams. In Proceedings of
the 2016 Conference on Empirical Methods in Natu-
ral Language Processing, Austin, Tex., 1–5 Novem-
ber 2016, pages 1504–1515.

Kelvin Xu, Jimmy Ba, Ryan Kiros, Kyunghyun Cho,
Aaron Courville, Ruslan Salakhudinov, Rich Zemel,
and Yoshua Bengio. 2015. Show, attend and tell:



4339

Neural image caption generation with visual atten-
tion. In Proceedings of the 32nd International Con-
ference on Machine Learning (Volume 37) , Lille,
France, 07–09 July 2015, pages 2048–2057.

Torsten Zesch, Michael Wojatzki, and Dirk
Scholten-Akoun. 2015. Task-independent fea-
tures for automated essay grading. In Proceedings
of the 10th Workshop on Innovative Use of NLP for
Building Educational Applications, Denver, Col., 4
June 2015, pages 224–232.

Muyu Zhang, Vanessa Wei Feng, Bing Qin, Graeme
Hirst, Ting Liu, and Jingwen Huang. 2015. Encod-
ing world knowledge in the evaluation of local co-
herence. In Proceedings of the 2015 Conference of
the North American Chapter of the Association for
Computational Linguistics: Human Language Tech-
nologies, Denver, Col., 31 May – 5 June 2015, pages
1087–1096.

Will Y. Zou, Richard Socher, Daniel Cer, and Christo-
pher D. Manning. 2013. Bilingual word embeddings
for phrase-based machine translation. In Proceed-
ings of the 2013 Conference on Empirical Meth-
ods in Natural Language Processing, Seattle, Wash.,
18–21 October 2013, pages 1393–1398.


