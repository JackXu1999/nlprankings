



















































Vis-Eval Metric Viewer: A Visualisation Tool for Inspecting and Evaluating Metric Scores of Machine Translation Output


Proceedings of NAACL-HLT 2018: Demonstrations, pages 71–75
New Orleans, Louisiana, June 2 - 4, 2018. c©2018 Association for Computational Linguistics

Vis-Eval Metric Viewer: A Visualisation Tool for Inspecting and
Evaluating Metric Scores of Machine Translation Output

David Steele and Lucia Specia
Department of Computer Science

University of Sheffield
Sheffield, UK

dbsteele1,l.specia@sheffield.ac.uk

Abstract

Machine Translation systems are usually eval-
uated and compared using automated evalua-
tion metrics such as BLEU and METEOR to
score the generated translations against human
translations. However, the interaction with the
output from the metrics is relatively limited
and results are commonly a single score along
with a few additional statistics. Whilst this
may be enough for system comparison it does
not provide much useful feedback or a means
for inspecting translations and their respective
scores. Vis-Eval Metric Viewer (VEMV) is a
tool designed to provide visualisation of mul-
tiple evaluation scores so they can be easily in-
terpreted by a user. VEMV takes in the source,
reference, and hypothesis files as parameters,
and scores the hypotheses using several popu-
lar evaluation metrics simultaneously. Scores
are produced at both the sentence and dataset
level and results are written locally to a se-
ries of HTML files that can be viewed on a
web browser. The individual scored sentences
can easily be inspected using powerful search
and selection functions and results can be vi-
sualised with graphical representations of the
scores and distributions.

1 Introduction

Automatic evaluation of Machine Translation
(MT) hypotheses is key for system development
and comparison. Even though human assessment
ultimately provides more reliable and insight-
ful information, automatic evaluation is faster,
cheaper, and often considered more consistent.

Many metrics have been proposed for MT that
compare system translations against human refer-
ences, with the most popular being BLEU (Pa-
pineni et al., 2002), METEOR (Denkowski and
Lavie, 2014), TER (Snover et al., 2006), and, more
recently, BEER (Stanojevic and Sima’an, 2014).
These and other automatic metrics are often criti-

cised for providing scores that can be non-intuitive
and uninformative, especially at the sentence level
(Zhang et al., 2004; Song et al., 2013; Babych,
2014). Additionally, scores across different met-
rics can be inconsistent with each other. This in-
consistency can be an indicator of linguistic prop-
erties of the translations which should be further
analysed. However, multiple metrics are not al-
ways used and any discrepancies among them tend
to be ignored.

Vis-Eval Metric Viewer (VEMV) was devel-
oped as a tool bearing in mind the aforementioned
issues. It enables rapid evaluation of MT output,
currently employing up to eight popular metrics.
Results can be easily inspected (using a typical
web browser) especially at the segment level, with
each sentence (source, reference, and hypothesis)
clearly presented in interactive score tables, along
with informative statistical graphs. No server or
internet connection is required. Only readily avail-
able packages or libraries are used locally.

Ultimately VEMV is an accessible utility that
can be run quickly and easily on all the main plat-
forms.

Before describing the technical specification of
the VEMV tool and its features in Section 3, we
give an overview of existing metric visualisation
tools in Section 2.

2 Related Tools

Several tools have been developed to visualise the
output of MT evaluation metrics that go beyond
displaying just single scores and/or a few statistics.

Despite its criticisms and limitations, BLEU is
still regarded as the de facto evaluation metric used
for rating and comparing MT systems. It was one
of the earliest metrics to assert a high enough cor-
relation with human judgments.

Interactive BLEU (iBleu) (Madnani, 2011) is

71



a visual and interactive scoring environment that
uses BLEU. Users select the source, reference, and
hypothesis files using a graphical user interface
(GUI) and these are scored. The dataset BLEU
score is shown alongside a bar chart of sentence
scores. Users can select one of the sentences by
clicking on the individual bars in the chart. When
a sentence is selected its source and hypothesis
translation is also shown, along with the standard
BLEU statistics (e.g. score and n-gram informa-
tion for the segment). Whilst iBLEU does provide
some interactivity, using the graph itself to choose
the sentences is not very intuitive. In addition the
tool provides results for only one metric.

METEOR is another popular metric used to
compute sentence and dataset-level scores based
on reference and hypothesis files. One of its
main components is to word-align the words in
the reference and hypothesis. The Meteor-X-Ray
tool generates graphical output with visualisation
of word alignments and scores. The alignments
and score distributions are used to generate simple
graphs (output to PDF). Whilst the graphs do pro-
vide extra information there is little in the way of
interactivity.

MT-ComparEval (Klejch et al., 2015) is a dif-
ferent evaluation visualisation tool, available to
be used online1 or downloaded locally. Its pri-
mary function is to enable users, via a GUI, to
compare two (or more) MT system outputs, us-
ing BLEU as the evaluation metric. It shows re-
sults at both the sentence and dataset level high-
lighting confirmed, improving, and worsening n-
grams for each MT system with respect to the
other. Sentence-level metrics (also n-gram) in-
clude precision, recall, and F-Measure informa-
tion as well as score differences between MT sys-
tems for a given sentence. Users can upload their
own datasets to view sentence-level and dataset
scores, albeit with a very limited choice of met-
rics. The GUI provides some interaction with the
evaluation results and users can make a number of
preference selections via check boxes.

The Asiya Toolkit (Giménez et al., 2010) is a
visualisation tool that can be used online or as a
stand-alone tool. It offers a comprehensive suite of
metrics, including many linguistically motivated
ones. Unless the goal is to run a large number of
metrics, the download version is not very practi-
cal. It relies on many external tools such as syn-

1http://wmt.ufal.cz/

tactic and semantic parsers. The online tool2 aims
to offer a more practical solution, where users can
upload their translations. The tool offers a mod-
ule for sentence-level inspection through interac-
tive tables. Some basic dataset-level graphs are
also displayed and can be used to compare system
scores.

In comparison to the other software described
here, VEMV is a light yet powerful utility, which
offers a wide enough range of metrics and can
be easily extended to add other metrics. It has
a very specific purpose in that it is designed for
rapid and simple use locally, without the need for
servers, access to the internet, uploads, or large
installs. Users can quickly get evaluation scores
from a number of mainstream metrics and view
them immediately in easily navigable interactive
score tables. We contend that currently there is no
other similar tool that is lightweight and offers this
functionality and simplicity.

3 Vis-Eval Metric Viewer Software &
Features

This section provides an overview of the VEMV
software and outlines the required input param-
eters, technical specifications, and highlights a
number of the useful features.

3.1 The Software

VEMV is essentially a multi-metric evaluation
tool that uses three tokenised text files (source, ref-
erence, and hypothesis) as input parameters and
scores the hypothesis translation (MT system out-
put) using up to eight popular metrics: BLEU,
MT-Eval3 (MT NIST & MT BLEU), METEOR,
BEER, TER, Word Error Rate (WER), and Edit
Distance (E-Dist).4 All results are displayed via
easily navigable web pages that include details
of all sentences and scores (shown in interactive
score tables - Figure 1). A number of graphs show-
ing various score distributions are also created.

The key aims of VEMV are to make the eval-
uation of MT system translations easy to under-
take and to provide a wide range of feedback that
helps the user to inspect how well their system per-
formed, both at the sentence and dataset level.

2At the time of writing the online version did not work.
3https://www.nist.gov/
4A WER like metric that calculates the Levenshtein (edit)

distance between two strings, but at the character level.

72



Figure 1: A screenshot of an interactive score table showing two example sentences and their respective scores.

3.2 Input and Technical Specification

VEMV is written in Python 3 (also compatible
with Python 2.7). To run the tool, the following
software needs to be installed:

• Python >= 2.7 (required)

• NLTK5 >= 3.2.4 (required)

• Numpy (required)

• Matplotlib / Seaborn (optional - for graphs)

• Perl (optional - for MT BLEU, MT NIST)

• Java (optional - for METEOR, BEER, TER)

With the minimum required items installed the
software will generate scores for standard BLEU,
WER, and E-Dist. The optional items enable a
user to run a wider range of metrics and produce
nearly 200 graphs during evaluation.

The input commands to run the software can be
typed directly into the command-line on any plat-
form, or passed as arguments in an interactive de-
velopment environment (IDE) such as Spyder.6

Once the software has been run (see Section
3.5), a folder containing all of the generated
HTML, text, and image files is produced. A user
will typically explore the output by opening the
‘main.html’ file in a browser (Chrome, Firefox,
and Opera have been tested) and navigating it like
with any (offline) website. The text files contain
the output for the various metric scores and can be
inspected in detail. The graphs are output as im-
age files (PNGs), which are primarily viewed in
the HTML pages, but can also be used separately
for reports (e.g. Figure 3 in Section 3.4)

5http://www.nltk.org
6https://github.com/spyder-ide/spyder/

releases

3.3 Main Features

Here we outline some key features of the Vis-Eval
Metric Viewer tool:

Scoring with multiple evaluation metrics

Currently VEMV uses eight evaluation met-
rics to score individual sentences and the whole
document. All results are shown side by side for
comparison purposes and can be inspected at a
granular level (Figure 1).

A glance at the two sentences in Figure 1 al-
ready provides numerous points for analysis. For
example, the MT in sentence 2 is a long way
from the reference and receives low metric scores.
However, whilst not identical to the reference, the
MT is correct and could be interchanged with the
reference without losing meaning. For sentence 1
the MT is only a single word away from the refer-
ence and receives good scores, (much higher than
sentence 2) although the meaning is incorrect. The
interactive display enables the user to easily exam-
ine such phenomena in a given dataset.

Clear and easily navigable output

The main output is shown as a series of web
pages and can be viewed in modern browsers.
The browsers themselves also have a number of
powerful built-in functions, such as page search,
which are applicable to any of the output pages,
adding an extra layer of functionality.

The output consists of easily navigable interac-
tive score tables and graphs, logically organised
across web pages. The tool includes its own search
facility (for target and source sentences) and the
option to show or hide metric scores to aid clarity,
especially useful for comparing only a selection of
metrics. All of the segment level metric scores can
be sorted according to the metric of interest.

73



Figure 2: A screenshot of the VisEval Metric Viewer main page.

Results saved locally

Once scored, the generated text files, images, and
HTML pages are saved locally in a number of
organised folders. The majority of the text files
are made up from the standard raw output of the
metrics themselves. The image files are statistical
graphs produced from the metric scores. Both the
text and image files can be inspected directly on a
metric by metric basis and used for reference. The
VEMV tool brings together the text and images in
the HTML files to form the main viewable output.

Runtime user options

The minimal default settings will quickly
produce scores for standard BLEU, WER and
E-Dist. Numerous parameters can be set on the
command line enabling the user to choose any or
all of the additional metrics and whether or not to
generate graphs.

A number of the metrics (especially BLEU and
METEOR) have a plethora of parameters, which
can be selected. To avoid the need for complex
command line inputs the metric level parameters
can be placed in an easily editable text based con-
figuration file, which in turn is passed to the com-
mand line.

In addition, the user can choose which metric
will be the dominant one for sorting and display
purposes (the default is BLEU) and there is an op-
tion for selecting how many score bins or pages
to use to show the sentences. The default is 100
pages (one for every percentage point), but some
users may prefer fewer pages (e.g. 10 or 20) in
order to simplify the main interface and general
navigation.

An accessibility flag has also been added. It re-
moves some of the colour formatting from the dis-
plays making it easier for users with visual impair-
ments (e.g colour blindness).

3.4 Viewing the Actual Output

Figure 2 shows the main page of the software. In
this case all eight metrics were used as shown by
the mini graph icons. Each of these mini graph
icons act as a link. Ten score bins (circular icons)
were selected as a parameter.

Users can click on any of the links/icons to nav-
igate to the various pages. Clicking on the circular
icons opens the sentence level score pages (Fig-
ure 1) showing individual sentences with a given
score. Clicking on the mini graph icons takes the
user to the graph display web pages for the respec-
tive metrics or the general document wide statis-
tics. Figure 3, for example, is a metric graph show-
ing the distribution of standard BLEU scores for
the dataset. In this case the chart in Figure 3 would
be accessed by clicking on the very left hand mini
graph icon on the main page shown in Figure 2.

Figure 3: A graph showing the distribution of standard
BLEU scores.

74



3.5 Downloading and Running the Tool
Vis-Eval Metric Viewer can currently be down-
loaded from the following location on GitHub:
https://github.com/David-Steele/
VisEval_Metric_Viewer.

The associated README file provides instruc-
tions on how to get started with using the tool, and
what to do if you run into any problems.

In terms of hardware requirements, a computer
with at least 2GB of RAM and 300MB of available
storage is needed to run the software.

A short video demonstration of these and other
features of the Vis-Eval Metric Viewer software
can be found online at: https://youtu.be/
nUmdlXGYeMs.

4 Conclusion and Future Work

The Vis-Eval Metric Viewer tool was designed
with three main aims:

• To provide a useful tool that is easy to install
(using readily available packages), and sim-
ple to use and run on a local machine without
the need for a server or internet connection.

• To offer a single place for scoring translations
using multiple popular metrics.

• To provide in depth visual feedback making it
easy to examine segment level metric scores.

The tool offers a light weight solution that
makes it easy to compare multiple-metric scores
in a clear manner. Feedback can be interactively
explored and searched rapidly with ease, whilst
numerous graphs provide additional information.
The tool can be run locally on any platform. All
results are saved in self-contained folders for easy
access.

We plan to add the following functionalities to
VEMV in the future:

• Dynamic graphs, enabling users to select (in
real time) variables to compare, and other
features such as zooming in/out.

• Inclusion of a few additional popular light-
weight evaluation metrics. The modular de-
sign of the software means that adding new
metrics is a relatively trivial process.

• Using the saved output from the tool to com-
pare multiple MT systems against one an-
other.

References
Bogdan Babych. 2014. Automated mt evaluation met-

rics and their limitations. Tradumàtica, (12):464–
470.

Michael Denkowski and Alon Lavie. 2014. Meteor
universal: Language specific translation evaluation
for any target language. In Proceedings of the Ninth
Workshop on Statistical Machine Translation, pages
376–380, Baltimore, Maryland, USA. Association
for Computational Linguistics.

Giménez, Jesús Màrquez, and Lluı́s. 2010. Asiya:
An open toolkit for automatic machine translation
(meta-)evaluation. The Prague Bulletin of Mathe-
matical Linguistics, 94:77–86.

Ondřej Klejch, Eleftherios Avramidis, Aljoscha Bur-
chardt, and Martin Popel. 2015. MT-compareval:
Graphical evaluation interface for machine transla-
tion development. The Prague Bulletin of Mathe-
matical Linguistics, (104):63–74.

Nitin Madnani. 2011. ibleu: Interactively debugging
& scoring statistical machine translation systems. In
Proceedings of the Fifth IEEE International Confer-
ence on Semantic Computing, pages 213–214.

Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: A method for automatic eval-
uation of machine translation. In Proceedings of the
40th Annual Meeting on Association for Computa-
tional Linguistics, pages 311–318, Stroudsburg, PA,
USA. Association for Computational Linguistics.

Matthew Snover, Bonnie Dorr, Richard Schwartz, Lin-
nea Micciulla, and John Makhoul. 2006. A study
of translation edit rate with targeted human annota-
tion. In In Proceedings of Association for Machine
Translation in the Americas, pages 223–231.

Xingyi Song, Trevor Cohn, and Lucia Specia. 2013.
BLEU deconstructed: Designing a better MT eval-
uation metric. International Journal of Computa-
tional Linguistics and Applications, (4):29–44.

Milos Stanojevic and Khalil Sima’an. 2014. Beer: Bet-
ter evaluation as ranking. In Proceedings of the
Ninth Workshop on Statistical Machine Translation,
pages ”414–419”. ”Association for Computational
Linguistics”.

Ying Zhang, Stephan Vogel, and Alex Waibel. 2004.
Interpreting bleu/nist scores: How much improve-
ment do we need to have a better system. In Pro-
ceedings of Language Resources and Evaluation,
pages 2051–2054.

75


