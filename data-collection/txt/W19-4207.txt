



















































IT–IST at the SIGMORPHON 2019 Shared Task: Sparse Two-headed Models for Inflection


Proceedings of the 16th Workshop on Computational Research in Phonetics, Phonology, and Morphology, pages 50–56
Florence, Italy. August 2, 2019 c©2019 Association for Computational Linguistics

50

IT–IST at the SIGMORPHON 2019 Shared Task: Sparse Two-headed
Models for Inflection

Ben Peters† and André F.T. Martins†‡
†Instituto de Telecomunicações, Lisbon, Portugal

‡Unbabel, Lisbon, Portugal
benzurdopeters@gmail.com, andre.martins@unbabel.com

Abstract
This paper presents the Instituto de
Telecomunicações–Instituto Superior Técnico
submission to Task 1 of the SIGMORPHON
2019 Shared Task. Our models combine
sparse sequence-to-sequence models with a
two-headed attention mechanism that learns
separate attention distributions for the lemma
and inflectional tags. Among submissions to
Task 1, our models rank second and third.
Despite the low data setting of the task (only
100 in-language training examples), they
learn plausible inflection patterns and often
concentrate all probability mass into a small
set of hypotheses, making beam search exact.

1 Introduction

Morphological inflection is the task of producing
an inflected form, given a lemma and a set of
inflectional tags. A widespread approach to the
task is the attention-based sequence-to-sequence
model (seq2seq; Bahdanau et al., 2015; Kann and
Schütze, 2016); such models perform well but are
difficult to interpret. To mitigate this shortcom-
ing, we employ an alternative architecture which
combines sparse seq2seq modeling (Peters et al.,
2019) with two-headed attention that attends sep-
arately to the lemma and inflectional tags (Ács,
2018). The attention and output distributions are
computed with the sparsemax function and mod-
els are trained to minimize sparsemax loss (Mar-
tins and Astudillo, 2016). Sparsemax, unlike soft-
max, can assign exactly zero attention weight to
irrelevant source tokens and exactly zero probabil-
ity to implausible hypotheses. We apply our mod-
els to Task 1 at the SIGMORPHON 2019 Shared
Task (McCarthy et al., 2019), which extends mor-
phological inflection to a cross-lingual setting. We
present two sparse seq2seq architectures:

• DOUBLEATTN (it-ist-01-1) is a reim-
plementation of the two-headed attention

model (Ács, 2018) which substitutes sparse-
max and its loss for softmax and cross en-
tropy loss. It uses separate encoders and at-
tention heads for the lemma and inflections,
and concatenates the outputs of the attention
heads.

• GATEDATTN (it-ist-02-1) replaces the
attention concatenation with a sparse gate
which interpolates the lemma and inflection
attention. The intuition is that the lemma and
inflectional tags are not likely to be equally
important at all time steps. For example, in
a suffixing language, the first several gener-
ated characters are likely to be identical to the
lemma; inflectional tags are not relevant. The
sparse gate allows the model to learn to shift
focus between the two attentions while ignor-
ing the other at a given time step.

GATEDATTN and DOUBLEATTN rank second
and third, respectively, among submissions to
Task 1. In addition, their behavior is highly in-
terpretable: they mostly learn to attend to a single
lemma hidden state at a time, progressing mono-
tonically from left to right, while their inflection
attention learns patterns which reflect underlying
morphological structure. The sparse output layer
often allows the model to concentrate all probabil-
ity mass into a single hypothesis, providing a cer-
tificate that decoding is exact. Our analysis shows
that sparsity is also highly predictive of perfor-
mance on the shared task metrics, showing that the
models “know what they know”.

2 Models

Our architecture is mostly the same as a standard
RNN-based seq2seq model with attention. In this
section, we outline the changes needed to extend
this model to use sparsemax and two-headed at-
tention in a multilingual setting.

mailto:benzurdopeters@gmail.com
mailto:andre.martins@unbabel.com


51

2.1 Sparsemax

Our models’ sparsity comes from the sparsemax
function (Martins and Astudillo, 2016), which
computes the Euclidean projection of a vector z ∈
Rn onto the n–dimensional probability simplex
4n := {p ∈ Rn : p ≥ 0,1>p = 1}:

sparsemax(z) := argmin
p∈4n

‖p− z‖2 (1)

Like softmax, sparsemax converts an arbitrary
real-valued vector into a probability distribution.
The critical difference is that sparsemax can as-
sign exactly zero probability, whereas softmax is
strictly positive. Sparsemax is differentiable al-
most everywhere and can be computed quickly,
allowing its use as a drop-in replacement for soft-
max. It has previously been used in seq2seq
for computing both attention weights (Malaviya
et al., 2018) and output probabilities (Peters et al.,
2019). Sparse attention is attractive in morpholog-
ical inflection because it resembles hard attention,
which has been successful on the task (Aharoni
and Goldberg, 2017; Wu et al., 2018).

2.2 Encoder–Decoder Model

Multilingual embeddings Each encoder and
decoder uses an embedding layer to convert one-
hot token representations into dense embeddings.
To account for the bilingual nature of Task 1, each
of our embedding layers contains two look-up ta-
bles: one for the sequence of input tokens, and
the other for the language of the sequence. At
each time step, the current token’s embedding is
concatenated to a language embedding.1 Each
encoder and decoder uses a separate embedding
layer; no weights are tied. Characters and inflec-
tional tags use embeddings of size Dc, while lan-
guage embeddings are of size D`. Thus the total
embedding size is D = Dc +D`.

Encoders The lemma and inflection encoders
are both bidirectional LSTMs (Graves and
Schmidhuber, 2005). An encoder’s forward and
backward hidden states are concatenated, forming
a sequence of source hidden states. We set the size
of these hidden states as D in all experiments.

Decoder The decoder is a unidirectional LSTM
(Hochreiter and Schmidhuber, 1997) with input

1The language embedding is the same at all time steps
within an example; there is no code-switching in this task.

feeding (Luong et al., 2015). At time step t, it
computes a hidden state st ∈ RD. Conditioned
on st and the hidden state sequences from the
lemma and inflection encoders, a two-headed at-
tention mechanism then computes an attentional
hidden state s̃t ∈ RD. The decoder LSTM is ini-
tialized only with the lemma encoder’s state.

Attention head At time t, an attention head
computes a context vector ct ∈ RD conditioned
on the decoder state st and an encoder state se-
quence H = [h1, . . . ,hJ ]. A head consists of
two modular components:

1. Alignment Compute a vector a ∈ RJ of
alignment scores between st and H . We
use the general attention scorer (Luong et al.,
2015), which computes aj := s>t Wahj .

2. Context Compute the context vector ct as
a weighted sum of H: ct :=

∑J
j=1 πjhj ,

where π = sparsemax(a) is a sparse vector
of alignment scores in the simplex.

In Luong et al.’s single-headed attention, the at-
tentional hidden state s̃t = tanh(Ws[ct; st]) is
computed by a concatenation layer from the con-
text vector and pre-attention hidden state. How-
ever, our two-headed attention mechanism pro-
duces two context vectors and so must be com-
puted differently. We use two different formula-
tions, which we describe next.

DOUBLEATTN uses the same strategy for com-
bining multiple context vectors as Ács (2018): the
lemma and inflection context vectors ut and vt
and the target hidden state st are inputs to a con-
catenation layer:

s̃t = tanh(Wd[ut;vt; st]) (2)

whereWd ∈ RD×3D.

GATEDATTN, on the other hand, computes sep-
arate candidate attentional hidden states for the
two context vectors:

s̃ut = tanh(Wu[ut; st]) (3)

s̃vt = tanh(Wv[vt; st]) (4)

where Wu,Wv ∈ RD×2D. We define gate
weights Wg ∈ R2×3D and gate bias bg ∈ R2 and



52

Model Acc. ↑ Lev. Dist. ↓

DOUBLEATTN 48.999 1.2918
GATEDATTN 50.179 1.3209

Baseline (Wu and Cotterell, 2019) 48.549 1.3281

Table 1: Task 1 test results on the SIGMORPHON
2019 Shared Task, averaged across language pairs.

use a sparse gate to compute weights pt ∈ 42 for
the two candidate states:

pt = sparsemax(Wg[ut;vt; st] + bg) (5)

We then stack the two candidate states s̃ut and s̃vt
into a matrix S̃t ∈ R2×D and use the gate weights
to compute s̃t as a weighted sum of them:

s̃t = ptS̃ (6)

Just as a two-dimensional softmax is equivalent
to a sigmoid, this two-dimensional sparsemax is
a hard sigmoid, as was pointed out by Martins and
Astudillo (2016). It provides extra interpretability
in the form of a three-way answer about what is
relevant at a time step: the lemma, the inflections,
or both.

Sparse outputs After the attentional hidden
state is computed, an output layer computes scores
for each output type z = Wzs̃ + bz . These are
then converted into a sparse probability distribu-
tion p? = sparsemax(z). The model is trained
to minimize the sparsemax loss (Martins and As-
tudillo, 2016), defined as

Lsparsemax(y,z) :=
1

2
(‖ey − z‖2 − ‖p? − z‖2)

(7)
where y is the index of the gold target and ey is
a one-hot vector. The sparsemax loss is differen-
tiable, convex, and has a margin, and its gradient
is sparse. Although softmax-based models use the
cross entropy loss, this is not possible for our mod-
els because the cross entropy loss is infinite when
the model assigns zero probability to the gold tar-
get.

3 Results

Our test results are shown in Table 1. Our two
models ranked second and third among official
submissions to Task 1.

Hyperparameter Value

Character embedding size 180
Tag embedding size 180
Language embedding size 20
RNN size 200
Lemma encoder layers 2
Inflection encoder layers {1, 2}
Dropout {0.3, 0.4, 0.5}

Table 2: Hyperparameters for all models. Bracketed
values were tuned individually for each language pair.

3.1 Experimental set-up

Each model was trained with early stopping for a
maximum of 30 epochs with a batch size of 64. We
used the Adam optimizer (Kingma and Ba, 2015)
with a learning rate of 10−3, which was halved
when validation accuracy failed to improve for
three consecutive epochs. We tuned the dropout
and the number of inflection encoder layers on a
separate grid for each language pair. Our hyperpa-
rameter ranges are shown in Table 2. At test time,
we decoded with a beam size of 5. We oversam-
pled the low resource training data 100 times and
did not use synthetic data or filter the corpora. We
implemented our models in PyTorch (Paszke et al.,
2017) with a codebase derived from OpenNMT-py
(Klein et al., 2017).2

Hyperparameters for Uzbek The Uzbek train-
ing set contains only 1060 examples, much
smaller than the other high resource corpora, and
initial results with Uzbek language pairs were
poor. We improved performance by oversampling
the Uzbek data 10 times (yielding roughly the
same high-low balance as the other pairs) and re-
ducing the initial learning rate to 10−4.

4 Analysis

Next we interpret our models’ behavior on a selec-
tion of language pairs from Task 1.

4.1 Sparse Attention

Table 3 shows the sparsity of our attention mech-
anisms, averaged across language families. The
attention is extremely sparse, especially over the
lemma: models attend to fewer than 1.1 lemma
characters per target character on average. Spar-
sity is more varied between language families in
the inflection attention. This may be explained by

2Our code is available at https://github.com/
deep-spin/SIGMORPHON2019.

https://github.com/deep-spin/SIGMORPHON2019
https://github.com/deep-spin/SIGMORPHON2019


53

DOUBLEATTN GATEDATTN
Family Lemma Infl. Lemma Infl. Total #Langs

Afro-Asiatic 1.11 1.55 1.14 1.27 1.62 5
Baltic 1.02 1.33 1.02 1.34 1.54 1
Celtic 1.23 1.54 1.20 1.59 2.00 12
Dravidian 1.69 1.42 1.86 1.35 1.95 1
Germanic 1.05 1.56 1.05 1.39 1.61 17
Greek 1.15 1.52 1.18 1.62 2.11 1
Indo-Iranian 1.10 1.35 1.11 1.37 1.67 7
Murrinhpatha 1.16 1.28 1.15 1.35 1.71 1
Niger-Congo 1.07 1.30 1.04 1.09 1.11 1
NW Caucasian 1.02 1.08 1.02 1.20 1.25 2
Quechua 1.24 1.19 1.09 1.23 1.42 1
Romance 1.01 1.27 1.02 1.30 1.39 10
Slavic 1.08 1.31 1.07 1.35 1.55 9
Turkic 1.06 1.09 1.08 1.15 1.35 20
Uralic 1.03 1.21 1.04 1.31 1.48 12

Overall 1.09 1.33 1.09 1.33 1.56 100

Table 3: Average number of positions with nonzero attention per target time step on the Task 1 development sets,
grouped by the family of the low resource language. For DOUBLEATTN, this is simply averaged over all target
time steps. For GATEDATTN, the lemma attention nonzeros are summed only over time steps in which the gate
is active for the lemma, and similarly for the inflection attention nonzeros. The ‘Total’ column for GATEDATTN
indicates the average number of nonzeros over all time steps after accounting for the gate.

typological differences between languages, which
we next analyze in detail.

Turkic languages are characterized by concate-
native inflections (Bickel and Nichols, 2013b)
which represent individual features (monoexpo-
nence; Bickel and Nichols, 2013a). Monoexpo-
nence should allow the inflection attention to con-
centrate on a single tag at a time, and Table 3 con-
firms that Turkic inflection attention is among the
sparsest for both DOUBLEATTN and GATEDATTN
models. The Azeri attention plot in Figure 1 illus-
trates that the inflection attention usually focuses
on a single morpheme at a time, with some dis-
crepancies at morpheme boundaries, where other
tags may be relevant because of voicing assimi-
lation rules. Furthermore, the sparse gate gener-
ally allows the model to focus on only one atten-
tion head at a time: in the Azeri example, there
is only one position at which both attention heads
are used. This position is the final consonant of
the lemma, which appears to change because of
a phonological environment created by the suffix.
The shared task results suggest that sparse inflec-
tion attention is a good inductive bias for aggluti-
native languages: one of our models has the best

test accuracy among task submissions on 11 of
20 pairs where the low resource language is Tur-
kic and 11 of 12 pairs in the typologically similar
Uralic languages.

Germanic languages present different chal-
lenges, which may explain our models’ less sparse
inflection attention. Often several inflections are
fused into a single affix; a familiar example is the
German suffix st, which marks a verb as present
tense, second person, and singular, but has no sep-
arable parts that represent these features individ-
ually. The North Frisian plot in Figure 1 demon-
strates the less sparse nature of Germanic inflec-
tion attention. Producing “wulst” from the lemma
“wel” requires both a suffix and a change to the
lemma, and multiple inflectional tags are attended
to at several time steps. The fusional nature of
the morphology means there is not a clear align-
ment between the inflected sequence and the tags.
This in reflected in the fact that at many time steps,
DOUBLEATTN and GATEDATTN disagree about
which tags to attend to. Unlike in the Turkic ex-
ample, GATEDATTN’s gate usually gives weight
to both attention heads. This makes sense because
the inflection requires a change to the lemma, not



54

k ö m k N AB
L
SG PS

S1
S

k
ö

m

y
i

m
d

n
</s>

k ö m k N AB
L
SG PS

S1
S

w e l V IN
D
PS

T
PF

V
2 SG

w
u
l
s
t

</s>

w e l V IN
D
PS

T
PF

V
2 SG

Figure 1: Pairs of attention plots for Azeri (left) and North Frisian (right) words with models trained on the Turkish–
Azeri and Danish–North Frisian language pairs. Within each pair, the left plot comes from the DOUBLEATTN
model and the right plot from GATEDATTN. Gray squares have zero attention weight. The dashed vertical line
separates the lemma and inflection attention heads. Attention values in the GATEDATTN plots are scaled by gate
weights, which is why there is no inflection attention for the first several positions of the Azeri word.

just a suffix that follows it.

4.2 Sparse Output Layer

Sparse output probabilities provide tools for anal-
ysis that are not available to softmax-based mod-
els: when no hypotheses are pruned, they provide
a certificate that beam search is exact; when only
one hypothesis is possible, this gives an indication
of the model’s certainty; and when probability is
distributed among a small set of hypotheses, it is
easy to reason about what phenomena continue to
confuse the model.

Certainty When the probability distribution is
completely concentrated at each time step, the
model will be able to generate only one hypoth-
esis, regardless of the beam width. When this
happens for a particular input, the model can be
said to be certain for that input. This also triv-
ially guarantees that beam search is exact because
no hypotheses have been pruned. As Figure 2
shows, certainty is a strong indication of perfor-
mance. This suggests future work using certainty
as a validation metric alternative to accuracy and
loss.

Interpretable ambiguity Our Turkish–Azeri
GATEDATTN model demonstrates that there is
also useful information to be gleaned from the
cases where the model produces multiple hypothe-
ses. Of the 100 examples in the development set,
GATEDATTN concentrated all probability mass
into a single hypothesis for 79 of them, but the
other 21 examples exhibit ambiguities that have
linguistically plausible interpretations:

0 20 40 60 80
Certainty Rate

0

25

50

75

100
Ac

cu
ra

cy

Figure 2: Percentage of inputs for which the selected
GATEDATTN model concentrates all probability into a
single hypothesis compared to the word-level accuracy
on the development set. Each point is a language pair.

• Consonant alternations In 13 of the 21 ex-
amples, the hypotheses differ in their treat-
ment of stop consonants, which have very
similar phonological alternations in the two
languages that are represented in orthogra-
phy. The ambiguity is a sign that the model
has not mastered Azeri phonological rules.
Nine of the examples concern lemma-final
“k” and “q”, which have slightly different
rules in Azeri than Turkish.3

3This judgment is based on inspection of the Azeri data
and prior knowledge of Turkish.



55

i ç e c e k </s>

@ k </s>
@ c e k </s>

@ k </s>

72.7%

27.3%

67.5%

32.5%

52.0%

48.0%

Figure 3: The Turkish–Azeri GATEDATTN model’s full
beam search for the Azeri lemma “içmek” and the tags
V 3 SG FUT. All other sequences have zero proba-
bility. The correct form is “iç@c@k”, while the model
prefers “içecek”, which would be correct with Turkish
vowel harmony rules.

• Vowel harmony In two other examples, the
model produced multiple guesses for the
vowels in the future tense marker. One of
these examples is shown in Figure 3. In both
instances, Azeri vowel harmony rules would
generate “@” in the suffix, but the model in-
stead produced “e”, which is correct with
Turkish vowel harmony. This shows the in-
fluence of the high resource language.

• Other cases The last six non-certain exam-
ples consist of a loanword with an unusual
character sequence, two instances where one
hypothesis has the wrong possessive suffix,
and two where a hypothesis inserts or drops
a character. The top prediction was nonethe-
less correct in all six.

This sort of analysis is not possible with tradi-
tional dense models because probability can never
become concentrated in a small set of hypotheses
and it is impossible to separate legitimate ambigu-
ities from the long tail of implausible hypotheses.

Paradigm completion Figure 3 suggests that
our models do a good job of concentrating prob-
ability in a small number of hypotheses. This
raises the question of whether, by underspecify-
ing the inflectional tags, the set of possible hy-
potheses in the beam can be made to resemble
a lemma’s complete paradigm. To investigate,
we trained monolingual models with the English,
German, and Turkish data from the high resource
setting of Task 1 of the CoNLL–SIGMORPHON
2018 Shared Task (Cotterell et al., 2018). We used
mostly the same hyperparameters as for this year’s
submission, except that there are no language em-
beddings, and the inflection tags are not used and
the models have single-headed attention over the
lemma sequences. We increased the beam width to

10 in order to accomodate the models’ greater un-
certainty. With English, this often works well: for
the regular verb “jitter”, the model’s only possible
hypotheses are “jittered”, “jittering”, “jitters”, and
“jitter”, which is the complete paradigm. Irregu-
lar verbs often have a handful of other hypotheses,
and sometimes the beam gives some probability
to misspellings. Something similar can be seen
in German, although the beam rarely contains all
surface forms. For German nouns, the beam often
shows uncertainty about plural formation: the hy-
potheses for “Nadelbaum” include “Nadelbaume”,
“Nadelbäume”, and “Nadelbäumer”, all of which
are plausible German plurals. Turkish has very
large paradigms, so in general it is not possible
to fit all forms into a beam of any reasonable size.
However, the hypotheses in the beam do typically
correspond to correct forms.

5 Conclusion

We presented a new style of seq2seq model which
brings together two-headed attention (Ács, 2018)
and sparse modeling for morphological inflection
(Peters et al., 2019). Our models learn sparse at-
tention distributions in both attention heads. Their
sparse probability distribution over hypotheses of-
ten allows beam search to become exact, while the
remaining ambiguities often have a clear linguistic
interpretation. The two versions of our model rank
second and third among submissions to Task 1.

Acknowledgments

This work was supported by the European Re-
search Council (ERC StG DeepSPIN 758969),
and by the Fundação para a Ciência e Tecnolo-
gia through contracts UID/EEA/50008/2019 and
CMUPERI/TIC/0046/2014 (GoLocal). We thank
Gonçalo Correia, Aitor Egurtzegi, Erick Fonseca,
Pedro Martins, Tsvetomila Mihaylova, Vlad Nic-
ulae, Marcos Treviso, and the anonymous review-
ers, for helpful discussion and feedback.

References
Judit Ács. 2018. BME-HAS system for CoNLL–

SIGMORPHON 2018 shared task: Universal mor-
phological reinflection. Proceedings of the CoNLL
SIGMORPHON 2018 Shared Task: Universal Mor-
phological Reinflection, pages 121–126.

Roee Aharoni and Yoav Goldberg. 2017. Morphologi-
cal inflection generation with hard monotonic atten-
tion. In Proc. ACL.

https://www.aclweb.org/anthology/K18-3016
https://www.aclweb.org/anthology/K18-3016
https://www.aclweb.org/anthology/K18-3016
https://arxiv.org/abs/1611.01487
https://arxiv.org/abs/1611.01487
https://arxiv.org/abs/1611.01487


56

Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Ben-
gio. 2015. Neural machine translation by jointly
learning to align and translate. In Proc. ICLR.

Balthasar Bickel and Johanna Nichols. 2013a. Ex-
ponence of selected inflectional formatives. In
Matthew S. Dryer and Martin Haspelmath, editors,
The World Atlas of Language Structures Online.
Max Planck Institute for Evolutionary Anthropol-
ogy, Leipzig.

Balthasar Bickel and Johanna Nichols. 2013b. Fusion
of selected inflectional formatives. In Matthew S.
Dryer and Martin Haspelmath, editors, The World
Atlas of Language Structures Online. Max Planck
Institute for Evolutionary Anthropology, Leipzig.

Ryan Cotterell, Christo Kirov, John Sylak-Glassman,
Gėraldine Walther, Ekaterina Vylomova, Arya D
McCarthy, Katharina Kann, Sebastian Mielke, Gar-
rett Nicolai, Miikka Silfverberg, David Yarowsky,
Jason Eisner, and Mans Hulden. 2018. The
CoNLL–SIGMORPHON 2018 shared task: Uni-
versal morphological reinflection. Proc. CoNLL–
SIGMORPHON.

Alex Graves and Jürgen Schmidhuber. 2005. Frame-
wise phoneme classification with bidirectional
LSTM and other neural network architectures. Neu-
ral Networks, 18(5-6):602–610.

Sepp Hochreiter and Jürgen Schmidhuber. 1997.
Long short-term memory. Neural Computation,
9(8):1735–1780.

Katharina Kann and Hinrich Schütze. 2016. Single-
Model Encoder-Decoder with Explicit Morphologi-
cal Representation for Reinflection. In Proc. ACL.

Diederik Kingma and Jimmy Ba. 2015. Adam: A
method for stochastic optimization. In Proc. ICLR.

Guillaume Klein, Yoon Kim, Yuntian Deng, Jean
Senellart, and Alexander M Rush. 2017. OpenNMT:
Open-source toolkit for neural machine translation.
arXiv e-prints.

Minh-Thang Luong, Hieu Pham, and Christopher D
Manning. 2015. Effective approaches to attention-
based neural machine translation. In Proc. EMNLP.

Chaitanya Malaviya, Pedro Ferreira, and André FT
Martins. 2018. Sparse and constrained attention for
neural machine translation. In Proc. ACL.

André FT Martins and Ramón Fernandez Astudillo.
2016. From softmax to sparsemax: A sparse model
of attention and multi-label classification. In Proc.
of ICML.

Arya D. McCarthy, Ekaterina Vylomova, Shijie Wu,
Chaitanya Malaviya, Lawrence Wolf-Sonkin, Gar-
rett Nicolai, Christo Kirov, Miikka Silfverberg, Se-
bastian Mielke, Jeffrey Heinz, Ryan Cotterell, and
Mans Hulden. 2019. The SIGMORPHON 2019

shared task: Crosslinguality and context in morphol-
ogy. In Proceedings of the 16th SIGMORPHON
Workshop on Computational Research in Phonetics,
Phonology, and Morphology”, Florence, Italy. As-
sociation for Computational Linguistics.

Adam Paszke, Sam Gross, Soumith Chintala, Gre-
gory Chanan, Edward Yang, Zachary DeVito, Zem-
ing Lin, Alban Desmaison, Luca Antiga, and Adam
Lerer. 2017. Automatic differentiation in PyTorch.
In Proc. NeurIPS Autodiff Workshop.

Ben Peters, Vlad Niculae, and André FT Martins. 2019.
Sparse Sequence-to-Sequence Models. In Proc.
ACL.

Shijie Wu and Ryan Cotterell. 2019. Exact Hard
Monotonic Attention for Character-Level Transduc-
tion. Proc. ACL.

Shijie Wu, Pamela Shapiro, and Ryan Cotterell. 2018.
Hard non-monotonic attention for character-level
transduction. In Proc. EMNLP.

https://arxiv.org/abs/1409.0473
https://arxiv.org/abs/1409.0473
https://wals.info/chapter/21
https://wals.info/chapter/21
https://wals.info/chapter/20
https://wals.info/chapter/20
https://arxiv.org/abs/1810.07125
https://arxiv.org/abs/1810.07125
https://arxiv.org/abs/1810.07125
https://mediatum.ub.tum.de/doc/1290193/file.pdf
https://mediatum.ub.tum.de/doc/1290193/file.pdf
https://mediatum.ub.tum.de/doc/1290193/file.pdf
https://dl.acm.org/citation.cfm?id=1246450
https://www.aclweb.org/anthology/P16-2090
https://www.aclweb.org/anthology/P16-2090
https://www.aclweb.org/anthology/P16-2090
https://arxiv.org/abs/1412.6980
https://arxiv.org/abs/1412.6980
http://arxiv.org/abs/1701.02810
https://arxiv.org/abs/1701.02810
http://arxiv.org/abs/1701.02810
https://arxiv.org/abs/1701.02810
https://arxiv.org/abs/1508.04025
https://arxiv.org/abs/1508.04025
http://aclweb.org/anthology/P18-2059
http://aclweb.org/anthology/P18-2059
https://arxiv.org/abs/1602.02068
https://arxiv.org/abs/1602.02068
https://openreview.net/pdf?id=BJJsrmfCZ
https://arxiv.org/abs/1905.05702
https://arxiv.org/abs/1905.06319
https://arxiv.org/abs/1905.06319
https://arxiv.org/abs/1905.06319
https://arxiv.org/abs/1808.10024
https://arxiv.org/abs/1808.10024

