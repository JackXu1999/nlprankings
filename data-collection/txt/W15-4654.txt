



















































Learning Domain-Independent Dialogue Policies via Ontology Parameterisation


Proceedings of the SIGDIAL 2015 Conference, pages 412–416,
Prague, Czech Republic, 2-4 September 2015. c©2015 Association for Computational Linguistics

Learning Domain-Independent Dialogue Policies via Ontology
Parameterisation

Zhuoran Wang1∗, Tsung-Hsien Wen2, Pei-Hao Su2, Yannis Stylianou1
1Toshiba Research Europe Ltd., Cambridge, UK

2Engineering Department, University of Cambridge, UK

Abstract

This paper introduces a novel approach
to eliminate the domain dependence of
dialogue state and action representations,
such that dialogue policies trained based
on the proposed representation can be
transferred across different domains. The
experimental results show that the policy
optimised in a restaurant search domain
using our domain-independent representa-
tions can be deployed to a laptop sale do-
main, achieving a task success rate very
close (96.4% relative) to that of the policy
optimised on in-domain dialogues.

1 Introduction

Statistical approaches to Spoken Dialogue Sys-
tems (SDS), particularly, Partially Observable
Markov Decision Processes (POMDPs) (Young
et al., 2013), have demonstrated great success
in improving the robustness of dialogue poli-
cies to error-prone Automatic Speech Recognition
(ASR). However, building statistical SDS (SSDS)
for different application domains is time consum-
ing. Traditionally, each component of such SSDS
needs to be trained based on domain-specific data,
which are not always easy to obtain. Moreover,
in many cases, one will need a basic (e.g. rule-
based) working SDS to be built before starting the
data collection procedure, where developing the
initial system for a new domain requires a signifi-
cant amount of human expertise.

In this paper, we introduce a simple but effective
approach to eliminate domain dependence of dia-
logue policies, by exploring the nature and com-
monness of the underlying tasks of SDS in dif-
ferent domains, and parameterising different slots
defined in the domain ontologies into a common

∗ZW’s present address is Baidu Inc., Beijing, China.

feature space according to their relations and po-
tential contributions to the underlying tasks. Af-
ter the parameterisation, the resulting policy can
be applied to different domains that realise a same
abstract task (see §3.3 for examples).

Existing works on domain-extension/transfer
for SDS include domain-independent intermediate
semantic extractors for Spoken Language Under-
standing (SLU) (Li et al., 2014), domain-general
rules (Wang and Lemon, 2013; Sun et al., 2014)
and delexicalised deep classifiers (Henderson et
al., 2014; Mrkšić et al., 2015) for dialogue state
tracking, and domain-extensible/transferable sta-
tistical dialogue policies (Lemon et al., 2006;
Gašić et al., 2013; Gašić et al., 2015). When com-
pared to the closely related methods by Gašić et al.
and Lemon et al. that manually tie slots in differ-
ent domains, our approach provides a more flex-
ible way to parametrically measure the similarity
between different domain ontologies and directly
addresses the nature of the underlying tasks.

For the ease of access to the proposed technique
(§3), we start from a brief review of POMDP-SDS
in §2. Promising experimental results are achieved
based on both simulated users and human subjects
as shown in §4, followed by conclusions (§5).

2 POMDP-SDS: A Brief Overview

A POMDP is a powerful tool for modelling se-
quential decision making problems under uncer-
tainty, by optimising the policy to maximise long-
term cumulative rewards. Concretely. at each turn
of a dialogue, a typical POMDP-SDS parses an
observed ASR n-best list with confidence scores
into semantic representations (again with associ-
ated confidence scores), and estimates a distribu-
tion over (unobservable) user goals, called a be-
lief state. After this, the dialogue policy selects
a semantic-level system action, which will be re-
alised by Natural Language Generation (NLG) be-
fore synthesising the speech response to the user.

412



The semantic representations in SDS normally
consist of two parts, a communication func-
tion (e.g. inform, deny, confirm, etc.)
and (optionally) a list of slot-value pairs (e.g.
food=pizza, area=centre, etc.). The prior
knowledge defining the slot-values in a particular
domain is called the domain ontology.

Dialogue policy optimisation can be solved via
Reinforcement Learning (RL), where the aim is
to estimate a quantity Q(b,a), for each b and a,
reflecting the expected cumulative rewards of the
system executing action a at belief state b, such
that the optimal action a∗ can be determined for
a given b according to a∗ = arg maxaQ(b,a).
Due the exponentially large state-action space an
SDS can incur, function approximation is nec-
essary, where it is assumed that Q(b,a) ≈
fθ(φ(b,a)). Here θ denotes the model param-
eter to be learnt, and φ(·) is a feature function
that maps (b,a) to a feature vector. To compute
Q(b,a), one can either use a low-dimensional
summary belief (Williams and Young, 2005) or
the full belief itself if kernel methods are applied
(Gašić et al., 2012). But in both cases, the action a
will be a summary action (see §3 for more details)
to achieve tractable computations.

3 Domain-Independent Featurisation

For the convenience of further discussion, we
firstly take a closer look at how summary actions
can be derived from their corresponding master
actions. Assume that according to its communi-
cation function, a system action a can take one
of the following forms: a() (e.g. reqmore()),
a(s) (e.g. request(food)), a(s = v)
(e.g. confirm(area=north)), a(s = v1,
s = v2) (e.g. select(food=noodle,
food=pizza)), and a(s1 = v1, . . . , sn =
vn) (e.g. offer(name="Chop Chop",
food=Chinese)), where a stands for the com-
munication function, s∗ and v∗ denote slots and
values respectively. Usually it is unnecessary for
the system to address a hypothesis less believable
than the top hypothesis in the belief (or the top
two hypotheses in the ‘select’ case). There-
fore, by abstracting the actual values, the sys-
tem actions can be represented as a

(
s = btops

)
,

a
(
s = btops , s = b

second
s

)
and a

(
btopjoint

)
, where

bs denotes the marginal belief with respect to slot
s, bjoint stands for the joint belief, and b

top∗ and
bsecond∗ denote the top and second hypotheses of

a given b∗, respectively. After this, summary ac-
tions can be defined as as (for actions depending
on s) and a (for those having no operands or only
taking joint hypotheses as operands, i.e. indepen-
dent of any particular slot). Furthermore, one can
uniquely map such summary actions back to their
master actions, by substituting the respective top
(and second if necessary) hypotheses in the belief
into the corresponding slots.

Based on the above definition, we can re-write
the master action a as as, where s denotes the
slot that a depends on when summarised. Here,
s is fully derived from a and can be null (when
the summary action of a is just a). Recalling the
RL problem, conventionally, φ can be expressed
as φ(b,as) = δ(as) ⊗ ψ(b) where δ is the Kro-
necker delta,⊗ is the tensor product, and generally
speaking, ψ(·) featurises the belief state, which
can yield a summary belief in particular cases.

3.1 “Focus-aware” belief summarisation

Without losing generality, one can assume that
the communication functions a are domain-
independent. However, since the slots s are
domain-specific (defined by the ontology), both as
and b will be domain-dependent.

Making ψ(b) domain-independent can be triv-
ial. A commonly used representation of b con-
sists of a set of individual belief vectors, denoted
as {bjoint,b◦} ∪ {bs}s∈S , where b◦ stands for
the section of b independent of any slots (e.g.
the belief over communication methods, such as
“by constraint”, “by name”, etc. (Thomson and
Young, 2010)) and S stands for the set of in-
formable (see Appendix A) slots defined in the do-
main ontology. One can construct a feature func-
tion ψ(b, s) = ψ1(bjoint)⊕ ψ2(b◦)⊕ ψ3(bs) for
a given s and let φ(b,as) = δ(as) ⊗ ψ(b, s),
where⊕ stands for the operator to concatenate two
vectors. (In other words, the belief summarisa-
tion here only focuses on the slot being addressed
by the proposed action, regardless of the beliefs
for the other slots.) As the mechanism in each
ψ∗ to featurise its operand b∗ can be domain-
independent (see §3.3 for an example), the result-
ing overall feature vector will be domain-general.

3.2 Ontology (slot) parameterisation

If we could further parameterise each slot s in a
domain-general way (as ϕ(s)), and define

φ(b,as) = δ(a)⊗ [ϕa(s)⊕ ψa(b, s)] (1)

413



the domain dependence of the overall feature func-
tion φ will be eliminated1. Note here, to make the
definition more general, we assume that the fea-
ture functions ϕa and ψa depend on a, such that a
different featurisation can be applied for each a.

To achieve a meaningful parameterisation
ϕa(s), we need to investigate how each slot s is
related to the completion of the underlying task.
More concretely, for example, if the underlying
task is to obtain user’s constraint on each slot
so that the system can conduct a database (DB)
search to find suitable entities (e.g. venues, prod-
ucts, etc.), then the slot features should describe
the potentiality of the slot to refine the search re-
sults (reduce the number of matching entities) if
that slot is filled. For another example, if the task
is to gather necessary (plus optional) information
to execute a system command (e.g. setting a re-
minder or planning a route), where the number of
values of each slot can be unbounded, then the
slots features should indicate whether the slot is
required or optional. In addition, the slots may
have some specific characteristics causing people
to address them differently in a dialogue. For
example, when buying a laptop, more likely one
would talk about the price first than the battery rat-
ing. Therefore, features describing the priority of
each slot are also necessary to yield natural dia-
logues. We give a complete list of features in §3.3
for a working example, to demonstrate how two
unrelated domains can share a common ontology
parameterisation.

3.3 A working example

We use restaurant search and laptop sale as two
example domains to explain the above idea. The
underlying tasks of the both problems here can be
regarded as DB search. Appendix A gives the de-
tailed ontology definitions of the two domains.

Firstly, the following notations are introduced
for the convenience of discussion. Let Vs denote
the set of the values that a slot s can take, and |·| be
the size of a set. Assume that h = (s1 = v1∧ . . .∧
sn = vn) is a user goal hypothesis consisting a set
of slot-value pairs. We use DB(h) to denote the set
of the entities in the DB satisfying h. In addition,
we define bxc to be the largest integer less than
and equal to x. After this, for each informable slot

1An alternative featurisation can be φ(b,as) = δ(a) ⊗
ϕa(s)⊗ψa(b, s), but our preliminary experiments show that
⊗ results in similar but slightly worse policies. Therefore, we
stick with ⊕ in this paper.

s defined in Table A.1, the following quantities are
used for its parameterisation.

• Number of values
– a continuous feature2, 1/|Vs|;
– discrete features mapping |Vs| intoN (= 6)

bins, indexed by min{blog2 |Vs|c, N}.
• Importance: two features describing, respec-

tively, how likely a slot will and will not oc-
cur in a dialogue.

• Priority: three features denoting, respectively,
how likely a slot will be the first, the second,
and a later attribute to address in a dialogue.

• Value distribution in the DB: the en-
tropy of the normalised histogram
(|DB(s = v)|/|DB|)v∈Vs .

• Potential contribution to DB search: given
the current top user goal hypothesis h∗ and a
pre-defined threshold τ (= 12)

– how likely filling s will reduce the number
of matching DB records to below τ , i.e.
|{v : v ∈ Vs, |DB(h∗ ∧ s = v)| ≤ τ}| /|Vs|;

– how likely filling swill not reduce the num-
ber of matching DB records to below τ , i.e.
|{v : v ∈ Vs, |DB(h∗ ∧ s = v)| > τ}| /|Vs|;

– how likely filling s will result in no
matching records found in the DB, i.e.
|{v : v ∈ Vs,DB(h∗ ∧ s = v) = ∅}| /|Vs|.

The importance and priority features used in this
work are manually assigned binary values, but ide-
ally, if one has some in-domain human dialogue
examples (e.g. from Wizard-of-Oz experiments),
such feature values can be derived from simple
statistics on the corpus. In addition, we make the
last set of features only applicable to those slots
not observed in the top joint hypothesis.

The summary belief features used in this work
are sketched as follows. For each informable slot s
and each of its applicable action types a, ψa(b, s)
extracts the probability of btops , the entropy of
bs, the probability difference between the top two
marginal hypotheses (discretised into 5 bins with
interval size 0.2) and the non-zero rate (|{v : v ∈
Vs,bs(v) > 0}|/|Vs|). In addition, if the slot is
requestable, the probability of it being requested

2The normalisation is to make this feature to have a simi-
lar value range to the others, for numerical stability purposes
in Gaussian Process (GP) based policy learning (see §4).

414



System Reward Success (%) #Turns
DIPin-domain 12.5±0.3 98.3±1.2 7.2±0.3
DIPtransferred 12.2±0.4 97.8±0.9 7.4±0.3

Table 1: Policy evaluations in the laptop sale do-
main based on simulated dialogues.

System #Dialogues Success (%) Score
DIPin-domain 122 84.4 4.51
DIPtransferred 140 81.4 4.83

Table 2: Policy evaluations using human subjects.

by the user (Thomson and Young, 2010) is used as
an extra feature. A similar featurisation procedure
(except the “requested” probability) is applied to
the joint belief as well, from which the obtained
features are used for all communication functions.
To capture the nature of the underlying task (DB
search), we define two additional features for the
joint belief, an indicator [[|DB(btopjoint)| ≤ τ ]] and
a real-valued feature |DB(btopjoint)|/τ if the former
is false, where τ is the same pre-defined threshold
used for slot parameterisation as introduced above.
There are also a number of slot-independent fea-
tures applied to all action types, including the be-
lief over communication methods (Thomson and
Young, 2010) and the marginal confidence scores
of user dialogue act types in the current turn.

4 Experimental Results

In the following experiments, the proposed
domain-independent parameterisation (DIP)
method were integrated with a generic dialogue
state tracker (Wang and Lemon, 2013) to yield an
overall domain-independent dialogue manager.
Firstly, we trained DIP dialogue policies in the
restaurant search domain using GP-SARSA based
on a state-of-the-art agenda-based user simulator3

(Schatzmann et al., 2007), in comparison with the
GP-SARSA learning process for the well-known
BUDS system (Thomson and Young, 2010)
(where full beliefs are used (Gašić and Young,
2014)), as shown in Figure 1. It can be found that
the proposed method results in faster convergence
and can even achieve slightly better performance
than the conventional approach.

After this, we directly deployed the DIP poli-

3For all the experiments in this work, the confusion rate
of the simulator was set to 15% and linear kernels were used
for GP-SARSA.

# of dialogues
0 1,000 2,000 3,000 4,000 5,000 6,000 7,000 8,000 9,000 10,000

S
u
cc

e
ss

 r
a
te

0.8

0.9

1

BUDS GP-SARSA
DIP GP-SARSA

# of dialogues
0 1,000 2,000 3,000 4,000 5,000 6,000 7,000 8,000 9,000 10,000

A
ve

ra
g
e
 r

e
w

a
rd

6

8

10

12

14

BUDS GP-SARSA
DIP GP-SARSA

Figure 1: Training GP-SARSA policies for BUDS
(full belief) and DIP in the restaurant search do-
main. Every point is averaged over 5 policies each
evaluated on 1000 simulated dialogues, with the
error bar being standard deviation.

cies trained in the restaurant search domain to the
laptop sale domain, and compared its performance
with an in-domain policy trained using the simula-
tor (configured to the laptop sale domain). Table 1
shows that the performance of the transferred pol-
icy is almost identical to the in-domain policy.

Finally, we chose the best in-domain and trans-
ferred DIP policies and deployed them into end-
to-end laptop sale SDSs, for human subject exper-
iments based on MTurk. After each dialogue, the
user was also asked to provide a subjective score
for the naturalness of the interaction, ranging from
1 (very unnatural) to 6 (very natural). The results
are summarised in Table 2, where the success rate
difference (3%) between the in-domain policy and
the transferred policy is statistically insignificant,
and surprisingly, the users on average regard the
transferred policy as slightly more natural than the
in-domain policy.

5 Conclusion

This paper proposed a domain-independent ontol-
ogy parameterision framework to enable domain-
transfer of optimised dialogue policies. Exper-
imental results show that when transferred to a
new domain, dialogue policies trained based on
the DIP representations can achieve very close
performance to those policies optimised using in-
domain dialogues. Bridging the (very small) per-
formance gap here should also be simple, if one
takes the transferred policy as the prior and con-
ducts domain-adaptation similar to (Gašić et al.,
2015). This will be addressed in our future work.

415



Acknowledgements

The authors would like to thank David Vandyke,
Milica Gašić and Steve Young for providing the
BUDS system and the simulator, as well as for
their help in setting up the crowdsourcing exper-
iments.

References
Milica Gašić and Steve Young. 2014. Gaussian

processes for POMDP-based dialogue manager op-
timization. IEEE/ACM Transactions on Audio,
Speech and Language Processing, 22(1):28–40.

Milica Gašić, Matthew Henderson, Blaise Thomson,
Pirros Tsiakoulis, and Steve J. Young. 2012. Pol-
icy optimisation of POMDP-based dialogue systems
without state space compression. In SLT 2012.

Milica Gašić, Catherine Breslin, Matthew Hender-
son, Dongho Kim, Martin Szummer, Blaise Thom-
son, Pirros Tsiakoulis, and Steve Young. 2013.
POMDP-based dialogue manager adaptation to ex-
tended domains. In SIGDIAL 2013.

Milica Gašić, Dongho Kim, Pirros Tsiakoulis, and
Steve Young. 2015. Distributed dialogue policies
for multi-domain statistical dialogue management.
In ICASSP 2015.

Matthew Henderson, Blaise Thomson, and Steve J.
Young. 2014. Robust dialog state tracking using
delexicalised recurrent neural networks and unsu-
pervised adaptation. In SLT 2014.

Oliver Lemon, Kallirroi Georgila, and James Hender-
son. 2006. Evaluating effectiveness and portabil-
ity of reinforcement learned dialogue strategies with
real users: the TALK TownInfo evaluation. In SLT
2006.

Qi Li, Gökhan Tür, Dilek Hakkani-Tür, Xiang Li,
Tim Paek, Asela Gunawardana, and Chris Quirk.
2014. Distributed open-domain conversational un-
derstanding framework with domain independent
extractors. In SLT 2014.

Nikola Mrkšić, Diarmuid Ó Séaghdha, Blaise Thom-
son, Milica Gašić, David Vandyke Pei-Hao Su,
Tsung-Hsien Wen, and Steve Young. 2015. Multi-
domain dialog state tracking using recurrent neural
networks. In ACL-IJCNLP 2015.

Jost Schatzmann, Blaise Thomson, Karl Weilhammer,
Hui Ye, and Steve Young. 2007. Agenda-based
user simulation for bootstrapping a POMDP dia-
logue system. In HLT-NAACL 2007; Companion
Volume, Short Papers.

Kai Sun, Lu Chen, Su Zhu, and Kai Yu. 2014. A gener-
alized rule based tracker for dialogue state tracking.
In SLT 2014.

Blaise Thomson and Steve Young. 2010. Bayesian
update of dialogue state: A POMDP framework for
spoken dialogue systems. Computer Speech and
Language, 24(4):562–588.

Zhuoran Wang and Oliver Lemon. 2013. A simple
and generic belief tracking mechanism for the Dia-
log State Tracking Challenge: On the believability
of observed information. In SIGDIAL 2013.

Jason D. Williams and Steve Young. 2005. Scaling up
POMDPs for dialog management: The “Summary
POMDP” method. In ASRU 2005.

Steve Young, Milica Gašić, Blaise Thomson, and Ja-
son D. Williams. 2013. POMDP-based statistical
spoken dialogue systems: a review. Proceedings of
the IEEE, PP(99):1–20.

A Ontology Definitions for the Example
Domains

Slot #Values Info. Rqst.
food 91 yes yes
area 5 yes yes

pricerange 3 yes yes
name 111 yes yes
phone – no yes

R
es

ta
ur

an
t

postcode – no yes
signature – no yes

description – no yes
family 5 yes no

purpose 2 yes yes
pricerange 3 yes yes

weightrange 3 yes yes
batteryrating 3 yes yes

L
ap

to
p

driverange 3 yes yes
name 123 yes no
price – no yes

weight – no yes
hard drive – no yes
dimension – no yes

Table A.1: Ontologies for the restaurant search
and laptop sale domains. “Info.” denotes in-
formable slots, for which user can provide values;
“Rqst.” denotes requestable slots, for which user
can ask for information.

416


