



















































An Empirical Study of Differences between Conversion Schemes and Annotation Guidelines


Proceedings of the Second International Conference on Dependency Linguistics (DepLing 2013), pages 298–307,
Prague, August 27–30, 2013. c© 2013 Charles University in Prague, Matfyzpress, Prague, Czech Republic

An empirical study of differences between conversion schemes and
annotation guidelines∗

Anders Søgaard
Center for Language Technology

University of Copenhagen
DK-2300 Copenhagen S
soegaard@hum.ku.dk

Abstract

We establish quantitative methods for
comparing and estimating the quality
of dependency annotations or conversion
schemes. We use generalized tree-edit dis-
tance to measure divergence between an-
notations and propose theoretical learn-
ability, derivational perplexity and down-
stream performance for evaluation. We
present systematic experiments with tree-
to-dependency conversions of the Penn-
III treebank, as well as observations from
experiments using treebanks from multi-
ple languages. Our most important ob-
servations are: (a) parser bias makes most
parsers insensitive to non-local differences
between annotations, but (b) choice of an-
notation nevertheless has significant im-
pact on most downstream applications,
and (c) while learnability does not corre-
late with downstream performance, learn-
able annotations will lead to more robust
performance across domains.

1 Introduction

Syntactic structures in dependency parsing are
moving targets. While intrinsic evaluations often
give the impression that syntactic structures are
carved in stone, in reality we have little evidence
in favor of the structures we posit. While most lin-
guists agree on how to analyze core syntactic phe-
nomena, there is widespread disagreement about
a number of cases. Do auxiliary verbs head main
verbs? Do prepositions head their nominal com-

∗Section 5 is joint work with Jakob Elming, Anders Jo-
hannsen, Sigrid Klerke, Emanuele Lapponi and Hector Mar-
tinez, published at NAACL 2013.

plements? And how should we analyze punctua-
tion?

Many dependency treebanks are created by au-
tomatic conversion from pre-existing constituency
treebanks. Since there exist linguistic phenomena
whose analyses linguist do not agree on, it comes
as no surprise that different conversion schemes
have been proposed over the years (Collins, 1999;
Yamada and Matsumoto, 2003; Johansson and
Nugues, 2007). The output of these schemes differ
considerably in their choices concerning head sta-
tus and dependency relation inventories (Schwartz
et al., 2012; Johansson, 2013).

The number of languages for which we have
several dependency treebanks, is limited (Johans-
son, 2013), but the availability of different tree-to-
dependency conversion schemes raises the ques-
tion of what scheme is better? So does the ex-
istence of different parsers relying on different
linguistic formalisms, but whose output can be
mapped to dependencies (Tsarfaty et al., 2012).
But is the question of which is better really a
meaningful question? Better at what?

Schwartz et al. (2012) propose to evaluate con-
version schemes in terms of learnability. We argue
that while learnability is relevant to assess the ro-
bustness of dependency schemes, the most impor-
tant parameter when choosing conversion schemes
in practice is down-stream performance. We cite
Elming et al. (2013), who show that down-stream
performance is very sensitive to choice of conver-
sion scheme. We also suggest that derivational
perplexity (Søgaard and Haulrich, 2010) is a less
biased measure of robustness than learnability –
at least the way it is measured in Schwartz et
al. (2012).

The paper presents (a) an empirical analysis
of distance between conversion schemes, (b) an

298



Clear cases Difficult cases
Head Dependent ? ?
Verb Subject Auxiliary Main verb
Verb Object Complementizer Verb
Noun Attribute Coordinator Conjuncts
Verb Adverbial Preposition Nominal

Punctuation

Figure 1: Clear and difficult cases in dependency
annotation.

analysis of the theoretical learnability of conver-
sion schemes, (c) a complexity analysis of conver-
sion schemes in terms of derivational perplexity,
and (d) empirical evaluations of the downstream
usefulness of conversion schemes. Section 2
introduces a few common conversion schemes
and their linguistic differences. In our empiri-
cal analyses we will focus on standard conver-
sions of the Wall Street Journal section of the
Penn-III treebank of English (Marcus et al., 1993).
Section 3 introduces three distance metrics de-
fined over pairs of output dependency structures.
The results presented in this section suggests that
parser bias cancels out many of the differences
between conversion schemes. Section 4 dis-
cusses the learnability and derivational perplexity
of tree-to-dependency conversion schemes. Sec-
tion 5 presents a series of experiments, evaluat-
ing the downstream performance of conversion
schemes in negation scope resolution, sentence
compression, statistical machine translation, se-
mantic role labeling and author perspective clas-
sification. Section 6 concludes with a discussion
of the analyses presented in the previous sections.
In our experiments we will use the publicly avail-
able MATE parser (Bohnet, 2010). Obviously the
downstream performance of a conversion scheme
depends on the parsing model chosen and how
syntactic features are incorporated in the down-
stream task, but we do not vary parser or syntactic
feature representations in our experiments.

2 Tree-to-dependency conversion
schemes

Annotation guidelines used in modern depen-
dency treebanks and tree-to-dependency conver-
sion schemes for converting constituent-based
treebanks into dependency treebanks are typically
based on a specific dependency grammar theory,

such as the Prague School’s Functional Generative
Description, Meaning-Text Theory, or Hudson’s
Word Grammar. In practice most parsers con-
strain dependency structures to be tree-like struc-
tures such that each word has a single syntac-
tic head, limiting diversity between annotation a
bit; but while many dependency treebanks taking
this format agree on how to analyze many syn-
tactic constructions, there are still many construc-
tions these treebanks analyze differently. See Fig-
ure 1 for a standard overview of clear and more
difficult cases.

The difficult cases in Figure 1 are difficult for
the following reason. In the easy cases mor-
phosyntactic and semantic evidence cohere. Verbs
govern subjects morpho-syntactically and seem
semantically more important. In the difficult
cases, however, morpho-syntactic evidence is in
conflict with the semantic evidence. While aux-
iliary verbs have the same distribution as finite
verbs in head position and share morpho-syntactic
properties with them, and govern the infinite main
verbs, main verbs seem semantically superior, ex-
pressing the main predicate. There may be dis-
tributional evidence that complementizers head
verbs syntactically, but the verbs seem more im-
portant from a semantic point of view. Some au-
thors have distinguished between the notion of
functional (distributional) and substantive depen-
dency heads (Ivanova et al., 2012).

Tree-to-dependency conversion schemes
used to convert constituent-based treebanks
into dependency-based ones also take different
stands on the difficult cases. In this paper we
consider four different conversion schemes:
the Yamada-Matsumoto conversion scheme
(Yamada) (Yamada and Matsumoto, 2003), the
CoNLL (2007) format, the Stanford conversion
scheme used in the English Web Treebank (Petrov
and McDonald, 2012), and the LTH conversion
scheme (Johansson and Nugues, 2007). The
Yamada scheme can be replicated by running
penn2malt.jar available at

http://w3.msi.vxu.se/∼nivre/
research/Penn2Malt.html

We used Malt dependency labels (see online docu-
mentation). The Yamada scheme is an elaboration
of the Collins scheme (Collins, 1999), which is not
included in our experiments. The Stanford conver-
sion scheme can be replicated using the Stanford
converter available at

299



FORM1 FORM2 Yamada CoNLL Stanford LTH
Auxiliary Main verb 1 1 2 2
Complementizer Verb 1 2 2 2
Coordinator Conjuncts 2 1 2 2
Preposition Nominal 1 1 1 2

Figure 2: Head decisions in conversions. Note: Yamada also differ from CoNLL in proper names.

LTH Stanford Yamada
CoNLL 91.1% 89.7% 92.7%
LTH - 89.7% 89.6%
Stanford - - 90.1%

Table 1: Unlabeled TED accuracies between con-
version schemes

LTH Stanford Yamada
CoNLL 6.05 5.70 5.14
LTH - 6.85 7.06
Stanford - - 6.24

Table 2: L1-distances between conversion
schemes

http://nlp.stanford.edu/software/

stanford-dependencies.shtml

The CoNLL 2007 conversion scheme can be ob-
tained by running pennconverter.jar available at

http://nlp.cs.lth.se/software/

treebank converter/

with the ’conll07’ flag set. The LTH conver-
sion scheme can be obtained by running penncon-
verter.jar with the ’oldLTH’ flag set.

We list the differences in Figure 2. It is clear
from this list that the LTH scheme uses substan-
tive heads more often than the other schemes. This
makes sense as it was designed for downstream
semantic role labeling (Johansson and Nugues,
2007). Somewhat surprisingly the scheme does
not always lead to better semantic role labeling
performance when relying on predicted syntactic
parses, however (see Results).

3 Distance between schemes

We use three parse evaluation metrics to estimate
distances between tree-to-dependency schemes.

The NED scores between pairs of gold anno-
tated data using different conversion schemes give

an empirical estimate of the coarser differences
between the schemes. The NED scores between
the Yamada scheme and the CoNLL and LTH
schemes is 91.9-92.0%, for example, while the
NED score between CoNLL and LTH is 93.9%.
Interestingly, the NED between pairs of MATE
outputs on our SMT tuning section (see Section
5.3) using different conversion schemes is 100% in
all cases. This seems to indicate that differences
in down-stream performance (see Table 4) should
not be found in major theoretical differences, but
rather small differences such as edge flippings and
label granularity.

The UA scores (unlabeled attachment) punish
edge flippings, and the fact that we observe low
UA scores between conversion schemes support
the above picture and also indicate that the differ-
ences are quite substantial. The overlap as mea-
sured by UA score between the Yamada and the
CoNLL scheme is 78.9%, for example. These two
schemes agree on the syntactic heads of about 4/5
words. The UA score between Yamada and LTH
is 63.1%. Again we see that differences between
gold annotated datasets using different conversion
schemes are bigger than when comparing output
pairs. It seems that parser bias is canceling out
differences between conversion schemes.

We also report unlabeled TED scores (Tsar-
faty et al., 2012) between output trees. TED is a
three-step distance computation that first abstracts
away from the directionality of head-dependent re-
lations. In the second step, we separate the com-
mon and consistent parts of the two output trees,
and in the third step we compute the tree-edit oper-
ations necessary to translate between the inconsis-
tent subtrees. We use the SMT tune section again.
The unlabeled TED accuracy between CoNLL and
Yamada is 92.7%, and the L1-distance is only
5.14. See tabels 1 and 2 for more results. The L1-
distances, together with the other metrics, suggest
that CoNLL is more similar to the other conver-
sion schemes than any other pair of schemes. See
Figure 3.

300



bl Yamada CoNLL Stanford LTH
DEPRELS - 12 21 47 41
PTB-23 (LAS) - 88.99 88.52 81.36∗ 87.52
PTB-23 (UAS) - 90.21 90.12 84.22∗ 90.29
Neg: scope F1 - 81.27 80.43 78.70 79.57
Neg: event F1 - 76.19 72.90 73.15 76.24
Neg: full negation F1 - 67.94 63.24 61.60 64.31
SentComp F1 68.47 72.07 64.29 71.56 71.56
SMT-dev-Meteor 35.80 36.06 36.06 36.16 36.08
SMT-test-Meteor 37.25 37.48 37.50 37.58 37.51
SMT-dev-BLEU 13.66 14.14 14.09 14.04 14.06
SMT-test-BLEU 14.67 15.04 15.04 14.96 15.11
SRL-22-gold - 81.35 83.22 84.72 84.01
SRL-23-gold - 79.09 80.85 80.39 82.01
SRL-22-pred - 74.41 76.22 78.29 66.32
SRL-23-pred - 73.42 74.34 75.80 64.06
bitterlemons.org 96.08 97.06 95.58 96.08 96.57

Table 4: Results. ∗: Low parsing results on PTB-23 using Stanford are explained by changes between
the PTB-III and the Ontonotes 4.0 release of the English Treebank.

Yamada Stanford

CoNLL

LTH

Figure 3: L1-distances between conversion
schemes

gold smt-tune
w-ppl d-ppl w-ppl d-ppl

CoNLL 243.3 545.6 273.5 430.4
LTH 243.3 536.2 273.5 428.0
Stanford 243.3 541.9 273.5 425.4
Yamada 243.3 520.8 273.5 426.5

Table 3: Derivational perplexity of converted tree-
banks

4 Learnability and derivational
perplexity

Schwartz et al. (2012) study the learnability of
different conversion schemes, by relating choises
concerning head status to parser performance.
How does making prepositions head their com-
plements affect parser performance, for example?
The first two lines in Table 4 suggest, in line with
Schwartz et al. (2012), that Yamada is more learn-
able than the other three schemes.

The derivational perplexity of a treebank T
(Søgaard and Haulrich, 2010) is defined as the per-
plexity (per word) of the derivation language of
f(T ), where f(T ) is the canonical derivation or-
ders of the dependency trees in T: The derivation
language of f(T ) is the set of strings σ : w1...wn
such that for any wi, wj wi ≺ wj in dependency
structure d if wi was attached to d in f(T ) prior
to the attachment of wj , according to their canoni-
cal derivation. Canonical derivations are also used
to train transition-based dependency parsers, and
we use the arc-eager algorithm in our experiments
below (Nivre et al., 2007). We use a trigram lan-
guage model with Knesser-Ney smoothing, repli-
cating the setup in Søgaard and Haulrich (2010).

The results in Table 3 seem to indicate that the
Yamada scheme is more learnable than the more
recent ones, but again the differences seemed to
be cancelled out by parser bias, and differences
in derivational perplexity of the output (again
using the tuning section of the SMT data) become
insignificant. The reason for the absolute drop
in numbers, as well as the drop in differences, is

301



probably the shorter sentence length and more
straight-forward syntax often associated with spo-
ken language such as the parliament discussions
in the Europarl data.

Learnability vs. derivational perplex-
ity. Søgaard and Haulrich (2010) show that
derivational perplexity correlates well with per-
formance in multi-lingual parsing (using data from
the CoNLL-X and CoNLL 2007 shared tasks),
and therefore also with learnability (Schwartz et
al., 2012). We think derivational perplexity is a
better measure of performance robustness for two
reasons: because it is a parser-independent metric,
(i) derivational perplexities are not influenced by
regularization, and (ii) derivational perplexities
are not influenced by different parser biases.
Changing a parameter in a tree-to-dependency
conversion scheme may affect parser perfor-
mance for several reasons: It is well known that
adding features that never fire sometimes leads
to improved performance with state-of-the-art
parsers such as the MaltParser (Nivre et al., 2007),
because regularization is sensitive to the total
number of features, and it is not always clear
whether a choice of head status leads to improved
performance because the head choice is more
learnable, or because it has an effect on regular-
ization. Finally, annotation interacts with parser
bias. Some choices of head status may be easy
to learn for transition-based dependency parsers,
but comparatively harder for graph-based ones.
See McDonald and Nivre (2007) for an analysis
of the biases of transition-based and graph-based
dependency parsers. Since derivational perplexity
is parser-independent we are not sensitive to
regularization or parser bias, only to the choice of
canonical derivation scheme.

5 Downstream performance

Dependency parsing has proven useful for a wide
range of NLP applications, including statistical
machine translation (Galley and Manning, 2009;
Xu et al., 2009; Elming and Haulrich, 2011)
and sentiment analysis (Joshi and Penstein-Rose,
2009; Johansson and Moschitti, 2010). Below
we introduce five NLP applications where depen-
dency parsing has been succesfully applied: nega-
tion resolution, semantic role labeling, statisti-
cal machine translation, sentence compression and
perspective classification. We will then report on

evaluations of the downstream effects of the four
conversion schemes in these five applications, first
published in Elming et al. (2013).

In the five applications we use syntactic fea-
tures in slightly different ways. While our statisti-
cal machine translation and sentence compression
systems use dependency relations as additional in-
formation about words and on a par with POS,
our negation resolution system uses dependency
paths, conditioning decisions on both dependency
arcs and labels. In perspective classification, we
use dependency triples (e.g. SUBJ(John, snore)) as
features, while the semantic role labeling system
conditions on a lot of information, including the
word form of the head, the dependent and the argu-
ment candidates, the concatenation of the depen-
dency labels of the predicate, and the labeled de-
pendency relations between predicate and its head,
its arguments, dependents or siblings.

5.1 Negation resolution

Negation resolution (NR) is the task of finding
negation cues, e.g. the word not, and determin-
ing their scope, i.e. the tokens they affect. NR
has recently seen considerable interest in the NLP
community (Morante and Sporleder, 2012; Velldal
et al., 2012) and was the topic of the 2012 *SEM
shared task (Morante and Blanco, 2012).

The data set used in this work, the Conan Doyle
corpus (CD),1 was released in conjunction with
the *SEM shared task. The annotations in CD
extend on cues and scopes by introducing an-
notations for in-scope events that are negated in
factual contexts. The following is an example
from the corpus showing the annotations for cues
(bold), scopes (underlined) and negated events
(italicized):

(1) Since we have been so
unfortunate as to miss him [. . . ]

CD-style scopes can be discontinuous and over-
lapping. Events are a portion of the scope that is
semantically negated, with its truth value reversed
by the negation cue.

The NR system used in this work (Lapponi et
al., 2012), one of the best performing systems in
the *SEM shared task, is a CRF model for scope
resolution that relies heavily on features extracted
from dependency graphs. The feature model con-
tains token distance, direction, n-grams of word

1http://www.clips.ua.ac.be/sem2012-st-neg/data.html

302



REFERENCE: Zum Glück kam ich beim Strassenbahnfahren an die richtige Stelle .
SOURCE: Luckily , on the way to the tram , I found the right place .
Yamada: Glücklicherweise hat auf dem Weg zur S-Bahn , stellte ich fest , dass der richtige Ort .
CoNLL: Glücklicherweise hat auf dem Weg zur S-Bahn , stellte ich fest , dass der richtige Ort .
Stanford: Zum Glück fand ich auf dem Weg zur S-Bahn , am richtigen Platz .
LTH: Zum Glück fand ich auf dem Weg zur S-Bahn , am richtigen Platz .
BASELINE: Zum Glück hat auf dem Weg zur S-Bahn , ich fand den richtigen Platz .

Figure 4: Examples of SMT output.

ORIGINAL: * 68000 sweden ab of uppsala , sweden , introduced the teleserve , an integrated answering
machine and voice-message handler that links a macintosh to touch-tone phones .

BASELINE: 68000 sweden ab introduced the teleserve an integrated answering
machine and voice-message handler .

Yamada 68000 sweden ab introduced the teleserve integrated answering
machine and voice-message handler .

CoNLL 68000 sweden ab sweden introduced the teleserve integrated answering
machine and voice-message handler .

Stanford 68000 sweden ab introduced the teleserve integrated answering
machine and voice-message handler .

LTH 68000 sweden ab introduced the teleserve an integrated answering
machine and voice-message handler .

HUMAN: 68000 sweden ab introduced the teleserve integrated answering
machine and voice-message handler .

Figure 5: Examples of sentence compression output.

Syntactic

constituent
dependency relation
parent head POS
grand parent head POS
word form+dependency relation
POS+dependency relation

Cue-dependent

directed dependency distance
bidirectional dependency distance
dependency path
lexicalized dependency path

Figure 6: Features used to train the conditional
random field models

forms, lemmas, POS and combinations thereof, as
well as the syntactic features presented in Figure 6.
The results in our experiments are obtained from
configurations that differ only in terms of tree-to-
dependency conversions, and are trained on the
training set and tested on the development set of
CD. Since the negation cue classification compo-
nent of the system does not rely on dependency
features at all, the models are tested using gold
cues.

Table 4 shows F1 scores for scopes, events and
full negations, where a true positive correctly as-
signs both scope tokens and events to the rightful
cue. The scores are produced using the evaluation
script provided by the *SEM organizers.

5.2 Semantic role labeling

Semantic role labeling (SRL) is the attempt to de-
termine semantic predicates in running text and la-

bel their arguments with semantic roles. In our
experiments we have reproduced the second best-
performing system in the CoNLL 2008 shared task
in syntactic and semantic parsing (Johansson and
Nugues, 2008).2

The English training data for the CoNLL 2008
shared task were obtained from PropBank and
NomBank. For licensing reasons, we used
OntoNotes 4.0, which includes PropBank, but not
NomBank. This means that our system is only
trained to classify verbal predicates. We used
the Clearparser conversion tool3 to convert the
OntoNotes 4.0 and subsequently supplied syntac-
tic dependency trees using our different conver-
sion schemes. We rely on gold standard argument
identification and focus solely on the performance
metric semantic labeled F1.

5.3 Statistical machine translation

The effect of the different conversion schemes
was also evaluated on SMT. We used the reorder-
ing by parsing framework described by Elming
and Haulrich (2011). This approach integrates
a syntactically informed reordering model into a
phrase-based SMT system. The model learns to
predict the word order of the translation based
on source sentence information such as syntactic

2http://nlp.cs.lth.se/software/semantic parsing: propbank
nombank frames

3http://code.google.com/p/clearparser/

303



dependency relations. Syntax-informed SMT is
known to be useful for translating between lan-
guages with different word orders (Galley and
Manning, 2009; Xu et al., 2009), e.g. English and
German.

The baseline SMT system is created as de-
scribed in the guidelines from the original shared
task.4 Only modifications are that we use truecas-
ing instead of lowercasing and recasing, and allow
training sentences of up to 80 words. We used data
from the English-German restricted task: ∼3M
parallel words of news, ∼46M parallel words of
Europarl, and ∼309M words of monolingual Eu-
roparl and news. We use newstest2008 for tuning,
newstest2009 for development, and newstest2010
for testing. Distortion limit was set to 10, which
is also where the baseline system performed best.
The phrase table and the lexical reordering model
is trained on the union of all parallel data with a
max phrase length of 7, and the 5-gram language
model is trained on the entire monolingual data
set.

We test four different experimental systems that
only differ with the baseline in the addition of
a syntactically informed reordering model. The
baseline system was one of the tied best perform-
ing system in the WMT 2011 shared task on this
dataset. The four experimental systems have re-
ordering models that are trained on the first 25,000
sentences of the parallel news data that have been
parsed with each of the tree-to-dependency con-
version schemes. The reordering models condition
reordering on the word forms, POS, and syntactic
dependency relations of the words to be reordered,
as described in Elming and Haulrich (2011). The
paper shows that while reordering by parsing leads
to significant improvements in standard metrics
such as BLEU (Papineni et al., 2002) and ME-
TEOR (Lavie and Agarwal, 2007), improvements
are more spelled out with human judgements. All
SMT results reported below are averages based on
5 MERT runs following Clark et al. (2011).

5.4 Sentence compression

Sentence compression is a restricted form of sen-
tence simplification with numerous usages, in-
cluding text simplification, summarization and
recognizing textual entailment. The most com-
monly used dataset in the literature is the Ziff-

4 http://www.statmt.org/wmt11/translation-task.html

Davis corpus.5 A widely used baseline for sen-
tence compression experiments is the two mod-
els introduced in Knight and Marcu (2002): the
noisy-channel model and the decision tree-based
model. Both are tree-based methods that find the
most likely compressed syntactic tree and outputs
the yield of this tree. McDonald et al. (2006)
instead use syntactic features to directly find the
most likely compressed sentence.

Here we learn a discriminative HMM model
(Collins, 2002) of sentence compression using
MIRA (Crammer and Singer, 2003), compara-
ble to previously explored models of noun phrase
chunking. Our model is thus neither tree-based
nor sentence-based. Instead we think of sentence
compression as a sequence labeling problem. We
compare a model informed by word forms and
predicted POS with models also informed by pre-
dicted dependency labels. The baseline feature
model conditions emission probabilities on word
forms and POS using a ±2 window and combi-
nations thereoff. The augmented syntactic feature
model simply adds dependency labels within the
same window.

5.5 Perspective classification

Finally, we include a document classification
dataset from Lin and Hauptmann (2006).6 The
dataset consists of blog posts posted at bitter-
lemons.org by Israelis and Palestinians. The bit-
terlemons.org website is set up to ”contribute to
mutual understanding through the open exchange
of ideas.” In the dataset, each blog post is labeled
as either Israeli or Palestinian. Our baseline model
is just a standard bag-of-words model, and the sys-
tem adds dependency triplets to the bag-of-words
model in a way similar to Joshi and Penstein-
Rose (2009). We do not remove stop words, since
perspective classification is similar to authorship
attribution, where stop words are known to be in-
formative. We evaluate performance doing cross-
validation over the official training data, setting
the parameters of our learning algorithm for each
fold doing cross-validation over the actual train-
ing data. We used soft-margin support vector ma-
chine learning (Cortes and Vapnik, 1995), tuning
the kernel (linear or polynomial with degree 3) and
C = {0.1, 1, 5, 10}.

5LDC Catalog No.: LDC93T3A.
6https://sites.google.com/site/weihaolinatcmu/data

304



5.6 Results

Our results are presented in Table 4. The pars-
ing results are obtained relying on predicted POS
rather than, as often done in the dependency pars-
ing literature, relying on gold-standard POS. Note
that they comply with the result in Schwartz et
al. (2012) that Yamada annotation is more easily
learnable.

The negation resolution results are signifi-
cantly better using syntactic features in Yamada
annotation. It is not surprising that a syntactically
oriented conversion scheme performs well in this
task.

The case-sensitive BLEU evaluation of the
SMT systems indicates that choice of conversion
scheme has no significant impact on overall per-
formance. The difference to the baseline system
is significant (p < 0.01), showing that the re-
ordering model leads to improvement using any
of the schemes. Differences between schemes are
insignificant. The reason probably is that long-
distance differences between the schemes are can-
celled out by parser bias. However, the conversion
schemes lead to very different translations. This
can be seen, for example, by the fact that the (nor-
malized) string edit distance between translations
of different syntactically informed SMT systems
is 12% higher than within each system (across dif-
ferent MERT optimizations).

The reordering approach puts a lot of weight
on the syntactic dependency relations. As a con-
sequence, the number of relation types used in
the conversion schemes proves important. Con-
sider the example in Figure 4. German requires
the verb in second position (V2), which is picked
up by the Stanford and LTH systems. Interest-
ingly, the four schemes produce virtually identi-
cal structures for the source sentence, but they dif-
fer in their labeling. Where CoNLL and Yamada
use the same relation for the first two constituents
(ADV and vMOD, respectively), Stanford and
LTH distinguish between them (ADVMOD/PREP
and ADV/LOC). This distinction may be what en-
ables learning V2 translations, since the model
may learn to move the verb after the sentence ad-
verbial. In the other schemes, sentence adverbials
are not distinguished from locational adverbials.
Generally, Stanford and LTH have more than twice
as many relation types as the other schemes.

The schemes Stanford and LTH lead to bet-
ter SRL performance than CoNLL and Yamada

when relying on gold-standard syntactic depen-
dency trees. This supports the claims put for-
ward in Johansson and Nugues (2007). These an-
notations also happen to use a larger set of de-
pendency labels, however, and syntactic structures
may be harder to reconstruct, as reflected by la-
beled attachment scores (LAS) in syntactic pars-
ing. The biggest drop in SRL performance going
from gold-standard to predicted syntactic trees is
clearly for the LTH scheme, at an average 17.8%
absolute loss (Yamada 5.8%; CoNLL 6.8%; Stan-
ford 5.5%; LTH 17.8%).

The Stanford scheme resembles LTH in most
respects, but in preposition-noun dependencies it
marks the preposition as the head rather than the
noun. This is an important difference for SRL, be-
cause semantic arguments are often nouns embed-
ded in prepositional phrases, like agents in passive
constructions. It may also be that the difference
in performance is simply explained by the syntac-
tic analysis of prepositional phrases being easier
to reconstruct.

The sentence compression results are generally
much better than the models proposed in Knight
and Marcu (2002). Their noisy channel model ob-
tains an F1 compression score of 14.58%, whereas
the decision tree-based model obtains an F1 com-
pression score of 31.71%. While F1 scores should
be complemented by human judgements, as there
are typically many good sentence compressions of
any source sentence, we believe that error reduc-
tions of more than 50% indicate that the models
used here (though previously unexplored in the lit-
erature) are fully competitive with state-of-the-art
models.

We also see that the models using syntactic
features perform better than our baseline model,
except for the model using CoNLL dependency
annotation. This may be surprising to some,
since distributional information is often consid-
ered important in sentence compression (Knight
and Marcu, 2002). Some output examples are
presented in Figure 5. Unsurprisingly, it is seen
that the baseline model produces grammatically
incorrect output, and that most of our syntactic
models correct the error leading to ungrammat-
icality. The model using Stanford annotation is
an exception. We also see that CoNLL intro-
duces another error. We believe that this is due
to the way the CoNLL tree-to-dependency conver-
sion scheme handles coordination. While the word

305



ADV AMOD CC COORDDEP EXP GAP IOBJ LGS NMOD OBJ P PMOD PRD PRN PRT ROOT SBJ VC VMOD
0.00

0.05

0.10

0.15

0.20

0.25

0.30

0.35

0.40

La
b
e
ls

srl
neg

Figure 7: Distributions of dependency labels in the
Yamada-Matsumoto scheme

Sweden is not coordinated, it occurs in a context,
surrounded by commas, that is very similar to co-
ordinated items.

In perspective classification we see that syn-
tactic features based on Yamada and LTH annota-
tions lead to improvements, with Yamada leading
to slightly better results than LTH. The fact that a
syntactically oriented conversion scheme leads to
the best results may reflect that perspective clas-
sification, like authorship attribution, is less about
content than stylistics.

While LTH seems to lead to the overall best
results, we stress the fact that the five tasks con-
sidered here are incommensurable. What is more
interesting is that, task to task, results are so
different. The semantically oriented conversion
schemes, Stanford and LTH, lead to the best re-
sults in SRL, but with a significant drop for LTH
when relying on predicted parses, while the Ya-
mada scheme is competitive in the other four tasks.
This may be because distributional information is
more important in these tasks than in SRL.

The distribution of dependency labels seems
relatively stable across applications, but differ-
ences in data may of course also affect the
usefulness of different annotations. Note that
CoNLL leads to very good results for negation res-
olution, but bad results for SRL. See Figure 7 for
the distribution of labels in the CoNLL conversion
scheme on the SRL and negation scope resolu-
tion data. Many differences relate to differences
in sentence length. The negation resolution data
is literary text with shorter sentences, which there-
fore uses more punctuation and has more root de-
pendencies than newspaper articles. On the other
hand we do see very few predicate dependencies
in the SRL data. This may affect down-stream re-
sults when classifying verbal predicates in SRL.

We also note that the number of dependency la-
bels have less impact on results in general than
we would have expected. The number of depen-
dency labels and the lack of support for some of
them may explain the drop with predicted syntac-
tic parses in our SRL results, but generally we ob-
tain our best results with Yamada and LTH anno-
tations, which have 12 and 41 dependency labels,
respectively.

6 Discussion and conclusion

In our experiments we made several observations.
Available tree-to-dependency conversion schemes
are very different. On the other hand we saw that
many of the non-local differences between conver-
sion schemes are not learned by state-of-the-art
parsers, making parser output across conversion
schemes less different from gold annotations. This
suggests that only more local differences are im-
portant for downstream performance, which may
also explain the small differences observed in our
SMT experiments.

While the CoNLL scheme is very learnable
(Schwartz et al., 2012), second to Yamada, down-
stream performance suggest that it is a subopti-
mal conversion scheme. The Yamada scheme is
both very learnable (1st), leads to very good down-
stream performance (1st or 2nd in 4/5 downstream
applications), and it has low derivational perplex-
ity. We have argued that this is a better metric for
performance robustness than learnability.

Note that our results extend beyond dependency
parsing. Ivanova et al. (2012) show that con-
verted dependency structures bear similarities to
both Stanford dependencies and DELPH-IN syn-
tactic derivation structures, but they are not ex-
plicit about what conversion scheme was used.

In future work we would like to combine the
different methodologies discussed here to be able
to learn robust annotation for given end applica-
tions that optimize end performance.

References

Bernd Bohnet. 2010. Top accuracy and fast depen-
dency parsing is not a contradiction. In COLING.

Jonathan H. Clark, Chris Dyer, Alon Lavie, and
Noah A. Smith. 2011. Better hypothesis testing for
statistical machine translation: controlling for opti-
mizer instability. In ACL.

Mike Collins. 1999. Head-driven statistical models

306



for natural language parsing. Ph.D. thesis, Univer-
sity of Pennsylvania.

Michael Collins. 2002. Discriminative training meth-
ods for Hidden Markov Models. In EMNLP.

Corinna Cortes and Vladimir Vapnik. 1995. Support-
vector networks. Machine Learning, 20(3):273–
297.

Koby Crammer and Yoram Singer. 2003. Ultraconser-
vative algorithms for multiclass problems. In JMLR.

Jakob Elming and Martin Haulrich. 2011. Reordering
by parsing. In Proceedings of International Work-
shop on Using Linguistic Information for Hybrid
Machine Translation (LIHMT-2011).

Jakob Elming, Anders Johannsen, Sigrid Klerke,
Emanuele Lapponi, Hector Martinez Alonso, and
Anders Søgaard. 2013. Down-stream effects of
tree-to-dependency conversions. In NAACL.

Michel Galley and Christopher Manning. 2009.
Quadratic-time dependency parsing for machine
translation. In ACL.

Angelina Ivanova, Stephan Oepen, Lilja Øvrelid, and
Dan Flickinger. 2012. Who did what to whom? a
contrastive study of syntactico-semantic dependen-
cies. In LAW.

Richard Johansson and Alessandro Moschitti. 2010.
Syntactic and semantic structure for opinion expres-
sion detection. In CoNLL.

Richard Johansson and Pierre Nugues. 2007. Ex-
tended constituent-to-dependency conversion for
English. In NODALIDA.

Richard Johansson and Pierre Nugues. 2008.
Dependency-based syntactic-semantic analysis with
propbank and nombank. In CoNLL.

Richard Johansson. 2013. Training parsers on incom-
patible treebanks. In NAACL.

Mahesh Joshi and Carolyn Penstein-Rose. 2009. Gen-
eralizing dependency features for opinion mining.
In ACL.

Kevin Knight and Daniel Marcu. 2002. Summariza-
tion beyond sentence extraction: a probabilistic ap-
proach to sentence compression. Artificial Intelli-
gence, 139:91–107.

Emanuele Lapponi, Erik Velldal, Lilja Øvrelid, and
Jonathon Read. 2012. UiO2: Sequence-labeling
negation using dependency features. In *SEM.

Alon Lavie and Abhaya Agarwal. 2007. Meteor: an
automatic metric for mt evaluation with high levels
of correlation with human judgments. In WMT.

Wei-Hao Lin and Alexander Hauptmann. 2006. Are
these documents written from different perspec-
tives? In COLING-ACL.

Mitchell Marcus, Mary Marcinkiewicz, and Beatrice
Santorini. 1993. Building a large annotated cor-
pus of English: the Penn Treebank. Computational
Linguistics, 19(2):313–330.

Ryan McDonald and Joakim Nivre. 2007. Character-
izing the errors of data-driven dependency parsers.
In EMNLP-CoNLL.

Ryan McDonald. 2006. Discriminative sentence com-
pression with soft syntactic evidence. In EACL.

Roser Morante and Eduardo Blanco. 2012. *sem 2012
shared task: Resolving the scope and focus of nega-
tion. In *SEM.

Roser Morante and Caroline Sporleder. 2012. Modal-
ity and negation: An introduction to the special is-
sue. Computational linguistics, 38(2):223–260.

Joakim Nivre, Johan Hall, Sandra Kübler, Ryan Mc-
Donald, Jens Nilsson, Sebastian Riedel, and Deniz
Yuret. 2007. The CoNLL 2007 Shared Task on
Dependency Parsing. In Proceedings of the CoNLL
Shared Task Session of EMNLP-CoNLL 2007, pages
915–932, Prague, Czech Republic.

Kishore Papineni, Salim Roukus, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: a method for automatic
evaluation of machine translation. In Proceedings of
the 40th Annual Meeting of the Association for Com-
putational Linguistics, pages 311–318, Philadelphia,
Pennsylvania.

Slav Petrov and Ryan McDonald. 2012. Overview of
the 2012 Shared Task on Parsing the Web. In Notes
of the First Workshop on Syntactic Analysis of Non-
Canonical Language (SANCL).

Roy Schwartz, Omri Abend, and Ari Rappoport. 2012.
Learnability-based syntactic annotation design. In
COLING.

Anders Søgaard and Martin Haulrich. 2010. On the
derivation perplexity of treebanks. In TLT.

Reut Tsarfaty, Joakim Nivre, and Evelina Andersson.
2012. Cross-framework evaluation for statistical
parsing. In EACL.

Erik Velldal, Lilja Øvrelid, Jonathon Read, and
Stephan Oepen. 2012. Speculation and negation:
Rules, rankers, and the role of synta. Computational
linguistics, 38(2):369–410.

Peng Xu, Jaeho Kang, Michael Ringgaard, and Franz
Och. 2009. Using a dependency parser to im-
prove SMT for subject-object-verb languages. In
Proceedings of the North American Chapter of the
Association for Computational Linguistics - Human
Language Technologies (NAACL-HLT) 2009, pages
245–253, Boulder, Colorado.

Hiroyasu Yamada and Yuji Matsumoto. 2003. Statis-
tical dependency analysis with support vector ma-
chines. In Proceedings of the 8th International
Workshop on Parsing Technologies, pages 195–206,
Nancy, France.

307


