








































Exploring Compositional Architectures and Word Vector Representations
for Prepositional Phrase Attachment

Yonatan Belinkov, Tao Lei, Regina Barzilay
Massachusetts Institute of Technology

{belinkov, taolei, regina}@csail.mit.edu

Amir Globerson
The Hebrew University

gamir@cs.huji.ac.il

Abstract

Prepositional phrase (PP) attachment disam-
biguation is a known challenge in syntactic
parsing. The lexical sparsity associated with
PP attachments motivates research in word
representations that can capture pertinent syn-
tactic and semantic features of the word. One
promising solution is to use word vectors in-
duced from large amounts of raw text. How-
ever, state-of-the-art systems that employ such
representations yield modest gains in PP at-
tachment accuracy.

In this paper, we show that word vector repre-
sentations can yield significant PP attachment
performance gains. This is achieved via a
non-linear architecture that is discriminatively
trained to maximize PP attachment accuracy.
The architecture is initialized with word vec-
tors trained from unlabeled data, and relearns
those to maximize attachment accuracy. We
obtain additional performance gains with al-
ternative representations such as dependency-
based word vectors. When tested on both En-
glish and Arabic datasets, our method outper-
forms both a strong SVM classifier and state-
of-the-art parsers. For instance, we achieve
82.6% PP attachment accuracy on Arabic,
while the Turbo and Charniak self-trained
parsers obtain 76.7% and 80.8% respectively.1

1 Introduction

The problem of prepositional phrase (PP) attach-
ment disambiguation has been under investigation

1The code and data for this work are available at http:
//groups.csail.mit.edu/rbg/code/pp.

She ate spaghetti with butter

She ate spaghetti with chopsticks

Figure 1: Two sentences illustrating the importance
of lexicalization in PP attachment decisions. In the
top sentence, the PP with butter attaches to the noun
spaghetti. In the bottom sentence, the PP with chop-
sticks attaches to the verb ate.

for a long time. However, despite at least two
decades of research (Brill and Resnik, 1994; Rat-
naparkhi et al., 1994; Collins and Brooks, 1995), it
remains a major source of errors for state-of-the-art
parsers. For instance, in a comparative evaluation of
parser performance on the Wall Street Journal cor-
pus, Kummerfeld et al. (2012) report that PP attach-
ment is the largest source of errors across all parsers.
Moreover, the extent of improvement over time has
been rather limited, amounting to about 32% error
reduction since the work of (Collins, 1997).

PP attachments are inherently lexicalized and
part-of-speech (POS) tags are not sufficient for their
correct disambiguation. For example, the two sen-
tences in Figure 1 vary by a single noun — butter
vs chopsticks. However, this word determines the
structure of the whole PP attachment. If the corre-

561

Transactions of the Association for Computational Linguistics, vol. 2, pp. 561–572, 2014. Action Editor: Ryan McDonald.
Submission batch: 10/2014; Revision batch 11/2014; Published 12/2014. c©2014 Association for Computational Linguistics.

http://groups.csail.mit.edu/rbg/code/pp
http://groups.csail.mit.edu/rbg/code/pp


sponding word is not observed in the training data, a
standard lexicalized parser does not have sufficient
information to distinguish between these two cases.
In fact, 72% of head-child pairs (e.g. spaghetti-
butter) from the Wall Street Journal test set are un-
seen in training. Not surprisingly, resolving these
ambiguities is challenging for parsers that have re-
stricted access to word semantics.

These considerations have motivated recent ex-
plorations in using distributed word representa-
tions for syntactic parsing (Cirik and Şensoy, 2013;
Socher et al., 2013; Lei et al., 2014). Low-
dimensional word embeddings help unveil seman-
tic similarity between words, thereby alleviating
the data sparsity problem associated with PP at-
tachment. In this context, large amounts of raw
data used to construct embeddings effectively en-
rich limited syntactic annotations. While these ap-
proaches show initial promise, they still lag behind
self-trained parsers (McClosky et al., 2006). These
parsers also utilize raw data but in a different way:
self-trained parsers use it to get additional (noisy)
annotations, without computing new word represen-
tations. These results suggest that embedding-based
representations have not yet been utilized to their
full potential.

We show that embedding-based representations
can indeed significantly improve PP attachment ac-
curacy. We achieve this by using such represen-
tations within a compositional neural network ar-
chitecture. The representations are initially learned
from an unlabeled corpus, but are then further dis-
criminatively trained to maximize PP attachment
accuracy. We also explore alternative representa-
tions such as dependency-based word vectors that
are trained from parsed texts using the syntactic con-
text in a dependency tree.

We test our approach for PP attachment disam-
biguation on English and Arabic datasets, com-
paring it to full-scale parsers and a support vec-
tor machine (SVM) ranker. Our model outper-
forms all baselines, including a self-trained parser.
The difference is particularly apparent on Arabic.
For instance, our model achieves PP attachment
accuracy of 82.6% while the Turbo (Martins et
al., 2013), RBG (Lei et al., 2014), and Charniak
self-trained (McClosky et al., 2006) parsers obtain
76.7%, 80.3%, and 80.8% respectively. Our results

demonstrate that relearning the embeddings con-
tributes to the model performance, across a range of
configurations. We also notice that representations
based on syntactic context are more powerful than
those based on linear context. This may explain the
improved performance of self-trained parsers over
parsers that rely on linear context embeddings.

2 Related Work

Problem formulation Typically, PP attachment
disambiguation is modeled as a binary classification
decision between a preceding noun or verb (Brill
and Resnik, 1994; Ratnaparkhi et al., 1994; Collins
and Brooks, 1995; Olteanu and Moldovan, 2005;
Šuster, 2012). In addition, the problem of PP at-
tachment has also been addressed in the context of
full parsing (Atterer and Schütze, 2007; Agirre et al.,
2008). For instance, Green (2009) engineered state-
split features for the Stanford parser to improve Ara-
bic PP attachment.

In this work, we do isolate PP attachments from
other parsing decisions. At the same time, we con-
sider a more realistic scenario where multiple can-
didate heads are allowed. We also compare against
full-scale parsers and show that our model predic-
tions improve a state-of-the-art dependency parser.

Information sources Lexical sparsity associated
with disambiguating PP attachments (Figure 1) has
spurred researchers to exploit a wide range of infor-
mation sources. On the one hand, researchers have
explored using manually crafted resources (Stetina
and Nagao, 1997; Gamallo et al., 2003; Olteanu
and Moldovan, 2005; Medimi and Bhattacharyya,
2007). For instance, Agirre et al. (2008) demon-
strate that using WordNet semantic classes bene-
fits PP attachment performance. On the other hand,
researchers have looked into using co-occurrence
statistics from raw text (Volk, 2002; Olteanu and
Moldovan, 2005; Gala and Lafourcade, 2007). Such
statistics can be translated into word vectors from
which a cosine similarity score is calculated (Šuster,
2012). We also rely on word vectors, but our model
captures more complex relations among them.

Algorithmic approach Our work is most similar
to recursive neural network parsers (Costa et al.,
2003; Menchetti et al., 2005; Socher et al., 2010). In

562



particular, Socher et al. (2013) obtain good parsing
performance by building compositional representa-
tions from word vectors. However, to combat the
computational complexity of the full parsing sce-
nario, they rely on a probabilistic context-free gram-
mar to prune search space. In contrast, focusing on
PP attachment allows us to consider various neu-
ral network architectures that are more appropriate
for this task, including ternary, binary, and distance-
dependent compositions. Furthermore, we investi-
gate modifications to the original word vectors in
several important directions: enriching word vectors
with semantic and syntactic knowledge resources,
relearning them by backpropagating errors from su-
pervised data, and using dependency-based vectors.
We show that such modifications lead to better word
vectors and significant performance gains.

3 Model

We begin by introducing some notation. All vectors
v ∈ Rn are assumed to be column vectors. We de-
note a given sentence by x and the set of prepositions
in x by PR(x). In other words, PR(x) is the set of
words whose POS tags are prep. The PP attachment
label of the preposition z ∈ PR(x) is denoted by
y(z) ∈ x. Namely, y(z) = h indicates that the head
of the preposition z is h.

Our classification approach is to construct a scor-
ing function s(x, z, h; θ) for a preposition z ∈
PR(x) and its candidate head h in the sentence x.
We then choose the head by maximizing s(x, z, h; θ)
over h. The set of possible candidates {h} can be of
arbitrary size, thus departing from the binary classi-
fication scenario considered in much of the previous
work (Section 2). The set of parameters is θ.

3.1 Compositional framework

Our approach to constructing the score function is
as follows. First, we assume that all words in the
sentence are represented as vectors in Rn. Next,
we compose vectors corresponding to the relevant
preposition, its candidate head, and other words in
the sentence to obtain a new vector p ∈ Rn. The
final score is a linear function of this vector.

The basic composition operation is defined as a
single layer in a neural network (Socher et al., 2010).
Given vectors u,v ∈ Rn, representing two words,

we form a new vector via a function:

g(W[u;v] + b) ∈ Rn (1)

where b ∈ Rn is a vector of bias terms, [u;v] ∈ R2n
is a concatenation of u and v into a column vector,
W ∈ Rn×2n is a composition matrix, and g is a
non-linear activation function.2

Given a candidate head h for preposition z, we
apply such compositions to a set of words, resulting
in a vector p. The final score s(x, z, h; θ) is given
by w · p, where w ∈ Rn is a weight vector. The
parameters to be learned are θ = (W,b,w).

3.2 Composition architectures
There are various possible ways to compose and ob-
tain the vector p. Table 1 shows three basic compo-
sition architectures that are used in our model. In all
cases, elements like the head of the PP, the preposi-
tion, and the first child of the preposition are com-
posed using Eq. 1 to derive a parent vector that is
then scored by the score vector w. The architec-
tures differ in the number of compositions and their
type. For instance, the Head-Child model uses only
the head and child in a single composition, ignoring
the preposition. The Head-Prep-Child-Ternary com-
poses all three elements simultenuously, reflecting
ternary interactions. The Head-Prep-Child model,
on the other hand, first composes the preposition and
child to form a parent p1 representing the PP, then
composes p1 with the head into another parent p2
(= p) that is scored by w. This two-step process fa-
cilitates capturing different syntactic relations with
different composition matrices. We turn to this next.

Granularity The basic composition architectures
(Table 1) assume a global matrix W for all composi-
tion operations. In the case of the Head-Prep-Child
model, we also consider a local variant with differ-
ent matrices for the two compositions: Wbottom for
composing the preposition z with its child c into a
parent p1 representing the PP, and Wtop for com-
posing the head h with p1 into a parent p2. The
composition equations are then:

p1 = g(W
bottom[z; c] + bbottom)

p2 = g(W
top[h;p1] + b

top)

2We use tanh which performed slightly better than sigmoid
in preliminary experiments.

563



Model Equations Structure
Head-Child (HC) p = g(W[h; c] + b) p

ch

Head-Prep-Child (HPC) p1 = g(W[z; c] + b) p2

p1

cz

h
p2 = g(W[h;p1] + b)

Head-Prep-Child-Ternary
(HPCT)

p = g(WTern[h; z; c] + b) p

czh

Table 1: Basic composition architectures. h, z, c ∈ Rn are vectors for the head, the preposition, and
its child respectively; p,p1,p2 ∈ Rn are parent vectors created during composition operations; W ∈
Rn×2n,WTern ∈ Rn×3n are binary and ternary composition matrices respectively; b ∈ Rn is a bias term;
and g is a non-linear function.

In this case, the set of parameters is θ =
(Wtop;btop;Wbottom;bbottom;w). We call this
variant the Head-Prep-Child-Local (HPCL) model.

The composition architectures described thus far
only considered the composed words but not their
relative position in the sentence. Such position in-
formation may be useful, since candidates closer to
the preposition are typically more likely to attach.
To model this difference, we introduce distance-
dependent parameters and modify the Head-Prep-
Child model (Table 1, middle row) as follows: for
a head h at distance d from the preposition, we let:

p2 = g(W
d[h;p1] + b

d)

where Wd ∈ Rn×2n and bd ∈ Rn are the ma-
trix and bias for composing with heads at dis-
tance d from the preposition. p1 is defined as
in Table 1. The set of parameters is then θ =
({Wd;bd}d;W;b;w). To reduce the number of
parameters we use only d = 1, . . . , 5, and clip dis-
tances greater than 5. We name this model Head-
Prep-Child-Dist (HPCD).

Context It may also be useful to exploit words sur-
rounding the candidate head such as the following
word. This can be integrated in the composition ar-
chitectures in the following way: for each candidate
head, represented by a vector h ∈ Rn, concatenate

a vector representing the word following the can-
didate. If such a vector is not available, append a
zero vector. This results in a new vector h′ ∈ R2n
representing the head. To compose it with a vector
p1 ∈ Rn representing the PP, we use a composition
matrix of size n × 3n, similar to the ternary com-
position described above. We refer to this model as
Head-Prep-Child-Next (HPCN).

3.3 Training
For training, we adopt a max-margin framework.
Given a training corpus of pairs of sentences and
attachments, {x(i), y(i)}, we seek to minimize the
following objective function:

J(θ) =
T∑

i=1

∑

z∈PR(x(i))
max
h

[
s(x(i), z, h; θ)

−s(x(i), z, y(i)(z); θ) + ∆(h, y(i)(z))
]

(2)

where ∆ is the zero-one loss.
For optimization we use minibatch AdaGrad

(Duchi et al., 2011). Note that the objective is non-
differentiable so AdaGrad is used with the subgradi-
ent of J(θ), calculated with backpropagation.

For regularization we use Dropout (Hinton et al.,
2012), a recent method for preventing co-adaptation
of features, where input units to the neural network

564



are randomly dropped. Random dropping occurs in-
dependently for each training example and has the
effect of creating multiple thinned networks that are
trained with shared parameters. In our implementa-
tion we dropout input units before each non-linear
layer, including the initial word vectors. We do not
dropout units after the final non-linear layer. Note
that Dropout is known to be especially useful when
combined with AdaGrad (Wager et al., 2013).

Hyperparameters and initialization We use the
following default hyperparameters without further
tuning unless noted otherwise: Dropout parameter
ρ = 0.5 (Hinton et al., 2012), AdaGrad initial learn-
ing rate η = 1.0 (Dyer, n.d.), and minibatch size
of 500. Learned parameters are initialized simi-
larly to previous work (Bengio and Glorot, 2010;
Socher et al., 2013): composition matrices are set
to W = 0.5[I I] + �, where � ∼ U(− 1n , 1n); bias
terms b are set to zero; and the weight vector is set
to w ∼ U(− 1√

n
, 1√

n
).

4 Word vector representations

Our approach assumes a vector representation for
each word. Such representations have gained popu-
larity in recent years, due to the ability to train them
from large unlabeled datasets, and their ease of use
in a wide variety of tasks (Turian et al., 2010).

There are various approaches to training vector
representations (Collobert and Weston, 2008; Ben-
gio et al., 2009). Here we chose to focus on the
Skip-gram method recently proposed by Mikolov et
al. (2013a). The Skip-gram model maximizes the
average log-probability of every word generating its
context, which is modeled via a neural net architec-
ture, but without the non-linearity. To improve effi-
ciency, this probability is approximated by a hierar-
chical softmax (Mikolov et al., 2013b) with vocabu-
lary words represented in a binary Huffman tree.3

In the simplest variant of our method, we train
the Skip-gram representation on unlabeled text, and
use it as a fixed representation when training the PP
attachment model (see Section 3.3). Below we con-
sider several variations on this approach.

3Preliminary experiments with other model variations (e.g.
negative sampling) have not led to notable performance gains.

Figure 2: Illustration of an enriched word vector.
Initial dimensions learned from raw texts are en-
riched with binary vectors indicating part-of-speech
tags, VerbNet frames, and WordNet hypernyms.

4.1 Relearning word vectors
The Skip-gram word vectors are originally learned
from raw text, with the objective of maximizing the
likelihood of co-occurring words. Here our goal is
to maximize PP attachment accuracy, and it is possi-
ble that a different representation is optimal for this
task. We may thus take a discriminative approach
and update the vectors to maximize PP attachment
accuracy. Technically this just requires taking the
subgradient of our objective (Eq. 2) with respect to
the word vectors, and updating them accordingly.

Adding the word vectors as parameters signifi-
cantly increases the number of free parameters in the
model, and may lead to overfitting. To reduce this
effect, we use Dropout regularization (Section 3.3).
We also employ a smaller initial learning rate for the
word vectors compared to other model parameters.4

Finally, note that since the objective is non-
convex, the vectors obtained after this procedure
will typically depend on the initial value used. The
relearning procedure may thus be viewed as fine-
tuning the word vectors to improve PP attachment
accuracy.

4.2 Enriching word vectors
The word vectors we use are trained from raw text.
However, it is easy to enrich them using structured
knowledge resources such as VerbNet or WordNet,
as well as morpho-syntactic information available in
treebanks.

Our approach to enriching word vectors is to ex-
tend them with binary vectors. For example, given
a vector h for the candidate head, we add binary-
valued dimensions for its part-of-speech and that of
the following word. Next we add a binary dimension
for VerbNet indicating whether the candidate head

4We tuned η = [0.001, 0.01, 0.1, 1] on the English dev set
and chose the best value (η = 0.001) for all other experiments.

565



appears with the preposition in a verb frame. Finally,
for each top hypernym in WordNet, we add a binary
dimension indicating whether it is a hypernym of
the candidate head, aiming for semantic clustering
information. Note that we do not perform sense dis-
ambiguation so this information may be noisy.

Figure 2 illustrates the resulting enriched vector.
Similar dimensions are appended to vectors repre-
senting other words participating in the composi-
tions. Our experiments show that such an extension
significantly improves performance.

4.3 Syntactic word vectors

In the standard Skip-gram model word vectors are
trained from raw text using the linear context of
neighboring words. We also consider an alterna-
tive method for creating word vectors by using the
syntactic context of words. Such syntactic context
is expected to be relevant for resolving PP attach-
ments. Given a dependency-parsed text, we follow
Bansal et al. (2014) and create a new corpus of tu-
ples (l, g, p, c, l), for every word c, its parent p with
dependency label l, and its grandparent g. Then we
train an ordinary Skip-gram model on this corpus,
but with a small window size of 2. Note that the label
l appears on both ends so it contributes to the con-
text of the word as well as its grandparent. We find
that syntactic vectors yield significant performance
gains compared to standard vectors.5

5 Experimental setup

5.1 Extracting PP attachments

Instances of PP attachment decisions are extracted
from standard treebanks. We use the CATiB de-
pendency treebank (Habash and Roth, 2009) for
Arabic and a conversion of the Penn treebank
(PTB) to dependency format for English.6 Standard
train/dev/test splits are used: sections 2-21/22/23 of
the PTB for English, and the split from the SPRML
shared-task for Arabic (Seddah et al., 2013). As Ta-
ble 2 shows, the datasets of the two languages are
fairly similar in size, except for the much larger set
of prepositions in the English data.

Extracting instances of PP attachments from the
treebanks is done in the following way. For each

5We also experimented with another method for creating
syntactic vectors by Levy and Goldberg (2014) and observed

Arabic English
Train Test Train Test

Total 42,387 3,917 35,359 1,951
Candidates 4.5 4.3 3.7 3.6
Vocab sizes
All 8,230 2,944 11,429 2,440
Heads 8,225 2,936 10,395 2,133
Preps 13 10 72 46
Children 4,222 1,424 5,504 983

Table 2: Statistics of extracted PP attachments,
showing total sizes, average number of candidate
heads, and vocabulary sizes.

Arabic English
Corpus arTenTen Wikipedia BLLIP
Tokens 130M 120M 43M
Types 43K 218K 317K

Table 3: Statistics of Arabic and English corpora
used for creating word vectors.

preposition, we look for all possible candidate heads
in a fixed preceding window. Typically, these will
be nouns or verbs. Only prepositions with a noun
child are considered, leaving out some rare excep-
tions. Empirically, limiting candidate heads to ap-
pear close enough before the preposition is not an
unrealistic assumption: we choose a 10-word win-
dow and find that it covers about 94/99% of Ara-
bic/English PP attachments. Unambiguous attach-
ments with a single possible candidate are discarded.

5.2 Creating word vectors

The initial word vectors are created from raw texts
using the Skip-gram model with hierarchical soft-
max, as described in Section 4.7 We use a portion of
Wikipedia for English8 and the arTenTen corpus for
Arabic, containing web texts crawled in 2012 (Be-
linkov et al., 2013; Arts et al., 2014). Table 3

similar performance gains.
6We used the Pennconverter tool: http://nlp.cs.

lth.se/software/treebank-converter.
7We used the word2vec tool: https://code.google.

com/p/word2vec, with default settings. We experimented
with word vectors of 25, 50, 100, and 200 dimensions, and
found 100 to work best in most cases.

8http://mattmahoney.net/dc/textdata.

566

http://nlp.cs.lth.se/software/treebank-converter
http://nlp.cs.lth.se/software/treebank-converter
https://code.google.com/p/word2vec
https://code.google.com/p/word2vec
http://mattmahoney.net/dc/textdata


shows the comparable sizes of the datasets. The
Arabic corpus has been tokenized and lemmatized
with MADA (Habash and Rambow, 2005; Habash et
al., 2005), a necessary procedure in order to separate
some prepositions from their child words. In addi-
tion, lemmatization reduces vocabulary size and fa-
cilitates sharing information between different mor-
phological variants that have the same meaning.

For syntactic word vectors, we use the English
vectors in (Bansal et al., 2014), which were trained
from a parsed BLLIP corpus (minus PTB). For Ara-
bic, we first convert the morphologically-processed
arTenTen corpus to CoNLL format with the SPMRL
shared-task scripts (Seddah et al., 2013). Then we
parse the corpus with a baseline MST parser (Sec-
tion 5.3) and create syntactic word vectors as de-
scribed in Section 4.3. The Arabic syntactic vectors
will be made available to the research community.

For enriching word vectors, we use part-of-speech
information9 from the treebanks as well as the Ara-
bic and English VerbNets (Kipper et al., 2008;
Mousser, 2010) and WordNets (Rodrı́quez et al.,
2008; Princeton University, 2010). In total, these
resources add to each word vector 46/67 extended
dimensions in Arabic/English, representing syntac-
tic and semantic information about the word.

5.3 Baselines

We compare against full-scale parsers, an SVM
ranker, and a simple but strong baseline of always
choosing the closest candidate head.

Parsers We mostly compare with dependency
parsers, including the state-of-the-art Turbo (Mar-
tins et al., 2010; Martins et al., 2013) and RBG
parsers (Lei et al., 2014), in addition to a second-
order MST parser (McDonald et al., 2005) and
the Malt parser (Nivre et al., 2006). We also
compare with two constituency parsers: an RNN
parser (Socher et al., 2013), which also uses word
vectors and a neural network approach, and the
Charniak self-trained reranking parser (McClosky et
al., 2006). We train all parsers on the train/dev sets
and report their PP attachment accuracy on the test
sets.10 For the self-trained parser we followed the

9We use gold POS tags in all systems and experiments.
10The only exception is the RNN parser, for which we use

the built-in English model in Stanford parser’s (version 3.4);

Source Feature Template
Treebank hw, pw, cw, hw-cw,

ht, nt, hpd
WordNet hh, ch
VerbNet hpvf
Word Vectors sim(hv,cv)
Brown Clusters hc*, pc*, cc*,

hc4, pc4, cc4,
hc*-cc*, hc4-cc4

Table 4: Feature templates for the SVM baseline.
Bi-lexical templates appear with a “-”. Abbrevi-
ations: hw/pw/cw = head/prep/child word, ht =
head tag, nt = next word tag, hpd = head-prep dis-
tance; hh/ch = head/child hypernym; hpvf = head-
prep found in verb frame; hv/cv = head/child vec-
tor; hc*/pc*/cc* = head/prep/child full bit string,
hc4/pc4/cc4 = head/prep/child 4-bit prefix.

procedure in (McClosky et al., 2006) with the same
unsupervised datasets that are used in our PP model.

SVM We consider a learning-to-rank formulation
for our problem, where each example provides a cor-
rect candidate head and several incorrect candidates.
We order these in a simple list where the correct can-
didate has the highest rank and all other candidates
have a single lower rank. We then rank these with
an SVM ranker11 and select the top candidate. This
formulation is necessary because we depart from the
binary classification scenario that was used in previ-
ous work (Section 2).

The SVM ranker uses the following features: the
candidate head, preposition, and child; bi-lexical
conjunctions of head-child; part-of-speech tags of
the head and the following word; and the candi-
date head’s distance from the preposition. We also
add top WordNet hypernyms for head and child,
and an indicator of whether the preposition appears
in the head’s sub-categorization frame in VerbNet.
This configuration parallels the information used in
our model but fails to exploit raw data. Therefore,
we consider two more types of features. First, we
use word vectors by computing cosine similarity be-
tween vectors of the candidate head and the child

for Arabic we do train a new RNN model.
11We use SVMRank: http://www.cs.cornell.

edu/people/tj/svm_light/svm_rank.html.

567

http://www.cs.cornell.edu/people/tj/svm_light/svm_rank.html
http://www.cs.cornell.edu/people/tj/svm_light/svm_rank.html


of the preposition. This feature was found useful
in previous work on PP attachment (Šuster, 2012).
While this limits the contribution of the word vec-
tors to the learned model to one dimension, attempts
to use more dimensions in the SVM were unsuccess-
ful.12 In contrast, the compositional models better
capture the full dimensionality of the word vectors.

A second type of features induced from raw data
that we consider are Brown clusters, which were
found to be useful in dependency parsing (Koo et
al., 2008). Compared to distributed vectors, Brown
clusters provide a more discrete representation that
is easier to incorporate in the SVM. We create clus-
ters from our unsupervised corpora using the Liang
(2005) implementation of Brown’s algorithm, and
add features in the spirit of (Koo et al., 2008).
Specifically, we add full and prefixed bit strings for
the head, preposition, and child, as well as bi-lexical
versions for head-child pairs.13 Table 4 shows a
summary of the SVM features.

6 Results

Table 5 summarizes the results of our model and
other systems. Our best results are obtained with the
Head-Prep-Child-Dist (HPCD) model using syn-
tactic vectors, enriching, and relearning. The full
model outperforms both full-scale parsers and a ded-
icated SVM model. More advanced parsers do
demonstrate higher accuracy on the PP attachment
task, but our method outperforms them as well. Note
that the self-trained reranking parser (Charniak-RS)
performs especially well and quite better than the
RNN parser. This trend is consistent with the results
in (Kummerfeld et al., 2012; Socher et al., 2013).

Our compositional architecture is effective in ex-
ploiting raw data: using only standard word vec-
tors with no enriching, our HPCD (basic) model per-
forms comparably to an SVM with access to all en-
riching features. Once we improve the representa-
tion, we outperform both the SVM and full parsers.
In comparison, the contribution of raw data to the
SVM, as either word vectors or Brown clusters, is
rather limited.

12For example, we tried adding all word vector dimensions
as features, as well as element-wise products of the vectors rep-
resenting the head and the child.

13As in (Koo et al., 2008), we limit the number of unique bit
strings to 1,000 so full strings are not equivalent to word forms.

System Arabic English
Closest 62.7 81.7
SVM 77.0 86.0
w/ word vectors 77.5 85.9
w/ Brown clusters 78.0 85.7
w/ Brown clusters+prefixes 77.0 85.7
Malt 75.4 79.7
MST 76.7 86.8
Turbo 76.7 88.3
RBG 80.3 88.4
RNN 68.9 85.1
Charniak-RS 80.8 88.6
HPCD (basic) 77.1 85.4
w/ enriching 80.4 87.7
w/ syntactic 79.1 87.1
w/ relearning 80.0 86.6
HPCD (full) 82.6 88.7
RBG + HPCD (full) 82.7 90.1

Table 5: PP attachment accuracy of our HPCD
model compared to other systems. HPCD (full) uses
syntactic vectors with enriching and relearning. The
last row is a modified RBG parser with a feature for
the PP predictions of our model.

The relative performance is consistent across both
English and Arabic. The table also demonstrates that
the Arabic dataset is more challenging for all mod-
els. This can be explained by a larger average candi-
date set (Table 2), a freer word order that manifests
in longer attachments (average head and PP distance
is 3.3 in Arabic vs 1.5 in English), and the lexical
sparsity induced by the richer morphology.

Effect on parsing To investigate how our PP at-
tachment model contributes to the general parsing
task, we incorporated the predictions of our model
in an existing dependency parser. We modified the
RBG parser (Lei et al., 2014) such that a binary arc
feature fires for every PP attachment predicted by
our model. For both test sets, we find that the pars-
ing performance, measured as the unlabeled attach-
ment score (UAS), increases by adding the predic-
tions in this way (Table 6). The modified parser also
achieves the best PP attachment numbers (Table 5).

Interestingly, incorporating the PP predictions in
a parser leads to a gain in parsing performance that

568



System Arabic English
RBG 87.70 93.96
RBG + predicted PP 87.95 94.05
RBG + oracle PP 89.09 94.42

Table 6: Parsing performance (UAS) of the RBG
parser, with predicted and oracle PPs.

is relatively larger than the gain in PP accuracy. For
example, relative to an oracle upper bound of forc-
ing gold PP arcs in the parser output (Table 6), the
reduction in English parsing errors is 20%, whereas
the reduction in PP errors is only 15%. This affirms
the importance of PP attachment disambiguation for
predicting other attachments in the sentence.

RRR dataset Much of the previous work on PP
attachment focused on a binary classification sce-
nario (Section 2) and has been evaluated on the RRR
dataset (Ratnaparkhi et al., 1994). Such systems
cannot be easily evaluated in our setting which al-
lows multiple candidate heads. On the other hand,
our full model exploits contextual information that
is not available in the RRR dataset. Nevertheless,
using a simpler version of our model we obtain an
accuracy of 85.6% on the RRR test set.14 This is
comparable to much of the previous work (Olteanu
and Moldovan, 2005), but still lags behind the 88.1%
of Stetina and Nagao (1997), who also used Word-
Net information. However, our use of WordNet is
rather limited compared to theirs, indicating that our
enriching method can be improved with other types
of information.

6.1 Alternative composition architectures

In this section we analyze how different composition
architectures (Section 3.2) contribute to the overall
performance. To isolate the contribution of the ar-
chitecture, we focus on standard (linear) word vec-
tors, with no relearning or enriching. As Figure 3
shows, simpler models tend to perform worse than
more complex ones. The best variants use differ-
ent composition matrices based on the distance of
the candidate head from the PP (HPCD, HPCDN).
While the results shown are for 100-dimensional

14Here we applied basic preprocessing similarly to (Collins
and Brooks, 1995), converting 4-digit numbers to YEAR and
other numbers to NUMBER; other tokens were lower-cased.

Figure 3: PP attachment accuracy of different archi-
tectures. (HC) uses only the candidate head and the
child of the preposition; (HPC*) models use head,
preposition, and child, with the following variants:
(HPCT) ternary composition; (HPCL) local matri-
ces for top and bottom compositions; (HPCN) con-
text words; (HPCD) distance-dependent matrices;
(HPCDN) combines HPCD+HPCN.

vectors, similar trends are observed with lower di-
mensions, although the gaps between simple and
complex models are then more substantial.

We have also experimented with compositions
through the entire PP subtree. However, this resulted
in a performance drop (to about 50%), implying that
adding more words to the composite representation
of the PP does not lead to a distinguishing represen-
tation with regards to the possible candidate heads.

6.2 Alternative representations

In this section, we analyze how different word vector
representations (Section 4) contribute to our model.
We focus on the HPCD model, which builds a two-
step composite structure with distance-dependent
composition matrices. We take the basic represen-
tation to be standard (linear) word vectors, without
enriching or relearning. In each paragraph below,
we investigate how a different aspect of the repre-
sentation affects PP attachment performance.

Relearning word vectors In traditional architec-
tures, the process of word vector induction is inde-
pendent of the way the vector is used in the pars-
ing algorithm. We hypothesize that by connecting

569



Figure 4: Effects of relearning standard word vec-
tors in English and Arabic.

these two processes and tailoring the word vectors
to the task at hand, we can further improve the ac-
curacy of the PP attachments. We thus relearn the
word vectors during training as described in Sec-
tion 4.1. Indeed, as Figure 4 shows, doing so con-
sistently improves performance, especially with low
dimensional vectors. Interestingly, syntactic word
vectors also benefit from the update (Table 8). This
indicates that the supervised PP attachments provide
complementary signal to noisy dependencies used to
construct syntactic vectors.

Enriching word vectors A substantial body of
work has demonstrated that multiple features can
help in disambiguating PP attachments (Section 2).
To this end, we enrich word vectors with addi-
tional knowledge resources (Section 4.2). As Ta-
ble 7 shows, this enrichment yields sizable perfor-
mance gains. Most of the gain comes from part-
of-speech information, while WordNet and VerbNet
have a smaller contribution. Updating the word vec-
tors during training has an additional positive effect.

Note that even with no enrichment, our model
performs comparably to an SVM with access to all
enriching features (Table 5). When enriched, our
model outperforms the SVM by a margin of 2-3%.
With relearning, the gaps are even larger.

Syntactic word vectors While most of the work
in parsing relies on linear word vectors (Socher et
al., 2013; Lei et al., 2014), we consider an alter-
native vector representation that captures syntactic

Representation Arabic English
w/o enriching 77.1 85.4
w/ enriching
+POS 78.5 86.4
+NextPOS 79.7 87.5
+WordNet+VerbNet 80.4 87.7
w/ enriching+relearning 81.7 88.1
w/ enriching+relearn.+syn. 82.6 88.7

Table 7: PP attachment accuracy when enriching
word vectors with part-of-speech tags of the candi-
date head (POS) and the following word (NextPOS),
and with WordNet and VerbNet features.

Representation Arabic English
Linear 77.1 85.4
Syntactic 79.1 87.1
Syntactic w/ relearning 80.7 87.7

Table 8: PP attachment accuracy of linear (standard)
and syntactic (dependency-based) word vectors.

context. As described in Section 4.3, such vectors
are induced from a large corpus processed by an au-
tomatic dependency parser. While the corpus is most
likely fraught with parsing mistakes, it still contains
sufficient dependency information for learning high-
quality word vectors. Table 8 confirms our assump-
tions: using syntactically-informed vectors yields
significant performance gains.

7 Conclusion

This work explores word representations for PP at-
tachment disambiguation, a key problem in syntac-
tic parsing. We show that word vectors, induced
from large volumes of raw data, yield significant
PP attachment performance gains. This is achieved
via a non-linear architecture that is discriminatively
trained to maximize PP attachment accuracy. We
demonstrate performance gains by using alternative
representations such as syntactic word vectors and
by enriching vectors with semantic and syntactic in-
formation. We also find that the predictions of our
model improve the parsing performance of a state-
of-the-art dependency parser.

570



Acknowledgments

This research is developed in collaboration with
the Arabic Language Technologies (ALT) group at
Qatar Computing Research Institute (QCRI) within
the IYAS project. The authors acknowledge the sup-
port of the U.S. Army Research Office under grant
number W911NF-10-1-0533, the DARPA BOLT
program and the US-Israel Binational Science Foun-
dation (BSF, Grant No 2012330). We thank the
MIT NLP group and the TACL reviewers for their
comments, and Djamé Seddah and Mohit Bansal for
helping with scripts and data. Any opinions, find-
ings, conclusions, or recommendations expressed in
this paper are those of the authors, and do not neces-
sarily reflect the views of the funding organizations.

References

Eneko Agirre, Timothy Baldwin, and David Martinez.
2008. Improving Parsing and PP Attachment Per-
formance with Sense Information. In Proceedings of
ACL-HLT.

Tressy Arts, Yonatan Belinkov, Nizar Habash, Adam Kil-
garriff, and Vit Suchomel. 2014. arTenTen: Arabic
Corpus and Word Sketches. Journal of King Saud Uni-
versity - Computer and Information Sciences.

Michaela Atterer and Hinrich Schütze. 2007. Preposi-
tional Phrase Attachment Without Oracles. Computa-
tional Linguistics, 33(4).

Mohit Bansal, Keving Gimpel, and Karen Livescu. 2014.
Tailoring Continuous Word Representations for De-
pendency Parsing. In Proceedings of ACL.

Yonatan Belinkov, Nizar Habash, Adam Kilgarriff, Noam
Ordan, Ryan Roth, and Vı́t Suchomel. 2013. arTen-
Ten: a new, vast corpus for Arabic. In Proceedings of
WACL.

Yoshua Bengio and Xavier Glorot. 2010. Understanding
the difficulty of training deep feedforward neural net-
works. In Proceedings of AISTATS, volume 9, May.

Yoshua Bengio, Jérôme Louradour, Ronan Collobert, and
Jason Weston. 2009. Curriculum learning. In Pro-
ceedings of ICML.

Eric Brill and Philip Resnik. 1994. A Rule-Based
Approach to Prepositional Phrase Attachment Disam-
biguation. In Proceedings of COLING, volume 2.

Volkan Cirik and Hüsnü Şensoy. 2013. The AI-KU Sys-
tem at the SPMRL 2013 Shared Task : Unsupervised
Features for Dependency Parsing. In Proceedings of
SPMRL.

Michael Collins and James Brooks. 1995. Prepo-
sitional Phrase Attachment through a Backed-Off
Model. CoRR.

Michael Collins. 1997. Three Generative, Lexicalised
Models for Statistical Parsing. In Proceedings of ACL.

Ronan Collobert and Jason Weston. 2008. A Unified
Architecture for Natural Language Processing: Deep
Neural Networks with Multitask Learning. In Pro-
ceedings of ICML.

Fabrizio Costa, Paolo Frasconi, Vincenzo Lombardo, and
Giovanni Soda. 2003. Towards Incremental Parsing of
Natural Language Using Recursive Neural Networks.
Applied Intelligence, 19(1-2).

John Duchi, Elad Hazan, and Yoram Singer. 2011.
Adaptive Subgradient Methods for Online Learning
and Stochastic Optimization. JMLR, 12.

Chris Dyer. n.d. Notes on AdaGrad. Unpublished
manuscript, available at http://www.ark.cs.
cmu.edu/cdyer/adagrad.pdf.

Nuria Gala and Mathieu Lafourcade. 2007. PP attach-
ment ambiguity resolution with corpus-based pattern
distributions and lexical signatures. ECTI-CIT Trans-
actions on Computer and Information Technology, 2.

Pablo Gamallo, Alexandre Agustini, and Gabriel P.
Lopes. 2003. Acquiring Semantic Classes to Elabo-
rate Attachment Heuristics. In Progress in Artificial
Intelligence, volume 2902 of LNCS. Springer Berlin
Heidelberg.

Spence Green. 2009. Improving Parsing Per-
formance for Arabic PP Attachment Ambi-
guity. Unpublished manuscript, available at
http://www-nlp.stanford.edu/courses/
cs224n/2009/fp/30-tempremove.pdf.

Nizar Habash and Owen Rambow. 2005. Arabic Tok-
enization, Part-of-Speech Tagging and Morphological
Disambiguation in One Fell Swoop. In Proceedings of
ACL.

Nizar Habash and Ryan Roth. 2009. CATiB: The
Columbia Arabic Treebank. In Proceedings of the
ACL-IJCNLP.

Nizar Habash, Owen Rambow, and Ryan Roth. 2005.
MADA+TOKAN: A Toolkit for Arabic Tokenization,
Diacritization, Morphological Disambiguation, POS
Tagging, Stemming and Lemmatization. In Proceed-
ings of the Second International Conference on Arabic
Language Resources and Tools.

Geoffrey E. Hinton, Nitish Srivastava, Alex Krizhevsky,
Ilya Sutskever, and Ruslan Salakhutdinov. 2012. Im-
proving neural networks by preventing co-adaptation
of feature detectors. CoRR.

Karin Kipper, Anna Korhonen, Neville Ryant, and
Martha Palmer. 2008. A large-scale classification of
English verbs. Language Resources and Evaluation,
42(1).

571

http://www.ark.cs.cmu.edu/cdyer/adagrad.pdf
http://www.ark.cs.cmu.edu/cdyer/adagrad.pdf
http://www-nlp.stanford.edu/courses/cs224n/2009/fp/30-tempremove.pdf
http://www-nlp.stanford.edu/courses/cs224n/2009/fp/30-tempremove.pdf


Terry Koo, Xavier Carreras, and Michael Collins. 2008.
Simple Semi-supervised Dependency Parsing. In Pro-
ceedings of ACL-HLT.

Jonathan K. Kummerfeld, David Hall, James R. Curran,
and Dan Klein. 2012. Parser Showdown at the Wall
Street Corral: An Empirical Investigation of Error
Types in Parser Output. In Proceedings of EMNLP-
CoNLL.

Tao Lei, Yu Xin, Yuan Zhang, Regina Barzilay, and
Tommi Jaakkola. 2014. Low-Rank Tensors for Scor-
ing Dependency Structures. In Proceedings of ACL.

Omer Levy and Yoav Goldberg. 2014. Dependency-
Based Word Embeddings. In Proceedings of ACL.

Percy Liang. 2005. Semi-Supervised Learning for Natu-
ral Language. Master’s thesis, Massachusetts Institute
of Technology.

Andre Martins, Noah Smith, Eric Xing, Pedro Aguiar,
and Mario Figueiredo. 2010. Turbo Parsers: Depen-
dency Parsing by Approximate Variational Inference.
In Proceedings of EMNLP.

Andre Martins, Miguel Almeida, and Noah A. Smith.
2013. Turning on the Turbo: Fast Third-Order Non-
Projective Turbo Parsers. In Proceedings of ACL.

David McClosky, Eugene Charniak, and Mark Johnson.
2006. Effective Self-Training for Parsing. In Proceed-
ings of HLT-NAACL.

Ryan McDonald, Koby Crammer, and Fernando Pereira.
2005. Online Large-Margin Training of Dependency
Parsers. In Proceedings of ACL.

Srinivas Medimi and Pushpak Bhattacharyya. 2007. A
Flexible Unsupervised PP-attachment Method Using
Semantic Information. In Proceedings of IJCAI.

Sauro Menchetti, Fabrizio Costa, Paolo Frasconi, and
Massimiliano Pontil. 2005. Wide coverage natural
language processing using kernel methods and neural
networks for structured data. Pattern Recognition Let-
ters, 26(12).

Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey
Dean. 2013a. Efficient Estimation of Word Represen-
tations in Vector Space. In Proceedings of Workshop
at ICLR.

Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg Corrado,
and Jeffrey Dean. 2013b. Distributed Representations
of Words and Phrases and their Compositionality. In
Proceedings of NIPS.

Jaouad Mousser. 2010. A Large Coverage Verb Taxon-
omy for Arabic. In Proceedings of LREC.

J. Nivre, J. Hall, and J. Nilsson. 2006. MaltParser: A
Data-Driven Parser-Generator for Dependency Pars-
ing. In Proceedings of LREC.

Marian Olteanu and Dan Moldovan. 2005. PP-
attachment Disambiguation using Large Context. In
Proceedings of HLT-EMNLP.

Princeton University. 2010. WordNet. http://
wordnet.princeton.edu.

Adwait Ratnaparkhi, Jeff Reynar, and Salim Roukos.
1994. A Maximum Entropy Model for Prepositional
Phrase Attachment. In Proceedings of HLT.

Horacio Rodrı́quez, David Farwell, Javi Ferreres, Manuel
Bertran, Musa Alkhalifa, and M. Antonia Martı́. 2008.
Arabic WordNet: Semi-automatic Extensions using
Bayesian Inference. In Proceedings of LREC.

Djamé Seddah, Reut Tsarfaty, Sandra Kübler, Marie Can-
dito, Jinho D. Choi, Richárd Farkas, Jennifer Fos-
ter, et al. 2013. Overview of the SPMRL 2013
Shared Task: A Cross-Framework Evaluation of Pars-
ing Morphologically Rich Languages. In Proceedings
of SPMRL.

Richard Socher, Christopher D. Manning, and Andrew Y.
Ng. 2010. Learning Continuous Phrase Representa-
tions and Syntactic Parsing with Recursive Neural Net-
works. In Proceedings of NIPS Deep Learning and
Unsupervised Feature Learning Workshop.

Richard Socher, John Bauer, Christopher D. Manning,
and Ng Andrew Y. 2013. Parsing with Compositional
Vector Grammars. In Proceedings of ACL.

Jiri Stetina and Makoto Nagao. 1997. Corpus Based
PP Attachment Ambiguity Resolution with a Semantic
Dictionary. In Fifth Workshop on Very Large Corpora.

Simon Šuster. 2012. Resolving PP-attachment ambi-
guity in French with distributional methods. Mas-
ter’s thesis, Université de Lorraine & Rijksuniversiteit
Groningen.

Joseph Turian, Lev-Arie Ratinov, and Yoshua Bengio.
2010. Word Representations: A Simple and General
Method for Semi-Supervised Learning. In Proceed-
ings of ACL.

Martin Volk. 2002. Combining Unsupervised and Super-
vised Methods for PP Attachment Disambiguation. In
Proceedings of COLING.

Stefan Wager, Sida Wang, and Percy Liang. 2013.
Dropout Training as Adaptive Regularization. In Pro-
ceedings of NIPS.

572

http://wordnet.princeton.edu
http://wordnet.princeton.edu

