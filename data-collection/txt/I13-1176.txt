










































A Dynamic Confusion Score for Dependency Arc Labels


International Joint Conference on Natural Language Processing, pages 1237–1242,
Nagoya, Japan, 14-18 October 2013.

A Dynamic Confusion Score for Dependency Arc Labels

Sambhav Jain and Bhasha Agrawal
Language Technologies Research Center

IIIT-Hyderabad, India
{sambhav.jain, bhasha.agrawal}@research.iiit.ac.in

Abstract
In this paper we propose an approach to
dynamically compute a confusion score
for dependency arc labels, in typed depen-
dency parsing framework. This score ac-
companies the parsed output and aims to
administer an informed account of parse
correctness, detailed down to each edge of
the parse. The methodology explores the
confusion encountered by the oracle of a
data driven parser, in predicting an arc la-
bel. We support our hypothesis by empiri-
cally illustrating, for 20 languages, that the
labels with a high confusion score are no-
tably the predominant parsing errors.

1 Introduction

Recently, a major research drive has been to-
wards building data driven dependency parsers for
various languages. Shared tasks like CoNLL-X
and CoNLL 2007 have acted as development and
testing grounds for various efforts in the field.
The majority of the emerged systems follow ei-
ther graph based paradigm (McDonald et al.,
2005; McDonald and Pereira, 2006) eg. MST
Parser (McDonald et al., 2005) or transition based
paradigm (Yamada and Matsumoto, 2003; Nivre
and Scholz, 2004) eg. MaltParser(Nivre et al.,
2007).

A time complexity relatively lower than their
earlier counterparts makes the above parsers apt
for use by real time NLP applications like Ma-
chine Translation (Galley and Manning, 2009).
However, Popel et al.(2011) in the context of MT
pointed out that an incorrect parse can hurt the ac-
curacy of output. Thus, correctness of individual
parses becomes a key factor in such a setup.

This calls for measures which can indicate up-
front the quality of each individual output. A rele-
vant work in this direction is by Mejer and Cram-
mer (2012). They proposed methods to estimate

confidence of correctness of predicted parse in a
graph based parsing scenario using MSTParser.
Graph-based and transition-based parsers exhibit
two very distinctive approaches towards parsing,
each having its own strengths and limitations (Mc-
Donald and Nivre, 2007; Zhang and Clark, 2008).
The diversity in the two techniques motivated us to
explore and formulate a similar measure in tran-
sition based paradigm. We choose MaltParser1

(Nivre et al., 2007), which produces a parse tree
using a shift-reduce based transition algorithm, to
work with. It uses SVM to train an oracle to pre-
dict parsing action. The measures proposed by
Mejer and Crammer (2012) can not be straight off
applied in transition based parsers, as unlike the
graph based approach they commit local opera-
tions and thus can not directly produce globally
optimum k-best parses.

We propose computing an entropy based label
confusion score, dynamically computed and as-
signed while parsing (the computational details
are presented in section 2 of this paper). Since en-
tropy measures the uncertainty in a random vari-
able, we prefer to call the measure, in our ap-
proach, confusion score. This measure aims to
give a more informed picture of the parsed output.

We integrated our approach on top of the current
functionality of MaltParser, adjusting it to accredit
a confusion score with each arc label predicted in
the output. Figure 1 depicts a typical parse from
our proposed system where each arc label has been
designated a confusion score.

The measure can also be utilized to flag poten-
tial incorrectly parsed edges which later, can ei-
ther be manually corrected or altogether discarded
(to fall back on lower level but more accurate fea-
tures). We empirically illustrate in section 4, that
such a score can be effectively used for automatic
error detection in parsed outputs and guide manual

1MaltParser version 1.7 from http://www.maltparser.org

1237



was

market

The

NMOD0.054

equity

VMOD
# 0.869

SBJ0.195

illiquid

VMOD 0.396

Figure 1: A dependency parse tree with edge con-
fusion score. ’#’ represents incorrect arc label.

correction.

2 Dynamic Confusion Score

MaltParser outputs a single best parse by greedily
choosing parsing actions advocated by an oracle
trained on the training data. In a typed dependency
framework, the parser performs two distinct kinds
of actions; attachment of vertices and assigning
arc labels to edges. MaltParser provides a choice2

to train separate oracles for these two kinds or a
single oracle that jointly predicts attachment and
arc label.

In case of separate oracles for attachment and
label prediction, the parser queries the attachment
oracle until a Left Arc or Right Arc action is pre-
dicted. The label oracle is then queried for an arc
label in the given context. There is further a provi-
sion for having a separate oracle for Left Arc and
Right Arc label prediction. Nevertheless, an ora-
cle is always queried for a parsing action against a
given context.

2.1 Uncertainty in Label Prediction
The problem of predicting arc label correctness
can mathematically be posed as follows: For ran-
dom variables X and Y representing context and
parsing-actions respectively; an oracle φ is defined
such that φ : X −→ Y . Here, Y is always a closed
set, comprising permissible parser actions. The
uncertainty in predicting Y to one of its possible
values y1, ..., yn can be attributed as the confusion,
the oracle has in prediction. It is known that en-
tropy is a measure of the uncertainty in a random
variable (Ihara, 1993). Thus, this uncertainty can
be quantitatively determined by entropy(H) calcu-
lated by the following formula

H(Y/X) = −
n∑
i=1

p(yi/X) log p(yi/X) (1)

2http://www.maltparser.org/userguide.html#predstrate

were p(yi/X) is the posterior probability of yi be-
ing predicted as the parsing action in the given
context X . The higher the entropy, the more un-
certain the oracle is about the prediction.

However, there is no readily available provision
indicating the magnitude of confusion the oracle
encounters during prediction. The rest of this sec-
tion presents a sequential account of our approach.

2.2 SVM based Oracle

The oracle discussed earlier, is a multiclass classi-
fier which predicts a transition action based on the
context. MaltParser employs Support Vector Ma-
chine (Cortes and Vapnik, 1995) for classification
and provides an option between LIBSVM (Chang
and Lin, 2011) and LIBLINEAR (Fan et al., 2008)
to build the classifier(s). Both implement “one-vs-
one multi-class classification” method which in-
corporates nC2 binary models (n denotes number
of classes), one for each distinct pair of classes.
Prediction is done by voting among these binary
classifiers and the class with maximum votes is
emitted as decision class. This method does not
exert any sort of probabilities and in our sce-
nario we seek posterior probability estimates of
the classes.

2.3 Posterior Probabilities

Platt et al. (1999) showed that posterior probabil-
ities can be estimated in SVM by training the pa-
rameters of an additional sigmoid function to map
the SVM output into probabilities. Later, Wu et al.
(2004) extended the idea for multiclass probabil-
ity estimates by combining pairwise class proba-
bilities. In our work, we utilize the second method
(proposed in Wu et al. (2004)), which suggests the
following optimization formula:-

min
p

k∑
i=1

∑
j:j 6=i

(rjipi − rijpj)2

subject to
k∑
i=1

pi = 1, pi ≥ 0,∀i

where,

k = total classes,

rij = probability of i in binary classifier with classes i & j,

pi = probability of i in multiclass estimation

The solution to above optimization, furnishes
multiclass estimation of probability for each class.

1238



2.4 Entropy as Confusion Measure

Now, with the available class posterior probabili-
ties, entropy based confusion measure is computed
using equation 1. The base of the log is the num-
ber of label classes which ensures the entropy to
be in the range of 0 to 1. Confusion score for arc
label would require querying the arc label oracle,
and thus MaltParser must be configured to deliver
separate oracles for attachment and arc label in the
training phase.

3 Extendibility of the Confusion Score

This section presents the possible extensions and
constraints of the proposed score.

3.1 Extension to Full Parse Confusion

The confusion score in the proposed approach is
calculated separately for each arc label. Calcula-
tion of a confusion score for a full parse can be
done by taking an average over the edges in the
parse. Other measures like average of worst k la-
bels, score of worst label itself, etc. can also be
adopted. This enables visualization of the confu-
sion for the complete parse tree. This can be apt
in the scenario of a large collection of parse trees,
such as treebanks and also for applications like
Active Learning (Tang et al., 2002; Hwa, 2004).

3.2 Extendibility to Other Algorithms

Since our method executes at the oracle level, it
is independent of the algorithmic choice used in
the parser. Also, pseudo projective transformation
(Nivre and Nilsson, 2005) too is an extrinsic pro-
cess, so non-projectivity does not perturb our ap-
proach.

3.3 Extendibility for Attachments

The approach, at first, may seem extendable to
attachments also since it would require querying
the attachment oracle for attachment confusion.
However it is not extendable to edge correctness
prediction. The restricting factor being the pres-
ence of non-labeling parser action i.e. Shift and
Reduce. Since these transitions are not decom-
posed over the tree edges, the oracle confusion as-
sociated with them can not be delineated to any
edges. For example, at a given point in the parsing
process, assuming the arc-standard system (but the
same holds for other algorithms also), the oracle
will need to decide if it should perform a Left,

0 0.2 0.4 0.6 0.8 1
0

20

40

60

80

100

Confusion Score

P
er
ce
n
ta
g
e

F-score
Precision
Recall

Figure 2: Precision, recall and f-score for various
values of confusion score on ‘Hungarian’ devel-
opment set.

Right or Shift action. This decision will influ-
ence not only the single edges added by the left
or right operation, but also other, future edges. It
should be noticed that the Shift action will not add
any edge, but will have a very complex effect on
the set of edges that could or could not be added in
the future. Thus, the entropy of this particular de-
cision cannot be attached to any specific final edge
and hence the methodology can not be extendend
to arc-attachments.

4 Error Detection in Parser Output

In this section, we empirically illustrate the effi-
cacy of our proposed measure in automatic error
detection and guiding manual error correction.

4.1 Automatic Error Detection
Automatic error detection aims to efficiently de-
termine and flag incorrectly predicted edges. The
edges exhibiting a high confusion score are also
highly probable to be incorrect, as the oracle is un-
certain in its decision. Using this insight, an edge-
label is flagged as potential error if its confusion
score is above a pre-calculated threshold(θ).

In this task we have focused on the arc label
correctness, i.e. we flag the edges which have a
high probability for an incorrect arc label.

4.2 Data and Experimental Setup
We conducted experiments on 20 languages, using
data from CoNLL-X (Buchholz and Marsi, 2006),
CoNLL 2007 (Nilsson et al., 2007) and MTPIL
COLING 2012 (Sharma et al., 2012) shared tasks
on dependency parsing. We carried out exper-
iments on the systems proposed in Nivre et al.

1239



Language Threshold F-score Precision Recall EDI EDI EDI
(θ) (%) (%) (%) 1% edges(%) 5% edges(%) 10% edges(%)

Arabic# 0.14 50.71 41.39 65.45 4.43 20.98 36.68
Basque# 0.16 51.74 46.62 58.13 2.57 9.58 24.41
Catalan# 0.11 48.67 48.54 48.79 7.63 26.53 44.79
Chinese# 0.09 41.35 41.68 41.04 5.70 20.02 36.00
Czech# 0.08 51.61 47.85 56.02 4.20 18.82 35.92

English# 0.09 47.71 57.78 40.63 8.54 33.89 44.58
Greek# 0.12 54.47 44.76 69.54 4.84 20.28 35.89

Hungarian# 0.36 61.80 69.64 55.54 6.23 31.14 53.49
Italian# 0.15 43.48 39.43 48.45 2.57 19.73 40.37

Turkish# 0.14 53.58 43.38 70.05 3.99 21.90 40.82
Hindi♠ 0.16 49.80 43.57 58.11 4.86 20.48 35.09

Bulgarian¶ 0.12 47.52 40.45 57.60 7.85 34.88 55.63
Danish¶ 0.12 49.77 41.85 61.41 6.26 30.39 50.32
Dutch¶ 0.11 50.29 47.46 53.47 2.63 15.39 34.25

German¶ 0.09 44.44 37.33 54.91 4.37 23.62 45.64
Japanese¶ 0.09 41.43 29.12 71.75 4.90 24.49 57.59

Portuguese¶ 0.11 48.13 44.61 52.25 10.43 35.23 54.51
Slovene¶ 0.14 54.29 44.84 68.78 5.72 19.38 34.15
Spanish¶ 0.09 40.00 31.10 56.03 7.43 29.21 46.33
Swedish¶ 0.13 48.47 42.36 56.63 5.03 24.04 42.37
Average - 48.96 44.19 57.23 5.51 24.00 42.44

Table 1: Language wise results for automatic error detection task. EDI x% edges= Error detected on
inspecting top x% of total edges. Data Source:- ¶:CoNLL-X, #:CoNLL 2007, ♠:MTPIL COLING 2012

(2006), Hall et al. (2007) and Singla et al. (2012),
which are individually, the best performing Malt-
Parser based systems, in the respective shared
tasks. All the results reported here are on the offi-
cial test sets.

4.3 Identifying Optimum Threshold(θ)

Threshold(θ) is a crucial parameter in the exper-
imental setup. An optimum θ is chosen by mak-
ing use of the development set. We iteratively
increase candidate values for θ, from minimum
to maximum possible value of confusion score,
with an adequate interval. Corresponding to each
of these values, the incorrect edges are flagged
and precision, recall & F -score (Manning et al.,
2008) are calculated. The value asserting the max-
imum F -score is chosen as the final θ. Here for
simplicity, we have used balanced F -score, i.e.
F1-score. However, as per the application and
available resources, a relevant Fβ can be chosen
to maximize the yield on the input effort.

Fβ = (1 + β
2)× precision× recall

(β2 × precision) + recall

Figure 2 depicts precision, recall and
F1-score corresponding to each candidate value

of θ for Hungarian development data. The max-
imum F1-score is attained at 0.36 which is thus
taken as the final θ for Hungarian.

Since, CoNLL-X and CoNLL 2007 datasets do
not provide development sets, we hold out ran-
dom 10% sentences of a training set as develop-
ment data. The remaining training data is utilized
to train a parser and identify an optimum threshold
on the development set, as explained earlier. How-
ever, the final training is performed on the entire
training data and evaluated on the test set.

4.4 Results and Discussion

Table 1 exhibits the results obtained for auto-
matic error identification. An average F1-score
of 48.96%, precision of 44.19% and recall of
57.23% is obtained over 20 languages in the task.

To efficiently capture the efficacy of our ap-
proach, another metric is presented (columns 6-8)
which corresponds to the percentage of errors de-
tected by inspecting top 1%, 5% and 10% of total
edges. The metric gives a more precise picture of
the effort required to correct errors.

Our experiments indicate that on average
42.44% errors can be detected by just inspecting
top 10% of total edges. This portrays that the

1240



effort required here is one fourth as compared to
that in conventional sequential correction. On in-
specting top 5% and 1% of all edges, 24.00% and
5.51% errors can be detected. Best results are ob-
tained for Japanese and Portuguese where 57.59%
of errors are detected by merely inspecting 10%
of total edges for Japanese, while 35.23% and
10.43% errors are detected on inspecting 5% and
1% edges respectively for Portuguese.

A comparison with Mejer and Crammer (2012)
is not possible as they only give confidence scores
for parent-child attachments while our approach
gives confusion scores for parent-child edge’s de-
pendency label. In a typed dependency frame-
work both attachments and labels are significant
and hence our approach is complementing Mejer
and Crammer (2012).

5 Conclusion and Future Work

This paper presents our effort towards computing
a confusion score that can estimate, upfront, the
correctness of the dependency parsed tree. The
confusion score, accredited with each edge of the
output, is targeted to give an informed picture of
the parsed tree quality. We supported our hypoth-
esis by experimentally illustrating that the edges
with a higher confusion score are the predominant
parsing errors.

Not only parsed output, manual treebank valida-
tion too can benefit from such a score. An n-fold
cross validation scheme can be adopted, in this
case, to compute and assign confusion scores and
detect annotation errors. Also, this score has scope
in active learning where unannotated instances ex-
hibiting high confusion can be prioritized for man-
ual annotation.

References
Sabine Buchholz and Erwin Marsi. 2006. Conll-x

shared task on multilingual dependency parsing. In
Proceedings of the Tenth Conference on Computa-
tional Natural Language Learning, pages 149–164.
Association for Computational Linguistics.

Chih-Chung Chang and Chih-Jen Lin. 2011. Lib-
svm: a library for support vector machines. ACM
Transactions on Intelligent Systems and Technology
(TIST), 2(3):27.

Corinna Cortes and Vladimir Vapnik. 1995. Support-
vector networks. Machine learning, 20(3):273–297.

Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, Xiang-
Rui Wang, and Chih-Jen Lin. 2008. Liblinear: A

library for large linear classification. The Journal of
Machine Learning Research, 9:1871–1874.

Michel Galley and Christopher D Manning. 2009.
Quadratic-time dependency parsing for machine
translation. In Proceedings of the Joint Confer-
ence of the 47th Annual Meeting of the ACL and the
4th International Joint Conference on Natural Lan-
guage Processing of the AFNLP: Volume 2-Volume
2, pages 773–781. Association for Computational
Linguistics.

Johan Hall, Jens Nilsson, Joakim Nivre, Gülşen Eryiit,
Beáta Megyesi, Mattias Nilsson, and Markus Saers.
2007. Single malt or blended? a study in multilin-
gual parser optimization. Proceedings of the CoNLL
Shared Task of EMNLP-CoNLL 2007, pages 933–
939.

Rebecca Hwa. 2004. Sample selection for statistical
parsing. Computational Linguistics, 30(3):253–276.

Shunsuke Ihara. 1993. Information theory for contin-
uous system, volume 2. World Scientific Publishing
Company.

Christopher D Manning, Prabhakar Raghavan, and
Hinrich Schütze. 2008. Introduction to information
retrieval, volume 1. Cambridge University Press
Cambridge.

Ryan T McDonald and Joakim Nivre. 2007. Charac-
terizing the errors of data-driven dependency parsing
models. In EMNLP-CoNLL, pages 122–131.

Ryan McDonald and Fernando Pereira. 2006. Online
learning of approximate dependency parsing algo-
rithms. In Proceedings of EACL, volume 6, pages
81–88.

Ryan McDonald, Fernando Pereira, Kiril Ribarov, and
Jan Hajič. 2005. Non-projective dependency pars-
ing using spanning tree algorithms. In Proceedings
of the conference on Human Language Technology
and Empirical Methods in Natural Language Pro-
cessing, pages 523–530. Association for Computa-
tional Linguistics.

Avihai Mejer and Koby Crammer. 2012. Are you
sure?: confidence in prediction of dependency tree
edges. In Proceedings of the 2012 Conference of
the North American Chapter of the Association for
Computational Linguistics: Human Language Tech-
nologies, pages 573–576. Association for Computa-
tional Linguistics.

Jens Nilsson, Sebastian Riedel, and Deniz Yuret. 2007.
The conll 2007 shared task on dependency parsing.
In Proceedings of the CoNLL Shared Task Session of
EMNLP-CoNLL, pages 915–932. sn.

Joakim Nivre and Jens Nilsson. 2005. Pseudo-
projective dependency parsing. In Proceedings of
the 43rd Annual Meeting on Association for Compu-
tational Linguistics, pages 99–106. Association for
Computational Linguistics.

1241



Joakim Nivre and Mario Scholz. 2004. Determinis-
tic dependency parsing of english text. In Proceed-
ings of the 20th international conference on Compu-
tational Linguistics, page 64. Association for Com-
putational Linguistics.

Joakim Nivre, Johan Hall, Jens Nilsson, Gülşen Eryiit,
and Svetoslav Marinov. 2006. Labeled pseudo-
projective dependency parsing with support vector
machines. In Proceedings of the Tenth Confer-
ence on Computational Natural Language Learning,
pages 221–225. Association for Computational Lin-
guistics.

Joakim Nivre, Johan Hall, Jens Nilsson, Atanas
Chanev, Gülsen Eryigit, Sandra Kubler, Svetoslav
Marinov, and Erwin Marsi. 2007. Maltparser:
A language-independent system for data-driven de-
pendency parsing. Natural Language Engineering,
13(2):95.

John Platt et al. 1999. Probabilistic outputs for sup-
port vector machines and comparisons to regularized
likelihood methods. Advances in large margin clas-
sifiers, 10(3):61–74.

Martin Popel, David Mareček, Nathan Green, and
Zdeněk Žabokrtskỳ. 2011. Influence of parser
choice on dependency-based mt. In Proceedings of
the Sixth Workshop on Statistical Machine Transla-
tion, pages 433–439. Association for Computational
Linguistics.

Dipti Misra Sharma, Prashanth Mannem, Joseph van-
Genabith, Sobha Lalitha Devi, Radhika Mamidi, and
Ranjani Parthasarathi, editors. 2012. Proceedings
of the Workshop on Machine Translation and Pars-
ing in Indian Languages. The COLING 2012 Orga-
nizing Committee, Mumbai, India, December.

Karan Singla, Aniruddha Tammewar, Naman Jain, and
Sambhav Jain. 2012. Two-stage approach for
hindi dependency parsing using maltparser. Train-
ing, 12041(268,093):22–27.

Min Tang, Xiaoqiang Luo, and Salim Roukos. 2002.
Active learning for statistical natural language pars-
ing. In Proceedings of the 40th Annual Meeting
on Association for Computational Linguistics, pages
120–127. Association for Computational Linguis-
tics.

Ting-Fan Wu, Chih-Jen Lin, and Ruby C Weng. 2004.
Probability estimates for multi-class classification
by pairwise coupling. The Journal of Machine
Learning Research, 5:975–1005.

Hiroyasu Yamada and Yuji Matsumoto. 2003. Statis-
tical dependency analysis with support vector ma-
chines. In Proceedings of 8th International Work-
shop on Parsing Technologies, pages 195–206.

Yue Zhang and Stephen Clark. 2008. A tale of
two parsers: investigating and combining graph-
based and transition-based dependency parsing us-
ing beam-search. In Proceedings of the Conference

on Empirical Methods in Natural Language Pro-
cessing, pages 562–571. Association for Computa-
tional Linguistics.

1242


