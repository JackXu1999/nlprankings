



















































UW-FinSent at SemEval-2017 Task 5: Sentiment Analysis on Financial News Headlines using Training Dataset Augmentation


Proceedings of the 11th International Workshop on Semantic Evaluations (SemEval-2017), pages 872–876,
Vancouver, Canada, August 3 - 4, 2017. c©2017 Association for Computational Linguistics

UW-FinSent at SemEval-2017 Task 5:
Sentiment Analysis on Financial News Headlines

using Training Dataset Augmentation

Vineet John
University of Waterloo

vineet.john@uwaterloo.ca

Olga Vechtomova
University of Waterloo

ovechtomova@uwaterloo.ca

Abstract

This paper discusses the approach taken
by the UWaterloo team to arrive at a solu-
tion for the Fine-Grained Sentiment Anal-
ysis problem posed by Task 5 of SemEval
2017. The paper describes the document
vectorization and sentiment score predic-
tion techniques used, as well as the de-
sign and implementation decisions taken
while building the system for this task.
The system uses text vectorization mod-
els, such as N-gram, TF-IDF and para-
graph embeddings, coupled with regres-
sion model variants to predict the senti-
ment scores. Amongst the methods exam-
ined, unigrams and bigrams coupled with
simple linear regression obtained the best
baseline accuracy. The paper also explores
data augmentation methods to supplement
the training dataset. This system was de-
signed for Subtask 2 (News Statements
and Headlines).

1 Introduction

The goal of this SemEval task is to identify fine-
grained levels of sentiment polarity in financial
news headlines and microblog posts. Specifically,
the task aims at identifying bullish (optimistic)
sentiment, expressing the belief that the stock
price will increase, and bearish (pessimistic) sen-
timent, expressing the belief that the stock price
will decline. The expressed sentiment is quan-
tified as floating point values in the range of -1
(very negative/bearish) to 1 (very positive/bullish),
with 0 denoting neutral sentiment. (Cortis et al.,
2017). This paper describes our system developed
for subtask 2 (News Statements and Headlines).

While developing the system for this subtask,
we systematically evaluated a number of alterna-

tive solutions for each step in the pipeline. Specif-
ically, we investigated different document vector-
ization approaches, such as N-gram models, TF-
IDF and paragraph vectors. A number of regres-
sion models were evaluated, namely, Simple Lin-
ear Regression, Support Vector Regression and
XGBoost Linear Regression.

One of the challenges with performing senti-
ment analysis in the financial domain is scarcity
of training data. We explored different approaches
to augment the training data provided by the task
organizers with training data from other sources
in the financial domain, as well as using out-of-
domain sentiment resources.

2 Approach

The overall approach to predicting the sentiment
of the test dataset headlines is detailed below.

• Pre-Processing & Cleaning
This step is needed to simplify and sanitize
the input set of headlines. In the context of
this task, since the headlines were short snip-
pets ranging from 5 to 15 words in length,
the only pre-processing done was replacing
the name of the organization being spoken of
in the headlines, with a generic organization
name, to reduce the feature space.

• Text Vectorization
The objective is to vectorize the textual con-
tent of the headlines into a numeric repre-
sentation that a statistical learning model can
then be trained on. N-gram models, TF-
IDF and Paragraph Vector implementations
were explored for this purpose. N-gram mod-
els generally performed the best on the trial
dataset, followed by TF-IDF, and Paragraph
Vectors. Of the different N-gram configura-
tions experimented with, word N-grams that

872



used a combination of unigrams and bigrams
achieved the best baseline scores. These tech-
niques are further discussed in Section 3.

• Statistical Model Learning
The objective is to use the vector representa-
tions of the headlines as features and learn a
model to predict the sentiment scores. Simple
Linear Regression, Support Vector Regres-
sion and XGBoost Linear Regression were
the learning methods that were used. The lin-
ear regression methods consistently outper-
formed Support Vector Regression and XG-
Boost regression in experiments on the train-
ing dataset. These techniques are discussed
in Section 4.

3 Document Vectorization

Document vectorization is needed to convert the
text content of the SemEval headlines into a nu-
meric vector representation that can be utilized as
features, which can then be used to train a machine
learning model on. The methods for vectorization
used are listed in the subsections below.

3.1 N-gram Model

For the purpose of this task, a vectorizer imple-
mentation using Scikit-Learn (Pedregosa et al.,
2011) was used to obtain vector representations
of the SemEval headlines, since they have been
proven to be an effective representation of tex-
tual content for sentiment classification in general
(Wang and Manning, 2012).

3.2 TF-IDF Model

The TF-IDF implementation in Scikit-Learn (Pe-
dregosa et al., 2011) was used to obtain vector rep-
resentations of the SemEval headlines.

3.3 Paragraph Vector Model

A Paragraph Vector representation model is com-
prised of an unsupervised learning algorithm
that learns fixed-size vector representations for
variable-length pieces of texts such as sentences
and documents (Le and Mikolov, 2014). The
vector representations are learned to predict the
surrounding words in contexts sampled from the
paragraph. In the context of the SemEval head-
lines, the vector representations were learned for
the complete headline.

Two distinct implementations were explored
while attempting to vectorize the headlines using
the Paragraph Vector approach.

• Doc2Vec: A Python library implementation
in Gensim1.

• FastText: A standalone implementation in
C++ (Bojanowski et al., 2016) (Joulin et al.,
2016).

Doc2Vec was the final choice that was opted for
due to the ease of integration into the existing sys-
tem. The paragraph embeddings for Doc2Vec are
trained using the SemEval training headlines cor-
pus.

4 Regression Models

Three different regression implementations were
used to train models to predict the sentiment
scores of the headlines:

• Simple Linear Regression
This is the standard version of linear regres-
sion that simply learns the weights for the
feature vector that minimize the cost func-
tion, which is represented as a Euclidean loss
function.

• Support Vector Regression
The idea of SVR is based on the computation
of a linear regression function in a high di-
mensional feature space where the input data
is mapped using a non-linear function (Basak
et al., 2007).

Instead of minimizing the observed training
error, Support Vector Regression (SVR) at-
tempts to minimize the generalization error
bound so as to achieve generalized perfor-
mance.

• XGBoost Regression
This is an ensemble method for regression
that coalesces several ‘weak’ learners into
a single ‘strong’ learner by iteratively min-
imizing the least squares error or Euclidean
loss incurred by the cost function (Chen and
Guestrin, 2016).

The hyper-parameters applicable are the reg-
ularization parameter (λ) and the gradient de-
scent step-size / learning rate (α).

1https://radimrehurek.com/gensim/models/doc2vec.html

873



Vectorization Method Learning Model R2 Score Cosine Similarity
Unigrams & Bigrams Simple Linear Regression 0.38 0.63
Unigrams & Bigrams Support Vector Regression 0.38 0.63
Unigrams & Bigrams XGBoost Regression 0.21 0.50

TF-IDF Simple Linear Regression -0.10 0.50
TF-IDF Support Vector Regression 0.38 0.63
TF-IDF XGBoost Regression 0.19 0.47

Doc2Vec Simple Linear Regression -4.69 0.04
Doc2Vec Support Vector Regression -0.05 0.08
Doc2Vec XGBoost Regression -0.06 0.06

Table 1: Experimental Results

The implementation library utilized for the Sim-
ple Linear Regression and Support Vector Regres-
sion techniques is Scikit-Learn (Pedregosa et al.,
2011), whereas the XGB Python library was used
for the XGBoost regression implementation.

5 Training Dataset Augmentation

A few different datasets were used to train the
models on, in an attempt to identify the best rep-
resentative training set. The dataset augmentation
strategies used are enumerated below.

• Article Content Expansion
To increase the number of features to train on,
it was decided to retrieve the full text con-
tent of the articles corresponding to the arti-
cle headlines. This was achieved by creating
an application to search for the article head-
lines that were part of the training set using
an online search engine, and to retrieve the
full-text of the article by scraping the content
from the source websites.

This application is implemented in Java and
is open-source2. The implementation can
be extended to augment any set of headlines
with the corresponding article content.

The assumption made here is that the senti-
ment expressed in the article headline suffi-
ciently proxies the sentiment in the actual ar-
ticle content.

• Amazon Product Reviews
This corpus is a set of Amazon product re-
views3, each consisting of the review text and

2https://github.com/v1n337/news-article-extractor
3http://jmcauley.ucsd.edu/data/amazon/

a star rating on the scale of 1-5. To normal-
ize the dataset, the rating scores 1 & 2 are as-
sumed to be associated with negative reviews,
3 with neutral and 4 & 5 with positive re-
views. This score range was then mapped to
a -1 to 1 scale to match the sentiment scores
of the training data. In total, 100, 000 docu-
ments from this dataset were used to augment
the existing training dataset.

• Financial Phrasebank
This dataset is specific to the financial do-
main and is manually annotated (Malo et al.,
2014). It is comprised of a set of financial
snippets from stock market related news that
have been annotated with the classes positive,
negative and neutral.

To normalize the labels, neutral was assigned
a sentiment score of 0 and experiments were
run for positive ∈ (1, 0.5) and negative ∈
(−1,−0.5).

None of the above strategies proved to be a good
augmentation of the existing data, since their ad-
dition to the training datasets did not show any im-
provements in the overall cross-validated accuracy
score.

6 System Implementation

The entire system was coded in Python with the
use of the Scikit-Learn (Pedregosa et al., 2011),
XGB and Gensim libraries. This includes a frame-
work for automated testing of accuracy scores to
arrive at the best hyper-parameters to be used for
unigram & bigram word count combinations, as
well as Doc2Vec hyper-parameters.

The system implementation includes all the plu-
gins pertaining to the different document vector-

874



Vectorization Method Learning Model Cosine Similarity
Unigrams & Bigrams Simple Linear Regression 0.644
Unigrams & Bigrams XGBoost Regression 0.547

Table 2: SemEval Task 5 Submissions

ization techniques and statistical learning tech-
niques discussed in sections 3 and 4 respectively.

The code is open source4 and is available to
replicate the results published in this paper along
with the instructions to operate the system.

7 Experimental Results

For arriving at the baseline scores, an exhaustive
set of tests were conducted using each of the docu-
ment vectorization techniques in combination with
the regression techniques described in the previous
sections.

Using the automated test-suite included as part
of the system, it was concluded that the Doc2Vec
model performed best when the number of dimen-
sions (features) of text is around 832 and the learn-
ing algorithm completes 40 passes before settling
on a vector representation. It was also concluded,
that a combinations of unigrams & bigrams had
the best baseline accuracy scores for the training
datasets.

The measure of accuracy used was theR2 score,
also called the co-efficient of determination. The
R2 score can be computed using the below for-
mula:

R2 = 1−
∑N

i=1(yi − fi)2∑N
i=1(yi − ȳ)2

where y is the gold set score vector and f is the
predicted score vector, andN is the number of test
samples.

The experimental results for the Training and
the Trial datasets are shown in Table 1. The best
baselines scores seem to favor the simplest vector-
ization model, i.e. unigrams & bigrams.

8 System Evaluation

For the two submissions permitted by SemEval,
the methods used for the submissions made are de-
scribed in Table 2.

The evaluation was done using the task evalua-
tion metric, the cosine score (Cortis et al., 2017).

cosine score = cosine weight ∗ cosine(G,P )
4https://github.com/v1n337/semeval2017-task5

where

cosine(G,P ) =
∑N

i=1Gi ∗ Pi√∑N
i=1Gi

2 ∗
√∑N

i=1 Pi
2

and

cosine weight =
|P |
|G|

and G, P are the gold set scores and the predicted
scores respectively, for N test samples.

The simplest model implemented, using Uni-
grams & Bigrams, combined with Simple Lin-
ear Regression, was what yielded the best per-
formance by the system, with a cosine similarity
score of 0.644.

9 Conclusions and Future Work

This paper has described the UW-FinSent system
developed by the UWaterloo team for Task 5, Sub-
task 2 during SemEval 2017.

The experimental results indicate that the us-
age of simpler techniques like N-gram text vec-
torization and linear regression to predict the
continuous-valued scores achieve better results
than bag-of-words or deep learning feature extrac-
tion techniques.

A recurring topic that needed to be addressed
during the progress on this task was the fact there
there were no reliable datasets that could accu-
rately augment the training set. In the future, we
plan to develop automatic methods for generating
high quality, sentiment-annotated training datasets
for the financial domain.

Acknowledgements

The authors are grateful to the organizers for their
support for this task. The authors would also like
to thank (Malo et al., 2014) for sharing the Finan-
cial Phrasebank Dataset for the purposes of our
evaluation.

References
Debasish Basak, Srimanta Pal, and Dipak Chan-

dra Patranabis. 2007. Support vector regression.
Neural Information Processing-Letters and Reviews
11(10):203–224.

875



Piotr Bojanowski, Edouard Grave, Armand Joulin,
and Tomas Mikolov. 2016. Enriching word vec-
tors with subword information. arXiv preprint
arXiv:1607.04606 .

Tianqi Chen and Carlos Guestrin. 2016. Xgboost: A
scalable tree boosting system. In Proceedings of
the 22Nd ACM SIGKDD International Conference
on Knowledge Discovery and Data Mining. ACM,
pages 785–794.

Keith Cortis, André Freitas, Tobias Dauert, Manuela
Huerlimann, Manel Zarrouk, Siegfried Handschuh,
and Brian Davis. 2017. Semeval-2017 task 5:
Fine-grained sentiment analysis on financial mi-
croblogs and news. In Proceedings of the 11th
International Workshop on Semantic Evaluation
(SemEval-2017). Association for Computational
Linguistics, Vancouver, Canada, pages 517–533.
http://www.aclweb.org/anthology/S17-2089.

Armand Joulin, Edouard Grave, Piotr Bojanowski, and
Tomas Mikolov. 2016. Bag of tricks for efficient text
classification. arXiv preprint arXiv:1607.01759 .

Quoc V Le and Tomas Mikolov. 2014. Distributed rep-
resentations of sentences and documents. In ICML.
volume 14, pages 1188–1196.

Pekka Malo, Ankur Sinha, Pekka Korhonen, Jyrki Wal-
lenius, and Pyry Takala. 2014. Good debt or bad
debt: Detecting semantic orientations in economic
texts. Journal of the Association for Information
Science and Technology 65(4):782–796.

Fabian Pedregosa, Gaël Varoquaux, Alexandre Gram-
fort, Vincent Michel, Bertrand Thirion, Olivier
Grisel, Mathieu Blondel, Peter Prettenhofer, Ron
Weiss, Vincent Dubourg, et al. 2011. Scikit-learn:
Machine learning in python. Journal of Machine
Learning Research 12(Oct):2825–2830.

Sida Wang and Christopher D Manning. 2012. Base-
lines and bigrams: Simple, good sentiment and topic
classification. In Proceedings of the 50th Annual
Meeting of the Association for Computational Lin-
guistics: Short Papers-Volume 2. Association for

Computational Linguistics, pages 90–94.

876


