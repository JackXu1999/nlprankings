





































Overview of the 2nd Workshop on Asian Translation
Toshiaki Nakazawa
Japan Science and

Technology Agency

nakazawa@pa.jst.jp

Hideya Mino
National Institute of

Information and
Communications Technology

hideya.mino@nict.go.jp

Isao Goto
NHK

goto.i-es@nhk.or.jp

Graham Neubig
Nara Institute of

Science and Technology

neubig@is.naist.jp

Sadao Kurohashi
Kyoto University

kuro@i.kyoto-u.ac.jp

Eiichiro Sumita
National Institute of

Information and
Communications Technology
eiichiro.sumita@nict.go.jp

Abstract

This paper presents the results of the
shared tasks from the 2nd workshop
on Asian translation (WAT2015) includ-
ing J↔E, J↔C scientific paper trans-
lation subtasks and C→J, K→J patent
translation subtasks. For the WAT2015,
12 institutions participated in the shared
tasks. About 500 translation results have
been submitted to the automatic evalua-
tion server, and selected submissions were
manually evaluated.

1 Introduction

The Workshop on Asian Translation (WAT) is a
new open evaluation campaign focusing on Asian
languages. Following the success of the previ-
ous workshop WAT2014(Nakazawa et al., 2014),
WAT2015 brings together machine translation re-
searchers and users to try, evaluate, share and dis-
cuss brand-new ideas of machine translation. We
are working toward the practical use of machine
translation among all Asian countries.

For the 2nd WAT, we adopt new transla-
tion subtasks “Chinese-to-Japanese and Korean-
to-Japanese patent translation” in addition to the
subtasks that were conducted in WAT2014.

WAT is unique for the following reasons:

• Open innovation platform
The test data is fixed and open, so evalua-
tions can be repeated on the same data set
to confirm changes in translation accuracy
over time. WAT has no deadline for auto-
matic translation quality evaluation (contin-
uous evaluation), so translation results can be
submitted at any time.

• Domain and language pairs
WAT is the world’s first workshop that
uses scientific papers as the domain, and
Chinese↔Japanese and Korean→Japanese
as language pairs. In the future, we will add
more Asian languages, such as Vietnamese,
Indonesian, Thai, Burmese and so on.

• Evaluation method
Evaluation is done both automatically and
manually. For human evaluation, WAT uses
crowdsourcing, which is low cost and allows
multiple evaluations, as the first-stage evalu-
ation. Also, JPO adequacy evaluation is con-
ducted for the selected submissions accord-
ing to the crowdsourcing evaluation results.

2 Dataset

WAT uses the Asian Scientific Paper Excerpt Cor-
pus (ASPEC)1 and JPO Patent Corpus (JPC) 2 as
the dataset.

2.1 ASPEC

ASPEC is constructed by the Japan Science and
Technology Agency (JST) in collaboration with
the National Institute of Information and Com-
munications Technology (NICT). It consists of a
Japanese-English scientific paper abstract corpus
(ASPEC-JE), which is used for J↔E subtasks, and
a Japanese-Chinese scientific paper excerpt cor-
pus (ASPEC-JC), which is used for J↔C subtasks.
The statistics for each corpus are described in Ta-
ble1.

1http://lotus.kuee.kyoto-u.ac.jp/ASPEC/
2http://lotus.kuee.kyoto-u.ac.jp/WAT/patent/index.html

1
Proceedings of the 2nd Workshop on Asian Translation (WAT2015), pages 1‒28, 

Kyoto, Japan, 16th October 2015. 
2015 Copyright is held by the author(s).



LangPair Train Dev DevTest Test
ASPEC-JE 3,008,500 1,790 1,784 1,812
ASPEC-JC 672,315 2,090 2,148 2,107

Table 1: Statistics for ASPEC.

2.1.1 ASPEC-JE
The training data for ASPEC-JE was constructed
by the NICT from approximately 2 million
Japanese-English scientific paper abstracts owned
by the JST. Because the abstracts are comparable
corpora, the sentence correspondences are found
automatically using the method from (Utiyama
and Isahara, 2007). Each sentence pair is accom-
panied by a similarity score and the field symbol.
The similarity scores are calculated by the method
from (Utiyama and Isahara, 2007). The field sym-
bols are single letters A-Z and show the scientific
field for each document3. The correspondence be-
tween the symbols and field names, along with the
frequency and occurrence ratios for the training
data, are given in the README file from ASPEC-
JE.

The development, development-test and test
data were extracted from parallel sentences from
the Japanese-English paper abstracts owned by
JST that are not contained in the training data.
Each data set contains 400 documents. Further-
more, the data has been selected to contain the
same relative field coverage across each data set.
The document alignment was conducted automat-
ically and only documents with a 1-to-1 alignment
are included. It is therefore possible to restore the
original documents. The format is the same as for
the training data except that there is no similarity
score.

2.1.2 ASPEC-JC
ASPEC-JC is a parallel corpus consisting of
Japanese scientific papers from the literature
database and electronic journal site J-STAGE of
JST that have been translated to Chinese with per-
mission from the necessary academic associations.
The parts selected were abstracts and paragraph
units from the body text, as these contain the high-
est overall vocabulary coverage.

The development, development-test and test
data are extracted at random from documents con-
taining single paragraphs across the entire cor-
pus. Each set contains 400 paragraphs (docu-
ments). Therefore, there are no documents sharing

3http://opac.jst.go.jp/bunrui/index.html

LangPair Train Dev DevTest Test
JPC-CJ 1,000,000 2,000 2,000 2,000
JPC-KJ 1,000,000 2,000 2,000 2,000

Table 2: Statistics for JPC.

the same data across the training, development,
development-test and test sets.

2.2 JPC

JPC was constructed by the Japan Patent Of-
fice (JPO). It consists of a Chinese-Japanese
patent description corpus (JPC-CJ) and Korean-
Japanese patent description corpus (JPC-KJ) with
four sections, which are Chemistry, Electricity,
Mechanical engineering, and Physics, based on
International Patent Classification (IPC). Each
corpus is separated into training, development,
development-test and test data, which are sentence
pairs. This corpus was used for patent subtasks
C→J and K→J. The statistics for each corpus are
described in Table2.

The Sentence pairs in each data were randomly
extracted from a description part of comparable
patent documents under the condition that a sim-
ilarity score between sentences is greater than or
equal to the threshold value 0.05. The simi-
larity score was calculated by the method from
(Utiyama and Isahara, 2007) as with ASPEC. Doc-
ument pairs which were used to extract sentence
pairs for each data were not used for the other
data. Furthermore, the sentence pairs was ex-
tracted to be same number among the four sec-
tions. The maximize number of sentence pairs
which are extracted from one document pair was
limited to 60 for training data and 20 for the devel-
opent, development-test and test data. The train-
ing data for JPC-CJ was made with sentence pairs
of Chinese-Japanese patent documents published
in 2012. For JPC-KJ, the training data was ex-
tracted from sentence pairs of Korean-Japanese
patent documents published in 2011 and 2012.
The development, development-test and test data
for JPC-CJ and JPC-KJ were respectively made
with 100 patent documents published in 2013.

3 Baseline Systems

Human evaluations were conducted as pairwise
comparisons between the translation results for a
specific baseline system and translation results for

2



each participant’s system. That is, the specific
baseline system was the standard for human eval-
uation. A phrase-based statistical machine trans-
lation (SMT) system was adopted as the specific
baseline system at WAT 2015, which is the same
system as that at WAT 2014.

In addition to the results for the baseline phrase-
based SMT system, we produced results for the
baseline systems that consisted of a hierarchical
phrase-based SMT system, a string-to-tree syntax-
based SMT system, a tree-to-string syntax-based
SMT system, seven commercial rule-based ma-
chine translation (RBMT) systems, and two online
translation systems. The SMT baseline systems
consisted of publicly available software, and the
procedures for building the systems and for trans-
lating using the systems were published on the
WAT web page4. We used Moses (Koehn et al.,
2007; Hoang et al., 2009) as the implementation
of the baseline SMT systems. The Berkeley parser
(Petrov et al., 2006) was used to obtain syntactic
annotations. The baseline systems are shown in
Table 3.

The commercial RBMT systems and the online
translation systems were operated by the organiz-
ers. We note that these RBMT companies and on-
line translation companies did not submit them-
selves. Because our objective is not to compare
commercial RBMT systems or online translation
systems from companies that did not themselves
participate, the system IDs of these systems are
anonymous in this paper.

3.1 Training Data
We used the following data for training the SMT
baseline systems.

• Training data for the language model: All of
the target language sentences in the parallel
corpus.

• Training data for the translation model: Sen-
tences that were 40 words or less in length.
(For Japanese–English training data, we only
used train-1.txt, which consists of one mil-
lion parallel sentence pairs with high similar-
ity scores.)

• Development data for tuning: All of the de-
velopment data.

3.2 Common Settings for Baseline SMT
We used the following tools for tokenization.

4http://lotus.kuee.kyoto-u.ac.jp/WAT/

• Juman version 7.05 for Japanese segmenta-
tion.

• Stanford Word Segmenter version 2014-01-
046 (Chinese Penn Treebank (CTB) model)
for Chinese segmentation.

• The Moses toolkit for English tokenization.
• Mecab-ko7 for Korean segmentation.

To obtain word alignments, GIZA++ and grow-
diag-final-and heuristics were used. We used 5-
gram language models with modified Kneser-Ney
smoothing, which were built using a tool in the
Moses toolkit (Heafield et al., 2013).

3.3 Phrase-based SMT

We used the following Moses configuration for the
phrase-based SMT system.

• distortion-limit = 20 except for KJ and
distortion-limit = 0 for KJ

• msd-bidirectional-fe lexicalized reordering
• Phrase score option: GoodTuring

The default values were used for the other system
parameters.

3.4 Hierarchical Phrase-based SMT

We used the following Moses configuration for the
hierarchical phrase-based SMT system.

• max-chart-span = 1000
• Phrase score option: GoodTuring

The default values were used for the other system
parameters.

3.5 String-to-Tree Syntax-based SMT

We used the Berkeley parser to obtain target lan-
guage syntax. We used the following Moses con-
figuration for the string-to-tree syntax-based SMT
system.

• max-chart-span = 1000
• Phrase score option: GoodTuring
• Phrase extraction options: MaxSpan = 1000,

MinHoleSource = 1, and NonTermConsec-
Source.

The default values were used for the other system
parameters.

5http://nlp.ist.i.kyoto-u.ac.jp/EN/index.php?JUMAN
6http://nlp.stanford.edu/software/segmenter.shtml
7https://bitbucket.org/eunjeon/mecab-ko/

3



ASPEC JPC
System ID System Type JE EJ JC CJ CJ KJ
SMT Phrase Moses’ Phrase-based SMT SMT ✓ ✓ ✓ ✓ ✓ ✓
SMT Hiero Moses’ Hierarchical Phrase-based SMT SMT ✓ ✓ ✓ ✓ ✓ ✓
SMT S2T Moses’ String-to-Tree Syntax-based SMT and Berkeley parser SMT ✓ ✓
SMT T2S Moses’ Tree-to-String Syntax-based SMT and Berkeley parser SMT ✓ ✓ ✓
RBMT X The Honyaku V15 (Commercial system) RBMT ✓ ✓
RBMT X ATLAS V14 (Commercial system) RBMT ✓ ✓
RBMT X PAT-Transer 2009 (Commercial system) RBMT ✓ ✓
RBMT X J-Beijing 7 (Commercial system) RBMT ✓ ✓ ✓
RBMT X Hohrai 2011 (Commercial system) RBMT ✓ ✓ ✓
RBMT X J Soul 9 (Commercial system) RBMT ✓
RBMT X Korai 2011 (Commercial system) RBMT ✓
Online X Google translate (August, 2015) (SMT) ✓ ✓ ✓ ✓ ✓ ✓
Online X Bing translator (August and September, 2015) (SMT) ✓ ✓ ✓ ✓ ✓ ✓

Table 3: Baseline Systems

3.6 Tree-to-String Syntax-based SMT
We used the Berkeley parser to obtain source lan-
guage syntax. We used the following Moses con-
figuration for the baseline tree-to-string syntax-
based SMT system.

• max-chart-span = 1000
• Phrase score option: GoodTuring
• Phrase extraction options: MaxSpan = 1000,

MinHoleSource = 1, MinWords = 0,
NonTermConsecSource, and AllowOnlyU-
nalignedWords.

The default values were used for the other system
parameters.

4 Automatic Evaluation

4.1 Procedure for Calculating Automatic
Evaluation Score

We calculated automatic evaluation scores for the
translation results by applying two popular met-
rics: BLEU (Papineni et al., 2002) and RIBES
(Isozaki et al., 2010). BLEU scores were cal-
culated using multi-bleu.perl distributed with the
Moses toolkit (Koehn et al., 2007); RIBES scores
were calculated using RIBES.py version 1.02.4 8.
All scores for each task were calculated using one
reference. Before the calculation of the automatic
evaluation scores, the translation results were tok-
enized with word segmentation tools for each lan-
guage.

For Japanese segmentation, we used three dif-
ferent tools: Juman version 7.0 (Kurohashi et al.,
1994), KyTea 0.4.6 (Neubig et al., 2011) with Full
SVM model 9 and MeCab 0.996 (Kudo, 2005)

8http://www.kecl.ntt.co.jp/icl/lirg/ribes/index.html
9http://www.phontron.com/kytea/model.html

with IPA dictionary 2.7.0 10. For Chinese segmen-
tation we used two different tools: KyTea 0.4.6
with Full SVM Model in MSR model and Stanford
Word Segmenter version 2014-06-16 with Chi-
nese Penn Treebank (CTB) and Peking University
(PKU) model 11 (Tseng, 2005). For Korean seg-
mentation we used mecab-ko 12. For English seg-
mentation we used tokenizer.perl 13 in the Moses
toolkit.

Detailed procedures for the automatic evalua-
tion are shown on the WAT2015 evaluation web
page 14.

4.2 Automatic Evaluation System
The participants submit translation results via
an automatic evaluation system deployed on the
WAT2015 web page, which automatically gives
evaluation scores for the uploaded results. Fig-
ure 1 shows the submission interface for partici-
pants. The system requires participants to provide
the following information when they upload trans-
lation results:

• Subtask:

– Scientific papers subtask (J ↔ E, J ↔
C);

– Patents subtask (C → J , K → J);

• Method (SMT, RBMT, SMT and RBMT,
EBMT, Other);

10http://code.google.com/p/mecab/downloads/detail?
name=mecab-ipadic-2.7.0-20070801.tar.gz

11http://nlp.stanford.edu/software/segmenter.shtml
12https://bitbucket.org/eunjeon/mecab-ko/
13https://github.com/moses-smt/mosesdecoder/tree/

RELEASE-2.1.1/scripts/tokenizer/tokenizer.perl
14http://lotus.kuee.kyoto-u.ac.jp/WAT/evaluation/index.html

4



Fi
gu

re
1:

T
he

su
bm

is
si

on
w

eb
pa

ge
fo

rp
ar

tic
ip

an
ts

5



• Use of other resources in addition to ASPEC
or JPC;

• Permission to publish the automatic evalua-
tion scores on the WAT2015 web page.

The server for the system stores all submitted in-
formation, including translation results and scores,
although participants can confirm only the infor-
mation that they uploaded. Information about
translation results that participants permit to be
published is disclosed on the web page. In addi-
tion to submitting translation results for automatic
evaluation, participants submit the results for hu-
man evaluation using the same web interface. This
automatic evaluation system will remain available
even after WAT2015. Anybody can register to use
the system on the registration web page 15.

5 Human Evaluation

In WAT2015, we conducted 2 kinds of human
evaluations: pairwise crowdsourcing evaluation
and JPO adequacy evaluation.

5.1 Pairwise Crowdsourcing Evaluation

The pairwise crowdsourcing evaluation is the
same as the last year. We used Lancers as the
crowdsourcing platform. There are two reasons of
choosing Lancers. One is that we can set the cat-
egory of the crowdsourcing task (’Translation’ in
this case). We can reach the appropriate workers
by choosing the appropriate categories. The other
reason is that we can assign the task to identity-
verified workers. This function guaranteed the
quality of the workers. These two advantages en-
sure a high evaluation quality.

We used the same sentences as the last year for
the pairwise crowdsourcing evaluation. We ran-
domly chose documents from the Test set from
the ASPEC data, for a total of 400 sentence pairs
for JE and JC. We excluded documents contain-
ing sentences longer than 100 Japanese characters.
Each submission is compared with the baseline
translation (Phrase-based SMT, described in Sec-
tion 3) and given a Crowd score16.

5.1.1 Pairwise Evaluation of Sentences
We conducted pairwise evaluation of each of the
400 test sentences. The input sentence and two
translations (the baseline and a submission) are

15http://lotus.kuee.kyoto-u.ac.jp/WAT/registration/index.html
16It was called HUMAN score in WAT2014.

shown to the workers, and the workers are asked to
judge which of the translation is better, or if they
are of the same quality. The order of the two trans-
lations are at random. Figure 2 illustrates the eval-
uation.

5.1.2 Voting
The crowdsourcing workers are not specialists,
and thus the quality of the judgments are not nec-
essarily precise. To guarantee the quality of the
evaluations, each sentence is evaluated by 5 dif-
ferent workers and the final decision is made de-
pending on the 5 judgements17. We define each
judgement ji(i = 1, · · · , 5) as:

ji =


1 if better than the baseline
−1 if worse than the baseline
0 if the quality is the same

The final decision D is defined as follows using
S =

∑
ji:

D =


win (S ≥ 2)
loss (S ≤ −2)
tie (otherwise)

5.1.3 Crowd Score Calculation
Suppose that W is the number of wins compared
to the baseline, L is the number of losses and T
is the number of ties. The Crowd score can be
calculated by the following formula:

Crowd = 100× W − L
W + L+ T

From the definition, the Crowd score ranges be-
tween -100 and 100.

5.1.4 Confidence Interval Estimation
There are several ways to estimate a confidence
interval. We chose to use bootstrap resampling
(Koehn, 2004) to estimate the 95% confidence in-
terval. The procedure is as follows:

1. randomly select 300 sentences from the 400
human evaluation sentences, and calculate
the Crowd score of the selected sentences

2. iterate the previous step 1000 times and get
1000 Crowd scores

3. sort the 1000 scores and estimate the 95%
confidence interval by discarding the top 25
scores and the bottom 25 scores

17We used 3 judgements in WAT2014.

6



Figure 2: Illustration of the crowdsourcing evaluation. The workers are asked to judge which translation
is better, or the same.

5.1.5 Cost
A major benefit of using crowdsourcing is that it
reduces the cost of evaluations. In WAT2015, one
judgment costs 5 JPY. The evaluation of a submis-
sion requires 5 (judgments) × 400 (sentence pairs)
= 2,000 judgments and costs 5 × 2,000 = 10,000
JPY. The time for the evaluation differs depend-
ing on the translation direction. On average, one
evaluation takes a couple of days.

5.2 JPO Adequacy Evaluation

The participants’ systems, which achieved the
top 3 highest scores among the pairwise crowd-
sourcing evaluation results of each subtask, were
also evaluated with the JPO adequacy evaluation.
The JPO adequacy evaluation was carried out by
translation experts with a quality evaluation cri-
terion for translated patent documents which the
Japanese Patent Office (JPO) decided. In addi-
tion to the top 3 systems, the Sense 1 system of
the JPC-KJ subtask, which was the lower score on
the pairwise crowdsourcing evaluation despite the
high score on the automatic evaluation, was eval-
uated exceptionally. For each system, two anno-
tators evaluate the test sentences to guarantee the
quality.

5.2.1 Evaluation of Sentences
The number of test sentences for the JPO adequacy
evaluation is 200. The 200 test sentences were ran-
domly selected from the 400 test sentences of the
pairwise evaluation. The test sentence include the
input sentence, the submitted system’s translation
and the reference translation.

5 All important information is transmitted correctly.
(100%)

4 Almost all important information is transmitted cor-
rectly. (80%–)

3 More than half of important information is trans-
mitted correctly. (50%–)

2 Some of important information is transmitted cor-
rectly. (20%–)

1 Almost all important information is NOT transmit-
ted correctly. (–20%)

Table 4: The JPO adequacy criterion

5.2.2 Evaluation Criterion

Table 4 shows the JPO adequacy criterion from
5 to 1. The evaluation is performed subjectively.
“Important information” represents the technical
factors and their relationships. The degree of
importance of each element is also considered
to evaluate. The percentages in each grade are
rough indications for the transmission degree of
the source sentence meanings. The detailed cri-
terion can be found on the JPO document (in
Japanese) 18.

6 Participants List

Table 5 shows the list of participants for
WAT2015. This includes not only Japanese orga-
nizations, but also some organizations from out-
side Japan. 12 teams submitted one or more
translation results to the both automatic evaluation
server and human evaluation.

18http://www.jpo.go.jp/shiryou/toushin/chousa/
tokkyohonyaku hyouka.htm

7



ASPEC JPC
Team ID Organization JE EJ JC CJ CJ KJ
NAIST (Neubig et al., 2015) Nara Institute of Science and Technology ✓ ✓ ✓ ✓
Kyoto-U (Richardson et al., 2015) Kyoto University ✓ ✓ ✓ ✓ ✓
WEBLIO MT (Zhu, 2015) Weblio, Inc. ✓
TMU (Matsuo et al., 2015) Tokyo Metropolitan University ✓
BJTUNLP (Shan et al., 2015) Beijing Jiaotong University ✓
Sense (Tan et al., 2015) Saarland University & Nanyang Technological University ✓ ✓ ✓
NICT (Ding et al., 2015) National Institute of Information and Communication Technology ✓ ✓
TOSHIBA (Sonoh and Kinoshita, 2015) Toshiba Corporation ✓ ✓ ✓ ✓ ✓ ✓
WASUIPS (Yang et al., 2015) Waseda University ✓
naver (Lee et al., 2015) NAVER Corporation ✓ ✓
EHR (Ehara, 2015) Ehara NLP Research Laboratory ✓ ✓ ✓ ✓
ntt (Sudoh and Nagata, 2015) NTT Communication Science Laboratories ✓

Table 5: List of participants who submitted translation results to WAT2015 and their participation in each
subtasks.

7 Evaluation Results

In this section, the evaluation results for WAT2015
are reported from several perspectives. Some of
the results for both automatic and human evalu-
ations are also accessible at the WAT2015 web-
site19.

7.1 Official Automatic Evaluation Results

Figures 3 and 4 show the official automatic evalua-
tion results for the representative submissions and
baseline systems. The automatic evaluation results
for all the submissions are shown in Section Ap-
pendix A.

7.2 Official Crowdsourcing Evaluation
Results

Crowd Score
Figure 5 shows the official crowdsourcing evalu-
ation results. The error bars in the figures show
the 95% confidence interval (see Section 5.1.4).
Note that overlapping error bars between two sub-
missions do not necessarily mean that there is no
significant difference. If an error bar crosses the
x-axis (Crowd score = 0), it means that there is no
significant difference between the submission and
the baseline (SMT Phrase).

Statistical Significance Testing between
Submissions
Tables 6, 7, 8, 9, 10 and 11 show the results of sta-
tistical significance testing for the JE, EJ, JC and
CJ translations respectively where all the pairs of
submissions are tested. ≫, ≫ and > mean that
the system in the row is better than the system in

19http://lotus.kuee.kyoto-u.ac.jp/WAT/evaluation/index.html

the column at a significance level of p < 0.01, 0.05
and 0.1 respectively. Testing is also done by the
bootstrap resampling as follows:

1. randomly select 300 sentences from the 400
crowdsourcing evaluation sentences, and cal-
culate the Crowd scores on the selected sen-
tences for both systems

2. iterate the previous step 1000 times and count
the number of wins (W ), losses (L) and ties
(T )

3. calculate p = LW+L

Inter-annotator Agreement
To assess the reliability of agreement between the
crowdsourcing workers, we calculated the Fleiss’
κ (Fleiss and others, 1971) values. The results are
shown in Table 12. We can see that the κ val-
ues are larger for X → J translations than for J
→ X translations. This may be because we used a
Japanese crowdsourcing service for the evaluation
and so the majority of the crowdsourcing workers
are Japanese, and the evaluation of one’s mother
tongue is much easier than for other languages in
general. Also, K → J evaluations seem to be more
consistent than the other directions. This may be
because the quality of K → J translations are much
better than the other directions.

Correlation between Automatic and
Crowdsourcing Evaluations
Figure 6 and 7 show the correlations between the
automatic evaluation measures (BLEU/RIBES)
and the Crowd score.

8



K
yo

to
-U

2

TO
SH

IB
A

2

TO
SH

IB
A

1

R
B

M
T

D

K
yo

to
-U

1

N
IC

T
1

N
A

IS
T

2

SM
T

S2
T

N
IC

T
2

O
nl

in
e

D

Se
ns

e
1

Se
ns

e
2

T
M

U

NAIST 1 - ≫ ≫ ≫ ≫ ≫ ≫ ≫ ≫ ≫ ≫ ≫ ≫
Kyoto-U 2 ≫ ≫ ≫ ≫ ≫ ≫ ≫ ≫ ≫ ≫ ≫ ≫
TOSHIBA 2 ≫ ≫ ≫ ≫ ≫ ≫ ≫ ≫ ≫ ≫ ≫
TOSHIBA 1 ≫ ≫ ≫ ≫ ≫ ≫ ≫ ≫ ≫ ≫
RBMT D - - ≫ ≫ ≫ ≫ ≫ ≫ ≫
Kyoto-U 1 - ≫ ≫ ≫ ≫ ≫ ≫ ≫
NICT 1 ≫ ≫ ≫ ≫ ≫ ≫ ≫
NAIST 2 ≫ ≫ ≫ ≫ ≫ ≫
SMT S2T - ≫ ≫ ≫ ≫
NICT 2 ≫ ≫ ≫ ≫
Online D ≫ ≫ ≫
Sense 1 ≫ ≫
Sense 2 ≫

Table 6: Statistical significance testing of the ASPEC-JE Crowd scores.

W
E

B
L

IO
M

T
1

na
ve

r2

K
yo

to
-U

2

N
A

IS
T

2

na
ve

r1

W
E

B
L

IO
M

T
2

K
yo

to
-U

1

TO
SH

IB
A

O
nl

in
e

A

E
H

R

SM
T

T
2S

R
B

M
T

B

Se
ns

e
2

Se
ns

e
1

NAIST 1 ≫ ≫ ≫ ≫ ≫ ≫ ≫ ≫ ≫ ≫ ≫ ≫ ≫ ≫
WEBLIO MT 1 - > ≫ ≫ ≫ ≫ ≫ ≫ ≫ ≫ ≫ ≫ ≫
naver 2 - > ≫ ≫ ≫ ≫ ≫ ≫ ≫ ≫ ≫ ≫
Kyoto-U 2 - > ≫ ≫ ≫ ≫ ≫ ≫ ≫ ≫ ≫
NAIST 2 - ≫ ≫ ≫ ≫ ≫ ≫ ≫ ≫ ≫
naver 1 ≫ ≫ ≫ ≫ ≫ ≫ ≫ ≫ ≫
WEBLIO MT 2 > > ≫ ≫ ≫ ≫ ≫ ≫
Kyoto-U 1 - ≫ ≫ ≫ ≫ ≫ ≫
TOSHIBA ≫ ≫ ≫ ≫ ≫ ≫
Online A - ≫ ≫ ≫ ≫
EHR - ≫ ≫ ≫
SMT T2S ≫ ≫ ≫
RBMT B ≫ ≫
Sense 2 ≫

Table 7: Statistical significance testing of the ASPEC-EJ Crowd scores.

7.3 Chronological Evaluation

Figure 8 shows the chronological evaluation re-
sults of ASPEC. The x-axis indicates the BLEU
score and the y-axis indicates the Crowd score.
Note that the first 3 annotations among 5 by
the crowdsourcing were used, and the decision
for each sentence is made by the same criteria
in WAT2014 for calculating the Crowd score of
WAT2015 submissions.

7.4 Official JPO Adequacy Evaluation
Results

Table 13 and Figure 9 show the JPO Adequacy
Evaluation results for the selected submissions.
The weights for the weighted κ (Cohen, 1968) is
defined as |Evaluation1− Evaluation2|/4.

As described in Section 5.2, we selected top 3

teams per subtask for the JPO adequacy evaluation
according to the pairwise crowdsourcing evalua-
tion results. However, for JPC K → J, we excep-
tionally selected 4 teams including the top 3 teams
and the Sense team. This is because the Sense
team achieved notably better automatic evaluation
scores, but got the worst crowdsourcing evaluation
result, and we thought this is a very interesting
case.

Figure 10 shows the summary of automatic
and human evaluations for the selected submis-
sions. We can see that all of Crowd, BLEU and
RIBES scores partially correlate to the JPO ade-
quacy score, but none of them perfectly correlates.
Especially, for JPC-KJ, both BLEU and RIBES
failed to correctly evaluate the quality of Sense
team.

9



K
yo

to
-U

1

K
yo

to
-U

2

SM
T

S2
T

N
A

IS
T

1

N
A

IS
T

2

TO
SH

IB
A

1

R
B

M
T

B

O
nl

in
e

D

TOSHIBA 2 - ≫ ≫ ≫ ≫ ≫ ≫ ≫
Kyoto-U 1 > ≫ ≫ ≫ ≫ ≫ ≫
Kyoto-U 2 ≫ ≫ ≫ ≫ ≫ ≫
SMT S2T - ≫ ≫ ≫ ≫
NAIST 1 ≫ ≫ ≫ ≫
NAIST 2 - ≫ ≫
TOSHIBA 1 ≫ ≫
RBMT B >

Table 8: Statistical significance testing of the ASPEC-JC Crowd scores.
N

A
IS

T
2

E
H

R

K
yo

to
-U

2

TO
SH

IB
A

1

SM
T

T
2S

K
yo

to
-U

1

B
JT

U
N

L
P

TO
SH

IB
A

2

O
nl

in
e

A

R
B

M
T

A

NAIST 1 ≫ ≫ ≫ ≫ ≫ ≫ ≫ ≫ ≫ ≫
NAIST 2 - ≫ ≫ ≫ ≫ ≫ ≫ ≫ ≫
EHR ≫ ≫ ≫ ≫ ≫ ≫ ≫ ≫
Kyoto-U 2 - - - ≫ ≫ ≫ ≫
TOSHIBA 1 - - ≫ ≫ ≫ ≫
SMT T2S - ≫ ≫ ≫ ≫
Kyoto-U 1 ≫ ≫ ≫ ≫
BJTUNLP ≫ ≫ ≫
TOSHIBA 2 ≫ ≫
Online A ≫

Table 9: Statistical significance testing of the ASPEC-CJ Crowd scores.

From the evaluation results, the following can
be observed (see Figure 10):

• Neural Network based re-ranking is effective
(NAIST, Kyoto-U, naver).

• The top SMT outperformed RBMT for CJ
and KJ patent translation.

• K→J patent translation achieved high scores
for automatic and human evaluations.

• A new problem of automatic evaluation was
found in the KJ evaluation.

8 Submitted Data

The number of published automatic evaluation re-
sults for the twelve teams exceeded 400 before the
start of WAT2015, and 56 translation results for
pairwise crowsdourcing evaluation were submit-
ted by twelve teams. Furthermore, we selected
3 translation results from each subtask and eval-
uated them for JPO adequacy evaluation. We will
organize the all of the submitted data for human
evaluation and make this public.

9 Conclusion and Future Perspective

This paper summarizes the WAT2015 machine
translation evaluation campaign. We had 12 par-
ticipants worldwide, and collected a large num-
ber of useful submissions for improving the cur-
rent machine translation systems by analyzing the
submissions and identifying the issues.

For the next WAT workshop, we plan to conduct
context-aware MT evaluations. The test data for
WAT are prepared using the paragraphs as the unit,
while almost all other evaluation campaigns use
the sentences as the unit. Therefore, it is suitable
to investigate the importance of context in transla-
tion.

We would also be very happy to include other
languages if the resources are available.

Appendix A Submissions

Tables 14, 15, 16, 17, 18, 19, summarize all
the submissions listed in the automatic evalua-
tion server at the time of the WAT2015 work-
shop (16th, October, 2015). The OTHER RE-
SOURCES column shows the use of resources
such as parallel corpora, monolingual corpora and

10



TO
SH

IB
A

2

E
H

R
1

SM
T

T
2S

nt
t1

TO
SH

IB
A

1

K
yo

to
-U

1

E
H

R
2

nt
t 2

O
nl

in
e

A

W
A

SU
IP

S

R
B

M
T

A

Kyoto-U 2 - ≫ ≫ ≫ ≫ ≫ ≫ ≫ ≫ ≫ ≫
TOSHIBA 2 - > ≫ ≫ ≫ ≫ ≫ ≫ ≫ ≫
EHR 1 - ≫ ≫ ≫ ≫ ≫ ≫ ≫ ≫
SMT T2S ≫ ≫ ≫ ≫ ≫ ≫ ≫ ≫
ntt 1 - - ≫ ≫ ≫ ≫ ≫
TOSHIBA 1 - ≫ ≫ ≫ ≫ ≫
Kyoto-U 1 ≫ ≫ ≫ ≫ ≫
EHR 2 - ≫ ≫ ≫
ntt 2 ≫ ≫ ≫
Online A ≫ ≫
WASUIPS ≫

Table 10: Statistical significance testing of the JPC-CJ Crowd scores.

na
ve

r2

N
IC

T
2

E
H

R
1

N
IC

T
1

na
ve

r1

E
H

R
2

TO
SH

IB
A

2

Se
ns

e
2

T O
SH

IB
A

1

SM
T

H
ie

ro

R
B

M
T

A

Se
ns

e
1

Online A ≫ ≫ ≫ ≫ ≫ ≫ ≫ ≫ ≫ ≫ ≫ ≫
naver 2 ≫ ≫ ≫ ≫ ≫ ≫ ≫ ≫ ≫ ≫ ≫
NICT 2 - > ≫ ≫ ≫ ≫ ≫ ≫ ≫ ≫
EHR 1 - ≫ ≫ ≫ ≫ ≫ ≫ ≫ ≫
NICT 1 - - ≫ ≫ ≫ ≫ ≫ ≫
naver 1 - > ≫ ≫ ≫ ≫ ≫
EHR 2 - > ≫ > ≫ ≫
TOSHIBA 2 - - - ≫ ≫
Sense 2 - - ≫ ≫
TOSHIBA 1 - ≫ ≫
SMT Hiero ≫ ≫
RBMT A ≫

Table 11: Statistical significance testing of the JPC-KJ Crowd scores.

parallel dictionaries in addition to ASPEC or JPC.

11



ASPEC-JE
SYSTEM ID κ
NAIST 1 0.104
Kyoto-U 2 0.070
TOSHIBA 2 0.076
TOSHIBA 1 0.080
RBMT D 0.070
Kyoto-U 1 0.112
NICT 1 0.118
NAIST 2 0.109
SMT S2T 0.046
NICT 2 0.078
Online D 0.044
Sense 1 0.077
Sense 2 0.073
TMU 0.065
ave. 0.080

ASPEC-EJ
SYSTEM ID κ
NAIST 1 0.243
WEBLIO MT 1 0.216
naver 2 0.170
Kyoto-U 2 0.219
NAIST 2 0.187
naver 1 0.193
WEBLIO MT 2 0.170
Kyoto-U 1 0.208
TOSHIBA 0.221
Online A 0.170
EHR 0.196
SMT T2S 0.206
RBMT B 0.171
Sense 2 0.200
Sense 1 0.140
ave. 0.194

ASPEC-JC
SYSTEM ID κ
TOSHIBA 2 0.159
Kyoto-U 1 0.022
Kyoto-U 2 -0.042
SMT S2T -0.013
NAIST 1 0.004
NAIST 2 0.021
TOSHIBA 1 0.072
RBMT B -0.031
Online D 0.019
ave. 0.023

ASPEC-CJ
SYSTEM ID κ
NAIST 1 0.116
NAIST 2 0.066
EHR 0.155
Kyoto-U 2 0.078
TOSHIBA 1 0.171
SMT T2S 0.169
Kyoto-U 1 0.043
BJTUNLP 0.115
TOSHIBA 2 0.095
Online A 0.076
RBMT A 0.094
ave. 0.107

JPC-CJ
SYSTEM ID κ
Kyoto-U 2 0.087
TOSHIBA 2 0.117
EHR 1 0.111
SMT T2S 0.109
ntt 1 0.114
TOSHIBA 1 0.105
Kyoto-U 1 0.057
EHR 2 0.148
ntt 2 0.042
Online A 0.027
WASUIPS 0.044
RBMT A 0.054
ave. 0.084

JPC-KJ
SYSTEM ID κ
Online A 0.142
naver 2 0.248
NICT 2 0.317
EHR 1 0.221
NICT 1 0.295
naver 1 0.388
EHR 2 0.213
TOSHIBA 2 0.305
Sense 2 0.323
TOSHIBA 1 0.304
SMT Hiero 0.246
RBMT A 0.125
Sense 1 0.200
ave. 0.255

Table 12: The Fleiss’ kappa values for the crowdsourcing evaluation results.

ASPEC-JE Annotator A Annotator B all weighted
SYSTEM ID average average average κ κ
NAIST 1 3.605 4.055 3.830 0.250 0.439
Kyoto-U 2 3.495 3.805 3.650 0.296 0.475
TOSHIBA 2 3.450 3.755 3.600 0.269 0.459

ASPEC-EJ Annotator A Annotator B all weighted
SYSTEM ID average average average κ κ
NAIST 1 3.865 4.220 4.043 0.377 0.535
naver 2 3.760 4.240 4.000 0.371 0.544
WEBLIO MT 1 3.585 4.040 3.813 0.356 0.535

ASPEC-JC Annotator A Annotator B all weighted
SYSTEM ID average average average κ κ
NAIST 1 3.600 2.740 3.170 0.151 0.296
Kyoto-U 1 3.330 2.400 2.865 0.162 0.319
TOSHIBA 2 3.220 2.275 2.748 0.112 0.287

ASPEC-CJ Annotator A Annotator B all weighted
SYSTEM ID average average average κ κ
NAIST 1 3.970 3.785 3.878 0.247 0.417
Kyoto-U 2 3.785 3.700 3.743 0.292 0.433
EHR 3.250 3.245 3.248 0.199 0.418

JPC-CJ Annotator A Annotator B all weighted
SYSTEM ID average average average κ κ
Kyoto-U 2 3.570 3.245 3.408 0.287 0.513
EHR 1 3.410 3.225 3.318 0.351 0.531
TOSHIBA 2 3.405 3.095 3.250 0.349 0.557

JPC-KJ Annotator A Annotator B all weighted
SYSTEM ID average average average κ κ
naver 2 4.900 4.655 4.778 0.329 0.333
NICT 2 4.905 4.610 4.758 0.186 0.218
Online A 4.735 4.315 4.525 0.183 0.300
Sense 1 4.445 4.195 4.320 0.367 0.528

Table 13: JPO adequacy evaluation results.

12



Figure 3: Official automatic evaluation results of ASPEC.

13



Figure 4: Official automatic evaluation results of JPC.

14



!"#$##%%

!&#$##%%

!'#$##%%

!(#$##%%

#$##%%

(#$##%%

'#$##%%

&#$##%%

"#$##%%

)#$##%%

*
+,
-.
%(
%

/0
12
1!
3
%'
%

.4
-5
,6
+%
'%

.4
-5
,6
+%
(%

76
8
.%
9
%

/0
12
1!
3
%(
%

*
,:
.%
(%

*
+,
-.
%'
%

-8
.%
-'
.%

*
,:
.%
'%

4
;<
=;
>%
9
%

->
;?
>%
(%

->
;?
>%
'%

.8
3
%

!"#$%&'$(%)*+,($-./0.1*2("3*)4!

!"#$##%%

!&#$##%%

!'#$##%%

#$##%%

'#$##%%

&#$##%%

"#$##%%

(#$##%%

)
*+
,-
%.
%

/
01
2+
3
%4
-%
.%

56
78
9%'
%

:;
<=
<!
>
%'
%

)
*+
,-
%'
%

56
78
9%.
%

/
01
2+
3
%4
-%
'%

:;
<=
<!
>
%.
%

-3
,?
+1
*%

3
5@
A5
8%
*%

0?
B%

,4
-%
-'
,%

B1
4
-%
1%

,8
5C
8%
'%

,8
5C
8%
.%

!"#$%&$'(%)*+,($-./0.1*2("3*)4!

!"#$%%&&

!"%$%%&&

!'#$%%&&

!'%$%%&&

!#$%%&&

%$%%&&

#$%%&&

'%$%%&&

'#$%%&&

"%$%%&&

"#$%%&&

()
*+
,-
.&
"&

/0
12
1!
3
&'
&

/0
12
1!
3
&"
&

*4
(&
*"
(&

5
.,
*(
&'
&

5
.,
*(
&"
&

()
*+
,-
.&
'&

6-
4
(&
-&

)
78
97
:&
;
&

!"#$%&'%(%)*+,($-./0.1*2("3*)4!

!"#$##%%

!&#$##%%

!'#$##%%

!(#$##%%

#$##%%

(#$##%%

'#$##%%

&#$##%%

"#$##%%

)#$##%%

*
+,
-.
%(
%

*
+,
-.
%'
%

/0
1%

23
45
4!
6
%'
%

.7
-0
,8
+%
(%

-9
.%
.'
-%

23
45
4!
6
%(
%

8:
.6
*
;<
%

.7
-0
,8
+%
'%

7
=>
?=
@%
+%

18
9
.%
+%

!"#$%&%'(%)*+,($-./0.1*2("3*)4!

!"#$##%%

!&#$##%%

!'#$##%%

!(#$##%%

!)#$##%%

#$##%%

)#$##%%

(#$##%%

'#$##%%

&#$##%%

*+
,-
,!
.
%(
%

/0
12
34
5%
(%

62
7%
)%

18
/%
/(
1%

9:
%)
%

/0
12
34
5%
)%

*+
,-
,!
.
%)
%

62
7%
(%

9:
%(
%

0
9;
<9
=%
5%

>
51
.
3?
1%

74
8
/%
5%

!"#$#!%#&'()%*+,-.,/'0%12'&3!

!"#$##%%

!&#$##%%

!'#$##%%

#$##%%

'#$##%%

&#$##%%

"#$##%%

(#$##%%

)#$##%%

*
+
,-+
.
%/
%

+
01
.
2%
&
%

3
45
6%
&
%

7
8
9
%'
%

3
45
6%
'
%

+
01
.
2%
'
%

7
8
9
%&
%

6*
:8
4;
/
%&
%

:.
+
<.
%&
%

6*
:8
4;
/
%'
%

:=
6%
8
-.
2>
%

9
;
=
6%
/
%

:.
+
<.
%'
%

!"#$%!&#'()*&+,-./-0(1&23('4!

Figure 5: Official pairwise crowdsourcing evaluation results.

15



!"#$%&'&

()*+*,-&.&

%/$0#1"&.&

%/$0#1"&'&

213%&4& ()*+*,-&'&!#5%&'&

!"#$%&.&

$3%&$.%&!#5%&.&

/67869&4&

$96:9&'&

$96:9&.&

%3-&

2;&<&=>?'@@A&

,B=>==&&

,.=>==&&

,'=>==&&

=>==&&

'=>==&&

.=>==&&

B=>==&&

C=>==&&

'C& '?& 'D& .=& ..& .C& .?&

!"#$%&'$(%)*+,-./$0!

!"#$%&'&

()*+*,-&.&

%/$0#1"&.&

%/$0#1"&'&

213%&4& ()*+*,-&'&!#5%&'&

!"#$%&.&$3%&$.%&

!#5%&.&

/67869&4&

$96:9&'&

$96:9&.&

%3-&

2;&<&=>??=@A&

,@=>==&&

,.=>==&&

,'=>==&&

=>==&&

'=>==&&

.=>==&&

@=>==&&

B=>==&&

C=& C.& CB& CC& CD& ?=& ?.& ?B& ?C&

!"#$%&'$(%)*+,-./0$"!

!"#$%&'&
()*+#,&-%&'&

./012&3&
4567689&3&

!"#$%&3&
./012&'&()*+#,&-%&3&

4567689&'& %,$:#*"&
,.;<.1&"&

):=&
$-%&%3$&

=*-%&*&

$1.>1&3&
$1.>1&'&

=?&@&ABCDEFG&

8EABAA&&

83ABAA&&

ABAA&&

3ABAA&&

EABAA&&

HABAA&&

'A& 'G& 3A& 3G& CA& CG& EA&

!"#$%&$'(%)*+,-./$0!

!"#$%&'&

()*+#,&-%&'&./012&3&

4567689&3&
!"#$%&3& ./012&'&

()*+#,&-%&3&4567689&'&

%,$:#*"&
,.;<.1&"&

):=&

$-%&%3$&

=*-%&*&

$1.>1&3&
$1.>1&'&

=?&@&ABCDE3&

8FABAA&&

83ABAA&&

ABAA&&

3ABAA&&

FABAA&&

GABAA&&

GA& GE& HA& HE& CA& CE&

!"#$%&$'(%)*+,-./0$"!

!"#$%&'()(
*+,-,./(0(

*+,-,./()(
#1!(#)!(

2'%#!(0(

2'%#!()(

!"#$%&'(0(

3&1!(&(

"45647(8(

39(:(;<=>0?(

.);<;;((

.0@<;;((

.0;<;;((

.@<;;((

;<;;((

@<;;((

0;<;;((

0@<;;((

);<;;((

0;( 0@( );( )@( A;( A@(

!"#$%&'%(%)*+,-./$0!

!"#$%&'()(

*+,-,./(0(
*+,-,./()(

#1!(#)!(
2'%#!(0(

2'%#!()(!"#$%&'(0(

3&1!(&(

34(5(6789:);(

.)6766((

.0:766((

.06766((

.:766((

6766((

:766((

06766((

0:766((

)6766((

;<( ;8( ;=( =6( =)( =<(

!"#$%&'%(%)*+,-./0$"(

!"#$%&'&

!"#$%&(&)*+&

,-./.01&(&
%2$*#3"&'&

$4%&%($&

,-./.01&'&

35%1!67&

%2$*#3"&(&

289:8;&"&

+34%&"&

+<&=&>?@A>>'&

0B>?>>&&

0C>?>>&&

0(>?>>&&

0'>?>>&&

>?>>&&

'>?>>&&

(>?>>&&

C>?>>&&

B>?>>&&

D& '>& 'D& (>& (D& C>& CD& B>& BD&

!"#$%&%'(%)*+,-./$0!

!"#$%&'&

!"#$%&(&)*+&

,-./.01&(&

%2$*#3"&'&

$4%&%($&
,-./.01&'&

35%1!67&

%2$*#3"&(&

289:8;&"&

+34%&"&

+<&=&>?@A@BC&

0A>?>>&&

0D>?>>&&

0(>?>>&&

0'>?>>&&

>?>>&&

'>?>>&&

(>?>>&&

D>?>>&&

A>?>>&&

EE& F>& FE& @>& @E& C>& CE& B>&

!"#$%&%'(%)*+,-./0$"!

Figure 6: The correlations between the BLEU/RIBES and Crowd scores of ASPEC.

16



!"#$#%&'('

)*+,-./'('
0,1'2'+3)')(+'
45'2'

)*+,-./'2'
!"#$#%&'2'

0,1'('45'('

*46748'/' 9/+&-:+'

1.3)'/'

1;'<'=>?@A@'

%@=>==''

%B=>==''

%C=>==''

%(=>==''

%2=>==''

=>==''

2=>==''

(=>==''

C=>==''

B=>==''

2=' 2@' (=' (@' C=' C@' B=' B@'

!"#$#!%#&'()*+,-.%

!"#$#%&'('

)*+,-./'('
0,1'2'+3)')(+'

45'2'
)*+,-./'2'

!"#$#%&'2'

0,1'('45'('

*46748'/' 9/+&-:+'

1.3)'/'

1;'<'=>?(@@A'

%B=>==''

%C=>==''

%D=>==''

%(=>==''

%2=>==''

=>==''

2=>==''

(=>==''

D=>==''

C=>==''

EB' A=' AB' ?=' ?B'

!"#$#!%#&'()*+,-./%

!"#$"%&'&

"()%*&+&

,-./&+&
012&3&

,-./&3&

"()%*&3&
012&+& /!41-5'&+&

4%"6%&+&
/!41-5'&3&

47/&1$%*8&

257/&'&

4%"6%&3&
9+:;::&&

93:;::&&

:;::&&

3:;::&&

+:;::&&

<:;::&&

=:;::&&

=:& >:& ?:& @:& A:& B:&

!"#$%!&#'()*+,-./!

!"#$"%&'&

"()%*&+&

,-./&+&
012&3& ,-./&3&

"()%*&3&
012&+& /!41-5'&+&

4%"6%&+&
/!41-5'&3&

47/&1$%*8&

257/&'&

4%"6%&3&

9+:;::&&

93:;::&&

:;::&&

3:;::&&

+:;::&&

<:;::&&

=:;::&&

>?& >>& @:& @+& @=& @?&

!"#$%!&#'()*+,-./0&

!"#$%&'&

()*+&'&,-.&/&

()*+&/&

!"#$%&/&,-.&'&

+01-)23&'&1$!4$&'&

+01-)23&/&15+&-6$%7&

8988&&

'988&&

:988&&

;988&&

<988&&

/8988&&

/'988&&

/:988&&

/;988&&

/<988&&

'8988&&

;=& >8& >=&

!"#$%!&#'()*+,-./&01234'51*6!

!"#$%&'&

()*+&'&,-.&/&

()*+&/&

!"#$%&/&

,-.&'&
+01-)23&'&

1$!4$&'&

+01-)23&/&
15+&-6$%7&

8988&&

'988&&

:988&&

;988&&

<988&&

/8988&&

/'988&&

/:988&&

/;988&&

/<988&&

'8988&&

=>& =>9?& =:& =:9?& =?&

!"#$%!&#'()*+,-./0&12345'62*7!

Figure 7: The correlations between the BLEU/RIBES and Crowd scores of JPC.

17



!"#$

!%#$

!&#$

!'#$

#$

'#$

&#$

%#$

"#$

(#$

'"$ ')$ '*$ &#$ &&$ &"$ &)$

!"#$%&'$(%)*+,+-+./01-($21-314+,!

+,-./$

01232!4$

/5.6-7,$

/84$

.9:;9$

.8/$.&/$

5:<=:9$>$

?78/$>$

!"#$

!%#$

!&#$

#$

&#$

%#$

"#$

'#$

(#$

)#$

&#$ &($ %#$ %($ "#$ "($ '#$

!"#$%&$'(%)*+,+-+./01-($21-314+,!

*+,-.$

/012,345.$

67898!:$

-;<=;$

-5.$.%-$

3<>?<;$+$

@15.$1$

!"#$

!%&$

!%#$

!&$

#$

&$

%#$

%&$

"#$

&$ %#$ %&$ "#$ "&$ '#$ '&$

!"#$%&'%(%)*+,+-+./01-($21-314+,!

()*+,$

-./0/!1$

,2+3*4)$

+5,$+",$

267869$:$

;45,$4$

!"#$

!%#$

!&#$

!'#$

#$

'#$

&#$

%#$

"#$

(#$

#$ ($ '#$ '($ &#$ &($ %#$ %($ "#$ "($

!"#$%&%'(%)*+,+-+./01-($21-314+,!

)*+,-$

./010!2$

345$

,6-$-&,$

789:8;$*$

5<6-$*$

Figure 8: The chronological evaluation results of ASPEC. (The x-axis indicates the BLEU score and the
y-axis indicates the Crowd score.)

18



!"#$%&
!'#$%& !$&

!(#%&

$)& $)#$%&

')#$%&

$*#"%& $'#$%&

'(& '$#%&
''&

)& %#%& +#%&

(,&

'(,&

$(,&

!(,&

*(,&

%(,&

)(,&

"(,&

-(,&

+(,&

'((,&

./012&'& 3456578&$& 291:0;/&$&

!"#$%&'$(!)*+,-./($0-1,-234(5*6,176!

'&

$&

!&

*&

%&

!"#$"% !"#$"%
&'%

&(#$"%
$)%

$*#$"%

+(%
+!%

+)%

+$#"% +&%
+"#"%

$% +#*"% &#$"%

(,%

+(,%

$(,%

&(,%

!(,%

"(,%

)(,%

*(,%

'(,%

-(,%

+((,%

./012%+% 34567%$% 89:;0<=>2%+%

!"#$%&$'(!)*+,-./($0-1,-234(5*6,176!

+%

$%

&%

!%

"%

!"#$%
!&% '#"$%

&$#$%

&!%
&(%

&$#&$%

&$#"$%
&$%

&(%

&)%
&$#"$%

!!#"$%
!"#&$% !'#$%

(*%

!(*%

&(*%

+(*%

)(*%

$(*%

,(*%

"(*%

-(*%

'(*%

!((*%

./012%!% 3456578%!% 291:0;/%&%

!"#$%&'%(!)*+,-./($0-1,-234(5*6,176!

!%

&%

+%

)%

$%

!"#$% !$#&$%

'(#&$%

(&%
()#&$%

!*%

!+#&$%
!*#&$%

()#!$%

$#!$% "#$%

'&#!$%

'#$% '#!$%
+#&$%

,-%

',-%

!,-%

(,-%

),-%

$,-%

+,-%

&,-%

*,-%

",-%

',,-%

./012%'% 3456578%!% 9:;%

!"#$%&%'(!)*+,-./($0-1,-234(5*6,176!

'%

!%

(%

)%

$%

!"#$%&
!'#$%& !'#%&

'(#%&

!!#!%& !)&

!*#$%& *)&
*)#%&

'(#$%& '(& '(&

"#!%& (& ')&

)+&

')+&

!)+&

*)+&

,)+&

%)+&

-)+&

$)+&

()+&

")+&

'))+&

./01023&!& 456&'& 7895:;<&!&

!"#$#!%&'()*+,-%./+0*+123%4(5*065!

'&

!&

*&

,&

%&

!"#!$% !!%

&'#($% $"#$%

('#($% ()#!$%

*(#$%

(*#!$%

)% )#($%
&#!$%

))%

'#$%
+#!$%
(%

',%

)',%

(',%

*',%

+',%

$',%

&',%

!',%

"',%

-',%

)'',%

./012%(% 3456%(% 7.89.1%:% ;1.<1%)%

!"#$%!&'()*+,-.&/0,1+,234&5)6+176!

)%

(%

*%

+%

$%

Figure 9: Distribution of JPO adequacy evaluations.

19



!"#!$$
!"%&$$ !"%'$$

!&"&'$$
!("&'$$

(&"''$$(&")*$$
(("#+$$ (("#+$$

,)"+%$$
,(")%$$ ,*"#&$$

'$$

*'$$

('$$

!'$$

)'$$

&'$$

%'$$

,'$$

#'$$

'$$

*$$

($$

!$$

)$$

&$$

-./01$*$ 2345467$($ 1809/:.$($

!
"#
$
%
&'
()
*
&+
,'
)
-
!

.
%
/
0
1
2
34
!

.-5)!67)8)92:12;#<8-1==2"4!

.;<=>?@3$

AB4C;$

:DE7$

F/:E0$

!"#!$$ !"##$$
%"&'$$

()")*$$

*%")*$$ *%"+*$$

%*"&%$$ %!"(#$$ %%")%$$

&'"'*$$ &#"##$$ &#"!+$$

#$$

'#$$

)#$$

%#$$

!#$$

*#$$

(#$$

+#$$

&#$$

,#$$

#$$

'$$

)$$

%$$

!$$

*$$

-./01$'$ 23456$)$ 789:/;<=1$'$

!
"#
$
%
&'
()
*
&+
,'
)
-
!

.
%
/
0
1
2
34
!

.-5)!6)78)92:12;#<8-1==2"4!

.>5?@3AB$

C6DE>$

9:8F$

G/980$

!"#$%%

&"'$%%
&"$(%%

$"))%%

#*"))%% #$"))%%

!#"*#%% &+"++%% !)")$%%

'!"&'%%
')"$#%% '#"$!%%

)%%

#)%%

&)%%

!)%%

,)%%

()%%

*)%%

$)%%

')%%

+)%%

)%%

#%%

&%%

!%%

,%%

(%%

-./01%#% 2345467%#% 1809/:.%&%

!
"#
$
%
&'
()
*
&+
,'
)
-
!

.
%
/
0
1
2
34
5

.-6)!78!5)92:12;#<5-1==2"4!

.;<=>?@3%

AB4C;%

:DE7%

F/:E0%

!"##$$
!"%&$$

!"'($$

!("%($$

)#"(*$$

'("%($$

&)"%($$
!#"(!$$ !%"+*$$

#("()$$ #&"*%$$ #'",*$$

*$$

)*$$

'*$$

!*$$

&*$$

(*$$

,*$$

%*$$

#*$$

+*$$

*$$

)$$

'$$

!$$

&$$

($$

-./01$)$ 2345467$'$ 89:$

!
"#
$
%
&'
()
*
&+
,'
)
-
!

.
%
/
0
1
2
34
5

.-6)!7!85)92:12;#<5-1==2"4!

.;<=>?@3$

AB4C;$

DE87$

:/D80$

!"#$%% !"!&%% !"&'%%

&("')%%

&&"))%%
&#"&'%%

#$"!'%% #$")*%% #$"$&%%

+&"+'%% +&"()%% +&"&&%%

)%%

$)%%

&)%%

!)%%

#)%%

')%%

*)%%

()%%

+)%%

,)%%

)%%

$%%

&%%

!%%

#%%

'%%

-./0/12%&% 345%$% 67849:;%&%

!
"#
$
%
&'
()
*
&+
,'
)
-
.

/
%
0
1
2
3
45
!

67!8!6.)93:23;#<.-2==3"5!

;<=>?@A.%

BC/D<%

:E32%

59:38%

!"#$%% !"#&%%
!"'(%%

!"()%%

*!"#'%%
*+"'+%%

($"#'%%

,*#"#'%%

#*"($%% #+"$*%%

''"+'%%

$'")(%%

-!"($%% -!"('%%
-+"-)%%

-'"!'%%

,)+%%

+%%

)+%%

!+%%

&+%%

$+%%

*++%%

,*%%

+%%

*%%

)%%

(%%

!%%

'%%

./012%)% 3456%)% 7.89.1%:% ;1.<1%*%

!
"#
$
%
&'
()
*
&+
,'
)
-
!

.
%
/
0
1
2
34
!

56!7859):2;12<#=9-1>>2"49

:=1>?/@A%

52BC=%

DEFG%

H4DF;%

Figure 10: Summary of automatic and human evaluations.

20



SY
ST

E
M

ID
ID

M
E

T
H

O
D

O
T

H
E

R
R

E
SO

U
R

C
E

S
B

L
E

U
R

IB
E

S
C

ro
w

d
SY

ST
E

M
D

E
SC

R
IP

T
IO

N
SM

T
H

ie
ro

2
SM

T
N

O
18

.7
2

0.
65

10
66

+7
.7

5
H

ie
ra

rc
hi

ca
l P

hr
as

e-
ba

se
d

SM
T

(2
01

4)
SM

T
Ph

ra
se

6
SM

T
N

O
18

.4
5

0.
64

51
37

—
–

Ph
ra

se
-b

as
ed

SM
T

SM
T

S2
T

9
SM

T
N

O
20

.3
6

0.
67

82
53

+2
5.

50
St

ri
ng

-t
o-

Tr
ee

SM
T

(2
01

4)
R

B
M

T
E

76
O

th
er

Y
E

S
14

.8
2

0.
66

38
51

—
–

R
B

M
T

E
R

B
M

T
F

79
O

th
er

Y
E

S
13

.8
6

0.
66

13
87

—
–

R
B

M
T

F
O

nl
in

e
D

77
5

O
th

er
Y

E
S

16
.8

5
0.

67
66

09
+0

.2
5

O
nl

in
e

D
(2

01
5)

SM
T

S2
T

87
7

SM
T

N
O

20
.3

6
0.

67
82

53
+7

.0
0

St
ri

ng
-t

o-
Tr

ee
SM

T
(2

01
5)

R
B

M
T

D
88

7
O

th
er

Y
E

S
15

.2
9

0.
68

33
78

+1
6.

75
R

B
M

T
D

(2
01

5)
O

nl
in

e
C

89
2

O
th

er
Y

E
S

10
.2

9
0.

62
25

64
—

–
O

nl
in

e
C

(2
01

5)
N

A
IS

T
1

65
5

SM
T

N
O

25
.4

1
0.

74
95

73
+3

5.
50

Tr
av

at
ar

Sy
st

em
w

ith
N

eu
ra

lM
T

R
er

an
ki

ng
an

d
Pa

rs
er

Se
lf

Tr
ai

ni
ng

N
A

IS
T

2
76

6
SM

T
N

O
22

.6
2

0.
72

27
98

+1
1.

75
Tr

av
at

ar
Sy

st
em

w
ith

Pa
rs

er
Se

lf
Tr

ai
ni

ng
K

yo
to

-U
1

79
6

E
B

M
T

N
O

21
.3

1
0.

70
64

80
+1

6.
50

K
yo

to
E

B
M

T
sy

st
em

w
ith

ou
tr

er
an

ki
ng

K
yo

to
-U

2
82

9
E

B
M

T
N

O
22

.8
9

0.
72

45
55

+3
2.

50
K

yo
to

E
B

M
T

sy
st

em
w

ith
bi

lin
gu

al
R

N
N

L
M

re
ra

nk
in

g
T

M
U

84
7

SM
T

N
O

15
.8

5
0.

62
88

97
-2

5.
50

PB
SM

T
w

ith
de

pe
nd

en
cy

ba
se

d
ph

ra
se

se
gm

en
ta

tio
n

Se
ns

e
1

86
0

SM
T

Y
E

S
16

.9
6

0.
61

07
75

-7
.7

5
Pa

ss
iv

e
JS

T
x1

Se
ns

e
2

86
1

SM
T

Y
E

S
16

.6
1

0.
60

90
08

-1
2.

75
Pe

rv
as

iv
e

JS
T

x1
N

IC
T

1
48

8
SM

T
N

O
18

.9
8

0.
65

98
83

+1
6.

00
ou

r b
as

el
in

e
(D

L
=6

)+
de

pe
nd

en
cy

-b
as

ed
pr

e-
re

or
de

ri
ng

[D
in

g+
20

15
]

N
IC

T
2

49
2

SM
T

N
O

18
.9

6
0.

68
44

85
+6

.5
0

ou
rb

as
el

in
e

(D
L

=9
)+

re
ve

rs
e

pr
e-

re
or

de
ri

ng
[K

at
z-

B
ro

w
n

&
C

ol
lin

s
20

08
]

TO
SH

IB
A

1
50

6
SM

T
an

d
R

B
M

T
Y

E
S

23
.0

0
0.

71
57

95
+2

1.
25

Sy
st

em
co

m
bi

na
tio

n
SM

T
an

d
R

B
M

T
(S

PE
)w

ith
R

N
N

L
M

la
ng

ua
ge

m
od

el
TO

SH
IB

A
2

52
9

SM
T

an
d

R
B

M
T

Y
E

S
22

.8
9

0.
71

85
40

+2
5.

00
R

B
M

T
w

ith
SP

E
(S

ta
tis

tic
al

Po
st

E
di

tin
g)

sy
st

em

Ta
bl

e
14

:A
SP

E
C

-J
E

su
bm

is
si

on
s

21



SY
ST

E
M

ID
ID

M
E

T
H

O
D

O
T

H
E

R
B

L
E

U
R

IB
E

S
C

ro
w

d
SY

ST
E

M
D

E
SC

R
IP

T
IO

N
R

E
SO

U
R

C
E

S
ju

m
an

ky
te

a
m

ec
ab

ju
m

an
ky

te
a

m
ec

ab
SM

T
Ph

ra
se

5
SM

T
N

O
27

.4
8

29
.8

0
28

.2
7

0.
68

37
35

0.
69

19
26

0.
69

53
90

—
–

Ph
ra

se
-b

as
ed

SM
T

SM
T

T
2S

12
SM

T
N

O
31

.0
5

33
.4

4
32

.1
0

0.
74

88
83

0.
75

80
31

0.
76

05
16

+3
4.

25
Tr

ee
-t

o-
St

ri
ng

SM
T

(2
01

4)
R

B
M

T
A

68
O

th
er

Y
E

S
12

.8
6

14
.4

3
13

.1
6

0.
67

01
67

0.
67

64
64

0.
67

89
34

—
–

R
B

M
T

A
R

B
M

T
C

95
O

th
er

Y
E

S
12

.1
9

13
.3

2
12

.1
4

0.
66

83
72

0.
67

26
45

0.
67

60
18

—
–

R
B

M
T

C
SM

T
H

ie
ro

36
7

SM
T

N
O

30
.1

9
32

.5
6

30
.9

4
0.

73
47

05
0.

74
69

78
0.

74
77

22
+3

1.
50

H
ie

ra
rc

hi
ca

l P
hr

as
e-

ba
se

d
SM

T
(2

01
4)

O
nl

in
e

A
77

4
O

th
er

Y
E

S
18

.2
2

19
.7

7
18

.4
6

0.
70

58
82

0.
71

39
60

0.
71

81
50

+3
4.

25
O

nl
in

e
A

(2
01

5)
SM

T
T

2S
87

5
SM

T
N

O
31

.0
5

33
.4

4
32

.1
0

0.
74

88
83

0.
75

80
31

0.
76

05
16

+3
0.

00
T r

ee
-t

o-
St

ri
ng

SM
T

(2
01

5)
R

B
M

T
B

88
3

O
th

er
Y

E
S

13
.1

8
14

.8
5

13
.4

8
0.

67
19

58
0.

68
07

48
0.

68
26

83
+9

.7
5

R
B

M
T

B
(2

01
5)

O
nl

in
e

B
88

9
O

th
er

Y
E

S
17

.8
0

19
.5

2
18

.1
1

0.
69

33
59

0.
70

19
66

0.
70

38
59

—
–

O
nl

in
e

B
(2

01
5)

N
A

IS
T

1
76

1
SM

T
N

O
35

.8
3

38
.1

7
36

.6
1

0.
81

14
79

0.
81

38
27

0.
82

03
37

+6
2.

25
Tr

av
at

ar
Sy

st
em

w
ith

N
eu

ra
lM

T
R

er
an

ki
ng

N
A

IS
T

2
76

3
SM

T
N

O
34

.3
8

36
.5

8
35

.1
6

0.
79

24
47

0.
79

64
89

0.
80

22
28

+4
9.

75
T r

av
at

ar
Sy

st
em

B
as

el
in

e
K

yo
to

-U
1

80
5

E
B

M
T

N
O

30
.6

9
33

.2
5

31
.7

1
0.

76
77

78
0.

77
66

72
0.

77
83

58
+4

0.
50

K
yo

to
E

B
M

T
sy

st
em

w
ith

ou
tr

er
an

ki
ng

K
yo

to
-U

2
83

2
E

B
M

T
N

O
33

.0
6

35
.5

7
33

.9
9

0.
78

95
14

0.
79

71
82

0.
79

99
79

+5
1.

00
K

yo
to

E
B

M
T

sy
st

em
w

ith
bi

lin
gu

al
R

N
N

L
M

re
ra

nk
in

g
W

E
B

L
IO

M
T

1
78

6
SM

T
N

O
33

.2
3

36
.2

1
34

.0
5

0.
80

47
22

0.
80

90
65

0.
81

43
37

+5
3.

75
N

M
T,

L
ST

M
Se

ar
ch

,5
en

se
m

bl
es

,b
ea

m
si

ze
20

,U
N

K
re

pl
ac

-
in

g,
Sy

st
em

C
om

bi
na

tio
n

w
ith

N
M

T
sc

or
e

(P
ic

k
to

p-
1k

re
su

lts
fr

om
N

M
T

)
W

E
B

L
IO

M
T

2
81

3
O

th
er

N
O

30
.7

2
34

.1
9

31
.5

7
0.

79
68

63
0.

80
26

66
0.

80
71

86
+4

3.
50

N
M

T,
L

ST
M

Se
ar

ch
,

B
ea

m
Si

ze
20

,
E

ns
em

bl
e

of
2

m
od

el
s,

U
N

K
re

pl
ac

in
g

Se
ns

e
1

70
0

SM
T

N
O

24
.1

3
26

.2
4

24
.9

6
0.

63
73

78
0.

64
27

89
0.

64
78

31
-3

6.
25

B
as

el
in

e-
di

ct
m

t
Se

ns
e

2
71

5
SM

T
Y

E
S

24
.4

3
26

.5
8

25
.3

6
0.

63
59

33
0.

64
15

17
0.

64
66

82
-3

1.
00

Pa
ss

iv
e

JS
T

x3
TO

SH
IB

A
52

4
SM

T
an

d
R

B
M

T
Y

E
S

32
.0

6
34

.1
7

32
.7

6
0.

77
09

89
0.

77
85

70
0.

78
04

67
+4

0.
25

R
B

M
T

w
ith

SP
E

(S
ta

tis
tic

al
Po

st
E

di
tin

g)
sy

st
em

na
ve

r1
83

6
SM

T
N

O
33

.1
4

35
.7

5
33

.9
3

0.
80

72
80

0.
81

14
87

0.
81

73
43

+4
8.

50
N

M
T

on
ly

na
ve

r2
83

7
SM

T
N

O
34

.6
0

36
.1

4
35

.3
0

0.
79

99
66

0.
80

31
54

0.
80

87
87

+5
3.

25
SM

T
t2

s
+

Sp
el

lc
or

re
ct

io
n

+
N

M
T

re
ra

nk
in

g
E

H
R

74
2

SM
T

N
O

29
.7

8
32

.3
6

30
.7

1
0.

75
35

76
0.

76
60

44
0.

76
81

05
+3

2.
50

Ph
ra

se
ba

se
d

SM
T

w
ith

pr
eo

rd
er

in
g.

T a
bl

e
15

:A
SP

E
C

-E
J

su
bm

is
si

on
s

22



SY
ST

E
M

ID
ID

M
E

T
H

O
D

O
T

H
E

R
B

L
E

U
R

IB
E

S
C

ro
w

d
SY

ST
E

M
D

E
SC

R
IP

T
IO

N
R

E
SO

U
R

C
E

S
ky

te
a

st
an

fo
rd

(c
tb

)
st

an
fo

rd
(p

ku
)

ky
te

a
st

an
fo

rd
(c

tb
)

st
an

fo
rd

(p
ku

)
SM

T
H

ie
ro

3
SM

T
N

O
27

.7
1

27
.7

0
27

.3
5

0.
80

91
28

0.
80

95
61

0.
81

13
94

+3
.7

5
H

ie
ra

rc
hi

ca
l P

hr
as

e-
ba

se
d

SM
T

(2
01

4)
SM

T
Ph

ra
se

7
SM

T
N

O
27

.9
6

28
.0

1
27

.6
8

0.
78

89
61

0.
79

02
63

0.
79

09
37

—
–

Ph
ra

se
-b

as
ed

SM
T

SM
T

S2
T

10
SM

T
N

O
28

.6
5

28
.6

5
28

.3
5

0.
80

76
06

0.
80

94
57

0.
80

84
17

+1
4.

00
St

ri
ng

-t
o-

Tr
ee

SM
T

(2
01

4)
R

B
M

T
C

24
4

R
B

M
T

N
O

9.
62

9.
96

9.
59

0.
64

22
78

0.
64

87
58

0.
64

53
85

—
–

R
B

M
T

C
O

nl
in

e
D

77
7

O
th

er
Y

E
S

10
.7

3
10

.3
3

10
.0

8
0.

66
04

84
0.

66
08

47
0.

66
04

82
-1

4.
75

O
nl

in
e

D
(2

01
5)

SM
T

S2
T

88
1

SM
T

N
O

28
.6

5
28

.6
5

28
.3

5
0.

80
76

06
0.

80
94

57
0.

80
84

17
+7

.7
5

St
ri

ng
-t

o-
T r

ee
SM

T
(2

01
5)

R
B

M
T

B
88

6
O

th
er

Y
E

S
17

.8
6

17
.7

5
17

.4
9

0.
74

48
18

0.
74

58
85

0.
74

37
94

-1
1.

00
R

B
M

T
B

(2
01

5)
O

nl
in

e
C

89
1

O
th

er
Y

E
S

7.
44

7.
05

6.
75

0.
61

19
64

0.
61

50
48

0.
61

21
58

—
–

O
nl

in
e

C
(2

01
5)

N
A

IS
T

1
83

8
SM

T
N

O
31

.6
1

31
.5

9
31

.4
2

0.
83

27
65

0.
83

42
45

0.
83

37
21

+7
.0

0
Tr

av
at

ar
Sy

st
em

w
ith

N
eu

ra
lM

T
R

er
an

ki
ng

N
A

IS
T

2
83

9
SM

T
N

O
30

.0
6

29
.9

2
29

.7
3

0.
81

50
84

0.
81

66
24

0.
81

64
62

+2
.7

5
T r

av
at

ar
Sy

st
em

B
as

el
in

e
K

yo
to

-U
1

77
8

E
B

M
T

N
O

29
.9

9
29

.7
6

29
.8

1
0.

80
70

83
0.

80
82

75
0.

80
80

10
+1

6.
00

K
yo

to
E

B
M

T
sy

st
em

w
ith

ou
tr

er
an

ki
ng

K
yo

to
-U

2
79

3
E

B
M

T
N

O
31

.4
0

31
.2

6
31

.2
3

0.
82

69
86

0.
82

69
19

0.
82

71
90

+1
2.

50
K

yo
to

E
B

M
T

sy
st

em
w

ith
bi

lin
gu

al
R

N
N

L
M

re
ra

nk
in

g
TO

SH
IB

A
1

50
5

SM
T

an
d

R
B

M
T

Y
E

S
30

.1
7

30
.1

5
29

.8
9

0.
81

34
90

0.
81

32
33

0.
81

34
41

+2
.5

0
SP

E
(S

ta
tis

tic
al

Po
st

E
di

tin
g)

Sy
st

em
TO

SH
IB

A
2

67
6

SM
T

an
d

R
B

M
T

Y
E

S
30

.0
7

30
.1

4
29

.8
3

0.
81

72
94

0.
81

69
84

0.
81

69
81

+1
7.

00
Sy

st
em

co
m

bi
na

tio
n

SM
T

an
d

R
B

M
T

(S
PE

)w
ith

R
N

N
L

M
la

n-
gu

ag
e

m
od

el
+

po
st

-p
ro

ce
ss

in
g

Ta
bl

e
16

:A
SP

E
C

-J
C

su
bm

is
si

on
s

23



SY
ST

E
M

ID
ID

M
E

T
H

O
D

O
T

H
E

R
B

L
E

U
R

IB
E

S
C

ro
w

d
SY

ST
E

M
D

E
SC

R
IP

T
IO

N
R

E
SO

U
R

C
E

S
ju

m
an

ky
te

a
m

ec
ab

ju
m

an
ky

te
a

m
ec

ab
SM

T
H

ie
ro

4
SM

T
N

O
35

.4
3

35
.9

1
35

.6
4

0.
81

04
06

0.
79

87
26

0.
80

76
65

+4
.7

5
H

ie
ra

rc
hi

ca
l P

hr
as

e-
ba

se
d

SM
T

(2
01

4)
SM

T
Ph

ra
se

8
SM

T
N

O
34

.6
5

35
.1

6
34

.7
7

0.
77

24
98

0.
76

63
84

0.
77

10
05

—
–

Ph
ra

se
-b

as
ed

SM
T

SM
T

T
2S

13
SM

T
N

O
36

.5
2

37
.0

7
36

.6
4

0.
82

52
92

0.
82

04
90

0.
82

50
25

+1
6.

00
T r

ee
-t

o-
St

ri
ng

SM
T

(2
01

4)
R

B
M

T
D

24
2

R
B

M
T

N
O

8.
39

8.
70

8.
30

0.
64

11
89

0.
62

64
00

0.
63

33
19

—
–

R
B

M
T

D
O

nl
in

e
A

77
6

O
th

er
Y

E
S

11
.5

3
12

.8
2

11
.6

8
0.

58
82

85
0.

59
03

93
0.

59
28

87
-1

9.
00

O
nl

in
e

A
(2

01
5)

SM
T

T
2S

87
9

SM
T

N
O

36
.5

2
37

.0
7

36
.6

4
0.

82
52

92
0.

82
04

90
0.

82
50

25
+1

7.
25

T r
ee

-t
o-

St
ri

ng
SM

T
(2

01
5)

R
B

M
T

A
88

5
O

th
er

Y
E

S
9.

37
9.

87
9.

35
0.

66
62

77
0.

65
24

02
0.

66
17

30
-2

8.
00

R
B

M
T

A
(2

01
5)

O
nl

in
e

B
89

0
O

th
er

Y
E

S
10

.4
1

11
.0

3
10

.3
6

0.
59

73
55

0.
59

28
41

0.
59

72
98

—
–

O
nl

in
e

B
(2

01
5)

N
A

IS
T

1
83

4
SM

T
N

O
41

.7
5

42
.9

5
41

.9
3

0.
85

50
89

0.
84

77
46

0.
85

45
87

+3
5.

75
Tr

av
at

ar
Sy

st
em

w
ith

N
eu

ra
lM

T
R

er
an

ki
ng

N
A

IS
T

2
83

5
SM

T
N

O
39

.3
6

40
.5

1
39

.4
7

0.
83

43
88

0.
82

71
48

0.
83

41
30

+2
5.

75
T r

av
at

ar
Sy

st
em

B
as

el
in

e
K

yo
to

-U
1

84
4

E
B

M
T

N
O

36
.3

0
37

.2
2

36
.4

4
0.

81
97

43
0.

81
45

81
0.

81
87

94
+1

6.
75

K
yo

to
E

B
M

T
sy

st
em

w
ith

ou
tr

er
an

ki
ng

K
yo

to
-U

2
84

5
E

B
M

T
N

O
38

.5
3

39
.4

1
38

.6
6

0.
84

06
81

0.
83

44
51

0.
83

90
63

+1
8.

50
K

yo
to

E
B

M
T

sy
st

em
w

ith
bi

lin
gu

al
R

N
N

L
M

re
ra

nk
in

g
B

JT
U

N
L

P
86

2
SM

T
N

O
34

.7
2

34
.8

7
34

.7
9

0.
80

70
12

0.
79

24
88

0.
80

24
30

+6
.5

0
a

de
pe

nd
en

cy
-t

o-
st

ri
ng

m
od

el
fo

rS
M

T
TO

SH
IB

A
1

50
8

SM
T

an
d

R
B

M
T

Y
E

S
37

.4
7

37
.4

4
37

.3
4

0.
82

72
91

0.
81

73
95

0.
82

54
72

+1
8.

00
Sy

st
em

co
m

bi
na

tio
n

SM
T

an
d

R
B

M
T

(S
PE

)w
ith

R
N

N
L

M
la

n-
gu

ag
e

m
od

el
TO

SH
IB

A
2

52
5

SM
T

an
d

R
B

M
T

Y
E

S
35

.8
5

36
.0

2
35

.7
3

0.
82

47
40

0.
81

53
88

0.
82

24
23

-1
.0

0
R

B
M

T
w

ith
SP

E
(S

ta
tis

tic
al

Po
st

E
di

tin
g)

sy
st

em
E

H
R

72
0

SM
T

an
d

R
B

M
T

Y
E

S
37

.9
0

38
.6

8
37

.9
8

0.
82

60
03

0.
81

86
20

0.
82

48
06

+2
5.

75
Sy

st
em

co
m

bi
na

tio
n

of
R

B
M

T
w

ith
us

er
di

ct
io

na
ry

pl
us

SP
E

an
d

ph
ra

se
ba

se
d

SM
T

w
ith

pr
eo

rd
er

in
g.

C
an

di
da

te
se

le
ct

io
n

by
la

ng
ua

ge
m

od
el

sc
or

e.

T a
bl

e
17

:A
SP

E
C

-C
J

su
bm

is
si

on
s

24



SY
ST

E
M

ID
ID

M
E

T
H

O
D

O
T

H
E

R
B

L
E

U
R

IB
E

S
C

ro
w

d
SY

ST
E

M
D

E
SC

R
IP

T
IO

N
R

E
SO

U
R

C
E

S
ju

m
an

k y
te

a
m

ec
ab

ju
m

an
k y

te
a

m
ec

ab
SM

T
H

ie
ro

43
0

SM
T

N
O

39
.2

2
39

.5
2

39
.1

4
0.

80
60

58
0.

80
20

59
0.

80
45

23
—

–
H

ie
ra

rc
hi

ca
lP

hr
as

e-
ba

se
d

SM
T

SM
T

Ph
ra

se
43

1
SM

T
N

O
38

.3
4

38
.5

1
38

.2
2

0.
78

20
19

0.
77

89
21

0.
78

14
56

—
–

Ph
ra

se
-b

as
ed

SM
T

SM
T

T
2S

43
2

SM
T

N
O

39
.3

9
39

.9
0

39
.3

9
0.

81
49

19
0.

81
13

50
0.

81
35

95
+2

0.
75

T r
ee

-t
o-

St
ri

ng
SM

T
(2

01
5)

O
nl

in
e

A
64

7
O

th
er

Y
E

S
26

.8
0

27
.8

1
26

.8
9

0.
71

22
42

0.
70

72
64

0.
71

12
73

-7
.0

0
O

nl
in

e
A

(2
01

5)
O

nl
in

e
B

64
8

O
th

er
Y

E
S

12
.3

3
12

.7
2

12
.4

4
0.

64
89

96
0.

64
12

55
0.

64
87

42
—

–
O

nl
in

e
B

(2
01

5)
R

B
M

T
A

75
9

R
B

M
T

N
O

10
.4

9
10

.7
2

10
.3

5
0.

67
40

60
0.

66
40

98
0.

66
73

49
-3

9.
25

R
B

M
T

A
(2

01
5)

R
B

M
T

B
76

0
R

B
M

T
N

O
7.

94
8.

07
7.

73
0.

59
62

00
0.

58
18

37
0.

58
69

41
—

–
R

B
M

T
B

K
yo

to
-U

1
78

1
E

B
M

T
N

O
37

.8
7

38
.6

2
37

.7
1

0.
79

97
30

0.
79

77
00

0.
79

89
79

+1
4.

50
B

as
el

in
e

w
/o

re
ra

nk
in

g
K

yo
to

-U
2

86
4

E
B

M
T

N
O

41
.3

5
41

.9
2

41
.1

6
0.

82
85

43
0.

82
41

99
0.

82
72

30
+2

7.
50

K
yo

to
E

B
M

T
sy

st
em

w
ith

bi
lin

gu
al

R
N

N
L

M
re

ra
nk

in
g

(o
nl

y
ch

ar
ac

te
r-

ba
se

m
od

el
)

T O
SH

IB
A

1
50

4
SM

T
an

d
R

B
M

T
Y

E
S

41
.8

2
41

.9
0

41
.6

0
0.

82
05

68
0.

81
35

36
0.

81
76

14
+1

4.
50

C
om

bi
na

tio
n

of
ph

ra
se

-b
as

ed
SM

T
an

d
SP

E
sy

st
em

s.
TO

SH
IB

A
2

52
6

SM
T

an
d

R
B

M
T

Y
E

S
41

.1
2

40
.8

7
40

.5
9

0.
82

22
68

0.
81

42
49

0.
81

89
81

+2
4.

25
R

B
M

T
w

ith
SP

E
(S

ta
tis

tic
al

Po
st

E
di

tin
g)

sy
st

em
W

A
SU

IP
S

85
3

SM
T

N
O

33
.4

8
34

.5
5

33
.5

5
0.

77
39

85
0.

77
10

99
0.

77
22

02
-1

2.
00

C
om

bi
ni

ng
sa

m
pl

in
g-

ba
se

d
al

ig
nm

en
t

an
d

bi
lin

gu
al

hi
er

ar
ch

i-
ca

ls
ub

-s
en

te
nt

ia
la

lig
nm

en
tm

et
ho

ds
.

E
H

R
1

67
1

SM
T

an
d

R
B

M
T

Y
E

S
41

.0
6

42
.2

4
41

.1
5

0.
82

69
87

0.
82

19
83

0.
82

50
56

+2
2.

00
Sy

st
em

co
m

bi
na

tio
n

of
R

B
M

T
w

ith
us

er
di

ct
io

na
ry

pl
us

SP
E

an
d

ph
ra

se
ba

se
d

SM
T

w
ith

pr
eo

rd
er

in
g.

C
an

di
da

te
se

le
ct

io
n

by
la

ng
ua

ge
m

od
el

sc
or

e.
E

H
R

2
82

8
SM

T
an

d
R

B
M

T
Y

E
S

40
.3

5
40

.1
6

39
.9

2
0.

81
95

16
0.

81
29

82
0.

81
67

43
+8

.2
5

R
B

M
T

w
ith

us
er

di
ct

io
na

ry
pl

us
SP

E
nt

t1
73

6
SM

T
N

O
40

.6
0

41
.1

0
40

.6
3

0.
82

34
36

0.
82

02
52

0.
82

20
26

+1
6.

25
A

pr
e-

or
de

ri
ng

-b
as

ed
PB

M
T

w
ith

pa
te

nt
-t

un
ed

de
pe

nd
en

cy
pa

rs
in

g
an

d
ph

ra
se

ta
bl

e
sm

oo
th

in
g.

nt
t2

81
1

SM
T

N
O

39
.7

7
40

.0
8

39
.8

8
0.

81
62

88
0.

81
19

11
0.

81
55

43
+8

.0
0

A
pr

e-
or

de
ri

ng
-b

as
ed

PB
M

T
w

ith
pa

te
nt

-t
un

ed
de

pe
nd

en
cy

pa
rs

in
g,

le
ar

ni
ng

-b
as

ed
pr

e-
or

de
ri

ng
,a

nd
ph

ra
se

ta
bl

e
sm

oo
th

-
in

g.

T a
bl

e
18

:J
PC

-C
J

su
bm

is
si

on
s

25



SY
ST

E
M

ID
ID

M
E

T
H

O
D

O
T

H
E

R
B

L
E

U
R

IB
E

S
C

ro
w

d
SY

ST
E

M
D

E
SC

R
IP

T
IO

N
R

E
SO

U
R

C
E

S
ju

m
an

ky
te

a
m

ec
ab

ju
m

an
ky

te
a

m
ec

ab
SM

T
Ph

ra
se

43
8

SM
T

N
O

69
.2

2
70

.3
6

69
.7

3
0.

94
13

02
0.

93
97

29
0.

94
07

56
—

–
Ph

ra
se

-b
as

ed
SM

T
SM

T
H

ie
ro

43
9

SM
T

N
O

67
.4

1
68

.6
5

68
.0

0
0.

93
71

62
0.

93
59

03
0.

93
65

70
+2

.7
5

H
ie

ra
rc

hi
ca

lP
hr

as
e-

ba
se

d
SM

T
(2

01
5)

O
nl

in
e

B
65

1
O

th
er

Y
E

S
36

.4
1

38
.7

2
37

.0
1

0.
85

17
45

0.
85

22
63

0.
85

19
45

—
–

O
nl

in
e

B
(2

01
5)

O
nl

in
e

A
65

2
O

th
er

Y
E

S
55

.0
5

56
.8

4
55

.4
6

0.
90

91
52

0.
90

93
85

0.
90

88
38

+3
8.

75
O

nl
in

e
A

(2
01

5)
R

B
M

T
A

65
3

O
th

er
Y

E
S

42
.0

0
43

.9
7

42
.4

5
0.

87
63

96
0.

87
37

34
0.

87
51

46
-7

.2
5

R
B

M
T

A
(2

01
5)

R
B

M
T

B
65

4
O

th
er

Y
E

S
34

.7
4

37
.5

1
35

.5
4

0.
84

57
12

0.
84

90
14

0.
84

62
28

—
–

R
B

M
T

B
Se

ns
e

1
65

7
SM

T
N

O
85

.2
3

85
.2

0
85

.2
3

0.
95

45
06

0.
95

44
48

0.
95

44
35

-1
7.

75
U

ni
co

de
2S

tr
in

g
w

ith
de

vt
es

tM
E

R
T

ru
n2

Se
ns

e
2

83
3

SM
T

N
O

70
.2

3
71

.1
1

70
.5

1
0.

94
24

15
0.

94
08

87
0.

94
16

87
+3

.5
0

B
as

el
in

e
w

ith
on

ly
tr

ai
n.

ja
/k

o
N

IC
T

1
50

1
SM

T
N

O
70

.6
2

71
.5

2
70

.9
2

0.
94

33
48

0.
94

24
02

0.
94

27
48

+8
.2

5
ou

r
ba

se
lin

e:
ch

ar
ac

te
r-

ba
se

d
/

PB
SM

T
in

M
O

SE
S

(D
L

=0
,

m
ax

-p
hr

as
e-

le
n=

9,
no

le
x-

re
or

de
ri

ng
)/

SR
IL

M
(9

-g
ra

m
)

N
IC

T
2

51
3

SM
T

N
O

70
.8

1
71

.7
0

71
.1

1
0.

94
34

63
0.

94
25

19
0.

94
29

04
+1

0.
50

ou
rb

as
el

in
e

+
po

st
-p

ro
ce

ss
in

g
of

br
ac

ke
tb

al
an

ci
ng

TO
SH

IB
A

1
55

4
SM

T
an

d
R

B
M

T
Y

E
S

70
.5

1
70

.8
4

70
.7

1
0.

94
21

83
0.

93
95

45
0.

94
14

71
+3

.0
0

Sy
st

em
co

m
bi

na
tio

n
SM

T,
SP

E
an

d
R

B
M

T
w

ith
R

N
N

L
M

+
po

st
-p

ro
ce

ss
in

g
TO

SH
IB

A
2

56
8

SM
T

Y
E

S
71

.0
1

71
.4

4
71

.2
6

0.
94

37
94

0.
94

12
87

0.
94

31
81

+4
.5

0
Ph

ra
se

-b
as

ed
SM

T
w

ith
R

N
N

L
M

re
ra

nk
in

g
+

po
st

-p
ro

ce
ss

in
g

na
ve

r1
63

0
SM

T
N

O
70

.9
1

71
.7

6
71

.1
8

0.
94

39
28

0.
94

28
00

0.
94

33
76

+6
.7

5
co

m
bi

ne
d

tw
o

ph
ra

se
-b

as
ed

sy
st

em
s,

po
st

-p
ro

ce
ss

in
g

na
ve

r2
81

6
SM

T
N

O
71

.3
8

72
.2

7
71

.6
8

0.
94

38
14

0.
94

28
05

0.
94

35
84

+1
4.

75
SM

T
PB

+
N

M
T

re
ra

nk
in

g
E

H
R

1
50

0
SM

T
an

d
R

B
M

T
Y

E
S

70
.6

7
71

.5
2

70
.9

3
0.

94
30

40
0.

94
18

93
0.

94
27

71
+1

0.
25

Sy
st

em
co

m
bi

na
tio

n
of

R
B

M
T

w
ith

us
er

di
ct

io
na

ry
pl

us
SP

E
an

d
ph

ra
se

ba
se

d
SM

T.
C

an
di

da
te

se
le

ct
io

n
by

la
ng

ua
ge

m
od

el
sc

or
e.

E
H

R
2

83
1

SM
T

an
d

R
B

M
T

Y
E

S
70

.1
3

70
.8

6
70

.3
5

0.
94

18
87

0.
94

03
41

0.
94

15
17

+6
.5

0
R

B
M

T
w

ith
us

er
di

ct
io

na
ry

pl
us

SP
E

T a
bl

e
19

:J
PC

-K
J

su
bm

is
si

on
s

26



References
Jacob Cohen. 1968. Weighted kappa: Nominal scale

agreement with provision for scaled disagreement or
partial credit. Psychological Bulletin, 70(4):213 –
220.

Chenchen Ding, Masao Utiyama, and Eiichiro Sumita.
2015. NICT at WAT 2015. In Proceedings of
the 2nd Workshop on Asian Translation (WAT2015),
pages 42–47, Kyoto, Japan, October.

Terumasa Ehara. 2015. System Combination of
RBMT plus SPE and Preordering plus SMT. In Pro-
ceedings of the 2nd Workshop on Asian Translation
(WAT2015), pages 29–34, Kyoto, Japan, October.

J.L. Fleiss et al. 1971. Measuring nominal scale agree-
ment among many raters. Psychological Bulletin,
76(5):378–382.

Kenneth Heafield, Ivan Pouzyrevsky, Jonathan H.
Clark, and Philipp Koehn. 2013. Scalable modified
kneser-ney language model estimation. In Proceed-
ings of the 51st Annual Meeting of the Association
for Computational Linguistics (Volume 2: Short Pa-
pers), pages 690–696, Sofia, Bulgaria, August. As-
sociation for Computational Linguistics.

Hieu Hoang, Philipp Koehn, and Adam Lopez. 2009.
A unified framework for phrase-based, hierarchical,
and syntax-based statistical machine translation. In
Proceedings of the International Workshop on Spo-
ken Language Translation, pages 152–159.

Hideki Isozaki, Tsutomu Hirao, Kevin Duh, Katsuhito
Sudoh, and Hajime Tsukada. 2010. Automatic
evaluation of translation quality for distant language
pairs. In Proceedings of the 2010 Conference on
Empirical Methods in Natural Language Process-
ing, EMNLP ’10, pages 944–952, Stroudsburg, PA,
USA. Association for Computational Linguistics.

Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, Chris Dyer, Ondrej Bojar, Alexandra
Constantin, and Evan Herbst. 2007. Moses: Open
source toolkit for statistical machine translation. In
Annual Meeting of the Association for Computa-
tional Linguistics (ACL), demonstration session.

Philipp Koehn. 2004. Statistical significance tests for
machine translation evaluation. In Dekang Lin and
Dekai Wu, editors, Proceedings of EMNLP 2004,
pages 388–395, Barcelona, Spain, July. Association
for Computational Linguistics.

T. Kudo. 2005. Mecab : Yet another
part-of-speech and morphological analyzer.
http://mecab.sourceforge.net/.

Sadao Kurohashi, Toshihisa Nakamura, Yuji Mat-
sumoto, and Makoto Nagao. 1994. Improve-
ments of Japanese morphological analyzer JUMAN.
In Proceedings of The International Workshop on
Sharable Natural Language, pages 22–28.

Hyoung-Gyu Lee, JaeSong Lee, Jun-Seok Kim, and
Chang-Ki Lee. 2015. NAVER Machine Translation
System for WAT 2015. In Proceedings of the 2nd
Workshop on Asian Translation (WAT2015), pages
69–73, Kyoto, Japan, October.

Junki Matsuo, Kenichi Ohwada, and Mamoru Ko-
machi. 2015. Source Phrase Segmentation and
Translation for Japanese-English Translation Using
Dependency Structure. In Proceedings of the 2nd
Workshop on Asian Translation (WAT2015), pages
99–104, Kyoto, Japan, October.

Toshiaki Nakazawa, Hideya Mino, Isao Goto, Sadao
Kurohashi, and Eiichiro Sumita. 2014. Overview
of the 1st Workshop on Asian Translation. In Pro-
ceedings of the 1st Workshop on Asian Translation
(WAT2014), pages 1–19, Tokyo, Japan, October.

Graham Neubig, Yosuke Nakata, and Shinsuke Mori.
2011. Pointwise prediction for robust, adaptable
japanese morphological analysis. In Proceedings of
the 49th Annual Meeting of the Association for Com-
putational Linguistics: Human Language Technolo-
gies: Short Papers - Volume 2, HLT ’11, pages 529–
533, Stroudsburg, PA, USA. Association for Com-
putational Linguistics.

Graham Neubig, Makoto Morishita, and Satoshi Naka-
mura. 2015. Neural Reranking Improves Sub-
jective Quality of Machine Translation: NAIST at
WAT2015. In Proceedings of the 2nd Workshop on
Asian Translation (WAT2015), pages 35–41, Kyoto,
Japan, October.

Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a method for automatic eval-
uation of machine translation. In ACL, pages 311–
318.

Slav Petrov, Leon Barrett, Romain Thibaux, and Dan
Klein. 2006. Learning accurate, compact, and
interpretable tree annotation. In Proceedings of
the 21st International Conference on Computational
Linguistics and 44th Annual Meeting of the Associa-
tion for Computational Linguistics, pages 433–440,
Sydney, Australia, July. Association for Computa-
tional Linguistics.

John Richardson, Raj Dabre, Fabien Cromières, Toshi-
aki Nakazawa, and Sadao Kurohashi. 2015. Ky-
otoEBMT System Description for the 2nd Workshop
on Asian Translation. In Proceedings of the 2nd
Workshop on Asian Translation (WAT2015), pages
54–60, Kyoto, Japan, October.

Hua Shan, Yujie Zhang, Lu Bai, and Te Luo. 2015. A
Dependency-to-String Model for Chinese-Japanese
SMT System. In Proceedings of the 2nd Workshop
on Asian Translation (WAT2015), pages 82–86, Ky-
oto, Japan, October.

Satoshi Sonoh and Satoshi Kinoshita. 2015. Toshiba
MT System Description for the WAT2015 Work-
shop. In Proceedings of the 2nd Workshop on Asian

27



Translation (WAT2015), pages 48–53, Kyoto, Japan,
October.

Katsuhito Sudoh and Masaaki Nagata. 2015. Chinese-
to-Japanese Patent Machine Translation based on
Syntactic Pre-ordering forWAT 2015. In Proceed-
ings of the 2nd Workshop on Asian Translation
(WAT2015), pages 95–98, Kyoto, Japan, October.

Liling Tan, Jon Dehdari, and Josef van Genabith. 2015.
An Awkward Disparity between BLEU / RIBES
Scores and Human Judgements in Machine Transla-
tion. In Proceedings of the 2nd Workshop on Asian
Translation (WAT2015), pages 74–81, Kyoto, Japan,
October.

Huihsin Tseng. 2005. A conditional random field word
segmenter. In In Fourth SIGHAN Workshop on Chi-
nese Language Processing.

Masao Utiyama and Hitoshi Isahara. 2007. A
japanese-english patent parallel corpus. In MT sum-
mit XI, pages 475–482.

Wei Yang, Zhongwen Zhao, Baosong Yang, and Yves
Lepage. 2015. Sampling-based Alignment and
Hierarchical Sub-sentential Alignment in Chinese-
Japanese Translation of Patents. In Proceedings of
the 2nd Workshop on Asian Translation (WAT2015),
pages 87–94, Kyoto, Japan, October.

Zhongyuan Zhu. 2015. Evaluating Neural Machine
Translation in English-Japanese Task. In Proceed-
ings of the 2nd Workshop on Asian Translation
(WAT2015), pages 61–68, Kyoto, Japan, October.

28




