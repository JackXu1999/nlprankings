



















































Extended Topic Model for Word Dependency


Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics
and the 7th International Joint Conference on Natural Language Processing (Short Papers), pages 506–510,

Beijing, China, July 26-31, 2015. c©2015 Association for Computational Linguistics

Extended Topic Model for Word Dependency

Tong Wang1, Vish Viswanath2 and Ping Chen1
1University of Massachusetts Boston, Boston, MA

2Harvard School of Public Health, Boston, MA
tongwang0001@gmail.com, Ping.Chen@umb.edu

Vish Viswanath@dfci.harvard.edu

Abstract

Topic Model such as Latent Dirichlet
Allocation(LDA) makes assumption that
topic assignment of different words are
conditionally independent. In this paper,
we propose a new model Extended Global
Topic Random Field (EGTRF) to model
non-linear dependencies between words.
Specifically, we parse sentences into de-
pendency trees and represent them as a
graph, and assume the topic assignment of
a word is influenced by its adjacent words
and distance-2 words. Word similarity in-
formation learned from large corpus is in-
corporated to enhance word topic assign-
ment. Parameters are estimated efficiently
by variational inference and experimen-
tal results on two datasets show EGTRF
achieves lower perplexity and higher log
predictive probability.

1 Introduction

Probabilistic topic model such as Latent Dirich-
let Allocation(LDA) (Blei et al, 2003) has been
widely used for discovering latent topics from
document collections by capturing words’ co-
occuring relation. However, the “bag of words”
assumption is employed in most existing topic
models, it assumes the order of words can be ig-
nored and topic assignment of each word is condi-
tionally independent given the topic mixture of a
document.

To relax the “bag of words” assumption, many
extended topic models have been proposed to ad-
dress the limitation of conditional independence.
Wallach (Wallach, 2006) explores a hierarchical
generative probabilistic model that incorporates
both n-gram statistics and latent topic variables.
Gruber (Gruber et al, 2007) models the topics of
words in the document as a Markov chain, and as-
sumes all words in the same sentence are more

likely to have the same topic. Zhu (Zhu et al,
2010) incorporates Markov dependency between
topic assignments of neighboring words, and em-
ploys a general structure of the GLM to define
a conditional distribution of latent topic assign-
ments over words. Most of the models above are
limited to model linear topical dependencies be-
tween words, word topical dependencies can also
be modeled by a non-linear way. In Syntactic topic
models (Boyd-Graber et al, 2009), each word of
a sentence is generated by a distribution that com-
bines document-specific topic weights and parse-
tree-specific syntactic transitions.

In Global Topic Random Field(GTRF)
model (Li et al, 2014), sentences of a document
are parsed into dependency trees (Marneffe et
al, 2008) (Manning et al, 2014) (Marneffe et
al, 2006). They show topics of semantically
or syntactically dependent words achieve the
highest similarity and are able to provide more
useful information in topic modeling, which is
also the basic assumption of our model. Then
they propose GTRF to model non-linear topical
dependencies, word topics are sampled based
on graph structure instead of “bag of words”
representation, the conditional independence of
word topic assignment is thus relaxed.

However, GTRF assumes topic assignment of a
word vertex depends on the topic mixture of the
document and its neighboring word vertices, ig-
noring the fact that word vertex can also be influ-
enced by the distance-2 or further word vertices.
In this paper, we extend GTRF model and present
a novel model Extended Global Topic Random
Field (EGTRF) to exploit topical dependency be-
tween words. In EGTRF, the topic assignment of a
word is assumed to depend on both distance-1 and
distance-2 word vertices. An example of a simple
document that has two sentences shows in Figure
1. The two sentences are parsed into dependency
trees respectively, and then merged into a graph.

506



stands

LDA alloation

latent dirichlet

(a) Sentence 1

discovers

It topics

latent

corpus

(b) Sentence 2

stands

LDA alloation

latent

topics

discovers

it corpus

dirichlet

(c) Document

Example document: LDA stands for latent dirichlet allocation. It discovers

latent topcis from corpus.

Example word vertex: allocation

Distance-1 word vertics: {stands, latent, dirichlet}
Distance-2 word vertics: {LDA, topics}

Figure 1: Dependency tree example

Some hidden dependency relations can also be ex-
tracted by merging dependency trees. For exam-
ple, word “allocation” has a new distance-2 word
“topics” after merging. Therefore, EGTRF can
exploit more semantically or syntactically word
dependencies. Theoretically, we can also model
the distance further than 2, however, it leads to
more complicated computation and small increase
of performance.

Another advantage of EGTRF is it incorporates
word features. The word vector representations
are very interesting because the learned vectors
explicitly encode many linguistic regularities and
patterns (Mikolov et al, 2013). We use the pre-
trained model from Google News dataset(about
100 billion words) using word2vec1 tool to repre-
sent each word as a 300-dimensional word vector,
and apply normalized word similarity as a con-
fidence score to indicate how possible two word
vertices share same topic.

We organized the paper as below: EGTRF is
presented in Section 2, variational inference and
parameter estimation are derived in Section 3, ex-
periments on two datasets are showed in Section
4, we conclude the paper in Section 5.

2 Extended Global Topic Random Field

In this section, we first present Extended Global
Random Field(EGRF) in section 2.1, then show
how to model topical dependencies using EGRF
in section 2.2. We incorporate word similarity in-
formation into model in section 2.3.

1https://code.google.com/p/word2vec/

2.1 Extended Global Random Field
After representing document to undirected graph
on previous section, we extend Global Random
Field and give the definition of Extended Global
Random Field to model the graph as below:

Given an undirected graph G, word vertex set is
denoted as W = {wi|i = 1, 2, ..n}, where wi is a
word vertex, and n is the number of unique words
in a document. E1 is distance-1 edge set, E1 =
{(wi, wj)|∃path between wi, wj that length is 1}.
E2 is distance-2 edge set, E2 =
{(wi, wj)|∃path between wi, wj that length is 2}.
The state(topic assignment) of a word vertex w is
generated from Z = {zi|i = 1, 2, ..., k}, k is the
number of topics.

P (G) = fG(g) =
1

| E1 | + | E2 |
∏

w∈W
f(zw)×

(
∑

(w′1,w′′1 )∈E1

f(1)(zw′1
, zw′′1

) +
∑

(w′2,w′′2 )∈E2

f(2)(zw′2
, zw′′2

))

(1)

s.t. 1.f(z) > 0, f(1)(z
′
, z
′′

) > 0, f(2)(z
′
, z
′′

) > 0

2.
∑
z∈Z

f(z) = 1

3.
∑

z′,z′′∈Z
f(z

′
)f(z

′′
)f(1)(z

′
, z
′′

) = 1

4.
∑

z′,z′′∈Z
f(z

′
)f(z

′′
)f(2)(z

′
, z
′′

) = 1

In Equation (1), f(z) is the function defined on
word vertex, which is a probability measure be-
cause of the constraints 1 and 2. f(1)(z, z′) and
f(2)(z, z′) are the function defined on edge set E1
and E2. f(1) and f(2) are not necessarily probabil-
ity measure, however, summing over all possible
states of the product of the edge and the linked
word pair should equal to 1, which are from con-
straints 3 and 4. So f(z′)f(z′′)f(1)(z′, z′′) and
f(z′)f(z′′)f(2)(z′, z′′) are probability measure. g
is one sample of word topic assignments from
graph G. If Equation (1) satisfies all the four con-
straints, it is easy to verify P (G) is also a prob-
ability measure since summing over all possible
samples g equals to 1.

We define the random field as in Equation (1)
a Extended Global Random Field (EGRF). And
EGRF does not have normalization factor, which
is much simplier than models with intractable nor-
malizing factor.

2.2 Topic Model Using EGRF
We define Extended Global Topic Random Field
based on EGRF. EGTRF is a generative proba-

507



bilistic model, the basic idea is that documents
are represented as mixtures of topics, words are
generated depending on the topic mixtures and
graph structure of current document. The genera-
tive process for word sequence of a document is
described as below:

For each document d in corpus D:
Transform document d into graph.
Choose θ ∼ Dir(α).
For each of the n words wn in d:

Choose topic zn ∼ Pegrf (z | θ),
Choose word wn ∼Multi(βzn,wn).

Given Dirichlet priorα, word distribution of topics
β, topic mixture of document θ, topic assignments
z and words w. We obtain the marginal distribu-
tion of a document:

p(w | α, β) =
∫
P (θ | α)

∑
z

Pegrf (z | θ)
∏
n

P (wn | zwn , β)dθ
(2)

We can see the marginal distribution is similar
to LDA except topic assignment of word is sam-
pled by Extended Global Random Field instead
of Multinomial. So the word topic assignment is
no longer conditionally independent. According
to EGRF described in section 2.1, we define the
probability of topic sequence z as below:

Pegrf (z | θ) =
1

| E1 | + | E2 |
∏

w∈V
f(zw)×

(
∑

(w′1,w′′1 )∈E1

f(1)(zw′1
, zw′′1

) +
∑

(w′2,w′′2 )∈E2

f(2)(zw′2
, zw′′2

))

(3)

where f(zw) = Multi(zw|θ) (4)
f(1)(zw′1

, zw′′1
) = σz

w′1
=z

w′′1
λ1 + σz

w′1
6=z

w′′1
λ2 (5)

f(2)(zw′2
, zw′′2

) = σz
w′2

=z
w′′2

λ3 + σz
w′2
6=z

w′′2
λ4 (6)

σ is an indicator function and equals 1 if the
topic assignments of two words on an edge are
same. In order to model Equation (3) as an EGRF,
it must satisfy all the four constraints in Equation
(1). Equation (4) defines word vertex as multino-
mial distribution, and we assign λ1, λ2, λ3 and λ4
nonzero values, then it is clear to verify constraint
1 and 2 are satisfied. To satisfy the constraint 3
and 4, combine with (5), (6), we get the relation
between λ1 and λ2, λ3 and λ4.

∑
θ
2
i λ1 + (1−

∑
θ
2
i )λ2 = 1 i = 1, 2, ..|E1| (7)∑

θ
2
i λ3 + (1−

∑
θ
2
i )λ4 = 1 i = 1, 2, ..|E2| (8)

Lower λ2, λ4 give higher reward to the edge
that connects two word vertices with same topic.
If (7) and (8) hold true, Equation (3) is an EGRF.
And we define the topic model based on EGRF as
Extended Global Topic Random Field(EGTRF).
If |E2| = 0, |E1| 6= 0, EGTRF is equivalent to
GTRF. If |E1| = 0, |E2| = 0, EGTRF is equiva-
lent to LDA.

2.3 Word Similarity Information
The coherent edge is the edge that the two linked
words have same topic. In distance-i edge set,
i= 1, 2. ECi includes all coherent edges, ENCi
contains all non-coherent edges. Then equation
(3) can be represented as below:

Pegrf (z | θ)

=
1

| E1 | + | E2 |
∏

w∈V
Multi(zw | θ)×

(| EC1 | λ1+ | ENC1 | λ2+ | EC2 | λ3+ | ENC2 | λ4)

=

∏
w∈V

Multi(zw | θ)

(| E1 | + | E2 |)θT θ
× (| EC1 | (1− λ2)+ | E1 | λ2θT θ+

| EC2 | (1− λ4)+ | E2 | λ4θT θ)
(9)

From the second line to the third line of Equa-
tion (9), we represent λ1, λ3 as the function of
λ2, λ4 based on (7) and (8). The expectation of
the number of edges in Eci can be computed as:

E(| ECi |) =
∑

(w1,w2)∈Ei
φ

T
w1
φw2Sw1,w2 (10)

φ is the K dimensional variational multinomial
parameters and can be thought as the posterior
probability of a word given the topic assignment.
Sw1,w2 is the similarity measure between word w1
and w2.

As we discussed in section 1, word similarity
information Sw1,w2 works as a confidence score to
model how likely two words on an edge have same
topic. And we make assumption that two words
are more likely to have same topic if they have a
higher similarity score. To get the similarity score
between words, we use word2vec tool to learn the
word representation of each word from pre-trained
model. The word representations are computed
using neural networks, and the learned representa-
tions explicitly encode many linguistic regularities

508



Figure 2: Experimental results on NIPS(left) and 20 news(right) data

and patterns from the corpus. Normalized similar-
ity between word vectors can be regarded as the
confidence score of how possible two words have
same topic. In this way, knowledge from large cor-
pus other than current document collections is in-
corporated to guide topic modeling.

3 Posterior Inference and Parameter
Estimation

We derive Variational Inference for posterior in-
ference. The variational function q is same to the
original LDA paper (Blei et al, 2003). All terms
except P (z|θ) in likelihood function are also same
to LDA, Based on Equation (9), we obtain:

Eq [logPegrf (z | θ)]
≈Eq [log(

∏
n

Multi(zwn | θ))]+

1− λ2
ζ1

Eq(| EC1 |) +
1− λ4
ζ1

Eq(| EC2 |)+

(
| E1 | λ2+ | E2 | λ4

ζ1
− | E1 | + | E2 |

ζ2
)Eq(θ

T
θ)+

log ζ1 − log ζ2

(11)

We get the approximation in Equation (11)
from Taylor series, where ζ1 and ζ2 are Taylor
approximation. Eq(| ECi |) is obtained directly
from (10),Eq(θT θ) is from the property of Dirich-
let distribution. The updating rule of α and β are
same to LDA, γ is updated using Newton method
since we can not obtain the direct updating rule for
γ. φ can be approximated as:

φwn,i ∝ βi,vexp(Ψ(γi)+
1− λ2
ζ1

×
∑

(wn,wm)∈E1
φwm,iSm,n+

1− λ4
ζ1

×
∑

(wn,wp)∈E2
φwp,iSp,n) (12)

EM algorithm is applied using above updating
rules. At E-step, we estimate the best γ and φ
given current α and β. At M-step, we update new
α and β based on obtained γ and φ. We run such
iterations until convergence.

4 Experiment

In this section we study the empirical performance
of EGTRF on two datasets. For each dataset, we
remove very short documents, and compute a vo-
cabulary by removing stop words, rare words, fre-
quent words. Eighty percent data are used for
training, others for testing.

• 20 News Groups: After processing, it con-
tains 13706 documents with a vocabulary of
5164 terms.

• NIPS data (Globerson et al, 2004): Span-
ning from 2000 to 2005. After processing,
it contains 843 documents with a vocabulary
of 6098 terms.

We evaluate how well a model fits the data with
held-out perplexity (Blei et al, 2003) and predic-
tive distribution (Hoffman et al, 2013). Lower
perplexity, higher log predictive probability indi-
cate better generalization performance. We im-
plement GTRF without adding self defined edges
from the original paper, and set λ2 = 0.2 to give
higher reward to edges from E1 that the two word
vertices have same topic. We set λ4 = 1.2 to
give lower(even negative) reward to edges from
E2 that the two word vertices have same topic in
EGTRF, since the distance-1 words are expected
to have greater topical affects than distance-2

509



words. Word is represented as vector from pre-
trained Google News dataset, we use the word vec-
tor learned from original corpus when the word
does not exist in pre-trained Google News dataset.

We choose 10, 20, 30, 50 topics for 20 news
dataset, 10, 15, 20, 25 topics for NIPS dataset.
Figure 2 shows the experimental results of four
models: lda, gtrf, egtrf(EGTRF without word
similarity information), and egtrf+s(EGTRF with
word similarity information) on two datasets. The
results show EGTRF outperforms LDA and GTRF
in general, and EGTRF with word similarity infor-
mation achieves best performance.

We believe modeling distance-2 word vertices
can exploit more semantically or syntactically
word dependencies from document, and word sim-
ilarity information obtained from large corpus can
make up the lack of sufficient information from the
original corpus. Therefore, adding the influence of
distance-2 word vertices and word similarity infor-
mation can improve performance of topic model-
ing.

5 Conclusion

In this paper, we extended Global Topic Random
Field(GTRF) and proposed a novel topic model
Extended Global Topic Random Field(EGTRF)
which can model dependency relation between
adjacent words and distance-2 words. Word
topics are drawed by Extended Global Random
Field(EGRF) instead of Multinomial, the con-
ditional independence of word topic assignment
is thus relaxed. Word similarity information
learned from large corpus is incorporated into the
model. Experiments on two datasets show EGTRF
achieves better performance than GTRF and LDA,
which confirm our assumption that adding topical
dependency of distance-2 words and incorporating
word similarity information can improve model
performance.

References

Amir Globerson, Gal Chechik, Fernando Pereira, Naf-
tali Tishby Euclidean Embedding of Co-occurrence
Data. In Advances in neural information processing
systems. pp. 497-504. 2004.

Amit Gruber, Michal Rosen-Zvi and Yair Wei. Hid-
den Topic Markov Models. In Proceedings of the
Eleventh International Conference on Artificial In-
telligence and Statistic. pp. 163-170. 2007.

David Blei, Andrew Ng., and Michael Jordan. La-
tent Dirichlet Allocation. The Journal of Machine
Learning Research. 3:993-1022, 2003.

Hanna M Wallach. Topic modeling: Beyond bag-of-
words. In International Conference on Machine
Learning. pp. 977-984. ACM, 2006.

Jordan Boyd-Graber and David Blei. Syntactic topic
models. In Neural Information Processing Systems.
pp. 185-192. 2009.

Jun Zhu and Eric P. Xing. Conditional Topic Random
Fields. In Proceedings of the 27th International
Conference on Machine Learning. 2010.

Christopher D. Manning, Mihai Surdeanu, John Bauer,
Jenny Finkel, Steven J. Bethard and David Mc-
Closky. The Stanford CoreNLP Natural Language
Processing Toolkit. In Proceedings of 52nd Annual
Meeting of the Association for Computational Lin-
guistics: System Demonstrations. pp. 55-60. 2014.

Marie-Catherine de Marneffe, Bill MacCartney, and
Christopher D. Manning. Generating typed depen-
dency parses from phrase structure parses. In Pro-
ceedings of LREC. Vol. 6, No. 2006, pp. 449-454.
2006.

Marie-Catherine de Marneffe, Christopher D. Man-
ning. The Stanford typed dependencies represen-
tation. In COLING 2008 Workshop on Cross-
framework and Cross-domain Parser Evaluation.
pp. 1-8. 2008.

Matthew Hoffman, David Blei, Chong Wang, John
Paisley Stochastic Variational Inference The Jour-
nal of Machine Learning Research. 14(1), 1303-
1347. 2013.

Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey
Dean. Efficient Estimation of Word Representations
in Vector Space. In Proceedings of Workshop at
ICLR. arXiv:1301.3781, 2013.

Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg Cor-
rado, and Jeffrey Dean. Distributed Representations
of Words and Phrases and their Compositionality. In
Proceedings of NIPS. pp. 3111-3119. 2013.

Tomas Mikolov, Wen-tau Yih, and Geoffrey Zweig.
Linguistic Regularities in Continuous Space Word
Representations. In Proceedings of NAACL HLT.
pp. 746-751, 2013.

Zhixing Li, Siqiang Wen, Juanzi Li, Peng Zhang and
Jie Tang. On Modeling Non-linear Topical Depen-
dencies. In Proceedings of the 31th International
Conference on Machine Learning. pp. 458-466,
2014.

510


