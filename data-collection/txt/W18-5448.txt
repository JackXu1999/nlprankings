



















































Language Modeling Teaches You More than Translation Does: Lessons Learned Through Auxiliary Syntactic Task Analysis


Proceedings of the 2018 EMNLP Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP, pages 359–361
Brussels, Belgium, November 1, 2018. c©2018 Association for Computational Linguistics

359

Language Modeling Teaches You More Syntax than Translation Does:
Lessons Learned Through Auxiliary Task Analysis

Kelly W. Zhang1
kz918@nyu.edu

Samuel R. Bowman1,2,3
bowman@nyu.edu

1Dept. of Computer Science
New York University

60 Fifth Avenue
New York, NY 10011

2Center for Data Science
New York University

60 Fifth Avenue
New York, NY 10011

3Dept. of Linguistics
New York University
10 Washington Place
New York, NY 10003

1 Introduction

Recently, researchers have found that deep
LSTMs (Hochreiter and Schmidhuber, 1997)
trained on tasks like machine translation learn sub-
stantial syntactic and semantic information about
their input sentences, including part-of-speech
(Belinkov et al., 2017a,b; Blevins et al., 2018).
These findings begin to shed light on why pre-
trained representations, like ELMo and CoVe, are
so beneficial for neural language understanding
models (Peters et al., 2018; McCann et al., 2017).
We still, though, do not yet have a clear under-
standing of how the choice of pretraining objec-
tive affects the type of linguistic information that
models learn. With this in mind, we compare
four objectives—language modeling, translation,
skip-thought, and autoencoding—on their ability
to induce syntactic and part-of-speech informa-
tion, holding constant the quantity and genre of the
training data, as well as the LSTM architecture.

2 Methodology

We control for the data domain by exclusively
training on datasets from WMT 2016 (Bojar et al.,
2016). We train models on all tasks using the par-
allel En-De corpus, which allows us to make fair
comparisons across all tasks. We also augment the
parallel data with a large monolingual corpus from
WMT to examine how the performance of the un-
supervised tasks scales with more data.

We analyze representations learned by lan-
guage models and by the encoders of sequence-
to-sequence models.1 Following Belinkov et al.
(2017a), after pretraining, we fix the LSTM model
parameters and use the hidden states to train aux-
iliary classifiers on several probing tasks. We

1All our encoders are 2-layer, bidirectional LSTMs (500-
D in each direction)—except for our large forward language
models, which are 1000-D and unidirectional.

use two syntactic evaluation tasks: part-of-speech
(POS) tagging on Penn Treebank WSJ (Marcus
et al., 1993) and Combinatorial Categorical Gram-
mar (CCG) supertagging on CCG Bank (Hocken-
maier and Steedman, 2007). CCG supertagging
allows us to measure the degree to which models
learn syntactic structure above the word. We also
measure how much LSTMs simply memorize in-
put sequences with a word identity prediction task.

3 Results

Comparing Pretraining Tasks For all pretrain-
ing dataset sizes, bidirectional language model
(BiLM) and translation encoder representations
outperform skip-thought and autoencoder repre-
sentations on both POS and CCG tagging. Trans-
lation encoders, however, slightly underperform
BiLMs, even when both models are trained on
the same amount of data. Furthermore, BiLMs
trained on the smallest amount of data (1 mil-
lion sentences) outperform models trained on all
other tasks using larger dataset sizes (5 million
sentences for translation, and 63 million sentences
for skip-thought and autoencoding). Especially
since BiLMs do not require aligned data to train,
the superior performance of BiLM representations
on these tasks suggests that BiLMs (like ELMo)
are better than translation encoders (like CoVe) for
transfer learning of syntactic information.

Untrained Baseline Surprisingly, we find that
the untrained LSTM baseline—frozen after ran-
dom initialization—performs quite well on syn-
tactic tagging tasks (a few percentage points be-
hind BiLMs) when using all auxiliary data; how-
ever, decreasing the amount of classifier training
data leads to a significantly greater drop in the un-
trained encoder performance compared to trained
encoders. We hypothesize that the classifiers can
recover neighboring word identity information—



360

Figure 1: POS accuracies when training on different amounts of encoder and classifier data. We show
results for the best performing layer of each model. The most frequent class baseline is word-conditional.

even from untrained LSTMs representations—and
thus perform well on tagging tasks by memorizing
word configurations and their associated tags from
the training data. We test this hypothesis directly
with the word identity task.

Word Identity For this task, we train classifiers
to take LSTM hidden states and predict the identi-
ties of the words from different time steps. For ex-
ample, for the sentence “I love NLP .” and a time
step shift of -2, we would train the classifier to take
the hidden state for “NLP” and predict the word
“I”. While trained encoders outperform untrained
ones on both POS and CCG tagging, we find that
all trained LSTMs underperform untrained ones
on word identity prediction. This finding confirms
that trained encoders genuinely capture substantial
syntactic features, beyond mere word identity, that
the auxiliary classifiers can use.

Effect of Depth Belinkov et al. (2017a) find
that, for translation models, the first layer con-
sistently outperforms the second on POS tagging.
We find that this pattern holds for all our models,
except BiLMs, where the first and second layers
perform equivalently. This pattern occurs even in
untrained models, which suggests that POS infor-
mation is stored on the lower layer not necessarily
because the training tasks encourage this, but due
to properties of the deep LSTM architecture. For
CCG supertagging though, the second layer per-
forms better than the first in some cases (first layer
performs best for untrained LSTMs). Which layer

performs best appears to be independent of abso-
lute performance on the supertagging task.

On word identity prediction, we find that for
both trained and untrained models, the first layer
outperforms the second layer when predicting the
identity of the immediate neighbors of a word.
However, the second layer tends to outperform
the first at predicting the identity of more distant
neighboring words. As is the case for convolu-
tional neural networks, our findings suggest that
depth in recurrent neural networks has the effect
of increasing the “receptive field” and allows the
upper layers to have representations that capture a
larger context. These results reflect the findings of
Blevins et al. (2018) that for trained models, upper
levels of LSTMs encode more abstract syntactic
information, since more abstract information gen-
erally requires larger context information.

4 Conclusion

By controlling for the genre and quantity of the
training data, we make fair comparisons between
several data-rich training tasks in their ability to
induce syntactic information. Our results suggest
that for transfer learning, bidirectional language
models like ELMo (Peters et al., 2018) capture
more useful features than translation encoders—
and that this holds even on genres for which data is
not abundant. Our work also highlights the inter-
esting behavior of untrained LSTMs, which show
an ability to preserve the contents of their inputs
better than trained models.



361

References
Yonatan Belinkov, Nadir Durrani, Fahim Dalvi, Hassan

Sajjad, and James R. Glass. 2017a. What do Neural
Machine Translation Models Learn about Morphol-
ogy? ACL.

Yonatan Belinkov, Lluı́s Màrquez, Hassan Sajjad,
Nadir Durrani, Fahim Dalvi, and James Glass.
2017b. Evaluating Layers of Representation in Neu-
ral Machine Translation on Part-of-Speech and Se-
mantic Tagging Tasks. IJCNLP.

Terra Blevins, Omer Levy, and Luke Zettlemoyer.
2018. Deep RNNs Learn Hierarchical Syntax. ACL.

Ondrej Bojar, Rajen Chatterjee, Christian Federmann,
Yvette Graham, Barry Haddow, Matthias Huck,
Antonio Jimeno Yepes, Philipp Koehn, Varvara
Logacheva, Christof Monz, Matteo Negri, Aure-
lie Neveol, Mariana Neves, Martin Popel, Matt
Post, Raphael Rubino, Carolina Scarton, Lucia Spe-
cia, Marco Turchi, Karin Verspoor, and Marcos
Zampieri. 2016. Findings of the 2016 Conference
on Machine Translation (WMT16). ACL.

Sepp Hochreiter and Jüergen Schmidhuber. 1997.
Long Short-Term Memory. Neural Computation,
9(8):1735–1780.

Julia Hockenmaier and Mark Steedman. 2007. CCG-
bank: A Corpus of CCG Derivations and Depen-
dency Structures Extracted from the Penn Treebank.
Computational Linguistics.

Mitchell P. Marcus, Mary Ann Marcinkiewicz, and
Beatrice Santorini. 1993. Building a Large Anno-
tated Corpus of English: The Penn Treebank. Com-
putational Linguistics.

Bryan McCann, James Bradbury, Caiming Xiong, and
Richard Socher. 2017. Learned in Translation: Con-
textualized Word Vectors. NIPS.

Matthew E. Peters, Mark Neumann, Mohit Iyyer, Matt
Gardner, Christopher Clark, Kenton Lee, and Luke
Zettlemoyer. 2018. Deep contextualized word rep-
resentations. NAACL.


