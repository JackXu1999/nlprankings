




































Multi-Task Learning for Japanese Predicate Argument Structure Analysis


Proceedings of NAACL-HLT 2019, pages 3404–3414
Minneapolis, Minnesota, June 2 - June 7, 2019. c©2019 Association for Computational Linguistics

3404

Multi-task Learning for Japanese Predicate Argument Structure Analysis

Hikaru Omori
Tokyo Metropolitan University

omori-hikaru@ed.tmu.ac.jp

Mamoru Komachi
Tokyo Metropolitan University
komachi@tmu.ac.jp

Abstract

An event-noun is a noun that has an argu-
ment structure similar to a predicate. Re-
cent works, including those considered state-
of-the-art, ignore event-nouns or build a single
model for solving both Japanese predicate ar-
gument structure analysis (PASA) and event-
noun argument structure analysis (ENASA).
However, because there are interactions be-
tween predicates and event-nouns, it is not suf-
ficient to target only predicates. To address
this problem, we present a multi-task learn-
ing method for PASA and ENASA. Our multi-
task models improved the performance of both
tasks compared to a single-task model by shar-
ing knowledge from each task. Moreover, in
PASA, our models achieved state-of-the-art re-
sults in overall F1 scores on the NAIST Text
Corpus. In addition, this is the first work to
employ neural networks in ENASA.

1 Introduction

Japanese predicate argument structure analysis
(PASA) examines semantic structures between the
predicate and its arguments in a text. The identifi-
cation of the argument structure such as “who did
what to whom?” is useful for natural language pro-
cessing that requires deep analysis of complicated
sentences such as machine translation and recog-
nizing textual entailment. PASA is a task targeted
at predicates such as verbs and adjectives. How-
ever, there are also many nouns that have event-
related arguments in a sentence. We call these
nouns that refer to events event-nouns, for exam-
ple, a verbal noun (sahen nouns) such as houkoku
“report” or a deverbal noun (nominalized forms of
verbs) such as sukui “rescue.”

Figure 1 shows examples of PASA and event-
noun argument structure analysis (ENASA). In
the NAIST Text Corpus (Iida et al., 2007), both
predicates and event-nouns have one of three core

(a) He reports the result to his boss.

(b) His progress report was too short; hence, he got
scolded by his boss.

(c) He sent a brief note to his boss.

Figure 1: Examples of PASA and ENASA. The edges
denote dependency paths.

case roles, nominative (NOM), accusative (ACC),
and dative (DAT) as an argument. According to
Iida et al. (2007), predicates have almost no ar-
gument in the same bunsetsu1 phrase. However,
in the case of event-nouns, approximately half
of the accusative and dative arguments appear in
the same bunsetsu phrase. Accordingly, although
PASA and ENASA are semantically highly re-
lated, they are syntactically different tasks. How-
ever, most previous studies focused on predi-
cates only; hence, there are few studies that focus

1Functional chunk in Japanese. It consists of one or more
content words (noun, verb, adjective, etc.) followed by zero
or more function words (postposition, auxiliary verb, etc.). A
verb phrase in Japanese thus cannot bear noun arguments in
the same bunsetsu.



3405

on event-nouns (Komachi et al., 2007; Taira et al.,
2008). To identify the semantic units of a sentence
and to correctly understand syntactic relations, it
is not sufficient to target only PASA.

Thus, we propose a multi-task learning model
that effectively leverages ENASA and improves
PASA. Our proposed model is based on an end-to-
end multilayer bi-directional recurrent neural net-
work (RNN) used in recent works, and the model
has networks that distinguish task-independent in-
formation and task-specific information.

In summary, the main contributions of this work
are the following:

1. This is the first attempt to design a multi-task
learning framework for PASA and ENASA,
and we show that our models improve the
performance of both tasks.

2. Although our model is a simple model that
does not consider the interactions between
multiple predicates, it achieves a state-of-the-
art result on the NAIST Text Corpus (NTC)
in PASA by combining syntactic information
as one of the features.

3. For ENASA, this is the first work to em-
ploy neural networks to effectively incorpo-
rate PASA.

2 Related Work

2.1 Japanese PASA and ENASA Approaches
Many machine learning-based methods have been
studied in Japanese PASA. Traditional models
take pointwise approaches that construct inde-
pendent models for each core case role (NOM,
ACC, DAT). Taira et al. (2008) proposed a super-
vised model that learns features of each case us-
ing decision lists and support vector machines.
Imamura et al. (2009) proposed a model that com-
bines a maximum entropy model with a language
model trained from large-scale newspaper articles.
Hayashibe et al. (2011) designed three models ex-
ploiting argument position and type and deter-
mined the maximum likelihood output using pair-
wise comparison.

However, the joint approach that optimizes the
scores of all predicate-argument pairs in a sen-
tence simultaneously showed better results than
the pointwise approach. Yoshikawa et al. (2011)
proposed a model that considers dependency be-
tween multiple predicate-argument relations us-
ing Markov logic networks. Ouchi et al. (2015)

jointly optimized the combinations among multi-
ple predicates and arguments in a sentence using a
bipartite graph.

Except for (Taira et al., 2008), these stud-
ies focused on the analysis of predicates while
there are few studies that focus on event-nouns.
Komachi et al. (2007) decomposed ENASA into
two tasks: event-hood determination and argu-
ment identification; they proposed a supervised
method using lexico-syntactic patterns. Event-
hood determination is the most important char-
acteristic that semantically differentiates ENASA
from PASA. It is a task to determine whether a
noun refers to an event (e.g., houkoku can refer to
either “to report” or the outcome of reporting ac-
tion, “a report”). Since the previous ENASA mod-
els adopted the pointwise approach with a single
model, they did not explore the effective features
in each task. In contrast, our models simultane-
ously optimize three core case roles. Moreover,
the proposed models allow us to distinguish be-
tween task-shared and task-specific features using
multi-task learning.

2.2 PASA using neural networks

Some neural models have achieved higher perfor-
mance than traditional machine learning models
in Japanese PASA. Shibata et al. (2016) replaced
Ouchi et al. (2015)’s scoring function with feed
forward neural networks. Matsubayashi and Inui
(2017) represented a dependency path between a
predicate and its argument with path embeddings
and showed that even the local model without mul-
tiple predicates can outperform a global model.

Moreover, some end-to-end models have been
proposed in Japanese PASA. Ouchi et al. (2017)
proposed an end-to-end model based on the
model using eight-layer bi-directional long short-
term memory (LSTM) proposed by Zhou and Xu
(2015) and considered the interaction of mul-
tiple predicates simultaneously using a Grid
RNN. Matsubayashi and Inui (2018) combined
self-attention with Ouchi et al. (2017)’s model
to directly capture interaction among multiple
predicate-arguments. In particular, the model
improved the performance of arguments that
have no syntactic dependency with predicates
and achieved a state-of-the-art result on Japanese
PASA.



3406

2.3 Semantic Role Labeling

Semantic role labeling (SRL) is a similar task
to Japanese PASA. Recently, several end-to-end
models using neural networks showed high per-
formance in English SRL (Zhou and Xu, 2015;
He et al., 2017; Tan et al., 2018). Strubell et al.
(2018) proposed a multi-task learning model
that jointly learned dependency parsing, part-of-
speech tagging, predicate detection, and SRL
based on multi-head self-attention. Ouchi et al.
(2018) proposed a span-based SRL model using
bi-directional LSTMs and achieved state-of-the-
art results. The authors scored all possible spans
for each label and selected correct spans satisfy-
ing constraints when decoding. In terms of the
event-noun research, Gerber and Chai (2010) used
pointwise mutual information (PMI) as a feature
for 10 event-nouns with high frequency and iden-
tified semantic roles using a logistic regression
model.

There were several LSTM models that also
achieved high accuracy gains in Chinese SRL
(Wang et al., 2015; Roth and Lapata, 2016;
Sha et al., 2016; Marcheggiani et al., 2017;
Qian et al., 2017). For event-nouns, Li et al.
(2009) showed that combining effective features
in verbal SRL with nominal SRL can improve
results. Although the authors did not demonstrate
that verbal SRL also improves performance in
combination with nominal SRL, we show that our
model improves performance in both PASA and
ENASA.

3 Japanese PASA and ENASA

3.1 Task Description

Japanese predicate (event-noun) argument struc-
ture analysis is a task to extract arguments for
certain predicates (event-nouns) and assign three
case labels, NOM, ACC and DAT (Iida et al.,
2007). Arguments are divided into four categories
(Taira et al., 2008) according to the positions with
their predicates (event-nouns).

Dep Arguments depend on their predicate (event-
noun), or a predicate (event-noun) depends
on its arguments.

Zero Arguments and their predicate (event-noun)
are in the same sentence, but the arguments
are omitted by zero anaphora. Therefore,
they have no direct dependency.

Inter-zero Zero anaphoric arguments and their
predicate (event-noun) are not in the same
sentence.

Bunsetsu Arguments and their event-noun are in
the same bunsetsu.

A sentence w = w1, w2, · · · , wT and a predi-
cate (event-noun) p = p1, p2, · · · , pq are given
as input. Iida et al. (2006), Imamura et al. (2009),
and Sasano and Kurohashi (2011) also analyze
Inter-zero, which is a difficult task because the
whole document must be searched. Follow-
ing existing research (Ouchi et al., 2015, 2017;
Matsubayashi and Inui, 2017, 2018; Taira et al.,
2008), we only focus on three categories where ar-
guments and their predicate (event-noun) are in the
same sentence. In addition, we exclude the Bun-
setsu category from the PASA evaluation follow-
ing Ouchi et al. (2017) and Matsubayashi and Inui
(2018).

3.2 End-to-end Single Model
Our single model is based on an end-to-end ap-
proach (Zhou and Xu, 2015; Ouchi et al., 2017;
Matsubayashi and Inui, 2018). Additionally, we
add new features. Figure 2 shows the network ar-
chitecture of our base model.

3.2.1 Input Layer
Each word wt ∈ [w1, · · · , wT ] is converted to a
feature representation xt ∈ [x1, · · · ,xT ] at the in-
put layer. We use six types of features. The feature
representation xt is defined as follows:

xt = x
as
t ⊕ x

posi
t ⊕ x

dep
t ⊕ x

type
t ⊕ xtaskp (1)

where (⊕) indicates concatenation of vectors.

Argument Structure Predicate (event-noun)
wp and argument candidates wt are converted to
the vectors xast ∈ R2dw by the word embedding
matrix.

Position This is a feature that represents the po-
sitional relation between wp and wt. The feature
is calculated by subtracting the word index of ar-
gument candidates from the word index of pred-
icates (event-nouns). We use two types of units
to represent relative position: word unit pwordt and
bunsetsu unit pbunsetsut , which are converted to the
word positional vector pwordt ∈ Rdp and the bun-
setsu positional vector pbunsetsut ∈ Rdp , respec-
tively, by the word and bunsetsu positional embed-



3407

Figure 2: End-to-end single model.

ding matrices. We concatenate these two vectors
and obtain the positional vectors xposit ∈ R2dp .

Dependency This is a feature that represents the
dependency relation between wp and wt. We set
five types of dependency relations:

i). Argument candidates depend on the predicate
(event-noun).

ii). The predicate (event-noun) depends on the
argument candidates.

iii). No dependency relations between the predi-
cate (event-noun) and argument candidates.

iv). The predicate and candidate arguments are in
the same bunsetsu.

v). The event-noun and candidate arguments are
in the same bunsetsu.

The dependency relation dt is converted to the de-
pendency vector xdept ∈ Rdd by the dependency
relation embedding matrix. The dependency type
in Figure 2 shows how to make dependency fea-
tures in Figure 1b as an example. We define the
dependency type from the syntactic information
annotated in the NTC.

In previous work, dependency features are used
differently from our study. Imamura et al. (2009)
used a binary feature that represents whether or
not there is a dependency relation between the
predicate and its arguments. We employ more
fine-grained relation types to adapt to event-nouns.
Matsubayashi and Inui (2017) represented the in-
teractions between a predicate and its arguments
using path embedding. In contrast, we define
different types for a predicate and event-noun to
distinguish event-nouns from predicates and learn
embeddings to find the associated latent structures.

Event-hood Type This is a binary feature to flag
all predicates (event-nouns) in a sentence inspired
by Matsubayashi and Inui (2018). The purpose of
this feature is to prevent predicates from becoming
arguments and to help some event-nouns become
arguments. The event-hood type vector xtypet ∈
R2 of a candidate indicates [0,1] if the candidate is
a predicate, [1,0] if the candidate is an event-noun,
and [0,0] otherwise. The predicate and event-noun
are annotated in the NTC.

Task Label This is a binary feature vector
xtaskp ∈ R1 that indicates 1 if the task is predicate
argument structure analysis; otherwise, 0.

3.2.2 RNN Layer
We use the gated recurrent unit (GRU) (Cho et al.,
2014) for RNN. The RNN layers are made up
of L layers of stacked bi-directional GRU. Ad-
ditionally, we apply the residual connections
(He et al., 2016) following Ouchi et al. (2017);
Matsubayashi and Inui (2018). At each time step
t, the hidden state hlt ∈ Rdh in the l ∈ [1, · · · , L]-
th layer is calculated as follows:

hlt =

{
gl(hl−1t ,h

l
t−1) (l = odd)

gl(hl−1t ,h
l
t+1) (l = even)

(2)

where gl(·) denotes the l-th layer GRU function.
In addition, h0t = xt.

3.2.3 Output Layer
In the output layer, we input each hidden state hLt .
Then, we obtain the output vector ot using the
softmax function:

ot = softmax(Woh
L
t + bo) (3)

where Wo ∈ R4×dh is the parameter matrix, and
bo ∈ R4 is the bias term. The output vector repre-
sents the probability for each argument candidate



3408

(a) Single (b) Multi-input (c) Multi-RNN

(d) Multi-output (e) Multi-ALL

Figure 3: Proposed models: (a) Single, (b) Multi-input, (c) Multi-RNN, (d) Multi-output, (e) Multi-ALL.

over four labels, [NOM, ACC, DAT, ELSE]. ELSE
denotes that the candidate argument does not have
a case label. In testing, the maximum probability
label is selected as the output label. We train the
model using the cross-entropy loss function.

4 Multi-task Model

Multi-task learning has been successfully applied
to various natural language processing tasks
(Collobert et al., 2011; Søgaard and Goldberg,
2016; Luong et al., 2016; Hashimoto et al.,
2017; Liu et al., 2017; Stoyanov et al., 2018;
Marasovic and Frank, 2018; Strubell et al., 2018).
One of the advantages of multi-task learning
is that it learns better representation, which is
robust against task-dependent noise by increasing
training data. In this paper, we introduce multi-
task learning to PASA and ENASA for the first
time. We propose three methods to extend the
end-to-end single model to the multi-task learning
model in the input layer, RNN layer, and output
layer. Figure 3 shows the proposed models. Our
final model combines all three methods (Figure
3e).

4.1 Multi Input Layer

Even if the surface form is the same, the con-
texts are different for predicates and event-nouns.
For example, the event-noun houkoku “report” in
Figure 1b has an argument in the same bunsetsu

unlike predicates. Moreover, the event-noun also
has a nominative argument role for the predicate
mijikai “short”. Therefore, given this, we pre-
pare a task-specific word embedding matrix that
addresses the task-specific distribution of words.
The predicate is converted to PASA-specific vec-
tors xpt ∈ Rd

′
w by the PASA-specific predicate em-

bedding matrix. Similarly, the event-noun is con-
verted to ENASA-specific vectors xnt ∈ Rd

′
w by

the ENASA-specific event-noun embedding ma-
trix. These matrices are randomly initialized and
can be learned during training.

The feature vector xt is defined as follows:

xt =

{
xt ⊕ xpt (PASA)
xt ⊕ xnt (ENASA)

(4)

4.2 Multi RNN Layer
Previous work (Søgaard and Goldberg, 2016;
Hashimoto et al., 2017) proposed hierarchical
multi-task learning models that exploited features
obtained from easy tasks for difficult tasks. These
studies showed that performance improves when
low-layer RNN representations are trained in easy
tasks and high-layer RNN are leveraged for dif-
ficult tasks. Therefore, we construct a network
that hierarchically overlaps a task-specific RNN
on a task-independent RNN. Lower RNN lay-
ers learn task-independent knowledge representa-
tions. Then, the task-specific RNN adjusts the rep-
resentations for each task. At each time step t, the



3409

hidden state ml
′
t ∈ Rd

′
h in the l′ ∈ [1, · · · , L′]-th

layer is calculated as follows:

ml
′
t =

{
gl

′
(ml

′−1
t ,m

l′
t−1) (l

′ = odd)

gl
′
(ml

′−1
t ,m

l′
t+1) (l

′ = even)
(5)

gl
′
(·) =

{
gl

′
p (·) (PASA)
gl

′
n (·) (ENASA)

(6)

where gl
′
(·), gl′p (·), and gl

′
n (·) denote the l′-th layer

GRU functions. In addition, m0t = h
L
t .

4.3 Multi Output Layer
The position of arguments is different with respect
to predicates and event-nouns. For example, pred-
icates seldom have arguments in the same bun-
setsu. In contrast, event-nouns often have argu-
ments in the same bunsetsu, compound nouns, for
example. Therefore, it is intuitive and natural to
divide the output layer into task-independent and
task-specific layers. The task-specific output vec-
tors are calculated as follows:

opt = W
p
oht + b

p
o (7)

ont = W
n
oht + b

n
o (8)

gt = σ(Wght + bg) (9)

where Wpo ,Wno ,Wg ∈ R4×dh are the parameter
matrices, and bpo ,bno ,bg ∈ R4 are the bias terms.
ht is the hidden state of the last layer. We com-
bine task-specific output vectors opt ,o

n
t with task-

independent output vector ot by the gate gt.

ct =

{
gt ⊙ ot + (1− gt)⊙ opt (PASA)
gt ⊙ ot + (1− gt)⊙ ont (ENASA)

(10)

ot = softmax(ct) (11)

where (⊙) denotes the element-wise product. The
output vector ot represents the probability of
[NOM, ACC, DAT, ELSE].

5 Experiments

5.1 Dataset and Setting
We use NTC 1.5 for our experiments. We divide
the dataset into training, development, and test sets
in the same way as Taira et al. (2008). We use
morphological and syntactic information, such as
the word boundaries, the bunsetsu boundaries and
the dependency relations provided in the NTC.

For the development and test sets, if there are
two or more arguments annotated with the same

the dimension of word embeddings dw 300
the dimension of position embeddings dp 16
the dimension of dependency embeddings dd 16
the dimension of hidden states dh 300
the number of GRU layers L 4
the dimension of task-specific word embeddings d′w 16
the dimension of task-specific hidden states d′h 300
the number of task-specific GRU layers L′ 2
dropout rate 0.4
batch size 8
gradient clipping 4

Table 1: Hyperparameters.

case label in a sentence, we set an argument that
only has a dependency relation with a predicate
as a correct answer and assign the ELSE label to
other arguments. If there is no dependency rela-
tion, we set an argument with the shortest distance
|wp − wt| as a correct answer. If the distance is
equal, an argument on the left side of a predicate
is considered a correct answer.

In NTC 1.5, if there is a predicate phrase, such
as “verbal noun + suru,” suru is annotated as a
predicate word. We consider the verbal noun as
the predicate word at the preprocessing step to
match the surface of a predicate with that of an
event-noun. Take the predicate houkoku-suru “to
report” and an event-noun houkoku “report” as an
example. Although wp before preprocessing are
suru and houkoku, wp are unified to houkoku after
preprocessing.

5.2 Hyperparameters

We use pre-trained embeddings2 for the initial val-
ues of the word embedding matrix. The initial val-
ues of the other embedding matrices are sampled
according to a uniform distribution of [-0.25,0.25].
We convert words appearing more than once in the
training set into word vectors and the remaining
words into the unknown word vector. We adopt
AdaDelta (ϵ = 10−6，ρ = 0.95) as the optimiza-
tion method. We set the number of epochs to 20
and evaluate the model with the highest F1 scores
on the development set. Table 1 shows the hyper-
parameters.

5.3 Results

We evaluate each model with the NTC 1.5 test.
The experimental results for the argument struc-
ture analysis of predicates and event-nouns are
shown in Tables 2 and 3.

2http://www.asahi.com/shimbun/medialab/word embedding



3410

Dep Zero
Method ALL SD ALL NOM ACC DAT ALL NOM ACC DAT
Ouchi+ 17 81.42 88.17 88.75 93.68 64.38 47.12 50.65 32.35 7.52
M&I 17 83.50 ±0.17 89.89 91.19 95.18 61.90 51.79 54.69 41.8 17
M&I 18 83.94 ±0.12 90.26 90.88 94.99 67.57 55.55 57.99 48.9 23
Single 83.62 ±0.17 90.09 90.45 94.84 69.77 51.87 54.73 43.48 11.40
Multi-input 83.88 ±0.11 90.27 90.65 95.12 69.86 53.01 55.82 44.68 10.77
Multi-RNN 83.91 ±0.23 90.17 90.58 95.07 67.94 53.31 55.85 45.71 9.97
Multi-output 83.77 ±0.20 90.13 90.68 94.89 68.16 53.93 56.73 43.79 9.45
Multi-ALL 83.82 ±0.10 90.15 90.68 95.06 67.56 53.50 56.37 45.36 8.70
Multi-RNN+DEP 84.55 ±0.11 90.69 91.28 95.25 70.07 51.56 54.29 42.67 1.85
Multi-output+DEP 84.73 ±0.11 90.82 91.46 95.29 70.69 52.29 55.14 42.15 1.81
Multi-ALL+DEP 84.75 ±0.16 90.88 91.40 95.37 71.02 52.35 55.10 42.54 2.32
M&I 17 (ens. of 5) 84.07 90.24 91.59 95.29 62.61 53.66 56.47 44.7 16
M&I 18 (ens. of 10) 85.34 91.26 91.84 95.57 70.8 58.07 60.21 52.5 26
Multi-RNN+DEP (ens. of 5) 85.85 91.61 92.11 95.87 72.63 53.41 55.96 46.10 0
Multi-output+DEP (ens. of 5) 85.83 91.52 92.12 95.69 72.72 54.35 57.02 45.95 0
Multi-ALL+DEP (ens. of 5) 86.01 91.63 92.15 95.80 72.95 54.99 57.84 45.20 0

Table 2: F1 scores on the PASA test set. Single is a base model without multi-task learning.

Dep Zero Bunsetsu
Method ALL SD ALL NOM ACC DAT ALL NOM ACC DAT ALL NOM ACC DAT
Taira+ 08 on NTC 1.4 68.01 62.46 56.05 36.19 20.46 6.62 78.93 77.96 58.13
Single 66.21 ±0.15 74.64 76.06 74.54 51.28 46.05 49.67 33.36 13.63 78.24 76.67 81.75 48.55
Multi-input 67.89 ±0.42 75.62 76.63 75.78 57.17 49.07 52.81 36.95 19.39 79.35 77.31 83.31 51.03
Multi-RNN 67.96 ±0.44 75.86 76.90 76.33 54.46 48.67 52.18 38.47 18.89 79.08 77.24 82.89 50.93
Multi-output 67.96 ±0.17 76.25 77.18 76.90 54.97 48.74 52.48 36.09 19.64 79.02 77.00 83.04 50.60
Multi-ALL 68.00 ±0.41 75.90 77.16 76.05 53.00 49.66 53.37 37.64 14.46 79.05 77.32 82.61 51.83
Multi-ALL+DEP 67.68 ±0.39 75.95 77.18 76.11 55.26 47.57 51.21 35.14 15.65 79.06 77.44 82.66 51.10
Multi-ALL (ens. of 5) 71.14 78.63 79.66 78.83 58.29 52.49 56.41 39.02 16.42 81.90 80.25 85.21 56.29
Multi-ALL+DEP (ens. of 5) 69.90 77.86 78.89 78.16 58.46 49.36 53.10 36.36 17.23 81.16 79.74 84.57 52.99

Table 3: F1 scores on the ENASA test set.

Predicate Argument Structure Analysis The
first set of rows in Table 2 shows the results of
previous models. Ouchi+ 17 is the model from
the Multi-Seq model in (Ouchi et al., 2017).
M&I 17 is the model in (Matsubayashi and Inui,
2017). M&I 18 is the model from the MP-POOL-
SELFATT model in (Matsubayashi and Inui,
2018).

The second set of rows in Table 2 shows the re-
sults of the proposed models. These models do
not use the dependency feature. Compared with
the single model, all multi-task learning models
improved the overall F1 scores. Among them,
Multi-RNN improved the overall F1 score from
the single model by 0.29 points. In previous work,
Ouchi et al. (2017); Matsubayashi and Inui (2018)
see improvements of 0.27 and 0.55 F1 points
in their baseline models by considering multi-
ple predicate-argument interactions. Therefore,
we show that multi-task learning with ENASA
achieved comparable effects as these studies in
PASA.

The third set of rows shows the results of pro-
posed models using all features including the de-
pendency feature. Multi-ALL+DEP achieved the
best F1 score among all the models including pre-
vious state-of-the-art models. In particular, the de-
pendency feature was effective for Dep arguments.
On the other hand, the performance for Zero ar-
guments was poor. This result suggests that the
dependency feature causes the model to optimize
mainly for Dep arguments since Dep arguments
are more numerous than Zero arguments.

The fourth set of rows shows the results of en-
semble models. Overall, our proposed model out-
performed the previous ensemble model by 0.67
points in the overall F1 score. Moreover, our mod-
els are simple models that independently analyze
each predicate in a sentence. Nevertheless, our
models achieved higher results than Ouchi et al.
(2017); Matsubayashi and Inui (2018). Although
recent works have researched the method whereby
multiple predicate-argument interactions are con-
sidered simultaneously, how to use syntactic in-



3411

(a) predicate: 結成 “organize,” NOM:会長 “president,” ACC:会派 “faction.”

(b) predicate: 左右 “determine,” NOM:カギ “key,” ACC:行方 “whereabouts.”

(c) event-noun: 打開 “break,” NOM:カギ “key,” ACC:事態 “situation.”

(d) predicate: 回避 “avoid,” NOM:トップ “top,” ACC:責任 “responsibility.”

(e) predicate: 押す “push,” ACC:丸 “circle,” DAT:左下 “lower left.”

(f) predicate: 果たす “play,” NOM: E, ACC:役割 “role,” DAT:発病 “pathogenesis.”

Figure 4: Examples of analysis errors on the PASA test set

formation in the end-to-end model is a subject for
future work.

Event-noun Argument Structure Analysis
The first set of rows in Table 3 shows the results
of a previous model in event-noun argument
structure analysis. Taira+ 08 is the model from
(Taira et al., 2008). Since its scores are from NTC
1.4, the model cannot be directly compared to
our models. Compared with the single model, all
multi-task models improved the overall F1 scores.
However, Multi-ALL+DEP compared unfavorably
with Multi-ALL even though it was the best
PASA architecture. Therefore, this implies that
the dependency type feature between the predicate
and its argument is not effective in ENASA.

5.4 Analysis

In Figure 4, we compare the PASA results from
test sets for each model. In Examples (a), (b)
and (d), the single model failed to predict cor-
rect arguments but the Multi-RNN model correctly
predicted arguments. In Example (a), the sin-
gle model incorrectly predicted that arguments do
not exist in this sentence. Comparing the train-
ing set of each task, although the number of event-
nouns is approximately one-third of the number of
predicates, the number of kessei 結成 “organize
(event-nouns)” is approximately twice the num-
ber of kessei結成 “organize (predicates).” Accord-
ingly, we showed that the Multi-RNN model ef-
fectively leverages the information of event-nouns
using multi-task learning.

In Example (b), the single model incorrectly



3412

predicted that the NOM argument does not ex-
ist, but the multi-RNN predicted the correct argu-
ments. Comparing the training set, there is sayuu
左右 “determine (predicate)” but not sayuu 左
右 “determine (event-noun).” However, there are
some kagi カギ “key (arguments of predicates)”
in the PASA training set, and there is one kagiカ
ギ “key (argument of event-noun)” in the ENASA
training set (Example (c)). Moreover, in Exam-
ple (c), dakai 打開 “break (event-noun)” depends
on kagi カギ “key” like sayuu 左右 “determine
(predicate)” in Example (b); however, no pred-
icate depends on kagi カギ “key” in the train-
ing set. Accordingly, the Multi-RNN model also
leverages the arguments of event-nouns and the
positional relations between event-nouns and their
arguments.

Example (d) is an interesting case in which
a predicate kaihi 回避 “avoid” and its argument
sekinin 責任 “responsibility” are located in the
same bunsetsu. Although this argument type
(Bunsetsu) is excluded from the evaluation target
in PASA, it is common as a compound noun in
ENASA. Therefore, the single model wrongly pre-
dicted that the ACC argument does not exist, but
multi-RNN was able to predict the answer using
the specific knowledge of event-nouns.

In contrast, in Example (e), the single model
correctly predicted the answer, but the multi-RNN
model failed to predict the correct arguments.
Multi-RNN incorrectly predicted that the DAT ar-
gument does not exist in this sentence. However,
niに, a postpositional particle located after an ar-
gument, often indicates a dative case. Neverthe-
less, multi-RNN often predicted a wrong DAT ar-
gument by ignoring ni に. Therefore, for DAT
analysis, the information of event-nouns adversely
affects PASA.

In Example (f), the Multi-ALL+DEP model
correctly predicted the answer, but the Multi-
ALL model failed. Specifically, Multi-ALL+DEP
correctly predicted that the ACC argument is
yakuwari 役割 “role,” which is dependent on
hatasu 果たす “play.” However, the Multi-ALL
incorrectly predicted that the ACC argument is
kaimei 解明 “solution.” Similarly, Multi-ALL
without syntactic information made many mis-
takes, including attributive modification, such as
Figure 1c. Table 4 shows the results of the
two PASA models for attributive modification in-
stances. Multi-ALL+DEP considerably outper-

ALL NOM ACC DAT
Multi-ALL 80.31 83.37 72.16 19.48
Multi-ALL+DEP 81.83 84.67 74.41 28.31

Table 4: F1 scores on the PASA test set with respect to
attributive modifications.

formed Multi-ALL for all cases using dependency
features. Therefore, these results suggest that the
dependency type feature is effective for PASA
with respect to attributive modifications.

6 Conclusion

We design a multi-task learning model for predi-
cate and event-noun argument structure analysis.
The experiment results show that the multi-task
models outperform the single-task model on the
NAIST Test Corpus for both tasks. Moreover, our
model achieves a state-of-the-art result for PASA.
In addition, this is the first work to employ neural
networks for ENASA. In future work, we plan to
consider multiple predicates and event-nouns.

References
Kyunghyun Cho, Bart van Merrienboer, Caglar Gul-

cehre, Dzmitry Bahdanau, Fethi Bougares, Holger
Schwenk, and Yoshua Bengio. 2014. Learning
phrase representations using RNN encoder–decoder
for statistical machine translation. In Proceedings of
EMNLP, pages 1724–1734.

Ronan Collobert, Jason Weston, Léon Bottou, Michael
Karlen, Koray Kavukcuoglu, and Pavel Kuksa.
2011. Natural language processing (almost) from
scratch. J. Mach. Learn. Res., 12:2493–2537.

Matthew Gerber and Joyce Chai. 2010. Beyond Nom-
Bank: A study of implicit arguments for nominal
predicates. In Proceedings of ACL, pages 1583–
1592.

Kazuma Hashimoto, caiming xiong, Yoshimasa Tsu-
ruoka, and Richard Socher. 2017. A joint many-task
model: Growing a neural network for multiple NLP
tasks. In Proceedings of EMNLP, pages 1923–1933.

Yuta Hayashibe, Mamoru Komachi, and Yuji Mat-
sumoto. 2011. Japanese predicate argument struc-
ture analysis exploiting argument position and type.
In Proceedings of IJCNLP, pages 201–209.

Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian
Sun. 2016. Deep residual learning for image recog-
nition. In Proceedings of CVPR, pages 770–778.

Luheng He, Kenton Lee, Mike Lewis, and Luke Zettle-
moyer. 2017. Deep semantic role labeling: What
works and what’s next. In Proceedings of ACL,
pages 473–483.



3413

Ryu Iida, Kentaro Inui, and Yuji Matsumoto. 2006. Ex-
ploiting syntactic patterns as clues in zero-anaphora
resolution. In Proceedings of COLING/ACL, pages
625–632.

Ryu Iida, Mamoru Komachi, Kentaro Inui, and Yuji
Matsumoto. 2007. Annotating a Japanese text cor-
pus with predicate-argument and coreference rela-
tions. In Proceedings of the Linguistic Annotation
Workshop, pages 132–139.

Kenji Imamura, Kuniko Saito, and Tomoko Izumi.
2009. Discriminative approach to predicate-
argument structure analysis with zero-anaphora res-
olution. In Proceedings of ACL-IJCNLP, pages 85–
88.

Mamoru Komachi, Ryu Iida, Kentaro Inui, and Yuji
Matsumoto. 2007. Learning based argument struc-
ture analysis of event-nouns in Japanese. In Pro-
ceedings of PACLING, pages 120–128.

Junhui Li, Guodong Zhou, Hai Zhao, Qiaoming Zhu,
and Peide Qian. 2009. Improving nominal SRL in
Chinese language with verbal SRL information and
automatic predicate recognition. In Proceedings of
EMNLP, pages 1280–1288.

Pengfei Liu, Xipeng Qiu, and Xuanjing Huang. 2017.
Adversarial multi-task learning for text classifica-
tion. In Proceedings of ACL, pages 1–10.

Minh-Thang Luong, Quoc V. Le, Ilya Sutskever, Oriol
Vinyals, and Lukasz Kaiser. 2016. Multi-task se-
quence to sequence learning. In Proceedings of
ICLR.

Ana Marasovic and Anette Frank. 2018. SRL4ORL:
Improving opinion role labeling using multi-task
learning with semantic role labeling. In Proceedings
of NAACL, pages 583–594.

Diego Marcheggiani, Anton Frolov, and Ivan Titov.
2017. A simple and accurate syntax-agnostic neural
model for dependency-based semantic role labeling.
In Proceedings of CoNLL, pages 411–420.

Yuichiroh Matsubayashi and Kentaro Inui. 2017. Re-
visiting the design issues of local models for
Japanese predicate-argument structure analysis. In
Proceedings of IJCNLP, pages 128–133.

Yuichiroh Matsubayashi and Kentaro Inui. 2018.
Distance-free modeling of multi-predicate interac-
tions in end-to-end Japanese predicate argument
structure analysis. In Proceedings of COLING,
pages 94–106.

Hiroki Ouchi, Hiroyuki Shindo, Kevin Duh, and Yuji
Matsumoto. 2015. Joint case argument identifica-
tion for Japanese predicate argument structure anal-
ysis. In Proceedings of ACL-IJCNLP, pages 961–
970.

Hiroki Ouchi, Hiroyuki Shindo, and Yuji Matsumoto.
2017. Neural modeling of multi-predicate interac-
tions for Japanese predicate argument structure anal-
ysis. In Proceedings of ACL, pages 1591–1600.

Hiroki Ouchi, Hiroyuki Shindo, and Yuji Matsumoto.
2018. A span selection model for semantic role
labeling. In Proceedings of EMNLP, pages 1630–
1642.

Feng Qian, Lei Sha, Baobao Chang, LuChen Liu, and
Ming Zhang. 2017. Syntax aware LSTM model for
semantic role labeling. In Proceedings of the 2nd
Workshop on Structured Prediction for Natural Lan-
guage Processing, pages 27–32.

Michael Roth and Mirella Lapata. 2016. Neural se-
mantic role labeling with dependency path embed-
dings. In Proceedings of ACL, pages 1192–1202.

Ryohei Sasano and Sadao Kurohashi. 2011. A dis-
criminative approach to Japanese zero anaphora res-
olution with large-scale lexicalized case frames. In
Proceedings of IJCNLP, pages 758–766.

Lei Sha, Sujian Li, Baobao Chang, Zhifang Sui, and
Tingsong Jiang. 2016. Capturing argument relation-
ship for Chinese semantic role labeling. In Proceed-
ings of EMNLP, pages 2011–2016.

Tomohide Shibata, Daisuke Kawahara, and Sadao
Kurohashi. 2016. Neural network-based model for
Japanese predicate argument structure analysis. In
Proceedings of ACL, pages 1235–1244.

Anders Søgaard and Yoav Goldberg. 2016. Deep
multi-task learning with low level tasks supervised
at lower layers. In Proceedings of ACL, pages 231–
235.

Veselin Stoyanov, Heng Ji, Ying Lin, and Shengqi
Yang. 2018. A multi-lingual multi-task architecture
for low-resource sequence labeling. In Proceedings
of ACL, pages 799–809.

Emma Strubell, Patrick Verga, Daniel Andor,
David Weiss, and Andrew McCallum. 2018.
Linguistically-informed self-attention for semantic
role labeling. In Proceedings of EMNLP, pages
5027–5038.

Hirotoshi Taira, Sanae Fujita, and Masaaki Nagata.
2008. A Japanese predicate argument structure anal-
ysis using decision lists. In Proceedings of EMNLP,
pages 523–532.

Zhixing Tan, Mingxuan Wang, Jun Xie, Yidong Chen,
and Xiaodong Shi. 2018. Deep semantic role la-
beling with self-attention. In Proceedings of AAAI,
pages 4929–4936.

Zhen Wang, Tingsong Jiang, Baobao Chang, and Zhi-
fang Sui. 2015. Chinese semantic role labeling with
bidirectional recurrent neural networks. In Proceed-
ings of EMNLP, pages 1626–1631.



3414

Katsumasa Yoshikawa, Masayuki Asahara, and Yuji
Matsumoto. 2011. Jointly extracting Japanese
predicate-argument relation with Markov logic. In
Proceedings of IJCNLP, pages 1125–1133.

Jie Zhou and Wei Xu. 2015. End-to-end learning of
semantic role labeling using recurrent neural net-
works. In Proceedings of ACL-IJCNLP, pages
1127–1137.


