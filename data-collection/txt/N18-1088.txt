



















































Parsing Tweets into Universal Dependencies


Proceedings of NAACL-HLT 2018, pages 965–975
New Orleans, Louisiana, June 1 - 6, 2018. c©2018 Association for Computational Linguistics

Parsing Tweets into Universal Dependencies

Yijia Liu
Harbin Institute of Technology

yjliu@ir.hit.edu.cn

Yi Zhu
University of Cambridge

yz568@cam.ac.uk

Wanxiang Che Bing Qin
Harbin Institute of Technology

Nathan Schneider
Georgetown University

Noah A. Smith
University of Washington

Abstract

We study the problem of analyzing tweets
with Universal Dependencies (UD; Nivre
et al., 2016). We extend the UD guide-
lines to cover special constructions in
tweets that affect tokenization, part-of-
speech tagging, and labeled dependen-
cies. Using the extended guidelines, we
create a new tweet treebank for English
(TWEEBANK V2) that is four times larger
than the (unlabeled) TWEEBANK V1 intro-
duced by Kong et al. (2014). We char-
acterize the disagreements between our
annotators and show that it is challeng-
ing to deliver consistent annotation due
to ambiguity in understanding and ex-
plaining tweets. Nonetheless, using the
new treebank, we build a pipeline sys-
tem to parse raw tweets into UD. To over-
come annotation noise without sacrific-
ing computational efficiency, we propose
a new method to distill an ensemble of 20
transition-based parsers into a single one.
Our parser achieves an improvement of 2.2
in LAS over the un-ensembled baseline
and outperforms parsers that are state-of-
the-art on other treebanks in both accuracy
and speed.

1 Introduction

NLP for social media messages is challenging, re-
quiring domain adaptation and annotated datasets
(e.g., treebanks) for training and evaluation. Pi-
oneering work by Foster et al. (2011) annotated
7,630 tokens’ worth of tweets according to the
phrase-structure conventions of the Penn Treebank
(PTB; Marcus et al., 1993), enabling conversion to
Stanford Dependencies. Kong et al. (2014) further
studied the challenges in annotating tweets and

presented a tweet treebank (TWEEBANK), consist-
ing of 12,149 tokens and largely following con-
ventions suggested by Schneider et al. (2013),
fairly close to Yamada and Matsumoto (2003) de-
pendencies (without labels). Both annotation ef-
forts were highly influenced by the PTB, whose
guidelines have good grammatical coverage on
newswire. However, when it comes to informal,
unedited, user-generated text, the guidelines may
leave many annotation decisions unspecified.

Universal Dependencies (Nivre et al., 2016,
UD) were introduced to enable consistent anno-
tation across different languages. To allow such
consistency, UD was designed to be adaptable to
different genres (Wang et al., 2017) and languages
(Guo et al., 2015; Ammar et al., 2016). We pro-
pose that analyzing the syntax of tweets can bene-
fit from such adaptability. In this paper, we intro-
duce a new English tweet treebank of 55,607 to-
kens that follows the UD guidelines, but also con-
tends with social media-specific challenges that
were not covered by UD guidelines.1 Our anno-
tation includes tokenization, part-of-speech (POS)
tags, and (labeled) Universal Dependencies. We
characterize the disagreements among our annota-
tors and find that consistent annotation is still chal-
lenging to deliver even with the extended guide-
lines.

Based on these annotations, we nonetheless de-
signed a pipeline to parse raw tweets into Uni-
versal Dependencies. Our pipeline includes: a
bidirectional LSTM (bi-LSTM) tokenizer, a word
cluster–enhanced POS tagger (following Owoputi
et al., 2013), and a stack LSTM parser with
character-based word representations (Ballesteros
et al., 2015), which we refer to as our “baseline”
parser. To overcome the noise in our annotated

1We developed our treebank independently of a similar
effort for Italian tweets (Sanguinetti et al., 2017). See §2.5
for a comparison.

965



data and achieve better performance without sac-
rificing computational efficiency, we distill a 20-
parser ensemble into a single greedy parser (Hin-
ton et al., 2015). We show further that learn-
ing directly from the exploration of the ensemble
parser is more beneficial than learning from the
gold standard “oracle” transition sequence. Exper-
imental results show that an improvement of more
than 2.2 points in LAS over the baseline parser
can be achieved with our distillation method. It
outperforms other state-of-the-art parsers in both
accuracy and speed.

The contributions of this paper include:

• We study the challenges of annotating tweets
in UD (§2) and create a new tweet treebank
(TWEEBANK V2), which includes tokeniza-
tion, part-of-speech tagging, and labeled Uni-
versal Dependencies. We also characterize
the difficulties of creating such annotation.

• We introduce and evaluate a pipeline system
to parse the raw tweet text into Universal De-
pendencies (§3). Experimental results show
that it performs better than a pipeline of the
state-of-the-art alternatives.

• We propose a new distillation method for
training a greedy parser, leading to better per-
formance than existing methods and without
efficiency sacrifices.

Our dataset and system are publicly available
at https://github.com/Oneplus/Tweebank and
https://github.com/Oneplus/twpipe.

2 Annotation

We first review TWEEBANK V1 of Kong et al.
(2014), the previous largest Twitter dependency
annotation effort (§2.1). Then we introduce the
differences in our tokenization (§2.2) and part-of-
speech (§2.3) (re)annotation with O’Connor et al.
(2010) and Gimpel et al. (2011), respectively, on
which TWEEBANK V1 was built. We describe our
effort of adapting the UD conventions to cover
tweet-specific constructions (§2.4). Finally, we
present our process of creating a new tweet tree-
bank, TWEEBANK V2, and characterize the diffi-
culties in reaching consistent annotations (§2.6).

2.1 Background: TWEEBANK
The annotation effort we describe stands in con-
trast to the previous work by Kong et al. (2014).

Their aim was the rapid development of a de-
pendency parser for tweets, and to that end they
contributed a new annotated corpus, TWEEBANK,
consisting of 12,149 tokens. Their annotations
added unlabeled dependencies to a portion of the
data annotated with POS tags by Gimpel et al.
(2011) and Owoputi et al. (2013) after rule-based
tokenization (O’Connor et al., 2010). Kong et
al. also contributed a system for parsing; we de-
fer the discussion of their parser to §3.

Kong et al.’s rapid, small-scale annotation ef-
fort was heavily constrained. It was carried out by
annotators with only cursory training, no clear an-
notation guidelines, and no effort to achieve con-
sensus on controversial cases. Annotators were
allowed to underspecify their analyses. Most of
the work was done in a very short amount of time
(a day). Driven both by the style of the text they
sought to annotate and by exigency, some of their
annotation conventions included:

• Allowing an annotator to exclude tokens
from the dependency tree. A clear criterion
for exclusion was not given, but many tokens
were excluded because they were deemed
“non-syntactic.”

• Allowing an annotator to merge a multiword
expression into a single node in the depen-
dency tree, with no internal structure. Anno-
tators were allowed to take the same step with
noun phrases.

• Allowing multiple roots, since a single tweet
might contain more than one sentence.

These conventions were justified on the grounds of
making the annotation easier for non-experts, but
they must be revisited in our effort to apply UD to
tweets.

2.2 Tokenization

Our tokenization strategy lies between the strategy
of O’Connor et al. (2010) and that of UD. There is
a tradeoff between preservation of original tweet
content and respecting the UD guidelines.

The regex-based tokenizer of O’Connor et al.
(2010)—which was originally designed for an ex-
ploratory search interface called TweetMotif, not
for NLP—preserves most whitespace-delimited
tokens, including hashtags, at-mentions, emoti-
cons, and unicode glyphs. They also treat contrac-
tions and acronyms as whole tokens and do not

966



split them. UD tokenization,2 in order to better
serve dependency annotation, treats each syntactic
word as a token. They therefore more aggressively
split clitics from contractions (e.g., gonna is tok-
enized as gon and na; its is tokenized as it and s
when s is a copula). But acronyms are not touched
in the UD tokenization guidelines. Thus, we fol-
low the UD tokenization for contractions and leave
acronyms like idc (“I don’t care”) as a single to-
ken.

In the different direction of splitting tokens,
UD guidelines also suggest to merge multi-token
words (e.g., 20 000) into one single token in some
special cases. We witnessed a small number of
tweets that contain multi-token words (e.g., Y O,
and R E T W E E T) but didn’t combine them for
simplicity. Such tokens only account for 0.07%
and we use the UD goeswith relation to resolve
these cases in the dependency annotations.

2.3 Part-of-Speech Annotation

Before turning to UD annotations, we
(re)annotated the data with POS tags, for
consistency with other UD efforts, which adopt
the universal POS tagset.3 In some cases, non-
corresponding tag conflicts arose between the
UD English Web Treebank treebank conventions
(UD_English-EWT; de Marneffe et al., 2014)4

and the conventions of Gimpel et al. (2011).
In these cases, we always conformed to UD,
enabling consistency (e.g., when we exploit the
existing UD_English-EWT treebank in our parser
for tweets, §3). For example, the nominal URL in
Figure 2 is tagged as other (X) and + is tagged as
symbol (SYM) rather than conjunction (CCONJ).

Tokens that do not have a syntactic function (see
Figure 1, discussed at greater length in the next
section) were usually annotated as other (X), ex-
cept for emoticons, which are tagged as symbol
(SYM), following UD_English-EWT.

Tokens that abbreviate multiple words (such as
idc) are resolved to the POS of the syntactic head
of the expression, following UD conventions (in
this example, the head care is a verb, so idc is
tagged as a verb). When the token is not phrasal,
we use the POS of the left-most sub-phrase. For

2http://universaldependencies.org/u/overview/
tokenization.html

3A revised and extended version of Petrov et al. (2012)
with 17 tags.

4https://github.com/UniversalDependencies/UD_

English-EWT

example, mfw (“my face when”) is tagged as a
noun (for face).

Compared to the effort of Gimpel et al. (2011),
our approach simplifies some matters. For exam-
ple, if a token is not considered syntactic by UD
conventions, it gets an other (X) tag (Gimpel et
al. had more extensive conventions). Other phe-
nomena, like abbreviations, are more complicated
for us, as discussed above; Gimpel et al. used a
single part of speech for such expressions.

Another important difference follows from the
difference in tokenization. As discussed in §2.2,
UD calls for more aggressive tokenization than
that of O’Connor et al. (2010) which opted out of
splitting contractions and possessives. As a con-
sequence of adopting O’Connor et al.’s (2010) to-
kenization, Gimpel et al. introduced new parts of
speech for these cases instead.5 For us, these to-
kens must be split, but universal parts of speech
can be applied.

2.4 Universal Dependencies Applied to
Tweets

We adopt UD version 2 guidelines to annotate
the syntax of tweets. In applying UD annotation
conventions to tweets, the choices of Kong et al.
(2014) must be revisited. We consider the key
questions that arose in our annotation effort, and
how we resolved them.

Acronym abbreviations. We follow Kong et al.
(2014) and annotate the syntax of an acronym as a
single word without normalization. Their syntac-
tic functions are decided according to their con-
text. Eisenstein (2013) studied the necessity of
normalization in social media text and argued that
such normalization is problematic. Our solution to
the syntax of abbreviations follows the spirit of his
argument. Because abbreviations which clearly
carry syntactic functions only constitute 0.06% of
the tokens in our dataset, we believe that normal-
ization for acronyms is an unnecessarily compli-
cated step.

Non-syntactic tokens. The major characteristic
that distinguishes tweets from standard texts is that
a large proportion of tokens don’t carry any syn-
tactic function. In our annotation, there are five
types of non-syntactic tokens commonly seen in
tweets: sentiment emoticons, retweet markers and

5These tags only account for 2.7% of tokens, leading to
concerns about data sparseness in tagging and all downstream
analyses.

967



Perfect ♥ RT @coldplay : Fix You from the back #ColdplayMinneapolis http://bit.ly/2dj2WCl Nice p ...
ADJ SYM X X PUNCT VERB NOUN ADP DET NOUN X X ADJ X PUNCT

root

discourse

root

discourse

discourse
punct

obj

obl

case
det

discourse
list

root

discourse
punct

Figure 1: An example to illustrate non-syntactic tokens: sentiment emoticon, retweet marker and its following
at-mention, topical hashtag, referential URL, and truncated word. This is a concatenation of three real tweets.

@username its #awesome u gonna ♥ it Chk out our cooool project on http://project_link + RT it
PROPN PRON ADJ PRON VERB VERB PRON VERB ADP PRON ADJ NOUN ADP X SYM VERB PRON

root

vocative
nsubj

parataxis
nsubj xcomp obj

parataxis

compound:prt

obj
nmod:poss

amod

obl

case

conj

cc obj

Figure 2: An example to illustrate informal but syntactic tokens. This is a contrived example inspired by several
tweets.

syntactic (%) non-syntactic (%)
emoticons 0.25 0.95
RT 0.14 2.49
hashtag 1.02 1.24
URL 0.67 2.38
truncated words 0.00 0.49
total 2.08 7.55

Table 1: Proportions of non-syntactic tokens in our
annotation. These statistics are obtained on 140
character–limited tweets.

their following at-mentions, topical hashtags, ref-
erential URLs, and truncated words.6 Figure 1 il-
lustrates examples of these non-syntactic tokens.
As discussed above, these are generally tagged
with the other (X) part of speech, except emoti-
cons, which are tagged as symbol (SYM). In our an-
notation, 7.55% of all tokens are belong to one of
the five types; detailed statistics can be found in
Table 1.

It is important to note that these types may, in
some contexts, have syntactic functions. For ex-
ample, besides being a discourse marker, RT can
abbreviate the verb retweet; emoticons and hash-
tags may be used as content words within a sen-
tence; and at-mentions can be normal vocative
proper nouns: see Figure 2. Therefore, the cri-

6The tweets we analyze have at most 140 characters. Al-
though Twitter has doubled the tweet length limit to 280 char-
acters since our analysis, we believe this type of token will
still remain.

teria for annotating a token as non-syntactic must
be context-dependent.

Inspired by the way UD deals with punctuation
(which is canonically non-syntactic), we adopt the
following conventions:

• If a non-syntactic token is within a sentence
that has a clear predicate, it will be attached
to this predicate. The retweet construction is
a special case and we will discuss its treat-
ment in the following paragraph.

• If the whole sentence is a sequence of non-
syntactic tokens, we attach all these tokens to
the first one.

• Non-syntactic tokens are mostly labeled as
discourse, but URLs are always labeled as
list, following the UD_English-EWT dataset.

Kong et al. (2014) proposed an additional pre-
processing step, token selection, in their annota-
tion process. They required the annotators to first
select the non-syntactic tokens and exclude them
from the final dependency annotation. In order to
keep our annotation conventions in line with UD
norms and preserve the original tweets as much
as possible, we include non-syntactic tokens in
our annotation following the conventions above.
Compared with Kong et al. (2014), we also gave
a clear definition of non-syntactic tokens, which
helped us avoid confusion during annotation.

968



Retweet construction. Figure 1 shows an exam-
ple of the retweet construction (RT @coldplay :).
This might be treated as a verb phrase, with RT as
a verb and the at-mention as an argument. This
solution would lead to an uninformative root word
and, since this expression is idiomatic to Twit-
ter, might create unnecessary confusion for down-
stream applications aiming to identify the main
predicate(s) of a tweet. We therefore treat the
whole expression as non-syntactic, including as-
signing the other (X) part of speech to both RT and
@coldplay, attaching the at-mention to RT with
the discourse label and the colon to RT with the
punct(uation) label, and attaching RT to the predi-
cate of the following sentence.

Constructions handled by UD. A number of
constructions that are especially common in
tweets are handled by UD conventions: ellipsis,
irregular word orders, and paratactic phrases and
sentences not explicitly delineated by punctuation.

Vocative at-mentions. Another idiomatic con-
struction on Twitter is a vocative at-mention
(sometimes a signal that a tweet is a reply to a
tweet by the mentioned user). We treat these at-
mentions as vocative expressions, labeling them
with POS tag proper noun (PROPN) and attaching
them to the main predicate of the sentence it is
within with the label vocative as in UD guidelines
(see Figure 2 for an example).

2.5 Comparison to PoSTWITA-UD

The first Twitter treebank annotated with Uni-
versal Dependencies was the PosTWITA-UD cor-
pus for Italian (Sanguinetti et al., 2017), which
consists of 6,738 tweets (119,726 tokens). In
their convention, tokenization tends to preserve
the original tweet content but two special cases,
articulated prepositions (e.g., nella as in la) and
clitic clusters (e.g. guardandosi as guardando si),
are tokenized. Their lemmas include spelling nor-
malization, whereas our lemmas only normalize
casing and inflectional morphology. The current
UD guidelines on lemmas are flexible, so variation
between treebanks is expected.7

With respect to tweet-specific constructions,
Sanguinetti et al.’s (2017) and our interpretations
of headedness are the same, but we differ in the
relation label. For topical hashtags, we use dis-

7http://universaldependencies.org/u/overview/
morphology.html#lemmas

course while they used parataxis. In referen-
tial URLs, we use list (following the precedent
of UD_English-EWT) while they used dep. Our
choice of discourse for sentiment emoticons is in-
spired by the observation that emoticons are an-
notated as discourse by UD_English-EWT; San-
guinetti et al. (2017) used the same relation for the
emoticons. Retweet constructions and truncated
words were not explicitly touched by Sanguinetti
et al. (2017). Judging from the released tree-
bank8, the RT marker, at-mention, and colon in the
retweet construction are all attached to the pred-
icate of the following sentence with dep, voca-
tive:mention and punct. We expect that the official
UD guidelines will eventually adopt standards for
these constructions so the treebanks can be harmo-
nized.

2.6 TWEEBANK V2

Following the guidelines presented above, we cre-
ate a new Twitter dependency treebank, which we
call TWEEBANK V2.

2.6.1 Data Collection
TWEEBANK V2 is built on the original data of
TWEEBANK V1 (840 unique tweets, 639/201 for
training/test), along with an additional 210 tweets
sampled from the POS-tagged dataset of Gimpel
et al. (2011) and 2,500 tweets sampled from the
Twitter stream from February 2016 to July 2016.9

The latter data source consists of 147.4M English
tweets after being filtered by the lang attribute in
the tweet JSON and langid.py.10 As done by Kong
et al. (2014), the annotation unit is always the
tweet in its entirety—which may consist of mul-
tiple sentences—not the sentence alone. Before
annotation, we use a simple regular expression to
anonymize usernames and URLs.

2.6.2 Annotation Process
Our annotation process was conducted in two
stages. In the first stage, 18 researchers worked
on the TWEEBANK V1 portion and the additional
210 tweets and created the initial annotations in
one day. Before annotating, they were given a tu-
torial overview of the general UD annotation con-
ventions and our guidelines specifically for anno-
tating tweets. Both the guidelines and annotations

8https://github.com/UniversalDependencies/UD_

Italian-PoSTWITA
9Data downloaded from https://archive.org/.

10https://github.com/saffsd/langid.py

969



TWEEBANK V1 TWEEBANK V2
split tweets tokens tweets tokens
train 639 9,310 1,639 24,753
dev. – – 710 11,742
test 201 2,839 1,201 19,112

total 840 12,149 3,550 55,607

Table 2: Statistics of TWEEBANK V2 and comparison
with TWEEBANK V1.

Olympic gold medallist whipped by John Fisher

root

amod
compound acl

obl
case flat

root

nsubj

Figure 3: An example of disagreement; one annotator’s
parse is shown above, disagreeing arcs from the other
annotator are shown below. This is a real example in
our annotation.

were further refined by the authors of this paper to
increase the coverage of our guidelines and solve
inconsistencies between different annotators dur-
ing this exercise. In the second stage, a tokenizer,
a POS tagger, and a parser were trained on the an-
notated data from the first stage (1,050 tweets in
total), and used to automatically analyze the sam-
pled 2,500 tweets. Authors of this paper manu-
ally corrected the parsed data and finally achieved
3,550 labeled tweets.11 Newly created annotations
are split into train, development, and test sets and
appended to the original splits of TWEEBANK V1.
Statistics of our annotations and data splits are
shown in Table 2.

We report the inter-annotator agreement be-
tween the annotators in the second stage. There
is very little disagreement on the tokenization an-
notation. The agreement rate is 96.6% on POS,
88.8% on unlabeled dependencies, and 84.3% on
labeled dependencies. Further analysis shows the
major disagreements on POS involve entity names
(30.6%) and topical hashtags (18.1%). Taking the
example in Figure 1, “Fix you” can be understood
as a verbal phrase but also as the name of the Cold-
play’s single and tagged as proper noun. An exam-

11Manual annotation was done with Arborator (Gerdes,
2013), a web platform for drawing dependency trees.

t s g o n n a b e$ $

0 1 1 0 0 1 0 1 0$ $

… …i

1

Char
Embed.

Figure 4: The bi-LSTM tokenizer that segments ‘its
gonna be’ into ‘it s gon na be’.

ple of a disagreement on dependencies is shown
in Figure 3. Depending on whether this is an ex-
ample of a zero copula construction, or a clause-
modified noun, either annotation is plausible.

3 Parsing Pipeline

We present a pipeline system to parse tweets into
Universal Dependencies. We evaluate each com-
ponent individually, and the system as a whole.

3.1 Tokenizer
Tokenization, as the initial step of many NLP
tasks, is non-trivial for informal tweets, which
include hashtags, at-mentions, and emoticons
(O’Connor et al., 2010). Context is often required
for tokenization decisions; for example, the aster-
isk in 4*3 is a separate token signifying multipli-
cation, but the asterisk in sh*t works as a mask to
evoke censorship and should not be segmented.

We introduce a new character-level bidirec-
tional LSTM (bi-LSTM) sequence-labeling model
(Huang et al., 2015; Ma and Hovy, 2016) for tok-
enization. Our model takes the raw sentence and
tags each character in this sentence as whether it is
the beginning of a word (1 as the beginning and 0
otherwise). Figure 4 shows the architecture of our
tokenization model. Space is treated as an input
but deterministically assigned a special tag $.

Experimental results. Our preliminary results
showed that our model trained on the combination
of UD_English-EWT and TWEEBANK V2 outper-
formed the one trained only on the UD_English-
EWT or TWEEBANK V2, consistent with previ-
ous work on dialect treebank parsing (Wang et al.,
2017). So we trained our tokenizer on the train-
ing portion of TWEEBANK V2 combined with the
UD_English-EWT training set and tested on the
TWEEBANK V2 test set. We report F1 scores,
combining precision and recall for token identi-
fication. Table 3 shows the tokenization results,

970



System F1
Stanford CoreNLP 97.3

Twokenizer 94.6
UDPipe v1.2 97.4

our bi-LSTM tokenizer 98.3

Table 3: Tokenizer comparison on the TWEEBANK V2
test set.

compared to other available tokenizers. Stan-
ford CoreNLP (Manning et al., 2014) and Two-
kenizer (O’Connor et al., 2010)12 are rule-based
systems and were not adapted to the UD tok-
enization scheme. The UDPipe v1.2 (Straka and
Straková, 2017) model was re-trained on the same
data as our system. Compared with UDPipe, we
use an LSTM instead of a GRU in our model
and we also use a larger size for hidden units (64
vs. 20), which has stronger representational power.
Our bi-LSTM tokenizer achieves the best accuracy
among all these tokenizers. These results speak to
the value of statistical modeling in tokenization for
informal texts.

3.2 Part-of-Speech Tagger

Part-of-speech tagging for tweets has been exten-
sively studied (Ritter et al., 2011; Gimpel et al.,
2011; Derczynski et al., 2013; Owoputi et al.,
2013; Gui et al., 2017). We therefore consider
existing POS taggers for tweets instead of devel-
oping our own. On the annotation scheme de-
signed in §2.3, based on UD and adapted for Twit-
ter, we compared several existing systems: the
Stanford CoreNLP tagger, Owoputi et al.’s (2013)
word cluster–enhanced tagger (both greedy and
CRF variants), and Ma and Hovy’s (2016) neu-
ral network tagger which achieves the state-of-the-
art performance on PTB. Gui et al. (2017) pre-
sented a state-of-the-art neural tagger for Twit-
ter, but their implementation works only with the
PTB tagset, so we exclude it. All compared sys-
tems were re-trained on the combination of the
UD_English-EWT and TWEEBANK V2 training
sets. We use Twitter-specific GloVe embeddings
released by Pennington et al. (2014) in all neural
taggers and parsers.13

12We use the updated version of Twokenizer from Owoputi
et al. (2013).

13http://nlp.stanford.edu/data/glove.twitter.
27B.zip

System Accuracy
Stanford CoreNLP 90.6

Owoputi et al., 2013 (greedy) 93.7
Owoputi et al., 2013 (CRF) 94.6

Ma and Hovy, 2016 92.5

Table 4: POS tagger comparison on gold-standard to-
kens in the TWEEBANK V2 test set.

Tokenization System F1
Stanford CoreNLP 92.3

our bi-LSTM tokenizer (§3.1) 93.3

Table 5: Owoputi et al. (2013) POS tagging perfor-
mance with automatic tokenization on the TWEEBANK
V2 test set.

Experimental results. We tested the POS tag-
gers on the TWEEBANK V2 test set. Results
with gold-standard tokenization are shown in Ta-
ble 4. Careful feature engineering and Brown
et al. (1992) clusters help Owoputi et al.’s (2013)
feature-based POS taggers to outperform Ma and
Hovy’s (2016) neural network model.

Results of the Owoputi et al. (2013) tagger with
non-greedy inference on automatically tokenized
data are shown in Table 5. We see that errors in to-
kenization do propagate, but tagging performance
is above 93% with our tokenizer.

3.3 Parser

Social media applications typically require pro-
cessing large volumes of data, making speed an
important consideration. We therefore begin with
the neural greedy stack LSTM parser introduced
by Ballesteros et al. (2015), which can parse a sen-
tence in linear time and harnesses character repre-
sentations to construct word vectors, which should
help mitigate the challenge of spelling variation.
We encourage the reader to refer their paper for
more details about the model.

In our initial experiments, we train our parser on
the combination of UD_English-EWT and TWEE-
BANK V2 training sets. Gold-standard tokeniza-
tion and automatic POS tags are used. Automatic
POS tags are assigned with 5-fold jackknifing.
Hyperparameters are tuned on the TWEEBANK V2
development set. Unlabeled attachment score and
labeled attachment score (including punctuation)
are reported. All the experiments were run on a
Xeon E5-2670 2.6 GHz machine.

Reimers and Gurevych (2017) and others have

971



System UAS LAS Kt/s
Kong et al. (2014) 81.4 76.9 0.3
Dozat et al. (2017) 81.8 77.7 1.7

Ballesteros et al. (2015) 80.2 75.7 2.3
Ensemble (20) 83.4 79.4 0.2

Distillation (α = 1.0) 81.8 77.6 2.3
Distillation (α = 0.9) 82.0 77.8 2.3

Distillation w/ exploration 82.1 77.9 2.3

Table 6: Dependency parser comparison on TWEE-
BANK V2 test set, with automatic POS tags. We use
Ballesteros et al. (2015) as our baseline and build the
ensemble and distilling model over it. The “Kt/s” col-
umn shows the parsing speed evaluated by thousands
of tokens the model processed per second.

pointed out that neural network training is nonde-
terministic and depends on the seed for the random
number generator. Our preliminary experiments
confirm this finding, with a gap of 1.4 LAS on de-
velopment data between the best (76.2) and worst
(74.8) runs. To control for this effect, we report the
average of five differently-seeded runs, for each of
our models and the compared ones.

Initial results. The first section of Table 6 com-
pares the stack LSTM with TWEEBOPARSER (the
system of Kong et al., 2014) and the state-of-the-
art parser in the CoNLL 2017 evaluations, due to
Dozat et al. (2017). Kong et al.’s (2014) parser
is a graph-based parser with lexical features and
word cluster and it uses dual decomposition for
decoding. The parser in Dozat et al. (2017) is also
a graph-based parser but includes character-based
word representations and uses a biaffine classifier
to predict whether an attachment exists between
two words. Both of the compared systems require
superlinear runtime due to graph-based parsing.
They are re-trained on the same data as our sys-
tem. Our baseline lags behind by nearly two LAS
points but runs faster than both of them.

Ensemble. Due to ambiguity in the training
data—which most loss functions are not robust to
(Frénay and Verleysen, 2014), including the log
loss we use, following Ballesteros et al. (2015)—
and due to the instability of neural network train-
ing, we follow Dietterich (2000) and consider an
ensemble of twenty parsers trained using differ-
ent random initialization. To parse at test time,
the transition probabilities of the twenty mem-
bers of the ensemble are averaged. The result
achieves LAS of 79.4, outperforming all three sys-

tems above (Table 6).

Distillation. The shortcoming of the 20-parser
ensemble is, of course, that it requires twenty
times the runtime of a single greedy parser, mak-
ing it the slowest system in our comparison. Kun-
coro et al. (2016) proposed the distillation of 20
greedy transition-based parser into a single graph-
based parser; they transformed the votes of the en-
semble into a structured loss function. However,
as Kuncoro et al. pointed out, it is not straightfor-
ward to use a structured loss in a transition-based
parsing algorithm. Because fast runtime is so im-
portant for NLP on social media, we introduce a
new way to distill our greedy ensemble into a sin-
gle transition-based parser (the first such attempt,
to our knowledge).

Our approach applies techniques from Hinton
et al. (2015) and Kim and Rush (2016) to parsing.
Note that training a transition-based parser typi-
cally involves the transformation of the training
data into a sequence of “oracle” state-action pairs.
Let q(a | s) denote the distilled model’s probabil-
ity of an action a given parser state s; let p(a | s)
be the probability under the ensemble (i.e., the av-
erage of the 20 separately-trained ensemble mem-
bers). To train the distilled model, we minimize
the interpolation between their distillation loss and
the conventional log loss:

argminq α
∑

i

∑

a

−p(a | si) · log q(a | si)
︸ ︷︷ ︸

distillation loss
(1)

+ (1− α)
∑

i

− log q(ai | si)︸ ︷︷ ︸
log loss

Distilling from this parser leads to a single
greedy transition-based parser with 77.8 LAS—
better than past systems but worse than our more
expensive ensemble. The effect of α is illustrated
in Figure 5; generally paying closer attention to
the ensemble, rather than the conventional log loss
objective, leads to better performance.

Learning from exploration. When we set α =
1, we eliminate the oracle from the estimation pro-
cedure (for the distilled model). This presents
an opportunity to learn with exploration, by ran-
domly sampling transitions from the ensemble,
found useful in recent methods for training greedy
models that use dynamic oracles (Goldberg and
Nivre, 2012, 2013; Kiperwasser and Goldberg,

972



●

●

●●

●

●

●

●

●

●

●

77.4
77.6

77.377.3
77

76.7
76.4

76.1
75.8

75.1
74.9

75

76

77

78

1.00.90.80.70.60.50.40.30.20.10.0

alpha

la
s

Figure 5: The effect of α on distillation.

Pipeline stage Score Ours SOTA
Tokenization F1 98.3 97.3
POS tagging F1 93.3 92.2
UD parsing LAS F1 74.0 71.4

Table 7: Evaluating our pipeline against a state-of-the-
art pipeline.

2016; Ballesteros et al., 2016). We find that this
approach outperforms the conventional distillation
model, coming in 1.5 points behind the ensemble
(last line of Table 6).

Pipeline evaluation. Finally, we report our full
pipeline’s performance in Table 7. We also com-
pare our model with a pipeline of the state-of-the-
art systems (labeled “SOTA”): Stanford CoreNLP
tokenizer,14 Owoputi et al.’s (2013) tagger, and
Dozat et al.’s (2017) parser. Our system differs
from the state-of-the-art pipeline in the tokeniza-
tion and parser components. From Table 7, our
pipeline outperforms the state of the art when eval-
uated in pipeline manner. The results also em-
phasize the importance of tokenization: without
gold tokenization UD parsing performance drops
by about four points.

4 Conclusion

We study the problem of parsing tweets into Uni-
versal Dependencies. We adapt the UD guidelines
to cover special constructions in tweets and create
the TWEEBANK V2, which has 55,607 tokens. We
characterize the disagreements among our annota-
tors and argue that inherent ambiguity in this genre
makes consistent annotation a challenge. Using
this new treebank, we build a pipeline system to
parse tweets into UD. We also propose a new
method to distill an ensemble of 20 greedy parsers
into a single one to overcome annotation noise

14We choose the Stanford CoreNLP tokenizer in the spirit
of comparing rule-based and statistical methods.

without sacrificing efficiency. Our parser achieves
an improvement of 2.2 in LAS over a strong base-
line and outperforms other state-of-the-art parsers
in both accuracy and speed.

Acknowledgments

We thank Elizabeth Clark, Lucy Lin, Nelson Liu,
Kelvin Luu, Phoebe Mulcaire, Hao Peng, Maarten
Sap, Chenhao Tan, and Sam Thomson at the Uni-
versity of Washington, and Austin Blodgett, Lucia
Donatelli, Joe Garman, Emma Manning, Angela
Yang, and Yushi Zhang at Georgetown University
for their annotation efforts in the first round. We
are grateful for the support from Lingpeng Kong at
the initial stage of this project. We also thank the
anonymous reviewers for their helpful comments
and suggestions. This work was supported by the
National Key Basic Research Program of China
via grant 2014CB340503 and the National Natu-
ral Science Foundation of China (NSFC) via grant
61632011.

References
Waleed Ammar, George Mulcaire, Miguel Ballesteros,

Chris Dyer, and Noah A. Smith. 2016. Many lan-
guages, one parser. TACL 4.

Miguel Ballesteros, Chris Dyer, and Noah A. Smith.
2015. Improved transition-based parsing by mod-
eling characters instead of words with LSTMs. In
Proc. of EMNLP.

Miguel Ballesteros, Yoav Goldberg, Chris Dyer, and
Noah A. Smith. 2016. Training with exploration
improves a greedy stack LSTM parser. In Proc. of
EMNLP.

Peter F. Brown, Peter V. deSouza, Robert L. Mer-
cer, Vincent J. Della Pietra, and Jenifer C. Lai.
1992. Class-based n-gram models of natural lan-
guage. Computational Linguistics 18(4).

Marie-Catherine de Marneffe, Timothy Dozat, Na-
talia Silveira, Katri Haverinen, Filip Ginter, Joakim
Nivre, and Christopher D. Manning. 2014. Univer-
sal Stanford Dependencies: A cross-linguistic typol-
ogy. In LREC.

Leon Derczynski, Alan Ritter, Sam Clark, and Kalina
Bontcheva. 2013. Twitter part-of-speech tagging for
all: Overcoming sparse and noisy data. In Proc. of
RANLP 2013.

Thomas G. Dietterich. 2000. An experimental com-
parison of three methods for constructing ensem-
bles of decision trees: Bagging, boosting, and
randomization. Machine Learning 40(2):139–157.
https://doi.org/10.1023/A:1007607513941.

973



Timothy Dozat, Peng Qi, and Christopher D. Manning.
2017. Stanford’s graph-based neural dependency
parser at the CoNLL 2017 shared task. In Proc.
of CoNLL 2017 Shared Task: Multilingual Parsing
from Raw Text to Universal Dependencies.

Jacob Eisenstein. 2013. What to do about bad language
on the internet. In Proc. of NAACL.

Jennifer Foster, Özlem Çetinoǧlu, Joachim Wagner,
Joseph Le Roux, Stephen Hogan, Joakim Nivre,
Deirdre Hogan, and Josef Van Genabith. 2011.
#hardtoparse: POS tagging and parsing the Twitter-
verse. In Proc. of the 5th AAAI Conference on Ana-
lyzing Microtext.

Benoît Frénay and Michel Verleysen. 2014. Classifica-
tion in the presence of label noise: A survey. IEEE
Transactions on Neural Networks and Learning Sys-
tems 25:845–869.

Kim Gerdes. 2013. Collaborative dependency annota-
tion. In Proc. of DepLing.

Kevin Gimpel, Nathan Schneider, Brendan O’Connor,
Dipanjan Das, Daniel Mills, Jacob Eisenstein,
Michael Heilman, Dani Yogatama, Jeffrey Flanigan,
and Noah A. Smith. 2011. Part-of-speech tagging
for twitter: Annotation, features, and experiments.
In Proc. of ACL.

Yoav Goldberg and Joakim Nivre. 2012. A dynamic
oracle for arc-eager dependency parsing. In Proc. of
COLING.

Yoav Goldberg and Joakim Nivre. 2013. Training de-
terministic parsers with non-deterministic oracles.
TACL 1.

Tao Gui, Qi Zhang, Haoran Huang, Minlong Peng, and
Xuanjing Huang. 2017. Part-of-speech tagging for
Twitter with adversarial neural networks. In Proc.
of EMNLP.

Jiang Guo, Wanxiang Che, David Yarowsky, Haifeng
Wang, and Ting Liu. 2015. Cross-lingual depen-
dency parsing based on distributed representations.
In Proc. of ACL.

Geoffrey E. Hinton, Oriol Vinyals, and Jeffrey Dean.
2015. Distilling the knowledge in a neural network.
CoRR abs/1503.02531.

Zhiheng Huang, Wei Xu, and Kai Yu. 2015. Bidi-
rectional LSTM-CRF models for sequence tagging.
CoRR abs/1508.01991.

Yoon Kim and Alexander M. Rush. 2016. Sequence-
level knowledge distillation. In Proc. of EMNLP.

Eliyahu Kiperwasser and Yoav Goldberg. 2016. Sim-
ple and accurate dependency parsing using bidirec-
tional LSTM feature representations. TACL 4.

Lingpeng Kong, Nathan Schneider, Swabha
Swayamdipta, Archna Bhatia, Chris Dyer, and
Noah A. Smith. 2014. A dependency parser for
tweets. In Proc. of EMNLP.

Adhiguna Kuncoro, Miguel Ballesteros, Lingpeng
Kong, Chris Dyer, and Noah A. Smith. 2016. Dis-
tilling an ensemble of greedy dependency parsers
into one MST parser. In Proc. of EMNLP.

Xuezhe Ma and Eduard Hovy. 2016. End-to-end
sequence labeling via bi-directional LSTM-CNNs-
CRF. In Proc. of ACL.

Christopher D. Manning, Mihai Surdeanu, John Bauer,
Jenny Finkel, Steven J. Bethard, and David Mc-
Closky. 2014. The Stanford CoreNLP natural lan-
guage processing toolkit. In ACL System Demon-
strations.

Mitchell P. Marcus, Beatrice Santorini, and Mary Ann
Marcinkiewicz. 1993. Building a large annotated
corpus of English: The Penn Treebank. Computa-
tional Linguistics 19(2):313–330.

Joakim Nivre, Marie-Catherine de Marneffe, Filip Gin-
ter, Yoav Goldberg, Jan Hajic, Christopher D. Man-
ning, Ryan McDonald, Slav Petrov, Sampo Pyysalo,
Natalia Silveira, Reut Tsarfaty, and Daniel Zeman.
2016. Universal Dependencies v1: A multilingual
treebank collection. In Proc. of LREC.

Brendan O’Connor, Michel Krieger, and David Ahn.
2010. TweetMotif: Exploratory search and topic
summarization for Twitter.

Olutobi Owoputi, Brendan O’Connor, Chris Dyer,
Kevin Gimpel, Nathan Schneider, and Noah A.
Smith. 2013. Improved part-of-speech tagging for
online conversational text with word clusters. In
Proc. of NAACL.

Jeffrey Pennington, Richard Socher, and Christopher
Manning. 2014. GloVe: Global vectors for word
representation. In Proc. of EMNLP.

Slav Petrov, Dipanjan Das, and Ryan McDonald. 2012.
A universal part-of-speech tagset. In Proc. of LREC.

Nils Reimers and Iryna Gurevych. 2017. Reporting
score distributions makes a difference: Performance
study of LSTM-networks for sequence tagging. In
Proc. of EMNLP.

Alan Ritter, Sam Clark, Mausam, and Oren Etzioni.
2011. Named entity recognition in tweets: An ex-
perimental study. In Proc. of EMNLP.

Manuela Sanguinetti, Cristina Bosco, Alessandro
Mazzei, Alberto Lavelli, and Fabio Tamburini. 2017.
Annotating Italian social media texts in Universal
Dependencies. In Proc. of Depling. Pisa, Italy.

Nathan Schneider, Brendan O’Connor, Naomi Saphra,
David Bamman, Manaal Faruqui, Noah A. Smith,
Chris Dyer, and Jason Baldridge. 2013. A frame-
work for (under)specifying dependency syntax with-
out overloading annotators. In Proc. of the 7th Lin-
guistic Annotation Workshop and Interoperability
with Discourse.

974



Milan Straka and Jana Straková. 2017. Tokenizing,
POS tagging, lemmatizing and parsing UD 2.0 with
UDPipe. In Proc. of the CoNLL 2017 Shared Task:
Multilingual Parsing from Raw Text to Universal
Dependencies.

Hongmin Wang, Yue Zhang, GuangYong Leonard
Chan, Jie Yang, and Hai Leong Chieu. 2017. Uni-
versal Dependencies parsing for colloquial Singa-
porean English. In Proc. of ACL.

Hiroyasu Yamada and Yuji Matsumoto. 2003. Statis-
tical dependency analysis with support vector ma-
chines. In Proc. of IWPT .

975


