



















































Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics


Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics, pages 1029–1039
Vancouver, Canada, July 30 - August 4, 2017. c©2017 Association for Computational Linguistics

https://doi.org/10.18653/v1/P17-1095

Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics, pages 1029–1039
Vancouver, Canada, July 30 - August 4, 2017. c©2017 Association for Computational Linguistics

https://doi.org/10.18653/v1/P17-1095

Bayesian Modeling of Lexical Resources for Low-Resource Settings

Nicholas Andrews and Mark Dredze and Benjamin Van Durme and Jason Eisner
Department of Computer Science and Human Language Technology Center of Excellence

Johns Hopkins University
3400 N. Charles St., Baltimore, MD 21218 USA

{noa,eisner,mdredze,vandurme}@jhu.edu

Abstract

Lexical resources such as dictionaries and
gazetteers are often used as auxiliary data
for tasks such as part-of-speech induction
and named-entity recognition. However,
discriminative training with lexical features
requires annotated data to reliably estimate
the lexical feature weights and may result
in overfitting the lexical features at the ex-
pense of features which generalize better.
In this paper, we investigate a more robust
approach: we stipulate that the lexicon is
the result of an assumed generative process.
Practically, this means that we may treat
the lexical resources as observations under
the proposed generative model. The lexi-
cal resources provide training data for the
generative model without requiring sepa-
rate data to estimate lexical feature weights.
We evaluate the proposed approach in two
settings: part-of-speech induction and low-
resource named-entity recognition.

1 Introduction

Dictionaries and gazetteers are useful in many
natural language processing tasks. These lexical
resources may be derived from freely available
sources (such as Wikidata and Wiktionary) or con-
structed for a particular domain. Lexical resources
are typically used to complement existing anno-
tations for a given task (Ando and Zhang, 2005;
Collobert et al., 2011). In this paper, we focus
instead on low-resource settings where task annota-
tions are unavailable or scarce. Specifically, we use
lexical resources to guide part-of-speech induction
(§4) and to bootstrap named-entity recognizers in
low-resource languages (§5).

Given their success, it is perhaps surprising that
incorporating gazetteers or dictionaries into dis-

criminative models (e.g. conditional random fields)
may sometimes hurt performance. This phenom-
ena is called weight under-training, in which lexi-
cal features—which detect whether a name is listed
in the dictionary or gazetteer—are given exces-
sive weight at the expense of other useful features
such as spelling features that would generalize to
unlisted names (Smith et al., 2005; Sutton et al.,
2006; Smith and Osborne, 2006). Furthermore, dis-
criminative training with lexical features requires
sufficient annotated training data, which poses chal-
lenges for the unsupervised and low-resource set-
tings we consider here.

Our observation is that Bayesian modeling pro-
vides a principled solution. The lexicon is itself a
dataset that was generated by some process. Prac-
tically, this means that lexicon entries (words or
phrases) may be treated as additional observations.
As a result, these entries provide information about
how names are spelled. The presence of the lexi-
con therefore now improves training of the spelling
features, rather than competing with the spelling
features to help explain the labeled corpus.

A downside is that generative models are typi-
cally less feature-rich than their globally normal-
ized discriminative counterparts (e.g. conditional
random fields). In designing our approach—the
hierarchical sequence memoizer (HSM)—we aim
to be reasonably expressive while retaining prac-
tically useful inference algorithms. We propose a
Bayesian nonparametric model to serve as a gener-
ative distribution responsible for both lexicon and
corpus data. The proposed model memoizes previ-
ously used lexical entries (words or phrases) but
backs off to a character-level distribution when
generating novel types (Teh, 2006; Mochihashi
et al., 2009). We propose an efficient inference
algorithm for the proposed model using particle
Gibbs sampling (§3). Our code is available at
https://github.com/noa/bayesner.

1029

https://doi.org/10.18653/v1/P17-1095
https://doi.org/10.18653/v1/P17-1095


2 Model

Our goal is to fit a model that can automatically
annotate text. We observe a supervised or unsu-
pervised training corpus. For each label y in the
annotation scheme, we also observe a lexicon of
strings of type y. For example, in our tagging task
(§4), a dictionary provides us with a list of words
for each part-of-speech tag y. (These lists need not
be disjoint.) For named-entity recognition (NER,
§5), we use a list of words or phrases for each
named-entity type y (PER, LOC, ORG, etc.).1

2.1 Modeling the lexicon

We may treat the lexicon for type y, of size my, as
having been produced by a set of my IID draws
from an unknown distribution Py over the words
or named entities of type y. It therefore provides
some evidence about Py. We will later assume that
Py is also used when generating mentions of these
words or entities in text. Thanks to this sharing of
Py, if x = Washington is listed in the gazetteer
of locations (y = LOC), we can draw the same con-
clusions as if we had seen a LOC-labeled instance
of Washington in a supervised corpus.

Generalizing this a bit, we may suppose that one
observation of string x in the lexicon is equivalent
to c labeled tokens of x in a corpus, where the
constant c > 0 is known as a pseudocount. In
other words, observing a lexicon of my distinct
types {x1, . . . , xmy} is equivalent to observing a
labeled pseudocorpus of cmy tokens. Notice that
given such an observation, the prior probability of
any candidate distribution Py is reweighted by the
likelihood (cmy)!(c!)my · (Py(x1)Py(x2) · · ·Py(xmy))c.
Therefore, this choice of Py can have relatively
high posterior probability only to the extent that it
assigns high probability to all of the lexicon types.

2.2 Discussion

We employ the above model because it has rea-
sonable qualitative behavior and because computa-
tionally, it allows us to condition on observed lexi-
cons as easily as we condition on observed corpora.
However, we caution that as a generative model
of the lexicon, it is deficient, in the sense that it

1Dictionaries and knowledge bases provide more infor-
mation than we use in this paper. For instance, Wikidata
also provides a wealth of attributes and other metadata for
each entity s. In principle, this additional information could
also be helpful in estimating Py(s); we leave this intriguing
possibility for future work.

allocates probability mass to events that cannot ac-
tually correspond to any lexicon. After all, drawing
cmy IID tokens from Py is highly unlikely to result
in exactly c tokens of each of my different types,
and yet a run of our system will always assume
that precisely this happened to produce each ob-
served lexicon! To avoid the deficiency, one could
assume that the lexicon was generated by rejection
sampling: that is, the gazetteer author repeatedly
drew samples of size cmy from Py until one was
obtained that had this property, and then returned
the set of distinct types in that sample as the lexi-
con for y. But this is hardly a realistic description
of how gazetteers are actually constructed. Rather,
one imagines that the gazetteer author simply har-
vested a lexicon of frequent types from Py or from
a corpus of tokens generated from Py. For example,
a much better generative story is that the lexicon
was constructed as the first my distinct types to
appear ≥ c times in an unbounded sequence of IID
draws from Py. When c = 1, this is equivalent
to modeling the lexicon as my draws without re-
placement from Py.2 Unfortunately, draws without
replacement are no longer IID or exchangeable: or-
der matters. It would therefore become difficult to
condition inference and learning on an observed
lexicon, because we would need to explicitly sum
or sample over the possibilities for the latent se-
quence of tokens (or stick segments). We therefore
adopt the simpler deficient model.

A version of our lexicon model (with c = 1)
was previously used by Dreyer and Eisner (2011,
Appendix C), who observed a list of verb paradigm
types rather than word or entity-name types.

2.3 Prior distribution over Py
We assume a priori that Py was drawn from a
Pitman-Yor process (PYP) (Pitman and Yor, 1997).
Both the lexicon and the ordinary corpus are ob-
servations that provide information about Py. The
PYP is defined by three parameters: a concentra-
tion parameter α, a discount parameter d, and a
base distribution Hy. In our case, Hy is a distribu-
tion over X = Σ∗, the set of possible strings over
a finite character alphabet Σ.

For example, HLOC is used to choose new place
names, so it describes what place names tend to

2If we assume that Py was drawn from a Pitman-Yor pro-
cess prior (as in §2.3) using the stick-breaking method (Pitman,
1996), it is also equivalent to modeling the lexicon as the set
of labels of the first my stick segments (which tend to have
high probability).

1030



look like in the language. The draw PLOC ∼
PYP(d, α,HLOC) is an “adapted” version of HLOC.
It is PLOC that determines how often each name is
mentioned in text (and whether it is mentioned in
the lexicon). Some names such as Washington
that are merely plausible under HLOC are far more
frequent under PLOC, presumably because they
were chosen as the names of actual, significant
places. These place names were randomly drawn
from HLOC as part of the procedure for drawing Py.

The expected value of Py is H (i.e., H is the
mean of the PYP distribution), but if α and d are
small, then a typical draw of Py will be rather dif-
ferent from H , with much of the probability mass
falling on a subset of the strings.

At training or test time, when deciding whether
to label a corpus token of x = Washington as
a place or person, we will be interested in the rel-
ative values of PLOC(x) and PPER(x). In practice,
we do not have to represent the unknown infinite
object Py, but can integrate over its possible values.
When Py ∼ PYP(d, α,Hy), then a sequence of
draws X1, X2, . . . ∼ Py is distributed according to
a Chinese restaurant process, via

Py(Xi+1 = x | X1, . . . , Xi) (1)

=
customers(x)− d · tables(x)

α+ i

+
α+ d ·∑x′ tables(x′)

α+ i
Hy(x)

where customers(x) ≤ i is the number of
times that x appeared among X1, . . . , Xi, and
tables(x) ≤ customers(x) is the number of those
times that x was drawn from Hy (where each
Py(Xi | · · · ) defined by (1) is interpreted as a
mixture distribution that sometimes uses Hy).

2.4 Form of the base distribution Hy

By fitting Hy on corpus and lexicon data, we learn
what place names or noun strings tend to look like
in the language. By simultaneously fitting Py, we
learn which ones are commonly mentioned. Recall
that under our model, tokens are drawn from Py but
the underlying types are drawn fromHy, e.g.,Hy is
responsible for (at least) the first token of each type.

A simple choice for Hy is a Markov process
that emits characters in Σ ∪ {$}, where $ is a dis-
tinguished stop symbol that indicates the end of
the string. Thus, the probability of producing $
controls the typical string length under Hy.

We use a more sophisticated model of strings—a
sequence memoizer (SM), which is a (hierarchi-
cal) Bayesian treatment of variable-order Markov
modeling (Wood et al., 2009). The SM allows
dependence on an unbounded history, and the prob-
ability of a given sequence (string) can be found
efficiently much as in equation (1).

Given a string x = a1 · · · aJ ∈ Σ∗, the SM
assigns a probability to it via

Hy(a1:J) =
( J∏

j=1

Hy(aj | a1:j−1)
)
Hy($ | a1:J)

=
( J∏

j=1

Hy,a1:j−1(aj)
)
Hy,a1:J ($) (2)

where Hy,u(a) denotes the conditional probability
of character a given the left context u ∈ Σ∗. Each
Hy,u is a distribution over Σ, defined recursively
as

Hy,� ∼ PYP(d�, α�,UΣ) (3)
Hy,u ∼ PYP(d|u|, α|u|, Hy,σ(u))

where � is the empty sequence, UΣ is the uni-
form distribution over Σ ∪ {$}, and σ(u) drops
the first symbol from u. The discount and concen-
tration parameters (d|u|, α|u|) are associated with
the lengths of the contexts |u|, and should gener-
ally be larger for longer (more specific) contexts,
implying stronger backoff from those contexts.3

Our inference procedure is largely indifferent to
the form of Hy, so the SM is not the only option.
It would be possible to inject more assumptions
into Hy, for instance via structured priors for mor-
phology or a grammar of name structure. Another
possibility is to use a parametric model such as
a neural language model (e.g., Jozefowicz et al.
(2016)), although this would require an inner-loop
of gradient optimization.

2.5 Modeling the sequence of tags y

We now turn to modeling the corpus. We assume
that each sentence is generated via a sequence of
latent labels y = y1:T ∈ Y∗.4 The observations

3We fix these hyperparameters using the values suggested
in (Wood et al., 2009; Gasthaus and Teh, 2010), which we find
to be quite robust in practice. One could also resample their
values (Blunsom and Cohn, 2010); we experimented with this
but did not observe any consistent advantage to doing so in
our setting.

4The label sequence is terminated by a distinguished end-
of-sequence label, again written as $.

1031



x1:T are then generated conditioned on the label
sequence via the corresponding Py distribution (de-
fined in §2.3). All observations with the same label
y are drawn from the same Py, and thus this subse-
quence of observations is distributed according to
the Chinese restaurant process (1).

We model y using another sequence memo-
izer model. This is similar to other hierarchical
Bayesian models of latent sequences (Goldwater
and Griffiths, 2007; Blunsom and Cohn, 2010), but
again, it does not limit the Markov order (the num-
ber of preceding labels that are conditioned on).
Thus, the probability of a sequence of latent types
is computed in the same way as the base distribu-
tion in §2.4, that is,

p(y1:T ) :=
( T∏

t=1

Gy1:t−1(yt)
)
Gy1:T ($) (4)

where Gv(y) denotes the conditional probability
of latent label y ∈ Y given the left context v ∈ Y∗.
Each Gv is a distribution over Y , defined recur-
sively as

G� ∼ PYP(d�, α�,UY) (5)
Gv ∼ PYP(d|v|, α|v|, Gσ(v))

The probability of transitioning to label yt de-
pends on the assignments of all previous labels
y1 . . . yt−1.

For part-of-speech induction, each label yt is the
part-of-speech associated with the corresponding
word xt. For named-entity recognition, we say that
each word token is labeled with a named entity
type (LOC, PER, . . . ),5 or with itself if it is not
a named entity but rather a “context word.” For
example, the word token xt = Washington
could have been emitted from the label yt = LOC,
or from yt = PER, or from yt = Washington
itself (in which case p(xt | yt) = 1). This uses a
much larger set of labels Y than in the traditional
setup where all context words are emitted from the
same latent label type O. Of course, most labels
are impossible at most positions (e.g., yt cannot be
Washington unless xt = Washington). This
scheme makes our generative model sensitive to
specific contexts (which is accomplished in dis-
criminative NER systems by contextual features).
For example, the SM for y can learn that spoke
to PER yesterday is a common 4-gram

5In §3.2, we will generalize this labeling scheme to allow
multi-word named entities such as New York.

in the label sequence y, and thus we are more
likely to label Washington as a person if x =
. . .spoke to Washington yesterday . . ..

We need one change to make this work, since
now Y must include not only the standard NER
labels Y ′ = {PER, LOC, ORG, GPE} but also words
like Washington. Indeed, now Y = Y ′ ∪ Σ∗.
But no uniform distribution exists over the infinite
set Σ∗, so how should we replace the base distribu-
tion UY over labels in equation (5)? Answer: To
draw from the new base distribution, sample y ∼
UY ′ ∪{CONTEXT}. If y = CONTEXT, however, then
“expand” it by resampling y ∼ HCONTEXT. Here
HCONTEXT is the base distribution over spellings of
context words, and is learned just like the other Hy
distributions in §2.4.

3 Inference via particle Markov chain
Monte Carlo

3.1 Sequential sampler

Taking Y to be a random variable, we are interested
in the posterior distribution p(Y = y | x) over la-
bel sequences y given the emitted word sequence
x. Our model does not admit an efficient dynamic
programming algorithm, owing to the dependen-
cies introduced among the Yt when we marginalize
over the unknown G and P distributions that gov-
ern transitions and emissions, respectively. In con-
trast to tagging with a hidden Markov model tag-
ging, the distribution of each label Yt depends on
all previous labels y1:t−1, for two reasons: ¬ The
transition distribution p(Yt = y | y1:t−1) has un-
bounded dependence because of the PYP prior (4).
­ The emission distribution p(xt | Yt = y) de-
pends on the emissions observed from any earlier
tokens of y, because of the Chinese restaurant pro-
cess (1). When ­ is the only complication, block
Metropolis-Hastings samplers have proven effec-
tive (Johnson et al., 2007). However, this approach
uses dynamic programming to sample from a pro-
posal distribution efficiently, which ¬ precludes in
our case. Instead, we use sequential Monte Carlo
(SMC)—sometimes called particle filtering—as a
proposal distribution. Particle filtering is typically
used in online settings, including word segmenta-
tion (Borschinger and Johnson, 2011), to make de-
cisions before all of x has been observed. However,
we are interested in the inference (or smoothing)
problem that conditions on all of x (Dubbin and
Blunsom, 2012; Tripuraneni et al., 2015).

SMC employs a proposal distribution q(y | x)

1032



whose definition decomposes as follows:

q(y1 | x1)
T∏

t=2

q(yt | y1:t−1,x1:t) (6)

for T = |x|. To sample a sequence of latent
labels, first sample an initial label y1 from q1,
then proceed incrementally by sampling yt from
qt(· | y1:t−1,x1:t) for t = 2, . . . , T . The fi-
nal sampled sequence y is called a particle, and
is given an unnormalized importance weight of
w̃ = w̃T · p($ | y1:T ) where w̃T was built up via

w̃t := w̃t−1 ·
p(y1:t,x1:t)

p(y1:t−1,x1:t−1) q(yt | y1:t−1,x1:t)
(7)

The SMC procedure consists of generating a sys-
tem of M weighted particles whose unnormalized
importance weights w̃(m) : 1 ≤ m ≤ M are
normalized into w(m) := w̃(m)/

∑M
m=1 w̃

(m). As
M → ∞, SMC provides a consistent estimate of
the marginal likelihood p(x) as 1M

∑M
m=1 w̃

(m),
and samples from the weighted particle system are
distributed as samples from the desired posterior
p(y | x) (Doucet and Johansen, 2009).
Particle Gibbs. We employ SMC as a kernel in
an MCMC sampler (Andrieu et al., 2010). In par-
ticular, we use a block Gibbs sampler in which
we iteratively resample the hidden labeling y of
a sentence x conditioned on the current labelings
for all other sentences in the corpus. In this con-
text, the algorithm is called conditional SMC since
one particle is always fixed to the previous sam-
pler state for the sentence being resampled, which
ensures that the MCMC procedure is ergodic. At
a high level, this procedure is analogous to other
Gibbs samplers (e.g. for topic models), except that
the conditional SMC (CSMC) kernel uses auxiliary
variables (particles) in order to generate the new
block variable assignments. The procedure is out-
lined in Algorithm 1. Given a previous latent state
assignment y′1:T and observations x1:T , the CSMC
kernel produces a new latent state assignment via
M auxiliary particles where one particle is fixed to
the previous assignment. For ergodicity, M ≥ 2,
where larger values of M may improve mixing rate
at the expense of increased computation per step.

Proposal distribution. The choice of proposal dis-
tribution q is crucial to the performance of SMC
methods. In the case of continuous latent variables,

it is common to propose yt from the transition prob-
ability p(Yt | y1:t−1) because this distribution usu-
ally has a simple form that permits efficient sam-
pling. However, it is possible to do better in the
case of discrete latent variables. The optimal pro-
posal distribution is the one which minimizes the
variance of the importance weights, and is given by

q(yt | y1:t−1,x1:t) := p(yt | y1:t−1,x1:t) (8)

=
p(yt | y1:t−1)p(xt | yt)

p(xt | y1:t−1)

where

p(xt | y1:t−1)=
∑

yt∈Y
p(yt | y1:t−1)p(xt | yt) (9)

Substituting this expression in equation (7) and
simplifying yields the incremental weight update:

w̃t := w̃t−1 · p(xt | y1:t−1) (10)

Resampling. In filtering applications, it is com-
mon to use resampling operations to prevent weight
degeneracy. We do not find resampling necessary
here for three reasons. First, note that we resam-
ple hidden label sequences that are only as long as
the number of words in a given sentence. Second,
we use a proposal which minimizes the variance
of the weights. Finally, we use SMC as a kernel
embedded in an MCMC sampler; asymptotically,
this procedure yields samples from the desired pos-
terior regardless of degeneracy (which only affects
the mixing rate). Practically speaking, one can di-
agnose the need for resampling via the effective
sample size (ESS) of the particle system:

ESS :=
1

∑M
m=1(w̃

(m))2
=

(
∑M

m=1w
(m))2

∑M
m=1(w

(m))2

In our experiments, we find that ESS remains high
(a significant fraction of M ) even for long sen-
tences, suggesting that resampling is not necessary
to enable mixing of the the Gibbs sampler.

Decoding. In order to obtain a single latent vari-
able assignment for evaluation purposes, we simply
take the state of the Markov chain after a fixed num-
ber of iterations of particle Gibbs. In principle, one
could collect many samples during particle Gibbs
and use them to perform minimum Bayes risk de-
coding under a given loss function. However, this
approach is somewhat slower and did not appear to
improve performance in preliminary experiments

1033



Algorithm 1 Conditional SMC
1: procedure CSMC(x1:T , y′1:T , M )
2: Draw y(m)1 (eqn. 8) for m ∈ [1,M − 1]
3: Set y(M)1 = y

′
1

4: Set w̃(m)1 (eqn. 10) for m ∈ [1,M ]
5: for t = 2 to T do
6: Draw y(m)t (eqn. 8) for m ∈ [1,M −1]
7: Set yMt = y

′
t

8: Set w̃(m)t (eqn. 10) for m ∈ [1,M ]
9: Set w̃(m) = w̃(m)T p($|y1:T ) for m ∈ [1,M ]

10: Draw index k where p(k = m) ∝ w̃(m)
11: return y(k)1:T

3.2 Segmental sampler

We now present an sampler for settings such as
NER where each latent label emits a segment con-
sisting of 1 or more words. We make use of the
same transition distribution p(yt | y1:t−1), which
determines the probability of a label in a given
context, and an emission distribution p(xt | yt)
(namely Pyt); these are assumed to be drawn
from hierarchical Pitman-Yor processes described
in §2.5 and §2.1, respectively. To allow the xt to be
a multi-word string, we simply augment the charac-
ter set with a distinguished space symbol ∈ Σ that
separates words within a string. For instance, New
York would be generated as the 9-symbol sequence
New York$.

Although the model emits New York all at
once, we still formulate our inference procedure as
a particle filter that proposes one tag for each word.
Thus, for a given segment label type y, we allow
two tag types for its words:

• I-y corresponds to a non-final word in a seg-
ment of type y (in effect, a word with a fol-
lowing attached).
• E-y corresponds to the final word in a segment

of type y.

For instance, x1:2 = New York would be anno-
tated as a location segment by defining y1:2 =
I-LOC E-LOC. This says that y1:2 has jointly
emitted x1:2, an event with prior probability
PLOC(New York). Each word that is not part
of a named entity is considered to be a single-
word segment. For example, if the next word
were x3 = hosted then it should be tagged with
y3 = hosted as in §2.5, in which case x3 was
emitted with probability 1.

To adapt the sampler described in §3.1 for the
segmental case, we need only to define the transi-
tion and emission probabilities used in equation (8)
and its denominator (9).

For the transition probabilities, we want to model
the sequence of segment labels. If yt−1 is an I- tag,
we take p(yt | y1:t−1) = 1 , since then yt merely
continues an existing segment. Otherwise yt starts
a new segment, and we take p(yt | y1:t−1) = 1 to
be defined by the PYP’s probability Gy1:t−1(yt) as
usual, but where we interpret the subscript y1:t−1
to refer to the possibly shorter sequence of segment
labels implied by those t− 1 tags.

For the emission probabilities, if yt has the form
I-y or E-y, then its associated emission probabil-
ity no longer has the form p(xt | yt), since the
choice of xt also depends on any words emitted
earlier in the segment. Let s ≤ t be the starting
position of the segment that contains t. If yt = E-y,
then the emission probability is proportional to
Py(xs xs+1 . . . xt). If yt = I-y then the emis-
sion probability is proportional to the prefix prob-
ability

∑
x Py(x) where x ranges over all strings

in Σ∗ that have xs xs+1 . . . xt as a proper pre-
fix. Prefix probabilities in Hy are easy to compute
because Hy has the form of a language model, and
prefix probabilities in Py are therefore also easy to
compute (using a prefix tree for efficiency).

This concludes the description of the segmental
sampler. Note that the particle Gibbs procedure is
unchanged.

4 Inducing parts-of-speech with
type-level supervision

Automatically inducing parts-of-speech from raw
text is a challenging problem (Goldwater et al.,
2005). Our focus here is on the easier problem
of type-supervised part-of-speech induction, in
which (partial) dictionaries are used to guide in-
ference (Garrette and Baldridge, 2012; Li et al.,
2012). Conditioned on the unlabeled corpus and
dictionary, we use the MCMC procedure described
in §3.1 to impute the latent parts-of-speech.

Since dictionaries are freely available for hun-
dreds of languages,6 we see this as a mild addi-
tional requirement in practice over the purely unsu-
pervised setting.

In prior work, dictionaries have been used as con-
straints on possible parts-of-speech: words appear-
ing in the dictionary take one of their known parts-

6https://www.wiktionary.org/

1034



of-speech. In our setting, however, the dictionar-
ies are not constraints but evidence. If monthly
is listed in (only) the adjective lexicon, this tells
us that PADJ sometimes generates monthly and
therefore that HADJ may also tend to generate
other words that end with -ly. However, for us,
PADV(monthly) > 0 as well, allowing us to still
correctly treat monthly as a possible adverb if we
later encounter it in a training or test corpus.

4.1 Experiments

We follow the experimental procedure described
in Li et al. (2012), and use their released code and
data to compare to their best model: a second-order
maximum entropy Markov model parametrized
with log-linear features (SHMM-ME). This model
uses hand-crafted features designed to distinguish
between different parts-of-speech, and it has spe-
cial handling for rare words. This approach is sur-
prisingly effective and outperforms alternate ap-
proaches such as cross-lingual transfer (Das and
Petrov, 2011). However, it also has limitations,
since words that do not appear in the dictionary
will be unconstrained, and spurious or incorrect
lexical entries may lead to propagation of errors.

The lexicons are taken from the Wiktionary
project; their size and coverage are documented
by (Li et al., 2012). We evaluate our model on
multi-lingual data released as part of the CoNLL
2007 and CoNLL-X shared tasks. In particular, we
use the same set of languages as Li et al. (2012).7

For our method, we impute the parts-of-speech by
running particle Gibbs for 100 epochs, where one
epoch consists of resampling the states for a each
sentence in the corpus. The final sampler state is
then taken as a 1-best tagging of the unlabeled data.

Results. The results are reported in Table 1.
We find that our hierarchical sequence memoizer
(HSM) matches or exceeds the performance of the
baseline (SHMM-ME) for nearly all the tested lan-
guages, particularly for morphologically rich lan-
guages such as German where the spelling distribu-
tions Hy may capture regularities. It is interesting
to note that our model performs worse relative to
the baseline for English; one possible explanation
is that the baseline uses hand-engineered features
whereas ours does not, and these features may have
been tuned using English data for validation.

7With the exception of Dutch. Unlike the other CoNLL lan-
guages, Dutch includes phrases, and the procedure by which
these were split into tokens was not fully documented.

Our generative model is supposed to exploit lex-
icons well. To see what is lost from using a genera-
tive model, we also compared with Li et al. (2012)
on standard supervised tagging without any lexi-
cons. Even here our generative model is very com-
petive, losing only on English and Swedish.

5 Boostrapping NER with type-level
supervision

Name lists and dictionaries are useful for NER
particularly when in-domain annotations are scarce.
However, with little annotated data, discriminative
training may be unable to reliably estimate lexical
feature weights and may overfit. In this section, we
are interested in evaluating our proposed Bayesian
model in the context of low-resource NER.

5.1 Data

Most languages do not have corpora annotated for
parts-of-speech, named-entities, syntactic parses,
or other linguistic annotations. Therefore, rapidly
deploying natural language technologies in a new
language may be challenging. In the context of
facilitating relief responses in emergencies such as
natural disasters, the DARPA LORELEI (Low Re-
source Languages for Emergent Incidents) program
has sponsored the development and release of repre-
sentative “language packs” for Turkish and Uzbek
with more languages planned (Strassel and Tracey,
2016). We use the named-entity annotations as part
of these language packs which include persons, lo-
cations, organizations, and geo-political entities, in
order to explore bootstrapping named-entity recog-
nition from small amounts of data. We consider
two types of data: ¬ in-context annotations, where
sentences are fully annotated for named-entities,
and ­ lexical resources.

The LORELEI language packs lack adequate in-
domain lexical resources for our purposes. There-
fore, we simulate in-domain lexical resources
by holding out portions of the annotated de-
velopment data and deriving dictionaries and
name lists from them. For each label y ∈
{PER, LOC, ORG, GPE, CONTEXT}, our lexicon
for y lists all distinct y-labeled strings that appear
in the held-out data. This setup ensures that the
labels associated with lexicon entries correspond
to the annotation guidelines used in the data we
use for evaluation. It avoids possible problems that
might arise when leveraging noisy out-of-domain
knowledge bases, which we may explore in future.

1035



Model Danish German Greek English Italian Portuguese Spanish Swedish Mean

Wiktionary
SHMM-ME 83.3 85.8 79.2 87.1 86.5 84.5 86.4 86.1 84.9
HSM 83.7 90.7 81.7 84.0 86.7 85.5 87.6 86.8 85.8

Supervised
SHMM-ME 93.9 97.4 95.1 95.8 93.8 95.5 93.8 95.5 95.1
HSM 95.2 97.4 97.4 95.2 94.5 96.0 95.6 92.2 95.3

Table 1: Part-of-speech induction results in multiple languages.

5.2 Evaluation
In this section we report supervised NER experi-
ments on two low-resource languages: Turkish and
Uzbek. We vary both the amount of supervision
as well as the size of the lexical resources. A chal-
lenge when evaluating the performance of a model
with small amounts of training data is that there
may be high-variance in the results. In order to
have more confidence in our results, we perform
bootstrap resampling experiments in which the
training set, evaluation set, and lexical resources
are randomized across several replications of the
same experiment (for each of the data conditions).
We use 10 replications for each of the data condi-
tions reported in Figures 1–2, and report both the
mean performance and 95% confidence intervals.

Baseline. We use the Stanford NER system
with a standard set of language-independent fea-
tures (Finkel et al., 2005).8. This model is a condi-
tional random field (CRF) with feature templates
which include character n-grams as well as word
shape features. Crucially, we also incorporate lexi-
cal features. The CRF parameters are regularized
using an L1 penalty and optimized via Orthant-wise
limited-memory quasi-Newton optimization (An-
drew and Gao, 2007). For both our proposed
method and the discriminative baseline, we use
a fixed set of hyperparameters (i.e. we do not use
a separate validation set for tuning each data con-
dition). In order to make a fair comparison to the
CRF, we use our sampler for forward inference
only, without resampling on the test data.

Results. We show learning curves as a function
of supervised training corpus size. Figure 1 shows
that our generative model strongly beats the base-
line in this low-data regime. In particular, when
there is little annotated training data, our proposed
generative model can compensate by exploiting the
lexicon, while the discriminative baseline scores
terribly. The performance gap decreases with larger

8We also experimented with neural models, but found that
the CRF outperformed them in low-data conditions.

supervised corpora, which is consistent with prior
results comparing generative and discriminative
training (Ng and Jordan, 2002).

In Figure 2, we show the effect of the lexi-
con’s size: as expected, larger lexicons are better.
The generative approach significantly outperforms
the discriminative baseline at any lexicon size, al-
though its advantage drops for smaller lexicons or
larger training corpora.

In Figure 1 we found that increasing the pseudo-
count c consistently decreases performance, so we
used c = 1 in our other experiments.9

6 Conclusion

This paper has described a generative model for
low-resource sequence labeling and segmentation
tasks using lexical resources. Experiments in semi-
supervised and low-resource settings have demon-
strated its applicability to part-of-speech induction
and low-resource named-entity recognition. There
are many potential avenues for future work. Our
model may be useful in the context of active learn-
ing where efficient re-estimation and performance
in low-data conditions are important. It would also
be interesting to explore more expressive parame-
terizations, such recurrent neural networks for Hy.
In the space of neural methods, differentiable mem-
ory (Santoro et al., 2016) may be more flexible
than the PYP prior, while retaining the ability of
the model to cache strings observed in the gazetteer.

Acknowledgments

This work was supported by the JHU Human Lan-
guage Technology Center of Excellence, DARPA
LORELEI, and NSF grant IIS-1423276. Thanks to
Jay Feldman for early discussions.

9Why? Even a pseudocount of c = 1 is enough to ensure
that Py(s) � Hy(s), since the prior probability Hy(s) is
rather small for most strings in the lexicon. Indeed, perhaps
c < 1 would have increased performance, particularly if the
lexicon reflects out-of-domain data. This could be arranged,
in effect, by using a hierarchical Bayesian model in which the
lexicon and corpus emissions are not drawn from the identical
distribution Py but only from similar (coupled) distributions.

1036



100 200 300 400 500
# sentence

0

10

20

30

40

50

60

F1

Model
baseline
c=1
c=10
c=100

Figure 1: Absolute NER performance for Turkish (y-axis) as a function of corpus size (x-axis). The
y-axis gives the F1 score on a held-out evaluation set (averaged over 10 bootstrap replicates, with error
bars showing 95% confidence intervals). Our generative approach is compared to a baseline discriminative
model with lexicon features (lowest curve). 500 held-out sentences were used to create the lexicon for
both methods. Note that increasing the pseudocount c for lexicon entries (upper curves) tends to decrease
performance for the generative model; we therefore take c = 1 in all other experiments. This graph shows
Turkish; the corresponding Uzbek figure is available as supplementary material.

100 200 300 400 500
# sentence

0

10

20

30

40

F1
 m

od
el

 - 
F1

 b
as

el
in

e

Gazetteer size
1000
100
10

Figure 2: Relative NER performance for Turkish (y-axis) as a function of corpus size (x-axis). In this
graph, c = 1 is constant and the curves instead compare different lexicon sizes derived from 10, 100,
and 1000 held-out sentences. The y-axis now gives the difference F1model − F1baseline, so positive values
indicate improvement over the baseline due to the proposed model. Gains are highest for large lexicons
and for small corpora. Again, the corresponding Uzbek figure is available as supplementary material.

1037



References
Rie Kubota Ando and Tong Zhang. 2005. A frame-

work for learning predictive structures from multi-
ple tasks and unlabeled data. Journal of Machine
Learning Research 6:1817–1853.

Galen Andrew and Jianfeng Gao. 2007. Scalable
training of L1-regularized log-linear models. In
Proceedings of the 24th International Conference
on Machine Learning. pages 33–40.

Christophe Andrieu, Arnaud Doucet, and Roman
Holenstein. 2010. Particle Markov chain Monte
Carlo methods. Journal of the Royal Statisti-
cal Society: Series B (Statistical Methodology)
72(3):269–342.

Phil Blunsom and Trevor Cohn. 2010. A hierarchical
Pitman-Yor process HMM for unsupervised part-
of-speech induction. In Proceedings of the 49th
Annual Meeting of the Association for Computa-
tional Linguistics.

Benjamin Borschinger and Mark Johnson. 2011. A
particle filter algorithm for Bayesian wordsegmen-
tation. In Proceedings of the Australasian Lan-
guage Technology Association Workshop 2011.
Canberra, Australia, pages 10–18.

Ronan Collobert, Jason Weston, Léon Bottou, Michael
Karlen, Koray Kavukcuoglu, and Pavel Kuksa.
2011. Natural language processing (almost) from
scratch. Journal of Machine Learning Research
12:2493–2537.

Dipanjan Das and Slav Petrov. 2011. Unsupervised
part-of-speech tagging with bilingual graph-based
projections. In Proceedings of the 49th Annual
Meeting of the Association for Computational Lin-
guistics: Human Language Technologies-Volume 1.
pages 600–609.

Arnaud Doucet and Adam M. Johansen. 2009. A
tutorial on particle filtering and smoothing: Fif-
teen years later. Handbook of Nonlinear Filtering
12:656–704.

Markus Dreyer and Jason Eisner. 2011. Discovering
morphological paradigms from plain text using a
Dirichlet process mixture model. In Proceedings
of the Conference on Empirical Methods in Natural
Language Processing (EMNLP). Edinburgh, pages
616–627.

Gregory Dubbin and Phil Blunsom. 2012. Unsuper-
vised Bayesian part of speech inference with parti-
cle Gibbs. In Proceedings of the 2012 European
Conference on Machine Learning and Knowledge
Discovery in Databases - Volume Part I. Springer-
Verlag, Berlin, Heidelberg, ECML PKDD’12,
pages 760–773.

Jenny Rose Finkel, Trond Grenager, and Christopher
Manning. 2005. Incorporating non-local informa-
tion into information extraction systems by Gibbs

sampling. In Proceedings of the 43rd Annual Meet-
ing on Association for Computational Linguistics.
Stroudsburg, PA, USA, ACL ’05, pages 363–370.

Dan Garrette and Jason Baldridge. 2012. Type-
supervised hidden Markov models for part-of-
speech tagging with incomplete tag dictionaries. In
Proceedings of the 2012 Joint Conference on Em-
pirical Methods in Natural Language Processing
and Computational Natural Language Learning.
pages 821–831.

Jan Gasthaus and Yee Whye Teh. 2010. Improve-
ments to the sequence memoizer. In NIPS. pages
685–693.

Sharon Goldwater and Thomas L. Griffiths. 2007. A
fully Bayesian approach to unsupervised part-of-
speech tagging. In Proceedings of the 45th Annual
Meeting of the Association for Computational Lin-
guistics. Prague, Czech Republic, pages 744–751.

Sharon Goldwater, Mark Johnson, and Thomas L.
Griffiths. 2005. Interpolating between types and
tokens by estimating power-law generators. In Ad-
vances in Neural Information Processing Systems.
pages 459–466.

Mark Johnson, Thomas L. Griffiths, and Sharon Gold-
water. 2007. Bayesian inference for PCFGs via
Markov chain Monte Carlo. In HLT-NAACL. pages
139–146.

Rafal Jozefowicz, Oriol Vinyals, Mike Schuster,
Noam Shazeer, and Yonghui Wu. 2016. Explor-
ing the limits of language modeling. Computing
Research Repository arXiv:1602.02410.

Shen Li, Joao V Graça, and Ben Taskar. 2012. Wiki-ly
supervised part-of-speech tagging. In Proceedings
of the 2012 Joint Conference on Empirical Methods
in Natural Language Processing and Computa-
tional Natural Language Learning. pages 1389–
1398.

Daichi Mochihashi, Takeshi Yamada, and Naonori
Ueda. 2009. Bayesian unsupervised word segmen-
tation with nested Pitman-Yor language modeling.
In Proceedings of the Joint Conference of the 47th
Annual Meeting of the ACL and the 4th Interna-
tional Joint Conference on Natural Language Pro-
cessing of the AFNLP: Volume 1-Volume 1. pages
100–108.

Andrew Y. Ng and Michael I. Jordan. 2002. On dis-
criminative vs. generative classifiers: A comparison
of logistic regression and naive Bayes. Advances in
Neural Information Processing Systems 2:841–848.

Jim Pitman. 1996. Some developments of the
Blackwell-MacQueen urn scheme. In T. S. Fer-
guson, L. S. Shapley, and J. B. MacQueen, editors,
Statistics, Probability and Game Theory: Papers
in Honor of David Blackwell, Institute of Mathe-
matical Statistics, volume 30 of IMS Lecture Notes-
Monograph series, pages 245–267.

1038



Jim Pitman and Marc Yor. 1997. The two-parameter
Poisson-Dirichlet distribution derived from a stable
subordinator. The Annals of Probability pages 855–
900.

Adam Santoro, Sergey Bartunov, Matthew Botvinick,
Daan Wierstra, and Timothy P. Lillicrap. 2016.
One-shot learning with memory-augmented neu-
ral networks. Computing Research Repository
arXiv:1605.06065.

Andrew Smith, Trevor Cohn, and Miles Osborne.
2005. Logarithmic opinion pools for conditional
random fields. In Proceedings of the 43rd Annual
Meeting on Association for Computational Linguis-
tics. pages 18–25.

Andrew Smith and Miles Osborne. 2006. Using
gazetteers in discriminative information extrac-
tion. In Proceedings of the Tenth Conference on
Computational Natural Language Learning. pages
133–140.

Stephanie Strassel and Jennifer Tracey. 2016. Lorelei
language packs: Data, tools, and resources for tech-
nology development in low resource languages. In
Proceedings of the Tenth International Conference
on Language Resources and Evaluation (LREC
2016). European Language Resources Association
(ELRA), Paris, France.

Charles Sutton, Michael Sindelar, and Andrew Mc-
Callum. 2006. Reducing weight undertraining in
structured discriminative learning. In Proceedings
of the Main Conference on Human Language Tech-
nology Conference of the North American Chapter
of the Association of Computational Linguistics.
Stroudsburg, PA, USA, HLT-NAACL ’06, pages
89–95.

Yee Whye Teh. 2006. A hierarchical Bayesian lan-
guage model based on Pitman-Yor processes. In
Proceedings of the 21st International Conference
on Computational Linguistics and the 44th annual
meeting of the Association for Computational Lin-
guistics. pages 985–992.

Nilesh Tripuraneni, Shixiang Gu, Hong Ge, and
Zoubin Ghahramani. 2015. Particle Gibbs for in-
finite hidden Markov models. In Proceedings of
the 28th International Conference on Neural Infor-
mation Processing Systems. MIT Press, Cambridge,
MA, USA, NIPS’15, pages 2395–2403.

Frank Wood, Cédric Archambeau, Jan Gasthaus,
Lancelot James, and Yee Whye Teh. 2009. A
stochastic memoizer for sequence data. In Proceed-
ings of the 26th Annual International Conference
on Machine Learning. pages 1129–1136.

1039


	Bayesian Modeling of Lexical Resources for Low-Resource Settings

