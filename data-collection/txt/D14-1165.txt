



















































Typed Tensor Decomposition of Knowledge Bases for Relation Extraction


Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1568–1579,
October 25-29, 2014, Doha, Qatar. c©2014 Association for Computational Linguistics

Typed Tensor Decomposition of Knowledge Bases for Relation Extraction

Kai-Wei Chang†∗ Wen-tau Yih\ Bishan Yang‡∗ Christopher Meek\

†University of Illinois, Urbana, IL 61801, USA
‡Cornell University, Ithaca, NY 14850, USA

\Microsoft Research, Redmond, WA 98052, USA

Abstract

While relation extraction has traditionally
been viewed as a task relying solely on
textual data, recent work has shown that
by taking as input existing facts in the form
of entity-relation triples from both knowl-
edge bases and textual data, the perfor-
mance of relation extraction can be im-
proved significantly. Following this new
paradigm, we propose a tensor decompo-
sition approach for knowledge base em-
bedding that is highly scalable, and is es-
pecially suitable for relation extraction.
By leveraging relational domain knowl-
edge about entity type information, our
learning algorithm is significantly faster
than previous approaches and is better
able to discover new relations missing
from the database. In addition, when ap-
plied to a relation extraction task, our ap-
proach alone is comparable to several ex-
isting systems, and improves the weighted
mean average precision of a state-of-the-
art method by 10 points when used as a
subcomponent.

1 Introduction

Identifying the relationship between entities from
free text, relation extraction is a key task for ac-
quiring new facts to increase the coverage of a
structured knowledge base. Given a pre-defined
database schema, traditional relation extraction
approaches focus on learning a classifier using tex-
tual data alone, such as patterns between the oc-
currences of two entities in documents, to deter-
mine whether the entities have a particular rela-
tion. Other than using the existing known facts
to label the text corpora in a distant supervision
setting (Bunescu and Mooney, 2007; Mintz et al.,

∗Work conducted while interning at Microsoft Research.

2009; Riedel et al., 2010; Ritter et al., 2013), an
existing knowledge base is typically not involved
in the process of relation extraction.

However, this paradigm has started to shift re-
cently, as researchers showed that by taking exist-
ing facts of a knowledge base as an integral part of
relation extraction, the model can leverage richer
information and thus yields better performance.
For instance, Riedel et al. (2013) borrowed the
idea of collective filtering and constructed a ma-
trix where each row is a pair of entities and each
column is a particular relation. For a true entity-
relation triple (e1, r, e2), either from the text cor-
pus or from the knowledge base, the correspond-
ing entry in the matrix is 1. A previously unknown
fact (i.e., triple) can be discovered through ma-
trix decomposition. This approach can be viewed
as creating vector representations of each relation
and candidate pair of entities. Because each entity
does not have its own representation, relationships
of any unpaired entities cannot be discovered. Al-
ternatively, Weston et al. (2013) created two types
of embedding – one based on textual similarity and
the other based on knowledge base, where the lat-
ter maps each entity and relation to the same d-
dimensional vector space using a model proposed
by Bordes et al. (2013a). They also showed that
combining these two models results in a signif-
icant improvement over the model trained using
only textual data.

To make such an integrated strategy work, it is
important to capture all existing entities and rela-
tions, as well as the known facts, from both tex-
tual data and large databases. In this paper, we
propose a new knowledge base embedding model,
TRESCAL, that is highly efficient and scalable,
with relation extraction as our target application.
Our work is built on top of RESCAL (Nickel
et al., 2011), which is a tensor decomposition
method that has proven its scalability by factoring
YAGO (Biega et al., 2013) with 3 million entities

1568



and 41 million triples (Nickel et al., 2012). We
improve the tensor decomposition model with two
technical innovations. First, we exclude the triples
that do not satisfy the relational constraints (e.g.,
both arguments of the relation spouse-of need to
be person entities) from the loss, which is done
by selecting sub-matrices of each slice of the ten-
sor during training. Second, we introduce a math-
ematical technique that significantly reduces the
computational complexity in both time and space
when the loss function contains a regularization
term. As a consequence, our method is more than
four times faster than RESCAL, and is also more
accurate in discovering unseen triples.

Our contributions are twofold. First, compared
to other knowledge base embedding methods de-
veloped more recently, it is much more efficient
to train our model. As will be seen in Sec. 5,
when applied to a large knowledge base created
using NELL (Carlson et al., 2010) that has 1.8M
entity-relation triples, our method finishes training
in 4 to 5 hours, while an alternative method (Bor-
des et al., 2013a) needs almost 3 days. Moreover,
the prediction accuracy of our model is competi-
tive to others, if not higher. Second, to validate its
value to relation extraction, we apply TRESCAL to
extracting relations from a free text corpus along
with a knowledge base, using the data provided
in (Riedel et al., 2013). We show that TRESCAL
is complementary to existing systems and signif-
icantly improves their performance when using it
as a subcomponent. For instance, this strategy im-
proves the weighted mean average precision of the
best approach in (Riedel et al., 2013) by 10 points
(47% to 57%).

The remainder of this paper is organized as fol-
lows. We survey most related work in Sec. 2 and
provide the technical background of our approach
in Sec. 3. Our approach is detailed in Sec. 4, fol-
lowed by the experimental validation in Sec. 5. Fi-
nally, Sec. 6 concludes the paper.

2 Related Work

Our approach of creating knowledge base em-
bedding is based on tensor decomposition, which
is a well-developed mathematical tool for data
analysis. Existing tensor decomposition models
can be categorized into two main families: the
CP and Tucker decompositions. The CP (CAN-
DECOMP/PARAFAC) decomposition (Kruskal,
1977; Kiers, 2000) approximates a tensor by a sum

of rank-one tensors, while the Tucker decompo-
sition (Tucker, 1966), also known as high-order
SVD (De Lathauwer et al., 2000), factorizes a ten-
sor into a core tensor multiplied by a matrix along
each dimension. A highly scalable distributional
algorithm using the Map-Reduce architecture has
been proposed recently for computing CP (Kang et
al., 2012), but not for the Tucker decomposition,
probably due to its inherently more complicated
model form.

Matrix and tensor decomposition methods have
been applied to modeling multi-relational data.
For instance, Speer et al. (2008) aimed to cre-
ate vectors of latent components for representing
concepts in a common sense knowledge base us-
ing SVD. Franz et al. (2009) proposed TripleRank
to model the subject-predicate-object
RDF triples in a tensor, and then applied the CP
decomposition to identify hidden triples. Fol-
lowing the same tensor encoding, Nickel et al.
(2011) proposed RESCAL, a restricted form of
Tucker decomposition for discovering previously
unknown triples in a knowledge base, and later
demonstrated its scalability by applying it to
YAGO, which was encoded in a 3M × 3M × 38
tensor with 41M triples (Nickel et al., 2012).

Methods that revise the objective function
based on additional domain information have been
proposed, such as MrWTD, a multi-relational
weighted tensor decomposition method (London
et al., 2013), coupled matrix and tensor fac-
torization (Papalexakis et al., 2014), and col-
lective matrix factorization (Singh and Gordon,
2008). Alternatively, instead of optimizing for the
least-squares reconduction loss, a non-parametric
Bayesian approach for 3-way tensor decomposi-
tion for modeling relational data has also been pro-
posed (Sutskever et al., 2009). Despite the exis-
tence of a wide variety of tensor decomposition
models, most methods do not scale well and have
only been tested on datasets that are much smaller
than the size of real-world knowledge bases.

Multi-relational data can be modeled by neural-
network methods as well. For instance, Bordes et
al. (2013b) proposed the Semantic Matching En-
ergy model (SME), which aims to have the same
d-dimensional vector representations for both en-
tities and relations. Given the vectors of entities
e1, e2 and relation r. They first learn the latent
representations of (e1, r) and (e2, r). The score
of (e1, r, e2) is determined by the inner product

1569



of the vectors of (e1, r) and (e2, r). Later, they
proposed a more scalable method called translat-
ing embeddings (TransE) (Bordes et al., 2013a).
While both entities and relations are still repre-
sented by vectors, the score of (e1, r, e2) becomes
the negative dissimilarity measure of the corre-
sponding vectors −‖ei + rk − ej‖, motivated by
the work in (Mikolov et al., 2013b; Mikolov et al.,
2013a). Alternatively, Socher et al. (2013) pro-
posed a Neural Tensor Network (NTN) that repre-
sents entities in d-dimensional vectors created sep-
arately by averaging pre-trained word vectors, and
then learns a d×d×m tensor describing the inter-
actions between these latent components in each
of the m relations. All these methods optimize
for loss functions that are more directly related to
the true objective – the prediction accuracy of cor-
rect entity-relation triples, compared to the mean-
squared reconstruction error in our method. Nev-
ertheless, they typically require much longer train-
ing time.

3 Background

In this section, we first describe how entity-
relation triples are encoded in a tensor. We then
introduce the recently proposed tensor decompo-
sition method, RESCAL (Nickel et al., 2011) and
explain how it adopts an alternating least-squares
method, ASALSAN (Bader et al., 2007), to com-
pute the factorization.

3.1 Encoding Binary Relations in a Tensor

Suppose we are given a knowledge base with
n entities and m relation types, and the facts
in the knowledge base are denoted as a set of
entity-relation triples T = {(ei, rk, ej)}, where
i, j ∈ {1, 2, · · ·n} and k ∈ {1, 2, · · ·m}. A
triple (ei, rk, ej) simply means that the i-th en-
tity and the j-th entity have the k-th relation.
Following (Franz et al., 2009), these triples can
naturally be encoded in a 3-way tensor X ∈
{0, 1}n×n×m, such that Xi,j,k = 1 if and only if
the triple (ei, rk, ej) ∈ T 1. The tensor can be
viewed as consisting of m slices, where each slice
is an n×n square matrix, denoting the interactions
of the entities of a particular relation type. In the
remainder of this paper, we will use Xk to refer to
the k-th slice of the tensor X . Fig. 1 illustrates this
representation.

1This representation can easily be extended for a proba-
bilistic knowledge base by allowing nonnegative real values.

e1   en

e 1
  
 e

n

χ

χ
k

Figure 1: A tensor encoding of m binary relation
types and n entities. A sliceXk denotes the entities
having the k-th relation.

3.2 RESCAL

In order to identify latent components in a ten-
sor for collective learning, Nickel et al. (2011)
proposed RESCAL, which is a tensor decomposi-
tion approach specifically designed for the multi-
relational data described in Sec. 3.1. Given a ten-
sor Xn×n×m, RESCAL aims to have a rank-r ap-
proximation, where each slice Xk is factorized as

Xk ≈ ARkAT . (1)

A is an n × r matrix, where the i-th row denotes
the r latent components of the i-th entity. Rk is an
asymmetric r × r matrix that describes the inter-
actions of the latent components according to the
k-th relation. Notice that while Rk differs in each
slice, A remains the same.

A and Rk are derived by minimizing the loss
function below.

min
A,Rk

f(A,Rk) + λ · g(A,Rk), (2)

where f(A,Rk) = 12
(∑

k ‖Xk −ARkAT ‖2F
)

is the mean-squared reconstruction error and
g(A,Rk) = 12

(‖A‖2F +∑k ‖Rk‖2F ) is the regu-
larization term.

RESCAL is a special form of Tucker decom-
position (Tucker, 1966) operating on a 3-way ten-
sor. Its model form (Eq. (1)) can also be regarded
as a relaxed form of DEDICOM (Bader et al.,
2007), which derives the low-rank approximation
as: Xk ≈ ADkRDkAT . To compare RESCAL
to other tensor decomposition methods, interested
readers can refer to (Kolda and Bader, 2009).

1570



The optimization problem in Eq. (2) can be
solved using the efficient alternating least-squares
(ALS) method. This approach alternatively fixes
Rk to solve for A and then fixes A to solve
Rk. The whole procedure stops until f(A,Rk)‖X‖2F con-
verges to some small threshold � or the maximum
number of iterations has been reached.

By finding the solutions where the gradients are
0, we can derive the update rules of A and Rk as
below.

A←
[∑

k

XkARTk +X Tk ARk
][∑

k

Bk+Ck+λI
]−1

,

where Bk = RkAT ARTk and Ck = RTk AT ARk.

vec(Rk)←
(
ZT Z + λI

)−1
ZT vec(Xk), (3)

where vec(Rk) is the vectorization of Rk, Z =
A⊗A and the operator ⊗ is the Kronecker prod-
uct.

Complexity Analysis Following the analysis in
(Nickel et al., 2012), we assume that each Xk is a
sparse matrix, and let p be the number of non-zero
entries2. The complexity of computing XkARTk
and X Tk ARk is O(pr + nr2). Evaluating Bk and
Ck requires O(nr2) and the matrix inversion re-
quires O(r3). Therefore, the complexity of updat-
ing A isO(pr+nr2) assuming n� r. The updat-
ing rule of Rk involves inverting an r2 × r2 ma-
trix. Therefore, directly computing the inversion
requires time complexity O(r6) and space com-
plexity O(r4). Although Nickel et al. (2012) con-
sidered using QR decomposition to simplify the
updates, it is still time consuming with the time
complexity O(r6 + pr2). Therefore, the total time
complexity isO(r6+pr2) and the step of updating
Rk is the bottleneck in the optimization process.
We will describe how to reduce the time complex-
ity of this step to O(nr2 + pr) in Section 4.2.

4 Approach

We describe how we leverage the relational do-
main knowledge in this section. By removing the
incompatible entity-relation triples from the loss

2Notice that we use a slightly different definition of p
from the one in (Nickel et al., 2012). The time complexity
of multiplying an n × n sparse matrix Xk with p non-zero
entries by an n× r dense matrix is O(pr) assuming n� r.

function, training can be done much more effi-
ciently and results in a model with higher pre-
diction accuracy. In addition, we also introduce
a mathematical technique to reduce the compu-
tational complexity of the tensor decomposition
methods when taking into account the regulariza-
tion term.

4.1 Applying Relational Domain Knowledge

In the domain of knowledge bases, the notion of
entity types is the side information that commonly
exists and dictates whether some entities can be
legitimate arguments of a given predicate. For
instance, suppose the relation of interest is born-
in, which denotes the birth location of a person.
When asked whether an incompatible pair of en-
tities, such as two person entities like Abraham
Lincoln and John Henry, having this rela-
tion, we can immediately reject the possibility. Al-
though the type information and the constraints
are readily available, it is overlooked in the pre-
vious work on matrix and tensor decomposition
models for knowledge bases (Riedel et al., 2013;
Nickel et al., 2012). Ignoring the type information
has two implications. Incompatible entity-relation
triples still participate in the loss function of the
optimization problem, which incurs unnecessary
computation. Moreover, by choosing values for
these incompatible entries we introduce errors in
training the model that can reduce the quality of
the model.

Based on this observation, we propose Typed-
RESCAL, or TRESCAL, which leverages the en-
tity type information to improve both the effi-
ciency of model training and the quality of the
model in term of prediction accuracy. We em-
ploy a direct and simple approach by excluding
the triples of the incompatible entity types from
the loss in Eq. (2). For each relation, let Lk and
Rk be the set of entities with a compatible type to
the k-th relation. That is, (ei, rk, ej) is a feasible
triple if and only if ei ∈ Lk and ej ∈ Rk. For no-
tational convenience, we use Akl ,Akr to denote
the sub-matrices of A that consists of rows asso-
ciated with Lk and Rk, respectively. Analogously,
let Xklr be the sub-matrix of Xk that consists of
only the entity pairs compatible to the k-th rela-
tion. The rows and columns of Xklr map to the en-
tities in Akl and Akr , respectively. In other words,
entries of Xk but not in Xklr do not satisfy the type
constraint and are ignored from the computation.

1571



~ ~   

χ
k A 

A 
T

Rk

A kl
A kr

T

χ
klr

e Lk

e Rk

Figure 2: The construction of TRESCAL. Suppose
the k-th relation is born-in. Lk is then a set of
person entities and Rk is a set of location entities.
Only the sub-matrix corresponds to the compati-
ble entity pairs (i.e., Xklr ) and the sub-matrices of
the associated entities (i.e., Akl and A

T
kr

) will be
included in the loss.

Fig. 2 illustrates this construction.
TRESCAL solves the following optimization

problem:

min
A,Rk

f ′(A,Rk) + λ · g(A,Rk), (4)

where f ′(A,Rk) = 12
∑

k ‖Xklr −AklRkATkr‖2F
and g(A,Rk) = 12

(‖A‖2F +∑k ‖Rk‖2F ).
Similarly, A and Rk can be solved using the

alternating least-squares method. The update rule
of A is

A←
[∑

k

(XklrAkrRTk + X TklrAklRk)
]
×

[∑
k

Bkr + Ckl + λI
]−1

,

where Bkr = RkATkrAkrRTk and Ckl =
RTk ATklAklRk.

The update ofRk becomes:

vec(Rk)←
(
ATkrAkr ⊗ATklAkl + λI

)−1×
vec(Akl

TXklrAkr),
(5)

Complexity Analysis Let n̄ be the average
number of entities with a compatible type to a
relation. Follow a similar derivation in Sec. 3.2,
the time complexity of updating A isO(pr+ n̄r2)
and the time complexity of updating Rk remains
to be O(r6 + pr2).

4.2 Handling Regularization Efficiently
Examining the update rules of both RESCAL
and TRESCAL, we can see that the most time-
consuming part is the matrix inversions. For
RESCAL, this is the term (ZT Z+λI)−1 in Eq. (3),
where Z = A⊗A. Nickel et al. (2011) made the
observation that if λ = 0, the matrix inversion can
be calculated by

(ZT Z)−1 = (AT A)−1A⊗ (AT A)−1A.

Then, it only involves an inversion of an r× r ma-
trix, namely AT A. However, if λ > 0, directly
calculating Eq. (3) requires to invert an r2 × r2
matrix and thus becomes a bottleneck in solving
Eq. (2).

To reduce the computational complexity of
the update rules of Rk, we compute the inver-
sion

(
ZT Z + λI

)−1 by applying singular value
decomposition (SVD) to A, such that A =
UΣVT , where U and V are orthogonal matrices
and Σ is a diagonal matrix. Then by using proper-
ties of the Kronecker product we have:(

ZT Z + λI
)−1

=
(
λI + VΣ2VT ⊗VΣ2VT )−1

=
(
λI + (V ⊗V)(Σ2 ⊗Σ2)(V ⊗V)T )−1

= (V ⊗V) (λI + Σ2 ⊗Σ2)−1 (V ⊗V)T .
The last equality holds because V ⊗ V is

also an orthogonal matrix. We leave the de-
tailed derivations in Appendix A. Notice that(
λI + Σ2 ⊗Σ2)−1 is a diagonal matrix. There-

fore, the inversion calculation is trivial.
This technique can be applied to TRESCAL

as well. By applying SVD to both Akl
and Akr , we have Akl = UklΣklV

T
kl

and
Akr = UkrΣkrV

T
kr

, respectively. The computa-

tion of
(
ATkrAkr ⊗ATklAkl + λI

)−1
of Eq. (5)

thus becomes:

(Vkl⊗Vkr)
(
λI + Σ2kl ⊗Σ2kr

)−1 (Vkl⊗Vkr)T .
The procedure of updating R is depicted in Al-

gorithm 1.

Complexity Analysis For RESCAL, V and Σ
can be computed by finding eigenvectors of AT A.
Therefore, computing SVD of A costs O(nr2 +
r3) = O(nr2). Computing Step 4 in Algorithm 1
takes O(nr2 + pr). Step 5 and Step 6 require

1572



Algorithm 1 UpdatingR in TRESCAL
Require: X , A, and entity sets Rk,Lk,∀k
Ensure: Rk,∀k.

1: for k = 1 . . .m do
2: [Ukl ,Σ

2
kl
,Vkl ]← SVD(ATklAkl).

3: [Ukr ,Σ
2
kr
,Vkr ]← SVD(ATkrAkr).

4: M1 ← VTklATklXklrAkrVkr .
5: M2 ← diag(Σ2kl) diag(Σ2kr)T + λ1.

(1 is a matrix of all ones. Function diag
converts the diagonal entries of a matrix to
a vector. )

6: Rk ← Vkl(M1./M2)VTkr .
(The operator “./” is element-wise divi-
sion.)

7: end for

O(r2) and O(r3), respectively. The overall time
complexity of updatingRk becomesO(nr2 +pr).

Using a similar derivation, the time complex-
ity of updating Rk in TRESCAL is O(n̄r2 + pr).
Therefore, the total complexity of each iteration is
O(n̄r2 + pr).

5 Experiments

We conduct two sets of experiments. The first
evaluates the proposed TRESCAL algorithm on
inferring unknown facts using existing relation–
entity triples, while the second demonstrates its
application to relation extraction when a text cor-
pus is available.

5.1 Knowledge Base Completion

We evaluate our approach on a knowledge base
generated by the CMU Never Ending Language
Learning (NELL) project (Carlson et al., 2010).
NELL collects human knowledge from the web
and has generated millions of entity-relation
triples. We use the data generated from version
165 for training3, and collect the new triples gen-
erated between NELL versions 166 and 533 as the
development set and those generated between ver-
sion 534 and 745 as the test set4. The data statistics
of the training set are summarized in Table 1. The
numbers of triples in the development and test sets
are 19,665 and 117,889, respectively. Notice that
this dataset is substantially larger than the datasets
used in recent work. For example, the Freebase
data used in (Socher et al., 2013) and (Bordes et

3http://www.cs.cmu.edu/˜nlao/
4http://bit.ly/trescal

NELL
# entities 753k
# relation types 229
# entity types 300
# entity-relation triples 1.8M

Table 1: Data statistics of the training set from
NELL in our experiments.

al., 2013a) have 316k and 483k5 triples, respec-
tively, compared to 1.8M in this dataset.

In the NELL dataset, the entity type informa-
tion is encoded in a specific relation, called Gen-
eralization. Each entity in the knowledge base is
assigned to at least one category presented by the
Generalization relationship. Based on this infor-
mation, the compatible entity type constraint of
each relation can be easily identified. Specifically,
we examined the entities and relations that occur
in the triples of the training data, and counted all
the types appearing in these instances of a given
relation legitimate.

We implement RESCAL and TRESCAL in
MATLAB with the Matlab tensor Toolbox (Bader
et al., 2012). With the efficient implementation
described in Section 4.2, all experiments can be
conducted on a commodity PC with 16 GB mem-
ory. We set the maximal number of iterations of
both RESCAL and TRESCAL to be 10, which we
found empirically to be enough to generate a sta-
ble model. Note that Eq. (4) is non-convex, and the
optimization process does not guarantee to con-
verge to a global minimum. Therefore, initial-
izing the model properly might be important for
the performance. Following the implementation of
RESCAL, we initialize A by performing singular
value decomposition over X̄ = ∑k(Xk + X Tk ),
such that X̄ = UΣVT and set A = U. Then,
we apply the update rule ofRk to initialize {Rk}.
RESCAL and TRESCAL have two types of param-
eters: (1) the rank r of the decomposed tensor and
(2) the regularization parameter λ. We tune the
rank parameter on development set in a range of
{100, 200, 300, 400} and the regularization pa-
rameter in a range of {0.01, 0.05, 0.1, 0.5, 1}.

For comparison, we also use the code released
by Bordes et al. (2013a), which is implemented
using Python and the Theano library (Bergstra
et al., 2010), to train a TransE model using the

5In (Bordes et al., 2013a), there is a much larger dataset,
FB1M, that has 17.5M triples used for evaluation. However,
this dataset has not been released.

1573



Entity Retrieval Relation Retrieval
TransE RESCAL TRESCAL TransE RESCAL TRESCAL

w/o type checking 51.41%‡ 51.59% 54.79% 75.88% 73.15%† 76.12%
w/ type checking 67.56% 62.91%‡ 69.26% 70.71%‡ 73.08%† 75.70%

Table 2: Model performance in mean average precision (MAP) on entity retrieval and relation retrieval.
† and ‡ indicate the comparison to TRESCAL in the same setting is statistically significant using a paired-
t test on average precision of each query, with p < 0.01 and p < 0.05, respectively. Enforcing type
constraints during test time improves entity retrieval substantially, but does not help in relation retrieval.

same NELL dataset. We reserved randomly 1%
of the training triples for the code to evaluate the
model performance in each iteration. As sug-
gested in their paper, we experiment with sev-
eral hyper-parameters, including learning rate of
{0.01, 0.001}, the latent dimension of {50, 100}
and the similarity measure of {L1, L2}. In addi-
tion, we also adjust the number of batches of {50,
100, 1000}. Of all the configurations, we keep the
models picked by the method, as well as the fi-
nal model after 500 training iterations. The final
model is chosen by the performance on our devel-
opment set.

5.1.1 Training Time Reduction
We first present experimental results demonstrat-
ing that TRESCAL indeed reduces the time re-
quired to factorize a knowledge database, com-
pared to RESCAL. The experiment is conducted
on NELL with r = 300 and λ = 0.1. When
λ 6= 0, the original RESCAL algorithm described
in (Nickel et al., 2011; Nickel et al., 2012) cannot
handle a large r, because updating matrices {Rk}
requires O(r4) memory. Later in this section, we
will show that in some situation a large rank r is
necessary for achieving good testing performance.

Comparing TRESCAL with RESCAL, each it-
eration of TRESCAL takes 1,608 seconds, while
that of RESCAL takes 7,415 seconds. In other
words, by inducing the entity type information
and constraints, TRESCAL enjoys around 4.6 times
speed-up, compared to an improved regularized
version of RESCAL. When updating A and {Rk}
TRESCAL only requires operating on sub-matrices
of A, {Rk} and {Xk}, which reduces the compu-
tation substantially. In average, TRESCAL filters
96% of entity triples that have incompatible types.

In contrast, it takes TransE at least 2 days and 19
hours to finish training the model (the default 500
iterations)6, while TRESCAL finishes the training

6It took almost 4 days to train the best TransE model that

in roughly 4 to 5 hours7.

5.1.2 Test Performance Improvement
We consider two different types of tasks to evalu-
ate the prediction accuracy of different models –
entity retrieval and relation retrieval.

Entity Retrieval In the first task, we collect a
set of entity-relation pairs {(ei, rk)} and aim at
predicting ej such that the tuple (ei, rk, ej) is a
recorded triple in the NELL knowledge base. For
each pair (ei, rk), we collect triples {(ei, rk, e∗j )}
from the NELL test corpus as positive samples
and randomly pick 100 entries e′j to form negative
samples {ei, rk, e′j}. Given A and Rk from the
factorization generated by RESCAL or TRESCAL,
the score assigned to a triple {ei, rk, e′j} is com-
puted by aTi Rkaj where ai and aj are the i-th
and j-th rows of A. In TransE, the score is de-
termined by the negative dissimilarity measures of
the learned embeddings: −d(ei, rk, e′j) = −‖ei +
rk − e′j‖22.

We evaluate the performance using mean aver-
age precision (MAP), which is a robust and sta-
ble metric (Manning et al., 2008). As can be
observed in Table 2 (left), TRESCAL achieves
54.79%, which outperforms 51.59% of RESCAL
and 51.41% of TransE. Adding constraints during
test time by assigning the lowest score to the en-
tity triples with incompatible types improves re-
sults of all models – TRESCAL still performs the
best (69.26%), compared to TransE (67.56%) and
RESCAL (62.91%).

Relation Retrieval In the second task, given a
relation type rk, we are looking for the entity pairs
(ei, ej) that have this specific relationship. To gen-
erate test data, for each relation type, we collect

is included in Table 2.
7We also tested the released code from (Socher et al.,

2013) for training a neural tensor network model. However,
we are not able to finish the experiments as each iteration of
this method takes almost 5 hours.

1574



gold entity pairs from the NELL knowledge base
as positive samples and randomly pick a set of en-
tity pairs as negative samples such that the number
of positive samples are the same as negative ones.

Results presented in Table 2 (right) show that
TRESCAL achieves 76.12%, while RESCAL and
TransE are 73.15% and 75.88%, respectively.
Therefore, incorporating the type information in
training seems to help in this task as well. Enforc-
ing the type constraints during test time does not
help as in entity retrieval. By removing incom-
patible entity pairs, the performance of TRESCAL,
RESCAL and TransE drop slightly to 75.70%,
73.08% and 70.71% respectively. One possible
explanation is that the task of relation retrieval is
easier than entity retrieval. The incorrect type in-
formation of some entities ends up filtering out a
small number of entity pairs that were retrieved
correctly by the model.

Notice that TRESCAL achieves different levels
of performance on various relations. For example,
it performs well on predicting AthletePlaysSport
(81%) and CoachesInLeague (88%), but achieves
suboptimal performance on predicting Works-
For (49%) and BuildingLocatedInCity (35%).
We hypothesize that it is easier to gener-
alize entity-relation triples when the relation
has several related relations. For examples,
AthletePlaysForTeam and TeamPlaysSport may
help discover entity-relation triples of Ath-
letePlaysSport.

5.1.3 Sensitivity to Parameters
We also study if TRESCAL is sensitive to the rank
parameter r and the regularization parameter λ,
where the detailed results can be found in Ap-
pendix B. In short, we found that increasing the
rank r generally leads to better models. Also,
while the model is not very sensitive to the value
of the regularization parameter λ, tuning λ is still
necessary for achieving the best performance.

5.2 Relation Extraction

Next, we apply TRESCAL to the task of extract-
ing relations between entities, jointly from a text
corpus and a structured knowledge base. We use
a corpus from (Riedel et al., 2013) that is cre-
ated by aligning the entities in NYTimes and Free-
base. The corpus consists of a training set and a
test set. In the training set, a list of entity pairs
are provided, along with surface patterns extracted
from NYTimes and known relations obtained from

Freebase. In the test set, only the surface patterns
are given. By jointly factoring a matrix consist-
ing of the surface patterns and relations, Riedel et
al. (2013) show that their model is able to capture
the mapping between the surface patterns and the
structured relations and hence is able to extract the
entity relations from free text. In the following, we
show that TRESCAL can be applied to this task.

We focus on the 19 relations listed in Table 1
of (Riedel et al., 2013) and only consider the
surface patterns that co-occur with these 19 re-
lations. We prune the surface patterns that oc-
cur less than 5 times and remove the entities that
are not involved in any relation and surface pat-
tern. Based on the training and test sets, we
build a 80,698×80,698×1,652 tensor, where each
slice captures a particular structured relation or a
surface pattern between two entities. There are
72 fine types extracted from Freebase assigned
to 53,836 entities that are recorded in Freebase.
In addition, special types, PER, LOC, ORG and
MISC, are assigned to the remaining 26,862 enti-
ties based on the predicted NER tags provided by
the corpus. A type is considered incompatible to a
relation or a surface pattern if in the training data,
none of the argument entities of the relation be-
longs to the type. We use r = 400 and λ = 0.1 in
TRESCAL to factorize the tensor.

We compare the proposed TRESCAL model to
RI13 (Riedel et al., 2013), YA11 (Yao et al., 2011),
MI09 (Mintz et al., 2009) and SU12 (Surdeanu et
al., 2012)8. We follow the protocol used in (Riedel
et al., 2013) to evaluate the results. Given a re-
lation as query, the top 1,000 entity pairs output
by each system are collected and the top 100 ones
are judged manually. Besides comparing individ-
ual models, we also report the results of combined
models. To combine the scores from two models,
we simply normalize the scores of entity-relation
tuples to zero mean and unit variance and take the
average. The results are summarized in Table 3.

As can been seen in the table, using TRESCAL
alone is not very effective and its performance is
only compatible to MI09 and YA11, and is sig-
nificantly inferior to RI13. This is understandable
because the problem setting favors RI13 as only
entity pairs that have occurred in the text or the
database will be considered in RI13, both during
model training and testing. In contrast, TRESCAL

8The corpus and the system outputs are from http://
www.riedelcastro.org/uschema

1575



Relation # MI09 YA11 SU12 RI13 TR TR+SU12 TR+RI13
person/company 171 0.41 0.40 0.43 0.49 0.43 0.53 0.64
location/containedby 90 0.39 0.43 0.44 0.56 0.23 0.46 0.58
parent/child 47 0.05 0.10 0.25 0.31 0.19 0.24 0.35
person/place of birth 43 0.32 0.31 0.34 0.37 0.50 0.61 0.66
person/nationality 38 0.10 0.30 0.09 0.16 0.13 0.16 0.22
author/works written 28 0.52 0.53 0.54 0.71 0.00 0.39 0.62
person/place of death 26 0.58 0.58 0.63 0.63 0.54 0.72 0.89
neighborhood/neighborhood of 13 0.00 0.00 0.08 0.67 0.08 0.13 0.73
person/parents 8 0.21 0.24 0.51 0.34 0.01 0.16 0.38
company/founders 7 0.14 0.14 0.30 0.39 0.06 0.17 0.44
film/directed by 4 0.06 0.15 0.25 0.30 0.03 0.13 0.35
sports team/league 4 0.00 0.43 0.18 0.63 0.50 0.29 0.63
team/arena stadium 3 0.00 0.06 0.06 0.08 0.00 0.04 0.09
team owner/teams owned 2 0.00 0.50 0.70 0.75 0.00 0.00 0.75
roadcast/area served 2 1.00 0.50 1.00 1.00 0.50 0.83 1.00
structure/architect 2 0.00 0.00 1.00 1.00 0.00 0.02 1.00
composer/compositions 2 0.00 0.00 0.00 0.12 0.00 0.00 0.12
person/religion 1 0.00 1.00 1.00 1.00 0.00 1.00 1.00
film/produced by 1 1.00 1.00 1.00 0.33 0.00 1.00 0.25
Weighted MAP 0.33 0.36 0.39 0.47 0.30 0.44 0.57

Table 3: Weighted Mean Average Precisions. The # column shows the number of true facts in the pool.
Bold faced are winners per relation, italics indicate ties based on a sign test.

predicts all the possible combinations between en-
tities and relations, which makes the model less fit
to the task. However, when combining TRESCAL
with a pure text-based method, such as SU12,
we can clearly see TRESCAL is complementary
to SU12 (0.39 to 0.44 in weighted MAP score),
which makes the results competitive to RI13.

Interestingly, although both TRESCAL and RI13
leverage information from the knowledge base, we
find that by combining them, the performance is
improved quite substantially (0.47 to 0.57). We
suspect that the reason is that in our construc-
tion, each entity has its own vector representa-
tion, which is lacked in RI13. As a result, the
new triples that TRESCAL finds are very different
from those found by RI13. Nevertheless, com-
bining more methods do not always yield an im-
provement. For example, combining TR, RI13 and
SU12 together (not included in Table 3) achieves
almost the same performance as TR+RI13.

6 Conclusions

In this paper we developed TRESCAL, a tensor
decomposition method that leverages relational
domain knowledge. We use relational domain
knowledge to capture which triples are potentially
valid and found that, by excluding the triples that
are incompatible when performing tensor decom-
position, we can significantly reduce the train-
ing time and improve the prediction performance
as compared with RESCAL and TransE. More-

over, we demonstrated its effectiveness in the ap-
plication of relation extraction. Evaluated on the
dataset provided in (Riedel et al., 2013), the per-
formance of TRESCAL alone is comparable to sev-
eral existing systems that leverage the idea of dis-
tant supervision. When combined with the state-
of-the-art systems, we found that the results can
be further improved. For instance, the weighted
mean average precision of the previous best ap-
proach in (Riedel et al., 2013) has been increased
by 10 points (47% to 57%).

There are a number of interesting potential ex-
tensions of our work. First, while the experiments
in this paper are on traditional knowledge bases
and textual data, the idea of leveraging relational
domain knowledge is likely to be of value to other
linguistic databases as well. For instance, part-of-
speech tags can be viewed as the “types” of words.
Incorporating such information in other tensor de-
composition methods (e.g., (Chang et al., 2013))
may help lexical semantic representations. Sec-
ond, relational domain knowledge goes beyond
entity types and their compatibility with specific
relations. For instance, the entity-relation triple
(e1, child-of, e2) can be valid only if e1.type =
person ∧ e2.type = person ∧ e1.age < e2.age.
It would be interesting to explore the possibility
of developing efficient methods to leverage other
types of relational domain knowledge. Finally, we
would like to create more sophisticated models of
knowledge base embedding, targeting complex in-

1576



ference tasks to better support semantic parsing
and question answering.

Acknowledgments

We thank Sebastian Riedel for providing the data
for experiments. We are also grateful to the anony-
mous reviewers for their valuable comments.

References
Brett W Bader, Richard A Harshman, and Tamara G

Kolda. 2007. Temporal analysis of semantic graphs
using ASALSAN. In ICDM, pages 33–42. IEEE.

Brett W. Bader, Tamara G. Kolda, et al. 2012. Matlab
tensor toolbox version 2.5. Available online, Jan-
uary.

James Bergstra, Olivier Breuleux, Frédéric Bastien,
Pascal Lamblin, Razvan Pascanu, Guillaume Des-
jardins, Joseph Turian, David Warde-Farley, and
Yoshua Bengio. 2010. Theano: a CPU and
GPU math expression compiler. In Proceedings
of the Python for Scientific Computing Conference
(SciPy), June. Oral Presentation.

Joanna Biega, Erdal Kuzey, and Fabian M Suchanek.
2013. Inside YOGO2s: a transparent information
extraction architecture. In WWW, pages 325–328.

A. Bordes, N. Usunier, A. Garcia-Duran, J. Weston,
and O. Yakhnenko. 2013a. Translating Embeddings
for Modeling Multi-relational Data. In Advances in
Neural Information Processing Systems 26.

Antoine Bordes, Xavier Glorot, Jason Weston, and
Yoshua Bengio. 2013b. A semantic matching en-
ergy function for learning with multi-relational data.
Machine Learning, pages 1–27.

Razvan Bunescu and Raymond Mooney. 2007. Learn-
ing to extract relations from the web using mini-
mal supervision. In Proceedings of the 45th Annual
Meeting of the Association of Computational Lin-
guistics, pages 576–583, Prague, Czech Republic,
June. Association for Computational Linguistics.

Andrew Carlson, Justin Betteridge, Bryan Kisiel, Burr
Settles, Estevam R. Hruschka Jr., and Tom M.
Mitchell. 2010. Toward an architecture for never-
ending language learning. In AAAI.

Kai-Wei Chang, Wen-tau Yih, and Christopher Meek.
2013. Multi-relational latent semantic analysis. In
Proceedings of the 2013 Conference on Empirical
Methods in Natural Language Processing, pages
1602–1612, Seattle, Washington, USA, October.
Association for Computational Linguistics.

Lieven De Lathauwer, Bart De Moor, and Joos Vande-
walle. 2000. A multilinear singular value decompo-
sition. SIAM journal on Matrix Analysis and Appli-
cations, 21(4):1253–1278.

Thomas Franz, Antje Schultz, Sergej Sizov, and Steffen
Staab. 2009. Triplerank: Ranking semantic web
data by tensor decomposition. In The Semantic Web-
ISWC 2009, pages 213–228. Springer.

U Kang, Evangelos Papalexakis, Abhay Harpale, and
Christos Faloutsos. 2012. Gigatensor: scaling ten-
sor analysis up by 100 times-algorithms and discov-
eries. In KDD, pages 316–324. ACM.

Henk AL Kiers. 2000. Towards a standardized nota-
tion and terminology in multiway analysis. Journal
of chemometrics, 14(3):105–122.

Tamara G. Kolda and Brett W. Bader. 2009. Ten-
sor decompositions and applications. SIAM Review,
51(3):455–500, September.

Joseph B Kruskal. 1977. Three-way arrays: rank and
uniqueness of trilinear decompositions, with appli-
cation to arithmetic complexity and statistics. Lin-
ear algebra and its applications, 18(2):95–138.

Alan J Laub, 2005. Matrix analysis for scientists and
engineers, chapter 13, pages 139–150. SIAM.

Ben London, Theodoros Rekatsinas, Bert Huang, and
Lise Getoor. 2013. Multi-relational learning using
weighted tensor decomposition with modular loss.
Technical report, University of Maryland College
Park. http://arxiv.org/abs/1303.1733.

C. Manning, P. Raghavan, and H. Schutze. 2008.
Introduction to Information Retrieval. Cambridge
University Press.

T. Mikolov, I. Sutskever, K. Chen, G. Corrado, and
J. Dean. 2013a. Distributed representations of
words and phrases and their compositionality. In
Advances in Neural Information Processing Systems
26.

Tomas Mikolov, Wen-tau Yih, and Geoffrey Zweig.
2013b. Linguistic regularities in continuous space
word representations. In Proceedings of the 2013
Conference of the North American Chapter of the
Association for Computational Linguistics: Human
Language Technologies, pages 746–751, Atlanta,
Georgia, June. Association for Computational Lin-
guistics.

Mike Mintz, Steven Bills, Rion Snow, and Daniel Ju-
rafsky. 2009. Distant supervision for relation ex-
traction without labeled data. In Proceedings of the
Joint Conference of the 47th Annual Meeting of the
ACL and the 4th International Joint Conference on
Natural Language Processing of the AFNLP, pages
1003–1011, Suntec, Singapore, August. Association
for Computational Linguistics.

Maximilian Nickel, Volker Tresp, and Hans-Peter
Kriegel. 2011. A three-way model for collective
learning on multi-relational data. In ICML, pages
809–816.

1577



Maximilian Nickel, Volker Tresp, and Hans-Peter
Kriegel. 2012. Factorizing YAGO: scalable ma-
chine learning for linked data. In WWW, pages 271–
280.

Evangelos E Papalexakis, Tom M Mitchell, Nicholas D
Sidiropoulos, Christos Faloutsos, Partha Pratim
Talukdar, and Brian Murphy. 2014. Turbo-smt:
Accelerating coupled sparse matrix-tensor factoriza-
tions by 200x. In SDM.

Sebastian Riedel, Limin Yao, and Andrew McCallum.
2010. Modeling relations and their mentions with-
out labeled text. In Proceedings of ECML/PKDD
2010. Springer.

Sebastian Riedel, Limin Yao, Andrew McCallum, and
Benjamin M. Marlin. 2013. Relation extraction
with matrix factorization and universal schemas. In
NAACL, pages 74–84.

Alan Ritter, Luke Zettlemoyer, Mausam, and Oren Et-
zioni. 2013. Modeling missing data in distant su-
pervision for information extraction. Transactions
of the Association for Computational Linguistics,
1:367–378, October.

Ajit P Singh and Geoffrey J Gordon. 2008. Relational
learning via collective matrix factorization. In Pro-
ceedings of the 14th ACM SIGKDD international
conference on Knowledge discovery and data min-
ing, pages 650–658. ACM.

Richard Socher, Danqi Chen, Christopher D. Manning,
and Andrew Y. Ng. 2013. Reasoning With Neural
Tensor Networks For Knowledge Base Completion.
In Advances in Neural Information Processing Sys-
tems 26.

Robert Speer, Catherine Havasi, and Henry Lieberman.
2008. Analogyspace: Reducing the dimensionality
of common sense knowledge. In AAAI, pages 548–
553.

Mihai Surdeanu, Julie Tibshirani, Ramesh Nallapati,
and Christopher D. Manning. 2012. Multi-instance
multi-label learning for relation extraction. In Pro-
ceedings of the Conference on Empirical Methods
in Natural Language Processing and Computational
Natural Language Learning (EMNLP-CoNLL).

Ilya Sutskever, Joshua B Tenenbaum, and Ruslan
Salakhutdinov. 2009. Modelling relational data us-
ing Bayesian clustered tensor factorization. In NIPS,
pages 1821–1828.

Ledyard R Tucker. 1966. Some mathematical notes
on three-mode factor analysis. Psychometrika,
31(3):279–311.

Jason Weston, Antoine Bordes, Oksana Yakhnenko,
and Nicolas Usunier. 2013. Connecting language
and knowledge bases with embedding models for re-
lation extraction. In Proceedings of the 2013 Con-
ference on Empirical Methods in Natural Language
Processing, pages 1366–1371, Seattle, Washington,

USA, October. Association for Computational Lin-
guistics.

Limin Yao, Aria Haghighi, Sebastian Riedel, and An-
drew McCallum. 2011. Structured relation dis-
covery using generative models. In Proceedings of
the 2011 Conference on Empirical Methods in Nat-
ural Language Processing, pages 1456–1466, Edin-
burgh, Scotland, UK., July. Association for Compu-
tational Linguistics.

Appendix A Detailed Derivation

We first introduce some lemmas that will be useful
for our derivation. Lemmas 2, 3 and 4 are the basic
properties of the Kronecker product. Their proofs
can be found at (Laub, 2005).

Lemma 1. Let V be an orthogonal matrix and
Σ a diagonal matrix. Then (I + VΣVT )−1 =
V(I + Σ)−1VT .
Proof.

(I + VΣVT )−1 = (VIVT + VΣVT )−1

= V(I + Σ)−1VT

Lemma 2. (A⊗B)(C⊗D) = AC⊗BD.
Lemma 3. (A⊗B)T = AT ⊗BT .
Lemma 4. If A and B are orthogonal matrices,
then A⊗B will also be an orthogonal matrix.

Let Z = A ⊗ A and apply singular value
decomposition to A = UΣVT . The term(
ZT Z + λI

)−1 can be rewritten as:
(
ZT Z + λI

)−1
=
(
λI + (AT ⊗AT )(A⊗A))−1 (6)

=
(
λI + AT A⊗AT A)−1 (7)

=
(
λI + VΣ2VT ⊗VΣ2VT )−1 (8)

=
(
λI + (V ⊗V)(Σ2 ⊗Σ2)(V ⊗V)T )−1

(9)

= (V ⊗V) (λI + Σ2 ⊗Σ2)−1 (V ⊗V)T
(10)

Eq. (6) is from replacing Z with A ⊗ A and
Lemma 3. Eq. (7) is from Lemma 2. Eq. (8) is
from the properties of SVD, where U and V are
orthonormal matrices. Eq. (9) is from Lemma 2
and Lemma 3. Finally, Eq. (10) comes from
Lemma 1.

1578



Figure 3: Prediction performance of TRESCAL
and RESCAL with different rank (r).

Figure 4: Prediction performance of TRESCAL
with different regularization parameter (λ).

Appendix B Hyper-parameter Sensitivity

We study if TRESCAL is sensitive to the rank
parameter r and the regularization parameter λ.
We use the task of relation retrieval and present
the model performance on the development set.
Fig. 3 shows the performance of TRESCAL and
RESCAL with different rank (r) values while fix-
ing λ = 0.01. Results show that both TRESCAL
and RESCAL achieve better performance when r
is reasonably large. TRESCAL obtains a bet-
ter model with smaller r than RESCAL, because
TRESCAL only needs to fit the triples of the com-
patible entity types. Therefore, it allows to use
smaller number of latent variables to fit the train-
ing data.

Fixing r = 400, Fig. 4 shows the performance
of TRESCAL at different values of the regulariza-
tion parameter λ, including no regularization at
all (λ = 0). While the results suggest that the
method is not very sensitive to λ, tuning λ is still
necessary for achieving the best performance.

1579


