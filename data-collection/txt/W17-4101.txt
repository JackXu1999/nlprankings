



















































Character and Subword-Based Word Representation for Neural Language Modeling Prediction


Proceedings of the First Workshop on Subword and Character Level Models in NLP, pages 1–13,
Copenhagen, Denmark, September 7, 2017. c©2017 Association for Computational Linguistics.

Character and Subword-Based word Representation for Neural Language
Modeling prediction

Matthieu Labeau
LIMSI-CNRS / Orsay, France

labeau@limsi.fr

Alexandre Allauzen
LIMSI-CNRS / Orsay, France
allauzen@limsi.fr

Abstract

Most of neural language models use dif-
ferent kinds of embeddings for word pre-
diction. While word embeddings can be
associated to each word in the vocabulary
or derived from characters as well as fac-
tored morphological decomposition, these
word representations are mainly used to
parametrize the input, i.e. the context of
prediction. This work investigates the ef-
fect of using subword units (character and
factored morphological decomposition) to
build output representations for neural lan-
guage modeling. We present a case study
on Czech, a morphologically-rich lan-
guage, experimenting with different input
and output representations. When work-
ing with the full training vocabulary, de-
spite unstable training, our experiments
show that augmenting the output word rep-
resentations with character-based embed-
dings can significantly improve the perfor-
mance of the model. Moreover, reducing
the size of the output look-up table, to let
the character-based embeddings represent
rare words, brings further improvement.

1 Introduction

Most of neural language models, such as n-gram
models (Bengio et al., 2003) are word based and
rely on the definition of a finite vocabulary V .
Therefore, a look-up table maps each wordw ∈ V
to a vector of real features, and is stored in a ma-
trix. While this approach yields significant im-
provement for a variety of tasks and languages,
see for instance (Schwenk, 2007) in speech recog-
nition and (Le et al., 2012; Devlin et al., 2014;
Bahdanau et al., 2014) in machine translation, it
induces several limitations.

For morphologically-rich languages, like Czech
or German, the lexical coverage is still an impor-
tant issue, since there is a combinatorial explosion
of word forms, most of which are hardly observed
on training data. On the one hand, growing the
look-up table is not a solution, since it would in-
crease the number of parameters without having
enough training examples for a proper estimation.
On the other hand, rare words can be replaced by
a special token. This acts as a word class merg-
ing very different words without any distinction,
while using different word classes to handle out-
of-vocabulary words (OOVs) (Allauzen and Gau-
vain, 2005) does not really solve this issue, since
rare words are difficult to classify. Moreover, for
most inflected or agglutinative forms, as well as
for compound words, the word structure is over-
looked, wasting parameters for modeling forms
that could be more efficiently handled by word de-
composition into subwords units.

Using subword units, whether they are built via
a different supervised method with embedded lan-
guage knowledge, or from the training data, has
been attempted many times, especially for speech
recognition. The main goal is to reduce the OOV
rate. While most of them were focused on a spe-
cific language, (Creutz et al., 2007) is a represen-
tative example of such a model applied to several
morphologically-rich languages.

One of the first occurrences of general lan-
guage models integrating morphological features
to represent words are the factored language
model (Bilmes and Kirchhoff, 2003) and its neu-
ral version (Alexandrescu and Kirchhoff, 2006).
Input words are represented by their embedding,
plus several other features, some of which include
morphemes. To alleviate the impact of OOVs,
(Mueller and Schuetze, 2011) used morphologi-
cal features for class-based predictions when in-
put words are unknown, obtaining state-of-the-art

1



results on English. More recently, several types
of language models represent words as function of
subwords units: using a recursive structure (Lu-
ong et al., 2013), or an additive one (Botha and
Blunsom, 2014). Quite a lot of work has been
made on language models that extract features di-
rectly from the character sequence, whether they
use character n-grams (Sperr et al., 2013), or char-
acters composed by a convolutional layer (Santos
and Zadrozny, 2014; Kim et al., 2015) or a Bi-
LSTM layer (Ling et al., 2015). This avoids us-
ing an external morphological analyser. We can
note that these types of models have also been ap-
plied with success to several other task, including
learning word representations (Qiu et al., 2014;
Cotterell et al., 2016; Bojanowski et al., 2016;
Wieting et al., 2016), POS tagging (Plank et al.,
2016; Ma and Hovy, 2016; Heigold et al., 2017),
Named entity recognition (Gillick et al., 2016),
Parsing (Ballesteros et al., 2015) and Machine
translation (Costa-jussà and Fonollosa, 2016). Re-
cently, an exhaustive summary of previous work
on word representation by composing subword
units was presented in (Vania and Lopez, 2017).
This work also compares the types of subword
unit, how they are composed, and their impact on
various morphological typologies.

While recurrent neural networks have shown
excellent performances for character-level lan-
guage modeling (Sutskever et al., 2011; Hermans
and Schrauwen, 2013), the results of such models
are usually worse than those that use word-level
prediction, since they have to consider a far longer
history of tokens to be able to predict the next one
correctly. However, more recent work (Hwang and
Sung, 2017) seems to obtain very satisfactory re-
sults with a supplementary word-level layer that
allows a better processing of the longer history.

Our work focuses on replacing output word
embeddings by representations built from sub-
words. To the best of our knowledge, such a model
has only been proposed in (Józefowicz et al.,
2016), which evaluates the use of convolutional
and LSTM layers to build word representations
for outputs words. They allow the model to trade
size against perplexity, since their model performs
worse than the classic softmax approach, but with
far less parameters. We first propose to study the
training of a language model which augments or
completely replaces output words representations
with character-based representations. We compare

the effect of different architectures, as well as the
effect of different input representations. Our re-
sults show that:

• When evaluating perplexity on the full training
vocabulary, using an augmented output repre-
sentation improves the model performance.

• Not using the look-up table for rare words also
improves the model performance.

Finally, we describe a short experiment with fac-
toring the output predictions using a morphologi-
cal analysis, which we believe could lead to a fa-
cilitated word generation when combined with re-
inflexion models.

Our paper is organized as follows: Section 2
describes the general architecture of the language
model, and of the representations used, as well as
its training, Section 3 presents the experiments and
Section 4 gives our results and discussion.

2 Language model

We use a recurrent neural language
model (Mikolov et al., 2010). The input of the net-
work is a sequence of words S = (w1, . . . , w|S|).
Given a fixed sized vocabulary V , the lan-
guage model outputs a multinomial distribution
P (wi = j|wi−11 ), ∀j ∈ V for each position i in
the sequence, and with the prediction contexte
wi−11 = w1, . . . , wi−1. This allows us to compute
the following probability :

P (w1, . . . , w|S|) =
|S|∏
i=1

P (wi|wi−11 )

Our model uses the LSTM variant (Hochreiter
and Schmidhuber, 1997). The hidden state hi will
be computed using the previous hidden state and a
computed representation rwi of the word in posi-
tion i in the sequence:

hi = LSTM(rwi ,hi−1)

The conditional probability distribution of the
next word is computed with a softmax function:

P (wi = j|wi−11 ) =
exp

(
hiroutj + bj

)
∑
k∈V

exp
(
hiroutk + bk

) (1)

2



We propose to improve output word embed-
dings by using representations built from sub-
words, as it is often done for input words.

Usually, input and output word embeddings are
parameters, stored in look-up matrices W and
Wout. The word embedding rwordw of a word w
is simply the column of W corresponding to its
index in the vocabulary V:

rwordw = [W]w

2.1 Representing words

We consider two other types of representations:
decomposition of the words into characters (or n-
grams of characters), and decomposing them into
a Lemma and positional tags using a morpholog-
ical analysis. An example of these different de-
compositions is shown in table 1.

Representation Decomposition

Word poc̆átku
Characters p+o+c̆+á+t+k+u
Character 3-grams poc̆+oc̆á+c̆át+átk+tku
Lemma + Tags poc̆átek+N+MascIn+Sg+Loc+Act

Table 1: Example of subword decompositions
used for Czech word

2.1.1 Character-based representations
A word w is a character sequence {c1, .., c|w|}
represented by their embeddings {rcharc1 , .., rcharc|w| },
where rcharci = [C]ci denotes the vector associated
to the character ci. To infer a word embedding
from its character embeddings, we use two differ-
ent architectures:

First, a convolution layer (Waibel et al., 1990;
Collobert et al., 2011), similar to layers used
in (Santos and Zadrozny, 2014; Kim et al., 2015),
applies a convolution filter WCNNnc over a sliding
window of nc characters, producing local features:

xnnc = W
CNN
nc (r

char
cn−nc+1 : .. : r

char
cn )

T + bCNNnc

where xnnc is a vector obtained for each posi-
tion n in the word. The embeddings of w is then
obtained by applying a max-pooling and the acti-
vation function φ:

[rncw ]i = φ
(
|w|−nc+1

max
n=1

[xnnc ]i

)
(2)

We can use multiple filters of ncf different sizes
and concatenate their results:

rCharCNNw = (r
nc1
w : . . . : r

ncf
w ) (3)

Our second method uses a bi-LSTM (Hochreiter
and Schmidhuber, 1997; Graves et al., 2005), on
characters, similarly to (Ling et al., 2015). It com-
bines the final states

−−→
h|w| and

←−
h1 of two LSTMs,

respectively over the character sequence and the
reverse character sequence, which are computed
as such:

−→
hi = LSTM(rcharci ,

−−→
hi−1)

←−
hj = LSTM(rcharcj ,

←−−
hj+1)

rCharBiLSTMw =
−−→
h|w| :

←−
h1 (4)

2.1.2 Lemma+Tags decomposition
For morphologically-rich languages, the different
morphological properties of a word (gender, case,
... ) are usually encoded using mutliple tags as
shown in table 1. Therefore a word w is decom-
posed into a lemma l along with a set of associ-
ated sub-tags T = {t1, .., t|T |} of fixed size |T |.
For a given word, a single tag can be simply cre-
ated by the concatenation of the subtags. How-
ever, this implies a large tagset and mitigates the
generalization power since some sub-tags combi-
nations can remain unobserved on training data.
In this work we prefer a factored representation
where each sub-tags is considered independently.

Lemmas, similarly to surface forms, are repre-
sented by |VL| vectors stored in a look-up matrix
L, and rlemmal = [L]l. For every words, each sub-
tag has its own vocabulary and its own look-up
matrix. However, the additional cost is negligi-
ble given their small size (see table 3). To infer a
word embedding from a sub-tags set, we also use
two methods. First, we simply concatenate their
embeddings:

rTagConcatT = r
tag1
t1

: . . . : rtagiti : . . . : r
tag|T |
t|T | (5)

The second method uses a bidirectionnal LSTM
on the sequence of tags T , using exactly the same
structure as in section 2.1.1:

rTagBiLSTMT =
−−→
h|T | :

←−
h1 (6)

3



Highway
Layer

LSTM

Softmax

’to’ ’reálné’ ’?’

Look-up table
C

Max-Pooling

WCNN3Look-up table
W

ewordw e
CharCNN
w

Words + CharCNN
for all words in V

’je’ ’to’ ’reálné’

w c1 c2 c3 c4 c5

Words +
CharCNN

Figure 1: Example architecture of our language model, when using word embeddings and a character
CNN to build both input and output word representations.

2.2 Training

Our final model, as illustrated in figure 1, uses
concatenation of word, character-based or lemma
and tags embeddings, to obtain input and out-
put word representations. Following (Kim et al.,
2015), we used a Highway layer (Srivastava et al.,
2015) to model interactions between concatenated
embeddings of various sources.

Usually, such a model is trained by maximizing
the log-likelihood. For a given word wi given its
preceding sequence w1, . . . , wi−1, the model pa-
rameters θ are estimated in order to maximize the
following function for all the sequences observed
in the training data:

LL(θ) =
|S|∑
i=1

logPθ(wi|wi−11 ) (7)

This objective function implies a very costly
summation imposed by the softmax activation of
the output layer: large output vocabularies cause
a computational bottleneck due to the output nor-
malization.

Different solutions have been proposed, as
shortlists (Schwenk, 2007), hierarchical soft-
max (Morin and Bengio, 2005; Mnih and Hinton,

2009; Le et al., 2011), or self-normalisation tech-
niques (Devlin et al., 2014; Andreas et al., 2015;
Chen et al., 2016). Sampling-based techniques ex-
plore a different solution, where a limited number
of negative examples are sampled to reduce the
normalization cost. Working with a large vocab-
ulary, and with output representations potentially
more costly to compute, we choose to use the fol-
lowing sampling-based training algorithms:

• Target sampling, which is based on importance
sampling (Bengio and Sénécal, 2008; Jean et al.,
2015), directly approximates the normalization
over V by normalizing over a sampled subset.
Indeed, the gradient of the objective described
in equation 7 is written as:

∂

∂θ
logPθ(wi|wi−11 ) =

∂

∂θ
(hiroutwi + bwi)

− Ew∼Pθ(.|wi−11 )
[
∂

∂θ
(hiroutwi + bwi)

]
(8)

The idea is to approximate the expectation of
the second term by importance sampling a sub-
set of V from a proposal distribution Q. Tar-
get sampling implies associating with a part

4



Di of the training data a subset Vi of V that
corresponds to the target words of Di plus a
small subset of the remaining words. The re-
sulting objective is equivalent to approximating
the probability computed in equation 1 by nor-
malizing it only over Vi.
• Noise contrastive estimation (NCE), introduced

in (Gutmann and Hyvärinen, 2012; Mnih and
Teh, 2012), aims to discriminate between one
example sampled from the real data D and k
from a noise distribution Pn, and results in the
model being theoretically unnormalized. The
idea is to sample examples according to a mix-
ture:

P (w|wi−11 ) =
1

k + 1
PD(w|wi−11 )

+
k

k + 1
Pn(w|wi−11 )

(9)

and train the model to recover whether the sam-
ple came from the data or the noise distribution.
This is done by minimizing the binary cross-
entropy of recognizing the current sample’s ori-
gin, using the posterior probabilities:

P (w ∼ PD|w,wi−11 ) =
Pθ(w|wi−11 )

Pθ(w|wi−11 ) + kPn(w|wi−11 )
(10)

P (w ∼ Pn|w,wi−11 ) = 1
− P (w ∼ PD|w,wi−11 )

(11)

Besides, the probabilities intervening in equa-
tion 10 can be replaced by unnormalized scores
at training time, since we can consider normal-
izing quantities as parameters to be learned.

• BlackOut (Ji et al., 2015), also approximating
the normalization computation, with a weighted
sampling scheme and a discriminative objec-
tive.It can be considered as a variant from NCE
where we sample a set of k examples Sk from a
proposal distributionQ. We then proceed to ap-
ply NCE with a re-weighted noise distribution

Pn(w|wi−11 ) =
1
k

∑
wj∈Sk

Q(wj)
Q(w) Pθ(ww|w

i−1
1 )

(12)
which empirically behaves far better than NCE,
providing an improved stability. BlackOut can
also be linked to Importance sampling.

Ultimately, these three algorithms approximate
the negative log-likelihood computed on a number
k of negative samples from V , using an easy to
sample distribution.

3 Experiments

Experiments are carried out on Czech, a morpho-
logically rich language using the different criteria
described in section 2.2.

3.1 Data
We used data from the parallel corpus News-
commentary 2015, from the WMT News MT
Task. The data consists in 210K word sequences,
amounting in about 4,7M tokens. We divided the
data into a training, development and testing sets,
these last two amounting to 150K tokens each. In
our experiments, we use different vocabulary sizes
by varying the frequency threshold: words are se-
lected when their frequency in the training data are
stricly higher than the threshold. Table 2 shows
the correspondences between vocabulary sizes and
these thresholds.

fTh |VTh|
0 (All words) 159142

1 66743
5 37010
10 25295

Table 2: Vocabulary sizes for different frequency
thresholds

The lemma and tags decomposition pre-
sented in section 2.1.2 were obtained with Mor-
phodita (Straková et al., 2014). There is 12 tag
categories for Czech. Vocabulary sizes for charac-
ters, lemma and tags are detailed in table 3.

|VC | |VL| |Vtagi |i=1..|T |
155 61364 [12, 65, 11, 6, 9, 6, 3, 5, 5, 4, 3, 3]

Table 3: Vocabulary sizes for subword units

3.2 Setup
The different versions of our model used in ex-
periments are shown in table 4. We used a High-
way layer when there is a concatenation of em-
beddings of different sources, which is for almost
all architectures. We tried applying a Highway
layer to the output representation, but it seemed

5



almost always counter-productive, rendering train-
ing more unstable. In all experiments presented
here, weights are not tied between input and output
representations, since our preliminary experiments
with tied weights always gave worst results. Be-
sides, we didn’t mix structures for character-level
representations (for example, using an input Char-
CNN and output CharLSTM) since our first ex-
periments gave systematically worse results than
using the same structures). When using different
types of representations, we kept consistency be-
tween vocabularies: if both lemmas and words are
used in a model, any lemma considered unknown
will have its corresponding word unknown, and in-
versely. The same (or corresponding) vocabularies
are used for inputs, outputs, and evaluation. The
only exception is presented in section 4.4. When
using a character-based output representation, dur-
ing evaluation, the unknown token is built from a
specific character token, a specific lemma token,
and 12 specific tag tokens that are parameters of
the model.

Input representation rw Eq

Words rwordw
CharCNN Hw(rCharCNNw ) 3
CharBiLSTM Hw(rCharBiLSTMw ) 4
Words + CharCNN Hw(rwordw : r

CharCNN
w )

Words + CharBiLSTM Hw(rwordw : r
CharBiLSTM
w )

Lemma + Tags Concat. Hw(rlemmal : r
TagConcat
T ) 5

Lemma + TagsBiLSTM Hw(rlemmal : r
TagBiLSTM
T ) 6

Output representation routw
Words rwordw
Words + CharCNN rwordw : r

CharCNN
w

Words + CharBiLSTM rwordw : r
CharBiLSTM
w

Lemmas rlemmal
Lemmas + CharCNN rlemmal : r

CharCNN
l

Lemmas + CharLSTM rlemmal : r
BiLSTM
l

Table 4: Detail of input and output representations
used in our experiments. Hw designate the use of
a Highway layer

Our experiments aim at comparing potential use
of subword-based word representation, and thus
are not directed towards performance. For this rea-
son, we used the same implementation for all ex-
periments and did not specifically try to optimize
the general model structure or the dimensional hy-
perparameters, neither compared our results with
benchmarks on Czech corpora.

3.3 Training and evaluation

Language models are evaluated with perplexity:

PPL = exp

 |S|∑
i=1

− logPθ(wi|wi−11 )
|S|


over all sequences in the testing data. Perplex-

ity is computed for a fixed output vocabulary V ,
which allows to compare models using the same
output vocabulary. However, we can’t evaluate
model performance on out-of-vocabulary words,
since those are to be classified as the unknown to-
ken in V .

Our models are implemented with Tensor-
flow (Abadi et al., 2015). We use the Adam algo-
rithm (Kingma and Ba, 2014) with an initial learn-
ing rate of 5 ∗ 10−4 for training, over a maximum
of 10 epochs, with a batch size of 128 sequences.
However, since the training is often unstable, the
model backtracks to the last checkpoint if it does
not improve its performance on validation data af-
ter 1/10 of an epoch, and stop training after 10 un-
successful loadings in a row. To avoid overfitting,
we use dropout with probability 0.5 on recurrent
layers, and L2 regularization on feedforward lay-
ers.

We use two hidden layers, and choose our em-
beddings dimensions in order to obtain, for each
type of representation, an embedding dimension
of 150. In the case of the CNN, we used filters
of 3, 5 and 7 characters, of dimension 30, 50, and
70. Whether we use NCE, blackOut, or impor-
tance sampling, we draw k = 500 noise samples
by batch. For all experiments, we report the per-
plexity on test data at the end of training. Results
presented in tables 5, 6, 7 are the average of the
results obtained on 5 models, and the standard de-
viation.

4 Results

4.1 Influence of the vocabulary size
We first train our model with different vocabulary
sizes. As shown in figure 2, our model fails to im-
prove upon the conventional word model when the
output vocabulary size is relatively small (shown
on the two leftmost graphs). More precisely, mod-
els that use word and character-based representa-
tions at the output seem unable to learn after a
couple of iterations. We first link this behaviour
to the difficulty met by the authors in (Józefowicz
et al., 2016): since most logits are tied when we
use an output character-based representation - as

6



0 1 2 3 4
fTh = 10

200

400

600

800

1000

Te
st

 p
er

pl
ex

ity

0 1 2 3 4
fTh = 5

0 1 2 3 4
fTh = 1

Words
Words+CharCNN

0 1 2 3 4
fTh = 0

Figure 2: Test perplexities obtained when training models using Words as input representation and
Words+CharCNN as output representations, for various vocabulary sizes. Corresponding vocabulary
sizes are given in table 2. The models are trained with target sampling.

opposed to independently learned word embed-
dings, the function mapping from word to word
representation is smoother and training becomes
more difficult. They used a smaller learning rate
and a low dimensional correction factor, learned
for each word, as a work-around.

However, increasing the vocabulary size re-
duces this effect . This is especially clear with the
whole training vocabulary (on the rightmost graph
of figure 2): in this setup, using a character-based
representation improves the performance of the
model. We can assume that, for rare words, learn-
ing independent embeddings fails since scarce up-
dates of these embeddings are insufficient. For the
rare words, combining word and character-based
embeddings allows the model to better counteract
the sparsity issue.

4.2 Choice of the training criterion

Given the previous results, we use the full train-
ing vocabulary to assess the impact of the train-
ing criterion. However, using this full training
vocabulary renders training very unstable, espe-
cially with sampling-based algorithms. Stability
issues, especially for the Noise-contrastive estima-
tion, have previously been discussed (Chen et al.,
2016; Józefowicz et al., 2016). We shortly ex-
perimented to choose the most practical criterion
to use. Figure 3 shows the shape of the training
curves. While target sampling and blackOut both
seem to work properly, NCE needs far more noise
samples to converge. We believe this is related
to the tensorflow implementation, which re-use
the same noise samples for every example in the
batch, which leads to a lack of diversity in negative

0 1 2 3 4

Test perplexity

12K

20K

28K

NC
E

0 1 2 3 4
400

800

1200

1600

2000

Bl
ac

kO
ut

0 1 2 3 4
400

800

1200

1600

2000

Im
po

rt
an

ce
 S

am
pl

in
g

Words
Words+CharCNN

Figure 3: Test perplexities obtained when train-
ing models using Words as input representation
and Words+CharCNN as output representations,
for various training methods: Noise contrastive es-
timation, blackOut and Target sampling.

7



examples. Augmenting the number of samples or
reducing the size of the batch are possible solu-
tions, but they increase the training time. Black-
Out obtains better results because, while very sim-
ilar to NCE, the scores used as coming from the
noise distribution are context-dependent, which
brings diversity to negative examples.

Overall, across several experiments, target sam-
pling performs better than blackOut, and we
choose to use it for the rest of our experiments.
Since training is still quite unstable, depending on
the architecture, we report results across 5 train-
ings for the next sections.

Output Representation
Words

Words + Char

Input Representation CNN BiLSTM

Words 563 ± 53 432 ± 18 480 ± 31

Char
CNN 698 ± 41 543 ± 16 -
BiLSTM 971 ± 24 - 938 ± 48

Words +
Char

CNN 495 ± 34 411 ± 40 -
BiLSTM 537 ± 24 - 480 ± 21

Lemmas +
Tags

Concat. 521 ± 47 424 ± 22 502 ± 54
BiLSTM 541 ± 8 445 ± 24 496 ± 39

Table 5: Average test perplexities obtained when
training 5 models with target sampling, for various
input/output representations. Results in bold are
the best models for a given output representation.

4.3 Effects of the representation choice
Table 5 gathers the main experimental results to
assess which combination of input and output rep-
resentations gives the best performance. For any
input representation, augmenting the output rep-
resentation with a character-based embedding im-
proves the performance of the model. It is espe-
cially true for convolutional layers. We also can
notice that the improvement is better for models
that performed badly with basic output word em-
beddings.

Overall, biLSTMs perform worse than their
convolution/concatenation counterparts. Finally,
the best average perplexity of 495 for word only
output representations is improved to an average
of 411 for augmented output representations.

Other output representations: First, our ex-
periments with only character-based embeddings
as output representations give results far worse
than those reported in 5, with our best model ob-
taining an average perplexity of ≈ 2500. Train-

ing is also far more unstable. We believe these
results are linked to the difficulties mentioned
in (Józefowicz et al., 2016) and in section 4.1.

We also tried to use the lemma+tags decompo-
sition presented in section 2.1.2, but without suc-
cess. When tags were ambiguous across several
occurrences of the same words, we tried using spe-
cific tokens, or choosing the most frequent tags,
but in both cases the model severely overfits.

Finally, we tried to use word embeddings pre-
trained with word2vec (Mikolov et al., 2013) as
output representations. We obtained results very
similar to those of classical word embeddings,
with a small but noticeable improvement when the
input representation used LSTM. However, these
improvements are still well under those obtained
by augmenting the output representation with a
character-based embedding.

4.4 Influence of the size of the word
embeddings vocabulary

Input
Representation

fWTh
Words + CharCNN

All words Frequent words Rare words

Words

0 432 ± 18 286 ± 13 5170 ± 1310
1 415 ± 26 258 ± 13 6510 ± 780
5 390 ± 20 250 ± 14 7210 ± 1290
10 416 ± 20 265 ± 11 8100 ± 580

CharCNN

0 543 ± 16 348 ± 10 6400 ± 1480
1 523 ± 17 328 ± 22 5070 ± 1080
5 478 ± 16 316 ± 27 7000 ± 1800
10 488 ± 31 338 ± 25 8210 ± 2160

Words +
CharCNN

0 411 ± 40 271 ± 30 4470 ± 190
1 374 ± 10 242 ± 7 5020 ± 870
5 367 ± 14 241 ± 8 5560 ± 1220
10 393 ± 19 254 ± 13 6600 ± 1910

Lemma +
TagsConcat.

All 449 ± 26 293 ± 16 5830 ± 760
1 439 ± 34 287 ± 11 6220 ± 1080
5 408± 32 269 ± 20 4600 ± 1280
10 430 ± 32 269 ± 20 8410 ± 1140

Lemma +
TagsBiLSTM

All 445 ± 24 288 ± 13 7150 ± 1560
1 424 ± 34 281 ± 21 5380 ± 1000
5 387± 9 258 ± 5 4300 ± 1390
10 442 ± 17 287 ± 13 7390 ± 1690

Table 6: Test perplexity averaged on 5 models
trained with target sampling, for various input rep-
resentations and output word look-up table sizes.
Corresponding vocabulary sizes are given in ta-
ble 2. Test perplexities are given for all words,
frequent words (frequency > 10) and rare words
(frequency < 10). In bold are the best models for
a given input representation.

8



Following our observations in section 4.1, we
then assess the effect of reducing the word vo-
cabulary size for Words+CharCNN output repre-
sentation. We don’t change the size of the event
space: when constructing output representations
for words under a chosen frequency, we simply
don’t use the word representation. For example,
using a threshold of fWTh = 10 means that words
that appear less thant ten times won’t have their
own word embedding, and will be represented
by the unknown word token combined with their
character-based representation. Results are shown
in table 6. We can see that for all input representa-
tions, using a specific unknown token in place of a
specific word embedding for words appearing less
than 5 times in training data gives the best perfor-
mance. Reducing the look-up table to words only
appearing more than 10 times gives worse results,
while they are still better than if we keep the full
table. However, there is no clear trend when look-
ing at the rare words perplexities, which are very
hard to interpret, given their very high standard de-
viation. With a smaller output word look-up table,
our best average perplexity of 411 is reduced to
376, which is a very sizeable overall improvement.

Output Representation
Lemmas

Lemmas + Char

Input Representation CNN BiLSTM

Words 240 ± 12 220 ± 9 222 ± 12

Char
CNNN 308 ± 15 270 ± 11 -
BiLSTM 477 ± 17 - 429 ± 9

Words +
Char

CNN 234 ± 9 203 ± 7 -
BiLSTM 238 ± 6 - 225 ± 11

Lemmas +
Tags

Concat. 239 ± 3 211 ± 5 217 ± 9
BiLSTM 232 ± 5 203 ± 6 212 ± 6

Table 7: Test perplexities averaged on 5 models on
lemmas with a multiple objectives cost function.
Results are given for various input/output repre-
sentations. In bold are the best models for a given
output representation.

4.5 Predicting root and tags jointly
While using the lemma+tags decomposition to
build output representation was not, in our ex-
periments, successful, we investigated a factorised
prediction of lemma and tags. We used different
costs for predicting lemmas and each tag, which
are summed into a final objective function. As re-
cently seen in (Martinez et al., 2016; Burlot and

Yvon, 2017), these objectives are individually eas-
ier when working with morphologically-rich lan-
guages, and fully inflected words can be obtained
by using morphological inflection models, which
have been shown to be quite successful (Faruqui
et al., 2016; Kann et al., 2017).

Table 7 shows the test perplexities on lemmas
for various input and output representations. We
can observe that in all cases training is far more
stable, with generally lower standard deviations.
In this case, using a lemma+tags with a BiLSTM
or a Words+CharCNN input representation both
give the best results, while augmenting the output
representation of the lemma with a character-build
embedding also improves results. This makes the
joint learning of a factored prediction and reinflec-
tion language model a very interesting direction
for future work.

5 Conclusion

We described a neural language model allowing
the use of subword units for both input and output
word representations. While in our experiments
training with a full vocabulary is unstable, we can
identify important trends: augmenting output rep-
resentations with character-based embeddings im-
proves the model performance, and in this setup,
replacing independent word embeddings by the
unknown token for rare words yields further im-
provement. It is worth noticing that this also opens
the vocabulary, since our model can be used to
rescore unknown words. Additional experiments
suggest that factoring the output of the model with
a lemma+tags decomposition, then re-inflecting
these into words, could make generation easier:
this is a direction we plan to investigate.

Acknowledgements

We wish to thank the anonymous reviewers for
their helpful comments. This work has been
funded by the European Union’s Horizon 2020
research and innovation programme under grant
agreement No. 645452 (QT21).

References
Martı́n Abadi, Ashish Agarwal, Paul Barham, Eugene

Brevdo, Zhifeng Chen, Craig Citro, Greg S. Cor-
rado, Andy Davis, Jeffrey Dean, Matthieu Devin,
Sanjay Ghemawat, Ian Goodfellow, Andrew Harp,
Geoffrey Irving, Michael Isard, Yangqing Jia, Rafal
Jozefowicz, Lukasz Kaiser, Manjunath Kudlur, Josh

9



Levenberg, Dan Mané, Rajat Monga, Sherry Moore,
Derek Murray, Chris Olah, Mike Schuster, Jonathon
Shlens, Benoit Steiner, Ilya Sutskever, Kunal Tal-
war, Paul Tucker, Vincent Vanhoucke, Vijay Va-
sudevan, Fernanda Viégas, Oriol Vinyals, Pete
Warden, Martin Wattenberg, Martin Wicke, Yuan
Yu, and Xiaoqiang Zheng. 2015. TensorFlow:
Large-scale machine learning on heterogeneous sys-
tems. Software available from tensorflow.org.
http://tensorflow.org/.

Andrei Alexandrescu and Katrin Kirchhoff. 2006.
Factored neural language models. In Pro-
ceedings of the Human Language Technology
Conference of the NAACL, Companion Volume:
Short Papers. Association for Computational
Linguistics, New York City, USA, pages 1–4.
http://www.aclweb.org/anthology/N/N06/N06-
2001.

A. Allauzen and J.L Gauvain. 2005. Open vocabulary
asr for audiovisual document indexation. In IEEE
International Conference on Acoustics, Speech and
Signal Processing (ICASSP).

Jacob Andreas, Maxim Rabinovich, Michael I. Jor-
dan, and Dan Klein. 2015. On the accu-
racy of self-normalized log-linear models. In
Advances in Neural Information Processing Sys-
tems 28: Annual Conference on Neural Infor-
mation Processing Systems 2015, December 7-
12, 2015, Montreal, Quebec, Canada. pages
1783–1791. http://papers.nips.cc/paper/5806-on-
the-accuracy-of-self-normalized-log-linear-models.

Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua
Bengio. 2014. Neural machine translation by
jointly learning to align and translate. CoRR
abs/1409.0473.

Miguel Ballesteros, Chris Dyer, and Noah A. Smith.
2015. Improved transition-based parsing by model-
ing characters instead of words with lstms. In Pro-
ceedings of the 2015 Conference on Empirical Meth-
ods in Natural Language Processing. Association
for Computational Linguistics, Lisbon, Portugal,
pages 349–359. http://aclweb.org/anthology/D15-
1041.

Yoshua Bengio, Réjean Ducharme, Pascal Vincent, and
Christian Janvin. 2003. A neural probabilistic lan-
guage model. Journal of Machine Learning Re-
search 3:1137–1155.

Yoshua Bengio and Jean-Sébastien Sénécal. 2008.
Adaptive importance sampling to accelerate train-
ing of a neural probabilistic language model. IEEE
Trans. Neural Networks 19(4):713–722.

Jeff A. Bilmes and Katrin Kirchhoff. 2003. Factored
language models and generalized parallel backoff.
In Proceedings of the 2003 Conference of the North
American Chapter of the Association for Compu-
tational Linguistics on Human Language Technol-
ogy: Companion Volume of the Proceedings of

HLT-NAACL 2003–short Papers - Volume 2. As-
sociation for Computational Linguistics, Strouds-
burg, PA, USA, NAACL-Short ’03, pages 4–6.
https://doi.org/10.3115/1073483.1073485.

Piotr Bojanowski, Edouard Grave, Armand Joulin, and
Tomas Mikolov. 2016. Enriching word vectors
with subword information. CoRR abs/1607.04606.
http://arxiv.org/abs/1607.04606.

Jan A. Botha and Phil Blunsom. 2014. Composi-
tional Morphology for Word Representations and
Language Modelling. In Proceedings of the Inter-
national Conference of Machine Learning (ICML).
Beijing, China.

Franck Burlot and François Yvon. 2017. Learning
morphological normalization for translation from
and into morphologically rich language. The Prague
Bulletin of Mathematical Linguistics (Proc. EAMT)
(108):49–60.

Wenlin Chen, David Grangier, and Michael Auli. 2016.
Strategies for training large vocabulary neural lan-
guage models. In Proceedings of the 54th Annual
Meeting of the Association for Computational Lin-
guistics (Volume 1: Long Papers). Association for
Computational Linguistics, Berlin, Germany, pages
1975–1985. http://www.aclweb.org/anthology/P16-
1186.

Ronan Collobert, Jason Weston, Léon Bottou, Michael
Karlen, Koray Kavukcuoglu, and Pavel Kuksa.
2011. Natural language processing (almost)
from scratch. J. Mach. Learn. Res. 12:2493–2537.
http://dl.acm.org/citation.cfm?id=1953048.2078186.

Marta R. Costa-jussà and José A. R. Fonollosa. 2016.
Character-based neural machine translation. In
Proceedings of the 54th Annual Meeting of the
Association for Computational Linguistics (Vol-
ume 2: Short Papers). Association for Computa-
tional Linguistics, Berlin, Germany, pages 357–361.
http://anthology.aclweb.org/P16-2058.

Ryan Cotterell, Hinrich Schütze, and Jason Eis-
ner. 2016. Morphological smoothing and ex-
trapolation of word embeddings. In Proceed-
ings of the 54th Annual Meeting of the As-
sociation for Computational Linguistics (Volume
1: Long Papers). Association for Computational
Linguistics, Berlin, Germany, pages 1651–1660.
http://www.aclweb.org/anthology/P16-1156.

Mathias Creutz, Teemu Hirsimäki, Mikko Kurimo,
Antti Puurula, Janne Pylkkönen, Vesa Siivola, Matti
Varjokallio, Ebru Arisoy, Murat Saraçlar, and An-
dreas Stolcke. 2007. Morph-based speech recog-
nition and modeling of out-of-vocabulary words
across languages. ACM Trans. Speech Lang. Pro-
cess. 5(1):3:1–3:29.

Jacob Devlin, Rabih Zbib, Zhongqiang Huang, Thomas
Lamar, Richard Schwartz, and John Makhoul.

10



2014. Fast and robust neural network joint mod-
els for statistical machine translation. In Pro-
ceedings of the 52nd Annual Meeting of the Asso-
ciation for Computational Linguistics (Volume 1:
Long Papers). Association for Computational Lin-
guistics, Baltimore, Maryland, pages 1370–1380.
http://www.aclweb.org/anthology/P14-1129.

Manaal Faruqui, Yulia Tsvetkov, Graham Neubig, and
Chris Dyer. 2016. Morphological inflection gener-
ation using character sequence to sequence learn-
ing. In Proceedings of the 2016 Conference of
the North American Chapter of the Association
for Computational Linguistics: Human Language
Technologies. Association for Computational Lin-
guistics, San Diego, California, pages 634–643.
http://www.aclweb.org/anthology/N16-1077.

Dan Gillick, Cliff Brunk, Oriol Vinyals, and Amarnag
Subramanya. 2016. Multilingual language process-
ing from bytes. In Proceedings of the 2016 Confer-
ence of the North American Chapter of the Associ-
ation for Computational Linguistics: Human Lan-
guage Technologies. Association for Computational
Linguistics, San Diego, California, pages 1296–
1306. http://www.aclweb.org/anthology/N16-1155.

Alex Graves, Santiago Fernández, and Jürgen Schmid-
huber. 2005. Bidirectional LSTM networks for im-
proved phoneme classification and recognition. In
Artificial Neural Networks: Formal Models and
Their Applications - ICANN 2005, 15th Interna-
tional Conference, Warsaw, Poland, September 11-
15, 2005, Proceedings, Part II. pages 799–804.

Michael U. Gutmann and Aapo Hyvärinen. 2012.
Noise-contrastive estimation of unnormalized sta-
tistical models, with applications to natural image
statistics. J. Mach. Learn. Res. 13(1):307–361.

Georg Heigold, Guenter Neumann, and Josef van
Genabith. 2017. An extensive empirical eval-
uation of character-based morphological tagging
for 14 languages. In Proceedings of the 15th
Conference of the European Chapter of the As-
sociation for Computational Linguistics: Vol-
ume 1, Long Papers. Association for Computa-
tional Linguistics, Valencia, Spain, pages 505–513.
http://www.aclweb.org/anthology/E17-1048.

Michiel Hermans and Benjamin Schrauwen. 2013.
Training and analysing deep recurrent neural net-
works. In C. J. C. Burges, L. Bottou, M. Welling,
Z. Ghahramani, and K. Q. Weinberger, editors,
Advances in Neural Information Processing Sys-
tems 26, Curran Associates, Inc., pages 190–
198. http://papers.nips.cc/paper/5166-training-and-
analysing-deep-recurrent-neural-networks.pdf.

Sepp Hochreiter and Jürgen Schmidhuber. 1997. Long
short-term memory. Neural Comput. 9(8):1735–
1780. https://doi.org/10.1162/neco.1997.9.8.1735.

Kyuyeon Hwang and Wonyong Sung. 2017. Character-
level language modeling with hierarchical recur-

rent neural networks. In 2017 IEEE Inter-
national Conference on Acoustics, Speech and
Signal Processing, ICASSP 2017, New Orleans,
LA, USA, March 5-9, 2017. pages 5720–5724.
https://doi.org/10.1109/ICASSP.2017.7953252.

Sébastien Jean, Kyunghyun Cho, Roland Memisevic,
and Yoshua Bengio. 2015. On using very large
target vocabulary for neural machine translation.
In Proceedings of the 53rd Annual Meeting of the
Association for Computational Linguistics and the
7th International Joint Conference on Natural Lan-
guage Processing (Volume 1: Long Papers). Associ-
ation for Computational Linguistics, Beijing, China,
pages 1–10. http://www.aclweb.org/anthology/P15-
1001.

Shihao Ji, S. V. N. Vishwanathan, Nadathur Satish,
Michael J. Anderson, and Pradeep Dubey. 2015.
Blackout: Speeding up recurrent neural network lan-
guage models with very large vocabularies. CoRR
abs/1511.06909. http://arxiv.org/abs/1511.06909.

Rafal Józefowicz, Oriol Vinyals, Mike Schuster, Noam
Shazeer, and Yonghui Wu. 2016. Exploring the lim-
its of language modeling. CoRR abs/1602.02410.
http://arxiv.org/abs/1602.02410.

Katharina Kann, Ryan Cotterell, and Hinrich Schütze.
2017. Neural multi-source morphological re-
inflection. In Proceedings of the 15th Con-
ference of the European Chapter of the As-
sociation for Computational Linguistics: Vol-
ume 1, Long Papers. Association for Computa-
tional Linguistics, Valencia, Spain, pages 514–524.
http://www.aclweb.org/anthology/E17-1049.

Yoon Kim, Yacine Jernite, David Sontag, and Alexan-
der M Rush. 2015. Character-aware neural language
models. arXiv preprint arXiv:1508.06615 .

Diederik P. Kingma and Jimmy Ba. 2014. Adam:
A method for stochastic optimization. CoRR
abs/1412.6980. http://arxiv.org/abs/1412.6980.

Hai-Son Le, Alexandre Allauzen, and François Yvon.
2012. Continuous space translation models with
neural networks. In Proceedings of the 2012 Con-
ference of the North American Chapter of the As-
sociation for Computational Linguistics: Human
Language Technologies. Association for Computa-
tional Linguistics, Montréal, Canada, pages 39–48.
http://www.aclweb.org/anthology/N12-1005.

Hai-Son Le, Ilya Oparin, Alexandre Allauzen, Jean-
Luc Gauvain, and Francois Yvon. 2011. Struc-
tured output layer neural network language model.
In IEEE International Conference on Acoustics,
Speech and Signal Processing (ICASSP). Prague,
Czech Republic, pages 5524–5527.

Wang Ling, Chris Dyer, Alan W Black, Isabel Tran-
coso, Ramon Fermandez, Silvio Amir, Luis Marujo,
and Tiago Luis. 2015. Finding function in form:
Compositional character models for open vocabu-
lary word representation. In Proceedings of the

11



2015 Conference on Empirical Methods in Natu-
ral Language Processing. Association for Compu-
tational Linguistics, Lisbon, Portugal, pages 1520–
1530. http://aclweb.org/anthology/D15-1176.

Thang Luong, Richard Socher, and Christopher Man-
ning. 2013. Better word representations with recur-
sive neural networks for morphology. In Proceed-
ings of the Seventeenth Conference on Computa-
tional Natural Language Learning. Association for
Computational Linguistics, Sofia, Bulgaria, pages
104–113. http://www.aclweb.org/anthology/W13-
3512.

Xuezhe Ma and Eduard Hovy. 2016. End-to-end
sequence labeling via bi-directional lstm-cnns-crf.
In Proceedings of the 54th Annual Meeting of the
Association for Computational Linguistics (Volume
1: Long Papers). Association for Computational
Linguistics, Berlin, Germany, pages 1064–1074.
http://www.aclweb.org/anthology/P16-1101.

Mercedes Garcia Martinez, Loc Barrault, and Fethi
Bougares. 2016. Factored neural machine transla-
tion architectures. In International Workshop on
Spoken Language Translation (IWSLT’16). Seattle
(USA).

Tomas Mikolov, Martin Karafiát, Lukás Burget, Jan
Cernocký, and Sanjeev Khudanpur. 2010. Recur-
rent neural network based language model. In IN-
TERSPEECH 2010, 11th Annual Conference of the
International Speech Communication Association,
Makuhari, Chiba, Japan, September 26-30, 2010.
pages 1045–1048.

Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S
Corrado, and Jeff Dean. 2013. Distributed
representations of words and phrases and their
compositionality. In C. J. C. Burges, L. Bottou,
M. Welling, Z. Ghahramani, and K. Q. Wein-
berger, editors, Advances in Neural Information
Processing Systems 26. Curran Associates, Inc.,
pages 3111–3119. http://papers.nips.cc/paper/5021-
distributed-representations-of-words-and-phrases-
and-their-compositionality.pdf.

Andriy Mnih and Geoffrey E Hinton. 2009. A scal-
able hierarchical distributed language model. In
D. Koller, D. Schuurmans, Y. Bengio, and L. Bottou,
editors, Advances in Neural Information Processing
Systems 21, Curran Associates, Inc., pages 1081–
1088. http://papers.nips.cc/paper/3583-a-scalable-
hierarchical-distributed-language-model.pdf.

Andriy Mnih and Yee Whye Teh. 2012. A fast and sim-
ple algorithm for training neural probabilistic lan-
guage models. In ICML. icml.cc / Omnipress.

Frederic Morin and Yoshua Bengio. 2005. Hierarchi-
cal probabilistic neural network language model. In
Robert G. Cowell and Zoubin Ghahramani, editors,
Proceedings of the Tenth International Workshop
on Artificial Intelligence and Statistics. Society for
Artificial Intelligence and Statistics, pages 246–252.

http://www.iro.umontreal.ca/ lisa/pointeurs/hierarchical-
nnlm-aistats05.pdf.

Thomas Mueller and Hinrich Schuetze. 2011. Im-
proved modeling of out-of-vocabulary words us-
ing morphological classes. In Proceedings of the
49th Annual Meeting of the Association for Com-
putational Linguistics: Human Language Tech-
nologies. Association for Computational Linguis-
tics, Portland, Oregon, USA, pages 524–528.
http://www.aclweb.org/anthology/P11-2092.

Barbara Plank, Anders Søgaard, and Yoav Goldberg.
2016. Multilingual part-of-speech tagging with
bidirectional long short-term memory models and
auxiliary loss. In Proceedings of the 54th Annual
Meeting of the Association for Computational Lin-
guistics (Volume 2: Short Papers). Association for
Computational Linguistics, Berlin, Germany, pages
412–418. http://anthology.aclweb.org/P16-2067.

Siyu Qiu, Qing Cui, Jiang Bian, Bin Gao, and Tie-
Yan Liu. 2014. Co-learning of word representations
and morpheme representations. In Proceedings of
COLING 2014, the 25th International Conference
on Computational Linguistics: Technical Papers.
Dublin City University and Association for Com-
putational Linguistics, Dublin, Ireland, pages 141–
150. http://www.aclweb.org/anthology/C14-1015.

Cicero D. Santos and Bianca Zadrozny. 2014. Learning
character-level representations for part-of-speech
tagging. In Tony Jebara and Eric P. Xing, editors,
Proceedings of the 31st International Conference
on Machine Learning (ICML-14). JMLR Workshop
and Conference Proceedings, pages 1818–1826.
http://jmlr.org/proceedings/papers/v32/santos14.pdf.

Holger Schwenk. 2007. Continuous space language
models. Comput. Speech Lang. 21(3):492–518.

Henning Sperr, Jan Niehues, and Alex Waibel. 2013.
Letter n-gram-based input encoding for continu-
ous space language models. In Proceedings of
the Workshop on Continuous Vector Space Models
and their Compositionality. Association for Compu-
tational Linguistics, Sofia, Bulgaria, pages 30–39.
http://www.aclweb.org/anthology/W13-3204.

Rupesh Kumar Srivastava, Klaus Greff, and
Jürgen Schmidhuber. 2015. Training very
deep networks. CoRR abs/1507.06228.
http://arxiv.org/abs/1507.06228.

Jana Straková, Milan Straka, and Jan Hajič. 2014.
Open-source tools for morphology, lemmatiza-
tion, pos tagging and named entity recognition.
In Proceedings of 52nd Annual Meeting of the
Association for Computational Linguistics: Sys-
tem Demonstrations. Association for Computational
Linguistics, Baltimore, Maryland, pages 13–18.
http://www.aclweb.org/anthology/P14-5003.

Ilya Sutskever, James Martens, and Geoffrey Hinton.
2011. Generating text with recurrent neural net-

12



works. In Lise Getoor and Tobias Scheffer, edi-
tors, Proceedings of the 28th International Confer-
ence on Machine Learning (ICML-11). ACM, New
York, NY, USA, ICML ’11, pages 1017–1024.

Clara Vania and Adam Lopez. 2017. From char-
acters to words to in between: Do we cap-
ture morphology? CoRR abs/1704.08352.
http://arxiv.org/abs/1704.08352.

Alexander Waibel, Toshiyuki Hanazawa, Geofrey Hin-
ton, Kiyohiro Shikano, and Kevin J. Lang. 1990.
Readings in Speech Recognition, Morgan Kaufmann
Publishers Inc., San Francisco, CA, USA, chap-
ter Phoneme Recognition Using Time-delay Neural
Networks, pages 393–404.

John Wieting, Mohit Bansal, Kevin Gimpel, and Karen
Livescu. 2016. Charagram: Embedding words and
sentences via character n-grams. In Proceedings of
the 2016 Conference on Empirical Methods in Natu-
ral Language Processing. Association for Computa-
tional Linguistics, Austin, Texas, pages 1504–1515.
https://aclweb.org/anthology/D16-1157.

13


