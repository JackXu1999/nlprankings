



















































Multi-modal Summarization for Asynchronous Collection of Text, Image, Audio and Video


Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 1092–1102
Copenhagen, Denmark, September 7–11, 2017. c©2017 Association for Computational Linguistics

Multi-modal Summarization for Asynchronous Collection of Text, Image,
Audio and Video

Haoran Li1,2, Junnan Zhu1,2, Cong Ma1,2, Jiajun Zhang1,2 and Chengqing Zong1,2,3
1 National Laboratory of Pattern Recognition, CASIA, Beijing, China

2 University of Chinese Academy of Sciences, Beijing, China
3 CAS Center for Excellence in Brain Science and Intelligence Technology, Shanghai, China
{haoran.li, junnan.zhu, cong.ma, jjzhang, cqzong}@nlpr.ia.ac.cn

Abstract

The rapid increase in multimedia data
transmission over the Internet necessitates
the multi-modal summarization (MMS)
from collections of text, image, audio and
video. In this work, we propose an extrac-
tive multi-modal summarization method
that can automatically generate a textual
summary given a set of documents, im-
ages, audios and videos related to a specif-
ic topic. The key idea is to bridge the se-
mantic gaps between multi-modal content.
For audio information, we design an ap-
proach to selectively use its transcription.
For visual information, we learn the joint
representations of text and images using a
neural network. Finally, all of the multi-
modal aspects are considered to generate
the textual summary by maximizing the
salience, non-redundancy, readability and
coverage through the budgeted optimiza-
tion of submodular functions. We further
introduce an MMS corpus in English and
Chinese, which is released to the public1.
The experimental results obtained on this
dataset demonstrate that our method out-
performs other competitive baseline meth-
ods.

1 Introduction

Multimedia data (including text, image, audio and
video) have increased dramatically recently, which
makes it difficult for users to obtain important in-
formation efficiently. Multi-modal summarization
(MMS) can provide users with textual summaries
that can help acquire the gist of multimedia data in
a short time, without reading documents or watch-
ing videos from beginning to end.

1http://www.nlpr.ia.ac.cn/cip/jjzhang.htm

The existing applications related to MMS in-
clude meeting record summarization (Erol et al.,
2003; Gross et al., 2000), sport video sum-
marization (Tjondronegoro et al., 2011; Hasan
et al., 2013), movie summarization (Evangelopou-
los et al., 2013; Mademlis et al., 2016), pictorial
storyline summarization (Wang et al., 2012), time-
line summarization (Wang et al., 2016b) and social
multimedia summarization (Del Fabro et al., 2012;
Bian et al., 2013; Schinas et al., 2015; Bian et al.,
2015; Shah et al., 2015, 2016). When summariz-
ing meeting recordings, sport videos and movies,
such videos consist of synchronized voice, visual
and captions. For the summarization of pictorial
storylines, the input is a set of images with text
descriptions. None of these applications focus on
summarizing multimedia data that contain asyn-
chronous information about general topics.

In this paper, as shown in Figure 1, we propose
an approach to a generate textual summary from
a set of asynchronous documents, images, audios
and videos on the same topic.

Since multimedia data are heterogeneous and
contain more complex information than pure tex-
t does, MMS faces a great challenge in address-
ing the semantic gap between different modali-
ties. The framework of our method is shown in
Figure 1. For the audio information contained in
videos, we obtain speech transcriptions through
Automatic Speech Recognition (ASR) and design
a method to use these transcriptions selectively.
For visual information, including the key-frames
extracted from videos and the images that appear
in documents, we learn the joint representations
of texts and images by using a neural network; we
then can identify the text that is relevant to the im-
age. In this way, audio and visual information can
be integrated into a textual summary.

Traditional document summarization involves t-
wo essential aspects: (1) Salience: the summa-

1092



Twenty-four MSF doctors, 

nurses, logisticians and 

hygiene and sanitation experts 

are already in the country, 

while additional staff will 

strengthen the team in the 

coming days. With the help of 

the local community, MSF’s

emergency 

teams 

focus on 

searching.

The decease’s symptoms 

include severe fever and 

muscle pain, weakness, 

vomiting and diarrhea.

Afterwards, 

organs shut 

down, causing 

unstoppable 

bleeding. The spread of the 

illness is said to be through 

traveling mourners. 

Multi-modal Data Pre-processing
Documents

Videos Speech 

Transcriptions

Key-frames

Text

Image

Audio Features

Salience Calculating

Text-image Matching

Emergency teams focus on searching.

Jointly Optimizing Textual Summarization

Salience

Non-redundancy

Readability

Coverage for Image 

Ebola haemorrhagic fever 

is a rare but serious 

disease that spreads 

rapidly through direct 

contact with infected 

people.

Emergency teams focus 

on searching.

...

Figure 1: The framework of our MMS model.

ry should retain significant content of the input
documents. (2) Non-redundancy: the summary
should contain as little redundant content as pos-
sible. For MMS, we consider two additional as-
pects: (3) Readability: because speech transcrip-
tions are occasionally ill-formed, we should try to
get rid of the errors introduced by ASR. For ex-
ample, when a transcription provides similar in-
formation to a sentence in documents, we should
prefer the sentence to the transcription presented
in the summary. (4) Coverage for the visual in-
formation: images that appear in documents and
videos often capture event highlights that are usu-
ally very important. Thus, the summary should
cover as much of the important visual information
as possible. All of the aspects can be jointly opti-
mized by the budgeted maximization of submodu-
lar functions (Khuller et al., 1999).

Our main contributions are as follows:

• We design an MMS method that can automat-
ically generate a textual summary from a set
of asynchronous documents, images, audios
and videos related to a specific topic.

• To select the representative sentences, we
consider four criteria that are jointly opti-
mized by the budgeted maximization of sub-
modular functions.

• We introduce an MMS corpus in English and
Chinese. The experimental results on this
dataset demonstrate that our system can take
advantage of multi-modal information and
outperforms other baseline methods.

2 Related Work

2.1 Multi-document Summarization

Multi-document summarization (MDS) attempts
to extract important information for a set of docu-
ments related to a topic to generate a short sum-

mary. Graph based methods (Mihalcea and Ta-
rau, 2004; Wan and Yang, 2006; Zhang et al.,
2016) are commonly used. LexRank (Erkan and
Radev, 2011) first builds a graph of the docu-
ments, in which each node represents a sentence
and the edges represent the relationship between
sentences. Then, the importance of each sentence
is computed through an iterative random walk.

2.2 Multi-modal Summarization

In recent years, much work has been done to sum-
marize meeting recordings, sport videos, movies,
pictorial storylines and social multimedia.

Erol et al. (2003) aim to create important seg-
ments of a meeting recording based on audio, tex-
t and visual activity analysis. Tjondronegoro et
al. (2011) propose a way to summarize a sporting
event by analyzing the textual information extract-
ed from multiple resources and identifying the im-
portant content in a sport video. Evangelopoulos
et al. (2013) use an attention mechanism to detect
salient events in a movie. Wang et al. (2012) and
Wang et al. (2016b) use image-text pairs to gen-
erate a pictorial storyline and timeline summariza-
tion. Li et al. (2016) develop an approach for mul-
timedia news summarization for searching results
on the Internet, in which the hLDA model is intro-
duced to discover the topic structure of the news
documents. Then, a news article and an image are
chosen to represent each topic. For social medi-
a summarization, Fabro et al. (2012) and Schinas
et al. (2015) propose to summarize the real-life
events based on multimedia content such as pho-
tos from Flickr and videos from YouTube. Bian et
al. (2013; 2015) propose a multimodal LDA to de-
tect topics by capturing the correlations between
textual and visual features of microblogs with em-
bedded images. The output of their method is a set
of representative images that describe the events.
Shah et al. (2015; 2016) introduce EventBuilder

1093



which produces text summaries for a social event
leveraging Wikipedia and visualizes the event with
social media activities.

Most of the above studies focus on synchronous
multi-modal content, i.e., in which images are
paired with text descriptions and videos are paired
with subtitles. In contrast, we perform summa-
rization from asynchronous (i.e., there is no given
description for images and no subtitle for videos)
multi-modal information about news topics, in-
cluding multiple documents, images and videos,
to generate a fixed length textual summary. This
task is both more general and more challenging.

3 Our Model

3.1 Problem Formulation

The input is a collection of multi-modal dataM =
{D1, ..., D|D|, V1, ..., V|V |} related to a news topic
T , where each document Di = {Ti, Ii} consist-
s of text Ti and image Ii (there may be no image
for some documents). Vi denotes video. | · | de-
notes the cardinality of a set. The objective of our
work is to automatically generate textual summary
to represent the principle content ofM.

3.2 Model Overview

There are many essential aspects in generating a
good textual summary for multi-modal data. The
salient content in documents should be retained,
and the key facts in videos and images should be
covered. Further, the summary should be readable
and non-redundant and should follow the fixed
length constraint. We propose an extraction-based
method in which all these aspects can be jointly
optimized by the budgeted maximization of sub-
modular functions defined as follows:

max
S⊆T
{F(S) :

∑
s∈S

ls ≤ L} (1)

where T is the set of sentences, S is the summary,
ls is length (number of words) of sentence s, L is
budget, i.e., length constraint for the summary, and
submodular function F(S) is the summary score
related to the above-mentioned aspects.

Text is the main modality of documents, and in
some cases, images are embedded in documents.
Videos consist of at least two types of modalities:
audio and visual. Next, we give overall processing
methods for different modalities.

Audio, i.e., speech, can be automatically tran-
scribed into text by using an ASR system2. Then,
we can leverage a graph-based method to calcu-
late the salience score for all of the speech tran-
scriptions and for the original sentences in doc-
uments. Note that speech transcriptions are of-
ten ill-formed; thus, to improve the readability, we
should try to avoid the errors introduced by ASR.
In addition, audio features including acoustic con-
fidence (Valenza et al., 1999), audio power (Chris-
tel et al., 1998) and audio magnitude (Dagtas and
Abdel-Mottaleb, 2001) have proved to be helpful
for speech and video summarization which will
benefit our method.

For visual, which is actually a sequence of im-
ages (frames), because most of the neighboring
frames contain redundant information, we first ex-
tract the most meaningful frames, i.e., the key-
frames, which can provide the key facts for the
whole video. Then, it is necessary to perform se-
mantic analysis between text and visual. To this
end, we learn the joint representations for textu-
al and visual modalities and can then identify the
sentence that is relevant to the image. In this way,
we can guarantee the coverage of generated sum-
mary for the visual information.

3.3 Salience for Text
We apply a graph-based LexRank algorith-
m (Erkan and Radev, 2011) to calculate salience
score of the text unit, including the sentences
in documents and the speech transcriptions from
videos. LexRank first constructs a graph based on
the text units and their relationship and then con-
ducts an iteratively random walk to calculate the
salience score of the text unit, sa(ti), until conver-
gence using the following equation:

Sa(ti) = µ
∑

j

Sa(tj) ·Mji + 1− µ
N

(2)

where µ is the damping factor that is set to 0.85.
N is the total number of the text units. Mji is the
relationship between text unit ti and tj , which is
computed as follows:

Mji = sim(tj , ti) (3)

The text unit ti is represented by averaging the
embeddings of the words (except stop-words) in
ti. sim(·) denotes cosine similarity between two
texts (negative similarities are replaced with 0).

2We use IBM Watson Speech to Text service:
www.ibm.com/watson/developercloud/speech-to-text.html

1094



Document 

sentences

Speech 

transcriptions 

e1

e2 e3

v1 v2

v3 v4 v5

Figure 2: LexRank with guidance strategies. e1
is guided because speech transcription v3 is relat-
ed to document sentence v1; e2 and e3 are guided
because of audio features. Other edges without ar-
row are bidirectional.

For MMS task, we propose two guidance strate-
gies to amend the affinity matrix M and calculate
salience score of the text as shown in Figure 2.

3.3.1 Readability Guidance Strategies
The random walk process can be understood as a
recommendation: Mji in Equation 2 denotes that
tj will recommend ti to the degree of Mji. The
affinity matrix M in the LexRank model is sym-
metric, which means Mij = Mji. In contrast,
for MMS, considering the unsatisfactory quality
of speech recognition, symmetric affinity matri-
ces are inappropriate. Specifically, to improve
the readability, for a speech transcription, if there
is a sentence in document that is related to this
transcription, we would prefer to assign the tex-
t sentence a higher salience score than that as-
signed to the transcribed one. To this end, the pro-
cess of a random walk should be guided to con-
trol the recommendation direction: when a doc-
ument sentence is related to a speech transcrip-
tion, the symmetric weighted edge between them
should be transformed into a unidirectional edge,
in which we invalidate the direction from docu-
ment sentence to the transcribed one. In this way,
speech transcriptions will not be recommended by
the corresponding document sentences. Impor-
tant speech transcriptions that cannot be covered
by documents still have the chance to obtain high
salience scores. For the pair of a sentence ti and
a speech transcription tj , Mij is computed as fol-
lows:

Mij =
{

0, if sim(ti, tj) > Ttext
sim(ti, tj), otherwise

(4)
where threshold Ttext is used to determine whether
a sentence is related to others. We obtain the
proper semantic similarity threshold by testing on
Microsoft Research Paraphrase (MSRParaphrase)
dataset (Quirk et al., 2004). It is a publicly avail-

able paraphrase corpus that consists of 5801 pairs
of sentences, of which 3900 pairs are semantically
equivalent.

3.3.2 Audio Guidance Strategies
Some audio features can guide the summariza-
tion system to select more important and read-
able speech transcriptions. Valenza et al. (1999)
use acoustic confidence to obtain accurate and
readable summaries of broadcast news program-
s. Christel et al. (1998) and Dagtas and Abdel-
Mottaleb (2001) apply audio power and audio
magnitude to find significant audio events. In
our work, we first balance these three feature s-
cores for each speech transcription by dividing
their respective maximum values among the whole
amount of audio, and we then average these scores
to obtain the final audio score for speech transcrip-
tion. For each adjacent speech transcription pair
(tk, tk′ ), if the audio score a(tk) for tk is small-
er than a certain threshold while a(tk′ ) is greater,
which means that tk′ is more important and read-
able than tk, then tk should recommend tk′ , but
tk′ should not recommend tk. We formulate it as
follows: {

Mkk′ = sim(tk, tk′ )
Mk′k = 0

if a(tk) < Taudio and a(tk′ ) > Taudio
(5)

where the threshold Taudio is the average audio s-
core for all the transcriptions in the audio.

Finally, affinity matrices are normalized so that
each row adds up to 1.

3.4 Text-Image Matching
The key-frames contained in videos and the im-
ages embedded in documents often captures news
highlights in which the important ones should be
covered by the textual summary. Before measur-
ing the coverage for images, we should train the
model to bridge the gap between text and image,
i.e., to match the text and image.

We start by extracting key-frames of videos
based on shot boundary detection. A shot is de-
fined as an unbroken sequence of frames. The
abrupt transition of RGB histogram features often
indicates shot boundaries (Zhuang et al., 1998).
Specifically, when the transition of the RGB his-
togram feature for adjacent frames is greater than
a certain ratio3 of the average transition for the w-
hole video, we segment the shot. Then, the frames

3The ratio is determined by testing on the

1095



in the middle of each shot are extracted as key-
frames. These key-frames and images in docu-
ments make up the image set that the summary
should cover.

Next, it is necessary to perform a semantic anal-
ysis between the text and the image. To this end,
we learn the joint representations for textual and
visual modalities by using a model trained on the
Flickr30K dataset (Young et al., 2014), which con-
tains 31,783 photographs of everyday activities,
events and scenes harvested from Flickr. Each
photograph is manually labeled with 5 textual de-
scriptions. We apply the framework of Wang et
al. (2016a), which achieves state-of-the-art perfor-
mance for text-image matching task on the Flick-
r30K dataset. The image is encoded by the VG-
G model (Simonyan and Zisserman, 2014) that
has been trained on the ImageNet classification
task following the standard procedure (Wang et al.,
2016a). The 4096-dimensional feature from the
pre-softmax layer is used to represent the image.
The text is first encoded by the Hybrid Gaussian-
Laplacian mixture model (HGLMM) using the
method of Klein et al. (2014). Then, the HGLM-
M vectors are reduced to 6000 dimensions through
PCA. Next, the sentence vector vs and image vec-
tor vi are mapped to a joint space by a two-branch
neural network as follows:{

x = W2 · f(W1 · vs + bs)
y = V2 · f(V1 · vi + bi) (6)

where W1 ∈ R2048×6000, bs ∈ R2048, W2 ∈
R512×2048, V1 ∈ R2048×4096, bi ∈ R2048, V2 ∈
R512×2048, f is Rectified Linear Unit (ReLU).

The max-margin learning framework is applied
to optimize the neural network as follows:

L =
∑
i,k

max[0,m+ s(xi, yi)− s(xi, yk)]

+ λ1
∑
i,k

max[0,m+ s(xi, yi)− s(xk, yi)]
(7)

where for positive text-image pair (xi, yi), the
top K most violated negative pairs (xi, yk) and
(xk, yi) in each mini-batch are sampled. The ob-
jective function L favors higher matching score
s(xi, yi) (cosine similarity) for positive text-image
pairs than for negative pairs4.

shot detection dataset of TRECVID. http://www-
nlpir.nist.gov/projects/trecvid/

4In the experiments, K = 50, m = 0.1 and λ1 = 2.
Wang et al. (2016a) also proved that structure-preserving con-
straints can make 1% Recall@1 improvement.

Note that the images in Flickr30K are similar
to our task. However, the image descriptions are
much simpler than the text in news, so the mod-
el trained on Flickr30K cannot be directly used
for our task. For example, some of the informa-
tion contained in the news, such as the time and
location of events, cannot be directly reflected by
images. To solve this problem, we simplify each
sentence and speech transcription based on seman-
tic role labelling (Gildea and Jurafsky, 2002), in
which each predicate indicates an event and the
arguments express the relevant information of this
event. ARG0 denotes the agent of the event, and
ARG1 denotes the action. The assumption is that
the concepts including agent, predicate and ac-
tion compose the body of the event, so we ex-
tract “ARG0+predicate+ARG1” as the simplified
sentence that is used to match the images. It is
worth noting that there may be multiple predicate-
argument structures for one sentence and we ex-
tract all of them.

After the text-image matching model is trained
and the sentences are simplified, for each text-
image pair (Ti, Ij) in our task, we can identify the
matched pairs if the score s(Ti, Ij) is greater than
a threshold Tmatch. We set the threshold as the
average matching score for the positive text-image
pair in Flickr30K, although the matching perfor-
mance for our task could in principle be improved
by adjusting this parameter.

3.5 Multi-modal Summarization

We model the salience of a summary S as the sum
of salience scores Sa(ti)5 of the sentence ti in
the summary, combining a λ-weighted redundan-
cy penalty term:

Fs(S) =
∑
ti∈S

Sa(ti)− λs|S|
∑

ti,tj∈S
sim(ti, tj) (8)

We model the summary S coverage for the im-
age set I as the weighted sum of image covered by
the summary:

Fc(S) =
∑
pi∈I

Im(pi)bi (9)

where the weight Im(pi) for the image pi is
the length ratio between the shot pi and the w-
hole videos. bi is a binary variable to indicate

5Normalized by the maximum value among all the sen-
tences.

1096



whether an image pi is covered by the summary,
i.e., whether there is at least one sentence in the
summary matching the image.

Finally, considering all the modalities, the ob-
jective function is defined as follows:

Fm(S) = 1
Ms

∑
ti∈S

Sa(ti) +
1
Mc

∑
pi∈I

Im(pi)bi

− λm|S|
∑
i,j∈S

sim(ti, tj)

(10)
where Ms is the summary score obtained by E-
quation 8 and Mc is the summary score obtained
by Equation 9. The aim of Ms and Mc is to bal-
ance the aspects of salience and coverage for im-
ages. λs, and λm are determined by testing on
development set. Note that to guaranteed mono-
tone of F , λs, and λm should be lower than the
minimum salience score of sentences. To further
improve non-redundancy, we make sure that sim-
ilarity between any pair of sentences in the sum-
mary is lower than Ttext.

Equations 8,9 and 10 are all monotone submod-
ular functions under the budget constraint. Thus,
we apply the greedy algorithm (Lin and Bilmes,
2010) guaranteeing near-optimization to solve the
problem.

4 Experiment

4.1 Dataset
There is no benchmark dataset for MMS. We con-
struct a dataset as follows. We select 50 news top-
ics in the most recent five years, 25 in English and
25 in Chinese. We set 5 topics for each language as
a development set. For each topic, we collect 20
documents within the same period using Google
News search6 and 5-10 videos in CCTV.com7 and
Youtube8. More details of the corpus are illustrat-
ed in Table 1. Some examples of news topics are
provided Table 2.

We employ 10 graduate students to write ref-
erence summaries after reading documents and
watching videos on the same topic. We keep 3 ref-
erence summaries for each topic. The criteria for
summarizing documents lie in: (1) retaining im-
portant content of the input documents and videos;
(2) avoiding redundant information; (3) having a

6http://news.google.com/
7http://www.cctv.com/
8https://www.youtube.com/

good readability; (4) following the length limit.
We set the length constraint for each English and
Chinese summary to 300 words and 500 charac-
ters, respectively.

#Sentence #Word #Shot Video Length

English 492.1 12,104.7 47.2 197s
Chinese 402.1 9,689.3 49.3 207s

Table 1: Corpus statistics.

English

(1) Nepal earthquake
(2) Terror attack in Paris
(3) Train derailment in India
(4) Germanwings crash
(5) Refugee crisis in Europe

Chinese

(6) “东方之星”客船翻沉
(“Oriental Star”passenger ship sinking)
(7)银川公交大火
(The bus fire in Yinchuan)
(8)香港占中
(Occupy Central in HONG KONG)
(9)李娜澳网夺冠
(Li Na wins Australian Open)
(10)抗议“萨德”反导系统
(Protest against “THAAD”anti-missile system)

Table 2: Examples of news topics.

4.2 Comparative Methods
Several models are compared in our experiments,
including generating summaries with differen-
t modalities and different approaches to leverage
images.

Text only. This model generates summaries on-
ly using the text in documents.

Text + audio. This model generates summaries
using the text in documents and the speech tran-
scriptions but without guidance strategies.

Text + audio + guide. This model generates
summaries using the text in documents and the
speech transcriptions with guidance strategies.

The following models generate summaries us-
ing both documents and videos but take advantage
of images in different ways. The salience scores
for text are obtained with guidance strategies.

Image caption. The image is first captioned
using the model of Vinyals et al. (2016) which
achieved first place in the 2015 MSCOCO Image
Captioning Challenge. This model generates sum-
maries using text in documents, speech transcrip-
tion and image captions.

Note that the above-mentioned methods gener-
ate summaries by using Equation 8 and the follow-

1097



ing methods using Equation 8 ,9 and 10.
Image caption match. This model uses gener-

ated image captions to match the text; i.e., if the
similarity between a generated image caption and
a sentence exceeds the threshold Ttext, the image
and the sentence match.

Image alignment. The images are aligned to
the text in the following ways: The images in a
document are aligned to all the sentences in this
document and the key-frames in a shot are aligned
to all the speech transcriptions in this shot.

Image match. The texts are matched with im-
ages using the approach introduced in Section 3.4.

4.3 Implementation Details
We perform sentence9 and word tokenization, and
all the Chinese sentences are segmented by S-
tanford Chinese Word Segmenter (Tseng et al.,
2005). We apply Stanford CoreNLP toolkit (Levy
and D. Manning, 2003; Klein and D. Manning,
2003) to perform lexical parsing and use se-
mantic role labelling approach proposed by Yang
and Zong (2014). We use 300-dimension skip-
gram English word embeddings which are pub-
licly available10. Given that text-image match-
ing model and image caption generation model are
trained in English, to create summaries in Chinese,
we first translate the Chinese text into English vi-
a Google Translation11 and then conduct text and
image matching.

4.4 Multi-modal Summarization Evaluation
We use the ROUGE-1.5.5 toolkit (Lin and Hov-
y, 2003) to evaluate the output summaries. This
evaluation metric measures the summary quality
by matching n-grams between generated summa-
ry and reference summary. Table 3 and Table 4
show the averaged ROUGE-1 (R-1), ROUGE-2
(R-2) and ROUGE-SU4 (R-SU4) F-scores regard-
ing to the three reference summaries for each topic
in English and Chinese.

For the results of the English MMS, from
the first three lines in Table 3 we can see that
when summarizing without visual information, the
method with guidance strategies performs slight-
ly better than do the first two methods. Because
Rouge mainly measures word overlaps, manual e-
valuation is needed to confirm the impact of guid-
ance strategies on improving readability. It is in-

9We exclude sentences containing less than 5 words.
10https://code.google.com/archive/p/word2vec/
11https://translate.google.com

Method R-1 R-2 R-SU4

Text only 0.422 0.114 0.166
Text + audio 0.422 0.109 0.164
Text + audio + guide 0.440 0.117 0.171
Image caption 0.435 0.111 0.167
Image caption match 0.429 0.115 0.166
Image alignment 0.409 0.082 0.082
Image match 0.442 0.133 0.187

Table 3: Experimental results (F-score) for En-
glish MMS.

Method R-1 R-2 R-SU4

Text only 0.409 0.113 0.167
Text + audio 0.407 0.111 0.166
Text + audio + guide 0.411 0.115 0.173
Image caption match 0.381 0.092 0.149
Image alignment 0.368 0.096 0.143
Image match 0.414 0.125 0.173

Table 4: Experimental results (F-score) for Chi-
nese MMS.

troduced in Section 4.5. The rating ranges from 1
(the poorest) to 5 (the best). When summarizing
with textual and visual modalities, performances
are not always improved, which indicates that the
models of image caption, image caption match
and image alignment are not suitable to MMS.
The image match model has a significant advan-
tage over other comparative methods, which illus-
trates that it can make use of multi-modal infor-
mation.

Table 4 shows the Chinese MMS results, which
are similar to the English results that the image
match model achieves the best performance. We
find that the performance enhancement for the im-
age match model is smaller in Chinese than it is
in English, which may be due to the errors intro-
duced by machine translation.

We provides a generated summary in English
using the image match model, which is shown in
Figure 3.

4.5 Manual Summary Quality Evaluation

The readability and informativeness for sum-
maries are difficult to evaluate formally. We ask
five graduate students to measure the quality of
summaries generated by different methods. We
calculate the average score for all of the topics,
and the results are displayed in Table 5. Overal-
l, our method with guidance strategies achieves
higher scores than do the other methods, but it
is still obviously poorer than the reference sum-

1098



Ramchandra Tewari , a passenger who suffered a head injury , said he was 

asleep when he was suddenly flung to the floor of his coach . The impact of 

the derailment was so strong that one of the coaches landed on top of 

another , crushing the one below , said Brig. Anurag Chibber , who was 

heading the army 's rescue team . `` We fear there could be many more dead 

in the lower coach , '' he said , adding that it was unclear how many people 

were in the coach . Kanpur is a major railway junction , and hundreds of 

trains pass through the city every day . `` I heard a loud noise , '' passenger 

Satish Mishra said . Some railway officials told local media they suspected 

faulty tracks caused the derailment . Fourteen cars in the 23-car train 

derailed , Modak said . We do n't expect to find any more bodies , '' said 

Zaki Ahmed , police inspector general in the northern city of Kanpur , about 

65km from the site of the crash in Pukhrayan . When they tried to leave 

through one of the doors , they found the corridor littered with bodies , he 

said . The doors would n't open but we somehow managed to come out . But 

it has a poor safety record , with thousands of people dying in accidents 

every year , including in train derailments and collisions . By some analyst 

estimates , the railways need 20 trillion rupees (  $ 293.34 billion )  of 

investment by 2020 , and India is turning to partnerships with private 

companies and seeking loans from other countries to upgrade its network .

Figure 3: An example of generated summary for the news topic “India train derailment”. The sentences
covering the images are labeled by the corresponding colors. The text can be partly related to the image
because we use simplified sentence based on SRL to match the images. We can find some mismatched
sentences, such as the sentence “Fourteen cars in the 23-car train derailed , Modak said .” where our
text-image matching model may misunderstand the “car ” as a “motor vehicle” but not a “coach”.

maries. Specifically, when speech transcription-
s are not considered, the informativeness of the
summary is the worst. However, adding speech
transcriptions without guidance strategies decreas-
es readability to a large extent, which indicates
that guidance strategies are necessary for MMS.
The image match model achieves higher informa-
tiveness scores than do the other methods without
using images.

We give two instances of readability guidance
that arise between document text (DT) and speech
transcriptions (ST) in Table 6. The errors intro-
duced by ASR include segmentation (instance A)
and recognition (instance B) mistakes.

Method Read Inform

English

Text only 3.72 3.28
Text + audio 3.08 3.44
Text + audio + guide 3.68 3.64
Image match 3.67 3.83
Reference 4.52 4.36

Chinese

Text only 3.64 3.40
Text + audio 3.16 3.48
Text + audio + guide 3.60 3.72
Image match 3.62 3.92
Reference 4.88 4.84

Table 5: Manual summary quality evaluation.
“Read” denotes “Readability” and “Inform” de-
notes “informativeness”.

A
DT There were 12 bodies at least pulled fromthe rubble in the square.
ST Still being pulled from the rubble.

CST
Many people are still being pulled from
the rubble.

B

DT Conflict between police and protesterslit up on Tuesday.

ST
Late night tensions between police and
protesters briefly lit up this Baltimore
neighborhood Tuesday.

CST
Late-night tensions between police and
protesters briefly lit up in a Baltimore
neighborhood Tuesday.

Table 6: Guidance examples. “CST” denotes man-
ually modified correct ST. ASR errors are marked
red and revisions are marked blue.

4.6 How Much is the Image Worth

Text-image matching is the toughest module for
our framework. Although we use a state-of-the-art
approach to match the text and images, the per-
formance is far from satisfactory. To find a some-
what strong upper-bound of the task, we choose
five topics for each language to manually label the
text-image matching pairs. The MMS results on
these topics are shown in Table 7 and Table 8. The
experiments show that with the ground truth text-
image matching result, the summary quality can
be promoted to a considerable extent, which indi-
cates visual information is crucial for MMS.

An image and the corresponding texts obtained
using different methods are given in Figure 4 an d
Figure 5. We can conclude that the image caption

1099



Image caption:
A group of people standing on top of a lush green field.
Image caption match:
We could barely stay standing.
Image hard alignment:
The need for doctors would grow as more survivors were pulled 
from the rubble.
Image match:
The search, involving US, Indian and Nepali military choppers and a 
battalion of 400 Nepali soldiers, has been joined by two MV-22B 
Osprey.
Image manually match:
The military helicopter was on an aid mission in Dolakha district 
near Tibet.

Figure 4: An example image with corresponding
English texts that different methods obtain.

and the image caption match contain little of the
image’s intrinsically intended information. The
image alignment introduces more noise because
it is possible that the whole text in documents or
the speech transcriptions in shot are aligned to the
document images or the key-frames, respectively.
The image match can obtain similar results to the
image manually match, which illustrates that the
image match can make use of visual information
to generate summaries.

Method R-1 R-2 R-SU4

Text + audio + guide 0.426 0.105 0.167
Image caption 0.423 0.106 0.167
Image caption match 0.400 0.086 0.149
Image alignment 0.399 0.069 0.136
Image match 0.436 0.126 0.177
Image manually match 0.446 0.150 0.207

Table 7: Experimental results (F-score) for En-
glish MMS on five topics with manually labeled
text-image pairs.

Method R-1 R-2 R-SU4

Text + audio + guide 0.417 0.115 0.171
Image caption match 0.396 0.095 0.152
Image alignment 0.306 0.072 0.111
Image match 0.401 0.127 0.179
Image manually match 0.419 0.162 0.208

Table 8: Experimental results (F-score) for Chi-
nese MMS on five topics with manually labeled
text-image pairs.

Image caption match:
就 星 州 民众 举行 抗议 集会 ， 文尚 均 表示 ， 国防部 愿意 与 
当地 居民 沟通 。
(On behalf of the protest rally of people in Seongju, Moon Sang-
gyun said that the Ministry of National Defense is willing to 
communicate with local residents.)
Image hard alignment:
朴槿惠 在 国家 安全 保障 会议 上 呼吁 民众 支持 “ 萨德 ” 部
署 。
(Park Geun-hye called on people to support the "THAAD" 
deployment in the National Security Council. )
Image match:
从 7月 12日 开始 ， 当地 民众 连续 数日 在 星 州郡 厅 门口 请
愿 。
(The local people petitioned in front of the Seongju County Office 
for days from July 12.)
Image manually match:
 当天 ， 星 州郡 数千 民众 集会 ， 抗议 在 当地 部署 “ 萨
德 ”
(On that day, thousands of people gathered in Seongju to protest the 
local deployment of "THAAD". )

Figure 5: An example image with corresponding
Chinese texts that different methods obtain.

5 Conclusion

This paper addresses an asynchronous MMS task,
namely, how to use related text, audio and video
information to generate a textual summary. We
formulate the MMS task as an optimization prob-
lem with a budgeted maximization of submodular
functions. To selectively use the transcription of
audio, guidance strategies are designed using the
graph model to effectively calculate the salience
score for each text unit, leading to more readable
and informative summaries. We investigate vari-
ous approaches to identify the relevance between
the image and texts, and find that the image match
model performs best. The final experimental re-
sults obtained using our MMS corpus in both En-
glish and Chinese demonstrate that our system can
benefit from multi-modal information.

Adding audio and video does not seem to im-
prove dramatically over text only model, which
indicates that better models are needed to capture
the interactions between text and other modalities,
especially for visual. We also plan to enlarge our
MMS dataset, specifically to collect more videos.

Acknowledgments

The research work has been supported by the Nat-
ural Science Foundation of China under Grant No.
61333018 and No. 61403379.

1100



References
Jingwen Bian, Yang Yang, and Tat-Seng Chua. 2013.

Multimedia summarization for trending topics in mi-
croblogs. In Proceedings of the 22nd ACM interna-
tional conference on Conference on information &
knowledge management, pages 1807–1812. ACM.

Jingwen Bian, Yang Yang, Hanwang Zhang, and Tat-
Seng Chua. 2015. Multimedia summarization for
social events in microblog stream. IEEE Transac-
tions on Multimedia, 17(2):216–228.

Michael G Christel, Michael A Smith, C Roy Tay-
lor, and David B Winkler. 1998. Evolving video
skims into useful multimedia abstractions. In Pro-
ceedings of the SIGCHI conference on Human fac-
tors in computing systems, pages 171–178. ACM
Press/Addison-Wesley Publishing Co.

Serhan Dagtas and Mohamed Abdel-Mottaleb. 2001.
Extraction of tv highlights using multimedia fea-
tures. In Multimedia Signal Processing, 2001 IEEE
Fourth Workshop on, pages 91–96. IEEE.

Manfred Del Fabro, Anita Sobe, and Laszlo
Böszörmenyi. 2012. Summarization of real-life
events based on community-contributed content. In
The Fourth International Conferences on Advances
in Multimedia, pages 119–126.

Gunes Erkan and Dragomir R. Radev. 2011. Lexrank:
Graph-based lexical centrality as salience in tex-
t summarization. Journal of Qiqihar Junior Teach-
ers College, 22:2004.

Berna Erol, D-S Lee, and Jonathan Hull. 2003. Mul-
timodal summarization of meeting recordings. In
Multimedia and Expo, 2003. ICME’03. Proceed-
ings. 2003 International Conference on, volume 3,
pages III–25. IEEE.

Georgios Evangelopoulos, Athanasia Zlatintsi, Alexan-
dros Potamianos, Petros Maragos, Konstantinos Ra-
pantzikos, Georgios Skoumas, and Yannis Avrithis.
2013. Multimodal saliency and fusion for movie
summarization based on aural, visual, and textu-
al attention. IEEE Transactions on Multimedia,
15(7):1553–1568.

Daniel Gildea and Daniel Jurafsky. 2002. Automatic
labeling of semantic roles. Computational Linguis-
tics, Volume 28, Number 3, September 2002.

Ralph Gross, Michael Bett, Hua Yu, Xiaojin Zhu, Yue
Pan, Jie Yang, and Alex Waibel. 2000. Towards
a multimodal meeting record. In Multimedia and
Expo, 2000. ICME 2000. 2000 IEEE International
Conference on, volume 3, pages 1593–1596. IEEE.

Taufiq Hasan, Hynek Bořil, Abhijeet Sangwan, and
John HL Hansen. 2013. Multi-modal highlight
generation for sports videos using an information-
theoretic excitability measure. EURASIP Journal on
Advances in Signal Processing, 2013(1):173.

Samir Khuller, Anna Moss, and Joseph Seffi Naor.
1999. The budgeted maximum coverage problem.
Information Processing Letters, 70(1):39–45.

Benjamin Klein, Guy Lev, Gil Sadeh, and Lior Wolf.
2014. Fisher vectors derived from hybrid gaussian-
laplacian mixture models for image annotation.
arXiv preprint arXiv:1411.7399.

Dan Klein and Christopher D. Manning. 2003. Ac-
curate unlexicalized parsing. In Proceedings of the
41st Annual Meeting of the Association for Compu-
tational Linguistics.

Roger Levy and Christopher D. Manning. 2003. Is it
harder to parse chinese, or the chinese treebank? In
Proceedings of the 41st Annual Meeting of the Asso-
ciation for Computational Linguistics.

Zechao Li, Jinhui Tang, Xueming Wang, Jing Liu, and
Hanqing Lu. 2016. Multimedia news summariza-
tion in search. ACM Transactions on Intelligent Sys-
tems and Technology (TIST), 7(3):33.

Chin-Yew Lin and Eduard Hovy. 2003. Automatic e-
valuation of summaries using n-gram co-occurrence
statistics. In Proceedings of the 2003 Human Lan-
guage Technology Conference of the North Ameri-
can Chapter of the Association for Computational
Linguistics.

Hui Lin and Jeff Bilmes. 2010. Multi-document sum-
marization via budgeted maximization of submod-
ular functions. In Human Language Technologies:
The 2010 Annual Conference of the North American
Chapter of the Association for Computational Lin-
guistics, pages 912–920. Association for Computa-
tional Linguistics.

Ioannis Mademlis, Anastasios Tefas, Nikos Nikolaidis,
and Ioannis Pitas. 2016. Multimodal stereoscopic
movie summarization conforming to narrative char-
acteristics. IEEE Transactions on Image Process-
ing, 25(12):5828–5840.

Rada Mihalcea and Paul Tarau. 2004. Proceedings of
the 2004 Conference on Empirical Methods in Natu-
ral Language Processing, chapter TextRank: Bring-
ing Order into Text.

Chris Quirk, Chris Brockett, and William Dolan. 2004.
Proceedings of the 2004 Conference on Empirical
Methods in Natural Language Processing, chap-
ter Monolingual Machine Translation for Paraphrase
Generation.

Manos Schinas, Symeon Papadopoulos, Georgios
Petkos, Yiannis Kompatsiaris, and Pericles A
Mitkas. 2015. Multimodal graph-based event detec-
tion and summarization in social media streams. In
Proceedings of the 23rd ACM international confer-
ence on Multimedia, pages 189–192. ACM.

Rajiv Ratn Shah, Anwar Dilawar Shaikh, Yi Yu, Wen-
jing Geng, Roger Zimmermann, and Gangshan Wu.
2015. Eventbuilder: Real-time multimedia event

1101



summarization by visualizing social media. In Pro-
ceedings of the 23rd ACM international conference
on Multimedia, pages 185–188. ACM.

Rajiv Ratn Shah, Yi Yu, Akshay Verma, Suhua Tang,
Anwar Dilawar Shaikh, and Roger Zimmermann.
2016. Leveraging multimodal information for even-
t summarization and concept-level sentiment analy-
sis. Knowledge-Based Systems, 108:102–109.

Karen Simonyan and Andrew Zisserman. 2014. Very
deep convolutional networks for large-scale image
recognition. arXiv preprint arXiv:1409.1556.

Dian Tjondronegoro, Xiaohui Tao, Johannes Sasongko,
and Cher Han Lau. 2011. Multi-modal summariza-
tion of key events and top players in sports tourna-
ment videos. In Applications of Computer Vision
(WACV), 2011 IEEE Workshop on, pages 471–478.
IEEE.

H. Tseng, P. Chang, G. Andrew, D. Jurafsky, and
C. Manning. 2005. A conditional random field word
segmenter.

Robin Valenza, Tony Robinson, Marianne Hickey, and
Roger Tucker. 1999. Summarisation of spoken au-
dio through information extraction. In ESCA Tuto-
rial and Research Workshop (ETRW) on Accessing
Information in Spoken Audio.

Oriol Vinyals, Alexander Toshev, Samy Bengio, and
Dumitru Erhan. 2016. Show and tell: Lesson-
s learned from the 2015 mscoco image captioning
challenge. IEEE transactions on pattern analysis
and machine intelligence.

Xiaojun Wan and Jianwu Yang. 2006. Improved
affinity graph based multi-document summarization.
In Proceedings of the Human Language Technolo-
gy Conference of the NAACL, Companion Volume:
Short Papers.

Dingding Wang, Tao Li, and Mitsunori Ogihara. 2012.
Generating pictorial storylines via minimum-weight
connected dominating set approximation in multi-
view graphs. In AAAI.

Liwei Wang, Yin Li, and Svetlana Lazebnik. 2016a.
Learning deep structure-preserving image-text em-
beddings. In Proceedings of the IEEE Conference
on Computer Vision and Pattern Recognition, pages
5005–5013.

Yang William Wang, Yashar Mehdad, R. Dragomir
Radev, and Amanda Stent. 2016b. A low-rank ap-
proximation approach to learning joint embeddings
of news stories and images for timeline summariza-
tion. In Proceedings of the 2016 Conference of
the North American Chapter of the Association for
Computational Linguistics: Human Language Tech-
nologies, pages 58–68. Association for Computa-
tional Linguistics.

Haitong Yang and Chengqing Zong. 2014. Multi-
predicate semantic role labeling. In Proceedings of
the 2014 Conference on Empirical Methods in Natu-
ral Language Processing (EMNLP), pages 363–373.
Association for Computational Linguistics.

Peter Young, Alice Lai, Micah Hodosh, and Julia
Hockenmaier. 2014. From image descriptions to vi-
sual denotations. Transactions of the Association of
Computational Linguistics, 2:67–78.

Jiajun Zhang, Yu Zhou, Chengqing Zong, Jiajun
Zhang, Yu Zhou, Chengqing Zong, Jiajun Zhang,
Chengqing Zong, and Yu Zhou. 2016. Abstrac-
tive cross-language summarization via translation
model enhanced predicate argument structure fus-
ing. IEEE/ACM Transactions on Audio, Speech and
Language Processing (TASLP), 24(10):1842–1853.

Yueting Zhuang, Yong Rui, Thomas S Huang, and
Sharad Mehrotra. 1998. Adaptive key frame extrac-
tion using unsupervised clustering. In Image Pro-
cessing, 1998. ICIP 98. Proceedings. 1998 Inter-
national Conference on, volume 1, pages 866–870.
IEEE.

1102


