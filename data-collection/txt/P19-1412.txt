



















































Do You Know That Florence Is Packed with Visitors? Evaluating State-of-the-art Models of Speaker Commitment


Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 4208–4213
Florence, Italy, July 28 - August 2, 2019. c©2019 Association for Computational Linguistics

4208

Do you know that Florence is packed with visitors?
Evaluating state-of-the-art models of speaker commitment

Nanjiang Jiang and Marie-Catherine de Marneffe
Department of Linguistics
The Ohio State University

{jiang.1879, demarneffe.1}@osu.edu

Abstract

When a speaker, Mary, asks Do you know that
Florence is packed with visitors?, we take her
to believe that Florence is packed with visitors,
but not if she asks Do you think that Florence
is packed with visitors? Inferring speaker
commitment (aka event factuality) is crucial
for information extraction and question an-
swering. Here we explore the hypothesis that
linguistic deficits drive the error patterns of
speaker commitment models by analyzing the
linguistic correlates of model errors on a chal-
lenging naturalistic dataset. We evaluate two
state-of-the-art speaker commitment models
on the CommitmentBank, an English dataset
of naturally occurring discourses. The Com-
mitmentBank is annotated with speaker com-
mitment towards the content of the comple-
ment (Florence is packed with visitors in our
example) of clause-embedding verbs (know,
think) under four entailment-canceling envi-
ronments. We found that a linguistically-
informed model outperforms a LSTM-based
one, suggesting that linguistic knowledge is
needed to capture such challenging naturalis-
tic data. A breakdown of items by linguistic
features reveals asymmetrical error patterns:
while the models achieve good performance
on some classes (e.g., negation), they fail to
generalize to the diverse linguistic construc-
tions (e.g., conditionals) in natural language,
highlighting directions for improvement.

1 Introduction

Prediction of speaker commitment1 is the task of
determining to what extent the speaker is com-
mitted to an event in a sentence as actual, non-
actual, or uncertain. This matters for downstream
NLP applications, such as information extraction

1Previous work uses event factuality, verdicality, or com-
mitted belief; the terms refer to the same linguistic phe-
nomenon, perhaps with different emphasis.

or question answering: for instance, we should ex-
tract from example (1) in Table 1 that the speaker
could wish someone dead, but from (3) that people
should not be allowed to carry guns in their vehi-
cles, even though both events are embedded under
believe and negation.

There has been work on factors leading to
speaker commitment in theoretical linguistics (i.a.,
Karttunen (1971); Simons et al. (2010)) and com-
putational linguistics (i.a., Diab et al. (2009);
Saurı́ and Pustejovsky (2012); Prabhakaran et al.
(2015)), but mostly on constructed or newswire
examples, which may simplify the task by fail-
ing to reflect the lexical and syntactic diversity
of naturally occurring utterances. de Marneffe
et al. (2019) introduced the CommitmentBank,
a dataset of naturally occurring sentences anno-
tated with speaker commitment towards the con-
tent of complements of clause-embedding verbs
under canceling-entailment environments (nega-
tion, modal, question and conditional), to study the
linguistic correlates of speaker commitment. In
this paper, we use it to evaluate two state-of-the-art
(SoA) models of speaker commitment: Stanovsky
et al. (2017) and Rudinger et al. (2018). The Com-
mitmentBank, restricted to specific linguistic con-
structions, is a good test case. It allows us to eval-
uate whether current speaker commitment models
achieve robust language understanding, by analyz-
ing their performance on specific challenging lin-
guistic constructions.

2 The CommitmentBank corpus

The CommitmentBank2 consists of 1,200 natu-
rally occurring items involving clause-embedding
verbs under four entailment-canceling environ-
ments (negations, modals, questions, condition-

2The data is available at https://github.com/
mcdm/CommitmentBank

https://github.com/mcdm/CommitmentBank
https://github.com/mcdm/CommitmentBank


4209

Figure 1: Mean commitment scores in all of the CommitmentBank. Italicized verbs are factive, plain nonfactive.

(1) Context The answer is no, no no. Not now, not ever.
Target I never believed I could wish anyone dead︸ ︷︷ ︸

Gold:1.56, Rule-based:3.0, Hybrid: 0.50

but last night changed all that.

(2) Context Revenue is estimated at $18.6 million. The maker of document image processing equipment said the state procurement
division had declared FileNet in default on its contract with the secretary of state uniform commercial code division.

Target FileNet said it doesn’t believe the state has a valid basis of default and is reviewing its legal rights under the contract︸ ︷︷ ︸
Gold: -0.47, Rule-based: 3.0, Hybrid: 1.08

,

but said it can’t predict the outcome of the dispute.

(3) Context A: Yeah, that’s crazy. B: and then you come here in the Dallas area, um,
Target I don’t believe that people should be allowed to carry guns in their vehicled︸ ︷︷ ︸

Gold: -2.64, Rule-based: 3.0, Hybrid: 1.40

.

Table 1: Examples from the CommitmentBank, with gold scores and predictions from rule-based and hybrid
models. Embedding verbs in bold, entailment-canceling environments italicized. The gold score is the mean
annotators’ speaker commitment judgments towards the content of the complement.

als). Three genres are represented: newswire from
the Wall Street Journal (WSJ), fiction from the
British National Corpus, and dialog from Switch-
board. Each item consists of up to two context
sentences and one target sentence, as shown in Ta-
ble 1. For each item, speaker commitment judg-
ments were gathered on Mechanical Turk from at
least eight native English speakers. Participants
judged whether or not the speaker is certain that
the content of the complement in the target sen-
tence is true, using a Likert scale labeled at 3
points (+3/speaker is certain that the complement
is true, 0/speaker is not certain whether it is true or
false, -3/speaker is certain that it is false). We took
the mean annotations of each item as gold score
of speaker commitment. Figure 1 shows the mean
annotations per embedding verb.

Restricted set We identified a subset of the
CommitmentBank that displays high agreement
among annotators. We divided the range of in-
teger ratings [−3, 3] into three sub-ranges: [1, 3]
where the speaker is committed to the complement

p, 0 where the speaker is uncommitted towards p,
[−3,−1] where the speaker is committed to ¬p.
We selected the items for which at least 80% of
the annotations fall into the same sub-range. This
gives 556 items, with 37 clause-embedding verbs.
Figure 2 shows that the proportion of items with
different linguistic features in the restricted set is
similar to the proportion in the full set, suggest-
ing that the restricted set is representative of the
original data. The full CommitmentBank has a
Krippendorff’s α of 0.53, while α is 0.74 on the
restricted set.

3 Models of speaker commitment

We evaluate the performance of two speaker com-
mitment models on the CommitmentBank: a rule-
based model (Stanovsky et al., 2017) and a neural-
based one (Rudinger et al., 2018).

Rule-based model Stanovsky et al. (2017) pro-
posed a rule-based model based on a determinis-
tic algorithm based on TruthTeller (Lotan et al.,
2013), which uses a top-down approach on a de-



4210

Figure 2: Number of items with different features in
the full and restricted sets of the CommitmentBank.

pendency tree and predicts speaker commitment
score in [−3, 3] according to the implicative sig-
natures (Karttunen, 2012) of the predicates, and
whether the predicates are under the scope of
negation and uncertainty modifiers. For example,
refuse p entails ¬p, so the factuality of its comple-
ment p gets flipped if encountered.

Neural-based model Rudinger et al. (2018) in-
troduced three neural models for speaker commit-
ment: a linear biLSTM, a dependency tree biL-
STM, a hybrid model that ensembles the two.
Rudinger et al. (2018) also proposed a multi-
task training scheme in which a model is trained
on four factuality datasets: FactBank (Saurı́ and
Pustejovsky, 2009), UW (Lee et al., 2015), MEAN-
TIME (Minard et al., 2016) and UDS (Rudinger
et al., 2018), all with annotations on a [−3, 3]
scale. Each dataset has shared biLSTM weights
but specific regression parameters.

Reference datasets The FactBank, UW, and
MEANTIME datasets all consist of sentences from
news articles. Each event in FactBank was anno-
tated by 2 annotators, with 0.81 Cohen’s κ. UW
has 5 annotations for each event, and MEANTIME
has 6. UDS contains sentences from the English
Web Treebank (Bies et al., 2012), which con-
tains weblogs, newsgroups, emails, reviews, and
question-answers. It has 2 annotations for each
predicate, with 0.66 Cohen’s κ. All four datasets
have annotations biased towards +3, because (1)
they are newswire-heavy with sentences describ-
ing known factual events, and (2) most annotations
are for main-clause predicates instead of predi-
cates in an embedded clause.

Table 2 gives the number of predicates in each
dataset and state-of-the-art results obtained. Two
metrics were reported for both models: mean ab-
solute error (MAE), measuring the absolute fit,

# Predicate SoA
r MAE

FactBank 9,761 0.86 0.31
MEANTIME 1,395 0.61 0.23
UW 13,644 0.75 0.42
UDS 27,289 0.77 0.96

Table 2: The number of annotated predicates in each
dataset, and previous state-of-the-art performance. The
score on UW with MAE was obtained by Stanovsky
et al. (2017), while the other scores were obtained by
Rudinger et al. (2018).

and Pearson’s r correlation, measuring how well
the model captures variability in the data. Pear-
son’s r is considered more informative than MAE
because the reference sets are biased towards +3.

4 Evaluation

We evaluated the models of Stanovsky et al.
(2017) and Rudinger et al. (2018) on the Commit-
mentBank. We used Stanovsky et al. (2017)’s rule-
based annotator3 to get commitment ratings for
the embedded predicates of the target sentences.
Following Rudinger et al. (2018), we trained the
linear, tree, and hybrid biLSTM models using the
multi-task training scheme on the four factuality
datasets they used, which produced four predic-
tions. Following White et al. (2018), we used
cross-validated ridge regression to predict a final
score using the four predictions.

We include a majority baseline “All -2.0” (al-
ways predicting -2.0, since -2.0 is the most fre-
quent answer in the full and restricted Commit-
mentBank). The results are shown in Figure 3.
The rule-based model outperforms the biLSTM
models on the full set, but overall both SoA mod-
els do not perform very well on the Commitment-
Bank. As shown in Figure 3, the Commitment-
Bank is substantially more challenging for these
models than the reference datasets, with lower cor-
relation and higher absolute error rates than were
obtained for any of these other datasets.

5 Analysis

Focusing on the restricted set, we perform detailed
error analysis of the outputs of the rule-based and
hybrid biLSTM models, which achieved the best

3https://github.com/gabrielStanovsky/
unified-factuality

https://github.com/gabrielStanovsky/unified-factuality
https://github.com/gabrielStanovsky/unified-factuality


4211

Rule-b
ased Linear Tree Hybri

d
0

0.2

0.4

0.6

0.41

0.24

0.09

0.23

0.5

0.31

0.13

0.33

Pe
ar

so
n
r Full Restricted

All -2
.0

Rule-b
ased Linear Tree Hybri

d

1.5

2

2.5

3

2.14

1.54

2.42

1.61 1.67

2.2
2.04

1.95
2.05

1.87

M
A

E

Full Restricted

Figure 3: Pearson r correlation and Mean Absolute Error (MAE) on All -2.0 baseline, Rule-based annotator
(Stanovsky et al., 2017), and three biLSTM models in Rudinger et al. (2018). Pearson r is undefined for All -2.0.
All correlations are statistically significant (p < 0.05).

r MAE
Feature Value Rule Hybr. Rule Hybr.

Embedding

Cond. NA 0.02 2.08 1.50
Modal -0.01 0.21 † 1.37 1.08
Negation 0.45 † 0.22 † 2.26 2.40
Question -0.22 0.29 † 2.35 1.25

Genre
Fiction 0.25 † 0.18 † 1.72 1.47
Dialog 0.50 † 0.21 † 2.27 2.23
News 0.14 0.10 2.94 2.10

Factive
Yes -0.14 0.23 † 1.58 1.20
No 0.49 † 0.25 † 2.23 2.16

NegRaising 0.04 -0.07 1.91 2.77

Table 3: Performance and number of items per feature.
The scores in bold indicate the classes on which each
model has the best performance (with respect to both
metrics). † marks statistical significance of Pearson’s
correlation (p < 0.05).

correlation. Table 3 shows performance for the
following linguistic features, and Figure 4 shows
scatterplots of gold judgments vs. predictions.

Embedding environment The rule-based
model can only capture inferences involving
negation (r = 0.45), while the hybrid model
performs more consistently across negation,
modal, and question (r ∼ 0.25). Both models
cannot handle inferences with conditionals.

The model’s performance on the negation items
also vary with respect to genre: the rule-based
model has significant correlations for fiction (r =
0.45) and dialog (r = 0.32), while the hybrid model
has correlations between 0.05 and 0.2 for all three
genres, none reaching significance. About 40%
of the modal and question items involve factive
verbs, therefore the performance of these environ-
ments also correlate with the models’ performance
on factive verbs (elaborated on below).

Genre Both models achieve the best correla-
tion on dialog (Switchboard), and the worst on
newswire (WSJ). The poor performance on WSJ
might be due to its scores in CommitmentBank be-
ing more widespread (reflected in Figure 4) than
annotations in the reference datasets (e.g., MEAN-
TIME), which tend to be biased towards +3. The
good performance of the rule-based model on dia-
log could be due to the fact that 70% of the items
in dialog are in a negation environment with a non-
factive verb.

Factive embedding verb Lexicalist theories
(i.a., Karttunen 1973; Heim 1983) predict that
complements of factive verbs are commitments of
the speaker. This tendency is reflected in Fig-
ures 1 and 4 where most sentences with factives
have higher mean commitment scores. Both mod-
els get better MAE on factives, but better cor-
relation on nonfactives. The improved MAE of
the rule-based model might be due to its use of
factive/implicative signatures. However, the poor
correlations suggest that neither model can ro-
bustly capture the variability in inference which
exists in sentences involving factive/nonfactive
verbs (see i.a. Beaver 2010; de Marneffe et al.
2019).

Neg-raising Within sentences with negation, we
examine the models’ performance on sentences
with “neg-raising” reading, where a negation
in the matrix clause (not {think/believe} p) is
interpreted as negating the complement clause
(think/believe not p), as in example (3) in Table 1
where we understand the speaker to be commit-
ted to people should not be allowed to carry guns
in their vehicles. We identify “neg-raising” items
as items with a negation embedding environment,
think or believe verb, and a negative commitment
score. There is almost no correlation between both



4212

Figure 4: Gold scores vs. model prediction. Each point
is a sentence. Lines show perfect predictions.

models’ predictions and gold judgments (Table 3),
suggesting that the models are not able to capture
neg-raising inferences.

Model behavior Figure 4 shows that the hybrid
model predictions are mostly positive, whereas the
rule-based model predictions are clustered at −3
and +3. This suggests that the rule-based model
cannot capture the gradience present in commit-
ment judgments, while the hybrid model struggles
to recognize negative commitments.

To better interpret the models’ outputs, we eval-
uate them in a classification setting. We use Gaus-
sian mixture models to obtain three clusters for the
mean gold scores and the predictions of both mod-
els. We assign the cluster with the highest mean to
+: speaker is certain that the complement is true,
the one with the lowest mean to -: speaker is cer-
tain that it is false, and the remaining one to o:
speaker is not certain about its truth. We report
precision, recall and F1 in Table 4. The rule-based
model predicts + by default unless it has clear ev-
idence (e.g., negation) for negative commitment.
This behavior is reflected in the high precision for
-. Both models perform well on + and -, but nei-
ther is able to identify no commitment (o).

6 Conclusion

Our evaluation of two SoA models for speaker
commitment on the CommitmentBank shows that
the models perform better on sentences with nega-

Precision Recall F1 Count
Rule Hybr. Rule Hybr. Rule Hybr.

+ 0.58 0.64 0.91 0.51 0.71 0.56 251
- 0.99 0.67 0.55 0.20 0.70 0.31 268
o 0.00 0.06 0.00 0.46 0 0.11 37

Total 0.74 0.61 0.67 0.35 0.66 0.41 556

Table 4: Classification performance of the models.

tion, and with nonfactive embedding verbs. How-
ever, they are not able to generalize to other lin-
guistic environments such as conditional, modal,
and neg-raising, which display inference patterns
that are important for information extraction. Both
models are able to identify the polarity of commit-
ment, but cannot capture its gradience. The rule-
based model, outperforming the biLSTM mod-
els on the full CommitmentBank, shows that a
linguistically-informed model scales more suc-
cessfully to challenging naturalistic data.

In the long run, to perform robust language un-
derstanding, models will need to incorporate more
linguistic foreknowledge and be able to generalize
to a wider range of linguistic constructions.

Acknowledgment

We thank the anonymous reviewers for their valu-
able feedback. We also thank Micha Elsner, Cory
Shain, and the Clippers group at The Ohio State
University for their helpful discussions and sug-
gestions. This material is based upon work sup-
ported by the National Science Foundation under
Grant No. IIS-1464252.

References
David Beaver. 2010. Have you noticed that your

belly button lint colour is related to the colour of
your clothing? In Rainer Bäuerle, Uwe Reyle,
and Thomas Ede Zimmermann, editors, Presupposi-
tions and Discourse: Essays Offered to Hans Kamp,
pages 65–99. Leiden, The Netherlands: Brill.

Ann Bies, Justin Mott, Colin Warner, and Seth Kulick.
2012. English web treebank. Linguistic Data Con-
sortium, Philadelphia, PA.

Mona Diab, Lori Levin, Teruko Mitamura, Owen Ram-
bow, Vinodkumar Prabhakaran, and Weiwei Guo.
2009. Committed belief annotation and tagging.
In Proceedings of the Third Linguistic Annotation
Workshop, pages 68–73.

Irene Heim. 1983. On the projection problem for pre-
suppositions. In West Coast Conference on Formal
Linguistics (WCCFL) 2, pages 114–125.



4213

Lauri Karttunen. 1971. Some observations on factivity.
Papers in Linguistics, 4:55–69.

Lauri Karttunen. 1973. Presuppositions and compound
sentences. Linguistic Inquiry, 4(2):169–193.

Lauri Karttunen. 2012. Simple and phrasal implica-
tives. In Proceedings of the First Joint Confer-
ence on Lexical and Computational Semantics - Vol-
ume 1: Proceedings of the Main Conference and
the Shared Task, and Volume 2: Proceedings of the
Sixth International Workshop on Semantic Evalua-
tion, pages 124–131.

Kenton Lee, Yoav Artzi, Yejin Choi, and Luke Zettle-
moyer. 2015. Event detection and factuality assess-
ment with non-expert supervision. In Proceedings
of the 2015 Conference on Empirical Methods in
Natural Language Processing, pages 1643–1648.

Amnon Lotan, Asher Stern, and Ido Dagan. 2013.
TruthTeller: Annotating predicate truth. In Pro-
ceedings of the 2013 Conference of the North Amer-
ican Chapter of the Association for Computational
Linguistics: Human Language Technologies, pages
752–757.

Marie-Catherine de Marneffe, Mandy Simons, and Ju-
dith Tonhauser. 2019. The commitmentBank: Inves-
tigating projection in naturally occurring discourse.
In Sinn und Bedeutung 23.

Anne-Lyse Myriam Minard, Manuela Speranza, Ruben
Urizar, Begona Altuna, Marieke van Erp, Anneleen
Schoen, and Chantal van Son. 2016. MEANTIME,
the newsreader multilingual event and time corpus.
In Proceedings of the 10th International Confer-
ence on Language Resources and Evaluation (LREC
2016), pages 4417–4422.

Vinodkumar Prabhakaran, Tomas By, Julia Hirschberg,
Owen Rambow, Samira Shaikh, Tomek Strza-
lkowski, Jennifer Tracey, Michael Arrigo, Ru-
payan Basu, Micah Clark, Adam Dalton, Mona
Diab, Louise Guthrie, Anna Prokofieva, Stephanie
Strassel, Gregory Werner, Yorick Wilks, and Janyce
Wiebe. 2015. A new dataset and evaluation for be-
lief/factuality. In Proceedings of the Fourth Joint
Conference on Lexical and Computational Seman-
tics, pages 82–91.

Rachel Rudinger, Aaron Steven White, and Benjamin
Van Durme. 2018. Neural models of factuality. In
Proceedings of the 2018 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics, pages 731–744.

Roser Saurı́ and James Pustejovsky. 2009. FactBank:
A corpus annotated with event factuality. Language
resources and evaluation, 43(3):227.

Roser Saurı́ and James Pustejovsky. 2012. Are you
sure that this happened? Assessing the factuality de-
gree of events in text. Computational Linguistics,
38(2):261–299.

Mandy Simons, Judith Tonhauser, David Beaver, and
Craige Roberts. 2010. What projects and why. In
Proceedings of Semantics and Linguistic Theory 20.
CLC Publications.

Gabriel Stanovsky, Judith Eckle-Kohler, Yevgeniy
Puzikov, Ido Dagan, and Iryna Gurevych. 2017. In-
tegrating deep linguistic features in factuality pre-
diction over unified datasets. In Proceedings of the
55th Annual Meeting of the Association for Compu-
tational Linguistics (Volume 2: Short Papers), pages
352–357.

Aaron Steven White, Rachel Rudinger, Kyle Rawlins,
and Benjamin Van Durme. 2018. Lexicosyntactic
inference in neural models. In Proceedings of the
2018 Conference on Empirical Methods in Natural
Language Processing, pages 4717–4724.

http://dl.acm.org/citation.cfm?id=2387636.2387659
http://dl.acm.org/citation.cfm?id=2387636.2387659
http://aclweb.org/anthology/N13-1091
https://doi.org/10.18653/v1/P17-2056
https://doi.org/10.18653/v1/P17-2056
https://doi.org/10.18653/v1/P17-2056

