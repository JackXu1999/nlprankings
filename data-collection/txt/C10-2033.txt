285

Coling 2010: Poster Volume, pages 285–293,

Beijing, August 2010

An Efﬁcient Shift-Reduce Decoding Algorithm for Phrased-Based

Machine Translation

Yang Feng, Haitao Mi, Yang Liu and Qun Liu

Key Laboratory of Intelligent Information Processing

Institute of Computing Technology

Chinese Academy of Sciences

{fengyang,htmi,yliu,liuqun}@ict.ac.cn

Abstract

In statistical machine translation, decod-
ing without any reordering constraint is
an NP-hard problem. Inversion Transduc-
tion Grammars (ITGs) exploit linguistic
structure and can well balance the needed
ﬂexibility against complexity constraints.
Currently,
translation models with ITG
constraints usually employs the cube-time
CYK algorithm. In this paper, we present
a shift-reduce decoding algorithm that can
generate ITG-legal translation from left to
right in linear time. This algorithm runs
in a reduce-eager style and is suited to
phrase-based models. Using the state-of-
the-art decoder Moses as the baseline, ex-
periment results show that the shift-reduce
algorithm can signiﬁcantly improve both
the accuracy and the speed on different
test sets.

1 Introduction

languages,

In statistical machine translation, for the diver-
sity of natural
the word order of
source and target language may differ and search-
ing through all possible translations is NP-hard
(Knight, 1999). So some measures have to be
taken to reduce search space: either using a search
algorithm with pruning technique or restricting
possible reorderings.

Currently, beam search is widely used (Till-
mann and Ney, 2003; Koehn, 2004) to reduce
search space. However, the pruning technique
adopted by this algorithm is not risk-free. As a
result, the best partial translation may be ruled out

during pruning. The more aggressive the prun-
ing is, the more likely the best translation escapes.
There should be a tradeoff between the speed and
the accuracy. If some heuristic knowledge is em-
ployed to guide the search, the search algorithm
can discard some implausible hypotheses in ad-
vance and focus on more possible ones.

Inversion Transduction Grammars (ITGs) per-
mit a minimal extra degree of ordering ﬂexibility
and are particularly well suited to modeling or-
dering shifts between languages (Wu, 1996; Wu,
1997). They can well balance the needed ﬂex-
ibility against complexity constraints. Recently,
ITG has been successfully applied to statistical
machine translation (Zens and Ney, 2003; Zens
et al., 2004; Xiong et al., 2006). However, ITG
generally employs the expensive CYK parsing al-
gorithm which runs in cube time. In addition, the
CYK algorithm can not calculate language model
exactly in the process of decoding, as it can not
catch the full history context of the left words in a
hypothesis.

In this paper, we introduce a shift-reduce de-
coding algorithm with ITG constraints which runs
in a left-to-right manner. This algorithm parses
source words in the order of their corresponding
translations on the target side. In the meantime,
it gives all candidate ITG-legal reorderings. The
shift-reduce algorithm is different from the CYK
algorithm, in particular:

• It produces translation in a left-to-right man-
ner. As a result, language model probability
can be calculated more precisely in the light
of full history context.

• It decodes much faster. Applied with distor-

286

target side

(a) straight

target side

target side

(b) inverted

(c) discontinuous

Figure 1: Orientation of two blocks.

tion limit, shift-reduce decoding algorithm
can run in linear time, while the CYK runs
in cube time.

• It holds ITG structures generated during de-
coding. That is to say, it can directly give
ITG-legal spans, which leads to faster de-
coding. Furthermore, it can be extended to
syntax-based models.

We evaluated the performance of the shift-
reduce decoding algorithm by adding ITG con-
straints to the state-of-the-art decoder Moses. We
did experiments on three data sets: NIST MT08
data set, NIST MT05 data set and China Work-
shop on Machine Translation 2007 data set. Com-
pared to Moses, the improvements of the accuracy
are 1.59, 0.62, 0.8 BLEU score, respectively, and
the speed improvements are 15%, 24%, 30%, re-
spectively.

2 Decoding with ITG constraints

In this paper, we employ the shift-reduce algo-
rithm to add ITG constraints to phrase-based ma-
chine translation model.
It is different from the
traditional shift-reduce algorithm used in natural
language parsing. On one hand, as natural lan-
guage parsing has to cope with a high degree of
ambiguity, it need take ambiguity into considera-
tion. As a result, the traditional one often suffers
shift-reduce divergence. Nonetheless, the shift-
reduce algorithm in this paper does not pay atten-
tion to ambiguity and acts in a reduce-eager man-
ner. On the other hand, the traditional algorithm
can not ensure that all reorderings observe ITG
constraints, so we have to modify the traditional
algorithm to import ITG constraints.

We will introduce the shift-reduce decoding al-
gorithm in the following two steps: First, we

1
zairu1

‡2
shijian2

3
diaocha3

4
ziliaode4

5
diannao5

6
zaoqie6

The laptop with inquiry data

on the event was stolen

(a)

A11

A10

A9

A8

A7

A1

The laptop
diannao5

A2
with
zairu1

A3
inquiry
diaocha3

A4
data

A5

on the event

ziliaode4

shijian2

A6

was stolen
zaoqie6

(b)

Figure 2: A Chinese-to-English sentence pair and
its corresponding ITG tree.

will deduce how to integrate the shift-reduce al-
gorithm and ITG constraints and show its correct-
ness (Section 2.1). Second, we will describe the
shift-reduce decoding algorithm in details (Sec-
tion 2.2).

2.1 Adding ITG constraints

In the process of decoding, a source phrase is re-
garded as a block and a source sentence is seen
as a sequence of blocks. The orientation of two
blocks whose translations are adjacent on the tar-
get side can be straight, inverted or discontinu-
ous, as shown in Figure 1. According to ITG,
two blocks which are straight or inverted can be
merged into a single block. For parsing, differ-
ent mergence order of a sequence of continuous
blocks may yield different derivations.
In con-
trast, the phrase-based machine translation does
not compute reordering probabilities hierarchi-
cally, so the mergence order will not impact the
computation of reordering probabilities. As a
result, the shift-reduce decoding algorithm need
not take into consideration the shift-reduce diver-
gence. It merges two continuous blocks as soon
as possible, acting in a reduce-eager style.

Every ITG-legal sentence pair has a corre-

287

S zairu1 shijian2 diaocha3 ziliaode4 diannao5 zaoqie6

S zairu1 shijian2 diaocha3 ziliaode4 diannao5 zaoqie6

S zairu1 shijian2 diaocha3 ziliaode4 diannao5 zaoqie6

The laptop

The laptop with

The laptop with inquiry

(a)

(b)

(c)

S zairu1 shijian2 diaocha3 ziliaode4 diannao5 zaoqie6

S zairu1 shijian2 diaocha3 ziliaode4 diannao5 zaoqie6

S zairu1 shijian2 diaocha3 ziliaode4 diannao5 zaoqie6

The laptop with inquiry data

The laptop with inquiry data

The laptop with inquiry data on the event

(d)

(e)

(f)


S zairu1 shijian2 diaocha3 ziliaode4 diannao5 zaoqie6

S zairu1 shijian2 diaocha3 ziliaode4 diannao5 zaoqie6

S zairu1 shijian2 diaocha3 ziliaode4 diannao5 zaoqie6

The laptop with inquiry data on the event

The laptop with inquiry data on the event

The laptop with inquiry data on the event

(g)

(h)

(i)

Figure 3: The partial translation procedure of the sentence in Figure 2.

sponding ITG tree, and source words covered
by every node (eg. A1, ..., A11 in Figure 2(b))
in the ITG tree can be seen as a block. By
watching the tree in Figure 2, we can ﬁnd that
a block must be adjacent to the block either on
its left or on its right, then they can be merged
into a larger block. For example, A2 matches
the block [zairu1] and A8 matches the block
[shijian2 diaocha3 ziliaode4].1 The two blocks
are adjacent and they are merged into a larger
block [zairu1 shijian2 diaocha3 ziliaode4],
covered by A9. The procedure of translating
zairu1 shijian2 diaocha3 ziliaode4 diannao5
is illustrated in Figure 3.

For example,

For a hypothesis during decoding, we assign it
the current block, the left neigh-
three factors:
boring uncovered span and the right neighbor-
ing uncovered span.
in Figure
3(c), the current block is [diaocha3] and the left
neighboring uncovered span is [shijian2] and the
right neighboring uncovered span is [ziliaode4].
[zaoqie6] is not thought of as the right neighbor-
ing block, for it is not adjacent to [diaocha3]. The
next covered block is [ziliaode4] (as shown in
Figure 3(d)). For [diaocha3] and [ziliaode4] are
adjacent, they are merged. In Figure 3(e), the cur-
rent block is [diaocha3 ziliaode4].

A sentence is translated with ITG constraints iff

1The words within a block are sorted by their order in the

source sentence.

its source side can be covered by an ITG tree. That
is to say, for every hypothesis during decoding, the
next block to cover must be selected from the left
or right neighboring uncovered span.

First, we show that if the next block to cover is
selected in this way, the translation must observe
ITG constraints. For every hypothesis during de-
coding, the immediate left and right words of the
current block face the following three conditions:
(1) The immediately left word is not covered
and the immediately right word is covered, then
the next block to cover must be selected from the
left neighboring uncovered span, eg. for the cur-
rent block [diaocha3 ziliaode4] in Figure 3(e). In
this condition, the ITG tree can be constructed in
the following two ways: either all words in the left
neighboring uncovered span are translated ﬁrst,
then this span is merged with the current span
(taking three nodes as an example, this case is
shown in Figure 4(a)), or the right part of the left
neighboring uncovered span is merged with the
current block ﬁrst, then the new block is merged
with the rest part of the left neighboring uncov-
ered span (shown in Figure 4(b)). In a word, only
after all words in the left neighboring uncovered
span are covered, other words can be covered.

(2) The immediately right word is not covered
and the immediately left word is covered. Simi-
larly, only after all words in the right neighboring
uncovered span are covered, other words can be

288

• St: the stack for covered blocks. The blocks
are pushed in the order that they are covered,
not the order that they are in the source sen-
tence.

• Sl : the stack for the left uncovered spans of
the current block. When a block is pushed
into St, its corresponding left neighboring
uncovered span is pushed into Sl.

• Sr :the stack for the right uncovered spans of
the current block. When a block is pushed
into St, its corresponding right neighboring
uncovered span is pushed into Sr.

A translation conﬁguration is a triple c =
hSt, Sl, Sri. Given a source sentence f =
f1, f2, ..., fm, we import a virtual start word and
the whole translation procedure can be seen as
a sequence of transitions from cs to ct, where
[1, m]i is the initial conﬁgura-
cs = h[0], ∅,
tion, ct = h[0, m], ∅, ∅i is the terminal con-
ﬁguration. The conﬁguration for Figure 3 (e) is
h[0][5][1][3, 4], [2], [6]i.

We deﬁne three types of

transitions from
a conﬁguration to another . Assume the cur-
rent conﬁguration c = h [ft11, ft12]...[ftk1, ftk2],
[fl11, fl12]...[flu1, flu2], [frv1, frv2]...[fr11, fr12] i,
then :

• Transitions LShift pop the top element
[flu1, flu2] from Sl and select a block [i, j]
In addition,
from [flu1, flu2] to translate.
they push [i, j] into St, and if i 6= flu1, they
push [flu1, i − 1] into Sl, and if j 6= flu2,
they push [j + 1, flu2] into Sr. The precondi-
tion to operate the transition is that Sl is not
null and the top span of Sl is adjacent to the
top block of St. Formally, the precondition
is flu2 + 1 = ftk1.

• Transitions RShift pop the top element
[frv1, frv2] of Sr and select a block [i, j]
In addition,
from [frv1, frv2] to translate.
they push [i, j] into St, and if i 6= frv1, they
push [frv1, i−1] into Sl, and if j 6= frv2, they
push [j + 1, frv2] into Sr. The precondition
is that Sr is not null and the top span of Sr is

(a)

(b)

Figure 4: The two ways that the current block is
merged with its left neighboring uncovered span.
The third node in the ﬁrst row denotes the current
block, the ﬁrst and second nodes in the ﬁrst row
denote left and right parts of the left neighboring
uncovered span, respectively.

covered.

(3) The immediately left and right words are
neither covered. The next block can be selected
from either the left or the right neighboring uncov-
ered span until the immediate left or right word is
covered.

The above operations can be performed recur-
sively until the whole source sentence is merged
into a single block, so the reordering observes ITG
constraints.

Now, we show that translation which is not gen-
erated in the above way must violate ITG con-
straints.

If the next block is selected out of the neighbor-
ing uncovered spans, the current block can be nei-
ther adjacent to the last covered block nor adjacent
to the selected next block, so the current block can
not be merged with any block and the whole sen-
tence can not be covered by an ITG tree. As in
Figure 3(b), if the next block to cover is [zaoqie6],
then [zairu1] is neither adjacent to [diannao5]
nor adjacent to [zaoqie6].

We can conclude that if we select the next block
from the left or right neighboring uncovered span
of the current block, then the translation must ob-
serve ITG constraints.

2.2 Shift-Reduce Decoding Algorithm
In order to generate the translation with ITG con-
straints, the shift-reduce algorithm have to keep
trace of covered blocks, left and right neighboring
uncovered spans. Formally, the shift-reduce de-
coding algorithm uses the following three stacks:

289

adjacent to the top block of St. Formally, the
precondition is ftk2 + 1 = frv1.

• Transitions Reduce pop the top two blocks
[ftk−11, ftk−12], [ftk1, ftk2] from St and push
the merged span [ftk−11, ftk2] into St. The
precondition is that the top two blocks are ad-
jacent. Formally, the precondition is ftk−12 +
1 = ftk1

The transition sequence of the example in Fig-
ure 2 is listed in Figure 5. For the purpose of
efﬁciency, transitions Reduce are integrated with
transitions LShift and RShift in practical imple-
mentation. Before transitions LShift and RShift
push [i, j] into St, they check whether [i, j] is ad-
jacent to the top block of St. If so, they change
the top block into the merged block directly.

In practical implementation, in order to further
restrict search space, distortion limit is applied be-
sides ITG constraints: a source phrase can be cov-
ered next only when it is ITG-legal and its distor-
tion does not exceed distortion limit. The distor-
tion d is calculated by d = |starti − endi−1 − 1|,
where starti is the start position of the current
phrase and endi−1 is the last position of the last
translated phrase.

3 Related Work

Galley and Manning (2008) present a hierarchi-
cal phrase reordering model aimed at improving
non-local reorderings. Via the hierarchical mer-
gence of two blocks, the orientation of long dis-
tance words can be computed. Their shift-reduce
algorithm does not import ITG constraints and ad-
mits the translation violating ITG constraints.

(2004)

Zens et al.

introduce a left-to-
right decoding algorithm with ITG constraints
on the alignment template system (Och et al.,
1999).
Their algorithm processes candidate
source phrases one by one through the whole
search space and checks if the candidate phrase
complies with ITG constraints. Besides, their al-
gorithm checks validity via cover vector and does
not formalize ITG structure. The shift-reduce de-
coding algorithm holds ITG structure via three
stacks. As a result, it can offer ITG-legal spans
directly and decode faster. Furthermore, with

Transition St
[0]
RShift
[0][5]
LShift
[0][5][1]
RShift
[0][5][1][3]
RShift
[0][5][1][3][4]
Reduce [0][5][1][3, 4]
LShift
Reduce [0][5][1][2, 4]
Reduce [0][5][1, 4]
Reduce [0][1, 5]
Reduce [0, 5]
RShift
Reduce [0, 6]

[0, 5][6]

[0][5][1][3, 4][2]

Sl
∅
[1, 4]
∅
[2]
[2]
[2]
∅
∅
∅
∅
∅
∅
∅

Sr
[1, 6]
[6]
[2, 4][6]
[4][6]
[6]
[6]
[6]
[6]
[6]
[6]
[6]
∅
∅

Figure 5: Transition sequence for the example in
Figure 2. The top nine transitions correspond to
Figure 3 (a), ... , Figure 3 (i), respectively.

the help of ITG structure, it can be extended to
syntax-based models easily.

Xiong et al.

(2006) propose a BTG-based
model, which uses the context to determine the
orientation of two adjacent spans. It employs the
cube-time CYK algorithm.

4 Experiments

We compare the shift-reduce decoder with the
state-of-the-art decoder Moses (Koehn et al.,
2007).
The shift-reduce decoder was imple-
mented by modifying the normal search algo-
rithm of Moses to our shift-reduce algorithm,
without cube pruning (Huang and Chiang, 2005).
We retained the features of Moses:
four trans-
lation features, three lexical reordering features
(straight, inverted and discontinuous), linear dis-
tortion, phrase penalty, word penalty and language
model, without importing any new feature. The
decoding conﬁgurations used by all the decoders,
including beam size, phrase table limit and so on,
were the same, so the performance was compared
fairly.

First, we will show the performance of shift-
reduce algorithm on three data sets with large
training data sets (Section 4.1). Then, we will
analyze the performance elaborately in terms of
accuracy, speed and search ability with a smaller

290

training data set (Section 4.2). All experiments
were done on Chinese-to-English translation tasks
and all results are reported with case insensitive
BLEU score. Statistical signiﬁcance were com-
puted using the sign-test described in Collins et
al. (Collins et al., 2005).

4.1 Performance Evaluation
We did three experiments to compare the perfor-
mance of the shift-reduce decoder, Moses and the
decoder with ITG constraints using cover vector
(denoted as CV). 2 The shift-reduce decoder de-
coded with two sets of parameters: one was tuned
by itself (denoted as SR) and the other was tuned
by Moses (denoted as SR-same), using MERT
(Och, 2003). Two searching algorithms of Moses
are considered: one is the normal search algorithm
without cubing pruning (denoted as Moses), the
other is the search algorithm with cube pruning
(denoted as Moses-cb). For all the decoders, the
distortion limit was set to 6, the nbest size was set
to 100 and the phrase table limit was 50.

In the ﬁrst experiment, the development set is
part of NIST MT06 data set including 862 sen-
tences, the test set is NIST MT08 data set and
the training data set contains 5 million sentence
pairs. We used a 5-gram language model which
were trained on the Xinhua and AFP portion of
the Gigaword corpus. The results are shown in
Table 1(a).

In the second experiment, the development data
set is NIST MT02 data set and the test set is NIST
MT05 data set. Language model and the training
data set are the same to that of the ﬁrst experiment.
The result is shown in Table 1(b).

In the third experiment, the development set
is China Workshop on Machine Translation 2008
data set (denoted as CWMT08) and the test set
is China Workshop on Machine Translation 2007
data set (denoted as CWMT07). The training set
contains 2 Million sentence pairs and the language
model are a 6-gram language model trained on
the Reuter corpus and English corpus. Table 1(c)
gives the results.

In the above three experiments, SR decoder

2The decoder CV is implemented by adding the ITG con-
straints to Moses using the algorithm described in (Zens et
al., 2004).

NIST06 NIST08
30.24
Moses
30.27
Moses-cb
CV
30.35
SR-same ——
30.47
SR

25.08
23.80
26.23**
25.09
26.67**

speed
4.827
1.501
4.335
3.856
4.126

(a)

NIST02 NIST05
35.68
Moses
35.42
Moses-cb
CV
35.45
SR-same ——
SR

35.80
35.03
36.56**
35.84
36.42**

35.99*

(b)

CWMT08 CWMT07
27.75
Moses
27.82
Moses-cb
CV
27.71
SR-same ——
SR

25.91
25.16
26.58**
25.97
26.71**

28.14*

speed
7.142
1.811
6.276
5.008
5.432

speed
3.061
0.548
2.331
1.988
2.106

(c)

Table 1: Performance comparison. Moses: Moses
without cube pruning, Moses-cb: Moses with
cube pruning, CV: the decoder using cover vector,
SR-same: the shift-reduce decoder decoding with
parameters tunes by Moses, SR: the shift-reduce
decoder with parameters tuned by itself. The sec-
ond column stands for develop set, the third col-
umn stands for test set and speed column shows
the average time (seconds) of translating one sen-
tence in the test set. **: signiﬁcance at the .01
level.

improves the accuracy by 1.59, 0.62, 0.8 BLEU
score (p < .01), respectively, and improves the
speed by 15%, 24%, 30%, respectively. we can
see that SR can improve both the accuracy and
the speed while SR-same can increase the speed
signiﬁcantly with a slight improvement on the ac-
curacy. As both SR and CV decode with ITG
constraints, they match each other on the accu-

291

U
E
L
B

30.00

29.50

29.00

28.50

28.00

27.50

27.00

SR
SR-same
Moses

d=-1

d=-1

 1  2  3  4  5  6  7  8  9  10  11  12  13  14  15  16  17

average decoding speed (s)

d=-1

Figure 6: Performance comparison on NIST05.
For a curve, the dots correspond to distortion limit
4, 6, 8, 10, 14 and no distortion from left to right.
d = −1 stands for no distortion limit.

racy. However, the speed of SR is faster than CV.
Cube pruning can improve decoding speed dra-
matically, but it is not risk-free pruning technol-
ogy, so the BLEU score declines obviously.

4.2 Performance Analysis

We make performance analysis with the same ex-
periment conﬁguration as the second experiment
in Section 4.1, except that the training set in
the analysis experiment is FBIS corpus, includ-
ing 289k sentence pairs. In the following exper-
iments, Moses employs the normal search algo-
rithm without cube pruning.

the distortion limit will

For the decoders employ the linear distortion
feature,
inﬂuence the
translation accuracy. Besides, with different dis-
tortion limit, the proportion of ITG-legal transla-
tion generated by Moses will differ. The smaller
the distortion limit is, the greater the proportion is.
So we ﬁrst compare the performance with differ-
ent distortion limit.

We compare the shift-reduce decoder with
Moses using different distortion limit. The re-
sults are shown in Figure 6. When distortion limit
is set to 6, every decoder gets a peak value and
SR has an improvement of 0.66 BLEU score over
Moses. From the curves, we can see that the
BLEU score of SR-same with distortion limit 8

37.00
36.50
36.00
35.50
35.00
34.50
34.00
33.50
33.00
32.50
32.00
31.50
31.00
30.50
30.00
29.50
29.00
28.50
28.00

28.00

27.50

27.00

26.50

26.00

25.50

25.00

U
E
L
B

U
E
L
B

SR
SR-same
Moses

 4

 6

 8

 10

 12

 14

 16

distortion limit
ITG set

(a)

SR
SR-same
Moses

 4

 6

 8

 10

 12

 14

 16

distortion limit

(b)

rest set

Figure 7: Accuracy comparison on the ITG set
and rest set of NIST05. The ITG set includes the
sentences the translations of which generated by
Moses are ITG-legal, and the rest set contains the
rest sentences. distortion limit = 16 denotes no
distortion limit.

is lower than that of Mose with distortion limit
6. This is because the decoding speed of SR-
same with distortion limit 8 is not faster than that
of Moses with distortion limit 6. On the whole,
compared to Moses, SR-same can improve the ac-
curacy slightly with much faster decoding speed,
and SR can obtain improvements on both the ac-
curacy and the speed.

We split the test set into two sets: one contains
the sentences, the translations of which generated
by Moses are ITG-legal (denoted as ITG set) and
the other contains the rest (denoted as rest set).
From Figure 7, we can see that no matter on the
ITG set or on the rest set, SR decoder can gain ob-
vious accuracy improvements with all distortion

292

d

4
6
8
10
12
14
-1

Moses SR-same
28.67
31.34
32.59
34.36
33.16
35.98
34.13

28.68
31.42
32.93*
34.99**
33.61**
36.25*
34.96**

ITG
total < =
1050
758
594
456
454
383
351

8
51
72
80
63
60
39

1042
705
516
365
380
316
308

> Moses SR-same
0
2
6
11
11
7
4

25.82
25.72
25.65
26.50*
27.13
26.67*
26.78**

25.61
25.78
25.68
26.04
27.01
26.35
26.17

rest

total <
0
32
32
324
488
82
147
626
165
628
699
203
154
731

= >
32
0
290
2
3
403
476
3
462
1
1
495
577
0

Table 2: Search ability comparison. The ITG set and the rest set of NIST05 were tested, respectively.
On the ITG set, the following six factors are reported from left to right: BLEU score of Moses, BLEU
score of SR-same, the number of sentences in the ITG set, the number of sentences the translation
probabilities of which computed by Moses, compared to that computed by SR, is lower, equal and
greater. The rest set goes similarly. *: signiﬁcance at the .05 level, **: signiﬁcance at the .01 level.

limit. While SR-same decoder only gets better re-
sults on the ITG set with all distortion limit. This
may result from the use of the linear distortion
feature. Moses may generate hypotheses the dis-
tortion of which is forbidden in the shift-reduce
decoder. This especially sharpens on the rest set.
So SR-same may suffer from an improper linear
distortion parameter.

The search ability of Moses and the shift-
reduce decoder are evaluated, too. The translation
must be produced with the same set of parameters.
In our experiments, we employed the parameters
tuned by Moses. The test was done on the ITG and
the rest set, respectively. The results are shown in
Table 2. As the distortion limit becomes greater,
the number of the ITG-legal translation generated
by Moses becomes smaller. On the ITG set, trans-
lation probabilities from the shift-reduce decoder
is either greater or equal to that from Moses on
most sentences, and BLEU scores of shift-reduce
decoder is greater than that of Moses with all
distortion limit. Although the search space of
shift-reduce decoder is smaller than that of Moses,
shift-reduce decoder can give the translation that
Moses can not reach. On the rest set, for most sen-
tences, the translation probabilities from Moses is
greater than that from shift-reduce decoder. But
only when distortion limit is 6 and 8, the BLEU
score of Moses is greater than that of the shift-
reduce decoder. We may conclude that greater
score does not certainly lead to greater BLEU
score.

5 Conclusions and Future Work

In this paper, we present a shift-reduce decod-
ing algorithm for phrase-based translation model
that can generate the ITG-legal translation in lin-
ear time. The algorithm need not consider shift-
reduce divergence and performs reduce operation
as soon as possible. We compare the performance
of the shift-reduce decoder with the state-of-the-
art decoder Moses. Experiment results show that
the shift-reduce algorithm can improve both the
accuracy and the speed signiﬁcantly on different
test sets. We further analyze the performance and
ﬁnd that on the ITG set, the shift-reduce decoder
is superior over Moses in terms of accuracy, speed
and search ability, while on the rest set, it does
not display advantage, suffering from improper
parameters.

Next, we will extend the shift-reduce algorithm
to syntax-based translation models, to see whether
it works.

6 Acknowledgement

The authors were supported by National Natural
Science Foundation of China Contract 60736014,
National Natural Science Foundation of China
Contract 60873167 and High Technology R&D
Program Project No. 2006AA010108. We are
grateful
to the anonymous reviewers for their
valuable comments.

293

Zens, Richard, Hermann Ney, Taro Watanable, and
Eiichiro Sumita.
2004. Reordering constraints
for phrase-based statistical machine translation. In
Proc. of COLING, pages 205–211.

References
Collins, Michael, Philipp Koehn, and Ivona Kucerova.
2005. Clause restructuring for statistical machine
translation. In Proc. of ACL, pages 531–540.

Galley, Michel and Christopher D. Manning. 2008. A
simple and effective hierarchical phrase reordering
model. In Proc. of EMNLP, pages 848–856.

Huang, Liang and David Chiang. 2005. Better k-best
parsing. In Proceedings of the Ninth International
Workshop on Parsing Technologies (IWPT), pages
53–64.

Knight, Kevin. 1999. Decoding complexity in word-
replacement translation models. Computational
Linguistics, 25:607–615.

Koehn, Philipp, Hieu Hoang, Alexandra Birch Mayne,
Christopher Callison-Burch, Marcello Federico,
Nicola Bertoldi, Brooke Cowan, Wade Shen, Chris-
tine Moran, Richard Zens, Chris Dyer, Ondrej Bo-
jar, Alexandra Constantin, and Evan Herbst. 2007.
Moses: Open source toolkit for statistical machine
translation. In Proc. of the 45th ACL, Demonstra-
tion Session.

Koehn, Philipp. 2004. Pharaoh: A beam search de-
coder for phrased-based statistical machine transla-
tion. In Proc. of AMTA, pages 115–124.

Och, Frans J., Christoph Tillmann, and Hermann Ney.
1999.
Improved alignment models for statistical
machine translation. In Proc. of EMNLP, pages 20–
28.

Och, Frans J. 2003. Minimum error rate training in
In Proc. of ACL,

statistical machine translation.
pages 160–167.

Tillmann, Chirstoph and Hermann Ney.

2003.
Word reordering and a dynamic programming beam
search algorithm for statistical machine translation.
Computational Linguistics, 29:97–133.

Wu, Dekai. 1996. A polynomial-time algorithm for
In Proc. of ACL,

statistical machine translation.
pages 152–158.

Wu, Dekai. 1997. Stochastic inversion transduction
grammars and bilingual parsing of parallel corpora.
Computational Linguistics, 23:377–403.

Xiong, Deyi, Qun Liu, and Shouxun Lin. 2006. Maxi-
mum entropy based phrase reordering model for sta-
tistical machine translation. In Proc. of ACL, pages
521–528.

Zens, Richard and Hermann Ney. 2003. A compara-
tive study on reordering constraints in statistical ma-
chine translation. In Proc. of ACL, pages 144–151.

