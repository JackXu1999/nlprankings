



















































A Two-level Classifier for Discriminating Similar Languages


Proceedings of the Joint Workshop on Language Technology for Closely Related Languages, Varieties and Dialects, pages 73–77,
Hissar, Bulgaria, September 10, 2015. c©2015 Association for Computational Linguistics

A two-level classifier for discriminating similar languages

Judit Ács
Budapest University of

Technology and Economics
judit@aut.bme.hu

László Grad-Gyenge
Eötvös Loránd University
laszlo.grad-gyenge

@creo.hu

Thiago Bruno
Rodrigues de Rezende Oliveira

Universidade Federal do
Vale do São Francisco

thiago brro@hotmail.com

Abstract

The BRUniBP team’s submission is pre-
sented for the Discriminating between
Similar Languages Shared Task 2015. Our
method is a two phase classifier that uti-
lizes both character and word-level fea-
tures. The evaluation shows 100% accu-
racy on language group identification and
93.66% accuracy on language identifica-
tion. The main contribution of the paper is
a memory-efficient correlation based fea-
ture selection method.

1 Introduction

The discrimination of similar languages (DSL)
(Zampieri et al., 2015) can be defined as the sub-
task of the language identification (LI) problem.
LI is a fundamental task in the area of natural lan-
guage processing (NLP). The primary goal of LI
is to determine the language of a written text. In
practical applications, LI acts as a preprocessor of
various NLP techniques as for example machine
translation, sentiment analysis or even web search.
LI is currently an actively researched topic, DSL
is also in the focus of interest (Tiedemann and
Ljubešić, 2012).

Unlike well-separated languages, multilingual-
ism, varieties or dialects of language can seriously
degrade the quality of LI. DSL, noisy data, non-
well-formatted text, short sentences, mixed lan-
guage (i.e. tweets) are other examples of challeng-
ing problems in this field. In this paper we focus
on the DSL problem on a shared task. Our experi-
ment shows that discrimination between Bosnian,
Croatian and Serbian and between Argentinian
and Peninsular Spanish are the most challenging
tasks for our methods.

Most state of the art methods solve the DSL
task in two phases. In the first phase the language
group is to be identified, in the second phase the

language is to be selected. The first decision of
the model is more coarse and high level, the sec-
ond labelling is to be more specialized as differ-
ent language groups have different separating fea-
tures. Regarding the information representation,
most methods work with statistical features of the
source text. The statistical features are n-grams at
the word and at the character level. The parameter
n in the fixed-length character and word n-gram
models ranges from 1 to 6.

In our approach a maximum entropy classifier
and SVM with different kernels were evaluated.
The results show that maximum entropy delivers
comparable results to SVM while it is consider-
ably faster. To tackle the issue of zero probabil-
ities resulting from unseen n-grams, Katz’s back-
off smoothing (Katz, 1987) is applied. Training
a classifier on a large number of features requires
substantial computing resources, which we do not
have readily accessible. Features are pruned to
less than 10,000 according to their pairwise Pear-
son correlation with the labels. The code is avail-
able on GitHub.1

Section 2 presents related work. Section 3 de-
scribes the dataset the methods are evaluated on.
Section 4 provides an overview of the architec-
ture of our method and describes the classifica-
tion method. Section 5 gives insight into how the
text is preprocessed before calculating the statis-
tical features. Section 6 presents the evaluation
results. Section 7 concludes the paper.

2 Related work

Most of the DSL methods have a two phase archi-
tecture. The first level is to determine the language
group, the second level is to discriminate within
the language group.

(Porta and Sancho, 2014) utilize maximum en-
tropy models for the DSL task. The first classifier

1http://github.com/juditacs/dsl

73



determines the language group, the second works
with empirically selected features that achieved
best performance for the specific language group.
(Lui et al., 2014) also define a two phase ap-
proach involving a POS-tagger. (Goutte et al.,
2014) label the language group with a probabilis-
tic model based on word co-occurrences in doc-
uments. To discriminate at the language group
level, SVM based classification is used. (King et
al., 2014) compare naı̈ve Bayes, logistic regres-
sion and SVM based classifiers. They also prepro-
cess the data with manually defined methods as
named entity removal and English word removal.
(Purver, 2014) introduces a single-level approach,
training a linear SVM on word and character n-
grams of length 1-3.

3 Dataset

Our method is evaluated on the DSLCC
dataset (Tan et al., 2014), which dataset is
provided for the shared task. As Tan et al. de-
scribe the collection and the preparation of the
dataset in detail, we only provide a summary in
Table 1. The dataset contains 6 language groups
of closely related languages and dialects plus
one group called other. The language groups
are presented in the first column (Group) of the
table. The second column (Language) identifies
the language, the third column (code) contains a
short identifier for each language.

For each language, the dataset consists of
20 000 sentences. Each list of sentences is divided
into two parts as 18 000 sentence training sample
and 2 000 sentence development sample.

4 Method

To solve the shared task, we introduce a two-level
architecture. On the first level we utilize a clas-
sifier to distinguish between the language groups.
We refer to this classifier later as inter-group clas-
sifier. The inter-group classifier is described in
Section 4.1. To conduct a more specialized deci-
sion, to distinguish between the languages in a lan-
guage group, a second-level classifier is utilized.
This classifier is titled the intra-group classifier
and is described in Section 4.2

4.1 Inter-group classifier

Although the dataset contains 7 language groups,
we trained the classifier on 14 labels according to
the languages (instead of groups) and grouped the

Group Language Code

A
Bulgarian bg

Macedonian mk

B
Bosnian bs
Croatian hr
Serbian sr

C
Czech cs
Slovak sk

D
Argentinian Spanish es-AR
Peninsular Spanish es-ES

E
Brazilian Portuguese pt-BR
European Portuguese pt-PT

F
Malay my

Indonesian id

X other xx

Table 1: Language groups and languages

corresponding labels together according to the lan-
guage groups.

From the variety of features tested (see Sec-
tion 4.2), tf-idf delivered the best results for the
inter-group classification. Although tf-idf is a
more common method for information retrieval
tasks, it can also be defined for the current task
as follows

document set of all sentences in one language,

term one word, see Section 5 for details,

query one test sentence.

The inter-group classifier operates in two steps.
In the first step the top 100,000 keywords are ex-
tracted for each language. In the second step the
weighted sum of keywords is computed for each
sentence in each language and the language with
the highest score is chosen.

The inter-group classifier provides 100% accu-
racy on language group identification. Regarding
language labelling, the accuracy of the inter-group
classifier is 92.54%.

We used the following tf , idf and qf weight-
ings:

tf t,d = log(1 + ft,d),

where ft,d is the raw frequency of a term in all
sentences in a language.

74



idf t = log(1 +
N

nt
),

where N is the number of languages and nt is the
number of languages in which term t appears and

qf t =
(

0.5 + 0.5
ft,q

maxt ft,q

)
× log N

nt
,

where qf t is the weight of term t, maxt ft,q is the
highest tf score for term t in any language.

4.2 Intra-group classifier
The language groups are refined by the intra-group
classifier further. In the case of groups A, C, D, E
and F, there are 2 languages two distinguish be-
tween. In the case of group B, there are 3 lan-
guages to label. In the case of “group” X there is
only 1 language, the intra-group classifier is not
used in this case.

Various features are extracted and 6 models are
trained for the 6 groups. Character and word n-
grams, Katz’s backoff scores, tf-idf scores and
stopword n-grams are used as features.

4.2.1 N-grams
Character n-grams proved to be the most promi-
nent feature in last year’s DSL task (Zampieri et
al., 2014). However, the number of character n-
grams grows exponentially with n in theory and
subexponentially in practice but it still results in a
large number of features. This is the reason why
we involved feature selection.

Since PCA and other popular dimension reduc-
tion methods are very memory-intensive, Pear-
son correlation is involved as a feature selection
method. To select the most relevant features, for
each feature, the absolute value of the Pearson cor-
relation with the labels is calculated and based on
this value, the top n features are selected.

4.2.2 Katz’s backoff smoothing
Our baseline system is an implementation of
Katz’s backoff smoothing with training option that
works well in the general setting.2 It is possible to
train and test with this system up to n = 4 grams
with reasonable memory consumption. For further
memory-saving, see Section 5.

There are several variants of Katz’s backoff
smoothing, the one used here discounts the Max-
imum Likelihood estimations with a constant fac-

2https://github.com/juditacs/langid

tor and distributes the leftover probability mass ac-
cording to lower order n-grams.

Pbo(cn|c1, . . . , cn) =

=

{
C(c1,...,cn)−d
C(c1,...,cn−1) , if C(c1, . . . , cn) > 0

αc1,...,cn−1Pbo(cn|c2, . . . , cn−1)otherwise,

where αc1,...,cn−1 is the left-over probability
mass from discounting:

αc1,...,cn−1 = 1−
∑

cn:C(c1,...,cn)>0

C(c1, . . . , cn)− d
C(c1, . . . , cn−1)

.

The probabilities for all the languages are calcu-
lated on n-grams of various size, n is ranging from
1 to 4. The language with the highest probability is
selected. Both the probabilities and the language
are used as features and are passed to the intra-
group classifier.

4.2.3 Tf-idf
Similarly to the case of the inter-group classi-
fier, tf-idf scores are calculated for each language
group. The language group specific tf-idf scores
are based on the sentences only in the specific lan-
guage group. The tf-idf scores are the used as fea-
tures later for the intra-group classification.

4.2.4 Word bigrams
Word bigrams are extracted and after selection are
used as features. The selection of word bigrams is
similar to the selection of the character n-grams.
The absolute value of Pearson correlation of the
bigrams with the labels is calculated and then the
top n bigrams are selected. In our experiment n is
set to 1 000.

4.2.5 Stopwords
Although language varieties may use virtually the
same vocabulary, we assume that common expres-
sions, word and clause order may differ and the
order of stopwords reflects this difference. In each
language, we filtered the corpus to its 200 most
frequent words, most of which are stopwords. The
filtered sentences were fed as input to the tf-idf
(see Section 4.1) and the word bigram extractor
(see Section 4.2.4), resulting in a much smaller
feature number, therefore no feature selection was
necessary.

75



Accuracy

run1 0.9331428571
run2 0.9366428571
run3 0.9348571429

Table 2: Overall accuracy of our three runs

4.2.6 Classifier
Scikit-learn’s (Pedregosa et al., 2011) maximum
entropy and SVM based classifiers are evaluated.
In the case of SVM, different kernels are utilized.
Due to space limitations and focus we do not pub-
lish these evaluation results. As the maximum en-
tropy classifier delivers comparable results to the
SVM based method, we use the maximum entropy
classifier as it is considerably faster.

5 Preprocessing and tokenization

In order to reduce the number of components that
do not contribute much to language identification,
the corpus is preprocessed before feature extrac-
tion. The preprocessing pipeline consists of the
following steps

lowercasing Python’s unicode.lower func-
tion is used

puncutation filtering standard punctuation
(Python’s string.punctuation) and
additional quotation symbols are removed

whitespace normalization multiple consecutive
whitespaces in the same sentence are re-
placed with a single space

digit replacement numbers are replaced with a
single 0

All steps are applied before feature extraction
except in the case of tf-idf, where lowercasing is
not performed.

The preprocessed text is tokenized with NLTK
(Bird, 2006). Although the tokenizer is trained
on English punctuation corpus, it performs reason-
ably well for the current languages.

6 Results

Three runs are submitted. The first two runs are
the same except that in the second run we took ad-
vantage of the fact that the labels were balanced.
The third run only differed in thresholding for

Group Languages Accuracy

A bg,mk 1.0
B bs,hr,sr 0.8417
C cs,sk 1.0
D es-AR,es-ES 0.867
E pt-BR,pt-PT 0.929
F id,my 0.998

other xx 1.0

sum 0.9366

Table 3: Detailed results of our best run

group B. The overall accuracy of each run is listed
in Table 2 and the detailed results of our best run
can be found in Table 3.

To have a deeper insight into the limitations
of solving the shared task, a manual evaluation
has been performed on the 152 sentences mis-
classified Portuguese sentences by our best run
by two Brazilian native speakers. The annota-
tors have been asked to label each sentence as
BR (Brazilian), PT (European Portuguese) or UN
(Unknown). UN tag has been introduced to avoid
guessing. Their very low agreement on the labels
(Cohen’s kappa 0.28) and the fact that only 22 sen-
tences have been labeled correctly (according to
the gold standard) by both of them suggests that
there is very little room for improvement on the
shared task.

7 Conclusion

We presented the system description of our
DSL2015 task submission which performed an
overall accuracy of 93.66%. We ended up the 5th
place out of 8 submissions.

We introduced a two-level classifier with a vari-
ety of features: character and word n-grams, tf-
idf, stopword bigrams and tf-idf, and smoothed
language models. The first level solely relies on
the output of tf-idf, capable of grouping languages
with 100% accuracy. The second level combines
all features and uses the maximum entropy classi-
fier to classify languages within language groups.

Memory efficiency is a key issue in our re-
search. Our most important steps consume less
than 5GB RAM.

76



References
Steven Bird. 2006. NLTK: the natural language

toolkit. In Proceedings of the COLING/ACL on In-
teractive presentation sessions, pages 69–72. Asso-
ciation for Computational Linguistics.

Cyril Goutte, Serge Léger, and Marine Carpuat. 2014.
The NRC system for discriminating similar lan-
guages. In Proceedings of the First Workshop on
Applying NLP Tools to Similar Languages, Varieties
and Dialects, pages 139–145, Dublin, Ireland, Au-
gust. Association for Computational Linguistics.

S. Katz. 1987. Estimation of probabilities from sparse
data for the language model component of a speech
recognizer. IEEE Transactions on Acoustics, Speech
and Signal processing, 35(3):400–401.

Ben King, Dragomir Radev, and Steven Abney. 2014.
Experiments in sentence language identification
with groups of similar languages. In Proceedings of
the First Workshop on Applying NLP Tools to Sim-
ilar Languages, Varieties and Dialects, pages 146–
154, Dublin, Ireland, August. Association for Com-
putational Linguistics.

Marco Lui, Ned Letcher, Oliver Adams, Long Duong,
Paul Cook, and Timothy Baldwin. 2014. Explor-
ing methods and resources for discriminating simi-
lar languages. In Proceedings of the First Workshop
on Applying NLP Tools to Similar Languages, Vari-
eties and Dialects, pages 129–138, Dublin, Ireland,
August. Association for Computational Linguistics.

F. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel,
B. Thirion, O. Grisel, M. Blondel, P. Pretten-
hofer, R. Weiss, V. Dubourg, J. Vanderplas, A. Pas-
sos, D. Cournapeau, M. Brucher, M. Perrot, and
E. Duchesnay. 2011. Scikit-learn: Machine learn-
ing in Python. Journal of Machine Learning Re-
search, 12:2825–2830.

Jordi Porta and José-Luis Sancho. 2014. Using max-
imum entropy models to discriminate between sim-
ilar languages and varieties. In Proceedings of the
First Workshop on Applying NLP Tools to Similar
Languages, Varieties and Dialects, pages 120–128,
Dublin, Ireland, August. Association for Computa-
tional Linguistics.

Matthew Purver. 2014. A simple baseline for dis-
criminating similar languages. In Proceedings of the
First Workshop on Applying NLP Tools to Similar
Languages, Varieties and Dialects, pages 155–160,
Dublin, Ireland, August. Association for Computa-
tional Linguistics.

Liling Tan, Marcos Zampieri, Nikola Ljubešic, and
Jörg Tiedemann. 2014. Merging comparable data
sources for the discrimination of similar languages:
The dsl corpus collection. In Proceedings of the 7th
Workshop on Building and Using Comparable Cor-
pora, pages 11–15, Reykjavik, Iceland.

Jörg Tiedemann and Nikola Ljubešić. 2012. Efficient
discrimination between closely related languages.
In Proceedings of COLING 2012, pages 2619–2634,
Mumbai, India.

Marcos Zampieri, Liling Tan, Nikola Ljubešić, and
Jörg Tiedemann. 2014. A report on the DSL Shared
Task 2014. In Proceedings of the First Workshop on
Applying NLP Tools to Similar Languages, Varieties
and Dialects, pages 58–67, Dublin, Ireland, August.
Association for Computational Linguistics.

Marcos Zampieri, Liling Tan, Nikola Ljubešić, Jörg
Tiedemann, and Preslav Nakov. 2015. Overview
of the DSL Shared Task 2015. In Proceedings of
the Joint Workshop on Language Technology for
Closely Related Languages, Varieties and Dialects
(LT4VarDial), Hissar, Bulgaria.

77


