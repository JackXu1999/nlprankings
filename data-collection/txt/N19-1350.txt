




































Learning to Describe Unknown Phrases with Local and Global Contexts


Proceedings of NAACL-HLT 2019, pages 3467–3476
Minneapolis, Minnesota, June 2 - June 7, 2019. c©2019 Association for Computational Linguistics

3467

Learning to Describe Unknown Phrases with Local and Global Contexts

Shonosuke Ishiwatari† Hiroaki Hayashi‡ Naoki Yoshinaga§ Graham Neubig‡
Shoetsu Sato† Masashi Toyoda§ Masaru Kitsuregawa¶§
† The University of Tokyo ‡ Carnegie Mellon University

§ Institute of Industrial Science, the University of Tokyo ¶ National Institute of Informatics
†§¶{ishiwatari,ynaga,shoetsu,toyoda,kitsure}@tkl.iis.u-tokyo.ac.jp

‡{hiroakih, gneubig}@cs.cmu.edu

Abstract

When reading a text, it is common to become
stuck on unfamiliar words and phrases, such
as polysemous words with novel senses, rarely
used idioms, internet slang, or emerging enti-
ties. If we humans cannot figure out the mean-
ing of those expressions from the immediate
local context, we consult dictionaries for def-
initions or search documents or the web to
find other global context to help in interpre-
tation. Can machines help us do this work?
Which type of context is more important for
machines to solve the problem? To answer
these questions, we undertake a task of de-
scribing a given phrase in natural language
based on its local and global contexts. To
solve this task, we propose a neural description
model that consists of two context encoders
and a description decoder. In contrast to the
existing methods for non-standard English ex-
planation (Ni and Wang, 2017) and defini-
tion generation (Noraset et al., 2017; Gadetsky
et al., 2018), our model appropriately takes im-
portant clues from both local and global con-
texts. Experimental results on three existing
datasets (including WordNet, Oxford and Ur-
ban Dictionaries) and a dataset newly created
from Wikipedia demonstrate the effectiveness
of our method over previous work.

1 Introduction

When we read news text with emerging entities,
text in unfamiliar domains, or text in foreign lan-
guages, we often encounter expressions (words or
phrases) whose senses we do not understand. In
such cases, we may first try to figure out the mean-
ings of those expressions by reading the surround-
ing words (local context) carefully. Failing to do
so, we may consult dictionaries, and in the case of
polysemous words, choose an appropriate mean-
ing based on the context. Learning novel word
senses via dictionary definitions is known to be

Figure 1: Local & Global Context-aware Description
generator (LOG-CaD).

more effective than contextual guessing (Fraser,
1998; Chen, 2012). However, very often, hand-
crafted dictionaries do not contain definitions of
expressions that are rarely used or newly created.
Ultimately, we may need to read through the entire
document or even search the web to find other oc-
curances of the expression (global context) so that
we can guess its meaning.

Can machines help us do this work? Ni and
Wang (2017) have proposed a task of generating
a definition for a phrase given its local context.
However, they follow the strict assumption that the
target phrase is newly emerged and there is only a
single local context available for the phrase, which
makes the task of generating an accurate and co-
herent definition difficult (perhaps as difficult as
a human comprehending the phrase itself). On
the other hand, Noraset et al. (2017) attempted to
generate a definition of a word from an embed-
ding induced from massive text (which can be seen
as global context). This is followed by Gadet-
sky et al. (2018) that refers to a local context to
disambiguate polysemous words by choosing rel-
evant dimensions of their word embeddings. Al-



3468

though these research efforts revealed that both lo-
cal and global contexts are useful in generating
definitions, none of these studies exploited both
contexts directly to describe unknown phrases.

In this study, we tackle the task of describing
(defining) a phrase when given its local and global
contexts. We present LOG-CaD, a neural descrip-
tion generator (Figure 1) to directly solve this task.
Given an unknown phrase without sense defini-
tions, our model obtains a phrase embedding as
its global context by composing word embeddings
while also encoding the local context. The model
therefore combines both pieces of information to
generate a natural language description.

Considering various applications where we
need definitions of expressions, we evaluated
our method with four datasets including Word-
Net (Noraset et al., 2017) for general words, the
Oxford dictionary (Gadetsky et al., 2018) for pol-
ysemous words, Urban Dictionary (Ni and Wang,
2017) for rare idioms or slang, and a newly-
created Wikipedia dataset for entities.

Our contributions are as follows:

• We propose a general task of defining un-
known phrases given their contexts. This
task is a generalization of three related
tasks (Noraset et al., 2017; Ni and Wang,
2017; Gadetsky et al., 2018) and involves var-
ious situations where we need definitions of
unknown phrases (§ 2).

• We propose a method for generating nat-
ural language descriptions for unknown
phrases with local and global contexts
(§ 3).

• As a benchmark to evaluate the ability of the
models to describe entities, we build a large-
scale dataset from Wikipedia and Wikidata
for the proposed task. We release our dataset
and the code1 to promote the reproducibility
of the experiments (§ 4).

• The proposed method achieves the state-of-
the-art performance on our new dataset and
the three existing datasets used in the related
studies (Noraset et al., 2017; Ni and Wang,
2017; Gadetsky et al., 2018) (§ 5).

1https://github.com/shonosuke/
ishiwatari-naacl2019

2 Context-aware Phrase Description
Generation

In this section, we define our task of describing
a phrase in a specific context. Given an unde-
fined phrase Xtrg = {xj , · · · , xk} with its con-
text X = {x1, · · · , xI} (1 ≤ j ≤ k ≤ I), our
task is to output a description Y = {y1, · · · , yT }.
Here, Xtrg can be a word or a short phrase and is
included in X . Y is a definition-like concrete and
concise sentence that describes the Xtrg.

For example, given a phrase “sonic boom” with
its context “the shock wave may be caused by
sonic boom or by explosion,” the task is to gen-
erate a description such as “sound created by an
object moving fast.” If the given context has been
changed to “this is the first official tour to sup-
port the band’s latest studio effort, 2009’s Sonic
Boom,” then the appropriate output would be “al-
bum by Kiss.”

The process of description generation can be
modeled with a conditional language model as

p(Y |X,Xtrg) =
T∏
t=1

p(yt|y<t, X,Xtrg). (1)

3 LOG-CaD: Local & Global
Context-aware Description Generator

In this section, we describe our idea of utilizing
local and global contexts in the description gener-
ation task, and present the details of our model.

3.1 Local & global contexts

When we find an unfamiliar phrase in text and it
is not defined in dictionaries, how can we humans
come up with its meaning? As discussed in Sec-
tion 1, we may first try to figure out the mean-
ing of the phrase from the immediate context, and
then read through the entire document or search
the web to understand implicit information behind
the text.

In this paper, we refer to the explicit contextual
information included in a given sentence with the
target phrase (i.e., the X in Eq. (1)) as “local con-
text,” and the implicit contextual information in
massive text as “global context.” While both local
and global contexts are crucial for humans to un-
derstand unfamiliar phrases, are they also useful
for machines to generate descriptions? To verify
this idea, we propose to incorporate both local and
global contexts to describe an unknown phrase.



3469

3.2 Proposed model
Figure 1 shows an illustration of our LOG-CaD
model. Similarly to the standard encoder-decoder
model with attention (Bahdanau et al., 2015; Lu-
ong and Manning, 2016), it has a context encoder
and a description decoder. The challenge here is
that the decoder needs to be conditioned not only
on the local context, but also on its global context.
To incorporate the different types of contexts, we
propose to use a gate function similar to Noraset
et al. (2017) to dynamically control how the global
and local contexts influence the description.

Local & global context encoders We first de-
scribe how to model local and global contexts.
Given a sentence X and a phrase Xtrg, a bi-
directional LSTM (Gers et al., 1999) encoder gen-
erates a sequence of continuous vectors H =
{h1 · · · ,hI} as

hi = Bi-LSTM(hi−1,hi+1,xi), (2)

where xi is the word embedding of word xi. In
addition to the local context, we also utilize the
global context obtained from massive text. This
can be achieved by feeding a phrase embedding
xtrg to initialize the decoder (Noraset et al., 2017)
as

y0 = xtrg. (3)

Here, the phrase embedding xtrg is calculated by
simply summing up all the embeddings of words
that consistute the phrase Xtrg. Note that we use
a randomly-initialized vector if no pre-trained em-
bedding is available for the words in Xtrg.

Description decoder Using the local and global
contexts, a description decoder computes the
conditional probability of a description Y with
Eq. (1), which can be approximated with another
LSTM as

st = LSTM(yt−1, s′t−1), (4)

dt = ATTENTION(H, st), (5)

ctrg = CNN(Xtrg), (6)

s′t = GATE(st,xtrg, ctrg,dt), (7)

p(yt|y<t, Xtrg) = softmax(Ws′s′t + bs′), (8)

where st is a hidden state of the decoder LSTM
(s0 = ~0), and yt−1 is a jointly-trained word em-
bedding of the previous output word yt−1. In what
follows, we explain each equation in detail.

Attention on local context Considering the fact
that the local context can be relatively long (e.g.,
around 20 words on average in our Wikipedia
dataset introduced in Section 4), it is hard for
the decoder to focus on important words in local
contexts. In order to deal with this problem, the
ATTENTION(·) function in Eq. (5) decides which
words in the local context X to focus on at each
time step. dt is computed with an attention mech-
anism (Luong and Manning, 2016) as

dt =

T∑
i=1

αihi, (9)

αi = softmax(UhhTi Usst), (10)

where Uh and Us are matrices that map the en-
coder and decoder hidden states into a common
space, respectively.

Use of character information In order to cap-
ture the surface information of Xtrg, we construct
character-level CNNs (Eq. (6)) following (No-
raset et al., 2017). Note that the input to the
CNNs is a sequence of words in Xtrg, which are
concatenated with special character “ ,” such as
“sonic boom.” Following Noraset et al. (2017),
we set the CNN kernels of length 2-6 and size
10, 30, 40, 40, 40 respectively with a stride of 1 to
obtain a 160-dimensional vector ctrg.

Gate function to control local & global contexts
In order to capture the interaction between the lo-
cal and global contexts, we adopt a GATE(·) func-
tion (Eq. (7)) which is similar to Noraset et al.
(2017). The GATE(·) function updates the LSTM
output st to s′t depending on the global context
xtrg, local context dt, and character-level infor-
mation ctrg as

ft = [xtrg;dt; ctrg] (11)

zt = σ(Wz[ft; st] + bz), (12)

rt = σ(Wr[ft; st] + br), (13)

s̃t = tanh(Ws[(rt � ft); st] + bs), (14)
s′t = (1− zt)� st + zt � s̃t, (15)

where σ(·), � and ; denote the sigmoid function,
element-wise multiplication, and vector concate-
nation, respectively. W∗ and b∗ are weight ma-
trices and bias terms, respectively. Here, the up-
date gate zt controls how much the original hid-
den state st is to be changed, and the reset gate rt
controls how much the information from ft con-
tributes to word generation at each time step.



3470

Figure 2: Context-aware description dataset extracted from Wikipedia and Wikidata.

4 Wikipedia Dataset

Our goal is to let machines describe unfamiliar
words and phrases, such as polysemous words,
rarely used idioms, or emerging entities. Among
the three existing datasets, WordNet and Oxford
dictionary mainly target the words but not phrases,
thus are not perfect test beds for this goal. On the
other hand, although the Urban Dictionary dataset
contains descriptions of rarely-used phrases, the
domain of its targeted words and phrases is lim-
ited to Internet slang.

In order to confirm that our model can generate
the description of entities as well as polysemous
words and slang, we constructed a new dataset for
context-aware phrase description generation from
Wikipedia2 and Wikidata3 which contain a wide
variety of entity descriptions with contexts. The
overview of the data extraction process is shown
in Figure 2. Each entry in the dataset consists of
(1) a phrase, (2) its description, and (3) context (a
sentence).

For preprocessing, we applied Stanford Tok-
enizer4 to the descriptions of Wikidata items and
the articles in Wikipedia. Next, we removed
phrases in parentheses from the Wikipedia arti-
cles, since they tend to be paraphrasing in other
languages and work as noise. To obtain the con-
texts of each item in Wikidata, we extracted the

2https://dumps.wikimedia.org/enwiki/
20170720/

3https://dumps.wikimedia.org/
wikidatawiki/entities/20170802/

4https://nlp.stanford.edu/software/
tokenizer.shtml

sentence which has a link referring to the item
through all the first paragraphs of Wikipedia arti-
cles and replaced the phrase of the links with a spe-
cial token [TRG]. Wikidata items with no descrip-
tion or no contexts are ignored. This utilization of
links makes it possible to resolve the ambiguity of
words and phrases in a sentence without human
annotations, which is a major advantage of using
Wikipedia. Note that we used only links whose an-
chor texts are identical to the title of the Wikipedia
articles, since the users of Wikipedia sometimes
link mentions to related articles.

5 Experiments

We evaluate our method by applying it to describe
words in WordNet5 (Miller, 1995) and Oxford
Dictionary,6 phrases in Urban Dictionary7 and
Wikipedia/Wikidata.8 For all of these datasets, a
given word or phrase has an inventory of senses
with corresponding definitions and usage exam-
ples. These definitions are regarded as ground-
truth descriptions.

Datasets To evaluate our model on the word de-
scription task on WordNet, we followed Noraset
et al. (2017) and extracted data from WordNet us-
ing the dict-definition9 toolkit. Each entry
in the data consists of three elements: (1) a word,
(2) its definition, and (3) a usage example of the

5https://wordnet.princeton.edu/
6https://en.oxforddictionaries.com/
7https://www.urbandictionary.com/
8https://www.wikidata.org
9https://github.com/NorThanapon/

dict-definition



3471

Corpus #Phrases #Entries Phrase Context Desc.
length length length

WordNet

Train 7,938 13,883 1.00 5.81 6.61
Valid 998 1,752 1.00 5.64 6.61
Test 1,001 1,775 1.00 5.77 6.85

Oxford Dictionary

Train 33,128 97,855 1.00 17.74 11.02
Valid 8,867 12,232 1.00 17.80 10.99
Test 8,850 12,232 1.00 17.56 10.95

Urban Dictionary

Train 190,696 411,384 1.54 10.89 10.99
Valid 26,876 57,883 1.54 10.86 10.95
Test 26,875 38,371 1.68 11.14 11.50

Wikipedia

Train 151,995 887,455 2.10 18.79 5.89
Valid 8,361 44,003 2.11 19.21 6.31
Test 8,397 57,232 2.10 19.02 6.94

Table 1: Statistics of the word/phrase description
datasets.

Corpus Domain Inputs Cov. emb.

WordNet General words 100.00%
Oxford Dictionary General words 83.04%
Urban Dictionary Internet slang phrases 21.00%
Wikipedia Proper nouns phrases 26.79%

Table 2: Domains, expressions to be described, and the
coverage of pre-trained embeddings of the expressions
to be described.

word. We split this dataset to obtain Train, Valida-
tion, and Test sets. If a word has multiple defini-
tions/examples, we treat them as different entries.
Note that the words are mutually exclusive across
the three sets. The only difference between our
dataset and theirs is that we extract the tuples only
if the words have their usage examples in Word-
Net. Since not all entries in WordNet have usage
examples, our dataset is a small subset of Noraset
et al. (2017).

In addition to WordNet, we use the Oxford Dic-
tionary following Gadetsky et al. (2018), the Ur-
ban Dictionary following Ni and Wang (2017) and
our Wikipedia dataset described in the previous
section. Table 1 and Table 2 show the properties
and statistics of the four datasets, respectively.

To simulate a situation in a real application
where we might not have access to global context
for the target phrases, we did not train domain-
specific word embeddings on each dataset. In-
stead, for all of the four datasets, we use the same

Global Local I-Attn. LOG-CaD

# Layers of Enc-LSTMs - 2 2 2
Dim. of Enc-LSTMs - 600 600 600
Dim. of Attn. vectors - 300 300 300
Dim. of input word emb. 300 - 300 300
Dim. of char. emb. 160 160 - 160
# Layers of Dec-LSTMs 2 2 2 2
Dim. of Dec-LSTMs 300 300 300 300
Vocabulary size 10k 10k 10k 10k
Dropout rate 0.5 0.5 0.5 0.5

Table 3: Hyperparameters of the models

pre-trained CBOW10 vectors trained on Google
news corpus as global context following previous
work (Noraset et al., 2017; Gadetsky et al., 2018).
If the expression to be described consists of mul-
tiple words, its phrase embedding is calculated
by simply summing up all the CBOW vectors of
words in the phrase, such as “sonic” and “boom.”
(See Figure 1). If pre-trained CBOW embeddings
are unavailable, we instead use a special [UNK]
vector (which is randomly initialized with a uni-
form distribution) as word embeddings. Note that
our pre-trained embeddings only cover 26.79% of
the words in the expressions to be described in
our Wikipedia dataset, while it covers all words
in WordNet dataset (See Table 2). Even if no
reliable word embeddings are available, all mod-
els can capture the character information through
character-level CNNs (See Figure 1).

Models We implemented four methods: (1)
Global (Noraset et al., 2017), (2) Local (Ni and
Wang, 2017) with CNN, (3) I-Attention (Gadetsky
et al., 2018), and our proposed model, (4) LOG-
CaD. The Global model is our reimplementation
of the best model (S + G + CH) in Noraset et al.
(2017). It can access the global context of a phrase
to be described, but has no ability to read the lo-
cal context. The Local model is the reimplemen-
tation of the best model (dual encoder) in Ni and
Wang(2017). In order to make a fair comparison
of the effectiveness of local and global contexts,
we slightly modify the original implementation by
Ni and Wang(2017); as the character-level encoder
in the Local model, we adopt CNNs that are ex-
actly the same as the other two models instead of
the original LSTMs.

The I-Attention is our reimplementation of
the best model (S + I-Attention) in Gadetsky

10GoogleNews-vectors-negative300.bin.gz
at https://code.google.com/archive/p/
word2vec/



3472

Model WordNet Oxford Urban Wikipedia

Global 24.10 15.05 6.05 44.77
Local 22.34 17.90 9.03 52.94
I-Attention 23.77 17.25 10.40 44.71
LOG-CaD 24.79 18.53 10.55 53.85

Table 4: BLEU scores on four datasets.

Model Annotated score

Local 2.717
LOG-CaD 3.008

Table 5: Averaged human annotated scores on
Wikipedia dataset.

et al.(2018). Similar to our model, it uses both
local and global contexts. Unlike our model, how-
ever, it does not use character information to pre-
dict descriptions. Also, it cannot directly use the
local context to predict the words in descriptions.
This is because the I-Attention model indirectly
uses the local context only to disambiguate the
phrase embedding xtrg as

x′trg = xtrg �m, (16)

m = σ(Wm

∑I
i=1 FFNN(hi)

I
+ bm). (17)

Here, the FFNN(·) function is a feed-forward neu-
ral network that maps the encoded local contexts
hi to another space. The mapped local contexts
are then averaged over the length of the sentence
X to obtain a representation of the local context.
This is followed by a linear layer and a sigmoid
function to obtain the soft binary mask m which
can filter out the unrelated information included in
global context. Finally, the disambiguated phrase
embedding x′trg is then used to update the decoder
hidden state as

st = LSTM([yt−1;x′trg], st−1). (18)

All four models (Table 3) are implemented with
the PyTorch framework (Ver. 1.0.0).11

Automatic Evaluation Table 4 shows the
BLEU (Papineni et al., 2002) scores of the out-
put descriptions. We can see that the LOG-CaD
model consistently outperforms the three baselines
in all four datasets. This result indicates that us-
ing both local and global contexts helps describe
the unknown words/phrases correctly. While the

11http://pytorch.org/

Input: waste

Context: #1 #2

if the effort brings no
compensating gain it
is a waste

We waste the dirty
water by channeling it
into the sewer

Reference: useless or profitless
activity

to get rid of

Global: to give a liquid for a liquid

Local: a state of being as-
signed to a particular
purpose

to make a break of a
wooden instrument

I-Attention: a person who makes
something that can be
be be done

to remove or remove
the contents of

LOG-CaD: a source of something
that is done or done

to remove a liquid

Table 6: Descriptions for a word in WordNet.

Input: daniel o’neill

Context: #1 #2

after being enlarged
by publisher daniel
o’neill it was report-
edly one of the largest
and most prosperous
newspapers in the
united states.

in 1967 he returned to
belfast where he met
fellow belfast artist
daniel o’neill.

Reference: american journalist irish artist

Global: american musician

Local: american publisher british musician

I-Attention: american musician american musician

LOG-CaD: american writer british musician

Table 7: Descriptions for a phrase in Wikipedia.

I-Attention model also uses local and global con-
texts, its performance was always lower than the
LOG-CaD model. This result shows that using
local context to predict description is more effec-
tive than using it to disambiguate the meanings in
global context.

In particular, the low BLEU scores of Global
and I-Attention models on Wikipedia dataset sug-
gest that it is necessary to learn to ignore the
noisy information in global context if the cover-
age of pre-trained word embeddings is extremely
low (see the third and fourth rows in Table 2). We
suspect that the Urban Dictionary task is too dif-
ficult and the results are unreliable considering its
extremely low BLEU scores and high ratio of un-
known tokens in generated descriptions.



3473

Input: q

Context: #1 #2 #3 #4

q-lets and co. is a fil-
ipino and english infor-
mative children ’s show
on q in the philippines .

she was a founding pro-
ducer of the cbc radio
one show ” q ” .

the q awards are the uk
’s annual music awards
run by the music maga-
zine ” q ” .

charles fraser-smith was an
author and one-time mis-
sionary who is widely cred-
ited as being the inspira-
tion for ian fleming ’s james
bond quartermaster q .

Reference: philippine tv network canadian radio show british music magazine fictional character from
james bond

Global: american rapper

Local: television channel television show show magazine american writer

I-Attention: american rapper american rapper american rapper american rapper

LOG-CaD: television station in the
philippines

television program british weekly music
journalism magazine

[unk] [unk]

Table 8: Descriptions for a word in Wikipedia.

Manual Evaluation To compare the proposed
model and the strongest baseline in Table 4 (i.e.,
the Local model), we performed a human evalu-
ation on our dataset. We randomly selected 100
samples from the test set of the Wikipedia dataset
and asked three native English speakers to rate the
output descriptions from 1 to 5 points as: 1) com-
pletely wrong or self-definition, 2) correct topic
with wrong information, 3) correct but incom-
plete, 4) small details missing, 5) correct. The av-
eraged scores are reported in Table 5. Pair-wise
bootstrap resampling test (Koehn, 2004) for the
annotated scores has shown that the superiority of
LOG-CaD over the Local model is statistically
significant (p < 0.01).

Qualitative Analysis Table 6 shows a word in
the WordNet, while Table 7 and Table 8 show the
examples of the entities in Wikipedia as examples.
When comparing the two datasets, the quality of
generated descriptions of Wikipedia dataset is sig-
nificantly better than that of WordNet dataset. The
main reason for this result is that the size of train-
ing data of the Wikipedia dataset is 64x larger than
the WordNet dataset (See Table 1).

For all examples in the three tables, the Global
model can only generate a single description for
each input word/phrase because it cannot access
any local context. In the Wordnet dataset, only
the I-Attention and LOG-CaD models can suc-
cessfully generate the concept of “remove” given
the context #2. This result suggests that consid-
ering both local and global contexts are essential
to generate correct descriptions. In our Wikipedia

dataset, both the Local and LOG-CaD models can
describe the word/phrase considering its local con-
text. For example, both the Local and LOG-CaD
models could generate “american” in the descrip-
tion for “daniel o’neill” given “united states” in
context #1, while they could generate “british”
given “belfast” in context #2. A similar trend
can also be observed in Table 8, where LOG-CaD
could generate the locational expressions such as
“philippines” and “british” given the different con-
texts. On the other hand, the I-Attention model
could not describe the two phrases, taking into ac-
count the local contexts. We will present an anal-
ysis of this phenomenon in the next section.

6 Discussion

In this section, we present analyses on how the lo-
cal and global contexts contribute to the descrip-
tion generation task. First, we discuss how the lo-
cal context helps the models to describe a phrase.
Then, we analyze the impact of global context un-
der the situation where local context is unreliable.

6.1 How do the models utilize local contexts?
Local context helps us (1) disambiguate polyse-
mous words and (2) infer the meanings of un-
known expressions. Can machines also utilize the
local context? In this section, we discuss the two
roles of local context in description generation.

Considering that the pre-trained word em-
beddings are obtained from word-level co-
occurrences in massive text, more information is
mixed up into a single vector as the more senses
the word has. While Gadetsky et al. (2018) de-



3474

1 2 3 4+
# senses

20

30

40

50

60
BL

EU
Global
Local
I-Attention
LOG-CaD

(a) Number of senses of the phrase.

-20 20-30 30-40 40-
Unknown words in a phrase [%]

20

30

40

50

60

BL
EU

Global
Local
I-Attention
LOG-CaD

(b) Unknown words ratio in the phrase.

-10  10-15 15-20 20-25 25- 
Length of local context [# words]

20

30

40

50

60

BL
EU

Local
I-Attention
LOG-CaD

(c) Length of the local context.

Figure 3: Impact of various parameters of a phrase to be described on BLEU scores of the generated descriptions.

signed the I-Attention model to filter out unre-
lated meanings in the global context given local
context, they did not discuss the impact of the
number of senses has on the performance of defi-
nition generation. To understand the influence of
the ambiguity of phrases to be defined on the gen-
eration performance, we did an analysis on our
Wikipedia dataset. Figure 3(a) shows that the de-
scription generation task becomes harder as the
phrases to be described become more ambiguous.
In particular, when a phrase has an extremely large
number of senses, (i.e., #senses ≥ 4), the Global
model drops its performance significantly. This re-
sult indicates that the local context is necessary to
disambiguate the meanings in global context.

As shown in Table 2, a large proportion of
the phrases in our Wikipedia dataset includes un-
known words (i.e., only 26.79% of words in the
phrases have their pre-trained embeddings). This
fact indicates that the global context in this dataset
is not fully reliable. Then our next question is,
how does the lack of information from global con-
text affect the performance of phrase description?
Figure 3(b) shows the impact of unknown words
in the phrases to be described on the performance.
As we can see from the result, the advantage of
LOG-CaD and Local models over Global and I-
Attention models becomes larger as the unknown
words increases. This result suggests that we need
to fully utilize local contexts especially in prac-
tical applications where the phrases to be defined
have many unknown words. Here, Figure 3(b) also
shows a counterintuitive phenomenon that BLEU
scores increase as the ratio of unknown words in a
phrase increase. This is mainly because unknown
phrases tend to be person names such as writ-
ers, actors, or movie directors. Since these enti-
ties have fewer ambiguities in categories, they can
be described in extremely short sentences that are

easy for all four models to decode (e.g., “finnish
writer” or “american television producer”).

6.2 How do the models utilize global
contexts?

As discussed earlier, local contexts are important
to describe unknown expressions, but how about
global contexts? Assuming a situation where we
cannot obtain much information from local con-
texts (e.g., infer the meaning of “boswellia” from
a short local context “Here is a boswellia”), global
contexts should be essential to understand the
meaning. To confirm this hypothesis, we analyzed
the impact of the length of local contexts on BLEU
scores. Figure 3(c) shows that when the length
of local context is extremely short (l ≤ 10), the
LOG-CaD model becomes much stronger than
the Local model. This result indicates that not
only local context but also global context help
models describe the meanings of phrases.

7 Related Work

In this study, we address a task of describing a
given phrase with its context. In what follows, we
explain existing tasks that are related to our work.

Our task is closely related to word sense dis-
ambiguation (WSD) (Navigli, 2009), which iden-
tifies a pre-defined sense for the target word with
its context. Although we can use it to solve our
task by retrieving the definition sentence for the
sense identified by WSD, it requires a substantial
amount of training data to handle a different set of
meanings of each word, and cannot handle words
(or senses) which are not registered in the dictio-
nary. Although some studies have attempted to de-
tect novel senses of words for given contexts (Erk,
2006; Lau et al., 2014), they do not provide def-
inition sentences. Our task avoids these difficul-
ties in WSD by directly generating descriptions for



3475

phrases or words. It also allows us to flexibly tailor
a fine-grained definition for the specific context.

Paraphrasing (Androutsopoulos and Malakasi-
otis, 2010; Madnani and Dorr, 2010) (or text
simplification (Siddharthan, 2014)) can be used
to rephrase words with unknown senses. How-
ever, the target of paraphrase acquisition are
words/phrases with no specified context. Al-
though a few studies (Connor and Roth, 2007;
Max, 2009; Max et al., 2012) consider sub-
sentential (context-sensitive) paraphrases, they do
not intend to obtain a definition-like description as
a paraphrase of a word.

Recently, Noraset et al. (2017) introduced a
task of generating a definition sentence of a word
from its pre-trained embedding. Since their task
does not take local contexts of words as inputs,
their method cannot generate an appropriate def-
inition for a polysemous word for a specific con-
text. To cope with this problem, Gadetsky et al.
(2018) proposed a definition generation method
that works with polysemous words in dictionar-
ies. They presented a model that utilizes local
context to filter out the unrelated meanings from
a pre-trained word embedding in a specific con-
text. While their method use local context for dis-
ambiguating the meanings that are mixed up in
word embeddings, the information from local con-
texts cannot be utilized if the pre-trained embed-
dings are unavailable or unreliable. On the other
hand, our method can fully utilize the local con-
text through an attentional mechanism, even if the
reliable word embeddings are unavailable.

The most related work to this paper is Ni and
Wang (2017). Focusing on non-standard English
phrases, they proposed a model to generate the
explanations solely from local context. They fol-
lowed the strict assumption that the target phrase
was newly emerged and there was only a single lo-
cal context available, which made the task of gen-
erating an accurate and coherent definition diffi-
cult. Our proposed task and model are more gen-
eral and practical than Ni and Wang (2017); where
(1) we use Wikipedia, which includes expressions
from various domains, and (2) our model takes ad-
vantage of global contexts if available.

Our task of describing phrases with its context
is a generalization of the three tasks (Noraset et al.,
2017; Ni and Wang, 2017; Gadetsky et al., 2018),
and the proposed method utilizes both local and
global contexts of an expression in question.

8 Conclusions

This paper sets up a task of generating a natural
language description for an unknown phrase with
a specific context, aiming to help us acquire un-
known word senses when reading text. We ap-
proached this task by using a variant of encoder-
decoder models that capture the given local con-
text with the encoder and global contexts with the
decoder initialized by the target phrase’s embed-
ding induced from massive text. We performed ex-
periments on three existing datasets and one newly
built from Wikipedia and Wikidata. The experi-
mental results confirmed that the local and global
contexts complement one another and are both es-
sential; global contexts are crucial when local con-
texts are short and vague, while the local context
is important when the target phrase is polysemous,
rare, or unseen.

As future work, we plan to modify our
model to use multiple contexts in text to im-
prove the quality of descriptions, considering
the “one sense per discourse” hypothesis (Gale
et al., 1992). We will release the newly
built Wikipedia dataset and the experimental
codes for the academic and industrial communi-
ties at https://github.com/shonosuke/
ishiwatari-naacl2019 to facilitate the re-
producibility of our results and their use in various
application contexts.

Acknowledgements

The authors are grateful to Thanapon Noraset
for sharing the details of his implementation of
the previous work. We also thank the anony-
mous reviewers for their careful reading of our
paper and insightful comments, and the members
of Kitsuregawa-Toyoda-Nemoto-Yoshinaga-Goda
laboratory in the University of Tokyo for proof-
reading the draft.

This work was partially supported by Grant-in-
Aid for JSPS Fellows (Grant Number 17J06394)
and Commissioned Research (201) of the Na-
tional Institute of Information and Communica-
tions Technology of Japan.

References
Ion Androutsopoulos and Prodromos Malakasiotis.

2010. A survey of paraphrasing and textual entail-
ment methods. Journal of Artificial Intelligence Re-
search, 38:135–187.



3476

Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Ben-
gio. 2015. Neural machine translation by jointly
learning to align and translate. In Proceedings of the
Third International Conference on Learning Repre-
sentations (ICLR).

Yuzhen Chen. 2012. Dictionary use and vocabulary
learning in the context of reading. International
Journal of Lexicography, 25(2):216–247.

Michael Connor and Dan Roth. 2007. Context sensi-
tive paraphrasing with a global unsupervised clas-
sifier. In Proceedings of the 18th European Confer-
ence on Machine Learning (ECML), pages 104–115.

Katrin Erk. 2006. Unknown word sense detection as
outlier detection. In Proceedings of the Human Lan-
guage Technology Conference of the North Amer-
ican Chapter of the Association of Computational
Linguistics (NAACL), pages 128–135.

Carol A. Fraser. 1998. The role of consulting a dictio-
nary in reading and vocabulary learning. Canadian
Journal of Applied Linguistics, 2(1-2):73–89.

Artyom Gadetsky, Ilya Yakubovskiy, and Dmitry
Vetrov. 2018. Conditional generators of words def-
initions. In Proceedings of the 56th Annual Meet-
ing of the Association for Computational Linguistics
(ACL), Short Papers, pages 266–271.

William A. Gale, Kenneth W. Church, and David
Yarowsky. 1992. One sense per discourse. In Pro-
ceedings of the workshop on Speech and Natural
Language, HLT, pages 233–237.

Felix A. Gers, Jürgen Schmidhuber, and Fred Cum-
mins. 1999. Learning to forget: Continual predic-
tion with lstm. In Proceedings of the Ninth Inter-
national Conference on Artificial Neural Networks
(ICANN), pages 850–855.

Philipp Koehn. 2004. Statistical significance tests for
machine translation evaluation. In Proceedings of
the 2004 Conference on Empirical Methods in Natu-
ral Language Processing (EMNLP), pages 388–395.

Jey Han Lau, Paul Cook, Diana McCarthy, Spandana
Gella, and Timothy Baldwin. 2014. Learning word
sense distributions, detecting unattested senses and
identifying novel senses using topic models. In Pro-
ceedings of the 52nd Annual Meeting of the Asso-
ciation for Computational Linguistics (ACL), pages
259–270.

Minh-Thang Luong and Christopher D. Manning.
2016. Achieving open vocabulary neural machine
translation with hybrid word-character models. In
Proceedings of the 54th Annual Meeting of the Asso-
ciation for Computational Linguistics (ACL), pages
1054–1063.

Nitin Madnani and Bonnie J. Dorr. 2010. Generat-
ing phrasal and sentential paraphrases: A survey
of data-driven methods. Computational Linguistics,
36(3):341–387.

Aurélien Max. 2009. Sub-sentencial paraphrasing by
contextual pivot translation. In Proceedings of the
2009 Workshop on Applied Textual Inference, pages
18–26.

Aurélien Max, Houda Bouamor, and Anne Vilnat.
2012. Generalizing sub-sentential paraphrase acqui-
sition across original signal type of text pairs. In
Proceedings of the 2012 Joint Conference on Empir-
ical Methods in Natural Language Processing and
Computational Natural Language Learning, pages
721–731.

George A Miller. 1995. Wordnet: a lexical database for
english. Communications of the ACM, 38(11):39–
41.

Roberto Navigli. 2009. Word sense disambiguation: A
survey. ACM COMPUTING SURVEYS, 41(2):1–69.

Ke Ni and William Yang Wang. 2017. Learning to ex-
plain non-standard English words and phrases. In
Proceedings of the 8th International Joint Confer-
ence on Natural Language Processing (IJCNLP),
pages 413–417.

Thanapon Noraset, Chen Liang, Larry Birnbaum, and
Doug Downey. 2017. Definition modeling: Learn-
ing to define word embeddings in natural language.
In Proceedings of the 31st AAAI Conference on Ar-
tificial Intelligence (AAAI), pages 3259–3266.

Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a method for automatic eval-
uation of machine translation. In Proceedings of
40th Annual Meeting of the Association for Compu-
tational Linguistics (ACL), pages 311–318.

Advaith Siddharthan. 2014. A survey of research on
text simplification. International Journal of Applied
Linguistics, 165(2):259–298.


