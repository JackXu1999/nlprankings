



















































Document Classification by Inversion of Distributed Language Representations


Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics
and the 7th International Joint Conference on Natural Language Processing (Short Papers), pages 45–49,

Beijing, China, July 26-31, 2015. c©2015 Association for Computational Linguistics

Document Classification by Inversion of
Distributed Language Representations

Matt Taddy
University of Chicago Booth School of Business

taddy@chicagobooth.edu

Abstract

There have been many recent advances
in the structure and measurement of dis-
tributed language models: those that map
from words to a vector-space that is rich in
information about word choice and com-
position. This vector-space is the dis-
tributed language representation.

The goal of this note is to point out
that any distributed representation can be
turned into a classifier through inversion
via Bayes rule. The approach is simple
and modular, in that it will work with
any language representation whose train-
ing can be formulated as optimizing a
probability model. In our application to 2
million sentences from Yelp reviews, we
also find that it performs as well as or bet-
ter than complex purpose-built algorithms.

1 Introduction

Distributed, or vector-space, language representa-
tions V consist of a location, or embedding, for
every vocabulary word in RK , where K is the di-
mension of the latent representation space. These
locations are learned to optimize, perhaps approx-
imately, an objective function defined on the origi-
nal text such as a likelihood for word occurrences.

A popular example is the Word2Vec machinery
of Mikolov et al. (2013). This trains the distributed
representation to be useful as an input layer for
prediction of words from their neighbors in a Skip-
gram likelihood. That is, to maximize

t+b∑
j 6=t, j=t−b

log pV(wsj | wst) (1)

summed across all words wst in all sentences ws,
where b is the skip-gram window (truncated by the
ends of the sentence) and pV(wsj |wst) is a neural

network classifier that takes vector representations
for wst and wsj as input (see Section 2).

Distributed language representations have been
studied since the early work on neural networks
(Rumelhart et al., 1986) and have long been ap-
plied in natural language processing (Morin and
Bengio, 2005). The models are generating much
recent interest due to the large performance gains
from the newer systems, including Word2Vec and
the Glove model of Pennington et al. (2014), ob-
served in, e.g., word prediction, word analogy
identification, and named entity recognition.

Given the success of these new models, re-
searchers have begun searching for ways to adapt
the representations for use in document classifica-
tion tasks such as sentiment prediction or author
identification. One naive approach is to use ag-
gregated word vectors across a document (e.g., a
document’s average word-vector location) as input
to a standard classifier (e.g., logistic regression).
However, a document is actually an ordered path
of locations throughRK , and simple averaging de-
stroys much of the available information.

More sophisticated aggregation is proposed in
Socher et al. (2011; 2013), where recursive neu-
ral networks are used to combine the word vectors
through the estimated parse tree for each sentence.
Alternatively, Le and Mikolov’s Doc2Vec (2014)
adds document labels to the conditioning set in (1)
and has them influence the skip-gram likelihood
through a latent input vector location in V . In each
case, the end product is a distributed representa-
tion for every sentence (or document for Doc2Vec)
that can be used as input to a generic classifier.

1.1 Bayesian Inversion

These approaches all add considerable model and
estimation complexity to the original underlying
distributed representation. We are proposing a
simple alternative that turns fitted distributed lan-
guage representations into document classifiers

45



without any additional modeling or estimation.
Write the probability model that the represen-

tation V has been trained to optimize (likeli-
hood maximize) as pV(d), where document d =
{w1, ...wS} is a set of sentences – ordered vectors
of word identities. For example, in Word2Vec the
skip-gram likelihood in (1) yields

log pV(d) =
∑
s

∑
t

t+b∑
j 6=t, j=t−b

log pVy(wsj | wst).
(2)

Even when such a likelihood is not explicit it will
be implied by the objective function that is opti-
mized during training.

Now suppose that your training documents are
grouped by class label, y ∈ {1 . . . C}. We can
train separate distributed language representations
for each set of documents as partitioned by y;
for example, fit Word2Vec independently on each
sub-corpus Dc = {di : yi = c} and obtain the
labeled distributed representation map Vc. A new
document d has probability pVc(d) if we treat it as
a member of class c, and Bayes rule implies

p(y|d) = pVy(d)πy∑
c pVc(d)πc

(3)

where πc is our prior probability on class label c.
Thus distributed language representations

trained separately for each class label yield
directly a document classification rule via (3).
This approach has a number of attractive qualities.
Simplicity: The inversion strategy works for any
model of language that can (or its training can) be
interpreted as a probabilistic model. This makes
for easy implementation in systems that are al-
ready engineered to fit such language represen-
tations, leading to faster deployment and lower
development costs. The strategy is also inter-
pretable: whatever intuition one has about the dis-
tributed language model can be applied directly to
the inversion-based classification rule. Inversion
adds a plausible model for reader understanding
on top of any given language representation.
Scalability: when working with massive corpora
it is often useful to split the data into blocks as part
of distributed computing strategies. Our model of
classification via inversion provides a convenient
top-level partitioning of the data. An efficient sys-
tem could fit separate by-class language represen-
tations, which will provide for document classi-
fication as in this article as well as class-specific

answers for NLP tasks such as word prediction or
analogy. When one wishes to treat a document as
unlabeled, NLP tasks can be answered through en-
semble aggregation of the class-specific answers.
Performance: We find that, in our examples, in-
version of Word2Vec yields lower misclassifica-
tion rates than both Doc2Vec-based classification
and the multinomial inverse regression (MNIR) of
Taddy (2013b). We did not anticipate such out-
right performance gain. Moreover, we expect that
with calibration (i.e., through cross-validation)
of the many various tuning parameters available
when fitting both Word and Doc 2Vec the perfor-
mance results will change. Indeed, we find that all
methods are often outperformed by phrase-count
logistic regression with rare-feature up-weighting
and carefully chosen regularization. However, the
out-of-the-box performance of Word2Vec inver-
sion argues for its consideration as a simple default
in document classification.

In the remainder, we outline classification
through inversion of a specific Word2Vec model
and illustrate the ideas in classification of Yelp
reviews. The implementation requires only a
small extension of the popular gensim python
library (Rehurek and Sojka, 2010); the ex-
tended library as well as code to reproduce
all of the results in this paper are available
on github. In addition, the yelp data is
publicly available as part of the correspond-
ing data mining contest at kaggle.com. See
github.com/taddylab/deepir for detail.

2 Implementation

Word2Vec trains V to maximize the skip-gram
likelihood based on (1). We work with the Huff-
man softmax specification (Mikolov et al., 2013),
which includes a pre-processing step to encode
each vocabulary word in its representation via a
binary Huffman tree (see Figure 1).

Each individual probability is then

pV(w|wt) =
L(w)−1∏
j=1

σ
(
ch [η(w, j + 1)]u>η(w,j)vwt

)
(4)

where η(w, i) is the ith node in the Huffman tree
path, of length L(w), for word w; σ(x) = 1/(1 +
exp[−x]); and ch(η) ∈ {−1,+1} translates from
whether η is a left or right child to +/- 1. Every
word thus has both input and output vector coor-
dinates, vw and [uη(w,1) · · ·uη(w,L(w))]. Typically,

46



Figure 1: Binary Huffman encoding of a 4 word
vocabulary, based upon 18 total utterances. At
each step proceeding from left to right the two
nodes with lowest count are combined into a par-
ent node. Binary encodings are read back off of
the splits moving from right to left.

only the input space V = [vw1 · · ·vwp ], for a p-
word vocabulary, is reported as the language rep-
resentation – these vectors are used as input for
NLP tasks. However, the full representation V in-
cludes mapping from each word to both V and U.

We apply the gensim python implementation
of Word2Vec, which fits the model via stochastic
gradient descent (SGD), under default specifica-
tion. This includes a vector space of dimension
K = 100 and a skip-gram window of size b = 5.

2.1 Word2Vec Inversion
Given Word2Vec trained on each of C class-
specific corpora D1 . . . DC , leading to C distinct
language representations V1 . . .VC , classification
for new documents is straightforward. Consider
the S-sentence document d: each sentence ws is
given a probability under each representation Vc
by applying the calculations in (1) and (4). This
leads to the S×C matrix of sentence probabilities,
pVc(ws), and document probabilities are obtained

pVc(d) =
1
S

∑
s

pVc(ws). (5)

Finally, class probabilities are calculated via
Bayes rule as in (3). We use priors πc = 1/C, so
that classification proceeds by assigning the class

ŷ = argmaxc pVc(d). (6)

3 Illustration

We consider a corpus of reviews provided by Yelp
for a contest on kaggle.com. The text is tok-
enized simply by converting to lowercase before
splitting on punctuation and white-space. The

training data are 230,000 reviews containing more
than 2 million sentences. Each review is marked
by a number of stars, from 1 to 5, and we fit
separate Word2Vec representations V1 . . .V5 for
the documents at each star rating. The valida-
tion data consist of 23,000 reviews, and we ap-
ply the inversion technique of Section 2 to score
each validation document d with class probabili-
ties q = [q1 · · · q5], where qc = p(c|d).

The probabilities will be used in three different
classification tasks; for reviews as

a. negative at 1-2 stars, or positive at 3-5 stars;

b. negative 1-2, neutral 3, or positive 4-5 stars;

c. corresponding to each of 1 to 5 stars.

In each case, classification proceeds by sum-
ming across the relevant sub-class probabilities.
For example, in task a, p(positive) = q3 +
q4 + q5. Note that the same five fitted Word2Vec
representations are used for each task.

We consider a set of related comparator tech-
niques. In each case, some document repre-
sentation (e.g., phrase counts or Doc2Vec vec-
tors) is used as input to logistic regression pre-
diction of the associated review rating. The lo-
gistic regressions are fit under L1 regularization
with the penalties weighted by feature standard
deviation (which, e.g., up-weights rare phrases)
and selected according to the corrected AICc cri-
teria (Flynn et al., 2013) via the gamlr R pack-
age of Taddy (2014). For multi-class tasks b-c,
we use distributed Multinomial regression (DMR;
Taddy 2015) via the distrom R package. DMR
fits multinomial logistic regression in a factorized
representation wherein one estimates independent
Poisson linear models for each response category.
Document representations and logistic regressions
are always trained using only the training corpus.

Doc2Vec is also fit via gensim, using the same
latent space specification as for Word2Vec: K =
100 and b = 5. As recommended in the doc-
umentation, we apply repeated SGD over 20 re-
orderings of each corpus (for comparability, this
was also done when fitting Word2Vec). Le and
Mikolov provide two alternative Doc2Vec specifi-
cations: distributed memory (DM) and distributed
bag-of-words (DBOW). We fit both. Vector rep-
resentations for validation documents are trained
without updating the word-vector elements, lead-
ing to 100 dimensional vectors for each docu-
ment for each of DM and DCBOW. We input

47



●●

●
●
●
●

●

●
●

●
●

●

●●●●

●●●●

●

●
●

●●●

●

●

●
●
●

●

●

●

●

●●●
●
●●●●

●

●●●
●
●●●●●

●

●●●●●●

●

●●●
●●●●●●
●
●
●●●

●

●●●●●●

●

●●●●●

●

●

●

●
●

●

●

●

●

●

●

●

●

●
●

●

●

●

●

●

●

●

●

●

●

●
●

●

●

●

●

●

●

●

●

●

●

●

●

●

●
●

●

●

●

●

●
●

●●

●

●

●

●

●

●

●

●

●●●●

●

●

●

●

●

●

●

●

●

●

●

●
●

●

●

●

●

●

●

●

●

●●

●

●
●

●

●

●

●●

●●
●

●

●

●

●
●

●●

●

●

●

●

●

●

●●

●

●

●●

●

●

●

●

●

●

●

●

●

●

●

●

●●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●●

●

●

●

●

●

●
●

●●

●

●●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●
●

●

●●

●

●

●

●

●
●

●

●

●

●

●

●

●

●

●

●●
●
●
●
●

●

●

●

●
●
●

●

●

●

●

●●

●

●

●

●

●
●

●
●

●

●

●

●

●
●

●
●

●

●

●

●

●

●

●

●

●

●

●

●

●

●●●

●

●

●

●

●●●

●

●

●

●

●

●

●

●●

●●
●

●●

●

●

●

●

●

●

●●

●

●

●

●

●

●

●

●

●

●
●

●
●●

●●

●

●

●

●●

●

●

●

●

●
●●

●

●

●●
●

●

●

●●
●
●●

●
●

●

●

●

●
●

●

●

●

●

●

●●
●
●

●

●

●
●
●

●

●
●

●

●
●
●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●
●

●
●

●

●●

●

●
●

●

●

●
●

●

●

●

●

●

●●

●●

●

●

●

●

●

●

●

●

●

●

●
●

●

●

●

●

●

●

●

●●

●

●

●

●

●

●

●

●

●

●

●

●
●

●

●

●

●

●

●

●

●

●

●

●

●

●
●
●

●

●

●

1 2 3 4 5

0.
0

0.
2

0.
4

0.
6

0.
8

1.
0

word2vec inversion

●

●

●

●

●

●

●●

●

●

●

●

●

●

●

●

●
●

●

●

●

●

●

●

●

●●

●

●
●

●

●

●

●

●

●

●

●

●

●

●

●

●

●
●

●

●●

●

●

●

●

●●

●●●
●

●

●

●●●

●

●●

●

●

●

●

●●

●

●

●

●
●

●

●

●

●●

●

●

●

●

●●

●

●

●

●

●

●

●

●

●

●

●

●

●
●

●

●

●●

●

●

●

●

●

●●

●

●

●
●

●

●

●

●

●

●
●
●

●

●

●

●

●●

●

●

●

●

●

●

●

●

●

●

●
●

●

●

●

●

●

●

●

●

●

●●

●

●
●●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●●

●

●

●

●

●

●

●

●

●
●

●

●●

●

●

●

●

●●

●
●

●

●

●

●

●
●

●

●●●

●

●
●

●

●

●●

●

●

●
●

●
●

●

●

●

●
●

●

●●

●

●

●

●

●
●

●

●

●

●●

●

●

●

●

●

●
●

●
●

●

●●●

●

●

●

●

●

●

●
●●

●

●
●

●

●

●

●
●
●●●
●

●

●

●

●
●
●
●

●

●

●

●

●

●

●

●●

●●

●

●

●

●●

●

●

●

●

●

●
●

●
●
●

●

●

●

●

●
●

●

●

●

●●
●

●

●

●

●

●

●

●

●

●

●●
●●

●

●●
●

●
●

●●

●

●

●

●
●

●

●●●

●

●

●

●

●

●●

●

●

●

●●

●

●
●

●

●
●

●
●

●

●

●

●

●

●

●

●●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●
●

●●

●
●

●

●

●
●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●
●

●

●

●

●

●

●

●

●

●●

●

●

●

●

●

●
●
●
●
●

●

●

●

●

●●
●

●

●

●
●

●

●

●

●

●●

●

●

●●

●

●●

●

●
●

●

●●

●

●

●

●

●●
●

●●●

●

●

●

●

●

●

●

●

●

●

●

●
●

●

●
●

●●

●

●●

●

●

●

●

●

●

●

●

●
●●

●

●
●
●

●

●

●

●

●

●●

●

●●

●
●

●

●
●

●

●
●●
●
●

●

●

●

●

●

●

●

●

●

●
●

●

●

●

●●

●

●●●

●

●
●

●

●

●

●

●

●

●

●

●●

●●
●
●

●

●

●

●

●

●

●

●

●●
●

●

●

●

●

●

●

●

●

●

●

●

●

●

●
●

●

●

●

●
●
●

●

●

●

●

●

●
●●●
●

●

●
●

●

●

●

●

●

●

●

●

●

●

●●●
●
●

●

●

●

●
●

●●

●

●●
●

●

●
●

●

●

●

●

●●●

●
●

●

●

●

●

●

●

●

●
●
●●

●

●

●

●

●
●

●

●
●

●

●

●

●

●

●

●

●●

●

●

●

●

●

●

●

●

●

●

●
●

●

●

●●

●

●

●

●

●

●

●

●

●

●
●
●
●●

●

●●
●

●

●

●

●

●

●

●

●

●

●

●
●
●

●

●

●

●

●

●

●

●

●●

●
●

●

●●

●

●

●

●

●●

●●

●
●●
●●

●

●

●

●

●

●

●

●

●

●
●

●

●
●

●
●
●

●

●

●

●

●

●
●

●

●

●

●●
●
●

●

●

●

●

●

●

●

●
●

●

●

●

●

●

●
●

●

●

●

●
●●

●

●

●

●

●

●

●
●

●●

●

●

●●

●

●

●●

●

●●

●

●

●

●

●

●

●

●●
●

●

●

●

●●
●●
●
●

●
●

●

●

●

●

●

●●

●

●●
●

●

●

●

●

●

●

●

●

●

●
●

●

●

●●●●
●●

●

●
●

●

●

●

●
●●

●

●
●
●

●

●

●

●●

●

●
●●

●

●

●

●●

●

●

●
●●

●

●

●●

●

●

●

●●

●

●

●

●●

●

●●●
●

●
●●
●

●
●

●

●

●

●

●

●

●
●

●

●
●
●

●

●

●

●●

●

●

●

●

●

●

●

●

●
●●

●
●

●

●

●●●●

●
●

●●

●

●

●
●

●

●●

●
●

●

●

●
●
●

●

●

●

●

●

●

●

●

●
●

●

●

●

●
●
●
●

●

●

●

●

●

●●

●

●

●

●

●

●

●
●

●

●

●

●

●

●
●
●

●

●
●

●

●●

●
●
●

●

●
●

●

●

●

●

●

●
●
●
●

●

●

●

●

●

●

●

●

●

●
●●

●

●

●
●
●●

●

●

●

●
●

●

●
●●

●

●

●

●●

●

●

●

●●

●

●

●●●
●
●

●

●

●

●

●

●●
●

●

●

●
●

●

●

●

●

●
●●
●

●

●

●

●

●

●

●

●

●

●●

●

●●

●

●

●

●

●

●

●

●

●
●●

●

●

●

●

●

●

●

●●

●

●
●

●

●

●

●

●

●

●●

●

●

●

●

●

●

●

●

●
●
●●

●

●
●

●

●

●
●

●●

●

●

●

●

●●●●●

●

●
●

●●

●

●

●

●

●●●

●

●

●

●

●●
●
●

●

●

●
●

●

●

●
●
●

●

●

●●
●

●

●

●

●

●

●

●
●

●

●

●

●●

●
●

●

●●

●

●●

●●●●●
●●

●

●

●●●●

●

●

●

●

●

●

●
●

●

●

●

●●

●

●

●
●

●

●

●

●

●

●

●

●

●

●

●●

●

●

●

●●

●●

●
●
●

●

●

●

●

●

●

●

●

●

●

●

●
●

●

●

●

●

●

●●
●●
●

●

●

●
●

●

●

●

●

●

●

●

●

●

●

●

●●

●

●

●
●●
●
●●

●

●

●

●

●●
●

●

●

●

●

●
●

●

●

●

●●
●
●
●

●

●

●

●

●

●
●

●

●

●

●

●

●

●●●●●

●●
●●

●

●

●

●

●

●

●

●
●

●

●

●

●

●

●●

●

●

●

●

●

●

●

●

●

●●
●

●

●

●

●

●

●

●

●

●
●

●
●
●

●

●

●

●

●

●

●●
●
●●

●●

●

●

●
●

●

●
●

●

●

●
●

●

●

●

●

●

●

●

●●
●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●
●
●

●

●

●●

●

●

●

●

●

●

●

●

●

●

●

●●

●
●

●

●
●

●

●

●

●

●

●●

●

●

●
●

●

●

●

●

●

●

●

●

●

●

●

●●
●●

●

●

●
●
●

●

●

●
●

●
●

●

●

●

●

●

●

●

●

●

●

●●

●

●●

●

●
●●
●

●

●

1 2 3 4 5

0.
0

0.
2

0.
4

0.
6

0.
8

1.
0

phrase regression

●
●
●●

●●
●

●

●

●

●●
●
●●

●●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●
●

●

●

●

●

●

●
●●

●

●

●

●

●

●●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●●●●

●

●

●

●●●
●

●

●
●

●

●
●

●

●

●

●

●
●

●

●

●

●

●

●

●●

●

●

●

●

●

●

●

●

●

●

●

●

●

●
●●

●

●

●

●
●

●

●

●

●

●

●

●

●

●

●

●●
●
●

●

●

●
●

●

●●

●

●
●

●

●
●

●●

●

●
●

●

●

●

●●

●

●

●

●

●

●

●

●
●
●

●

●
●

●
●

●●

●

●

●

●
●

●

●

●

●

●

●●

●●

●

●●

●

●

●

●
●

●

●
●
●

●●●

●●
●

●

●●

●

●

●

●

●
●

●●
●●●
●●

●

●

●

●

●

●●

●

●
●●

●

●

●

●●
●●

●
●●

●

●

●●
●

●

●

●

●

●●
●

●
●

●●●

●

●

●●●●●●●

●

●
●

●

●
●

●

●

●

●

●

●

●

●

●●

●

●

●

●
●

●

●●
●

●

●●

●
●

●

●

●●

●

●

●

●

●

●

●

●●

●

●

●

●

●

●

●

●

●
●
●

●

●●

●

●

●●

●

●

●

●

●

●

●

●
●

●

●

●
●

●

●

●

●
●
●

●

●

●

●

●

●

●

●

●●●
●
●
●
●
●

●

●

●●

●

●

●

●

●

●
●

●

●

●

●

●

●

●

●

●

●
●
●
●

●

●

●

●

●

●
●
●
●●
●●

●

●

●
●

●

●●

●
●

●

●

●

●

●

●

●●
●

●

●●●

●

●

●
●
●●
●●

●
●●
●

●

●
●

●

●

●

●●

●

●

●

●
●

●

●

●

●●
●

●

●
●

●

●

●

●

●

●

●

●

●

●●
●
●

●
●

●

●●

●
●

●

●
●

●

●
●

●
●

●

●
●

●

●

●

●

●

●
●

●

●

●

●

●

●

●
●●

●

●

●●

●●

●

●

●

●

●

●

●

●

●

●

●

●●●

●
●
●

●

●

●

●
●●
●
●●

●
●●
●

●●

●●

●

●
●

●
●
●

●

●●

●

●

●
●
●

●

●

●
●●
●

●

●

●

●

●

●

●

●

●●●

●

●●

●

●●●●

●

●

●

●

●

●

●

●

●

●

●

●

●●

●

●

●
●

●

●

●
●
●

●

●

●

●

●

●
●

●

●

●

●

●
●●
●●
●

●

●

●

●●

●

●
●

●

●
●●

●
●
●●

●
●

●

●
●

●

●●

●

●

●
●

●

●

●

●
●

●

●

●

●

●
●

●

●

●

●●●
●
●●

●

●●

●

●

●

●●

●

●●●

●

●

●

●

●

●
●●
●●

●

●

●

●

●

●

●●

●●

●
●

●

●

●

●

●

●

●
●

●
●

●

●

●

●●

●

●
●

●

●

●

●
●

●

●

●
●
●

●●●●
●●●
●

●

●

●
●
●
●

●●
●
●

●

●

●

●
●

●
●
●

●
●●

●

●

●

●

●

●

●

●

●

●

●

●
●
●

●●

●

●●

●

●

●

●
●
●

●
●

●●
●

●

●

●

●●

●
●

●

●

●

●

●
●

●
●

●

●

●

●●

●

●
●●●

●

●

●

●

●
●

●

●

●
●

●

●

●●
●

●
●

●

●

●

●

●

●

●

●
●

●

●

●

●
●●

●

●

●

●

●

●

●●

●

●

●

●
●

●

●

●

●
●
●●●

●●
●
●
●

●

●

●

●

●

●

●

●

●

●

●

●

●●●
●●

●
●

●
●

●

●
●●

●
●
●●

●

●
●●

●

●
●

●

●

●

●

●

●

●
●

●

●

●●

●

●

●
●

●

●

●

●

●
●
●

●

●●

●

●

●

●

●

●

●●
●

●●

●

●

●

●
●
●
●

●
●●

●

●
●

●

●●●

●

●

●

●

●●

●

●
●
●

●
●
●
●

●

●

●

●

●

●

●

●●●

●●

●●
●

●
●●

●●●

●

●

●

●

●
●

●
●
●
●

●
●
●

●

●●

●
●
●
●

●
●
●

●●●
●●
●
●
●

●

●

●
●

●

●
●
●●
●

●

●●

●

●
●
●

●●
●

●●
●
●

●

●
●

●

●
●●
●

●

●

●
●

●

●

●

●

●

●

●
●
●

●●

●

●

●●
●●

●

●

●●
●●●●

●

●

●
●

●
●●●
●

●

●

●

●
●

●

●

●

●

●

●

●●

●

●

●

●

●●●

●

●

●
●
●●●

●
●●●
●

●●●
●●

●

●
●

●

●

●

●

●

●

●

●
●

●

●●

●

●●●

●

●
●
●

●

●

●●
●
●

●●

●●●

●

●●
●

●

●

●

●

●

●●

●

●
●●

●

●

●
●
●

●

●
●

●

●●

●

●

●

●

●

●

●

●

●

●
●

●●●●

●

●
●

●

●

●

●
●●

●

●●●

●

●

●

1 2 3 4 5

0.
0

0.
2

0.
4

0.
6

0.
8

1.
0

doc2vec regression

●

●

●

●

●
●

●

●

●
●

●
●●

●
●

●

●

●

●●

●
●
●

●

●●

●●
●●●

●

●
●●
●●

●
●
●●

●

●

●

●●

●

●
●

●

●

●

●

●
●
●

●

●
●

●●●

●●

●●●

●
●

●●●
●

●
●

●

●

●

●

●

●● ●●

●

●

●

●●

●

●

●

●●

●

●

●
●
●●

●●

●●

●●

●
●

●●
●

●●

●
●

●

●

●●●

●
●

●●

●

●

●

●

●
●

●

●

●●

●

●
●
●●

●

●
●●●

●

●

●
●
●

●
●

●●

●

●●
●
●
●

●

●

●

●
●

●
●

●

●

●

●

●
●●
●●●

●●

●

●
●

●

●●●●●
●●
●
●●

●

●

●●●

●●

●

●

●

●●

●

●

●

●

●

●
●

●
●

●

●

●●

●

●

●

●

●

●

●

●

●
●

●

●

●

●

●

●

●

●

●

●●

●●
●

●

●

●
●

●

●

●

●

●

●●

●

●
●

●

●●

●

●

●

●

●
●

●

●

●

●

●

●
●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●
●

●
●

●

●

●

●

●
●

●

●

●●

●

●

●
●

●

●

●

●
●

●

●

●

●

●

●

●

●

●

●

●

●

●

●●

●

●

●
●●

●

●
●
●

●

●

●

●

●

●

●
●

●

●

●

●

●

●

●

●

●
●

●

●

●●

●
●
●

●

●

●

●

●●

●

●

●●

●

●

●

●

●
●

●
●●

●

●

●

●●

●
●

●

●

●
●

●

●

●
●

●

●

●

●

●

●

●

●

●

●

●

●
●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●●

●

●

●

●
●

●

●

●

●

●

●

●

●

●

●

●

●
●

●

●●

●

●

●

●

●

●

●

●

●●●

●

●

●

●

●
●

●

●

●

●●

●

●

●
●●

●

●

●
●

●●

●

●

●

●

●
●

●
●●

●

●

●

●

●

●

●

●

●

●●

●

●
●

●

●

●
●

●

●
●

●

●

●

●

●

●

●

●
●

●

●●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●
●●
●

●

●

●

●

●

●

●

●

●

●

●

●

●●
●

●

●

●

●

●

●
●●
●

●

●

●

●

●

●

●

●

●
●●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●●

●

●
●

●

●

●

●

●

●

●

●

●

●

●
●

●

●

●
●

●

●
●

●

●
●
●
●

●

●
●

●

●●

●

●

●

●

●

●

●

●
●

●

●
●●

●

●

●
●

●

●
●

●

●

●

●

●

●

●

●
●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●
●

●

●
●
●

●

●

●●

●

●

●

●

●

●

●●

●

●

●
●

●

●
●

●

●

●

●

●
●
●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●●

●
●

●●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●
●

●

●

●

●

●

●

●●

●

●●

●

●

●

●

●

●

●

●

●

●

●
●

●

●

●

●

●

●

●
●

●

●

●

●

●

●

●●

●

●

●

●

●

●

●
●

●

●

●

●

●

●

●●

●

●
●

●

●

●

●
●

●

●

●

●

●
●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●●

●

●

●

●●

●

●

●

●
●

●

●

●

●●

●

●

●
●●

●

●

●

●

●

●

●

●

●
●

●

●

●

●

●

●

●

●
●

●

●

●

●

●●

●

●

●●
●

●

●
●

●

●

●

●

●

●

●

●

●
●

●

●

●

●

●
●

●

●
●

●

●

●

●

●

●

●

●●

●

●

●

●

●

●
●

●

●

●

●

●

●

●

●
●
●

●

●

●

●

●●

●
●
●●

●

●●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●
●

●

●
●

●

●●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●●
●

●

●

●

●

●

●

●

●

●
●

●

●

●●
●

●
●
●
●

●

●

●
●

●

●

●

●
●●●

●

●

●

●

●

●
●

●

●●●

●

●

●
●●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●
●

●

●●

●

●

●

●

●

●

●
●

●

●

●

●

●

●

●●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●
●

●

●

●

●
●

●

●

●

●

●●

●

●

●
●

●

●

●

●

●

●

●

●

●

●

●●

●

●

●●

●

●
●

●

●●

●

●

●●

●

●

●

●

●

●

●
●●
●
●

●
●
●●●●

●

●

●

●

●

●

●

●

●

●

●●●
●

●

●
●

●

●

●

●

●

●

●

●

●

●●

●

●

●

●●

●

●

●

●

●●

●●

●

●●

●

●
●●

●●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●
●●

●

●●●

●

●

●

●

●

●●

●

●

●

●

●

●

●

●

●

●
●
●

●

●

●

●
●●
●

●

●

●

●

●

●

●

●

●

●

●●

●

●●

●

●●

●

●

●
●
●
●

●

●

●

●●

●

●

●

●●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●
●

●

●

●●

●

●●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●
●

●

●

●

●

●

●

●

●

●●

●

●

●

●

●

●

●●

●

●

●

●
●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●●

●
●

●

●

●

●

●

●

●

●

●

●

●●

●●

●

●

●

●

●

●
●●
●

●

●

●

●

●●

●
●

●

●●

●

●
●

●●

●

●
●

●●

●

●

●

●

●

●

●

●
●

●

●

●

●●

●

●

●

●
●
●

●

●

●

●

●

●

●

●

●

●

●
●

●

●

●

●

●
●

●

●

●●

●●

●

●

●●

●

●

●

●

●

●

●
●

●

●

●

●

●

●

●

●

●

●

●●

●

●

●

●

●

●

●

●

●

●

●●

●

●

●

●

●●

●

●

●

●

●

●

●

●

●

●
●

●

●

●

●

●

●

●●

●
●

●
●
●

●

●

●

●

●

●
●
●

●

●

●

●

●

●

●

●

●

●

●

●●

●

●
●

●

●

●

●

●
●
●

●

●

●

●

●

●

●

●

●

●

●

●

●

●
●

●

●

●

●

●

●

●

●
●

●

●

●

●
●

●

●

●

●●
●●

●

●

●

●

●

●

●
●

●

●

●

●
●

●

●

●

●●

●

●

●

●

●
●

●

●

●
●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●
●

●

●

●

●

●

●

●

●

●

●

●

●
●●
●

●

●

●

●

●

●

●
●

●

●

●

●

●
●
●

●

●

●
●
●
●
●

●●

●

●

●
●

●

●
●

●

●

●

●

●

●

●

●

●

●

●

●

●●

●

●

●

●●

●

●
●●

●

●●

●

●

●

●

●

●

●
●

●

●

●●

●
●
●

●

●

●

●

●

●

●

●
●

●
●

●

●

●

●

●

●

●●
●

●

●

●

●

●

●●

●

●

●
●

●

●

●

●

●

●

●

●

●●

●

●●●

●

●

●

●

●

●

●

●

●

●

●●

●

●
●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●●

●

●

●

●

●

●
●

●

●●
●●
●
●
●

●

●●

●

●

●
●

●
●

●

●

●

●

●●●

●

●

●

●●

●

●

●

●

●

●

●

●
●
●

●●
●

●

●●

●

●

●

●

●

●

●

●●

●
●

●

●

●

●
●

●

●
●
●●

●

●

●

●
●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●
●

●

●

●

●

●

●

●●●

●

●

●

●

●

●

●●

●

●

●

●

●
●

●

●●

●

●

●

●

●

●●

●

●

●

●

●●

●●●

●

●

●

●

●
●

●

●

●

●

●

●

●

●

●

●

●

●●

●

●

●

●

●

●

●

●

●

●●●

●

●

●

●

●

●

●
●

●

●

●

●

●

●

●

●

●

●
●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●●

●

●

●

●

●

●

●
●●

●

●

●

●

●

●

●
●

●

●●●
●●

●

●

●

●

●

●

●

●

●●●

●

●

●

●

●

●

●
●●

●

●

●

●

●

●

●●

●

●

●

●

●

●

●

●
●

●

●

●

●

●

●

●

●●●
●●●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●●

●●

●

●●

●

●

●

●

●

●

●
●

●

●

●
●
●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●●

●

●

●●

●

●
●

●●●

●

●

●

●●

●

●

●

●

●

●

●
●●

●●●
●

●

●

●●
●

●

●

●

●

●

●

●

●

●

●●

●
●

●

●

●

●

●

●

●

●
●

●●

●

●●

●

●

●

●

●

●

●

●

●

●
●

●●●●●

●

●

●

●●●

●

●

●

●

●
●

●

●

●

●

●

●

●

●

●

●

●●

●

●

●

●

●

●●

●●

●

●

●

●

●

●

●

●

●
●

●

●
●

●

●

●

●

●

●

●●

●

●

●

●

●

●

●●

●

●
●●
●

●

●

●

●

●

●

●●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●●

●

●

●

●

●

●

●

●

●

●
●
●

●

●

●
●

●●

●

●

●

●

●●

●

●

●

●

●●

●

●

●

●

●

●

●●

●

●

●

●

●

●

●

●

●

●
●

●

●

●

●
●

●

●

●

●●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●●

●

●

●

●
●

●

●
●

●

●

●

●

●

●

●

●

●●
●

●

●

●

●

●

●
●

●

●
●

●●

●

●

●

●

●

●

●

●
●

●

●

●

1 2 3 4 5

0.
0

0.
2

0.
4

0.
6

0.
8

1.
0

mnir

stars

pr
ob

ab
ili

ty
 p

os
iti

ve

Figure 2: Out-of-Sample fitted probabilities of a review being positive (having greater than 2 stars) as a
function of the true number of review stars. Box widths are proportional to number of observations in
each class; roughly 10% of reviews have each of 1-3 stars, while 30% have 4 stars and 40% have 5 stars.

each, as well as the combined 200 dimensional
DM+DBOW representation, to logistic regression.

Phrase regression applies logistic regression of re-
sponse classes directly onto counts for short 1-2
word ‘phrases’. The phrases are obtained using
gensim’s phrase builder, which simply combines
highly probable pairings; e.g., first date and
chicken wing are two pairings in this corpus.

MNIR, the multinomial inverse regression of
Taddy (2013a; 2013b; 2015) is applied as im-
plemented in the textir package for R. MNIR
maps from text to the class-space of inter-
est through a multinomial logistic regression of
phrase counts onto variables relevant to the class-
space. We apply MNIR to the same set of 1-2
word phrases used in phrase regression. Here, we
regress phrase counts onto stars expressed numeri-
cally and as a 5-dimensional indicator vector, lead-
ing to a 6-feature multinomial logistic regression.
The MNIR procedure then uses the 6×pmatrix of
feature-phrase regression coefficients to map from
phrase-count to feature space, resulting in 6 di-
mensional ‘sufficient reduction’ statistics for each
document. These are input to logistic regression.

Word2Vec aggregation averages fitted word rep-
resentations for a single Word2Vec trained on all
sentences to obtain a fixed-length feature vector
for each review (K = 100, as for inversion). This
vector is then input to logistic regression.

3.1 Results

Misclassification rates for each task on the valida-
tion set are reported in Table 1. Simple phrase-
count regression is consistently the strongest per-
former, bested only by Word2Vec inversion on
task b. This is partially due to the relative strengths
of discriminative (e.g., logistic regression) vs gen-

a (NP) b (NNP) c (1-5)

W2V inversion .099 .189 .435
Phrase regression .084 .200 .410

D2V DBOW .144 .282 .496
D2V DM .179 .306 .549

D2V combined .148 . 284 .500
MNIR .095 .254 .480

W2V aggregation .118 .248 .461

Table 1: Out-of-sample misclassification rates.

erative (e.g., all others here) classifiers: given
a large amount of training text, asymptotic effi-
ciency of logistic regression will start to work in
its favor over the finite sample advantages of a
generative classifier (Ng and Jordan, 2002; Taddy,
2013c). However, the comparison is also unfair
to Word2Vec and Doc2Vec: both phrase regres-
sion and MNIR are optimized exactly under AICc
selected penalty, while Word and Doc 2Vec have
only been approximately optimized under a sin-
gle specification. The distributed representations
should improve with some careful engineering.

Word2Vec inversion outperforms the other doc-
ument representation-based alternatives (except,
by a narrow margin, MNIR in task a). Doc2Vec
under DBOW specification and MNIR both do
worse, but not by a large margin. In contrast to
Le and Mikolov, we find here that the Doc2Vec
DM model does much worse than DBOW. Re-
gression onto simple within- document aggrega-
tions of Word2Vec perform slightly better than any
Doc2Vec option (but not as well as the Word2Vec
inversion). This again contrasts the results of Le
and Mikolov and we suspect that the more com-
plex Doc2Vec model would benefit from a careful

48



tuning of the SGD optimization routine.1

Looking at the fitted probabilities in detail we
see that Word2Vec inversion provides a more use-
ful document ranking than any comparator (in-
cluding phrase regression). For example, Figure
2 shows the probabilities of a review being ‘pos-
itive’ in task a as a function of the true star rat-
ing for each validation review. Although phrase
regression does slightly better in terms of misclas-
sification rate, it does so at the cost of classifying
many terrible (1 star) reviews as positive. This oc-
curs because 1-2 star reviews are more rare than 3-
5 star reviews and because words of emphasis (e.g.
very, completely, and !!!) are used both
in very bad and in very good reviews. Word2Vec
inversion is the only method that yields positive-
document probabilities that are clearly increasing
in distribution with the true star rating. It is not dif-
ficult to envision a misclassification cost structure
that favors such nicely ordered probabilities.

4 Discussion

The goal of this note is to point out inversion as an
option for turning distributed language representa-
tions into classification rules. We are not arguing
for the supremacy of Word2Vec inversion in par-
ticular, and the approach should work well with al-
ternative representations (e.g., Glove). Moreover,
we are not even arguing that it will always outper-
form purpose-built classification tools. However,
it is a simple, scalable, interpretable, and effective
option for classification whenever you are working
with such distributed representations.

References
Cheryl Flynn, Clifford Hurvich, and Jefferey Simonoff.

2013. Efficiency for Regularization Parameter Se-
lection in Penalized Likelihood Estimation of Mis-
specified Models. Journal of the American Statisti-
cal Association, 108:1031–1043.

Quoc V. Le and Tomas Mikolov. 2014. Distributed
representations of sentences and documents. In Pro-

1Note also that the unsupervised document representa-
tions – Doc2Vec or the single Word2Vec used in Word2Vec
aggregation – could be trained on larger unlabeled corpora. A
similar option is available for Word2Vec inversion: one could
take a single Word2Vec model trained on a large unlabeled
corpora as a shared baseline (prior) and update separate mod-
els with additional training on each labeled sub-corpora. The
representations will all be shrunk towards a baseline language
model, but will differ according to distinctions between the
language in each labeled sub-corpora.

ceedings of the 31 st International Conference on
Machine Learning.

Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S. Cor-
rado, and Jeff Dean. 2013. Distributed representa-
tions of words and phrases and their compositional-
ity. In Advances in Neural Information Processing
Systems, pages 3111–3119.

Frederic Morin and Yoshua Bengio. 2005. Hierarchi-
cal probabilistic neural network language model. In
Proceedings of the International Workshop on Arti-
ficial Intelligence and Statistics, pages 246–252.

Andrew Y. Ng and Michael I. Jordan. 2002. On Dis-
criminative vs Generative Classifiers: A Compar-
ison of Logistic Regression and naive Bayes. In
Advances in Neural Information Processing Systems
(NIPS).

Jeffrey Pennington, Richard Socher, and Christo-
pher D. Manning. 2014. Glove: Global vectors for
word representation. Proceedings of the Empiricial
Methods in Natural Language Processing (EMNLP
2014), 12.

Radim Rehurek and Petr Sojka. 2010. Software
Framework for Topic Modelling with Large Cor-
pora. In Proceedings of the LREC 2010 Workshop
on New Challenges for NLP Frameworks, pages 45–
50.

David Rumelhart, Geoffrey Hinton, and Ronald
Williams. 1986. Learning representations by back-
propagating errors. Nature, 323:533–536.

Richard Socher, Cliff C. Lin, Chris Manning, and An-
drew Y. Ng. 2011. Parsing natural scenes and natu-
ral language with recursive neural networks. In Pro-
ceedings of the 28th international conference on ma-
chine learning (ICML-11), pages 129–136.

Richard Socher, Alex Perelygin, Jean Y. Wu, Jason
Chuang, Christopher D. Manning, Andrew Y. Ng,
and Christopher Potts. 2013. Recursive deep mod-
els for semantic compositionality over a sentiment
treebank. In Proceedings of the conference on
empirical methods in natural language processing
(EMNLP), volume 1631, page 1642.

Matt Taddy. 2013a. Measuring Political Sentiment
on Twitter: Factor Optimal Design for Multinomial
Inverse Regression. Technometrics, 55(4):415–425,
November.

Matt Taddy. 2013b. Multinomial Inverse Regression
for Text Analysis. Journal of the American Statisti-
cal Association, 108:755–770.

Matt Taddy. 2013c. Rejoinder: Efficiency and struc-
ture in MNIR. Journal of the American Statistical
Association, 108:772–774.

Matt Taddy. 2014. One-step estimator paths for con-
cave regularization. arXiv:1308.5623.

Matt Taddy. 2015. Distributed Multinomial Regres-
sion. Annals of Applied Statistics, To appear.

49


