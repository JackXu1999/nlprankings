



















































Learning Word Reorderings for Hierarchical Phrase-based Statistical Machine Translation


Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics
and the 7th International Joint Conference on Natural Language Processing (Short Papers), pages 542–548,

Beijing, China, July 26-31, 2015. c©2015 Association for Computational Linguistics

Learning Word Reorderings for Hierarchical Phrase-based Statistical
Machine Translation

Jingyi Zhang1,2, Masao Utiyama1, Eiichro Sumita1, Hai Zhao3,4
1National Institute of Information and Communications Technology,

3-5Hikaridai, Keihanna Science City, Kyoto 619-0289, Japan
2Graduate School of Information Science, Nara Institute of Science and Technology,

Takayama, Ikoma, Nara 630-0192, Japan
3Department of Computer Science and Engineering,

Shanghai Jiao Tong University, Shanghai 200240, China
4Key Laboratory of Shanghai Education Commission for Intelligent Interaction

and Cognitive Engineering, Shanghai Jiao Tong University, Shanghai 200240, China
jingyizhang/mutiyama/eiichiro.sumita@nict.go.jp

zhaohai@cs.sjtu.edu.cn

Abstract
Statistical models for reordering source
words have been used to enhance the hier-
archical phrase-based statistical machine
translation system. Existing word reorder-
ing models learn the reordering for any
two source words in a sentence or only
for two continuous words. This paper pro-
poses a series of separate sub-models to
learn reorderings for word pairs with dif-
ferent distances. Our experiments demon-
strate that reordering sub-models for word
pairs with distance less than a specific
threshold are useful to improve translation
quality. Compared with previous work,
our method may more effectively and effi-
ciently exploit helpful word reordering in-
formation.

1 Introduction

The hierarchical phrase-based model (Chiang,
2005) is capable of capturing rich translation
knowledge with the synchronous context-free
grammar. But selecting proper translation rules
during decoding is a challenge as a huge number
of hierarchical rules can be applied to one source
sentence.

Chiang (2005) used a log-linear model to com-
pute rule weights with features similar to Pharaoh
(Koehn et al., 2003). However, to select appropri-
ate rules, more effective criteria are required. A lot
of work has been done for better rule selection. He
et al. (2008) and Liu et al. (2008) used maximum
entropy approaches to integrate rich contextual in-
formation for target side rule selection. Cui et al.
(2010) proposed a joint model to select hierarchi-
cal rules for both source and target sides.

Hayashi et al. (2010) demonstrated the ef-
fectiveness of using word reordering information
within hierarchical phrase-based SMT by integrat-
ing Tromble and Eisner (2009)’s word reordering
model into decoder as a feature, which estimates
the probability of any two source words in a sen-
tence being reordered during translating. Feng
et al. (2013) proposed a word reordering model
to learn reorderings only for continuous words,
which reduced computation cost a lot compared
with Tromble and Eisner (2009)’s model and still
achieved significant reordering improvement over
the baseline system.

In this paper, we incorporate word reordering
information into hierarchical phrase-based SMT
by training a series of separate reordering sub-
models for word pairs with different distances.
We will demonstrate that the translation perfor-
mance achieves consistent improvement as more
sub-models for longer distance reorderings being
integrated, but the improvement levels off quickly.
That means sub-models for reordering distance
longer than a given threshold do not improve trans-
lation quality significantly. Compared with previ-
ous models (Tromble and Eisner, 2009; Feng et al.,
2013), our method makes full use of helpful word
reordering information and also avoids unneces-
sary computation cost for long distance reorder-
ings. Besides, our reordering model is learned
by feed-forward neural network (FNN) for better
performance and uses efficient caching strategy to
further reduce time cost.

Phrase reordering models have also been inte-
grated into hierarchical phrase-based SMT. Phrase
reordering models were originally developed for
phrase-based SMT (Koehn et al., 2005; Zens and
Ney, 2006; Ni et al., 2009; Li et al., 2014) and

542



could not be used in hierarchical phrase-based
model directly. Nguyen and Vogel (2013) and
Cao et al. (2014) proposed to integrate phrase-
based reordering features into hierarchical phrase-
based SMT. However, their work limited to learn-
ing the reordering of continuous phrases. For short
phrases, in extreme cases, when phrase length is
one, their model only learned reordering for con-
tinuous word pairs like Feng et al. (2013)’s work,
while our model can be applied to word pairs with
longer distances.

2 Our Approach

Let em1 = e1, . . . , em be a target translation of
f l1 = f1, . . . , fl and A be word alignments be-
tween em1 and f

l
1, our model estimates the reorder-

ing probability of the source sentence as follows:

Pr
(
f l1, e

m
1 , A

)
≈

N∏
n=1

∏
i,j:1≤i<j≤l,j−i=n

Pr
(
f l1, e

m
1 , A, i, j

) (1)
where Pr

(
f l1, e

m
1 , A, i, j

)
is the reordering prob-

ability of the word pair 〈fi, fj〉 during translat-
ing; N is the maximum distance for source word
reordering, which is empirically determined by
supposing that estimating reorderings longer than
N does not improve translation performance any
more.

Previous word reordering models (Tromble and
Eisner, 2009; Feng et al., 2013) consider the re-
ordering of a source word pair to be reversed or
not. When a source word is aligned to several
uncontinuous target words, it can be hard to de-
termine if a word pair is reversed or not. They
solved this problem by only using one alignment
from multiple alignments and ignoring the others.
In contrast, our model handles all alignments as
shown below.

Suppose that fi is aligned to πi (πi ≥ 0) target
words. When πi > 0, {aik|1 ≤ k ≤ πi} stands for
the positions of target words aligned to fi. If πi =
0 or πj = 0, Pr

(
f l1, e

m
1 , A, i, j

)
= 1, otherwise,

Pr
(
f l1, e

m
1 , A, i, j

)
=

πi∏
u=1

πj∏
v=1

Pr
(
oijuv|fi−3, ..., fj+3, eaiu , eajv

)
where

oijuv =

{
0 (aiu ≤ ajv)
1 (aiu > ajv)

(2)

We train a series of sub-models,

M1,M2, . . . ,MN

Algorithm 1 Extract training instances.
Require: A pair of parallel sentence f l1 and em1 with word

alignments.
Ensure: Training examples for M1,M2, . . . ,MN .

for i = 1 to l − 1 do
for j = i+ 1 to l do

if j − i ≤ N then
for u = 1 to πi do

for v = 1 to πj do
if aiu ≤ ajv then(

fi−3, ..., fj+3, eaiu , eajv , 0
)

is a neg-
ative instance for Mj−i

else(
fi−3, ..., fj+3, eaiu , eajv , 1

)
is a posi-

tive instance for Mj−i

to learn reorderings for word pairs with different
distances. That means, for the word pair 〈fi, fj〉
with distance j − i = n, its reordering proba-
bility Pr

(
oijuv|fi−3, ..., fj+3, eaiu , eajv

)
is esti-

mated by Mn. Different sub-models are trained
and integrated into the translation system sepa-
rately.

Each sub-model Mn is implemented by an
FNN, which has the same structure with the neu-
ral language model in (Vaswani et al., 2013).
The input to Mn is a sequence of n + 9
words: fi−3, ..., fj+3, eaiu , eajv . The input layer
projects each word into a high dimensional
vector using a matrix of input word embed-
dings. Two hidden layers can combine all in-
put data1. The output layer has two neurons that
give Pr

(
oijuv = 1|fi−3, ..., fj+3, eaiu , eajv

)
and

Pr
(
oijuv = 0|fi−3, ..., fj+3, eaiu , eajv

)
.

那个 戴 眼镜 的 男生 是 詹姆士 

That guy who wears  glasses  is James  

Figure 1: A Chinese-English sentence pair.

The backpropagation algorithm is used to train
these reordering sub-models. The training in-
stances for each sub-model are extracted from the
word-aligned parallel corpus according to Algo-
rithm 1. For example, the word pair “戴(wears)
男生(guy)” in Figure 1 will be extracted as a pos-
itive instance for M3. The input of this instance is
as follows: “<s> <s>那个戴眼镜的男生是

1If we choose the averaged perceptron algorithm to learn
reordering task as used in (Hayashi et al., 2010), we need to
artificially select n-gram features, which is not necessary for
FNN.

543



詹姆士</s> wears guy”, where<s> and</s>
represent the beginning and ending of a sentence.
If a word never occurs or only occurs once in train-
ing corpus, we replace it with a special symbol
<unk>.

3 Integration into the Decoder

In the hierarchical phrase-based model, a transla-
tion rule r is like:

X → 〈γ, α,∼〉
where X is a nonterminal, γ and α are re-

spectively source and target strings of terminals
and nonterminals, and ∼ is the alignment between
nonterminals and terminals in γ and α.

Each rule has several features and the feature
weights are tuned by the minimum error rate train-
ing (MERT) algorithm (Och, 2003). To integrate
our model into the hierarchical phrase-based trans-
lation system, a new feature scoren (r) is added
to each rule r for each Mn. The score of this fea-
ture is calculated during decoding. Note that these
scores are correspondingly calculated for differ-
ent sub-modelsMn and the sub-model weights are
tuned separately.

Suppose that r is applied to the input sentence
f l1, where

• r covers the source span [fϕ, fϑ]
• γ contains nonterminals {Xk|1 ≤ k ≤ K}
• Xk covers the span [fϕk , fϑk ]
Then

scoren (r)

=
∑

〈i,j〉∈S−
K⋃

k=1

Sk∧j−i=n

log
(
Pr
(
f l1, e

m
1 , A, i, j

))
where
S : {〈i, j〉 |ϕ ≤ i < j ≤ ϑ}
Sk : {〈i, j〉 |ϕk ≤ i < j ≤ ϑk}

For example, if a rule “X1 X2 男生→ X1 guy
X2” is applied to the input sentence in Figure 1,
then

[fϕ, fϑ] = [1, 5] ; [fϕ1 , fϑ1 ] = [1, 1] ; [fϕ2 , fϑ2 ] = [2, 4]

S −
K⋃
k=1

Sk =

{
〈1, 2〉 , 〈1, 3〉 , 〈1, 4〉 , 〈1, 5〉 ,
〈2, 5〉 , 〈3, 5〉 , 〈4, 5〉

}
One concern in using target features is the com-

putational efficiency, because reordering probabil-
ities have to be calculated during decoding. So we
cache probabilities to reduce the expensive neural
network computation in experiments.

4 Experiments

We evaluated the proposed approach for Chinese-
to-English (CE) and Japanese-to-English (JE)
translation tasks. The official datasets for the
patent machine translation task at NTCIR-9 (Goto
et al., 2011) were used. The detailed statistics for
training, development and test sets are given in Ta-
ble 1.

SOURCE TARGET

CE

TRAINING #Sents 954K
#Words 37.2M 40.4M
#Vocab 288K 504K

DEV #Sents 2K
TEST #Sents 2K

JE

TRAINING #Sents 3.14M
#Words 118M 104M
#Vocab 150K 273K

DEV #Sents 2K
TEST #Sents 2K

Table 1: Data sets.

In NTCIR-9, the development and test sets were
both provided for CE task while only the test set
was provided for the JE task. Therefore, we used
the sentences from the NTCIR-8 JE test set as the
development set for JE task. The word segmenta-
tion was done by BaseSeg (Zhao et al., 2006; Zhao
and Kit, 2008; Zhao et al., 2010; Zhao and Kit,
2011; Zhao et al., 2013) for Chinese and Mecab2

for Japanese.
To learn neural reordering models, the train-

ing and development sets were put together to ob-
tain symmetric word alignments using GIZA++
(Och and Ney, 2003) and the grow-diag-final-
and heuristic (Koehn et al., 2003). The reorder-
ing instances extracted from the aligned training
and development sets were used as the training
and validation data respectively for learning neu-
ral reordering models. Neural reordering models
were trained by the toolkit NPLM (Vaswani et al.,
2013). For CE task, training instances extracted
from all the 1M sentence pairs were used to train
neural reordering models. For JE task, training
instances were from 1M sentence pairs that were
randomly selected from all the 3.14M sentence
pairs.

We also implemented Hayashi et al. (2010)’s
model for comparison. The training instances for
their model were extracted from the same sentence
pairs as ours.

2http://sourceforge.net/projects/mecab/files/

544



Base Hayashi M11 M
2
1 M

3
1 M

4
1

model
CE 32.95 34.25 34.78 35.75 35.97 36.05
JE 30.13 30.70 31.35 32.07 32.40 32.60

(a) BLEU scores
CE Base Hayashi M11 M

2
1 M

3
1

model
Hayashi
model �
M11 � �
M21 � � �
M31 � � � >
M41 � � � > −
JE Base Hayashi M11 M

2
1 M

3
1

model
Hayashi
model �
M11 � �
M21 � � �
M31 � � � �
M41 � � � � −

(b) Significance test results using bootstrap sampling (Koehn,
2004) w.r.t. BLEU scores. The symbol � represents a sig-
nificant difference at the p < 0.01 level; > represents a sig-
nificant difference at the p < 0.05 level; − means not signif-
icantly different at p = 0.05.

Table 2: Translation results.

For each translation task, the recent version
of the Moses hierarchical phrase-based decoder
(Koehn et al., 2007) with the training scripts was
used as the baseline system Base. We used the
default parameters for Moses. A 5-gram language
model was trained on the target side of the training
corpus by IRST LM Toolkit3 with the improved
Kneser-Ney smoothing.

We integrated our reordering models into Base.
Table 2 gives detailed translation results. “Hayashi
model” represents the method of (Hayashi et al.,
2010). “M j1 (j = 1, 2, 3, 4)” means that Base was
augmented with the reordering scores calcuated
from a series of sub-models M1 to Mj .

As shown in Table 2, integrating only M1,
which predicts reordering for two continuous
source words, has already given BLEU improve-
ment 1.8% and 1.2% over baseline on CE and
JE, respectively. As more sub-models for longer
distance reordering being integrated, the transla-
tion performance improved consistently, though
the improvement leveled off quickly. For CE and
JE tasks, Mn with n ≥ 3 and n ≥ 4, respectively,
cannot give further performance improvement at
any significant level.

Why did the improvement level off quickly?

3http://hlt.fbk.eu/en/irstlm

Sub-model M1 M2 M3 M4
CE 93.9 92.8 92.2 91.2
JE 92.9 91.3 90.1 89.3

(a) Our model
Reordering
Distance 1 2 3 4
CE 90.1 88.3 87.0 85.6
JE 85.3 81.9 80.6 78.8

(b) Hayashi model

Table 3: Classification accuracy (%).

In other words, why do long distance reordering
models have a much less leverage over translation
performance than short ones?

First, the prediction accuracy decreases as the
reordering distance increasing. Table 3a gives
classification accuracies on the validation data for
each sub-model. The reason for accuracy decreas-
ing is that the input size of sub-model grows as
reordering distance increasing. Namely, long dis-
tance reordering needs to consider more compli-
cated context.

Second, we attribute the influence decrease of
the longer reordering models to the redundancy of
the predictions among different reordering mod-
els. For example, in Figure 1, when word pairs
“男生(guy) 是(is)” and “是(is) 詹姆士(James)”
are both predicted to be not reversed, the reorder-
ing for “男生(guy) 詹姆士(James)” can be logi-
cally determined to be not reversed without further
reordering model prediction. That means, some-
times, a long distance word reordering can be de-
termined by a series of shorter word reordering
pairs.

But still, some predictions for longer reorder-
ing are useful. For example, the reordering
of “戴(wears) 男生(guy)” cannot be determined
when “戴(wears)眼镜(glasses)” is predicted to be
not reversed and “眼镜(glasses)男生(guy)” is re-
versed. This is the reason why translation perfor-
mance improves as more sub-models being inte-
grated.

As shown in Table 2, with 4 sub-models be-
ing integrated, our model improved baseline sys-
tem significantly and also outperformed Hayashi
model clearly. It is easy to understand, since our
model was trained by feed-forward neural network
on a high dimensional space and incorporated rich
context information, while Hayashi model used
the averaged perceptron algorithm and simple fea-
tures. Table 3b shows the prediction accuracies

545



of Hayashi model. Note that Hayashi model pre-
dicts reorderings for all word pairs, but only pre-
diction accuracies for word pairs with distance 4
or less are shown. Compared with Table 3a, the
prediction accuracy of our model is much higher
than Hayashi model. Actually, FNN is not suitable
for Hayashi model since the computation cost for
Hayashi model is quite expensive. Using FNN to
reorder all word pairs could cost nearly one minute
to translate one sentence according to our experi-
ments, while integrating 4 sub-models only cost
10 seconds4.

Compared with Hayashi model, our model not
only speeds up decoding time but also reduces
the training time. Training for Hayashi model
is much slower since word pairs with all differ-
ent distances are used as training data. By using
separate sub-models, we can train each sub-model
one by one and stop when translation performance
cannot be improved any more. However, despite
of efficiency, one unified model will theoretically
have better performance than separate sub-models
since separate sub-models do not share training in-
stances and the unified model will suffer less from
data sparsity. So, we did some extra experiments
and trained a neural network which had the same
structure as M4 to learn reorderings for all word
pairs with distance 4 or less, instead of using 4
separate neural networks. A specific word null
was used since word pairs with distance 1,2,3 do
not have enough inputs for M4. The significance
test results showed that translation performance
had no significant difference between one unified
model and multiple sub-models. This is because
the training corpus for our model is quite large, so
separate training sets are sufficient for each sub-
model to learn the reorderings well. Besides, us-
ing neural networks to learn these sub-models on
a continuous space can relieve the data sparsity
problem to some extent.

Note that if we only integrate M4 into Base, the
translation quality of Base can be improved in our
preliminary experiments. But M4 cannot predict
reorderings for word pairs with distance less than
4. So M31 will be still needed for predicting re-
orderings of word pairs with distance 1,2,3. But
after M31 being integrated, M4 will not be needed
due to the redundancy of the predictions among

4Note that cache was used in all our experiments to reduce
the expensive neural network computation cost and turned out
to be very useful. Without caching, integrating 4 sub-models
could cost nearly 7 minutes to translate a sentence.

different reordering models.

5 Conclusion

In this paper, we propose to enhance hierarchi-
cal phrase-based SMT by training a series of sep-
arate sub-models to learn reorderings for word
pairs with distances less than a specific thresh-
old, based on the experimental fact that longer dis-
tance reordering models are not quite helpful for
translation quality. Compared with Hayashi et al.
(2010)’s work, our model is much more efficient
and keeps all helpful word reordering informa-
tion. Besides, our reordering model is learned by
feed-forward neural network and incorporates rich
context information for better performance. On
both Chinese-to-English and Japanese-to-English
translation tasks, the proposed model outperforms
the previous model significantly.

Acknowledgments

Masao Utiyama and Hai Zhao are corresponding
authors. This work was done when the first au-
thor was a master’s student at Shanghai Jiao Tong
University. Hai Zhao was supported by the Na-
tional Natural Science Foundation of China un-
der Grants 60903119, 61170114 and 61272248,
the National Basic Research Program of China un-
der Grant 2013CB329401, the Science and Tech-
nology Commission of Shanghai Municipality un-
der Grant 13511500200, the European Union Sev-
enth Framework Program under Grant 247619, the
Cai Yuanpei Program (CSC fund 201304490199,
201304490171), and the art and science interdisci-
pline funds of Shanghai Jiao Tong University un-
der Grant 14X190040031(14JCRZ04).

References
Hailong Cao, Dongdong Zhang, Mu Li, Ming Zhou,

and Tiejun Zhao. 2014. A lexicalized reorder-
ing model for hierarchical phrase-based translation.
In Proceedings of COLING 2014, the 25th Inter-
national Conference on Computational Linguistics:
Technical Papers, pages 1144–1153.

David Chiang. 2005. A hierarchical phrase-based
model for statistical machine translation. In Pro-
ceedings of the 43rd Annual Meeting of the Associa-
tion for Computational Linguistics (ACL’05), pages
263–270.

Lei Cui, Dongdong Zhang, Mu Li, Ming Zhou, and
Tiejun Zhao. 2010. A joint rule selection model

546



for hierarchical phrase-based translation. In Pro-
ceedings of the ACL 2010 Conference Short Papers,
pages 6–11.

Minwei Feng, Jan-Thorsten Peter, and Hermann Ney.
2013. Advancements in reordering models for sta-
tistical machine translation. In Proceedings of the
51st Annual Meeting of the Association for Compu-
tational Linguistics (Volume 1: Long Papers), pages
322–332.

Isao Goto, Bin Lu, Ka Po Chow, Eiichiro Sumita, and
Benjamin K Tsou. 2011. Overview of the patent
machine translation task at the NTCIR-9 workshop.
In Proceedings of The 9th NII Test Collection for IR
Systems Workshop Meeting, pages 559–578.

Katsuhiko Hayashi, Hajime Tsukada, Katsuhito Su-
doh, Kevin Duh, and Seiichi Yamamoto. 2010.
Hierarchical phrase-based machine translation with
word-based reordering model. In Proceedings of the
23rd International Conference on Computational
Linguistics (Coling 2010), pages 439–446.

Zhongjun He, Qun Liu, and Shouxun Lin. 2008. Im-
proving statistical machine translation using lexical-
ized rule selection. In Proceedings of the 22nd In-
ternational Conference on Computational Linguis-
tics (Coling 2008), pages 321–328.

Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In
Proceedings of the 2003 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics on Human Language Technology-
Volume 1, pages 48–54.

Philipp Koehn, Amittai Axelrod, Alexandra Birch,
Chris Callison-Burch, Miles Osborne, David Tal-
bot, and Michael White. 2005. Edinburgh system
description for the 2005 IWSLT speech translation
evaluation. In The International Workshop on Spo-
ken Language Translation, pages 68–75.

Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, Chris Dyer, Ondrej Bojar, Alexan-
dra Constantin, and Evan Herbst. 2007. Moses:
Open source toolkit for statistical machine transla-
tion. In Proceedings of the 45th Annual Meeting of
the Association for Computational Linguistics Com-
panion Volume Proceedings of the Demo and Poster
Sessions, pages 177–180.

Philipp Koehn. 2004. Statistical significance tests for
machine translation evaluation. In Proceedings of
EMNLP 2004, pages 388–395.

Peng Li, Yang Liu, Maosong Sun, Tatsuya Izuha, and
Dakun Zhang. 2014. A neural reordering model for
phrase-based translation. In Proceedings of COL-
ING 2014, the 25th International Conference on
Computational Linguistics: Technical Papers, pages
1897–1907.

Qun Liu, Zhongjun He, Yang Liu, and Shouxun Lin.
2008. Maximum entropy based rule selection model
for syntax-based statistical machine translation. In
Proceedings of the 2008 Conference on Empirical
Methods in Natural Language Processing, pages
89–97.

ThuyLinh Nguyen and Stephan Vogel. 2013. Integrat-
ing phrase-based reordering features into a chart-
based decoder for machine translation. In Proceed-
ings of the 51st Annual Meeting of the Association
for Computational Linguistics (Volume 1: Long Pa-
pers), pages 1587–1596.

Yizhao Ni, Craig Saunders, Sandor Szedmak, and Ma-
hesan Niranjan. 2009. Handling phrase reorderings
for machine translation. In Proceedings of the ACL-
IJCNLP 2009 Conference Short Papers, pages 241–
244.

Franz Josef Och and Hermann Ney. 2003. A sys-
tematic comparison of various statistical alignment
models. Computational linguistics, 29(1):19–51.

Franz Josef Och. 2003. Minimum error rate training
in statistical machine translation. In Proceedings of
the 41st Annual Meeting of the Association for Com-
putational Linguistics, pages 160–167.

Roy Tromble and Jason Eisner. 2009. Learning linear
ordering problems for better translation. In Proceed-
ings of the 2009 Conference on Empirical Methods
in Natural Language Processing, pages 1007–1016.

Ashish Vaswani, Yinggong Zhao, Victoria Fossum,
and David Chiang. 2013. Decoding with large-
scale neural language models improves translation.
In Proceedings of the 2013 Conference on Empiri-
cal Methods in Natural Language Processing, pages
1387–1392.

Richard Zens and Hermann Ney. 2006. Discriminative
reordering models for statistical machine translation.
In Proceedings on the Workshop on Statistical Ma-
chine Translation, pages 55–63.

Hai Zhao and Chunyu Kit. 2008. Exploiting unla-
beled text with different unsupervised segmentation
criteria for chinese word segmentation. Research in
Computing Science, 33:93–104.

Hai Zhao and Chunyu Kit. 2011. Integrating unsu-
pervised and supervised word segmentation: The
role of goodness measures. Information Sciences,
181(1):163–183.

Hai Zhao, Chang-Ning Huang, and Mu Li. 2006. An
improved chinese word segmentation system with
conditional random field. In Proceedings of the Fifth
SIGHAN Workshop on Chinese Language Process-
ing, pages 162–165.

Hai Zhao, Chang-Ning Huang, Mu Li, and Bao-Liang
Lu. 2010. A unified character-based tagging frame-
work for chinese word segmentation. ACM Trans-
actions on Asian Language Information Processing
(TALIP), 9(2):5.

547



Hai Zhao, Masao Utiyama, Eiichiro Sumita, and Bao-
Liang Lu. 2013. An empirical study on word seg-
mentation for chinese machine translation. In Com-
putational Linguistics and Intelligent Text Process-
ing, pages 248–263.

548


