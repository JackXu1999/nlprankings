



















































Computational analysis of Gondi dialects


Proceedings of the Fourth Workshop on NLP for Similar Languages, Varieties and Dialects, pages 26–35,
Valencia, Spain, April 3, 2017. c©2017 Association for Computational Linguistics

Computational analysis of Gondi dialects

Taraka Rama and Çağrı Çöltekin and Pavel Sofroniev
Department of Linguistics

University of Tübingen, Germany
taraka-rama.kasicheyanula@uni-tuebingen.de

cagri.coeltekin@sfs.uni-tuebingen.de
pavel.sofroniev@student.uni-tuebingen.de

Abstract

This paper presents a computational anal-
ysis of Gondi dialects spoken in central
India. We present a digitized data set of
the dialect area, and analyze the data us-
ing different techniques from dialectome-
try, deep learning, and computational bi-
ology. We show that the methods largely
agree with each other and with the ear-
lier non-computational analyses of the lan-
guage group.

1 Introduction

Gondi languages are spoken across a large region
in the central part of India (cf. figure 1). The lan-
guages belong to the Dravidian language family
and are closely related to Telugu, a major literary
language spoken in South India. The Gondi lan-
guages received wide attention in comparative lin-
guistics (Burrow and Bhattacharya, 1960; Garap-
ati, 1991; Smith, 1991) due to their dialectal vari-
ation. On the one hand, the languages look like a
dialect chain while, on the other hand, some of the
dialects are shown to exhibit high levels of mutual
unintelligibility (Beine, 1994).

Smith (1991) and Garapati (1991) perform clas-
sical comparative analyses of the dialects and clas-
sify the Gondi dialects into two subgroups: North-
west and Southeast. Garapati (1991) compares
Gondi dialects where most of the dialects belong
to Northwest subgroup and only three dialects be-
long to Southeast subgroup. In a different study,
Beine (1994) collected lexical word lists tran-
scribed in International Phonetic Alphabet (IPA)
for 210 concepts belonging to 46 sites and at-
tempted to perform a classification based on word
similarity. Beine (1994) determines two words to
be cognate (having descended from the same com-
mon ancestor) if they are identical in form and

meaning. The average similarity between two sites
is determined as the average number of identical
words between the two sites. The author describes
the experiments of the results qualitatively and
does not perform any quantitative analysis. Until
now, there has been no computational analysis of
the lexical word lists to determine the exact rela-
tionship between these languages. We digitize the
dataset and then perform a computational analysis.

Recent years have seen an increase in the num-
ber of computational methods applied to the study
of both dialect and language classification. For in-
stance, Nerbonne (2009) applied Levenshtein dis-
tance for the classification of Dutch and German
dialects. Nerbonne finds that the classification of-
fered by Levenshtein distance largely agrees with
the traditional dialectological knowledge of Dutch
and German areas. In this paper, we apply the di-
alectometric analysis to the Gondi language word
lists.

In the related field of computational histor-
ical linguistics, Gray and Atkinson (2003) ap-
plied Bayesian phylogenetic methods from com-
putational biology to date the age of Proto-Indo-
European language tree. The authors use cog-
nate judgments given by historical linguists to in-
fer both the topology and the root age of the Indo-
European family. In parallel to this work, Kon-
drak (2009) applied phonetically motivated string
similarity measures and word alignment inspired
methods for the purpose of determining if two
words are cognates or not. This work was fol-
lowed by List (2012) and Rama (2015) who em-
ployed statistical and string kernel methods for de-
termining cognates in multilingual word lists.

In typical dialectometric studies (Nerbonne,
2009), the assumption that all the pronunciations
of a particular word are cognates is often justified
by the data. However, we cannot assume that this
is the case in Gondi dialects since there are sig-

26



Figure 1: The Gondi language area with major cities in focus. The dialect/site codes and the geographical
distribution of the codes are based on Beine (1994).

nificant amount of lexical replacement due to bor-
rowing (from contact) and internal lexical innova-
tions. Moreover, the previous comparative linguis-
tic studies classify the Gondi dialects using sound
correspondences and lexical cognates. In this pa-
per, we will use the Pointwise Mutual Information
(Wieling et al., 2009) method for obtaining sound
change matrices and use the matrix to automati-
cally identify cognates.

The comparative linguistic research classified
the Gondi dialects into five different genetic
groups (cf. table 1). However, the exact branching
of the Gondi dialects is yet a open question. In this
paper, we apply both dialectometric and phyloge-
netic approaches to determine the exact branching
structure of the dialects.

The paper is organized as followed. In sec-
tion 2, we describe the dataset and the gold stan-
dard dialect classification used in our experiments.
In section 3, we describe the various techniques
for computing and visualizing the dialectal differ-
ences. In section 4, we describe the results of the
different analyses. We conclude the paper in sec-
tion 5.

2 Datasets

The word lists for our experiments are derived
from the fieldwork of Beine (1994). Beine (1994)
provides multilingual word lists for 210 mean-
ings in 46 sites in central India which is shown
in figure 1. In the following sections, we use
the Glottolog classification (Nordhoff and Ham-
marström, 2011) as gold standard to evaluate the
various analyses. Glottolog is a openly avail-
able resource that summarizes the genetic relation-

ships of the world’s dialects and languages from
published scholarly linguistic articles. For refer-
ence, we provide the Glottolog classification1 of
the Gondi dialects in table 1. The Glottolog clas-
sification is derived from comparative linguistics
(Garapati, 1991; Smith, 1991) and dialect mutual
intelligibility tests (Beine, 1994).

Dialect codes Classification

gdh, gam, gar, gse, glb,
gtd, gkt, gch, prg, gka,
gwa, grp, khu, ggg, gcj,
bhe, pmd, psh, pkh, ght

Northwest Gondi, Northern
Gondi

rui, gki, gni, dog, gut,
gra, lxg

Northwest Gondi, Southern
Gondi

met, get, mad, gba, goa,
mal, gja, gbh, mbh

Southeast Gondi, General
Southeast Gondi, Hill
Maria-Koya, Hill Maria

mku, mdh, ktg, mud,
mso, mlj, gok

Southeast Gondi, General
Southeast Gondi, Muria

bhm, bhb, bhs Southeast Gondi, General
Southeast Gondi, Bison Horn
Maria

Table 1: Classification of the 46 sites according to
Glottolog (Nordhoff and Hammarström, 2011).

The whole dialect region is divided into two
major groups: Northwest Gondi and Southeast
Gondi which are divided into five major sub-
groups: Northern Gondi, Southern Gondi, Hill
Maria, Bison Horn Maria, Muria where Northern
Gondi and Southern Gondi belong to the North-
west Gondi branch whereas the rest of the sub-
groups belong to Southeast Gondi branch. It has

1http://glottolog.org/resource/
languoid/id/gond1265

27



to be noted that there is no gold standard about the
internal structure of dialects belonging to each di-
alect group.

3 Methods for comparing and visualizing
dialectal differences

We use the IPA transcribed data to compute
both unweighted and weighted string similar-
ity/distance between two words. We use the same
IPA data to train LSTM autoencoders introduced
by Rama and Çöltekin (2016) and project the au-
toencoder based distances onto a map.

As mentioned earlier, the dialectometric analy-
ses typically assume that all words that share the
same meaning are cognates. However, as shown
by Garapati (1991), some Gondi dialects exhibit
a clear tree structure. Both dialectometric and
autoencoder methods only provide an aggregate
amount of similarity between dialects and do not
work with cognates directly. The methods are sen-
sitive to lexical differences only through high dis-
similarity of phonetic strings. Since lexical and
phonetic differences are likely to indicate differ-
ent processes of linguistic change, we also analyze
the categorical differences due to lexical borrow-
ings/changes. For this purpose, we perform auto-
matic cognate identification and then use the in-
ferred cognates to perform both Bayesian phylo-
genetic analysis and dialectometric analysis.

3.1 Dialectometry

3.1.1 Computing aggregate distances
In this subsection, we describe how Levenshtein
distance and autoencoder based methods are em-
ployed for computing site-site distances.

Levenshtein distance: Levenshtein distance is
defined as the minimum number of edit operations
(insertion, deletion, and substitution) that are re-
quired to transform one string to another. We use
the Gabmap (Nerbonne et al., 2011) implementa-
tion of Levenshtein distance to compute site-site
differences.

Autoencoders: Rama and Çöltekin (2016) in-
troduced LSTM autoencoders for the purpose of
dialect classification. Autoencoders were em-
ployed by Hinton and Salakhutdinov (2006) for
reducing the dimensionality of images and docu-
ments. Autoencoders learn a dense representation
that can be used for clustering the documents and
images.

An autoencoder network consists of two parts:

encoder and decoder. The encoder network takes
a word as an input and transforms the word to a
fixed dimension representation. The fixed dimen-
sion representation is then supplied as an input to
a decoder network that attempts to reconstruct the
input word. In our paper, both the encoder and
decoder networks are Long-Short Term Memory
networks (Hochreiter and Schmidhuber, 1997).

In this paper, each word is represented as a
sequence (x1, . . . xT ) of one-hot vectors of di-
mension |P | where P is the total number (58)
of IPA symbols across the dialects. The encoder
is a LSTM network that transforms each word
into h ∈ Rk where k is predetermined before-
hand (in this paper, k is assigned a value of 32).
The decoder consists of another LSTM network
that takes h as input at each timestep to predict
an output representation. Each output represen-
tation is then supplied to a softmax function to
yield x̂t ∈ R|P |. The autoencoder network is
trained using the binary cross-entropy function
(−∑t xtlog(x̂t) + (1 − xt)log(1 − x̂t)) where,
xt is a 1-hot vector and x̂t is the output of the soft-
max function at timestep t to learn both the en-
coder and decoder LSTM’s parameters. Following
Rama and Çöltekin (2016), we use a bidirectional
LSTM as the encoder network and a unidirec-
tional LSTM as the decoder network. Our autoen-
coder model was implemented using Keras (Chol-
let, 2015) with Tensorflow (Abadi et al., 2016) as
the backend.

3.1.2 Visualization
We use Gabmap, a web-based application for di-
alectometric analysis for visualizing the site-site
distances (Nerbonne et al., 2011; Leinonen et al.,
2016).2 Gabmap provides a number of methods
for analyzing and visualizing dialect data. Below,
we present maps and graphics that are results of
multi-dimensional scaling (MDS) clustering.

For all analyses, Gabmap aggregates the differ-
ences calculated over individual items (concepts)
to a site-site distance matrix. With phonetic data, it
uses site-site differences based on string edit dis-
tance with a higher penalty for vowel–consonant
alignments and a lower penalty for the alignments
of sound pairs that differ only in IPA diacritics.
With binary data, Gabmap uses Hamming dis-
tances to compute the site-site differences. The
cognate clusters obtained from the automatic iden-

2Available at http://gabmap.nl/.

28



tification procedure (section 2.2) forms categories
(cognate clusters) which are analyzed using bi-
nary distances. Finally, we also visualize the dis-
tances from the autoencoders (section 2.1) using
Gabmap.

Gabmap provides various agglomerative hierar-
chical clustering methods for clustering analyses.
In all the results below, we use Ward’s method for
calculating cluster differences. For our analyses,
we present the clustering results on (color) maps
and dendrograms. Since the clustering is known to
be relatively unstable, we also present probabilis-
tic dendrograms that are produced by noisy clus-
tering (Nerbonne et al., 2008). In noisy clustering,
a single cluster analysis is performed a large num-
ber of times (∼ 100) by adding a small noise to
the distance matrix that is proportional to the stan-
dard deviation of the original distance matrix. The
combined analysis then provides statistical sup-
port for the branches in a dendrogram.

The multi-dimensional scaling (MDS) is a use-
ful analysis/visualization technique for verifying
the clustering results and visualizing the dialect
continuum. A site-site (linguistic) distance ma-
trix represents each site on a multi-dimensional
space. MDS ‘projects’ these distances to a smaller
dimensional space that can be visualized easily. In
dialect data, the distances in few most-important
MDS dimensions correlate highly with the orig-
inal distances, and these dimensions often cor-
respond to linguistically meaningful dimensions.
Below, we also present maps where areas around
the linguistic similar locations are plotted using
similar colors.

3.2 Phylogenetic approaches

3.2.1 Automatic cognate detection
Given a multilingual word list for a concept, the
automatic cognate detection procedure (Hauer and
Kondrak, 2011) can be broken into two parts:

1. Compute a pairwise similarity score for all
word pairs in the concept.

2. Supply the pairwise similarity matrix to a
clustering algorithm to output clusters that
show high similarity with one another.

Needleman-Wunsch algorithm (NW, Needle-
man and Wunsch (1970); the similarity counter-
part of Levenshtein distance) is a possible choice
for computing the similarity between two words.
The NW algorithm maximizes similarity whereas

Levenshtein distance minimizes the distance be-
tween two words. The NW algorithm assigns a
score of 1 for character match and a score −1 for
character mismatch. Unlike Levenshtein distance,
NW algorithm assigns a penalty score for opening
a gap (deletion operation) and a penalty for gap
extension which models the fact that deletion op-
erations occur in chunks (Jäger, 2013).

The NW algorithm is not sensitive to differ-
ent sound segment pairs, but a realistic algorithm
should assign higher similarity score to sound cor-
respondences such as /l/ ∼ /r/ than the sound cor-
respondences /p/ ∼ /r/. The weighted Needleman-
Wunsch algorithm takes a segment-segment sim-
ilarity matrix as input and then aligns the two
strings to maximize the similarity between the two
words.

In dialectometry (Wieling et al., 2009), the
segment-segment similarity matrix is estimated
using pointwise mutual information (PMI). The
PMI score for two sounds x and y is defined as
followed:

pmi(x, y) = log
p(x, y)
p(x)p(y)

(1)

where, p(x, y) is the probability of x, y being
matched in a pair of cognate words, whereas, p(x)
is the probability of x. A positive PMI value be-
tween x and y indicates that the probability of
x being aligned with y in a pair of cognates is
higher than what would be expected by chance.
Conversely, a negative PMI value indicates that an
alignment of x with y is more likely the result of
chance than of shared inheritance.

The PMI based computation requires a prior list
of plausible cognates for computing a weighted
similarity matrix between sound segments. In the
initial step, we extract cross-lingual word pairs
that have a Levenshtein distance less than 0.5 and
treat them as a list of plausible cognates. The PMI
estimation procedure is described as followed:

1. Compute alignments between the word pairs
using a vanilla Needleman-Wunsch algo-
rithm.3

2. The computed alignments from step 1 are
then used to compute similarity between seg-
ments x, y according to the following for-
mula:

3We set the gap-opening penalty to -2.5 and gap extension
penalty to -1.75.

29



3. The PMI matrix obtained from step 2 is used
to realign the word pairs and generate a new
list of segment alignments. The new list of
alignments is employed to compute a new
PMI matrix.

4. Steps 2 and 3 are repeated until the difference
between PMI matrices reach zero.

In our experience, five iterations were sufficient
to reach convergence. At this stage, we use the
PMI matrix to compute a word similarity matrix
between the words belonging to a single meaning.
The word similarity matrix was converted into a
word distance matrix using the following transfor-
mation: (1+exp(x))−1 where, x is the PMI score
between two words. We use the InfoMap cluster-
ing algorithm (List et al., 2016) for the purpose of
identifying cognate clusters.

3.2.2 Bayesian phylogenetic inference
The Bayesian phylogenetics originated in evolu-
tionary biology and works by inferring the evo-
lutionary relationship (trees) between DNA se-
quences of species. The same method is applied
to binary traits of species (Yang, 2014). A binary
trait is typically a presence or absence of a evolu-
tionary character in an biological organism. Com-
putational biologists employ a probabilistic substi-
tution model θ that models the transition probabil-
ities from 0 → 1 and 1 → 0. The substitution
matrix would be a 2 × 2 matrix in the case of a
binary data matrix.

A evolutionary tree that explains the relation-
ship between languages consist of topology (τ )
and branch lengths (T). The likelihood of the bi-
nary data to a tree is computed using the pruning
algorithm (Felsenstein, 1981). Ideally, identifying
the best tree would involve exhaustive enumera-
tion of the trees and calculating the likelihood of
the binary matrix for each tree. However, the num-
ber of possible binary tree topologies grows facto-
rially ((2n − 3)!! where, n is the number of lan-
guages) and, hence intractable even for a small
number (20) of languages. The inference problem
would be to estimate the joint posterior density of
τ, θ,T.

The Bayesian phylogenetic inference program
(MrBayes;4 Ronquist and Huelsenbeck (2003)) re-
quires a binary matrix (languages × number of
clusters) of 0s and 1s, where, each column shows
if a language is present in a cluster or not. The

4http://mrbayes.sourceforge.net/

German Hund 1 0
Swedish hund 1 0
Hindi kutta 0 1

Table 2: Binary matrix for meaning “dog”.

cognate clusters are converted into a binary ma-
trix of 0s and 1s in the following manner. A word
for a meaning would belong to one or more cog-
nate sets. For example, in the case of German,
Swedish, and Hindi, the word for dog in German
‘Hund’ and Swedish ‘hund’ would belong to the
same cognate set, while Hindi ‘kutta’ would be-
long to a different category. The binary trait ma-
trix for these languages for a single meaning, dog,
would be as in table 2. A Bayesian phylogenetic
analysis employs a Markov-Chain Monte-Carlo
procedure to navigate across the tree space. In this
paper, we ran two independent runs until the trees
inferred by the two runs do not differ beyond a
threshold of 0.01. In summary, we ran both the
chains for 4 million states and sampled trees at ev-
ery 500 states to avoid auto-correlation. Then, we
threw away the initial one million states as burn-in
and generated a summary tree of the post burn-in
runs (Felsenstein, 2004). The summary tree con-
sists of only those branches which have occurred
more than 50% of the time in the posterior sample,
consisting of 25000 trees.

4 Results

In this section, we present visualizations of differ-
ences in the language area using MDS and noisy
clustering.

4.1 String edit distance
In the left map in Figure 2, the first three MDS
dimensions are mapped to RGB color space, visu-
alizing the differences between the locations. Note
that the major dialectal differences outlined in ta-
ble 1 are visible in this visualization. For exam-
ple, the magenta and yellow-green regions sep-
arate the Bison Horn Maria and the Hill Maria
groups from the surrounding areas with sharp con-
trasts. The original linguistic distances and the
distances based on first three MDS dimensions
correlate with r = 0.90, hence, retaining about
81% of the variation in the original distances. The
middle map in figure 2 displays only the first di-
mension, which seems to represent a difference
between north and south. On the other hand, the

30



Figure 2: MDS analysis performed by Gabmap with string edit distance. The left map shows first three
MDS dimensions mapped to RGB color space. The middle map shows only the first dimension, and
the right map shows the second MDS dimension. The first three dimensions correlate with the original
distances with r = 0.73, r = 0.55 and r = 0.41, respectively, and first three dimensions with r = 0.90.

bhm
bhs 84
bhb

100

mso
mud 100

mlj
100

72

bhe
khu 100
ggg

100

gam
gar 100

gch
gdh 75
gwa

100

prg
96

99

ght
glb
grp

100

67

gka
gse 100

67

pkh
psh 100

pmd
100

gcj
100

gkt
gtd 67

67

52

dog
gki 100
rui

100

gra
52

gni
lxg 100
gut

100

52

gba
get 100
gja

100

mad
met 100
mal

100
100

goa

100

gbh
mbh 100

mdh
mku 100
ktg

60

gok
100

100

0.0 0.1 0.2 0.3 0.4

Figure 3: Clustering analysis performed by Gabmap with string edit distance using Ward’s method and
the color in the map indicate 5 dialect groups. Probabilistic dendrogram from the default Gabmap anal-
ysis (string edit distance).

right map (second MDS dimension) seems to indi-
cate a difference between Bison Horn Maria (and
to some extent Muria) and the rest.

The clustering results are also complementary
to the MDS analysis. The 5-way cluster map
presented in figure 3 indicates the expected di-
alect groups described in table 1. Despite some
unexpected results in the detailed clustering, the
probabilistic dendrogram presented in figure 3
also shows that the main dialect groups are stable
across noisy clustering experiments. For instance,
the Bison Horn Maria group (bhm, bhs, bhb) pre-
sented on the top part of the dendrogram indicates
a very stable group: these locations are clustered
together in all the noisy clustering experiments.
Similarly, the next three locations (mco, mud, mlj,
belonging to Muria area) also show a very strong
internal consistency, and combine with the Bison
Horn Maria group in 72% of the noisy clustering

experiments. However, other members of Muria
group (mdh, mku, ktg, gok at the bottom of the
probabilistic dendrogram) seem to be placed often
apart from the rest of the group.

4.2 Binary distances
Next, we present the MDS analysis based on lex-
ical distances in figure 4. For this analysis, we
identify cognates for each meaning (cf. section
2.2), and treat the cognate clusters found in each
location as the only (categorical) features for anal-
ysis. The overall picture seems to be similar to
the analysis based on the phonetic data, although
the north-south differences are more visible in
this analysis. Besides the first three dimensions
(left map), both first (middle map) and second
(right map) dimensions indicate differences be-
tween north and south. The left figure shows that
there is a gradual transition from the Northern di-
alects (gtd, gkt, prg, ggg, khu, bhe, gcj, pmd, psh,

31



Figure 4: MDS analysis performed by Gabmap with categorical differences. The left map shows first
three MDS dimensions mapped to RGB color space. The middle map shows only the first dimension, and
the right map shows the second MDS dimension. The first three dimensions correlate with the original
distances with r = 0.77, r = 0.53 and r = 0.41, respectively, and first three dimensions with r = 0.94.

bhm
bhs 73
bhb

100

gok
mlj 65

mso
mud 100

90
67

bhe
khu 100
ggg

100

prg
56

gch
gwa 100
gdh
gkt 100

90

65

gam
gar 100
gka
gse 100

100

ght

68

glb
grp 91

97

75

gtd

59

pkh
psh 100
gcj

70

pmd
99

99

dog
gki 100
rui

100

gni
lxg 100
gut

100

gra
100

99

gba
get 100

goa
100

gbh
70

mbh

70

mad
mal
met

100

gja
90

mdh
mku 100

90

ktg

88

100

0.0 0.1 0.2 0.3 0.4

Figure 5: The dendrogram shows the results of the hierarchical clustering (left) based on binary matrix.
Probabilistic dendrogram from the Gabmap analysis with Hamming distances.

pkh) to the rest of the northern dialects that share
borders with Muria and Southern dialects. There
is a transition between Southern dialects to the Hill
Maria dialects where, the Hill Maria dialects do
not show much variation.

The clustering analysis of the binary matrix
from cognate detection step is projected on the ge-
ographical map of the region in figure 5. The map
retrieves the five clear subgroups listed in table 1.
Then, we perform a noisy clustering analysis of
the Hamming distance matrix which is shown in
the same figure. The dendrogram places Bison-
Horn Maria dialects (bhm, bhs, bhb) along with
the eastern dialects of Muria subgroup. It also
places all the Northern Gondi dialects into a sin-
gle cluster with high confidence. The dendrogram
also places all the southern dialects into a single
cluster. On the other hand, the dendrogram incor-
rectly places the Hill Maria dialects along with the
western dialects of Muria subgroup. With slight

variation in the detail, the cluster analysis and the
probabilistic dendrogram presented in figure 5 are
similar to the analysis based on phonetic differ-
ences.

4.3 Autoencoder distances
The MDS analysis of autoencoder-based distances
are shown in figure 6. The RGB color map of
the first three dimensions shows the five dialect
regions. The figure shows a clear boundary be-
tween Northern and Southern Gondi dialects. The
map shows the Bison Horn Maria region to be
of distinct blue color that does not show much
variance. The autoencoder MDS dimensions cor-
relate the highest with the autoencoder distance
matrix. The first dimension (middle map in fig-
ure 6) clearly distinguishes the Northern dialects
from the rest. The second dimension distinguishes
Southern Gondi dialects and Muria dialects from
the rest of the dialects.

The clustering analysis of the autoencoder dis-

32



Figure 6: MDS analysis performed by Gabmap with autoencoder differences. The left map shows first
three MDS dimensions mapped to RGB color space. The middle map shows only the first dimension, and
the right map shows the second MDS dimension. The first three dimensions correlate with the original
distances with r = 0.74, r = 0.57 and r = 0.49, respectively, and first three dimensions with r = 0.92.

bhm
bhs 100
bhb

100

mso
mud 100

mlj
100

99

bhe
khu 100
ggg

100

gch
gwa 86
gdh

100

prg
100

82

gkt

85

gam
gar 100
gka

95

gse
95

glb
grp 90
ght

100

82

pkh
psh 100

pmd
100

gcj
100

gtd

51

dog
gki 100
rui

100

gba
get 100
gja

76

mad
mal 74
met

100
76

mdh
mku 100
gok

52

ktg
76

76

gni
lxg 100
gut

100

gra
96

76

gbh
mbh 100
goa

87

100

0.0 0.1 0.2 0.3 0.4 0.5

Figure 7: Probabilistic dendrogram from the
Gabmap analysis with autoencoder distances. The
clustering result is similar to the left map in fig-
ure 3.

tances are projected on to the geographical map
in figure 7. The map retrieves the five subgroups
in table 1. The noisy clustering clearly puts the
Bison Horn Maria group into a single cluster. It
also places all the northern dialects into a single
group with 100% confidence. On the other hand,
the dendrogram splits the Southern Gondi dialects
into eastern and western parts. The eastern parts
are placed along with the Hill Maria dialects. The
clustering analysis also splits the Muria dialects
into three parts. However, the dendrogram places
gok (a eastern Muria dialect) incorrectly with Far
Western Muria (mku).

4.4 Bayesian analysis

The summary tree of the Bayesian analysis is
shown in figure 8. The figure also shows the per-
centage of times each branch exists in the posterior
sample of trees. The tree clearly divided North-

0 . 0 2

g e t

g r p

gch

gba

met

g k t

k t g

mku

bhe

dog

pmd

gka
gse

g r a

gok

psh

goa

khu

mud

mal

p r g

gam

l x g

g j a

ggg

m l j

g h t

bhb

g t d

pkh

g a r

g c j

r u i

gdh

g n i

g k i

bhm

mso

mad

mdh

gbh

g u t

bhs

gwa

g l b

mbh

100

100

100

100

100

98

95

98

100

100
100

100

100

100

100

93

100

100

100

100

100

100

100

100

99

100

52

56

83

82

100

100

99

100

100

99

100

83

100

100

97

100

100

100

100

100

100

100

97

100

100

100

84

97

70

100

100

100

66

100

100

100

100

100

100

74

74
100

62

100

95

100

100

99

93

100

99

100

100

91

100

100

100

99

100

100

99

99

100

Figure 8: The majority consensus tree of the
Bayesian posterior trees.

west Gondi from Southeast Gondi groups. The
tree places all the Northern Gondi dialects into
a single group in 99% of the trees. The south-
ern dialects are split into two different branches
with rui, dog, gki branching later from the com-
mon Northwest Gondi later than the rest of the
Southern Gondi dialects. The tree clearly splits the
Hill Maria dialects from rest of Southeast Gondi
dialects. The tree also places all the Bison Horn
Maria dialects into a single group but does not put
them into a different group from the rest of the
Muria dialects.

5 Conclusion

In this paper, we performed analysis using tools
from dialectometry and computational historical

33



linguistics for the analysis of Gondi dialects. The
dialectometric analysis rightly retrieves all the
subgroups in the region. However, both edit dis-
tance and autoencoder distances differ in the noisy
clustering analysis. On the other hand, the noisy
clustering analysis on the binary cognate matrix
yields the best results. The Bayesian tree based on
cognate analysis also retrieves the top level sub-
groups right but does not clearly distinguish Bi-
son Horn Maria group from Muria dialects. As a
matter of fact, the Bayesian tree agrees the high-
est with the gold standard classification from Glot-
tolog.

The contributions of the paper is as followed.
We digitized a multilingual lexical wordlist for 46
dialects and applied both dialectometric and phy-
logenetic methods for the classification of dialects
and find that phylogenetic methods perform the
best when compared to the gold standard classi-
fication.

Acknowledgments

The authors thank the five reviewers for the com-
ments which helped improve the paper. The
first and third authors are supported by the ERC
Advanced Grant 324246 EVOLAEMP, which is
gratefully acknowledged.
The code and the data for the experiments
is available at https://github.com/
PhyloStar/Gondi-Dialect-Analysis

References
Martın Abadi, Ashish Agarwal, Paul Barham, Eugene

Brevdo, Zhifeng Chen, Craig Citro, Greg S Corrado,
Andy Davis, Jeffrey Dean, Matthieu Devin, et al.
2016. Tensorflow: Large-scale machine learning on
heterogeneous distributed systems. arXiv preprint
arXiv:1603.04467.

David K. Beine. 1994. A sociolinguistic survey of the
Gondi-speaking communities of central india. Mas-
ter’s thesis, San Diego State University, San Diego.

Thomas Burrow and S. Bhattacharya. 1960. A com-
parative vocabulary of the Gondi dialects. Journal
of the Asiatic Society, 2:73–251.

François Chollet. 2015. Keras. GitHub repository:
https://github. com/fchollet/keras.

Joseph Felsenstein. 1981. Evolutionary trees from
DNA sequences: A maximum likelihood approach.
Journal of Molecular Evolution, 17(6):368–376.

Joseph Felsenstein. 2004. Inferring phylogenies. Sin-
auer Associates, Sunderland, Massachusetts.

Umamaheshwar Rao Garapati. 1991. Subgrouping of
the Gondi dialects. In B. Lakshmi Bai and B. Ra-
makrishna Reddy, editors, Studies in Dravidian and
general linguistics: a festschrift for Bh. Krishna-
murti, pages 73–90. Centre of Advanced Study in
Linguistics, Osmania University.

Russell D. Gray and Quentin D. Atkinson. 2003.
Language-tree divergence times support the Ana-
tolian theory of Indo-European origin. Nature,
426(6965):435–439.

Bradley Hauer and Grzegorz Kondrak. 2011. Clus-
tering semantically equivalent words into cognate
sets in multilingual lists. In Proceedings of 5th In-
ternational Joint Conference on Natural Language
Processing, pages 865–873, Chiang Mai, Thailand,
November. Asian Federation of Natural Language
Processing.

Geoffrey E Hinton and Ruslan R Salakhutdinov. 2006.
Reducing the dimensionality of data with neural net-
works. Science, 313(5786):504–507.

Sepp Hochreiter and Jürgen Schmidhuber. 1997.
Long short-term memory. Neural computation,
9(8):1735–1780.

Gerhard Jäger. 2013. Phylogenetic inference from
word lists using weighted alignment with empiri-
cally determined weights. Language Dynamics and
Change, 3(2):245–291.

Grzegorz Kondrak. 2009. Identification of cognates
and recurrent sound correspondences in word lists.
Traitement Automatique des Langues et Langues
Anciennes, 50(2):201–235, October.

Therese Leinonen, Çağrı Çöltekin, and John Nerbonne.
2016. Using Gabmap. Lingua, 178:71–83.

Johann-Mattis List, Philippe Lopez, and Eric Bapteste.
2016. Using sequence similarity networks to iden-
tify partial cognates in multilingual wordlists. In
Proceedings of the 54th Annual Meeting of the As-
sociation for Computational Linguistics (Volume 2:
Short Papers), pages 599–605, Berlin, Germany,
August. Association for Computational Linguistics.

Johann-Mattis List. 2012. LexStat: Automatic de-
tection of cognates in multilingual wordlists. In
Proceedings of the EACL 2012 Joint Workshop
of LINGVIS & UNCLH, pages 117–125, Avignon,
France, April. Association for Computational Lin-
guistics.

Saul B. Needleman and Christian D. Wunsch. 1970.
A general method applicable to the search for simi-
larities in the amino acid sequence of two proteins.
Journal of Molecular Biology, 48(3):443–453.

John Nerbonne, Peter Kleiweg, Wilbert Heeringa, and
Franz Manni. 2008. Projecting dialect differ-
ences to geography: Bootstrap clustering vs. noisy

34



clustering. In Christine Preisach, Lars Schmidt-
Thieme, Hans Burkhardt, and Reinhold Decker, ed-
itors, Data Analysis, Machine Learning, and Appli-
cations. Proc. of the 31st Annual Meeting of the Ger-
man Classification Society, pages 647–654, Berlin.
Springer.

John Nerbonne, Rinke Colen, Charlotte Gooskens, Pe-
ter Kleiweg, and Therese Leinonen. 2011. Gabmap
– a web application for dialectology. Dialectologia,
Special Issue II:65–89.

John Nerbonne. 2009. Data-driven dialectology. Lan-
guage and Linguistics Compass, 3(1):175–198.

Sebastian Nordhoff and Harald Hammarström. 2011.
Glottolog/Langdoc: Defining dialects, languages,
and language families as collections of resources. In
Proceedings of the First International Workshop on
Linked Science, volume 783, pages 53–58.

Taraka Rama and Çağri Çöltekin. 2016. LSTM au-
toencoders for dialect analysis. In Proceedings of
the Third Workshop on NLP for Similar Languages,
Varieties and Dialects (VarDial3), pages 25–32, Os-
aka, Japan, December. The COLING 2016 Organiz-
ing Committee.

Taraka Rama. 2015. Automatic cognate identifica-
tion with gap-weighted string subsequences. In Pro-
ceedings of the 2015 Conference of the North Amer-
ican Chapter of the Association for Computational
Linguistics: Human Language Technologies., pages
1227–1231.

Fredrik Ronquist and John P Huelsenbeck. 2003.
Mrbayes 3: Bayesian phylogenetic inference under
mixed models. Bioinformatics, 19(12):1572–1574.

Ian Smith. 1991. Interpreting conflicting isoglosses:
Historical relationships among the Gondi dialects.
In B. Lakshmi Bai and B. Ramakrishna Reddy, edi-
tors, Studies in Dravidian and general linguistics: a
festschrift for Bh. Krishnamurti, pages 27–48. Cen-
tre of Advanced Study in Linguistics, Osmania Uni-
versity.

Martijn Wieling, Jelena Prokić, and John Nerbonne.
2009. Evaluating the pairwise string alignment of
pronunciations. In Proceedings of the EACL 2009
Workshop on Language Technology and Resources
for Cultural Heritage, Social Sciences, Humanities,
and Education, pages 26–34. Association for Com-
putational Linguistics.

Ziheng Yang. 2014. Molecular evolution: A statistical
approach. Oxford University Press, Oxford.

35


