



















































A Multi-Stage Memory Augmented Neural Network for Machine Reading Comprehension


Proceedings of the Workshop on Machine Reading for Question Answering, pages 21–30
Melbourne, Australia, July 19, 2018. c©2018 Association for Computational Linguistics

21

A Multi-Stage Memory Augmented Neural Network
for Machine Reading Comprehension

Seunghak Yu Sathish Indurthi Seohyun Back Haejun Lee
Samsung Research, Seoul, Korea

{seunghak.yu, s.indurthi, scv.back}@samsung.com

Abstract

Reading Comprehension (RC) of text is
one of the fundamental tasks in natu-
ral language processing. In recent years,
several end-to-end neural network mod-
els have been proposed to solve RC tasks.
However, most of these models suffer in
reasoning over long documents. In this
work, we propose a novel Memory Aug-
mented Machine Comprehension Network
(MAMCN) to address long-range depen-
dencies present in machine reading com-
prehension. We perform extensive exper-
iments to evaluate proposed method with
the renowned benchmark datasets such as
SQuAD, QUASAR-T, and TriviaQA. We
achieve the state of the art performance
on both the document-level (QUASAR-T,
TriviaQA) and paragraph-level (SQuAD)
datasets compared to all the previously
published approaches.

1 Introduction

Reading Comprehension (RC) is essential for
understanding human knowledge written in text
form. One possible way of measuring RC is
by formulating it as answer span prediction style
Question Answering (QA) task, which is finding
an answer to the question based on the given doc-
ument(s). Recently, influential deep learning ap-
proaches have been proposed to solve this QA
task. Wang and Jiang (2017); Seo et al. (2017) pro-
pose the attention mechanism between question
and context for question-aware contextual repre-
sentation. Wang et al. (2017) refine these con-
textual representations by using self-attention to
improve the performance. Even further perfor-
mance improvement is gained by using contextu-
alized word representations for query and context

(Salant and Berant, 2017; Peters et al., 2018; Yu
et al., 2018).

Based on those approaches, several methods
have successfully made progress towards reaching
human-level performance on SQuAD (Rajpurkar
et al., 2016). Each training example in the SQuAD
only has the relevant paragraph with the corre-
sponding answer. However, most of the docu-
ments present in the real-world are long, contain-
ing relevant and irrelevant paragraphs, and do not
guarantee answer presence. Therefore the models
proposed to solve SQuAD have difficulty in apply-
ing to real-world documents (Joshi et al., 2017).
Recently, QUASAR-T (Dhingra et al., 2017) and
TriviaQA (Joshi et al., 2017) datasets have been
proposed to resemble real-world document. These
datasets use document-level evidence as training
example instead of using only the relevant para-
graph and evidence does not guarantee answer
presence, which makes them more realistic.

To effectively comprehend long documents
present in the QUASAR-T and TriviaQA datasets,
the QA models have to resolve long-range de-
pendencies present in these documents. In this
work, we build a QA model that can understand
long documents by utilizing Memory Augmented
Neural Networks (MANNs) (Graves et al., 2014;
Weston et al., 2015b). This type of neural net-
works decouples the memory capacity from the
number of model parameters. While there have
been several attempts to use MANNs in managing
long-range dependencies, applications are limited
to only toy datasets (Sukhbaatar et al., 2015; We-
ston et al., 2015a; Kumar et al., 2016; Graves et al.,
2016). Compared to the previous approaches, we
mainly focus on the document-level QA task on
QUASAR-T and TriviaQA. We also apply our
model to SQuAD to show that our model even
works well on the paragraph-level.

Our contributions in this work are as follows:



22

(1) We develop Memory Augmented Machine
Comprehension Network (MAMCN) to solve
document-level RC task. (2) Our method achieves
the state of the art performance compared to all
the published results on both the document-level
(QUASAR-T and TriviaQA) and paragraph-level
(SQuAD) benchmarks. In TriviaQA we achieve
71.91 and 69.60 F1 scores for Web and Wikipedia
domains respectively. Also, we achieved 86.73 F1
compared to the human performance of 91.22 F1
in SQuAD benchmark. These results show that
MAMCN is a crucial component for QA task, es-
pecially they are useful in comprehending long
documents.

2 Related Work

Many neural networks have been proposed to
solve answer span QA task. Ranking continu-
ous text spans within a passage was proposed by
Yu et al. (2016) and Lee et al. (2016). Wang
and Jiang (2017) combine match-LSTM, origi-
nally introduced in (Wang and Jiang, 2016) and
pointer networks to produce the boundary of the
answer. Since then, most of the models adopted
pointer networks as a prediction layer and then
focused on improving other layers. Some meth-
ods focused on devising more accurate attention
method; Seo et al. (2017); Wang et al. (2017);
Xiong et al. (2017) employ attention mechanism
to match the question context mutually; In addi-
tion, Liu et al. (2017a) apply multi-layer attention
and Huang et al. (2017b) expand to multi-level at-
tention to get more enriched attention information.

Other approaches use contextualized word rep-
resentations to further improve the performance.
Salant and Berant (2017); Peters et al. (2018) uti-
lize embedding from pre-trained language model
as an additional feature and Yu et al. (2018) select
machine translation model instead. Also, there are
few attempts at augmenting memory capacity of
the model (Hu et al., 2017; Pan et al., 2017). Hu
et al. (2017) refine the contextual representation
with multi-hops, and Pan et al. (2017) simply use
the encoded query representations as a memory
vector for refining the answer prediction, which
are not meant to handle long-range dependency
that we consider in this work.

3 Proposed Model

We propose a memory augmented reader for
answer-span style QA task. Answer-span style QA

task is defined as follows. Question and document
can be represented by sequence of words q =
{wqi }mi=1, d = {wdj }nj=1 respectively. Answer-
span a = {wdk}ek=s (where, 1 ≤ s ≤ e ≤ n)
should be returned, given q and d. We embed the
sequence of words in q and d to get contextual rep-
resentations. These contextual representations are
used for calculating question-aware context repre-
sentation which is controlled by external memory
unit and used for predicting answer-span. We de-
scribe more details of each layer in following sec-
tions and depict overall architecture of proposed
model in Figure 1.

3.1 Contextual Representation

Word Embedding: We use two different kinds of
embeddings to get the richer feature representa-
tion for each word in the question and document.
Word-level embedding helps to have a represen-
tation explaining semantic similarities as proxim-
ity in high dimensional vector space. In addi-
tion to this, we utilize character-level embedding
by applying convolution filters to address out-of-
vocabulary and infrequent words problem. We
concatenate both embeddings [ew; ec] to represent
each word embedding as e ∈ Rl.

Contextual Embedding: We compute the con-
textual representation for each word in the ques-
tion and document by using bi-directional GRU
(Cho et al., 2014) as follows:

cqi = BiGRUq(e
q
i ,hi−1,hi+1), (1)

cdj = BiGRUd(e
d
j ,hj−1,hj+1), (2)

where eqi , e
d
j are the word embeddings for

each word in the question and document, and
hi−1,hi+1 are the hidden states of the forward
and reverse GRUs respectively. The final ques-
tion and document contextual representation are
Cq ∈ Rm×2l and Cd ∈ Rn×2l.

Co-attention: We compute question-aware rep-
resentations for each word in the document by
adopting the attention mechanism in (Clark and
Gardner, 2017). To get these representations, we
first compute the similarity matrix S followed by
bi-directional attention between the words in the
question and document as follows:

sij = wqC
q
i,: +wdC

d
j,: +wh(C

q
i,: �C

d
j,:) (3)

wq, wd, and wh are trainable weights and �
is element-wise multiplication. Each element of



23

Co-attention WeightsCo-attention

BiGRU BiGRU BiGRU BiGRU BiGRU
Contextual
Embedding

Document Question

Word / Character  
Embedding

Contoller

Prediction

BiGRU BiGRUBiGRU

BiGRU BiGRUBiGRU

EndStart

BiGRU + Softmax BiGRU + Softmax

c
q

1c
d

1 c
d

2

e
d

1 e
d

2 e
q

1

c
d

n c
q

m

e
d

n e
q

m

M
0

M1 Mn-1 Mn

o1 o2 on

C1, : C2, : Cn, :

s1 s2 sn
Read

Write

+++++++

|||||||||||||||||||

+++++++

|||||||||||||||||||

||  ||  ||  ||  || 

+++++++

Memory

x1 x2 xn

Figure 1: The architecture of Memory Augmented Machine Comprehension Network (MAMCN).

similarity matrix sij represents attention between
ith word in the question and jth word in the docu-
ment. We can get the attention weights of ques-
tion words to each document word by applying
column-wise softmax to S.

aqij =
( esij∑m

k=1 e
skj

)
(4)

Aq:,j is attention weights of questions to jth
word in the document. We can get attended ques-
tion vectors for the document words by multiply-
ing entire attention matrix Aq to the contextual
embedding of question Cq.

C̃
q
= AqTCq ∈ Rn×2l (5)

Also, we can get attention weights on the docu-
ment words for the question ad by applying soft-
max to the column-wise max values (vd) of atten-
tion matrixAq.

adj =
( evdj∑n

k=1 e
vdk

)
, (vdj = max

1≤i≤m
aqij) (6)

This attention weight on the document words ad

is duplicated n times for each row to makeAd and

applied to contextual embedding of document Cd

to get attended document vectors.

C̃
d
= AdCd ∈ Rn×2l (7)

Finally, we can make question-aware contextual
representation C by concatenating as follows:

C = [Cd; C̃
q
;Cd� C̃q;Cd� C̃d] ∈ Rn×8l (8)

3.2 Memory Controller
Memory controller allows us to utilize external
memory to compensate for the limited memory
capacity of the recurrent layers. We develop the
memory controller inspired by the memory frame-
work of (Graves et al., 2016). The operation of the
controller at time step t is given by:

ot, it = Controller(Ct,:,M t−1) (9)

It takes the contextual representation vectorCt,:
as input, and the external memory matrix of the
previous time step M t−1 ∈ Rp×q, where p is the
number of memory locations, and q is the dimen-
sion of each location. We choose two layers of
BiGRUs as the recurrent layer for the controller.
The contextual representations are fed into the first
layer to capture interactions between contexts.

st = BiGRU(Ct,:,ht−1,ht+1) ∈ R2l. (10)



24

Then, st and the subset of memory vectors ob-
tained from the memory matrix are concatenated
to generate an input vector xt for the second layer,

xt = [st;m
1
t−1; · · · ;mst−1] ∈ R2l+sq, (11)

where s is the number of read vectors. After feed-
ing the input vector xt to the second recurrent
layer, the controller uses the outputs of the layer
hmt to emit the output vector ot and the interface
vector it. We describe details of obtaining each
vector as follows:

hmt = BiGRU(xt,h
m
t−1,h

m
t+1) (12)

Output Vector: The controller makes the out-
put vector as a weighted sum vt of hidden state of
the recurrent layer and memory vectors.

vt = Woh
m
t +Wm[m

1
t ; · · · ;mst ] (13)

We add a residual connection between the con-
troller’s input and output to mitigate the infor-
mation morphing that can occur when interacting
with memory. As a result, we get a long-term
dependency-aware output vector as follows:

ot = Wvvt +WcCt,: ∈ R2l (14)

Interface Vector: At the same time, the con-
troller generates interface vector it for the memory
interaction based on hmt as follows:

it = Wih
m
t ∈ Rsq+5s+3q+3. (15)

We consider it as a concatenation of various
functional vectors which determine the basic op-
eration of the memory such as memory address-
ing, read and write. The complete list of functional
vectors is described in Table 1.

Memory Addressing : We use content-based
addressing mechanism for read and write opera-
tions. In content-based addressing, the memory
locations required to read/write at the current time
step t are obtained by using the probability distri-
bution over the memory locations, which is a sim-
ilarity function between the key vector and each
memory vector.

ci = softmax (cos(M i,:,k)α̃) (16)

cos is the cosine similarity and α̃ is a constrained
strength value to [1,∞) by (1 + log(1 + eα)).

Operation Name Vector

Read
key {kr,it }si=1 ∈ Rq

strength {αr,it }si=1 ∈ R
mode {πit}si=1 ∈ R3

Write

key kwt ∈ Rq
strength αwt ∈ R

erase vector et ∈ Rq
write vector vt ∈ Rq

free gate {gf,it }si=1 ∈ R
allocate gate gat ∈ R

write gate gwt ∈ R

Table 1: The list of functional vectors that make
up the interface vector of controller.

Read Operation: Each read head performs a
read operation by weighting over entire memory.

crt = softmax (cos(M t,k
r
t )α̃

r
t ) (17)

Along with this, we use a temporal memory
linkage matrix to associate memories together,
which keep track of consecutively modified loca-
tions in the memory. The multiplication of tempo-
ral link matrix and read weights from the previous
time-step gives backward bt and forward weights
f t which helps to track the temporal order of the
memory. The read weights are obtained from the
linear combination of corresponding weights and
the mode vectors π̃ normalized by softmax.

wrt = π̃[0]bt + π̃[1]c
r
t + π̃[2]f t (18)

These read weights are applied to memory loca-
tions to get the final read vectors as follow:

mi =

p∑
i

Mi,:w
r,i
t (19)

Write Operation: Similar to the read heads in
read operation, a write head determines where to
write by using content-based weighting.

cwt = softmax (cos(M t−1,k
w
t )α̃

w
t ) (20)

We adopted dynamic memory allocation as de-
scribed in (Graves et al., 2016) to maintain a free
list and a usage vector to track the memory free-
ness. Based on the memory freeness, allocation
weights at are calculated to indicate where to
write, and it is interpolated with content-based
weights to get locations for writing. Write gate



25

Dataset
Total

ADL
Train / Dev / Test

SQuAD 87,599 / 10,570 / UNK 142
QUASAR-T

25,465 / 2,043 / 2,068 221
(Short)

QUASAR-T
26,318 / 2,129 / 2,102 392

(Long)
TriviaQA

528,979 / 68,621 / 65,509 631
(Web)

TriviaQA
110,648 / 14,229 / 13,661 955

(Wikipedia)

Table 2: Data statistics of SQuAD, QUASAR-
T, and TriviaQA. The Average Document Length
(ADL) represents the average number of words.
In TriviaQA, ADL was calculated after truncating
the documents to the first 1200 words.

gwt decides whether to write or not and allocation
gate gat determines the degree of interpolation.

wwt = g
w
t [g

a
tat + (1− gat )cwt ] (21)

After finding the location to write with this
weight, write operation on the memory is per-
formed as follows:

M t =M t−1 � (Jp,q −wwt eTt ) +wwt vTt (22)

where � is element-wise multiplication and Jp,q
is p by q matrix of ones.

3.3 Prediction Layer
We feed output vector o from memory controller
to prediction layer. First, it goes to bi-directional
GRU and is then linearly projected to get prob-
ability distribution of start position of answer
Pr(as|q,d). The end position Pr(ae|q,d) is cal-
culated the same way with the hidden states of the
start position concatenated as an additional input.
These probabilities are also used for model op-
timization while training in the form of negative
log-likelihood probability.

4 Experimental Results

In this section, we present our experimental setup
for evaluating the performance of our MAMCN
model. We select different datasets based on their
average document length to check the effective-
ness of external memory on RC task. We com-
pare the performance of our model with all the
published results and the baseline memory aug-
mented model. The baseline model is developed

by replacing modeling layer in BiDAF (Seo et al.,
2017) model with the memory controller from
DNC (Graves et al., 2016).

4.1 Data Set

We perform experiments with recently proposed
QUASAR-T and TriviaQA datasets to see the
performance of our model on the long docu-
ments. The QUASAR-T dataset consists of fac-
toid question-answer pairs and a corresponding
large background corpus (Callan et al., 2009) to
comprehend. In TriviaQA, question-answers pairs
are collected from trivia and quiz-league websites.
The evidence documents for these QA pairs are
collected from Wikipedia articles and Web search
results. The average length of the documents in
these datasets are much longer than SQuAD and
question-documents pairs are collected in a decou-
pled way, making them more difficult to compre-
hend. In the case of TriviaQA, we truncate the
documents to 1200 words for training and 2000
words for the test. Even these truncated docu-
ments are 3 to 5 times longer than SQuAD doc-
uments. The average length of original documents
in TriviaQA is about 3,000 words, so there is no
guarantee that above truncated documents contain
the answer for a given question.

We also conduct experiments on SQuAD
dataset to show our model can even work well on
paragraph-level data. The SQuAD contains a col-
lection of Wikipedia articles and crowd-sourced
question-answer pairs. Even though SQuAD
dataset pushed existing models to achieve signif-
icant performance improvements in solving RC
task, the document length does not resemble the
length of the real-world document.

The statistics of all these datasets are shown in
Table 2. We use official train, dev, and test splits
provided in all these datasets for experiments.

4.2 Implementation Details

We develop MAMCN using Tensorflow1 deep
learning framework and Sonnet2 library. For
the word-level embedding, we tokenize the docu-
ments using NLTK toolkit (Bird and Loper, 2004)
and substitute words with GloVe 6B (Pennington
et al., 2014) 300-dimensional word embeddings.
We also use 20-dimensional character-level em-
beddings which are learned during training. The

1www.tensorflow.org
2https://github.com/deepmind/sonnet



26

Dataset Model
Dev set Test set

EM F1 EM F1
Short-documents MAMCN 64.87 68.88 68.13 70.32

(ADL=221) BiDAF + DNC 51.18 54.77 54.81 58.24
BiDAF 45.40 50.90 47.60 52.40

Long-documents MAMCN 60.05 63.23 63.44 65.19
(ADL=392) BiDAF + DNC 48.67 52.25 52.15 54.43

BiDAF 37.00 42.50 39.50 44.50

Table 3: Performance results on QUASAR-T dataset.

Domain Model
Full Verified

EM F1 EM F1
Web MAMCN 66.82 71.91 81.01 84.12

(ADL=631) BiDAF + SA + SN (Clark and Gardner, 2017) 66.37 71.32 79.97 83.70
Reading Twice for NLU (Weissenborn, 2017) 50.56 56.73 63.20 67.97

M-Reader (Hu et al., 2017) 46.65 52.89 56.96 61.48
BiDAF + DNC 42.34 48.65 51.50 57.17

MEMEN (Pan et al., 2017) 44.25 48.34 53.27 57.64
BiDAF (Seo et al., 2017) 40.74 47.05 49.54 55.80

Wikipedia MAMCN 64.41 69.60 70.21 75.49
(ADL=955) BiDAF + SA + SN (Clark and Gardner, 2017) 63.99 68.93 67.98 72.88

QANet (Yu et al., 2018) 51.10 56.60 53.30 59.20
Reading Twice for NLU (Weissenborn, 2017) 48.60 55.10 53.40 59.90

M-Reader (Hu et al., 2017) 46.94 52.85 54.45 59.46
BiDAF + DNC 42.57 48.30 46.23 51.61

MEMEN (Pan et al., 2017) 43.16 46.90 49.28 55.83
BiDAF (Seo et al., 2017) 40.32 45.91 44.86 50.71

Table 4: Single model results on TriviaQA dataset3 (Web and Wikipedia). SA: Self-attention, SN: Shared
normalization.

hidden size is set to 200 for QUASAR-T and Triv-
iaQA, and 100 for SQuAD. In the memory con-
troller, we use 100 x 36 size memory initialized
with zeros, 4 read heads and 1 write head. The
optimizer is AdaDelta (Zeiler, 2012) with an ini-
tial learning rate of 0.5. We train our model for
12 epochs, and batch size is set to 30. During the
training, we keep the exponential moving average
of weights with 0.001 decay and use these aver-
ages at test time.

4.3 Results
We use Exact Match (EM) and F1 Score as evalu-
ation metrics for all the datasets. EM measures the
percentage of the predictions that exactly matches
with the corresponding ground truth answers. The
F1 score measures the overlap between the predic-
tions and corresponding ground truth answers.

QUASAR-T: The results on QUASAR-T are
3https://competitions.codalab.org/competitions/17208

shown in Table 3. As described in Table 3, the
baseline (BiDAF + DNC) results in a reason-
able gain, however, our proposed memory con-
troller gives more performance improvement. We
achieve 68.13 EM and 70.32 F1 for short docu-
ments and 63.44 and 65.19 for long documents
which are the current best results.

TriviaQA: We compare proposed model with
all the previously suggested approaches as shown
in Table 4. We perform the experiments on both
Web and Wikipedia domains and report evalua-
tion results for “Full” and “Verified” cases. “Full”
is not guaranteed to have all the supporting fac-
tors to answer the question, however, it is the en-
tire dataset selected with distant supervision. The
“Verified” is a subset of the “Full” dataset cleaned
by the human annotators to guarantee the presence
of supporting facts to answer.

Our model achieves the state of the art perfor-
mance over the existing approaches as shown in



27

Model
Test set

AF SA
EM F1

MAMCN + ELMo + DC 79.69 86.73 O O
BiDAF + Self-attention + ELMo (Peters et al., 2018) 78.58 85.83 O O
MAMCN + ELMo 77.44 85.13 O -
RaSoR + TR + LM (Salant and Berant, 2017) 77.58 84.16 O -
QANet (Yu et al., 2018) 76.24 84.60 O O
SAN (Liu et al., 2017b) 76.83 84.40 O O
FusionNet (Huang et al., 2017b) 75.97 83.90 O O
RaSoR + TR (Salant and Berant, 2017) 75.79 83.26 O -
Conducter-net (Liu et al., 2017a) 74.41 82.74 O O
Reinforced Mnemonic Reader (Hu et al., 2017) 73.20 81.80 O O
BiDAF + Self-attention (Clark and Gardner, 2017) 72.14 81.05 - O
MEMEN (Pan et al., 2017) 70.98 80.36 O -
MAMCN 70.99 79.94 - -
r-net (Wang et al., 2017) 71.30 79.70 - O
Document Reader (Chen et al., 2017) 70.73 79.35 O -
FastQAExt (Weissenborn et al., 2017) 70.85 78.86 - O

...
...

...
...

...
Human Performance 82.30 91.22

Table 5: Single model results on SQuAD dataset4. The last two columns in tables indicate whether mod-
els use additional feature and self-attention. AF: Additional feature augmentation for word embedding,
SA: Self-attention, DC: Densely connected embedding block.

Table 4. In the case of full evaluation, we get
66.82 EM, 71.91 F1 for web domain, and 64.41
EM, 69.60 F1 for Wikipedia domain. The pro-
posed model works best in noisy data with dis-
tant supervision. In the case of human verified
data, performance increases further. We get 81.01
EM and 84.12 F1 for web domain and 70.21 EM
and 75.49 F1 for Wikipedia. It is encouraging
that these results were obtained without any help
of additional feature augmentation, such as utiliz-
ing hidden states of pre-trained language model
or additional semantic/syntactic features which are
commonly used in other models. Also, our model
does not need self-attention layer which is preva-
lently used in previous models.

SQuAD: The results on SQuAD are shown in
Table 5. In the longer documents case (QUASAR-
T and TriviaQA), MAMCN with external mem-
ory performed well because the controller can ag-
gregate information effectively from the long se-
quences, however, due to the small length of the
documents in SQuAD, the existing methods based
on the recurrent layers without external memory
are also sufficient to achieve reasonable perfor-

4https://rajpurkar.github.io/SQuAD-explorer/

Self-attention

FC

Input

BiGRU

Output

Figure 2: Densely connected embedding block

mance. The last two columns in the Table 5 indi-
cate whether each model uses any additional fea-
ture augmentation and/or self-attention. All the
models with the help of these additional feature
augmentation and/or self-attention achieve further
performance gain on SQuAD dataset.

Our vanilla model (MAMCN) achieved the best
performance among the models which are not us-
ing additional feature augmentation and/or self-
attention layer mechanisms. We also adopted
these mechanisms one by one to show that our
model is compatible with them. First, we add



28

No. Question-Document-Answer triplet
Question : In which activity are banderillas used ?

1

Context : Semi-naked animal rights activists staged a fake - bloody protest outside the
European Parliament on Thursday to draw attention to the suffering of bulls during
bullfights . Around 30 people taking part in the protest organized by PETA ( People for the

Ethical Treatment of Animals ) lay on the ground with banderillas, the traditional darts used
to wound and weaken bulls in the fight, attached to their backs, some spattered with fake
blood. Bullfighting - ever popular in Spain - and the European Union’s ...
Ground truth: Bullfight, Bullfights
Question : What boxer was stripped of his heavyweight boxing titles when he refused his US
army induction in April, 1967 ?

2

Context : ... This slideshow consists mostly of boxers who have continued to fight on, despite
the hindrance of being in their senior years. It also includes two or three boxers who have
launched stellar career comebacks from the brink of failure, or exile. George Foreman Al
Bello retirement after being defeated by Jimmy Young in Puerto Rico in 1977 shocked the
boxing community, the announcement of his return in 1987 sent the sport into raptures. In
what ... Muhammad Ali made several comebacks in his career, but the one that stands out
has to be the rebuilding of his career after being stripped of his titles for refusing to go to war
in Vietnam ... After the U.S army found him to have sub-par reading and spelling skills, he
was deemed unsuitable for service in 1966. One year later, however, they revised their criteria,
making the champion eligible for national service. Ali refused, on moral grounds, and was
consequently stripped of his boxing license and titles permanently. After three years of legal
proceedings ...
Ground truth: Muhammad Ali

Table 6: Examples are from devset of TriviaQA (Web). The solid and dashed rectangular text boxes
indicate predictions from the MAMCN and ‘BiDAF + Self-attention’ models respectively.

ELMo (Peters et al., 2018) which is the weighted
sum of hidden layers of language model with reg-
ularization as an additional feature to our word
embeddings. This helped our model (MAMCN +
ELMo) to improve F1 to 85.13 and EM to 77.44
and is the best among the models only with the
additional feature augmentation.

Secondly, we add self-attention with dense con-
nections to our model. Recently, Huang et al.
(2017a) have shown that adding a connection be-
tween each layer to every other layer in con-
volutional networks improves the performance
by a huge margin. Inspired by this, we build
densely connected embedding block along with
self-attention to increase performance further in
the case of SQuAD. The suggested embedding
block is shown in Figure 2. Each layer concate-
nates all the inputs from the previous layers di-
rectly connected to it. We replace all the BiGRU
units with this embedding block except the con-
troller layer in our model (MAMCN + ELMo +
DC). We achieve the state of the art performance,
86.73 F1 and 79.69 EM, with the help of this em-

bedding block.
To show the effectiveness of our method in

addressing long-term dependencies, we collected
two examples from the devset of TriviaQA, shown
in Table 6. Finding an answer in these examples
require resolving long-term dependencies. The
first example requires understanding dependency
present between two sentences while answering.
The ‘BiDAF + Self-attention’ model predicts in-
correct answer by shallow matching a sentence
which is syntactically close to the question. Our
model predicts the correct answer by better com-
bining the information from the two sentences.
In the second example, the answer is present
remotely in the document, the ‘BiDAF + Self-
attention’ model without external memory face
difficulty in comprehending this long document
and predicts the wrong answer whereas our model
predicts correct answer.

5 Conclusion

We proposed a multi-stage memory augmented
neural network model to comprehend long docu-



29

ments in QA task. The proposed model achieved
the state of the art results on the recently re-
leased large-scale QA benchmark datasets such
as QUASAR-T, TriviaQA, and SQuAD. The re-
sults suggest that proposed method is helpful for
addressing long-range dependencies in QA task.
The future work involves implementing scalable
read/write heads to handle larger size external
memory to reason over multiple documents.

Acknowledgments

We would like to thank Percy Liang and Pranav
Samir Rajpurkar for helping us in the SQuAD sub-
missions.

References
Steven Bird and Edward Loper. 2004. Nltk: the natural

language toolkit. In Proceedings of the ACL 2004 on
Interactive poster and demonstration sessions. As-
sociation for Computational Linguistics, page 31.

Jamie Callan, Mark Hoy, Changkuk Yoo, and Le Zhao.
2009. Clueweb09 data set.

Danqi Chen, Adam Fisch, Jason Weston, and Antoine
Bordes. 2017. Reading wikipedia to answer open-
domain questions. arXiv preprint arXiv:1704.00051
.

Kyunghyun Cho, Bart Van Merriënboer, Caglar Gul-
cehre, Dzmitry Bahdanau, Fethi Bougares, Holger
Schwenk, and Yoshua Bengio. 2014. Learning
phrase representations using rnn encoder-decoder
for statistical machine translation. arXiv preprint
arXiv:1406.1078 .

Christopher Clark and Matt Gardner. 2017. Simple
and effective multi-paragraph reading comprehen-
sion. arXiv preprint arXiv:1710.10723 .

Bhuwan Dhingra, Kathryn Mazaitis, and William W
Cohen. 2017. Quasar: Datasets for question an-
swering by search and reading. arXiv preprint
arXiv:1707.03904 .

Alex Graves, Greg Wayne, and Ivo Danihelka.
2014. Neural turing machines. arXiv preprint
arXiv:1410.5401 .

Alex Graves, Greg Wayne, Malcolm Reynolds,
Tim Harley, Ivo Danihelka, Agnieszka Grabska-
Barwińska, Sergio Gómez Colmenarejo, Edward
Grefenstette, Tiago Ramalho, John Agapiou, et al.
2016. Hybrid computing using a neural net-
work with dynamic external memory. Nature
538(7626):471.

Minghao Hu, Yuxing Peng, and Xipeng Qiu. 2017. Re-
inforced mnemonic reader for machine comprehen-
sion. CoRR, abs/1705.02798 .

Gao Huang, Zhuang Liu, Kilian Q Weinberger, and
Laurens van der Maaten. 2017a. Densely connected
convolutional networks. In Proceedings of the IEEE
conference on computer vision and pattern recogni-
tion. volume 1, page 3.

Hsin-Yuan Huang, Chenguang Zhu, Yelong Shen, and
Weizhu Chen. 2017b. Fusionnet: Fusing via fully-
aware attention with application to machine compre-
hension. arXiv preprint arXiv:1711.07341 .

Mandar Joshi, Eunsol Choi, Daniel Weld, and Luke
Zettlemoyer. 2017. Triviaqa: A large scale distantly
supervised challenge dataset for reading comprehen-
sion. In Proceedings of the 55th Annual Meeting of
the Association for Computational Linguistics (Vol-
ume 1: Long Papers). volume 1, pages 1601–1611.

Ankit Kumar, Ozan Irsoy, Peter Ondruska, Mohit
Iyyer, James Bradbury, Ishaan Gulrajani, Victor
Zhong, Romain Paulus, and Richard Socher. 2016.
Ask me anything: Dynamic memory networks for
natural language processing. In International Con-
ference on Machine Learning. pages 1378–1387.

Kenton Lee, Shimi Salant, Tom Kwiatkowski, Ankur
Parikh, Dipanjan Das, and Jonathan Berant. 2016.
Learning recurrent span representations for ex-
tractive question answering. arXiv preprint
arXiv:1611.01436 .

Rui Liu, Wei Wei, Weiguang Mao, and Maria Chik-
ina. 2017a. Phase conductor on multi-layered at-
tentions for machine comprehension. arXiv preprint
arXiv:1710.10504 .

Xiaodong Liu, Yelong Shen, Kevin Duh, and Jian-
feng Gao. 2017b. Stochastic answer networks for
machine reading comprehension. arXiv preprint
arXiv:1712.03556 .

Boyuan Pan, Hao Li, Zhou Zhao, Bin Cao, Deng Cai,
and Xiaofei He. 2017. Memen: Multi-layer embed-
ding with memory networks for machine compre-
hension. arXiv preprint arXiv:1707.09098 .

Jeffrey Pennington, Richard Socher, and Christopher
Manning. 2014. Glove: Global vectors for word
representation. In Proceedings of the 2014 confer-
ence on empirical methods in natural language pro-
cessing (EMNLP). pages 1532–1543.

Matthew E Peters, Mark Neumann, Mohit Iyyer, Matt
Gardner, Christopher Clark, Kenton Lee, and Luke
Zettlemoyer. 2018. Deep contextualized word rep-
resentations. arXiv preprint arXiv:1802.05365 .

Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and
Percy Liang. 2016. Squad: 100,000+ questions for
machine comprehension of text. In Proceedings of
the 2016 Conference on Empirical Methods in Nat-
ural Language Processing. pages 2383–2392.

Shimi Salant and Jonathan Berant. 2017. Contextu-
alized word representations for reading comprehen-
sion. arXiv preprint arXiv:1712.03609 .



30

Minjoon Seo, Aniruddha Kembhavi, Ali Farhadi, and
Hannaneh Hajishirzi. 2017. Bidirectional attention
flow for machine comprehension. In Proceedings of
the International Conference on Learning Represen-
tations .

Sainbayar Sukhbaatar, Jason Weston, and Rob Fer-
gus. 2015. End-to-end memory networks. In Ad-
vances in neural information processing systems.
pages 2440–2448.

Shuohang Wang and Jing Jiang. 2016. Learning natu-
ral language inference with lstm. In Proceedings of
the 2016 Conference of the North American Chap-
ter of the Association for Computational Linguistics:
Human Language Technologies. pages 1442–1451.

Shuohang Wang and Jing Jiang. 2017. Machine com-
prehension using match-lstm and answer pointer.
In Proceedings of the International Conference on
Learning Representations .

Wenhui Wang, Nan Yang, Furu Wei, Baobao Chang,
and Ming Zhou. 2017. Gated self-matching net-
works for reading comprehension and question an-
swering. In Proceedings of the 55th Annual Meet-
ing of the Association for Computational Linguistics
(Volume 1: Long Papers). volume 1, pages 189–198.

Dirk Weissenborn. 2017. Dynamic integration of back-
ground knowledge in neural nlu systems. arXiv
preprint arXiv:1706.02596 .

Dirk Weissenborn, Georg Wiese, and Laura Seiffe.
2017. Making neural qa as simple as possible but
not simpler. In Proceedings of the 21st Confer-
ence on Computational Natural Language Learning
(CoNLL 2017). pages 271–280.

Jason Weston, Antoine Bordes, Sumit Chopra, Alexan-
der M Rush, Bart van Merriënboer, Armand Joulin,
and Tomas Mikolov. 2015a. Towards ai-complete
question answering: A set of prerequisite toy tasks.
arXiv preprint arXiv:1502.05698 .

Jason Weston, Sumit Chopra, and Antoine Bordes.
2015b. Memory networks.

Caiming Xiong, Victor Zhong, and Richard Socher.
2017. Dynamic coattention networks for question
answering. In Proceedings of the International Con-
ference on Learning Representations .

Adams Wei Yu, David Dohan, Quoc Le, Thang Luong,
Rui Zhao, and Kai Chen. 2018. Qanet: Combining
local convolution with global self-attention for read-
ing comprehension .

Yang Yu, Wei Zhang, Kazi Hasan, Mo Yu, Bing Xi-
ang, and Bowen Zhou. 2016. End-to-end reading
comprehension with dynamic answer chunk rank-
ing. arXiv preprint arXiv:1610.09996 .

Matthew D Zeiler. 2012. Adadelta: an adaptive learn-
ing rate method. arXiv preprint arXiv:1212.5701 .


