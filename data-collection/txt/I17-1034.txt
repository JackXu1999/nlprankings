



















































Text Sentiment Analysis based on Fusion of Structural Information and Serialization Information


Proceedings of the The 8th International Joint Conference on Natural Language Processing, pages 336–341,
Taipei, Taiwan, November 27 – December 1, 2017 c©2017 AFNLP

Text Sentiment Analysis based on Fusion of Structural Information and
Serialization Information

Ling Gan and Houyu Gong
College of Computer Science and Tecnology

Chongqing University of Posts and Telecommunications
Chongqing 400065, China

ganling@cqupt.edu.cn,gonghouyu b103@163.com

Abstract

Tree-structured Long Short-Term Memo-
ry (Tree-LSTM) has been proved to be an
effective method in the sentiment analy-
sis task. It extracts structural information
on text, and uses Long Short-Term Memo-
ry (LSTM) cell to prevent gradient vanish.
However, though combining the LSTM
cell, it is still a kind of model that extract-
s the structural information and almost
not extracts serialization information. In
this paper, we propose three new mod-
els in order to combine those two kind-
s of information: the structural informa-
tion generated by the Constituency Tree-
LSTM and the serialization information
generated by Long-Short Term Memory
neural network. Our experiments show
that combining those two kinds of infor-
mation can give contributes to the perfor-
mance of the sentiment analysis task com-
pared with the single Constituency Tree-
LSTM model and the LSTM model.

1 Introduction

Text sentiment analysis, namely Opinion mining,
is an important research direction in the field of
Natural Language Processing (NLP). It aims to ex-
tract the author’s subjective information from text
and provide useful values for us. In recent years,
there were more and more researchers paying at-
tention to the study of text sentiment analysis.

Up to date, a variety of methods have been de-
veloped for improving the performance of sen-
timent analysis models. The distributed repre-
sentation for words has been proposed in 2003
(Bengio et al., 2003). This model trained by t-
wo kinds of three-layer neural networks gener-
ates vectors to represent words. Glove, which is

the improvement of the model mentioned above,
has been proposed in 2014 (Pennington et al.,
2014). The improved representation of words
gives contribution to the research of NLP tasks
including sentiment analysis, and are used as
the input of sentiment analysis models. There
are several kinds of deep learning method-
s to extract text features. Sequential models
such as Recurrent Neural Network (Schmidhuber,
1990), Bidirectional Recurrent Neural Networks
(Member et al., 1997), Long Short-Term Memo-
ry (Hochreiter and Schmidhuber, 2012) and Gat-
ed Recurrent Unit (Cho et al., 2014) mainly ex-
tract serialization information of text. Multi-layer
sequential models have also been proposed for
sentiment analysis (Wang et al., 2016)(Tang et al.,
2015)(He et al., 2016)(Yang et al., 2016). Tree-
structured models extract structural information.
The first tree-structured model named Recur-
sive Neural Network has been proposed in 2012
(Socher et al., 2012b), followed by more model-
s such as Matrix-Vector Recursive Neural Net-
work (Socher et al., 2012a) and Recursive Neural
Tensor Network (Socher et al., 2013). In 2015,
Tree-structured Long Short-Term Memory Neural
Network (Tree-LSTM), which combines the LST-
M cell and tree-structured models, has been pro-
posed and it outperforms the traditional LSTM and
tree-structured neural networks (Le and Zuidema,
2015)(Tai et al., 2015)(Zhu et al., 2015). Differ-
ent from the traditional tree-structured model,
Tree-LSTM uses the LSTM cell to control the in-
formation from bottom to top so that it can effec-
tively prevent the vanishing gradient problem.

As mentioned above, though combining the L-
STM cell, Tree-LSTM does not really combine
the structural information and serialization infor-
mation. In this paper, we introduce three models:
Tree-Composition LSTM (TC-LSTM), Leaf-Tree
LSTM (LT-LSTM) and Leaf-Composition-Tree L-

336



STM (LCT-LSTM). Those models combine those
two kinds of information and experiments show
that they perform better than the traditional LSTM
and the Constituency Tree-LSTM model.

The remainder of this paper is organized as fol-
lows: Section 2 introduces the LSTM and Tree-
LSTM model which are related to our work. Sec-
tion 3 introduces the models proposed in this pa-
per. Experimental results are shown in Section 4
and in Section 5 we give the conclusions and fu-
ture work.

2 Background

2.1 Long Short-Term Memory
Recurrent Neural Network (RNN) (Schmidhuber,
1990) encodes text information according to time.
Giving a sentence, its words are encoded by the
model in chronological order. For example, xt
represents the vector of input word at time step t,
the hidden unit of time step t can be calculated as
follows:

ht = tanh(Wxt + Uht−1 + b). (1)

ht represents the hidden layer at time step t,
ht−1 represents the hidden state at time step t− 1,
W is the weight matrix of the input layer, U is the
weight matrix between ht−1 and ht, b represents
the bias and tanh is the activation function which
can normalize output information:

tanh(x) =
sinh(x)
cosh(x)

=
ex − e−x
ex + e−x

. (2)

At each time step, the hidden layer produces an
output layer:

ot = softmax(V ht + b). (3)

Usually, the output of the final time step can
be used to represent the feature of a sentence.
Then, after forward propagation, the weights of
model are trained by the backward propagation.
Traditional RNN model has the problem of gra-
dient vanish. Long Short-Term Memory (LSTM)
(Hochreiter and Schmidhuber, 2012) Neural Net-
work has effectively solved the problem. The
model has four gates which help to selectively for-
get or remember information. A memory cell has
also been added to memory information transmit-
ted over time steps. The information calculated by
an LSTM unit can be shown as follows (Tai et al.,
2015):

it = σ(W (i)xt + U iht−1 + bi), (4)

ft = σ(W (f)xt + Ufht−1 + bf ), (5)

ot = σ(W (o)xt + Uoht−1 + bo), (6)

ut = tanh(W (u)xt + Uuht−1 + bu), (7)

ct = it ∗ ut + ft ∗ ct−1, (8)
ht = ot ∗ tanh(ct). (9)

Here, it, ft, ut, ot denote the four gates, ct is
the memory cell, ∗ represents element-wise mul-
tiplication. Intuitively, input gate (it) and update
gate (ut) denote how much the memory cell up-
date information, forget gate (ft) determines how
much the memory cell forget history information
and output gate (ot) controls how much the hidden
unit get information from the cell. σ represents the
sigmoid function. The weights of different layers
are different but they are shared at each time step
in the same layer.

2.2 Tree-LSTM

Dependency Tree-LSTM and Constituency Tree-
LSTM are two types of Tree-LSTM structures. We
discuss the latter because it achieves a better per-
formance in the sentiment analysis task (Tai et al.,
2015). Constituency Tree-LSTM includes three
types of layers. Input layer includes the leaf nodes,
it consists of the words in the sentence, each word
is represented by a vector. Composition layer acts
as the hidden layer which composes the informa-
tion flowing from the leaf nodes to the root node.
Each composition unit can be seen as the structural
feature of its leaf nodes. The final composition n-
ode (root node) represents the structural feature of
the whole sentence, it is the input of output lay-
er. LSTM cell is used to control the information
flowed bottom-up. Different from the cell in se-
quential models, the hidden information of a com-
position node comes from their two child nodes:

ij = σ(W (i)xj +
N∑

l=1

U
(i)
l hjl + b

(i)), (10)

fjk = σ(W (f)xj +
N∑

l=1

U
(f)
kl hjl + b

(f)), (11)

oj = σ(W (o)xj +
N∑

l=1

U
(o)
l hjl + b

(o)), (12)

337



uj = tanh(W (u)xj +
N∑

l=1

U
(u)
l hjl + b

(u)), (13)

cj = ij ∗ uj +
N∑

l=1

fjl ∗ cjl, (14)

hj = oj ∗ tanh(cj). (15)
It is worth noting that, in the input layer, the in-

formation composing the gates only include the in-
put words (xj) but do not have hidden information
(hjl). In the composition layer and output layer,
only hidden information from two sub nodes par-
ticipates in the construction of gates. The struc-
ture of Constituency Tree-LSTM model is shown
in Figure 1.

Figure 1: Constituency Tree-LSTM model. The
upward arrows represent the direction of forward
propagation, the downward arrows represent the
direction of the backward propagation.

3 Our model

In this section, we discuss three new models which
combine the structural information generated by
tree-structured model (Constituency Tree-LSTM)
and serialization information generated by sequen-
tial model (LSTM).

3.1 TC-LSTM
Constituency Tree-LSTM uses composition node
at the root of the tree to represent the feature of
sentence. We propose a new model named Tree-
Composition LSTM (TC-LSTM) which generates
a new feature taking all the leaf nodes, compo-
sition nodes and their sequential information in-
to account. Firstly, we use postorder traversal to
get all the nodes in the tree, and those nodes are

treated as a sequence. The sequence contains not
only the words in the sentence, but also the struc-
tural information in their parent composition n-
odes. Then we put the sequence into LSTM model
for training, thus obtaining the serialization infor-
mation of those hidden nodes with structural in-
formation. It is worth noting that, though sharing
the same hidden nodes, in our first proposed mod-
el, the weights of the original Tree-LSTM module
and the new added LSTM module are trained in-
dependently. The input word vectors firstly per-
form forward propagation on the Tree-LSTM, and
then, the sequence mentioned above is obtained.
Then the forward propagation of the sequence is
performed on the LSTM model. The output er-
ror and gradient of the two modules are obtained
through the training label separately. Finally, the
backward propagation performs independently of
each other. The gradients of back propagation up-
dating the word vectors of input layer only flows
from Tree-LSTM module.

TC-LSTM model is shown in Figure 2. After
training, we only use the output of LSTM mod-
ule to test on test data in order to verify the per-
formance of sequential information extracted from
tree-structured model.

Figure 2: TC-LSTM model. New LSTM module
is added to the original Tree-LSTM model. They
shared the same hidden nodes but trained separate-
ly. Only the output of LSTM module is used to do
the prediction.

3.2 LT-LSTM

We propose the Leaf-Tree LSTM (LT-LSTM)
model to give a combination of the structural in-
formation and sequential information of a sen-
tence. Similar to TC-LSTM, this model has t-

338



wo modules but it only has one output. The first
module is the same to Tree-LSTM, and the sec-
ond module is the LSTM module which takes the
leaf nodes of the tree, namely only the words of
a sentence as input. During the forward propaga-
tion, we add the output of two modules and take
the result as the output of the whole model. Gradi-
ent of the whole model is generated by the output,
and then, assigned to the output of two modules
for their backward propagation. Figure 3 shows
the structure of the LT-LSTM model.

The output represents the combination of those
two kinds of information: structural information
and serialization information. Correspondingly,
The gradient of the output layer contains the er-
ror information of the Tree-LSTM module and L-
STM module. Letting the gradient propagate top-
down through those two modules can make them
learn from each other, thus having the chance to
make the comprehensive performance of the w-
hole model better.

Figure 3: LT-LSTM model. The new added LST-
M model only takes the leaf nodes as input. The
output represent the fusion of two kinds of infor-
mation, and its gradient trains the whole model.

3.3 LCT-LSTM

The third model proposed by us is Leaf-
Composition-Tree LSTM (LCT-LSTM).Different
from the LT-LSTM and TC-LSTM, this model
not only takes the composition nodes into consid-
eration when building the LSTM layer, but also
makes sum of the outputs of two modules men-
tioned above. That is, LCT-LSTM can be seen as
the composition of TC-LSTM and LT-LSTM for

the reason of not only building the sequential fea-
ture for the composition nodes which contain the
structural information, but also combining the se-
quential feature and the structural feature of the
input sentence. The structure of LCT-LSTM is
shown in Figure 4.

Figure 4: LCT-LSTM model.

4 Experiment

4.1 Dataset

We evaluate our proposed models on the Stanford
Sentiment Tree Bank (SST) dataset, which con-
tains sentences collected from movie reviews. The
sentences in the dataset are split into three parts:
8544 for training, 1101 for development and 2210
for test. SST dataset has two classification tasks,
one for fine-grained classification (five categories:
very negative, negative, neutral, positive, and very
positive) and the other for binary classification (t-
wo categories: negative and positive). The fine-
grained subtask is evaluated on 8544/1101/2210
splits, and the binary classification is evaluated on
6920/872/1821 splits (there are fewer sentences
because the neutral examples are excluded). Ev-
ery sentence in the dataset is processed into tree
structure, and every phrase (corresponding to the
nodes in the tree) in the sentence is also labeled.

4.2 Hyperparameters and Training Details

We use the Glove vectors of 300 dimension
(Pennington et al., 2014) to represent the input
words. Word embeddings are fine-tuned during
training and the learning rate used for the input
layer is 0.1, for the other layers is 0.05. Adagrad

339



θ − all
Models F B θ − com
LSTM 271955 271653 271200
Tree-LSTM 317555 317253 316800
TC-LSTM 499510 498906 498755
LT-LSTM 499510 498906 498755
LCT-LSTM 499510 498906 498755

Table 1: Parameters of models. θ − all represents
the number of all the parameters in a model for F
(fine-grained) tasks and B (binary tasks). θ− com
represents parameters of composition layer. TC-
LSTM, LT-LSTM and LCT-LSTM have the same
number of parameters but they are trained in dif-
ferent ways.

algorithm is used for training, the minibatch size
set by us is 25, L2-regularization is used for each
batch using the value of 1e-4, and the dropout for
the output layer is 0.5.

The dimension of the input layer is the same as
the word vector, and the hidden layer consisted of
tree nodes has the dimension of 150. For the se-
quential module, both of the inputs and the hidden
layer have the dimension of 150, the vectors of leaf
nodes are projected into the 150 dimension when
put into the sequential part. The numbers of pa-
rameters for all the models are shown in Table 1.

Every model is trained on the training set for
20 epochs, and tested on the development set for
validation after finishing every epoch. We choose
the parameters performing best among them to do
the evaluation on the test set. For every model, we
repeat experiments for 8 times and take the aver-
age of their results as the final performance of the
model.

4.3 Baseline

The models proposed by us fuse the structural
information and serialization information, so we
compare those models with other models which
do not combine those two kinds of information.
We choose the Constituency Tree-LSTM, LSTM
and BiLSTM mentioned in 2015 (Tai et al., 2015)
as the baseline models, other tree-structure models
such as RNN, MV-RNN and RNTN are also used
for comparison.

4.4 Result

The results of experiment are shown in Table 2.
We use accuracy to measure the performance of
models.

Models Fine-grained Binary

LSTM (Tai et al., 2015) 46.4 84.9
Bi-LSTM (Tai et al., 2015) 49.1 87.5
RNN (Socher et al., 2013) 43.2 82.4
MV-RNN (Socher et al., 2013) 44.4 82.9
RNTN (Socher et al., 2013) 45.7 85.4
Constituency Tree-LSTM 50.7 88.0

TC-LSTM 49.6 88.2
LT-LSTM 51.0 88.5
LCT-LSTM 50.9 88.7

Table 2: The result of accuracy on the test
set. Fine-grained represents the five-category
classification and the Binary represents the posi-
tive/negative classification.

From Table 2, we can see that on the whole, the
models fusing structural and serialization informa-
tion outperform other models which do not com-
bine those two kinds of information. LT-LSTM
achieves the best performance among our com-
pared models in the fine-grained subtask and LCT-
LSTM has the best performance in the binary sub-
task. TC-LSTM performs slightly better than Con-
stituency Tree-LSTM in the binary subtask but
worse than fine-grained subtask, but it still per-
forms better than other single sequential models
and tree-structured models.

We find that building the serialization feature
for the nodes in tree-structure (TC-LSTM) does
not really help the tree-structural models, but fus-
ing the structural information and serialization in-
formation gives help to it. While fusing, adding
the hidden nodes containing the structural infor-
mation to the sequential model (LCT-LSTM) per-
forms better in the binary subtask, but slightly
worse in the fine-grained subtask compared to the
model does not do so (LT-LSTM).

5 Conclusion

In this paper, we propose three new models in or-
der to explore the effect of fusing the structural
and sequential information. We evaluate our mod-
els on the Stanford Sentiment Tree Bank (SST).
Experiments show that fusing the structural in-
formation and sequential information is an effec-
tive way to improve the performance of model-
s proposed before. Future work can be focused
on finding better ways to fusing those two fea-
tures. Other models, such as the Bidirection-
al Long Short-Term Memory (BiLSTM), Bidi-
rectional Tree-LSTM (Teng and Zhang, 2016) and
TreeGRU (Kokkinos and Potamianos, 2017) can

340



be used in place of the tree-structured model and
the sequential model used in our models. Atten-
tion mechanism (Luong et al., 2015) can also be
used to do some improvement.

References
Yoshua Bengio, Rjean Ducharme, Pascal Vincent,

Christian Jauvin, K Jaz, Thomas Hofmann, Toma-
so Poggio, and John Shawe-Taylor. 2003. Journal
of machine learning research 3 (2003) 1137–1155
submitted 4/02; published 2/03 a neural probabilis-
tic language model .

Kyunghyun Cho, Bart Van Merrienboer, Caglar Gul-
cehre, Dzmitry Bahdanau, Fethi Bougares, Holger
Schwenk, and Yoshua Bengio. 2014. Learning
phrase representations using rnn encoder-decoder
for statistical machine translation. Computer Sci-
ence .

Yunchao He, Liang Chih Yu, Chin Sheng Yang,
K. Robert Lai, and Weiyi Liu. 2016. Yzu-nlp team
at semeval-2016 task 4: Ordinal sentiment clas-
sification using a recurrent convolutional network.
In International Workshop on Semantic Evaluation.
pages 251–255.

Sepp Hochreiter and Schmidhuber. 2012. Long short-
term memory. Neural Computation 9(8):1735–
1780.

Filippos Kokkinos and Alexandros Potamianos. 2017.
Structural attention neural networks for improved
sentiment analysis .

Phong Le and Willem Zuidema. 2015. Compositional
distributional semantics with long short term mem-
ory. Computer Science .

Minh Thang Luong, Hieu Pham, and Christopher D.
Manning. 2015. Effective approaches to attention-
based neural machine translation. Computer Sci-
ence .

Member, IEEE, Mike Schuster, and Kuldip K. Pali-
wal. 1997. Bidirectional recurrent neural net-
works. IEEE Transactions on Signal Processing
45(11):2673–2681.

Jeffrey Pennington, Richard Socher, and Christopher
Manning. 2014. Glove: Global vectors for word
representation. In Conference on Empirical Meth-
ods in Natural Language Processing. pages 1532–
1543.

Schmidhuber. 1990. Recurrent networks adjusted by
adaptive critics .

R. Socher, A. Perelygin, J. Y. Wu, J. Chuang, C. D.
Manning, A. Y. Ng, and C. Potts. 2013. Recursive
deep models for semantic compositionality over a
sentiment treebank .

Richard Socher, Brody Huval, Christopher D Manning,
and Andrew Y Ng. 2012a. Semantic compositional-
ity through recursive matrix-vector spaces. In Joint
Conference on Empirical Methods in Natural Lan-
guage Processing and Computational Natural Lan-
guage Learning. pages 1201–1211.

Richard Socher, Chiung Yu Lin, Andrew Y. Ng, and
Christopher D. Manning. 2012b. Parsing natural
scenes and natural language with recursive neural
networks. In International Conference on Machine
Learning, ICML 2011, Bellevue, Washington, Usa,
June 28 - July. pages 129–136.

Kai Sheng Tai, Richard Socher, and Christopher D.
Manning. 2015. Improved semantic representation-
s from tree-structured long short-term memory net-
works. Computer Science 5(1):: 36.

Duyu Tang, Bing Qin, and Ting Liu. 2015. Documen-
t modeling with gated recurrent neural network for
sentiment classification. In Conference on Empiri-
cal Methods in Natural Language Processing. pages
1422–1432.

Zhiyang Teng and Yue Zhang. 2016. Bidirectional
tree-structured lstm with head lexicalization .

Jin Wang, Liang Chih Yu, K. Robert Lai, and Xuejie
Zhang. 2016. Dimensional sentiment analysis using
a regional cnn-lstm model. In Meeting of the Asso-
ciation for Computational Linguistics. pages 225–
230.

Zichao Yang, Diyi Yang, Chris Dyer, Xiaodong He,
Alex Smola, and Eduard Hovy. 2016. Hierarchical
attention networks for document classification. In
Conference of the North American Chapter of the
Association for Computational Linguistics: Human
Language Technologies. pages 1480–1489.

Xiaodan Zhu, Parinaz Sobhani, and Hongyu Guo.
2015. Long short-term memory over tree structures
.

341


