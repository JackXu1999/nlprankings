




































Extracting Entities and Relations with Joint Minimum Risk Training


Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 2256–2265
Brussels, Belgium, October 31 - November 4, 2018. c©2018 Association for Computational Linguistics

2256

Extracting Entities and Relations with Joint Minimum Risk Training
Changzhi Sun2, Yuanbin Wu1, 2, Man Lan1, 2, Shiliang Sun2,

Wenting Wang3, Kuang-Chih Lee3, and Kewen Wu3

1Shanghai Key Laboratory of Multidimensional Information Processing
2Department of Computer Science and Technology, East China Normal University

3Alibaba Group
{changzhisun}@stu.ecnu.edu.cn

{ybwu,mlan,slsun}@cs.ecnu.edu.cn
{nentiao.wwt,kuang-chih.lee,kewen.wukw}@alibaba-inc.com

Abstract

We investigate the task of joint entity relation
extraction. Unlike prior efforts, we propose a
new lightweight joint learning paradigm based
on minimum risk training (MRT). Specifically,
our algorithm optimizes a global loss function
which is flexible and effective to explore inter-
actions between the entity model and the rela-
tion model. We implement a strong and simple
neural network where the MRT is executed.
Experiment results on the benchmark ACE05
and NYT datasets show that our model is able
to achieve state-of-the-art joint extraction per-
formances.

1 Introduction

Detecting entities and relations is usually the first
step towards extracting structured knowledge from
plain texts. Its goal is to identify text spans repre-
senting typed objects (entities) and semantic rela-
tions among those text spans (relations). For ex-
ample, in the following sentence,

[Associated Press]ORG [writer]PER [Patrick
McDowell]PER in [Kuwait City]GPE.

“Associate Press” is an organization entity (ORG),
“writer” is a person entity (PER), and the two en-
tities have an affiliation relation (ORG-AFF).

Two types of models have been applied to the
extraction task, the pipeline model and the joint
model. In the pipeline setting, the task is bro-
ken down into independently learned components
(an entity model and a relation model). De-
spite its flexibility, the pipeline ignores interac-
tions between the two models. For example, the
entity model doesn’t look at relation annotations
which are useful for identifying entities (e.g., if
an ORG-AFF relation exists, the entity model can
only assign ORG and AFF to its entities). The
joint setting, on the other hand, extracts entities

relation
model

joint decoder 

MRT light interaction

heavy interaction

entity decoder relation decoder sharedparameters

no interaction

entity decoder relation decoder 

entity
model

Figure 1: Paradigms of joint entity relation extraction.

and relations in a unified model, which can ex-
plore shared information and alleviate error prop-
agations between models. Here we will focus on
joint models.

One simple joint learning paradigm is through
sharing parameters (Miwa and Bansal, 2016; Kati-
yar and Cardie, 2017). Typically, instead of train-
ing two independent models, the entity and rela-
tion model can share some input features or in-
ternal hidden states. It has an advantage that no
additional constraint is required on the two sub-
models. But the connections among sub-models
are still not fully explored due to independent sub-
model decoders. For example, to get signals from
relation annotations, the entity model needs to
wait for the relation model to update the shared
parameters. To further utilize the interaction be-
tween decoders, some complex joint decoding al-
gorithms (e.g., simultaneously decoding entities
and relations in beam search) have been carefully
studied (Li and Ji, 2014; Katiyar and Cardie, 2016;
Zhang et al., 2017; Zheng et al., 2017). In this
paradigm, it is important (and hard) to make a
good balance between the exactness of the joint
decoding algorithm and capacities of individual
sub-models.

In this work, we propose a joint minimum risk
training (MRT) (Och, 2003; Smith and Eisner,
2006) method for the entity and relation extraction



2257

task. It provides a lightweight way to strengthen
connections between the entity model and the rela-
tion model, and keeps their capacities unaffected.
Given an input x and a loss function ∆(ŷ,y)
(measuring the difference between model output
ŷ and the true y), MRT seeks a posterior P (ŷ|x)
to minimize the expected loss Eŷ∼P (ŷ|x)∆(ŷ,y).
Comparing with prior joint decoding algorithms,
the MRT-based algorithm is simple and can be ap-
plied to a broad range of entity relation extraction
models without changing the original sub-models
and decoders (Figure 1).

One advantage of the MRT-based method is that
it can explicitly optimize a global sentence-level
loss (e.g., F1 score) rather than local token-level
losses. Therefore, it may catch more sentence-
level information in the training time and match
evaluation metrics better in the testing time. Fur-
thermore, besides the handcrafted losses, we also
try to directly learn a loss function from data dur-
ing the joint MRT process. The automatically ob-
tained loss would help MRT to calibrate its risk
estimation with knowledge from the data distribu-
tion. On the other hand, comparing with preivous
single task MRT, the joint MRT algorithm here
will integrate messages from different sub-models,
which is the key step for enhancing decoder in-
teractions in the joint learning. As a result, the
training of the entity model now can directly ac-
knowledge the loss of the relation model (without
waiting for shared parameters) and vice versa.

We compile the proposed joint MRT with a
strong neural network-based model which uses re-
current neural networks (RNN) in the entity model
and convolutional neural networks (CNN) in the
relation model. On benchmark ACE05 and NYT
datasets, we show that the new RNN+CNN struc-
ture outperforms previous neural network-based
models. After adding the joint MRT, our model
is able to achieve state-of-the-art performances.

To summarize, our main contributions include

1. proposing a new joint learning paradigm based
on minimum risk training for the joint entity
relation extraction task.

2. implementing a strong and simple neural-
network-based entity relation extraction model
which carries the proposed MRT algorithm. 1

1Our implementation is available at https://
github.com/changzhisun/entrel-joint-mrt.

3. achieving state-of-the-art results on two bench-
mark datasets (ACE05 and NYT).

2 Related Work

In many pipelined entity relation extraction sys-
tems, one first learns an entity model, then learns
a relation model based on entities generated by the
entity model (Miwa et al., 2009; Chan and Roth,
2011; Lin et al., 2016). Such systems are often
flexible to incorporate different data sources and
different learning algorithms. However, they may
also suffer from error propagation and data ineffi-
ciency. To tackle the problem, many recent studies
try to develop joint extraction algorithms.

Parameter sharing is a basic strategy in joint
learning paradigms. For example, in (Miwa and
Bansal, 2016), the entity model is a sentence-level
RNN, and the relation model is a dependency tree
path RNN which takes hidden states of the en-
tity model as features (i.e., the shared parame-
ters). Our basic extraction model is similar to
theirs but with a CNN-based relation model. Simi-
larly, Katiyar and Cardie (2017) build a simplified
relation model on the entity RNN using the atten-
tion mechanism.

To further explore interactions between the en-
tity decoder and the relation decoder, some joint
decoding algorithms were studied. For example,
Katiyar and Cardie (2016) propose a CRF-based
model which conducts joint decoding with aug-
mented transition matrices. Zheng et al. (2017)
propose to directly encode relations in the sequen-
tial labelling tag set. Both of them are exact decod-
ing algorithms, but they need adding constraints
on the relation model (e.g., Zheng et al. (2017)
cannot handle entities which appear in multiple re-
lations). On the other side, Li and Ji (2014) de-
velop a joint decoding algorithm based on beam
search. Zhang et al. (2017) study a globally nor-
malized joint model. They retain capacities of sub-
models, while their decoding algorithms are inex-
act. Here, we introduce MRT to the task, which is
a more lightweight setting of joint learning.

Minimum risk training is a learning framework
which tries to handle models with arbitrary dis-
crepancy metrics (i.e., losses of a model output
w.r.t. the true answer) (Och, 2003; Smith and Eis-
ner, 2006; Gimpel and Smith, 2010). It has been
successfully applied to many NLP tasks. Some
recent work include (He and Deng, 2012; Shen
et al., 2016) which apply MRT to (neural) ma-

https://github.com/changzhisun/entrel-joint-mrt
https://github.com/changzhisun/entrel-joint-mrt


2258

chine translation, (Xu et al., 2016) which develops
a shift-reduce CCG parser to directly optimize F1,
and (Ayana et al., 2016) which uses a MRT-based
model for summarization. We note that most pre-
vious applications of MRT focus on a single job,
while the joint entity relation extraction consists of
two sub-tasks. Investigating MRT in joint learning
scenarios is the main topic of this work.

Finally, the sampling algorithm of solving MRT
is similar to the policy gradient algorithm in rein-
forcement learning (RL) (Sutton and Barto, 1998).
Some recent NLP applications which share the key
idea of MRT but are described with RL language
also show promising results (e.g., dialog systems
(Li et al., 2016), machine translation (Nguyen
et al., 2017)). The idea of learning loss functions
from data is similar to inverse reinforcement learn-
ing (Abbeel and Ng, 2004; Ratliff et al., 2006).

3 The Approach

We define the joint entity and relation extraction
task following the setting of (Miwa and Bansal,
2016). Given an input sentence s = w1, . . . , w|s|
(wi is a word), the task is to extract a set of enti-
ties E and a set of relations R. An entity e ∈ E is
a sequence of words labelling with an entity type
(e.g., person (PER), organization (ORG)). Let Te
be the set of possible entity types. A relation r is a
triple (e1, e2, l), where e1 and e2 are two entities,
l is a relation type describing the semantic relation
between e1 and e2 (e.g., organization affiliation re-
lation (ORG-AFF)). Let Tr be the set of possible
relation types.

In our joint extraction method (Figure 2), we
treat entity detection as a sequence labelling task
(Section 3.1) and relation detection as a classifi-
cation task (Section 3.2). Models of the two tasks
share parameters and are trained jointly. Departing
from previous joint learning algorithms (Miwa and
Bansal, 2016; Katiyar and Cardie, 2017; Zhang
et al., 2017), we introduce minimum risk training
to the joint extraction model. It optimizes a global
loss function and bridges the discrepancy between
training and testing (Section 3.3).

3.1 Entity Detection

To represent entities in s, we assign a tag ti to each
word wi following the BILOU tagging scheme: ti
takes a value in {B-∗,I-∗,L-∗,O,U-∗}, where
B, I, L and O denote the begin, inside, end and
outside of an entity, U denotes a single word en-

tity and ∗ ∈ Te represents different entity types.
For example, for a person (PER) entity “Patrick
McDowell”, we assign B-PER to “Patrick” and
L-PER to “McDowell”. Given an input sentence
s, the entity model predicts the tags of words
t̂ = t̂1, t̂2, . . . , t̂|s| by learning from the true tags
t = t1, t2, . . . , t|s|.

We use a bidirectional long short term memory
(bi-LSTM) network (Hochreiter and Schmidhu-
ber, 1997) to solve the sequence labelling task. At
each sentence position i, a forward LSTM chain
computes a hidden state vector h⃗i by recursively
collecting information from the beginning of s
to the current position i. Similarly, a backward
LSTM chain collects information ⃗hi from the end
of s to the position i.

h⃗i = LSTM(xi, h⃗i−1; θ⃗),

⃗hi = LSTM(xi, ⃗hi+1; ⃗θ).

The word representation xi of wi has two parts
xi = wi ⊕ ci (⊕ is the vector concatenation). wi
is a word embedding of word wi (from an embed-
ding look-up table We). ci is a character-based
representation of wi which is obtained by running
a convolution neural network on the character se-
quence of wi: ci = CNN(char(wi);θc).

To predict the tag t̂i, we combine the forward
and the backward hidden vector to hi = h⃗i ⊕ ⃗hi,
and apply a softmax function on hi to get the pos-
terior of t̂i,

Pent(t̂i|s;θE) = Softmax(WE · hi), (1)

where θE = {We,θc, θ⃗, ⃗θ,WE} are parameters
of the entity model. Given an input sentence s and
its ground truth tag sequence t, the training objec-
tive is to minimize Lent, 2

Lent(θE) = −
1

|s|

|s|∑
i=1

logPent(t̂i = ti|s;θE).

3.2 Relation Detection
Given a set of detected entities Ê (obtaining from
the entity tag sequence t̂), we consider all en-
tity pairs in Ê as candidate relations. The task
of relation detection is to predict a relation type
l ∈ Tr for each pair, 3 and output a relation set

2We have also tried biLSTM-CRF (Huang et al., 2015)
as an advanced entity model, but performances are nearly the
same in our experiments.

3We include a NONE relation type in Tr which means that
there exists no relation between e1 and e2.



2259

�

fe1 fe2fleft fright fdist

l
+

hi vi

B-PER B-GPE L-GPEL-PER O

PER GPE

CNN CNN CNN

MLP

word (w)

char (c)

bi-LSTM (h)

Softmax

Patrick McDowell in Kuwait City

fmiddle

hi

vi

Lent + Lrel

Lmrt

tags (t̂)

Joint Learning with MRT

global loss

local losses

Figure 2: Our network structure for the joint entity and relation extraction.

R̂ = {(e1, e2, l)|e1, e2 ∈ Ê , e1 ̸= e2, l ∈ Tr}.
To build the relation model, we extract two types
of features, namely, features regarding words in
e1, e2 and features regarding contexts of the entity
pair (e1, e2).

To extract features on words in e1, e2, we use
two convolutional neural networks. Taking e1 as
an example, for each word wi in e1, we first col-
lect wi’s bi-LSTM hidden vector hi from the en-
tity model. Then, we concatenate hi with a one-
hot entity tag representation vi of t̂i. We build a
feature vector fe1 for e1 by running a CNN (a sin-
gle convolution layer with a max-pooling layer) on
vectors {hi⊕vi|wi ∈ e1}. Similarly, we build fe2
for e2 with another CNN.

For context features of the entity pair (e1, e2),
we build three feature vectors by looking at words
between e1 and e2 (fmiddle), words on the left of the
pair (fleft) and words on the right of the pair (fright).
For fmiddle, we run a CNN on words between e1
and e2 like the case of fe1 , fe2 . For fleft and fright,
we use the “LSTM-Minus” method as (Wang and
Chang, 2016; Zhang et al., 2017). Assume that the
left context of (e1, e2) is from sentence position 0
to i, then fleft = h⃗i⊕ ( ⃗h0− ⃗hi+1). Similarly, if the
right context of (e1, e2) is from j to |s| − 1, then
fright = (h⃗|s|−1 − h⃗j−1)⊕ ⃗hj . We also use a one-
hot feature fdist to describe the distance between e1
and e2 in the sentence.

Finally, fe1 , fe2 , fmiddle, fleft, fright and fdist are
concatenated to a single vector fe1,e2 . To get the

posterior of the relation type l̂, we apply a multi-
layer perceptron with one hidden layer on fe1,e2 ,

Prel(l̂|s, e1, e2;θR)
= Softmax(W2 · ReLU(W1 · fe1,e2)), (2)

where θR = {θe1 , θe2 , θmiddle, W1,W2} contains
parameters of the relation model (shared parame-
ters with the entity model are omitted).

Given an input sentence s, the training objective
is to minimize

Lrel(θR) = −
∑

e1,e2∈Ê
e1 ̸=e2

logPrel(l̂ = l|s, e1, e2;θR)
|Ê |(|Ê | − 1)

,

where the true label l of a candidate entity pair
(e1, e2) can be read from true annotations.

3.3 Joint Minimum Risk Training
To jointly learn the entity model and the relation
model, one common strategy is to optimize the
combined objective function L̃ = Lent + Lrel,
where the joint learning is accomplished by the
shared parameters. However, we would think that
L̃ optimizes a “local” loss by observing that a) in
both Lent and Lrel, the loss functions are calcu-
lated by only looking at local parts. For example,
the loss in Lent is based on the correctness of lo-
cal entity tags ti rather than a global measurement
(e.g., F1 score of extracted entities), b) both the
entity model and the relation model are unaware
of the loss from the other side. For example, the



2260

entity model needs to wait for the relation model to
update the shared parameters rather than get direct
supervision from the loss of the relation model.

Here we introduce the minimum risk training
framework to the joint model. Comparing with
optimizing the local loss in L̃, the joint MRT will
optimize a global loss and provide a tighter con-
nection between the entity decoder and the rela-
tion decoder. To illustrate the algorithm, we first
aggregate some notations.

Let y ≜ (E ,R) contain the ground truth entity
tag sequence and relations, ŷ ≜ (Ê , R̂) contain
outputs of the joint extraction model and Y(s) be
the set of all possible outputs of the input sentence
s (y, ŷ ∈ Y(s)). We define the joint probability,

P (ŷ|s;θ) = P (Ê |s;θE)P (R̂|s, Ê ;θR)
=

∏
i

Pent(t̂i|s;θE)
∏

e1,e2∈Ê
e1 ̸=e2

Prel(l̂|s, e1, e2;θR),

where θ = θE
∪

θR is the joint model parameter,
and Pent, Prel are in Equation 1 and 2.

The objective of MRT is to minimize the fol-
lowing expected loss (i.e., risk),

Eŷ∼P (ŷ|s;θ)∆(ŷ,y) =
∑

ŷ∈Y(s)

P (ŷ|s;θ)∆(ŷ,y),

(3)

where ∆(ŷ,y) is a (arbitrary) loss function de-
scribing the difference between ŷ and y.

In our model, the loss function ∆(ŷ,y) is the
key factor to enhance the joint extraction perfor-
mances. First, in ∆(ŷ,y), we consider sentence-
level F1 scores of entity and relation extraction re-
sults (denoted by Fent(Ê , E), Frel(R̂,R)). Specif-
ically, we use 1 − Fent(Ê , E) and 1 − Frel(R̂,R)
as the metric of the entity loss and the relation loss
respectively. On the one hand, F1 scores charac-
terize the overall performance of the outputs and
make the training objective be consistent with the
testing time evaluation metric. On the other hand,
F1 scores cannot be decomposed onto local pre-
dictions of Ê and R̂ like the log losses in Lent and
Lrel, thus we need a different training algorithm.

Second, different from previous applications of
MRT on single tasks (Xu et al., 2016; Shen et al.,
2016), we have two sources of losses in the joint
extraction. By integrating losses of individual
tasks in the learning algorithm, the entity model
could forecast how plausible a candidate entity is
according to the relation model, and the relation

Algorithm 1 The Sampling Algorithm
Input: Entity model θE , relation model θR, sentence s, the

sample size K
Output: A subset Y ′(s) of Y(s)
1: Y ′(s)← {(E ,R)} // add the ground truth
2: while |Y ′(s)| ≤ K do
3: i← 1
4: while i ≤ |s| do
5: with prob. 0.9, sample t′i ∼ Pent(·|s;θE)
6: with prob. 0.1, sample t′i uniformly
7: i← i+ 1
8: end while
9: E ′ ← t′ = t′1, t′2, · · · , t′|s|

10: R′ ← ∅
11: for e1, e2 ∈ Ê , e1 ̸= e2 do
12: sample l′ ∼ Prel(·|s, e1, e2;θR)
13: R′ ←R′ ∪ {(e1, e2, l′)}
14: end for
15: Y ′(s)← Y ′(s) ∪ {(E ′,R′)}
16: end while

model could also know the confidence of the en-
tity extraction results. Here, we define a global
loss by adding losses of the two models,

∆E+R(ŷ,y) = 1−
1

2
[Fent(Ê , E) + Frel(R̂,R)].

To compare with ∆E+R, we also try two al-
ternatives of ∆(ŷ,y) in experiments, namely,
∆E(ŷ,y) = 1 − Fent(Ê , E) and ∆R(ŷ,y) =
1− Frel(R̂,R). They only look one model’s loss.

Third, in addition to handcrafted loss functions,
we further ask whether the joint MRT model could
benefit from automatic “loss engineering”. Specif-
ically, let Γ(ŷ) be the loss learned from the train-
ing set, we augment ∆(ŷ,y) of the MRT objec-
tive with Γ(ŷ), and require the learning process
to assign a smaller Γ value (with a margin) to the
ground truth output y than other ŷ ∈ Y \{y},

min .
∑

ŷ∈Y(s)

P (ŷ|s;θ) (∆(ŷ,y) + Γ(ŷ)) + ξ

s.t. Γ(y∗)− Γ(y) ≥ 1− ξ, ξ ≥ 0, (4)

where y∗ = argminŷ∈Y (s) Γ(ŷ). Here, we sim-
ply set Γ(ŷ) = 1 − P (ŷ|s;θ) 4 and reformulate
above objective as∑

ŷ∈Y(s)

P (ŷ|s;θ) (∆(ŷ,y)− P (ŷ|s;θ))

+ [1− P (y|s;θ) + P (y∗|s;θ)]+ . (5)

where [u]+ = max(u, 0) is the hinge loss.
Optimizing the expected loss is hard since the

size of Y(s) is exponential. In practice, we could
4Further study on different Γ(ŷ) is left for future work.



2261

approximate the expectation in Equation 3 by sam-
pling a tractable subset Y ′(s) of Y(s). Specifi-
cally, we first obtain an entity set E ′ by sampling
(without replacement) an entity tag sequence t′

from Pent. 5 Then based on the sampled entities,
we get a relation set R′ by sampling l′ from Prel
for each entity pairs. Algorithm 1 lists the pseudo
code. 6 In experiments, we also try a variant of
Algorithm 1 which only samples from the entity
model, and selects relation labels with the maxi-
mum posterior (i.e., doesn’t sample relations).

With the sampled subset Y ′(s), we consider a
revised version of the original MRT objective,

Lmrt(θ) =
∑

ŷ∈Y ′(s)

Q(ŷ|s;θ, µ, α)∆(ŷ,y), (6)

where Q(ŷ|s;θ, µ, α) is a re-normalization of
P (ŷ|s;θ) on the subset Y ′(s), 7

Q(ŷ|s;θ, µ, α) = 1Z [P (Ê |s,θE)
µP (R̂|s, Ê ,θR)1−µ]α

Z =
∑

(E ′,R′)∈Y ′(s)

[P (E ′|s,θE)µP (R′|s, E ′,θR)1−µ]α

The hyper-parameter α controls the sharpness of
the Q distribution (Och, 2003), and µ weights the
importance of the entity model and the relation
model in Q. Similary, we can rewrite the objec-
tive in Equation 5 with Y ′(s) and Q.

Finally, we remark that if we view MRT as a
fine tuning step, it can be applied in any joint
learning model based on building the joint distri-
bution P (ŷ|s,θ) (e.g., the globally normalized P
in (Zhang et al., 2017)). Thus, we would think
that MRT is a flexible and lightweight framework
for the joint learning.

3.4 Training
To train the joint extraction model, we first pre-
train the model with objective L̃ (i.e., minimize
the local loss), then optimize the local loss and
the global loss simultaneously with objective L̃+
Lmrt. The setting is slightly different from previ-
ous work which only optimize Lmrt in the second
step. We find that adding L̃ in the experiments
could make the training more stable.

5To accelerate sampling, we borrow the idea of ε-greedy
in reinforcement learning: with probability 0.9, we sample t′i
from Pent, and with probability 0.1, we sample it uniformly.

6The time complexity is O(K|s|) which is the same to
the beam search algorithm with beam size K (Zhang et al.,
2017).

7Here we follow the literature of MRT to apply the re-
normalization on Y ′(s). Another formulation is the policy
gradient framework which sticks to the original probability.

When training with L̃ in the pre-training step,
we apply the scheduled sampling strategy (Ben-
gio et al., 2015) in the entity model as (Miwa
and Bansal, 2016). Models are regularized with
dropout and trained using Adadelta (Zeiler, 2012).
We give the full derivation of Equation 6’s gradi-
ent in the supplementary. 8

We select models using development sets:
within a fix number of epochs, the model with the
best relation extraction performance on the devel-
opment set is picked out for testing. 9

4 Experiments

We evaluate the proposed model on two datasets.
ACE05 is a standard corpus for the entity relation
extraction task. It is labelled with 7 entity types
and 6 relation types. We use the same split of
ACE05 documents as previous work (351 train-
ing, 80 development, and 80 testing). 10 NYT
(Riedel et al., 2010) is a larger corpus which is la-
belled with 3 entity types and 24 relation types. 11

The training set has 353k relation triples which are
generated by distant supervision. It also provides
another 3880 manually labelled relation triples.
Following (Ren et al., 2017; Zheng et al., 2017),
we exclude the None relation label and randomly
select 10% of the labelled data as the develop-
ment set. We will mainly discuss the results on
ACE05 where many previous joint learning mod-
els are available for comparison.

We list detailed hyper-parameter settings in the
supplementary. Note that, except µ, α,K which
are introduced in the joint MRT and selected on
the development set, 12 we don’t tune hyper-
parameters extensively. For example, we use the
same setting in both ACE 05 and NYT rather than
tune parameters on each of them.

As previous work, we evaluate performances
8We remark that the MRT objective (Equation 6) is dif-

ferentiable with respect to model parameters (Shen et al.,
2016). The non-decomposability of the F1 score does not
make the model non-differentiable. In our implementation,
the gradient is automatically calculated using autograd tools.
Please see the supplementary for more details.

9We focus on the performance of the ent-to-end relation
extraction, so we select models by the relation extraction re-
sults. It is also possible to consider both the performances of
the entity model and the relation model. We leave the study
of advanced model selection algorithms for future work.

10We use the dataset in https://github.com/tticoin/LSTM-
ER, which is from (Miwa and Bansal, 2016).

11https://github.com/shanzhenren/CoType.
12The default setting is α = 10−4, µ = 1.0,K = 3

in systems without self-learned Γ loss and α = 1, µ =
1.0,K = 2 in systems with Γ loss.



2262

Model Entity Relation
P R F P R F

L&J (2014) 85.2 76.9 80.8 65.4 39.8 49.5
M&B (2016) 82.9 83.9 83.4 57.2 54.0 55.6
Zhang (2017) - - 83.5 - - 57.5
K&C (2017) 84.0 81.3 82.6 55.5 51.8 53.6
NN 84.0 82.9 83.4 59.5 56.3 57.8
MRT 83.9 83.2 83.6 64.9 55.1 59.6

Table 1: Results on the ACE05 test data. (Miwa and Bansal,
2016) and (Katiyar and Cardie, 2017) are joint training sys-
tems without joint decoding. (Li and Ji, 2014) and (Zhang
et al., 2017) are joint decoding algorithms. NN is our neu-
ral network model without minimum risk training. MRT is
minimum risk training with loss Γ (Equation 5). We omit
pipeline methods which underperform joint models (see (Li
and Ji, 2014) for details).

using precision (P), recall (R) and F1 scores.
Specifically, an output entity e is correct if its type
and the region of its head are correct, and an out-
put relation r is correct if its e1, e2, l are correct
(i.e., “exact match”).

4.1 Results on ACE05

We first compare proposed models with previous
work (Table 1). In general, our plain neural net-
work model (NN) is competitive, and after compil-
ing with MRT, it achieves non-negligible improve-
ment over existing state-of-the-art systems. (both
on the entity and the relation extraction). 13 We
have following two detailed comparisons.

Among systems which only rely on shared pa-
rameters ((Miwa and Bansal, 2016; Katiyar and
Cardie, 2017) and NN), NN gives the best result
(we give detailed results on different relation types
in the supplement). One possible reason is that
the “RNN+CNN” network structure is not fully
explored in previous joint learning models. More
importantly, it suggests that how to build powerful
sub-models and utilize shared parameters are still
among the key problems of the task.

Comparing with the best joint decoding sys-
tem which adopts global normalization in train-
ing (Zhang et al., 2017), MRT mainly improves
the relation extraction results. We think that the
improvement may come from the sentence-level
loss applied in MRT: both systems consider inter-
actions between decoders, and both objectives are
approximated by sampling, but MRT optimizes F1
score while Zhang et al. (2017) optimize label ac-

13It is worth noting that our models don’t access addi-
tional linguistic resources such as POS tags and dependency
trees. We have tried to add syntactic features in (Zhang et al.,
2017), but didn’t observe improvements.

Settings F1 of Entity F1 of Relation

Default
sampling

∆E 83.8 +0.4 57.9 +0.1
∆R 83.5 +0.1 58.9 +1.1
∆E+R 83.6 +0.2 59.0 +1.2
Γ 83.6 +0.2 58.3 +0.5
Γ +∆E+R 83.6 +0.2 59.6 +1.8

Only
sampling
entity

∆E 83.7 +0.3 57.4 -0.4
∆R 83.5 +0.1 59.1 +1.3
∆E+R 83.6 +0.2 57.9 +0.1
Γ 83.6 +0.2 58.7 +0.9
Γ +∆E+R 83.3 -0.1 59.2 +1.4

Table 2: MRT with different loss functions and sampling
methods. The numbers in subscripts indicate improvements
over the NN setting in Table 1.

curacy. For the joint decoding system in (Li and
Ji, 2014), although it cannot beat recent neural
network-based models, it is interesting to compare
MRT with a feature-enriched version of (Li and Ji,
2014)’s model in the future work.

Next, we evaluate the joint MRT with different
loss functions and sampling methods.

As mentioned in Section 3.3, we have three op-
tions (∆E+R, ∆E , ∆R) for ∆(ŷ,y) and a self-
learned loss function Γ. The first five rows of Ta-
ble 2 show their performances on the test data. We
have three observations regarding the results.

1. ∆R,∆E+R have higher relation F1 scores than
∆E and NN. Thus, adding relation loss in ∆(ŷ,y)
is helpful for relation extraction. We think that
knowing the relation loss could bias the entity
model to highlight the entities appearing in rela-
tions, which provides a better candidate relation
set for the relation extraction model.

2. ∆E has the best entity extraction results, which
implies that the sentence-level entity loss alone
could benefit entity extraction. While after adding
relation loss (∆E+R), the entity performance
slightly decreases. One reason might be that our
model selection strategy only focuses on the re-
lation part (footnote 9), thus the model with im-
proved entity performances may not be selected.

3. The learned loss Γ can help to improve perfor-
mances, but only using Γ is not as effective as
the handcrafted ∆ functions (which are tailored
to the evaluation metrics). By combining both the
prior knowledge and information from the dataset,
Γ +∆E+R achieves the best results.

Regarding the sampling method, we test a vari-
ant of Algorithm 1 which samples entities but not



2263

relations (the last five rows of Table 2). Comparing
with the default sampling algorithm, it has similar
entity extraction performances, but its behaviour
on the relation extraction is different. Specifically,
adding entity loss in ∆(ŷ,y) (i.e., ∆E ,∆E+R)
now affects relation results negatively. It may sug-
gest that when only exploring the output of entity
extraction, the entity loss may dominate the rela-
tion loss, and trap the joint model to exploit the
entity model only. On the other hand, the perfor-
mances of self-learned loss Γ are less sensitive to
the sampling method. We haven’t had a clear un-
derstanding of the relationship between sampling
algorithms and loss functions, but the above re-
sults show that adding data-related loss function
could improve the robustness of MRT in practice.

Thirdly, we present influences of hyper-
parameters for MRT with ∆E+R on the develop-
ment set in Figure 3 and 4 (other settings have sim-
ilar results). We find that, for the parameters ex-
amined here, it is hard for the entity model and the
relation model to agree with each other: parame-
ters achieving high relation performances usually
get low entity performances, and vise versa. Thus,
if we perform the model selection by only look-
ing at relation extraction results, the joint model
may sacrifice entity extraction performances. For
α and µ (Figure 3), we observe that on the ACE05
dataset, the model prefers a small α (which means
a sharper Q) and µ at boundary (i.e., Q is either
close to the entity model or the relation model).
Regarding the sample size K (Figure 4), we don’t
observe a convergence of performances in a small
range of K. Since the computation cost increases
rapidly as we increase the sample size (K = 5 is
about 2x slower than K = 3 in our implementa-
tion), we stick to a small K.

Finally, due to lack of space, we provide more
discussions on model configurations (including re-
sults regarding different entity pair distances, ad-
ditional experiments on tuning hyper parameters
etc.), and detailed error analyses on concrete sam-
ples in the supplementary.

4.2 Results on NYT

We briefly list results on the NYT dataset in Ta-
ble 3. The baseline methods are (Ren et al., 2017)
which is based on a joint embedding of entities
and relations, and (Zheng et al., 2017) which con-
ducts joint decoding with an augmented sequence
labelling tag set. Both NN and MRT outperform

E R
1e+00

81.3

82.1
E R
1e-01

E R
1e-02

E R
1e-03

E R
1e-04

E R
1e-05

81.3

82.1

81.3

82.1

81.3

82.1

81.3

82.1

81.3

82.1

81.3

82.1

52.1

53.2

   
 0

.0
   

52.1

53.2

   
 0

.1
   

52.1

53.2

   
 0

.3
   

52.1

53.2

   
 0

.5
   

52.1

53.2

   
 0

.7
   

52.1

53.2

   
 0

.9
   

52.1

53.2

   
 1

.0
   

Figure 3: MRT with different Q distributions on the devel-
opment set. Rows are settings of µ, and columns are settings
of α. In each cell, we draw F1 scores of the entity extraction
(the left gray bar) and the relation extraction (the right dark
bar) under the combination of corresponding α and µ.

2 3 4 5 6
Sample size (K)

81.00

81.25

81.50

81.75

82.00

F1
 s

co
re

Entity
Relation

52.20

52.75

53.30

53.85

54.40

Figure 4: MRT with different sample size K on the devel-
opment set.

baseline results. In particular, comparing with the
joint tagging scheme in (Zheng et al., 2017), MRT
adds no constraint on the relation extraction model
and can explore the large NYT training set more
effectively. At the same time, since the training set
is automatically generated, the global losses ob-
served in MRT are also noisy. Like recent work on
bandit structured prediction (Kreutzer et al., 2017;
Nguyen et al., 2017), the results here suggest that
MRT could be a reasonable choice when the su-
pervision of the joint learning is partial and noisy.

5 Conclusion

We introduced minimum risk training to the task
of joint entity and relation extraction. We showed
that, with a global loss function, MRT could en-
hance the connection between the sub-models.
Extensive experiments on benchmark datasets wit-
ness the effectiveness of the joint MRT.



2264

Model Relation
P R F

(Zheng et al., 2017) 61.5 41.4 49.5
NN 61.8 43.3 50.9
MRT 67.4 42.0 51.7

(Ren et al., 2017) 42.3 51.1 46.3
NN (exact match) 59.4 41.7 49.0
MRT (exact match) 65.2 40.6 50.0

Table 3: Results on the NYT dataset. To compare with (Ren
et al., 2017), we give results under the “exact match” crite-
rion as ACE05. To compare with (Zheng et al., 2017), we
give results which ignore the entity type in the justification of
relations. We use α = 1, µ = 1,K = 2 and ∆E+R + Γ.

Acknowledgement

The authors wish to thank the reviewers for
their helpful comments and suggestions. This
research is (partially) supported by STCSM
(18ZR1411500), NSFC(61673179), Shanghai
Knowledge Service Platform Project (ZF1213),
and Shanghai Key Laboratory of Trustworthy
Computing (07dz22304201604). The corre-
sponding authors are Yuanbin Wu, Man Lan and
Shiliang Sun.

References
Pieter Abbeel and Andrew Y. Ng. 2004. Apprentice-

ship learning via inverse reinforcement learning. In
Machine Learning, Proceedings of the Twenty-first
International Conference (ICML 2004), Banff, Al-
berta, Canada, July 4-8, 2004.

Shiqi Shen Ayana, Zhiyuan Liu, and Maosong Sun.
2016. Neural headline generation with minimum
risk training. arXiv preprint arXiv:1604.01904.

Samy Bengio, Oriol Vinyals, Navdeep Jaitly, and
Noam Shazeer. 2015. Scheduled sampling for se-
quence prediction with recurrent neural networks.
In Advances in Neural Information Processing Sys-
tems, pages 1171–1179.

Yee Seng Chan and Dan Roth. 2011. Exploiting
syntactico-semantic structures for relation extrac-
tion. In Proceedings of the 49th Annual Meeting of
the Association for Computational Linguistics: Hu-
man Language Technologies, pages 551–560, Port-
land, Oregon, USA. Association for Computational
Linguistics.

Kevin Gimpel and Noah A. Smith. 2010. Softmax-
margin crfs: Training log-linear models with cost
functions. In Human Language Technologies: The
2010 Annual Conference of the North American
Chapter of the Association for Computational Lin-
guistics, pages 733–736, Los Angeles, California.
Association for Computational Linguistics.

Xiaodong He and Li Deng. 2012. Maximum expected
bleu training of phrase and lexicon translation mod-
els. In Proceedings of the 50th Annual Meeting of
the Association for Computational Linguistics (Vol-
ume 1: Long Papers), pages 292–301, Jeju Island,
Korea. Association for Computational Linguistics.

Sepp Hochreiter and Jürgen Schmidhuber. 1997.
Long short-term memory. Neural Computation,
9(8):1735–1780.

Zhiheng Huang, Wei Xu, and Kai Yu. 2015. Bidi-
rectional LSTM-CRF models for sequence tagging.
CoRR, abs/1508.01991.

Arzoo Katiyar and Claire Cardie. 2016. Investigating
lstms for joint extraction of opinion entities and rela-
tions. In Proceedings of the 54th Annual Meeting of
the Association for Computational Linguistics (Vol-
ume 1: Long Papers), pages 919–929, Berlin, Ger-
many. Association for Computational Linguistics.

Arzoo Katiyar and Claire Cardie. 2017. Going out
on a limb: Joint extraction of entity mentions and
relations without dependency trees. In Proceed-
ings of the 55th Annual Meeting of the Association
for Computational Linguistics (Volume 1: Long Pa-
pers), pages 917–928, Vancouver, Canada. Associa-
tion for Computational Linguistics.

Julia Kreutzer, Artem Sokolov, and Stefan Riezler.
2017. Bandit structured prediction for neural
sequence-to-sequence learning. In Proceedings of
the 55th Annual Meeting of the Association for Com-
putational Linguistics (Volume 1: Long Papers),
pages 1503–1513, Vancouver, Canada. Association
for Computational Linguistics.

Jiwei Li, Will Monroe, Alan Ritter, Dan Jurafsky,
Michel Galley, and Jianfeng Gao. 2016. Deep rein-
forcement learning for dialogue generation. In Pro-
ceedings of the 2016 Conference on Empirical Meth-
ods in Natural Language Processing, pages 1192–
1202, Austin, Texas. Association for Computational
Linguistics.

Qi Li and Heng Ji. 2014. Incremental joint extrac-
tion of entity mentions and relations. In Proceed-
ings of the 52nd Annual Meeting of the Association
for Computational Linguistics (Volume 1: Long Pa-
pers), pages 402–412, Baltimore, Maryland. Asso-
ciation for Computational Linguistics.

Yankai Lin, Shiqi Shen, Zhiyuan Liu, Huanbo Luan,
and Maosong Sun. 2016. Neural relation extraction
with selective attention over instances. In Proceed-
ings of the 54th Annual Meeting of the Association
for Computational Linguistics (Volume 1: Long Pa-
pers), pages 2124–2133, Berlin, Germany. Associa-
tion for Computational Linguistics.

Makoto Miwa and Mohit Bansal. 2016. End-to-end re-
lation extraction using lstms on sequences and tree
structures. In Proceedings of the 54th Annual Meet-
ing of the Association for Computational Linguistics



2265

(Volume 1: Long Papers), pages 1105–1116, Berlin,
Germany. Association for Computational Linguis-
tics.

Makoto Miwa, Rune Sætre, Yusuke Miyao, and
Jun’ichi Tsujii. 2009. A rich feature vector for
protein-protein interaction extraction from multiple
corpora. In Proceedings of the 2009 Conference
on Empirical Methods in Natural Language Pro-
cessing, pages 121–130, Singapore. Association for
Computational Linguistics.

Khanh Nguyen, Hal Daumé III, and Jordan Boyd-
Graber. 2017. Reinforcement learning for bandit
neural machine translation with simulated human
feedback. In Proceedings of the 2017 Conference on
Empirical Methods in Natural Language Process-
ing, pages 1464–1474, Copenhagen, Denmark. As-
sociation for Computational Linguistics.

Franz Josef Och. 2003. Minimum error rate train-
ing in statistical machine translation. In Proceed-
ings of the 41st Annual Meeting of the Association
for Computational Linguistics, pages 160–167, Sap-
poro, Japan. Association for Computational Linguis-
tics.

Nathan D. Ratliff, J. Andrew Bagnell, and Martin
Zinkevich. 2006. Maximum margin planning. In
Machine Learning, Proceedings of the Twenty-Third
International Conference (ICML 2006), Pittsburgh,
Pennsylvania, USA, June 25-29, 2006, pages 729–
736.

Xiang Ren, Zeqiu Wu, Wenqi He, Meng Qu, Clare R
Voss, Heng Ji, Tarek F Abdelzaher, and Jiawei Han.
2017. Cotype: Joint extraction of typed entities and
relations with knowledge bases. In Proceedings of
the 26th International Conference on World Wide
Web, pages 1015–1024. International World Wide
Web Conferences Steering Committee.

Sebastian Riedel, Limin Yao, and Andrew McCal-
lum. 2010. Modeling relations and their men-
tions without labeled text. In Machine Learning
and Knowledge Discovery in Databases, European
Conference, ECML PKDD 2010, Barcelona, Spain,
September 20-24, 2010, Proceedings, Part III, pages
148–163.

Shiqi Shen, Yong Cheng, Zhongjun He, Wei He, Hua
Wu, Maosong Sun, and Yang Liu. 2016. Minimum
risk training for neural machine translation. In Pro-
ceedings of the 54th Annual Meeting of the Associa-
tion for Computational Linguistics (Volume 1: Long
Papers), pages 1683–1692, Berlin, Germany. Asso-
ciation for Computational Linguistics.

David A. Smith and Jason Eisner. 2006. Minimum risk
annealing for training log-linear models. In Pro-
ceedings of the COLING/ACL 2006 Main Confer-
ence Poster Sessions, pages 787–794, Sydney, Aus-
tralia. Association for Computational Linguistics.

Richard S. Sutton and Andrew G. Barto. 1998. In-
troduction to Reinforcement Learning, 1st edition.
MIT Press, Cambridge, MA, USA.

Wenhui Wang and Baobao Chang. 2016. Graph-based
dependency parsing with bidirectional lstm. In Pro-
ceedings of the 54th Annual Meeting of the Associa-
tion for Computational Linguistics (Volume 1: Long
Papers), pages 2306–2315, Berlin, Germany. Asso-
ciation for Computational Linguistics.

Wenduan Xu, Michael Auli, and Stephen Clark. 2016.
Expected f-measure training for shift-reduce parsing
with recurrent neural networks. In Proceedings of
the 2016 Conference of the North American Chap-
ter of the Association for Computational Linguis-
tics: Human Language Technologies, pages 210–
220, San Diego, California. Association for Com-
putational Linguistics.

Matthew D Zeiler. 2012. Adadelta: an adaptive learn-
ing rate method. arXiv preprint arXiv:1212.5701.

Meishan Zhang, Yue Zhang, and Guohong Fu. 2017.
End-to-end neural relation extraction with global op-
timization. In Proceedings of the 2017 Conference
on Empirical Methods in Natural Language Pro-
cessing, pages 1731–1741, Copenhagen, Denmark.
Association for Computational Linguistics.

Suncong Zheng, Feng Wang, Hongyun Bao, Yuexing
Hao, Peng Zhou, and Bo Xu. 2017. Joint extraction
of entities and relations based on a novel tagging
scheme. In Proceedings of the 55th Annual Meet-
ing of the Association for Computational Linguistics
(Volume 1: Long Papers), pages 1227–1236, Van-
couver, Canada. Association for Computational Lin-
guistics.


