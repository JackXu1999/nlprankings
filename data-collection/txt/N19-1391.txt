



















































Context-Aware Cross-Lingual Mapping


Proceedings of NAACL-HLT 2019, pages 3906–3911
Minneapolis, Minnesota, June 2 - June 7, 2019. c©2019 Association for Computational Linguistics

3906

Context-Aware Cross-Lingual Mapping

Hanan Aldarmaki1 and Mona Diab1,2
1The George Washington University

2AWS, Amazon AI
aldarmaki@gwu.edu,diabmona@amazon.com

Abstract

Cross-lingual word vectors are typically ob-
tained by fitting an orthogonal matrix that
maps the entries of a bilingual dictionary from
a source to a target vector space. Word vectors,
however, are most commonly used for sen-
tence or document-level representations that
are calculated as the weighted average of word
embeddings. In this paper, we propose an
alternative to word-level mapping that bet-
ter reflects sentence-level cross-lingual simi-
larity. We incorporate context in the transfor-
mation matrix by directly mapping the aver-
aged embeddings of aligned sentences in a par-
allel corpus. We also implement cross-lingual
mapping of deep contextualized word embed-
dings using parallel sentences with word align-
ments. In our experiments, both approaches
resulted in cross-lingual sentence embeddings
that outperformed context-independent word
mapping in sentence translation retrieval. Fur-
thermore, the sentence-level transformation
could be used for word-level mapping without
loss in word translation quality.

1 Introduction

Cross-lingual word vector models aim to embed
words from multiple languages into a shared vec-
tor space to enable cross-lingual transfer and dic-
tionary expansion (Upadhyay et al., 2016). One
of the most common and effective approaches for
obtaining bilingual word embeddings is by fitting
a linear transformation matrix on the entries of a
bilingual seed dictionary (Mikolov et al., 2013).
This approach is versatile and scalable: multilin-
gual embeddings can be obtained by mapping the
vector spaces of multiple languages into a shared
target language, typically English. In addition,
imposing an orthogonality constraint on the map-
ping ensures that the original pair-wise distances
are preserved after the transformation and results

in better word translation retrieval (Artetxe et al.,
2016; Smith et al., 2017).

While word vector spaces tend to be globally
consistent across language variations (Aldarmaki
et al., 2018), individual words like homographs
with unrelated senses (e.g. ‘bank’, ‘coast’) and
phrasal verbs (‘stand up’, ‘stand out’) are likely
to behave less consistently in multilingual vector
spaces due to their different usage distributions.
Consequently, using such words in the alignment
dictionary may result in suboptimal overall map-
ping. We propose two approaches to counteract
this effect by incorporating sentential context in
the mapping process without explicit word sense
disambiguation or additional linguistic resources.
The first approach is based on the recently pro-
posed contextualized embeddings from language
models, ELMo (Peters et al., 2018). Using a
parallel corpus with word-alignments, we extract
contextualized embeddings to construct a context-
aware dictionary for mapping. The second ap-
proach is to learn a transformation between sen-
tence embeddings rather than individual word em-
beddings. Since these embeddings include context
that spans full sentences, we surmise that a map-
ping learned at this level would be more robust to
individual word misalignments.

We used a constrained set of parallel sentences
ranging from one hundred to a million sentences
for alignment. We then evaluated the resultant
mappings on sentence translation retrieval among
English, Spanish, and German as test languages.
Our results show that context-aware mappings sig-
nificantly outperform context-independent cross-
lingual word mappings using reasonably-sized
parallel corpora, particularly when using contextu-
alized word embeddings. In addition, when aver-
aging static word embeddings, the sentence-level
mapping can still be used for word-level mapping
without loss in word translation quality.



3907

2 Related Work

For cross-lingual alignment, we follow the popu-
lar approach of fitting a linear transformation ma-
trix between word vector spaces that are indepen-
dently trained for each language. Aligning mono-
lingual word vector spaces using a seed dictio-
nary was originally proposed in Mikolov et al.
(2013). In Artetxe et al. (2016) and Smith et al.
(2017), it was shown that imposing an orthogonal-
ity constraint on the transformation leads to bet-
ter word translation quality. Recently, contextu-
alized word embeddings were proposed, where a
sequential neural network is trained as a language
model and then used to extract context-sensitive
word representations from the hidden states (Pe-
ters et al., 2018). We use parallel text in order
to align independently-trained contextualized em-
beddings across languages. Schuster et al. (2019)
independently proposed a cross-lingual alignment
approach for contextualized embeddings without
the use of parallel text.

3 Approach

3.1 Orthogonal Bilingual Mapping

Given a dictionary of source to target pairs
〈x, y〉 and matrix representations X and Y whose
columns are vector representations of the corre-
sponding dictionary items, we seek to find an or-
thogonal transformation matrix R that minimizes
the distances between the transformed vectors in
RX and Y . Formally,

R = arg min
R̂

‖R̂X − Y ‖ s. t. R̂T R̂ = I (1)

where ‖.‖ denotes the Frobenius norm. The or-
thogonality constraint ensures that pair-wise dis-
tances in the original source vector space are pre-
served after the transformation. As shown in
(Schönemann, 1966), the solution can be found by
singular value decomposition of Y XT

Y XT = UΣV T

Then,
R = UV T (2)

The resultant transformation, R, can then be
used to transform additional vectors in the source
vector space. The quality of the transformation
depends on the size and accuracy of the initial

dictionary, and it is typically evaluated on word
translation precision using nearest neighbor search
(Smith et al., 2017).

3.2 Mapping of Contextualized Embeddings
Word embeddings in a given language tend to have
similar structures as their translations in a tar-
get language (Aldarmaki et al., 2018), which en-
ables orthogonal mappings of word vector spaces
to generalize well across various languages. How-
ever, items in bilingual dictionaries typically re-
fer to specific word senses. In a given dictio-
nary pair, the source word may have multiple
senses that are not consistent with its aligned tar-
get translation (and vise versa), which could re-
sult in suboptimal global mappings. Intuitively,
better mappings could be obtained using sense-
disambiguated word embeddings, which could be
approximated from context. ELMo (Embeddings
from Language Models) is a recently-proposed
deep model for obtaining contextualized word em-
beddings, which are calculated as the hidden states
of a bi-LSTM network trained as a language model
(Peters et al., 2018). The network can be used in
lieu of static word embeddings within other mod-
els, which yields better performance in a range of
tasks, including word sense disambiguation. Sen-
tence embeddings can be obtained from ELMo
by averaging the contextualized word embeddings
(Perone et al., 2018).

Since ELMo generates dynamic, context-
dependent vectors, we cannot use a simple word-
level dictionary to map the model across lan-
guages. Instead, we use a parallel corpus with
word alignments, i.e using an IBM Model (Brown
et al., 1993), to extract a dynamic dictionary of
aligned contextualized word embeddings. De-
pending on the size of the parallel corpus, a large
dictionary can be extracted to learn an orthogonal
mapping as described in Section 3.1, which is then
applied post-hoc on newly generated contextual-
ized embeddings.

3.3 Sentence-Level Mapping
An alternative general approach for obtaining a
context-aware mapping is to learn sentence-level
transformations. Intuitively, a sentence is less am-
biguous than stand-alone words since the words
are interpreted within a specific context, so a map-
ping learned at the sentence-level is likely to be
less sensitive to individual word inconsistencies.
Therefore, we learn the mapping as described in



3908

Section 3.1 using a dictionary of aligned sentence
embeddings. Over a large parallel corpus, the ag-
gregate mapping can yield a more optimal global
solution compared to word-level mapping. This
approach can be applied using any model capable
of generating monolingual sentence embeddings.
In this work, we use the average of word vectors
in each sentence, where the word vectors are either
static or contextualized. For inference, monolin-
gual sentence embeddings are generated first, then
mapped to the target space using the sentence-
level transformation matrix.1

4 Experiments

We used skip-gram with subword information, i.e
FastText (Bojanowski et al., 2017), for the static
word embeddings, and ELMo for contextualized
word embeddings. Sentence embeddings were
calculated from ELMo as the arithmetic average of
the contextualized embeddings 2. For FastText, we
applied weighted averaging using smooth inverse
frequency (Arora et al., 2017), which works better
for sentence similarity compared to other averag-
ing schemes (Aldarmaki and Diab, 2018).

4.1 Data and Processing

We trained and aligned all models using the same
monolingual and parallel datasets. For mono-
lingual training, we used the 1 Billion Word
benchmark (Chelba et al., 2014) for English, and
equivalent subsets of ∼400 million tokens from
WMT’13 (Bojar et al., 2013) news crawl data. We
trained monolingual ELMo and FastText with de-
fault parameters. We used the WMT’13 common-
crawl data for cross-lingual mapping, and the
WMT’13 test sets for evaluating sentence trans-
lation retrieval. For all datasets, the only prepro-
cessing we performed was tokenization.

4.2 Evaluation Framework

We evaluated the cross-lingual mapping ap-
proaches on sentence translation retrieval, where
we calculate the accuracy of retrieving the correct
translation from the target side of a test parallel
corpus using nearest neighbor search with cosine
similarity. To assess the minimum bilingual data

1Since we use vector averaging, it doesn’t matter whether
we apply the learned transformation to the word embeddings
before averaging, or to the sentence embeddings after aver-
aging.

2We found that using the arithmetic average for ELMo
yields better results than weighted averaging.

requirements of each approach and measure how
the various models respond to additional data, we
split the training parallel corpus into smaller sub-
sets of increasing sizes, starting from 100 to a mil-
lion sentences (we double the size at each step).
Data splits and evaluation scripts are available
at https://github.com/h-aldarmaki/
sent_translation_retrieval.

4.3 Alignment Schemes
For ELMo, word embeddings need to be calcu-
lated from context, so we extracted a dictionary
of contextualized words from the parallel cor-
pora by first applying word-level alignments us-
ing Fast Align (Dyer et al., 2013). We then cal-
culated the contextualized embeddings for source
and target sentences, and extracted a dictionary
from the aligned words that have a one-to-one
alignment (i.e. we excluded phrasal alignments).
Since this can result in a very large dictionary,
we capped the number of dictionary words at 1M
for efficiency. For a fair comparison with Fast-
Text word-level mapping, we extracted a dictio-
nary from word alignment probabilities using the
same parallel sets. For each word in the source
language, we extracted its translation as the word
with the maximum alignment probability if the
maximum was unique3. As a baseline, we used
static dictionaries from (Conneau et al., 2017) to
obtain word-level mappings (dict). All align-
ments were performed from the source languages
to English.

4.3.1 Results
Sentence translation retrieval results in all lan-
guage directions are shown in Figure 1 (note the
x-axis denotes the size of the alignment corpus in
log scale). The arrows indicate the translation di-
rection from source to target, with en for English,
es for Spanish, and de for German. For clarity, the
legend shows the average accuracies in the final
step (1M).

Overall, ELMo word alignment resulted in the
highest sentence translation retrieval accuracies,
even with small amounts of training data; it ex-
ceeded the static dictionary baseline at around
2K parallel sentences. Sentence-level mapping
outperformed word-level mapping only when ad-
ditional parallel data were used (over 50K sen-
tences).

3Using other dictionary pairs generally resulted in lower
performance.

 https://github.com/h-aldarmaki/sent_translation_retrieval
 https://github.com/h-aldarmaki/sent_translation_retrieval


3909

1K 10K 100K 1M

.2

.5

.8

1
N

N
ac

cu
ra

cy
es → en

1K 10K 100K 1M

.2

.5

.8

1
de → en

1K 10K 100K 1M

.2

.5

.8

1

N
N

ac
cu

ra
cy

en → es

1K 10K 100K 1M

.2

.5

.8

1 en → de

1K 10K 100K 1M

.2

.5

.8

1

# parallel sentences

N
N

ac
cu

ra
cy

es → de

1K 10K 100K 1M

.2

.5

.8

1

# parallel sentences

de → es

1K 10K 100K 1M

.2

.5

.8

1

# parallel sentences

Average

ELMo (word) 82.23%
ELMo (sent) 84.03%
FastText (word) 74.00%
FastText (sent) 76.92%
FastText (dict) 69.04%

Figure 1: Nearest neighbor sentence translation ac-
curacy as a function of (log) parallel corpus size.
(word) refers to word-level mapping, (sent) to
sentence-level mapping, and (dict) refers to the
baseline (using a static dictionary for mapping). The
legend shows the average accuracies of each model us-
ing 1M parallel sentences.

With 1M sentences, sentence-level mapping of
FastText yielded an increase of ∼3% in all direc-
tions. Sentence-level ELMo underperformed in
the→ en directions until we used 100K sentences,
where we observed a sharp increase in accuracy
compared to the previous step of 50K sentences.
For ELMo, we note particular improvements in
zero-shot translation retrieval between the source
languages: es and de, where ELMo-based models
performed much higher than FastText. The oppo-
site is true for the → en directions, although the
difference is not as notable. This is an interesting

Language pair
Mapping level

word sentence
From source language to en:

es-en
k=1 56.46 54.43
k=5 70.93 68.97

de-en
k=1 50.00] 47.85
k=5 63.45 62.69

From en to source language:

en-es
k=1 56.98 57.52
k=5 72.68 72.15

en-de
k=1 42.32 43.27
k=5 63.99 62.84

Translation between source languages:

de-es
k=1 36.14 37.07
k=5 53.72 54.85

es-de
k=1 31.55 34.22
k=5 51.37 52.07

Average
k=1 45.58 45.73
k=5 62.69 62.26

Table 1: Word translation precision at k (%) using k
nearest neighbor search, with k ∈ {1, 5}.

Language pair
Mapping level

word sentence
en-es 0.6280 0.6362
en-de 0.6480 0.6476
es-de 0.6349 0.6383
Average 0.6370 0.6407

Table 2: The harmonic mean of Pearson and Spearman
correlations with human judgment on the SemEval’17
cross-lingual word similarity task.

observation and may indicate that contextualized
dictionaries result in a more balanced mapping,
while context-independent embeddings overfit the
mapping to the specific direction used for align-
ment.

4.3.2 Word-level Evaluation
Cross-lingual word embeddings are typically eval-
uated in word-translation retrieval: the precision
of correctly retrieving a translation from the vo-
cabulary of another language. Since this is a
context-free task, we evaluated the performance of
static word embeddings, FastText, using word vs.
sentence mapping (with 1M parallel sentences).
The transformation matrix learned at the sentence
level is used to transform the word embeddings.

We used the dictionaries from (Conneau et al.,
2017). We also evaluated on the SemEval’17
cross-lingual word similarity task (Camacho-
Collados et al., 2017), which is measured using
the average of Pearson and Spearman correla-
tion coefficients against human judgements. As
shown in Tables 1 and 2, the mapping learned at



3910

the sentence-level yields equivalent performance
to word-level mapping. While word-level map-
ping was slightly better in translating from source
languages (German and Spanish) to English, the
sentence-level mapping was better when trans-
lating between the source languages. In the
word similarity task, sentence-level mappings per-
formed slightly better in two out of the three cases.
Overall, the performance of both models are com-
parable, which indicates that a single transforma-
tion matrix learned at the sentence-level can be
used for both word and sentence-level tasks.

5 Discussion and Conclusions

We introduced alternatives to the popular word
mapping approach that incorporate context in the
mapping process. Given parallel corpora, context-
aware mappings were learned by mapping aligned
contextualized word embeddings or directly map-
ping the parallel sentence embeddings. Exper-
imental results showed significant gains in sen-
tence translation retrieval using contextualized
mappings compared to context-independent word
mapping. While word-level mappings worked bet-
ter with smaller parallel corpora, the performance
of sentence-level mapping continued to increase
with additional data until it outperformed word-
level mapping. In future work, we will explore
the viability of the sentence mapping approach on
other sentence embedding models.

References
Hanan Aldarmaki and Mona Diab. 2018. Evaluation

of unsupervised compositional representations. Pro-
ceedings of the 27th International Conference on
Computational Linguistics.

Hanan Aldarmaki, Mahesh Mohan, and Mona Diab.
2018. Unsupervised word mapping using structural
similarities in monolingual embeddings. Transac-
tions of the Association of Computational Linguis-
tics, 6.

Sanjeev Arora, Yingyu Liang, and Tengyu Ma. 2017.
A simple but tough-to-beat baseline for sentence em-
beddings.

Mikel Artetxe, Gorka Labaka, and Eneko Agirre. 2016.
Learning principled bilingual mappings of word em-
beddings while preserving monolingual invariance.
In Proceedings of the 2016 Conference on Empiri-
cal Methods in Natural Language Processing.

Piotr Bojanowski, Edouard Grave, Armand Joulin, and
Tomas Mikolov. 2017. Enriching word vectors with

subword information. Transactions of the Associa-
tion for Computational Linguistics, 5:135–146.

Ondrej Bojar, Christian Buck, Chris Callison-Burch,
Christian Federmann, Barry Haddow, Philipp
Koehn, Christof Monz, Matt Post, Radu Soricut, and
Lucia Specia. 2013. Findings of the 2013 workshop
on statistical machine translation. In Proceedings of
the eighth workshop on statistical machine transla-
tion, pages 1–44.

Peter F Brown, Vincent J Della Pietra, Stephen A Della
Pietra, and Robert L Mercer. 1993. The mathemat-
ics of statistical machine translation: Parameter esti-
mation. Computational linguistics, 19(2):263–311.

Jose Camacho-Collados, Mohammad Taher Pilehvar,
Nigel Collier, and Roberto Navigli. 2017. Semeval-
2017 task 2: Multilingual and cross-lingual semantic
word similarity. In Proceedings of the 11th Interna-
tional Workshop on Semantic Evaluation (SemEval-
2017), pages 15–26.

Ciprian Chelba, Tomas Mikolov, Mike Schuster, Qi Ge,
Thorsten Brants, Phillipp Koehn, and Tony Robin-
son. 2014. One billion word benchmark for mea-
suring progress in statistical language modeling. In
Fifteenth Annual Conference of the International
Speech Communication Association.

Alexis Conneau, Guillaume Lample, Marc’Aurelio
Ranzato, Ludovic Denoyer, and Hervé Jégou. 2017.
Word translation without parallel data. arXiv
preprint arXiv:1710.04087.

Chris Dyer, Victor Chahuneau, and Noah A Smith.
2013. A simple, fast, and effective reparameteriza-
tion of ibm model 2. In Proceedings of the 2013
Conference of the North American Chapter of the
Association for Computational Linguistics: Human
Language Technologies, pages 644–648.

Tomas Mikolov, Quoc V Le, and Ilya Sutskever. 2013.
Exploiting similarities among languages for ma-
chine translation. arXiv preprint arXiv:1309.4168.

Christian S Perone, Roberto Silveira, and Thomas S
Paula. 2018. Evaluation of sentence embeddings
in downstream and linguistic probing tasks. arXiv
preprint arXiv:1806.06259.

Matthew Peters, Mark Neumann, Mohit Iyyer, Matt
Gardner, Christopher Clark, Kenton Lee, and Luke
Zettlemoyer. 2018. Deep contextualized word repre-
sentations. In Proceedings of the 2018 Conference
of the North American Chapter of the Association
for Computational Linguistics: Human Language
Technologies, Volume 1 (Long Papers).

Peter H Schönemann. 1966. A generalized solution of
the orthogonal procrustes problem. Psychometrika,
31(1):1–10.

Tal Schuster, Ori Ram, Regina Barzilay, and Amir
Globerson. 2019. Cross-lingual alignment of con-
textual word embeddings, with applications to



3911

zero-shot dependency parsing. arXiv preprint
arXiv:1902.09492.

Samuel L Smith, David HP Turban, Steven Hamblin,
and Nils Y Hammerla. 2017. Offline bilingual word
vectors, orthogonal transformations and the inverted
softmax. arXiv preprint arXiv:1702.03859.

Shyam Upadhyay, Manaal Faruqui, Chris Dyer, and
Dan Roth. 2016. Cross-lingual models of word em-
beddings: An empirical comparison. In Proceed-
ings of the 54th Annual Meeting of the Association
for Computational Linguistics (Volume 1: Long Pa-
pers), volume 1.


