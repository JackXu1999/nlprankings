



















































Training Hybrid Language Models by Marginalizing over Segmentations


Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 1477–1482
Florence, Italy, July 28 - August 2, 2019. c©2019 Association for Computational Linguistics

1477

Training Hybrid Language Models by Marginalizing over Segmentations

Edouard Grave Sainbayar Sukhbaatar Piotr Bojanowski Armand Joulin
Facebook AI Research

{egrave,sainbar,bojanowski,ajoulin}@fb.com

Abstract

In this paper, we study the problem of hy-
brid language modeling, that is using models
which can predict both characters and larger
units such as character ngrams or words. Us-
ing such models, multiple potential segmenta-
tions usually exist for a given string, for ex-
ample one using words and one using charac-
ters only. Thus, the probability of a string is
the sum of the probabilities of all the possi-
ble segmentations. Here, we show how it is
possible to marginalize over the segmentations
efficiently, in order to compute the true prob-
ability of a sequence. We apply our technique
on three datasets, comprising seven languages,
showing improvements over a strong character
level language model.

1 Introduction

Statistical language modeling is the problem of
estimating a probability distribution over text
data (Bahl et al., 1983). Most approaches formu-
late this problem at the word level, by first seg-
menting the text using a fixed vocabulary. A limi-
tation of these methods is that they cannot generate
new words, or process out of vocabulary words. A
popular alternative is to directly model sequences
at the character level. These models can poten-
tially generate any sequence, and are thus some-
times referred to as open vocabulary. However,
they tend to underperform compared to word level
models when trained on the same data.

For these reasons, a few works have proposed
hybrid models, that work both at the character and
word level (or sometimes groups of characters). A
first class of hybrid models switch between word
and character level representations, depending on
whether they predict that the upcoming word is
in the vocabulary or not (Kawakami et al., 2017;
Mielke and Eisner, 2019). For example, a first

model can be trained on tokenized data, where out-
of-vocabulary words are replaced by the <unk>
token. A second model is then used to generate
the character sequences corresponding to out-of-
vocabulary words. Another approach, which does
not require tokenization, is to process groups of
characters, which are obtained based on linguis-
tic knowledge or low level statistics. These in-
clude merging characters using mutual informa-
tion (Mikolov et al., 2012) or the byte pair en-
coding algorithm (Sennrich et al., 2016). This ap-
proach first produces a segmentation for the text,
and then learns a language model on it. However,
some sequences have multiple possible segmenta-
tions, and a model considering a single one might
underestimate the true probability of the sequence.
Thus, it is important to marginalize over the set of
segmentations to obtain the true probability of a
sequence (van Merriënboer et al., 2017; Buckman
and Neubig, 2018).

In this paper, we propose an alternative ap-
proach to address this limitation, and in particu-
lar, to train models by marginalizing over the set
of segmentations. As the number of possible seg-
mentations grows exponentially with the sequence
size, using an efficient algorithm such as dynamic
programming is important. Computing the repre-
sentation of the context at the character level al-
lows to apply dynamic programming to this prob-
lem, without using approximations. This tech-
nique was previously considered in the context of
automatic speech recognition (Wang et al., 2017)
or to copy tokens from the input for code genera-
tion (Ling et al., 2016). We evaluate our method
on three datasets for character level language mod-
eling, showing that adding n-grams to the predic-
tions improve the perplexity of the model.



1478

2 Approach

The goal of character level language modeling is
to learn a probability distribution over sequences
of characters c1, ..., cT . Using the chain rule, such
a distribution can be factorized as the product of
the probability distribution of a character condi-
tioned on its history:

p(c1, ..., cT ) =

T∏
t=1

p(ct | c0, ..., ct−1),

where c0 is a special symbol indicating the begin-
ning of the sequence. In this paper, we learn these
conditional probability distributions using neural
networks. For each time step t, a neural network
builds a representation ht from the history that is
used to predict the upcoming character. This rep-
resentation can be obtained from any architecture,
such as feedforward (Bengio et al., 2003) or re-
current networks (Mikolov et al., 2010). We fo-
cus on the transformer network, recently intro-
duced by Vaswani et al. (2017), because of its high
performance on character level language model-
ing (Dai et al., 2018). We refer to Vaswani et al.
(2017) for the details of this architecture.

2.1 Hybrid language models

Hybrid language models predict multiple tokens,
instead of one, at each time step. One way to per-
form this is to add n-grams to the output vocabu-
lary of the model. Under such models, a charac-
ter sequence has multiple segmentations, and the
model estimates its probability by summing the
probability of all its segmentations. For example,
if the model predicts bigrams in addition to char-
acters, the word dog can be decomposed as

[d], [o], [g] or [do], [g] or [d], [og].

Thus, the probability of the sequence of characters
dog is given by

p(dog) = p(d)× p(o | d)× p(g | do)
+ p(do)× p(g | do) + p(d)× p(og | d).

More formally, let us denote by S(c1:T ) the set
of all possible segmentations of a given sequence
c1:T = c1, ..., cT . Then, the probability of the
character sequence is

p(c1:T ) = p(c1, ..., cT ) =
∑

s∈S(c)

p(s). (1)

The set of all possible segmentations grows expo-
nentially with the sequence size, making it imprac-
tical to evaluate this probability by directly sum-
ming over all segmentations.

2.2 Factorization of the segmentation
probabilities

A segmentation s can be decomposed into a se-
quence s1, ..., sK of consecutive atoms in the vo-
cabulary on which we apply the chain rule to get:

p(s) =

K∏
k=1

p(sk | s0, ..., sk−1).

Using this factorization of the probability distri-
bution, it is not possible to directly apply dynamic
programming to compute the probability of a se-
quence. The reason is that the conditional distri-
bution of symbols depends on the segmentation,
preventing to reuse computation across different
segmentations. For example, previous work pro-
posed to use the segmentation both in the input
and output of the model. The hidden representa-
tions ht of the neural network were thus intrin-
sically linked to the segmentation, preventing to
share computations. A potential workaround is
to merge the different representations correspond-
ing to all the segmentations ending at the same
character, for example by avergaging them (van
Merriënboer et al., 2017; Buckman and Neubig,
2018). In our case, we use n-grams only in the out-
put, making the representations ht independent of
the segmentations, and the application of dynamic
programming straightforward.

To do so, we define the conditional distribu-
tion using characters, instead of the segmentation.
Given a sequence s1, ..., sK of n-grams, we intro-
duce the concatenation operator concat, such that

concat(s1, ..., sK) = c1, ..., cJ

corresponds to the sequence of J characters that
compose the segmentation sequence. For exam-
ple, the two segmentations [do],[g],[s] and
[d],[og],[s] of the word dogs share the same
output from the concat operator:

concat([do], [g], [s]) = d, o, g, s,
concat([d], [og], [s]) = d, o, g, s.

We now define the conditional distribution as

p(sk | s1:k−1) = p(sk | concat(s1:k−1)). (2)



1479

This reformulation is exact under the conditional
independence assumption, i.e., that the symbol at
position t in the character sequence is indepen-
dent of the segmentation, given the characters up
to time t−1. In the next section, we show how, un-
der this assumption, the probability of a sequence
can be computed with dynamic programming.

2.3 Dynamic programming
For this section, we restrict ourselves to predict-
ing characters and bigrams for simplicity. How-
ever, our approach is straightforward to apply to
n-grams or words. Given a sequence of character
c = c1, ..., cT , all segmentations end with either
the character cT or the bigram cT−1cT . More pre-
cisely, we can decompose the probability of c as:

p(c1:T ) =
∑

s∈S(c1:T−1)

p(cT | s)p(s)

+
∑

s∈S(c1:T−2)

p(cT−1cT | s)p(s).

Using the reformulation of the conditional prob-
ability of Eq. (2) under the conditional indepen-
dence assumption on segmentations, we get

p(c1:T ) =
∑

s∈S(c1:T−1)

p(cT | c1:T−1)p(s)

+
∑

s∈S(c1:T−2)

p(cT−1cT | c1:T−2)p(s).

We now move the conditional probabilities out of
the sums:

p(c1:T ) = p(cT | c1:T−1)
∑

s∈S(c1:T−1)

p(s)

+ p(cT−1cT | c1:T−2)
∑

s∈S(c1:T−2)

p(s).

Finally, using Eq. (1), we obtain a recurrence rela-
tion over the sequence probabilities:

p(c1:T ) = p(cT | c1:T−1)p(c1:T−1)
+ p(cT−1cT | c1:T−2)p(c1:T−2).

We can thus optimize over all the possible segmen-
tations using dynamic programing.

2.4 Conditional distribution of symbols
In this section, we briefly describe how to model
the conditional probability distribution of sym-
bols, either characters or ngrams, given the char-

acter history. We learn a character level neural net-
work to encode the context with hidden represen-
tation ht for each character t. The probability dis-
tribution of the next symbol, either a character or
a n-gram, is obtained by taking the softmax over
the full vocabulary, which includes both characters
and longer elements:

p(· | c0, ..., ct−1) = softmax(Wht).

Note that we get only one probability distribution
over n-grams of different lengths.

2.5 Training procedure

We learn the parameters of our model by min-
imizing the negative log-likelihood of the train-
ing data, using the probability introduced in Eq.
(1). We rely on automatic differentiation to com-
pute the gradients, and thus, only need to imple-
ment the forward computation, which relies on
dynamic programming. Empirically, we observed
that training a model from scratch with this objec-
tive is sometimes unstable. We thus consider an al-
ternative training objective, used at the beginning
of training. For each position, this loss is equal
to the sum of the negative log-probabilities of the
n-grams corresponding to that position. More for-
mally, given a sequence of length T , this objective
is equal to

−
T∑
t=1

N−1∑
n=1

log (p(ct:t+n | c1:t−1)) ,

and N is the size of the longest n-grams consid-
ered (we can pad n-grams when t + n > T or
exclude them from this loss).

3 Experiments

In this section, we describe the experiments that
we performed to evaluate our approach on charac-
ter level language modeling.

3.1 Datasets

We consider 3 datasets derived from Wikipedia ar-
ticles, but with different preprocessing.

Text8. The text8 dataset of M. Mahoney1 con-
tains 100 million characters from Wikipedia, and
was preprocessed to only contains the lowercase
letters a-z and nonconsecutive spaces.

1http://mattmahoney.net/dc/textdata



1480

Model Cs De En Es Fi Fr Ru Avg.

HCLM (Kawakami et al., 2017) 2.035 1.641 1.622 1.555 1.796 1.508 1.810 1.710
HCLM cache (Kawakami et al., 2017) 1.984 1.588 1.538 1.498 1.711 1.467 1.761 1.649
Full (Mielke and Eisner, 2019) 2.240 1.618 1.506 1.469 1.896 1.434 1.969 1.733
Full (tok) (Mielke and Eisner, 2019) 1.928 1.465 1.387 1.363 1.751 1.319 1.709 1.560
BPE (Mielke and Eisner, 2019) 1.897 1.455 1.439 1.403 1.685 1.365 1.643 1.555
BPE (tok) (Mielke and Eisner, 2019) 1.856 1.414 1.386 1.362 1.652 1.317 1.598 1.512

Transformer baseline 1.777 1.406 1.393 1.37 1.525 1.34 1.616 1.489
Our approach 1.715 1.352 1.341 1.326 1.445 1.299 1.508 1.426

Table 1: Test set bpc on the MWC dataset. The hyperparameters for our method are chosen on the validation set of
WikiText2. Note that Mielke and Eisner (2019) applied the BPE baseline and their method to both tokenized
and non-tokenized data. All the other methods were applied on non-tokenized data only.

Model Test

BN LSTM (Cooijmans et al., 2016) 1.36
HM LSTM (Chung et al., 2016) 1.29
RHN (Zilly et al., 2017) 1.27
Large mLSTM (Krause et al., 2016) 1.27
12L Transf. (Al-Rfou et al., 2018) 1.18

Transformer baseline 1.176
Our approach 1.156

Table 2: Test set bpc on the text8 dataset.

WikiText2. The WikiText2 dataset was intro-
duced by Merity et al. (2017) with a different pre-
processing from text8: numbers, capital letters
and special characters are kept. The vocabulary
size is 1152.2 We use the raw version of the
dataset, which is tokenized but where rare words
are not replaced by the <unk> token. The train-
ing data contains 10.9 millions characters.

MWC. The multilingual Wikipedia corpus
(MWC) of Kawakami et al. (2017) is very simi-
lar in size and preprocessing as WikiText2,
but contains documents in 7 languages: Czech
(cs), German (de), English (en), Spanish (es),
Finnish (fi), French (fr) and Russian (ru).
Unlike Wikitext2, the MWC dataset is not
tokenized. The training sets range from 6.1M
characters for Czech to 15.6M characters for
English, and we refer the reader to Kawakami
et al. (2017) for detailed statistics on this corpus.3

2As opposed to previous work, we keep all characters that
appears in the train, validation or test splits of the data.

3Again, we keep all characters that appears in the data.

Model Test

HCLM (Kawakami et al., 2017) 1.670
HCLM cache (Kawakami et al., 2017) 1.500
BPE (Mielke and Eisner, 2019) 1.468
Full (Mielke and Eisner, 2019) 1.455

Transformer baseline 1.417
Our approach 1.366

Table 3: Test set bpc on the WikiText2 dataset.

3.2 Technical details
Following recent work on character language
modeling with transformers, we use a model with
12 layers of dimension 512, and 4 attention heads.
We use a feedforward block of dimension 2048
for MWC and WikiText2, and 3072 for text8.
We set the attention length to 512, and the batch
size to 8. We use Adagrad (Duchi et al., 2011)
to learn the parameters of our models. Follow-
ing Vaswani et al. (2017), we start with a learn-
ing rate of 0, increase it linearly for k timesteps,
then keep it constant, before halving it at every
epochs for the last 10 epochs. We use a learn-
ing rate of 0.04 and warmup of 16k steps for the
text8 dataset, and a learning rate of 0.025 and
warmup of 8k steps for the WikiText2 and MWC
datasets. In order to have an efficient model at
inference time, we use the caching mechanism
from Dai et al. (2018) to store the hidden repre-
sentations of the previous batch, as well as rel-
ative position weights. We pick a dropout rate
in the set {0.1, 0.2, 0.3, 0.4, 0.5}, using the vali-
dation set. In the experiments, we use n-grams
of size up to 4, excluding n-grams appearing less
than 200 times (1000 times on text8) to limit the



1481

size of the vocabulary. Thus, segmentations which
contain out-of-vocabulary n-grams have a proba-
bility equal to zero.

3.3 Results
In Table 1, we report results on the MWC
dataset, comparing our approach to the models of
Kawakami et al. (2017) and Mielke and Eisner
(2019). Our approach significantly improves the
state of the art on this dataset. Some of the gain is
due to the change of architecture for a transformer.
However, we observe that marginalizing over seg-
mentations also improves over the character level
transformer baseline, showing the benefits of our
method. Finally, as opposed to Mielke and Eisner
(2019), our approach does not need to tokenize the
data to perform well on this dataset.

In Table 2 and Table 3, we report results on the
text8 and wikitext2 datasets respectively.
As for the MWC dataset, our approach significantly
improves the perplexity compared to our character
level transformer baseline. Note that the state of
the art on text8 is 1.08 bpc on the test set with
a 24-layer transformer network (Dai et al., 2018).
This model is significantly larger than ours, con-
taining almost 8 times more parameters.

4 Conclusion

In this paper, we study the problem of hybrid
language modeling, where models can predict n-
grams, instead of unigrams only. A technical chal-
lenge for learning these models is that a given
string can have multiple segmentations, and one
needs to marginalize over the set of segmentations.
We introduce a simple technique to do so, allow-
ing to apply dynamic programming for learning
and inference. Using this approach, we improve
the state of the art on the MWC and WikiText2
datasets, used to evaluate hybrid language models.

Acknowledgements
We thank the anonymous reviewers for their help-
ful comments.

References
Rami Al-Rfou, Dokook Choe, Noah Constant, Mandy

Guo, and Llion Jones. 2018. Character-level lan-
guage modeling with deeper self-attention. arXiv
preprint arXiv:1808.04444.

Lalit R Bahl, Frederick Jelinek, and Robert L Mercer.
1983. A maximum likelihood approach to continu-

ous speech recognition. IEEE Transactions on Pat-
tern Analysis & Machine Intelligence, (2):179–190.

Yoshua Bengio, Réjean Ducharme, Pascal Vincent, and
Christian Jauvin. 2003. A neural probabilistic lan-
guage model. JMLR.

Jacob Buckman and Graham Neubig. 2018. Neural lat-
tice language models. TACL.

Junyoung Chung, Sungjin Ahn, and Yoshua Bengio.
2016. Hierarchical multiscale recurrent neural net-
works. arXiv preprint arXiv:1609.01704.

Tim Cooijmans, Nicolas Ballas, César Laurent, Çağlar
Gülçehre, and Aaron Courville. 2016. Re-
current batch normalization. arXiv preprint
arXiv:1603.09025.

Zihang Dai, Zhilin Yang, Yiming Yang, William W
Cohen, Jaime Carbonell, Quoc V Le, and Ruslan
Salakhutdinov. 2018. Transformer-xl: Language
modeling with longer-term dependency.

John Duchi, Elad Hazan, and Yoram Singer. 2011.
Adaptive subgradient methods for online learning
and stochastic optimization. JMLR.

Kazuya Kawakami, Chris Dyer, and Phil Blunsom.
2017. Learning to create and reuse words in open-
vocabulary neural language modeling. In ACL.

Ben Krause, Liang Lu, Iain Murray, and Steve Renals.
2016. Multiplicative lstm for sequence modelling.
arXiv preprint arXiv:1609.07959.

Wang Ling, Edward Grefenstette, Karl Moritz Her-
mann, Tomáš Kočiskỳ, Andrew Senior, Fumin
Wang, and Phil Blunsom. 2016. Latent predic-
tor networks for code generation. arXiv preprint
arXiv:1603.06744.

Stephen Merity, Caiming Xiong, James Bradbury, and
Richard Socher. 2017. Pointer sentinel mixture
models. In ICLR.

Bart van Merriënboer, Amartya Sanyal, Hugo
Larochelle, and Yoshua Bengio. 2017. Multiscale
sequence modeling with a learned dictionary. arXiv
preprint arXiv:1707.00762.

Sebastian J Mielke and Jason Eisner. 2019. Spell once,
summon anywhere: A two-level open-vocabulary
language model. In AAAI.

Tomáš Mikolov, Martin Karafiát, Lukáš Burget, Jan
Černockỳ, and Sanjeev Khudanpur. 2010. Recurrent
neural network based language model. In Eleventh
annual conference of the international speech com-
munication association.

Tomáš Mikolov, Ilya Sutskever, Anoop Deoras, Hai-
Son Le, Stefan Kombrink, and Jan Cernocky.
2012. Subword language modeling with neu-
ral networks. preprint (http://www. fit. vutbr.
cz/imikolov/rnnlm/char. pdf), 8.



1482

Rico Sennrich, Barry Haddow, and Alexandra Birch.
2016. Neural machine translation of rare words with
subword units. In ACL.

Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob
Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz
Kaiser, and Illia Polosukhin. 2017. Attention is all
you need. In NIPS.

Chong Wang, Yining Wang, Po-Sen Huang, Abdel-
rahman Mohamed, Dengyong Zhou, and Li Deng.
2017. Sequence modeling via segmentations. In
ICML.

Julian Georg Zilly, Rupesh Kumar Srivastava, Jan
Koutnı́k, and Jürgen Schmidhuber. 2017. Recurrent
highway networks. In ICML.


