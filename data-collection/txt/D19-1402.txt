



















































ProSeqo: Projection Sequence Networks for On-Device Text Classification


Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing
and the 9th International Joint Conference on Natural Language Processing, pages 3894–3903,
Hong Kong, China, November 3–7, 2019. c©2019 Association for Computational Linguistics

3894

ProSeqo: Projection Sequence Networks for On-Device Text Classification

Zornitsa Kozareva
Google

Mountain View, CA, USA
zornitsa@kozareva.com

Sujith Ravi
Google Research

Mountain View, CA, USA
sravi@google.com

Abstract
We propose a novel on-device sequence model
for text classification using recurrent projec-
tions. Our model ProSeqo uses dynamic re-
current projections without the need to store or
look up any pre-trained embeddings. This re-
sults in fast and compact neural networks that
can perform on-device inference for complex
short and long text classification tasks.

We conducted exhaustive evaluation on multi-
ple text classification tasks. Results show that
ProSeqo outperformed state-of-the-art neural
and on-device approaches for short text clas-
sification tasks such as dialog act and intent
prediction. To the best of our knowledge,
ProSeqo is the first on-device long text clas-
sification neural model. It achieved compara-
ble results to previous neural approaches for
news article, answers and product categoriza-
tion, while preserving small memory footprint
and maintaining high accuracy.

1 Introduction

In the last decade, research in deep neural net-
works has lead to tremendous advances and state-
of-the-art performance on wide range of Natu-
ral Language Processing (NLP), speech and vi-
sion applications. Coupled with the tremendous
growth and adoption of smart devices such as mo-
bile phones, watches, Internet of Things, conver-
sational AI devices like Alexa and Google Home,
these deep learning improvements have resulted
in text and speech becoming the primary modes
of communication (Cohen, 2008). To date, the
majority of the models run on the server side by
taking user input, sending the data to the server
(or cloud) for processing and returning the results
back to the user. This often results in latency de-
lays, inconsistent user experiences and concerns
over user privacy.

One way to surmount these challenges is to de-
velop and deploy text and speech models that run

inference entirely on-device and return accurate
predictions in real-time. To make this work, the
on-device models have to be very small in size
(few kilobytes or megabytes) to fit on-devices like
mobile phone, watch and IoT; to have low latency
and be as accurate as server side models. These
on-device challenges opened up new active area
of research, which recently has shown promising
results for speech (Lin et al., 2018), wake word
detection (He et al., 2017), dialog act (Ravi and
Kozareva, 2018) and intent prediction short text
classification (Ravi and Kozareva, 2019).

Previous attempts for on-device text classifica-
tion used hashed<input text, output class> pairs.
Unfortunately, such models cannot handle exam-
ples that are not part of the lookup table and
cannot generalize to more complex tasks like the
ones we solve in this paper – long text classi-
fication, conversational intent prediction. (Bui
et al., 2018) combined graphs with neural net-
works to improve the robustness of the model,
but this resulted in large model sizes that can-
not fit on-device. The most successful work is
the self-governing neural network (SGNN) from
(Ravi and Kozareva, 2018) and (SGNN++) from
(Ravi and Kozareva, 2019), which use locality-
sensitive projections (Ravi, 2017, 2019) to encode
short text into low-dimensional representations.
SGNN reached state-of-the-art results on dialog
acts outperforming recurrent networks. While suc-
cessful, SGNN’s static projections make it hard to
generalize to longer sequences.

In this work, we take one step further by de-
veloping a novel embedding free on-device neu-
ral model that combines dynamic projections with
recurrent operations to learn powerful representa-
tions that can more powerfully encode text repre-
sentations and can be equally efficient at short and
long text classification tasks. The main contribu-
tions of our work are:



3895

• We propose novel on-device dynamic projec-
tion sequence network ProSeqo for short and
long text classification; To the best of our
knowledge, this is the first on-device neural
network for long text classification;

• Unlike SGNN which uses static projections
and fully connected networks, ProSeqo is
the first work to introduce embedding-free
LSTMs that combine dynamic projections
with recurrent operations to learn powerful
yet compact neural networks applicable for
on-device scenarios;

• We conduct exhaustive evaluations and com-
parisons against state-of-the-art on-device
and deep learning models for short and long
text classification;

• Our results show that ProSeqo outperformed
state-of-the-art on-device SGNN network
(Ravi and Kozareva, 2018) with up to +8.9%
accuracy on short text classification and
+35.9% accuracy on long document classifi-
cation. This shows that ProSeqo’s dynamic
recurrent projections are more powerful than
the static ones used in SGNN;

• Similarly, our results show that ProSeqo out-
performed the non-on-device neural models
like RNN (Khanpour et al., 2016), CNN (Lee
and Dernoncourt, 2016) for dialog acts and
intent prediction; as well as LSTMs (Zhang
et al., 2015), character CNNs (Zhang et al.,
2015; Bui et al., 2018) for long document
classification of news, answers and product
reviews. This is a significant advancement
given ProSeqo’s small model size compared
to these widely used approaches;

• Finally, we conduct a series of ablation stud-
ies to demonstrate the effect of ProSeqo’s
projection size on accuracy and the ability of
ProSeqo to produce small model sizes while
retaining high accuracy.

2 ProSeqo: Projection Sequence
Networks for On-device Text
Classification

We introduce a neural architecture, ProSeqo, a
projection-based sequence network, that is de-
signed to (a) be efficient and learn compact
embedding-free neural representations of text at

the semantic level, (b) capture contextual and mor-
phological information at the character-level, and
(c) model longer context across words and sen-
tences that is suited for both short and long-text
classification tasks.

The key differences to prior work (e.g., RNNs,
CNNs) are that we use context information to dy-
namically compute text representations and learn
compact neural networks for text classification
that are suited for on-device applications. Our
work departs significantly from recent on-device
work (Ravi and Kozareva, 2018) and (Ravi and
Kozareva, 2019) which leverage projection oper-
ations to learn efficient networks that can even be
transferred to other tasks (Sankar et al., 2019b).
However, they are limited to short-text classifica-
tion tasks. In contrast, our model effectively uses
contextual information and combines recurrent
and projection operations to achieve efficiency and
enable learning more powerful neural networks
that generalize well and can solve more complex
language classification tasks. As a result, our pro-
jection sequence network is able to dynamically
project and learn efficient neural classifier mod-
els that are competitive with state-of-the-art RNNs
and CNNs without the need to store or lookup any
pre-trained embeddings.

2.1 Model Overview

The overall architecture of ProSeqo is shown in
Figure 1. It consists of multiple parts: a word con-
text projector, a recurrent projector, a projection
sequence encoder, and a classification layer. Next,
we describe in details each component.

2.2 Word Context Projector

Given an input text xi (sentence or document) with
sequence of words wit, where wit refers to t-th
word in the input, we first project the input xi to a
vector representation P(xi). Majority of the neural
models used in NLP rely on embedding matrices
(word or character level) to achieve this. Instead,
we dynamically compute word vector representa-
tions using context and locality-sensitive projec-
tions (Ravi, 2017).

2.2.1 Character and Morphology Projection
Each word contains characters and morphological
information that can be informative for the task.
For example, the words amaze and amazing,
may appear slightly different but capture simi-
lar semantics for a review classification task. It



3896

Figure 1: Model architecture for ProSeqo: On-device Projection Sequence Neural Networks.

is particularly challenging for NLP tasks to han-
dle large vocabularies and model unknown words
missing from training data. Previous methods
have proposed to overcome this by relying on
character-level embeddings and other neural mod-
els like character-CNNs. However, these methods
are often complex and slow to compute for long
text (e.g., convolution kernels on devices with-
out significant computational capacity) and still
require explicitly storing character or sub-word se-
quences. For example, even simple character tri-
grams over 26 characters in the English alphabet
requires storing and looking up V = 263 = 18K
entries in character embedding table and complex-
ity of O(V · d), where d is the dimensionality of
the embedding. For word embeddings, V can be
as large as 100K to millions of entries.

We extract character-level sequence informa-
tion (i.e., character sequences) and use locality-
sensitive projections (Ravi, 2017, 2019) to dy-
namically project them to a vector representation
PChar(wit) with just O(T · d) complexity, where
T << V .

PChar(wit) = P(witc1...C ) (1)
= P(−→wit) (2)

where, witc1...C is the sequence of characters c ∈
[1, C] in word wit and −→wit is a sparse vector com-
prised of character-level features (character skip-
grams) extracted from the word wit. We can
also extend this to include other morphological
(e.g., stemming) or syntax features (e.g., Part-

of-Speech) but this adds further complexity and
requires lexicons or further pre-processing steps
(e.g., stemmers, taggers) which may not be avail-
able on device during inference. Hence, we only
use simple raw features like character skip-grams
in the projector.

P(−→wit) = LSHproj(−→wit,
−−→
PT,d) (3)

= [ sgn(−→wit ·
−−→
P1,1), ..., (4)

..., sgn(−→wit ·
−−→
PT,d) ]

We then apply LSH projections that dynam-
ically compute locality-sensitive projections and
transform the extracted sparse vector −→wit (char-
acter sequence features) into a compact, low-
dimensional binary representation P(−→wit). The
projection functions

−→
P used to compute the binary

vector are dynamically generated from observed
features in −→wit and a random seed. T, d represents
the number of projection functions and dimenson-
ality of each binary vector produced by each pro-
jection function. So, the output of the character-
level projector is a T · d binary vector.

The locality-sensitive nature of the projections
help learn a compact representation that capture
semantic similarity (Sankar et al., 2019a) (i.e.,
words with similar character or morphology are
mapped to the same binary representation) with a
small memory footprint. For more details on LSH
projections, refer to (Ravi, 2017).



3897

2.2.2 Context Projection
In addition to characters, the surrounding con-
text also contains important information useful to
represent in the word vector. Unlike characters
though, word context is typically not fixed in lan-
guage and changes across sentences and docu-
ments. To address this, we extend the projection
mechanism to capture word meaning using con-
textual information.

PContext(wit) = P(context(wit)) (5)
= P(wit−∆...wit−1, (6)

wit+1...wit+∆)

= P(−−−→wit,∆) (7)

= LSHproj(
−−−→wit,∆,

−−→
PT,d) (8)

where, ∆ represents the context size around the
current word wit, −−−→wit,∆ is the context features ex-
tracted from neighboring context. We use simple
raw features like word unigrams to model the con-
text and apply dynamic projections on the sparse
context feature vector to transform this to a binary
context projection vector.

2.3 Recurrent Projector

We use LSTM to model the state of word pro-
jection sequences within a sentence or document.
This allows the model to capture words appear-
ing in different parts of the input text, and use the
projection space to learn an efficient neural rep-
resentation for the task. At each time-step t, we
compute the word projection by combining both
character and context information, and transform-
ing them via projections.

P(xt) = [ PChar(xt), PContext(xt) ] (9)

The LSTM-projector PRNN then computes the
new state hPt using the projector output as follows

fPt = σ(Wf · [ht−1,P(xt)] + bf ) (10)
iPt = σ(Wi · [ht−1,P(xt)] + bi) (11)

C̃Pt = tanh(WC · [ht−1,P(xt)] + bC) (12)

CPt = f
P
t ∗ CPt−1 + iPt ∗ C̃Pt (13)

oPt = σ(Wo · [ht−1,P(xt)] + bo) (14)
hPt = o

P
t ∗ tanh(CPt ) (15)

where, fP, iP, oP represent the projection for-
get, input and output gates respectively, σ is a
non-linearity (ReLu) and W, b are the weight and

bias parameters. hPt is the new state computed
from previous state hPt−1. Note that we can replace
LSTM with GRU (Cho et al., 2014) or other vari-
ants to control the gating mechanism and derive
other recurrent projector model variants.

2.4 Projection Sequence Encoder

We use a bidirectional recurrent projector PRNN
that encodes the input word sequence by summa-
rizing information from both directions for word
projections. The sequence encoder captures both
character-level and contextual information across
time steps through the projection space ΩP.

The projection sequence encoder contains a for-
ward encoder

−−→
PRNN that processes the sentence (or

document) from wi1 to wiT and the backward en-
coder

←−−
PRNN which reads the input text in reverse.

x
−→
hPi =

−−→
PRNN(xi) (16)

←−
hPi =

←−−
PRNN(xi) (17)

hi = [
−→
hPi ,
←−
hPi ] (18)

We concatenate the two states to model the state
hi and transition of word projection sequences
within a sentence or document.
Stacked Recurrent Projections: We also stack
the recurrent projections to create deeper, multi-
layered ProSeqo. Each layer is a bidirectional re-
current projection encoder with information from
the forward and backward states passed to the next
layer. In our work, we train ProSeqo with 2 layers.

2.5 Text Classifier

The final state of the projection sequence encoder
hiF is combined with an output layer to learn a
classifier for a given task.

yi = softmax(WF · hiF + bF ) (19)

We train ProSeqo via backpropagation using
stochastic gradient descent with negative log like-
lihood of correct labels as the loss.

3 Text Classification Tasks

In this section, we describe the tasks and datasets
for our experimental evaluation. We choose the
datasets to compare against prior state-of-the-art
on-device work (Ravi and Kozareva, 2018), and
also to test the generalizability of our on-device
model on short and long texts.



3898

Text Classification Task #Classes Vocabulary Avg. Length Train Test
SWDA Dialog Act 42 20,000 7 193,000 5,000
MRDA Dialog Act 6 12,000 8 78,000 15,000

ATIS Intent Prediction 21 950 11 4,478 893
SNIPS Intent Prediction 7 11,241 10 13,084 700

AG News Categorization 4 156,000 38 120,000 7,600
Y!A Yahoo! Answers Categorization 10 1,554,607 108 1,400,000 60,000

AMZN Product Review Prediction 5 1,919,336 92 3,000,000 650,000

Table 1: Text Classification Tasks and Dataset Characteristics

3.1 Dataset Description

• SWDA: Switchboard Dialog Act Corpus is a
popular open domain dialog corpus between two
speakers with 42 dialog acts (Godfrey et al., 1992;
Jurafsky et al., 1997). The dataset is used in the
on-device work of (Ravi and Kozareva, 2018).
• MRDA: Meeting Recorder Dialog Act is a dia-
log corpus of multiparty meetings annotated with
6 dialog acts (Adam et al., 2003; Shriberg et al.,
2004). The dataset was also used by on-device
work of (Ravi and Kozareva, 2018). We used the
same data split as (Lee and Dernoncourt, 2016;
Ortega and Vu, 2017; Ravi and Kozareva, 2018).
• ATIS: The Airline Travel Information Systems
dataset (Tür et al., 2010) is widely used in di-
alog and speech research. The dataset contains
audio recordings of people making flight reserva-
tions (Tür et al., 2010; Goo et al., 2018).
• SNIPS: To test the generalizability of our model,
we use another NLU dataset with custom intent-
engines collected by Snips personal voice assis-
tant. We used the data from (Goo et al., 2018).
Compared to the single-domain ATIS dataset,
Snips is more complicated mostly because of the
rich and diverse intent repository combined with
the larger vocabulary.
• AG: AG News corpus is a collection of news
articles on the web, where each document has a
title and a description field. We used the dataset
from (Zhang et al., 2015).
• Y!A: Yahoo! Answers is a text classification
task with 10 diverse classes: Society & Culture,
Science & Mathematics, Health, Education & Ref-
erence, Computers & Internet, Sports, Business &
Finance, Entertainment & Music, Family & Rela-
tionships and Politics & Government. Each doc-
ument contains a question title, question context
and best answers. We obtained the data from
(Zhang et al., 2015). Note that (Yang et al., 2016)
used a smaller test sample for evaluation. To
present fair results, we will not compare to (Yang

et al., 2016).
• AMZN: Amazon review dataset is obtained
from (Zhang et al., 2015). Resolving this task
can be very helpful for product categorization
(Kozareva, 2015). The corpora has reviews with
ratings ranging from 1 to 5. Following prior work,
we use 3,000,000 reviews for training and 650,000
reviews for testing.

3.2 Dataset Characteristics

Table 1 shows the datasets characteristics such as
the type of the task, number of classes, vocabu-
lary size, average text length, train and test sizes.
As shown in Table 1, the datasets are very diverse,
some have few thousand training examples, while
others have millions. The test sizes also differ.
Similarly, there are tasks with small and huge vo-
cabularies. These diverse characteristics help vali-
date and capture the ability of our on-device model
ProSeqo to generalize to different tasks, data con-
straints, settings and vocabulary. With respect to
the text length, we have two major categories:
short text tasks such as dialog act and intent pre-
diction, and long text tasks such as news, product
reviews and answer categorization.

3.3 Experimental Setup & Model Parameters

We setup our experiments as given a classification
task and dataset, we apply ProSeqo to train an on-
device model. For each task, we report Accuracy
on the test set. Unlike typical NLP approaches in-
cluding baseline neural methods, we do not ap-
ply any vocabulary pruning or pre-processing at
all to the input sentences and documents, except
for splitting tokens by space.

We use a 2-layer ProSeqo neural network with
recurrent projections of size T = 60, d = 14
and 256 hidden dimensions to represent state in
each bidirectional LSTM-projection layer. We use
character 7-grams (with 1-skip) and context size of
1 to model the projector described in Section 2.2.
ProSeqo network is trained with SGD and Adam



3899

Model SWDA MRDA ATIS SNIPS
ProSeqo (our on-device model) 88.3 90.1 97.8 97.9

SGNN(Ravi and Kozareva, 2018)(on-device) 83.1 86.7 88.9 93.4
RNN(Khanpour et al., 2016) 80.1 86.8 - -

RNN+Attention(Ortega and Vu, 2017) 73.8 84.3 - -
CNN(Lee and Dernoncourt, 2016) 73.1 84.6 - -

GatedIntentAtten.(Goo et al., 2018) - - 94.1 96.8
GatedFullAtten.(Goo et al., 2018) - - 93.6 97.0

JointBiLSTM(Hakkani-Tur et al., 2016) - - 92.6 96.9
Atten.RNN(Liu and Lane, 2016) - - 91.1 96.7

Table 2: Short Text Classification On-device Results & Comparisons to Prior Work

optimizer (Kingma and Ba, 2014) over shuffled
mini-batches of size 100. We did not do any addi-
tional dataset-specific tuning or processing.

4 STC: Short Text Classification Results

This section focuses on the multiple short text
classification experiments we have conducted. Ta-
ble 2 shows the results on the dialog act and in-
tent prediction tasks for SWDA, MRDA, ATIS
and SNIPS datasets. Overall, ProSeqo reached the
highest performance and significantly improved
upon prior state-of-the-art on-device neural ap-
proaches (Ravi and Kozareva, 2018). Similarly,
ProSeqo outperformed non-on-device neural mod-
els like RNN (Khanpour et al., 2016), CNN
(Lee and Dernoncourt, 2016) and joint neural ap-
proaches (Goo et al., 2018; Hakkani-Tur et al.,
2016).

4.1 STC: Comparison with On-Device Work
We compare our on-device model against state-of-
the-art on-device short text classification approach
SGNN (Ravi and Kozareva, 2018). SGNN was
evaluated only on the SWDA and MRDA dialog
act tasks (Ravi and Kozareva, 2018) and reached
state-of-the-art performance against prior non-on-
device neural approaches (Khanpour et al., 2016).
We directly compare performance on the same
SWDA and MRDA datasets. As shown in Table 2
ProSeqo reaches +5.3% improvement for SWDA
and +3.4% accuracy improvement for MRDA.

Since we want to compare on-device perfor-
mance on a wider spectrum of tasks and datasets,
we re-implemented SGNN with the same param-
eters (Ravi and Kozareva, 2018). We run ex-
periments on ATIS and SNIPS intent prediction
tasks and reported results in Table 2 in italic to
indicate that this is a re-implementation of (Ravi
and Kozareva, 2018) and previously these on-

device results were not reported. As shown in
Table 2, SGNN consistently performs well on
dialog act and intent prediction tasks. Overall,
ProSeqo reached +8.9% accuracy improvements
on ATIS and +4.5% accuracy improvements on
SNIPS compared to SGNN. This shows that ProS-
eqo’s recurrent dynamic projections learn more
powerful representations than the static SGNN
ones, this also leads to significant performance im-
provements on multiple tasks.

4.2 STC: Comparison with Non-On-Device

The main objective of on-device work is to de-
velop small and efficient models that fit on devices
with limited memory and capacity. In contrast, the
non-on-device models do not have any constraints
and can use all resources available on server side.
Therefore, it is not fair to directly compare the on-
device and non-on-device models. Taking these
differences in consideration, we show in Table 2
results from non-on-device models with the ob-
jective to highlight the power of on-device models
and their capability to produce small memory foot-
print models, which are competitive in accuracy
to those using pre-trained embeddings and uncon-
strained resources.

As shown in Table 2, prior work focused on
developing the best approach for a specific task
and dataset, resulting in not having a single model
across all tasks. ProSeqo is the only approach
spanning across all tasks and datasets. In terms
of SWDA and MRDA, ProSeqo significantly im-
proves with +8.2% accuracy and +3.3% accu-
racy the best performing non-on-device neural ap-
proach (Khanpour et al., 2016). For ATIS and
SNIPs, the most recent state-of-art approaches use
joint intent and slot prediction model (Hakkani-
Tur et al., 2016; Liu and Lane, 2016; Goo et al.,
2018), where the slot model recognizes the enti-



3900

Model AG Y!A AMZN
ProSeqo (our on-device model) 91.5 72.4 62.3

SGNN (Ravi and Kozareva, 2018)(on-device) 57.6 36.5 39.3
FastText-full(Joulin et al., 2016) 92.5 72.3 60.2

CharCNNLargeWithThesau.(Zhang et al., 2015) 90.6 71.2 59.6
CNN+NGM(Bui et al., 2018) 86.9 - -
LSTM-full(Zhang et al., 2015) 86.1 70.8 59.4

Hier.-Attention(Yang et al., 2016) - - 63.6
Hier.-AVE(Yang et al., 2016) - - 62.9

Table 3: Long Text Classification On-device Results & Comparisons to Prior Work

ties and their semantic categories in the text, and
the intent prediction model uses the slot informa-
tion. (Liu and Lane, 2016) showed that joint in-
tent and slot model improves upon the individual
ones. While the entities and their semantic types
are useful, ProSeqo does not rely on such addi-
tional information. ProSeqo simply uses the text
and different context to learn compact semantic
projection representations on-the-fly to make the
intent prediction. ProSeqo improves with +3.7%
the best approach for ATIS, which is a gated in-
tent attention (Goo et al., 2018), and with +1% the
best approach for SNIPS, which is gated full at-
tention (Goo et al., 2018). (Goo et al., 2018) de-
veloped two complimentary approaches one with
full attention and one with gated intent attention,
yet they do not have consistent improvements on
both datasets and tasks. In our case, our on-device
ProSeqo model consistently improved upon both
tasks, and reached state-of-the-art performance
over prior non-on-device neural approaches. This
is a significant improvement given the small size
of ProSeqo and the fact that it does not store or
lookup any pre-trained embedding matrices.

5 LDC: Long Document Classification
Results

This section focuses on long document classi-
fication results. Table 3 shows the results on
three tasks and datasets (AG, Y!A, AMZN). Over-
all, ProSeqo significantly improved upon the on-
device neural model SGNN (Ravi and Kozareva,
2018) with +23% to +35.9% accuracy. ProS-
eqo also reached comparable performance to prior
non-on-device neural LSTMs and character CNNs
approaches (Zhang et al., 2015; Bui et al., 2018).

5.1 LDC: Comparison with On-Device Work
To the best of our knowledge, ProSeqo is the
first on-device neural approach for long docu-

ment classification. Since prior on-device work fo-
cused mostly on short text, we re-implemented the
on-device state-of-the-art neural approach SGNN
(Ravi and Kozareva, 2018) and run it on multiple
long text tasks and datasets. Similarly to the short
text section, we reported SGNN results in Table 3
in italic to indicate that this is a re-implementation
of (Ravi and Kozareva, 2018) and previously re-
sults on these datasets and tasks were not reported.

As shown in Table 3, SGNN performed well
on short texts, but it did not reach high perfor-
mance on long documents. It it important to high-
light that ProSeqo maintained consistent perfor-
mance across all short and long document classi-
fication tasks. For long texts, ProSeqo improved
SGNN with +33.9% accuracy on AG news, with
+35.9% accuracy on Y!A and with +23% accuracy
on Amazon product review classification. These
are significant improvements and further show the
importance of ProSeqo’s recurrent dynamic pro-
jections for short and long text classification.

5.2 LDC: Comparison with Non-On-Device

Similarly to the short text classification, we also
conduct a comparison of non-on-device model for
long document classification. As highlighted be-
fore, such comparison is not fair due to the fact
that non-on-device models do not have to com-
ply with small memory, small size and low la-
tency constraints. Bearing this in mind, we show
results in Table 3. As it can be seen, only Fast-
Text (Joulin et al., 2016), CharCNN with The-
saurus (Zhang et al., 2015) and LSTM-full (Zhang
et al., 2015) have been evaluated on all datasets,
while CNN+NGM (Bui et al., 2018) focused on
reaching the best performance for AG news. ProS-
eqo outperforms (Joulin et al., 2016; Zhang et al.,
2015,?; Bui et al., 2018), and reaches compara-
ble results to FastText (Joulin et al., 2016) on AG
news. The hierarchical attention and hierarchi-



3901

cal average models of (Yang et al., 2016) were
evaluated only on the Amazon reviews and Y!A
datasets. However, we noticed that the test set of
the Y!A dataset in (Yang et al., 2016) was smaller
compared to the data used in (Zhang et al., 2015;
Joulin et al., 2016) and our experiments. To make
consistent and fair comparisons, we did not in-
clude (Yang et al., 2016). For Amazon reviews,
ProSeqo reached comparable performance to hi-
erarchical average and hierarchical attention mod-
els (Yang et al., 2016). Overall, ProSeqo reached
similar performance to non-on-device neural ap-
proaches, which use pre-trained embeddings while
ProSeqo does not store or lookup any pre-trained
embedding matrices.

6 ProSeqo Ablation Studies

This section shows two ablation studies focusing
on the: (1) effect of the projection size on accuracy
and (2) model size on accuracy.

6.1 Effect of Projection Size on Accuracy

During series of evaluations on multiple short and
long text classification tasks, and comparing re-
sults against prior state-of-the-art neural networks
and on-device neural models, ProSeqo achieved
robust performance irrespective of the task, vocab-
ulary size, training and test datasets. This is signif-
icant improvement given the small memory size
of the model and the fact that we used the same
model, configuration and model parameters across
all seven NLP tasks unlike prior work which fine-
tuned on specific data.

We also evaluated ProSeqo with different num-
ber of parameters T , d used in the recurrent projec-
tion. Table 4 shows the performance of ProSeqo
on the AG News classification task and indicates
that even if we reduce the projection size by 3x,
we still achieve 90% accuracy with a small drop
in performance of 1.4% (91.5→ 90.09).

Projection Size (T · d) 300 600 800
Compression Ratio (S1S2 ) 15× 10

4 7.5× 104 5.6× 104

Accuracy 90.0 91.0 91.5

Table 4: Effect of ProSeqo’s Recurrent Projection Size
on Accuracy for AG News Classification. Compres-
sion Ratio is computed wrt ProSeqo model size (S1)
vs. LSTM baseline (S2) with word embeddings (vo-
cabulary size=150K, embedding size=300).

6.2 ProSeqo’s Model Size

In addition to the quality evaluations, comparisons
and results, we also highlight how compact and
fast our on-device model is, which is very im-
portant for fitting the model on small memory-
constrained devices such as mobile phones and
watches. For this purpose, we measure the current
size of our ProSeqo model. We do not have the
exact parameter counts and model sizes from the
previous neural approaches such as RNNs, BiL-
STMs, and Hierarchical Attention models. But
we make an observation that number of parame-
ters in the recurrent layers of these baseline net-
works are similar to the parameters encoded in the
recurrent projector layers in the ProSeqo model.
Hence we can approximately compute the differ-
ence in model sizes between ProSeqo and other
neural models by taking into consideration the size
of embeddings fed into the recurrent layers along
with the training data vocabulary size.

As highlighted in the main contributions of our
approach ProSeqo is an embedding-free method,
which means that it does not have to store or look
up any embedding matrices. This makes the model
small in comparison to existing neural approaches.
For instance, to calculate the model size of ProS-
eqo as explained in Section 2 we need to consider
(T ·d) where d is the number of LSH bits specified
for each projection vector, and T is the number of
projection functions used resulting in (T · d) =
60 × 14 parameters shared across all time-steps.
Independent of the task, length of the text (short vs
long) and vocabulary sizes, ProSeqo’s model size
remains constant (T · d) across all seven tasks and
datasets. Comparing ProSeqo’s model size against
a neural approach that uses Glove (or word2vec)
word embeddings with 300 dimensions and vocab-
ulary size V for the datasets, we can easily see that
the neural models will have bigger sizes and their
model size also varies depending on the task and
vocabulary size. For instance, the vocabulary size
in AG dataset is 150K resulting in 150K × 300
more model parameters than ProSeqo. For Yahoo
this is 1.5M × 300 and for Amazon model size
is 1.9M × 300. As it can be seen, ProSeqo pro-
duces models that are orders of magnitude smaller
compared to prior neural approaches. On AG
dataset, ProSeqo model has 5 · 104x fewer param-
eters for the first layer which is equivalent to just
3.3KB in size using float32 parameters com-
pared to 180MB for other neural methods. ProS-



3902

eqo’s size remains constant independent of the text
lengths, vocabulary size and classification tasks.
Even with neural models like charCNN that use
character-level embeddings which is typically ap-
plied in conjunction with word embeddings, ProS-
eqo still yields an order or two magnitude smaller
model sizes. The computation within ProSeqo is
fast, O(T · d · n) multiply-add operations for
n-length words regardless of word/character vo-
cabulary sizes.

7 Conclusion

We proposed a novel Projection Sequence Neural
Network (ProSeqo) for on-device text classifica-
tion. Evaluations on multiple text classification
tasks show that ProSeqo significantly improved
accuracy compared to state-of-the-art on-device
neural network SGNN (Ravi and Kozareva, 2018)
on short text with +3.4% for MRDA, +4.5% on
SNIPS, +5.3% on SWDA and +8.9% on ATIS;
and long documents with +23% on Amazon prod-
uct reviews, +33.9% on AG news, +35.9% on
Y!A. Similarly, ProSeqo improved non-on-device
neural RNN, CNN and joint approaches (Khan-
pour et al., 2016; Lee and Dernoncourt, 2016;
Hakkani-Tur et al., 2016; Goo et al., 2018) on
short text and reached comparable results on long
documents (Joulin et al., 2016; Zhang et al., 2015;
Bui et al., 2018). This is significant improvement
given ProSeqo’s small network size, and the fact
that we used the same architecture and parameters
across all seven NLP tasks, while prior work tuned
depending on the task and dataset.

References

Janin Adam, Don Baron, Jane Edwards, Dan Ellis,
David Gelbart, Nelson Morgan, Barbara Peskin,
Thilo Pfau, Elizabeth Shriberg, Andreas Stolcke,
and Chuck Wooters. 2003. The icsi meeting cor-
pus. In Proceedings of the 5TH SIGdial Workshop
on Discourse and Dialogue, pages 364–367.

Thang D. Bui, Sujith Ravi, and Vivek Ramavajjala.
2018. Neural graph learning: Training neural net-
works using graphs. In Proceedings of the Eleventh
ACM International Conference on Web Search and
Data Mining, WSDM ’18, pages 64–71.

Kyunghyun Cho, Bart van Merriënboer, Çalar
Gülçehre, Dzmitry Bahdanau, Fethi Bougares, Hol-
ger Schwenk, and Yoshua Bengio. 2014. Learning
phrase representations using rnn encoder–decoder
for statistical machine translation. In Proceedings of

the 2014 Conference on Empirical Methods in Nat-
ural Language Processing (EMNLP), pages 1724–
1734, Doha, Qatar. Association for Computational
Linguistics.

Jordan Cohen. 2008. Embedded speech recognition
applications in mobile phones: Status, trends, and
challenges. pages 5352 – 5355.

John J. Godfrey, Edward C. Holliman, and Jane Mc-
Daniel. 1992. Switchboard: Telephone speech cor-
pus for research and development. In Proceed-
ings of the 1992 IEEE International Conference on
Acoustics, Speech and Signal Processing - Volume
1, ICASSP’92, pages 517–520. IEEE Computer So-
ciety.

Chih-Wen Goo, Guang Gao, Yun-Kai Hsu, Chih-Li
Huo, Tsung-Chieh Chen, Keng-Wei Hsu, and Yun-
Nung Chen. 2018. Slot-gated modeling for joint
slot filling and intent prediction. In Proceedings of
the 2018 Conference of the North American Chap-
ter of the Association for Computational Linguistics:
Human Language Technologies, Volume 2 (Short
Papers), pages 753–757. Association for Computa-
tional Linguistics.

Dilek Hakkani-Tur, Gokhan Tur, Asli Celikyilmaz,
Yun-Nung Vivian Chen, Jianfeng Gao, Li Deng, and
Ye-Yi Wang. 2016. Multi-domain joint semantic
frame parsing using bi-directional rnn-lstm. In Pro-
ceedings of The 17th Annual Meeting of the Interna-
tional Speech Communication Association (INTER-
SPEECH 2016).

Yanzhang He, Rohit Prabhavalkar, Kanishka Rao,
Wei Li, Anton Bakhtin, and Ian McGraw.
2017. Streaming small-footprint keyword spot-
ting using sequence-to-sequence models. CoRR,
abs/1710.09617.

Armand Joulin, Edouard Grave, Piotr Bojanowski,
Matthijs Douze, Hervé Jégou, and Tomas Mikolov.
2016. Fasttext.zip: Compressing text classification
models. CoRR, abs/1612.03651.

Daniel Jurafsky, Rebecca Bates, Rachel Martin
Noah Coccaro, Marie Meteer, Klaus Ries, Eliza-
beth Shriberg, Audreas Stolcke, Paul Taylor, and
Van Ess-Dykema. 1997. Automatic detection of
discourse structure for speech recognition and un-
derstanding. In Proceedings of IEEE Workshop on
Automatic Speech Recognition and Understanding,
pages 88–95.

Hamed Khanpour, Nishitha Guntakandla, and Rod-
ney Nielsen. 2016. Dialogue act classification in
domain-independent conversations using a deep re-
current neural network. In Proceedings of COLING
2016, the 26th International Conference on Compu-
tational Linguistics: Technical Papers, pages 2012–
2021.

Diederik P. Kingma and Jimmy Ba. 2014. Adam:
A method for stochastic optimization. CoRR,
abs/1412.6980.

http://www.aclweb.org/anthology/D14-1179
http://www.aclweb.org/anthology/D14-1179
http://www.aclweb.org/anthology/D14-1179
https://doi.org/10.1109/ICASSP.2008.4518869
https://doi.org/10.1109/ICASSP.2008.4518869
https://doi.org/10.1109/ICASSP.2008.4518869
http://arxiv.org/abs/1710.09617
http://arxiv.org/abs/1710.09617
http://arxiv.org/abs/1612.03651
http://arxiv.org/abs/1612.03651


3903

Zornitsa Kozareva. 2015. Everyone likes shopping!
multi-class product categorization for e-commerce.
In Proceedings of the 2015 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics: Human Language Technologies,
pages 1329–1333.

Ji Young Lee and Franck Dernoncourt. 2016. Sequen-
tial short-text classification with recurrent and con-
volutional neural networks. In Proceedings of the
2016 Conference of the North American Chapter of
the Association for Computational Linguistics: Hu-
man Language Technologies, pages 515–520.

Zhong Qiu Lin, Audrey G. Chung, and Alexander
Wong. 2018. Edgespeechnets: Highly efficient deep
neural networks for speech recognition on the edge.
CoRR, abs/1810.08559.

Bing Liu and Ian Lane. 2016. Attention-based recur-
rent neural network models for joint intent detection
and slot filling. Proceedings of The 17th Annual
Meeting of the International Speech Communication
Association (INTERSPEECH 2016).

Daniel Ortega and Ngoc Thang Vu. 2017. Neural-
based context representation learning for dialog act
classification. In Proceedings of the 18th Annual
SIGdial Meeting on Discourse and Dialogue, pages
247–252.

Sujith Ravi. 2017. Projectionnet: Learning efficient
on-device deep networks using neural projections.
CoRR, abs/1708.00630.

Sujith Ravi. 2019. Efficient on-device models using
neural projections. In Proceedings of the 36th In-
ternational Conference on Machine Learning, vol-
ume 97 of Proceedings of Machine Learning Re-
search, pages 5370–5379.

Sujith Ravi and Zornitsa Kozareva. 2018. Self-
governing neural networks for on-device short text
classification. In Proceedings of the 2018 Confer-
ence on Empirical Methods in Natural Language
Processing, Brussels, Belgium, October 31 - Novem-
ber 4, 2018, pages 804–810.

Sujith Ravi and Zornitsa Kozareva. 2019. On-device
structured and context partitioned projection net-
works. In Proceedings of the 57th Annual Meet-
ing of the Association for Computational Linguis-
tics, pages 3784–3793.

Chinnadhurai Sankar, Sujith Ravi, and Zornitsa
Kozareva. 2019a. On the robustness of projection
neural networks for efficient text representation: An
empirical study. ArXiv, abs/1908.05763.

Chinnadhurai Sankar, Sujith Ravi, and Zornitsa
Kozareva. 2019b. Transferable neural projection
representations. In Proceedings of the 2019 Con-
ference of the North American Chapter of the Asso-
ciation for Computational Linguistics: Human Lan-
guage Technologies, pages 3355–3360.

Elizabeth Shriberg, Raj Dhillon, Sonali Bhagat, Jeremy
Ang, and Hannah Carvey. 2004. The icsi meeting
recorder dialog act (mrda) corpus. In Proceedings
of the 5th SIGdial Workshop on Discourse and Di-
alogue, pages 97–100, Cambridge, Massachusetts,
USA. Association for Computational Linguistics.

Gökhan Tür, Dilek Hakkani-Tür, and Larry P. Heck.
2010. What is left to be understood in atis? In
Proceedings of 2010 IEEE Spoken Language Tech-
nology Workshop (SLT), pages 19–24.

Zichao Yang, Diyi Yang, Chris Dyer, Xiaodong He,
Alex Smola, and Eduard Hovy. 2016. Hierarchi-
cal attention networks for document classification.
In Proceedings of the 2016 Conference of the North
American Chapter of the Association for Compu-
tational Linguistics: Human Language Technolo-
gies, pages 1480–1489. Association for Computa-
tional Linguistics.

Xiang Zhang, Junbo Zhao, and Yann LeCun. 2015.
Character-level convolutional networks for text clas-
sification. In Proceedings of the 28th International
Conference on Neural Information Processing Sys-
tems - Volume 1, NIPS’15, pages 649–657. MIT
Press.

http://arxiv.org/abs/1810.08559
http://arxiv.org/abs/1810.08559
http://arxiv.org/abs/1708.00630
http://arxiv.org/abs/1708.00630
https://www.aclweb.org/anthology/P19-1368
https://www.aclweb.org/anthology/P19-1368
https://www.aclweb.org/anthology/P19-1368
https://www.aclweb.org/anthology/N19-1339
https://www.aclweb.org/anthology/N19-1339

