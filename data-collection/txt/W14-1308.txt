



















































A Cascaded Approach for Social Media Text Normalization of Turkish


Proceedings of the 5th Workshop on Language Analysis for Social Media (LASM) @ EACL 2014, pages 62–70,
Gothenburg, Sweden, April 26-30 2014. c©2014 Association for Computational Linguistics

A Cascaded Approach for Social Media Text Normalization of Turkish

Dilara Torunoǧlu
Dep. of Computer Eng.

Istanbul Technical University
Istanbul, Turkey

torunoglud@itu.edu.tr

Gülşen Eryiǧit
Dep. of Computer Eng.

Istanbul Technical University
Istanbul, Turkey

gulsen.cebiroglu@itu.edu.tr

Abstract
Text normalization is an indispensable
stage for natural language processing of
social media data with available NLP
tools. We divide the normalization prob-
lem into 7 categories, namely; letter case
transformation, replacement rules & lexi-
con lookup, proper noun detection, deasci-
ification, vowel restoration, accent nor-
malization and spelling correction. We
propose a cascaded approach where each
ill formed word passes from these 7 mod-
ules and is investigated for possible trans-
formations. This paper presents the first
results for the normalization of Turkish
and tries to shed light on the different chal-
lenges in this area. We report a 40 per-
centage points improvement over a lexicon
lookup baseline and nearly 50 percentage
points over available spelling correctors.

1 Introduction

With the increasing number of people using micro
blogging sites like Facebook and Twitter, social
media became an indefinite source for machine
learning area especially for natural language pro-
cessing. This service is highly attractive for infor-
mation extraction, text mining and opinion min-
ing purposes as the large volumes of data available
online daily. The language used in this platform
differs severely from formally written text in that,
people do not feel forced to write grammatically
correct sentences, generally write like they talk or
try to impress their thoughts within a limited num-
ber of characters (such as in Twitter 140 charac-
ters). This results with a totally different language
than the conventional languages. The research on
text normalization of social media gained speed
towards the end of the last decade and as always,
almost all of these elementary studies are con-
ducted on the English language. We know from

earlier research results that morphologically rich
languages such as Turkish differ severely from En-
glish and the methods tailored for English do not
fit for these languages. It is the case for text nor-
malization as well.

Highly inflectional or agglutinative languages
share the same characteristic that a unique lemma
in these languages may have hundreds of possible
surface forms. This increases the data sparsity in
statistical models. For example, it’s pointed out in
Hakkani-Tür et al. (2000) that, it is due to Turk-
ish language’s inflectional and derivational mor-
phology that the number of distinct word forms
is very large compared to English distinct word
size (Table 1). This large vocabulary size is the
reason why the dictionary1 lookup or similarity
based approaches are not suitable for this kind of
languages. And in addition to this, it is not an
easy task to collect manually annotated data which
could cover all these surface forms and their re-
lated mistakes for statistical approaches.

Corpus Size Turkish English
1M words 106,547 33,398
10M words 417,775 97,734

Table 1: Vocabulary sizes for two Turkish and En-
glish corpora (Hakkani-Tür et al., 2000)

In this paper, we propose a cascaded approach
for the social text normalization (specifically for
Tweets) of Turkish language. The approach is
a combination of rule based and machine learn-
ing components for different layers of normaliza-
tion, namely; letter case transformation, replace-
ment rules & lexicon lookup, proper noun detec-
tion, deasciification, vowel restoration, accent nor-
malization and spelling correction. Following the
work of Han and Baldwin (2011), we divided the
work into two stages: ill formed word detection

1For these languages, it is theoretically impossible to put
every possible surface form into a dictionary.

62



and candidate word generation. Our contribution
is: 1. a new normalization model which could be
applied to other morphologically rich languages as
well with appropriate NLP tools 2. the first re-
sults and test data sets for the text normalization
of Turkish.

The paper is structured as follows: Section 2
and 3 give brief information about related work
and morphologically rich languages, Section 4
presents our normalization approach and Section
5 the experimental setup, Section 6 gives our ex-
perimental results and discussions and Section 7
the conclusion.

2 Related Work

An important part of the previous studies have
taken the normalization task either as a lexi-
con lookup (together with or without replacement
rules) or as a statistical problem. There also ex-
ist many studies which use their combination. In
these studies, a lexicon lookup is firstly employed
for most common usage of slang words, abbrevi-
ations etc. and then a machine learning method
is employed for the rest. Zhang et al. (2013) uses
replacement rules and a graph based model in or-
der to select the best rule combinations. Wang and
Ng (2013) uses a beam search decoder. Hassan
and Menezes (2013) propose an unsupervised ap-
proach which uses Random Walks on a contextual
similarity bipartite graph constructed from n-gram
sequences. In Han and Baldwin (2011), word sim-
ilarity and context is used during lexicon lookup.
Cook and Stevenson (2009) uses an unsupervised
noisy channel model. Clark and Araki (2011)
makes dictionary lookup. Liu et al. (2012) uses
a unified letter transformation to generate possi-
ble ill formed words in order to use them in the
training phase of a noisy channel model. Eisen-
stein (2013) analyzes phonological factors in so-
cial media writing.

Others, treating the normalization task as a
machine translation (MT) problem which tries
to translate from an ill formed language to a
conventional one, form also another important
group. For example the papers from Kaufmann
and Kalita (2010), Pennell and Liu (2011), Aw et
al. (2006) and Beaufort et al. (2010) may be col-
lected under this group. Since the emergence of
social media is very recent, only the latest stud-
ies are focused on this area and the earlier ones
generally work for the text normalization in TTS

(text-to-speech), ASR (automatic speech recogni-
tion) systems or SMS messages. Social media nor-
malization poses new challenges on top of these,
for example Twitter statuses contains mentions
(@user name), hashtags (#topic), variant number
of emoticons ( e.g. :) :@ <3 @>– ) and spe-
cial keywords (RT - retweet, DM - direct message
etc.).

Although very rare, there are also some stud-
ies on languages other than English and these
are mostly for speech recognition and SMS mes-
sages , e.g. Panchapagesan et al. (2004) for Hindi
TTS, Nguyen et al. (2010) for Vietnamese TTS,
Jia et al. (2008) for Mandarin TTS, Khan and
Karim (2012) for Urdu SMS. To the best of our
knowledge, our study is the first attempt for the
normalization of social media data for morpholog-
ically rich languages.

3 Morphologically Rich Languages

Morphologically rich languages such as Turkish,
Finnish, Korean, Hebrew etc., pose significant
challenges for natural language processing tasks
(Tsarfaty et al., 2013; Sarikaya et al., 2009). As
stated previously, the highly productive morphol-
ogy of these languages results in a very large num-
ber of word forms from a given stem. Table 2 lists
only a few (among hundreds of possible) surface
forms for the Turkish stem “ev” (house).

Surface form English
ev house
eve to the house
evde at the house
evdeki (which is) at the house
evdekiler those (who are) at the house
evdekilerde at those (who are)

Table 2: Some surface forms for “ev” (house)

Sarikaya et al. (2009) list the emerging prob-
lems as below:

1. increase in dictionary size
2. poor language model probability estimation
3. higher out-of-vocabulary (OOV) rate
4. inflection gap for machine translation2

That is why, the normalization methods pro-
posed so far (adapting MT or language models or

2Since, the number of possible word surface forms after
inflections is very high, the alignment and translation accura-
cies in these languages are very badly affected.

63



lexicon lookup approaches) do not seem appropri-
ate for the processing of morphologically rich lan-
guages, as in our case for Turkish.

4 The Proposed Architecture

We divide the normalization task into two parts:
Ill-formed word detection and candidate genera-
tion. Figure 1 presents the architecture of the pro-
posed normalization approach. The following sub-
sections provide the details for both of these two
parts and their components.

Before sending the input into these stages, we
first use our tokenizer specifically tailored for
Twitter for splitting the tweets into meaningful to-
kens. Our tokenizer is actually the first step of
our normalization process since: 1. It intelligently
splits the wrongly written word-punctuation com-
binations (e.g. “a,b” to [a , b]), while leaving “Ah-
met’den” (from Ahmet) is left as it is since the
apostrophe sign is used to append inflectional fea-
tures to a proper noun.) 2. It does special pro-
cessing for emoticons and consecutive punctua-
tion marks so that they still reside together after
the tokenization (e.g. :D or !!!!! are output as they
occur).

Figure 1: Normalization architecture

4.1 Ill-formed Word Detection

As stated earlier, since it is not possible to use a
lexicon lookup table for morphologically rich lan-
guages, we use a morphological analyzer (Şahin

et al., 2013) and an abbreviation list3 and a list of
1045 abbreviations for controlling in-vocabulary
(IV) words (labeled with a +NC “No Change” la-
bel for further use). By this way, we filter all the
out-of-vocabulary (OOV) words and transfer them
to the candidate generation process. Mentions
(@user name), hashtags (#topic), emoticons (:D) ,
vocatives (“ahahahaha”) and keywords (“RT”) are
also assumed to be OOV words since we want to
detect these and tag them with special labels to be
later used in higher-level NLP modules (e.g. POS
tagging, syntactic analysis).

4.2 Candidate Generation

In the candidate generation part, we have seven
components (rule based or machine learning mod-
els) which work sequentially. The outputs of each
of these components are controlled by the morpho-
logical analyzer and if the normalized form from a
component becomes an IV word then the process
is terminated and the output is labeled with a rele-
vant tag (provided in Table 3). Otherwise, the can-
didate generation process continues with the next
component over the original input (except for the
“Letter Case Transformation” and “Replacement
Rules & Lexicon Lookup” components where the
input is replaced by the modified output although
it is still not an IV word, (see Section 4.2.1 and
4.2.2 for details).

Label Component
+NC No Change
+LCT Letter Case Transformation
+RR Replacement Rules & Lexicon Lookup
+PND Proper Noun Detection
+DA Deasciification
+VR Vowel Restoration
+AN Accent Normalization
+NoN No Suggested Normalization

Table 3: Component Labels

4.2.1 Letter Case Transformation
An OOV token, coming to this stage, may be in
one of the 4 different forms: lowercase, UPPER-
CASE, Proper Noun Case or miXEd CaSe. If
the token is in lowercase and does not possess
any specific punctuation marks for proper nouns
(i.e. ’ (apostrophe) or . (period)) , it is directly

3obtained from TLA (Turkish Language Association)
http://www.tdk.gov.tr/index.php?option=
com_content&id=198:Kisaltmalar

64



transferred to the next stage without any change
(e.g. umuttan (from hope)). If the token is in
Proper Noun Case (e.g. Umut’tan), it is accepted
as a correct proper noun (even if it does not oc-
cur within the morphological analyzer’s lexicon or
was previously detected as an OOV word), left un-
touched (taking the label +NC) and excluded from
all future evaluations.

For UPPERCASE, miXEd CaSe and lowercase
words, we convert them into Proper Noun Case if
they either contain an apostrophe (which is used
in Turkish to separate inflectional suffixes from a
proper noun) or a period (.) which is used for-
mally in Turkish to denote abbreviations. These
words are labeled with a “+LCT” label after the
normalization. If the word does not contain any
of these two marks, it is then converted into low-
ercase form and processed by the morphological
analyzer as explained at the beginning of Sec-
tion 4.2. It should be noted that all words going
out from this component towards next stages are
transformed into lowercase from this point on.

“ahmet’ten” – Proper Noun

“AHMET’TEN” – Proper Noun

“EACL.”- Abbreviation

4.2.2 Replacement Rules & Lexicon Look-up
While normalizing the tweets, we have to deal
with the following problems:

1. Slang words

2. Character repetition in interjections

3. Twitter-specific words

4. Emo style writing

We created a slang word lexicon of 272 words.
This lexicon contains entries as the following:
“kib” for “kendine iyi bak” (take care of your-
self ), “nbr” for “ne haber” (what’s up). The tokens
within the lexicon are directly replaced with their
normalized forms.

Repetition of some characters within a word is
a very common method to express exclamation
in messages, such as in “lütfeeeennnn” instead of
“lütfen” (please), “çooooooook” instead of “çok”
(very) and “ayyyyy” instead of “ay” (oh!). We re-
duce the repeated characters into a single character
in the case that the consecutive occurrence count
is greater than 2.

The usage of Twitter-specific words such as
hashtags (“#topic”), mentions (“@user name”),
emoticons (“:)”), vocatives (“hahahhah”,
“hööööö”) and keywords (“RT”) also causes
a host of problems. The recurring patterns in
vocatives are reduced into minimal forms during
the normalization process, as for “haha” instead
of “hahahhah” and “hö” instead of “hööööö”.

Emo style writing, as in the example “$eker
4you” instead of “şeker senin için” (sweety, it’s
for you), is another problematic field for the nor-
malization task. We created 35 replacement rules
with regular expressions in order to automatically
correct or label the given input for Twitter-specific
words and Emo style writing. Examples include
“$ → ş”, “� → e”, “3 → e” and “!→ i”.
Through these replacement rules, we are able to
correct most instances of Emo style writing.

Our regular expressions also label the following
token types by the given specific labels for future
reference:
• Mentions: Nicknames that refer

to users on Twitter are labeled as e.g.
@mention[@dida]
• Hashtags: Hashtags that refer to trend-

ing topics on Twitter are labeled as e.g.
@hashtag[#geziparki]
• Vocatives: Vocatives are labeled as e.g.

@vocative[hehe]
• Smileys: Emoticons are labeled as e.g.

@smiley[:)]
• Twitter-specific Keywords: Keywords like

“RT”, “DM”, “MT”, “Reply” etc. are labeled as
e.g. @keyword[RT]

Figure 2 shows the normalized version of a
tweet in informal Turkish that could be translated
like “@dida what’s up, why don’t you call #of-
fended :(”, before and after being processed by this
component. Although the word “aramıon” also
needs normalization as “aramıyorsun” (you don’t
call), this transformation is not realized within the
current component and applied later in the accent
normalization component given in Section 4.2.6.

4.2.3 Proper Noun Detection
As previously stated, all OOV words coming to
this stage are in lowercase. In this component, our
aim is to detect proper nouns erroneously written
in lowercase (such as “ahmetten” or “ahmetden”)
and convert them to proper noun case with correct
formatting (“Ahmet’ten” for the aforementioned
examples).

65



@dida

��

nbr

�� ��

neden

��

aramıon

��

#kırıldım

��

: (

��
@mention[@dida] ne haber neden aramıon @hashtag[#kırıldım] @smiley[: (]

Figure 2: Normalization with Replacement Rules & Lexicon Look-up

For this purpose, we use proper name gazetteers
from Şeker and Eryiğit (2012) together with a
newly added organization gazetteer of 122 tokens
in order to check whether a given word could
be a proper noun. Turkish proper nouns are
very frequently selected from common nouns such
as “Çiçek” (flower), “Şeker” (sugar) and “İpek”
(silk). Therefore, it is quite difficult to recog-
nize such words as proper nouns when they are
written in lowercase, as the task could not be ac-
complished by just checking the existence of such
words within the gazetteers.

For our proper noun detection component, we
use the below strategy:
1. We reduce the size of the gazetteers by remov-
ing all words with length ≤ 2 characters, or with
a ratio value under our specified threshold (1.5).
Ratio value is calculated, according to the formula
given in Equation 1, considering the occurrence
counts from two big corpora, the METU-Sabancı
Treebank (Say et al., 2002) and the web corpus
of Sak et al. (2011). Table 4 gives the counts for
three sample words. One may observe from the
table that “ahmet” occured 40 times in proper case
and 20 times in lower case form within the two
corpora resulting in a ratio value of 2.0. Since the
ratio value for “umut” is only 0.4 (which is un-
der our threshold), this noun is removed from our
gazetteers so that it would not be transformed into
proper case in case it is found to occur in low-
ercase form. A similar case holds for the word
“sağlam” (healthy). Although it is a very frequent
Turkish family name, it is observed in our corpora
mostly as a common noun with a ratio value of
0.09.

ratio(wn) =
Occurence in Propercase(wn)
Occurence in Lowercase(wn)

(1)
2. We pass the tokens to a morphological an-

alyzer for unknown words (Şahin et al., 2013)
and find possible lemmata as in the example be-
low. We then search for the longest possible stem
within our gazetteers (e.g. the longest stem for
“ahmetten” found within the name gazetteer is

Proper Case Lowercase Sense Ratio
Sağlam=9 sağlam=100 healthy Ratio=0.09
Umut=40 umut=100 hope Ratio=0.4
Ahmet=40 ahmet=20 n/a Ratio=2.0

Table 4: Example of Ratio Values

“ahmet”), and when a stem is found within the
gazetteers, the initial letter of the stem is capital-
ized and the inflectional suffixes after the stem are
separated by use of an apostrophe (“Ahmet’ten”).
If none of the possible stems is found within the
gazetteers, the word is left as is and transferred to
the next stage in its original form.

“ahmet +Noun+A3sg+Pnon+Abl”

“ahmette +Noun+A3sg+Pnom+Loc”

“ahmetten +Noun+A3sg+Pnon+Nom”

4.2.4 Deasciification
The role of the deasciifier is the reconstruction of
Turkish-specific characters with diacritics (i.e. ı,
İ, ş, ö, ç, ğ, ü) from their ASCII-compliant coun-
terparts (i.e. i, I, s, o, c, g, u). Most users of so-
cial media use asciified letters, which should be
corrected in order to obtain valid Turkish words.
The task is also not straightforward because of the
ambiguity potential in asciified forms, as between
the words “yasa” (law) and “yaşa” (live). For
this stage, we use the deasciifier of Yüret (Yüret
and de la Maza, 2006) which implements the
GPA algorithm (which itself is basically a decision
tree implementation) in order to produce the most
likely deasciified form of the input.

4.2.5 Vowel Restoration
There is a new trend of omitting vowels in typ-
ing among the Turkish social media users, in or-
der to reduce the message length. In this stage, we
process tokens written with consonants only (e.g.
“svyrm”), which is how vowel omission often hap-
pens. The aim of the vowel restoration is the gen-
eration of the original word by adding vowels into
the appropriate places (e.g. “svyrm” to “seviyo-
rum” (I love)). We employed a vocalizer (Adalı

66



and Eryiğit, 2014) which uses CRFs for the con-
struction of the most probable vocalized output.

4.2.6 Accent Normalization

In the social media platform, people generally
write like they talk by transferring the pronounced
versions of the words directly to the written text.
Eisenstein (2013) also discusses the situation for
the English case. In the accent normalization mod-
ule we are trying to normalize this kind of writings
into proper forms. Some examples are given be-
low:

“gidicem” instead of “gideceğim”

(I’ll go)

“geliyonmu?” instead of “geliyor musun?”

(Are you coming?)

In this component, we first try to detect the most
common verb accents (generally endings such as
“-cem, -yom, -çaz” etc.) used in social media and
then uses regular expression rules in order to re-
place these endings with their equivalent morpho-
logical analysis. One should note that since in
most of the morphologically rich languages, the
verb also carries inflections related to the person
agreement, we produce rules for catching all the
possible surface forms of these accents.

Table 5 introduces some of these re-
placement rules (column 1 and column 3).
As a result, the word “gidcem” becomes
“git+Verb+Pos+Fut+A1sg”4. We then use a
morphological generator and takes the cor-
rected output (if any) “gideceğim” (I’ll go) for
“git+Verb+Pos+Fut+A1sg”5.

We also have more complex replacement rules
in order to process more complex accent problems.
To give an example, the proper form of the word
“gidiyonmu” is actually “gidiyor musun” (are you
going) and in the formal form it is the question
enclitic (“mu”) which takes the person agreement
(“-sun” 2. person singular) where as in the accent
form the person agreement appears before “mu” as
a single letter “gidiyonmu”.

4Please note that, we also change the last letter of the stem
according to the harmonization rules of Turkish: the last let-
ters “bcdg” are changed to “pçtk”.

5the morphological tags in the table stands for: +Pos:
Positive, +Prog1: Present continuous tense, +A2sg: 2. per-
son singular, +Fut: Future tense, +A1sg: 1. person singular,
+A1pl: 1. person plural

Accent Correct Morph.
endings endings Analysis
+iyon +iyorsun +Verb+Pos+Prog1+A2sg
+cem +eceğim +Verb+Pos+Fut+A1sg
+caz +acağız +Verb+Pos+Fut+A1pl

Table 5: Accent Normalization Replacement
Rules

4.2.7 Spelling Correction
As the last component of our normalization ap-
proach, we propose to use a high performance
spelling corrector. This spelling corrector should
especially give a high precision score rather than
recall since the false positives have a very harm-
ing effect on the normalization task by producing
outputs with a totally different meaning. Unfortu-
nately, we could not find such a corrector for Turk-
ish. We tested with an MsWord plugin and the
spelling corrector of Zemberek (Akın and Akın,
2007) and obtained a negative impact by using
both. We are planning to create such a spelling
corrector as future work.

If an OOV word couldn’t still be normalized at
the end of the proposed iterative model (consisting
7 components), it is labeled with a “+NoN” label
and left in its original input format.

5 Experimental Setup

In this section we provide information about our
used data sets, our evaluation strategy and the used
models in the experiments.

5.1 Data Sets

To test our success rates, we used a total of 1,200
tweets aligned and normalized manually. The
manual alignment is a one-to-many token align-
ment task from the original input towards the nor-
malized forms. To give an example, the slang us-
age “kib” will be aligned to 3 tokens (“kendine
iyi bak” (take care of yourself )) on the normal-
ized tweet. Although there are cases for many-to-
one alignment (such as in “cats,dogs”), these are
handled in the tokenization stage before the nor-
malization. We used half of this data set as our
validation set during the development of our pro-
posed components and reserved the remaining 600
tweets (collected from a different time slot) as a to-
tally unseen data set for using at the end. Table 6
provides some statistics over these data sets: the
number of tweets, the number of tokens and the

67



Data Sets # Tweets # Tokens # OOV
Validation Set 600 6,322 2,708
Test Set 600 7,061 2,192

Table 6: Description of the Data Sets

number of OOV tokens.
Besides the aforementioned datasets, we also

had access to a much bigger Twitter data set
consisting of 4,049 manually normalized tweets
(Eryiğit et al., 2013) (59,012 tokens in total). The
only difference of this data set is that the tweets
are not aligned on token level as in the previously
introduced data sets. That is why, it is not possi-
ble to use them for gold standard evaluation of our
system. But in order to be able to have an idea
about the performance of the previous approaches
regarding lexicon lookup, we decided to automat-
ically align this set and create a baseline lexicon
lookup model for comparison purposes. (see the
details in Section 5.3).

5.2 Evaluation Method
We evaluated our work both for ill formed word
detection and candidate generation separately. For
ill formed word detection, we provide precision
(P), recall (R), f-measure (F) and accuracy (Acc.)
scores. For candidate generation, we provide only
the accuracy scores (the number of correctly nor-
malized tokens over the total number of detected
ill formed words).

5.3 Compared Models
To the best of our knowledge this study is the
first attempt for the normalization of Turkish so-
cial media data. Since there are only spelling cor-
rector systems available for the task we compared
the proposed model with them. In other words, we
compared 3 different models with our proposed
system:
Model 1 (MsWord) is the model where we use an
api for getting the MsWord Turkish spelling sug-
gestions. Although this is not a tool developed for
normalization purposes we wanted to see its suc-
cess on our data sets. We accepted the top best
suggestion as the normalized version for the input
tokens.
Model 2 (Zemberek) (Akın and Akın, 2007) is also
an open source spelling corrector for Turkish.
Model 3 (Lookup Table) is a model that we devel-
oped with the aim of creating a baseline lookup
approach for comparison. For this purpose, we

first used GIZA++ (Och and Ney, 2000) in order
to automatically align the normalized tweets (us-
ing the 4,049 tweets’ data set presented in Sec-
tion 5.1) and created a lookup table with the pro-
duced aligned token sequences. We then used this
lookup table to check for the existence of each ill
formed word and get its normalized counterpart.

6 Experimental Results

Table 7 and Table 8 gives the results of the ill
formed word detection for different systems for
the validation set and the test set consecutively. In
these experiments, we do not provide the results of
the “Lookup Table” model since the ill formed de-
tection part of it is exactly the same with our pro-
posed model. For MsWord and Zemberek we con-
sidered each modified word as an ill formed word
detected by that system. We can see from the ta-
bles that our proposed model has an f-measure of
ill formed word detection 0.78. As it is explained
in Section 4.1, our ill formed word detection ap-
proach is very straightforward and it uses only a
morphological analyzer and an abbreviation list
in order to detect OOV words. Thus, one may
wonder why the scores for the proposed model
are not very close to 1 although it outperforms
all of its available rivals. This is because, there
exists nearly 20% of the ill formed tokens which
are not suspended to our morphological filter al-
though they are manually annotated as ill formed
by human annotators. This is certainly possible
for morphologically rich languages since a word
surface form may be the valid analysis of many
stems. The ill formed word “çalışıcım” is a good
example for this situation. Although this word
will be understood by most of the people as the ill
formed version of the word “çalışacağım” (I’m go-
ing to work), it is considered by the morphological
analyzer as a valid Turkish word since although
very rare, it could also be the surface form of
the word “çalış” with additional derivational and
inflectional suffixes “çalış+ıcı+m” meaning “my
worker”.

Systems P R F Acc.
MsWord 0.25 0.59 0.35 0.58
Zemberek 0.21 0.17 0.19 0.21
Proposed Model 0.75 0.81 0.78 0.80

Table 7: Ill Formed Word Detection Evaluation
Results on Validation Set

68



Systems P R F Acc.
MsWord 0.24 0.19 0.21 0.56
Zemberek 0.11 0.29 0.20 0.11
Proposed Model 0.71 0.72 0.71 0.86

Table 8: Ill Formed Word Detection Evaluation
Results on Test Set

Data Set Systems Accuracy
MsWord 0.25

Validation Set Zemberek 0.21
Lookup Table 0.34
Proposed Model 0.75
MsWord 0.24

Test Set Zemberek 0.11
Lookup Table 0.31
Proposed Model 0.71

Table 9: Candidate Generation Results on Data
Sets

Table 9 gives the evaluation scores of each dif-
ferent system for both the validation and test data
sets. Although the lookup model is very basic,
one can observe from the table that it outperforms
both MsWord and Zemberek. Our proposed iter-
ative model obtains the highest scores (75% for
validation and 71% for test sets) with a relative
improvement of 40 percentage points over the lex-
icon lookup baseline.

7 Conclusion

In this paper we presented a cascaded normaliza-
tion model for Turkish which could also be applied
to the morphologically rich languages with appro-
priate NLP tools. The model has two main parts:
ill formed word detection and candidate word gen-
eration consisting of 7 normalization stages (let-
ter case transformation, replacement rules & lex-
icon lookup, proper noun detection, deasciifica-
tion, vowel restoration, accent normalization and
spelling correction) executed sequentially one on
top of the other one. We present the first and high-
est results for Turkish text normalization6 of so-
cial media data with a 86% accuracy of ill formed
word detection and 71% accuracy for candidate
word generation. A morphological analyzer is
used for the detection of ill formed words. But
we believe the accuracy of this first detection stage

6The produced test sets and the Web interface of the
Turkish Normalizer is available via http://tools.nlp.itu.edu.tr
(Eryiğit, 2014)

may be improved by the addition of a lexicon
lookup (before the morphological filter) consisting
the most frequent normalization cases extracted
from manually normalized data if available. Thus,
as a future work we plan to extend our work both
on the ill formed word detection and on the cre-
ation of a spelling corrector with social web data
in focus.

Acknowledgment

This work is part of our ongoing research project
“Parsing Turkish Web 2.0 Sentences” supported
by ICT COST Action IC1207 TUBITAK 1001
(grant no: 112E276). The authors want to thank
Turkcell Global Bilgi for sharing the manually
normalized data of user comments from the Tele-
com domain. We also want to thank Ozan Arkan
Can for his valuable discussions and helps during
the data preparation.

References
Kübra Adalı and Gülşen Eryiğit. 2014. Vowel and

diacritic restoration for social media texts. In 5th
Workshop on Language Analysis for Social Media
(LASM) at EACL, Gothenburg, Sweden, April. As-
sociation for Computational Linguistics.

Ahmet Afsin Akın and Mehmet Dündar Akın. 2007.
Zemberek, an open source nlp framework for turkic
languages. Structure.

AiTi Aw, Min Zhang, Juan Xiao, and Jian Su. 2006.
A phrase-based statistical model for sms text nor-
malization. In Proc. of the COLING/ACL on
Main conference poster sessions, COLING-ACL
’06, pages 33–40, Stroudsburg, PA, USA. Associ-
ation for Computational Linguistics.

Richard Beaufort, Sophie Roekhaut, Louise-Amélie
Cougnon, and Cédrick Fairon. 2010. A hybrid
rule/model-based finite-state framework for normal-
izing sms messages. In Proc. of the 48th Annual
Meeting of the Association for Computational Lin-
guistics, ACL ’10, pages 770–779, Stroudsburg, PA,
USA. Association for Computational Linguistics.

Eleanor Clark and Kenji Araki. 2011. Text normal-
ization in social media: progress, problems and ap-
plications for a pre-processing system of casual en-
glish. Procedia-Social and Behavioral Sciences,
27:2–11.

Paul Cook and Suzanne Stevenson. 2009. An
unsupervised model for text message normaliza-
tion. In Proc. of the Workshop on Computational
Approaches to Linguistic Creativity, CALC ’09,
pages 71–78, Stroudsburg, PA, USA. Association
for Computational Linguistics.

69



Jacob Eisenstein. 2013. Phonological factors in social
media writing. In Proc. of the Workshop on Lan-
guage Analysis in Social Media, pages 11–19, At-
lanta, Georgia, June. Association for Computational
Linguistics.

Gülşen Eryiğit, Fatih Samet Çetin, Meltem Yanık,
Tanel Temel, and İyas Çiçekli. 2013. Turksent:
A sentiment annotation tool for social media. In
Proc. of the 7th Linguistic Annotation Workshop
and Interoperability with Discourse, pages 131–134,
Sofia, Bulgaria, August. Association for Computa-
tional Linguistics.

Gülşen Eryiğit. 2014. ITU Turkish NLP web service.
In Proc. of the Demonstrations at the 14th Confer-
ence of the European Chapter of the Association
for Computational Linguistics (EACL), Gothenburg,
Sweden, April. Association for Computational Lin-
guistics.

Dilek Z. Hakkani-Tür, Kemal Oflazer, and Gökhan Tür.
2000. Statistical morphological disambiguation for
agglutinative languages. In Proc. of the 18th confer-
ence on Computational linguistics - Volume 1, COL-
ING ’00, pages 285–291, Stroudsburg, PA, USA.
Association for Computational Linguistics.

Bo Han and Timothy Baldwin. 2011. Lexical normali-
sation of short text messages: Makn sens a #twitter.
In Proc. of the 49th ACL HLT, pages 368–378, Port-
land, Oregon, USA, June. Association for Computa-
tional Linguistics.

Hany Hassan and Arul Menezes. 2013. Social text nor-
malization using contextual graph random walks. In
Proc. of the 51st ACL, pages 1577–1586, Sofia, Bul-
garia, August. Association for Computational Lin-
guistics.

Yuxiang Jia, Dezhi Huang, Wu Liu, Shiwen Yu, and
Haila Wang. 2008. Text normalization in Mandarin
text-to-speech system. In ICASSP, pages 4693–
4696. IEEE.

Max Kaufmann and Jugal Kalita. 2010. Syntactic nor-
malization of Twitter messages. In Proc. of the 8th
International Conference on Natural Language Pro-
cessing (ICON 2010), Chennai, India. Macmillan In-
dia.

Osama A Khan and Asim Karim. 2012. A rule-based
model for normalization of sms text. In Tools with
Artificial Intelligence (ICTAI), 2012 IEEE 24th In-
ternational Conference on, volume 1, pages 634–
641. IEEE.

Fei Liu, Fuliang Weng, and Xiao Jiang. 2012. A
broad-coverage normalization system for social me-
dia language. In Proc. of the 50th ACL, pages 1035–
1044, Stroudsburg, PA, USA. Association for Com-
putational Linguistics.

Thu-Trang Thi Nguyen, Thanh Thi Pham, and Do-Dat
Tran. 2010. A method for vietnamese text normal-
ization to improve the quality of speech synthesis.

In Proc. of the 2010 Symposium on Information and
Communication Technology, SoICT ’10, pages 78–
85, New York, NY, USA. ACM.

Franz Josef Och and Hermann Ney. 2000. Giza++:
Training of statistical translation models.

K Panchapagesan, Partha Pratim Talukdar, N Sridhar
Krishna, Kalika Bali, and AG Ramakrishnan. 2004.
Hindi text normalization. In Fifth International
Conference on Knowledge Based Computer Systems
(KBCS), pages 19–22. Citeseer.

Deana Pennell and Yang Liu. 2011. A character-level
machine translation approach for normalization of
sms abbreviations. In IJCNLP, pages 974–982.

Muhammet Şahin, Umut Sulubacak, and Gülşen
Eryiğit. 2013. Redefinition of turkish morphology
using flag diacritics. In Proc. of The Tenth Sym-
posium on Natural Language Processing (SNLP-
2013), Phuket, Thailand, October.

Haşim Sak, Tunga Güngör, and Murat Saraçlar. 2011.
Resources for Turkish morphological processing.
Lang. Resour. Eval., 45(2):249–261, May.

Ruhi Sarikaya, Katrin Kirchhoff, Tanja Schultz, and
Dilek Hakkani-Tur. 2009. Introduction to the spe-
cial issue on processing morphologically rich lan-
guages. Trans. Audio, Speech and Lang. Proc.,
17(5):861–862, July.

Bilge Say, Deniz Zeyrek, Kemal Oflazer, and Umut
Özge. 2002. Development of a corpus and a tree-
bank for present-day written Turkish. In Proc. of the
Eleventh International Conference of Turkish Lin-
guistics, Famaguste, Cyprus, August.

Gökhan Akın Şeker and Gülşen Eryiğit. 2012. Initial
explorations on using CRFs for Turkish named en-
tity recognition. In Proc. of COLING 2012, Mum-
bai, India, 8-15 December.

Reut Tsarfaty, Djamé Seddah, Sandra Kübler, and
Joakim Nivre. 2013. Parsing morphologically rich
languages: Introduction to the special issue. Com-
putational Linguistics, 39(1):15–22.

Pidong Wang and Hwee Tou Ng. 2013. A beam-
search decoder for normalization of social media
text with application to machine translation. In
Proc. of NAACL-HLT, pages 471–481.

Deniz Yüret and Michael de la Maza. 2006. The
greedy prepend algorithm for decision list induc-
tion. In Proc. of the 21st international conference
on Computer and Information Sciences, ISCIS’06,
pages 37–46, Berlin, Heidelberg. Springer-Verlag.

Congle Zhang, Tyler Baldwin, Howard Ho, Benny
Kimelfeld, and Yunyao Li. 2013. Adaptive parser-
centric text normalization. In Proc. of the 51st ACL,
pages 1159–1168, Sofia, Bulgaria, August. Associa-
tion for Computational Linguistics.

70


