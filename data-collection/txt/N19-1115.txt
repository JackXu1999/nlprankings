




































Cooperative Learning of Disjoint Syntax and Semantics


Proceedings of NAACL-HLT 2019, pages 1118–1128
Minneapolis, Minnesota, June 2 - June 7, 2019. c©2019 Association for Computational Linguistics

1118

Cooperative Learning of Disjoint Syntax and Semantics

Serhii Havrylov ∗
ILCC, University of Edinburgh / Edinburgh, UK

s.havrylov@ed.ac.uk

Germán Kruszewski & Armand Joulin
Facebook AI Research

{germank,ajoulin}@fb.com

Abstract
There has been considerable attention devoted
to models that learn to jointly infer an ex-
pression’s syntactic structure and its seman-
tics. Yet, Nangia and Bowman (2018) has re-
cently shown that the current best systems fail
to learn the correct parsing strategy on math-
ematical expressions generated from a sim-
ple context-free grammar. In this work, we
present a recursive model inspired by Choi
et al. (2018) that reaches near perfect accu-
racy on this task. Our model is composed
of two separated modules for syntax and se-
mantics. They are cooperatively trained with
standard continuous and discrete optimisation
schemes. Our model does not require any lin-
guistic structure for supervision, and its re-
cursive nature allows for out-of-domain gen-
eralisation. Additionally, our approach per-
forms competitively on several natural lan-
guage tasks, such as Natural Language Infer-
ence and Sentiment Analysis.

1 Introduction

Standard linguistic theories propose that natural
language is structured as nested constituents or-
ganised in the form of a tree (Partee et al., 1990).
However, most popular models, such as the Long
Sort-Term Memory network (LSTM) (Hochreiter
and Schmidhuber, 1997), process text without im-
posing a grammatical structure. To bridge this gap
between theory and practice models that process
linguistic expressions in a tree-structured manner
have been considered in recent work (Socher et al.,
2013; Tai et al., 2015; Zhu et al., 2015; Bowman
et al., 2016). These tree-based models explicitly
require access to the syntactic structure for the
text, which is not entirely satisfactory.

Indeed, parse tree level supervision requires a
significant amount of annotations from expert lin-

∗Work done while the author was an intern at Facebook
AI Research.

guists. These trees have been annotated with dif-
ferent goals in mind than the tasks we are using
them for. Such discrepancy may result in a de-
terioration of the performance of models relying
on them. Recently, several attempts were made
to learn these models without explicit supervi-
sion for the parser (Yogatama et al., 2016; Mail-
lard et al., 2017; Choi et al., 2018). However,
Williams et al. (2018a) has recently shown that the
structures learned by these models cannot be as-
cribed to discovering meaningful syntactic struc-
ture. These models even fail to learn the simple
context-free grammar of nested mathematical op-
erations (Nangia and Bowman, 2018).

In this work, we present an extension of Choi
et al. (2018), that successfully learns these simple
grammars while preserving competitive perfor-
mance on several standard linguistic tasks. Con-
trary to previous work, our model makes a clear
distinction between the parser and the composi-
tional function. These two modules are trained
with different algorithms, cooperating to build a
semantic representation that optimises the objec-
tive function. The parser’s goal is to generate
a tree structure for the sentence. The composi-
tional function follows this structure to produce
the sentence representation. Our model contains
a continuous component, the compositional func-
tion, and a discrete one, the parser. The whole
system is trained end-to-end with a mix of rein-
forcement learning and gradient descent. Droz-
dov and Bowman (2017) has noticed the difficulty
of mixing these two optimisation schemes without
one dominating the other. This typically leads to
the “coadaptation problem” where the parser sim-
ply follows the compositional function and fails to
produce meaningful syntactic structures. In this
work, we show that this pitfall can be avoided
by synchronising the learning paces of the two
optimisation schemes. This is achieved by com-



1119

bining several recent advances in reinforcement
learning. First, we use input-dependent control
variates to reduce the variance of our gradient esti-
mates (Ross, 1997). Then, we apply multiple gra-
dient steps to the parser’s policy while controlling
for its learning pace using the Proximal Policy Op-
timization (PPO) of Schulman et al. (2017). The
code for our model is publicly available1.

2 Preliminaries

In this section, we present existing works on Re-
cursive Neural Networks and their training in the
absence of supervision on the syntactic structures.

2.1 Recursive Neural Networks

A Recursive Neural Network (RvNN) has
its architecture defined by a directed acyclic
graph (DAG) given alongside with an input se-
quence (Goller and Kuchler, 1996). RvNNs are
commonly used in NLP to generate sentence rep-
resentation that leverages available syntactic infor-
mation, such as a constituency or a dependency
parse trees (Socher et al., 2011).

Given an input sequence and its associ-
ated DAG, a RvNN processes the sequence by ap-
plying a transformation to the representations of
the tokens lying on the lowest levels of the DAG.
This transformation, or compositional function,
merges these representations into representations
for the nodes on the next level of the DAG. This
process is repeated recursively along the graph
structure until the top-level nodes are reached. In
this work, we assume that the compositional func-
tion is the same for every node in the graph.

Tree-LSTM. We focus on a specific type
of RvNNs, the tree-based long short-term memory
network (Tree-LSTM) of Tai et al. (2015) and Zhu
et al. (2015). Its compositional function general-
izes the LSTM cell of Hochreiter and Schmidhu-
ber (1997) to tree-structured topologies, i.e.,

z
i
fl
fr
o

 =

tanh
σ
σ
σ
σ


(
R

[
hl
hr

]
+ b

)
,

cp = z� i+ cl � fl + cr � fr,
hp = tanh(cp)� o,

1https://github.com/facebookresearch/
latent-treelstm

where σ and tanh are the sigmoid and hyperbolic
tangent functions. Tree-LSTM cell is differen-
tiable with respect to its recursion matrix R, bias b
and its input. The gradients of a Tree-LSTM can
thus be computed with backpropagation through
structure (BPTS) (Goller and Kuchler, 1996).

2.2 Learning with RvNNs

A tree-based RvNN is a function fθ parameter-
ized by a d dimensional vector θ that predicts an
output y given an input x and a tree t. Given a
dataset D of N triplets (x, t, y), the parameters of
the RvNN are learned with the following minimi-
sation problem:

min
θ∈Rd

1

N

∑
(x,t,y)∈D

`(fθ(x, t), y), (1)

where ` is a logistic regression function. These
models need an externally provided parsing tree
for each input sentence during both training and
evaluation. Alternatives, such as the shift-reduce-
based SPINN model of Bowman et al. (2016),
learn an internal parser from the given trees. While
these solutions do not need external trees during
evaluation, they still require tree level annotations
for training. More recent work has focused on
learning a latent parser with no direct supervision.

2.3 Latent tree models

Latent tree models aim at jointly learning the com-
positional function fθ and a parser without super-
vision on the syntactic structures (Yogatama et al.,
2016; Maillard et al., 2017; Choi et al., 2018). The
latent parser is defined as a parametric probabil-
ity distribution over trees conditioned on the in-
put sequence. The parameters of this tree distribu-
tion pφ(.|x) are represented by a vector φ. Given
a dataset D of pairs of input sequences x and out-
puts y, the parameters θ and φ are jointly learned
by minimising the following objective function:

min
θ,φ
L(θ, φ) = 1

N

∑
(x,y)

`(Eφ[fθ(x, t)], y), (2)

where Eφ is the expectation with respect to
the pφ(.|x) distribution. Directly minimising this
objective function is often difficult due to ex-
pensive marginalisation of the unobserved trees.
Hence, when ` is a convex function (e.g. cross
entropy of an exponential family) usually an up-
per bound of Eq. (2) can be derived by applying



1120

Jensen’s inequality:

L̂(θ, φ) = 1
N

∑
(x,y)

Eφ[`(fθ(x, t), y)]. (3)

Learning a distribution over a set of discrete items
involves a discrete optimisation scheme. For ex-
ample, the RL-SPINN model of Yogatama et al.
(2016) uses a mix of gradient descent for θ
and REINFORCE for φ (Williams et al., 2018a).
Drozdov and Bowman (2017) has recently ob-
served that this optimisation strategy tends to pro-
duce poor parsers, e.g., parsers that only generate
left-branching trees. The effect, called the coad-
aptation issue, is caused by both bias in the pars-
ing strategy and a difference in convergence paces
of continuous and discrete optimisers. Typically,
the parameters θ are learned more rapidly than φ.
This limits the exploration of the search space to
parsing strategies similar to those found at the be-
ginning of the training.

2.3.1 Gumbel Tree-LSTM

In their Gumbel Tree-LSTM model, Choi et al.
(2018) propose an alternative parsing strategy to
avoid the coadaptation issue. Their parser incre-
mentally merges a pair of consecutive constituents
until a single one remains. This strategy reduces
the bias towards certain tree configurations ob-
served with RL-SPINN.

Each word i of the input sequence is represented
by an embedding vector. A leaf transformation
maps this vector to pair of vectors r0i=(h

0
i , c

0
i ).

We considered three types of leaf transforma-
tions: affine transformation, LSTM and bidirec-
tional LSTM. The resulting representations form
the initial states of the Tree-LSTM. In the ab-
sence of supervision, the tree is built in a bottom-
up fashion by recursively merging consecutive
constituents (i, i + 1) based on merge-candidate
scores. On each level k of the bottom-up deriva-
tion, the merge-candidate score of the pair (i, i+1)
is computed as follow:

sk(i) = 〈q,Tree-LSTM(rki , rki+1)〉,

where q is a trainable query vector and rki is
the constituent representation at position i after k
mergings. We merge a pair (i∗, i∗ + 1) sam-
pled from the Categorical distribution built on the
merge-candidate scores. The representations of

the constituents are then updated as follow:

rk+1i =


rki , i < i

∗,

Tree-LSTM(rki , r
k
i+1) i = i

∗,

rki+1 i > i
∗.

This procedure is repeated until one constituent re-
mains. Its hidden state is the input sentence rep-
resentation. This procedure is non-differentiable.
Choi et al. (2018) use an approximation based on
the Gumbel-Softmax distribution (Maddison et al.,
2016; Jang et al., 2016) and the reparametrization
trick (Kingma and Welling, 2013).

This relaxation makes the problem differen-
tiable at the cost of a bias in the gradient esti-
mates (Jang et al., 2016). This difference between
the real objective function and their approxima-
tion could explain why their method cannot re-
cover simple context-free grammars (Nangia and
Bowman, 2018). We investigate this question by
proposing an alternative optimisation scheme that
directly aims for the correct objective function.

3 Our model

We consider the problem defined in Eq. (3) to
jointly learn a composition function and an in-
ternal parser. Our model is composed of the
parser of Choi et al. (2018) and the Tree-LSTM
for the composition function. As suggested in past
work (Mnih et al., 2016; Schulman et al., 2017),
we added an entropy H over the tree distribution
to the objective function:

min
θ, φ
L̂(θ, φ)− λ

∑
x

H(t | x), (4)

where λ > 0. This regulariser improves explo-
ration by preventing early convergence to a subop-
timal deterministic parsing strategy. The new ob-
jective function is differentiable with respect to θ,
but not φ, the parameters of the parser. Learning θ
follows the same procedure with BPTS as if the
tree would be externally given.

In the rest of this section, we discuss the opti-
mization of the parser and a cooperative training
strategy to reduce the coadaptation issue.

3.1 Unbiased gradient estimation
We cast the training of the parser as a reinforce-
ment learning problem. The parser is an agent
whose reward function is the negative of the loss
function defined in Eq. (3). Its action space is the



1121

space of binary trees. The agent’s policy is a prob-
ability distribution over binary trees that decom-
poses as a sequence of K merging actions:

pφ(t|x) =
K∏
k=0

πφ(a
i
k|rk), (5)

where rk = (rk0, . . . , r
k
K−k). The loss func-

tion is optimised with respect to φ with REIN-
FORCE (Williams, 1992). REINFORCE requires
a considerable number of random samples to ob-
tain a gradient estimate with a reasonable level
of variance. This number is positively correlated
with the size of the search space, which is expo-
nentially large in the case of binary trees. We con-
sider several extensions of REINFORCE to cir-
cumvent this problem.

Variance reduction. An alternative solution to
increasing the number of samples is the control
variates method (Ross, 1997). It takes advantage
of random variables with known expected values
and positive correlation with the quantity whose
expectation is tried to be estimated. Given an
input-output pair (x, y) and tree t sampled from
pφ(t|x) , let’s define the random variable G as:

G(t) = `(fθ(x, t), y)
∂log pφ(t|x)

∂φ
. (6)

According to REINFORCE, calculating the gra-
dient with respect to φ for the pair (x, y) is then
equivalent to determining the unknown mean of
the random variable G(t)2. Let’s assume there
is a control variate, i.e., a random variable b(t)
that positively correlates with G and has known
expected value with respect to pφ(.|x). Given N
samples of the G(t) and the control variate b(t),
the new gradient estimator is:

GCV = Epφ(t|x)[b(t)]+
1

N

[
N∑
i=1

(G(ti)− b(ti))

]
.

A popular control variate, or baseline, used in
REINFORCE is the moving average of recent
rewards multiplied by the score function (Ross,
1997):

b(t) = c∇φ log pφ(t|x).

It has a zero mean under the pφ(.|x) distribution
and it positively correlates with G(t).

2Note that while we are computing the gradients using
`, we could also directly optimise the parser with respect to
downstream accuracy.

Surrogate loss. REINFORCE often is imple-
mented via a surrogate loss defined as follow:

Êt [rφ(t)`(fθ(x, t), y)] , (7)

where Êt is the empirical average over a finite
batch of samples and rφ(t) =

pφ(t|x)
pφold (t|x)

is the prob-
ability ratio with φold standing for the parameters
before the update.

Input-dependent baseline. The moving aver-
age baseline cannot detect changes in rewards
caused by structural differences in the inputs. In
our case, a long arithmetic expression is much
harder to parse than a short one, systematically
leading to their lower rewards. This structural dif-
ferences in the rewards aggravate the credit as-
signment problem by encouraging REINFORCE
to discard actions sampled for longer sequences
even though there might be some subsequences of
actions that produce correct parsing subtrees.

A solution is to make the baseline input-
dependent. In particular, we use the self-critical
training (SCT) baseline of Rennie et al. (2017),
defined as:

b(t, x) = cθ,φ(x)∇φ log pφ(t | x),

where cθ,φ is the reward obtained with the policy
used at test time, i.e., t̂ = argmax pφ(t|x). This
control variate has a zero mean under the pφ(t|x)
distribution and correlates positively with the gra-
dients. Computing the argmax of a policy among
all possible binary trees has exponential complex-
ity. We replace it with a simpler greedy decoding,
i.e, a tree t is selected by following a sequence of
greedy actions âk:

âk = argmaxπφ(ak | r̂k).

This approximation is very efficient and comput-
ing the baseline requires only one additional for-
ward pass.

Gradient normalization. We empirically ob-
serve significant fluctuations in the gradient
norms. This creates instability that can not be
reduced by additive terms, such as the input-
dependent baselines. A solution is to divide the
gradients by a coarse approximation of their norm,
e.g., a running estimate of the reward standard de-
viation (Mnih and Gregor, 2014). This trick en-
sures that the rewards remain approximately in the
unit ball, making the learning process less sensi-
tive to steep changes in the loss.



1122

3.2 Synchronizing syntax and semantics
learning with PPO

The gradients of the loss function from the Eq. (4)
are calculated using two different schemes, BPST
for the composition function parameters θ and RE-
INFORCE for the parser parameters φ. Then, both
are updated with SGD. The estimate of the gradi-
ent with respect to φ has higher variance compared
to the estimate with respect to θ. Hence, using
the same learning rate schedule does not necessar-
ily correspond to the same real pace of learning.
It is φ parameters that are harder to optimise, so
to improve training stability and convergence it is
reasonable to aim for such updates that does not
change the policy too much or too little. A simple
yet effective solution is the Proximal Policy Opti-
mization (PPO) of Schulman et al. (2017). It con-
siders the next surrogate loss:

Êt
[
max

{
rφ(t)` (fθ(x, t), y) , r

c
φ(t)` (fθ(x, t), y)

}]
,

Where rcφ(t) = clip (rφ(t), 1− �, 1 + �) and �
is a real number in (0; 0.5]. The first argument
of the max is the surrogate loss for REINFORCE.
The clipped ratio in the second argument disincen-
tivises the optimiser from performing updates re-
sulting in large tree probability changes. With this,
the policy parameters can be optimised with re-
peated K steps of SGD to ensure a similar “pace”
of learning between the parser and the composi-
tional function.

4 Related work

Besides the works mentioned in Sec. 2 and Sec. 3,
there is a vast literature on learning latent parsers.
Early connectionist work in inferring context-free
grammars proposed stack-augmented models and
relied on explicit supervision on the strings that
belonged to the target language and those that did
not (Giles et al., 1989; Sun, 1990; Das et al., 1992;
Mozer and Das, 1992). More recently, new stack-
augmented models were shown to learn latent
grammars from positive evidence alone (Joulin
and Mikolov, 2015). In parallel to these, other sta-
tistical approaches were proposed to automatically
induce grammars from unparsed text (Sampson,
1986; Magerman and Marcus, 1990; Carroll and
Charniak, 1992; Brill, 1993; Klein and Manning,
2002). Our work departs from these approaches
in that we aim at learning a latent grammar in the
context of performing some given task.

Socher et al. (2011) uses a surrogate auto-
encoder objective to search for a constituency
structure, merging nodes greedily based on the re-
construction loss. Maillard et al. (2017) defines
a relaxation of a CYK-like chart parser that is
trained for a particular task. A similar idea is in-
troduced in Le and Zuidema (2015) where an au-
tomatic parser prunes the chart to reduce the over-
all complexity of the algorithm. Another strat-
egy, similar in nature, has been recently proposed
by Corro and Titov (2018), where Gumbel noise
is used with differentiable dynamic programming
to generate dependency trees. In contrast, Yo-
gatama et al. (2016) learns a Shift-Reduce parser
using reinforcement learning. Maillard and Clark
(2018) further proposes a beam search strategy to
overcome learning trivial trees. On a different
vein, Vlad Niculae (2018) proposes a quadratic
penalty term over the posterior distribution of non-
projective dependency trees to enforce sparsity of
the relaxation. Finally, there is a large body of
work in Reinforcement Learning that aims at dis-
covering how to combine elementary modules to
solve complex tasks (Singh, 1992; Chang et al.,
2018; Sahni et al., 2017). Due to the limited space,
we will not discuss them in further details.

5 Experiments

We conducted experiments on three different
tasks: evaluating mathematical expressions on
the ListOps dataset (Nangia and Bowman, 2018),
sentiment analysis on the SST dataset (Socher
et al., 2013) and natural language inference task
on the SNLI (Bowman et al., 2015) and MultiNLI
(Williams et al., 2018b) datasets.

Technical details. For ListOps, we follow the
experimental protocol of Nangia and Bowman
(2018), i.e., a 128 dimensional model and a ten-
way softmax classifier. However, we replace their
multi-layer perceptron (MLP) by a linear classi-
fier. The validation set is composed of 1k ex-
amples randomly selected from the training set.
For SST and NLI, we follow the setup of Choi
et al. (2018): we initialise the word vectors with
GloVe300D (Pennington et al., 2014) and train
an MLP classifier on the sentence representations.
The hyperparameters are selected on the valida-
tion set using 5 random seeds for each configura-
tion. Our hyperparameters are the learning rate,
weight decay, the regularisation parameter λ, the
leaf transformations, variance reduction hyperpa-



1123

No baseline Moving average Self critical

No PPO PPO No PPO PPO No PPO PPO

min 61.7 61.4 61.7 59.4 63.7 98.2
max 70.1 76.6 74.3 96.0 64.1 99.6
mean± std 66.2 ±3.2 66.5 ±5.9 65.5 ± 4.7 67.5 ±14.3 64.0 ±0.1 99.2 ±0.5

Table 1: Accuracy on ListOps test set for our model with three different baselines, with and without PPO. We use
K = 15 for PPO.

Model Accuracy

LSTM* 71.5±1.5
RL-SPINN* 60.7±2.6
Gumbel Tree-LSTM* 57.6±2.9

Ours 99.2±0.5

Table 2: Accuracy on the ListOps dataset. All models
have 128 dimensions. Results for models with * are
taken from Nangia and Bowman (2018).

rameters and the number of updatesK in PPO. We
use an adadelta optimizer (Zeiler, 2012).

5.1 ListOps

The ListOps dataset probes the syntax learning
ability of latent tree models (Nangia and Bow-
man, 2018). It is designed to have a single cor-
rect parsing strategy that a model must learn in
order to succeed. It is composed of prefix arith-
metic expressions and the goal is to predict the
numerical output associated with the evaluation
of the expression. The sequences are made of
integers in [0, 9] and 4 operations: MIN, MAX,
MED and SUM MOD. The output is an integer
in the range [0, 9]. For example, the expression
[MIN 2 [MAX 0 1] [MIN 6 3 ] 5 ] is
mapped to the output 1. The ListOps task is thus
a sequence classification problem with 10 classes.
There are 90k training examples and 10k test ex-
amples. It is worth mentioning that the underly-
ing semantic of operations and symbols is not pro-
vided. In other words, a model has to infer from
examples that [MIN 0 1] = 0.

As shown in Table 2, the current leading la-
tent tree models are unable to learn the correct
parsing strategy on ListOps (Nangia and Bowman,
2018). They even achieve performance worse than
purely sequential recurrent networks. On the other
hand, our model achieves near perfect accuracy on

this task, suggesting that our model is able to dis-
cover the correct parsing strategy. Our model dif-
fers in several ways from the Gumbel Tree-LSTM
of Choi et al. (2018) that could explain this gap
in performance. In the rest of this section, we per-
form an ablation study on our model to understand
the importance of each of these differences.

Impact of the baseline and PPO. We report the
impact of our design choices on the performance
in Table 1. Our model without baseline nor PPO
is vanilla REINFORCE. The baselines only im-
prove performance when PPO is used. Further-
more, these ablated models without PPO perform
on-par with the RL-SPINN model (see Table 2).
This confirms our expectations for models that fail
to synchronise syntax and semantics learning.

Interestingly, using PPO has a positive impact
on both baselines, but accuracy remains low with
the moving average baseline. The reduction of
variance induced by the SCT baseline leads to a
near-perfect recovery of the good parsing strategy
in all five experiments. This shows the importance
of this baseline for the stability of our approach.

Sensitivity to hyperparameters. Our model is
relatively robust to hyperparameters changes when
we use the SCT baseline and PPO. For example,
changing the leaf transformation or dimensionality
of the model has a minor impact on performance.
However, we have observed that the choice of the
optimiser has a significant impact. For example,
the average performance drops to 73.0% if we re-
place Adadelta by Adam (Kingma and Ba, 2014).
Yet, the maximum value out of 5 runs remains rel-
atively high, 99.0%.

Untied parameters. As opposed to previous
work, the parameters of the parser and the compo-
sition function are not tied in our model. Without
this separation between syntax and semantics, it
would be impossible to update one module with-



1124

Figure 1: Blue crosses depict an average accuracy
of five models on the test examples that have lengths
within certain range. Black circles illustrate individual
models.

out changing the other. The gradient direction is
then dominated by the low variance signal from
the semantic component, making it hard to learn
the parser. We confirmed experimentally that our
model with tied parameters fails to find the correct
parser and its accuracy drops to 64.7%.

Extrapolation and Grammaticality. Recursive
models have the potential to generalise to any se-
quence length. Our model was trained with se-
quences of length up to 130 tokens. We test the
ability of the model to generalise to longer se-
quences by generating additional expressions of
lengths 200 to 1000. As shown in Fig.1, our
model has a little loss in accuracy as the length
increases to ten times the maximum length seen
during training.

On the other hand, we notice that final represen-
tations produced by the parser are very similar to
each other. Indeed, the cosine similarity between
these vectors for the test set has a mean value of
0.998 with a standard deviation of 0.002. There
are two possible explanations for this observation:
either our model assigns similar representations to
valid expressions, or it produces a trivial uninfor-
mative representation regardless of the expression.
To verify which explanation is correct, we gener-
ate ungrammatical expressions by removing either
one operation token or one closing bracket sym-
bol for each sequence in the test set. As shown in
Figure 2, in contrast to grammatical expressions,
ungrammatical ones tend to be very different from
each other: “Happy families are all alike; every
unhappy family is unhappy in its own way.” The
only exception, marked by a mode near 1, come

Figure 2: The distributions of cosine similarity for el-
ements from the different sets of mathematical expres-
sions. A logarithmic scale is used for y-axis.

from ungrammatical expressions that represent in-
complete expressions because of missing a clos-
ing bracket at the end. This kind of sequences
were seen by the parser during training and they
indeed have to be represented by the same vec-
tor. These observations show that our model does
not produce a trivial representation, but identifies
the rules and constraints of the grammar. More-
over, vectors for grammatical sequences are so dif-
ferent from vectors for ungrammatical ones that
you can tell them apart with 99.99% accuracy by
simply measuring their cosine similarity to a ran-
domly chosen grammatical vector from the train-
ing set. Interestingly, we have not observed a sim-
ilar signal from the vectors generated by the com-
position function. Even learning a naive classifier
between grammatical and ungrammatical expres-
sions on top of these representations achieves an
accuracy of only 75%. This suggests that most of
the syntactic information is captured by the parser,
not the composition function.

5.2 Natural Language Inference

We next evaluate our model on natural language
inference using the Stanford Natural Language
Inference (SNLI) (Bowman et al., 2015) and
MultiNLI (Williams et al., 2018b) datasets. Nat-
ural language inference consists in predicting the
relationship between two sentences which can be
either entailment, contradiction, or neutral. The
task can be formulated as a three-way classifica-
tion problem. The results are shown in Tables
3 and 4. When training the model on MultiNLI
dataset we augment the training data with the
SNLI data and use matched versions of the de-



1125

Model Dim. Acc.

Yogatama et al. (2016) 100 80.5
Maillard et al. (2017) 100 81.6
Choi et al. (2018) 100 82.6
Ours 100 84.3±0.3

Bowman et al. (2016) 300 83.2
Munkhdalai and Yu (2017) 300 84.6
Choi et al. (2018) 300 85.6
Choi et al. (2018)† 300 83.7
Choi et al. (2018)* 300 84.9± 0.1
Ours 300 85.1±0.2

Chen et al. (2017) 600 85.5
Choi et al. (2018) 600 86.0
Ours 600 84.6±0.2

Table 3: Results on SNLI. *: publicly available code
and hyperparameter optimization was used to obtain re-
sults. †: results are taken from Williams et al. (2018a)

Model Dim. Acc.

LSTM† 300 69.1
SPINN† 300 67.5
RL-SPINN† 300 67.4
Gumbel Tree-LSTM† 300 69.5

Ours 300 70.7±0.3

Table 4: Results on MultiNLI. †: results are taken from
Williams et al. (2018a).

velopment and test sets. Surprisingly, two out of
four models for MultiNLI task collapsed to left-
branching parsing strategies. This collapse can be
explained by the absence of the entropy regularisa-
tion and the small number of PPO updates K = 1,
which were determined to be optimal via hyper-
parameter optimisation. As with ListOps, using
an Adadelta optimizer significantly improves the
training of the model.

5.3 Sentiment Analysis
We evaluate our model on a sentiment classifica-
tion task using the Stanford Sentiment Treebank
(SST) of Socher et al. (2013). All sentences in
SST are represented as binary parse trees, and each
subtree of a parse tree is annotated with the corre-
sponding sentiment score. There are two versions
of the dataset, with either binary labels, “negative”
or “positive”, (SST-2) or five labels, representing
fine-grained sentiments (SST-5). As shown in Ta-

SST-2 SST-5

Sequential sentence representation
Radford et al. (2017) 91.8 52.9
McCann et al. (2017) 90.3 53.7
Peters et al. (2018) - 54.7

RvNN based models with external tree
Socher et al. (2013) 85.4 45.7
Tai et al. (2015) 88.0 51.0
Munkhdalai and Yu (2017) 89.3 53.1
Looks et al. (2017) 89.4 52.3

RvNN based models with latent tree
Yogatama et al. (2016) 86.5 -
Choi et al. (2018) 90.7 53.7
Choi et al. (2018)∗ 90.3±0.5 51.6±0.8

Ours 90.2±0.2 51.5±0.4

Table 5: Accuracy results of models on the SST. All
the numbers are from Choi et al. (2018) but ∗ where
we used their publicly available code and performed
hyperparameter optimization.

ble 5, our results are in line with previous work,
confirming the benefits of using latent syntactic
parse trees instead of the predefined syntax.

We noticed that all models trained on NLI or
sentiment analysis tasks have parsing policies with
relatively high entropy. This indicates that the al-
gorithm does not prefer any specific grammar. In-
deed, generated trees are very similar to balanced
ones. This result is in line with Shi et al. (2018)
where they observe that binary balanced tree en-
coder gets the best results on most classification
tasks.

We also compare with state-of-the-art
sequence-based models. For the most part,
these models are pre-trained on larger datasets
and fine-tuned on these tasks. Nonetheless, they
outperform recursive models by a significant
margin. Performance on these datasets is more
impacted by pre-training than by learning the
syntax. It would be interesting to see if a similar
pre-training would also improve the performance
of recursive models with latent tree learning.

6 Conclusion

In this paper, we have introduced a novel model
for learning latent tree parsers. Our approach re-
lies on a separation between syntax and semantics.
This allows dedicated optimisation schemes for



1126

each module. In particular, we found that it is im-
portant to have an unbiased estimator of the parser
gradients and to allow multiple gradient steps with
PPO. When tested on a CFG, our learned parser
generalises to sequences of any length and dis-
tinguishes grammatical from ungrammatical ex-
pressions by forming meaningful representations
for well-formed expressions. For natural language
tasks, instead, the model prefers to fall back to
trivial strategies, in line with what was previously
observed by Shi et al. (2018). Additionally, our
approach performs competitively on several real
natural language tasks. In the future, we would
like to explore further relaxation-based techniques
for learning the parser, such as REBAR (Tucker
et al., 2017) or ReLAX (Grathwohl et al., 2017).
Finally, we plan to look into applying recursive ap-
proaches to language modelling as a pre-training
step and measure if it has the same impact on
downstream tasks as sequential models.

Acknowledgments
We would like to thank Alexander Koller, Ivan
Titov, Wilker Aziz and anonymous reviewers for
their helpful suggestions and comments.

References
Samuel R. Bowman, Gabor Angeli, Christopher Potts,

and Christopher D. Manning. 2015. A large anno-
tated corpus for learning natural language inference.
In Proceedings EMNLP 2015, pages 632–642.

Samuel R. Bowman, Jon Gauthier, Abhinav Ras-
togi, Raghav Gupta, Christopher D. Manning, and
Christopher Potts. 2016. A fast unified model for
parsing and sentence understanding. In Proceedings
of the ACL 2016, Volume 1: Long Papers.

Eric Brill. 1993. Automatic grammar induction and
parsing free text: A transformation-based approach.
In Proceedings of ACL 93, pages 259–265.

Glenn Carroll and Eugene Charniak. 1992. Two exper-
iments on learning probabilistic dependency gram-
mars from corpora. Department of Computer Sci-
ence, Univ.

Michael B. Chang, Abhishek Gupta, Sergey Levine,
and Thomas L. Griffiths. 2018. Automatically com-
posing representation transformations as a means for
generalization. CoRR, abs/1807.04640.

Qian Chen, Xiaodan Zhu, Zhen-Hua Ling, Si Wei, Hui
Jiang, and Diana Inkpen. 2017. Enhanced LSTM for
natural language inference. In Proceedings of ACL
2017, Volume 1: Long Papers, pages 1657–1668.

Jihun Choi, Kang Min Yoo, and Sang-goo Lee. 2018.
Learning to compose task-specific tree structures. In
Proceedings of AAAI 2018.

Caio Corro and Ivan Titov. 2018. Differentiable
perturb-and-parse: Semi-supervised parsing with
a structured variational autoencoder. CoRR,
abs/1807.09875.

Sreerupa Das, C Lee Giles, and Guo-Zheng Sun. 1992.
Learning context-free grammars: Capabilities and
limitations of a recurrent neural network with an
external stack memory. In Proceedings of CogSci
1992, page 14.

Andrew Drozdov and Samuel Bowman. 2017. The
coadaptation problem when learning how and what
to compose. Proceedings of the 2nd Workshop on
Representation Learning for NLP.

C. Lee Giles, Guo-Zheng Sun, Hsing-Hen Chen, Yee-
Chun Lee, and Dong Chen. 1989. Higher order
recurrent networks and grammatical inference. In
Proceedings of NIPS 1989, pages 380–387.

Christoph Goller and Andreas Kuchler. 1996. Learning
task-dependent distributed representations by back-
propagation through structure. Neural Networks,
1:347–352.

Will Grathwohl, Dami Choi, Yuhuai Wu, Geoffrey
Roeder, and David K. Duvenaud. 2017. Back-
propagation through the void: Optimizing control
variates for black-box gradient estimation. CoRR,
abs/1711.00123.

Sepp Hochreiter and Jürgen Schmidhuber. 1997.
Long short-term memory. Neural computation,
9(8):1735–1780.

Eric Jang, Shixiang Gu, and Ben Poole. 2016. Cat-
egorical reparameterization with gumbel-softmax.
CoRR, abs/1611.01144.

Armand Joulin and Tomas Mikolov. 2015. Inferring
algorithmic patterns with stack-augmented recurrent
nets. In Proceedings of NIPS 2015, pages 190–198.

Diederik P. Kingma and Jimmy Ba. 2014. Adam:
A method for stochastic optimization. CoRR,
abs/1412.6980.

Diederik P. Kingma and Max Welling. 2013. Auto-
encoding variational bayes. CoRR, abs/1312.6114.

Dan Klein and Christopher D Manning. 2002. A gener-
ative constituent-context model for improved gram-
mar induction. In Proceedings of ACL 2002, pages
128–135.

Phong Le and Willem H. Zuidema. 2015. The for-
est convolutional network: Compositional distribu-
tional semantics with a neural chart and without bi-
narization. In Proceedings of EMNLP 2015, pages
1155–1164.



1127

Moshe Looks, Marcello Herreshoff, DeLesley
Hutchins, and Peter Norvig. 2017. Deep learning
with dynamic computation graphs. arXiv preprint
arXiv:1702.02181.

Chris J. Maddison, Andriy Mnih, and Yee Whye Teh.
2016. The concrete distribution: A continuous
relaxation of discrete random variables. CoRR,
abs/1611.00712.

David M Magerman and Mitchell P Marcus. 1990.
Parsing a natural language using mutual information
statistics. In AAAI, volume 90, pages 984–989.

Jean Maillard and Stephen Clark. 2018. Latent tree
learning with differentiable parsers: Shift-reduce
parsing and chart parsing. CoRR, abs/1806.00840.

Jean Maillard, Stephen Clark, and Dani Yogatama.
2017. Jointly learning sentence embeddings
and syntax with unsupervised tree-lstms. CoRR,
abs/1705.09189.

Bryan McCann, James Bradbury, Caiming Xiong, and
Richard Socher. 2017. Learned in translation: Con-
textualized word vectors. In Proceedings of NIPS
2017, pages 6294–6305.

Andriy Mnih and Karol Gregor. 2014. Neural vari-
ational inference and learning in belief networks.
arXiv preprint arXiv:1402.0030.

Volodymyr Mnih, Adrià Puigdomènech Badia, Mehdi
Mirza, Alex Graves, Timothy P. Lillicrap, Tim
Harley, David Silver, and Koray Kavukcuoglu.
2016. Asynchronous methods for deep reinforce-
ment learning. In Proceedings of ICML 2016, pages
1928–1937.

Michael Mozer and Sreerupa Das. 1992. A connection-
ist symbol manipulator that discovers the structure
of context-free languages. In Proceedings of NIPS
1992, pages 863–870.

Tsendsuren Munkhdalai and Hong Yu. 2017. Neural
tree indexers for text understanding. In Proceedings
of EACL 2017, volume 1, page 11. NIH Public Ac-
cess.

Nikita Nangia and Samuel R. Bowman. 2018. Listops:
A diagnostic dataset for latent tree learning. In Pro-
ceedings of NAACL-HLT 2018, Student Research
Workshop, pages 92–99.

Barbara BH Partee, Alice G ter Meulen, and Robert
Wall. 1990. Mathematical methods in linguistics,
volume 30. Springer Science & Business Media.

Jeffrey Pennington, Richard Socher, and Christo-
pher D. Manning. 2014. Glove: Global vectors
for word representation. In Proceedings of EMNLP
2014, pages 1532–1543.

Matthew E Peters, Mark Neumann, Mohit Iyyer, Matt
Gardner, Christopher Clark, Kenton Lee, and Luke
Zettlemoyer. 2018. Deep contextualized word rep-
resentations. arXiv preprint arXiv:1802.05365.

Alec Radford, Rafal Jozefowicz, and Ilya Sutskever.
2017. Learning to generate reviews and discovering
sentiment. arXiv preprint arXiv:1704.01444.

Steven J. Rennie, Etienne Marcheret, Youssef Mroueh,
Jarret Ross, and Vaibhava Goel. 2017. Self-critical
sequence training for image captioning. In Proceed-
ings of CVPR 2017, pages 1179–1195.

Sheldon M. Ross. 1997. Simulation (2. ed.). Statistical
modeling and decision science. Academic Press.

Himanshu Sahni, Saurabh Kumar, Farhan Tejani, and
Charles L. Isbell. 2017. Learning to compose skills.
CoRR, abs/1711.11289.

Geoffrey Sampson. 1986. A stochastic approach to
parsing. In Proceedings Coling 1986 Volume 1, vol-
ume 1.

John Schulman, Filip Wolski, Prafulla Dhariwal, Alec
Radford, and Oleg Klimov. 2017. Proximal policy
optimization algorithms. CoRR, abs/1707.06347.

Haoyue Shi, Hao Zhou, Jiaze Chen, and Lei Li. 2018.
On tree-based neural sentence modeling. CoRR,
abs/1808.09644.

Satinder P. Singh. 1992. Transfer of learning by com-
posing solutions of elemental sequential tasks. Ma-
chine Learning, 8:323–339.

Richard Socher, Jeffrey Pennington, Eric H. Huang,
Andrew Y. Ng, and Christopher D. Manning. 2011.
Semi-supervised recursive autoencoders for predict-
ing sentiment distributions. In Proceedings of
EMNLP 2011, pages 151–161.

Richard Socher, Alex Perelygin, Jean Wu, Jason
Chuang, Christopher D. Manning, Andrew Y. Ng,
and Christopher Potts. 2013. Recursive deep mod-
els for semantic compositionality over a sentiment
treebank. In Proceedings of EMNLP 2013, pages
1631–1642.

G Sun. 1990. Connectionist pushdownautomata that
learn context-free gram-mars. In Proc. IJCNN’90,
volume 1, pages 577–580.

Kai Sheng Tai, Richard Socher, and Christopher D.
Manning. 2015. Improved semantic representations
from tree-structured long short-term memory net-
works. In Proceedings of ACL 2015, Volume 1:
Long Papers, pages 1556–1566.

George Tucker, Andriy Mnih, Chris J. Maddison, John
Lawson, and Jascha Sohl-Dickstein. 2017. REBAR:
low-variance, unbiased gradient estimates for dis-
crete latent variable models. In Proceedings of NIPS
2017, pages 2624–2633.

Claire Cardie Vlad Niculae, André F. T. Martins. 2018.
Towards dynamic computation graphs via sparse la-
tent structure. CoRR, abs/1809.00653.



1128

Adina Williams, Andrew Drozdov, and Samuel R.
Bowman. 2018a. Do latent tree learning models
identify meaningful structure in sentences? TACL,
6:253–267.

Adina Williams, Nikita Nangia, and Samuel Bowman.
2018b. A broad-coverage challenge corpus for sen-
tence understanding through inference. In Proceed-
ings of NAACL-HLT 2018, Volume 1 (Long Papers),
pages 1112–1122.

Ronald J. Williams. 1992. Simple statistical gradient-
following algorithms for connectionist reinforce-
ment learning. Machine Learning, 8:229–256.

Dani Yogatama, Phil Blunsom, Chris Dyer, Edward
Grefenstette, and Wang Ling. 2016. Learning to
compose words into sentences with reinforcement
learning. CoRR, abs/1611.09100.

Matthew D. Zeiler. 2012. ADADELTA: an adaptive
learning rate method. CoRR, abs/1212.5701.

Xiao-Dan Zhu, Parinaz Sobhani, and Hongyu Guo.
2015. Long short-term memory over recursive
structures. In Proceedings of ICML 2015, pages
1604–1612.


