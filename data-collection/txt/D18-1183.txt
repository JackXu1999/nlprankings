















































Neural Multitask Learning for Simile Recognition


Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 1543–1553
Brussels, Belgium, October 31 - November 4, 2018. c©2018 Association for Computational Linguistics

1543

Neural Multitask Learning for Simile Recognition

Lizhen Liu†, Xiao Hu†, Wei Song†⇤, Ruiji Fu‡, Ting Liu§, Guoping Hu‡
†Information Engineering, Capital Normal University, Beijing, China

‡iFLYTEK Research, Beijing, China
§Harbin Institute of Technology, Harbin, China

{liz liu7480,xiaohu,wsong}@cnu.edu.cn, {rjfu, gphu}@iflytek.com, tliu@ir.hit.edu.cn

Abstract

Simile is a special type of metaphor, where
comparators such as like and as are used to
compare two objects. Simile recognition is
to recognize simile sentences and extract sim-
ile components, i.e., the tenor and the vehi-
cle. This paper presents a study of simile
recognition in Chinese. We construct an anno-
tated corpus for this research, which consists
of 11.3k sentences that contain a compara-
tor. We propose a neural network framework
for jointly optimizing three tasks: simile sen-
tence classification, simile component extrac-
tion and language modeling. The experimen-
tal results show that the neural network based
approaches can outperform all rule-based and
feature-based baselines. Both simile sentence
classification and simile component extraction
can benefit from multitask learning. The for-
mer can be solved very well, while the latter is
more difficult.

1 Introduction

A metaphor is a figure of speech that describes
an object or action in a way that isn’t literally
true. Metaphors are common in human lan-
guage. Shutova and Teufel (2010) reported that
241 among 760 sentences in an annotated corpus
contain a metaphor. The use of metaphors helps to
explain an idea or realize rhetorical effects through
an analogical procedure. Metaphor analysis has
been drawn more attention for expanding current
natural language processing (NLP) to high-level
semantic tasks (Carbonell, 1980).

Metaphors reflect creative thought of humans.
On the other hand, inferring the meaning of a
metaphor has to integrate background knowledge,
which makes it difficult to automatically recog-
nize metaphors in language. Previous work on

⇤corresponding author

metaphor recognition mainly depends on linguis-
tic cues (Goatly, 2011) and selectional preference
violation on a pair of concepts (Fass, 1991) or
their domains (Mason, 2004). The domains can
be created by knowledge bases such as WordNet
(Mason, 2004) or based on automatic clustering
(Shutova et al., 2010).

In this paper, we focus on a special type of
metaphor—simile. A simile is a figure of speech
that directly compares two things using connect-
ing words such as like, as, than in English and
“œ” or “πÇ” in Chinese. Due to the use of such
comparators, it is much easier to locate similes
compared with locating other types of metaphors.
As a result, it is possible to collect and annotate
large scale of simile sentences and investigate data
driven simile recognition. This task is to find sim-
ile sentences and extract simile components, i.e.,
the tenor and the vehicle. The mined simile struc-
tures can potentially be used to support general
metaphor analysis, where large scale training data
is lacking.

However, simile recognition is still challenging
due to the diversity of syntactic roles of a word
and the distinction between metaphorical and lit-
eral comparisons. As shown in Table 1, a sentence
containing a comparator may not trigger a simile.
It is necessary to analyze the relationship between
meanings of concepts. And It is also difficult to
define a complete set of rules to extract the objects
to be compared with high accuracy and coverage.

This paper presents an end-to-end neural net-
work framework for simile sentence recognition.
Specifically, we make following contributions:

• We build a dataset consisting of 11.3k sen-
tences containing a frequently used compara-
tor “œ” for simile recognition in Chinese,
which can support data-driven approaches.
In contrast to English, datasets on simile or



1544

1. Ÿ*[iP]tenor Óóœœœ�4[[]vehicle
This [boy]tenor is as strong as a [bull]vehicle Simile
2. Ÿ*iPóœœœ88
The boy looks like his father Literal
3. ÷ÕÕ‘‘Ñ©Ä,œœœ/J…÷�Åæ«
He patted his uncle as if telling him not to be sad Literal
4. œœœ÷Ÿ7Ñf��îÂÙ†™õ
The students like him should work even harder Literal

Table 1: Sentences that contain the comparator “œ”.

metaphor analysis are relatively less in Chi-
nese. This dataset provides a new resource
for related research. 1

• We propose a neural multitask learning
framework jointly optimizing three tasks:
simile sentence classification, simile compo-
nent extraction and language modeling. Sim-
ile classification is to determine whether a
sentence with a comparator contains a sim-
ile, without knowing exactly what the tenor
and the vehicle are. Simile component ex-
traction aims to locate the tenor and the vehi-
cle in a simile sentence. Intuitively, the two
tasks should benefit each other. We design
our model to enhance interactions between
the two tasks. We also borrow the idea of
Rei (2017) by incorporating a language mod-
eling task, which attempts to predict neigh-
bor words. All three tasks consider the whole
sentence so that rich context information is
involved.

• We conduct comprehensive experiments.
The results demonstrate that the neural end-
to-end framework is superior to feature-
based and rule-based baselines and every sin-
gle model can benefit from multitask learn-
ing. Simile sentence classification can be
solved very well, while simile component ex-
traction is more chellenging. With multi-
task learning enhanced classifier and extrac-
tor, a classification-then-extraction method
achieves the best performance for simile
component extraction.

2 Related Work

2.1 Metaphor/Simile Analysis
Metaphor analysis becomes active in recent years.
The tasks include metaphor recognition, metaphor

1The dataset is at https://github.com/cnunlp/
Chinese-Simile-Recognition-Dataset

explanation and metaphor generation (Shutova
et al., 2013; Veale, 1995; Jang et al., 2016).

Simile is a special type of metaphor with
the comparator and it is relatively easier to lo-
cate metaphorical parts. Niculae and Danescu-
Niculescu-Mizil (2014) aimed to distinguish a
comparison from figurative or literal in product re-
views using a series of linguistic cues as features.
It is similar to simile sentence classification. So
we take it as a baseline. In their work, they as-
sumed that the components can be correctly rec-
ognized. In our work, we use an automated com-
ponent extractor instead.

Syntactic patterns are often used for extracting
potential simile components and semantic analy-
sis is then used to distinguish similes from literal
comparisons (Niculae and Yaneva, 2013; Niculae,
2013). The main limitation is that such pattern
based method is difficult to deal with sentences
with complex structures. As a result, the coverage
is relatively small.

Qadir et al. (2016) used syntactic structures,
dictionary definitions, statistical cooccurrence,
and word embedding vectors to infer implicit
properties in similes. Qadir et al. (2015) also built
a classifier with lexical features, semantic features,
and sentiment features to infer the affective polar-
ity of simile in twitters. Veale and Hao (2007) and
Veale (2012a) utilized knowledge generated by
similes to deal with metaphor and irony, and Veale
(2012b) built a lexical stereotype model from sim-
iles. These work demonstrates the wide applica-
tions of simile recognition.

In Chinese, Li et al. (2008) proposed a feature-
based method for simile recognition. Their evalu-
ation was done on a small dataset. The annotated
data in this work is much larger.

2.2 Multitask Learning for NLP

Many researchers have proposed to jointly learn
multiple tasks with shared representations (Col-
lobert and Weston, 2008). Improvements are
reported on joint models between closely re-
lated tasks, such as text classification (Liu et al.,
2016), POS tagging and parsing (Zhang and
Weiss, 2016), parsing and named entity recogni-
tion (NER) (Finkel and Manning, 2010), NER and
linking (Luo et al., 2015), extraction of entities and
relations (Miwa and Bansal, 2016). Bingel and
Søgaard (2017) offered a systematic view of re-
lations between different tasks.



1545

3 Task and Data

3.1 Task Description
A metaphor is as a matter of cross-domain map-
pings in conceptual structure which are expressed
in language. Lakoff and Johnson (2008) explains
it as a mapping between the target and the source,
corresponding to the terms tenor and vehicle. The
tenor is the subject to which attributes are as-
cribed, while the vehicle is the object whose at-
tributes are borrowed.

Simile can be seen as a special type of
metaphor, which is signaled by explicit markers
such as like or as in English and “œ” or “πÇ”
in Chinese. We call such words comparators
(Hanks, 2012). Notice that a sentence containing
a comparator doesn’t guarantee that it is a simile
sentence. Consider the examples in Table 1. All
sentences contain the comparator “œ”. The first
two sentences form comparison structures, but the
first one triggers a cross-domain concept mapping,
while the second one is a literal comparison. The
word “œ” in the third sentence means as if, rather
than forming a comparison, and the comparator in
the fourth sentence is to give examples.

Simile Recognition task can be defined as:
Given a sentence containing a comparator, deter-
mine whether it is a simile sentence, if so, extract
the tenor and the vehicle from it.

Simile recognition involves two subtasks.
Simile Sentence Classification (SC). For a sen-

tence containing a comparator, determine whether
the comparator triggers a metaphorical compari-
son, in another word, whether the sentence is a
simile sentence.

Simile Component Extraction (CE). For a
simile sentence, extract text spans that are corre-
sponding to the tenor and the vehicle objects re-
spectively.

Both tasks have a realistic significance. Sim-
ile can be seen as a rhetorical device for making
thoughts or expressions more vivid. Simile sen-
tence classification could be used to provide a sig-
nal to evaluate rhetorical effects of writings. Sim-
ile component extraction is potentially useful for
building cognitive knowledge base.

3.2 Data
We construct a dataset from Chinese student es-
says written in Mandarin Chinese. We focus on
the comparator “œ”, which is the most often used
simile comparator in Chinese. The data annotation

involves the following steps: (1) we sampled sen-
tences that contain the word “œ” from a sentence
index built on more than 20,000 student essays; (2)
we asked two annotators to label every sentence
as a simile sentence or not; (3) the annotators fur-
ther annotated boundaries of simile components,
the tenor and the vehicle, in simile sentences.

Two points are important for annotation: (1) the
definition of simile and (2) the boundary of simile
components.

We desire that a simile sentence should satisfy
at least two standards. First, there exists explicit
tenor and vehicle, which are from different seman-
tic domains. Second, the tenor and vehicle should
have similar properties. We provided a manual and
positive/negative examples to annotators. Even so,
there are still fuzzy cases. The two annotators first
labeled a sample of 200 sentences independently
and then we measured their agreement and asked
them to discuss the disagreements. After discus-
sion, they labeled another set of sampled sentences
to check whether they really reached a consen-
sus. This process iterated several times. Their fi-
nal inner-annotator agreement on simile sentence
classification can reach to 91%. Finally, they la-
beled all sentences in the whole dataset.

Next, the annotators should further label simile
components in simile sentences, which are usually
noun phrases. Boundaries of simile components
are required to be annotated as compact as pos-
sible until they can’t be simplified any more. In
most cases, we asked the annotators to label the
head noun phrase without a modifier. A modi-
fier often plays a role as a shared property. For
example, in the sentence 7iÑ8œ�*¢˘
ú(The boy’s face is like a red apple), 8(face)
and ˘ú(apple) would be annotated as the tenor
and vehicle respectively, while7iÑ(boy’s) and
¢(red) are not included. In contrast, in the sen-
tence)œiPÑ8Ùÿ1ÿ(The weather is
like a child’s face, which changes unpredictably),
iPÑ8(child’s face) is preferred to8(face) as
a component, because8(face) alone can’t capture
the property well. We used one annotator’s anno-
tation of 200 sentences as the gold answer and the
other’s annotation as the prediction to computer
the F1 score, which is 93.57% on all components
and 90.7% on tenors and 96.47% on vehicles.

Table 2 shows the basic statistics of our dataset.



1546

Word embeddings

树叶
(the leaf)

像
(is like)

蝴蝶
(a butterfly)

ts o vs

Bi-LSTM

Hidden layer

Label representation

CRF layer

Labels

Words

Shared representation

Simile or not

+

<s> </s>像 树叶 蝴蝶 像

A
tte

nt
io

n
ve
ct
or

Bi-LSTM

Sentence representation

Labeling 
connection

Bi-LSTM

Backward
Forward

SoftmaxSimile component extraction

Simile sentence classification

Language modeling

Predicting 
the next 
words

Figure 1: The proposed multitask learning framework, which jointly optimizes three tasks.

#Sentence 11337
#Simile sentence 5088
#Literal sentence 6249
#Token 334k
#Tenor 5183
#Vehicle 5119
#Unique tenor concept 1680
#Unique vehicle concept 1972
#Tenor-vehicle pair 5214
#Unique tenor-vehicle pair 4521
Avg. #token per tenor 1.033
Avg. #token per vehicle 1.056
Avg. #token per sentence 29.47
Avg. #pair per simile sentence 1.024

Table 2: Statistics of the annotated simile dataset

4 Multitask Learning Approach

4.1 Motivation

Intuitively, the two subtasks in simile recognition
can benefit each other and the interactions between
them should not be ignored. If the component ex-
tractor knows that a sentence contains a simile, it
would be more confident to extract the tenor and
the vehicle. On the other hand, if the component
extractor tells the classifier that the tenor and the
vehicle likely exist, the classifier gets additional
information for decision.

Therefore, we propose a multitask learning ap-
proach to combine them. Our approach jointly op-
timizes three tasks: simile sentence classification,
simile component extraction and language model-
ing. Language modeling is used as auxiliary task,
which can help capture local information. Figure 1

illustrates the main framework, which is based on
neural networks. We will first explain the repre-
sentation layers that are shared by multiple tasks,
and then introduce separate prediction layers for
individual tasks.

4.2 Shared Representation
Word embedding layer. We first map words to
dense distributed word embeddings. Since our
dataset is not so large, we make use of pre-trained
word embeddings, which are trained on a much
larger corpus with Word2Vec toolkit (Mikolov
et al., 2013).
Sentence representation layer. Recurrent neu-
ral networks (RNNs) have become the natural
choice for handling sequential data to capture
long-range dependencies. Given a sentence X =
(x1, x2, ..., xn) containing n words as an input, the
RNNs produce H = (h1, h2, ..., hn) as the hid-
den states to represent the semantic of partial se-
quence so far. Recently, Long Short Term Mem-
ory (LSTM) model (Hochreiter and Schmidhuber,
1997) has been proved more effective in various
NLP tasks. Therefore, we use LSTM as the basic
memory cell. At time step t, LSTM takes the hid-
den state from the previous time step and the word
embedding from the current step as input, and pro-
duces a new hidden state, as shown in Formula 1.

�!
ht = LSTM(xt, LSTM(

��!
ht�1)) (1)

The LSTM architecture is sensitive to word or-



1547

der, and the bidirectional LSTM (Schuster and
Paliwal, 2002) allows model to look arbitrarily far
at both the past and the future for the sake of
grasping the whole sentence. Noted the forward
LSTM as

�!
h , and the backward as

 �
h . Bidirec-

tional LSTM concatenates the forward and back-
ward states as the representation at the tth time
step, i.e., ht =

h�!
ht ;
 �
ht

i
.

4.3 Task 1: Simile Component Extraction

We view simile component extraction as a se-
quence labeling problem. We convert the anno-
tated dataset to IOBES scheme (indicating Inside,
Outside, Beginning, Ending, Single) (Ratinov and
Roth, 2009). We use different prefixes to distin-
guish the tenor and the vehicle components. For
example, tb and vb indicate the beginning of a
tenor and a vehicle respectively.

4.3.1 Neural Sequence Labeling Model
Conditional Random Field (CRF) (Lafferty et al.,
2001) is a standard solution in such scenario to
exploit the dependency among labels.To further
make use of the dense representation of words, we
build a CRF layer on the shared representation lay-
ers following (Lample et al., 2016).

Formally, H = (h1, h2, ..., hn) is a sequence of
hidden states produced by the bidirectional LSTM
for a sentence X and y = (y1, y2, ..., yn) is the tag
sequence, yi 2 L and |L| = k. Define  (H, y) as
the score of the sequence.

 (H, y) =
nX

t=0

Ayt,yt+1 +
nX

t=1

Pt,yt (2)

where A 2 Rk⇥k is a transition matrix and
Ayt,yt+1 records the score of a transition from cur-
rent label yt to next label yt+1; P = (p1, ..., pn) 2
Rn⇥k is the emission matrix and Pt,yt represents
the score of assigning tag yt to xt.

Here, ht is the tth hidden state that is as
assigned by the bidirectional LSTM. It is first
mapped to a hidden layer through a feedforward
layer. After a non-linear activation transition
tanh, the output of the hidden layer is mapped to
a k-dimension vector pt, through another feedfor-
ward layer.

pt = Wp · tanh (Wtht) (3)

where Wt and Wp are parameter matrixes. pt can
be seen as a tag score vector given the current word
without considering context words.

Taking the whole state sequence into account,
the probability of tag sequence y given sentence
X is:

p(y|H) =  (H, y)P
ey2Y e

 (H,ey) (4)

where Y indicates all possible sequences. Learn-
ing algorithm attempts to optimize the model by
maximizing the log-likelihood of correct tag se-
quence. Thus, the loss function is

Ece = �log(p(y|H))

= � (H, y) + log
X

ey2Y
e
 (H,ey) (5)

4.4 Task 2: Simile Sentence Classification
The second task is simile sentence classification.
To fully exploit contextual information, we con-
sider all words in a sentence. For each word, in-
stead of using hidden state ht only, we combine ht
and its score vector pt as a representation st:

st = [ht; pt]

Since pt is directly related to the component
extraction task, this labeling connection operation
increases the interaction between the two tasks.

However, words in a sentence should not con-
tribute the same for classification. Intuitively, the
words corresponding to the tenor or the vehicle
or near comparators should be more important.
Therefore, we introduce the attention mechanism,
which was firstly proposed for neural machine
translation (Bahdanau et al., 2014).

Given the sequence of expanded word represen-
tations S = (s1, s2, ..., sn), the attention vector is
computed via:

↵ = softmax (tanh (W↵S)) (6)

where W↵ is a parameter matrix. The semantic
representation of the sentence is:

r = ↵T · S (7)

This representation is fed into an activation and
a softmax layer to generate the probability dis-
tribution. The loss function is the negative log-
likelihood of the correct classification tag:

Esc = � log (p (y|s)) . (8)



1548

4.5 Task 3: Language Modeling
Although LSTM can capture long dependencies,
simile structure may be more related to local con-
texts. In many cases, the comparator and the ve-
hicle are near, and similes often have some collo-
cations involving the comparator such as “œ...�
7(the same as)” or “1œ(just like)”. As a result,
we attempt to emphasize such local information.

Inspired by (Rei, 2017), we also incorporate
language modeling as an auxiliary task. For each
word, we let the model predict the next word. In
our case, the representation of each word ht is
firstly mapped into a low dimension vector space
through a nonlinear transform.

�!
mt = tanh

⇣��!
Wm ·

�!
ht

⌘
(9)

And the vehicle word is predicted by maximizing
the probability of the specific next word, which is
generated by a softmax layer.

P (wt+1|�!mt) = softmax
⇣�!
Wq ·�!mt

⌘
(10)

where, �!mt indicates the forward language model-
ing specific features. Wm, Wq are trainable pa-
rameters.

The loss function for a sequence is defined as
the sum of the negative log-likelihood of the pre-
dicted words.

��!
Elm = �

n�1X

t=1

log (P (wt+1|�!mt)) (11)

We can also predict the previous words in the
same way and get another loss function noted as ��
Elm. The losses in double direction are summed
to be the loss function for language modeling task.

Elm =
��!
Elm +

 ��
Elm (12)

4.6 The Final Loss Function
The final loss function for each sentence is a
weighted sum of task-specific loss functions.

E = � · Elm + � · Ece + ✏ · Esc (13)

where �, �, ✏ are non-negative weights, which are
used to control the importance of three tasks. In
experiments, they are hyper-parameters assigned
beforehand, and we constrain the sum of �, � and
✏ to one.

5 Evaluation

5.1 Settings
The dataset is randomly divided into 5 folds, 4
of which are used as training set and validation
set (80% for training, 20% for validation), and
the rest one fold is used as test set. All models
were trained on the training set. The best hyper-
parameters were gained based on the results on the
validation set. The results reported were all evalu-
ated on the test set.

We conduct word segmentation, part-of-speech
(POS) tagging and dependency parsing with HIT-
LTP2.The word embeddings were pre-trained us-
ing Word2Vec (Mikolov et al., 2013), on a large
essay corpus crawled from the web. We adopt the
Theano framework (Theano Development Team,
2016) to implement neural network models.

The dimension of word embeddings is 50. The
hidden size of LSTM is 128 for each direction.
The dimension of activation layers for component
extraction, simile sentence classification and lan-
guage modeling are set to 64, 32 and 64 respec-
tively. A dropout layer (Srivastava et al., 2014)
is used between the word embedding layer and
the bidirectional LSTM layer with the probabil-
ity of 0.5. Moreover, early stopping (Prechelt,
1998) is adopted to finish the learning process.
The AdaDelta (Zeiler, 2012) strategy is used for
parameter optimization with a learning rate of 1.0.

5.2 Evaluating Simile Sentence Classification
5.2.1 Comparisons
We compare the following systems.

Feature based approaches. With manually de-
signed features, we build two Random Forest clas-
sifiers to determine whether a sentence contains a
simile. Baseline1 follows (Niculae and Danescu-
Niculescu-Mizil, 2014). It first extracts candidate
simile components and then uses a classifier to de-
termine whether they form a simile. We adopt
our best neural component extractor (will be in-
troduced in Section 5.3.1) to extract components.
The classifier uses features including: (1) bag-of-
words; (2) corresponding occurrence within con-
stituents; (3) word embeddings of extracted com-
ponents. Niculae and Danescu-Niculescu-Mizil
(2014) also used domain specific information and
lexicon knowledge, which our data lacks. Base-
line2 is based on (Li et al., 2008), which doesn’t

2
https://github.com/HIT-SCIR/pyltp



1549

Model Simile ClassificationP R F1
Baseline1 0.6523 0.4752 0.5498
Baseline2 0.7661 0.7832 0.7745
Singletask (SC) 0.7751 0.8895 0.8284
Multitask (SC+CE) 0.8056 0.8886 0.8450
Multitask (SC+LM) 0.8021 0.9105 0.8525
Multitask (SC+CE+LM) 0.8084 0.9220 0.8615

Table 3: Experimental results on simile sentence clas-
sification. SC: simile sentence classification; CE: com-
ponent extraction; LM: language modeling.

need to identify components beforehand. The fea-
tures include: (1) the tokens and POS tags of the
words around the comparator within a fixed win-
dow (set to 5 in experiments); (2) the tokens, POS
tags and dependency relation tags of the words that
have dependency relations with the comparator.

Singletask(SC). This system is a simplified ver-
sion of our proposed model in Section 4 by consid-
ering the simile sentence classification task only.

Multitask learning approaches. The full ar-
chitecture is described in Section 4. To see their
contributions, we add simile component extrac-
tion, language modeling and their combination in-
crementally.

5.2.2 Results
Table 3 shows the performance of the systems.
The results are reported with the precision (P), re-
call (R), and their harmonic mean F1 score (F1).

The two feature based methods perform differ-
ently. Baseline1 performs poorly. The reason may
be that the classification depends on the perfor-
mance of component extraction, while even our
best component extractor performs far from per-
fect, which brings error propagation. In addition,
classifying with component related features only
ignores much context, which further decreases the
performance. Baseline2 considers context win-
dows and outperforms baseline1 largely. This con-
firms our intuition that context information im-
plies the semantic of simile expression.

Furthermore, we have other observations: (1)
neural network based approaches largely outper-
form feature-based classifiers;(2) multitask learn-
ing approaches outperform every single task ap-
proach and other baselines. Both the component
extraction and the language modeling task con-
tribute for simile sentence classification. Com-
ponent extraction improves the precision and lan-
guage modeling improves both the precision and
the recall. Combining them together can achieve

the best performance. The improvement of F1
score can reach to 3.3% compared with the best
single task model.

5.3 Evaluating Simile Component Extraction
5.3.1 Comparisons
We compare the following systems for simile com-
ponent extraction.

Rule based approach. We follow (Niculae and
Yaneva, 2013) to design syntactic patterns for ex-
traction. We convert the original patterns to fit the
outputs of the parser we used.

CRF model. Since we view component extrac-
tion as a sequence labeling problem, a CRF model
with manually designed feature templates is used
as a baseline. Feature template is designed for ev-
ery word. The features include the tokens and their
POS tags within a fixed context window (set to 5
in experiments). We also use dependency parsing
based features to capture dependencies between
words.

Singletask(CE). We remove the simile classi-
fication and language modeling modules from the
multitask learning framework introduced in Sec-
tion 4 to build an end-to-end single task compo-
nent extractor.

Pipeline approaches. Pipeline approaches
first classify a sentence as simile or not,
and then extract components from simile sen-
tences. We investigate two combinations:
RandomForest!CRF, we use baseline2 for sen-
tence classification and CRF for component ex-
traction; SingleSC!SingleCE, we use neural net-
work based single task sentence classifier and
component extractor.

Multitask learning approaches. We exploit
simile sentence classification and language mod-
eling modules to enhance component extraction in
our multitask learning framework.

5.3.2 Results and Discussion
A component (i.e., the tenor or the vehicle) is
judged to be correct only if both the boundary and
the tag exactly match the gold answer. For a sim-
ile sentence, we should extract both the tenor and
the vehicle rather than only partial components.
Therefore, we use pair-wise level precision (P), re-
call (R) and F1 score (F1) for evaluation. A tenor-
vehicle pair is viewed as correct only if both com-
ponents are correct.

Table 4 shows the results of various systems and
settings on two test sets. The first dataset consists



1550

Model Gold simile sentences Whole test setP R F1 P R F1
Rule based 0.4094 0.1805 0.2505 —
CRF 0.5619 0.5907 0.5760 0.3157 0.3698 0.3406
Singletask (CE) 0.7297 0.7854 0.7564 0.5580 0.6489 0.5998
RandomForest ! CRF — 0.4591 0.4980 0.4778
SingleSC ! SingleCE — 0.5720 0.7074 0.6325
Multitask (CE+SC) — 0.5409 0.6400 0.5861
Multitask (CE+LM) 0.7530 0.7876 0.7699 0.5741 0.7015 0.6306
Multitask (CE+SC+LM) — 0.5599 0.6989 0.6211
Optimized pipeline — 0.6160 0.7361 0.6707

Table 4: Experimental results on component extraction. Experiments on dataset of simile sentences assume that
the sentence classifier is perfect. CE: component extraction; SC: simile sentence classification; LM: language
modeling.

of all manually labeled simile sentences in the test
set and the second dataset is the whole test set. We
want to compare how component extraction sys-
tems work when they know whether a sentence
contains a simile or not. We report and discuss
the results from the following aspects.

The effect of simile sentence classification.
First, we can compare the results in the middle
column and the rightmost column in Table 4. It is
clear that the component extraction systems work
much better when they know whether a sentence
contains a simile or not. Second, we can see that
both pipelines (the feature-based and the neural
network based) achieve a better performance com-
pared with extracting components directly using
either the CRF model or the neural single task
model. Third, Multitask(CE+SC) doesn’t bring
significant improvements compared with the sin-
gle task neural model. These observations indi-
cate that simile sentence classification is suitable
to be a pre-processing for simile component clas-
sification. It is necessary to further study how to
use high level predictions (sentence classification)
to learn better representations for consistently im-
proving local predictions (simile component ex-
traction).

Rule based, feature-based and neural mod-
els. We can see that even on gold simile sentences,
the rule based method doesn’t work well. The
poor performance of the rule based approach is
due to the following reasons. First, the rule-based
method is difficult to deal with complex sentence
structures. It often fails when there are multiple
subordinate clauses. Second, the comparator “œ”
in Chinese has multiple syntactic roles, sometimes
is used as a verb, sometimes is used as a preposi-
tion. Third, the accuracy of Chinese dependency
parser still has room to be improved.

The CRF method performs significantly bet-
ter, because it considers more contextual signals.
Our neural single task model achieves large im-
provements on both datasets. This verifies the
effectiveness of the end-to-end approach. Neu-
ral models can see a long range of context and
learn features automatically. The word embed-
dings learned on external resources implicitly have
semantic domain information, which is not only
useful for generalization but also important for fig-
urative language processing.

The effect of language modeling. Surpris-
ingly, using language modeling as an auxiliary
task is very useful, especially when dealing with
noisy sentences. It gains a 1.3% F1 improvement
on the gold simile sentences due to the improve-
ment on the precision and a 3% F1 improvement
on the whole test set due to a large improvement
on the recall. Generally, language modeling may
help learn better task specific representations, es-
pecially when data size is limited (Rei, 2017). An-
other reason may be that language modeling aims
to make local predictions, the same as simile com-
ponent extraction. Additional information from
the same level may be more useful.

Some observations help understand the effect of
language modeling. Figure 2 illustrates the rel-
ative distance of tenors and vehicles to the com-
parator. Both tenors and vehicles tend to occur
near the comparator and have a clear preference
on which side of the comparator. Tenors are more
dispersed compared with vehicles, which may in-
crease the difficulty. As shown in Table 5, the per-
formance on identifying tenors is obviously worse
than identifying vehicles in both settings.

Figure 3 shows the distribution of extracted
components by two settings on the whole test set.
We can see that with language modeling, Mul-



1551

Model Tenor VehicleP R F1 P R F1
Singletask(CE) 0.7156 0.6935 0.7044 0.7789 0.8313 0.8043
Multitask (CE+LM) 0.6792 0.7881 0.7296 0.7393 0.9026 0.8128

Table 5: Experimental results on identifying individual types of components.

titask(CE+LM) makes predictions more aggres-
sively compared with Singletask(CE) and tends to
recognize more nearby components. We also ob-
serve that Multitask(CE+LM) identifies more cor-
rect UNK components, which are out of vocabu-
lary or low frequency concepts. This means that
language modeling leads the model to consider
more local contextual patterns.

Figure 2: Relative distance of tenors and vehicles to the
comparator in the dataset.

Figure 3: The distribution of extracted simile compo-
nents on the whole test set.

Optimized Pipeline. According to the results and
analysis, we can summarize that (1) simile sen-
tence classification can achieve good performance
by jointly optimizing three tasks; (2) simile com-
ponents can be improved with language modeling
as an auxiliary task; (3) simile sentence classifi-
cation is suitable to be used as a pre-precessing
for simile component classification. Therefore, we
could build an optimized pipeline. We first use the
enhanced simile sentence classifier to filter simile

sentences and then use the enhanced component
extractor to extract tenors and vehicles. As shown
in Table 4, the optimized pipeline performs better
than the strongest multitask learning setting.

However, in all settings, the precision scores
are lower compared with the recall scores. This
indicates that compared with identifying surface
patterns, distinguishing metaphorical from lit-
eral meanings is much harder and more external
knowledge should be incorporated.

6 Conclusion

This paper presented a study on simile recogni-
tion by exploiting neural networks. We construct
a manually annotated dataset for advancing the re-
search on simile analysis in Chinese. We propose
a multitask learning framework, which jointly op-
timizes three tasks: simile sentence classification,
simile component extraction and language model-
ing. The experimental results demonstrate the ef-
fectiveness of proposed approaches. It shows that
simile sentence classification and simile compo-
nent extraction both benefit from multitask learn-
ing. Simile sentence classification can achieve a
high performance and simile component extrac-
tion still has a lot of room to improve.

In future, we plan to extend this work in several
aspects: (1) enrich the simile component struc-
ture by adding shared properties or events so that
the extracted structures would be more useful for
metaphor processing; (2) improve representation
learning for recognition by incorporating external
knowledge; (3) apply simile recognition to study
the use of figurative language in writings.

Acknowledgements

The research work is funded by the National Nat-
ural Science Foundation of China (No.61876113),
High-level Teachers in Beijing Municipal Uni-
versities in the Period of 13th Five-year Plan
(CIT&TCD20170322),Beijing Educational Com-
mittee Science and Technology Development
Planned(No. KM201610028015),Humanity &
Social Science general project of Ministry of Edu-
cation(No.14YJC740087).



1552

References
Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua

Bengio. 2014. Neural machine translation by
jointly learning to align and translate. CoRR,
abs/1409.0473.

Joachim Bingel and Anders Søgaard. 2017. Identify-
ing beneficial task relations for multi-task learning
in deep neural networks. In Proceedings of the 15th
Conference of the European Chapter of the Associa-
tion for Computational Linguistics: Volume 2, Short
Papers, pages 164–169. Association for Computa-
tional Linguistics.

Jaime G Carbonell. 1980. Metaphor: A key to extensi-
ble semantic analysis. In Proceedings of the 18th
annual meeting on Association for Computational
Linguistics, pages 17–21. Association for Compu-
tational Linguistics.

Ronan Collobert and Jason Weston. 2008. A unified
architecture for natural language processing: Deep
neural networks with multitask learning. In Pro-
ceedings of the 25th international conference on
Machine learning, pages 160–167. ACM.

Dan Fass. 1991. met*: A method for discriminating
metonymy and metaphor by computer. Computa-
tional Linguistics, 17(1):49–90.

Jenny Rose Finkel and Christopher D. Manning. 2010.
Hierarchical joint learning: Improving joint parsing
and named entity recognition with non-jointly la-
beled data. In Proceedings of the 48th Annual Meet-
ing of the Association for Computational Linguis-
tics, pages 720–728. Association for Computational
Linguistics.

Andrew Goatly. 2011. The language of metaphors.
Routledge.

Patrick Hanks. 2012. The roles and structure of com-
parisons, similes, and metaphors in natural language
(an analogical system). Prose (in honor of the Dick-
ens Bicentennial), page 5.

Sepp Hochreiter and Jürgen Schmidhuber. 1997.
Long short-term memory. Neural Computation,
9(8):1735–1780.

Hyeju Jang, Yohan Jo, Qinlan Shen, Michael
Miller, Seungwhan Moon, and Carolyn Rose. 2016.
Metaphor detection with topic transition, emotion
and cognition in context. In Proceedings of the
54th Annual Meeting of the Association for Compu-
tational Linguistics (Volume 1: Long Papers), pages
216–225. Association for Computational Linguis-
tics.

John Lafferty, Andrew McCallum, and Fernando C.N.
Pereira. 2001. Conditional random fields: Prob-
abilistic models for segmenting and labeling se-
quence data. In Proceedings of the 18th Interna-
tional Conference on Machine Learning, volume
951, pages 282–289.

George Lakoff and Mark Johnson. 2008. Metaphors
we live by. University of Chicago press.

Guillaume Lample, Miguel Ballesteros, Sandeep Sub-
ramanian, Kazuya Kawakami, and Chris Dyer. 2016.
Neural architectures for named entity recognition.
In Proceedings of the 2016 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics: Human Language Technologies,
pages 260–270.

Bin Li, Li-li Yu, Min Shi, and Wei-guang Qu. 2008.
Computation of chinese simile with ”xiang”. Jour-
nal of Chinese Information Processing, 22(6):27–
32.

Pengfei Liu, Xipeng Qiu, and Xuanjing Huang. 2016.
Recurrent neural network for text classification with
multi-task learning. In IJCAI.

Gang Luo, Xiaojiang Huang, Chin-Yew Lin, and Za-
iqing Nie. 2015. Joint entity recognition and disam-
biguation. In Proceedings of the 2015 Conference
on Empirical Methods in Natural Language Pro-
cessing, pages 879–888. Association for Computa-
tional Linguistics.

Zachary J Mason. 2004. Cormet: a computational,
corpus-based conventional metaphor extraction sys-
tem. Computational linguistics, 30(1):23–44.

Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey
Dean. 2013. Efficient estimation of word represen-
tations in vector space. CoRR, abs/1301.3781.

Makoto Miwa and Mohit Bansal. 2016. End-to-end re-
lation extraction using lstms on sequences and tree
structures. In Proceedings of the 54th Annual Meet-
ing of the Association for Computational Linguistics
(Volume 1: Long Papers), pages 1105–1116. Asso-
ciation for Computational Linguistics.

Vlad Niculae. 2013. Comparison pattern matching and
creative simile recognition. In Proceedings of the
Joint Symposium on Semantic Processing. Textual
Inference and Structures in Corpora, pages 110–
114.

Vlad Niculae and Cristian Danescu-Niculescu-Mizil.
2014. Brighter than gold: Figurative language in
user generated comparisons. In Proceedings of the
2014 Conference on Empirical Methods in Natural
Language Processing (EMNLP), pages 2008–2018.
Association for Computational Linguistics.

Vlad Niculae and Victoria Yaneva. 2013. Computa-
tional considerations of comparisons and similes. In
51st Annual Meeting of the Association for Com-
putational Linguistics Proceedings of the Student
Research Workshop, pages 89–95. Association for
Computational Linguistics.

Lutz Prechelt. 1998. Automatic early stopping using
cross validation: quantifying the criteria. Neural
Networks, 11(4):761–767.



1553

Ashequl Qadir, Ellen Riloff, and Marilyn Walker. 2015.
Learning to recognize affective polarity in similes.
In Proceedings of the 2015 Conference on Empiri-
cal Methods in Natural Language Processing, pages
190–200. Association for Computational Linguis-
tics.

Ashequl Qadir, Ellen Riloff, and Marilyn A. Walker.
2016. Automatically inferring implicit properties in
similes. In Proceedings of the 2016 Conference of
the North American Chapter of the Association for
Computational Linguistics: Human Language Tech-
nologies, pages 1223–1232. Association for Compu-
tational Linguistics.

Lev Ratinov and Dan Roth. 2009. Conll ’09 de-
sign challenges and misconceptions in named en-
tity recognition. In CoNLL ’09: Proceedings of
the Thirteenth Conference on Computational Natu-
ral Language Learning, pages 147–155.

Marek Rei. 2017. Semi-supervised multitask learn-
ing for sequence labeling. In Proceedings of the
55th Annual Meeting of the Association for Com-
putational Linguistics (Volume 1: Long Papers),
pages 2121–2130, Vancouver, Canada. Association
for Computational Linguistics.

M. Schuster and K.K. Paliwal. 2002. Bidirectional re-
current neural networks. IEEE Transactions on Sig-
nal Processing, 45(11):2673–2681.

Ekaterina Shutova, Lin Sun, and Anna Korhonen.
2010. Metaphor identification using verb and noun
clustering. In Proceedings of the 23rd International
Conference on Computational Linguistics (Coling
2010), pages 1002–1010, Beijing, China. Coling
2010 Organizing Committee.

Ekaterina Shutova and Simone Teufel. 2010. Metaphor
corpus annotated for source - target domain map-
pings. In Proceedings of the Seventh conference on
International Language Resources and Evaluation
(LREC’10), pages 3255–3261. European Languages
Resources Association (ELRA).

Ekaterina Shutova, Simone Teufel, and Anna Korho-
nen. 2013. Statistical metaphor processing. Compu-
tational Linguistics, 39(2):301–353.

Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky,
Ilya Sutskever, and Ruslan Salakhutdinov. 2014.
Dropout: a simple way to prevent neural networks
from overfitting. Journal of Machine Learning Re-
search, 15(1):1929–1958.

Theano Development Team. 2016. Theano: A Python
framework for fast computation of mathematical ex-
pressions. arXiv e-prints, abs/1605.02688.

Tony Veale. 1995. Metaphor, memory and meaning:
Symbolic and connectionist issues in metaphor in-
terpretation.

Tony Veale. 2012a. A computational exploration of
creative similes. Metaphor in Use: Context, culture,
and communication, 38:329–343.

Tony Veale. 2012b. A context-sensitive, multi-faceted
model of lexico-conceptual affect. In Proceedings
of the 50th Annual Meeting of the Association for
Computational Linguistics (Volume 2: Short Pa-
pers), pages 75–79. Association for Computational
Linguistics.

Tony Veale and Yanfen Hao. 2007. Learning to under-
stand figurative language: from similes to metaphors
to irony. In Proceedings of the Annual Meeting
of the Cognitive Science Society, volume 29, pages
683–688.

Matthew D. Zeiler. 2012. ADADELTA: an adaptive
learning rate method. CoRR, abs/1212.5701.

Yuan Zhang and David Weiss. 2016. Stack-
propagation: Improved representation learning for
syntax. In Proceedings of the 54th Annual Meet-
ing of the Association for Computational Linguistics
(Volume 1: Long Papers), pages 1557–1566. Asso-
ciation for Computational Linguistics.


