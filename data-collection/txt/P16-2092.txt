



















































Dependency-based Gated Recursive Neural Network for Chinese Word Segmentation


Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, pages 567–572,
Berlin, Germany, August 7-12, 2016. c©2016 Association for Computational Linguistics

Dependency-based Gated Recursive Neural Network
for Chinese Word Segmentation

Jingjing Xu and Xu Sun
MOE Key Laboratory of Computational Linguistics, Peking University

School of Electronics Engineering and Computer Science, Peking University
{xujingjing, xusun}@pku.edu.cn

Abstract

Recently, many neural network models
have been applied to Chinese word seg-
mentation. However, such models focus
more on collecting local information while
long distance dependencies are not well
learned. To integrate local features with
long distance dependencies, we propose a
dependency-based gated recursive neural
network. Local features are first collect-
ed by bi-directional long short term mem-
ory network, then combined and refined to
long distance dependencies via gated re-
cursive neural network. Experimental re-
sults show that our model is a competitive
model for Chinese word segmentation.

1 Introduction

Word segmentation is an important pre-process
step in Chinese language processing. Most wide-
ly used approaches treat Chinese word segmenta-
tion (CWS) task as a sequence labeling problem in
which each character in the input sequence is as-
signed with a tag. Many previous approaches have
been effectively applied to CWS problem (Laf-
ferty et al., 2001; Xue and Shen, 2003; Sun et
al., 2012; Sun, 2014; Sun et al., 2013; Cheng et
al., 2015). However, these approaches incorpo-
rated many handcrafted features, thus restricting
the generalization ability of these models. Neural
network models have the advantage of minimiz-
ing the effort in feature engineering. Collobert et
al. (2011) developed a general neural network ar-
chitecture for sequence labeling tasks. Following
this work, neural network approaches have been
well studied and widely applied to CWS task with
good results (Zheng et al., 2013; Pei et al., 2014;
Ma and Hinrichs, 2015; Chen et al., 2015).

地面    积    了    厚厚    的    雪         
 

这    块    地    面积    还    真    不小 
 

“The ground is covered with thick snow ” 

“This area is really not small.” 

Figure 1: An illustration for the segmentation am-
biguity. The character “面” is labeled as “E” (end
of word) in the top sentence while labeled as “B”
(begin of word) in the bottom one even though
“面” has the same adjacent characters, “地” and
“积”.

However, these models focus more on collect-
ing local features while long distance dependen-
cies are not well learned. In fact, relying on the
information of adjacent words is not enough for
CWS task. An example is shown in Figure 1. The
character “面” has different tags in two sentences,
even with the same adjacent characters, “地” and
“ 积”. Only long distance dependencies can help
the model recognize tag correctly in this example.
Thus, long distance information is an importan-
t factor for CWS task.

The main limitation of chain structure for se-
quence labeling is that long distance dependencies
decay inevitably. Though forget gate mechanis-
m is added, it is difficult for bi-directional long
short term memory network (Bi-LSTM), a kind of
chain structure, to avoid this problem. In general,
tree structure works better than chain structure to
model long term information. Therefore, we use
gated recursive neural network (GRNN) (Chen et
al., 2015) which is a kind of tree structure to cap-
ture long distance dependencies.

Motivated by the fact, we propose the
dependency-based gated recursive neural network
(DGRNN) to integrate local features with long dis-

567



tance dependencies. Figure 2 shows the structure
of DGRNN. First of all, local features are col-
lected by Bi-LSTM. Secondly, GRNN recursive-
ly combines and refines local features to capture
long distance dependencies. Finally, with the help
of local features and long distance dependencies,
our model generates the probability of the tag of
word.

The main contributions of the paper are as fol-
lows:

• We present the dependency-based gated re-
cursive neural network to combine local fea-
tures with long distance dependencies.

• To verify the effectiveness of the proposed
approach, we conduct experiments on three
widely used datasets. Our proposed model
achieves the best performance compared with
other state-of-the-art approaches.

2 Dependency-based Gated Recursive
Neural Network

In order to capture local features and long distance
dependencies, we propose dependency-based gat-
ed recursive neural network. Figure 2 illustrates
the structure of the model.

…
…

   

        

…
…

   

       

…
…

 

Window Context “This area is really not small.” 

块(Ci-2) 地(Ci-1) 面(Ci) 积(Ci+1) 还(Ci+2) 

Layer 1 

Layer 2 Cell 

Output Layer 

Cell Cell Cell 

Layer 3 Cell Cell Cell 

Figure 2: Architecture of DGRNN for Chinese
Word Segmentation. Cell is the basic unit of GRN-
N.

2.1 Collect Local Features

We use bi-directional long short term memory
(Bi-LSTM) with single layer to collect local fea-
tures. Bi-LSTM is composed of two directional

tanh sig sig 

tanh 

f(t) 

h(t) 

s(t) 

i(t) 

s(t-1) 

sig 

o(t) 

x(t) , h
(t-1) 

Figure 3: Structure of LSTM unit. The behavior
of the LSTM cell is controlled by three “gates”,
namely input gate i(t), forget gate f (t) and output
gate o(t).

long short term memory networks with single lay-
er, which can model word representation with con-
text information. Figure 3 shows the calculation
process of LSTM. The behavior of LSTM cell is
controlled by three “gates”, namely input gate i(t),
forget gate f (t) and output gate o(t). The input
of LSTM cell are x(t), s(t−1) and h(t−1). x(t) is
the character embeddings of input sentence. s(t−1)

and h(t−1) stand for the state and output of the for-
mer LSTM cell, respectively. The core of the L-
STM model is s(t), which is computed using the
former state of cell and two gates, i(t) and f (t). In
the end, the output of LSTM cell h(t) is calculated
making use of s(t) and o(t).

2.2 Refine Long Distance Dependencies

GRNN recursively combines and refines local fea-
tures to capture long distance dependencies. The
structure of GRNN is like a binary tree, where ev-
ery two continuous vectors in a sentence is com-
bined to form a new vector. For a sequence s with
length n, there are n layers in total. Figure 4 shows
the calculation process of GRNN cell. The core of
GRNN cell are two kinds of gates, reset gates, rL,
rR, and update gates z. Reset gates control how
to adjust the proportion of the input hi−1 and hi,
which results to the current new activation h

′
. By

the update gates, the activation of an output neu-
ron can be regarded as a choice among the current
new activation h

′
, the left child hi−1 and the right

child hi.

2.3 Loss Function

Following the work of Pei et al. (2014), we adop-
t the max-margin criterion as loss function. For
an input sentence c[1:n] with a tag sequence t[1:n],
a sentence-level score is given by the sum of net-

568



h
i 

h
i-1 

h’
 

Y
 

r
L rR 

Z
 

Figure 4: The structure of GRNN cell.

work scores:

s(c[1:n], t[1:n], θ) =
n∑

i=1

fθ(ti|c[i−2:i+2]) (1)

where s(c[1:n], t[1:n], θ) is the sentence-level score.
n is the length of c[1:n]. fθ(ti|c[i−2:i+2]) is the s-
core output for tag ti at the ith character by the
network with parameters θ.

We define a structured margin loss ∆(yi, ŷ) for
predicting a tag sequence ŷ and a given correct
tag sequence yi:

∆(yi, ŷ) =
n∑

j=1

κ1{yi,j ̸= yi} (2)

where κ is a discount parameter. This leads to the
regularized objective function for m training ex-
amples:

J(θ) =
1
m

m∑
i=1

li(θ) +
λ

2
∥θ∥2 (3)

li(θ) = max
ŷ⊆Y (xi)

((s(xi, ŷ, θ)

+ ∆(yi, ŷ)) − s(xi, yi, θ)) (4)
where J(θ) is a loss function with parameters θ.
λ is regularization factor. By minimizing this ob-
ject, the score of the correct tag sequence yi is in-
creased and score of the highest scoring incorrect
tag sequence ŷ is decreased.

2.4 Amplification Gate and Training
A direct adaptive method for faster backpropaga-
tion learning method (RPROP) (Riedmiller and

Braun, 1993) was a practical adaptive learning
method to train large neural networks. We use
mini-batch version RPROP (RMSPROP) (Hinton,
2012) to minimize the loss function.

Intuitively, extra hidden layers are able to im-
prove accuracy performance. However, it is com-
mon that extra hidden layers decrease classifica-
tion accuracy. This is mainly because extra hidden
layers lead to the inadequate training of later lay-
ers due to the vanishing gradient problem. This
problem will decline the utilization of local and
long distance information in our model. To over-
come this problem, we propose a simple ampli-
fication gate mechanism which appropriately ex-
pands the value of gradient while not changing the
direction.

Higher amplification may not always perfor-
m better while lower value may bring about the
unsatisfied result. Therefore, the amplification
gate must be carefully selected. Large magnifi-
cation will cause expanding gradient problem. On
the contrary, small amplification gate will hardly
reach the desired effect. Thus, we introduce the
threshold mechanism to guarantee the robustness
of the algorithm, where gradient which is greater
than threshold will not be expanded. Amplifica-
tion gate of difference layer is distinct. For every
sample, the training procedure is as follows.

First, recursively calculate mt and vt which de-
pend on the gradient of time t− 1 or the square of
gradient respectively. β1 and β2 aim to control the
impact of last state.

mt = β1 · mt−1 + (1 − β1) · gt (5)

vt = β2 · vt−1 + (1 − β2) · g2t (6)
Second, calculate ∆W (t) based on vt and

square of mt. ϵ and µ are smooth parameters.

M(w, t) = vt − m2t (7)

∆W (t) =
ϵgt,i√

M(w, t) + µ
(8)

Third, update weight based on the amplification
gate and ∆W (t). The parameter update for the ith
parameter for the Θt,i at time step t with amplifi-
cation gate γ is as follows:

Θt,i = Θt,i − γ∆W (t) (9)

569



0 5 10 15

0.92

0.93

0.94

0.95

Epoch

F
−

sc
or

e

 

 

DGRNN
DGRNN+AG

(a) PKU

0 5 10 15

0.93

0.94

0.95

0.96

Epoch

F
−

sc
or

e

 

 

DGRNN
DGRNN+AG

(b) MSRA

0 5 10 15
0.91

0.92

0.93

0.94

0.95

Epoch

F
−

sc
or

e

 

 

DGRNN
DGRNN+AG

(c) CTB6

Figure 5: Results for DGRNN with amplification gate (AG) on three development datasets.

3 Experiments

3.1 Data and Settings
We evaluate our proposed approach on three
datasets, PKU, MSRA and CTB6. The PKU and
MSRA data both are provided by the second In-
ternational Chinese Word Segmentation Bakeof-
f (Emerson, 2005) and CTB6 is from Chinese
TreeBank 6.01 (Xue et al., 2005). We randomly
divide the whole training data into the 90% sen-
tences as training set and the rest 10% sentences
as development set. All datasets are preprocessed
by replacing the Chinese idioms and the continu-
ous English characters. The character embeddings
are pre-trained on unlabeled data, Chinese Giga-
word corpus2. We use MSRA dataset to prepro-
cess model weights before training on CTB6 and
PKU datasets.

Following previous work and our experimen-
tal results, hyper parameters configurations are set
as follows: minibatch size n = 16, window size
w = 5, character embedding size d1 = 100, am-
plification gate range γ = [0, 4] and margin loss
discount κ = 0.2. All weight matrixes are diag-
onal matrixes and randomly initialized by normal
distribution.

3.2 Experimental Results and Discussions
We first compare our model with baseline meth-
ods, Bi-LSTM and GRNN on three datasets. The
results evaluated by F-score (F1 score) are report-
ed in Table 1.

• Bi-LSTM. First, the output of Bi-LSTM is
concatenated to a vector. Second, softmax
layer takes the vector as input and generates
each tag probability.

1https://catalog.ldc.upenn.edu/LDC2007T36
2https://catalog.ldc.upenn.edu/LDC2003T09

Model (Unigram) PKU MSRA CTB6
Bi-LSTM 95.0 95.8 95.2

GRNN 95.8 96.2 95.5
Pei et al. (2014) 94.0 94.9 *

Chen et al. (2015) 96.1 96.2 95.6
DGRNN 96.1 96.3 95.8

Table 1: Comparisons for DGRNN and other neu-
ral approaches based on traditional unigram em-
beddings.

Model PKU MSRA CTB6
Zhang et al. (2006) 95.1 97.1 *
Zhang et al. (2007) 94.5 97.2 *
Sun et al. (2009) 95.2 97.3 *
Sun et al. (2012) 95.4 97.4 *

Zhang et al. (2013) 96.1 97.4 *
DGRNN 96.1 96.3 95.8

Table 2: Comparisons for DGRNN and state-of-
the-art non-neural network approaches on F-score.

• GRNN. The structure of GRNN is recursive.
GRNN combines adjacent word vectors to
the more abstract representation in bottom-up
way.

Furthermore, we conduct experiments with am-
plification gate on three development datasets.
Figure 5 shows that amplification gate significant-
ly increases F-score on three datasets. Amplifi-
cation even achieves 0.9% improvement on CTB6
dataset. It is demonstrated that amplification gate
is an effective mechanism.

We compare our proposed model with previ-
ous neural approaches on PKU, MSRA and CT-
B6 test datasets. Experimental results are report-
ed in Table 1. It can be clearly seen that our
approach achieves the best results compared with

570



Dataset Model Result

MSRA
Bi-LSTM t = 5.94, p < 1 × 10−4

GRNN t = 1.22, p = 0.22

PKU
Bi-LSTM t = 15.54, p < 1 × 10−4

GRNN t = 4.43, p < 1 × 10−4

CTB6
Bi-LSTM t = 5.01, p < 1 × 10−4

GRNN t = 2.55, p = 2.48 × 10−2

Table 3: The t-test results for DGRNN and base-
lines.

other neural networks on traditional unigram em-
beddings. It is possible that bigram embeddings
may achieve better results. With the help of bi-
gram embeddings, Pei et al. (2014) can achieve
95.2% and 97.2% F-scores on PKU and MSRA
datasets and Chen et al. (2015) can achieve 96.4%,
97.6% and 95.8% F-scores on PKU, MSRA and
CTB6 datasets. However, performance varies a-
mong these bigram models since they have dif-
ferent ways of involving bigram embeddings. Be-
sides, the training speed would be very slow after
adding bigram embeddings. Therefore, we only
compare our model on traditional unigram embed-
dings.

We also compare DGRNN with other state-of-
the-art non-neural networks, as shown in Table 2.
Chen et al. (2015) implements the work of Sun
and Xu (2011) on CTB6 dataset and achieves
95.7% F-score. We achieve the best result on P-
KU dataset only with unigram embeddings. The
experimental results show that our model is a com-
petitive model for Chinese word segmentation.

3.3 Statistical Significance Tests

We use the t-test to intuitively show the improve-
ment of DGRNN over baselines. According to the
results shown in Table 3, we can draw a conclu-
sion that, by conventional criteria, this improve-
ment is considered to be statistically significant
between DGRNN with baselines, except for GRN-
N approach on MSRA dataset.

4 Conclusions

In this work, we propose dependency-based recur-
sive neural network to combine local features with
long distance dependencies, which achieves sub-
stantial improvement over the state-of-the-art ap-
proaches. Our work indicates that long distance
dependencies can improve the performance of lo-
cal segmenter. In the future, we will study alterna-

tive ways of modeling long distance dependencies.

5 Acknowledgments

We thank Xiaoyan Cai for her valuable sug-
gestions. This work was supported in part by
National Natural Science Foundation of China
(No. 61300063), National High Technology Re-
search and Development Program of China (863
Program, No. 2015AA015404), and Doctoral
Fund of Ministry of Education of China (No.
20130001120004). Xu Sun is the corresponding
author.

References
Xinchi Chen, Xipeng Qiu, Chenxi Zhu, and Xuan-

jing Huang. 2015. Gated recursive neural network
for chinese word segmentation. In ACL (1), pages
1744–1753. The Association for Computer Linguis-
tics.

Fei Cheng, Kevin Duh, and Yuji Matsumoto. 2015.
Synthetic word parsing improves chinese word seg-
mentation. In Proceedings of the 53rd Annual Meet-
ing of the Association for Computational Linguistics
and the 7th International Joint Conference on Natu-
ral Language Processing (Volume 2: Short Papers),
pages 262–267, Beijing, China, July. Association for
Computational Linguistics.

Ronan Collobert, Jason Weston, Léon Bottou, Michael
Karlen, Koray Kavukcuoglu, and Pavel Kuksa.
2011. Natural language processing (almost) from
scratch. J. Mach. Learn. Res., 12:2493–2537,
November.

Thomas Emerson. 2005. The second international chi-
nese word segmentation bakeoff. In Proceedings of
the Fourth SIGHAN Workshop on Chinese Language
Processing, pages 123–133.

G. Hinton. 2012. Lecture 6.5: rmsprop: divide the gra-
dient by a running average of its recent magnitude.
coursera: Neural networks for machine learning.

John D. Lafferty, Andrew McCallum, and Fernando
C. N. Pereira. 2001. Conditional random fields:
Probabilistic models for segmenting and labeling se-
quence data. In Proceedings of the Eighteenth In-
ternational Conference on Machine Learning, num-
ber 8 in ICML ’01, pages 282–289, San Francisco,
CA, USA. Morgan Kaufmann Publishers Inc.

Jianqiang Ma and Erhard Hinrichs. 2015. Accurate
linear-time chinese word segmentation via embed-
ding matching. In Proceedings of the 53rd Annual
Meeting of the Association for Computational Lin-
guistics and the 7th International Joint Conference
on Natural Language Processing (Volume 1: Long
Papers), pages 1733–1743, Beijing, China, July. As-
sociation for Computational Linguistics.

571



Wenzhe Pei, Tao Ge, and Baobao Chang. 2014. Max-
margin tensor neural network for chinese word seg-
mentation. In Proceedings of the 52nd Annual Meet-
ing of the Association for Computational Linguis-
tics (Volume 1: Long Papers), pages 293–303, Bal-
timore, Maryland, June. Association for Computa-
tional Linguistics.

Martin Riedmiller and Heinrich Braun. 1993. A
direct adaptive method for faster backpropagation
learning: The rprop algorithm. In IEEE INTERNA-
TIONAL CONFERENCE ON NEURAL NETWORK-
S, pages 586–591.

Weiwei Sun and Jia Xu. 2011. Enhancing chi-
nese word segmentation using unlabeled data. In
Conference on Empirical Methods in Natural Lan-
guage Processing, EMNLP 2011, 27-31 July 2011,
John Mcintyre Conference Centre, Edinburgh, Uk,
A Meeting of Sigdat, A Special Interest Group of the
ACL, pages 970–979.

Xu Sun, Yaozhong Zhang, Takuya Matsuzaki, Yoshi-
masa Tsuruoka, and Jun’ichi Tsujii. 2009. A dis-
criminative latent variable chinese segmenter with
hybrid word/character information. In Proceedings
of Human Language Technologies: The 2009 An-
nual Conference of the North American Chapter
of the Association for Computational Linguistics,
pages 56–64, Boulder, Colorado, June. Association
for Computational Linguistics.

Xu Sun, Houfeng Wang, and Wenjie Li. 2012. Fast on-
line training with frequency-adaptive learning rates
for chinese word segmentation and new word detec-
tion. In Proceedings of the 50th Annual Meeting of
the Association for Computational Linguistics (Vol-
ume 1: Long Papers), pages 253–262, Jeju Island,
Korea, July. Association for Computational Linguis-
tics.

Xu Sun, Yao zhong Zhang, Takuya Matsuzaki, Yoshi-
masa Tsuruoka, and Jun’ichi Tsujii. 2013. Prob-
abilistic chinese word segmentation with non-local
information and stochastic training. Inf. Process.
Manage., 49(3):626–636.

Xu Sun. 2014. Structure regularization for structured
prediction. In Advances in Neural Information Pro-
cessing Systems 27, pages 2402–2410.

N. Xue and L. Shen. 2003. Chinese Word Segmen-
tation as LMR Tagging. In Proceedings of the 2nd
SIGHAN Workshop on Chinese Language Process-
ing.

Naiwen Xue, Fei Xia, Fu-dong Chiou, and Marta
Palmer. 2005. The Penn Chinese TreeBank: Phrase
structure annotation of a large corpus. Natural Lan-
guage Engineering, 11(2):207–238, June.

Yue Zhang and Stephen Clark. 2007. Chinese segmen-
tation with a word-based perceptron algorithm. In
Proceedings of the 45th Annual Meeting of the As-
sociation of Computational Linguistics, pages 840–
847, Prague, Czech Republic, June. Association for
Computational Linguistics.

Ruiqiang Zhang, Genichiro Kikui, and Eiichiro Sumi-
ta. 2006. Subword-based tagging by condition-
al random fields for chinese word segmentation.
In Proceedings of the Human Language Technolo-
gy Conference of the NAACL, Companion Volume:
Short Papers, NAACL-Short ’06, pages 193–196,
Stroudsburg, PA, USA. Association for Computa-
tional Linguistics.

Longkai Zhang, Houfeng Wang, Xu Sun, and Mairgup
Mansur. 2013. Exploring representations from un-
labeled data with co-training for chinese word seg-
mentation. In EMNLP, pages 311–321. ACL.

Xiaoqing Zheng, Hanyang Chen, and Tianyu Xu.
2013. Deep learning for chinese word segmentation
and pos tagging. In EMNLP, pages 647–657. ACL.

572


