



















































Structured Alignment Networks for Matching Sentences


Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 1554–1564
Brussels, Belgium, October 31 - November 4, 2018. c©2018 Association for Computational Linguistics

1554

Structured Alignment Networks for Matching Sentences

Yang Liu∗∗♠, Matt Gardner♦, and Mirella Lapata♠

♠University of Edinburgh
♦Allen Institute for Artificial Intelligence

yang.liu2@ed.ac.uk mattg@allenai.org mlap@inf.ed.ac.uk

Abstract

Many tasks in natural language processing in-
volve comparing two sentences to compute
some notion of relevance, entailment, or sim-
ilarity. Typically, this comparison is done ei-
ther at the word level or at the sentence level,
with no attempt to leverage the inherent struc-
ture of the sentence. When sentence structure
is used for comparison, it is obtained during a
non-differentiable pre-processing step, leading
to propagation of errors. We introduce a model
of structured alignments between sentences,
showing how to compare two sentences by
matching their latent structures. Using a struc-
tured attention mechanism, our model matches
candidate spans in the first sentence to can-
didate spans in the second sentence, simulta-
neously discovering the tree structure of each
sentence. Our model is fully differentiable
and trained only on the matching objective.
We evaluate this model on two tasks, entail-
ment detection and answer sentence selection,
and find that modeling latent tree structures re-
sults in superior performance. Analysis of the
learned sentence structures shows they can re-
flect some syntactic phenomena.

1 Introduction

There are many tasks in natural language process-
ing that require matching two sentences: natural
language inference (Bowman et al., 2015; Nangia
et al., 2017) and paraphrase detection (Wang et al.,
2017b) are classification tasks over sentence pairs,
and question answering often requires an align-
ment between a question and a passage of text that
may contain the answer (Tan et al., 2016a; Ra-
jpurkar et al., 2016; Joshi et al., 2017).

Most neural models for these tasks perform
comparisons between the two sentences either at

∗Work done during an internship at Allen Institute for
Artificial Intelligence.

the word level (Parikh et al., 2016), or at the sen-
tence level (Bowman et al., 2015). Word-level
comparisons ignore the inherent structure of the
sentences being compared, at best relying on a re-
current neural network such as an LSTM (Hochre-
iter and Schmidhuber, 1997) to incorporate some
amount of context from neighboring words into
each word’s representation. Sentence-level com-
parisons can incorporate the structure of each sen-
tence individually (Bowman et al., 2016; Tai et al.,
2015), but cannot easily compare substructures
between the sentences, as these are all squashed
into a single vector. Some models do incorporate
sentence structure by comparing subtrees between
the two sentences (Zhao et al., 2016; Chen et al.,
2017), but require pipelined approaches where a
parser is run in a non-differentiable preprocessing
step, losing the benefits of end-to-end training.

In this paper we propose a method, which we
call structured alignment networks, to perform
comparisons between substructures in two sen-
tences, in a more interpretable way, and without
relying on an external, non-differentiable parser.
We use a structured attention mechanism (Kim
et al., 2017; Liu and Lapata, 2018) to compute a
structured alignment between the two sentences,
jointly learning a latent tree structure for each sen-
tence and aligning spans between the two sen-
tences.

Our method constructs a CKY chart for each
sentence using the inside-outside algorithm (Man-
ning et al., 1999), which is fully differentiable (Li
and Eisner, 2009; Gormley et al., 2015). This chart
has a node for each possible span in the sentence,
and a score for the likelihood of that span being
a constituent in a parse of the sentence, marginal-
ized over all possible parses. We take these two
charts and find alignments between them, repre-
senting each span in each sentence with structured
attention over spans in the other sentence. These



1555

span representations, weighted by the span’s like-
lihood, are then used to compare the two sen-
tences. In this way, we can perform compar-
isons between sentences by leveraging their in-
ternal structure in an end-to-end, fully differen-
tiable model, trained only on one final objective.
Our model helps obtain more precise representa-
tions of the sentence pair, with the learned tree
structures and the alignment between them, and
provides better interpretability, which most neural
models lack in sentence matching tasks.

We evaluate this model on two sentence com-
parison datasets: SNLI (Bowman et al., 2015) and
TREC-QA (Voorhees and Tice, 2000). We find
that comparing sentences at the span level consis-
tently outperforms comparing at the word level.
Additionally, the learned sentence structures rep-
resent well-formed trees that reflect some syntac-
tic phenomena.

2 Word-level Comparison Baseline

We first describe a common word-level compari-
son model, called decomposable attention (Parikh
et al., 2016). This model was first proposed for
the natural language inference task, but similar
mechanisms have been used in many other tasks,
such as for aligning question and passage words
in the bi-directional attention model for ques-
tion answering (Seo et al., 2017). This model
serves as our main point of comparison, as our
latent tree matching model simply replaces the
word-level comparisons in decomposable atten-
tion model with span comparisons.

The decomposable attention model consists of
three steps: attend, compare, and aggregate. As
input, the model takes two sentences a and b
represented by sequences of word embeddings
[a1, · · · ,am] and [b1, · · · , bn]. In the attend step,
the model computes attention scores for each pair
of words across the two input sentences and nor-
malizes them as a soft alignment from a to b (and
vice versa):

eij = F1(ai)
TF1(bj) (1)

Bi =

n∑
j=1

exp(eij)∑n
k=1 exp(eik)

bj (2)

Aj =

m∑
i=1

exp(eij)∑m
k=1 exp(ekj)

ai (3)

where F1 is a feed-forward neural network, Bi is
the weighted summation of the words in b that are

softly aligned to word ai and vice versa forAj .
In the compare step, the input vectors ai and

bj are concatenated with their corresponding at-
tended vector Bi and Aj , and fed into a feed-
forward neural network, giving a comparison be-
tween each word and the words it aligns to in the
other sentence:

vai = F2([ai,Bi]) ∀i ∈ [1, · · · ,m] (4)
vbj = F2([bj ,Aj ]) ∀j ∈ [1, · · · , n] (5)

The aggregate step is a simple summation of
vai and vbj for each word in sentence a and b,
and the two resulting fixed-length vectors are con-
catenated and fed into a linear layer with Wy as
the weight matrix, followed by a softmax layer for
predicting the distribution y:

va =

m∑
i=1

vai vb =

n∑
j=1

vbj (6)

y = softmax(Wy[va,vb])) (7)

The decomposable attention model completely
ignores the order and context of words in the se-
quence. There are some efforts to strengthen the
decomposable attention model with a recurrent
neural network (Liu and Lapata, 2018) or intra-
sentence attention (Parikh et al., 2016). However,
these models amount to simply changing the input
vectors a and b, and still only perform a word-
level alignment between the two sentences.

3 Structured Alignment Networks

Language is inherently tree structured, and the
meaning of sentences comes largely from compos-
ing the meanings of subtrees (Chomsky, 2002). It
is natural, then, to compare the meaning of two
sentences by comparing their substructures (Mac-
Cartney and Manning, 2009). For example, when
determining the relationship between two sen-
tences in Figure 1, the ideal units of comparison
are spans determined by subtrees: “is in Seattle”
compared to “based in Washington state”.

The challenge with comparing spans drawn
from subtrees is that the tree structure of the sen-
tence is latent and must be inferred, either dur-
ing pre-processing or in the model itself. In this
section we present a model that operates on the
latent tree structure of each sentence, comparing
all possible spans in one sentence with all possi-
ble spans in the second sentence, weighted by how



1556

A: the headquarter of BOEING is in Seattle      

B: Boeing is a company based in Washington state

Figure 1: Example span alignments of a sen-
tence pair, where different colors indicate match-
ing spans. Note that some spans overlap, which
cannot happen in a single tree; our model consid-
ers all possible span comparisons, weighted by the
spans’ marginal likelihood.

likely each span is to appear as a constituent in a
parse of the sentence. We use the non-terminal
nodes of a binary constituency parse to represent
spans. Because of this choice of representation,
we can use the nodes in a CKY parsing chart to ef-
ficiently marginalize span likelihood over all pos-
sible parses for each sentence, and compare nodes
in each sentence’s chart.

3.1 Learning Latent Constituency Trees
A constituency parser can be partially formal-
ized as a graphical model with the following
cliques (Klein and Manning, 2004): the latent
variables cikj ∈ 0, 1 for all i < j, indicating
whether the span from the i-th token to the j-th
token (spanij) is a constituency node built from
the merging of sub-node spanik and span(k+1)j .
Given a sentence x = [xi, · · · , xn], the probability
of a tree z is,

p(z|x) =
∏

cikj∈z p(cikj = 1)∑
z′∈Z

∏
cikj∈z′ p(cikj = 1)

(8)

where Z represents all possible constituency trees
for x.

The parameters for the graph-based CRF con-
stituency parser are δikj reflecting the scores of
spanij forming a binary constituency node with k
as the splitting point. It is possible to calcu-
late the marginal probability of each constituency
node p(cijk = 1|x) using the inside-outside algo-
rithm (Klein and Manning, 2003). Although the
inside-outside algorithm is constrained to gener-
ate a binary tree, this is not a severe limitation,
as most structures can be easily binarized (Finkel
et al., 2008).

In a typical constituency parser, the score δikj is
parameterized according to the production rules of
a grammar, e.g., with normalized categorical dis-
tributions for each non-terminal. Our unlabeled
grammar effectively has only a single production

 

wi wk-1 wk wj

wk wi-1 wi wjw1 wn

 

 

(a) Inside pass

 

wi wk-1 wk wj

wk wi-1 wi wjw1 wn

 

 

(b) Outside pass

Figure 2: The inside-outside algorithm. (a) is the
process for calculating the inside score αij . Three
yellow spaces indicate αi(k−1), αkj and δikj . (b)
is a part of the process for calculating the outside
scoreβij , with target span spanij as the right child
of a non-terminal. The blue space indicates βkj
and two yellow spaces indicate αk(i−1) and δkij .

rule, however, we parameterize these scores as bi-
linear functions operating on the representations
of the two subtrees being merged. For the in-
side pass, as illustrated in Figure 2a, the inside
scoreαij for span from position i to j is marginal-
ized over the splitting points k:

δikj = sp
T
ikWsp(k+1)j (9)

αij =
∑

i < k ≤ j
δikjαi(k−1)αkj (10)

where spij ∈ Rd is the representation for the span,
and W ∈ Rd∗d is the weight matrix. This process
is calculated recursively from bottom to root, gen-
erating the score for each possible constituent.

For the outside pass, the outside score βij is:

βij =
∑

1≤k<i
δkijαk(i−1)βkj

+
∑

j<k≤n
δijkα(j+1)kβik (11)

where the first term is the score for spanij be-
ing the right child on a non-terminal node and the
second term is the score for spanij being the left
child. In Figure 2b, we illustrate the outside pro-
cess with the target span spanij being the right



1557

child of a non-terminal node. This process is cal-
culated recursively from root to bottom.

The normalized marginal probability ρij for
each span spanij , where 1 ≤ i < n, i < j ≤ n
can be calculated by:

ρij = αijβij/α0n (12)

To compute the representations of all pos-
sible spans, we use Long Short-Term Mem-
ory Neural Networks (LSTMs; Hochreiter and
Schmidhuber 1997) with max-pooling and mi-
nus features (Cross and Huang, 2016; Liu
and Lapata, 2017). We represent each sen-
tence as a sequence of word embeddings
[wsos,w1, · · · ,wt, · · · ,wn,weos] and run a bidi-
rectional LSTM to obtain the output vectors.
ht = [~ht, ~ht] is the output vector for the tth word,
and ~ht and ~ht are the output vectors from the for-
ward and backward directions, respectively. We
represent a constituent from position i to j with a
span vector spij :

spmaxpoolij = max(hi, · · · ,hj) (13)

spminusij = [
~hj − ~hi−1, ~hi − ~hj+1] (14)

spij = [sp
maxpool
ij , sp

minus
ij ] (15)

where max(xi, · · · ,xj) is the max-pooling oper-
ation over the sequence of output vectors within
this constituent.

After applying the parsing process on two sen-
tences, we obtain the marginal probabilities for
all potential spans of the two constituency trees,
which can then be used for aligning.

3.2 Learning Structured Alignments

After learning latent constituency trees for each
sentence, we are able to perform span-level com-
parisons between the two sentences, instead of
the word-level comparisons done by the decom-
posable attention model. The structure of these
two comparison models is the same, but the ba-
sic elements of our structured alignment model are
spans instead of words, and the marginal proba-
bilities obtained from the inside-outside algorithm
are used as a re-normalization value for incorpo-
rating structural information into the alignments.

For sentence a, we have the representation spaij
for each spanij and its marginal probability ρaij .
And for sentence b, we also get spbij and ρ

b
ij . The

attention scores are computed between all pairs of

spans across the two sentences, and the attended
vectors can be calculated as:

eij,kl = F1(sp
a
ij)

TF1(sp
b
kl) (16)

Bij =
n∑

k=1

n∑
l=k

exp(eij,kl + ln(ρ
b
kl))

n∑
s=1

n∑
t=s

exp(eij,st + ln(ρbst))

spbkl

(17)

Akl =

m∑
i=1

m∑
j=i

exp(eij,kl + ln(ρ
a
ij))

m∑
s=1

m∑
t=s

exp(est,kl + ln(ρ
a
st))

spaij

(18)

Then, the span vectors are concatenated with
the attended vectors and fed into a feed-forward
neural network:

vaij = F2([sp
a
ij ,Bij ]) (19)

vbkl = F2([sp
b
kl,Akl]) (20)

To aggregate these vectors, instead of using di-
rect summation, we apply weighted summation
with the marginal probabilities as weights:

va =

m∑
i=1

m∑
j=i

ρaijv
a
ij ;vb =

n∑
k=1

n∑
l=1

ρbklv
b
kl (21)

where ρa and ρb work like the self-attention mech-
anism of (Lin et al., 2017) to replace the summa-
tion pooling step. We use a softmax function to
compute the predicted distribution y of the input
sentence pair:

y = softmax(Wy[va,vb]) (22)

4 Experiments

We evaluate our structured alignment model on
two natural language matching tasks: question an-
swering as sentence selection and natural language
inference. We view our approach as a module for
replacing the widely-used word-level alignment
which can be plugged into other neural models.
For that reason, our experiments are not intended
to show performance improvements over state-of-
the-art neural network architectures. Rather our
evaluation studies aim to address three questions:
(a) whether our methods can be trained effectively
in an end-to-end fashion; (b) whether they yield
improvements over standard word-level alignment
models; and (c) whether they can learn plausible
latent constituency tree structures.



1558

Models MAP MRR
Word-level Attention 0.764 0.842
Simple Span Alignment 0.772 0.851
Simple Span Alignment + External Parser 0.780 0.846
Structured Alignment (Shared Parameters) 0.780 0.860
Structured Alignment (Separated Parameters) 0.786 0.860
QA-LSTM (Tan et al., 2016b) 0.730 0.824
Attentive Pooling Network (Santos et al., 2016) 0.753 0.851
Pairwise Word Interaction (He and Lin, 2016) 0.777 0.836
Lexical Decomposition and Composition (Wang et al., 2016) 0.771 0.845
Noise-Contrastive Estimation (Rao et al., 2016) 0.801 0.877
BiMPM (Wang et al., 2017b) 0.802 0.875

Table 1: Results of our models (top) and previously proposed systems (bottom) on the TREC-QA test set.

For both tasks, we initialize our model
with 300D 840B GloVe word embeddings (Pen-
nington et al., 2014). The hidden size for the BiL-
STM is 150. The feed-forward networks F1 and
F2 are two-layer perceptrons with ReLU as the
hidden activation function and the size of the hid-
den and output layers is set to 300. All hyper-
parameters are selected based on the model’s per-
formance on the development set.

4.1 Answer Sentence Selection

We first study the effectiveness of our model for
answer sentence selection tasks. Given a question,
answer sentence selection aims to rank a list of
candidate answer sentences based on their related-
ness to the question. We experiment on the TREC-
QA dataset (Wang et al., 2007), in which all ques-
tions with only positive or negative answers are
removed. This leaves us with 1,162 training ques-
tions, 65 development questions and 68 test ques-
tions. Experimental results are listed in Table 1.
We measure performance by the mean average
precision (MAP) and mean reciprocal rank (MRR)
using the standard TREC evaluation script.

In the first block of Table 1, we compare our
model and variants thereof against several base-
lines. The first baseline is the Word-level De-
composable Attention model strengthened with a
bidirectional LSTM for obtaining a contextualized
representation for each word. The second baseline
is a Simple Span Alignment model; we use an MLP
layer over the LSTM outputs to calculate the un-
normalized scores and replace the inside-outside
algorithm with a simple softmax function to ob-
tain the probability distribution over all candidate

spans. We also introduce a pipelined baseline
where we extract constituents from trees parsed by
the CoreNLP (Manning et al., 2014) constituency
parser, and use the Simple Span Alignment model
to only align these constituents.

As shown in Table 1, we use two variants of the
Structured Alignment model, since the structure of
the question and the answer sentence may be dif-
ferent; the first model shares parameters across the
question and the answer for computing the struc-
tures, while the second one uses separate param-
eters. We view the sentence selection task as a
binary classification problem and the final ranking
is based on the predicted probability of the sen-
tence containing the correct answer (positive la-
bel). We apply dropout to the output of the BiL-
STM with dropout ratio set to 0.2. All param-
eters (including word embeddings) are updated
with AdaGrad (Duchi et al., 2011), and the learn-
ing rate is set to 0.05.

Table 1 (second block) also reports the perfor-
mance of various comparison systems and state-
of-the-art models. As can be seen, on both MAP
and MRR metrics, structured alignment models
perform better than the decomposable attention
model, showing that structural bias is helpful for
matching a question to the correct answer sen-
tence. We also observe that using separate param-
eters achieves higher scores on both metrics. The
simple span alignment model obtains results simi-
lar to the decomposable attention model, suggest-
ing that the shallow softmax distribution is ineffec-
tive for capturing structural information and may
even introduce redundant noise. The pipelined
model with an external parser also slightly im-



1559

Models Accuracy # Parameters
Word-level Attention 85.8 1.1M
Simple Span Alignment 85.4 1.26M
Simple Span Alignment + External Parser 85.6 1.17M
Structured Alignment 86.3 1.44M
Classifier with handcrafted features (Bowman et al., 2015) 78.2 -
LSTM encoders (Bowman et al., 2015) 80.6 3.0M
LSTM with inter-attention (Rocktäschel et al., 2016) 83.5 252K
Matching LSTMs (Wang and Jiang, 2015) 86.1 1.9M
LSTMN with deep attention fusion (Cheng et al., 2016) 86.3 3.4M
Enhanced BiLSTM Inference Model (Chen et al., 2016) 88.0 4.3M
Densely Interactive Inference Network (Gong et al., 2017) 88.0 -

Table 2: Test accuracy (%) on the SNLI dataset. Wherever available we also provide the number of
parameters (excluding embeddings).

proves upon the baseline, but still cannot outper-
form the end-to-end trained structured alignment
model which achieves results comparable with
several strong baselines with fewer parameters. As
mentioned earlier, our model could be used as a
plug-in component for other more complex mod-
els, and may boost their performance by modeling
the latent structures. At the same time, the struc-
tured alignment can provide better interpretability
for sentence matching tasks, which is a defect of
most neural models.

4.2 Natural Language Inference

The second task we consider is natural language
inference, where the input is a pair of premise
and hypothesis sentences, and the goal is to pre-
dict whether the premise entails the hypothesis,
contradicts the hypothesis, or neither. For this
task, we use the Stanford NLI dataset (Bowman
et al., 2015). After removing sentences with un-
known labels, we obtain 549,367 pairs for train-
ing, 9,842 for development and 9,824 for testing.

We compare our model against the same base-
lines used in the question answering task. All
parameters (including word embeddings) are up-
dated with AdaGrad (Duchi et al., 2011), and the
learning rate is set to 0.05. Dropout is used with
ratio 0.2. The structured alignment model in this
experiment uses shared parameters for computing
latent tree structures, since both the premise and
hypothesis are declarative sentences.

The results of our experiments are shown in
Table 2. Similar to the answer selection task,
the tree matching model outperforms the decom-
posable model. Our structured alignment model

gains 0.5% in accuracy over the baseline word-
level comparison model without any additional an-
notation, simply from introducing a structural bias
in the alignment between the sentences. Simple
span alignment, however, is not helpfult and even
slightly degrades the performance over the word-
level model.

4.3 Analysis of Learned Tree Structures

In this section, we give a brief qualitative analysis
of the learned tree structures. We present the CKY
charts for two randomly-selected sentence pairs in
the SNLI test set in Figure 3. Recall that the CKY
chart shows the likelihood of each span appear-
ing as a constituent in the parse of the sentence,
marginalized over all possible parses. By visu-
alizing these span probabilities, we can see that
the model learns structures which correspond to
known syntactic structures.

In subfigure (a), we can see that band is play-
ing is a very-likely span, as is at a large venue.
In subfigure (b), the phrases performing at a lo-
cal bar and at a local bar or club also receive
high probabilities. For the second sentence pair,
we see that the model can even resolve some at-
tachment ambiguities correctly. The prepositional
phrase with green feathers, has a very low score
for being attached to women. Instead, the model
prefers to attach it to lingerie, forming the span
lingerie with green feathers. We also present the
top-5 spans and their alignments in subfigures (c)
and (d), which can be used to interpret model de-
cisions for sentence matching tasks.

The analysis above and our experimental re-
sults in the previous section suggest that our



1560

ba
nd

is pla
yin
g
mu
sic

at a larg
e
ven

ue

the

band

is

playing

music

at

a

large

(a) Premise Sentence

ba
nd

pe
rfo
rm
ing

at a loc
al

ba
r

or clu
b

a

band

performing

at

a

local

bar

or

(b) Hypothesis sentence

a band performing

at a local bar or club

at a local bar

local bar or club

performing at a local bar

is playing music

playing music

large venue

at a large venue

the band is playing music

(c) Alignment of top-5 spans

wo
me
n

in ling
eri
e

wit
h

gre
en

fea
the
rs

aro
un
d

wa
ist

two

women

in

lingerie

with

green

feathers

around

(d) Premise Sentence

wo
me
n

do n't we
ar

mu
ch

clo
thi
ng

two

women

do

n't

wear

much

(e) Hypothesis sentence

a band performing

at a local bar or club

at a local bar

local bar or club

performing at a local bar

is playing music

playing music

large venue

at a large venue

the band is playing music

two women in lingerie

lingerie with

with green feathers around waist

lingerie with green feathers

two women two women

much clothing

wear much

women do n't wear much

n't wear much clothing

(f) Alignment of top-5 spans

Figure 3: (a), (b), (d), and (e) are CKY charts showing marginalized span probabilities for sentence pairs
in the SNLI test set. Each cell uses depth of the color to represent the probability of the span (from
the i-th word to the j-th word) forming a proper constituent. (c) and (f) are the alignments of the top-5
spans from hypothesis sentence to the premise sentence, where the boldness of the lines indicates the
probability of spans being aligned.

model is able to learn tree structures which are
closely related to syntax, and in addition reflect
the semantic-level characteristics of the task at
hand. In both question answering and natural
language inference tasks, we observe that struc-
tured alignment leads to performance improve-
ments over word-level models. This is in con-
trast to prior work (Williams et al., 2017), where
the discovery of tree structures based on a seman-
tic objective is not helpful. Although we use the
same supervision signal in our model, a difference
between the two approaches is that they are try-
ing to learn tree structures for each sentence in-
dependently, performing comparisons at the sen-
tence level only. Comparing spans directly forces
the model to induce trees with comparable con-
stituents, giving the model a stronger inductive
bias.

Although our main goal is not to induce a gram-
mar, we perform some simple experiments to com-
pare the learned latent trees with parser-generated
ones. We parse the sentences in both test-sets
with the CoreNLP (Manning et al., 2014) con-

stituency parser to obtain silver trees. Based on
the parsing part of the trained structured align-
ment model, we compute the marginal probabil-
ities of test sentences and feed them into CKY al-
gorithm (Younger, 1967) to find the most likely
constituency trees. We then convert both silver
and latent trees to sets of constituent brackets,
and calculate the accuracy of the learned brack-
ets against the silver parses. We use different
combinations of training- and test-sets to examine
the transferability of the learned tree structures.
The results are shown in Table 3. We can see
that although our model does not have any tree-
structured input during training, it can still outper-
form the left-branching (LB) and right-branching
baselines (RB) and achieve some consistency with
the parser generated trees.

5 Related Work

Sentence comparison models The Stanford
natural language inference dataset (Bowman et al.,
2015), and the expanded multi-genre natural lan-
guage inference dataset (Nangia et al., 2017), are



1561

tested on
trained on

SNLI TREC LB RB

SNLI 15.1 10.7 12.8 6.0
TREC 12.3 11.4 10.5 3.2

Table 3: Brackets accuracy of latent learned trees
against silver trees from CoreNLP parser. We
show the transferability of the learned parser by
applying it on a test-set different from the training-
set. For example, we train the structured align-
ment model on the TREC-QA data, and apply it
on SNLI to obtain the tree distributions. LB and
RB are left- and right-branching baselines.

the most well-known recent sentence comparison
tasks. The literature on this comparison task is
far too extensive to include here, although the re-
cent shared task on Multi-NLI gives a good sur-
vey of sentence-level comparison models (Nan-
gia et al., 2017). Some of these models use sen-
tence structures, which are obtained either in a la-
tent fashion (Bowman et al., 2016) or during pre-
processing (Zhao et al., 2016), but they squash all
of the structure into a single vector, losing the abil-
ity to easily compare substructures between the
two sentences.

For models doing a word-level comparison,
the decomposable attention model (Parikh et al.,
2016), which we have discussed already in this pa-
per, is the most salient example, although many
similar models exist in the literature (Chen et al.,
2017; Wang et al., 2017b). The idea of word-level
alignments between a question and a passage is
also pervasive in the recent question answering lit-
erature (Seo et al., 2017; Wang et al., 2017a).

Finally, and most similar to our approach, sev-
eral models have been proposed that directly
compare subtrees between two sentences (Chen
et al., 2017; Zhao et al., 2016). However, all of
these models are pipelined; they obtain the sen-
tence structure in a non-differentiable preprocess-
ing step, losing the benefits of end-to-end train-
ing. Ours is the first model to allow comparison
between latent tree structures, trained end-to-end
on the comparison objective.

Structured attention While it has long been
known that inference in graphical models is dif-
ferentiable (Li and Eisner, 2009; Domke, 2011),
and using inference in, e.g., a CRF (Lafferty et al.,
2001) as the last layer in a neural network is com-
mon practice (Liu and Lapata, 2017; Lample et al.,

2016), the use of inference algorithms as interme-
diate layers in end-to-end neural networks is a re-
cent development. Kim et al. (2017) were the first
to use inference to compute structured attentions
over latent sentence variables, inducing tree struc-
tures trained on the end-to-end objective. Liu and
Lapata (2018) showed how to do this more effi-
ciently, although their work is still limited to struc-
tured attention over a single sentence. Our model
is the first to include latent structured alignments
between two sentences.

Grammar Induction Unsupervised grammar
induction is a well-studied problem (Cohen and
Smith, 2009). The most recent work in this di-
rection was the Neural E-DMV model of Jiang
et al. (2016). While our goal is not to induce a
grammar, we do produce a probabilistic grammar
as a byproduct of our model. Our results suggest
that training on more complex objectives may be
a good way to pursue grammar induction in the
future; forcing the model to construct consistent,
comparable subtrees between the two sentences is
a strong signal for grammar induction. Very re-
cently, a few models attempt to infer latent depen-
dency tree structures with neural models in sen-
tence modeling tasks (Yogatama et al., 2017; Choi
et al., 2018).

6 Conclusions

In this paper we have considered the problem of
comparing two sentences in natural language pro-
cessing models. We have shown how to move
beyond word- and sentence-level comparison to
comparing spans between two sentences, without
the need for an external parser. Through experi-
ments on sentence comparison datasets, we have
seen that span comparisons consistently outper-
form word-level comparisons, with no additional
supervision. The proposed model can be trained
effectively, in an end-to-end fashion and is able to
induce plausible tree structures.

Our results have several implications for future
work. First, the success of span comparisons over
word-level comparisons suggests that it may be
advantageous to include such comparisons in more
complex models, either for comparing two sen-
tences directly, or as intermediate parts of models
for more complex tasks, such as reading compre-
hension. Second, our model’s ability to infer trees
from a semantic objective is intriguing, and sug-
gestive of future opportunities in grammar induc-



1562

tion research. The use of the inside-outside algo-
rithm unavoidably renders the full model er (by 5–
8 times) compared to the decomposable attention
model. We hope to find a more efficient way to
accelerate this dynamic programming method on
a GPU.

Acknowledgments This research is supported
by a Google PhD Fellowship to the first author.
We acknowledge the financial support of the Eu-
ropean Research Council (Lapata; award number
681760).

References
Samuel R. Bowman, Gabor Angeli, Christopher Potts,

and Christopher D. Manning. 2015. A large anno-
tated corpus for learning natural language inference.
In Proceedings of the 2015 Conference on Empiri-
cal Methods in Natural Language Processing, pages
632–642, Lisbon, Portugal.

Samuel R. Bowman, Jon Gauthier, Abhinav Ras-
togi, Raghav Gupta, Christopher D. Manning, and
Christopher Potts. 2016. A fast unified model for
parsing and sentence understanding. In Proceed-
ings of the 54th Annual Meeting of the Association
for Computational Linguistics (Volume 1: Long Pa-
pers), pages 1466–1477, Berlin, Germany.

Qian Chen, Xiaodan Zhu, Zhen-Hua Ling, Si Wei, Hui
Jiang, and Diana Inkpen. 2017. Enhanced LSTM for
natural language inference. In Proceedings of the
55th Annual Meeting of the Association for Compu-
tational Linguistics (Volume 1: Long Papers), pages
1657–1668, Vancouver, Canada.

Qian Chen, Xiaodan Zhu, Zhenhua Ling, Si Wei, and
Hui Jiang. 2016. Enhancing and combining sequen-
tial and tree LSTM for natural language inference.
arXiv preprint arXiv:1609.06038.

Jianpeng Cheng, Li Dong, and Mirella Lapata. 2016.
Long short-term memory-networks for machine
reading. In Proceedings of the 2016 Conference on
Empirical Methods in Natural Language Process-
ing, pages 551–561, Austin, Texas.

Jihun Choi, Kang Min Yoo, and Sang goo Lee. 2018.
Learning to compose task-specific tree structures. In
Proceedings of the 32nd AAAI Conference on Arti-
ficial Intelligence, pages 5094–5101, New Orleans,
Louisiana.

Noam Chomsky. 2002. Syntactic structures. Walter de
Gruyter.

Shay Cohen and Noah A. Smith. 2009. Shared lo-
gistic normal distributions for soft parameter tying
in unsupervised grammar induction. In Proceed-
ings of Human Language Technologies: The 2009

Annual Conference of the North American Chap-
ter of the Association for Computational Linguistics,
pages 74–82, Boulder, Colorado.

James Cross and Liang Huang. 2016. Span-based con-
stituency parsing with a structure-label system and
provably optimal dynamic oracles. In Proceedings
of the 2016 Conference on Empirical Methods in
Natural Language Processing, pages 1–11, Austin,
Texas.

Justin Domke. 2011. Parameter learning with trun-
cated message-passing. In Proceedings of the 2011
IEEE Conference on Computer Vision and Pattern
Recognition, pages 2937–2943, Colorado Springs,
Colorado.

John Duchi, Elad Hazan, and Yoram Singer. 2011.
Adaptive subgradient methods for online learning
and stochastic optimization. Journal of Machine
Learning Research, 12:2121–2159.

Jenny Rose Finkel, Alex Kleeman, and Christopher D.
Manning. 2008. Efficient, feature-based, condi-
tional random field parsing. In Proceedings of ACL-
08: HLT, pages 959–967, Columbus, Ohio.

Yichen Gong, Heng Luo, and Jian Zhang. 2017. Natu-
ral language inference over interaction space. arXiv
preprint arXiv:1709.04348.

Matthew Gormley, Mark Dredze, and Jason Eisner.
2015. Approximation-aware dependency parsing by
belief propagation. Transactions of the Association
for Computational Linguistics, 3:489–501.

Hua He and Jimmy Lin. 2016. Pairwise word interac-
tion modeling with deep neural networks for seman-
tic similarity measurement. In Proceedings of the
2016 Conference of the North American Chapter of
the Association for Computational Linguistics: Hu-
man Language Technologies, pages 937–948, San
Diego, California.

Sepp Hochreiter and Jürgen Schmidhuber. 1997.
Long short-term memory. Neural computation,
9(8):1735–1780.

Yong Jiang, Wenjuan Han, and Kewei Tu. 2016. Un-
supervised neural dependency parsing. In Proceed-
ings of the 2016 Conference on Empirical Methods
in Natural Language Processing, pages 763–771,
Austin, Texas.

Mandar Joshi, Eunsol Choi, Daniel Weld, and Luke
Zettlemoyer. 2017. Triviaqa: A large scale distantly
supervised challenge dataset for reading comprehen-
sion. In Proceedings of the 55th Annual Meeting of
the Association for Computational Linguistics (Vol-
ume 1: Long Papers), pages 1601–1611, Vancouver,
Canada.

Yoon Kim, Carl Denton, Luong Hoang, and Alexan-
der M Rush. 2017. Structured attention networks.
In Proceedings of the 5th International Cofnerence
on Learning Representations, Toulon, France.



1563

Dan Klein and Christopher Manning. 2004. Corpus-
based induction of syntactic structure: Models of
dependency and constituency. In Proceedings of
the 42nd Meeting of the Association for Compu-
tational Linguistics, Main Volume, pages 478–485,
Barcelona, Spain.

Dan Klein and Christopher D Manning. 2003. Fast ex-
act inference with a factored model for natural lan-
guage parsing. In Advances in Neural Information
Processing Systems 15, pages 3–10. MIT Press.

John Lafferty, Andrew McCallum, and Fernando C.N.
Pereira. 2001. Conditional random fields: Prob-
abilistic models for segmenting and labeling se-
quence data. In Proceedings of the 18th Interna-
tional Conference on Machine Learning, pages 282–
289, Williamstown, Massachusetts.

Guillaume Lample, Miguel Ballesteros, Sandeep Sub-
ramanian, Kazuya Kawakami, and Chris Dyer. 2016.
Neural architectures for named entity recognition.
In Proceedings of the 2016 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics: Human Language Technologies,
pages 260–270, San Diego, California.

Zhifei Li and Jason Eisner. 2009. First- and second-
order expectation semirings with applications to
minimum-risk training on translation forests. In
Proceedings of the 2009 Conference on Empirical
Methods in Natural Language Processing, pages
40–51, Singapore.

Zhouhan Lin, Minwei Feng, Cicero Nogueira dos San-
tos, Mo Yu, Bing Xiang, Bowen Zhou, and Yoshua
Bengio. 2017. A structured self-attentive sentence
embedding. arXiv preprint arXiv:1703.03130.

Yang Liu and Mirella Lapata. 2017. Learning contex-
tually informed representations for linear-time dis-
course parsing. In Proceedings of the 2017 Con-
ference on Empirical Methods in Natural Language
Processing, pages 1289–1298, Copenhagen, Den-
mark.

Yang Liu and Mirella Lapata. 2018. Learning struc-
tured text representations. Transactions of the Asso-
ciation for Computational Linguistics, 6:63–75.

Bill MacCartney and Christopher D Manning. 2009.
An extended model of natural logic. In Proceedings
of the international conference on computational se-
mantics.

Christopher Manning, Mihai Surdeanu, John Bauer,
Jenny Finkel, Steven Bethard, and David McClosky.
2014. The stanford corenlp natural language pro-
cessing toolkit. In Proceedings of 52nd Annual
Meeting of the Association for Computational Lin-
guistics: System Demonstrations, pages 55–60, Bal-
timore, Maryland.

Christopher D Manning, Hinrich Schütze, et al. 1999.
Foundations of statistical natural language process-
ing, volume 999. MIT Press.

Nikita Nangia, Adina Williams, Angeliki Lazaridou,
and Samuel R Bowman. 2017. The repeval 2017
shared task: Multi-genre natural language infer-
ence with sentence representations. arXiv preprint
arXiv:1707.08172.

Ankur Parikh, Oscar Täckström, Dipanjan Das, and
Jakob Uszkoreit. 2016. A decomposable attention
model for natural language inference. In Proceed-
ings of the 2016 Conference on Empirical Methods
in Natural Language Processing, pages 2249–2255,
Austin, Texas.

Jeffrey Pennington, Richard Socher, and Christopher
Manning. 2014. Glove: Global vectors for word
representation. In Proceedings of the 2014 Con-
ference on Empirical Methods in Natural Language
Processing (EMNLP), pages 1532–1543, Doha,
Qatar.

Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and
Percy Liang. 2016. Squad: 100,000+ questions for
machine comprehension of text. In Proceedings of
the 2016 Conference on Empirical Methods in Natu-
ral Language Processing, pages 2383–2392, Austin,
Texas.

Jinfeng Rao, Hua He, and Jimmy Lin. 2016. Noise-
contrastive estimation for answer selection with
deep neural networks. In Proceedings of the 25th
ACM International on Conference on Information
and Knowledge Management Conference, pages
1913–1916, Indianapolis, Indiana.

Tim Rocktäschel, Edward Grefenstette, Karl Moritz
Hermann, Tomáš Kočiskỳ, and Phil Blunsom. 2016.
Reasoning about entailment with neural attention.
In Proceedings of the 4th International Cofner-
ence on Learning Representations, San Juan, Puerto
Rico.

Cicero dos Santos, Ming Tan, Bing Xiang, and Bowen
Zhou. 2016. Attentive pooling networks. arXiv
preprint arXiv:1602.03609.

Minjoon Seo, Aniruddha Kembhavi, Ali Farhadi, and
Hannaneh Hajishirzi. 2017. Bidirectional attention
flow for machine comprehension. In Proceeding of
the 5th International Cofnerence on Learning Rep-
resentations, Toulon, France.

Kai Sheng Tai, Richard Socher, and Christopher D.
Manning. 2015. Improved semantic representations
from tree-structured long short-term memory net-
works. In Proceedings of the 53rd Annual Meet-
ing of the Association for Computational Linguistics
and the 7th International Joint Conference on Natu-
ral Language Processing (Volume 1: Long Papers),
pages 1556–1566, Beijing, China.

Ming Tan, Cicero dos Santos, Bing Xiang, and Bowen
Zhou. 2016a. Improved representation learning for
question answer matching. In Proceedings of the
54th Annual Meeting of the Association for Compu-
tational Linguistics (Volume 1: Long Papers), pages
464–473, Berlin, Germany.



1564

Ming Tan, Cicero dos Santos, Bing Xiang, and Bowen
Zhou. 2016b. LSTM-based deep learning models
for non-factoid answer selection. In Proceedings of
the ICLR 2016 Workshop Track, San Juan, Puerto
Rico.

Ellen M Voorhees and Dawn M Tice. 2000. Building
a question answering test collection. In Proceedings
of the 23rd annual international ACM SIGIR confer-
ence on Research and development in information
retrieval, pages 200–207, Athens, Greece.

Mengqiu Wang, Noah A. Smith, and Teruko Mita-
mura. 2007. What is the Jeopardy model? a
quasi-synchronous grammar for QA. In Proceed-
ings of the 2007 Joint Conference on Empirical
Methods in Natural Language Processing and Com-
putational Natural Language Learning (EMNLP-
CoNLL), pages 22–32, Prague, Czech Republic.

Shuohang Wang and Jing Jiang. 2015. Learning natu-
ral language inference with LSTM. arXiv preprint
arXiv:1512.08849.

Wenhui Wang, Nan Yang, Furu Wei, Baobao Chang,
and Ming Zhou. 2017a. Gated self-matching net-
works for reading comprehension and question an-
swering. In Proceedings of the 55th Annual Meet-
ing of the Association for Computational Linguistics
(Volume 1: Long Papers), pages 189–198, Vancou-
ver, Canada.

Zhiguo Wang, Wael Hamza, and Radu Florian. 2017b.
Bilateral multi-perspective matching for natural lan-
guage sentences. In Proceedings of the 26th Inter-
national Joint Conference on Artificial Intelligence,
pages 4144–4150, Melbourne, Australia.

Zhiguo Wang, Haitao Mi, and Abraham Ittycheriah.
2016. Sentence similarity learning by lexical de-
composition and composition. In Proceedings of
COLING 2016, the 26th International Conference
on Computational Linguistics: Technical Papers,
pages 1340–1349, Osaka, Japan.

Adina Williams, Andrew Drozdov, and Samuel R Bow-
man. 2017. Learning to parse from a semantic ob-
jective: It works. is it syntax? arXiv preprint
arXiv:1709.01121.

Dani Yogatama, Phil Blunsom, Chris Dyer, Edward
Grefenstette, and Wang Ling. 2017. Learning to
compose words into sentences with reinforcement
learning. In Proceedings of the 5th International
Cofnerence on Learning Representations, Toulon,
France.

Daniel H Younger. 1967. Recognition and parsing of
context-free languages in time n3. Information and
Control, 10:189–208.

Kai Zhao, Liang Huang, and Mingbo Ma. 2016. Tex-
tual entailment with structured attentions and com-
position. In Proceedings of COLING 2016, the
26th International Conference on Computational
Linguistics: Technical Papers, pages 2248–2258,
Osaka, Japan.


