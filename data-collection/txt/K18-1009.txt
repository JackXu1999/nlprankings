



















































Global Attention for Name Tagging


Proceedings of the 22nd Conference on Computational Natural Language Learning (CoNLL 2018), pages 86–96
Brussels, Belgium, October 31 - November 1, 2018. c©2018 Association for Computational Linguistics

86

Global Attention for Name Tagging

Boliang Zhang, Spencer Whitehead, Lifu Huang and Heng Ji
Computer Science Department

Rensselaer Polytechnic Institute
{zhangb8,whites5,huangl7,jih}@rpi.edu

Abstract

Many name tagging approaches use local con-
textual information with much success, but fail
when the local context is ambiguous or lim-
ited. We present a new framework to improve
name tagging by utilizing local, document-
level, and corpus-level contextual informa-
tion. We retrieve document-level context from
other sentences within the same document and
corpus-level context from sentences in other
topically related documents. We propose a
model that learns to incorporate document-
level and corpus-level contextual informa-
tion alongside local contextual information via
global attentions, which dynamically weight
their respective contextual information, and
gating mechanisms, which determine the in-
fluence of this information. Extensive exper-
iments on benchmark datasets show the ef-
fectiveness of our approach, which achieves
state-of-the-art results for Dutch, German, and
Spanish on the CoNLL-2002 and CoNLL-
2003 datasets.1.

1 Introduction

Name tagging, the task of automatically identify-
ing and classifying named entities in text, is often
posed as a sentence-level sequence labeling prob-
lem where each token is labeled as being part of a
name of a certain type (e.g., location) or not (Chin-
chor and Robinson, 1997; Tjong Kim Sang and
De Meulder, 2003). When labeling a token, lo-
cal context (i.e., surrounding tokens) is crucial be-
cause the context gives insight to the semantic
meaning of the token. However, there are many in-
stances in which the local context is ambiguous or
lacks sufficient content. For example, in Figure 1,
the query sentence discusses “Zywiec” selling a

1The programs are publicly available for research pur-
pose: https://github.com/boliangz/global_
attention_ner

product and profiting from these sales, but the lo-
cal contextual information is ambiguous as more
than one entity type could be involved in a sale.
As a result, the baseline model mistakenly tags
“Zywiec” as a person (PER) instead of the cor-
rect tag, which is organization (ORG). If the model
has access to supporting evidence that provides ad-
ditional, clearer contextual information, then the
model may use this information to correct the mis-
take given the ambiguous local context.

Baseline:
So far this year [PER Zywiec], whose full name
is Zaklady Piwowarskie w Zywcu SA, has netted six
million zlotys on sales of 224 million zlotys.

So far this year [ORG Zywiec], whose full name
is Zaklady Piwowarskie w Zywcu SA, has netted six
million zlotys on sales of 224 million zlotys. 

Our model (Document­level + Corpus­level Attention):

Van Boxmeer also said [ORG Zywiec] would be boosted
by its recent shedding of soft drinks which only
accounted for about three percent of the firm's
overall sales and for which 7.6 million zlotys in
provisions had already been made.

Polish brewer [ORG Zywiec]'s 1996 profit slump may
last into next year due in part to hefty
depreciation charges, but recent high investment
should help the firm defend its 10­percent market
share, the firm's chief executive said. 

Document­level Supporting Evidence:

The [ORG Zywiec] logo includes all of the most
important historical symbols of the brewery and
Poland itself. 

[LOC Zywiec] is a town in south­central
Poland 32,242 inhabitants (as of November 2007).

Corpus­level Supporting Evidence:

Figure 1: Example from the baseline and our model
with some supporting evidence.

Additional context may be found from other
sentences in the same document as the query sen-
tence (document-level). In Figure 1, the sentences
in the document-level supporting evidence provide
clearer clues to tag “Zywiec” as ORG, such as
the references to “Zywiec” as a “firm”. A con-
cern of leveraging this information is the amount
of noise that is introduced. However, across all the

https://github.com/boliangz/global_attention_ner
https://github.com/boliangz/global_attention_ner


87

data in our experiments (Section 3.1), we find that
an average of 35.43% of named entity mentions in
each document are repeats and, when a mention
appears more than once in a document, an aver-
age of 98.78% of these mentions have the same
type. Consequently, one may use the document-
level context to overcome the ambiguities of the
local context while introducing little noise.

Although a significant amount of named en-
tity mentions are repeated, 64.57% of the men-
tions are unique. In such cases, the sentences at
the document-level cannot serve as a source of
additional context. Nevertheless, one may find
additional context from sentences in other doc-
uments in the corpus (corpus-level). Figure 1
shows some of the corpus-level supporting evi-
dence for “Zywiec”. In this example, similar
to the document-level supporting evidence, the
first sentence in this corpus-level evidence dis-
cusses the branding of “Zywiec”, corroborating
the ORG tag. Whereas the second sentence intro-
duces noise because it has a different topic than
the current sentence and discusses the Polish town
named “Zywiec”, one may filter these noisy con-
texts, especially when the noisy contexts are ac-
companied by clear contexts like the first sentence.

We propose to utilize local, document-level, and
corpus-level contextual information to improve
name tagging. Generally, we follow the one sense
per discourse hypothesis introduced by Yarowsky
(2003). Some previous name tagging efforts ap-
ply this hypothesis to conduct majority voting for
multiple mentions with the same name string in
a discourse through a cache model (Florian et al.,
2004) or post-processing (Hermjakob et al., 2017).
However, these rule-based methods require man-
ual tuning of thresholds. Moreover, it’s challeng-
ing to explicitly define the scope of discourse.
We propose a new neural network framework
with global attention to tackle these challenges.
Specifically, for each token in a query sentence,
we propose to retrieve sentences that contain the
same token from the document-level and corpus-
level contexts (e.g., document-level and corpus-
level supporting evidence for “Zywiec” in Fig-
ure 1). To utilize this additional information, we
propose a model that, first, produces representa-
tions for each token that encode the local context
from the query sentence as well as the document-
level and corpus-level contexts from the retrieved
sentences. Our model uses a document-level at-

tention and corpus-level attention to dynamically
weight the document-level and corpus-level con-
textual representations, emphasizing the contex-
tual information from each level that is most rel-
evant to the local context and filtering noise such
as the irrelevant information from the mention
“[LOC Zywiec]” in Figure 1. The model learns
to balance the influence of the local, document-
level, and corpus-level contextual representations
via gating mechanisms. Our model predicts a tag
using the local, gated-attentive document-level,
and gated-attentive corpus-level contextual repre-
sentations, which allows our model to predict the
correct tag, ORG, for “Zywiec” in Figure 1.

The major contributions of this paper are: First,
we propose to use multiple levels of contextual
information (local, document-level, and corpus-
level) to improve name tagging. Second, we
present two new attentions, document-level and
corpus-level, which prove to be effective at ex-
ploiting extra contextual information and achieve
the state-of-the-art.

2 Model

We first introduce our baseline model. Then, we
enhance this baseline model by adding document-
level and corpus-level contextual information to
the prediction process via our document-level and
corpus-level attention mechanisms, respectively.

2.1 Baseline

We consider name tagging as a sequence label-
ing problem, where each token in a sequence is
tagged as the beginning (B), inside (I) or out-
side (O) of a name mention. The tagged names
are then classified into predefined entity types.
In this paper, we only use the person (PER), or-
ganization (ORG), location (LOC), and miscel-
laneous (MISC) types, which are the predefined
types in CoNLL-02 and CoNLL-03 name tagging
dataset (Tjong Kim Sang and De Meulder, 2003).

Our baseline model has two parts: 1) En-
coding the sequence of tokens by incorporat-
ing the preceding and following contexts us-
ing a bi-directional long short-term memory (Bi-
LSTM) (Graves et al., 2013), so each token is as-
signed a local contextual embedding. Here, fol-
lowing Ma and Hovy (2016a), we use the con-
catenation of pre-trained word embeddings and
character-level word representations composed by
a convolutional neural network (CNN) as input



88

to the Bi-LSTM. 2) Using a Conditional Random
Fields (CRFs) output layer to render predictions
for each token, which can efficiently capture de-
pendencies among name tags (e.g., “I-LOC” can-
not follow “B-ORG”).

The Bi-LSTM CRF network is a strong baseline
due to its remarkable capability of modeling con-
textual information and label dependencies. Many
recent efforts combine the Bi-LSTM CRF net-
work with language modeling (Liu et al., 2017;
Peters et al., 2017, 2018) to boost the name tag-
ging performance. However, they still suffer from
the limited contexts within individual sequences.
To overcome this limitation, we introduce two at-
tention mechanisms to incorporate document-level
and corpus-level supporting evidence.

2.2 Document-level Attention

Many entity mentions are tagged as multiple types
by the baseline approach within the same docu-
ment due to ambiguous contexts (14.43% of the
errors in English, 18.55% in Dutch, and 17.81% in
German). This type of error is challenging to ad-
dress as most of the current neural network based
approaches focus on evidence within the sentence
when making decisions. In cases where a sentence
is short or highly ambiguous, the model may ei-
ther fail to identify names due to insufficient in-
formation or make wrong decisions by using noisy
context. In contrast, a human in this situation
may seek additional evidence from other sentences
within the same document to improve judgments.

In Figure 1, the baseline model mistak-
enly tags “Zywiec” as PER due to the am-
biguous context “whose full name is...”,
which frequently appears around a person’s
name. However, contexts from other sentences
in the same document containing “Zywiec”
(e.g., sq and sr in Figure 2), such as “’s
1996 profit...” and “would be boosted
by its recent shedding...”, indicate that
“Zywiec” ought to be tagged as ORG. Thus,
we incorporate the document-level supporting
evidence with the following attention mecha-
nism (Bahdanau et al., 2015).

Formally, given a document D = {s1, s2, ...},
where si = {wi1, wi2, ...} is a sequence of
words, we apply a Bi-LSTM to each word in si,
generating local contextual representations hi =
{hi1,hi2, ...}. Next, for each wij , we retrieve the
sentences in the document that contain wij (e.g.,

sq and sr in Figure 2) and select the local contex-
tual representations of wij from these sentences as
supporting evidence, h̃ij = {h̃

1
ij , h̃

2
ij , ...} (e.g., h̃qj

and h̃rk in Figure 2), where hij and h̃ij are ob-
tained with the same Bi-LSTM. Since each repre-
sentation in the supporting evidence is not equally
valuable to the final prediction, we apply an atten-
tion mechanism to weight the contextual represen-
tations of the supporting evidence:

ekij = v> tanh
(
Whhij +Wh̃h̃

k
ij + be

)
,

αkij = Softmax
(
ekij

)
,

where hij is the local contextual representation of
word j in sentence si and h̃

k
ij is the k-th support-

ing contextual representation. Wh, Wh̃ and be are
learned parameters. We compute the weighted av-
erage of the supporting representations by

H̃ij =
∑
k=1

αkij h̃
k
ij ,

where H̃ij denotes the contextual representation of
the supporting evidence for wij .

For each word wij , its supporting evidence rep-
resentation, H̃ij , provides a summary of the other
contexts where the word appears. Though this ev-
idence is valuable to the prediction process, we
must mitigate the influence of the supporting ev-
idence since the prediction should still be made
primarily based on the query context. Therefore,
we apply a gating mechanism to constrain this in-
fluence and enable the model to decide the amount
of the supporting evidence that should be incorpo-
rated in the prediction process, which is given by

rij = σ(WH̃,rH̃ij +Wh,rhij + br) ,

zij = σ(WH̃,zH̃ij +Wh,zhij + bz) ,

gij = tanh(Wh,ghij + zij � (WH̃,gH̃ij + bg)) ,

Dij = rij � hij + (1− rij)� gij ,

where all W , b are learned parameters and Dij is
the gated supporting evidence representation for
wij .

2.3 Topic-aware Corpus-level Attention
The document-level attention fails to generate sup-
porting evidence when the name appears only
once in a single document. In such situations,
we analogously select supporting sentences from
the entire corpus. Unfortunately, different from



89

So far this year Zywiec, whose full name is
Zaklady Piwowarskie w Zywcu SA , has netted six
million zlotys on sales of 224 million zlotys .

So far this year Zywiec , whose full name is Zaklady Piwowarskie w Zywcu SA , has
netted six million zlotys on sales of 224 million zlotys .

Polish brewer Zywiec 's 1996 profit slump may last into next year due in part to
hefty depreciation charges , but recent high investment should help the firm defend
its 00­percent market share , the firm 's chief executive said .

Van Boxmeer also said Zywiec would be boosted by its recent shedding of soft drinks
which only accounted for about three percent of the firm 's overall sales and for
which 0.0 million zlotys in provisions had already been made .

The two largest brands are Heineken and Amstel. 

The list includes Cruzcampo, Affligem and Zywiec .

Bidirectional LSTM Encoder

Bidirectional LSTM Encoder

Bidirectional LSTM Encoder

Bidirectional LSTM Encoder

A +

G

A

+

G

A Attentive Summation G Gated Summation

contextual representaions of
Zywiec from topically related
documents

Bi­LSTM CRF Layer

Bi­LSTM CRF Layer

concatenate

A Attentive Summation

G Gated Summation

Bidirectional LSTM Encoder

A

+

G

A Attentive Summation G Gated Summation

contextual representaions of Zywiec from
topically related documents

Bi­LSTM CRF Layer

The Zywiec logo includes all of the most
important historical symbols of the brewery and
Poland itself. 

Zywiec is a town in south­central Poland 32,242
inhabitants (as of November 2007). 

Bidirectional LSTM Encoder

Bidirectional LSTM Encoder

Corpus­level supporting sentences

Document­level supporting sentences

Figure 2: Document-level Attention Architecture. (Within-sequence context in red incorrectly indicates
the name as PER, and document-level context in green correctly indicates the name as ORG.)

the sentences that are naturally topically relevant
within the same documents, the supporting sen-
tences from the other documents may be about
distinct topics or scenarios, and identical phrases
may refer to various entities with different types,
as in the example in Figure 1. To narrow down
the search scope from the entire corpus and avoid
unnecessary noise, we introduce a topic-aware
corpus-level attention which clusters the docu-
ments by topic and carefully selects topically re-
lated sentences to use as supporting evidence.

We first apply Latent Dirichlet allocation
(LDA) (Blei et al., 2003) to model the topic dis-
tribution of each document and separate the doc-
uments into N clusters based on their topic dis-
tributions.2 As in Figure 3, we retrieve supporting
sentences for each word, such as “Zywiec”, from
the topically related documents and employ an-
other attention mechanism (Bahdanau et al., 2015)
to the supporting contextual representations, ĥij =
{ĥ1ij , ĥ

2
ij , ...} (e.g., h̃xi and h̃yi in Figure 3). This

yields a weighted contextual representation of the
corpus-level supporting evidence, Ĥij , for each
wij , which is similar to the document-level sup-
porting evidence representation, H̃ij , described in

2N = 20 in our experiments.

section 2.2. We use another gating mechanism to
combine Ĥij and the local contextual representa-
tion, hij , to obtain the corpus-level gated support-
ing evidence representation, Cij , for each wij .

So far this year Zywiec, whose full name is
Zaklady Piwowarskie w Zywcu SA , has netted six
million zlotys on sales of 224 million zlotys .

So far this year Zywiec , whose full name is Zaklady Piwowarskie w Zywcu SA , has
netted six million zlotys on sales of 224 million zlotys .

Polish brewer Zywiec 's 1996 profit slump may last into next year due in part to
hefty depreciation charges , but recent high investment should help the firm defend
its 00­percent market share , the firm 's chief executive said .

Van Boxmeer also said Zywiec would be boosted by its recent shedding of soft drinks
which only accounted for about three percent of the firm 's overall sales and for
which 0.0 million zlotys in provisions had already been made .

The two largest brands are Heineken and Amstel. 

The list includes Cruzcampo, Affligem and Zywiec .

Bidirectional LSTM Encoder

Bidirectional LSTM Encoder

Bidirectional LSTM Encoder

Bidirectional LSTM Encoder

A +

G

A

+

G

A Attentive Summation G Gated Summation

contextual representaions of
Zywiec from topically related
documents

Bi­LSTM CRF Layer

Bi­LSTM CRF Layer

concatenate

A Attentive Summation

G Gated Summation

Bidirectional LSTM Encoder

A

+

G

A Attentive Summation G Gated Summation

contextual representaions of Zywiec from
topically related documents

Bi­LSTM CRF Layer

The Zywiec logo includes all of the most
important historical symbols of the brewery and
Poland itself. 

Zywiec is a town in south­central Poland 32,242
inhabitants (as of November 2007). 

Bidirectional LSTM Encoder

Bidirectional LSTM Encoder

Corpus­level supporting sentences

Document­level supporting sentences

Figure 3: Corpus-level Attention Architecture.



90

2.4 Tag Prediction

For each word wij of sentence si, we concatenate
its local contextual representation hij , document-
level gated supporting evidence representation
Dij , and corpus-level gated supporting evidence
representation Cij to obtain its final representa-
tion. This representation is fed to another Bi-
LSTM to further encode the supporting evidence
and local contextual features into an unified repre-
sentation, which is given as input to an affine-CRF
layer for label prediction.

3 Experiments

3.1 Dataset

We evaluate our methods on the CoNLL-2002
and CoNLL-2003 name tagging datasets (Tjong
Kim Sang and De Meulder, 2003). The CoNLL-
2002 dataset contains name tagging annotations
for Dutch (NLD) and Spanish (ESP), while the
CoNLL-2003 dataset contains annotations for En-
glish (ENG) and German (DEU). Both datasets
have four pre-defined name types: person (PER),
organization (ORG), location (LOC) and miscel-
laneous (MISC).3

Code Train Dev. Test
NLD 202,931 (13,344) 37,761 (2,616) 68,994 (3,941)
ESP 264,715 (18,797) 52,923 (4,351) 51,533 (3,558)
ENG 204,567 (23,499) 51,578 (5,942) 46,666 (5,648)
DEU 207,484 (11,651) 51,645 (4,669) 52,098 (3,602)

Table 1: # of tokens in name tagging datasets statis-
tics. # of names is given in parentheses.

We select at most four document-level sup-
porting sentences and five corpus-level support-
ing sentences.4 Since the document-level attention
method requires input from each individual docu-
ment, we do not evaluate it on the CoNLL-2002
Spanish dataset which lacks document delimiters.
We still evaluate the corpus-level attention on the
Spanish dataset by randomly splitting the dataset
into documents (30 sentences per document). Al-
though randomly splitting the sentences does not
yield perfect topic modeling clusters, experiments
show the corpus-level attention still outperforms
the baseline (Section 3.3).

3The miscellaneous category consists of names that do not
belong to the other three categories.

4Both numbers are tuned from 1 to 10 and selected when
the model performs best on the development set.

Hyper-parameter Value
CharCNN Filter Number 25
CharCNN Filter Widths [2, 3, 4]
Lower Bi-LSTM Hidden Size 100
Lower Bi-LSTM Dropout Rate 0.5
Upper Bi-LSTM Hidden Size 100
Learning Rate 0.005
Batch Size N/A∗

Optimizer SGD (Bottou, 2010)
∗ Each batch is a document. The batch size varies as the
different document length.

Table 2: Hyper-parameters.

3.2 Experimental Setup

For word representations, we use 100-dimensional
pre-trained word embeddings and 25-dimensional
randomly initialized character embeddings. We
train word embeddings using the word2vec pack-
age.5 English embeddings are trained on the En-
glish Giga-word version 4, which is the same cor-
pus used in (Lample et al., 2016). Dutch, Span-
ish, and German embeddings are trained on corre-
sponding Wikipedia articles (2017-12-20 dumps).
Word embeddings are fine-tuned during training.

Table 2 shows our hyper-parameters. For
each model with an attention, since the Bi-
LSTM encoder must encode the local, document-
level, and/or corpus-level contexts, we pre-train
a Bi-LSTM CRF model for 50 epochs, add our
document-level attention and/or corpus-level at-
tention, and then fine-tune the augmented model.
Additionally, Reimers and Gurevych (2017) report
that neural models produce different results even
with same hyper-parameters due to the variances
in parameter initialization. Therefore, we run each
model ten times and report the mean as well as the
maximum F1 scores.

3.3 Performance Comparison

We compare our methods to three categories of
baseline name tagging methods:

• Vanilla Name Tagging Without any additional
resources and supervision, the current state-of-
the-art name tagging model is the Bi-LSTM-
CRF network reported by Lample et al. (2016)
and Ma and Hovy (2016b), whose difference
lies in using a LSTM or CNN to encode char-
acters. Our methods fall in this category.

• Multi-task Learning Luo et al. (2015); Yang
et al. (2017) apply multi-task learning to boost

5https://github.com/tmikolov/word2vec



91

Table 1

0 41.5 41.5
1 59.33 59.33
2 55.62 55.62
3 58.89 58.89
4 62.8 62.8
5 62.33 62.33
6 70.39 70.39
7 69.81 69.81
8 73.25 73.25
9 75.68 75.68

10 76.12 76.12
11 73.1 73.1
12 75.1 75.1
13 77.16 77.16
14 75.8 75.8
15 80.66 80.66
16 78.11 78.11
17 78.42 78.42
18 78.38 78.38
19 79.49 79.49
20 80.51 80.51
21 77.35 77.35
22 80.81 80.81
23 79.63 79.63
24 81.69 81.69
25 80.9 80.9
26 80.71 80.71
27 80.37 80.37
28 80.2 80.2
29 80.4 80.4
30 80.46 80.46
31 82.55 82.55
32 81.55 81.55
33 82.62 82.62
34 81.28 81.28
35 82.61 82.61
36 82.26 82.26
37 81.53 81.53
38 82.36 82.36
39 82.36 82.36
40 80.71 80.71
41 83.61 83.61
42 83.93 83.93
43 83.5 83.5
44 81.97 81.97
45 83.21 83.21
46 83.76 83.76
47 82.74 82.74
48 82.96 82.96
49 84.31 84.31
50 83.78 83.78
51 81.03 80.03
52 82.57 81.57
53 83.8 82.8
54 83.87 82.87
55 84.42 83.42
56 84.19 83.19
57 84.82 83.82
58 84.36 83.36
59 84.52 83.52
60 84.83 83.83
61 85.06 84.06
62 85.27 84.27
63 84.23 83.23
64 84.1 83.1
65 85.49 84.49
66 85.07 84.07
67 85.13 84.13
68 85.37 84.37
69 85.63 84.63
70 85.18 84.18
71 85.77 84.77
72 86.22 85.22
73 86.08 85.08
74 85.29 84.29
75 85.23 84.23
76 84.83 83.83
77 85.96 84.96
78 85.25 84.25
79 85.19 84.19

F1
(%

)

75

77.6

80.2

82.8

85.4

88

Training Epochs

0 10 20 30 40 50 60 70 80 90

�1

(a) Dutch (F1 scales between 75%-88%)

Table 1

0 67.89 66.89 68.89
1 72.29 71.29 73.29
2 74.78 73.78 75.78
3 75.74 74.74 76.74
4 78.25 77.25 79.25
5 78.05 77.05 79.05
6 79.56 78.56 80.56
7 79.83 78.83 80.83
8 80.67 79.67 81.67
9 80.1 79.1 81.1

10 81.05 80.05 82.05
11 80.76 79.76 81.76
12 81.46 80.46 82.46
13 81.09 80.09 82.09
14 81.28 80.28 82.28
15 82.37 81.37 83.37
16 82.13 81.13 83.13
17 81.63 80.63 82.63
18 82.85 81.85 83.85
19 83.94 82.94 84.94
20 82.99 81.99 83.99
21 82.98 81.98 83.98
22 83.09 82.09 84.09
23 84.09 83.09 85.09
24 83.22 82.22 84.22
25 83.43 82.43 84.43
26 83.0 82.0 84.0
27 83.77 82.77 84.77
28 84.23 83.23 85.23
29 84.71 83.71 85.71
30 84.43 83.43 85.43
31 84.91 83.91 85.91
32 83.43 82.43 84.43
33 84.63 83.63 85.63
34 84.05 83.05 85.05
35 84.16 83.16 85.16
36 84.31 83.31 85.31
37 84.83 83.83 85.83
38 84.6 83.6 85.6
39 84.76 83.76 85.76
40 84.67 83.67 85.67
41 84.8 83.8 85.8
42 84.71 83.71 85.71
43 84.83 83.83 85.83
44 84.51 83.51 85.51
45 85.16 84.16 86.16
46 84.67 83.67 85.67
47 84.69 83.69 85.69
48 85.26 84.26 86.26
49 84.97 83.97 85.97
50 84.86 83.86 85.86
51 84.14 83.14 85.14
52 85.37 84.37 85.67
53 85.59 84.59 85.89
54 85.57 84.57 85.87
55 85.47 84.47 85.77
56 85.56 84.56 85.86
57 84.94 83.94 85.24
58 85.29 84.29 85.59
59 85.53 84.53 85.83
60 85.71 84.71 86.01
61 85.34 84.34 85.64
62 85.49 84.49 85.79
63 85.63 84.63 85.93
64 85.69 84.69 85.99
65 85.47 84.47 85.77
66 85.45 84.45 85.75
67 85.72 84.72 86.02
68 85.12 84.12 85.42
69 85.53 84.53 85.83
70 85.5 84.5 85.8
71 85.48 84.48 85.78
72 85.58 84.58 85.88
73 85.66 84.66 85.96
74 85.76 84.76 86.06
75 85.56 84.56 85.86
76 85.47 84.47 85.77
77 85.03 84.03 85.33
78 85.64 84.64 85.94
79 85.33 84.33 85.63
80 85.31 84.31 85.61
81 85.11 84.11 85.41
82 85.36 84.36 85.66
83 85.37 84.37 85.67

F1
(%

)

82

82.8

83.6

84.4

85.2

86

Training Epochs

0 10 20 30 40 50 60 70 80 90

�1

(b) Spanish (F1 scales between 82%-86%)

Table 1

f1 raw_f1
0 82.81 82.81
1 86.13 86.13
2 87.64 87.64
3 88.57 88.57
4 89.0 89.0
5 89.35 89.35
6 89.2 89.2
7 89.16 89.16
8 89.53 89.53
9 89.82 89.82

10 90.19 90.19
11 90.07 90.07
12 90.3 90.3
13 90.25 90.25
14 90.24 90.24
15 90.28 90.28
16 90.67 90.67
17 90.58 90.58
18 90.5 90.5
19 90.59 90.59
20 90.52 90.52
21 90.68 90.68
22 90.67 90.67
23 90.72 90.72
24 90.64 90.64
25 90.77 90.77
26 90.79 90.79
27 90.67 90.67
28 90.79 90.79
29 90.73 90.73
30 90.77 90.77
31 90.93 90.93
32 90.87 90.87
33 90.78 90.78
34 90.72 90.72
35 90.71 90.71
36 90.78 90.78
37 90.8 90.8
38 90.83 90.83
39 90.87 90.87
40 90.82 90.82
41 90.89 90.89
42 90.76 90.76
43 90.88 90.88
44 90.86 90.86
45 90.88 90.88
46 90.89 90.89
47 90.75 90.75
48 90.87 90.87
49 90.92 90.92
50 90.58 90.48
51 90.98 90.88
52 90.95 90.85
53 90.96 90.86
54 90.96 90.86
55 91.22 91.12
56 91.11 91.01
57 91.15 91.05
58 90.8 90.7
59 91.06 90.96
60 91.03 90.93
61 91.44 91.34
62 91.14 91.04
63 91.08 90.98
64 91.12 91.02
65 91.04 90.94
66 91.08 90.98
67 91.23 91.13
68 91.05 90.95
69 91.1 91.0
70 91.05 90.95
71 91.19 91.09
72 91.26 91.16
73 91.15 91.05
74 91.24 91.14
75 91.23 91.13
76 91.17 91.07
77 91.23 91.13

F1
(%

)

90

90.4

90.8

91.2

91.6

Training Epochs

0 10 20 30 40 50 60 70 80 90

�1

(c) English (F1 scales between 90%-91.6%)

Table 1

0 60.67 60.67
1 66.07 66.07
2 68.82 68.82
3 70.77 70.77
4 71.71 71.71
5 72.91 72.91
6 73.36 73.36
7 74.49 74.49
8 74.95 74.95
9 74.33 74.33

10 75.19 75.19
11 75.37 75.37
12 75.81 75.81
13 76.51 76.51
14 76.3 76.3
15 76.57 76.57
16 76.73 76.73
17 76.86 76.86
18 76.68 76.68
19 77.18 77.18
20 77.19 77.19
21 77.38 77.38
22 76.97 76.97
23 77.25 77.25
24 77.19 77.19
25 77.43 77.43
26 77.56 77.56
27 77.85 77.85
28 77.82 77.82
29 77.46 77.46
30 77.75 77.75
31 77.81 77.81
32 77.72 77.72
33 77.71 77.71
34 77.47 77.47
35 77.84 77.84
36 77.89 77.89
37 77.57 77.57
38 77.84 77.84
39 78.02 78.02
40 77.85 77.85
41 77.83 77.83
42 77.89 77.89
43 77.89 77.89
44 78.04 78.04
45 77.73 77.73
46 77.77 77.77
47 77.78 77.78
48 77.95 77.95
49 77.86 77.86
50 77.93 77.93
51 77.66 77.66
52 77.69 77.79 77.49
53 77.66 77.76 77.46
54 78.1 78.2 77.9
55 78.2 78.3 78.0
56 77.97 78.07 77.77
57 78.36 78.46 78.16
58 77.89 77.99 77.69
59 78.66 78.76 78.46
60 78.61 78.71 78.41
61 78.64 78.74 78.44
62 77.59 77.69 77.39
63 77.96 78.06 77.76
64 78.37 78.47 78.17
65 78.14 78.24 77.94
66 77.98 78.08 77.78
67 78.2 78.3 78.0
68 78.51 78.61 78.31
69 78.55 78.65 78.35
70 78.35 78.45 78.15
71 77.85 77.95 77.65
72 78.25 78.35 78.05
73 78.05 78.15 77.85
74 78.51 78.61 78.31
75 78.22 78.32 78.02
76 78.32 78.42 78.12
77 78.26 78.36 78.06
78 78.49 78.59 78.29
79 78.15 78.25 77.95
80 78.24 78.34 78.04
81 78.26 78.36 78.06
82 78.34 78.44 78.14
83 78.25 78.35 78.05
84 78.01 78.11 77.81
85 77.99 78.09 77.79
86 78 78.1 77.8

F1
(%

)

76

76.6

77.2

77.8

78.4

79

Training Epochs

0 10 20 30 40 50 60 70 80 90

�1

(d) German (F1 scales between 76%-79%)

Figure 4: Average F1 score for each epoch of the ten runs of our model with both document-level and
corpus-level attentions. Epochs 1-50 are the pre-training phase and 51-100 are the fine-tuning phase.

name tagging performance by introducing ad-
ditional annotations from related tasks such as
entity linking and part-of-speech tagging.

• Join-learning with Language Model Peters
et al. (2017); Liu et al. (2017); Peters et al.
(2018) leverage a pre-trained language model
on a large external corpus to enhance the se-
mantic representations of words in the local cor-
pus. Peters et al. (2018) achieve a high score on
the CoNLL-2003 English dataset using a giant
language model pre-trained on a 1 Billion Word
Benchmark (Chelba et al., 2013).

Table 3 presents the performance comparison
among the baselines, the aforementioned state-
of-the-art methods, and our proposed methods.
Adding only the document-level attention offers a
F1 gain of between 0.37% and 1.25% on Dutch,
English, and German. Similarly, the addition of
the corpus-level attention yields a F1 gain be-
tween 0.46% to 1.08% across all four languages.
The model with both attentions outperforms our
baseline method by 1.60%, 0.56%, and 0.79% on
Dutch, English, and German, respectively. Using
a paired t-test between our proposed model and
the baselines on 10 randomly sampled subsets, we
find that the improvements are statistically signifi-
cant (p ≤ 0.015) for all settings and all languages.

By incorporating the document-level and
corpus-level attentions, we achieve state-of-the-art
performance on the Dutch (NLD), Spanish (ESP)
and German (DEU) datasets. For English, our
methods outperform the state-of-the-art methods
in the “Vanilla Name Tagging” category. Since
the document-level and corpus-level attentions in-
troduce redundant and topically related informa-
tion, our models are compatible with the language
model enhanced approaches. It is interesting to
explore the integration of these two methods, but
we leave this to future explorations.

Figure 4 presents, for each language, the learn-
ing curves of the full models (i.e., with both
document-level and corpus-level attentions). The
learning curve is computed by averaging the F1
scores of the ten runs at each epoch. We first pre-
train a baseline Bi-LSTM CRF model from epoch
1 to 50. Then, starting at epoch 51, we incor-
porate the document-level and corpus-level atten-
tions to fine-tune the entire model. As shown in
Figure 4, when adding the attentions at epoch 51,
the F1 score drops significantly as new parameters
are introduced to the model. The model gradually
adapts to the new information, the F1 score rises,
and the full model eventually outperforms the pre-
trained model. The learning curves strongly prove
the effectiveness of our proposed methods.



92

Code Model F1 (%)
(Gillick et al., 2015) reported 82.84
(Lample et al., 2016) reported 81.74

NLD

(Yang et al., 2017) reported 85.19

Our Baseline mean 85.43max 85.80

Doc-lvl Attention mean 86.82max 87.05

Corpus-lvl Attention mean 86.41max 86.88

Both
mean 87.14
max 87.40
∆ +1.60

ESP

(Gillick et al., 2015) reported 82.95
(Lample et al., 2016) reported 85.75
(Yang et al., 2017) reported 85.77

Our Baseline mean 85.33max 85.51

Corpus-lvl Attention mean 85.77max 86.01
∆ +0.50

(Luo et al., 2015) reported 91.20

ENG

(Lample et al., 2016) reported 90.94
(Ma and Hovy, 2016b) reported 91.21
(Liu et al., 2017) reported 91.35
(Peters et al., 2017) reported 91.93
(Peters et al., 2018) reported 92.22

Our Baseline mean 90.97max 91.23

Doc-lvl Attention mean 91.43max 91.60

Corpus-lvl Attention mean 91.41max 91.71

Both
mean 91.64
max 91.81
∆ +0.58

(Gillick et al., 2015) reported 76.22

DEU

(Lample et al., 2016) reported 78.76

Our Baseline mean 78.15max 78.42

Doc-lvl Attention mean 78.90max 79.19

Corpus-lvl Attention mean 78.53max 78.88

Both
mean 78.83
max 79.21
∆ +0.79

Table 3: Performance of our methods versus the
baseline and state-of-the-art models.

We also compare our approach with a sim-
ple rule-based propagation method, where we use
token-level majority voting to make labels con-
sistent on document-level and corpus-level. The
score of document-level propagation on English is
90.21% (F1), and the corpus-level propagation is
89.02% which are both lower than the BiLSTM-
CRF baseline 90.97%.

3.4 Qualitative Analysis

Table 5 compares the name tagging results from
the baseline model and our best models. All ex-

amples are selected from the development set.
In the Dutch example, “Granada” is the name

of a city in Spain, but also the short name of
“Granada Media”. Without ORG related con-
text, “Granada” is mistakenly tagged as LOC by
the baseline model. However, the document-level
and corpus-level supporting evidence retrieved by
our method contains the ORG name “Granada
Media”, which strongly indicates “Granada” to
be an ORG in the query sentence. By adding the
document-level and corpus-level attentions, our
model successfully tags “Granada” as ORG.

In example 2, the OOV word “Kaczmarek” is
tagged as ORG in the baseline output. In the re-
trieved document-level supporting sentences, PER
related contextual information, such as the pro-
noun “he”, indicates “Kaczmarek” to be a PER.
Our model correctly tags “Kaczmarek” as PER
with the document-level attention.

In the German example, “Grünen” (Greens) is
an OOV word in the training set. The character
embedding captures the semantic meaning of the
stem “Grün” (Green) which is a common non-
name word, so the baseline model tags “Grünen”
as O (outside of a name). In contrast, our model
makes the correct prediction by incorporating the
corpus-level attention because in the related sen-
tence from the corpus “Bundesvorstandes
der Grünen” (Federal Executive of the Greens)
indicates “Grünen” to be a company name.

3.5 Remaining Challenges

By investigating the remaining errors, most of the
named entity type inconsistency errors are elimi-
nated, however, a few new errors are introduced
due to the model propagating labels from negative
instances to positive ones. Figure 5 presents a neg-
ative example, where our model, being influenced
by the prediction “[B-ORG Indianapolis]”
in the supporting sentence, incorrectly predicts
“Indianapolis” as ORG in the query sen-
tence. A potential solution is to apply sentence
classification (Kim, 2014; Ji and Smith, 2017)
to the documents, divide the document into fine-
grained clusters of sentences, and select support-
ing sentences within the same cluster.

In morphologically rich languages, words may
have many variants. When retrieving supporting
evidence, our exact query word match criterion
misses potentially useful supporting sentences that
contain variants of the word. Normalization and



93

#1 Dutch
Baseline [B-LOC Granada] overwoog vervolgens een bod op Carlton uit te brengen, maar daar ziet het 

concern nu van af.
Granada then considered issuing a bid for Carlton, but the concern now sees it.

Our model [B-ORG Granada] overwoog vervolgens een bod op Carlton uit te brengen, maar daar ziet het 
concern nu van af.

D-lvl sentences [B-ORG Granada] [I-ORG Media] neemt belangen in United News.
Granada Media takes interests in United News.

C-lvl sentences Het Britse concern [B-ORG Granada] [I-ORG Media] heeft voor 1,75 miljard pond sterling (111 
miljard Belgische frank) aandelen gekocht van United News Media.
The British group Granada Media has bought shares of GBP 1.75 trillion (111 billion Belgian 
francs) from United News Media.

#2 English
Baseline Initially Poland offered up to 75 percent of Ruch but in March [ORG Kaczmarek] cancelled the 

tender and offered a minority stake with an option to increase the equity.
Our model Initially Poland offered up to 75 percent of Ruch but in March [PER Kaczmarek] cancelled the 

tender and offered a minority stake with an option to increase the equity.
D-lvl sentences [PER Kaczmarek] said in May he was unhappy that only one investor ended up bidding for Ruch.  
#3 German
Baseline Diese Diskussion werde ausschlaggebend sein für die Stellungnahme der Grünen in dieser Frage.

This discussion will be decisive for the opinion of the Greens on this question.
Our model Diese Diskussion werde ausschlaggebend sein für die Stellungnahme der [B-ORG Grünen] in dieser 

Frage.
C-lvl sentences Auch das Mitglied des Bundesvorstandes der [B-ORG Grünen], Helmut Lippelt, sprach sich für ein 

Berufsheer au.
Helmut Lippelt, a member of the Federal Executive of the Greens, also called for a 
professional army.

#4 Negative Example
Reference [B-LOC Indianapolis] 1996-12-06
Our model [B-ORG Indianapolis] 1996-12-06
D-lvl sentence The injury-plagued [B-ORG Indianapolis] [I-ORG Colts] lost another quarterback on Thursday but 

last year's AFC finalists rallied together to shoot down the Philadelphia Eagles 37-10 in a 
showdown of playoff contenders.

* D-lvl sentences: document-level supporting sentences.
* C-lvl sentences: corpus-level supporting sentences.

Figure 5: Comparison of name tagging results between the baseline and our methods.

morphological analysis can be applied in this case
to help fetch supporting sentences.

4 Related Work

Name tagging methods based on sequence label-
ing have been extensively studied recently. Huang
et al. (2015) and Lample et al. (2016) proposed
a neural architecture consisting of a bi-directional
long short-term memory network (Bi-LSTM) en-
coder and a conditional random field (CRF) out-
put layer (Bi-LSTM CRF). This architecture has
been widely explored and demonstrated to be ef-
fective for sequence labeling tasks. Efforts incor-
porated character level compositional word em-
beddings, language modeling, and CRF re-ranking
into the Bi-LSTM CRF architecture which im-
proved the performance (Ma and Hovy, 2016a;
Liu et al., 2017; Sato et al., 2017; Peters et al.,
2017, 2018). Similar to these studies, our ap-
proach is also based on a Bi-LSTM CRF archi-
tecture. However, considering the limited contexts
within each individual sequence, we design two
attention mechanisms to further incorporate top-
ically related contextual information on both the
document-level and corpus-level.

There have been efforts in other areas of infor-
mation extraction to exploit features beyond indi-
vidual sequences. Early attempts (Mikheev et al.,
1998; Mikheev, 2000) on MUC-7 name tagging
dataset used document centered approaches. A
number of approaches explored document-level
features (e.g., temporal and co-occurrence pat-
terns) for event extraction (Chambers and Juraf-
sky, 2008; Ji and Grishman, 2008; Liao and Gr-
ishman, 2010; Do et al., 2012; McClosky and
Manning, 2012; Berant et al., 2014; Yang and
Mitchell, 2016). Other approaches leveraged fea-
tures from external resources (e.g., Wiktionary or
FrameNet) for low resource name tagging and
event extraction (Li et al., 2013; Huang et al.,
2016; Liu et al., 2016; Zhang et al., 2016; Cotterell
and Duh, 2017; Zhang et al., 2017; Huang et al.,
2018). Yaghoobzadeh and Schütze (2016) aggre-
gated corpus-level contextual information of each
entity to predict its type and Narasimhan et al.
(2016) incorporated contexts from external infor-
mation sources (e.g., the documents that contain
the desired information) to resolve ambiguities.
Compared with these studies, our work incorpo-
rates both document-level and corpus-level con-



94

textual information with attention mechanisms,
which is a more advanced and efficient way to cap-
ture meaningful additional features. Additionally,
our model is able to learn how to regulate the in-
fluence of the information outside the local context
using gating mechanisms.

5 Conclusions and Future Work

We propose document-level and corpus-level at-
tentions for name tagging. The document-level
attention retrieves additional supporting evidence
from other sentences within the document to en-
hance the local contextual information of the
query word. When the query word is unique in
the document, the corpus-level attention searches
for topically related sentences in the corpus. Both
attentions dynamically weight the retrieved con-
textual information and emphasize the information
most relevant to the query context. We present
gating mechanisms that allow the model to reg-
ulate the influence of the supporting evidence on
the predictions. Experiments demonstrate the ef-
fectiveness of our approach, which achieves state-
of-the-art results on benchmark datasets.

We plan to apply our method to other tasks,
such as event extraction, and explore integrating
language modeling into this architecture to further
boost name tagging performance.

Acknowledgments

This work was supported by the U.S. DARPA
AIDA Program No. FA8750-18-2-0014,
LORELEI Program No. HR0011-15-C-0115, Air
Force No. FA8650-17-C-7715, NSF IIS-1523198
and U.S. ARL NS-CTA No. W911NF-09-2-0053.
The views and conclusions contained in this
document are those of the authors and should not
be interpreted as representing the official policies,
either expressed or implied, of the U.S. Gov-
ernment. The U.S. Government is authorized to
reproduce and distribute reprints for Government
purposes notwithstanding any copyright notation
here on.

References

Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Ben-
gio. 2015. Neural machine translation by jointly
learning to align and translate. In Proceedings of the
2015 International Conference on Learning Repre-
sentations.

Jonathan Berant, Vivek Srikumar, Pei-Chun Chen,
Abby Vander Linden, Brittany Harding, Brad
Huang, Peter Clark, and Christopher D Manning.
2014. Modeling biological processes for reading
comprehension. In Proceedings of the 2014 Con-
ference on Empirical Methods in Natural Language
Processing.

David M Blei, Andrew Y Ng, and Michael I Jordan.
2003. Latent dirichlet allocation. Journal of ma-
chine Learning research.

Léon Bottou. 2010. Large-scale machine learning with
stochastic gradient descent. In Proceedings of the
2010 International Conference on Computational
Statistics.

Nathanael Chambers and Dan Jurafsky. 2008. Jointly
combining implicit constraints improves temporal
ordering. In Proceedings of the 2008 Conference on
Empirical Methods in Natural Language Process-
ing.

Ciprian Chelba, Tomas Mikolov, Mike Schuster, Qi Ge,
Thorsten Brants, Phillipp Koehn, and Tony Robin-
son. 2013. One billion word benchmark for measur-
ing progress in statistical language modeling. arXiv
preprint arXiv:1312.3005.

Nancy Chinchor and Patricia Robinson. 1997. Muc-7
named entity task definition. In Proceedings of the
7th Conference on Message Understanding.

Ryan Cotterell and Kevin Duh. 2017. Low-
resource named entity recognition with cross-
lingual, character-level neural conditional random
fields. In Proceedings of the Eighth International
Joint Conference on Natural Language Processing.

Quang Xuan Do, Wei Lu, and Dan Roth. 2012. Joint
inference for event timeline construction. In Pro-
ceedings of the 2012 Joint Conference on Empirical
Methods in Natural Language Processing and Com-
putational Natural Language Learning.

R. Florian, H. Hassan, A. Ittycheriah, H. Jing,
N. Kambhatla, X. Luo, N. Nicolov, and S. Roukos.
2004. A statistical model for multilingual entity
detection and tracking. In Proceedings of the Hu-
man Language Technology Conference of the North
American Chapter of the Association for Computa-
tional Linguistics (HLT-NAACL 2004).

Dan Gillick, Cliff Brunk, Oriol Vinyals, and Amarnag
Subramanya. 2015. Multilingual language process-
ing from bytes. arXiv preprint arXiv:1512.00103.

Alan Graves, Navdeep Jaitly, and Abdel-rahman Mo-
hamed. 2013. Hybrid speech recognition with deep
bidirectional lstm. In Proceedings of the 2013 IEEE
Workshop on Automatic Speech Recognition and
Understanding.

Ulf Hermjakob, Qiang Li, Daniel Marcu, Jonathan
May, Sebastian J. Mielke, Nima Pourdamghani,



95

Michael Pust, Xing Shi, Kevin Knight, Tomer Lev-
inboim, Kenton Murray, David Chiang, Boliang
Zhang, Xiaoman Pan, Di Lu, Ying Lin, and Heng
Ji. 2017. Incident-driven machine translation and
name tagging for low-resource languages. Machine
Translation, pages 1–31.

Lifu Huang, Taylor Cassidy, Xiaocheng Feng, Heng
Ji, Clare R Voss, Jiawei Han, and Avirup Sil. 2016.
Liberal event extraction and event schema induction.
In Proceedings of the 54th Annual Meeting of the As-
sociation for Computational Linguistics.

Lifu Huang, Kyunghyun Cho, Boliang Zhang, Heng
Ji, and Kevin Knight. 2018. Multi-lingual common
semantic space construction via cluster-consistent
word embedding. arXiv preprint arXiv:1804.07875.

Zhiheng Huang, Wei Xu, and Kai Yu. 2015. Bidirec-
tional lstm-crf models for sequence tagging. arXiv
preprint arXiv:1508.01991.

Heng Ji and Ralph Grishman. 2008. Refining event
extraction through cross-document inference. Pro-
ceedings of the 2008 Annual Meeting of the Associ-
ation for Computational Linguistics.

Yangfeng Ji and Noah Smith. 2017. Neural discourse
structure for text categorization. Proceedings of the
2017 Annual Meeting of the Association for Compu-
tational Linguistics.

Yoon Kim. 2014. Convolutional neural networks for
sentence classification. Proceedings of the 2014
Conference on Empirical Methods in Natural Lan-
guage Processing.

Guillaume Lample, Miguel Ballesteros, Sandeep Sub-
ramanian, Kazuya Kawakami, and Chris Dyer. 2016.
Neural architectures for named entity recognition.
In Proceedings of 2016 Annual Conference of the
North American Chapter of the Association for
Computational Linguistics.

Qi Li, Heng Ji, and Liang Huang. 2013. Joint event
extraction via structured prediction with global fea-
tures. In Proceedings of the 51st Annual Meeting of
the Association for Computational Linguistics.

Shasha Liao and Ralph Grishman. 2010. Using doc-
ument level cross-event inference to improve event
extraction. In Proceedings of the 48th Annual Meet-
ing of the Association for Computational Linguis-
tics.

Liyuan Liu, Jingbo Shang, Frank Xu, Xiang Ren, Huan
Gui, Jian Peng, and Jiawei Han. 2017. Empower
sequence labeling with task-aware neural language
model.

Shulin Liu, Yubo Chen, Shizhu He, Kang Liu, and
Jun Zhao. 2016. Leveraging framenet to improve
automatic event detection. In Proceedings of the
54th Annual Meeting of the Association for Compu-
tational Linguistics.

Gang Luo, Xiaojiang Huang, Chin-Yew Lin, and Za-
iqing Nie. 2015. Joint entity recognition and disam-
biguation. In Proceedings of the 2015 Conference
on Empirical Methods in Natural Language Pro-
cessing.

Xuezhe Ma and Eduard Hovy. 2016a. End-to-end se-
quence labeling via bi-directional lstm-cnns-crf.

Xuezhe Ma and Eduard Hovy. 2016b. End-to-end se-
quence labeling via bi-directional lstm-cnns-crf. In
Proceedings of the 54th Annual Meeting of the As-
sociation for Computational Linguistics.

David McClosky and Christopher D Manning. 2012.
Learning constraints for consistent timeline extrac-
tion. In Proceedings of the 2012 Joint Confer-
ence on Empirical Methods in Natural Language
Processing and Computational Natural Language
Learning.

Andrei Mikheev. 2000. Document centered approach
to text normalization. In Proceedings of the 23rd
annual international ACM SIGIR conference on Re-
search and development in information retrieval,
pages 136–143. ACM.

Andrei Mikheev, Claire Grover, and Marc Moens.
1998. Description of the ltg system used for muc-
7. In Seventh Message Understanding Conference
(MUC-7): Proceedings of a Conference Held in
Fairfax, Virginia, April 29-May 1, 1998.

Karthik Narasimhan, Adam Yala, and Regina Barzilay.
2016. Improving information extraction by acquir-
ing external evidence with reinforcement learning.
arXiv preprint arXiv:1603.07954.

Matthew E Peters, Waleed Ammar, Chandra Bhagavat-
ula, and Russell Power. 2017. Semi-supervised se-
quence tagging with bidirectional language models.

Matthew E Peters, Mark Neumann, Mohit Iyyer, Matt
Gardner, Christopher Clark, Kenton Lee, and Luke
Zettlemoyer. 2018. Deep contextualized word rep-
resentations. arXiv preprint arXiv:1802.05365.

Nils Reimers and Iryna Gurevych. 2017. Optimal hy-
perparameters for deep lstm-networks for sequence
labeling tasks. arXiv preprint arXiv:1707.06799.

Motoki Sato, Hiroyuki Shindo, Ikuya Yamada, and Yuji
Matsumoto. 2017. Segment-level neural conditional
random fields for named entity recognition. In Pro-
ceedings of the Eighth International Joint Confer-
ence on Natural Language Processing.

Erik F Tjong Kim Sang and Fien De Meulder.
2003. Introduction to the conll-2003 shared task:
Language-independent named entity recognition. In
Proceedings of the 2003 Annual Conference of the
North American Chapter of the Association for
Computational Linguistics.



96

Yadollah Yaghoobzadeh and Hinrich Schütze.
2016. Corpus-level fine-grained entity typing
using contextual information. arXiv preprint
arXiv:1606.07901.

Bishan Yang and Tom Mitchell. 2016. Joint extrac-
tion of events and entities within a document con-
text. arXiv preprint arXiv:1609.03632.

Zhilin Yang, Ruslan Salakhutdinov, and William W
Cohen. 2017. Transfer learning for sequence tag-
ging with hierarchical recurrent networks. arXiv
preprint arXiv:1703.06345.

David Yarowsky. 2003. Unsupervised word sense dis-
ambiguation rivaling supervised methods. In Proc.
ACL1995.

Boliang Zhang, Di Lu, Xiaoman Pan, Ying Lin, Hal-
idanmu Abudukelimu, Heng Ji, and Kevin Knight.
2017. Embracing non-traditional linguistic re-
sources for low-resource language name tagging. In
Proceedings of the Eighth International Joint Con-
ference on Natural Language Processing (Volume 1:
Long Papers).

Boliang Zhang, Xiaoman Pan, Tianlu Wang, Ashish
Vaswani, Heng Ji, Kevin Knight, and Daniel Marcu.
2016. Name tagging for low-resource incident lan-
guages based on expectation-driven learning. In
Proceedings of the 2016 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics: Human Language Technologies.


