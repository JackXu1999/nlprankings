876

Coling 2010: Poster Volume, pages 876–884,

Beijing, August 2010

Semantic Classiﬁcation of Automatically Acquired Nouns

using Lexico-Syntactic Clues

Graduate School of Informatics

Graduate School of Informatics

Yugo Murawaki

Kyoto University

Sadao Kurohashi

Kyoto University

murawaki@nlp.kuee.kyoto-u.ac.jp

kuro@i.kyoto-u.ac.jp

Abstract

In this paper, we present a two-stage ap-
proach to acquire Japanese unknown mor-
phemes from text with full POS tags as-
signed to them. We ﬁrst acquire unknown
morphemes only making a morphology-
level distinction, and then apply semantic
classiﬁcation to acquired nouns. One ad-
vantage of this approach is that, at the sec-
ond stage, we can exploit syntactic clues
in addition to morphological ones because
as a result of the ﬁrst stage acquisition, we
can rely on automatic parsing. Japanese
semantic classiﬁcation poses an interest-
ing challenge: proper nouns need to be
distinguished from common nouns.
It
is because Japanese has no orthographic
distinction between common and proper
nouns and no apparent morphosyntactic
distinction between them. We explore
lexico-syntactic clues that are extracted
from automatically parsed text and inves-
tigate their effects.

the joint

1 Introduction
A dictionary plays an important role in Japanese
morphological analysis, or
task of
segmentation and part-of-speech (POS)
tag-
ging (Kurohashi et al., 1994; Asahara and Mat-
sumoto, 2000; Kudo et al., 2004). Like Chi-
nese and Thai, Japanese does not delimit words
by white-space. This makes the ﬁrst step of nat-
ural language processing more ambiguous than
simple POS tagging. Accordingly, morphemes in
a pre-deﬁned dictionary compactly represent our
knowledge about both segmentation and POS.

One obvious problem with the dictionary-based
approach is caused by unknown morphemes,

or morphemes not deﬁned in the dictionary.
Even though, historically, extensive human re-
sources were used to build high-coverage dictio-
naries (Yokoi, 1995), texts other than newspa-
per articles, in particular web pages, contain a
large number of unknown morphemes. These un-
known morphemes often cause segmentation er-
rors. For example, morphological analyzer JU-
MAN 6.01 wrongly segments the phrase “さっぽ
ろ駅” (saQporo eki, “Sapporo Station”), where “
さっぽろ” (saQporo) is an unknown morpheme,
as follows:

“さ” (sa, noun-common, “difference”),
“っ” (Q, UNK), “ぽ” (po, UNK),
“ろ” (ro, noun-common, “sumac”) and
“駅” (eki, noun-common, “station”),

where UNK refers to unknown morphemes auto-
matically identiﬁed by the analyzer. Such an er-
roneous sequence has disastrous effects on appli-
cations of morphological analysis. For example, it
can hardly be identiﬁed as a LOCATION in named
entity recognition.

One solution to the unknown morpheme prob-
lem is unknown morpheme acquisition (Mori and
Nagao, 1996; Murawaki and Kurohashi, 2008). It
is the task of automatically augmenting the dictio-
nary by acquiring unknown morphemes from text.
In the above example, the goal is to acquire the
morpheme “さっぽろ” (saQporo) with the POS
tag “noun-location name.” However, unknown
morpheme acquisition usually adopts a coarser
POS tagset that only represents the morphology
level distinction among noun, verb and adjective.
This means that “さっぽろ” (saQporo) is acquired
as just a noun and that the semantic label “loca-
tion name” remains to be assigned. The reason
only the morphology level distinction is made is

1http://nlp.kuee.kyoto-u.ac.jp/

nl-resource/juman-e.html

877

that the semantic level distinction cannot easily
be captured with morphological clues that are ex-
ploited in unknown morpheme acquisition.

In this paper, we investigate the remaining
problem and introduce the new task of seman-
tic classiﬁcation that is to be applied to automat-
ically acquired nouns.
In this task, we can ex-
ploit syntactic clues in addition to morphologi-
cal ones because, as a result of acquisition, we
can now rely on automatic parsing. For exam-
ple, since text containing “さっぽろ” (saQporo,
noun-unclassiﬁed) is correctly segmented, we can
extract not only the phrase “saQporo station,” but
the tree fragment “ϕ go to saQporo,” and we can
determine its semantic label.

Japanese semantic classiﬁcation poses an inter-
esting challenge: proper nouns need to be distin-
guished from common nouns. Like Chinese and
Thai, Japanese has no orthographic distinction be-
tween common and proper nouns as there is no
such thing as capitalization.
In addition, there
seems no morphosyntactic (i.e. grammatical) dis-
tinction between them.

In this paper, we explore lexico-syntactic clues
that can be extracted from automatically parsed
text. We train a classiﬁcation model on manually
registered nouns and apply it to automatically ac-
quired nouns. We then investigate the effects of
lexico-syntactic clues.

2 Semantic Classiﬁcation Task

2.1 Two-Stage Approach to Unknown

Morpheme Acquisition

Our goal is to identify unknown morphemes in un-
segmented text and assign POS tags to them. In
this section, we omit the details of boundary iden-
tiﬁcation (segmentation) and review the Japanese
POS tagset to see why we propose a two-stage ap-
proach to assign full POS tags.

The Japanese POS tagset derives from tradi-
tional grammar. It is a mixture of several linguis-
tic levels: morphology, syntax and semantics. In
other words, information encoded in a POS tag
is more than how the morpheme behaves in a se-
quence of morphemes. In fact, POS tags given to
pre-deﬁned morphemes are useful for applications
of morphological analysis, such as dependency

parsing (Kudo and Matsumoto, 2002), named en-
tity recognition (Asahara and Matsumoto, 2003;
Sasano and Kurohashi, 2008) and anaphora res-
olution (Iida et al., 2009; Sasano and Kurohashi,
2009). In these applications, POS tags are incor-
porated as features for models.

On the other hand, the mixed nature of the POS
tagset poses a challenge to unknown morpheme
acquisition. Previous approaches (Mori and Na-
gao, 1996; Murawaki and Kurohashi, 2008) di-
rectly or indirectly reply on morphology, or our
knowledge on how a morpheme behaves in a se-
quence of morphemes. This means that semantic
level distinction is difﬁcult to make in these ap-
proaches, and in fact, is left unresolved. To be
speciﬁc, nouns are only distinguished from verbs
and adjectives but they have subcategories in the
original tagset. These are what we try to classify
acquired nouns into in this paper.

2.2 Semantic Labels
The Japanese noun subcategories may require an
explanation since they are different from the En-
glish ones (Marcus et al., 1993) in many re-
spects. Singular and mass nouns are not distin-
guished from plural nouns because Japanese has
no grammatical distinction between them. More
importantly for this paper, proper nouns have sub-
categories such as person name, location name
and organization name in addition to the distinc-
tion from common nouns. These subcategories
provide important information to named entity
recognition among other applications. For proper
nouns, we adopt these subcategories as semantic
labels in our task.

In contrast to proper nouns, common nouns
have only one subcategory “common.” How-
ever, we consider that subcategories of common
nouns similar to those of proper nouns are use-
ful for, for example, anaphora resolution (Sasano
and Kurohashi, 2009). We adopt the “categories”
of morphological analyzer JUMAN, with which
common nouns in its dictionary are annotated.
There are 22 “categories” including PERSON,
ORGANIZATION and CONCEPT. We collapse
these “categories” into coarser semantic labels
that roughly correspond to those for proper nouns.
To sum up, we deﬁne 9 semantic labels as shown

878

Table 1: List of semantic labels.

sources1

labels

PSN-C

OTH-P

ORG-P

LOC-P

PSN-P

P/C

proper

subPOS:place name

subPOS:proper noun

subPOS:person name

subPOS:organization name

automatically acquired nouns
manually registered nouns
佐祐理 (sayuri, a given name)
松井 (matsui, a surname)
キョン (kyoN, a nickname)
ジョージ (jˆoji, “George”)
アキバ (akiba, “Akihabara”)
京都 (kyouto, “Kyoto”)
ドイツ (doitsu, “Germany”) ワイキキ (waikiki, “Waikiki”)
日銀 (nichigin, a bank)
マツダ (matsuda, “Mazda”)
ヤフー (yahˆu, “Yahoo”)
NHK (a broadcaster)
平成 (heisei, an era name)
ジプシー (jipushˆı, “Gypsy”)
スラブ (surabu, “Slav”)
先生 (seNsei, “teacher”)
メル友 (merutomo, “keypal”)
スタッフ (sutaQfu, “staff”) ニート (nˆıto, “NEET”)
職場 (shokuba, “ofﬁce”)
囲炉裏 (irori, “hearth”)
カフェ (kafe, “cafe”)
圃場 (hojou, “farm ﬁeld”)
政府 (seifu, “government”) メーカ (mˆeka, “manufacturer”)
チーム (chˆımu, “team”)
弊所 (heisho, “our ofﬁce”)
犬 (inu, “dog”)
チワワ (chiwawa, “Chihuahua”)
マンタ (maNta, “manta”)
顔 (kao, “face”)
主張 (shuchou, “argument”) 甚平 (jiNbei, a kind of clothing)
枕 (makura, “pillow”)
着メロ (chakumero, “ringtone”)
1 A subPOS refers to a subcategory of noun. For example, PSN-P corresponds to the POS tag “noun-person name”.
2 category:PLACE-INSTITUTION, category:PLACE-INSTITUION PART and others.

category:PERSON
category:PLACE-∗2
category:ORGANIZATION
category:ANIMAL and
category:ANIMAL-PART
other categories

common

LOC-C

ORG-C

ANI-C

OTH-C

in Table 1.

2.3 Related Tasks
A line of research is dedicated to identify un-
known morphemes with varying degrees of identi-
ﬁcation. Asahara and Matsumoto (2004) only fo-
cus on boundary identiﬁcation (segmentation) of
unknown morphemes. Mori and Nagao (1996),
Nagata (1999) and Murawaki and Kurohashi
(2008) assign POS tags at the morphology level.
Uchimoto et al. (2001) assign full POS tags but
unsurprisingly the accuracy is low. Nakagawa
and Matsumoto (2006) also assign full POS tags.
They address the fact that local information used
in previous studies is inherently insufﬁcient and
present a method that uses global information,
in other words, takes into consideration all oc-
currences of each unknown word in a document.
They report an improvement in tagging proper
nouns in Japanese.

A related task is named entity recognition
(NER). It can handle a named entity longer than
a single morpheme and is usually formalized as a
chunking problem. Since Japanese does not de-
limit words by white-space, the unit of chunk-
ing can be a character (Asahara and Matsumoto,
2003; Kazama and Torisawa, 2008) or a mor-
pheme (Sasano and Kurohashi, 2008). In either
case, NER models encode the output of morpho-
logical analysis and therefore are affected by its

errors. In fact, Saito et al. (2007) report that a ma-
jority of unknown named entities (those never ap-
pear in a training corpus) contain unknown mor-
phemes as their constituents and that NER models
perform poorly on them. A straightforward solu-
tion to this problem would be to acquire unknown
morphemes and to assign semantic labels to them.
Another related task is supersense tagging (Cia-
ramita and Johnson, 2003; Curran, 2005; Cia-
ramita and Altun, 2006). A supersense corre-
sponds to one of the 26 broad categories deﬁned
by WordNet (Fellbaum, 1998). Each noun synset
is associated with a supersense. For example,
“chair” has supersenses PERSON, ARTIFACT
and ACT because it belongs to several synsets.

Since supersense tagging is studied in English,
it differs from our task in several respects. In En-
glish, the distinction between common and proper
nouns is clear. In fact, the tagging models can use
POS features even for unknown nouns. In addi-
tion, the syntactic behavior of English nouns is
different from that of Japanese nouns (Gil, 1987).
Deﬁniteness is not marked in Japanese as it lacks
determiners (e.g. “the” and “a”), and Japanese has
no obligatory plural marking. On the other hand,
Japanese obligatorily uses numeral classiﬁers to
indicate the count of nouns, as in
(1)

saN
three
three volumes of books, or three books,

satsu
CL

hoN
book

no
GEN

879

where “satsu” is a numeral classiﬁer for books. A
number together with its numeral classiﬁer forms
a numeral quantiﬁer. Numeral quantiﬁers would
be informative about the semantic categories of
nouns. Note that Japanese shares the above fea-
tures with Chinese and Thai. Our ﬁndings in this
paper may hold for these languages.

3 Proposed Method
3.1 Lexico-Syntactic Clues
In the task of semantic classiﬁcation, we can ex-
ploit syntactic clues in addition to morpholog-
ical ones. As a result of unknown morpheme
acquisition, text containing acquired morphemes,
or former unknown morphemes, is correctly seg-
mented. Now we can treat automatic parsing as
(at least partly) reliable with regard to acquired
morphemes.

For noun X, we use the following sets of fea-

tures for classiﬁcation.

call: noun phrase Y that appears in a pat-
tern like “Y called X” and “Y such as X,” e.g.
“call:kuni” from
kuni
X
country
X
a country called X.

iu
call

to
QT

cf: predicate with a case marker with which it
takes X as an argument, e.g. “cf:tooru:wo” from

wo
ACC

tooru
pass

X
X
ϕ pass through X.

demo: demonstrative that modiﬁes X, e.g.
“demo:kono” from “kono X” (this X) and
“demo:doNna” from “doNna X” (what kind of
X).

ncf1: noun phrase which X modiﬁes with the
genitive case marker “no,” e.g. “ncf1:heya” from

heya
room

no
GEN

X
X
X’s room.

ncf2: noun phrase that modiﬁes X with the
genitive case marker “no,” e.g. “ncf2:subete”
from

no
GEN

X
X

subete
all
all X.

suf: sufﬁx or sufﬁx-like noun that follows X,
e.g. “suf:saN” from “X saN” (Mr./Ms. X) and
“suf:eki” from “X eki” (X station).

Using automatically parsed text to extract syn-
tactic features has an advantage. Since no manual
annotation is necessary, we can utilize a huge raw
corpus. On the other hand, parsing errors are in-
evitable. However, we can circumvent this prob-
lem by using the constraints of Japanese depen-
dency structures: head-ﬁnal and projective. The
simplest example is the second last element of a
sentence, which always depends on the last ele-
ment. With these constraints, we can focus on
syntactically unambiguous dependency pairs and
extract syntactic features accurately. We follow
Kawahara and Kurohashi (2001) to extract a pair
of an argument noun and a predicate (cf), and
Sasano et al. (2004) to extract a pair of nouns con-
nected with the genitive case marker “no” (ncf1
and ncf2).

Noun X can be part of a compound noun. We
leave it for named entity recognition. Except for
suf, we extract features only when X alone forms
a word. Similarly, we extract suf features only
when X and a sufﬁx alone form a noun phrase.

For call, ncf1, and ncf2, we generalize
For “hoN”
the feature

numerals within noun phrases.
(book) in example 1, we extract
“ncf2:<NUM>satsu.”

Instances for Classiﬁcation

3.2
Now that features are extracted for each noun, the
question is how to combine them together to make
an instance for classiﬁcation. One factor we need
to consider is polysemy: a noun can be a person
name in one context and a location name in an-
other. If we combine features extracted from the
whole corpus, they may represent several seman-
tic labels.

Modeling a mixture of semantic labels might
be a solution, but we do not take this approach on
the grounds that each occurrence of a noun corre-
sponds to a single semantic label.

In our strategy, we perform classiﬁcation mul-
tiple times for each noun and aggregate the results
at the end. The features for each classiﬁcation are
extracted from a relatively small subset of a cor-
pus where the noun is supposedly consistent in

880

In the ﬁeld of named
terms of semantic labels.
entity recognition, it is known that label consis-
tency holds strongly at the level of a document
and less strongly across different documents (Kr-
ishnan and Manning, 2006). Thus we start with a
document and gradually cluster related documents
until a sufﬁcient number of features are obtained.
For the speciﬁc procedures we took in the experi-
ments, see Section 4.1.

3.3 Training Data
Following unknown morpheme acquisition (Mu-
rawaki and Kurohashi, 2008), we create training
data using manually registered nouns, for which
we can obtain correct semantic labels. We per-
form the same procedure as above to make in-
stances of registered nouns.

Some registered nouns are tagged with more
than one semantic label, which we call “explicit
polysemy.” We drop them from the training data.
The remaining problem is “implicit polysemy.”
Nouns are sometimes used with an uncovered
sense. In preliminary experiments, we found that
a typical case of implicit polysemy was that a
proper noun derived from a basic noun. To al-
leviate this problem, we use an NE tagger for ﬁl-
tering. We run an NE tagger over a small portion
of the corpus and extract common nouns that are
frequently tagged as named entities. Then we re-
move these nouns from the training data.
We also drop nouns that appear extremely fre-
quently such as “人” (hito, “person”), “事” (koto,
“thing”) and “私” (watashi, “I”2). Since acquired
nouns to be classiﬁed are typically low frequency
morphemes, they would not behave similarly to
these basic nouns.

3.4 Classiﬁer
To assign a semantic label to each instance, we use
a multiclass discriminative classiﬁer. The input it
takes is an instance that is represented by a feature
vector x ∈ Rd. The output is one semantic label
y ∈ Y , where Y is the set of semantic labels.
We use a linear classiﬁer. It has a weight vector
wy ∈ Rd for each y and outputs y that maximizes
2Japanese personal pronouns are treated as common
nouns because they show no special morphosyntactic behav-
ior.

the inner product of wy and x.

y = argmax

⟨wy, x⟩.

y

Several methods have been proposed to esti-
mate weight vector wy from training data. We use
online algorithms because they are easy to imple-
ment and scale to huge instances. We try the Per-
ceptron family of algorithms.

4 Experiments
4.1 Settings
We used JUMAN for morphological analysis and
KNP3 for dependency parsing. The dictionary
of JUMAN was augmented with automatically
acquired morphemes (Murawaki and Kurohashi,
2008). The number of manually registered mor-
phemes was 120 thousands while there were
13,071 acquired morphemes, of which 12,615
morphemes were nouns.

We used a web corpus that was compiled
through the procedures proposed by Kawahara
and Kurohashi (2006). It consisted of 100 million
pages.

We ﬁrst extracted features from the web cor-
pus. To keep the model size manageable, we
used 447,082 features that appeared more than
100 times in the corpus.

We constructed training data from manually
registered nouns and test data from automatically
acquired nouns. For each noun, we combined text
together until the number of features grew to more
than 100. We started with a single web page, then
merge pages that share a domain name and ﬁ-
nally clustered texts across different domains. We
split the web corpus into 40 subcorpora and ap-
plied this procedure in parallel. We used Bayon4
for clustering domain texts. We sequentially read
texts and applied the repeated bisections cluster-
ing every time some 5,000 pages were appended.
The vectors for clustering were nouns, both regis-
tered and acquired, with their tf-idf scores. We ob-
tained 4,843,085 instances for 10,613 registered
nouns and 196,098 instances for 2,556 acquired
nouns.

3http://nlp.kuee.kyoto-u.ac.jp/

nl-resource/knp-e.html

4http://code.google.com/p/bayon/

881

Table 2: Results of semantic classiﬁcation.

acquired nouns

learning algorithms
Averaged Perceptron
Passive-Aggressive
Conﬁdence-Weighted
baseline1
1 assign OTH-C to all instances.

86.40% (432 / 500)
87.00% (435 / 500)
85.20% (426 / 500)
69.60% (348 / 500)

registered nouns

88.59% (123,113 / 138,971)
91.68% (127,407 / 138,971)
89.66% (124,604 / 138,971)
79.14% (109,980 / 138,971)

Table 3: Examples of aggregated instances.

acquired nouns
ヒカル (hikaru, a person name)
チワワ (chiwawa, “Chihuahua”)
かみさん (kamisaN, colloq. “wife”)
ラスベガス (rasubegasu, “Las Vegas”)
アップル (aQpuru, “Apple/apple”)
メルマガ (merumaga, abbr. of “mail magazine”)

instances
84
128
131
136
187
1,622

labels

PSN-P:58.33%, PSN-C:41.67%
ANI-C:54.69%, OTH-C:45.31%
PSN-C:100%
LOC-P:97.06%, LOC-C:2.94%
ORG-P:63.10%, PSN-C:34.76%, OTH-C:2.14%
OTH-C:99.32%, LOC-C:0.55%, PSN-C:0.06%

In order to handle polysemy, we evaluated se-
mantic classiﬁcation on an instance-by-instance
basis. We randomly selected 500 instances from
the test data and manually assigned the correct la-
bels to them. For comparison purposes, we also
classiﬁed registered nouns. We split the training
data: 829 nouns or 138,971 instances for testing
and the rest for training.

We trained the model with three online learn-
ing algorithms, (1) the averaged version (Collins,
2002) of Perceptron (Crammer and Singer, 2003),
(2) the Passive-Aggressive algorithm (Crammer
et al., 2006), and (3) the Conﬁdence-Weighted
algorithm (Crammer et al., 2009). For Passive-
Aggressive algorithm, we used PA-I and set pa-
rameter C to 1. For Conﬁdence-Weighted, we
used the single-constraint updates. All algorithms
iterated ﬁve times through the training data.

4.2 Results
Table 2 shows the results of semantic classiﬁca-
tion. All algorithms signiﬁcantly improved over
the baseline. As suggested by the gap in accu-
racy between acquired and registered nouns in the
baseline method, the label distribution of the train-
ing data differed from that of the test data, but the
decrease in accuracy was smaller than expected.

The Passive-Aggressive algorithm performed
best on both acquired and registered nouns. For
the rest of this paper, we report the results of the
Passive-Aggressive algorithm.

Table 3 shows aggregated instances of some ac-
quired nouns. Although classiﬁcation sometimes
failed, correct labels took the majority. How-

ever, it is noticeable that PSN-P was frequently
misidentiﬁed as PSN-C while PSN-C was cor-
rectly identiﬁed. This phenomenon is clearly seen
in the confusion matrix (Table 4). Half of PSN-P
instances were misidentiﬁed as PSN-C but the
percentage of errors in the opposite direction was
just above 9%. We will investigate this in the next
section.

4.3 Discussion
Our interest is in determining what kinds of fea-
tures are effective in semantic classiﬁcation. We
ﬁrst performed standard ablation experiments. We
trained a series of models on the training data af-
ter removing each feature set. The training and
test data were the same with those in Section 4.1.
Table 5 shows the results of ablation experi-
ments. Signiﬁcant decreases in accuracy are ob-
served in the cf dataset. This is easily explained by
the fact that more than half of features belonged
to cf. The ratio of ncf1 was much the same with
that of ncf2, but the removal of ncf1 resulted in a
worse performance in classifying registered nouns
than that of ncf2. This means that a modiﬁee of a
noun explains more about the noun than its modi-
ﬁer.

The ablation experiments cannot capture inter-
esting properties of features because each feature
set has a great diversity within it. Next, we di-
rectly examine features instead. Since we use a
simple linear classiﬁer, a feature has |Y | corre-
sponding weights, each of which represents how
likely a noun belongs to label y. For example,
features whose weights for PSN-C are the largest

882

Table 4: Confusion matrix of acquired nouns.

Actual
PSN-C
4

LOC-C

ORG-C

ANI-C

d
e
t
c
i
d
e
r
P

PSN-P
LOC-P
ORG-P
OTH-P
PSN-C
LOC-C
ORG-C
ANI-C
OTH-C

PSN-P
16

LOC-P

ORG-P
1

OTH-P

16
2

3

2

1

4

1

1

OTH-C
1
1

2
4
2

338

39

1

10

13

1

28
9

Table 5: Results of ablation experiments.

acquired nouns

registered nouns

feature set
-call
-cf
-demo
-ncf1
-ncf2
-suf
all
1 The proportion of each feature set that appears in the instances of the test

ratio1
0.23% 87.60% (438 / 500)
54.84% 84.80% (424 / 500)
2.40% 88.00% (440 / 500)
19.03% 87.20% (436 / 500)
18.40% 85.60% (428 / 500)
5.10% 87.40% (437 / 500)
87.00% (435 / 500)

91.58% (127,276 / 138,971)
88.96% (123,630 / 138,971)
91.38% (126,996 / 138,971)
89.23% (124,008 / 138,971)
91.54% (127,220 / 138,971)
91.30% (126,889 / 138,971)
91.68% (127,407 / 138,971)

data.

include:

• cf:nakusu:wo (“ϕ lose X to the disease”),
• cf:oshieru:ni (“ϕ1 teach X ϕ2”),
• ncf2:ooku (“many/much X”), and
• ncf2:<NUM>niN (X is modiﬁed
<NUM> plus a numeral classiﬁer
persons).

by
for

As brieﬂy mentioned in Section 2.3, Japanese
numeral quantiﬁers received scholarly attention
in the ﬁelds of linguistic philosophy and lin-
guistics in relation to the count/mass distinc-
tion (Quine, 1969; Gil, 1987).
In our feature
sets, numeral quantiﬁers typically appear as ncf2,
e.g. “ncf2:<NUM>niN.” The weights given to
them demonstrate their effectiveness in semantic
classiﬁcation. They discriminate common nouns
from proper nouns as the weights given to com-
mon nouns are larger with wide margins. It is not
surprising because, say, the phrase “two Johns” is
semantically acceptable but extremely rare in re-
ality. They are also informative about the distinc-
tion among PSN, LOC and others. For example,
the classiﬁer “niN” for persons suggest the noun in
question is a person while “keN” for houses would
modify a location-like noun. However, we found
quite a few “noises” about these features in data.

The modiﬁee of a numeral expression is not al-
ways the noun to be counted, as demonstrated by
the following example:
(2)

niN
CL

no
GEN

saN
three
matters among the three persons.

moNdai
problem

From the above, the feature “ncf2:<NUM>niN”
is extracted although “moNdai” is OTH-C. Theis
“noise” is attributed to the genitive case marker
“no” because it can denote a wide range of rela-
tions between two nouns. We might be able to
avoid this problem if we focus on “ﬂoating” nu-
meral quantiﬁers. A ﬂoating numeral quantiﬁer
has no direct dependency relation to the noun to
be counted, as in
ga
(3)
NOM

seito
student
three students were absent,

keQseki
absence

saN
three

shita
do

niN
CL

where the numeral quantiﬁer modiﬁes the verb
phrase instead of the noun.
Further work is
needed to anchor ﬂoating numeral quantiﬁers
since they bring a different kind of ambiguity
themselves (Bond et al., 1998).

Closely related to numeral quantiﬁers are quan-
tiﬁcational nouns that appear as “ncf2:ooku”
(“many/much”), “ncf2:subete” (“all”) and oth-
ers. They distinguish common nouns from proper

883

nouns but does not make a further classiﬁca-
tion. The same is true of other numeral expres-
sions such as “cf:hueru:ga” (“X increase in num-
ber”) and “cf:nai:ga” (“there is no X” or “X
do not exist”). We found that, other than nu-
meral expressions, some features distinguished
common nouns from proper nouns because they
indicated the noun denoted an attribute. Such fea-
tures include “cf:naru:ni” (“ϕ become X”) and
“cf:kaneru:wo” (“ϕ double as X”).

We expected that demonstratives

(demo)
served similar functions to quantiﬁcational ex-
pressions, but it turned out to be more com-
plex. The distal demonstrative “ano” (“that”) of-
ten modiﬁes proper nouns to give emphasis.
In
fact,
the model gave larger weights to proper
nouns. On the other hand, interrogative demon-
stratives such as “dono” (“which”) and “doNna”
(“what kind of”) are rarely used with proper nouns
although semantically acceptable.

As seen above, there is an abundant variety
of features that distinguish common nouns from
proper nouns. Also, it is not difﬁcult to make a
distinction among PSN, LOC and others although
the far largest cluster OTH-C sometimes absorbs
other instances. The remaining question is how to
distinguish proper nouns from common nouns, or
speciﬁcally PSN-P from PSN-C. We examined
features that gave larger weights to PSN-P than
to PSN-C. They generally had smaller margins
in weights than those which distinguish PSN-C
from PSN-P. Among them,
features such as
“cf:utau:ga” (“X sing”) and “cf:hanasu:ni” (“ϕ
talk to X”) have no problem with being used for
common nouns in terms of both semantics and
pragmatics. They seem to have resulted from
over-training. There were seemingly appropriate
features such as “suf:saNchi” (“X’s house”) and
“suf:seNshu” (honoriﬁc sufﬁx for players), but
they were not ubiquitous in the corpus. PSN-P in-
stances suffered from lack of distinctive features.
One solution to this problem is to combine ad-
ditional knowledge about person names. For ex-
ample, a Japanese family name is followed by a
given name, and most Chinese names consist of
three Chinese characters. However, quite a few
person names in the web corpus do not follow
the usual patterns of person names because they

are handles (or nicknames) and names for ﬁc-
tional characters. Thus it would be desirable to be
able to classify person names without additional
knowledge.

5 Conclusion

In this paper, we presented the new task of seman-
tic classiﬁcation of Japanese nouns and applied it
to nouns automatically acquired from text. Unlike
in unknown morpheme identiﬁcation in previous
studies, we can exploit automatically parsed text.
We explored lexico-syntactic clues and investi-
gated their effects. We found plenty of features
that distinguished common nouns from proper
nouns, but few features worked in the opposite di-
rection. Further work is needed to overcome this
bias.

References
Asahara, Masayuki and Yuji Matsumoto. 2000. Ex-
tended models and tools for high-performance part-
of-speech tagger. In Proc. of COLING 2000, pages
21–27.

Asahara, Masayuki and Yuji Matsumoto.

2003.
Japanese named entity extraction with redundant
morphological analysis.
In Proc. of HLT/NAACL
2003, pages 8–15.

Asahara, Masayuki and Yuji Matsumoto.

2004.
Japanese unknown word identiﬁcation by character-
based chunking.
In Proc. COLING 2004, pages
459–465.

Bond, Francis, Daniela Kurz, and Satoshi Shirai.
1998. Anchoring ﬂoating quantiﬁers in Japanese-
to-English machine translation.
In Proc. of COL-
ING 1998, pages 152–159.

Ciaramita, Massimiliano and Yasemin Altun. 2006.
Broad-coverage sense disambiguation and informa-
tion extraction with a supersense sequence tagger.
In Proc. of EMNLP 2006, pages 594–602.

Ciaramita, Massimiliano and Mark Johnson. 2003.
Supersense tagging of unknown nouns in WordNet.
In Proc. of EMNLP 2003, pages 168–175.

Collins, Michael. 2002. Discriminative training meth-
ods for hidden markov models: Theory and ex-
periments with perceptron algorithms. In Proc. of
EMNLP 2002, pages 1–8.

884

Crammer, Koby and Yoram Singer. 2003. Ultracon-
servative online algorithms for multiclass problems.
Journal of Machine Learning Research, 3:951–991.

Crammer, Koby, Ofer Dekel, Joseph Keshet, Shai
Shalev-Shwartz, and Yoram Singer. 2006. Online
passive-aggressive algorithms. Journal of Machine
Learning Research, 7:551–585.

Crammer, Koby, Mark Dredze, and Alex Kulesza.
2009. Multi-class conﬁdence weighted algorithms.
In Proc. of EMNLP 2009, pages 496–504.

Curran, James R. 2005. Supersense tagging of un-
known nouns using semantic similarity. In Proc. of
ACL 2005, pages 26–33.

Fellbaum, Christiane, editor. 1998. WordNet: An
Electronic Lexical Database. The MIT Press, Cam-
bridge, MA.

Gil, David. 1987. Deﬁniteness, NP conﬁgurationality
and the count-mass distinction. In Reuland, Eric J.
and Alice G. B. ter Meulen, editors, The Representa-
tion of (In)deﬁniteness, pages 254–269. MIT Press.

Iida, Ryu, Kentaro Inui, and Yuji Matsumoto. 2009.
Capturing salience with a trainable cache model for
zero-anaphora resolution. In Proc. of ACL/IJCNLP
2009, pages 647–655.

Kawahara, Daisuke and Sadao Kurohashi.

2001.
Japanese case frame construction by coupling the
verb and its closest case component.
In Proc. of
HLT 2001, pages 204–210.

Kawahara, Daisuke and Sadao Kurohashi.

2006.
Case frame compilation from the web using high-
performance computing.
In Proc. of LREC-06,
pages 1344–1347.

Kazama, Jun’ichi and Kentaro Torisawa. 2008. Induc-
ing gazetteers for named entity recognition by large-
scale clustering of dependency relations. In Proc. of
ACL 2008, pages 407–415, June.

Krishnan, Vijay and Christopher D. Manning. 2006.
An effective two-stage model for exploiting non-
local dependencies in named entity recognition. In
Proc. of COLING-ACL 2006, pages 1121–1128.

Kudo, Taku and Yuji Matsumoto. 2002.

dependency analysis using cascaded chunking.
Proc. of CONLL 2002, pages 1–7.

Japanese
In

Kudo, Taku, Kaoru Yamamoto, and Yuji Matsumoto.
Applying conditional random ﬁelds to
In Proc. of

2004.
Japanese morphological analysis.
EMNLP 2004, pages 230–237.

Kurohashi, Sadao, Toshihisa Nakamura, Yuji Mat-
sumoto, and Makoto Nagao. 1994. Improvements
of Japanese morphological analyzer JUMAN.
In
Proc. of The International Workshop on Sharable
Natural Language Resources, pages 22–38.

Marcus, Mitchell P., Mary Ann Marcinkiewicz, and
Beatrice Santorini. 1993. Building a large anno-
tated corpus of English: the Penn treebank. Com-
putational Linguistics, 19(2):313–330.

Mori, Shinsuke and Makoto Nagao. 1996. Word ex-
traction from corpora and its part-of-speech estima-
tion using distributional analysis. In Proc. of COL-
ING 1996, volume 2, pages 1119–1122.

Murawaki, Yugo and Sadao Kurohashi. 2008. Online
acquisition of Japanese unknown morphemes us-
ing morphological constraints. In Proc. of EMNLP
2008, pages 429–437.

Nagata, Masaaki. 1999. A part of speech estimation
method for Japanese unknown words using a statis-
tical model of morphology and context. In Proc. of
ACL 1999, pages 277–284.

Nakagawa, Tetsuji and Yuji Matsumoto. 2006. Guess-
ing parts-of-speech of unknown words using global
information. In Proc. of COLING-ACL 2006, pages
705–712.

Quine, Willard Van. 1969. Ontological Relativity and

Other Essays. Columbia University Press.

Saito, Kuniko, Jun Suzuki, and Kenji Imamura. 2007.
Extraction of named entities from blogs using CRF.
In Proc. of The 13th Annual Meeting of The Associ-
ation for Natural Language Processing, pages 107–
110. (in Japanese).

Sasano, Ryohei and Sadao Kurohashi. 2008. Japanese
named entity recognition using structural natural
language processing.
In Proc. of IJCNLP 2008,
pages 607–612.

Sasano, Ryohei and Sadao Kurohashi. 2009. A prob-
abilistic model for associative anaphora resolution.
In Proc. of EMNLP 2009, pages 1455–1464.

Sasano, Ryohei, Daisuke Kawahara, and Sadao Kuro-
hashi. 2004. Automatic construction of nominal
case frames and its application to indirect anaphora
resolution. In Proc. of COLING 2004, pages 1201–
1207.

Uchimoto, Kiyotaka, Satoshi Sekine, and Hitoshi Isa-
hara. 2001. The unknown word problem: a mor-
phological analysis of Japanese using maximum en-
tropy aided by a dictionary.
In Proc. of EMNLP
2001, pages 91–99.

Yokoi, Toshio. 1995. The EDR electronic dictionary.

Communications of the ACM, 38(11):42–44.

