



















































Authorship Attribution and Author Profiling of Lithuanian Literary Texts


Proceedings of the 5th Workshop on Balto-Slavic Natural Language Processing, pages 96–105,
Hissar, Bulgaria, 10–11 September 2015.

Authorship Attribution and Author Profiling of Lithuanian Literary Texts

Jurgita Kapočiūtė-Dzikienė
Vytautas Magnus University
K. Donelaičio 58, LT-44248,

Kaunas, Lithuania
jurgita.k.dz@gmail.com

Andrius Utka
Vytautas Magnus University
K. Donelaičio 58, LT-44248,

Kaunas, Lithuania
a.utka@hmf.vdu.lt

Ligita Šarkutė
Kaunas Univ. of Technology
K. Donelaičio 73, LT-44029,

Kaunas, Lithuania
ligita.sarkute@ktu.lt

Abstract

In this work we are solving authorship at-
tribution and author profiling tasks (by fo-
cusing on the age and gender dimensions)
for the Lithuanian language. This paper
reports the first results on literary texts,
which we compared to the results, pre-
viously obtained with different functional
styles and language types (i.e., parliamen-
tary transcripts and forum posts).

Using the Naïve Bayes Multinomial and
Support Vector Machine methods we in-
vestigated an impact of various stylistic,
character, lexical, morpho-syntactic fea-
tures, and their combinations; the differ-
ent author set sizes of 3, 5, 10, 20, 50,
and 100 candidate authors; and the dataset
sizes of 100, 300, 500, 1,000, 2,000, and
5,000 instances in each class. The high-
est 89.2% accuracy in the authorship at-
tribution task using a maximum number
of candidate authors was achieved with
the Naïve Bayes Multinomial method and
document-level character tri-grams. The
highest 78.3% accuracy in the author pro-
filing task focusing on the age dimension
was achieved with the Support Vector Ma-
chine method and token lemmas. An ac-
curacy reached 100% in the author profil-
ing task focusing on the gender dimension
with the Naïve Bayes Multinomial method
and rather small datasets, where various
lexical, morpho-syntactic, and character
feature types demonstrated a very similar
performance.

1 Introduction

With the constant influx of anonymous or
pseudonymous electronic text documents (forum

posts, Internet comments, tweets, etc.) the au-
thorship analysis is becoming more and more top-
ical. In this respect it is important to consider
the anonymity factor, as it allows everyone to ex-
press their opinions freely, but on the other hand,
opens a gate for different cyber-crimes. Therefore
the authorship research –which for a long time
in the past was mainly focused on literary ques-
tions of unknown or disputed authorship– drifts to-
wards more practical applications in such domains
as forensics, security, user targeted services, etc.
Available text corpora, linguistic tools, and sophis-
ticated methods even more accelerate the devel-
opment of the authorship research field, which is
no longer limited to authorship attribution (when
identifying who, from a closed-set of candidate
authors, is the actual author of a given anonymous
text document) only. Other research directions in-
volve the author verification task (when deciding
if a given text is written by a certain author or
not); the plagiarism detection task (when search-
ing for similarities between two different texts or
parts within a single text); the author profiling
task (when extracting information about author’s
characteristics, typically covering the basic demo-
graphic dimensions as the age, gender, native lan-
guage or psychometric traits); etc. In this paper
we focus on authorship attribution (AA) and au-
thor profiling (AP) problems covering the age and
gender dimensions.

Some researchers claim that in the scenario
when an author makes no efforts to modify his/her
writing style, authorship identification problems
can be tackled due to an existing “stylometric fin-
gerprint” notion –an individual and uncontrolled
habit to express thoughts in certain unique ways,
which is kept constant in all writings by the same
author. Van Halteren (2005) even named this phe-
nomenon a “human stylome” in analogy to a DNA
“genome”. However, Juola (2007) argues that
strict implications are not absolutely correct, be-

96



cause the “genome” is stable, but the writing style
tends to evolve over time. More stable (e.g., gen-
der) and changing (e.g., age, social status, educa-
tion, etc.) demographic characteristics affect the
writing style, thus making AP a solvable task for
these dimensions.

With the breakthrough of the Internet era lit-
erary texts in the authorship research were grad-
ually replaced with e-mails (Abbasi and Chen,
2008), (de Vel et al., 2001), web forum mes-
sages (Solorio et al., 2011), online chats (Cristani
et al., 2012), (Inches et al., 2013), Internet
blogs (Koppel et al., 2011) or tweets (Sousa-Silva
et al., 2011; Schwartz et al., 2013), which, in
turn, contributed to a development of Computa-
tional Linguistic methods able to cope effectively
with the following problems: short texts, non-
normative language texts, many candidate authors,
etc. The discovered advanced techniques helped
to achieve even higher accuracy in AA and AP
tasks on literary texts (under so-called “ideal con-
ditions”). Although the research on literary texts
have lost popularity due to the decrease in demand
of their practical applications, the results obtained
on literary texts can still be interesting from the
scientific point of view, as it may perform some
kind of a baseline function in the comparative re-
search. In this respect we believe that our present
paper, which focuses on literary texts, will deliver
valuable results. Besides, the obtained AA and
AP results will be compared with the results pre-
viously reported on the Lithuanian language cov-
ering other functional styles and language types.

2 Related Works

Despite archaic rule-based approaches (attribut-
ing texts to authors/characteristics depending on
a set of manually constructed rules) and some
rare attempts to deal with unlabeled data (Nasir
et al., 2014; Qian et al., 2014) automatic AA
and AP tasks are tackled with Supervised Ma-
chine Learning (SML) or Similarity-Based (SB)
techniques (for review see (Stamatatos, 2009)).
In the SML paradigm, texts of known author-
ship/characteristic (training data) are used to con-
struct a classifier which afterwards attributes
anonymous documents. In the SB paradigm, an
anonymous text is attributed to the particular au-
thor/characteristic whose text is the most simi-
lar according to some calculated similarity mea-
sure. The comparative experiments prove superi-

ority of the SB methods over the SML techniques,
e.g., Memory-Based Learning produced better
results compared to Naïve Bayes and Decision
Trees (Zhao and Zobel, 2005); the Delta method
surpassed performance levels achieved by the pop-
ular Support Vector Machine method (Jockers and
Witten, 2010). However, the SB approaches are
considered to be more suitable for the problems
with a big number of classes and limited training
data, e.g., the Memory-Based Learning method
applied on 145 authors outperformed Support Vec-
tor Machines (Luyckx and Daelemans, 2008), ap-
plied on 100,000 candidate authors outperformed
Naïve Bayes, Support Vector Machine and Reg-
ularized Least Squares Classification (Narayanan
et al., 2012). In our research we have at most
100 candidate authors, 6 age and 2 gender groups,
therefore the SML approaches seem to be the most
suitable choice. Besides, many AA and AP tasks
are solved using the popular Support Vector Ma-
chine method, which in the contemporary compu-
tational research is considered as the most accu-
rate, thus the most suitable technique for differ-
ent text classification problems (e.g., superiority
of Support Vector Machine is proved in (Zheng et
al., 2006)). However, a selection of classification
method itself is not as important as a proper selec-
tion of an appropriate feature type.

Starting from Mendenhall (1887) the first sty-
lometric techniques were based on the quanti-
tative features (so-called “style markers”) such
as a sentence or word length, number of sylla-
bles per word, type-token ratio, vocabulary rich-
ness function, lexical repetition, etc. (for review
see (Holmes, 1998)). However, these feature types
are considered to be suitable only for homoge-
neous and long texts (e.g., entire books) and for
the datasets having only a few candidate authors.
The first modern pioneering work of Mosteller and
Wallace (1963) –who obtained promising AA re-
sults on The Federalist papers with the Bayesian
method applied on frequencies of a few dozens
function words– triggered many posterior exper-
iments with various feature types. In the contem-
porary research the most widespread approach is
to represent text documents as vectors of frequen-
cies, which elements cover specific layers of lin-
guistic information (lexical, morpho-syntactic, se-
mantic, character, etc.). The best feature types are
determined only after an experimental investiga-
tion.

97



Since most AA and AP research works deal
with Germanic languages, providing no recom-
mendations that could work with the morpholog-
ically rich, highly inflective, derivationally com-
plex languages (such as Lithuanian), having rela-
tively free word order in sentences, our focus is
on the research done for the Baltic and Slavic lan-
guages, which by their nature and characteristics
are the most similar to Lithuanian.

The AA experiments for the Polish language
were performed with the literary texts of 2 au-
thors using the feed-forward multilayer Percep-
tron method (with one or two hidden layers) and
the sigmoid activation function trained by the
back-propagation algorithm (Stańczyk and Cyran,
2007). The experiments with lexical (function
words), syntactic (punctuation marks), and combi-
nation of both feature types revealed superiority of
syntactic features. Eder (2011) applied the Delta
method on the Polish, English, Latin, German
datasets each containing 20 prose writers and then
compared obtained AA results. A bootstrap-like
procedure –testing a large number of randomly
chosen permutations of original data with the k-
Nearest Neighbor method in each trial and calcu-
lating an average accuracy score– helped to avoid
fuzziness with unconvincing results. The best re-
sults for the Polish language texts were achieved
with a mix of word unigrams and bigrams, with
word unigrams for English, with a combination of
words and character penta-grams for Latin, with
character tri-grams for German.

Kukushkina et al. (2001) applied first-order
Markov chains on the Russian literary texts writ-
ten by 82 authors. All matrices –containing tran-
sition frequency pairs of text elements– composed
during the training process for each candidate au-
thor were later used to compute probabilities of
anonymous texts. The researchers investigated
word-level (an original word form or it’s lemma)
character bigrams, pairs of coarse-grained or fine-
grained part-of-speech tags and obtained the best
results with word-level (in the original form) char-
acter bigrams. Kanishcheva (2014) presented the
implemented software able to solve AA tasks
for the Russian language. The offered linguistic
model is based on statistical characteristics and
can fill the lexical database of the author’s vocab-
ulary. Any attribution decision is taken after cal-
culations of a proximity value between texts.

For the Croatian language the AA task was

solved using the Support Vector Machine method
with the radial basis (Reicher et al., 2010). The
researchers used 4 datasets (newspaper texts of
25 authors, on-line blogs of 22 authors, Croat-
ian literature classics of 20 authors, Internet fo-
rum posts of 19 authors). They tested a big va-
riety of features and their combinations: func-
tion words, idf weighted function words, frequen-
cies of coarse-grained part-of-speech tags, fine-
grained part-of-speech tags with normalized fre-
quencies, part-of-speech tri-grams, part-of-speech
tri-grams with function words, other features (in-
cluding punctuation, frequencies of word lengths,
sentence-length frequency values, etc.). The best
results were achieved with a combination of func-
tion words, punctuation marks, word and sentence
length frequency values.

Zečević (2011) investigated byte-level charac-
ter n-grams on the Serbian newspaper dataset of
3 authors. The researcher explored an influence
of the author profile size (varying from 20 up
to 5,000 most frequent n-grams) and the n-gram
length (up to 7). All n-grams were stored in a
structure called a prefix tree; an author attribution
decision was taken by the 1-Nearest Neighbor al-
gorithm based on the distance metric combining
the dissimilarity measure and the simplified pro-
file intersection. The best results were achieved
with the n-grams of n > 2 and the profile size
larger than 500. In the posterior work (Zečević
and Utvić, 2012) researchers added 3 more can-
didate authors to the dataset and investigated an
impact of syllables using the simplified profile in-
tersection similarity measure. However, syllables
were not robust enough to outperform byte-level
character n-grams.

Other research works (as for the Slovene lan-
guage in (Zwitter Vitez, 2012)) demonstrate po-
tentials to solve AA or AP tasks. They represent
available text corpora, linguistic tools and discuss
possible methods, feature types, an importance of
AA and AP tasks, etc.

For the Lithuanian language the AA research
was done with 100 candidate authors and two
datasets of parliamentary transcripts and forum
posts (Kapočiūtė-Dzikienė et al., 2015). The re-
searchers explored the Naïve Bayes Multinomial
and Support Vector Machine methods with a big
variety of feature types: lexical, morpho-syntactic,
character, and stylistic. The best results on the par-
liamentary transcripts dataset were achieved with

98



the Support Vector Machine method and morpho-
syntactic features; on the forum posts dataset –
with the Support Vector Machine method and
character features. The previous AP research on
the Lithuanian language was done with parliamen-
tary transcripts focusing on the age, gender, and
political attitude dimensions (Kapočiūtė-Dzikienė
et al., 2014). The best results on the age dimen-
sion were achieved with the Support Vector Ma-
chine method and a mix of lemma unigrams, bi-
grams, and tri-grams; on the gender and political
attitude dimensions – with the Support Vector Ma-
chine method and a mix of lemma unigrams and
bi-grams.

Hence, AA and AP research using classifi-
cation methods is done on parliamentary tran-
scripts (representing normative language) and fo-
rum posts (representing non-normative language)
for the Lithuanian language, but there are no re-
ported results on literary texts so far. Since a pur-
pose of this paper is to perform the comparative
analysis with the previous research done on parlia-
mentary transcripts and forum posts, AA and AP
tasks with literary texts will be solved by keep-
ing all experimental conditions (concerning meth-
ods and their parameters, feature types, author set
sizes, dataset sizes, etc.) as similar as possible.

3 Methodology

In a straightforward form, both AA and AP prob-
lems fit a standard paradigm of a text classification
problem (Sebastiani, 2002).

Thus, text documents di belonging to the
dataset D are presented as numerical vectors cap-
turing statistics (absolute counts in our case) of
potentially relevant features. Each di can be at-
tributed to one element from a closed-set of can-
didate authors/characteristics, defined as classes
C = {cj}.

A function ϕ determines a mapping how each
di is attributed to cj in a training dataset DT .

Our goal is to find a method (by combining
classification techniques, feature types, and fea-
ture sets) which could discover as close approxi-
mation of ϕ as possible.

3.1 Datasets

186 literary works (in particular, novels, novel-
las, essays, publicistic novels, drama) taken from
the Contemporary Corpus of the Lithuanian Lan-
guage (Marcinkevičienė, 2000) cover the period of

37 years from 1972 to 2012. These literary works
were split into text snippets containing 2,000 sym-
bols (including white-spaces), thus an average text
document length varies from ∼283 to ∼290 to-
kens. Although the average text length does not fit
the recommendations given by Eder (2010) (2,500
tokens for Latin and 5,000 for English, German,
Polish or Hungarian) or Koppel et al. (2007) (500
tokens), these texts are not as extremely short as
used in, e.g., in Luyckx (2011) or Micros and Per-
ifanos (2011) AA research works, where reason-
able results were achieved with only ∼60 tokens
per text.

After previously described pre-processing, we
composed 3 datasets:

• LIndividual, which was used in our AA task
(see Table 1). The experiments with this
dataset involved balanced/full versions and
the increasing number of candidate authors
(3, 5, 10, 20, 50, and 100).

• LAge used in our AP task by focusing on
the age dimension (see Table 2) contains 6
age groups (≤29, 30-39, 40-49, 50-59, 60-69,
and ≥70).1 The age group of any author was
determined by calculating a difference be-
tween the author’s birth date an the publish-
ing date of his/her literary work. An opposite
to the related research works (e.g., (Schler et
al., 2006) or (Koppel et al., 2009)) we did not
eliminate intermediate age groups, thus we
did not simplify our task. The experiments
performed with the balanced dataset versions
(unless there was not enough text samples in
the “main pool”) of 100, 300, 500, 1,000,
2,000, 5,000 text documents in each class.

• LGender used in our AP task focusing on the
gender dimension (see Table 2) contains 2
gender groups (male and female). The exper-
iments performed with the balanced dataset
versions of 100, 300, 500, 1,000, 2,000,
5,000 text documents in each class.

A distribution of 100 authors by their age and
gender is given in Table 3. The LAge and LGen-
der datasets contain randomly selected texts, pro-
viding no meta information about their authors.

1The chosen grouping is commonly used in the so-
cial studies, e.g., in the largest data archive in Europe
(http://www.gesis.org), as well as in the Lithua-
nian Data Archive for Social Science and Humanities
(http://www.lidata.eu).

99



Numb. of
classes

Numb. of text
documents

Numb. of
tokens

Numb. of distinct
tokens (types)

Numb. of distinct
lemmas

Avg. numb of
tokens in a doc.

3 450 128,622 39,306 20,099 285.832,156 612,030 105,200 42,347 283.87

5 750 214,117 58,282 26,846 285.493,099 877,788 136,798 51,638 283.25

10 1,500 430,849 84,838 35,424 287.235,102 1,456,039 176,146 64,001 285.39

20 3,000 867,657 133,163 52,005 289.228,661 2,492,637 236,505 84,566 287.80

50 7,500 217,6019 229,726 84,952 290.1416,317 4,721,452 343,827 124,117 289.36

100 15,000 4,347,165 332,251 120,676 289.8125,564 7,395,147 436,686 159,175 289.28

Table 1: Statistics about LIndividual: an upper value in each cell represents the balanced dataset of 150
texts in each class, a lower value– imbalanced (full). The set of authors is the same in both dataset
versions.

Dataset Numb. of textdocuments
Numb. of
tokens

Numb. of distinct
tokens (types)

Numb. of distinct
lemmas

Avg. numb of
tokens in a doc.

LAge 27,264 7,912,886 454,165 165,432 290.23
LGender 10,000 2,899,837 271,189 99,242 289.98

Table 2: Statistics about the balanced LAge and LGender datasets containing 5,000 text documents in
each class. The LGender dataset is not completely balanced due to the lack of texts in the age groups of
≤29 and ≥70.

≤29 30-39 40-49 50-59 60-69 ≥70
Male 5 13 13 13 13 12

Female 7 8 6 4 3 3
Total 12 21 19 17 16 15

Table 3: Distribution of authors by their age and
gender.

3.2 Machine Learning Methods
In order to compare obtained AA and AP results
with the previously reported, experimental con-
ditions have to be as similar as possible. Thus,
the choice of classification method was restricted
to Naïve Bayes Multinomial (NBM) (introduced
by Lewis and Gale (1994)) and Support Vector
Machine (SVM) (introduced by Cortes and Vap-
nik (1995)). Both SML techniques are used in the
recent AA and AP tasks due to their advantages.

3.3 Features
The choice of features (by which documents are
represented) is as important as the choice of clas-
sification method. To find out what could work
with the Lithuanian literary texts, we tested a big
variety of different feature types, covering stylis-
tic, character, lexical and morpho-syntactic levels:

• sm – style markers: an average sentence and
word length; a standardized type/token ratio.

• fwd – function words (topic-neutral): prepo-
sitions, pronouns, conjunctions, particles,
interjections, and onomatopoeias, which
were automatically recognized in texts with
the Lithuanian morphological analyzer-
lemmatizer “Lemuoklis” (Zinkevičius,
2000).

• chr – (language-independent) document-
level character n-grams with n ∈ [2, 7].
• lex – tokens and a mix of their n-grams up to
n ∈ [2, 3] (e.g., in n = 3 case not only tri-
grams, but bi-grams and unigrams would be
used as well).

• lem – lemmas and a mix of their n-grams
up to n ∈ [2, 3]. The lemmatization was
done with “Lemuoklis” which replaced rec-
ognized words with their lemmas, trans-
formed generic words into appropriate lower-
case letters and all numbers into a special tag.

• pos – coarse-grained part-of-speech tags
(such as noun, verb, adjective, etc., deter-
mined with “Lemuoklis”) and a mix of their
n-grams up to n ∈ [2, 3].
• lexpos, lempos, lexmorf, lemmorf – the

compound features of lex+pos, lem+pos,

100



lex+morf, lem+morf, respectively, and a mix
of their n-grams up to n ∈ [2, 3]. Here morf
indicates a fine-grained part-of-speech tag
composed of coarse-grained tag with the ad-
ditional morphological information as case,
gender, tense, etc.

4 Experimental Setup and Results

All experiments were carried out with the strati-
fied 10-fold cross-validation and evaluated using
the accuracy and f-score metrics.2

For each dataset version (described in Sec-
tion 3.1) the random

∑
P 2(cj) and major-

ity maxP (cj) baselines were calculated (where
P (cj) is the probability of class cj) and the higher
one of these values is presented in the following
figures. The statistical significance between differ-
ent results was evaluated using McNemar’s (1947)
test with one degree of freedom.

In all experiments we used WEKA 3.7 ma-
chine learning toolkit (Hall et al., 2009); 1,000
the most relevant features (using the types de-
scribed in Section 3.3), ranked by the calculated
χ2 values; the SVM method with the SMO poly-
nomial kernel (Platt, 1998) (because it gave the
highest accuracy in the comparative experiments,
done with parliamentary transcripts and forum
data (Kapočiūtė-Dzikienė et al., 2015)) and the
NBM method (described in Section 3.2). Remain-
ing parameters were set to their default values.

The highest achieved accuracies (in terms of
all explored feature types) with both classification
techniques for AA and AP tasks are presented in
Figure 1 and Figure 2, respectively. For the ac-
curacies obtained with different feature types us-
ing the most accurate classification method and the
datasets presented in Table 1, Table 2 see Table 4.

5 Discussion

All obtained results are reasonable, as they exceed
the random and majority baselines.

If we compare the results in Figure 1 with the
previously reported results on parliamentary tran-
scripts and forum posts (Kapočiūtė-Dzikienė et
al., 2015), SVM is not the best technique in all
cases here. Despite it slightly outperforms NBM
on the smaller datasets (with <20 candidate au-
thors), but under-performs on the larger ones. We

2F-scores show the same trend as accuracy values in all
our experiments, therefore we do not present them in the fol-
lowing figures and tables.

suppose that the simple NBM technique coped
effectively with our AA task due to the follow-
ing reasons: used literary texts are more homoge-
neous (a literary work/author rate is 1.86), longer
(∼1.34 and ∼6.88 times longer compared to par-
liamentary transcripts and forum posts, respec-
tively), have more stable vocabulary, and clearer
synonymy compared to parliamentary transcripts
(covering a period of 23 years) or forum posts
(covering a bunch of different topics). Moreover,
the writing style of each author in literary works
is expressed more clearly, therefore the drop in the
accuracy when adding new authors to the dataset
was not as steep as with parliamentary transcripts
or forum posts. Even with 100 candidate authors
the accuracy on literary texts almost reaches the
threshold of 90% (see Figure 1) exceeding the re-
sults of parliamentary transcripts and forum posts
by ∼18.6% and ∼54.6%, respectively. Besides,
the dataset balancing boosted the accuracy on par-
liamentary transcripts and reduced on forum posts,
but gave no noticeable impact on literary texts.
Since literary texts written by the same author are
very similar in style, new texts added to the dataset
could not make any significant impact.

Zooming into the feature types in Table 4 al-
lows us to state that lexical information dominates
character on the smaller datasets (having ≤50
candidate authors). However, when the number
of candidates is small (≤20) many different fea-
tures (based on character, lexical, lemma or com-
pound lexical and morpho-syntactic information)
perform equally well; with 50 – only unigrams of
lemmas (sometimes complemented with part-of-
speech tags) are significantly better compared to
the rest types; with 100 – only character tri-grams
are the best. The most surprising is the fact that the
character feature type gave the best results on the
largest dataset. Typically when dealing with mor-
phologically rich languages and normative texts,
morphological features are the most accurate (e.g.,
on Greek (Stamatatos et al., 2001) or on He-
brew (Koppel et al., 2006) texts). The Lithua-
nian language is not an exception, i.e., the exper-
iments with parliamentary transcripts showed that
token lemmas (or their n-grams) is the best feature
type, whereas on forum posts (where the morpho-
logical tools could not be maximally helpful due
to the text specifics) character features gave the
highest accuracy. On the other hand, a robustness
of character n-grams is not very surprising: i.e.,

101



Figure 1: The accuracy (y axis) dependence on a number of candidate authors (x axis). Each column
shows the maximum achieved accuracy over all explored feature types. Grey columns represent the
NBM method, white – SVM, black parts represent the higher value of random/majority baselines.

Figure 2: The accuracy (y axis) dependence on a number of instances in each class (x axis). For the other
notations see the caption of Figure 1.

character n-grams can capture lexical preferences
without any need of linguistic background knowl-
edge; moreover, we used document-level character
n-grams which incorporate information about con-
tiguous words. Besides, literary texts are not too
complicated for AA tasks, therefore shorter char-
acter n-grams (tri-grams in our case) are enough to
capture author’s style differences without mapping
too obviously to specific words.

The SVM method outperformed the NBM
method on the larger datasets (having more in-
stances in each class) in all AP tasks (see the
results on LAge and LGender presented in Fig-
ure 2 and on parliamentary transcripts reported by
Kapočiūtė et al. (2014)). The results obtained with
literary texts focusing on the age dimension do not
contradict the results achieved with parliamentary
transcripts: the highest boost in the accuracy is
reached on the largest datasets (containing 5,000
instances in each class). The results obtained with
literary texts focusing on the gender dimension are
absolutely opposite, i.e., the best performance on
literary texts was demonstrated with the smaller
datasets, on the parliamentary transcripts – with

the largest. We suppose that this unexpected situa-
tion (when the smaller datasets seem more optimal
for capturing the gender characteristics) happened
when instances were randomly selected from the
“main pool”, i.e., if the first selected instances
were the most typical for the writing style of males
and females, less characteristic instances added
afterwards could only degrade AP performance.
However, the precise answer to this question is
possible only after a detailed error analysis which
is planned in our future research. Nevertheless the
dataset of 100, 300 or 500 instances in each class
is too small to be recommended for any AP tasks.

Zooming into Table 4 allows us to state that to-
ken lemmas is the best feature type on LAge. Be-
sides, lemma information (in particular, a mix of
lemma tri-grams, bi-grams, and unigrams) gave
the best results on parliamentary transcripts as
well. Marginally the best feature type dealing
with the largest LGender dataset is token lem-
mas complemented with the part-of-speech infor-
mation; with the smaller LGender datasets vari-
ous lexical, morpho-syntactic, and character fea-
ture types demonstrated high and very similar per-

102



Feature
type

LIndividual (balanced) LIndividual LAge LGender3 5 10 20 50 100 3 5 10 20 50 100
sm 0.616 0.380 0.249 0.163 0.089 0.045 0.635 0.441 0.312 0.195 0.114 0.072 0.245 0.560

fwd 0.942 0.825 0.823 0.701 0.575 0.442 0.966 0.874 0.862 0.751 0.634 0.480 0.445 0.710
chr2 0.989 0.965 0.973 0.896 0.874 0.782 0.990 0.978 0.979 0.911 0.895 0.836 0.649 0.811
chr3 0.998 0.992 0.986 0.932 0.934 0.892 0.998 0.991 0.989 0.920 0.921 0.891 0.722 0.859
chr4 0.998 0.991 0.985 0.922 0.896 0.756 0.997 0.988 0.986 0.914 0.896 0.791 0.726 0.858
chr5 0.993 0.983 0.973 0.915 0.833 0.662 0.995 0.987 0.979 0.906 0.854 0.703 0.698 0.849
chr6 0.987 0.980 0.962 0.897 0.787 0.633 0.993 0.984 0.968 0.894 0.824 0.672 0.678 0.843
chr7 0.987 0.977 0.958 0.884 0.761 0.611 0.992 0.985 0.961 0.884 0.795 0.639 0.656 0.834
lex1 1.000 0.993 0.993 0.937 0.928 0.841 0.998 0.994 0.991 0.933 0.909 0.820 0.753 0.884
lex2 1.000 0.991 0.991 0.933 0.916 0.840 0.998 0.994 0.989 0.929 0.895 0.777 0.745 0.885
lex3 1.000 0.992 0.991 0.934 0.916 0.841 0.998 0.994 0.989 0.929 0.895 0.775 0.745 0.884

lem1 0.998 0.989 0.993 0.943 0.952 0.889 0.999 0.990 0.991 0.940 0.940 0.882 0.783 0.889
lem2 0.998 0.991 0.992 0.936 0.941 0.886 0.999 0.991 0.990 0.936 0.925 0.837 0.771 0.884
lem3 0.998 0.988 0.991 0.938 0.941 0.885 0.999 0.989 0.990 0.937 0.925 0.834 0.769 0.884
pos1 0.702 0.632 0.549 0.356 0.218 0.143 0.890 0.641 0.553 0.368 0.209 0.142 0.340 0.616
pos2 0.882 0.768 0.784 0.612 0.516 0.408 0.891 0.780 0.789 0.641 0.529 0.430 0.437 0.649
pos3 0.909 0.797 0.817 0.691 0.615 0.516 0.909 0.801 0.818 0.689 0.618 0.533 0.441 0.648

lexpos1 1.000 0.992 0.993 0.940 0.925 0.838 0.998 0.994 0.990 0.933 0.908 0.814 0.750 0.883
lexpos2 1.000 0.991 0.991 0.934 0.910 0.839 0.998 0.993 0.990 0.927 0.892 0.776 0.741 0.880
lexpos3 1.000 0.991 0.990 0.936 0.911 0.837 0.998 0.993 0.989 0.927 0.892 0.774 0.741 0.878

lempos1 0.998 0.989 0.993 0.943 0.951 0.888 0.999 0.990 0.991 0.940 0.938 0.880 0.741 0.890
lempos2 1.000 0.992 0.991 0.938 0.939 0.887 0.998 0.992 0.989 0.935 0.924 0.840 0.770 0.885
lempos3 1.000 0.989 0.991 0.937 0.939 0.886 0.998 0.992 0.990 0.934 0.923 0.835 0.771 0.882
lexmorf1 1.000 0.991 0.995 0.939 0.926 0.838 0.998 0.994 0.990 0.933 0.907 0.814 0.749 0.882
lexmorf2 1.000 0.992 0.989 0.935 0.912 0.835 0.997 0.993 0.989 0.927 0.890 0.772 0.740 0.880
lexmorf3 1.000 0.991 0.990 0.935 0.911 0.835 0.997 0.992 0.989 0.927 0.890 0.771 0.739 0.879

lemmorf1 1.000 0.992 0.991 0.937 0.932 0.850 0.998 0.994 0.991 0.932 0.913 0.828 0.754 0.886
lemmorf2 1.000 0.988 0.989 0.930 0.916 0.850 0.998 0.993 0.988 0.927 0.894 0.783 0.745 0.875
lemmorf3 1.000 0.988 0.990 0.930 0.916 0.849 0.998 0.994 0.988 0.926 0.895 0.782 0.746 0.876

Table 4: The accuracy values achieved on LIndividual with NBM; on LAge and Gender with SVM and
5,000 instances in each class. In each column the best results are presented in bold, the results that do
not significantly differ from the best one are underlined.

formance. Besides, the best feature type on par-
liamentary transcripts is a mix of lemma bi-grams
and unigrams. However, a robustness of lemmata
is not surprising having in mind that we were deal-
ing with the morphologically complex language
and normative texts.

6 Conclusions

In this paper we report the first authorship attribu-
tion and author profiling results obtained on the
Lithuanian literary texts. The results are com-
pared with previously reported on parliamentary
transcripts and forum posts.

When solving the authorship attribution task we
experimentally investigated the effect of the author
set size by gradually increasing the number of can-
didate authors up to 100. The best results dealing
with the maximum author set were achieved with
the Naïve Bayes Multinomial method and charac-
ter tri-grams. The results exceeded the baselines
by ∼88.2% and reached 89.2% of the accuracy.

When solving the author profiling task we ex-
perimentally investigated the effect of balanced

dataset size by gradually increasing the number of
instances in each class up to 5,000. The best re-
sults for the age dimension were achieved with the
maximum dataset, token lemmas, and the Support
Vector Machine method; for the gender dimension
very good performance was demonstrated already
with the small datasets, using lemma unigrams and
the Support Vector Machine method. The results
focusing on the age and gender dimensions ex-
ceeded baselines by ∼60% and ∼50% reaching
78.3% and 100% of the accuracy, respectively.

The comparative analysis show that it is much
easier to capture age, gender and individual author
differences with literary texts than with parliamen-
tary transcripts or forum posts.

In the future research we are planning to make
the detailed error analysis, which could help us to
improve the accuracy; to expand the number of au-
thors and profiling dimensions.

Acknowledgments

This research was funded by a grant (No. LIT-8-
69) from the Research Council of Lithuania.

103



References
Ahmed Abbasi and Hsinchun Chen. 2008. Writer-

prints: a stylometric approach to identity-level
identification and similarity detection in cyberspace.
ACM Transactions on Information Systems,
26(2):1–29.

Corina Cortes and Vladimir Vapnik. 1995. Support-
vector networks. Machine Learning, 20(3):273–
297.

Marco Cristani, Giorgio Roffo, Cristina Segalin,
Loris Bazzani, Alessandro Vinciarelli, and Vittorio
Murino. 2012. Conversationally-inspired stylomet-
ric features for authorship attribution in instant mes-
saging. In Proceedings of the 20th ACM Interna-
tional Conference on Multimedia, pages 1121–1124.

Olivier de Vel, Alison M. Anderson, Malcolm W. Cor-
ney, and George M. Mohay. 2001. Mining e-mail
content for author identification forensics. SIGMOD
Record, 30(4):55–64.

Maciej Eder. 2010. Does size matter? Authorship
attribution, small samples, big problem. In Digi-
tal Humanities 2010: Conference Abstracts, pages
132–135.

Maciej Eder. 2011. Style-markers in authorship attri-
bution a cross-language study of the authorial finger-
print. Studies in Polish Linguistics, 6(1):99–114.

Mark Hall, Eibe Frank, Holmes Geoffrey, Bernhard
Pfahringer, Peter Reutemann, and Ian H. Witten.
2009. The WEKA data mining software: an update.
SIGKDD Explorations, 11(1):10–18.

David I. Holmes. 1998. The evolution of stylometry
in humanities scholarship. Literary and Linguistic
Computing, 13(3):111–117.

Giacomo Inches, Morgan Harvey, and Fabio Crestani.
2013. Finding participants in a chat: authorship at-
tribution for conversational documents. In Interna-
tional Conference on Social Computing, pages 272–
279.

Matthew L. Jockers and Daniela M. Witten. 2010. A
comparative study of machine learning methods for
authorship attribution. Literary and Linguistic Com-
puting, 25(2):215–223.

Patrick Juola. 2007. Future trends in authorship attri-
bution. In Advances in Digital Forensics III - IFIP
International Conference on Digital Forensics, vol-
ume 242, pages 119–132.

Olga Kanishcheva. 2014. Using of the statistical
method for authorship attribution of the text. In Pro-
ceedings of the 1st International Electronic Confer-
ence on Entropy and Its Applications, volume 1.

Jurgita Kapočiūtė-Dzikienė, Ligita Šarkutė, and An-
drius Utka. 2014. Automatic author profiling of
Lithuanian parliamentary speeches: exploring the

influence of features and dataset sizes. In Human
Language Technologies – The Baltic Perspective,
pages 99–106.

Jurgita Kapočiūtė-Dzikienė, Ligita Šarkutė, and An-
drius Utka. 2015. The effect of author set size in
authorship attribution for Lithuanian. In NODAL-
IDA 2015: 20th Nordic Conference of Computa-
tional Linguistics, pages 87–96.

Moshe Koppel, Dror Mughaz, and Navot Akiva. 2006.
New methods for attribution of rabbinic literature. A
Journal for Hebrew Descriptive, Computational and
Applied Linguistics, 57:5–18.

Moshe Koppel, Jonathan Schler, and Elisheva
Bonchek-Dokow. 2007. Measuring differentiabil-
ity: unmasking pseudonymous authors. Journal of
Machine Learning Research, 8:1261–1276.

Moshe Koppel, Jonathan Schler, and Shlomo Arga-
mon. 2009. Computational methods in authorship
attribution. Journal of the Association for Informa-
tion Science and Technology, 60(1):9–26.

Moshe Koppel, Jonathan Schler, and Shlomo Arga-
mon. 2011. Authorship attribution in the wild. Lan-
guage Resources and Evaluation, 45(1):83–94.

Olga Vladimirovna Kukushkina, Anatoly Anatol’evich
Polikarpov, and Dmitrij Viktorovich Khmelev.
2001. Using literal and grammatical statistics for au-
thorship attribution. Problems of Information Trans-
mission, 37(2):172–184.

David D. Lewis and William A. Gale. 1994. A se-
quential algorithm for training text classifiers. In
17th Annual International ACM-SIGIR Conference
on Research and Development in Information Re-
trieval, pages 3–12.

Kim Luyckx and Walter Daelemans. 2008. Author-
ship attribution and verification with many authors
and limited data. In Proceedings of the 22Nd Inter-
national Conference on Computational Linguistics,
volume 1, pages 513–520.

Kim Luyckx. 2011. Authorship attribution of e-mail
as a multi-class task. In Notebook for PAN at CLEF
2011. Cross-Language Evaluation Forum (Notebook
Papers/Labs/Workshop).

Rūta Marcinkevičienė. 2000. Tekstynų lingvistika
(teorija ir paktika) [Corpus linguistics (theory and
practice)]. Darbai ir dienos, 24:7–63. In Lithua-
nian.

Quinn Michael McNemar. 1947. Note on the sam-
pling error of the difference between correlated pro-
portions or percentages. Psychometrika, 12(2):153–
157.

Thomas Corwin Mendenhall. 1887. The characteristic
curves of composition. Science, 9(214):237–246.

104



George K. Mikros and Kostas Perifanos. 2011. Au-
thorship identification in large email collections: ex-
periments using features that belong to different lin-
guistic levels. In Notebook for PAN at CLEF 2011.
Cross-Language Evaluation Forum (Notebook Pa-
pers/Labs/Workshop).

Frederik Mosteller and David L. Wallace. 1963. In-
ference in an authorship problem. Journal Of The
American Statistical Association, 58(302):275–309.

Arvind Narayanan, Hristo Paskov, Neil Zhenqiang
Gong, John Bethencourt, Emil Stefanov, Eui
Chul Richard Shin, and Dawn Song. 2012. On the
feasibility of internet-scale author identification. In
Proceedings of the 2012 IEEE Symposium on Secu-
rity and Privacy, pages 300–314.

Jamal Abdul Nasir, Nico Görnitz, and Ulf Brefeld.
2014. An off-the-shelf approach to authorship attri-
bution. The 25th International Conference on Com-
putational Linguistics, pages 895–904.

John C. Platt. 1998. Fast training of support vec-
tor machines using sequential minimal optimization.
In Advances in Kernel Methods – Support Vector
Learning, pages 185–208.

Tieyun Qian, Bing Liu, Li Chen, and Zhiyong Peng.
2014. Tri-training for authorship attribution with
limited training data. In Proceedings of the 52nd An-
nual Meeting of the Association for Computational
Linguistics, volume 2, pages 345–351.

Tomislav Reicher, Ivan Krišto, Igor Belša, and Artur
Šilić. 2010. Automatic authorship attribution for
texts in Croatian language using combinations of
features. In Knowledge-Based and Intelligent In-
formation and Engineering Systems, volume 6277,
pages 21–30.

Jonathan Schler, Moshe Koppel, Shlomo Argamon,
and James W. Pennebaker. 2006. Effects of age and
gender on blogging. In Proceedings of AAAI Spring
Symposium on Computational Approaches for Ana-
lyzing Weblogs, pages 199–205.

Roy Schwartz, Oren Tsur, Ari Rappoport, and Moshe
Koppel. 2013. Authorship attribution of micro-
messages. In Empirical Methods in Natural Lan-
gauge Processing, pages 1880–1891.

Fabrizio Sebastiani. 2002. Machine learning in au-
tomated text categorization. ACM Computing Sur-
veys, 34:1–47.

Thamar Solorio, Sangita Pillay, Sindhu Raghavan, and
Manuel Montes-y Gómez. 2011. Modality specific
meta features for authorship attribution in web fo-
rum posts. In The 5th International Joint Confer-
ence on Natural Language Processing, pages 156–
164.

Rui Sousa-Silva, Gustavo Laboreiro, Luís Sarmento,
Tim Grant, Eugénio C. Oliveira, and Belinda Maia.

2011. ’twazn me!!! ;(’ Automatic authorship anal-
ysis of micro-blogging messages. In Proceedings of
the 16th International Conference on Natural Lan-
guage Processing and Information Systems, pages
161–168.

Efstathios Stamatatos, Nikos Fakotakis, and Georgios
Kokkinakis. 2001. Computer-based authorship at-
tribution without lexical measures. Computers and
the Humanities, 35(2):193–214.

Efstathios Stamatatos. 2009. A survey of modern
authorship attribution methods. Journal of the As-
sociation for Information Science and Technology,
60(3):538–556.

Urszula Stańczyk and Krzysztof A. Cyran. 2007. Ma-
chine learning approach to authorship attribution
of literary texts. International Journal of Applied
Mathematics and Informatics, 1(4):151–158.

Hans Van Halteren, R. Harald Baayen, Fiona Tweedie,
Marco Haverkort, and Anneke Neijt. 2005. New
machine learning methods demonstrate the exis-
tence of a human stylome. Journal of Quantitative
Linguistics, 12:65–77.

Andelka Zečević and Miloš Utvić. 2012. An author-
ship attribution for Serbian. In Local Proceedings
of the Fifth Balkan Conference in Informatics, pages
109–112.

Andelka Zečević. 2011. N-gram based text classifica-
tion according to authorship. In Proceedings of the
Student Research Workshop associated with Recent
Advances in Natural Language Processing, pages
145–149.

Ying Zhao and Justin Zobel. 2005. Effective and scal-
able authorship attribution using function words. In
Proceedings of the Second AIRS Asian Information
Retrieval Symposium, pages 174–189.

Rong Zheng, Jiexun Li, Hsinchun Chen, and Zan
Huang. 2006. A framework for authorship iden-
tification of online messages: writing-style features
and classification techniques. Journal of the Ameri-
can Society for Information Science and Technology,
57(3):378–393.

Vytautas Zinkevičius. 2000. Lemuoklis – mor-
fologinei analizei [Morphological analysis with
Lemuoklis]. Darbai ir dienos, 24:246–273. In
Lithuanian.

Ana Zwitter Vitez. 2012. Authorship attribution:
specifics for Slovene. Slavia Centralis, 1(14):75–
85.

105


