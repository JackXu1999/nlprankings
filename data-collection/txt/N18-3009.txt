



















































Using Aspect Extraction Approaches to Generate Review Summaries and User Profiles


Proceedings of NAACL-HLT 2018, pages 68–75
New Orleans, Louisiana, June 1 - 6, 2018. c©2017 Association for Computational Linguistics

Using Aspect Extraction Approaches
to Generate Review Summaries and User Profiles

Christopher Mitcheltree∗ Veronica Wharton∗ Avneesh Saluja
Airbnb AI Lab

San Francisco, CA, USA
firstname.lastname@airbnb.com

Abstract

Reviews of products or services on Internet
marketplace websites contain a rich amount
of information. Users often wish to survey
reviews or review snippets from the perspec-
tive of a certain aspect, which has resulted in a
large body of work on aspect identification and
extraction from such corpora. In this work, we
evaluate a newly-proposed neural model for
aspect extraction on two practical tasks. The
first is to extract canonical sentences of var-
ious aspects from reviews, and is judged by
human evaluators against alternatives. A k-
means baseline does remarkably well in this
setting. The second experiment focuses on the
suitability of the recovered aspect distributions
to represent users by the reviews they have
written. Through a set of review reranking
experiments, we find that aspect-based pro-
files can largely capture notions of user pref-
erences, by showing that divergent users gen-
erate markedly different review rankings.

1 Introduction

Aspect extraction has traditionally been associ-
ated with the sentiment analysis community (Liu,
2012; Pontiki et al., 2016), with the goal being
to decompose a small document of text (e.g., a
review) into multiple facets, each of which may
possess their own sentiment marker. For exam-
ple, a restaurant review may comment on the am-
biance, service, and food, preventing the assign-
ment of a uniform sentiment over the entire re-
view. A common approach to aspect extraction
is to treat the aspects as latent variables and uti-
lize latent Dirichlet allocation (LDA; Blei et al.
(2003)) to extract relevant aspects from a collec-
tion of documents in an unsupervised (Titov and
McDonald, 2008; Brody and Elhadad, 2010) or
semi-supervised (Mukherjee and Liu, 2012) fash-

∗Equal contribution.

ion. Subsequent research has taken the latent vari-
able approach further by encoding more compli-
cated dependencies between aspects and sentiment
(Zhao et al., 2010), or between aspects, ratings,
and sentiment (Diao et al., 2014), using probabilis-
tic graphical models (Koller and Friedman, 2009)
to jointly learn the parameters.

However, it has been argued that the coherence
of aspects extracted from the family of LDA-based
approaches is low; words clustered together within
a specific aspect are often unrelated, which can be
attributed to the lack of word co-occurrence in-
formation in these models (Mimno et al., 2011),
since conventional LDA assumes each word in a
document is generated independently. Recently,
He et al. (2017) proposed a neural attention-based
aspect extraction (ABAE) approach, which like
LDA, is an unsupervised model. The starting
point is a set of word embeddings, where the vec-
tor representation of the word encapsulates co-
occurrence1. The embeddings are used to rep-
resent a sentence as a bag-of-words, weighted
with a self-attention mechanism (Lin et al., 2017),
and learning amounts to encoding the resulting
attention-based sentence embedding as a linear
combination of aspect embeddings, optimized us-
ing an autoencoder formulation (§2). The attention
mechanism thus learns to highlight words that will
be pertinent for aspect identification.

In this work, we apply the ABAE model to a
large corpus of reviews on Airbnb2, an online mar-
ketplace for travel; users (guests) utilize the site
to find accommodation (listings) all around the
world, and a large number of these guests write
reviews of the listing post-stay. We first provide
additional details on the workings of the ABAE

1words that co-occur with each other get mapped to points
close to each other in the embedding space (Harris, 1968;
Schütze, 1998).

2www.airbnb.com

68



model (§2). ABAE is then applied to two tasks:
the first (§3.1) is to extract a representative sen-
tence from a set of listing-specific reviews for a
number of pre-defined aspects e.g., cleanliness and
location, with the efficacy of extractive summa-
rization evaluated by humans (§4.3). Surprisingly,
we find that the k-means baseline performs very
well on aspects that occur more frequently, but
ABAE may be better for infrequent aspects.

In the second task (§3.2), we analyze the suit-
ability of aspect embeddings to represent guest
profiles. The hypothesis is that the content of guest
reviews reveals the guest’s preferences and pri-
orities (Chen et al., 2015), and that these prefer-
ences correspond to extracted aspects. We inves-
tigate several ways to aggregate sentence-level as-
pect embeddings at the review and user levels and
compute distances between user aspect and listing
review embeddings, in order to personalize list-
ing reviews by reranking them for each user. The
correlation between guest profile distances (com-
puted on pairs of guests) and review rank distances
(computed on pairs of ordinal rankings over re-
views) is then measured to evaluate our hypothesis
(§4.4). We find a robust relationship between dis-
tances in the two spaces, with the correlation in-
creasing at finer granularities like sentences com-
pared to reviews or listings.

2 Background

To start, we provide a brief background of the
ABAE model. For additional details, please re-
fer to the original paper (He et al., 2017). At
a high level, the ABAE model is an autoencoder
that minimizes the reconstruction error between a
weighted bag-of-words (BoW) representation of a
sentence (where the weights are determined by a
self-attention mechanism) and a linear combina-
tion of aspect embeddings. The linear combina-
tion represents the probabilities of the sentence be-
longing to each of the aspects.

The first step in ABAE is to compute the em-
bedding zs ∈ Rd for a sentence s:

zs =
n∑

i=1

aiewi

where ewi is the word embedding e ∈ Rd for word
wi. As in the original paper, we use word vectors
trained using the skip-gram model with negative
sampling (Mikolov et al., 2013). The attention
weights ai are computed as a multiplicative self-

attention model:

ai = softmax(eTwi ·M · ys)

ys =
n∑

i=1

ewi

where ys is simply the uniformly-weighted BoW
embedding of the sentence, and M ∈ Rd×d is a
learned attention model.

The next step is to compute the aspect-based
sentence representation rs ∈ Rd in terms of an
aspect embeddings matrix T ∈ RK×d, where K
is the number of aspects:

ps = softmax(W · zs + b)
rs = T

T · ps

where ps ∈ RK is the weight (probability) vector
over K aspect embeddings, and W ∈ RK×d,b ∈
RK are parameters of a multiclass logistic regres-
sion model.

The model is trained to minimize reconstruction
error (using the cosine distance between rs and zs)
with a contrastive max-margin objective function
(Weston et al., 2011). In addition, an orthogonal-
ity penalty term is added to the objective, which
encourages the aspect embedding matrix T to pro-
duce diverse (orthogonal) aspect embeddings.

3 Tasks

To evaluate the utility of ABAE, we craft two
methods of evaluation that mimic the practical
ways in which aspect extraction can be used on
a marketplace website with reviews.

3.1 Extractive Summarization
The first task is a direct evaluation of the quality
of the recovered aspects: we use ABAE to select
review sentences of a listing that are representa-
tive of a set of preselected aspects, namely cleanli-
ness, communication, and location. “Cleanliness”
refers to how clean the listing is, “communica-
tion” refers to communication between the listing
host and the guest, and “location” refers to the
qualities of or amenities in the listing’s neighbor-
hood. Refer to Table 3 for representative words
for each aspect. Thus, aspect extraction is used to
summarize listing reviews along several manually-
defined topics.

We benchmark the ABAE model’s extracted as-
pects against those from two baselines: LDA and
k-means. For each experimental setup, the authors

69



assigned one of four interpretable labels (corre-
sponding to the identified aspects and the “other”
category) to each unlabeled aspect by evaluating
the 50 words most associated with that aspect3.
LDA’s topics are represented as distributions over
words, so the most associated words correspond
to those that occur with highest probability. For
k-means and ABAE, each aspect is represented as
a point in word embedding space4, so we retrieve
the 50 closest words to each point using cosine dis-
tance as a measure.

After aspect identification, we infer aspect dis-
tributions for the review sentences of an unseen
test set of listings. For LDA, identification simply
amounts to computing the (approximate) posterior
over topic mixtures for a set of review sentences,
and selecting the sentences with the highest prob-
ability in the specified aspect. For k-means, each
sentence is represented as a uniformly-weighted
BoW embedding, and we retrieve the sentences
that are closest to the centroids that correspond to
our preselected aspects. ABAE is similar, except
the self-attention mechanism is applied to com-
pute an attention-based BoW embedding, and we
retrieve the sentences closest to the aspect embed-
dings corresponding to our aspects of interest. For
some aspects (e.g., location and communication),
there is a many-to-one mapping between the re-
covered word clusters and the aspect label. In
these cases, we compute the average of the aspect
embeddings, and the closest sentences to the re-
sulting points are retrieved.

In our selection process, we retrieve the three
most representative sentences (across all reviews
of that listing) for each aspect. Three human an-
notators then evaluated the appropriateness of the
selected aspect for each sentence via binary judg-
ments. For example, an evaluator was presented
with the sentence “Easy to get there from center of
city by subway and bus.”, along with the inferred
aspect (location), for which a binary “yes/no” re-
sponse suffices. Results aggregated by experimen-
tal setup and aspect are presented in §4.3.

3.2 Aspects as Profiles

An aspect extraction model provides a distribu-
tion over aspects for each sentence, and we can
consider these distributions as interpretable sen-
tence embeddings (since we can assign a mean-

3Inter-annotator agreements for each setup are provided
in Table 3.

4The aspect embeddings in ABAE are initialized using the
k-means centroids.

ing corresponding to an aspect for each of the ex-
tracted word clusters). These embeddings can be
used to provide guest profiles by aggregating as-
pect distributions over review sentences that a user
has written across different listings on the website.
Such profiles arguably capture finer-grained infor-
mation about guest preferences than an aggregate
star rating across all aspects. Star ratings are also
heavily positively-biased: more than 80% of re-
views on Airbnb rate the maximum of 5 stars.

There are many conceivable ways to aggregate
the sentence distributions, with some of the factors
of variation being:

1. level of hierarchy: is the guest considered to
be a bag-of-reviews (BoR), sentences (BoS),
or words i.e., do we weight longer sentences
or reviews with more sentences higher when
computing the aggregated representation?

2. time decay: how do we treat more recently
written reviews compared to earlier ones?

3. average or maximum: is the aggregate repre-
sentation a (weighted) average, or should we
consider the maximum value across each as-
pect and renormalize?

The same considerations also arise when comput-
ing the representations of the objects to be ranked
e.g., a listing embedding as the aggregation of its
component sentence embeddings.

For our evaluation (§4.4), guest profiles are
computed by averaging distributions across a
guest’s review sentences uniformly (BoS), equiv-
alent to a BoR representation weighted by the
review length (number of sentences). We also
experimented with a BoR representation using
uniformly-weighted reviews, and the results are
very similar to the BoS representation. We con-
sidered computing the guest profile by utilizing
the maximum value (probability) of each aspect
dimension across all review sentences written by
the user and renormalizing the resulting embed-
ding using the softmax function, but this approach
resulted in high-entropic guest profiles with lim-
ited use downstream. More complex aggregation
functions, like using an exponential moving aver-
age to upweight recent reviews, is an interesting
future direction to explore.

4 Evaluation

We now look at the qualitative and quantitative
performance of ABAE across the two tasks. Af-
ter providing statistics on the review corpus that
forms the basis of our evaluation, we qualita-
tively analyze the recovered aspects of the model,

70



compared to k-means and LDA baselines. On a
heldout evaluation set, human evaluators assessed
whether the model-extracted aspects correspond to
their understanding of the predefined ones by in-
specting the top-ranked sentences for each aspect.
Furthermore, the quality of the guest profile em-
beddings was evaluated by looking at the correla-
tion between distances in the aspect space and the
ordinal position of reviews on a given listing page,
with the hypothesis that guests who write reviews
with divergent content or aspects should receive
rankings that are very different.

Our experiments were implemented using the
pyTorch package5. Word vectors were trained us-
ing Gensim (Řehůřek and Sojka, 2010) with 5 neg-
ative samples, window size 5, and dimension 200,
and Scikit-learn (Pedregosa et al., 2011) was used
to run the k-means algorithm and LDA with the
default settings. For ABAE, we used Adam with
a learning rate of 0.001 (and the default β param-
eters) with a batch size of 50, 20 negative sam-
ples, and an orthogonality penalty weight of 0.1.
All experiments were run on an Amazon AWS
p2.8xlarge instance.

4.1 Datasets

The corpus was extracted from all reviews across
all listings on Airbnb written between January 1,
2010 and January 1, 2017. We used spaCy6 to
segment reviews into sentences and remove non-
English sentences. All sentences were subse-
quently preprocessed in the same manner as He
et al. (2017), which entailed restricting the vocab-
ulary to the 9,000 most frequent words in the cor-
pus after stopword and punctuation removal. From
the resulting set, we randomly sampled 10 million
sentences across 5.8 million guests and 1.8 mil-
lion listings to form a training set, and used the re-
maining unsampled sentences to select validation
and test sets for the human evaluation (§4.3) and
ranking (§4.4) experiments.

To select datasets for human evaluation, we
identified all listings with at least 50 and at most
100 reviews in all languages and filtered out any
listing in the training set, resulting in 900 list-
ings which were split into validation and test sets.
The validation set is used to select an appropri-
ate number of aspects, by computing coherence
scores (Mimno et al., 2011) as the number of as-
pects is varied in the ABAE model (§4.2). The
test set was used to extract review sentences that

5http://pytorch.org/
6http://spacy.io/

were presented to our human evaluators; we en-
sured that every listing in the test set has at least 3
non-empty English review sentences.

For the ranking correlation experiments, we first
identified users who had written at least 10 re-
view sentences in our corpus and removed those
users that featured in the training set from this list.
We then selected 20 users uniformly at random to
form our validation set i.e., to compute guest pro-
files for7. A subset of the human evaluation test
set was used to compute the correlation between
aspect space and ranking order distances; we se-
lected all listings that had at least 20 review sen-
tences, resulting in 69 listings for evaluation. Ta-
ble 1 presents a summary of corpus statistics for
all of the datasets used in this work.

Set Task Tokens Sentences Guests Listings
Train - 68.0mil 10.0mil 5.8mil 1.8mil
Val §3.1 91,124 14,173 3719 721
Test §3.1 21,069 3389 920 168
Val §3.2 3189 543 20 202
Test §3.2 13,925 2269 587 69

Table 1: Corpus statistics for the datasets that we use. All
numbers are computed after preprocessing.

4.2 Recovered Aspects
Table 2 presents coherence scores for the ABAE
model as we varied the number of aspects. Sim-
ilar to He et al. (2017), we considered a “docu-
ment” to be a sentence, but treating reviews as
documents or all reviews of a listing as a docu-
ment revealed similar trends. The table shows that
coherence score improvements taper off after 30
aspects, so we chose this aspect value for further
experiments.

Num. Num. Representative Words Sum
Aspects 10 30 50
5 -125 -1106 -2829 -4060
10 -148 -1244 -3017 -4409
15 -126 -1069 -2656 -3851
30 -101 -760 -1917 -2778
40 -84 -701 -1765 -2550

Table 2: Coherence scores as a function of the number of
aspects and the number of representative words used to com-
pute the scores (higher is better). The summed values indicate
significant improvement from 15 to 30 aspects. For details on
computing coherence score, see Mimno et al. (2011).

Next, for each 30-aspect experimental setup,
we identified the word clusters corresponding
to the set of preselected aspects by labeling

7The most prolific guest in this set had written 66 review
sentences.

71



Aspects
Setup Location Cleanliness Communication Fleiss’ κ
k-means union, music, minute, dozen,

quarter, chain, zoo, buffet, nord,
theater (3 clusters, 10.2%)

master, conditioners, boiler,
fabric, roll, smelling, dusty,
shutter, dirty, installed (1 clus-
ter, 3.6%)

welcomed, sorted, proactive,
fix, checkin, prior, replied, pro-
cess, communicator, ahead (3
clusters, 9.9%)

0.58

LDA restaurant, location, flat, walk,
away, back, short, minute, bus,
come (4 clusters, 16.2%)

house, comfortable, clean, bed,
beach, street, part, modern, ap-
partment, cool (1 cluster, 3.5%)

helpful, arrival, wonderful, cof-
fee, loved, use, warm, commu-
nication, friendly, got (4 clus-
ters, 14.5%)

0.46

ABAE statue, tavern, woodsy, street,
takeaway, woodland, cathedral,
specialty, idyllic, attraction (6
clusters, 18.4%)

clean, neat, pictured, im-
maculate, spotless, stylish,
described, uncluttered, tidy,
classy (1 cluster, 3.5%)

dear, u, responsive, greeted, in-
struction, communicative, sent,
contract, attentive, key (3 clus-
ters, 10.1%)

0.46

Table 3: Representative words for each aspect of interest across experimental setups, along with the number of clusters
mapped to that aspect in parentheses as well as the percentage of validation set sentences assigned to that cluster (the remaining
sentences were assigned to “Other”). For the aspects with multiple clusters, we select a roughly equal number of words from
each cluster. Misspellings are deliberate.

each revealed cluster with a value from the set
{cleanliness, communication, location, other}.
Note that the mapping from clusters to identified
aspects is many-to-one (i.e., multiple clusters for
the same aspect were identified for two of the three
aspects, namely location and communication.) In
fact, the number of clusters associated with each
aspect is a proxy for the frequency with which
these aspects occur in the corpus. To verify this
claim, we computed aspect-based representations
(§3.1) for each sentence in the validation set used
for comparing coherence scores, and utilized these
representations to compute sentence similarities
to each cluster, followed by a softmax in order
to assign fractional counts i.e., a soft clustering
approach. For each setup, Table 3 provides the top
10 words associated with each aspect, the number
of clusters mapped to that aspect, and the number
of validation sentences assigned to the aspect. The
location and communication aspects are 3 to 6
times more prevalent than the cleanliness aspect.

Qualitatively, the ABAE aspects are more co-
herent, especially in the cleanliness aspect, and
do not include irrelevant words (often verbs) that
are not indicative of any conceivable aspect, like
“got”, “use”, or “come”. k-means selects rele-
vant words to indicate the aspect, but the aspects
are relatively incoherent compared to ABAE. LDA
has a difficult time identifying relevant words, in-
dicating the importance of the attention mecha-
nism in ABAE. Interestingly, we found that the
inter-annotator agreement (Fleiss’ κ) was slightly
higher for the k-means baseline, but all scores are
in the range of moderate agreement.

Aspects
Setup Loc Clean Comm
k-means 0.85/0.68 0.30/0.26 0.62/0.43
LDA 0.16/0.17 0.09/0.10 0.11/0.13
ABAE 0.45/0.46 0.45/0.32 0.41/0.35

Table 4: Precision@1 and precision@3 for the extractive
summarization task, as judged by our human evaluators.

4.3 Extracting Prototypical Sentences
Table 4 presents precision@1 and precision@3 re-
sults for each experimental setup-aspect pair, as
evaluated by our human annotators. There are a
total of 168 listings × 3 experimental setups × 3
aspects× 3 sentences per aspect = 4536 examples
to evaluate; we set aside 795 examples to com-
pute inter-annotator agreement, resulting in 2042
examples per annotator. Fleiss’ κ = 0.69, which
is quite high given the difficulty of the task8.

The most surprising result is that the k-means
baseline is actually the strongest performer in the
location and communication aspects. Nonethe-
less, the result is encouraging since it suggests
that, for some aspects of interest to us, a simple
k-means approach and uniformly-weighted BoW
embeddings suffices. It is interesting to note that
the strong baseline performance occurs with the
aspects that occur more frequently in the corpus,
as discussed in §4.2, suggesting that ABAE is
more useful with aspects that occur more rarely in
our corpus (e.g., cleanliness). For future work, we
propose to evaluate this hypothesis in more depth
by applying the approaches in this paper to the
long tail of rarer aspects. The disappointing per-
formance of LDA shows that its lack of aware-

8The communication aspect (referring to host responsive-
ness and timeliness) is often easily confused with the friend-
liness of the host or staff.

72



Ranking BoS listings

0.000 0.005 0.010 0.015 0.020
Distance b/t pair of guests (Symmetric KL-divergence)

1.00

0.75

0.50

0.25

0.00

0.25

0.50

0.75

1.00

Ke
nd

al
l's

 T
au

(a) R2 = 0.39.

Ranking BoS reviews
(averaged over listings)

0.000 0.005 0.010 0.015 0.020
Distance b/t pair of guests (Symmetric KL-divergence)

1.00

0.75

0.50

0.25

0.00

0.25

0.50

0.75

1.00

Ke
nd

al
l's

 T
au

(b) R2 = 0.73.

Ranking sentences
(averaged over listings)

0.000 0.005 0.010 0.015 0.020
Distance b/t pair of guests (Symmetric KL-divergence)

1.00

0.75

0.50

0.25

0.00

0.25

0.50

0.75

1.00

Ke
nd

al
l's

 T
au

(c) R2 = 0.75.

0.00 0.01 0.02 0.03 0.04 0.05 0.06
Distance b/t pair of guests (Symmetric KL-divergence)

1.00

0.75

0.50

0.25

0.00

0.25

0.50

0.75

1.00

Ke
nd

al
l's

 T
au

(d) R2 = 0.46.

0.00 0.01 0.02 0.03 0.04 0.05 0.06
Distance b/t pair of guests (Symmetric KL-divergence)

1.00

0.75

0.50

0.25

0.00

0.25

0.50

0.75

1.00
Ke

nd
al

l's
 T

au

(e) R2 = 0.81.

0.00 0.01 0.02 0.03 0.04 0.05 0.06
Distance b/t pair of guests (Symmetric KL-divergence)

1.00

0.75

0.50

0.25

0.00

0.25

0.50

0.75

1.00

Ke
nd

al
l's

 T
au

(f) R2 = 0.84.

Figure 1: Plots showing the relationship between distance in aspect space and ranking space, for guest profiles computed as
bag-of-sentences representations. Figures 1a, 1b, and 1c are produced with the ABAE model, and figures 1d, 1e, while 1f are
produced with the k-means model.

ness for word co-occurrence is damaging for as-
pect identification.

4.4 Review Ranking

Figure 1 presents results for the ranking corre-
lation experiments with the ABAE and k-means
models. The validation set is used to compute pair-
wise distances between all

(
20
2

)
= 190 guest pairs

using the symmetric KL divergence, since guest
profiles are probability distributions over aspects.
This divergence forms the x-axis for our plots. We
then rerank several objects of interest, and com-
pute the rank correlation coefficient (Kendall’s τ )
between pairs of rankings; this coefficient forms
the y-axis for our plots. Lastly, the correlation be-
tween the distance in aspect space (between pairs
of user profiles) and the distance in ranking space
(between pairs of rankings over objects, as mea-
sured by Kendall’s τ ) with R2 values stated in the
captions.

With the guest profiles, we ranked the following
objects using the symmetric KL divergence:

1. listings, where each listing is represented as a
BoS (similar results were achieved when con-
sidering each listing as a BoR).

2. reviews within a listing: for each guest pair
and listing, we ranked the reviews using each

guest’s profile and computed Kendall’s τ be-
tween the ranked pair of reviews. That score
was then averaged over the 69 listings to yield
a single score for each guest pair.

3. sentences within a listing: similar to reviews
within a listing, except Kendall’s τ was com-
puted over ranked sentences. The averaging
step was the same as above.

Since ABAE extracts aspects at the sentence-level,
we would expect to see sentence-based represen-
tations result in higher correlations than other rep-
resentations. Indeed, if we rank smaller units (i.e.,
sentences vs. listings), the correlation with dis-
tances in aspect space is higher (0.75 vs. 0.39 in
the case of ABAE, 0.84 vs. 0.46 in the case of
k-means). Interestingly, the correlation results are
slightly better for k-means: the range of values for
the pairwise distances (x-axis) is much larger, so
it seems like the k-means guest profiles are better
at capturing extremely divergent users, and the re-
sulting ranking pairs are more divergent too. Table
5 presents an example of divergent rankings over
review sentences for a given listing from two dif-
ferent guest profiles using the ABAE model.

73



Rank Guest 1 Guest 2
1 Room is cozy and clean only the washroom feel a

little bit old.
Within walking distance to Feng Chia Night Mar-
ket yet quiet enough when it comes time to rest.

2 Clean and comfortable room for the lone traveller
or couples.

Nice and clean place to stay, very near to Fengjia
night market.

3 The room is very good, as good as on the photos,
and also clean.

Overall my TaiChung trip was good and really con-
venient place to stay at Nami’s place.

4 Nice and clean place to stay, very near to Fengjia
night market.

Ia a great place to stay, clean.

5 Within walking distance to Feng Chia Night Mar-
ket yet quiet enough when it comes time to rest.

Near feng jia night market.

Table 5: From the experiment in §4.4, ranked review sentences for two different guest profiles for the same listing using the
ABAE model. The first guest’s profile focuses on the listing interior and cleanliness aspects, whereas the second guest is more
interested in location.

5 Conclusion

In this work, we evaluated a recently proposed
neural-based aspect extraction model in several
settings. First, we used the inferred sentence-level
aspects to select prototypical review sentences of
a listing for a given aspect, and evaluated this as-
pect identification/extractive summarization task
using human evaluators benchmarked against two
baselines. Interestingly, the k-means baseline does
quite well on frequently-occurring aspects. Sec-
ond, the sentence-level aspects were also used to
compute user profiles by grouping reviews that in-
dividual users have written. We showed that these
embeddings are effective in reranking sentences,
reviews, or listings in order to personalize this con-
tent to individual users.

For future work, we wish to investigate alterna-
tive ways to aggregate and compute user profiles
and compute distances between objects to rank
and user profiles. We would also like to utilize
human evaluators to judge the rankings produced
in the review reranking experiments.

Acknowledgments

We thank the authors of the ABAE paper (He et al.,
2017) for providing us with their code9, which we
used to benchmark our internal implementation.
We are also grateful to our human evaluators.

References
David Blei, Andrew Ng, and Michael Jordan. 2003.

Latent dirichlet allocation. Journal of Machine
Learning Research, 3:993–1022.

Samuel Brody and Noemie Elhadad. 2010. An unsu-
pervised aspect-sentiment model for online reviews.
In Proceedings of NAACL-HLT.

9https://github.com/ruidan/
Unsupervised-Aspect-Extraction

Li Chen, Guanliang Chen, and Feng Wang. 2015. Rec-
ommender systems based on user reviews: the state
of the art. User Modeling and User-Adapted Inter-
action.

Qiming Diao, Minghui Qiu, Chao-Yuan Wu, Alexan-
der J. Smola, Jing Jiang, and Chong Wang. 2014.
Jointly modeling aspects, ratings and sentiments for
movie recommendation (jmars). In Proceedings of
KDD.

Zellig Harris. 1968. Mathematical structures of lan-
guage. In Interscience tracts in pure and applied
mathematics.

Ruidan He, Wee Sun Lee, Hwee Tou Ng, and Daniel
Dahlmeier. 2017. An unsupervised neural attention
model for aspect extraction. In Proceedings of ACL.

Daphne Koller and Nir Friedman. 2009. Probabilistic
Graphical Models: Principles and Techniques. The
MIT Press.

Zhouhan Lin, Minwei Feng, Cı́cero Nogueira dos San-
tos, Mo Yu, Bing Xiang, Bowen Zhou, and Yoshua
Bengio. 2017. A structured self-attentive sentence
embedding. In Proceedings of ICLR.

Bing Liu. 2012. Sentiment Analysis and Opinion Min-
ing. Morgan & Claypool Publishers.

Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg Cor-
rado, and Jeffrey Dean. 2013. Distributed represen-
tations of words and phrases and their composition-
ality. CoRR: abs/1310.4546.

David Mimno, Hanna M. Wallach, Edmund Talley,
Miriam Leenders, and Andrew McCallum. 2011.
Optimizing semantic coherence in topic models. In
Proceedings of EMNLP.

Arjun Mukherjee and Bing Liu. 2012. Aspect extrac-
tion through semi-supervised modeling. In Proceed-
ings of ACL.

F. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel,
B. Thirion, O. Grisel, M. Blondel, P. Pretten-
hofer, R. Weiss, V. Dubourg, J. Vanderplas, A. Pas-
sos, D. Cournapeau, M. Brucher, M. Perrot, and
E. Duchesnay. 2011. Scikit-learn: Machine learning

74



in Python. Journal of Machine Learning Research,
12:2825–2830.

Maria Pontiki, Dimitris Galanis, Haris Papageorgiou,
Ion Androutsopoulos, Suresh Manandhar, Moham-
mad AL-Smadi, Mahmoud Al-Ayyoub, Yanyan
Zhao, Bing Qin, Orphee De Clercq, Veronique
Hoste, Marianna Apidianaki, Xavier Tannier, Na-
talia Loukachevitch, Evgeniy Kotelnikov, Núria Bel,
Salud Marı́a Jiménez-Zafra, and Gülşen Eryiğit.
2016. Semeval-2016 task 5: Aspect based sentiment
analysis. In Proceedings of SemEval.

Radim Řehůřek and Petr Sojka. 2010. Software Frame-
work for Topic Modelling with Large Corpora. In
Proceedings of the LREC 2010 Workshop on New
Challenges for NLP Frameworks.

Hinrich Schütze. 1998. Automatic word sense discrim-
ination. Computational Linguistics, 24:97–123.

Ivan Titov and Ryan McDonald. 2008. Modeling on-
line reviews with multi-grain topic models. In Pro-
ceedings of WWW.

Jason Weston, Samy Bengio, and Nicolas Usunier.
2011. Wsabie: Scaling up to large vocabulary im-
age annotation. In Proceedings of IJCAI.

Wayne Xin Zhao, Jing Jiang, Hongfei Yan, and Xiaom-
ing Li. 2010. Jointly modeling aspects and opin-
ions with a maxent-lda hybrid. In Proceedings of
EMNLP.

75


