



















































Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics


Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 335–340
Vancouver, Canada, July 30 - August 4, 2017. c©2017 Association for Computational Linguistics

https://doi.org/10.18653/v1/P17-2053

Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 335–340
Vancouver, Canada, July 30 - August 4, 2017. c©2017 Association for Computational Linguistics

https://doi.org/10.18653/v1/P17-2053

Group Sparse CNNs for Question Classification with Answer Sets

Mingbo Ma Liang Huang
School of EECS

Oregon State University
Corvallis, OR 97331, USA

{mam,liang.huang}@oregonstate.edu

Bing Xiang Bowen Zhou
IBM Watson Group

T. J. Watson Research Center
Yorktown Heights, NY 10598, USA

{bingxia,zhou}@us.ibm.com

Abstract

Question classification is an important
task with wide applications. However, tra-
ditional techniques treat questions as gen-
eral sentences, ignoring the corresponding
answer data. In order to consider answer
information into question modeling, we
first introduce novel group sparse autoen-
coders which refine question representa-
tion by utilizing group information in the
answer set. We then propose novel group
sparse CNNs which naturally learn ques-
tion representation with respect to their
answers by implanting group sparse au-
toencoders into traditional CNNs. The
proposed model significantly outperform
strong baselines on four datasets.

1 Introduction

Question classification has applications in many
domains ranging from question answering to di-
alog systems, and has been increasingly popular
in recent years. Several recent efforts (Kim, 2014;
Kalchbrenner et al., 2014; Ma et al., 2015) treat
questions as general sentences and employ Con-
volutional Neural Networks (CNNs) to achieve re-
markably strong performance in the TREC ques-
tion classification task.

We argue, however, that those general sentence
modeling frameworks neglect two unique proper-
ties of question classification. First, different from
the flat and coarse categories in most sentence
classification tasks (i.e. sentimental classification),
question classes often have a hierarchical struc-
ture such as those from the New York State DMV
FAQ1 (see Fig. 1). Another unique aspect of ques-
tion classification is the well prepared answers for
each question or question category. These answer

1Crawled from http://nysdmv.custhelp.com/app/home.
This data and our code will be at http://github.com/cosmmb.

1: Driver License/Permit/Non-Driver ID
a: Apply for original (49 questions)
b: Renew or replace (24 questions)
...
2: Vehicle Registrations and Insurance
a: Buy, sell, or transfer a vehicle (22 questions)
b: Reg. and title requirements (42 questions)
...
3: Driving Record / Tickets / Points
...

Figure 1: Examples from NYDMV FAQs. There
are 8 top-level categories, 47 sub-categories, and
537 questions (among them 388 are unique; many
questions fall into multiple categories).

sets generally cover a larger vocabulary (than the
questions themselves) and provide richer informa-
tion for each class. We believe there is a great po-
tential to enhance question representation with ex-
tra information from corresponding answer sets.

To exploit the hierarchical and overlapping
structures in question categories and extra infor-
mation from answer sets, we consider dictionary
learning (Candès and Wakin, 2008; Rubinstein
et al., 2010) which is a common approach for rep-
resenting samples from many correlated groups
with external information. This learning pro-
cedure first builds a dictionary with a series of
grouped bases. These bases can be initialized ran-
domly or from external data (from the answer set
in our case) and optimized during training through
Sparse Group Lasso (SGL) (Simon et al., 2013).

To apply dictionary learning to CNN, we first
develop a neural version of SGL, Group Sparse
Autoencoders (GSAs), which to the best of our
knowledge, is the first full neural model with
group sparse constraints. The encoding matrix
of GSA (like the dictionary in SGL) is grouped
into different categories. The bases in different
groups can be either initialized randomly or by

335

https://doi.org/10.18653/v1/P17-2053
https://doi.org/10.18653/v1/P17-2053


the sentences in corresponding answer categories.
Each question sentence will be reconstructed by
a few bases within a few groups. GSA can use
either linear or nonlinear encoding or decoding
while SGL is restricted to be linear. Eventually,
to model questions with sparsity, we further pro-
pose novel Group Sparse Convolutional Neural
Networks (GSCNNs) by implanting the GSA onto
CNNs, essentially enforcing group sparsity be-
tween the convolutional and classification layers.
This framework is a jointly trained neural model
to learn question representation with group sparse
constraints from both question and answer sets.

2 Group Sparse Autoencoders

2.1 Sparse Autoencoders

Autoencoder (Bengio et al., 2007) is an unsuper-
vised neural network which learns the hidden rep-
resentations from data. When the number of hid-
den units is large (e.g., bigger than input dimen-
sion), we can still discover the underlying struc-
ture by imposing sparsity constraints, using sparse
autoencoders (SAE) (Ng, 2011):

Jsparse(ρ) = J + α

s∑

j=1

KL(ρ‖ρ̂j) (1)

where J is the autoencoder reconstruction loss, ρ
is the desired sparsity level which is small, and
thus Jsparse(ρ) is the sparsity-constrained version
of loss J . Here α is the weight of the sparsity
penalty term defined below:

KL(ρ‖ρ̂j) = ρ log
ρ

ρ̂j
+ (1− ρ) log 1− ρ

1− ρ̂j
(2)

where

ρ̂j =
1

m

m∑

i=1

hij

represents the average activation of hidden unit j
over m examples (SAE assumes the input features
are correlated).

As described above, SAE has a similar objec-
tive to traditional sparse coding which tries to find
sparse representations for input samples. Besides
applying simple sparse constraints to the network,
group sparse constraints is also desired when the
class categories are structured and overlapped. In-
spired by group sparse lasso (Yuan and Lin, 2006)
and sparse group lasso (Simon et al., 2013), we
propose a novel architecture below.

2.2 Group Sparse Autoencoders
Group Sparse Autoencoder (GSA), unlike SAE,
categorizes the weight matrix into different
groups. For a given input, GSA reconstructs the
input signal with the activations from only a few
groups. Similar to the average activation ρ̂j for
sparse autoencoders, GSA defines each grouped
average activation for the hidden layer as follows:

η̂p =
1

mg

m∑

i=1

g∑

l=1

‖hip,l‖2 (3)

where g represents the size of each group, and η̂j
first sums up all the activations within pth group,
then computes the average pth group respond
across different samples’ hidden activations.

Similar to Eq. 2, we also use KL divergence
to measure the difference between estimated intra-
group activation and global group sparsity:

KL(η‖η̂p) = η log
η

η̂p
+ (1− η) log 1− η

1− η̂p
(4)

where G is the number of groups. Then the objec-
tive function of GSA is:

Jgroupsparse(ρ, η) = J + α

s∑

j=1

KL(ρ‖ρ̂j)

+ β
G∑

p=1

KL(η‖η̂p)
(5)

where ρ and η are constant scalars which are
our target sparsity and group-sparsity levels, resp.
When α is set to zero, GSA only considers the
structure between difference groups. When β is
set to zero, GSA is reduced to SAE.

2.3 Visualizing Group Sparse Autoencoders
In order to have a better understanding of GSA, we
use the MNIST dataset to visualize GSA’s internal
parameters. Fig. 2 and Fig. 3 illustrate the pro-
jection matrix and the corresponding hidden acti-
vations. We use 10,000 training samples. We set
the size of the hidden layer to 500 with 10 groups.
Fig. 2(a) visualizes the input image for hand writ-
ten digit 0.

In Fig. 2(b), we find similar patterns within each
group. For example, group 8 has different forms
of digit 0, and group 9 includes different forms of
digit 7. However, it is difficult to see any mean-
ingful patterns from the projection matrix of basic
autoencoders in Fig. 2(c).

336



(a)

1

2

3

4

5

6

7

8

9

10

(b) (c)

Figure 2: The input figure with hand written digit 0 is shown in (a). Figure (b) is the visualization of
trained projection matrix W on MNIST dataset. Different rows represent different groups of W in Eq. 5.
For each group, we only show the first 15 (out of 50) bases. The red numbers on the left side are the
indices of 10 different groups. Figure (c) is the projection matrix from basic autoencoders.

1 | 2 | 3 | 4 | 5 | 6 | 7 | 8 | 9 | 10
(a) (b)

Figure 3: (a): the hidden activations h for the input image in Fig. 2(a). The red numbers corresponds to
the index in Fig. 2(b). (b): the hidden activations h for the same input image from basic autoencoders.

Fig. 3(a) shows the hidden activations with re-
spect to the input image of digit 0. The patterns of
the 10th row in Fig. 2(b) are very similar to digit
1 which is very different from digit 0 in shape.
Therefore, there is no activation in group 10 in
Fig. 3(a). The majority of hidden layer activations
are in groups 1, 2, 6 and 8, with group 8 being the
most significant. When compared to the projection
matrix visualization in Fig. 2(b), these results are
reasonable since the 8th row has the most similar
patterns of digit 0. However, we could not find any
meaningful pattern from the hidden activations of
basic autoencoder as shown in Fig. 3(b).

GSA could be directly applied to small image
data (e.g. MINIST dataset) for pre-training. How-
ever, in tasks which prefer dense semantic rep-
resentations (e.g. sentence classification), we still
need CNNs to learn the sentence representation
automatically. In order to combine advantages
from GSA and CNNs, we propose Group Sparse

Convolutional Neural Networks below.

3 Group Sparse CNNs

CNNs were first proposed by (LeCun et al., 1995)
in computer vision and adapted to NLP by (Col-
lobert et al., 2011). Recently, many CNN-based
techniques have achieved great successes in sen-
tence modeling and classification (Kim, 2014;
Kalchbrenner et al., 2014).

Following sequential CNNs, one dimensional
convolutions operate the convolution kernel in se-
quential order xi,j = xi ⊕ xi+1 ⊕ · · · ⊕ xi+j ,
where xi ∈ Re represents the e dimensional word
representation for the i-th word in the sentence,
and ⊕ is the concatenation operator. Therefore
xi,j refers to concatenated word vector from the
i-th word to the (i+ j)-th word in sentence.

A convolution operates a filter w ∈ Rn×e to
a window of n words xi,i+n with bias term b′ by
ai = σ(w · xi,i+n + b′) with non-linear activation

337



Any  interesting   places   to   visit   in   Lisbon

… … … … … …

N
 fi

lte
rs

(

Pooling Feed
 into 
NN

Group Sparse Auto-Encoder

Convolutional Layer

WTz h

z h�W,b(·)�W,b(·)�W,b(·)
Figure 4: Group Sparse CNN. We add an extra dictionary learning layer between sentence representation
z and the final classification layer. W is the projection matrix (functions as a dictionary) that converts z
to the group sparse representation h (Eq. 5). Different colors in the projection matrix represent different
groups. We show Wᵀ instead of W for presentation purposes. Darker colors in h mean larger values
and white means zero.

function σ to produce a new feature. The filter w
is applied to each word in the sentence, generating
the feature map a = [a1, a2, · · · , aL] where L is
the sentence length. We then use â = max{a} to
represent the entire feature map after max-pooling.

In order to capture different aspects of patterns,
CNNs usually randomly initialize a set of filters
with different sizes and values. Each filter will
generate a feature as described above. To take all
the features generated by N different filters into
count, we use z = [â1, · · · , âN ] as the final rep-
resentation. In conventional CNNs, this z will be
directly fed into classifiers after the sentence rep-
resentation is obtained, e.g. fully connected neural
networks (Kim, 2014). There is no easy way for
CNNs to explore the possible hidden representa-
tions with underlaying structures.

In order to exploit these structures, we pro-
pose Group Sparse Convolutional Neural Net-
works (GSCNNs) by placing one extra layer be-
tween the convolutional and the classification lay-
ers. This extra layer mimics the functionality of
GSA from Section 2. Shown in Fig. 4, after the
conventional convolutional layer, we get the fea-
ture map z for each sentence. In stead of directly
feeding it into a fully connected neural network
for classification, we enforce the group sparse con-
straint on z in a way similar to the group sparse
constraints on hidden layer in GSA from Sec. 2.
Then, we use the sparse hidden representation h
in Eq. 5 as the new sentence representation, which
is then fed into a fully connected neural network
for classification. The parameters W in Eq. 5 will

also be fine tunned during the last step.
Different ways of initializing the projection ma-

trix in Eq. 5 can be summarized below:

• Random Initialization: When there is no an-
swer corpus available, we first randomly ini-
tializeN vectors to represent the group infor-
mation from the answer set. Then we clus-
ter these N vectors into G categories with g
centroids for each category. These centroids
from different categories will be the initial-
ized bases for projection matrix W which
will be learned during training.

• Initialization from Questions: Instead of
using random initialized vectors, we can also
use question sentences for initializing the
projection matrix when the answer set is not
available. We need to pre-train the sentences
with CNNs to get the sentence representa-
tion. We then select G largest categories in
terms of number of question sentences. Then
we get g centroids from each category by k-
means. We concatenate these G × g vectors
to form the projection matrix.

• Initialization from Answers: This is the
most ideal case. We follow the same proce-
dure as above, with the only difference being
using the answer sentences in place of ques-
tion sentences to pre-train the CNNs.

4 Experiments

Since there is little effort to use answer sets in
question classification, we did not find any suit-

338



Datasets Ct Cs Ndata Ntest Nans Multi-label
TREC 6 50 5952 500 - No
INSURANCE - 319 1580 303 2176 Yes
DMV 8 47 388 50 2859 Yes
YAHOO Ans 27 678 8871 3027 10365 No

Table 1: Summary of datasets. Ct and Cs are
the numbers of top-level and sub- categories, resp.
Ndata, Ntest, Nans are the sizes of data set, test
set, and answer set, resp. Multilabel means each
question can belong to multiple categories.

able datasets which are publicly available. We
collected two datasets ourselves and also used
two other well-known ones. These datasets are
summarized in Table 1. INSURANCE is a pri-
vate dataset we collected from a car insurance
company’s website. Each question is classified
into 319 classes with corresponding answer data.
All questions which belong to the same category
share the same answers. The DMV dataset is col-
lected from New York State the DMV’s FAQ web-
site. The YAHOO Ans dataset is only a subset
of the original publicly available YAHOO Answers
dataset (Fleming et al., 2012; Shah and Pomerantz,
2010). Though not very suitable for our frame-
work, we still included the frequently used TREC
dataset (factoid question type classification) for
comparison.

We only compare our model’s performance with
CNNs for two following reasons: we consider our
“group sparsity” as a modification to the general
CNNs for grouped feature selection. This idea is
orthogonal to any other CNN-based models and
can be easily applied to them; in addition, as dis-
cussed in Sec. 1, we did not find any other model
in comparison with solving question classification
tasks with answer sets.

There is crucial difference between the INSUR-
ANCE and DMV datasets on one hand and the YA-
HOO set on the other. In INSURANCE and DMV,
all questions in the same (sub)category share the
same answers, whereas YAHOO provides individ-
ual answers to each question.

For multi-label classification (INSURANCE and
DMV), we replace the softmax layer in CNNs
with a sigmoid layer which predicts each category
independently while softmax is not.

All experimental results are summarized in Ta-
ble 2. The improvements are substantial for IN-
SURANCE and DMV, but not as significant for
YAHOO and TREC. One reason for this is the

TREC INSUR. DMV
YAHOO dataset

sub top unseen
CNN† 93.6 51.2 60 20.8 53.9 47

+sparsity‡ 93.2 51.4 62 20.2 54.2 46
WR 93.8 53.5 62 21.8 54.5 48
WQ 94.2 53.8 64 22.1 54.1 48
WA - 55.4 66 22.2 55.8 53

Table 2: Experimental results. Baselines:
†sequential CNNs (α = β = 0 in Eq. 5), ‡CNNs
with global sparsity (β = 0). WR: randomly
initialized projection matrix. WQ: question-
initialized projection matrix. WA: answer set-
initialized projection matrix. There are three dif-
ferent classification settings for YAHOO: subcate-
gory, top-level category, and top-level accuracies
on unseen sub-labels.

questions in YAHOO/TREC are shorter, which
makes the group information harder to encode.
Another reason is that each question in YA-
HOO/TREC has a single label, and thus can not
fully benefit from group sparse properties.

Besides the conventional classification tasks,
we also test our proposed model on an unseen-
label case. In these experiments, there are a few
sub-category labels that are not included in the
training data. However, we still hope that our
model could still return the correct parent cate-
gory for these unseen subcategories at test time.
In the testing set of YAHOO dataset, we randomly
add 100 questions whose subcategory labels are
unseen in training set. The classification results of
YAHOO-unseen in Table 2 are obtained by map-
ping the predicted subcategories back to top-level
categories. The improvements are substantial due
to the group information encoding.

5 Conclusions

In order to better represent question sentences with
answer sets and group structure, we first presented
a novel GSA framework, a neural version of dic-
tionary learning. We then proposed group sparse
convolutional neural networks by embedding GSA
into CNNs, which result in significantly better
question classification over strong baselines.

Acknowledgment

We thank the anonymous reviewers for their sug-
gestions. This work is supported in part by
NSF IIS-1656051, DARPA FA8750-13-2-0041
(DEFT), DARPA XAI, a Google Faculty Research
Award, and an HP Gift.

339



References
Yoshua Bengio, Pascal Lamblin, Dan Popovici, and

Hugo Larochelle. 2007. Greedy layer-wise training
of deep networks. In Advances in Neural Informa-
tion Processing Systems 19.

Emmanuel J. Candès and Michael B. Wakin. 2008.
An Introduction To Compressive Sampling. In
Signal Processing Magazine, IEEE. volume 25.
http://dx.doi.org/10.1109/msp.2007.914731.

R. Collobert, J. Weston, L. Bottou, M. Karlen,
K. Kavukcuoglu, and P. Kuksa. 2011. Natural lan-
guage processing (almost) from scratch. In Journal
of Machine Learning Research. volume 12, pages
2493–2537.

Simon Fleming, Dan Chalmers, and Ian Wakeman.
2012. A deniable and efficient question and answer
service over ad hoc social networks. In Information
Retrieval.

Nal Kalchbrenner, Edward Grefenstette, and Phil Blun-
som. 2014. A convolutional neural network for
modelling sentences. In Proceedings of the 52nd
Annual Meeting of the Association for Computa-
tional Linguistics. Association for Computational
Linguistics.

Yoon Kim. 2014. Convolutional neural networks for
sentence classification. In Proceedings of the 2014
Conference on Empirical Methods in Natural Lan-
guage Processing (EMNLP). Association for Com-
putational Linguistics, Doha, Qatar, pages 1746–
1751. http://www.aclweb.org/anthology/D14-1181.

Y. LeCun, L. Jackel, L. Bottou, A. Brunot, C. Cortes,
J. Denker, H. Drucker, I. Guyon, U. Mller,
E. Sckinger, P. Simard, and V. Vapnik. 1995. Com-
parison of learning algorithms for handwritten digit
recognition. In International Conference on Artifi-
cial Neural Networks. pages 53–60.

Mingbo Ma, Liang Huang, Bing Xiang, and Bowen
Zhou. 2015. Dependency-based convolutional neu-
ral networks for sentence embedding. In Proceed-
ings of ACL 2015.

Andrew Ng. 2011. Sparse autoencoder. In CS294A
Lecture notes. Stanford University, page 72.

R. Rubinstein, A. M. Bruckstein, and M. Elad. 2010.
Dictionaries for sparse representation modeling. In
Neural Computation.

Chirag Shah and Jefferey Pomerantz. 2010. Evaluating
and predicting answer quality in community qa. In
Proceedings of the 33rd International ACM SIGIR
Conference on Research and Development in Infor-
mation Retrieval. ACM, New York, NY, USA.

Noah Simon, Jerome Friedman, Trevor Hastie, and
Rob Tibshirani. 2013. A sparse-group lasso. In
Journal of Computational and Graphical Statistics.

Ming Yuan and Yi Lin. 2006. Model selection and es-
timation in regression with grouped variables. In
Journal of the Royal Statistical Society. volume 68,
pages 49–67.

340


	Group Sparse CNNs for Question Classification with Answer Sets

