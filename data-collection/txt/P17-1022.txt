



















































Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics


Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics, pages 232–242
Vancouver, Canada, July 30 - August 4, 2017. c©2017 Association for Computational Linguistics

https://doi.org/10.18653/v1/P17-1022

Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics, pages 232–242
Vancouver, Canada, July 30 - August 4, 2017. c©2017 Association for Computational Linguistics

https://doi.org/10.18653/v1/P17-1022

Translating Neuralese

Jacob Andreas Anca Dragan Dan Klein
Computer Science Division

University of California, Berkeley
{jda,anca,klein}@cs.berkeley.edu

Abstract

Several approaches have recently been pro-
posed for learning decentralized deep mul-
tiagent policies that coordinate via a dif-
ferentiable communication channel. While
these policies are effective for many tasks,
interpretation of their induced communi-
cation strategies has remained a challenge.
Here we propose to interpret agents’ mes-
sages by translating them. Unlike in typi-
cal machine translation problems, we have
no parallel data to learn from. Instead we
develop a translation model based on the
insight that agent messages and natural lan-
guage strings mean the same thing if they
induce the same belief about the world in a
listener. We present theoretical guarantees
and empirical evidence that our approach
preserves both the semantics and pragmat-
ics of messages by ensuring that players
communicating through a translation layer
do not suffer a substantial loss in reward rel-
ative to players with a common language.1

1 Introduction

Several recent papers have described approaches
for learning deep communicating policies (DCPs):
decentralized representations of behavior that en-
able multiple agents to communicate via a differ-
entiable channel that can be formulated as a recur-
rent neural network. DCPs have been shown to
solve a variety of coordination problems, including
reference games (Lazaridou et al., 2016b), logic
puzzles (Foerster et al., 2016), and simple control
(Sukhbaatar et al., 2016). Appealingly, the agents’
communication protocol can be learned via direct

1 We have released code and data at http://github.
com/jacobandreas/neuralese.

z(1)a z
(2)
a

z
(1)
b z

(2)
b

Figure 1: Example interaction between a pair of agents in a
deep communicating policy. Both cars are attempting to cross
the intersection, but cannot see each other. By exchanging
message vectors z(t), the agents are able to coordinate and
avoid a collision. This paper presents an approach for under-
standing the contents of these message vectors by translating
them into natural language.

backpropagation through the communication chan-
nel, avoiding many of the challenging inference
problems associated with learning in classical de-
centralized decision processes (Roth et al., 2005).

But analysis of the strategies induced by DCPs
has remained a challenge. As an example, Figure 1
depicts a driving game in which two cars, which
are unable to see each other, must both cross an
intersection without colliding. In order to ensure
success, it is clear that the cars must communi-
cate with each other. But a number of successful
communication strategies are possible—for exam-
ple, they might report their exact (x, y) coordinates
at every timestep, or they might simply announce
whenever they are entering and leaving the inter-
section. If these messages were communicated
in natural language, it would be straightforward
to determine which strategy was being employed.
However, DCP agents instead communicate with
an automatically induced protocol of unstructured,
real-valued recurrent state vectors—an artificial
language we might call “neuralese,” which superfi-
cially bears little resemblance to natural language,
and thus frustrates attempts at direct interpretation.

232

https://doi.org/10.18653/v1/P17-1022
https://doi.org/10.18653/v1/P17-1022


We propose to understand neuralese messages
by translating them. In this work, we present a sim-
ple technique for inducing a dictionary that maps
between neuralese message vectors and short natu-
ral language strings, given only examples of DCP
agents interacting with other agents, and humans
interacting with other humans. Natural language
already provides a rich set of tools for describing
beliefs, observations, and plans—our thesis is that
these tools provide a useful complement to the visu-
alization and ablation techniques used in previous
work on understanding complex models (Strobelt
et al., 2016; Ribeiro et al., 2016).

While structurally quite similar to the task of
machine translation between pairs of human lan-
guages, interpretation of neuralese poses a number
of novel challenges. First, there is no natural source
of parallel data: there are no bilingual “speakers”
of both neuralese and natural language. Second,
there may not be a direct correspondence between
the strategy employed by humans and DCP agents:
even if it were constrained to communicate using
natural language, an automated agent might choose
to produce a different message from humans in a
given state. We tackle both of these challenges by
appealing to the grounding of messages in game-
play. Our approach is based on one of the core
insights in natural language semantics: messages
(whether in neuralese or natural language) have
similar meanings when they induce similar beliefs
about the state of the world.

Based on this intuition, we introduce a transla-
tion criterion that matches neuralese messages with
natural language strings by minimizing statistical
distance in a common representation space of dis-
tributions over speaker states. We explore several
related questions:

• What makes a good translation, and under
what conditions is translation possible at all?
(Section 4)

• How can we build a model to translate
between neuralese and natural language?
(Section 5)

• What kinds of theoretical guarantees can
we provide about the behavior of agents
communicating via this translation model?
(Section 6)

Our translation model and analysis are general,
and in fact apply equally to human–computer and

large bird 
black wings 

black crown

agent translator

agent translator

small brown 
light brown 
dark brown

Figure 2: Overview of our approach—best-scoring transla-
tions generated for a reference game involving images of birds.
The speaking agent’s goal is to send a message that uniquely
identifies the bird on the left. From these translations it can be
seen that the learned model appears to discriminate based on
coarse attributes like size and color.

human–human translation problems grounded in
gameplay. In this paper, we focus our experiments
specifically on the problem of interpreting commu-
nication in deep policies, and apply our approach
to the driving game in Figure 1 and two reference
games of the kind shown in Figure 2. We find that
this approach outperforms a more conventional ma-
chine translation criterion both when attempting
to interoperate with neuralese speakers and when
predicting their state.

2 Related work

A variety of approaches for learning deep policies
with communication were proposed essentially si-
multaneously in the past year. We have broadly
labeled these as “deep communicating policies”;
concrete examples include Lazaridou et al. (2016b),
Foerster et al. (2016), and Sukhbaatar et al. (2016).
The policy representation we employ in this paper
is similar to the latter two of these, although the
general framework is agnostic to low-level model-
ing details and could be straightforwardly applied
to other architectures. Analysis of communication
strategies in all these papers has been largely ad-
hoc, obtained by clustering states from which simi-
lar messages are emitted and attempting to manu-
ally assign semantics to these clusters. The present
work aims at developing tools for performing this
analysis automatically.

Most closely related to our approach is that of
Lazaridou et al. (2016a), who also develop a model
for assigning natural language interpretations to
learned messages; however, this approach relies
on supervised cluster labels and is targeted specif-
ically towards referring expression games. Here
we attempt to develop an approach that can handle
general multiagent interactions without assuming a
prior discrete structure in space of observations.

233



The literature on learning decentralized multi-
agent policies in general is considerably larger
(Bernstein et al., 2002; Dibangoye et al., 2016).
This includes work focused on communication in
multiagent settings (Roth et al., 2005) and even
communication using natural language messages
(Vogel et al., 2013b). All of these approaches em-
ploy structured communication schemes with man-
ually engineered messaging protocols; these are, in
some sense, automatically interpretable, but at the
cost of introducing considerable complexity into
both training and inference.

Our evaluation in this paper investigates com-
munication strategies that arise in a number of dif-
ferent games, including reference games and an
extended-horizon driving game. Communication
strategies for reference games were previously ex-
plored by Vogel et al. (2013a), Andreas and Klein
(2016) and Kazemzadeh et al. (2014), and refer-
ence games specifically featuring end-to-end com-
munication protocols by Yu et al. (2016). On the
control side, a long line of work considers nonver-
bal communication strategies in multiagent policies
(Dragan and Srinivasa, 2013).

Another group of related approaches focuses on
the development of more general machinery for
interpreting deep models in which messages have
no explicit semantics. This includes both visualiza-
tion techniques (Zeiler and Fergus, 2014; Strobelt
et al., 2016), and approaches focused on generat-
ing explanations in the form of natural language
(Hendricks et al., 2016; Vedantam et al., 2017).

3 Problem formulation

Games Consider a cooperative game with two
players a and b of the form given in Figure 3. At
every step t of this game, player a makes an ob-
servation x(t)a and receives a message z

(t−1)
b from

b. It then takes an action u(t)a and sends a message
z
(t)
a to b. (The process is symmetric for b.) The

distributions p(ua|xa, zb) and p(za|xa) together
define a policy π which we assume is shared by
both players, i.e. p(ua|xa, zb) = p(ub|xb, za) and
p(za|xa) = p(zb|xb). As in a standard Markov
decision process, the actions (u(t)a , u

(t)
b ) alter the

world state, generating new observations for both
players and a reward shared by both.

The distributions p(z|x) and p(u|x, z) may also
be viewed as defining a language: they specify how
a speaker will generate messages based on world
states, and how a listener will respond to these mes-

a

b

x(1)a

x
(1)
b x

(2)
b

u(1)a u
(2)
a

u
(2)
bu

(1)
b

z(1)a z
(2)
a

z
(1)
b

z
(2)
b

a

b

x(2)a

0.3: stop 
0.5: forward 
0.1: left 
0.1: right

observations actions messages

Figure 3: Schematic representation of communication games.
At every timestep t, players a and b make an observation x(t)

and receive a message z(t−1), then produce an action u(t) and
a new message z(t).

sages. Our goal in this work is to learn to translate
between pairs of languages generated by different
policies. Specifically, we assume that we have ac-
cess to two policies for the same game: a “robot
policy” πr and a “human policy” πh. We would
like to use the representation of πh, the behavior of
which is transparent to human users, in order to un-
derstand the behavior of πr (which is in general an
uninterpretable learned model); we will do this by
inducing bilingual dictionaries that map message
vectors zr of πr to natural language strings zh of
πh and vice-versa.

Learned agents πr Our goal is to present tools
for interpretation of learned messages that are ag-
nostic to the details of the underlying algorithm for
acquiring them. We use a generic DCP model as
a basis for the techniques developed in this paper.
Here each agent policy is represented as a deep
recurrent Q network (Hausknecht and Stone, 2015).
This network is built from communicating cells of
the kind depicted in Figure 4. At every timestep,
this agent receives three pieces of information: an

x(t)a

z
(t�1)
b

h(t�1)a h
(t)
a

u(t)a

z(t)aMLP
GRU

Figure 4: Cell implementing a single step of agent commu-
nication (compare with Sukhbaatar et al. (2016) and Foerster
et al. (2016)). MLP denotes a multilayer perceptron; GRU
denotes a gated recurrent unit (Cho et al., 2014). Dashed lines
represent recurrent connections.

234



observation of the current state of the world, the
agent’s memory vector from the previous timestep,
and a message from the other player. It then pro-
duces three outputs: a predicted Q value for every
possible action, a new memory vector for the next
timestep, and a message to send to the other agent.

Sukhbaatar et al. (2016) observe that models of
this form may be viewed as specifying a single
RNN in which weight matrices have a particular
block structure. Such models may thus be trained
using the standard recurrent Q-learning objective,
with communication protocol learned end-to-end.

Human agents πh The translation model we de-
velop requires a representation of the distribution
over messages p(za|xa) employed by human speak-
ers (without assuming that humans and agents pro-
duce equivalent messages in equivalent contexts).
We model the human message generation process
as categorical, and fit a simple multilayer percep-
tron model to map from observations to words and
phrases used during human gameplay.

4 What’s in a translation?

What does it mean for a message zh to be a “trans-
lation” of a message zr? In standard machine trans-
lation problems, the answer is that zh is likely to
co-occur in parallel data with zr; that is, p(zh|zr)
is large. Here we have no parallel data: even if
we could observe natural language and neuralese
messages produced by agents in the same state, we
would have no guarantee that these messages ac-
tually served the same function. Our answer must
instead appeal to the fact that both natural language
and neuralese messages are grounded in a common
environment. For a given neuralese message zr,
we will first compute a grounded representation
of that message’s meaning; to translate, we find a
natural-language message whose meaning is most
similar. The key question is then what form this
grounded meaning representation should take. The
existing literature suggests two broad approaches:

Semantic representation The meaning of a mes-
sage za is given by its denotations: that is, by the
set of world states of which za may be felicitously
predicated, given the existing context available to
a listener. In probabilistic terms, this says that the
meaning of a message za is represented by the dis-
tribution p(xa|za, xb) it induces over speaker states.
Examples of this approach include Guerin and Pitt
(2001) and Pasupat and Liang (2016).

Pragmatic representation The meaning of a
message za is given by the behavior it induces in
a listener. In probabilistic terms, this says that the
meaning of a message za is represented by the dis-
tribution p(ub|za, xb) it induces over actions given
the listener’s observation xb. Examples of this ap-
proach include Vogel et al. (2013a) and Gauthier
and Mordatch (2016).

These two approaches can give rise to rather dif-
ferent behaviors. Consider the following example:

square hexagon circle

few many many

The top language (in blue) has a unique name for
every kind of shape, while the bottom language (in
red) only distinguishes between shapes with few
sides and shapes with many sides. Now imagine
a simple reference game with the following form:
player a is covertly assigned one of these three
shapes as a reference target, and communicates
that reference to b; b must then pull a lever labeled
large or small depending on the size of the
target shape. Blue language speakers can achieve
perfect success at this game, while red language
speakers can succeed at best two out of three times.

How should we translate the blue word hexagon
into the red language? The semantic approach sug-
gests that we should translate hexagon as many:
while many does not uniquely identify the hexagon,
it produces a distribution over shapes that is clos-
est to the truth. The pragmatic approach instead
suggests that we should translate hexagon as few,
as this is the only message that guarantees that the
listener will pull the correct lever large. So in
order to produce a correct listener action, the trans-
lator might have to “lie” and produce a maximally
inaccurate listener belief.

If we were exclusively concerned with building
a translation layer that allowed humans and DCP
agents to interoperate as effectively as possible, it
would be natural to adopt a pragmatic representa-
tion strategy. But our goals here are broader: we
also want to facilitate understanding, and specif-
ically to help users of learned systems form true
beliefs about the systems’ computational processes
and representational abstractions. The example
above demonstrates that “pragmatically” optimiz-
ing directly for task performance can sometimes
lead to translations that produce inaccurate beliefs.

235



We instead build our approach around seman-
tic representations of meaning. By preserving se-
mantics, we allow listeners to reason accurately
about the content and interpretation of messages.
We might worry that by adopting a semantics-first
view, we have given up all guarantees of effective
interoperation between humans and agents using
a translation layer. Fortunately, this is not so: as
we will see in Section 6, it is possible to show that
players communicating via a semantic translator
perform only boundedly worse (and sometimes bet-
ter!) than pairs of players with a common language.

5 Translation models

In this section, we build on the intuition that mes-
sages should be translated via their semantics to
define a concrete translation model—a procedure
for constructing a natural language ↔ neuralese
dictionary given agent and human interactions.

We understand the meaning of a message za to
be represented by the distribution p(xa|za, xb) it
induces over speaker states given listener context.
We can formalize this by defining the belief
distribution β for a message z and context xb as:

β(za, xb) = p(xa|za, xb) =
p(za|xa)p(xb|xa)∑
x′a
p(za|x′a)p(xb|x′a)

Here we have modeled the listener as performing
a single step of Bayesian inference, using the lis-
tener state and the message generation model (by
assumption shared between players) to compute
the posterior over speaker states. While in gen-
eral neither humans nor DCP agents compute ex-
plicit representations of this posterior, past work
has found that both humans and suitably-trained
neural networks can be modeled as Bayesian rea-
soners (Frank et al., 2009; Paige and Wood, 2016).

This provides a context-specific representation
of belief, but for messages z and z′ to have the same
semantics, they must induce the same belief over
all contexts in which they occur. In our probabilis-
tic formulation, this introduces an outer expectation
over contexts, providing a final measure q of the
quality of a translation from z to z′:

q(z, z′) = E
[
DKL(β(z,Xb) || β(z′, Xb)) | z, z′

]

=
∑

xa,xb

p(xa, xb|z, z′)DKL(β(z, xb) || β(z′, xb))

∝
∑

xa,xb

p(xa, xb) · p(z|xa) · p(z′|xa)
· DKL(β(z, xb) || β(z′, xb)); (1)

Algorithm 1 Translating messages

given: a phrase inventory L
function TRANSLATE(z)

return argminz′∈L q̂(z, z′)

function q̂(z, z′)
// sample contexts and distractors
xai, xbi ∼ p(Xa, Xb) for i = 1..n
x′ai ∼ p(Xa|xbi)
// compute context weights
w̃i ← p(z|xai) · p(z′|xai)
wi ← w̃i/

∑
j w̃j

// compute divergences
ki ←

∑
x∈{xa,x′a} p(z|x) log

p(z|x)
p(z′|x)

return
∑

iwiki

recalling that in this setting

DKL(β || β′) =
∑

xa

p(xa|z, xb) log
p(xa|z, xb)
p(xa|z′, xb)

∝
∑

xa

p(xa|xb)p(z|xa) log
p(z|xa)
p(z′|xa)

(2)

which is zero when the messages z and z′ give rise
to identical belief distributions and increases as
they grow more dissimilar. To translate, we would
like to compute tr(zr) = argminzh q(zr, zh) and
tr(zh) = argminzr q(zh, zr). Intuitively, Equa-
tion 1 says that we will measure the quality of a
proposed translation z 7→ z′ by asking the follow-
ing question: in contexts where z is likely to be
used, how frequently does z′ induce the same belief
about speaker states as z?

While this translation criterion directly encodes
the semantic notion of meaning described in Sec-
tion 4, it is doubly intractable: the KL divergence
and outer expectation involve a sum over all obser-
vations xa and xb respectively; these sums are not
in general possible to compute efficiently. To avoid
this, we approximate Equation 1 by sampling. We
draw a collection of samples (xa, xb) from the prior
over world states, and then generate for each sam-
ple a sequence of distractors (x′a, xb) from p(x

′
a|xb)

(we assume access to both of these distributions
from the problem representation). The KL term
in Equation 1 is computed over each true sample
and its distractors, which are then normalized and
averaged to compute the final score.

Sampling accounts for the outer p(xa, xb) in
Equation 1 and the inner p(xa|xb) in Equation 2.

236



a

b

xa z

xb

u
Figure 5: Simplified game representation used for analysis in
Section 6. A speaker agent sends a message to a listener agent,
which takes a single action and receives a reward.

The only quantities remaining are of the form
p(z|xa). In the case of neuralese, this distribu-
tion already is part of the definition of the agent
policy πr and can be reused directly. For natural
language, we use transcripts of human interactions
to fit a model that maps from world states to a dis-
tribution over frequent utterances as discussed in
Section 3. Details of these model implementations
are provided in Appendix B, and the full translation
procedure is given in Algorithm 1.

6 Belief and behavior

The translation criterion in the previous section
makes no reference to listener actions at all. The
shapes example in Section 4 shows that some
model performance might be lost under translation.
It is thus reasonable to ask whether this transla-
tion model of Section 5 can make any guarantees
about the effect of translation on behavior. In this
section we explore the relationship between belief-
preserving translations and the behaviors they pro-
duce, by examining the effect of belief accuracy
and strategy mismatch on the reward obtained by
cooperating agents.

To facilitate this analysis, we consider a sim-
plified family of communication games with the
structure depicted in Figure 5. These games can be
viewed as a subset of the family depicted in Fig-
ure 3; and consist of two steps: a listener makes
an observation xa and sends a single message z
to a speaker, which makes its own observation xb,
takes a single action u, and receives a reward. We
emphasize that the results in this section concern
the theoretical properties of idealized games, and
are presented to provide intuition about high-level
properties of our approach. Section 8 investigates
empirical behavior of this approach on real-world
tasks where these ideal conditions do not hold.

Our first result is that translations that minimize
semantic dissimilarity q cause the listener to take
near-optimal actions:2

2Proof is provided in Appendix A.

Proposition 1.
Semantic translations reward rational listeners.
Define a rational listener as one that chooses the
best action in expectation over the speaker’s state:

U(z, xb) = argmax
u

∑

xa

p(xa|xb, z)r(xa, xb, u)

for a reward function r ∈ [0, 1] that depends only
on the two observations and the action.3 Now let a
be a speaker of a language r, b be a listener of the
same language r, and b′ be a listener of a different
language h. Suppose that we wish for a and b′ to
interact via the translator tr : zr 7→ zh (so that
a produces a message zr, and b′ takes an action
U(zh = tr(zr), xb′)). If tr respects the semantics
of zr, then the bilingual pair a and b′ achieves only
boundedly worse reward than the monolingual pair
a and b. Specifically, if q(zr, zh) ≤ D, then

Er(Xa, Xb, U(tr(Z))

≥ Er(Xa, Xb, U(Z))−
√
2D (3)

So as discussed in Section 4, even by committing
to a semantic approach to meaning representation,
we have still succeeded in (approximately) captur-
ing the nice properties of the pragmatic approach.

Section 4 examined the consequences of a mis-
match between the set of primitives available in
two languages. In general we would like some
measure of our approach’s robustness to the lack of
an exact correspondence between two languages.
In the case of humans in particular we expect that
a variety of different strategies will be employed,
many of which will not correspond to the behavior
of the learned agent. It is natural to want some as-
surance that we can identify the DCP’s strategy as
long as some human strategy mirrors it. Our second
observation is that it is possible to exactly recover
a translation of a DCP strategy from a mixture of
humans playing different strategies:

Proposition 2.
Semantic translations find hidden correspondences.
Consider a fixed robot policy πr and a set of
human policies {πh1 , πh2 , . . . } (recalling from
Section 3 that each π is defined by distributions

3This notion of rationality is a fairly weak one: it permits
many suboptimal communication strategies, and requires only
that the listener do as well as possible given a fixed speaker—
a first-order optimality criterion likely to be satisfied by any
richly-parameterized model trained via gradient descent.

237



p(z |xa) and p(u |z , xb)). Suppose further that
the messages employed by these human strate-
gies are disjoint; that is, if phi(z |xa) > 0, then
phj (z |xa) = 0 for all j 6= i. Now suppose that
all q(zr , zh) = 0 for all messages in the support
of some phi(z |xa) and > 0 for all j 6= i. Then
every message zr is translated into a message pro-
duced by πhi , and messages from other strategies
are ignored.

This observation follows immediately from the
definition of q(zr, zh), but demonstrates one of
the key distinctions between our approach and a
conventional machine translation criterion. Maxi-
mizing p(zh|zr) will produce the natural language
message most often produced in contexts where
zr is observed, regardless of whether that message
is useful or informative. By contrast, minimizing
q(zh, zr) will find the zh that corresponds most
closely to zr even when zh is rarely used.

The disjointness condition, while seemingly
quite strong, in fact arises naturally in many
circumstances—for example, players in the driving
game reporting their spatial locations in absolute
vs. relative coordinates, or speakers in a color refer-
ence game (Figure 6) discriminating based on light-
ness vs. hue. It is also possible to relax the above
condition to require that strategies be only locally
disjoint (i.e. with the disjointness condition holding
for each fixed xa), in which case overlapping hu-
man strategies are allowed, and the recovered robot
strategy is a context-weighted mixture of these.

7 Evaluation

7.1 Tasks
In the remainder of the paper, we evaluate the em-
pirical behavior of our approach to translation. Our
evaluation considers two kinds of tasks: reference
games and navigation games. In a reference game
(e.g. Figure 6a), both players observe a pair of can-
didate referents. A speaker is assigned a target ref-
erent; it must communicate this target to a listener,
who then performs a choice action corresponding
to its belief about the true target. In this paper we
consider two variants on the reference game: a sim-
ple color-naming task, and a more complex task
involving natural images of birds. For examples
of human communication strategies for these tasks,
we obtain the XKCD color dataset (McMahan and
Stone, 2015; Monroe et al., 2016) and the Caltech
Birds dataset (Welinder et al., 2010) with accom-

(a) (b)

(c)

Figure 6: Tasks used to evaluate the translation model. (a–b)
Reference games: both players observe a pair of reference
candidates (colors or images); Player a is assigned a target
(marked with a star), which player b must guess based on
a message from a. (c) Driving game: each car attempts to
navigate to its goal (marked with a star). The cars cannot see
each other, and must communicate to avoid a collision.

panying natural language descriptions (Reed et al.,
2016). We use standard train / validation / test splits
for both of these datasets.

The final task we consider is the driving task
(Figure 6c) first discussed in the introduction. In
this task, two cars, invisible to each other, must
each navigate between randomly assigned start and
goal positions without colliding. This task takes
a number of steps to complete, and potentially in-
volves a much broader range of communication
strategies. To obtain human annotations for this
task, we recorded both actions and messages gener-
ated by pairs of human Amazon Mechanical Turk
workers playing the driving game with each other.
We collected close to 400 games, with a total of
more than 2000 messages exchanged, from which
we held out 100 game traces as a test set.

7.2 Metrics

A mechanism for understanding the behavior of
a learned model should allow a human user both
to correctly infer its beliefs and to successfully
interoperate with it; we accordingly report results
of both “belief” and “behavior” evaluations.

To support easy reproduction and comparison
(and in keeping with standard practice in machine

238



translation), we focus on developing automatic
measures of system performance. We use the avail-
able training data to develop simulated models of
human decisions; by first showing that these mod-
els track well with human judgments, we can be
confident that their use in evaluations will corre-
late with human understanding. We employ the
following two metrics:

Belief evaluation This evaluation focuses on the
denotational perspective in semantics that moti-
vated the initial development of our model. We
have successfully understood the semantics of a
message zr if, after translating zr 7→ zh, a human
listener can form a correct belief about the state
in which zr was produced. We construct a simple
state-guessing game where the listener is presented
with a translated message and two state observa-
tions, and must guess which state the speaker was
in when the message was emitted.

When translating from natural language to neu-
ralese, we use the learned agent model to directly
guess the hidden state. For neuralese to natural
language we must first construct a “model human
listener” to map from strings back to state repre-
sentations; we do this by using the training data to
fit a simple regression model that scores (state, sen-
tence) pairs using a bag-of-words sentence repre-
sentation. We find that our “model human” matches
the judgments of real humans 83% of the time on
the colors task, 77% of the time on the birds task,
and 77% of the time on the driving task. This gives
us confidence that the model human gives a reason-
ably accurate proxy for human interpretation.

Behavior evaluation This evaluation focuses on
the cooperative aspects of interpretability: we mea-
sure the extent to which learned models are able
to interoperate with each other by way of a transla-
tion layer. In the case of reference games, the goal
of this semantic evaluation is identical to the goal
of the game itself (to identify the hidden state of
the speaker), so we perform this additional prag-
matic evaluation only for the driving game. We
found that the most data-efficient and reliable way
to make use of human game traces was to construct
a “deaf” model human. The evaluation selects a
full game trace from a human player, and replays
both the human’s actions and messages exactly (dis-
regarding any incoming messages); the evaluation
measures the quality of the natural-language-to-
neuralese translator, and the extent to which the

(a)

as speaker
R H

as
lis

te
ne

r R 1.00
0.50 random
0.70 direct
0.73 belief (ours)

H*
0.50

0.830.72
0.86

(b)

as speaker
R H

as
lis

te
ne

r R 0.95
0.50 random
0.55 direct
0.60 belief (ours)

H*
0.50

0.770.57
0.75

Table 1: Evaluation results for reference games. (a) The colors
task. (b) The birds task. Whether the model human is in a
listener or speaker role, translation based on belief matching
outperforms both random and machine translation baselines.

learned agent model can accommodate a (real) hu-
man given translations of the human’s messages.

Baselines We compare our approach to two base-
lines: a random baseline that chooses a translation
of each input uniformly from messages observed
during training, and a direct baseline that directly
maximizes p(z′|z) (by analogy to a conventional
machine translation system). This is accomplished
by sampling from a DCP speaker in training states
labeled with natural language strings.

8 Results

In all below, “R” indicates a DCP agent, “H” in-
dicates a real human, and “H*” indicates a model
human player.

Reference games Results for the two reference
games are shown in Table 1. The end-to-end trained
model achieves nearly perfect accuracy in both

magenta,  hot,  rose,  violet,  purple

magenta,  hot,  violet,  rose,  purple

olive,  puke,  pea,  grey,  brown

pinkish,  grey,  dull,  pale,  light

Figure 7: Best-scoring translations generated for color task.

239



as speaker
R H

as
lis

te
ne

r R 0.85
0.50 random
0.45 direct
0.61 belief (ours)

H*
0.5

0.770.45
0.57

Table 2: Belief evaluation results for the driving game. Driving
states are challenging to identify based on messages alone (as
evidenced by the comparatively low scores obtained by single-
language pairs) . Translation based on belief achieves the best
overall performance in both directions.

R / R H / H R / H

1.93 / 0.71 — / 0.77
1.35 / 0.64 random
1.49 / 0.67 direct
1.54 / 0.67 belief (ours)

Table 3: Behavior evaluation results for the driving game.
Scores are presented in the form “reward / completion rate”.
While less accurate than either humans or DCPs with a shared
language, the models that employ a translation layer obtain
higher reward and a greater overall success rate than baselines.

cases, while a model trained to communicate in
natural language achieves somewhat lower perfor-
mance. Regardless of whether the speaker is a
DCP and the listener a model human or vice-versa,
translation based on the belief-matching criterion
in Section 5 achieves the best performance; indeed,
when translating neuralese color names to natural
language, the listener is able to achieve a slightly
higher score than it is natively. This suggests that
the automated agent has discovered a more effec-
tive strategy than the one demonstrated by humans
in the dataset, and that the effectiveness of this
strategy is preserved by translation. Example trans-
lations from the reference games are depicted in
Figure 2 and Figure 7.

Driving game Behavior evaluation of the driving
game is shown in Table 3, and belief evaluation is
shown in Table 2. Translation of messages in the
driving game is considerably more challenging than
in the reference games, and scores are uniformly
lower; however, a clear benefit from the belief-
matching model is still visible. Belief matching
leads to higher scores on the belief evaluation in
both directions, and allows agents to obtain a higher
reward on average (though task completion rates
remain roughly the same across all agents). Some
example translations of driving game messages are
shown in Figure 8.

at goal 

done 

left to top

going in intersection 

proceed 

going

you first 

following 

going down

Figure 8: Best-scoring translations generated for driving task
generated from the given speaker state.

9 Conclusion

We have investigated the problem of interpreting
message vectors from deep networks by translat-
ing them. After introducing a translation criterion
based on matching listener beliefs about speaker
states, we presented both theoretical and empirical
evidence that this criterion outperforms a conven-
tional machine translation approach at recovering
the content of message vectors and facilitating col-
laboration between humans and learned agents.

While our evaluation has focused on under-
standing the behavior of deep communicating poli-
cies, the framework proposed in this paper could
be much more generally applied. Any encoder–
decoder model (Sutskever et al., 2014) can be
thought of as a kind of communication game played
between the encoder and the decoder, so we can
analogously imagine computing and translating
“beliefs” induced by the encoding to explain what
features of the input are being transmitted. The cur-
rent work has focused on learning a purely categor-
ical model of the translation process, supported by
an unstructured inventory of translation candidates,
and future work could explore the compositional
structure of messages, and attempt to synthesize
novel natural language or neuralese messages from
scratch. More broadly, the work here shows that
the denotational perspective from formal seman-
tics provides a framework for precisely framing the
demands of interpretable machine learning (Wil-
son et al., 2016), and particularly for ensuring that
human users without prior exposure to a learned
model are able to interoperate with it, predict its
behavior, and diagnose its errors.

240



Acknowledgments

JA is supported by a Facebook Graduate Fellowship
and a Berkeley AI / Huawei Fellowship. We are
grateful to Lisa Anne Hendricks for assistance with
the Caltech Birds dataset.

References
Jacob Andreas and Dan Klein. 2016. Reasoning about

pragmatics with neural listeners and speakers. In
Proceedings of the Conference on Empirical Meth-
ods in Natural Language Processing.

Daniel S Bernstein, Robert Givan, Neil Immerman,
and Shlomo Zilberstein. 2002. The complexity of
decentralized control of Markov decision processes.
Mathematics of operations research 27(4):819–840.

Kyunghyun Cho, Bart van Merriënboer, Dzmitry Bah-
danau, and Yoshua Bengio. 2014. On the properties
of neural machine translation: Encoder-decoder ap-
proaches. arXiv preprint arXiv:1409.1259 .

Jilles Steeve Dibangoye, Christopher Amato, Olivier
Buffet, and François Charpillet. 2016. Optimally
solving Dec-POMDPs as continuous-state MDPs.
Journal of Artificial Intelligence Research 55:443–
497.

Anca Dragan and Siddhartha Srinivasa. 2013. Gener-
ating legible motion. In Robotics: Science and Sys-
tems.

Jakob Foerster, Yannis M Assael, Nando de Freitas,
and Shimon Whiteson. 2016. Learning to commu-
nicate with deep multi-agent reinforcement learning.
In Advances in Neural Information Processing Sys-
tems. pages 2137–2145.

Michael C Frank, Noah D Goodman, Peter Lai, and
Joshua B Tenenbaum. 2009. Informative communi-
cation in word production and word learning. In Pro-
ceedings of the 31st annual conference of the cogni-
tive science society. pages 1228–1233.

Yang Gao, Oscar Beijbom, Ning Zhang, and Trevor
Darrell. 2016. Compact bilinear pooling. In Pro-
ceedings of the IEEE Conference on Computer Vi-
sion and Pattern Recognition. pages 317–326.

Jon Gauthier and Igor Mordatch. 2016. A paradigm for
situated and goal-driven language learning. arXiv
preprint arXiv:1610.03585 .

Frank Guerin and Jeremy Pitt. 2001. Denotational se-
mantics for agent communication language. In Pro-
ceedings of the fifth international conference on Au-
tonomous agents. ACM, pages 497–504.

Matthew Hausknecht and Peter Stone. 2015. Deep
recurrent q-learning for partially observable mdps.
arXiv preprint arXiv:1507.06527 .

Lisa Anne Hendricks, Zeynep Akata, Marcus
Rohrbach, Jeff Donahue, Bernt Schiele, and Trevor
Darrell. 2016. Generating visual explanations. In
European Conference on Computer Vision. Springer,
pages 3–19.

Sahar Kazemzadeh, Vicente Ordonez, Mark Matten,
and Tamara L Berg. 2014. ReferItGame: Referring
to objects in photographs of natural scenes. In Pro-
ceedings of the Conference on Empirical Methods in
Natural Language Processing. pages 787–798.

Diederik Kingma and Jimmy Ba. 2014. Adam: A
method for stochastic optimization. arXiv preprint
arXiv:1412.6980 .

Angeliki Lazaridou, Alexander Peysakhovich, and
Marco Baroni. 2016a. Multi-agent cooperation and
the emergence of (natural) language. arXiv preprint
arXiv:1612.07182 .

Angeliki Lazaridou, Nghia The Pham, and
Marco Baroni. 2016b. Towards multi-agent
communication-based language learning. arXiv
preprint arXiv:1605.07133 .

Brian McMahan and Matthew Stone. 2015. A
Bayesian model of grounded color semantics. Trans-
actions of the Association for Computational Lin-
guistics 3:103–115.

Will Monroe, Noah D Goodman, and Christopher Potts.
2016. Learning to generate compositional color de-
scriptions. arXiv preprint arXiv:1606.03821 .

Brooks Paige and Frank Wood. 2016. Inference net-
works for sequential monte carlo in graphical mod-
els. volume 48.

Panupong Pasupat and Percy Liang. 2016. Inferring
logical forms from denotations. arXiv preprint
arXiv:1606.06900 .

Scott Reed, Zeynep Akata, Honglak Lee, and Bernt
Schiele. 2016. Learning deep representations of
fine-grained visual descriptions. In Proceedings of
the IEEE Conference on Computer Vision and Pat-
tern Recognition. pages 49–58.

Marco Tulio Ribeiro, Sameer Singh, and Carlos
Guestrin. 2016. Why should I trust you?: Explain-
ing the predictions of any classifier. In Proceedings
of the 22nd ACM SIGKDD International Conference
on Knowledge Discovery and Data Mining. ACM,
pages 1135–1144.

Maayan Roth, Reid Simmons, and Manuela Veloso.
2005. Reasoning about joint beliefs for execution-
time communication decisions. In Proceedings
of the fourth international joint conference on Au-
tonomous agents and multiagent systems. ACM,
pages 786–793.

Hendrik Strobelt, Sebastian Gehrmann, Bernd Huber,
Hanspeter Pfister, and Alexander M Rush. 2016. Vi-
sual analysis of hidden state dynamics in recurrent
neural networks. arXiv preprint arXiv:1606.07461 .

241



Sainbayar Sukhbaatar, Rob Fergus, et al. 2016. Learn-
ing multiagent communication with backpropaga-
tion. In Advances in Neural Information Processing
Systems. pages 2244–2252.

Ilya Sutskever, Oriol Vinyals, and Quoc VV Le. 2014.
Sequence to sequence learning with neural networks.
In Advances in Neural Information Processing Sys-
tems. pages 3104–3112.

Ramakrishna Vedantam, Samy Bengio, Kevin Murphy,
Devi Parikh, and Gal Chechik. 2017. Context-aware
captions from context-agnostic supervision. arXiv
preprint arXiv:1701.02870 .

Adam Vogel, Max Bodoia, Christopher Potts, and
Daniel Jurafsky. 2013a. Emergence of Gricean max-
ims from multi-agent decision theory. In Proceed-
ings of the Human Language Technology Confer-
ence of the North American Chapter of the Asso-
ciation for Computational Linguistics. pages 1072–
1081.

Adam Vogel, Christopher Potts, and Dan Jurafsky.
2013b. Implicatures and nested beliefs in approx-
imate Decentralized-POMDPs. In ACL (2). pages
74–80.

P. Welinder, S. Branson, T. Mita, C. Wah, F. Schroff,
S. Belongie, and P. Perona. 2010. Caltech-UCSD
Birds 200. Technical Report CNS-TR-2010-001,
California Institute of Technology.

Andrew Gordon Wilson, Been Kim, and William Her-
lands. 2016. Proceedings of nips 2016 workshop on
interpretable machine learning for complex systems.
arXiv preprint arXiv:1611.09139 .

Licheng Yu, Hao Tan, Mohit Bansal, and Tamara L
Berg. 2016. A joint speaker-listener-reinforcer
model for referring expressions. arXiv preprint
arXiv:1612.09542 .

Matthew D Zeiler and Rob Fergus. 2014. Visualizing
and understanding convolutional networks. In Euro-
pean conference on computer vision. Springer, pages
818–833.

242


	Translating Neuralese

