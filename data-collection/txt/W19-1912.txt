



















































Medical Entity Linking using Triplet Network


Proceedings of the 2nd Clinical Natural Language Processing Workshop, pages 95–100
Minneapolis, Minnesota, June 7, 2019. c©2019 Association for Computational Linguistics

95

Medical Entity Linking using Triplet Network

1Ishani Mondal 1Sukannya Purkayastha 1Sudeshna Sarkar 1Pawan Goyal
2Jitesh K Pillai 2Amitava Bhattacharyya 2Mahanandeeshwar Gattu

1Department of Computer Science and Engineering, IIT Kharagpur
2Excelra Knowledge Solutions Pvt Ltd, Hyderabad, India

Abstract

Entity linking (or Normalization) is an essen-
tial task in text mining that maps the entity
mentions in the medical text to standard en-
tities in a given Knowledge Base (KB). This
task is of great importance in the medical do-
main. It can also be used for merging different
medical and clinical ontologies. In this paper,
we center around the problem of disease link-
ing or normalization. This task is executed in
two phases: candidate generation and candi-
date scoring. In this paper, we present an ap-
proach to rank the candidate Knowledge Base
entries based on their similarity with disease
mention. We make use of the Triplet Net-
work for candidate ranking. While the existing
methods have used carefully generated sieves
and external resources for candidate genera-
tion, we introduce a robust and portable can-
didate generation scheme that does not make
use of the hand-crafted rules. Experimental
results on the standard benchmark NCBI dis-
ease dataset demonstrate that our system out-
performs the prior methods by a significant
margin.

1 Introduction

A disease is an abnormal medical condition that
poses a negative impact on the organisms and en-
abling access to disease information is the goal
of various information extraction as well as text
mining tasks. The task of disease normalization
consists of assigning a unique concept identifier
to the disease names occurring in the clinical text.
However, this task is challenging as the diseases
mentioned in the text may display morphological
or orthographical variations, may utilize different
word orderings or equivalent words. Consider the
following examples:

Example 1: “..characteristics of the disor-
der include a short trunk and extremities...”
Source : (PMID:7874117)
Example 2: “Renal amyloidosis, prevented by
colchicine, is the most severe complication of
FMF ...” Source : (PMID:10364520)

In Example 1, the disease mention short trunk
and extremities should be mapped to a candidate
Knowledge Base entry(D006130) containing syn-
onyms like Growth Disorder. In Example 2, Re-
nal amyloidosis should be assigned to Knowledge
Base ID (C538249) which has synonyms such as,
Amyloidosis 8.

Based on our studies and analysis of the med-
ical literature, it has been observed that the
same disease name may occur in multiple variant
forms such as. synonyms replacement (e.g.“lung
cancer”, “lung carcinoma”), spelling variation
(“Acetolysis”, “acetolisis”), a short description
modifier precedes the disease name (e.g. “mas-
sive heart attack”), different word orderings (eg.
“alpha-galactosidase deficiency”, “deficiency of
alpha-galactosidase”).

In this paper, we have formulated the task of
learning mention-candidate pair similarity using
Triplet Networks (Hoffer and Ailon, 2015). Fur-
thermore, we have explored in-domain word1 and
subword embeddings (Bojanowski et al., 2017) as
input representations. We find that sub-word infor-
mation boosts up the performance due to gained
information for out-of-vocabulary terms and word
compositionality of the disease mentions.

The primary contributions of this paper are
three-fold: 1) By identifying positive and neg-
ative candidates concerning a disease mention,
we optimize the Triplet Network with a loss
function that influences the relative distance con-
straint 2) We have explored the capability of in-

1http://evexdb.org/pmresources/vec-space-models/



96

Dataset Abstracts Total Unique
Training set 692 5932 1538
Test set 100 960 427
Total 792 6892 1965

Table 1: NCBI Disease Corpus Statistics

domain sub-word level information2 in solving the
task of disease normalization. 3) Unlike exist-
ing systems (D’Souza and Ng, 2015), (Li et al.,
2017), we present a robust and portable candi-
date generation approach without making use of
external resources or hand-engineered sieves to
deal with morphological variations. Our system
achieves state-of-the-art performance on NCBI
disease dataset (Dogan et al., 2014)

2 Dataset

The NCBI disease corpus (Dogan et al., 2014)
contains 792 Pubmed abstracts with disorder con-
cepts manually annotated. In this dataset, disorder
mentions in each abstract are manually annotated
with the identifier of the concept in the reference
ontology to which it refers. It uses MEDIC lexi-
con (Davis et al., 2012) as the reference ontology.
(See Table 1 for dataset statistics)

3 Methodology

The dataset consists of a certain number of abbre-
viations, in order to identify these cases, we have
considered the mentions composed of all upper-
case letters as abbreviations. We find the disease
mentions immediately preceding the abbreviated
terms and substitute all the occurrences of the ab-
breviated words in that document with the corre-
sponding expanded disease mentions. Our system
primarily consists of two modules: 1) Candidate
generation: (See section 3.1) Generate potential
candidates from the Knowledge Base correspond-
ing to a disease mention. 2) Candidate ranking:
(See section 3.2) Rank those potential candidates
corresponding to a disease mention.

3.1 Candidate generation

In this section, we discuss the algorithm which
generates the potential candidates to which the dis-
ease mentions might be referring. In this study,
the Knowledge Base entries were sampled from

2https://github.com/ncbi-nlp/BioSentVec.git

the entire MEDIC Lexicon, but not limited to only
annotations in the NCBI Disease Corpus.

For a given disease mention, the candidate gen-
eration algorithm generates candidates from the
Knowledge Base entries. Suppose, the Knowledge
Base consists of k entries, each having a certain
number of synonyms. Each multi-word synonym
represented by the sum of its word embeddings.
For a given mention m consisting of l words rep-
resented by {m1,m2, . . . ,ml}, we represent m as
the sum of its word embeddings. The steps for the
candidate generation algorithm are as follows:

• Step 1: Candidate Set 1, {C1} : Calculate
the cosine similarity between vector repre-
sentation of each synonym (candidate) of the
KBIDs and the mention. Identify the top k1
ids whose candidates have cosine similarity
greater than or equal to threshold t1.

• Step 2: Candidate Set 2, {C2}: Calculate the
Jaccard overlap of the mention and the can-
didates of each KBID. Identify the top k2 ids
having Jaccard overlap score greater than or
equal to threshold t2.

Note: min (| C1 |,| C2 |)≤| C1 ∪ C2 | ≤(k1+k2)
In our experiments, we choose t1 = 0.7, t2 = 0.1,
k1 = 3 and k2 = 7.

We provide examples of candidates generated
from our proposed algorithm below.

Mention: “bacteremic infections due to Neis-
seria”
Candidate Set 1, {C1} = {“bacterial neisseria
infections”}
Candidate Set 2, {C2} = {“bacterial neisseria
infections” , “DNA-virus infections” , “Screw-
Worm Infections” }

3.2 Candidate Ranking

Assume that there are n candidates represented
by {c1, c2, . . . , cn} for an entity mention m, we
use a Triplet Network which has proven to per-
form well in many Computer Vision (Hoffer and
Ailon, 2015) as well as Natural Language Process-
ing tasks (Clark and Manning, 2016) . As such
given a triplet, the idea is to leverage the notion
of reducing the distance between the mention and
its positive candidate while increasing the distance
between the mention and its negative candidate.



97

Figure 1: Pictorial Representation of the training Data Generation Process

3.2.1 Triplet data generation
In order to learn better semantic representations
between a disease mention and its correspond-
ing candidates, we have generated training data in
the form of triplets consisting of disease mention
m, positive candidate qp, negative candidate qn.
The triplet is represented as (qp, m, qni) where i
∈ {1 . . . [|k1 ∪k2| −1]}.

An example of the triplet data is given below:

Disease Mention: “bacteremic infections due
to Neisseria”
Positive Candidate: “bacterial neisseria infec-
tions”
Negative Candidates: “DNA-virus infec-
tions”, “Screw-Worm Infections”.
The triplets are as follows:

• (“bacterial neisseria infections”, “bac-
teremic infections due to Neisseria”,
“DNA-virus infections”)

• (“bacterial neisseria infections”, “bac-
teremic infections due to Neisseria”,
“Screw-Worm Infections”)

3.2.2 Model Architecture
The Triplet Network architecture as proposed by
(Hoffer and Ailon, 2015) has been adopted for the
task of entity normalization. To train the model,
each triplet consisting of mentions and its candi-
dates are fed into the parameter-shared network
(Conv), as a sequence of word embeddings. For
a triplet, (qp, m, qni) the layer outputs their rep-
resentations Conv(qp), Conv(m) and Conv(qni)
respectively. Our objective is to make the repre-

Figure 2: The word and sub-word embeddings of triplet
(‘bacterial neisseria infections’, ‘bacteremic infections
due to Neisseria’, ’Screw-Worm Infections’) are fed as
batches into the Triplet Network.

sentations of m and qp closer than the represen-
tations of m and qni . Thus the next layer uses a
distance function, denoted by dis, to compute the
distances as follows:

dp = dis(Conv(m), Conv(qp))

dni = dis(Conv(m), Conv(qni))

Here dp specifies the distance between target
disease mention m and qp while dni specifies the
distance between target disease mention m and
qni . The triplet loss function (L) used for achiev-
ing this goal has been formulated as follows:

L = max (dp − dni + α, 0)

Another variable α, a hyperparameter is added
to the loss equation which defines how far away



98

the dissimilarities should be. Thereafter, by us-
ing this loss function, we calculate the gradients
and update the parameters of the network based on
these gradient values. For training the network, we
take mention m and randomly sample qp and qni
and compute their loss function and update their
gradients.

We use 200-dimensional word2vec (Mikolov
et al., 2013) embeddings trained on Wikipedia and
Pubmed PMC-Corpus (Pyysalo et al., 2013) as in-
put to Conv. To deal with the huge number of
out-of- vocabulary terms in the medical domain,
we have used the fastText based sub-word em-
beddings (Galea et al., 2018). fastText (Bo-
janowski et al., 2017) has been applied on PubMed
and MIMIC-III (Johnson et al., 2016) to generate
200- dimensional word embeddings, the window
size being 20, learning rate 0.05, sampling thresh-
old 1e-4, and negative examples 10 (yijia zhang
et al., 2018).

3.2.3 Training Details
Conv is composed of one convolutional and max-
pooling layer. ReLU non-linearity (Maas, 2013)
is applied between two consecutive layers. The
final embedding of Conv is a fixed-length(128)
vector. For dis and the loss function we use the
L2 distance (Danielsson, 1980). The triplet loss
has been applied. For training we use Adam Opti-
mizer (Kingma and Ba, 2015) with an initial learn-
ing rate of 0.001. Training has been done for
50 epochs, and early stopping has been employed
on the basis of the accuracy of the validation set.
After hyperparameter tuning, several experiments
have been performed, and the results on the best
hyperparameter settings have been reported.

3.2.4 Evaluation
After the model has been trained, we evaluate the
rank of each of the disease mentions in the test set.
For each of the disease mention m in the test set,
we run the candidate generation algorithm to find
out the maximum cosine similar candidates for the
potential KnowledgeBaseIDs. The positive candi-
date is labelled as 1 while the rest has been labelled
as 0. During the process of evaluation, we calcu-
late the similarity score between the disease men-
tion and its candidates. The similarity scores are
then sorted in descending manner in order to rank
the candidates based on its similarity. We choose
the candidate with the maximum similarity score
for each of the disease mentions.

Model Name Accuracy
(D’Souza and Ng, 2015) 84.65
(Li et al., 2017) 86.10
Triplet CNN + static word2vec 86.09
Triplet CNN + dynamic word2vec 87.85
Triplet CNN + subword 89.65
Triplet CNN + subword + abb 90.01

Table 2: The table shows the accuracy of our system in
comparison with the baseline systems.

We choose the evaluation measure as accuracy.
Since, the highest similar candidate is of primary
interest in the task of entity linking, so we choose
the top-K ( Where K = 1).
TP = It signifies that the highest ranked candi-

date for disease mention m is the actual referent
KnowledgebaseID.
FP = It signifies that the highest ranked candi-

date for disease mention m is not the actual refer-
ent KnowledgebaseID.

Accuracy =
TP

TP + FP

4 Results

We report accuracy for our system in finding the
correct Knowledge Base ID corresponding to a
disease mention in the text. Table 2 shows that
in comparison with the existing baseline systems,
Subword information as input to the Triplet Net-
work and abbreviation expansion from the doc-
ument context (Triplet CNN+subword+abb) per-
forms the best. From the feature ablation, it is
clear that the in-domain word embeddings((Triplet
CNN + dynamic word2vec) and (Triplet CNN +
static word2vec)) are essential for capturing better
semantic representations.

5 Analysis

In this section, we throw some light on both the
merits and demerits of the proposed system with
respect to the baseline models.

5.1 Merit Analysis

We compare our results with other rule-based and
neural network based methods known to perform
well on this standard dataset. To gain more in-
sights into our proposed model, in particular, the
importance of the domain-specific word and sub-
word representation to capture the semantic and



99

syntactic similarity using Triplet Network, we se-
lect some examples from the labeled test set. In
figure 2, two different cases have been shown
which demonstrate the performance gap between
our and the existing baseline systems.

In Example 1, the disease mention “inherited
neurodegeneration” was not mapped with “here-
dodegenerative disorders” ( D020271) by the ex-
isting methods, because of their incapability to
capture the semantic similarity. In contrast to this,
our system obtains additional semantic and syn-
tactic information from the domain-specific sub-
word embeddings and thereby maps to the correct
concept ID.

In Example 2, the abbreviation “AS” is poly-
semous in nature as it can either be mapped to
the concepts like “Angelman Syndrome” (PMID
: 9585605 ) and “Ankolysing Spondylitis” ( PMID
: 9336417). Due to the lack of contextual infor-
mation in the existing models, they were not able
to handle the polysemous nature of the abbrevia-
tions; but abbreviation expansion from the docu-
ment level context in our system handles this sce-
nario.

Figure 3: The NER tags as input are shown in pur-
ple, the gold standard conceptID is shown in green,
the predictions from the baseline systems are shown in
red, whereas the prediction from the proposed system
is shown in blue.

5.2 Demerit Analysis

The error types incurred by the proposed system
have been explained in detail as follows:

1) Ambiguous distribution of importance to
the disease name: The system fails to under-
stand which part of disease mention to provide
more attention while performing normalization.
Suppose the disease mention is “colorectal ade-
noma”, during normalization, the system mistak-

enly normalizes the disease to the concept ID pre-
dominated by “colorectal”. Automatic identifica-
tion of such semantic attention is challenging and
deserves a significant spot in the future research.

2) Incorrect mapping of certain ambiguous
disease names: Suppose the disease mention dys-
morphic features in “..loss of MAGEL2 may be
critical to abnormalities in brain development and
dysmorphic features in individuals with PWS..” (
PMID: 10915770 ) has been mapped to D057215
whereas the same disease mention in “..She had
minor dysmorphic features consistent with those
of..” ( PMID: 8071957 ) has been assigned to
D000013. Since, in these two examples, the dis-
ease mention in these two examples have been as-
signed as ”diseaseClass” and ”Modifier” features
respectively. It happens due to different NER fea-
tures of the mention annotated in the dataset. But
incorporating this NER feature in our proposed
model unnecessarily generates huge number of
false positives.

6 Conclusion

In this paper, we have formulated the task of en-
tity linking as a candidate ranking approach. Us-
ing a Triplet Network, we learn high-quality rep-
resentations of candidates, tailored to reveal rela-
tive distances between the disease mention and its
positive and negative candidates. Furthermore, we
take a step towards eliminating the need to gener-
ate candidates based on hand-crafted rules and ex-
ternal knowledge resources. Though our method
outperforms the existing systems by a strong mar-
gin, there is a scope for improvement in terms of
attention-based disease similarity (viz, “Neisseric
infections” imply the importance of “Neisseric”
during its similarity computation with the “bac-
terial neisseria infections”). An intriguing course
for future work is to further explore the robustness
and scalability of this approach to other clinical
datasets for entity normalization.

Acknowledgments

This work has been supported by the project “Ef-
fective Drug Repurposing through literature and
patent mining, data integration and development
of systems pharmacology platform” sponsored by
MHRD, India and Excelra Knowledge Solutions,
Hyderabad. Besides, the authors would like to
thank the anonymous reviewers for their valuable
comments and feedback.



100

References
Piotr Bojanowski, Edouard Grave, Armand Joulin, and

Tomas Mikolov. 2017. Enriching word vectors with
subword information. Transactions of the Associa-
tion for Computational Linguistics, 5:135–146.

Kevin Clark and Christopher D. Manning. 2016. Deep
reinforcement learning for mention-ranking coref-
erence models. In Proceedings of the 2016 Con-
ference on Empirical Methods in Natural Language
Processing, pages 2256–2262. Association for Com-
putational Linguistics.

Per-Erik Danielsson. 1980. Euclidean distance map-
ping. Computer Graphics and Image Processing,
14(3):227 – 248.

Allan Peter Davis, Thomas C. Wiegers, Michael C.
Rosenstein, and Carolyn J. Mattingly. 2012. Medic:
a practical disease vocabulary used at the compara-
tive toxicogenomics database. Database (Oxford),
2012:bar065–bar065. 22434833[pmid].

Rezarta Islamaj Dogan, Robert Leaman, and Zhiyong
Lu. 2014. Ncbi disease corpus: a resource for dis-
ease name recognition and concept normalization. J
Biomed Inform, 47:1–10. 24393765[pmid].

Jennifer D’Souza and Vincent Ng. 2015. Sieve-based
entity linking for the biomedical domain. In Pro-
ceedings of the 53rd Annual Meeting of the Associ-
ation for Computational Linguistics and the 7th In-
ternational Joint Conference on Natural Language
Processing (Volume 2: Short Papers), pages 297–
302.

Dieter Galea, Ivan Laponogov, and Kirill A. Veselkov.
2018. Sub-word information in pre-trained biomed-
ical word representations: evaluation and hyper-
parameter optimization. In BioNLP.

Elad Hoffer and Nir Ailon. 2015. Deep metric learning
using triplet network. In SIMBAD.

Alistair E. W. Johnson, Tom J. Pollard, Lu Shen, Li-
wei H. Lehman, Mengling Feng, Mohammad Ghas-
semi, Benjamin Moody, Peter Szolovits, Leo An-
thony Celi, and Roger G. Mark. 2016. Mimic-iii,
a freely accessible critical care database. Scientific
Data, 3:160035 EP –. Data Descriptor.

Diederik P. Kingma and Jimmy Ba. 2015. Adam:
A method for stochastic optimization. CoRR,
abs/1412.6980.

Haodi Li, Qingcai Chen, Buzhou Tang, Xiaolong
Wang, Hua Xu, Baohua Wang, and Dong Huang.
2017. Cnn-based ranking for biomedical entity
normalization. BMC Bioinformatics, 18(Suppl
11):385–385. 28984180[pmid].

Andrew L. Maas. 2013. Rectifier nonlinearities im-
prove neural network acoustic models.

Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Cor-
rado, and Jeff Dean. 2013. Distributed representa-
tions of words and phrases and their composition-
ality. In C. J. C. Burges, L. Bottou, M. Welling,
Z. Ghahramani, and K. Q. Weinberger, editors, Ad-
vances in Neural Information Processing Systems
26, pages 3111–3119. Curran Associates, Inc.

Sampo Pyysalo, F Ginter, Hans Moen, T Salakoski, and
Sophia Ananiadou. 2013. Distributional semantics
resources for biomedical text processing. Proceed-
ings of Languages in Biology and Medicine.

yijia zhang, qingyu chen, zhihao yang, hongfei lin,
and Zhiyong Lu. 2018. BioWordVec: Improving
Biomedical Word Embeddings with Subword Infor-
mation and MeSH Ontology.

https://doi.org/10.18653/v1/D16-1245
https://doi.org/10.18653/v1/D16-1245
https://doi.org/10.18653/v1/D16-1245
https://doi.org/https://doi.org/10.1016/0146-664X(80)90054-4
https://doi.org/https://doi.org/10.1016/0146-664X(80)90054-4
https://doi.org/10.1093/database/bar065
https://doi.org/10.1093/database/bar065
https://doi.org/10.1093/database/bar065
https://doi.org/10.1016/j.jbi.2013.12.006
https://doi.org/10.1016/j.jbi.2013.12.006
https://doi.org/10.1038/sdata.2016.35
https://doi.org/10.1038/sdata.2016.35
https://doi.org/10.1186/s12859-017-1805-7
https://doi.org/10.1186/s12859-017-1805-7
http://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf
http://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf
http://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf
https://doi.org/10.6084/m9.figshare.6882647.v2
https://doi.org/10.6084/m9.figshare.6882647.v2
https://doi.org/10.6084/m9.figshare.6882647.v2

