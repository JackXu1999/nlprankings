




































Refining Pretrained Word Embeddings Using Layer-wise Relevance Propagation


Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 4840–4846
Brussels, Belgium, October 31 - November 4, 2018. c©2018 Association for Computational Linguistics

4840

Refining Pretrained Word Embeddings
Using Layer-wise Relevance Propagation

Akira Utsumi
Department of Informatics & Artificial Intelligence eXploration Research Center

The University of Electro-Communications, Chofu, Tokyo 182-8585, Japan
utsumi@uec.ac.jp

Abstract
In this paper, we propose a simple method
for refining pretrained word embeddings us-
ing layer-wise relevance propagation. Given
a target semantic representation one would
like word vectors to reflect, our method first
trains the mapping between the original word
vectors and the target representation using a
neural network. Estimated target values are
then propagated backward toward word vec-
tors, and a relevance score is computed for
each dimension of word vectors. Finally, the
relevance score vectors are used to refine the
original word vectors so that they are projected
into the subspace that reflects the information
relevant to the target representation. The eval-
uation experiment using binary classification
of word pairs demonstrates that the refined
vectors by our method achieve the higher per-
formance than the original vectors.

1 Introduction

The recent success of neural NLP is partially but
largely due to the development of word embedding
techniques (Goldberg, 2017). Although a consid-
erable number of studies have been made on train-
ing word embeddings from distributional informa-
tion of language (Mikolov et al., 2013; Penning-
ton et al., 2014; Bojanowski et al., 2017; Nickel
and Kiela, 2017), one recent research trend is to
refine or fine-tune pretrained word embeddings.
One promising approach is the use of other in-
formation such as multimodal information (Bruni
et al., 2014; Kiela et al., 2014; Kiela and Clark,
2015; Kiela et al., 2015a; Silberer et al., 2017) and
language resources (Faruqui et al., 2015; Faruqui
and Dyer, 2015; Kiela et al., 2015b; Rothe and
Schütze, 2017; Yu and Dredze, 2014). Other
refinement methods include task-specific embed-
dings (Bolukbasi et al., 2016; Yu et al., 2017) and
the selective use of multiple embeddings (Bolle-
gala et al., 2017; Kiela et al., 2018).

In this paper, we propose a different approach to
refining pretrained word embeddings so that word
vectors reflect the information relevant for a spe-
cific knowledge. Our method utilizes layer-wise
relevance propagation (Bach et al., 2015; Samek
et al., 2017), which has been proposed as a general
framework for decomposing predictions of mod-
ern AI systems, in particular deep learning sys-
tems. The basic idea of layer-wise relevance prop-
agation is to quantitatively measure the contribu-
tion of each fragment of an input vector (e.g., a
single pixel of an image) to the prediction as a rel-
evance score. Using relevance scores, our method
projects word vectors into the subspace that better
reflects the target knowledge. The assumption un-
derlying our approach is that the information for
any given target knowledge is contained in pre-
trained word embeddings. Our method attempts
to make the best use of the information contained
in word vectors by estimating the importance in
reflecting a target knowledge.

To the best of our knowledge, this paper is the
first to employ the technique of layer-wise rele-
vance propagation for refining word embeddings.
Our method can be applied to word vectors x
trained by any word embedding method. This im-
plies that our method does not compete with other
refinement methods, but they are complementary;
it can be used for word vectors refined by other
methods. In addition, our method can refine word
vectors for any target knowledge y, from a single
binary value to a structured representation, as long
as a function y = f(x) can be learned.

2 Method for Refining Word Vectors

Our method comprises the following three steps:
(1) it trains a prediction function from a pretrained
word vector to a target representation; (2) com-
putes a relevance score for each dimension of the



4841

word vector; and (3) projects word vectors into the
subspace using the relevance scores. In this sec-
tion, these three steps are explained in detail.

2.1 Training the Prediction Function

Given pairs of an input word vector x(i) to be re-
fined and a target knowledge representation y(i)

for a word w(i), the proposed method trains a func-
tion y(i) = f(x(i)). In this paper, we use a neural
network as a learning method, but other learning
methods such as linear transformation and SVM
can also be used. Note that a scalar value or a class
label can be used as a target representation y(i).

2.2 Computing Relevance Scores

This step derives an explanation of the predic-
tion in terms of input variables, namely the impor-
tance of each dimension of a word vector x(i) for
the prediction ŷ(i) = f(x(i)). In layer-wise rele-
vance propagation, the score of the correct predic-
tion ŷ(i)j is redistributed backward using relevance
propagation rules. By repeatedly applying prop-
agation rules, it assigns a relevance score r(i,j)k to
each dimension x(i)k of a word vector x

(i). As a re-
sult, a relevance score vector r(i,j) is obtained for
each word vector x(i) and target dimension y(i)j .

Among a number of propagation rules (Bach
et al., 2015), we use the “alpha-beta” rule for mul-
tilayer neural networks. The relevance score R(l)i
of the i-th unit u(l)i in the l-th layer is a function of
upper-layer relevances R(l+1)j defined by:

R
(l,l+1)
i←j = R

(l+1)
j ·

(
α

z+ij∑
i z

+
ij

+ β
z−ij∑
i z
−
ij

)
(1)

R
(l)
i =

∑
j

R
(l,l+1)
i←j (2)

z
(l,l+1)
ij = x

(l)
i w

(l,l+1)
ij (3)

where x(l)i is an activation of the unit u
(l)
i , w

(l,l+1)
ij

is a weight connecting u(l)i to u
(l+1)
j , and z

+
ij and

z−ij denote the positive and negative part of z
(l,l+1)
ij .

As a result, relevance scores r(i,j)k of the word vec-
tor x(i) and the target dimension y(i)j are obtained

as relevance scores R(1)k of the input layer. The
parameters α and β denote the importance of pos-
itive and negative evidence for predicting a tar-
get representations and should be chosen such that
α + β = 1. In this paper, we assume that posi-

tive and negative evidence equally contributes to
the prediction and thus set α = β = 0.5.

2.3 Projecting Word Vectors into a Subspace
The basic idea of projection is that n-dimensional
word vectors are projected into m-dimensional
vectors whose relevance scores are more than or
equal to a threshold θR.

First, for a target dimension j of y, relevance
score vectors are averaged over words relevant to
the target dimension as follows:

r(j) = g2

(∑
wi∈Vj g1(r

(i,j))

|Vj |

)
(4)

{g1(x)}i =

{
xi (xi ≥ θR1)
0 (otherwise)

{g2(x)}i =

{
xi

maxi xi
( ximaxi xi ≥ θR2)

0 (otherwise)

where Vj is a set of words w(i) such that ŷ
(i)
j ≥ θT .

The functions g1 and g2 are used for downplay-
ing irrelevant dimensions. For example, the tar-
get knowledge is the property of Visually dark and
Vvisually dark is {chocolate, crow, night}. By aver-
aging relevance score vectors of these words, we
obtain the mean relevance vector r(visually dark)

that represents the importance of word vector di-
mension in predicting whether a given word has
the property of Visually dark.

Finally, using the mean relevance vector r(j),
word vectors xi is transformed into vectors z

(j)
i

of a subspace for the target dimension. This
is achieved by weighting xi by component-wise
multiplication of xi and r(j) and removing the di-
mensions of zero relevance. Formally, the projec-
tion is defined by the n by m projection matrix
T (j) as follows:

z
(j)
i = xiT

(j) (5)

T
(j)
ik =

r
(j)
i (r

(j)
i > 0 and it is the k-th

nonzero dimension of r(j))
0 (otherwise)

(6)

3 Evaluation Experiment

In order to justify the effectiveness of the proposed
method, we conducted an evaluation experiment
using binary classification of word pairs.

Corpus: All word vectors were trained on
the Corpus of Contemporary American English



4842

Domain Properties
Vision Vision, Bright, Dark, Color, Pattern, Large,

Small, Motion, Biomotion, Fast, Slow,
Shape, Complexity, Face, Body

Somatic Touch, Temperature, Texture, Weight, Pain
Audition Audition, Loud, Low, High, Sound, Music,

Speech
Gustation Taste
Olfaction Smell
Motor Head, UpperLimb, LowerLimb, Practice
Spatial Landmark, Path, Scene, Near, Toward, Away,

Number
Temporal Time, Duration, Long, Short
Causal Caused, Consequential
Social Social, Human, Communication, Self
Cognition Cognition
Emotion Benefit, Harm, Pleasant, Unpleasant, Happy,

Sad, Angry, Disgusted, Fearful, Surprised
Drive Drive, Needs
Attention Attention, Arousal

Table 1: 65 properties in Binder et al.’s (2016) dataset

(COCA), which includes 0.56G word tokens.
Words that occurred less than 30 times in the cor-
pus were ignored, resulting in the vocabulary of
108,230 words. Three context windows of size 3,
5, and 10 were used for training.

Word embedding: We used two representative
models, namely skip-gram with negative sampling
(SGNS) (Mikolov et al., 2013) and GloVe (Pen-
nington et al., 2014). We trained 100-, 200- and
300-dimensional word vectors from the corpus.

Target knowledge representation: We used
Binder et al.’s (2016) brain-based semantic vec-
tors of 535 words as a target representation. 1 This
representation comprises 65 properties in Table 1,
which are based entirely on functional divisions in
the human brain. Each word is represented as a
65-dimensional vector and each dimension corre-
sponds to one of these properties. Each value of
the brain-based vectors represents the salience of
the corresponding property, which is calculated as
a mean salience rating on a 7-point scale ranging
from 0 to 6. Because these properties are based on
not only perceptual properties but also a variety of
other properties such as affective, social, and cog-
nitive ones, this dataset is suitable for evaluation.

Refining word vectors: The prediction func-
tion f was trained using a three-layer neural net-
work comprising an input layer for n-dimensional
word vectors, one hidden layer with n/2 sigmoid
units, and a linear output layer. The parameters
θT , θR1 and θR2 for projection were estimated us-

1http://www.neuro.mcw.edu/semanticrepresentations.html

Bright, Dark, Color, Pattern, Large, Small, Motion,
Fast, Slow, Shape, Temperature, Texture, Weight, Loud,
Sound, Taste, Smell, Fearful

Table 2: 18 properties in CSLB dataset

ing 10-fold cross-validation and grid search. 2

Task: We used a binary classification task of
judging whether a pair of words is similar or not
with respect to each property of Table 1. For ex-
ample, night and chocolate should be judged as
similar with respect to the property of Dark, while
night and ice should be judged as dissimilar with
respect to that property. For each property, we
chose 10 words with the highest salience and 10
words with the lowest salience from the vocabu-
lary of brain-based vectors, and generated 45 high-
salience word pairs and 100 pairs of high-salience
and low-salience words. Note that we did not
consider low-score word pairs because it does not
make sense to ask whether words (e.g., peace and
wit) that do not have a property (e.g., Dark) are
similar with respect to that property.

To confirm the generality of our method, we
also generated another evaluation dataset for un-
trained words (i.e., words not included in Binder
et al.’s vocabulary) using CSLB concept property
norms of 638 words (Devereux et al., 2014). 3 Af-
ter removing words contained in Binder et al.’s vo-
cabulary, we chose properties that were closely re-
lated to Binder et al.’s properties and possessed by
at least 10 words. As a result, the generated dataset
contained 18 properties listed in Table 2, because
the property norm mainly includes perceptual and
functional properties.

Binary classification was carried out by com-
puting cosine similarity between vectors of paired
words and classifying the n highest pairs into sim-
ilar pairs. Hence, the classification performance
was measured by average precision.

4 Results

Table 3 shows mean average precisions across
65 properties for the original word embeddings
(Orig) and the refined embeddings by our method
(Refn). The asterisk indicates that the mean av-
erage precision of the refined vectors is signifi-

2The range in grid search was [3.0, 4.5] with a step size of
0.1 for θT , [0.0, 0.02n] with a step size of 0.001n for θR1 of
n hundred word vector dimension, and [0.0, 0.7] with a step
size of 0.05 for θR2 .

3https://cslb.psychol.cam.ac.uk/propnorms

http://www.neuro.mcw.edu/semanticrepresentations.html
https://cslb.psychol.cam.ac.uk/propnorms


4843

SGNS GloVe
win dim Orig Refn Orig Refn

10 300 75.3 78.6* 67.4 70.4*
10 200 75.9 79.3* 67.8 73.7*
10 100 76.1 77.0* 68.7 71.6*
5 300 75.4 78.8* 67.7 71.8*
5 200 75.6 79.4* 68.0 73.9*
5 100 77.2 78.3 68.9 70.1
3 300 75.5 79.3* 67.6 71.5*
3 200 76.5 77.9* 68.2 70.8*
3 100 77.4 79.0* 68.4 71.2*

Table 3: Mean average precision for Binder et al.’s
(2016) dataset

Figure 1: A scatterplot of average precision of the orig-
inal versus refined vectors for 65 properties in the case
of SGNS with win= 5 and dim= 200. The diagonal
reference line y = x indicates that the original and re-
fined vectors have equal precision.

cantly higher than that of the original vectors by
Wilcoxon signed-rank test (p< .05). For all word
embeddings, the refined vectors achieved higher
mean average precision than the original ones.
Furthermore, in almost all cases, the improvement
is statistically significant. This result demonstrates
that the proposed method is successful in refining
word embeddings so that vector similarity better
reflects the target knowledge.

Figure 1 depicts the difference of average pre-
cision between the original word vectors and the
refined vectors for each target property. Most of
the properties are plotted above the diagonal ref-
erence line, indicating that these properties are
better represented by the refined vectors. Note

SGNS GloVe
win dim Orig Refn Orig Refn

10 300 57.9 60.4* 56.8 58.5
10 200 58.0 59.3 56.3 56.6
10 100 58.8 58.3 55.9 56.0
5 300 58.8 61.1* 56.4 59.3*
5 200 58.5 62.6* 55.6 56.8
5 100 58.9 60.9 56.5 55.5
3 300 58.5 58.8 55.2 55.6
3 200 58.9 59.3 54.5 54.1
3 100 59.3 58.8 53.5 54.4

Table 4: Mean average precision for CSLB property
norm dataset

that properties plotted below the diagonal line, for
which refined word vectors yielded lower preci-
sion than the original vectors, are sensorimotor or
spatiotemporal properties. This result is consis-
tent with Utsumi’s (2018) finding that these kinds
of knowledge are less likely to be encoded in word
vectors.

Table 4 shows the result of binary classification
for CSLB property norm dataset. In most cases,
the refined vectors of untrained words also yielded
better performance than the original vectors. In
some cases, however, refinement did not improve
the performance. One of the reasons for this fail-
ure would be that a small set of vocabulary words
in Binder et al.’s (2016) dataset is not enough for
the subspace to generalize to untrained words.

To confirm whether the projected subspace bet-
ter reflects the target knowledge than the original
space, we visualize both spaces using MDS in Fig-
ure 2. Although all 535 words are embedded into
the two-dimensional space, Figure 2 only shows
words used in binary classification task, namely
words with the 10 highest salience (denoted by
red dots) and 20 lowest salience for a given prop-
erty. As shown in Figure 2, our method refines the
vectors of salient words to be more similar in the
subspace, while preserving the other similarity of
words.

5 Related Work

Prior work on word embedding refinement can
be classified into general purpose refinement and
specific target refinement. Many existing stud-
ies have attempted to refine word vectors to im-
prove the performance of general-purpose simi-
larity computation. These studies generally re-



4844

(a) Property Dark

(b) Property Time

(c) Property Pleasant

Figure 2: Two-dimensional MDS visualization of the
original space (trained by SGNS with win= 5 and
dim= 200) and the projected subspace (θT = 3.0,
θR1 = 0.026 and θR2 = 0.10). Left: Original space,
Right: Projected subspace

fine word vectors by solving an optimization prob-
lem whose objective function reflects the simi-
larity obtained by language resources, such as
WordNet (Faruqui et al., 2015; Yu and Dredze,
2014; Rothe and Schütze, 2017), Freebase (Rothe
and Schütze, 2017), Paraphrase Database (Faruqui
et al., 2015; Yu and Dredze, 2014), free associ-
ation norm (Kiela et al., 2015b), and dictionary
(Wang et al., 2015). Our method differs from them
in that it is proposed for specific target refinement.
In other words, the refined vectors by general pur-
pose refinement method can be further refined to
extract a specific knowledge by our method.

Most prior studies for specific purpose refine-
ment propose a method specialized for a specific
task such as sentiment analysis (Labutov and Lip-
son, 2013; Tang et al., 2016; Yu et al., 2017) and
lexical entailment (Mrkšić et al., 2016; Vulić and
Mrkšić, 2018). On the other hand, our method

refines word vectors for a specific knowledge or
task, but it is not specialized for a knowledge or
task.

Rothe et al. (2016) and Rothe and Schütze
(2016) are conceptually similar to our approach;
their method refines word vectors for a specific
knowledge but it is not specialized for a certain
task. The merit of our method is that any types of
representation can be used as a target, while their
method is limited to binary labels. Furthermore,
while their method learns an orthogonal transfor-
mation of pretrained word vectors by directly op-
timizing the objective function, our method can
project word vectors to a subspace independent of
training method for a prediction function.

6 Conclusion

In this paper, we propose a method for refin-
ing pretrained word vectors using layer-wise rele-
vance propagation. We demonstrated that the pro-
posed method can refine word vectors so that they
better reflect the target knowledge. One of our mo-
tivation is to make embeddings more interpretable
and useful. In other studies (Utsumi, 2015, 2018),
we have analyzed the internal knowledge encoded
in text-based word embeddings, while this study is
the first step toward a general method for utilizing
the internal knowledge of word embeddings.

In future work, we have to modify the refine-
ment method by relevance propagation to be more
effective by exploring the mechanism of how the
internal knowledge of word vectors is extracted by
multilayer neural networks and examining the ef-
fectiveness of other relevance propagation meth-
ods. It would also be vital for future work to ex-
plore efficient combinations with other refinement
methods using language resources.

Acknowledgments

This research was supported by JSPS KAKENHI
Grant Numbers JP15H02713 and SCAT Research
Grant.

References
Sebastian Bach, Alexander Binder, Grégoire Mon-

tavon, Frederick Klauschen, Klaus-Robert Müller,
and Wojciech Samek. 2015. On pixel-wise explana-
tions for non-linear classifier decisions by layer-wise
relevance propagation. PLoS ONE, 10(7):e0130140.

Jeffrey R. Binder, Lisa L. Conant, Colin J. Humphries,
Leonardo Fernandino, Stephen B. Simons, Mario

https://doi.org/10.1371/journal.pone.0130140
https://doi.org/10.1371/journal.pone.0130140
https://doi.org/10.1371/journal.pone.0130140


4845

Aguilar, and Rutvik H. Desai. 2016. Toward a brain-
based componential semantic representation. Cog-
nitive Neuropsychology, 33(3–4):130–174.

Piotr Bojanowski, Edouard Grave, Armand Joulin, and
Tomas Mikolov. 2017. Enriching word vectors with
subword information. Transactions of the Associa-
tion for Computational Linguistics, 5:135–146.

Danushka Bollegala, Kohei Hayashi, and Ken-ichi
Kawarabayashi. 2017. Learning linear transforma-
tions between counting-based and prediction-based
word embeddings. PLoS ONE, 12(9):e0184544.

Tolga Bolukbasi, Kai-Wei Chang, James Y. Zou,
Venkatesh Saligrama, and Adam T. Kalai. 2016.
Man is to computer programmer as woman is to
homemaker? debiasing word embeddings. In Ad-
vances in Neural Information Processing Systems
29, pages 4349–4357.

Elia Bruni, Nam K. Tran, and Marco Baroni. 2014.
Multimodal distributional semantics. Jounral of Ar-
tificial Intelligence Research, 49:1–47.

Barry J. Devereux, Lorraine K. Tyler, Jeroen Geertzen,
and Billi Randall. 2014. The Centre for Speech,
Language and the Brain (CSLB) concept property
norms. Behavior Research Methods, 46:1119–1127.

Manaal Faruqui, Jesse Dodge, Kumar Sujay Jauhar,
Chris Dyer, Eduard Hovy, and A. Noah Smith. 2015.
Retrofitting word vectors to semantic lexicons. In
Proceedings of the 2015 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics: Human Language Technologies,
pages 1606–1615.

Manaal Faruqui and Chris Dyer. 2015. Non-
distributional word vector representation. In Pro-
ceedings of the 53rd Annual Meeting of the Associa-
tion for Computational Linguistics, pages 464–469.

Yoab Goldberg. 2017. Neural Network Methods for
Natural Language Processing. Morgan & Claypool
Publishers.

Douwe Kiela, Luana Bulat, and Stephen Clark. 2015a.
Grounding semantics in olfactory perception. In
Proceedings of the 53rd Annual Meeting of the
Association for Computational Linguistics and the
7th International Joint Conference on Natural Lan-
guage Processing, pages 231–236.

Douwe Kiela and Stephen Clark. 2015. Multi- and
cross-modal semantics beyond vision: Grounding
in auditory perception. In Proceedings of the 2015
Conference on Empirical Methods in Natural Lan-
guage Processing, pages 2461–2470.

Douwe Kiela, Felix Hill, and Stephen Clark. 2015b.
Specializing word embeddings for similarity or re-
latedness. In Proceedings of the 2015 Conference on
Empirical Methods in Natural Language Process-
ing, pages 2044–2048.

Douwe Kiela, Felix Hill, Anna Korhonen, and Stephen
Clark. 2014. Improving multi-modal representa-
tions using image dispersion: Why less is sometimes
more. In Proceedings of the 52nd Annual Meet-
ing of the Association for Computational Linguis-
tics, pages 835–841.

Douwe Kiela, Changhan Wang, and Kyunghyun
Cho. 2018. Context-attentive embeddings for im-
proved sentence representations. arXiv:1804.07983
[cs.CL].

Igor Labutov and Hod Lipson. 2013. Re-embedding
words. In Proceedings of the 51st Annual Meet-
ing of the Association for Computational Linguis-
tics, pages 489–493.

Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey
Dean. 2013. Efficient estimation of word represen-
tations in vector space. In Proceedings of Workshop
at the International Conference on Learning Repre-
sentation (ICLR).

Nikola Mrkšić, Diarmuid Ó Séaghdha, BlaiseThom-
son, Milica Gašić, Lina Rojas-Barahona, Pei-Hao
Su, David Vandyke, Tsung-Hsien Wen, and Steve
Young. 2016. Counter-fitting word vectors to lin-
guistic constraints. In Proceedings of the 2016 Con-
ference of the North American Chapter of the Asso-
ciation for Computational Linguistics: Human Lan-
guage Technologies, pages 142–148.

Maximilian Nickel and Douwe Kiela. 2017. Poincaré
embeddings for learning hierarchical representa-
tions. In I. Guyon, U. V. Luxburg, S. Bengio,
H. Wallach, R. Fergus, S. Vishwanathan, and R. Gar-
nett, editors, Advances in Neural Information Pro-
cessing Systems 30, pages 6338–6347.

Jeffrey Pennington, Richard Socher, and Christo-
pher D. Manning. 2014. GloVe: Global vectors for
word representation. In Proceedings of the 2014
Conference on Empirical Methods in Natural Lan-
guage Processing, pages 1532–1543.

Sascha Rothe, Sebastian Ebert, and Hinrich Schütze.
2016. Ultradense word embeddings by orthogonal
transformation. In Proceedings of the 2016 Confer-
ence of the North American Chapter of the Associ-
ation for Computational Linguistics: Human Lan-
guage Technologies, pages 767–777.

Sascha Rothe and Hinrich Schütze. 2016. Word
embedding calculus in meaningful ultradense sub-
spaces. In Proceedings of the 54th Annual Meet-
ing of the Association for Computational Linguis-
tics, pages 512–517.

Sascha Rothe and Hinrich Schütze. 2017. Autoex-
tend: Combining word embeddings with semantic
resources. Computational Linguistics, 43(3):593–
617.

Wojciech Samek, Thomas Wiegand, and Klaus-Robert
Müller. 2017. Explainable artificial intelligence:
Understanding, visualizing and interpreting deep
learning models. arXiv:1708.08296 [cs.A1].

https://doi.org/10.1080/02643294.2016.1147426
https://doi.org/10.1080/02643294.2016.1147426
http://aclweb.org/anthology/Q17-1010
http://aclweb.org/anthology/Q17-1010
https://doi.org/10.1371/journal.pone.0184544
https://doi.org/10.1371/journal.pone.0184544
https://doi.org/10.1371/journal.pone.0184544
http://papers.nips.cc/paper/6228-man-is-to-computer-programmer-as-woman-is-to-homemaker-debiasing-word-embeddings.pdf
http://papers.nips.cc/paper/6228-man-is-to-computer-programmer-as-woman-is-to-homemaker-debiasing-word-embeddings.pdf
https://doi.org/10.1613/jair.4135
https://doi.org/10.3758/s13428-013-0420-4
https://doi.org/10.3758/s13428-013-0420-4
https://doi.org/10.3758/s13428-013-0420-4
http://www.aclweb.org/anthology/N15-1184
http://www.aclweb.org/anthology/P15-2076
http://www.aclweb.org/anthology/P15-2076
https://doi.org/10.2200/S00762ED1V01Y201703HLT037
https://doi.org/10.2200/S00762ED1V01Y201703HLT037
http://www.aclweb.org/anthology/P15-2038
http://www.aclweb.org/anthology/D15-1293
http://www.aclweb.org/anthology/D15-1293
http://www.aclweb.org/anthology/D15-1293
http://www.aclweb.org/anthology/D15-1242
http://www.aclweb.org/anthology/D15-1242
http://aclweb.org/anthology/P14-2135
http://aclweb.org/anthology/P14-2135
http://aclweb.org/anthology/P14-2135
http://arxiv.org/abs/arXiv:1804.07983
http://arxiv.org/abs/arXiv:1804.07983
http://www.aclweb.org/anthology/P13-2087
http://www.aclweb.org/anthology/P13-2087
http://arxiv.org/abs/arXiv:1301.3781
http://arxiv.org/abs/arXiv:1301.3781
http://www.aclweb.org/anthology/N16-1018
http://www.aclweb.org/anthology/N16-1018
http://papers.nips.cc/paper/7213-poincare-embeddings-for-learning-hierarchical-representations.pdf
http://papers.nips.cc/paper/7213-poincare-embeddings-for-learning-hierarchical-representations.pdf
http://papers.nips.cc/paper/7213-poincare-embeddings-for-learning-hierarchical-representations.pdf
http://www.aclweb.org/anthology/D14-1162
http://www.aclweb.org/anthology/D14-1162
http://www.aclweb.org/anthology/N16-1091
http://www.aclweb.org/anthology/N16-1091
http://www.aclweb.org/anthology/P16-2083
http://www.aclweb.org/anthology/P16-2083
http://www.aclweb.org/anthology/P16-2083
https://doi.org/10.1162/COLI a 00294
https://doi.org/10.1162/COLI a 00294
https://doi.org/10.1162/COLI a 00294
http://arxiv.org/abs/arXiv:1708.08296
http://arxiv.org/abs/arXiv:1708.08296
http://arxiv.org/abs/arXiv:1708.08296


4846

Carina Silberer, Vittorio Ferrari, and Mirella Lapata.
2017. Visually grounded meaning representations.
IEEE Transactions on Pattern Recognition and Ma-
chine Intelligence, 39(11):2284–2297.

Duyu Tang, Furu Wei, Bing Qin, Nan Yang, Ting Liu,
and Ming Zhou. 2016. Sentiment embeddings with
applications to sentiment analysis. IEEE Transac-
tions on Knowledge and Data Engineering, 28:496–
509.

Akira Utsumi. 2015. A complex network approach
to distributional semantic models. PLoS ONE,
10(8):e0136277.

Akira Utsumi. 2018. A neurobiologically motivated
analysis of distributional semantic models. In Pro-
ceedings of the 40th Annual Conference of the Cog-
nitive Science Society (CogSci2018), pages 1147–
1152.

Ivan Vulić and Nikola Mrkšić. 2018. Specialising word
vectors for lexical entailment. In Proceedings of the
2018 Conference of the North American Chapter of
the Association for Computational Linguistics: Hu-
man Language Technologies, pages 1134–1145.

Tong Wang, Abdel-rahman Mohamed, and Graeme
Hirst. 2015. Learning lexical embeddings with syn-
tactic and lexicographic knowledge. In Proceedings
of the 53rd Annual Meeting of the Association for
Computational Linguistics and the 7th International
Joint Conference on Natural Language Processing
(Volume 2: Short Papers), pages 458–463.

Liang-Chih Yu, Jin Wang, K. Robert Lai, and Xuejie
Zhang. 2017. Refining word embeddings for sen-
timent analysis. In Proceedings of the 2017 Con-
ference on Empirical Methods in Natural Language
Processing, pages 534–539.

Mo Yu and Mark Dredze. 2014. Improving lexical em-
beddings with semantic knowledge. In Proceedings
of the 52nd Annual Meeting of the Association for
Computational Linguistics, pages 545–550.

https://doi.org/10.1109/TPAMI.2016.2635138
https://doi.org/10.1109/TKDE.2015.2489653
https://doi.org/10.1109/TKDE.2015.2489653
https://doi.org/10.1371/journal.pone.0136277
https://doi.org/10.1371/journal.pone.0136277
http://mindmodeling.org/cogsci2018/papers/0226/
http://mindmodeling.org/cogsci2018/papers/0226/
http://aclweb.org/anthology/N18-1103
http://aclweb.org/anthology/N18-1103
http://www.aclweb.org/anthology/P15-2075
http://www.aclweb.org/anthology/P15-2075
http://aclweb.org/anthology/D17-1056
http://aclweb.org/anthology/D17-1056
http://aclweb.org/anthology/P14-2089
http://aclweb.org/anthology/P14-2089

