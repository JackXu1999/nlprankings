



















































DAL: Dual Adversarial Learning for Dialogue Generation


Proceedings of the Workshop on Methods for Optimizing and Evaluating Neural Language Generation (NeuralGen), pages 11–20
Minneapolis, Minnesota, USA, June 6, 2019. c©2019 Association for Computational Linguistics

11

DAL: Dual Adversarial Learning for Dialogue Generation

Shaobo Cui 1, Rongzhong Lian 2, Di Jiang 2, Yuanfeng Song 2, Siqi Bao 2, Yong Jiang 1
1 Tsinghua University, China

2 Baidu Inc., China
cuishaobo16@mails.tsinghua.edu.cn

{lianrongzhong,jiangdi,songyuanfeng,baosiqi}@baidu.com
jiangy@sz.tsinghua.edu.cn

Abstract
In open-domain dialogue systems, genera-
tive approaches have attracted much attention
for response1 generation. However, existing
methods are heavily plagued by generating
safe responses and unnatural responses. To al-
leviate these two problems, we propose a novel
framework named Dual Adversarial Learn-
ing (DAL) for high-quality response genera-
tion. DAL innovatively utilizes the duality
between query generation and response gen-
eration to avoid safe responses and increase
the diversity of the generated responses. Ad-
ditionally, DAL uses adversarial learning to
mimic human judges and guides the system
to generate natural responses. Experimental
results demonstrate that DAL effectively im-
proves both diversity and overall quality of the
generated responses. DAL outperforms state-
of-the-art methods regarding automatic met-
rics and human evaluations.

1 Introduction

In recent years, open-domain dialogue systems are
gaining much attention owing to their great po-
tential in applications such as educational robots,
emotional companion, and chitchat. The existing
approaches for open-domain dialogue systems can
be divided into two categories: retrieval-based ap-
proaches (Hu et al., 2014; Ji et al., 2014) and gen-
erative approaches (Ritter et al., 2011; Shang et al.,
2015). The retrieval-based approaches are based
on conventional information retrieval techniques
and strongly rely on the underlying corpus (Wang
et al., 2013; Lu and Li, 2013). Since the capabil-
ity of retrieval-based approaches is strongly lim-
ited by corpus, generative approaches are attract-
ing more attention in the field of open-domain di-
alogue research. The de facto backbone of gener-
ative approaches is the Seq2Seq model (Bahdanau

1We use query and response to denote the first and second
utterances in a single-turn dialogue.

et al., 2014) , which is essentially an encoder-
decoder neural network architecture. Despite their
success, Seq2Seq model and its variants (Sordoni
et al., 2015; Vinyals and Le, 2015) are heavily
plagued by safe responses (generic and dull re-
sponses such as “I don’t know” or “Me too”) and
unnatural responses (such as “I want to go, but I
don’t want to go”).

In this paper, we propose a novel frame-
work named Dual Adversarial Learning (DAL)
to alleviate the aforementioned two problems.
DAL consists of two generative adversarial net-
works (GANs): one for query generation and the
other for response generation. The response gen-
eration model is used to transfer from the query
domain Q to the response domain R, while the
query generation model is for transformation from
R to Q. Here we consider the response gener-
ation task and the query generation task as dual
tasks. The generators of these two GANs are con-
nected through the duality constraint. As such, in
DAL, there are two kinds of signals that jointly in-
struct the optimization of generators: (1) the dual
signal from the duality constraint between these
two generators; (2) the adversarial signal from
the discriminators. The dual signal is utilized to
model the mutual relation between query genera-
tion and response generation. We use an instance
to better illustrate this mutual relation: for a given
query “Where to have dinner?”, compared with
a safe response “I dont know”, a more diverse
and specific response “The Indian cuisine around
the corner is great” usually has a higher probabil-
ity of being transformed back to the given query.
DAL takes full advantage of this intuition via dual
learning, which avoids generating safe responses
and improves the diversity of the generated re-
sponses. Additionally, in order to make the gen-
erated responses as natural as possible, the adver-
sarial signal in DAL mimics human judges to alle-



12

viate unnatural responses. We compare DAL with
state-of-the-art methods through extensive exper-
iments, and DAL demonstrates superior perfor-
mance regarding automatic metrics, human eval-
uations, and efficiency.

There are crucial differences between our
dual approach and Maximum Mutual Informa-
tion (MMI) (Li et al., 2016) though both utilize
the reverse dependency to improve the diversity
of the generated responses. Due to the challeng-
ing mutual information objective, the distribution
p(r|q) is same as that in vanilla Seq2Seq in MMI.
More specifically, p(r|q) in MMI is trained only
by maximum likelihood estimation (MLE) objec-
tive at training time (we use p(r|q) to denote the
probability distribution of predicting the response
r given the query q). The mutual information in
MMI is utilized only at inference time, and the
inference process is not only time-consuming but
also inaccurate in MMI. However, p(r|q) in our
dual approach is trained by not only the maxi-
mum likelihood estimation objective but also the
diversity objective (duality constraint) at training
time. Since the dual approach directly incorpo-
rates the reverse dependency information at the
training time, it can avoid the time-consuming in-
ference plaguing MMI. Additionally, the dual ap-
proach does not need to maintain a large size op-
tional response set for the time-consuming rerank-
ing strategy in MMI-bidi (one variant of MMI).
The dual approach shows its efficiency superiority
over MMI in real-life applications, which is shown
in our efficiency experiment.

Our dual approach is quite different from the re-
inforcement learning based structure having two
Seq2Seq models in (Zhang et al., 2018) 2 . In
(Zhang et al., 2018), G1, which generates a re-
sponse r̂ given a query q, uses the conditional
probability P2(q|r̂) calculated by G2 as the co-
herence measure to guide G1 in the reinforce-
ment learning process. Similarly, G2, which gen-
erates a query q̂ given a response r, uses the con-
ditional probability P1(r|q̂) calculated by G1 as
the coherence measure to guide G2 in the rein-
forcing learning process. However, in our work,
we utilize the joint probability p(q, r) to connect
these two Seq2Seq models and thus avoid unstable
and time-consuming reinforcement learning in the
dual approach. Besides, our DAL framework is

2Our dual approach is finished independently with this
work in addition to the crucial difference. We did not notice
this paper until our work is done.

strongly different from previous structures that are
composed of two GANs, such as CycleGAN (Zhu
et al., 2017), DiscoGAN (Kim et al., 2017) and
DualGAN (Yi et al., 2017). Those works can only
be utilized on the image translation task and two
generators are connected by cycle consistency, i.e.,
for each image x in domain X , the image transla-
tion cycle is supposed to bring x to the original
image: x → G1(x) → G2(G1(x)) ≈ x . How-
ever, cycle consistency is difficult to be applied
into the text generation task. In our paper, we
use the joint distribution of query-response pairs
rather than cycle consistency to enforce the dual-
ity between these two dual generators.

The contributions of this paper are as follows:
• To the best of our knowledge, this is the first
work that adopts the duality to avoid safe re-
sponses for dialogue generation. It sheds light on
the utility of query generation in improving the
performance of response generation.
• DAL is a novel framework that integrates dual
learning and adversarial learning, which comple-
mentary and jointly contributes to generating both
diverse and natural responses.

The rest of this paper is organized as follows.
The related work is firstly reviewed. The DAL
framework is introduced in Section 3 and the train-
ing of DAL is described in Section 4. Experimen-
tal results are shown in Section 5, followed by the
conclusion of this paper in Section 6.

2 Related Work

Dual Learning Many machine learning tasks
have emerged in dual forms, such as dual neural
machine translation (dual-NMT) (He et al.,
2016), image classification and conditional image
generation (van den Oord et al., 2016). Dual
learning (He et al., 2016) is proposed on the
assumption that the dual correlation could be used
to improve both the primal task and its dual task:
the primal task aims to map from input space X
to output space Y , whereas the dual task takes
samples from space Y and maps to space X .
Tang et al. (2017) implemented a dual framework
for the question answering system. Their model
regards the answer selection (given a question
and its several candidate answers, select the most
satisfying answer to answer the question) and the
question generation as dual tasks, which increases
the performance of both.
Adversarial Learning Adversarial learn-



13

q Gθqr r̂ Dϕqr Rqr
Policy Gradient

r Gθrq q̂ Dϕrq Rrq

Policy Gradient
Duality

true
queries

Seq2Seq-based generator
input: real queries

output: generated responses
generated
responses

discriminator
input: <q, r̂>

output: being-real probability

true
responses

Seq2Seq-based generator
input: real responses

output: generated queries

generated
queries

discriminator
input: <q̂, r>

output: being-real probability

(a) The architecture of DAL.

Query

Response

GRU-based
Enocder

GRU-based
Encoder

vq

vr

F
C

layers

Output

vq ⊕ vrvq ⊕ vr

(b) The architecture of the discriminator.

Figure 1: Dual Adversarial Learning.

ing (Goodfellow et al., 2014), or Generative
Adversarial Networks (GAN), has proven to be a
promising approach for generation task. A GAN
usually contains two neural networks: a generator
G and a discriminator D. G generates samples
while D is trained to distinguish generated
samples from true samples. By regarding the
sequence generation as an action-taking problem
in reinforcement learning, Li et al. (2017) pro-
posed to apply GAN to dialogue generation, in
which the output of the discriminator is used as
the reward for the generator’s optimization.
Work on the Safe Response Problem There
is some existing work on the safe response prob-
lem. The first kind of approach is to introduce
specific keywords (Mou et al., 2016) or topic
information (Xing et al., 2017) into the generated
responses. These methods help to increase the
dialogue coherence (Peng et al., 2019) by key-
words introduction. However, these methods shift
the difficulty from diverse response generation to
keyword or topic prediction, which are also chal-
lenging tasks. The second kind of approach takes
the reverse dependency (the query generation task
given the responses) into consideration. Li et al.
(2016) considered the reverse dependency and
proposed Maximum Mutual Information (MMI)
method, which is empirically plagued by un-
grammatical responses (MMI-antiLM) and huge
decoding space (MMI-bidi).

3 DAL Framework

In this section, we firstly given an overview of
DAL framework and then elaborate the discrim-
inators and the generations. We also present the
reason why duality promotes diversity.

3.1 Overview
The architecture of DAL is presented in Fig-
ure 1(a). The real query and response are denoted

by q and r, whereas the generated query and re-
sponse are denoted as q̂ and r̂. DAL consists of
two GANs (one for query generation and the other
for response generation). Generators are denoted
by Gθqr and Gθrq and the corresponding discrim-
inators are denoted as Dφqr and Dφrq . The input
of Gθqr is a real query q and the output is the gen-
erated response r̂. Similarly, for Gθrq , the input
is a real response r and the output is the gener-
ated query q̂. For Dφqr , the input is the ficto-
facto query-response pair 〈q, r̂〉, and the output
Rqr is estimated probability of the query-response
pair being human-generated, which is estimated
by Dφqr . Analogously, the input of Dφrq is the
ficto-facto pair 〈q̂, r〉, and the output Rrq is the es-
timated probability of the input pair being human-
generated. Gθqr and Gθrq are connected by the
duality constraint derived from the joint probabil-
ity P (q, r). The adversarial signal from discrimi-
nators, Rqr, Rrq, are passed to the corresponding
generators as the reward through policy gradient.

3.2 Discriminator
The discriminator mimics a human judge and
guides the generator to generate natural utterances.
The architecture of the discriminator is shown
in Figure 1(b). Gated Recurrent Unit (GRU)
based (Bahdanau et al., 2014) neural networks are
used to obtain the query embedding vq and the
response embedding vr. The concatenation vec-
tor vq ⊕ vr is used as the abstract representation
of the query-response pair. vq ⊕ vr is further
passed through two fully-connected layers. The
output of the last fully-connected layer is the esti-
mated probability of the query-response pair being
human-generated. The objective of the discrimi-
nator is formalized as follows:

min
φ
− E〈q,r〉∼pdata [log (Dφ(〈q, r〉))]

− E〈q,r〉∼Gθ [log (1−Dφ(〈q, r〉))]
(1)



14

q1

q2

· · ·

q3

r1

r2

So cold in Beijing!

What happened to Joe?

Where to have dinner?

I don’t know!

The Indian cuisine

around the corner is great!

(a) An example corpus.

So cold in Beijing!What happened to Joe?

Where to have dinner? · · ·

I don’t know!

The Indian · · ·

safe response

diverse response

(b) Queries and responses with duality constraint.

Figure 2: An example to illustrate why duality promotes diversity.

where pdata denotes the real-world query-response
distribution. For the response generation task, Dφ
is Dφqr and Gθ is Gθqr , while for the query gener-
ation task, Dφ is Dφrq and Gθ is Gθrq .

3.3 Dual Generators
Both generators adopt the Seq2Seq structure, in
which GRU is used as the basic unit. The con-
straint between the dual tasks (query generation
and response generation) can be represented with
the joint probability P (q, r):

P (q, r) = Pq(q)P (r|q; θqr) = Pr(r)P (q|r; θrq) (2)

where Pq(q) and Pr(r) are language models pre-
trained on the query corpus and the response
corpus. In this paper, we use smoothed bi-
gram language models for both Pq(q) and Pr(r).
P (r|q; θqr) and P (q|r; θrq) are the dual genera-
tors. Both P (r|q; θqr) and P (q|r; θrq) can be ob-
tained through the markov chain rule:{

P (r|q; θqr) =
∏|r|
t=1 P (r

t|r0:t−1, q; θqr)
P (q|r; θrq) =

∏|q|
t=1 P (q

t|q0:t−1, r; θrq)

where P (rt|r0:t−1, q; θqr) and P (qt|q0:t−1, r; θrq)
are formulations of decoders in Seq2Seq models.

3.4 Duality Promotes Diversity
To better illustrate why duality increases the di-
versity of the generated responses, we show some
query-response pair examples in Figure 2(a). In
Figure 2(a), each directional arrow starts from a
query while ends at its corresponding response.
It can be observed that: (1) Safe response r1 :
“I don’t know” connects to many queries, i.e.,
{q1, q2, q3, · · · }. (2) More diverse and specific re-
sponse r2 : “The Indian cuisine around the corner
is great”, nevertheless, exactly corresponds to only
one query q3 : “Where to have dinner?”. 3

3There may exist several other queries that can be replied
using “The Indian cuisine around the corner is great”. But

In the training process of Gθrq , the in-
crease of logP (q3|r2; θrq), denoted by
∆ logP (q3|r2; θrq) 4, is much bigger than
the increase of logP (q3|r1; θrq), denoted by
∆ logP (q3|r1; θrq). Formally,

∆ logP (q3|r2; θrq)� ∆ logP (q3|r1; θrq)

The reason behind this phenomenon is as fol-
lows. The safe response r1 relates with
queries {q1, q2, q3, · · · }. When Gθrq is pro-
vided with 〈q1, r1〉 or 〈q2, r1〉, Gθrq is opti-
mized to increase the log conditional probabil-
ity logP (q1|r1; θrq) or logP (q2|r1; θrq), it is in-
evitable that logP (q3|r1, θrq) will decrease to a
certain extent, since these log conditional proba-
bilities share the same parameters θrq. The same
principle applies to logP (q2|r1, θrq) whenGθrq is
provided with 〈q1, r1〉 or 〈q3, r1〉. However, the
diverse response r2 is uniquely connected to the
query q3, in that case, Gθrq takes all efforts to in-
crease logP (q3|r2, θrq).

With the duality constraint in Eq. 2, we obtain:

P (q|r; θrq)
P (r|q; θqr)

=
Pq(q)

Pr(r)
= k(q, r). (3)

Since both Pq(q) and Pr(r) are obtained from
the pre-trained language models, both of them
are constant for any query-response pair 〈q, r〉.
k(q, r) =

Pq(q)
Pr(r)

is also constant for any 〈q, r〉.
Take the log formulation of Eq. 3, we can obtain:

logP (q|r; θrq)− logP (r|q; θqr) = log k(q, r).

From above equation, we observe that the increase
of logP (q|r; θrq), denoted as ∆ logP (q|r; θrq),

this number is much smaller than those that can be replied
using “I don’t know”. For simplicity, we only show only one
query here for the response “The Indian cuisine around the
corner is great”. This would not affect the following analysis.

4The reason why the probability is in log formulation is
that the probability which the maximum likelihood objective
optimize is in log formulation rather than origin formulation



15

and the increase of logP (r|q; θqr), denoted by
∆ logP (r|q; θqr), is supposed to be equal for any
query-response pair 〈q, r〉, since log k(q, r) is con-
stant during the training process. Therefore,

∆ logP (q3|r2; θrq)� ∆ logP (q3|r1; θrq)

in turn makes

∆ logP (r2|q3; θqr)� ∆ logP (r1|q3; θqr).

When Gθqr finishes its training process, we obtain
P (r2|q3; θqr)� P (r1|q3; θqr). This indicates that
it is more likely for Gθqr to assign higher proba-
bility to the diverse response given the query.

We use Figure 2(b) to visually explain this intu-
ition. We suppose that both queries and responses
“possess” their own spatial space. The coordinates
of the ellipse and the rectangle represent the lo-
cations of the query q and the response r in the
spatial space. The distance between q and r rep-
resents the probability of transforming between q
and r, namely P (q|r) and P (r|q). The shorter the
distance, the larger the probability. When Gθqr
and Gθrq are provided with a query-response pair
〈q, r〉, the training objectives of Gθqr and Gθrq are
to increase the probability P (r|q) and P (q|r), i.e.,
to shorten the distance between q and r. Since the
safe response r1 corresponds to {q1, q2, q3, · · · },
the position of this safe response is determined by
all involved queries. Because each of these in-
volved queries attempts to “drag” r1 close to itself,
the safe response r1 “chooses” to keep a distance
with each of them to balance the involved queries.
However, the diverse response r2 corresponds to
exactly one query q3. r2 “selects” to stay as close
to q3 as possible. As it can be seen from the fig-
ure, the distance between q3 and r2 is much shorter
than the distance between q3 and r1, i.e., P (r2|q3)
is much larger than P (r1|q3). In other words, with
the duality constraint, Gθqr tends to generate di-
verse responses rather than safe responses.

4 Training of DAL

Duality Constraint for Diversity Direct en-
forcement of the constraint in Eq. 2 is intractable.
The duality constraint in Eq. 2 can be relaxed into
a regularization term (Tang et al., 2017):

Υ = [logPr(r) + logP (q|r; θrq)
− logPq(q)− logP (r|q; θqr)]2

. (4)

We minimize Υ to enforce the duality constraint
in order to generate more diverse responses.

Adversarial Signal for Naturalness The de-
coding phase in the Seq2Seq model involves
sampling discrete words. This discrete sam-
pling makes the optimization of the gener-
ator based upon the discriminator’s guidance
non-differentiable. To circumvent the non-
differentiable obstacle, we optimize each gener-
ator through reinforcement learning. The policy
gradient is applied to pass the discriminator’s ad-
versarial signal to the generator. The discrimi-
nator Dφ gives a score J(θ) based on its judg-
ment of how likely the generated 〈q, r〉 is human-
generated:

J(θ) = E〈x,y〉∈Gθ [Dφ(〈x, y〉)].

For response generation, J(θ) is J(θqr), Gθ is
Gθqr , Dφ is Dφqr , x is the real query and y is the
generated response. Analogously, in query gener-
ation, J(θ) is J(θrq), Gθ is Gθrq , Dφ is Dφrq , x
is the real response and y is the generated query.
J(θ) is used as the reward for the optimization
of Gθ. With the likelihood ration trick (Williams,
1992; Sutton et al., 2000), the gradient of J(θ) can
be approximated as:

∇θJ(θ) ' [Dφ(〈x, y〉)− b] · ∇θ log(p(y|x; θ)),

where b is used to reduce the variance of the esti-
mation while keeping the estimation unbiased, and
p(y|x; θ) is the probability distribution defined by
the generator Gθ.
Combined Gradient In DAL, the gradient for
updating each generator is the weighted com-
bination of ∇θJ(θ) (for natural responses) and
∇θΥ (for avoidance of safe responses):{
∇θqrGθqr = ∇θqrΥ− λqr · ∇θqrJ(θqr)
∇θrqGθrq = ∇θrqΥ− λrq · ∇θrqJ(θrq)

. (5)

Teacher Forcing When the generator is trained
with only the adversarial signals from the discrim-
inator and the duality constraint, the training pro-
cess of the generator easily collapses. This is be-
cause the discriminator sometimes is remarkably
better than the corresponding generator in cer-
tain training batches. The discriminator can eas-
ily discriminate all the generated utterances from
real ones. The generator realizes that it gen-
erates low-quality samples but cannot figure out
the good standard. To stabilize the training pro-
cess, after each update with the combined gra-
dient ∇θqrGθqr or ∇θrqGθrq , the generators are



16

provided with real query-response pairs and are
strengthened with maximum likelihood training,
which is also known as Teacher Forcing (Li et al.,
2017; Lamb et al., 2016). The training procedure

Algorithm 1 Training of DAL.
Input: Pre-trained language models: Pq(q) on

query corpus and Pr(r) on response corpus.
Output: Gθqr and Gθrq

1: Randomly initialize Gθqr ,Gθrq , Dφqr , Dφrq .
2: Pre-train Gθqr and Gθrq using MLE.
3: Pre-train Dφqr and Dφrq by Eq. 1.
4: while models have not converged do
5: for i = 1, · · · , d do
6: Update Dφqr and Dφrq by Eq. 1.
7: end for
8: for j = 1, · · · , g do
9: Sample 〈q, r〉 from real-world data.

10: Update Gθqr by∇θqrGθqr in Eq. 5.
11: Teacher Forcing: update Gθqrwith 〈q, r〉
12: Update Gθrq by∇θrqGθrq in Eq. 5.
13: Teacher Forcing: update Gθrq with 〈q, r〉
14: end for
15: end while

of DAL is presented in Algorithm 1. Firstly, we
use maximum likelihood estimation to pre-train
Gθqr and Gθrq . Analogously, Dφqr and Dφrq are
also pre-trained according to Eq. 1. After the
pre-training phase, each generator is optimized by
both duality constraint and adversarial signal, fol-
lowed with the regularization of Teacher Forcing.
The corresponding discriminators are simultane-
ously optimized.

5 Experiments

5.1 Experimental Settings

Baselines In order to verify the performance
of DAL, we compare the following methods:
Seq2Seq: the standard Seq2Seq model (Sutskever
et al., 2014). MMI-anti: the mutual infor-
mation method (Li et al., 2016), which uses
an anti-language model in inference. MMI-
bidi: the mutual information method (Li et al.,
2016), which first generates a N-best response
set with p(r|q) and then reranks this response
set with p(q|r) in inference. Adver-REIN: the
adversarial method adopting REINFORCE algo-
rithm (Li et al., 2017). GAN-AEL: the adver-
sarial method with an approximate embedding
layer to solve the non-differentiable problem (Xu

et al., 2017). DAL-Dual (ours): DAL trained
only with maximum likelihood (Teacher Forcing)
and duality constraint (∇θqrΥ or ∇θrqΥ). DAL-
DuAd (ours): DAL-Dual with adversarial learn-
ing (Algorithm 1).

Both DAL-Dual and DAL-DuAd are methods
proposed by us: the former incorporates the dual
signal only, while the later combines the dual sig-
nal and the adversarial signal. In DAL-Dual, the
guidance of each generator can be formulated as

∇θGθ = ∇θMLE + λdual · ∇θΥ,

where ∇θMLE is the guidance from teacher forc-
ing and∇θΥ is the guidance from the duality con-
straint. In DAL-DuAd, the guidance of each gen-
erator can be formulated as

∇θGθ = ∇θMLE +λdual ·∇θΥ +λgan ·∇θJ(θ),

where∇θJ(θ) is the adversarial signal.
Experimental Settings A Sina Weibo
dataset (Zhou et al., 2017) is employed to
train the models. We treat each query-response
pair as a single-turn conversation. Attention
mechanism (Luong et al., 2015) is applied in
all the methods to enhance the performance.
All the methods are implemented based on the
open source tools Pytorch(Paszke et al., 2017)
and OpenNMT (Klein et al., 2017). 1,000,565
query-response pairs are employed as the training
data, 3,000 pairs as the validation data. The test
data is another unique 10,000 query-response
pairs. The length of all the dialogue utterances
in the training corpus ranges from 5 to 50.
Batch size is set to 64. The vocabulary size
is set to 50,000. The dimension of word em-
bedding is set to 500. All the methods adopt
a beam size of 5 in the decoding phase. The
maximum length of the target sequence is set to
50. Gradient clipping strategy is adopted when
the norm exceeds a threshold of 5. There are
2 fully-connected layers (1000*500, 500*1) in
the discriminator structure of DAL-DuAd. The
vanilla Seq2Seq, MMI-anti and MMI-bidi use
SGD as the optimizer, whose initial learning rate
is 1.0. Adver-REIN, GAN-AEL, DAL-Dual, and
DAL-DuAd use Adam (Kingma and Ba, 2014)
as the optimizer, whose initial learning rate is
0.001, β1 = 0.9, and β2 = 0.999. Both Adam and
SGD used in all the methods adopt a decay rate of
0.5 after the 8th epoch. The dropout (Srivastava
et al., 2014) probability is set to 0.5. λdual is set



17

to 0.025 for both Gθqr and Gθrq . λdual is set to
0.025 and λgan is set to 1 for both Gθqr and Gθrq .
In Algorithm 1, d is set to 1 and g is set to 5. In
MMI-bidi, the size of the N-best list is set to 5. In
MMI-anti, γ is set to 0.15 and λ is set to 0.3.

5.2 Experimental Results

We firstly evaluate DAL on the task of generating
of diverse responses. Then we resort to human an-
notators to evaluate the overall quality of the gen-
erated responses. Finally, we present several cases
generated by all the involved method.
Response Diversity DISTINCT is a well-
recognized metric to evaluate the diversity of the
generated responses (Li et al., 2016; Xing et al.,
2017). In our experiment, we employ DISTINCT-
1 and DISTINCT-2, which calculate distinct uni-
grams and bigrams in the generated responses re-
spectively. Table 1 presents the results of the five
methods.

Method DISTINCT-1 DISTINCT-2
Seq2Seq 0.031 0.137
MMI-anti 0.033 0.141
MMI-bidi 0.034 0.143

Adver-REIN 0.036 0.145
GAN-AEL 0.038 0.149

DAL-Dual (ours) 0.052 0.209
DAL-DuAd (ours) 0.049 0.201

Table 1: Results of diversity evaluation.

From Table 1, we have the following ob-
servations: (1) Both MMI-anti and MMI-bidi
slightly improve the performance as compared
with Seq2Seq. MMI-bidi heavily relies on the di-
versity of the N-best response set generated by
p(r|q). When N is not large enough to include
some infrequently-occurring responses into the
optional set, this set may lack diversity, and thus
the ultimate response obtained with the reranking
strategy also lacks diversity. However, when N is
large, some responses having low coherence with
the given query will be included in the optional set,
and such responses may be selected as the final re-
sponse, which hurts the performance of MMI-bidi.
Therefore, the selection of N is an arduous task.
MMI-anti also heavily relies on the anti-language
model to obtain diverse responses. (2) Compared
with Seq2Seq, our DAL-Dual improves diversity
by 67.7% measured by DISTINCT-1 and 52.6%
measured by DISTINCT-2, which reveals the ef-
fectiveness of the dual approach in improving di-
versity. (3) As expected, compared with Adver-
Rein and GAN-AEL, our DAL-DuAd further im-

proves the diversity of the generated responses.
This observation proves our assumption that, with
the guidance of discriminators Dφqr and Dφrq , the
generator Gθrq is able to influence the generator
Gθqr to produce more diverse responses. We do
notice that DAL-Dual achieves slightly better per-
formance than DAL-DuAd on diversity. The rea-
son is that sometimes adversarial methods tend to
generate some short but quality responses such as
“Let’s go!” for given queries such as “We can have
dinner together tonight. ” or “There is an exhi-
bition at the National Museum.”. However, this
short but natural response would harm diversity.
Response Quality Since the word overlap-
based metrics such as BLEU (Papineni et al.,
2002) and embedding-based metrics are inappro-
priate for response quality evaluation due to their
low correlation with human judgment (Liu et al.,
2016; Mou et al., 2016), we resort to human anno-
tators to evaluate the overall quality of the gener-
ated responses. We employ 3 annotators to eval-
uate the quality of 200 responses generated from
each of the aforementioned methods. 2: the re-
sponse is natural, relevant and informative. 1: the
response is appropriate for the given query but
may not be very informative. 0: the response is
completely irrelevant, incoherent or contains syn-
tactic errors. The final score for each response is
the average of the scores from all the annotators.
The human evaluation results are listed in Table 2.

Method Human rating Kappa
Seq2Seq 0.470 0.56
MMI-anti 0.568 0.46
MMI-bidi 0.523 0.60

Adver-REIN 0.767 0.49
GAN-AEL 0.758 0.52

DAL-Dual (ours) 0.730 0.47
DAL-DuAd (ours) 0.778 0.50

Table 2: Results of human elevation: response quality.

The agreement among annotators is calculated
with Fleiss’ kappa (Fleiss, 1971). The agreement
ratio is in a range from 0.4 to 0.6, showing moder-
ate agreement. Based on the results, we have the
following observations: (1) DAL-DuAd achieves
the highest quality score, indicating that our DAL-
DuAd has the ability to produce coherent and in-
formative responses. (2) Adver-REIN and GAN-
AEL also obtain fairly good pointwise scores. This
is because the adversarial learning mechanism ef-
fectively guides the generated responses to be
close to the human-generated responses. (3) Com-



18

Chinese English (translated)

First
case

Query 我无聊的把鼠标垫的内容一字不落的看完了 I feel so bored that I read all the words printed on
the mouse pad.

Seq2Seq 不要这样嘛！ Come on!
MMI-anti 哈哈哈哈哈哈! Haha Haha Haha!
MMI-bidi 不错不错！ Good, good!
Adver-REIN 没事没事！ It’s nothing!
GAN-AEL 哈哈。。。 Haha ...
DAL-Dual 你太可爱了！ You are so cute!
DAL-DuAd 我也是这么想的. I also catch such an idea.

Second
case

Query 昨天刚看了《等风来》, 想去下尼泊尔, 这个神奇的国度. I watched the movie Up in the Wind yesterday. It
inspires me to visit Nepal, such an amazing country.

Seq2Seq 我也想去, 可是没去过. I want to go, but I haven’t.
MMI-anti 不错啊! Good!
MMI-bidi 真的假的？？？ Seriously???
Adver-REIN 我也想去, 可是没去过. I want to go, but I haven’t.
GAN-AEL 也我就怕语言问题. Also I am concern about the language.
DAL-Dual 真的很神奇！ It’s really amazing!
DAL-DuAd 好神奇的国度! What an amazing country!

Figure 3: Case study.

pared with Seq2Seq, MMI-anti and MMI-bidi, our
DAL-Dual obtains relatively satisfactory perfor-
mance on overall quality. It shows that the dual
signal can also improve the overall quality.
Case Study We present several cases in Fig-
ure 3. For the first case involving the content
on the mouse pad, most of the baselines gener-
ate generic responses such as“Come on!”, “Haha!”
or “It’s nothing!”. On the contrary, our DAL-
Dual and DAL-DuAd method produce much more
diverse and informative responses, such as “You
are so cute!” and “I also catch such an idea.”.
These two entertaining responses are also topi-
cally coherent and logically consistent with the
given query. In the second cases, our methods are
also capable of capturing the topic amazing coun-
try shown in the query, and well generate the di-
verse and coherent responses following the topic
of the query, such as “What an amazing country!”
or “It is really amazing!”. In contrast, the base-
lines still tend to provide safe responses lacking
diversity to different queries.

5.3 Comparison of Efficiency

Efficiency is a crucial factor for real-life appli-
cations such as online chatbots. We conduct an
experiment to evaluate the efficiency of all the
methods under study. The efficiency experiment
is conducted ten times on one Tesla K40m GPU
whose memory is 11471M. The average time con-
sumed by each method to generate the responses
for 1000 queries is reported in Figure 4. MMI-
bidi-5, MMI-bidi-10 and MMI-bidi-20 denote the
MMI-bidi method with the N-best size of 5, 10
and 20 respectively. We can see that MMI-anti
and GAN-AEL are the most time-consuming in all
the baselines. Besides, we note that MMI-bidi
method with the reranking strategy, even with a
relatively small N-best size of 5, consumes much

longer time than our methods, which severely lim-
its MMI-bidi’s application in practice. However,
Seq2Seq, Adver-REIN, DAL-Dual and DAL-DuAd
have very similar efficiency performance. Com-
pared with Seq2Seq and Adver-REIN, DAL-Dual
and DAL-DuAd achieve much better performance
on diversity and overall quality. Therefore, DAL
is more suitable for real-life applications.

0 500 1000 1500 2000

Time/seconds

Seq2Seq

MMI-anti

MMI-bidi-5

MMI-bidi-10

MMI-bidi-20

DAL-Dual

Adver-REIN

GAN-AEL

DAL-DuAd

180

1916

465

779

1457

172

182

1018

174

Figure 4: Time consumed by different methods.

6 Conclusion

We propose a novel framework named DAL to al-
leviate two prominent problems (safe responses
and unnatural responses) plaguing dialogue gen-
eration. The dual learning proposed in this paper
is the first effort to utilize the reverse dependency
between queries and responses to reduce the prob-
ability of safe response generation and improve
the diversity of the generated responses. Adver-
sarial learning makes the generated responses as
natural to human-generated ones as possible. DAL
seamlessly integrates dual learning and adversarial
learning, which are complementary to each other.
Experimental results show that DAL achieves bet-
ter performance than the state-of-the-art methods
in terms of diversity, overall quality and efficiency.



19

References
Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Ben-

gio. 2014. Neural machine translation by jointly
learning to align and translate. arXiv preprint
arXiv:1409.0473.

Joseph L Fleiss. 1971. Measuring nominal scale agree-
ment among many raters. Psychological bulletin,
76(5):378.

Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza,
Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron
Courville, and Yoshua Bengio. 2014. Generative ad-
versarial nets. In NIPS, pages 2672–2680.

Di He, Yingce Xia, Tao Qin, Liwei Wang, Nenghai Yu,
Tieyan Liu, and Wei-Ying Ma. 2016. Dual learning
for machine translation. In NIPS, pages 820–828.

Baotian Hu, Zhengdong Lu, Hang Li, and Qingcai
Chen. 2014. Convolutional neural network architec-
tures for matching natural language sentences. In
NIPS, pages 2042–2050.

Zongcheng Ji, Zhengdong Lu, and Hang Li. 2014. An
information retrieval approach to short text conver-
sation. arXiv preprint arXiv:1408.6988.

Taeksoo Kim, Moonsu Cha, Hyunsoo Kim, Jung Kwon
Lee, and Jiwon Kim. 2017. Learning to discover
cross-domain relations with generative adversarial
networks. In ICML, pages 1857–1865.

Diederik P Kingma and Jimmy Ba. 2014. Adam: A
method for stochastic optimization. arXiv preprint
arXiv:1412.6980.

Guillaume Klein, Yoon Kim, Yuntian Deng, Jean
Senellart, and Alexander Rush. 2017. Opennmt:
Open-source toolkit for neural machine translation.
Proceedings of ACL 2017, System Demonstrations,
pages 67–72.

Alex M Lamb, Anirudh Goyal ALIAS PARTH
GOYAL, Ying Zhang, Saizheng Zhang, Aaron C
Courville, and Yoshua Bengio. 2016. Professor
forcing: A new algorithm for training recurrent net-
works. In NIPS, pages 4601–4609.

Jiwei Li, Michel Galley, Chris Brockett, Jianfeng Gao,
and Bill Dolan. 2016. A diversity-promoting ob-
jective function for neural conversation models. In
NAACL-HLT, pages 110–119.

Jiwei Li, Will Monroe, Tianlin Shi, Sėbastien Jean,
Alan Ritter, and Dan Jurafsky. 2017. Adversarial
learning for neural dialogue generation. In EMNLP,
pages 2157–2169.

Chia-Wei Liu, Ryan Lowe, Iulian Serban, Mike Nose-
worthy, Laurent Charlin, and Joelle Pineau. 2016.
How not to evaluate your dialogue system: An em-
pirical study of unsupervised evaluation metrics for
dialogue response generation. In EMNLP, pages
2122–2132.

Zhengdong Lu and Hang Li. 2013. A deep architecture
for matching short texts. In NIPS, pages 1367–1375.

Thang Luong, Hieu Pham, and Christopher D Man-
ning. 2015. Effective approaches to attention-based
neural machine translation. In EMNLP, pages
1412–1421.

Lili Mou, Yiping Song, Rui Yan, Ge Li, Lu Zhang, and
Zhi Jin. 2016. Sequence to backward and forward
sequences: A content-introducing approach to gen-
erative short-text conversation. In COLING, pages
3349–3358.

Aaron van den Oord, Nal Kalchbrenner, Lasse Espe-
holt, Oriol Vinyals, Alex Graves, et al. 2016. Condi-
tional image generation with pixelcnn decoders. In
NIPS, pages 4790–4798.

Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a method for automatic eval-
uation of machine translation. In ACL, pages 311–
318.

Adam Paszke, Sam Gross, Soumith Chintala, Gre-
gory Chanan, Edward Yang, Zachary DeVito, Zem-
ing Lin, Alban Desmaison, Luca Antiga, and Adam
Lerer. 2017. Automatic differentiation in pytorch.
In NIPS-W.

Jinhua Peng, Zongyang Ma, Di Jiang, and Hua Wu.
2019. Integrating bayesian and neural networks for
discourse coherence. In Companion Proceedings of
the 2019 World Wide Web Conference. International
World Wide Web Conferences Steering Committee.

Alan Ritter, Colin Cherry, and William B Dolan. 2011.
Data-driven response generation in social media. In
EMNLP, pages 583–593.

Lifeng Shang, Zhengdong Lu, and Hang Li. 2015.
Neural responding machine for short-text conversa-
tion. In ACL, volume 1, pages 1577–1586.

Alessandro Sordoni, Michel Galley, Michael Auli,
Chris Brockett, Yangfeng Ji, Margaret Mitchell,
Jian-Yun Nie, Jianfeng Gao, and Bill Dolan. 2015.
A neural network approach to context-sensitive gen-
eration of conversational responses. In NAACL-
HLT, pages 196–205.

Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky,
Ilya Sutskever, and Ruslan Salakhutdinov. 2014.
Dropout: a simple way to prevent neural networks
from overfitting. The Journal of Machine Learning
Research, 15(1):1929–1958.

Ilya Sutskever, Oriol Vinyals, and Quoc V Le. 2014.
Sequence to sequence learning with neural net-
works. In NIPS, pages 3104–3112.

Richard S Sutton, David A McAllester, Satinder P
Singh, and Yishay Mansour. 2000. Policy gradi-
ent methods for reinforcement learning with func-
tion approximation. In NIPS, pages 1057–1063.



20

Duyu Tang, Nan Duan, Tao Qin, and Ming Zhou. 2017.
Question answering and question generation as dual
tasks. arXiv preprint arXiv:1706.02027.

Oriol Vinyals and Quoc Le. 2015. A neural conversa-
tional model. arXiv preprint arXiv:1506.05869.

Hao Wang, Zhengdong Lu, Hang Li, and Enhong
Chen. 2013. A dataset for research on short-text
conversations. In EMNLP, pages 935–945.

Ronald J Williams. 1992. Simple statistical gradient-
following algorithms for connectionist reinforce-
ment learning. Machine learning, 8(3-4):229–256.

Chen Xing, Wei Wu, Yu Wu, Jie Liu, Yalou Huang,
Ming Zhou, and Wei-Ying Ma. 2017. Topic aware
neural response generation. In AAAI, pages 3351–
3357.

Zhen Xu, Bingquan Liu, Baoxun Wang, SUN
Chengjie, Xiaolong Wang, Zhuoran Wang, and
Chao Qi. 2017. Neural response generation via gan
with an approximate embedding layer. In EMNLP,
pages 617–626.

Zili Yi, Hao Zhang, Ping Tan, and Minglun Gong.
2017. Dualgan: Unsupervised dual learning for
image-to-image translation. In ICCV, pages 2868–
2876. IEEE.

Hainan Zhang, Yanyan Lan, Jiafeng Guo, Jun Xu, and
Xueqi Cheng. 2018. Reinforcing coherence for se-
quence to sequence model in dialogue generation.
In IJCAI, pages 4567–4573.

Hao Zhou, Minlie Huang, Tianyang Zhang, Xiaoyan
Zhu, and Bing Liu. 2017. Emotional chatting
machine: Emotional conversation generation with
internal and external memory. arXiv preprint
arXiv:1704.01074.

Jun-Yan Zhu, Taesung Park, Phillip Isola, and Alexei A
Efros. 2017. Unpaired image-to-image transla-
tion using cycle-consistent adversarial networks. In
ICCV, pages 2242–2251. IEEE.


