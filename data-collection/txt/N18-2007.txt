



















































Neural Models for Reasoning over Multiple Mentions Using Coreference


Proceedings of NAACL-HLT 2018, pages 42–48
New Orleans, Louisiana, June 1 - 6, 2018. c©2018 Association for Computational Linguistics

Neural Models for Reasoning over Multiple Mentions using Coreference

Bhuwan Dhingra1 Qiao Jin2 Zhilin Yang1
William W. Cohen1 Ruslan Salakhutdinov1

1Carnegie Mellon University
2University of Pittsburgh

{bdhingra,zhiliny,wcohen,rsalakhu}@cs.cmu.edu, qij9@pitt.edu

Abstract

Many problems in NLP require aggregating in-
formation from multiple mentions of the same
entity which may be far apart in the text. Ex-
isting Recurrent Neural Network (RNN) lay-
ers are biased towards short-term dependen-
cies and hence not suited to such tasks. We
present a recurrent layer which is instead bi-
ased towards coreferent dependencies. The
layer uses coreference annotations extracted
from an external system to connect entity men-
tions belonging to the same cluster. Incorpo-
rating this layer into a state-of-the-art reading
comprehension model improves performance
on three datasets – Wikihop, LAMBADA and
the bAbi AI tasks – with large gains when
training data is scarce.

1 Introduction

A long-standing goal of NLP is to build systems
capable of reasoning about the information present
in text. One important form of reasoning for Ques-
tion Answering (QA) models is the ability to ag-
gregate information from multiple mentions of en-
tities. We call this coreference-based reasoning
since multiple pieces of information, which may
lie across sentence, paragraph or document bound-
aries, are tied together with the help of referring
expressions which denote the same real-world en-
tity. Figure 1 shows examples.

QA models which directly read text to answer
questions (commonly known as Reading Com-
prehension systems) (Hermann et al., 2015; Seo
et al., 2017a), typically consist of RNN layers.
RNN layers have a bias towards sequential re-
cency (Dyer, 2017), i.e. a tendency to favor
short-term dependencies. Attention mechanisms
alleviate part of the issue, but empirical studies
suggest RNNs with attention also have difficulty
modeling long-term dependencies (Daniluk et al.,
2017). We conjecture that when training data

is scarce, and inductive biases play an important
role, RNN-based models would have trouble with
coreference-based reasoning.

Context: [...] mary got the football there [...] mary
went to the bedroom [...] mary travelled to the hallway
[...]
Question: where was the football before the hallway ?

Context: Louis-Philippe Fiset [...] was a local physician
and politician in the Mauricie area [...] is located in the
Mauricie region of Quebec, Canada [...]
Question: country of citizenship – louis-philippe fiset ?

Figure 1: Example questions which require
coreference-based reasoning from the bAbi dataset
(top) and Wikihop dataset (bottom). Coreferences are
in bold, and the correct answers are underlined.

At the same time, systems for coreference res-
olution have seen a gradual increase in accuracy
over the years (Durrett and Klein, 2013; Wise-
man et al., 2016; Lee et al., 2017). Hence, in this
work we use the annotations produced by such
systems to adapt a standard RNN layer by intro-
ducing a bias towards coreferent recency. Specif-
ically, given an input sequence and coreference
clusters extracted from an external system, we in-
troduce a term in the update equations for Gated
Recurrent Units (GRU) (Cho et al., 2014) which
depends on the hidden state of the coreferent an-
tecedent of the current token (if it exists). This
way hidden states are propagated along corefer-
ence chains and the original sequence in parallel.

We compare our Coref-GRU layer with the reg-
ular GRU layer by incorporating it in a recent
model for reading comprehension. On synthetic
data specifically constructed to test coreference-
based reasoning (Weston et al., 2015), C-GRUs
lead to a large improvement over regular GRUs.
We show that the structural bias introduced and
coreference signals are both important to reach
high performance in this case. On a more re-

42



alistic dataset (Welbl et al., 2017), with noisy
coreference annotations, we see small but signif-
icant improvements over a state-of-the-art base-
line. As we reduce the training data, the gains be-
come larger. Lastly, we apply the same model to
a broad-context language modeling task (Paperno
et al., 2016), where coreference resolution is an
important factor, and show improved performance
over state-of-the-art.

2 Related Work

Entity-based models. Ji et al. (2017) presented
a generative model for jointly predicting the next
word in the text and its gold-standard corefer-
ence annotation. The difference in our work is
that we look at the task of reading comprehen-
sion, and also work in the more practical set-
ting of system extracted coreferences. EntNets
(Henaff et al., 2016) also maintain dynamic mem-
ory slots for entities, but do not use coreference
signals and instead update all memories after read-
ing each sentence, which leads to poor perfor-
mance in the low-data regime (c.f. Table 1). Yang
et al. (2017) model references in text as explicit
latent variables, but limit their work to text gen-
eration. Kobayashi et al. (2016) used a pooling
operation to aggregate entity information across
multiple mentions. Wang et al. (2017) also noted
the importance of reference resolution for read-
ing comprehension, and we compare our model to
their one-hot pointer reader.

Syntactic-recency. Recent work has used syn-
tax, in the form of dependency trees, to replace the
sequential recency bias in RNNs with a syntactic
recency bias (Tai et al., 2015; Swayamdipta, 2017;
Qian et al., 2017; Chen et al., 2017). However,
syntax only looks at dependencies within sentence
boundaries, whereas our focus here is on longer
ranges. Our resulting layer is structurally similar
to GraphLSTMs (Peng et al., 2017), with an addi-
tional attention mechanism over the graph edges.
However, while Peng et al. (2017) found that using
coreference did not lead to any gains for the task of
relation extraction, here we show that it has a pos-
itive impact on the reading comprehension task.
Self-Attention (Vaswani et al., 2017) models are
becoming popular for modeling long-term depen-
dencies, and may also benefit from coreference in-
formation to bias the learning of those dependen-
cies. Here we focus on recurrent layers and leave
such an analysis to future work.

Mary went … she 

… 

… 

Mary went … she 

… 

… 
hft�1

hft
hfyt

hbt0

hbt0+1xt xt0

hbyt0

Figure 2: Forward (left) and Backward (right) Coref-
GRU layers. Mary and she are coreferent.

Part of this work was described in an unpub-
lished preprint (Dhingra et al., 2017b). The cur-
rent paper extends that version and focuses exclu-
sively on coreference relations. We also report re-
sults on the WikiHop dataset, including the perfor-
mance of the model in the low-data regime.

3 Model

Coref-GRU (C-GRU) Layer. Suppose we are
given an input sequence w1, w2, . . . , wT along
with their word vectors x1, . . . , xT and annota-
tions for the most recent coreferent antecedent for
each token y1, . . . , yT , where yt ∈ {0, . . . , t − 1}
and yt = 0 denotes the null antecedent (for tokens
not belonging to any cluster). We assume all to-
kens belonging to a mention in a cluster belong to
that cluster, and there are C clusters in total. Our
recurrent layer is adapted from GRU cells (Cho
et al., 2014), but similar extensions can be derived
for other recurrent cells as well. The update equa-
tions in a GRU all take the same basic form given
by:

f(Wxt + Uht−1 + b).

The bias for sequential recency comes from the
second term Uht−1. In this work we add an-
other term to introduce a bias towards coreferent
recency instead:

f(Wxt+αtUφs(ht−1)+(1−αt)U ′φc(hyt)+ b),

where hyt is the hidden state of the coreferent an-
tecedent of wt (with h0 = 0), φs and φc are non-
linear functions applied to the hidden states com-
ing from the sequential antecedent and the coref-
erent antecedent, respectively, and αt is a scalar
weight which decides the relative importance of
the two terms based on the current input (so that,
for example, pronouns may assign a higher weight
for the coreference state). When yt = 0, αt is
set to 1, otherwise it is computed using a key-
based addressing scheme (Miller et al., 2016), as
αt = softmax(xTt k), where k is a trainable key

43



vector. In this work we use simple slicing func-
tions φs(x) = x[1 : d/2], and φc(x) = x[d/2 : d]
which decompose the hidden states into a sequen-
tial and a coreferent component, respectively. Fig-
ure 2 (left) shows an illustration of the layer, and
the full update equations are given in Appendix A.

Connection to Memory Networks. We can
also view the model as a memory network
(Sukhbaatar et al., 2015) with a memory state Mt
at each time step which is a C × d matrix. The
rows of this memory matrix correspond to the
state of each coreference cluster at time step t.
The main difference between Coref-GRUs and a
typical memory network such as EntNets (Henaff
et al., 2016) lies in the fact that we use corefer-
ence annotations to read and write from the mem-
ory rather than let model learn how to access the
memory. With Coref-GRUs, only the content of
the memories needs to be learned. As we shall see
in Section 4, this turns out to be a useful bias in
the low-data regime.

Bidirectional C-GRU. To extend to the bidi-
rectional case, a second layer is fed the same se-
quence in the reverse direction, xT , . . . , x1 and
yt ∈ {0, t + 1, . . . , T} now denotes the immedi-
ately descendent coreferent token from wt. Out-
puts from the two layers are then concatenated to
form the bi-directional output (see Figure 2).

Complexity. The resulting layer has the same
time-complexity as that of a regular GRU layer.
The memory complexity increases since we have
to keep track of the hidden states for each coref-
erence cluster in the input. If there are C clusters
and B is the batch size, the resulting complexity
is by O(BTCd). This scales linearly with the in-
put size T , however we leave exploration of more
efficient architectures to future work.

Reading comprehension architecture. All
tasks we look at involve tuples of the form
(p, q, a, C), where the goal is to find the answer
a from candidates C to question q with passage
p as context. We use the Gated-Attention (GA)
reader (Dhingra et al., 2017a) as a base architec-
ture, which computes representations of the pas-
sage by passing it through multiple bidirectional
GRU layers with an attention mechanism in be-
tween layers. We compare the original GA archi-
tecture (GA w/ GRU) with one where the bidirec-
tional GRU layers are replaced with bidirectional
C-GRU layers (GA w/ C-GRU). Performance is
reported in terms of the accuracy of detecting the

correct answer from C, and all models are trained
using cross-entropy loss. When comparing two
models we ensure the number of parameters are
the same in each. Other implementation details
are listed in Appendix B.

4 Experiments & Results

Method Avg Max # failed

EntNets (Henaff et al., 2016) – 0.704 15
QRN (Seo et al., 2017b) – 0.901 7

Bi-GRU 0.727 0.767 13
Bi-C-GRU 0.790 0.831 12
GA w/ GRU 0.764 0.810 10
GA w/ GRU + 1-hot 0.766 0.808 9
GA w/ C-GRU 0.870 0.886 5

Table 1: Accuracy on bAbi-1K, averaged across all 20
tasks. Following previous work we run each task for 10
random seeds, and report the Avg and Max (based on
dev set) performance. A task is considered failed if its
Max performance is < 0.95.

BAbi AI tasks. Our first set of experiments
are on the 1K training version of the synthetic
bAbi AI tasks (Weston et al., 2015). The pas-
sages and questions in this dataset are generated
using templates, removing many complexities in-
herent in natural language, but it still provides a
useful testbed for us since some tasks are specifi-
cally constructed to test the coreference-based rea-
soning we tackle here. Experiments on more nat-
ural data are described below.

Table 1 shows a comparison of EntNets (Henaff
et al., 2016), QRNs (Seo et al., 2017b) (the best
published results on bAbi-1K), and our models.
We also include the results for a single layer ver-
sion of GA Reader (which we denote simply as
Bi-GRU or Bi-C-GRU when using coreference)
to enable fair comparison with EntNets. In each
case we see clear improvements of using C-GRU
layers over GRU layers. Interestingly, EntNets,
which have>99% performance when trained with
10K examples only reach 70% performance with
1K training examples. The Bi-C-GRU model sig-
nificantly improves on this baseline, which shows
that, with less data, coreference annotations can
provide a useful bias for a memory network on
how to read and write memories.

A break-down of task-wise performance is
given in Appendix C. Comparing C-GRU to the
GRU based method, we find that the main gains
are on tasks 2 (two supporting facts), 3 (three sup-
porting facts) and 16 (basic induction). All these

44



0.0 0.1 0.2 0.3 0.4
% removed coreferences

0.5

0.6

0.7

0.8

0.9

1.0
A

cc
ur

ac
y

GA w/ GRU
GA w/ random­GRU
GA w/ C­GRU

Training progress
0.0

0.1

0.2

0.3

0.4

0.5

V
al

id
at

io
n

ex
p

(−
lo
ss

)

full C-GRU

full GRU

5K C-GRU

5K GRU

1K C-GRU

1K GRU

Figure 3: Left: Accuracy of GA w/ C-GRU as corefer-
ence annotations are removed for bAbi task 3. Right:
Expected probability of correct answer (exp (−loss))
on Validation set as training progresses on Wikihop
dataset for 1K, 5K and the full training datasets.

tasks require aggregation of information across
sentences to derive the answer. Comparing to the
QRN baseline, we found that C-GRU was signif-
icantly worse on task 15 (basic deduction). On
closer examination we found that this was because
our simplistic coreference module which matches
tokens exactly was not able to resolve “mice” to
“mouses” and “cats” to “cat”. On the other hand,
C-GRU was significantly better than QRN on task
16 (basic induction).

We also include a baseline which uses coref-
erence features as 1-hot vectors appended to the
input word vectors (GA w/ GRU + 1-hot). This
provides the model with information about the
coreference clusters, but does not improve perfor-
mance, suggesting that the regular GRU is unable
to track the given coreference information across
long distances to solve the task. On the other
hand, in Figure 3 (left) we show how the per-
formance of GA w/ C-GRU varies as we remove
gold-standard mentions from coreference clusters,
or if we replace them with random mentions (GA
w/ random-GRU). In both cases there is a sharp
drop in performance, showing that specifically us-
ing coreference for connecting mentions is impor-
tant.

Wikihop dataset. Next we apply our model to
the Wikihop dataset (Welbl et al., 2017), which is
specifically constructed to test multi-hop reading
comprehension across documents. Each instance
in this dataset consists of a collection of passages
(p1, . . . , pN ), and a query of the form (h, r) where
h is an entity and r is a relation. The task is to find
the tail entity t from a set of provided candidates
C. As preprocessing we concatenate all documents
in a random order, and extract coreference anno-

Method Follow
Follow
+single

Follow
+multiple Overall

Dev Dev Dev Dev Test

1K

GA w/ GRU 0.307 0.332 0.287 0.263 –
GA w/ C-GRU 0.355 0.370 0.354 0.330 –

5K

GA w/ GRU 0.382 0.385 0.390 0.336 –
GA w/ C-GRU 0.452 0.454 0.460 0.401 –

full

BiDAF – – – – 0.429
GA w/ GRU 0.606 0.615 0.604 0.549 –
GA w/ C-GRU 0.614 0.616 0.614 0.560† 0.593

Table 2: Accuracy on Wikihop. Follow: annotated as
answer follows from the given passages. Follow +mul-
tiple: annotated as requiring multiple passages for an-
swering. Follow +single annotated as requiring one
passage for answering. †p = 0.057 using Mcnemar’s
test compared to GA w/ GRU.

tations from the Berkeley Entity Resolution sys-
tem (Durrett and Klein, 2013) which gets about
62% F1 score on the CoNLL 2011 test set. We
only keep the coreference clusters which contain
at least one candidate from C or an entity which
co-occurs with the head entity h. We report results
in Table 2 when using the full training set, as well
as when using a reduced training set of sizes 1K
and 5K, to test the model under a low-data regime.
In Figure 3 we also show the training curves of
exp (−loss) on the validation set.

We see higher performance for the C-GRU
model in the low data regime, and better gen-
eralization throughout the training curve for all
three settings. This supports our conjecture that
the GRU layer has difficulty learning the kind
of coreference-based reasoning required in this
dataset, and that the bias towards coreferent re-
cency helps with that. However, perhaps sur-
prisingly, given enough data both models per-
form comparably. This could either indicate that
the baseline learns the required reasoning patterns
when given enough data, or, that the bias towards
corefence-based reasoning hurts performance for
some other types of questions. Indeed, there are
9% questions which are answered correctly by the
baseline but not by C-GRU, however, we did not
find any consistent patterns among these in our
analyses. Lastly, we note that both models vastly
outperform the best reported result of BiDAf from

45



(Welbl et al., 2017)1. We believe this is because
the GA models select answers from the list of
candidatees, whereas BiDAF ignores those candi-
dates.

Method overall context

Chu et al. (2017) 0.4900 –
GA w/ GRU 0.5398 0.6677
GA w/ GRU + 1-hot 0.5338 0.6603
GA w/ C-GRU 0.5569 0.6888†

Table 3: Accuracy on LAMBADA test set, averaged
across two runs with random initializations. context:
passages for which the answer is in context. overall:
full test set for comparison to prior work. †p < 0.0001
using Mcnemar’s test compared to GA w/ GRU.

LAMBADA dataset. Our last set of exper-
iments is on the broad-context language model-
ing task of LAMBADA dataset (Paperno et al.,
2016). This dataset consists of passages 4-5 sen-
tences long, where the last word needs to be pre-
dicted. Interestingly, though, the passages are fil-
tered such that human volunteers were able to pre-
dict the missing token given the full passage, but
not given only the last sentence. Hence, predict-
ing these tokens involves a broader understanding
of the whole passage. Analysis of the questions
(Chu et al., 2017) suggests that around 20% of the
questions need coreference understanding to an-
swer correctly. Hence, we apply our model which
uses coreference information for this task.

We use the same setup as Chu et al. (2017)
which formulated the problem as a reading com-
prehension one by treating the last sentence as
query, and the remaining passage as context to ex-
tract the answer from. In this manner only 80% of
the questions are answerable, but the performance
increases substantially compared to pure language
modeling based approaches. For this dataset we
used Stanford CoreNLP to extract coreferences
(Clark and Manning, 2015), which achieved 0.63
F1 on the CoNLL test set. Table 3 shows a com-
parison of the GA w/ GRU baseline and GA w/ C-
GRU models. We see a significant gain in perfor-
mance when using the layer with coreference bias.
Furthermore, the 1-hot baseline which uses the
same coreference information, but with sequential
recency bias fails to improve over the regular GRU

1The official leaderboard at http://qangaroo.cs.
ucl.ac.uk/leaderboard.html shows two models
with better performance than reported here (as of April 2018).
Since we were unable to find publications for these models
we omit them here.

layer. While the improvement for C-GRU is small,
it is significant, and we note that questions in this
dataset involve several different types of reasoning
out of which we only tackle one specific kind. The
proposed GA w/ C-GRU layer sets a new state-of-
the-art on this dataset.

5 Conclusion

We present a recurrent layer with a bias towards
coreferent recency, with the goal of tackling read-
ing comprehension problems which require aggre-
gating information from multiple mentions of the
same entity. Our experiments show that when
combined with a powerful reading architecture,
the layer provides a useful inductive bias for solv-
ing problems of this kind. In future work, we aim
to apply this model to other problems where long-
term dependencies at the document level are im-
portant. Noise in the coreference annotations has
a detrimental effect on the performance (Figure 3),
hence we also aim to explore joint models which
learn to do coreference resolution and reading to-
gether.

Acknowledgments

This work was supported by NSF under grants
CCF-1414030 and IIS-1250956 and by grants
from Google.

References
Huadong Chen, Shujian Huang, David Chiang, and Ji-

ajun Chen. 2017. Improved neural machine transla-
tion with a syntax-aware encoder and decoder. ACL.

Kyunghyun Cho, Bart Van Merriënboer, Caglar Gul-
cehre, Dzmitry Bahdanau, Fethi Bougares, Holger
Schwenk, and Yoshua Bengio. 2014. Learning
phrase representations using rnn encoder-decoder
for statistical machine translation. arXiv preprint
arXiv:1406.1078.

Zewei Chu, Hai Wang, Kevin Gimpel, and David
McAllester. 2017. Broad context language model-
ing as reading comprehension. EACL.

Kevin Clark and Christopher D Manning. 2015. Entity-
centric coreference resolution with model stacking.
In Proceedings of the 53rd Annual Meeting of the
Association for Computational Linguistics and the
7th International Joint Conference on Natural Lan-
guage Processing (Volume 1: Long Papers), vol-
ume 1, pages 1405–1415.

Michał Daniluk, Tim Rocktäschel, Johannes Welbl,
and Sebastian Riedel. 2017. Frustratingly short at-
tention spans in neural language modeling. ICLR.

46



Bhuwan Dhingra, Hanxiao Liu, Zhilin Yang,
William W Cohen, and Ruslan Salakhutdinov.
2017a. Gated-attention readers for text comprehen-
sion. ACL.

Bhuwan Dhingra, Zhilin Yang, William W Cohen, and
Ruslan Salakhutdinov. 2017b. Linguistic knowl-
edge as memory for recurrent neural networks.
arXiv preprint arXiv:1703.02620.

Greg Durrett and Dan Klein. 2013. Easy victories and
uphill battles in coreference resolution. In EMNLP,
pages 1971–1982.

Chris Dyer. 2017. Should neural network architecture
reflect linguistic structure? CoNLL Keynote.

Mikael Henaff, Jason Weston, Arthur Szlam, Antoine
Bordes, and Yann LeCun. 2016. Tracking the world
state with recurrent entity networks. arXiv preprint
arXiv:1612.03969.

Karl Moritz Hermann, Tomas Kocisky, Edward
Grefenstette, Lasse Espeholt, Will Kay, Mustafa Su-
leyman, and Phil Blunsom. 2015. Teaching ma-
chines to read and comprehend. In Advances in Neu-
ral Information Processing Systems, pages 1693–
1701.

Yangfeng Ji, Chenhao Tan, Sebastian Martschat, Yejin
Choi, and Noah A Smith. 2017. Dynamic entity rep-
resentations in neural language models. EMNLP.

Sosuke Kobayashi, Ran Tian, Naoaki Okazaki, and
Kentaro Inui. 2016. Dynamic entity representation
with max-pooling improves machine reading. In
Proceedings of the 2016 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics: Human Language Technologies,
pages 850–855.

Kenton Lee, Luheng He, Mike Lewis, and Luke Zettle-
moyer. 2017. End-to-end neural coreference resolu-
tion. EMNLP.

Alexander Miller, Adam Fisch, Jesse Dodge, Amir-
Hossein Karimi, Antoine Bordes, and Jason We-
ston. 2016. Key-value memory networks for
directly reading documents. arXiv preprint
arXiv:1606.03126.

Denis Paperno, Germán Kruszewski, Angeliki Lazari-
dou, Quan Ngoc Pham, Raffaella Bernardi, San-
dro Pezzelle, Marco Baroni, Gemma Boleda, and
Raquel Fernández. 2016. The lambada dataset:
Word prediction requiring a broad discourse context.
ACL.

Nanyun Peng, Hoifung Poon, Chris Quirk, Kristina
Toutanova, and Wen-tau Yih. 2017. Cross-sentence
n-ary relation extraction with graph lstms. Transac-
tions of the Association for Computational Linguis-
tics, 5:101–115.

Jeffrey Pennington, Richard Socher, and Christo-
pher D. Manning. 2014. Glove: Global vectors for
word representation. In Empirical Methods in Nat-
ural Language Processing (EMNLP), pages 1532–
1543.

Feng Qian, Lei Sha, Baobao Chang, Lu-chen Liu,
and Ming Zhang. 2017. Syntax aware lstm model
for chinese semantic role labeling. arXiv preprint
arXiv:1704.00405.

Minjoon Seo, Aniruddha Kembhavi, Ali Farhadi, and
Hannaneh Hajishirzi. 2017a. Bidirectional attention
flow for machine comprehension. ICLR.

Minjoon Seo, Sewon Min, Ali Farhadi, and Hannaneh
Hajishirzi. 2017b. Query-reduction networks for
question answering. ICLR.

Sainbayar Sukhbaatar, Jason Weston, Rob Fergus, et al.
2015. End-to-end memory networks. In Advances
in neural information processing systems, pages
2440–2448.

Swabha Swayamdipta. 2017. Learning Algorithms for
Broad-Coverage Semantic Parsing. Ph.D. thesis,
Carnegie Mellon University Pittsburgh, PA.

Kai Sheng Tai, Richard Socher, and Christopher D
Manning. 2015. Improved semantic representations
from tree-structured long short-term memory net-
works. ACL.

Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob
Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz
Kaiser, and Illia Polosukhin. 2017. Attention is all
you need. NIPS.

Hai Wang, Takeshi Onishi, Kevin Gimpel, and David
McAllester. 2017. Emergent logical structure in
vector representations of neural readers. 2nd Work-
shop on Representation Learning for NLP, ACL.

Dirk Weissenborn, Georg Wiese, and Laura Seiffe.
2017. Fastqa: A simple and efficient neural archi-
tecture for question answering. CoNLL.

Johannes Welbl, Pontus Stenetorp, and Sebastian
Riedel. 2017. Constructing datasets for multi-hop
reading comprehension across documents. arXiv
preprint arXiv:1710.06481.

Jason Weston, Antoine Bordes, Sumit Chopra, Alexan-
der M Rush, Bart van Merriënboer, Armand Joulin,
and Tomas Mikolov. 2015. Towards ai-complete
question answering: A set of prerequisite toy tasks.
arXiv preprint arXiv:1502.05698.

Sam Wiseman, Alexander M Rush, and Stuart M
Shieber. 2016. Learning global features for coref-
erence resolution. NAACL.

Zichao Yang, Phil Blunsom, Chris Dyer, and Wang
Ling. 2017. Reference-aware language models.
EMNLP.

47



A C-GRU update equations

For simplicity, we introduce the variablemt which
concatenates (||) the sequential and coreferent hid-
den states:

mt = αtφs(ht−1)||(1− αt)φc(hyt)

Then the update equations are given by:

rt = σ(W
rxt + U

rmt + b
r)

zt = σ(W
zxt + U

zmt + b
z)

h̃t = tanh(W
hxt + rt � Uhmt + bh)

ht = (1− zt)�mt + zth̃t

The attention parameter αt is given by:

αt =
expxTt k1

expxTt k1 + expx
T
t k2

where k1 and k2 are trainable key vectors.

B Implementation details

We use K = 3 layers with the GA architecture.
We keep the same hyperparameter settings when
using GRU or C-GRU layers, which we describe
here.

For the bAbi dataset, we use a hidden state size
of d = 64, batch size of B = 32, and learning rate
0.01 which is halved after every 120 updates. We
also use dropout with rate 0.1 at the output of each
layer. The maximum number of coreference clus-
ters across all tasks was C = 13. Half of the tasks
in this dataset are extractive, meaning the answer
is present in the passage, whereas the other half
are classification tasks, where the answer is in a
list of candidates which may not be in the passage.
For the extractive tasks, we use the attention sum
layer as described in the GA Reader paper (Dhin-
gra et al., 2017a). For the classification tasks we
replace this with a softmax layer for predicting one
of the classes.

For the Wikihop dataset, we use a hidden state
size of d = 64, batch size B = 16, and learn-
ing rate of 0.0005 which was halved every 2500
updates. The maximum number of coreference
clusters was set to 50 for this dataset. We used
dropout of 0.2 in between the intermediate layers,
and initialized word embeddings with Glove (Pen-
nington et al., 2014). We also used character em-
beddings, which were concatenated with the word
embeddings, of size 10. These were output from a

CNN layer with 50 filters each of width 5. Follow-
ing (Weissenborn et al., 2017), we also appended
a feature to the word embeddings in the passage
which indicated if the token appeared in the query
or not.

For the LAMBADA dataset, we use a hidden
state size of d = 256, batch size of B = 64, and
learning rate of 0.0005 which was halved every 2
epochs. Word vectors were initialized with Glove,
and dropout of 0.2 was applied after intermediate
layers. The maximum number of coreference clus-
ters in this dataset was 15.

C Task-wise bAbi performance

Task QRN GA w/GRU
GA w/
C-GRU

1: Single Supporting Fact 1.000 0.997 1.000
2: Two Supporting Facts 0.993 0.345 0.990
3: Three Supporting Facts 0.943 0.558 0.982
4: Two Argument Relations 1.000 1.000 1.000
5: Three Argument Relations 0.989 0.989 0.993
6:Yes/No Questions 0.991 0.962 0.976
7: Counting 0.904 0.946 0.976
8: Lists / Sets 0.944 0.947 0.964
9: Simple Negation 1.000 0.991 0.990
10: Indefinite Knowledge 1.000 0.992 0.986
11: Basic Coreference 1.000 0.995 0.996
12: Conjunction 1.000 1.000 0.996
13: Compound Coreference 1.000 0.998 0.993
14: Time Reasoning 0.992 0.895 0.849
15: Basic Deduction 1.000 0.521 0.470
16: Basic Induction 0.470 0.488 0.999
17: Positional Reasoning 0.656 0.580 0.574
18: Size Reasoning 0.921 0.908 0.896
19: Path Finding 0.213 0.095 0.099
20: Agent’s Motivation 0.998 0.998 1.000

Average 0.901 0.810 0.886

Table 4: Breakdown of task-wise performance on bAbi
dataset. Tasks where C-GRU is significant better /
worse than either GRU or QRNs are highlighted.

48


