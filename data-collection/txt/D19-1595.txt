



















































Feature2Vec: Distributional semantic modelling of human property knowledge


Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing
and the 9th International Joint Conference on Natural Language Processing, pages 5853–5859,
Hong Kong, China, November 3–7, 2019. c©2019 Association for Computational Linguistics

5853

Feature2Vec: Distributional semantic modelling of human property
knowledge

Steven Derby Paul Miller Barry Devereux

Queen’s University Belfast, Belfast, United Kingdom
{sderby02, p.miller, b.devereux}@qub.ac.uk

Abstract
Feature norm datasets of human conceptual
knowledge, collected in surveys of human
volunteers, yield highly interpretable models
of word meaning and play an important role
in neurolinguistic research on semantic cog-
nition. However, these datasets are limited
in size due to practical obstacles associated
with exhaustively listing properties for a large
number of words. In contrast, the develop-
ment of distributional modelling techniques
and the availability of vast text corpora have
allowed researchers to construct effective vec-
tor space models of word meaning over large
lexicons. However, this comes at the cost
of interpretable, human-like information about
word meaning. We propose a method for map-
ping human property knowledge onto a dis-
tributional semantic space, which adapts the
word2vec architecture to the task of modelling
concept features. Our approach gives a mea-
sure of concept and feature affinity in a sin-
gle semantic space, which makes for easy and
efficient ranking of candidate human-derived
semantic properties for arbitrary words. We
compare our model with a previous approach,
and show that it performs better on several
evaluation tasks. Finally, we discuss how
our method could be used to develop efficient
sampling techniques to extend existing feature
norm datasets in a reliable way.

1 Introduction

Distributional semantic modelling of word mean-
ing has become a popular method for building
pretrained lexical representations for downstream
Natural Language Processing (NLP) tasks (Baroni
and Lenci, 2010; Mikolov et al., 2013; Penning-
ton et al., 2014; Levy and Goldberg, 2014; Bo-
janowski et al., 2017). In this approach, meaning
is encoded in a dense vector space model, such that
words (or concepts) that have vector representa-
tions that are spatially close together are similar in

meaning. A criticism of these real-valued embed-
ding vectors is the opaqueness of their representa-
tional dimensions and their lack of cognitive plau-
sibility and interpretability (Murphy et al., 2012;
Şenel et al., 2018). In contrast, human conceptual
property knowledge is often modelled in terms of
relatively sparse and interpretable vectors, based
on verbalizable, human-elicited features collected
in property knowledge surveys (McRae et al.,
2005; Devereux et al., 2014). However, gather-
ing and collating human-elicited property knowl-
edge for concepts is very labour intensive, limiting
both the number of words for which a rich fea-
ture set can be gathered, as well as the complete-
ness of the feature listings for each word. Neural
embedding models, on the other hand, learn from
large corpora of text in an unsupervised fashion,
allowing very detailed, high-dimensional semantic
models to be constructed for a very large number
of words. In this paper, we propose Feature2Vec,
a computational framework that combines infor-
mation from human-elicited property knowledge
and information from distributional word embed-
dings, allowing us to exploit the strengths and ad-
vantages of both approaches. Feature2Vec maps
human property norms onto a pretrained vector
space model of word meaning. The embedding
of feature-based information in the pretrained em-
bedding space makes it possible to rank the rele-
vance of features using cosine similarity, and we
demonstrate how simple composition of features
can be used to approximate concept vectors.

2 Related Work

Several property-listing studies have been con-
ducted with human participants in order to build
property norms – datasets of normalized human-
verbalizable feature listings for lexical concepts
(McRae et al., 2005; Devereux et al., 2014).



5854

One use of feature norms is to critically exam-
ine distributional semantic models on their abil-
ity to encode grounded, human-elicited seman-
tic knowledge. For example, Rubinstein et al.
(2015) demonstrated that state-of-the-art distribu-
tional semantic models fail to predict attributive
properties of concept words (e.g. the properties
is-red and is-round for the word apple) as ac-
curately as taxonomic properties (e.g. is-a-fruit).
Similarly, Sommerauer and Fokkens (2018) inves-
tigated the types of semantic knowledge encoded
within pretrained word embeddings, concluding
that some properties cannot be learned by super-
vised classifiers. Collell and Moens (2016) com-
pared linguistic and visual representations of ob-
ject concepts on their ability to represent different
types of property knowledge. Research has shown
that state-of-the-art distributional semantic mod-
els built from text corpora fail to capture impor-
tant aspects of meaning related to grounded per-
ceptual information, as this kind of information is
not adequately represented in the statistical regu-
larities of text data (Li and Gauthier, 2017; Kelly
et al., 2014). Motivated by these issues, Silberer
(2017) constructed multimodal semantic models
from text and image data, with the goal of ground-
ing word meaning using visual attributes. More
recently, Derby et al. (2018) built similar models
with the added constraint of sparsity, demonstrat-
ing that sparse multimodal vectors provide a more
faithful representation of human semantic repre-
sentations. Finally, the work that most resembles
ours is that of Fagarasan et al. (2015), who use
Partial Least Squares Regression (PLSR) to learn a
mapping from a word embedding model onto spe-
cific conceptual properties. Concurrent work re-
cently undertaken by Li and Summers-Stay (2019)
replaces the PLSR model with a feedforward neu-
ral network. In our work, we instead map prop-
erty knowledge directly into vector space models
of word meaning, rather than learning a supervised
predictive function from concept embedding di-
mensions to feature terms.

3 Method

We make primary comparison with the work of
Fagarasan et al. (2015), although their approach
differs from ours in that they map from an embed-
ding space onto the feature space, while we learn
a mapping from the feature domain onto the em-
bedding space. We outline both methods below.

3.1 Distributional Semantic Models

For our experiments, we make use of the pre-
trained GloVe embeddings (Pennington et al.,
2014) provided in the Spacy1 package trained on
the Common Crawl2. The GloVe model includes
685, 000 tokens with embedding vectors of di-
mension 300, providing excellent lexical coverage
with a rich set of semantic representations.

In our analyses we use both the McRae property
norms (McRae et al., 2005), which contain 541
concepts and 2526 features, and the CSLB norms
(Devereux et al., 2014) which have 638 concepts
with 2725 features. For both sets of norms, a fea-
ture is listed for a concept if it has been elicited by
five or more participants in the property norming
study. The number of participants listing a given
feature for a given concept name is termed the pro-
duction frequency for that concept×feature pair.
This gives sparse production frequency vectors for
each concept over all the features in the norms.

3.2 Partial Least Square Regression (PLSR)

Fagarasan et al. (2015) used partial least squares
regression (PLSR) to map between the GloVe em-
bedding space and property norm vectors. Sup-
pose we have two real-valued matricesG ∈ Rn×m
and F ∈ Rn×k. In this context, G and F represent
GloVe embedding vectors and property norm fea-
ture vectors, respectively. For n available concept
words, G is a matrix which consists of stacked
pretrained embeddings from GloVe and F is the
(sparse) matrix of production frequencies for each
concept×feature pair. G and F share the same
row indexing for concept words. For a new di-
mension size p ∈ N, a partial least squared regres-
sion learns two new subspaces with dimensions
n × p, which have maximal covariance between
them. The algorithm solves this problem by learn-
ing a mapping from the matrix G onto F , similar
to a regression model. The fitted regression model
thus provides a framework for predicting vectors
in the feature space from vectors in the embedding
space.

In this work, we use the PLSR approach as a
baseline for our model. In implementing PLSR,
we set the intermediate dimension size to 50, fol-
lowing Fagarasan et al. (2015). We also build a
PLSR model using 120 dimensions, which in pre-
liminary experimentation we found gave the best

1https://spacy.io/models/en#en core web lg
2http://commoncrawl.org/



5855

Figure 1: Diagram illustrating the relationship of Feature2Vec to the standard Skip-Gram Word2Vec architecture.
(a) The standard Skip-Gram Word2Vec architecture with negative sampling. Word and context embeddings are
randomly initialized, and both sets of embeddings are trainable. (b) Our Feature2Vec architecture, which uses
pretrained word embeddings that are fixed during training. The network learns semantic feature embeddings for
feature labels in property norm datasets instead of context embeddings.

performance from a range of values tested.

3.3 Skip-Gram Word2Vec
Mikolov et al. (2013) proposed learning word em-
beddings using a predictive neural-network ap-
proach. In particular, the skip-gram implemen-
tation with negative sampling mines word co-
occurrences within a text window, and the net-
work must learn to predict the surrounding context
from the target word. More specifically, for a vo-
cabulary V , two sets of embeddings are learned
through gradient decent, one for target embed-
dings and one for context embeddings. Given a
target word w ∈ V and a context word c ∈ V in
its window, the network calculates the dot product
for the embeddings for w and v and a sigmoid ac-
tivation is applied to the output (Fig. 1(a)). Nega-
tive samples are also generated for training, where
the context is not in the target word’s window. Let
(w, c) ∈ D be the positive word and context pairs
and (w, c) ∈ D′ be the negative word and context
pairs. Then, using binary cross entropy loss, we
learn a parameterization θ of the neural network
that maximizes the function

J (θ) =
∑

(w,c)∈D

log(σ(vc · vw))

+
∑

(w,c)∈D′
log(σ(−vc · vw))

where σ is the sigmoid function and vw and vc
are the corresponding real-valued embeddings for
the target words and context words (Goldberg and
Levy, 2014).

In this work, we adapt this skip-gram approach
to the task of constructing semantic representa-
tions of human property norms by mapping prop-
erties into an embedding space (Figure 1). We
achieve this by using a neural network to predict
the properties from the input word, using the skip-
gram architecture with negative sampling on the
properties. We replace context embeddings and
windowed co-occurrence counts from the conven-
tional skip-gram architecture with property em-
beddings and concept-feature production frequen-
cies. The loss function for training remains the
same; however, there are two modifications to the
learning process. The first is that the target embed-
dings for the concept words are pre-trained (i.e.
the GloVe embeddings), and gradients are not ap-
plied to this embedding matrix. The layer for the
property norms is randomly initialized, and gra-
dients are applied to these vectors to learn a se-
mantic representation for properties aligned to the
pre-trained distributional semantic space for the
words. Secondly, the negative samples are gen-
erated from randomly sampled properties. We
downweight negative samples by multiplying their
associated loss by one over the negative sampling
rate, so that the system pays more attention to
real cases and less to the incorrect negative exam-
ples. Due to the sparsity of word-feature produc-
tion frequencies, we generate all positive instances
and randomly sample negative examples after each
epoch to create a new set of training samples. We
name this approach Feature2Vec3. We use a learn-

3The code for the model is available at
https://github.com/stevend94/Feature2Vec



5856

McRae CSLB
Model Top 1 Top 5 Top 10 Top 20 Top 1 Top 5 Top 10 Top 20

PLSR 50 3.55 14.18 29.79 47.52 2.90 23.19 44.20 60.87
PLSR 120 3.55 25.53 43.26 53.90 7.25 34.78 55.80 71.74

Feature2Vec 4.96 34.75 45.39 60.99 4.35 36.23 53.62 76.09

Table 1: Accuracy scores (percent) for retrieving the correct concept from amongst the top N most similar nearest
neighbours. For PLSR, predicted feature vectors for test concepts are compared to actual feature vectors. For
Feature2Vec, predicted concept word embeddings are compared to the actual GloVe embedding vectors.

ing rate of 0.001 and Adam optimization to train
for 120 epochs. We use a negative sampling rate
of 20 for both the McRae and CLSB datasets. In
contrast to the PSLR approach, Feature2Vec learns
a mapping from the feature norms onto the GloVe
embedding space.

4 Experiments

We train with the McRae and CSLB property
norms separately and report evaluations for each
dataset. For the McRae dataset we use 400 ran-
domly selected concepts for training and the re-
maining 141 for testing, and for the CSLB dataset
we use 500 randomly selected concepts for train-
ing and the remaining 138 for testing4.

4.1 Predicting Feature Vectors

We first evaluate how well the baseline PLSR
model performs on the feature vector reconstruc-
tion task used by Fagarasan et al. (2015). In this
evaluation, the feature vector for a test concept is
predicted and we test whether the real concept vec-
tor is within the top N most similar neighbours of
the predicted vector. We report results over both
50 (as in Fagarasan et al. (2015)) and 120 dimen-
sions for a range of values of N (Table 1).

4.2 Constructing Concept Representations
from Feature Vectors

For Feature2Vec, we embed property norm fea-
tures into the GloVe semantic space, giving a rep-
resentation of properties in terms of GloVe dimen-
sions. To predict a held-out concept embedding,
we build a representation of the concept word
by averaging the learned feature embedding vec-
tors for that word using the ground truth informa-
tion from the property norm dataset. This gives
a method to construct embeddings for new words

4We use the Python pickle package to store the numpy
state for reproducible results in our code.

McRae CSLB
Model Train Test Train Test

PLSR 50 49.52 31.67 50.58 40.25
PLSR 120 68.66 32.97 65.42 40.71

Feature2Vec 90.70 35.33 90.31 44.30

Table 2: The average percentage of features that each
method can predict for a given concept vector.

using property knowledge and associated produc-
tion frequencies (for example, for a held-out word
unicorn, its GloVe embedding vector might be
predicted from all features of horse, along with the
features is-white, has-a-horn, and is-fantastical).
We compare these predicted embeddings to the
held-out Glove embeddings (Table 1). However,
we note that this approach is different to the PLSR
models, so we do not make a direct compari-
son between PLSR and Feature2vec nearest neigh-
bour results. Nevertheless, the results show that
the word embeddings composed from the learned
Feature2Vec feature embeddings appear relatively
frequently amongst the most similar neighbour
words in the pretrained GloVe space, indicating
that feature embedding composition approximates
the original word embedddings reasonably well.

4.3 Predicting Property Knowledge

The evaluation task that we are most interested in
is how well the models can predict feature knowl-
edge for concepts, given the distributional seman-
tic vectors. More specifically, for a given con-
cept with K features, we wish to take the top K
predicted features according to each method, and
record the overlap with the true property norm list-
ing. In this evaluation, we make direct compar-
isons between all three models (PLSR 50, PLSR
120, & Feature2Vec). For the PLSR models, we
predict the feature vector for a given target word
using the embedding vector as input and take the
top K weighted features. For Feature2Vec, we



5857

Concepts Properties

Kingfisher
has wings does fly has a beak has feathers is a bird
does eat has a tail does swim has legs does lay eggs

Avocado
is eaten edible is tasty does grow is green is healthy is used in cooking
has skin peel is red is food is a vegetable

Door
made of metal has a door doors is useful has a handle handles made of wood
made of plastic is heavy is furniture does contain hold is found in kitchens

Dragon
is big large is an animal has a tail does eat is dangerous
has legs has claws is grey is small does fly

Table 3: Top 10 predictions from CSLB-trained Feature2Vec model. The properties in bold and underlined are
those that agree with the available ground truth features from the CSLB dataset. The first two concepts are from
the CSLB test set, whilst the final two words were randomly sampled from the word embedding lexicon.

rank all feature embeddings by their distance to
the embedding for the target word, using cosine
similarity, and take the top K most similar fea-
tures (Table 2). The results demonstrate that Fea-
ture2Vec outperforms the PLSR models on prop-
erty knowledge prediction, for both training and
testing datasets.

4.4 Analysis

Following previous work, we provide the top 10
feature predictions for a few sample concepts, dis-
played in Table 3. Properties underlined and in
bold represent features that match the available
ground truth data (i.e., the concept×feature pair
occurs in the norms). The first two words in Ta-
ble 3 were sampled from the CSLB norms test set,
whilst the last two words were randomly sampled
from the word embedding lexicon and are not con-
cept words appearing in the CSLB norms. We
find that the predicted features that are not con-
tained within the ground truth property set still
tend to be quite reasonable, even for the two con-
cepts not in the test dataset. As property norms
do not represent an exhaustive listing of prop-
erty knowledge, this is not surprising, and pre-
dicted properties not in the norms are not nec-
essarily errors (Devereux et al., 2009; Fagarasan
et al., 2015). Moreover, the set of features used
within the norms are dependent on the concepts
that were presented to the human participants. It
is therefore notable that the conceptual represen-
tations predicted by our model for the two out-
of-norms concept words are particularly plausible,

even though the attributes were never intended to
conceptually represent these words. Our analy-
sis supports the view that such supervised models
could be utilised as an assistive tool for surveying
much larger vocabularies of words.

5 Conclusion

We proposed a method for constructing distribu-
tional semantic vectors for human property norms
from a pretrained vector space model of word
meaning, which outperforms previous methods
for predicting concept features on two property
norm datasets. As discussed by Fagarasan et al.
(2015) and others, it is clear that property norm
datasets provide only a semi-complete picture of
human conceptual knowledge, and more extensive
surveys may provide additional useful property
knowledge information. By predicting plausible
semantic features for concepts through the lever-
aging of corpus-derived word embedding data, our
method offers a useful tool for guiding the ex-
pensive and laborious process of collecting prop-
erty norm listings. For example, existing prop-
erty norm datasets can be extended through human
verification of features predicted with high confi-
dence by Feature2Vec, with these features being
added to the norms and subsequently incorporated
into Feature2Vec in an iterative, semi-supervised
manner (Kelly et al., 2012). Thus, Feature2Vec
provides a useful heuristic to add interpretable
feature-based information to these datasets for
new words in a practical and efficient way.



5858

References
Marco Baroni and Alessandro Lenci. 2010. Dis-

tributional memory: A general framework for
corpus-based semantics. Computational Linguis-
tics, 36(4):673–721.

Piotr Bojanowski, Edouard Grave, Armand Joulin, and
Tomas Mikolov. 2017. Enriching word vectors with
subword information. Transactions of the Associa-
tion for Computational Linguistics, 5:135–146.

Guillem Collell and Marie-Francine Moens. 2016. Is
an image worth more than a thousand words? on the
fine-grain semantic differences between visual and
linguistic representations. In Proceedings of COL-
ING 2016, the 26th International Conference on
Computational Linguistics: Technical Papers, pages
2807–2817.

Steven Derby, Paul Miller, Brian Murphy, and Barry
Devereux. 2018. Using sparse semantic embed-
dings learned from multimodal text and image data
to model human conceptual knowledge. In Pro-
ceedings of the 22nd Conference on Computational
Natural Language Learning, pages 260–270, Brus-
sels, Belgium. Association for Computational Lin-
guistics.

Barry Devereux, Nicholas Pilkington, Thierry Poibeau,
and Anna Korhonen. 2009. Towards unrestricted,
large-scale acquisition of feature-based conceptual
representations from corpus data. Research on Lan-
guage and Computation, 7(2):137–170.

Barry J Devereux, Lorraine K Tyler, Jeroen Geertzen,
and Billi Randall. 2014. The Centre for Speech,
Language and the Brain (CSLB) concept property
norms. Behavior Research Methods, 46(4):1119–
1127.

Luana Fagarasan, Eva Maria Vecchi, and Stephen
Clark. 2015. From distributional semantics to fea-
ture norms: grounding semantic models in human
perceptual data. In Proceedings of the 11th Inter-
national Conference on Computational Semantics,
pages 52–57.

Yoav Goldberg and Omer Levy. 2014. word2vec
explained: deriving mikolov et al.’s negative-
sampling word-embedding method. arXiv preprint
arXiv:1402.3722.

Colin Kelly, Barry Devereux, and Anna Korhonen.
2012. Semi-supervised learning for automatic con-
ceptual property extraction. In Proceedings of the
3rd Workshop on Cognitive Modeling and Com-
putational Linguistics (CMCL 2012), pages 11–20,
Montréal, Canada. Association for Computational
Linguistics.

Colin Kelly, Barry Devereux, and Anna Korhonen.
2014. Automatic extraction of property norm-like
data from large text corpora. Cognitive Science,
38(4):638–682.

Omer Levy and Yoav Goldberg. 2014. Dependency-
based word embeddings. In Proceedings of the 52nd
Annual Meeting of the Association for Computa-
tional Linguistics (Volume 2: Short Papers), vol-
ume 2, pages 302–308.

Dandan Li and Douglas Summers-Stay. 2019. Map-
ping distributional semantics to property norms with
deep neural networks. Big Data and Cognitive Com-
puting, 3(2):30.

Lucy Li and Jon Gauthier. 2017. Are distributional
representations ready for the real world? Evaluat-
ing word vectors for grounded perceptual meaning.
In Proceedings of the First Workshop on Language
Grounding for Robotics, pages 76–85, Vancouver,
Canada. Association for Computational Linguistics.

Ken McRae, George S Cree, Mark S Seidenberg, and
Chris McNorgan. 2005. Semantic feature produc-
tion norms for a large set of living and nonliving
things. Behavior Research Methods, 37(4):547–
559.

Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Cor-
rado, and Jeff Dean. 2013. Distributed representa-
tions of words and phrases and their compositional-
ity. In Advances in Neural Information Processing
Systems, pages 3111–3119.

Brian Murphy, Partha Talukdar, and Tom Mitchell.
2012. Learning effective and interpretable seman-
tic models using non-negative sparse embedding.
In Proceedings of COLING 2012, pages 1933–
1950, Mumbai, India. The COLING 2012 Organiz-
ing Committee.

Jeffrey Pennington, Richard Socher, and Christopher
Manning. 2014. Glove: Global vectors for word
representation. In Proceedings of the 2014 Con-
ference on Empirical Methods in Natural Language
Processing (EMNLP), pages 1532–1543, Doha,
Qatar. Association for Computational Linguistics.

Dana Rubinstein, Effi Levi, Roy Schwartz, and Ari
Rappoport. 2015. How well do distributional mod-
els capture different types of semantic knowledge?
In Proceedings of the 53rd Annual Meeting of the
Association for Computational Linguistics and the
7th International Joint Conference on Natural Lan-
guage Processing (Volume 2: Short Papers), vol-
ume 2, pages 726–730.

L. K. Şenel, I. Utlu, V. Yücesoy, A. Koç, and
T. Çukur. 2018. Semantic structure and inter-
pretability of word embeddings. IEEE/ACM Trans-
actions on Audio, Speech, and Language Process-
ing, 26(10):1769–1779.

Carina Silberer. 2017. Grounding the meaning of
words with visual attributes. In Visual Attributes,
pages 331–362. Springer.

Pia Sommerauer and Antske Fokkens. 2018. Firearms
and tigers are dangerous, kitchen knives and ze-
bras are not: Testing whether word embeddings can

https://doi.org/10.18653/v1/K18-1026
https://doi.org/10.18653/v1/K18-1026
https://doi.org/10.18653/v1/K18-1026
https://doi.org/10.1007/s11168-010-9068-8
https://doi.org/10.1007/s11168-010-9068-8
https://doi.org/10.1007/s11168-010-9068-8
https://www.aclweb.org/anthology/W12-1702
https://www.aclweb.org/anthology/W12-1702
https://doi.org/10.1111/cogs.12091
https://doi.org/10.1111/cogs.12091
https://doi.org/10.18653/v1/W17-2810
https://doi.org/10.18653/v1/W17-2810
https://doi.org/10.18653/v1/W17-2810
https://www.aclweb.org/anthology/C12-1118
https://www.aclweb.org/anthology/C12-1118
https://doi.org/10.3115/v1/D14-1162
https://doi.org/10.3115/v1/D14-1162
https://doi.org/10.1109/TASLP.2018.2837384
https://doi.org/10.1109/TASLP.2018.2837384
https://doi.org/10.18653/v1/W18-5430
https://doi.org/10.18653/v1/W18-5430
https://doi.org/10.18653/v1/W18-5430


5859

tell. In Proceedings of the 2018 EMNLP Workshop
BlackboxNLP: Analyzing and Interpreting Neural
Networks for NLP, pages 276–286, Brussels, Bel-
gium. Association for Computational Linguistics.

https://doi.org/10.18653/v1/W18-5430

