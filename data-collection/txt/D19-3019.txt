



















































Joey NMT: A Minimalist NMT Toolkit for Novices


Proceedings of the 2019 EMNLP and the 9th IJCNLP (System Demonstrations), pages 109–114
Hong Kong, China, November 3 – 7, 2019. c©2019 Association for Computational Linguistics

109

Joey NMT: A Minimalist NMT Toolkit for Novices

Julia Kreutzer
Computational Linguistics

Heidelberg University
kreutzer@cl.uni-heidelberg.de

Joost Bastings
ILLC

University of Amsterdam
bastings@uva.nl

Stefan Riezler
Computational Linguistics & IWR

Heidelberg University
riezler@cl.uni-heidelberg.de

Abstract
We present Joey NMT, a minimalist neural
machine translation toolkit based on PyTorch
that is specifically designed for novices. Joey
NMT provides many popular NMT features in
a small and simple code base, so that novices
can easily and quickly learn to use it and adapt
it to their needs. Despite its focus on sim-
plicity, Joey NMT supports classic architec-
tures (RNNs, transformers), fast beam search,
weight tying, and more, and achieves perfor-
mance comparable to more complex toolk-
its on standard benchmarks. We evaluate the
accessibility of our toolkit in a user study
where novices with general knowledge about
Pytorch and NMT and experts work through
a self-contained Joey NMT tutorial, show-
ing that novices perform almost as well as
experts in a subsequent code quiz. Joey
NMT is available at https://github.
com/joeynmt/joeynmt.

1 Introduction

Since the first successes of neural machine trans-
lation (NMT), various research groups and indus-
try labs have developed open source toolkits spe-
cialized for NMT, based on new open source deep
learning platforms. While toolkits like OpenNMT
(Klein et al., 2018), XNMT (Neubig et al., 2018)
and Neural Monkey (Helcl and Libovický, 2017)
aim at readability and extensibility of their code-
base, their target group are researchers with a
solid background in machine translation and deep
learning, and with experience in navigating, un-
derstanding and handling large code bases. How-
ever, none of the existing NMT tools has been de-
signed primarily for readability or accessibility for
novices, nor has anyone studied quality and ac-
cessibility of such code empirically. On the other
hand, it is an important challenge for novices to
understand how NMT is implemented, what fea-
tures each toolkit implements exactly, and which

toolkit to choose in order to code their own project
as fast and simple as possible.

We present an NMT toolkit especially designed
for novices, providing clean, well documented,
and minimalistic code, that is yet of comparable
quality to more complex codebases on standard
benchmarks. Our approach is to identify the core
features of NMT that have not changed over the
last years, and to invest in documentation, simplic-
ity and quality of the code. These core features
include standard network architectures (RNN,
transformer, different attention mechanisms, input
feeding, configurable encoder/decoder bridge),
standard learning techniques (dropout, learning
rate scheduling, weight tying, early stopping cri-
teria), and visualization/monitoring tools.

We evaluate our codebase in several ways:
Firstly, we show that Joey NMT’s comment-to-
code ratio is almost twice as high as other toolk-
its which are roughly 9-10 times larger. Secondly,
we present an evaluation on standard benchmarks
(WMT17, IWSLT) where we show that the core
architectures implemented in Joey NMT achieve
comparable performance to more complex state-
of-the-art toolkits. Lastly, we conduct a user
study where we test the code understanding of
novices, i.e. students with basic knowledge
about NMT and PyTorch, against expert coders.
While novices, after having worked through a self-
contained Joey NMT tutorial, needed more time
to answer each question in an in-depth code quiz,
they achieved only marginally lower scores than
the experts. To our knowledge, this is the first user
study on the accessibility of NMT toolkits.

2 Joey NMT

2.1 NMT Architectures

This section formalizes the Joey NMT imple-
mentation of autoregressive recurrent and fully-

https://github.com/joeynmt/joeynmt
https://github.com/joeynmt/joeynmt


110

attentional models.
In the following, a source sentence of length lx

is represented by a sequence of one-hot encoded
vectors x1,x2, . . . ,xlx for each word. Analo-
gously, a target sequence of length ly is repre-
sented by a sequence of one-hot encoded vectors
y1,y2, . . . ,yly .

2.1.1 RNN
Joey NMT implements the RNN encoder-decoder
variant from Luong et al. (2015).

Encoder. The encoder RNN transforms the in-
put sequence x1, . . . ,xlx into a sequence of vec-
tors h1, . . . ,hlx with the help of the embeddings
matrix Esrc and a recurrent computation of states

hi = RNN(Esrc xi,hi−1); h0 = 0.

The RNN consists of either GRU or a LSTM units.
For a bidirectional RNN, hidden states from both
directions are are concatenated to form hi. The
initial encoder hidden state h0 is a vector of ze-
ros. Multiple layers can be stacked by using each
resulting output sequence h1, . . . ,hlx as the input
to the next RNN layer.

Decoder. The decoder uses input feeding (Lu-
ong et al., 2015) where an attentional vector s̃ is
concatenated with the representation of the previ-
ous word as input to the RNN. Decoder states are
computed as follows:

st = RNN([Etrg yt−1; s̃t−1], st−1)

s0 =


tanh(Wbridge hlx + bbridge) if bridge
hlx if last
0 otherwise

s̃t = tanh(Watt[st; ct] + batt)

The initial decoder state is configurable to be ei-
ther a non-linear transformation of the last encoder
state (“bridge”), or identical to the last encoder
state (“last”), or a vector of zeros.

Attention. The context vector ct is computed
with an attention mechanism scoring the previous
decoder state st−1 and each encoder state hi:

ct =
∑
i

ati · hi

ati =
exp(score(st−1,hi))∑
k exp(score(st−1,hk))

where the scoring function is a multi-layer percep-
tron (Bahdanau et al., 2015) or a bilinear transfor-
mation (Luong et al., 2015).

Output. The output layer produces a vector
ot = Wout s̃t, which contains a score for each to-
ken in the target vocabulary. Through a softmax
transformation, these scores can be interpreted as
a probability distribution over the target vocabu-
lary V that defines an index over target tokens vj .

p(yt = vj | x, y<t) =
exp(ot[j])∑|V|
k=1 exp(ot[k])

2.1.2 Transformer
Joey NMT implements the Transformer from
Vaswani et al. (2017), with code based on The An-
notated Transformer blog (Rush, 2018).

Encoder. Given an input sequence x1, . . . ,xlx ,
we look up the word embedding for each input
word using Esrcxi, add a position encoding to it,
and stack the resulting sequence of word embed-
dings to form matrix X ∈ Rlx×d, where lx is the
sentence length and d the dimensionality of the
embeddings.

We define the following learnable parameters:1

A ∈ Rd×da B ∈ Rd×da C ∈ Rd×do

where da is the dimensionality of the attention (in-
ner product) space and do the output dimensional-
ity. Transforming the input matrix with these ma-
trices into new word representations H

H = softmax
(
XAB>X>

)︸ ︷︷ ︸
self-attention

XC

which have been updated by attending to all other
source words. Joey NMT implements multi-
headed attention, where this transformation is
computed k times, one time for each head with
different parameters A,B,C.

After computing all k Hs in parallel, we con-
catenate them and apply layer normalization and a
final feed-forward layer:

H = [H(1); . . . ;H(k)]

H ′ = layer-norm(H) +X

H (enc) = feed-forward(H ′) +H ′

We set do = d/k, so that H ∈ Rlx×d. Multiple of
these layers can be stacked by setting X = H (enc)

and repeating the computation.

1Exposition adapted from Michael Collins https://
youtu.be/jfwqRMdTmLo

https://youtu.be/jfwqRMdTmLo
https://youtu.be/jfwqRMdTmLo


111

Decoder. The Transformer decoder operates in
a similar way as the encoder, but takes the stacked
target embeddings Y∈Rly×d as input:

H = softmax
(
YAB>Y >

)︸ ︷︷ ︸
masked self-attention

YC

For each target position attention to future in-
put words is inhibited by setting those attention
scores to −inf before the softmax. After obtain-
ing H ′ = H + Y , and before the feed-forward
layer, we compute multi-headed attention again,
but now between intermediate decoder representa-
tions H ′ and final encoder representations H (enc):

Z = softmax
(
H ′AB>H (enc)

>)︸ ︷︷ ︸
src-trg attention

H (enc)C

H (dec) = feed-forward(layer-norm(H ′ + Z))

We predict target words with H (dec)Wout.

2.2 Features
In the spirit of minimalism, we follow the 80/20
principle (Pareto, 1896) and aim to achieve 80%
of the translation quality with 20% of a common
toolkit’s code size. For this purpose we identi-
fied the most common features (the bare neces-
sities) in recent works and implementations.2 It
includes standard architectures (see §2.1), label
smoothing, dropout in multiple places, various at-
tention mechanisms, input feeding, configurable
encoder/decoder bridge, learning rate scheduling,
weight tying, early stopping criteria, beam search
decoding, an interactive translation mode, visual-
ization/monitoring of learning progress and atten-
tion, checkpoint averaging, and more.

2.3 Documentation
The code itself is documented with doc-strings and
in-line comments (especially for tensor shapes),
and modules are tested with unit tests. The doc-
umentation website3 contains installation instruc-
tions, a walk-through tutorial for training, tun-
ing and testing an NMT model on a toy task4,
an overview of code modules, and a detailed API
documentation. In addition, we provide thorough

2We refer the reader to the additional technical description
in https://arxiv.org/abs/1907.12484: Table 6
in Appendix A.1 compares Joey NMT’s features with several
popular NMT toolkits and shows that Joey NMT covers all
features that those toolkits have in common.

3https://joeynmt.readthedocs.io
4Demo video: https://youtu.be/PzWRWSIwSYc

Counts OpenNMT-py XNMT Joey NMT

Files 94 82 20
Code 10,287 11,628 2,250
Comments 3,372 4,039 1,393

Comment/Code Ratio 0.33 0.35 0.62

Table 1: Python code statistics for OpenNMT-py (com-
mit hash 624a0b3a), XNMT (a87e7b94) and Joey
NMT (e55b615).

answers to frequently asked questions regarding
usage, configuration, debugging, implementation
details and code extensions, and recommend re-
sources, such as data collections, PyTorch tutorials
and NMT background material.

2.4 Code Complexity

In order to facilitate fast code comprehension and
navigation (Wiedenbeck et al., 1999), Joey NMT
objects have at most one level of inheritance. Ta-
ble 1 compares Joey NMT with OpenNMT-py
and XNMT (selected for their extensibility and
thoroughness of documentation) in terms of code
statistics, i.e. lines of Python code, lines of com-
ments and number of files.5 OpenNMT-py and
XNMT have roughly 9-10x more lines of code,
spread across 4-5x more files than Joey NMT .
These toolkits cover more than the essential fea-
tures for NMT (see §2.2), in particular for other
generation or classification tasks like image cap-
tioning and language modeling. However, Joey
NMT’s comment-to-code ratio is almost twice as
high, which we hope will give code readers better
guidance in understanding and extending the code.

2.5 Benchmarks

Our goal is to achieve a performance that is com-
parable to other NMT toolkits, so that novices
can start off with reliable benchmarks that are
trusted by the community. This will allow them
to build on Joey NMT for their research, should
they want to do so. We expect novices to have lim-
ited resources available for training, i.e., not more
than one GPU for a week, and therefore we fo-
cus on benchmarks that are within this scope. Pre-
trained models, data preparation scripts and con-
figuration files for the following benchmarks will
be made available on https://github.com/
joeynmt/joeynmt.

5Using https://github.com/AlDanial/cloc

https://arxiv.org/abs/1907.12484
https://joeynmt.readthedocs.io
https://youtu.be/PzWRWSIwSYc
https://github.com/joeynmt/joeynmt
https://github.com/joeynmt/joeynmt
https://github.com/AlDanial/cloc


112

System Groundhog RNN Best RNN Transformer

en-de lv-en layers en-de lv-en en-de lv-en

NeuralMonkey 13.7 10.5 1/1 13.7 10.5 – –
OpenNMT-Py 18.7 10.0 4/4 22.0 13.6 – –
Nematus 23.9 14.3 8/8 23.8 14.7 – –
Sockeye 23.2 14.4 4/4 25.6 15.9 27.5 18.1
Marian 23.5 14.4 4/4 25.9 16.2 27.4 17.6
Tensor2Tensor – – – – – 26.3 17.7

Joey NMT 23.5 14.6 4/4 26.0 15.8 27.4 18.0

Table 2: Results on WMT17 newstest2017. Comparative scores are from Hieber et al. (2018).

WMT17. We use the settings of Hieber et al.
(2018), using the exact same data, pre-processing,
and evaluation using WMT17-compatible Sacre-
BLEU scores (Post, 2018).6 We consider the set-
ting where toolkits are used out-of-the-box to train
a Groundhog-like model (1-layer LSTMs, MLP
attention), the ‘best found’ setting where Hieber
et al. train each model using the best settings that
they could find, and the Transformer base setting.7

Table 2 shows that Joey NMT performs very well
compared against other shallow, deep and Trans-
former models, despite its simple code base.8

IWSLT14. This is a popular benchmark because
of its relatively small size and therefore fast train-
ing time. We use the data, pre-processing, and
word-based vocabulary of Wiseman and Rush
(2016) and evaluate with SacreBLEU.9 Table 3
shows that Joey NMT performs well here, with
both its recurrent and its Transformer model. We
also included BPE results for future reference.

System de-en

Wiseman and Rush (2016) 22.5
Bahdanau et al. (2017) 27.6
Joey NMT (RNN, word) 27.1
Joey NMT (RNN, BPE32k) 27.3
Joey NMT (Transformer, BPE32k) 31.0

Table 3: IWSLT14 test results.

6
BLEU+case.mixed+lang.[en-lv|en-de]+numrefs.1+smooth.exp+

test.wmt17+tok.13a+version.1.3.6
7Note that the scores reported for other models reflect

their state when evaluated in Hieber et al. (2018).
8Blog posts like Rush (2018) and Bastings (2018) also

offer simple code, but they do not perform as well.
9
BLEU+case.lc+numrefs.1+smooth.exp+tok.none+version.1.3.6

3 User Study

The target group for Joey NMT are novices who
will use NMT in a seminar project, a thesis, or an
internship. Common tasks are to re-implement a
paper, extend standard models by a small novel
element, or to apply them to a new task. In or-
der to evaluate how well novices understand Joey
NMT, we conducted a user study comparing the
code comprehension of novices and experts.

3.1 Study Design

Participants. The novice group is formed of
eight undergraduate students with a Computa-
tional Linguistics major that have all passed in-
troductory courses to Python and Machine Learn-
ing, three of them also a course about Neural Net-
works. None of them had practical experience
with training or implementing NMT models nor
PyTorch, but two reported theoretic understanding
of NMT. They attended a 20h crash course intro-
ducing NMT and Pytorch basics.10 Note that we
did not teach Joey NMT explicitly in class, but the
students independently completed the Joey NMT
tutorial.

As a control group (the “experts”), six gradu-
ate students with NMT as topic of their thesis or
research project participated in the study. In con-
trast to the novices, this group of participants has a
solid background in Deep Learning and NMT, had
practical experience with NMT. All of them had
previously worked with NMT in PyTorch.

Conditions. The participation in the study was
voluntary and not graded. Participants were not
allowed to work in groups and had a maximum

10See §?? in the supplemental material of https://
arxiv.org/abs/1907.12484 for details.

https://arxiv.org/abs/1907.12484
https://arxiv.org/abs/1907.12484


113

time of 3h to complete the quiz. They had pre-
viously locally installed Joey NMT11 and could
browse the code with the tools of their choice (IDE
or text editor). They were instructed to explore
the Joey NMT code with the help of the quiz, in-
formed about the purpose of the study, and agreed
to the use of their data in this study. Both groups
of participants had to learn about Joey NMT in a
self-guided manner, using the same tutorial, code,
and documentation. The quiz was executed on the
university’s internal e-learning platform. Partici-
pants could jump between questions, review their
answers before finally submitting all answers and
could take breaks (without stopping the timer).
Answers to the questions were published after all
students had completed the test.

Question design. The questions are not de-
signed to test the participant’s prior knowledge
on the topic, but to guide their exploration of the
code. The questions are either free text, multiple
choice or binary choice. There are three blocks of
questions:12

1. Usage of Joey NMT : nine questions on how
to interpret logs, check whether models were
saved, interpret attention matrices, pre-/post-
process, and to validate whether the model is
doing what it is built for.

2. Configuring Joey NMT : four questions that
make the users configure Joey NMT in such
a way that it works for custom situations, e.g.
with custom data, with a constant learning
rate, or creating model of desired size.

3. Joey NMT Code: eighteen questions target-
ing the detailed understanding of the Joey
NMT code: the ability to navigate between
python modules, identify dependencies, and
interpret what individual code lines are do-
ing, hypothesize how specific lines in the
code would have to get changed to change
the behavior (e.g. working with a different
optimizer). The questions in this block were
designed in a way that in order to find the cor-
rect answers, every python module contained
in Joey NMT had to be visited at least once.

11Joey NMT commit hash 0708d596, prior to the Trans-
former implementation.

12https://arxiv.org/abs/1907.12484 con-
tains the full list of questions, complete statistics and details
of the LME analysis.

Every question is awarded one point if answered
correctly. Some questions require manual grading,
most of them have one correct answer. We record
overall completion time and time per question.13

3.2 Analysis
Total duration and score. Experts took on av-
erage 77 min to complete the quiz, novices 118
min, which is significantly slower (one-tailed t-
test, p < 0.05). Experts achieved on average 82%
of the total points, novices 66%. According to the
t-test the difference in total scores between groups
is significant at p < 0.05. An ANOVA reveals
that there is a significant difference in total dura-
tion and scores within the novices group, but not
within the experts group.

Per question analysis. No question was incor-
rectly answered by everyone. Three questions (#6,
#11, #18) were correctly answered by everyone–
they were appeared to be easiest to answer and
did not require deep understanding of the code.
In addition, seven questions (#1, #13, #15, #21,
#22, #28, #29) were correctly answered by all ex-
perts, but not all novices–here their NMT expe-
rience was useful for working with hyperparame-
ters and peculiarities like special tokens. However,
for only one question, regarding the differences
in data processing between training and validation
(#16), the difference between average expert and
novice score was significant (at p < 0.05). Six
questions (#9, #18, #21, #25, #31) show a signifi-
cantly longer average duration for novices than ex-
perts. These questions concerned post-processing,
initialization, batching, end conditions for train-
ing termination and plotting, and required detailed
code inspection.

LME. In order to analyze the dependence of
scores and duration on particular questions and in-
dividual users, we performed a linear mixed ef-
fects (LME) analysis using the R library lme4
(Bates et al., 2015). Participants and questions are
treated as random effects (categorical), the level
of expertise as fixed effect (binary). Duration and
score per question are response variables.14 For
both response variables the variability is higher

13Time measurement is noisy, since full minutes are mea-
sured and students might take breaks at various points in time.

14Modeling expertise with higher granularity instead of the
binary classification into expertise groups (individual vari-
ables for experience with PyTorch, NMT and background in
deep learning) did not have a significant effect on the model,
since the number of participants is relatively low.

https://arxiv.org/abs/1907.12484


114

depending on the question than on the user (6x
higher for score, 2x higher for time). The inter-
cepts of the fixed effects show that novices score
on average 0.14 points less while taking 2.47 min
longer on each question than experts. The impact
of the fixed effect is significant at p < 0.05.

3.3 Findings
First of all, we observe that the design of the ques-
tions was engaging enough for the students be-
cause all participants invested at least 1h to com-
plete the quiz voluntarily. The experts also re-
ported having gained new insights into the code
through the quiz. We found that there are sig-
nificant differences between both groups: Most
prominently, the novices needed more time to an-
swer each question, but still succeeded in answer-
ing the majority of questions correctly. There are
larger variances within the group of novices, be-
cause they had to develop individual strategies to
explore the code and use the available resources
(documentation, code search, IDE), while experts
could in many cases rely on prior knowledge.

4 Conclusion

We presented Joey NMT, a toolkit for sequence-
to-sequence learning designed for NMT novices.
It implements the most common NMT features
and achieves performance comparable to more
complex toolkits, while being minimalist in its de-
sign and code structure. In comparison to other
toolkits, it is smaller in size and but more exten-
sively documented. A user study on code acces-
sibility confirmed that the code is comprehensibly
written and structured. We hope that Joey NMT
will ease the burden for novices to get started with
NMT, and can serve as a basis for teaching.

Acknowledgments

We would like to thank Sariya Karimova, Philipp
Wiesenbach, Michael Staniek and Tsz Kin Lam
for their feedback on the early stages of the code
and for their bug fixes. We also thank the student
and expert participants of the user study.

References
Dzmitry Bahdanau, Philemon Brakel, Kelvin Xu,

Anirudh Goyal, Ryan Lowe, Joelle Pineau, Aaron C.
Courville, and Yoshua Bengio. 2017. An actor-critic
algorithm for sequence prediction. In ICLR.

Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Ben-
gio. 2015. Neural machine translation by jointly
learning to align and translate. In ICLR.

Joost Bastings. 2018. The annotated encoder-decoder
with attention.

Douglas Bates, Martin Mächler, Ben Bolker, and Steve
Walker. 2015. Fitting linear mixed-effects mod-
els using lme4. Journal of Statistical Software,
67(1):1–48.

Jindřich Helcl and Jindřich Libovický. 2017. Neural
Monkey: An Open-source Tool for Sequence Learn-
ing. PBML.

Felix Hieber, Tobias Domhan, Michael Denkowski,
David Vilar, Artem Sokolov, Ann Clifton, and Matt
Post. 2018. The sockeye neural machine translation
toolkit at amta 2018. In AMTA.

Guillaume Klein, Yoon Kim, Yuntian Deng, Vincent
Nguyen, Jean Senellart, and Alexander Rush. 2018.
Opennmt: Neural machine translation toolkit. In
AMTA.

Thang Luong, Hieu Pham, and Christopher D. Man-
ning. 2015. Effective approaches to attention-based
neural machine translation. In EMNLP.

Graham Neubig, Matthias Sperber, Xinyi Wang,
Matthieu Felix, Austin Matthews, Sarguna Padman-
abhan, Ye Qi, Devendra Sachan, Philip Arthur,
Pierre Godard, John Hewitt, Rachid Riad, and Lim-
ing Wang. 2018. XNMT: The extensible neural ma-
chine translation toolkit. In AMTA.

Vilfredo Pareto. 1896. Cours d’économie politique:
professé á l’Université de Lausanne, volume 1. F.
Rouge.

Matt Post. 2018. A call for clarity in reporting BLEU
scores. In WMT.

Alexander Rush. 2018. The annotated transformer.

Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob
Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz
Kaiser, and Illia Polosukhin. 2017. Attention is all
you need. In NeurIPS.

Susan Wiedenbeck, Vennila Ramalingam, Suseela
Sarasamma, and Cynthia L Corritore. 1999. A com-
parison of the comprehension of object-oriented and
procedural programs by novice programmers. Inter-
acting with Computers, 11(3):255–282.

Sam Wiseman and Alexander M. Rush. 2016.
Sequence-to-sequence learning as beam-search op-
timization. In EMNLP, Austin, Texas.


