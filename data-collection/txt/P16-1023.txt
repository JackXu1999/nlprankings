



















































Intrinsic Subspace Evaluation of Word Embedding Representations


Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, pages 236–246,
Berlin, Germany, August 7-12, 2016. c©2016 Association for Computational Linguistics

Intrinsic Subspace Evaluation of Word Embedding Representations

Yadollah Yaghoobzadeh and Hinrich Schütze
Center for Information and Language Processing

University of Munich, Germany
yadollah@cis.lmu.de

Abstract

We introduce a new methodology for in-
trinsic evaluation of word representations.
Specifically, we identify four fundamen-
tal criteria based on the characteristics of
natural language that pose difficulties to
NLP systems; and develop tests that di-
rectly show whether or not representations
contain the subspaces necessary to satisfy
these criteria. Current intrinsic evalua-
tions are mostly based on the overall simi-
larity or full-space similarity of words and
thus view vector representations as points.
We show the limits of these point-based
intrinsic evaluations. We apply our evalu-
ation methodology to the comparison of a
count vector model and several neural net-
work models and demonstrate important
properties of these models.

1 Introduction

Distributional word representations or embeddings
are currently an active area of research in nat-
ural language processing (NLP). The motivation
for embeddings is that knowledge about words is
helpful in NLP. Representing words as vocabulary
indexes may be a good approach if large train-
ing sets allow us to learn everything we need to
know about a word to solve a particular task; but
in most cases it helps to have a representation that
contains distributional information and allows in-
ferences like: “above” and “below” have similar
syntactic behavior or “engine” and “motor” have
similar meaning.

Several methods have been introduced to assess
the quality of word embeddings. We distinguish
two different types of evaluation in this paper: (i)
extrinsic evaluation evaluates embeddings in an
NLP application or task and (ii) intrinsic evalu-

ation tests the quality of representations indepen-
dent of a specific NLP task.

Each single word is a combination of a large
number of morphological, lexical, syntactic, se-
mantic, discourse and other features. Its em-
bedding should accurately and consistently repre-
sent these features, and ideally a good evaluation
method must clarify this and give a way to analyze
the results. The goal of this paper is to build such
an evaluation.

Extrinsic evaluation is a valid methodology, but
it does not allow us to understand the properties
of representations without further analysis; e.g., if
an evaluation shows that embedding A works bet-
ter than embedding B on a task, then that is not an
analysis of the causes of the improvement. There-
fore, extrinsic evaluations do not satisfy our goals.

Intrinsic evaluation analyzes the generic quality
of embeddings. Currently, this evaluation mostly
is done by testing overall distance/similarity of
words in the embedding space, i.e., it is based
on viewing word representations as points and
then computing full-space similarity. The assump-
tion is that the high dimensional space is smooth
and similar words are close to each other. Sev-
eral datasets have been developed for this purpose,
mostly the result of human judgement; see (Baroni
et al., 2014) for an overview. We refer to these
evaluations as point-based and as full-space be-
cause they consider embeddings as points in the
space – sub-similarities in subspaces are generally
ignored.

Point-based intrinsic evaluation computes a
score based on the full-space similarity of two
words: a single number that generally does not
say anything about the underlying reasons for a
lower or higher value of full-space similarity. This
makes it hard to interpret the results of point-based
evaluation and may be the reason that contradic-
tory results have been published; e.g., based on

236



point-based evaluation, some papers have claimed
that count-based representations perform as well
as learning-based representations (Levy and Gold-
berg, 2014a). Others have claimed the opposite
(e.g., Mikolov et al. (2013), Pennington et al.
(2014), Baroni et al. (2014)).

Given the limits of current evaluations, we pro-
pose a new methodology for intrinsic evaluation
of embeddings by identifying generic fundamen-
tal criteria for embedding models that are impor-
tant for representing features of words accurately
and consistently. We develop corpus-based tests
using supervised classification that directly show
whether the representations contain the informa-
tion necessary to meet the criteria or not. The
fine-grained corpus-based supervision makes the
sub-similarities of words important by looking at
the subspaces of word embeddings relevant to the
criteria, and this enables us to give direct insights
into properties of representation models.

2 Related Work

Baroni et al. (2014) evaluate embeddings on dif-
ferent intrinsic tests: similarity, analogy, synonym
detection, categorization and selectional prefer-
ence. Schnabel et al. (2015) introduce tasks with
more fine-grained datasets. These tasks are unsu-
pervised and generally based on cosine similarity;
this means that only the overall direction of vec-
tors is considered or, equivalently, that words are
modeled as points in a space and only their full-
space distance/closeness is considered. In con-
trast, we test embeddings in a classification set-
ting and different subspaces of embeddings are an-
alyzed. Tsvetkov et al. (2015) evaluate embed-
dings based on their correlations with WordNet-
based linguistic embeddings. However, correla-
tion does not directly evaluate how accurately and
completely an application can extract a particular
piece of information from an embedding.

Extrinsic evaluations are also common (cf. (Li
and Jurafsky, 2015; Köhn, 2015; Lai et al., 2015)).
Li and Jurafsky (2015) conclude that embed-
ding evaluation must go beyond human-judgement
tasks like similarity and analogy. They suggest to
evaluate on NLP tasks. Köhn (2015) gives similar
suggestions and also recommends the use of su-
pervised methods for evaluation. Lai et al. (2015)
evaluate embeddings in different tasks with differ-
ent setups and show the contradictory results of
embedding models on different tasks. Idiosyn-

crasies of different downstream tasks can affect
extrinsic evaluations and result in contradictions.

3 Criteria for word representations

Each word is a combination of different proper-
ties. Depending on the language, these properties
include lexical, syntactic, semantic, world knowl-
edge and other features. We call these properties
facets. The ultimate goal is to learn representa-
tions for words that accurately and consistently
contain these facets. Take the facet gender (GEN)
as an example. We call a representation 100% ac-
curate for GEN if information it contains about
GEN is always accurate; we call the representation
100% consistent for GEN if the representation of
every word that has a GEN facet contains this in-
formation.

We now introduce four important criteria that a
representation must satisfy to represent facets ac-
curately and consistently. These criteria are ap-
plied across different problems that NLP applica-
tions face in the effective use of embeddings.

Nonconflation. A word embedding must keep
the evidence from different local contexts sepa-
rate – “do not conflate” – because each context
can infer specific facets of the word. Embeddings
for different word forms with the same stem, like
plural and singular forms or different verb tenses,
are examples vulnerable to conflation because they
occur in similar contexts.

Robustness against sparseness. One aspect of
natural language that poses great difficulty for sta-
tistical modeling is sparseness. Rare words are
common in natural language and embedding mod-
els must learn useful representations based on a
small number of contexts.

Robustness against ambiguity. Another cen-
tral problem when processing words in NLP is
lexical ambiguity (Cruse, 1986; Zhong and Ng,
2010). Polysemy and homonymy of words can
make it difficult for a statistical approach to gen-
eralize and infer well. Embeddings should fully
represent all senses of an ambiguous word. This
criterion becomes more difficult to satisfy as dis-
tributions of senses become more skewed, but a
robust model must be able to overcome this.

Accurate and consistent representation of
multifacetedness. This criterion addresses set-
tings with large numbers of facets. It is based
on the following linguistic phenomenon, a phe-
nomenon that occurs frequently crosslinguistically

237



(Comrie, 1989). (i) Words have a large number
of facets, including phonetic, morphological, syn-
tactic, semantic and topical properties. (ii) Each
facet by itself constitutes a small part of the over-
all information that a representation should cap-
ture about a word.

4 Experimental setup and results

We now design experiments to directly evaluate
embeddings on the four criteria. We proceed as
follows. First, we design a probabilistic context
free grammar (PCFG) that generates a corpus that
is a manifestation of the underlying phenomenon.
Then we train our embedding models on the cor-
pus. The embeddings obtained are then evaluated
in a classification setting, in which we apply a lin-
ear SVM (Fan et al., 2008) to classify embeddings.
Finally, we compare the classification results for
different embedding models and analyze and sum-
marize them.

Selecting embedding models. Since this paper
is about developing a new evaluation methodol-
ogy, the choice of models is not important as long
as the models can serve to show that the proposed
methodology reveals interesting differences with
respect to the criteria.

On the highest level, we can distinguish two
types of distributional representations. Count
vectors (Sahlgren, 2006; Baroni and Lenci,
2010; Turney and Pantel, 2010) live in a high-
dimensional vector space in which each dimen-
sion roughly corresponds to a (weighted) count
of cooccurrence in a large corpus. Learned vec-
tors are learned from large corpora using machine
learning methods: unsupervised methods such as
LSI (e.g., Deerwester et al. (1990), Levy and
Goldberg (2014b)) and supervised methods such
as neural networks (e.g., Mikolov et al. (2013))
and regression (e.g., Pennington et al. (2014)). Be-
cause of the recent popularity of learning-based
methods, we consider one count-based and five
learning-based distributional representation mod-
els.

The learning-based models are: (i) vLBL
(henceforth: LBL) (vectorized log-bilinear lan-
guage model) (Mnih and Kavukcuoglu, 2013),
(ii) SkipGram (henceforth: SKIP) (skipgram bag-
of-word model), (iii) CBOW (continuous bag-of-
word model (Mikolov et al., 2013), (iv) Struc-
tured SkipGram (henceforth SSKIP), (Ling et al.,
2015) and CWindow (henceforth CWIN) (contin-

1 P (aV b|S) = 1/4
2 P (bV a|S) = 1/4
3 P (aWa|S) = 1/8
4 P (aWb|S) = 1/8
5 P (bWa|S) = 1/8
6 P (bWb|S) = 1/8
7 P (vi|V ) = 1/5 0 ≤ i ≤ 4
8 P (wi|W ) = 1/5 0 ≤ i ≤ 4

Figure 1: Global conflation grammar. Words vi
occur in a subset of the contexts of words wi, but
the global count vector signatures are the same.

uous window model) (Ling et al., 2015). These
models learn word embeddings for input and tar-
get spaces using neural network models.

For a given context, represented by the input
space representations of the left and right neigh-
bors ~vi−1 and ~vi+1, LBL, CBOW and CWIN pre-
dict the target space ~vi by combining the contexts.
LBL combines ~vi−1 and ~vi+1 linearly with posi-
tion dependent weights and CBOW (resp. CWIN)
combines them by adding (resp. concatenation).
SKIP and SSKIP predict the context words vi−1
or vi+1 given the input space ~vi. For SSKIP, con-
text words are in different spaces depending on
their position to the input word. In summary,
CBOW and SKIP are learning embeddings using
bag-of-word (BoW) models, but the other three,
CWIN, SSKIP and LBL, are using position depen-
dent models. We use word2vec1 for SKIP and
CBOW, wang2vec2 for SSKIP and CWIN, and
Lai et al. (2015)’s implementation3 for LBL.

The count-based model is position-sensitive
PPMI, Levy and Goldberg (2014a)’s explicit vec-
tor space representation model.4 For a vocabulary
of size V , the representation ~w of w is a vector
of size 4V , consisting of four parts corresponding
to the relative positions r ∈ {−2,−1, 1, 2} with
respect to occurrences of w in the corpus. The
entry for dimension word v in the part of ~w cor-
responding to relative position r is the PPMI (pos-
itive pointwise mutual information) weight of w
and v for that relative position. The four parts of
the vector are length normalized. In this paper, we
use only two relative positions: r ∈ {−1, 1}, so
each ~w has two parts, corresponding to immediate
left and right neighbors.

1code.google.com/archive/p/word2vec
2github.com/wlin12/wang2vec
3github.com/licstar/compare
4bitbucket.org/omerlevy/hyperwords

238



4.1 Nonconflation

Grammar. The PCFG grammar shown in Fig-
ure 1 generates vi words that occur in two types
of contexts: a-b (line 1) and b-a (line 2); and wi
words that also occur in these two contexts (lines
4 and 5), but in addition occur in a-a (line 3) and
b-b (line 6) contexts. As a result, the set of con-
texts in which vi and wi occur is different, but if
we simply count the number of occurrences in the
contexts, then vi and wi cannot be distinguished.

Dataset. We generated a corpus of 100,000 sen-
tences. Words that can occur in a-a and b-b con-
texts constitute the positive class, all other words
the negative class. The words v3, v4, w3, w4 were
assigned to the test set, all other words to the train-
ing set.

Results. We learn representations of words by
our six models and train one SVM per model; it
takes a word representation as input and outputs
+1 (word can occur in a-a/b-b) or -1 (it cannot).
The SVMs trained on PPMI and CBOW repre-
sentations assigned all four test set words to the
negative class; in particular, w3 and w4 were in-
correctly classified. Thus, the accuracy of clas-
sification for these models (50%) was not better
than random. The SVMs trained on LBL, SSKIP,
SSKIP and CWIN representations assigned all
four test set words to the correct class: v3 and v4
were assigned to the negative class and w3 and w4
were assigned to the positive class.

Discussion. The property of embedding mod-
els that is relevant here is that PPMI is an aggre-
gation model, which means it calculates aggregate
statistics for each word and then computes the fi-
nal word embedding from these aggregate statis-
tics. In contrast, all our learning-based models are
iterative models: they iterate over the corpus and
each local context of a word is used as a training
instance for learning its embedding.

For iterative models, it is common to use com-
position of words in the context, as in LBL,
CBOW and CWIN. Non-compositional iterative
models like SKIP and SSKIP are also popular.
Aggregation models can also use composite fea-
tures from context words, but these features are
too sparse to be useful. The reason that the model
of Agirre et al. (2009) is rarely used is precisely its
inability to deal with sparseness. All widely used
distributional models employ individual word oc-
currences as basic features.

The bad PPMI results are explained by the fact

1 P (AV B|S) = 1/2
2 P (CWD|S) = 1/2
3 P (ai|A) = 1/10 0 ≤ i ≤ 9
4 P (bi|B) = 1/10 0 ≤ i ≤ 9
5 P (ci|C) = 1/10 0 ≤ i ≤ 9
6 P (di|D) = 1/10 0 ≤ i ≤ 9
7 P (vi|V ) = 1/10 0 ≤ i ≤ 9
8 P (wi|W ) = 1/10 0 ≤ i ≤ 9
9 L′ = L(S)

10 ∪ {aiuibi|0 ≤ i ≤ 9}
11 ∪ {cixidi|0 ≤ i ≤ 9}

Figure 2: In language L′, frequent vi and rare ui
occur in a-b contexts; frequent wi and rare xi oc-
cur in c-d contexts. Word representations should
encode possible contexts (a-b vs. c-d) for both fre-
quent and rare words.

that it is an aggregation model: the PPMI model
cannot distinguish two words with the same global
statistics – as is the case for, say, v3 and w3. The
bad result of CBOW is probably connected to its
weak (addition) composition of context, although
it is an iterative compositional model. Simple rep-
resentation of context words with iterative updat-
ing (through backpropagation in each training in-
stance), can influence the embeddings in a way
that SKIP and SSKIP get good results, although
they are non-compositional.

As an example of conflation occurring in the
English Wikipedia, consider this simple example.
We replace all single digits by “7” in tokenization.
We learn PPMI embeddings for the tokens and see
that among the one hundred nearest neighbors of
“7” are the days of the week, e.g., “Friday”. As an
example of a conflated feature consider the word
“falls” occurring immediately to the right of the
target word. The weekdays as well as single dig-
its often have the immediate right neighbor “falls”
in contexts like “Friday falls on a public holiday”
and “2 out of 3 falls match” – tokenized as “7 out
of 7 falls match” – in World Wrestling Entertain-
ment (WWE). The left contexts of “Friday” and
“7” are different in these contexts, but the PPMI
model does not record this information in a way
that would make the link to “falls” clear.

4.2 Robustness against sparseness

Grammar. The grammar shown in Figure 2 gen-
erates frequent vi and rare ui in a-b contexts (lines
1 and 9); and frequent wi and rare xi in c-d con-
texts (lines 2 and 10). The language generated by
the PCFG on lines 1–8 is merged on lines 9–11
with the ten contexts a0u0b0 . . . a9u9b9 (line 9)

239



and the ten contexts c0x0d0 . . . c9x9d9 (line 10);
that is, each of the ui and xi occurs exactly once
in the merged language L′, thus modeling the phe-
nomenon of sparseness.

Dataset. We generated a corpus of 100,000 sen-
tences using the PCFG (lines 1–8) and added the
20 rare sentences (lines 9–11). We label all words
that can occur in c-d contexts as positive and all
other words as negative. The singleton words ui
and xi were assigned to the test set, all other words
to the training set.

Results. After learning embeddings with differ-
ent models, the SVM trained on PPMI representa-
tions assigned all twenty test words to the negative
class. This is the correct decision for the ten ui
(since they cannot occur in a c-d context), but the
incorrect decision for the xi (since they can occur
in a c-d context). Thus, the accuracy of classifica-
tion was 50% and not better than random. The
SVMs trained on learning-based representations
classified all twenty test words correctly.

Discussion. Representations of rare words in
the PPMI model are sparse. The PPMI represen-
tations of the ui and xi only contain two nonzero
entries, one entry for an ai or ci (left context) and
one entry for a bi or di (right context). Given this
sparseness, it is not surprising that representations
are not a good basis for generalization and PPMI
accuracy is random.

In contrast, learning-based models learn that the
ai, bi, ci and di form four different distributional
classes. The final embeddings of the ai after learn-
ing is completed are all close to each other and
the same is true for the other three classes. Once
the similarity of two words in the same distribu-
tional class (say, the similarity of a5 and a7) has
been learned, the contexts for the ui (resp. xi) look
essentially the same to embedding models as the
contexts of the vi (resp.wi). Thus, the embeddings
learned for the ui will be similar to those learned
for the vi. This explains why learning-based repre-
sentations achieve perfect classification accuracy.

This sparseness experiment highlights an im-
portant difference between count vectors and
learned vectors. Count vector models are less
robust in the face of sparseness and noise be-
cause they base their representations on individ-
ual contexts; the overall corpus distribution is
only weakly taken into account, by way of PPMI
weighting. In contrast, learned vector models
make much better use of the overall corpus distri-

1 P (AV1B|S) =10/20
2 P (CW1D|S)=9/20
3 P (CW2D|S)=β·1/20
4 P (AW2B|S) =(1− β)·1/20
5 P (ai|A) =1/10 0 ≤ i ≤ 9
6 P (bi|B) =1/10 0 ≤ i ≤ 9
7 P (ci|C) =1/10 0 ≤ i ≤ 9
8 P (di|D) =1/10 0 ≤ i ≤ 9
9 P (vi|V1) =1/50 0 ≤ i ≤ 49

10 P (wi|W1) =1/45 5 ≤ i ≤ 49
11 P (wi|W2) =1/5 0 ≤ i ≤ 4

Figure 3: Ambiguity grammar. vi and w5 . . . w49
occur in a-b and c-d contexts only, respectively.
w0 . . . w4 are ambiguous and occur in both con-
texts.

bution and they can leverage second-order effects
for learning improved representations. In our ex-
ample, the second order effect is that the model
first learns representations for the ai, bi, ci and di
and then uses these as a basis for inferring the sim-
ilarity of ui to vi and of xi to wi.

4.3 Robustness against ambiguity

Grammar. The grammar in Figure 3 generates
two types of contexts that we interpret as two dif-
ferent meanings: a-b contexts (lines 1,4) and c-d
contexts (lines 2, 3). vi occur only in a-b contexts
(line 1), w5 . . . w49 occur only in c-d contexts (line
2); thus, they are unambiguous. w0 . . . w4 are am-
biguous and occur with probability β in c-d con-
texts (line 3) and with probability (1 − β) in a-b
contexts (lines 3, 4). The parameter β controls the
skewedness of the sense distribution; e.g., the two
senses are equiprobable for β = 0.5 and the sec-
ond sense (line 4) is three times as probable as the
first sense (line 3) for β = 0.25.

Dataset. The grammar specified in Figure 3
was used to generate a training corpus of 100,000
sentences. Label criterion: A word is labeled posi-
tive if it can occur in a c-d context, as negative oth-
erwise. The test set consists of the five ambiguous
words w0 . . . w4. All other words are assigned to
the training set.

Linear SVMs were trained for the binary clas-
sification task on the train set. 50 trials of this
experiment were run for each of eleven values of
β: β = 2−α where α ∈ {1.0, 1.1, 1.2, . . . , 2.0}.
Thus, for the smallest value of α, α = 1.0, the two
senses have the same frequency; for the largest
value of α, α = 2.0, the dominant sense is three
times as frequent as the less frequent sense.

Results. Figure 4 shows accuracy of the classi-

240



1.0 1.2 1.4 1.6 1.8 2.0

0.
0

0.
2

0.
4

0.
6

0.
8

1.
0

alpha

ac
cu

ra
cy

pmi

lbl

cbow

skip

cwin

sskip

Figure 4: SVM classification results for the am-
biguity dataset. X-axis: α = − log2 β. Y-axis:
classification accuracy:

fication on the test set: the proportion of correctly
classified words out of a total of 250 (five words
each in 50 trials).

All models perform well for balanced sense fre-
quencies; e.g., for α = 1.0, β = 0.5, the SVMs
were all close to 100% accurate in predicting that
the wi can occur in a c-d context. PPMI accuracy
falls steeply when α is increased from 1.4 to 1.5. It
has a 100% error rate for α ≥ 1.5. Learning-based
models perform better in the order CBOW (least
robust), LBL, SSKIP, SKIP, CWIN (most robust).
Even for α = 2.0, CWIN and SKIP are still close
to 100% accurate.

Discussion. The evaluation criterion we have
used here is a classification task. The classifier at-
tempts to answer a question that may occur in an
application – can this word be used in this con-
text? Thus, the evaluation criterion is: does the
word representation contain a specific type of in-
formation that is needed for the application.

Another approach to ambiguity is to compute
multiple representations for a word, one for each
sense. We generally do not yet know what the
sense of a word is when we want to use its
word representation, so data-driven approaches
like clustering have been used to create represen-
tations for different usage clusters of words that
may capture some of its senses. For example,
Reisinger and Mooney (2010) and Huang et al.
(2012) cluster the contexts of each word and then
learn a different representation for each cluster.
The main motivation for this approach is the as-
sumption that single-word distributional represen-
tations cannot represent all senses of a word well
(Huang et al., 2012). However, Li and Jurafsky
(2015) show that simply increasing the dimension-

1 P (NFn|S) =1/4
2 P (AFa|S) =1/4
3 P (NMn|S) =1/4
4 P (AMf |S) =1/4
5 P (ni|N) =1/5 0 ≤ i ≤ 4
6 P (ai|A) =1/5 0 ≤ i ≤ 4
7 P (xnfi U

nf
i |Fn) =1/5 0 ≤ i ≤ 4

8 P (f |Unfi ) =1/2
9 P (µ(Unfi )|Unfi ) =1/2

10 P (xafi U
af
i |Fa) =1/5 0 ≤ i ≤ 4

11 P (f |U afi ) =1/2
12 P (µ(U afi )|U afi ) =1/2
13 P (xnmi U

nm
i |Mn) =1/5 0 ≤ i ≤ 4

14 P (m|Unmi ) =1/2
15 P (µ(Unmi )|Unmi )=1/2
16 P (xami U

am
i |Mf ) =1/5 0 ≤ i ≤ 4

17 P (m|U ami ) =1/2
18 P (µ(U ami )|U ami ) =1/2

Figure 5: This grammar generates nouns (xn.i ) and
adjectives (xa.i ) with masculine (x

.m
i ) and feminine

(x.fi ) gender as well as paradigm features ui. µ
maps each U to one of {u0 . . . u4}. µ is randomly
initialized and then kept fixed.

ality of single-representation gets comparable re-
sults to using multiple-representation. Our results
confirm that a single embedding can be robust
against ambiguity, but also show the main chal-
lenge: skewness of sense distribution.

4.4 Accurate and consistent representation of
multifacetedness

Grammar. The grammar shown in Figure 5 mod-
els two syntactic categories, nouns and adjectives,
whose left context is highly predictable: it is one
of five left context words ni (resp. ai) for nouns,
see lines 1, 3, 5 (resp. for adjectives, see lines 2, 4,
6). There are two grammatical genders: feminine
(corresponding to the two symbols Fn and Fa)
and masculine (corresponding to the two symbols
Mn and Ma). The four combinations of syntac-
tic category and gender are equally probable (lines
1–4). In addition to gender, nouns and adjec-
tives are distinguished with respect to morpholog-
ical paradigm. Line 7 generates one of five fem-
inine nouns (xnfi ) and the corresponding paradigm
markerU nfi . A noun has two equally probable right
contexts: a context indicating its gender (line 8)
and a context indicating its paradigm (line 9). µ
is a function that maps each U to one of five mor-
phological paradigms {u0 . . . u4}. µ is randomly
initialized before a corpus is generated and kept
fixed.

The function µ models the assignment of

241



paradigms to nouns and adjectives. Nouns
and adjectives can have different (or the same)
paradigms, but for a given noun or adjective the
paradigm is fixed and does not change. Lines 7–
9 generate gender and paradigm markers for fem-
inine nouns, for which we use the symbols xnfi .
Lines 10–18 cover the three other cases: mas-
culine nouns (xnmi ), feminine adjectives (x

af
i ) and

masculine adjectives (xami ).
Dataset. We perform 10 trials. In each trial,

µ is initialized randomly and a corpus of 100,000
sentences is generated. The train set consists of
the feminine nouns (xnfi , line 7) and the masculine
nouns (xnmi , line 13). The test set consists of the
feminine (xafi ) and masculine (x

am
i ) adjectives.

Results. Embeddings have been learned, SVMs
are trained on the binary classification task femi-
nine vs. masculine and evaluated on test. There
was not a single error: accuracy of classifications
is 100% for all embedding models.

Discussion. The facet gender is indicated di-
rectly by the distribution and easy to learn. For
a noun or adjective x, we simply have to check
whether f or m occurs to its right anywhere in the
corpus. PPMI stores this information in two di-
mensions of the vectors and the SVM learns this
fact perfectly. The encoding of “f or m occurs to
the right” is less direct in the learning-based rep-
resentation of x, but the experiment demonstrates
that they also reliably encode it and the SVM reli-
ably picks it up.

It would be possible to encode the facet in just
one bit in a manually designed representation.
While all representations are less compact than a
one-bit representation – PPMI uses two real di-
mensions, learning-based models use an activation
pattern over several dimensions – it is still true that
most of the capacity of the embeddings is used for
encoding facets other than gender: syntactic cat-
egories and paradigms. Note that there are five
different instances each of feminine/masculine ad-
jectives, feminine/masculine nouns and ui words,
but only two gender indicators: f and m. This
is a typical scenario across languages: words are
distinguished on a large number of morphological,
grammatical, semantic and other dimensions and
each of these dimensions corresponds to a small
fraction of the overall knowledge we have about a
given word.

Point-based tests do not directly evaluate spe-
cific facets of words. In similarity datasets,

there is no individual test on facets – only full-
space similarity is considered. There are test
cases in analogy that hypothetically evaluate spe-
cific facets like gender of words, as in king-
man+woman=queen. However, it does not con-
sider the impact of other facets and assumes the
only difference of “king” and “queen” is gen-
der. A clear example that words usually differ on
many facets, not just one, is the analogy: Lon-
don:England ∼ Ankara:Turkey. political-capital-
of applies to both, cultural-capital-of only to Lon-
don:England since Istanbul is the cultural capital
of Turkey.

To make our argument more clear, we designed
an additional experiment that tries to evaluate gen-
der in our dataset based on similarity and anal-
ogy methods. In the similarity evaluation, we
search for the nearest neighbor of each word and
accuracy is the proportion of nearest neighbors
that have the same gender as the search word.
In the analogy evaluation, we randomly select
triples of the form <xc1g1i ,x

c1g2
j ,x

c2g2
k > where

(c1, c2) ∈ {(noun, adjective), (adjective, noun)}
and (g1, g2) ∈ {(masculine, feminine), (feminine,
masculine) }. We then compute ~s = ~xc1g1i −
~xc1g2j + ~x

c2g2
k and identify the word whose vec-

tor is closest to ~s where the three vectors ~xc1g1i ,
~xc1g2j , ~x

c2g2
k are excluded. If the nearest neighbor

of ~s is of type ~xc2g1l , then the search is successful;
e.g., for ~s = ~xnfi − ~xnmj + ~xamk , the search is suc-
cessful if the nearest neighbor is feminine. We did
this evaluation on the same test set for PPMI and
LBL embedding models. Error rates were 29% for
PPMI and 25% for LBL (similarity) and 16% for
PPMI and 14% for LBL (analogy). This high er-
ror, compared to 0% error for SVM classification,
indicates it is not possible to determine the pres-
ence of a low entropy facet accurately and consis-
tently when full-space similarity and analogy are
used as test criteria.

5 Analysis

In this section, we first summarize and analyze the
lessons we learned through experiments in Sec-
tion 4. After that, we show how these lessons are
supported by a real natural-language corpus.

5.1 Learned lessons

(i) Two words with clearly different context dis-
tributions should receive different representations.
Aggregation models fail to do so by calculating

242



all entities head entities tail entities
MLP 1NN MLP 1NN MLP 1NN

PPMI 61.6 44.0 69.2 63.8 43.0 28.5
LBL 63.5 51.7 72.7 66.4 44.1 32.8

CBOW 63.0 53.5 71.7 69.4 39.1 29.9
CWIN 66.1 53.0 73.5 68.6 46.8 31.4
SKIP 64.5 57.1 69.9 71.5 49.8 34.0

SSKIP 66.2 52.8 73.9 68.5 45.5 31.4

Table 1: Entity typing results using embeddings
learned with different models.

global statistics.
(ii) Embedding learning can have different ef-

fectiveness for sparse vs. non-sparse events. Thus,
models of representations should be evaluated
with respect to their ability to deal with sparse-
ness; evaluation data sets should include rare as
well as frequent words.

(iii) Our results in Section 4.3 suggest that
single-representation approaches can indeed rep-
resent different senses of a word. We did a classi-
fication task that roughly corresponds to the ques-
tion: does this word have a particular meaning?
A representation can fail on similarity judgement
computations because less frequent senses occupy
a small part of the capacity of the representa-
tion and therefore have little impact on full-space
similarity values. Such a failure does not neces-
sarily mean that a particular sense is not present
in the representation and it does not necessarily
mean that single-representation approaches per-
form poor on real-world tasks. However, we saw
that even though single-representations do well on
balanced senses, they can pose a challenge for am-
biguous words with skewed senses.

(iv) Lexical information is complex and multi-
faceted. In point-based tests, all dimensions are
considered together and their ability to evaluate
specific facets or properties of a word is limited.
The full-space similarity of a word may be high-
est to a word that has a different value on a low-
entropy facet. Any good or bad result on these
tasks is not sufficient to conclude that the repre-
sentation is weak. The valid criterion of quality is
whether information about the facet is consistently
and accurately stored.

5.2 Extrinsic evaluation: entity typing

To support the case for sub-space evaluation and
also to introduce a new extrinsic task that uses the
embeddings directly in supervised classification,
we address a fine-grained entity typing task.

Learning taxonomic properties or types of
words has been used as an evaluation method
for word embeddings (Rubinstein et al., 2015).
Since available word typing datasets are quite
small (cf. Baroni et al. (2014), Rubinstein et al.
(2015)), entity typing can be a promising alter-
native, which enables to do supervised classifi-
cation instead of unsupervised clustering. Enti-
ties, like other words, have many properties and
therefore belong to several semantic types, e.g.,
“Barack Obama” is a POLITICIAN, AUTHOR and
AWARD WINNER. We perform entity typing by
learning types of knowledge base entities from
their embeddings; this requires looking at sub-
spaces because each entity can belong to multiple
types.

We adopt the setup of Yaghoobzadeh and
Schütze (2015) who present a dataset of Freebase
entities;5 there are 102 types (e.g., POLITICIAN
FOOD, LOCATION-CEMETERY) and most entities
have several. More specifically, we use a multi-
layer-perceptron (MLP) with one hidden layer to
classify entity embeddings to 102 FIGER types.
To show the limit of point-based evaluation, we
also experimentally test an entity typing model
based on cosine similarity of entity embeddings.
To each test entity, we assign all types of the entity
closest to it in the train set. We call this approach
1NN (kNN for k = 1).6

We take part of ClueWeb, which is annotated
with Freebase entities using automatic annota-
tion of FACC17 (Gabrilovich et al., 2013), as
our corpus. We then replace all mentions of
entities with their Freebase identifier and learn
embeddings of words and entities in the same
space. Our corpus has around 6 million sen-
tences with at least one annotated entity. We
calculate embeddings using our different models.
Our hyperparameters: for learning-based mod-
els: dim=100, neg=10, iterations=20, window=1,
sub=10−3; for PPMI: SVD-dim=100, neg=1, win-
dow=1, cds=0.75, sub=10−3, eig=0.5. See (Levy
et al., 2015) for more information about the mean-
ing of hyperparameters.

Table 1 gives results on test for all (about 60,000
entities), head (freq > 100; about 12,200 enti-
ties) and tail (freq < 5; about 10,000 entities).
The MLP models consistently outperform 1NN on

5cistern.cis.lmu.de/figment
6We tried other values of k, but results were not better.
7lemurproject.org/clueweb12/FACC1

243



all and tail entities. This supports our hypothe-
sis that only part of the information about types
that is present in the vectors can be determined by
similarity-based methods that use the overall di-
rection of vectors, i.e., full-space similarity.

There is little correlation between results of
MLP and 1NN in all and head entities, and the
correlation between their results in tail entities is
high.8 For example, for all entities, using 1NN,
SKIP is 4.3% (4.1%) better, and using MLP is
1.7% (1.6%) worse than SSKIP (CWIN). The
good performance of SKIP on 1NN using cosine
similarity can be related to its objective function,
which maximizes the cosine similarity of cooccur-
ing token embeddings.

The important question is not similarity, but
whether the information about a specific type ex-
ists in the entity embeddings or not. Our results
confirm our previous observation that a classifica-
tion by looking at subspaces is needed to answer
this question. In contrast, based on full-space sim-
ilarity, one can infer little about the quality of em-
beddings. Based on our results, SSKIP and CWIN
embeddings contain more accurate and consistent
information because MLP classifier gives better
results for them. However, if we considered 1NN
for comparison, SKIP and CBOW would be supe-
rior.

6 Conclusion and future work

We have introduced a new way of evaluating dis-
tributional representation models. As an alterna-
tive to the common evaluation tasks, we proposed
to identify generic criteria that are important for an
embedding model to represent properties of words
accurately and consistently. We suggested four
criteria based on fundamental characteristics of
natural language and designed tests that evaluate
models on the criteria. We developed this evalua-
tion methodology using PCFG-generated corpora
and applied it on a case study to compare different
models of learning distributional representations.

While we showed important differences of the
embedding models, the goal was not to do a com-
prehensive comparison of them. We proposed an
innovative way of doing intrinsic evaluation of
embeddings. Our evaluation method gave direct
insight about the quality of embeddings. Addi-
tionally, while most intrinsic evaluations consider

8The spearman correlation between MLP and 1NN for
all=0.31, head=0.03, tail=0.75.

word vectors as points, we used classifiers that
identify different small subspaces of the full space.
This is an important desideratum when designing
evaluation methods because of the multifaceted-
ness of natural language words: they have a large
number of properties, each of which only occupies
a small proportion of the full-space capacity of the
embedding.

Based on this paper, there are serveral lines of
investigation we plan to conduct in the future. (i)
We will attempt to support our results on arti-
ficially generated corpora by conducting experi-
ments on real natural language data. (ii) We will
study the coverage of our four criteria in evalu-
ating word representations. (iii) We modeled the
four criteria using separate PCFGs, but they could
also be modeled by one single unified PCFG. One
question that arises is then to what extent the four
criteria are orthogonal and to what extent interde-
pendent. A single unified grammar may make it
harder to interpret the results, but may give addi-
tional and more fine-grained insights as to how the
performance of embedding models is influenced
by different fundamental properties of natural lan-
guage and their interactions.

Finally, we have made the simplifying assump-
tion in this paper that the best conceptual frame-
work for thinking about embeddings is that the
embedding space can be decomposed into sub-
spaces: either into completely orthogonal sub-
spaces or – less radically – into partially “over-
lapping” subspaces. Furthermore, we have made
the assumption that the smoothness and robustness
properties that are the main reasons why embed-
dings are used in NLP can be reduced to similar-
ities in subspaces. See Rothe et al. (2016) and
Rothe and Schütze (2016) for work that makes
similar assumptions.

The fundamental assumptions here are decom-
posability and linearity. The smoothness proper-
ties could be much more complicated. However
even if this was the case, then much of the gen-
eral framework of what we have presented in this
paper would still apply; e.g., the criterion that a
particular facet be fully and correctly represented
is as important as before. But the validity of the
assumption that embedding spaces can be decom-
posed into “linear” subspaces should be investi-
gated in the future.

Acknowledgments. This work was supported
by DFG (SCHU 2246/8-2).

244



References
Eneko Agirre, Enrique Alfonseca, Keith B. Hall, Jana

Kravalova, Marius Pasca, and Aitor Soroa. 2009.
A study on similarity and relatedness using distri-
butional and wordnet-based approaches. In Human
Language Technologies: Conference of the North
American Chapter of the Association of Computa-
tional Linguistics, Proceedings, May 31 - June 5,
2009, Boulder, Colorado, USA, pages 19–27.

Marco Baroni and Alessandro Lenci. 2010. Dis-
tributional memory: A general framework for
corpus-based semantics. Computational Linguis-
tics, 36(4):673–721.

Marco Baroni, Georgiana Dinu, and Germán
Kruszewski. 2014. Don’t count, predict! A
systematic comparison of context-counting vs.
context-predicting semantic vectors. In Proceedings
of the 52nd Annual Meeting of the Association
for Computational Linguistics, ACL 2014, pages
238–247.

Bernard Comrie. 1989. Language universals and lin-
guistic typology: Syntax and morphology. Black-
well, 2nd edition.

D. A. Cruse. 1986. Lexical Semantics. Cambridge
University Press, Cambridge, MA.

Scott Deerwester, Susan T. Dumais, George W. Fur-
nas, Thomas K. Landauer, and Richard Harshman.
1990. Indexing by latent semantic analysis. Jour-
nal of the American Society for Information Science,
41(6):391–407.

Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, Xiang-
Rui Wang, and Chih-Jen Lin. 2008. Liblinear: A
library for large linear classification. Journal of Ma-
chine Learning Research, 9:1871–1874.

Evgeniy Gabrilovich, Michael Ringgaard, and Amar-
nag Subramanya. 2013. Facc1: Freebase annotation
of clueweb corpora.

Eric H. Huang, Richard Socher, Christopher D. Man-
ning, and Andrew Y. Ng. 2012. Improving word
representations via global context and multiple word
prototypes. In The 50th Annual Meeting of the Asso-
ciation for Computational Linguistics, Proceedings
of the Conference, July 8-14, 2012, Jeju Island, Ko-
rea - Volume 1: Long Papers, pages 873–882.

Arne Köhn. 2015. What?s in an embedding? ana-
lyzing word embeddings through multilingual eval-
uation. In Proceedings of the 2015 Conference on
Empirical Methods in Natural Language Process-
ing, pages 2067–2073, Lisbon, Portugal, September.

Siwei Lai, Kang Liu, Liheng Xu, and Jun Zhao. 2015.
How to generate a good word embedding? CoRR,
abs/1507.05523.

Omer Levy and Yoav Goldberg. 2014a. Linguistic reg-
ularities in sparse and explicit word representations.
In CoNLL.

Omer Levy and Yoav Goldberg. 2014b. Neural word
embedding as implicit matrix factorization. In Ad-
vances in Neural Information Processing Systems
27: Annual Conference on Neural Information Pro-
cessing Systems 2014, December 8-13 2014, Mon-
treal, Quebec, Canada, pages 2177–2185.

Omer Levy, Yoav Goldberg, and Ido Dagan. 2015. Im-
proving distributional similarity with lessons learned
from word embeddings. TACL, 3:211–225.

Jiwei Li and Dan Jurafsky. 2015. Do multi-sense em-
beddings improve natural language understanding?
In Proceedings of the 2015 Conference on Empiri-
cal Methods in Natural Language Processing, pages
1722–1732, Lisbon, Portugal, September.

Wang Ling, Chris Dyer, Alan W. Black, and Isabel
Trancoso. 2015. Two/too simple adaptations of
word2vec for syntax problems. In NAACL HLT
2015, The 2015 Conference of the North American
Chapter of the Association for Computational Lin-
guistics: Human Language Technologies, Denver,
Colorado, USA, May 31 - June 5, 2015, pages 1299–
1304.

Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey
Dean. 2013. Efficient estimation of word represen-
tations in vector space. In Proceedings of ICLR.

Andriy Mnih and Koray Kavukcuoglu. 2013. Learning
word embeddings efficiently with noise-contrastive
estimation. In NIPS, pages 2265–2273.

Jeffrey Pennington, Richard Socher, and Christo-
pher D. Manning. 2014. Glove: Global vectors for
word representation. In EMNLP, pages 1532–1543.

Joseph Reisinger and Raymond J Mooney. 2010.
Multi-prototype vector-space models of word mean-
ing. In Human Language Technologies: The 2010
Annual Conference of the North American Chap-
ter of the Association for Computational Linguistics,
pages 109–117. Association for Computational Lin-
guistics.

Sascha Rothe and Hinrich Schütze. 2016. Word
embedding calculus in meaningful ultradense sub-
spaces. In ACL.

Sascha Rothe, Sebastian Ebert, and Hinrich Schütze.
2016. Ultradense embeddings by orthogonal trans-
formation. In NAACL.

Dana Rubinstein, Effi Levi, Roy Schwartz, and Ari
Rappoport. 2015. How well do distributional mod-
els capture different types of semantic knowledge?
In Proceedings of the 53rd Annual Meeting of the
Association for Computational Linguistics and the
7th International Joint Conference on Natural Lan-
guage Processing of the Asian Federation of Natural
Language Processing, ACL 2015, July 26-31, 2015,
Beijing, China, Volume 2: Short Papers, pages 726–
730.

245



Magnus Sahlgren. 2006. The Word-Space Model.
Ph.D. thesis, Stockholm University.

Tobias Schnabel, Igor Labutov, David Mimno, and
Thorsten Joachims. 2015. Evaluation methods for
unsupervised word embeddings. In Proceedings of
the 2015 Conference on Empirical Methods in Nat-
ural Language Processing, pages 298–307, Lisbon,
Portugal, September.

Yulia Tsvetkov, Manaal Faruqui, Wang Ling, Guil-
laume Lample, and Chris Dyer. 2015. Evaluation of
word vector representations by subspace alignment.
In Proceedings of the 2015 Conference on Empiri-
cal Methods in Natural Language Processing, pages
2049–2054, Lisbon, Portugal, September.

Peter D. Turney and Patrick Pantel. 2010. From fre-
quency to meaning: Vector space models of seman-
tics. J. Artif. Intell. Res. (JAIR), 37:141–188.

Yadollah Yaghoobzadeh and Hinrich Schütze. 2015.
Corpus-level fine-grained entity typing using con-
textual information. In Proceedings of the 2015
Conference on Empirical Methods in Natural Lan-
guage Processing, pages 715–725, Lisbon, Portugal,
September.

Zhi Zhong and Hwee Tou Ng. 2010. It makes sense:
A wide-coverage word sense disambiguation sys-
tem for free text. In ACL 2010, Proceedings of the
48th Annual Meeting of the Association for Com-
putational Linguistics, July 11-16, 2010, Uppsala,
Sweden, System Demonstrations, pages 78–83.

246


