



















































Atypical Inputs in Educational Applications


Proceedings of NAACL-HLT 2018, pages 60–67
New Orleans, Louisiana, June 1 - 6, 2018. c©2017 Association for Computational Linguistics

Atypical Inputs in Educational Applications

Su-Youn Yoon, Aoife Cahill, Anastassia Loukina,
Klaus Zechner, Brian Riordan, Nitin Madnani

Educational Testing Service
660 Rosedale Road, Princeton, NJ

syoon, acahill, aloukina, kzechner, briodan, nmadnani@ets.org

Abstract

In large-scale educational assessments, the use
of automated scoring has recently become
quite common. While the majority of student
responses can be processed and scored with-
out difficulty, there are a small number of re-
sponses that have atypical characteristics that
make it difficult for an automated scoring sys-
tem to assign a correct score. We describe a
pipeline that detects and processes these kinds
of responses at run-time. We present the most
frequent kinds of what are called non-scorable
responses along with effective filtering mod-
els based on various NLP and speech process-
ing technologies. We give an overview of two
operational automated scoring systems —one
for essay scoring and one for speech scoring—
and describe the filtering models they use. Fi-
nally, we present an evaluation and analysis of
filtering models used for spoken responses in
an assessment of language proficiency.

1 Introduction

An automated scoring system can assess con-
structed responses such as essays to open-ended
questions faster than human raters, often at lower
cost, with the resulting scores being consistent
over time. These advantages have prompted
strong demand for high-performing automated
scoring systems for various educational applica-
tions. However, even state-of-the-art automated
scoring systems face numerous challenges when
used in a large-scale operational setting. For in-
stance, some responses have atypical characteris-
tics that make it difficult for an automated scor-
ing system to provide a valid score. A spoken
response, for example, with a lot of background
noise may suffer from frequent errors in auto-
mated speech recognition (ASR), and the linguis-
tic features generated from the erroneous ASR
word hypotheses may be inaccurate. As a result,

the automated score based on the inaccurate fea-
tures may differ greatly from the score a human
expert would assign. Furthermore, it may substan-
tially weaken the validity of the automated scor-
ing system.1 More recently, some studies have
systematically evaluated the impact of atypical in-
puts, particularly gaming responses, on the va-
lidity of automated scoring of essays (Lochbaum
et al., 2013) and short-answers (Higgins and Heil-
man, 2014). They showed that automated scor-
ing systems tend to be more vulnerable than hu-
man raters to students trying to game the system.
Consistent with these findings, Zhang (2013) ar-
gued that the ability to detect abnormal perfor-
mance is one of the most important requirements
of a high-stakes automated scoring system. How-
ever, despite its importance, and compared to the
large body of work describing the empirical per-
formance of automated scoring systems, there has
been little discussion of how NLP tools and tech-
niques can contribute to improving the detection
of atypical inputs.

In this paper we present a typical processing
pipeline for automated scoring of essay and spo-
ken responses and describe the points in the pro-
cess where handling of atypical inputs and system
failures can occur. In particular, we describe some
of the NLP technologies used at each of these
points and the role of NLP components as filters
in an automated scoring pipeline. We present a
case study on building automated filters to detect
problematic responses in an assessment of spoken
English.

2 Detecting Atypical Inputs

In this section, we give an overview of an auto-
mated scoring pipeline and describe how atypical

1Test validity is the extent to which a test accurately mea-
sures what it is supposed to measure.

60



inputs can be detected and processed in applica-
tions that automatically score the language quality
of spoken responses and essays.

A typical automated scoring pipeline has three
major components: (1) the student response is
captured by the input capture module; (2) the sys-
tem computes a wide range of linguistic features
that measure various aspects of language profi-
ciency using NLP and/or speech processing tech-
nologies; and (3) a pre-trained automated scoring
model predicts a proficiency score using the lin-
guistic features. The components above the dotted
line in Figure 1 illustrate the typical processing se-
quence.

However, this scoring pipeline is a simplified
version of what happens in a real-life operational
system. In a large-scale assessment, there are usu-
ally a small number of atypical responses, where
the automated scoring system would have diffi-
culty in predicting valid scores.2 We call these
problematic responses non-scorable. In order to
handle these problematic inputs in real-time, we
can add filtering models (FMs) as sub-modules
of the automated scoring systems. The filter-
ing models detect and process different kinds of
problematic inputs at different points in the typ-
ical pipeline. Figure 1 indicates three points at
which filtering models could be employed in an
operationally-deployed automated scoring system
(below the dotted line). The points at which FMs
could be introduced are after (1) input capture,
(2) feature generation and (3) score generation.
Responses that are flagged by the FMs are han-
dled in different ways depending on the applica-
tion context. The responses can receive an auto-
mated score (with a warning that it is unreliable),
they can be rejected and receive no score, or they
can be sent to a human rater for manual scoring.

Analogous to the FMs, non-scorable responses
can also be classified into three main groups.
Non-scorable responses in the feature generation
and score generation groups are mostly system-
initiated non-scorable responses, where system
components had critical errors at the feature gen-
eration or score generation stages. Non-scoreable
responses in the input capture group can be further
classified into (a) system-initiated and (b) user-

2Many scoring systems could produce some score for
these problematic responses, however it is unlikely to be cor-
rect. It is therefore important for the overall validity of the
test and automated scoring system, to be able to identify such
responses and treat them correctly.

initiated non-scorable responses. User-initiated
non-scorable responses can occur for a number of
reasons, both for good faith (where the students
try their best to answer the question appropriately)
and bad faith (where the students do not make
a reasonable attempt to answer the question) at-
tempts. Students making good-faith attempts to
answer questions may still present an automated
scoring system with atypical input. For example,
in a speaking assessment, inappropriate distance
to the microphone may result in a distorted record-
ing; or in a written assessment a student may mis-
read the question, and unintentionally write an
off-topic response. Bad faith responses can come
from unmotivated students, or students trying to
game the system. These responses represent a
wide range of (often very creative) inputs that can
be troublesome for an automated scoring system
that is ill-prepared to handle them. If system com-
ponents fail or the system is not confident in its
prediction, the automated processing pipeline also
needs to be able to handle this correctly and fairly.

Next we describe some of the techniques that
have been proposed for detecting non-scorable re-
sponses at each of the stages.

2.1 Input Capture Filtering Models

Some system-initiated errors that should be
flagged at the input capture stage are: severe audio
problems typically caused by equipment malfunc-
tion, background noise, or problems with test tak-
ers’ recording level (speaking proficiency assess-
ment) or text capture failures (writing proficiency
assessment).

There is also a wide range of potential user-
initiated non-scorable responses. Some of the
most frequent categories include (a) response in
non-target language; (b) off-topic; (c) generic re-
sponses;3 (d) repetition of the question; (e) canned
responses;4 (f) banging on the keyboard; and (g)
no-response.

Five of the categories mentioned relate to topi-
cality. Off-topic responses and generic responses
are unrelated to the prompt, while the prompt-
repetition responses and canned responses can be
considered repetition or plagiarism. For auto-
mated essay scoring, off-topic detection systems

3Responses that only include simple unrelated sen-
tences such as “I don’t know,” “this is too difficult”
“why do I have to answer,” etc.

4Responses that only include memorized segments from
external sources (often websites)

61



Figure 1: A diagram of the overall architecture of a generic automated scoring pipeline. Above the dotted line
are the key stages in automated scoring. Below the dotted line are the possible additions to the pipeline to handle
atypical inputs using filtering models (FMs).

have been developed based on question-specific
content models, such as a standard vector space
model (VSM) built for each question (Bernstein
et al., 2000; Higgins et al., 2006; Louis and Hig-
gins, 2010).

For speaking tests eliciting highly or moder-
ately restricted speech, filtering models based on
features derived from ASR systems such as nor-
malized confidence scores and language model
(LM) scores can achieve good performance in
identifying topic-related non-scorable responses
(van Doremalen et al., 2009; Lo et al., 2010;
Cheng and Shen, 2011). However, this approach
is not appropriate for a speaking test that elic-
its unconstrained spontaneous speech. More re-
cently, similar to techniques that have been ap-
plied in essay scoring, systems based on document
similarity measures and topic detection were de-
veloped to detect spoken non-scorable responses.
In addition, neural networks and word embed-
dings, which have the advantage of capturing top-
ically relevant words that are not identical, have
been used in Malinin et al. (2017) and Yoon et al.
(2017), and this has resulted in further improve-
ments over systems using only traditional lexical
similarity features.

Unlike off-topic responses, canned responses
include pre-existing material. These can often be
identified by matching responses to test prepara-
tion websites or other student responses. Potthast
et al. (2014) give an overview of approaches to
detecting plagiarism in written texts based on the
systems that competed in the PAN-2014 shared

task on plagiarism detection. Wang et al. (2016)
developed a spoken canned response detection
system using similar techniques applied in essay
plagiarism detection.

In addition, various speech processing and NLP
techniques have also been used to detect other
types of non-scorable responses: language iden-
tification technology for non-English detection
(Yoon and Higgins, 2011) and speaker recogni-
tion technology for automated impostor detec-
tion (Qian et al., 2016). “Banging on the key-
board” can be identified by analyzing part-of-
speech sequences and looking for ill-formed se-
quences (Higgins et al., 2006).

2.2 Feature Generation Filtering Models

The most typical way for a response to be flagged
at the Feature Generation stage is for an inter-
nal component to fail. For example, in an au-
tomated speech scoring system the ASR system,
or the speech-signal processing component may
fail. In addition, parsers and taggers also some-
times fail to produce analyses, particularly on ill-
formed language-learner responses. In order to de-
tect sub-optimal ASR performance, filtering mod-
els have been developed using signal processing
technology and features derived from ASR sys-
tems, e.g., confidence scores and normalized LM
scores (Jeon and Yoon, 2012; Cheng and Shen,
2011).

It should be noted that while some of the user-
initiated non-scorable responses undetected at the
input capture stage would likely also cause fea-

62



ture generation failures (e.g., no-speech may cause
empty ASR hypothesis which result in feature
generation failure), other types (e.g., gaming re-
sponses) would simply cause subtle differences
in feature values leading to potential inflation of
the automated scores without causing clear sub-
process failures.

2.3 Score Generation Filtering Models
It is also possible to identify responses that may
not have received a correct score from the auto-
mated scoring system by looking at the output of
the scoring model directly. van Dalen et al. (2015)
developed an automated essay scoring system that
uses a Gaussian process to not only generate pro-
ficiency scores, but also give a measure of the un-
certainty of the generated score. They proposed a
process that uses the uncertainty measure to filter
responses with high uncertainty and send them to
human raters for scoring.

2.4 Adjusting Scoring for
non-scorable responses

We consider two main scoring scenarios:

• Human raters in the loop: there are sev-
eral ways that human scoring can be com-
bined with automated scoring. The two most
common situations are co-grading (a major-
ity of responses are scored by both human
raters and the automated scoring system) and
hybrid scoring (a majority of the responses
are scored by the automated scoring system,
while only subset of responses are scored by
human raters for quality control purposes).

• Sole automated scoring: all responses are
scored by only the automated scoring sys-
tem; there are no humans involved in scoring.
Such situations could include practice tests or
classroom tools.

If a response is flagged as non-scorable in a
scoring situation that has a human in the loop, the
most typical behavior is for the response to be sent
to a human rater for additional scoring. The score
from the automated system may or may not be
combined with human scores, depending on the
use case and the kind of flag.

If a response is flagged as non-scorable in a sole
scoring situation, there are two main ways to pro-
cess the response. Either no score is given and a
message is returned to the user that their response

could not be successfully processed. Or alterna-
tively, a score is given with a warning that it is
unreliable.

3 Practical Implementation of Filtering
Models

In this section we describe two systems for
automated scoring of CRs: (1) e-rater – an
automated scoring system for essays and (2)
SpeechRaterSM – an automated scoring system
for spoken responses. We describe the kinds of
filters used by both systems.

3.1 Automated Essay Scoring

The e-rater system (Attali and Burstein, 2006) au-
tomatically evaluates the writing quality of es-
says by taking key aspects of writing into ac-
count. It aligns the writing construct, via scor-
ing rubrics, to NLP methods that identify vari-
ous linguistic features of writing. The feature
classes include the following: (a) grammatical er-
rors (e.g., subject-verb agreement errors), (b) word
usage errors (e.g., their versus there), (c) errors in
writing mechanics (e.g., spelling), (d) presence of
essay-based discourse elements (e.g., thesis state-
ment, main points, supporting details, and conclu-
sions), (e) development of essay-based discourse
elements, (f) a feature that considers correct us-
age of prepositions and collocations (e.g., power-
ful computer vs. strong computer), and (g) sen-
tence variety. The features are combined in a sim-
ple linear model learned from an appropriate data
sample for the target populations.

In a high-stakes testing scenario with e-rater,
there is a human in the loop. The first step in the
pipeline is that a human rater reviews the essay
(FM1). If they deem the essay to be non-scorable
(e.g., because it is off-topic, or gibberish), the es-
say is immediately sent to another human for adju-
dication and there is no automated score produced.

If the first human rater assigns a valid score to
the essay, it is then passed to the e-rater engine.
The e-rater engine applies a filter that does auto-
mated garbage detection as its first step and filters
out responses that it detects as being non-English
(FM1). This filter uses unusual POS tag sequences
to identify non-English responses. The number
of such responses should be very low with a hu-
man as the first filter. Non-garbage responses are
then passed to the next stage of processing – fea-
ture extraction. At this point there are two differ-

63



ent kinds of filters. The first kind of filter com-
pares the response to other responses seen dur-
ing the training of the model. If the current re-
sponse is too dissimilar (in terms of length, vocab-
ulary, etc.), it is flagged (FM2). These filters rely
on typical textual-similarity techniques including
content-vector analysis (Salton et al., 1975) and
distributional similarity. The second kind of filter
flags responses for which the engine cannot reli-
ably determine a score because an internal com-
ponent (e.g., a parser) has indicated that its output
may be unreliable (FM2). In both cases flagged
responses are sent to a human for adjudication.

Table 1 gives a summary of the frequency of
each of the types of filters applied in a high-stakes
standardized test of English proficiency. The val-
ues were computed using a large sample of over 2
million candidate essays. The final score for the
responses can range from 0–5. It can be seen that
the average final score assigned to most of the es-
says flagged is very low. The average final score
assigned to essays automatically flagged as being
too dissimilar to the training data is higher. Typi-
cally this category of flags are designed to be con-
servative and sometimes flag perfectly reasonable
essays, simply to err on the side of caution.

Freq.
(%)

Avg.
Score

FM1 (human) 0.47 0.04
FM1 (automated) 0.02 0.96
FM2 (dissimilar) 0.95 2.65
FM2 (engine uncertainty) 0.06 1.09

Table 1: Frequency of different kinds of filters in high-
stakes e-rater deployment with the average final score
assigned to the responses flagged by each filter.

3.2 Automated Speech Scoring

The SpeechRaterSM system is an automated
oral proficiency scoring system for non-native
speakers’ of English (Zechner et al., 2009). It
has been operationally deployed to score low-
stakes speaking practice tests. In order to score
a spoken response, the input capture module in
SpeechRaterSM records the audio. Next, the
ASR system generates word hypotheses and time
stamps. The feature generation modules create a
wide range of linguistic features measuring flu-
ency, pronunciation and prosody, vocabulary and
grammar usage based on the ASR outputs and

NLP and speech processing technologies (e.g., a
POS tagger, a dependency parser, pitch and en-
ergy analysis software). In addition, it generates
a set of features to monitor the quality of ASR and
the audio quality of input responses.

Because of the low-stakes nature of the tests,
only limited types of non-scorable responses have
so far been observed in the data. There were some
system-initiated non-scorable responses. Of the
user-initiated non-scorable responses, the major-
ity are no-response and the proportion of gaming
responses is close to none. As a result, the filter-
ing models in SpeechRaterSM system are much
simpler than the e-rater system; it consists of just
one FM at the location of FM2, comprised of a set
of rules along with a statistical model based on a
subset of SpeechRaterSM features. A detailed
description and evaluation of its performance is
summarized in Section 4. Finally, non-flagged re-
sponses are scored by the automated scoring mod-
uleand flagged responses are not scored.

4 Case Study: Developing filtering
models for an Automated Speech
Scoring System

In this section, we will introduce a filtering model
for SpeechRaterSM developed for a low-stakes
English proficiency practice test comprised of
multiple questions which elicit unconstrained and
spontaneous speech with duration of 45 to 60 sec-
onds. All responses were scored by the automated
scoring system (sole automated scoring scenario).

We collected 6, 000 responses from 1, 000 test
takers, and expert human raters assigned a score
on a scale of 1 to 4, where 1 indicates a low
speaking proficiency and 4 indicates a high speak-
ing proficiency. In addition, the raters also an-
notated whether each response fell into the in-
put capture non-scorable group.5 A total of
605 responses (10.1%) were annotated as being
non-scorable responses. The majority of them
were due to recording failures (7.0%), followed
by no-response (3.0%) and non-English (0.1%).
The data was randomly partitioned into Model
Training (4, 002 responses) and Model Evaluation
(1, 998 responses).

The ASR system was based on a gender-
independent acoustic model and a trigram lan-

5The human raters did not annotate any errors caused by
system component failures (system errors at the feature gen-
eration and scoring model stage).

64



guage model trained on 800 hours of spoken re-
sponses extracted from the same English profi-
ciency test (but not overlapping with the Model
Building set) using the Kaldi toolkit (Povey et al.,
2011). In a separate study, the ASR system
achieved a Word Error Rate (WER) of 37% on
the held-out dataset (Tao et al., 2016). Although
the performance of our ASR on non-native speak-
ers’ spontaneous speech was a state-of-the art, it
showed substantially high WER for a small num-
ber of responses. Our analysis showed that the fea-
ture values and the scores for such responses were
unreliable.

Initially, we developed two FMs. The Baseline
FM was comprised of a statistical model trained on
the Model Training partition and a set of rules to
detect no-responses and responses with poor audio
quality (recording errors). The extended FM was
comprised of the baseline FM and an additional
rule, ASRErrorFilter to detect responses for which
the ASR result is likely to be highly erroneous.

The performance of two FMs on the evaluation
dataset is given in Table 2.

%
flagged

acc. pre. recall fscore

Baseline 9% 0.96 0.83 0.80 0.81
Extended 13% 0.95 0.66 0.90 0.76

Table 2: Performance of FMs in detecting non-
scorable.

The accuracy and fscore of the baseline FM was
0.96 and 0.81, respectively. The extended model
achieved a higher recall with slightly lower accu-
racy and fscore than the baseline. This was an ex-
pected result, since the extended model was de-
signed to detect responses with high ASR errors
that are likely to cause high human-machine score
difference (HMSD) and we choose to err on the
side of caution and flag more responses at the risk
of flagging some good ones.

In order to investigate to what extent the ex-
tended FM can identify responses with a large
HMSD, we also calculated an absolute HMSD for
each response. After excluding responses anno-
tated as non-scorable by human raters, the remain-
ing 1, 775 responses in the evaluation set were
used in this analysis. Table 4 shows the distribu-
tion of absolute system-human score differences
for flagged and unflagged responses by the ex-
tended FM.

# response
HMSD

mean SD max
Flagged 77 0.83 0.58 2.38
Unflagged 1698 0.48 0.34 1.96

Table 3: Average human-machine score differences.

The average HMSD for the flagged responses
was quite large (0.83), given that the proficiency
score scale was 1 to 4. Furthermore, it was
1.73 times higher than that of unflagged re-
sponses (0.48). The extended FM indeed cor-
rectly identified responses for which the auto-
mated scores were substantially different from the
human scores. In contrast, the number of re-
sponses flagged by the extended FM was sub-
stantially higher than the baseline FM. 4% of re-
sponses scorable by human raters were flagged
and they could not receive scores.

5 Discussion and Conclusions

We discussed the issue of atypical inputs in the
context of automated scoring. Non-scorable re-
sponses can cause critical issues in various sub-
processes of automated scoring systems and the
automated scores for these responses may differ
greatly from the correct scores. In order to address
this issue, we augmented a typical automated scor-
ing pipeline and included a set of filtering models
to detect non-scorable responses based on various
NLP and speech processing technologies. Finally,
we described two automated scoring systems de-
ployed to score essay and spoken responses from
large scale standardized assessments.

An alternative to the approach presented here
(individual filtering models for different kinds of
inputs), is to simply train one single classifier to
detect non-scorable responses (perhaps as an ad-
ditional score point). Depending on the context of
the automated scoring system, this may be suffi-
cient, however for our purposes, it was important
to have more fine grained control over the different
kinds of filters (setting thresholds, etc.). This gives
us the freedom to treat and route each of the types
of flags differently. This is particularly important
if a system is being used in both high-stakes and
low-stakes testing scenarios since the number and
type of non-scorable responses varies by scenario.

Section 4, in its comparison between two FMs,
highlights an important trade-off between the ac-
curacy of automated scores and the percentage of

65



filtered responses by the filtering model. By pro-
actively filtering out non-scorable responses, the
automated scoring system (the extended FM, in
this study) can prevent the generation of erroneous
scores and improve the quality and validity of the
automated scores. However, this may result in a
higher percentage of scoring failures which could
cause higher costs (e.g., additional human scoring,
or providing free retest or refund to the test takers).
These two important factors should be carefully
considered during system development.

References
Yigal Attali and Jill Burstein. 2006. Automated es-

say scoring with e–rater V.2. Journal of Technology,
Learning, and Assessment 4(3).

J. Bernstein, J. DeJong, D. Pisoni, and B. Townshend.
2000. Two experiments in automated scoring of
spoken language proficiency. In Proceedings of
the Workshop on Integrating Speech Technology in
Learning.

Jian Cheng and Jianqiang Shen. 2011. Off-topic detec-
tion in automated speech assessment applications.
In Proceedings of the 12th Annual Conference of
the International Speech Communication Associa-
tion. pages 1597–1600.

Derrick Higgins, Jill Burstein, and Yigal Attali. 2006.
Identifying off-topic student essays without topic-
specific training data. Natural Language Engineer-
ing 12(2):145–159.

Derrick Higgins and Michael Heilman. 2014. Manag-
ing what we can measure: Quantifying the suscep-
tibility of automated scoring systems to gaming be-
havior. Educational Measurement: Issues and Prac-
tice 33(3):36–46.

Je Hun Jeon and Su-Youn Yoon. 2012. Acoustic
feature-based non-scorable response detection for an
automated speaking proficiency assessment. In Pro-
ceedings of the 13th Annual Conference of the Inter-
national Speech Communication Association. pages
1275–1278.

Wai-Kit Lo, Alissa M Harrison, and Helen Meng. 2010.
Statistical phone duration modeling to filter for in-
tact utterances in a computer-assisted pronunciation
training system. In Proceedings of the Conference
on Acoustics, Speech and Signal Processing. pages
5238–5241.

Karen E Lochbaum, Mark Rosenstein, Peter Foltz, and
Marcia A Derr. 2013. Detection of gaming in auto-
mated scoring of essays with the IEA. Presented at
75th Annual meeting of NCME .

Annie Louis and Derrick Higgins. 2010. Off-topic es-
say detection using short prompt texts. In Proceed-
ings of the 5th Workshop on Innovative Use of NLP
for Building Educational Applications. pages 92–95.

Andrey Malinin, Kate Knill, Anton Ragni, Yu Wang,
and Mark JF Gales. 2017. An attention based model
for off-topic spontaneous spoken response detection:
An initial study. In Proceedings of the 7th Workshop
on Speech and Language Technology in Education.
pages 144–149.

Martin Potthast, Matthias Hagen, Anna Beyer,
Matthias Busse, Martin Tippmann, Paolo Rosso, and
Benno Stein. 2014. Overview of the 6th interna-
tional competition on plagiarism detection. In Pro-
ceedings of the CLEF Conference on Multilingual
and Multimodal Information Access Evaluation.

Daniel Povey, Arnab Ghoshal, Gilles Boulianne, Lukas
Burget, Ondrej Glembek, Nagendra Goel, Mirko
Hannemann, Petr Motlicek, Yanmin Qian, Petr
Schwarz, et al. 2011. The Kaldi speech recognition
toolkit. In Proceedings of the workshop on Auto-
matic Speech Recognition and Understanding.

Yao Qian, Jidong Tao, David Suendermann-Oeft, Kee-
lan Evanini, Alexei V Ivanov, and Vikram Rama-
narayanan. 2016. Noise and metadata sensitive bot-
tleneck features for improving speaker recognition
with non-native speech input. In Proceedings of the
17th Annual Conference of the International Speech
Communication Association. pages 3648–3652.

G. Salton, A. Wong, and C. S. Yang. 1975. A vector
space model for automatic indexing. Communica-
tions of the ACM pages 613–620.

Jidong Tao, Shabnam Ghaffarzadegan, Lei Chen, and
Klaus Zechner. 2016. Exploring deep learning
architectures for automatically grading non-native
spontaneous speech. In Proceedings of the Confer-
ence on Acoustics, Speech and Signal Processing.
pages 6140–6144.

Rogier C. van Dalen, Kate M. Knill, and Mark J. F.
Gales. 2015. Automatically grading learners en-
glish using a gaussian process. In Proceedings of
the Workshop on Speech and Language Technology
in Education. pages 7–12.

Joost van Doremalen, Helmet Strik, and Cartia Cuc-
chiarini. 2009. Utterance verification in language
learning applications. In Proceedings of the Work-
shop on Speech and Language Technology in Edu-
cation.

Xinhao Wang, Keelan Evanini, James Bruno, and
Matthew Mulholland. 2016. Automatic plagiarism
detection for spoken responses in an assessment of
english language proficiency. In Proceedings of the
Workshop on Spoken Language Technology Work-
shop (SLT). pages 121–128.

66



Su-Youn Yoon and Derrick Higgins. 2011. Non-
English response detection method for automated
proficiency scoring system. In Proceedings of the
6th Workshop on Innovative Use of NLP for Build-
ing Educational Applications. pages 161–169.

Su-Youn Yoon, Chong Min Lee, Ikkyu Choi, Xinhao
Wang, Matthew Mulholland, and Keelan Evanini.
2017. Off-topic spoken response detection with
word embeddings. In Proceedings of the 18th An-
nual Conference of the International Speech Com-
munication Association. pages 2754–2758.

Klaus Zechner, Derrick Higgins, Xiaoming Xi, and
David M. Williamson. 2009. Automatic scoring of
non-native spontaneous speech in tests of spoken
English. Speech Communication 51:883–895.

Mo Zhang. 2013. Contrasting automated and human
scoring of essays. ETS R & D Connections 21:1–11.

67


