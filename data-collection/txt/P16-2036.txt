



















































Don't Count, Predict! An Automatic Approach to Learning Sentiment Lexicons for Short Text


Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, pages 219–224,
Berlin, Germany, August 7-12, 2016. c©2016 Association for Computational Linguistics

Don’t Count, Predict! An Automatic Approach to Learning Sentiment
Lexicons for Short Text

Duy Tin Vo and Yue Zhang
Singapore University of Technology and Design

08 Somapah Road, Singapore 487372
duytin vo@mymail.sutd.edu.sg and yue zhang@sutd.edu.sg

Abstract

We describe an efficient neural network
method to automatically learn sentiment
lexicons without relying on any manual
resources. The method takes inspiration
from the NRC method, which gives the
best results in SemEval13 by leveraging
emoticons in large tweets, using the PMI
between words and tweet sentiments to de-
fine the sentiment attributes of words. We
show that better lexicons can be learned
by using them to predict the tweet senti-
ment labels. By using a very simple neu-
ral network, our method is fast and can
take advantage of the same data volume as
the NRC method. Experiments show that
our lexicons give significantly better accu-
racies on multiple languages compared to
the current best methods.

1 Introduction

Sentiment lexicons contain the sentiment polar-
ity and/or the strength of words or phrases (Bac-
cianella et al., 2010; Taboada et al., 2011; Tang et
al., 2014a; Ren et al., 2016a). They have been used
for both rule-based (Taboada et al., 2011) and un-
supervised (Turney, 2002; Hu and Liu, 2004; Kir-
itchenko et al., 2014) or supervised (Mohammad
et al., 2013; Tang et al., 2014b; Vo and Zhang,
2015) machine-learning-based sentiment analysis.
As a result, constructing sentiment lexicons is one
important research task in sentiment analysis.

Many approaches have been proposed to con-
struct sentiment lexicons. Traditional methods
manually label the sentiment attributes of words
(Hu and Liu, 2004; Wilson et al., 2005; Taboada
et al., 2011). One benefit of such lexicons is high
quality. On the other hand, the methods are time-
consuming, requiring language and domain exper-

tise. Recently, statistical methods have been ex-
ploited to learn sentiment lexicons automatically
(Esuli and Sebastiani, 2006; Baccianella et al.,
2010; Mohammad et al., 2013). Such methods
leverage knowledge resources (Bravo-Marquez et
al., 2015) or labeled sentiment data (Tang et al.,
2014a), giving significantly better coverage com-
pared to manual lexicons.

Among the automatic methods, Mohammad et
al. (2013) proposed to use tweets with emoticons
or hashtags as training data. The main advantage
is that such training data are abundant, and manual
annotation can be avoided. Despite that emoticons
or hashtags can be noisy in indicating the senti-
ment of a tweet, existing research (Go et al., 2009;
Pak and Paroubek, 2010; Agarwal et al., 2011;
Kalchbrenner et al., 2014; Ren et al., 2016b) has
shown that effectiveness of such data when used
to supervise sentiment classifiers.

Mohammad et al. (2013) collect sentiment lexi-
cons by calculating pointwise mutual information
(PMI) between words and emoticons. The result-
ing lexicons give the best results in a SemEval13
benchmark (Nakov et al., 2013). In this paper,
we show that a better lexicon can be learned by
directly optimizing the prediction accuracy, tak-
ing the lexicon as input and emoticon as the out-
put. The correlation between our method and the
method of Mohammad et al. (2013) is analogous
to the “predicting” vs “counting” correlation be-
tween distributional and distributed word repre-
sentations (Baroni et al., 2014).

We follow Esuli and Sebastiani (2006) in us-
ing two simple attributes to represent each sen-
timent word, and take inspiration from Mikolov
et al. (2013) in using a very simple neural net-
work for sentiment prediction. The method can
leverage the same data as Mohammad et al.
(2013) and therefore benefits from both scale
and annotation independence. Experiments show

219



that the neural model gives the best results on
standard benchmarks across multiple languages.
Our code and lexicons are publicly available at
https://github.com/duytinvo/acl2016.

2 Related work

Existing methods for automatically learning sen-
timent lexicons can be classified into three main
categories. The first category augments existing
lexicons with sentiment information. For exam-
ple, Esuli and Sebastiani (2006) and Baccianella
et al. (2010) use a tuple (pos, neg, neu) to rep-
resent each word, where pos, neg and neu stand
for possibility, negativity and neutrality, respec-
tively, training these attributes by extracting fea-
tures from WordNet. These methods rely on the
taxonomic structure of existing lexicons, which
are limited to specific languages.

The second approach expands existing lexicons,
which are typically manually labeled. For exam-
ple, Tang et al. (2014a) apply a neural network to
learn sentiment-oriented embeddings from a small
amount of annotated tweets, and then expand a
set of seed sentiment words by measuring vector
space distances between words. Bravo-Marquez et
al. (2015) extend an existing lexicon by classifying
words using manual features. These methods are
also limited to domains and languages with man-
ual resources.

The third line of methods constructs lexicons
from scratch by accumulating statistical informa-
tion over large data. Turney (2002) proposes to es-
timate the sentiment polarity of words by calculat-
ing PMI between seed words and search hits. Mo-
hammad et al. (2013) improve the method by com-
puting sentiment scores using distance-supervised
data from emoticon-baring tweets instead of seed
words. This approach can be used to automatically
extract multilingual sentiment lexicons (Salameh
et al., 2015; Mohammad et al., 2015) without us-
ing manual resources, which makes it more flex-
ible compared to the first two methods. We con-
sider it as our baseline.

We use the same data source as Mohammad
et al. (2013) to train lexicons. However, rather
than relying on PMI, we take a machine-learning
method in optimizing the prediction accuracy of
emoticons using the lexicons. To leverage large
data, we use a very simple neural network to train
the lexicons.

Figure 1: Our model.

3 Baseline

Mohammad et al. (2013) employ emoticons and
relevant hashtags contained in a tweet as the senti-
ment label of the tweet. Given a set of tweets with
their labels, the sentiment score (SS) for a token w
was computed as:

SS(w) = PMI(w, pos)− PMI(w, neg), (1)

where pos represents the positive label and neg
represents the negative label. PMI stands for
pointwise mutual information, which is

PMI(w, pos) = log2
freq(w, pos) ∗N

freq(w) ∗ freq(pos) (2)

Here freq(w , pos) is the number of times the term
w occurs in positive tweets, freq(w) is the total
frequency of term w in the corpus, freq(pos) is
the total number of tokens in positive tweets, and
N is the total number of tokens in the corpus.
PMI (w ,neg) is calculated in a similar way. Thus,
Equation 1 is equal to:

SS(w) = log2
freq(w, pos) ∗ freq(neg)
freq(w, neg) ∗ freq(pos) (3)

4 Model

We follow Esuli and Sebastiani (2006), using pos-
itivity and negativity attributes to define lexicons.
In particular, each word takes the form w = (n, p),
where n denotes negativity and p denotes positiv-
ity (n, p ∈ R). As shown in Figure 1, given a
tweet tw = w1, w2, ..., wn, a simple neural net-
work is used to predict its two-dimensional senti-
ment label y, where [1,0] for negative and [0,1] for
positive tweets. The predicted sentiment probabil-
ity y of a tweet is computed as:

h =
∑

i

(wi) (4)

220



Language #pos #neg #Tweets
English 4.5M 4.5M 9M
Arabic 400k 400k 800k

Table 1: Emoticon-based training data.

y = softmax (hW ) (5)

where W is fixed to the diagonal matrix (W ∈
R2x2).

We follow Go et al. (2009) in defining the sen-
timent labels of tweets via emoticons. Each token
is first initialized by random negative and positive
attribute scores in [-0.25,0.25], and then trained
by supervised learning. The cross-entropy error
is employed as the objective function:

loss(tw) = −
∑

ŷ. log(y) (6)

Backpropagation is applied to learn (n, p) for each
token. Optimization is done using stochastic gra-
dient descent over shuffled mini-batches, with the
AdaDelta update rule (Zeiler, 2012). All mod-
els are trained over 5 epochs with a batch size of
50. Due to its simplicity, the method is very fast,
training a sentiment lexicon over 9 million tweets
within 35 minutes per epoch on an Intelr core™
i7-3770 CPU @ 3.40 GHz.

5 Sentiment Classification

The resulting lexicon can be used in both unsu-
pervised and supervised sentiment classifiers. The
former is implemented by summing the sentiment
scores of all tokens contained in a given document
(Taboada et al., 2011; Kiritchenko et al., 2014). If
the total sentiment score is larger than 0, the doc-
ument is classified as positive. Here only one pos-
itivity attribute is required to represent a lexicon,
and we use the contrast between the positivity and
negativity attributes (p− n) as the score.

The supervised method makes use of sentiment
lexicons as features for machine learning classifi-
cation. Given a document D, we follow Zhu et al.
(2014) and extract the following features:

• The number of sentiment tokens in D, where
sentiment tokens are word tokens whose sen-
timent scores are not zero in a lexicon;

• The total sentiment score of a document:∑
wi∈D SS(wi);

• The maximal score: maxwi∈DSS(wi);

Type #pos #neg #Tweets

Supervised
train 3009 1187 4196
dev 483 283 766
test 1313 490 1803

Unsupervised 4805 1960 6765

Table 2: Statistics of the Semeval13.

• The total scores of positive and negative
words in D;

• The sentiment score of the last token in D.

Again we use SS(wi) = pwi − nwi as the sen-
timent score of each word wi, because the meth-
ods are based on a single sentiment score value for
each word.

6 Experiments

6.1 Experimental Settings

Training data: To automatically obtain train-
ing data, we use the Twitter Developers API1 to
crawl emoticon tweets2 of English and Arabic
from February 2014 to September 2014. We fol-
low Go et al. (2009), removing all emoticons used
to collect training data from the tweets, and Tang
et al. (2014b), ignoring tweets which are less than
7 tokens. A Twitter tokenizer (Gimpel et al., 2011)
is applied to preprocess all tweets. Rare words that
occur less than 5 times in the vocabulary are re-
moved. HTTP links and username are replaced by
〈http〉 and 〈user〉, respectively. The statistics of
training data is shown in Table 1.

Sentiment classifier: We use LibLinear3 (Fan
et al., 2008) as the supervised classifier on bench-
mark datasets. The parameter c is tuned by making
a grid search (Hsu et al., 2003) on the accuracy of
development set on the English dataset and five-
fold cross validation on the Arabic dataset.

Evaluation: We follow Kiritchenko et al.
(2014) in employing precision (P), recall (R) and
F1 score (F) to evaluate unsupervised classifica-
tion. We follow Hsu et al. (2003) and use accuracy
(acc), the tuning criterion, to evaluate supervised
classification.

Code and lexicons: We make the Python
implementation of our models and the
resulting sentiment lexicons available at
https://github.com/duytinvo/acl2016

1https://dev.twitter.com/
2:), : ), :-), :D, =) for positive and :(, : (, :-( for negative
3https://www.csie.ntu.edu.tw/∼cjlin/liblinear/

221



Lexicons Unsup SupP R F Acc

WEKA ED 61 55.9 55.4 73.8STS 66.4 52.5 47.7 73.7
HIT 75.3 73.3 74.1 78.5

NRC Hashtag 70.3 71.4 70.8 77.4Emoticon 73.2 74.6 73.8 79.9
nnLexicon 74.4 77.3 75.3 81.3

Table 3: Results on SemEval13 (English).

Labels Balanced Unbalancedtrain dev test train dev test
#pos

481 159 159

481 159 159
#neg 1012 336 336
#mix 500 166 166
#obj 4015 1338 1338

#Tweets 1924 636 636 6008 1999 1999

Table 4: Standard splits of ASTD.

6.2 English Lexicons

The Twitter benchmark of SemEval13 (Nakov et
al., 2013) is used as the English test set. In or-
der to evaluate both unsupervised and supervised
methods, we follow Tang et al. (2014b) and Kir-
itchenko et al. (2014), removing neutral tweets.
The statistics is shown in Table 2. We compare our
lexicon with the lexicons of NRC4 (Mohammad et
al., 2013), HIT5 (Tang et al., 2014a) and WEKA6

(Bravo-Marquez et al., 2015). As shown in Table
3, using the unsupervised sentiment classification
method (unsup) in Section 5, our lexicon gives sig-
nificantly better result in comparison with count-
based lexicons of NRC. Under both settings, our
lexicon yields the best results compared to other
methods.

6.3 Arabic Lexicons

We employ the standard Arabic Twitter dataset
ASTD (Nabil et al., 2015), which consists of about
10,000 tweets with 4 labels: objective (obj), neg-
ative (neg), positive (pos) and mixed subjective
(mix). The standard splits of ASTD are shown in
Table 4. We follow Nabil et al. (2015) by merg-
ing training and validating data for learning model.
We compare our lexicon with only the lexicons of
NRC7 (Salameh et al., 2015), because the meth-
ods of Tang et al. (2014a) and Bravo-Marquez et
al. (2015) depend on manual resources, which are

4http://saifmohammad.com/WebPages/Abstracts/NRC-
SentimentAnalysis.htm

5http://ir.hit.edu.cn/∼dytang/
6http://www.cs.waikato.ac.nz/∼fjb11/
7http://saifmohammad.com/WebPages/ArabicSA.html

Lexicons Balanced Unbalanced

NRC Hashtag 31.9 63.4Emoticon 31.4 65.3
nnLexicon 33.3 66.5

Table 5: Results on ASTD (Arabic).

Words nnLexicon NRC
bad -1.122 -1.295
worse -1.626 -1.417
worst -2.256 -1.875
busy -0.520 -0.003
busier -0.609 0.106∗

busiest -1.254 -0.712
suitable 0.502 -0.040∗

satisfy 0.570 -0.173∗

lazy -0.462 0.224∗

scummy -0.852 0.049∗

old wine 0.453 0.552
old meat -0.172 0.014∗

strong memory 0.081 -0.083∗

strong snowstorm -0.554 0.182∗

Table 6: Example sentiment scores, where ∗ de-
notes incorrect polarity.

not available. As shown in Table 5, our lexicon
consistently gives the best performance on both
the balanced and unbalanced datasets, showing the
advantage of “predicting” over “counting”.

6.4 Analysis

Table 6 shows examples of our predicting-based
lexicon and the counting-based lexicon of Mo-
hammad et al. (2013). First, both lexicons can
correctly reflect the strength of emotional words
(e.g. bad, worse, worst), which demonstrates that
our method can learn statistical relevance as effec-
tively as PMI. Second, we find many cases where
our lexicon gives the correct polarity (e.g. suit-
able, lazy) but the lexicon of Mohammad et al.
(2013) does not. To quantitatively compare the
lexicons, we calculated the accuracies of their po-
larities (i.e. sign) by using the manually-annotated
lexicon of Hu and Liu (2004) as the gold stan-
dard. We take the intersection between the au-
tomatic lexicons and the lexicon of Hu and Liu
(2004) as the test set, which contains 3270 words.
The polarity accuracy of our lexicon is 78.2%, in
contrast to 76.9% by the lexicon of Mohammad
et al. (2013), demonstrating the relative strength
of our method. Third, by having two attributes
(n, p) instead of one, our lexicon is better in com-
positionality (e.g. SS(strong memory) > 0,
SS(strong snowstorm) < 0).

222



7 Conclusion

We constructed a sentiment lexicon for short text
automatically using an efficient neural network,
showing that prediction-based training is better
than counting-based training for learning from
large tweets with emoticons. In standard evalu-
ations, the method gave better accuracies across
multiple languages compared to the state-of-the-
art counting-based method.

References
Apoorv Agarwal, Boyi Xie, Ilia Vovsha, Owen Ram-

bow, and Rebecca Passonneau. 2011. Sentiment
analysis of twitter data. In Proceedings of the Work-
shop on Languages in Social Media, LSM ’11, pages
30–38.

Stefano Baccianella, Andrea Esuli, and Fabrizio Sebas-
tiani. 2010. Sentiwordnet 3.0: An enhanced lexical
resource for sentiment analysis and opinion mining.
In Proceedings of LREC, volume 10, pages 2200–
2204.

Marco Baroni, Georgiana Dinu, and Germán
Kruszewski. 2014. Don’t count, predict! a
systematic comparison of context-counting vs.
context-predicting semantic vectors. In Proceedings
of ACL, pages 238–247.

Felipe Bravo-Marquez, Eibe Frank, and Bernhard
Pfahringer. 2015. Positive, negative, or neu-
tral: learning an expanded opinion lexicon from
emoticon-annotated tweets. In Proceedings of IJ-
CAI, pages 1229–1235.

Andrea Esuli and Fabrizio Sebastiani. 2006. Senti-
wordnet: A publicly available lexical resource for
opinion mining. In Proceedings of LREC, volume 6,
pages 417–422.

Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, Xiang-
Rui Wang, and Chih-Jen Lin. 2008. Liblinear: A
library for large linear classification. The Journal of
Machine Learning Research, 9:1871–1874.

Kevin Gimpel, Nathan Schneider, Brendan O’Connor,
Dipanjan Das, Daniel Mills, Jacob Eisenstein,
Michael Heilman, Dani Yogatama, Jeffrey Flanigan,
and A. Noah Smith. 2011. Part-of-speech tagging
for twitter: Annotation, features, and experiments.
In Proceedings of ACL-HLT, pages 42–47.

Alec Go, Richa Bhayani, and Lei Huang. 2009. Twit-
ter sentiment classification using distant supervision.
CS224N Project Report, Stanford, 1:12.

Chih-Wei Hsu, Chih-Chung Chang, Chih-Jen Lin, et al.
2003. A practical guide to support vector classifica-
tion.

Minqing Hu and Bing Liu. 2004. Mining and summa-
rizing customer reviews. In Proceedings of KDD,
KDD ’04, pages 168–177.

Nal Kalchbrenner, Edward Grefenstette, and Phil Blun-
som. 2014. A convolutional neural network for
modelling sentences. In Proceedings of ACL, pages
655–665.

Svetlana Kiritchenko, Xiaodan Zhu, and Saif M. Mo-
hammad. 2014. Sentiment analysis of short infor-
mal texts. J. Artif. Intell. Res., 50:723–762.

Tomas Mikolov, Kai Chen, Greg Corrado, and Jef-
frey Dean. 2013. Efficient estimation of word
representations in vector space. arXiv preprint
arXiv:1301.3781.

Saif M. Mohammad, Svetlana Kiritchenko, and Xiao-
dan Zhu. 2013. Nrc-canada: Building the state-of-
the-art in sentiment analysis of tweets. In Proceed-
ings of SemEval-2013, June.

Saif M Mohammad, Mohammad Salameh, and Svet-
lana Kiritchenko. 2015. How translation alters sen-
timent. Journal of Artificial Intelligence Research,
54:1–20.

Mahmoud Nabil, Mohamed Aly, and Amir F Atiya.
2015. Astd: Arabic sentiment tweets dataset. In
Proceedings of EMNLP, pages 2515–2519.

Preslav Nakov, Sara Rosenthal, Zornitsa Kozareva,
Veselin Stoyanov, Alan Ritter, and Theresa Wilson.
2013. Semeval-2013 task 2: Sentiment analysis in
twitter. In Second Joint Conference on Lexical and
Computational Semantics (*SEM), Volume 2: Pro-
ceedings of SemEval-2013, pages 312–320.

Alexander Pak and Patrick Paroubek. 2010. Twitter
based system: Using twitter for disambiguating sen-
timent ambiguous adjectives. In Proceedings of the
5th International Workshop on Semantic Evaluation,
SemEval ’10, pages 436–439.

Yafeng Ren, Yue Zhang, Meishan Zhang, and
Donghong Ji. 2016a. Context-sensitive twitter sen-
timent classification using neural network. In Pro-
ceedings of AAAI.

Yafeng Ren, Yue Zhang, Meishan Zhang, and
Donghong Ji. 2016b. Improving twitter sentiment
classification using topic-enriched multi-prototype
word embeddings. In Proceedings of AAAI.

Mohammad Salameh, Saif Mohammad, and Svetlana
Kiritchenko. 2015. Sentiment after translation: A
case-study on arabic social media posts. In Proceed-
ings of NAACL-HLT, pages 767–777, May–June.

Maite Taboada, Julian Brooke, Milan Tofiloski, Kim-
berly Voll, and Manfred Stede. 2011. Lexicon-
based methods for sentiment analysis. Computa-
tional linguistics, 37(2):267–307.

223



Duyu Tang, Furu Wei, Bing Qin, Ming Zhou, and Ting
Liu. 2014a. Building large-scale twitter-specific
sentiment lexicon : A representation learning ap-
proach. In Proceedings of COLING, pages 172–182,
August.

Duyu Tang, Furu Wei, Nan Yang, Ming Zhou, Ting
Liu, and Bing Qin. 2014b. Learning sentiment-
specific word embedding for twitter sentiment clas-
sification. In Proceedings of ACL, pages 1555–
1565, June.

Peter Turney. 2002. Thumbs up or thumbs down? se-
mantic orientation applied to unsupervised classifi-
cation of reviews. In Proceedings of ACL.

Duy-Tin Vo and Yue Zhang. 2015. Target-dependent
twitter sentiment classification with rich automatic
features. In Proceedings of IJCAI, pages 1347–
1353, July.

Theresa Wilson, Janyce Wiebe, and Paul Hoffmann.
2005. Recognizing contextual polarity in phrase-
level sentiment analysis. In Proceedings of HLT-
EMNLP.

Matthew D Zeiler. 2012. Adadelta: an adaptive learn-
ing rate method. arXiv preprint arXiv:1212.5701.

Xiaodan Zhu, Svetlana Kiritchenko, and Saif Moham-
mad. 2014. Nrc-canada-2014: Recent improve-
ments in the sentiment analysis of tweets. In Pro-
ceedings of SemEval-2014, pages 443–447.

224


