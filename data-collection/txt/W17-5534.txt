



















































Adversarial evaluation for open-domain dialogue generation


Proceedings of the SIGDIAL 2017 Conference, pages 284–288,
Saarbrücken, Germany, 15-17 August 2017. c©2017 Association for Computational Linguistics

Adversarial Evaluation for Open-Domain Dialogue Generation

Elia Bruni and Raquel Fernández
Institute for Logic, Language and Computation

University of Amsterdam
elia.bruni@gmail.com raquel.fernandez@uva.nl

Abstract

We investigate the potential of adversarial
evaluation methods for open-domain dia-
logue generation systems, comparing the
performance of a discriminative agent to
that of humans on the same task. Our re-
sults show that the task is hard, both for
automated models and humans, but that a
discriminative agent can learn patterns that
lead to above-chance performance.

1 Introduction

End-to-end dialogue response generation systems
trained to produce a plausible utterance given
some limited dialogue context are receiving in-
creased attention (Vinyals and Le, 2015; Sordoni
et al., 2015; Serban et al., 2016; Li et al., 2016).
However, for systems dealing with chatbot-style
open-dialogue, where task completion is not appli-
cable, evaluating the quality of their responses re-
mains a challenge. Most current models are evalu-
ated with measures such as perplexity and overlap-
based metrics like BLEU, that compare the gener-
ated response to the ground-truth response in an
actual dialogue. This kind of measures, however,
correlate very weakly or not at all with human
judgements on response quality (Liu et al., 2016).

In this paper, we explore a different approach to
evaluating open-domain dialogue response gener-
ation systems, inspired by the classic Turing Test
(Turing, 1950): measuring the quality of the gen-
erated responses on their indistinguishability from
human output. This approach has been prelimi-
nary explored in recent work under the heading
of adversarial evaluation (Kannan and Vinyals,
2016; Li et al., 2017), drawing a parallel with
generative adversarial learning (Goodfellow et al.,
2014). Here we concentrate on exploring the po-
tential and the limits of such an adversarial eval-

uation approach by conducting an in-depth anal-
ysis. We implement a discriminative model and
train it on the task of distinguishing between ac-
tual and “fake” dialogue excerpts and evaluate its
performance, as well as the feasibility of the task
more generally, by conducting an experiment with
human judgements. Results show that the task is
hard not only for the discriminative model, but
also for human judges. We then implement a sim-
ple chatbot agent for dialogue generation and test
the discriminator on this data, again comparing
its performance to that of humans on this task.
We show that both humans and the discriminative
model can be fooled by the generator in a signifi-
cant amount of cases.

2 The Discriminative Agent

Our discriminative agent is a binary classifier
which takes as input a sequence of dialogue ut-
terances and predicts whether the dialogue is real
or fake. The agent treats as positive examples of
coherent dialogue actual dialogue passages and as
negative examples passages where the last utter-
ance has been randomly replaced. Random re-
placement has been used in the past to study dis-
course coherence (Li and Hovy, 2014).

2.1 Model
The classifier is modelled as an attention-based
bidirectional LSTM. LSTMs are indeed very ef-
fective to model word sequences, and are espe-
cially suited for learning on data with long dis-
tance dependencies (Hochreiter and Schmidhuber,
1997) such as multi-turn dialogues. The bidi-
rectional LSTM includes both a forward function
(
−−−−→
LSTM, which reads the sentence si from wi1

to wiT ) and a backward function (
←−−−−
LSTM, which

reads the sentence si from wiT to wi1):

xit = Wewit, t ∈ [1, T ] [1]

284



−→
h it =

−−−−→
LSTM(xit), t ∈ [1, T ] [2]

←−
h it =

←−−−−
LSTM(xit), t ∈ [T, 1] [3]

The words of a dialogue turn do not always con-
tribute equally to determine coherence. We thus
use an attention mechanism to extract words that
are important to detect plausibility or coherence
of a dialogue passage and parametrize their aggre-
gation accordingly. Having an aggregated vector
representation which is adaptive to the content of
each time step allows the classifier to assign large
weights to the most “discriminative” words. Con-
temporarily, the attention should also have an ad-
vantage in modelling long sequences by consider-
ing different word locations in the dialogue in a
relatively even manner:

uit = tanh(Wwhit + bw) [4]

αit =
exp(uTituw)∑
t exp(u

T
ituw)

,
∑

i αihi [5]

We first compute the hidden representation of hit
through a one-layer MLP uit; we then weight the
importance of uit by computing its similarity to a
word-level context vector, normalized via a soft-
max function. The context vector is learned end-
to-end by the classifier and is meant to represent
a general query about the level of “discriminabil-
ity” of a word (see, e.g., Sukhbaatar et al. 2015 or
Yang et al. 2016). The output of the attention is
then fed to a sigmoid function, which returns the
probability of the input being real or fake:

p = sigmoid(W vc + bc) [6]

As loss function we then use the negative log like-
lihood of the correct labels:

L = −∑d log pdj [7]
2.2 Training Details

We trained the discriminator with a combination
of three different datasets: MovieTriples, SubTle
and Switchboard. MovieTriples (Serban et al.,
2016) has been created from the Movie-Dic cor-
pus of film transcripts (Banchs, 2012) and con-
tains 3-utterance passages between two interlocu-
tors who alternate in the conversation. SubTle

(Ameixa et al., 2014) is made of 2-utterance pas-
sages extracted from movie subtitles. To discour-
age the pairing of utterances coming from differ-
ent movie scenes, we selected only those pairs
with a maximum difference of 1 second between
the first and the second turn. Switchboard (God-
frey et al., 1992) is a corpus of transcribed tele-
phone conversations. We ignored utterances that
consist only of non-verbal acts such as laughter,
and selected sequences of three consecutive utter-
ances. In all cases, we consider the last utterance
of a passage the target response, and the previous
utterances, the context. For the three datasets, we
restrict ourselves to dialogue passages where the
context and the response have a length of 3 to 25
tokens each. We concatenated the three datasets,
obtaining a total of 3,289,835 dialogue passages
(46,499 from MovieTriples, 3,211,899 from Sub-
Tle, and 77,936 from Switchboard).

For training, we limit the vocabulary size to
the top 25K most frequent words.1 We used
mini-batch stochastic gradient descent, shuffling
the batches each epoch. We use a bidirectional
layer, with 500 cells, and 500-dimensional embed-
dings (we tried with more layers and higher num-
ber of cells without significant improvements).
All model parameters are uniformly initialized in
[−0.1, 0.1] and as optimizer we used Adam with
an initial learning rate of 0.001. Dropout with
probability 0.3 was applied to the LSTMs.

3 Human Evaluation

To assess the performance of our discriminative
model, we conduct an experiment with human an-
notators. To our knowledge, this is the first study
of its kind ever conducted. Previous human evalu-
ation experiments of dialogue generation systems
have mostly consisted in asking participants to
choose the better response between two options
generated by different models or to rate a gener-
ated dialogue along several dimensions (Vinyals
and Le, 2015; Lowe et al., 2017; Li et al., 2017).
In contrast, here we present humans with the same
task faced by the discriminator: We show them a
dialogue passage and ask them to decide whether,
given the first one or two utterances of context, the
shown continuation is the actual follow-up utter-
ance in the original dialogue or a random response.

The data for this experiment consists of 900 pas-

1All remaining words are converted into the universal to-
ken <unk>.

285



data discriminator humans agreement
real random real random Fleiss’ π

Acc P R F1 P R F1 Acc P R F1 P R F1 hum disc
SWB .583 .549 .933 .691 .778 .233 .359 .670 .650 .714 .690 .695 .604 .647 .299 .068
MOV .677 .645 .787 .709 .726 .567 .637 .677 .664 .713 .688 .690 .640 .664 .303 .258
SUB .737 .763 .687 .723 .715 .787 .749 .640 .635 .660 .647 .646 .620 .633 .304 .301

Table 1: Accuracy, Precision, Recall, and F-score of discriminator and humans against ground-truth.
Inter-annotator agreement among humans and between the discriminator and the human majority class.

sages: 300 randomly selected per dataset, with
50% real and 50% fake dialogues. We use the
CrowdFlower platform to recruit annotators, re-
stricting the pool to English native speakers.2

Each item is classified as real or random by three
different annotators. A total of 137 annotators par-
ticipated in the experiment, with each of them an-
notating between 10 and 150 items.

We test the discriminator on the same data
and compare its performance to the human judge-
ments. Chance level accuracy for both humans
and the discriminator is 50%, namely when real
and fake passages are indistinguishable from each
other. The results are summarised in Table 1. Let
us first consider the performance of humans on
the task. We compute inter-annotator agreement
using Fleiss π (Fleiss, 1971), suitable for assess-
ing multi-coder annotation tasks. Agreement is
low: π = 0.30 across the 3 corpora, indicating
that the task is challenging for humans (there is
limited consensus on whether the shown dialogue
passages are plausible or not). Looking into the
human performance with respect to the ground
truth, we see similar accuracy scores for Switch-
board and MovieTriples, while accuracy is lower
for SubTle, where the context consists of one ut-
terance only. Across the three datasets, we observe
slightly higher F-score for positive instances (real)
than negative instances (random). For the positive
instances, recall is higher than precision, while the
opposite is true for negative instances. Arguably,
this indicates that humans tend to accommodate
responses that in fact are random as possible co-
herent continuations of a dialogue, and will only
flag them as fake if they are utterly surprising.

We compute the agreement of the discrimina-
tor’s predictions and the human majority class
over 3 annotators. For Switchboard, agreement is
at chance level (π = .07), while for the other two

2We use strict quality controls, only accepting annota-
tors considered “highly trusted” by CrowdFlower (www.
crowdflower.com) and requiring 90% accuracy on so-
called “test questions”. Annotators are paid $4 cents per item.

datasets it is on a par with agreement among hu-
mans. As for the discriminator’s performance with
respect to the ground truth, not surprisingly we
obtain low accuracy on Switchboard, but slightly
higher accuracy than humans in the other datasets,
in particular SubTle, possibly due to the larger
amount of training data from this corpus. In what
follows, we investigate what information the dis-
criminator may be exploiting to make its predic-
tions.

4 Analysis

To inspect the discriminator’s internal representa-
tion of the dialogue turns, at testing time we run
two extra forward passes, inputting context and
target separately, and compute the cosine similar-
ity between the respective LSTM hidden states.
We find some clear patterns: The context and re-
sponse of the dialogue passages classified as co-
herent by the discriminator (true and false pos-
itives) have significantly higher cosine similarity
than the passages classified as fake (true and false
negatives). This holds across the 3 datasets (p <
.001 on a two-sample Wilcoxon rank sum test)
and indicates that the discriminator is exploiting
this information to make its predictions. We also
observe that, while there is a tendency to higher
cosine similarity in the ground-truth positive in-
stances than in the negative ones in Switchboard
(p= .05) and MovieTriples (p= .03), the effect is
highly significant in SubTle (p<.001), which is in
line with the higher performance of the discrimi-
nator on this corpus. Since accuracy is higher than
humans in this case, presumably the discriminator
is sensitive to patterns that may not be apparent to
humans. Whether this capacity is useful for de-
veloping generative models that interact with hu-
mans, however, is an open question.

We find another interesting pattern within the
attention mass distribution between context and
target: For true and false positives, higher atten-
tion is concentrated on the response (≈ 90%),

286



Figure 1: Attention visualization.

while for true and false negatives the attention is
more balanced between the two (≈ 50%). Figure 1
shows three sample dialogue passages with word-
level attention weights displayed in different color
intensities. The token <s> separates the context
from the target response. The sample at the top
is a passage from SubTle that humans judged to
be incoherent, but that was rightly classified by
the discriminator as a positive instance (the pas-
sage is real). The sample in the middle (a passage
from MovieTriples where the target is random) il-
lustrates how attention weights are more balanced
in negative instances. Finally, the sample at the
bottom shows a passage from MovieTriples rightly
classified as coherent by human annotators and by
the discriminative agent. As can be seen, atten-
tion is more prominent on the target response, with
particular focus on the pronoun ‘she’ whose an-
tecedent ‘her’ in the context also receives some at-
tention mass. In all cases the token </s> receives
high attention, suggesting that the discriminative
agent is keeping track of turn alternations.

5 Discriminating Generated Responses

We implement a baseline generative agent to test
the extent to which the discriminator’s ability
to distinguish between generated and actual re-
sponses is comparable to humans.

5.1 The Generator Agent

The generator directly models the conditional
probability p(y|x) of outputting the subsequent di-
alogue turn y1, ..., ym given some previous con-
text x1, ..., xn. The model consists of a SEQ2SEQ
model, divided into two components: an encoder
which computes a representation for the dialogue
context and a decoder which generates the subse-
quent dialogue turn one word at a time. A natural
choice for implementing both the encoder and the
decoder is to use an LSTM (see Section 2). The

decoder is also equipped with an attention system.
We train the generator to predict the next dia-

logue turn given the preceding dialogue history on
the OpenSubtitles dataset (Tiedemann, 2009). We
considered each line in the dataset as a target to be
predicted by the model and the concatenation of
the two foregoing lines as the source context. We
opt for OpenSubtitles rather than for the cleaner
datasets used for training the discriminative agent,
because the SEQ2SEQ model requires a very large
amount of data to converge, and with more than 80
million triples, OpenSubtitles is one of the largest
dialogue dataset available.

During training, we filtered out passages with
context or target longer than 25 words. We used
mini-batch stochastic gradient descent, shuffling
the batches each epoch. We use stacking LSTM
with 2 bidirectional layers, each with 2048 cells,
and 500-dimensional embeddings. All model pa-
rameters are uniformly initialized in [−0.1, 0.1];
we train using SGD, with a start learning rate of
1, and after 5 epochs we start halving the learning
rate at each epoch; the mini-batch size is set to 64
and we rescale the normalized gradients whenever
the norm exceeds 5. We also apply dropout with
probability 0.3 on the LSTMs.

5.2 Results

We test our discriminative agent on the task of
distinguishing passages with real responses ver-
sus generated responses and, as before, compare
its performance to human performance. For this
evaluation, we selected a random sample of 30
generated instances per corpus, avoiding repeated
generated responses and responses with <unk>
tokens since these would make the human judge-
ments trivial. A summary of results is shown in
Table 2. We can see that human accuracy is at
chance level, while the discriminator’s is above
chance, again suggesting that the discriminator
may pick up on patterns that are not discernible to
humans. The higher performance on SubTle may
again be explained by the larger amount of train-
ing data from this dataset. We also observe very
low inter-annotator agreement, with even negative
π for the discriminator with respect to humans in
the case of Switchboard.

6 Conclusions

In this paper, we investigated the use of an ad-
versarial setting for open domain dialogue eval-

287



data discriminator humans agreement
real generated real generated Fleiss’ π

Acc P R F1 P R F1 Acc P R F1 P R F1 hum disc
SWB .567 .538 .933 .683 .750 .200 .316 .517 .511 .755 .610 .532 .277 .365 .194 –.130
MOV .633 .618 .700 .656 .654 .567 .607 .467 .478 .733 .579 .428 .200 .273 .177 .062
SUB .700 .773 .567 .654 .658 .833 .736 .511 .508 .678 .581 .517 .344 .413 .258 .129

Table 2: Performance of discriminator and humans against ground-truth for generator experiment. Inter-
annotator agreement among humans and between the discriminator and the human majority class.

uation, providing novel results on human perfor-
mance that are informative of the difficulty of the
task and the strategies employed to tackle it. We
found that there is limited consensus among hu-
man annotators on what counts as a coherent dia-
logue passages when only 1 or 2 utterances of con-
text are provided, but that nevertheless a discrim-
inative model is able to learn patterns that lead to
above-chance performance.

References
David Ameixa, Luisa Coheur, Pedro Fialho, and Paulo

Quaresma. 2014. Luke, I am your father: Dealing
with out-of-domain requests by using movies subti-
tles. In International Conference on Intelligent Vir-
tual Agents. Springer, pages 13–21.

Rafael E Banchs. 2012. Movie-DiC: a movie dialogue
corpus for research and development. In Proceed-
ings ACL-2012: Short Papers-Volume 2. pages 203–
207.

Joseph L. Fleiss. 1971. Measuring nominal scale
agreement among many raters. Psychological Bul-
letin 76(5):378–382.

John J Godfrey, Edward C Holliman, and Jane Mc-
Daniel. 1992. Switchboard: Telephone speech cor-
pus for research and development. In Proceedings
of the IEEE International Conference on Acoustics,
Speech, and Signal Processing (ICASSP-92). vol-
ume 1, pages 517–520.

Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza,
Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron
Courville, and Yoshua Bengio. 2014. Generative ad-
versarial nets. In Advances in neural information
processing systems. pages 2672–2680.

Sepp Hochreiter and Jürgen Schmidhuber. 1997.
Long short-term memory. Neural computation
9(8):1735–1780.

Anjuli Kannan and Oriol Vinyals. 2016. Adversarial
evaluation of dialogue models. In NIPS Workshop
on Adversarial Training.

Jiwei Li and Eduard H. Hovy. 2014. A model of coher-
ence based on distributed sentence representation.
In Proceedings of EMNLP.

Jiwei Li, Will Monroe, Alan Ritter, Michel Galley,
Jianfeng Gao, and Dan Jurafsky. 2016. Deep rein-
forcement learning for dialogue generation. In Pro-
ceedings of EMNLP.

Jiwei Li, Will Monroe, Tianlin Shi, Alan Ritter, and
Dan Jurafsky. 2017. Adversarial learning for neural
dialogue generation. Preprint arXiv:1701.06547 .

Chia-Wei Liu, Ryan Lowe, Iulian Serban, Mike Nose-
worthy, Laurent Charlin, and Joelle Pineau. 2016.
How NOT To Evaluate Your Dialogue System: An
Empirical Study of Unsupervised Evaluation Met-
rics for Dialogue Response Generation. In Proceed-
ings of the 2016 Conference on EMNLP.

Ryan Lowe, Michael Noseworthy, Iulian Serban, Nico-
las Angelard-Gontier, Yoshua Bengio, and Joelle
Pineau. 2017. Towards an automatic Turing test:
Learning to evaluate dialogue responses. In Pro-
ceedings of ACL.

Iulian V Serban, Alessandro Sordoni, Yoshua Bengio,
Aaron Courville, and Joelle Pineau. 2016. Building
end-to-end dialogue systems using generative hier-
archical neural network models. In Proceedings of
the Thirtieth AAAI Conference on Artificial Intelli-
gence.

Alessandro Sordoni, Michel Galley, Michael Auli,
Chris Brockett, Yangfeng Ji, Margaret Mitchell,
Jian-Yun Nie, Jianfeng Gao, and Bill Dolan. 2015.
A neural network approach to context-sensitive gen-
eration of conversational responses. In Proceedings
of NAACL-HLT . pages 196–205.

Sainbayar Sukhbaatar, Jason Weston, Rob Fergus, et al.
2015. End-to-end memory networks. In Advances
in neural information processing systems.

Jörg Tiedemann. 2009. News from opus-a collection
of multilingual parallel corpora with tools and inter-
faces. In Recent advances in natural language pro-
cessing. volume 5, pages 237–248.

Alan M Turing. 1950. Computing machinery and in-
telligence. Mind 59(236):433–460.

Oriol Vinyals and Quoc V. Le. 2015. A neural conver-
sational model. In ICML Deep Learning Workshop.

Zichao Yang, Diyi Yang, Chris Dyer, Xiaodong He,
Alex Smola, and Eduard Hovy. 2016. Hierarchical
attention networks for document classification. In
Proceedings of NAACL-HLT . pages 1480–1489.

288


