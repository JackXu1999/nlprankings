



















































Sentence Modeling with Deep Neural Architecture using Lexicon and Character Attention Mechanism for Sentiment Classification


Proceedings of the The 8th International Joint Conference on Natural Language Processing, pages 536–544,
Taipei, Taiwan, November 27 – December 1, 2017 c©2017 AFNLP

Sentence Modeling with Deep Neural Architecture using Lexicon and
Character Attention Mechanism for Sentiment Classification

Huy Thanh Nguyen and Minh Le Nguyen
Japan Advanced Institute of Science and Technology

Ishikawa, Japan
{huy.nguyen, nguyenml}@jaist.ac.jp

Abstract

Tweet-level sentiment classification in
Twitter social networking has many chal-
lenges: exploiting syntax, semantic, sen-
timent and context in tweets. To ad-
dress these problems, we propose a
novel approach to sentiment analysis that
uses lexicon features for building lex-
icon embeddings (LexW2Vs) and gen-
erates character attention vectors (Char-
AVs) by using a Deep Convolutional Neu-
ral Network (DeepCNN). Our approach
integrates LexW2Vs and CharAVs with
continuous word embeddings (Continu-
ousW2Vs) and dependency-based word
embeddings (DependencyW2Vs) simulta-
neously in order to increase information
for each word into a Bidirectional Contex-
tual Gated Recurrent Neural Network (Bi-
CGRNN). We evaluate our model on two
Twitter sentiment classification datasets.
Experimental results show that our model
can improve the classification accuracy of
sentence-level sentiment analysis in Twit-
ter social networking.

1 Introduction

Tweet-level sentiment classification is a funda-
mental task of sentiment analysis in Twitter so-
cial networking and is essential to understand user
generated contents in social networking. Twit-
ter sentiment classification have intensively re-
searched in recent years (Go et al., 2009) (Nakov
et al., 2016). There are many works related to deep
learning methods involved learning word repre-
sentation (Socher et al., 2011). Word represen-
tation is central to deep learning and essential
feature extractor that encode different features of
words in their dimensions. The combination of

word representation and deep learning achieved
impressive results because word embeddings en-
able efficient computation of word similarities
through low-dimensional matrix operations (Kim,
2014). In addition, deep learning models achieved
remarkable performance. Some researchers use
Convolution Neural Network (CNN) for sentiment
classification. CNN utilizes convolution filters ap-
plied to local features. CNN models has been
shown to be effective for NLP. For example, the
model of (Dos Santos and Gatti, 2014) used CNN
to form a sentence-level representation for sen-
timent classification. In addition, Bidirectional
Gated Recurrent Neural Network (Bi-GRNN) is
another deep learning model that has achieved an
excellent result for sentiment analysis and other
traditional tasks (Chung et al., 2014).

Inspired by the models above, the goal of this
research is to build a model for exploiting syn-
tax, semantic, sentiment and context of tweets
by constructing four kinds of embeddings: Char-
AVs, LexW2Vs, ContinuousW2Vs and Depen-
dencyW2Vs. On the other hand, we modify Bi-
GRNN of (Chung et al., 2014) into Bi-CGRNN
to take word embeddings in order to produce a
sentence-wide representation from sentence com-
positions. Our paper makes the following contri-
butions:

• We construct a tweet processor which stan-
dardizes tweets by using pre-processing steps
and a semantic rule-based approach. We con-
struct four kinds of embeddings: CharAvs,
LexW2Vs, ContinuousW2Vs and Dependen-
cyW2Vs. A DeepCNN is used for training
CharAVs by producing fixed-size feature vec-
tors and attending on the best feature vec-
tors. CharAVs can capture the morphology
and shape of a word. The morphological and
shape information illustrate how words are

536



formed, and their relationship to other words.

• We create an integration of CharAVs and
LexW2Vs with ContinuousW2Vs and De-
pendencyW2Vs. Such embeddings are ad-
vanced continuous word embeddings and ad-
vanced dependency-based word embeddings.

• We modify a standard Bi-GRNN to be a Bi-
CGRNN by incorporating contextual features
(e.g., Syntactic contexts) in order to take both
advanced word embeddings. The output of
Bi-CGRNN is sentence compositions that are
formed into a sentence-wide representation.
The purpose of Bi-CGRNN is to connect the
information of words in a sequence and main-
tain the order of words for a sentence-level
representation.

The organization of the paper is as follows: Sec-
tion 2 describes the model architecture which in-
troduces the structure of the model. We explain
the basic idea of the model and the way of con-
structing the model. Section 3 shows results and
analysis and Section 4 summarizes this paper.

2 Model architecture

2.1 Basic Idea
Our proposed model consists of a tweet processor
and a deep learning module that are treated as two
distinct components. The tweet processor stan-
dardizes tweets, applies semantic rules and then
generates embeddings. The deep learning mod-
ule is a combination of Bi-CGRNN and Deep-
CNN. To formulate our challenges in increasing
the classification accuracy, we illustrate the basic
idea of our model in Figure 1 as follows: Tweets
are firstly considered by the tweet processor based
on pre-processing steps of (Go et al., 2009) and the
semantic rule-based approach from (Appel et al.,
2016). Then, we construct four kinds of em-
beddings in the representation-level step: Con-
tinuousW2Vs, CharAVs, LexW2Vs and Depen-
dencyW2Vs, where CharAVs are generated from
a DeepCNN. The DeepCNN is constructed from
two wide convolutions which can learn to recog-
nize specific n-grams at every position in a word
and allow features to be extracted independently
of these positions in the word. These features
maintain the order and relative positions of char-
acters and are formed at a higher abstract level. In
those embeddings, ContinuousW2Vs take the syn-
tax and semantic of words (Mikolov et al., 2013)

while the LexW2Vs can capture the sentiment of
words (Shin et al., 2016). DependencyW2Vs de-
rive the syntactic relations of words and exhibit
more functional similarity than the original skip-
gram embeddings led to form global syntactic con-
texts of words (Levy and Goldberg, 2014). Twitter
sentiment label belongs to global sentence level
while traditional word embeddings capture local
contexts only. Therefore, DependencyW2Vs are
useful in capturing global context of tweets. On
the other hand, we create two advanced embed-
dings by integrating LexW2Vs and CharAVs with
ContinuousW2Vs and DependencyW2Vs for Bi-
CGRNN. A Bi-CGRNN is enhanced from a stan-
dard Bi-GRNN of (Chung et al., 2014) by incorpo-
rating contextual features (e.g., dependency-based
contexts) into the model. The Bi-CGRNN pro-
duces a sentence-level representation from sen-
tence compositions in order to maintain the order
of word and capture syntax, semantic, sentiment
and context of a sentence based on these embed-
dings.

2.2 Data Preparation

• Stanford - Twitter Sentiment Corpus (STS
Corpus): STS Corpus contains 1,600K train-
ing tweets collected by a crawler from (Go
et al., 2009). (Go et al., 2009) constructed a
test set manually with 177 negative and 182
positive tweets. The Stanford test set is small.
However, it has been widely used in different
evaluation tasks (Go et al., 2009) (Dos Santos
and Gatti, 2014).

• Health Care Reform (HCR): This dataset
was constructed by crawling tweets contain-
ing the hashtag #hcr (Speriosu et al., 2011).
The task of this paper is to predict posi-
tive/negative tweets.

2.3 Tweet Processor

We first take the unique properties of Twitter to re-
duce the feature space such as Username, Usage of
links, None, URLs and Repeated Letters. We then
process retweets, stop words, links, URLs, men-
tions, punctuation and accentuation. For emoti-
cons in the dataset, we consider them as words in
order that deep learning classifiers can capture in-
formation from emoticons. Because the test set
contains emoticons, they do not influence classi-
fiers if emoticons do not contain in its training
data. Therefore, the emoticons would be useful

537



Figure 1: The overview of a deep learning system.

when classifying test data by using deep learn-
ing model. However, our preprocessing steps are
different from (Go et al., 2009), they remove the
emoticons out from their training datasets because
they revealed that the training process makes the
usage of emoticons as noisy labels and if they
consider the emoticons, there is a negative impact
on classification accuracy. In addition, traditional
classifiers can not focus on non-emoticons (e.g.,
unigrams and bi-grams) to predict exactly the sen-
timent of tweets. After the pre-processing steps,
we apply the semantic rules based on the idea of
(Appel et al., 2016) and use a tweet processor from
our previous work (Nguyen and Nguyen, 2017) to
remove unnecessary sub-sentences from tweets in
order that the deep learning model learns essential
features from tweets. The semantic rule-based ap-
proach can handles negation and shows advances
led to effectively affect the output of classifiers.

2.4 Representation Level
In this section, we describe how the kinds of
embeddings are constructed by the representa-
tion module. To construct embedding inputs for
our model, we use a fixed-sized word vocabu-
lary V word and a fixed-sized character vocabulary
V char. Given a word wi is composed from char-
acters {c1, c2, ..., cM}, the character-level embed-
dings are encoded by column vectors ui in the em-
bedding matrix W char ∈ Rdchar×|V char|, where
V char is the size of the character vocabulary. For

continuous word-level embedding rword, we use a
pre-trained word-level. We build every word wi
into two advanced word embeddings:

• Advanced continuous word embeddings vi =
[ri; ei; li] is constructed by three sub-vectors:
the pre-trained word-level embedding ri ∈
Rd

word
, the character attention vector ei ∈

Rl of wi where l is the length of the filter
of wide convolutions, the lexicon embedding
li ∈Rdscore where dscore is list of sentiment
scores for that word in lexicon datasets.

• Advanced dependency-based word embed-
dings di = [dei, ei; li] is also built by three
sub-vectors: the dependency-based word em-
bedding dei ∈ Rdword , the character atten-
tion vector ei and the lexicon embedding li.
The advanced dependency-based word em-
beddings contain syntactic contexts and is
increased information from LexW2Vs and
CharAVs.

This deals with three main problems: (i) Sentences
have any different size; (ii) Important information
of characters that can appear at any position in
a word are extracted; (iii) The syntax, semantic,
sentiment, context and the morphology of words
in a sentence are captured by concatenating two
advanced embeddings via Bi-CGRNN in order to
produce sentence representation.

538



We have N fixed-size CharAVs corresponding
to word-level embeddings in a sentence. In sub-
section 2.5, 2.6 and 2.7, we illustrate the methods
of constructing LexW2Vs, CharAVs using Deep-
CNN and DependencyW2Vs.

2.5 Lexicon Embeddings (LexW2Vs)
The LexW2Vs are constructed by taking scores
from various lexicon datasets. In lexicon datasets,
each word contains key-value pairs in which the
key is a word and the value is a list of sentiment
scores for that word. These scores range from -1 to
1, where -1 is most negative and 1 is most positive,
respectively.

For each word wi ∈ V word, where V word is a
fixed-sized word vocabulary, a lexicon embedding
is constructed by concatenating all of the scores
among lexicon datasets with respect to wi. If wi
does not exist in a certain dataset, 0 value is sub-
stituted. The lexicon embedding is a form of a
vector li ∈ Rdscore , where dscore is the total num-
ber of scores across all lexicon datasets. We use
seven lexicon datasets for building LexW2Vs:

• Bing Liu Opinion Lexicon (Hu and Liu,
2004).

• NRC Hashtag Sentiment Lexicon (Moham-
mad et al., 2013).

• Sentiment140 Lexicon (Mohammad et al.,
2013).

• NRC Sentiment140 Lexicon (Kiritchenko
et al., 2014).

• MaxDiff Twitter Sentiment Lexicon (Kir-
itchenko et al., 2014).

• National Research Council Canada (NRC)
Hashtag Affirmative and Negated Context
Sentiment Lexicon (Kiritchenko et al., 2014).

• Large-Scale Twitter-Specific Sentiment Lex-
icon (Tang et al., 2014).

The purpose of building LexW2Vs is to take the
different kinds of words for capturing the differ-
ent sentiments of words. Table 1 illustrates the
type of words for each dataset. We share idea
with (Shin et al., 2016). However, our LexW2Vs
are distinguished their approaches in the aspect:
We cover the colloquial expressions and colloquial
emoticons in tweets by using Large-scale Twitter-
Specific Sentiment Lexicon.

Lexicon dataset The type of words
Bing Liu Opinion Lexicon Sentiment adjective

words
NRC Hashtag Sentiment Lexi-
con

Hashtag emotion words
& Hashtag topic words

Sentiment140 Lexicon Emoticons & Senti-
ment words

NRC Sentiment140 Lexicon Affirmative context
words & Sentiment140
Negated Context words

MaxDiff Twitter Sentiment
Lexicon

Twitter sentiment
words

Hashtag Affirmative and
Negated Context Sentiment
Lexicon

Hashtag affirmative
words & Negated
contextual words

Large-Scale Twitter-Specific
Sentiment Lexicon

Colloquial words &
Emoticons

Table 1: The types of words in lexicon dataset.

We call such lexicon-based features as lexicon
embeddings because embeddings are a feature in-
put of deep learning model and describe the prop-
erties of a word or a phrase. Each word in each
lexicon datasets actually has many values that can
be built by training a neural network. The deep
learning model uses this input for calculating a
computational graph (weight matrix) that describe
relatedness among words (n-gram order).

2.6 Character Attention Vectors (CharAVs)

Figure 2 describes the way of forming a charac-
ter attention vector. We use a DeepCNN with two
wide convolutions. The first convolution produces
a fixed-size character feature vector named m-
gram features by extracting local features around
each character window of the given word and us-
ing a max pooling over vertical character win-
dows. The second convolution retrieves the fixed-
size character feature and transforms the represen-
tation to yield a character attention vector by per-
forming max pooling on each row of matrix in-
stead of each column. The purpose of this method
is to attend on the highest n-gram feature in or-
der to transform these m-gram features at previ-
ous level into a representation at a more focused
abstract level and produces an attention over only
the best feature vector. Character attention vec-
tors has two advantages: One is that this model
could adaptively assign an importance score to
each piece of word embedding according to its se-
mantic relatedness with characters of each word.
Another advantage is that this attention model is
differentiable, so that it could be easily trained
together with other components in an end-to-end
fashion. In the next sub-section, we introduce the

539



structure of CNN with wide convolution.

Figure 2: DeepCNN for the sequence of character
embeddings of a word. For example with 1 region
size is 2 and 4 feature maps in the first convolu-
tion and 1 region size is 3 with 3 feature maps in
the second convolution. The CharAVs is then cre-
ated by performing max pooling on each row of
the attention matrix.

Convolutional Neural Network: The convolu-
tion has a filter vector m and take the dot product
of filter m with each m-grams in the sequence of
characters si ∈ R of a word in order to obtain a
sequence c:

cj = mT sj−m+1:j (1)

Based on Equation 1, we have two types of convo-
lutions that depend on the range of the index j.
The narrow type requires that s ≥ m and pro-
duce a sequence c ∈ Rs−m+1. The wide type
does not require on s orm and produce a sequence
c ∈Rs+m−1. Out-of-range input values si where
i < 1 or i > s are taken to be zero.
Wide Convolution: Given a word wi composed
of M characters {c1, c2, ..., cM}, we take a char-
acter embedding ui ∈ Rdchar for each charac-
ter ci and construct a character matrix W char ∈
Rd

char×|V char|. The values of the embeddings ui
are parameters that are optimized during training.
The trained weights in the filter m correspond to
a feature detector which learns to recognize a spe-
cific class of n-grams. The use of a wide convolu-
tion has some advantages more than a narrow con-
volution because a wide convolution ensures that
all weights of filter reach the whole characters of

a word at the margins. The resulting matrix has
dimension d× (s+m− 1).

2.7 Dependency-based Word Vectors
(DependencyW2Vs)

To construct context embeddings, we use the idea
of (Levy and Goldberg, 2014) to derive syntactic
contexts based on the syntactic relations of a word.
Most previous works on neural word embeddings
take the contexts of a word by computing linear-
context words that precede and follow the target
word. However, these contexts can be exploited
similar by generalizing the SKIP-GRAM model.
The model for learning Dependency-based Word
Vectors is improved from SKIP-GRAM model in
which the linear bag of words contexts are re-
placed with arbitrary word contexts from depen-
dency tree. Syntactic contexts are derived from
produced dependency parse-trees. Specifically,
the bag-of-words in the SKIP-GRAM model yield
broad topical similarities, while the dependency-
based contexts yield more functional similarities
of a cohyponym nature. In the SKIP-GRAM
model, the contexts of a word are the words sur-
rounding it in the text. However, there is a lim-
itation of SKIP-GRAM word embeddings: Con-
texts need not correspond to all of the words and
the number of context-types maybe larger than the
number of word-types. Therefore, dependency-
based contexts capture more information than bag-
of-words contexts. In Figure 3, the contexts are
extracted for each word in the sentence and the
contexts of a word are derived from syntactic rela-
tions of a word in the sentence. For parsing syntac-
tic dependencies, we use a parser from (Goldberg
and Nivre, 2013) for Stanford dependencies and
the corpus are tagged with parts-of-speech using
Stanford parser 1.

After parsing each sentence, we consider word
context as Figure 3: For a target word w with mod-
ifiers m1,m2, ...,mn and a head h, we form the
contexts as (m1, lbl1), ..., (mn, lbln), (h, lbl−1h ),
where lbl is the type of the dependency relation
between the head and the modifier, lbl−1 is used
to mark the inverse-relation. The advantages of
syntactic dependencies are inclusive and more fo-
cused than bag-of-words. In addition, they can
capture relations that out-of-reach with small win-
dows and filter out contexts that are not directly re-
lated to the target word. For example, Australian

1https://nlp.stanford.edu/software/lex-parser.shtml

540



is not used as the context for discovers. There-
fore, we have more focused embeddings that cap-
ture more functional and less topical similarity.

Figure 3: Dependency-based context extraction
example (Levy and Goldberg, 2014)

2.8 Contextual Gated Recurrent Neural
Network (CGRNN)

Gated Recurrent Neural Network: The Bi-
GRNN is a version of (Chung et al., 2014) in
which a Gated Recurrent Unit (GRU) has two
gates, a reset gate rt and a update gate zt. In-
tuitively, the reset gate determines how to com-
bine the new input with the previous memory, and
the update gate defines how much of the previ-
ous memory to keep around. We use GRUs for
our model because GRUs are quite new and their
tradeoffs have not been fully explored yet. On the
other hand, GRUs have fewer parameters (U and
W are smaller) and thus may train a bit faster or
need less data to generalize. The equation 2 illus-
trates the construction of a GRU cell:

rt = σ(Wxrxt +Whrht−1 + br)
zt = σ(Wxzxt +Whzht−1 + bz)

ĥt = g(xtWxh + (rt � ht−1)Whh + bh)
ht = (1− zt)� ht−1 + zt � ĥt.

(2)

Contextual Gated Recurrent Neural Network:
Based on the idea of GRNN model, we build a
power of syntactic contexts into a standard Bi-
GRNN model which adapt GRNN cell to take both
words and syntactic contexts by modifying the
equations representing operations of the GRNN
cell. A GRNN cell is added dependency-based
word vector T to reset gate, update gate and hid-
den state. In the Equation 3, the term in bold is the
modification made to the original GRNN equation.

Based on these equations, adding dependency-
based word vector T is corresponding to consider-
ing a composite input [xi, T ] to the GRNN cell that
concatenates the advanced continuous word em-
beddings and advanced dependency-based word
embeddings.

rt = σ(Wxrxt +Whrht−1 + WTiT + br)
zt = σ(Wxzxt +Whzht−1 + WTiT + bz)

ĥt = g(xtWxh + (rt � ht−1)Whh + WTiT + bh)
ht = (1− zt)� ht−1 + zt � ĥt.

(3)

This approach of concatenating syntactic con-
texts and word embeddings works better in prac-
tice and deal with the context challenge in senti-
ment analysis.

3 Results and Analysis

3.1 Experimental setups
For the Stanford Twitter Sentiment Corpus (STS
Corpus), we use the number of samples as
(Dos Santos and Gatti, 2014). The training data
is selected 80K tweets for a training data and 16K
tweets for the development set randomly from the
training data of (Go et al., 2009). We conduct a
binary prediction for STS Corpus.

In Health Care Reform Corpus (HCR Corpus),
we also select 10% randomly for the development
set in a training set and construct as (Da Silva
et al., 2014) for comparison. We describe the sum-
mary of datasets in Table 2.

Data Set N c lw lc |Vw| |Vc|

STS
Train 80K

2
33 110

67083 134Dev 16K 28 48
Test 359 21 16

HCR
Train 621

2
25 70

3100 60Dev 636 26 16
Test 665 20 16

Table 2: Summary statistics for the datasets after
using semantic rules. c: the number of classes.
N : The number of tweets. lw: Maximum sen-
tence length. lc: Maximum character length. |Vw|:
Word alphabet size. |Vc|: Character alphabet size.

Hyperparameters: For all datasets, the filter win-
dow size (h) is 7 with 6 feature maps each for the
first wide convolution layer, the second wide con-
volution layer has a filter window size of 1 with 14
feature maps each. Dropout rate (p) is 0.5, l2 con-
straint, learning rate is 0.1 and momentum of 0.9.

541



Mini-batch size for STS Corpus is 100 and HCR
corpus is 4. Training is done through stochastic
gradient descent over shuffled mini-batches with
Adadelta update rule (Zeiler, 2012).
Pre-trained Word Vectors: We use the pub-
licly available Word2Vec 2 trained from 100 billion
words from Google and TwitterGlove 3 of Stanford
is performed on aggregated global word-word co-
occurrence statistics from a corpus. Word2Vec has
dimensionality of 300 and Twitter Glove have di-
mensionality of 200. Words that do not present in
the set of pre-train words are initialized randomly.

3.2 Experimental results

Table 3 shows the results of our model for senti-
ment analysis against other models. The differ-
ent kinds of models are constructed to evaluate
the impacts of embeddings on classification accu-
racy. We build the Bi-CGRNN enhanced Char-
AVs and LexW2Vs. In addition, we evaluate sepa-
rately the effectiveness of CharAVs and LexW2Vs
on Twitter datasets by incorporating with standard
Bi-GRNN.

We compare our model performance with the
approaches of (Go et al., 2009) and (Dos San-
tos and Gatti, 2014). The model of (Go et al.,
2009) displays the good results in the previous
time and the model of (Dos Santos and Gatti,
2014) reported the state-of-the-art so far by using
a charSCNN. Our model shows the result of 88.57
is the best prediction accuracy for STS Corpus.

For HCR Corpus, we compare the performance
with the results of (Da Silva et al., 2014) that used
an ensemble of multiple base classifiers (ENS)
such as Naive Bayes (NB), Random Forest (RF),
SVM and Logistic Regression (LR). The ENS
model is combined with bag-of-words (BoW), fea-
ture hashing (FH) and lexicons (lex). Our model
outperforms the model of (Da Silva et al., 2014)
with result of 80 so far.

3.3 Analysis

The model with CharAVs and LexW2Vs built
on top of GoogleW2V vectors and Dependen-
cyW2Vs is effective in order to increase classi-
fication accuracy. These experiments show that
CharAVs and LexW2Vs achieve good perfor-
mances and contribute in enhancing information
for words. However, the experiments indicate that

2code.google.com/p/word2vec/
3https://nlp.stanford.edu/projects/glove/

Model STS HCR
CharSCNN/Pre-trained (Dos Santos
and Gatti, 2014)

86.4 -

CharSCNN/Random (Dos Santos and
Gatti, 2014)

81.9 -

SCNN/Pre-trained (Dos Santos and
Gatti, 2014)

85.2 -

SCNN/Random (Dos Santos and Gatti,
2014)

82.2 -

MaxEnt (Go et al., 2009) 83.0 -
NB (Go et al., 2009) 82.7 -
SVM (Go et al., 2009) 82.2 -
SVM-BoW - 73.99
SVM-BoW + lex - 75.94
RF-BoW - 70.83
RF-BoW + lex - 72.93
LR-BoW - 73.83
LR-BoW + lex - 74.73
MNB-BoW - 72.48
MNB-BoW + lex - 75.33
ENS (RF + MNB + LR) - BoW - 75.19
ENS (SVM + RF + MNB + LR) - BoW
+ lex

- 76.99

Bi-CGRNN + CharAVs + LexW2Vs +
GoogleW2Vs

88.5 78.47

Bi-CGRNN + CharAVs + LexW2Vs +
GloveW2Vs

86.9 80.0

Bi-CGRNN + GoogleW2Vs 85.7 77.74
Bi-GRNN + CharAVs + GoogleW2Vs 86.0 79.09
Bi-GRNN + LexW2Vs + GoogleW2Vs 88.0 78.79

Table 3: Accuracy of different models for binary
classification.

CharAVs are more effective in small dataset than
LexW2Vs. In addition, the Bi-CGRNN enhanced
CharAVs and LexW2Vs have a great impact on
Twitter datasets. Table 4 displays the effective-
ness of our model in predicting the tweets. In
the table 4, the Bi-GRNN using LexW2Vs cap-
tures the positive words (green words) and nega-
tive words (red words) for computing scores and
predicts wrong labels because of the contexts of
tweets while the Bi-CGRNN enhanced CharAVs
and LexW2Vs recognizes contexts for true predic-
tion. For example, the model using LexW2Vs pre-
dicts the last tweet to be positive because ’strong’
word has sentiment stronger than ’no’ word, how-
ever, the context of this tweet is negative. The
pre-train word vectors are good, universal feature
extractors. The syntactic contexts support in deal-
ing context problems in tweets and the lexicons
support word embeddings in dealing the sentiment
of tweets. The difference between our model and
other approaches is the ability of our model to cap-
ture enough features and combine these features at
high level. In addition, the usage of DeepCNN for
characters can learns a structure of words in higher
abstract level. LexW2Vs and syntactic contexts

542



Model Input from HCR Corpus Gold Label Prediction
Bi-CGRNN + CharAVs +
LexW2Vs

@seanbaran74 well that’s what’s next. after #hcr
they’ll save the environment, give us CFLs and take
away our TVs.

Negative True

Bi-GRNN + LexW2Vs False
Bi-CGRNN + CharAVs +
LexW2Vs

All of us fighting for #HCR ask ourselves who
#imherefor. Who are you fighting for? http://bit.ly/9-st Positive

True

Bi-GRNN + LexW2Vs False
Bi-CGRNN + CharAVs +
LexW2Vs

Stephen Lynch strong ’no’ on health bill despite talk
with President obama http://bit.ly/cQIujP #hcr #tcot
#tlot

Negative True

Bi-GRNN + LexW2Vs False

Table 4: The label prediction between the Bi-GRNN model using LexW2Vs and the Bi-CGRNN model
using CharAVs and LexW2Vs (The red words are negative and the green words are positive).

contribute in supporting information for word em-
beddings. This helps the model not only learns
to recognize single n-grams of a word, negation,
but also patterns in n-grams led to form a structure
significance of a sentence.

4 Conclusions

In the present work, we build a model that solves
four challenges in Twitter: syntax, semantic, sen-
timent and context. Our results show the well-
establish evidence that CharAVs, LexW2Vs and
DependencyW2Vs are important ingredients for
ContinuousW2Vs in increasing classification ac-
curacy for sentiment analysis. Our source and pro-
cessed data are available at Github4.

Acknowledgments

This work was supported by JSPS KAKENHI
Grant number JP15K16048 and CREST, JST.

References
Orestes Appel, Francisco Chiclana, Jenny Carter, and

Hamido Fujita. 2016. A hybrid approach to the
sentiment analysis problem at the sentence level.
Knowledge-Based Systems, 108:110–124.

Junyoung Chung, Çaglar Gülçehre, KyungHyun Cho,
and Yoshua Bengio. 2014. Empirical evaluation of
gated recurrent neural networks on sequence model-
ing. CoRR, abs/1412.3555.

Nadia F. F. Da Silva, Eduardo R. Hruschka, and Es-
tevam R. Hruschka. 2014. Tweet sentiment analy-
sis with classifier ensembles. Decision Support Sys-
tems, 66:170–179.

C’icero N. Dos Santos and Maira Gatti. 2014. Deep
convolutional neural networks for sentiment analy-
sis of short texts. In COLING, pages 69–78.

4https://github.com/titanbt/contextualGRU-attention-
lexicon

Alec Go, Richa Bhayani, and Lei Huang. 2009. Twit-
ter sentiment classification using distant supervision.
CS224N Project Report, Stanford, 1(12).

Yoav Goldberg and Joakim Nivre. 2013. Training de-
terministic parsers with non-deterministic oracles.
Transactions of the association for Computational
Linguistics, 1:403–414.

Minqing Hu and Bing Liu. 2004. Mining and summa-
rizing customer reviews. In Proceedings of the tenth
ACM SIGKDD international conference on Knowl-
edge discovery and data mining, pages 168–177.
ACM.

Yoon Kim. 2014. Convolutional neural networks for
sentence classification. CoRR, abs/1408.5882.

Svetlana Kiritchenko, Xiaodan Zhu, and Saif M Mo-
hammad. 2014. Sentiment analysis of short in-
formal texts. Journal of Artificial Intelligence Re-
search, 50:723–762.

Omer Levy and Yoav Goldberg. 2014. Dependency-
based word embeddings. In Proceedings of the
52nd Annual Meeting of the Association for Com-
putational Linguistics, ACL 2014, June 22-27, 2014,
Baltimore, MD, USA, Volume 2: Short Papers, pages
302–308.

Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S. Cor-
rado, and Jeff Dean. 2013. Distributed representa-
tions of words and phrases and their compositional-
ity. In Advances in Neural Information Processing
Systems 26, pages 3111–3119. Curran Associates,
Inc.

Saif M. Mohammad, Svetlana Kiritchenko, and Xiao-
dan Zhu. 2013. Nrc-canada: Building the state-
of-the-art in sentiment analysis of tweets. CoRR,
abs/1308.6242.

Preslav Nakov, Alan Ritter, Sara Rosenthal, Fabrizio
Sebastiani, and Veselin Stoyanov. 2016. SemEval-
2016 task 4: Sentiment analysis in twitter. Proceed-
ings of SemEval, pages 1–18.

Huy Nguyen and Minh-Le Nguyen. 2017. A deep neu-
ral architecture for sentence-level sentiment classifi-
cation in twitter social networking. PACLING 2017.

543



Bonggun Shin, Timothy Lee, and Jinho D. Choi. 2016.
Lexicon integrated CNN models with attention for
sentiment analysis. CoRR, abs/1610.06272.

Richard Socher, Jeffrey Pennington, Eric H. Huang,
Andrew Y. Ng, and Christopher D. Manning. 2011.
Semi-supervised recursive autoencoders for predict-
ing sentiment distributions. In Proceedings of the
conference on empirical methods in natural lan-
guage processing, pages 151–161. Association for
Computational Linguistics.

Michael Speriosu, Nikita Sudan, Sid Upadhyay, and
Jason Baldridge. 2011. Twitter polarity classifica-
tion with label propagation over lexical links and the
follower graph. In Proceedings of the First work-
shop on Unsupervised Learning in NLP, pages 53–
63. Association for Computational Linguistics.

Duyu Tang, Furu Wei, Bing Qin, Ming Zhou, and Ting
Liu. 2014. Building large-scale twitter-specific sen-
timent lexicon : A representation learning approach.
In Proceedings of COLING 2014, the 25th Inter-
national Conference on Computational Linguistics:
Technical Papers, pages 172–182, Dublin, Ireland.

Matthew D. Zeiler. 2012. ADADELTA: an adap-
tive learning rate method. arXiv preprint
arXiv:1212.5701.

544


