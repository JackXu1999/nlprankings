



















































Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics


Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics, pages 1260–1272
Vancouver, Canada, July 30 - August 4, 2017. c©2017 Association for Computational Linguistics

https://doi.org/10.18653/v1/P17-1116

Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics, pages 1260–1272
Vancouver, Canada, July 30 - August 4, 2017. c©2017 Association for Computational Linguistics

https://doi.org/10.18653/v1/P17-1116

Unifying Text, Metadata, and User Network Representations
with a Neural Network for Geolocation Prediction

Yasuhide Miura†,‡
yasuhide.miura@fujixerox.co.jp

Motoki Taniguchi†
motoki.taniguchi@fujixerox.co.jp

Tomoki Taniguchi†
taniguchi.tomoki@fujixerox.co.jp

Tomoko Ohkuma†
ohkuma.tomoko@fujixerox.co.jp

†Fuji Xerox Co., Ltd.
‡Tokyo Institute of Technology

Abstract

We propose a novel geolocation prediction
model using a complex neural network.
Our model unifies text, metadata, and user
network representations with an attention
mechanism to overcome previous ensem-
ble approaches. In an evaluation using two
open datasets, the proposed model exhib-
ited a maximum 3.8% increase in accuracy
and a maximum of 6.6% increase in ac-
curacy@161 against previous models. We
further analyzed several intermediate lay-
ers of our model, which revealed that their
states capture some statistical characteris-
tics of the datasets.

1 Introduction
Social media sites have become a popular source
of information to analyze current opinions of nu-
merous people. Many researchers have worked
to realize various automated analytical methods
for social media because manual analysis of such
vast amounts of data is difficult. Geolocation
prediction is one such analytical method that has
been studied widely to predict a user location
or a document location. Location information
is crucially important information for analyses
such as disaster analysis (Sakaki et al., 2010), dis-
ease analysis (Culotta, 2010), and political anal-
ysis (Tumasjan et al., 2010). Such information is
also useful for analyses such as sentiment analysis
(Martı́nez-Cámara et al., 2014) and user attribute
analysis (Rao et al., 2010) to undertake detailed
region-specific analyses.

Geolocation prediction has been per-
formed for Wikipedia (Overell, 2009), Flickr
(Serdyukov et al., 2009; Crandall et al., 2009),
Facebook (Backstrom et al., 2010), and Twitter
(Cheng et al., 2010; Eisenstein et al., 2010).

Among these sources, Twitter is often preferred
because of its characteristics, which are suited
for geolocation prediction. First, some tweets
include geotags, which are useful as ground truth
locations. Secondly, tweets include metadata
such as timezones and self-declared locations that
can facilitate geolocation prediction. Thirdly, a
user network is obtainable by consideration of the
interaction between two users as a network link.

Herein, we propose a neural network model
to tackle geolocation prediction in Twitter. Past
studies have combined text, metadata, and user
network information with ensemble approaches
(Han et al., 2013, 2014; Rahimi et al., 2015a;
Jayasinghe et al., 2016) to achieve state-of-the-art
performance. Our model combines text, metadata,
and user network information using a complex
neural network. Neural networks have recently
shown effectiveness to capture complex represen-
tations combining simpler representations from
large-scale datasets (Goodfellow et al., 2016). We
intend to obtain unified text, metadata, and user
network representations with an attention mecha-
nism (Bahdanau et al., 2014) that is superior to the
earlier ensemble approaches. The contributions of
this paper are the following:

1. We propose a neural network model that
learns unified text, metadata, and user net-
work representations with an attention mech-
anism.

2. We show that the proposed model outper-
forms the previous ensemble approaches in
two open datasets.

3. We analyze some components of the pro-
posed model to gain insight into the unifica-
tion processes of the model.

Our model specifically emphasizes geolocation
prediction in Twitter to use benefits derived from
the characteristics described above. However, our

1260

https://doi.org/10.18653/v1/P17-1116
https://doi.org/10.18653/v1/P17-1116


model can be readily extended to other social me-
dia analyses such as user attribute analysis and po-
litical analysis, which can benefit from metadata
and user network information.

In subsequent sections of this paper, we explain
the related works in four perspectives in Section 2.
The proposed neural network model is described
in Section 3 along with two open datasets that we
used for evaluations in Section 4. Details of an
evaluation are reported in Section 5 with discus-
sions in Section 6. Finally, Section 7 concludes
the paper with some future directions.

2 Related Works
2.1 Text-based Approach
Probability distributions of words over locations
have been used to estimate the geolocations
of users. Maximum likelihood estimation ap-
proaches (Cheng et al., 2010, 2013) and language
modeling approaches minimizing KL-divergence
(Wing and Baldridge, 2011; Kinsella et al., 2011;
Roller et al., 2012) have succeeded in predicting
user locations using word distributions. Topic
modeling approaches to extract latent topics
with geographical regions (Eisenstein et al., 2010,
2011; Hong et al., 2012; Ahmed et al., 2013) have
also been explored considering word distributions.

Supervised machine learning methods with
word features are also popular in text-based
geolocation prediction. Multinomial Naive Bayes
(Han et al., 2012, 2014; Wing and Baldridge,
2011), logistic regression (Wing and Baldridge,
2014; Han et al., 2014), hierarchical logistic
regression (Wing and Baldridge, 2014), and a
multilayer neural network with stacked denois-
ing autoencoder (Liu and Inkpen, 2015) have
realized geolocation prediction from text. A
semi-supervised machine learning approach by
Cha et al. (2015) has also been produced using a
sparse-coding and dictionary learning.

2.2 User-network-based Approach
Social media often include interactions of several
kinds among users. These interactions can be re-
garded as links that form a network among users.
Several studies have used such user network in-
formation to predict geolocation. Backstrom et al.
(2010) introduced a probabilistic model to pre-
dict the location of a user using friendship in-
formation in Facebook. Friend and follower in-
formation in Twitter were used to predict user
locations with a most frequent friend algorithm

(Davis Jr. et al., 2011), a unified descriptive model
(Li et al., 2012b), location-based generative mod-
els (Li et al., 2012a), dynamic Bayesian networks
(Sadilek et al., 2012), a support vector machine
(Rout et al., 2013), and maximum likelihood es-
timation (McGee et al., 2013). Mention informa-
tion in Twitter is also used with label propaga-
tion models (Jurgens, 2013; Compton et al., 2014)
and an energy and social local coefficient model
(Kong et al., 2014). Jurgens et al. (2015) com-
pared nine user-network-based approaches target-
ing Twitter, controlling data conditions.

2.3 Metadata-based Approach
Metadata such as location fields are useful as ef-
fective clues to predict geolocation. Hecht et al.
(2011) reported that decent accuracy of geolo-
cation prediction can be achieved using location
fields. Approaches to combine metadata with texts
are also proposed to extend text-based approaches.
Combinatory approaches such as a dynami-
cally weighted ensemble method (Mahmud et al.,
2012), polygon stacking (Schulz et al., 2013),
stacking (Han et al., 2013, 2014), and average
pooling with a neural network (Miura et al., 2016)
have strengthened geolocation prediction.

2.4 Combinatory Approach Extending
User-network-based Approach

Several attempts have been made to combine user-
network-based approaches with other approaches.
A text-based approach with logistic regression
was combined with label propagation approaches
to enhance geolocation prediction (Rahimi et al.,
2015a,b, 2016). Jayasinghe et al. (2016) com-
bined nine components including text-based ap-
proaches, metadata-based approaches, and a user-
network-based approach with a cascade ensemble
method.

2.5 Comparisons with Proposed Model
A model we propose in Section 3 which com-
bines text, metadata, and user network informa-
tion with a neural network, can be regarded as
an alternative to approaches using text and meta-
data (Mahmud et al., 2012; Schulz et al., 2013;
Han et al., 2013, 2014; Miura et al., 2016), ap-
proaches with text and user network informa-
tion (Rahimi et al., 2015a,b), and an approach
with text, metadata, and user network information
(Jayasinghe et al., 2016). In Section 5, we demon-
strate that our model outperforms earlier models.

1261



messages 
(timeline)	

RNNL	

AttentionL	

FCU	

label	

location	 description	 timezone	

Timezone 
Embedding	

AttentionTL	

RNND	

AttentionD	

RNNM	

AttentionM	

AttentionU	

Word Embedding	

linked 
cities	

linked 
users	

+	
AttentionN	

City 
Embedding	

User 
Embedding	

AttentionUN	

FCUN	

TEXT	

TEXT&META	

USERNET	

Figure 1: Overview of the proposed model. RNN denotes a recurrent neural network layer. FC denotes
a fully connected layer. The striped layers are message-level processes. ⊕ represents element-wise
addition.

In terms of machine learning methods, our
model is a neural network model that shares some
similarity with previous neural network models
(Liu and Inkpen, 2015; Miura et al., 2016). Our
model and these previous models have two key
differences. First, our model integrates user net-
work information along with other information.
Secondly, our model combines text and metadata
with an attention mechanism (Bahdanau et al.,
2014).

3 Model
3.1 Proposed Model
Figure 1 presents an overview of our model: a
complex neural network for classification with a
city as a label. For each user, the model accepts
inputs of messages, a location field, a description
field, a timezone, linked users, and the cities of
linked users.

User network information is incorporated by
city embeddings and user embeddings of linked
users. User embeddings are introduced along with
city embeddings because linked users with city in-
formation1 are limited. We chose to let the model
learn geolocation representations of linked users
directly via user embeddings. The model can be

1City information are provided by a dataset. The detail of
the city information is explained in Section 4.

broken down to several components, details of
which are described in Section 3.1.1–3.1.4.

3.1.1 Text Component
We describe the text component of the model,
which is the “TEXT” section in Figure 1. Figure 2
presents an overview of the text component. The
component consists of a recurrent neural network
(RNN) (Graves, 2012) layer and attention layers.
An input of the component is a timeline of a user,
which consists of messages in a time sequence.

As an implementation of RNN, we used Gated
Recurrent Unit (GRU) (Cho et al., 2014) with a bi-
directional setting. In the RNN layer, word em-
beddings x of a message are processed with the
following transition functions:

zt = σ (W zxt + U zht−1 + bz) (1)

rt = σ (W rxt + U rht−1 + br) (2)

h̃t = tanh (W hxt + Uh (rt ⊙ ht−1) + bh)
(3)

ht = (1− zt)⊙ ht−1 + zt ⊙ h̃t (4)

where zt is an update gate, rt is a reset
gate, h̃t is a candidate state, ht is a state,
W z,W r,W h, U z, U r, Uh are weight matrices,
bz, br, bh are bias vectors, σ is a logistic sigmoid
function, and ⊙ is an element-wise multiplica-
tion operator. The bi-directional GRU outputs

−→
h

1262



messages 
(timeline)

timeline 
representation

AttentionTL

RNNM

AttentionM

Word Embedding

x1 xT

…input

h1

bi-directional 
recurrent  

states
…

g1 g2 gT

RNN 
features

…

x2

u1

context 
vectors

+
…α1g1 α2g2 αTgT

Attention 
features m

u2 uT

Attention 
Layer

RNN 
Layer h1

h2

h2

hT

hT

Figure 2: Overview of the text component with
detailed description of RNNM and AttentionM.

and
←−
h are concatenated to form g where gt =−→

ht∥
←−
ht and are passed to the first attention layer

AttentionM.
AttentionM computes a message representa-

tion m as a weighted sum of gt with weight αt:

m =
∑

t

αtgt (5)

αt =
exp

(
vTαut

)
∑

t exp (v
T
αut)

(6)

ut = tanh (W αgt + bα) (7)

where vα is a weight vector, W α is a weight ma-
trix, and bα a bias vector. ut is an attention con-
text vector calculated from gt with a single fully-
connected layer (Eq. 7). ut is normalized with
softmax to obtain αt as a probability (Eq. 6). The
message representation m is passed to the second
attention layer AttentionTL to obtain a timeline
representation from message representations.

3.1.2 Text and Metadata Component
We describe text and metadata components of the
model, which is the “TEXT&META” section in
Figure 1. This component considers the following
three types of metadata along with text: location
a text field in which a user is allowed to write the
user location freely, description a text field a user
can use for self-description, and timezone a selec-
tive field from which a user can choose a timezone.
Note that certain percentages of these fields are not
available2, and unknown tokens are used for inputs
in such cases.

2Han et al. (2014) reported missing percentages of 19%
for location, 24% for description, and 25%for timezone.

linked 
cities	

+	
AttentionN	

City 
Embedding	

User 
Embedding	

user network 
representation	

linked 
users	

linked 
user 1	

linked 
user N	

current 
user	

User 
Network	 …	

c1 cN

…inputs

p1 p2 pN

…

c2

u1

context 
vectors

+
…α1p1 α2p2 αNpN

Attention 
features m

u2 uN

Attention 
Layer

a1 aN

…
a2

+ ++

Figure 3: Overview of the user network compo-
nent with a detailed description of the element-
wise addition and AttentionN.

We process location fields and description fields
similarly to messages using an RNN layer and an
attention layer. Because there is only one loca-
tion and one description per user, a second atten-
tion layer is not required, as it is in the text com-
ponent. We also chose to share word embeddings
among the messages, the location, and the descrip-
tion processes because these inputs are all textual
information. For the timezone, an embedding is
assigned for each timezone value. A processed
timeline representation, a location representation,
and a description representation are then passed
to the attention layer AttentionU with a timezone
representation. AttentionU combines these four
representations and outputs a user representation.
This combination is done as in AttentionTL with
four representations as g1 . . . g4 in Eq. 5.

3.1.3 User Network Component
We describe the user network component of the
model, which is the “USERNET” section in Fig-
ure 1. Figure 3 presents an overview of the user
network component. The model has two inputs
linked cities and linked users. Users connected
with a user network are extracted as linked users.
We treat their cities3 as linked cities. Linked cities
and linked users are assigned with city embed-
dings c and user embeddings a respectively. c
and a are then processed to output p = c ⊕ a,
where ⊕ is an element-wise addition operator. p
is then passed to the subsequent attention layer
AttentionN to obtain a user network representa-

3A user with city information implies that the user is in-
cluded in a training set.

1263



TwitterUS 
(train)	

W-NUT 
(train)	

#user	 279K	 782K	
#tweet	 23.8M	 9.03M	
tweet/user	 85.6	 11.6	
#edge	 3.69M	 3.21M	
#reduced-edge	 2.11M	 1.01M	
reduced-edge/user	 7.04	 1.29	
#city	 339	 3028	

Table 1: Some properties of TwitterUS (train) and
W-NUT (train). We were able to obtain approxi-
mately 70–78% of the full datasets because of ac-
cessibility changes in Twitter.

tion as in AttentionU.

3.1.4 Model Output
An output of the text and metadata component
and an output of the mention network compo-
nent are further passed to the final attention layer
AttentionUN to obtain a merged user representa-
tion as in AttentionU. The merged user represen-
tation is then connected to labels with a fully con-
nected layer FCUN.

3.2 Sub-models of the Proposed Model
SUB-NN-TEXT We prepare a sub-model SUB-
NN-TEXT by adding FCU and FCUN to the text
component. This sub-model can be considered as
a variant of a neural network model by Yang et al.
(2016), which learns a representation of hierarchi-
cal text.

SUB-NN-UNET We prepare a sub-model SUB-
NN-UNET by connecting the text component
and the user network component with FCU,
AttentionUN, and FCUN. This model can be re-
garded as a model that uses text and user network
information.

SUB-NN-META We prepare a sub-model SUB-
NN-META by adding FCU and FCUN to the
metadata component. This model is a text-meta-
based model that uses text and metadata.

4 Data
4.1 Dataset Specifications
TwitterUS The first dataset we used is Twit-
terUS assembled by Roller et al. (2012), which
consists of 429K training users, 10K development
users, and 10K test users in a North American re-
gion. The ground truth location of a user is set
to the first geotag of the user in the dataset. We

collected TwitterUS tweets using TwitterAPI to re-
construct TwitterUS to obtain metadata along with
text. Up to date versions in November–December
2016 were used for the metadata4. We additionally
assigned city centers to ground truth geotags us-
ing the city category of Han et al. (2012) to make
city prediction possible in this dataset. TwitterUS
(train) in Table 1 presents some properties related
to the TwitterUS training set.

W-NUT The second dataset we used is W-NUT,
a user-level dataset of the geolocation prediction
shared task of W-NUT 2016 (Han et al., 2016).
The dataset consists of 1M training users, 10K de-
velopment users, and 10K test users. The ground
truth location of a user is decided by majority vot-
ing of the closest city center. Like in TwitterUS,
we obtained metadata and texts using TwitterAPI.
Up to date versions in August–September 2016
were used for the metadata. W-NUT (train) in Ta-
ble 1 presents some properties related to the W-
NUT training set.

4.2 Construction of the User Network
We construct mention networks (Jurgens, 2013;
Compton et al., 2014; Rahimi et al., 2015a,b)
from datasets as user networks. To do so,
we follow the approach of Rahimi et al. (2015a)
and Rahimi et al. (2015b) who use uni-directional
mention to set edges of a mention network. An
edge is set between the two users nodes if a
user mentions another user. The number of uni-
directional mention edges for TwitterUS and W-
NUT can be found in Table 1.

The uni-directional setting results to large num-
bers of edges, which often are computationally ex-
pensive to process. We restricted edges to satisfy
one of the following conditions to reduce the size:
(1) both users have ground truth locations or (2)
one user has a ground truth location and another
user is mentioned 5 times or more in a training set.
The number of reduced-edges with these condi-
tions in TwitterUS and W-NUT can be confirmed
in Table 1.

5 Evaluation
5.1 Implemented Baselines
5.1.1 LR
LR is an l1-regularized logistic regression model
with k-d tree regions (Roller et al., 2012) used

4TwitterAPI returns the current version of metadata even
for an old tweet.

1264



in Rahimi et al. (2015a). The model uses tf-
idf weighted bag-of-words unigrams for features.
This model is simple, but it has shown state-of-
the-art performance in cases when only text is
available.

5.1.2 MADCEL-B-LR
MADCEL-B-LR, a model presented by
(Rahimi et al., 2015a), combines LR with Modi-
fied Adsorption (MAD) (Talukdar and Crammer,
2009). MAD is a graph-based label propagation
algorithm that optimizes an objective with a prior
term, a smoothness term, and an uninforma-
tiveness term. LR is combined with MAD by
introducing LR results as dongle nodes to MAD.

This model includes an algorithm for the con-
struction of a mention network. The algorithm
removes celebrity users5 and collapses a men-
tion network6. We use binary edges for user net-
work edges because they performed slightly better
than weighted edges by accuracy@161 metric in
Rahimi et al. (2015a).

5.1.3 LR-STACK
LR-STACK is an ensemble learning model that
combines four LR classifiers (LR-MSG, LR-LOC,
LR-DESC, LR-TZ) with an l2-regularized logistic
regression meta-classifier (LR-2ND). LR-MSG,
LR-LOC, LR-DESC, and LR-TZ respectively use
messages, location fields, description fields, and
timezones as their inputs. This model is simi-
lar to the stacking (Wolpert, 1992) approach taken
in Han et al. (2013) and Han et al. (2014), which
showed superior performance compared to a fea-
ture concatenation approach.

The model takes the following three steps to
combine text and metadata: Step 1 LR-MSG, LR-
LOC, LR-DESC, and LR-TZ are trained using a
training set, Step 2 the outputs of the four classi-
fiers on the training set are obtained with 10-fold
cross validation, and Step 3 LR-2ND is trained us-
ing the outputs of the four classifiers.

5.1.4 MADCEL-B-LR-STACK
MADCEL-B-LR-STACK is a combined model of
MADCEL-B-LR and LR-STACK. LR-STACK re-
sults are introduced as dongle nodes to MAD in-
stead of LR results to combine text, metadata, and
network information.

5Users with more than t unique mentions.
6Users not included in training users or test users are re-

moved and disconnected edges with the removals are con-
verted to direct edges.

5.2 Model Configurations
5.2.1 Text Processor
We applied a lower case conversion, a unicode
normalization, a Twitter user name normalization,
and a URL normalization for text pre-processing.
The pre-processed text is then segmented us-
ing Twokenizer (Owoputi et al., 2013) to obtain
words.

5.2.2 Pre-training of Embeddings
We pre-trained word embeddings using messages,
location fields, and description fields of a train-
ing set using fastText (Bojanowski et al., 2016)
with the skip-gram algorithm. We also pre-trained
user embeddings using the non-reduced mention
network described in Section 4.2 of a training
set with LINE (Tang et al., 2015). The detail of
pre-training parameters are described in Appendix
A.1.

5.2.3 Neural Network Optimization
We chose an objective function of our models to
cross-entropy loss. l2 regularization was applied
to the RNN layers, the attention context vectors,
and the FC layers of our models to avoid over-
fitting. The objective function was minimized
through stochastic gradient descent over shuffled
mini-batches with Adam (Kingma and Ba, 2014).

5.2.4 Model Parameters
The layers and the embeddings in our models have
unit size and embedding dimension parameters.
Our models and the baseline models have reg-
ularization parameter α, which is sensitive to a
dataset. The baseline models have additional k-d
tree bucket size c, celebrity threshold t, and MAD
parameters µ1, µ2, and µ3, which are also data
sensitive.

We chose optimal values for these parameters
in terms of accuracy with a grid search using the
development sets of TwitterUS and W-NUT. De-
tails of the parameter selection strategies and the
selected values are described in Appendix A.2.

5.2.5 Metrics
We evaluate the models in the following four
commonly used metrics in geolocation predic-
tion: accuracy the percentage of correctly pre-
dicted cities, accuracy@161 a relaxed accuracy
that takes prediction errors within 161 km as cor-
rect predictions, median error distance median
value of error distances in predictions, and mean
error distance mean value of error distances in
predictions.

1265



Model	 Sign. Test ID	 Accuracy	
Accuracy

@161	
Error Distance	

Median	 Mean	

Baselines 
(reported)	

Han et al. (2012)	
Wing and Baldridge (2014)	
LR (Rahimi et al. 2015b) 
LR-NA (Rahimi et al. 2016)	
MADCEL-B-LR (Rahimi et al. 2015a) 
MADCEL-W-LR (Rahimi et al. 2015a) 

26.0	
-	
-	
- 
- 
-	

45.0	
49.2	
50 
51	
60 
60	

260	
170.5	
159 
148	
77 
78	

814	
703.6	
686 
636	
533 
529	

Baselines 
(implemented)	

LR	
MADCEL-B-LR	
LR-STACK	
MADCEL-B-LR-STACK	

i	
ii	
iii	
iv	

42.0	
50.2	
50.8	
55.7	

52.7	
60.1	
64.1	
67.7	

121.1	
66.5	
42.3*	
45.1	

666.6	
582.8	
427.7	
412.7	

Our Models	

SUB-NN-TEXT	
SUB-NN-UNET	
SUB-NN-META 
Proposed Model	

i	
ii	
iii	
iv	

44.9**	
51.0	
54.6**	
58.5**	

55.6**	
61.5*	
67.2**	
70.1**	

110.5	
65.0	
46.8	
41.9*	

585.1**	
481.5**	
356.3** 
335.7**	

Table 2: Performances of our models and the baseline models on TwitterUS. Significance tests were per-
formed between models with same Sign. Test IDs. The shaded lines represent values copied from related
papers. Asterisks denote significant improvements against paired counterparts with 1% confidence (**)
and 5% confidence (*).

Model	 Sign. Test ID	 Accuracy	
Accuracy

@161	
Error Distance	

Median	 Mean	
Baselines 
(reported)	

Miura et al. (2016) 
Jayasinghe et al. (2016) 

47.6	
52.6	

- 
-	

16.1	
21.7	

1122.3 
1928.8	

Baselines 
(implemented)	

LR	
MADCEL-B-LR	
LR-STACK	
MADCEL-B-LR-STACK	

i	
ii	
iii	
iv	

34.1	
36.2 
51.2	
51.6	

46.7 
49.7 
64.9	
65.3	

248.7 
166.3 
0.0 
0.0	

2216.4	
2120.6	
1496.4 
1471.9	

Our Models	

SUB-NN-TEXT 
SUB-NN-UNET	
SUB-NN-META 
Proposed Model	

i	
ii	
iii	
iv	

35.4**	
38.1** 
54.7**	
56.4**	

50.3**	
53.3** 
70.2**	
71.9**	

155.8**	
99.9**	
0.0 
0.0	

1592.6**	
1498.6** 
825.8**	
780.5**	

Table 3: Performance of our models and baseline models on W-NUT. The same notations as those in
Table 2 are used in this table.

5.3 Result
Performance on TwitterUS
Table 2 presents results of our models and the im-
plemented baseline models on TwitterUS. We also
list values from earlier reports (Han et al., 2012;
Wing and Baldridge, 2014; Rahimi et al., 2015a,b,
2016) to make our results readily comparable with
past reported values.

We performed some statistical significance tests
among model pairs that share the same inputs.
The values in the Sign. Test ID column of Table
2 represent the IDs of these pairs. As a prepa-
ration of statistical significance tests, accuracies,
accuracy@161s, and error distances of each test
user were calculated for each model pair. Two-
sided Fisher-Pittman Permutation tests were used
for testing accuracy and accuracy@161. Mood’s
median test was used for testing error distance in
terms of median. Paired t-tests were used for test-
ing error distance in terms of mean.

We confirmed the significance of improvements

in accuracy@161 and mean distance error for all
of our models. Three of our models also im-
proved in terms of accuracy. Especially, the pro-
posed model achieved a 2.8% increase in accu-
racy and a 2.4% increase in accuracy@161 against
the counterpart baseline model MADCEL-B-LR-
STACK. One negative result we found was the me-
dian error distance between SUB-NN-META and
LR-STACK. The baseline model LR-STACK per-
formed 4.5 km significantly better than our model.

Performance on W-NUT
Table 3 presents the results of our models and
the implemented baseline models on W-NUT. As
for TwitterUS, we listed values from Miura et al.
(2016) and Jayasinghe et al. (2016). We tested the
significance of these results in the same way as we
did for TwitterUS.

We confirmed significant improvement in the
four metrics for all of our models. The proposed
model achieved a 4.8% increase in accuracy and a

1266



description	

location	timeline	

timezone	

Figure 4: Estimated probability density functions
of the four representations in AttentionU.

6.6% increase in accuracy@161 against the coun-
terpart baseline model MADCEL-B-LR-STACK.
The accuracy is 3.8% higher against the previously
reported best value (Jayasinghe et al., 2016) which
combined texts, metadata, and user network infor-
mation with an ensemble method.

6 Discussion
6.1 Analyses of Attention Probabilities

6.1.1 Unification Strategies
In the evaluation, the proposed model has implic-
itly shown effectiveness at unifying text, meta-
data, and user network representations through im-
provements in the four metrics. However, details
of the unification processes are not clear from the
model outputs because they are merely the prob-
abilities of estimated locations. To gain insight
into the unification processes, we analyzed the
states of two attention layers: AttentionU and
AttentionUN in Figure 1.

Figure 4 presents the estimated probability den-
sity functions (PDFs) of the four input represen-
tations for AttentionU. These PDFs are esti-
mated with kernel density estimation from the de-
velopment sets of TwitterUS and W-NUT, where
all four representations are available. From the
PDFs, it is apparent that the model assigns higher
probabilities to time line representations than to
other three representations in TwitterUS compared
to W-NUT. This finding is reasonable because
timelines in TwitterUS consist of more tweets
(tweet/user in Table 1) and are likely to be more
informative than in W-NUT.

Figure 5 presents the estimated PDFs of user
network representations for AttentionUN. These

user network	

Figure 5: Estimated probability density functions
of user network representations in AttentionUN.

PDFs are estimated from the development sets
of TwitterUS and W-NUT, where both input rep-
resentations are available. Strong preference of
network representation for TwitterUS against W-
NUT is found in the PDFs. This finding is in-
tuitive because TwitterUS has substantially more
user network edges (reduced-edge/user in Table 1)
than W-NUT, which is likely to benefit more from
user network information.

6.1.2 Attention Patterns
We further analyzed the proposed model by clus-
tering attention probabilities to capture typical
attention patterns. For each user, we assigned
six attention probabilities of AttentionU and
AttentionUN as features for a clustering. A k-
means clustering was performed over these users
with 9 clusters. The clustering clearly separated
the users to 5 clusters for TwitterUS users and 4
clusters for W-NUT users. We extracted typical
users of each cluster by selecting the closest users
of the cluster centroids. Figure 6 shows a cluster-
ing result and the attention probabilities of these
users.

These attention probabilities can be considered
as typical attention patterns of the proposed model
and match with the previously estimated PDFs.
For example, cluster 2 and 3 represent an atten-
tion pattern that processes users by balancing the
representations of locations along with the repre-
sentations of timelines. Additionally, the location
probabilities in this pattern are in the right tail re-
gion of the location PDF.

6.2 Limitations of Proposed Model

6.2.1 City Prediction
The evaluation produced improvements in most
of our models in the four metrics. One excep-
tion we found was the median distance error be-
tween SUB-NN-META and LR-STACKING in
TwitterUS. Because the median distance error of
SUB-NN-META was quite low (46.8 km), we

1267



1	

2	

3	

4	

5	
6	

7	 8	
9	

TwitterUS	
W-NUT	

Cluster 
ID Dataset Timeline Location Description Timezone User 

User 
Network 

1 TwitterUS 0.843 0.082 0.040 0.035 0.359 0.641
2 W-NUT 0.517 0.317 0.081 0.085 0.732 0.268
3 TwitterUS 0.432 0.430 0.069 0.069 0.319 0.681
4 W-NUT 0.637 0.160 0.097 0.105 0.737 0.263
5 TwitterUS 0.593 0.219 0.114 0.075 0.230 0.770
6 TwitterUS 0.672 0.214 0.069 0.045 0.365 0.635
7 W-NUT 0.741 0.077 0.080 0.102 0.605 0.395
8 TwitterUS 0.766 0.099 0.068 0.067 0.222 0.778
9 W-NUT 0.800 0.067 0.056 0.078 0.730 0.270

Figure 6: A k-means clustering result and the attention probabilities of users that are closest to the cluster
centroids. The underlined values are the max values of the two datasets for each column.

Model	
Error Distance 

Median Mean σ 
Oracle	 23.3	 31.4	 30.1	

Table 4: Error distance values in TwitterUS with
oracle predictions. σ in the table denotes the stan-
dard deviation.

measured the performance of an oracle model
where city predictions are all correct (accuracy of
100%) in the test set.

Table 4 denotes this oracle performance. The
oracle mean error distance is 31.4 km. Its stan-
dard deviation is 30.1. Note that ground truth loca-
tions of TwitterUS are geotags and will not exactly
match the oracle city centers. These oracle values
imply that the current median error distances are
close to the lower bound of the city classification
approach and that they are difficult to improve.

6.2.2 Errors with High Confidences
The proposed model still contains 28–30% errors
even in accuracy@161. A qualitative analysis of
errors with high confidences was performed to in-
vestigate cases that the model fails. We found two
common types of error in the error analysis. The
first is a case when a location field is incorrect due
to a reason such as a house move. For example,
the model predicted “Hong Kong” for a user with
a location field of “Hong Kong” but has the gold
location of “Toronto”. The second is a case when
a user tweets a place name of a travel. For exam-
ple, the model predicted “San Francisco” for a user
who tweeted about a travel to “San Francisco” but
has the gold location of “Boston”.

These two types of error are difficult to han-
dle with the current architecture of the proposed
model. The architecture only supports single lo-
cation field which disables the model to track lo-
cation changes. The architecture also treats each

tweet independently which forbids the model to
express a temporal state like traveling.

7 Conclusion
As described in this paper, we proposed a complex
neural network model for geolocation prediction.
The model unifies text, metadata, and user net-
work information. The model achieved the max-
imum of a 3.8% increase in accuracy and a max-
imum of 6.6% increase in accuracy@161 against
several previous state-of-the-art models. We fur-
ther analyzed the states of several attention layers,
which revealed that the probabilities assigned to
timeline representations and user network repre-
sentations match to some statistical characteristics
of datasets.

As future works of this study, we are planning
to expand the proposed model to handle multi-
ple locations and a temporal state to capture lo-
cation changes and states like traveling. Addi-
tionally, we plan to apply the proposed model to
other social media analyses such as gender anal-
ysis and age analysis. In these analyses, meta-
data like location fields and timezones may not
be effective like in geolocation prediction. How-
ever, a user network is known to include various
user attributes information including gender and
age (McPherson et al., 2001) which suggests the
unification of text and user network information
to result in a success as in geolocation prediction.

Acknowledgments
We would like to thank the members of Okumura–
Takamura Group at Tokyo Institute of Technology
for having insightful discussions about user profil-
ing models in social media. We would also like to
thank the anonymous reviewer for their comments
to improve this paper.

1268



References
Amr Ahmed, Liangjie Hong, and Alexander J. Smola.

2013. Hierarchical geographical modeling of user
locations from social media posts. In Proceedings
of the 22nd International Conference on World Wide
Web. pages 25–36.

Lars Backstrom, Eric Sun, and Cameron Marlow. 2010.
Find me if you can: Improving geographical predic-
tion with social and spatial proximity. In Proceed-
ings of the 19th International Conference on World
Wide Web. pages 61–70.

Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua
Bengio. 2014. Neural machine translation by
jointly learning to align and translate. Com-
puting Research Repository abs/1409.0473.
http://arxiv.org/abs/1409.0473.

Piotr Bojanowski, Edouard Grave, Armand Joulin,
and Tomas Mikolov. 2016. Enriching word vec-
tors with subword information. arXiv preprint
arXiv:1607.04606 .

Miriam Cha, Youngjune Gwon, and H. T. Kung. 2015.
Twitter geolocation and regional classification via
sparse coding. In Proceedings of the Ninth Interna-
tional AAAI Conference on Web and Social Media.

Zhiyuan Cheng, James Caverlee, and Kyumin Lee.
2010. You are where you tweet: A content-based ap-
proach to geo-locating Twitter users. In Proceedings
of the 19th ACM International Conference on In-
formation and Knowledge Management. pages 759–
768.

Zhiyuan Cheng, James Caverlee, and Kyumin Lee.
2013. A content-driven framework for geolocating
microblog users. ACM Transactions on Intelligent
Systems and Technology 4(1):1–27. Article 2.

Kyunghyun Cho, Bart van Merrienboer, Caglar Gul-
cehre, Dzmitry Bahdanau, Fethi Bougares, Holger
Schwenk, and Yoshua Bengio. 2014. Learning
phrase representations using RNN encoder–decoder
for statistical machine translation. In Proceedings of
the 2014 Conference on Empirical Methods in Nat-
ural Language Processing. pages 1724–1734.

Ryan Compton, David Jurgens, and David Allen. 2014.
Geotagging one hundred million Twitter accounts
with total variation minimization. In Proceedings
of the 2014 IEEE International Conference on Big-
Data. pages 393–401.

David J. Crandall, Lars Backstrom, Daniel Hutten-
locher, and Jon Kleinberg. 2009. Mapping the
world’s photos. In Proceedings of the 18th Interna-
tional Conference on World Wide Web. pages 761–
770.

Aron Culotta. 2010. Towards detecting influenza epi-
demics by analyzing Twitter messages. In Proceed-
ings of the First Workshop on Social Media Analyt-
ics. pages 115–122.

Clodoveu A. Davis Jr., Gisele L. Pappa, Diogo
Rennó Rocha de Oliveira, and Filipe de L. Arcanjo.
2011. Inferring the location of Twitter messages
based on user relationships. Transactions in GIS
15(6):735–751.

Jacob Eisenstein, Amr Ahmed, and Eric P. Xing. 2011.
Sparse additive generative models of text. In Pro-
ceedings of the 28th International Conference on
Machine Learning. pages 1041–1048.

Jacob Eisenstein, Brendan O’Connor, Noah A. Smith,
and Eric P. Xing. 2010. A latent variable model for
geographic lexical variation. In Proceedings of the
2010 Conference on Empirical Methods in Natural
Language Processing. pages 1277–1287.

Ian Goodfellow, Yoshua Bengio, and Aaron Courville.
2016. Deep Learning. MIT Press.

Alex Graves. 2012. Supervised Sequence Labelling
with Recurrent Neural Networks, volume 385 of
Studies in Computational Intelligence. Springer-
Verlag Berlin Heidelberg.

Bo Han, Paul Cook, and Timothy Baldwin. 2012. Ge-
olocation prediction in social media data by finding
location indicative words. In Proceedings of COL-
ING 2012. pages 1045–1062.

Bo Han, Paul Cook, and Timothy Baldwin. 2013. A
stacking-based approach to twitter user geolocation
prediction. In Proceedings of the 51st Annual Meet-
ing of the Association for Computational Linguis-
tics: System Demonstrations. pages 7–12.

Bo Han, Paul Cook, and Timothy Baldwin. 2014. Text-
based Twitter user geolocation prediction. Journal
of Artificial Intelligence Research 49(1):451–500.

Bo Han, Afshin Rahimi, Leon Derczynski, and Timo-
thy Baldwin. 2016. Twitter geolocation prediction
shared task of the 2016 workshop on noisy user-
generated text. In Proceedings of the Second Work-
shop on Noisy User-generated Text. pages 213–217.

Brent Hecht, Lichan Hong, Bongwon Suh, and Ed H.
Chi. 2011. Tweets from Justin Bieber’s heart: the
dynamics of the location field in user profiles. In
Proceedings of the SIGCHI Conference on Human
Factors in Computing Systems. pages 237–246.

Liangjie Hong, Amr Ahmed, Siva Gurumurthy,
Alexander J. Smola, and Kostas Tsioutsiouliklis.
2012. Discovering geographical topics in the Twit-
ter stream. In Proceedings of the 21st International
Conference on World Wide Web. pages 769–778.

Gaya Jayasinghe, Brian Jin, James Mchugh, Bella
Robinson, and Stephen Wan. 2016. CSIRO Data61
at the WNUT geo shared task. In Proceedings of
the Second Workshop on Noisy User-generated Text.
pages 218–226.

1269



David Jurgens. 2013. That’s what friends are for: Infer-
ring location in online social media platforms based
on social relationships. In Proceedings of the Sev-
enth International AAAI Conference on Web and So-
cial Media.

David Jurgens, Tyler Finethy, James McCorriston,
Yi Xu, and Derek Ruths. 2015. Geolocation pre-
diction in Twitter using social networks: A critical
analysis and review of current practice. In Proceed-
ings of the Ninth International AAAI Conference on
Web and Social Media.

Diederik P. Kingma and Jimmy Ba. 2014. Adam: A
method for stochastic optimization. arXiv preprint
arXiv:1412.6980 .

Sheila Kinsella, Vanessa Murdock, and Neil O’Hare.
2011. ”I’m eating a sandwich in Glasgow”: Mod-
eling locations with tweets. In Proceedings of the
Third International Workshop on Search and Min-
ing User-generated Contents. pages 61–68.

Longbo Kong, Zhi Liu, and Yan Huang. 2014. SPOT:
Locating social media users based on social net-
work context. Proceedings of the VLDB Endowment
7(13):1681–1684.

Rui Li, Shengjie Wang, and Kevin Chen-Chuan Chang.
2012a. Multiple location profiling for users and rela-
tionships from social network and content. Proceed-
ings of the VLDB Endowment 5(11):1603–1614.

Rui Li, Shengjie Wang, Hongbo Deng, Rui Wang, and
Kevin Chen-Chuan Chang. 2012b. Towards social
user profiling: Unified and discriminative influence
model for inferring home locations. In Proceedings
of the 18th ACM SIGKDD International Conference
on Knowledge Discovery and Data Mining. pages
1023–1031.

Ji Liu and Diana Inkpen. 2015. Estimating user lo-
cation in social media with stacked denoising auto-
encoders. In Proceedings of the First Workshop on
Vector Space Modeling for Natural Language Pro-
cessing. pages 201–210.

Jalal Mahmud, Jeffrey Nichols, and Clemens Drews.
2012. Where is this tweet from? Inferring home lo-
cations of Twitter users. In Proceedings of the Sixth
International AAAI Conference on Weblogs and So-
cial Media.

Eugenio Martı́nez-Cámara, Maria Teresa Martı́n-
Valdivia, Luis Alfonso Ureña López, and Arturo
Montejo Raéz. 2014. Sentiment analysis in Twitter.
Natural Language Engineering 20(1):1–28.

Jeffrey McGee, James Caverlee, and Zhiyuan Cheng.
2013. Location prediction in social media based on
tie strength. In Proceedings of the 22nd ACM Inter-
national Conference on Information & Knowledge
Management. pages 459–468.

Miller McPherson, Lynn Smith-Lovin, and James M
Cook. 2001. Birds of a feather: Homophily in social
networks. Annual review of sociology 27(1):415–
444.

Yasuhide Miura, Motoki Taniguchi, Tomoki Taniguchi,
and Tomoko Ohkuma. 2016. A simple scalable neu-
ral networks based model for geolocation prediction
in Twitter. In Proceedings of the Second Workshop
on Noisy User-generated Text. pages 235–239.

Simon E. Overell. 2009. Geographic Information Re-
trieval: Classification, Disambiguation, and Model-
ing. Ph.D. thesis, Imperial College London.

Olutobi Owoputi, Brendan O’Connor, Chris Dyer,
Kevin Gimpel, Nathan Schneider, and Noah A.
Smith. 2013. Improved part-of-speech tagging for
online conversational text with word clusters. In
Proceedings of the 2013 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics: Human Language Technologies.
pages 380–390.

Afshin Rahimi, Trevor Cohn, and Timothy Baldwin.
2015a. Twitter user geolocation using a unified text
and network prediction model. In Proceedings of the
53rd Annual Meeting of the Association for Compu-
tational Linguistics and the 7th International Joint
Conference on Natural Language Processing (Vol-
ume 2: Short Papers). pages 630–636.

Afshin Rahimi, Trevor Cohn, and Timothy Baldwin.
2016. pigeo: A python geotagging tool. In Proceed-
ings of ACL-2016 System Demonstrations. pages
127–132.

Afshin Rahimi, Duy Vu, Trevor Cohn, and Timothy
Baldwin. 2015b. Exploiting text and network con-
text for geolocation of social media users. In Pro-
ceedings of the 2015 Conference of the North Amer-
ican Chapter of the Association for Computational
Linguistics: Human Language Technologies. pages
1362–1367.

Delip Rao, David Yarowsky, Abhishek Shreevats, and
Manaswi Gupta. 2010. Classifying latent user at-
tributes in Twitter. In Proceedings of the Second In-
ternational Workshop on Search and Mining User-
generated Contents. pages 37–44.

Stephen Roller, Michael Speriosu, Sarat Rallapalli,
Benjamin Wing, and Jason Baldridge. 2012. Super-
vised text-based geolocation using language models
on an adaptive grid. In Proceedings of the 2012 Joint
Conference on Empirical Methods in Natural Lan-
guage Processing and Computational Natural Lan-
guage Learning. pages 1500–1510.

Dominic Rout, Kalina Bontcheva, Daniel Preoţiuc-
Pietro, and Trevor Cohn. 2013. Where’s @wally?:
A classification approach to geolocating users based
on their social ties. In Proceedings of the 24th ACM
Conference on Hypertext and Social Media. pages
11–20.

1270



Adam Sadilek, Henry Kautz, and Jeffrey P. Bigham.
2012. Finding your friends and following them to
where you are. In Proceedings of the Fifth ACM
International Conference on Web Search and Data
Mining. pages 723–732.

Takeshi Sakaki, Makoto Okazaki, and Yutaka Matsuo.
2010. Earthquake shakes Twitter users: Real-time
event detection by social sensors. In Proceedings
of the 19th International Conference on World Wide
Web. pages 851–860.

Axel Schulz, Aristotelis Hadjakos, Heiko Paulheim,
Johannes Nachtwey, and Max Mühlhäuser. 2013.
A multi-indicator approach for geolocalization of
tweets. In Proceedings of the Seventh International
AAAI Conference on Web and Social Media.

Pavel Serdyukov, Vanessa Murdock, and Roelof van
Zwol. 2009. Placing Flickr photos on a map. In
Proceedings of the 32nd International ACM SIGIR
Conference on Research and Development in Infor-
mation Retrieval. pages 484–491.

Partha Pratim Talukdar and Koby Crammer. 2009.
New regularized algorithms for transductive learn-
ing. In Proceedings of the European Conference
on Machine Learning and Knowledge Discovery in
Databases: Part II. pages 442–457.

Jian Tang, Meng Qu, Mingzhe Wang, Ming Zhang, Jun
Yan, and Qiaozhu Mei. 2015. LINE: Large-scale in-
formation network embedding. In Proceedings of
the 24th International Conference on World Wide
Web. pages 1067–1077.

Andranik Tumasjan, Timm O. Sprenger, Philipp G.
Sandner, and Isabell M. Welpe. 2010. Predicting
elections with Twitter: What 140 characters reveal
about political sentiment. In Proceedings of the
Fourth International AAAI Conference on Weblogs
and Social Media. pages 178–185.

Benjamin Wing and Jason Baldridge. 2011. Simple su-
pervised document geolocation with geodesic grids.
In Proceedings of the 49th Annual Meeting of the
Association for Computational Linguistics: Human
Language Technologies. pages 955–964.

Benjamin Wing and Jason Baldridge. 2014. Hierar-
chical discriminative classification for text-based ge-
olocation. In Proceedings of the 2014 Conference
on Empirical Methods in Natural Language Pro-
cessing. pages 336–348.

David H. Wolpert. 1992. Stacked generalization. Neu-
ral Networks 5(2):241–259.

Zichao Yang, Diyi Yang, Chris Dyer, Xiaodong He,
Alex Smola, and Eduard Hovy. 2016. Hierarchi-
cal attention networks for document classification.
In Proceedings of the 2016 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics: Human Language Technologies.
pages 1480–1489.

1271



A Supplemental Materials
A.1 Parameters of Embedding Pre-training
Word embeddings were pre-trained with the pa-
rameters of learning rate=0.025, window size=5,
negative sample size=5, and epoch=5. User em-
beddings were pre-trained with the parameters of
initial learning rate=0.025, order=2, negative sam-
ple size=5, and training sample size=100M.

A.2 Model Parameters and Parameter
Selection Strategies

Unit Sizes, Embedding Dimensions, and a Max
Tweet Number
The layers and the embeddings in our models have
unit size and embedding dimension parameters.
We also restricted the maximum number of tweets
per user for TwitterUS to reduce memory foot-
prints. Table 5 shows the values for these param-
eters. Smaller values were set for TwitterUS be-
cause TwitterUS is approximately 2.6 times larger
in terms of tweet number. It was computationally
expensive to process TwiiterUS in the same set-
tings as W-NUT.

Regularization Parameters and Bucket Sizes
We chose optimal values of α using a grid search
with the development sets of TwitterUS and W-
NUT. The range of α was set as the following:
α ∈ {1e−4, 5e−5, 1e−5, 5e−6, 1e−6, 5e−7, 1e−7,
5e−8, 1e−8}.

We also chose optimal values of c using grid
search with the development sets of TwitterUS and
W-NUT for the baseline models. The range of c
was set as the following for TwitterUS:
c ∈ {50, 100, 150, 200, 250, 300, 339}.
The following was set for W-NUT:
c ∈ {100, 200, 300, 400, 500, 600, 700, 800, 900,
1000, 1500, 2000, 2500, 3000, 3028}.
Table 6 presents selected values of α and c. For
LR-STACK and MADCEl-B-LR-STACK, differ-
ent parameters of α and c were selected for each
logistic regression classifier.

MAD Parameters and Celebrity Threshold
The MAD parameters µ1, µ2, and µ3 and celebrity
threshold t were also chosen using grid search
with the development sets of TwitterUS and W-
NUT. The ranges of µ1, µ2, and µ3 were set as the
following:
µ1 ∈ {1.0}, µ2 ∈ {0.001, 0.01, 0.1, 1.0, 10.0},
µ3 ∈ {0.0, 0.001, 0.01, 0.1, 1.0, 10.0}.
The range of t for TwitterUS was set as t ∈
{2, . . . , 16}. The range of t for W-NUT was set

TwitterUS W-NUT 
RNN unit size	 100	 200	
Attention context vector size	 200	 400	
FC unit size	 200	 400	
Word embedding dimension	 100	 200	
Timezone embedding dimension	 200	 400	
City embedding dimension	 200	 400	
User embedding dimension	 200	 400	
Max tweet number per user	 200	 -	

Table 5: Unit sizes, embedding dimensions, and
max tweet numbers of our models.

Model	 Parameter	 TwitterUS W-NUT 
SUB-NN-TEXT	

α	

1e-8	 1e-7	

SUB-NN-UNET	 1e-6	 5e-8	

SUB-NN-META	 1e-8	 5e-8	

Proposed Model	 1e-6	 5e-8	

LR 
MADCEL-B-LR	

α	 1e-6	 5e-7	

c	 300	 3000	

LR-STACK 
MADCEL-B-LR-STACK	

αMSG	 1e-6	 5e-7	

αLOC	 1e-6	 1e-6	

αDESC	 5e-6	 1e-6	

αTZ	 1e-4	 5e-6	

α2ND	 1e-6	 1e-7	

cMSG	 300	 3000	
cLOC	 300	 3000	
cDESC	 250	 1500	
cTZ	 100	 2500	
c2ND	 300	 2000	

Table 6: Regularization parameters and bucket
sizes selected for our models and baseline models.

Model	 Parameter	 TwitterUS W-NUT 

MADCEL-B-LR	

µ1	 1.0	 1.0	
µ2	 1.0	 10.0	
µ3	 0.01	 0.1	
t	 5	 4	

MADCEL-B-LR-STACK	

µ1	 1.0	 1.0	
µ2	 1.0	 1.0	
µ3	 0.1	 0.0	
t	 4	 2	

Table 7: MAD parameters and celebrity threshold
selected for baseline models.

as t ∈ {2, . . . , 6}. Table 6 presents selected val-
ues of µ1, µ2, µ3, and t.

1272


	Unifying Text, Metadata, and User Network Representations with a Neural Network for Geolocation Prediction

