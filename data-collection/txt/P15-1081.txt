



















































Unifying Bayesian Inference and Vector Space Models for Improved Decipherment


Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics
and the 7th International Joint Conference on Natural Language Processing, pages 836–845,

Beijing, China, July 26-31, 2015. c©2015 Association for Computational Linguistics

Unifying Bayesian Inference and Vector Space Models for Improved
Decipherment

Qing Dou∗, Ashish Vaswani∗, Kevin Knight
Information Sciences Institute

Department of Computer Science
University of Southern California

{qdou,avaswani,knight}@isi.edu
Chris Dyer

School of Computer Science
Carnegie Mellon University
cdyer@cs.cmu.edu

Abstract

We introduce into Bayesian decipherment
a base distribution derived from similari-
ties of word embeddings. We use Dirich-
let multinomial regression (Mimno and
McCallum, 2012) to learn a mapping be-
tween ciphertext and plaintext word em-
beddings from non-parallel data. Exper-
imental results show that the base dis-
tribution is highly beneficial to decipher-
ment, improving state-of-the-art decipher-
ment accuracy from 45.8% to 67.4% for
Spanish/English, and from 5.1% to 11.2%
for Malagasy/English.

1 Introduction

Tremendous advances in Machine Translation
(MT) have been made since we began applying
automatic learning techniques to learn translation
rules automatically from parallel data. However,
reliance on parallel data also limits the develop-
ment and application of high-quality MT systems,
as the amount of parallel data is far from adequate
in low-density languages and domains.

In general, it is easier to obtain non-parallel
monolingual data. The ability to learn transla-
tions from monolingual data can alleviate obsta-
cles caused by insufficient parallel data. Motivated
by this idea, researchers have proposed different
approaches to tackle this problem. They can be
largely divided into two groups.

The first group is based on the idea proposed
by Rapp (1995), in which words are represented
as context vectors, and two words are likely to
be translations if their context vectors are simi-
lar. Initially, the vectors contained only context

∗ Equal contribution

words. Later extensions introduced more fea-
tures (Haghighi et al., 2008; Garera et al., 2009;
Bergsma and Van Durme, 2011; Daumé and Jagar-
lamudi, 2011; Irvine and Callison-Burch, 2013b;
Irvine and Callison-Burch, 2013a), and used more
abstract representation such as word embeddings
(Klementiev et al., 2012).

Another promising approach to solve this prob-
lem is decipherment. It has drawn significant
amounts of interest in the past few years (Ravi and
Knight, 2011; Nuhn et al., 2012; Dou and Knight,
2013; Ravi, 2013) and has been shown to improve
end-to-end translation. Decipherment views a for-
eign language as a cipher for English and finds
a translation table that converts foreign texts into
sensible English.

Both approaches have been shown to improve
quality of MT systems for domain adaptation
(Daumé and Jagarlamudi, 2011; Dou and Knight,
2012; Irvine et al., 2013) and low density lan-
guages (Irvine and Callison-Burch, 2013a; Dou et
al., 2014). Meanwhile, they have their own ad-
vantages and disadvantages. While context vec-
tors can take larger context into account, it re-
quires high quality seed lexicons to learn a map-
ping between two vector spaces. In contrast, de-
cipherment does not depend on any seed lexicon,
but only looks at a limited n-gram context.

In this work, we take advantage of both ap-
proaches and combine them in a joint inference
process. More specifically, we extend previous
work in large scale Bayesian decipherment by in-
troducing a better base distribution derived from
similarities of word embedding vectors. The main
contributions of this work are:
• We propose a new framework that combines

the two main approaches to finding transla-
tions from monolingual data only.

836



• We develop a new base-distribution tech-
nique that improves state-of-the art decipher-
ment accuracy by a factor of two for Span-
ish/English and Malagasy/English.
• We make our software available for future

research, functioning as a kind of GIZA for
non-parallel data.

2 Decipherment Model

In this section, we describe the previous decipher-
ment framework that we build on. This framework
follows Ravi and Knight (2011), who built an MT
system using only non-parallel data for translat-
ing movie subtitles; Dou and Knight (2012) and
Nuhn et al. (2012), who scaled decipherment to
larger vocabularies; and Dou and Knight (2013),
who improved decipherment accuracy with depen-
dency relations between words.

Throughout this paper, we use f to denote tar-
get language or ciphertext tokens, and e to denote
source language or plaintext tokens. Given cipher-
text f : f1...fn, the task of decipherment is to find
a set of parameters P (fi|ei) that convert f to sen-
sible plaintext. The ciphertext f can either be full
sentences (Ravi and Knight, 2011; Nuhn et al.,
2012) or simply bigrams (Dou and Knight, 2013).
Since using bigrams and their counts speeds up de-
cipherment, in this work, we treat f as bigrams,
where f = {fn}Nn=1 = {fn1 , fn2 }Nn=1.

Motivated by the idea from Weaver (1955), we
model an observed cipher bigram fn with the fol-
lowing generative story:
• First, a language model P (e) generates a se-

quence of two plaintext tokens en1 , e
n
2 with

probability P (en1 , e
n
2 ).

• Then, substitute en1 with fn1 and en2 with fn2
with probability P (fn1 | en1 ) · P (fn2 | en2 ).

Based on the above generative story, the proba-
bility of any cipher bigram fn is:

P (fn) =
∑
e1e2

P (e1e2)
2∏
i=1

P (fni | ei)

The probability of the ciphertext corpus,

P ({fn}Nn=1) =
N∏
n=1

P (fn)

There are two sets of parameters in the model:
the channel probabilities {P (f | e)} and the bi-
gram language model probabilities {P (e′ | e)},
where f ranges over the ciphertext vocabulary and

e, e′ range over the plaintext vocabulary. Given
a plaintext bigram language model, the training
objective is to learn P (f | e) that maximize
P ({fn}Nn=1). When formulated like this, one can
directly apply EM to solve the problem (Knight
et al., 2006). However, EM has time complexity
O(N ·V 2e ) and space complexityO(Vf ·Ve), where
Vf , Ve are the sizes of ciphertext and plaintext vo-
cabularies respectively, andN is the number of ci-
pher bigrams. This makes the EM approach un-
able to handle long ciphertexts with large vocabu-
lary size.

An alternative approach is Bayesian decipher-
ment (Ravi and Knight, 2011). We assume that
P (f | e) and P (e′ | e) are drawn from a Dirichet
distribution with hyper-parameters αf,e and αe,e′ ,
that is:

P (f | e) ∼ Dirichlet(αf,e)
P (e | e′) ∼ Dirichlet(αe,e′).

The remainder of the generative story is the
same as the noisy channel model for decipher-
ment. In the next section, we describe how we
learn the hyper parameters of the Dirichlet prior.
Given αf,e and αe,e′ , The joint likelihood of the
complete data and the parameters,

P ({fn, en}Nn=1, {P (f | e)}, {P (e | e′)})
= P ({fn | en}Nn=1, {P (f | e)})
P ({en}Nn=1, P (e | e′))

=
∏
e

Γ
(∑

f αf,e

)
∏
f Γ (αe,f )

∏
f

P (f | e)#(e,f)+αe,f−1

∏
e

Γ
(∑

e′ αe,e′
)∏

e′ Γ
(
αe,e′

) ∏
f

P (e | e′)#(e,e′)+αe,e′−1,

(1)

where #(e, f) and #(e, e′) are the counts of the
translated word pairs and plaintext bigram pairs in
the complete data, and Γ (·) is the Gamma func-
tion. Unlike EM, in Bayesian decipherment, we
no longer search for parameters P (f | e) that
maximize the likelihood of the observed cipher-
text. Instead, we draw samples from posterior dis-
tribution of the plaintext sequences given the ci-
phertext. Under the above Bayesian decipherment
model, it turns out that the probability of a par-
ticular cipher word fj having a value k, given the
current plaintext word ej , and the samples for all

837



the other ciphertext and plaintext words, f−j and
e−j , is:

P (fj = k | ej , f−j , e−j) =
#(k, ej)−j + αej ,k

#(ej)−j +
∑

f αej ,f
.

Where, #(k, ej)−j and #(ej)−j are the counts
of the ciphertext, plaintext word pair and plaintext
word in the samples excluding fj and ej . Simi-
larly, the probability of a plaintext word ej taking a
value l given samples for all other plaintext words,

P (ej = l | e−j) =
#(l, ej−1)−j + αl,ej−1

#(ej−1)−j +
∑

e αe,ej−1
.

(2)
Since we have large amounts of plaintext data,

we can train a high-quality dependency-bigram
language model, PLM (e | e′) and use it to guide
our samples and learn a better posterior distribu-
tion. For that, we define αe,e′ = αPLM (e | e′),
and set α to be very high. The probability of a
plaintext word (Equation 2) is now

P (ej = l | e−j) ≈ PLM (l | ej−1). (3)

To sample from the posterior, we iterate over the
observed ciphertext bigram tokens and use equa-
tions 2 and 3 to sample a plaintext token with prob-
ability

P (ej | e−j , f) ∝ PLM (ej | ej−1)
PLM (ej+1 | ej)P (fj | ej , f−j , e−j). (4)

In previous work (Dou and Knight, 2012), the
authors use symmetric priors over the channel
probabilities, where αe,f = α 1Vf , and they set α to
1. Symmetric priors over word translation prob-
abilities are a poor choice, as one would not a-
priori expect plaintext words and ciphertext words
to cooccur with equal frequency. Bayesian infer-
ence is a powerful framework that allows us to
inject useful prior information into the sampling
process, a feature that we would like to use. In the
next section, we will describe how we model and
learn better priors using distributional properties
of words. In subsequent sections, we show signif-
icant improvements over the baseline by learning
better priors.

3 Base Distribution with Cross-Lingual
Word Similarities

As shown in the previous section, the base dis-
tribution in Bayesian decipherment is given inde-
pendent of the inference process. A better base
distribution can improve decipherment accuracy.
Ideally, we should assign higher base distribution
probabilities to word pairs that are similar.

One straightforward way is to consider ortho-
graphic similarities. This works for closely related
languages, e.g., the English word “new” is trans-
lated as “neu” in German and “nueva” in Span-
ish. However, this fails when two languages are
not closely related, e.g., Chinese/English. Previ-
ous work aims to discover translations from com-
parable data based on word context similarities.
This is based on the assumption that words appear-
ing in similar contexts have similar meanings. The
approach straightforwardly discovers monolingual
synonyms. However, when it comes to finding
translations, one challenge is to draw a mapping
between the different context spaces of the two
languages. In previous work, the mapping is usu-
ally learned from a seed lexicon.

There has been much recent work in learn-
ing distributional vectors (embeddings) for words.
The most popular approaches are the skip-gram
and continuous-bag-of-words models (Mikolov et
al., 2013a). In Mikolov et al. (2013b), the au-
thors are able to successfully learn word trans-
lations using linear transformations between the
source and target word vector-spaces. However,
unlike our learning setting, their approach re-
lied on large amounts of translation pairs learned
from parallel data to train their linear transforma-
tions. Inspired by these approaches, we aim to ex-
ploit high-quality monolingual word embeddings
to help learn better posterior distributions in unsu-
pervised decipherment, without any parallel data.

In the previous section, we incorporated our
pre-trained language model in αe,e′ to steer our
sampling. In the same vein, we model αe,f us-
ing pre-trained word embeddings, enabling us to
improve our estimate of the posterior distribution.
In Mimno and McCallum (2012), the authors de-
velop topic models where the base distribution
over topics is a log-linear model of observed docu-
ment features, which permits learning better priors
over topic distributions for each document. Sim-
ilarly, we introduce a latent cross-lingual linear
mapping M and define:

838



αf,e = exp{vTeMvf}, (5)
where ve and vf are the pre-trained plaintext

word and ciphertext word embeddings. M is
the similarity matrix between the two embedding
spaces. αf,e can be thought of as the affinity of a
plaintext word to be mapped to a ciphertext word.
Rewriting the channel part of the joint likelihood
in equation 1,

P ({fn | en}Nn=1, {P (f | e)})

=
∏
e

Γ
(∑

f exp{vTeMvf}
)

∏
f Γ (exp{vTeMvf})∏

f

P (f | e)#(e,f)+exp{vTe Mvf}−1

Integrating out the channel probabilities, the
complete data log-likelihood of the observed ci-
phertext bigrams and the sampled plaintext bi-
grams,

P ({fn | en})

=
∏
e

Γ
(∑

f exp{vTeMvf}
)

∏
f Γ (exp{vTeMvf})∏

e

∏
f Γ
(
exp{vTeMvf}+ #(e, f)

)
Γ
(∑

f exp{vTeMvf}+ #(e)
) .

We also add a L2 regularization penalty on the
elements of M . The derivative of logP ({fn |
en} − λ2

∑
i,jM

2
i,j , where λ is the regularization

weight, with respect to M ,

∂ logP ({fn | en} − λ2
∑

i,jM
2
i,j

∂M

=
∑
e

∑
f

exp{vTeMvf}vevTf
(

Ψ

∑
f ′

exp{vTeMvf ′}
−

Ψ

∑
f ′

exp{vTeMvf ′}+ #(e)
+

+ Ψ
(
exp{vTeMvf}+ #(e, f)

)−
Ψ
(
exp{vTeMvf}

)− λM,
where we use

∂ exp{vTeMvf}
∂M

= exp{vTeMvf}
∂vTeMvf
∂M

= exp{vTeMvf}vevTf .
Ψ (·) is the Digamma function, the derivative of
log Γ (·). Again, following Mimno and McCal-
lum (2012), we train the similarity matrix M with
stochastic EM. In the E-step, we sample plaintext
words for the observed ciphertext using equation 4
and in the M-step, we learn M that maximizes
logP ({fn | en}) with stochastic gradient descent.
The time complexity of computing the gradient
is O(VeVf ). However, significant speedups can
be achieved by precomputing vevTf and exploiting
GPUs for Matrix operations.

After learning M , we can set

αe,f =
∑
f ′

exp{vTeMvf ′}
exp{vTeMvf}∑
f ′′ exp{vTeMvf ′′}

= αeme,f , (6)

where αe =
∑

f ′ exp{vTeMvf ′} is the concentra-
tion parameter andme,f =

exp{vTe Mvf}∑
f ′′ exp{vTe Mvf ′′} is an

element of the base measure me for plaintext word
e. In practice, we find that αe can be very large,
overwhelming the counts from sampling when we
only have a few ciphertext bigrams. Therefore, we
use me and set αe proportional to the data size.

4 Deciphering Spanish Gigaword

In this section, we describe our data and exper-
imental conditions for deciphering Spanish into
English.

4.1 Data
In our Spanish/English decipherment experiments,
we use half of the Gigaword corpus as monolin-
gual data, and a small amount of parallel data from
Europarl for evaluation. We keep only the 10k
most frequent word types for both languages and
replace all other word types with “UNK”. We also
exclude sentences longer than 40 tokens, which
significantly slow down our parser. After pre-
processing, the size of data for each language is
shown in Table 1. While we use all the mono-
lingual data shown in Table 1 to learn word em-
beddings, we only parse the AFP (Agence France-
Presse) section of the Gigaword corpus to extract

839



Spanish English

Training
992 million 940 million
(Gigaword) (Gigaword)

Evaluation
1.1 million 1.0 million
(Europarl) (Europarl)

Table 1: Size of data in tokens used in Span-
ish/English decipherment experiment

cipher dependency bigrams and build a plaintext
language model. We also use GIZA (Och and Ney,
2003) to align Europarl parallel data to build a dic-
tionary for evaluating our decipherment.

4.2 Systems

We implement a baseline system based on the
work described in Dou and Knight (2013). The
baseline system carries out decipherment on de-
pendency bigrams. Therefore, we use the Bohnet
parser (Bohnet, 2010) to parse the AFP section of
both Spanish and English versions of the Giga-
word corpus. Since not all dependency relations
are shared across the two languages, we do not ex-
tract all dependency bigrams. Instead, we only use
bigrams with dependency relations from the fol-
lowing list:
• Verb / Subject
• Verb / Object
• Preposition / Object
• Noun / Noun-Modifier
We denote the system that uses our new method

as DMRE (Dirichlet Multinomial Regression with
Embedings). The system is the same as the base-
line except that it uses a base distribution derived
from word embeddings similarities. Word embed-
dings are learned using word2vec (Mikolov et al.,
2013a).

For all the systems, language models are built
using the SRILM toolkit (Stolcke, 2002). We use
the modified Kneser-Ney (Kneser and Ney, 1995)
algorithm for smoothing.

4.3 Sampling Procedure

Motivated by the previous work, we use multiple
random restarts and an iterative sampling process
to improve decipherment (Dou and Knight, 2012).
As shown in Figure 1, we start a few sampling pro-
cesses each with a different random sample. Then
results from different runs are combined to initi-
ate the next sampling iteration. The details of the
sampling procedure are listed below:

Figure 1: Iterative sampling procedures

1. Extract dependency bigrams from parsing
outputs and collect their counts.

2. Keep bigrams whose counts are greater than
a threshold t. Then start N different ran-
domly seeded and initialized sampling pro-
cesses. Perform sampling.

3. At the end of sampling, extract word transla-
tion pairs (f, e) from the final sample. Es-
timate translation probabilities P (e|f) for
each pair. Then construct a translation ta-
ble by keeping translation pairs (f, e) seen
in more than one decipherment and use the
average P (e|f) as the new translation proba-
bility.

4. Start N different sampling processes again.
Initialize the first samples with the transla-
tion pairs obtained from the previous step
(for each dependency bigram f1, f2, find an
English sequence e1, e2, whose P (e1|f1) ·
P (e2|f2) · P (e1, e2)is the highest). Initial-
ize similarity matrix M with one learned by
previous sampling process whose posterior
probability is highest. Go to the third step,
repeat until it converges.

5. Lower the threshold t to include more bi-
grams into the sampling process. Go to the
second step, and repeat until t = 1.

840



The sampling process consists of sampling and
learning of similarity matrix M . The sampling
process creates training examples for learning M ,
and the new M is used to update the base distri-
bution for sampling. In our Spanish/English de-
cipherment experiments, we use 10 different ran-
dom starts. As pointed out in section 3, setting
αe to it’s theoretical value (equation 6) gives poor
results as it can be quite large. In experiments,
we set αe to a small value for the smaller data
sets and increase it as more ciphtertext becomes
available. We find that using the learned base dis-
tribution always improves decipherment accuracy,
however, certain ranges are better for a given data
size. We use αe values of 1, 2, and 5 for cipher-
texts with 100k, 1 million, and 10 million tokens
respectively. We leave automatic learning of αe
for future work.

5 Deciphering Malagasy

Despite spoken in Africa, Malagasy has its root
in Asia, and belongs to the Malayo-Polynesian
branch of the Austronesian language family.
Malagasy and English have very different word
order (VOS versus SVO). Generally, Malagasy is
a typical head-initial language: Determiners pre-
cede nouns, while other modifiers and relative
clauses follow nouns (e.g. ny “the” ankizilahy
“boy” kely “little”). The significant differences in
word order pose great challenges for both parsing
and decipherment.

5.1 Data

Table 2 lists the sizes of monolingual and parallel
data used in this experiment, released by Dou et al.
(2014). The monolingual data in Malagasy con-
tains news text collected from Madagascar web-
sites. The English monolingual data contains Gi-
gaword and an additional 300 million tokens of
African news. Parallel data (used for evaluation) is
collected from GlobalVoices, a multilingual news
website, where volunteers translate news into dif-
ferent languages.

5.2 Systems

The baseline system is the same as the base-
line used in Spanish/English decipherment exper-
iments. We use data provided in previous work
(Dou et al., 2014) to build a Malagasy depen-
dency parser. For English, we use the Turbo
parser, trained on the Penn Treebank (Martins et

Malagasy English

Training
16 million 1.2 billion

(Web)
(Gigaword
and Web)

Evaluation
2.0 million 1.8 million

(GlobalVoices) (GlobalVoices)

Table 2: Size of data in tokens used in Mala-
gasy/English decipherment experiment. Glob-
alVoices is a parallel corpus.

al., 2013).
Because the Malagasy parser does not predict

dependency relation types, we use the following
head-child part-of-speech (POS) tag patterns to se-
lect a subset of dependency bigrams for decipher-
ment:
• Verb / Noun
• Verb / Proper Noun
• Verb / Personal Pronoun
• Preposition / Noun
• Preposision / Proper Noun
• Noun / Adjective
• Noun / Determiner
• Noun / Verb Particle
• Noun / Verb Noun
• Noun / Cardinal
• Noun / Noun

5.3 Sampling Procedure

We use the same sampling protocol designed for
Spanish/English decipherment. We double the
number of random starts to 20. Further more,
compared with Spanish/English decipherment, we
find the base distribution plays a more important
role in achieving higher decipherment accuracy
for Malagasy/English. Therefore, we set αe to 10,
50, and 200 when deciphering 100k, 1 million, and
20 million token ciphtertexts, respectively.

6 Results

In this section, we first compare decipherment ac-
curacy of the baseline with our new approach.
Then, we evaluate the quality of the base distri-
bution through visualization.

We use top-5 type accuracy as our evaluation
metric for decipherment. Given a word type f
in Spanish, we find top-5 translation pairs (f, e)
ranked by P (e|f) from the learned decipherent
translation table. If any pair (f, e) can also be
found in a gold translation lexicon Tgold, we treat

841



Spanish/English Malagasy/English
Top 5k 10k 5k 10k

System Baseline DMRE Baseline DMRE Baseline DMRE Baseline DMRE
100k 1.9 12.4 1.1 7.1 1.2 2.7 0.6 1.4

1 million 7.3 37.7 4.2 23.6 2.5 5.8 1.3 3.2
10 million 29.0 64.7 23.4 43.7 5.4 11.2 3.0 6.9
100 million 45.8 67.4 39.4 58.1 N/A N/A N/A N/A

Table 3: Spanish/English, Malagasy/English decipherment top-5 accuracy (%) of 5k and 10k most fre-
quent word types

the word type f as correctly deciphered. Let |C|
be the number of word types correctly deciphered,
and |V | be the total number of word types evalu-
ated. We define type accuracy as |C||V | .

To create Tgold, we use GIZA to align a small
amount of Spanish/English parallel text (1 mil-
lion tokens for each language), and use the lexi-
con derived from the alignment as our gold trans-
lation lexicon. Tgold contains a subset of 4233
word types in the 5k most frequent word types,
and 7479 word types in the top 10k frequent word
types. We decipher the 10k most frequent Span-
ish word types to the 10k most frequent English
word types, and evaluate decipherment accuracy
on both the 5k most frequent word types as well
as the full 10k word types.

We evaluate accuracy for the 5k and 10k most
frequent word types for each language pair, and
present them in Table 3.

Figure 2: Learning curves of top-5 accuracy eval-
uated on 5k most frequent word types for Span-
ish/English decipherment.

We also present the learning curves of de-
cipherment accuracy for the 5k most frequent
word types. Figure 2 compares the baseline with

DMRE in deciphering Spanish into English. Per-
formance of the baseline is in line with previous
work (Dou and Knight, 2013). (The accuracy re-
ported here is higher as we evaluate top-5 accu-
racy for each word type.) With 100k tokens of
Spanish text, the baseline achieves 1.9% accuracy,
while DMRE reaches 12.4% accuracy, improving
the baseline by over 6 times. Although the gains
attenuate as we increase the number of ciphertext
tokens, they are still large. With 100 million ci-
pher tokens, the baseline achieves 45.8% accuracy,
while DMRE reaches 67.4% accuracy.

Figure 3: Learning curves of top-5 accuracy eval-
uated on 5k most frequent word types for Mala-
gasy/English decipherment.

Figure 3 compares the baseline with our new
approach in deciphering Malagasy into English.
With 100k tokens of data, the baseline achieves
1.2% accuracy, and DMRE improves it to 2.4%.
We observe consistent improvement throughout
the experiment. In the end, the baseline accuracy
obtains 5.8% accuracy, and DMRE improves it to
11.2%.

Low accuracy in Malagasy-English decipher-
ment is attributed to the following factors: First,

842



compared with the Spanish parser, the Mala-
gasy parser has lower parsing accuracy. Second,
word alignment between Malagasy and English is
more challenging, producing less correct transla-
tion pairs. Last but not least, the domain of the
English language model is much closer to the do-
main of the Spanish monolingual text compared
with that of Malagasy.

Overall, we achieve large consistent gains
across both language pairs. We hypothesize the
gain comes from a better base distribution that
considers larger context information. This helps
prevent the language model driving deicpherment
to a wrong direction.

Since our learned transformation matrix M sig-
nificantly improves decipherment accuracy, it’s
likely that it is translation preserving, that is,
plaintext words are transformed from their native
vector space to points in the ciphertext such that
translations are close to each other. To visualize
this effect, we take the 5k most frequent plaintext
words and transform them into new embeddings
in the ciphertext embedding space ve′ = vTeM ,
where M is learned from 10 million Spanish bi-
gram data. We then project the 5k most fre-
quent ciphertext words and the projected plain-
text words from the joint embedding space into a
2−dimensional space using t-sne (?).

In Figure 4, we see an instance of a recur-
ring phenomenon, where translation pairs are very
close and sometimes even overlap each other, for
example (judge, jueces), (secret, secretos). The
word “magistrado” does not appear in our evalu-
ation set. However, it is placed close to its possi-
ble translations. Thus, our approach is capable of
learning word translations that cannot be discov-
ered from limited parallel data.

We often also see translation clusters, where
translations of groups of words are close to each
other. For example, in Figure 5, we can see that
time expressions in Spanish are quite close to their
translations in English. Although better quality
translation visualizations (Mikolov et al., 2013b)
have been presented in previous work, they exploit
large amounts of parallel data to learn the mapping
between source and target words, while our trans-
formation is learned on non-parallel data.

These results show that our approach can
achieve high decipherment accuracy and discover
novel word translations from non-parallel data.

Figure 4: Translation pairs are often close and
sometimes overlap each other. Words in spanish
have been appended with spanish

Figure 5: Semantic groups of word-translations
appear close to each other.

7 Conclusion and Future Work

We proposed a new framework that simultane-
ously performs decipherment and learns a cross-
lingual mapping of word embeddings. Our
method is both theoretically appealing and prac-
tically powerful. The mapping is used to give de-
cipherment a better base distribution.

Experimental results show that our new algo-
rithm improved state-of-the-art decipherment ac-
curacy significantly: from 45.8% to 67.4% for
Spanish/English, and 5.1% to 11.2% for Mala-
gasy/English. This improvement could lead to fur-
ther advances in using monolingual data to im-
prove end-to-end MT.

In the future, we will work on making the our
approach scale to much larger vocabulary sizes us-
ing noise contrastive estimation (?), and apply it to
improve MT systems.

843



Acknowledgments

This work was supported by ARL/ARO
(W911NF-10-1-0533) and DARPA (HR0011-12-
C-0014).

References
Shane Bergsma and Benjamin Van Durme. 2011.

Learning bilingual lexicons using the visual similar-
ity of labeled web images. In Proceedings of the
Twenty-Second international joint conference on Ar-
tificial Intelligence - Volume Volume Three. AAAI
Press.

Bernd Bohnet. 2010. Top accuracy and fast depen-
dency parsing is not a contradiction. In Proceedings
of the 23rd International Conference on Computa-
tional Linguistics. Coling.

Hal Daumé, III and Jagadeesh Jagarlamudi. 2011. Do-
main adaptation for machine translation by mining
unseen words. In Proceedings of the 49th Annual
Meeting of the Association for Computational Lin-
guistics: Human Language Technologies. Associa-
tion for Computational Linguistics.

Qing Dou and Kevin Knight. 2012. Large scale deci-
pherment for out-of-domain machine translation. In
Proceedings of the 2012 Joint Conference on Empir-
ical Methods in Natural Language Processing and
Computational Natural Language Learning. Asso-
ciation for Computational Linguistics.

Qing Dou and Kevin Knight. 2013. Dependency-
based decipherment for resource-limited machine
translation. In Proceedings of the 2013 Conference
on Empirical Methods in Natural Language Pro-
cessing. Association for Computational Linguistics.

Qing Dou, Ashish Vaswani, and Kevin Knight. 2014.
Beyond parallel data: Joint word alignment and de-
cipherment improves machine translation. In Pro-
ceedings of the 2014 Conference on Empirical Meth-
ods in Natural Language Processing (EMNLP). As-
sociation for Computational Linguistics.

Nikesh Garera, Chris Callison-Burch, and David
Yarowsky. 2009. Improving translation lexicon in-
duction from monolingual corpora via dependency
contexts and part-of-speech equivalences. In Pro-
ceedings of the Thirteenth Conference on Computa-
tional Natural Language Learning. Association for
Computational Linguistics.

Aria Haghighi, Percy Liang, Taylor Berg-Kirkpatrick,
and Dan Klein. 2008. Learning bilingual lexicons
from monolingual corpora. In Proceedings of ACL-
08: HLT. Association for Computational Linguis-
tics.

Ann Irvine and Chris Callison-Burch. 2013a. Com-
bining bilingual and comparable corpora for low
resource machine translation. In Proceedings of

the Eighth Workshop on Statistical Machine Trans-
lation. Association for Computational Linguistics,
August.

Ann Irvine and Chris Callison-Burch. 2013b. Su-
pervised bilingual lexicon induction with multiple
monolingual signals. In Proceedings of the 2013
Conference of the North American Chapter of the
Association for Computational Linguistics: Human
Language Technologies. Association for Computa-
tional Linguistics.

Ann Irvine, Chris Quirk, and Hal Daume III. 2013.
Monolingual marginal matching for translation
model adaptation. In Proceedings of the Conference
on Empirical Methods in Natural Language Pro-
cessing (EMNLP). Association for Computational
Linguistics.

Alexandre Klementiev, Ivan Titov, and Binod Bhat-
tarai. 2012. Inducing crosslingual distributed rep-
resentations of words. In Proceedings of COLING
2012. The COLING 2012 Organizing Committee.

Reinhard Kneser and Hermann Ney. 1995. Improved
backing-off for m-gram language modeling. IEEE
International Conference on Acoustics, Speech, and
Signal Processing (ICASSP).

Kevin Knight, Anish Nair, Nishit Rathod, and Kenji
Yamada. 2006. Unsupervised analysis for deci-
pherment problems. In Proceedings of the COL-
ING/ACL 2006 Main Conference Poster Sessions.
Association for Computational Linguistics.

Andre Martins, Miguel Almeida, and Noah A. Smith.
2013. Turning on the turbo: Fast third-order non-
projective turbo parsers. In Proceedings of the 51st
Annual Meeting of the Association for Computa-
tional Linguistics (Volume 2: Short Papers). Asso-
ciation for Computational Linguistics.

Tomas Mikolov, Kai Chen, Greg Corrado, and Jef-
frey Dean. 2013a. Efficient estimation of word
representations in vector space. arXiv preprint
arXiv:1301.3781.

Tomas Mikolov, Quoc V Le, and Ilya Sutskever.
2013b. Exploiting similarities among lan-
guages for machine translation. arXiv preprint
arXiv:1309.4168.

David Mimno and Andrew McCallum. 2012. Topic
models conditioned on arbitrary features with
dirichlet-multinomial regression. arXiv preprint
arXiv:1206.3278.

Malte Nuhn, Arne Mauser, and Hermann Ney. 2012.
Deciphering foreign language by combining lan-
guage models and context vectors. In Proceedings
of the 50th Annual Meeting of the Association for
Computational Linguistics: Long Papers - Volume
1. Association for Computational Linguistics.

Franz Josef Och and Hermann Ney. 2003. A sys-
tematic comparison of various statistical alignment
models. Computational Linguistics.

844



Reinhard Rapp. 1995. Identifying word translations in
non-parallel texts. In Proceedings of the 33rd an-
nual meeting on Association for Computational Lin-
guistics. Association for Computational Linguistics.

Sujith Ravi and Kevin Knight. 2011. Deciphering for-
eign language. In Proceedings of the 49th Annual
Meeting of the Association for Computational Lin-
guistics: Human Language Technologies. Associa-
tion for Computational Linguistics.

Sujith Ravi. 2013. Scalable decipherment for machine
translation via hash sampling. In Proceedings of the
51th Annual Meeting of the Association for Compu-
tational Linguistics. Association for Computational
Linguistics.

Andreas Stolcke. 2002. SRILM - an extensible lan-
guage modeling toolkit. In Proceedings of the Inter-
national Conference on Spoken Language Process-
ing.

Warren Weaver, 1955. Translation (1949). Reproduced
in W.N. Locke, A.D. Booth (eds.). MIT Press.

845


