



















































Multi-grained Named Entity Recognition


Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 1430–1440
Florence, Italy, July 28 - August 2, 2019. c©2019 Association for Computational Linguistics

1430

Multi-Grained Named Entity Recognition

Congying Xia1,5, Chenwei Zhang1, Tao Yang2, Yaliang Li3∗,
Nan Du2, Xian Wu2, Wei Fan2, Fenglong Ma4, Philip Yu1,5

1University of Illinois at Chicago, Chicago, IL, USA
2Tencent Medical AI Lab, Palo Alto, CA, USA; 3Alibaba Group, Bellevue, WA, USA

4University at Buffalo, Buffalo, NY, USA; 5Zhejiang Lab, Hangzhou, China
{cxia8,czhang99,psyu}@uic.edu; yaliang.li@alibaba-inc.com

{tytaoyang,kevinxwu,davidwfan}@tencent.com
nandu2048@gmail.com; fenglong@buffalo.edu

Abstract
This paper presents a novel framework,
MGNER, for Multi-Grained Named Entity
Recognition where multiple entities or en-
tity mentions in a sentence could be non-
overlapping or totally nested. Different from
traditional approaches regarding NER as a se-
quential labeling task and annotate entities
consecutively, MGNER detects and recog-
nizes entities on multiple granularities: it is
able to recognize named entities without ex-
plicitly assuming non-overlapping or totally
nested structures. MGNER consists of a De-
tector that examines all possible word seg-
ments and a Classifier that categorizes enti-
ties. In addition, contextual information and a
self-attention mechanism are utilized through-
out the framework to improve the NER per-
formance. Experimental results show that
MGNER outperforms current state-of-the-art
baselines up to 4.4% in terms of the F1 score
among nested/non-overlapping NER tasks.

1 Introduction

Effectively identifying meaningful entities or en-
tity mentions from the raw text plays a crucial part
in understanding the semantic meanings of natu-
ral language. Such a process is usually known
as Named Entity Recognition (NER) and it is one
of the fundamental tasks in natural language pro-
cessing (NLP). A typical NER system takes an
utterance as the input and outputs identified enti-
ties, such as person names, locations, and organi-
zations. The extracted named entities can benefit
various subsequent NLP tasks, including syntac-
tic parsing (Koo and Collins, 2010), question an-
swering (Krishnamurthy and Mitchell, 2015) and
relation extraction (Lao and Cohen, 2010). How-
ever, accurately recognizing representative entities
in natural language remains challenging.

∗Work was done when the author Yaliang Li was at Ten-
cent America.

Previous works treat NER as a sequence label-
ing problem. For example, Lample et al. (2016)
achieve a decent performance on NER by incor-
porating deep recurrent neural networks (RNNs)
with conditional random field (CRF) (Lafferty
et al., 2001). However, a critical problem that
arises by treating NER as a sequence labeling task
is that it only recognizes non-overlapping entities
in a single, sequential scan on the raw text; it fails
to detect nested named entities which are embed-
ded in longer entity mentions, as illustrated in Fig-
ure 1.

Facility

Last night , at the Chinese embassy in France , 
there was a holiday atmosphere .

GPEGPE

Figure 1: An example from the ACE-2004 dataset
(Doddington et al., 2004) in which two GPEs (Geo-
graphical Entities) are nested in a Facility Entity.

Due to the semantic structures within natural
language, nested entities can be ubiquitous: e.g.
47% of the entities in the test split of ACE-2004
(Doddington et al., 2004) dataset overlap with
other entities, and 42% of the sentences contain
nested entities. Various approaches (Alex et al.,
2007; Lu and Roth, 2015; Katiyar and Cardie,
2018; Muis and Lu, 2017; Wang and Lu, 2018)
have been proposed in the past decade to extract
nested named entities. However, these models are
designed explicitly for recognizing nested named
entities. They usually do not perform well on non-
overlapping named entity recognition compared to
sequence labeling models.

To tackle the aforementioned drawbacks,
we propose a novel neural framework, named
MGNER, for Multi-Grained Named Entity
Recognition. It is suitable for tackling both
Nested NER and Non-overlapping NER. The idea



1431

of MGNER is natural and intuitive, which is to
first detect entity positions in various granularities
via a Detector and then classify these entities into
different pre-defined categories via a Classifier.
MGNER has five types of modules: Word
Processor, Sentence Processor, Entity Processor,
Detection Network, and Classification Network,
where each module can adopt a wide range of
neural network designs.

In summary, the contributions of this work are:

• We propose a novel neural framework named
MGNER for Multi-Grained Named Entity
Recognition, aiming to detect both nested and
non-overlapping named entities effectively in a
single model.

• MGNER is highly modularized. Each module
in MGNER can adopt a wide range of neu-
ral network designs. Moreover, MGNER can
be easily extended to many other related in-
formation extraction tasks, such as chunking
(Ramshaw and Marcus, 1999) and slot filling
(Mesnil et al., 2015).

• Experimental results show that MGNER is
able to achieve new state-of-the-art results on
both Nested Named Entity Recognition tasks
and Non-overlapping Named Entity Recogni-
tion tasks.

2 Related Work

Existing approaches for recognizing non-
overlapping named entities usually treat the
NER task as a sequence labeling problem. Var-
ious sequence labeling models achieve decent
performance on NER, including probabilistic
graph models such as Conditional Random Fields
(CRF) (Ratinov and Roth, 2009), and deep neural
networks like recurrent neural networks or con-
volutional neural networks (CNN). Hammerton
(2003) is the first work to use Long Short-Term
Memory (LSTM) for NER. Collobert et al. (2011)
employ a CNN-CRF structure, which obtains
competitive results to statistical models. Most re-
cent works leverage an LSTM-CRF architecture.
Huang et al. (2015) use hand-crafted spelling
features; Ma and Hovy (2016) and Chiu and
Nichols (2016) utilize a character CNN to repre-
sent spelling characteristics; Lample et al. (2016)
employ a character LSTM instead. Moreover, the
attention mechanism is also introduced in NER to
dynamically decide how much information to use

from a word or character level component (Rei
et al., 2016).

External resources have been used to further im-
prove the NER performance. Peters et al. (2017)
add pre-trained context embeddings from bidi-
rectional language models to NER. Peters et al.
(2018) learn a linear combination of internal hid-
den states stacked in a deep bidirectional lan-
guage model, ELMo, to utilize both higher-level
states which capture context-dependent aspects
and lower-level states which model aspects of syn-
tax. These sequence labeling models can only
detect non-overlapping entities and fail to detect
nested ones.

Various approaches have been proposed for
Nested Named Entity Recognition. Finkel and
Manning (2009) propose a CRF-based con-
stituency parser which takes each named entity as
a constituent in the parsing tree. Ju et al. (2018)
dynamically stack multiple flat NER layers and
extract outer entities based on the inner ones. Such
model may suffer from the error propagation prob-
lem if shorter entities are recognized incorrectly.

Another series of approaches for Nested NER
are based on hypergraphs. The idea of using hy-
pergraph is first introduced in Lu and Roth (2015),
which allows edges to be connected to differ-
ent types of nodes to represent nested entities.
Muis and Lu (2017) use a multigraph represen-
tation and introduce the notion of mention sep-
arator for nested entity detection. Both Lu and
Roth (2015) and Muis and Lu (2017) rely on the
hand-crafted features to extract nested entities and
suffer from structural ambiguity issue. Wang and
Lu (2018) present a neural segmental hypergraph
model using neural networks to obtain distributed
feature representation. Katiyar and Cardie (2018)
also adopt a hypergraph-based formulation and
learn the structure using an LSTM network in a
greedy manner. One issue of these hypergraph
approaches is the spurious structures of hyper-
graphs as they enumerate combinations of nodes,
types and boundaries to represent entities. In other
words, these models are specially designed for the
nested named entities and are not suitable for the
non-overlapping named entity recognition.

Xu et al. (2017) propose a local detection
method which relies on a Fixed-size Ordinally
Forgetting Encoding (FOFE) method to encode ut-
terance and a simple feed-forward neural network
to either reject or predict the entity label for each



1432

Context Representation

Context

 Self-Attention

Attentive Context

Word
Processor

Fully Connected 

ELMo

Word LSTM

Entity Representation

Hidden States

Entity LSTM

Hidden States

Sentence LSTM

Word
Processor

Char level

Character Embedding

Character LSTM

Word level

Word Emb

Word Representation

Postag Emb

ELMo

Category Probabilities

Char level

Character Embedding

Character LSTM

Word level

Word LSTM

Word Representation

Sentence Representation

Postag Emb

Fully Connected 

Attention LSTM

Context-aware Entity Representation

The Detector

Sentence
Processor

Detection
Network

The Classifier

Word Emb

Entity
Processor

Classification
Network

Entity Probabilities

Chinese
France

the Chinese embassy in France

Last night , at the Chinese embassy in France …
JJ    NN  , IN DT       JJ         NN     IN   NNP   …

the Chinese embassy in France
DT       JJ         NN     IN   NNP

Output

Input

Figure 2: The framework of MGNER for Multi-Grained Named Entity Recognition. It consists of a Detector and
a Classifier.

individual text fragment (Luan et al., 2018; Lee
et al., 2017; He et al., 2018). Their model is in
the same track with the framework we proposed
whereas the difference is that we separate the NER
task into two stages, i.e., detecting entity positions
and classifying entity categories.

3 The Proposed Framework

An overview of the proposed MGNER framework
for multi-grained entity recognition, is illustrated
in Figure 2. Specifically, MGNER consists of
two sub-networks: the Detector and the Classi-
fier. The Detector detects all the possible entity
positions while the Classifier aims at classifying
detected entities into pre-defined entity categories.
The Detector has three modules: 1) Word Proces-
sor which extracts word-level semantic features,
2) Sentence Processor that learns context informa-
tion for each utterance and 3) Detection Network
that decides whether a word segment is an entity
or not. The Classifier consists of 1) Word Proces-
sor which has the same structure as the one in the
Detector, 2) Entity Processor that obtains entity
features and 3) Classification Network that classi-
fies entity into pre-defined categories. In addition,
a self-attention mechanism is adopted in the En-

tity Processor to help the model capture and utilize
entity-related contextual information.

Each module in MGNER can be replaced with
a wide range of different neural network designs.
For example, BERT (Devlin et al., 2018) can be
used as the Word Processor and a capsule model
(Sabour et al., 2017; Xia et al., 2018) can be inte-
grated into the Classification Network.

It is worth mentioning that in order to improve
the learning speed as well as the performance
of MGNER, the Detector and the Classifier are
trained with a series of shared input features, in-
cluding the pre-trained word embeddings and the
pre-trained language model features. Sentence-
level semantic features trained in the Detector are
also transferred into the Classifier to introduce and
utilize the contextual information. We present the
key building blocks and the properties of the De-
tector in Section 3.1 and the Classifier in Section
3.2, respectively.

3.1 The Detector
The Detector is aimed at detecting possible en-
tity positions within each utterance. It takes an
utterance as the input and outputs a set of entity
candidates. Essentially, we use a semi-supervised
neural network inspired by (Peters et al., 2017)



1433

to model this process. The architecture of the
Detector is illustrated in the left part of Fig-
ure 2. Three major modules are contained in
the Detector: Word Processor, Sentence Proces-
sor and Detection Network. More specifically,
pre-trained word embeddings, POS tag infor-
mation and character-level word information are
used for generating semantically meaningful word
representations. Word representations obtained
from the Word Processor and the language model
embeddings—ELMo (Peters et al., 2018), are con-
catenated together to produce context-aware sen-
tence representations. Each possible word seg-
ment is then examined in the Detection Network
and to be decided whether accepted it as an entity
or not.

3.1.1 Word Processor

Word Processor extracts semantically meaningful
word representation for each token. Given an input
utterance with K tokens (t1, ..., tK), each token
tk(1 ≤ k ≤ K) is represented as

xk = [wk;pk; ck],

by using a concatenation of a pre-trained word em-
bedding wk, POS tag embedding pk if it exists,
and a character-level word information ck. The
pre-trained word embedding wk with a dimension
Dw is obtained from GloVe (Pennington et al.,
2014). The character-level word information ck
is obtained with a bidirectional LSTM (Hochreiter
and Schmidhuber, 1997) layer to capture the mor-
phological information. The hidden size of this
character LSTM is set as Dcl. As shown in the
bottom of Figure 2, character embeddings are fed
into the character LSTM. The final hidden states
from the forward and backward character LSTM
are concatenated as the character-level word infor-
mation ck. Those POS tagging embeddings and
character embeddings are randomly initialized and
learned within the learning process.

3.1.2 Sentence Processor

To learn the contextual information from each sen-
tence, another bidirectional LSTM, named word
LSTM, is applied to sequentially encode the ut-
terance. For each token, the forward hidden states
→
hk and the backward hidden states

←
hk are concate-

nated into the hidden states hk. The dimension of

the hidden states of the word LSTM is set as Dwl.

→
hk = LSTMfw(xk,

←
hk−1),

←
hk = LSTMbw(xk,

←
hk+1),

hk = [
→
hk;

←
hk].

(1)

Besides, we also utilize the language model em-
beddings pre-trained in an unsupervised way as
the ELMo model in (Peters et al., 2018). The pre-
trained ELMo embeddings and the hidden states
in the word LSTM hk are concatenated. Hence,
the concatenated hidden states hk for each token
can be reformulated as:

hk = [
→
hk;

←
hk;ELMok], (2)

where ELMok is the ELMo embeddings for to-
ken tk. Speficially, a three-layer bi-LSTM neural
network is trained as the language model. Since
the lower-level LSTM hidden states have the abil-
ity to model syntax properties and higher-level
LSTM hidden states can capture contextual in-
formation, ELMo computes the language model
embeddings as a weighted combination of all the
bidirectional LSTM hidden states:

ELMok = γ
∑L

l=0
ujh

LM
k,l , (3)

where γ is a task-specified scale parameter which
indicates the importance of the entire ELMo vec-
tor to the NER task. L is the number of layers
used in the pre-trained language model, the vector
u = [u0, · · · , uL] represents softmax-normalized
weights that combine different layers. hLMk,l is the
language model hidden state of layer l at the time
step k.

A sentence bidirectional LSTM layer with a
hidden dimension ofDsl is employed on top of the
concatenated hidden states hk. The forward and
backward hidden states in this sentence LSTM are
concatenated for each token as the final sentence
representation fk ∈ R2Dsl .

3.1.3 Detection Network
Using the semantically meaningful features ob-
tained in fk, we can identify possible entities
within each utterance. The strategy of finding en-
tities is to first generate all the word segments as
entity proposals and then estimate the probability
of each proposal as being an entity or not.

To enumerate all possible entity proposals, dif-
ferent lengths of entity proposals are generated



1434

surrounding each token position. For each to-
ken position, R entity proposals with the length
varies from 1 to the maximum length R are gen-
erated. Specifically, it is assumed that an in-
put utterance consists of a sequence of N tokens
(t1, t2, t3, t4, t5, t6, ..., tN ). To balance the per-
formance and the computational cost, we set R
as 6. We take each token position as the cen-
ter and generate 6 proposals surrounding it. All
the possible 6N proposals under the max-length
of 6 will be generated. As shown in Figure 3,
the entity proposals generated surrounding token
t3 are: (t3), (t3, t4), (t2, t3, t4), (t2, t3, t4, t5),
(t1, t2, t3, t4, t5), (t1, t2, t3, t4, t5, t6). Similar en-
tity proposals are generated for all the token po-
sitions and proposals that contain invalid indexes
like (t0,t1,t2) will be deleted. Hence we can obtain
all the valid entity proposals under the condition
that the max length is R.

Proposal 1: t2t1 t3 t4

Proposal 2: t2t1 t3 t4 t5

Proposal 3: t2t1 t3 t4 t5

Proposal 4: t2t1 t3 t4 t5

Proposal 5: t2t1 t3 t4 t5

Proposal 6: t2t1 t3 t4 t5

t5 t6

t6

t6

t6

t6

t6
Figure 3: All possible entity proposals generated sur-
rounding token t3 when the maximum length of an en-
tity proposal R is set as 6.

For each token, we simultaneously estimate the
probability of a proposal being an entity or not for
R proposals. A fully connected layer with a two-
class softmax function is used to determine the
quality of entity proposals:

sk = softmax (fkWp + bp) , (4)

where Wp ∈ R2Dsl×2R and bp ∈ R2R are
weights and the bias for the entity proposal layer;
sk contains 2R scores including R scores for be-
ing an entity and R scores for not being an entity
at position k. The cross-entropy loss is employed
in the Detector as follows:

Lp = −
∑K

k=1

∑R
r=1

yrk log s
r
k, (5)

where yrk is the label for proposal type r at po-
sition k and srk is the probability of being an en-
tity for proposal type r at position k. It is worth

mentioning that, most entity proposals are nega-
tive proposals. Thus, to balance the influence of
positive proposals and negative proposals in the
loss function, we keep all positive proposals and
use down-sampling for negative proposals when
calculating the loss Lp. For each batch, we fix the
number of the total proposals, including all pos-
itive proposals and sampled negative proposals,
used in the loss function as Nb. In the inference
procedure of the Detection Network, an entity pro-
posal will be recognized as an entity candidate if
its score of being an entity is higher than score of
not being an entity.

3.2 The Classifier

The Classifier module aims at classifying entity
candidates obtained from the Detector into differ-
ent pre-defined entity categories. For the nested
NER task, all the proposed entities will be saved
and fed into the Classifier. For the NER task
which has non-overlapping entities, we utilize
the non-maximum suppression (NMS) algorithm
(Neubeck and Van Gool, 2006) to deal with re-
dundant, overlapping entity proposals and output
real entity candidates. The idea of NMS is sim-
ple but effective: picking the entity proposal with
the maximum probability, deleting conflict entity
proposals, and repeating the previous process un-
til all the proposals are processed. Eventually, we
can get those non-conflict entity candidates as the
input of the Classifier.

To understand the contextual information of
the proposed entity, we utilize both sentence-level
context information and a self-attention mecha-
nism to help the model focus on entity-related con-
text tokens. The framework of the Classifier is
shown in the right part of Figure 2. Essentially, it
consists of three modules: Word Processor, Entity
Processor and Classification Network.

3.2.1 Word Processor
A same Word Processor as in the Detector is used
here to get the word representation for the entity
candidates obtained from the Detector. The word-
level embedding, which is the concatenation of
pre-trained word embedding and POS tag embed-
ding if it is exists, is transferred from the Word
Processor in the Detector to improve the perfor-
mance as well as to speed up the learning process.
The character-level LSTM and character embed-
dings are trained separately in the Detector and the
Classifier.



1435

ACE-2004 ACE-2005 CoNLL-2003
TRAIN DEV TEST TRAIN DEV TEST TRAIN DEV TEST

sentences
#total 6,799 829 879 7,336 958 1,047 14,987 3,466 3,684
#overlaps 2,683(39%) 293(35%) 373(42%) 2,683 (37%) 340(35%) 330 (32%) - - -

entities

#total 22,207 2,511 3,031 24,687 3,217 3,027 23,499 5,942 5,648
#overlaps 10,170 (46%) 1,091(43%) 1,418 (47%) 9,937 (40%) 1,192(37%) 1,184 (39%) - - -
length >6 1,439 (6%) 179(7%) 199 (7%) 1,343 (5%) 148(5%) 160 (6%) 23(0.1%) 8(0.1%) 0 (0%)
max length 57 35 43 49 30 27 10 10 6

Table 1: Corpora Statistics for the ACE-2004, ACE-2005 and CoNLL-2003 datasets.

3.2.2 Entity Processor
The word representation is fed into a bidirectional
word LSTM with hidden size Dwl and the hidden
states are concatenated with the ELMo language
model embeddings as the entity features. A bidi-
rectional LSTM with hidden size Del is applied to
the entity feature to capture sequence information
among the entity words. The last hidden states of
the forward and backward Entity LSTM are con-
catenated as the entity representation e ∈ R2Del .

The same word in different contexts may have
different semantic meanings. To this end, in our
model, we take the contextual information into
consideration when learning the semantic repre-
sentations of entity candidates. We capture the
contextual information from other words in the
same utterance. Denote c as the context feature
vector for these context words, and it can be ex-
tracted from the sentence representation fk in the
Detector. Hence, the sentence features trained in
the Detector is directly transferred to the Classi-
fier.

An easy way to model context words is to con-
catenate all the word representations or average
them. However, this naive approach may fail when
there exists a lot of unrelated context words. To
select high-relevant context words and learn an
accurate contextual representation, we propose a
self-attention mechanism to simulate and dynam-
ically control the relatedness between the context
and the entity. The self-attention module takes the
entity representation e and all the context features
C = [c1, c2, ..., cN] as the inputs, and outputs a
vector of attention weights a:

a = softmax(CWeT ), (6)

where W ∈ R2Dsl×2Del is a weight matrix for
the self-attention layer, and a is the self-attention
weight on different context words. To help the
model focus on entity-related context, the attentive
vector Catt is calculated as the attention-weighted
context:

Catt = a ∗C. (7)

The lengths of the attentive context Catt varies
in different contexts. However, the goal of the
Classification Network is to classify entity candi-
dates into different categories, and thus it requires
a fixed embedding size. We achieve that by adding
another LSTM layer. An Attention LSTM with
the hidden dimension Dml is used and the con-
catenation of the last hidden states in the forward
and backward LSTM layer as the context repre-
sentation m ∈ R2Dml . Hence the shape of the
context representation is aligned. We concatenate
the context representation and the entity represen-
tation together as a context-aware entity represen-
tation to classify entity candidates: o = [m; e].

3.2.3 Classification Network
A two-layer fully connected neural network is
used to classify candidates into pre-defined cate-
gories:

p = softmax (Wc2 (σ (oWc1 + bc1)) + bc2) ,
(8)

where Wc1 ∈ R(2Dml+2Del)×Dh , bc1 ∈ RDh
, Wc2 ∈ RDc1×(Dt+1), bc2 ∈ RDt+1 are the
weights for this fully connected neural network,
and Dt is the number of entity types. Actually,
this classification function classifies entity candi-
dates into (Dt + 1) types. Here we add one more
type as for the scenario that a candidate may not
be a real entity. Finally, the hinge-ranking loss is
adopted in the Classification Network:

Lc =
∑

yw∈Yw
max {0,∆ + pyw − pyr} , (9)

where pw is the probability for the wrong labels
yw, pr is the probability for the right label yr, and
∆ is a margin. The hinge-rank loss urges the prob-
ability for the right label higher than the probabil-
ity for the wrong labels and improves the classifi-
cation performance.

4 Experiments

To show the ability and effectiveness of our pro-
posed framework, MGNER, for Multi-Grained



1436

Named Entity Recognition, we conduct the exper-
iments on both Nested NER task and traditional
non-overlapping NER task.

4.1 Datasets

We mainly evaluate our framework on ACE-2004
and ACE-2005 (Doddington et al., 2004) with the
same splits used by previous works (Luo et al.,
2015; Wang and Lu, 2018) for the nested NER
task. Specifically, seven different types of entities
such as person, facility, weapon and vehicle, are
contained in the ACE datasets. For the traditional
NER task, we use the CoNLL-2003 dataset (Tjong
Kim Sang and De Meulder, 2003) which contains
four types of named entities: location, organiza-
tion, person and miscellaneous. An overview of
these three datasets is illustrated in Table 1. It can
be observed that most entities are less or equal to
6 tokens, and thus we select the maximum entity
length R = 6.

4.2 Implementation Details

We performed random search (Bergstra and Ben-
gio, 2012) for hyper-parameter optimization and
selected the best setting based on performance on
the development set. We employ the Adam opti-
mizer (Kingma and Ba, 2014) with learning rate
decay for all the experiments. The learning rate
is set as 0.001 at the beginning and exponential
decayed by 0.9 after each epoch. The batch size
of utterances is set as 20. In order to balance the
influence of positive proposals and negative pro-
posals, we use down-sampling for negative ones
and the total proposal number Nb for each batch
is 128. To alleviate over-fitting, we add dropout
regularizations after the word representation layer
and all the LSTM layers with a dropout rate of 0.5.
In addition, we employ the early stopping strategy
when there is no performance improvement on the
development dataset after three epochs. The pre-
trained word embeddings are from GloVe (Pen-
nington et al., 2014), and the word embedding
dimension Dw is 300. Besides, the ELMo 5.5B
data1 is utilized in the experiment for the language
model embedding. Moreover, the size of charac-
ter embedding ck is 100, and the hidden size of
the Character LSTM Dcl is also 100. The size of
POS tag embedding pk is 300 for the ACE datas-
sets and no POS tag information is used in the
CoNLL-2003 dataset. The hidden dimensions of

1https://allennlp.org/elmo

the Word LSTM layer Dwl, the Sentence LSTM
layer Dsl, the Entity LSTM layer Del and the At-
tention LSTM layer Dml are all set to 300. The
hidden dimension of the classification layer Dh is
50. The margin ∆ in the hinge-ranking loss for
the entity category classification is set to 5. The
ELMo scale parameter γ used in the Detector is
3.35 and 3.05 in the Classifier, respectively.

MODEL ACE-2004 ACE-2005P R F1 P R F1
Lu and Roth (2015) 70.0 56.9 62.8 66.3 59.2 62.5
Lample et al. (2016) 71.3 50.5 58.3 64.1 52.4 57.6
Muis and Lu (2017) 72.7 58.0 64.5 69.1 58.1 63.1
Xu et al. (2017) 68.2 54.3 60.5 67.4 55.1 60.6
Katiyar and Cardie (2018) 73.6 71.8 72.7 70.6 70.4 70.5
Ju et al. (2018) - - - 74.2 70.3 72.2
Wang et al. (2018) 74.9 71.8 73.3 74.5 71.5 73.0
Wang and Lu (2018) 78.0 72.4 75.1 76.8 72.3 74.5
MGNER w/o context 79.8 76.3 78.0 79.6 75.6 77.5
MGNER w/o attention 81.5 76.5 78.9 79.4 76.0 77.7
MGNER 81.7 77.4 79.5 79.0 77.3 78.2

Table 2: Performance on ACE-2004 and ACE-2005
test set for the Nested NER task.

4.3 Results

Nested NER Task. The proposed MGNER is
very suitable for detecting nested named entities
since every possible entity will be examined and
classified. In order to validate this advantage, we
compare MGNER with numerous baseline mod-
els: 1) Lu and Roth (2015) which propose the
mention hypergraphs for recognizing overlapping
entities; 2) Lample et al. (2016) which adopt the
LSTM-CRF stucture for sequence labelling; 3)
Muis and Lu (2017) which introduce mention sep-
arators to tag gaps between words for recogniz-
ing overlapping mentions; 4) Xu et al. (2017) that
propose a local detection method; 5) Katiyar and
Cardie (2018) which propose a hypergraph-based
model using LSTM for learning feature represen-
tations; 6) Ju et al. (2018) that use a layered model
which extracts outer entities based on inner ones;
7) Wang et al. (2018) which propose a neural
transition-based model that constructs nested men-
tions through a sequence of actions; 8) Wang and
Lu (2018) which adopt a neural segmental hyper-
graph model.

Experiment results of the Nested NER task on
the ACE-2004 and ACE-2005 datasets are re-
ported in Table 2. We can observe from Table
2 that, our proposed framework MGNER out-
performs all the baseline approaches. For both
datasets, our model improves the state-of-the-art



1437

result by around 4% in terms of precision, recall,
as well as the F1 score.

To study the contribution of different modules
in MGNER, we also report the performance of
two ablation variations of the proposed MGNER
at the bottom of Table 2. MGNER w/o attention
is a variation of MGNER which removes the self-
attention mechanism and MGNER w/o context
removes all the context information. To remove
the self-attention mechanism, we feed the context
feature C directly into a bi-directional LSTM to
obtain context representation m, other than the
attentive context vector Catt. As for MGNER
w/o context, we only use entity representation e
to do classification other than the context-aware
entity representation o. By adding the context
information, the F1 score improves 0.9% on the
ACE-2004 dataset and 0.7% on the ACE-2005
dataset. The self-attention mechanism improves
the F1 score by 0.6% on the ACE-2004 dataset and
0.5% on the ACE-2005 dataset.

MODEL OVERLAPPING NON-OVERLAPPINGP R F1 P R F1
Lu and Roth (2015) 68.1 52.6 59.4 64.1 65.1 64.6
Muis and Lu (2017) 70.4 55.0 61.8 67.2 63.4 65.2
Wang et al. (2018) 77.4 70.5 73.8 76.1 69.6 72.7
Wang and Lu (2018) 80.6 73.6 76.9 75.5 71.5 73.4
MGNER 82.6 76.0 79.2 77.8 79.5 78.6

Table 3: Results on different types of sentences (ACE-
2005).

To analyze how well our model performs on
overlapping and non-overlapping entities, we split
the test data into two portions: sentences with
and without overlapping entities (follow the splits
used by Wang and Lu (2018)). Four state-of-the-
art nested NER models are compared with our
proposed framework MGNER on the ACE-2005
dataset. As illustrated in Table 3, MGNER consis-
tently performs better than the baselines on both
portions, especially for the non-overlapping part.
This observation indicates that our model can bet-
ter recognize non-overlapping entities than previ-
ous nested NER models.

The first step in MGNER is to detect entity
positions using the Detector, where the effective-
ness of proposing correct entity candidates im-
mediately affects the performance of the whole
model. To this end, we provide the experiment
results of detecting correct entities in the Detector
module here. The precision, recall and F1 score
are 85.23 , 91.84, 88.41 for the ACE-2004 dataset
and 84.95, 89.35, 87.09 for the ACE-2005 dataset.

MODEL CoNLL-2003DEV TEST
Lu and Roth (2015) 89.2 83.8
Muis and Lu (2017) - 84.3
Xu et al. (2017) - 90.85
Wang and Lu (2018) - 90.2
Lample et al. (2016) - 90.94
Ma and Hovy (2016) 94.74 91.21
Chiu and Nichols (2016) 94.03± 0.23 91.62± 0.33
Peters et al. (2017) - 91.93± 0.19
Peters et al. (2018) - 92.22± 0.10
MGNER w/o context 95.21± 0.12 92.23± 0.06
MGNER w/o attention 95.23± 0.06 92.26± 0.09
MGNER 95.24± 0.13 92.28± 0.12

Table 4: F1 scores on CoNLL-2003 devlopement set
(DEV) and test set (TEST) for the English NER task.
Mean and standard deviation across five runs are re-
ported. Pos tags information are not used.

NER Task. We also evaluate the proposed
MGNER framework on the NER task which
needs to reorganize non-overlapping entities. Two
types of baseline models are compared here:
sequence labelling models which are designed
specifically for non-overlapping NER task and
nested NER models which also provide the ability
to detect non-overlapping mentions. The first type
of models including 1) Lample et al. (2016) which
adopt the LSTM-CRF structure; 2) Ma and Hovy
(2016) which use a LSTM-CNNs-CRF architec-
ture; 3) Chiu and Nichols (2016) which propose
a CNN-LSTM-CRF model; 4) Peters et al. (2017)
which add semi-supervised language model em-
beddings; and 5) Peters et al. (2018) which utilize
the state-of-the-art ELMo language model embed-
dings. The second types include four Nested mod-
els mentioned in the Nested NER section: 1) Luo
et al. (2015); 2) Muis and Lu (2017); 3) Xu et al.
(2017); 4) (Wang and Lu, 2018).

Table 4 shows the F1 scores of different ap-
proaches on CoNLL-2003 devlopement set and
test set for the English NER task. Mean and stan-
dard deviation across five runs are reported. It
can be observed from Table 4 that the proposed
MGNER model outperforms all the baselines.
The models designed for non-overlapping en-
tity detection usually performs better than Nested
NER models for the NER task. Our pro-
posed framework outperforms state-of-the-art re-
sults both on the NER and Nested NER task. Xu
et al. (2017) is the best baseline model among the
Nested models since it shares a similar idea of
our proposed framework by individually examin-



1438

ing each entity proposal. From the ablation study,
we can observe that by purely adding the context
information, the F1 score on the CoNLL-2003 test
set improves from 92.23 to 92.26, and by adding
the attention mechanism, the F1 score improves to
92.28.

We also provide the performance of detecting
non-overlapping entities in the Detector here. The
precision, recall and F1 score are 95.33, 95.69 and
95.51 on the CoNLL-2003 dataset.

5 Conclusions
In this work, we propose a novel neural framework
named MGNER for Multi-Grained Named En-
tity Recognition where multiple entities or entity
mentions in a sentence could be non-overlapping
or totally nested. MGNER is framework with
high modularity and each component in MGNER
can adopt a wide range of neural networks. Ex-
perimental results show that MGNER is able
to achieve state-of-the-art results on both nested
NER task and traditional non-overlapping NER
task.

Acknowledgments

We thank the reviewers for their valuable com-
ments. Special thanks go to Lu Wei from Sin-
gapore University of Technology and Design for
sharing the datasets split details. This work is sup-
ported in part by NSF through grants IIS-1526499,
IIS-1763325, and CNS-1626432.

References
Beatrice Alex, Barry Haddow, and Claire Grover. 2007.

Recognising nested named entities in biomedical
text. In Proceedings of the Workshop on BioNLP
2007: Biological, Translational, and Clinical Lan-
guage Processing, pages 65–72. Association for
Computational Linguistics.

James Bergstra and Yoshua Bengio. 2012. Random
search for hyper-parameter optimization. Journal of
Machine Learning Research, 13(Feb):281–305.

Jason PC Chiu and Eric Nichols. 2016. Named entity
recognition with bidirectional lstm-cnns. Transac-
tions of the Association for Computational Linguis-
tics, 4:357–370.

Ronan Collobert, Jason Weston, Léon Bottou, Michael
Karlen, Koray Kavukcuoglu, and Pavel Kuksa.
2011. Natural language processing (almost) from
scratch. Journal of Machine Learning Research,
12(Aug):2493–2537.

Jacob Devlin, Ming-Wei Chang, Kenton Lee, and
Kristina Toutanova. 2018. Bert: Pre-training of deep
bidirectional transformers for language understand-
ing. arXiv preprint arXiv:1810.04805.

George R Doddington, Alexis Mitchell, Mark A Przy-
bocki, Lance A Ramshaw, Stephanie M Strassel, and
Ralph M Weischedel. 2004. The automatic content
extraction (ace) program-tasks, data, and evaluation.
In LREC, volume 2, page 1.

Jenny Rose Finkel and Christopher D Manning. 2009.
Nested named entity recognition. In Proceedings
of the 2009 Conference on Empirical Methods in
Natural Language Processing: Volume 1-Volume 1,
pages 141–150. Association for Computational Lin-
guistics.

James Hammerton. 2003. Named entity recognition
with long short-term memory. In Proceedings of the
seventh conference on Natural language learning at
HLT-NAACL 2003-Volume 4, pages 172–175. Asso-
ciation for Computational Linguistics.

Luheng He, Kenton Lee, Omer Levy, and Luke Zettle-
moyer. 2018. Jointly predicting predicates and argu-
ments in neural semantic role labeling. In Proceed-
ings of the 56th Annual Meeting of the Association
for Computational Linguistics (Volume 2: Short Pa-
pers), pages 364–369.

Sepp Hochreiter and Jürgen Schmidhuber. 1997.
Long short-term memory. Neural computation,
9(8):1735–1780.

Zhiheng Huang, Wei Xu, and Kai Yu. 2015. Bidirec-
tional lstm-crf models for sequence tagging. arXiv
preprint arXiv:1508.01991.

Meizhi Ju, Makoto Miwa, and Sophia Ananiadou.
2018. A neural layered model for nested named en-
tity recognition. In Proceedings of the 2018 Con-
ference of the North American Chapter of the Asso-
ciation for Computational Linguistics: Human Lan-
guage Technologies, Volume 1 (Long Papers), vol-
ume 1, pages 1446–1459.

Arzoo Katiyar and Claire Cardie. 2018. Nested named
entity recognition revisited. In Proceedings of the
2018 Conference of the North American Chapter of
the Association for Computational Linguistics: Hu-
man Language Technologies, Volume 1 (Long Pa-
pers), volume 1, pages 861–871.

Diederik P Kingma and Jimmy Ba. 2014. Adam: A
method for stochastic optimization. arXiv preprint
arXiv:1412.6980.

Terry Koo and Michael Collins. 2010. Efficient third-
order dependency parsers. In Proceedings of the
48th Annual Meeting of the Association for Com-
putational Linguistics, pages 1–11. Association for
Computational Linguistics.



1439

Jayant Krishnamurthy and Tom M Mitchell. 2015.
Learning a compositional semantics for freebase
with an open predicate vocabulary. Transactions
of the Association for Computational Linguistics,
3:257–270.

John D. Lafferty, Andrew McCallum, and Fernando
C. N. Pereira. 2001. Conditional random fields:
Probabilistic models for segmenting and labeling se-
quence data. In ICML, pages 282–289.

Guillaume Lample, Miguel Ballesteros, Sandeep Sub-
ramanian, Kazuya Kawakami, and Chris Dyer. 2016.
Neural architectures for named entity recognition.
In Proceedings of NAACL-HLT, pages 260–270.

Ni Lao and William W Cohen. 2010. Relational re-
trieval using a combination of path-constrained ran-
dom walks. Machine learning, 81(1):53–67.

Kenton Lee, Luheng He, Mike Lewis, and Luke Zettle-
moyer. 2017. End-to-end neural coreference reso-
lution. In Proceedings of the 2017 Conference on
Empirical Methods in Natural Language Process-
ing, pages 188–197.

Wei Lu and Dan Roth. 2015. Joint mention extrac-
tion and classification with mention hypergraphs.
In Proceedings of the 2015 Conference on Empiri-
cal Methods in Natural Language Processing, pages
857–867.

Yi Luan, Luheng He, Mari Ostendorf, and Hannaneh
Hajishirzi. 2018. Multi-task identification of enti-
ties, relations, and coreference for scientific knowl-
edge graph construction. In Proceedings of the 2018
Conference on Empirical Methods in Natural Lan-
guage Processing, pages 3219–3232.

Gang Luo, Xiaojiang Huang, Chin-Yew Lin, and Za-
iqing Nie. 2015. Joint entity recognition and disam-
biguation. In Proceedings of the 2015 Conference
on Empirical Methods in Natural Language Pro-
cessing, pages 879–888.

Xuezhe Ma and Eduard Hovy. 2016. End-to-end se-
quence labeling via bi-directional lstm-cnns-crf. In
Proceedings of the 54th Annual Meeting of the As-
sociation for Computational Linguistics (Volume 1:
Long Papers), volume 1, pages 1064–1074.

Grégoire Mesnil, Yann Dauphin, Kaisheng Yao,
Yoshua Bengio, Li Deng, Dilek Hakkani-Tur, Xi-
aodong He, Larry Heck, Gokhan Tur, Dong Yu, et al.
2015. Using recurrent neural networks for slot fill-
ing in spoken language understanding. IEEE/ACM
Transactions on Audio, Speech, and Language Pro-
cessing, 23(3):530–539.

Aldrian Obaja Muis and Wei Lu. 2017. Labeling gaps
between words: Recognizing overlapping mentions
with mention separators. In Proceedings of the 2017
Conference on Empirical Methods in Natural Lan-
guage Processing, pages 2608–2618. Association
for Computational Linguistics.

Alexander Neubeck and Luc Van Gool. 2006. Efficient
non-maximum suppression. In Pattern Recognition,
2006. ICPR 2006. 18th International Conference on,
volume 3, pages 850–855. IEEE.

Jeffrey Pennington, Richard Socher, and Christopher
Manning. 2014. Glove: Global vectors for word
representation. In Proceedings of the 2014 confer-
ence on empirical methods in natural language pro-
cessing (EMNLP), pages 1532–1543.

Matthew Peters, Waleed Ammar, Chandra Bhagavat-
ula, and Russell Power. 2017. Semi-supervised se-
quence tagging with bidirectional language models.
In Proceedings of the 55th Annual Meeting of the
Association for Computational Linguistics (Volume
1: Long Papers), pages 1756–1765.

Matthew Peters, Mark Neumann, Mohit Iyyer, Matt
Gardner, Christopher Clark, Kenton Lee, and Luke
Zettlemoyer. 2018. Deep contextualized word rep-
resentations. In Proceedings of the 2018 Confer-
ence of the North American Chapter of the Associ-
ation for Computational Linguistics: Human Lan-
guage Technologies, Volume 1 (Long Papers), pages
2227–2237.

Lance A Ramshaw and Mitchell P Marcus. 1999. Text
chunking using transformation-based learning. In
Natural language processing using very large cor-
pora, pages 157–176. Springer.

Lev Ratinov and Dan Roth. 2009. Design challenges
and misconceptions in named entity recognition. In
Proceedings of the Thirteenth Conference on Com-
putational Natural Language Learning, pages 147–
155. Association for Computational Linguistics.

Marek Rei, Gamal Crichton, and Sampo Pyysalo. 2016.
Attending to characters in neural sequence label-
ing models. In Proceedings of COLING 2016,
the 26th International Conference on Computational
Linguistics: Technical Papers, pages 309–318.

Sara Sabour, Nicholas Frosst, and Geoffrey E Hin-
ton. 2017. Dynamic routing between capsules. In
Advances in neural information processing systems,
pages 3856–3866.

Erik F Tjong Kim Sang and Fien De Meulder.
2003. Introduction to the conll-2003 shared task:
Language-independent named entity recognition. In
Proceedings of the seventh conference on Natural
language learning at HLT-NAACL 2003-Volume 4,
pages 142–147. Association for Computational Lin-
guistics.

Bailin Wang and Wei Lu. 2018. Neural segmental
hypergraphs for overlapping mention recognition.
In Proceedings of the 2018 Conference on Empiri-
cal Methods in Natural Language Processing, pages
204–214.



1440

Bailin Wang, Wei Lu, Yu Wang, and Hongxia Jin.
2018. A neural transition-based model for nested
mention recognition. In Proceedings of the 2018
Conference on Empirical Methods in Natural Lan-
guage Processing, pages 1011–1017.

Congying Xia, Chenwei Zhang, Xiaohui Yan,
Yi Chang, and Philip Yu. 2018. Zero-shot user
intent detection via capsule neural networks. In
Proceedings of the 2018 Conference on Empirical

Methods in Natural Language Processing, pages
3090–3099.

Mingbin Xu, Hui Jiang, and Sedtawut Watcharawit-
tayakul. 2017. A local detection approach for named
entity recognition and mention detection. In Pro-
ceedings of the 55th Annual Meeting of the Associa-
tion for Computational Linguistics (Volume 1: Long
Papers), volume 1, pages 1237–1247.


