



















































Fine Grained Citation Span for References in Wikipedia


Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 1990–1999
Copenhagen, Denmark, September 7–11, 2017. c©2017 Association for Computational Linguistics

Fine-Grained Citation Span Detection for References in Wikipedia

Besnik Fetahu1, Katja Markert2 and Avishek Anand1

1 L3S Research Center, Leibniz University of Hannover
Hannover, Germany

{fetahu, anand}@L3S.de
2 Institute of Computational Linguistics, Heidelberg University

Heidelberg, Germany
markert@cl.uni-heidelberg.de

Abstract

Verifiability is one of the core editing prin-
ciples in Wikipedia, editors being encour-
aged to provide citations for the added
content. For a Wikipedia article, determin-
ing the citation span of a citation, i.e. what
content is covered by a citation, is impor-
tant as it helps decide for which content
citations are still missing.

We are the first to address the problem of
determining the citation span in Wikipedia
articles. We approach this problem by
classifying which textual fragments in an
article are covered by a citation. We pro-
pose a sequence classification approach
where for a paragraph and a citation,
we determine the citation span at a fine-
grained level.

We provide a thorough experimental eval-
uation and compare our approach against
baselines adopted from the scientific do-
main, where we show improvement for all
evaluation metrics.

1 Introduction

Citations uphold the crucial policy of verifiability
in Wikipedia. This policy requires Wikipedia con-
tributors to support their additions with citations
from authoritative external sources (web, news,
journal etc.). In particular, it states that “arti-
cles should be based on reliable, third-party, pub-
lished sources with a reputation for fact-checking
and accuracy”1. Not only are citations essential
in maintaining reliability, neutrality and authori-
tative assessment of content in such a collabora-
tively edited platform; but lack of citations are

1https://en.wikipedia.org/wiki/
Wikipedia:Identifying_reliable_sources

At the summit of the climb, carpet tacks[1] were thrown onto the road 
causing as many as thirty riders to puncture,[2][3] including Gilbert's team-
mates Cadel Evans and Steve Cummings,[39] while race leader Bradley 
Wiggins […] precaution.[42] As a result, […] and eventually soloed his 
way to a fourth career stage victory at the Tour.[47]  Sagan led home a 
group of four riders almost a minute behind, […] behind Sánchez.[39]

Figure 1: Sub-sentence level span for citation [1]

in a citing paragraph in a Wikipedia article.

essential signals for core editors for unreliability
checks.
However, there are two problems when it comes to
citing facts in Wikipedia. First, there is a long tail
of Wikipedia pages where citations are missing
and hence facts might be unverified. Second, cita-
tions might have different span granularities, i.e.,
the text encoding the fact(s), for which a citation is
intended, might span less than a sentence (see Fig-
ure 1) to multiple sentences. We denote the differ-
ent pieces of text which contain a citation marker
as fact statements or simply statements. For exam-
ple, Table 1 shows different statements for several
citations. The aim of this work is to automatically
and accurately determine citation spans in order
to improve coverage (Fetahu et al., 2015b, 2016)
and to assist editors in verifying citation quality at
a fine-grained level.

Earlier work on span determination is mostly
concerned with scientific texts (O’Connor, 1982;
Kaplan et al., 2016), operates at sentence level and
exploits explicit authoring cues specific to scien-
tific text. Although Wikipedia has well formed
text, it does not follow explicit scientific guide-
lines for placing citations. Moreover, most state-
ments can only be inferred from the citation text.

In this work, we operate at a sub-sentence level,
loosely referred to as text fragments, and take a
sequence prediction approach using a linear chain
CRF (Lafferty et al., 2001). We limit our work
to citations referring to web and news sources, as

1990



they are accessible online and present the most
prominent sources in Wikipedia (Fetahu et al.,
2015a). By using recent work on moving window
language models (Taneva and Weikum, 2013) and
the structure of the paragraph that includes a ci-
tation, we classify sequences of text fragments as
text that belong to a given citation. We are able to
tackle all citation span cases as shown in Table 1.

sub
sentence

Obama was born on August 4, 1961[c1], at
Kapi’olani Maternity · · · Honolulu[c2]; he
is the first · · · been born in Hawaii.[c3].

sentence He was reelected to the Illinois Senate in
1998, · · · in 2002.[c1]

multi
sentence

On May 25, 2011, Obama · · · to address · · ·
UK Parliament in Westminster Hall, Lon-
don. This was · · · Charles de Gaulle · · · and
Pope Benedict XVI.[c1]

Table 1: Varying degrees of citation span granu-
larity in Wikipedia text.

2 Problem Definition and Terminology

In this section, we describe the terminology and
define the problem of determining the citation
span in text in Wikipedia articles.

Terminology. We consider Wikipedia articles
W = {e1, . . . , en} from a Wikipedia snapshot.
We distinguish citations to external references in
text and denote them with ⟨pk, ci⟩, where ci repre-
sents a citation which occurs in paragraph pk with
positional index k in an entity e ∈ W . We will re-
fer to pk as the citing paragraph. Furthermore,
with citing sentence we refer to the sentence in
s ∈ pk, which contains ci. Note that pk can have
more than one citation as shown in Table 1.

Problem Definition. The task of determining
the citation span for a citation c and a paragraph
p, respectively ⟨p, c⟩ (or simply pc), is subject to
the citing paragraph and the citation content. In
particular, we refer with citation span to the tex-
tual fragments from p which are covered by c.
The fragments correspond to the sequence of sub-
sentences S(p) = ⟨δ11, δ21 , . . . , δk1 , . . . , δmn ⟩. We
obtain the sequence of sub-sentences from p by
splitting the sentences into sub-sentences or text
fragments based on the following punctuation de-
limiters ({, !; :?}). These delimitors do not al-
ways provide a perfect semantic segmentation of
sentences into facts. A more involved approach
could be taken akin to work in text summarization,

such as Zhou and Hovy (Zhou and Hovy, 2006)
or (Nenkova et al., 2007) who consider summary
units for a similar purpose.

Formally, we define the citation span in Equa-
tion 4 as the function of finding the subset S ′ ⊆ S
where the fragments in S ′ are covered by c.

φ(p, c) → S ′ ⊆ S, s.t. δ ∈ S ′ ∧ c ⊢ δ (1)

where c ⊢ δ states that δ is covered in c.

3 Related Work

Scientific Text. One of the first attempts to deter-
mine the citation span in text (O’Connor, 1982)
was carried out in the context of document re-
trieval. The citing statements from a document
were used as an index to retrieve the cited docu-
ment. The citing statements are extracted based on
heuristics starting from the citing sentence and are
expanded with sentences in a window of +/-2 sen-
tences, depending on them containing cue words
like ‘this’, ‘these’,. . . ‘above-mentioned’. We con-
sider the approach in (O’Connor, 1982) as a base-
line.

Kaplan et al. (2016) proposed the task of de-
termining the citation block based on a set of tex-
tual coherence features (e.g. grammatical or lex-
ical coherence). The citation block starts from
the citing sentence, with succeeding sentences
classified (through SVMs or CRFs) according to
whether they belong to the block. Abu-Jbara and
Radev (2012) determine the citation block by first
segmenting the sentences and then classifying in-
dividual words as being inside/outside the citation.
Finally, the segment is classified depending on the
word labels (majority of words being inside, at
least one, or all of them). This approach is not
applicable in our case due to the fact that words
in Wikipedia text are not domain or genre-specific
as one expects in scientific text, and as such their
classification does not work.

Citations in IR. The importance of determin-
ing the citation span has been acknowledged in the
field of Information Retrieval (IR). The focus is on
building citation indexes (Garfield, 1955) and im-
proving the retrieval of scientific articles (Ritchie
et al., 2008, 2006). Citing sentences on a fixed
window size are used to index documents and aid
the retrieval process.

Summarization. Citations have been success-
fully employed to generate summaries of scientific
articles (Qazvinian and Radev, 2008; Elkiss et al.,

1991



2008). In all cases, citing statements are either ex-
tracted manually or via heuristics such as extract-
ing only citing sentences. Similarly (Nanba and
Okumura, 1999) expand the summaries in addi-
tion to the citing sentence based on cue words (e.g.
‘In this’, ‘However’ etc.). The work in (Qazvinian
and Radev, 2010) goes one step beyond and con-
siders sentences which do not explicitly cite an-
other article. The task is to assign a binary label
to a sentence, indicating whether it contains con-
text for a cited paper. We use this approach as one
of our competitors. Again, the premise is that ci-
tations are marked explicitly and additional citing
sentences are found dependent on them.

Comparison to our work. The language style
and the composition of citations in Wikipedia and
in scientific text differ significantly. Citations are
explicit in scientific text (e.g. author names) and
are usually the first word in a sentence (Abu-
Jbara and Radev, 2012). In Wikipedia, citations
are implicit (see Table 1) and there are no cue
words in text which link to the provided citations.
Therefore, the proposed methodologies and fea-
tures from the scientific domain do not perform
optimally in our case.

Both (Qazvinian and Radev, 2010) and
(O’Connor, 1982) work at the sentence level. As,
in Wikipedia, citation span detection needs to be
performed at the sub-sentence level (see Table 1),
their method introduces erroneous spans as we
will show in our evaluation.

Related to our problem is the work on address-
ing quotation attribution. Pareti et al. (2013) pro-
pose an approach for direct and indirect quotation
attribution. The task is mostly based on lexical
cues and specific reporting verbs that are the sig-
nal for the majority of direct quotations. However,
in the case of quotation attribution the task is to
find the source, cue, and content of the quotation,
whereas in our case, for a given citing paragraph
and reference we simply assess which text frag-
ment is covered by the reference. We also do nor-
mally not have access to specific lexical links be-
tween the citation and its citation span.

4 Citation Span Approach

We approach the problem of citation span de-
tection in Wikipedia as a sequence classification
problem. For a citation c and a citing paragraph p,
we chunk the paragraph into textual fragments at
the sub-sentence granularity, shown in Equation 4.

Figure 2 shows an overview of the sequence
classification of textual fragments. We use a lin-
ear chain CRF (Lafferty et al., 2001), where for
any fragment δ we predict the label corresponding
to a random variable y which is either ‘covered’ or
‘not-covered’. We opt for CRFs since we can en-
code global dependencies between the text frag-
ments and the actual citation, thus, ensuring the
coherence and accuracy of the predicted labels.

Figure 2: Linear chain CRF representing the se-
quence of text fragments in a paragraph. In the
factors we encode the fitness to the given citation.

We now describe the features we compute for
the factors Ψ(yi, yi−1, δi) for a fragment δi w.r.t
the citation c. We determine the fitness of δi hold-
ing true or being covered by c. We denote with
fk the features for the factors Ψi(yi, yi−1, δi) for
sequence δi for the linear-chain CRF in Figure 2.

4.1 Structural Features
An important aspect to consider for citation span
detection is the structure of the citing paragraph,
and correspondingly its sentences. For a textual
fragment δ, we extract the following structural fea-
tures shown in Table 2.

factor description

fc
′

i presence of other citations in δi where
c′ ̸= c

f#s the number of sentences in p
f
|δi|
i the length in terms of characters of the

sub-sequence
fsi check if δi is in the same sentence as the

citation c
fs ̸=s

′
i check if δi is in the same sentence as

δi−1
fci the distance of fragment δi to the frag-

ment which contains citation c

Table 2: Structural features for a fragment δi.

From the features in Table 2, we highlight f ci
which specifies the distance of δ to the fragment
that cites c. The closer a fragment is to the cita-
tion the higher the likelihood of it being covered

1992



in c. In Wikipedia, depending on the citation and
the paragraph length, the validity of a citation is
densely concentrated in its nearby sub-sentences
(preceding and succeeding).

Furthermore, the features f#s and fsi (the num-
ber of sentences in p together with the feature con-
sidering if δ is in the same sentence as c) are strong
indicators for accurate prediction of the label of δ.
That is, it is more likely for a fragment δ to be cov-
ered by the citation if it appears in the same sen-
tence or sentences nearby to the citation marker.

However, as shown in Table 1 there are three
main citation span groups, and as such relying
only on the structure of the citing paragraph does
not yield optimal results. Hence, in the next group
we consider features that tie the individual frag-
ments in the citing paragraph with the citation as
shown in Figure 2.

4.2 Citation Features
A core indicator as to whether a fragment δ is cov-
ered by c is based on the lexical similarity between
δ and the content in c. We gather such evidence
by computing two similarity measures. We com-
pute the features fLMi and f

J
i between δ and para-

graphs in the citation content c.
The first measure, fLMi , corresponds to a

moving language window proposed in (Taneva
and Weikum, 2013). In this case, for each
word in either a paragraph in the citation c
or the sequence δ, we associate a language
model Mwi based on its context ϕ(wi) =
{wi−3, wi−2, wi−1, wi, wi+1, wi+2, wi+3} with a
window of +/- 3 words. The parameters for the
model Mwi are estimated as in Equation 2 for all
the words in the context ϕ(wi) and their frequen-
cies denoted with tf . With Mδ and Mp we denote
the overall models as estimated in Equation 2 for
the words in the respective fragments.

P (w|Mwi) =
tfw,ϕ(wi)∑

w′∈ϕ(wi) tfw′,ϕ(wi)
(2)

Finally, we compute the similarity of each word in
w ∈ δ against the language model of paragraph
p ∈ c in Equation 3, which corresponds to the
Kullback-Leibler divergence score.

fLMi = min
p∈c

[
−

∑
w∈δ

P (w|Mδ) log P (w|Mδ)
P (w|Mp)

]
(3)

The intuition behind fLMi is that for the frag-
ments δ we take into account the word similarity

and the similarity in the context they appear in w.r.t
a paragraph in a citation. In this way, we ensure
that the similarity is not by chance but is supported
by the context in which the word appears. Finally,
another advantage of this model is that we localize
the paragraphs in c which provide evidence for δ.

As an additional feature we compute fJi which
corresponds to the maximal jaccard similarity be-
tween δi and paragraphs p ∈ c.

Finally, as we will show in our experimental
evaluation in Section 5, there is a high correlation
between the citation span length and the length of
citation content in terms of sentences. Hence, we
add as an additional feature f c the number of sen-
tences in c.

4.3 Discourse Features

Sentences and fragments within a sentence can
be tied together by discourse relations. We an-
notate sentences with explicit discourse relations
based on an approach proposed in (Pitler and
Nenkova, 2009), using discourse connectives as
cues. The explicit discourse relations belong to
one of the following: temporal, contingency, ex-
pansion, comparison.

After extracting a discourse connective in a sen-
tence, we determine by its position to which frag-
ment it belongs and mark the fragment accord-
ingly. We denote with fdisci the discourse feature
for the fragment δi.2

4.4 Temporal Features

An important aspect that we consider here is the
temporal difference between two consecutive frag-
ments δi and δi−1. If there exists a temporal date
expression in δi and δi−1 and they point to differ-
ent time-points, this presents an indicator on the
transitioning between the states yi and yi−1. That
is, there is a higher likelihood of changing the state
in the sequence S for the labels yi and yi−1.

We compute the temporal feature
f

λ(i,i−1)
i ,indicating the difference in days be-

tween any two temporal expression extracted
from δi and δi−1. We extract the temporal
expression through a set of hand-crafted regular
expressions. We use the following expressions:
(1) DD Month YYYY, (2) DD MM YYYY, (3)

2Note that, although discourse relations hold between at
least two fragments or sentences, we only mark the individual
fragment in which the connective occurs with the discourse
relation type.

1993



type avg. |s| avg. |δ| avg ‘covered’
news 7.76 22.55 0.28
web 8.67 23.07 0.30

Table 3: Dataset statistics for citing paragraphs,
distinguishing between web and news references,
showing the average number of sentences, frag-
ments, and covered fragments.

MM DD YY(YY), (4) YYYY, with delimiters
(whitespace, ‘-’, ‘.’).

5 Experimental Setup

We now outline the experimental setup for evalu-
ating the citation span approach and the competi-
tors for this task. The data and the proposed ap-
proaches are made available at the paper URL3.

5.1 Dataset
We evaluate the citation span approaches on a ran-
dom sample of Wikipedia entities (snapshot of
20/11/2016). For the sampling process, we first
group entities based on the number of web or news
citations.4). We then sample from the specific
groups. This is due to the inherent differences
in citation spans for entities with different num-
bers of citations. For instance, entities with a high
number of citations tend to have shorter spans per
citation. Figure 3 shows the distribution of entities
from the different groups. From each sampled en-
tity, we extract all citing paragraphs that contain
either a web or news citation. Our sample consists
of 509 citing paragraphs from 134 entities.

Furthermore, since a paragraph may have more
than one citation, in our sampled citing para-
graphs, we have an average of 4.4 citations per
paragraph, which finally resulted in 408 unique
paragraphs. Table 3 shows the stats of the dataset.

5.2 Ground Truth
Setup. For the ground truth, the citation span of
c in paragraph p was manually determined by la-
beling each fragment in p with the binary label
covered or not-covered.

We set strict guidelines that help us generate re-
liable ground-truth annotations. We follow two
main guidelines: (i) requirement to read and com-
prehend the content in c, and (ii) matching of the

3http://l3s.de/˜fetahu/emnlp17/
4Wikipedia has an internal categorization of citations

based on the reference they point to.

100
101
102
103
104
105

(0,
10]

(10
,20

]

(20
,30

]

(30
,40

]

(40
,50

]

(50
,10

0]

(10
0,2

00]

(20
0,5

00]
> 5

00 

Citations

En
titi

es
 (l

og
 sc

ale
)

Entity Citation Distribution

Figure 3: Entity distribution based on the number
of news citations.

textual fragments from p as either being supported
explicitly or implicitly in c.5

The entire dataset was carefully annotated by
the first author. Later, a second annotator anno-
tated a 10% sample of the dataset with an inter-
rater agreement of κ = .84. We chose not to use
crowd-sourcing as the task is very complex and
hard to divide into small independent tasks. Since
the task requires reading and comprehending the
entire content in c and p, it takes on average up to
2.4 minutes to perform the evaluation for a single
item. In future, it would be worthwhile to conduct
more large-scale annotation exercises.

Citation Span Stats. Following the definition
in Equation 4, we determine the citation span at
the sub-sentence granularity level. Table 4 shows
the distribution of citations falling into the specific
spans for the citing paragraphs. We note that the
majority of citations have a span between half a
sentence and up to a sentence, yet, the remainder
of more than 20% of citation span across multiple
sentences in such paragraphs.

We define the citation span as the ratio of sub-
sentences which are covered by a given citation
over the total number of sub-sentences in the
sentence, consequentially in the citing paragraph.
That is, a citation is considered to have a span of
one sentence if it covers all its sub-sentences.

span(c, p) =
∑
s∈p

#δs ∈ S ′
#δs

(4)

where δs represents a sequence in sentence s ∈ p,
which are part of the the ground-truth.

In Figure 4, we analyze a possible factor in the
variance of the citation span. It is evident that for
longer cited documents the span increases. This is

5We excluded cases where the citation is not appropriate
for the paragraph at all. This is, for example, the case when
the language of c is not English.

1994



total ≤ .5 (.5, 1] (1, 2] (2, 5] > 5
news 318 35 201 54 22 6
web 191 13 121 27 25 6

Table 4: Citation span distribution based on the
number of sub-sentences in the citing paragraph.

intuitive since such documents carry more infor-
mation and consequentially their span in the cit-
ing paragraphs can be larger. An example is the
Wikipedia article 2008 US Open (tennis)
which has a citing paragraph with a citation span
of 7 sentences for an article of 30k characters
long6. We encoded this in the citation features f c.

0

3000

6000

9000

≤ 0.5 (0.5, 1] (1, 2] (2,5] > 5
Citation Span Buckets

Do
c L

en
gt

h Cite Type
news

web

Figure 4: Average document length for the differ-
ent span buckets for citation types web and news.

Additionally, within the different citation spans
we analyze how many of them contain skips for
two cases: (i) skip a fragment within a sentence,
and (ii) skip sentences in p. The results for both
cases are presented in Table 5.

span news web
skip δ skip s skip δ skip s

≤ 0.5 6% - - -
(0.5, 1] - - - 1%

(1, 2] - 8% - 19%
(2, 5] 5% 18% - 21%
> 5 - 20% - 67%

Table 5: The percentage of citations in a span with
fragment skips and sentence skips.

From the results in Table 4 and 5 we see that
simple heuristics on selecting complete sentences
or selecting consecutive sequences do not account
for the different citation span cases and skips at the
sentence and paragraph level. This leads to subop-
timal results and introduces erroneous spans. Fur-
thermore, we find that in 3.7% of the cases in our

6http://news.bbc.co.uk/sport1/hi/
tennis/7601195.stm

ground-truth, the citation spans include fragments
after the citation marker.

5.3 Baselines
We consider the following baselines as competi-
tors for our citation span approach.

Inter-Citation Text – IC. The span consists of
sentences which start either at the beginning of the
paragraph or at the end of a previous citation. The
granularity is at the sentence level.

Citation-Sentence-Window – CSW. The span
consists of sentences in a window of +/- 2 sen-
tences from the citing sentence (O’Connor, 1982).
The other sentences are included if they contain
specific cue words in fixed positions.

Citing Sentence – CS. The span consists of
only the citing sentence.

Markov Random Fields - MRF.
MRFs (Qazvinian and Radev, 2010) model
two functions. First, compatibility, which mea-
sures the similarity of sentences in p, and as such
allows to extract non-citing sentences. Second,
the potential, which measures the similarity
between sentences in c with sentences in p. We
use the provided implementation by the authors.

Citation Span Plain – CSPC. A plain clas-
sification setup using the features in Section 4,
where the sequences are classified in isolation. We
use Random Forests (Breiman, 2001) and evaluate
them with 5-fold cross validation.

5.4 Citation Span Approach Setup – CSPS
For our approach CSPS as mentioned in Sec-
tion 4, we opt for linear-chain CRFs and use the
implementation in (Okazaki, 2007). We evaluate
our models using 5-fold cross validation, and learn
the optimal parameters for the CRF model through
the L-BFGS approach (Liu and Nocedal, 1989).

5.5 Evaluation Metrics
We measure the performance of the citation span
approaches through the following metrics. We
will denote with W ′ the sampled entities, with
p = {pc, . . .} (pc refers to ⟨p, c⟩) the set of sam-
pled paragraphs from e, and with |p| the total
items from e.

Mean Average Precision – MAP . First, we
define precision for pc as the ratio P (pc) =
|S ′ ∩ St|/|S ′| of fragments present in S ′∩St over
S ′. We measure MAP as in Equation 5.

MAP =
1

|W ′|
∑

e∈W ′

∑
pc∈p P (pc)

|p| (5)

1995



Recall – R. We measure the recall for pc as the
ratio S ′ ∩ St over all fragments in St, R(pc) =
|S ′ ∩ St|/|St|. We average the individual recall
scores for e ∈ W ′ for the corresponding p.

R =
1

|W ′|
∑

e∈W ′

∑
pc∈p R(pc)

|p| (6)

Erroneous Span – ∆. We measure the num-
ber of extra words or extra sub-sentences (denoted
with ∆w and ∆δ) added by text fragments that are
not part of the ground-truth St. The ratio is rel-
ative to the number of words or sub-sentences in
the ground-truth for pc. We compute ∆w and ∆δ
in Equation 7 and 8, respectively.

∆w =
1

|W ′|
∑

e∈W ′

1

|p|
∑
pc∈p

∑
δ∈S′\St words(δ)∑

δ∈St words(δ)
(7)

∆δ =
1

|W ′|
∑

e∈W ′

1

|p|
∑
pc∈p

|S ′ \ St|
|St| (8)

6 Results and Discussion

6.1 Citation Span Robustness
Table 6 shows the results for the different ap-
proaches on determining the citation span for all
span cases shown in Table 4.

Accuracy. Not surprisingly, the baseline ap-
proaches perform reasonably well. CS which se-
lects only the citing sentence achieves a reasonable
MAP = 0.86 and similar recall. A slightly dif-
ferent baseline CSW achieves comparable scores
with MAP = 0.85. This is due to the inherent
span structure in Wikipedia, where a large portion
of citations span up to a sentence (see Table 4).
Therefore, in approximately 64% of the cases the
baselines will select the correct span. For the cases
where the span is more than a sentence, the draw-
back of these baselines is in coverage. We show in
the next section a detailed decomposition of the re-
sults and highlight why even in the simpler cases,
a sentence level granularity has its shortcomings
due to sequence skips as shown in Table 5.

Overall, when comparing CS as the best per-
forming baseline against our approach CSPS,
we achieve an overall score of MAP = 0.83
(a slight decrease of 3.6%), whereas in term of
F1 score, we have a decrease of 9%. The plain-
classification approach CSPC achieves similar
score with MAP = 0.86, whereas in terms of
F1 score, we have a decrease of 8%. As de-
scribed above and as we will see later on in Ta-
ble 7, the overall good performance of the baseline

approaches can be attributed to the citation span
distribution in our ground-truth.

On the other hand, an interesting observation is
that sophisticated approaches, geared towards sci-
entific domains like MRF perform poorly. We
attribute this to language style, i.e., in Wikipedia
there are no explicit citation hooks that are present
in scientific articles. Comparing to CSPS, we
outperform MRF by a large margin with an in-
crease in MAP by 84%.

When comparing the sequence classifier CSPS
to the plain classifier CSPC, we see a marginal
difference of 1.3% for F1. However, it will be-
come more evident later that classifying jointly the
text fragments for the different span buckets, out-
performs the plain classification model.

MAP R F1 ∆w ∆δ

MRF 0.45 0.78 0.56 308% 278%
IC 0.72 0.94 0.77 113% 115%

CSW 0.85 0.84 0.82 38% 31%
CS 0.86 0.84 0.82 35% 27%

CSPC 0.86 0.68 0.76 26% 23%
CSPS 0.83 0.69 0.75 32% 24%

Table 6: Evaluation results for the different cita-
tion span approaches.

Erroneous Span. One of the major drawbacks
of competing approaches is the granularity at
which the span is determined. This leads to er-
roneous spans. From Table 4 we see that approx-
imately in ∼10% of the cases the span is at sub-
sentence level, and in 28% the span is more than a
sentence.

The best performing baseline CS has an erro-
neous span of ∆w = 35% and ∆δ = 27%, in
terms of extra words and sub-sentences, respec-
tively. That is, nearly half of the determined span
is erroneous, or in other words it is not covered
in the provided citation. The MRF approach due
to its poor MAP score provides the largest erro-
neous spans with ∆w = 308% and ∆δ = 278%.
The amount of erroneous span is unevenly dis-
tributed, that is, in cases where the span is not at
the sentence level granularity the amount of erro-
neous span increases. A detailed analysis is pro-
vided in the next section.

Contrary to the baselines, for CSPS and simi-
larly for CSPC, we achieve the lowest erroneous
spans with ∆w = 32% and ∆δ = 26%, and
∆w = 24% and ∆w = 23%, respectively.

Compared to the remaining baselines, we

1996



achieve an overall relative decrease of 9% for
∆w(CSPS), and 34% for ∆w(CSPC), when
compared to the best performing baseline CS.

From the skips in sequences in Table 5 and the
unsuitability of sentence granularity for citation
spans, we analyze the locality of erroneous spans
w.r.t to the sequence that contains c, specifically
the distribution of erroneous spans preceding and
succeeding it. For the CS baseline, 71% of the
total erroneous spans are added by sequences pre-
ceding the citing sequence, contrary to 35% which
succeed it. In the case of CSPS, we have only 9%
of erroneous spans (for ∆δ) preceding the citation.

6.2 Citation Span and Feature Analysis

We now analyze how the approaches perform for
the different citation spans in Table 47. Addition-
ally, we analyze how our approach CSPS per-
forms when determining the span without access
to the content of c.

Citation Span. Table 7 shows the results for the
approaches under comparison for all the citation
span cases. In the case where the citation spans
up to a sentence, that is (0.5, 1], which presents
the simplest citation span case, the baselines per-
form reasonably well. This is due to the heuris-
tics they apply to determine the span, which in all
cases includes the citing sentence. In terms of F1
score, the baseline CS achieves a highly compet-
itive score of F1 = 0.97. Our approach CSPS
in this case has slight increase of 1% for F1 and
an increase of 3% for MAP . CSPC achieves a
similar performance in this case.

However, for the cases where the span is at the
sub-sentence level or across multiple sentences,
the performance of baselines drops drastically. In
the first bucket (≤ 0.5) which accounts for 9%
of ground-truth data, we achieve the highest score
with MAP = 0.87, though with lower recall than
the competitors with R = 0.56. The reason for
this is that the baselines take complete sentences,
thus, having perfect recall at the cost of accuracy.
In terms of F1 score we achieve 21% better results
than the best performing baseline CS.

For the span of (1, 2] we maintain an overall
high accuracy and recall, and have the highest F1
score. The improvement is 8% in terms of F1
score. Finally, for the last case where the span is
more than 2 sentences, we achieve MAP = 0.74,

7The models were retrained and tested for the different
buckets with 5-fold cross validation.

a marginal increase of 3%, however with lower re-
call, which results in an overall decrease of 4% for
F1. The statistical significance tests are indicated
with ** and * in Table 7.

≤ 0.5

9 % 11 %

872 %

274 %258 %

480 %

0

250

500

750

1000

CSPS CSPC CS CSW IC MRF

(0.5,1]

6 % 5 %

313 %

14 %12 %

80 %

0

100

200

300

CSPS CSPC CS CSW IC MRF

(1,2]

11 % 7 %

114 %

11 %10 %

65 %

0

50

100

150

CSPS CSPC CS CSW IC MRF

> 2

45 %

26 %

96 %

17 %16 %

68 %

0

30

60

90

CSPS CSPC CS CSW IC MRF

Citation Span Buckets

Er
ro

ne
ou

s S
pa

n 
   
Δ

w %
Figure 5: Erroneous spans for the different citation
span buckets. The y-axis presents the ∆w whereas
in the x-axis are shown the different approaches.

Erroneous Span. Figure 5 shows the erroneous
spans in terms of words for the metric ∆w for
all citation span cases. It is noteworthy that the
amount of error can be well beyond 100% due to
the ratio of the suggested span and the actual span
in our ground-truth, which can be higher.

In the first bucket (span of ≤ 0.5) with granu-
larity less than a sentence, all the competing ap-
proaches introduce large erroneous spans. For
CSPS we have a MAP = 0.87, and conse-
quentially we have the lowest ∆w = 9%, while
for CSPC we have only ∆w = 11%. In con-
trast, the non-ML competitors introduce a mini-
mum of ∆w(CS) = 182%, with MRFs having
the highest error. We also perform well in the
bucket (0.5, 1]. For larger spans, for instance, for
(1, 2], we are still slightly better, with roughly 3%
less erroneous span when comparing CSPC and
CS. However, only in the case of spans with > 2,
we perform below the CS baseline. Despite, the
smaller erroneous span, the CS baseline never in-
cludes more than one sentence, and as such it does
not include many erroneous spans for the larger
buckets. However, it is by definition unable to rec-
ognize any longer spans.

Feature Analysis. It is worthwhile to investigate
the performance gains in determining the citation
span without analyzing the content of the citation.
The reason for this is that there are several cita-

1997



≤ 0.5 (0.5, 1] (1, 2] > 2
MAP R F1 MAP R F1 MAP R F1 MAP R F1

MRF 0.15 0.88 0.27 0.44 0.80 0.61 0.59 0.74 0.57 0.59 0.63 0.55
IC 0.32 1.00 0.45 0.77 0.99 0.83 0.73 0.84 0.74 0.72 0.81 0.73
CSW 0.38 1.00 0.54 0.93 0.98 0.96 0.88 0.54 0.65 0.79 0.34 0.43
CS 0.40 1.00 0.56 0.94 0.98 0.97 0.90 0.53 0.65 0.80 0.32 0.42

CSPC 0.85 0.53 0.65 0.96 0.97 0.97 0.96 0.68 0.79 0.71 0.65 0.68
CSPS 0.87** 0.56 0.68** 0.96 0.98 0.98 0.88 0.73 0.80* 0.74 0.72 0.70

∆F1 CSPS ▲21% 0% ▲8% ▼4%

Table 7: Evaluation results for the citation span approaches for the different span cases. For the results
of CSPS we compute the relative increase/decrease of F1 score compared to the best result (based
on F1) from the competitors. We mark in bold the best results for the evaluation metrics, and indicate
with ** and * the results which are highly significant (p < 0.001) and significant (p < 0.05) based on
t-test statistics when compared to the best performing baselines (CS, IC, CSW, MRF) based on F1 score,
respectively.

tion categories for which access to the source can-
not be easily automated. Models which can deter-
mine the span accurately without the actual con-
tent have the advantage of generalizing to other
citation sources (e.g. books) for which the evalua-
tion is more challenging.8

Here, we disregard the citation features from
Section 4.2. In terms of MAP , we have a slight
decrease with MAP = 0.82 when compared to
the model with the citation features. For recall we
have a drop of 3%, resulting in R = 0.67.

This shows that by solely relying on the struc-
ture of the citing paragraph and other structural
and discourse features we can perform the task
with reasonable accuracy.

7 Conclusion

In this work, we tackled the problem of determin-
ing the fine-grained citation span of references in
Wikipedia. We started from the citing paragraph
and decomposed it into sequences consisting of
sub-sentences. To accurately determine the span
we proposed features that leverage the structure
of the paragraph, discourse and temporal features,
and finally analyzed the similarity between the cit-
ing paragraph and the citation content.

We introduce both a standard classifier as well
as a sequence classifier using a linear-chain CRF
model. For evaluation we manually annotated a
ground-truth dataset of 509 citing paragraphs. We
reported standard evaluation metrics and also in-

8At worst, one needs to read and comprehend the entire
book to determine if a fragment is covered by the citation.

troduced metrics that measure the amount of erro-
neous span.

We achieved a MAP = 0.86, in the case of
the plain classification model CSPC, and with
a marginal difference for CSPS with MAP =
0.83, across all cases with an erroneous span of
∆w = 26% or ∆w = 32%, depending on the
model. Thus, we provide accurate means on de-
termining the span and at the same time decrease
the erroneous span by 34% compared to the best
performing baselines. Moreover, we excel at de-
termining citation spans at the sub-sentence level.

In conclusion, this presents an initial attempt
on solving the citation span for references in
Wikipedia. As future work we foresee a larger
ground-truth and more robust approaches which
take into account factors such as a reference being
irrelevant to a citing paragraph and cases where
the evidence for a paragraph is implied rather than
explicitly stated in the reference.

Acknowledgments

This work is funded by the ERC Advanced Grant
ALEXANDRIA (grant no. 339233), and H2020
AFEL project (grant no. 687916).

References

Amjad Abu-Jbara and Dragomir R. Radev. 2012. Ref-
erence scope identification in citing sentences. In
Human Language Technologies: Conference of the
North American Chapter of the Association of Com-
putational Linguistics, Proceedings, June 3-8, 2012,
Montréal, Canada, pages 80–90.

1998



Leo Breiman. 2001. Random forests. Machine Learn-
ing, 45(1):5–32.

Aaron Elkiss, Siwei Shen, Anthony Fader, Günes
Erkan, David J. States, and Dragomir R. Radev.
2008. Blind men and elephants: What do citation
summaries tell us about a research article? JASIST,
59(1):51–62.

Besnik Fetahu, Abhijit Anand, and Avishek Anand.
2015a. How much is wikipedia lagging behind
news? In Proceedings of the ACM Web Science
Conference, WebSci 2015, Oxford, United Kingdom,
June 28 - July 1, 2015, pages 28:1–28:9.

Besnik Fetahu, Katja Markert, and Avishek Anand.
2015b. Automated news suggestions for populating
wikipedia entity pages. In Proceedings of the 24th
ACM International Conference on Information and
Knowledge Management, CIKM 2015, Melbourne,
VIC, Australia, October 19 - 23, 2015, pages 323–
332.

Besnik Fetahu, Katja Markert, Wolfgang Nejdl, and
Avishek Anand. 2016. Finding news citations for
wikipedia. In Proceedings of the 25th ACM Inter-
national on Conference on Information and Knowl-
edge Management, CIKM 2016, Indianapolis, IN,
USA, October 24-28, 2016, pages 337–346.

Eugene Garfield. 1955. Citation indexes for science:
A new dimension in documentation through associ-
ation of ideas. Science, 122(3159):108–111.

Dain Kaplan, Takenobu Tokunaga, and Simone Teufel.
2016. Citation block determination using textual co-
herence. JIP, 24(3):540–553.

John D. Lafferty, Andrew McCallum, and Fernando
C. N. Pereira. 2001. Conditional random fields:
Probabilistic models for segmenting and labeling se-
quence data. In Proceedings of the Eighteenth Inter-
national Conference on Machine Learning (ICML
2001), Williams College, Williamstown, MA, USA,
June 28 - July 1, 2001, pages 282–289.

Dong C Liu and Jorge Nocedal. 1989. On the limited
memory bfgs method for large scale optimization.
Mathematical programming, 45(1):503–528.

Hidetsugu Nanba and Manabu Okumura. 1999. To-
wards multi-paper summarization using reference
information. In Proceedings of the Sixteenth Inter-
national Joint Conference on Artificial Intelligence,
IJCAI 99, Stockholm, Sweden, July 31 - August 6,
1999. 2 Volumes, 1450 pages, pages 926–931.

Ani Nenkova, Rebecca J. Passonneau, and Kathleen
McKeown. 2007. The pyramid method: Incorpo-
rating human content selection variation in summa-
rization evaluation. TSLP, 4(2):4.

John O’Connor. 1982. Citing statements: Computer
recognition and use to improve retrieval. Inf. Pro-
cess. Manage., 18(3):125–131.

Naoaki Okazaki. 2007. Crfsuite: a fast implementation
of conditional random fields (crfs).

Silvia Pareti, Timothy O’Keefe, Ioannis Konstas,
James R. Curran, and Irena Koprinska. 2013. Au-
tomatically detecting and attributing indirect quota-
tions. In Proceedings of the 2013 Conference on
Empirical Methods in Natural Language Process-
ing, EMNLP 2013, 18-21 October 2013, Grand Hy-
att Seattle, Seattle, Washington, USA, A meeting of
SIGDAT, a Special Interest Group of the ACL, pages
989–999.

Emily Pitler and Ani Nenkova. 2009. Using syntax to
disambiguate explicit discourse connectives in text.
In ACL 2009, Proceedings of the 47th Annual Meet-
ing of the Association for Computational Linguistics
and the 4th International Joint Conference on Natu-
ral Language Processing of the AFNLP, 2-7 August
2009, Singapore, Short Papers, pages 13–16.

Vahed Qazvinian and Dragomir R. Radev. 2008. Sci-
entific paper summarization using citation summary
networks. In COLING 2008, 22nd International
Conference on Computational Linguistics, Proceed-
ings of the Conference, 18-22 August 2008, Manch-
ester, UK, pages 689–696.

Vahed Qazvinian and Dragomir R. Radev. 2010. Iden-
tifying non-explicit citing sentences for citation-
based summarization. In ACL 2010, Proceedings
of the 48th Annual Meeting of the Association for
Computational Linguistics, July 11-16, 2010, Upp-
sala, Sweden, pages 555–564.

Anna Ritchie, Stephen Robertson, and Simone Teufel.
2008. Comparing citation contexts for information
retrieval. In Proceedings of the 17th ACM Confer-
ence on Information and Knowledge Management,
CIKM 2008, Napa Valley, California, USA, October
26-30, 2008, pages 213–222.

Anna Ritchie, Simone Teufel, and Stephen Robertson.
2006. How to find better index terms through cita-
tions. In Proceedings of the Workshop on How Can
Computational Linguistics Improve Information Re-
trieval?, CLIIR ’06, pages 25–32, Stroudsburg, PA,
USA. Association for Computational Linguistics.

Bilyana Taneva and Gerhard Weikum. 2013. Gem-
based entity-knowledge maintenance. In 22nd
ACM International Conference on Information and
Knowledge Management, CIKM’13, San Francisco,
CA, USA, October 27 - November 1, 2013, pages
149–158.

Liang Zhou and Eduard H Hovy. 2006. On the summa-
rization of dynamically introduced information: On-
line discussions and blogs. In AAAI Spring sympo-
sium: Computational approaches to analyzing we-

blogs, page 237.

1999


