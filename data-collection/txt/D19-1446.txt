



















































Machine Translation With Weakly Paired Documents


Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing
and the 9th International Joint Conference on Natural Language Processing, pages 4375–4384,
Hong Kong, China, November 3–7, 2019. c©2019 Association for Computational Linguistics

4375

Machine Translation With Weakly Paired Documents

Lijun Wu1, Jinhua Zhu2, Fei Gao3, Di He4, Tao Qin5, Jianhuang Lai1, Tie-Yan Liu5
1School of Data and Computer Science, Sun Yat-sen University;

2 University of Science and Technology of China;
3 Institute of Computing Technology, Chinese Academy of Sciences;

4 Peking University, 5 Microsoft Research Asia;
1{wulijun3@mail2, stsljh@mail}.sysu.edu.cn,

2teslazhu@mail.ustc.edu.cn, 3gaofei17b@ict.ac.cn,
4di he@pku.edu.cn, 5{taoqin, tyliu}@microsoft.com

Abstract

Neural machine translation, which achieves
near human-level performance in some
languages, strongly relies on the large
amounts of parallel sentences, which hinders
its applicability to low-resource language
pairs. Recent works explore the possibility
of unsupervised machine translation with
monolingual data only, leading to much
lower accuracy compared with the supervised
one. Observing that weakly paired bilingual
documents are much easier to collect than
bilingual sentences, e.g., from Wikipedia,
news websites or books, in this paper, we
investigate training translation models with
weakly paired bilingual documents. Our
approach contains two components. 1) We
provide a simple approach to mine implicitly
bilingual sentence pairs from document pairs
which can then be used as supervised training
signals. 2) We leverage the topic consistency
of two weakly paired documents and learn
the sentence translation model by constrain-
ing the word distribution-level alignments.
We evaluate our method on weakly paired
documents from Wikipedia on six tasks, the
widely used WMT16 German↔English,
WMT13 Spanish↔English and WMT16
Romanian↔English translation tasks. We
obtain 24.1/30.3, 28.1/27.6 and 30.1/27.6
BLEU points separately, outperforming pre-
vious results by more than 5 BLEU points in
each direction and reducing the gap between
unsupervised translation and supervised
translation up to 50%.

1 Introduction

Neural machine translation (NMT) is a great suc-
cess of deep learning for natural language process-
ing (Koehn et al., 2003; Cho et al., 2014; Sutskev-
er et al., 2014; Bahdanau et al., 2015; Jean et al.,
2015; Vaswani et al., 2017; Gehring et al., 2017).
It has significantly outperformed statistical ma-

chine translation and reached near human-level
performance for several language pairs (Wu et al.,
2016; Hassan et al., 2018). Such breakthrough-
s heavily depend on the availability of large s-
cale bilingual sentence pairs. Taking WMT14
English→French task as an example, NMT us-
es 38 million parallel sentence pairs for training
(Vaswani et al., 2017). As such bilingual sentence
pairs are costly to collect, the success of NMT is
not fully realized on the vast majority of language
pairs, especially for low-resource languages. Re-
cently, Artetxe et al. (2017); Lample et al. (2017)
tackle this challenge by training NMT models us-
ing only monolingual data, which achieves consid-
erably good accuracy but still far away from that
of the state-of-the-art supervised models.

While it is costly to collect bilingual sentence
pairs by human translation, we notice that there
exists many weakly paired bilingual documents on
the Web. For example, for the entity “machine
learning”, Wikipedia has multiple articles in dif-
ferent languages (e.g., the English and German ar-
ticles). Though the two articles have very similar
content, they are not sentence-by-sentence trans-
lations since they may be independently created
by different people. Similarly, an English news
in BBC and a Chinese news in China Daily talk
about the same event but may have differences in
details. Furthermore, a popular novel in different
languages is usually liberal translation instead of
literal translation. We call such weakly aligned
documents weakly paired bilingual documents. In
this paper, we explore learning NMT models from
such weakly paired documents, which has sev-
eral advantages. First, weakly paired documents
are much easier to obtain than bilingual sentence
pairs. We can obtain bilingual document pairs
from Wikipedia pages, aligned news articles on in-
ternet, or even from books. Second, such weakly
paired documents have great coverage of different



4376

languages. For example, Wikipedia covers more
than 178 languages and most of them have paired
pages to English. This means that learning trans-
lation models from paired bilingual documents are
possible for many language pairs.

Weakly aligned document pairs can be uti-
lized for NMT training from two aspects. First,
although such two documents are not exactly
sentence-by-sentence translations, it is possible
that one specific sentence in one document is the
translation of one sentence in the other documen-
t. Such a sentence pair can be used as bilingual
signals for training. For example in Wikipedia, al-
though the web structure and paragraphs are gen-
erally different for entity “Beijing” in English and
“Pekin” in French, we find the first sentences of
the pages are quite semantically similar, in which
both of them define Beijing as the capital of China
(English: Beijing, formerly romanized as Peking,
is the capital of the People’s Republic of Chi-
na. French: Pekin, galement appele Beijing, est
la capitale de la Rpublique populaire de Chine.)
The challenge is how to mine such sentence pairs
from those document pairs. Second, although the
sentences in two weakly paired documents are
not aligned, the topics of the documents are well
aligned. Such topic alignment is a strong signal
that can be used to train NMT models.

In this paper, we propose a method to train N-
MT models by leveraging weakly paired bilingual
documents from Wikipedia. The key idea contains
two aspects: a) Mining implicitly aligned sentence
pairs. We provide a simple and efficient method to
mine bilingual sentence pairs from weakly aligned
document pairs. With the cross-lingual word em-
beddings trained from two monolingual corpora
(one for each language) using MUSE (Conneau
et al., 2017), we use weighted average of word
embeddings in a sentence as the sentence embed-
ding. We then select the sentence pairs using the
sentence embeddings with large cosine similarity
as bilingual sentence pairs to train NMT model-
s. b) Leveraging topic alignment as regularization.
Many previous works suggest that the word distri-
bution can be usecd to well characterize the topic
of the document (Petterson et al., 2010; Du et al.,
2015; Funatsu et al., 2014; Pedrosa et al., 2016;
Chemudugunta et al., 2007). To leverage the top-
ic consistency between weakly paired documents,
we minimize the KL-divergence of the word dis-
tributions between the ground-truth document and

the model-generated document.
Taking Wikipedia corpus as the training data,

we test our method on the widely used WMT16
German↔English, WMT13 Spanish↔English
and WMT16 Romanian↔English translation
tasks. Our method achieves 24.1/30.3 BLEU
points for WMT16 German↔English trans-
lations, 28.1/27.6 BLEU points for WMT13
Spanish↔English translations and 30.1/27.6
BLEU scores for WMT16 Romanian↔English
translations, outperforming the previous strong
unsupervised method by more than 5 BLEU
points and reduce the gap between unsupervised
translation and supervised translation up to 50%.

2 Our method

We contribute two ways to leverage weakly paired
documents: mining implicitly aligned sentence
pairs from the document pairs and aligning the
topic distributions of two documents in a weak-
ly aligned pair. Before diving into details, we first
introduce some notations.

Denote D = {(dXi , dYi )}, i ∈ {1, 2, ...,M} as
the set of weakly paired documents, in which doc-
ument dXi is aligned to d

Y
i . For example, d

X
i and

dYi are two cross-lingual linked Wikipedia pages.
Denote nXi and n

Y
i as the number of sentences in

document dXi and d
Y
i respectively. Note that usu-

ally nXi 6= nYi .
Without any confusions, we denote x as a sen-

tence in language X and y as a sentence in lan-
guage Y . We denote enc as the encoder for lan-
guage X and Y , which maps a sentence x or y
into a sequence of real vectors using parameter
θenc. We use dec with parameter θdec as the de-
coder, which takes the encoded vectors and tar-
get language tag (X or Y ) as inputs and outputs
a probability distribution over sentences in the tar-
get language. Let θ denote all the parameters of
the translation model. Similar to (Artetxe et al.,
2017; Lample et al., 2017), such a model can han-
dle both X → Y and Y → X translations.

2.1 Mining implicitly aligned sentence pairs

Different language versions of Wikipedia pages
about the same entity/event are usually creat-
ed by different people speaking different native
languages, and therefore most sentences in two
weakly aligned documents are not aligned. Even
though, there is still a small chance that some
bilingual sentences are aligned, and we try to



4377

mine such implicitly aligned sentence pairs and
use them as supervision for NMT model training.

Many works rely on aligned sentences or
bilingual dictionaries to mine the sentence pairs
(Adafre and De Rijke, 2006; Yasuda and Sumita,
2008; Fung and Cheung, 2004). For example, I-
mankulova et al. (2017) extract bilingual sentence
pairs from Wikipedia using a well-trained transla-
tion model learned from supervised sentence pairs.
This method does not work for us since we are un-
der unsupervised scenario and we do not have any
aligned bilingual sentence pairs. Instead, our idea
is to compute the similarity of two sentences us-
ing their cross-lingual sentence embeddings and
choose the pairs with large similarity as aligned
bilingual sentence pairs.

Sentence embedding is widely used to measure
textual similarity in text classification tasks (Arora
et al., 2017; Le and Mikolov, 2014; Wieting et al.,
2015). (Arora et al., 2017) compute the weight-
ed average of the word embedding in one sentence
where the weight depends on word frequency and
then project away the weighted average sentence
embeddings from their first principal componen-
t. This method achieves good performance on a
range of monolingual textual similarity task. We
extend their method from monolingual sentence
embedding to cross-lingual sentence embedding,
given the cross-lingual word embeddings are pre-
trained using MUSE (Conneau et al., 2017). The
detailed method is described as follows.

For each word w, we denote ew as the word
embedding trained from MUSE (Conneau et al.,
2017), p(w) as the estimated frequency from an-
other document and a as a predefined parameter
to calculate the weight of word embedding. We
denote ês as the weighted average sentence em-
bedding of sentence s and E as the embedding
matrix of all the sentences over the monolingual
corpus. Then we remove the first principal compo-
nents u1 ofE for every weighted average sentence
embedding ês. Removing the first principal com-
ponent from the sentence embeddings make them
more diverse and expressive in the embedding s-
pace, and thus the resulted embeddings are more
effective. We use the resulting embedding es as
the final sentence embedding, i.e.,

ês =
∑
w∈s

a

a+ p(w)
ew,

u1 ← PCA(E),
es = ês − u1uT1 ês.

Based on the sentence embedding, we estimate the
similarity between two sentences in different lan-
guages by their cosine similarity sim(sX , sY ) =
<e

sX
,e

sY
>

‖e
sX
‖‖e

sY
‖ . For each weakly aligned document

pair (dXi , d
Y
i ), we have n

X
i ×nYi pairs of sentences

and form a bipartite graph between sentences in
two documents where the weight of an edge be-
tween two cross-lingual sentences is their cosine
similarity score. The goal is to find the most con-
fident edges (sentence pairs) from this weighted
bipartite graph. We adopt a greedy selection ap-
proach with two constraints: The first constraint is
that the weight of a selected edge must be larger
than threshold c1, which is to ensure that the t-
wo sentences are similar enough. The second con-
straint is that the weight of a selected edge must
be larger than the weights of all other edges con-
nected to these two nodes (sentences) by threshold
c2. This ensures that the pair we selected is unique
enough. Denote S = {(sXj , sYj )} as the set of s-
elected sentence pairs. We use those pairs as su-
pervision for model training, i.e., minimizing the
negative log-likelihood as below.

Lp(S; θ) = −
1

|S|
∑

(sX ,sY )∈S

logPX→Y (s
Y |sX ; θ)

− 1
|S|

∑
(sX ,sY )∈S

logPY→X(s
X |sY ; θ).

2.2 Aligning Topic Distribution
Although cross-lingual linked Wikipedia pages are
not aligned in sentences, they are usually aligned
in topic distribution because they talk about the
same event or entity. For example, the English top-
ical words “politician”, “United States” and “pres-
ident” will appear in the English page for “Donald
Trump”, and similar topical words in French will
appear in the corresponding French page. That is,
if we translate an article from English to French
sentence-by-sentence, the word distribution of the
translated article should be generally similar to
the word distribution of the corresponding article
in French. Here we leverage the document-level
word-distribution alignment to enhance and regu-
larize the training of a translation model.

Given an NMT model, we first translate a docu-
ment dXi by sentence translation and obtain a doc-
ument d̄Yi . Then we evaluate the word distribu-
tions between the generated document d̄Yi and the
ground-truth document dYi , and use such signal to
optimize the model. However, straight-forward



4378

loss design, e.g., KL-divergence or Wasserstein
distance between the word distributions of d̄Yi and
dYi is not differentiable w.r.t. the NMT model, due
to the non-differentiable operation (greedy search
or beam search) while generating d̄Yi .

To address this challenge, we need to design
some loss function that is smooth with respect to
the model parameters. Our proposal is assuming
each generated sentence ŝYi,k ∈ d̄Yi is fixed and
“refeeding” the pair (sXi,k, ŝ

Y
i,k) to the model to get

the probability distribution over all the words at
each position. We calculate the word distribution
by averaging word probability distributions over
all sentences and positions of the generated docu-
ment. Mathematically, we have

P (wYi,k,t|sXi,k, ŝYi,k,<t) ∼ PX→Y (wYt |sXi,k, ŝYi,k,<t; θ),

P (wY ; dXi , θ) ∝
∑
i,k,t

P (wt|ŝYi,k,<t),

where P (wY ; dXi , θ) “acts” as the model-
generated word distribution for document dXi .
Note that this averaged distribution is dif-
ferentiable to model parameters. The word
distribution of the ground-truth document dYi
is P (wY ; dYi ) =

#w in dYi
#token in dYi

. We simply use
KL-divergence loss as the objective. Then the
document alignment loss for X → Y translation
is defined as

Ld(D; θ,X → Y ) =
1

|D|
∑

(dXi ,d
Y
i )∈D

KL(P (wY ; dYi )||P (wY ; dXi , θ)).

The corresponding document loss for Y → X
translation can be defined in the same way. The
final loss is as follows, which can be optimized
using backpropagation thanks to its smoothness.

Ld(D; θ) = Ld(D; θ,X → Y )+Ld(D; θ, Y → X).

2.3 Overall algorithm
In addition to the above two ways of using
Wikipedia data, the sentences in the weakly paired
documents can be used as monolingual data to
optimize the losses of the unsupervised machine
translation. Therefore, our proposed losses can be
combined with the loss functions of unsupervised
machine translation. Here we first recap unsuper-
vised machine translation (Lample et al., 2017),
and then present our algorithm in Algorithm 1.

The unsupervised machine translation
considers two loss functions. Given a
monolingual sentence s in language X/Y ,
the denoising auto-encoder loss is defined
as Ldae = −Es∼X [logP (s|c(s); θ)] −
Es∼Y [logP (s|c(s); θ)], where c(.) is to drop
and swap words in sentence s. As for the
reconstruction loss, given s in language X/Y
and the translated sentence s′ in Y/X by model
PX→Y /PY→X , the reconstruction loss is de-
fined as Lrec = −Es∼X [logPY→X(s|s′; θ)] −
Es∼Y [logPX→Y (s|s′; θ)].

Denote the combination of the monolingual da-
ta in languageX and Y asM . We define the over-
all loss on monolingual data M as Lm(M ; θ) =
Ldae +Lrec. Finally, the overall training objective
of our algorithm is to minimize the following loss
function with hyperparameters α and β:

L = Lm(M ; θ) + αLp(S; θ) + βLd(D; θ).

Algorithm 1 Training Algorithm
Require: Initial translation model with parameter

θ; monolingual dataset M , implicitly aligned
sentence pairs dataset S, weakly paired docu-
ments dataset D; optimizer Opt

1: while not converged do
2: Randomly sample a mini-batch monolin-

gual sentences from M , implicitly aligned
sentence pairs from S and weakly paired
documents from D

3: Calculate loss Lm, Lp and Ld
4: Update θ by minimizing the overall objec-

tive L using optimizer Opt
5: end while

3 Experiments

We test our method on several benchmark trans-
lation tasks. We first describe the data prepara-
tion and experimental design, and then present the
main results, followed by some deep studies.

3.1 Data preparation
Wikimedia offers free copies of all available con-
tents on Wikipedia for multiple languages. We
download the language specific Wikipedia con-
tents1, and use WikiExtractor2 to extract and clean

1For example, we download German Wikipedia contents
from https://dumps.wikimedia.org/dewiki/.

2https://github.com/attardi/
wikiextractor

https://dumps.wikimedia.org/dewiki/
https://github.com/attardi/wikiextractor
https://github.com/attardi/wikiextractor


4379

Language #Wiki Documents

English 5,684,240
German 2,201,782
Spanish 1,389,469

Romanian 387,627

Task #Document Pairs

English-German 948,631
English-Spanish 836,564

English-Romanian 87,289

Table 1: Statistics of Wikipedia data, including num-
bers of documents and weakly paired documents.

the texts. The numbers of Wikipedia documents
are listed in Table 1. We then use the sentence to-
kenizer from toolkit NLTK to generate segmented
sentences from Wikipedia documents.

Many Wikipedia pages contain external links to
the pages that describe the same entity but in dif-
ferent languages. We extract weakly paired doc-
uments using these external links. We filter out a
document pair if any document in the pair contain-
s less than 5 sentences. We also remove the sen-
tences longer than 100 words. We conduct exper-
iments on three language pairs: English-German
(En-De), English-Spanish (En-Es) and English-
Romanian (En-Ro) translations. The former two
language pairs are indeed high-resource language
pairs while the last is a low-resource language pair.
Statistics of the processed Wikipedia documents
are provided in Table 1.

We use the monolingual data as in Lample et al.
(2017, 2018) together with Wikipedia document
pairs to train NMT models. For the En-De task, we
use all available sentences from the WMT mono-
lingual News Crawl datasets from year 2007 to
2017 containing about 50 million sentences for
each language. For Romanian, it has more than 2
million sentences. For En-Es, we use News Crawl
datasets from year 2007 to 2012 containing about
10 million sentences. The translation models are
evaluated on newstest 2016 dataset for En-De and
En-Ro, and newstest 2013 dataset for En-Es which
are widely used (Koehn and Knowles, 2017).

3.2 Experimental Design

To mine implicitly aligned sentences from weak-
ly paired documents, we use the open-sourced
word embeddings trained by Fasttext (Joulin et al.,
2016) and use MUSE3 to build cross-lingual word

3https://github.com/facebookresearch/
MUSE

embeddings. We then generate sentence embed-
dings with the weighted average of cross-lingual
word embeddings and further remove the top-1
principal components of sentence embedding ma-
trix as introduced in Section 2.1. We set the two
thresholds c1 = 0.7 and c2 = 0.1 respectively
when selecting sentence pairs, and the parameter
a to calculate the weight of word embedding is
0.001. To translate a document d̄Xi for topic align-
ment, we use greedy search.

For the training of translation models, the
monolingual datasets, Wikipedia document pairs
and mined sentence pairs are jointly processed by
BPE (Sennrich et al., 2016b) with 60, 000 codes.
For model initialization, we follow (Lample et al.,
2018), which uses cross-lingual BPE embeddings
to initialize the shared lookup tables, and the
cross-lingual BPE embeddings are trained by fast-
Text with embedding dimension 512, a context
windows of size 5 and 10 negative samples. We
adopt the Transformer (Vaswani et al., 2017) ar-
chitecture in our experiments. We stack 6 layers
in both the encoder and the decoder. Following
(Lample et al., 2018), we share the lookup tables
between the encoder and the encoder, and between
the source and target languages. The dimension
of the hidden state is 512. The weights α and β
of the loss functions are set to be 1 and 0.05. For
training, we use Adam optimizer (Kingma and Ba,
2014) and the same learning rate scheduler as used
in (Vaswani et al., 2017). For decoding, we use
beam search with beam width 4 and length penal-
ty 0.6, and report the case-sensitive BLEU4 score.

3.3 Main Results
Our method is compared with several previous
works (Lample et al., 2018, 2017; Yang et al.,
2018) in Table 2. We also consider some sim-
ple and heuristic ways of using the weakly paired
documents as baselines. One baseline, referred
as “NMT + First Wikipedia Sentence”, is to di-
rectly use the first sentence of the aligned docu-
ments as an aligned sentence pair to train NMT
model. The motivation behind it is that usually
the first sentence of a Wikipedia document sum-
marizes the main content of the document, which
is more likely to be similar across languages. The
second baseline, referred as “NMT + Document
Translation”, is to treat the weakly aligned doc-

4https://github.com/moses-smt/
mosesdecoder/blob/master/scripts/
generic/multi-bleu.perl

https://github.com/facebookresearch/MUSE
https://github.com/facebookresearch/MUSE
https://github.com/moses-smt/mosesdecoder/blob/master/scripts/generic/multi-bleu.perl
https://github.com/moses-smt/mosesdecoder/blob/master/scripts/generic/multi-bleu.perl
https://github.com/moses-smt/mosesdecoder/blob/master/scripts/generic/multi-bleu.perl


4380

Unsupervised Method En→De De→En En→Es Es→En En→Ro Ro→En

Lample et al. (2017) 9.6 13.3 - - - -
Yang et al. (2018) 10.9 14.6 - - - -

NMT (Lample et al., 2018) 17.2 21.0 19.7 20.0 21.2 19.5
PBSMT (Lample et al., 2018) 17.9 22.9 - - 22.0 23.7
PBSMT + NMT (Lample et al., 2018) 20.2 25.2 - - 25.1 23.9

NMT + First Wiki Sentence 16.3 19.3 17.3 18.3 19.4 18.1
NMT + Document Translation 12.0 14.9 14.5 15.3 16.8 15.7

Ours 24.2 30.3 28.1 27.6 30.1 27.6

Supervised NMT 33.6 38.2 33.2 32.9 32.8 35.4

Table 2: BLEU scores compared with previous approaches.

uments as two long sentences and use them as a
bilingual sentence pair to train NMT models.

From Table 2, our approach achieves BLEU s-
core of 24.1 and 30.3 on En→De and De→En
translations respectively. The previous best perfor-
mance is achieved by (Lample et al., 2018): com-
bining phrase-based approach and neural machine
translation together, they achieve 20.2 and 25.2
BLEU scores on En→De and De→En translation
with monolingual data only. Our approach outper-
forms their method by more than 4 and 5 BLEU
points on En→De and De→En respectively, with
limited weakly paired sentences and documents.
For the En-Es, we achieve 28.1 and 27.6 BLEU
scores on En→Es and Es→En respectively, with
more than 8 and 7 points improvement over unsu-
pervised Transformer baseline models. Similarly,
for the En-Ro low-resource task, we achieve 30.1
and 27.6 BLEU scores for En→Ro and Ro→En
respectively, which has more than 5 and 4 points
improvement compared with previously reported
results. One interesting point is that there are only
nearly 6k aligned sentence pairs mined by our ap-
proach for En-Ro5, but we can still achieve a large
BLEU improvement. This result can well demon-
strate the effectiveness of our method in real low-
resource settings and the sentences mined by our
approach are in high quality.

For the two heuristic baselines, we find that they
hurt the performance of NMT models with more
than 2 points decrease in terms of the BLEU s-
core. The results indicate that the careful utiliza-
tion of Wikipedia data is important, which can al-
so be verified by the superior performance of our
approach.

Furthermore, we report the supervised result in

5Compared with nearly 200k aligned sentences pairs for
En-De, which can be found in Section 3.5.

the last row of Table 2. The supervised setting is
conducted on the full training set on WMT16 En-
De translation with 4.5 million parallel sentences,
and WMT13 En-Es translation with 3.8 million
parallel sentences. As shown in the table, our
approach takes a big step towards the supervised
result, and reduce the gap between unsupervised
translation and supervised translation up to 50%.

3.4 Discussions

To make a clear comparison on sentence pair ex-
traction, we work similarly to existing works in
our unsupervised scenario.

As introduced before, previous works usually
leverage bilingual pairs to train a model or a classi-
fier, or use a bilingual dictionary to mine sentence
pairs. In our unsupervised scenario, the most re-
lated initial model for selecting pairs is the unsu-
pervised NMT model. Therefore, we first train an
unsupervised NMT model followed Lample et al.
(2018) and use this model to mine the sentence
pairs. Specifically, we conduct following exper-
iments: a). Similarly to Adafre and De Rijke
(2006); Yasuda and Sumita (2008), for each x, we
generate the translation using the unsupervised N-
MT model, and select the most similar sentence to
the translation (w.r.t. BLEU) as paired data. b).
We use the model outputted translation probabili-
ty p(y|x) as the scoring function, as used in Smith
et al. (2010); Munteanu and Marcu (2005), and s-
elect sentence pairs with larger probabilities. As
the unsupervised NMT model is not good enough,
we found the selected sentence pairs were in poor
quality and using such poor data does not work
well. On De→En task, the BLEU score of the
model trained in a) and b) can only reach 23.4 and
19.8 (dropped) respectively. Both show that the
trained unsupervised NMT models are not good



4381

Our Method En→De De→En

with Lp and Ld 24.2 30.3
without Ld 22.9 28.7
without Lp 18.5 23.3

Table 3: Ablation study of our approach.

English-German

c1/c2 0.0 0.1 0.2

0.70 257,947 199,965 132,403
0.75 100,497 84,271 58,814

Table 4: Number of the mined sentence pairs w.r.t. dif-
ferent thresholds for English-German.

as expected.
Besides, we build a bilingual dictionary based

on the titles from cross-lingual Wikipedia pages
which we want to use to mine sentence pairs like
Fung and Cheung (2004) and evaluate the quali-
ty using tool from MUSE. However, the top 1 ac-
curacy of the word translation is poor (< 20%),
which makes it unable to select trustable sentence
pairs using such a weak lexicon. In contrast, by
leveraging our cross-lingual word embedding and
unsupervised sentence representation, the select-
ed sentences are much better (see more in Section
3.5). We believe our findings are important to the
field of unsupervised machine translation.

3.5 Further Studies

In this subsection, we provide an ablation study to
our method, check the performance variance with
respect to the thresholds for mining implicit sen-
tence pairs, and finally present several cases of
mined sentence pairs. The studies are conducted
on the En-De translation pair.

Ablation Study Our method leverages weakly
paired documents in two ways. To better under-
stand the importance of the two ways, we report
results from an ablation study in Table 3. From
the table, we can see that removing the topic align-
ment, the accuracy drops with more than 1 BLEU
points. Without the implicitly aligned sentence
pairs, the accuracy decreases with about 6 BLEU
points. These findings clearly demonstrate that
both the two ways are important, and both con-
tribute to the improvement of translation accuracy.

Impact of Sentence Quality As introduced be-
fore, to control the data quality of mined sentence
pairs, we set two constraints with threshold c1 and

c2. We present the paired sentence number and the
BLEU scores with respect to these two threshold-
s in Table 4 and Figure 1. We vary the threshold
c1 in {0.70, 0.75} and c2 in {0.0, 0.1, 0.2}. For
c1 ≤ 0.70, we observe that the sentence quality is
poor, while for c1 ≥ 0.75, we can only extract few
pairs. As shown in the table and the figure, with
more strict constraint, we obtain fewer sentence
pairs but with higher quality. We can see that there
is a clear trade-off between the data quality and the
data number. A good configuration is c1 = 0.7 and
c2 = 0.1.

Case Study of Mined Sentence Pairs We
present three cases of our selected sentence pairs
in Table 5. We can see that our approach can mine
high-quality pairs, such as the first case, in which
one sentence is a good translation of the other one.
Besides, our method can select interesting paired
sentences with similar, if not the exactly same,
semantics. As shown in the second case, the t-
wo sentences are almost semantically the same,
while the German word “gewählt” (“elected” in
English) is not included in the corresponding En-
glish sentence. Also in the last case, the detailed
description “15 Meter hoch” (which means “15
meters high” in English) in the German sentence is
missed in the English sentence. Although they are
not exact translations, such sentence pairs are still
very helpful for NMT model training, as demon-
strated by the results of our method.

4 Related Work

Using monolingual data to boost the machine
translation performance has attracted a lot of atten-
tion (Gulcehre et al., 2015; Sennrich et al., 2016a;
Zhang and Zong, 2016; Wu et al., 2018; He et al.,
2016; Wu et al., 2017), especially when the bilin-
gual supervision is limited. Sennrich et al. (2016a)
propose the back-translation approach which is
an effective way to augment the training sentence
pairs with ctarget-side monolingual data. He et al.
(2016) leverage both source-side and target-side
monolingual data in a dual learning framework.
However, these methods still require a relative-
ly large amount of labeled bilingual data. Re-
cently, Lample et al. (2017) and Artetxe et al.
(2017) make an initial study of unsupervised ma-
chine translation, in which the model is trained
by the monolingual data only. Lample et al.
(2017) leverage two key components to learn un-
supervised translation models: 1) suitable initial-



4382

0.0 0.1 0.2
c2

18

20

22

24

26

28
En De, c1 = 0.70
En De, c1 = 0.75

(a) En→De BLEU performance.

0.0 0.1 0.2
c2

24

26

28

30

32

34
De En, c1 = 0.70
De En, c1 = 0.75

(b) De→En BLEU performance.

Figure 1: BLEU performance w.r.t. different thresholds for English-German translation pair.

English There are several different types of roles to be used in different situations .
German Es gibt verschiedene Arten von Rollen , die in unterschiedlichen Situationen eingesetzt werden .

English He was a member of the Swedish Academy from 1912 .
German 1912 wurde er zum Mitglied der Schwedischen Akademie gewählt .

English The statue and its marble base stand tall .
German Die Statue ist , mit ihrem Sockel aus Marmor , 15 Meter hoch .

Table 5: Cases-studies of the sentence pairs mined by our approach for English-German.

ization of the translation model by cross-lingual
word embeddings, 2) denoising auto-encoder as
language model and reconstruction loss based
on translation-back-translation. Both works only
leverage monolingual sentences without quite rich
weakly paired documents from Web.

We study how to leverage document pairs
for training without supervised bilingual sentence
pairs from Wikipedia. Leveraging the free online
Wikipedia database as an additional source to im-
prove the natural language processing tasks has
attracted interest in recent years. For example,
Conneau et al. (2017) show that word translation
can be effectively learned based on the embedding
trained from Wikipedia corpora. This embedding
further becomes one of the key components for
unsupervised machine translation. Different from
using Wikipedia to train warm-start word embed-
dings, we aim to leverage more and stronger sig-
nals from such weakly paired documents to train
translation model. Hálek et al. (2011) use the cat-
egory information in Wikipedia corpus to improve
the translation of named entities. Drexler et al.
(2014) incorporate language models from target
language documents that are comparable to the
source documents in Wikipedia pages to improve
the document translation.

There are several works aim at extracting po-
tential sentence pairs from comparable corpus, but
most of them rely on a set of bilingual sentence
pairs to train a model or obtain a bilingual lexi-
con and use this model/lexicon to select sentence
pairs. For example, Adafre and De Rijke (2006)
and Yasuda and Sumita (2008) use a strong ma-
chine translation system to obtain a rough transla-
tion of a given page in one language into another,
and then calculate word overlap or BLEU score
between sentences as measure. Smith et al. (2010)
and Munteanu and Marcu (2005) develop a rank-
ing model/binary classifier to learn how likely a
sentence in target language is a translation of the
source language using parallel corpora. (Fung and
Cheung, 2004) use a small set of parallel corpora
to initialize their EM lexical learning and further
use this lexicon to iteratively mine sentence pairs.
However, in our unsupervised scenario, we have
no bilingual sentence pairs to train such a model
or lexicon to further select new sentence pairs.

Our work is also related to document trans-
lation (Tu et al., 2018) but with different goal-
s and settings. The goal of document translation
is to enhance sentence translation with stronger
signals beyond sentences by using richer input-
s (e.g., the topic information from the documen-



4383

t that contains this sentence). During training, it
takes one sentence as well as the cross-sentence
(document-level) information as input and predict-
s the ground-truth translation sentence in other
languages. Therefore, training a document trans-
lation model requires bilingual sentence pairs and
their surrounding contexts. In our scenario, our
goal is to learn a sentence translation model with
weaker signals than sentence pairs. We target
to extract useful information from weakly paired
documents to train a translation model without
human-labeled bilingual data.

5 Conclusion and Future Work

In this work, we propose a general method to train
machine translation models using weakly paired
bilingual documents from Web, e.g., Wikipedia.
Our approach contains two key components: min-
ing implicitly aligned sentence pairs and aligning
topic distributions. Experiments on public bench-
marks verify the effectiveness of our method.

For future work, we will apply our method to
more other language pairs. Besides, we will study
using weakly paired documents from other data re-
sources, such as news websites. Furthermore, we
will investigate better ways to utilize such weakly
paired documents, going beyond mining sentence
pairs and aligning topic distributions.

References
Sisay Fissaha Adafre and Maarten De Rijke. 2006.

Finding similar sentences across multiple languages
in wikipedia. In Proceedings of the Workshop on
NEW TEXT Wikis and blogs and other dynamic text
sources.

Sanjeev Arora, Yingyu Liang, and Tengyu Ma. 2017.
A simple but tough-to-beat baseline for sentence em-
beddings. In ICLR.

Mikel Artetxe, Gorka Labaka, Eneko Agirre, and
Kyunghyun Cho. 2017. Unsupervised neural ma-
chine translation. arXiv preprint arXiv:1710.11041.

Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Ben-
gio. 2015. Neural machine translation by jointly
learning to align and translate. ICLR.

Chaitanya Chemudugunta, Padhraic Smyth, and Mark
Steyvers. 2007. Modeling general and specific as-
pects of documents with a probabilistic topic model.
In Advances in neural information processing sys-
tems, pages 241–248.

Kyunghyun Cho, Bart van Merrienboer, Caglar Gul-
cehre, Dzmitry Bahdanau, Fethi Bougares, Holger

Schwenk, and Yoshua Bengio. 2014. Learning
phrase representations using rnn encoder–decoder
for statistical machine translation. In EMNLP, pages
1724–1734. Association for Computational Linguis-
tics.

Alexis Conneau, Guillaume Lample, Marc’Aurelio
Ranzato, Ludovic Denoyer, and Hervé Jégou. 2017.
Word translation without parallel data. arXiv
preprint arXiv:1710.04087.

Jennifer Drexler, Pushpendre Rastogi, Jacqueline
Aguilar, Benjamin Van Durme, and Matt Post. 2014.
A wikipedia-based corpus for contextualized ma-
chine translation. In LREC, pages 3593–3596.

Jianguang Du, Jing Jiang, Dandan Song, and Lejian
Liao. 2015. Topic modeling with document relative
similarities. IJCAI.

Toshiaki Funatsu, Yoichi Tomiura, Emi Ishita, and
Kosuke Furusawa. 2014. Extracting representative
words of a topic determined by latent dirichlet al-
location. in eknow 2014. In The Sixth Internation-
al Conference on Information, Process, and Knowl-
edge Management, pages 112–117.

Pascale Fung and Percy Cheung. 2004. Mining very-
non-parallel corpora: Parallel sentence and lexicon
extraction via bootstrapping and e. In Proceedings
of the 2004 Conference on Empirical Methods in
Natural Language Processing.

Jonas Gehring, Michael Auli, David Grangier, Denis
Yarats, and Yann N Dauphin. 2017. Convolutional
sequence to sequence learning. arXiv preprint arX-
iv:1705.03122.

Caglar Gulcehre, Orhan Firat, Kelvin Xu, Kyunghyun
Cho, Loic Barrault, Huei-Chi Lin, Fethi Bougares,
Holger Schwenk, and Yoshua Bengio. 2015. On us-
ing monolingual corpora in neural machine transla-
tion. arXiv preprint arXiv:1503.03535.

Ondrej Hálek, Rudolf Rosa, Ales Tamchyna, and On-
drej Bojar. 2011. Named entities from wikipedia for
machine translation. In ITAT, pages 23–30.

Hany Hassan, Anthony Aue, Chang Chen, Vishal
Chowdhary, Jonathan Clark, Christian Federman-
n, Xuedong Huang, Marcin Junczys-Dowmunt,
William Lewis, Mu Li, et al. 2018. Achieving hu-
man parity on automatic chinese to english news
translation. arXiv preprint arXiv:1803.05567.

Di He, Yingce Xia, Tao Qin, Liwei Wang, Nenghai Yu,
Tieyan Liu, and Wei-Ying Ma. 2016. Dual learn-
ing for machine translation. In Advances in Neural
Information Processing Systems, pages 820–828.

Aizhan Imankulova, Takayuki Sato, and Mamoru Ko-
machi. 2017. Improving low-resource neural ma-
chine translation with filtered pseudo-parallel cor-
pus. In Proceedings of the 4th Workshop on Asian
Translation (WAT2017), pages 70–78.



4384

Sébastien Jean, Kyunghyun Cho, Roland Memisevic,
and Yoshua Bengio. 2015. On using very large tar-
get vocabulary for neural machine translation. In
ACL-IJCNLP, pages 1–10. Association for Compu-
tational Linguistics.

Armand Joulin, Edouard Grave, Piotr Bojanowski,
Matthijs Douze, Hérve Jégou, and Tomas Mikolov.
2016. Fasttext.zip: Compressing text classification
models. arXiv preprint arXiv:1612.03651.

Diederik P Kingma and Jimmy Ba. 2014. Adam: A
method for stochastic optimization. arXiv preprint
arXiv:1412.6980.

Philipp Koehn and Rebecca Knowles. 2017. Six chal-
lenges for neural machine translation. In Pro-
ceedings of the First Workshop on Neural Machine
Translation, pages 28–39.

Philipp Koehn, Franz Josef Och, and Daniel Mar-
cu. 2003. Statistical phrase-based translation. In
NAACL-HLT, pages 48–54. Association for Compu-
tational Linguistics.

Guillaume Lample, Ludovic Denoyer, and Mar-
c’Aurelio Ranzato. 2017. Unsupervised machine
translation using monolingual corpora only. arXiv
preprint arXiv:1711.00043.

Guillaume Lample, Myle Ott, Alexis Conneau, Lu-
dovic Denoyer, and Marc’Aurelio Ranzato. 2018.
Phrase-based & neural unsupervised machine trans-
lation. arXiv preprint arXiv:1804.07755.

Quoc Le and Tomas Mikolov. 2014. Distributed rep-
resentations of sentences and documents. In Inter-
national Conference on Machine Learning, pages
1188–1196.

Dragos Stefan Munteanu and Daniel Marcu. 2005. Im-
proving machine translation performance by exploit-
ing non-parallel corpora. Computational Linguistic-
s, 31(4):477–504.

Gabriel Pedrosa, Marcelo Pita, Paulo Bicalho, Anisio
Lacerda, and Gisele L Pappa. 2016. Topic modeling
for short texts with co-occurrence frequency-based
expansion. In 2016 5th Brazilian Conference on In-
telligent Systems (BRACIS), pages 277–282. IEEE.

James Petterson, Wray Buntine, Shravan M Narayana-
murthy, Tibério S Caetano, and Alex J Smola. 2010.
Word features for latent dirichlet allocation. In Ad-
vances in Neural Information Processing Systems,
pages 1921–1929.

Rico Sennrich, Barry Haddow, and Alexandra Birch.
2016a. Improving neural machine translation mod-
els with monolingual data. In ACL.

Rico Sennrich, Barry Haddow, and Alexandra Birch.
2016b. Neural machine translation of rare words
with subword units. In ACL, volume 1, pages 1715–
1725.

Jason R Smith, Chris Quirk, and Kristina Toutanova.
2010. Extracting parallel sentences from compa-
rable corpora using document level alignment. In
NAACL-HLT, pages 403–411. Association for Com-
putational Linguistics.

Ilya Sutskever, Oriol Vinyals, and Quoc V Le. 2014.
Sequence to sequence learning with neural network-
s. In Advances in neural information processing sys-
tems, pages 3104–3112.

Zhaopeng Tu, Yang Liu, Shuming Shi, and Tong
Zhang. 2018. Learning to remember translation his-
tory with a continuous cache. Transactions of the
Association of Computational Linguistics, 6:407–
420.

Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob
Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz
Kaiser, and Illia Polosukhin. 2017. Attention is all
you need. In Advances in Neural Information Pro-
cessing Systems, pages 6000–6010.

John Wieting, Mohit Bansal, Kevin Gimpel, Karen
Livescu, and Dan Roth. 2015. From paraphrase
database to compositional paraphrase model and
back. arXiv preprint arXiv:1506.03487.

Lijun Wu, Fei Tian, Tao Qin, Jianhuang Lai, and Tie-
Yan Liu. 2018. A study of reinforcement learning
for neural machine translation. In Proceedings of
the 2018 Conference on Empirical Methods in Nat-
ural Language Processing, pages 3612–3621.

Lijun Wu, Li Zhao, Tao Qin, Jianhuang Lai, and Tie-
Yan Liu. 2017. Sequence prediction with unlabeled
data by reward function learning. In Proceedings of
the 26th International Joint Conference on Artificial
Intelligence, pages 3098–3104. AAAI Press.

Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V
Le, Mohammad Norouzi, Wolfgang Macherey,
Maxim Krikun, Yuan Cao, Qin Gao, Klaus
Macherey, et al. 2016. Google’s neural machine
translation system: Bridging the gap between human
and machine translation. In Transactions of the As-
sociation for Computational Linguistics.

Zhen Yang, Wei Chen, Feng Wang, and Bo Xu.
2018. Unsupervised neural machine translation with
weight sharing. In Proceedings of the 56th Annual
Meeting of the Association for Computational Lin-
guistics (Volume 1: Long Papers), pages 46–55.

Keiji Yasuda and Eiichiro Sumita. 2008. Method for
building sentence-aligned corpus from wikipedia. In
2008 AAAI Workshop on Wikipedia and Artificial In-
telligence (WikiAI08), pages 263–268.

Jiajun Zhang and Chengqing Zong. 2016. Exploit-
ing source-side monolingual data in neural machine
translation. In EMNLP, pages 1535–1545.


