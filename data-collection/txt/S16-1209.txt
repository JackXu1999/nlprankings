



















































MSejrKu at SemEval-2016 Task 14: Taxonomy Enrichment by Evidence Ranking


Proceedings of SemEval-2016, pages 1337–1341,
San Diego, California, June 16-17, 2016. c©2016 Association for Computational Linguistics

MSejrKu at SemEval-2016 Task 14: Taxonomy Enrichment by Evidence
Ranking

Michael Sejr Schlichtkrull
University of Copenhagen (Denmark)
michael.sejr@gmail.com

Héctor Martı́nez Alonso
Univ. Paris 7 - INRIA (France)

hector.martinez-alonso@inria.fr

Abstract

Automatic enrichment of semantic tax-
onomies with novel data is a relatively
unexplored task with potential benefits in a
broad array of natural language processing
problems. Task 14 of SemEval 2016 poses
the challenge of designing systems for this
task. In this paper, we describe and evaluate
several machine learning systems constructed
for our participation in the competition. We
demonstrate an f1-score of 0.680 for our
submitted systems — a small improvement
over the 0.679 produced by the hard baseline.

1 Introduction

This article describes our systems submitted for Se-
mEval2016, task 14 on taxonomy enrichment. 1 The
two submitted runs are based on Gaussian-kernel
SVMs, and fall respectively under the constrained
and the unconstrained condition of the shared task,
namely using only WordNet and the training data, or
incorporating outside sources. We chose our submit-
ted models by evaluation on the trial data in lieu of
a development set. Our runs perform better than the
very hard baseline with a small 2% margin, while
the best-performing heuristic outperforms the base-
line by a 5% margin.

2 Related Work

Existing methods for taxonomy enrichment can
roughly be divided into two categories: relying on
alignment between multiple taxonomies, or relying
on machine learning-based rating of subgraphs.

1http://alt.qcri.org/semeval2016/task14/

In (Jurgens and Pilehvar, 2015), Wordnet is ex-
tended with technical terms and rare lemmas from
Wiktionary. In (Suchanek et al., 2007), relation-
extraction is used to unify WordNet and Wikipedia.
(Navigli and Ponzetto, 2012) further automates the
alignment process itself. In (Toral et al., 2008),
named entities are brought from Wikipedia to Word-
Net through pattern matching.

In (Snow et al., 2006), the probabilities of tax-
onomies are evaluated based on evidence vectors as-
sociated to edges. A similar approach formulated
in terms of factor graphs can be seen in (Bansal et
al., 2013). Finally, (Yamada et al., 2011) employ a
hybrid strategy, scoring edges by likelihood of ap-
pearance in Wikipedia. Our approach also falls in
the second category, relying on a machine learning
process to rank hypernym-hyponym edges.

3 Data

There are three data splits: train, trial, and test, de-
scribed in Table 1. We use the trial data for model
selection. Notice that attach-actions represent the
vast majority of instances. We therefore focus ex-
clusively on finding correct integrations and disre-
gard the merge-attach distinction.

Several observations can be made from the distri-
bution of the labels in the training set. First, we de-
fine the closest synset sc(t, g) of a term t with gold
standard g as the synset such that a lemma of that
synset is represented in some description sentence
d ∈ D(t) and the shortest path δ(g, sc(t, g)) in the
taxonomy is minimized. Here, D(t) is the set of de-
scription sentences corresponding to t. In Figure 1,
we plot δ(g, sc(t, g)) versus number of instances.

1337



Dataset # Nouns # Verbs # Total Sentences / instance Tokens / sentence # Attach # Merge
Train 349 51 400 1.69 17.73 367 33
Trial 93 34 127 1.02 12.18 71 56
Test 512 82 600 1.21 18.45 569 31

Table 1: Overview of the datasets, showing the composition of nouns and verbs, merge-action and attach-actions, and the mean
number of description sentences per term and number of tokens per description sentence.

0 1 2 3 4
0

100

200

300

400

225

67

28
13 7

δ(g, sc(t, g))

#
In

st
an

ce
s

Figure 1: Number of instances in the training set such that
δ(g, sc(t, g)) corresponds respectively to 0, 1, 2, 3, and 4.

As is apparent from the Zipfian-like distribution,
sc constitutes an excellent guess for g. Defining
wc(t, d, g) for each d ∈ D(t) in similar fashion as
the synset minimizing δ(g, wc(t, g)), an analogous
observation can be made for each description sen-
tence. The result can be seen in Figure 2 below:

0 1 2 3 4
0

200

400

600

276

97
42 25

50

δ(g, wc(t, d, g))

#
Se

nt
en

ce
s

Figure 2: Number of sentences in the training set such that
δ(g, wc(t, d, g)) corresponds respectively to 0, 1, 2, 3, and 4.

We again observe a Zipfian-like distribution, sug-
gesting that each wc(t, d, g) represents a good can-
didate for sc(t, g). As terms often have multiple de-
scription sentences, we therefore have for each term
several good guesses for sc(t, g) and therefore g.
These observations form the basis of the strategies
discussed in Section 5.2.

4 Features

Our systems use both lexical and syntactic fea-
tures, with distributional features included in the
second run. As a first step, we POS-tag and depen-
dency parse the description sentences using the Mate
parser in (Bohnet et al., 2013).2 Additionally, we
apply the unsupervised word sense disambiguation
algorithm described in (Agirre and Soroa, 2009).3

4.1 Constrained Features
Description words are represented through features
based on position, word shape, morphology, POS-
tag, and dependency structure. Word senses are in-
corporated through the depths defined in (Devitt and
Vogel, 2004). Morphological and shape features
derived from the candidate term are also included,
along with a binary feature representing the descrip-
tion word appearing in the target term.

For the direct classification strategies, we also use
features derived from the candidate synset. These
include POS-tag, overlap between synset- and term-
description, and the length of the shortest path to the
description. For the ranking systems, we also utilize
pairwise features: the relative distance and position
in the description sentence, the relative distance and
position in the dependency tree, and the difference
in number of overlapping lemmas between the de-
scription sentence and the most likely senses.

4.2 Embedding Features
We extend the features defined above with Skip-
Gram embeddings as discussed in (Mikolov et al.,
2013). We train the Gensim model (Řehůřek and So-
jka, 2010) on a corpus consisting of the description
sentences and the Wikipedia entries for each term,
padded with the English part of the PolyGlot corpus
presented in (Al-Rfou et al., 2013).

2Available at: https://code.google.com/archive/p/mate-
tools/wikis/ParserAndModels.wiki .

3Available at: http://ixa2.si.ehu.es/ukb/ .

1338



Word-level features are extended with embed-
dings for the word itself and the dependency head.
Term-level features are extended with the sum of
the embedding vectors of each word in the n-gram.
Synset-level features are extended with the mean of
the embedding vectors for each lemma. Finally, all
pairwise features are extended with cosine distances.

5 Experiments

In this section, we explain the strategies we at-
tempted. In all cases, we discount the merge action,
focusing only on attaching.

5.1 Direct Classification

For comparison, we first attempt direct classifica-
tion: Given a term and a candidate synset, predict
whether that synset is a correct integration for the
term. Enrichment is done by ranking all synsets ac-
cording to prediction probability. We tried a linear
logistic regression classifier, and a non-linear neural
network classifier with a single hidden layer contain-
ing 100 units.

5.2 Right Word Classifier

As shown in Section 3, one can narrow down the
search space by a large margin through finding
wc(t, d, g). We propose to determine wc through
a machine learning process. Then, we estimate
sc(t, g) by taking a majority vote over sentences. We
experiment with four versions of the strategy.

We consider letting each sentence vote for the
most likely sense of the best candidate forwc(t, d, g)
with a weight of 1. Secondly, we consider including
votes for the second-best candidate and the second-
most likely senses, weighted by hyperparameters α
and β. We determine the values for α and β through
crossvalidation on the training set. We perform word
sense disambiguation either through most frequent
sense, or through Personalized Pagerank.

Ranking problems can be approached as point-
wise regression, or as pairwise classification. Rank-
ing by classification probability as discussed in the
previous section corresponds to a pointwise regres-
sion with logistic loss. Following (Hang, 2011),
transforming a pointwise problem into a pairwise
problem can improve performance, especially when
training data is scarce.

To transform the training set into a pairwise train-
ing set Lpairwise, we combine for each description
sentence d with content words of appropriate POS-
tag Wt,d every pair (w1, w2) ∈ Wt,d × Wt,d such
that one equals wc(t, d, g) and the other does not.
If w1 is the match and w2 is not, we add (w1, w2)
to Lpairwise with the positive label. Else, we add
(w1, w2) to Lpairwise with the negative label. We
train a classifier on this dataset, observing that a re-
lation ≺t,d is induced as positive and negative pre-
dictions on w1 and w2 correspond respectively to
w1 �t,d w2 and w1 ≺t,d w2.

We experiment with various classifiers and pa-
rameter settings through cross-validation. As
(w1, w2) ∈ Lpairwise ⇔ (w2, w1) ∈ Lpairwise, the
dataset is balanced and chance performs at an accu-
racy of 0.5. The best-performing classifier was the
Gaussian kernel SVM, with an accuracy of 0.81.

Through this classification, we learn for each term
t a relation ≺t,d on the words w ∈ Wt,d of each de-
scription sentence d ∈ D(t). This relation, however,
is asymmetric and therefore does not constitute an
ordering. We therefore define a new ordering ≺∗t,d
averaging over both directions of ≺t,d:

p(w1 ≺∗t,d w2) =
p(w1 ≺t,d w2) + p(w2 �t,d w1))

2

We then let w1 ≺∗t,d w2 if and only if p(w1 ≺∗t,d
w2) > 0.5. As ≺∗t,d is an ordering on the words of
each description sentence, we let the largest element
with respect to ≺∗t,d be our guess for wc.

6 Results

The results for each system can be seen in Table 2.
We provide results on the training data, the test data,
and the trial data which was used as a development
set in the model selection process. Apart from the
submitted runs, all systems have a perfect recall of
1.0. For the submitted runs, a software error caused
recall to drop to 0.97 as description words contain-
ing underscores were incorrectly processed.

The best-performing system on the trial data un-
der both conditions is the vote-based ranking with-
out sense disambiguation, outperforming the first
word, first sense-baseline by a small margin. Fol-
lowing this observation, we chose to submit the con-
strained and unconstrained variants of this system -
marked in bold in Table 2.

1339



System Train Trial Test
F1 Lemma M. F1 Lemma M. F1 Lemma M.

Random 0.35 0.00 0.39 0.00 0.37 0.00
First-word fist-sense 0.64 0.33 0.66 0.25 0.68 0.42
Logistic Regression 0.53 0.17 0.46 0.15 0.47 0.15
NN-classification 0.53 0.18 0.46 0.14 0.50 0.17
Rank-FS 0.69 0.43 0.66 0.28 0.68 0.42
Rank-WSD 0.72 0.42 0.65 0.29 0.70 0.42
Rank-vote-FS 0.70 0.42 0.67 0.27 0.67 0.43
Rank-vote-WSD 0.72 0.43 0.66 0.27 0.70 0.41
Rank-embed-FS 0.69 0.39 0.67 0.28 0.68 0.41
Rank-embed-WSD 0.73 0.42 0.67 0.28 0.68 0.42
Rank-embed-vote-FS 0.70 0.42 0.67 0.28 0.68 0.43
Rank-embed-vote-WSD 0.73 0.42 0.67 0.28 0.69 0.41

Table 2: Performance reported as the f1 of Wu & Palmer-score and recall, along with Lemma Matches. Results are shown for the
baselines, the direct classifiers, and the constrained and unconstrained versions of the ranking-based systems. The submitted runs

are marked in bold. Recall is 1.00 for all systems except for the submitted runs, where a software error caused a recall of 0.97.

7 Discussion

Investigating the errors made by the voting-based
first-sense systems, we see a pattern in which hy-
pernyms of the correct integration mistakenly are
chosen. Examples include steroid instead of an-
abolic steroid, drug instead of opiate, and person
instead of woman. Although these errors occur un-
der both conditions, they are slightly more frequent
in the constrained system. In (Levy et al., 2015),
evidence is presented that distributional embeddings
mainly encode hierarchical level rather than hierar-
chical branch. Our findings seem to support this
conclusion. Refining the usage of the embeddings
may well resolve this particular source of error.

In Table 1, we see how the training dataset not
only on the average has more description sentences
per term, but also longer description sentences. As
the Personalized Pagerank algorithm relies on a con-
text created from these sets of tokens, this may be
the source of the poor performance of sense disam-
biguation compared to most frequent sense on the
trial data seen in Table 2.

Similarly, the voting component was observed to
have almost no effect on the test and trial data, while
improving the performance on the training data by a
wide margin. When the number of description sen-
tences is smaller, it follows naturally that the number
of candidates for sc drops, thus reducing the effec-
tiveness of voting.

Examining the difference between the predictions
made by our systems and the baseline, we notice that
no terms exist where we our prediction is equal to
the gold standard, and the baseline is not. Rather, the
improvement demonstrated by our systems comes
in the form of wrongful guesses being closer to the
gold than is the case for the baseline. This is further
supported by the Zipfian observed in Section 3: our
guesses mostly fall within a distance of 0–2 edges of
the gold standard.

8 Conclusion

We have described several strategies for taxonomy
enrichment, showing F1-scores of respectively 0.68
and 0.70 for the submitted and the best-performing
systems. Our results show much better performance
on the training data, even for the unsupervised sense
disambiguation component. We have argued that the
difference in number of description sentences and
number of tokens per description sentence accounts
partially for this difference, and therefore propose
the gathering of additional evidence from outside
sources to be added to the descriptions as the natural
next step for future work. As our system demon-
strates high performance for candidate generation
and low performance for candidate selection, an-
other possibility for future work would be to com-
bine our approach with a filtering strategy in an en-
semble or pipeline.

1340



References

Eneko Agirre and Aitor Soroa. 2009. Personalizing
pagerank for word sense disambiguation. In Proceed-
ings of the 12th Conference of the European Chap-
ter of the Association for Computational Linguistics,
pages 33–41. Association for Computational Linguis-
tics.

Rami Al-Rfou, Bryan Perozzi, and Steven Skiena. 2013.
Polyglot: Distributed word representations for multi-
lingual nlp. In Proceedings of the Seventeenth Confer-
ence on Computational Natural Language Learning,
pages 183–192, Sofia, Bulgaria, August.

Mohit Bansal, David Burkett, Gerard De Melo, and Dan
Klein. 2013. Structured learning for taxonomy induc-
tion with belief propagation.

Bernd Bohnet, Joakim Nivre, Igor Boguslavsky, Richárd
Farkas, Filip Ginter, and Jan Hajič. 2013. Joint mor-
phological and syntactic analysis for richly inflected
languages. Transactions of the Association for Com-
putational Linguistics, 1:415–428.

Ann Devitt and Carl Vogel. 2004. The topology of word-
net: Some metrics.

LI Hang. 2011. A short introduction to learning to rank.
IEICE TRANSACTIONS on Information and Systems,
94(10):1854–1862.

David Jurgens and Mohammad Taher Pilehvar. 2015.
Reserating the awesometastic: An automatic extension
of the wordnet taxonomy for novel terms.

Omer Levy, Steffen Remus, Chris Biemann, Ido Dagan,
and Israel Ramat-Gan. 2015. Do supervised distribu-
tional methods really learn lexical inference relations?

Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Cor-
rado, and Jeff Dean. 2013. Distributed representa-
tions of words and phrases and their compositionality.
In Advances in neural information processing systems,
pages 3111–3119.

Roberto Navigli and Simone Paolo Ponzetto. 2012. Ba-
belnet: The automatic construction, evaluation and
application of a wide-coverage multilingual semantic
network. Artificial Intelligence, 193:217–250.

Radim Řehůřek and Petr Sojka. 2010. Software Frame-
work for Topic Modelling with Large Corpora. In Pro-
ceedings of the LREC 2010 Workshop on New Chal-
lenges for NLP Frameworks, pages 45–50, Valletta,
Malta, May. ELRA.

Rion Snow, Daniel Jurafsky, and Andrew Y Ng. 2006.
Semantic taxonomy induction from heterogenous evi-
dence. In Proceedings of the 21st International Con-
ference on Computational Linguistics and the 44th
annual meeting of the Association for Computational
Linguistics, pages 801–808. Association for Computa-
tional Linguistics.

Fabian M Suchanek, Gjergji Kasneci, and Gerhard
Weikum. 2007. Yago: a core of semantic knowledge.
In Proceedings of the 16th international conference on
World Wide Web, pages 697–706. ACM.

Antonio Toral, Rafael Munoz, and Monica Monachini.
2008. Named entity wordnet.

Ichiro Yamada, Jong-Hoon Oh, Chikara Hashimoto, Ken-
taro Torisawa, Jun’ichi Kazama, Stijn De Saeger, and
Takuya Kawada. 2011. Extending wordnet with hy-
pernyms and siblings acquired from wikipedia. In
IJCNLP, pages 874–882.

1341


