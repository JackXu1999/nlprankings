












































Untitled


Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 1247–1256
Brussels, Belgium, October 31 - November 4, 2018. c©2018 Association for Computational Linguistics

1247

Jointly Multiple Events Extraction via Attention-based Graph
Information Aggregation

Xiao Liu† and Zhunchen Luo‡ and Heyan Huang†∗

†School of Computer Science and Technology, Beijing Institute of Technology

100081 Beijing, China

{xiaoliu,hhy63}@bit.edu.cn
‡Information Research Center of Military Science, PLA Academy of Military Science

100142 Beijing, China

zhunchenluo@gmail.com

Abstract

Event extraction is of practical utility in natu-

ral language processing. In the real world, it

is a common phenomenon that multiple events

existing in the same sentence, where extracting

them are more difficult than extracting a sin-

gle event. Previous works on modeling the as-

sociations between events by sequential mod-

eling methods suffer a lot from the low effi-

ciency in capturing very long-range dependen-

cies. In this paper, we propose a novel Jointly

Multiple Events Extraction (JMEE) frame-

work to jointly extract multiple event trig-

gers and arguments by introducing syntactic

shortcut arcs to enhance information flow and

attention-based graph convolution networks to

model graph information. The experiment re-

sults demonstrate that our proposed frame-

work achieves competitive results compared

with state-of-the-art methods.

1 Introduction

Extracting events from natural language text is

an essential yet challenging task for natural lan-

guage understanding. When given a document,

event extraction systems need to recognize event

triggers with their specific types and their corre-

sponding arguments with the roles. Technically

speaking, as defined by the ACE 2005 dataset1,

a benchmark for event extraction (Grishman et al.,

2005), the event extraction task can be divided into

two subtasks, i.e., event detection (identifying and

classifying event triggers) and argument extraction

(identifying arguments of event triggers and label-

ing their roles).

In event extraction, it is a common phenomenon

that multiple events exist in the same sentence.

Extracting the correct multiple events from those

∗*Corresponding author.
1https://catalog.ldc.upenn.edu/

ldc2006t06

sentences is much more difficult than in the one-

event-one-sentence cases because those various

types of events are often associated with each

other. For example, in the sentence “He left the

company, and planned to go home directly.”, the

trigger word left may trigger a Transport (a person

left a place) event or an End-Position (a person re-

tired from a company) event. However, if we take

the following event triggered by go into consider-

ation, we are more confident to judge it as a Trans-

port event rather than an End-Position event. This

phenomenon is quite common in our real world, as

Injure and Die events are more likely to co-occur

with Attack events than others, whereas Marry and

Born events are less likely to co-occur with Attack

events. As we investigated in ACE 2005 dataset,

there are around 26.2% (1042/3978) sentences be-

long to this category.

Significant efforts have been dedicated to solv-

ing this problem. Most of them exploiting vari-

ous features (Liu et al., 2016b; Yang and Mitchell,

2016; Li et al., 2013; Keith et al., 2017; Liu et al.,

2016a; Li et al., 2015), introducing memory vec-

tors and matrices (Nguyen et al., 2016), introduc-

ing more transition arcs (Sha et al., 2018), keeping

more contextual information (Chen et al., 2015)

into sentence-level sequential modeling methods

like RNNs and CRFs. Some also seek features

in document-level methods (Liao and Grishman,

2010; Ji and Grishman, 2008). However, sentence-

level sequential modeling methods suffer a lot

from the low efficiency in capturing very long-

range dependencies while the feature-based meth-

ods require extensive human engineering, which

also largely affects model performance. Besides,

these methods do not adequately model the asso-

ciations between events.

An intuitive way to alleviate this phenomenon

is to introduce shortcut arcs represented by lin-

guistic resources like dependency parsing trees to



1248

Die Attack

[       ] [                                                                 ]
GEO

PER

[                ]
PER GEO WEA

[      ][                         ]

Victim

Place

Instrument

Agent

Target Attacker

Instrument

Figure 1: An example of dependency parsing result produced by Stanford CoreNLP. There are two events in the

sentence: a Die event triggered by the word killed with four arguments in red and an Attack event triggered by the

word barrage with three arguments in blue. The red dotted arc is the shortcut path consisting of three directed arcs

from trigger killed to another trigger barrage.

drain the information flow from a point to its target

through fewer transitions. Comparing to sequen-

tial order, modeling with these arcs often success-

fully reduce the needed hops from one event trig-

ger to another in the same sentences. In Figure

1, for example, there are two events: a Die event

triggered by the word killed with four arguments

in red and an Attack event triggered by the word

barrage with three arguments in blue. We need

six hops from killed to barrage according to se-

quential order, but only three hops according to the

arcs in dependency parsing tree (along the nmod-

arc from killed to witnesses, along the acl-arc from

witnesses to called, and along the xcomp-arc from

called to barrage). These three arcs consist of a

shortcut path2, draining the dependency syntactic

information flow from killed to barrage with fewer

hops3.

In this paper, we propose a novel Jointly

Multiple Events Extraction (JMEE) framework by

introducing syntactic shortcut arcs to enhance in-

formation flow and attention-based graphic convo-

lution networks to model the graph information.

To implement modeling with the shortcut arcs, we

adopt the graph convolutional networks (GCNs)

(Kipf and Welling, 2016; Marcheggiani and Titov,

2017; Nguyen and Grishman, 2018) to learn syn-

tactic contextual representations of each node by

the representative vectors of its immediate neigh-

bors in the graph. And then we utilize the syn-

tactic contextual representations to extract triggers

and arguments jointly by a self-attention mecha-

nism to aggregate information especially keeping

the associations between multiple events.

2In a shortcut path which consists of existing arcs, some
arcs may reverse their directions.

3The length of the longest path in a tree is always no more
than the sequential length consisting of the same number of
nodes, which means even in the worst cases, the shortcut path
will not perform worse than sequential modeling.

We extensively evaluate the proposed JMEE

framework with the widely-used ACE 2005

dataset to demonstrate its benefits in the experi-

ments especially in capturing the associations be-

tween events. To summary, our contribution in this

work is as follows:

• We propose a novel joint event extraction
framework JMEE based on syntactic struc-

tures which enhance information flow and

alleviate the phenomenon where multiple

events are in the same sentence.

• We propose a self-attention mechanism to ag-
gregate information especially keeping the

associations between multiple events and

prove it is useful in event extraction.

• We achieve the state-of-the-art performance
on the widely used datasets for event extrac-

tion using the proposed model with GCNs

and self-attention mechanism.

2 Approach

Generally, event extraction can be cast as a multi-

class classification problem deciding whether each

word in the sentence forms a part of event trig-

ger candidate and whether each entity in the sen-

tence plays a particular role in the event triggered

by the candidate triggers. There are two main

approaches to event extraction: (i) the joint ap-

proach that extracts event triggers and arguments

simultaneously as a structured prediction problem,

and (ii) the pipelined approach that first performs

trigger prediction and then identifies arguments in

separate stages. We follow the joint approach that

can effectively avoid the propagated errors in the

pipeline.

Additionally, we extract events in sentence-

level mainly for three reasons. Firstly, in our in-



1249



1250

people

in

connection

with

killings

Police

have

arrested

four

the

word POS-

tagging

entity

type

label

positional

Graph 

Convolution 

Network

n
m

o
d

n
m

o
d

su
b
j

self-attention

Trigger 

Classification

Trigger label in BIO

Current 

Word

Argument 

Role 

Labeling

Argument Role Result

Embedding Layer BiLSTM Layer GCN Layer Joint Extraction Layer

Figure 3: The architecture of our jointly multiple events extraction framework.

• The positional embedding vector of wi: If
wc is the current word, we encode the rela-

tive distance i − c from wi to wc as a real-
valued vector by looking up the randomly

initialized position embedding table (Nguyen

et al., 2016; Liu et al., 2017; Nguyen and Gr-

ishman, 2018).

• The entity type label embedding vector of
wi: Similarly to the POS-tagging label em-

bedding vector of wi, we annotate the entity

mentions in a sentence using BIO annotation

schema and transform the entity type labels to

real-valued vectors by looking up the embed-

ding table. It should be noticed that we use

the whole entity extent in ACE 2005 dataset

which contains overlapping entity mentions

and we sum all the possible entity type label

embedding vectors for each token.

The transformation from the token wi to the

vector xi essentially converts the input sentence

W into a sequence of real-valued vectors X =
(x1, x2, ..., xn), which will be feed into later mod-
ules to learn more effective representations for

event extraction.

2.2 Syntactic Graph Convolution Network

Considering an undirected graph G = (V, E)
as the syntactic parsing tree for sentence W ,

where V = v1, v2, ..., vn(|V| = n) and E are
sets of nodes and edges, respectively. In V ,
each vi is the node representing token wi in

W . Each edge (vi, vj) ∈ E is a directed syn-
tactic arc from token wi to token wj , with the

type label K(wi, wj). Additionally, to allow in-
formation to flow against the direction, we also

add reversed edge (vj , vi) with the type label
K ′(wi, wj). Following Kipf and Welling (2016),
we also add all the self-loops, i.e., (vi, vi) for
any vi ∈ V . For example, in the dependency
parsing tree shown in Figure 1, there are four

arcs in the subgraph with only two nodes “killed”

and “witnesses”: the dependency arc with the

type label K(“killed”, “witnesses”) = nmod,
the revresed dependency arc with the additional

type label K(“witnesses”, “killed”) = nmod′,
and the two self-loops of “killed” and “wit-

nesses” with type label K(“killed”, “killed”) =
K(“witnesses”, “witnesses”) = loop.

Therefore, in the k-th layer of syntactic graph

convolution network module, we can calculate the

graph convolution vector h
(k+1)
v for node v ∈ V

by:

h(k+1)v = f(
∑

u∈N (v)

(W
(k)
K(u,v)h

(k)
u + b

(k)
K(u,v))) (1)

where K(u, v) indicates the type label of the edge

(u, v); W
(k)
K(u,v) and b

(k)
K(u,v) are the weight matrix

and the bias for the certain type label K(u, v), re-
spectively; N (v) is the set of neighbors of v in-
cluding v (because of the self-loop); f is the ac-

tivation function. Moreover, we use the output of

the word representation module xi to initialize the

node representation h0vi of the first layer of GCNs.

After applying the above two changes, the num-

ber of predefined directed arc type label (let us say,

N ) will be doubled (to 2N + 1). It means we will

have 2N +1 sets of parameter pairs W
(k)
k and b

(k)
k

for a single layer of GCN. In this work, we use

Stanford Parser (Klein and Manning, 2003) to gen-

erate the arcs in dependency parsing trees for sen-

tences as the shortcut arcs. The current representa-



1251

tion contains approximately 50 different grammat-

ical relations, which is too high for the parameter

number of a single layer of GCN and not compat-

ible with the existing training data scale. To re-

duce the parameter numbers, following Marcheg-

giani and Titov (2017), we modify the definition

of type label K(wi, wj) to:

K(wi, wj) =







along, (vi, vj) ∈ E
rev, i! = j&(vj , vi) ∈ E
loop, i == j

(2)

where the new K(wi, wj) only have three type la-
bels.

As not all types of edges are equally informative

for the downstream task, moreover, there are also

noises in the generated syntactic parsing struc-

tures; we apply gates on the edges to weight their

individual importances. Inspired by Dauphin et al.

(2017); Marcheggiani and Titov (2017), we calcu-

late a weight g
(k)
u,v for each edge (u, v) indicating

the importance for event extraction by:

g(k)u,v = σ(h
(k)
u V

(k)
K(u,v) + d

(k)
K(u,v)) (3)

where σ is the logistic sigmoid function, V
(k)
K(u,v)

and d
(k)
K(u,v) are the weight matrix and the bias of

the gate. With this additional gating mechanism,

the final syntactic GCN computation is formulated

as

h(k+1)v = f(
∑

u∈N (v)

g(k)u,v(W
(k)
K(u,v)h

(k)
u + b

(k)
K(u,v)))

(4)

As stacking k layers of GCNs can model in-

formation in k hops, and sometimes the length of

shortcut path between two triggers is less than k,

to avoid information over-propagating, we adapt

highway units (Srivastava et al., 2015), which al-

low unimpeded information flowing across stack-

ing GCN layers. Typically, highway layers con-

duct nonlinear transformation as:

t = σ(WTh
k
v + bT ) (5)

h
(k+1)
v = h

(k+1)
v +t⊙g(WHh

k
v+bH)+(1−t)⊙h

k
v

(6)

where σ is the sigmoid function; ⊙ is the element-
wise product operation; g is a nonlinear activation

function; t is called transform gate and (1 − t) is

called carry gate. Therefore, the input of the k-th

GCN layers should be h
(k)

instead of h(k).

The GCNs are designed to capture the depen-

dencies between shortcut arcs, while the layer

number of GCNs limits the ability to capture lo-

cal graph information. However, in this cases, we

find that leveraging local sequential context will

help to expand the information flow without in-

creasing the layer number of GCNs, which means

LSTMs and GCNs maybe complementary. There-

fore, instead of feeding the word representation

X = (x1, x2, ..., xn) into the first GCN layer,
we follow Marcheggiani and Titov (2017), apply

Bidirectional LSTM (Bi-LSTM) (Hochreiter and

Schmidhuber, 1997) to encode the the word repre-

sentation X as:

−→p t =
−−−−→
LSTM(−→p t−1, xt) (7)

←−p t =
←−−−−
LSTM(←−p t−1, xt) (8)

and the input of t-th token to GCNs is xt =
[−→p t,

←−p t], where [, ] is the concatenation opera-
tion. The Bi-LSTM adaptively accumulates and

abstracts the context for each token in the sen-

tence.

2.3 Self-Attention Trigger Classification

When taking each token as the current word, we

get the representation D from all tokens calcu-

lated by GCNs. Traditional event extraction sys-

tems often use max-pooling or its amelioration to

aggregate information to each position. However,

the max-pooling aggregation mechanisms tend to

produce similar results after GCN modules in our

framework. For example, if we get the aggregated

vector Agi at each position i by this max-pooling

mechanism Agi = max pooling
n
j=1(Hj) with the

GCNs output {Hj |j = 1, ..., n} in which n is the
sentence length, and the vector Agi is all the same

at each position. Besides, predicting a trigger la-

bel for a token should take other possible trigger

candidates into consideration. To capture the asso-

ciations between triggers in a sentence, we design

a self-attention mechanism to aggregate informa-

tion especially keeping the associations between

multiple events.

Given the current token wi, the self-attention

score vector and the context vector at position i

are calculated as:

score = norm(exp(W2f(W1D+b1)+b2)) (9)



1252

Ci = [
n
∑

j=1,j!=i

scorej ∗Dj , Di] (10)

where norm means the normalization operation.

Then we feed the context vector Ci into a fully-

connected network to predict the trigger label in

BIO annotation schema as:

Ci = f(WcCi + bc) (11)

yti = softmax(WtCi + bt) (12)

where f is a non-linear activation and yti is the

final output of the i-th trigger label.

2.4 Argument Classification

When we have extracted an entire trigger candi-

date, which is meeting an O label after an I-Type

label or a B-Type label, we use the aggregated con-

text vector C to perform argument classification

on the entity list in the sentence.

For each entity-trigger pair, as both the entity

and the trigger candidate are likely to be a subse-

quence of tokens, we aggregate the context vectors

of subsequences to trigger candidate vector Ti and

entity vector Ej by average pooling along the se-

quence length dimension. Then we concatenate

them together and feed into a fully-connected net-

work to predict the argument role as:

yaij = softmax(Wa[Ti, Ej ] + ba) (13)

where yaij is the final output of which role the j-

th entity plays in the event triggered by the i-th

trigger candidate.

When training our framework, if the trigger can-

didate that we focus on is not a correct trigger, we

set all the golden argument labels concerning the

trigger candidate to OTHER (not any roles). With

this setting, the labels of the trigger candidate will

be further adjusted to reach a reasonable probabil-

ity distribution.

2.5 Biased Loss Function

In order to train the networks, we minimize the

joint negative log-likelihood loss function. Due

to the data sparsity in the ACE 2005 dataset, we

adapt our joint negative log-likelihood loss func-

tion by adding a bias item as:

J(θ) = −
N
∑

p=1

(

np
∑

i=1

I(yti)log(p(yti |θ))

+β

tp
∑

i=1

ep
∑

j=1

log(p(yaij |θ)))

(14)

where N is the number of sentences in training

corpus; np, tp and ep are the number of tokens,

extracted trigger candidates and entities of the p-th

sentence; I(yti) is an indicating function, if yti is
not O, it outputs a fixed positive floating number α

bigger than one, otherwise one; β is also a floating

number as a hyper-parameter like α.

3 Experiments

3.1 Experiment Settings

Dataset, Resources and Evaluation Metric

We evaluate our JMEE framework on the ACE

2005 dataset. The ACE 2005 dataset annotate 33

event subtypes and 36 role classes, along with the

NONE class and BIO annotation schema, we will

classify each token into 67 categories in event de-

tection and 37 categories in argument extraction.

To comply with previous work, we use the same

data split as the previous work (Ji and Grishman,

2008; Liao and Grishman, 2010; Li et al., 2013;

Chen et al., 2015; Liu et al., 2016b; Yang and

Mitchell, 2016; Nguyen et al., 2016; Sha et al.,

2018). This data split includes 40 newswire arti-

cles (881 sentences) for the test set, 30 other doc-

uments (1087 sentences) for the development set

and 529 remaining documents (21,090 sentences)

for the training set.

We deploy the Stanford CoreNLP toolkit5 to

preprocess the data, including tokenizing, sen-

tence splitting, pos-tagging and generating depen-

dency parsing trees.

Also, we follow the criteria of the previous work

(Ji and Grishman, 2008; Liao and Grishman, 2010;

Li et al., 2013; Chen et al., 2015; Liu et al., 2016b;

Yang and Mitchell, 2016; Nguyen et al., 2016; Sha

et al., 2018) to judge the correctness of the pre-

dicted event mentions.

Hyperparameter Setting

For all the experiments below, in the word rep-

resentation module, we use 300 dimensions for

the embeddings and 50 dimensions for the rest

5http://stanfordnlp.github.io/CoreNLP/



1253

Method

Trigger Trigger Argument Argument

Identification (%) Classification (%) Identification (%) Role (%)

P R F1 P R F1 P R F1 P R F1
Cross-Event N/A 68.7 68.9 68.8 50.9 49.7 50.3 45.1 44.1 44.6

JointBeam 76.9 65.0 70.4 73.7 62.3 67.5 69.8 47.9 56.8 64.7 44.4 52.7

DMCNN 80.4 67.7 73.5 75.6 63.6 69.1 68.8 51.9 59.1 62.2 46.9 53.5

PSL N/A 75.3 64.4 69.4 N/A N/A

JRNN 68.5 75.7 71.9 66.0 73.0 69.3 61.4 64.2 62.8 54.2 56.7 55.4

dbRNN N/A 74.1 69.8 71.9 71.3 64.5 67.7 66.2 52.8 58.7

JMEE 80.2 72.1 75.9 76.3 71.3 73.7 71.4 65.6 68.4 66.8 54.9 60.3

Table 1: Overall performance comparing to the state-of-the-art methods with golden-standard entities.

three embeddings including pos-tagging embed-

ding, positional embedding and entity type em-

bedding. In the syntactic GCN module, we use a

three-layer GCN, a one-layer Bi-LSTM with 220

hidden units, self-attention with 300 hidden units

and 200 hidden units for the rest transformation.

We also set dropout rate to 0.5 and L2-norm to 1e-

8. The batch size in our experiments is 32, and

we utilize a maximum length n = 50 of sentences
in the experiments by padding shorter sentences

and cutting off longer ones. These hyperparam-

eters are either randomly searched or chosen by

experiences when tuning in the development set.

We use ReLU (Glorot et al., 2011) as our non-

linear activate function. We apply the stochastic

gradient descent algorithm with mini-batches and

the AdaDelta update rule (Zeiler, 2012). The gra-

dients are computed using back-propagation. Dur-

ing training, besides the weight matrices, we also

fine-tune all the embedding tables.

3.2 Overall Performance

We compare our performance with the following

state-of-the-art methods:

1 Cross-Event is proposed by Liao and Grishman

(2010), which uses document level information

to improve the performance of event extraction;

2 JointBeam is the method proposed by Li et al.

(2013), which extracts events based on structure

prediction by manually designed features;

3 DMCNN is proposed by Chen et al. (2015),

which uses dynamic multi-pooling to keep mul-

tiple events’ information;

4 PSL is proposed by Liu et al. (2016b), which

uses a probabilistic reasoning model to classify

events by using latent and global information to

encode the associations between events;

5 JRNN is proposed by Nguyen et al. (2016),

which uses a bidirectional RNN and manually

designed features to jointly extract event trig-

gers and arguments.

6 dbRNN is proposed by Sha et al. (2018), which

adds dependency bridges over Bi-LSTM for

event extraction.

Table 1 shows the overall performance com-

paring to the above state-of-the-art methods with

golden-standard entities. From the table, we can

see that our JMEE framework achieves the best F1
scores for both trigger classification and argument-

related subtasks among all the compared methods.

There is a significant gain with the trigger classi-

fication and argument role labeling performances,

which is 2% higher over the best-reported mod-

els. These results demonstrate the effectivenesses

of our method to incorporate with the graph con-

volution and syntactic shortcut arcs.

3.3 Effect on Extracting Multiple Events

To evaluate the effect of our framework for allevi-

ating the multiple events phenomenon, we divide

the test data into two parts (1/1 and 1/N) follow-

ing Nguyen et al. (2016); Chen et al. (2015) and

perform evaluations separately. 1/1 means that

one sentence only has one trigger or one argu-

ment plays a role in one sentence; otherwise, 1/N

is used.

Table 2 illustrates the performance (F1 scores)

of JRNN (Nguyen et al., 2016), DMCNN (Chen

et al., 2015), the two baseline model Embed-

ding+T and CNN in Chen et al. (2015) and our

framework in trigger classification subtask and

argument role labeling subatsk. Embedding+T

uses word embedding vectors and the traditional

sentence-level features in Li et al. (2013), while



1254



1255

ing the multiple-event phenomenon. In our frame-

work, we introduce syntactic shortcut arcs to en-

hance information flow and adapt the graph convo-

lution network to capture the enhanced representa-

tion. Then a self-attention aggregation mechanism

is applied to aggregate the associations between

events. Besides, we jointly extract event triggers

and arguments by optimizing a biased loss func-

tion due to the imbalances in the dataset. The ex-

periment results demonstrate the effectiveness of

our proposed framework. In the future, we plan

to exploit the information of one argument which

plays different roles in various events to do better

in event extraction task.

Acknowledgments

We would like to thank Yansong Feng, Ying Zeng,

Xiaochi Wei, Qian Liu and Changsen Yuan for

their insightful comments and suggestions. We

also very appreciate the comments from anony-

mous reviewers which will help further improve

our work. This work is supported by National Nat-

ural Science Foundation of China (No. 61751201

and No. 61602490) and National Key R&D Plan

(No. 2017YFB0803302).

References

Yubo Chen, Liheng Xu, Kang Liu, Daojian Zeng,
and Jun Zhao. 2015. Event extraction via dy-
namic multi-pooling convolutional neural networks.
In Proceedings of the 53rd Annual Meeting of the
Association for Computational Linguistics and the
7th International Joint Conference on Natural Lan-
guage Processing of the Asian Federation of Natural
Language Processing, pages 167–176.

Yann N. Dauphin, Angela Fan, Michael Auli, and
David Grangier. 2017. Language modeling with
gated convolutional networks. In Proceedings of the
34th International Conference on Machine Learn-
ing, pages 933–941.

Xiaocheng Feng, Lifu Huang, Duyu Tang, Heng Ji,
Bing Qin, and Ting Liu. 2016. A language-
independent neural network for event detection. In
Proceedings of the 54th Annual Meeting of the Asso-
ciation for Computational Linguistics, pages 66–71.

Xavier Glorot, Antoine Bordes, and Yoshua Bengio.
2011. Deep sparse rectifier neural networks. In Pro-
ceedings of the 14th International Conference on Ar-
tificial Intelligence and Statistics, pages 315–323.

Ralph Grishman, David Westbrook, and Adam Meyers.
2005. Nyu’s english ace 2005 system description.
Journal on Satisfiability, 51(11):1927–1938.

Sepp Hochreiter and Jürgen Schmidhuber. 1997.
Long short-term memory. Neural Computation,
9(8):1735–1780.

Yu Hong, Jianfeng Zhang, Bin Ma, Jian-Min Yao,
Guodong Zhou, and Qiaoming Zhu. 2011. Using
cross-entity inference to improve event extraction.
In roceedings of the 49th Annual Meeting of the
Association for Computational Linguistics: Human
Language Technologies, pages 1127–1136.

Heng Ji and Ralph Grishman. 2008. Refining event ex-
traction through cross-document inference. In Pro-
ceedings of the 46th Annual Meeting of the Associa-
tion for Computational Linguistics, pages 254–262.

Katherine A. Keith, Abram Handler, Michael Pinkham,
Cara Magliozzi, Joshua McDuffie, and Brendan
O’Connor. 2017. Identifying civilians killed by po-
lice with distantly supervised entity-event extrac-
tion. In Proceedings of the 2017 Conference on
Empirical Methods in Natural Language Process-
ing, pages 1547–1557.

Thomas N. Kipf and Max Welling. 2016. Semi-
supervised classification with graph convolutional
networks. CoRR, abs/1609.02907.

Dan Klein and Christopher D. Manning. 2003. Ac-
curate unlexicalized parsing. In Proceedings of the
41st Annual Meeting of the Association for Compu-
tational Linguistics, pages 423–430.

Qi Li, Heng Ji, and Liang Huang. 2013. Joint event
extraction via structured prediction with global fea-
tures. In Proceedings of the 51st Annual Meeting
of the Association for Computational Linguistics,
pages 73–82.

Xiang Li, Thien Huu Nguyen, Kai Cao, and Ralph Gr-
ishman. 2015. Improving event detection with ab-
stract meaning representation. In Proceedings of the
1st Workshop on Computing News Storylines, pages
11–15.

Shasha Liao and Ralph Grishman. 2010. Using doc-
ument level cross-event inference to improve event
extraction. In Proceedings of the 48th Annual Meet-
ing of the Association for Computational Linguis-
tics, pages 789–797.

Jian Liu, Yubo Chen, Kang Liu, and Jun Zhao. 2018.
Event detection via gated multilingual attention
mechanism. In Proceedings of the 32nd AAAI Con-
ference on Artificial Intelligence, pages 4865–4872.

Shulin Liu, Yubo Chen, Shizhu He, Kang Liu, and
Jun Zhao. 2016a. Leveraging framenet to improve
automatic event detection. In Proceedings of the
54th Annual Meeting of the Association for Compu-
tational Linguistics, pages 2134–2143.

Shulin Liu, Yubo Chen, Kang Liu, and Jun Zhao. 2017.
Exploiting argument information to improve event
detection via supervised attention mechanisms. In



1256

Proceedings of the 55th Annual Meeting of the Asso-
ciation for Computational Linguistics, pages 1789–
1798.

Shulin Liu, Kang Liu, Shizhu He, and Jun Zhao. 2016b.
A probabilistic soft logic based approach to exploit-
ing latent and global information in event classifica-
tion. In Proceedings of the 30th AAAI Conference
on Artificial Intelligence, pages 2993–2999.

Wei Lu and Dan Roth. 2012. Automatic event extrac-
tion with structured preference modeling. In Pro-
ceedings of the 50th Annual Meeting of the Associa-
tion for Computational Linguistics, pages 835–844.

Diego Marcheggiani and Ivan Titov. 2017. Encoding
sentences with graph convolutional networks for se-
mantic role labeling. In Proceedings of the 2017
Conference on Empirical Methods in Natural Lan-
guage Processing, pages 1506–1515.

David McClosky, Mihai Surdeanu, and Christopher D.
Manning. 2011. Event extraction as dependency
parsing. In Proceedings of the 49th Annual Meet-
ing of the Association for Computational Linguis-
tics: Human Language Technologies, pages 1626–
1635.

Thien Huu Nguyen, Kyunghyun Cho, and Ralph Gr-
ishman. 2016. Joint event extraction via recurrent
neural networks. In Proceedings of the 2016 Con-
ference of the North American Chapter of the Asso-
ciation for Computational Linguistics: Human Lan-
guage Technologies, pages 300–309.

Thien Huu Nguyen and Ralph Grishman. 2016. Mod-
eling skip-grams for event detection with convolu-
tional neural networks. In Proceedings of the 2016
Conference on Empirical Methods in Natural Lan-
guage Processing, pages 886–891.

Thien Huu Nguyen and Ralph Grishman. 2018. Graph
convolutional networks with argument-aware pool-
ing for event detection. In Proceedings of the 32nd
AAAI Conference on Artificial Intelligence, pages
5900–5907.

Jeffrey Pennington, Richard Socher, and Christo-
pher D. Manning. 2014. Glove: Global vectors for
word representation. In Proceedings of the 2014
Conference on Empirical Methods in Natural Lan-
guage Processing, pages 1532–1543.

Roi Reichart and Regina Barzilay. 2012. Multi-event
extraction guided by global constraints. In Proceed-
ings of the 2012 Conference of the North American
Chapter of the Association for Computational Lin-
guistics: Human Language Technologies, pages 70–
79.

Lei Sha, Feng Qian, Baobao Chang, and Zhifang Sui.
2018. Jointly extracting event triggers and argu-
ments by dependency-bridge RNN and tensor-based
argument interaction. In Proceedings of the 32nd
AAAI Conference on Artificial Intelligence, pages
5916–5923.

Rupesh Kumar Srivastava, Klaus Greff, and Jürgen
Schmidhuber. 2015. Training very deep networks.
In Proceedings of the 28th Annual Conference
on Neural Information Processing Systems, pages
2377–2385.

Bishan Yang and Tom M. Mitchell. 2016. Joint ex-
traction of events and entities within a document
context. In Proceedings of the 2016 Conference of
the North American Chapter of the Association for
Computational Linguistics: Human Language Tech-
nologies, pages 289–299.

Matthew D. Zeiler. 2012. ADADELTA: an adaptive
learning rate method. CoRR, abs/1212.5701.


