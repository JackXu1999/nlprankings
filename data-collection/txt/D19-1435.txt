











































Parallel Iterative Edit Models for Local Sequence Transduction


Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing
and the 9th International Joint Conference on Natural Language Processing, pages 4260–4270,
Hong Kong, China, November 3–7, 2019. c©2019 Association for Computational Linguistics

4260

Parallel Iterative Edit Models for Local Sequence Transduction

Abhijeet Awasthi∗, Sunita Sarawagi , Rasna Goyal , Sabyasachi Ghosh , Vihari Piratla
Department of Computer Science and Engineering, IIT Bombay

Abstract

We present a Parallel Iterative Edit (PIE)
model for the problem of local sequence trans-
duction arising in tasks like Grammatical er-
ror correction (GEC). Recent approaches are
based on the popular encoder-decoder (ED)
model for sequence to sequence learning. The
ED model auto-regressively captures full de-
pendency among output tokens but is slow
due to sequential decoding. The PIE model
does parallel decoding, giving up the advan-
tage of modelling full dependency in the out-
put, yet it achieves accuracy competitive with
the ED model for four reasons: 1. predict-
ing edits instead of tokens, 2. labeling se-
quences instead of generating sequences, 3. it-
eratively refining predictions to capture de-
pendencies, and 4. factorizing logits over ed-
its and their token argument to harness pre-
trained language models like BERT. Experi-
ments on tasks spanning GEC, OCR correc-
tion and spell correction demonstrate that the
PIE model is an accurate and significantly
faster alternative for local sequence transduc-
tion. The code and pre-trained models for
GEC are available at https://github.
com/awasthiabhijeet/PIE.

1 Introduction

In local sequence transduction (LST) an input se-
quence x1, . . . , xn needs to be mapped to an out-
put sequence y1, . . . , ym where the x and y se-
quences differ only in a few positions, m is close
to n, and xi, yj come from the same vocabulary Σ.
An important application of local sequence trans-
duction that we focus on in this paper is Gram-
matical error correction (GEC). We contrast lo-
cal transduction with more general sequence trans-
duction tasks like translation and paraphrasing
which might entail different input-output vocab-
ulary and non-local alignments. The general se-

∗Correspondence to: awasthi@cse.iitb.ac.in

quence transduction task is cast as sequence to
sequence (seq2seq) learning and modeled popu-
larly using an attentional encoder-decoder (ED)
model. The ED model auto-regressively produces
each token yt in the output sequence conditioned
on all previous tokens y1, . . . , yt−1. Owing to
the remarkable success of this model in challeng-
ing tasks like translation, almost all state-of-the-art
neural models for GEC use it (Zhao et al., 2019;
Lichtarge et al., 2019; Ge et al., 2018b; Chol-
lampatt and Ng, 2018b; Junczys-Dowmunt et al.,
2018).

We take a fresh look at local sequence trans-
duction tasks and present a new parallel-iterative-
edit (PIE) architecture. Unlike the prevalent ED
model that is constrained to sequentially generat-
ing the tokens in the output, the PIE model gener-
ates the output in parallel, thereby substantially re-
ducing the latency of sequential decoding on long
inputs. However, matching the accuracy of exist-
ing ED models without the luxury of conditional
generation is highly challenging. Recently, paral-
lel models have also been explored in tasks like
translation (Stern et al., 2018; Lee et al., 2018; Gu
et al., 2018; Kaiser et al., 2018) and speech syn-
thesis (van den Oord et al., 2018), but their accu-
racy is significantly lower than corresponding ED
models. The PIE model incorporates the follow-
ing four ideas to achieve comparable accuracy on
tasks like GEC in spite of parallel decoding.
1. Output edits instead of tokens: First, instead
of outputting tokens from a large vocabulary,
we output edits such as copy, appends, deletes,
replacements, and case-changes which gen-
eralize better across tokens and yield a much
smaller vocabulary. Suppose in GEC we have
an input sentence: fowler fed dog. Ex-
isting seq2seq learning approaches would need
to output the four tokens Fowler, fed,
the, dog from a word vocabulary whereas we

https://github.com/awasthiabhijeet/PIE
https://github.com/awasthiabhijeet/PIE


4261

would predict the edits {Capitalize token
1, Append(the) to token 2, Copy
token 3}.
2. Sequence labeling instead of sequence gener-
ation: Second we perform in-place edits on the
source tokens and formulate local sequence trans-
duction as labeling the input tokens with edit com-
mands, rather than solving the much harder whole
sequence generation task involving a separate de-
coder and attention module. Since input and out-
put lengths are different in general such formu-
lation is non-trivial, particularly due to edits that
insert words. We create special compounds edits
that merge token inserts with preceding edits that
yield higher accuracy than earlier methods of inde-
pendently predicting inserts (Ribeiro et al., 2018;
Lee et al., 2018).
3. Iterative refinement: Third, we increase the
inference capacity of the parallel model by iter-
atively inputting the model’s own output for fur-
ther refinement. This handles dependencies im-
plicitly, in a way reminiscent of Iterative Condi-
tional Modes (ICM) fitting in graphical model in-
ference (Koller and Friedman, 2009). Lichtarge
et al. (2019) and Ge et al. (2018b) also refine iter-
atively but with ED models.
4. Factorize pre-trained bidirectional LMs:
Finally, we adapt recent pre-trained bidirectional
models like BERT (Devlin et al., 2018) by factor-
izing the logit layer over edit commands and their
token argument. Existing GEC systems typically
rely on conventional forward directional LM to
pretrain their decoder, whereas we show how to
use a bi-directional LM in the encoder, and that
too to predict edits.

Novel contributions of our work are as follows:

• Recognizing GEC as a local sequence trans-
duction (LST) problem, rather than machine
translation. We then cast LST as a fast
non-autoregressive, sequence labeling model
as against existing auto-regressive encoder-
decoder model.

• Our method of reducing LST to non-
autoregressive sequence labeling has many
novel elements: outputting edit operations
instead of tokens, append operations instead of
insertions in the edit space, and replacements
along with custom transformations.

• We show how to effectively harness a pre-

trained language model like BERT using our
factorized logit architecture with edit-specific
attention masks.

• The parallel inference in PIE is 5 to 15 times
faster than a competitive ED based GEC model
like (Lichtarge et al., 2019) which performs se-
quential decoding using beam-search. PIE also
attains close to state of the art performance on
standard GEC datasets. On two other local
transduction tasks, viz., OCR and spell correc-
tions the PIE model is fast and accurate w.r.t.
other existing models developed specifically for
local sequence transduction.

2 Our Method

We assume a fully supervised training setup where
we are given a parallel dataset of incorrect, correct
sequence pairs: D = {(xi,yi) : i = 1 . . . N}, and
an optional large corpus of unmatched correct se-
quencesL = {ỹ1, . . . , ỹU}. In GEC, this could be
a corpus of grammatically correct sentences used
to pre-train a language model.

Background: existing ED model Existing
seq2seq ED models factorize Pr(y|x) to capture
the full dependency between a yt and all previ-
ous y<t = y1, . . . , yt−1 as

∏m
t=1 Pr(yt|y<t,x).

An encoder converts input tokens x1, . . . , xn to
contextual states h1, . . . ,hn and a decoder sum-
marizes y<t to a state st. An attention distribu-
tion over contextual states computed from st deter-
mines the relevant input context ct and the output
token distribution is calculated as Pr(yt|y<t,x) =
Pr(yt|ct, st). Decoding is done sequentially us-
ing beam-search. When a correct sequence cor-
pus L is available, the decoder is pre-trained on
a next-token prediction loss and/or a trained LM
is used to re-rank the beam-search outputs (Zhao
et al., 2019; Chollampatt and Ng, 2018a; Junczys-
Dowmunt et al., 2018).

2.1 Overview of the PIE model

We move from generating tokens in the output se-
quence y using a separate decoder, to labelling the
input sequence x1, . . . , xn with edits e1, . . . , en.
For this we need to design a function Seq2Edits
that takes as input an (x,y) pair in D and outputs
a sequence e of edits from an edit space E where e
is of the same length as x in spite of x and y being
of different lengths. In Section 2.2 we show how
we design such a function.



4262

Training: We invoke Seq2Edits on D and learn
the parameters of a probabilistic model Pr(e|x, θ)
to assign a distribution over the edit labels on to-
kens of the input sequence. In Section 2.3 we de-
scribe the PIE architecture in more detail. The cor-
rect corpus L , when available, is used to pre-train
the encoder to predict an arbitrarily masked token
yt in a sequence y in L, much like in BERT. Un-
like in existing seq2seq systems where L is used
to pre-train the decoder that only captures forward
dependencies, in our pre-training the predicted to-
ken yt is dependent on both forward and backward
contexts. This is particularly useful for GEC-type
tasks where future context yt+1 . . . ym can be ap-
proximated by xt+1, . . . , xn.

Inference: Given an input x, the trained model
predicts the edit distribution for each input to-
ken independent of others, that is Pr(e|x, θ) =∏n

t=1 Pr(et|x, t, θ), and thus does not entail the
latency of sequential token generation of the ED
model. We output the most probable edits ê =
argmaxe Pr(e|x, θ). Edits are designed so that we
can easily get the edited sequence ŷ after applying
ê on x. ŷ is further refined by iteratively apply-
ing the model on the generated outputs ŷ until we
get a sequence identical to one of the previous se-
quences upto a maximum number of iterations I .

2.2 The Seq2Edits Function

Given a x = x1, . . . , xn and y = y1, . . . , ym
where m may not be equal to n, our goal is
to obtain a sequence of edit operations e =
(e1, . . . , en) : ei ∈ E such that applying edit ei on
the input token xi at each position i reconstructs
the output sequence y. Invoking an off-the-shelf
edit distance algorithm between x and y can give
us a sequence of copy, delete, replace, and insert
operations of arbitrary length. The main difficulty
is converting the insert operations into in-place ed-
its at each xi. Other parallel models (Ribeiro et al.,
2018; Lee et al., 2018) have used methods like pre-
dicting insertion slots in a pre-processing step, or
predicting zero or more tokens in-between any two
tokens in x. We will see in Section 3.1.4 that these
options do not perform well. Hence we design an
alternative edit space E that merges inserts with
preceding edit operations creating compound ap-
pend or replace operations. Further, we create a
dictionary Σa of common q-gram insertions or re-
placements observed in the training data.
Our edit space (E) comprises of copy (C) xi, delete

Require: x = (x1, . . . , xn), y = (y1, . . . , ym) and T : List
of Transformations
diffs← LEVENSHTEIN-DIST(x,y) with modified cost.
diffs ← In diffs break substitutions, merge inserts into q-
grams
Σa ←M most common inserts in training data
e← EMPTYARRAY(n)
for t← 1 to LENGTH(diffs) do

if diffs[t] = (C, xi) or (D, xi) then
ei ← diffs[t].op

else if diffs[t] = (I, xi, w) then
if ei = D then

if (xi, w) match transformation T ∈ T then
ei ← T

else
ei ← R(w) if w ∈ Σa else C

else if ei = C then
ei ← A(w) if w ∈ Σa else C

return e

Figure 1: The Seq2Edits function used to convert a se-
quence pair x,y into in-place edits.

(D) xi, append (A) a q-gram w ∈ Σa after copy-
ing xi, replace (R) xi with a q-gram w ∈ Σa. For
GEC, we additionally use transformations denoted
as T1, . . . , Tk which perform word-inflection (e.g.
arrive to arrival). The space of all edits is
thus:

E = {C, D, T1, . . . , Tk}
∪ {A(w) : w ∈ Σa}

∪ {R(w) : w ∈ Σa} (1)

Example 1
x [ Bolt can have run race ]
y [ Bolt could have run the race ]
diff (C,[) (C,Bolt) (D,can) (I,can,could) (C,have)

(C,run) (I,run,the) (C,race) (C,])
e C C R(could) C A(the) C C

↑ ↑ ↑ ↑ ↑ ↑ ↑
[ Bolt can have run race ]

Example 2
x [ He still won race ! ]
y [ However , he still won ! ]
diff (C,[) (I,[,However,) (D,He) (I,He,he) (C,still)

(C,won) (D,-race) (C,!) (C,])
e A(However,) T case C C D C C

↑ ↑ ↑ ↑ ↑ ↑ ↑
[ He still won race ! ]

Table 1: Two example sentence pairs converted into
respective edit sequences.

We present our algorithm for converting a se-
quence x and y into in-place edits on x using the
above edit space in Figure 1. Table 1 gives exam-
ples of converting (x, y) pairs to edit sequences.
We first invoke the Levenshtein distance algorithm
(Levenshtein, 1966) to obtain diff between x
and y with delete and insert cost as 1 as usual, but



4263

with a modified substitution cost to favor match-
ing of related words. We detail this modified cost
in the Appendix and show an example of how this
modification leads to more sensible edits. The
diff is post-processed to convert substitutions
into deletes followed by inserts, and consecutive
inserts are merged into a q-gram. We then create a
dictionary Σa of the M most frequent q-gram in-
serts in the training set. Thereafter, we scan the
diff left to right: a copy at xi makes ei = C, a
delete at xi makes ei = D, an insert w at xi and a
ei = C flips the ei into a ei = A(w) if w is in Σa,
else it is dropped, an insert w at xi and a ei = D
flips the ei into a ei = T(w) if a match found, else
a replace R(w) if w is in Σa, else it is dropped.

The above algorithm does not guarantee that
when e is applied on x we will recover y for all
sequences in the training data. This is because we
limit Σa to include only the M most frequently
inserted q-grams. For local sequence transduction
tasks, we expect a long chain of consecutive in-
serts to be rare, hence our experiments were per-
formed with q = 2. For example, in NUCLE
dataset (Ng et al., 2014) which has roughly 57.1K
sentences, less than 3% sentences have three or
more consecutive inserts.

2.3 The Parallel Edit Prediction Model

We next describe our model for predicting edits e :
e1, . . . , en on an input sequence x : x1, . . . , xn.
We use a bidirectional encoder to provide a con-
textual encoding of each xi. This can either be
multiple layers of bidirectional RNNs, CNNs or
deep bidirectional transformers. We adopt the
deep bidirectional transformer architecture since
it encodes the input in parallel. We pre-train the
model using L much like in BERT pre-training
recently proposed for language modeling(Devlin
et al., 2018). We first give an overview of BERT
and then describe our model.

Background: BERT The input is token embed-
ding xi and positional embedding pi for each to-
ken xi in the sequence x. Call these jointly as
h0i = [xi,pi]. Each layer ` produces a h

`
i at each

position i as a function of h`−1i and self-attention
over all h`−1j , j ∈ [1, n]. The BERT model is pre-
trained by masking out a small fraction of the in-
put tokens by a special MASK, and predicting the
masked word from its bidirectional context cap-
tured as the last layer output: h1, . . . ,hn.

Figure 2: A Parallel Edit Architecture based on a 2-
layer bidirectional transformer. Input sequence length
is 3. Arrows indicate attention mask for computation
of hli, r

l
i,a

l
i at position i for layer l.

2.3.1 A Default use of BERT
Since we have cast our task as a sequence labeling
task, a default output layer would be to compute
Pr(ei|x) as a softmax over the space of edits E
from each hi. If We denotes the softmax parame-
ter for edit e, we get:

Pr(ei = e|x) = softmax(W ᵀe hi) (2)

The softmax parameters, We in Equation 2, have
to be trained from scratch. We propose a method
to exploit token embeddings of the pre-trained lan-
guage model to warm-start the training of edits
like appends and replaces which are associated
with a token argument. Furthermore, for appends
and replaces, we provide a new method of com-
puting the hidden layer output via alternative in-
put positional embeddings and self-attention. We
do so without introducing any new parameters in
the hidden layers of BERT.

2.3.2 An Edit-factorized BERT Architecture
We adapt a pre-trained BERT-like bidirectional
language model to learn to predict edits as follows.
For suitably capturing the contexts for replace ed-
its, for each position i we create an additional in-
put comprising of r0i = [M,pi] where M is the em-
bedding for MASK token in the LM. Likewise for
a potential insert between i and i+ 1 we create an
input a0i = [M,

pi+pi+1
2 ] where the second compo-

nent is the average of the position embedding of
the ith and i+ 1th position. As shown in Figure 2,
at each layer ` we compute self-attention for the
ith replace unit r`i over h

`
j for all j 6= i and itself.

Likewise, for the append unit a`i the self-attention
is over all h`j for all js and itself. At the last layer
we have h1, . . . ,hn, r1, . . . , rn,a1, . . . ,an. Us-
ing these we compute logits factorized over edits



4264

and their token argument. For an edit e ∈ E at po-
sition i, let w denote the argument of the edit (if
any). As mentioned earlier, w can be a q-gram for
append and replace edits. Embedding of w, rep-
resented by φ(w) is obtained by summing up in-
dividual output embeddings of tokens in w. Addi-
tionally, in the outer layer we allocate edit-specific
parameters θ corresponding to each distinct com-
mand in E . Using these, various edit logits are
computed as follows:

Pr(ei|x) = softmax(logit(ei|x)) where
logit(ei|x) = (3)

θᵀChi + φ(xi)
ᵀhi + 0 if ei = C

θᵀA(w)hi + φ(xi)
ᵀhi + φ(w)

ᵀai if ei = A(w)

θᵀR(w)hi + 0 + (φ(w)− φ(xi))
ᵀri if ei = R(w)

θᵀDhi + 0 + 0 if ei = D
θᵀTkhi + φ(xi)

ᵀhi + 0 if ei = Tk

The first term in the RHS of above equations
captures edit specific score. The second term cap-
tures the score for copying the current word xi to
the output. The third term models the influence of
a new incoming token in the output obtained by
a replace or append edit. For replace edits, score
of the replaced word is subtracted from score of
the incoming word. For transformation we add the
copy score because they typically modify only the
word forms, hence we do not expect meaning of
the transformed word to change significantly.

The above equation provides insights on why
predicting independent edits is easier than predict-
ing independent tokens. Consider the append edit
(A(w)). Instead of independently predicting xi at i
andw at i+1, we jointly predict the tokens in these
two slots and contrast it with not inserting any new
w after xi in a single softmax. We will show em-
pirically (Sec 3.1.4) that such selective joint pre-
diction is key to obtaining high accuracy in spite
of parallel decoding.

Finally, loss for a training example (e,x) is ob-
tained by summing up the cross-entropy associ-
ated with predicting edit ei at each token xi.

L(e,x) = −Σi log(Pr(ei|x)) (4)

3 Experiments

We compare our parallel iterative edit (PIE) model
with state-of-the-art GEC models that are all based
on attentional encoder-decoder architectures. In

Section 3.2 we show that the PIE model is also ef-
fective on two other local sequence transduction
tasks: spell and OCR corrections. Hyperparam-
eters for all the experiments are provided in Ta-
ble 12, 13 of the Appendix.

3.1 Grammatical error correction (GEC)

We use Lang-8 (Mizumoto et al., 2011), NU-
CLE (Ng et al., 2014) and FCE (Yannakoudakis
et al., 2011) corpora, which jointly comprise 1.2
million sentence pairs in English. The valida-
tion dataset comprises of 1381 sentence pairs from
CoNLL-13 (Ng et al., 2013) test set. We initialize
our GEC model with the publicly available BERT-
LARGE1 model that was pre-trained on Wikipedia
(2500M words) and Book corpus (Zhu et al., 2015)
(800M words) to predict 15% randomly masked
words using its deep bidirectional context. Next,
we perform 2 epochs of training on a syntheti-
cally perturbed version of the One-Billion-word
corpus (Chelba et al., 2013). We refer to this as
synthetic training. Details of how we create the
synthetic corpus appear in Section A.3 of the ap-
pendix. Finally we fine-tune on the real GEC
training corpus for 2 epochs. We use a batch size
of 64 and learning rate 2e-5.
The edit space consists of copy, delete, 1000 ap-
pends, 1000 replaces and 29 transformations and
their inverse. Arguments of Append and Replace
operations mostly comprise punctuations, articles,
pronouns, prepositions, conjunctions and verbs.
Transformations perform inflections like add suf-
fix s, d, es, ing, ed or replace suffix s
to ing, d to s, etc. These transformations
were chosen out of common replaces in the train-
ing data such that many replace edits map to only
a few transformation edits, in order to help the
model better generalize replaces across different
words. In Section 3.1.4 we see that transforma-
tions increase the model’s recall. The complete
list of transformations appears in Table 10 of the
Appendix. We evaluate on F0.5 score over span-
level corrections from the MaxMatch (M2) scorer
(Dahlmeier and Ng, 2012) on CONLL-2014-test.
Like most existing GEC systems, we invoke a
spell-checker on the test sentences before apply-
ing our model. We also report GLEU+ (Napoles
et al., 2016) scores on JFLEG corpus (Napoles
et al., 2017) to evaluate fluency.

1https://github.com/google-research/
bert

https://github.com/google-research/bert
https://github.com/google-research/bert


4265

Work CONLL-14 JFLEG
P R F0.5 GLEU+

Zhao et al. (2019) 67.7 40.6 59.8 -
Lichtarge et al. (2019) 65.5 37.1 56.8 61.6
Chollampatt and Ng

(2018a)

69.9 23.7 46.4 51.3

PIE (This work) 66.1 43.0 59.7 60.3

Table 2: Comparison of non-ensemble model numbers
for various GEC models. Best non-ensemble results are
reported for each model.

3.1.1 Overall Results
Table 3 compares ensemble model results of PIE
and other state of the art models, which all hap-
pen to be seq2seq ED models and also use en-
semble decoding. For PIE, we simply average
the probability distribution over edits from 5 in-
dependent ensembles. In Table-2 we compare
non-ensemble numbers of PIE with the best avail-
able non-ensemble numbers of competing meth-
ods. On CoNLL-14 test-set our results are very
close to the highest reported by Zhao et al. (2019).
These results show that our parallel prediction
model is competitive without incurring the over-
heads of beam-search and slow decoding of se-
quential models. GLEU+ score, that rewards flu-
ency, is somewhat lower for our model on the JF-
LEG test set because of parallel predictions. We
do not finetune our model on the JFLEG dev set.
We expect these to improve with re-ranking using
a LM. All subsequent ablations and timing mea-
surements are reported for non-ensemble models.

3.1.2 Running Time Comparison
Parallel decoding enables PIE models to be con-
siderably faster than ED models. In Figure 3
we compare wall-clock decoding time of PIE
with 24 encoder layers (PIE-LARGE, F0.5 =
59.7), PIE with 12 encoder layers (PIE-BASE,
F0.5 = 56.6) and competitive ED architecture by
Lichtarge et al. (2019) with 6 encoder and 6 de-
coder layers (T2T, F0.5 = 56.8 ) on CoNLL-14
test set. All decoding experiments were run and
measured on a n1-standard-22 VM instance with
a single TPU shard (v-2.8). We observe that even
PIE-LARGE is between a factor of 5 to 15 faster
than an equivalent transformer-based ED model
(T2T) with beam-size 4. The running time of

2https://cloud.google.com/compute/
docs/machine-types#standard_machine_
types

Figure 3: Comparing average decoding time in mil-
liseconds (log scale) of PIE Vs transformer based ED
model with two beam widths. Numbers in legend de-
note M2 score.

PIE-LARGE increases sub-linearly with sentence
length whereas the ED model’s decoding time in-
creases linearly.

3.1.3 Impact of Iterative Refinement
We next evaluate the impact of iterative refine-
ments on accuracy in Table 4. Out of 1312 sen-
tences in the test set, only 832 sentences changed
in the first round which were then fed to the sec-
ond round where only 156 sentences changed, etc.
The average number of refinement rounds per ex-
ample was 2.7. In contrast, a sequential model on
this dataset would require 23.2 steps correspond-
ing to the average number of tokens in a sentence.
The F0.5 score increases from 57.9 to 59.5 at the
end of the second iteration.

Table 5 presents some sentences corrected by
PIE. We see that PIE makes multiple parallel edits
in a round if needed. Also, we see how refinement
over successive iterations captures output space
dependency. For example, in the second sentence
interact gets converted to interacted fol-
lowed by insertion of have in the next round.

3.1.4 Ablation study on the PIE Architecture
In this section we perform ablation studies to un-
derstand the importance of individual features of
the PIE model.
Synthetic Training We evaluate the impact of
training on the artifically generated GEC corpus
in row 2 of Table 6. We find that without it the
F0.5 score is 3.4 points lower.
Factorized Logits We evaluate the gains due to

https://cloud.google.com/compute/docs/machine-types#standard_machine_types
https://cloud.google.com/compute/docs/machine-types#standard_machine_types
https://cloud.google.com/compute/docs/machine-types#standard_machine_types


4266

Work Method CONLL-14 JFLEG
P R F0.5 GLEU+

Zhao et al. (2019)
Transformer + Pre-training + LM

+ Spellcheck + Ensemble Decoding (4)
71.6 38.7 61.2 61.0

Lichtarge et al. (2019)
Transformer + Pre-training

+ Iterative refinement + Ensemble Decoding (8)
66.7 43.9 60.4 63.3

Grundkiewicz and

Junczys-Dowmunt

(2018)

SMT + NMT (Bi-GRU) + LM
+ Ensemble Decoding (4) + Spellcheck

66.8 34.5 56.3 61.5

Chollampatt and Ng

(2018a)

CNN + LM + Ensemble Decoding (4) + Spellcheck 65.5 33.1 54.8 57.5

PIE (This work)
Transformer + LM + Pre-training + Spellcheck

+ Iterative refinement + Ensembles (5)
68.3 43.2 61.2 61.0

Table 3: Comparison of recent GEC models trained using publicly available corpus. All the methods here except
ours perform sequential decoding. Precision and Recall are represented by P and R respectively.

R1 R2 R3 R4
#edited 832 156 20 5
P 64.5 65.9 66.1 66.1
R 41.0 42.9 43.0 43.0
F0.5 57.9 59.5 59.7 59.7

Table 4: Statistics on successive rounds of iterative re-
finement. First row denotes number changed sentences
(out of 1312) with each round on CONLL-14 (test).

our edit-factorized BERT model (Section 2.3.2)
over the default BERT model (Section 2.3.1). In
Table 6 (row 3) we show that compared to the fac-
torized model (row 2) we get a 1.2 point drop in
F0.5 score in absence of factorization.

Inserts as Appends on the preceding word was
another important design choice. The alternative
of predicting insert independently at each gap
with a null token added to Σa performs 2.7 F0.5
points poorly (Table 6 row 4 vs row 2).

Transformation edits are significant as we ob-
serve a 6.3 drop in recall without them (row 5).

Impact of Language Model We evaluate the ben-
efit of starting from BERT’s pre-trained LM by
reporting accuracy from an un-initialized network
(row 6). We observe a 20 points drop in F0.5 es-
tablishing the importance of LMs in GEC.

Impact of Network Size We train the BERT-
Base model with one-third fewer parameters than
BERT-LARGE. From Table 6 (row 7 vs 1) we see
once again that size matters in deep learning!

x I started invoving into Facebook one years ago .
PIE1 I started involving in Facebook one year ago .
PIE2 I started involved in Facebook one year ago .
PIE3 I started getting involved in Facebook one year

ago .
x Since ancient times , human interact with others

face by face .
PIE1 Since ancient times , humans interacted with oth-

ers face to face .
PIE2 Since ancient times , humans have interacted

with others face to face .
x However , there are two sides of stories always .
PIE1 However , there are always two sides to stories

always .
PIE2 However , there are always two sides to the sto-

ries .
PIE3 However , there are always two sides to the story

.

Table 5: Parallel and Iterative edits done by PIE.

3.2 More Sequence Transduction Tasks
We demonstrate the effectiveness of PIE model on
two additional local sequence transduction tasks
recently used in (Ribeiro et al., 2018).

Spell Correction We use the twitter spell cor-
rection dataset (Aramaki, 2010) which con-
sists of 39172 pairs of original and corrected
words obtained from twitter. We use the same
train-dev-valid split as (Ribeiro et al., 2018)
(31172/4000/4000). We tokenize on characters,
and our vocabulary Σ and Σa comprises the 26
lower cased letters of English.

Correcting OCR errors We use the Finnish
OCR data set3 by (Silfverberg et al., 2016)
comprising words extracted from Early Modern

3https://github.com/mpsilfve/ocrpp

https://github.com/mpsilfve/ocrpp


4267

# Methods P R F0.5
1 PIE 66.1 43.0 59.7
2 − Synthetic training 67.2 34.2 56.3
3 −Factorized-logits 66.4 32.8 55.1
4 −Append +Inserts 57.4 42.5 53.6
5 −Transformations 63.6 27.9 50.6
6 −LM Pre-init 48.8 18.3 36.6
7 PIE on BERT-Base 67.8 34.0 56.6

Table 6: Ablation study on the PIE model.

Methods Spell OCR
Seq2Seq (Soft, lstm) 46.3 79.9
Seq2Seq (Hard,lstm) 52.2 58.4
Seq2Seq (Soft-T2T) 67.6 84.5
Ribeiro2018 (best model) 54.1 81.8
PIE 67.0 87.6

Table 7: Comparing PIE with Seq2Seq models (top-
part) and Ribeiro’s parallel model on two other local
sequence transduction tasks.

Finnish corpus of OCR processed newspaper text.
We use the same train-dev-test splits as provided
by (Silfverberg et al., 2016). We tokenize on char-
acters in the word. For a particular split, our vo-
cabulary Σ and Σa comprises of all the characters
seen in the training data of the split.

Architecture For all the tasks in this section,
PIE is a 4 layer self-attention transformer with 200
hidden units, 400 intermediate units and 4 atten-
tion heads. No L pre-initialization is done. Also,
number of iterations of refinements is set to 1.

Results Table 7 presents whole-word 0/1 accu-
racy for these tasks on PIE and the following
methods: Ribeiro et al. (2018)’s local transduc-
tion model (described in Section 4), and LSTM
based ED models with hard monotonic attention
(Aharoni and Goldberg, 2017) and soft-attention
(Bahdanau et al., 2015) as reported in (Ribeiro
et al., 2018). In addition, for a fair decoding time
comparison, we also train a transformer-based ED
model referred as Soft-T2T with 2 encoder, 2 de-
coder layers for spell-correction and 2 encoder, 1

PIE Soft-T2T
Spell 80.43 36.62
OCR 65.44 43.02

Table 8: Wall-clock decoding speed in words/second of
PIE and a comparable Seq2Seq T2T model.

decoder layer for OCR correction. We observe
that PIE’s accuracy is comparable with ED mod-
els in both the tasks. Table 8 compares decod-
ing speed of PIE with Soft-T2T in words/second.
Since more than 90% of words have fewer than 9
tokens and the token vocabulary Σ is small, de-
coding speed-ups of PIE over ED model on these
tasks is modest compared to GEC.

4 Related Work

Grammar Error Correction (GEC) is an ex-
tensively researched area in NLP. See Ng et al.
(2013) and Ng et al. (2014) for past shared
tasks on GEC, and this4 website for current
progress. Approaches attempted so far in-
clude rules (Felice et al., 2014), classifiers (Ro-
zovskaya and Roth, 2016), statistical machine
translation (SMT) (Junczys-Dowmunt and Grund-
kiewicz, 2016), neural ED models (Chollampatt
and Ng, 2018a; Junczys-Dowmunt et al., 2018;
Ge et al., 2018a), and hybrids (Grundkiewicz
and Junczys-Dowmunt, 2018). All recent neu-
ral approaches are sequential ED models that pre-
dict either word sequences (Zhao et al., 2019;
Lichtarge et al., 2019) or character sequences (Xie
et al., 2016) using either multi-layer RNNs (Ji
et al., 2017; Grundkiewicz and Junczys-Dowmunt,
2018) or CNNs(Chollampatt and Ng, 2018a; Ge
et al., 2018a) or Transformers (Junczys-Dowmunt
et al., 2018; Lichtarge et al., 2019). Our sequence
labeling formulation is similar to (Yannakoudakis
et al., 2017) and (Kaili et al., 2018) but the for-
mer uses it to only detect errors and the latter only
corrects five error-types using separate classifiers.
Edits have been exploited in earlier GEC systems
too but very unlike our method of re-architecting
the core model to label input sequence with ed-
its. Schmaltz et al. (2017) interleave edit tags in
target tokens but use seq2seq learning to predict
the output sequence. Chollampatt and Ng (2018a)
use edits as features for rescoring seq2seq pre-
dictions. Junczys-Dowmunt et al. (2018) use an
edit-weighted MLE objective to emphasise correc-
tive edits during seq2seq learning. Stahlberg et al.
(2019) use finite state transducers, whose state
transitions denote possible edits, built from an un-
labeled corpus to constrain the output of a neural
beam decoder to a small GEC-feasible space.

4https://nlpprogress.com/english/
grammatical_error_correction.html

https://nlpprogress.com/english/grammatical_error_correction.html
https://nlpprogress.com/english/grammatical_error_correction.html


4268

Parallel decoding in neural machine transla-
tion Kaiser et al. (2018) achieve partial paral-
lelism by first generating latent variables sequen-
tially to model dependency. Stern et al. (2018)
use a parallel generate-and-test method with mod-
est speed-up. Gu et al. (2018) generate all tokens
in parallel but initialize decoder states using latent
fertility variables to determine number of repli-
cas of an encoder state. We achieve the effect of
fertility using delete and append edits. Lee et al.
(2018) generate target sequences iteratively but re-
quire the target sequence length to be predicted at
start. In contrast our in-place edit model allows
target sequence length to change with appends.

Local Sequence Transduction is handled in
Ribeiro et al. (2018) by first predicting insert slots
in x using learned insertion patterns and then us-
ing a sequence labeling task to output tokens in
x or a special token delete. Instead, we out-
put edit operations including word transforma-
tions. Their pattern-based insert pre-slotting is
unlikely to work for more challenging tasks like
GEC. Koide et al. (2018) design a special edit-
invariant neural network for being robust to small
edit changes in input biological sequences. This is
a different task than ours of edit prediction. Yin
et al. (2019) is about neural representation of edits
specifically for structured objects like source code.
This is again a different problem than ours.

5 Conclusion

We presented a parallel iterative edit (PIE) model
for local sequence transduction with a focus on
the GEC task. Compared to the popular encoder-
decoder models that perform sequential decoding,
parallel decoding in the PIE model yields a fac-
tor of 5 to 15 reduction in decoding time. The
PIE model employs a number of ideas to match
the accuracy of sequential models in spite of par-
allel decoding: it predicts in-place edits using a
carefully designed edit space, iteratively refines its
own predictions, and effectively reuses state-of-
the-art pre-trained bidrectional language models.
In the future we plan to apply the PIE model to
more ambitious transduction tasks like translation.

Acknowledgements This research was partly
sponsored by a Google India AI/ML Research
Award and Google PhD Fellowship in Machine
Learning. We gratefully acknowledge Google’s
TFRC program for providing us Cloud-TPUs. We

thank Varun Patil for helping us improve the speed
of pre-processing and synthetic-data generation
pipelines.

References
Roee Aharoni and Yoav Goldberg. 2017. Morphologi-

cal inflection generation with hard monotonic atten-
tion. In Proceedings of the 55th Annual Meeting of
the Association for Computational Linguistics, ACL
2017, Vancouver, Canada, July 30 - August 4, Vol-
ume 1: Long Papers, pages 2004–2015.

Eiji Aramaki. 2010. Typo corpus. http://
luululu.com/tweet/.

Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Ben-
gio. 2015. Neural machine translation by jointly
learning to align and translate. ICLR.

Ciprian Chelba, Tomas Mikolov, Mike Schuster, Qi Ge,
Thorsten Brants, Phillipp Koehn, and Tony Robin-
son. 2013. One billion word benchmark for measur-
ing progress in statistical language modeling. arXiv
preprint arXiv:1312.3005.

Shamil Chollampatt and Hwee Tou Ng. 2018a. A mul-
tilayer convolutional encoder-decoder neural net-
work for grammatical error correction. In Proceed-
ings of the Thirty-Second AAAI Conference on Arti-
ficial Intelligence.

Shamil Chollampatt and Hwee Tou Ng. 2018b. Neural
quality estimation of grammatical error correction.
In Proceedings of the 2018 Conference on Empiri-
cal Methods in Natural Language Processing, pages
2528–2539.

Daniel Dahlmeier and Hwee Tou Ng. 2012. Better
evaluation for grammatical error correction. In Pro-
ceedings of the 2012 Conference of the North Amer-
ican Chapter of the Association for Computational
Linguistics: Human Language Technologies, pages
568–572. Association for Computational Linguis-
tics.

Jacob Devlin, Ming-Wei Chang, Kenton Lee, and
Kristina Toutanova. 2018. Bert: Pre-training of deep
bidirectional transformers for language understand-
ing. arXiv preprint arXiv:1810.04805.

Mariano Felice, Zheng Yuan, Øistein E Andersen, He-
len Yannakoudakis, and Ekaterina Kochmar. 2014.
Grammatical error correction using hybrid systems
and type filtering. In Proceedings of the Eigh-
teenth Conference on Computational Natural Lan-
guage Learning: Shared Task, pages 15–24.

Tao Ge, Furu Wei, and Ming Zhou. 2018a. Fluency
boost learning and inference for neural grammatical
error correction. In Proceedings of the 56th Annual
Meeting of the Association for Computational Lin-
guistics (Volume 1: Long Papers), volume 1, pages
1055–1065.

http://luululu.com/tweet/
http://luululu.com/tweet/
https://arxiv.org/abs/1409.0473
https://arxiv.org/abs/1409.0473


4269

Tao Ge, Furu Wei, and Ming Zhou. 2018b. Reaching
human-level performance in automatic grammatical
error correction: An empirical study. arXiv preprint
arXiv:1807.01270.

Roman Grundkiewicz and Marcin Junczys-Dowmunt.
2018. Near human-level performance in grammati-
cal error correction with hybrid machine translation.
In Proceedings of the 2018 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics: Human Language Technologies,
Volume 2 (Short Papers), pages 284–290.

Jiatao Gu, James Bradbury, Caiming Xiong, Vic-
tor O.K. Li, and Richard Socher. 2018. Non-
autoregressive neural machine translation. In Inter-
national Conference on Learning Representations
(ICLR).

Jianshu Ji, Qinlong Wang, Kristina Toutanova, Yongen
Gong, Steven Truong, and Jianfeng Gao. 2017. A
nested attention neural hybrid model for grammati-
cal error correction. In Proceedings of the 55th An-
nual Meeting of the Association for Computational
Linguistics (Volume 1: Long Papers), pages 753–
762, Vancouver, Canada. Association for Computa-
tional Linguistics.

Marcin Junczys-Dowmunt and Roman Grundkiewicz.
2016. Phrase-based machine translation is state-of-
the-art for automatic grammatical error correction.
In Proceedings of the 2016 Conference on Empiri-
cal Methods in Natural Language Processing, pages
1546–1556.

Marcin Junczys-Dowmunt, Roman Grundkiewicz,
Shubha Guha, and Kenneth Heafield. 2018. Ap-
proaching neural grammatical error correction as a
low-resource machine translation task. In Proceed-
ings of the 2018 Conference of the North American
Chapter of the Association for Computational Lin-
guistics: Human Language Technologies, Volume 1
(Long Papers), volume 1, pages 595–606.

Zhu Kaili, Chuan Wang, Ruobing Li, Yang Liu, Tian-
lei Hu, and Hui Lin. 2018. A simple but effective
classification model for grammatical error correc-
tion. arXiv preprint arXiv:1807.00488.

Lukasz Kaiser, Samy Bengio, Aurko Roy, Ashish
Vaswani, Niki Parmar, Jakob Uszkoreit, and Noam
Shazeer. 2018. Fast decoding in sequence models
using discrete latent variables. In Proceedings of the
35th International Conference on Machine Learn-
ing, pages 2390–2399.

Satoshi Koide, Keisuke Kawano, and Takuro Kutsuna.
2018. Neural edit operations for biological se-
quences. In Advances in Neural Information Pro-
cessing Systems 31.

D. Koller and N. Friedman. 2009. Probabilistic Graph-
ical Models: Principles and Techniques. MIT Press.

Jason Lee, Elman Mansimov, and Kyunghyun Cho.
2018. Deterministic non-autoregressive neural se-
quence modeling by iterative refinement. In Pro-
ceedings of the 2018 Conference on Empirical Meth-
ods in Natural Language Processing.

Vladimir I Levenshtein. 1966. Binary codes capable
of correcting deletions, insertions, and reversals. In
Soviet physics doklady, volume 10, pages 707–710.

Jared Lichtarge, Chris Alberti, Shankar Kumar, Noam
Shazeer, Niki Parmar, and Simon Tong. 2019. Cor-
pora generation for grammatical error correction.
arXiv preprint arXiv:1904.05780.

Tomoya Mizumoto, Mamoru Komachi, Masaaki Na-
gata, and Yuji Matsumoto. 2011. Mining revision
log of language learning sns for automated japanese
error correction of second language learners. In Pro-
ceedings of 5th International Joint Conference on
Natural Language Processing, pages 147–155.

Courtney Napoles, Keisuke Sakaguchi, Matt Post, and
Joel Tetreault. 2016. Gleu without tuning. arXiv
preprint arXiv:1605.02592.

Courtney Napoles, Keisuke Sakaguchi, and Joel
Tetreault. 2017. Jfleg: A fluency corpus and bench-
mark for grammatical error correction. In Proceed-
ings of the 15th Conference of the European Chap-
ter of the Association for Computational Linguistics:
Volume 2, Short Papers, pages 229–234, Valencia,
Spain. Association for Computational Linguistics.

Hwee Tou Ng, Siew Mei Wu, Ted Briscoe, Christian
Hadiwinoto, Raymond Hendy Susanto, and Christo-
pher Bryant. 2014. The CoNLL-2014 shared task
on grammatical error correction. In Proceedings of
the Eighteenth Conference on Computational Natu-
ral Language Learning: Shared Task, pages 1–14.

Hwee Tou Ng, Siew Mei Wu, Yuanbin Wu, Christian
Hadiwinoto, and Joel Tetreault. 2013. The conll-
2013 shared task on grammatical error correction.
In Proceedings of the Seventeenth Conference on
Computational Natural Language Learning: Shared
Task, pages 1–12, Sofia, Bulgaria. Association for
Computational Linguistics.

Aaron van den Oord, Yazhe Li, Igor Babuschkin, Karen
Simonyan, Oriol Vinyals, Koray Kavukcuoglu,
George van den Driessche, Edward Lockhart, Luis
Cobo, Florian Stimberg, Norman Casagrande, Do-
minik Grewe, Seb Noury, Sander Dieleman, Erich
Elsen, Nal Kalchbrenner, Heiga Zen, Alex Graves,
Helen King, Tom Walters, Dan Belov, and Demis
Hassabis. 2018. Parallel WaveNet: Fast high-
fidelity speech synthesis. In Proceedings of the
35th International Conference on Machine Learn-
ing, pages 3918–3926.

Joana Ribeiro, Shashi Narayan, Shay B. Cohen, and
Xavier Carreras. 2018. Local string transduction as
sequence labeling. In Proceedings of the 27th Inter-
national Conference on Computational Linguistics
(ACL-COLING).

https://doi.org/10.18653/v1/P17-1070
https://doi.org/10.18653/v1/P17-1070
https://doi.org/10.18653/v1/P17-1070
http://www.aclweb.org/anthology/E17-2037
http://www.aclweb.org/anthology/E17-2037
https://www.aclweb.org/anthology/W13-3601
https://www.aclweb.org/anthology/W13-3601


4270

Alla Rozovskaya and Dan Roth. 2016. Grammatical
error correction: Machine translation and classifiers.
In Proceedings of the 54th Annual Meeting of the
Association for Computational Linguistics (Volume
1: Long Papers), volume 1, pages 2205–2215.

Allen Schmaltz, Yoon Kim, Alexander M. Rush, and
Stuart M. Shieber. 2017. Adapting sequence models
for sentence correction. In EMNLP, pages 2807–
2813.

Miikka Silfverberg, Pekka Kauppinen, Krister Lindén,
et al. 2016. Data-driven spelling correction using
weighted finite-state methods. In The 54th Annual
Meeting of the Association for Computational Lin-
guistics Proceedings of the SIGFSM Workshop on
Statistical NLP and Weighted Automata. ACL.

Felix Stahlberg, Christopher Bryant, and Bill Byrne.
2019. Neural grammatical error correction
with finite state transducers. arXiv preprint
arXiv:1903.10625.

Mitchell Stern, Noam Shazeer, and Jakob Uszkoreit.
2018. Blockwise parallel decoding for deep autore-
gressive models. In Advances in Neural Information
Processing Systems 31.

Ziang Xie, Anand Avati, Naveen Arivazhagan, Dan Ju-
rafsky, and Andrew Y. Ng. 2016. Neural language
correction with character-based attention. CoRR,
abs/1603.09727.

Helen Yannakoudakis, Ted Briscoe, and Ben Medlock.
2011. A new dataset and method for automatically
grading esol texts. In Proceedings of the 49th An-
nual Meeting of the Association for Computational
Linguistics: Human Language Technologies-Volume
1, pages 180–189. Association for Computational
Linguistics.

Helen Yannakoudakis, Marek Rei, Øistein E Ander-
sen, and Zheng Yuan. 2017. Neural sequence-
labelling models for grammatical error correction.
In EMNLP, pages 2795–2806.

Pengcheng Yin, Graham Neubig, Miltiadis Allamanis,
Marc Brockschmidt, and Alexander L. Gaunt. 2019.
Learning to represent edits. In International Confer-
ence on Learning Representations.

Wei Zhao, Liang Wang, Kewei Shen, Ruoyu Jia, and
Jingming Liu. 2019. Improving grammatical er-
ror correction via pre-training a copy-augmented
architecture with unlabeled data. arXiv preprint
arXiv:1903.00138.

Yukun Zhu, Ryan Kiros, Rich Zemel, Ruslan Salakhut-
dinov, Raquel Urtasun, Antonio Torralba, and Sanja
Fidler. 2015. Aligning books and movies: Towards
story-like visual explanations by watching movies
and reading books. In Proceedings of the IEEE
international conference on computer vision, pages
19–27.


