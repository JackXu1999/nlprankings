



















































Japanese Word Reordering Integrated with Dependency Parsing


Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers,
pages 1186–1196, Dublin, Ireland, August 23-29 2014.

Japanese Word Reordering Integrated with Dependency Parsing

Kazushi Yoshida1,a) Tomohiro Ohno2,b) Yoshihide Kato3,c) Shigeki Matsubara1,d)
1Graduate School of Information Science, Nagoya University, Japan

2Information Technology Center, Nagoya University, Japan
3Information & Communications, Nagoya University, Japan

a)yoshida@db.ss.is.nagoya-u.ac.jp b)ohno@nagoya-u.jp
c)yoshihide@icts.nagoya-u.ac.jp d)matubara@nagoya-u.jp

Abstract

Although Japanese has relatively free word order, Japanese word order is not completely arbitrary
and has some sort of preference. Since such preference is incompletely understood, even native
Japanese writers often write Japanese sentences which are grammatically well-formed but not
easy to read. This paper proposes a method for reordering words in a Japanese sentence so
that the sentence becomes more readable. Our method can identify more suitable word order
than conventional word reordering methods by concurrently performing dependency parsing and
word reordering instead of sequentially performing the two processing steps. As the result of an
experiment on word reordering using newspaper articles, we confirmed the effectiveness of our
method.

1 Introduction

Japanese has relatively free word order, and thus Japanese sentences which make sense can be written
without having a strong awareness of word order. However, Japanese word order is not completely
arbitrary and has some sort of preference. Since such preference is incompletely understood, even native
Japanese writers often write Japanese sentences which are grammatically well-formed but not easy to
read. The word reordering of such sentences enables the readability to be improved.

There have been proposed some methods for reordering words in a Japanese sentence so that the
sentence becomes easier to read (Uchimoto et al., 2000; Yokobayashi et al., 2004). In addition, there
exist a lot of researches for estimating appropriate word order in various languages (Filippova and Strube,
2007; Harbusch et al., 2006; Kruijff et al., 2001; Ringger et al., 2004; Shaw and Hatzivassiloglou, 1999).
Although most of these previous researches used syntactic information, the sentences they used there
were what had been previously parsed. It is a problem that word reordering suffers the influence of
parsing errors. Furthermore, as the related works, there are various researches on word reordering for
improving the performance of statistical machine translation (Goto et al., 2012; Elming, 2008; Ge, 2010;
Christoph and Hermann, 2003; Nizar, 2007). These researches consider information as to both a source
language and a target language to handle word order differences between them. Therefore, their problem
setting is different from that for improving the readability of a single language.

This paper proposes a method for reordering words in a Japanese sentence so that the sentence becomes
easier to read for revision support. Our proposed method concurrently performs dependency parsing
and word reordering for an input sentence of which the dependency structure is still unknown. Our
method can identify more suitable word order than conventional word reordering methods because it
can concurrently consider the preference of both word order and dependency. An experiment using
newspaper articles showed the effectiveness of our method.

2 Word Order and Dependency in Japanese Sentences

There have been a lot of researches on Japanese word order in linguistics (for example, Nihongo Kijutsu
Bunpo Kenkyukai, 2009; Saeki, 1998), which have marshalled fundamental contributing factors which
This work is licenced under a Creative Commons Attribution 4.0 International License. Page numbers and proceedings footer
are added by the organizers. License details: http://creativecommons.org/licenses/by/4.0/

1186



naganen

(for 

years)

sekaiju-no

(all over 

the world)

hitobito-

ga

(people)

torikun-de-

ki-ta

(have tackled)

kare-

ga

(He)

mondai-wo

(the

problem)

tsuini

(finally)

S1 (inappropriate word order)

kaiketsu-

shi-ta

(resolved)

【He finally resolved the problem that people all over the world have tackled for years.】

S2 (appropriate word order)

naganen

(for 

years)

sekaiju-no

(all over 

the world)

hitobito-

ga

(people)

torikun-de-

ki-ta

(have tackled)

kare-

ga

(He)

mondai-wo

(the

problem)

tsuini

(finally)

kaiketsu-

shi-ta

(resolved)

Figure 1: Example of inappropriate/appropriate word order

decide the appropriate word order in detail. In a Japanese sentence, a predicate of the main clause is
fundamentally placed in last position, and thus, case elements, adverbial elements, or subordinate clauses
are located before it. In addition, case elements are basically placed in the order of a nominative, a dative
and an accusative. However, the basic order of case elements is often changed by being influenced from
grammatical and discourse factors. For example, it is pointed out that a long case element has strong
preference to be located at the beginning of a sentence even if the element is not nominative, as shown
in Figure 1.

In Figure 1, a box and an arrow express a bunsetsu1 and a dependency relation respectively. Both the
sentences S1 and S2 have the same meaning which is translated as “He finally resolved the problem that
people all over the world have tackled for years” in English. The difference between S1 and S2 is just in
their word orders in Japanese.

The word order of S1 is more difficult to read than that of S2 because the distance between the bun-
setsu “kare-ga (He)” and its modified bunsetsu “kaiketsu-shi-ta (resolved)” is large and thus the loads on
working memory become large. This example suggests that if the dependency structure of S1 is iden-
tified, that information is useful to reorder the word order of S1 to that of S2 so that it becomes easier
to read. In fact, most of the conventional word reordering methods have reordered words using the pre-
viously parsed dependency structure. However, the word order of S1 is thought to be more difficult to
parse than that of S2 because dependency parsers are usually trained on syntactically annotated corpora
in which sentences have the appropriate word order such as that in S2. This is why it is highly possible
that dependency parsing can achieve a higher accuracy by changing the word order of S1 to that of S2 in
advance.

The above observations indicate that word reordering and dependency parsing depend on each other.
Therefore, we consider it is more desirable to concurrently perform the two processings than to sequen-
tially perform them.

3 Word Reordering Method

In our method, a sentence, on which morphological analysis and bunsetsu segmentation have been per-
formed, is considered as the input2. We assume that the input sentence might have unsuitable word order,

1Bunsetsu is a linguistic unit in Japanese that roughly corresponds to a basic phrase in English. A bunsetsu consists of one
independent word and zero or more ancillary words. A dependency relation in Japanese is a modification relation in which a
modifier bunsetsu depends on a modified bunsetsu. That is, the modifier bunsetsu and the modified bunsetsu work as modifier
and modifyee, respectively.

2In order to focus attention on the comparison between our method and the conventional method, we assumed the input on
which the lower layer processings than dependency parsing have been performed. Even if morphological analysis and bunsetsu
segmentation are automatically performed on input sentences which have unsuitable word order, we can expect the accuracies

1187



which is not easy to read but grammatically well-formed. Our method identifies the suitable word order
which is easy to read by concurrently performing dependency parsing.

The simultaneous performing of dependency parsing and word reordering is realized by searching for
the maximum-likelihood pattern of word order and dependency structure for the input sentence. Note our
method reorders bunsetsus in a sentence without paraphrasing and does not reorder morphemes within a
bunsetsu.

3.1 Probabilistic Model for Word Reordering

When a sequence of bunsetsus in an input sentence B = b1· · ·bn is provided, our method identifies the
structure S which maximizes P (S|B). The structure S is defined as a tuple S = ⟨O, D⟩ where O =
{o1,2, o1,3, · · · , o1,n, · · · , oi,j , · · · , on−2,n−1, on−2,n, on−1,n} is the word order pattern after reordering
and D = {d1, · · · , dn−1} is dependency structure. Here, oi,j (1 ≤ i < j ≤ n) expresses the order
between bi and bj after reordering. oi,j is 1 if bi is located before bj , and is 0 otherwise. In addition, di
expresses the dependency relation whose modifier bunsetsu is bi.

P (S|B) for a S = ⟨O,D⟩ is calculated as follows.

P (S|B) = P (O,D|B)
=

√
P (O|B) × P (D|O, B) × P (D|B) × P (O|D, B) (1)

Formula (1) is obtained for the product of the following two formulas. According to the probability
theory, the calculated result of Formula (1) is equal to those of Formulas (2) and (3). However, in practice,
since each factor in the formulas is estimated based on the corpus used for training, the calculated results
of these formulas are different from each other. We use Formula (1) to estimate P (S|B) by using both
values of P (D|O, B) and P (O|D, B). In fact, we pre-experimentally confirmed that the calculated
result of Formula (1) was better than those of the others.

P (O, D|B) = P (O|B) × P (D|O, B) (2)
P (O, D|B) = P (D|B) × P (O|D, B) (3)

Assuming that order oi,j between two bunsetsus is independent of that between other two bunsetsus
and that each dependency relation di is independent of the others, each factor in Formula (1) can be
approximated as follows:

P (O|B) ∼=
n−1∏
i=1

n∏
j=i+1

P (oi,j |B) (4)

P (D|O, B) ∼=
n−1∏
i=1

P (di|O,B) (5)

P (D|B) ∼=
n−1∏
i=1

P (di|B) (6)

P (O|D, B) ∼=
n−1∏
i=1

n∏
j=i+1

P (oi,j |D,B) (7)

where P (oi,j |B) is the probability that the order between bi and bj is oi,j when B is provided, P (di|O, B)
is the probability that the dependency relation whose modifier bunsetsu is bi is di when the sentence
generated by reordering B according to O is provided, P (di|B) is the probability that the dependency
relation whose modifier bunsetsu is bi is di when B is provided, and P (oi,j |D, B) is the probability
that the order between bi and bj is oi,j when B where the dependency relation is D is provided. These
probabilities are estimated by the maximum entropy method.

remain comparatively high. This is because their processings use mainly local information.

1188



To estimate P (di|O, B), we used the features used in Uchimoto et al. (1999) except when eliminating
features about Japanese commas (called toten, which is a kind of punctuation) and quotation marks.
To estimate P (di|B), we used the features which can be obtained without information about the order
of input bunsetsus among the features used in estimating P (di|O,B). To estimate P (oi,j |D,B), if bi
and bj modifies the same bunsetsu, we used the features used in Uchimoto et al. (2000), except when
eliminating features about parallel relations and semantic features. Otherwise, we used the features left
after eliminating features about modified bunsetsus from those used in the above-mentioned case. To
estimate P (oi,j |B), we used the features which can be obtained without dependency information among
the features used to estimate P (Oi,j |D, B).
3.2 Search Algorithm

Since there are a huge number of the structures S = ⟨O, D⟩ which are theoretically possible for an input
sentence B, an efficient algorithm is desired. However, since O and D are dependent on each other,
it is difficult to find the optimal structure efficiently. In our research, we extend CYK algorithm used
in conventional dependency parsing to efficiently find the suboptimal S = ⟨O, D⟩ which maximizes
P (S|B) efficiently.

Our research assumes that an input sentence, which is grammatically well-formed, is reordered without
changing the meaning so that the sentence becomes much easier to read. From this assumption, we can
use following conditions for efficient search:

1. The dependency structure of an input sentence should satisfy the following Japanese syntactic con-
straints under the input word order:

• No dependency is directed from right to left.
• Dependencies don’t cross each other.
• Each bunsetsu, except the last one, depends on only one bunsetsu.

2. Even after the words are reordered, the dependency structure should satisfy the above-mentioned
Japanese syntactic constraints under the changed word order.

3. The dependency structures of a sentence before and after reordering should be identical.

Using the condition 1 and the condition 3, we can narrow down the search space of D to dependency
structures that satisfy Japanese syntactic constraints under the input word order. Furthermore, the search
space of O can be narrowed down to the word order patterns derived from the above narrowed depen-
dency structures based on the conditions 2 and 3. That is, after dependency structures possible for an
input sentence are narrowed down, we just have to find the word order patterns after reordering so that
each of the dependency structures is maintained and satisfies the Japanese syntactic constraints even
under the changed word order.

On the other hand, it is well known that CYK algorithm can efficiently find the optimal dependency
structure which satisfies Japanese syntactic constraints. Therefore, in our research, we have extended the
CYK algorithm for the conventional dependency parsing so that it can find the suboptimal D and O from
among the dependency structures and word order patterns which satisfy the conditions 1, 2 and 3.

3.2.1 Word Reordering Algorithm
Algorithm 1 shows our word reordering algorithm. In our algorithm, the n×n triangular matrix Mi,j(1 ≤
i ≤ j ≤ n) such as the left-side figure in Figure 2 is prepared for an input sentence consisting of n
numbers of bunsetsus. Mi,j , the element of the triangular matrix M in the i-th row and j-th column, is
filled by argmaxSi,jP (Si,j |Bi,j), which is the maximum-likelihood structure for an input subsequence
Bi,j = bi · · · bj . In this section, for convenience of explanation, we represent Si,j as a sequence of
dependency relations dx(i ≤ x ≤ j). For example, Si,j = didi+1 · · · d0j means that the first bunsetsu
is bi, the second is bi+1, · · · , the last is bj , and the dependency structure is {di, di+1, · · · , dj−1}. Here,
if we need to clearly specify the modified bunsetsu, we represent the dependency relation that bunsetsu

1189



Algorithm 1 word reordering algorithm
1: input B1,n = b1 · · · bn // input sentence
2: set Mi,j (1 ≤ i ≤ j ≤ n) // triangular matrix
3: set Ci,j (1 ≤ i ≤ j ≤ n) // set of structure candidates
4: for i = 1 to n do
5: Mi,i = d0i
6: end for
7: for d = 1 to n − 1 do
8: for i = 1 to n − d do
9: j = i + d

10: for k = i to j − 1 do
11: Ci,j = Ci,j ∪ ConcatReorder(Mi,k,Mk+1,j)
12: end for
13: Mi,j = argmaxSi,j∈Ci,jP (Si,j |Bi,j)
14: end for
15: end for
16: return M1,n

�Candidates generated by 

・By the concatenating process

・By the reordering process

M1,1= M1,2= M1,3= M1,4

M2,2= M2,3= M2,4

M3,3= M3,4=

M4,4=

2 2 3

3 4

4

�Candidates generated by 

・By the concatenating process

2 3 4

23 4

・ By the reordering process

2 3 4

is filled by the structure which maximizes 

among the following candidates.

Candidate 1:

Candidate 3:

Candidate 2:

： means that is located  before and 

depends on . For example, 

i j

：is filled by the maximum-likelihood 

structure for a subsequence from to .

3

1 1 2 2 31

No candidate is generated because has no child in .2 31 means .

by moving after , which is 

the first child of in 

Figure 2: Execution example of our search algorithm

bx modifies by as dyx. In addition, d
0
j means that the last bunsetsu of the subsequence don’t modify any

bunsetsu.
First, the statements of the lines 4 to 6 fill each of diagonal elements Mi,i (1 ≤ i ≤ n) with d0i . Next,

the statements of the lines 7 to 15 fill Mi,j in turn toward the upper right M1,n along the diagonal line,
starting from the diagonal elements Mi,i. The maximum-likelihood structure which should fill an Mi,j
is found as follows:

The statements of the lines 10 to 12 repeat the process of generating candidates of the maximum-
likelihood structure from Mi,k and Mk+1,j by the function ConcatReorder, and adding them to the set
of structure candidates Ci,j . The function ConcatReorder takes two arguments of Mi,k and Mk+1,j and
returns the set of candidates of the maximum-likelihood structure which should fill Mi,j . The function
ConcatReorder is composed of two processes: concatenating process and reordering process. First,
the concatenating process generates a candidate by simply concatenating Mi,k and Mk+1,j in turn about
the word order and connecting Mi,k and Mk+1,j by the dependency relation between the last bunsetsus
of them about the dependency structure, without changing the internal structure of each of them. For
example, when Mi,k = didi+1 · · · dk−1d0k and Mk+1,j = dk+1dk+2 · · · dj−1d0j are given as the argument,
the concatenating process generates “didi+1 · · · dk−1djkdk+1dk+2 · · · dj−1d0j .”

1190



Second, the reordering process generates candidates by reordering words in the candidate generated
by the concatenating process. The reordering is executed on the following conditions. The first condition
is that the dependency structure is maintained and satisfies the Japanese syntactic constraints even under
the changed word order. The second condition is that the order of any two words within each of Mi,k
and Mk+1,j is maintained. Concretely, the first reordered candidate is generated by moving Mi,k after
the first (leftmost) child3 of the last bunsetsu of Mk+1,j among the children in Mk+1,j . Then, the sec-
ond reordered candidate is generated by moving Mi,k after the second child. The reordering process is
continued until the last reordered candidate is generated by moving Mi,k after the last child. That is, the
number of candidates generated by the reordering process is equal to the number of children of the last
bunsetsu in Mk+1,j . For example, when Mi,k = didi+1 · · · dk−1d0k and Mk+1,j = djk+1djk+2 · · · djj−1d0j ,
which means all bunsetsus except the last one depend on the last one, are given, the reordering
process generates the following j − k − 1 candidates: “djk+1didi+1 · · · dk−1djkdjk+2djk+3 · · · djj−1d0j ,”
“djk+1d

j
k+2didi+1 · · · dk−1djkdjk+3djk+4 · · · djj−1d0j ,” . . ., and “djk+1djk+2 · · · djj−1didi+1 · · · dk−1djkd0j .”

Therefore, in this case, the function ConcatReorder finally returns the set of candidates of which size
is j−k, which includes the candidates generated by the reordering process and a candidate generated by
the concatenating process. Next, in the line 13, our algorithm fills in argmaxSi,j∈Ci,jP (Si,j |Bi,j) which
is the maximum-likelihood structure for a subsequence Bi,j on Mi,j .

Finally, our algorithm outputs M1,n as the maximum-likelihood structure of word order and depen-
dency structure for the input sentence.

Note that if the function ConcatReorder is changed to the function Concat in the line 11, our algorithm
becomes the same as CYK algorithm used in the conventional dependency parsing. The function Concat
takes two arguments of Mi,k and Mk+1,j and generates a candidate of the maximum-likelihood structure
which should fill Mi,j by the same way as the concatenating process in the function ConcatReorder.
Then, the function Concat returns the set which has the generated candidate as a element, of which size
is 1.

3.2.2 Execution Example of Word Reordering Algorithm
Figure 2 represents an example of execution of our word reordering algorithm in n = 4. The left side
of Figure 2 represents the triangle diagram which has 4 × 4 dimensions. The elements of the triangle
diagram M1,1,M2,2,M3,3,M4,4,M1,2,M2,3,M3,4, and M1,3 have already been filled in turn, and M2,4
is being filled. The right side of Figure 2 shows the process of calculating the maximum-likelihood
structure which should fill M2,4. First, in the loop from the line 10 to the line 12 in Algorithm 1,
two structure candidates are generated by ConcatReorder(M2,2, M3,4). The candidate 1 is generated
by the concatenating process, that is, by simply concatenating M2,2 and M3,4 and connecting the last
bunsetsu of M2,2 and that of M3,4. The candidate 2 is generated by the reordering process, that is, by
moving M2,2 after b3, which is the first child of b4 in M3,4. Second, the candidate 3 is generated by
the concatenating process in ConcatReorder(M2,3,M4,4). On the other hand, the reordering process in
ConcatReorder(M2,3,M4,4) generates no candidates because b4 has no child in M4,4. Among the three
structures generated in the above way, the structure which maximizes P (S2,4|B) = P (O2,4, D2,4|B2,4)
fills M2,4.

4 Experiment

To evaluate the effectiveness of our method, we conducted an experiment on word reordering by using
Japanese newspaper articles.

4.1 Outline of Experiment

In the experiment, as the test data, we used sentences generated by only changing the word order of
newspaper article sentences in Kyoto Text Corpus (Kurohashi and Nagao, 1998), maintaining the depen-
dency structure. That is, we artificially generated sentences which made sense but were not easy to read,

3When bi depends on bj , we call bi as a child of bj . Furthermore, if bj has more than or equal to one child, the children are
numbered from left to right based on their positions.

1191



kokkai-

wo

(the 

Diet)

toot-ta

(passed)

ato-

demo

(Even 

after)

seron-chosa-

de-wa

(according to 

opinion polls)

shohi-ze-

zoze-ga

(the consumption 

tax hike bill)

zoze-hantai-

ga

(opposing views 

to the bill)

Original sentence 

in newspaper articles 

(correct word order)

taise-

da

(are in 

majority)

【Even after the Diet passed the consumption tax hike bill,

according to opinion polls opposing views to the bill are in majority.】

Test data 

(input sentence)

test data generation

kokkai-

wo

(the 

Diet)

toot-ta

(passed)

ato-

demo

(Even 

after)

seron-chosa-

de-wa

(according to 

opinion polls)

shohi-ze-

zoze-ga

(the consumption 

tax hike bill)

zoze-hantai-

ga

(opposing views 

to the bill)

taise-

da

(are in 

majority)

Figure 3: Example of test data generation

in order to focus solely on problems caused by unsuitable word order. Figure 3 shows an example of the
test data generation. The generation procedure is as follows:

1. Find a bunsetsu modified by multiple bunsetsus from the sentence end.

2. Change randomly the order of the sub-trees which modify such bunsetsu.

3. Iterate 1 and 2 until reaching the beginning of the sentence.

In Figure 3, the bunsetsus “taise-da (are in the majority)” and “toot-ta (passed)” are found as bunsetsus
modified by multiple bunsetsus. For example, when “toot-ta (passed)” is found, the order of “shohi-
ze-zoze-ga (the consumption tax hike bill)” and “kokkai-wo (the Diet)” is randomly changed. In this
experiment, all Japanese commas (toten) in a sentence, and sentences which have quotation marks were
removed.

In this way, we artificially generated 865 sentences (7,620 bunsetsus) from newspaper articles of Jan.
9 in Kyoto Text Corpus and used them as the test data. As the training data, we used 7,976 sentences in 7
days’ newspaper articles (Jan. 1, 3-8). Here, we used the maximum entropy method tool (Zhang, 2008)
with the default options except “-i 1000.”

In the evaluation of word reordering, we obtained the following two measurements, which are defined
by Uchimoto et al. (2000):

• complete agreement: the percentage of the sentences in which all words’ order completely agrees
with that of the original sentence.

• pair agreement: the percentage of the pairs of bunsetsus whose word order agrees with that in the
original sentence. (For example, in Figure 3, if the word order of the input sentence is not changed
after reordering, the pair agreement is 52.4% (= 11/7C2) because the 11 pairs out of the 7C2 pairs
are the same as those in the original sentence.)

In the evaluation of dependency parsing, we obtained the dependency accuracy (the percentage of
correctly analyzed dependencies out of all dependencies) and sentence accuracy (the percentage of
the sentences in which all the dependencies are analyzed correctly), which are defined by Sekine et al.
(2000).

For comparison, we established two baselines. Both of the baselines execute the dependency pars-
ing primarily, and then, perform the word reordering by using the conventional word reordering method

1192



Table 1: Experimental results (word reordering)
pair agreement complete agreement

our method 77.3% (30,190/38,838) 25.7% (222/865)
baseline 1 75.4% (29,279/38,838)* 23.8% (206/865)
baseline 2 74.8% (29,067/38,838)* 23.5% (203/865)
no reordering 61.5% (23,886/38,838)* 8.0% (69/865)*

Note that the agreements followed by * differ signifi-
cantly from those of our method (p < 0.05).

Table 2: Experimental results (dependency parsing)
dependency accuracy sentence accuracy

our method 78.4% (5,293/6,755) 35.3% (305/865)
baseline 1 79.2% (5,350/6,755) 31.6% (273/865)*

baseline 2 81.2% (5,487/6,755)* 32.1% (278/865)*

Note that the accuracies followed by * differ sig-
nificantly from those of our method (p < 0.05).

(Uchimoto et al., 1999). The difference between the two is the method of dependency parsing. The
baselines 1 and 2 use the dependency parsing method proposed by Uchimoto et al. (2000) and the de-
pendency parsing tool CaboCha (Kudo and Matsumoto, 2002), respectively. The features used for the
word reordering in both the baselines are the same as those used to estimate P (oi,j |D, B) in our method.
Additionally, the features used for the dependency parsing in the baseline 1 are the same as those used to
estimate P (di|O, B) in our method.

4.2 Experimental Results

Table 1 shows the experimental results on word reordering of our method and the baselines. Here, the
last row shows the agreements measured by comparing the input word order with the correct word order.
The agreements mean the values which can be achieved with no reordering4. The pair and complete
agreements of our method were highest among all. The pair agreement of our method is significantly
different from those of both the baselines (p < 0.05) although there is no significant difference between
the complete agreements of them.

Next, Table 2 shows the experimental results on dependency parsing. The sentence accuracy of our
method is significantly higher than those of both the baselines (p < 0.05). On the other hand, the
dependency accuracy of our method is significantly lower than that of the baseline 2 although there is no
significant difference between the dependency accuracies of our method and the baseline 1 (p > 0.05).
Here, if the input sentences had the correct word order, the dependency accuracies of the baselines 1 and
2 were 86.4% (5,835/6,755) and 88.1% (5,950/6,755), respectively. We can see that the unsuitable word
order caused a large decrease of the accuracies of the conventional dependency parsing methods. This is
why the word order agreements of the baselines were decreased.

Figure 4 shows an example of sentences of which all bunsetsus were correctly reordered and the de-
pendency structure was correctly parsed only by our method. We can see that our method can achieve
the complicated word reordering. On the other hand, Figure 5 shows an example of sentences incorrectly
reordered and parsed by our method. In this example, our method could not identify the correct modified
bunsetsu and the appropriate position of the bunsetsu “arikata-wo (whole concept).” This is because the
dependency probability between the bunsetsu “arikata-wo (whole concept)” and the bunsetsu “fukume

4Some input sentences were in complete agreement with the original ordering. There were some cases that the randomly
reordered sentences accidentally have the same word order as the original ones. In addition, there were some sentences in
which all bunsetsus except the last one depend on the next bunsetsu. The word order of such sentences is not changed by the
test data generation procedure because the procedure is executed on condition of maintaining the dependency structure.

1193



Input sentence

(inappropriate word order) 

【Although I myself do not have an experience with a war, I think any generation should not glorify war.】

Output sentence

(correct word order and 

dependency structure)

itsu-

no

(any)

sedai-mo

(generation)

bika-su-beki-

de-nai-to

(should not 

glorify)

senso-

wo

(with 

a war)

senso-

wo

(war)

taiken-

shi-ta

(have an  

experience)

koto-

wa

(φ)

watashi-

jishin

(I myself)

omou

(think)

itsu-

no

(any)

sedai-mo

(generation)

bika-su-beki-

de-nai-to

(should not 

glorify)

senso-

wo

(with 

a war)

senso-

wo

(war)

taiken-

shi-ta

(have an  

experience)

koto-

wa

(φ)

watashi-

jishin

(I myself)

nai-ga

(although 

do not)

omou

(think)

nai-ga

(although 

do not)

:  shows an alignment of a bunsetsu before and after reordering.

:  shows a correct dependency relation.

φ  :  means there is no English word corresponding to the Japanese word.

Figure 4: Example of sentences correctly reordered and parsed by our method

【Whole concept of the examination of rice should be fundamentally revised including 

the transfer of control to a private sector or prefectural and city governments.】

Input sentence

(inappropriate word order) 

Output sentence

(incorrect word order and 

dependency structure)

kensa-no

(of the 

exami-

nation)

arikata-

wo

(whole 

concept)

todofuken-ya

(or prefectural 

and city 

governments)

minkan-

e-no

(to a private 

sector)

kome-

no

(of rice)

ikan-mo

(the 

transfer

of control)

fukume

(including)

konpon-

teki-ni

(fundamen-

tally)

minaosu-

beki-daro

(should 

be revised)

Original sentence

(correct word order and 

dependency structure)

kensa-no

(of the 

exami-

nation)

arikata-

wo

(whole 

concept)

todofuken-ya

(or prefectural 

and city 

governments)

minkan-

e-no

(to a private 

sector)

kome-

no

(of rice)

ikan-mo

(the 

transfer

of control)

fukume

(including)

konpon-

teki-ni

(fundamen-

tally)

minaosu-

beki-daro

(should 

be revised)

kensa-no

(of the 

exami-

nation)

arikata-

wo

(whole 

concept)

todofuken-ya

(or prefectural 

and city 

governments)

minkan-

e-no

(to a private 

sector)

kome-

no

(of rice)

ikan-mo

(the 

transfer

of control)

fukume

(including)

konpon-

teki-ni

(fundamen-

tally)

minaosu-

beki-daro

(should 

be revised)

: shows an alignment of a bunsetsu before and after reordering.

:  shows a correct dependency relation.

:  shows an incorrect dependency relation.

Figure 5: Example of sentences incorrectly reordered and parsed by our method

(including)” is higher than the one between the bunsetsu “arikata-wo (whole concept)” and the bunsetsu
“minaosu-beki-daro (should be revised)”, and the probability that the bunsetsu “arikata-wo (whole con-
cept)” is located at the left side of “fukume (including)” is higher than that of the right side. Since the
word order of the output sentence has a strong probability of causing a wrong interpretation like “The
transfer of control to a private sector or prefectural and city governments should be fundamentally re-
vised including whole concept of the examination of rice.”, this reordering has a harmful influence on
the comprehension. We need to study techniques for avoiding the word order which causes the change
of meanings in an input sentence.

From the above, we confirmed the effectiveness of our method on word reordering and dependency
parsing of a sentence of which the word order is not easy to read.

5 Conclusion

This paper proposed the method for reordering bunsetsus in a Japanese sentence. Our method can identify
suitable word order by concurrently performing word reordering and dependency parsing. Based on the

1194



idea of limiting the search space using the Japanese syntactic constraints, we made the search algorithm
by extending the CYK algorithm used in the conventional dependency parsing, and found the optimal
structure efficiently. The result of the experiment using newspaper articles showed the effectiveness of
our method.

In our future works, we would like to collect sentences written by Japanese subjects who do not have
much writing skills, to conduct an experiment using those sentences. In addition, we would like to
conduct a subjective evaluation to investigate whether the output sentences are indeed more readable
than the input ones.

Acknowledgments

This research was partially supported by the Grant-in-Aid for Young Scientists (B) (No.25730134) and
Challenging Exploratory Research (No.24650066) of JSPS.

References

Tillmann Christoph and Ney Hermann. 2003. Word reordering and a dynamic programming beam search
algorithm for statistical machine translation. Computational Linguistics, 29(1):97–133.

Jakob Elming. 2008. Syntactic reordering integrated with phrase-based SMT. In Proceedings of the
22nd International Conference on Computational Linguistics (COLING2008), pages 209–216.

Katja Filippova and Michael Strube. 2007. Generating constituent order in German clauses. In Proceed-
ings of the 45th Annual Meeting of the Association for Computational Linguistics (ACL2007), pages
320–327.

Niyu Ge. 2010. A direct syntax-driven reordering model for phrase-based machine translation. In
Proceedings of Human Language Technologies: The 11th Annual Conference of the North American
Chapter of the Association for Computational Linguistics (NAACL-HLT2010), pages 849–857.

Geert-Jan M. Kruijff, Ivana Kruijff-Korbayová, John Bateman, and Elke Teich. 2001. Linear order as
higher-level decision: Information structure in strategic and tactical generation. In Proceedings of the
8th European Workshop on Natural Language Generation (ENLG2001), pages 74–83.

Isao Goto, Masao Utiyama, and Eiichiro Sumita. 2012. Post-ordering by parsing for Japanese-English
statistical machine translation. In Proceedings of the 50th Annual Meeting of the Association for
Computational Linguistics (ACL2012), pages 311–316.

Karin Harbusch, Gerard Kempen, Camiel van Breugel, and Ulrich Koch. 2006. A generation-oriented
workbench for performance grammar: Capturing linear order variability in German and Dutch. In
Proceedings of the 4th International Natural Language Generation Conference (INLG2006), pages
9–11.

Nihongo Kijutsu Bunpo Kenkyukai, editor. 2009. Gendai nihongo bunpo 7 (Contemporary Japanese
Grammar 7), pages 165–182. Kuroshio Shuppan. (In Japanese).

Taku Kudo and Yuji Matsumoto. 2002. Japanese dependency analysis using cascaded chunking. In Pro-
ceedings of the 6th Conference on Computational Natural Language Learning (CoNLL2002), pages
63–69.

Sadao Kurohashi and Makoto Nagao. 1998. Building a Japanese parsed corpus while improving the
parsing system. In Proceedings of the 1st International Conference on Language Resources and
Evaluation (LREC ’98), pages 719–724.

Habash Nizar. 2007. Syntactic preprocessing for statistical machine translation. In Proceedings of the
11th Machine Translation Summit (MT SUMMIT XI), pages 215–222.

Eric Ringger, Michael Gamon, Robert C. Moore, David Rojas, Martine Smets, and Simon Corston-
Oliver. 2004. Linguistically informed statistical models of constituent structure for ordering in sen-
tence realization. In Proceedings of the 20th International Conference on Computational Linguis-
tics (COLING2004), pages 673–679.

1195



Tetsuo Saeki. 1998. Yosetsu nihonbun no gojun (Survey: Word Order in Japanese Sentences). Kuroshio
Shuppan. (In Japanese).

Satoshi Sekine, Kiyotaka Uchimoto, and Hitoshi Isahara. 2000. Backward beam search algorithm for de-
pendency analysis of Japanese. In Proceedings of the 18th International Conference on Computational
Linguistics (COLING2000), volume 2, pages 754–760.

James Shaw and Vasileios Hatzivassiloglou. 1999. Ordering among premodifiers. In Proceedings of the
37th Annual Meeting of the Association for Computational Linguistics (ACL ’99), pages 135–143.

Kiyotaka Uchimoto, Satoshi Sekine, and Hitoshi Isahara. 1999. Japanese dependency structure analysis
based on maximum entropy models. In Proceedings of the 9th Conference of the European Chapter
of the Association for Computational Linguistics (EACL ’99), pages 196–203.

Kiyotaka Uchimoto, Masaki Murata, Qing Ma, Satoshi Sekine, and Hitoshi Isahara. 2000. Word order
acquisition from corpora. In Proceedings of the 18th International Conference on Computational
Linguistics (COLING2000), volume 2, pages 871–877.

Hiroshi Yokobayashi, Akira Suganuma, and Rin-ichiro Taniguchi. 2004. Generating candidates for
rewriting based on an indicator of complex dependency and it’s application to a writing tool. Journal
of Information Processing Society of Japan, 45(5):1451–1459. (In Japanese).

Le Zhang. 2008. Maximum entropy modeling toolkit for Python and C++. http://homepages.
inf.ed.ac.uk/s0450736/maxent_toolkit.html. [Online; accessed 1-March-2008].

1196


