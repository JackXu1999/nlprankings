



















































Key Fact as Pivot: A Two-Stage Model for Low Resource Table-to-Text Generation


Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 2047–2057
Florence, Italy, July 28 - August 2, 2019. c©2019 Association for Computational Linguistics

2047

Key Fact as Pivot: A Two-Stage Model for Low Resource
Table-to-Text Generation

Shuming Ma,1,3 Pengcheng Yang,1,2 Tianyu Liu,1 Peng Li,3 Jie Zhou,3 Xu Sun1,2
1MOE Key Lab of Computational Linguistics, School of EECS, Peking University

2Deep Learning Lab, Beijing Institute of Big Data Research, Peking University
3Pattern Recognition Center, WeChat AI, Tencent Inc, China
{shumingma,yang pc,tianyu0421,xusun}@pku.edu.cn

{patrickpli,withtomzhou}@tencent.com

Abstract

Table-to-text generation aims to translate the
structured data into the unstructured text. Most
existing methods adopt the encoder-decoder
framework to learn the transformation, which
requires large-scale training samples. How-
ever, the lack of large parallel data is a ma-
jor practical problem for many domains. In
this work, we consider the scenario of low
resource table-to-text generation, where only
limited parallel data is available. We propose
a novel model to separate the generation into
two stages: key fact prediction and surface re-
alization. It first predicts the key facts from the
tables, and then generates the text with the key
facts. The training of key fact prediction needs
much fewer annotated data, while surface re-
alization can be trained with pseudo parallel
corpus. We evaluate our model on a biogra-
phy generation dataset. Our model can achieve
27.34 BLEU score with only 1, 000 parallel
data, while the baseline model only obtain the
performance of 9.71 BLEU score.1

1 Introduction

Table-to-text generation is to generate a descrip-
tion from the structured table. It helps readers to
summarize the key points in the table, and tell in
the natural language. Figure 1 shows an exam-
ple of table-to-text generation. The table provides
some structured information about a person named
“Denise Margaret Scott”, and the corresponding
text describes the person with the key information
in the table. Table-to-text generation can be ap-
plied in many scenarios, including weather report
generation (Liang et al., 2009), NBA news writ-
ing (Barzilay and Lapata, 2005), biography gener-
ation (Duboué and McKeown, 2002; Lebret et al.,
2016), and so on. Moreover, table-to-text genera-

1The codes are available at https://github.com/
lancopku/Pivot.

Denise Margaret Scott

Born 24 April 1955 
Melbourne, Victoria 

Nationality Australian

Other names Scotty

Occupation Comedian, actor, 
television and radio 
presenter

Known for Studio 10 

Partner(s) John Lane 

Children 2

Denise Margaret Scott (born 24 April 1955) is an 
Australian comedian, actor and television presenter.

Denise Margaret 
Scott

24 April 1955 

Australian

Comedian, actor, 
television and 
radio presenter

Key Fact 
Prediction

Surface 
Realization

Figure 1: An example of table-to-text generation, and
also a flow chart of our method.

tion is a good testbed of a model’s ability of un-
derstanding the structured knowledge.

Most of the existing methods for table-to-
text generation are based on the encoder-decoder
framework (Sutskever et al., 2014; Bahdanau
et al., 2014). They represent the source tables with
a neural encoder, and generate the text word-by-
word with a decoder conditioned on the source ta-
ble representation. Although the encoder-decoder
framework has proven successful in the area of
natural language generation (NLG) (Luong et al.,
2015; Chopra et al., 2016; Lu et al., 2017; Yang
et al., 2018), it requires a large parallel corpus, and
is known to fail when the corpus is not big enough.
Figure 2 shows the performance of a table-to-text
model trained with different number of parallel
data under the encoder-decoder framework. We
can see that the performance is poor when the par-
allel data size is low. In practice, we lack the large
parallel data in many domains, and it is expensive
to construct a high-quality parallel corpus.

This work focuses on the task of low resource
table-to-text generation, where only limited paral-

https://github.com/lancopku/Pivot
https://github.com/lancopku/Pivot


2048

0 10000 20000 30000 40000 50000 60000
Parallel Data Size

5

10

15

20

25

30

35
BL

EU

Figure 2: The BLEU scores of the a table-to-text model
trained with different number of parallel data under the
encoder-decoder framework on the WIKIBIO dataset.

lel data is available. Some previous work (Pudup-
pully et al., 2018; Gehrmann et al., 2018) formu-
lates the task as the combination of content se-
lection and surface realization, and models them
with an end-to-end model. Inspired by these work,
we break up the table-to-text generation into two
stages, each of which is performed by a model
trainable with only a few annotated data. Specifi-
cally, it first predicts the key facts from the tables,
and then generates the text with the key facts, as
shown in Figure 1. The two-stage method con-
sists of two separate models: a key fact prediction
model and a surface realization model. The key
fact prediction model is formulated as a sequence
labeling problem, so it needs much fewer anno-
tated data than the encoder-decoder models. Ac-
cording to our experiments, the model can obtain
87.92% F1 score with only 1, 000 annotated data.
As for the surface realization model, we propose a
method to construct a pseudo parallel dataset with-
out the need of labeled data. In this way, our model
can make full use of the unlabeled text, and allevi-
ate the heavy need of the parallel data.

The contributions of this work are as follows:

• We propose to break up the table-to-text gen-
eration into two stages with two separate
models, so that the model can be trained with
fewer annotated data.

• We propose a method to construct a pseudo
parallel dataset for the surface realization
model, without the need of labeled data.

• Experiments show that our proposed model
can achieve 27.34 BLEU score on a biogra-
phy generation dataset with only 1, 000 table-

text samples.

2 PIVOT: A Two-Stage Model

In this section, we introduce our proposed two-
stage model, which we denote as PIVOT. We first
give the formulation of the table-to-text generation
and the related notations. Then, we provide an
overview of the model. Finally, we describe the
two models for each stage in detail.

2.1 Formulation and Notations
Suppose we have a parallel table-to-text dataset P
with N data samples and an unlabeled text dataset
U with M samples. Each parallel sample con-
sists of a source table T and a text description
y = {y1, y2, · · · , yn}. The table T can be formu-
lated as K records T = {r1, r2, r3, · · · , rK}, and
each record is an attribute-value pair rj = (aj , vj).
Each sample in the unlabeled text dataset U is a
piece of text ȳ = {ȳ1, ȳ2, · · · , ȳn}.

Formally, the task of table-to-text generation
is to take the structured representations of ta-
ble T = {(a1, v1), (a2, v2), · · · , (am, vm)} as
input, and output the sequence of words y =
{y1, y2, · · · , yn}.

2.2 Overview
Figure 3 shows the overview architecture of our
proposed model. Our model contains two stages:
key fact prediction and surface realization. At
the first stage, we represent the table into a se-
quence, and use a table-to-pivot model to select
the key facts from the sequence. The table-to-
pivot model adpots a bi-directional Long Short-
term Memory Network (Bi-LSTM) to predict a bi-
nary sequence of whether each word is reserved
as the key facts. At the second stage, we build a
sequence-to-sequence model to take the key facts
selected in the first stage as input and emit the ta-
ble description. In order to make use of the un-
labeled text corpus, we propose a method to con-
struct pseudo parallel data to train a better surface
realization model. Moreover, we introduce a de-
noising data augmentation method to reduce the
risk of error propagation between two stages.

2.3 Preprocessing: Key Fact Selection
The two stages are trained separately, but we do
not have the labels of which words in the table are
the key facts in the dataset. In this work, we define
the co-occurrence facts between the table and the



2049

𝑥1

𝑥2

…

𝑥3

ℎ1

ℎ2

ℎ3

…

Denise Margaret Scott

Born 24 April 1955 
Melbourne, Victoria 

Nationality Australian

Other 
names

Scotty

Occupation Comedian, actor, 
television and radio 
presenter

Known for Studio 10 

Partner(s) John Lane 

Children 2 𝑥𝑚 ℎ𝑚

1

0

1

…

0

Denise Margaret Scott

Born 24 April 1955 Melbourne, Victoria 

Nationality Australian

Occupation Comedian, actor, television and radio presenter

ҧ𝑥1

ҧ𝑥2

ҧ𝑥𝑘

…

ℎ1

ℎ2

…

ℎ𝑘

A
tten
tio
n

𝑠1

𝑠2

𝑠𝑛

…

𝑦1

𝑦2

…

𝑦𝑛

Denise Margaret Scott (born 24 April 1955) is an 
Australian comedian, actor and television presenter.

Key Fact Prediction Surface Realization

Figure 3: The overview of our model. For illustration, the surface realization model is a vanilla Seq2Seq, while it
can also be a Transformer in our implementation.

text as the key facts, so we can label the key facts
automatically. Algorithm 1 illustrates the process
of automatically annotating the key facts. Given
a table and its associated text, we enumerate each
attribute-value pair in the table, and compute the
word overlap between the value and the text. The
word overlap is defined as the number of words
that are not stop words or punctuation but appear
in both the table and the text. We collect all val-
ues that have at least one overlap with the text, and
regard them as the key facts. In this way, we can
obtain a binary sequence with the 0/1 label de-
noting whether the values in the table are the key
facts. The binary sequence will be regarded as the
supervised signal of the key fact prediction model,
and the selected key facts will be the input of the
surface realization model.

2.4 Stage 1: Key Fact Prediction

The key fact prediction model is a Bi-LSTM layer
with a multi-layer perceptron (MLP) classifier to
determine whether each word is selected. In or-
der to represent the table, we follow the previous
work (Liu et al., 2018) to concatenate all the words
in the values of the table into a word sequence,
and each word is labeled with its attribute. In this
way, the table is represented as two sequences: the
value sequence {v1, v2, · · · , vm} and the attribute
sequence {a1, a2, · · · , am}. A word embedding
and an attribute embedding are used to transform

Algorithm 1 Automatic Key Fact Annotation
Input: A parallel corpora P = {(xi,yi)}, where xi is a

table, and yi is a word sequence.
1: Initial the selected key fact list W = []
2: for each sample (x,y) in the parallel dataset P do
3: x = {(v1, a1), (v2, a2), · · · , (vm, am)}
4: y = {y1, y2, · · · , yn}
5: Initial the selected attribute set A = {}
6: Initial the selected key fact list Wi = []
7: for each attribute-value pair (vi, ai) in table x do
8: if vi in y And vi is not stop word then
9: Append attribute ai into attribute set A

10: end if
11: if ai in A then
12: Append value vi into key fact list Wi
13: end if
14: end for
15: Collect the key fact list W += Wi
16: end for
Output: The selected key fact list W

two sequences into the vectors. Following (Le-
bret et al., 2016; Liu et al., 2018), we introduce
a position embedding to capture structured infor-
mation of the table. The position information is
represented as a tuple (p+w , pw), which includes
the positions of the token w counted from the be-
ginning and the end of the value respectively. For
example, the record of “(Name, Denise Margaret
Scott)” is represented as “({Denise, Name, 1, 3},
{Margaret, Name, 2, 2}, {Scott, Name, 3, 1})”.
In this way, each token in the table has an unique
feature embedding even if there exists two same
words. Finally, the word embedding, the attribute



2050

embedding, and the position embedding are con-
catenated as the input of the model x.
Table Encoder: The goal of the source table en-
coder is to provide a series of representations for
the classifier. More specifically, the table encoder
is a Bi-LSTM:

ht = BiLSTM(xt,~ht−1, ~ht+1) (1)

where ~ht and ~ht are the forward and the backward
hidden outputs respectively, ht is the concatena-
tion of ~ht and ~ht, and xt is the input at the t-th
time step.
Classifier: The output vector ht is fed into a MLP
classifier to compute the probability distribution of
the label p1(lt|x)

p1(lt|x) = softmax(Wcht + bc) (2)

where Wc and bc are trainable parameters of the
classifier.

2.5 Stage 2: Surface Realization
The surface realization stage aims to generate the
text conditioned on the key facts predicted in Stage
1. We adpot two models as the implementation of
surface realization: the vanilla Seq2Seq and the
Transformer (Vaswani et al., 2017).

Vanilla Seq2Seq: In our implementation, the
vanilla Seq2Seq consists of a Bi-LSTM encoder
and an LSTM decoder with the attention mecha-
nism. The Bi-LSTM encoder is the same as that of
the key fact prediction model, except that it does
not use any attribute embedding or position em-
bedding.

The decoder consists of an LSTM, an attention
component, and a word generator. It first generates
the hidden state st:

st = f(yt−1, st−1) (3)

where f(·, ·) is the function of LSTM for one time
step, and yt−1 is the last generated word at time
step t − 1. Then, the hidden state st from LSTM
is fed into the attention component:

vt = Attention(st,h) (4)

where Attention(·, ·) is the implementation of
global attention in (Luong et al., 2015), and h is
a sequence of outputs by the encoder.

Given the output vector vt from the attention
component, the word generator is used to compute

the probability distribution of the output words at
time step t:

p2(yt|x) = softmax(Wgvt + bg) (5)

where Wg and bg are parameters of the generator.
The word with the highest probability is emitted
as the t-th word.

Transformer: Similar to vanilla Seq2Seq, the
Transformer consists of an encoder and a decoder.
The encoder applies a Transformer layer to encode
each word into the representation ht:

ht = Transformer(xt,x) (6)

Inside the Transformer, the representation xt at-
tends to a collection of the other representations
x = {x1, x2, · · · , xm}. Then, the decoder pro-
duces the hidden state by attending to both the en-
coder outputs and the previous decoder outputs:

vt = Transformer(yt,y<t,h) (7)

Finally, the output vector is fed into a word gen-
erator with a softmax layer, which is the same as
Eq. 5.

For the purpose of simplicity, we omit the
details of the inner computation of the Trans-
former layer, and refer the readers to the related
work (Vaswani et al., 2017).

2.6 Pseudo Parallel Data Construction

The surface realization model is based on the
encoder-decoder framework, which requires a
large amount of training data. In order to aug-
ment the training data, we propose a novel method
to construct pseudo parallel data. The surface re-
alization model is used to organize and complete
the text given the key facts. Therefore, it is pos-
sible to construct the pseudo parallel data by re-
moving the skeleton of the text and reserving only
the key facts. In implementation, we label the text
with Stanford CoreNLP toolkit2 to assign the POS
tag for each word. We reserve the words whose
POS tags are among the tag set of {NN, NNS, NNP,
NNPS, JJ, JJR, JJS, CD, FW}, and remove the re-
maining words. In this way, we can construct a
large-scale pseudo parallel data to train the surface
realization model.

2https://stanfordnlp.github.io/
CoreNLP/index.html

https://stanfordnlp.github.io/CoreNLP/index.html
https://stanfordnlp.github.io/CoreNLP/index.html


2051

2.7 Denoising Data Augmentation
A problem of the two-stage model is that the er-
ror may propagate from the first stage to the sec-
ond stage. A possible solution is to apply beam
search to enlarge the searching space at the first
stage. However, in our preliminary experiments,
when the beam size is small, the diversity of pre-
dicted key facts is low, and also does not help to
improve the accuracy. When the beam size is big,
the decoding speed is slow but the improvement of
accuracy is limited.

To address this issue, we implement a method
of denoising data augmentation to reduce the hurt
from error propagation and improve the robust-
ness of our model. In practice, we randomly
drop some words from the input of surface realiza-
tion model, or insert some words from other sam-
ples. The dropping simulates the cases when the
key fact prediction model fails to recall some co-
occurrence, while the inserting simulates the cases
when the model predicts some extra facts from the
table. By adding the noise, we can regard these
data as the adversarial examples, which is able to
improve the robustness of the surface realization
model.

2.8 Training and Decoding
Since the two components of our model are sepa-
rate, the objective functions of the models are op-
timized individually.

Training of Key Fact Prediction Model: The
key fact prediction model, as a sequence labeling
model, is trained using the cross entropy loss:

L1 = −
m∑
i=1

p1(li|x) (8)

Training of Surface Realization Model: The
loss function of the surface realization model can
be written as:

L2 = −
n∑

i=1

p2(yi|x̄) (9)

where x̄ is a sequence of the selected key facts
at Stage 1. The surface realization model is also
trained with the pseudo parallel data as described
in Section 2.6. The objective function can be writ-
ten as:

L3 = −
n∑

i=1

p2(ȳi|x̂) (10)

where ȳ is the unlabeled text, and x̂ is the pseudo
text paired with ȳ.

Decoding: The decoding consists of two steps.
At the first step, it predicts the label by the key
fact prediction model:

l̂t = arg max
lt∈{0,1}

p1(lt|x) (11)

The word with l̂t = 1 is reserved, while that with
l̂t = 0 is discarded. Therefore, we can obtain a
sub-sequence x̄ after the discarding operation.

At the second step, the model emits the text with
the surface realization model:

ŷt = arg max
yt∈V

p2(yt|x̄) (12)

where V is the vocabulary size of the model.
Therefore, the word sequence {ŷ1, ŷ2, · · · , ŷN}
forms the generated text.

3 Experiments

We evaluate our model on a table-to-text gener-
ation benchmark. We denote the PIVOT model
under the vanilla Seq2Seq framework as PIVOT-
Vanilla, and that under the Transformer framework
as PIVOT-Trans.

3.1 Dataset

We use WIKIBIO dataset (Lebret et al., 2016)
as our benchmark dataset. The dataset contains
728, 321 articles from English Wikipedia, which
uses the first sentence of each article as the de-
scription of the related infobox. There are an av-
erage of 26.1 words in each description, of which
9.5 words also appear in the table. The table con-
tains 53.1 words and 19.7 attributes on average.
Following the previous work (Lebret et al., 2016;
Liu et al., 2018), we split the dataset into 80%
training set, 10% testing set, and 10% validation
set. In order to simulate the low resource scenario,
we randomly sample 1, 000 parallel sample, and
remove the tables from the rest of the training data.

3.2 Evaluation Metrics

Following the previous work (Lebret et al., 2016;
Wiseman et al., 2018), we use BLEU-4 (Papineni
et al., 2002), ROUGE-4 (F measure) (Lin and
Hovy, 2003), and NIST-4 (Belz and Reiter, 2006)
as the evaluation metrics.



2052

3.3 Implementation Details
The vocabulary is limited to the 20, 000 most com-
mon words in the training dataset. The batch size
is 64 for all models. We implement the early
stopping mechanism with a patience that the per-
formance on the validation set does not fall in 4
epochs. We tune the hyper-parameters based on
the performance on the validation set.

The key fact prediction model is a Bi-LSTM.
The dimensions of the hidden units, the word em-
bedding, the attribute embedding, and the position
embedding are 500, 400, 50, and 5, respectively.

We implement two models as the surface real-
ization models. For the vanilla Seq2Seq model,
we set the hidden dimension, the embedding di-
mension, and the dropout rate (Srivastava et al.,
2014) to be 500, 400, and 0.2, respectively. For the
Transfomer model, the hidden units of the multi-
head component and the feed-forward layer are
512 and 2048. The embedding size is 512, the
number of heads is 8, and the number of Trans-
former blocks is 6.

We use the Adam (Kingma and Ba, 2014)
optimizer to train the models. For the hyper-
parameters of Adam optimizer, we set the learn-
ing rate α = 0.001, two momentum parameters
β1 = 0.9 and β2 = 0.999, and � = 1 × 10−8.
We clip the gradients (Pascanu et al., 2013) to the
maximum norm of 5.0. We half the learning rate
when the performance on the validation set does
not improve in 3 epochs.

3.4 Baselines
We compare our models with two categories of
baseline models: the supervised models which ex-
ploit only parallel data (Vanilla Seq2Seq, Trans-
former, Struct-aware), and the semi-supervised
models which are trained on both parallel data and
unlabelled data (PretrainedMT, SemiMT). The
baselines are as follows:

• Vanilla Seq2Seq (Sutskever et al., 2014)
with the attention mechanism (Bahdanau
et al., 2014) is a popular model for natural
language generation.

• Transformer (Vaswani et al., 2017) is a
state-of-the-art model under the encoder-
decoder framework, based solely on attention
mechanisms.

• Struct-aware (Liu et al., 2018) is the state-
of-the-art model for table-to-text generation.

Model F1 P R

PIVOT (Bi-LSTM) 87.92 92.59 83.70

Model BLEU NIST ROUGE

Vanilla Seq2Seq 2.14 0.2809 0.47
Structure-S2S 3.27 0.9612 0.71

PretrainedMT 4.35 1.9937 0.91
SemiMT 6.76 3.5017 2.04

PIVOT-Vanilla 20.09 6.5130 18.31

Model BLEU NIST ROUGE

Transformer 5.48 1.9873 1.26

PretrainedMT 6.43 2.1019 1.77
SemiMT 9.71 2.7019 3.31

PIVOT-Trans 27.34 6.8763 19.30

Table 1: Results of our model and the baselines. Above
is the performance of the key fact prediction compo-
nent (F1: F1 score, P: precision, R: recall). Middle
is the comparison between models under the Vanilla
Seq2Seq framework. Below is the models imple-
mented with the transformer framework.

It models the inner structure of table with
a field-gating mechanism insides the LSTM,
and learns the interaction between tables and
text with a dual attention mechanism.

• PretrainedMT (Skorokhodov et al., 2018)
is a semi-supervised method to pretrain the
decoder of the sequence-to-sequence model
with a language model.

• SemiMT (Cheng et al., 2016) is a semi-
supervised method to jointly train the
sequence-to-sequence model with an auto-
encoder.

The supervised models are trained with the
same parallel data as our model, while the semi-
supervised models share the same parallel data and
the unlabeled data as ours.

3.5 Results
We compare our PIVOT model with the above
baseline models. Table 1 summarizes the results
of these models. It shows that our PIVOT model
achieves 87.92% F1 score, 92.59% precision, and
83.70% recall at the stage of key fact predic-
tion, which provides a good foundation for the
stage of surface realization. Based on the selected
key facts, our models achieve the scores of 20.09
BLEU, 6.5130 NIST, and 18.31 ROUGE under



2053

0 1000 6000 30000 60000 300000
Parallel Data Size

0

10

20

30

40
BL

EU
Vanilla
Pivot Vanilla

(a) Vanilla Seq2Seq v.s PIVOT-Vanilla

0 1000 6000 30000 60000 300000
Parallel Data Size

0

10

20

30

40

BL
EU

Transformer
Pivot Trans

(b) Transformer v.s PIVOT-Trans

Figure 4: The BLEU measure of our Pivot model and
the baselines trained with different parallel data size.

the vanilla Seq2Seq framework, and 27.34 BLEU,
6.8763 NIST, and 19.30 ROUGE under the Trans-
former framework, which significantly outperform
all the baseline models in terms of all metrics. Fur-
thermore, it shows that the implementation with
the Transformer can obtain higher scores than that
with the vanilla Seq2Seq.

3.6 Varying Parallel Data Size
We would like to further analyze the performance
of our model given different size of parallel size.
Therefore, we randomly shuffle the full parallel
training set. Then, we extract the first K sam-
ples as the parallel data, and modify the remaining
data as the unlabeled data by removing the tables.
We set K = 1000, 6000, 30000, 60000, 300000,
and compare our pivot models with both vanilla
Seq2Seq and Transformer. Figure 4 shows the
BLEU scores of our models and the baselines.
When the parallel data size is small, the pivot
model can outperform the vanilla Seq2Seq and
Transformer by a large margin. With the increase-
ment of the parallel data, the margin gets narrow
because of the upper bound of the model capacity.

1000 6000 30000 60000 300000
Parallel Data Size

80.0

82.5

85.0

87.5

90.0

92.5

95.0

97.5

100.0

F1
-s

co
re

Figure 5: The F1 score of the key fact prediction model
trained with different parallel data size.

Model BLEU NIST ROUGE

Vanilla Seq2Seq 2.14 0.2809 0.47
+ Pseudo 10.01 3.0620 6.55

Transformer 6.43 2.1019 1.77
+ Pseudo 14.35 4.1763 8.42

w/o Pseudo 11.08 3.6910 4.84
PIVOT-Vanilla 20.09 6.5130 18.31

w/o Pseudo 14.18 4.2686 7.10
PIVOT-Trans 27.34 6.8763 19.30

Table 2: Ablation study on the 1k training set for the
effect of pseudo parallel data.

Figure 5 shows the curve of the F1 score of the key
fact prediction model trained with different paral-
lel data size. Even when the number of annotated
data is extremely small, the model can obtain a
satisfying F1 score about 88%. In general, the F1
scores between the low and high parallel data sizes
are close, which validates the assumption that the
key fact prediction model does not rely on a heavy
annotated data.

3.7 Effect of Pseudo Parallel Data

In order to analyze the effect of pseudo parallel
data, we conduct ablation study by adding the data
to the baseline models and removing them from
our models. Table 2 summarizes the results of the
ablation study. Surprisingly, the pseudo parallel
data can not only help the pivot model, but also
significantly improve vanilla Seq2Seq and Trans-
former. The reason is that the pseudo parallel data
can help the models to improve the ability of sur-
face realization, which these models lack under
the condition of limited parallel data. The pivot



2054

Model BLEU NIST ROUGE

PIVOT-Vanilla 20.09 6.5130 18.31
w/o denosing 18.45 4.8714 11.43

PIVOT-Trans 27.34 6.8763 19.30
w/o denosing 25.72 6.5475 17.95

Table 3: Ablation study on the 1k training set for the
effect of the denoising data augmentation.

Transformer: a athletics -lrb- nfl -rrb- .
SemiMT: gustav dovid -lrb- born 25 august
1945 -rrb- is a former hungarian politician ,
who served as a member of the united states
-lrb- senate -rrb- from president to 1989 .
PIVOT-Trans: philippe adnot -lrb- born august
25 , 1945 -rrb- is a french senator , senator , and
a senator of the french senate .
Reference: philippe adnot -lrb- born 25 august
1945 in rhges -rrb- is a member of the senate
of france .

Table 4: An example of the generated text by our model
and the baselines on 1k training set.

models can outperform the baselines with pseudo
data, mainly because it breaks up the operation of
key fact prediction and surface realization, both of
which are explicitly and separately optimized.

3.8 Effect of Denoising Data Augmentation
We also want to know the effect of the denoising
data augmentation. Therefore, we remove the de-
noising data augmentation from our model, and
compare with the full model. Table 3 shows the
results of the ablation study. It shows that the data
augmentation brings a significant improvement to
the pivot models under both vanilla Seq2Seq and
Transformer frameworks, which demonstrates the
efficiency of the denoising data augmentation.

3.9 Qualitative Analysis
We provide an example to illustrate the improve-
ment of our model more intuitively, as shown in
Table 4. Under the low resource setting, the Trans-
former can not produce a fluent sentence, and
also fails to select the proper fact from the ta-
ble. Thanks to the unlabeled data, the SemiMT
model can generate a fluent, human-like descrip-
tion. However, it suffers from the hallucination
problem so that it generates some unseen facts,
which is not faithful to the source input. Although

the PIVOT model has some problem in generating
repeating words (such as “senator” in the exam-
ple), it can select the correct key facts from the
table, and produce a fluent description.

4 Related Work

This work is mostly related to both table-to-text
generation and low resource natural language gen-
eration.

4.1 Table-to-text Generation

Table-to-text generation is widely applied in many
domains. Duboué and McKeown (2002) pro-
posed to generate the biography by matching the
text with a knowledge base. Barzilay and Lapata
(2005) presented an efficient method for automat-
ically learning content selection rules from a cor-
pus and its related database in the sports domain.
Liang et al. (2009) introduced a system with a se-
quence of local decisions for the sportscasting and
the weather forecast. Recently, thanks to the suc-
cess of the neural network models, more work fo-
cused on the neural generative models in an end-
to-end style (Wiseman et al., 2017; Puduppully
et al., 2018; Gehrmann et al., 2018; Sha et al.,
2018; Bao et al., 2018; Qin et al., 2018). Lebret
et al. (2016) constructed a dataset of biographies
from Wikipedia, and built a neural model based on
the conditional neural language models. Liu et al.
(2018) introduced a structure-aware sequence-to-
sequence architecture to model the inner struc-
ture of the tables and the interaction between the
tables and the text. Wiseman et al. (2018) fo-
cused on the interpretable and controllable gener-
ation process, and proposed a neural model using
a hidden semi-markov model decoder to address
these issues. Nie et al. (2018) attempted to im-
prove the fidelity of neural table-to-text generation
by utilizing pre-executed symbolic operations in a
sequence-to-sequence model.

4.2 Low Resource Natural Language
Generation

The topic of low resource learning is one of the re-
cent spotlights in the area of natural language gen-
eration (Tilk and Alumäe, 2017; Tran and Nguyen,
2018). More work focused on the task of neu-
ral machine translation, whose models can gen-
eralize to other tasks in natural language genera-
tion. Gu et al. (2018) proposed a novel universal
machine translation which uses a transfer-learning



2055

approach to share lexical and sentence level repre-
sentations across different languages. Cheng et al.
(2016) proposed a semi-supervised approach that
jointly train the sequence-to-sequence model with
an auto-encoder, which reconstruct the monolin-
gual corpora. More recently, some work ex-
plored the unsupervised methods to totally remove
the need of parallel data (Lample et al., 2018b,a;
Artetxe et al., 2017; Zhang et al., 2018).

5 Conclusions

In this work, we focus on the low resource table-
to-text generation, where only limited parallel data
is available. We separate the generation into two
stages, each of which is performed by a model
trainable with only a few annotated data. Besides,
We propose a method to construct a pseudo paral-
lel dataset for the surface realization model, with-
out the need of any structured table. Experiments
show that our proposed model can achieve 27.34
BLEU score on a biography generation dataset
with only 1, 000 parallel data.

Acknowledgement

We thank the anonymous reviewers for their
thoughtful comments. This work was supported
in part by National Natural Science Foundation of
China (No. 61673028). Xu Sun is the correspond-
ing author of this paper.

References
Mikel Artetxe, Gorka Labaka, Eneko Agirre, and

Kyunghyun Cho. 2017. Unsupervised neural ma-
chine translation. CoRR, abs/1710.11041.

Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua
Bengio. 2014. Neural machine translation by
jointly learning to align and translate. CoRR,
abs/1409.0473.

Jun-Wei Bao, Duyu Tang, Nan Duan, Zhao Yan, Yuan-
hua Lv, Ming Zhou, and Tiejun Zhao. 2018. Table-
to-text: Describing table region with natural lan-
guage. In Proceedings of the Thirty-Second AAAI
Conference on Artificial Intelligence, (AAAI-18),
the 30th innovative Applications of Artificial Intel-
ligence (IAAI-18), and the 8th AAAI Symposium
on Educational Advances in Artificial Intelligence
(EAAI-18), New Orleans, Louisiana, USA, February
2-7, 2018, pages 5020–5027.

Regina Barzilay and Mirella Lapata. 2005. Collective
content selection for concept-to-text generation. In
HLT/EMNLP 2005, Human Language Technology
Conference and Conference on Empirical Methods

in Natural Language Processing, Proceedings of the
Conference, 6-8 October 2005, Vancouver, British
Columbia, Canada, pages 331–338.

Anja Belz and Ehud Reiter. 2006. Comparing auto-
matic and human evaluation of NLG systems. In
EACL 2006, 11st Conference of the European Chap-
ter of the Association for Computational Linguis-
tics, Proceedings of the Conference, April 3-7, 2006,
Trento, Italy.

Yong Cheng, Wei Xu, Zhongjun He, Wei He, Hua
Wu, Maosong Sun, and Yang Liu. 2016. Semi-
supervised learning for neural machine translation.
In Proceedings of the 54th Annual Meeting of the As-
sociation for Computational Linguistics, ACL 2016,
August 7-12, 2016, Berlin, Germany, Volume 1:
Long Papers.

Sumit Chopra, Michael Auli, and Alexander M. Rush.
2016. Abstractive sentence summarization with at-
tentive recurrent neural networks. In NAACL HLT
2016, The 2016 Conference of the North American
Chapter of the Association for Computational Lin-
guistics: Human Language Technologies, pages 93–
98.

Pablo Ariel Duboué and Kathleen R. McKeown. 2002.
Content planner construction via evolutionary al-
gorithms and a corpus-based fitness function. In
Proceedings of the International Natural Language
Generation Conference, Harriman, New York, USA,
July 2002, pages 89–96.

Sebastian Gehrmann, Falcon Z. Dai, Henry Elder, and
Alexander M. Rush. 2018. End-to-end content and
plan selection for data-to-text generation. In Pro-
ceedings of the 11th International Conference on
Natural Language Generation, Tilburg University,
The Netherlands, November 5-8, 2018, pages 46–56.

Jiatao Gu, Hany Hassan, Jacob Devlin, and Victor
O. K. Li. 2018. Universal neural machine translation
for extremely low resource languages. In Proceed-
ings of the 2018 Conference of the North American
Chapter of the Association for Computational Lin-
guistics: Human Language Technologies, NAACL-
HLT 2018, New Orleans, Louisiana, USA, June 1-6,
2018, Volume 1 (Long Papers), pages 344–354.

Diederik P Kingma and Jimmy Ba. 2014. Adam:
A method for stochastic optimization. CoRR,
abs/1412.6980.

Guillaume Lample, Alexis Conneau, Ludovic Denoyer,
and Marc’Aurelio Ranzato. 2018a. Unsupervised
machine translation using monolingual corpora only.
In International Conference on Learning Represen-
tations (ICLR).

Guillaume Lample, Myle Ott, Alexis Conneau, Lu-
dovic Denoyer, and Marc’Aurelio Ranzato. 2018b.
Phrase-based & neural unsupervised machine trans-
lation. In Proceedings of the 2018 Conference on
Empirical Methods in Natural Language Processing
(EMNLP).



2056

Rémi Lebret, David Grangier, and Michael Auli. 2016.
Neural text generation from structured data with ap-
plication to the biography domain. In Proceed-
ings of the 2016 Conference on Empirical Meth-
ods in Natural Language Processing, EMNLP 2016,
Austin, Texas, USA, November 1-4, 2016, pages
1203–1213.

Percy Liang, Michael I. Jordan, and Dan Klein. 2009.
Learning semantic correspondences with less super-
vision. In ACL 2009, Proceedings of the 47th An-
nual Meeting of the Association for Computational
Linguistics and the 4th International Joint Confer-
ence on Natural Language Processing of the AFNLP,
2-7 August 2009, Singapore, pages 91–99.

Chin-Yew Lin and Eduard H. Hovy. 2003. Auto-
matic evaluation of summaries using n-gram co-
occurrence statistics. In Human Language Technol-
ogy Conference of the North American Chapter of
the Association for Computational Linguistics, HLT-
NAACL 2003.

Tianyu Liu, Kexiang Wang, Lei Sha, Baobao Chang,
and Zhifang Sui. 2018. Table-to-text generation
by structure-aware seq2seq learning. In Proceed-
ings of the Thirty-Second AAAI Conference on Ar-
tificial Intelligence, (AAAI-18), the 30th innovative
Applications of Artificial Intelligence (IAAI-18), and
the 8th AAAI Symposium on Educational Advances
in Artificial Intelligence (EAAI-18), New Orleans,
Louisiana, USA, February 2-7, 2018, pages 4881–
4888.

Jiasen Lu, Caiming Xiong, Devi Parikh, and Richard
Socher. 2017. Knowing when to look: Adaptive at-
tention via a visual sentinel for image captioning. In
2017 IEEE Conference on Computer Vision and Pat-
tern Recognition, CVPR 2017, Honolulu, HI, USA,
July 21-26, 2017, pages 3242–3250.

Thang Luong, Hieu Pham, and Christopher D. Man-
ning. 2015. Effective approaches to attention-based
neural machine translation. In Proceedings of the
2015 Conference on Empirical Methods in Natural
Language Processing, EMNLP 2015, pages 1412–
1421.

Feng Nie, Jinpeng Wang, Jin-Ge Yao, Rong Pan, and
Chin-Yew Lin. 2018. Operation-guided neural net-
works for high fidelity data-to-text generation. In
Proceedings of the 2018 Conference on Empirical
Methods in Natural Language Processing, Brussels,
Belgium, October 31 - November 4, 2018, pages
3879–3889.

Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: a method for automatic
evaluation of machine translation. In Proceedings
of the 40th Annual Meeting of the Association for
Computational Linguistics, pages 311–318.

Razvan Pascanu, Tomas Mikolov, and Yoshua Bengio.
2013. On the difficulty of training recurrent neural
networks. In Proceedings of the 30th International

Conference on Machine Learning, ICML 2013, At-
lanta, GA, USA, 16-21 June 2013, pages 1310–1318.

Ratish Puduppully, Li Dong, and Mirella Lapata. 2018.
Data-to-text generation with content selection and
planning. CoRR, abs/1809.00582.

Guanghui Qin, Jin-Ge Yao, Xuening Wang, Jinpeng
Wang, and Chin-Yew Lin. 2018. Learning latent se-
mantic annotations for grounding natural language
to structured data. In Proceedings of the 2018 Con-
ference on Empirical Methods in Natural Language
Processing, Brussels, Belgium, October 31 - Novem-
ber 4, 2018, pages 3761–3771.

Lei Sha, Lili Mou, Tianyu Liu, Pascal Poupart, Su-
jian Li, Baobao Chang, and Zhifang Sui. 2018.
Order-planning neural text generation from struc-
tured data. In Proceedings of the Thirty-Second
AAAI Conference on Artificial Intelligence, (AAAI-
18), the 30th innovative Applications of Artificial In-
telligence (IAAI-18), and the 8th AAAI Symposium
on Educational Advances in Artificial Intelligence
(EAAI-18), New Orleans, Louisiana, USA, February
2-7, 2018, pages 5414–5421.

Ivan Skorokhodov, Anton Rykachevskiy, Dmitry
Emelyanenko, Sergey Slotin, and Anton Ponkratov.
2018. Semi-supervised neural machine translation
with language models. In Proceedings of the Work-
shop on Technologies for MT of Low Resource Lan-
guages, LoResMT@AMTA 2018, Boston, MA, USA,
March 21, 2018, pages 37–44.

Nitish Srivastava, Geoffrey E. Hinton, Alex
Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdi-
nov. 2014. Dropout: a simple way to prevent neural
networks from overfitting. Journal of Machine
Learning Research, 15(1):1929–1958.

Ilya Sutskever, Oriol Vinyals, and Quoc V. Le. 2014.
Sequence to sequence learning with neural net-
works. In Advances in Neural Information Process-
ing Systems 27: Annual Conference on Neural Infor-
mation Processing Systems 2014, pages 3104–3112.

Ottokar Tilk and Tanel Alumäe. 2017. Low-resource
neural headline generation. In Proceedings of
the Workshop on New Frontiers in Summariza-
tion, NFiS@EMNLP 2017, Copenhagen, Denmark,
September 7, 2017, pages 20–26.

Van-Khanh Tran and Le-Minh Nguyen. 2018. Dual
latent variable model for low-resource natural lan-
guage generation in dialogue systems. In Proceed-
ings of the 22nd Conference on Computational Nat-
ural Language Learning, CoNLL 2018, Brussels,
Belgium, October 31 - November 1, 2018, pages 21–
30.

Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob
Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz
Kaiser, and Illia Polosukhin. 2017. Attention is all
you need. In Advances in Neural Information Pro-
cessing Systems 30: Annual Conference on Neural



2057

Information Processing Systems 2017, 4-9 Decem-
ber 2017, Long Beach, CA, USA, pages 6000–6010.

Sam Wiseman, Stuart M. Shieber, and Alexander M.
Rush. 2017. Challenges in data-to-document gener-
ation. In Proceedings of the 2017 Conference on
Empirical Methods in Natural Language Process-
ing, EMNLP 2017, Copenhagen, Denmark, Septem-
ber 9-11, 2017, pages 2253–2263.

Sam Wiseman, Stuart M. Shieber, and Alexander M.
Rush. 2018. Learning neural templates for text gen-
eration. In Proceedings of the 2018 Conference on
Empirical Methods in Natural Language Process-
ing, Brussels, Belgium, October 31 - November 4,
2018, pages 3174–3187.

Pengcheng Yang, Xu Sun, Wei Li, Shuming Ma, Wei
Wu, and Houfeng Wang. 2018. SGM: sequence
generation model for multi-label classification. In
Proceedings of the 27th International Conference
on Computational Linguistics, COLING 2018, Santa
Fe, New Mexico, USA, August 20-26, 2018, pages
3915–3926.

Yi Zhang, Jingjing Xu, Pengcheng Yang, and Xu Sun.
2018. Learning sentiment memories for sentiment
modification without parallel data. In Proceedings
of the 2018 Conference on Empirical Methods in
Natural Language Processing, Brussels, Belgium,
October 31 - November 4, 2018, pages 1103–1108.


