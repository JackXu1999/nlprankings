




































LCFRS binarization and debinarization for directional parsing

Wolfgang Maier
Universität Düsseldorf

Institut für Sprache und Information
Universitätsstr. 1, 40225 Düsseldorf, Germany

maierw@hhu.de

Abstract

In data-driven parsing with Linear Context-Free
Rewriting System (LCFRS), markovized gram-
mars are obtained through the annotation of bi-
narization non-terminals during grammar bina-
rization, as in the corresponding work on PCFG
parsing. Since there is indication that directional
parsing with a non-binary LCFRS can be faster
than parsing with a binary LCFRS, we present
a debinarization procedure with which we can
obtain a non-binary LCFRS from a previously
binarized one. The resulting grammar retains
the markovization information. The algorithm
has been implemented and successfully applied
to the German NeGra treebank.

1 Introduction

Linear Context-Free Rewriting System (LCFRS), an
extension of CFG in which non-terminals can cover
more than a single span, can be used to model dis-
continuous constituents as well as non-projective de-
pendencies (Maier and Lichte, 2011). It has therefore
recently been exploited for direct data-driven pars-
ing of such structures. See, e.g., Maier and Søgaard
(2008), Maier and Kallmeyer (2010), van Cranen-
burgh (2011), van Cranenburgh et al. (2012), and
Kallmeyer and Maier (2013).

A major problem with data-driven probabilis-
tic LCFRS (PLCFRS) parsing is the high parsing
complexity. Given a binary grammar, CYK pars-
ing can be done in O(n3k) where k is the fan-
out of the grammar, that is, the maximum num-
ber of spans that a single non-terminal can cover
(Seki et al., 1991). While for PCFG parsing, k is
1, for PLCFRS, k will typically be ≈ 5. Sen-
tences with lengths around 23 to 25 words require

very high, unpractical parsing times (20 minutes and
more per sentence) (van Cranenburgh et al., 2011;
Kallmeyer and Maier, 2013).

One possibility to obtain faster parsing is to reduce
the fan-out of the grammar by reducing the number
of gaps in the trees from which the grammar is ex-
tracted. This has been done by Maier et al. (2012)
who transform the trees of the German TIGER tree-
bank such that a grammar fan-out of 2 is guar-
anteed. For unrestricted PLCFRS parsing, other
solutions have been implemented. The parser of
Kallmeyer and Maier (2013)1 offers A∗ parsing with
outside estimates (Klein and Manning, 2003a). With
this technique the practical sentence length limit is
shifted upwards by 7 to 10 words. Kallmeyer and
Maier also propose non-admissible estimates which
provide a greater speed-up but also let the results de-
grade. The parser of van Cranenburgh (2012)2 also
does not maintain exact search. It implements a
coarse-to-fine strategy in a more general PCFG is cre-
ated from the treebank PLCFRS using the algorithm
of Barthélemy et al. (2001). The PCFG chart is then
used to filter the PLCFRS chart. While this approach
in principle removes the limit3 on sentence length, it
also leads to degraded results.

Yet another solution for obtaining higher speeds
could be to turn to parsing strategies which allow for
the use of non-binary rules, such as directional CYK
parsing or Earley parsing. The reasoning behind this
assumption is as follows. Firstly, the longer the right-
hand side of a rule, the easier it is to check during pars-
ing on a symbolic basis if it is possible to use the rule

1See http://phil.hhu.de/rparse.
2See http://github.com/andreasvc/disco-dop.
3For very long sentences, i.e. > 60 words, the parser still has

extreme memory requirements.

113



in a complete parse or not by checking the require-
ments of the rule with respect to the yet unparsed part
of the input such as it is done, e.g., in Kallmeyer and
Maier (2009).4 A comparable strategy, the F gram-
mar projection estimate (Klein and Manning, 2003a),
has been employed in PCFG parsing. Secondly, prac-
tical experience with Range Concatenation Gram-
mar (RCG) (Boullier, 1998) and Grammatical Frame-
work (GF) (Ranta, 2004) parsing points in the
same direction. Lazy computation of instantiations
in RCG parsing as done in the TuLiPA system
(Kallmeyer et al., 2009) and the SYNTAXE parser
(Boullier and Deschamp, 1988) (Boullier, p.c.) seem
to be less effective with shorter right-hand sides of
rules because less constraints can be collected at
once. Practical experiments with the GF5 parser
(Angelov, 2009), which implements an Earley strat-
egy, indicate that certain optimizations loose their ef-
fect with binary grammars (Angelov, p.c.).

So why not just do directional parsing with the un-
binarized treebank grammar? It is common knowl-
edge that vanilla treebank PCFGs do not perform well.
This also holds for PLCFRS. Markovization has been
proven to be an effective remedy for both PCFG and
PLCFRS parsing. It can be achieved through the prob-
ability model itself (Collins, 1999) or by annotating
the treebank grammar (Klein and Manning, 2003b;
Kallmeyer and Maier, 2013): Instead of using a
unique non-terminal as in deterministic binarization,
one uses a single non-terminal and adorns it with the
vertical (“parent annotation”, see Johnson (1998)) and
horizontal context of the occurrence of the rule in the
treebank. This augments the coverage of the grammar
and helps to achieve better parsing results by adding a
possibly infinite number of implicit non-binary rules.

Our main contribution in this article is a debinariza-
tion algorithm with which a non-binary LCFRS can be
generated from a previously binarized LCFRS (which
fulfills certain conditions). Given a markovized bi-
narized grammar, the debinarized grammar contains
non-binary productions obtained through markoviza-
tion. We furthermore contribute a new compact nota-
tion for rules of a treebank LCFRS, i.e., of the variant
of LCFRS obtained by treebank grammar extraction,

4For such a check, it makes no difference if the rule is deter-
ministically binarized.

5http://grammaticalframework.org

and provide a formulation of deterministic binariza-
tion using this notation.

An implementation of the debinarization algorithm
has been tested on the German NeGra treebank
(Skut et al., 1997). First experimental results confirm
that in practice, the debinarized grammar can perform
better than than the plain treebank grammar.

The remainder of the article is structured as follows.
In the following section, we define treebank LCFRS
and introduce our new rule representation. We fur-
thermore introduce the binarization, resp. markoviza-
tion algorithm. In section 3, we introduce our new
debinarization algorithm. Section 4 presents the ex-
perimental evaluation and section 5 closes the article.

2 LCFRS

2.1 Definition

In LCFRS (Vijay-Shanker et al., 1987), a single non-
terminal can span k ≥ 1 continuous blocks of a string.
A CFG is simply a special case of an LCFRS in which
k = 1. k is called the fan-out of the non-terminal. We
notate LCFRS with a syntax of Simple Range Con-
catenation Grammars (SRCG) (Boullier, 1998), a for-
malism equivalent to LCFRS. In the following we de-
fine treebank LCFRS, the variant of LCFRS which
is obtained by the grammar extraction algorithm of
Maier and Søgaard (2008).

A treebank LCFRS is a tuple G = (N, T, V, P, S)
where

1. N is a finite set of non-terminals with a function
dim: N → N determining the fan-out of each
A ∈ N ;

2. T and V are disjoint finite sets of terminals and
variables;

3. S ∈ N is the start symbol with dim(S) = 1;

4. P is a finite set of rewriting rules where all r ∈ P
are either

(a) rules with rank m ≥ 1 of the form
A(α1, . . . , αdim(A)) → A1(X(1)1 , . . . , X(1)dim(A1))

· · · Am(X(m)1 , . . . , X(m)dim(Am))

where

i. A, A1, . . . , Am ∈ N , X(i)j ∈ V for 1 ≤
i ≤ m, 1 ≤ j ≤ dim(Ai) and

114



A(a) → ε (〈a〉 in yield of A)
B(b) → ε (〈b〉 in yield of B)
C(X, Y ) → A(X)A(Y ) (if 〈X〉 in the yield of A

and 〈Y 〉 in the yield of
A, then 〈X, Y 〉 in yield
of C)

S(XZY ) → C(X, Y )B(Z) (if 〈X, Y 〉 in yield of C
and 〈Z〉 in the yield of
B, then 〈XZY 〉 in yield
of S)

L = {anban | n > 0}

Figure 1: Yield example

ii. αi ∈ V + for 1 ≤ i ≤ dim(A) (we write
αi.j, 1 ≤ j ≤ |αi| for the jth variable
in αi), or

(b) rules of rank 0 of the form A(t) → ε where
A ∈ N , t ∈ T .

For all r ∈ P , every variable X that occurs in r
occurs exactly once in the left-hand side (LHS) and
exactly once in the right-hand side (RHS). Further-
more, if for two variables X1, X2 ∈ V , it holds that
X1 ≺ X2 on the RHS, then also X1 ≺ X2 on the
LHS.6

A rewriting rule describes how to compute the yield
of the LHS non-terminal from the yields of the RHS
non-terminals. The yield of S is the language of the
grammar. See figure 1 for an example.

The rank of G is the maximal rank of any of its
rules, its fan-out is the maximal fan-out of any of its
non-terminals.

The properties which distinguish a treebank
LCFRS from a regular LCFRS are the requirements
that

1. all non-terminal arguments are variables except
in terminating lexical rules in which the only ar-
gument of the LHS consists of a single terminal,
and

2. the ordering property.

These properties allows us to notate rules in a more
compact way. Let (N, T, V, P, S) be an LCFRS. A
rule r ∈ P

A(α1, . . . , αdim(A)) → A1(X(1)1 , . . . , X(1)dim(A1))
· · · Am(X(m)1 , . . . , X(m)dim(Am))

6This is the ordering property which Villemonte de la Clerg-
erie (2002) defines for RCGs. LCFRSs with this property
are called monotone (Michaelis, 2001), MCFGs non-permuting
(Kracht, 2003).

can be represented as a tuple (A → A1 · · · Am, ~ϕ),
where ~ϕ is a linearization vector which is defined as
follows. For all 1 ≤ i ≤ dim(A), 1 ≤ j ≤ |αi|,
~ϕ[i] is a subvector of ~ϕ with |~ϕ[i]| = |αi| and it
holds that ~ϕ[i][j] = x iff αi.j occurs in the argu-
ments of Ax. Let us consider an example: The rule
A(X1X2, X3X4) → B(X1, X3)C(X2, X4) can be
fully specified by (A → BC, [[1, 2], [1, 2]]).

We introduce additional notation for operations on
a linearization vector ~ϕ.

1. We write ~ϕ/(x, x′) for some x, x′ ∈ N for the
substitution of all occurrences of x in ~ϕ by x′.
~ϕ/s(x, x′) denotes the same substitution proce-
dure with the addition that after the substitution,
all occurrences of x′+ in ~ϕ are replaced by x′.

2. We write ~ϕ \ x for some x ∈ N for the dele-
tion of all occurrences of x in ~ϕ, followed by the
deletion of subvectors which become empty.

3. ~ϕ ↓x for some x ∈ N denotes the splitting of
all subvectors of ~ϕ such that a vector boundary
is introduced between all ~ϕ[i][j] and ~ϕ[i][j + 1],
1 ≤ i ≤ |~ϕ|, 1 ≤ j ≤ |~ϕ[i]| − 1 iff ~ϕ[i][j] = x.

Note that for every LCFRS, there is an equivalent
LCFRS which fulfills the treebank LCFRS conditions
(Kallmeyer, 2010).

A probabilistic (treebank) LCFRS (PLCFRS) is a
tuple 〈N, T, V, P, S, p〉 such that 〈N, T, V, P, S〉 is a
(treebank) LCFRS and p : P → [0..1] a function such
that for all A ∈ N : ΣA(~x)→~Φ∈P p(A(~x) → ~Φ) =
1. The necessary counts for a Maximum Likelihood
estimation can easily be obtained from the treebank
(Kallmeyer and Maier, 2013).

2.2 Binarization and Markovization

Using the linearization vector notation, deterministic
left-to-right binarization of treebank LCFRS can be
accomplished as in algorithm 1. The algorithm cre-
ates binary top and bottom rules but can easily be ex-
tended to create unary rules in these places. Note that
the algorithm behaves identically to other determin-
istic binarization algorithms such as, e.g., the one in
Kallmeyer (2010).

The algorithm works as follows. We start with the
linearization vector ~ϕ of the original rule. In each
binarization step, we “check off” from ~ϕ the infor-
mation which was used in the previous step. The

115



Algorithm 1 LCFRS binarization
Let (N, T, V, P, S) be a treebank LCFRS
P ′ = ∅
for all (A → A1 . . . Am, ~ϕr) in P with m > 2 do

pick new non-terminals C1, . . . , Cm−2
let ~ϕ = ~ϕr
let ~ϕg = ~ϕr/s(x, 2) for all x > 1
add the rule A → A1C1, ~ϕg to P ′
for all i, 1 ≤ i < m − 2 do

let ~ϕ = ~ϕ/(x, x − 1) for all x
let ~ϕ = (~ϕ ↓0) \ 0
let ~ϕg = ~ϕ/s(x, 2) for all x > 1
add the rule (Ci → Ai+1Ci+1, ~ϕg) to P ′

let ~ϕ = ~ϕ/(x, x − 1) for all x
let ~ϕ = (~ϕ ↓0) \ 0
add the rule (Cm−2 → Am−1Am, ~ϕ) to P ′

set P to P ′

linearization vectors of the binary rules are obtained
from the respective current state of ~ϕ by attributing
all information which will covered by the new bina-
rization non-terminal to exactly this non-terminal.

As an example, consider the binariza-
tion of the rule A(X1X2, X3X4, X5X6) →
B(X1, X3)C(X2, X4)D(X5)E(X6). The com-
pact representation of the rule is (A → BCDE,
[[1, 2][1, 2][3, 4]]). The first step yields the rule
(A → BC1, [[1, 2], [1, 2], [2]]). ~ϕ is set to the
linearization vector of the original rule. Subse-
quently, we remove from ~ϕ the information which
has already been used by subtracting one from all
integers in ~ϕ, splitting the vector at occurrences of
0 (i.e., new argument boundaries are introduced at
positions which have been covered by the previous
binarization step), and then removing all occurrences
of 0. From the resulting vector [[1], [1], [2, 3]], we
create the linearization vector for the second binary
rule (C1 → CC2, [[1], [1], [2]]) by attributing all
material covered by C2 to C2. In other words, the
last subvector can be reduced from [2, 3] to [2] since
only in the next and last binarization step we will
distinguish between 2 and 3. In the subsequent last
step, we again check off the already used material
from ~ϕ and end up with (C2 → DE, [[1, 2]]).

During binarization, a grammar can be markovized
by changing the choice of binarization non-terminals.
Instead of unique non-terminals C1, . . . , Cm−2, we

pick a single non-terminal @. At each binarization
step, to this non-terminal, we append the vertical oc-
currence context from the treebank, i.e., we perform
parent annotation (Johnson, 1998), and we append the
horizontal context. More precisely, the markoviza-
tion information for the binarization non-terminal that
comprises original RHS elements Ai . . . Am are the
first v elements of path from Ai to root vertically and
the first h elements of Ai . . . A0 horizontally.

3 Debinarization

The goal of the debinarization procedure is to obtain
a grammar with non-binary rules from a previously
binarized one. Algorithm 2 accomplishes the debina-
rization. It consists of a function debin, which, assum-
ing a treebank LCFRS (N, T, V, P, S) binarized with
algorithm 1, is called on all A ∈ N . During debi-
narization, linearization vectors must be recombined.
This is notated as function lin.

Called on some non-terminal A ∈ N , the algorithm
recursively substitutes all binarization non-terminals
on the right-hand sides of all binary A rules with the
right-hand sides of rules which have the same bina-
rization non-terminal on its LHS. The base case of
this recursion are rules which do not have binarization
non-terminals on their right-hand side. At each sub-
stitution, we recombine the corresponding lineariza-
tion vectors: When substituting the binarization non-
terminal C on the right-hand side of a binary produc-
tion by the RHS of a debinarized rule r with C on its
LHS, roughly, we replace the ith occurrence of 2 in
the linearization vector by the ith subvector of the lin-
earization vector of r, adding 1 to all of its elements.7

The probability of a rule in which a binarization
non-terminal on the RHS is substituted by the RHS of
a rule with this binarization non-terminal on its LHS
is simply the product of both probabilities.

As an example, consider the debinarization of the
non-terminal A given the binarized rules from our pre-
vious example. We have a single A rule, in which
we would recursively substitute the C1 on the right-
hand side with C C2 and in the following as base case
C2 with D E. In the base case, lin is not called, be-

7The algorithm would be slightly more complex if, during bi-
narization, we would permit that binarization non-terminals end
up on the left corner of the right-hand side of a binary produc-
tion. Algorithm 1 guarantees that binarization non-terminals oc-
cur only on the right corner.

116



cause no linearization vector recombination is neces-
sary. For the substitution of C2, we combine the lin-
earization vectors of the C1 and C2 rules. For this,
we first add 1 to all elements of the C2 vector, which
gets us [[2, 3]]. We then replace the only occurrence
of 2 in the C1 vector by [2, 3]. The result is the vec-
tor [[1], [1], [2, 3]]. For the substitution of C1 in the
A rule, we recombine our result vector with the vec-
tor of the A rule. For this, we first add 1 to all ele-
ments of the result of the vector recombination, which
gets us [[2], [2], [3, 4]]. We then replace all three oc-
currences of 2 in the vector [[1, 2], [1, 2], [2]] of the
binary A rule with the corresponding subvectors of
[[2], [2], [3, 4]]. This give us [[1, 2], [1, 2], [3, 4]], which
is the linearization vector of the original unbinarized
A rule.

In the case of deterministic binarization, the algo-
rithm yields a grammar which is equivalent to the
grammar before the binarization. With markovization,
i.e., with non-deterministic binarization, it is more dif-
ficult. Since we do not choose our binarization non-
terminals in a unique way, it is possible that a chain
of substitutions can lead us in a cycle, i.e., that we
reach the same binarization non-terminal again. A cy-
cle corresponds to an infinite number of implicit non-
binary rules. The smaller the binarization context, the
more likely it is that such a cycle occurs.

Several strategies are possible to avoid an infi-
nite loop of substitutions. The most obvious one is
to use a cache which contains all binarization non-
terminals seen during debinarization. If a substitu-
tion introduces a binarization non-terminal which is in
the cache, the corresponding production is discarded.
Another strategy is to discard rules with a probability
lower than a certain threshold.

4 Experiments

We have implemented the grammar extraction algo-
rithm from Maier and Søgaard (2008), as well as the
binarization and the debinarization algorithm. We im-
plement the debinarization algorithm with a probabil-
ity filter: A substitution of a binarization non-terminal
will not take place if the probability of the resulting
production falls under a certain threshold. We fur-
thermore implement a filter which excludes rules with
right-hand sides that exceed a certain length.

In order to experimentally test the debinarization al-

Algorithm 2 LCFRS debinarization
function debin(A ∈ N)
let R = ∅
for all (A → A1A2, ~ϕ) in P do

if A1, A2 are not bin. non-terminals then
add (A → A1A2, ~ϕ) to R

else
let D = debin(A2)
for all (D → D0 · · · Dm, ~ϕD) ∈ D do

add (A → A1D0 · · · Dm, lin(~ϕ, ~ϕD, 2)) to
R

return R

function lin(~ϕ, ~ϕD, s)
let c = 0
let ~ϕD = ~ϕD/(x, x + 1) for all x
for all occurrences of s in ~ϕ do

replace occ. of s with the content of ~ϕD[c]
c = c + 1

return ~ϕ

gorithm, we perform experiments on the NeGra tree-
bank (Skut et al., 1997) using rparse.8 In a first step,
we apply the algorithm for re-attaching elements to
the virtual root node described in (Maier et al., 2012).
All sentences longer than 20 words are excluded. For
parsing, we split the data and use the first 90% of all
sentences for training and the remainder for parsing.
We then extract the grammar from the training part
(results in 10,482 rules) and binarize them, using de-
terministic binarization and using markovization with
vertical and horizontal histories of 1, resp. 2. The
markovized grammar is debinarized, with an exper-
imentally determined logarithmic rule weight of 15
for the filtering and a limit of 15 on the lengths of
rule right-hand sides. This results in a grammar with
175,751 rules.

We then parse with the deterministically binarized
treebank grammar, with the markovized binary gram-
mar, and with debinarized grammar. Note that rparse
currently offers no directional parser. Therefore we
rebinarize the debinarized grammar using determinis-
tic binarization. Using bracket scoring (evalb)9, we
obtain the 75.49 F1 for the plain treebank grammar,

8See http://phil.hhu.de/rparse.
9See http://github.com/wmaier/evalb-lcfrs.

117



77.10 for the markovized grammar and 76.37 for the
debinarized grammar. This shows that the debina-
rized markovized grammar can perform better than
the plain treebank grammar. However, the large num-
ber of productions in the debinarized grammar indi-
cates that the possibilities of filtering must be investi-
gated more closely. This is left for future work.

5 Conclusion

We have presented a new compact representation for
grammar rules of a treebank LCFRS together with
a formulation of a binarization algorithm. Further-
more, we have presented a procedure for debinarizing
a previously binarized LCFRS. The resulting gram-
mar maintains the markovization information intro-
duced during binarization and can be used with di-
rectional parsing strategies. Experiments have shown
that the grammar can perform better than the plain
treebank grammar. There remains potential for op-
timization.

We are currently working on the integration of the
algorithm into a directional parsing strategy within a
data-driven PLCFRS parser.

Acknowledgments

I would like Laura Kallmeyer, Miriam Kaeshammer
and the three reviewers for comments and sugges-
tions.

References

Krasimir Angelov. 2009. Incremental parsing with Paral-
lel Multiple Context-Free Grammars. In Proceedings of
the 12th Conference of the European Chapter of the As-
sociation for Computational Linguistics, pages 69–76,
Athens, Greece.

François Barthélemy, Pierre Boullier, Philippe Deschamp,
and Éric Villemonte de la Clergerie. 2001. Guided pars-
ing of Range Concatenation Languages. In Proceedings
of the 39th Annual Meeting of the Association for Com-
putational Linguistics, pages 42–49, Toulouse, France.

Pierre Boullier and Philippe Deschamp, 1988.
Le système SYNTAXTM – manuel d’utilisation
et de mise en oeuvre sous UNIXTM .
http://syntax.gforge.inria.fr/
syntax3.8-manual.pdf, January 4, 2012.

Pierre Boullier. 1998. A generalization of mildly context-
sensitive formalisms. In Proceedings of the Fourth In-

ternational Workshop on Tree Adjoining Grammars and
Related Formalisms (TAG+4), pages 17–20, Philadel-
phia, PA.

Michael Collins. 1999. Head-driven statistical models for
natural language parsing. Ph.D. thesis, University of
Pennsylvania.

Mark Johnson. 1998. PCFG models of linguistic tree
representations. Computational Linguistics, 24(4):613–
632.

Laura Kallmeyer and Wolfgang Maier. 2009. An incre-
mental Earley parser for Simple Range Concatenation
Grammar. In Proceedings of the 11th International Con-
ference on Parsing Technologies (IWPT’09), pages 61–
64, Paris, France. Association for Computational Lin-
guistics.

Laura Kallmeyer and Wolfgang Maier. 2013. Data-driven
parsing using probabilistic linear context-free rewriting
systems. Computational Linguistics, 39(1):87–119.

Laura Kallmeyer, Wolfgang Maier, and Yannick Parmen-
tier. 2009. An Earley parsing algorithm for Range
Concatenation Grammars. In Proceedings of the ACL-
IJCNLP 2009 Conference Short Papers, pages 9–12,
Singapore.

Laura Kallmeyer. 2010. Parsing Beyond Context-Free
Grammars. Springer.

Dan Klein and Christopher D. Manning. 2003a. A* Pars-
ing: Fast exact viterbi parse selection. In Proceed-
ings of the 2003 Human Language Technology Confer-
ence of the North American Chapter of the Association
for Computational Linguistics, pages 40–47, Edmonton,
Canada.

Dan Klein and Christopher D. Manning. 2003b. Accu-
rate unlexicalized parsing. In Proceedings of the 41th
Annual Meeting of the Association for Computational
Linguistics, pages 423–430, Sapporo, Japan. Associa-
tion for Computational Linguistics.

Marcus Kracht. 2003. The Mathematics of Language.
Mouton de Gruyter, Berlin.

Wolfgang Maier and Laura Kallmeyer. 2010. Discontinu-
ity and non-projectivity: Using mildly context-sensitive
formalisms for data-driven parsing. In Proceedings of
the Tenth International Conference on Tree Adjoining
Grammars and Related Formalisms (TAG+10), pages
119–126, Yale University, New Haven, CT.

Wolfgang Maier and Timm Lichte. 2011. Character-
izing discontinuity in constituent treebanks. In For-
mal Grammar. 14th International Conference, FG 2009.
Bordeaux, France, July 25-26, 2009. Revised Selected

118



Papers, volume 5591 of Lecture Notes in Artificial Intel-
ligence, pages 167–182, Berlin/Heidelberg/New York.
Springer-Verlag.

Wolfgang Maier and Anders Søgaard. 2008. Treebanks
and mild context-sensitivity. In Philippe de Groote,
editor, Proceedings of the 13th Conference on For-
mal Grammar (FG-2008), pages 61–76, Hamburg, Ger-
many. CSLI Publications.

Wolfgang Maier, Miriam Kaeshammer, and Laura
Kallmeyer. 2012. Data-driven PLCFRS parsing revis-
ited: Restricting the fan-out to two. In Proceedings
of the Eleventh International Conference on Tree Ad-
joining Grammars and Related Formalisms (TAG+11),
Paris, France. to appear.

Jens Michaelis. 2001. On Formal Properties of Minimalist
Grammars. Ph.D. thesis, Potsdam University, Potsdam,
Germany.

Aarne Ranta. 2004. Grammatical Framework, a typetheo-
retical grammar formalism. Journal of Functional Pro-
gramming, 14(2):145–189.

Hiroyuki Seki, Takahashi Matsumura, Mamoru Fujii, and
Tadao Kasami. 1991. On multiple context-free gram-
mars. Theoretical Computer Science, 88(2):191–229.

Wojciech Skut, Brigitte Krenn, Thorsten Brants, and Hans
Uszkoreit. 1997. An annotation scheme for free word
order languages. In Proceedings of the 5th Applied
Natural Language Processing Conference, pages 88–95,
Washington, DC.

Andreas van Cranenburgh, Remko Scha, and Federico San-
gati. 2011. Discontinuous data-oriented parsing: A
mildly context-sensitive all-fragments grammar. In Pro-
ceedings of the Second Workshop on Statistical Pars-
ing of Morphologically Rich Languages (SPMRL 2011),
pages 34–44, Dublin, Ireland.

Andreas van Cranenburgh. 2012. Efficient parsing with
linear context-free rewriting systems. In Proceedings
of the 13th Conference of the European Chapter of the
Association for Computational Linguistics, pages 460–
470, Avignon, France.

K. Vijay-Shanker, David Weir, and Aravind K. Joshi. 1987.
Characterising structural descriptions used by various
formalisms. In Proceedings of the 25th Annual Meeting
of the Association for Computational Linguistics, pages
104–111, Stanford, CA. Association for Computational
Linguistics.

Éric Villemonte de la Clergerie. 2002. Parsing Mildly
Context-Sensitive Languages with Thread Automata. In
Proceedings of COLING 2002: The 19th International
Conference on Computational Linguistics, Taipei, Tai-
wan.

119


