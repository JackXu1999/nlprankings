



















































Low-Resource Name Tagging Learned with Weakly Labeled Data


Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing
and the 9th International Joint Conference on Natural Language Processing, pages 261–270,
Hong Kong, China, November 3–7, 2019. c©2019 Association for Computational Linguistics

261

Low-Resource Name Tagging Learned with Weakly Labeled Data

Yixin Cao1 Zikun Hu1 Tat-Seng Chua1
Zhiyuan Liu2 Heng Ji3

1School of Computing, National University of Singapore, Singapore
2Department of CST, Tsinghua University, Beijing, China

3Department of CS, University of Illinois Urbana-Champaign, U.S.A.
caoyixin2011@gmail.com,zikunhu@u.nus.edu,dcscts@nus.edu.sg

liuzy@tsinghua.edu.cn,hengji@illinois.edu

Abstract

Name tagging in low-resource languages or
domains suffers from inadequate training data.
Existing work heavily relies on additional in-
formation, while leaving those noisy annota-
tions unexplored that extensively exist on the
web. In this paper, we propose a novel neu-
ral model for name tagging solely based on
weakly labeled (WL) data, so that it can be
applied in any low-resource settings. To take
the best advantage of all WL sentences, we
split them into high-quality and noisy portions
for two modules, respectively: (1) a classifi-
cation module focusing on the large portion
of noisy data can efficiently and robustly pre-
train the tag classifier by capturing textual con-
text semantics; and (2) a costly sequence la-
beling module focusing on high-quality data
utilizes Partial-CRFs with non-entity sampling
to achieve global optimum. Two modules are
combined via shared parameters. Extensive
experiments involving five low-resource lan-
guages and fine-grained food domain demon-
strate our superior performance (6% and 7.8%
F1 gains on average) as well as efficiency1.

1 Introduction

Name tagging2 is the task of identifying the
boundaries of entity mentions in texts and classi-
fying them into the pre-defined entity types (e.g.,
person). It serves as a fundamental role as pro-
viding the essential inputs for many IE tasks, such
as Entity Linking (Cao et al., 2018a) and Relation
Extraction (Lin et al., 2017).

Many recent methods utilize a neural net-
work (NN) with Conditional Random Fields
(CRFs) (Lafferty et al., 2001) by treating name
tagging as a sequence labeling problem (Lample

1Our project can be found in https://github.com/
zig-kwin-hu/Low-Resource-Name-Tagging.

2Someone may call it Named Entity Recognition (NER).

… Barangay Ginebra and Formula Shell forming a rivalry… Barangay Ginebra and Formula Shell forming a rivalryO’NealShaquille… Barangay Ginebra and Formula Shell forming a rivalry

B-ORG O OO O B-LOCI-ORG

Formula shell won game one in Philippines …

Weakly Labelled, extensively exists on the web

Fully Labelled, expensive to obtain

s1
<latexit sha1_base64="zD0+iHg15Vq/defZn4JzGP6vt+A=">AAACxnicjVHLSsNAFD2Nr1pfVZdugkVwVRIRdFl002VF+4BaSjKd1tC8yEyUUgR/wK1+mvgH+hfeGaegFtEJSc6ce8+Zuff6aRgI6TivBWthcWl5pbhaWlvf2Nwqb++0RJJnjDdZEiZZx/cED4OYN2UgQ95JM+5Ffsjb/vhcxdu3PBNBEl/JScp7kTeKg2HAPEnUpei7/XLFqTp62fPANaACsxpJ+QXXGCABQ44IHDEk4RAeBD1duHCQEtfDlLiMUKDjHPcokTanLE4ZHrFj+o5o1zVsTHvlKbSa0SkhvRkpbRyQJqG8jLA6zdbxXDsr9jfvqfZUd5vQ3zdeEbESN8T+pZtl/lenapEY4lTXEFBNqWZUdcy45Lor6ub2l6okOaTEKTygeEaYaeWsz7bWCF276q2n4286U7Fqz0xujnd1Sxqw+3Oc86B1VHWdqntxXKmdmVEXsYd9HNI8T1BDHQ00yXuERzzh2apbsZVbd5+pVsFodvFtWQ8fAiGQHA==</latexit><latexit sha1_base64="zD0+iHg15Vq/defZn4JzGP6vt+A=">AAACxnicjVHLSsNAFD2Nr1pfVZdugkVwVRIRdFl002VF+4BaSjKd1tC8yEyUUgR/wK1+mvgH+hfeGaegFtEJSc6ce8+Zuff6aRgI6TivBWthcWl5pbhaWlvf2Nwqb++0RJJnjDdZEiZZx/cED4OYN2UgQ95JM+5Ffsjb/vhcxdu3PBNBEl/JScp7kTeKg2HAPEnUpei7/XLFqTp62fPANaACsxpJ+QXXGCABQ44IHDEk4RAeBD1duHCQEtfDlLiMUKDjHPcokTanLE4ZHrFj+o5o1zVsTHvlKbSa0SkhvRkpbRyQJqG8jLA6zdbxXDsr9jfvqfZUd5vQ3zdeEbESN8T+pZtl/lenapEY4lTXEFBNqWZUdcy45Lor6ub2l6okOaTEKTygeEaYaeWsz7bWCF276q2n4286U7Fqz0xujnd1Sxqw+3Oc86B1VHWdqntxXKmdmVEXsYd9HNI8T1BDHQ00yXuERzzh2apbsZVbd5+pVsFodvFtWQ8fAiGQHA==</latexit><latexit sha1_base64="zD0+iHg15Vq/defZn4JzGP6vt+A=">AAACxnicjVHLSsNAFD2Nr1pfVZdugkVwVRIRdFl002VF+4BaSjKd1tC8yEyUUgR/wK1+mvgH+hfeGaegFtEJSc6ce8+Zuff6aRgI6TivBWthcWl5pbhaWlvf2Nwqb++0RJJnjDdZEiZZx/cED4OYN2UgQ95JM+5Ffsjb/vhcxdu3PBNBEl/JScp7kTeKg2HAPEnUpei7/XLFqTp62fPANaACsxpJ+QXXGCABQ44IHDEk4RAeBD1duHCQEtfDlLiMUKDjHPcokTanLE4ZHrFj+o5o1zVsTHvlKbSa0SkhvRkpbRyQJqG8jLA6zdbxXDsr9jfvqfZUd5vQ3zdeEbESN8T+pZtl/lenapEY4lTXEFBNqWZUdcy45Lor6ub2l6okOaTEKTygeEaYaeWsz7bWCF276q2n4286U7Fqz0xujnd1Sxqw+3Oc86B1VHWdqntxXKmdmVEXsYd9HNI8T1BDHQ00yXuERzzh2apbsZVbd5+pVsFodvFtWQ8fAiGQHA==</latexit><latexit sha1_base64="zD0+iHg15Vq/defZn4JzGP6vt+A=">AAACxnicjVHLSsNAFD2Nr1pfVZdugkVwVRIRdFl002VF+4BaSjKd1tC8yEyUUgR/wK1+mvgH+hfeGaegFtEJSc6ce8+Zuff6aRgI6TivBWthcWl5pbhaWlvf2Nwqb++0RJJnjDdZEiZZx/cED4OYN2UgQ95JM+5Ffsjb/vhcxdu3PBNBEl/JScp7kTeKg2HAPEnUpei7/XLFqTp62fPANaACsxpJ+QXXGCABQ44IHDEk4RAeBD1duHCQEtfDlLiMUKDjHPcokTanLE4ZHrFj+o5o1zVsTHvlKbSa0SkhvRkpbRyQJqG8jLA6zdbxXDsr9jfvqfZUd5vQ3zdeEbESN8T+pZtl/lenapEY4lTXEFBNqWZUdcy45Lor6ub2l6okOaTEKTygeEaYaeWsz7bWCF276q2n4286U7Fqz0xujnd1Sxqw+3Oc86B1VHWdqntxXKmdmVEXsYd9HNI8T1BDHQ00yXuERzzh2apbsZVbd5+pVsFodvFtWQ8fAiGQHA==</latexit>

s2
<latexit sha1_base64="9HpV6YLUpskfSajXTx2xU6cogv8=">AAACxnicjVHLSsNAFD2Nr1pfVZdugkVwVZIi6LLopsuKthVqKUk6rUPzYmailCL4A27108Q/0L/wzpiCWkQnJDlz7j1n5t7rpyGXynFeC9bC4tLySnG1tLa+sblV3t5pyyQTAWsFSZiIK9+TLOQxaymuQnaVCuZFfsg6/vhMxzu3TEiexJdqkrJe5I1iPuSBp4i6kP1av1xxqo5Z9jxwc1BBvppJ+QXXGCBBgAwRGGIowiE8SHq6cOEgJa6HKXGCEDdxhnuUSJtRFqMMj9gxfUe06+ZsTHvtKY06oFNCegUpbRyQJqE8QVifZpt4Zpw1+5v31Hjqu03o7+deEbEKN8T+pZtl/lena1EY4sTUwKmm1DC6uiB3yUxX9M3tL1UpckiJ03hAcUE4MMpZn22jkaZ23VvPxN9Mpmb1PshzM7zrW9KA3Z/jnAftWtV1qu75UaV+mo+6iD3s45DmeYw6GmiiRd4jPOIJz1bDiq3MuvtMtQq5ZhfflvXwAQSBkB0=</latexit><latexit sha1_base64="9HpV6YLUpskfSajXTx2xU6cogv8=">AAACxnicjVHLSsNAFD2Nr1pfVZdugkVwVZIi6LLopsuKthVqKUk6rUPzYmailCL4A27108Q/0L/wzpiCWkQnJDlz7j1n5t7rpyGXynFeC9bC4tLySnG1tLa+sblV3t5pyyQTAWsFSZiIK9+TLOQxaymuQnaVCuZFfsg6/vhMxzu3TEiexJdqkrJe5I1iPuSBp4i6kP1av1xxqo5Z9jxwc1BBvppJ+QXXGCBBgAwRGGIowiE8SHq6cOEgJa6HKXGCEDdxhnuUSJtRFqMMj9gxfUe06+ZsTHvtKY06oFNCegUpbRyQJqE8QVifZpt4Zpw1+5v31Hjqu03o7+deEbEKN8T+pZtl/lena1EY4sTUwKmm1DC6uiB3yUxX9M3tL1UpckiJ03hAcUE4MMpZn22jkaZ23VvPxN9Mpmb1PshzM7zrW9KA3Z/jnAftWtV1qu75UaV+mo+6iD3s45DmeYw6GmiiRd4jPOIJz1bDiq3MuvtMtQq5ZhfflvXwAQSBkB0=</latexit><latexit sha1_base64="9HpV6YLUpskfSajXTx2xU6cogv8=">AAACxnicjVHLSsNAFD2Nr1pfVZdugkVwVZIi6LLopsuKthVqKUk6rUPzYmailCL4A27108Q/0L/wzpiCWkQnJDlz7j1n5t7rpyGXynFeC9bC4tLySnG1tLa+sblV3t5pyyQTAWsFSZiIK9+TLOQxaymuQnaVCuZFfsg6/vhMxzu3TEiexJdqkrJe5I1iPuSBp4i6kP1av1xxqo5Z9jxwc1BBvppJ+QXXGCBBgAwRGGIowiE8SHq6cOEgJa6HKXGCEDdxhnuUSJtRFqMMj9gxfUe06+ZsTHvtKY06oFNCegUpbRyQJqE8QVifZpt4Zpw1+5v31Hjqu03o7+deEbEKN8T+pZtl/lena1EY4sTUwKmm1DC6uiB3yUxX9M3tL1UpckiJ03hAcUE4MMpZn22jkaZ23VvPxN9Mpmb1PshzM7zrW9KA3Z/jnAftWtV1qu75UaV+mo+6iD3s45DmeYw6GmiiRd4jPOIJz1bDiq3MuvtMtQq5ZhfflvXwAQSBkB0=</latexit><latexit sha1_base64="9HpV6YLUpskfSajXTx2xU6cogv8=">AAACxnicjVHLSsNAFD2Nr1pfVZdugkVwVZIi6LLopsuKthVqKUk6rUPzYmailCL4A27108Q/0L/wzpiCWkQnJDlz7j1n5t7rpyGXynFeC9bC4tLySnG1tLa+sblV3t5pyyQTAWsFSZiIK9+TLOQxaymuQnaVCuZFfsg6/vhMxzu3TEiexJdqkrJe5I1iPuSBp4i6kP1av1xxqo5Z9jxwc1BBvppJ+QXXGCBBgAwRGGIowiE8SHq6cOEgJa6HKXGCEDdxhnuUSJtRFqMMj9gxfUe06+ZsTHvtKY06oFNCegUpbRyQJqE8QVifZpt4Zpw1+5v31Hjqu03o7+deEbEKN8T+pZtl/lena1EY4sTUwKmm1DC6uiB3yUxX9M3tL1UpckiJ03hAcUE4MMpZn22jkaZ23VvPxN9Mpmb1PshzM7zrW9KA3Z/jnAftWtV1qu75UaV+mo+6iD3s45DmeYw6GmiiRd4jPOIJz1bDiq3MuvtMtQq5ZhfflvXwAQSBkB0=</latexit>

B-ORG I-ORGB-NT I-NT

Figure 1: Example of weakly labeled data. B-NT and
I-NT denote incomplete labels without types.

et al., 2016), which has became a basic architec-
ture due to its superior performance. Nevertheless,
NN-CRFs require exhaustive human efforts for
training annotations, and may not perform well in
low-resource settings (Ni et al., 2017). Many ap-
proaches thus focus on transferring cross-domain,
cross-task and cross-lingual knowledge into name
tagging (Yang et al., 2017; Peng and Dredze, 2016;
Mayhew et al., 2017; Pan et al., 2017; Lin et al.,
2018; Xie et al., 2018). However, they are usually
limited by the extra knowledge resources that are
effective only in specific languages or domains.

Actually, in many low-resource settings, there
are extensive noisy annotations that naturally ex-
ist on the web yet to be explored (Ni et al., 2017).
In this paper, we propose a novel model for name
tagging that maximizes the potential of weakly
labeled (WL) data. As shown in Figure 1, s2
is weakly labeled, since only Formula shell and
Barangay Ginebra are annotated, leaving the re-
maining words unannotated.

WL data is more practical to obtain, since it is
difficult for people to accurately annotate those en-
tities that they do not know or are not interested
in. We can construct them from online resources,
such as the anchors in Wikipedia. However, the
following natures of WL data make learning name
tagging from them more challenging:

Partially-Labeled Sequence Automatically

https://github.com/zig-kwin-hu/Low-Resource-Name-Tagging
https://github.com/zig-kwin-hu/Low-Resource-Name-Tagging


262

derived WL data does not contain complete anno-
tations, thus can not be directly used for training.
Ni et al. (2017) select the sentences with highest
confidence, and assume missing labels as O (i.e.,
non-entity), but it will introduce a bias to recog-
nize mentions as non-entity. Another line of work
is to replace CRFs with Partial-CRFs (Täckström
et al., 2013), which assign unlabeled words with
all possible labels and maximize the total proba-
bility (Yang et al., 2018; Shang et al., 2018). How-
ever, they still rely on seed annotations or domain
dictionaries for high-quality training.

Massive Noisy Data WL corpora are usually
generated with massive noisy data including miss-
ing labels, incorrect boundaries and types. Previ-
ous work filtered out WL sentences by statistical
methods (Ni et al., 2017) or the output of a train-
able classifier (Yang et al., 2018). However, aban-
doning training data may exacerbate the issue of
inadequate annotation. Therefore, maximizing the
potential of massive noisy data as well as high-
quality part, yet being efficient, is challenging.

To address these issues, we first differentiate
noisy data from high-quality WL sentences via a
lightweight scoring strategy, which accounts for
the annotation confidence as well as the coverage
of all mentions in one sentence. To take best ad-
vantages of all WL data, we then propose a unified
neural framework that solves name tagging from
two perspectives: sequence labeling and classifi-
cation for two types of data, respectively.

Specifically, the classification module focuses
on noisy data to efficiently pretrain the tag clas-
sifier by capturing textual context semantics. It is
trained only using annotated words without noisy
unannotated words, and thus it is robust and effi-
cient during training. The costly sequence labeling
module is to achieve sequential optimum among
word tags. It further alleviates the burden of seed
annotations in Partial-CRFs and increases ran-
domness via Non-entity Sampling strategy, which
samples O words according to some linguistic na-
tures. These two modules are combined via shared
parameters. Our main contributions are as follows:

• We propose a novel neural name tagging
model that merely relies on WL data with-
out feature engineering. It can thus be
adapted for both low-resource languages and
domains, while no previous work deals with
them at the same time.

• We consider name tagging from two perspec-

tives of sequence labeling and classification,
to efficiently take the best advantage of both
high-quality and noisy WL data.

• We conduct extensive experiments in five
low-resource languages and a fine-grained
domain. Since few work has been done in
two types of low-resource settings simulta-
neously, we arrive at two types of baselines
from state-of-the-art methods. Our model
achieves significant improvements (6% and
7.8% F1 on average), yet being efficient
demonstrated in further ablation studies.

2 Related Work

Name tagging is an fundamental task of extracting
entity information, which shall benefit many ap-
plications, such as information extraction (Zhang
et al., 2017; Kuang et al., 2019; Cao et al., 2019a)
and recommendation (Wang et al., 2019; Cao
et al., 2019b). It can be treated as either a multi-
class classification problem (Hammerton, 2003;
Xu et al., 2017) or a sequence labeling prob-
lem (Collobert et al., 2011), but very little work
combined them together. The difference between
them mainly lies in whether the method mod-
els sequential label constraints, which have been
demonstrated effective in many NN-CRFs mod-
els (Lample et al., 2016; Ma and Hovy, 2016; Chiu
and Nichols, 2016). However, they require a large
amount of human annotated corpora, which are
usually expensive to obtain.

The above issue motivates a lot of work on name
tagging in low-resource languages or domains. A
typical line of effort focuses on introducing exter-
nal knowledge via transfer learning (Fritzler et al.,
2018; Hofer et al., 2018), such as the use of cross-
domain (Yang et al., 2017), cross-task (Peng and
Dredze, 2016; Lin et al., 2018) and cross-lingual
resources (Ni et al., 2017; Xie et al., 2018; Zafar-
ian et al., 2015; Zhang et al., 2016; Mayhew et al.,
2017; Tsai et al., 2016; Feng et al., 2018; Pan et al.,
2017). Although they achieve promising results,
there are a large amount of weak annotations on
the Web, which have not been well studied (Noth-
man et al., 2008; Ehrmann et al., 2011). Yang
et al. (2018); Shang et al. (2018) utilized Partial-
CRFs (Täckström et al., 2013) to model incom-
plete annotations for specific domains, but they
still rely on seed annotations or a domain dictio-
nary. Therefore, we aim at filling the gap in low-
resource name tagging research by using only WL



263

Noisy Weakly Labeled Data

Label Scores

Neural Network

Partial-CRFs with Non-Entity Sampling

B-ORGO I-ORG

Neural Network Shared
Parameter

B-ORG I-ORG … O

… Barangay Ginebra and Formula Shell forming an …The team is owned by Ginebra

Weakly Labeled Data Generation

O

Neural Name Tagging Model

Classification

Module

Sequence Labeling 

Module

Label Induction Data Selection Scheme

B-ORG
I-ORG

…

B-ORG
I-ORG

…

O

B-ORG
I-ORG

…

High-quality Weakly Labeled Data

O

B-ORG
I-ORG

…

O

B-ORG
I-ORG

…

Figure 2: Framework. Rectangles denote the main components for two steps, and rounded rectangles consist of two
modules of the neural model. In input sentences, bold fonts denote labeled words, and at the top is corresponding
outputs. We use Partial-CRFs to model all possible label sequences (red paths from left to right by picking up
one label per column) controlled by non-entity sampling (strikethrough labels according to the distribution). We
replace “UN” and “x-NT” label with corresponding possible labels to clarify the principle of PCRF.

data, and adapt it to arbitrary low-resource lan-
guages or domains, which can be further improved
by the above transfer-based methods.

3 Preliminaries and Framework

3.1 Preliminaries
We formally define the name tagging task as
follows: given a sequence of words X =
〈x1, · · · , xi, · · · , x|X|〉, it aims to infer a sequence
of labels Y = 〈y1, · · · , yi, · · · , y|X|〉, where |X|
is the length of the sequence, yi ∈ Y is the label
of the word xi, each label consists of the boundary
and type information, such as B-ORG indicating
that the word is Begin of an ORGanization en-
tity. To make notations consistent, we use Ỹ =
Y
⋃
{UN,B-NT,I-NT} to denote the label set of

WL data, where UN indicates that the word is un-
labeled, and NT denote only the type is unlabeled.
In other words, the word with UN may be any one
of the label in Y , and the word with NT may be
any type. We define Ỹ for notation clarity.

To deal with the issue of limited annotations,
we construct WL data D = {(X, Ỹ )} based on
Wikipedia anchors and taxonomy, where Ỹ =
〈ỹ1, · · · , ỹi, · · · , ỹ|X|〉 and ỹi ∈ Ỹ . An anchor
〈m, e〉 ∈ A links a mention m to an entity
e ∈ E, where m contains one or several consec-
utive words of length |m|. Particularly, we define
A(X) as the set of anchors in X . Most entities
are mapped to hierarchically organized categories,
namely taxonomy T , which provides category in-
formation C = {c}. We define C(e) as the cate-
gory set of e, and T↓(c) as the children of c.

3.2 Framework
The goal of our method is to extract WL data from
Wikipedia and use them as training corpora for
name tagging. As shown in Figure 2, there are
two steps in our framework:

Weakly Labeled Data Generation generates
as many WL data as possible for higher tagging
recall. It contains two components of label in-
duction and data selection scheme. First, the la-
bel induction assigns each word a label based on
Wikipedia anchors and taxonomy. Then, the data
selection scheme computes the quality scores for
the WL sentences by considering the coverage of
mentions as well as the label confidence. Accord-
ing to the scores, we split the entire set into two
parts: a small set of high-quality data for the se-
quence labeling module, and a large amount of
noisy data for the classification module.

Neural Name Tagging Model aims at effi-
ciently and robustly utilizing both high-quality
and noisy WL data, ensuring satisfying tagging
precision. It is to make best use of labeled words
via the sequence labeling module and the classifi-
cation module. More specifically, we pre-train the
classification module to capture the textual con-
text semantics from massive noisy data, and then
the sequence labeling module further fine-tunes
the shared neural networks using a Partial-CRFs
layer with Non-Entity Sampling.

4 Weakly Labeled Data Generation

Existing methods use Wikipedia (Ni et al., 2017;
Pan et al., 2017; Geiß et al., 2017) to train an extra



264

classifier to predict entity categories for name tag-
ging training. Instead, we aim at lowering the re-
quirements of additional resources in order to sup-
port more low-resource settings. We thus utilize a
lightweight strategy to generate WL data includ-
ing label induction and data selection scheme.

4.1 Label Induction

Given a sentence X including anchors A(X) and
taxonomy T , we aim at inducing a label ỹ ∈ Ỹ for
each word x ∈ X . Obviously, the words outside
of anchors should be labeled with UN, indicating
that it is unlabeled and could be O or unannotated
mentions. For the words in an anchor 〈m, e〉, we
label it according to the entity categories. For ex-
ample, words Formula and Shell (Figure 1) in s2
are labeled as B-ORG and I-ORG, respectively,
because mention Formula Shell is linked to en-
tity Shell Turbo Chargers, which belongs to cat-
egory Basketball teams. We trace it along the tax-
onomy T : Basketball teams→...→Organizations,
and find that it is a child of Organizations. Ac-
cording to a manually defined mapping Γ(Y) →
C (e.g., Γ(ORG) =Organizations), we denote all
the classes and their children with the same type
(e.g., ORG).

However, there are two minor issues. First, for
the entities without category information C(e) =
∅, we label them as B-NT or I-NT, indicating that
they have no type information. Second, for the
entities referring to multiple categories, we induce
labels that maximizes the conditional probability:

argmax
y∗

p(y∗|C(e)) =
∑

c∈C(e) 1(c ∈ T↓(Γ(y∗)))
|C(e)|

(1)
where 1(·) indicates 1 if holds true, otherwise 0.

By doing so, we obtain a set of WL sentences
D = {(X, Ỹ )}. However, the induction process
may introduce incorrect boundaries and types of
labels due to the crowdsourcing nature of source
data. We thus design a data selection scheme to
deal with the issues.

4.2 Data Selection Scheme

Following Ni et al. (2017), we compute quality
scores for sentences to distinguish high-quality
and noisy data from two aspects: the annotation
confidence and the annotation coverage.

The annotation confidence measures the likeli-
hood of the text spans being mentions (i.e., cor-

rectness of boundaries), and being assigned with
the types. We define it as follows:

q(X, Ỹ ) =

∑
(xi,ỹi)

1(ỹi ∈ Y)p(ỹi|C(e))p(C(e)|xi)

|X|
(2)

where p(C(e)|xi) is the conditional probability of
xi linking to an entity belong to category C(e),
we compute it based on its statistical frequency
among Wikipedia anchors.

The annotation coverage measures to which ra-
tio the words are being labeled in the sentence:

n(X, Ỹ ) =

∑
(xi,ỹi)

1(ỹi ∈ Y)

|X|
(3)

We select high-quality sentences Dhq satisfying:

q(X, Ỹ ) ≥ θq;n(X, Ỹ ) ≥ θn (4)

where θq and θn are the hyperparameters. Thus,
the remaining sentences are noisy Dnoise.

For example (Figure 2), the sentence ...
Barangay Ginebra and Formula Shell ... is high-
quality, and The team is owned by Ginebra is
noisy. This is because there are more anchors that
link Formula Shell to an organization entity and
the anchors within the sentence account for a large
proportion, leading to a higher quality score. Note
that Barangay and Ginebra are labeled with B-NT
and I-NT, indicating the type information is miss-
ing. Our model may learn the textual semantics for
classifying Ginebra to ORG from the noisy sen-
tence, where Ginebra is labeled with B-ORG.

5 Neural Name Tagging Model

Our neural model contains two modules that share
the same NN architecture except the Partial-CRFs
layer. Given Dhq and Dnoise, we first pre-train
the classification module using massive noisy data
Dnoise to efficiently capture the textual seman-
tics. Then, we use the sequence labeling mod-
ule to fine-tune the classification module on high-
quality data Dhq by considering the transitional
constraints among sequential labels.

5.1 Sequence Labeling Module

Before describing the NN of the classification
module, we first introduce the sequence labeling
module. Different from conventional NN-CRFs



265

models, we utilize the Partial CRFs layer to max-
imize the probability of all possible sequential la-
bels for the sentence with transitional constraints,
where the probability of missing word labels is
controlled by non-entity sampling.

Partial-CRFs
Partial-CRFs (PCRFs) was first proposed in the
field of Part-of-Speech Tagging (Täckström et al.,
2013). It can be trained when the coupled word
and label constraints provide only a partial signal
by assuming that the uncoupled words may refer to
multiple labels. Given (X, Ỹ ), we traverse all pos-
sible labels Y for each unannotated word {xi|ỹi ∈
UN,B-NT,I-NT} (e.g., the red paths in Figure 2),
and compute the total probability of possible fully
labeled sentences Y(X, Ỹ ) = {(X,Y )}:

p(Ỹ |X) =
Y(X,Ỹ )∑
(X,Y )

p(Y |X) (5)

where p(Y |X) = softmax(s(X,Y )), the same as
in CRFs, and the score function s(X,Y ) is:

s(X,Y ) =

|X|∑
i=0

Ayi,yi+1 +

|X|∑
i=1

Pxi,yi (6)

where Pxi,yi is the score indicating how possible
xi is labeled with yi, which is defined as the out-
put of NN and will be detailed in the next section.
Ayi,yi+1 is the transition score from label yi to yi+1
that is learned in this layer.

Instead of the single correct label sequence in
CRFs, the loss function of partial-CRFs is to min-
imize the negative log-probability of ground truth
over all possible labeled sequences:

L = −
Dnq∑
(X,Ỹ )

log p(Ỹ |X) (7)

Non-entity Sampling
A crucial drawback of using partial CRFs for WL
sentences is that there are no words labeled with O
(i.e., non-entity words) for training (Section 6.5).

To further alleviate the reliance on seed annota-
tions, we introduce non-entity sampling that sam-
ples O from unlabeled words as follows:

p(yi = O|xi, ỹi = N) =
α

3
(λ1f1 + λ2(1− f2) + λ3f3)

(8)

where α is non-entity ratio to balance how many
unlabeled words are sampled as O, we set α =
0.9 in experiments according to Augenstein et al.
(2017). Weighting parameters satisfy 0 ≤
λ1, λ2, λ3 ≤ 1, and f1, f2, f3 are feature scores.
We define f1 = 1(xi adjoins an entity), which im-
plies that the words around a mention are possi-
ble to be O; f2 is the ratio of the number of xi
labeled with entities to its total occurrences, re-
flecting how frequent a word is in a mention; and
f3 = tf ∗df , where tf is term frequency and df is
document frequency in Wikipedia articles.

As shown in Figure 2, three words and, forming
and an are labeled with N since they are outside of
anchors. During training, they should be regarded
as all labels of Y in Partial CRFs, while we sample
some of them as O words according to Equation 8.
Thus, and and an are instead treated as O words,
because they do not appear in any anchor, and are
too general due to a high f3 value.

5.2 Classification Module

To efficiently utilize the noisy WL sentences, this
module regards name tagging as a multi-label clas-
sification problem. On one hand, it predicts each
word’s label separately, naturally addressing the
issue of inconsecutive labels. On the other hand,
we only focus on the labeled words, so that the
module is robust to the noise since most noise
arises from the unlabeled words, and enjoy an ef-
ficient training procedure.

Formally, given a noisy sentence (X, Ỹ ) ∈
Dnoise, we classify words {xi|ỹi ∈ Y} by cap-
turing textual semantics within the context. Inde-
pendently of languages and domains, we combine
the character and word embeddings for each word,
then feed them into an encoder layer to capture
contextual information for the classification layer.

Character and Word Embeddings

As inputs, we introduce character information to
enhance word representations to improve the ro-
bustness to morphological and misspelling noise
following (Ma and Hovy, 2016). Concretely, we
represent a word x by concatenating word em-
bedding w and Convolutional Neural Networks
(CNN) (LeCun et al., 1989) based character em-
bedding c, which is obtained through convolution
operations over characters in a word followed by
max pooling and drop out techniques.



266

Encoder Layer
Given an arbitrary length of sentence X , this
component encodes the semantics of words
as well as their compositionality into a low-
dimensional vector space. The most common
encoders are CNN, Long-Short Term Memory
(LSTM) (Hochreiter and Schmidhuber, 1997) and
Transformer (Vaswani et al., 2017). We use the
bi-directional LSTM (Bi-LSTM) due to its supe-
rior performance. We discuss it in Section 6.2.

Bi-LSTM (Graves et al., 2013) has been widely
used in modeling sequential words, so as to cap-
ture both past and future input features for a given
word. It stacks a forward LSTM and a back-
ward LSTM, so that the output of a word xi is
hi = [

←−
h i;
−→
h i], where

−→
h i = LSTM(X1:i) and←−

h i = LSTM(Xi:|X|).

Classification Layer
The classification layer makes independent label-
ing decisions for each word, so that we can only
focus on labeled words, while robustly and effi-
ciently skip the noisy unlabeled words.

In this layer, we estimate the score Pxi,yi (Equa-
tion 6) for word xi being the label yi. We use a
fully connected layer followed by softmax to out-
put a probability-like score:

Pxi,yi = Softmax(Whi + b) (9)

where W ∈ R|Y|. Note that we have no training
instance for O words. Thus, we also use the non-
entity sampling (Section 5.1). Given (X, Ỹ ) ∈
Dnoise, this module is trained to minimize cross-
entropy of the predicted and ground truth:

Lc = −
Dnoise∑
(X,Ỹ )

1(ỹi ∈ Y)ỹi logPxi,yi (10)

5.3 Training and Inference

To distill the knowledge derived from noisy data,
we first pre-train the classification module, then
share the overall NN with the sequence labeling
module. If we choose a loose threshold θp and θn,
there is no noisy data and our model shall degrade
to the sequential model without the pre-trained
classifier. When the threshold is strict, there is no
high-quality data and our model will degrade to
the classification module only (Section 6.4).

For inference, we use the sequence labeling
module to predict the output label sequence with
the largest score as in Equation 6.

6 Experiment

We verify our model using five low-resource lan-
guages and a specific domain. Furthermore, we
investigate the impacts of the main components as
well as hyperparameters in the ablation study.

6.1 Experiment Settings

Datasets Since most datasets on low-resource
languages are not publicly available, we use
Wikipedia data as the “ground truth” follow-
ing Pan et al. (2017). Thus, we can test name
tagging in low-resource languages as well as do-
mains. We choose five languages: Welsh, Bengali,
Yoruba, Mongolian and Egyptian Arabic (or CY,
BN, YO, MN and ARZ for short), at different low-
resource levels, and select 3 types: Person, Loca-
tion and Organization. For food domain, we reor-
ganized the entities in Wikipedia category “Food
and drink” into 5 types: Drinks, Meat, Vegetables,
Condiments and Breads, for name tagging, and ex-
tract sentences containing those entities from all
English Wikipedia articles for as many data as
possible.

Train Test
#Sent #Ment #Sent # Ment

CY 106,541 146,524 1,193 3,256
BN 66,915 127,932 870 2838
YO 36,548 10,405 77 232
MN 19,250 27,820 173 439
ARZ 18,700 28,928 195 377

Food Domain 27,798 32,155 207 253
Drinks 8,615 9,218 62 67
Meat 7,685 8,841 53 68

Vegetables 6,155 7,235 45 58
Condiments 3,737 4,084 27 30

Breads 2,515 2,777 24 30

Table 1: The statistics of weakly labeled dataset.

We use 20190120 Wikipedia dump for WL data
construction, where the ratio of words in anchors
to the whole sentence is nearly 0.12, 0.07, 0.14,
0.07 and 0.06 for languages CY, BN, YO, MN
and ARZ, and 0.13 for food domain, demon-
strating that unlabeled words are dominant. By
heuristically setting θq = 0.1, θn = 0.9, we ob-
tain 56,571, 16,718, 4,131, 8,332, 6,266, 11,297
high-quality and 49,970, 50,197, 32,417, 10,918,
12,434, 16,501 noisy WL sentences for language



267

CY BN YO MN ARZ
P R F1 P R F1 P R F1 P R F1 P R F1

CNN-CRFs 84.4 76.2 80.1 92.0 89.1 90.5 80.9 68.9 74.4 87.3 85.5 86.3 88.6 86.7 87.6
BiLSTM-CRFs 86.0 77.8 81.6 93.3 91.5 92.3 74.1 68.9 71.3 89.0 85.5 87.1 89.5 88.5 89.0

Trans-CRFs 83.7 73.2 78.1 93.0 85.9 89.3 80.2 60.5 69.0 88.0 80.0 83.8 88.9 83.2 85.9
BiLSTM-PCRFs 85.2 79.6 82.3 91.2 92.7 91.9 68.1 70.2 69.1 82.5 91.2 86.6 84.0 90.7 87.1

Ours 82.8 82.5 82.6 93.4 93.5 93.4 73.5 76.8 75.1 86.9 93.6 90.1 87.7 91.5 89.5

Table 2: Performance (%) on low-resource languages.

CY, BN, YO, MN and ARZ, and food domain, re-
spectively. For correctness, we then pick up test
data of 25% sentences that has highest annotation
confidence and exceed 0.3 coverage. We randomly
choose 25% of high-quality data as validation for
early stop, and the rest for training. The statistics3

is in Table 1.
Training Details

For tuning of hyper-parameters, we set non-
entity feature weights to λ1 = 0, λ2 = 0.9, λ3 =
0.1 heuristically. We pre-train word embeddings
using Glove (Pennington et al., 2014), and fine-
tune embeddings during training. We set the di-
mension of words and characters as 100 and 30,
respectively. We use 30 filter kernels, where each
kernel has the size of 3 in character CNN, and
dropout rate is set to 0.5. For bi-LSTM, the hidden
state has 150 dimensions. The batch size is set to
32 and 64 for sequence labeling module and clas-
sification module. We adopt Adam with L2 reg-
ularization for optimization, and set the learning
rate and weight decay to 0.001 and 1e−9.
Baselines Since most low-resource name tag-
ging methods introduce external knowledge (Sec-
tion 2), which has limited availability and is out of
the scope for this paper, we arrive at two types of
baselines from weakly supervised models:

Typical NN-CRFs models (Ni et al., 2017) by
selecting high-quality WL data and regarding un-
labeled words as O, which usually achieve very
competitive results. NN denotes CNN, Trans-
former (Trans for short) or Bi-LSTM.

NN-PCRFs model (Yang et al., 2018; Shang
et al., 2018). Although they achieves state-of-
the-art performance, methods of this type are only
evaluated in specific domains and require a small
set of seed annotations or a domain dictionary.
We thus carefully adapt them to low-resource
languages and domains by selecting the highest-
quality WL data (θn > 0.3) as seeds4.

3The statistics includes noisy data, which greatly in-
creases the size but cannot be used for evaluation.

4We adopt the common part of their models related to

6.2 Results on Low-Resource Languages

Table 2 shows the overall performance of our pro-
posed model as well as the baseline methods (P
and R denote Precision and Recall). We can see:

Our method consistently outperforms all base-
lines in five languages w.r.t F1, mainly because we
greatly improve recall (2.7% to 9.34% on average)
by taking best advantage of WL data and being ro-
bust to noise via two modules. As for the preci-
sion, partial-CRFs perform poorly compared with
CRFs due to the uncertainty of unlabeled words,
while our method alleviates this issue by introduc-
ing linguistic features in non-entity sampling. An
exception occurs in CY, because it has the most
training data, which may bring more accurate in-
formation than sampling. Actually, we can tune
hyper-parameter non-entity ratioα to improve pre-
cision5, more studies can be found in Section 6.5.
Besides, the sampling technique can utilize more
prior features if available, we leave it in future.

Among all encoders, Bi-LSTM has greater abil-
ity for feature abstraction and achieves the highest
precision in most languages. An unexpected ex-
ception is Yoruba, where CNN achieves the higher
performance. This indicates that the three en-
coders capture textual semantics from different
perspectives, thus it is better to choose the encoder
by considering the linguistic natures.

As for the impacts of resources, all the mod-
els perform the worst in Yoruba. Interestingly, we
conclude that the performance for name tagging
in low-resource languages doesn’t depend entirely
on the absolute number of mentions in the train-
ing data, but largely on the average number of an-
notations per sentence. For example, Bengali has
1.9 mentions per sentence and all methods achieve
their best results, while the opposite is Welsh with

handling weakly labeled data, removing the other parts that
are specifically designed for domains, such as instance selec-
tor (Yang et al., 2018) which makes it worse since we have
already selected the high-quality data.

5In this table, we show the performance using the same
hyper-parameters in different languages for fairness.



268

(a) Efficiency analysis. (b) Impact of non-entity sampling ratio. (c) Impact of non-entity features.

Figure 3: Ablation study of our model in Mongolian.

1.4 mentions per sentence. This verifies our data
selection scheme (e.g., annotation coverage n(·)),
and we will give more discussion in Section 6.4.

6.3 Results on Low-Resource Domains

Food
D M V C B All

CNN-CRFs 67.8 69.8 57.9 42.8 46.5 60.9
BiLSTM-CRFs 64.9 69.0 62.8 50.0 62.2 63.5

Trans-CRFs 62.1 68.9 59.6 43.4 54.5 60.6
BiLSTM-PCRFs 66.1 70.7 67.2 44.4 58.3 64.4

Ours 72.0 76.4 70.0 53.5 66.6 70.1

Table 3: F1-score (%) on food domain.

Table 3 shows the overall performance in food
domain, where D, M, V, C and B denote: Drink,
Meat, Vegetables, Condiments and Breads. We
can observe that there is a performance drop com-
pared to that in low-resource languages, mainly
because of more types and sparse training data.
Our model outperforms all of the baselines in all
food types by 7.8% on average. The performance
in condiments is relatively low, because most of
them are composed of meat or vegetables, such as
steak sauce, which is overlapped with other types
and make the recognition more difficult.

Figure 4: Our predictions on a noisy WL sentence.

Here is a representative case demonstrating that
our model is robust to noise induced by unlabeled
words. In Figure 4, the sentence is from the noisy
WL training data of food domain, and only Maize
is labeled as B-V. Although our model is trained on
this sentence, it successfully predicts yams as B-V.
This example shows that our two-modules design

can utilize the noisy data while avoiding side ef-
fects caused by incomplete annotation.

6.4 Efficiency Analysis
We utilize θn, the main factor to annotation qual-
ity (Section 6.2), to trade off between high-quality
and noisy WL data. As shown in Figure 3(a),
the red curve denotes the training time and the
blue curve denotes F1. We can see that the per-
formance of our model is relatively stable when
θn ∈ [0, 0.15), while the time cost drops dramat-
ically (from 90 to 20 minutes), demonstrating the
robustness and efficiency of two-modules design.
When θn ∈ [0.15, 0.3], the performance decreases
greatly due to less available high-quality data for
sequence labeling module; meanwhile, little time
is saved through classification module. Thus, we
pick up θn = 0.1 in experiments. A special
case happens when θn = 0, our model degrades
to sequence labeling without pre-trained classifier.
We can see the performance is worse than that of
θn = 0.1 due to massive noisy data.

6.5 Impact of Non-Entity Sampling Ratio
We use non-entity ratio α to control sampling,
and a higher α denotes that more unlabeled words
are labeled with O. As shown in Figure 3(b), the
precision increases as more words are assigned
with labels, while the recall achieves two peaks
(α = 0.4, 0.9), leading to the highest F1 when
α = 0.9, which conforms to the statistics in Au-
genstein et al. (2017). There are two special cases.
When α = 0, our model degrades to a NN-PCRFs
model without non-entity sampling and there is
no seed annotations for training. We can see the
model performs poorly due to the dominant unla-
beled words (Section 5.1). When α = 1 indicating
all unlabeled words are sampled as O, our model



269

degrades to NN-CRFs model, which has higher
precision at the cost of recall. Clearly, the model
suffers from the bias to O labeling.

6.6 Impact of Non-Entity Features

We propose three features for non-entity samples:
nearby entities (f1), ever within entities (f2) and
term/document frequency (f3). We now investi-
gate how effective each feature is. Figure 3(c)
shows the performance of our model that sam-
ples non-entity words using each feature as well as
their combinations. The first bar denotes the per-
formance of sampling without any features. It is
not satisfying but competitive, indicating the im-
portance of non-entity sampling to partial-CRFs.
The single f2 contributes the most, and gets en-
hanced with f3 because they provide complemen-
tary information. Surprisingly, f1 seems better
than f3, but makes the model worse if we use it
combined with f2, f3, thus we set λ1 = 0.

7 Conclusions

In this paper, we propose a novel name tagging
model that consists of two modules of sequence
labeling and classification, which are combined
via shared parameters. We automatically con-
struct WL data from Wikipedia anchors and split
them into high-quality and noisy portions for train-
ing each module. The sequence labeling mod-
ule focuses on high-quality data and is costly due
to the partial-CRFs layer with non-entity sam-
pling, which models all possible label combina-
tions. The classification module focuses on the
annotated words in noisy data to pretrain the tag
classifier efficiently. The experimental results in
five low-resource languages and a specific domain
demonstrate the effectiveness and efficiency.

In the future, we are interested in incorporating
entity structural knowledge to enhance text rep-
resentation (Cao et al., 2017, 2018b), or transfer
learning (Sun et al., 2019) to deal with massive
rare words and entities for low-resource name tag-
ging, or introduce external knowledge for further
improvement.

Acknowledgments

NExT++ research is supported by the National Re-
search Foundation, Prime Minister’s Office, Sin-
gapore under its IRC@SG Funding Initiative.

References
Isabelle Augenstein, Leon Derczynski, and Kalina

Bontcheva. 2017. Generalisation in named entity
recognition: A quantitative analysis. Computer
Speech & Language.

Yixin Cao, Lei Hou, Juanzi Li, and Zhiyuan Liu.
2018a. Neural collective entity linking. In COL-
ING.

Yixin Cao, Lei Hou, Juanzi Li, Zhiyuan Liu,
Chengjiang Li, Xu Chen, and Tiansi Dong. 2018b.
Joint representation learning of cross-lingual words
and entities via attentive distant supervision. In
EMNLP.

Yixin Cao, Lifu Huang, Heng Ji, Xu Chen, and Juanzi
Li. 2017. Bridge text and knowledge by learning
multi-prototype entity mention embedding. In ACL.

Yixin Cao, Zhiyuan Liu, Chengjiang Li, Juanzi Li, and
Tat-Seng Chua. 2019a. Multi-channel graph neural
network for entity alignment. In ACL.

Yixin Cao, Xiang Wang, Xiangnan He, Zikun Hu, and
Tat-Seng Chua. 2019b. Unifying knowledge graph
learning and recommendation: Towards a better un-
derstanding of user preferences. In WWW.

Jason Chiu and Eric Nichols. 2016. Named entity
recognition with bidirectional lstm-cnns. TACL.

Ronan Collobert, Jason Weston, Léon Bottou, Michael
Karlen, Koray Kavukcuoglu, and Pavel Kuksa.
2011. Natural language processing (almost) from
scratch. JMLR.

Maud Ehrmann, Marco Turchi, and Ralf Steinberger.
2011. Building a multilingual named entity-
annotated corpus using annotation projection. In
Proceedings of the International Conference Recent
Advances in Natural Language Processing.

Xiaocheng Feng, Xiachong Feng, Bing Qin, Zhangyin
Feng, and Ting Liu. 2018. Improving low resource
named entity recognition using cross-lingual knowl-
edge transfer. In IJCAI.

Alexander Fritzler, Varvara Logacheva, and Mak-
sim Kretov. 2018. Few-shot classification in
named entity recognition task. arXiv preprint
arXiv:1812.06158.

Johanna Geiß, Andreas Spitz, and Michael Gertz. 2017.
Neckar: a named entity classifier for wikidata. In
International Conference of the German Society for
Computational Linguistics and Language Technol-
ogy. Springer.

Alex Graves, Abdel-rahman Mohamed, and Geoffrey
Hinton. 2013. Speech recognition with deep recur-
rent neural networks. In 2013 IEEE international
conference on acoustics, speech and signal process-
ing.



270

James Hammerton. 2003. Named entity recognition
with long short-term memory. In NAACL.

Sepp Hochreiter and Jürgen Schmidhuber. 1997. Long
short-term memory. Neural computation.

Maximilian Hofer, Andrey Kormilitzin, Paul Goldberg,
and Alejo Nevado-Holgado. 2018. Few-shot learn-
ing for named entity recognition in medical text.
arXiv preprint arXiv:1811.05468.

Jun Kuang, Yixin Cao, Jianbing Zheng, Xiangnan He,
Ming Gao, and Aoying Zhou. 2019. Improving neu-
ral relation extraction with implicit mutual relations.
arXiv preprint arXiv:1907.05333.

John Lafferty, Andrew McCallum, and Fernando CN
Pereira. 2001. Conditional random fields: Prob-
abilistic models for segmenting and labeling se-
quence data. In ICML.

Guillaume Lample, Miguel Ballesteros, Sandeep Sub-
ramanian, Kazuya Kawakami, and Chris Dyer. 2016.
Neural architectures for named entity recognition.
In NAACL.

Yann LeCun, Bernhard Boser, John S Denker, Don-
nie Henderson, Richard E Howard, Wayne Hubbard,
and Lawrence D Jackel. 1989. Backpropagation ap-
plied to handwritten zip code recognition. Neural
computation.

Yankai Lin, Zhiyuan Liu, and Maosong Sun. 2017.
Neural relation extraction with multi-lingual atten-
tion. In ACL.

Ying Lin, Shengqi Yang, Veselin Stoyanov, and Heng
Ji. 2018. A multi-lingual multi-task architecture for
low-resource sequence labeling. In ACL.

Xuezhe Ma and Eduard Hovy. 2016. End-to-end se-
quence labeling via bi-directional lstm-cnns-crf. In
ACL.

Stephen Mayhew, Chen-Tse Tsai, and Dan Roth. 2017.
Cheap translation for cross-lingual named entity
recognition. In EMNLP.

Jian Ni, Georgiana Dinu, and Radu Florian. 2017.
Weakly supervised cross-lingual named entity
recognition via effective annotation and representa-
tion projection. In ACL.

Joel Nothman, James R Curran, and Tara Murphy.
2008. Transforming wikipedia into named entity
training data. In Proceedings of the Australasian
Language Technology Association Workshop 2008.

Xiaoman Pan, Boliang Zhang, Jonathan May, Joel
Nothman, Kevin Knight, and Heng Ji. 2017. Cross-
lingual name tagging and linking for 282 languages.
In ACL.

Nanyun Peng and Mark Dredze. 2016. Improving
named entity recognition for chinese social media
with word segmentation representation learning. In
ACL.

Jeffrey Pennington, Richard Socher, and Christopher
Manning. 2014. Glove: Global vectors for word
representation. In EMNLP.

Jingbo Shang, Liyuan Liu, Xiaotao Gu, Xiang Ren,
Teng Ren, and Jiawei Han. 2018. Learning named
entity tagger using domain-specific dictionary. In
EMNLP.

Qianru Sun, Yaoyao Liu, Tat-Seng Chua, and Bernt
Schiele. 2019. Meta-transfer learning for few-shot
learning. In CVPR.

Oscar Täckström, Dipanjan Das, Slav Petrov, Ryan
McDonald, and Joakim Nivre. 2013. Token and type
constraints for cross-lingual part-of-speech tagging.
TACL.

Chen-Tse Tsai, Stephen Mayhew, and Dan Roth. 2016.
Cross-lingual named entity recognition via wikifica-
tion. In CoNLL.

Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob
Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz
Kaiser, and Illia Polosukhin. 2017. Attention is all
you need. In NeurIPS.

Xiang Wang, Dingxian Wang, Canran Xu, Xiangnan
He, Yixin Cao, and Tat-Seng Chua. 2019. Explain-
able reasoning over knowledge graphs for recom-
mendation. In Proceedings of the AAAI Conference
on Artificial Intelligence, volume 33, pages 5329–
5336.

Jiateng Xie, Zhilin Yang, Graham Neubig, Noah A
Smith, and Jaime Carbonell. 2018. Neural cross-
lingual named entity recognition with minimal re-
sources. In EMNLP.

Mingbin Xu, Hui Jiang, and Sedtawut Watcharawit-
tayakul. 2017. A local detection approach for named
entity recognition and mention detection. In ACL.

Yaosheng Yang, Wenliang Chen, Zhenghua Li,
Zhengqiu He, and Min Zhang. 2018. Distantly su-
pervised ner with partial annotation learning and re-
inforcement learning. In COLING.

Zhilin Yang, Ruslan Salakhutdinov, and William W
Cohen. 2017. Transfer learning for sequence tag-
ging with hierarchical recurrent networks. In ICLR.

Atefeh Zafarian, Ali Rokni, Shahram Khadivi, and So-
nia Ghiasifard. 2015. Semi-supervised learning for
named entity recognition using weakly labeled train-
ing data. In AISP.

Boliang Zhang, Xiaoman Pan, Tianlu Wang, Ashish
Vaswani, Heng Ji, Kevin Knight, and Daniel Marcu.
2016. Name tagging for low-resource incident lan-
guages based on expectation-driven learning. In
NAACL.

Jing Zhang, Yixin Cao, Lei Hou, Juanzi Li, and Hai-
Tao Zheng. 2017. Xlink: An unsupervised bilingual
entity linking system. In CCL.


