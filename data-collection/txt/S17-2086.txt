



















































ECNU at SemEval-2017 Task 8: Rumour Evaluation Using Effective Features and Supervised Ensemble Models


Proceedings of the 11th International Workshop on Semantic Evaluations (SemEval-2017), pages 491–496,
Vancouver, Canada, August 3 - 4, 2017. c©2017 Association for Computational Linguistics

ECNU at SemEval-2017 Task 8: Rumour Evaluation Using Effective
Features and Supervised Ensemble Models

Feixiang Wang1, Man Lan1,2∗, Yuanbin Wu1,2
1Department of Computer Science and Technology,
East China Normal University, Shanghai, P.R.China

2Shanghai Key Laboratory of Multidimensional Information Processing
51151201049@stu.ecnu.edu.cn, {mlan, ybwu}@cs.ecnu.edu.cn

Abstract

This paper describes our submissions to
task 8 in SemEval 2017, i.e., Determin-
ing rumour veracity and support for ru-
mours. Given a rumoured tweet and a
plethora of replied tweets, subtask A is to
label whether these tweets are support, de-
ny, query or comment, and subtask B aim-
s to predict the veracity (i.e., true, false,
and unverified) with a confidence (in range
of 0-1) of the given rumoured tweet. For
both subtasks, we adopted supervised ma-
chine learning methods incorporating rich
features. Since the training data is imbal-
anced, we specifically designed a two-step
classifier to address subtask A .

1 Introduction

With the rapid development of social media in re-
cent years, people cannot only stay abreast of on-
going events and breaking news, but also express
their own views freely. News can spread quickly
in social media platforms through a large amount
of users, whilst those pieces of unverified infor-
mation often spawn rumours. The RumourEval
(Derczynski et al., 2017) task aims to identify how
users in social media networks regard the originat-
ing rumours and reply to them, as well as analy-
sis and determining veracity of rumoured tweet-
s. The organizer provides tree-structured con-
versations that are associated with breaking news
and consisting of originating rumoured tweets and
tweets replying to them.

There are two subtasks in RumourEval. The
propose of subtask A is, given the related break-
ing news, to predict the class (i.e., support, de-
ny, query, and comment) of the originating ru-
moured tweet (i.e., source tweet) and reactions
(i.e., replied tweets). The goal of subtask B is to

determine the veracity and confidence of the given
rumoured tweet, participants are required to return
a label of rumour as true, false or unverified, with
a confidence value in the range of 0-1.

We treated the two subtasks as multi-
classification problems, and designed multiple
effective natural language processing (NLP)
features to build classifiers to make predictions.
Besides, rumour detection is relevant to sentiment
analysis, for example, support and deny can
be viewed as positive and negative sentiment
respectively. Therefore, we solved the problem
with the aid of a number of sentiment-related
features. Due to the imbalanced characteristic
of the training data, we specifically adopted a
two-step classifier to deal with subtask A. Firstly,
tweets would be separated into two categories:
comment and non-comment, then the tweets
labeled as non-comment would be classified as
support, deny or query. On the other hand, we
directly adopted a three-classification system for
subtask B to label rumoured tweets as true, false
or unverified along with confidence.

2 System Description

For both subtask, we extracted rich features from
the training data and then built classifiers to make
predictions. For subtask A, we designed a two-
step classification system. The first step (1-step)
classifier is to discriminate comment tweets from
non-comment tweets. And the second step (2-step)
classifier is to identify whether a tweet is support,
deny or query towards the rumour if the tweet was
labeled as non-comment in the 1-step classifica-
tion. The 1-step can be viewed as determining
whether a tweet is objective (comment) or sub-
jective (non-comment). The 2-step is actually to
classify a non-comment tweet that expresses posi-
tive (support), negative (deny) or doubtful (query)

491



sentiment. While for subtask B, we simply imple-
mented a three-classification system to determine
whether the given rumoured tweet is true, false or
unverified and returned a confidence of label.

2.1 Feature Engineering

In this section, we give the detail of feature engi-
neering. Five types of NLP features are designed
to capture effective information from the given
tweets.

Linguistic-informed Features

- Word N-grams: We extracted word n-grams
features (n = 1, 2) from tweets. However,
a word has various forms, therefore we also
constructed lemmatization and stem word n-
grams features (n = 1, 2). To accomplish
that, we acquired the lemmatization and stem
of words from the pending sentences, using
the Stanford CoreNLP tools1.

- NER: There are different types of words
in tweets, such as a tweet “Gunman Takes
Hostages In Sydney Cafe” that has useful in-
formation like person and location to help
to detect rumours. NER feature can effec-
tively express aforesaid information. The
12 types (i.e., DURATION, SET, NUM-
BER, LOCATION, PERSON, ORGANIZA-
TION, PERCENT, MISC, ORDINAL, TIME,
DATE, MONEY) named entities are labeled
by Stanford CoreNLP tools. We used a 12-
dimensions binary feature to indicate the en-
tities in tweet.

There are some particular elements in tweets,
that can help to predict labels of tweets. For in-
stance, hashtag and mentioned entity (e.g., “#se-
meval”, “@YouTube”) express the topic informa-
tion of the tweets, and several special punctuation
and emotions (e.g., “!”, “?”, and “:)”) reveal the
sentiment information of users.

Tweet domain Features
We collected all the hashtags and mentioned en-

tities appeared in training tweets, using unigram
feature to imply whether a tweet contained such
information.

- Punctuation: Considering that users often
use exclamation marks and question marks
to express strongly surprised and questioned

1http://stanfordnlp.github.io/CoreNLP/

feelings, we extracted 7-dimensions punctu-
ation features by recording rules of punctu-
ation marks in the tweets (i.e., whether there
is one or more question marks or exclamation
marks, whether there is a question mark or an
exclamation mark in the end of sentence).

- Emoticon: We collected 67 emoticons la-
beled with positive and negative scores from
the Internet2, and used a 67-dimensions fea-
ture to record the sentiment score of the e-
moticon in tweets.

- Event: Training data consists of plenty of
tree-structured conversations that cover eight
breaking news. We gathered several key-
words3 about these events from the Internet
to extract corresponding unigram feature.

Metadata contains important information and
can indicate the popularity of a tweet and the cred-
ibility of the author of a tweet. For example, fea-
tures like “favorite count: 1340”, “retweet count:
500” may indicate whether the tweet is being
watched; “verified: false”, “protected: true” per-
haps imply whether the author is trustworthy.

Tweet metadata Features
We extracted two types of metadata informa-

tion:

- Tweet metadata: We designed a 5-
dimensions feature that consists of tweet fa-
vorite count, retweet count, pre-retweet coun-
t (i.e., the retweet count of the last replied
tweet), create time gap (i.e., the time interval
between the tweet and previous replied tweet)
and tweet level (i.e., the layer of the tweet in
a tweet conversation flow). These numerical
characteristics are normalized by 0-1 normal-
ization.

- User metadata: In addition to the metada-
ta of a tweet, users also have some instru-
mentally valuable metadata as follows: list
count, followers count, user favourites count,
friends count, verified, protected, default pro-
file, profile use background image, and geo

2https://github.com/haierlord/resource/blob/master
/Emoticon.txt

3We enter the hashtag of source tweet on the Internet, to
collect keywords from the headlines of relevant news. For
example, we manually extracted “charlie”, “hebdo”, “attack”
and “terror” from the title “Charlie Hebdo attack: Three days
of terror - BBC News”.

492



enabled. The first four are numerical features
that need normalization, and others are bina-
ry features. The aforementioned features for-
m a 9-dimensions feature.

Word Vector Features
A lot of recent studies on NLP application-

s are reported to have good performance us-
ing word vectors, such as ducument classifi-
cation (Sebastiani, 2002), parsing (Socher et al.,
2013), and question answering (Lan et al., 2016a),
We adopted two widely-used word vectors, i.e.,
GoogleW2V (Mikolov et al., 2013) and GloVe
(Pennington et al., 2014). However, semantic
word vectors find similar words with similar
context rather than similar sentiment informa-
tion. Several recent works focused on sentimen-
t word vectors using neural network based mod-
els (Lan et al., 2016b). In this work, we also
adopted two sentiment word vectors, one is SSWE
(Tang et al., 2014) and the other is a home-made
sentiment word vector from our previous work. To
obtain the representation of a tweet, for each word
in a tweet, we concatenated the maximum, mini-
mum and mean of each dimension as a tweet vec-
tor [min-max-mean].

- GoogleW2V: We adopted the pre-trained
available 300-dimensions word vectors that
were trained on 100 billion words from
Google News by word2vec tool4

- GloVe: The 100-dimensions word vectors
we used were trained on 2 billion tweets and
supplied in GloVe5.

- SSWE: The sentiment-specific word embed-
dings were trained by using multi-hidden-
layers nerual network with a vector size of
50.

- ZSWE: The 200-dimensions home-made
sentiment word vectors were trained with
NRC140 tweet corpus by the Combined-
Sentiment Word Embedding Model.

Word-cluster Feature
To further group similar words into a small set

and to make better use of word semantic informa-
tion, we clustered all the words of tweets by k-
means algorithm. The pending words were first-
ly represented as 300-dimensions word vectors by

4https://code.google.com/archive/p/word2vec
5http://nlp.stanford.edu/projects/glove/

looking up pre-trained GoogleW2V, then grouped
into 80 clusters. Thus we adopted 80-dimensions
binary feature to mark whether the words of a cer-
tain cluster appeared in the tweet.

2.2 Learning algorithms and Evaluation
metrics

Based on above multiple features, we explored
several learning algorithms to build classifica-
tion models, e.g., Logistic Regression (LR), sup-
plied in liblinear tools6, Support Vector Machines
(SVM), Decision Trees (DT), Random Forests (R-
F), AdaBoost (ADB), and Gradient Tree Boost-
ing (GDB), implemented in scikit-learn7. We also
ensembled the effective learning algorithms using
majority vote strategy.

The official evaluation measure for both sub-
tasks is accuracy.

3 Experiments and Results

3.1 Datasets
The statistics of the datasets provided by SemEval
2017 task 8 are shown in Table 1.

Subtask A support(%) query(%) deny(%) comment(%)
train 841(19.8) 330(7.8) 333(7.9) 2, 734(64.5)
dev 69(24.6) 28(10.0) 11(3.9) 173(61.6)
test 94(9.0) 106(10.1) 71(6.8) 778(74.2)

Subtask B true(%) false(%) unverified(%) -
train 127(46.7) 50(18.4) 95(34.9) -
dev 10(40.0) 12(48.0) 3(12.0) -
test 8(28.6) 12(42.9) 8(28.6) -

Table 1: Statistics of training (train), development
(dev) and testing (test) data sets in SemEval 2017
Task 8.

The train and dev sets are associated with eight
different breaking news in English, i.e., char-
liehebdo, ebola-essien, ferguson, germanwings-
crash, ottawashooting, prince-toronto, putinmiss-
ing, and sydneysiege. They are made up of 297
Twitter conversations including 4, 519 tweets in
total. Apart from the eight original breaking news,
the test set adds two new, i.e., hillaryshealth and
save-marinajoyce, and it contains 28 conversa-
tions and 1, 049 tweets. This corpus is collect-
ed using the method described in (Zubiaga et al.,
2016).

3.2 Data Preprocessing
To deal with the informal characteristic of tweets,
we performed tweet normalization to convert elon-

6https://www.csie.ntu.edu.tw/ cjlin/liblinear/
7http://scikit-learn.org/

493



Subtask
Subtask A Subtask B

1-step 2-step -
Algorithm LR SVM DT ADB LR SVM RF ADB GDB LR SVM RF GDB

Tweet domain

Hashtag
√ √ √ √ √ √

Mentioned entity
√ √ √ √ √ √ √

Punctuation
√ √ √ √ √ √ √ √ √ √ √

Emoticon
√ √ √ √ √ √

Event
√ √ √

Metadata Tweet metadata
√ √ √ √ √ √ √

User metadata
√ √ √ √ √

Word-cluster Word-cluster
√ √ √ √ √ √ √

Linguistic

unigram
√ √ √

unigram lemma
√ √ √ √ √

unigram stem
√ √

bigram
bigram lemma

√ √
bigram stem

√
ner

√ √ √ √

Word Vector

GoogleW2V
√

GloVe
√ √ √

SSWE
√

ZSWE
√

Accuracy (%) 80.07 81.14 79.36 81.14 81.49 81.14 80.07 80.43 81.39 70.03 70.70 64.98 67.34
Ensemble (%) 83.99 80.07 71.04

Table 2: Results of feature and algorithm selection experiments for both Subtask A and Subtask B.
1-step, 2-step represent the first and second classification of subtask A respectively,

gated words and slang words into original word.
For elongated word (e.g., “sooo”), we implement-
ed a home-made application to transform it into
“so”, and for slang words, we collected a big dic-
tionary8 from the Internet to convert “LOL” into
“laugh out loud”. Then we conducted tokeniza-
tion, lemmatization and stemming with the aid of
Stanford CoreNLP tools9.

3.3 Experiments on training data

The Table 2 lists the results of the best feature
set with respect to top learning algorithms on t-
wo subtasks. Note that for subtask A, we adopted
a two-step classification. The accuracy of 1-step is
calculated on two classes (i.e., comment and non-
comment ), and that of 2-step is calculated on four
classes (i.e., support, deny, query and comment).
Since the dev set of subtask B is not enough (on-
ly 25 samples), we combined train and dev sets
and performed a 2-fold cross-validation. Further-
more, we also performed ensemble to combine the
results of top learning algorithms with their opti-
mum feature sets, which are shown as the last row
in Table 2.

From Table 2, we observe the findings as fol-
lows:
(1) Among 7 algorithms, LR and SVM consistent-
ly perform well in the three classifications. Be-
sides, ADB does a good job in two classifications
in subtask A, RF and GBD have a good perfor-

8https://github.com/haierlord/resource/blob/master/slangs
9http://stanfordnlp.github.io/CoreNLP/

mance in 2-step of subtask A and subtask B.
(2) Generally, Tweet domain, metadata and Word-
cluster features make a considerable contribution
for both subtasks, and they can achieve promising
performance with different algorithms. The pos-
sible reasons are: (a) Tweet domain features not
only contain sentiment information (e.g., Punctu-
ation and Emoticon), but also include topic in-
formation (e.g., Hashtag, Mentioned entity, and
Event). (b) The numerical characteristic (e.g,
tweet favorite count, retweet count, etc) of meta-
data can indicate that whether a tweet is being
closely watched and worthy of commenting. Bi-
nary features (e.g., friends count, is-verified, is-
protected, etc) reveal that whether the author of a
tweet is trustworthy. (c) The Word-cluster feature
provides semantic information.
(3) The performance of Linguistic-informed and
Word vector features in three classifications is
mixed. The Linguistic-informed features do not
work in the 1-step, however they contribute to the
2-step classification and subtask B. By observa-
tion, the lemmatization and stem n-gram outper-
form the original n-gram probably because that
lemmatization and stem unify the form of word-
s, thus reducing the dimension of feature and un-
necessary noise. For Word vector, GloVe slightly
outperforms other word vectors.
(4) From the algorithm comparison experiments,
the ensemble models for 1-step of subtask A
and subtask B are superior to the models using
single algorithms, different learning algorithms

494



contribute differently to the classification perfor-
mance, that is why we conduct majority vote to en-
semble those effective learning algorithms. How-
ever, we directly use the LR algorithm in 2-step on
account of its best performance.

3.4 System Configuration

Based on the above experimental results, we con-
structed our submissions as follows: For subtask
A, we employed an ensemble model incorporat-
ing LR, SVM, DT, and ADB for 1-step classifica-
tion, while used LR directly for 2-step classifica-
tion. For subtask B, we also adopted an ensemble
model with LR, SVM, RF, GDB to predict label-
s of rumoured tweets, and probabilities of label-
s returned as confidence values. The parameters
of every algorithms are listed as follows: LR with
c= 1, SVM with kernel=linear, c= 0.1, RF with
n estimators= 10, ADB with n estimators= 100,
GDB with n estimators= 100, and DT with de-
fault parameters.

3.5 Results on test data

Tabel 3 shows the officially-released results of our
models and top-ranked teams. We ranked the third
for both subtasks in terms of accuracy, the second
for subtask B on the RMSE evaluation (a higher
accuracy is better, while a lower RMSE is bet-
ter). The predict results of test data are inferior
to the results of dev set, especially for subtask B.
we partly blame it for two reasons: (1) The addi-
tion of two breaking news (i.e., hillaryshealth and
save-marinajoyce). The feature set used in subtask
B can not capture unseen words in new topics, so
the model may have a limited generalizability. (2)
The test set is too small (only 28 samples).

Subtask System Accuracy(%) RMSE

Subtask A

ECNU 77.8(3) -
Turing 78.4(1) -

Uwaterloo 78.0(2) -

Subtask B

ECNU 46.4(3) 0.736(2)
NileTMRG 53.6(1) 0.672(1)

IKM 53.6(2) 0.763(3)

Table 3: Performance of our models and top-
ranked teams on both two subtasks. The numbers
in the brackets are the official rankings.

4 Conclusion

For both subtasks, we adopted supervised machine
learning methods incorporating rich features. We

adopted a two-step classifier to address subtask A
to solve the imbalance of training data, and a sim-
plified three-classification for subtask B. We orig-
inally thought that features with good generaliza-
tion performance, such as Linguistic-informed and
Word vector features would perform well in both
subtasks, but in fact that was not the case. On the
contrary, good performance can be achieved with
several features like Tweet domain and Metada-
ta features closely related with the tweets. From
the final results in test data, in the future work, we
need to build a topic independent model to achieve
better generalizability.

Acknowledgments

This research is supported by grants from Science
and Technology Commission of Shanghai Munici-
pality (14DZ2260800 and 15ZR1410700), Shang-
hai Collaborative Innovation Center of Trustwor-
thy Software for Internet of Things (ZF1213) and
NSFC (61402175).

References
Leon Derczynski, Kalina Bontcheva, Maria Liaka-

ta, Rob Procter, Geraldine Wong Sak Hoi, and
Arkaitz Zubiaga. 2017. SemEval-2017 Task 8: Ru-
mourEval: Determining rumour veracity and sup-
port for rumours. In Proceedings of SemEval. ACL.

Man Lan, Guoshun Wu, Chunyun Xiao, Yuanbin Wu,
and Ju Wu. 2016a. Building mutually benefi-
cial relationships between question retrieval and an-
swer ranking to improve performance of communi-
ty question answering. In Neural Networks (IJCN-
N), 2016 International Joint Conference on. IEEE,
pages 832–839.

Man Lan, Zhihua Zhang, Yue Lu, and Ju Wu. 2016b.
Three convolutional neural network-based models
for learning sentiment word vectors towards senti-
ment analysis. In Neural Networks (IJCNN), 2016
International Joint Conference on. IEEE, pages
3172–3179.

Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Cor-
rado, and Jeff Dean. 2013. Distributed representa-
tions of words and phrases and their compositional-
ity. In Advances in neural information processing
systems. pages 3111–3119.

Jeffrey Pennington, Richard Socher, and Christopher D
Manning. 2014. Glove: Global vectors for word
representation. In EMNLP. volume 14, pages 1532–
1543.

Fabrizio Sebastiani. 2002. Machine learning in auto-
mated text categorization. ACM computing surveys
(CSUR) 34(1):1–47.

495



Richard Socher, John Bauer, Christopher D Manning,
and Andrew Y Ng. 2013. Parsing with composition-
al vector grammars. In ACL (1). pages 455–465.

Duyu Tang, Furu Wei, Nan Yang, Ming Zhou, Ting Li-
u, and Bing Qin. 2014. Learning sentiment-specific
word embedding for twitter sentiment classification.
In ACL (1). pages 1555–1565.

Arkaitz Zubiaga, Maria Liakata, Rob Procter, Geral-
dine Wong Sak Hoi, and Peter Tolmie. 2016.
Analysing how people orient to and spread rumours
in social media by looking at conversational threads.
PloS one 11(3):e0150989.

496


