



















































Bootstrapped Text-level Named Entity Recognition for Literature


Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, pages 344–350,
Berlin, Germany, August 7-12, 2016. c©2016 Association for Computational Linguistics

Bootstrapped Text-level Named Entity Recognition for Literature

Julian Brooke Timothy Baldwin
Computing and Information Systems

The University of Melbourne
jabrooke@unimelb.edu.au

tb@ldwin.net

Adam Hammond
English and Comparative Literature

San Diego State University
ahammond@mail.sdsu.edu

Abstract

We present a named entity recogni-
tion (NER) system for tagging fiction:
LitNER. Relative to more traditional ap-
proaches, LitNER has two important
properties: (1) it makes no use of hand-
tagged data or gazetteers, instead it boot-
straps a model from term clusters; and (2)
it leverages multiple instances of the same
name in a text. Our experiments show it to
substantially outperform off-the-shelf su-
pervised NER systems.

1 Introduction

Much of the work on applying NLP to the anal-
ysis of literature has focused on literary fig-
ures/characters in the text, e.g. in the context of so-
cial network analysis (Elson et al., 2010; Agarwal
et al., 2013; Ardanuy and Sporleder, 2015) or anal-
ysis of characterization (Bamman et al., 2014).
Named entity recognition (NER) of person names
is generally the first step in identifying characters;
locations are also a prevalent NE type, and can be
useful when tracking different plot threads (Wal-
lace, 2012), or trends in the settings of fiction.

There are not, to our knowledge, any NER
systems that are specifically targeted at litera-
ture, and most related work has used Stanford
CoreNLP as an off-the-shelf solution (Bamman
et al., 2014; Vala et al., 2015). In this paper, we
show that it is possible to take advantage of the
properties of fiction texts, in particular the repeti-
tion of names, to build a high-performing 3-class
NER system which distinguishes people and lo-
cations from other capitalized words and phrases.
Notably, we do this without any hand-labelled

data whatsoever, bootstrapping a text-level context
classifier from a low-dimensional Brown cluster-
ing of the Project Gutenberg corpus.

2 Related work

The standard approach to NER is to treat it as a
supervised sequential classification problem, typ-
ically using conditional random fields or similar
models, based on local context features as well
as properties of the token itself. Relevant to the
present work is the fact that, despite there being
some work on enforcing tag consistency across
multiple instances of the same token (Finkel et al.,
2005) and the use of non-local features (Ratinov
and Roth, 2009) to improve supervised sequential
models, the consensus seems to be that this non-
local information has a relatively modest effect on
performance in standard datasets, and as a result
off-the-shelf NER systems in practice treat each
sentence as a separate document, with multiple in-
stances of the same token in different sentences
viewed as entirely independent classification prob-
lems. We also note that although supervised NER
is the norm, there is a smaller body of work in
semi-supervised and unsupervised approaches to
NER and semantic lexicon induction, for instance
pattern bootstrapping (Nadeau et al., 2006; Thelen
and Riloff, 2002; McIntosh et al., 2011) as well as
generative approaches (Elsner et al., 2009).

In the context of literature, the most closely re-
lated task is character identification (Vala et al.,
2015), which is itself an intermediate task for char-
acter speech identification (He et al., 2013), analy-
sis of characterization (Bamman et al., 2014), and
analysis of social networks (Elson et al., 2010;
Agarwal et al., 2013; Ardanuy and Sporleder,
2015). In addition to NER, character identifica-

344



tion also involves clustering multiple aliases of the
same character, and discarding person names that
don’t correspond to characters. Vala et al. (2015)
identify some of the failures of off-the-shelf NER
with regards to character identification, and at-
tempt to fix them; their efforts are focused, how-
ever, on characters that are referred to by descrip-
tion rather than names or aliases.

3 Method

3.1 Corpus preparation and segmentation

The corpus we use for building and testing our
NER system is the 2010 image of the (US)
Project Gutenberg corpus,1 a reasonably compre-
hensive collection of out-of-copyright English lit-
erary texts, to our knowledge the largest that is
publicly available in a machine-readable, full-text
format. We access the texts via the GutenTag
tool (Brooke et al., 2015), which allows both filter-
ing of texts by genre as well as within-text filtering
to remove Project Gutenberg copyright informa-
tion, front and back matter (e.g. table of contents),
and headers. We focus here only on fiction texts
(i.e. novels and short stories); other kinds of liter-
ature (e.g. plays) are rare in the corpus and have
very different properties in terms of the distribu-
tion of names. The final corpus size is 10844 texts.

GutenTag also provides an initial segmenta-
tion of tokens into potential names, using a sim-
ple rule-based system which segments contiguous
capitalized words, potentially with common inter-
vening function words like of as well as leading
the (e.g. the King of Westeros). It largely (but
not entirely) overcomes the problem of sentence-
initial capitalization in English by generalizing
over an entire text; as long as a capitalized word
or phrase appears in a non-sentence initial po-
sition at least once in a text, it will be tagged
in the sentence-initial position as well. To im-
prove precision, the name tagger in the version of
GutenTag used for this paper (0.1.3) has lower
bounds on token count (at least 10) and an upper
bound on the length of names (no longer than 3
words). For this work, however, we remove those
restrictions to maximize recall. Though not our
primary concern, we return to evaluate the quality
of the initial segmentation in Section 5.

1http://www.gutenberg.org

3.2 Brown clustering

The next step is to induce Brown clusters (Brown
et al., 1992) over the pre-segmented corpus (in-
cluding potential names), using the tool of Liang
(2005). Briefly, Brown clusters are formed us-
ing an agglomerative hierarchical cluster of terms
based on their immediate context, placing terms
into categories to maximize the probability of con-
secutive terms over the entire corpus. Note that
using information from Brown clusters is a well
established technique in NER, but more typically
as features within a supervised framework (Miller
et al., 2004; Liang, 2005; Ritter et al., 2011); we
are unaware of any work using them directly as
a source of bootstrapped training examples. We
used default settings except for the number of clus-
ters (c): 50. The rationale for such a small cluster
size—the default is 1000, and NER systems which
use Brown clusters as features do better with even
more (Derczynski et al., 2015)—is that we want
to have clusters that correspond to major noun cat-
egories (e.g. PERSON and LOCATION), which we
consider the next most fundamental division be-
yond part-of-speech; 50 was selected because it is
roughly comparable to the size of the Penn Tree-
bank tagset (Marcus et al., 1993). We did not tune
this number, except to observe that larger num-
bers (e.g. 100 or 200) resulted in increasingly frag-
mented clusters for our entities of interest.

To automatically extract a seed list of peo-
ple and locations, we ranked the clusters by the
total (token) count of names (as identified by
GutenTag), and took the first cluster to be PER-
SON, and the second to be LOCATION; all other
clusters are considered OTHER, our third, catch-
all category. Alternatively, we could have set c
higher and manually grouped the clusters based on
the common words in the clusters, adding a thin
layer of supervision to the process; with a low c,
however, this was unnecessary since the composi-
tion and ranking of the clusters conformed exactly
to our expectations. The top-5 clusters by token
count of names are given in Table 1.2 Note the
presence of the multiword name New York in the
second cluster, as a result of the segmentation.

The most common words in the first two clus-
ters are mostly what we would expect, though
there is a bit of noise, e.g. Him included as a
place. The other clusters are messier, but still in-

2Note that each cluster generally includes large numbers
of non-names, which we ignore.

345



Count Top-10 name types
17.2M Tom, Jack, Dick, Mary, John

Harry, Peter, Frank, George, Jim
2.5M London, England, Paris, New York, France

Him, America, Rome, Europe, Boston
1.8M English, French, Lord, Indian, American

German, Christian, Indians, King, Italian
0.5M Sir, Doctor, Colonel, Madam, Major

Professor, Dieu, Squire, Heavens, Sire
0.5M Christmas, Spanish, British, Irish, Roman

Latin, Chinese, European, Dutch, Scotch

Table 1: Top-5 Brown clusters derived from PG
corpus, by token count of names

terpretable: e.g. Cluster 4 is a collection of terms
of address. Note that although we do not con-
sider an term like Doctor to be a person name,
Doctor Smith or the Doctor would be; in many
literary contexts characters are referred to by an
alias, and failure to deal properly with these sit-
uations is a significant problem with off-the-shelf
NER systems in literature (Vala et al., 2015). In
any case, Brown clustering works fairly well for
common names, but for rarer ones, the cluster-
ing is haphazard. Fiction, though, has many rare
names and locations, since authors will often in-
vent them. Another problem with Brown clus-
tering is that ignores possible sense distinctions:
for instance, Florence is both a city and a person
name. To avoid confusion, authors will generally
preserve one-sense-per-document, but this is not
true at the corpus level.

3.3 Text-level context classifier

The central element of our NER system is a text-
level classifier of names based on context. By
text-level, we mean that it assumes one-sense-per-
document, classifying a name for an entire doc-
ument, based on all instances of the name in the
document (Gale et al., 1992). It is trained on
the (text-level) “instances” of relatively common
names (appearing more than 100 times in the cor-
pus) from the 3 NE label types derived based on
the Brown clustering. That is, to build a training
set, we pass through the corpus and each time we
come across a common name in a particular doc-
ument, we build a feature vector corresponding to
all the contexts in that document, with the label
taken from the clustering. Our rationale here is
that the challenging part of NER in literature is
names that appear only in one text; by limiting
our context for common words to a single text,

we simulate the task for rarer words. Mary is a
common name, and may be a major character in
one text, but a minor one in another; hence, we
build a classifier that deals with both context-rich
and context-poor situations. The noisy training set
thus constructed has about 1 million examples.

Our feature set consists of filtered word fea-
tures in a 2-word window (w−2 w−1 w0 w+1 w+2)
around the token occurrences w0 of a target type
in a given text, made up of position-indexed uni-
grams (w−2, w−1, w+1 and w+2) and bigrams
(w−2w−1, w+1w+2 and w−1w+1), excluding uni-
grams when a subsuming bigram feature matched
(e.g. if we match trust in, we do not add trust and
in). For this we used the name-segmented corpus,
and when one of the words in the context was also
a name, we take the category from the Brown clus-
tering as the word (so w2 for London in from Lon-
don to New York is LOCATION, not New). Across
multiple tokens of the same type, we count the
same context only once, creating a binary feature
vector which was normalized by dividing by the
count of all non-zero entries once all contexts were
collected. To be included as features, the n-grams
had to occur with ≥ 10 different w0 target word
types. Note that given our bootstrapping setup,
the word type itself cannot be used directly as a
feature.

For classification, we use logistic regression
from scikit-learn (Pedregosa et al., 2011) trained
with SGD using L2 regularization (C = 1).3 The
only non-standard setting that we use is the “bal-
anced” option, which weights classes by the in-
verse of their count in the training set, countering
the preference for the majority class; we do this
because our bootstrapped distribution is an unre-
liable reflection of the true distribution, and also
because it makes it a fairer comparison to off-the-
shelf models with no access to this distribution.

3.4 Improved phrase classification
Relative to (true) supervised models, our boot-
strapped model suffers from being able to use only
context, and not the identity of the name itself.
In the case of names which are phrases, this is
troubling because there are many generalizations
to be made; for instance names ending with City
are locations. Our final model addresses this fail-
ing somewhat by using more information from our

3Using cross-validation over the training data, we tested
other solvers, L1 regularization, and settings of the C param-
eter, but saw no appreciable improvement in performance.

346



Brown clustering: from each of the initial and fi-
nal words across all names, we extract a set of
words Ws that appear at least ten times in position
s ∈ S, S = {initial, final} across all phrases.
Let c(w, t, s) be the the number of times a word
w ∈ Ws appears in the corpus at position s in
phrases which were Brown clustered into the en-
tity type t ∈ T , and p(t|r) be the original prob-
ability of phrase r being type t as determined by
the logistic regression classifier. For our two ho-
mogenous entity types (PERSON and LOCATION),
we calculate a new score p′:

p′(t|r) = p(t|r) +
∑
s∈S

( c(rs, t, s)∑
t′∈T c(rs, t′, s)

−
∑

w′∈Ws
c(w′,t,s)∑

t′∈T c(w′,t′,s)

|Ws|
)

(1)

The first term in the outermost summation in
Equation 1 is the proportion of occurrences of the
given expression in position s which correspond
to type t. To avoid applying too much weight to
the homogeneous classes, the second term in the
summation subtracts the average number of occur-
rences in the given position for all words in Ws.
As such, the total effect on the score can be neg-
ative. Note that if rs /∈ Ws, no modification is
made, and for the OTHER type p′(t|r) = p(t|r).
Once we have calculated p′(t|r) for each class, we
choose the t with the highest p′(t|r).

4 Evaluation

Our interest is in a general NER system for liter-
ature. Though there are a few novels which have
been tagged for characters (Vala et al., 2015), we
wanted to test our system relative to a much wider
range of fiction. To this end, we randomly sampled
texts, sentences, and then names within those sen-
tences from our name-segmented Project Guten-
berg corpus to produce a set of 1000 examples.
These were tagged by a single annotator, an En-
glish native speaker with a PhD in English Liter-
ature. The annotator was presented with the sen-
tence and the pre-segmented name of interest, and
asked (via written instructions) to categorize the
indicated name into PERSON, LOCATION, OTHER,
UNCERTAIN due to ambiguity, or segmentation er-
ror. We ran a separate two-annotator agreement
study over 200 examples which yielded a Cohen’s
Kappa of 0.84, suggesting high enough reliability
that a single annotator was sufficient. The class

System Acc FM
All PERSON baseline .696 —
OpenNLP .435 .572
LingPipe .528 .536
Stanford CoreNLP .786 .751
Brown clusters .803 .672
LitNER sentence +phrase .757 .671
LitNER text −phrase .855 .771
LitNER text +phrase .871 .792

Table 2: Performance of NER systems

distribution for the main annotation was 66.9%
PERSON, 10.2% LOCATION, 19.0% OTHER, 2.4%
UNCERTAIN, and 1.5% segmentation error. For
the main evaluation, we excluded both UNCER-
TAIN examples and segmentation errors, but had
our annotator provide correct segmentation for the
15 segmentation errors and carried out a separate
comparison on these.

We compare our system to a selection
of publicly available, off-the-shelf NER sys-
tems: OpenNLP,4 LingPipe,5 and Stanford
CoreNLP (Finkel et al., 2005), as well as the
initial Brown clustering. OpenNLP allowed us
to classify only PERSON and LOCATION, but for
Stanford CoreNLP and LingPipe we used
the existing 3-entity systems, with the ORGANI-
ZATION tag collapsed into OTHER (as it was in our
guidelines; instances of ORGANIZATION are rare
in literature). Since the exact segmentation guide-
lines likely varied across these systems—in par-
ticular, we found that Stanford CoreNLP of-
ten left off the title in names such as Mr. Smith—
and we didn’t want to focus on these issues, we
did not require exact matches of our name seg-
mentation; instead, we consider the entire name as
PERSON or LOCATION if any of the tokens were
tagged as such (names with both tags were con-
sidered OTHER). For our system (LitNER), we
test a version where only the immediate sentence
context is used (“sentence”), and versions based
on text context (“text”) with or without our phrase
improvement (“±phrase”).

We evaluate using two standard metrics: accu-
racy (“Acc”), and macroaveraged F-score (“FM”).
5 Results

The results in Table 2 show that our system eas-
ily bests the off-the-shelf systems when it is given

4https://opennlp.apache.org/
5http://alias-i.com/lingpipe

347



the contextual information from the entire text; the
difference is more stark for accuracy (+0.085 ab-
solute), though consistent for FM (+0.041 abso-
lute). Stanford CoreNLP is the only compet-
itive off-the-shelf system—the other two are far
too conservative when encountering names they
haven’t seen before. LitNER is also clearly better
than the Brown clusters it was trained on, partic-
ularly for FM (+0.120 absolute). With regards to
different options for LitNER, we see a major ben-
efit from considering all occurrences of the name
in the texts rather than just the one we are testing
on (Section 3.3), and a more modest benefit from
using the information on parts of phrases taken
from the Brown clustering (Section 3.4).

For the segmentation errors, we compared
our corrected segmentations with the segmen-
tation provided by the CRF-based Stanford
CoreNLP system, our best competitor. Only 2 of
the 15 were segmented correctly by Stanford
CoreNLP. This potential 0.002 improvement is
tiny compared to the 0.085 difference in accuracy
between the two systems.

6 Discussion

Aspects of the method presented here could the-
oretically be applied to NER in other genres and
other languages, but one important point we wish
to make is that our approach clearly takes advan-
tage of specific properties of (English) literature.
The initial rule-based segmentation, for instance,
depends on reliable capitalization of names, which
is often not present in social media, or in most non-
European languages. We have found more subtle
genre effects as well: for comparison, we applied
the preliminary steps of our approach to another
corpus of published texts which is of compara-
ble (token) size to the Project Gutenberg corpus,
namely the Gigaword newswire corpus (Graff and
Cieri, 2003), and noted degraded performance for
both segmentation and Brown clustering. With re-
spect to the former, the obvious issue is consid-
erably more complex proper nouns phrases such
as governmental organizations and related titles.
For the latter, there were several clusters in the top
10 (including the first one) which corresponded to
LOCATION, while the first (fairly) clean PERSON
cluster was the 15th largest; in general, individual
people, organizations, and other groupings of peo-
ple (e.g. by country of origin) were not well dis-
tinguished by Brown clustering in the Gigaword

corpus, at least not with the same low number of
clusters that worked well in the Project Gutenberg
corpus.

Also less than promising is the potential for
using text-level classification in other genres:
whereas the average number of token occurrences
of distinct name types within a single text in the
Project Gutenberg corpus is 5.9, this number is
just 1.6 for the much-shorter texts of the Giga-
word corpus. Except in cases where it is possible
to collapse texts into appropriately-sized groups
where the use of a particular name is likely to be
both common and consistent—an example might
be a collection of texts written by a single au-
thor, which in social media such as Twitter seems
to obey the classic one-sense-per-discourse rule
(Gella et al., 2014)—it’s not clear that this ap-
proach can be applied successfully in cases where
texts are relatively short, which is a far more com-
mon situation. We also note that relying primarily
on contextual classification while eschewing re-
sources such as gazetteers makes much less sense
outside the context of fiction; we would expect rel-
atively few fictitious entities in most genres.
LitNER tags names into only two main classes,

PERSON and LOCATION, plus a catch-all OTHER.
This coarse-grained tag set reflects not only the
practical limitations of the method, but also where
we believe automatic methods have potential to
provide useful information for literary analysis.
The other clusters in Table 1 reflect word cate-
gories which are relatively closed-class and much
less central to the fictional narratives as character
and setting; we don’t see a compelling case for
tagging them. When these and non-entities are ex-
cluded from OTHER, what remains is eclectic, in-
cluding names referring to small groups of people
(e.g. families), animals, gods, ships, and titles of
other works of literature.

7 Conclusion

In this paper, we have presented LitNER, an NER
system targeted specifically at fiction. Our results
show that a simple classifier, trained only with
noisy examples derived in an unsupervised fash-
ion, can easily beat a general-purpose supervised
system, provided it has access to the full context
of the text. Finally, we note that the NER tagging
provided by LitNER has been integrated into the
GutenTag tool (as of version 0.1.4).6

6See http://www.projectgutentag.org

348



References
Apoorv Agarwal, Anup Kotalwa, and Owen Rambow.

2013. Automatic extraction of social networks from
literary text: A case study on Alice in Wonderland.
In The Proceedings of the 6th International Joint
Conference on Natural Language Processign (IJC-
NLP ’13).

Mariona Coll Ardanuy and Caroline Sporleder. 2015.
Clustering of novels represented as social networks.
Linguistic Issues in Language Technology, 12(4).

David Bamman, Ted Underwood, and Noah A. Smith.
2014. A Bayesian mixed effects model of literary
character. In Proceedings of the 52st Annual Meet-
ing of the Association for Computational Linguistics
(ACL ’14).

Julian Brooke, Adam Hammond, and Graeme Hirst.
2015. GutenTag: An NLP-driven tool for digital hu-
manities research in the Project Gutenberg corpus.
In Proceedings of the 4nd Workshop on Computa-
tional Literature for Literature (CLFL ’15).

Peter F. Brown, Peter V. deSouza, Robert L. Mer-
cer, Vincent J. Della Pietra, and Jenifer C. Lai.
1992. Class-based n-gram models of natural lan-
guage. Computational Linguistics, 18(4):467–479.

Leon Derczynski, Sean Chester, and Kenneth S. Bgh.
2015. Tune your Brown clustering, please. In Pro-
ceedings of Recent Advances in Natural Language
Processing (RANLP 15).

Micha Elsner, Eugene Charniak, and Mark Johnson.
2009. Structured generative models for unsuper-
vised named-entity clustering. In Proceedings of
the 2009 Annual Conference of the North American
Chapter of the Association for Computational Lin-
guistics: Human Language Technologies (NAACL
’09).

David K. Elson, Nicholas Dames, and Kathleen R.
McKeown. 2010. Extracting social networks from
literary fiction. In Proceedings of the 48th Annual
Meeting of the Association for Computational Lin-
guistics (ACL ’10).

Jenny Rose Finkel, Trond Grenager, and Christopher
Manning. 2005. Incorporating non-local informa-
tion into information extraction systems by gibbs
sampling. In Proceedings of the 43rd Annual Meet-
ing on Association for Computational Linguistics
(ACL ’05).

William A. Gale, Kenneth W. Church, and David
Yarowsky. 1992. One sense per discourse. In Pro-
ceedings of the 4th DARPA Speech and Natural Lan-
guage Workshop.

Spandana Gella, Paul Cook, and Timothy Baldwin.
2014. One sense per tweeter ... and other lexical
semantic tales of twitter. In Proceedings of the 14th
Conference of the European Chapter of the Associa-
tion for Computational Linguistics.

David Graff and Christopher Cieri. 2003. English Gi-
gaword. Linguistic Data Consortium.

Hua He, Denilson Barbosa, and Grzegorz Kondrak.
2013. Identification of speakers in novels. In Pro-
ceedings of the 51st Annual Meeting of the Associa-
tion for Computational Linguistics (ACL ’13).

Percy Liang. 2005. Semi-supervised learning for nat-
ural language. Master’s thesis, MIT.

Mitchell P. Marcus, Beatrice Santorini, and Mary Ann
Marcinkiewicz. 1993. Building a large annotated
corpus of English: the Penn treebank. Computa-
tional Linguistics, 19(2):313–330.

Tara McIntosh, Lars Yencken, James R. Curran, and
Timothy Baldwin. 2011. Relation guided bootstrap-
ping of semantic lexicons. In Proceedings of the
49th Annual Meeting of the Association for Com-
putational Linguistics: Human Language Technolo-
gies (ACL HLT 2011).

Scott. Miller, Jethran. Guinness, and Alex Zamanian.
2004. Name tagging with word clusters and discrim-
inative training. In Proceedings of the 2004 Confer-
ence of the North American Chapter of the Associ-
ation for Computational Linguistics: Human Lan-
guage Technologies (NAACL HLT ’13).

David Nadeau, Peter D. Turney, and Stan Matwin.
2006. Unsupervised named-entity recognition:
Generating gazetteers and resolving ambiguity. In
Proceedings of the 19th International Conference on
Advances in Artificial Intelligence: Canadian Soci-
ety for Computational Studies of Intelligence, AI’06.

F. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel,
B. Thirion, O. Grisel, M. Blondel, P. Pretten-
hofer, R. Weiss, V. Dubourg, J. Vanderplas, A. Pas-
sos, D. Cournapeau, M. Brucher, M. Perrot, and
E. Duchesnay. 2011. Scikit-learn: Machine learn-
ing in Python. Journal of Machine Learning Re-
search, 12:2825–2830.

Lev Ratinov and Dan Roth. 2009. Design chal-
lenges and misconceptions in named entity recog-
nition. In Proceedings of the Thirteenth Confer-
ence on Computational Natural Language Learning
(CoNLL ’09).

Alan Ritter, Sam Clark, Mausam, and Oren Etzioni.
2011. Named entity recognition in tweets: An ex-
perimental study. In Proceedings of the 2011 Con-
ference on Empirical Methods in Natural Language
Processing (EMNLP 2011).

Michael Thelen and Ellen Riloff. 2002. A bootstrap-
ping method for learning semantic lexicons using
extraction pattern contexts. In Proceedings of the
2002 Conference on Empirical Methods in Natural
Language Processing (EMNLP 2002).

Hardik Vala, David Jurgens, Andrew Piper, and Derek
Ruths. 2015. Mr. Bennet, his coachman, and the
Archbishop walk into a bar but only one of them gets

349



recognized: On the difficulty of detecting characters
in literary texts. In Proceedings of the 2015 Con-
ference on Empirical Methods in Natural Language
Processing (EMNLP ’15).

Byron C. Wallace. 2012. Multiple narrative disentan-
glement: Unraveling Infinite Jest. In Proceedings of
the 2012 Conference of the North American Chap-
ter of the Association for Computational Linguistics:
Human Language Technologies (NAACL-HLT ’12).

350


