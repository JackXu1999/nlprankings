



















































Decomposing TAG Algorithms Using Simple Algebraizations


Proceedings of the 11th International Workshop on Tree Adjoining Grammars and Related Formalisms (TAG+11), pages 135–143,
Paris, September 2012.

Decomposing TAG Algorithms Using Simple Algebraizations

Alexander Koller
Dept. of Linguistics

University of Potsdam, Germany
koller@ling.uni-potsdam.de

Marco Kuhlmann
Dept. of Linguistics and Philology

Uppsala University, Sweden
marco.kuhlmann@lingfil.uu.se

Abstract

We review a number of different ‘algebraic’
perspectives on TAG and STAG in the frame-
work of interpreted regular tree grammars
(IRTGs). We then use this framework to derive
a new parsing algorithm for TAGs, based on
two algebras that describe strings and derived
trees. Our algorithm is extremely modular, and
can easily be adapted to the synchronous case.

1 Introduction

Much of the early and recent literature on tree-ad-
joining grammars (TAG) is concerned with work-
ing out the formal relationships between TAG and
other grammar formalisms. A common approach in
this line of research has been to conceive the way in
which TAG generates a string or a derived tree from
a grammar as a two-step process: first a derivation
tree is generated, then this derivation tree is mapped
into a term over some algebra and evaluated there.
Under this view, one can take different perspectives
on how the labour of generating a string or derived
tree should be divided between the mapping process
and the algebra. In a way that we will make pre-
cise, linear context-free rewriting systems (LCFRSs,
Weir (1988)) push much of the work into the algebra;
Shieber (2006)’s analysis of synchronous TAG as bi-
morphisms puts the burden mostly on the mapping
procedure; and a line of research using context-free
tree languages (CFTLs), of which Maletti (2010) is a
recent representative, strikes a balance between the
other two approaches.

This research has done much to clarify the formal
connections between TAG and other formalisms in

terms of generative capacity. It has not been par-
ticularly productive with respect to finding new al-
gorithms for parsing, training, and (in the synchron-
ous case) decoding. This is regrettable because stand-
ard parsing algorithms for TAG (Vijay-Shanker and
Joshi, 1985; Shieber et al., 1995) are complicated,
require relatively involved correctness proofs, and
are hard to teach. A similar criticism applies to pars-
ing algorithms for LCFRSs (Burden and Ljunglöf,
2005). So far, no new parsing algorithms have arisen
from Shieber’s work on bimorphisms, or from the
CFTL-based view. Indeed, Maletti (2010) leaves the
development of such algorithms as an open problem.

This paper makes two contributions. First, we
show how a number of the formal perspectives on
TAG mentioned above can be recast in a uniform
way as interpreted regular tree grammars (IRTGs,
Koller and Kuhlmann (2011)). IRTGs capture the
fundamental idea of generating strings, derived trees,
or other objects from a regular tree language, and
allow us to make the intuitive differences in how dif-
ferent perspectives divide the labour over the various
modules formally precise.

Second, we introduce two new algebras. One cap-
tures TAG string languages; the other describes TAG
derived tree languages. We show that both of these
algebras are regularly decomposable, which means
that the very modular algorithms that are available
for parsing, training, and decoding of IRTGs can be
applied to TAG. As an immediate consequence we
obtain algorithms for these problems (for both TAG
and STAG) that consist of small modules, each of
which is simpler to understand, prove correct, and
teach than a monolithic parser. As long as the gram-
mar is binary, this comes at no cost in asymptotic
parsing complexity.

135



The paper is structured as follows. In Section 2,
we introduce some formal foundations and review
IRTGs. In Section 3, we recast three existing per-
spectives on TAG as IRTGs. In Sections 4 and 5, we
present algebras for derived trees and strings in TAG;
we apply them to parsing in Section 6. Section 7
concludes and discusses future work.

2 Interpreted Regular Tree Grammars

We start by introducing some basic concepts.

2.1 Foundations
A signature is a finite set ˙ of function symbols f ,
each of which has been assigned a non-negative in-
teger called its rank. We write f jn to indicate that f
has rank n. For the following, let ˙ be a signature.

A tree over ˙ takes the form t D f .t1; : : : ; tn/,
where f jn 2 ˙ and t1; : : : ; tn are trees over ˙ . We
write T˙ for the set of all trees over ˙ . The nodes
of a tree can be identified by paths � 2 N� from the
root: The root has the address ", and the i th child of
the node with the address � has the address �i . We
write t .�/ for the symbol at path � in the tree t , and
t # � for the subtree of t at � .

A context over ˙ is a tree C 2 T˙[f�g which
contains a single leaf labeled with the hole �. C Œt
is the tree in T˙ which is obtained by replacing the
hole in C by some tree t 2 T˙ . We write C˙ for the
set of all contexts over ˙ .

A ˙-algebra A consists of a non-empty set A
called the domain and, for each function sym-
bol f jn 2 ˙ , a total function f A W An ! A, the
operation associated with f . Symbols of arity 0 are
also called constants. The trees in T˙ are called the
terms of this algebra. We can evaluate a term t 2 T˙
to an object JtKA 2 A by executing the operations:

Jf .t1; : : : ; tn/KA D f A.Jt1KA; : : : ; JtnKA/ :
One algebra that we will use throughout the paper
is the string algebra A� over some alphabet A. Its
elements are the strings over A; it has one binary
operation “�”, which concatenates its two arguments,
and one constant for each symbol in A which evalu-
ates to itself. Another important algebra is the term
algebra T˙ over some ranked signature ˙ . The do-
main of the term algebra is T˙ , and for each symbol
f jn 2 ˙ , we have f T˙ .t1; : : : ; tn/ D f .t1; : : : ; tn/,
i.e. every term evaluates to itself.

If some function f A is partial, then A is a partial
algebra; in this case there may be terms that do not
have a value.

A (tree) homomorphism is a total function
hW T˙ ! T� that expands symbols of ˙ into trees
over � while following the structure of the input
tree. Formally, h is specified by pairs .f; h.f //,
where f 2 ˙ is a symbol with some rank n, and
h.f / 2 T�[fx1;:::;xng is a term with variables. The
value of a term t D f .t1; : : : ; tn/ 2 T˙ under h is

h.f .t1; : : : ; tn// D h.f /Œh.t1/=x1; : : : ; h.tn/=xn :

2.2 Regular Tree Languages
Sets of trees can be specified by regular tree gram-
mars (RTGs) (Gécseg and Steinby, 1997; Comon
et al., 2008). Formally, an RTG is a construct
G D .N;˙;P; S/ where N and ˙ are signatures
of nonterminal and terminal symbols, S 2 N is a
start symbol, and P is a finite set of production rules
of the form A ! t , where A 2 N and t 2 TN[˙ .
Every nonterminal has rank 0. The language gener-
ated by G is the set L.G / � T˙ of all trees with only
terminal symbols that can be obtained by repeatedly
applying the production rules, starting from S .

The class of languages that can be generated by
regular tree grammars are called regular tree lan-
guages (RTLs). They share many of the closure prop-
erties that are familiar from regular string languages.
In particular, if L1 and L2 are regular and h is a
homomorphism, then L1 \ L2 and h�1.L/ are also
regular. If h is linear, then h.L1/ is regular as well.

2.3 Interpreted RTGs
Koller and Kuhlmann (2011) extend RTGs to inter-
preted regular tree grammars (IRTGs), which specify
relations between arbitrary algebras. In this paper,
we will focus on relations between strings and (de-
rived) trees, but IRTGs can be used also to describe
languages of and relations between other kinds of
algebras.

Formally, an IRTG G is a tuple .G ; I1; : : : ; Ik/
consisting of an RTG G and k � 1 interpretations
I1; : : : ; Ik . Each interpretation Ii is a pair .hi ;Ai /
of an algebra Ai and a tree homomorphism hi that
maps the trees generated by G to terms over Ai (see
Fig. 1). The language L.G/ is then defined as

L.G/ D f.Jh1.t/KA1 ; : : : ; Jhk.t/KAk / j t 2 L.G /g :

136



Figure 1: Our unified perspective on grammar formalisms: (a) ordinary grammar formalisms; (b) synchronous for-
malisms; (c) multiple “inputs” and “outputs”.

result of this can be (non-standardly) recorded as a
derivation tree, whose nodes are labeled by names
of production rules, and in which a rule application
a1 is the child of another a2 if a1 introduced a non-
terminal occurrence that was expanded by a2. In a
second step, we can transform this derivation tree
into a string by interpreting each rule application as
a string-concatenation operation.

While this picture seems complicated for context-
free grammars by themselves, the separation into
two different generative processes (first a derivation
tree, then the string from the derivation tree) is appli-
cable much more widely and, we argue, widely use-
ful. The general picture looks as follows. Consider
a regular tree grammar G over a signature Σ, an al-
gebra A with signature ∆, and a homomorphism h :
TΣ → T∆. If we apply h to any tree t ∈ L(G), we
obtain a term over A, which we can interpret as an
element of A. By collecting all such terms, we ob-
tain a language LA(G, h) = {�h(t)�A | t ∈ L(G)}
of elements of A. This perspective is illustrated in
Fig. 1a.

We can define an obvious membership problem:
Given some element a ∈ A, is a ∈ LA(G, h)? We
can also define a parsing problem: For every ele-
ment a ∈ LA(G, h), compute (some compact repre-
sentation of)

parsesA,G,h(a) = {t ∈ L(G) | �h(t)�A = a}.
We call the trees over Σ derivation trees, and the
trees in parses(a) the derivation trees of a.

In the case of context-free grammars, it is known
that the language of derivation trees is a regular tree
language (Comon et al., 2007). It is defined by an
RTG G over the signature of production rule names
of the context-free grammar G. For every produc-
tion rule r of the form A → ω1A1 . . . Anωn+1
(where A and all Ai are nonterminals, and the ωi are

S → NP VP
VP → V NP
NP → John
NP → Mary

V → loves

r1

r3 r2

r5 r4

Figure 2: A context-free grammar and one of its deriva-
tion trees.

possibly empty strings of terminals), G contains a
rule A → r(A1, . . . , An). We interpret such deriva-
tion trees into strings in the string algebra As over
some terminal alphabet T ; the elements of this alge-
bra are the strings in T ∗, and we have constants for
the elements of T and a binary string concatenation
operation ·. As a last step, we use a homorphism h to
map each rule into a term over As: for the above rule
r, we have h(r) = ω1 · x1 · . . . · xn · ωn+1. It can be
shown that under this construction, LAs(G, h) is ex-
actly L(G), the string language of the original gram-
mar.

For illustration, consider the context-free gram-
mar in Fig. 2a, and let’s say we want to parse the
sentence “John loves Mary”. The RTG for the gram-
mar contains rules such as S → r1(NP, V P );
it generates the derivation tree shown in Fig. 2b.
This tree can now be interpreted using a homomor-
phism h with h(r1) = x1 · x2, h(r3) = John,
etc. h maps the derivation tree in Fig. 2b to the
term (John · loves) · Mary over As, which eval-
uates to the string “John loves Mary”. This means
that it is a derivation tree of that string. In fact,
parses(“John loves Mary”) is the set that contains
only this derivation tree.

string transducer); these differences simply amount
to the appropriate selection of algebras and homo-
morphisms.

Plan of the paper. The paper is structured as fol-
lows. We will start by laying the formal foundations
in Section 2. We will then show how to combine bi-
morphisms with algebraic interpretations and illus-
trate the approach on several grammar formalisms in
Section 3. We will define generic algorithms in Sec-
tion 4. Section 7 discusses related work, and Sec-
tion 8 concludes.

2 Formal foundations

A signature is a finite set Σ of function symbols f ,
each of which has been assigned a non-negative in-
teger called its rank. Given a signature Σ, we can
define a (finite constructor) tree over Σ as a finite
tree whose nodes are labeled with symbols from Σ
such that a node with a label of rank n has exactly n
children. We write TΣ for the set of all trees over Σ.
Trees can be written as terms; f(t1, . . . , tn) stands
for the tree with root label f and subtrees t1, . . . , tn.
The nodes of a tree can be identified by the paths
π ∈ N∗ from the root to the node: The root has ad-
dress �, and the i-th child of the node below path π
has the address πi. We write t(π) for the symbol at
path π in the tree t.

A Σ-algebra A consists of a non-empty set A
called the domain and, for each symbol f ∈ Σ with
rank m, a total function fA : Am → A, called the
operation associated with f . We can evaluate a term
t ∈ TΣ to an object �t�A ∈ A by executing the op-
erations:

�σ(t1, . . . , tm)�A = fAσ (�t1�A, . . . , �tm�A) .

Sets of trees can be specified by regular tree
grammars (Gécseg and Steinby, 1997; Comon et
al., 2007). Formally, such a grammar is a structure
G = (N,Σ, P, S), where N is a signature of nonter-
minal symbols, all of which are taken to have rank 0,
Σ is a signature of terminal symbols, S ∈ N is a
distinguished start symbol, and P is a finite set of
productions of the form B → t, where B is a non-
terminal symbol, and t ∈ TN∪Σ. The productions
of a regular tree grammar are used as rewriting rules
on terms. More specifically, the derivation relation
of G is defined as follows. Let t1, t2 ∈ TN∪Σ be

terms. Then G derives t2 from t1 in one step, de-
noted by t1 ⇒G t2, if there exists a production of
the form B → t and t2 can be obtained by replacing
an occurrence of B in t1 by t. The language L(G)
generated by G is the set of all terms t ∈ TΣ that can
be derived, in zero or more steps, from the term S.

A (tree) homomorphism is a function h : TΣ →
T∆ which expands symbols of Σ into (possibly mul-
tiple) symbols of ∆ while following the structure
of the input tree. Formally, h is defined by a term
h(f) ∈ T∆∪{x1,...,xn} for each f ∈ Σ, where n is
the rank of f and the xi are variable symbols of rank
0. Given a term t ∈ TΣ, h(t) is defined recursively
by

h(f(t1, . . . , tn)) = h(f){h(t1)/x1, . . . , h(tn)/xn},
where {t�1/x1, . . . , t�n/xn} represents a substitution
that replaces all occurrences of xi with the respec-
tive t�i. A homomorphism is called linear if every
term h(f) contains each variable at most once.

Finally, a tree transducer is a device M for de-
scribing binary relations between trees; the first tree
in each pair is usually seen as the input and the sec-
ond as the output. They generalize string transduc-
ers to the tree case and are defined in more detail in
(Comon et al., 2007). A useful way of thinking of
a tree transducer is in terms of bimorphisms. A bi-
morphism is a triple B = (h1,G, h2) of an RTG G
and two homomorphisms h1, h2; it represents the bi-
nary relation {(h1(t), h2(t)) | t ∈ L(G)}. vielleicht
brauchen wir das hier gar nicht

3 Grammar formalisms based on tree
automata

We will now present a unified framework of
synchronous and non-synchronous grammar for-
malisms in terms of regular tree languages, tree ho-
momorphisms, and algebras. We will illustrate the
framework using ordinary context-free grammars
and synchronous tree-substitution grammars, but the
framework is much more general than this, and we
will hint at this at the end of the section.

3.1 Ordinary grammars
The process of generating a string from a context-
free grammar G can be seen as a two-step process.
In a first step, we generate a derivation of G by ex-
panding nonterminals using production rules. The

h

Figure 1: Our unified perspective on grammar formalisms: (a) ordinary grammar formalisms; (b) synchronous for-
malisms; (c) multiple “inputs” and “outputs”.

result of this can be (non-standardly) recorded as a
derivation tree, whose nodes are labeled by names
of production rules, and in which a rule application
a1 is the child of another a2 if a1 introduced a non-
terminal occurrence that was expanded by a2. In a
second step, we can transform this derivation tree
into a string by interpreting each rule application as
a string-concatenation operation.

While this picture seems complicated for context-
free grammars by themselves, the separation into
two different generative processes (first a derivation
tree, then the string from the derivation tree) is appli-
cable much more widely and, we argue, widely use-
ful. The general picture looks as follows. Consider
a regular tree grammar G over a signature Σ, an al-
gebra A with signature ∆, and a homomorphism h :
TΣ → T∆. If we apply h to any tree t ∈ L(G), we
obtain a term over A, which we can interpret as an
element of A. By collecting all such terms, we ob-
tain a language LA(G, h) = {�h(t)�A | t ∈ L(G)}
of elements of A. This perspective is illustrated in
Fig. 1a.

We can define an obvious membership problem:
Given some element a ∈ A, is a ∈ LA(G, h)? We
can also define a parsing problem: For every ele-
ment a ∈ LA(G, h), compute (some compact repre-
sentation of)

parsesA,G,h(a) = {t ∈ L(G) | �h(t)�A = a}.
We call the trees over Σ derivation trees, and the
trees in parses(a) the derivation trees of a.

In the case of context-free grammars, it is known
that the language of derivation trees is a regular tree
language (Comon et al., 2007). It is defined by an
RTG G over the signature of production rule names
of the context-free grammar G. For every produc-
tion rule r of the form A → ω1A1 . . . Anωn+1
(where A and all Ai are nonterminals, and the ωi are

S → NP VP
VP → V NP
NP → John
NP → Mary

V → loves

r1

r3 r2

r5 r4

Figure 2: A context-free grammar and one of its deriva-
tion trees.

possibly empty strings of terminals), G contains a
rule A → r(A1, . . . , An). We interpret such deriva-
tion trees into strings in the string algebra As over
some terminal alphabet T ; the elements of this alge-
bra are the strings in T ∗, and we have constants for
the elements of T and a binary string concatenation
operation ·. As a last step, we use a homorphism h to
map each rule into a term over As: for the above rule
r, we have h(r) = ω1 · x1 · . . . · xn · ωn+1. It can be
shown that under this construction, LAs(G, h) is ex-
actly L(G), the string language of the original gram-
mar.

For illustration, consider the context-free gram-
mar in Fig. 2a, and let’s say we want to parse the
sentence “John loves Mary”. The RTG for the gram-
mar contains rules such as S → r1(NP, V P );
it generates the derivation tree shown in Fig. 2b.
This tree can now be interpreted using a homomor-
phism h with h(r1) = x1 · x2, h(r3) = John,
etc. h maps the derivation tree in Fig. 2b to the
term (John · loves) · Mary over As, which eval-
uates to the string “John loves Mary”. This means
that it is a derivation tree of that string. In fact,
parses(“John loves Mary”) is the set that contains
only this derivation tree.

string transducer); these differences simply amount
to the appropriate selection of algebras and homo-
morphisms.

Plan of the paper. The paper is structured as fol-
lows. We will start by laying the formal foundations
in Section 2. We will then show how to combine bi-
morphisms with algebraic interpretations and illus-
trate the approach on several grammar formalisms in
Section 3. We will define generic algorithms in Sec-
tion 4. Section 7 discusses related work, and Sec-
tion 8 concludes.

2 Formal foundations

A signature is a finite set Σ of function symbols f ,
each of which has been assigned a non-negative in-
teger called its rank. Given a signature Σ, we can
define a (finite constructor) tree over Σ as a finite
tree whose nodes are labeled with symbols from Σ
such that a node with a label of rank n has exactly n
children. We write TΣ for the set of all trees over Σ.
Trees can be written as terms; f(t1, . . . , tn) stands
for the tree with root label f and subtrees t1, . . . , tn.
The nodes of a tree can be identified by the paths
π ∈ N∗ from the root to the node: The root has ad-
dress �, and the i-th child of the node below path π
has the address πi. We write t(π) for the symbol at
path π in the tree t.

A Σ-algebra A consists of a non-empty set A
called the domain and, for each symbol f ∈ Σ with
rank m, a total function fA : Am → A, called the
operation associated with f . We can evaluate a term
t ∈ TΣ to an object �t�A ∈ A by executing the op-
erations:

�σ(t1, . . . , tm)�A = fAσ (�t1�A, . . . , �tm�A) .

Sets of trees can be specified by regular tree
grammars (Gécseg and Steinby, 1997; Comon et
al., 2007). Formally, such a grammar is a structure
G = (N,Σ, P, S), where N is a signature of nonter-
minal symbols, all of which are taken to have rank 0,
Σ is a signature of terminal symbols, S ∈ N is a
distinguished start symbol, and P is a finite set of
productions of the form B → t, where B is a non-
terminal symbol, and t ∈ TN∪Σ. The productions
of a regular tree grammar are used as rewriting rules
on terms. More specifically, the derivation relation
of G is defined as follows. Let t1, t2 ∈ TN∪Σ be

terms. Then G derives t2 from t1 in one step, de-
noted by t1 ⇒G t2, if there exists a production of
the form B → t and t2 can be obtained by replacing
an occurrence of B in t1 by t. The language L(G)
generated by G is the set of all terms t ∈ TΣ that can
be derived, in zero or more steps, from the term S.

A (tree) homomorphism is a function h : TΣ →
T∆ which expands symbols of Σ into (possibly mul-
tiple) symbols of ∆ while following the structure
of the input tree. Formally, h is defined by a term
h(f) ∈ T∆∪{x1,...,xn} for each f ∈ Σ, where n is
the rank of f and the xi are variable symbols of rank
0. Given a term t ∈ TΣ, h(t) is defined recursively
by

h(f(t1, . . . , tn)) = h(f){h(t1)/x1, . . . , h(tn)/xn},
where {t�1/x1, . . . , t�n/xn} represents a substitution
that replaces all occurrences of xi with the respec-
tive t�i. A homomorphism is called linear if every
term h(f) contains each variable at most once.

Finally, a tree transducer is a device M for de-
scribing binary relations between trees; the first tree
in each pair is usually seen as the input and the sec-
ond as the output. They generalize string transduc-
ers to the tree case and are defined in more detail in
(Comon et al., 2007). A useful way of thinking of
a tree transducer is in terms of bimorphisms. A bi-
morphism is a triple B = (h1,G, h2) of an RTG G
and two homomorphisms h1, h2; it represents the bi-
nary relation {(h1(t), h2(t)) | t ∈ L(G)}. vielleicht
brauchen wir das hier gar nicht

3 Grammar formalisms based on tree
automata

We will now present a unified framework of
synchronous and non-synchronous grammar for-
malisms in terms of regular tree languages, tree ho-
momorphisms, and algebras. We will illustrate the
framework using ordinary context-free grammars
and synchronous tree-substitution grammars, but the
framework is much more general than this, and we
will hint at this at the end of the section.

3.1 Ordinary grammars
The process of generating a string from a context-
free grammar G can be seen as a two-step process.
In a first step, we generate a derivation of G by ex-
panding nonterminals using production rules. The

h

string transducer); these differences simply amount
to the appropriate selection of algebras and homo-
morphisms.

Plan of the paper. The paper is structured as fol-
lows. We will start by laying the formal foundations
in Section 2. We will then show how to combine bi-
morphisms with algebraic interpretations and illus-
trate the approach on several grammar formalisms in
Section 3. We will define generic algorithms in Sec-
tion 4. Section 7 discusses related work, and Sec-
tion 8 concludes.

2 Formal foundations

A signature is a finite set Σ of function symbols f ,
each of which has been assigned a non-negative in-
teger called its rank. Given a signature Σ, we can
define a (finite constructor) tree over Σ as a finite
tree whose nodes are labeled with symbols from Σ
such that a node with a label of rank n has exactly n
children. We write TΣ for the set of all trees over Σ.
Trees can be written as terms; f(t1, . . . , tn) stands
for the tree with root label f and subtrees t1, . . . , tn.
The nodes of a tree can be identified by the paths
π ∈ N∗ from the root to the node: The root has ad-
dress �, and the i-th child of the node below path π
has the address πi. We write t(π) for the symbol at
path π in the tree t.

A Σ-algebra A consists of a non-empty set A
called the domain and, for each symbol f ∈ Σ with
rank m, a total function fA : Am → A, called the
operation associated with f . We can evaluate a term
t ∈ TΣ to an object �t�A ∈ A by executing the op-
erations:

�σ(t1, . . . , tm)�A = fAσ (�t1�A, . . . , �tm�A) .

Sets of trees can be specified by regular tree
grammars (Gécseg and Steinby, 1997; Comon et
al., 2007). Formally, such a grammar is a structure
G = (N,Σ, P, S), where N is a signature of nonter-
minal symbols, all of which are taken to have rank 0,
Σ is a signature of terminal symbols, S ∈ N is a
distinguished start symbol, and P is a finite set of
productions of the form B → t, where B is a non-
terminal symbol, and t ∈ TN∪Σ. The productions
of a regular tree grammar are used as rewriting rules
on terms. More specifically, the derivation relation
of G is defined as follows. Let t1, t2 ∈ TN∪Σ be

terms. Then G derives t2 from t1 in one step, de-
noted by t1 ⇒G t2, if there exists a production of
the form B → t and t2 can be obtained by replacing
an occurrence of B in t1 by t. The language L(G)
generated by G is the set of all terms t ∈ TΣ that can
be derived, in zero or more steps, from the term S.

A (tree) homomorphism is a function h : TΣ →
T∆ which expands symbols of Σ into (possibly mul-
tiple) symbols of ∆ while following the structure
of the input tree. Formally, h is defined by a term
h(f) ∈ T∆∪{x1,...,xn} for each f ∈ Σ, where n is
the rank of f and the xi are variable symbols of rank
0. Given a term t ∈ TΣ, h(t) is defined recursively
by

h(f(t1, . . . , tn)) = h(f){h(t1)/x1, . . . , h(tn)/xn},
where {t�1/x1, . . . , t�n/xn} represents a substitution
that replaces all occurrences of xi with the respec-
tive t�i. A homomorphism is called linear if every
term h(f) contains each variable at most once.

Finally, a tree transducer is a device M for de-
scribing binary relations between trees; the first tree
in each pair is usually seen as the input and the sec-
ond as the output. They generalize string transduc-
ers to the tree case and are defined in more detail in
(Comon et al., 2007). A useful way of thinking of
a tree transducer is in terms of bimorphisms. A bi-
morphism is a triple B = (h1,G, h2) of an RTG G
and two homomorphisms h1, h2; it represents the bi-
nary relation {(h1(t), h2(t)) | t ∈ L(G)}. vielleicht
brauchen wir das hier gar nicht

3 Grammar formalisms based on tree
automata

We will now present a unified framework of
synchronous and non-synchronous grammar for-
malisms in terms of regular tree languages, tree ho-
momorphisms, and algebras. We will illustrate the
framework using ordinary context-free grammars
and synchronous tree-substitution grammars, but the
framework is much more general than this, and we
will hint at this at the end of the section.

3.1 Ordinary grammars
The process of generating a string from a context-
free grammar G can be seen as a two-step process.
In a first step, we generate a derivation of G by ex-
panding nonterminals using production rules. The

'h'

Figure 1: Our unified perspective on grammar formalisms: (a) ordinary grammar formalisms; (b) synchronous for-
malisms; (c) multiple “inputs” and “outputs”.

result of this can be (non-standardly) recorded as a
derivation tree, whose nodes are labeled by names
of production rules, and in which a rule application
a1 is the child of another a2 if a1 introduced a non-
terminal occurrence that was expanded by a2. In a
second step, we can transform this derivation tree
into a string by interpreting each rule application as
a string-concatenation operation.

While this picture seems complicated for context-
free grammars by themselves, the separation into
two different generative processes (first a derivation
tree, then the string from the derivation tree) is appli-
cable much more widely and, we argue, widely use-
ful. The general picture looks as follows. Consider
a regular tree grammar G over a signature Σ, an al-
gebra A with signature ∆, and a homomorphism h :
TΣ → T∆. If we apply h to any tree t ∈ L(G), we
obtain a term over A, which we can interpret as an
element of A. By collecting all such terms, we ob-
tain a language LA(G, h) = {�h(t)�A | t ∈ L(G)}
of elements of A. This perspective is illustrated in
Fig. 1a.

We can define an obvious membership problem:
Given some element a ∈ A, is a ∈ LA(G, h)? We
can also define a parsing problem: For every ele-
ment a ∈ LA(G, h), compute (some compact repre-
sentation of)

parsesA,G,h(a) = {t ∈ L(G) | �h(t)�A = a}.
We call the trees over Σ derivation trees, and the
trees in parses(a) the derivation trees of a.

In the case of context-free grammars, it is known
that the language of derivation trees is a regular tree
language (Comon et al., 2007). It is defined by an
RTG G over the signature of production rule names
of the context-free grammar G. For every produc-
tion rule r of the form A → ω1A1 . . . Anωn+1
(where A and all Ai are nonterminals, and the ωi are

S → NP VP
VP → V NP
NP → John
NP → Mary

V → loves

r1

r3 r2

r5 r4

Figure 2: A context-free grammar and one of its deriva-
tion trees.

possibly empty strings of terminals), G contains a
rule A → r(A1, . . . , An). We interpret such deriva-
tion trees into strings in the string algebra As over
some terminal alphabet T ; the elements of this alge-
bra are the strings in T ∗, and we have constants for
the elements of T and a binary string concatenation
operation ·. As a last step, we use a homorphism h to
map each rule into a term over As: for the above rule
r, we have h(r) = ω1 · x1 · . . . · xn · ωn+1. It can be
shown that under this construction, LAs(G, h) is ex-
actly L(G), the string language of the original gram-
mar.

For illustration, consider the context-free gram-
mar in Fig. 2a, and let’s say we want to parse the
sentence “John loves Mary”. The RTG for the gram-
mar contains rules such as S → r1(NP, V P );
it generates the derivation tree shown in Fig. 2b.
This tree can now be interpreted using a homomor-
phism h with h(r1) = x1 · x2, h(r3) = John,
etc. h maps the derivation tree in Fig. 2b to the
term (John · loves) · Mary over As, which eval-
uates to the string “John loves Mary”. This means
that it is a derivation tree of that string. In fact,
parses(“John loves Mary”) is the set that contains
only this derivation tree.

string transducer); these differences simply amount
to the appropriate selection of algebras and homo-
morphisms.

Plan of the paper. The paper is structured as fol-
lows. We will start by laying the formal foundations
in Section 2. We will then show how to combine bi-
morphisms with algebraic interpretations and illus-
trate the approach on several grammar formalisms in
Section 3. We will define generic algorithms in Sec-
tion 4. Section 7 discusses related work, and Sec-
tion 8 concludes.

2 Formal foundations

A signature is a finite set Σ of function symbols f ,
each of which has been assigned a non-negative in-
teger called its rank. Given a signature Σ, we can
define a (finite constructor) tree over Σ as a finite
tree whose nodes are labeled with symbols from Σ
such that a node with a label of rank n has exactly n
children. We write TΣ for the set of all trees over Σ.
Trees can be written as terms; f(t1, . . . , tn) stands
for the tree with root label f and subtrees t1, . . . , tn.
The nodes of a tree can be identified by the paths
π ∈ N∗ from the root to the node: The root has ad-
dress �, and the i-th child of the node below path π
has the address πi. We write t(π) for the symbol at
path π in the tree t.

A Σ-algebra A consists of a non-empty set A
called the domain and, for each symbol f ∈ Σ with
rank m, a total function fA : Am → A, called the
operation associated with f . We can evaluate a term
t ∈ TΣ to an object �t�A ∈ A by executing the op-
erations:

�σ(t1, . . . , tm)�A = fAσ (�t1�A, . . . , �tm�A) .

Sets of trees can be specified by regular tree
grammars (Gécseg and Steinby, 1997; Comon et
al., 2007). Formally, such a grammar is a structure
G = (N,Σ, P, S), where N is a signature of nonter-
minal symbols, all of which are taken to have rank 0,
Σ is a signature of terminal symbols, S ∈ N is a
distinguished start symbol, and P is a finite set of
productions of the form B → t, where B is a non-
terminal symbol, and t ∈ TN∪Σ. The productions
of a regular tree grammar are used as rewriting rules
on terms. More specifically, the derivation relation
of G is defined as follows. Let t1, t2 ∈ TN∪Σ be

terms. Then G derives t2 from t1 in one step, de-
noted by t1 ⇒G t2, if there exists a production of
the form B → t and t2 can be obtained by replacing
an occurrence of B in t1 by t. The language L(G)
generated by G is the set of all terms t ∈ TΣ that can
be derived, in zero or more steps, from the term S.

A (tree) homomorphism is a function h : TΣ →
T∆ which expands symbols of Σ into (possibly mul-
tiple) symbols of ∆ while following the structure
of the input tree. Formally, h is defined by a term
h(f) ∈ T∆∪{x1,...,xn} for each f ∈ Σ, where n is
the rank of f and the xi are variable symbols of rank
0. Given a term t ∈ TΣ, h(t) is defined recursively
by

h(f(t1, . . . , tn)) = h(f){h(t1)/x1, . . . , h(tn)/xn},
where {t�1/x1, . . . , t�n/xn} represents a substitution
that replaces all occurrences of xi with the respec-
tive t�i. A homomorphism is called linear if every
term h(f) contains each variable at most once.

Finally, a tree transducer is a device M for de-
scribing binary relations between trees; the first tree
in each pair is usually seen as the input and the sec-
ond as the output. They generalize string transduc-
ers to the tree case and are defined in more detail in
(Comon et al., 2007). A useful way of thinking of
a tree transducer is in terms of bimorphisms. A bi-
morphism is a triple B = (h1,G, h2) of an RTG G
and two homomorphisms h1, h2; it represents the bi-
nary relation {(h1(t), h2(t)) | t ∈ L(G)}. vielleicht
brauchen wir das hier gar nicht

3 Grammar formalisms based on tree
automata

We will now present a unified framework of
synchronous and non-synchronous grammar for-
malisms in terms of regular tree languages, tree ho-
momorphisms, and algebras. We will illustrate the
framework using ordinary context-free grammars
and synchronous tree-substitution grammars, but the
framework is much more general than this, and we
will hint at this at the end of the section.

3.1 Ordinary grammars
The process of generating a string from a context-
free grammar G can be seen as a two-step process.
In a first step, we generate a derivation of G by ex-
panding nonterminals using production rules. The

h1

string transducer); these differences simply amount
to the appropriate selection of algebras and homo-
morphisms.

Plan of the paper. The paper is structured as fol-
lows. We will start by laying the formal foundations
in Section 2. We will then show how to combine bi-
morphisms with algebraic interpretations and illus-
trate the approach on several grammar formalisms in
Section 3. We will define generic algorithms in Sec-
tion 4. Section 7 discusses related work, and Sec-
tion 8 concludes.

2 Formal foundations

A signature is a finite set Σ of function symbols f ,
each of which has been assigned a non-negative in-
teger called its rank. Given a signature Σ, we can
define a (finite constructor) tree over Σ as a finite
tree whose nodes are labeled with symbols from Σ
such that a node with a label of rank n has exactly n
children. We write TΣ for the set of all trees over Σ.
Trees can be written as terms; f(t1, . . . , tn) stands
for the tree with root label f and subtrees t1, . . . , tn.
The nodes of a tree can be identified by the paths
π ∈ N∗ from the root to the node: The root has ad-
dress �, and the i-th child of the node below path π
has the address πi. We write t(π) for the symbol at
path π in the tree t.

A Σ-algebra A consists of a non-empty set A
called the domain and, for each symbol f ∈ Σ with
rank m, a total function fA : Am → A, called the
operation associated with f . We can evaluate a term
t ∈ TΣ to an object �t�A ∈ A by executing the op-
erations:

�σ(t1, . . . , tm)�A = fAσ (�t1�A, . . . , �tm�A) .

Sets of trees can be specified by regular tree
grammars (Gécseg and Steinby, 1997; Comon et
al., 2007). Formally, such a grammar is a structure
G = (N,Σ, P, S), where N is a signature of nonter-
minal symbols, all of which are taken to have rank 0,
Σ is a signature of terminal symbols, S ∈ N is a
distinguished start symbol, and P is a finite set of
productions of the form B → t, where B is a non-
terminal symbol, and t ∈ TN∪Σ. The productions
of a regular tree grammar are used as rewriting rules
on terms. More specifically, the derivation relation
of G is defined as follows. Let t1, t2 ∈ TN∪Σ be

terms. Then G derives t2 from t1 in one step, de-
noted by t1 ⇒G t2, if there exists a production of
the form B → t and t2 can be obtained by replacing
an occurrence of B in t1 by t. The language L(G)
generated by G is the set of all terms t ∈ TΣ that can
be derived, in zero or more steps, from the term S.

A (tree) homomorphism is a function h : TΣ →
T∆ which expands symbols of Σ into (possibly mul-
tiple) symbols of ∆ while following the structure
of the input tree. Formally, h is defined by a term
h(f) ∈ T∆∪{x1,...,xn} for each f ∈ Σ, where n is
the rank of f and the xi are variable symbols of rank
0. Given a term t ∈ TΣ, h(t) is defined recursively
by

h(f(t1, . . . , tn)) = h(f){h(t1)/x1, . . . , h(tn)/xn},
where {t�1/x1, . . . , t�n/xn} represents a substitution
that replaces all occurrences of xi with the respec-
tive t�i. A homomorphism is called linear if every
term h(f) contains each variable at most once.

Finally, a tree transducer is a device M for de-
scribing binary relations between trees; the first tree
in each pair is usually seen as the input and the sec-
ond as the output. They generalize string transduc-
ers to the tree case and are defined in more detail in
(Comon et al., 2007). A useful way of thinking of
a tree transducer is in terms of bimorphisms. A bi-
morphism is a triple B = (h1,G, h2) of an RTG G
and two homomorphisms h1, h2; it represents the bi-
nary relation {(h1(t), h2(t)) | t ∈ L(G)}. vielleicht
brauchen wir das hier gar nicht

3 Grammar formalisms based on tree
automata

We will now present a unified framework of
synchronous and non-synchronous grammar for-
malisms in terms of regular tree languages, tree ho-
momorphisms, and algebras. We will illustrate the
framework using ordinary context-free grammars
and synchronous tree-substitution grammars, but the
framework is much more general than this, and we
will hint at this at the end of the section.

3.1 Ordinary grammars
The process of generating a string from a context-
free grammar G can be seen as a two-step process.
In a first step, we generate a derivation of G by ex-
panding nonterminals using production rules. The

'

string transducer); these differences simply amount
to the appropriate selection of algebras and homo-
morphisms.

Plan of the paper. The paper is structured as fol-
lows. We will start by laying the formal foundations
in Section 2. We will then show how to combine bi-
morphisms with algebraic interpretations and illus-
trate the approach on several grammar formalisms in
Section 3. We will define generic algorithms in Sec-
tion 4. Section 7 discusses related work, and Sec-
tion 8 concludes.

2 Formal foundations

A signature is a finite set Σ of function symbols f ,
each of which has been assigned a non-negative in-
teger called its rank. Given a signature Σ, we can
define a (finite constructor) tree over Σ as a finite
tree whose nodes are labeled with symbols from Σ
such that a node with a label of rank n has exactly n
children. We write TΣ for the set of all trees over Σ.
Trees can be written as terms; f(t1, . . . , tn) stands
for the tree with root label f and subtrees t1, . . . , tn.
The nodes of a tree can be identified by the paths
π ∈ N∗ from the root to the node: The root has ad-
dress �, and the i-th child of the node below path π
has the address πi. We write t(π) for the symbol at
path π in the tree t.

A Σ-algebra A consists of a non-empty set A
called the domain and, for each symbol f ∈ Σ with
rank m, a total function fA : Am → A, called the
operation associated with f . We can evaluate a term
t ∈ TΣ to an object �t�A ∈ A by executing the op-
erations:

�σ(t1, . . . , tm)�A = fAσ (�t1�A, . . . , �tm�A) .

Sets of trees can be specified by regular tree
grammars (Gécseg and Steinby, 1997; Comon et
al., 2007). Formally, such a grammar is a structure
G = (N,Σ, P, S), where N is a signature of nonter-
minal symbols, all of which are taken to have rank 0,
Σ is a signature of terminal symbols, S ∈ N is a
distinguished start symbol, and P is a finite set of
productions of the form B → t, where B is a non-
terminal symbol, and t ∈ TN∪Σ. The productions
of a regular tree grammar are used as rewriting rules
on terms. More specifically, the derivation relation
of G is defined as follows. Let t1, t2 ∈ TN∪Σ be

terms. Then G derives t2 from t1 in one step, de-
noted by t1 ⇒G t2, if there exists a production of
the form B → t and t2 can be obtained by replacing
an occurrence of B in t1 by t. The language L(G)
generated by G is the set of all terms t ∈ TΣ that can
be derived, in zero or more steps, from the term S.

A (tree) homomorphism is a function h : TΣ →
T∆ which expands symbols of Σ into (possibly mul-
tiple) symbols of ∆ while following the structure
of the input tree. Formally, h is defined by a term
h(f) ∈ T∆∪{x1,...,xn} for each f ∈ Σ, where n is
the rank of f and the xi are variable symbols of rank
0. Given a term t ∈ TΣ, h(t) is defined recursively
by

h(f(t1, . . . , tn)) = h(f){h(t1)/x1, . . . , h(tn)/xn},
where {t�1/x1, . . . , t�n/xn} represents a substitution
that replaces all occurrences of xi with the respec-
tive t�i. A homomorphism is called linear if every
term h(f) contains each variable at most once.

Finally, a tree transducer is a device M for de-
scribing binary relations between trees; the first tree
in each pair is usually seen as the input and the sec-
ond as the output. They generalize string transduc-
ers to the tree case and are defined in more detail in
(Comon et al., 2007). A useful way of thinking of
a tree transducer is in terms of bimorphisms. A bi-
morphism is a triple B = (h1,G, h2) of an RTG G
and two homomorphisms h1, h2; it represents the bi-
nary relation {(h1(t), h2(t)) | t ∈ L(G)}. vielleicht
brauchen wir das hier gar nicht

3 Grammar formalisms based on tree
automata

We will now present a unified framework of
synchronous and non-synchronous grammar for-
malisms in terms of regular tree languages, tree ho-
momorphisms, and algebras. We will illustrate the
framework using ordinary context-free grammars
and synchronous tree-substitution grammars, but the
framework is much more general than this, and we
will hint at this at the end of the section.

3.1 Ordinary grammars
The process of generating a string from a context-
free grammar G can be seen as a two-step process.
In a first step, we generate a derivation of G by ex-
panding nonterminals using production rules. The

1

n hn

...

1

string transducer); these differences simply amount
to the appropriate selection of algebras and homo-
morphisms.

Plan of the paper. The paper is structured as fol-
lows. We will start by laying the formal foundations
in Section 2. We will then show how to combine bi-
morphisms with algebraic interpretations and illus-
trate the approach on several grammar formalisms in
Section 3. We will define generic algorithms in Sec-
tion 4. Section 7 discusses related work, and Sec-
tion 8 concludes.

2 Formal foundations

A signature is a finite set Σ of function symbols f ,
each of which has been assigned a non-negative in-
teger called its rank. Given a signature Σ, we can
define a (finite constructor) tree over Σ as a finite
tree whose nodes are labeled with symbols from Σ
such that a node with a label of rank n has exactly n
children. We write TΣ for the set of all trees over Σ.
Trees can be written as terms; f(t1, . . . , tn) stands
for the tree with root label f and subtrees t1, . . . , tn.
The nodes of a tree can be identified by the paths
π ∈ N∗ from the root to the node: The root has ad-
dress �, and the i-th child of the node below path π
has the address πi. We write t(π) for the symbol at
path π in the tree t.

A Σ-algebra A consists of a non-empty set A
called the domain and, for each symbol f ∈ Σ with
rank m, a total function fA : Am → A, called the
operation associated with f . We can evaluate a term
t ∈ TΣ to an object �t�A ∈ A by executing the op-
erations:

�σ(t1, . . . , tm)�A = fAσ (�t1�A, . . . , �tm�A) .

Sets of trees can be specified by regular tree
grammars (Gécseg and Steinby, 1997; Comon et
al., 2007). Formally, such a grammar is a structure
G = (N,Σ, P, S), where N is a signature of nonter-
minal symbols, all of which are taken to have rank 0,
Σ is a signature of terminal symbols, S ∈ N is a
distinguished start symbol, and P is a finite set of
productions of the form B → t, where B is a non-
terminal symbol, and t ∈ TN∪Σ. The productions
of a regular tree grammar are used as rewriting rules
on terms. More specifically, the derivation relation
of G is defined as follows. Let t1, t2 ∈ TN∪Σ be

terms. Then G derives t2 from t1 in one step, de-
noted by t1 ⇒G t2, if there exists a production of
the form B → t and t2 can be obtained by replacing
an occurrence of B in t1 by t. The language L(G)
generated by G is the set of all terms t ∈ TΣ that can
be derived, in zero or more steps, from the term S.

A (tree) homomorphism is a function h : TΣ →
T∆ which expands symbols of Σ into (possibly mul-
tiple) symbols of ∆ while following the structure
of the input tree. Formally, h is defined by a term
h(f) ∈ T∆∪{x1,...,xn} for each f ∈ Σ, where n is
the rank of f and the xi are variable symbols of rank
0. Given a term t ∈ TΣ, h(t) is defined recursively
by

h(f(t1, . . . , tn)) = h(f){h(t1)/x1, . . . , h(tn)/xn},
where {t�1/x1, . . . , t�n/xn} represents a substitution
that replaces all occurrences of xi with the respec-
tive t�i. A homomorphism is called linear if every
term h(f) contains each variable at most once.

Finally, a tree transducer is a device M for de-
scribing binary relations between trees; the first tree
in each pair is usually seen as the input and the sec-
ond as the output. They generalize string transduc-
ers to the tree case and are defined in more detail in
(Comon et al., 2007). A useful way of thinking of
a tree transducer is in terms of bimorphisms. A bi-
morphism is a triple B = (h1,G, h2) of an RTG G
and two homomorphisms h1, h2; it represents the bi-
nary relation {(h1(t), h2(t)) | t ∈ L(G)}. vielleicht
brauchen wir das hier gar nicht

3 Grammar formalisms based on tree
automata

We will now present a unified framework of
synchronous and non-synchronous grammar for-
malisms in terms of regular tree languages, tree ho-
momorphisms, and algebras. We will illustrate the
framework using ordinary context-free grammars
and synchronous tree-substitution grammars, but the
framework is much more general than this, and we
will hint at this at the end of the section.

3.1 Ordinary grammars
The process of generating a string from a context-
free grammar G can be seen as a two-step process.
In a first step, we generate a derivation of G by ex-
panding nonterminals using production rules. The

m
'

h'1

h'm
(c)(b)(a)

...

Figure 1: Schematic view of some IRTGs. (a) Monolingual grammar, k D 1, L.G/ � A; (b) synchronous grammar,
k D 2, L.G/ � A � A0; (c) generalized synchronous grammar with n “input” and m “output” interpretations,
k D mC n, L.G/ � A1 � : : : �An �A01 � : : :A

0
m.

The trees in L.G / correspond to derivation trees in
TAG; the elements of L.G/ are the objects described
by the grammar. For instance, all context-free string
languages can be described using k D 1 and the
string algebra mentioned above (see Fig. 1a). String
relations defined by synchronous CFGs or STSGs are
exactly those described by IRTGs with k D 2 and
two such algebras (Fig. 1b).

When parsing IRTGs, we are given input objects
on a number of interpretations, and look for those
derivation trees t 2 L.G / that are consistent with
these input objects. Consider the case where we
have an input object a 2 A1 for a single interpreta-
tion; we are looking for the trees t 2 L.G / such that
Jh1.t/KA1 D a. Many important algebras (including
the string and term algebras) are regularly decompos-
able: for each a 2 A1, there is an RTG D.a/ – the
decomposition grammar of a – such that L.D.a//
is the set of all terms over A1 that evaluate to a.
Then the set of parses is L.G /\ h�11 .L.D.a///. Us-
ing the closure properties of RTLs, this can be com-
puted with a variety of generic algorithms, including
bottom-up and top-down algorithms for intersection.

Under the IRTG perspective, the distinction
between “monolingual” and synchronous grammars
boils down to the choice of k D 1 (Fig. 1a) vs. k > 1
(Fig. 1b,c). The parsing algorithm generalizes eas-
ily to the synchronous case, and supports both the
synchronous parsing of multiple input interpretations
and the decoding into multiple output interpretations.
See Koller and Kuhlmann (2011) for details.

3 Perspectives on TAG

There is an extensive literature on relating TAG
to other grammar formalisms. In this section, we
provide a uniform view on some of the most import-
ant such analyses by recasting them as IRTGs.

Derivation Trees. The fundamental insight that en-
ables us to convert TAGs into IRTGs is that the set
of derivation trees of a TAG G forms a regular tree
language (Vijay-Shanker et al., 1987). In the formu-
lation of Schmitz and Le Roux (2008), we can obtain
an RTG G describing the derivation trees by using
a nonterminal set fNS ; NA j N nonterminal of Gg;
the start symbol is SS . G is defined over a signature
whose symbols are the names of the elementary trees
in G. The production rules encode the way in which
these elementary trees can be combined using substi-
tution (by expanding a nonterminal of the form NS )
and adjunction (by expanding a nonterminal of the
form NA). In this way, the derivation trees of the
example grammar from Fig. 2a are described by an
RTG G0 D .N0; ˙0; P0; S0/ with productions

SS ! ˛1.NPS ; SA; VPA/

NPS ! ˛2.NPA/

VPA ! ˇ1.VPA/

SA; VPA; NPA ! nop

Notice that every node at which an adjunction may
take place is represented by a nonterminal symbol,
which must be expanded by a production rule. If
no adjunction takes place, we expand it with a rule
of the form N ! nop, which is available for every
nonterminal N (see Fig. 2b).

LCFRS. The view of TAG as a linear context-free
rewriting system (LCFRS; Weir (1988)) can be seen
as an IRTG as follows. Consider a ˙0-algebra AL
whose values are the derived trees of the TAG gram-
mar G. AL interprets each symbol in the derivation
tree as a complex tree-building operation which spells
out the derived trees. For instance, ˛AL1 is a function
(on three arguments, because ˛1 has rank 3) which
takes an initial tree t1 and two auxiliary trees t2 and
t3 as arguments, and returns the tree which results

137



S

NP !

sleeps

VP

NP

John

!1 !2

(b)

VP

sometimes VP *

"1 !1

!2 "1

(c)

S

NP

sleeps

VPJohn

VP

sometimes

(a)

nop

nop nop

Figure 2: A TAG grammar (a), together with a derivation tree (b) and a derived tree (c). The “nop” nodes in the
derivation tree indicate that no adjunction took place where one was possible, and are only needed for technical reasons.

from substituting and adjoining t1, t2, and t3 into ˛1
at appropriate places. Using such functions, one can
directly interpret the tree ˛1.˛2.nop/; nop; ˇ1.nop//
as the derived tree in Fig. 2c. Therefore, for the
IRTG GL D .G0; .id;AL//, where id is the identity
homomorphism on T˙0 , L.GL/ is exactly the set
of derived trees of G. One could instead obtain an
IRTG for describing the string language of G by us-
ing an interpretation into a ˙0-algebra of strings and
string tuples in which the elementary trees evaluate
to appropriate generalized concatenation operations.

STAG as Bimorphisms. Shieber (2004) proposes
a different perspective on the generative process of
synchronous tree substitution grammar (STSG). He
builds upon earlier work on bimorphisms and repres-
ents an STSG as an IRTG .G0; .h1; T�1/; .h2; T�2//,
where T�1 and T�2 are appropriate term algebras. In
this construction, the homomorphisms must carry
some of the load: In an STSG whose left-hand side
contains the trees ˛1 and ˛2 from Fig. 2a, we would
have h1.˛1/ D S.x1; VP.sleeps//. Shieber (2006)
later extended this approach to STAG by replacing
the tree homomorphisms with embedded push-down
transducers, a more powerful tree rewriting device.

Context-Free Tree Languages. Finally, Mönnich
(1998) noticed that the language of derived trees of
a TAG grammar is always a (monadic) context-free
tree language (CFTL). It has been known since the
seminal paper of Engelfriet and Schmidt (1977) that
every CFTL can be generated by evaluating the trees
of a regular tree language in a specific tree algebra
that Engelfriet and Schmidt call the “tree substitution
algebra”; we will call it the YIELD algebra T Y˙ here
to avoid confusion, after the name of the evaluation
function. Using this insight, we can capture Mön-

nich’s perspective by describing the derived trees of
a TAG grammar with an IRTG .G0; .h; T Y˙ //, where
h is a homomorphism that spells out the elementary
trees. Essentially, TAG derived trees are generated
by using a TSG of “building instructions” (spelled
out by the homomorphisms in a way that is similar
to Shieber’s), and evaluating the derived trees of the
TSG in the YIELD algebra. This idea was made ex-
plicit for TAG by Morawietz and Mönnich (2001)
and applied to STAG by Maletti (2010).

Discussion. These three different perspectives on
TAG as IRTGs are summarized in Fig. 3. The choice
of perspective has a significant impact on the al-
gorithms that are natural for it, and the challenges one
faces in developing them. The LCFRS view pushes
the work of constructing a derived tree almost entirely
into the algebra, which is relatively complicated and
not binary. This makes it tricky to define a uniform
algorithm for computing D.w/ for an input string
w. When an LCFRS is used to encode an STAG
grammar, it is also inconvenient to define a parsing
or decoding algorithm that only gets a left string
as its input. Shieber’s perspective pushes almost
all of the work into the translation from derivation
trees to derived trees. Parsing involves computing
the pre-image of D.w/ under embedded push-down
transducers, which is harder than for ordinary homo-
morphisms. The YIELD approach strikes a balance
between these two extremes, in that the workload is
more evenly balanced between the (ordinary) homo-
morphism and the algebra. To our knowledge, no
parsing or decoding algorithms for strings based on
this perspective have been worked out; Maletti (2010)
leaves this as an open problem. In the remainder of
this paper we will fill this gap.

138



homomorphisms algebras parsing synchronous

LCFRSs identity complex + (+)
bimorphisms embedded pushdown term - +
CFTLs tree homomorphisms YIELD - +

this paper tree homomorphisms YIELD (simplified) + +

Figure 3: Perspectives on TAG.

4 An Algebra for Derived Trees

We will first introduce an algebra for derived trees
and show how it can be used to cast TAG as an IRTG.
We will then introduce a suitable TAG string algebra
in Section 5.

4.1 The Tree Algebra TD�
The intuition of the derived tree algebra is that ad-
junction can be modeled using two substitutions of
trees into a context. Take an auxiliary tree as a con-
text C with a hole in place of the foot node, and say
we want to adjoin it at some node � in the tree t . This
can be done by splitting t into a context application
C 0Œt 0, where C 0 is the context in t with root " and
hole � . The result of the adjunction is then simply
C 0ŒC Œt . In the derivation tree algebra, we use a
special symbol @, which inserts its second argument
into the hole of its first argument. Thus, in this al-
gebra the term @.C 0;@.C; t// evaluates to the result
of the adjunction. The @ operation is a special case
of the composition symbols cn;k used in the YIELD
algebra of Engelfriet and Schmidt (1977). A similar
intuition underlies the idea of “lifting” in Morawietz
and Mönnich (2001), and the work of Maletti (2010).

Let � be a ranked signature of node labels. We
define the algebra TD� of all TAG derived trees over
� as a partial algebra whose domain contains all trees
over � and all contexts over �; that is, the domain is
T� [ C�. Every f jk 2 � is a k-place operation in
TD� . It takes k arguments t1; : : : ; tk 2 T� [ C�. It
is defined if either t1; : : : ; tk are all trees, or at most
one of them is a context. In either case, it returns the
result f .t1; : : : ; tk/. In addition, TD� has a constant
� that evaluates to the empty context �. Finally, the
binary operation @ substitutes an element of TD� into
a context. It is defined for arguments C; t where t
is a context and t is either a context or a tree. It
returns C Œt; this is a tree if t is a tree, and a context
otherwise.

4.2 An IRTG for TAG Derived Tree Languages
For any given TAG grammar G that uses some al-
phabet � of node labels in its derived trees, we can
now construct an IRTG G D .G ; .ht ; TD� // such that
L.G/ consists of exactly the derived trees that are
generated byG. We choose G to be a˙ -RTG that de-
scribes exactly the derivation trees ofG, and we need
to construct the homomorphism ht that maps deriv-
ation trees into “building instructions” for derived
trees, i.e. terms of TD� .

Each node in the derivation tree is labeled with the
name ˛ of an elementary tree (or nop); the subtrees
below it describe trees that are combined with this
elementary tree using substitution and adjunction.
The purpose of ht .˛/ is to spell out the way in which
˛ does this combining. Substitution is modeled by
simply leaving a variable in ht .˛/ in the appropriate
place; it will be filled with the initial tree when ht
is evaluated. Adjunction is modeled through the @
operator, as indicated above. Formally, let

A! ˛.B1S ; : : : ; B
k
S ; B

kC1
A ; : : : ; B

n
A/

be the (unique) rule in G that contains the symbol ˛.
Let i be a function that maps each substitution node
� in ˛ to the position of the nonterminal occurrence
that corresponds to � in the right-hand side of this
rule, i.e. to a number between 1 and k. Likewise, let i
map each node at which an adjunction may take place
to the position of the adjunction nonterminal, i.e. a
number between k C 1 and n. We define a function
h˛ for each ˛ that maps nodes � of ˛ to terms over
TD� , and let ht .˛/ D h˛."/. Then h˛.�/ D xi.�/
if � is a substitution node; h˛.�/ D a if � is a
lexical leaf with label a; h˛.�/ D � if � is a foot
node, and

h˛.�/ D @.xi.�/; f .h˛.�1/; : : : ; h˛.�n//

if � is a non-leaf with label f . In this way, we can
construct ht .˛/ for each elementary tree ˛.

139



We illustrate this construction by con-
verting the grammar of Fig. 2a into an
IRTG G D .G0; .ht ; TD� //, where � D
fS2; NP1; VP2; VP1; John0; sometimes0; sleeps0g.
The subscripts are needed to distinguish occurrences
of the label same “VP” in Fig. 2c with different
ranks. The homomorphism ht looks as follows:

ht .˛1/ D @.x2; S2.x1;@.x3; VP1.sleeps////
ht .˛2/ D @.x1; NP1.john//
ht .ˇ1/ D @.x1; VP2.sometimes;�//
ht .nop/ D �

Notice how the ability of ˛1 to allow adjunction at the
S2 and VP2 nodes translates into uses of @, which
perform the adjunctions by inserting the contexts
(= auxiliary trees) that are passed in x2 and x3, re-
spectively, in the right places. The NP substitution
happens by simply inserting the tree (= initial tree)
that is passed in x1. The term ht .ˇ1/ illustrates how
the contexts that model the auxiliary trees are built
by combining � (which stands for the empty context
containing just one hole) into larger structures. The
term ht .nop/ simply evaluates to the empty context;
adjoining it anywhere leaves a tree unchanged.

When applied to the derivation tree
in Fig. 2b, the homomorphism ht re-
turns the term @.�; S2.@.�; NP1.john//;
@.@.�; VP2.sometimes;�//; VP1.sleeps////.
This term evaluates to the derived tree in Fig. 2c.

5 A String Algebra for TAG

Now consider how the basic ideas from Section 4
can be applied to obtain an algebra for TAG string
languages. The domain of any such algebra must
contain both strings (for the yields of initial trees)
and pairs of strings (for the yields of auxiliary trees:
one string to the left of the foot node and one string
to the right). We have several options in defining
the operations on such an algebra. One option is to
use the same signature as for the derived tree algebra
from Section 4. Unfortunately, this has the effect that
the algebra contains operations of rank greater than
two, which increases the parsing complexity.

5.1 The TAG String Algebra AT

We choose to instead build a binary string algebra.
The string algebra AT for TAG over the finite alpha-
bet A is a partial algebra whose domain contains all

strings and all pairs of strings over A; that is, the
domain is A� [ .A� � A�/. We write w for a string
and w D .w1; w2/ for a string pair.

Every element a of A is a constant of AT with
aA

T

D a. There is also a constant � with �A
T

D

."; "/. AT has a binary partial concatenation opera-
tion conc, which is defined if at least one of its two
arguments is a string. When defined, it concatenates
strings and string pairs as follows:

concA
T

.w1; w2/ D w1w2

concA
T

.w1; w2/ D .w1w21; w22/

concA
T

.w1; w2/ D .w11; w12w2/

Finally, there is a binary partial wrapping operation
wrap, which is defined if its first argument is a string
pair. This operation wraps its first argument around
the second, as follows:

wrapA
T

.w1; w2/ D w11w2w12

wrapA
T

.w1; w2/ D .w11w21; w22w12/

Notice that these operations closely mirror the op-
erations for well-nested LCFRSs with fan-out 2 that
were used in Gómez-Rodríguez et al. (2010).

5.2 An IRTG for TAG String Languages

We can useAT to construct, for any given TAG gram-
mar G over some alphabet A of terminal symbols, an
IRTG G D .G ; .hs; AT // such that L.G/ consists
of exactly the strings that are generated by G. We
again describe the derivation trees using a ˙ -RTG G .
Say that AT is a �-algebra. It remains to construct a
homomorphism hs from T˙ to T�.

This is most easily done by defining a second ho-
momorphism hst that maps from TD� to A

T . hst
effectively reads off the yield of a tree or context, and
is defined by mapping each operation symbol of TD�
to a term over AT . In particular, it breaks tree-con-
structing symbols f 2 � in TD� up into sequences of
binary concatenation operations. Thus hs D hst ı ht
becomes a homomorphism from ˙ into terms of AT .

hst .f / D conc.x1; : : : ; conc.xk�1; xk// if k � 2

hst .f / D x1 if f j1
hst .f / D f if f j0
hst .@/ D wrap.x1; x2/
hst .�/ D �

140



To describe the string language Fig. 2a, we can
use the IRTG .G0; .hs; AT //, for the alphabet A D
fjohn; sometimes; sleepsg. The homomorphism hs
comes out as follows:

hs.˛1/ D wrap.x2; conc.x1;wrap.x3; sleeps///

hs.˛2/ D wrap.x1; john/

hs.ˇ1/ D wrap.x1; conc.sometimes;�//

hs.nop/ D �

Each hs.˛/ encodes the operation of the
elementary tree ˛ as a generalized concatena-
tion function on strings and string pairs. Ap-
plying hs to the derivation tree in Fig. 2b
produces the term wrap.�; conc.wrap.�; john/;
wrap.wrap.�; conc.sometimes;�//; sleeps///. This
term evaluates in AT to “John sometimes sleeps.”

5.3 Synchronous Grammars

In summary, for any TAG grammar G, we can
obtain an IRTG .G ; .hs; AT // for the strings de-
scribed by G, and an IRTG .G ; .ht ; TD� // for the
derived trees. Both IRTGs use the same central RTG
G . This means that we can combine both views
on TAG in a single IRTG with two interpretations,
G D .G ; .hs; A

T /; .ht ; T
D
� //.

We can take this idea one step further in order
to model synchronous TAG grammars. An STAG
grammar can be seen as an RTG generating the de-
rivation trees, plus two separate devices that build
the left and right derived tree from a given deriv-
ation tree (Shieber, 2004; Shieber, 2006). Each
“half” of the STAG grammar is simply an ordin-
ary TAG grammar; they are synchronized with each
other by requiring that in each STAG derivation,
the individual TAG components must use the same
derivation tree. As we have seen, an individual
TAG grammar can be represented as an IRTG with
two interpretations. We can therefore represent an
STAG grammar as an IRTG with four interpretations,
G D .G ; .h1s ; A

T
1 /; .h

1
t ; T

D
�1
/; .h2s ; A

T
2 /; .h

2
t ; T

D
�2
//.

The language of G consists of four-tuples contain-
ing two derived trees and their two associated string
yields – one each for the left and right-hand side of
the STAG grammar. Notice that unlike in the LCFRS
view on STAG, the four individual components are
kept separate at all points, and decoding any com-
bination of inputs into any combination of outputs is
straightforward.

6 Decomposing the Parsing Algorithm

With these two algebras in place, all that remains to
be done to define parsing and decoding algorithms
for TAG and STAG is to show that AT and TD� are
regularly decomposable; then the generic algorithms
for IRTG can do the rest.

6.1 Decomposition in the String Algebra

A term t that evaluates to some string or string pair
w in the string algebra AT describes how w can be
built recursively from smaller parts using concaten-
ation and wrapping. Just as in a CKY parser, these
parts are either spans Œi; k identifying the substring
wi : : : wk�1, or span pairs Œi; j; k; l identifying the
pair .wi : : : wj�1; wk : : : wl�1/ of substrings.

We can obtain a decomposition grammar D.w/
for w by using these spans and span pairs as nonter-
minals. The production rules of D.w/ spell out how
larger parts can be built from smaller ones using con-
catenation and wrapping operations, as follows:

Œi; k! conc.Œi; j ; Œj; k/

Œi; j; k; l! conc.Œi; j 0; Œj 0; j; k; l/

Œi; j; k; l! conc.Œi; j; k; k0; Œk0; l /

Œi; l ! wrap.Œi; j; k; l; Œj; k/

Œi; j; k; l! wrap.Œi; i 0; l 0; l ; Œi 0; j; k; l 0/

Œi; i C 1! wi

Œi; i; j; j ! �

The start symbol of D.w/ is the span that corres-
ponds to the entire string or string pair w. If w is a
string of length n, it is Œ1; n C 1; for a string pair
w D .w1 : : : wm�1; wm : : : wn/ 2 A

T , the start sym-
bol is Œ1;m;m; nC 1. Notice that the size of D.w/
is O.n6/ because of the second wrapping rule, and
the grammar can also be computed in time O.n6/.

6.2 Decomposition in the Derived Tree Algebra

The parts from which a term over the derived tree
algebra TD� builds some derived tree or context � 2
TD� are the subtrees or the contexts within � . Each
subtree can be identified by its root node � in � ; each
context can be identified by its root � and its hole � 0.

Thus we can obtain a decomposition grammar
D.�/ using a nonterminal A� to indicate the sub-
tree starting at � and a nonterminal B�=� 0 to indicate
the context from � to � 0. As above, the rules spell
out the ways in which larger subtrees and contexts

141



can be constructed from smaller parts:

A� ! f .A�1; : : : ; A�n/ t.�/ D f jn

A� ! @.B�=� 0 ; A� 0/ � 0 node in t # �

B�=� 0 ! f .A�1; : : : ; B�i=� 0 ; : : : ; A�n/ �i � �
0; t .�/ D f jn

B�=� 0 ! @.B�=� 00 ; B� 00=� 0/ � < �
00
� � 0

B�=� ! �

Again, the start symbol is simply the representa-
tions of � itself. If � is a tree, it is A"; for a context
with hole � , the start symbol is B"=� . If � has n
nodes, the grammar D.�/ has O.n3/ rules because
of the second rule for @.

6.3 Decomposing the TAG Parsing Algorithm

To illustrate the use of these decomposition grammars
in the context of the parsing algorithm of Section 2,
we parse the string w D “John sometimes sleeps”
using the IRTG G D .G0; .hs; AT // for the string
perspective on the example grammar from Fig. 2 (cf.
Section 5).

Step 1: Decomposition Grammar. First, the
parser computes the decomposition grammar D.w/.
As explained above, this grammar has the start sym-
bol Œ1; 4 and rules given in Fig. 4 (among others).
The complete grammar generates a set of 72 terms
over AT , each of which evaluates to w. The term
that is important here is wrap.�; conc.wrap.�; john/;
wrap.wrap.�; conc.sometimes;�//; sleeps///. Cru-
cially, the TAG grammar is completely irrelevant
at this point: L.D.w// consists of all terms over
AT which evaluate to w, regardless of whether they
correspond to grammatical derivation trees or not.

Step 2: Inverse Homomorphism. Next, we com-
pute an RTG G 0 for h�1s .L.D.w///. This grammar
describes all trees over ˙ that are mapped by hs into
terms that evaluate to w. Its nonterminal symbols are
still spans and span pairs, but the terminal symbols
are now names of elementary trees. The grammar
has the start symbol Œ1; 4 and the following rules:

Œ1; 4! ˛1.Œ1; 2; Œ1; 1; 4; 4; Œ2; 3; 4; 4/

Œ1; 2! ˛2.Œ1; 1; 2; 2/

Œ2; 3; 4; 4! ˇ1.Œ2; 2; 4; 4/

Œ1; 1; 4; 4! nop

Œ1; 1; 2; 2! nop

Œ2; 2; 4; 4! nop

Œ1; 2! john

Œ2; 3! sometimes

Œ3; 4! sleeps

Œ1; 1; 2; 2! �

Œ1; 2! wrap.Œ1; 1; 2; 2; Œ1; 2/

Œ3; 3; 4; 4! �

Œ2; 3; 4; 4! conc.Œ2; 3; Œ3; 3; 4; 4/

Œ2; 2; 4; 4! �

Œ2; 3; 4; 4! wrap.Œ2; 2; 4; 4; Œ2; 3; 4; 4/

Œ2; 4! wrap.Œ2; 3; 4; 4; Œ3; 4/

Œ1; 4! conc.Œ1; 2; Œ2; 4/

Œ1; 4! wrap.Œ1; 1; 4; 4; Œ1; 4/

Œ1; 1; 4; 4! �

Figure 4: The decomposition grammar.

An algorithm that computes G 0 is given by Koller
and Kuhlmann (2011). The basic idea is to simulate
the backwards application to the rules ofD.w/ on the
right-hand sides of the rules of the homomorphism hs .
As an example, consider the rule

hs.˛1/ D wrap.x2; conc.x1;wrap.x3; sleeps///

If we instantiate the variables x2; x1; x3 with the
spans Œ1; 2, Œ1; 1; 4; 4 and Œ2; 3; 4; 4, respectively,
then the backwards application of D.w/ yields Œ1; 4.
This warrants the first production of G 0.

Step 3: Intersection. G 0 is an RTG that represents
all derivation trees that are consistent with the input
string; these are not necessarily grammatical accord-
ing to G. On the other hand, G0 generates exactly the
grammatical derivation trees, including ones that do
not describe the input string. To obtain an RTG for
the derivation trees that are grammatical and match
the input, we intersect G0 and G 0. This yields a gram-
mar G 00 whose nonterminals are pairs of nonterminals
from G0 and G 0 and the following rules:

SŒ1;4 ! ˛1.NPŒ1;2; SŒ1;1;4;4; VPŒ2;3;4;4/

NPŒ1;2 ! ˛2.NPŒ1;1;2;2/

VPŒ2;3;4;4 ! ˇ1.VPŒ2;2;4;4/

SŒ1;1;4;4 ! nop

NPŒ1;1;2;2 ! nop

VPŒ2;2;4;4 ! nop

As expected, L.G 00/ contains a single tree, namely
the derivation tree in Fig. 2b.

142



Discussion. G 00 is essentially a standard TAG parse
chart forw. We have obtained it in three steps. Step 1
was an algebra-specific decomposition step; this was
the only step in the parsing algorithm that was partic-
ular to TAG. Steps 2 and 3 then performed generic op-
erations on RTGs, and are exactly the same whether
we would parse with respect to TAG, context-free
grammars, or a grammar formalism that describes
objects in some other algebra. Thus this is a TAG
parsing algorithm which decomposes into three parts,
each of which is easier to understand, teach, and
prove correct than a monolithic algorithm.

The runtime of the overall algorithm is O.n6/ as
long as both the algebra and the underlying RTG
are binary. The string algebra was binary by design;
furthermore, the RTG of every IRTG that encodes
a TAG can be brought into a binary normal form
(Gómez-Rodríguez et al., 2010). If we are parsing
input on several interpretations simultaneously, e.g.
in STAG parsing, binarization is not always possible
(Huang et al., 2009), and the parsing algorithm takes
exponential runtime. See also Koller and Kuhlmann
(2011) for a discussion of binarization.

7 Conclusion

We have shown how a variety of formal perspect-
ives on TAG can be uniformly understood in terms of
IRTGs. By introducing two new, regularly decompos-
able algebras for strings and derived trees, we have
shown how to obtain a modular parsing algorithm
for TAG and STAG. This algorithm can be adapted
to support synchronous parsing and decoding. For
IRTGs with weighted RTGs, which are capable of
capturing PTAG (Resnik, 1992) and synchronous
PTAG, we can also perform Viterbi parsing and EM
training on the parse chart.

The general advantage of the parsing algorithm
presented here is that it decomposes into simple com-
ponents. Recombining these yields an algorithm that
is essentially identical to the standard CKY parser
for TAG (Shieber et al., 1995). We can obtain other
parsing algorithms by varying the way in which inter-
section and inverse homomorphisms are computed.

Acknowledgments. We thank Matthias Büchse,
John Hale, Andreas Maletti, Heiko Vogler, and our re-
viewers for helpful comments, and Thutrang Nguyen
for an initial implementation.

References
H. Burden and P. Ljunglöf. 2005. Parsing linear context-

free rewriting systems. In Proc. of the 9th IWPT.
H. Comon, M. Dauchet, R. Gilleron, F. Jacquemard, D. Lu-

giez, C. Löding, S. Tison, and M. Tommasi. 2008. Tree
automata techniques and applications. Available on
http://tata.gforge.inria.fr/.

J. Engelfriet and E. Schmidt. 1977. IO and OI. I. Journal
of Computer and System Sciences, 15(3):328–353.

F. Gécseg and M. Steinby. 1997. Tree languages. In
G. Rozenberg and A. Salomaa, editors, Handbook of
Formal Languages, volume 3, pages 1–68. Springer.

C. Gómez-Rodríguez, M. Kuhlmann, and G. Satta. 2010.
Efficient parsing of well-nested linear context-free re-
writing systems. In Proc. of HLT/NAACL.

L. Huang, H. Zhang, D. Gildea, and K. Knight. 2009.
Binarization of synchronous context-free grammars.
Computational Linguistics, 35(4):559–595.

A. Koller and M. Kuhlmann. 2011. A generalized view
on parsing and translation. In Proc. of the 12th IWPT.

A. Maletti. 2010. A tree transducer model for synchron-
ous tree-adjoining grammars. In Proc. of the 48th ACL.

U. Mönnich. 1998. Adjunction as substitution. an al-
gebraic formulation of regular, context-free, and tree-
adjoining languages. In Proc. of FG.

F. Morawietz and U. Mönnich. 2001. A model-theoretic
description of tree adjoining grammars. Electronic
Notes in Theoretical Computer Science, 53.

P. Resnik. 1992. Probabilistic tree-adjoining grammar as
a framework for statistical natural language processing.
In Proc. of COLING.

S. Schmitz and J. Le Roux. 2008. Feature unification
in TAG derivation trees. In Proc. of the 9th TAG+
Workshop.

S. Shieber, Y. Schabes, and F. Pereira. 1995. Principles
and implementation of deductive parsing. Journal of
Logic Programming, 24(1–2):3–36.

S. Shieber. 2004. Synchronous grammars as tree trans-
ducers. In Proc. of the 7th TAG+ Workshop.

S. Shieber. 2006. Unifying synchronous tree-adjoining
grammars and tree transducers via bimorphisms. In
Proc. of the 11th EACL.

K. Vijay-Shanker and A. Joshi. 1985. Some computa-
tional properties of Tree Adjoining Grammars. In Proc.
of the 23rd ACL.

K. Vijay-Shanker, D. Weir, and A. Joshi. 1987. Char-
acterizing structural descriptions produced by various
grammatical formalisms. In Proc. of the 25th ACL.

D. Weir. 1988. Characterizing Mildly Context-Sensitive
Grammar Formalisms. Ph.D. thesis, University of
Pennsylvania, Philadelphia, USA.

143


