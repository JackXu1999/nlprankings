



















































Online Distilling from Checkpoints for Neural Machine Translation


Proceedings of NAACL-HLT 2019, pages 1932‚Äì1941
Minneapolis, Minnesota, June 2 - June 7, 2019. c¬©2019 Association for Computational Linguistics

1932

Online Distilling from Checkpoints for Neural Machine Translation

Hao-Ran Wei, Shujian Huang ‚àó, Ran Wang, Xin-yu Dai and Jiajun Chen
State Key Laboratory for Novel Software Technology

Nanjing University
Nanjing 210023, China

{weihr, huangsj, wangr, daixy, chenjj}@nlp.nju.edu.cn

Abstract

Current predominant neural machine transla-
tion (NMT) models often have a deep struc-
ture with large amounts of parameters, mak-
ing these models hard to train and easily suf-
fering from over-fitting. A common prac-
tice is to utilize a validation set to evaluate
the training process and select the best check-
point. Average and ensemble techniques on
checkpoints can lead to further performance
improvement. However, as these methods do
not affect the training process, the system per-
formance is restricted to the checkpoints gen-
erated in the original training procedure. In
contrast, we propose an online knowledge dis-
tillation method. Our method on-the-fly gener-
ates a teacher model from checkpoints, guid-
ing the training process to obtain better per-
formance. Experiments on several datasets
and language pairs show steady improvement
over a strong self-attention-based baseline sys-
tem. We also provide analysis on data-limited
setting against over-fitting. Furthermore, our
method leads to an improvement on a machine
reading experiment as well.

1 Introduction

Neural Machine Translation (NMT) (Cho et al.,
2014; Sutskever et al., 2014) has been rapidly de-
veloped during the past several years. For further
performance improvement, deeper and more ex-
pressive structures (Johnson et al., 2017; Barone
et al., 2017b; Gehring et al., 2017; Vaswani et al.,
2017) have been exploited. However, all of these
models have more than hundreds of millions of pa-
rameters, which makes the training process more
challenging. During the training of NMT models,
we notice the following two problematic phenom-
ena: First, the training process is unstable. This is
evidenced by the decreasing of training loss with

‚àóCorresponding Author.

fluctuate performance on the validation set. Second,
the performance on validation set usually begins to
worsen after several epochs, while the training loss
keeps decreasing, which suggests the model being
at risk of over-fitting.

In order to alleviate these issues, the common
practice is to periodically evaluate models on a
held-out set (with each evaluated model saved as a
checkpoint). Training is terminated when m con-
secutive checkpoints show no improvement and
select the checkpoint with best evaluation score
as the final model. Further improvement can be
achieved by utilizing more checkpoints, by smooth-
ing, which averages these checkpoints‚Äô parameters
to generate more desirable parameters (Sennrich
et al., 2016a); or by ensemble, which averages these
checkpoints‚Äô output probabilities at every step dur-
ing inference (Chen et al., 2017).

However, we notice that all of these methods
have a limitation. Once the training process gets pa-
rameters with poor performance, selecting, smooth-
ing or ensemble from the checkpoints in this pro-
cess may have limited generalization performance
as well. We impute the limitation to the ‚Äúoffline‚Äù
property of these methods. In other words, only
employing checkpoints after training cannot affect
the original training process.

In this paper, we propose to utilize checkpoints
to lead the training process. Our method is carried
out in a knowledge distillation manner. At each
training step, because being evaluated on the held-
out validation data, the best checkpoint up to the
current training step can be seen as a model with
the best generalization ability so far. Therefore,
we employ this checkpoint as the teacher model,
and let the current training model, as the student,
learn from the output probability distributions of
the teacher model, as well as truth translations in
the training data. Such kind of knowledge distil-
lation is performed on-the-fly because the teacher



1933

model could always be updated once any latest bet-
ter checkpoint is generated. We call our method
Online Distillation from Checkpoints (ODC).

We conduct experiments on four translation
tasks (including two low-resource tasks), and one
machine reading comprehension task. All the
results demonstrate that our ODC method can
achieve improvement upon strong baseline systems.
ODC also outperforms checkpoint smoothing and
ensemble methods, without extra cost during in-
ference. We can achieve further improvement by
combining ODC with those methods.

Major contributions of our work include:

1. In contrast to checkpoint smoothing and en-
semble which do not affect the training pro-
cess, we explore the way to distill knowledge
from checkpoints to lead the training process
in an on-the-fly manner(¬ß3.1, ¬ß3.2). We ob-
tain better performance by replacing the best
checkpoint with moving average parameters
at that step. (¬ß3.3)

2. We conduct experiments on four translation
tasks, including two low resource tasks. In all
the tasks our method outperforms strong base-
line systems (¬ß4.2, ¬ß4.3). We also conduct an
experiment on machine reading comprehen-
sion task and the result shows that our method
can be applied to other tasks too (¬ß4.4).

3. We conduct comprehensive analysis and show
that our method can significantly alleviate
over-fitting issue in low-resource condition
(¬ß5.1), and help to find a wider minimum
which brings better generation (¬ß5.2).

2 Background

2.1 Neural Machine Translation

Neural Machine Translation (NMT) systems learn
a conditional probability P (Y |X) for translating
a source sentence X = (x1, ..., xM ) to a target
sentence Y = (y1, ..., yN ), in which xi and yj are
the i-th word and j-th word in sentence X and Y ,
respectively. An NMT model usually consists of
an encoder (parameterized by Œ∏enc) and a decoder
(parameterized by Œ∏dec). The encoder transforms
a sequence of source tokens into a sequence of
hidden states:

H(X) = (h1, ..., hM ) = fenc(X; Œ∏enc). (1)

The decoder of NMT is usually a network com-
puting the conditional probability of each target
words yj based on its previous words and the source
sentence:

p(yj |y<j , X) ‚àù exp(fdec(y<j , sj , H(X); Œ∏dec)),
(2)

where sj is the hidden state of decoder at time step
j, p is the distribution of NMT model and Œ∏ is all
the parameters of NMT model.

The standard way to train an NMT model is to
minimize the cross-entropy between the one-hot
distribution of the target sentence and the NMT
model‚Äôs output distribution:

L(Œ∏) =‚àí
N‚àë
j=1

|V|‚àë
k=1

1{yj = k} (3)

¬∑ log p(yj = k|y<j , X; Œ∏),
Œ∏‚àó = arg min

Œ∏
L(Œ∏), (4)

where 1(¬∑) is the indicator function and V is the
target vocabulary.

2.2 Knowledge Distillation in Neural
Machine Translation

Knowledge distillation is a class of methods which
transfers knowledge from a pre-trained teacher
model T , to a student model S . The teacher model
can be a model with large capacity (Bucila et al.,
2006) or an ensemble of several models (Hinton
et al., 2015). In knowledge distillation, the stu-
dent model learns to match the predictions of the
teacher model. Concretely, assuming that we learn
a classification model (parameterized by Œ∏) on a set
of training samples in the form of (x, y) with |V|
classes. Instead of minimizing the cross-entropy
loss between one-hot label y and model‚Äôs output
probability p(y|x; Œ∏), knowledge distillation uses
the teacher model‚Äôs distribution q(¬∑|x) as ‚Äúsoft tar-
gets‚Äù and optimizes the loss:

LKD(Œ∏) =‚àí
|V|‚àë
k=1

q(y = k|x; Œ∏T ) (5)

log p(y = k|x; Œ∏),

where Œ∏T parameterizes the teacher model and
p(¬∑|x) is the distribution of the student model.

Kim and Rush (2016) proposed that, as the loss
of NMT model (Equation 4) can be factored into
minimizing cross-entropy loss between the target



1934

Checkpoints:

Teacher 
Models:

training steps direction

update teacher models

knowledge distillation

validation score 
(darker means better)

ùëáùëò‚Ä≤+1 ùëáùëò‚Ä≤+2ùëáùëò‚Ä≤ ùëáùëò‚Ä≤‚Ä≤ ùëáùëò‚Ä≤‚Ä≤+1

Figure 1: Illustration of online distillation from checkpoints(ODC). Darker color means better performance on
validation data. In validation step Tk‚Ä≤ , ODC selects the current best checkpoints as the teacher model; while in the
next validation step Tk‚Ä≤+1, the training generates a better checkpoint, and use it to update the teacher model. In
validation step Tk‚Ä≤+2, training model‚Äôs performance declines, and the teacher model is used to lead the training of
the model. The similar situation happens during the entire training process.

words and word-level probabilities of the NMT
model for every position at target side, knowledge
distillation on multi-class classification can be nat-
urally applied. They defined word-level knowledge
distillation (W-KD) on a sentence as:

LW-KD(Œ∏) = ‚àí
N‚àë
i=j

|V|‚àë
k=1

q(yj = k|y<j , H(x); Œ∏T )

(6)

¬∑ log p(yj = k|y<j , H(x); Œ∏),

where V is the target vocabulary.
They further proposed sequence-level knowl-

edge distillation (S-KD), which optimizes the stu-
dent model by matching the predictions of the
teacher model in the probability distribution over
the space of all possible target sequences:

LS-KD(Œ∏) =
‚àë
Y ‚ààœÑ

q(Y |X; Œ∏T ) log p(Y |X; Œ∏), (7)

where œÑ is the space of target side sentences. As
summing over exponential numbers of samples
here is intractable, they proposed to train student
model on samples generated by teacher model as
an approximation.

3 Online Distillation from Checkpoints

3.1 Online Knowledge Distillation
Traditional knowledge distillation maintains a
static teacher throughout the training process,

which not only requires one pre-training process to
obtain the teacher model, but also limits the power
of leading the training process.

In contrast, we are aiming at a more integrated
process where the teacher model does not come
from a separate training process, but from the cur-
rent training routine itself. More specifically, we
update the teacher along with the training process,
so the distilled knowledge could be updated when
a stronger model comes out. Figure 1 illustrates
the paradigm of our method.

In generation tasks, the knowledge distillation
could be performed at the word-level or sequence-
level. In this paper, we focus on the word-level dis-
tillation because this distillation only needs forced
teaching, which could be performed efficiently to-
gether with the training of the student model com-
pared to generating translations from teacher model.
It is more computational-friendly, especially when
the NMT models are built with parallelizable con-
volution (Gehring et al., 2017) or self-attention
structures (Vaswani et al., 2017).

3.2 Online Distillation from Best Checkpoint

Observed from the training process of NMT mod-
els, performance on the validation set does not im-
prove monotonically. When the performance of the
training model on the validation set declines, we
could always select the best checkpoint so far as
the teacher, because it has the best generalization
performance. Specially, when the best checkpoint



1935

is generated at the current time step, we only up-
date the teacher model but perform no distillation.
Figure 1 gives an illustration of this process.

The online distillation process is summarized
in Algorithm 1. We use t to denote the training
step and Œ∏t to denote the parameters at time step
t. We denote Tk as the time step the k-th time
when the model is evaluated on validation and
‚àÜT as the validation interval, for which Tk+1 =
Tk + ‚àÜT . Let TÃÇk (TÃÇk ‚â§ Tk) be the time step when
the best checkpoint is obtained up to Tk, and Œ∏T
as the teacher‚Äôs parameters to lead the following
training process. If the current checkpoint is the
best checkpoint so far, i.e. TÃÇk = Tk, we update
the teacher to be this new checkpoint Œ∏T = Œ∏TÃÇk (in
Line 16 and 20). The loss for the training process at
time step t (Tk < t < Tk+1) is defined as follows:

Lt(Œ∏) =

{
L(Œ∏) TÃÇk = Tk
L(Œ∏) + LW-KD(Œ∏) otherwise,

(8)

where L(Œ∏) and L(Œ∏)W-KD is defined in Equation 4
and 7, respectively (in Line 5-8).

3.3 Integrated with Mean Teacher
Knowledge distillation usually works better when
teacher models have better performance. As Tar-
vainen and Valpola (2017) proposed in their work,
averaging model parameters over training steps
tends to produce a more accurate model that using
final parameters directly. They called this method
as Mean Teacher.

Following Tarvainen and Valpola (2017), besides
updating parameters, we maintain the exponential
moving average (EMA) of the model parameters
as:

Œ∏
‚Ä≤
t = Œ±Œ∏

‚Ä≤
t‚àí1 + (1‚àí Œ±)Œ∏t, (9)

where t is the update step, Œ∏ is the parameters of the
training model and Œ∏

‚Ä≤
the parameters of EMA. Œ± is

the decay weight which is close to 1.0, and typically
in multiple-nines range, i.e., 0.999, 0.9999. By
doing so, at each timestep t, parameters of NMT
model Œ∏t has their corresponding EMA parameters
Œ∏
‚Ä≤
t.

Whenever we update teacher model Œ∏T with the
current best checkpoint, we can use its EMA param-
eters instead (in Line 17-18). It can further improve
the generalization ability of the teacher model, and
bring a better performance of knowledge distilla-
tion. We will show in ¬ß4.2 that using meaning
teacher indeed achieves better performance.

Algorithm 1: Online Distillation from Check-
points

1 Input: validation interval ‚àÜT ; validation count k;EMA
decay weight Œ±; initial model parameters Œ∏0

2 Initialization: k = 0; t = 0; T0 = ‚àí1, TÃÇ0 = ‚àí1;
Œ∏T = ‚àÖ; Œ∏‚Ä≤0 = Œ∏0; L0(Œ∏) = L(Œ∏)

3 while not reach stopping criteria do
4 repeat
5 if TÃÇk = Tk then
6 Lt(Œ∏) = L(Œ∏)
7 else
8 Lt(Œ∏) = L(Œ∏) + LW-KD(Œ∏)
9 minimize Lt(Œ∏) and update Œ∏t ;

10 Œ∏
‚Ä≤
t = Œ±Œ∏

‚Ä≤
t‚àí1 + (1‚àí Œ±)Œ∏t;

11 t = t+ 1 ;
12 until t mod ‚àÜT == 0;
13 Tk+1 = t;
14 evaluate on validation set;
15 if get better checkpoint then
16 TÃÇk+1 = t;
17 if use EMA as teacher then
18 Œ∏T = Œ∏

‚Ä≤
t;

19 else
20 Œ∏T = Œ∏t;

21 else
22 TÃÇk+1 = TÃÇk;

23 k = k + 1;

4 Experiments

4.1 Setups

To evaluate the effectiveness of our method,
we conduct experiments on four machine trans-
lation tasks: NIST Chinese-English, WMT17
Chinese-English, IWSLT15 English-Vietnamese,
and WMT17 English-Turkish. We conduct exper-
iments based on an open source implementation
of Transformer (Vaswani et al., 2017) model in
NJUNMT-pytorch1. For all the translation experi-
ments, we use SacreBLEU 2 to report reproducible
BLEU scores.

We also present an experiment on machine read-
ing comprehension, showing our method could also
be applied to other tasks.

Datasets For NIST Chinese-English translation
task, training data consists of 1.34M LDC sen-
tence pairs3, with 40.8M Chinese words and 45.8M
English words, respectively. We use NIST2003
dataset set as the validation set and NIST 2004,

1https://github.com/whr94621/NJUNMT-pytorch
2https://github.com/awslabs/sockeye/tree/master

/sockeye contrib/sacrebleu
3The corpora includes LDC2002E18, LDC2003E07,

LDC2003E14, Hansards portion of LDC2004T07,
LDC2004T08 and LDC2005T06



1936

SYSTEMS
NIST Chinese-English

NIST03 NIST04 NIST05 NIST06 Average ‚àÜ

RNNSearch (Zhang et al., 2018b) 36.59 39.57 35.56 35.29 - -
Transformer-base(Yang et al., 2018) 42.23 42.17 41.02 - - -

baseline 43.78 44.26 40.97 38.93 41.39 -
baseline + LKS 44.12 44.87 41.59 39.22 41.89 +0.50
baseline + BKS 44.23 44.98 41.62 39.74 42.11 +0.73
baseline + BKE 44.30 45.01 41.86 40.05 42.31 +0.92

ODC 45.33 45.18 42.60 39.67 42.48 +1.10
ODC + LKS 45.05 45.49 42.99 40.48 42.99 +1.60
ODC + BKS 45.35 45.49 43.21 39.96 42.89 +1.50
ODC + BKE 45.34 45.92 43.35 40.30 43.19 +1.80

ODC-EMA 45.52 45.72 43.01 40.65 43.13 +1.74

Table 1: Case-insensitive BLEU scores of Chinese-English translation on NIST datasets. ‚ÄúAverage‚Äùmeans average
scores on NIST04, 05 and 06.

2005, 2006 as test sets. We filter out sentence pairs
whose source or target side contain more than 50
words. We use BPE (Sennrich et al., 2016b) with
30K merge operations on both sides.

For WMT17 Chinese-English translation task,
we use the pre-processed version released by
WMT4. We only use CWMT part of WMT Cor-
pus. We use newsdev2017 as the validation set and
newstest2017 s the test set. We learn a BPE model
with 32K merge operations and keep all the BPE
tokens in the vocabulary. We limit the maximal
sentence length as 100 after BPE segmentation.

For IWSLT15 English-Vietnamese translation
task, we directly use the pre-processed data used in
Luong and Manning (2015) 5, which has 133K sen-
tence pairs, with 2.70M English words and 3.31M
Vietnamese words. We use the released validation
and test set, which has 1553 and 1268 sentences
respectively. Following the settings in Huang et al.
(2017), the Vietnamese and English vocabulary size
are 7,709 and 17,191, respectively.

For WMT17 English-Turkish translation task,
We use the pre-processed data released by
WMT176. It has 207K sentence pairs, with 5.21M
English words and 4.63 Turkish words. We
use newstest2016 as our validation set and new-
stest2017 as the test set. We use joint BPE segmen-
tation (Sennrich et al., 2017) to process the whole
training data. The merge operations are 16K.

4http://data.statmt.org/wmt18/translation-
task/preprocessed/zh-en/

5https://github.com/tefan-it/nmt-en-vi
6http://data.statmt.org/wmt17/translation-

task/preprocessed/tr-en/

Implementation Details Without specific state-
ment, we follow the transformer base v1 hyper-
parameters settings 7, with 6 layers in both encoder
and decoder, 512 hidden units and 8 attention heads
in multi-head attention mechanism and 2048 hid-
den units in feed-forward layers. Parameters are
optimized using Adam(Kingma and Ba, 2014). The
initial learning rate is set as 0.1 and scheduled ac-
cording to the method proposed in Vaswani et al.
(2017), with warm-up steps as 4000.

We periodically evaluate the training model on
the validation set by doing translation and compute
the BLEU scores. We stop training when 50 subse-
quent of BLEU scores on validation set do not get
improvement. We use beam search with beam size
as 5.

4.2 Evaluation on Chinese-English
Translation Tasks

We first evaluate the capability of our method for
improving performance when there are plenty of
training data. We conduct experiments on both
NIST and WMT17 Chinese-English Translation
tasks.

Results on NIST Dataset We compare our
method with several ways to utilize checkpoints8:

‚Ä¢ last-k-smoothing: After training the baseline
model, we average the parameters of the last
k checkpoints as the final model.

‚Ä¢ best-k-smoothing: Average the parameters
of the best k checkpoints, instead of the last k,

7https://github.com/tensorflow/tensor2tensor/blob/v1.3.0/
tensor2tensor/models/transformer.py

8We set k = 5 in this experiments.



1937

as the final model. In this case, checkpoints
may have better performance but higher vari-
ance which could be harmful to parameters
averaging.

‚Ä¢ best-k-ensemble: Do ensemble inference (av-
erage the output probabilities) with the best k
checkpoints (Chen et al., 2017).

As shown in Table 1, our baseline is comparable to
the other two recent published results (Zhang et al.
(2018b), Yang et al. (2018)). In consistent with
Chen et al. (2017), using checkpoints for smooth-
ing or ensemble does improve the baseline system.
Using EMA parameters also improve the baseline
system as well, which is in consist with (Tarvainen
and Valpola, 2017).

Compared to the baseline, our approach ODC
brings translation improvement across different
test sets and achieves 42.48 BLEU scores on aver-
age(+1.09 BLEU v.s. baseline). This result con-
firms that using best checkpoint as teacher indeed
helps improving the performance of the translation
model.

Besides, ODC is comparable to the best re-
sults among smoothing and ensemble on baseline‚Äôs
checkpoints (achieved by best-k-ensemble). Con-
sidering that best-k-ensemble needs to decode with
k models, while ODC decodes only one, our model
enjoys a better efficiency. Furthermore, we can
achieve further improvement by combining these
methods on checkpoints generated by ODC.

Results also show that ODC-EMA (¬ß3.3) could
achieve additional improvement from ODC itself
(43.13 v.s. 42.48 BLEU), demonstrating that us-
ing EMA of the best checkpoint instead can bring
better knowledge distillation performance, as it gen-
erates a better teacher model.

Results on WMT17 Dataset We present the re-
sults on WMT17 Chinese-English translation task
in Table 2. We report the results of the baseline,
ODC and a recent result published by Zhang et al.
(2018c). To make a fair comparison, we follow the
experiment setting in Zhang et al. (2018c). The
experiment results show similar trends with those
on the NIST datasets. Applying ODC leads to the
result of 24.22 BLEU, which is 0.85 BLEU higher
compared with baseline.

4.3 Evaluation on Low-resource Scenario
We also apply our method to two low resource trans-
lation tasks, i.e., IWSLT2015 English-Vietnamese

SYSTEM newsdev2017 newstest2017

Zhang et al. (2018c) - 23.01
baseline 21.96 23.37
ODC 22.24 24.22

Table 2: Case-sensitive BLEU scores on WMT17
Chinese-English Translation

(EN2VI) and WMT17 English-Turkish (EN2TR).
Due to the limited amount of training data, models
are more likely to suffer from over-fitting. There-
fore, we use a higher dropout rate of 0.2 and weight
decay, another common technique against over-
fitting, with decay weight set as 10‚àí3 as the default
setting. We implement weight decay as AdamW
(Loshchilov and Hutter, 2017) does.

Besides, we further experiment with grid search
on the validation set for optimal hyper-parameters
of dropout rate and weight decay, which may lead
to better results. We adopt a simple heuristic, which
first searches an optimal dropout rate, and then
further searches weight decay coefficients based
on this dropout. We experiment with dropout as
0.2, 0.3, 0.4, and weight decay as 10‚àí1, 10‚àí2 and
10‚àí3.

SYSTEMS EN2VI EN2TR

Zhang et al. (2018a) - 12.11
tensor2tensor 28.43 -

baseline 28.56 12.20
baseline w/ grid-search 29.01 12.51

ODC 29.47 12.92
ODC w/ grid-search 29.59 13.18

Table 3: Case-sensitive BLEU scores on two low re-
source translation tasks.

As in Table 3, our baseline is comparable to
two recent published results, respectively: EN2TR
from Zhang et al. (2018c) and EN2VI from offi-
cial release tensor2tensor problem9. Grid hyper-
parameter search does improve the baseline system.
ODC leads to better results compared to the base-
line, as well as the baseline with grid parameter
search. ODC can achieve further improvement
after searching for optimal hyper-parameters of
dropout and weight decay.

9https://github.com/tensorflow/tensor2tensor/pull/611



1938

4.4 Evaluation on Machine Reading
Comprehension

Although our main research is focused for the task
of machine translation, the idea of ODC could
be applied to other tasks as well. We experi-
ments on the Stanford Question Answering Dataset
(SQuAD) (Rajpurkar et al., 2016), a machine read-
ing comprehension task.

SQuAD contains 107,785 human-generated
reading comprehension questions, with 536
Wikipedia articles. Each question is associated
with a paragraph extracted from an article, and the
corresponding answer is a span from this article. A
machine reading comprehension model is designed
to predict the start and end positions in the article
of the answer.

The state-of-the-art machine reading compre-
hension system also employs a deep neural net-
work structure, which is similar to NMT. We ap-
ply our ODC method on BiDAF++ (Choi et al.,
2018), a multi-layer SQuAD model that augments
BiDAF (Seo et al., 2016) with self-attention and
contextualized embeddings. We evaluate the model
after each epoch and implement the knowledge
distillation by teaching the student with the out-
put distribution of answer start and end positions
predicted by the best checkpoint.

For the results, ODC improves a base BiDAF++
from 76.83 to 77.40, in EM scores, showing that
our method can be applied to a broader range of
tasks.

5 Analysis

We conduct further analysis to probe into the rea-
sons for the advantages of ODC. We first show
that our method can significantly alleviate the over-
fitting issue in data-limited condition. After that,
we show that parameters gained from our method
tend to be wider minimums, which represents better
generalization.

5.1 Effectiveness on reducing over-fitting

Taking IWSLT15 English-Vietnamese as a test-bed,
we analyze whether our method could help handle
the over-fitting issue. We first plot the curve of the
loss on the validation set at each training step for
the different models (in Figure 2, the top curve with
rounds). It is easy to see that the loss curve of the
baseline increases as the training goes after 50K
steps, indicating a severe over-fitting. With better
dropout rate and weight decay, the over-fitting is

Figure 2: Loss curves (top) and final BLEU scores (bot-
tom) on the validation set of baseline, baseline with
grid-search and ODC, respectively.

less severe; while with ODC the loss curve shows a
more steady trend of decrease, and is almost always
under the other two‚Äôs.

The final BLEU score on the validation set
(Figure 2, bottom) shows corresponding result.
The grid search of hyper-parameters improves the
BLEU from 26.06 to 26.42 in BLEU, while ODC
achieves 26.99.

Both results indicate that our method is more
effective at handling the over-fitting problem. We
hold that minimizing the cross-entropy between
the teacher model and the student model serves as
regularization to the training of the student model,
which avoids the model getting into over-fitting.

5.2 ODC Brings Wider Minimum

In the training process in Chinese-English tasks, we
do not observe obvious over-fitting issue as shown
in low resource translation tasks. In this section,
we analyze how ODC helps the model generaliza-
tion. Keskar et al. (2016) proposed that the width



1939

Figure 3: The upper plot shows the validation losses
curve along the line segment decided by parameters of
baseline and ODC. The bottom plot shows the stan-
dard deviations within the neighborhood of baseline
and ODC at different distances.

of the minimum in a loss surface is related to its
generalization ability. Therefore, we compare the
generalization capability between baseline system
and our ODC method by exploring around the pa-
rameters.

We make use of the visualization technique em-
ployed in (Goodfellow and Vinyals, 2014) and ana-
lyze the results on the NIST data set. Let Œ∏base and
Œ∏ODC denote the final parameters obtained from
baseline and ODC. Consider the line:

Œ∏(Œ±) = Œ± ¬∑ Œ∏ODC + (1.0‚àí Œ±) ¬∑ Œ∏base, (10)

which connects Œ∏base (Œ± = 0.0) and Œ∏ODC (Œ± =
1.0). We plot the value of Equation 4 as a function
of Œ± (normalized by count of words per sentence)
with Œ∏ = Œ∏(Œ±). We draw Œ± from ‚àí1.0 to 2.0 at
an interval of 0.02. In this way, the width of Œ∏base
and Œ∏ODC can be represented as the steepness of the
curve nearby. To further quantitatively represent
the steepness, we compute the standard deviation
of values on this curve within different distances to
the two parameters, respectively. We plot them in

Figure 3.
From Figure 3 we can see that the loss curve

behaves steeper around the parameters of baseline
than of ODC. Besides, the standard deviations of
losses around the baseline model are consistently
higher than ODC within all the distances. It is
evident that the parameters of ODC act as a wider
minimum c and explains why ODC can lead to a
more generalized model.

6 Related Works

6.1 Regularization in NMT

Regularization has broad applications in training
NMT models to improve performance and avoid
over-fitting. There are some common regulariza-
tion techniques, such as L2 normalization and
dropout (Srivastava et al., 2014). These methods
are simple and easy to implement but need care-
fully tuning on the validation set. These methods
are also orthogonal to our method.

There are also some works to exploit regular-
ization techniques in fine tuning of NMT model.
Barone et al. (2017a) proposed a tuneout method
which randomly replaces columns of weight matri-
ces of out-of-domain parameter matrices. Khayral-
lah et al. (2018) shared similar training object with
us, as they computed the KL divergence between
out-of-domain and in-domain model. Both of their
works request a pre-trained teacher model, while
we are work on a more general training problem
which does not require such kind of model.

6.2 Online Knowledge Distillation

While traditional knowledge distillation requires a
static, pre-trained teacher model, online knowledge
distillation tends to overcome this problem by se-
lecting or generating a teacher dynamically from
scratch.

To the best of our knowledge, Zhang et al. (2017)
is the first trial to replace the offline teacher model.
They trained peer models to teach each other simul-
taneously. Compared to their work, our method
uses the best checkpoint as the teacher, which
avoids introducing extra parameters. Furlanello
et al. (2018) tends to update teacher model during
the training procedure iteratively, but their method
needs to train the teacher model until convergence
in each iteration. Instead, our method only needs
one phase of training, whose overhead is relatively
small.



1940

Lan et al. (2018) using an ensemble of several
branches of the model as teacher for computer vi-
sion tasks, which only needs one-phase training as
well. However, their method relies heavily on the
multi-branch structures of the tasks, which are not
widely applicable in neural machine translation.

7 Conclusion

In this paper, we propose an online knowledge dis-
tillation method with the teacher model generated
from checkpoints during the training procedure.
Experiments on four machine translation tasks and
a machine reading task show that our method out-
performs strong baseline systems. Further analysis
shows that our method can effectively alleviate the
over-fitting issue, and tend to find a wider mini-
mum.

Acknowledgement

We would like to thank the anonymous review-
ers for their insightful comments. We also thank
Boxing Chen from Alibaba Group for his help-
ful comments. This work is supported by the Na-
tional Science Foundation of China (No. 61772261,
61672277) and the Jiangsu Province Research
Foundation for Basic Research (No. BK20170074).
Part of this work is supported by ‚Äú13th Five-Yea‚Äù
All-Army Common Information System Equip-
ment Pre-Research Project (No. 31510040201).

References
Antonio Valerio Miceli Barone, Barry Haddow, Ulrich

Germann, and Rico Sennrich. 2017a. Regulariza-
tion techniques for fine-tuning in neural machine
translation. In EMNLP 2017, pages 1489‚Äì1494.

Antonio Valerio Miceli Barone, Jindrich Helcl, Rico
Sennrich, Barry Haddow, and Alexandra Birch.
2017b. Deep architectures for neural machine trans-
lation. In WMT 2017.

Cristian Bucila, Rich Caruana, and Alexandru
Niculescu-Mizil. 2006. Model compression. In
KDD.

Hugh Chen, Scott Lundberg, and Su-In Lee. 2017.
Checkpoint ensembles: Ensemble methods from a
single training process. CoRR, abs/1710.03282.

Kyunghyun Cho, Bart van Merrienboer, CÃßaglar
GuÃàlcÃßehre, Dzmitry Bahdanau, Fethi Bougares, Hol-
ger Schwenk, and Yoshua Bengio. 2014. Learning
phrase representations using RNN encoder-decoder
for statistical machine translation. In EMNLP 2014.

Eunsol Choi, He He, Mohit Iyyer, Mark Yatskar, Wen
tau Yih, Yejin Choi, Percy Liang, and Luke S. Zettle-
moyer. 2018. Quac: Question answering in context.
In EMNLP.

Tommaso Furlanello, Zachary Chase Lipton, Michael
Tschannen, Laurent Itti, and Anima Anandkumar.
2018. Born again neural networks. In ICML.

Jonas Gehring, Michael Auli, David Grangier, Denis
Yarats, and Yann N. Dauphin. 2017. Convolutional
sequence to sequence learning. In ICML 2017.

Ian J. Goodfellow and Oriol Vinyals. 2014. Quali-
tatively characterizing neural network optimization
problems. CoRR, abs/1412.6544.

Geoffrey E. Hinton, Oriol Vinyals, and Jeffrey Dean.
2015. Distilling the knowledge in a neural network.
CoRR, abs/1503.02531.

Po-Sen Huang, Chong Wang, Dengyong Zhou, and
Li Deng. 2017. Neural phrase-based machine trans-
lation. CoRR, abs/1706.05565.

Melvin Johnson, Mike Schuster, Quoc V. Le, Maxim
Krikun, Yonghui Wu, Zhifeng Chen, Nikhil Tho-
rat, Fernanda B. VieÃÅgas, Martin Wattenberg, Greg
Corrado, Macduff Hughes, and Jeffrey Dean. 2017.
Google‚Äôs multilingual neural machine translation
system: Enabling zero-shot translation. TACL 2017,
5:339‚Äì351.

Nitish Shirish Keskar, Dheevatsa Mudigere, Jorge No-
cedal, Mikhail Smelyanskiy, and Ping Tak Peter
Tang. 2016. On large-batch training for deep learn-
ing: Generalization gap and sharp minima. CoRR,
abs/1609.04836.

Huda Khayrallah, Brian Thompson, Kevin Duh, and
Philipp Koehn. 2018. Regularized training objective
for continued training for domain adaptation in neu-
ral machine translation. In WMT 2018, pages 36‚Äì44.

Yoon Kim and Alexander M. Rush. 2016. Sequence-
level knowledge distillation. In EMNLP.

Diederik P. Kingma and Jimmy Ba. 2014. Adam:
A method for stochastic optimization. CoRR,
abs/1412.6980.

Xu Lan, Xiatian Zhu, and Shaogang Gong. 2018.
Knowledge distillation by on-the-fly native ensem-
ble. CoRR, abs/1806.04606.

Ilya Loshchilov and Frank Hutter. 2017. Fixing
weight decay regularization in adam. CoRR,
abs/1711.05101.

Minh-Thang Luong and Christopher D. Manning. 2015.
Stanford neural machine translation systems for spo-
ken language domain. In IWSLT 2015, Da Nang,
Vietnam.

Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and
Percy Liang. 2016. Squad: 100, 000+ questions for
machine comprehension of text. In EMNLP.



1941

Rico Sennrich, Alexandra Birch, Anna Currey, Ulrich
Germann, Barry Haddow, Kenneth Heafield, An-
tonio Valerio Miceli Barone, and Philip Williams.
2017. The university of edinburgh‚Äôs neural MT sys-
tems for WMT17. In WMT 2017.

Rico Sennrich, Barry Haddow, and Alexandra Birch.
2016a. Edinburgh neural machine translation sys-
tems for WMT 16. In ACL 2016, pages 371‚Äì376.

Rico Sennrich, Barry Haddow, and Alexandra Birch.
2016b. Neural machine translation of rare words
with subword units. In ACL 2016.

Min Joon Seo, Aniruddha Kembhavi, Ali Farhadi,
and Hannaneh Hajishirzi. 2016. Bidirectional at-
tention flow for machine comprehension. CoRR,
abs/1611.01603.

Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky,
Ilya Sutskever, and Ruslan Salakhutdinov. 2014.
Dropout: a simple way to prevent neural networks
from overfitting. The Journal of Machine Learning
Research, 15(1):1929‚Äì1958.

Ilya Sutskever, Oriol Vinyals, and Quoc V. Le. 2014.
Sequence to sequence learning with neural networks.
In NIPS 2014, pages 3104‚Äì3112.

Antti Tarvainen and Harri Valpola. 2017. Mean teach-
ers are better role models: Weight-averaged consis-
tency targets improve semi-supervised deep learning
results. In NIPS 2017, pages 1195‚Äì1204.

Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob
Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz
Kaiser, and Illia Polosukhin. 2017. Attention is all
you need. In NIPS.

Zhen Yang, Wei Chen, Feng Wang, and Bo Xu. 2018.
Improving neural machine translation with condi-
tional sequence generative adversarial nets. In
NAACL-HLT.

Biao Zhang, Deyi Xiong, and Jinsong Su. 2018a. Ac-
celerating neural transformer via an average atten-
tion network. In ACL.

Xiangwen Zhang, Jinsong Su, Yue Qin, Yang Liu, Ron-
grong Ji, and Hongji Wang. 2018b. Asynchronous
bidirectional decoding for neural machine transla-
tion. In AAAI.

Ying Zhang, Tao Xiang, Timothy M. Hospedales, and
Huchuan Lu. 2017. Deep mutdual learning. CoRR,
abs/1706.00384.

Zhirui Zhang, Shuangzhi Wu, Shujie Liu, Mu Li, Ming
Zhou, and Enhong Chen. 2018c. Regularizing neu-
ral machine translation by target-bidirectional agree-
ment. CoRR, abs/1808.04064.


