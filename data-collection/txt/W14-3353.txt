



















































Tolerant BLEU: a Submission to the WMT14 Metrics Task


Proceedings of the Ninth Workshop on Statistical Machine Translation, pages 409–413,
Baltimore, Maryland USA, June 26–27, 2014. c©2014 Association for Computational Linguistics

Tolerant BLEU: a Submission to the WMT14 Metrics Task

Jindřich Libovický and Pavel Pecina
Charles University in Prague, Faculty of Mathematics and Physics

Institute of Formal ad Applied Linguistics
{libovicky, pecina}@ufal.mff.cuni.cz

Abstract

This paper describes a machine translation
metric submitted to the WMT14 Metrics
Task. It is a simple modification of the
standard BLEU metric using a monolin-
gual alignment of reference and test sen-
tences. The alignment is computed as
a minimum weighted maximum bipartite
matching of the translated and the refer-
ence sentence words with respect to the
relative edit distance of the word prefixes
and suffixes. The aligned words are in-
cluded in the n-gram precision compu-
tation with a penalty proportional to the
matching distance. The proposed tBLEU
metric is designed to be more tolerant to
errors in inflection, which usually does not
effect the understandability of a sentence,
and therefore be more suitable for measur-
ing quality of translation into morphologi-
cally richer languages.

1 Introduction

Automatic evaluation of machine translation (MT)
quality is an important part of the machine trans-
lation pipeline. The possibility to run an evalua-
tion algorithm many times while training a system
enables the system to be optimized with respect to
such a metric (e.g., by Minimum Error Rate Train-
ing (Och, 2003)). By achieving a high correlation
of the metric with human judgment, we expect the
system performance to be optimized also with re-
spect to the human perception of translation qual-
ity.

In this paper, we propose an MT metric called
tBLEU (tolerant BLEU) that is based on the stan-
dard BLEU (Papineni et al., 2002) and designed to
suit better when translation into morphologically
richer languages. We aim to have a simple lan-
guage independent metric that correlates with hu-
man judgment better than the standard BLEU.

Several metrics try to address this problem
as well and usually succeed to gain a higher
correlation with human judgment (e.g. ME-
TEOR (Denkowski and Lavie, 2011), TerrorCat
(Fishel et al., 2012)). However, they usually
use some language-dependent tools and resources
(METEOR uses stemmer and parahprasing tables,
TerrorCat uses lemmatization and needs training
data for each language pair) which prevent them
from being widely adopted.

In the next section, the previous work is briefly
summarized. Section 3 describes the metric in de-
tail. The experiments with the metric are described
in Section 4 and their results are summarized in
Section 5.

2 Previous Work

BLEU (Papineni et al., 2002) is an established and
the most widely used automatic metric for evalua-
tion of MT quality. It is computed as a harmonic
mean of the n-gram precisions multiplied by the
brevity penalty coefficient which ensures also high
recall. Formally:

BLEU = BP · exp
(

4∑
n=1

1
4

log pn

)
,

where BP is the brevity penaly defined as follows:

BP =
{

1 if c > r
e1−

r
c otherwise

,

c is the length of the test sentence (number of to-
kens), r is the length of the reference sentence, and
pn is the proportion of n-grams from the test sen-
tence found in the reference translations.

The original experiments with the English to
Chinese translation (Papineni et al., 2002) re-
ported very high correlation of BLEU with human
judgments. However, these scores were computed
using multiple reference translations (to capture
translation variability) but in practice, only one

409



Reference:

Source:

Translation:

I(am(driving(a(new(red(car
Jedu(novým(červeným(autem

Jedu(s(novém(červeném(autob
7 3

Corrected(and(
wighted(translation: UJeduE(1p(UsE(1p(UnovýmE(243p(UčervenýmE(546p(UautemE(143p
Unigram(precision

Jedu
s
novým
červeným
autem

tBLEU(unigram(precision(=

1
1

243
546
143

Bigram(precision
Jedu(s
s(novým
novým(červeným
červeným(autem

tBLEU(bigram(precision(=

1
546
344

7412

11
6

5(≈(7b367

avgU1E1p(=
avgU1E(243p(=

avgU243E(546p(=
avgU546E143p(=

BLEU(unigram(precision(=(1(4(5(=(7b2

Jedu
s
novém
červeném
auto

Jedu(s
s(novém
novém(červeném
červeném(auto

16
12

4(≈(7b333

BLEU(bigram(precision(=(7(4(4(=(7

2
6
1

3
1

Figure 1: An example of the unigram and bigram precision computation for translation from English to
Czech with the test sentence having minor inflection errors and an additional preposition. The first two
lines contain the source sentence in English and a correct reference translation in Czech. On the third
line, there is an incorrectly translated sentence with errors in inflection. Between the second and the
third line, the matching with respect to the affix distance is shown. The fourth line contains the corrected
test sentence with the words weights. The bottom part of the figure shows computation of the unigram
and bigram precisions. The first column contains the original translation n-grams, the second one the
corrected n-grams, the third one the n-gram weights and the last one indicates whether a matching n-
gram is contained in the reference sentence.

reference translation is usually available and there-
fore the BLEU scores are often underestimated.

The main disadvantage of BLEU is the fact that
it treats words as atomic units and does not allow
any partial matches. Therefore, words which are
inflectional variants of each other are treated as
completely different words although their mean-
ing is similar (e.g. work, works, worked, working).
Further, the n-gram precision for n> 1 penalizes
difference in word order between the reference and
the test sentences even though in languages with
free word order both sentences can be correct (Bo-
jar et al., 2010; Condon et al., 2009).

There are also other widely recognized MT
evaluation metrics: The NIST score (Dodding-
ton, 2002) is also an n-gram based metric, but
in addition it reflects how informative particular
n-grams are. A metric that achieves a very high
correlation with human judgment is METEOR
(Denkowski and Lavie, 2011). It creates a mono-
lingual alignment using language dependent tools
as stemmers and synonyms dictionaries and com-
putes weighted harmonic mean of precision and
recall based on the matching.

Some metrics are based on measuring the

edit distance between the reference and test sen-
tences. The Position-Independent Error Rate
(PER) (Leusch et al., 2003) is computed as
a length-normalized edit distance of sentences
treated as bags of words. The Translation Edit
Rate (TER) (Snover et al., 2006) is a number of
edit operation needed to change the test sentence
to the most similar reference sentence. In this
case, the allowed editing operations are insertions,
deletions and substitutions and also shifting words
within a sentence.

A different approach is used in TerrorCat
(Fishel et al., 2012). It uses frequencies of auto-
matically obtained translation error categories as
base for machine-learned pairwise comparison of
translation hypotheses.

In the Workshop of Machine Translation
(WMT) Metrics Task, several new MT metrics
compete annually (Macháček and Bojar, 2013). In
the comptetition, METEOR and TerrorCat scored
better that the other mentioned metrics.

410



3 Metric Description

tBLEU is computed in in two steps. Similarly to
the METEOR score, we first make a monolingual
alignment between the reference and the test sen-
tences and then apply an algorithm similar to the
standard BLEU but with modified n-gram preci-
sions.

The monolingual alignment is computed as a
minimum weighted maximum bipartite matching
between words in a reference sentence and a trans-
lation sentence1 using the Munkres assignment al-
gorithm (Munkres, 1957).

We define a weight of an alignment link as the
affix distance of the test sentence word wti and the
reference sentence word wrj : Let S be the longest
common substring of wti and w

r
i . We can rewrite

the strings as a concatenation of a prefix, the com-
mon substring and a suffix:

wt = wti,pSw
t
i,s

wr = wrj,pSw
r
j,s

Further, we define the affix distance as:

AD(wr, wt)= max
{

1,
L(wrj,p,w

t
i,p)+L(w

r
s,j ,w

t
s,i)

|S|
}

if |S| > 0 and AD(wr, wt) = 1 otherwise. L is the
Levensthein distance between two strings.

For example the affix distance of two Czech
words vzpomenou and zapomenout (different
forms of verbs remember and forget) is computed
in the following way: The longest common sub-
string is pomenou which has a length of 7. The
prefixes are vz and za and their edit distance is 2.
The suffixes are an empty string and t which with
the edit distance 1. The total edit distance of pre-
fixes and suffixes is 3. By dividing the total edit
distance by the length of the longest common sub-
string, we get the affix distance 37 ≈ 0.43.

We denote the resulting set of matching pairs
of words as M = {(wri, wti)}mi=1 and for each test
sentence St = (wt1, ..., w

t
m) we create a corrected

sentence Ŝt = (ŵt1, ..., ŵ
t
m) such that

ŵti =
{
wr if ∃wt : (wr, wt)∈M & AD(wr, wt) ≤ �
wti otherwise.

This means that the words from the test sen-
tence which were matched with the affix distance

1The matching is always one-to-one which means that
some words remain unmatched if the sentences have differ-
ent number of words.

0 0.2 0.4 0.6 0.8 1
0.7

0.75

0.8

0.85

0.9

0.95

en-cs
en-de
en-es
en-fr

Affix distance threshold

P
ea

rs
on

's
 c

or
re

la
tio

n 
co

ef
fit

ie
nt

0 0.2 0.4 0.6 0.8 1
0.91

0.92

0.93

0.94

0.95

0.96

0.97

cs-en
de-en
es-en
fr-en

Affix distance threshold

P
ea

rs
on

's
 c

or
re

la
tio

n 
co

ef
fic

ie
nt

Figure 2: Dependence of the Pearson’s correlation
of tBLEU with the WMT13 human judgments on
the affix distance threshold for translations from
English and to English.

smaller than � are “corrected” by substituting them
by the matching words from the reference sen-
tence. The threshold � is a free parameter of the
metric. When the threshold is set to zero, no
corrections are made and therefore the metric is
equivalent to the standard BLEU.

The words in the corrected sentence are as-
signed the weights as follows:

v(ŵti) =
{

1−AD(ŵti , wti) if ŵti 6= wti
1 otherwise.

In other words, the weights penalize the corrected
words proportionally to the affix distance from the
original words.

While computing the n-gram precision, two
matching n-grams (ŵt1, . . . ŵ

t
n) and (w

r
1, . . . w

r
n)

contribute to the n-gram precision with a score of

s(wt1, . . . , w
t
n) =

n∑
i=1

v(ŵti) / n

instead of one as it is in the standard BLEU. The
rest of the BLEU score computation remains un-
changed. While using multiple reference transla-
tion, the matching is done for each of the refer-
ence sentence, and while computing the n-gram
precision, the reference sentences with the highest
weight is chosen. The computation of the n-gram
precision is illustrated in Figure 1.

411



direction BLEU METEOR tBLEU
en-cs .781 .860 .787
en-de .835 .868 .850
en-es .875 .878 .884
en-fr .887 .906 .906
from English .844 .878 .857

Table 1: System level Pearson’s correlation with
the human judgment for systems translating from
English computed on the WMT13 dataset.

4 Evaluation

We evaluated the proposed metric on the dataset
used for the WMT13 Metrics Task (Macháček and
Bojar, 2013). The dataset consists of 135 systems’
outputs in 10 directions (5 into English 5 out of
English). Each system’s output and the reference
translation contain 3000 sentences. According to
the WMT14 guidelines, we report the the Pear-
son’s correlation coefficient instead of the Spear-
man’s coefficient that was used in the last years.

Twenty values of the affix distance threshold
were tested in order to estimate what is the most
suitable threshold setting. We report only the sys-
tem level correlation because the metric is de-
signed to compare only the whole system outputs.

5 Results

The tBLEU metric generally improves the cor-
relation with human judgment over the standard
BLEU metric for directions from English to lan-
guages with richer inflection.

Examining the various threshold values showed
that dependence between the affix distance thresh-
old and the correlation with the human judgment
varies for different language pairs (Figure 2). For
translation from English to morphologically richer
languages than English – Czech, German, Spanish
and French – using the tBLEU metric increased
the correlation over the standard BLEU. For Czech
the correlation quickly decreases for threshold val-
ues bigger than 0.1, whereas for the other lan-
guages it still grows. We hypothesize this because
the big morphological changes in Czech can en-
tirely change the meaning.

For translation to English, the correlation
slightly increases with the increasing threshold
value for translation from French and Spanish, but
decreases for Czech and German.

There are different optimal affix distance

direction BLEU METEOR tBLEU
cs-en .925 .985 .927
de-en .916 .962 .917
es-en .957 .968 .953
fr-en .940 .983 .933
to English .923 .974 .935

Table 2: System level Pearson’s correlation with
the human judgment for systems translating to En-
glish computed on the WMT13 dataset.

thresholds for different language pairs. However,
the threshold of 0.05 was used for our WMT14
submission because it had the best average cor-
relation on the WMT13 data set. Tables 1 and
2 show the results of the tBLEU for the particu-
lar language pairs for threshold 0.05. While com-
pared to the BLEU score, the correlation is slightly
higher for translation from English and approxi-
mately the same for translation to English.

The results on the WMT14 dataset did not show
any improvement over the BLEU metric. The rea-
son of the results will be further examined.

6 Conclusion and Future Work

We presented tBLEU, a language-independent MT
metric based on the standard BLEU metric. It in-
troduced the affix distance – relative edit distances
of prefixes and suffixes of two string after remov-
ing their longest common substring. Finding a
matching between translation and reference sen-
tences with respect to this matching allows a pe-
nalized substitution of words which has been most
likely wrongly inflected and therefore less penal-
izes errors in inflection.

This metric achieves a higher correlation with
the human judgment than the standard BLEU
score for translation to morphological richer lan-
guages without the necessity to employ any lan-
guage specific tools.

In future work, we would like to improve word
alignment between test and reference translations
by introducing word position and potentially other
features, and implement tBLEU in MERT to ex-
amine its impact on system tuning.

7 Acknowledgements

This research has been funded by the Czech Sci-
ence Foundation (grant n. P103/12/G084) and the
EU FP7 project Khresmoi (contract no. 257528).

412



References
Ondřej Bojar, Kamil Kos, and David Mareček. 2010.

Tackling sparse data issue in machine translation
evaluation. In Proceedings of the ACL 2010 Con-
ference Short Papers, pages 86–91. Association for
Computational Linguistics.

Sherri Condon, Gregory A Sanders, Dan Parvaz, Alan
Rubenstein, Christy Doran, John Aberdeen, and
Beatrice Oshika. 2009. Normalization for auto-
mated metrics: English and arabic speech transla-
tion. Proceedings of MT Summit XII. Association
for Machine Translation in the Americas, Ottawa,
ON, Canada.

Michael Denkowski and Alon Lavie. 2011. Meteor
1.3: Automatic metric for reliable optimization and
evaluation of machine translation systems. In Pro-
ceedings of the Sixth Workshop on Statistical Ma-
chine Translation, WMT ’11, pages 85–91, Strouds-
burg, PA, USA. Association for Computational Lin-
guistics.

George Doddington. 2002. Automatic evaluation
of machine translation quality using n-gram co-
occurrence statistics. In Proceedings of the Sec-
ond International Conference on Human Language
Technology Research, HLT ’02, pages 138–145, San
Francisco, CA, USA. Morgan Kaufmann Publishers
Inc.

Mark Fishel, Rico Sennrich, Maja Popović, and Ondřej
Bojar. 2012. Terrorcat: a translation error
categorization-based mt quality metric. In Proceed-
ings of the Seventh Workshop on Statistical Machine
Translation, pages 64–70. Association for Compu-
tational Linguistics.

Gregor Leusch, Nicola Ueffing, Hermann Ney, et al.
2003. A novel string-to-string distance measure
with applications to machine translation evaluation.
In Proceedings of MT Summit IX, pages 240–247.
Citeseer.

Matouš Macháček and Ondřej Bojar. 2013. Results of
the WMT13 metrics shared task. In Proceedings of
the Eighth Workshop on Statistical Machine Trans-
lation, pages 45–51, Sofia, Bulgaria, August. Asso-
ciation for Computational Linguistics.

James Munkres. 1957. Algorithms for the assignment
and transportation problems. Journal of the Society
for Industrial & Applied Mathematics, 5(1):32–38.

Franz Josef Och. 2003. Minimum error rate training in
statistical machine translation. In Proceedings of the
41st Annual Meeting on Association for Computa-
tional Linguistics, pages 160–167, Sapporo, Japan.

Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a method for automatic
evaluation of machine translation. In Proceedings of
the 40th annual meeting on association for compu-
tational linguistics, pages 311–318. Association for
Computational Linguistics.

Matthew Snover, Bonnie Dorr, Richard Schwartz, Lin-
nea Micciulla, and John Makhoul. 2006. A study of
translation edit rate with targeted human annotation.
In Proceedings of association for machine transla-
tion in the Americas, pages 223–231.

413


