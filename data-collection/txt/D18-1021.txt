




































Joint Representation Learning of Cross-lingual Words and Entities via Attentive Distant Supervision


Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 227–237
Brussels, Belgium, October 31 - November 4, 2018. c©2018 Association for Computational Linguistics

227

Joint Representation Learning of Cross-lingual Words and Entities via
Attentive Distant Supervision

Yixin Cao1,2 Lei Hou2∗ Juanzi Li2 Zhiyuan Liu2
Chengjiang Li2 Xu Chen2 Tiansi Dong3

1School of Computing, National University of Singapore, Singapore
2Department of CST, Tsinghua University, Beijing, China

3B-IT, University of Bonn, Bonn, Germany
{caoyixin2011,iamlockelightning,successcx}@gmail.com

{houlei,liuzy,lijuanzi}@tsinghua.edu.cn
dongt@bit.uni-bonn.de

Abstract
Joint representation learning of words and enti-
ties benefits many NLP tasks, but has not been
well explored in cross-lingual settings. In this
paper, we propose a novel method for joint rep-
resentation learning of cross-lingual words and
entities. It captures mutually complementary
knowledge, and enables cross-lingual infer-
ences among knowledge bases and texts. Our
method does not require parallel corpora, and
automatically generates comparable data via
distant supervision using multi-lingual knowl-
edge bases. We utilize two types of regu-
larizers to align cross-lingual words and enti-
ties, and design knowledge attention and cross-
lingual attention to further reduce noises. We
conducted a series of experiments on three
tasks: word translation, entity relatedness, and
cross-lingual entity linking. The results, both
qualitatively and quantitatively, demonstrate
the significance of our method.

1 Introduction
Multi-lingual knowledge bases (KB) storemillions
of entities and facts in various languages, and pro-
vide rich background structural knowledge for un-
derstanding texts. On the other hand, text cor-
pus contains huge amount of statistical information
complementary to KBs. Many researchers lever-
age both types of resources to improve various nat-
ural language processing (NLP) tasks, such as ma-
chine reading (Yang and Mitchell, 2017), question
answering (He et al., 2017; Hao et al., 2017).
Most existing work jointly models KB and text

corpus to enhance each other by learning word and
entity representations in a unified vector space. For
example, Wang et al. (2014); Yamada et al. (2016);
Cao et al. (2017) utilize the co-occurrence infor-
mation to align similar words and entities with sim-
ilar embedding vectors. Toutanova et al. (2015);

∗Corresponding author.

Wu et al. (2016); Han et al. (2016); Weston et al.
(2013a); Wang and Li (2016) represent entities
based on their textual descriptions together with
the structured relations. Thesemethods focused on
mono-lingual settings. However, for cross-lingual
tasks (e.g., cross-lingual entity linking), these ap-
proaches need to introduce additional tools to do
translation, which suffers from extra costs and in-
evitable errors (Ji et al., 2015, 2016).
In this paper, we carry out cross-lingual joint

representation learning, which has not been fully
researched in the literature. We aim at creating a
unified space for words and entities in various lan-
guages, and easing cross-lingual semantic compar-
ison, which will benefit from the complementary
information in different languages. For instance,
two different meanings of word center in English
are expressed by two different words in Chinese:
center as the activity-specific building is expressed
by 中心, center as the basketball player role is 中
锋.
Our main challenge is the limited availability

of parallel corpus, which is usually either expen-
sive to obtain, or only available for certain narrow
domains (Gouws et al., 2015). Many work has
been done to alleviate the problem. One school
of methods uses adversarial technique or domain
adaption to match linguistic distribution (Zhang
et al., 2017b; Barone, 2016; Cao et al., 2016).
These methods do not require parallel corpora.
The weakness is that the training process is un-
stable and that the high complexity restricts the
methods only to small-scale data. Another line
of work uses pre-existing multi-lingual resources
to automatically generate “pseudo bilingual docu-
ments” (Vulic and Moens, 2015, 2016). However,
negative results have been observed due to the oc-
casional poor quality of training data (Vulic and
Moens, 2016). All above methods only focus on
words. We consider both words and entities, which



228

makes the parallel data issue more challenging.
In this paper, we propose a novel method

for joint representation learning of cross-lingual
words and entities. The basic idea is to capturemu-
tually complementary knowledge in a shared se-
mantic space, which enables joint inference among
cross-lingual knowledge base and texts without ad-
ditional translations. We achieve it by (1) utilizing
an existing multi-lingual knowledge base to auto-
matically generate cross-lingual supervision data,
(2) learning mono-lingual word and entity rep-
resentations, (3) applying cross-lingual sentence
regularizer and cross-lingual entity regularizer to
align similar words and entities with similar em-
beddings. The entire framework is trained using
a unified objective function, which is efficient and
applicable to arbitrary language pairs that exist in
multi-lingual KBs.
Particularly, we build a bilingual entity network

from inter-language links 1 in KBs for regulariz-
ing cross-lingual entities through a variant of skip-
gram model (Mikolov et al., 2013c). Thus, mono-
lingual structured knowledge of entities are not
only extended to cross-lingual settings, but also
augmented from other languages. On the other
hand, we utilize distant supervision to generate
comparable sentences for cross-lingual sentence
regularizer to model co-occurrence information
across languages. Compared with “pseudo bilin-
gual documents”, comparable sentences achieve
higher quality, because they rely not only on
the shared semantics at document level, but also
on cross-lingual information at sentence level.
We further introduce two attention mechanisms,
knowledge attention and cross-lingual attention, to
select informative data in comparable sentences.
Our contributions can be concluded as follows:

• We proposed a novel method that jointly
learns representations of not only cross-
lingual words but also cross-lingual entities in
a unified vector space, aiming to enhance the
embedding quality from each other via com-
plementary semantics.

• Our proposed model introduces distant su-
pervision coupled with attention mechanisms
to generate comparable data as cross-lingual
supervision, which can benefit many cross-
lingual analysis.

1https://en.wikipedia.org/wiki/Help:
Interlanguage_links

• We did qualitative analysis to have an in-
tuitive impression of our embeddings, and
quantitative analysis in three tasks: word
translation, entity relatedness, and cross-
lingual entity linking. Experiment results
show that our method demonstrates signifi-
cant improvements in all three tasks.

2 Related Work

Jointly representation learning of words and enti-
ties attracts much attention in the fields of Entity
Linking (Zhang et al., 2017a; Cao et al., 2018),
Relation Extraction (Weston et al., 2013b) and so
on, yet little work focuses on cross-lingual set-
tings. Inspiringly, we investigate the task of cross-
lingual word embedding models (Ruder et al.,
2017), and classify them into three groups accord-
ing to parallel corpora used as supervisions: (i)
methods requiring parallel corpus with aligned
words as constraint for bilingual word embed-
ding learning (Klementiev et al., 2012; Zou et al.,
2013; Wu et al., 2014; Luong et al., 2015; Am-
mar et al., 2016; Soricut and Ding, 2016). (ii)
methods using parallel sentences (i.e. translated
sentence pairs) as the semantic composition of
multi-lingual words (Gouws et al., 2015; Kociský
et al., 2014; Hermann and Blunsom, 2014; Chan-
dar et al., 2014; Shi et al., 2015; Mogadala and
Rettinger, 2016). (iii) methods requiring bilingual
lexicon to map words from one language into the
other (Mikolov et al., 2013b; Faruqui and Dyer,
2014; Xiao and Guo, 2014).
Themajor weakness of these methods is the lim-

ited availability of parallel corpora. One remedy is
to use existing multi-lingual resources (i.e. multi-
lingual KB). Camacho-Collados et al. (2015) com-
bines several KBs (Wikipedia, WordNet and Ba-
belNet) and leverages multi-lingual synsets to
learn word embeddings at sense level through an
extra post-processing step. Artetxe et al. (2017)
starts from a small bilingual lexicon and using
a self-learning approach to induce the structural
similarity of embedding spaces. Vulic and Moens
(2015, 2016) collect comparable documents on
same themes from multi-lingual Wikipedia, shuf-
fle and merge them to build “pseudo bilingual doc-
uments” as training corpora. However, the qual-
ity of “pseudo bilingual documents” are difficult
to control, resulting in poor performance in several
cross-lingual tasks (Vulic and Moens, 2016).
Another remedy matches linguistic distribu-

https://en.wikipedia.org/wiki/Help:Interlanguage_links
https://en.wikipedia.org/wiki/Help:Interlanguage_links


229

Joint
Representation

Learning

Cross-lingual
 Supervision

Data Generation

Cross-lingual
Sentence Regularizer

Cross-lingual
Entity Regularizer

Mono-lingual 
Representation Learning

was

Larry Faust
·NBA

NBA (zh)

basketball

Bilingual Semantic SpaceDetroit Pistons

player

American
NBA

All-star

NBA

Larry FoustAll-star

[[Lawrence Michael Foust]] was
an American basketball player

who spent 12 seasons in [[NBA]]

NBA (zh)

·
[[ · ]] [[NBA]]

1950 [[NBA ]] 1 5

English KB Chinese KB
and was an 8-time [[All-star]] … [[ ]] …

NBA
Detroit Pistons

[[ · ]] [[NBA]]

All-star
[[Lawrence Michael Foust]] was an American basketball 

player who spent 12 seasons in [[NBA]]

·

NBANBA (zh)

Comparable Sentences

Bilingual ENLarry Foust

NBA
Detroit Pistons

senk
szhk0

Figure 1: The overview framework of our method. The inputs and outputs of each step are listed in the three levels.
Particularly, there are three main components of joint representation learning. Red texts with brackets are anchors,
dashed lines denote entity relations, and solid lines are cross-lingual links.

tion via adversarial training (Barone, 2016; Zhang
et al., 2017b; Lample et al., 2018), domain adap-
tion (Cao et al., 2016). However, these methods
suffer from the instability of training process and
the high complexity. This either limits the scala-
bility of vocabulary size or relies on a strong dis-
tribution assumption.
Inspired by Vulic and Moens (2016), we gener-

ate highly qualified comparable sentences via dis-
tant supervision, which is one of the most promis-
ing approaches to addressing the issue of sparse
training data, and performs well in relation extrac-
tion (Lin et al., 2017a; Mintz et al., 2009; Zeng
et al., 2015; Hoffmann et al., 2011; Surdeanu et al.,
2012). Our comparable sentencesmay further ben-
efit many other cross-lingual analysis, such as in-
formation retrieval (Dong et al., 2014).

3 Preliminaries and Framework
3.1 Preliminaries
Given a multi-lingual KB, we take (i) text cor-
pus, (ii) entity and their relations, (iii) a set of an-
chors as inputs, and learn embeddings for each
word and each entity in various languages. For
clarity, we use English and Chinese as sample lan-
guages in the rest of the paper, and use superscript
y ∈ {en, zh} to denote language-specific parame-
ters2.

2We choose English and Chinese as example lan-
guages because they are top-ranked according to to-
tal number of speakers, the full list can be found in

We use multi-lingual Wikipedia as KB includ-
ing a set of entities Ey = {eyi } and their articles.
We concatenate these articles together, and form
text corpus Dy = ⟨wy1 , . . . , w

y
i , . . . , w

y
|D|⟩. Hy-

per links in articles are denoted by Anchors Ay =
{⟨wyi , e

y
j ⟩}, which indicates that word w

y
i refers

to entity eyj . Gy = (Ey,Ry) is the mono-lingual
Entity Network (EN), where Ry = {⟨eyi , e

y
j ⟩}

if there is a link between eyi , e
y
j . We use inter-

language links in Wikipedia as cross-lingual links
Ren−zh = {⟨eeni , ezhi′ ⟩}, indicating eeni , ezhi′ refer
to the same thing in English and Chinese. Cross-
lingual word and entity representation learning
is to map words and entities in different languages
into a unified semantic space. Each word and en-
tity obtain their embedding vectors3 wyi and e

y
j .

3.2 Framework
To alleviate the heavy burden of limited parallel
corpora and additional translation efforts, we uti-
lize existing multi-lingual resources to distantly
supervise cross-lingual word and entity represen-
tation learning, so that the shared embedding
space supports joint inference among KB and texts
across languages. As shown in Figure 1, our
framework has two steps: (1) Cross-lingual Su-

https://en.wikipedia.org/wiki/Lists_of_
languages_by_number_of_speakers.

3For the cross-lingual linked entities sharing the same
strings (e.g., NBA and NBA (zh)), which is an infrequent situ-
ation between languages, we use separated representations to
keep training objective consistent and avoid confusion.

https://en.wikipedia.org/wiki/Lists_of_languages_by_number_of_speakers
https://en.wikipedia.org/wiki/Lists_of_languages_by_number_of_speakers


230

pervision Data Generation builds a bilingual en-
tity network and generates comparable sentences
based on cross-lingual links; (2) Joint Represen-
tation Learning learns cross-lingual word and en-
tity embeddings using a unified objective function.
Our assumption throughout the entire framework
is as follows: The more words/entities two contexts
share, the more similar they are.
As shown in Figure 1, we build a bilingual

EN Gen−zh by using Gen,Gzh and cross-lingual
linksRen−zh. Thus, entities in different languages
shall be connected in a unified network to facil-
itate cross-lingual entity alignments. Meanwhile,
fromKB articles, we extract comparable sentences
Sen−zh = {⟨senk , szhk ⟩} as high qualified parallel
data to align similar words in different languages.
Based on generated cross-lingual data

Gen−zh,Sen−zh and mono-lingual data Dy,
Ay, where y ∈ {en, zh}, we jointly learn cross-
lingual word and entity embeddings through three
components: (1) Mono-lingual Representation
Learning, which learns mono-lingual word and
entity embeddings for each language by modeling
co-occurrence information through a variant of
skip-gram model (Mikolov et al., 2013c). (2)
Cross-lingual Entity Regularizer, which aligns
entities that refer to the same thing in different
languages by extending the mono-lingual model
to bilingual EN. For example, entity Foust in
English and entity 福 斯 特 (Foust) in Chinese
are closely embedded in the semantic space
because they share common neighbors in two
languages, All-star and NBA 选秀 (draft), etc..
(3) Cross-lingual Sentence Regularizer, which
models cross-lingual co-occurrence at sentence
level in order to learn translated words to have
most similar embeddings. For example, English
word basketball and the translated Chinese word
篮球 frequently co-occur in a pair of comparable
sentences, therefore, their vector representations
shall be close in the semantic space. The above
components are trained jointly under a unified
objective function.

4 Cross-lingual Supervision Data
Generation

This section introduces how to build a bilingual
entity network Gen−zh and comparable sentences
Sen−zh from a multi-lingual KB.

4.1 Bilingual Entity Network Construction
Entities with cross-lingual links refer to the same
thing, which implies they are equivalent across
languages. Conventional knowledge representa-
tion methods only add edges between eeni and
ezhi′ indicating a special “equivalent” relation (Zhu
et al., 2017). Instead, we build Gen−zh = (Een ∪
Ezh,Ren∪Rzh∪R̃en−zh) by enriching the neigh-
bors of cross-lingual linked entities. That is, we
add edges R̃en−zh between two mono-lingual ENs
by letting all neighbors of eeni be neighbors of ezhi′ ,
and vice versa, if ⟨eeni , ezhi′ ⟩ ∈ Ren−zh.
Gen−zh extends Gen and Gzh to bilingual set-

tings in a natural way. It not only keeps a con-
sistent objective in mono-lingual ENs—entities,
no matter in which language, will be embedded
closely if share common neighbors—but also en-
hances each other with more neighbors in the for-
eign language.
Following the method in Zhu et al. (2017), there

will be no edge between Chinese entity 福斯特
(Foust) and English entity Pistons, which implies
a wrong fact that 福斯特 (Foust) does not belong
to Pistons. Our method enriches the missing rela-
tion between entities 福斯特 (Foust) and 活塞队
(Pistons) in incomplete Chinese KB through cor-
responding English common neighbors, Allstar,
NBA, etc., as illustrated in Figure 1.

4.2 Comparable Sentences Generation
To supervise the cross-lingual representation
learning of words, we automatically generate com-
parable sentences as cross-lingual training data.
Comparable sentences are not translated paired
sentences, but sentences with the same topic in dif-
ferent languages. As shown in the middle layer
(Figure 1), the pair of sentences are comparable
sentences: (1) “Lawrence Michael Foust was an
American basketball player who spent 12 seasons
in NBA”, (2) “拉里·福斯特 (Lawrence Foust) 是
(was)美国 (American) NBA联盟 (association)的
(of) 前 (former) 职业 (professional) 篮球 (basket-
ball) 运动员 (player)”.
Inspired by the distant supervision technique

in relation extraction, we assume that sentence
senk in Wikipedia articles of entity eeni explicitly
or implicitly describes eeni (Yamada et al., 2017),
and that senk shall express a relation between eeni
and eenj if another entity eenj is in senk . Mean-
while, we find a comparable sentence szhk′ in an-
other language which satisfies szhk′ containing ezhj′



231

in Wikipedia articles of Chinese entity ezhi′ , where
⟨eeni , ezhi′ ⟩, ⟨eenj , ezhj′ ⟩ ∈ Ren−zh. As shown in Fig-
ure 1, the sentences in the second level are compa-
rable due to the similar theme of the relation be-
tween entity Foust and NBA. To find this type of
sentences, we search the anchors in the English
aritcle and Chinese article of cross-lingual entity
Foust, respectively, and extract the sentences in-
cluding another crosslingual entity NBA. Compa-
rable sentences can be regarded as cross-lingual
contexts.
Unfortunately, comparable sentences suffer

from two issues caused by distant supervision:
Wrong labelling. Take English as sample, there
may be several sentences senk,l|Ll=1 containing the
same entity eenj in the article of eeni . A straightfor-
ward solution is to concatenate them into a longer
sentence senk , but this increases the chance to in-
clude unrelated sentences.
Unbalanced information. Sometimes the pair
of sentences convey unbalanced information, e.g.,
the English sentence in the middle layer (Figure 1)
contains Foust spent 12 seasons in NBA while the
comparable Chinese sentence not.
To address the issues, we propose knowledge at-

tention and cross-lingual attention to filter out un-
related information at sentence level, and at word
level respectively.

5 Joint Representation Learning

As shown in Figure 2, there are three components
in learning cross-lingual word and entity represen-
tations, which are trained jointly. In this section,
we will describe them in detail.

5.1 Mono-lingual Representation Learning
Following Yamada et al. (2016); Cao et al. (2017),
we learn mono-lingual word/entity embeddings
based on corpus Dy, anchors Ay and entity net-
work Gy. Capturing the cooccurrence information
among words and entities, these embeddings serve
as the foundation and will be further extended to
bilingual settings using the proposed cross-lingual
regularizers, which will be detailed in the next sec-
tion. Monolingually, we utilize a variant of Skip-
gram model (Mikolov et al., 2013c) to predict the
contexts given current word/entity:

Lm =
∑

y∈{en,zh}

∑
xyi ∈{Dy ,Ay ,Gy}

logP (C(xyi )|x
y
i )

where xyi is either a word or an entity, and C(x
y
i )

denotes: (i) contextual words in a pre-defined win-
dow of xyi if x

y
i ∈ Dy, (ii) neighbor entities that

linked to xyi if x
y
i ∈ Gy, (iii) contextual words of

wyj if x
y
i is entity e

y
i in an anchor ⟨w

y
j , e

y
i ⟩ ∈ Ay.

5.2 Cross-lingual Entity Regularizer
The bilingual EN Gen−zh merges entities in dif-
ferent languages into a unified network, resulting
in the possibility of using the same objective as
in mono-lingual ENs. Thus, we naturally extend
mono-lingual function to cross-lingual settings:

Le =
∑

eyi ∈{Gen−zh}

logP (C′(eyi )|e
y
i )

where C′(eyi ) denotes cross-lingual contexts—
neighbor entities in different languages that linked
to eyi . Thus, by jointly learning mono-lingual rep-
resentation with cross-lingual entity regularizer,
words and entities share more common contexts,
and will have similar embeddings. As shown in
Figure 1, English entityNBA co-occurs with words
basketball and player in texts, so they are embed-
ded closely in the semantic space. Meanwhile,
cross-lingual linked entities NBA and NBA (zh)
have similar representations due to the most com-
mon neighbor entities, e.g., Foust.

5.3 Cross-lingual Sentence Regularizer
Comparable sentences provide cross-lingual co-
occurrence of words, thus, we can use them to
learn similar embeddings for the words that fre-
quently co-occur by minimizing the Euclidean dis-
tance as follows:

Ls =
∑

⟨senk ,s
zh
k′ ⟩∈S

en−zh

||senk − szhk′ ||2

where senk , szhk′ are sentence embeddings. Take En-
glish as sample language, we define it as the aver-
age sum of word vectors weighted by the combi-
nation of two types of attentions:

senk =
L∑
l=1

ψ(eenm , s
en
k,l)

∑
weni ∈senk,l

ψ′(weni , w
zh
j )weni

where senk,l|Ll=1 are sentences containing the
same entity (as mentioned in Section 4.2), and
ψ(eenm , s

en
k,l) is knowledge attention that aims



232

Cross-lingual
Sentence Regularizer

Cross-lingual Entity RegularizereNBAeAllstar

500

501

502

503

504

505

506

507

508

509

510

511

512

513

514

515

516

517

518

519

520

521

522

523

524

525

526

527

528

529

530

531

532

533

534

535

536

537

538

539

540

541

542

543

544

545

546

547

548

549

550

551

552

553

554

555

556

557

558

559

560

561

562

563

564

565

566

567

568

569

570

571

572

573

574

575

576

577

578

579

580

581

582

583

584

585

586

587

588

589

590

591

592

593

594

595

596

597

598

599

ACL 2018 Submission ***. Confidential Review Copy. DO NOT DISTRIBUTE.

ψ(ei, sk,l) ∝ sim(ei,
∑

wm∈sk,l

wm)

ψ(ei, sk,l) l
sk ei

ψ(ei, sk,l) = 1
|sk| = 1

Cross-lingual Attention

ψ′(wem, w
z
n) ∝

wem∈sek,wzn∈s
z
k

sim(wem, w
z
n)

American basketball player

美国 篮球 运动员 12 sea-
sons sek 前 (former) s

z
k

sek =
∑

sek,l∈s
e
k

ψ(eei , s
e
k,l)

∑

wem∈sek,l

ψ′(wem, w
z
n)wem

5.4 Training

L = Lm + Le + γLs

γ

6 Experiments

6.1 Experiment Settings

E R

5

400

401

402

403

404

405

406

407

408

409

410

411

412

413

414

415

416

417

418

419

420

421

422

423

424

425

426

427

428

429

430

431

432

433

434

435

436

437

438

439

440

441

442

443

444

445

446

447

448

449

450

451

452

453

454

455

456

457

458

459

460

461

462

463

464

465

466

467

468

469

470

471

472

473

474

475

476

477

478

479

480

481

482

483

484

485

486

487

488

489

490

491

492

493

494

495

496

497

498

499

ACL 2018 Submission ***. Confidential Review Copy. DO NOT DISTRIBUTE.

A straightforward solution is to concatenate them
into a longer sentence senk , but this increases the
chance to include unrelated sentences.
Unbalanced information. Sometimes the pair of
sentences convey different information, e.g., the
English sentence in layer 2 (Figure 1) contains
Foust spent 12 seasons in NBA while the compa-
rable Chinese sentence not.
To address the issues, we propose knowledge at-

tention and cross-lingual attention to filter out un-
related information at sentence level and at word
level, respectively. [[这里感觉改动较大]]

5 Joint Representation Learning

5.1 Mono-lingual Representation Learning
Following (Yamada et al., 2016; Cao et al., 2017),
we learn mono-lingual word/entity embeddings
based on corpus Dy, anchors Ay and entity net-
work Gy. We utilize a variant of Skip-gram
model (Mikolov et al., 2013c) to predict the con-
texts given current word/entity:

Lm =
∑

y∈{en,zh}

∑

xyi ∈{Dy ,Ay ,Gy}

logP (C(xyi )|x
y
i )

(1)
where xyi is either a word or an entity, and C(x

y
i )

denotes: (i) contextual words in a pre-defined win-
dow of xyi if x

y
i ∈ Dy, (ii) neighbor entities that

linked to xyi if x
y
i ∈ Gy, (iii) contextual words of

wyj if x
y
i is entity e

y
i in an anchor ⟨w

y
j , e

y
i ⟩ ∈ Ay.

5.2 Cross-lingual Entity Regularizer
The bilingual EN Gen−zh merges entities in dif-
ferent languages into a unified network, resulting
in the possibility of using the same objective as
in mono-lingual ENs. Thus, we naturally extend
mono-lingual function to cross-lingual settings:

Le =
∑

eyi ∈{Gen−zh}

logP (C′(eyi )|e
y
i ) (2)

where C′(eyi ) denotes cross-lingual contexts—
neighbor entities in different languages that linked
to eyi . Thus, by jointly learning mono-lingual rep-
resentation with cross-lingual entity regularizer,
words and entities share more common contexts,
and will have similar embeddings. As shown in
Figure 1, English entityNBA co-occurs with words
basketball and player in texts, so they are embed-
ded close in the semantic space. Meanwhile, cross-
lingual linked entitiesNBA andNBA (zh) have sim-
ilar representations due to themost common neigh-
bor entities, e.g., Foust.

5.3 Cross-lingual Sentence Regularizer
Comparable sentences provide cross-lingual co-
occurrence of words, thus, we learn similar em-
beddings for the words that frequently co-occur to-
gether by minimizing the Euclidean distance:

Ls =
∑

⟨senk ,s
zh
k′ ⟩∈S

en−zh

||senk − szhk′ ||2 (3)

where senk , s
zh
k′ are sentence embeddings. Take En-

glish as sample language, we define it as the aver-
age sum of word vectors weighted by the combi-
nation of two types of attentions:

senk =
∑

l∈L
ψ(eenm , s

en
k,l)

∑

weni ∈senk,l

ψ′(weni , w
zh
j )weni

(4)
where {senk,l|l ∈ L} is a set of sentences con-
taining the same entity (as mentioned in Sec-
tion 4.2), and ψ(eenm , senk,l) is knowledge attention
that aims at filter out wrong labelling sentences,
and ψ′(weni , wzhj ) is cross-lingual attention to deal
with the unbalanced information through possible
aligned words.

Knowledge Attention

Suppose that sentences {senk,l|l ∈ L} contain the
same entities in articles of entity eym, the wrong la-
belling errors increase because some of them are
almost irrelevant to eym. Knowledge attention aims
at filtering out wrong labelled sentences through
smaller weights and related sentences with higher
weights. Thus, we define it proportional to the sim-
ilarity between syk,l and e

y
m:

ψ(eym, s
y
k,l) ∝ sim(e

y
m,

∑

wyi ∈s
y
k,l

wyi ) (5)

where sim is similarity measurement, and we
use cosine similarity in the rest of the pa-
per. We normalize knowledge attention such that∑L

l ψ(e
y
m, s

y
k,l) = 1.

Cross-lingual Attention

Inspired by self-attention mechanism (Lin et al.,
2017b), we motivate cross-lingual attention focus-
ing on potential information from comparable sen-
tences themselves. The intuition is to find possible
alignedwords between languages, and filter out the
words without alignments. We define it according
to the maximum similarity:

Mono-lingual Representation Learning

Zh

1

000

001

002

003

004

005

006

007

008

009

010

011

012

013

014

015

016

017

018

019

020

021

022

023

024

025

026

027

028

029

030

031

032

033

034

035

036

037

038

039

040

041

042

043

044

045

046

047

048

049

050

051

052

053

054

055

056

057

058

059

060

061

062

063

064

065

066

067

068

069

070

071

072

073

074

075

076

077

078

079

080

081

082

083

084

085

086

087

088

089

090

091

092

093

094

095

096

097

098

099

ACL 2018 Submission ***. Confidential Review Copy. DO NOT DISTRIBUTE.

Joint Representation Learning of Cross-lingual Words and Entities via
Attentive Distant Supervision

Anonymous ACL submission

Abstract

Jointly learning word and entity represen-
tations benefits many NLP tasks, while has
not been well explored in cross-lingual set-
tings. In this paper, we propose a novel
method that integrates cross-lingual word
and entity representation learning to enable
joint inference among knowledge base and
text across languages, capturing mutually
complementary knowledge. Instead of re-
liance on parallel data, we automatically
generate cross-lingual training data via dis-
tant supervision over multi-lingual knowl-
edge bases. We also propose two types
of knowledge attention and cross-lingual
attention to select the most informative
words and filter out noise, which will fur-
ther improve the performance. In exper-
iments, separate tasks of word translation
and entity relatedness demonstrate the ef-
fectiveness of our method with an average
gain of 20% and 3% over baselines, re-
spectively. Using entity linking as a case
study, the results on benchmark dataset
verify the quality of our embeddings.

1 Introduction

Multi-lingual knowledge bases (KB), storing mil-
lions of entities and their facts in various lan-
guages, provide rich structured knowledge for un-
derstanding natural language beyond texts. Mean-
while, abundant text corpus contains large amount
of potential knowledge complementary to existing
KBs. Therefore, researchers leverage both types
of resources to improve various natural language
processing (NLP) related tasks, such as relation ex-
traction (Weston et al., 2013; Lin et al., 2017), and
entity linking (Tsai and Roth, 2016; Yamada et al.,
2016; Cao et al., 2017; Ji et al., 2016).

Most existing work jointly models KB and text
corpus to enhance each other by learning word and
entity representations in a unified vector space. For
example, (Wang et al., 2014; Yamada et al., 2016;
Cao et al., 2017) utilize the coherence informa-
tion to align similar words and entities with sim-
ilar embedding vectors. Another approach in (Han
et al., 2016; Toutanova et al., 2015; Wu et al.,
2016) learns to represent entities based on their
textual descriptions together with the structured re-
lations. However, these methods only focus on
mono-lingual settings, and few researches have
been done in cross-lingual scenarios.
In this paper, we propose to learn cross-lingual

word and entity representations in the same seman-
tic space, to enable joint inference among KB and
text across languages without any additional trans-
lation mechanism, which is usually expensive and
may introduce inevitable errors. Our embeddings
are helpful to break down language gaps in many
tasks, such as cross-lingual entity linking, in which
the major challenge lies in measuring the similar-
ity between entities and corresponding mentioned
words in different languages.
e , w ,w
The intuition is that, words and entities in

various languages share some common semantic
meanings1, but there are also ways in which they
differ. On one hand, we utilize their shared seman-
tics to align similar words and entities with simi-
lar embedding vectors, no matter they are in the
same language or not. On the other hand, cross-
lingual embeddings will benefit from different lan-
guages due to the complementary knowledge. For
instance, textual ambiguity in one language may
disappear in another language, e.g., the two mean-

1Some cross-lingual pioneering work observe that word
embeddings trained separately on monolingual corpora ex-
hibit isomorphic structure across languages (Mikolov et al.,
2013; Zhang et al., 2017).

1

000

001

002

003

004

005

006

007

008

009

010

011

012

013

014

015

016

017

018

019

020

021

022

023

024

025

026

027

028

029

030

031

032

033

034

035

036

037

038

039

040

041

042

043

044

045

046

047

048

049

050

051

052

053

054

055

056

057

058

059

060

061

062

063

064

065

066

067

068

069

070

071

072

073

074

075

076

077

078

079

080

081

082

083

084

085

086

087

088

089

090

091

092

093

094

095

096

097

098

099

ACL 2018 Submission ***. Confidential Review Copy. DO NOT DISTRIBUTE.

Joint Representation Learning of Cross-lingual Words and Entities via
Attentive Distant Supervision

Anonymous ACL submission

Abstract

Jointly learning word and entity represen-
tations benefits many NLP tasks, while has
not been well explored in cross-lingual set-
tings. In this paper, we propose a novel
method that integrates cross-lingual word
and entity representation learning to enable
joint inference among knowledge base and
text across languages, capturing mutually
complementary knowledge. Instead of re-
liance on parallel data, we automatically
generate cross-lingual training data via dis-
tant supervision over multi-lingual knowl-
edge bases. We also propose two types
of knowledge attention and cross-lingual
attention to select the most informative
words and filter out noise, which will fur-
ther improve the performance. In exper-
iments, separate tasks of word translation
and entity relatedness demonstrate the ef-
fectiveness of our method with an average
gain of 20% and 3% over baselines, re-
spectively. Using entity linking as a case
study, the results on benchmark dataset
verify the quality of our embeddings.

1 Introduction

Multi-lingual knowledge bases (KB), storing mil-
lions of entities and their facts in various lan-
guages, provide rich structured knowledge for un-
derstanding natural language beyond texts. Mean-
while, abundant text corpus contains large amount
of potential knowledge complementary to existing
KBs. Therefore, researchers leverage both types
of resources to improve various natural language
processing (NLP) related tasks, such as relation ex-
traction (Weston et al., 2013; Lin et al., 2017), and
entity linking (Tsai and Roth, 2016; Yamada et al.,
2016; Cao et al., 2017; Ji et al., 2016).

Most existing work jointly models KB and text
corpus to enhance each other by learning word and
entity representations in a unified vector space. For
example, (Wang et al., 2014; Yamada et al., 2016;
Cao et al., 2017) utilize the coherence informa-
tion to align similar words and entities with sim-
ilar embedding vectors. Another approach in (Han
et al., 2016; Toutanova et al., 2015; Wu et al.,
2016) learns to represent entities based on their
textual descriptions together with the structured re-
lations. However, these methods only focus on
mono-lingual settings, and few researches have
been done in cross-lingual scenarios.
In this paper, we propose to learn cross-lingual

word and entity representations in the same seman-
tic space, to enable joint inference among KB and
text across languages without any additional trans-
lation mechanism, which is usually expensive and
may introduce inevitable errors. Our embeddings
are helpful to break down language gaps in many
tasks, such as cross-lingual entity linking, in which
the major challenge lies in measuring the similar-
ity between entities and corresponding mentioned
words in different languages.
eNBA , w , w ,w
The intuition is that, words and entities in

various languages share some common semantic
meanings1, but there are also ways in which they
differ. On one hand, we utilize their shared seman-
tics to align similar words and entities with simi-
lar embedding vectors, no matter they are in the
same language or not. On the other hand, cross-
lingual embeddings will benefit from different lan-
guages due to the complementary knowledge. For
instance, textual ambiguity in one language may
disappear in another language, e.g., the two mean-

1Some cross-lingual pioneering work observe that word
embeddings trained separately on monolingual corpora ex-
hibit isomorphic structure across languages (Mikolov et al.,
2013; Zhang et al., 2017).

1

000

001

002

003

004

005

006

007

008

009

010

011

012

013

014

015

016

017

018

019

020

021

022

023

024

025

026

027

028

029

030

031

032

033

034

035

036

037

038

039

040

041

042

043

044

045

046

047

048

049

050

051

052

053

054

055

056

057

058

059

060

061

062

063

064

065

066

067

068

069

070

071

072

073

074

075

076

077

078

079

080

081

082

083

084

085

086

087

088

089

090

091

092

093

094

095

096

097

098

099

ACL 2018 Submission ***. Confidential Review Copy. DO NOT DISTRIBUTE.

Joint Representation Learning of Cross-lingual Words and Entities via
Attentive Distant Supervision

Anonymous ACL submission

Abstract

Jointly learning word and entity represen-
tations benefits many NLP tasks, while has
not been well explored in cross-lingual set-
tings. In this paper, we propose a novel
method that integrates cross-lingual word
and entity representation learning to enable
joint inference among knowledge base and
text across languages, capturing mutually
complementary knowledge. Instead of re-
liance on parallel data, we automatically
generate cross-lingual training data via dis-
tant supervision over multi-lingual knowl-
edge bases. We also propose two types
of knowledge attention and cross-lingual
attention to select the most informative
words and filter out noise, which will fur-
ther improve the performance. In exper-
iments, separate tasks of word translation
and entity relatedness demonstrate the ef-
fectiveness of our method with an average
gain of 20% and 3% over baselines, re-
spectively. Using entity linking as a case
study, the results on benchmark dataset
verify the quality of our embeddings.

1 Introduction

Multi-lingual knowledge bases (KB), storing mil-
lions of entities and their facts in various lan-
guages, provide rich structured knowledge for un-
derstanding natural language beyond texts. Mean-
while, abundant text corpus contains large amount
of potential knowledge complementary to existing
KBs. Therefore, researchers leverage both types
of resources to improve various natural language
processing (NLP) related tasks, such as relation ex-
traction (Weston et al., 2013; Lin et al., 2017), and
entity linking (Tsai and Roth, 2016; Yamada et al.,
2016; Cao et al., 2017; Ji et al., 2016).

Most existing work jointly models KB and text
corpus to enhance each other by learning word and
entity representations in a unified vector space. For
example, (Wang et al., 2014; Yamada et al., 2016;
Cao et al., 2017) utilize the coherence informa-
tion to align similar words and entities with sim-
ilar embedding vectors. Another approach in (Han
et al., 2016; Toutanova et al., 2015; Wu et al.,
2016) learns to represent entities based on their
textual descriptions together with the structured re-
lations. However, these methods only focus on
mono-lingual settings, and few researches have
been done in cross-lingual scenarios.
In this paper, we propose to learn cross-lingual

word and entity representations in the same seman-
tic space, to enable joint inference among KB and
text across languages without any additional trans-
lation mechanism, which is usually expensive and
may introduce inevitable errors. Our embeddings
are helpful to break down language gaps in many
tasks, such as cross-lingual entity linking, in which
the major challenge lies in measuring the similar-
ity between entities and corresponding mentioned
words in different languages.
eNBA , w , w ,w
The intuition is that, words and entities in

various languages share some common semantic
meanings1, but there are also ways in which they
differ. On one hand, we utilize their shared seman-
tics to align similar words and entities with simi-
lar embedding vectors, no matter they are in the
same language or not. On the other hand, cross-
lingual embeddings will benefit from different lan-
guages due to the complementary knowledge. For
instance, textual ambiguity in one language may
disappear in another language, e.g., the two mean-

1Some cross-lingual pioneering work observe that word
embeddings trained separately on monolingual corpora ex-
hibit isomorphic structure across languages (Mikolov et al.,
2013; Zhang et al., 2017).

1

000

001

002

003

004

005

006

007

008

009

010

011

012

013

014

015

016

017

018

019

020

021

022

023

024

025

026

027

028

029

030

031

032

033

034

035

036

037

038

039

040

041

042

043

044

045

046

047

048

049

050

051

052

053

054

055

056

057

058

059

060

061

062

063

064

065

066

067

068

069

070

071

072

073

074

075

076

077

078

079

080

081

082

083

084

085

086

087

088

089

090

091

092

093

094

095

096

097

098

099

ACL 2018 Submission ***. Confidential Review Copy. DO NOT DISTRIBUTE.

Joint Representation Learning of Cross-lingual Words and Entities via
Attentive Distant Supervision

Anonymous ACL submission

Abstract

Jointly learning word and entity represen-
tations benefits many NLP tasks, while has
not been well explored in cross-lingual set-
tings. In this paper, we propose a novel
method that integrates cross-lingual word
and entity representation learning to enable
joint inference among knowledge base and
text across languages, capturing mutually
complementary knowledge. Instead of re-
liance on parallel data, we automatically
generate cross-lingual training data via dis-
tant supervision over multi-lingual knowl-
edge bases. We also propose two types
of knowledge attention and cross-lingual
attention to select the most informative
words and filter out noise, which will fur-
ther improve the performance. In exper-
iments, separate tasks of word translation
and entity relatedness demonstrate the ef-
fectiveness of our method with an average
gain of 20% and 3% over baselines, re-
spectively. Using entity linking as a case
study, the results on benchmark dataset
verify the quality of our embeddings.

1 Introduction

Multi-lingual knowledge bases (KB), storing mil-
lions of entities and their facts in various lan-
guages, provide rich structured knowledge for un-
derstanding natural language beyond texts. Mean-
while, abundant text corpus contains large amount
of potential knowledge complementary to existing
KBs. Therefore, researchers leverage both types
of resources to improve various natural language
processing (NLP) related tasks, such as relation ex-
traction (Weston et al., 2013; Lin et al., 2017), and
entity linking (Tsai and Roth, 2016; Yamada et al.,
2016; Cao et al., 2017; Ji et al., 2016).

Most existing work jointly models KB and text
corpus to enhance each other by learning word and
entity representations in a unified vector space. For
example, (Wang et al., 2014; Yamada et al., 2016;
Cao et al., 2017) utilize the coherence informa-
tion to align similar words and entities with sim-
ilar embedding vectors. Another approach in (Han
et al., 2016; Toutanova et al., 2015; Wu et al.,
2016) learns to represent entities based on their
textual descriptions together with the structured re-
lations. However, these methods only focus on
mono-lingual settings, and few researches have
been done in cross-lingual scenarios.
In this paper, we propose to learn cross-lingual

word and entity representations in the same seman-
tic space, to enable joint inference among KB and
text across languages without any additional trans-
lation mechanism, which is usually expensive and
may introduce inevitable errors. Our embeddings
are helpful to break down language gaps in many
tasks, such as cross-lingual entity linking, in which
the major challenge lies in measuring the similar-
ity between entities and corresponding mentioned
words in different languages.
eNBA , w , w ,w
The intuition is that, words and entities in

various languages share some common semantic
meanings1, but there are also ways in which they
differ. On one hand, we utilize their shared seman-
tics to align similar words and entities with simi-
lar embedding vectors, no matter they are in the
same language or not. On the other hand, cross-
lingual embeddings will benefit from different lan-
guages due to the complementary knowledge. For
instance, textual ambiguity in one language may
disappear in another language, e.g., the two mean-

1Some cross-lingual pioneering work observe that word
embeddings trained separately on monolingual corpora ex-
hibit isomorphic structure across languages (Mikolov et al.,
2013; Zhang et al., 2017).

1

000

001

002

003

004

005

006

007

008

009

010

011

012

013

014

015

016

017

018

019

020

021

022

023

024

025

026

027

028

029

030

031

032

033

034

035

036

037

038

039

040

041

042

043

044

045

046

047

048

049

050

051

052

053

054

055

056

057

058

059

060

061

062

063

064

065

066

067

068

069

070

071

072

073

074

075

076

077

078

079

080

081

082

083

084

085

086

087

088

089

090

091

092

093

094

095

096

097

098

099

ACL 2018 Submission ***. Confidential Review Copy. DO NOT DISTRIBUTE.

Joint Representation Learning of Cross-lingual Words and Entities via
Attentive Distant Supervision

Anonymous ACL submission

Abstract

Jointly learning word and entity represen-
tations benefits many NLP tasks, while has
not been well explored in cross-lingual set-
tings. In this paper, we propose a novel
method that integrates cross-lingual word
and entity representation learning to enable
joint inference among knowledge base and
text across languages, capturing mutually
complementary knowledge. Instead of re-
liance on parallel data, we automatically
generate cross-lingual training data via dis-
tant supervision over multi-lingual knowl-
edge bases. We also propose two types
of knowledge attention and cross-lingual
attention to select the most informative
words and filter out noise, which will fur-
ther improve the performance. In exper-
iments, separate tasks of word translation
and entity relatedness demonstrate the ef-
fectiveness of our method with an average
gain of 20% and 3% over baselines, re-
spectively. Using entity linking as a case
study, the results on benchmark dataset
verify the quality of our embeddings.

1 Introduction

Multi-lingual knowledge bases (KB), storing mil-
lions of entities and their facts in various lan-
guages, provide rich structured knowledge for un-
derstanding natural language beyond texts. Mean-
while, abundant text corpus contains large amount
of potential knowledge complementary to existing
KBs. Therefore, researchers leverage both types
of resources to improve various natural language
processing (NLP) related tasks, such as relation ex-
traction (Weston et al., 2013; Lin et al., 2017), and
entity linking (Tsai and Roth, 2016; Yamada et al.,
2016; Cao et al., 2017; Ji et al., 2016).

Most existing work jointly models KB and text
corpus to enhance each other by learning word and
entity representations in a unified vector space. For
example, (Wang et al., 2014; Yamada et al., 2016;
Cao et al., 2017) utilize the coherence informa-
tion to align similar words and entities with sim-
ilar embedding vectors. Another approach in (Han
et al., 2016; Toutanova et al., 2015; Wu et al.,
2016) learns to represent entities based on their
textual descriptions together with the structured re-
lations. However, these methods only focus on
mono-lingual settings, and few researches have
been done in cross-lingual scenarios.
In this paper, we propose to learn cross-lingual

word and entity representations in the same seman-
tic space, to enable joint inference among KB and
text across languages without any additional trans-
lation mechanism, which is usually expensive and
may introduce inevitable errors. Our embeddings
are helpful to break down language gaps in many
tasks, such as cross-lingual entity linking, in which
the major challenge lies in measuring the similar-
ity between entities and corresponding mentioned
words in different languages.
eNBA , w , w ,w
The intuition is that, words and entities in

various languages share some common semantic
meanings1, but there are also ways in which they
differ. On one hand, we utilize their shared seman-
tics to align similar words and entities with simi-
lar embedding vectors, no matter they are in the
same language or not. On the other hand, cross-
lingual embeddings will benefit from different lan-
guages due to the complementary knowledge. For
instance, textual ambiguity in one language may
disappear in another language, e.g., the two mean-

1Some cross-lingual pioneering work observe that word
embeddings trained separately on monolingual corpora ex-
hibit isomorphic structure across languages (Mikolov et al.,
2013; Zhang et al., 2017).

1

000

001

002

003

004

005

006

007

008

009

010

011

012

013

014

015

016

017

018

019

020

021

022

023

024

025

026

027

028

029

030

031

032

033

034

035

036

037

038

039

040

041

042

043

044

045

046

047

048

049

050

051

052

053

054

055

056

057

058

059

060

061

062

063

064

065

066

067

068

069

070

071

072

073

074

075

076

077

078

079

080

081

082

083

084

085

086

087

088

089

090

091

092

093

094

095

096

097

098

099

ACL 2018 Submission ***. Confidential Review Copy. DO NOT DISTRIBUTE.

Joint Representation Learning of Cross-lingual Words and Entities via
Attentive Distant Supervision

Anonymous ACL submission

Abstract

Jointly learning word and entity represen-
tations benefits many NLP tasks, while has
not been well explored in cross-lingual set-
tings. In this paper, we propose a novel
method that integrates cross-lingual word
and entity representation learning to enable
joint inference among knowledge base and
text across languages, capturing mutually
complementary knowledge. Instead of re-
liance on parallel data, we automatically
generate cross-lingual training data via dis-
tant supervision over multi-lingual knowl-
edge bases. We also propose two types
of knowledge attention and cross-lingual
attention to select the most informative
words and filter out noise, which will fur-
ther improve the performance. In exper-
iments, separate tasks of word translation
and entity relatedness demonstrate the ef-
fectiveness of our method with an average
gain of 20% and 3% over baselines, re-
spectively. Using entity linking as a case
study, the results on benchmark dataset
verify the quality of our embeddings.

1 Introduction

Multi-lingual knowledge bases (KB), storing mil-
lions of entities and their facts in various lan-
guages, provide rich structured knowledge for un-
derstanding natural language beyond texts. Mean-
while, abundant text corpus contains large amount
of potential knowledge complementary to existing
KBs. Therefore, researchers leverage both types
of resources to improve various natural language
processing (NLP) related tasks, such as relation ex-
traction (Weston et al., 2013; Lin et al., 2017), and
entity linking (Tsai and Roth, 2016; Yamada et al.,
2016; Cao et al., 2017; Ji et al., 2016).

Most existing work jointly models KB and text
corpus to enhance each other by learning word and
entity representations in a unified vector space. For
example, (Wang et al., 2014; Yamada et al., 2016;
Cao et al., 2017) utilize the coherence informa-
tion to align similar words and entities with sim-
ilar embedding vectors. Another approach in (Han
et al., 2016; Toutanova et al., 2015; Wu et al.,
2016) learns to represent entities based on their
textual descriptions together with the structured re-
lations. However, these methods only focus on
mono-lingual settings, and few researches have
been done in cross-lingual scenarios.
In this paper, we propose to learn cross-lingual

word and entity representations in the same seman-
tic space, to enable joint inference among KB and
text across languages without any additional trans-
lation mechanism, which is usually expensive and
may introduce inevitable errors. Our embeddings
are helpful to break down language gaps in many
tasks, such as cross-lingual entity linking, in which
the major challenge lies in measuring the similar-
ity between entities and corresponding mentioned
words in different languages.
eNBA
The intuition is that, words and entities in

various languages share some common semantic
meanings1, but there are also ways in which they
differ. On one hand, we utilize their shared seman-
tics to align similar words and entities with simi-
lar embedding vectors, no matter they are in the
same language or not. On the other hand, cross-
lingual embeddings will benefit from different lan-
guages due to the complementary knowledge. For
instance, textual ambiguity in one language may
disappear in another language, e.g., the two mean-

1Some cross-lingual pioneering work observe that word
embeddings trained separately on monolingual corpora ex-
hibit isomorphic structure across languages (Mikolov et al.,
2013; Zhang et al., 2017).

EneNBAeAllstar

wbasketball

wAmerican eFoust

eFoustwbasketballwAmerican

wplayer

1

000

001

002

003

004

005

006

007

008

009

010

011

012

013

014

015

016

017

018

019

020

021

022

023

024

025

026

027

028

029

030

031

032

033

034

035

036

037

038

039

040

041

042

043

044

045

046

047

048

049

050

051

052

053

054

055

056

057

058

059

060

061

062

063

064

065

066

067

068

069

070

071

072

073

074

075

076

077

078

079

080

081

082

083

084

085

086

087

088

089

090

091

092

093

094

095

096

097

098

099

ACL 2018 Submission ***. Confidential Review Copy. DO NOT DISTRIBUTE.

Joint Representation Learning of Cross-lingual Words and Entities via
Attentive Distant Supervision

Anonymous ACL submission

Abstract

Jointly learning word and entity represen-
tations benefits many NLP tasks, while has
not been well explored in cross-lingual set-
tings. In this paper, we propose a novel
method that integrates cross-lingual word
and entity representation learning to enable
joint inference among knowledge base and
text across languages, capturing mutually
complementary knowledge. Instead of re-
liance on parallel data, we automatically
generate cross-lingual training data via dis-
tant supervision over multi-lingual knowl-
edge bases. We also propose two types
of knowledge attention and cross-lingual
attention to select the most informative
words and filter out noise, which will fur-
ther improve the performance. In exper-
iments, separate tasks of word translation
and entity relatedness demonstrate the ef-
fectiveness of our method with an average
gain of 20% and 3% over baselines, re-
spectively. Using entity linking as a case
study, the results on benchmark dataset
verify the quality of our embeddings.

1 Introduction

Multi-lingual knowledge bases (KB), storing mil-
lions of entities and their facts in various lan-
guages, provide rich structured knowledge for un-
derstanding natural language beyond texts. Mean-
while, abundant text corpus contains large amount
of potential knowledge complementary to existing
KBs. Therefore, researchers leverage both types
of resources to improve various natural language
processing (NLP) related tasks, such as relation ex-
traction (Weston et al., 2013; Lin et al., 2017), and
entity linking (Tsai and Roth, 2016; Yamada et al.,
2016; Cao et al., 2017; Ji et al., 2016).

Most existing work jointly models KB and text
corpus to enhance each other by learning word and
entity representations in a unified vector space. For
example, (Wang et al., 2014; Yamada et al., 2016;
Cao et al., 2017) utilize the coherence informa-
tion to align similar words and entities with sim-
ilar embedding vectors. Another approach in (Han
et al., 2016; Toutanova et al., 2015; Wu et al.,
2016) learns to represent entities based on their
textual descriptions together with the structured re-
lations. However, these methods only focus on
mono-lingual settings, and few researches have
been done in cross-lingual scenarios.
In this paper, we propose to learn cross-lingual

word and entity representations in the same seman-
tic space, to enable joint inference among KB and
text across languages without any additional trans-
lation mechanism, which is usually expensive and
may introduce inevitable errors. Our embeddings
are helpful to break down language gaps in many
tasks, such as cross-lingual entity linking, in which
the major challenge lies in measuring the similar-
ity between entities and corresponding mentioned
words in different languages.
e , w ,w
The intuition is that, words and entities in

various languages share some common semantic
meanings1, but there are also ways in which they
differ. On one hand, we utilize their shared seman-
tics to align similar words and entities with simi-
lar embedding vectors, no matter they are in the
same language or not. On the other hand, cross-
lingual embeddings will benefit from different lan-
guages due to the complementary knowledge. For
instance, textual ambiguity in one language may
disappear in another language, e.g., the two mean-

1Some cross-lingual pioneering work observe that word
embeddings trained separately on monolingual corpora ex-
hibit isomorphic structure across languages (Mikolov et al.,
2013; Zhang et al., 2017).

att
att

att

… …
wbasketballwplayer

… …

1

000

001

002

003

004

005

006

007

008

009

010

011

012

013

014

015

016

017

018

019

020

021

022

023

024

025

026

027

028

029

030

031

032

033

034

035

036

037

038

039

040

041

042

043

044

045

046

047

048

049

050

051

052

053

054

055

056

057

058

059

060

061

062

063

064

065

066

067

068

069

070

071

072

073

074

075

076

077

078

079

080

081

082

083

084

085

086

087

088

089

090

091

092

093

094

095

096

097

098

099

ACL 2018 Submission ***. Confidential Review Copy. DO NOT DISTRIBUTE.

Joint Representation Learning of Cross-lingual Words and Entities via
Attentive Distant Supervision

Anonymous ACL submission

Abstract

Jointly learning word and entity represen-
tations benefits many NLP tasks, while has
not been well explored in cross-lingual set-
tings. In this paper, we propose a novel
method that integrates cross-lingual word
and entity representation learning to enable
joint inference among knowledge base and
text across languages, capturing mutually
complementary knowledge. Instead of re-
liance on parallel data, we automatically
generate cross-lingual training data via dis-
tant supervision over multi-lingual knowl-
edge bases. We also propose two types
of knowledge attention and cross-lingual
attention to select the most informative
words and filter out noise, which will fur-
ther improve the performance. In exper-
iments, separate tasks of word translation
and entity relatedness demonstrate the ef-
fectiveness of our method with an average
gain of 20% and 3% over baselines, re-
spectively. Using entity linking as a case
study, the results on benchmark dataset
verify the quality of our embeddings.

1 Introduction

Multi-lingual knowledge bases (KB), storing mil-
lions of entities and their facts in various lan-
guages, provide rich structured knowledge for un-
derstanding natural language beyond texts. Mean-
while, abundant text corpus contains large amount
of potential knowledge complementary to existing
KBs. Therefore, researchers leverage both types
of resources to improve various natural language
processing (NLP) related tasks, such as relation ex-
traction (Weston et al., 2013; Lin et al., 2017), and
entity linking (Tsai and Roth, 2016; Yamada et al.,
2016; Cao et al., 2017; Ji et al., 2016).

Most existing work jointly models KB and text
corpus to enhance each other by learning word and
entity representations in a unified vector space. For
example, (Wang et al., 2014; Yamada et al., 2016;
Cao et al., 2017) utilize the coherence informa-
tion to align similar words and entities with sim-
ilar embedding vectors. Another approach in (Han
et al., 2016; Toutanova et al., 2015; Wu et al.,
2016) learns to represent entities based on their
textual descriptions together with the structured re-
lations. However, these methods only focus on
mono-lingual settings, and few researches have
been done in cross-lingual scenarios.
In this paper, we propose to learn cross-lingual

word and entity representations in the same seman-
tic space, to enable joint inference among KB and
text across languages without any additional trans-
lation mechanism, which is usually expensive and
may introduce inevitable errors. Our embeddings
are helpful to break down language gaps in many
tasks, such as cross-lingual entity linking, in which
the major challenge lies in measuring the similar-
ity between entities and corresponding mentioned
words in different languages.
e , w ,w
The intuition is that, words and entities in

various languages share some common semantic
meanings1, but there are also ways in which they
differ. On one hand, we utilize their shared seman-
tics to align similar words and entities with simi-
lar embedding vectors, no matter they are in the
same language or not. On the other hand, cross-
lingual embeddings will benefit from different lan-
guages due to the complementary knowledge. For
instance, textual ambiguity in one language may
disappear in another language, e.g., the two mean-

1Some cross-lingual pioneering work observe that word
embeddings trained separately on monolingual corpora ex-
hibit isomorphic structure across languages (Mikolov et al.,
2013; Zhang et al., 2017).

1

000

001

002

003

004

005

006

007

008

009

010

011

012

013

014

015

016

017

018

019

020

021

022

023

024

025

026

027

028

029

030

031

032

033

034

035

036

037

038

039

040

041

042

043

044

045

046

047

048

049

050

051

052

053

054

055

056

057

058

059

060

061

062

063

064

065

066

067

068

069

070

071

072

073

074

075

076

077

078

079

080

081

082

083

084

085

086

087

088

089

090

091

092

093

094

095

096

097

098

099

ACL 2018 Submission ***. Confidential Review Copy. DO NOT DISTRIBUTE.

Joint Representation Learning of Cross-lingual Words and Entities via
Attentive Distant Supervision

Anonymous ACL submission

Abstract

Jointly learning word and entity represen-
tations benefits many NLP tasks, while has
not been well explored in cross-lingual set-
tings. In this paper, we propose a novel
method that integrates cross-lingual word
and entity representation learning to enable
joint inference among knowledge base and
text across languages, capturing mutually
complementary knowledge. Instead of re-
liance on parallel data, we automatically
generate cross-lingual training data via dis-
tant supervision over multi-lingual knowl-
edge bases. We also propose two types
of knowledge attention and cross-lingual
attention to select the most informative
words and filter out noise, which will fur-
ther improve the performance. In exper-
iments, separate tasks of word translation
and entity relatedness demonstrate the ef-
fectiveness of our method with an average
gain of 20% and 3% over baselines, re-
spectively. Using entity linking as a case
study, the results on benchmark dataset
verify the quality of our embeddings.

1 Introduction

Multi-lingual knowledge bases (KB), storing mil-
lions of entities and their facts in various lan-
guages, provide rich structured knowledge for un-
derstanding natural language beyond texts. Mean-
while, abundant text corpus contains large amount
of potential knowledge complementary to existing
KBs. Therefore, researchers leverage both types
of resources to improve various natural language
processing (NLP) related tasks, such as relation ex-
traction (Weston et al., 2013; Lin et al., 2017), and
entity linking (Tsai and Roth, 2016; Yamada et al.,
2016; Cao et al., 2017; Ji et al., 2016).

Most existing work jointly models KB and text
corpus to enhance each other by learning word and
entity representations in a unified vector space. For
example, (Wang et al., 2014; Yamada et al., 2016;
Cao et al., 2017) utilize the coherence informa-
tion to align similar words and entities with sim-
ilar embedding vectors. Another approach in (Han
et al., 2016; Toutanova et al., 2015; Wu et al.,
2016) learns to represent entities based on their
textual descriptions together with the structured re-
lations. However, these methods only focus on
mono-lingual settings, and few researches have
been done in cross-lingual scenarios.
In this paper, we propose to learn cross-lingual

word and entity representations in the same seman-
tic space, to enable joint inference among KB and
text across languages without any additional trans-
lation mechanism, which is usually expensive and
may introduce inevitable errors. Our embeddings
are helpful to break down language gaps in many
tasks, such as cross-lingual entity linking, in which
the major challenge lies in measuring the similar-
ity between entities and corresponding mentioned
words in different languages.
e , w ,w
The intuition is that, words and entities in

various languages share some common semantic
meanings1, but there are also ways in which they
differ. On one hand, we utilize their shared seman-
tics to align similar words and entities with simi-
lar embedding vectors, no matter they are in the
same language or not. On the other hand, cross-
lingual embeddings will benefit from different lan-
guages due to the complementary knowledge. For
instance, textual ambiguity in one language may
disappear in another language, e.g., the two mean-

1Some cross-lingual pioneering work observe that word
embeddings trained separately on monolingual corpora ex-
hibit isomorphic structure across languages (Mikolov et al.,
2013; Zhang et al., 2017).

Ls = ||sen � szh||2
第 5章 跨语⾔的词和实体联合表⽰学习

eNBAeAllstar

500

501

502

503

504

505

506

507

508

509

510

511

512

513

514

515

516

517

518

519

520

521

522

523

524

525

526

527

528

529

530

531

532

533

534

535

536

537

538

539

540

541

542

543

544

545

546

547

548

549

550

551

552

553

554

555

556

557

558

559

560

561

562

563

564

565

566

567

568

569

570

571

572

573

574

575

576

577

578

579

580

581

582

583

584

585

586

587

588

589

590

591

592

593

594

595

596

597

598

599

ACL 2018 Submission ***. Confidential Review Copy. DO NOT DISTRIBUTE.

ψ(ei, sk,l) ∝ sim(ei,
∑

wm∈sk,l

wm)

ψ(ei, sk,l) l
sk ei

ψ(ei, sk,l) = 1
|sk| = 1

Cross-lingual Attention

ψ′(wem, w
z
n) ∝

wem∈sek,wzn∈s
z
k

sim(wem, w
z
n)

American basketball player

美国 篮球 运动员 12 sea-
sons sek 前 (former) s

z
k

sek =
∑

sek,l∈s
e
k

ψ(eei , s
e
k,l)

∑

wem∈sek,l

ψ′(wem, w
z
n)wem

5.4 Training

L = Lm + Le + γLs

γ

6 Experiments

6.1 Experiment Settings

E R

5

400

401

402

403

404

405

406

407

408

409

410

411

412

413

414

415

416

417

418

419

420

421

422

423

424

425

426

427

428

429

430

431

432

433

434

435

436

437

438

439

440

441

442

443

444

445

446

447

448

449

450

451

452

453

454

455

456

457

458

459

460

461

462

463

464

465

466

467

468

469

470

471

472

473

474

475

476

477

478

479

480

481

482

483

484

485

486

487

488

489

490

491

492

493

494

495

496

497

498

499

ACL 2018 Submission ***. Confidential Review Copy. DO NOT DISTRIBUTE.

A straightforward solution is to concatenate them
into a longer sentence senk , but this increases the
chance to include unrelated sentences.
Unbalanced information. Sometimes the pair of
sentences convey different information, e.g., the
English sentence in layer 2 (Figure 1) contains
Foust spent 12 seasons in NBA while the compa-
rable Chinese sentence not.
To address the issues, we propose knowledge at-

tention and cross-lingual attention to filter out un-
related information at sentence level and at word
level, respectively. [[这里感觉改动较大]]

5 Joint Representation Learning

5.1 Mono-lingual Representation Learning
Following (Yamada et al., 2016; Cao et al., 2017),
we learn mono-lingual word/entity embeddings
based on corpus Dy, anchors Ay and entity net-
work Gy. We utilize a variant of Skip-gram
model (Mikolov et al., 2013c) to predict the con-
texts given current word/entity:

Lm =
∑

y∈{en,zh}

∑

xyi ∈{Dy ,Ay ,Gy}

logP (C(xyi )|x
y
i )

(1)
where xyi is either a word or an entity, and C(x

y
i )

denotes: (i) contextual words in a pre-defined win-
dow of xyi if x

y
i ∈ Dy, (ii) neighbor entities that

linked to xyi if x
y
i ∈ Gy, (iii) contextual words of

wyj if x
y
i is entity e

y
i in an anchor ⟨w

y
j , e

y
i ⟩ ∈ Ay.

5.2 Cross-lingual Entity Regularizer
The bilingual EN Gen−zh merges entities in dif-
ferent languages into a unified network, resulting
in the possibility of using the same objective as
in mono-lingual ENs. Thus, we naturally extend
mono-lingual function to cross-lingual settings:

Le =
∑

eyi ∈{Gen−zh}

logP (C′(eyi )|e
y
i ) (2)

where C′(eyi ) denotes cross-lingual contexts—
neighbor entities in different languages that linked
to eyi . Thus, by jointly learning mono-lingual rep-
resentation with cross-lingual entity regularizer,
words and entities share more common contexts,
and will have similar embeddings. As shown in
Figure 1, English entityNBA co-occurs with words
basketball and player in texts, so they are embed-
ded close in the semantic space. Meanwhile, cross-
lingual linked entitiesNBA andNBA (zh) have sim-
ilar representations due to themost common neigh-
bor entities, e.g., Foust.

5.3 Cross-lingual Sentence Regularizer
Comparable sentences provide cross-lingual co-
occurrence of words, thus, we learn similar em-
beddings for the words that frequently co-occur to-
gether by minimizing the Euclidean distance:

Ls =
∑

⟨senk ,s
zh
k′ ⟩∈S

en−zh

||senk − szhk′ ||2 (3)

where senk , s
zh
k′ are sentence embeddings. Take En-

glish as sample language, we define it as the aver-
age sum of word vectors weighted by the combi-
nation of two types of attentions:

senk =
∑

l∈L
ψ(eenm , s

en
k,l)

∑

weni ∈senk,l

ψ′(weni , w
zh
j )weni

(4)
where {senk,l|l ∈ L} is a set of sentences con-
taining the same entity (as mentioned in Sec-
tion 4.2), and ψ(eenm , senk,l) is knowledge attention
that aims at filter out wrong labelling sentences,
and ψ′(weni , wzhj ) is cross-lingual attention to deal
with the unbalanced information through possible
aligned words.

Knowledge Attention

Suppose that sentences {senk,l|l ∈ L} contain the
same entities in articles of entity eym, the wrong la-
belling errors increase because some of them are
almost irrelevant to eym. Knowledge attention aims
at filtering out wrong labelled sentences through
smaller weights and related sentences with higher
weights. Thus, we define it proportional to the sim-
ilarity between syk,l and e

y
m:

ψ(eym, s
y
k,l) ∝ sim(e

y
m,

∑

wyi ∈s
y
k,l

wyi ) (5)

where sim is similarity measurement, and we
use cosine similarity in the rest of the pa-
per. We normalize knowledge attention such that∑L

l ψ(e
y
m, s

y
k,l) = 1.

Cross-lingual Attention

Inspired by self-attention mechanism (Lin et al.,
2017b), we motivate cross-lingual attention focus-
ing on potential information from comparable sen-
tences themselves. The intuition is to find possible
alignedwords between languages, and filter out the
words without alignments. We define it according
to the maximum similarity:

4

300

301

302

303

304

305

306

307

308

309

310

311

312

313

314

315

316

317

318

319

320

321

322

323

324

325

326

327

328

329

330

331

332

333

334

335

336

337

338

339

340

341

342

343

344

345

346

347

348

349

350

351

352

353

354

355

356

357

358

359

360

361

362

363

364

365

366

367

368

369

370

371

372

373

374

375

376

377

378

379

380

381

382

383

384

385

386

387

388

389

390

391

392

393

394

395

396

397

398

399

ACL 2018 Submission ***. Confidential Review Copy. DO NOT DISTRIBUTE.

NBA

Lawrence Foust

All-star

[[Lawrence Michael Foust]] was
an American basketball player

who spent 12 seasons in [[NBA]]

NBA (zh)

·
[[ · ]] [[NBA]]

1950 [[NBA ]] 1 5

English KB Chinese KB

独立日 美国独立日

Sen Szh een ezh

L =
∑

xi ∈D

∑

xo ∈C(xi )
P(xo |xi)

L =
∑

(ei ,ej )∈Ec
| |C(ei) − C(e j)| |2

Ec

L =
∑

ei ∈E
P(N (ei)|ei) +

∑

(ei ,ej )∈Ec
P(N (e j)|ei)

N (ei)

L =
∑

(ei ,mi )∈A
P(ei |mi ,C(ei)) +

∑

(ei ,ej )∈Ec
P(e j |mi ,C(ei))

独立日 美国独立日

Sen Szh een ezh

L =
∑

xi ∈D

∑

xo ∈C(xi )
P(xo |xi)

L =
∑

(ei ,ej )∈Ec
| |C(ei) − C(e j)| |2

Ec

L =
∑

ei ∈E
P(N (ei)|ei) +

∑

(ei ,ej )∈Ec
P(N (e j)|ei)

N (ei)

L =
∑

(ei ,mi )∈A
P(ei |mi ,C(ei)) +

∑

(ei ,ej )∈Ec
P(e j |mi ,C(ei))

was

Lawrence
NBA

NBA (zh)

basketball

Semantic Space

Representation
Learning

Cross-lingual
Sentence Regularizer

Cross-lingual
Entity Regularizer

[[ · ]] [[NBA]]

All-star

[[Lawrence Michael Foust]] was an American basketball 
player who spent 12 seasons in [[NBA]]

·

NBA
NBA (zh)

Comparable Sentences

Bi-lingual EN

att
att

att

… …

Ls = | |Sen − Szh | |2

L =
∑

xi ∈D

∑

xo ∈C(xi )
P(xo |xi )

L =
∑

(ei ,ej )∈Ec
| |C(ei ) − C(ej )| |2

Ec

L =
∑

ei ∈E
P(N (ei )|ei ) +

∑

(ei ,ej )∈Ec
P(N (ej )|ei )

N (ei )

L =
∑

(ei ,mi )∈A
P(ei |mi ,C(ei )) +

∑

(ei ,ej )∈Ec
P(ej |mi ,C(ei ))

=

500

501

502

503

504

505

506

507

508

509

510

511

512

513

514

515

516

517

518

519

520

521

522

523

524

525

526

527

528

529

530

531

532

533

534

535

536

537

538

539

540

541

542

543

544

545

546

547

548

549

550

551

552

553

554

555

556

557

558

559

560

561

562

563

564

565

566

567

568

569

570

571

572

573

574

575

576

577

578

579

580

581

582

583

584

585

586

587

588

589

590

591

592

593

594

595

596

597

598

599

ACL 2018 Submission ***. Confidential Review Copy. DO NOT DISTRIBUTE.

skm ≈
wki +

∑
C(wki)

Ls =
∑

<skm,slm>∈Sk,l

|| − ||2

ekm
ekm

k

αnkm ∝ sim( ,
∑

wki∈snkm

)

αnkm
n skm

ekm αnkm

αki,lj ∝
wki∈skm,wlj∈slm

sim(wki, wlj)

=
∑

snkm∈skm

αnkm
∑

wki∈snkm

αki,lj

L = Lc + γLs

γ

400

401

402

403

404

405

406

407

408

409

410

411

412

413

414

415

416

417

418

419

420

421

422

423

424

425

426

427

428

429

430

431

432

433

434

435

436

437

438

439

440

441

442

443

444

445

446

447

448

449

450

451

452

453

454

455

456

457

458

459

460

461

462

463

464

465

466

467

468

469

470

471

472

473

474

475

476

477

478

479

480

481

482

483

484

485

486

487

488

489

490

491

492

493

494

495

496

497

498

499

ACL 2018 Submission ***. Confidential Review Copy. DO NOT DISTRIBUTE.

eki elj < eki, elj >∈ Rc

een,Kobe
ezh,Kobe

een,Joe ezh,Joe

< een,Kobe, ezh,Kobe >
< een,Joe, ezh,Joe >

D A
KN

Lc =
∑

xi∈{D,A,KN}

P (C(xi)|xi)

xi
C(xi)

C(xi)
xi

xi ∈ D
xi ∈ KN

wj xi
< wj , ei >∈ A

een,Joe wen,player
wen,NBA

een,LosAngelesLakers
een,Joe een,Kobe ezh,Kobe

wki
Cl(wki)

Ls =
∑

<skm,slm∈Sk,l

∑

wki∈skm

||wki −
∑

Cl(wki)||2

< skm, slm >∈ Sk,l m
k l

Cross-lingual
 supervision

Data Generation

and was an 8-time [[All-star]] … [[ ]] …

NBA

Lawrence Foust

NBA

player

American
NBA

All-star
eLawrence

wbasketballwplayer
… …

1

000

001

002

003

004

005

006

007

008

009

010

011

012

013

014

015

016

017

018

019

020

021

022

023

024

025

026

027

028

029

030

031

032

033

034

035

036

037

038

039

040

041

042

043

044

045

046

047

048

049

050

051

052

053

054

055

056

057

058

059

060

061

062

063

064

065

066

067

068

069

070

071

072

073

074

075

076

077

078

079

080

081

082

083

084

085

086

087

088

089

090

091

092

093

094

095

096

097

098

099

ACL 2018 Submission ***. Confidential Review Copy. DO NOT DISTRIBUTE.

Joint Representation Learning of Cross-lingual Words and Entities via
Attentive Distant Supervision

Anonymous ACL submission

Abstract

Jointly learning word and entity represen-
tations benefits many NLP tasks, while has
not been well explored in cross-lingual set-
tings. In this paper, we propose a novel
method that integrates cross-lingual word
and entity representation learning to enable
joint inference among knowledge base and
text across languages, capturing mutually
complementary knowledge. Instead of re-
liance on parallel data, we automatically
generate cross-lingual training data via dis-
tant supervision over multi-lingual knowl-
edge bases. We also propose two types
of knowledge attention and cross-lingual
attention to select the most informative
words and filter out noise, which will fur-
ther improve the performance. In exper-
iments, separate tasks of word translation
and entity relatedness demonstrate the ef-
fectiveness of our method with an average
gain of 20% and 3% over baselines, re-
spectively. Using entity linking as a case
study, the results on benchmark dataset
verify the quality of our embeddings.

1 Introduction

Multi-lingual knowledge bases (KB), storing mil-
lions of entities and their facts in various lan-
guages, provide rich structured knowledge for un-
derstanding natural language beyond texts. Mean-
while, abundant text corpus contains large amount
of potential knowledge complementary to existing
KBs. Therefore, researchers leverage both types
of resources to improve various natural language
processing (NLP) related tasks, such as relation ex-
traction (Weston et al., 2013; Lin et al., 2017), and
entity linking (Tsai and Roth, 2016; Yamada et al.,
2016; Cao et al., 2017; Ji et al., 2016).

Most existing work jointly models KB and text
corpus to enhance each other by learning word and
entity representations in a unified vector space. For
example, (Wang et al., 2014; Yamada et al., 2016;
Cao et al., 2017) utilize the coherence informa-
tion to align similar words and entities with sim-
ilar embedding vectors. Another approach in (Han
et al., 2016; Toutanova et al., 2015; Wu et al.,
2016) learns to represent entities based on their
textual descriptions together with the structured re-
lations. However, these methods only focus on
mono-lingual settings, and few researches have
been done in cross-lingual scenarios.
In this paper, we propose to learn cross-lingual

word and entity representations in the same seman-
tic space, to enable joint inference among KB and
text across languages without any additional trans-
lation mechanism, which is usually expensive and
may introduce inevitable errors. Our embeddings
are helpful to break down language gaps in many
tasks, such as cross-lingual entity linking, in which
the major challenge lies in measuring the similar-
ity between entities and corresponding mentioned
words in different languages.
e , w ,w
The intuition is that, words and entities in

various languages share some common semantic
meanings1, but there are also ways in which they
differ. On one hand, we utilize their shared seman-
tics to align similar words and entities with simi-
lar embedding vectors, no matter they are in the
same language or not. On the other hand, cross-
lingual embeddings will benefit from different lan-
guages due to the complementary knowledge. For
instance, textual ambiguity in one language may
disappear in another language, e.g., the two mean-

1Some cross-lingual pioneering work observe that word
embeddings trained separately on monolingual corpora ex-
hibit isomorphic structure across languages (Mikolov et al.,
2013; Zhang et al., 2017).

1

000

001

002

003

004

005

006

007

008

009

010

011

012

013

014

015

016

017

018

019

020

021

022

023

024

025

026

027

028

029

030

031

032

033

034

035

036

037

038

039

040

041

042

043

044

045

046

047

048

049

050

051

052

053

054

055

056

057

058

059

060

061

062

063

064

065

066

067

068

069

070

071

072

073

074

075

076

077

078

079

080

081

082

083

084

085

086

087

088

089

090

091

092

093

094

095

096

097

098

099

ACL 2018 Submission ***. Confidential Review Copy. DO NOT DISTRIBUTE.

Joint Representation Learning of Cross-lingual Words and Entities via
Attentive Distant Supervision

Anonymous ACL submission

Abstract

Jointly learning word and entity represen-
tations benefits many NLP tasks, while has
not been well explored in cross-lingual set-
tings. In this paper, we propose a novel
method that integrates cross-lingual word
and entity representation learning to enable
joint inference among knowledge base and
text across languages, capturing mutually
complementary knowledge. Instead of re-
liance on parallel data, we automatically
generate cross-lingual training data via dis-
tant supervision over multi-lingual knowl-
edge bases. We also propose two types
of knowledge attention and cross-lingual
attention to select the most informative
words and filter out noise, which will fur-
ther improve the performance. In exper-
iments, separate tasks of word translation
and entity relatedness demonstrate the ef-
fectiveness of our method with an average
gain of 20% and 3% over baselines, re-
spectively. Using entity linking as a case
study, the results on benchmark dataset
verify the quality of our embeddings.

1 Introduction

Multi-lingual knowledge bases (KB), storing mil-
lions of entities and their facts in various lan-
guages, provide rich structured knowledge for un-
derstanding natural language beyond texts. Mean-
while, abundant text corpus contains large amount
of potential knowledge complementary to existing
KBs. Therefore, researchers leverage both types
of resources to improve various natural language
processing (NLP) related tasks, such as relation ex-
traction (Weston et al., 2013; Lin et al., 2017), and
entity linking (Tsai and Roth, 2016; Yamada et al.,
2016; Cao et al., 2017; Ji et al., 2016).

Most existing work jointly models KB and text
corpus to enhance each other by learning word and
entity representations in a unified vector space. For
example, (Wang et al., 2014; Yamada et al., 2016;
Cao et al., 2017) utilize the coherence informa-
tion to align similar words and entities with sim-
ilar embedding vectors. Another approach in (Han
et al., 2016; Toutanova et al., 2015; Wu et al.,
2016) learns to represent entities based on their
textual descriptions together with the structured re-
lations. However, these methods only focus on
mono-lingual settings, and few researches have
been done in cross-lingual scenarios.
In this paper, we propose to learn cross-lingual

word and entity representations in the same seman-
tic space, to enable joint inference among KB and
text across languages without any additional trans-
lation mechanism, which is usually expensive and
may introduce inevitable errors. Our embeddings
are helpful to break down language gaps in many
tasks, such as cross-lingual entity linking, in which
the major challenge lies in measuring the similar-
ity between entities and corresponding mentioned
words in different languages.
e , w ,w
The intuition is that, words and entities in

various languages share some common semantic
meanings1, but there are also ways in which they
differ. On one hand, we utilize their shared seman-
tics to align similar words and entities with simi-
lar embedding vectors, no matter they are in the
same language or not. On the other hand, cross-
lingual embeddings will benefit from different lan-
guages due to the complementary knowledge. For
instance, textual ambiguity in one language may
disappear in another language, e.g., the two mean-

1Some cross-lingual pioneering work observe that word
embeddings trained separately on monolingual corpora ex-
hibit isomorphic structure across languages (Mikolov et al.,
2013; Zhang et al., 2017).

1

000

001

002

003

004

005

006

007

008

009

010

011

012

013

014

015

016

017

018

019

020

021

022

023

024

025

026

027

028

029

030

031

032

033

034

035

036

037

038

039

040

041

042

043

044

045

046

047

048

049

050

051

052

053

054

055

056

057

058

059

060

061

062

063

064

065

066

067

068

069

070

071

072

073

074

075

076

077

078

079

080

081

082

083

084

085

086

087

088

089

090

091

092

093

094

095

096

097

098

099

ACL 2018 Submission ***. Confidential Review Copy. DO NOT DISTRIBUTE.

Joint Representation Learning of Cross-lingual Words and Entities via
Attentive Distant Supervision

Anonymous ACL submission

Abstract

Jointly learning word and entity represen-
tations benefits many NLP tasks, while has
not been well explored in cross-lingual set-
tings. In this paper, we propose a novel
method that integrates cross-lingual word
and entity representation learning to enable
joint inference among knowledge base and
text across languages, capturing mutually
complementary knowledge. Instead of re-
liance on parallel data, we automatically
generate cross-lingual training data via dis-
tant supervision over multi-lingual knowl-
edge bases. We also propose two types
of knowledge attention and cross-lingual
attention to select the most informative
words and filter out noise, which will fur-
ther improve the performance. In exper-
iments, separate tasks of word translation
and entity relatedness demonstrate the ef-
fectiveness of our method with an average
gain of 20% and 3% over baselines, re-
spectively. Using entity linking as a case
study, the results on benchmark dataset
verify the quality of our embeddings.

1 Introduction

Multi-lingual knowledge bases (KB), storing mil-
lions of entities and their facts in various lan-
guages, provide rich structured knowledge for un-
derstanding natural language beyond texts. Mean-
while, abundant text corpus contains large amount
of potential knowledge complementary to existing
KBs. Therefore, researchers leverage both types
of resources to improve various natural language
processing (NLP) related tasks, such as relation ex-
traction (Weston et al., 2013; Lin et al., 2017), and
entity linking (Tsai and Roth, 2016; Yamada et al.,
2016; Cao et al., 2017; Ji et al., 2016).

Most existing work jointly models KB and text
corpus to enhance each other by learning word and
entity representations in a unified vector space. For
example, (Wang et al., 2014; Yamada et al., 2016;
Cao et al., 2017) utilize the coherence informa-
tion to align similar words and entities with sim-
ilar embedding vectors. Another approach in (Han
et al., 2016; Toutanova et al., 2015; Wu et al.,
2016) learns to represent entities based on their
textual descriptions together with the structured re-
lations. However, these methods only focus on
mono-lingual settings, and few researches have
been done in cross-lingual scenarios.
In this paper, we propose to learn cross-lingual

word and entity representations in the same seman-
tic space, to enable joint inference among KB and
text across languages without any additional trans-
lation mechanism, which is usually expensive and
may introduce inevitable errors. Our embeddings
are helpful to break down language gaps in many
tasks, such as cross-lingual entity linking, in which
the major challenge lies in measuring the similar-
ity between entities and corresponding mentioned
words in different languages.
e , w ,w
The intuition is that, words and entities in

various languages share some common semantic
meanings1, but there are also ways in which they
differ. On one hand, we utilize their shared seman-
tics to align similar words and entities with simi-
lar embedding vectors, no matter they are in the
same language or not. On the other hand, cross-
lingual embeddings will benefit from different lan-
guages due to the complementary knowledge. For
instance, textual ambiguity in one language may
disappear in another language, e.g., the two mean-

1Some cross-lingual pioneering work observe that word
embeddings trained separately on monolingual corpora ex-
hibit isomorphic structure across languages (Mikolov et al.,
2013; Zhang et al., 2017).

1

000

001

002

003

004

005

006

007

008

009

010

011

012

013

014

015

016

017

018

019

020

021

022

023

024

025

026

027

028

029

030

031

032

033

034

035

036

037

038

039

040

041

042

043

044

045

046

047

048

049

050

051

052

053

054

055

056

057

058

059

060

061

062

063

064

065

066

067

068

069

070

071

072

073

074

075

076

077

078

079

080

081

082

083

084

085

086

087

088

089

090

091

092

093

094

095

096

097

098

099

ACL 2018 Submission ***. Confidential Review Copy. DO NOT DISTRIBUTE.

Joint Representation Learning of Cross-lingual Words and Entities via
Attentive Distant Supervision

Anonymous ACL submission

Abstract

Jointly learning word and entity represen-
tations benefits many NLP tasks, while has
not been well explored in cross-lingual set-
tings. In this paper, we propose a novel
method that integrates cross-lingual word
and entity representation learning to enable
joint inference among knowledge base and
text across languages, capturing mutually
complementary knowledge. Instead of re-
liance on parallel data, we automatically
generate cross-lingual training data via dis-
tant supervision over multi-lingual knowl-
edge bases. We also propose two types
of knowledge attention and cross-lingual
attention to select the most informative
words and filter out noise, which will fur-
ther improve the performance. In exper-
iments, separate tasks of word translation
and entity relatedness demonstrate the ef-
fectiveness of our method with an average
gain of 20% and 3% over baselines, re-
spectively. Using entity linking as a case
study, the results on benchmark dataset
verify the quality of our embeddings.

1 Introduction

Multi-lingual knowledge bases (KB), storing mil-
lions of entities and their facts in various lan-
guages, provide rich structured knowledge for un-
derstanding natural language beyond texts. Mean-
while, abundant text corpus contains large amount
of potential knowledge complementary to existing
KBs. Therefore, researchers leverage both types
of resources to improve various natural language
processing (NLP) related tasks, such as relation ex-
traction (Weston et al., 2013; Lin et al., 2017), and
entity linking (Tsai and Roth, 2016; Yamada et al.,
2016; Cao et al., 2017; Ji et al., 2016).

Most existing work jointly models KB and text
corpus to enhance each other by learning word and
entity representations in a unified vector space. For
example, (Wang et al., 2014; Yamada et al., 2016;
Cao et al., 2017) utilize the coherence informa-
tion to align similar words and entities with sim-
ilar embedding vectors. Another approach in (Han
et al., 2016; Toutanova et al., 2015; Wu et al.,
2016) learns to represent entities based on their
textual descriptions together with the structured re-
lations. However, these methods only focus on
mono-lingual settings, and few researches have
been done in cross-lingual scenarios.
In this paper, we propose to learn cross-lingual

word and entity representations in the same seman-
tic space, to enable joint inference among KB and
text across languages without any additional trans-
lation mechanism, which is usually expensive and
may introduce inevitable errors. Our embeddings
are helpful to break down language gaps in many
tasks, such as cross-lingual entity linking, in which
the major challenge lies in measuring the similar-
ity between entities and corresponding mentioned
words in different languages.
e , w ,w
The intuition is that, words and entities in

various languages share some common semantic
meanings1, but there are also ways in which they
differ. On one hand, we utilize their shared seman-
tics to align similar words and entities with simi-
lar embedding vectors, no matter they are in the
same language or not. On the other hand, cross-
lingual embeddings will benefit from different lan-
guages due to the complementary knowledge. For
instance, textual ambiguity in one language may
disappear in another language, e.g., the two mean-

1Some cross-lingual pioneering work observe that word
embeddings trained separately on monolingual corpora ex-
hibit isomorphic structure across languages (Mikolov et al.,
2013; Zhang et al., 2017).

eNBAeAllstar

Mono-lingual Representation Learning

Zh

1

000

001

002

003

004

005

006

007

008

009

010

011

012

013

014

015

016

017

018

019

020

021

022

023

024

025

026

027

028

029

030

031

032

033

034

035

036

037

038

039

040

041

042

043

044

045

046

047

048

049

050

051

052

053

054

055

056

057

058

059

060

061

062

063

064

065

066

067

068

069

070

071

072

073

074

075

076

077

078

079

080

081

082

083

084

085

086

087

088

089

090

091

092

093

094

095

096

097

098

099

ACL 2018 Submission ***. Confidential Review Copy. DO NOT DISTRIBUTE.

Joint Representation Learning of Cross-lingual Words and Entities via
Attentive Distant Supervision

Anonymous ACL submission

Abstract

Jointly learning word and entity represen-
tations benefits many NLP tasks, while has
not been well explored in cross-lingual set-
tings. In this paper, we propose a novel
method that integrates cross-lingual word
and entity representation learning to enable
joint inference among knowledge base and
text across languages, capturing mutually
complementary knowledge. Instead of re-
liance on parallel data, we automatically
generate cross-lingual training data via dis-
tant supervision over multi-lingual knowl-
edge bases. We also propose two types
of knowledge attention and cross-lingual
attention to select the most informative
words and filter out noise, which will fur-
ther improve the performance. In exper-
iments, separate tasks of word translation
and entity relatedness demonstrate the ef-
fectiveness of our method with an average
gain of 20% and 3% over baselines, re-
spectively. Using entity linking as a case
study, the results on benchmark dataset
verify the quality of our embeddings.

1 Introduction

Multi-lingual knowledge bases (KB), storing mil-
lions of entities and their facts in various lan-
guages, provide rich structured knowledge for un-
derstanding natural language beyond texts. Mean-
while, abundant text corpus contains large amount
of potential knowledge complementary to existing
KBs. Therefore, researchers leverage both types
of resources to improve various natural language
processing (NLP) related tasks, such as relation ex-
traction (Weston et al., 2013; Lin et al., 2017), and
entity linking (Tsai and Roth, 2016; Yamada et al.,
2016; Cao et al., 2017; Ji et al., 2016).

Most existing work jointly models KB and text
corpus to enhance each other by learning word and
entity representations in a unified vector space. For
example, (Wang et al., 2014; Yamada et al., 2016;
Cao et al., 2017) utilize the coherence informa-
tion to align similar words and entities with sim-
ilar embedding vectors. Another approach in (Han
et al., 2016; Toutanova et al., 2015; Wu et al.,
2016) learns to represent entities based on their
textual descriptions together with the structured re-
lations. However, these methods only focus on
mono-lingual settings, and few researches have
been done in cross-lingual scenarios.
In this paper, we propose to learn cross-lingual

word and entity representations in the same seman-
tic space, to enable joint inference among KB and
text across languages without any additional trans-
lation mechanism, which is usually expensive and
may introduce inevitable errors. Our embeddings
are helpful to break down language gaps in many
tasks, such as cross-lingual entity linking, in which
the major challenge lies in measuring the similar-
ity between entities and corresponding mentioned
words in different languages.
e , w ,w
The intuition is that, words and entities in

various languages share some common semantic
meanings1, but there are also ways in which they
differ. On one hand, we utilize their shared seman-
tics to align similar words and entities with simi-
lar embedding vectors, no matter they are in the
same language or not. On the other hand, cross-
lingual embeddings will benefit from different lan-
guages due to the complementary knowledge. For
instance, textual ambiguity in one language may
disappear in another language, e.g., the two mean-

1Some cross-lingual pioneering work observe that word
embeddings trained separately on monolingual corpora ex-
hibit isomorphic structure across languages (Mikolov et al.,
2013; Zhang et al., 2017).

1

000

001

002

003

004

005

006

007

008

009

010

011

012

013

014

015

016

017

018

019

020

021

022

023

024

025

026

027

028

029

030

031

032

033

034

035

036

037

038

039

040

041

042

043

044

045

046

047

048

049

050

051

052

053

054

055

056

057

058

059

060

061

062

063

064

065

066

067

068

069

070

071

072

073

074

075

076

077

078

079

080

081

082

083

084

085

086

087

088

089

090

091

092

093

094

095

096

097

098

099

ACL 2018 Submission ***. Confidential Review Copy. DO NOT DISTRIBUTE.

Joint Representation Learning of Cross-lingual Words and Entities via
Attentive Distant Supervision

Anonymous ACL submission

Abstract

Jointly learning word and entity represen-
tations benefits many NLP tasks, while has
not been well explored in cross-lingual set-
tings. In this paper, we propose a novel
method that integrates cross-lingual word
and entity representation learning to enable
joint inference among knowledge base and
text across languages, capturing mutually
complementary knowledge. Instead of re-
liance on parallel data, we automatically
generate cross-lingual training data via dis-
tant supervision over multi-lingual knowl-
edge bases. We also propose two types
of knowledge attention and cross-lingual
attention to select the most informative
words and filter out noise, which will fur-
ther improve the performance. In exper-
iments, separate tasks of word translation
and entity relatedness demonstrate the ef-
fectiveness of our method with an average
gain of 20% and 3% over baselines, re-
spectively. Using entity linking as a case
study, the results on benchmark dataset
verify the quality of our embeddings.

1 Introduction

Multi-lingual knowledge bases (KB), storing mil-
lions of entities and their facts in various lan-
guages, provide rich structured knowledge for un-
derstanding natural language beyond texts. Mean-
while, abundant text corpus contains large amount
of potential knowledge complementary to existing
KBs. Therefore, researchers leverage both types
of resources to improve various natural language
processing (NLP) related tasks, such as relation ex-
traction (Weston et al., 2013; Lin et al., 2017), and
entity linking (Tsai and Roth, 2016; Yamada et al.,
2016; Cao et al., 2017; Ji et al., 2016).

Most existing work jointly models KB and text
corpus to enhance each other by learning word and
entity representations in a unified vector space. For
example, (Wang et al., 2014; Yamada et al., 2016;
Cao et al., 2017) utilize the coherence informa-
tion to align similar words and entities with sim-
ilar embedding vectors. Another approach in (Han
et al., 2016; Toutanova et al., 2015; Wu et al.,
2016) learns to represent entities based on their
textual descriptions together with the structured re-
lations. However, these methods only focus on
mono-lingual settings, and few researches have
been done in cross-lingual scenarios.
In this paper, we propose to learn cross-lingual

word and entity representations in the same seman-
tic space, to enable joint inference among KB and
text across languages without any additional trans-
lation mechanism, which is usually expensive and
may introduce inevitable errors. Our embeddings
are helpful to break down language gaps in many
tasks, such as cross-lingual entity linking, in which
the major challenge lies in measuring the similar-
ity between entities and corresponding mentioned
words in different languages.
eNBA , w , w ,w
The intuition is that, words and entities in

various languages share some common semantic
meanings1, but there are also ways in which they
differ. On one hand, we utilize their shared seman-
tics to align similar words and entities with simi-
lar embedding vectors, no matter they are in the
same language or not. On the other hand, cross-
lingual embeddings will benefit from different lan-
guages due to the complementary knowledge. For
instance, textual ambiguity in one language may
disappear in another language, e.g., the two mean-

1Some cross-lingual pioneering work observe that word
embeddings trained separately on monolingual corpora ex-
hibit isomorphic structure across languages (Mikolov et al.,
2013; Zhang et al., 2017).

1

000

001

002

003

004

005

006

007

008

009

010

011

012

013

014

015

016

017

018

019

020

021

022

023

024

025

026

027

028

029

030

031

032

033

034

035

036

037

038

039

040

041

042

043

044

045

046

047

048

049

050

051

052

053

054

055

056

057

058

059

060

061

062

063

064

065

066

067

068

069

070

071

072

073

074

075

076

077

078

079

080

081

082

083

084

085

086

087

088

089

090

091

092

093

094

095

096

097

098

099

ACL 2018 Submission ***. Confidential Review Copy. DO NOT DISTRIBUTE.

Joint Representation Learning of Cross-lingual Words and Entities via
Attentive Distant Supervision

Anonymous ACL submission

Abstract

Jointly learning word and entity represen-
tations benefits many NLP tasks, while has
not been well explored in cross-lingual set-
tings. In this paper, we propose a novel
method that integrates cross-lingual word
and entity representation learning to enable
joint inference among knowledge base and
text across languages, capturing mutually
complementary knowledge. Instead of re-
liance on parallel data, we automatically
generate cross-lingual training data via dis-
tant supervision over multi-lingual knowl-
edge bases. We also propose two types
of knowledge attention and cross-lingual
attention to select the most informative
words and filter out noise, which will fur-
ther improve the performance. In exper-
iments, separate tasks of word translation
and entity relatedness demonstrate the ef-
fectiveness of our method with an average
gain of 20% and 3% over baselines, re-
spectively. Using entity linking as a case
study, the results on benchmark dataset
verify the quality of our embeddings.

1 Introduction

Multi-lingual knowledge bases (KB), storing mil-
lions of entities and their facts in various lan-
guages, provide rich structured knowledge for un-
derstanding natural language beyond texts. Mean-
while, abundant text corpus contains large amount
of potential knowledge complementary to existing
KBs. Therefore, researchers leverage both types
of resources to improve various natural language
processing (NLP) related tasks, such as relation ex-
traction (Weston et al., 2013; Lin et al., 2017), and
entity linking (Tsai and Roth, 2016; Yamada et al.,
2016; Cao et al., 2017; Ji et al., 2016).

Most existing work jointly models KB and text
corpus to enhance each other by learning word and
entity representations in a unified vector space. For
example, (Wang et al., 2014; Yamada et al., 2016;
Cao et al., 2017) utilize the coherence informa-
tion to align similar words and entities with sim-
ilar embedding vectors. Another approach in (Han
et al., 2016; Toutanova et al., 2015; Wu et al.,
2016) learns to represent entities based on their
textual descriptions together with the structured re-
lations. However, these methods only focus on
mono-lingual settings, and few researches have
been done in cross-lingual scenarios.
In this paper, we propose to learn cross-lingual

word and entity representations in the same seman-
tic space, to enable joint inference among KB and
text across languages without any additional trans-
lation mechanism, which is usually expensive and
may introduce inevitable errors. Our embeddings
are helpful to break down language gaps in many
tasks, such as cross-lingual entity linking, in which
the major challenge lies in measuring the similar-
ity between entities and corresponding mentioned
words in different languages.
eNBA , w , w ,w
The intuition is that, words and entities in

various languages share some common semantic
meanings1, but there are also ways in which they
differ. On one hand, we utilize their shared seman-
tics to align similar words and entities with simi-
lar embedding vectors, no matter they are in the
same language or not. On the other hand, cross-
lingual embeddings will benefit from different lan-
guages due to the complementary knowledge. For
instance, textual ambiguity in one language may
disappear in another language, e.g., the two mean-

1Some cross-lingual pioneering work observe that word
embeddings trained separately on monolingual corpora ex-
hibit isomorphic structure across languages (Mikolov et al.,
2013; Zhang et al., 2017).

1

000

001

002

003

004

005

006

007

008

009

010

011

012

013

014

015

016

017

018

019

020

021

022

023

024

025

026

027

028

029

030

031

032

033

034

035

036

037

038

039

040

041

042

043

044

045

046

047

048

049

050

051

052

053

054

055

056

057

058

059

060

061

062

063

064

065

066

067

068

069

070

071

072

073

074

075

076

077

078

079

080

081

082

083

084

085

086

087

088

089

090

091

092

093

094

095

096

097

098

099

ACL 2018 Submission ***. Confidential Review Copy. DO NOT DISTRIBUTE.

Joint Representation Learning of Cross-lingual Words and Entities via
Attentive Distant Supervision

Anonymous ACL submission

Abstract

Jointly learning word and entity represen-
tations benefits many NLP tasks, while has
not been well explored in cross-lingual set-
tings. In this paper, we propose a novel
method that integrates cross-lingual word
and entity representation learning to enable
joint inference among knowledge base and
text across languages, capturing mutually
complementary knowledge. Instead of re-
liance on parallel data, we automatically
generate cross-lingual training data via dis-
tant supervision over multi-lingual knowl-
edge bases. We also propose two types
of knowledge attention and cross-lingual
attention to select the most informative
words and filter out noise, which will fur-
ther improve the performance. In exper-
iments, separate tasks of word translation
and entity relatedness demonstrate the ef-
fectiveness of our method with an average
gain of 20% and 3% over baselines, re-
spectively. Using entity linking as a case
study, the results on benchmark dataset
verify the quality of our embeddings.

1 Introduction

Multi-lingual knowledge bases (KB), storing mil-
lions of entities and their facts in various lan-
guages, provide rich structured knowledge for un-
derstanding natural language beyond texts. Mean-
while, abundant text corpus contains large amount
of potential knowledge complementary to existing
KBs. Therefore, researchers leverage both types
of resources to improve various natural language
processing (NLP) related tasks, such as relation ex-
traction (Weston et al., 2013; Lin et al., 2017), and
entity linking (Tsai and Roth, 2016; Yamada et al.,
2016; Cao et al., 2017; Ji et al., 2016).

Most existing work jointly models KB and text
corpus to enhance each other by learning word and
entity representations in a unified vector space. For
example, (Wang et al., 2014; Yamada et al., 2016;
Cao et al., 2017) utilize the coherence informa-
tion to align similar words and entities with sim-
ilar embedding vectors. Another approach in (Han
et al., 2016; Toutanova et al., 2015; Wu et al.,
2016) learns to represent entities based on their
textual descriptions together with the structured re-
lations. However, these methods only focus on
mono-lingual settings, and few researches have
been done in cross-lingual scenarios.
In this paper, we propose to learn cross-lingual

word and entity representations in the same seman-
tic space, to enable joint inference among KB and
text across languages without any additional trans-
lation mechanism, which is usually expensive and
may introduce inevitable errors. Our embeddings
are helpful to break down language gaps in many
tasks, such as cross-lingual entity linking, in which
the major challenge lies in measuring the similar-
ity between entities and corresponding mentioned
words in different languages.
eNBA , w , w ,w
The intuition is that, words and entities in

various languages share some common semantic
meanings1, but there are also ways in which they
differ. On one hand, we utilize their shared seman-
tics to align similar words and entities with simi-
lar embedding vectors, no matter they are in the
same language or not. On the other hand, cross-
lingual embeddings will benefit from different lan-
guages due to the complementary knowledge. For
instance, textual ambiguity in one language may
disappear in another language, e.g., the two mean-

1Some cross-lingual pioneering work observe that word
embeddings trained separately on monolingual corpora ex-
hibit isomorphic structure across languages (Mikolov et al.,
2013; Zhang et al., 2017).

1

000

001

002

003

004

005

006

007

008

009

010

011

012

013

014

015

016

017

018

019

020

021

022

023

024

025

026

027

028

029

030

031

032

033

034

035

036

037

038

039

040

041

042

043

044

045

046

047

048

049

050

051

052

053

054

055

056

057

058

059

060

061

062

063

064

065

066

067

068

069

070

071

072

073

074

075

076

077

078

079

080

081

082

083

084

085

086

087

088

089

090

091

092

093

094

095

096

097

098

099

ACL 2018 Submission ***. Confidential Review Copy. DO NOT DISTRIBUTE.

Joint Representation Learning of Cross-lingual Words and Entities via
Attentive Distant Supervision

Anonymous ACL submission

Abstract

Jointly learning word and entity represen-
tations benefits many NLP tasks, while has
not been well explored in cross-lingual set-
tings. In this paper, we propose a novel
method that integrates cross-lingual word
and entity representation learning to enable
joint inference among knowledge base and
text across languages, capturing mutually
complementary knowledge. Instead of re-
liance on parallel data, we automatically
generate cross-lingual training data via dis-
tant supervision over multi-lingual knowl-
edge bases. We also propose two types
of knowledge attention and cross-lingual
attention to select the most informative
words and filter out noise, which will fur-
ther improve the performance. In exper-
iments, separate tasks of word translation
and entity relatedness demonstrate the ef-
fectiveness of our method with an average
gain of 20% and 3% over baselines, re-
spectively. Using entity linking as a case
study, the results on benchmark dataset
verify the quality of our embeddings.

1 Introduction

Multi-lingual knowledge bases (KB), storing mil-
lions of entities and their facts in various lan-
guages, provide rich structured knowledge for un-
derstanding natural language beyond texts. Mean-
while, abundant text corpus contains large amount
of potential knowledge complementary to existing
KBs. Therefore, researchers leverage both types
of resources to improve various natural language
processing (NLP) related tasks, such as relation ex-
traction (Weston et al., 2013; Lin et al., 2017), and
entity linking (Tsai and Roth, 2016; Yamada et al.,
2016; Cao et al., 2017; Ji et al., 2016).

Most existing work jointly models KB and text
corpus to enhance each other by learning word and
entity representations in a unified vector space. For
example, (Wang et al., 2014; Yamada et al., 2016;
Cao et al., 2017) utilize the coherence informa-
tion to align similar words and entities with sim-
ilar embedding vectors. Another approach in (Han
et al., 2016; Toutanova et al., 2015; Wu et al.,
2016) learns to represent entities based on their
textual descriptions together with the structured re-
lations. However, these methods only focus on
mono-lingual settings, and few researches have
been done in cross-lingual scenarios.
In this paper, we propose to learn cross-lingual

word and entity representations in the same seman-
tic space, to enable joint inference among KB and
text across languages without any additional trans-
lation mechanism, which is usually expensive and
may introduce inevitable errors. Our embeddings
are helpful to break down language gaps in many
tasks, such as cross-lingual entity linking, in which
the major challenge lies in measuring the similar-
ity between entities and corresponding mentioned
words in different languages.
eNBA , w , w ,w
The intuition is that, words and entities in

various languages share some common semantic
meanings1, but there are also ways in which they
differ. On one hand, we utilize their shared seman-
tics to align similar words and entities with simi-
lar embedding vectors, no matter they are in the
same language or not. On the other hand, cross-
lingual embeddings will benefit from different lan-
guages due to the complementary knowledge. For
instance, textual ambiguity in one language may
disappear in another language, e.g., the two mean-

1Some cross-lingual pioneering work observe that word
embeddings trained separately on monolingual corpora ex-
hibit isomorphic structure across languages (Mikolov et al.,
2013; Zhang et al., 2017).

1

000

001

002

003

004

005

006

007

008

009

010

011

012

013

014

015

016

017

018

019

020

021

022

023

024

025

026

027

028

029

030

031

032

033

034

035

036

037

038

039

040

041

042

043

044

045

046

047

048

049

050

051

052

053

054

055

056

057

058

059

060

061

062

063

064

065

066

067

068

069

070

071

072

073

074

075

076

077

078

079

080

081

082

083

084

085

086

087

088

089

090

091

092

093

094

095

096

097

098

099

ACL 2018 Submission ***. Confidential Review Copy. DO NOT DISTRIBUTE.

Joint Representation Learning of Cross-lingual Words and Entities via
Attentive Distant Supervision

Anonymous ACL submission

Abstract

Jointly learning word and entity represen-
tations benefits many NLP tasks, while has
not been well explored in cross-lingual set-
tings. In this paper, we propose a novel
method that integrates cross-lingual word
and entity representation learning to enable
joint inference among knowledge base and
text across languages, capturing mutually
complementary knowledge. Instead of re-
liance on parallel data, we automatically
generate cross-lingual training data via dis-
tant supervision over multi-lingual knowl-
edge bases. We also propose two types
of knowledge attention and cross-lingual
attention to select the most informative
words and filter out noise, which will fur-
ther improve the performance. In exper-
iments, separate tasks of word translation
and entity relatedness demonstrate the ef-
fectiveness of our method with an average
gain of 20% and 3% over baselines, re-
spectively. Using entity linking as a case
study, the results on benchmark dataset
verify the quality of our embeddings.

1 Introduction

Multi-lingual knowledge bases (KB), storing mil-
lions of entities and their facts in various lan-
guages, provide rich structured knowledge for un-
derstanding natural language beyond texts. Mean-
while, abundant text corpus contains large amount
of potential knowledge complementary to existing
KBs. Therefore, researchers leverage both types
of resources to improve various natural language
processing (NLP) related tasks, such as relation ex-
traction (Weston et al., 2013; Lin et al., 2017), and
entity linking (Tsai and Roth, 2016; Yamada et al.,
2016; Cao et al., 2017; Ji et al., 2016).

Most existing work jointly models KB and text
corpus to enhance each other by learning word and
entity representations in a unified vector space. For
example, (Wang et al., 2014; Yamada et al., 2016;
Cao et al., 2017) utilize the coherence informa-
tion to align similar words and entities with sim-
ilar embedding vectors. Another approach in (Han
et al., 2016; Toutanova et al., 2015; Wu et al.,
2016) learns to represent entities based on their
textual descriptions together with the structured re-
lations. However, these methods only focus on
mono-lingual settings, and few researches have
been done in cross-lingual scenarios.
In this paper, we propose to learn cross-lingual

word and entity representations in the same seman-
tic space, to enable joint inference among KB and
text across languages without any additional trans-
lation mechanism, which is usually expensive and
may introduce inevitable errors. Our embeddings
are helpful to break down language gaps in many
tasks, such as cross-lingual entity linking, in which
the major challenge lies in measuring the similar-
ity between entities and corresponding mentioned
words in different languages.
eNBA
The intuition is that, words and entities in

various languages share some common semantic
meanings1, but there are also ways in which they
differ. On one hand, we utilize their shared seman-
tics to align similar words and entities with simi-
lar embedding vectors, no matter they are in the
same language or not. On the other hand, cross-
lingual embeddings will benefit from different lan-
guages due to the complementary knowledge. For
instance, textual ambiguity in one language may
disappear in another language, e.g., the two mean-

1Some cross-lingual pioneering work observe that word
embeddings trained separately on monolingual corpora ex-
hibit isomorphic structure across languages (Mikolov et al.,
2013; Zhang et al., 2017).

En

eLawrence

wwas wan

eLawrence

eNBAeAllstarwan wbasketball

wAmerican

Figure 1: The framework of our method. The inputs and outputs of each step are listed in the right
side, and in the left side there are three main components of joint representation learning. Red texts
with brackets are anchors, dashed lines between entities denote relations, and solid lines are cross-lingual
links.

mon neighbors have similar embeddings, no mat-
ter in which language, e.g., English entity Foust
and Chinese entity 福斯特 (Foust) are embed-
ded close in semantic space due to the common
neighborsNBA,All-star andNBA选秀 (draft), etc.
(3) Cross-lingual Sentence Regularizer aims to
learn mutually translated words with similar em-
beddings by pushing their cross-lingual contexts
(i.e. comparable sentences) together. For exam-
ple, English word basketball and the translated
Chinese word 篮球 frequently co-occur in com-
parable sentences, and are close in the semantic
space.
All word/entity embeddings are trained jointly

under a unified optimization objective. Next, we
will introduce how to generate bi-lingual EN and
comparable sentences as well as the three compo-
nents for joint representation learning in turn.

3.3 Cross-lingual Supervision Data
Generation

This section introduces how to extract more cross-
lingual clues from multi-lingual KB in the form of
bi-lingual EN and comparable sentences.

Bi-lingual Entity Network Construction

Conventional knowledge representation meth-
ods normally regard cross-lingual links as a spe-
cial equivalence type of relation between two en-
tities (Zhu et al., 2017). However, we argue that
this may mislead to an inconsistent training ob-

jective since a cross-lingual link actually contains
multiple relations. For example (Figure 1), there
will be no direct relation betweenChinese entity福
斯特 (Foust) and English entity Piston by merely
adding the equivalence relation between Foust and
福斯特, which is in contradiction with the fact that
Foust belongs to Piston, no matter in which lan-
guage.
Therefore, we build bi-lingual entity network by

making cross-lingual linked entities that inherit all
relations from each other. Concretely, we enhance
mono-EN by adding edges from all neighbors of
entity eei to ezj if < eei , ezj >∈ Re−z (layer 2).
e

Comparable Sentences Generation

We utilize distant supervision to generate com-
parable sentences from Wikipedia articles. As
shown in Figure 1, from the page articles of cross-
lingual linked entities eeKobe and e

z
Kobe, we extract

those sentences including another cross-lingual
linked entities eeJoe and ezJoe as comparable sen-
tences Se−z = {< sek, szk >}.
The intuition is that we consider each sentence

in a Wikipedia article has a pseudo mention of
the page entity (talking something about the en-
tity) (Yamada et al., 2017). Thus, if a sentence also
mentions another entity, it implicitly expresses
their relation. Therefore, we make a similar as-
sumption as in relation extraction: If two enti-
ties participate in a relation, and both of them

Zh

1

000

001

002

003

004

005

006

007

008

009

010

011

012

013

014

015

016

017

018

019

020

021

022

023

024

025

026

027

028

029

030

031

032

033

034

035

036

037

038

039

040

041

042

043

044

045

046

047

048

049

050

051

052

053

054

055

056

057

058

059

060

061

062

063

064

065

066

067

068

069

070

071

072

073

074

075

076

077

078

079

080

081

082

083

084

085

086

087

088

089

090

091

092

093

094

095

096

097

098

099

ACL 2018 Submission ***. Confidential Review Copy. DO NOT DISTRIBUTE.

Joint Representation Learning of Cross-lingual Words and Entities via
Attentive Distant Supervision

Anonymous ACL submission

Abstract

Jointly learning word and entity represen-
tations benefits many NLP tasks, while has
not been well explored in cross-lingual set-
tings. In this paper, we propose a novel
method that integrates cross-lingual word
and entity representation learning to enable
joint inference among knowledge base and
text across languages, capturing mutually
complementary knowledge. Instead of re-
liance on parallel data, we automatically
generate cross-lingual training data via dis-
tant supervision over multi-lingual knowl-
edge bases. We also propose two types
of knowledge attention and cross-lingual
attention to select the most informative
words and filter out noise, which will fur-
ther improve the performance. In exper-
iments, separate tasks of word translation
and entity relatedness demonstrate the ef-
fectiveness of our method with an average
gain of 20% and 3% over baselines, re-
spectively. Using entity linking as a case
study, the results on benchmark dataset
verify the quality of our embeddings.

1 Introduction

Multi-lingual knowledge bases (KB), storing mil-
lions of entities and their facts in various lan-
guages, provide rich structured knowledge for un-
derstanding natural language beyond texts. Mean-
while, abundant text corpus contains large amount
of potential knowledge complementary to existing
KBs. Therefore, researchers leverage both types
of resources to improve various natural language
processing (NLP) related tasks, such as relation ex-
traction (Weston et al., 2013; Lin et al., 2017), and
entity linking (Tsai and Roth, 2016; Yamada et al.,
2016; Cao et al., 2017; Ji et al., 2016).

Most existing work jointly models KB and text
corpus to enhance each other by learning word and
entity representations in a unified vector space. For
example, (Wang et al., 2014; Yamada et al., 2016;
Cao et al., 2017) utilize the coherence informa-
tion to align similar words and entities with sim-
ilar embedding vectors. Another approach in (Han
et al., 2016; Toutanova et al., 2015; Wu et al.,
2016) learns to represent entities based on their
textual descriptions together with the structured re-
lations. However, these methods only focus on
mono-lingual settings, and few researches have
been done in cross-lingual scenarios.
In this paper, we propose to learn cross-lingual

word and entity representations in the same seman-
tic space, to enable joint inference among KB and
text across languages without any additional trans-
lation mechanism, which is usually expensive and
may introduce inevitable errors. Our embeddings
are helpful to break down language gaps in many
tasks, such as cross-lingual entity linking, in which
the major challenge lies in measuring the similar-
ity between entities and corresponding mentioned
words in different languages.
e , w ,w
The intuition is that, words and entities in

various languages share some common semantic
meanings1, but there are also ways in which they
differ. On one hand, we utilize their shared seman-
tics to align similar words and entities with simi-
lar embedding vectors, no matter they are in the
same language or not. On the other hand, cross-
lingual embeddings will benefit from different lan-
guages due to the complementary knowledge. For
instance, textual ambiguity in one language may
disappear in another language, e.g., the two mean-

1Some cross-lingual pioneering work observe that word
embeddings trained separately on monolingual corpora ex-
hibit isomorphic structure across languages (Mikolov et al.,
2013; Zhang et al., 2017).

1

000

001

002

003

004

005

006

007

008

009

010

011

012

013

014

015

016

017

018

019

020

021

022

023

024

025

026

027

028

029

030

031

032

033

034

035

036

037

038

039

040

041

042

043

044

045

046

047

048

049

050

051

052

053

054

055

056

057

058

059

060

061

062

063

064

065

066

067

068

069

070

071

072

073

074

075

076

077

078

079

080

081

082

083

084

085

086

087

088

089

090

091

092

093

094

095

096

097

098

099

ACL 2018 Submission ***. Confidential Review Copy. DO NOT DISTRIBUTE.

Joint Representation Learning of Cross-lingual Words and Entities via
Attentive Distant Supervision

Anonymous ACL submission

Abstract

Jointly learning word and entity represen-
tations benefits many NLP tasks, while has
not been well explored in cross-lingual set-
tings. In this paper, we propose a novel
method that integrates cross-lingual word
and entity representation learning to enable
joint inference among knowledge base and
text across languages, capturing mutually
complementary knowledge. Instead of re-
liance on parallel data, we automatically
generate cross-lingual training data via dis-
tant supervision over multi-lingual knowl-
edge bases. We also propose two types
of knowledge attention and cross-lingual
attention to select the most informative
words and filter out noise, which will fur-
ther improve the performance. In exper-
iments, separate tasks of word translation
and entity relatedness demonstrate the ef-
fectiveness of our method with an average
gain of 20% and 3% over baselines, re-
spectively. Using entity linking as a case
study, the results on benchmark dataset
verify the quality of our embeddings.

1 Introduction

Multi-lingual knowledge bases (KB), storing mil-
lions of entities and their facts in various lan-
guages, provide rich structured knowledge for un-
derstanding natural language beyond texts. Mean-
while, abundant text corpus contains large amount
of potential knowledge complementary to existing
KBs. Therefore, researchers leverage both types
of resources to improve various natural language
processing (NLP) related tasks, such as relation ex-
traction (Weston et al., 2013; Lin et al., 2017), and
entity linking (Tsai and Roth, 2016; Yamada et al.,
2016; Cao et al., 2017; Ji et al., 2016).

Most existing work jointly models KB and text
corpus to enhance each other by learning word and
entity representations in a unified vector space. For
example, (Wang et al., 2014; Yamada et al., 2016;
Cao et al., 2017) utilize the coherence informa-
tion to align similar words and entities with sim-
ilar embedding vectors. Another approach in (Han
et al., 2016; Toutanova et al., 2015; Wu et al.,
2016) learns to represent entities based on their
textual descriptions together with the structured re-
lations. However, these methods only focus on
mono-lingual settings, and few researches have
been done in cross-lingual scenarios.
In this paper, we propose to learn cross-lingual

word and entity representations in the same seman-
tic space, to enable joint inference among KB and
text across languages without any additional trans-
lation mechanism, which is usually expensive and
may introduce inevitable errors. Our embeddings
are helpful to break down language gaps in many
tasks, such as cross-lingual entity linking, in which
the major challenge lies in measuring the similar-
ity between entities and corresponding mentioned
words in different languages.
eNBA , w , w ,w
The intuition is that, words and entities in

various languages share some common semantic
meanings1, but there are also ways in which they
differ. On one hand, we utilize their shared seman-
tics to align similar words and entities with simi-
lar embedding vectors, no matter they are in the
same language or not. On the other hand, cross-
lingual embeddings will benefit from different lan-
guages due to the complementary knowledge. For
instance, textual ambiguity in one language may
disappear in another language, e.g., the two mean-

1Some cross-lingual pioneering work observe that word
embeddings trained separately on monolingual corpora ex-
hibit isomorphic structure across languages (Mikolov et al.,
2013; Zhang et al., 2017).

1

000

001

002

003

004

005

006

007

008

009

010

011

012

013

014

015

016

017

018

019

020

021

022

023

024

025

026

027

028

029

030

031

032

033

034

035

036

037

038

039

040

041

042

043

044

045

046

047

048

049

050

051

052

053

054

055

056

057

058

059

060

061

062

063

064

065

066

067

068

069

070

071

072

073

074

075

076

077

078

079

080

081

082

083

084

085

086

087

088

089

090

091

092

093

094

095

096

097

098

099

ACL 2018 Submission ***. Confidential Review Copy. DO NOT DISTRIBUTE.

Joint Representation Learning of Cross-lingual Words and Entities via
Attentive Distant Supervision

Anonymous ACL submission

Abstract

Jointly learning word and entity represen-
tations benefits many NLP tasks, while has
not been well explored in cross-lingual set-
tings. In this paper, we propose a novel
method that integrates cross-lingual word
and entity representation learning to enable
joint inference among knowledge base and
text across languages, capturing mutually
complementary knowledge. Instead of re-
liance on parallel data, we automatically
generate cross-lingual training data via dis-
tant supervision over multi-lingual knowl-
edge bases. We also propose two types
of knowledge attention and cross-lingual
attention to select the most informative
words and filter out noise, which will fur-
ther improve the performance. In exper-
iments, separate tasks of word translation
and entity relatedness demonstrate the ef-
fectiveness of our method with an average
gain of 20% and 3% over baselines, re-
spectively. Using entity linking as a case
study, the results on benchmark dataset
verify the quality of our embeddings.

1 Introduction

Multi-lingual knowledge bases (KB), storing mil-
lions of entities and their facts in various lan-
guages, provide rich structured knowledge for un-
derstanding natural language beyond texts. Mean-
while, abundant text corpus contains large amount
of potential knowledge complementary to existing
KBs. Therefore, researchers leverage both types
of resources to improve various natural language
processing (NLP) related tasks, such as relation ex-
traction (Weston et al., 2013; Lin et al., 2017), and
entity linking (Tsai and Roth, 2016; Yamada et al.,
2016; Cao et al., 2017; Ji et al., 2016).

Most existing work jointly models KB and text
corpus to enhance each other by learning word and
entity representations in a unified vector space. For
example, (Wang et al., 2014; Yamada et al., 2016;
Cao et al., 2017) utilize the coherence informa-
tion to align similar words and entities with sim-
ilar embedding vectors. Another approach in (Han
et al., 2016; Toutanova et al., 2015; Wu et al.,
2016) learns to represent entities based on their
textual descriptions together with the structured re-
lations. However, these methods only focus on
mono-lingual settings, and few researches have
been done in cross-lingual scenarios.
In this paper, we propose to learn cross-lingual

word and entity representations in the same seman-
tic space, to enable joint inference among KB and
text across languages without any additional trans-
lation mechanism, which is usually expensive and
may introduce inevitable errors. Our embeddings
are helpful to break down language gaps in many
tasks, such as cross-lingual entity linking, in which
the major challenge lies in measuring the similar-
ity between entities and corresponding mentioned
words in different languages.
eNBA , w , w ,w
The intuition is that, words and entities in

various languages share some common semantic
meanings1, but there are also ways in which they
differ. On one hand, we utilize their shared seman-
tics to align similar words and entities with simi-
lar embedding vectors, no matter they are in the
same language or not. On the other hand, cross-
lingual embeddings will benefit from different lan-
guages due to the complementary knowledge. For
instance, textual ambiguity in one language may
disappear in another language, e.g., the two mean-

1Some cross-lingual pioneering work observe that word
embeddings trained separately on monolingual corpora ex-
hibit isomorphic structure across languages (Mikolov et al.,
2013; Zhang et al., 2017).

1

000

001

002

003

004

005

006

007

008

009

010

011

012

013

014

015

016

017

018

019

020

021

022

023

024

025

026

027

028

029

030

031

032

033

034

035

036

037

038

039

040

041

042

043

044

045

046

047

048

049

050

051

052

053

054

055

056

057

058

059

060

061

062

063

064

065

066

067

068

069

070

071

072

073

074

075

076

077

078

079

080

081

082

083

084

085

086

087

088

089

090

091

092

093

094

095

096

097

098

099

ACL 2018 Submission ***. Confidential Review Copy. DO NOT DISTRIBUTE.

Joint Representation Learning of Cross-lingual Words and Entities via
Attentive Distant Supervision

Anonymous ACL submission

Abstract

Jointly learning word and entity represen-
tations benefits many NLP tasks, while has
not been well explored in cross-lingual set-
tings. In this paper, we propose a novel
method that integrates cross-lingual word
and entity representation learning to enable
joint inference among knowledge base and
text across languages, capturing mutually
complementary knowledge. Instead of re-
liance on parallel data, we automatically
generate cross-lingual training data via dis-
tant supervision over multi-lingual knowl-
edge bases. We also propose two types
of knowledge attention and cross-lingual
attention to select the most informative
words and filter out noise, which will fur-
ther improve the performance. In exper-
iments, separate tasks of word translation
and entity relatedness demonstrate the ef-
fectiveness of our method with an average
gain of 20% and 3% over baselines, re-
spectively. Using entity linking as a case
study, the results on benchmark dataset
verify the quality of our embeddings.

1 Introduction

Multi-lingual knowledge bases (KB), storing mil-
lions of entities and their facts in various lan-
guages, provide rich structured knowledge for un-
derstanding natural language beyond texts. Mean-
while, abundant text corpus contains large amount
of potential knowledge complementary to existing
KBs. Therefore, researchers leverage both types
of resources to improve various natural language
processing (NLP) related tasks, such as relation ex-
traction (Weston et al., 2013; Lin et al., 2017), and
entity linking (Tsai and Roth, 2016; Yamada et al.,
2016; Cao et al., 2017; Ji et al., 2016).

Most existing work jointly models KB and text
corpus to enhance each other by learning word and
entity representations in a unified vector space. For
example, (Wang et al., 2014; Yamada et al., 2016;
Cao et al., 2017) utilize the coherence informa-
tion to align similar words and entities with sim-
ilar embedding vectors. Another approach in (Han
et al., 2016; Toutanova et al., 2015; Wu et al.,
2016) learns to represent entities based on their
textual descriptions together with the structured re-
lations. However, these methods only focus on
mono-lingual settings, and few researches have
been done in cross-lingual scenarios.
In this paper, we propose to learn cross-lingual

word and entity representations in the same seman-
tic space, to enable joint inference among KB and
text across languages without any additional trans-
lation mechanism, which is usually expensive and
may introduce inevitable errors. Our embeddings
are helpful to break down language gaps in many
tasks, such as cross-lingual entity linking, in which
the major challenge lies in measuring the similar-
ity between entities and corresponding mentioned
words in different languages.
eNBA , w , w ,w
The intuition is that, words and entities in

various languages share some common semantic
meanings1, but there are also ways in which they
differ. On one hand, we utilize their shared seman-
tics to align similar words and entities with simi-
lar embedding vectors, no matter they are in the
same language or not. On the other hand, cross-
lingual embeddings will benefit from different lan-
guages due to the complementary knowledge. For
instance, textual ambiguity in one language may
disappear in another language, e.g., the two mean-

1Some cross-lingual pioneering work observe that word
embeddings trained separately on monolingual corpora ex-
hibit isomorphic structure across languages (Mikolov et al.,
2013; Zhang et al., 2017).

1

000

001

002

003

004

005

006

007

008

009

010

011

012

013

014

015

016

017

018

019

020

021

022

023

024

025

026

027

028

029

030

031

032

033

034

035

036

037

038

039

040

041

042

043

044

045

046

047

048

049

050

051

052

053

054

055

056

057

058

059

060

061

062

063

064

065

066

067

068

069

070

071

072

073

074

075

076

077

078

079

080

081

082

083

084

085

086

087

088

089

090

091

092

093

094

095

096

097

098

099

ACL 2018 Submission ***. Confidential Review Copy. DO NOT DISTRIBUTE.

Joint Representation Learning of Cross-lingual Words and Entities via
Attentive Distant Supervision

Anonymous ACL submission

Abstract

Jointly learning word and entity represen-
tations benefits many NLP tasks, while has
not been well explored in cross-lingual set-
tings. In this paper, we propose a novel
method that integrates cross-lingual word
and entity representation learning to enable
joint inference among knowledge base and
text across languages, capturing mutually
complementary knowledge. Instead of re-
liance on parallel data, we automatically
generate cross-lingual training data via dis-
tant supervision over multi-lingual knowl-
edge bases. We also propose two types
of knowledge attention and cross-lingual
attention to select the most informative
words and filter out noise, which will fur-
ther improve the performance. In exper-
iments, separate tasks of word translation
and entity relatedness demonstrate the ef-
fectiveness of our method with an average
gain of 20% and 3% over baselines, re-
spectively. Using entity linking as a case
study, the results on benchmark dataset
verify the quality of our embeddings.

1 Introduction

Multi-lingual knowledge bases (KB), storing mil-
lions of entities and their facts in various lan-
guages, provide rich structured knowledge for un-
derstanding natural language beyond texts. Mean-
while, abundant text corpus contains large amount
of potential knowledge complementary to existing
KBs. Therefore, researchers leverage both types
of resources to improve various natural language
processing (NLP) related tasks, such as relation ex-
traction (Weston et al., 2013; Lin et al., 2017), and
entity linking (Tsai and Roth, 2016; Yamada et al.,
2016; Cao et al., 2017; Ji et al., 2016).

Most existing work jointly models KB and text
corpus to enhance each other by learning word and
entity representations in a unified vector space. For
example, (Wang et al., 2014; Yamada et al., 2016;
Cao et al., 2017) utilize the coherence informa-
tion to align similar words and entities with sim-
ilar embedding vectors. Another approach in (Han
et al., 2016; Toutanova et al., 2015; Wu et al.,
2016) learns to represent entities based on their
textual descriptions together with the structured re-
lations. However, these methods only focus on
mono-lingual settings, and few researches have
been done in cross-lingual scenarios.
In this paper, we propose to learn cross-lingual

word and entity representations in the same seman-
tic space, to enable joint inference among KB and
text across languages without any additional trans-
lation mechanism, which is usually expensive and
may introduce inevitable errors. Our embeddings
are helpful to break down language gaps in many
tasks, such as cross-lingual entity linking, in which
the major challenge lies in measuring the similar-
ity between entities and corresponding mentioned
words in different languages.
eNBA , w , w ,w
The intuition is that, words and entities in

various languages share some common semantic
meanings1, but there are also ways in which they
differ. On one hand, we utilize their shared seman-
tics to align similar words and entities with simi-
lar embedding vectors, no matter they are in the
same language or not. On the other hand, cross-
lingual embeddings will benefit from different lan-
guages due to the complementary knowledge. For
instance, textual ambiguity in one language may
disappear in another language, e.g., the two mean-

1Some cross-lingual pioneering work observe that word
embeddings trained separately on monolingual corpora ex-
hibit isomorphic structure across languages (Mikolov et al.,
2013; Zhang et al., 2017).

1

000

001

002

003

004

005

006

007

008

009

010

011

012

013

014

015

016

017

018

019

020

021

022

023

024

025

026

027

028

029

030

031

032

033

034

035

036

037

038

039

040

041

042

043

044

045

046

047

048

049

050

051

052

053

054

055

056

057

058

059

060

061

062

063

064

065

066

067

068

069

070

071

072

073

074

075

076

077

078

079

080

081

082

083

084

085

086

087

088

089

090

091

092

093

094

095

096

097

098

099

ACL 2018 Submission ***. Confidential Review Copy. DO NOT DISTRIBUTE.

Joint Representation Learning of Cross-lingual Words and Entities via
Attentive Distant Supervision

Anonymous ACL submission

Abstract

Jointly learning word and entity represen-
tations benefits many NLP tasks, while has
not been well explored in cross-lingual set-
tings. In this paper, we propose a novel
method that integrates cross-lingual word
and entity representation learning to enable
joint inference among knowledge base and
text across languages, capturing mutually
complementary knowledge. Instead of re-
liance on parallel data, we automatically
generate cross-lingual training data via dis-
tant supervision over multi-lingual knowl-
edge bases. We also propose two types
of knowledge attention and cross-lingual
attention to select the most informative
words and filter out noise, which will fur-
ther improve the performance. In exper-
iments, separate tasks of word translation
and entity relatedness demonstrate the ef-
fectiveness of our method with an average
gain of 20% and 3% over baselines, re-
spectively. Using entity linking as a case
study, the results on benchmark dataset
verify the quality of our embeddings.

1 Introduction

Multi-lingual knowledge bases (KB), storing mil-
lions of entities and their facts in various lan-
guages, provide rich structured knowledge for un-
derstanding natural language beyond texts. Mean-
while, abundant text corpus contains large amount
of potential knowledge complementary to existing
KBs. Therefore, researchers leverage both types
of resources to improve various natural language
processing (NLP) related tasks, such as relation ex-
traction (Weston et al., 2013; Lin et al., 2017), and
entity linking (Tsai and Roth, 2016; Yamada et al.,
2016; Cao et al., 2017; Ji et al., 2016).

Most existing work jointly models KB and text
corpus to enhance each other by learning word and
entity representations in a unified vector space. For
example, (Wang et al., 2014; Yamada et al., 2016;
Cao et al., 2017) utilize the coherence informa-
tion to align similar words and entities with sim-
ilar embedding vectors. Another approach in (Han
et al., 2016; Toutanova et al., 2015; Wu et al.,
2016) learns to represent entities based on their
textual descriptions together with the structured re-
lations. However, these methods only focus on
mono-lingual settings, and few researches have
been done in cross-lingual scenarios.
In this paper, we propose to learn cross-lingual

word and entity representations in the same seman-
tic space, to enable joint inference among KB and
text across languages without any additional trans-
lation mechanism, which is usually expensive and
may introduce inevitable errors. Our embeddings
are helpful to break down language gaps in many
tasks, such as cross-lingual entity linking, in which
the major challenge lies in measuring the similar-
ity between entities and corresponding mentioned
words in different languages.
eNBA
The intuition is that, words and entities in

various languages share some common semantic
meanings1, but there are also ways in which they
differ. On one hand, we utilize their shared seman-
tics to align similar words and entities with simi-
lar embedding vectors, no matter they are in the
same language or not. On the other hand, cross-
lingual embeddings will benefit from different lan-
guages due to the complementary knowledge. For
instance, textual ambiguity in one language may
disappear in another language, e.g., the two mean-

1Some cross-lingual pioneering work observe that word
embeddings trained separately on monolingual corpora ex-
hibit isomorphic structure across languages (Mikolov et al.,
2013; Zhang et al., 2017).

EneNBAeAllstar

wbasketball

wAmerican

eFoust eFoust

wbasketballwAmericanwplayer

5

400

401

402

403

404

405

406

407

408

409

410

411

412

413

414

415

416

417

418

419

420

421

422

423

424

425

426

427

428

429

430

431

432

433

434

435

436

437

438

439

440

441

442

443

444

445

446

447

448

449

450

451

452

453

454

455

456

457

458

459

460

461

462

463

464

465

466

467

468

469

470

471

472

473

474

475

476

477

478

479

480

481

482

483

484

485

486

487

488

489

490

491

492

493

494

495

496

497

498

499

ACL 2018 Submission ***. Confidential Review Copy. DO NOT DISTRIBUTE.

A straightforward solution is to concatenate them
into a longer sentence senk , but this increases the
chance to include unrelated sentences.
Unbalanced information. Sometimes the pair of
sentences convey different information, e.g., the
English sentence in layer 2 (Figure 1) contains
Foust spent 12 seasons in NBA while the compa-
rable Chinese sentence not.
To address the issues, we propose knowledge at-

tention and cross-lingual attention to filter out un-
related information at sentence level and at word
level, respectively. [[这里感觉改动较大]]

5 Joint Representation Learning

5.1 Mono-lingual Representation Learning
Following (Yamada et al., 2016; Cao et al., 2017),
we learn mono-lingual word/entity embeddings
based on corpus Dy, anchors Ay and entity net-
work Gy. We utilize a variant of Skip-gram
model (Mikolov et al., 2013c) to predict the con-
texts given current word/entity:

Lm =
∑

y∈{en,zh}

∑

xyi ∈{Dy ,Ay ,Gy}

logP (C(xyi )|x
y
i )

(1)
where xyi is either a word or an entity, and C(x

y
i )

denotes: (i) contextual words in a pre-defined win-
dow of xyi if x

y
i ∈ Dy, (ii) neighbor entities that

linked to xyi if x
y
i ∈ Gy, (iii) contextual words of

wyj if x
y
i is entity e

y
i in an anchor ⟨w

y
j , e

y
i ⟩ ∈ Ay.

5.2 Cross-lingual Entity Regularizer
The bilingual EN Gen−zh merges entities in dif-
ferent languages into a unified network, resulting
in the possibility of using the same objective as
in mono-lingual ENs. Thus, we naturally extend
mono-lingual function to cross-lingual settings:

Le =
∑

eyi ∈{Gen−zh}

logP (C′(eyi )|e
y
i ) (2)

where C′(eyi ) denotes cross-lingual contexts—
neighbor entities in different languages that linked
to eyi . Thus, by jointly learning mono-lingual rep-
resentation with cross-lingual entity regularizer,
words and entities share more common contexts,
and will have similar embeddings. As shown in
Figure 1, English entityNBA co-occurs with words
basketball and player in texts, so they are embed-
ded close in the semantic space. Meanwhile, cross-
lingual linked entitiesNBA andNBA (zh) have sim-
ilar representations due to themost common neigh-
bor entities, e.g., Foust.

5.3 Cross-lingual Sentence Regularizer
Comparable sentences provide cross-lingual co-
occurrence of words, thus, we learn similar em-
beddings for the words that frequently co-occur to-
gether by minimizing the Euclidean distance:

Ls =
∑

⟨senk ,s
zh
k′ ⟩∈S

en−zh

||senk − szhk′ ||2 (3)

where senk , s
zh
k′ are sentence embeddings. Take En-

glish as sample language, we define it as the aver-
age sum of word vectors weighted by the combi-
nation of two types of attentions:

senk =
∑

l∈L
ψ(eenm , s

en
k,l)

∑

weni ∈senk,l

ψ′(weni , w
zh
j )weni

(4)
where {senk,l|l ∈ L} is a set of sentences con-
taining the same entity (as mentioned in Sec-
tion 4.2), and ψ(eenm , senk,l) is knowledge attention
that aims at filter out wrong labelling sentences,
and ψ′(weni , wzhj ) is cross-lingual attention to deal
with the unbalanced information through possible
aligned words.

Knowledge Attention

Suppose that sentences {senk,l|l ∈ L} contain the
same entities in articles of entity eym, the wrong la-
belling errors increase because some of them are
almost irrelevant to eym. Knowledge attention aims
at filtering out wrong labelled sentences through
smaller weights and related sentences with higher
weights. Thus, we define it proportional to the sim-
ilarity between syk,l and e

y
m:

ψ(eym, s
y
k,l) ∝ sim(e

y
m,

∑

wyi ∈s
y
k,l

wyi ) (5)

where sim is similarity measurement, and we
use cosine similarity in the rest of the pa-
per. We normalize knowledge attention such that∑L

l ψ(e
y
m, s

y
k,l) = 1.

Cross-lingual Attention

Inspired by self-attention mechanism (Lin et al.,
2017b), we motivate cross-lingual attention focus-
ing on potential information from comparable sen-
tences themselves. The intuition is to find possible
alignedwords between languages, and filter out the
words without alignments. We define it according
to the maximum similarity:

4

300

301

302

303

304

305

306

307

308

309

310

311

312

313

314

315

316

317

318

319

320

321

322

323

324

325

326

327

328

329

330

331

332

333

334

335

336

337

338

339

340

341

342

343

344

345

346

347

348

349

350

351

352

353

354

355

356

357

358

359

360

361

362

363

364

365

366

367

368

369

370

371

372

373

374

375

376

377

378

379

380

381

382

383

384

385

386

387

388

389

390

391

392

393

394

395

396

397

398

399

ACL 2018 Submission ***. Confidential Review Copy. DO NOT DISTRIBUTE.

NBA

Lawrence Foust

All-star

[[Lawrence Michael Foust]] was
an American basketball player

who spent 12 seasons in [[NBA]]

NBA (zh)

·
[[ · ]] [[NBA]]

1950 [[NBA ]] 1 5

English KB Chinese KB

独立日 美国独立日

Sen Szh een ezh

L =
∑

xi ∈D

∑

xo ∈C(xi )
P(xo |xi)

L =
∑

(ei ,ej )∈Ec
| |C(ei) − C(e j)| |2

Ec

L =
∑

ei ∈E
P(N (ei)|ei) +

∑

(ei ,ej )∈Ec
P(N (e j)|ei)

N (ei)

L =
∑

(ei ,mi )∈A
P(ei |mi ,C(ei)) +

∑

(ei ,ej )∈Ec
P(e j |mi ,C(ei))

独立日 美国独立日

Sen Szh een ezh

L =
∑

xi ∈D

∑

xo ∈C(xi )
P(xo |xi)

L =
∑

(ei ,ej )∈Ec
| |C(ei) − C(e j)| |2

Ec

L =
∑

ei ∈E
P(N (ei)|ei) +

∑

(ei ,ej )∈Ec
P(N (e j)|ei)

N (ei)

L =
∑

(ei ,mi )∈A
P(ei |mi ,C(ei)) +

∑

(ei ,ej )∈Ec
P(e j |mi ,C(ei))

was

Lawrence
NBA

NBA (zh)

basketball

Semantic Space

Representation
Learning

Cross-lingual
Sentence Regularizer

Cross-lingual
Entity Regularizer

[[ · ]] [[NBA]]

All-star

[[Lawrence Michael Foust]] was an American basketball 
player who spent 12 seasons in [[NBA]]

·

NBA
NBA (zh)

Comparable Sentences

Bi-lingual EN

att
att

att

… …

Ls = | |Sen − Szh | |2

L =
∑

xi ∈D

∑

xo ∈C(xi )
P(xo |xi )

L =
∑

(ei ,ej )∈Ec
| |C(ei ) − C(ej )| |2

Ec

L =
∑

ei ∈E
P(N (ei )|ei ) +

∑

(ei ,ej )∈Ec
P(N (ej )|ei )

N (ei )

L =
∑

(ei ,mi )∈A
P(ei |mi ,C(ei )) +

∑

(ei ,ej )∈Ec
P(ej |mi ,C(ei ))

=

500

501

502

503

504

505

506

507

508

509

510

511

512

513

514

515

516

517

518

519

520

521

522

523

524

525

526

527

528

529

530

531

532

533

534

535

536

537

538

539

540

541

542

543

544

545

546

547

548

549

550

551

552

553

554

555

556

557

558

559

560

561

562

563

564

565

566

567

568

569

570

571

572

573

574

575

576

577

578

579

580

581

582

583

584

585

586

587

588

589

590

591

592

593

594

595

596

597

598

599

ACL 2018 Submission ***. Confidential Review Copy. DO NOT DISTRIBUTE.

skm ≈
wki +

∑
C(wki)

Ls =
∑

<skm,slm>∈Sk,l

|| − ||2

ekm
ekm

k

αnkm ∝ sim( ,
∑

wki∈snkm

)

αnkm
n skm

ekm αnkm

αki,lj ∝
wki∈skm,wlj∈slm

sim(wki, wlj)

=
∑

snkm∈skm

αnkm
∑

wki∈snkm

αki,lj

L = Lc + γLs

γ

400

401

402

403

404

405

406

407

408

409

410

411

412

413

414

415

416

417

418

419

420

421

422

423

424

425

426

427

428

429

430

431

432

433

434

435

436

437

438

439

440

441

442

443

444

445

446

447

448

449

450

451

452

453

454

455

456

457

458

459

460

461

462

463

464

465

466

467

468

469

470

471

472

473

474

475

476

477

478

479

480

481

482

483

484

485

486

487

488

489

490

491

492

493

494

495

496

497

498

499

ACL 2018 Submission ***. Confidential Review Copy. DO NOT DISTRIBUTE.

eki elj < eki, elj >∈ Rc

een,Kobe
ezh,Kobe

een,Joe ezh,Joe

< een,Kobe, ezh,Kobe >
< een,Joe, ezh,Joe >

D A
KN

Lc =
∑

xi∈{D,A,KN}

P (C(xi)|xi)

xi
C(xi)

C(xi)
xi

xi ∈ D
xi ∈ KN

wj xi
< wj , ei >∈ A

een,Joe wen,player
wen,NBA

een,LosAngelesLakers
een,Joe een,Kobe ezh,Kobe

wki
Cl(wki)

Ls =
∑

<skm,slm∈Sk,l

∑

wki∈skm

||wki −
∑

Cl(wki)||2

< skm, slm >∈ Sk,l m
k l

Cross-lingual
 supervision

Data Generation

and was an 8-time [[All-star]] … [[ ]] …

NBA

Lawrence Foust

NBA

player

American
NBA

All-star
eLawrence

wbasketballwplayer
… …

1

000

001

002

003

004

005

006

007

008

009

010

011

012

013

014

015

016

017

018

019

020

021

022

023

024

025

026

027

028

029

030

031

032

033

034

035

036

037

038

039

040

041

042

043

044

045

046

047

048

049

050

051

052

053

054

055

056

057

058

059

060

061

062

063

064

065

066

067

068

069

070

071

072

073

074

075

076

077

078

079

080

081

082

083

084

085

086

087

088

089

090

091

092

093

094

095

096

097

098

099

ACL 2018 Submission ***. Confidential Review Copy. DO NOT DISTRIBUTE.

Joint Representation Learning of Cross-lingual Words and Entities via
Attentive Distant Supervision

Anonymous ACL submission

Abstract

Jointly learning word and entity represen-
tations benefits many NLP tasks, while has
not been well explored in cross-lingual set-
tings. In this paper, we propose a novel
method that integrates cross-lingual word
and entity representation learning to enable
joint inference among knowledge base and
text across languages, capturing mutually
complementary knowledge. Instead of re-
liance on parallel data, we automatically
generate cross-lingual training data via dis-
tant supervision over multi-lingual knowl-
edge bases. We also propose two types
of knowledge attention and cross-lingual
attention to select the most informative
words and filter out noise, which will fur-
ther improve the performance. In exper-
iments, separate tasks of word translation
and entity relatedness demonstrate the ef-
fectiveness of our method with an average
gain of 20% and 3% over baselines, re-
spectively. Using entity linking as a case
study, the results on benchmark dataset
verify the quality of our embeddings.

1 Introduction

Multi-lingual knowledge bases (KB), storing mil-
lions of entities and their facts in various lan-
guages, provide rich structured knowledge for un-
derstanding natural language beyond texts. Mean-
while, abundant text corpus contains large amount
of potential knowledge complementary to existing
KBs. Therefore, researchers leverage both types
of resources to improve various natural language
processing (NLP) related tasks, such as relation ex-
traction (Weston et al., 2013; Lin et al., 2017), and
entity linking (Tsai and Roth, 2016; Yamada et al.,
2016; Cao et al., 2017; Ji et al., 2016).

Most existing work jointly models KB and text
corpus to enhance each other by learning word and
entity representations in a unified vector space. For
example, (Wang et al., 2014; Yamada et al., 2016;
Cao et al., 2017) utilize the coherence informa-
tion to align similar words and entities with sim-
ilar embedding vectors. Another approach in (Han
et al., 2016; Toutanova et al., 2015; Wu et al.,
2016) learns to represent entities based on their
textual descriptions together with the structured re-
lations. However, these methods only focus on
mono-lingual settings, and few researches have
been done in cross-lingual scenarios.
In this paper, we propose to learn cross-lingual

word and entity representations in the same seman-
tic space, to enable joint inference among KB and
text across languages without any additional trans-
lation mechanism, which is usually expensive and
may introduce inevitable errors. Our embeddings
are helpful to break down language gaps in many
tasks, such as cross-lingual entity linking, in which
the major challenge lies in measuring the similar-
ity between entities and corresponding mentioned
words in different languages.
e , w ,w
The intuition is that, words and entities in

various languages share some common semantic
meanings1, but there are also ways in which they
differ. On one hand, we utilize their shared seman-
tics to align similar words and entities with simi-
lar embedding vectors, no matter they are in the
same language or not. On the other hand, cross-
lingual embeddings will benefit from different lan-
guages due to the complementary knowledge. For
instance, textual ambiguity in one language may
disappear in another language, e.g., the two mean-

1Some cross-lingual pioneering work observe that word
embeddings trained separately on monolingual corpora ex-
hibit isomorphic structure across languages (Mikolov et al.,
2013; Zhang et al., 2017).

1

000

001

002

003

004

005

006

007

008

009

010

011

012

013

014

015

016

017

018

019

020

021

022

023

024

025

026

027

028

029

030

031

032

033

034

035

036

037

038

039

040

041

042

043

044

045

046

047

048

049

050

051

052

053

054

055

056

057

058

059

060

061

062

063

064

065

066

067

068

069

070

071

072

073

074

075

076

077

078

079

080

081

082

083

084

085

086

087

088

089

090

091

092

093

094

095

096

097

098

099

ACL 2018 Submission ***. Confidential Review Copy. DO NOT DISTRIBUTE.

Joint Representation Learning of Cross-lingual Words and Entities via
Attentive Distant Supervision

Anonymous ACL submission

Abstract

Jointly learning word and entity represen-
tations benefits many NLP tasks, while has
not been well explored in cross-lingual set-
tings. In this paper, we propose a novel
method that integrates cross-lingual word
and entity representation learning to enable
joint inference among knowledge base and
text across languages, capturing mutually
complementary knowledge. Instead of re-
liance on parallel data, we automatically
generate cross-lingual training data via dis-
tant supervision over multi-lingual knowl-
edge bases. We also propose two types
of knowledge attention and cross-lingual
attention to select the most informative
words and filter out noise, which will fur-
ther improve the performance. In exper-
iments, separate tasks of word translation
and entity relatedness demonstrate the ef-
fectiveness of our method with an average
gain of 20% and 3% over baselines, re-
spectively. Using entity linking as a case
study, the results on benchmark dataset
verify the quality of our embeddings.

1 Introduction

Multi-lingual knowledge bases (KB), storing mil-
lions of entities and their facts in various lan-
guages, provide rich structured knowledge for un-
derstanding natural language beyond texts. Mean-
while, abundant text corpus contains large amount
of potential knowledge complementary to existing
KBs. Therefore, researchers leverage both types
of resources to improve various natural language
processing (NLP) related tasks, such as relation ex-
traction (Weston et al., 2013; Lin et al., 2017), and
entity linking (Tsai and Roth, 2016; Yamada et al.,
2016; Cao et al., 2017; Ji et al., 2016).

Most existing work jointly models KB and text
corpus to enhance each other by learning word and
entity representations in a unified vector space. For
example, (Wang et al., 2014; Yamada et al., 2016;
Cao et al., 2017) utilize the coherence informa-
tion to align similar words and entities with sim-
ilar embedding vectors. Another approach in (Han
et al., 2016; Toutanova et al., 2015; Wu et al.,
2016) learns to represent entities based on their
textual descriptions together with the structured re-
lations. However, these methods only focus on
mono-lingual settings, and few researches have
been done in cross-lingual scenarios.
In this paper, we propose to learn cross-lingual

word and entity representations in the same seman-
tic space, to enable joint inference among KB and
text across languages without any additional trans-
lation mechanism, which is usually expensive and
may introduce inevitable errors. Our embeddings
are helpful to break down language gaps in many
tasks, such as cross-lingual entity linking, in which
the major challenge lies in measuring the similar-
ity between entities and corresponding mentioned
words in different languages.
e , w ,w
The intuition is that, words and entities in

various languages share some common semantic
meanings1, but there are also ways in which they
differ. On one hand, we utilize their shared seman-
tics to align similar words and entities with simi-
lar embedding vectors, no matter they are in the
same language or not. On the other hand, cross-
lingual embeddings will benefit from different lan-
guages due to the complementary knowledge. For
instance, textual ambiguity in one language may
disappear in another language, e.g., the two mean-

1Some cross-lingual pioneering work observe that word
embeddings trained separately on monolingual corpora ex-
hibit isomorphic structure across languages (Mikolov et al.,
2013; Zhang et al., 2017).

1

000

001

002

003

004

005

006

007

008

009

010

011

012

013

014

015

016

017

018

019

020

021

022

023

024

025

026

027

028

029

030

031

032

033

034

035

036

037

038

039

040

041

042

043

044

045

046

047

048

049

050

051

052

053

054

055

056

057

058

059

060

061

062

063

064

065

066

067

068

069

070

071

072

073

074

075

076

077

078

079

080

081

082

083

084

085

086

087

088

089

090

091

092

093

094

095

096

097

098

099

ACL 2018 Submission ***. Confidential Review Copy. DO NOT DISTRIBUTE.

Joint Representation Learning of Cross-lingual Words and Entities via
Attentive Distant Supervision

Anonymous ACL submission

Abstract

Jointly learning word and entity represen-
tations benefits many NLP tasks, while has
not been well explored in cross-lingual set-
tings. In this paper, we propose a novel
method that integrates cross-lingual word
and entity representation learning to enable
joint inference among knowledge base and
text across languages, capturing mutually
complementary knowledge. Instead of re-
liance on parallel data, we automatically
generate cross-lingual training data via dis-
tant supervision over multi-lingual knowl-
edge bases. We also propose two types
of knowledge attention and cross-lingual
attention to select the most informative
words and filter out noise, which will fur-
ther improve the performance. In exper-
iments, separate tasks of word translation
and entity relatedness demonstrate the ef-
fectiveness of our method with an average
gain of 20% and 3% over baselines, re-
spectively. Using entity linking as a case
study, the results on benchmark dataset
verify the quality of our embeddings.

1 Introduction

Multi-lingual knowledge bases (KB), storing mil-
lions of entities and their facts in various lan-
guages, provide rich structured knowledge for un-
derstanding natural language beyond texts. Mean-
while, abundant text corpus contains large amount
of potential knowledge complementary to existing
KBs. Therefore, researchers leverage both types
of resources to improve various natural language
processing (NLP) related tasks, such as relation ex-
traction (Weston et al., 2013; Lin et al., 2017), and
entity linking (Tsai and Roth, 2016; Yamada et al.,
2016; Cao et al., 2017; Ji et al., 2016).

Most existing work jointly models KB and text
corpus to enhance each other by learning word and
entity representations in a unified vector space. For
example, (Wang et al., 2014; Yamada et al., 2016;
Cao et al., 2017) utilize the coherence informa-
tion to align similar words and entities with sim-
ilar embedding vectors. Another approach in (Han
et al., 2016; Toutanova et al., 2015; Wu et al.,
2016) learns to represent entities based on their
textual descriptions together with the structured re-
lations. However, these methods only focus on
mono-lingual settings, and few researches have
been done in cross-lingual scenarios.
In this paper, we propose to learn cross-lingual

word and entity representations in the same seman-
tic space, to enable joint inference among KB and
text across languages without any additional trans-
lation mechanism, which is usually expensive and
may introduce inevitable errors. Our embeddings
are helpful to break down language gaps in many
tasks, such as cross-lingual entity linking, in which
the major challenge lies in measuring the similar-
ity between entities and corresponding mentioned
words in different languages.
e , w ,w
The intuition is that, words and entities in

various languages share some common semantic
meanings1, but there are also ways in which they
differ. On one hand, we utilize their shared seman-
tics to align similar words and entities with simi-
lar embedding vectors, no matter they are in the
same language or not. On the other hand, cross-
lingual embeddings will benefit from different lan-
guages due to the complementary knowledge. For
instance, textual ambiguity in one language may
disappear in another language, e.g., the two mean-

1Some cross-lingual pioneering work observe that word
embeddings trained separately on monolingual corpora ex-
hibit isomorphic structure across languages (Mikolov et al.,
2013; Zhang et al., 2017).

1

000

001

002

003

004

005

006

007

008

009

010

011

012

013

014

015

016

017

018

019

020

021

022

023

024

025

026

027

028

029

030

031

032

033

034

035

036

037

038

039

040

041

042

043

044

045

046

047

048

049

050

051

052

053

054

055

056

057

058

059

060

061

062

063

064

065

066

067

068

069

070

071

072

073

074

075

076

077

078

079

080

081

082

083

084

085

086

087

088

089

090

091

092

093

094

095

096

097

098

099

ACL 2018 Submission ***. Confidential Review Copy. DO NOT DISTRIBUTE.

Joint Representation Learning of Cross-lingual Words and Entities via
Attentive Distant Supervision

Anonymous ACL submission

Abstract

Jointly learning word and entity represen-
tations benefits many NLP tasks, while has
not been well explored in cross-lingual set-
tings. In this paper, we propose a novel
method that integrates cross-lingual word
and entity representation learning to enable
joint inference among knowledge base and
text across languages, capturing mutually
complementary knowledge. Instead of re-
liance on parallel data, we automatically
generate cross-lingual training data via dis-
tant supervision over multi-lingual knowl-
edge bases. We also propose two types
of knowledge attention and cross-lingual
attention to select the most informative
words and filter out noise, which will fur-
ther improve the performance. In exper-
iments, separate tasks of word translation
and entity relatedness demonstrate the ef-
fectiveness of our method with an average
gain of 20% and 3% over baselines, re-
spectively. Using entity linking as a case
study, the results on benchmark dataset
verify the quality of our embeddings.

1 Introduction

Multi-lingual knowledge bases (KB), storing mil-
lions of entities and their facts in various lan-
guages, provide rich structured knowledge for un-
derstanding natural language beyond texts. Mean-
while, abundant text corpus contains large amount
of potential knowledge complementary to existing
KBs. Therefore, researchers leverage both types
of resources to improve various natural language
processing (NLP) related tasks, such as relation ex-
traction (Weston et al., 2013; Lin et al., 2017), and
entity linking (Tsai and Roth, 2016; Yamada et al.,
2016; Cao et al., 2017; Ji et al., 2016).

Most existing work jointly models KB and text
corpus to enhance each other by learning word and
entity representations in a unified vector space. For
example, (Wang et al., 2014; Yamada et al., 2016;
Cao et al., 2017) utilize the coherence informa-
tion to align similar words and entities with sim-
ilar embedding vectors. Another approach in (Han
et al., 2016; Toutanova et al., 2015; Wu et al.,
2016) learns to represent entities based on their
textual descriptions together with the structured re-
lations. However, these methods only focus on
mono-lingual settings, and few researches have
been done in cross-lingual scenarios.
In this paper, we propose to learn cross-lingual

word and entity representations in the same seman-
tic space, to enable joint inference among KB and
text across languages without any additional trans-
lation mechanism, which is usually expensive and
may introduce inevitable errors. Our embeddings
are helpful to break down language gaps in many
tasks, such as cross-lingual entity linking, in which
the major challenge lies in measuring the similar-
ity between entities and corresponding mentioned
words in different languages.
e , w ,w
The intuition is that, words and entities in

various languages share some common semantic
meanings1, but there are also ways in which they
differ. On one hand, we utilize their shared seman-
tics to align similar words and entities with simi-
lar embedding vectors, no matter they are in the
same language or not. On the other hand, cross-
lingual embeddings will benefit from different lan-
guages due to the complementary knowledge. For
instance, textual ambiguity in one language may
disappear in another language, e.g., the two mean-

1Some cross-lingual pioneering work observe that word
embeddings trained separately on monolingual corpora ex-
hibit isomorphic structure across languages (Mikolov et al.,
2013; Zhang et al., 2017).

eNBAeAllstar

Mono-lingual Representation Learning

Zh

1

000

001

002

003

004

005

006

007

008

009

010

011

012

013

014

015

016

017

018

019

020

021

022

023

024

025

026

027

028

029

030

031

032

033

034

035

036

037

038

039

040

041

042

043

044

045

046

047

048

049

050

051

052

053

054

055

056

057

058

059

060

061

062

063

064

065

066

067

068

069

070

071

072

073

074

075

076

077

078

079

080

081

082

083

084

085

086

087

088

089

090

091

092

093

094

095

096

097

098

099

ACL 2018 Submission ***. Confidential Review Copy. DO NOT DISTRIBUTE.

Joint Representation Learning of Cross-lingual Words and Entities via
Attentive Distant Supervision

Anonymous ACL submission

Abstract

Jointly learning word and entity represen-
tations benefits many NLP tasks, while has
not been well explored in cross-lingual set-
tings. In this paper, we propose a novel
method that integrates cross-lingual word
and entity representation learning to enable
joint inference among knowledge base and
text across languages, capturing mutually
complementary knowledge. Instead of re-
liance on parallel data, we automatically
generate cross-lingual training data via dis-
tant supervision over multi-lingual knowl-
edge bases. We also propose two types
of knowledge attention and cross-lingual
attention to select the most informative
words and filter out noise, which will fur-
ther improve the performance. In exper-
iments, separate tasks of word translation
and entity relatedness demonstrate the ef-
fectiveness of our method with an average
gain of 20% and 3% over baselines, re-
spectively. Using entity linking as a case
study, the results on benchmark dataset
verify the quality of our embeddings.

1 Introduction

Multi-lingual knowledge bases (KB), storing mil-
lions of entities and their facts in various lan-
guages, provide rich structured knowledge for un-
derstanding natural language beyond texts. Mean-
while, abundant text corpus contains large amount
of potential knowledge complementary to existing
KBs. Therefore, researchers leverage both types
of resources to improve various natural language
processing (NLP) related tasks, such as relation ex-
traction (Weston et al., 2013; Lin et al., 2017), and
entity linking (Tsai and Roth, 2016; Yamada et al.,
2016; Cao et al., 2017; Ji et al., 2016).

Most existing work jointly models KB and text
corpus to enhance each other by learning word and
entity representations in a unified vector space. For
example, (Wang et al., 2014; Yamada et al., 2016;
Cao et al., 2017) utilize the coherence informa-
tion to align similar words and entities with sim-
ilar embedding vectors. Another approach in (Han
et al., 2016; Toutanova et al., 2015; Wu et al.,
2016) learns to represent entities based on their
textual descriptions together with the structured re-
lations. However, these methods only focus on
mono-lingual settings, and few researches have
been done in cross-lingual scenarios.
In this paper, we propose to learn cross-lingual

word and entity representations in the same seman-
tic space, to enable joint inference among KB and
text across languages without any additional trans-
lation mechanism, which is usually expensive and
may introduce inevitable errors. Our embeddings
are helpful to break down language gaps in many
tasks, such as cross-lingual entity linking, in which
the major challenge lies in measuring the similar-
ity between entities and corresponding mentioned
words in different languages.
e , w ,w
The intuition is that, words and entities in

various languages share some common semantic
meanings1, but there are also ways in which they
differ. On one hand, we utilize their shared seman-
tics to align similar words and entities with simi-
lar embedding vectors, no matter they are in the
same language or not. On the other hand, cross-
lingual embeddings will benefit from different lan-
guages due to the complementary knowledge. For
instance, textual ambiguity in one language may
disappear in another language, e.g., the two mean-

1Some cross-lingual pioneering work observe that word
embeddings trained separately on monolingual corpora ex-
hibit isomorphic structure across languages (Mikolov et al.,
2013; Zhang et al., 2017).

1

000

001

002

003

004

005

006

007

008

009

010

011

012

013

014

015

016

017

018

019

020

021

022

023

024

025

026

027

028

029

030

031

032

033

034

035

036

037

038

039

040

041

042

043

044

045

046

047

048

049

050

051

052

053

054

055

056

057

058

059

060

061

062

063

064

065

066

067

068

069

070

071

072

073

074

075

076

077

078

079

080

081

082

083

084

085

086

087

088

089

090

091

092

093

094

095

096

097

098

099

ACL 2018 Submission ***. Confidential Review Copy. DO NOT DISTRIBUTE.

Joint Representation Learning of Cross-lingual Words and Entities via
Attentive Distant Supervision

Anonymous ACL submission

Abstract

Jointly learning word and entity represen-
tations benefits many NLP tasks, while has
not been well explored in cross-lingual set-
tings. In this paper, we propose a novel
method that integrates cross-lingual word
and entity representation learning to enable
joint inference among knowledge base and
text across languages, capturing mutually
complementary knowledge. Instead of re-
liance on parallel data, we automatically
generate cross-lingual training data via dis-
tant supervision over multi-lingual knowl-
edge bases. We also propose two types
of knowledge attention and cross-lingual
attention to select the most informative
words and filter out noise, which will fur-
ther improve the performance. In exper-
iments, separate tasks of word translation
and entity relatedness demonstrate the ef-
fectiveness of our method with an average
gain of 20% and 3% over baselines, re-
spectively. Using entity linking as a case
study, the results on benchmark dataset
verify the quality of our embeddings.

1 Introduction

Multi-lingual knowledge bases (KB), storing mil-
lions of entities and their facts in various lan-
guages, provide rich structured knowledge for un-
derstanding natural language beyond texts. Mean-
while, abundant text corpus contains large amount
of potential knowledge complementary to existing
KBs. Therefore, researchers leverage both types
of resources to improve various natural language
processing (NLP) related tasks, such as relation ex-
traction (Weston et al., 2013; Lin et al., 2017), and
entity linking (Tsai and Roth, 2016; Yamada et al.,
2016; Cao et al., 2017; Ji et al., 2016).

Most existing work jointly models KB and text
corpus to enhance each other by learning word and
entity representations in a unified vector space. For
example, (Wang et al., 2014; Yamada et al., 2016;
Cao et al., 2017) utilize the coherence informa-
tion to align similar words and entities with sim-
ilar embedding vectors. Another approach in (Han
et al., 2016; Toutanova et al., 2015; Wu et al.,
2016) learns to represent entities based on their
textual descriptions together with the structured re-
lations. However, these methods only focus on
mono-lingual settings, and few researches have
been done in cross-lingual scenarios.
In this paper, we propose to learn cross-lingual

word and entity representations in the same seman-
tic space, to enable joint inference among KB and
text across languages without any additional trans-
lation mechanism, which is usually expensive and
may introduce inevitable errors. Our embeddings
are helpful to break down language gaps in many
tasks, such as cross-lingual entity linking, in which
the major challenge lies in measuring the similar-
ity between entities and corresponding mentioned
words in different languages.
eNBA , w , w ,w
The intuition is that, words and entities in

various languages share some common semantic
meanings1, but there are also ways in which they
differ. On one hand, we utilize their shared seman-
tics to align similar words and entities with simi-
lar embedding vectors, no matter they are in the
same language or not. On the other hand, cross-
lingual embeddings will benefit from different lan-
guages due to the complementary knowledge. For
instance, textual ambiguity in one language may
disappear in another language, e.g., the two mean-

1Some cross-lingual pioneering work observe that word
embeddings trained separately on monolingual corpora ex-
hibit isomorphic structure across languages (Mikolov et al.,
2013; Zhang et al., 2017).

1

000

001

002

003

004

005

006

007

008

009

010

011

012

013

014

015

016

017

018

019

020

021

022

023

024

025

026

027

028

029

030

031

032

033

034

035

036

037

038

039

040

041

042

043

044

045

046

047

048

049

050

051

052

053

054

055

056

057

058

059

060

061

062

063

064

065

066

067

068

069

070

071

072

073

074

075

076

077

078

079

080

081

082

083

084

085

086

087

088

089

090

091

092

093

094

095

096

097

098

099

ACL 2018 Submission ***. Confidential Review Copy. DO NOT DISTRIBUTE.

Joint Representation Learning of Cross-lingual Words and Entities via
Attentive Distant Supervision

Anonymous ACL submission

Abstract

Jointly learning word and entity represen-
tations benefits many NLP tasks, while has
not been well explored in cross-lingual set-
tings. In this paper, we propose a novel
method that integrates cross-lingual word
and entity representation learning to enable
joint inference among knowledge base and
text across languages, capturing mutually
complementary knowledge. Instead of re-
liance on parallel data, we automatically
generate cross-lingual training data via dis-
tant supervision over multi-lingual knowl-
edge bases. We also propose two types
of knowledge attention and cross-lingual
attention to select the most informative
words and filter out noise, which will fur-
ther improve the performance. In exper-
iments, separate tasks of word translation
and entity relatedness demonstrate the ef-
fectiveness of our method with an average
gain of 20% and 3% over baselines, re-
spectively. Using entity linking as a case
study, the results on benchmark dataset
verify the quality of our embeddings.

1 Introduction

Multi-lingual knowledge bases (KB), storing mil-
lions of entities and their facts in various lan-
guages, provide rich structured knowledge for un-
derstanding natural language beyond texts. Mean-
while, abundant text corpus contains large amount
of potential knowledge complementary to existing
KBs. Therefore, researchers leverage both types
of resources to improve various natural language
processing (NLP) related tasks, such as relation ex-
traction (Weston et al., 2013; Lin et al., 2017), and
entity linking (Tsai and Roth, 2016; Yamada et al.,
2016; Cao et al., 2017; Ji et al., 2016).

Most existing work jointly models KB and text
corpus to enhance each other by learning word and
entity representations in a unified vector space. For
example, (Wang et al., 2014; Yamada et al., 2016;
Cao et al., 2017) utilize the coherence informa-
tion to align similar words and entities with sim-
ilar embedding vectors. Another approach in (Han
et al., 2016; Toutanova et al., 2015; Wu et al.,
2016) learns to represent entities based on their
textual descriptions together with the structured re-
lations. However, these methods only focus on
mono-lingual settings, and few researches have
been done in cross-lingual scenarios.
In this paper, we propose to learn cross-lingual

word and entity representations in the same seman-
tic space, to enable joint inference among KB and
text across languages without any additional trans-
lation mechanism, which is usually expensive and
may introduce inevitable errors. Our embeddings
are helpful to break down language gaps in many
tasks, such as cross-lingual entity linking, in which
the major challenge lies in measuring the similar-
ity between entities and corresponding mentioned
words in different languages.
eNBA , w , w ,w
The intuition is that, words and entities in

various languages share some common semantic
meanings1, but there are also ways in which they
differ. On one hand, we utilize their shared seman-
tics to align similar words and entities with simi-
lar embedding vectors, no matter they are in the
same language or not. On the other hand, cross-
lingual embeddings will benefit from different lan-
guages due to the complementary knowledge. For
instance, textual ambiguity in one language may
disappear in another language, e.g., the two mean-

1Some cross-lingual pioneering work observe that word
embeddings trained separately on monolingual corpora ex-
hibit isomorphic structure across languages (Mikolov et al.,
2013; Zhang et al., 2017).

1

000

001

002

003

004

005

006

007

008

009

010

011

012

013

014

015

016

017

018

019

020

021

022

023

024

025

026

027

028

029

030

031

032

033

034

035

036

037

038

039

040

041

042

043

044

045

046

047

048

049

050

051

052

053

054

055

056

057

058

059

060

061

062

063

064

065

066

067

068

069

070

071

072

073

074

075

076

077

078

079

080

081

082

083

084

085

086

087

088

089

090

091

092

093

094

095

096

097

098

099

ACL 2018 Submission ***. Confidential Review Copy. DO NOT DISTRIBUTE.

Joint Representation Learning of Cross-lingual Words and Entities via
Attentive Distant Supervision

Anonymous ACL submission

Abstract

Jointly learning word and entity represen-
tations benefits many NLP tasks, while has
not been well explored in cross-lingual set-
tings. In this paper, we propose a novel
method that integrates cross-lingual word
and entity representation learning to enable
joint inference among knowledge base and
text across languages, capturing mutually
complementary knowledge. Instead of re-
liance on parallel data, we automatically
generate cross-lingual training data via dis-
tant supervision over multi-lingual knowl-
edge bases. We also propose two types
of knowledge attention and cross-lingual
attention to select the most informative
words and filter out noise, which will fur-
ther improve the performance. In exper-
iments, separate tasks of word translation
and entity relatedness demonstrate the ef-
fectiveness of our method with an average
gain of 20% and 3% over baselines, re-
spectively. Using entity linking as a case
study, the results on benchmark dataset
verify the quality of our embeddings.

1 Introduction

Multi-lingual knowledge bases (KB), storing mil-
lions of entities and their facts in various lan-
guages, provide rich structured knowledge for un-
derstanding natural language beyond texts. Mean-
while, abundant text corpus contains large amount
of potential knowledge complementary to existing
KBs. Therefore, researchers leverage both types
of resources to improve various natural language
processing (NLP) related tasks, such as relation ex-
traction (Weston et al., 2013; Lin et al., 2017), and
entity linking (Tsai and Roth, 2016; Yamada et al.,
2016; Cao et al., 2017; Ji et al., 2016).

Most existing work jointly models KB and text
corpus to enhance each other by learning word and
entity representations in a unified vector space. For
example, (Wang et al., 2014; Yamada et al., 2016;
Cao et al., 2017) utilize the coherence informa-
tion to align similar words and entities with sim-
ilar embedding vectors. Another approach in (Han
et al., 2016; Toutanova et al., 2015; Wu et al.,
2016) learns to represent entities based on their
textual descriptions together with the structured re-
lations. However, these methods only focus on
mono-lingual settings, and few researches have
been done in cross-lingual scenarios.
In this paper, we propose to learn cross-lingual

word and entity representations in the same seman-
tic space, to enable joint inference among KB and
text across languages without any additional trans-
lation mechanism, which is usually expensive and
may introduce inevitable errors. Our embeddings
are helpful to break down language gaps in many
tasks, such as cross-lingual entity linking, in which
the major challenge lies in measuring the similar-
ity between entities and corresponding mentioned
words in different languages.
eNBA , w , w ,w
The intuition is that, words and entities in

various languages share some common semantic
meanings1, but there are also ways in which they
differ. On one hand, we utilize their shared seman-
tics to align similar words and entities with simi-
lar embedding vectors, no matter they are in the
same language or not. On the other hand, cross-
lingual embeddings will benefit from different lan-
guages due to the complementary knowledge. For
instance, textual ambiguity in one language may
disappear in another language, e.g., the two mean-

1Some cross-lingual pioneering work observe that word
embeddings trained separately on monolingual corpora ex-
hibit isomorphic structure across languages (Mikolov et al.,
2013; Zhang et al., 2017).

1

000

001

002

003

004

005

006

007

008

009

010

011

012

013

014

015

016

017

018

019

020

021

022

023

024

025

026

027

028

029

030

031

032

033

034

035

036

037

038

039

040

041

042

043

044

045

046

047

048

049

050

051

052

053

054

055

056

057

058

059

060

061

062

063

064

065

066

067

068

069

070

071

072

073

074

075

076

077

078

079

080

081

082

083

084

085

086

087

088

089

090

091

092

093

094

095

096

097

098

099

ACL 2018 Submission ***. Confidential Review Copy. DO NOT DISTRIBUTE.

Joint Representation Learning of Cross-lingual Words and Entities via
Attentive Distant Supervision

Anonymous ACL submission

Abstract

Jointly learning word and entity represen-
tations benefits many NLP tasks, while has
not been well explored in cross-lingual set-
tings. In this paper, we propose a novel
method that integrates cross-lingual word
and entity representation learning to enable
joint inference among knowledge base and
text across languages, capturing mutually
complementary knowledge. Instead of re-
liance on parallel data, we automatically
generate cross-lingual training data via dis-
tant supervision over multi-lingual knowl-
edge bases. We also propose two types
of knowledge attention and cross-lingual
attention to select the most informative
words and filter out noise, which will fur-
ther improve the performance. In exper-
iments, separate tasks of word translation
and entity relatedness demonstrate the ef-
fectiveness of our method with an average
gain of 20% and 3% over baselines, re-
spectively. Using entity linking as a case
study, the results on benchmark dataset
verify the quality of our embeddings.

1 Introduction

Multi-lingual knowledge bases (KB), storing mil-
lions of entities and their facts in various lan-
guages, provide rich structured knowledge for un-
derstanding natural language beyond texts. Mean-
while, abundant text corpus contains large amount
of potential knowledge complementary to existing
KBs. Therefore, researchers leverage both types
of resources to improve various natural language
processing (NLP) related tasks, such as relation ex-
traction (Weston et al., 2013; Lin et al., 2017), and
entity linking (Tsai and Roth, 2016; Yamada et al.,
2016; Cao et al., 2017; Ji et al., 2016).

Most existing work jointly models KB and text
corpus to enhance each other by learning word and
entity representations in a unified vector space. For
example, (Wang et al., 2014; Yamada et al., 2016;
Cao et al., 2017) utilize the coherence informa-
tion to align similar words and entities with sim-
ilar embedding vectors. Another approach in (Han
et al., 2016; Toutanova et al., 2015; Wu et al.,
2016) learns to represent entities based on their
textual descriptions together with the structured re-
lations. However, these methods only focus on
mono-lingual settings, and few researches have
been done in cross-lingual scenarios.
In this paper, we propose to learn cross-lingual

word and entity representations in the same seman-
tic space, to enable joint inference among KB and
text across languages without any additional trans-
lation mechanism, which is usually expensive and
may introduce inevitable errors. Our embeddings
are helpful to break down language gaps in many
tasks, such as cross-lingual entity linking, in which
the major challenge lies in measuring the similar-
ity between entities and corresponding mentioned
words in different languages.
eNBA , w , w ,w
The intuition is that, words and entities in

various languages share some common semantic
meanings1, but there are also ways in which they
differ. On one hand, we utilize their shared seman-
tics to align similar words and entities with simi-
lar embedding vectors, no matter they are in the
same language or not. On the other hand, cross-
lingual embeddings will benefit from different lan-
guages due to the complementary knowledge. For
instance, textual ambiguity in one language may
disappear in another language, e.g., the two mean-

1Some cross-lingual pioneering work observe that word
embeddings trained separately on monolingual corpora ex-
hibit isomorphic structure across languages (Mikolov et al.,
2013; Zhang et al., 2017).

1

000

001

002

003

004

005

006

007

008

009

010

011

012

013

014

015

016

017

018

019

020

021

022

023

024

025

026

027

028

029

030

031

032

033

034

035

036

037

038

039

040

041

042

043

044

045

046

047

048

049

050

051

052

053

054

055

056

057

058

059

060

061

062

063

064

065

066

067

068

069

070

071

072

073

074

075

076

077

078

079

080

081

082

083

084

085

086

087

088

089

090

091

092

093

094

095

096

097

098

099

ACL 2018 Submission ***. Confidential Review Copy. DO NOT DISTRIBUTE.

Joint Representation Learning of Cross-lingual Words and Entities via
Attentive Distant Supervision

Anonymous ACL submission

Abstract

Jointly learning word and entity represen-
tations benefits many NLP tasks, while has
not been well explored in cross-lingual set-
tings. In this paper, we propose a novel
method that integrates cross-lingual word
and entity representation learning to enable
joint inference among knowledge base and
text across languages, capturing mutually
complementary knowledge. Instead of re-
liance on parallel data, we automatically
generate cross-lingual training data via dis-
tant supervision over multi-lingual knowl-
edge bases. We also propose two types
of knowledge attention and cross-lingual
attention to select the most informative
words and filter out noise, which will fur-
ther improve the performance. In exper-
iments, separate tasks of word translation
and entity relatedness demonstrate the ef-
fectiveness of our method with an average
gain of 20% and 3% over baselines, re-
spectively. Using entity linking as a case
study, the results on benchmark dataset
verify the quality of our embeddings.

1 Introduction

Multi-lingual knowledge bases (KB), storing mil-
lions of entities and their facts in various lan-
guages, provide rich structured knowledge for un-
derstanding natural language beyond texts. Mean-
while, abundant text corpus contains large amount
of potential knowledge complementary to existing
KBs. Therefore, researchers leverage both types
of resources to improve various natural language
processing (NLP) related tasks, such as relation ex-
traction (Weston et al., 2013; Lin et al., 2017), and
entity linking (Tsai and Roth, 2016; Yamada et al.,
2016; Cao et al., 2017; Ji et al., 2016).

Most existing work jointly models KB and text
corpus to enhance each other by learning word and
entity representations in a unified vector space. For
example, (Wang et al., 2014; Yamada et al., 2016;
Cao et al., 2017) utilize the coherence informa-
tion to align similar words and entities with sim-
ilar embedding vectors. Another approach in (Han
et al., 2016; Toutanova et al., 2015; Wu et al.,
2016) learns to represent entities based on their
textual descriptions together with the structured re-
lations. However, these methods only focus on
mono-lingual settings, and few researches have
been done in cross-lingual scenarios.
In this paper, we propose to learn cross-lingual

word and entity representations in the same seman-
tic space, to enable joint inference among KB and
text across languages without any additional trans-
lation mechanism, which is usually expensive and
may introduce inevitable errors. Our embeddings
are helpful to break down language gaps in many
tasks, such as cross-lingual entity linking, in which
the major challenge lies in measuring the similar-
ity between entities and corresponding mentioned
words in different languages.
eNBA
The intuition is that, words and entities in

various languages share some common semantic
meanings1, but there are also ways in which they
differ. On one hand, we utilize their shared seman-
tics to align similar words and entities with simi-
lar embedding vectors, no matter they are in the
same language or not. On the other hand, cross-
lingual embeddings will benefit from different lan-
guages due to the complementary knowledge. For
instance, textual ambiguity in one language may
disappear in another language, e.g., the two mean-

1Some cross-lingual pioneering work observe that word
embeddings trained separately on monolingual corpora ex-
hibit isomorphic structure across languages (Mikolov et al.,
2013; Zhang et al., 2017).

En

eLawrence

wwas wan

eLawrence

eNBAeAllstarwan wbasketball

wAmerican

Figure 1: The framework of our method. The inputs and outputs of each step are listed in the right
side, and in the left side there are three main components of joint representation learning. Red texts
with brackets are anchors, dashed lines between entities denote relations, and solid lines are cross-lingual
links.

mon neighbors have similar embeddings, no mat-
ter in which language, e.g., English entity Foust
and Chinese entity 福斯特 (Foust) are embed-
ded close in semantic space due to the common
neighborsNBA,All-star andNBA选秀 (draft), etc.
(3) Cross-lingual Sentence Regularizer aims to
learn mutually translated words with similar em-
beddings by pushing their cross-lingual contexts
(i.e. comparable sentences) together. For exam-
ple, English word basketball and the translated
Chinese word 篮球 frequently co-occur in com-
parable sentences, and are close in the semantic
space.
All word/entity embeddings are trained jointly

under a unified optimization objective. Next, we
will introduce how to generate bi-lingual EN and
comparable sentences as well as the three compo-
nents for joint representation learning in turn.

3.3 Cross-lingual Supervision Data
Generation

This section introduces how to extract more cross-
lingual clues from multi-lingual KB in the form of
bi-lingual EN and comparable sentences.

Bi-lingual Entity Network Construction

Conventional knowledge representation meth-
ods normally regard cross-lingual links as a spe-
cial equivalence type of relation between two en-
tities (Zhu et al., 2017). However, we argue that
this may mislead to an inconsistent training ob-

jective since a cross-lingual link actually contains
multiple relations. For example (Figure 1), there
will be no direct relation betweenChinese entity福
斯特 (Foust) and English entity Piston by merely
adding the equivalence relation between Foust and
福斯特, which is in contradiction with the fact that
Foust belongs to Piston, no matter in which lan-
guage.
Therefore, we build bi-lingual entity network by

making cross-lingual linked entities that inherit all
relations from each other. Concretely, we enhance
mono-EN by adding edges from all neighbors of
entity eei to ezj if < eei , ezj >∈ Re−z (layer 2).
e

Comparable Sentences Generation

We utilize distant supervision to generate com-
parable sentences from Wikipedia articles. As
shown in Figure 1, from the page articles of cross-
lingual linked entities eeKobe and e

z
Kobe, we extract

those sentences including another cross-lingual
linked entities eeJoe and ezJoe as comparable sen-
tences Se−z = {< sek, szk >}.
The intuition is that we consider each sentence

in a Wikipedia article has a pseudo mention of
the page entity (talking something about the en-
tity) (Yamada et al., 2017). Thus, if a sentence also
mentions another entity, it implicitly expresses
their relation. Therefore, we make a similar as-
sumption as in relation extraction: If two enti-
ties participate in a relation, and both of them

4

300

301

302

303

304

305

306

307

308

309

310

311

312

313

314

315

316

317

318

319

320

321

322

323

324

325

326

327

328

329

330

331

332

333

334

335

336

337

338

339

340

341

342

343

344

345

346

347

348

349

350

351

352

353

354

355

356

357

358

359

360

361

362

363

364

365

366

367

368

369

370

371

372

373

374

375

376

377

378

379

380

381

382

383

384

385

386

387

388

389

390

391

392

393

394

395

396

397

398

399

ACL 2018 Submission ***. Confidential Review Copy. DO NOT DISTRIBUTE.

NBA

Lawrence Foust

All-star

[[Lawrence Michael Foust]] was
an American basketball player

who spent 12 seasons in [[NBA]]

NBA (zh)

·
[[ · ]] [[NBA]]

1950 [[NBA ]] 1 5

English KB Chinese KB

独立日 美国独立日

Sen Szh een ezh

L =
∑

xi ∈D

∑

xo ∈C(xi )
P(xo |xi)

L =
∑

(ei ,ej )∈Ec
| |C(ei) − C(e j)| |2

Ec

L =
∑

ei ∈E
P(N (ei)|ei) +

∑

(ei ,ej )∈Ec
P(N (e j)|ei)

N (ei)

L =
∑

(ei ,mi )∈A
P(ei |mi ,C(ei)) +

∑

(ei ,ej )∈Ec
P(e j |mi ,C(ei))

独立日 美国独立日

Sen Szh een ezh

L =
∑

xi ∈D

∑

xo ∈C(xi )
P(xo |xi)

L =
∑

(ei ,ej )∈Ec
| |C(ei) − C(e j)| |2

Ec

L =
∑

ei ∈E
P(N (ei)|ei) +

∑

(ei ,ej )∈Ec
P(N (e j)|ei)

N (ei)

L =
∑

(ei ,mi )∈A
P(ei |mi ,C(ei)) +

∑

(ei ,ej )∈Ec
P(e j |mi ,C(ei))

was

Lawrence
NBA

NBA (zh)

basketball

Semantic Space

Representation
Learning

Cross-lingual
Sentence Regularizer

Cross-lingual
Entity Regularizer

[[ · ]] [[NBA]]

All-star

[[Lawrence Michael Foust]] was an American basketball 
player who spent 12 seasons in [[NBA]]

·

NBA
NBA (zh)

Comparable Sentences

Bi-lingual EN

att
att

att

… …

Ls = | |Sen − Szh | |2

L =
∑

xi ∈D

∑

xo ∈C(xi )
P(xo |xi )

L =
∑

(ei ,ej )∈Ec
| |C(ei ) − C(ej )| |2

Ec

L =
∑

ei ∈E
P(N (ei )|ei ) +

∑

(ei ,ej )∈Ec
P(N (ej )|ei )

N (ei )

L =
∑

(ei ,mi )∈A
P(ei |mi ,C(ei )) +

∑

(ei ,ej )∈Ec
P(ej |mi ,C(ei ))

=

500

501

502

503

504

505

506

507

508

509

510

511

512

513

514

515

516

517

518

519

520

521

522

523

524

525

526

527

528

529

530

531

532

533

534

535

536

537

538

539

540

541

542

543

544

545

546

547

548

549

550

551

552

553

554

555

556

557

558

559

560

561

562

563

564

565

566

567

568

569

570

571

572

573

574

575

576

577

578

579

580

581

582

583

584

585

586

587

588

589

590

591

592

593

594

595

596

597

598

599

ACL 2018 Submission ***. Confidential Review Copy. DO NOT DISTRIBUTE.

skm ≈
wki +

∑
C(wki)

Ls =
∑

<skm,slm>∈Sk,l

|| − ||2

ekm
ekm

k

αnkm ∝ sim( ,
∑

wki∈snkm

)

αnkm
n skm

ekm αnkm

αki,lj ∝
wki∈skm,wlj∈slm

sim(wki, wlj)

=
∑

snkm∈skm

αnkm
∑

wki∈snkm

αki,lj

L = Lc + γLs

γ

400

401

402

403

404

405

406

407

408

409

410

411

412

413

414

415

416

417

418

419

420

421

422

423

424

425

426

427

428

429

430

431

432

433

434

435

436

437

438

439

440

441

442

443

444

445

446

447

448

449

450

451

452

453

454

455

456

457

458

459

460

461

462

463

464

465

466

467

468

469

470

471

472

473

474

475

476

477

478

479

480

481

482

483

484

485

486

487

488

489

490

491

492

493

494

495

496

497

498

499

ACL 2018 Submission ***. Confidential Review Copy. DO NOT DISTRIBUTE.

eki elj < eki, elj >∈ Rc

een,Kobe
ezh,Kobe

een,Joe ezh,Joe

< een,Kobe, ezh,Kobe >
< een,Joe, ezh,Joe >

D A
KN

Lc =
∑

xi∈{D,A,KN}

P (C(xi)|xi)

xi
C(xi)

C(xi)
xi

xi ∈ D
xi ∈ KN

wj xi
< wj , ei >∈ A

een,Joe wen,player
wen,NBA

een,LosAngelesLakers
een,Joe een,Kobe ezh,Kobe

wki
Cl(wki)

Ls =
∑

<skm,slm∈Sk,l

∑

wki∈skm

||wki −
∑

Cl(wki)||2

< skm, slm >∈ Sk,l m
k l

Cross-lingual
 supervision

Data Generation

and was an 8-time [[All-star]] … [[ ]] …

NBA

Lawrence Foust

NBA

player

American
NBA

All-star
eLawrence

wbasketballwplayer
… …

1

000

001

002

003

004

005

006

007

008

009

010

011

012

013

014

015

016

017

018

019

020

021

022

023

024

025

026

027

028

029

030

031

032

033

034

035

036

037

038

039

040

041

042

043

044

045

046

047

048

049

050

051

052

053

054

055

056

057

058

059

060

061

062

063

064

065

066

067

068

069

070

071

072

073

074

075

076

077

078

079

080

081

082

083

084

085

086

087

088

089

090

091

092

093

094

095

096

097

098

099

ACL 2018 Submission ***. Confidential Review Copy. DO NOT DISTRIBUTE.

Joint Representation Learning of Cross-lingual Words and Entities via
Attentive Distant Supervision

Anonymous ACL submission

Abstract

Jointly learning word and entity represen-
tations benefits many NLP tasks, while has
not been well explored in cross-lingual set-
tings. In this paper, we propose a novel
method that integrates cross-lingual word
and entity representation learning to enable
joint inference among knowledge base and
text across languages, capturing mutually
complementary knowledge. Instead of re-
liance on parallel data, we automatically
generate cross-lingual training data via dis-
tant supervision over multi-lingual knowl-
edge bases. We also propose two types
of knowledge attention and cross-lingual
attention to select the most informative
words and filter out noise, which will fur-
ther improve the performance. In exper-
iments, separate tasks of word translation
and entity relatedness demonstrate the ef-
fectiveness of our method with an average
gain of 20% and 3% over baselines, re-
spectively. Using entity linking as a case
study, the results on benchmark dataset
verify the quality of our embeddings.

1 Introduction

Multi-lingual knowledge bases (KB), storing mil-
lions of entities and their facts in various lan-
guages, provide rich structured knowledge for un-
derstanding natural language beyond texts. Mean-
while, abundant text corpus contains large amount
of potential knowledge complementary to existing
KBs. Therefore, researchers leverage both types
of resources to improve various natural language
processing (NLP) related tasks, such as relation ex-
traction (Weston et al., 2013; Lin et al., 2017), and
entity linking (Tsai and Roth, 2016; Yamada et al.,
2016; Cao et al., 2017; Ji et al., 2016).

Most existing work jointly models KB and text
corpus to enhance each other by learning word and
entity representations in a unified vector space. For
example, (Wang et al., 2014; Yamada et al., 2016;
Cao et al., 2017) utilize the coherence informa-
tion to align similar words and entities with sim-
ilar embedding vectors. Another approach in (Han
et al., 2016; Toutanova et al., 2015; Wu et al.,
2016) learns to represent entities based on their
textual descriptions together with the structured re-
lations. However, these methods only focus on
mono-lingual settings, and few researches have
been done in cross-lingual scenarios.
In this paper, we propose to learn cross-lingual

word and entity representations in the same seman-
tic space, to enable joint inference among KB and
text across languages without any additional trans-
lation mechanism, which is usually expensive and
may introduce inevitable errors. Our embeddings
are helpful to break down language gaps in many
tasks, such as cross-lingual entity linking, in which
the major challenge lies in measuring the similar-
ity between entities and corresponding mentioned
words in different languages.
e , w ,w
The intuition is that, words and entities in

various languages share some common semantic
meanings1, but there are also ways in which they
differ. On one hand, we utilize their shared seman-
tics to align similar words and entities with simi-
lar embedding vectors, no matter they are in the
same language or not. On the other hand, cross-
lingual embeddings will benefit from different lan-
guages due to the complementary knowledge. For
instance, textual ambiguity in one language may
disappear in another language, e.g., the two mean-

1Some cross-lingual pioneering work observe that word
embeddings trained separately on monolingual corpora ex-
hibit isomorphic structure across languages (Mikolov et al.,
2013; Zhang et al., 2017).

1

000

001

002

003

004

005

006

007

008

009

010

011

012

013

014

015

016

017

018

019

020

021

022

023

024

025

026

027

028

029

030

031

032

033

034

035

036

037

038

039

040

041

042

043

044

045

046

047

048

049

050

051

052

053

054

055

056

057

058

059

060

061

062

063

064

065

066

067

068

069

070

071

072

073

074

075

076

077

078

079

080

081

082

083

084

085

086

087

088

089

090

091

092

093

094

095

096

097

098

099

ACL 2018 Submission ***. Confidential Review Copy. DO NOT DISTRIBUTE.

Joint Representation Learning of Cross-lingual Words and Entities via
Attentive Distant Supervision

Anonymous ACL submission

Abstract

Jointly learning word and entity represen-
tations benefits many NLP tasks, while has
not been well explored in cross-lingual set-
tings. In this paper, we propose a novel
method that integrates cross-lingual word
and entity representation learning to enable
joint inference among knowledge base and
text across languages, capturing mutually
complementary knowledge. Instead of re-
liance on parallel data, we automatically
generate cross-lingual training data via dis-
tant supervision over multi-lingual knowl-
edge bases. We also propose two types
of knowledge attention and cross-lingual
attention to select the most informative
words and filter out noise, which will fur-
ther improve the performance. In exper-
iments, separate tasks of word translation
and entity relatedness demonstrate the ef-
fectiveness of our method with an average
gain of 20% and 3% over baselines, re-
spectively. Using entity linking as a case
study, the results on benchmark dataset
verify the quality of our embeddings.

1 Introduction

Multi-lingual knowledge bases (KB), storing mil-
lions of entities and their facts in various lan-
guages, provide rich structured knowledge for un-
derstanding natural language beyond texts. Mean-
while, abundant text corpus contains large amount
of potential knowledge complementary to existing
KBs. Therefore, researchers leverage both types
of resources to improve various natural language
processing (NLP) related tasks, such as relation ex-
traction (Weston et al., 2013; Lin et al., 2017), and
entity linking (Tsai and Roth, 2016; Yamada et al.,
2016; Cao et al., 2017; Ji et al., 2016).

Most existing work jointly models KB and text
corpus to enhance each other by learning word and
entity representations in a unified vector space. For
example, (Wang et al., 2014; Yamada et al., 2016;
Cao et al., 2017) utilize the coherence informa-
tion to align similar words and entities with sim-
ilar embedding vectors. Another approach in (Han
et al., 2016; Toutanova et al., 2015; Wu et al.,
2016) learns to represent entities based on their
textual descriptions together with the structured re-
lations. However, these methods only focus on
mono-lingual settings, and few researches have
been done in cross-lingual scenarios.
In this paper, we propose to learn cross-lingual

word and entity representations in the same seman-
tic space, to enable joint inference among KB and
text across languages without any additional trans-
lation mechanism, which is usually expensive and
may introduce inevitable errors. Our embeddings
are helpful to break down language gaps in many
tasks, such as cross-lingual entity linking, in which
the major challenge lies in measuring the similar-
ity between entities and corresponding mentioned
words in different languages.
e , w ,w
The intuition is that, words and entities in

various languages share some common semantic
meanings1, but there are also ways in which they
differ. On one hand, we utilize their shared seman-
tics to align similar words and entities with simi-
lar embedding vectors, no matter they are in the
same language or not. On the other hand, cross-
lingual embeddings will benefit from different lan-
guages due to the complementary knowledge. For
instance, textual ambiguity in one language may
disappear in another language, e.g., the two mean-

1Some cross-lingual pioneering work observe that word
embeddings trained separately on monolingual corpora ex-
hibit isomorphic structure across languages (Mikolov et al.,
2013; Zhang et al., 2017).

1

000

001

002

003

004

005

006

007

008

009

010

011

012

013

014

015

016

017

018

019

020

021

022

023

024

025

026

027

028

029

030

031

032

033

034

035

036

037

038

039

040

041

042

043

044

045

046

047

048

049

050

051

052

053

054

055

056

057

058

059

060

061

062

063

064

065

066

067

068

069

070

071

072

073

074

075

076

077

078

079

080

081

082

083

084

085

086

087

088

089

090

091

092

093

094

095

096

097

098

099

ACL 2018 Submission ***. Confidential Review Copy. DO NOT DISTRIBUTE.

Joint Representation Learning of Cross-lingual Words and Entities via
Attentive Distant Supervision

Anonymous ACL submission

Abstract

Jointly learning word and entity represen-
tations benefits many NLP tasks, while has
not been well explored in cross-lingual set-
tings. In this paper, we propose a novel
method that integrates cross-lingual word
and entity representation learning to enable
joint inference among knowledge base and
text across languages, capturing mutually
complementary knowledge. Instead of re-
liance on parallel data, we automatically
generate cross-lingual training data via dis-
tant supervision over multi-lingual knowl-
edge bases. We also propose two types
of knowledge attention and cross-lingual
attention to select the most informative
words and filter out noise, which will fur-
ther improve the performance. In exper-
iments, separate tasks of word translation
and entity relatedness demonstrate the ef-
fectiveness of our method with an average
gain of 20% and 3% over baselines, re-
spectively. Using entity linking as a case
study, the results on benchmark dataset
verify the quality of our embeddings.

1 Introduction

Multi-lingual knowledge bases (KB), storing mil-
lions of entities and their facts in various lan-
guages, provide rich structured knowledge for un-
derstanding natural language beyond texts. Mean-
while, abundant text corpus contains large amount
of potential knowledge complementary to existing
KBs. Therefore, researchers leverage both types
of resources to improve various natural language
processing (NLP) related tasks, such as relation ex-
traction (Weston et al., 2013; Lin et al., 2017), and
entity linking (Tsai and Roth, 2016; Yamada et al.,
2016; Cao et al., 2017; Ji et al., 2016).

Most existing work jointly models KB and text
corpus to enhance each other by learning word and
entity representations in a unified vector space. For
example, (Wang et al., 2014; Yamada et al., 2016;
Cao et al., 2017) utilize the coherence informa-
tion to align similar words and entities with sim-
ilar embedding vectors. Another approach in (Han
et al., 2016; Toutanova et al., 2015; Wu et al.,
2016) learns to represent entities based on their
textual descriptions together with the structured re-
lations. However, these methods only focus on
mono-lingual settings, and few researches have
been done in cross-lingual scenarios.
In this paper, we propose to learn cross-lingual

word and entity representations in the same seman-
tic space, to enable joint inference among KB and
text across languages without any additional trans-
lation mechanism, which is usually expensive and
may introduce inevitable errors. Our embeddings
are helpful to break down language gaps in many
tasks, such as cross-lingual entity linking, in which
the major challenge lies in measuring the similar-
ity between entities and corresponding mentioned
words in different languages.
e , w ,w
The intuition is that, words and entities in

various languages share some common semantic
meanings1, but there are also ways in which they
differ. On one hand, we utilize their shared seman-
tics to align similar words and entities with simi-
lar embedding vectors, no matter they are in the
same language or not. On the other hand, cross-
lingual embeddings will benefit from different lan-
guages due to the complementary knowledge. For
instance, textual ambiguity in one language may
disappear in another language, e.g., the two mean-

1Some cross-lingual pioneering work observe that word
embeddings trained separately on monolingual corpora ex-
hibit isomorphic structure across languages (Mikolov et al.,
2013; Zhang et al., 2017).

1

000

001

002

003

004

005

006

007

008

009

010

011

012

013

014

015

016

017

018

019

020

021

022

023

024

025

026

027

028

029

030

031

032

033

034

035

036

037

038

039

040

041

042

043

044

045

046

047

048

049

050

051

052

053

054

055

056

057

058

059

060

061

062

063

064

065

066

067

068

069

070

071

072

073

074

075

076

077

078

079

080

081

082

083

084

085

086

087

088

089

090

091

092

093

094

095

096

097

098

099

ACL 2018 Submission ***. Confidential Review Copy. DO NOT DISTRIBUTE.

Joint Representation Learning of Cross-lingual Words and Entities via
Attentive Distant Supervision

Anonymous ACL submission

Abstract

Jointly learning word and entity represen-
tations benefits many NLP tasks, while has
not been well explored in cross-lingual set-
tings. In this paper, we propose a novel
method that integrates cross-lingual word
and entity representation learning to enable
joint inference among knowledge base and
text across languages, capturing mutually
complementary knowledge. Instead of re-
liance on parallel data, we automatically
generate cross-lingual training data via dis-
tant supervision over multi-lingual knowl-
edge bases. We also propose two types
of knowledge attention and cross-lingual
attention to select the most informative
words and filter out noise, which will fur-
ther improve the performance. In exper-
iments, separate tasks of word translation
and entity relatedness demonstrate the ef-
fectiveness of our method with an average
gain of 20% and 3% over baselines, re-
spectively. Using entity linking as a case
study, the results on benchmark dataset
verify the quality of our embeddings.

1 Introduction

Multi-lingual knowledge bases (KB), storing mil-
lions of entities and their facts in various lan-
guages, provide rich structured knowledge for un-
derstanding natural language beyond texts. Mean-
while, abundant text corpus contains large amount
of potential knowledge complementary to existing
KBs. Therefore, researchers leverage both types
of resources to improve various natural language
processing (NLP) related tasks, such as relation ex-
traction (Weston et al., 2013; Lin et al., 2017), and
entity linking (Tsai and Roth, 2016; Yamada et al.,
2016; Cao et al., 2017; Ji et al., 2016).

Most existing work jointly models KB and text
corpus to enhance each other by learning word and
entity representations in a unified vector space. For
example, (Wang et al., 2014; Yamada et al., 2016;
Cao et al., 2017) utilize the coherence informa-
tion to align similar words and entities with sim-
ilar embedding vectors. Another approach in (Han
et al., 2016; Toutanova et al., 2015; Wu et al.,
2016) learns to represent entities based on their
textual descriptions together with the structured re-
lations. However, these methods only focus on
mono-lingual settings, and few researches have
been done in cross-lingual scenarios.
In this paper, we propose to learn cross-lingual

word and entity representations in the same seman-
tic space, to enable joint inference among KB and
text across languages without any additional trans-
lation mechanism, which is usually expensive and
may introduce inevitable errors. Our embeddings
are helpful to break down language gaps in many
tasks, such as cross-lingual entity linking, in which
the major challenge lies in measuring the similar-
ity between entities and corresponding mentioned
words in different languages.
e , w ,w
The intuition is that, words and entities in

various languages share some common semantic
meanings1, but there are also ways in which they
differ. On one hand, we utilize their shared seman-
tics to align similar words and entities with simi-
lar embedding vectors, no matter they are in the
same language or not. On the other hand, cross-
lingual embeddings will benefit from different lan-
guages due to the complementary knowledge. For
instance, textual ambiguity in one language may
disappear in another language, e.g., the two mean-

1Some cross-lingual pioneering work observe that word
embeddings trained separately on monolingual corpora ex-
hibit isomorphic structure across languages (Mikolov et al.,
2013; Zhang et al., 2017).

eNBAeAllstar

Mono-lingual Representation Learning

Zh

1

000

001

002

003

004

005

006

007

008

009

010

011

012

013

014

015

016

017

018

019

020

021

022

023

024

025

026

027

028

029

030

031

032

033

034

035

036

037

038

039

040

041

042

043

044

045

046

047

048

049

050

051

052

053

054

055

056

057

058

059

060

061

062

063

064

065

066

067

068

069

070

071

072

073

074

075

076

077

078

079

080

081

082

083

084

085

086

087

088

089

090

091

092

093

094

095

096

097

098

099

ACL 2018 Submission ***. Confidential Review Copy. DO NOT DISTRIBUTE.

Joint Representation Learning of Cross-lingual Words and Entities via
Attentive Distant Supervision

Anonymous ACL submission

Abstract

Jointly learning word and entity represen-
tations benefits many NLP tasks, while has
not been well explored in cross-lingual set-
tings. In this paper, we propose a novel
method that integrates cross-lingual word
and entity representation learning to enable
joint inference among knowledge base and
text across languages, capturing mutually
complementary knowledge. Instead of re-
liance on parallel data, we automatically
generate cross-lingual training data via dis-
tant supervision over multi-lingual knowl-
edge bases. We also propose two types
of knowledge attention and cross-lingual
attention to select the most informative
words and filter out noise, which will fur-
ther improve the performance. In exper-
iments, separate tasks of word translation
and entity relatedness demonstrate the ef-
fectiveness of our method with an average
gain of 20% and 3% over baselines, re-
spectively. Using entity linking as a case
study, the results on benchmark dataset
verify the quality of our embeddings.

1 Introduction

Multi-lingual knowledge bases (KB), storing mil-
lions of entities and their facts in various lan-
guages, provide rich structured knowledge for un-
derstanding natural language beyond texts. Mean-
while, abundant text corpus contains large amount
of potential knowledge complementary to existing
KBs. Therefore, researchers leverage both types
of resources to improve various natural language
processing (NLP) related tasks, such as relation ex-
traction (Weston et al., 2013; Lin et al., 2017), and
entity linking (Tsai and Roth, 2016; Yamada et al.,
2016; Cao et al., 2017; Ji et al., 2016).

Most existing work jointly models KB and text
corpus to enhance each other by learning word and
entity representations in a unified vector space. For
example, (Wang et al., 2014; Yamada et al., 2016;
Cao et al., 2017) utilize the coherence informa-
tion to align similar words and entities with sim-
ilar embedding vectors. Another approach in (Han
et al., 2016; Toutanova et al., 2015; Wu et al.,
2016) learns to represent entities based on their
textual descriptions together with the structured re-
lations. However, these methods only focus on
mono-lingual settings, and few researches have
been done in cross-lingual scenarios.
In this paper, we propose to learn cross-lingual

word and entity representations in the same seman-
tic space, to enable joint inference among KB and
text across languages without any additional trans-
lation mechanism, which is usually expensive and
may introduce inevitable errors. Our embeddings
are helpful to break down language gaps in many
tasks, such as cross-lingual entity linking, in which
the major challenge lies in measuring the similar-
ity between entities and corresponding mentioned
words in different languages.
e , w ,w
The intuition is that, words and entities in

various languages share some common semantic
meanings1, but there are also ways in which they
differ. On one hand, we utilize their shared seman-
tics to align similar words and entities with simi-
lar embedding vectors, no matter they are in the
same language or not. On the other hand, cross-
lingual embeddings will benefit from different lan-
guages due to the complementary knowledge. For
instance, textual ambiguity in one language may
disappear in another language, e.g., the two mean-

1Some cross-lingual pioneering work observe that word
embeddings trained separately on monolingual corpora ex-
hibit isomorphic structure across languages (Mikolov et al.,
2013; Zhang et al., 2017).

1

000

001

002

003

004

005

006

007

008

009

010

011

012

013

014

015

016

017

018

019

020

021

022

023

024

025

026

027

028

029

030

031

032

033

034

035

036

037

038

039

040

041

042

043

044

045

046

047

048

049

050

051

052

053

054

055

056

057

058

059

060

061

062

063

064

065

066

067

068

069

070

071

072

073

074

075

076

077

078

079

080

081

082

083

084

085

086

087

088

089

090

091

092

093

094

095

096

097

098

099

ACL 2018 Submission ***. Confidential Review Copy. DO NOT DISTRIBUTE.

Joint Representation Learning of Cross-lingual Words and Entities via
Attentive Distant Supervision

Anonymous ACL submission

Abstract

Jointly learning word and entity represen-
tations benefits many NLP tasks, while has
not been well explored in cross-lingual set-
tings. In this paper, we propose a novel
method that integrates cross-lingual word
and entity representation learning to enable
joint inference among knowledge base and
text across languages, capturing mutually
complementary knowledge. Instead of re-
liance on parallel data, we automatically
generate cross-lingual training data via dis-
tant supervision over multi-lingual knowl-
edge bases. We also propose two types
of knowledge attention and cross-lingual
attention to select the most informative
words and filter out noise, which will fur-
ther improve the performance. In exper-
iments, separate tasks of word translation
and entity relatedness demonstrate the ef-
fectiveness of our method with an average
gain of 20% and 3% over baselines, re-
spectively. Using entity linking as a case
study, the results on benchmark dataset
verify the quality of our embeddings.

1 Introduction

Multi-lingual knowledge bases (KB), storing mil-
lions of entities and their facts in various lan-
guages, provide rich structured knowledge for un-
derstanding natural language beyond texts. Mean-
while, abundant text corpus contains large amount
of potential knowledge complementary to existing
KBs. Therefore, researchers leverage both types
of resources to improve various natural language
processing (NLP) related tasks, such as relation ex-
traction (Weston et al., 2013; Lin et al., 2017), and
entity linking (Tsai and Roth, 2016; Yamada et al.,
2016; Cao et al., 2017; Ji et al., 2016).

Most existing work jointly models KB and text
corpus to enhance each other by learning word and
entity representations in a unified vector space. For
example, (Wang et al., 2014; Yamada et al., 2016;
Cao et al., 2017) utilize the coherence informa-
tion to align similar words and entities with sim-
ilar embedding vectors. Another approach in (Han
et al., 2016; Toutanova et al., 2015; Wu et al.,
2016) learns to represent entities based on their
textual descriptions together with the structured re-
lations. However, these methods only focus on
mono-lingual settings, and few researches have
been done in cross-lingual scenarios.
In this paper, we propose to learn cross-lingual

word and entity representations in the same seman-
tic space, to enable joint inference among KB and
text across languages without any additional trans-
lation mechanism, which is usually expensive and
may introduce inevitable errors. Our embeddings
are helpful to break down language gaps in many
tasks, such as cross-lingual entity linking, in which
the major challenge lies in measuring the similar-
ity between entities and corresponding mentioned
words in different languages.
eNBA , w , w ,w
The intuition is that, words and entities in

various languages share some common semantic
meanings1, but there are also ways in which they
differ. On one hand, we utilize their shared seman-
tics to align similar words and entities with simi-
lar embedding vectors, no matter they are in the
same language or not. On the other hand, cross-
lingual embeddings will benefit from different lan-
guages due to the complementary knowledge. For
instance, textual ambiguity in one language may
disappear in another language, e.g., the two mean-

1Some cross-lingual pioneering work observe that word
embeddings trained separately on monolingual corpora ex-
hibit isomorphic structure across languages (Mikolov et al.,
2013; Zhang et al., 2017).

1

000

001

002

003

004

005

006

007

008

009

010

011

012

013

014

015

016

017

018

019

020

021

022

023

024

025

026

027

028

029

030

031

032

033

034

035

036

037

038

039

040

041

042

043

044

045

046

047

048

049

050

051

052

053

054

055

056

057

058

059

060

061

062

063

064

065

066

067

068

069

070

071

072

073

074

075

076

077

078

079

080

081

082

083

084

085

086

087

088

089

090

091

092

093

094

095

096

097

098

099

ACL 2018 Submission ***. Confidential Review Copy. DO NOT DISTRIBUTE.

Joint Representation Learning of Cross-lingual Words and Entities via
Attentive Distant Supervision

Anonymous ACL submission

Abstract

Jointly learning word and entity represen-
tations benefits many NLP tasks, while has
not been well explored in cross-lingual set-
tings. In this paper, we propose a novel
method that integrates cross-lingual word
and entity representation learning to enable
joint inference among knowledge base and
text across languages, capturing mutually
complementary knowledge. Instead of re-
liance on parallel data, we automatically
generate cross-lingual training data via dis-
tant supervision over multi-lingual knowl-
edge bases. We also propose two types
of knowledge attention and cross-lingual
attention to select the most informative
words and filter out noise, which will fur-
ther improve the performance. In exper-
iments, separate tasks of word translation
and entity relatedness demonstrate the ef-
fectiveness of our method with an average
gain of 20% and 3% over baselines, re-
spectively. Using entity linking as a case
study, the results on benchmark dataset
verify the quality of our embeddings.

1 Introduction

Multi-lingual knowledge bases (KB), storing mil-
lions of entities and their facts in various lan-
guages, provide rich structured knowledge for un-
derstanding natural language beyond texts. Mean-
while, abundant text corpus contains large amount
of potential knowledge complementary to existing
KBs. Therefore, researchers leverage both types
of resources to improve various natural language
processing (NLP) related tasks, such as relation ex-
traction (Weston et al., 2013; Lin et al., 2017), and
entity linking (Tsai and Roth, 2016; Yamada et al.,
2016; Cao et al., 2017; Ji et al., 2016).

Most existing work jointly models KB and text
corpus to enhance each other by learning word and
entity representations in a unified vector space. For
example, (Wang et al., 2014; Yamada et al., 2016;
Cao et al., 2017) utilize the coherence informa-
tion to align similar words and entities with sim-
ilar embedding vectors. Another approach in (Han
et al., 2016; Toutanova et al., 2015; Wu et al.,
2016) learns to represent entities based on their
textual descriptions together with the structured re-
lations. However, these methods only focus on
mono-lingual settings, and few researches have
been done in cross-lingual scenarios.
In this paper, we propose to learn cross-lingual

word and entity representations in the same seman-
tic space, to enable joint inference among KB and
text across languages without any additional trans-
lation mechanism, which is usually expensive and
may introduce inevitable errors. Our embeddings
are helpful to break down language gaps in many
tasks, such as cross-lingual entity linking, in which
the major challenge lies in measuring the similar-
ity between entities and corresponding mentioned
words in different languages.
eNBA , w , w ,w
The intuition is that, words and entities in

various languages share some common semantic
meanings1, but there are also ways in which they
differ. On one hand, we utilize their shared seman-
tics to align similar words and entities with simi-
lar embedding vectors, no matter they are in the
same language or not. On the other hand, cross-
lingual embeddings will benefit from different lan-
guages due to the complementary knowledge. For
instance, textual ambiguity in one language may
disappear in another language, e.g., the two mean-

1Some cross-lingual pioneering work observe that word
embeddings trained separately on monolingual corpora ex-
hibit isomorphic structure across languages (Mikolov et al.,
2013; Zhang et al., 2017).

1

000

001

002

003

004

005

006

007

008

009

010

011

012

013

014

015

016

017

018

019

020

021

022

023

024

025

026

027

028

029

030

031

032

033

034

035

036

037

038

039

040

041

042

043

044

045

046

047

048

049

050

051

052

053

054

055

056

057

058

059

060

061

062

063

064

065

066

067

068

069

070

071

072

073

074

075

076

077

078

079

080

081

082

083

084

085

086

087

088

089

090

091

092

093

094

095

096

097

098

099

ACL 2018 Submission ***. Confidential Review Copy. DO NOT DISTRIBUTE.

Joint Representation Learning of Cross-lingual Words and Entities via
Attentive Distant Supervision

Anonymous ACL submission

Abstract

Jointly learning word and entity represen-
tations benefits many NLP tasks, while has
not been well explored in cross-lingual set-
tings. In this paper, we propose a novel
method that integrates cross-lingual word
and entity representation learning to enable
joint inference among knowledge base and
text across languages, capturing mutually
complementary knowledge. Instead of re-
liance on parallel data, we automatically
generate cross-lingual training data via dis-
tant supervision over multi-lingual knowl-
edge bases. We also propose two types
of knowledge attention and cross-lingual
attention to select the most informative
words and filter out noise, which will fur-
ther improve the performance. In exper-
iments, separate tasks of word translation
and entity relatedness demonstrate the ef-
fectiveness of our method with an average
gain of 20% and 3% over baselines, re-
spectively. Using entity linking as a case
study, the results on benchmark dataset
verify the quality of our embeddings.

1 Introduction

Multi-lingual knowledge bases (KB), storing mil-
lions of entities and their facts in various lan-
guages, provide rich structured knowledge for un-
derstanding natural language beyond texts. Mean-
while, abundant text corpus contains large amount
of potential knowledge complementary to existing
KBs. Therefore, researchers leverage both types
of resources to improve various natural language
processing (NLP) related tasks, such as relation ex-
traction (Weston et al., 2013; Lin et al., 2017), and
entity linking (Tsai and Roth, 2016; Yamada et al.,
2016; Cao et al., 2017; Ji et al., 2016).

Most existing work jointly models KB and text
corpus to enhance each other by learning word and
entity representations in a unified vector space. For
example, (Wang et al., 2014; Yamada et al., 2016;
Cao et al., 2017) utilize the coherence informa-
tion to align similar words and entities with sim-
ilar embedding vectors. Another approach in (Han
et al., 2016; Toutanova et al., 2015; Wu et al.,
2016) learns to represent entities based on their
textual descriptions together with the structured re-
lations. However, these methods only focus on
mono-lingual settings, and few researches have
been done in cross-lingual scenarios.
In this paper, we propose to learn cross-lingual

word and entity representations in the same seman-
tic space, to enable joint inference among KB and
text across languages without any additional trans-
lation mechanism, which is usually expensive and
may introduce inevitable errors. Our embeddings
are helpful to break down language gaps in many
tasks, such as cross-lingual entity linking, in which
the major challenge lies in measuring the similar-
ity between entities and corresponding mentioned
words in different languages.
eNBA , w , w ,w
The intuition is that, words and entities in

various languages share some common semantic
meanings1, but there are also ways in which they
differ. On one hand, we utilize their shared seman-
tics to align similar words and entities with simi-
lar embedding vectors, no matter they are in the
same language or not. On the other hand, cross-
lingual embeddings will benefit from different lan-
guages due to the complementary knowledge. For
instance, textual ambiguity in one language may
disappear in another language, e.g., the two mean-

1Some cross-lingual pioneering work observe that word
embeddings trained separately on monolingual corpora ex-
hibit isomorphic structure across languages (Mikolov et al.,
2013; Zhang et al., 2017).

1

000

001

002

003

004

005

006

007

008

009

010

011

012

013

014

015

016

017

018

019

020

021

022

023

024

025

026

027

028

029

030

031

032

033

034

035

036

037

038

039

040

041

042

043

044

045

046

047

048

049

050

051

052

053

054

055

056

057

058

059

060

061

062

063

064

065

066

067

068

069

070

071

072

073

074

075

076

077

078

079

080

081

082

083

084

085

086

087

088

089

090

091

092

093

094

095

096

097

098

099

ACL 2018 Submission ***. Confidential Review Copy. DO NOT DISTRIBUTE.

Joint Representation Learning of Cross-lingual Words and Entities via
Attentive Distant Supervision

Anonymous ACL submission

Abstract

Jointly learning word and entity represen-
tations benefits many NLP tasks, while has
not been well explored in cross-lingual set-
tings. In this paper, we propose a novel
method that integrates cross-lingual word
and entity representation learning to enable
joint inference among knowledge base and
text across languages, capturing mutually
complementary knowledge. Instead of re-
liance on parallel data, we automatically
generate cross-lingual training data via dis-
tant supervision over multi-lingual knowl-
edge bases. We also propose two types
of knowledge attention and cross-lingual
attention to select the most informative
words and filter out noise, which will fur-
ther improve the performance. In exper-
iments, separate tasks of word translation
and entity relatedness demonstrate the ef-
fectiveness of our method with an average
gain of 20% and 3% over baselines, re-
spectively. Using entity linking as a case
study, the results on benchmark dataset
verify the quality of our embeddings.

1 Introduction

Multi-lingual knowledge bases (KB), storing mil-
lions of entities and their facts in various lan-
guages, provide rich structured knowledge for un-
derstanding natural language beyond texts. Mean-
while, abundant text corpus contains large amount
of potential knowledge complementary to existing
KBs. Therefore, researchers leverage both types
of resources to improve various natural language
processing (NLP) related tasks, such as relation ex-
traction (Weston et al., 2013; Lin et al., 2017), and
entity linking (Tsai and Roth, 2016; Yamada et al.,
2016; Cao et al., 2017; Ji et al., 2016).

Most existing work jointly models KB and text
corpus to enhance each other by learning word and
entity representations in a unified vector space. For
example, (Wang et al., 2014; Yamada et al., 2016;
Cao et al., 2017) utilize the coherence informa-
tion to align similar words and entities with sim-
ilar embedding vectors. Another approach in (Han
et al., 2016; Toutanova et al., 2015; Wu et al.,
2016) learns to represent entities based on their
textual descriptions together with the structured re-
lations. However, these methods only focus on
mono-lingual settings, and few researches have
been done in cross-lingual scenarios.
In this paper, we propose to learn cross-lingual

word and entity representations in the same seman-
tic space, to enable joint inference among KB and
text across languages without any additional trans-
lation mechanism, which is usually expensive and
may introduce inevitable errors. Our embeddings
are helpful to break down language gaps in many
tasks, such as cross-lingual entity linking, in which
the major challenge lies in measuring the similar-
ity between entities and corresponding mentioned
words in different languages.
eNBA , w , w ,w
The intuition is that, words and entities in

various languages share some common semantic
meanings1, but there are also ways in which they
differ. On one hand, we utilize their shared seman-
tics to align similar words and entities with simi-
lar embedding vectors, no matter they are in the
same language or not. On the other hand, cross-
lingual embeddings will benefit from different lan-
guages due to the complementary knowledge. For
instance, textual ambiguity in one language may
disappear in another language, e.g., the two mean-

1Some cross-lingual pioneering work observe that word
embeddings trained separately on monolingual corpora ex-
hibit isomorphic structure across languages (Mikolov et al.,
2013; Zhang et al., 2017).

1

000

001

002

003

004

005

006

007

008

009

010

011

012

013

014

015

016

017

018

019

020

021

022

023

024

025

026

027

028

029

030

031

032

033

034

035

036

037

038

039

040

041

042

043

044

045

046

047

048

049

050

051

052

053

054

055

056

057

058

059

060

061

062

063

064

065

066

067

068

069

070

071

072

073

074

075

076

077

078

079

080

081

082

083

084

085

086

087

088

089

090

091

092

093

094

095

096

097

098

099

ACL 2018 Submission ***. Confidential Review Copy. DO NOT DISTRIBUTE.

Joint Representation Learning of Cross-lingual Words and Entities via
Attentive Distant Supervision

Anonymous ACL submission

Abstract

Jointly learning word and entity represen-
tations benefits many NLP tasks, while has
not been well explored in cross-lingual set-
tings. In this paper, we propose a novel
method that integrates cross-lingual word
and entity representation learning to enable
joint inference among knowledge base and
text across languages, capturing mutually
complementary knowledge. Instead of re-
liance on parallel data, we automatically
generate cross-lingual training data via dis-
tant supervision over multi-lingual knowl-
edge bases. We also propose two types
of knowledge attention and cross-lingual
attention to select the most informative
words and filter out noise, which will fur-
ther improve the performance. In exper-
iments, separate tasks of word translation
and entity relatedness demonstrate the ef-
fectiveness of our method with an average
gain of 20% and 3% over baselines, re-
spectively. Using entity linking as a case
study, the results on benchmark dataset
verify the quality of our embeddings.

1 Introduction

Multi-lingual knowledge bases (KB), storing mil-
lions of entities and their facts in various lan-
guages, provide rich structured knowledge for un-
derstanding natural language beyond texts. Mean-
while, abundant text corpus contains large amount
of potential knowledge complementary to existing
KBs. Therefore, researchers leverage both types
of resources to improve various natural language
processing (NLP) related tasks, such as relation ex-
traction (Weston et al., 2013; Lin et al., 2017), and
entity linking (Tsai and Roth, 2016; Yamada et al.,
2016; Cao et al., 2017; Ji et al., 2016).

Most existing work jointly models KB and text
corpus to enhance each other by learning word and
entity representations in a unified vector space. For
example, (Wang et al., 2014; Yamada et al., 2016;
Cao et al., 2017) utilize the coherence informa-
tion to align similar words and entities with sim-
ilar embedding vectors. Another approach in (Han
et al., 2016; Toutanova et al., 2015; Wu et al.,
2016) learns to represent entities based on their
textual descriptions together with the structured re-
lations. However, these methods only focus on
mono-lingual settings, and few researches have
been done in cross-lingual scenarios.
In this paper, we propose to learn cross-lingual

word and entity representations in the same seman-
tic space, to enable joint inference among KB and
text across languages without any additional trans-
lation mechanism, which is usually expensive and
may introduce inevitable errors. Our embeddings
are helpful to break down language gaps in many
tasks, such as cross-lingual entity linking, in which
the major challenge lies in measuring the similar-
ity between entities and corresponding mentioned
words in different languages.
eNBA
The intuition is that, words and entities in

various languages share some common semantic
meanings1, but there are also ways in which they
differ. On one hand, we utilize their shared seman-
tics to align similar words and entities with simi-
lar embedding vectors, no matter they are in the
same language or not. On the other hand, cross-
lingual embeddings will benefit from different lan-
guages due to the complementary knowledge. For
instance, textual ambiguity in one language may
disappear in another language, e.g., the two mean-

1Some cross-lingual pioneering work observe that word
embeddings trained separately on monolingual corpora ex-
hibit isomorphic structure across languages (Mikolov et al.,
2013; Zhang et al., 2017).

En

eLawrence

wwas wan

eLawrence

eNBAeAllstarwan wbasketball

wAmerican

Figure 1: The framework of our method. The inputs and outputs of each step are listed in the right
side, and in the left side there are three main components of joint representation learning. Red texts
with brackets are anchors, dashed lines between entities denote relations, and solid lines are cross-lingual
links.

mon neighbors have similar embeddings, no mat-
ter in which language, e.g., English entity Foust
and Chinese entity 福斯特 (Foust) are embed-
ded close in semantic space due to the common
neighborsNBA,All-star andNBA选秀 (draft), etc.
(3) Cross-lingual Sentence Regularizer aims to
learn mutually translated words with similar em-
beddings by pushing their cross-lingual contexts
(i.e. comparable sentences) together. For exam-
ple, English word basketball and the translated
Chinese word 篮球 frequently co-occur in com-
parable sentences, and are close in the semantic
space.
All word/entity embeddings are trained jointly

under a unified optimization objective. Next, we
will introduce how to generate bi-lingual EN and
comparable sentences as well as the three compo-
nents for joint representation learning in turn.

3.3 Cross-lingual Supervision Data
Generation

This section introduces how to extract more cross-
lingual clues from multi-lingual KB in the form of
bi-lingual EN and comparable sentences.

Bi-lingual Entity Network Construction

Conventional knowledge representation meth-
ods normally regard cross-lingual links as a spe-
cial equivalence type of relation between two en-
tities (Zhu et al., 2017). However, we argue that
this may mislead to an inconsistent training ob-

jective since a cross-lingual link actually contains
multiple relations. For example (Figure 1), there
will be no direct relation betweenChinese entity福
斯特 (Foust) and English entity Piston by merely
adding the equivalence relation between Foust and
福斯特, which is in contradiction with the fact that
Foust belongs to Piston, no matter in which lan-
guage.
Therefore, we build bi-lingual entity network by

making cross-lingual linked entities that inherit all
relations from each other. Concretely, we enhance
mono-EN by adding edges from all neighbors of
entity eei to ezj if < eei , ezj >∈ Re−z (layer 2).
e

Comparable Sentences Generation

We utilize distant supervision to generate com-
parable sentences from Wikipedia articles. As
shown in Figure 1, from the page articles of cross-
lingual linked entities eeKobe and e

z
Kobe, we extract

those sentences including another cross-lingual
linked entities eeJoe and ezJoe as comparable sen-
tences Se−z = {< sek, szk >}.
The intuition is that we consider each sentence

in a Wikipedia article has a pseudo mention of
the page entity (talking something about the en-
tity) (Yamada et al., 2017). Thus, if a sentence also
mentions another entity, it implicitly expresses
their relation. Therefore, we make a similar as-
sumption as in relation extraction: If two enti-
ties participate in a relation, and both of them1

000

001

002

003

004

005

006

007

008

009

010

011

012

013

014

015

016

017

018

019

020

021

022

023

024

025

026

027

028

029

030

031

032

033

034

035

036

037

038

039

040

041

042

043

044

045

046

047

048

049

050

051

052

053

054

055

056

057

058

059

060

061

062

063

064

065

066

067

068

069

070

071

072

073

074

075

076

077

078

079

080

081

082

083

084

085

086

087

088

089

090

091

092

093

094

095

096

097

098

099

ACL 2018 Submission ***. Confidential Review Copy. DO NOT DISTRIBUTE.

Joint Representation Learning of Cross-lingual Words and Entities via
Attentive Distant Supervision

Anonymous ACL submission

Abstract

Jointly learning word and entity represen-
tations benefits many NLP tasks, while has
not been well explored in cross-lingual set-
tings. In this paper, we propose a novel
method that integrates cross-lingual word
and entity representation learning to enable
joint inference among knowledge base and
text across languages, capturing mutually
complementary knowledge. Instead of re-
liance on parallel data, we automatically
generate cross-lingual training data via dis-
tant supervision over multi-lingual knowl-
edge bases. We also propose two types
of knowledge attention and cross-lingual
attention to select the most informative
words and filter out noise, which will fur-
ther improve the performance. In exper-
iments, separate tasks of word translation
and entity relatedness demonstrate the ef-
fectiveness of our method with an average
gain of 20% and 3% over baselines, re-
spectively. Using entity linking as a case
study, the results on benchmark dataset
verify the quality of our embeddings.

1 Introduction

Multi-lingual knowledge bases (KB), storing mil-
lions of entities and their facts in various lan-
guages, provide rich structured knowledge for un-
derstanding natural language beyond texts. Mean-
while, abundant text corpus contains large amount
of potential knowledge complementary to existing
KBs. Therefore, researchers leverage both types
of resources to improve various natural language
processing (NLP) related tasks, such as relation ex-
traction (Weston et al., 2013; Lin et al., 2017), and
entity linking (Tsai and Roth, 2016; Yamada et al.,
2016; Cao et al., 2017; Ji et al., 2016).

Most existing work jointly models KB and text
corpus to enhance each other by learning word and
entity representations in a unified vector space. For
example, (Wang et al., 2014; Yamada et al., 2016;
Cao et al., 2017) utilize the coherence informa-
tion to align similar words and entities with sim-
ilar embedding vectors. Another approach in (Han
et al., 2016; Toutanova et al., 2015; Wu et al.,
2016) learns to represent entities based on their
textual descriptions together with the structured re-
lations. However, these methods only focus on
mono-lingual settings, and few researches have
been done in cross-lingual scenarios.
In this paper, we propose to learn cross-lingual

word and entity representations in the same seman-
tic space, to enable joint inference among KB and
text across languages without any additional trans-
lation mechanism, which is usually expensive and
may introduce inevitable errors. Our embeddings
are helpful to break down language gaps in many
tasks, such as cross-lingual entity linking, in which
the major challenge lies in measuring the similar-
ity between entities and corresponding mentioned
words in different languages.
e , w ,w
The intuition is that, words and entities in

various languages share some common semantic
meanings1, but there are also ways in which they
differ. On one hand, we utilize their shared seman-
tics to align similar words and entities with simi-
lar embedding vectors, no matter they are in the
same language or not. On the other hand, cross-
lingual embeddings will benefit from different lan-
guages due to the complementary knowledge. For
instance, textual ambiguity in one language may
disappear in another language, e.g., the two mean-

1Some cross-lingual pioneering work observe that word
embeddings trained separately on monolingual corpora ex-
hibit isomorphic structure across languages (Mikolov et al.,
2013; Zhang et al., 2017).

att
att

att

… …
wbasketballwplayer

… …

1

000

001

002

003

004

005

006

007

008

009

010

011

012

013

014

015

016

017

018

019

020

021

022

023

024

025

026

027

028

029

030

031

032

033

034

035

036

037

038

039

040

041

042

043

044

045

046

047

048

049

050

051

052

053

054

055

056

057

058

059

060

061

062

063

064

065

066

067

068

069

070

071

072

073

074

075

076

077

078

079

080

081

082

083

084

085

086

087

088

089

090

091

092

093

094

095

096

097

098

099

ACL 2018 Submission ***. Confidential Review Copy. DO NOT DISTRIBUTE.

Joint Representation Learning of Cross-lingual Words and Entities via
Attentive Distant Supervision

Anonymous ACL submission

Abstract

Jointly learning word and entity represen-
tations benefits many NLP tasks, while has
not been well explored in cross-lingual set-
tings. In this paper, we propose a novel
method that integrates cross-lingual word
and entity representation learning to enable
joint inference among knowledge base and
text across languages, capturing mutually
complementary knowledge. Instead of re-
liance on parallel data, we automatically
generate cross-lingual training data via dis-
tant supervision over multi-lingual knowl-
edge bases. We also propose two types
of knowledge attention and cross-lingual
attention to select the most informative
words and filter out noise, which will fur-
ther improve the performance. In exper-
iments, separate tasks of word translation
and entity relatedness demonstrate the ef-
fectiveness of our method with an average
gain of 20% and 3% over baselines, re-
spectively. Using entity linking as a case
study, the results on benchmark dataset
verify the quality of our embeddings.

1 Introduction

Multi-lingual knowledge bases (KB), storing mil-
lions of entities and their facts in various lan-
guages, provide rich structured knowledge for un-
derstanding natural language beyond texts. Mean-
while, abundant text corpus contains large amount
of potential knowledge complementary to existing
KBs. Therefore, researchers leverage both types
of resources to improve various natural language
processing (NLP) related tasks, such as relation ex-
traction (Weston et al., 2013; Lin et al., 2017), and
entity linking (Tsai and Roth, 2016; Yamada et al.,
2016; Cao et al., 2017; Ji et al., 2016).

Most existing work jointly models KB and text
corpus to enhance each other by learning word and
entity representations in a unified vector space. For
example, (Wang et al., 2014; Yamada et al., 2016;
Cao et al., 2017) utilize the coherence informa-
tion to align similar words and entities with sim-
ilar embedding vectors. Another approach in (Han
et al., 2016; Toutanova et al., 2015; Wu et al.,
2016) learns to represent entities based on their
textual descriptions together with the structured re-
lations. However, these methods only focus on
mono-lingual settings, and few researches have
been done in cross-lingual scenarios.
In this paper, we propose to learn cross-lingual

word and entity representations in the same seman-
tic space, to enable joint inference among KB and
text across languages without any additional trans-
lation mechanism, which is usually expensive and
may introduce inevitable errors. Our embeddings
are helpful to break down language gaps in many
tasks, such as cross-lingual entity linking, in which
the major challenge lies in measuring the similar-
ity between entities and corresponding mentioned
words in different languages.
e , w ,w
The intuition is that, words and entities in

various languages share some common semantic
meanings1, but there are also ways in which they
differ. On one hand, we utilize their shared seman-
tics to align similar words and entities with simi-
lar embedding vectors, no matter they are in the
same language or not. On the other hand, cross-
lingual embeddings will benefit from different lan-
guages due to the complementary knowledge. For
instance, textual ambiguity in one language may
disappear in another language, e.g., the two mean-

1Some cross-lingual pioneering work observe that word
embeddings trained separately on monolingual corpora ex-
hibit isomorphic structure across languages (Mikolov et al.,
2013; Zhang et al., 2017).

1

000

001

002

003

004

005

006

007

008

009

010

011

012

013

014

015

016

017

018

019

020

021

022

023

024

025

026

027

028

029

030

031

032

033

034

035

036

037

038

039

040

041

042

043

044

045

046

047

048

049

050

051

052

053

054

055

056

057

058

059

060

061

062

063

064

065

066

067

068

069

070

071

072

073

074

075

076

077

078

079

080

081

082

083

084

085

086

087

088

089

090

091

092

093

094

095

096

097

098

099

ACL 2018 Submission ***. Confidential Review Copy. DO NOT DISTRIBUTE.

Joint Representation Learning of Cross-lingual Words and Entities via
Attentive Distant Supervision

Anonymous ACL submission

Abstract

Jointly learning word and entity represen-
tations benefits many NLP tasks, while has
not been well explored in cross-lingual set-
tings. In this paper, we propose a novel
method that integrates cross-lingual word
and entity representation learning to enable
joint inference among knowledge base and
text across languages, capturing mutually
complementary knowledge. Instead of re-
liance on parallel data, we automatically
generate cross-lingual training data via dis-
tant supervision over multi-lingual knowl-
edge bases. We also propose two types
of knowledge attention and cross-lingual
attention to select the most informative
words and filter out noise, which will fur-
ther improve the performance. In exper-
iments, separate tasks of word translation
and entity relatedness demonstrate the ef-
fectiveness of our method with an average
gain of 20% and 3% over baselines, re-
spectively. Using entity linking as a case
study, the results on benchmark dataset
verify the quality of our embeddings.

1 Introduction

Multi-lingual knowledge bases (KB), storing mil-
lions of entities and their facts in various lan-
guages, provide rich structured knowledge for un-
derstanding natural language beyond texts. Mean-
while, abundant text corpus contains large amount
of potential knowledge complementary to existing
KBs. Therefore, researchers leverage both types
of resources to improve various natural language
processing (NLP) related tasks, such as relation ex-
traction (Weston et al., 2013; Lin et al., 2017), and
entity linking (Tsai and Roth, 2016; Yamada et al.,
2016; Cao et al., 2017; Ji et al., 2016).

Most existing work jointly models KB and text
corpus to enhance each other by learning word and
entity representations in a unified vector space. For
example, (Wang et al., 2014; Yamada et al., 2016;
Cao et al., 2017) utilize the coherence informa-
tion to align similar words and entities with sim-
ilar embedding vectors. Another approach in (Han
et al., 2016; Toutanova et al., 2015; Wu et al.,
2016) learns to represent entities based on their
textual descriptions together with the structured re-
lations. However, these methods only focus on
mono-lingual settings, and few researches have
been done in cross-lingual scenarios.
In this paper, we propose to learn cross-lingual

word and entity representations in the same seman-
tic space, to enable joint inference among KB and
text across languages without any additional trans-
lation mechanism, which is usually expensive and
may introduce inevitable errors. Our embeddings
are helpful to break down language gaps in many
tasks, such as cross-lingual entity linking, in which
the major challenge lies in measuring the similar-
ity between entities and corresponding mentioned
words in different languages.
e , w ,w
The intuition is that, words and entities in

various languages share some common semantic
meanings1, but there are also ways in which they
differ. On one hand, we utilize their shared seman-
tics to align similar words and entities with simi-
lar embedding vectors, no matter they are in the
same language or not. On the other hand, cross-
lingual embeddings will benefit from different lan-
guages due to the complementary knowledge. For
instance, textual ambiguity in one language may
disappear in another language, e.g., the two mean-

1Some cross-lingual pioneering work observe that word
embeddings trained separately on monolingual corpora ex-
hibit isomorphic structure across languages (Mikolov et al.,
2013; Zhang et al., 2017).

4

300

301

302

303

304

305

306

307

308

309

310

311

312

313

314

315

316

317

318

319

320

321

322

323

324

325

326

327

328

329

330

331

332

333

334

335

336

337

338

339

340

341

342

343

344

345

346

347

348

349

350

351

352

353

354

355

356

357

358

359

360

361

362

363

364

365

366

367

368

369

370

371

372

373

374

375

376

377

378

379

380

381

382

383

384

385

386

387

388

389

390

391

392

393

394

395

396

397

398

399

ACL 2018 Submission ***. Confidential Review Copy. DO NOT DISTRIBUTE.

NBA

Lawrence Foust

All-star

[[Lawrence Michael Foust]] was
an American basketball player

who spent 12 seasons in [[NBA]]

NBA (zh)

·
[[ · ]] [[NBA]]

1950 [[NBA ]] 1 5

English KB Chinese KB

独立日 美国独立日

Sen Szh een ezh

L =
∑

xi ∈D

∑

xo ∈C(xi )
P(xo |xi)

L =
∑

(ei ,ej )∈Ec
| |C(ei) − C(e j)| |2

Ec

L =
∑

ei ∈E
P(N (ei)|ei) +

∑

(ei ,ej )∈Ec
P(N (e j)|ei)

N (ei)

L =
∑

(ei ,mi )∈A
P(ei |mi ,C(ei)) +

∑

(ei ,ej )∈Ec
P(e j |mi ,C(ei))

独立日 美国独立日

Sen Szh een ezh

L =
∑

xi ∈D

∑

xo ∈C(xi )
P(xo |xi)

L =
∑

(ei ,ej )∈Ec
| |C(ei) − C(e j)| |2

Ec

L =
∑

ei ∈E
P(N (ei)|ei) +

∑

(ei ,ej )∈Ec
P(N (e j)|ei)

N (ei)

L =
∑

(ei ,mi )∈A
P(ei |mi ,C(ei)) +

∑

(ei ,ej )∈Ec
P(e j |mi ,C(ei))

was

Lawrence
NBA

NBA (zh)

basketball

Semantic Space

Representation
Learning

Cross-lingual
Sentence Regularizer

Cross-lingual
Entity Regularizer

[[ · ]] [[NBA]]

All-star

[[Lawrence Michael Foust]] was an American basketball 
player who spent 12 seasons in [[NBA]]

·

NBA
NBA (zh)

Comparable Sentences

Bi-lingual EN

att
att

att

… …

Ls = | |Sen − Szh | |2

L =
∑

xi ∈D

∑

xo ∈C(xi )
P(xo |xi )

L =
∑

(ei ,ej )∈Ec
| |C(ei ) − C(ej )| |2

Ec

L =
∑

ei ∈E
P(N (ei )|ei ) +

∑

(ei ,ej )∈Ec
P(N (ej )|ei )

N (ei )

L =
∑

(ei ,mi )∈A
P(ei |mi ,C(ei )) +

∑

(ei ,ej )∈Ec
P(ej |mi ,C(ei ))

=

500

501

502

503

504

505

506

507

508

509

510

511

512

513

514

515

516

517

518

519

520

521

522

523

524

525

526

527

528

529

530

531

532

533

534

535

536

537

538

539

540

541

542

543

544

545

546

547

548

549

550

551

552

553

554

555

556

557

558

559

560

561

562

563

564

565

566

567

568

569

570

571

572

573

574

575

576

577

578

579

580

581

582

583

584

585

586

587

588

589

590

591

592

593

594

595

596

597

598

599

ACL 2018 Submission ***. Confidential Review Copy. DO NOT DISTRIBUTE.

skm ≈
wki +

∑
C(wki)

Ls =
∑

<skm,slm>∈Sk,l

|| − ||2

ekm
ekm

k

αnkm ∝ sim( ,
∑

wki∈snkm

)

αnkm
n skm

ekm αnkm

αki,lj ∝
wki∈skm,wlj∈slm

sim(wki, wlj)

=
∑

snkm∈skm

αnkm
∑

wki∈snkm

αki,lj

L = Lc + γLs

γ

400

401

402

403

404

405

406

407

408

409

410

411

412

413

414

415

416

417

418

419

420

421

422

423

424

425

426

427

428

429

430

431

432

433

434

435

436

437

438

439

440

441

442

443

444

445

446

447

448

449

450

451

452

453

454

455

456

457

458

459

460

461

462

463

464

465

466

467

468

469

470

471

472

473

474

475

476

477

478

479

480

481

482

483

484

485

486

487

488

489

490

491

492

493

494

495

496

497

498

499

ACL 2018 Submission ***. Confidential Review Copy. DO NOT DISTRIBUTE.

eki elj < eki, elj >∈ Rc

een,Kobe
ezh,Kobe

een,Joe ezh,Joe

< een,Kobe, ezh,Kobe >
< een,Joe, ezh,Joe >

D A
KN

Lc =
∑

xi∈{D,A,KN}

P (C(xi)|xi)

xi
C(xi)

C(xi)
xi

xi ∈ D
xi ∈ KN

wj xi
< wj , ei >∈ A

een,Joe wen,player
wen,NBA

een,LosAngelesLakers
een,Joe een,Kobe ezh,Kobe

wki
Cl(wki)

Ls =
∑

<skm,slm∈Sk,l

∑

wki∈skm

||wki −
∑

Cl(wki)||2

< skm, slm >∈ Sk,l m
k l

Cross-lingual
 supervision

Data Generation

and was an 8-time [[All-star]] … [[ ]] …

NBA

Lawrence Foust

NBA

player

American
NBA

All-star
eLawrence

wbasketballwplayer
… …

1

000

001

002

003

004

005

006

007

008

009

010

011

012

013

014

015

016

017

018

019

020

021

022

023

024

025

026

027

028

029

030

031

032

033

034

035

036

037

038

039

040

041

042

043

044

045

046

047

048

049

050

051

052

053

054

055

056

057

058

059

060

061

062

063

064

065

066

067

068

069

070

071

072

073

074

075

076

077

078

079

080

081

082

083

084

085

086

087

088

089

090

091

092

093

094

095

096

097

098

099

ACL 2018 Submission ***. Confidential Review Copy. DO NOT DISTRIBUTE.

Joint Representation Learning of Cross-lingual Words and Entities via
Attentive Distant Supervision

Anonymous ACL submission

Abstract

Jointly learning word and entity represen-
tations benefits many NLP tasks, while has
not been well explored in cross-lingual set-
tings. In this paper, we propose a novel
method that integrates cross-lingual word
and entity representation learning to enable
joint inference among knowledge base and
text across languages, capturing mutually
complementary knowledge. Instead of re-
liance on parallel data, we automatically
generate cross-lingual training data via dis-
tant supervision over multi-lingual knowl-
edge bases. We also propose two types
of knowledge attention and cross-lingual
attention to select the most informative
words and filter out noise, which will fur-
ther improve the performance. In exper-
iments, separate tasks of word translation
and entity relatedness demonstrate the ef-
fectiveness of our method with an average
gain of 20% and 3% over baselines, re-
spectively. Using entity linking as a case
study, the results on benchmark dataset
verify the quality of our embeddings.

1 Introduction

Multi-lingual knowledge bases (KB), storing mil-
lions of entities and their facts in various lan-
guages, provide rich structured knowledge for un-
derstanding natural language beyond texts. Mean-
while, abundant text corpus contains large amount
of potential knowledge complementary to existing
KBs. Therefore, researchers leverage both types
of resources to improve various natural language
processing (NLP) related tasks, such as relation ex-
traction (Weston et al., 2013; Lin et al., 2017), and
entity linking (Tsai and Roth, 2016; Yamada et al.,
2016; Cao et al., 2017; Ji et al., 2016).

Most existing work jointly models KB and text
corpus to enhance each other by learning word and
entity representations in a unified vector space. For
example, (Wang et al., 2014; Yamada et al., 2016;
Cao et al., 2017) utilize the coherence informa-
tion to align similar words and entities with sim-
ilar embedding vectors. Another approach in (Han
et al., 2016; Toutanova et al., 2015; Wu et al.,
2016) learns to represent entities based on their
textual descriptions together with the structured re-
lations. However, these methods only focus on
mono-lingual settings, and few researches have
been done in cross-lingual scenarios.
In this paper, we propose to learn cross-lingual

word and entity representations in the same seman-
tic space, to enable joint inference among KB and
text across languages without any additional trans-
lation mechanism, which is usually expensive and
may introduce inevitable errors. Our embeddings
are helpful to break down language gaps in many
tasks, such as cross-lingual entity linking, in which
the major challenge lies in measuring the similar-
ity between entities and corresponding mentioned
words in different languages.
e , w ,w
The intuition is that, words and entities in

various languages share some common semantic
meanings1, but there are also ways in which they
differ. On one hand, we utilize their shared seman-
tics to align similar words and entities with simi-
lar embedding vectors, no matter they are in the
same language or not. On the other hand, cross-
lingual embeddings will benefit from different lan-
guages due to the complementary knowledge. For
instance, textual ambiguity in one language may
disappear in another language, e.g., the two mean-

1Some cross-lingual pioneering work observe that word
embeddings trained separately on monolingual corpora ex-
hibit isomorphic structure across languages (Mikolov et al.,
2013; Zhang et al., 2017).

1

000

001

002

003

004

005

006

007

008

009

010

011

012

013

014

015

016

017

018

019

020

021

022

023

024

025

026

027

028

029

030

031

032

033

034

035

036

037

038

039

040

041

042

043

044

045

046

047

048

049

050

051

052

053

054

055

056

057

058

059

060

061

062

063

064

065

066

067

068

069

070

071

072

073

074

075

076

077

078

079

080

081

082

083

084

085

086

087

088

089

090

091

092

093

094

095

096

097

098

099

ACL 2018 Submission ***. Confidential Review Copy. DO NOT DISTRIBUTE.

Joint Representation Learning of Cross-lingual Words and Entities via
Attentive Distant Supervision

Anonymous ACL submission

Abstract

Jointly learning word and entity represen-
tations benefits many NLP tasks, while has
not been well explored in cross-lingual set-
tings. In this paper, we propose a novel
method that integrates cross-lingual word
and entity representation learning to enable
joint inference among knowledge base and
text across languages, capturing mutually
complementary knowledge. Instead of re-
liance on parallel data, we automatically
generate cross-lingual training data via dis-
tant supervision over multi-lingual knowl-
edge bases. We also propose two types
of knowledge attention and cross-lingual
attention to select the most informative
words and filter out noise, which will fur-
ther improve the performance. In exper-
iments, separate tasks of word translation
and entity relatedness demonstrate the ef-
fectiveness of our method with an average
gain of 20% and 3% over baselines, re-
spectively. Using entity linking as a case
study, the results on benchmark dataset
verify the quality of our embeddings.

1 Introduction

Multi-lingual knowledge bases (KB), storing mil-
lions of entities and their facts in various lan-
guages, provide rich structured knowledge for un-
derstanding natural language beyond texts. Mean-
while, abundant text corpus contains large amount
of potential knowledge complementary to existing
KBs. Therefore, researchers leverage both types
of resources to improve various natural language
processing (NLP) related tasks, such as relation ex-
traction (Weston et al., 2013; Lin et al., 2017), and
entity linking (Tsai and Roth, 2016; Yamada et al.,
2016; Cao et al., 2017; Ji et al., 2016).

Most existing work jointly models KB and text
corpus to enhance each other by learning word and
entity representations in a unified vector space. For
example, (Wang et al., 2014; Yamada et al., 2016;
Cao et al., 2017) utilize the coherence informa-
tion to align similar words and entities with sim-
ilar embedding vectors. Another approach in (Han
et al., 2016; Toutanova et al., 2015; Wu et al.,
2016) learns to represent entities based on their
textual descriptions together with the structured re-
lations. However, these methods only focus on
mono-lingual settings, and few researches have
been done in cross-lingual scenarios.
In this paper, we propose to learn cross-lingual

word and entity representations in the same seman-
tic space, to enable joint inference among KB and
text across languages without any additional trans-
lation mechanism, which is usually expensive and
may introduce inevitable errors. Our embeddings
are helpful to break down language gaps in many
tasks, such as cross-lingual entity linking, in which
the major challenge lies in measuring the similar-
ity between entities and corresponding mentioned
words in different languages.
e , w ,w
The intuition is that, words and entities in

various languages share some common semantic
meanings1, but there are also ways in which they
differ. On one hand, we utilize their shared seman-
tics to align similar words and entities with simi-
lar embedding vectors, no matter they are in the
same language or not. On the other hand, cross-
lingual embeddings will benefit from different lan-
guages due to the complementary knowledge. For
instance, textual ambiguity in one language may
disappear in another language, e.g., the two mean-

1Some cross-lingual pioneering work observe that word
embeddings trained separately on monolingual corpora ex-
hibit isomorphic structure across languages (Mikolov et al.,
2013; Zhang et al., 2017).

1

000

001

002

003

004

005

006

007

008

009

010

011

012

013

014

015

016

017

018

019

020

021

022

023

024

025

026

027

028

029

030

031

032

033

034

035

036

037

038

039

040

041

042

043

044

045

046

047

048

049

050

051

052

053

054

055

056

057

058

059

060

061

062

063

064

065

066

067

068

069

070

071

072

073

074

075

076

077

078

079

080

081

082

083

084

085

086

087

088

089

090

091

092

093

094

095

096

097

098

099

ACL 2018 Submission ***. Confidential Review Copy. DO NOT DISTRIBUTE.

Joint Representation Learning of Cross-lingual Words and Entities via
Attentive Distant Supervision

Anonymous ACL submission

Abstract

Jointly learning word and entity represen-
tations benefits many NLP tasks, while has
not been well explored in cross-lingual set-
tings. In this paper, we propose a novel
method that integrates cross-lingual word
and entity representation learning to enable
joint inference among knowledge base and
text across languages, capturing mutually
complementary knowledge. Instead of re-
liance on parallel data, we automatically
generate cross-lingual training data via dis-
tant supervision over multi-lingual knowl-
edge bases. We also propose two types
of knowledge attention and cross-lingual
attention to select the most informative
words and filter out noise, which will fur-
ther improve the performance. In exper-
iments, separate tasks of word translation
and entity relatedness demonstrate the ef-
fectiveness of our method with an average
gain of 20% and 3% over baselines, re-
spectively. Using entity linking as a case
study, the results on benchmark dataset
verify the quality of our embeddings.

1 Introduction

Multi-lingual knowledge bases (KB), storing mil-
lions of entities and their facts in various lan-
guages, provide rich structured knowledge for un-
derstanding natural language beyond texts. Mean-
while, abundant text corpus contains large amount
of potential knowledge complementary to existing
KBs. Therefore, researchers leverage both types
of resources to improve various natural language
processing (NLP) related tasks, such as relation ex-
traction (Weston et al., 2013; Lin et al., 2017), and
entity linking (Tsai and Roth, 2016; Yamada et al.,
2016; Cao et al., 2017; Ji et al., 2016).

Most existing work jointly models KB and text
corpus to enhance each other by learning word and
entity representations in a unified vector space. For
example, (Wang et al., 2014; Yamada et al., 2016;
Cao et al., 2017) utilize the coherence informa-
tion to align similar words and entities with sim-
ilar embedding vectors. Another approach in (Han
et al., 2016; Toutanova et al., 2015; Wu et al.,
2016) learns to represent entities based on their
textual descriptions together with the structured re-
lations. However, these methods only focus on
mono-lingual settings, and few researches have
been done in cross-lingual scenarios.
In this paper, we propose to learn cross-lingual

word and entity representations in the same seman-
tic space, to enable joint inference among KB and
text across languages without any additional trans-
lation mechanism, which is usually expensive and
may introduce inevitable errors. Our embeddings
are helpful to break down language gaps in many
tasks, such as cross-lingual entity linking, in which
the major challenge lies in measuring the similar-
ity between entities and corresponding mentioned
words in different languages.
e , w ,w
The intuition is that, words and entities in

various languages share some common semantic
meanings1, but there are also ways in which they
differ. On one hand, we utilize their shared seman-
tics to align similar words and entities with simi-
lar embedding vectors, no matter they are in the
same language or not. On the other hand, cross-
lingual embeddings will benefit from different lan-
guages due to the complementary knowledge. For
instance, textual ambiguity in one language may
disappear in another language, e.g., the two mean-

1Some cross-lingual pioneering work observe that word
embeddings trained separately on monolingual corpora ex-
hibit isomorphic structure across languages (Mikolov et al.,
2013; Zhang et al., 2017).

1

000

001

002

003

004

005

006

007

008

009

010

011

012

013

014

015

016

017

018

019

020

021

022

023

024

025

026

027

028

029

030

031

032

033

034

035

036

037

038

039

040

041

042

043

044

045

046

047

048

049

050

051

052

053

054

055

056

057

058

059

060

061

062

063

064

065

066

067

068

069

070

071

072

073

074

075

076

077

078

079

080

081

082

083

084

085

086

087

088

089

090

091

092

093

094

095

096

097

098

099

ACL 2018 Submission ***. Confidential Review Copy. DO NOT DISTRIBUTE.

Joint Representation Learning of Cross-lingual Words and Entities via
Attentive Distant Supervision

Anonymous ACL submission

Abstract

Jointly learning word and entity represen-
tations benefits many NLP tasks, while has
not been well explored in cross-lingual set-
tings. In this paper, we propose a novel
method that integrates cross-lingual word
and entity representation learning to enable
joint inference among knowledge base and
text across languages, capturing mutually
complementary knowledge. Instead of re-
liance on parallel data, we automatically
generate cross-lingual training data via dis-
tant supervision over multi-lingual knowl-
edge bases. We also propose two types
of knowledge attention and cross-lingual
attention to select the most informative
words and filter out noise, which will fur-
ther improve the performance. In exper-
iments, separate tasks of word translation
and entity relatedness demonstrate the ef-
fectiveness of our method with an average
gain of 20% and 3% over baselines, re-
spectively. Using entity linking as a case
study, the results on benchmark dataset
verify the quality of our embeddings.

1 Introduction

Multi-lingual knowledge bases (KB), storing mil-
lions of entities and their facts in various lan-
guages, provide rich structured knowledge for un-
derstanding natural language beyond texts. Mean-
while, abundant text corpus contains large amount
of potential knowledge complementary to existing
KBs. Therefore, researchers leverage both types
of resources to improve various natural language
processing (NLP) related tasks, such as relation ex-
traction (Weston et al., 2013; Lin et al., 2017), and
entity linking (Tsai and Roth, 2016; Yamada et al.,
2016; Cao et al., 2017; Ji et al., 2016).

Most existing work jointly models KB and text
corpus to enhance each other by learning word and
entity representations in a unified vector space. For
example, (Wang et al., 2014; Yamada et al., 2016;
Cao et al., 2017) utilize the coherence informa-
tion to align similar words and entities with sim-
ilar embedding vectors. Another approach in (Han
et al., 2016; Toutanova et al., 2015; Wu et al.,
2016) learns to represent entities based on their
textual descriptions together with the structured re-
lations. However, these methods only focus on
mono-lingual settings, and few researches have
been done in cross-lingual scenarios.
In this paper, we propose to learn cross-lingual

word and entity representations in the same seman-
tic space, to enable joint inference among KB and
text across languages without any additional trans-
lation mechanism, which is usually expensive and
may introduce inevitable errors. Our embeddings
are helpful to break down language gaps in many
tasks, such as cross-lingual entity linking, in which
the major challenge lies in measuring the similar-
ity between entities and corresponding mentioned
words in different languages.
e , w ,w
The intuition is that, words and entities in

various languages share some common semantic
meanings1, but there are also ways in which they
differ. On one hand, we utilize their shared seman-
tics to align similar words and entities with simi-
lar embedding vectors, no matter they are in the
same language or not. On the other hand, cross-
lingual embeddings will benefit from different lan-
guages due to the complementary knowledge. For
instance, textual ambiguity in one language may
disappear in another language, e.g., the two mean-

1Some cross-lingual pioneering work observe that word
embeddings trained separately on monolingual corpora ex-
hibit isomorphic structure across languages (Mikolov et al.,
2013; Zhang et al., 2017).

eNBAeAllstar

Mono-lingual Representation Learning

Zh

1

000

001

002

003

004

005

006

007

008

009

010

011

012

013

014

015

016

017

018

019

020

021

022

023

024

025

026

027

028

029

030

031

032

033

034

035

036

037

038

039

040

041

042

043

044

045

046

047

048

049

050

051

052

053

054

055

056

057

058

059

060

061

062

063

064

065

066

067

068

069

070

071

072

073

074

075

076

077

078

079

080

081

082

083

084

085

086

087

088

089

090

091

092

093

094

095

096

097

098

099

ACL 2018 Submission ***. Confidential Review Copy. DO NOT DISTRIBUTE.

Joint Representation Learning of Cross-lingual Words and Entities via
Attentive Distant Supervision

Anonymous ACL submission

Abstract

Jointly learning word and entity represen-
tations benefits many NLP tasks, while has
not been well explored in cross-lingual set-
tings. In this paper, we propose a novel
method that integrates cross-lingual word
and entity representation learning to enable
joint inference among knowledge base and
text across languages, capturing mutually
complementary knowledge. Instead of re-
liance on parallel data, we automatically
generate cross-lingual training data via dis-
tant supervision over multi-lingual knowl-
edge bases. We also propose two types
of knowledge attention and cross-lingual
attention to select the most informative
words and filter out noise, which will fur-
ther improve the performance. In exper-
iments, separate tasks of word translation
and entity relatedness demonstrate the ef-
fectiveness of our method with an average
gain of 20% and 3% over baselines, re-
spectively. Using entity linking as a case
study, the results on benchmark dataset
verify the quality of our embeddings.

1 Introduction

Multi-lingual knowledge bases (KB), storing mil-
lions of entities and their facts in various lan-
guages, provide rich structured knowledge for un-
derstanding natural language beyond texts. Mean-
while, abundant text corpus contains large amount
of potential knowledge complementary to existing
KBs. Therefore, researchers leverage both types
of resources to improve various natural language
processing (NLP) related tasks, such as relation ex-
traction (Weston et al., 2013; Lin et al., 2017), and
entity linking (Tsai and Roth, 2016; Yamada et al.,
2016; Cao et al., 2017; Ji et al., 2016).

Most existing work jointly models KB and text
corpus to enhance each other by learning word and
entity representations in a unified vector space. For
example, (Wang et al., 2014; Yamada et al., 2016;
Cao et al., 2017) utilize the coherence informa-
tion to align similar words and entities with sim-
ilar embedding vectors. Another approach in (Han
et al., 2016; Toutanova et al., 2015; Wu et al.,
2016) learns to represent entities based on their
textual descriptions together with the structured re-
lations. However, these methods only focus on
mono-lingual settings, and few researches have
been done in cross-lingual scenarios.
In this paper, we propose to learn cross-lingual

word and entity representations in the same seman-
tic space, to enable joint inference among KB and
text across languages without any additional trans-
lation mechanism, which is usually expensive and
may introduce inevitable errors. Our embeddings
are helpful to break down language gaps in many
tasks, such as cross-lingual entity linking, in which
the major challenge lies in measuring the similar-
ity between entities and corresponding mentioned
words in different languages.
e , w ,w
The intuition is that, words and entities in

various languages share some common semantic
meanings1, but there are also ways in which they
differ. On one hand, we utilize their shared seman-
tics to align similar words and entities with simi-
lar embedding vectors, no matter they are in the
same language or not. On the other hand, cross-
lingual embeddings will benefit from different lan-
guages due to the complementary knowledge. For
instance, textual ambiguity in one language may
disappear in another language, e.g., the two mean-

1Some cross-lingual pioneering work observe that word
embeddings trained separately on monolingual corpora ex-
hibit isomorphic structure across languages (Mikolov et al.,
2013; Zhang et al., 2017).

1

000

001

002

003

004

005

006

007

008

009

010

011

012

013

014

015

016

017

018

019

020

021

022

023

024

025

026

027

028

029

030

031

032

033

034

035

036

037

038

039

040

041

042

043

044

045

046

047

048

049

050

051

052

053

054

055

056

057

058

059

060

061

062

063

064

065

066

067

068

069

070

071

072

073

074

075

076

077

078

079

080

081

082

083

084

085

086

087

088

089

090

091

092

093

094

095

096

097

098

099

ACL 2018 Submission ***. Confidential Review Copy. DO NOT DISTRIBUTE.

Joint Representation Learning of Cross-lingual Words and Entities via
Attentive Distant Supervision

Anonymous ACL submission

Abstract

Jointly learning word and entity represen-
tations benefits many NLP tasks, while has
not been well explored in cross-lingual set-
tings. In this paper, we propose a novel
method that integrates cross-lingual word
and entity representation learning to enable
joint inference among knowledge base and
text across languages, capturing mutually
complementary knowledge. Instead of re-
liance on parallel data, we automatically
generate cross-lingual training data via dis-
tant supervision over multi-lingual knowl-
edge bases. We also propose two types
of knowledge attention and cross-lingual
attention to select the most informative
words and filter out noise, which will fur-
ther improve the performance. In exper-
iments, separate tasks of word translation
and entity relatedness demonstrate the ef-
fectiveness of our method with an average
gain of 20% and 3% over baselines, re-
spectively. Using entity linking as a case
study, the results on benchmark dataset
verify the quality of our embeddings.

1 Introduction

Multi-lingual knowledge bases (KB), storing mil-
lions of entities and their facts in various lan-
guages, provide rich structured knowledge for un-
derstanding natural language beyond texts. Mean-
while, abundant text corpus contains large amount
of potential knowledge complementary to existing
KBs. Therefore, researchers leverage both types
of resources to improve various natural language
processing (NLP) related tasks, such as relation ex-
traction (Weston et al., 2013; Lin et al., 2017), and
entity linking (Tsai and Roth, 2016; Yamada et al.,
2016; Cao et al., 2017; Ji et al., 2016).

Most existing work jointly models KB and text
corpus to enhance each other by learning word and
entity representations in a unified vector space. For
example, (Wang et al., 2014; Yamada et al., 2016;
Cao et al., 2017) utilize the coherence informa-
tion to align similar words and entities with sim-
ilar embedding vectors. Another approach in (Han
et al., 2016; Toutanova et al., 2015; Wu et al.,
2016) learns to represent entities based on their
textual descriptions together with the structured re-
lations. However, these methods only focus on
mono-lingual settings, and few researches have
been done in cross-lingual scenarios.
In this paper, we propose to learn cross-lingual

word and entity representations in the same seman-
tic space, to enable joint inference among KB and
text across languages without any additional trans-
lation mechanism, which is usually expensive and
may introduce inevitable errors. Our embeddings
are helpful to break down language gaps in many
tasks, such as cross-lingual entity linking, in which
the major challenge lies in measuring the similar-
ity between entities and corresponding mentioned
words in different languages.
eNBA , w , w ,w
The intuition is that, words and entities in

various languages share some common semantic
meanings1, but there are also ways in which they
differ. On one hand, we utilize their shared seman-
tics to align similar words and entities with simi-
lar embedding vectors, no matter they are in the
same language or not. On the other hand, cross-
lingual embeddings will benefit from different lan-
guages due to the complementary knowledge. For
instance, textual ambiguity in one language may
disappear in another language, e.g., the two mean-

1Some cross-lingual pioneering work observe that word
embeddings trained separately on monolingual corpora ex-
hibit isomorphic structure across languages (Mikolov et al.,
2013; Zhang et al., 2017).

1

000

001

002

003

004

005

006

007

008

009

010

011

012

013

014

015

016

017

018

019

020

021

022

023

024

025

026

027

028

029

030

031

032

033

034

035

036

037

038

039

040

041

042

043

044

045

046

047

048

049

050

051

052

053

054

055

056

057

058

059

060

061

062

063

064

065

066

067

068

069

070

071

072

073

074

075

076

077

078

079

080

081

082

083

084

085

086

087

088

089

090

091

092

093

094

095

096

097

098

099

ACL 2018 Submission ***. Confidential Review Copy. DO NOT DISTRIBUTE.

Joint Representation Learning of Cross-lingual Words and Entities via
Attentive Distant Supervision

Anonymous ACL submission

Abstract

Jointly learning word and entity represen-
tations benefits many NLP tasks, while has
not been well explored in cross-lingual set-
tings. In this paper, we propose a novel
method that integrates cross-lingual word
and entity representation learning to enable
joint inference among knowledge base and
text across languages, capturing mutually
complementary knowledge. Instead of re-
liance on parallel data, we automatically
generate cross-lingual training data via dis-
tant supervision over multi-lingual knowl-
edge bases. We also propose two types
of knowledge attention and cross-lingual
attention to select the most informative
words and filter out noise, which will fur-
ther improve the performance. In exper-
iments, separate tasks of word translation
and entity relatedness demonstrate the ef-
fectiveness of our method with an average
gain of 20% and 3% over baselines, re-
spectively. Using entity linking as a case
study, the results on benchmark dataset
verify the quality of our embeddings.

1 Introduction

Multi-lingual knowledge bases (KB), storing mil-
lions of entities and their facts in various lan-
guages, provide rich structured knowledge for un-
derstanding natural language beyond texts. Mean-
while, abundant text corpus contains large amount
of potential knowledge complementary to existing
KBs. Therefore, researchers leverage both types
of resources to improve various natural language
processing (NLP) related tasks, such as relation ex-
traction (Weston et al., 2013; Lin et al., 2017), and
entity linking (Tsai and Roth, 2016; Yamada et al.,
2016; Cao et al., 2017; Ji et al., 2016).

Most existing work jointly models KB and text
corpus to enhance each other by learning word and
entity representations in a unified vector space. For
example, (Wang et al., 2014; Yamada et al., 2016;
Cao et al., 2017) utilize the coherence informa-
tion to align similar words and entities with sim-
ilar embedding vectors. Another approach in (Han
et al., 2016; Toutanova et al., 2015; Wu et al.,
2016) learns to represent entities based on their
textual descriptions together with the structured re-
lations. However, these methods only focus on
mono-lingual settings, and few researches have
been done in cross-lingual scenarios.
In this paper, we propose to learn cross-lingual

word and entity representations in the same seman-
tic space, to enable joint inference among KB and
text across languages without any additional trans-
lation mechanism, which is usually expensive and
may introduce inevitable errors. Our embeddings
are helpful to break down language gaps in many
tasks, such as cross-lingual entity linking, in which
the major challenge lies in measuring the similar-
ity between entities and corresponding mentioned
words in different languages.
eNBA , w , w ,w
The intuition is that, words and entities in

various languages share some common semantic
meanings1, but there are also ways in which they
differ. On one hand, we utilize their shared seman-
tics to align similar words and entities with simi-
lar embedding vectors, no matter they are in the
same language or not. On the other hand, cross-
lingual embeddings will benefit from different lan-
guages due to the complementary knowledge. For
instance, textual ambiguity in one language may
disappear in another language, e.g., the two mean-

1Some cross-lingual pioneering work observe that word
embeddings trained separately on monolingual corpora ex-
hibit isomorphic structure across languages (Mikolov et al.,
2013; Zhang et al., 2017).

1

000

001

002

003

004

005

006

007

008

009

010

011

012

013

014

015

016

017

018

019

020

021

022

023

024

025

026

027

028

029

030

031

032

033

034

035

036

037

038

039

040

041

042

043

044

045

046

047

048

049

050

051

052

053

054

055

056

057

058

059

060

061

062

063

064

065

066

067

068

069

070

071

072

073

074

075

076

077

078

079

080

081

082

083

084

085

086

087

088

089

090

091

092

093

094

095

096

097

098

099

ACL 2018 Submission ***. Confidential Review Copy. DO NOT DISTRIBUTE.

Joint Representation Learning of Cross-lingual Words and Entities via
Attentive Distant Supervision

Anonymous ACL submission

Abstract

Jointly learning word and entity represen-
tations benefits many NLP tasks, while has
not been well explored in cross-lingual set-
tings. In this paper, we propose a novel
method that integrates cross-lingual word
and entity representation learning to enable
joint inference among knowledge base and
text across languages, capturing mutually
complementary knowledge. Instead of re-
liance on parallel data, we automatically
generate cross-lingual training data via dis-
tant supervision over multi-lingual knowl-
edge bases. We also propose two types
of knowledge attention and cross-lingual
attention to select the most informative
words and filter out noise, which will fur-
ther improve the performance. In exper-
iments, separate tasks of word translation
and entity relatedness demonstrate the ef-
fectiveness of our method with an average
gain of 20% and 3% over baselines, re-
spectively. Using entity linking as a case
study, the results on benchmark dataset
verify the quality of our embeddings.

1 Introduction

Multi-lingual knowledge bases (KB), storing mil-
lions of entities and their facts in various lan-
guages, provide rich structured knowledge for un-
derstanding natural language beyond texts. Mean-
while, abundant text corpus contains large amount
of potential knowledge complementary to existing
KBs. Therefore, researchers leverage both types
of resources to improve various natural language
processing (NLP) related tasks, such as relation ex-
traction (Weston et al., 2013; Lin et al., 2017), and
entity linking (Tsai and Roth, 2016; Yamada et al.,
2016; Cao et al., 2017; Ji et al., 2016).

Most existing work jointly models KB and text
corpus to enhance each other by learning word and
entity representations in a unified vector space. For
example, (Wang et al., 2014; Yamada et al., 2016;
Cao et al., 2017) utilize the coherence informa-
tion to align similar words and entities with sim-
ilar embedding vectors. Another approach in (Han
et al., 2016; Toutanova et al., 2015; Wu et al.,
2016) learns to represent entities based on their
textual descriptions together with the structured re-
lations. However, these methods only focus on
mono-lingual settings, and few researches have
been done in cross-lingual scenarios.
In this paper, we propose to learn cross-lingual

word and entity representations in the same seman-
tic space, to enable joint inference among KB and
text across languages without any additional trans-
lation mechanism, which is usually expensive and
may introduce inevitable errors. Our embeddings
are helpful to break down language gaps in many
tasks, such as cross-lingual entity linking, in which
the major challenge lies in measuring the similar-
ity between entities and corresponding mentioned
words in different languages.
eNBA , w , w ,w
The intuition is that, words and entities in

various languages share some common semantic
meanings1, but there are also ways in which they
differ. On one hand, we utilize their shared seman-
tics to align similar words and entities with simi-
lar embedding vectors, no matter they are in the
same language or not. On the other hand, cross-
lingual embeddings will benefit from different lan-
guages due to the complementary knowledge. For
instance, textual ambiguity in one language may
disappear in another language, e.g., the two mean-

1Some cross-lingual pioneering work observe that word
embeddings trained separately on monolingual corpora ex-
hibit isomorphic structure across languages (Mikolov et al.,
2013; Zhang et al., 2017).

1

000

001

002

003

004

005

006

007

008

009

010

011

012

013

014

015

016

017

018

019

020

021

022

023

024

025

026

027

028

029

030

031

032

033

034

035

036

037

038

039

040

041

042

043

044

045

046

047

048

049

050

051

052

053

054

055

056

057

058

059

060

061

062

063

064

065

066

067

068

069

070

071

072

073

074

075

076

077

078

079

080

081

082

083

084

085

086

087

088

089

090

091

092

093

094

095

096

097

098

099

ACL 2018 Submission ***. Confidential Review Copy. DO NOT DISTRIBUTE.

Joint Representation Learning of Cross-lingual Words and Entities via
Attentive Distant Supervision

Anonymous ACL submission

Abstract

Jointly learning word and entity represen-
tations benefits many NLP tasks, while has
not been well explored in cross-lingual set-
tings. In this paper, we propose a novel
method that integrates cross-lingual word
and entity representation learning to enable
joint inference among knowledge base and
text across languages, capturing mutually
complementary knowledge. Instead of re-
liance on parallel data, we automatically
generate cross-lingual training data via dis-
tant supervision over multi-lingual knowl-
edge bases. We also propose two types
of knowledge attention and cross-lingual
attention to select the most informative
words and filter out noise, which will fur-
ther improve the performance. In exper-
iments, separate tasks of word translation
and entity relatedness demonstrate the ef-
fectiveness of our method with an average
gain of 20% and 3% over baselines, re-
spectively. Using entity linking as a case
study, the results on benchmark dataset
verify the quality of our embeddings.

1 Introduction

Multi-lingual knowledge bases (KB), storing mil-
lions of entities and their facts in various lan-
guages, provide rich structured knowledge for un-
derstanding natural language beyond texts. Mean-
while, abundant text corpus contains large amount
of potential knowledge complementary to existing
KBs. Therefore, researchers leverage both types
of resources to improve various natural language
processing (NLP) related tasks, such as relation ex-
traction (Weston et al., 2013; Lin et al., 2017), and
entity linking (Tsai and Roth, 2016; Yamada et al.,
2016; Cao et al., 2017; Ji et al., 2016).

Most existing work jointly models KB and text
corpus to enhance each other by learning word and
entity representations in a unified vector space. For
example, (Wang et al., 2014; Yamada et al., 2016;
Cao et al., 2017) utilize the coherence informa-
tion to align similar words and entities with sim-
ilar embedding vectors. Another approach in (Han
et al., 2016; Toutanova et al., 2015; Wu et al.,
2016) learns to represent entities based on their
textual descriptions together with the structured re-
lations. However, these methods only focus on
mono-lingual settings, and few researches have
been done in cross-lingual scenarios.
In this paper, we propose to learn cross-lingual

word and entity representations in the same seman-
tic space, to enable joint inference among KB and
text across languages without any additional trans-
lation mechanism, which is usually expensive and
may introduce inevitable errors. Our embeddings
are helpful to break down language gaps in many
tasks, such as cross-lingual entity linking, in which
the major challenge lies in measuring the similar-
ity between entities and corresponding mentioned
words in different languages.
eNBA , w , w ,w
The intuition is that, words and entities in

various languages share some common semantic
meanings1, but there are also ways in which they
differ. On one hand, we utilize their shared seman-
tics to align similar words and entities with simi-
lar embedding vectors, no matter they are in the
same language or not. On the other hand, cross-
lingual embeddings will benefit from different lan-
guages due to the complementary knowledge. For
instance, textual ambiguity in one language may
disappear in another language, e.g., the two mean-

1Some cross-lingual pioneering work observe that word
embeddings trained separately on monolingual corpora ex-
hibit isomorphic structure across languages (Mikolov et al.,
2013; Zhang et al., 2017).

1

000

001

002

003

004

005

006

007

008

009

010

011

012

013

014

015

016

017

018

019

020

021

022

023

024

025

026

027

028

029

030

031

032

033

034

035

036

037

038

039

040

041

042

043

044

045

046

047

048

049

050

051

052

053

054

055

056

057

058

059

060

061

062

063

064

065

066

067

068

069

070

071

072

073

074

075

076

077

078

079

080

081

082

083

084

085

086

087

088

089

090

091

092

093

094

095

096

097

098

099

ACL 2018 Submission ***. Confidential Review Copy. DO NOT DISTRIBUTE.

Joint Representation Learning of Cross-lingual Words and Entities via
Attentive Distant Supervision

Anonymous ACL submission

Abstract

Jointly learning word and entity represen-
tations benefits many NLP tasks, while has
not been well explored in cross-lingual set-
tings. In this paper, we propose a novel
method that integrates cross-lingual word
and entity representation learning to enable
joint inference among knowledge base and
text across languages, capturing mutually
complementary knowledge. Instead of re-
liance on parallel data, we automatically
generate cross-lingual training data via dis-
tant supervision over multi-lingual knowl-
edge bases. We also propose two types
of knowledge attention and cross-lingual
attention to select the most informative
words and filter out noise, which will fur-
ther improve the performance. In exper-
iments, separate tasks of word translation
and entity relatedness demonstrate the ef-
fectiveness of our method with an average
gain of 20% and 3% over baselines, re-
spectively. Using entity linking as a case
study, the results on benchmark dataset
verify the quality of our embeddings.

1 Introduction

Multi-lingual knowledge bases (KB), storing mil-
lions of entities and their facts in various lan-
guages, provide rich structured knowledge for un-
derstanding natural language beyond texts. Mean-
while, abundant text corpus contains large amount
of potential knowledge complementary to existing
KBs. Therefore, researchers leverage both types
of resources to improve various natural language
processing (NLP) related tasks, such as relation ex-
traction (Weston et al., 2013; Lin et al., 2017), and
entity linking (Tsai and Roth, 2016; Yamada et al.,
2016; Cao et al., 2017; Ji et al., 2016).

Most existing work jointly models KB and text
corpus to enhance each other by learning word and
entity representations in a unified vector space. For
example, (Wang et al., 2014; Yamada et al., 2016;
Cao et al., 2017) utilize the coherence informa-
tion to align similar words and entities with sim-
ilar embedding vectors. Another approach in (Han
et al., 2016; Toutanova et al., 2015; Wu et al.,
2016) learns to represent entities based on their
textual descriptions together with the structured re-
lations. However, these methods only focus on
mono-lingual settings, and few researches have
been done in cross-lingual scenarios.
In this paper, we propose to learn cross-lingual

word and entity representations in the same seman-
tic space, to enable joint inference among KB and
text across languages without any additional trans-
lation mechanism, which is usually expensive and
may introduce inevitable errors. Our embeddings
are helpful to break down language gaps in many
tasks, such as cross-lingual entity linking, in which
the major challenge lies in measuring the similar-
ity between entities and corresponding mentioned
words in different languages.
eNBA
The intuition is that, words and entities in

various languages share some common semantic
meanings1, but there are also ways in which they
differ. On one hand, we utilize their shared seman-
tics to align similar words and entities with simi-
lar embedding vectors, no matter they are in the
same language or not. On the other hand, cross-
lingual embeddings will benefit from different lan-
guages due to the complementary knowledge. For
instance, textual ambiguity in one language may
disappear in another language, e.g., the two mean-

1Some cross-lingual pioneering work observe that word
embeddings trained separately on monolingual corpora ex-
hibit isomorphic structure across languages (Mikolov et al.,
2013; Zhang et al., 2017).

En

eLawrence

wwas wan

eLawrence

eNBAeAllstarwan wbasketball

wAmerican

Figure 1: The framework of our method. The inputs and outputs of each step are listed in the right
side, and in the left side there are three main components of joint representation learning. Red texts
with brackets are anchors, dashed lines between entities denote relations, and solid lines are cross-lingual
links.

mon neighbors have similar embeddings, no mat-
ter in which language, e.g., English entity Foust
and Chinese entity 福斯特 (Foust) are embed-
ded close in semantic space due to the common
neighborsNBA,All-star andNBA选秀 (draft), etc.
(3) Cross-lingual Sentence Regularizer aims to
learn mutually translated words with similar em-
beddings by pushing their cross-lingual contexts
(i.e. comparable sentences) together. For exam-
ple, English word basketball and the translated
Chinese word 篮球 frequently co-occur in com-
parable sentences, and are close in the semantic
space.
All word/entity embeddings are trained jointly

under a unified optimization objective. Next, we
will introduce how to generate bi-lingual EN and
comparable sentences as well as the three compo-
nents for joint representation learning in turn.

3.3 Cross-lingual Supervision Data
Generation

This section introduces how to extract more cross-
lingual clues from multi-lingual KB in the form of
bi-lingual EN and comparable sentences.

Bi-lingual Entity Network Construction

Conventional knowledge representation meth-
ods normally regard cross-lingual links as a spe-
cial equivalence type of relation between two en-
tities (Zhu et al., 2017). However, we argue that
this may mislead to an inconsistent training ob-

jective since a cross-lingual link actually contains
multiple relations. For example (Figure 1), there
will be no direct relation betweenChinese entity福
斯特 (Foust) and English entity Piston by merely
adding the equivalence relation between Foust and
福斯特, which is in contradiction with the fact that
Foust belongs to Piston, no matter in which lan-
guage.
Therefore, we build bi-lingual entity network by

making cross-lingual linked entities that inherit all
relations from each other. Concretely, we enhance
mono-EN by adding edges from all neighbors of
entity eei to ezj if < eei , ezj >∈ Re−z (layer 2).
e

Comparable Sentences Generation

We utilize distant supervision to generate com-
parable sentences from Wikipedia articles. As
shown in Figure 1, from the page articles of cross-
lingual linked entities eeKobe and e

z
Kobe, we extract

those sentences including another cross-lingual
linked entities eeJoe and ezJoe as comparable sen-
tences Se−z = {< sek, szk >}.
The intuition is that we consider each sentence

in a Wikipedia article has a pseudo mention of
the page entity (talking something about the en-
tity) (Yamada et al., 2017). Thus, if a sentence also
mentions another entity, it implicitly expresses
their relation. Therefore, we make a similar as-
sumption as in relation extraction: If two enti-
ties participate in a relation, and both of them

eJordan

Ls = ||sen � szh||2

图 5.4 跨语⾔词和实体联合表⽰学习模型

Lm =
∑

y∈{en,zh}

∑
xi ∈D̂y

log P(C(xi)|xi) +
∑
ei ∈Ey

log P(N(ei)|ei) +
∑

<mk,e j>∈Ay
log P(ej |c′mk )

(5-1)
where xyi is either a word or an entity, and C(x

y
i ) denotes: (i) contextual words in a

pre-defined window of xyi if x
y
i ∈ Dy, (ii) neighbor entities that linked to x

y
i if x

y
i ∈ Gy,

(iii) contextual words of wyj if x
y
i is entity e

y
i in an anchor ⟨w

y
j , e

y
i ⟩ ∈ Ay.

5.5.2 跨语言实体正则项

5.5.3 跨语言句对正则项

5.5.4 训练

5.6 实验

5.6.1 实验设置

5.6.2 定性分析

5.6.3 单词翻译效果分析

5.6.4 实体相关性效果分析

5.6.5 跨语言实体链接效果分析

5.7 本章小结

70

eFoust

4

300

301

302

303

304

305

306

307

308

309

310

311

312

313

314

315

316

317

318

319

320

321

322

323

324

325

326

327

328

329

330

331

332

333

334

335

336

337

338

339

340

341

342

343

344

345

346

347

348

349

350

351

352

353

354

355

356

357

358

359

360

361

362

363

364

365

366

367

368

369

370

371

372

373

374

375

376

377

378

379

380

381

382

383

384

385

386

387

388

389

390

391

392

393

394

395

396

397

398

399

ACL 2018 Submission ***. Confidential Review Copy. DO NOT DISTRIBUTE.

NBA

Lawrence Foust

All-star

[[Lawrence Michael Foust]] was
an American basketball player

who spent 12 seasons in [[NBA]]

NBA (zh)

·
[[ · ]] [[NBA]]

1950 [[NBA ]] 1 5

English KB Chinese KB

独立日 美国独立日

Sen Szh een ezh

L =
∑

xi ∈D

∑

xo ∈C(xi )
P(xo |xi)

L =
∑

(ei ,ej )∈Ec
| |C(ei) − C(e j)| |2

Ec

L =
∑

ei ∈E
P(N (ei)|ei) +

∑

(ei ,ej )∈Ec
P(N (e j)|ei)

N (ei)

L =
∑

(ei ,mi )∈A
P(ei |mi ,C(ei)) +

∑

(ei ,ej )∈Ec
P(e j |mi ,C(ei))

独立日 美国独立日

Sen Szh een ezh

L =
∑

xi ∈D

∑

xo ∈C(xi )
P(xo |xi)

L =
∑

(ei ,ej )∈Ec
| |C(ei) − C(e j)| |2

Ec

L =
∑

ei ∈E
P(N (ei)|ei) +

∑

(ei ,ej )∈Ec
P(N (e j)|ei)

N (ei)

L =
∑

(ei ,mi )∈A
P(ei |mi ,C(ei)) +

∑

(ei ,ej )∈Ec
P(e j |mi ,C(ei))

was

Lawrence
NBA

NBA (zh)

basketball

Semantic Space

Representation
Learning

Cross-lingual
Sentence Regularizer

Cross-lingual
Entity Regularizer

[[ · ]] [[NBA]]

All-star

[[Lawrence Michael Foust]] was an American basketball 
player who spent 12 seasons in [[NBA]]

·

NBA
NBA (zh)

Comparable Sentences

Bi-lingual EN

att
att

att

… …

Ls = | |Sen − Szh | |2

L =
∑

xi ∈D

∑

xo ∈C(xi )
P(xo |xi )

L =
∑

(ei ,ej )∈Ec
| |C(ei ) − C(ej )| |2

Ec

L =
∑

ei ∈E
P(N (ei )|ei ) +

∑

(ei ,ej )∈Ec
P(N (ej )|ei )

N (ei )

L =
∑

(ei ,mi )∈A
P(ei |mi ,C(ei )) +

∑

(ei ,ej )∈Ec
P(ej |mi ,C(ei ))

=

500

501

502

503

504

505

506

507

508

509

510

511

512

513

514

515

516

517

518

519

520

521

522

523

524

525

526

527

528

529

530

531

532

533

534

535

536

537

538

539

540

541

542

543

544

545

546

547

548

549

550

551

552

553

554

555

556

557

558

559

560

561

562

563

564

565

566

567

568

569

570

571

572

573

574

575

576

577

578

579

580

581

582

583

584

585

586

587

588

589

590

591

592

593

594

595

596

597

598

599

ACL 2018 Submission ***. Confidential Review Copy. DO NOT DISTRIBUTE.

skm ≈
wki +

∑
C(wki)

Ls =
∑

<skm,slm>∈Sk,l

|| − ||2

ekm
ekm

k

αnkm ∝ sim( ,
∑

wki∈snkm

)

αnkm
n skm

ekm αnkm

αki,lj ∝
wki∈skm,wlj∈slm

sim(wki, wlj)

=
∑

snkm∈skm

αnkm
∑

wki∈snkm

αki,lj

L = Lc + γLs

γ

400

401

402

403

404

405

406

407

408

409

410

411

412

413

414

415

416

417

418

419

420

421

422

423

424

425

426

427

428

429

430

431

432

433

434

435

436

437

438

439

440

441

442

443

444

445

446

447

448

449

450

451

452

453

454

455

456

457

458

459

460

461

462

463

464

465

466

467

468

469

470

471

472

473

474

475

476

477

478

479

480

481

482

483

484

485

486

487

488

489

490

491

492

493

494

495

496

497

498

499

ACL 2018 Submission ***. Confidential Review Copy. DO NOT DISTRIBUTE.

eki elj < eki, elj >∈ Rc

een,Kobe
ezh,Kobe

een,Joe ezh,Joe

< een,Kobe, ezh,Kobe >
< een,Joe, ezh,Joe >

D A
KN

Lc =
∑

xi∈{D,A,KN}

P (C(xi)|xi)

xi
C(xi)

C(xi)
xi

xi ∈ D
xi ∈ KN

wj xi
< wj , ei >∈ A

een,Joe wen,player
wen,NBA

een,LosAngelesLakers
een,Joe een,Kobe ezh,Kobe

wki
Cl(wki)

Ls =
∑

<skm,slm∈Sk,l

∑

wki∈skm

||wki −
∑

Cl(wki)||2

< skm, slm >∈ Sk,l m
k l

Cross-lingual
 supervision

Data Generation

and was an 8-time [[All-star]] … [[ ]] …

NBA

Lawrence Foust

NBA

player

American
NBA

All-star
eLawrence

wbasketballwplayer
… …

1

000

001

002

003

004

005

006

007

008

009

010

011

012

013

014

015

016

017

018

019

020

021

022

023

024

025

026

027

028

029

030

031

032

033

034

035

036

037

038

039

040

041

042

043

044

045

046

047

048

049

050

051

052

053

054

055

056

057

058

059

060

061

062

063

064

065

066

067

068

069

070

071

072

073

074

075

076

077

078

079

080

081

082

083

084

085

086

087

088

089

090

091

092

093

094

095

096

097

098

099

ACL 2018 Submission ***. Confidential Review Copy. DO NOT DISTRIBUTE.

Joint Representation Learning of Cross-lingual Words and Entities via
Attentive Distant Supervision

Anonymous ACL submission

Abstract

Jointly learning word and entity represen-
tations benefits many NLP tasks, while has
not been well explored in cross-lingual set-
tings. In this paper, we propose a novel
method that integrates cross-lingual word
and entity representation learning to enable
joint inference among knowledge base and
text across languages, capturing mutually
complementary knowledge. Instead of re-
liance on parallel data, we automatically
generate cross-lingual training data via dis-
tant supervision over multi-lingual knowl-
edge bases. We also propose two types
of knowledge attention and cross-lingual
attention to select the most informative
words and filter out noise, which will fur-
ther improve the performance. In exper-
iments, separate tasks of word translation
and entity relatedness demonstrate the ef-
fectiveness of our method with an average
gain of 20% and 3% over baselines, re-
spectively. Using entity linking as a case
study, the results on benchmark dataset
verify the quality of our embeddings.

1 Introduction

Multi-lingual knowledge bases (KB), storing mil-
lions of entities and their facts in various lan-
guages, provide rich structured knowledge for un-
derstanding natural language beyond texts. Mean-
while, abundant text corpus contains large amount
of potential knowledge complementary to existing
KBs. Therefore, researchers leverage both types
of resources to improve various natural language
processing (NLP) related tasks, such as relation ex-
traction (Weston et al., 2013; Lin et al., 2017), and
entity linking (Tsai and Roth, 2016; Yamada et al.,
2016; Cao et al., 2017; Ji et al., 2016).

Most existing work jointly models KB and text
corpus to enhance each other by learning word and
entity representations in a unified vector space. For
example, (Wang et al., 2014; Yamada et al., 2016;
Cao et al., 2017) utilize the coherence informa-
tion to align similar words and entities with sim-
ilar embedding vectors. Another approach in (Han
et al., 2016; Toutanova et al., 2015; Wu et al.,
2016) learns to represent entities based on their
textual descriptions together with the structured re-
lations. However, these methods only focus on
mono-lingual settings, and few researches have
been done in cross-lingual scenarios.
In this paper, we propose to learn cross-lingual

word and entity representations in the same seman-
tic space, to enable joint inference among KB and
text across languages without any additional trans-
lation mechanism, which is usually expensive and
may introduce inevitable errors. Our embeddings
are helpful to break down language gaps in many
tasks, such as cross-lingual entity linking, in which
the major challenge lies in measuring the similar-
ity between entities and corresponding mentioned
words in different languages.
e , w ,w
The intuition is that, words and entities in

various languages share some common semantic
meanings1, but there are also ways in which they
differ. On one hand, we utilize their shared seman-
tics to align similar words and entities with simi-
lar embedding vectors, no matter they are in the
same language or not. On the other hand, cross-
lingual embeddings will benefit from different lan-
guages due to the complementary knowledge. For
instance, textual ambiguity in one language may
disappear in another language, e.g., the two mean-

1Some cross-lingual pioneering work observe that word
embeddings trained separately on monolingual corpora ex-
hibit isomorphic structure across languages (Mikolov et al.,
2013; Zhang et al., 2017).

1

000

001

002

003

004

005

006

007

008

009

010

011

012

013

014

015

016

017

018

019

020

021

022

023

024

025

026

027

028

029

030

031

032

033

034

035

036

037

038

039

040

041

042

043

044

045

046

047

048

049

050

051

052

053

054

055

056

057

058

059

060

061

062

063

064

065

066

067

068

069

070

071

072

073

074

075

076

077

078

079

080

081

082

083

084

085

086

087

088

089

090

091

092

093

094

095

096

097

098

099

ACL 2018 Submission ***. Confidential Review Copy. DO NOT DISTRIBUTE.

Joint Representation Learning of Cross-lingual Words and Entities via
Attentive Distant Supervision

Anonymous ACL submission

Abstract

Jointly learning word and entity represen-
tations benefits many NLP tasks, while has
not been well explored in cross-lingual set-
tings. In this paper, we propose a novel
method that integrates cross-lingual word
and entity representation learning to enable
joint inference among knowledge base and
text across languages, capturing mutually
complementary knowledge. Instead of re-
liance on parallel data, we automatically
generate cross-lingual training data via dis-
tant supervision over multi-lingual knowl-
edge bases. We also propose two types
of knowledge attention and cross-lingual
attention to select the most informative
words and filter out noise, which will fur-
ther improve the performance. In exper-
iments, separate tasks of word translation
and entity relatedness demonstrate the ef-
fectiveness of our method with an average
gain of 20% and 3% over baselines, re-
spectively. Using entity linking as a case
study, the results on benchmark dataset
verify the quality of our embeddings.

1 Introduction

Multi-lingual knowledge bases (KB), storing mil-
lions of entities and their facts in various lan-
guages, provide rich structured knowledge for un-
derstanding natural language beyond texts. Mean-
while, abundant text corpus contains large amount
of potential knowledge complementary to existing
KBs. Therefore, researchers leverage both types
of resources to improve various natural language
processing (NLP) related tasks, such as relation ex-
traction (Weston et al., 2013; Lin et al., 2017), and
entity linking (Tsai and Roth, 2016; Yamada et al.,
2016; Cao et al., 2017; Ji et al., 2016).

Most existing work jointly models KB and text
corpus to enhance each other by learning word and
entity representations in a unified vector space. For
example, (Wang et al., 2014; Yamada et al., 2016;
Cao et al., 2017) utilize the coherence informa-
tion to align similar words and entities with sim-
ilar embedding vectors. Another approach in (Han
et al., 2016; Toutanova et al., 2015; Wu et al.,
2016) learns to represent entities based on their
textual descriptions together with the structured re-
lations. However, these methods only focus on
mono-lingual settings, and few researches have
been done in cross-lingual scenarios.
In this paper, we propose to learn cross-lingual

word and entity representations in the same seman-
tic space, to enable joint inference among KB and
text across languages without any additional trans-
lation mechanism, which is usually expensive and
may introduce inevitable errors. Our embeddings
are helpful to break down language gaps in many
tasks, such as cross-lingual entity linking, in which
the major challenge lies in measuring the similar-
ity between entities and corresponding mentioned
words in different languages.
e , w ,w
The intuition is that, words and entities in

various languages share some common semantic
meanings1, but there are also ways in which they
differ. On one hand, we utilize their shared seman-
tics to align similar words and entities with simi-
lar embedding vectors, no matter they are in the
same language or not. On the other hand, cross-
lingual embeddings will benefit from different lan-
guages due to the complementary knowledge. For
instance, textual ambiguity in one language may
disappear in another language, e.g., the two mean-

1Some cross-lingual pioneering work observe that word
embeddings trained separately on monolingual corpora ex-
hibit isomorphic structure across languages (Mikolov et al.,
2013; Zhang et al., 2017).

1

000

001

002

003

004

005

006

007

008

009

010

011

012

013

014

015

016

017

018

019

020

021

022

023

024

025

026

027

028

029

030

031

032

033

034

035

036

037

038

039

040

041

042

043

044

045

046

047

048

049

050

051

052

053

054

055

056

057

058

059

060

061

062

063

064

065

066

067

068

069

070

071

072

073

074

075

076

077

078

079

080

081

082

083

084

085

086

087

088

089

090

091

092

093

094

095

096

097

098

099

ACL 2018 Submission ***. Confidential Review Copy. DO NOT DISTRIBUTE.

Joint Representation Learning of Cross-lingual Words and Entities via
Attentive Distant Supervision

Anonymous ACL submission

Abstract

Jointly learning word and entity represen-
tations benefits many NLP tasks, while has
not been well explored in cross-lingual set-
tings. In this paper, we propose a novel
method that integrates cross-lingual word
and entity representation learning to enable
joint inference among knowledge base and
text across languages, capturing mutually
complementary knowledge. Instead of re-
liance on parallel data, we automatically
generate cross-lingual training data via dis-
tant supervision over multi-lingual knowl-
edge bases. We also propose two types
of knowledge attention and cross-lingual
attention to select the most informative
words and filter out noise, which will fur-
ther improve the performance. In exper-
iments, separate tasks of word translation
and entity relatedness demonstrate the ef-
fectiveness of our method with an average
gain of 20% and 3% over baselines, re-
spectively. Using entity linking as a case
study, the results on benchmark dataset
verify the quality of our embeddings.

1 Introduction

Multi-lingual knowledge bases (KB), storing mil-
lions of entities and their facts in various lan-
guages, provide rich structured knowledge for un-
derstanding natural language beyond texts. Mean-
while, abundant text corpus contains large amount
of potential knowledge complementary to existing
KBs. Therefore, researchers leverage both types
of resources to improve various natural language
processing (NLP) related tasks, such as relation ex-
traction (Weston et al., 2013; Lin et al., 2017), and
entity linking (Tsai and Roth, 2016; Yamada et al.,
2016; Cao et al., 2017; Ji et al., 2016).

Most existing work jointly models KB and text
corpus to enhance each other by learning word and
entity representations in a unified vector space. For
example, (Wang et al., 2014; Yamada et al., 2016;
Cao et al., 2017) utilize the coherence informa-
tion to align similar words and entities with sim-
ilar embedding vectors. Another approach in (Han
et al., 2016; Toutanova et al., 2015; Wu et al.,
2016) learns to represent entities based on their
textual descriptions together with the structured re-
lations. However, these methods only focus on
mono-lingual settings, and few researches have
been done in cross-lingual scenarios.
In this paper, we propose to learn cross-lingual

word and entity representations in the same seman-
tic space, to enable joint inference among KB and
text across languages without any additional trans-
lation mechanism, which is usually expensive and
may introduce inevitable errors. Our embeddings
are helpful to break down language gaps in many
tasks, such as cross-lingual entity linking, in which
the major challenge lies in measuring the similar-
ity between entities and corresponding mentioned
words in different languages.
e , w ,w
The intuition is that, words and entities in

various languages share some common semantic
meanings1, but there are also ways in which they
differ. On one hand, we utilize their shared seman-
tics to align similar words and entities with simi-
lar embedding vectors, no matter they are in the
same language or not. On the other hand, cross-
lingual embeddings will benefit from different lan-
guages due to the complementary knowledge. For
instance, textual ambiguity in one language may
disappear in another language, e.g., the two mean-

1Some cross-lingual pioneering work observe that word
embeddings trained separately on monolingual corpora ex-
hibit isomorphic structure across languages (Mikolov et al.,
2013; Zhang et al., 2017).

1

000

001

002

003

004

005

006

007

008

009

010

011

012

013

014

015

016

017

018

019

020

021

022

023

024

025

026

027

028

029

030

031

032

033

034

035

036

037

038

039

040

041

042

043

044

045

046

047

048

049

050

051

052

053

054

055

056

057

058

059

060

061

062

063

064

065

066

067

068

069

070

071

072

073

074

075

076

077

078

079

080

081

082

083

084

085

086

087

088

089

090

091

092

093

094

095

096

097

098

099

ACL 2018 Submission ***. Confidential Review Copy. DO NOT DISTRIBUTE.

Joint Representation Learning of Cross-lingual Words and Entities via
Attentive Distant Supervision

Anonymous ACL submission

Abstract

Jointly learning word and entity represen-
tations benefits many NLP tasks, while has
not been well explored in cross-lingual set-
tings. In this paper, we propose a novel
method that integrates cross-lingual word
and entity representation learning to enable
joint inference among knowledge base and
text across languages, capturing mutually
complementary knowledge. Instead of re-
liance on parallel data, we automatically
generate cross-lingual training data via dis-
tant supervision over multi-lingual knowl-
edge bases. We also propose two types
of knowledge attention and cross-lingual
attention to select the most informative
words and filter out noise, which will fur-
ther improve the performance. In exper-
iments, separate tasks of word translation
and entity relatedness demonstrate the ef-
fectiveness of our method with an average
gain of 20% and 3% over baselines, re-
spectively. Using entity linking as a case
study, the results on benchmark dataset
verify the quality of our embeddings.

1 Introduction

Multi-lingual knowledge bases (KB), storing mil-
lions of entities and their facts in various lan-
guages, provide rich structured knowledge for un-
derstanding natural language beyond texts. Mean-
while, abundant text corpus contains large amount
of potential knowledge complementary to existing
KBs. Therefore, researchers leverage both types
of resources to improve various natural language
processing (NLP) related tasks, such as relation ex-
traction (Weston et al., 2013; Lin et al., 2017), and
entity linking (Tsai and Roth, 2016; Yamada et al.,
2016; Cao et al., 2017; Ji et al., 2016).

Most existing work jointly models KB and text
corpus to enhance each other by learning word and
entity representations in a unified vector space. For
example, (Wang et al., 2014; Yamada et al., 2016;
Cao et al., 2017) utilize the coherence informa-
tion to align similar words and entities with sim-
ilar embedding vectors. Another approach in (Han
et al., 2016; Toutanova et al., 2015; Wu et al.,
2016) learns to represent entities based on their
textual descriptions together with the structured re-
lations. However, these methods only focus on
mono-lingual settings, and few researches have
been done in cross-lingual scenarios.
In this paper, we propose to learn cross-lingual

word and entity representations in the same seman-
tic space, to enable joint inference among KB and
text across languages without any additional trans-
lation mechanism, which is usually expensive and
may introduce inevitable errors. Our embeddings
are helpful to break down language gaps in many
tasks, such as cross-lingual entity linking, in which
the major challenge lies in measuring the similar-
ity between entities and corresponding mentioned
words in different languages.
e , w ,w
The intuition is that, words and entities in

various languages share some common semantic
meanings1, but there are also ways in which they
differ. On one hand, we utilize their shared seman-
tics to align similar words and entities with simi-
lar embedding vectors, no matter they are in the
same language or not. On the other hand, cross-
lingual embeddings will benefit from different lan-
guages due to the complementary knowledge. For
instance, textual ambiguity in one language may
disappear in another language, e.g., the two mean-

1Some cross-lingual pioneering work observe that word
embeddings trained separately on monolingual corpora ex-
hibit isomorphic structure across languages (Mikolov et al.,
2013; Zhang et al., 2017).

eNBAeAllstar

Mono-lingual Representation Learning

Zh

1

000

001

002

003

004

005

006

007

008

009

010

011

012

013

014

015

016

017

018

019

020

021

022

023

024

025

026

027

028

029

030

031

032

033

034

035

036

037

038

039

040

041

042

043

044

045

046

047

048

049

050

051

052

053

054

055

056

057

058

059

060

061

062

063

064

065

066

067

068

069

070

071

072

073

074

075

076

077

078

079

080

081

082

083

084

085

086

087

088

089

090

091

092

093

094

095

096

097

098

099

ACL 2018 Submission ***. Confidential Review Copy. DO NOT DISTRIBUTE.

Joint Representation Learning of Cross-lingual Words and Entities via
Attentive Distant Supervision

Anonymous ACL submission

Abstract

Jointly learning word and entity represen-
tations benefits many NLP tasks, while has
not been well explored in cross-lingual set-
tings. In this paper, we propose a novel
method that integrates cross-lingual word
and entity representation learning to enable
joint inference among knowledge base and
text across languages, capturing mutually
complementary knowledge. Instead of re-
liance on parallel data, we automatically
generate cross-lingual training data via dis-
tant supervision over multi-lingual knowl-
edge bases. We also propose two types
of knowledge attention and cross-lingual
attention to select the most informative
words and filter out noise, which will fur-
ther improve the performance. In exper-
iments, separate tasks of word translation
and entity relatedness demonstrate the ef-
fectiveness of our method with an average
gain of 20% and 3% over baselines, re-
spectively. Using entity linking as a case
study, the results on benchmark dataset
verify the quality of our embeddings.

1 Introduction

Multi-lingual knowledge bases (KB), storing mil-
lions of entities and their facts in various lan-
guages, provide rich structured knowledge for un-
derstanding natural language beyond texts. Mean-
while, abundant text corpus contains large amount
of potential knowledge complementary to existing
KBs. Therefore, researchers leverage both types
of resources to improve various natural language
processing (NLP) related tasks, such as relation ex-
traction (Weston et al., 2013; Lin et al., 2017), and
entity linking (Tsai and Roth, 2016; Yamada et al.,
2016; Cao et al., 2017; Ji et al., 2016).

Most existing work jointly models KB and text
corpus to enhance each other by learning word and
entity representations in a unified vector space. For
example, (Wang et al., 2014; Yamada et al., 2016;
Cao et al., 2017) utilize the coherence informa-
tion to align similar words and entities with sim-
ilar embedding vectors. Another approach in (Han
et al., 2016; Toutanova et al., 2015; Wu et al.,
2016) learns to represent entities based on their
textual descriptions together with the structured re-
lations. However, these methods only focus on
mono-lingual settings, and few researches have
been done in cross-lingual scenarios.
In this paper, we propose to learn cross-lingual

word and entity representations in the same seman-
tic space, to enable joint inference among KB and
text across languages without any additional trans-
lation mechanism, which is usually expensive and
may introduce inevitable errors. Our embeddings
are helpful to break down language gaps in many
tasks, such as cross-lingual entity linking, in which
the major challenge lies in measuring the similar-
ity between entities and corresponding mentioned
words in different languages.
e , w ,w
The intuition is that, words and entities in

various languages share some common semantic
meanings1, but there are also ways in which they
differ. On one hand, we utilize their shared seman-
tics to align similar words and entities with simi-
lar embedding vectors, no matter they are in the
same language or not. On the other hand, cross-
lingual embeddings will benefit from different lan-
guages due to the complementary knowledge. For
instance, textual ambiguity in one language may
disappear in another language, e.g., the two mean-

1Some cross-lingual pioneering work observe that word
embeddings trained separately on monolingual corpora ex-
hibit isomorphic structure across languages (Mikolov et al.,
2013; Zhang et al., 2017).

1

000

001

002

003

004

005

006

007

008

009

010

011

012

013

014

015

016

017

018

019

020

021

022

023

024

025

026

027

028

029

030

031

032

033

034

035

036

037

038

039

040

041

042

043

044

045

046

047

048

049

050

051

052

053

054

055

056

057

058

059

060

061

062

063

064

065

066

067

068

069

070

071

072

073

074

075

076

077

078

079

080

081

082

083

084

085

086

087

088

089

090

091

092

093

094

095

096

097

098

099

ACL 2018 Submission ***. Confidential Review Copy. DO NOT DISTRIBUTE.

Joint Representation Learning of Cross-lingual Words and Entities via
Attentive Distant Supervision

Anonymous ACL submission

Abstract

Jointly learning word and entity represen-
tations benefits many NLP tasks, while has
not been well explored in cross-lingual set-
tings. In this paper, we propose a novel
method that integrates cross-lingual word
and entity representation learning to enable
joint inference among knowledge base and
text across languages, capturing mutually
complementary knowledge. Instead of re-
liance on parallel data, we automatically
generate cross-lingual training data via dis-
tant supervision over multi-lingual knowl-
edge bases. We also propose two types
of knowledge attention and cross-lingual
attention to select the most informative
words and filter out noise, which will fur-
ther improve the performance. In exper-
iments, separate tasks of word translation
and entity relatedness demonstrate the ef-
fectiveness of our method with an average
gain of 20% and 3% over baselines, re-
spectively. Using entity linking as a case
study, the results on benchmark dataset
verify the quality of our embeddings.

1 Introduction

Multi-lingual knowledge bases (KB), storing mil-
lions of entities and their facts in various lan-
guages, provide rich structured knowledge for un-
derstanding natural language beyond texts. Mean-
while, abundant text corpus contains large amount
of potential knowledge complementary to existing
KBs. Therefore, researchers leverage both types
of resources to improve various natural language
processing (NLP) related tasks, such as relation ex-
traction (Weston et al., 2013; Lin et al., 2017), and
entity linking (Tsai and Roth, 2016; Yamada et al.,
2016; Cao et al., 2017; Ji et al., 2016).

Most existing work jointly models KB and text
corpus to enhance each other by learning word and
entity representations in a unified vector space. For
example, (Wang et al., 2014; Yamada et al., 2016;
Cao et al., 2017) utilize the coherence informa-
tion to align similar words and entities with sim-
ilar embedding vectors. Another approach in (Han
et al., 2016; Toutanova et al., 2015; Wu et al.,
2016) learns to represent entities based on their
textual descriptions together with the structured re-
lations. However, these methods only focus on
mono-lingual settings, and few researches have
been done in cross-lingual scenarios.
In this paper, we propose to learn cross-lingual

word and entity representations in the same seman-
tic space, to enable joint inference among KB and
text across languages without any additional trans-
lation mechanism, which is usually expensive and
may introduce inevitable errors. Our embeddings
are helpful to break down language gaps in many
tasks, such as cross-lingual entity linking, in which
the major challenge lies in measuring the similar-
ity between entities and corresponding mentioned
words in different languages.
eNBA , w , w ,w
The intuition is that, words and entities in

various languages share some common semantic
meanings1, but there are also ways in which they
differ. On one hand, we utilize their shared seman-
tics to align similar words and entities with simi-
lar embedding vectors, no matter they are in the
same language or not. On the other hand, cross-
lingual embeddings will benefit from different lan-
guages due to the complementary knowledge. For
instance, textual ambiguity in one language may
disappear in another language, e.g., the two mean-

1Some cross-lingual pioneering work observe that word
embeddings trained separately on monolingual corpora ex-
hibit isomorphic structure across languages (Mikolov et al.,
2013; Zhang et al., 2017).

1

000

001

002

003

004

005

006

007

008

009

010

011

012

013

014

015

016

017

018

019

020

021

022

023

024

025

026

027

028

029

030

031

032

033

034

035

036

037

038

039

040

041

042

043

044

045

046

047

048

049

050

051

052

053

054

055

056

057

058

059

060

061

062

063

064

065

066

067

068

069

070

071

072

073

074

075

076

077

078

079

080

081

082

083

084

085

086

087

088

089

090

091

092

093

094

095

096

097

098

099

ACL 2018 Submission ***. Confidential Review Copy. DO NOT DISTRIBUTE.

Joint Representation Learning of Cross-lingual Words and Entities via
Attentive Distant Supervision

Anonymous ACL submission

Abstract

Jointly learning word and entity represen-
tations benefits many NLP tasks, while has
not been well explored in cross-lingual set-
tings. In this paper, we propose a novel
method that integrates cross-lingual word
and entity representation learning to enable
joint inference among knowledge base and
text across languages, capturing mutually
complementary knowledge. Instead of re-
liance on parallel data, we automatically
generate cross-lingual training data via dis-
tant supervision over multi-lingual knowl-
edge bases. We also propose two types
of knowledge attention and cross-lingual
attention to select the most informative
words and filter out noise, which will fur-
ther improve the performance. In exper-
iments, separate tasks of word translation
and entity relatedness demonstrate the ef-
fectiveness of our method with an average
gain of 20% and 3% over baselines, re-
spectively. Using entity linking as a case
study, the results on benchmark dataset
verify the quality of our embeddings.

1 Introduction

Multi-lingual knowledge bases (KB), storing mil-
lions of entities and their facts in various lan-
guages, provide rich structured knowledge for un-
derstanding natural language beyond texts. Mean-
while, abundant text corpus contains large amount
of potential knowledge complementary to existing
KBs. Therefore, researchers leverage both types
of resources to improve various natural language
processing (NLP) related tasks, such as relation ex-
traction (Weston et al., 2013; Lin et al., 2017), and
entity linking (Tsai and Roth, 2016; Yamada et al.,
2016; Cao et al., 2017; Ji et al., 2016).

Most existing work jointly models KB and text
corpus to enhance each other by learning word and
entity representations in a unified vector space. For
example, (Wang et al., 2014; Yamada et al., 2016;
Cao et al., 2017) utilize the coherence informa-
tion to align similar words and entities with sim-
ilar embedding vectors. Another approach in (Han
et al., 2016; Toutanova et al., 2015; Wu et al.,
2016) learns to represent entities based on their
textual descriptions together with the structured re-
lations. However, these methods only focus on
mono-lingual settings, and few researches have
been done in cross-lingual scenarios.
In this paper, we propose to learn cross-lingual

word and entity representations in the same seman-
tic space, to enable joint inference among KB and
text across languages without any additional trans-
lation mechanism, which is usually expensive and
may introduce inevitable errors. Our embeddings
are helpful to break down language gaps in many
tasks, such as cross-lingual entity linking, in which
the major challenge lies in measuring the similar-
ity between entities and corresponding mentioned
words in different languages.
eNBA , w , w ,w
The intuition is that, words and entities in

various languages share some common semantic
meanings1, but there are also ways in which they
differ. On one hand, we utilize their shared seman-
tics to align similar words and entities with simi-
lar embedding vectors, no matter they are in the
same language or not. On the other hand, cross-
lingual embeddings will benefit from different lan-
guages due to the complementary knowledge. For
instance, textual ambiguity in one language may
disappear in another language, e.g., the two mean-

1Some cross-lingual pioneering work observe that word
embeddings trained separately on monolingual corpora ex-
hibit isomorphic structure across languages (Mikolov et al.,
2013; Zhang et al., 2017).

1

000

001

002

003

004

005

006

007

008

009

010

011

012

013

014

015

016

017

018

019

020

021

022

023

024

025

026

027

028

029

030

031

032

033

034

035

036

037

038

039

040

041

042

043

044

045

046

047

048

049

050

051

052

053

054

055

056

057

058

059

060

061

062

063

064

065

066

067

068

069

070

071

072

073

074

075

076

077

078

079

080

081

082

083

084

085

086

087

088

089

090

091

092

093

094

095

096

097

098

099

ACL 2018 Submission ***. Confidential Review Copy. DO NOT DISTRIBUTE.

Joint Representation Learning of Cross-lingual Words and Entities via
Attentive Distant Supervision

Anonymous ACL submission

Abstract

Jointly learning word and entity represen-
tations benefits many NLP tasks, while has
not been well explored in cross-lingual set-
tings. In this paper, we propose a novel
method that integrates cross-lingual word
and entity representation learning to enable
joint inference among knowledge base and
text across languages, capturing mutually
complementary knowledge. Instead of re-
liance on parallel data, we automatically
generate cross-lingual training data via dis-
tant supervision over multi-lingual knowl-
edge bases. We also propose two types
of knowledge attention and cross-lingual
attention to select the most informative
words and filter out noise, which will fur-
ther improve the performance. In exper-
iments, separate tasks of word translation
and entity relatedness demonstrate the ef-
fectiveness of our method with an average
gain of 20% and 3% over baselines, re-
spectively. Using entity linking as a case
study, the results on benchmark dataset
verify the quality of our embeddings.

1 Introduction

Multi-lingual knowledge bases (KB), storing mil-
lions of entities and their facts in various lan-
guages, provide rich structured knowledge for un-
derstanding natural language beyond texts. Mean-
while, abundant text corpus contains large amount
of potential knowledge complementary to existing
KBs. Therefore, researchers leverage both types
of resources to improve various natural language
processing (NLP) related tasks, such as relation ex-
traction (Weston et al., 2013; Lin et al., 2017), and
entity linking (Tsai and Roth, 2016; Yamada et al.,
2016; Cao et al., 2017; Ji et al., 2016).

Most existing work jointly models KB and text
corpus to enhance each other by learning word and
entity representations in a unified vector space. For
example, (Wang et al., 2014; Yamada et al., 2016;
Cao et al., 2017) utilize the coherence informa-
tion to align similar words and entities with sim-
ilar embedding vectors. Another approach in (Han
et al., 2016; Toutanova et al., 2015; Wu et al.,
2016) learns to represent entities based on their
textual descriptions together with the structured re-
lations. However, these methods only focus on
mono-lingual settings, and few researches have
been done in cross-lingual scenarios.
In this paper, we propose to learn cross-lingual

word and entity representations in the same seman-
tic space, to enable joint inference among KB and
text across languages without any additional trans-
lation mechanism, which is usually expensive and
may introduce inevitable errors. Our embeddings
are helpful to break down language gaps in many
tasks, such as cross-lingual entity linking, in which
the major challenge lies in measuring the similar-
ity between entities and corresponding mentioned
words in different languages.
eNBA , w , w ,w
The intuition is that, words and entities in

various languages share some common semantic
meanings1, but there are also ways in which they
differ. On one hand, we utilize their shared seman-
tics to align similar words and entities with simi-
lar embedding vectors, no matter they are in the
same language or not. On the other hand, cross-
lingual embeddings will benefit from different lan-
guages due to the complementary knowledge. For
instance, textual ambiguity in one language may
disappear in another language, e.g., the two mean-

1Some cross-lingual pioneering work observe that word
embeddings trained separately on monolingual corpora ex-
hibit isomorphic structure across languages (Mikolov et al.,
2013; Zhang et al., 2017).

1

000

001

002

003

004

005

006

007

008

009

010

011

012

013

014

015

016

017

018

019

020

021

022

023

024

025

026

027

028

029

030

031

032

033

034

035

036

037

038

039

040

041

042

043

044

045

046

047

048

049

050

051

052

053

054

055

056

057

058

059

060

061

062

063

064

065

066

067

068

069

070

071

072

073

074

075

076

077

078

079

080

081

082

083

084

085

086

087

088

089

090

091

092

093

094

095

096

097

098

099

ACL 2018 Submission ***. Confidential Review Copy. DO NOT DISTRIBUTE.

Joint Representation Learning of Cross-lingual Words and Entities via
Attentive Distant Supervision

Anonymous ACL submission

Abstract

Jointly learning word and entity represen-
tations benefits many NLP tasks, while has
not been well explored in cross-lingual set-
tings. In this paper, we propose a novel
method that integrates cross-lingual word
and entity representation learning to enable
joint inference among knowledge base and
text across languages, capturing mutually
complementary knowledge. Instead of re-
liance on parallel data, we automatically
generate cross-lingual training data via dis-
tant supervision over multi-lingual knowl-
edge bases. We also propose two types
of knowledge attention and cross-lingual
attention to select the most informative
words and filter out noise, which will fur-
ther improve the performance. In exper-
iments, separate tasks of word translation
and entity relatedness demonstrate the ef-
fectiveness of our method with an average
gain of 20% and 3% over baselines, re-
spectively. Using entity linking as a case
study, the results on benchmark dataset
verify the quality of our embeddings.

1 Introduction

Multi-lingual knowledge bases (KB), storing mil-
lions of entities and their facts in various lan-
guages, provide rich structured knowledge for un-
derstanding natural language beyond texts. Mean-
while, abundant text corpus contains large amount
of potential knowledge complementary to existing
KBs. Therefore, researchers leverage both types
of resources to improve various natural language
processing (NLP) related tasks, such as relation ex-
traction (Weston et al., 2013; Lin et al., 2017), and
entity linking (Tsai and Roth, 2016; Yamada et al.,
2016; Cao et al., 2017; Ji et al., 2016).

Most existing work jointly models KB and text
corpus to enhance each other by learning word and
entity representations in a unified vector space. For
example, (Wang et al., 2014; Yamada et al., 2016;
Cao et al., 2017) utilize the coherence informa-
tion to align similar words and entities with sim-
ilar embedding vectors. Another approach in (Han
et al., 2016; Toutanova et al., 2015; Wu et al.,
2016) learns to represent entities based on their
textual descriptions together with the structured re-
lations. However, these methods only focus on
mono-lingual settings, and few researches have
been done in cross-lingual scenarios.
In this paper, we propose to learn cross-lingual

word and entity representations in the same seman-
tic space, to enable joint inference among KB and
text across languages without any additional trans-
lation mechanism, which is usually expensive and
may introduce inevitable errors. Our embeddings
are helpful to break down language gaps in many
tasks, such as cross-lingual entity linking, in which
the major challenge lies in measuring the similar-
ity between entities and corresponding mentioned
words in different languages.
eNBA , w , w ,w
The intuition is that, words and entities in

various languages share some common semantic
meanings1, but there are also ways in which they
differ. On one hand, we utilize their shared seman-
tics to align similar words and entities with simi-
lar embedding vectors, no matter they are in the
same language or not. On the other hand, cross-
lingual embeddings will benefit from different lan-
guages due to the complementary knowledge. For
instance, textual ambiguity in one language may
disappear in another language, e.g., the two mean-

1Some cross-lingual pioneering work observe that word
embeddings trained separately on monolingual corpora ex-
hibit isomorphic structure across languages (Mikolov et al.,
2013; Zhang et al., 2017).

1

000

001

002

003

004

005

006

007

008

009

010

011

012

013

014

015

016

017

018

019

020

021

022

023

024

025

026

027

028

029

030

031

032

033

034

035

036

037

038

039

040

041

042

043

044

045

046

047

048

049

050

051

052

053

054

055

056

057

058

059

060

061

062

063

064

065

066

067

068

069

070

071

072

073

074

075

076

077

078

079

080

081

082

083

084

085

086

087

088

089

090

091

092

093

094

095

096

097

098

099

ACL 2018 Submission ***. Confidential Review Copy. DO NOT DISTRIBUTE.

Joint Representation Learning of Cross-lingual Words and Entities via
Attentive Distant Supervision

Anonymous ACL submission

Abstract

Jointly learning word and entity represen-
tations benefits many NLP tasks, while has
not been well explored in cross-lingual set-
tings. In this paper, we propose a novel
method that integrates cross-lingual word
and entity representation learning to enable
joint inference among knowledge base and
text across languages, capturing mutually
complementary knowledge. Instead of re-
liance on parallel data, we automatically
generate cross-lingual training data via dis-
tant supervision over multi-lingual knowl-
edge bases. We also propose two types
of knowledge attention and cross-lingual
attention to select the most informative
words and filter out noise, which will fur-
ther improve the performance. In exper-
iments, separate tasks of word translation
and entity relatedness demonstrate the ef-
fectiveness of our method with an average
gain of 20% and 3% over baselines, re-
spectively. Using entity linking as a case
study, the results on benchmark dataset
verify the quality of our embeddings.

1 Introduction

Multi-lingual knowledge bases (KB), storing mil-
lions of entities and their facts in various lan-
guages, provide rich structured knowledge for un-
derstanding natural language beyond texts. Mean-
while, abundant text corpus contains large amount
of potential knowledge complementary to existing
KBs. Therefore, researchers leverage both types
of resources to improve various natural language
processing (NLP) related tasks, such as relation ex-
traction (Weston et al., 2013; Lin et al., 2017), and
entity linking (Tsai and Roth, 2016; Yamada et al.,
2016; Cao et al., 2017; Ji et al., 2016).

Most existing work jointly models KB and text
corpus to enhance each other by learning word and
entity representations in a unified vector space. For
example, (Wang et al., 2014; Yamada et al., 2016;
Cao et al., 2017) utilize the coherence informa-
tion to align similar words and entities with sim-
ilar embedding vectors. Another approach in (Han
et al., 2016; Toutanova et al., 2015; Wu et al.,
2016) learns to represent entities based on their
textual descriptions together with the structured re-
lations. However, these methods only focus on
mono-lingual settings, and few researches have
been done in cross-lingual scenarios.
In this paper, we propose to learn cross-lingual

word and entity representations in the same seman-
tic space, to enable joint inference among KB and
text across languages without any additional trans-
lation mechanism, which is usually expensive and
may introduce inevitable errors. Our embeddings
are helpful to break down language gaps in many
tasks, such as cross-lingual entity linking, in which
the major challenge lies in measuring the similar-
ity between entities and corresponding mentioned
words in different languages.
eNBA
The intuition is that, words and entities in

various languages share some common semantic
meanings1, but there are also ways in which they
differ. On one hand, we utilize their shared seman-
tics to align similar words and entities with simi-
lar embedding vectors, no matter they are in the
same language or not. On the other hand, cross-
lingual embeddings will benefit from different lan-
guages due to the complementary knowledge. For
instance, textual ambiguity in one language may
disappear in another language, e.g., the two mean-

1Some cross-lingual pioneering work observe that word
embeddings trained separately on monolingual corpora ex-
hibit isomorphic structure across languages (Mikolov et al.,
2013; Zhang et al., 2017).

En

eLawrence

wwas wan

eLawrence

eNBAeAllstarwan wbasketball

wAmerican

Figure 1: The framework of our method. The inputs and outputs of each step are listed in the right
side, and in the left side there are three main components of joint representation learning. Red texts
with brackets are anchors, dashed lines between entities denote relations, and solid lines are cross-lingual
links.

mon neighbors have similar embeddings, no mat-
ter in which language, e.g., English entity Foust
and Chinese entity 福斯特 (Foust) are embed-
ded close in semantic space due to the common
neighborsNBA,All-star andNBA选秀 (draft), etc.
(3) Cross-lingual Sentence Regularizer aims to
learn mutually translated words with similar em-
beddings by pushing their cross-lingual contexts
(i.e. comparable sentences) together. For exam-
ple, English word basketball and the translated
Chinese word 篮球 frequently co-occur in com-
parable sentences, and are close in the semantic
space.
All word/entity embeddings are trained jointly

under a unified optimization objective. Next, we
will introduce how to generate bi-lingual EN and
comparable sentences as well as the three compo-
nents for joint representation learning in turn.

3.3 Cross-lingual Supervision Data
Generation

This section introduces how to extract more cross-
lingual clues from multi-lingual KB in the form of
bi-lingual EN and comparable sentences.

Bi-lingual Entity Network Construction

Conventional knowledge representation meth-
ods normally regard cross-lingual links as a spe-
cial equivalence type of relation between two en-
tities (Zhu et al., 2017). However, we argue that
this may mislead to an inconsistent training ob-

jective since a cross-lingual link actually contains
multiple relations. For example (Figure 1), there
will be no direct relation betweenChinese entity福
斯特 (Foust) and English entity Piston by merely
adding the equivalence relation between Foust and
福斯特, which is in contradiction with the fact that
Foust belongs to Piston, no matter in which lan-
guage.
Therefore, we build bi-lingual entity network by

making cross-lingual linked entities that inherit all
relations from each other. Concretely, we enhance
mono-EN by adding edges from all neighbors of
entity eei to ezj if < eei , ezj >∈ Re−z (layer 2).
e

Comparable Sentences Generation

We utilize distant supervision to generate com-
parable sentences from Wikipedia articles. As
shown in Figure 1, from the page articles of cross-
lingual linked entities eeKobe and e

z
Kobe, we extract

those sentences including another cross-lingual
linked entities eeJoe and ezJoe as comparable sen-
tences Se−z = {< sek, szk >}.
The intuition is that we consider each sentence

in a Wikipedia article has a pseudo mention of
the page entity (talking something about the en-
tity) (Yamada et al., 2017). Thus, if a sentence also
mentions another entity, it implicitly expresses
their relation. Therefore, we make a similar as-
sumption as in relation extraction: If two enti-
ties participate in a relation, and both of them

4

300

301

302

303

304

305

306

307

308

309

310

311

312

313

314

315

316

317

318

319

320

321

322

323

324

325

326

327

328

329

330

331

332

333

334

335

336

337

338

339

340

341

342

343

344

345

346

347

348

349

350

351

352

353

354

355

356

357

358

359

360

361

362

363

364

365

366

367

368

369

370

371

372

373

374

375

376

377

378

379

380

381

382

383

384

385

386

387

388

389

390

391

392

393

394

395

396

397

398

399

ACL 2018 Submission ***. Confidential Review Copy. DO NOT DISTRIBUTE.

NBA

Lawrence Foust

All-star

[[Lawrence Michael Foust]] was
an American basketball player

who spent 12 seasons in [[NBA]]

NBA (zh)

·
[[ · ]] [[NBA]]

1950 [[NBA ]] 1 5

English KB Chinese KB

独立日 美国独立日

Sen Szh een ezh

L =
∑

xi ∈D

∑

xo ∈C(xi )
P(xo |xi)

L =
∑

(ei ,ej )∈Ec
| |C(ei) − C(e j)| |2

Ec

L =
∑

ei ∈E
P(N (ei)|ei) +

∑

(ei ,ej )∈Ec
P(N (e j)|ei)

N (ei)

L =
∑

(ei ,mi )∈A
P(ei |mi ,C(ei)) +

∑

(ei ,ej )∈Ec
P(e j |mi ,C(ei))

独立日 美国独立日

Sen Szh een ezh

L =
∑

xi ∈D

∑

xo ∈C(xi )
P(xo |xi)

L =
∑

(ei ,ej )∈Ec
| |C(ei) − C(e j)| |2

Ec

L =
∑

ei ∈E
P(N (ei)|ei) +

∑

(ei ,ej )∈Ec
P(N (e j)|ei)

N (ei)

L =
∑

(ei ,mi )∈A
P(ei |mi ,C(ei)) +

∑

(ei ,ej )∈Ec
P(e j |mi ,C(ei))

was

Lawrence
NBA

NBA (zh)

basketball

Semantic Space

Representation
Learning

Cross-lingual
Sentence Regularizer

Cross-lingual
Entity Regularizer

[[ · ]] [[NBA]]

All-star

[[Lawrence Michael Foust]] was an American basketball 
player who spent 12 seasons in [[NBA]]

·

NBA
NBA (zh)

Comparable Sentences

Bi-lingual EN

att
att

att

… …

Ls = | |Sen − Szh | |2

L =
∑

xi ∈D

∑

xo ∈C(xi )
P(xo |xi )

L =
∑

(ei ,ej )∈Ec
| |C(ei ) − C(ej )| |2

Ec

L =
∑

ei ∈E
P(N (ei )|ei ) +

∑

(ei ,ej )∈Ec
P(N (ej )|ei )

N (ei )

L =
∑

(ei ,mi )∈A
P(ei |mi ,C(ei )) +

∑

(ei ,ej )∈Ec
P(ej |mi ,C(ei ))

=

500

501

502

503

504

505

506

507

508

509

510

511

512

513

514

515

516

517

518

519

520

521

522

523

524

525

526

527

528

529

530

531

532

533

534

535

536

537

538

539

540

541

542

543

544

545

546

547

548

549

550

551

552

553

554

555

556

557

558

559

560

561

562

563

564

565

566

567

568

569

570

571

572

573

574

575

576

577

578

579

580

581

582

583

584

585

586

587

588

589

590

591

592

593

594

595

596

597

598

599

ACL 2018 Submission ***. Confidential Review Copy. DO NOT DISTRIBUTE.

skm ≈
wki +

∑
C(wki)

Ls =
∑

<skm,slm>∈Sk,l

|| − ||2

ekm
ekm

k

αnkm ∝ sim( ,
∑

wki∈snkm

)

αnkm
n skm

ekm αnkm

αki,lj ∝
wki∈skm,wlj∈slm

sim(wki, wlj)

=
∑

snkm∈skm

αnkm
∑

wki∈snkm

αki,lj

L = Lc + γLs

γ

400

401

402

403

404

405

406

407

408

409

410

411

412

413

414

415

416

417

418

419

420

421

422

423

424

425

426

427

428

429

430

431

432

433

434

435

436

437

438

439

440

441

442

443

444

445

446

447

448

449

450

451

452

453

454

455

456

457

458

459

460

461

462

463

464

465

466

467

468

469

470

471

472

473

474

475

476

477

478

479

480

481

482

483

484

485

486

487

488

489

490

491

492

493

494

495

496

497

498

499

ACL 2018 Submission ***. Confidential Review Copy. DO NOT DISTRIBUTE.

eki elj < eki, elj >∈ Rc

een,Kobe
ezh,Kobe

een,Joe ezh,Joe

< een,Kobe, ezh,Kobe >
< een,Joe, ezh,Joe >

D A
KN

Lc =
∑

xi∈{D,A,KN}

P (C(xi)|xi)

xi
C(xi)

C(xi)
xi

xi ∈ D
xi ∈ KN

wj xi
< wj , ei >∈ A

een,Joe wen,player
wen,NBA

een,LosAngelesLakers
een,Joe een,Kobe ezh,Kobe

wki
Cl(wki)

Ls =
∑

<skm,slm∈Sk,l

∑

wki∈skm

||wki −
∑

Cl(wki)||2

< skm, slm >∈ Sk,l m
k l

Cross-lingual
 supervision

Data Generation

and was an 8-time [[All-star]] … [[ ]] …

NBA

Lawrence Foust

NBA

player

American
NBA

All-star
eLawrence

wbasketballwplayer
… …

1

000

001

002

003

004

005

006

007

008

009

010

011

012

013

014

015

016

017

018

019

020

021

022

023

024

025

026

027

028

029

030

031

032

033

034

035

036

037

038

039

040

041

042

043

044

045

046

047

048

049

050

051

052

053

054

055

056

057

058

059

060

061

062

063

064

065

066

067

068

069

070

071

072

073

074

075

076

077

078

079

080

081

082

083

084

085

086

087

088

089

090

091

092

093

094

095

096

097

098

099

ACL 2018 Submission ***. Confidential Review Copy. DO NOT DISTRIBUTE.

Joint Representation Learning of Cross-lingual Words and Entities via
Attentive Distant Supervision

Anonymous ACL submission

Abstract

Jointly learning word and entity represen-
tations benefits many NLP tasks, while has
not been well explored in cross-lingual set-
tings. In this paper, we propose a novel
method that integrates cross-lingual word
and entity representation learning to enable
joint inference among knowledge base and
text across languages, capturing mutually
complementary knowledge. Instead of re-
liance on parallel data, we automatically
generate cross-lingual training data via dis-
tant supervision over multi-lingual knowl-
edge bases. We also propose two types
of knowledge attention and cross-lingual
attention to select the most informative
words and filter out noise, which will fur-
ther improve the performance. In exper-
iments, separate tasks of word translation
and entity relatedness demonstrate the ef-
fectiveness of our method with an average
gain of 20% and 3% over baselines, re-
spectively. Using entity linking as a case
study, the results on benchmark dataset
verify the quality of our embeddings.

1 Introduction

Multi-lingual knowledge bases (KB), storing mil-
lions of entities and their facts in various lan-
guages, provide rich structured knowledge for un-
derstanding natural language beyond texts. Mean-
while, abundant text corpus contains large amount
of potential knowledge complementary to existing
KBs. Therefore, researchers leverage both types
of resources to improve various natural language
processing (NLP) related tasks, such as relation ex-
traction (Weston et al., 2013; Lin et al., 2017), and
entity linking (Tsai and Roth, 2016; Yamada et al.,
2016; Cao et al., 2017; Ji et al., 2016).

Most existing work jointly models KB and text
corpus to enhance each other by learning word and
entity representations in a unified vector space. For
example, (Wang et al., 2014; Yamada et al., 2016;
Cao et al., 2017) utilize the coherence informa-
tion to align similar words and entities with sim-
ilar embedding vectors. Another approach in (Han
et al., 2016; Toutanova et al., 2015; Wu et al.,
2016) learns to represent entities based on their
textual descriptions together with the structured re-
lations. However, these methods only focus on
mono-lingual settings, and few researches have
been done in cross-lingual scenarios.
In this paper, we propose to learn cross-lingual

word and entity representations in the same seman-
tic space, to enable joint inference among KB and
text across languages without any additional trans-
lation mechanism, which is usually expensive and
may introduce inevitable errors. Our embeddings
are helpful to break down language gaps in many
tasks, such as cross-lingual entity linking, in which
the major challenge lies in measuring the similar-
ity between entities and corresponding mentioned
words in different languages.
e , w ,w
The intuition is that, words and entities in

various languages share some common semantic
meanings1, but there are also ways in which they
differ. On one hand, we utilize their shared seman-
tics to align similar words and entities with simi-
lar embedding vectors, no matter they are in the
same language or not. On the other hand, cross-
lingual embeddings will benefit from different lan-
guages due to the complementary knowledge. For
instance, textual ambiguity in one language may
disappear in another language, e.g., the two mean-

1Some cross-lingual pioneering work observe that word
embeddings trained separately on monolingual corpora ex-
hibit isomorphic structure across languages (Mikolov et al.,
2013; Zhang et al., 2017).

1

000

001

002

003

004

005

006

007

008

009

010

011

012

013

014

015

016

017

018

019

020

021

022

023

024

025

026

027

028

029

030

031

032

033

034

035

036

037

038

039

040

041

042

043

044

045

046

047

048

049

050

051

052

053

054

055

056

057

058

059

060

061

062

063

064

065

066

067

068

069

070

071

072

073

074

075

076

077

078

079

080

081

082

083

084

085

086

087

088

089

090

091

092

093

094

095

096

097

098

099

ACL 2018 Submission ***. Confidential Review Copy. DO NOT DISTRIBUTE.

Joint Representation Learning of Cross-lingual Words and Entities via
Attentive Distant Supervision

Anonymous ACL submission

Abstract

Jointly learning word and entity represen-
tations benefits many NLP tasks, while has
not been well explored in cross-lingual set-
tings. In this paper, we propose a novel
method that integrates cross-lingual word
and entity representation learning to enable
joint inference among knowledge base and
text across languages, capturing mutually
complementary knowledge. Instead of re-
liance on parallel data, we automatically
generate cross-lingual training data via dis-
tant supervision over multi-lingual knowl-
edge bases. We also propose two types
of knowledge attention and cross-lingual
attention to select the most informative
words and filter out noise, which will fur-
ther improve the performance. In exper-
iments, separate tasks of word translation
and entity relatedness demonstrate the ef-
fectiveness of our method with an average
gain of 20% and 3% over baselines, re-
spectively. Using entity linking as a case
study, the results on benchmark dataset
verify the quality of our embeddings.

1 Introduction

Multi-lingual knowledge bases (KB), storing mil-
lions of entities and their facts in various lan-
guages, provide rich structured knowledge for un-
derstanding natural language beyond texts. Mean-
while, abundant text corpus contains large amount
of potential knowledge complementary to existing
KBs. Therefore, researchers leverage both types
of resources to improve various natural language
processing (NLP) related tasks, such as relation ex-
traction (Weston et al., 2013; Lin et al., 2017), and
entity linking (Tsai and Roth, 2016; Yamada et al.,
2016; Cao et al., 2017; Ji et al., 2016).

Most existing work jointly models KB and text
corpus to enhance each other by learning word and
entity representations in a unified vector space. For
example, (Wang et al., 2014; Yamada et al., 2016;
Cao et al., 2017) utilize the coherence informa-
tion to align similar words and entities with sim-
ilar embedding vectors. Another approach in (Han
et al., 2016; Toutanova et al., 2015; Wu et al.,
2016) learns to represent entities based on their
textual descriptions together with the structured re-
lations. However, these methods only focus on
mono-lingual settings, and few researches have
been done in cross-lingual scenarios.
In this paper, we propose to learn cross-lingual

word and entity representations in the same seman-
tic space, to enable joint inference among KB and
text across languages without any additional trans-
lation mechanism, which is usually expensive and
may introduce inevitable errors. Our embeddings
are helpful to break down language gaps in many
tasks, such as cross-lingual entity linking, in which
the major challenge lies in measuring the similar-
ity between entities and corresponding mentioned
words in different languages.
e , w ,w
The intuition is that, words and entities in

various languages share some common semantic
meanings1, but there are also ways in which they
differ. On one hand, we utilize their shared seman-
tics to align similar words and entities with simi-
lar embedding vectors, no matter they are in the
same language or not. On the other hand, cross-
lingual embeddings will benefit from different lan-
guages due to the complementary knowledge. For
instance, textual ambiguity in one language may
disappear in another language, e.g., the two mean-

1Some cross-lingual pioneering work observe that word
embeddings trained separately on monolingual corpora ex-
hibit isomorphic structure across languages (Mikolov et al.,
2013; Zhang et al., 2017).

1

000

001

002

003

004

005

006

007

008

009

010

011

012

013

014

015

016

017

018

019

020

021

022

023

024

025

026

027

028

029

030

031

032

033

034

035

036

037

038

039

040

041

042

043

044

045

046

047

048

049

050

051

052

053

054

055

056

057

058

059

060

061

062

063

064

065

066

067

068

069

070

071

072

073

074

075

076

077

078

079

080

081

082

083

084

085

086

087

088

089

090

091

092

093

094

095

096

097

098

099

ACL 2018 Submission ***. Confidential Review Copy. DO NOT DISTRIBUTE.

Joint Representation Learning of Cross-lingual Words and Entities via
Attentive Distant Supervision

Anonymous ACL submission

Abstract

Jointly learning word and entity represen-
tations benefits many NLP tasks, while has
not been well explored in cross-lingual set-
tings. In this paper, we propose a novel
method that integrates cross-lingual word
and entity representation learning to enable
joint inference among knowledge base and
text across languages, capturing mutually
complementary knowledge. Instead of re-
liance on parallel data, we automatically
generate cross-lingual training data via dis-
tant supervision over multi-lingual knowl-
edge bases. We also propose two types
of knowledge attention and cross-lingual
attention to select the most informative
words and filter out noise, which will fur-
ther improve the performance. In exper-
iments, separate tasks of word translation
and entity relatedness demonstrate the ef-
fectiveness of our method with an average
gain of 20% and 3% over baselines, re-
spectively. Using entity linking as a case
study, the results on benchmark dataset
verify the quality of our embeddings.

1 Introduction

Multi-lingual knowledge bases (KB), storing mil-
lions of entities and their facts in various lan-
guages, provide rich structured knowledge for un-
derstanding natural language beyond texts. Mean-
while, abundant text corpus contains large amount
of potential knowledge complementary to existing
KBs. Therefore, researchers leverage both types
of resources to improve various natural language
processing (NLP) related tasks, such as relation ex-
traction (Weston et al., 2013; Lin et al., 2017), and
entity linking (Tsai and Roth, 2016; Yamada et al.,
2016; Cao et al., 2017; Ji et al., 2016).

Most existing work jointly models KB and text
corpus to enhance each other by learning word and
entity representations in a unified vector space. For
example, (Wang et al., 2014; Yamada et al., 2016;
Cao et al., 2017) utilize the coherence informa-
tion to align similar words and entities with sim-
ilar embedding vectors. Another approach in (Han
et al., 2016; Toutanova et al., 2015; Wu et al.,
2016) learns to represent entities based on their
textual descriptions together with the structured re-
lations. However, these methods only focus on
mono-lingual settings, and few researches have
been done in cross-lingual scenarios.
In this paper, we propose to learn cross-lingual

word and entity representations in the same seman-
tic space, to enable joint inference among KB and
text across languages without any additional trans-
lation mechanism, which is usually expensive and
may introduce inevitable errors. Our embeddings
are helpful to break down language gaps in many
tasks, such as cross-lingual entity linking, in which
the major challenge lies in measuring the similar-
ity between entities and corresponding mentioned
words in different languages.
e , w ,w
The intuition is that, words and entities in

various languages share some common semantic
meanings1, but there are also ways in which they
differ. On one hand, we utilize their shared seman-
tics to align similar words and entities with simi-
lar embedding vectors, no matter they are in the
same language or not. On the other hand, cross-
lingual embeddings will benefit from different lan-
guages due to the complementary knowledge. For
instance, textual ambiguity in one language may
disappear in another language, e.g., the two mean-

1Some cross-lingual pioneering work observe that word
embeddings trained separately on monolingual corpora ex-
hibit isomorphic structure across languages (Mikolov et al.,
2013; Zhang et al., 2017).

1

000

001

002

003

004

005

006

007

008

009

010

011

012

013

014

015

016

017

018

019

020

021

022

023

024

025

026

027

028

029

030

031

032

033

034

035

036

037

038

039

040

041

042

043

044

045

046

047

048

049

050

051

052

053

054

055

056

057

058

059

060

061

062

063

064

065

066

067

068

069

070

071

072

073

074

075

076

077

078

079

080

081

082

083

084

085

086

087

088

089

090

091

092

093

094

095

096

097

098

099

ACL 2018 Submission ***. Confidential Review Copy. DO NOT DISTRIBUTE.

Joint Representation Learning of Cross-lingual Words and Entities via
Attentive Distant Supervision

Anonymous ACL submission

Abstract

Jointly learning word and entity represen-
tations benefits many NLP tasks, while has
not been well explored in cross-lingual set-
tings. In this paper, we propose a novel
method that integrates cross-lingual word
and entity representation learning to enable
joint inference among knowledge base and
text across languages, capturing mutually
complementary knowledge. Instead of re-
liance on parallel data, we automatically
generate cross-lingual training data via dis-
tant supervision over multi-lingual knowl-
edge bases. We also propose two types
of knowledge attention and cross-lingual
attention to select the most informative
words and filter out noise, which will fur-
ther improve the performance. In exper-
iments, separate tasks of word translation
and entity relatedness demonstrate the ef-
fectiveness of our method with an average
gain of 20% and 3% over baselines, re-
spectively. Using entity linking as a case
study, the results on benchmark dataset
verify the quality of our embeddings.

1 Introduction

Multi-lingual knowledge bases (KB), storing mil-
lions of entities and their facts in various lan-
guages, provide rich structured knowledge for un-
derstanding natural language beyond texts. Mean-
while, abundant text corpus contains large amount
of potential knowledge complementary to existing
KBs. Therefore, researchers leverage both types
of resources to improve various natural language
processing (NLP) related tasks, such as relation ex-
traction (Weston et al., 2013; Lin et al., 2017), and
entity linking (Tsai and Roth, 2016; Yamada et al.,
2016; Cao et al., 2017; Ji et al., 2016).

Most existing work jointly models KB and text
corpus to enhance each other by learning word and
entity representations in a unified vector space. For
example, (Wang et al., 2014; Yamada et al., 2016;
Cao et al., 2017) utilize the coherence informa-
tion to align similar words and entities with sim-
ilar embedding vectors. Another approach in (Han
et al., 2016; Toutanova et al., 2015; Wu et al.,
2016) learns to represent entities based on their
textual descriptions together with the structured re-
lations. However, these methods only focus on
mono-lingual settings, and few researches have
been done in cross-lingual scenarios.
In this paper, we propose to learn cross-lingual

word and entity representations in the same seman-
tic space, to enable joint inference among KB and
text across languages without any additional trans-
lation mechanism, which is usually expensive and
may introduce inevitable errors. Our embeddings
are helpful to break down language gaps in many
tasks, such as cross-lingual entity linking, in which
the major challenge lies in measuring the similar-
ity between entities and corresponding mentioned
words in different languages.
e , w ,w
The intuition is that, words and entities in

various languages share some common semantic
meanings1, but there are also ways in which they
differ. On one hand, we utilize their shared seman-
tics to align similar words and entities with simi-
lar embedding vectors, no matter they are in the
same language or not. On the other hand, cross-
lingual embeddings will benefit from different lan-
guages due to the complementary knowledge. For
instance, textual ambiguity in one language may
disappear in another language, e.g., the two mean-

1Some cross-lingual pioneering work observe that word
embeddings trained separately on monolingual corpora ex-
hibit isomorphic structure across languages (Mikolov et al.,
2013; Zhang et al., 2017).

eNBAeAllstar

Mono-lingual Representation Learning

Zh

1

000

001

002

003

004

005

006

007

008

009

010

011

012

013

014

015

016

017

018

019

020

021

022

023

024

025

026

027

028

029

030

031

032

033

034

035

036

037

038

039

040

041

042

043

044

045

046

047

048

049

050

051

052

053

054

055

056

057

058

059

060

061

062

063

064

065

066

067

068

069

070

071

072

073

074

075

076

077

078

079

080

081

082

083

084

085

086

087

088

089

090

091

092

093

094

095

096

097

098

099

ACL 2018 Submission ***. Confidential Review Copy. DO NOT DISTRIBUTE.

Joint Representation Learning of Cross-lingual Words and Entities via
Attentive Distant Supervision

Anonymous ACL submission

Abstract

Jointly learning word and entity represen-
tations benefits many NLP tasks, while has
not been well explored in cross-lingual set-
tings. In this paper, we propose a novel
method that integrates cross-lingual word
and entity representation learning to enable
joint inference among knowledge base and
text across languages, capturing mutually
complementary knowledge. Instead of re-
liance on parallel data, we automatically
generate cross-lingual training data via dis-
tant supervision over multi-lingual knowl-
edge bases. We also propose two types
of knowledge attention and cross-lingual
attention to select the most informative
words and filter out noise, which will fur-
ther improve the performance. In exper-
iments, separate tasks of word translation
and entity relatedness demonstrate the ef-
fectiveness of our method with an average
gain of 20% and 3% over baselines, re-
spectively. Using entity linking as a case
study, the results on benchmark dataset
verify the quality of our embeddings.

1 Introduction

Multi-lingual knowledge bases (KB), storing mil-
lions of entities and their facts in various lan-
guages, provide rich structured knowledge for un-
derstanding natural language beyond texts. Mean-
while, abundant text corpus contains large amount
of potential knowledge complementary to existing
KBs. Therefore, researchers leverage both types
of resources to improve various natural language
processing (NLP) related tasks, such as relation ex-
traction (Weston et al., 2013; Lin et al., 2017), and
entity linking (Tsai and Roth, 2016; Yamada et al.,
2016; Cao et al., 2017; Ji et al., 2016).

Most existing work jointly models KB and text
corpus to enhance each other by learning word and
entity representations in a unified vector space. For
example, (Wang et al., 2014; Yamada et al., 2016;
Cao et al., 2017) utilize the coherence informa-
tion to align similar words and entities with sim-
ilar embedding vectors. Another approach in (Han
et al., 2016; Toutanova et al., 2015; Wu et al.,
2016) learns to represent entities based on their
textual descriptions together with the structured re-
lations. However, these methods only focus on
mono-lingual settings, and few researches have
been done in cross-lingual scenarios.
In this paper, we propose to learn cross-lingual

word and entity representations in the same seman-
tic space, to enable joint inference among KB and
text across languages without any additional trans-
lation mechanism, which is usually expensive and
may introduce inevitable errors. Our embeddings
are helpful to break down language gaps in many
tasks, such as cross-lingual entity linking, in which
the major challenge lies in measuring the similar-
ity between entities and corresponding mentioned
words in different languages.
e , w ,w
The intuition is that, words and entities in

various languages share some common semantic
meanings1, but there are also ways in which they
differ. On one hand, we utilize their shared seman-
tics to align similar words and entities with simi-
lar embedding vectors, no matter they are in the
same language or not. On the other hand, cross-
lingual embeddings will benefit from different lan-
guages due to the complementary knowledge. For
instance, textual ambiguity in one language may
disappear in another language, e.g., the two mean-

1Some cross-lingual pioneering work observe that word
embeddings trained separately on monolingual corpora ex-
hibit isomorphic structure across languages (Mikolov et al.,
2013; Zhang et al., 2017).

1

000

001

002

003

004

005

006

007

008

009

010

011

012

013

014

015

016

017

018

019

020

021

022

023

024

025

026

027

028

029

030

031

032

033

034

035

036

037

038

039

040

041

042

043

044

045

046

047

048

049

050

051

052

053

054

055

056

057

058

059

060

061

062

063

064

065

066

067

068

069

070

071

072

073

074

075

076

077

078

079

080

081

082

083

084

085

086

087

088

089

090

091

092

093

094

095

096

097

098

099

ACL 2018 Submission ***. Confidential Review Copy. DO NOT DISTRIBUTE.

Joint Representation Learning of Cross-lingual Words and Entities via
Attentive Distant Supervision

Anonymous ACL submission

Abstract

Jointly learning word and entity represen-
tations benefits many NLP tasks, while has
not been well explored in cross-lingual set-
tings. In this paper, we propose a novel
method that integrates cross-lingual word
and entity representation learning to enable
joint inference among knowledge base and
text across languages, capturing mutually
complementary knowledge. Instead of re-
liance on parallel data, we automatically
generate cross-lingual training data via dis-
tant supervision over multi-lingual knowl-
edge bases. We also propose two types
of knowledge attention and cross-lingual
attention to select the most informative
words and filter out noise, which will fur-
ther improve the performance. In exper-
iments, separate tasks of word translation
and entity relatedness demonstrate the ef-
fectiveness of our method with an average
gain of 20% and 3% over baselines, re-
spectively. Using entity linking as a case
study, the results on benchmark dataset
verify the quality of our embeddings.

1 Introduction

Multi-lingual knowledge bases (KB), storing mil-
lions of entities and their facts in various lan-
guages, provide rich structured knowledge for un-
derstanding natural language beyond texts. Mean-
while, abundant text corpus contains large amount
of potential knowledge complementary to existing
KBs. Therefore, researchers leverage both types
of resources to improve various natural language
processing (NLP) related tasks, such as relation ex-
traction (Weston et al., 2013; Lin et al., 2017), and
entity linking (Tsai and Roth, 2016; Yamada et al.,
2016; Cao et al., 2017; Ji et al., 2016).

Most existing work jointly models KB and text
corpus to enhance each other by learning word and
entity representations in a unified vector space. For
example, (Wang et al., 2014; Yamada et al., 2016;
Cao et al., 2017) utilize the coherence informa-
tion to align similar words and entities with sim-
ilar embedding vectors. Another approach in (Han
et al., 2016; Toutanova et al., 2015; Wu et al.,
2016) learns to represent entities based on their
textual descriptions together with the structured re-
lations. However, these methods only focus on
mono-lingual settings, and few researches have
been done in cross-lingual scenarios.
In this paper, we propose to learn cross-lingual

word and entity representations in the same seman-
tic space, to enable joint inference among KB and
text across languages without any additional trans-
lation mechanism, which is usually expensive and
may introduce inevitable errors. Our embeddings
are helpful to break down language gaps in many
tasks, such as cross-lingual entity linking, in which
the major challenge lies in measuring the similar-
ity between entities and corresponding mentioned
words in different languages.
eNBA , w , w ,w
The intuition is that, words and entities in

various languages share some common semantic
meanings1, but there are also ways in which they
differ. On one hand, we utilize their shared seman-
tics to align similar words and entities with simi-
lar embedding vectors, no matter they are in the
same language or not. On the other hand, cross-
lingual embeddings will benefit from different lan-
guages due to the complementary knowledge. For
instance, textual ambiguity in one language may
disappear in another language, e.g., the two mean-

1Some cross-lingual pioneering work observe that word
embeddings trained separately on monolingual corpora ex-
hibit isomorphic structure across languages (Mikolov et al.,
2013; Zhang et al., 2017).

1

000

001

002

003

004

005

006

007

008

009

010

011

012

013

014

015

016

017

018

019

020

021

022

023

024

025

026

027

028

029

030

031

032

033

034

035

036

037

038

039

040

041

042

043

044

045

046

047

048

049

050

051

052

053

054

055

056

057

058

059

060

061

062

063

064

065

066

067

068

069

070

071

072

073

074

075

076

077

078

079

080

081

082

083

084

085

086

087

088

089

090

091

092

093

094

095

096

097

098

099

ACL 2018 Submission ***. Confidential Review Copy. DO NOT DISTRIBUTE.

Joint Representation Learning of Cross-lingual Words and Entities via
Attentive Distant Supervision

Anonymous ACL submission

Abstract

Jointly learning word and entity represen-
tations benefits many NLP tasks, while has
not been well explored in cross-lingual set-
tings. In this paper, we propose a novel
method that integrates cross-lingual word
and entity representation learning to enable
joint inference among knowledge base and
text across languages, capturing mutually
complementary knowledge. Instead of re-
liance on parallel data, we automatically
generate cross-lingual training data via dis-
tant supervision over multi-lingual knowl-
edge bases. We also propose two types
of knowledge attention and cross-lingual
attention to select the most informative
words and filter out noise, which will fur-
ther improve the performance. In exper-
iments, separate tasks of word translation
and entity relatedness demonstrate the ef-
fectiveness of our method with an average
gain of 20% and 3% over baselines, re-
spectively. Using entity linking as a case
study, the results on benchmark dataset
verify the quality of our embeddings.

1 Introduction

Multi-lingual knowledge bases (KB), storing mil-
lions of entities and their facts in various lan-
guages, provide rich structured knowledge for un-
derstanding natural language beyond texts. Mean-
while, abundant text corpus contains large amount
of potential knowledge complementary to existing
KBs. Therefore, researchers leverage both types
of resources to improve various natural language
processing (NLP) related tasks, such as relation ex-
traction (Weston et al., 2013; Lin et al., 2017), and
entity linking (Tsai and Roth, 2016; Yamada et al.,
2016; Cao et al., 2017; Ji et al., 2016).

Most existing work jointly models KB and text
corpus to enhance each other by learning word and
entity representations in a unified vector space. For
example, (Wang et al., 2014; Yamada et al., 2016;
Cao et al., 2017) utilize the coherence informa-
tion to align similar words and entities with sim-
ilar embedding vectors. Another approach in (Han
et al., 2016; Toutanova et al., 2015; Wu et al.,
2016) learns to represent entities based on their
textual descriptions together with the structured re-
lations. However, these methods only focus on
mono-lingual settings, and few researches have
been done in cross-lingual scenarios.
In this paper, we propose to learn cross-lingual

word and entity representations in the same seman-
tic space, to enable joint inference among KB and
text across languages without any additional trans-
lation mechanism, which is usually expensive and
may introduce inevitable errors. Our embeddings
are helpful to break down language gaps in many
tasks, such as cross-lingual entity linking, in which
the major challenge lies in measuring the similar-
ity between entities and corresponding mentioned
words in different languages.
eNBA , w , w ,w
The intuition is that, words and entities in

various languages share some common semantic
meanings1, but there are also ways in which they
differ. On one hand, we utilize their shared seman-
tics to align similar words and entities with simi-
lar embedding vectors, no matter they are in the
same language or not. On the other hand, cross-
lingual embeddings will benefit from different lan-
guages due to the complementary knowledge. For
instance, textual ambiguity in one language may
disappear in another language, e.g., the two mean-

1Some cross-lingual pioneering work observe that word
embeddings trained separately on monolingual corpora ex-
hibit isomorphic structure across languages (Mikolov et al.,
2013; Zhang et al., 2017).

1

000

001

002

003

004

005

006

007

008

009

010

011

012

013

014

015

016

017

018

019

020

021

022

023

024

025

026

027

028

029

030

031

032

033

034

035

036

037

038

039

040

041

042

043

044

045

046

047

048

049

050

051

052

053

054

055

056

057

058

059

060

061

062

063

064

065

066

067

068

069

070

071

072

073

074

075

076

077

078

079

080

081

082

083

084

085

086

087

088

089

090

091

092

093

094

095

096

097

098

099

ACL 2018 Submission ***. Confidential Review Copy. DO NOT DISTRIBUTE.

Joint Representation Learning of Cross-lingual Words and Entities via
Attentive Distant Supervision

Anonymous ACL submission

Abstract

Jointly learning word and entity represen-
tations benefits many NLP tasks, while has
not been well explored in cross-lingual set-
tings. In this paper, we propose a novel
method that integrates cross-lingual word
and entity representation learning to enable
joint inference among knowledge base and
text across languages, capturing mutually
complementary knowledge. Instead of re-
liance on parallel data, we automatically
generate cross-lingual training data via dis-
tant supervision over multi-lingual knowl-
edge bases. We also propose two types
of knowledge attention and cross-lingual
attention to select the most informative
words and filter out noise, which will fur-
ther improve the performance. In exper-
iments, separate tasks of word translation
and entity relatedness demonstrate the ef-
fectiveness of our method with an average
gain of 20% and 3% over baselines, re-
spectively. Using entity linking as a case
study, the results on benchmark dataset
verify the quality of our embeddings.

1 Introduction

Multi-lingual knowledge bases (KB), storing mil-
lions of entities and their facts in various lan-
guages, provide rich structured knowledge for un-
derstanding natural language beyond texts. Mean-
while, abundant text corpus contains large amount
of potential knowledge complementary to existing
KBs. Therefore, researchers leverage both types
of resources to improve various natural language
processing (NLP) related tasks, such as relation ex-
traction (Weston et al., 2013; Lin et al., 2017), and
entity linking (Tsai and Roth, 2016; Yamada et al.,
2016; Cao et al., 2017; Ji et al., 2016).

Most existing work jointly models KB and text
corpus to enhance each other by learning word and
entity representations in a unified vector space. For
example, (Wang et al., 2014; Yamada et al., 2016;
Cao et al., 2017) utilize the coherence informa-
tion to align similar words and entities with sim-
ilar embedding vectors. Another approach in (Han
et al., 2016; Toutanova et al., 2015; Wu et al.,
2016) learns to represent entities based on their
textual descriptions together with the structured re-
lations. However, these methods only focus on
mono-lingual settings, and few researches have
been done in cross-lingual scenarios.
In this paper, we propose to learn cross-lingual

word and entity representations in the same seman-
tic space, to enable joint inference among KB and
text across languages without any additional trans-
lation mechanism, which is usually expensive and
may introduce inevitable errors. Our embeddings
are helpful to break down language gaps in many
tasks, such as cross-lingual entity linking, in which
the major challenge lies in measuring the similar-
ity between entities and corresponding mentioned
words in different languages.
eNBA , w , w ,w
The intuition is that, words and entities in

various languages share some common semantic
meanings1, but there are also ways in which they
differ. On one hand, we utilize their shared seman-
tics to align similar words and entities with simi-
lar embedding vectors, no matter they are in the
same language or not. On the other hand, cross-
lingual embeddings will benefit from different lan-
guages due to the complementary knowledge. For
instance, textual ambiguity in one language may
disappear in another language, e.g., the two mean-

1Some cross-lingual pioneering work observe that word
embeddings trained separately on monolingual corpora ex-
hibit isomorphic structure across languages (Mikolov et al.,
2013; Zhang et al., 2017).

1

000

001

002

003

004

005

006

007

008

009

010

011

012

013

014

015

016

017

018

019

020

021

022

023

024

025

026

027

028

029

030

031

032

033

034

035

036

037

038

039

040

041

042

043

044

045

046

047

048

049

050

051

052

053

054

055

056

057

058

059

060

061

062

063

064

065

066

067

068

069

070

071

072

073

074

075

076

077

078

079

080

081

082

083

084

085

086

087

088

089

090

091

092

093

094

095

096

097

098

099

ACL 2018 Submission ***. Confidential Review Copy. DO NOT DISTRIBUTE.

Joint Representation Learning of Cross-lingual Words and Entities via
Attentive Distant Supervision

Anonymous ACL submission

Abstract

Jointly learning word and entity represen-
tations benefits many NLP tasks, while has
not been well explored in cross-lingual set-
tings. In this paper, we propose a novel
method that integrates cross-lingual word
and entity representation learning to enable
joint inference among knowledge base and
text across languages, capturing mutually
complementary knowledge. Instead of re-
liance on parallel data, we automatically
generate cross-lingual training data via dis-
tant supervision over multi-lingual knowl-
edge bases. We also propose two types
of knowledge attention and cross-lingual
attention to select the most informative
words and filter out noise, which will fur-
ther improve the performance. In exper-
iments, separate tasks of word translation
and entity relatedness demonstrate the ef-
fectiveness of our method with an average
gain of 20% and 3% over baselines, re-
spectively. Using entity linking as a case
study, the results on benchmark dataset
verify the quality of our embeddings.

1 Introduction

Multi-lingual knowledge bases (KB), storing mil-
lions of entities and their facts in various lan-
guages, provide rich structured knowledge for un-
derstanding natural language beyond texts. Mean-
while, abundant text corpus contains large amount
of potential knowledge complementary to existing
KBs. Therefore, researchers leverage both types
of resources to improve various natural language
processing (NLP) related tasks, such as relation ex-
traction (Weston et al., 2013; Lin et al., 2017), and
entity linking (Tsai and Roth, 2016; Yamada et al.,
2016; Cao et al., 2017; Ji et al., 2016).

Most existing work jointly models KB and text
corpus to enhance each other by learning word and
entity representations in a unified vector space. For
example, (Wang et al., 2014; Yamada et al., 2016;
Cao et al., 2017) utilize the coherence informa-
tion to align similar words and entities with sim-
ilar embedding vectors. Another approach in (Han
et al., 2016; Toutanova et al., 2015; Wu et al.,
2016) learns to represent entities based on their
textual descriptions together with the structured re-
lations. However, these methods only focus on
mono-lingual settings, and few researches have
been done in cross-lingual scenarios.
In this paper, we propose to learn cross-lingual

word and entity representations in the same seman-
tic space, to enable joint inference among KB and
text across languages without any additional trans-
lation mechanism, which is usually expensive and
may introduce inevitable errors. Our embeddings
are helpful to break down language gaps in many
tasks, such as cross-lingual entity linking, in which
the major challenge lies in measuring the similar-
ity between entities and corresponding mentioned
words in different languages.
eNBA , w , w ,w
The intuition is that, words and entities in

various languages share some common semantic
meanings1, but there are also ways in which they
differ. On one hand, we utilize their shared seman-
tics to align similar words and entities with simi-
lar embedding vectors, no matter they are in the
same language or not. On the other hand, cross-
lingual embeddings will benefit from different lan-
guages due to the complementary knowledge. For
instance, textual ambiguity in one language may
disappear in another language, e.g., the two mean-

1Some cross-lingual pioneering work observe that word
embeddings trained separately on monolingual corpora ex-
hibit isomorphic structure across languages (Mikolov et al.,
2013; Zhang et al., 2017).

1

000

001

002

003

004

005

006

007

008

009

010

011

012

013

014

015

016

017

018

019

020

021

022

023

024

025

026

027

028

029

030

031

032

033

034

035

036

037

038

039

040

041

042

043

044

045

046

047

048

049

050

051

052

053

054

055

056

057

058

059

060

061

062

063

064

065

066

067

068

069

070

071

072

073

074

075

076

077

078

079

080

081

082

083

084

085

086

087

088

089

090

091

092

093

094

095

096

097

098

099

ACL 2018 Submission ***. Confidential Review Copy. DO NOT DISTRIBUTE.

Joint Representation Learning of Cross-lingual Words and Entities via
Attentive Distant Supervision

Anonymous ACL submission

Abstract

Jointly learning word and entity represen-
tations benefits many NLP tasks, while has
not been well explored in cross-lingual set-
tings. In this paper, we propose a novel
method that integrates cross-lingual word
and entity representation learning to enable
joint inference among knowledge base and
text across languages, capturing mutually
complementary knowledge. Instead of re-
liance on parallel data, we automatically
generate cross-lingual training data via dis-
tant supervision over multi-lingual knowl-
edge bases. We also propose two types
of knowledge attention and cross-lingual
attention to select the most informative
words and filter out noise, which will fur-
ther improve the performance. In exper-
iments, separate tasks of word translation
and entity relatedness demonstrate the ef-
fectiveness of our method with an average
gain of 20% and 3% over baselines, re-
spectively. Using entity linking as a case
study, the results on benchmark dataset
verify the quality of our embeddings.

1 Introduction

Multi-lingual knowledge bases (KB), storing mil-
lions of entities and their facts in various lan-
guages, provide rich structured knowledge for un-
derstanding natural language beyond texts. Mean-
while, abundant text corpus contains large amount
of potential knowledge complementary to existing
KBs. Therefore, researchers leverage both types
of resources to improve various natural language
processing (NLP) related tasks, such as relation ex-
traction (Weston et al., 2013; Lin et al., 2017), and
entity linking (Tsai and Roth, 2016; Yamada et al.,
2016; Cao et al., 2017; Ji et al., 2016).

Most existing work jointly models KB and text
corpus to enhance each other by learning word and
entity representations in a unified vector space. For
example, (Wang et al., 2014; Yamada et al., 2016;
Cao et al., 2017) utilize the coherence informa-
tion to align similar words and entities with sim-
ilar embedding vectors. Another approach in (Han
et al., 2016; Toutanova et al., 2015; Wu et al.,
2016) learns to represent entities based on their
textual descriptions together with the structured re-
lations. However, these methods only focus on
mono-lingual settings, and few researches have
been done in cross-lingual scenarios.
In this paper, we propose to learn cross-lingual

word and entity representations in the same seman-
tic space, to enable joint inference among KB and
text across languages without any additional trans-
lation mechanism, which is usually expensive and
may introduce inevitable errors. Our embeddings
are helpful to break down language gaps in many
tasks, such as cross-lingual entity linking, in which
the major challenge lies in measuring the similar-
ity between entities and corresponding mentioned
words in different languages.
eNBA
The intuition is that, words and entities in

various languages share some common semantic
meanings1, but there are also ways in which they
differ. On one hand, we utilize their shared seman-
tics to align similar words and entities with simi-
lar embedding vectors, no matter they are in the
same language or not. On the other hand, cross-
lingual embeddings will benefit from different lan-
guages due to the complementary knowledge. For
instance, textual ambiguity in one language may
disappear in another language, e.g., the two mean-

1Some cross-lingual pioneering work observe that word
embeddings trained separately on monolingual corpora ex-
hibit isomorphic structure across languages (Mikolov et al.,
2013; Zhang et al., 2017).

En

eLawrence

wwas wan

eLawrence

eNBAeAllstarwan wbasketball

wAmerican

Figure 1: The framework of our method. The inputs and outputs of each step are listed in the right
side, and in the left side there are three main components of joint representation learning. Red texts
with brackets are anchors, dashed lines between entities denote relations, and solid lines are cross-lingual
links.

mon neighbors have similar embeddings, no mat-
ter in which language, e.g., English entity Foust
and Chinese entity 福斯特 (Foust) are embed-
ded close in semantic space due to the common
neighborsNBA,All-star andNBA选秀 (draft), etc.
(3) Cross-lingual Sentence Regularizer aims to
learn mutually translated words with similar em-
beddings by pushing their cross-lingual contexts
(i.e. comparable sentences) together. For exam-
ple, English word basketball and the translated
Chinese word 篮球 frequently co-occur in com-
parable sentences, and are close in the semantic
space.
All word/entity embeddings are trained jointly

under a unified optimization objective. Next, we
will introduce how to generate bi-lingual EN and
comparable sentences as well as the three compo-
nents for joint representation learning in turn.

3.3 Cross-lingual Supervision Data
Generation

This section introduces how to extract more cross-
lingual clues from multi-lingual KB in the form of
bi-lingual EN and comparable sentences.

Bi-lingual Entity Network Construction

Conventional knowledge representation meth-
ods normally regard cross-lingual links as a spe-
cial equivalence type of relation between two en-
tities (Zhu et al., 2017). However, we argue that
this may mislead to an inconsistent training ob-

jective since a cross-lingual link actually contains
multiple relations. For example (Figure 1), there
will be no direct relation betweenChinese entity福
斯特 (Foust) and English entity Piston by merely
adding the equivalence relation between Foust and
福斯特, which is in contradiction with the fact that
Foust belongs to Piston, no matter in which lan-
guage.
Therefore, we build bi-lingual entity network by

making cross-lingual linked entities that inherit all
relations from each other. Concretely, we enhance
mono-EN by adding edges from all neighbors of
entity eei to ezj if < eei , ezj >∈ Re−z (layer 2).
e

Comparable Sentences Generation

We utilize distant supervision to generate com-
parable sentences from Wikipedia articles. As
shown in Figure 1, from the page articles of cross-
lingual linked entities eeKobe and e

z
Kobe, we extract

those sentences including another cross-lingual
linked entities eeJoe and ezJoe as comparable sen-
tences Se−z = {< sek, szk >}.
The intuition is that we consider each sentence

in a Wikipedia article has a pseudo mention of
the page entity (talking something about the en-
tity) (Yamada et al., 2017). Thus, if a sentence also
mentions another entity, it implicitly expresses
their relation. Therefore, we make a similar as-
sumption as in relation extraction: If two enti-
ties participate in a relation, and both of them

4

300

301

302

303

304

305

306

307

308

309

310

311

312

313

314

315

316

317

318

319

320

321

322

323

324

325

326

327

328

329

330

331

332

333

334

335

336

337

338

339

340

341

342

343

344

345

346

347

348

349

350

351

352

353

354

355

356

357

358

359

360

361

362

363

364

365

366

367

368

369

370

371

372

373

374

375

376

377

378

379

380

381

382

383

384

385

386

387

388

389

390

391

392

393

394

395

396

397

398

399

ACL 2018 Submission ***. Confidential Review Copy. DO NOT DISTRIBUTE.

NBA

Lawrence Foust

All-star

[[Lawrence Michael Foust]] was
an American basketball player

who spent 12 seasons in [[NBA]]

NBA (zh)

·
[[ · ]] [[NBA]]

1950 [[NBA ]] 1 5

English KB Chinese KB

独立日 美国独立日

Sen Szh een ezh

L =
∑

xi ∈D

∑

xo ∈C(xi )
P(xo |xi)

L =
∑

(ei ,ej )∈Ec
| |C(ei) − C(e j)| |2

Ec

L =
∑

ei ∈E
P(N (ei)|ei) +

∑

(ei ,ej )∈Ec
P(N (e j)|ei)

N (ei)

L =
∑

(ei ,mi )∈A
P(ei |mi ,C(ei)) +

∑

(ei ,ej )∈Ec
P(e j |mi ,C(ei))

独立日 美国独立日

Sen Szh een ezh

L =
∑

xi ∈D

∑

xo ∈C(xi )
P(xo |xi)

L =
∑

(ei ,ej )∈Ec
| |C(ei) − C(e j)| |2

Ec

L =
∑

ei ∈E
P(N (ei)|ei) +

∑

(ei ,ej )∈Ec
P(N (e j)|ei)

N (ei)

L =
∑

(ei ,mi )∈A
P(ei |mi ,C(ei)) +

∑

(ei ,ej )∈Ec
P(e j |mi ,C(ei))

was

Lawrence
NBA

NBA (zh)

basketball

Semantic Space

Representation
Learning

Cross-lingual
Sentence Regularizer

Cross-lingual
Entity Regularizer

[[ · ]] [[NBA]]

All-star

[[Lawrence Michael Foust]] was an American basketball 
player who spent 12 seasons in [[NBA]]

·

NBA
NBA (zh)

Comparable Sentences

Bi-lingual EN

att
att

att

… …

Ls = | |Sen − Szh | |2

L =
∑

xi ∈D

∑

xo ∈C(xi )
P(xo |xi )

L =
∑

(ei ,ej )∈Ec
| |C(ei ) − C(ej )| |2

Ec

L =
∑

ei ∈E
P(N (ei )|ei ) +

∑

(ei ,ej )∈Ec
P(N (ej )|ei )

N (ei )

L =
∑

(ei ,mi )∈A
P(ei |mi ,C(ei )) +

∑

(ei ,ej )∈Ec
P(ej |mi ,C(ei ))

=

500

501

502

503

504

505

506

507

508

509

510

511

512

513

514

515

516

517

518

519

520

521

522

523

524

525

526

527

528

529

530

531

532

533

534

535

536

537

538

539

540

541

542

543

544

545

546

547

548

549

550

551

552

553

554

555

556

557

558

559

560

561

562

563

564

565

566

567

568

569

570

571

572

573

574

575

576

577

578

579

580

581

582

583

584

585

586

587

588

589

590

591

592

593

594

595

596

597

598

599

ACL 2018 Submission ***. Confidential Review Copy. DO NOT DISTRIBUTE.

skm ≈
wki +

∑
C(wki)

Ls =
∑

<skm,slm>∈Sk,l

|| − ||2

ekm
ekm

k

αnkm ∝ sim( ,
∑

wki∈snkm

)

αnkm
n skm

ekm αnkm

αki,lj ∝
wki∈skm,wlj∈slm

sim(wki, wlj)

=
∑

snkm∈skm

αnkm
∑

wki∈snkm

αki,lj

L = Lc + γLs

γ

400

401

402

403

404

405

406

407

408

409

410

411

412

413

414

415

416

417

418

419

420

421

422

423

424

425

426

427

428

429

430

431

432

433

434

435

436

437

438

439

440

441

442

443

444

445

446

447

448

449

450

451

452

453

454

455

456

457

458

459

460

461

462

463

464

465

466

467

468

469

470

471

472

473

474

475

476

477

478

479

480

481

482

483

484

485

486

487

488

489

490

491

492

493

494

495

496

497

498

499

ACL 2018 Submission ***. Confidential Review Copy. DO NOT DISTRIBUTE.

eki elj < eki, elj >∈ Rc

een,Kobe
ezh,Kobe

een,Joe ezh,Joe

< een,Kobe, ezh,Kobe >
< een,Joe, ezh,Joe >

D A
KN

Lc =
∑

xi∈{D,A,KN}

P (C(xi)|xi)

xi
C(xi)

C(xi)
xi

xi ∈ D
xi ∈ KN

wj xi
< wj , ei >∈ A

een,Joe wen,player
wen,NBA

een,LosAngelesLakers
een,Joe een,Kobe ezh,Kobe

wki
Cl(wki)

Ls =
∑

<skm,slm∈Sk,l

∑

wki∈skm

||wki −
∑

Cl(wki)||2

< skm, slm >∈ Sk,l m
k l

Cross-lingual
 supervision

Data Generation

and was an 8-time [[All-star]] … [[ ]] …

NBA

Lawrence Foust

NBA

player

American
NBA

All-star
eLawrence

wbasketballwplayer
… …

1

000

001

002

003

004

005

006

007

008

009

010

011

012

013

014

015

016

017

018

019

020

021

022

023

024

025

026

027

028

029

030

031

032

033

034

035

036

037

038

039

040

041

042

043

044

045

046

047

048

049

050

051

052

053

054

055

056

057

058

059

060

061

062

063

064

065

066

067

068

069

070

071

072

073

074

075

076

077

078

079

080

081

082

083

084

085

086

087

088

089

090

091

092

093

094

095

096

097

098

099

ACL 2018 Submission ***. Confidential Review Copy. DO NOT DISTRIBUTE.

Joint Representation Learning of Cross-lingual Words and Entities via
Attentive Distant Supervision

Anonymous ACL submission

Abstract

Jointly learning word and entity represen-
tations benefits many NLP tasks, while has
not been well explored in cross-lingual set-
tings. In this paper, we propose a novel
method that integrates cross-lingual word
and entity representation learning to enable
joint inference among knowledge base and
text across languages, capturing mutually
complementary knowledge. Instead of re-
liance on parallel data, we automatically
generate cross-lingual training data via dis-
tant supervision over multi-lingual knowl-
edge bases. We also propose two types
of knowledge attention and cross-lingual
attention to select the most informative
words and filter out noise, which will fur-
ther improve the performance. In exper-
iments, separate tasks of word translation
and entity relatedness demonstrate the ef-
fectiveness of our method with an average
gain of 20% and 3% over baselines, re-
spectively. Using entity linking as a case
study, the results on benchmark dataset
verify the quality of our embeddings.

1 Introduction

Multi-lingual knowledge bases (KB), storing mil-
lions of entities and their facts in various lan-
guages, provide rich structured knowledge for un-
derstanding natural language beyond texts. Mean-
while, abundant text corpus contains large amount
of potential knowledge complementary to existing
KBs. Therefore, researchers leverage both types
of resources to improve various natural language
processing (NLP) related tasks, such as relation ex-
traction (Weston et al., 2013; Lin et al., 2017), and
entity linking (Tsai and Roth, 2016; Yamada et al.,
2016; Cao et al., 2017; Ji et al., 2016).

Most existing work jointly models KB and text
corpus to enhance each other by learning word and
entity representations in a unified vector space. For
example, (Wang et al., 2014; Yamada et al., 2016;
Cao et al., 2017) utilize the coherence informa-
tion to align similar words and entities with sim-
ilar embedding vectors. Another approach in (Han
et al., 2016; Toutanova et al., 2015; Wu et al.,
2016) learns to represent entities based on their
textual descriptions together with the structured re-
lations. However, these methods only focus on
mono-lingual settings, and few researches have
been done in cross-lingual scenarios.
In this paper, we propose to learn cross-lingual

word and entity representations in the same seman-
tic space, to enable joint inference among KB and
text across languages without any additional trans-
lation mechanism, which is usually expensive and
may introduce inevitable errors. Our embeddings
are helpful to break down language gaps in many
tasks, such as cross-lingual entity linking, in which
the major challenge lies in measuring the similar-
ity between entities and corresponding mentioned
words in different languages.
e , w ,w
The intuition is that, words and entities in

various languages share some common semantic
meanings1, but there are also ways in which they
differ. On one hand, we utilize their shared seman-
tics to align similar words and entities with simi-
lar embedding vectors, no matter they are in the
same language or not. On the other hand, cross-
lingual embeddings will benefit from different lan-
guages due to the complementary knowledge. For
instance, textual ambiguity in one language may
disappear in another language, e.g., the two mean-

1Some cross-lingual pioneering work observe that word
embeddings trained separately on monolingual corpora ex-
hibit isomorphic structure across languages (Mikolov et al.,
2013; Zhang et al., 2017).

1

000

001

002

003

004

005

006

007

008

009

010

011

012

013

014

015

016

017

018

019

020

021

022

023

024

025

026

027

028

029

030

031

032

033

034

035

036

037

038

039

040

041

042

043

044

045

046

047

048

049

050

051

052

053

054

055

056

057

058

059

060

061

062

063

064

065

066

067

068

069

070

071

072

073

074

075

076

077

078

079

080

081

082

083

084

085

086

087

088

089

090

091

092

093

094

095

096

097

098

099

ACL 2018 Submission ***. Confidential Review Copy. DO NOT DISTRIBUTE.

Joint Representation Learning of Cross-lingual Words and Entities via
Attentive Distant Supervision

Anonymous ACL submission

Abstract

Jointly learning word and entity represen-
tations benefits many NLP tasks, while has
not been well explored in cross-lingual set-
tings. In this paper, we propose a novel
method that integrates cross-lingual word
and entity representation learning to enable
joint inference among knowledge base and
text across languages, capturing mutually
complementary knowledge. Instead of re-
liance on parallel data, we automatically
generate cross-lingual training data via dis-
tant supervision over multi-lingual knowl-
edge bases. We also propose two types
of knowledge attention and cross-lingual
attention to select the most informative
words and filter out noise, which will fur-
ther improve the performance. In exper-
iments, separate tasks of word translation
and entity relatedness demonstrate the ef-
fectiveness of our method with an average
gain of 20% and 3% over baselines, re-
spectively. Using entity linking as a case
study, the results on benchmark dataset
verify the quality of our embeddings.

1 Introduction

Multi-lingual knowledge bases (KB), storing mil-
lions of entities and their facts in various lan-
guages, provide rich structured knowledge for un-
derstanding natural language beyond texts. Mean-
while, abundant text corpus contains large amount
of potential knowledge complementary to existing
KBs. Therefore, researchers leverage both types
of resources to improve various natural language
processing (NLP) related tasks, such as relation ex-
traction (Weston et al., 2013; Lin et al., 2017), and
entity linking (Tsai and Roth, 2016; Yamada et al.,
2016; Cao et al., 2017; Ji et al., 2016).

Most existing work jointly models KB and text
corpus to enhance each other by learning word and
entity representations in a unified vector space. For
example, (Wang et al., 2014; Yamada et al., 2016;
Cao et al., 2017) utilize the coherence informa-
tion to align similar words and entities with sim-
ilar embedding vectors. Another approach in (Han
et al., 2016; Toutanova et al., 2015; Wu et al.,
2016) learns to represent entities based on their
textual descriptions together with the structured re-
lations. However, these methods only focus on
mono-lingual settings, and few researches have
been done in cross-lingual scenarios.
In this paper, we propose to learn cross-lingual

word and entity representations in the same seman-
tic space, to enable joint inference among KB and
text across languages without any additional trans-
lation mechanism, which is usually expensive and
may introduce inevitable errors. Our embeddings
are helpful to break down language gaps in many
tasks, such as cross-lingual entity linking, in which
the major challenge lies in measuring the similar-
ity between entities and corresponding mentioned
words in different languages.
e , w ,w
The intuition is that, words and entities in

various languages share some common semantic
meanings1, but there are also ways in which they
differ. On one hand, we utilize their shared seman-
tics to align similar words and entities with simi-
lar embedding vectors, no matter they are in the
same language or not. On the other hand, cross-
lingual embeddings will benefit from different lan-
guages due to the complementary knowledge. For
instance, textual ambiguity in one language may
disappear in another language, e.g., the two mean-

1Some cross-lingual pioneering work observe that word
embeddings trained separately on monolingual corpora ex-
hibit isomorphic structure across languages (Mikolov et al.,
2013; Zhang et al., 2017).

1

000

001

002

003

004

005

006

007

008

009

010

011

012

013

014

015

016

017

018

019

020

021

022

023

024

025

026

027

028

029

030

031

032

033

034

035

036

037

038

039

040

041

042

043

044

045

046

047

048

049

050

051

052

053

054

055

056

057

058

059

060

061

062

063

064

065

066

067

068

069

070

071

072

073

074

075

076

077

078

079

080

081

082

083

084

085

086

087

088

089

090

091

092

093

094

095

096

097

098

099

ACL 2018 Submission ***. Confidential Review Copy. DO NOT DISTRIBUTE.

Joint Representation Learning of Cross-lingual Words and Entities via
Attentive Distant Supervision

Anonymous ACL submission

Abstract

Jointly learning word and entity represen-
tations benefits many NLP tasks, while has
not been well explored in cross-lingual set-
tings. In this paper, we propose a novel
method that integrates cross-lingual word
and entity representation learning to enable
joint inference among knowledge base and
text across languages, capturing mutually
complementary knowledge. Instead of re-
liance on parallel data, we automatically
generate cross-lingual training data via dis-
tant supervision over multi-lingual knowl-
edge bases. We also propose two types
of knowledge attention and cross-lingual
attention to select the most informative
words and filter out noise, which will fur-
ther improve the performance. In exper-
iments, separate tasks of word translation
and entity relatedness demonstrate the ef-
fectiveness of our method with an average
gain of 20% and 3% over baselines, re-
spectively. Using entity linking as a case
study, the results on benchmark dataset
verify the quality of our embeddings.

1 Introduction

Multi-lingual knowledge bases (KB), storing mil-
lions of entities and their facts in various lan-
guages, provide rich structured knowledge for un-
derstanding natural language beyond texts. Mean-
while, abundant text corpus contains large amount
of potential knowledge complementary to existing
KBs. Therefore, researchers leverage both types
of resources to improve various natural language
processing (NLP) related tasks, such as relation ex-
traction (Weston et al., 2013; Lin et al., 2017), and
entity linking (Tsai and Roth, 2016; Yamada et al.,
2016; Cao et al., 2017; Ji et al., 2016).

Most existing work jointly models KB and text
corpus to enhance each other by learning word and
entity representations in a unified vector space. For
example, (Wang et al., 2014; Yamada et al., 2016;
Cao et al., 2017) utilize the coherence informa-
tion to align similar words and entities with sim-
ilar embedding vectors. Another approach in (Han
et al., 2016; Toutanova et al., 2015; Wu et al.,
2016) learns to represent entities based on their
textual descriptions together with the structured re-
lations. However, these methods only focus on
mono-lingual settings, and few researches have
been done in cross-lingual scenarios.
In this paper, we propose to learn cross-lingual

word and entity representations in the same seman-
tic space, to enable joint inference among KB and
text across languages without any additional trans-
lation mechanism, which is usually expensive and
may introduce inevitable errors. Our embeddings
are helpful to break down language gaps in many
tasks, such as cross-lingual entity linking, in which
the major challenge lies in measuring the similar-
ity between entities and corresponding mentioned
words in different languages.
e , w ,w
The intuition is that, words and entities in

various languages share some common semantic
meanings1, but there are also ways in which they
differ. On one hand, we utilize their shared seman-
tics to align similar words and entities with simi-
lar embedding vectors, no matter they are in the
same language or not. On the other hand, cross-
lingual embeddings will benefit from different lan-
guages due to the complementary knowledge. For
instance, textual ambiguity in one language may
disappear in another language, e.g., the two mean-

1Some cross-lingual pioneering work observe that word
embeddings trained separately on monolingual corpora ex-
hibit isomorphic structure across languages (Mikolov et al.,
2013; Zhang et al., 2017).

1

000

001

002

003

004

005

006

007

008

009

010

011

012

013

014

015

016

017

018

019

020

021

022

023

024

025

026

027

028

029

030

031

032

033

034

035

036

037

038

039

040

041

042

043

044

045

046

047

048

049

050

051

052

053

054

055

056

057

058

059

060

061

062

063

064

065

066

067

068

069

070

071

072

073

074

075

076

077

078

079

080

081

082

083

084

085

086

087

088

089

090

091

092

093

094

095

096

097

098

099

ACL 2018 Submission ***. Confidential Review Copy. DO NOT DISTRIBUTE.

Joint Representation Learning of Cross-lingual Words and Entities via
Attentive Distant Supervision

Anonymous ACL submission

Abstract

Jointly learning word and entity represen-
tations benefits many NLP tasks, while has
not been well explored in cross-lingual set-
tings. In this paper, we propose a novel
method that integrates cross-lingual word
and entity representation learning to enable
joint inference among knowledge base and
text across languages, capturing mutually
complementary knowledge. Instead of re-
liance on parallel data, we automatically
generate cross-lingual training data via dis-
tant supervision over multi-lingual knowl-
edge bases. We also propose two types
of knowledge attention and cross-lingual
attention to select the most informative
words and filter out noise, which will fur-
ther improve the performance. In exper-
iments, separate tasks of word translation
and entity relatedness demonstrate the ef-
fectiveness of our method with an average
gain of 20% and 3% over baselines, re-
spectively. Using entity linking as a case
study, the results on benchmark dataset
verify the quality of our embeddings.

1 Introduction

Multi-lingual knowledge bases (KB), storing mil-
lions of entities and their facts in various lan-
guages, provide rich structured knowledge for un-
derstanding natural language beyond texts. Mean-
while, abundant text corpus contains large amount
of potential knowledge complementary to existing
KBs. Therefore, researchers leverage both types
of resources to improve various natural language
processing (NLP) related tasks, such as relation ex-
traction (Weston et al., 2013; Lin et al., 2017), and
entity linking (Tsai and Roth, 2016; Yamada et al.,
2016; Cao et al., 2017; Ji et al., 2016).

Most existing work jointly models KB and text
corpus to enhance each other by learning word and
entity representations in a unified vector space. For
example, (Wang et al., 2014; Yamada et al., 2016;
Cao et al., 2017) utilize the coherence informa-
tion to align similar words and entities with sim-
ilar embedding vectors. Another approach in (Han
et al., 2016; Toutanova et al., 2015; Wu et al.,
2016) learns to represent entities based on their
textual descriptions together with the structured re-
lations. However, these methods only focus on
mono-lingual settings, and few researches have
been done in cross-lingual scenarios.
In this paper, we propose to learn cross-lingual

word and entity representations in the same seman-
tic space, to enable joint inference among KB and
text across languages without any additional trans-
lation mechanism, which is usually expensive and
may introduce inevitable errors. Our embeddings
are helpful to break down language gaps in many
tasks, such as cross-lingual entity linking, in which
the major challenge lies in measuring the similar-
ity between entities and corresponding mentioned
words in different languages.
e , w ,w
The intuition is that, words and entities in

various languages share some common semantic
meanings1, but there are also ways in which they
differ. On one hand, we utilize their shared seman-
tics to align similar words and entities with simi-
lar embedding vectors, no matter they are in the
same language or not. On the other hand, cross-
lingual embeddings will benefit from different lan-
guages due to the complementary knowledge. For
instance, textual ambiguity in one language may
disappear in another language, e.g., the two mean-

1Some cross-lingual pioneering work observe that word
embeddings trained separately on monolingual corpora ex-
hibit isomorphic structure across languages (Mikolov et al.,
2013; Zhang et al., 2017).

eNBAeAllstar

Mono-lingual Representation Learning

Zh

1

000

001

002

003

004

005

006

007

008

009

010

011

012

013

014

015

016

017

018

019

020

021

022

023

024

025

026

027

028

029

030

031

032

033

034

035

036

037

038

039

040

041

042

043

044

045

046

047

048

049

050

051

052

053

054

055

056

057

058

059

060

061

062

063

064

065

066

067

068

069

070

071

072

073

074

075

076

077

078

079

080

081

082

083

084

085

086

087

088

089

090

091

092

093

094

095

096

097

098

099

ACL 2018 Submission ***. Confidential Review Copy. DO NOT DISTRIBUTE.

Joint Representation Learning of Cross-lingual Words and Entities via
Attentive Distant Supervision

Anonymous ACL submission

Abstract

Jointly learning word and entity represen-
tations benefits many NLP tasks, while has
not been well explored in cross-lingual set-
tings. In this paper, we propose a novel
method that integrates cross-lingual word
and entity representation learning to enable
joint inference among knowledge base and
text across languages, capturing mutually
complementary knowledge. Instead of re-
liance on parallel data, we automatically
generate cross-lingual training data via dis-
tant supervision over multi-lingual knowl-
edge bases. We also propose two types
of knowledge attention and cross-lingual
attention to select the most informative
words and filter out noise, which will fur-
ther improve the performance. In exper-
iments, separate tasks of word translation
and entity relatedness demonstrate the ef-
fectiveness of our method with an average
gain of 20% and 3% over baselines, re-
spectively. Using entity linking as a case
study, the results on benchmark dataset
verify the quality of our embeddings.

1 Introduction

Multi-lingual knowledge bases (KB), storing mil-
lions of entities and their facts in various lan-
guages, provide rich structured knowledge for un-
derstanding natural language beyond texts. Mean-
while, abundant text corpus contains large amount
of potential knowledge complementary to existing
KBs. Therefore, researchers leverage both types
of resources to improve various natural language
processing (NLP) related tasks, such as relation ex-
traction (Weston et al., 2013; Lin et al., 2017), and
entity linking (Tsai and Roth, 2016; Yamada et al.,
2016; Cao et al., 2017; Ji et al., 2016).

Most existing work jointly models KB and text
corpus to enhance each other by learning word and
entity representations in a unified vector space. For
example, (Wang et al., 2014; Yamada et al., 2016;
Cao et al., 2017) utilize the coherence informa-
tion to align similar words and entities with sim-
ilar embedding vectors. Another approach in (Han
et al., 2016; Toutanova et al., 2015; Wu et al.,
2016) learns to represent entities based on their
textual descriptions together with the structured re-
lations. However, these methods only focus on
mono-lingual settings, and few researches have
been done in cross-lingual scenarios.
In this paper, we propose to learn cross-lingual

word and entity representations in the same seman-
tic space, to enable joint inference among KB and
text across languages without any additional trans-
lation mechanism, which is usually expensive and
may introduce inevitable errors. Our embeddings
are helpful to break down language gaps in many
tasks, such as cross-lingual entity linking, in which
the major challenge lies in measuring the similar-
ity between entities and corresponding mentioned
words in different languages.
e , w ,w
The intuition is that, words and entities in

various languages share some common semantic
meanings1, but there are also ways in which they
differ. On one hand, we utilize their shared seman-
tics to align similar words and entities with simi-
lar embedding vectors, no matter they are in the
same language or not. On the other hand, cross-
lingual embeddings will benefit from different lan-
guages due to the complementary knowledge. For
instance, textual ambiguity in one language may
disappear in another language, e.g., the two mean-

1Some cross-lingual pioneering work observe that word
embeddings trained separately on monolingual corpora ex-
hibit isomorphic structure across languages (Mikolov et al.,
2013; Zhang et al., 2017).

1

000

001

002

003

004

005

006

007

008

009

010

011

012

013

014

015

016

017

018

019

020

021

022

023

024

025

026

027

028

029

030

031

032

033

034

035

036

037

038

039

040

041

042

043

044

045

046

047

048

049

050

051

052

053

054

055

056

057

058

059

060

061

062

063

064

065

066

067

068

069

070

071

072

073

074

075

076

077

078

079

080

081

082

083

084

085

086

087

088

089

090

091

092

093

094

095

096

097

098

099

ACL 2018 Submission ***. Confidential Review Copy. DO NOT DISTRIBUTE.

Joint Representation Learning of Cross-lingual Words and Entities via
Attentive Distant Supervision

Anonymous ACL submission

Abstract

Jointly learning word and entity represen-
tations benefits many NLP tasks, while has
not been well explored in cross-lingual set-
tings. In this paper, we propose a novel
method that integrates cross-lingual word
and entity representation learning to enable
joint inference among knowledge base and
text across languages, capturing mutually
complementary knowledge. Instead of re-
liance on parallel data, we automatically
generate cross-lingual training data via dis-
tant supervision over multi-lingual knowl-
edge bases. We also propose two types
of knowledge attention and cross-lingual
attention to select the most informative
words and filter out noise, which will fur-
ther improve the performance. In exper-
iments, separate tasks of word translation
and entity relatedness demonstrate the ef-
fectiveness of our method with an average
gain of 20% and 3% over baselines, re-
spectively. Using entity linking as a case
study, the results on benchmark dataset
verify the quality of our embeddings.

1 Introduction

Multi-lingual knowledge bases (KB), storing mil-
lions of entities and their facts in various lan-
guages, provide rich structured knowledge for un-
derstanding natural language beyond texts. Mean-
while, abundant text corpus contains large amount
of potential knowledge complementary to existing
KBs. Therefore, researchers leverage both types
of resources to improve various natural language
processing (NLP) related tasks, such as relation ex-
traction (Weston et al., 2013; Lin et al., 2017), and
entity linking (Tsai and Roth, 2016; Yamada et al.,
2016; Cao et al., 2017; Ji et al., 2016).

Most existing work jointly models KB and text
corpus to enhance each other by learning word and
entity representations in a unified vector space. For
example, (Wang et al., 2014; Yamada et al., 2016;
Cao et al., 2017) utilize the coherence informa-
tion to align similar words and entities with sim-
ilar embedding vectors. Another approach in (Han
et al., 2016; Toutanova et al., 2015; Wu et al.,
2016) learns to represent entities based on their
textual descriptions together with the structured re-
lations. However, these methods only focus on
mono-lingual settings, and few researches have
been done in cross-lingual scenarios.
In this paper, we propose to learn cross-lingual

word and entity representations in the same seman-
tic space, to enable joint inference among KB and
text across languages without any additional trans-
lation mechanism, which is usually expensive and
may introduce inevitable errors. Our embeddings
are helpful to break down language gaps in many
tasks, such as cross-lingual entity linking, in which
the major challenge lies in measuring the similar-
ity between entities and corresponding mentioned
words in different languages.
eNBA , w , w ,w
The intuition is that, words and entities in

various languages share some common semantic
meanings1, but there are also ways in which they
differ. On one hand, we utilize their shared seman-
tics to align similar words and entities with simi-
lar embedding vectors, no matter they are in the
same language or not. On the other hand, cross-
lingual embeddings will benefit from different lan-
guages due to the complementary knowledge. For
instance, textual ambiguity in one language may
disappear in another language, e.g., the two mean-

1Some cross-lingual pioneering work observe that word
embeddings trained separately on monolingual corpora ex-
hibit isomorphic structure across languages (Mikolov et al.,
2013; Zhang et al., 2017).

1

000

001

002

003

004

005

006

007

008

009

010

011

012

013

014

015

016

017

018

019

020

021

022

023

024

025

026

027

028

029

030

031

032

033

034

035

036

037

038

039

040

041

042

043

044

045

046

047

048

049

050

051

052

053

054

055

056

057

058

059

060

061

062

063

064

065

066

067

068

069

070

071

072

073

074

075

076

077

078

079

080

081

082

083

084

085

086

087

088

089

090

091

092

093

094

095

096

097

098

099

ACL 2018 Submission ***. Confidential Review Copy. DO NOT DISTRIBUTE.

Joint Representation Learning of Cross-lingual Words and Entities via
Attentive Distant Supervision

Anonymous ACL submission

Abstract

Jointly learning word and entity represen-
tations benefits many NLP tasks, while has
not been well explored in cross-lingual set-
tings. In this paper, we propose a novel
method that integrates cross-lingual word
and entity representation learning to enable
joint inference among knowledge base and
text across languages, capturing mutually
complementary knowledge. Instead of re-
liance on parallel data, we automatically
generate cross-lingual training data via dis-
tant supervision over multi-lingual knowl-
edge bases. We also propose two types
of knowledge attention and cross-lingual
attention to select the most informative
words and filter out noise, which will fur-
ther improve the performance. In exper-
iments, separate tasks of word translation
and entity relatedness demonstrate the ef-
fectiveness of our method with an average
gain of 20% and 3% over baselines, re-
spectively. Using entity linking as a case
study, the results on benchmark dataset
verify the quality of our embeddings.

1 Introduction

Multi-lingual knowledge bases (KB), storing mil-
lions of entities and their facts in various lan-
guages, provide rich structured knowledge for un-
derstanding natural language beyond texts. Mean-
while, abundant text corpus contains large amount
of potential knowledge complementary to existing
KBs. Therefore, researchers leverage both types
of resources to improve various natural language
processing (NLP) related tasks, such as relation ex-
traction (Weston et al., 2013; Lin et al., 2017), and
entity linking (Tsai and Roth, 2016; Yamada et al.,
2016; Cao et al., 2017; Ji et al., 2016).

Most existing work jointly models KB and text
corpus to enhance each other by learning word and
entity representations in a unified vector space. For
example, (Wang et al., 2014; Yamada et al., 2016;
Cao et al., 2017) utilize the coherence informa-
tion to align similar words and entities with sim-
ilar embedding vectors. Another approach in (Han
et al., 2016; Toutanova et al., 2015; Wu et al.,
2016) learns to represent entities based on their
textual descriptions together with the structured re-
lations. However, these methods only focus on
mono-lingual settings, and few researches have
been done in cross-lingual scenarios.
In this paper, we propose to learn cross-lingual

word and entity representations in the same seman-
tic space, to enable joint inference among KB and
text across languages without any additional trans-
lation mechanism, which is usually expensive and
may introduce inevitable errors. Our embeddings
are helpful to break down language gaps in many
tasks, such as cross-lingual entity linking, in which
the major challenge lies in measuring the similar-
ity between entities and corresponding mentioned
words in different languages.
eNBA , w , w ,w
The intuition is that, words and entities in

various languages share some common semantic
meanings1, but there are also ways in which they
differ. On one hand, we utilize their shared seman-
tics to align similar words and entities with simi-
lar embedding vectors, no matter they are in the
same language or not. On the other hand, cross-
lingual embeddings will benefit from different lan-
guages due to the complementary knowledge. For
instance, textual ambiguity in one language may
disappear in another language, e.g., the two mean-

1Some cross-lingual pioneering work observe that word
embeddings trained separately on monolingual corpora ex-
hibit isomorphic structure across languages (Mikolov et al.,
2013; Zhang et al., 2017).

1

000

001

002

003

004

005

006

007

008

009

010

011

012

013

014

015

016

017

018

019

020

021

022

023

024

025

026

027

028

029

030

031

032

033

034

035

036

037

038

039

040

041

042

043

044

045

046

047

048

049

050

051

052

053

054

055

056

057

058

059

060

061

062

063

064

065

066

067

068

069

070

071

072

073

074

075

076

077

078

079

080

081

082

083

084

085

086

087

088

089

090

091

092

093

094

095

096

097

098

099

ACL 2018 Submission ***. Confidential Review Copy. DO NOT DISTRIBUTE.

Joint Representation Learning of Cross-lingual Words and Entities via
Attentive Distant Supervision

Anonymous ACL submission

Abstract

Jointly learning word and entity represen-
tations benefits many NLP tasks, while has
not been well explored in cross-lingual set-
tings. In this paper, we propose a novel
method that integrates cross-lingual word
and entity representation learning to enable
joint inference among knowledge base and
text across languages, capturing mutually
complementary knowledge. Instead of re-
liance on parallel data, we automatically
generate cross-lingual training data via dis-
tant supervision over multi-lingual knowl-
edge bases. We also propose two types
of knowledge attention and cross-lingual
attention to select the most informative
words and filter out noise, which will fur-
ther improve the performance. In exper-
iments, separate tasks of word translation
and entity relatedness demonstrate the ef-
fectiveness of our method with an average
gain of 20% and 3% over baselines, re-
spectively. Using entity linking as a case
study, the results on benchmark dataset
verify the quality of our embeddings.

1 Introduction

Multi-lingual knowledge bases (KB), storing mil-
lions of entities and their facts in various lan-
guages, provide rich structured knowledge for un-
derstanding natural language beyond texts. Mean-
while, abundant text corpus contains large amount
of potential knowledge complementary to existing
KBs. Therefore, researchers leverage both types
of resources to improve various natural language
processing (NLP) related tasks, such as relation ex-
traction (Weston et al., 2013; Lin et al., 2017), and
entity linking (Tsai and Roth, 2016; Yamada et al.,
2016; Cao et al., 2017; Ji et al., 2016).

Most existing work jointly models KB and text
corpus to enhance each other by learning word and
entity representations in a unified vector space. For
example, (Wang et al., 2014; Yamada et al., 2016;
Cao et al., 2017) utilize the coherence informa-
tion to align similar words and entities with sim-
ilar embedding vectors. Another approach in (Han
et al., 2016; Toutanova et al., 2015; Wu et al.,
2016) learns to represent entities based on their
textual descriptions together with the structured re-
lations. However, these methods only focus on
mono-lingual settings, and few researches have
been done in cross-lingual scenarios.
In this paper, we propose to learn cross-lingual

word and entity representations in the same seman-
tic space, to enable joint inference among KB and
text across languages without any additional trans-
lation mechanism, which is usually expensive and
may introduce inevitable errors. Our embeddings
are helpful to break down language gaps in many
tasks, such as cross-lingual entity linking, in which
the major challenge lies in measuring the similar-
ity between entities and corresponding mentioned
words in different languages.
eNBA , w , w ,w
The intuition is that, words and entities in

various languages share some common semantic
meanings1, but there are also ways in which they
differ. On one hand, we utilize their shared seman-
tics to align similar words and entities with simi-
lar embedding vectors, no matter they are in the
same language or not. On the other hand, cross-
lingual embeddings will benefit from different lan-
guages due to the complementary knowledge. For
instance, textual ambiguity in one language may
disappear in another language, e.g., the two mean-

1Some cross-lingual pioneering work observe that word
embeddings trained separately on monolingual corpora ex-
hibit isomorphic structure across languages (Mikolov et al.,
2013; Zhang et al., 2017).

1

000

001

002

003

004

005

006

007

008

009

010

011

012

013

014

015

016

017

018

019

020

021

022

023

024

025

026

027

028

029

030

031

032

033

034

035

036

037

038

039

040

041

042

043

044

045

046

047

048

049

050

051

052

053

054

055

056

057

058

059

060

061

062

063

064

065

066

067

068

069

070

071

072

073

074

075

076

077

078

079

080

081

082

083

084

085

086

087

088

089

090

091

092

093

094

095

096

097

098

099

ACL 2018 Submission ***. Confidential Review Copy. DO NOT DISTRIBUTE.

Joint Representation Learning of Cross-lingual Words and Entities via
Attentive Distant Supervision

Anonymous ACL submission

Abstract

Jointly learning word and entity represen-
tations benefits many NLP tasks, while has
not been well explored in cross-lingual set-
tings. In this paper, we propose a novel
method that integrates cross-lingual word
and entity representation learning to enable
joint inference among knowledge base and
text across languages, capturing mutually
complementary knowledge. Instead of re-
liance on parallel data, we automatically
generate cross-lingual training data via dis-
tant supervision over multi-lingual knowl-
edge bases. We also propose two types
of knowledge attention and cross-lingual
attention to select the most informative
words and filter out noise, which will fur-
ther improve the performance. In exper-
iments, separate tasks of word translation
and entity relatedness demonstrate the ef-
fectiveness of our method with an average
gain of 20% and 3% over baselines, re-
spectively. Using entity linking as a case
study, the results on benchmark dataset
verify the quality of our embeddings.

1 Introduction

Multi-lingual knowledge bases (KB), storing mil-
lions of entities and their facts in various lan-
guages, provide rich structured knowledge for un-
derstanding natural language beyond texts. Mean-
while, abundant text corpus contains large amount
of potential knowledge complementary to existing
KBs. Therefore, researchers leverage both types
of resources to improve various natural language
processing (NLP) related tasks, such as relation ex-
traction (Weston et al., 2013; Lin et al., 2017), and
entity linking (Tsai and Roth, 2016; Yamada et al.,
2016; Cao et al., 2017; Ji et al., 2016).

Most existing work jointly models KB and text
corpus to enhance each other by learning word and
entity representations in a unified vector space. For
example, (Wang et al., 2014; Yamada et al., 2016;
Cao et al., 2017) utilize the coherence informa-
tion to align similar words and entities with sim-
ilar embedding vectors. Another approach in (Han
et al., 2016; Toutanova et al., 2015; Wu et al.,
2016) learns to represent entities based on their
textual descriptions together with the structured re-
lations. However, these methods only focus on
mono-lingual settings, and few researches have
been done in cross-lingual scenarios.
In this paper, we propose to learn cross-lingual

word and entity representations in the same seman-
tic space, to enable joint inference among KB and
text across languages without any additional trans-
lation mechanism, which is usually expensive and
may introduce inevitable errors. Our embeddings
are helpful to break down language gaps in many
tasks, such as cross-lingual entity linking, in which
the major challenge lies in measuring the similar-
ity between entities and corresponding mentioned
words in different languages.
eNBA , w , w ,w
The intuition is that, words and entities in

various languages share some common semantic
meanings1, but there are also ways in which they
differ. On one hand, we utilize their shared seman-
tics to align similar words and entities with simi-
lar embedding vectors, no matter they are in the
same language or not. On the other hand, cross-
lingual embeddings will benefit from different lan-
guages due to the complementary knowledge. For
instance, textual ambiguity in one language may
disappear in another language, e.g., the two mean-

1Some cross-lingual pioneering work observe that word
embeddings trained separately on monolingual corpora ex-
hibit isomorphic structure across languages (Mikolov et al.,
2013; Zhang et al., 2017).

1

000

001

002

003

004

005

006

007

008

009

010

011

012

013

014

015

016

017

018

019

020

021

022

023

024

025

026

027

028

029

030

031

032

033

034

035

036

037

038

039

040

041

042

043

044

045

046

047

048

049

050

051

052

053

054

055

056

057

058

059

060

061

062

063

064

065

066

067

068

069

070

071

072

073

074

075

076

077

078

079

080

081

082

083

084

085

086

087

088

089

090

091

092

093

094

095

096

097

098

099

ACL 2018 Submission ***. Confidential Review Copy. DO NOT DISTRIBUTE.

Joint Representation Learning of Cross-lingual Words and Entities via
Attentive Distant Supervision

Anonymous ACL submission

Abstract

Jointly learning word and entity represen-
tations benefits many NLP tasks, while has
not been well explored in cross-lingual set-
tings. In this paper, we propose a novel
method that integrates cross-lingual word
and entity representation learning to enable
joint inference among knowledge base and
text across languages, capturing mutually
complementary knowledge. Instead of re-
liance on parallel data, we automatically
generate cross-lingual training data via dis-
tant supervision over multi-lingual knowl-
edge bases. We also propose two types
of knowledge attention and cross-lingual
attention to select the most informative
words and filter out noise, which will fur-
ther improve the performance. In exper-
iments, separate tasks of word translation
and entity relatedness demonstrate the ef-
fectiveness of our method with an average
gain of 20% and 3% over baselines, re-
spectively. Using entity linking as a case
study, the results on benchmark dataset
verify the quality of our embeddings.

1 Introduction

Multi-lingual knowledge bases (KB), storing mil-
lions of entities and their facts in various lan-
guages, provide rich structured knowledge for un-
derstanding natural language beyond texts. Mean-
while, abundant text corpus contains large amount
of potential knowledge complementary to existing
KBs. Therefore, researchers leverage both types
of resources to improve various natural language
processing (NLP) related tasks, such as relation ex-
traction (Weston et al., 2013; Lin et al., 2017), and
entity linking (Tsai and Roth, 2016; Yamada et al.,
2016; Cao et al., 2017; Ji et al., 2016).

Most existing work jointly models KB and text
corpus to enhance each other by learning word and
entity representations in a unified vector space. For
example, (Wang et al., 2014; Yamada et al., 2016;
Cao et al., 2017) utilize the coherence informa-
tion to align similar words and entities with sim-
ilar embedding vectors. Another approach in (Han
et al., 2016; Toutanova et al., 2015; Wu et al.,
2016) learns to represent entities based on their
textual descriptions together with the structured re-
lations. However, these methods only focus on
mono-lingual settings, and few researches have
been done in cross-lingual scenarios.
In this paper, we propose to learn cross-lingual

word and entity representations in the same seman-
tic space, to enable joint inference among KB and
text across languages without any additional trans-
lation mechanism, which is usually expensive and
may introduce inevitable errors. Our embeddings
are helpful to break down language gaps in many
tasks, such as cross-lingual entity linking, in which
the major challenge lies in measuring the similar-
ity between entities and corresponding mentioned
words in different languages.
eNBA
The intuition is that, words and entities in

various languages share some common semantic
meanings1, but there are also ways in which they
differ. On one hand, we utilize their shared seman-
tics to align similar words and entities with simi-
lar embedding vectors, no matter they are in the
same language or not. On the other hand, cross-
lingual embeddings will benefit from different lan-
guages due to the complementary knowledge. For
instance, textual ambiguity in one language may
disappear in another language, e.g., the two mean-

1Some cross-lingual pioneering work observe that word
embeddings trained separately on monolingual corpora ex-
hibit isomorphic structure across languages (Mikolov et al.,
2013; Zhang et al., 2017).

En

eLawrence

wwas wan

eLawrence

eNBAeAllstarwan wbasketball

wAmerican

Figure 1: The framework of our method. The inputs and outputs of each step are listed in the right
side, and in the left side there are three main components of joint representation learning. Red texts
with brackets are anchors, dashed lines between entities denote relations, and solid lines are cross-lingual
links.

mon neighbors have similar embeddings, no mat-
ter in which language, e.g., English entity Foust
and Chinese entity 福斯特 (Foust) are embed-
ded close in semantic space due to the common
neighborsNBA,All-star andNBA选秀 (draft), etc.
(3) Cross-lingual Sentence Regularizer aims to
learn mutually translated words with similar em-
beddings by pushing their cross-lingual contexts
(i.e. comparable sentences) together. For exam-
ple, English word basketball and the translated
Chinese word 篮球 frequently co-occur in com-
parable sentences, and are close in the semantic
space.
All word/entity embeddings are trained jointly

under a unified optimization objective. Next, we
will introduce how to generate bi-lingual EN and
comparable sentences as well as the three compo-
nents for joint representation learning in turn.

3.3 Cross-lingual Supervision Data
Generation

This section introduces how to extract more cross-
lingual clues from multi-lingual KB in the form of
bi-lingual EN and comparable sentences.

Bi-lingual Entity Network Construction

Conventional knowledge representation meth-
ods normally regard cross-lingual links as a spe-
cial equivalence type of relation between two en-
tities (Zhu et al., 2017). However, we argue that
this may mislead to an inconsistent training ob-

jective since a cross-lingual link actually contains
multiple relations. For example (Figure 1), there
will be no direct relation betweenChinese entity福
斯特 (Foust) and English entity Piston by merely
adding the equivalence relation between Foust and
福斯特, which is in contradiction with the fact that
Foust belongs to Piston, no matter in which lan-
guage.
Therefore, we build bi-lingual entity network by

making cross-lingual linked entities that inherit all
relations from each other. Concretely, we enhance
mono-EN by adding edges from all neighbors of
entity eei to ezj if < eei , ezj >∈ Re−z (layer 2).
e

Comparable Sentences Generation

We utilize distant supervision to generate com-
parable sentences from Wikipedia articles. As
shown in Figure 1, from the page articles of cross-
lingual linked entities eeKobe and e

z
Kobe, we extract

those sentences including another cross-lingual
linked entities eeJoe and ezJoe as comparable sen-
tences Se−z = {< sek, szk >}.
The intuition is that we consider each sentence

in a Wikipedia article has a pseudo mention of
the page entity (talking something about the en-
tity) (Yamada et al., 2017). Thus, if a sentence also
mentions another entity, it implicitly expresses
their relation. Therefore, we make a similar as-
sumption as in relation extraction: If two enti-
ties participate in a relation, and both of them

4

300

301

302

303

304

305

306

307

308

309

310

311

312

313

314

315

316

317

318

319

320

321

322

323

324

325

326

327

328

329

330

331

332

333

334

335

336

337

338

339

340

341

342

343

344

345

346

347

348

349

350

351

352

353

354

355

356

357

358

359

360

361

362

363

364

365

366

367

368

369

370

371

372

373

374

375

376

377

378

379

380

381

382

383

384

385

386

387

388

389

390

391

392

393

394

395

396

397

398

399

ACL 2018 Submission ***. Confidential Review Copy. DO NOT DISTRIBUTE.

NBA

Lawrence Foust

All-star

[[Lawrence Michael Foust]] was
an American basketball player

who spent 12 seasons in [[NBA]]

NBA (zh)

·
[[ · ]] [[NBA]]

1950 [[NBA ]] 1 5

English KB Chinese KB

独立日 美国独立日

Sen Szh een ezh

L =
∑

xi ∈D

∑

xo ∈C(xi )
P(xo |xi)

L =
∑

(ei ,ej )∈Ec
| |C(ei) − C(e j)| |2

Ec

L =
∑

ei ∈E
P(N (ei)|ei) +

∑

(ei ,ej )∈Ec
P(N (e j)|ei)

N (ei)

L =
∑

(ei ,mi )∈A
P(ei |mi ,C(ei)) +

∑

(ei ,ej )∈Ec
P(e j |mi ,C(ei))

独立日 美国独立日

Sen Szh een ezh

L =
∑

xi ∈D

∑

xo ∈C(xi )
P(xo |xi)

L =
∑

(ei ,ej )∈Ec
| |C(ei) − C(e j)| |2

Ec

L =
∑

ei ∈E
P(N (ei)|ei) +

∑

(ei ,ej )∈Ec
P(N (e j)|ei)

N (ei)

L =
∑

(ei ,mi )∈A
P(ei |mi ,C(ei)) +

∑

(ei ,ej )∈Ec
P(e j |mi ,C(ei))

was

Lawrence
NBA

NBA (zh)

basketball

Semantic Space

Representation
Learning

Cross-lingual
Sentence Regularizer

Cross-lingual
Entity Regularizer

[[ · ]] [[NBA]]

All-star

[[Lawrence Michael Foust]] was an American basketball 
player who spent 12 seasons in [[NBA]]

·

NBA
NBA (zh)

Comparable Sentences

Bi-lingual EN

att
att

att

… …

Ls = | |Sen − Szh | |2

L =
∑

xi ∈D

∑

xo ∈C(xi )
P(xo |xi )

L =
∑

(ei ,ej )∈Ec
| |C(ei ) − C(ej )| |2

Ec

L =
∑

ei ∈E
P(N (ei )|ei ) +

∑

(ei ,ej )∈Ec
P(N (ej )|ei )

N (ei )

L =
∑

(ei ,mi )∈A
P(ei |mi ,C(ei )) +

∑

(ei ,ej )∈Ec
P(ej |mi ,C(ei ))

=

500

501

502

503

504

505

506

507

508

509

510

511

512

513

514

515

516

517

518

519

520

521

522

523

524

525

526

527

528

529

530

531

532

533

534

535

536

537

538

539

540

541

542

543

544

545

546

547

548

549

550

551

552

553

554

555

556

557

558

559

560

561

562

563

564

565

566

567

568

569

570

571

572

573

574

575

576

577

578

579

580

581

582

583

584

585

586

587

588

589

590

591

592

593

594

595

596

597

598

599

ACL 2018 Submission ***. Confidential Review Copy. DO NOT DISTRIBUTE.

skm ≈
wki +

∑
C(wki)

Ls =
∑

<skm,slm>∈Sk,l

|| − ||2

ekm
ekm

k

αnkm ∝ sim( ,
∑

wki∈snkm

)

αnkm
n skm

ekm αnkm

αki,lj ∝
wki∈skm,wlj∈slm

sim(wki, wlj)

=
∑

snkm∈skm

αnkm
∑

wki∈snkm

αki,lj

L = Lc + γLs

γ

400

401

402

403

404

405

406

407

408

409

410

411

412

413

414

415

416

417

418

419

420

421

422

423

424

425

426

427

428

429

430

431

432

433

434

435

436

437

438

439

440

441

442

443

444

445

446

447

448

449

450

451

452

453

454

455

456

457

458

459

460

461

462

463

464

465

466

467

468

469

470

471

472

473

474

475

476

477

478

479

480

481

482

483

484

485

486

487

488

489

490

491

492

493

494

495

496

497

498

499

ACL 2018 Submission ***. Confidential Review Copy. DO NOT DISTRIBUTE.

eki elj < eki, elj >∈ Rc

een,Kobe
ezh,Kobe

een,Joe ezh,Joe

< een,Kobe, ezh,Kobe >
< een,Joe, ezh,Joe >

D A
KN

Lc =
∑

xi∈{D,A,KN}

P (C(xi)|xi)

xi
C(xi)

C(xi)
xi

xi ∈ D
xi ∈ KN

wj xi
< wj , ei >∈ A

een,Joe wen,player
wen,NBA

een,LosAngelesLakers
een,Joe een,Kobe ezh,Kobe

wki
Cl(wki)

Ls =
∑

<skm,slm∈Sk,l

∑

wki∈skm

||wki −
∑

Cl(wki)||2

< skm, slm >∈ Sk,l m
k l

Cross-lingual
 supervision

Data Generation

and was an 8-time [[All-star]] … [[ ]] …

NBA

Lawrence Foust

NBA

player

American
NBA

All-star
eLawrence

wbasketballwplayer
… …

1

000

001

002

003

004

005

006

007

008

009

010

011

012

013

014

015

016

017

018

019

020

021

022

023

024

025

026

027

028

029

030

031

032

033

034

035

036

037

038

039

040

041

042

043

044

045

046

047

048

049

050

051

052

053

054

055

056

057

058

059

060

061

062

063

064

065

066

067

068

069

070

071

072

073

074

075

076

077

078

079

080

081

082

083

084

085

086

087

088

089

090

091

092

093

094

095

096

097

098

099

ACL 2018 Submission ***. Confidential Review Copy. DO NOT DISTRIBUTE.

Joint Representation Learning of Cross-lingual Words and Entities via
Attentive Distant Supervision

Anonymous ACL submission

Abstract

Jointly learning word and entity represen-
tations benefits many NLP tasks, while has
not been well explored in cross-lingual set-
tings. In this paper, we propose a novel
method that integrates cross-lingual word
and entity representation learning to enable
joint inference among knowledge base and
text across languages, capturing mutually
complementary knowledge. Instead of re-
liance on parallel data, we automatically
generate cross-lingual training data via dis-
tant supervision over multi-lingual knowl-
edge bases. We also propose two types
of knowledge attention and cross-lingual
attention to select the most informative
words and filter out noise, which will fur-
ther improve the performance. In exper-
iments, separate tasks of word translation
and entity relatedness demonstrate the ef-
fectiveness of our method with an average
gain of 20% and 3% over baselines, re-
spectively. Using entity linking as a case
study, the results on benchmark dataset
verify the quality of our embeddings.

1 Introduction

Multi-lingual knowledge bases (KB), storing mil-
lions of entities and their facts in various lan-
guages, provide rich structured knowledge for un-
derstanding natural language beyond texts. Mean-
while, abundant text corpus contains large amount
of potential knowledge complementary to existing
KBs. Therefore, researchers leverage both types
of resources to improve various natural language
processing (NLP) related tasks, such as relation ex-
traction (Weston et al., 2013; Lin et al., 2017), and
entity linking (Tsai and Roth, 2016; Yamada et al.,
2016; Cao et al., 2017; Ji et al., 2016).

Most existing work jointly models KB and text
corpus to enhance each other by learning word and
entity representations in a unified vector space. For
example, (Wang et al., 2014; Yamada et al., 2016;
Cao et al., 2017) utilize the coherence informa-
tion to align similar words and entities with sim-
ilar embedding vectors. Another approach in (Han
et al., 2016; Toutanova et al., 2015; Wu et al.,
2016) learns to represent entities based on their
textual descriptions together with the structured re-
lations. However, these methods only focus on
mono-lingual settings, and few researches have
been done in cross-lingual scenarios.
In this paper, we propose to learn cross-lingual

word and entity representations in the same seman-
tic space, to enable joint inference among KB and
text across languages without any additional trans-
lation mechanism, which is usually expensive and
may introduce inevitable errors. Our embeddings
are helpful to break down language gaps in many
tasks, such as cross-lingual entity linking, in which
the major challenge lies in measuring the similar-
ity between entities and corresponding mentioned
words in different languages.
e , w ,w
The intuition is that, words and entities in

various languages share some common semantic
meanings1, but there are also ways in which they
differ. On one hand, we utilize their shared seman-
tics to align similar words and entities with simi-
lar embedding vectors, no matter they are in the
same language or not. On the other hand, cross-
lingual embeddings will benefit from different lan-
guages due to the complementary knowledge. For
instance, textual ambiguity in one language may
disappear in another language, e.g., the two mean-

1Some cross-lingual pioneering work observe that word
embeddings trained separately on monolingual corpora ex-
hibit isomorphic structure across languages (Mikolov et al.,
2013; Zhang et al., 2017).

1

000

001

002

003

004

005

006

007

008

009

010

011

012

013

014

015

016

017

018

019

020

021

022

023

024

025

026

027

028

029

030

031

032

033

034

035

036

037

038

039

040

041

042

043

044

045

046

047

048

049

050

051

052

053

054

055

056

057

058

059

060

061

062

063

064

065

066

067

068

069

070

071

072

073

074

075

076

077

078

079

080

081

082

083

084

085

086

087

088

089

090

091

092

093

094

095

096

097

098

099

ACL 2018 Submission ***. Confidential Review Copy. DO NOT DISTRIBUTE.

Joint Representation Learning of Cross-lingual Words and Entities via
Attentive Distant Supervision

Anonymous ACL submission

Abstract

Jointly learning word and entity represen-
tations benefits many NLP tasks, while has
not been well explored in cross-lingual set-
tings. In this paper, we propose a novel
method that integrates cross-lingual word
and entity representation learning to enable
joint inference among knowledge base and
text across languages, capturing mutually
complementary knowledge. Instead of re-
liance on parallel data, we automatically
generate cross-lingual training data via dis-
tant supervision over multi-lingual knowl-
edge bases. We also propose two types
of knowledge attention and cross-lingual
attention to select the most informative
words and filter out noise, which will fur-
ther improve the performance. In exper-
iments, separate tasks of word translation
and entity relatedness demonstrate the ef-
fectiveness of our method with an average
gain of 20% and 3% over baselines, re-
spectively. Using entity linking as a case
study, the results on benchmark dataset
verify the quality of our embeddings.

1 Introduction

Multi-lingual knowledge bases (KB), storing mil-
lions of entities and their facts in various lan-
guages, provide rich structured knowledge for un-
derstanding natural language beyond texts. Mean-
while, abundant text corpus contains large amount
of potential knowledge complementary to existing
KBs. Therefore, researchers leverage both types
of resources to improve various natural language
processing (NLP) related tasks, such as relation ex-
traction (Weston et al., 2013; Lin et al., 2017), and
entity linking (Tsai and Roth, 2016; Yamada et al.,
2016; Cao et al., 2017; Ji et al., 2016).

Most existing work jointly models KB and text
corpus to enhance each other by learning word and
entity representations in a unified vector space. For
example, (Wang et al., 2014; Yamada et al., 2016;
Cao et al., 2017) utilize the coherence informa-
tion to align similar words and entities with sim-
ilar embedding vectors. Another approach in (Han
et al., 2016; Toutanova et al., 2015; Wu et al.,
2016) learns to represent entities based on their
textual descriptions together with the structured re-
lations. However, these methods only focus on
mono-lingual settings, and few researches have
been done in cross-lingual scenarios.
In this paper, we propose to learn cross-lingual

word and entity representations in the same seman-
tic space, to enable joint inference among KB and
text across languages without any additional trans-
lation mechanism, which is usually expensive and
may introduce inevitable errors. Our embeddings
are helpful to break down language gaps in many
tasks, such as cross-lingual entity linking, in which
the major challenge lies in measuring the similar-
ity between entities and corresponding mentioned
words in different languages.
e , w ,w
The intuition is that, words and entities in

various languages share some common semantic
meanings1, but there are also ways in which they
differ. On one hand, we utilize their shared seman-
tics to align similar words and entities with simi-
lar embedding vectors, no matter they are in the
same language or not. On the other hand, cross-
lingual embeddings will benefit from different lan-
guages due to the complementary knowledge. For
instance, textual ambiguity in one language may
disappear in another language, e.g., the two mean-

1Some cross-lingual pioneering work observe that word
embeddings trained separately on monolingual corpora ex-
hibit isomorphic structure across languages (Mikolov et al.,
2013; Zhang et al., 2017).

1

000

001

002

003

004

005

006

007

008

009

010

011

012

013

014

015

016

017

018

019

020

021

022

023

024

025

026

027

028

029

030

031

032

033

034

035

036

037

038

039

040

041

042

043

044

045

046

047

048

049

050

051

052

053

054

055

056

057

058

059

060

061

062

063

064

065

066

067

068

069

070

071

072

073

074

075

076

077

078

079

080

081

082

083

084

085

086

087

088

089

090

091

092

093

094

095

096

097

098

099

ACL 2018 Submission ***. Confidential Review Copy. DO NOT DISTRIBUTE.

Joint Representation Learning of Cross-lingual Words and Entities via
Attentive Distant Supervision

Anonymous ACL submission

Abstract

Jointly learning word and entity represen-
tations benefits many NLP tasks, while has
not been well explored in cross-lingual set-
tings. In this paper, we propose a novel
method that integrates cross-lingual word
and entity representation learning to enable
joint inference among knowledge base and
text across languages, capturing mutually
complementary knowledge. Instead of re-
liance on parallel data, we automatically
generate cross-lingual training data via dis-
tant supervision over multi-lingual knowl-
edge bases. We also propose two types
of knowledge attention and cross-lingual
attention to select the most informative
words and filter out noise, which will fur-
ther improve the performance. In exper-
iments, separate tasks of word translation
and entity relatedness demonstrate the ef-
fectiveness of our method with an average
gain of 20% and 3% over baselines, re-
spectively. Using entity linking as a case
study, the results on benchmark dataset
verify the quality of our embeddings.

1 Introduction

Multi-lingual knowledge bases (KB), storing mil-
lions of entities and their facts in various lan-
guages, provide rich structured knowledge for un-
derstanding natural language beyond texts. Mean-
while, abundant text corpus contains large amount
of potential knowledge complementary to existing
KBs. Therefore, researchers leverage both types
of resources to improve various natural language
processing (NLP) related tasks, such as relation ex-
traction (Weston et al., 2013; Lin et al., 2017), and
entity linking (Tsai and Roth, 2016; Yamada et al.,
2016; Cao et al., 2017; Ji et al., 2016).

Most existing work jointly models KB and text
corpus to enhance each other by learning word and
entity representations in a unified vector space. For
example, (Wang et al., 2014; Yamada et al., 2016;
Cao et al., 2017) utilize the coherence informa-
tion to align similar words and entities with sim-
ilar embedding vectors. Another approach in (Han
et al., 2016; Toutanova et al., 2015; Wu et al.,
2016) learns to represent entities based on their
textual descriptions together with the structured re-
lations. However, these methods only focus on
mono-lingual settings, and few researches have
been done in cross-lingual scenarios.
In this paper, we propose to learn cross-lingual

word and entity representations in the same seman-
tic space, to enable joint inference among KB and
text across languages without any additional trans-
lation mechanism, which is usually expensive and
may introduce inevitable errors. Our embeddings
are helpful to break down language gaps in many
tasks, such as cross-lingual entity linking, in which
the major challenge lies in measuring the similar-
ity between entities and corresponding mentioned
words in different languages.
e , w ,w
The intuition is that, words and entities in

various languages share some common semantic
meanings1, but there are also ways in which they
differ. On one hand, we utilize their shared seman-
tics to align similar words and entities with simi-
lar embedding vectors, no matter they are in the
same language or not. On the other hand, cross-
lingual embeddings will benefit from different lan-
guages due to the complementary knowledge. For
instance, textual ambiguity in one language may
disappear in another language, e.g., the two mean-

1Some cross-lingual pioneering work observe that word
embeddings trained separately on monolingual corpora ex-
hibit isomorphic structure across languages (Mikolov et al.,
2013; Zhang et al., 2017).

1

000

001

002

003

004

005

006

007

008

009

010

011

012

013

014

015

016

017

018

019

020

021

022

023

024

025

026

027

028

029

030

031

032

033

034

035

036

037

038

039

040

041

042

043

044

045

046

047

048

049

050

051

052

053

054

055

056

057

058

059

060

061

062

063

064

065

066

067

068

069

070

071

072

073

074

075

076

077

078

079

080

081

082

083

084

085

086

087

088

089

090

091

092

093

094

095

096

097

098

099

ACL 2018 Submission ***. Confidential Review Copy. DO NOT DISTRIBUTE.

Joint Representation Learning of Cross-lingual Words and Entities via
Attentive Distant Supervision

Anonymous ACL submission

Abstract

Jointly learning word and entity represen-
tations benefits many NLP tasks, while has
not been well explored in cross-lingual set-
tings. In this paper, we propose a novel
method that integrates cross-lingual word
and entity representation learning to enable
joint inference among knowledge base and
text across languages, capturing mutually
complementary knowledge. Instead of re-
liance on parallel data, we automatically
generate cross-lingual training data via dis-
tant supervision over multi-lingual knowl-
edge bases. We also propose two types
of knowledge attention and cross-lingual
attention to select the most informative
words and filter out noise, which will fur-
ther improve the performance. In exper-
iments, separate tasks of word translation
and entity relatedness demonstrate the ef-
fectiveness of our method with an average
gain of 20% and 3% over baselines, re-
spectively. Using entity linking as a case
study, the results on benchmark dataset
verify the quality of our embeddings.

1 Introduction

Multi-lingual knowledge bases (KB), storing mil-
lions of entities and their facts in various lan-
guages, provide rich structured knowledge for un-
derstanding natural language beyond texts. Mean-
while, abundant text corpus contains large amount
of potential knowledge complementary to existing
KBs. Therefore, researchers leverage both types
of resources to improve various natural language
processing (NLP) related tasks, such as relation ex-
traction (Weston et al., 2013; Lin et al., 2017), and
entity linking (Tsai and Roth, 2016; Yamada et al.,
2016; Cao et al., 2017; Ji et al., 2016).

Most existing work jointly models KB and text
corpus to enhance each other by learning word and
entity representations in a unified vector space. For
example, (Wang et al., 2014; Yamada et al., 2016;
Cao et al., 2017) utilize the coherence informa-
tion to align similar words and entities with sim-
ilar embedding vectors. Another approach in (Han
et al., 2016; Toutanova et al., 2015; Wu et al.,
2016) learns to represent entities based on their
textual descriptions together with the structured re-
lations. However, these methods only focus on
mono-lingual settings, and few researches have
been done in cross-lingual scenarios.
In this paper, we propose to learn cross-lingual

word and entity representations in the same seman-
tic space, to enable joint inference among KB and
text across languages without any additional trans-
lation mechanism, which is usually expensive and
may introduce inevitable errors. Our embeddings
are helpful to break down language gaps in many
tasks, such as cross-lingual entity linking, in which
the major challenge lies in measuring the similar-
ity between entities and corresponding mentioned
words in different languages.
e , w ,w
The intuition is that, words and entities in

various languages share some common semantic
meanings1, but there are also ways in which they
differ. On one hand, we utilize their shared seman-
tics to align similar words and entities with simi-
lar embedding vectors, no matter they are in the
same language or not. On the other hand, cross-
lingual embeddings will benefit from different lan-
guages due to the complementary knowledge. For
instance, textual ambiguity in one language may
disappear in another language, e.g., the two mean-

1Some cross-lingual pioneering work observe that word
embeddings trained separately on monolingual corpora ex-
hibit isomorphic structure across languages (Mikolov et al.,
2013; Zhang et al., 2017).

eNBAeAllstar

Mono-lingual Representation Learning

Zh

1

000

001

002

003

004

005

006

007

008

009

010

011

012

013

014

015

016

017

018

019

020

021

022

023

024

025

026

027

028

029

030

031

032

033

034

035

036

037

038

039

040

041

042

043

044

045

046

047

048

049

050

051

052

053

054

055

056

057

058

059

060

061

062

063

064

065

066

067

068

069

070

071

072

073

074

075

076

077

078

079

080

081

082

083

084

085

086

087

088

089

090

091

092

093

094

095

096

097

098

099

ACL 2018 Submission ***. Confidential Review Copy. DO NOT DISTRIBUTE.

Joint Representation Learning of Cross-lingual Words and Entities via
Attentive Distant Supervision

Anonymous ACL submission

Abstract

Jointly learning word and entity represen-
tations benefits many NLP tasks, while has
not been well explored in cross-lingual set-
tings. In this paper, we propose a novel
method that integrates cross-lingual word
and entity representation learning to enable
joint inference among knowledge base and
text across languages, capturing mutually
complementary knowledge. Instead of re-
liance on parallel data, we automatically
generate cross-lingual training data via dis-
tant supervision over multi-lingual knowl-
edge bases. We also propose two types
of knowledge attention and cross-lingual
attention to select the most informative
words and filter out noise, which will fur-
ther improve the performance. In exper-
iments, separate tasks of word translation
and entity relatedness demonstrate the ef-
fectiveness of our method with an average
gain of 20% and 3% over baselines, re-
spectively. Using entity linking as a case
study, the results on benchmark dataset
verify the quality of our embeddings.

1 Introduction

Multi-lingual knowledge bases (KB), storing mil-
lions of entities and their facts in various lan-
guages, provide rich structured knowledge for un-
derstanding natural language beyond texts. Mean-
while, abundant text corpus contains large amount
of potential knowledge complementary to existing
KBs. Therefore, researchers leverage both types
of resources to improve various natural language
processing (NLP) related tasks, such as relation ex-
traction (Weston et al., 2013; Lin et al., 2017), and
entity linking (Tsai and Roth, 2016; Yamada et al.,
2016; Cao et al., 2017; Ji et al., 2016).

Most existing work jointly models KB and text
corpus to enhance each other by learning word and
entity representations in a unified vector space. For
example, (Wang et al., 2014; Yamada et al., 2016;
Cao et al., 2017) utilize the coherence informa-
tion to align similar words and entities with sim-
ilar embedding vectors. Another approach in (Han
et al., 2016; Toutanova et al., 2015; Wu et al.,
2016) learns to represent entities based on their
textual descriptions together with the structured re-
lations. However, these methods only focus on
mono-lingual settings, and few researches have
been done in cross-lingual scenarios.
In this paper, we propose to learn cross-lingual

word and entity representations in the same seman-
tic space, to enable joint inference among KB and
text across languages without any additional trans-
lation mechanism, which is usually expensive and
may introduce inevitable errors. Our embeddings
are helpful to break down language gaps in many
tasks, such as cross-lingual entity linking, in which
the major challenge lies in measuring the similar-
ity between entities and corresponding mentioned
words in different languages.
e , w ,w
The intuition is that, words and entities in

various languages share some common semantic
meanings1, but there are also ways in which they
differ. On one hand, we utilize their shared seman-
tics to align similar words and entities with simi-
lar embedding vectors, no matter they are in the
same language or not. On the other hand, cross-
lingual embeddings will benefit from different lan-
guages due to the complementary knowledge. For
instance, textual ambiguity in one language may
disappear in another language, e.g., the two mean-

1Some cross-lingual pioneering work observe that word
embeddings trained separately on monolingual corpora ex-
hibit isomorphic structure across languages (Mikolov et al.,
2013; Zhang et al., 2017).

1

000

001

002

003

004

005

006

007

008

009

010

011

012

013

014

015

016

017

018

019

020

021

022

023

024

025

026

027

028

029

030

031

032

033

034

035

036

037

038

039

040

041

042

043

044

045

046

047

048

049

050

051

052

053

054

055

056

057

058

059

060

061

062

063

064

065

066

067

068

069

070

071

072

073

074

075

076

077

078

079

080

081

082

083

084

085

086

087

088

089

090

091

092

093

094

095

096

097

098

099

ACL 2018 Submission ***. Confidential Review Copy. DO NOT DISTRIBUTE.

Joint Representation Learning of Cross-lingual Words and Entities via
Attentive Distant Supervision

Anonymous ACL submission

Abstract

Jointly learning word and entity represen-
tations benefits many NLP tasks, while has
not been well explored in cross-lingual set-
tings. In this paper, we propose a novel
method that integrates cross-lingual word
and entity representation learning to enable
joint inference among knowledge base and
text across languages, capturing mutually
complementary knowledge. Instead of re-
liance on parallel data, we automatically
generate cross-lingual training data via dis-
tant supervision over multi-lingual knowl-
edge bases. We also propose two types
of knowledge attention and cross-lingual
attention to select the most informative
words and filter out noise, which will fur-
ther improve the performance. In exper-
iments, separate tasks of word translation
and entity relatedness demonstrate the ef-
fectiveness of our method with an average
gain of 20% and 3% over baselines, re-
spectively. Using entity linking as a case
study, the results on benchmark dataset
verify the quality of our embeddings.

1 Introduction

Multi-lingual knowledge bases (KB), storing mil-
lions of entities and their facts in various lan-
guages, provide rich structured knowledge for un-
derstanding natural language beyond texts. Mean-
while, abundant text corpus contains large amount
of potential knowledge complementary to existing
KBs. Therefore, researchers leverage both types
of resources to improve various natural language
processing (NLP) related tasks, such as relation ex-
traction (Weston et al., 2013; Lin et al., 2017), and
entity linking (Tsai and Roth, 2016; Yamada et al.,
2016; Cao et al., 2017; Ji et al., 2016).

Most existing work jointly models KB and text
corpus to enhance each other by learning word and
entity representations in a unified vector space. For
example, (Wang et al., 2014; Yamada et al., 2016;
Cao et al., 2017) utilize the coherence informa-
tion to align similar words and entities with sim-
ilar embedding vectors. Another approach in (Han
et al., 2016; Toutanova et al., 2015; Wu et al.,
2016) learns to represent entities based on their
textual descriptions together with the structured re-
lations. However, these methods only focus on
mono-lingual settings, and few researches have
been done in cross-lingual scenarios.
In this paper, we propose to learn cross-lingual

word and entity representations in the same seman-
tic space, to enable joint inference among KB and
text across languages without any additional trans-
lation mechanism, which is usually expensive and
may introduce inevitable errors. Our embeddings
are helpful to break down language gaps in many
tasks, such as cross-lingual entity linking, in which
the major challenge lies in measuring the similar-
ity between entities and corresponding mentioned
words in different languages.
eNBA , w , w ,w
The intuition is that, words and entities in

various languages share some common semantic
meanings1, but there are also ways in which they
differ. On one hand, we utilize their shared seman-
tics to align similar words and entities with simi-
lar embedding vectors, no matter they are in the
same language or not. On the other hand, cross-
lingual embeddings will benefit from different lan-
guages due to the complementary knowledge. For
instance, textual ambiguity in one language may
disappear in another language, e.g., the two mean-

1Some cross-lingual pioneering work observe that word
embeddings trained separately on monolingual corpora ex-
hibit isomorphic structure across languages (Mikolov et al.,
2013; Zhang et al., 2017).

1

000

001

002

003

004

005

006

007

008

009

010

011

012

013

014

015

016

017

018

019

020

021

022

023

024

025

026

027

028

029

030

031

032

033

034

035

036

037

038

039

040

041

042

043

044

045

046

047

048

049

050

051

052

053

054

055

056

057

058

059

060

061

062

063

064

065

066

067

068

069

070

071

072

073

074

075

076

077

078

079

080

081

082

083

084

085

086

087

088

089

090

091

092

093

094

095

096

097

098

099

ACL 2018 Submission ***. Confidential Review Copy. DO NOT DISTRIBUTE.

Joint Representation Learning of Cross-lingual Words and Entities via
Attentive Distant Supervision

Anonymous ACL submission

Abstract

Jointly learning word and entity represen-
tations benefits many NLP tasks, while has
not been well explored in cross-lingual set-
tings. In this paper, we propose a novel
method that integrates cross-lingual word
and entity representation learning to enable
joint inference among knowledge base and
text across languages, capturing mutually
complementary knowledge. Instead of re-
liance on parallel data, we automatically
generate cross-lingual training data via dis-
tant supervision over multi-lingual knowl-
edge bases. We also propose two types
of knowledge attention and cross-lingual
attention to select the most informative
words and filter out noise, which will fur-
ther improve the performance. In exper-
iments, separate tasks of word translation
and entity relatedness demonstrate the ef-
fectiveness of our method with an average
gain of 20% and 3% over baselines, re-
spectively. Using entity linking as a case
study, the results on benchmark dataset
verify the quality of our embeddings.

1 Introduction

Multi-lingual knowledge bases (KB), storing mil-
lions of entities and their facts in various lan-
guages, provide rich structured knowledge for un-
derstanding natural language beyond texts. Mean-
while, abundant text corpus contains large amount
of potential knowledge complementary to existing
KBs. Therefore, researchers leverage both types
of resources to improve various natural language
processing (NLP) related tasks, such as relation ex-
traction (Weston et al., 2013; Lin et al., 2017), and
entity linking (Tsai and Roth, 2016; Yamada et al.,
2016; Cao et al., 2017; Ji et al., 2016).

Most existing work jointly models KB and text
corpus to enhance each other by learning word and
entity representations in a unified vector space. For
example, (Wang et al., 2014; Yamada et al., 2016;
Cao et al., 2017) utilize the coherence informa-
tion to align similar words and entities with sim-
ilar embedding vectors. Another approach in (Han
et al., 2016; Toutanova et al., 2015; Wu et al.,
2016) learns to represent entities based on their
textual descriptions together with the structured re-
lations. However, these methods only focus on
mono-lingual settings, and few researches have
been done in cross-lingual scenarios.
In this paper, we propose to learn cross-lingual

word and entity representations in the same seman-
tic space, to enable joint inference among KB and
text across languages without any additional trans-
lation mechanism, which is usually expensive and
may introduce inevitable errors. Our embeddings
are helpful to break down language gaps in many
tasks, such as cross-lingual entity linking, in which
the major challenge lies in measuring the similar-
ity between entities and corresponding mentioned
words in different languages.
eNBA , w , w ,w
The intuition is that, words and entities in

various languages share some common semantic
meanings1, but there are also ways in which they
differ. On one hand, we utilize their shared seman-
tics to align similar words and entities with simi-
lar embedding vectors, no matter they are in the
same language or not. On the other hand, cross-
lingual embeddings will benefit from different lan-
guages due to the complementary knowledge. For
instance, textual ambiguity in one language may
disappear in another language, e.g., the two mean-

1Some cross-lingual pioneering work observe that word
embeddings trained separately on monolingual corpora ex-
hibit isomorphic structure across languages (Mikolov et al.,
2013; Zhang et al., 2017).

1

000

001

002

003

004

005

006

007

008

009

010

011

012

013

014

015

016

017

018

019

020

021

022

023

024

025

026

027

028

029

030

031

032

033

034

035

036

037

038

039

040

041

042

043

044

045

046

047

048

049

050

051

052

053

054

055

056

057

058

059

060

061

062

063

064

065

066

067

068

069

070

071

072

073

074

075

076

077

078

079

080

081

082

083

084

085

086

087

088

089

090

091

092

093

094

095

096

097

098

099

ACL 2018 Submission ***. Confidential Review Copy. DO NOT DISTRIBUTE.

Joint Representation Learning of Cross-lingual Words and Entities via
Attentive Distant Supervision

Anonymous ACL submission

Abstract

Jointly learning word and entity represen-
tations benefits many NLP tasks, while has
not been well explored in cross-lingual set-
tings. In this paper, we propose a novel
method that integrates cross-lingual word
and entity representation learning to enable
joint inference among knowledge base and
text across languages, capturing mutually
complementary knowledge. Instead of re-
liance on parallel data, we automatically
generate cross-lingual training data via dis-
tant supervision over multi-lingual knowl-
edge bases. We also propose two types
of knowledge attention and cross-lingual
attention to select the most informative
words and filter out noise, which will fur-
ther improve the performance. In exper-
iments, separate tasks of word translation
and entity relatedness demonstrate the ef-
fectiveness of our method with an average
gain of 20% and 3% over baselines, re-
spectively. Using entity linking as a case
study, the results on benchmark dataset
verify the quality of our embeddings.

1 Introduction

Multi-lingual knowledge bases (KB), storing mil-
lions of entities and their facts in various lan-
guages, provide rich structured knowledge for un-
derstanding natural language beyond texts. Mean-
while, abundant text corpus contains large amount
of potential knowledge complementary to existing
KBs. Therefore, researchers leverage both types
of resources to improve various natural language
processing (NLP) related tasks, such as relation ex-
traction (Weston et al., 2013; Lin et al., 2017), and
entity linking (Tsai and Roth, 2016; Yamada et al.,
2016; Cao et al., 2017; Ji et al., 2016).

Most existing work jointly models KB and text
corpus to enhance each other by learning word and
entity representations in a unified vector space. For
example, (Wang et al., 2014; Yamada et al., 2016;
Cao et al., 2017) utilize the coherence informa-
tion to align similar words and entities with sim-
ilar embedding vectors. Another approach in (Han
et al., 2016; Toutanova et al., 2015; Wu et al.,
2016) learns to represent entities based on their
textual descriptions together with the structured re-
lations. However, these methods only focus on
mono-lingual settings, and few researches have
been done in cross-lingual scenarios.
In this paper, we propose to learn cross-lingual

word and entity representations in the same seman-
tic space, to enable joint inference among KB and
text across languages without any additional trans-
lation mechanism, which is usually expensive and
may introduce inevitable errors. Our embeddings
are helpful to break down language gaps in many
tasks, such as cross-lingual entity linking, in which
the major challenge lies in measuring the similar-
ity between entities and corresponding mentioned
words in different languages.
eNBA , w , w ,w
The intuition is that, words and entities in

various languages share some common semantic
meanings1, but there are also ways in which they
differ. On one hand, we utilize their shared seman-
tics to align similar words and entities with simi-
lar embedding vectors, no matter they are in the
same language or not. On the other hand, cross-
lingual embeddings will benefit from different lan-
guages due to the complementary knowledge. For
instance, textual ambiguity in one language may
disappear in another language, e.g., the two mean-

1Some cross-lingual pioneering work observe that word
embeddings trained separately on monolingual corpora ex-
hibit isomorphic structure across languages (Mikolov et al.,
2013; Zhang et al., 2017).

1

000

001

002

003

004

005

006

007

008

009

010

011

012

013

014

015

016

017

018

019

020

021

022

023

024

025

026

027

028

029

030

031

032

033

034

035

036

037

038

039

040

041

042

043

044

045

046

047

048

049

050

051

052

053

054

055

056

057

058

059

060

061

062

063

064

065

066

067

068

069

070

071

072

073

074

075

076

077

078

079

080

081

082

083

084

085

086

087

088

089

090

091

092

093

094

095

096

097

098

099

ACL 2018 Submission ***. Confidential Review Copy. DO NOT DISTRIBUTE.

Joint Representation Learning of Cross-lingual Words and Entities via
Attentive Distant Supervision

Anonymous ACL submission

Abstract

Jointly learning word and entity represen-
tations benefits many NLP tasks, while has
not been well explored in cross-lingual set-
tings. In this paper, we propose a novel
method that integrates cross-lingual word
and entity representation learning to enable
joint inference among knowledge base and
text across languages, capturing mutually
complementary knowledge. Instead of re-
liance on parallel data, we automatically
generate cross-lingual training data via dis-
tant supervision over multi-lingual knowl-
edge bases. We also propose two types
of knowledge attention and cross-lingual
attention to select the most informative
words and filter out noise, which will fur-
ther improve the performance. In exper-
iments, separate tasks of word translation
and entity relatedness demonstrate the ef-
fectiveness of our method with an average
gain of 20% and 3% over baselines, re-
spectively. Using entity linking as a case
study, the results on benchmark dataset
verify the quality of our embeddings.

1 Introduction

Multi-lingual knowledge bases (KB), storing mil-
lions of entities and their facts in various lan-
guages, provide rich structured knowledge for un-
derstanding natural language beyond texts. Mean-
while, abundant text corpus contains large amount
of potential knowledge complementary to existing
KBs. Therefore, researchers leverage both types
of resources to improve various natural language
processing (NLP) related tasks, such as relation ex-
traction (Weston et al., 2013; Lin et al., 2017), and
entity linking (Tsai and Roth, 2016; Yamada et al.,
2016; Cao et al., 2017; Ji et al., 2016).

Most existing work jointly models KB and text
corpus to enhance each other by learning word and
entity representations in a unified vector space. For
example, (Wang et al., 2014; Yamada et al., 2016;
Cao et al., 2017) utilize the coherence informa-
tion to align similar words and entities with sim-
ilar embedding vectors. Another approach in (Han
et al., 2016; Toutanova et al., 2015; Wu et al.,
2016) learns to represent entities based on their
textual descriptions together with the structured re-
lations. However, these methods only focus on
mono-lingual settings, and few researches have
been done in cross-lingual scenarios.
In this paper, we propose to learn cross-lingual

word and entity representations in the same seman-
tic space, to enable joint inference among KB and
text across languages without any additional trans-
lation mechanism, which is usually expensive and
may introduce inevitable errors. Our embeddings
are helpful to break down language gaps in many
tasks, such as cross-lingual entity linking, in which
the major challenge lies in measuring the similar-
ity between entities and corresponding mentioned
words in different languages.
eNBA , w , w ,w
The intuition is that, words and entities in

various languages share some common semantic
meanings1, but there are also ways in which they
differ. On one hand, we utilize their shared seman-
tics to align similar words and entities with simi-
lar embedding vectors, no matter they are in the
same language or not. On the other hand, cross-
lingual embeddings will benefit from different lan-
guages due to the complementary knowledge. For
instance, textual ambiguity in one language may
disappear in another language, e.g., the two mean-

1Some cross-lingual pioneering work observe that word
embeddings trained separately on monolingual corpora ex-
hibit isomorphic structure across languages (Mikolov et al.,
2013; Zhang et al., 2017).

1

000

001

002

003

004

005

006

007

008

009

010

011

012

013

014

015

016

017

018

019

020

021

022

023

024

025

026

027

028

029

030

031

032

033

034

035

036

037

038

039

040

041

042

043

044

045

046

047

048

049

050

051

052

053

054

055

056

057

058

059

060

061

062

063

064

065

066

067

068

069

070

071

072

073

074

075

076

077

078

079

080

081

082

083

084

085

086

087

088

089

090

091

092

093

094

095

096

097

098

099

ACL 2018 Submission ***. Confidential Review Copy. DO NOT DISTRIBUTE.

Joint Representation Learning of Cross-lingual Words and Entities via
Attentive Distant Supervision

Anonymous ACL submission

Abstract

Jointly learning word and entity represen-
tations benefits many NLP tasks, while has
not been well explored in cross-lingual set-
tings. In this paper, we propose a novel
method that integrates cross-lingual word
and entity representation learning to enable
joint inference among knowledge base and
text across languages, capturing mutually
complementary knowledge. Instead of re-
liance on parallel data, we automatically
generate cross-lingual training data via dis-
tant supervision over multi-lingual knowl-
edge bases. We also propose two types
of knowledge attention and cross-lingual
attention to select the most informative
words and filter out noise, which will fur-
ther improve the performance. In exper-
iments, separate tasks of word translation
and entity relatedness demonstrate the ef-
fectiveness of our method with an average
gain of 20% and 3% over baselines, re-
spectively. Using entity linking as a case
study, the results on benchmark dataset
verify the quality of our embeddings.

1 Introduction

Multi-lingual knowledge bases (KB), storing mil-
lions of entities and their facts in various lan-
guages, provide rich structured knowledge for un-
derstanding natural language beyond texts. Mean-
while, abundant text corpus contains large amount
of potential knowledge complementary to existing
KBs. Therefore, researchers leverage both types
of resources to improve various natural language
processing (NLP) related tasks, such as relation ex-
traction (Weston et al., 2013; Lin et al., 2017), and
entity linking (Tsai and Roth, 2016; Yamada et al.,
2016; Cao et al., 2017; Ji et al., 2016).

Most existing work jointly models KB and text
corpus to enhance each other by learning word and
entity representations in a unified vector space. For
example, (Wang et al., 2014; Yamada et al., 2016;
Cao et al., 2017) utilize the coherence informa-
tion to align similar words and entities with sim-
ilar embedding vectors. Another approach in (Han
et al., 2016; Toutanova et al., 2015; Wu et al.,
2016) learns to represent entities based on their
textual descriptions together with the structured re-
lations. However, these methods only focus on
mono-lingual settings, and few researches have
been done in cross-lingual scenarios.
In this paper, we propose to learn cross-lingual

word and entity representations in the same seman-
tic space, to enable joint inference among KB and
text across languages without any additional trans-
lation mechanism, which is usually expensive and
may introduce inevitable errors. Our embeddings
are helpful to break down language gaps in many
tasks, such as cross-lingual entity linking, in which
the major challenge lies in measuring the similar-
ity between entities and corresponding mentioned
words in different languages.
eNBA
The intuition is that, words and entities in

various languages share some common semantic
meanings1, but there are also ways in which they
differ. On one hand, we utilize their shared seman-
tics to align similar words and entities with simi-
lar embedding vectors, no matter they are in the
same language or not. On the other hand, cross-
lingual embeddings will benefit from different lan-
guages due to the complementary knowledge. For
instance, textual ambiguity in one language may
disappear in another language, e.g., the two mean-

1Some cross-lingual pioneering work observe that word
embeddings trained separately on monolingual corpora ex-
hibit isomorphic structure across languages (Mikolov et al.,
2013; Zhang et al., 2017).

En

eLawrence

wwas wan

eLawrence

eNBAeAllstarwan wbasketball

wAmerican

Figure 1: The framework of our method. The inputs and outputs of each step are listed in the right
side, and in the left side there are three main components of joint representation learning. Red texts
with brackets are anchors, dashed lines between entities denote relations, and solid lines are cross-lingual
links.

mon neighbors have similar embeddings, no mat-
ter in which language, e.g., English entity Foust
and Chinese entity 福斯特 (Foust) are embed-
ded close in semantic space due to the common
neighborsNBA,All-star andNBA选秀 (draft), etc.
(3) Cross-lingual Sentence Regularizer aims to
learn mutually translated words with similar em-
beddings by pushing their cross-lingual contexts
(i.e. comparable sentences) together. For exam-
ple, English word basketball and the translated
Chinese word 篮球 frequently co-occur in com-
parable sentences, and are close in the semantic
space.
All word/entity embeddings are trained jointly

under a unified optimization objective. Next, we
will introduce how to generate bi-lingual EN and
comparable sentences as well as the three compo-
nents for joint representation learning in turn.

3.3 Cross-lingual Supervision Data
Generation

This section introduces how to extract more cross-
lingual clues from multi-lingual KB in the form of
bi-lingual EN and comparable sentences.

Bi-lingual Entity Network Construction

Conventional knowledge representation meth-
ods normally regard cross-lingual links as a spe-
cial equivalence type of relation between two en-
tities (Zhu et al., 2017). However, we argue that
this may mislead to an inconsistent training ob-

jective since a cross-lingual link actually contains
multiple relations. For example (Figure 1), there
will be no direct relation betweenChinese entity福
斯特 (Foust) and English entity Piston by merely
adding the equivalence relation between Foust and
福斯特, which is in contradiction with the fact that
Foust belongs to Piston, no matter in which lan-
guage.
Therefore, we build bi-lingual entity network by

making cross-lingual linked entities that inherit all
relations from each other. Concretely, we enhance
mono-EN by adding edges from all neighbors of
entity eei to ezj if < eei , ezj >∈ Re−z (layer 2).
e

Comparable Sentences Generation

We utilize distant supervision to generate com-
parable sentences from Wikipedia articles. As
shown in Figure 1, from the page articles of cross-
lingual linked entities eeKobe and e

z
Kobe, we extract

those sentences including another cross-lingual
linked entities eeJoe and ezJoe as comparable sen-
tences Se−z = {< sek, szk >}.
The intuition is that we consider each sentence

in a Wikipedia article has a pseudo mention of
the page entity (talking something about the en-
tity) (Yamada et al., 2017). Thus, if a sentence also
mentions another entity, it implicitly expresses
their relation. Therefore, we make a similar as-
sumption as in relation extraction: If two enti-
ties participate in a relation, and both of them

Figure 2: The nerual model for jointly representation learning.

at filtering out wrong labelling sentences, and
ψ′(weni , w

zh
j ) is cross-lingual attention to deal

with the unbalanced information through possible
aligned words.
Next, we will introduce the two types of atten-

tions in detail.

Knowledge Attention

Suppose that sentences senk,l|Ll=1 contain the same
entities in articles of entity eenm , the wrong labelling
errors increase, because some senk,l is almost irrele-
vant to eenm . Knowledge attention assigns smaller
weights to wrong labelled sentences, and higher
weights to related sentences. Thus, we define it
proportional to the similarity between senk,l and eenm :

ψ(eenm , s
en
k,l) ∝ sim(eenm ,

∑
weni ∈senk,l

weni )

where sim is similarity measurement. We
use cosine similarity in the presented work.
Knowledge attention is normalized to satisfy∑L

l ψ(e
en
m , s

en
k,l) = 1.

Cross-lingual Attention

Inspired by self-attention mechanism (Lin et al.,
2017b), we motivate cross-lingual attention focus-
ing on potential information from comparable sen-
tences themselves. The intuition is to find possible
alignedwords between languages, and filter out the
words without alignments. We define it according
to the maximum similarity computed by our cross-
lingual word embeddings:

ψ′(weni , w
zh
j ) ∝ max

weni ∈senk ,w
zh
j ∈szhk′

sim(weni ,wzhj )

We set a threshold for discarding non-aligned
words if ψ′(weni , wzhj ) < θ, and make a normal-
ization for selected words. We set θ = 0 in exper-
iments. Thus, unbalanced information is trimmed
to the commonmeanings between senk and szhk′ . For
example (Figure 1), words American, basketball,
player are selected due to their aligned Chinese
words 美国, 篮球, 运动员, while 12 seasons in
senk or 前 (former) in szhk′ are discarded due to low
attentions.
The reason of using such regularizer lies in

two points: (1) the embeddings of cross-lingual
aligned words become closer within the pair of
comparable sentences, and meanwhile (2) the dis-
tance between their contexts is also minimized,
which keeps the same way as used in mono-lingual
word embeddings training—the words sharing
more contexts have similar embeddings. In this
way, our regularizer follows a similar assumption
with (Gouws et al., 2015): The more frequently
two words occur in parallel/comparable sentence
pairs, the closer their representation will be.

5.4 Training
All above components are jointly trained using the
overall objective function as follows:

L = Lm + Le + γLs

where γ is a hyper-parameter to tune the effect of
cross-lingual sentence regularizer, and set to 1 in



233

experiments. We use Softmax as probability func-
tion, and negative sampling and SGD for efficient
optimization (Mikolov et al., 2013a).

6 Experiments
In this section, we describe some qualitative
analysis with nearest neighbors and quantita-
tive experiments with the tasks of word trans-
lation, entity relatedness and cross-lingual en-
tity linking to verify the quality of cross-
lingual word embeddings, entity embeddings
and the joint inference among them, respec-
tively. The codes of our proposed model
can be found in https://github.com/
TaoMiner/MultiLingualEmbedding.

6.1 Experiment Settings

Word Entity
vocab (m) token (b) vocab (m) token (b)

En 1.99 1.90 3.94 0.41
Zh 0.55 0.17 0.58 0.06
Es 0.70 0.48 0.70 0.04
Ja 0.46 0.45 0.88 0.08
It 0.67 0.40 1.09 0.12
Tr 0.33 0.05 0.22 0.01

Table 1: Multi-lingual KB Statistics.

We choose Wikipedia, the April 2017 dump, as
multi-lingual KB and six popular languages for
evaluation. The preprocessing consists of follow-
ing steps: converting texts into lower cases, filter-
ing out symbols and low frequency words and en-
tities (less than 5), and tokenizing Chinese corpus
using Jieba4 and Japanese corpus using mecab5.
The statistics is listed in Table 1. For brevity, we
adopt two-letter abbreviations: ‘En’, ‘Zh’, ‘Es’,
‘Ja’, ‘It’ and ‘Tr’ for English, Chinese, Span-
ish, Japanese, Italian and Turkish, respectively.
The token sub-column denotes the total number of
word/entity in the entire training corpus, and we
use ‘m’ to denote million and ‘b’ for billion.
For cross-lingual settings, we choose five lan-

guage pairs to compare with state-of-the-art meth-
ods, whose statistics is listed in Table 2.
We trained our method using the suggested

parameters in Skip-gram model (Mikolov et al.,
2013c) and evaluate the embeddings shared by all
tasks for fairly comparison. We set training epoch
as 2 to ensure convergence, which costs nearly 20

4https://github.com/fxsjy/jieba
5http://taku910.github.io/mecab/

Cross-lingual Comparable Bilingual EN
Links (m) Sentences(m) E(m) R(b)

Es-En 0.82 4.66 4.64 0.58
Zh-En 0.51 2.02 4.52 0.57
Ja-Zh 0.26 1.04 1.46 0.19
It-En 0.74 3.83 5.03 0.68
Tr-En 0.15 0.75 4.16 0.44

Table 2: Cross-lingual Data Statistics.

hours on the server with 64 core CPU and 188GB
memory. The embedding dimension is set to 200
and context window size is 5. For each positive
example, we sample 5 negative examples.

6.2 Qualitative Analysis

Translation words (Chinese)
篮球 (+),篮球队 (basketball team),湖人 (lakers),男子
篮球 (men’s basketball),湖人队 (the lakers),国王队 (the
Kings),美式足球 (American football),中锋 (center)
Nearest entities (Chinese)
NBA,篮球 (Basketball) ,控球后卫 (Point guard), NBA
选秀 (draft), 香港男子甲一组男子篮球联赛 (Hong
Kong men’s top basketball league), 橄榄球 (American
football),东方篮球队 (Eastern basketball team)
Nearest words
nba, wnba, player, twyman, professional, pick, 76ers
Nearest entities
Professional sports, Varsity letter, Sports agent, All-
America, Final four, All-star, College basketball

Table 3: Cross-lingual nearest words and entities of En-
glish word basketball.

Wemanually checked nearest neighbors to have
a straightforward impression of the quality of our
embeddings. The nearest neighbors of English
word basketball is listed in Table 3.
As Table 3 shows, we find the correct translation

ranked at top 1 (marked by +), and the listed words
as well as English nearest words are all basketball
related, indicating a higher quality of our cross-
lingual word embeddings. Interestingly, we found
that although all nearest entities are sports related,
e.g., NBA or Professional sports, there is an ob-
vious culture divergence between Chinese entities
and English entities, such asHongKong basketball
league v.s. All-America.

6.3 Word Translation
Following (Zhang et al., 2017b), we test our cross-
lingual word embeddings on benchmark dataset
including over 2,000 bilingual word pairs on av-
erage. The ground truth is obtained from Open

https://github.com/TaoMiner/MultiLingualEmbedding
https://github.com/TaoMiner/MultiLingualEmbedding
https://github.com/fxsjy/jieba
http://taku910.github.io/mecab/


234

Es-En It-En Ja-Zh Tr-En Zh-En
large small large small large small large small large small

TM - 48.61 - 37.95 - 26.67 - 11.15 4.79 21.79
IA - 60.41 - 46.52 - 36.35 - 17.11 7.08 32.29
Bilbowa 53 65.96 - - - - - - - -
BWESG 48.88 66.38 36.84 51.29 30.93 37.80 21.36 35.59 20.57 29.17
Adversarial - 71.97 - 58.60 - 43.02 - 17.18 7.92 43.31
Ours-noatt 68.34 77.1 62.22 65.90 37.00 42.30 57.47 60.51 35.90 42.80
Ours 70.41 78.50 63.07 67.85 41.30 46.70 54.40 59.31 35.66 44.67

Table 4: Word Translation.

Multilingual WordNet6 or Google translation. We
compare all methods using the same vocabulary,
and analyze the vocabulary size’s impact by set-
ting a nearly 5k small scale and 50k large scale.
We choose several state-of-the-art methods as

baseline, using different level of parallel data: (1)
TM (Mikolov et al., 2013b), IA (Zhang et al.,
2016) are pioneers and popular transformation
based methods using bilingual lexicon. (2) Bil-
bowa (Gouws et al., 2015) is typical work using
parallel sentences and performs quite well. (3)
BWESG (Vulic and Moens, 2016) is similar to
our method and achieves best performance in the
literature of using comparable data. (4) Adver-
sarial model (Zhang et al., 2017b) is the state-of-
the-arts without parallel data. Besides, we re-
move attention from our method to investigate the
impacts from attention mechanisms, marked with
Ours-noatt.
For fair comparison, we report the results in

original paper (Zhang et al., 2017b) except Bil-
bowa and BWESG, which didn’t report their re-
sults on the same benchmark datasets. So, we care-
fully implement them using released codes on the
same training corpus as ours with suggested pa-
rameters. Nevertheless, we do not have perfor-
mance reports of Zh-En, It-En, Tr-En and Ja-Zh
with Bilbowa due to the lack of parallel data used
in the original paper. As shown in Table 4, we can
see:

• Our proposed method significantly outper-
forms all the baseline methods with average
gains of 21% and 9.1% on large and small
vocabulary. This proves the high quality of
our generated cross-lingual data and the ef-
fectiveness of our joint framework.

• The pair of languages have similar culture
achieves better performance (Es-En, It-En,
Tr-En, Ja-Zh) than that have different cultural
origins, e.g., Zh-En.

6http://compling.hss.ntu.edu.sg/omw

• Languages with richer corpus have better
translations because adequate training data
helps to capture more accurate cross-lingual
semantics (Es-En, It-En, Tr-En v.s. Ja-Zh).

• Our method has less performance reduction
between small and large vocabulary than
methods based on parallel word pairs, be-
cause we adopt a consistent objective func-
tion which aligns cross-lingual semantics,
and simultaneously keeps their own mono-
lingual semantics.

• Attention mechanisms further improve the
performance, mainly because they help to
select the most informative words and sen-
tences, filtering out unrelated data.

6.4 Entity Relatedness
With respect to our entity embeddings, we have
conducted experiments to evaluate English entity
relatedness following (Ganea and Hofmann, 2017;
Hoffart et al., 2011), in which the dataset con-
tains 3,314 entities, and each entity has 91 candi-
date entities labeled with 1 or 0, indicating whether
they are semantically related. Given an entity, we
rank candidate entities according to their similarity
based on our embeddings, and evaluate the rank-
ing quality through two standard metrics: normal-
ized discounted cumulative gain (NDCG) (Järvelin
andKekäläinen, 2002) andmean average precision
(MAP) (Manning et al., 2008).
To give a comprehensive fair comparison, we

choose several widely used and state-of-the-art
methods as our baselines, and compare with the
results in the original papers: (1) WLM (Milne
and Witten, 2008), the popular semantic similar-
ity measurement based on Wikipedia anchor links.
(2) ALIGN (Yamada et al., 2016) andMPME (Cao
et al., 2017), state-of-the-arts that jointly learn
word and entity embeddings using mono-lingual
EN. (3) Deep Joint (DJ) model (Ganea and Hof-
mann, 2017), deep neural model that achieves the

http://compling.hss.ntu.edu.sg/omw


235

best performance of entity relatedness.
NDCG MAP

@1 @5 @10
WLM .54 .52 .55 .48
ALIGN (d=500) .59 .56 .59 .52
MPME .61 .61 .65 .58
DJ (d=300) .63 .61 .64 .58
Ours (Zh-En) .62 .62 .66 .59
Ours (Es-En) .61 .61 .65 .59
Ours (Tr-En) .62 .62 .65 .59
Ours (It-En) .61 .61 .65 .58
Ours-e (Es-En) .62 .62 .67 .61
Ours-e (Es-En,epoch=5) .64 .64 .68 .62

Table 5: Entity Relatedness.

Table 5 shows the results of baseline methods as
well as our methods based on different languages.
We also test the cases of our method without train-
ing cross-lingual words, marked as Ours-e. We can
see our method outperforms all baseline methods
by introducing cross-lingual information, and all
bilingual ENs lead to similar results. Strangely,
ALIGN and DJ with more embedding dimensions
seemly fails to capture overall relatedness (per-
formance reduction from top@1 to top@5). The
best performance of Ours-e implies that training
cross-lingual word slightly harms the performance
of entity embeddings. We can introduce additional
sense embeddings in future (Cao et al., 2017).
Although favorable improvements has been

achieved by using our English entity embeddings,
it shall be fewer than that of other languages, be-
cause resources of English are already quite rich,
and even richer than many other languages, thus
contributions from other languages will be less sig-
nificant than vice versa. Due to the limitation of
the publication, we neglect to report experiment
results on the vice versa direction.

6.5 Cross-lingual Entity Linking
Entity linking, the task of identifing the language-
specific reference entity for mentions in texts,
raises the key challenges of comparing the rel-
evance between entities and contextual words
around the mentions (Cao et al., 2015; Nguyen
et al., 2016). Recently, the surge of cross-lingual
analysis pushes the entity linking task on cross-
lingual settings (Ji et al., 2015). Therefore, we
comprehensively measure our joint inference abil-
ity among words and entities using the tri-lingual
EL benchmark dataset KBP2015, which consists
of 944 documents and 38,831 mentions, and di-
vides them into 444 and 500 documents for train-
ing and evaluation. Note that the main purpose of

it is not to beat other EL models but to evaluate the
quality of our embeddings, so we adopt a simple
classifier GBRT (Gradient Boost Regression Tree)
basedmethod as in (Cao et al., 2017; Yamada et al.,
2016), replace with our cross-lingual embeddings,
and filter out mentions that are out of our vocabu-
lary.

English Spanish Chinese
Top system 73.7 80.4 83.1
Second system 66.2 71.5 78.1
Ours 73.9 79.1 81.3

Table 6: Tri-lingual Entity Linking.

Table 6 shows the top 1 linking accuracy (%).
We can see our method performs much better than
the second ranked system, and is competitive with
the top ranked system. Considering that the sys-
tems utilize additional translation tools (Ji et al.,
2015), we conclude that our embeddings are high
qualified for joint inference among entities and
words in different languages.

7 Conclusions

In this paper, we propose a novel method to jointly
learn cross-lingual word and entity representa-
tions that enables effective inference among cross-
lingual knowledge bases and texts. Instead of par-
allel data, we use distant supervision over multi-
lingual KB to generate high quality comparable
data as cross-lingual supervision signals for two
types of regularizer. We introduce attention mech-
anism to further improve the training quality. A
series of experiments on several tasks verify the
effectiveness of our methods as well as the quality
of cross-lingual word and entity embeddings.
In the future, we will enrich semantics of low-

resourced languages by cross-lingual linking to
rich-resourced languages, and extend more cross-
lingual words and entities to multi-lingual settings.

8 Acknowledgments

The work is supported by NSFC key project (No.
61533018，U1736204，61661146007), Ministry
of Education and China Mobile Research Fund
(No. 20181770250), and THUNUS NExT++ Co-
Lab. Partial financial support from P3ML project
funded by BMBF of Germany under grant number
01/S17064 is greatly acknowledged.



236

References
Waleed Ammar, George Mulcaire, Yulia Tsvetkov,
Guillaume Lample, Chris Dyer, and Noah A. Smith.
2016. Massively multilingual word embeddings.
CoRR.

Mikel Artetxe, Gorka Labaka, and Eneko Agirre. 2017.
Learning bilingual word embeddings with (almost)
no bilingual data. In ACL.

Antonio Valerio Miceli Barone. 2016. Towards cross-
lingual distributed representations without paral-
lel text trained with adversarial autoencoders. In
Rep4NLP@ACL.

José Camacho-Collados, Mohammad Taher Pilehvar,
and Roberto Navigli. 2015. Nasari: a novel ap-
proach to a semantically-aware representation of
items. In HLT-NAACL.

Hailong Cao, Tiejun Zhao, Shu Zhang, and Yao Meng.
2016. A distribution-based model to learn bilingual
word embeddings. In COLING.

Yixin Cao, Lei Hou, Juanzi Li, and Zhiyuan Liu. 2018.
Neural collective entity linking. In COLING.

Yixin Cao, Lifu Huang, Heng Ji, Xu Chen, and Juan-
Zi Li. 2017. Bridge text and knowledge by learning
multi-prototype entity mention embedding. In ACL.

Yixin Cao, Juanzi Li, Xiaofei Guo, Shuanhu Bai, Heng
Ji, and Jie Tang. 2015. Name list only? target entity
disambiguation in short texts. In EMNLP.

A. P. Sarath Chandar, Stanislas Lauly, Hugo
Larochelle, Mitesh M. Khapra, Balaraman Ravin-
dran, Vikas C. Raykar, and Amrita Saha. 2014. An
autoencoder approach to learning bilingual word
representations. In NIPS.

Meiping Dong, Yong Cheng, Yang Liu, Jia Xu,
Maosong Sun, Tatsuya Izuha, and Jie Hao. 2014.
Query lattice for translation retrieval. In COLING.

Manaal Faruqui and Chris Dyer. 2014. Improving vec-
tor space word representations using multilingual
correlation. In EACL.

Octavian-Eugen Ganea and Thomas Hofmann. 2017.
Deep joint entity disambiguation with local neural
attention. In EMNLP.

Stephan Gouws, Yoshua Bengio, and Gregory S. Cor-
rado. 2015. Bilbowa: Fast bilingual distributed rep-
resentations without word alignments. In ICML.

Xu Han, Zhiyuan Liu, and Maosong Sun. 2016. Joint
representation learning of text and knowledge for
knowledge graph completion. CoRR.

Yanchao Hao, Yuanzhe Zhang, Kang Liu, Shizhu He,
Zhanyi Liu, Hua Wu, and Jun Zhao. 2017. An end-
to-end model for question answering over knowl-
edge base with cross-attention combining global
knowledge. In ACL.

Shizhu He, Cao Liu, Kang Liu, and Jun Zhao.
2017. Generating natural answers by incorporating
copying and retrieving mechanisms in sequence-to-
sequence learning. In ACL.

Karl Moritz Hermann and Phil Blunsom. 2014. Multi-
lingual models for compositional distributed seman-
tics. In ACL.

Johannes Hoffart, Mohamed Amir Yosef, Ilaria Bor-
dino, Hagen Fürstenau, Manfred Pinkal, Marc Span-
iol, Bilyana Taneva, Stefan Thater, and Gerhard
Weikum. 2011. Robust disambiguation of named en-
tities in text. In EMNLP.

Raphael Hoffmann, Congle Zhang, Xiao Ling, Luke S.
Zettlemoyer, and Daniel S.Weld. 2011. Knowledge-
based weak supervision for information extraction of
overlapping relations. In ACL.

Kalervo Järvelin and Jaana Kekäläinen. 2002. Cumu-
lated gain-based evaluation of ir techniques. TOIS.

Heng Ji, Joel Nothman, Hoa Trang Dang, and Syd-
ney Informatics Hub. 2016. Overview of tac-
kbp2016 tri-lingual edl and its impact on end-to-end
cold-start kbp. In TAC.

Heng Ji, Joel Nothman, Ben Hachey, and Radu Flo-
rian. 2015. Overview of tac-kbp2015 tri-lingual en-
tity discovery and linking. In TAC.

Alexandre Klementiev, Ivan Titov, and Binod Bhat-
tarai. 2012. Inducing crosslingual distributed rep-
resentations of words. In COLING.

Tomás Kociský, Karl Moritz Hermann, and Phil Blun-
som. 2014. Learning bilingual word representations
by marginalizing alignments. In ACL.

Guillaume Lample, Alexis Conneau, Ludovic Denoyer,
Hervé Jégou, et al. 2018. Word translation without
parallel data. ICLR.

Yankai Lin, Zhiyuan Liu, and Maosong Sun. 2017a.
Neural relation extraction with multi-lingual atten-
tion. In ACL.

Zhouhan Lin, Minwei Feng, Cícero Nogueira dos San-
tos, Mo Yu, Bing Xiang, Bowen Zhou, and Yoshua
Bengio. 2017b. A structured self-attentive sentence
embedding. CoRR.

Thang Luong, Hieu Pham, and Christopher D. Man-
ning. 2015. Bilingual word representations with
monolingual quality in mind. In VS@HLT-NAACL.

Christopher D. Manning, Prabhakar Raghavan, and
Hinrich Schütze. 2008. Introduction to information
retrieval.

Tomas Mikolov, Kai Chen, Gregory S. Corrado, and
Jeffrey Dean. 2013a. Efficient estimation of word
representations in vector space. CoRR.



237

TomasMikolov, Quoc V Le, and Ilya Sutskever. 2013b.
Exploiting similarities among languages for machine
translation. CoRR.

Tomas Mikolov, Ilya Sutskever, Kai Chen, Gregory S.
Corrado, and Jeffrey Dean. 2013c. Distributed rep-
resentations of words and phrases and their compo-
sitionality. In NIPS.

David Milne and Ian H. Witten. 2008. An effective,
low-cost measure of semantic relatedness obtained
from wikipedia links. In AAAI.

Mike Mintz, Steven Bills, Rion Snow, and Daniel Ju-
rafsky. 2009. Distant supervision for relation extrac-
tion without labeled data. In ACL/IJCNLP.

Aditya Mogadala and Achim Rettinger. 2016. Bilin-
gual word embeddings from parallel and non-
parallel corpora for cross-language text classifica-
tion. In HLT-NAACL.

Thien Huu Nguyen, Nicolas Fauceglia, Mariano Ro-
driguez Muro, Oktie Hassanzadeh, Alfio Massimil-
iano Gliozzo, and Mohammad Sadoghi. 2016. Joint
learning of local and global features for entity link-
ing via neural networks. In COLING.

Sebastian Ruder, Ivan Vulić, and Anders Søgaard.
2017. A survey of cross-lingual word embedding
models. arXiv preprint arXiv:1706.04902.

Tianze Shi, Zhiyuan Liu, Yang Liu, and Maosong Sun.
2015. Learning cross-lingual word embeddings via
matrix co-factorization. In ACL.

Radu Soricut and Nan Ding. 2016. Multilingual word
embeddings using multigraphs. CoRR.

Mihai Surdeanu, Julie Tibshirani, Ramesh Nallapati,
and Christopher D. Manning. 2012. Multi-instance
multi-label learning for relation extraction. In
EMNLP-CoNLL.

Kristina Toutanova, Danqi Chen, Patrick Pantel, Hoi-
fung Poon, Pallavi Choudhury, and Michael Gamon.
2015. Representing text for joint embedding of text
and knowledge bases. In EMNLP.

Ivan Vulic and Marie-Francine Moens. 2015. Bilin-
gual word embeddings from non-parallel document-
aligned data applied to bilingual lexicon induction.
In ACL.

Ivan Vulic andMarie-Francine Moens. 2016. Bilingual
distributed word representations from document-
aligned comparable data. JAIR.

Zhen Wang, Jianwen Zhang, Jianlin Feng, and Zheng
Chen. 2014. Knowledge graph and text jointly em-
bedding. In EMNLP.

Zhigang Wang and Juan-Zi Li. 2016. Text-enhanced
representation learning for knowledge graph. In IJ-
CAI.

Jason Weston, Antoine Bordes, Oksana Yakhnenko,
and Nicolas Usunier. 2013a. Connecting language
and knowledge bases with embedding models for re-
lation extraction. In ACL.

Jason Weston, Antoine Bordes, Oksana Yakhnenko,
and Nicolas Usunier. 2013b. Connecting language
and knowledge bases with embedding models for re-
lation extraction. In EMNLP.

Haiyang Wu, Daxiang Dong, Xiaoguang Hu, Dian-
hai Yu, Wei He, Hua Wu, Haifeng Wang, and Ting
Liu. 2014. Improve statistical machine translation
with context-sensitive bilingual semantic embedding
model. In EMNLP.

Jiawei Wu, Ruobing Xie, Zhiyuan Liu, and Maosong
Sun. 2016. Knowledge representation via joint
learning of sequential text and knowledge graphs.
CoRR.

Min Xiao and Yuhong Guo. 2014. Distributed word
representation learning for cross-lingual dependency
parsing. In CoNLL.

Ikuya Yamada, Hiroyuki Shindo, Hideaki Takeda, and
Yoshiyasu Takefuji. 2016. Joint learning of the em-
bedding of words and entities for named entity dis-
ambiguation. In CoNLL.

Ikuya Yamada, Hiroyuki Shindo, Hideaki Takeda, and
Yoshiyasu Takefuji. 2017. Learning distributed rep-
resentations of texts and entities from knowledge
base. TACL.

Bishan Yang and Tom M. Mitchell. 2017. Leverag-
ing knowledge bases in lstms for improving machine
reading. In ACL.

Daojian Zeng, Kang Liu, Yubo Chen, and Jun Zhao.
2015. Distant supervision for relation extraction
via piecewise convolutional neural networks. In
EMNLP.

Jing Zhang, Yixin Cao, Lei Hou, Juanzi Li, and Hai-Tao
Zheng. 2017a. Xlink: an unsupervised bilingual en-
tity linking system. In Chinese Computational Lin-
guistics and Natural Language Processing Based on
Naturally Annotated Big Data.

Meng Zhang, Yang Liu, Huanbo Luan, and Maosong
Sun. 2017b. Adversarial training for unsupervised
bilingual lexicon induction. In ACL.

Yuan Zhang, David Gaddy, Regina Barzilay, and
Tommi S. Jaakkola. 2016. Ten pairs to tag - multi-
lingual pos tagging via coarse mapping between em-
beddings. In HLT-NAACL.

Hao Zhu, Ruobing Xie, Zhiyuan Liu, and Maosong
Sun. 2017. Iterative entity alignment via joint
knowledge embeddings. In IJCAI.

Will Y. Zou, Richard Socher, Daniel M. Cer, and
Christopher D. Manning. 2013. Bilingual word em-
beddings for phrase-based machine translation. In
EMNLP.


