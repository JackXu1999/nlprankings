



















































Weighting Finite-State Transductions With Neural Context


Proceedings of NAACL-HLT 2016, pages 623–633,
San Diego, California, June 12-17, 2016. c©2016 Association for Computational Linguistics

Weighting Finite-State Transductions With Neural Context

Pushpendre Rastogi and Ryan Cotterell and Jason Eisner
Department of Computer Science, Johns Hopkins University
{pushpendre,ryan.cotterell,eisner}@jhu.edu

Abstract

How should one apply deep learning to tasks
such as morphological reinflection, which
stochastically edit one string to get another? A
recent approach to such sequence-to-sequence
tasks is to compress the input string into a
vector that is then used to generate the out-
put string, using recurrent neural networks. In
contrast, we propose to keep the traditional
architecture, which uses a finite-state trans-
ducer to score all possible output strings, but
to augment the scoring function with the help
of recurrent networks. A stack of bidirec-
tional LSTMs reads the input string from left-
to-right and right-to-left, in order to summa-
rize the input context in which a transducer
arc is applied. We combine these learned fea-
tures with the transducer to define a probabil-
ity distribution over aligned output strings, in
the form of a weighted finite-state automaton.
This reduces hand-engineering of features, al-
lows learned features to examine unbounded
context in the input string, and still permits ex-
act inference through dynamic programming.
We illustrate our method on the tasks of mor-
phological reinflection and lemmatization.

1 Introduction

Mapping one character sequence to another is a
structured prediction problem that arises frequently
in NLP and computational linguistics. Common
applications include grapheme-to-phoneme (G2P),
transliteration, vowelization, normalization, mor-
phology, and phonology. The two sequences may
have different lengths.

Traditionally, such settings have been modeled
with weighted finite-state transducers (WFSTs) with
parametric edge weights (Mohri, 1997; Eisner,
2002). This requires manual design of the transducer
states and the features extracted from those states.
Alternatively, deep learning has recently been tried

for sequence-to-sequence transduction (Sutskever et
al., 2014). While training these systems could dis-
cover contextual features that a hand-crafted para-
metric WFST might miss, they dispense with impor-
tant structure in the problem, namely the monotonic
input-output alignment. This paper describes a nat-
ural hybrid approach that marries simple FSTs with
features extracted by recurrent neural networks.

Our novel architecture allows efficient modeling
of globally normalized probability distributions over
string-valued output spaces, simultaneously with au-
tomatic feature extraction. We evaluate on morpho-
logical reinflection and lemmatization tasks, show-
ing that our approach strongly outperforms a stan-
dard WFST baseline as well as neural sequence-to-
sequence models with attention. Our approach also
compares reasonably with a state-of-the-art WFST
approach that uses task-specific latent variables.

2 Notation and Background

Let Σx be a discrete input alphabet and Σy be a dis-
crete output alphabet. Our goal is to define a con-
ditional distribution p(y | x) where x ∈ Σ∗x and
y ∈ Σ∗y and x and y may be of different lengths.

We use italics for characters and boldface for
strings. xi denotes the ith character of x, and xi:j de-
notes the substring xi+1xi+2 · · ·xj of length j− i ≥
0. Note that xi:i = ε, the empty string. Let n = |x|.

Our approach begins by hand-specifying an un-
weighted finite-state transducer (FST), F , that non-
deterministically maps any well-formed input x to
all appropriate outputs y. An FST is a directed graph
whose vertices are called states, and whose arcs are
each labeled with some pair s :t, representing a pos-
sible edit of a source substring s ∈ Σ∗x into a target
substring t ∈ Σ∗y. A path π from the FST’s initial
state to its final state represents an alignment of x
to y, where x and y (respectively) are the concate-
nations of the s and t labels of the arcs along π. In

623



3210 s a y

Figure 1: An automaton encoding the English word say.

$

?:a

?:s
s

a

?:s

Σ:ε

?:a

Σ:ε

?:s

?:aΣ:ε

Figure 2: An example transducer F , whose state remem-
bers the most recent output character (or $ if none). Only
a few of the states are shown, with all arcs among them.
The Σ wildcard matches any symbol in Σx; the “?” wild-
card matches the empty string ε or any symbol in Σx.

general, two strings x,y can be aligned through ex-
ponentially many paths, via different edit sequences.

If we represent x as a straight-line finite-state au-
tomaton (Figure 1), then composing x with F (Fig-
ure 2) yields a new FST, G (Figure 3). The paths in
G are in 1-1 correspondence with exactly the paths
in F that have input x. G can have cycles, allowing
outputs of unbounded length.

Each path in G represents an alignment of x to
some string in Σ∗y. We say p(y | x) is the total prob-
ability of all paths in G that align x to y (Figure 4).

But how to define the probability of a path? Tra-
ditionally (Eisner, 2002), each arc in F would also
be equipped with a weight. The weight of a path in
F , or the corresponding path in G, is the sum of its
arcs’ weights. We would then define the probability
p(π) of a path π in G as proportional to expw(π),
where w(·) ∈ R denotes the weight of an object.

The weight of an arc h© s:t−→ h′© in F is traditionally
defined as a function of features of the edit s : t and
the names (h, h′) of the source and target states. In
effect, h summarizes the alignment between the pre-
fixes of x and y that precede this edit, and h′ sum-
marizes the alignment of the suffixes that follow it.

Thus, while the weight of an edit s :t may depend
on context, it traditionally does so only through h
and h′. So if F has k states, then the edit weight can
only distinguish among k different types of preced-
ing or following context.

0, s

0, a

1, s

1, a

2, s

2, a 3, a

3, s

ε:sε:sε:sε:s s:s a:s y:s

s:ε a:ε y:ε

s:ε a:ε y:ε

ε:s

ε:aε:aε:aε:a s:a a:a y:a

ε:a ε:s ε:a ε:s ε:a ε:s ε:a
s:a

s:s

a:a

a:s

y:a

y:s
0, $

ε:s

ε:a

s:a

s:s

Figure 3: An example of the transducer G, which pairs
the string x=say with infinitely many possible strings y.
ThisGwas created as the composition of the straight-line
input automaton (Figure 1) and the transducer F (Fig-
ure 2). Thus, the state of G tracks the states of those two
machines: the position in x and the most recent output
character. To avoid a tangled diagram, this figure shows
only a few of the states (the start state plus all states of
the form i,s© or i,a©), with all arcs among them.
3 Incorporating More Context

That limitation is what we aim to correct in this
paper, by augmenting our representation of con-
text. Our contextual weighting approach will assign
weights directly to G’s arcs, instead of to F ’s arcs.

Each arc of G can be regarded as a “token” of
an edit arc in F : it “applies” that edit to a particu-
lar substring of x. It has the form i,h© s:t−→j,h′©, and
represents the replacement of xi:j = s by t. The
finite-state composition construction produced this
arc of G by combining the arc h© s:t−→ h′© in F with
the path i© s j© in the straight-line automaton rep-
resenting x. The latter automaton uses integers as
state names: it is 0© x1−→ 1© x2−→ . . . xn−→ n©.

Our top-level idea is to make the weight of this arc
in G depend also on (x, i, j), so that it can consider
unbounded input context around the edit’s location.
Arc weights can now consider arbitrary features of
the input x and the position i, j—exactly like the
potential functions of a linear-chain conditional ran-
dom field (CRF), which also defines p(y | x).

Why not just use a CRF? That would only model a
situation that enforced |y| = |x| with each character
yi aligned to xi, since the emissions of a CRF corre-
spond to edits s : t with |s| = |t| = 1. An FST can
also allow edits with |s| 6= |t|, if desired, so it can be
fit to (x,y) pairs of different lengths with unknown

624



0,$,0

0,s,1

1,$,0

1,s,1

2,$,0

2,s,1

0,a,2 1,a,2 2,a,2

0,i,3 1,i,3 2,i,3

0,d,4 1,d,4 2,d,4 3,d,4

3,$,0

3,s,1

3,a,2

3,i,3

ε:a
ε:s

ε:i
ε:d

ε:a
ε:s

ε:i
ε:d

ε:s

ε:s

ε:a ε:a

ε:i

ε:i

ε:d

ε:d

s:ε a:ε y:ε

s:ε a:ε y:ε

s:ε a:ε y:ε

s:ε a:ε y:ε

s:ε a:ε y:ε

s:s

a:a

y:ia:i

a:d

a:s

y:a

y:s

y:d

s:a

s:i

s:d

Figure 4: A compact lattice of the exponentially many
paths in the transducer G of Figure 3 that align in-
put string x=say with output string y=said. To find
p(y | x), we must sum over these paths (i.e., alignments).
The lattice is created by composing G with y, which se-
lects all paths in G that output y. Note that horizontal
movement makes progress through x; vertical movement
makes progress through y. The lattice’s states specialize
states in G so that they also record a position in y.

alignment, summing over their possible alignments.
A standard weighted FST F is similar to a dy-

namic linear-chain CRF. Both are unrolled against
the input x to get a dynamic programming lattice G.
But they are not equivalent. By weighting G instead
of F , we combine the FST’s advantage (aligning
unequal-length strings x,y via a latent path) with
the CRF’s advantage (arbitrary dependence on x).

To accomplish this weighting in practice, sec-
tions 4–5 present a trainable neural architecture for
an arc weight function w = f(s, t, h, h′,x, i, j).
The goal is to extract continuous features from all
of x. While our specific architecture is new, we are
not the first to replace hand-crafted log-linear mod-
els with trainable neural networks (see section 9).1

Note that as in a CRF, our arc weights cannot
consider arbitrary features of y, only of x. Still, a
weight’s dependence on states h, h′ does let it de-
pend on a finite amount of information about y (also
possible in CRFs/HCRFs) and its alignment to x.

In short, our model p(y | x) makes the weight of
1For CRFs, this sacrifices the convexity of the log-likelihood

training objective. But for FSTs, that objective was usually non-
convex to begin with, because the alignment path is latent.

an s :t edit, applied to substring xi:j , depend jointly
on s :t and two summaries of the edit’s context:
• a finite-state summary (h, h′) of its context in

the aligned (x,y) pair, as found by the FST F
• a vector-valued summary of the context in x

only, as found by a recurrent neural network
The neural vector is generally a richer summary of
the context, but it considers only the input-side con-
text. We are able to efficiently extract these rich fea-
tures from the single input x, but not from each of
the very many possible outputs y. The job of the
FST F is to compute additional features that also
depend on the output.2 Thus our model of p(y | x)
is defined by an FST together with a neural network.

4 Neural Context Features

Our arc weight function f will make use of a vec-
tor γi:j (computed from x, i, j) to characterize the
substring xi:j that is being replaced, in context. We
define γi:j as the concatenation of a left vector αj
(describing the prefix x0:j) and a right vector βi
(describing the suffix xi:n), which characterize xi:j
jointly with its left or right context. We use γi to
abbreviate γi−1:i, just as xi abbreviates xi−1:i.

To extract αj , we read the string x one character
at a time with an LSTM (Hochreiter and Schmidhu-
ber, 1997), a type of trainable recurrent neural net-
work that is good at extracting relevant features from
strings. αj is the LSTM’s output after j steps (which
read x0:j). Appendix A reviews how αj ∈ Rq is
computed for j = 1, . . . , n using the recursive, dif-
ferentiable update rules of the LSTM architecture.

We also read the string x in reverse with a second
LSTM. βi ∈ Rq is the second LSTM’s output after
n− i steps (which read reverse(xi:n)).

We regard the two LSTMs together as a BiLSTM
function (Graves and Schmidhuber, 2005) that reads
x (Figure 5). For each bounded-length substring
xi:j , the BiLSTM produces a characterization γi:j
of that substring in context, in O(n) total time.

We now define a “deep BiLSTM,” which stacks
up K BiLSTMs. This deepening is aimed at ex-
tracting the kind of rich features that Sutskever et al.

2Each arc of G is used in a known input context, but could
be reused in many output contexts—different paths in G. Those
contexts are only guaranteed to share h, h′. So the arc weight
cannot depend on any other features of the output context.

625



s a y
�1 �2

↵3↵2↵1

�3

↵0

�0

�1 �2 �3

Figure 5: A level-1 BiLSTM reading the word x=say.

↵
(k)
0 ↵

(k)
1 ↵

(k)
2 ↵

(k)
3

�
(k)
3�

(k)
2�

(k)
1�

(k)
0

�
(k)
1 �

(k)
2

�
(k�1)
3�

(k�1)
2�

(k�1)
1

�
(k)
3

Figure 6: Level k > 1 of a deep BiLSTM. (We augment
the shown input vectors with level k− 1’s input vectors.)

(2014) and Vinyals et al. (2015) found so effective
in a different structured prediction architecture.

The kth-level BiLSTM (Figure 6) reads a se-
quence of input vectors x(k)1 ,x

(k)
2 , . . . ,x

(k)
n ∈

Rd(k) , and produces a sequence of vectors
γ

(k)
1 ,γ

(k)
2 , . . . ,γ

(k)
n ∈ R2q. At the initial level

k = 1, we define x(1)i = exi ∈ Rd
(1)

, a vector em-
bedding of the character xi ∈ Σx. For k > 1, we
take x(k)i to be γ

(k−1)
i , concatenated with x

(k−1)
i for

good measure. Thus, d(k) = 2q + d(k−1).
After this deep generalization, we define γi:j to be

the concatenation of all γ(k)i:j (rather than just γ
(K)
i:j ).

This novel deep BiLSTM architecture has more
connections than a pair of deep LSTMs, since α(k)i
depends not only on α(k−1)i but also on β

(k−1)
i .

Thus, while we may informally regard α(k)i as be-
ing a deep summary of the prefix x0:i, it actually
depends indirectly on all of x (except when k = 1).

5 The Arc Weight Function

Given the vector γi:j , we can now compute the
weight of the edit arc i,h© s:t−→j,h′© in G, namely w =
f(s, t, h, h′,x, i, j). Many reasonable functions are
possible. Here we use one that is inspired by the log-
bilinear language model (Mnih and Hinton, 2007):

w = (es,γi,j , exi , exj+1) · rt,h,h′,type(s:t) (1)
The first argument to the inner product is an em-

bedding es ∈ Rd(1) of the source substring s, con-

catenated to the edit’s neural context and also (for
good measure) its local context.3 For example, if
|s| = 1, i.e. s is a single character, then we would
use the embedding of that character as es. Note
that the embeddings es for |s| = 1 are also used
to encode the local context characters and the level-
1 BiLSTM input. We learn these embeddings, and
they form part of our model’s parameter vector θ.

The second argument is a joint embedding of the
other properties of the edit: the target substring t,
the edit arc’s state labels from F , and the type of the
edit (INS, DEL, or SUB: see section 8). When re-
placing s in a particular context, which fixes the first
argument, we will prefer those replacements whose
r embeddings yield a high inner product w. We will
learn the r embeddings as well; note that their di-
mensionality must match that of the first argument.

The model’s parameter vector θ includes the
O((d(K))2) parameters from the 2K LSTMs, where
d(K) = O(d(1) + Kq). It also O(d(1)S) parame-
ters for the embeddings es of the S different input
substrings mentioned by F , and O(d(K)T ) for the
embeddings rt,h,h′,type(s:t) of the T “actions” in F .

6 Training

We train our model by maximizing the conditional
log-likelihood objective,∑

(x,y∗)∈dataset
log p(y∗ | x) (2)

Recall that p(y∗ | x) sums over all alignments. As
explained by Eisner (2002), it can be computed as
the pathsum of the composition G ◦ y∗ (Figure 4),
divided by the pathsum of G (which gives the nor-
malizing constant for the distribution p(y | x)). The
pathsum of a weighted FST is the total weight of all
paths from the initial state to a final state, and can be
computed by the forward algorithm.4

3Our present implementation handles INS edits (for which
j = i) a bit differently, using (exi+1 ,γi:i+1, exi , exi+2) rather
than (eε,γi:i, exi , exi+1). This is conveniently the same vector
that is used for all other competing edits at this i position (as
they all have |s| = 1 in our present implementation); it provides
an extra character of lookahead.

4If an FST has cycles, such as the self-loops in the example
of Figure 3, then the forward algorithm’s recurrence equations
become cyclic, and must be solved as a linear system rather than
sequentially. This is true regardless of how the FST’s weights

626



Eisner (2002) and Li and Eisner (2009) also ex-
plain how to compute the partial derivatives of
p(y∗ | x) with respect to the arc weights, essen-
tially by using the forward-backward algorithm. We
backpropagate further from these partials to find the
gradient of (2) with respect to all our model param-
eters. We describe our gradient-based maximization
procedure in section 10.3, along with regularization.

Our model does not have to be trained with the
conditional log likelihood objective. It could be
trained with other objectives such as empirical risk
or softmax-margin (Li and Eisner, 2009; Gimpel and
Smith, 2010), or with error-driven updates such as in
the structured perceptron (Collins, 2002).

7 Inference and Decoding

For a new input x at test time, we can now construct
a weighted FST, G, that defines a probability distri-
bution over all aligned output strings. This can be
manipulated to make various predictions about y∗

and its alignment.
In our present experiments, we find the most prob-

able (highest-weighted) path in G (Dijkstra, 1959),
and use its output string ŷ as our prediction. Note
that Dijkstra’s algorithm is exact; no beam search is
required as in some neural sequence models.

On the other hand, ŷ may not be the most prob-
able string—extracting that from a weighted FST is
NP-hard (Casacuberta and de la Higuera, 1999). The
issue is that the total probability of each y is split
over many paths. Still, this is a well-studied prob-
lem in NLP. Instead of the Viterbi approximation,
we could have used a better approximation, such as
crunching (May and Knight, 2006) or variational de-
coding (Li et al., 2009). We actually did try crunch-
ing the 10000-best outputs but got no significant im-
provement, so we do not report those results.

8 Transducer Topology

In our experiments, we choose F to be a simple con-
textual edit FST as illustrated in Figure 2. Just as in
Levenshtein distance (Levenshtein, 1966), it allows
all edits s : t where |s| ≤ 1, |t| ≤ 1, |s| + |t| 6= 0.
We consider the edit type to be INS if s = ε, DEL if

are defined. (For convenience, our experiments in this paper
avoid cycles by limiting consecutive insertions: see section 8.)

t = ε, and SUB otherwise. Note that copy is a SUB
edit with s = t.

For a “memoryless” edit process (Ristad and
Yianilos, 1996), the FST would require only a sin-
gle state. By contrast, we use |Σx|+ 1 states, where
each state records the most recent output character
(initially, a special “beginning-of-string” symbol $).
That is, the state label h is the “history” output char-
acter immediately before the edit s : t, so the state
label h′ is the history before the next edit, namely the
final character of ht. For edits other than DEL, ht is
a bigram of y, which can be evaluated (in context)
by the arc weight function w = f(s, t, h, h′,x, i, j).

Naturally, a weighted version of this FST F is far
too simple to do well on real NLP tasks (as we show
in our experiments). The magic comes from instead
weightingG so that we can pay attention to the input
context γi:j .

The above choice of F corresponds to the
“(0, 1, 1) topology” in the more general scheme of
Cotterell et al. (2014). For practical reasons, we
actually modify it to limit the number of consec-
utive INS edits to 3.5 This trick bounds |y| to be
< 4 · (|x| + 1), ensuring that the pathsums in sec-
tion 6 are finite regardless of the model parameters.
This simplifies both the pathsum algorithm and the
gradient-based training (Dreyer, 2011). Less im-
portantly, since G becomes acyclic, Dijkstra’s algo-
rithm in section 7 simplifies to the Viterbi algorithm.

9 Related Work

Our model adds to recent work on linguistic se-
quence transduction using deep learning.

Graves and Schmidhuber (2005) combined BiL-
STMs with HMMs. Later, “sequence-to-sequence”
models were applied to machine translation by
Sutskever et al. (2014) and to parsing by Vinyals
et al. (2015). That framework did not model any
alignment between x and y, but adding an “atten-
tion” mechanism provides a kind of soft alignment
that has improved performance on MT (Bahdanau et
al., 2015). Faruqui et al. (2016) apply these meth-
ods to morphological reinflection (the only other ap-
plication to morphology we know of). Grefenstette

5This multiplies the number of states 4-fold, since each state
must also record a count in [0, 3] of immediately preceding INS
edits. No INS edit arc is allowed from a state with counter 3.
The counter is not considered by the arc weight function.

627



et al. (2015) recently augmented the sequence-to-
sequence framework with a continuous analog of
stack and queue data structures, to better handle
long-range dependencies often found in linguistic
data.

Some recent papers have used LSTMs or BiL-
STMs, as we do, to define probability distributions
over action sequences that operate directly on an in-
put sequence. Such actions are aligned to the in-
put. For example, Andor et al. (2016) score edit
sequences using a globally normalized model, and
Dyer et al. (2015) evaluate the local probability of
a parsing action given past actions (and their result)
and future words. These architectures are powerful
because their LSTMs can examine the output struc-
ture; but as a result they do not permit dynamic pro-
gramming and must fall back on beam search.

Our use of dynamic programming for efficient ex-
act inference has long been common in non-neural
architectures for sequence transduction, including
FST systems that allow “phrasal” replacements s : t
where |s|, |t| > 1 (Chen, 2003; Jiampojamarn et
al., 2007; Bisani and Ney, 2008). Our work aug-
ments these FSTs with neural networks, much as
others have augmented CRFs. In this vein, Durrett
and Klein (2015) augment a CRF parser (Finkel et
al., 2008) to score constituents with a feedforward
neural network. Likewise, FitzGerald et al. (2015)
employ feedforward nets as a factor in a graphical
model for semantic role labeling. Many CRFs have
incorporated feedforward neural networks (Bridle,
1990; Peng et al., 2009; Do and Artieres, 2010;
Vinel et al., 2011; Fujii et al., 2012; Chen et al.,
2015, and others). Some work augments CRFs with
BiLSTMs: Huang et al. (2015) report results on
part-of-speech tagging and named entity recognition
with a linear-chain CRF-BiLSTM, and Kong et al.
(2015) on Chinese word segmentation and handwrit-
ing recognition with a semi-CRF-BiLSTM.

10 Experiments

We evaluated our approach on two morphological
generation tasks of reinflection (section 10.1) and
lemmatization (section 10.2). In the reinflection
task, the goal is to transduce verbs from one inflected
form into another, whereas the lemmatization task
requires the model to reduce an inflected verb to its

root form.
We compare our WFST-LSTM against two stan-

dard baselines, a WFST with hand-engineered fea-
tures and the Moses phrase-based MT system
(Koehn et al., 2007), as well as the more complex
latent-variable model of Dreyer et al. (2008). The
comparison with Dreyer et al. (2008) is of noted in-
terest since their latent variables are structured par-
ticularly for morphological transduction tasks—we
are directly testing the ability of the LSTM to struc-
ture its hidden layer as effectively as linguistically
motivated latent-variables. Additionally, we provide
detailed ablation studies and learning curves which
show that our neural-WFSA hybrid model can gen-
eralize even with very low amounts of training data.

10.1 Morphological Reinflection

Following Dreyer (2011), we conducted our ex-
periments on the following transduction tasks from
the CELEX (Baayen et al., 1993) morphological
database: 13SIA 7→ 13SKE, 2PIE 7→ 13PKE, 2PKE
7→ z and rP 7→ pA.6 We refer to these tasks as 13SIA,
2PIE, 2PKE and rP, respectively.

Concretely, each task requires us to map a Ger-
man inflection into another inflection. Consider
the 13SIA task and the German verb abreiben
(“to rub off”). We require the model to learn
to map a past tense form abrieb to a present
tense form abreibe—this involves a combination
of stem change and affix generation. Sticking
with the same verb abreiben, task 2PIE requires
the model to transduce abreibt to abreiben—
this requires an insertion and a substitution at
the end. The tasks 2PKE and rP are somewhat
more challenging since performing well on these
tasks requires the model to learn complex trans-
duction: abreiben to abzureiben and abreibt
to abgerieben, respectively. These are complex
transductions with phenomenon like infixation in
specific contexts (abzurieben) and circumfixation
(abgerieben) along with additional stem and af-
fix changes. See Dreyer (2011) for more details and
examples of these tasks.

We use the datasets of Dreyer (2011). Each exper-
6Glossary: 13SIA=1st/3rd sg. ind. past; 13SKE=1st/3rd

sg. subjunct. pres.; 2PIE=2nd pl. ind. pres.; 13PKE=1st/3rd
pl. subjunct. pres.; 2PKE=2nd pl. subjunct. pres.; z=infinitive;
rP=imperative pl.; pA=past part.

628



iment sampled a different dataset of 2500 examples
from CELEX, dividing this into 500 training + 1000
validation + 1000 test examples. Like them, we re-
port exact-match accuracy on the test examples, av-
eraged over 5 distinct experiments of this form. We
also report results when the training and validation
data are swapped in each experiment, which doubles
the training size.

10.2 Lemmatization

Lemmatization is a special case of morphological re-
inflection where we map an inflected form of a word
to its lemma (canonical form), i.e., the target inflec-
tion is fixed. This task is quite useful for NLP, as
dictionaries typically list only the lemma for a given
lexical entry, rather than all possible inflected forms.
In the case of German verbs, the lemma is taken to
be the infinitive form, e.g., we map the past partici-
ple abgerieben to the infinitive abreiben.

Following Dreyer (2011), we use a subset of
the lemmatization dataset created by Wicentowski
(2002) and perform 10-fold experiments on four lan-
guages: Basque (5843), English (4915), Irish (1376)
and Tagalog (9545), where the numbers in paren-
thesis indicate the total number of data pairs avail-
able. For each experimental fold the total data was
divided into train, development and test sets in the
proportion of 80:10:10 and we report test accuracy
averaged across folds.

10.3 Settings and Training Procedure

We set the hyperparameters of our model to K = 4
(stacking depth), d(1) = 10 (character embedding
dimension), and q = 15 (LSTM state dimension).
The alphabets Σx and Σy are always equal; their size
is language-dependent, typically ≈ 26 but larger in
languages like Basque and Irish where our datasets
include properly accented characters. With |Σ| = 26
and the above settings for the hyperparameters, the
number of parameters in our models is 352, 801.

We optimize these parameters through stochas-
tic gradient descent of the negative log-likelihood
objective, and regularize the training procedure
through dropout accompanied with gradient clip-
ping and projection of parameters onto L2-balls with
small radii, which is equivalent to adding a group-
ridge regularization term to the training objective.
The learning rate decay schedule, gradient clipping

Model 13SIA 2PIE 2PKE rP
Moses15 85.3 94.0 82.8 70.8
Dreyer (Backoff) 82.8 88.7 74.7 69.9
Dreyer (Lat-Class) 84.8 93.6 75.7 81.8
Dreyer (Lat-Region) 87.5 93.4 87.4 84.9

BiLSTM-WFST 85.1 94.4 85.5 83.0

Model Ensemble 85.8 94.6 86.0 83.8

BiLSTM-WFST (2 × Data) 87.6 94.8 88.1 85.7
Table 1: Exact match accuracy on the morphological re-
inflection task. All the results in the first half of the table
are taken from Dreyer (2011), whose experimental setup
we copied exactly. The Moses15 result is obtained by ap-
plying the SMT toolkit Moses (Koehn et al., 2007) over
letter strings with 15-character context windows. Dreyer
(Backoff) refers to the ngrams+x model which has access
to all the “backoff features.” Dreyer (Lat-Class) is the
ngrams+x+latent class model, and Dreyer (Lat Region)
refers to the ngrams+x+latent class + latent change re-
gion model. The “Model Ensemble” row displays the
performance of an ensemble including our full model and
the 7 models that we performed ablation on. In each col-
umn, we boldfaced the highest result and those that were
not significantly worse (sign test, p < 0.05). Finally, the
last row reports the performance of our BiLSTM-WFST
when trained on twice the training data.

threshold, radii of L2-balls, and dropout frequency
were tuned by hand on development data.

In our present experiments, we made one change
to the architecture. Treating copy edits like other
SUB edits led to poor performance: the system was
unable to learn that all SUB edits with s = t were ex-
tremely likely. In the experiments reported here, we
addressed the problem by simply tying the weights
of all copy edits regardless of context, bypassing (1)
and instead setting w = c where c is a learned pa-
rameter of the model. See section 11 for discussion.

10.4 Results

Table 1 and 2 show our results. We can see that
our proposed BiLSTM-WFST model always outper-
forms all but the most complex latent-variable model
of Dreyer (2011); it is competitive with that model,
but only beats it once individually. All of Dreyer’s
models include output trigram features, while we
only use bigrams.

Figure 7 shows learning curves for the 13SIA and
2PKE tasks: test accuracy when we train on less
data. Curiously, at 300 data points the performance
of our model is tied to Dreyer (2011). We also note

629



Model Basque English Irish Tagalog
Base (W) 85.3 91.0 43.3 0.3
WFAffix (W) 80.1 93.1 70.8 81.7
ngrams (D) 91.0 92.4 96.8 80.5

ngrams + x (D) 91.1 93.4 97.0 83.0
ngrams + x + l (D) 93.6 96.9 97.9 88.6

BiLSTM-WFST 91.5 94.5 97.9 97.4
Table 2: Lemmatization results on Basque, English, Irish
and Tagalog. Comparison systems marked with (W) are
taken from Wicentowski (2002) and systems marked with
a (D) are taken from Dreyer (2011). We outperform base-
lines on all languages and are competitive with the latent-
variable approach (ngrams + x + l), beating it in two
cases: Irish and Tagalog.

50100 300 500 1000
55

60

65

70

75

80

85

90

A
cc

u
ra

cy

2PKE

50100 300 500 1000
72

74

76

78

80

82

84

86

88
13SIA

BiLSTM-WFST

Dreyer (Lat-Region)

Dreyer (Backoff)

Moses15

Figure 7: Learning Curves

that our model always outperforms the Moses15
baseline on all training set sizes except on the 2PKE
task with 50 training samples.

In general, the results from our experiments are
a promising indicator that LSTMs are capable of
extracting linguistically relevant features for mor-
phology. Our model outperforms all baselines, and
is competitive with and sometimes surpasses the
latent-variable model of Dreyer et al. (2008) without
any of the hand-engineered features or linguistically
inspired latent variables.

On morphological reinflection, we outperform all
of Dreyer et al’s models on 2PIE, but fall short of his
latent-change region model on the other tasks (out-
performing the other models). On lemmatization,
we outperform all of Wicentowski’s models on all
the languages and all of Dreyer et al.’s models on
Irish and Tagalog, but, but not on English and Irish.
This suggests that perhaps further gains are possible
through using something like Dreyer’s FST as our
F . Indeed, this would be compatible with much re-
cent work that gets best results from a combination
of automatically learned neural features and hand-

Systems 13SIA 2PIE 2PKE rP
Deep BiLSTM w/ Tying 86.8 94.8 87.9 81.1

Deep BiLSTM (No Context) 86.5 94.3 87.8 78.8
Deep BiLSTMs w/o Copying 86.5 94.6 86.5 80.7
Shallow BiLSTM 86.4 94.7 86.1 80.6
Bi-Deep LSTM 86.1 94.2 86.5 78.6
Deep MonoLSTM 84.0 93.8 85.6 67.3
Shallow MonoLSTM 84.2 94.5 84.9 68.2
No LSTM (Local Context) 83.6 88.5 83.2 68.0
Deep BiLSTM w/o Tying 69.7 78.5 77.9 66.7
No LSTM (No Context) 70.7 84.9 72.4 64.1

Seq2Seq-Att-4Layer 76.0 91.4 81.2 79.3
Seq2Seq-Att-1Layer 77.2 89.6 82.1 79.1
Seq2Seq-4Layer 2.5 5.2 11.5 6.4
Seq2Seq-1Layer 9.1 11.1 14.1 11.9

Table 3: Ablation experiments: Exact-match accuracy of
the different systems averaged on 5 folds of validation
portions of the morphological induction dataset.

engineered features.

10.5 Analysis of Results

We analyzed our lemmatization errors for all the lan-
guages on one fold of the datasets. On the English
lemmatization task, 7 of our 27 errors simply copied
the input word to the output: ate, kept, went,
taught, torn, paid, strung. This suggests
that our current aggressive parameter tying for copy
edits may predict a high probability for a copy edit
even in contexts that should not favor it.

Also we found that the FST sometimes produced
non-words while lemmatizing the input verbs. For
example it mapped picnicked 7→ picnick,
happen 7→ hapen, exceed 7→ excy and
lining 7→ lin. Since these strings would be rare
in a corpus, many such errors could be avoided by
a reranking approach that combined the FST’s path
score with a string frequency feature.

In order to better understand our architecture and
the importance of its various components, we per-
formed an ablation study on the validation por-
tions of the morphological induction datasets, shown
in Table 3. We can see in particular that using a BiL-
STM instead of an LSTM, increasing the depth of
the network, and including local context all helped
to improve the final accuracy.

“Deep BiLSTM w/ Tying” refers to our complete
model. The other rows are ablation experiments—
architectures that are the same as the first row except
in the specified way. “Deep BiLSTM (No Context)”

630



omits local context exi , exj+1 from (1). “Deep BiL-
STMs w/o Copying” does not concatenate a copy of
x(k−1)i into x

(k)
i and simplifies γi:j to be γ

(K)
i:j only.

“Shallow BiLSTM” reduces K from 4 to 1. “Bi-
Deep LSTM” replaces our deep BiLSTM with two
deep LSTMs that run in opposite directions but do
not interact with each other. “Deep MonoLSTM”
redefines βi to be the empty vector,g i.e. it replaces
the deep BiLSTM with a deep left-to-right LSTM.
“Shallow MonoLSTM” replaces the deep BiLSTM
with a shallow left-to-right LSTM. “No LSTM (Lo-
cal Context)” omits γi:j from the weight function al-
together. “Deep BiLSTM w/o Tying” does not use
the parameter tying heuristic for copy edits. “No
LSTM (No Context)” is the simplest model that we
consider. It removes γi:j , exi and exj+1 and in fact,
it is precisely a weighting of the edits in our origi-
nal FST F , without further considering the context
in which an edit is applied.

Finally, to compare the performance of our
method to baseline neural encoder-decoder models,
we trained 1-layer and 4-layer neural sequence-to-
sequence models with and without attention, us-
ing the publicly available morph-trans toolkit
(Faruqui et al., 2016). We show the performance
of these models in the lower half of the table. The
results consistently show that sequence-to-sequence
transduction models that lack the constraints of
monotonic alignment perform worse than our pro-
posed models on morphological transduction tasks.

11 Future Work

As neither our FST-LSTM model or the latent-
variable WFST model of Dreyer et al. (2008) uni-
formly outperforms the other, a future direction is
to improve the FST we use in our model—e.g., by
augmenting its states with explicit latent variables.
Other improvements to the WFST would be to in-
crease the amount of history h stored in the states,
and to allow s, t to be longer than a single character,
which would allow the model to segment x.

We are not committed to the arc weight function
in (1), and we believe that further investigation here
could improve performance. The goal is to define
the weight of h© s:t−→ h′© in the context summarized by
αi,βj (the context around xi:j = s) and/or αj ,βi
(which incorporate s as well).

Any parametric function of the variables
(s, t, h, h′,αi,αj ,βi,βj) could be used—for
example, a neural network or a multilinear function.
This function might depend on learned embeddings
of the separate objects s, t, h, h′, but also on learned
joint embeddings of pairs of these objects (which
adds finer-grained parameters), or hand-specified
properties of the objects such as their phonological
features (which adds backoff parameters).

A basic approach along the lines of (1) would
use an inner product of some encoding of the
arc (s, t, h, h′) with some encoding of the context
(s, h, h′,αi,αj ,βi,βj). Note that this formulation
lets the objects s, h, h′ play a dual role—they may
appear as part of the arc and/or as part of the context.
This is because we must judge whether s = xi:j
(with “label” h©−→ h′©) is an appropriate input seg-
ment given the string context around xi:j , but if this
segment is chosen, in turn it provides additional con-
text to judge whether t is an appropriate output.

High-probability edits s : t typically have t ≈ s:
a perfect copy, or a modified copy that changes just
one or two features such as phonological voicing or
orthographic capitalization. Thus, we are interested
in learning a shared set of embeddings for Σx ∪Σy,
and making the arc weight depend on features of the
“discrepancy vector” et − es, such as this vector’s
components7 and their absolute magnitudes, which
would signal discrepancies of various sorts.

12 Conclusions

We have presented a hybrid FST-LSTM architec-
ture for string-to-string transduction tasks. This
approach combines classical finite-state approaches
to transduction and newer neural approaches. We
weight the same FST arc differently in different con-
texts, and use LSTMs to automatically extract fea-
tures that determine these weights. This reduces the
need to engineer a complex topology for the FST
or to hand-engineer its weight features. We evalu-
ated one such model on the tasks of morphological
reinflection and lemmatization. Our approach out-
performs several baselines and is competitive with
(and sometimes surpasses) a latent-variable model
hand-crafted for morphological transduction tasks.

7In various directions—perhaps just the basis directions,
which might come to encode distinctive features, via training.

631



Acknowledgements

This research was supported by the Defense Advanced
Research Projects Agency under the Deep Exploration
and Filtering of Text (DEFT) Program, agreement num-
ber FA8750-13-2-001; by the National Science Founda-
tion under Grant No. 1423276; and by a DAAD fellow-
ship to the second author. We would like to thank Markus
Dreyer for providing the datasets and Manaal Faruqui for
providing implementations of the baseline sequence-to-
sequence models. Finally we would like to thank Mo Yu,
Nanyun Peng, Dingquan Wang and Elan Hourticolon-
Retzler for helpful discussions and early prototypes for
some of the neural network architectures.

References
Daniel Andor, Chris Alberti, David Weiss, Aliaksei

Severyn, Alessandro Presta, Kuzman Ganchev, Slav
Petrov, and Michael Collins. 2016. Globally nor-
malized transition-based neural networks. Available
at arXiv.org as arXiv:1603.06042, March.

R. Harald Baayen, Richard Piepenbrock, and Rijn van H.
1993. The CELEX lexical data base on CD-ROM.

Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Ben-
gio. 2015. Neural machine translation by jointly
learning to align and translate. In Proceedings of
ICLR.

Maximilian Bisani and Hermann Ney. 2008. Joint-
sequence models for grapheme-to-phoneme conver-
sion. Speech Communication, 50(5):434–451.

John S. Bridle. 1990. Training stochastic model recog-
nition algorithms as networks can lead to maximum
mutual information estimation of parameters. In Pro-
ceedings of NIPS, pages 211–217.

Francisco Casacuberta and Colin de la Higuera. 1999.
Optimal linguistic decoding is a difficult compu-
tational problem. Pattern Recognition Letters,
20(8):813–821.

Liang-chieh Chen, Alexander Schwing, Alan Yuille, and
Raquel Urtasun. 2015. Learning deep structured mod-
els. In Proceedings of ICML, pages 1785–1794.

Stanley F. Chen. 2003. Conditional and joint models for
grapheme-to-phoneme conversion. In Proceedings of
EUROSPEECH.

Michael Collins. 2002. Discriminative training meth-
ods for hidden Markov models: Theory and experi-
ments with perceptron algorithms. In Proceedings of
the Conference on Empirical Methods in Natural Lan-
guage Processing, Philadelphia, July.

Ryan Cotterell, Nanyun Peng, and Jason Eisner. 2014.
Stochastic contextual edit distance and probabilistic
FSTs. In Proceedings of ACL, pages 625–630, June.

E. W. Dijkstra. 1959. A note on two problems in con-
nexion with graphs. Numerische Mathematik, 1(1).

Trinh-Minh-Tri Do and Thierry Artieres. 2010. Neu-
ral conditional random fields. In Proceedings of AIS-
TATS. JMLR.

Markus Dreyer, Jason R. Smith, and Jason Eisner. 2008.
Latent-variable modeling of string transductions with
finite-state methods. In Proceedings of EMNLP, pages
1080–1089, October.

Markus Dreyer. 2011. A Non-Parametric Model for the
Discovery of Inflectional Paradigms from Plain Text
Using Graphical Models over Strings. Ph.D. thesis,
Johns Hopkins University, Baltimore, MD, April.

Greg Durrett and Dan Klein. 2015. Neural CRF parsing.
In Proceedings of ACL.

Chris Dyer, Miguel Ballesteros, Wang Ling, Austin
Matthews, and Noah A. Smith. 2015. Transition-
based dependency parsing with stack long short-term
memory. In Proceedings of ACL-IJCNLP, pages 334–
343, July.

Jason Eisner. 2002. Parameter estimation for probabilis-
tic finite-state transducers. In Proceedings of ACL,
pages 1–8, July.

Manaal Faruqui, Yulia Tsvetkov, Graham Neubig, and
Chris Dyer. 2016. Morphological inflection genera-
tion using character sequence to sequence learning. In
Proceedings of NAACL. Code available at https:
//github.com/mfaruqui/morph-trans.

Jenny Rose Finkel, Alex Kleeman, and Christopher D.
Manning. 2008. Efficient, feature-based, conditional
random field parsing. In Proceedings of ACL, pages
959–967.

Nicholas FitzGerald, Oscar Täckström, Kuzman
Ganchev, and Dipanjan Das. 2015. Semantic role
labeling with neural network factors. In Proceedings
of EMNLP, pages 960–970.

Yasuhisa Fujii, Kazumasa Yamamoto, and Seiichi Naka-
gawa. 2012. Deep-hidden conditional neural fields for
continuous phoneme speech recognition. In Proceed-
ings of IWSML.

Kevin Gimpel and Noah A. Smith. 2010. Softmax-
margin CRFs: Training log-linear models with cost
functions. In Proceedings of NAACL-HLT, pages 733–
736, June.

Alex Graves and Jürgen Schmidhuber. 2005. Frame-
wise phoneme classification with bidirectional lstm
and other neural network architectures. Neural Net-
works, 18(5):602–610.

Alex Graves. 2012. Supervised Sequence Labelling with
Recurrent Neural Networks. Springer.

Edward Grefenstette, Karl Moritz Hermann, Mustafa Su-
leyman, and Phil Blunsom. 2015. Learning to trans-
duce with unbounded memory. In Proceedings of
NIPS, pages 1819–1827.

632



Sepp Hochreiter and Jürgen Schmidhuber. 1997. Long
short-term memory. Neural Computation.

Zhiheng Huang, Wei Xu, and Kai Yu. 2015. Bidi-
rectional LSTM-CRF models for sequence tagging.
arXiv preprint arXiv:1508.01991.

Sittichai Jiampojamarn, Grzegorz Kondrak, and Tarek
Sherif. 2007. Applying many-to-many alignments
and hidden Markov models to letter-to-phoneme con-
version. In Proceedings of NAACL-HLT, pages 372–
379.

Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran, Richard
Zens, et al. 2007. Moses: Open source toolkit
for statistical machine translation. In Proceedings of
ACL (Interactive Poster and Demonstration Sessions),
pages 177–180.

Lingpeng Kong, Chris Dyer, and Noah A. Smith. 2015.
Segmental recurrent neural networks. arXiv preprint
arXiv:1511.06018.

Vladimir I. Levenshtein. 1966. Binary codes capable of
correcting deletions, insertions, and reversals. Soviet
Physics Doklady, 10(8):707–710.

Zhifei Li and Jason Eisner. 2009. First- and second-order
expectation semirings with applications to minimum-
risk training on translation forests. In Proceedings of
EMNLP, pages 40–51, August.

Zhifei Li, Jason Eisner, and Sanjeev Khudanpur. 2009.
Variational decoding for statistical machine transla-
tion. In Proceedings of ACL, pages 593–601.

Jonathan May and Kevin Knight. 2006. A better n-best
list: Practical determinization of weighted finite tree
automata. In Proceedings of NAACL, pages 351–358.

Andriy Mnih and Geoffrey Hinton. 2007. Three new
graphical models for statistical language modelling. In
Proceedings of ICML, pages 641–648. ACM.

Mehryar Mohri. 1997. Finite-state transducers in lan-
guage and speech processing. Computational Linguis-
tics, 23(2):269–311.

Jian Peng, Liefeng Bo, and Jinbo Xu. 2009. Conditional
neural fields. In Proceedings of NIPS, pages 1419–
1427.

Eric Sven Ristad and Peter N. Yianilos. 1996. Learning
string edit distance. Technical Report CS-TR-532-96,
Princeton University, Department of Computer Sci-
ence, October. Revised October 1997.

Ilya Sutskever, Oriol Vinyals, and Quoc Le. 2014. Se-
quence to sequence learning with neural networks. In
Proceedings of NIPS.

Antoine Vinel, Trinh Minh Tri Do, and Thierry Artieres.
2011. Joint optimization of hidden conditional ran-
dom fields and non-linear feature extraction. In Pro-
ceedings of ICDAR, pages 513–517. IEEE.

Oriol Vinyals, Łukasz Kaiser, Terry Koo, Slav Petrov,
Ilya Sutskever, and Geoffrey Hinton. 2015. Gram-
mar as a foreign language. In Proceedings of NIPS,
pages 2755–2763.

Richard Wicentowski. 2002. Modeling and Learning
Multilingual Inflectional Morphology in a Minimally
Supervised Framework. Ph.D. thesis, Johns Hopkins
University.

633


