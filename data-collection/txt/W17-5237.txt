



















































deepCybErNet at EmoInt-2017: Deep Emotion Intensities in Tweets


Proceedings of the 8th Workshop on Computational Approaches to Subjectivity, Sentiment and Social Media Analysis, pages 259–263
Copenhagen, Denmark, September 7–11, 2017. c©2017 Association for Computational Linguistics

deepCybErNet at EmoInt-2017: Deep Emotion Intensities in Tweets
Vinayakumar R and Premjith B and Sachin Kumar S and Soman K P

Center for Computational Engineering and Networking,
Amrita School of Engineering, Amrita Vishwa Vidyapeetham,

Amrita University, India
vinayakumarr77@gmail.com

Prabaharan Poornachandran
Center for Cyber Security Systems and Networks,

Amrita School of Engineering, Amrita Vishwa Vidyapeetham,
Amrita University, India

Abstract

This working note presents the method-
ology used in deepCybErNet submission
to the shared task on Emotion Intensities
in Tweets (EmoInt) WASSA-2017. The
goal of the task is to predict a real val-
ued score in the range [0-1] for a particular
tweet with an emotion type. To do this, we
used Bag-of-Words and embedding based
on recurrent network architecture. We
have developed two systems and experi-
ments are conducted on the Emotion In-
tensity shared Task 1 data base at WASSA-
2017. A system which uses word em-
bedding based on recurrent network archi-
tecture has achieved highest 5 fold cross-
validation accuracy. This has used embed-
ding with recurrent network to extract op-
timal features at tweet level and logistic
regression for prediction. These methods
are highly language independent and ex-
perimental results shows that the proposed
methods is apt for predicting a real valued
score in than range [0-1] for a given tweet
with its emotion type.

1 Introduction

Internet has become an essential platform to carry
out daily activities to our lives. People use social
media resources like Twitter, Facebook, What-
sApp, Hike, WeChat etc. to share their language
such as views or emotions, stance over issues,
reviews related to products, services, blogs etc.
In recent days, the amount of language sharing
through the internet is ubiquitous. This neces-
sitates the need of analyzing reviews to identify
the emotions including estimating the degree to
which an emotion is expressed in text. Unlike
natural language, the user reviews are small; rich

information is represented through nonstandard
language such as emoticons, emojis, creatively
spelled words (happee), and hash-tagged words
(#happy). These factors can make a high influence
on the social and economic behavior worldwide
like real-world applications such as marketing, e-
Governance, business intelligence, social analysis
and applications in Natural Language Processing
(NLP) - information extraction, question answer-
ing, textual entailment, etc. Many methods have
been introduced by researchers for emotion anno-
tation work. This gives binary labels for the given
text (Alm et al., 2005), (Aman and Szpakowicz,
2007; Brooks et al., 2013),(Neviarouskaya et al.,
2009), (Bollen et al., 2011), (Summa et al., 2016).
only one annotation work exists for providing a
real valued score as annotation for a given text
(Strapparava and Mihalcea, 2007). This was a task
included in the SemEval-2007 shared task. Many
methods devised for automatic emotion classifica-
tion (Werbos, 1990), (Summa et al., 2016), (Mo-
hammad, 2012), (Bollen et al., 2011), (Aman and
Szpakowicz, 2007), (Brooks et al., 2013). How-
ever, only less amount work exists on emotion
regression other than SemEval-2007 shared task
(Strapparava and Mihalcea, 2007).

In this paper, we use Bag-of-Words (BOW) and
a Bag-of-Words (BOW) based recurrent embed-
ding system for predicting a real valued score in
the range [0-1]. In first case, BOW is used to ob-
tain the feature representation for the tweets and
classification is done using logistic regression. We
also employed an RNN and LSTM based method
for mining the features at tweets level. These
methods are language independent. So irrespec-
tive of the language, we can use these approaches
for finding the stance of micro blogging posts.

The rest of the paper is organized as follows.
Section 2 discusses information of shared task.
Section 3 discusses the proposed methodology.

259



Dataset Anger Fear Joy Sadness
Training 857 1147 823 786
Development 84 110 79 74
Testing 760 995 714 673

Table 1: Statistics of Tweet Emotion Intensity
dataset

Section 4.2 provides experimental analysis and re-
sults and at last conclusion is placed in Section 5.

2 Task description

The Emotion Intensity Task is a shared task
of 8th Workshop on Computational Approaches
to Subjectivity, Sentiment & Social Media
Analysis (WASSA 2017) in conjunction with
the EMNLP 2017 conference (Mohammad and
Bravo-Marquez, 2017). The aim of the task is to
obtain a real valued score in the range [0-1] for
the given tweet including an emotion type . The
tweets in training, validation and testing are from
four different categories such as anger, fear, joy,
sadness. Each tweet has an emotion type with its
score in the range [0-1], where 0 denotes that the
tweet has maximally away from its emotion and 1
denotes that the tweet has maximally closer to its
emotion . The detailed statistics of the data set is
described in Table 1.

3 Methodology

This section provides the information of the pro-
posed approach for predicting a real valued score
in the range [0-1] for a given twee with an emotion
type . We used two approaches; (1) Bag-of-words
(BoW) based word embedding(2) Recurrent Neu-
ral Network (RNN) based word embedding

3.1 Bag-of-words based system for Emotion
Intensities in Tweets

The embedding size was set to 256 so that each
word is now represented using a 256 dimension
vector and word length to 70. Anger, Fear,
Joy and Sadness have 857, 1147, 823 and 786
instances. We constructed matrix of shape
857X70, 1147X70, 823X70 and 786X70 for
training instances and 84X70, 110X70, 79X70
and 74X70 for development instances. Next, we
replace each word with their corresponding word
embedding and this forms an input tensor of shape
857X70X256, 1147X70X256, 823X70X256
and 786X70X256 for training instances and
84X70X256, 110X70X256, 79X70X256 and

74X70X256 for development instances. At
last, an input tensor is transformed to matrix
of shape 857X256, 1147X256, 823X256
and 786X256 for training instances and
84X256, 110X256, 79X256 and 74X256
for development instances using max-pooling
approach. These matrices are passed to logistic
regression and a real valued score is chosen for a
given tweet with an emotion type using argmax
function.

3.2 Recurrent neural network (RNN) based
system for Emotion Intensities in Tweets

Recurrent neural network is largely used deep
learning architecture for sequence data modeling.
This has achieved significant results in various
tasks exists in the field of natural language pro-
cessing (LeCun et al., 2015). It generally looks
same as feed forward networks (FFN), addition-
ally contains self-recurrent connection in units
(Elman, 1990). This cyclic loop carries out in-
formation from one time-step to another. Conse-
quently, RNN are able to learn the temporal pat-
terns by considering the past information in esti-
mating the present states. Generally, RNN takes
input as xt ∈ Rn and hit−1 ∈ Rm of arbitrary
length to compute succeeding hidden state vector
hit by using the following formulae recursively.

ht = f(wxhxt + whhht−1 + b) (1)

ot = sf(wohht + bot) (2)

Where f is the nonlinear activation function, par-
ticularly logistic sigmoid function (σ) applied on
element wise, hi0 is usually initialized to 0 at time-
step t0 and wxh ∈ Rm×n, whh ∈ Rm×m and
b ∈ Rm are arguments of affine transformation.
Here o is the output at time step t.

Using RNN approach, a system was im-
plemented for predicting a real valued score
in the range [0-1] for emotional intensities in
tweets. By following the aforementioned mech-
anism, we constructed an input tensor of shape
857X70X256, 1147X70X256, 823X70X256
and 786X70X256 for training instances and
84X70X256, 110X70X256, 79X70X256 and
74X70X256 for development instances. So the
embedding matrix for the tweets of size 70X256
in both training and development are now reduced
to 256 dimensional vectors. So, embedding
matrices of size 857X256, 1147X256, 823X256
and 786X256 were used as training samples and

260



84X256, 110X256, 79X256 and 74X256 were
taken as development instances and then fed into
the RNN layer followed by logistic regression for
prediction.

3.3 Long short-term memory based system
for Emotion Intensities in Tweets

RNN issues vanishing and exploding gradient is-
sue in memorizing long-term dependencies (Ben-
gio et al., 1994). To reduce, (Hochreiter and
Schmidhuber, 1997) has introduced long short-
term memory (LSTM). Unlike RNN simple units
in recurrent hidden layer, LSTM has introduced
a memory block. A memory block is a complex
processing unit that contains one or more mem-
ory cell, adaptive gates such as input gate and out-
put gate and Constant Error Carousel (CEC). A
memory block stores an information and updates
them across time-steps based on the input and out-
put gates. Input and output gate controls the in-
put and output flow of information to a memory
cell. Additionally, it is has a built-in value as 1 for
constant Error carousel (CEC). This value will be
activated when in the absence of value from the
outside the signal. Moreover, (Gers et al., 1999)
introduced forget gate, (Gers et al., 2002) intro-
duced peephole connections to the memory block
in LSTM. A forget gate facilitates to forget or re-
set the values across time steps and peephole con-
nections helps to learn precise timing of the out-
puts. The newly proposed architecture has per-
formed well in learning long-range temporal de-
pendencies in various artificial intelligence (AI)
tasks (LeCun et al., 2015). Generally, at each time
step an LSTM network considers the following 3
inputs; xt, ht−1, ct−1 and outputs ht, ct through
the following equations

it = σ(wixt + Uiht−1 + Vimt−1 + bi) (3)

ft = σ(wfxt + Ufht−1 + Vfmt−1 + bf ) (4)

ot = σ(woxt + Uoht−1 + Vomt−1 + bo) (5)

m̃t = tanh(wmxt + Umht−1 + bm) (6)

mt = f it � mt−1 + it � m̃ (7)
ht = ot � tanh(mt) (8)

Where xt is the input at time step t, σ is sigmoid
non-linear activation function, tanh is hyperbolic
tangent non-linear activation function,� denotes
element-wise multiplication. Concretely, at t = 0

Method Emotion Pearson Spearman

Bow

Anger 0.677 0.697
Fear 0.675 0.685
Joy 0.601 0.621
Sadness 0.657 0.647

RNN

Anger 0.718 0.707
Fear 0.715 0.75
Joy 0.601 0.721
Sadness 0.707 0.71

LSTM

Anger 0.721 0.736
Fear 0.72 0.753
Joy 0.621 0.725
Sadness 0.737 0.724

Table 2: 5-fold cross validation with embedding
vector size 128

hidden and memory cell state vectors such as h0
and c0 are initialized to 0.

We followed subsections 3.1 and 3.2 to develop
a LSTM based system for predicting a real valued
score in the range [0-1] for a given emotion includ-
ing its emotion type. This system is constructed by
simple replacing RNN layer with LSTM.

4 Experiments

All deep learning architecture are trained using
GPU enabled TensorFlow (Abadi et al., 2016)
with backpropogation through time (BPTT) (Wer-
bos, 1990).

4.1 Parameter Selection

To choose optimal parameter for embedding size,
the LSTM model is trained with embedding size
128 and 256 and the performance of them is eval-
uated on the development data set. The detailed
evaluation results are displayed in Tables 2 and 3.
We didn’t use any hyper parameter tuning mecha-
nism for tweet length instead we used static length
70 in all our experiments.

4.2 Evaluation results

We have submitted one run based on LSTM based
recurrent embedding system to WASSA2017 and
the detailed results is displayed in Tables 4 and 5

Analysis of training results and testing results
showed that there is a significant difference in the
performance measure. This is due to the overfit-
ting of the model to the training data because, a
deep learning framework requires huge amount of
data to learn the features. Unavailability of such

261



Method Emotion Pearson Spearman

BoW

Anger 0.681 0.71
Fear 0.682 0.695
Joy 0.611 0.632
Sadness 0.661 0.654

RNN

Anger 0.721 0.714
Fear 0.724 0.761
Joy 0.613 0.742
Sadness 0.714 0.721

LSTM

Anger 0.731 0.741
Fear 0.741 0.764
Joy 0.634 0.732
Sadness 0.739 0.731

Table 3: 5-fold cross validation with embedding
vector size 256

sufficient training data samples caused the overfit-
ting of the system. This in turn affected the accu-
racy of prediction.

5 Conclusion

This working note has presented a language in-
dependent approach based on BoW and recurrent
based embedding for predicting a real valued score
in the range [0-1] for a given tweet with an emo-
tion type. LSTM network has outperformed both
bag-of-words embedding and recurrent based em-
bedding mechanism. This is primarily due to
the fact that LSTM has capability to learn long-
temporal dependencies across time steps. Due to
less number of instances in training data, the accu-
racy of the proposed mechanism is less. Though,
the efficacy of embedding’s of RNN and LSTM is
considerable and paves the manner in future to use
for predicting real valued score in the range [0-1]
with more training instances including its emotion
type. To justify that the proposed deep learning
mechanism has capability to perform better with
large amount of instances will be remained as one
direction towards future work.

References

Martı́n Abadi, Paul Barham, Jianmin Chen, Zhifeng
Chen, Andy Davis, Jeffrey Dean, Matthieu Devin,
Sanjay Ghemawat, Geoffrey Irving, Michael Is-
ard, et al. 2016. Tensorflow: A system for large-
scale machine learning. In Proceedings of the 12th
USENIX Symposium on Operating Systems Design
and Implementation (OSDI). Savannah, Georgia,
USA.

Cecilia Ovesdotter Alm, Dan Roth, and Richard
Sproat. 2005. Emotions from text: machine learn-
ing for text-based emotion prediction. In Proceed-
ings of the conference on human language technol-
ogy and empirical methods in natural language pro-
cessing. Association for Computational Linguistics,
pages 579–586.

Saima Aman and Stan Szpakowicz. 2007. Identify-
ing expressions of emotion in text. In International
Conference on Text, Speech and Dialogue. Springer,
pages 196–205.

Yoshua Bengio, Patrice Simard, and Paolo Frasconi.
1994. Learning long-term dependencies with gradi-
ent descent is difficult. IEEE transactions on neural
networks 5(2):157–166.

Johan Bollen, Huina Mao, and Alberto Pepe. 2011.
Modeling public mood and emotion: Twitter sen-
timent and socio-economic phenomena. ICWSM
11:450–453.

Michael Brooks, Katie Kuksenok, Megan K Torkild-
son, Daniel Perry, John J Robinson, Taylor J Scott,
Ona Anicello, Ariana Zukowski, Paul Harris, and
Cecilia R Aragon. 2013. Statistical affect detec-
tion in collaborative chat. In Proceedings of the
2013 conference on Computer supported coopera-
tive work. ACM, pages 317–328.

Jeffrey L Elman. 1990. Finding structure in time. Cog-
nitive science 14(2):179–211.

Felix A Gers, Jürgen Schmidhuber, and Fred Cummins.
1999. Learning to forget: Continual prediction with
lstm .

Felix A Gers, Nicol N Schraudolph, and Jürgen
Schmidhuber. 2002. Learning precise timing with
lstm recurrent networks. Journal of machine learn-
ing research 3(Aug):115–143.

Sepp Hochreiter and Jürgen Schmidhuber. 1997.
Long short-term memory. Neural computation
9(8):1735–1780.

Yann LeCun, Yoshua Bengio, and Geoffrey Hinton.
2015. Deep learning. Nature 521(7553):436–444.

Saif M Mohammad. 2012. # emotional tweets. In Pro-
ceedings of the First Joint Conference on Lexical
and Computational Semantics-Volume 1: Proceed-
ings of the main conference and the shared task, and
Volume 2: Proceedings of the Sixth International
Workshop on Semantic Evaluation. Association for
Computational Linguistics, pages 246–255.

Saif M Mohammad and Felipe Bravo-Marquez. 2017.
Emotion intensities in tweets.

Alena Neviarouskaya, Helmut Prendinger, and Mit-
suru Ishizuka. 2009. Compositionality principle in
recognition of fine-grained emotions from text. In
ICWSM.

262



Emotion Pearson Spearman Average Pearson Average Spearman
Anger 0.176 0.155

0.076 0.071
Fear 0.023 0.011
Joy -0.019 0.008

Sadness 0.124 0.108

Table 4: Test results in range [0-1]

Emotion Pearson Spearman Average Pearson Average Spearman
Anger 0.19 0.164

0.14 0.134
Fear 0.077 0.061
Joy -0.057 0.071
Sadness 0.235 0.238

Table 5: Test results in range [0.5-1]

Carlo Strapparava and Rada Mihalcea. 2007. Semeval-
2007 task 14: Affective text. In Proceedings of
the 4th International Workshop on Semantic Eval-
uations. Association for Computational Linguistics,
pages 70–74.

Anja Summa, Bernd Resch, Geoinformatics-Z GIS,
and Michael Strube. 2016. Microblog emotion clas-
sification by computing similarity in text, time, and
space. In Proceedings of the Workshop on Compu-
tational Modeling of Peoples Opinions, Personality,
and Emotions in Social Media. pages 153–162.

Paul J Werbos. 1990. Backpropagation through time:
what it does and how to do it. Proceedings of the
IEEE 78(10):1550–1560.

263


