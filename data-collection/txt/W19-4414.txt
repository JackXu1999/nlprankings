



















































Learning to combine Grammatical Error Corrections


Proceedings of the Fourteenth Workshop on Innovative Use of NLP for Building Educational Applications, pages 139–148
Florence, Italy, August 2, 2019. c©2019 Association for Computational Linguistics

139

Learning to combine Grammatical Error Corrections

Yoav Kantor∗ Yoav Katz∗ Leshem Choshen∗ Edo Cohen-Karlik
Naftali Liberman Assaf Toledo Amir Menczel Noam Slonim

IBM Research AI
{yoavka,katz,leshem.choshen}@il.ibm.com

{edo.cohen,naftali.liberman,assaf.toledo}@ibm.com
{amir.menczel,noams}@il.ibm.com

Abstract

The field of Grammatical Error Correction
(GEC) has produced various systems to deal
with focused phenomena or general text edit-
ing. We propose an automatic way to combine
black-box systems. Our method automatically
detects the strength of a system or the combi-
nation of several systems per error type, im-
proving precision and recall while optimizing
F score directly. We show consistent improve-
ment over the best standalone system in all
the configurations tested. This approach also
outperforms average ensembling of different
RNN models with random initializations.

In addition, we analyze the use of BERT for
GEC - reporting promising results on this end.
We also present a spellchecker created for this
task which outperforms standard spellcheckers
tested on the task of spellchecking.

This paper describes a system submission
to Building Educational Applications 2019
Shared Task: Grammatical Error Correc-
tion(Bryant et al., 2019).

Combining the output of top BEA 2019 shared
task systems using our approach, currently
holds the highest reported score in the open
phase of the BEA 2019 shared task, improving
F0.5 by 3.7 points over the best result reported.

1 Introduction

Unlike other generation tasks (e.g. Machine
Translation and Text Summarization), Grammat-
ical Error Correction (GEC) contains separable
outputs, edits that could be extracted from sen-
tences, categorized (Bryant et al., 2017) and eval-
uated separately (Choshen and Abend, 2018a).
Throughout the years different approaches were
considered, some focused on specific error types
(Rozovskaya et al., 2014) and others adjusted sys-
tems from other tasks (Zhao et al., 2019). While

∗Contributed equally

the first receive high precision, the latter often
have high recall and differ in what they correct. To
benefit from both worlds, pipelines (Rozovskaya
and Roth, 2016) and rescoring hybrids (Grund-
kiewicz and Junczys-Dowmunt, 2018) were intro-
duced. Another suggested method for combining
is average ensembling (Junczys-Dowmunt et al.,
2018), used when several end to end neural net-
works are trained.

As single systems tend to have low recall
(Choshen and Abend, 2018b), pipelining systems
may propagate errors and may not benefit from
more than one system per error. Rescoring reduces
recall and may not be useful with many systems
(Grundkiewicz and Junczys-Dowmunt, 2018). We
propose a new method for combining systems (§4)
that can combine many systems and relies solely
on their output, i.e., it uses systems as a black-box.
We show our system outperforms average ensem-
bling, has benefits even when combining a single
system with itself, and produces the new state of
the art by combining several existing systems (§5).

To develop a system we trained GEC systems
and gathered outputs from black-box systems (§3).
One of the most frequent error types is spelling
errors, we compared off of the shelf spellcheck-
ers, systems developed for this error type specifi-
cally, to a new spellchecker (§3.1), finding that our
spellchecker outperforms common spellcheckers
on the task of spellchecking.

Another system tested was modifications of
BERT (Devlin et al., 2018) to correct errors, al-
lowing for less reliance on parallel data and more
generalizability across domains (§3.4).

Lastly, we tested generating synthetic errors
(Felice and Yuan, 2014) as a way to replace data in
an unsupervised scenario. While finding that mim-
icking the error distribution and generating errors
on the same domain is better, we did not eventu-
ally participate in the low-resource track.



140

2 Data

2.1 Preprocessing

Many systems assume the input is standard un-
tokenized English sentences. In these cases, we
detokenized the input data sets and then tokenized
again to perform the combination and evalua-
tion steps. For training the Nematus network,
we passed the data tokenization and truecasing
(Koehn et al., 2007) and trained BPE (Sennrich
et al., 2015).

2.2 Synthetic Error Generation

Generating training data for the GEC problem is
expensive and slow when done manually by hu-
man annotators. Most machine-learning based
systems today benefit from the quantity and rich-
ness of the training data, therefore, generating syn-
thetic data has a lot of potential, as was also shown
in previous work (Felice and Yuan, 2014). We
generate data with errors by applying corrections
backwards. Meaning, if a correction adds a miss-
ing word X to a sentence, to produce the cor-
responding error we remove X from a sentence.
And if a correction removes a redundant word X
from a sentence, to produce the corresponding er-
ror we add word X in a random location in a sen-
tence. And if a correction replaces word X with
word Y in a sentence, to produce the correspond-
ing error we replace word Y with word X in a
sentence. In order to preserve the distribution of
errors as found in the W&I+LOCNESS train data
set, we analyze it and measure the distribution of
corrections in it. We measure the distribution of
number of corrections in a sentence and distribu-
tion of specific corrections. Using these distri-
butions and a corpus of gold (correct) sentences
we produce errors with similar distributions. We
first randomly select the number of corrections in
a sentence according to the distribution measured
before. Then, we randomly select specific correc-
tions according to the distribution of corrections.
We then find all sentences where all corrections
can be applied backwards and pick one of them
randomly. Lastly, we generate the errors in the
sentence and add the gold sentence and error sen-
tence to corresponding output files.

3 Systems

3.1 Constructing a spellchecker

Many tools are available for spelling correction.
Yet, with a few heuristics we managed to get a
comparatively high result. As by Errant (Bryant
et al., 2017), our spellchecker receives a better
F0.5 score of spelling (type R:SPELL) than other
leading open-source spell-checkers. A compari-
son can be found at §5.1.

Our method of correcting spelling mistakes is
as follows. As a preprocessing stage, we go over
a large monolingual corpus - specifically a 6 mil-
lion sentences corpus taken from books in project
Gutenberg1. We count the number of occurrences
of each word (in it’s surface form), skipping words
with less than 3 characters and words that are not
composed exclusively of letters. We also use an
English dictionary (both US and GB) from Libre-
Office site 2 for enriching our data with English
words that are not in our books corpus. When cor-
recting a sentence, we find words that are not in
our word-count (or in it and have a count below
3) nor in the Dictionary. Skipping words with dig-
its or if it was all upper case. These words are
suspected to be misspelled and we try to correct
them.

For every misspelled word we try to find a re-
placement word by going over the words in the
word-count data (words with count greater than
20) in a descending order of occurrences. For
each suggested word, we check if it can be con-
sidered as a correction for the misspelled word by
two methods. First, we check if the original word
and the candidate correction differ from each other
by swapping two characters. If not, we calculate
the distance between the two words using Leven-
shtein distance (Levenshtein, 1966) and check if
the distance is 1. We return the most frequent word
that satisfies one of these conditions . If no candi-
date is found, we do the same with all words in
the dictionary in a lexicographical order. If still
no candidate is found, we check if we can split
the misspelled word into two words that are in our
word-count data or in the dictionary.

3.2 Nematus

We trained 4 neural machine translation systems
based on Nematus (Sennrich et al., 2017) Trans-

1https://www.gutenberg.org
2https://cgit.freedesktop.org/

libreoffice/dictionaries/tree/en

https://www.gutenberg.org
https://cgit.freedesktop.org/libreoffice/dictionaries/tree/en
https://cgit.freedesktop.org/libreoffice/dictionaries/tree/en


141

former (Vaswani et al., 2017) implementation. All
parameters used are the ones suggested for the
2017 Workshop on Machine Translation 3. As
training data we used all the restricted data, i.e.,
FCE (Dale and Kilgarriff, 2011), LANG8 (Mizu-
moto et al., 2011), NUCLE (Dahlmeier et al.,
2013) and W&I+LOCNESS (Bryant et al., 2019;
Granger, 1998) (upsampled 10 times). Each of
the four trained models was regarded as a separate
correction method and all systems were combined
using our method (§4), this was especially benefi-
cial as ensembling is not yet implemented for the
transformer. See §5.4 for comparison of the two
ensembling methods over RNN based Nematus.

3.3 Off the shelf

LanguageTool. LanguageTool is a free gram-
mar correction tool mainly based on spellchecking
and rules. We used language tool programmatic
API to obtain all the possible corrections and ap-
plied all the suggestions.

Grammarly. Grammarly is the company own-
ing the world leading grammar correction product,
as such it is the obvious candidate to be used as a
component and to assess the potential of combin-
ing black box systems. We used their free web
interface to correct the dev and test sets. Gram-
marly does not support a programmatic API, so
this process was manual. We uploaded the texts af-
ter detokenization into the web interface. For each
suggested correction, we took the top prediction
without human discretion. The reason to choose
the top prediction was to allow combining using a
single reference of Grammarly.

Spelling correction. We tested Enchant, Jam-
Spell and Norvig spellcheckers, finding our
spellchecker outperforms those in terms of
spelling correction (See §5).

3.4 BERT

BERT (Bidirectional Encoder Representations
from Transformers) (Devlin et al., 2018) is a
language representation model. BERT is ex-
tremely effective in general purpose tasks, among
its virtues, BERT holds a syntactic understand-
ing of a language (Goldberg, 2019). Initial pre-
training of BERT was performed over a large cor-
pora jointly on two tasks: (1) Masked Language

3https://github.com/EdinburghNLP/
wmt17-transformer-scripts

Model - randomly replace words with a predefined
token, [MASK], and predict the missing word. (2)
Next Sentence Prediction - given a pair of sen-
tences A and B, does sentence B follow sentence
A.

Our general approach for using BERT to solve
the GEC task is by iteratively querying BERT as
a black box language model, reminding former
use of language models (Dahlmeier and Ng, 2012;
Bryant and Briscoe, 2018). To detect missing
words we add [MASK] between every two words,
if BERT suggests a word with high confidence, we
conclude that this word is missing in this gap. To
detect unnecessary words, we replace words with
the [MASK] token and if all the suggestions re-
turned from BERT have a low probability, we con-
clude that the masked word was unnecessary. For
replacing words, we perform the same procedure
by replacing each word with [MASK] and check-
ing if BERT returns a different word with high
probability.

The described process produces many unde-
sired replacements/deletions due to BERT’s ver-
satile nature, for example, given a sentence such
as:

There are few ways to get there.

BERT may suggest replacing few with many.
Such a replacement preserves the grammatically
soundness of the sentence, but alters the seman-
tic meaning. Hence, although possibly improving
fluency, arguably the true goal of GEC (Napoles
et al., 2017), this behaviour does not align with
the goals of GEC requiring semantic preservation
(Choshen and Abend, 2018c). In order to focus the
exploration space of BERT’s suggestions, we limit
replacements/deletions to operate within a prede-
fined word set. The word sets considered included
syntactically interchangeable words, often sharing
some semantic properties. When considering a re-
moval correction, we remove a word only if the
returned values from BERT are not in the same
word-set as the replaced word. Replacement is al-
lowed only within the same word set. For exam-
ple, a typical mistake which occurred frequently
in the dataset is wrong usage of determiners such
as a and an, given the word set {a, an} and the
sentence:

Is that a armadillo?

The mechanism described limits the replace-
ment correction options to suggest making a

https://github.com/languagetool-org/languagetool
https://www.grammarly.com/
https://www.abisource.com/projects/enchant/
https://github.com/bakwc/JamSpell
https://github.com/bakwc/JamSpell
https://github.com/barrust/pyspellchecker
https://github.com/EdinburghNLP/wmt17-transformer-scripts
https://github.com/EdinburghNLP/wmt17-transformer-scripts


142

replacement-correction of a with an to result with
the corrected sentence

Is that an armadillo?

At each iteration of this process, a correction
(addition/replacement/deletion) is performed and
the resulting sentence is then used as the input to
the next iteration. Each replacement/addition of
the [MASK] token is a single candidate for a spe-
cific correction. Given an input sequence, each
possible correction gives rise to a different candi-
date which is then sent to BERT. The most proba-
ble correction (above a minimal threshold) is then
selected, this process accounts for one iteration.
The resulting sentence is then processed again and
the best correction is chosen until all corrections
have a low probability in which case the sentence
is assumed to be correct.

The above mechanism with threshold values be-
tween 0.6 and 0.98 did not yield satisfying re-
sults. For this reason, in the submitted system
we limit the mechanism significantly, ignoring ad-
ditions and deletions to focus solely on the re-
place corrections. Word sets were chosen from
the most frequent errors in the training data across
different error types (excluding punctuation marks
R:PUNCT).

Another approach for using BERT is by fine-
tuning BERT to the specific data at hand. Since the
GEC task is naturally limited to specific types of
errors, we fine-tuned the Masked Language Model
task using synthetic data. Instead of randomly re-
placing words with the [MASK] token, we replace
only specific words in a distribution which mim-
ics the training data. This process should create a
bias in the language model towards the prediction
of words which we want to correct. Unfortunately,
these efforts did not bear fruit. The authors believe
a more extensive exploration of experimental set-
tings may prove beneficial.

4 Combining systems

Combining the output of multiple systems has the
potential to improve both recall and precision. Re-
call is increased because typically different sys-
tems focus on different aspects of the problem and
can return corrections which are not identified by
other systems (Bryant et al., 2017). Precision can
be increased by utilizing the fact that if multiple
systems predict the same annotations, we can be
more confident that this correction is correct.

The outputs of Seq2Seq models, differing in
training parameters, can be merged using an en-
semble approach, where the predictions of the
models for each possible word in the sequence
are used to compute a merged prediction. It was
shown that even an ensemble of models trained
with the same hyperparameters but with different
instances of random initialization can yield benefit
(Junczys-Dowmunt et al., 2018).

The idea of automatically combining multiple
system outputs is not new to other fields and was
successfully used in the Named Entity Recogni-
tion (NER) and Entity linking (EL) tasks. Jiang
et al. (2016) evaluated multiple NER systems and
based on these results, manually selected a rule
for combining the two best systems, building a
hybrid system that outperformed the standalone
systems. Ruiz and Poibeau (2015) used the pre-
cision calculated on a training corpus to calcu-
late a weighted vote for each EL output on un-
seen data. Dlugolinskỳ et al. (2013) used deci-
sion tree classifier to identify which output to ac-
cept. They used a feature set based on the overall
text, NE surface form, the NE type and the over-
lap between different outputs. In GEC, combining
was also proposed but was ad-hoc rather than auto-
matic and general. Combining was done by either
piping (Rozovskaya and Roth, 2016), where each
system receives the output of the last system, or
correction of specific phenomena per system (Ro-
zovskaya and Roth, 2011), or more involved meth-
ods tailored to the systems used (Grundkiewicz
and Junczys-Dowmunt, 2018). This required man-
ual adjustments and refinements for every set of
systems.

Evaluating by a corpus level measure such
as F score renders combining systems diffi-
cult. Systems developed towards F0.5 tend to
reduce recall improving precision (Choshen and
Abend, 2018b), while avoiding catastrophic errors
(Choshen and Abend, 2018c) this behaviour might
reduce the flexibility of the combination. It is pos-
sible to tune systems to other goals (e.g. recall)
(Grundkiewicz and Junczys-Dowmunt, 2018) and
thus achieve more versatile systems, but that is not
the case when using black-box systems, and hence
left for future inspection.

System pair. We propose a method to combine
multiple systems by directly optimizing Fβ for a
chosen β, in the field 0.5 is usually used. We begin
by considering a combination of two systems



143

1. Given a development set, where E are the
sentences with errors and G are the gold an-
notations, generate M2gold file, which con-
tains all the gold corrections to the sentences.

2. Correct E with each of the systems, to re-
ceive corrected sentences hypothesis Hi.

3. GenerateM2i for each system i by comparing
the systems’ output Hi and the E input.

4. Split the annotations of the systems into three
subsets: H1\2 - all the suggested annotations
of system1 which were not suggested by
system2; H2\1 - all the suggested annota-
tions of system2 which were not suggested
by system1; andH1∩2 - all the suggested an-
notations in common.

5. Generate M2 files for each of the three sets:
M21\2, M

2
1\2, M

2
1∩2.

6. Evaluate the performance on each of the
three subsets of annotations, split by error
type, by comparing M2subset with M

2
gold. For

each subset and each error type, we obtain
TP error−typesubset , FP

error−type
subset , FN

error−type
subset .

7. Define selection variables Serror−typesubset which
determine the probability an edit of the spe-
cific error type in a specific subset of edits
will be used. According to the way subsets
were built, each edit corresponds to exactly
one subset (e.g. 1 \ 2).

8. For all error types and subset of ed-
its, compute the optimal selection variables
Serror−typesubset that maximize fβ by solving

0 ≤ Serror−typesubset ≤ 1

total =
∑

t∈error−type
TP t1∩2 + FN

t
1∩2

TP =
∑

t∈error−type,s∈subset
TP ts ∗ Sts

FP =
∑

t∈error−type,s∈subset
FP ts ∗ Sts

FN = total − TP
Sopt = argmax

S
fβ(TP, FP, FN)

This is a convex optimization problem with
linear constraints and pose no difficulty to
standard solvers.

Sopterror−typesubset need not be integer, although in
practice they usually are. 4. In our submission,
for simplicity, we avoid these cases and round
Sopterror−typesubset to nearest integer value (either 0 or
1). But our implementation allows sampling.

A major concern is to what extent does the pre-
cision and recall statistics per error type and subset
on the development set represent the actual distri-
bution expected during inference on unseen data.
Assuming the development set and the unseen are
sampled from the same distributions, the confi-
dence is correlated with the number of samples
seen for each error-type and subset.

Assuming errors come from a binomial distri-
bution, we try to estimate the conditional prob-
ability P (|prectest − precdev| < 0.15 | precdev).
Given more than 20 samples, the probability for
15% difference in development and test precision
is 14.5%, and if there are 50 samples, this proba-
bility drops to 2.8%. In the experiments, we ignore
error-types where there are less than 2 samples.

The process of correcting an unseen set of sen-
tences T is as follows:

1. Correct T by every system i, to receive cor-
rected sentences hypothesis Hi .

2. Generate M2i files for each system by com-
paring the systems’ output Hi and the T in-
put.

3. Split the annotations of the systems into three
sets: H1\2 , H2\1 , and H1∩2 .

4. Generate M2 files for each of the three sets:
M21\2, M

2
2\1, M

2
1∩2.

5. Remove all annotations from the M2 files for
which Sopterror−typesubset = 0.

6. Merge all the annotations from the modified
M21\2,M

2
2\1, andM

2
1∩2 files to createM

2
final.

If there are overlapping annotations - we cur-
rently select an arbitrary annotation.

7. Apply all the corrections in M2final to T and
receive the final output.

In Table 1, we present the results of the most
frequent error types when combining two systems,

4Non integer value can occur when a 0 value yields high
precision and low recall, and a 1 value yields low precision
and high recall. In this case, randomly selecting a subset of
the corrections will yield a medium recall and medium preci-
sion, which maximizes fβ



144

Nematus and Grammarly. As expected, the preci-
sion on corrections found by both systems is sig-
nificantly higher than those found by a single sys-
tem. For correction type ’R:OTHER’, for exam-
ple, the precision on common corrections is 0.67,
compared to 0.17 and 0.28 of the respective stan-
dalone systems. Therefore, the optimal solution
uses only the corrections produced by both sys-
tems. We can also see that in some error types
(e.g., R:SPELL or R:DET) the precision of cor-
rections identified by the Nematus system is low
enough that the optimization algorithm selected
only the corrections by Grammarly.

Multiple systems. When N > 2 systems are
available, it is possible to extend the above ap-
proach by creating more disjoint subsets, which
include any of the 2N subsets of corrections.
When N is large, many of these subsets will be
very small, and therefore may not contain mean-
ingful statistics. We propose an iterative approach,
where at each step two systems are combined. The
results of this combination can be then combined
with other systems. This approach works better
when the development set is small, but can also
suffers from over-fitting to the dev set, because
subsequent combination steps are performed on
the results of the previous merges steps, which
were already optimized on the same data set.

5 Experiments

As our system is based on various parts and mainly
focuses on the ability to smartly combine those,
we experiment with how each of the parts work
separately. A special focus is given to combining
strong components, black-box components and
single components as combining is a crucial part
of the innovation in this system.

5.1 Spell checkers’ comparison

We’ve compared our home-brewed spell-checker
with JamSpell5, Norvig6 and ENCHANT7. When
comparing the results over all error categories,
our spell-checker has relatively low results (See
Table 2). However, when comparing the results
in spelling (R:SPELL) category alone, our spell-
checker excels (See Table 3).

5https://github.com/bakwc/JamSpell
6https://github.com/barrust/

pyspellchecker
7https://github.com/AbiWord/enchant

5.2 Nematus

We trained Nematus using several different data
sets. First, we trained using only the W&I train
set data, we then added Lang8, FCE and Nucle
data sources. Since Lang8 is significantly larger
than W&I train set, inspired by Junczys-Dowmunt
et al. (2018), we upsampled W&I 10 times so that
it will have more significant effect on the training
process. This procedure improved results signifi-
cantly (See Table 4).

5.3 Synthetic Error Generation

We also tried training Nematus over synthetic er-
rors data. We generated errors using data from two
different domains. Books from project Gutenberg
and gold sentences from W&I train set. Addition-
ally, we varied data sizes and observed the effect
on the results (See Table 5). These experiments
show that relying on the source domain is crucial
and it is best to generate data using text from sim-
ilar domain. When using the synthetic W&I train
set we reached a score that is just a little lower
than the score when training over W&I train set di-
rectly (0.19 vs 0.23). This might suggest that there
is potential in using synthetic data when combined
with other data sets and promise for synthetic data
methods for unsupervised GEC.

5.4 Combining

The experiments regarding combining were per-
formed on the dev set, which was not used for
training the systems. The dev set was split to two
randomly. The optimal selection of error-types
and subsets to combine was done on one half, and
we report system results on the second half. For
example, when combining the output of the Ne-
matus and Grammarly systems under 10 different
fold partitions, the average F0.5 improvement over
the best of the two systems was 6.2 points, with
standard deviation of 0.28 points.

Improvement of a single tool. Even given a
single system, we are able to improve the sys-
tem’s performance by eschewing predictions on
low performing error types. This filtering proce-
dure has a minor effect and is exemplified in Table
6. While such findings are known to exist implic-
itly by the cycles of development (Choshen and
Abend, 2018b), and were suggested as beneficial
for rule based and statistical machine translation
systems when precision is 0 (Felice et al., 2014),

https://github.com/bakwc/JamSpell
https://github.com/barrust/pyspellchecker
https://github.com/barrust/pyspellchecker
https://github.com/AbiWord/enchant


145

error-type Frequency S1\2 P1\2 R1\2 S1∩2 P1∩2 R1∩2 S2\1 P2\1 R2\1
R:PUNCT %4 1.0 0.47 0.15 0.0 0.0 0.0 1.0 0.4 0.01
U:DET %4 1.0 0.38 0.07 1.0 0.77 0.15 1.0 0.51 0.2
R:VERB %5 1.0 0.5 0.02 0.0 1.0 0.01 1.0 0.57 0.02
M:DET %5 0.0 0.29 0.05 1.0 0.68 0.12 1.0 0.4 0.31
R:ORTH %5 0.0 0.28 0.22 1.0 0.86 0.13 1.0 0.46 0.18
R:SPELL %5 0.0 0.32 0.04 1.0 1.0 0.11 1.0 0.66 0.65
R:VERB:TENSE %5 1.0 0.54 0.15 0.0 0.0 0.0 0.0 0.0 0.0
R:PREP %6 1.0 0.37 0.07 1.0 0.72 0.07 1.0 0.56 0.1
R:OTHER %11 0.0 0.17 0.02 1.0 0.67 0.02 0.0 0.28 0.04
M:PUNCT %15 1.0 0.55 0.17 1.0 0.68 0.06 1.0 0.38 0.12

Table 1: Combination statistics of the most common error types over two systems - Nematus and Grammarly

All Categories P R F0.5
Norvig 0.5217 0.0355 0.1396
Enchant 0.2269 0.0411 0.1192
Jamspell 0.4385 0.0449 0.1593
our 0.5116 0.0295 0.1198

Table 2: Comparison of Grammatical Error Perfor-
mance of Spellcheckers. Jamspell achieves the best
score as previously suggested.

R:SPELL P R F0.5
Norvig 0.5775 0.6357 0.5882
Enchant 0.316 0.6899 0.3544
Jamspell 0.5336 0.6977 0.5599
our 0.6721 0.5297 0.6378

Table 3: Comparison of spellcheckers on spelling. Our
method outperforms other methods.

to the best of our knowledge we are the first to re-
port those results directly, on non trivial precision
with neural network based systems. In explicitly
filtering corrections by error types we gain two ad-
ditional benefits over the mere score improvement.
First, the weak spots of the system are empha-
sized, and work might be directed to improving
components or combining with a relevant strong
system. Second, the system itself is not discour-
aged or changed to stop producing those correc-
tions. So, if future enhancement would improve
this type of errors enough, it will show up in re-
sults, without discouraging smaller improvements
done on the way.

Restricted track. In Table 7 we present the re-
sults of our shared task restricted track submis-
sion. The submission includes four Nematus
models, our spellchecker, and Bert based system

Training Data P R F0.5
W&I train set 0.3187 0.1112 0.232
W&I train set
+ lang8 + FCE

0.4604 0.0742 0.225

W&I train set
(upsampled X 10)

+ Lang8 + FCE + Nucle
0.4738 0.1529 0.333

Table 4: Nematus performance on W&I dev set by
training data. The use of more data improves the sys-
tem, but only when the training from the domain is up-
sampled.

Data Source Size (sentences) F0.5
Gutenberg Books 650,000 0.1483
Gutenberg Books 7,000,000 0.1294

W&I train set 1,300,000 0.1919

Table 5: Size of synthetic datasets and Nematus scores
when trained on them.

(§3.4). This generated a 6 point improvement on
the dev set of f0.5 when compared the best stan-
dalone Nematus model.

Off the shelf systems. As can be seen in Ta-
ble 8 when we combine the system with several
off the self systems, we get 3 point improvement
over the restricted baseline, and a 9 point improve-

System P R F0.5
Language Tool 0.2905 0.1004 0.2107

Filtered Language Tool 0.4005 0.0889 0.2355
Grammarly 0.4846 0.1808 0.3627

Filtered Grammarly 0.5342 0.1715 0.3754
Nematus 0.52 0.1751 0.373

Filtered Nematus 0.554 0.1647 0.3761

Table 6: Change in performance when avoiding hard
errors.



146

System P R F0.5
(1) Nematus1 0.4788 0.1544 0.3371
(2) Nematus2 0.4839 0.1583 0.3429
(3) Nematus3 0.4842 0.1489 0.3338
(4) Nematus4 0.4843 0.1502 0.3352

(5) Spellchecker 0.5154 0.0308 0.1242
(6) Bert 0.0132 0.0147 0.0135

1+2 0.4972 0.1854 0.3721
1+2+3 0.5095 0.1904 0.3816

1+2+3+4 0.4926 0.2017 0.3824
1+2+3+4+5 0.5039 0.2233 0.4027

1+2+3+4+5+6 0.5029 0.2278 0.4051

Table 7: Performance of systems and iterative combi-
nation of them. Combination improves both precision
and recall even using low performing systems.

ment over the best standalone system. This im-
plies there is a promise in combining existing ap-
proaches which we can’t improve ourselves to har-
ness some of their correction power. 8

System P R F0.5
(1) Restricted-best 0.5029 0.2278 0.4051
(2) Language Tool 0.2699 0.0955 0.1977

(3) Grammerly 0.4783 0.1825 0.3612
(4) Jamspell 0.423 0.0413 0.1484

1+2 0.5274 0.2175 0.4105
1+2+3 0.522 0.2656 0.4375

1+2+3+4 0.5221 0.2641 0.4367

Table 8: Combining with off the shelf systems helps.

Ensemble VS Combining models results. Ne-
matus has average ensembling built-in which en-
ables inference over several RNN models by per-
forming geometric average of the individual mod-
els’ probability distributions. Combining outper-
forms the built-in ensemble by almost 4 points
(See Table 9). It is also important to note that
while average ensemble improves precision, it re-
duces recall. Combination is balancing precision
and recall, improving both, in a way that maxi-
mizes F0.5. The last observation is far from trivial
as most ways to combine systems would empha-
size one or the other, e.g., piping would support
mainly recall perhaps reducing precision. Lastly,
combining is based on the types of errors and is
linguistically motivated, and hence could be fur-

8Although some of the systems use only rules and non-
parallel data, we did not include them in our submission to
the restricted tracked, as we are not their originators.

ther improved by smart categorization and per-
haps improvements of automatic detection (Bryant
et al., 2017).

System P R F0.5
(1) Nematus RNN 1 0.4676 0.1157 0.2908
(2) Nematus RNN 2 0.4541 0.1223 0.2944
(3) Nematus RNN 3 0.484 0.1191 0.3002
(4) Nematus RNN 4 0.4839 0.1184 0.2991
1+2+3+4 ensemble 0.5577 0.1131 0.3122

1+2+3+4 combination 0.4861 0.166 0.3508

Table 9: Combining fares better compared to ensemble.

Combining the shared task systems. After the
completion of the competition test phase, several
teams agreed to release their outputs on the dev
and test set. We combined them using the entire
dev set and submitted the results to the open phase
of the restricted track for evaluation. This achieves
a 3.7 point improvement in F0.5 and a 6.5 point
improvement in precision over the best standalone
results (See Table 10). This means this combina-
tion is the best result currently known in the field
as assessed by the BEA 2019 shared task.

System P R F0.5
(1) UEDIN-MS 72.28 60.12 69.47

(2) Kakao&Brain 75.19 51.91 69.00
(3) Shuyao 70.17 55.39 66.61

(4) CAMB-CUED 66.75 53.93 63.72
1+2 78.31 58.00 73.18
3+4 74.99 54.41 69.72

1+2+3+4 78.74 56.04 72.84

Table 10: Test set results when combining systems
from the competition used as black boxes. The com-
bination is the new state of the art.

6 Conclusion and Future Work

In this paper, we have shown how combining mul-
tiple GEC systems, using a pure black-box ap-
proach, can improve state of the art results in the
error correction task.

Additional variants of this combination ap-
proach can be further examined. The approach
can work with any disjoint partition systems’ cor-
rections. We can consider combining more than
2 systems at the same time, or we can consider
more refined subsets of two systems. For exam-
ple, the setH1\2 of all the suggested corrections of



147

system1 which were not suggested by system2,
can be split to the two sets: H1overlapping2 and
H1non−overlapping2, the former containing correc-
tions of system 1 which have an overlapping (but
different) corrections by system2, and the later
corrections of system1 which have no overlap
with any annotation of system2.

Several other approaches can be taken. The
problem can be formulated as multiple-sequence
to single sequence problem. The input sequences
are the original text and n system corrections. The
output sequence is the combined correction. Dur-
ing training, the gold correction is used. Given
sufficient labeled data, it may be possible for such
a system to learn subtle distinctions which may re-
sult in better combinations without relying on sep-
arating error types or iterative combinations.

In addition, we harnessed Bert for GEC and
showed a simple spellchecking mechanism yields
competitive results to the leading spellcheckers.

References
Christopher Bryant and Ted Briscoe. 2018. Language

model based grammatical error correction without
annotated training data. In Proceedings of the Thir-
teenth Workshop on Innovative Use of NLP for
Building Educational Applications, pages 247–253,
New Orleans, Louisiana. Association for Computa-
tional Linguistics.

Christopher Bryant, Mariano Felice, Øistein E. Ander-
sen, and Ted Briscoe. 2019. The BEA-2019 Shared
Task on Grammatical Error Correction. In Proceed-
ings of the 14th Workshop on Innovative Use of NLP
for Building Educational Applications. Association
for Computational Linguistics.

Christopher Bryant, Mariano Felice, and Ted Briscoe.
2017. Automatic annotation and evaluation of error
types for grammatical error correction. In ACL.

Leshem Choshen and Omri Abend. 2018a. Automatic
metric validation for grammatical error correction.
In Proceedings of the 56th Annual Meeting of the
Association for Computational Linguistics (Volume
1: Long Papers), pages 1372–1382.

Leshem Choshen and Omri Abend. 2018b. Inherent
biases in reference-based evaluation for grammatical
error correction and text simplification. In ACL.

Leshem Choshen and Omri Abend. 2018c. Reference-
less measure of faithfulness for grammatical error
correction. arXiv preprint arXiv:1804.03824.

Daniel Dahlmeier and Hwee Tou Ng. 2012. A beam-
search decoder for grammatical error correction. In
Proceedings of the 2012 Joint Conference on Empir-
ical Methods in Natural Language Processing and

Computational Natural Language Learning, pages
568–578. Association for Computational Linguis-
tics.

Daniel Dahlmeier, Hwee Tou Ng, and Siew Mei Wu.
2013. Building a large annotated corpus of learner
english: The nus corpus of learner english. In Pro-
ceedings of the eighth workshop on innovative use
of NLP for building educational applications, pages
22–31.

Robert Dale and Adam Kilgarriff. 2011. Helping our
own: The hoo 2011 pilot shared task. In Proceed-
ings of the 13th European Workshop on Natural Lan-
guage Generation, pages 242–249. Association for
Computational Linguistics.

Jacob Devlin, Ming-Wei Chang, Kenton Lee, and
Kristina Toutanova. 2018. Bert: Pre-training of deep
bidirectional transformers for language understand-
ing. arXiv preprint arXiv:1810.04805.

Štefan Dlugolinskỳ, Peter Krammer, Marek Ciglan,
Michal Laclavı́k, and Ladislav Hluchỳ. 2013. Com-
bining named enitity recognition tools. Making
Sense of Microposts (# MSM2013).

Mariano Felice and Zheng Yuan. 2014. Generating ar-
tificial errors for grammatical error correction. In
Proceedings of the Student Research Workshop at
the 14th Conference of the European Chapter of the
Association for Computational Linguistics, pages
116–126.

Mariano Felice, Zheng Yuan, Øistein E Andersen, He-
len Yannakoudakis, and Ekaterina Kochmar. 2014.
Grammatical error correction using hybrid systems
and type filtering. In Proceedings of the Eigh-
teenth Conference on Computational Natural Lan-
guage Learning: Shared Task, pages 15–24.

Yoav Goldberg. 2019. Assessing bert’s syntactic abili-
ties. CoRR, abs/1901.05287.

Sylviane Granger. 1998. The computerized learner cor-
pus: a versatile new source of data for sla research.

Roman Grundkiewicz and Marcin Junczys-Dowmunt.
2018. Near human-level performance in grammati-
cal error correction with hybrid machine translation.
arXiv preprint arXiv:1804.05945.

Ridong Jiang, Rafael E Banchs, and Haizhou Li. 2016.
Evaluating and combining name entity recognition
systems. In Proceedings of the Sixth Named Entity
Workshop, pages 21–27.

Marcin Junczys-Dowmunt, Roman Grundkiewicz,
Shubha Guha, and Kenneth Heafield. 2018. Ap-
proaching neural grammatical error correction as a
low-resource machine translation task. In NAACL-
HLT.

Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,

https://doi.org/10.18653/v1/W18-0529
https://doi.org/10.18653/v1/W18-0529
https://doi.org/10.18653/v1/W18-0529
http://arxiv.org/abs/1901.05287
http://arxiv.org/abs/1901.05287


148

Richard Zens, et al. 2007. Moses: Open source
toolkit for statistical machine translation. In Pro-
ceedings of the 45th annual meeting of the associ-
ation for computational linguistics companion vol-
ume proceedings of the demo and poster sessions,
pages 177–180.

Vladimir I Levenshtein. 1966. Binary codes capable
of correcting deletions, insertions, and reversals. In
Soviet physics doklady, volume 10, pages 707–710.

Tomoya Mizumoto, Mamoru Komachi, Masaaki Na-
gata, and Yuji Matsumoto. 2011. Mining revision
log of language learning sns for automated japanese
error correction of second language learners. In Pro-
ceedings of 5th International Joint Conference on
Natural Language Processing, pages 147–155.

Courtney Napoles, Keisuke Sakaguchi, and Joel R.
Tetreault. 2017. Jfleg: A fluency corpus and bench-
mark for grammatical error correction. In EACL.

Alla Rozovskaya and Dan Roth. 2011. Algorithm se-
lection and model adaptation for esl correction tasks.
In ACL.

Alla Rozovskaya and Dan Roth. 2016. Grammatical
error correction: Machine translation and classifiers.
In Proceedings of the 54th Annual Meeting of the
Association for Computational Linguistics (Volume
1: Long Papers), volume 1, pages 2205–2215.

Alla Rozovskaya, Dan Roth, and Vivek Srikumar.
2014. Correcting grammatical verb errors. In Pro-
ceedings of the 14th Conference of the European
Chapter of the Association for Computational Lin-
guistics, pages 358–367.

Pablo Ruiz and Thierry Poibeau. 2015. Combining
open source annotators for entity linking through
weighted voting. In Joint Conference on Lexical
and Computational Semantics (* SEM 2015), pages
211–215.

Rico Sennrich, Orhan Firat, Kyunghyun Cho, Alexan-
dra Birch, Barry Haddow, Julian Hitschler, Marcin
Junczys-Dowmunt, Samuel Läubli, Antonio Valerio
Miceli Barone, Jozef Mokry, and Maria Nadejde.
2017. Nematus: a toolkit for neural machine trans-
lation. In Proceedings of the Software Demonstra-
tions of the 15th Conference of the European Chap-
ter of the Association for Computational Linguistics,
pages 65–68, Valencia, Spain. Association for Com-
putational Linguistics.

Rico Sennrich, Barry Haddow, and Alexandra Birch.
2015. Neural machine translation of rare words with
subword units. arXiv preprint arXiv:1508.07909.

Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob
Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz
Kaiser, and Illia Polosukhin. 2017. Attention is all
you need. In NIPS.

Wei Zhao, Liang Wang, Kewei Shen, Ruoyu Jia, and
Jingming Liu. 2019. Improving grammatical er-
ror correction via pre-training a copy-augmented
architecture with unlabeled data. arXiv preprint
arXiv:1903.00138.

http://aclweb.org/anthology/E17-3017
http://aclweb.org/anthology/E17-3017

