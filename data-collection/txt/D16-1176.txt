



















































Modelling Interaction of Sentence Pair with Coupled-LSTMs


Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 1703–1712,
Austin, Texas, November 1-5, 2016. c©2016 Association for Computational Linguistics

Modelling Interaction of Sentence Pair with Coupled-LSTMs

Pengfei Liu Xipeng Qiu∗ Yaqian Zhou Jifan Chen Xuanjing Huang
Shanghai Key Laboratory of Intelligent Information Processing, Fudan University

School of Computer Science, Fudan University
825 Zhangheng Road, Shanghai, China

{pfliu14,xpqiu,zhouyaqian,jfchen14, xjhuang}@fudan.edu.cn

Abstract

Recently, there is rising interest in modelling
the interactions of two sentences with deep
neural networks. However, most of the exist-
ing methods encode two sequences with sepa-
rate encoders, in which a sentence is encoded
with little or no information from the other
sentence. In this paper, we propose a deep
architecture to model the strong interaction
of sentence pair with two coupled-LSTMs.
Specifically, we introduce two coupled ways
to model the interdependences of two LSTMs,
coupling the local contextualized interactions
of two sentences. We then aggregate these in-
teractions and use a dynamic pooling to select
the most informative features. Experiments on
two very large datasets demonstrate the effi-
cacy of our proposed architectures.

1 Introduction

Distributed representations of words or sentences
have been widely used in many natural language
processing (NLP) tasks, such as text classification
(Kalchbrenner et al., 2014; Liu et al., 2015), ques-
tion answering and machine translation (Sutskever
et al., 2014) and so on. Among these tasks, a com-
mon problem is modelling the relevance/similarity
of the sentence pair, which is also called text seman-
tic matching.

Recently, deep learning based models is rising a
substantial interest in text semantic matching and
have achieved some great progresses (Hu et al.,
2014; Qiu and Huang, 2015; Wan et al., 2016).

∗Corresponding author.

According to the phases of interaction between
two sentences, previous models can be classified
into three categories.
Weak interaction Models Some early works fo-
cus on sentence level interactions, such as ARC-
I(Hu et al., 2014), CNTN(Qiu and Huang, 2015)
and so on. These models first encode two sequences
with some basic (Neural Bag-of-words, BOW) or
advanced (RNN, CNN) components of neural net-
works separately, and then compute the matching
score based on the distributed vectors of two sen-
tences. In this paradigm, two sentences have no in-
teraction until arriving final phase.
Semi-interaction Models Some improved meth-
ods focus on utilizing multi-granularity represen-
tation (word, phrase and sentence level), such as
MultiGranCNN (Yin and Schütze, 2015) and Multi-
Perspective CNN (He et al., 2015). Another kind
of models use soft attention mechanism to obtain
the representation of one sentence by depending on
representation of another sentence, such as ABCNN
(Yin et al., 2015), Attention LSTM(Rocktäschel et
al., 2015; Hermann et al., 2015). These models can
alleviate the weak interaction problem, but are still
insufficient to model the contextualized interaction
on the word as well as phrase level.
Strong Interaction Models These models di-
rectly build an interaction space between two sen-
tences and model the interaction at different posi-
tions, such as ARC-II (Hu et al., 2014), MV-LSTM
(Wan et al., 2016) and DF-LSTMs(Liu et al., 2016).
These models can easily capture the difference be-
tween semantic capacity of two sentences.

In this paper, we propose a new deep neural net-
work architecture to model the strong interactions

1703



of two sentences. Different with modelling two sen-
tences with separated LSTMs, we utilize two inter-
dependent LSTMs, called coupled-LSTMs, to fully
affect each other at different time steps. The out-
put of coupled-LSTMs at each step depends on both
sentences. Specifically, we propose two interdepen-
dent ways for the coupled-LSTMs: loosely coupled
model (LC-LSTMs) and tightly coupled model (TC-
LSTMs). Similar to bidirectional LSTM for single
sentence (Schuster and Paliwal, 1997; Graves and
Schmidhuber, 2005), there are four directions can be
used in coupled-LSTMs. To utilize all the informa-
tion of four directions of coupled-LSTMs, we aggre-
gate them and adopt a dynamic pooling strategy to
automatically select the most informative interaction
signals. Finally, we feed them into a fully connected
layer, followed by an output layer to compute the
matching score.

The contributions of this paper can be summa-
rized as follows.

1. Different with the architectures of using sim-
ilarity matrix, our proposed architecture di-
rectly model the strong interactions of two sen-
tences with coupled-LSTMs, which can cap-
ture the useful local semantic relevances of two
sentences. Our architecture can also capture
the multiple granular interactions by several
stacked coupled-LSTMs layers.

2. Compared to previous works on text matching,
we perform extensive empirical studies on two
very large datasets. The massive scale of the
datasets allows us to train a very deep neu-
ral network and present an elaborate qualitative
analysis of our models, which gives an intuitive
understanding how our model worked.

2 Sentence Modelling with LSTM

Long short-term memory network (LSTM) (Hochre-
iter and Schmidhuber, 1997) is a type of recurrent
neural network (RNN) (Elman, 1990), and specifi-
cally addresses the issue of learning long-term de-
pendencies.

We define the LSTM units at each time step t to
be a collection of vectors in Rd: an input gate it, a
forget gate ft, an output gate ot, a memory cell ct
and a hidden state ht. d is the number of the LSTM

units. The elements of the gating vectors it, ft and
ot are in [0, 1].

The LSTM is precisely specified as follows.




c̃t
ot
it
ft


 =




tanh
σ
σ
σ


TA,b

[
xt

ht−1

]
, (1)

ct = c̃t � it + ct−1 � ft, (2)
ht = ot � tanh (ct) , (3)

where xt is the input at the current time step; TA,b
is an affine transformation which depends on param-
eters of the network A and b. σ denotes the logistic
sigmoid function and � denotes elementwise multi-
plication.

The update of each LSTM unit can be written pre-
cisely as follows

(ht, ct) = LSTM(ht−1, ct−1,xt). (4)

Here, the function LSTM(·, ·, ·) is a shorthand for
Eq. (1-3).

3 Coupled-LSTMs for Strong Sentence
Interaction

To deal with two sentences, one straightforward
method is to model them with two separate LSTMs.
However, this method is difficult to model local in-
teractions of two sentences. An improved way is to
introduce attention mechanism, which has been used
in many tasks, such as machine translation (Bah-
danau et al., 2014) and question answering (Her-
mann et al., 2015).

Inspired by the multi-dimensional recurrent neu-
ral network (Graves et al., 2007; Graves and
Schmidhuber, 2009; Byeon et al., 2015) and grid
LSTM (Kalchbrenner et al., 2015) in computer vi-
sion community, we propose two models to capture
the interdependences between two parallel LSTMs,
called coupled-LSTMs (C-LSTMs).

To facilitate our models, we firstly give some def-
initions. Given two sequences X = x1, x2, · · · , xn
and Y = y1, y2, · · · , ym, we let xi ∈ Rd denote the
embedded representation of the word xi. The stan-
dard LSTM have one temporal dimension. When
dealing with a sentence, LSTM regards the posi-
tion as time step. At position i of sentence x1:n,

1704



h
(1)
1 h

(1)
2 h

(1)
3

h
(2)
1 h

(2)
2 h

(2)
3

(a) Parallel LSTMs

h
(1)
1 h

(1)
2 h

(1)
3

h
(2)
1 h

(2)
2 h

(2)
3

(b) Attention LSTMs

h
(1)
41 h

(2)
41 h

(1)
42 h

(2)
42 h

(1)
43 h

(2)
43 h

(1)
44 h

(2)
44

h
(1)
31 h

(2)
31 h

(1)
32 h

(2)
32 h

(1)
33 h

(2)
33 h

(1)
34 h

(2)
34

h
(1)
21 h

(2)
21 h

(1)
22 h

(2)
22 h

(1)
23 h

(2)
23 h

(1)
24 h

(2)
24

h
(1)
11 h

(2)
11 h

(1)
12 h

(2)
12 h

(1)
13 h

(2)
13 h

(1)
14 h

(2)
14

(c) Loosely coupled-LSTMs

h41 h42 h43 h44

h31 h32 h33 h34

h21 h22 h23 h24

h11 h12 h13 h14

(d) Tightly coupled-
LSTMs

Figure 1: Four different coupled-LSTMs.

the output hi reflects the meaning of subsequence
x0:i = x0, · · · , xi.

To model the interaction of two sentences as early
as possible, we define hi,j to represent the interac-
tion of the subsequences x0:i and y0:j .

Figure 1(c) and 1(d) illustrate our two propose
models. For intuitive comparison of weak interac-
tion parallel LSTMs, we also give parallel LSTMs
and attention LSTMs in Figure 1(a) and 1(b)1.

We describe our two proposed models as follows.

3.1 Loosely Coupled-LSTMs (LC-LSTMs)

To model the local contextual interactions of two
sentences, we enable two LSTMs to be interde-
pendent at different positions. Inspired by Grid
LSTM (Kalchbrenner et al., 2015) and word-by-
word attention LSTMs (Rocktäschel et al., 2015),
we propose a loosely coupling model for two inter-
dependent LSTMs.

More concretely, we refer to h(1)i,j as the encoding
of subsequence x0:i in the first LSTM influenced by
the output of the second LSTM on subsequence y0:j .
Meanwhile, h(2)i,j is the encoding of subsequence y0:j
in the second LSTM influenced by the output of the
first LSTM on subsequence x0:i

1In Rocktäschel et al. (2015) model, conditioned LSTM was
used, meaning that h(1)1 is produced conditioned on h

(2)
3

h
(1)
i,j and h

(2)
i,j are computed as

h
(1)
i,j = LSTM

1(H
(1)
i−1, c

(1)
i−1,j ,xi), (5)

h
(2)
i,j = LSTM

2(H
(2)
j−1, c

(2)
i,j−1,yj), (6)

where

H
(1)
i−1 = [h

(1)
i−1,j ,h

(2)
i−1,j ], (7)

H
(2)
j−1 = [h

(1)
i,j−1,h

(2)
i,j−1]. (8)

3.2 Tightly Coupled-LSTMs (TC-LSTMs)
The hidden states of LC-LSTMs are the combi-
nation of the hidden states of two interdependent
LSTMs, whose memory cells are separated. In-
spired by the configuration of the multi-dimensional
LSTM (Byeon et al., 2015), we further conflate
both the hidden states and the memory cells of
two LSTMs. We assume that hi,j directly model
the interaction of the subsequences x0:i and y0:j ,
which depends on two previous interaction hi−1,j
and hi,j−1, where i, j are the positions in sentence
X and Y .

We define a tightly coupled-LSTMs units as fol-
lows.




c̃i,j
oi,j
ii,j
f1i,j
f2i,j



=




tanh
σ
σ
σ
σ



TA,b




xi
yj

hi,j−1
hi−1,j


 , (9)

ci,j = c̃i,j � ii,j + [ci,j−1, ci−1,j ]T
[
f1i,j
f2i,j

]
(10)

hi,j = ot � tanh (ci,j) (11)
where the gating units ii,j and oi,j determine which
memory units are affected by the inputs through c̃i,j ,
and which memory cells are written to the hidden
units hi,j . TA,b is an affine transformation which
depends on parameters of the network A and b. In
contrast to the standard LSTM defined over time,
each memory unit ci,j of a tightly coupled-LSTMs
has two preceding states ci,j−1 and ci−1,j and two
corresponding forget gates f1i,j and f

2
i,j .

3.3 Analysis of Two Proposed Models
Our two proposed coupled-LSTMs can be formu-
lated as
(hi,j , ci,j) = C-LSTMs(hi−1,j ,hi,j−1, ci−1,j , ci,j−1,xi,yj),

(12)

1705



x1, · · · ,xn

y
1
,·
··
,y

m

∑ ∑
· · · Pooling FullyConnected

Layer

Output
Layer

Input Layer Stacked C-LSTMs Pooling Layer

Figure 2: Architecture of coupled-LSTMs for sentence-pair encoding. Inputs are fed to four C-LSTMs fol-
lowed by an aggregation layer. Blue cuboids represent different contextual information from four directions.

where C-LSTMs can be either TC-LSTMs or
LC-LSTMs.

The input consists of two type of information
at step (i, j) in coupled-LSTMs: temporal dimen-
sion hi−1,j ,hi,j−1, ci−1,j , ci,j−1 and depth dimen-
sion xi,yj . The difference between TC-LSTMs and
LC-LSTMs is the dependence of information from
temporal and depth dimension.

Interaction Between Temporal Dimensions The
TC-LSTMs model the interactions at position (i, j)
by merging the internal memory ci−1,j ci,j−1 and
hidden state hi−1,j hi,j−1 along row and column di-
mensions. In contrast with TC-LSTMs, LC-LSTMs
firstly use two standard LSTMs in parallel, produc-
ing hidden states h1i,j and h

2
i,j along row and column

dimensions respectively, which are then merged to-
gether flowing next step.

Interaction Between Depth Dimension In TC-
LSTMs, each hidden state hi,j at higher layer re-
ceives a fusion of information xi and yj , flowed
from lower layer. However, in LC-LSTMs, the in-
formation xi and yj are accepted by two corre-
sponding LSTMs at the higher layer separately.

The two architectures have their own charac-
teristics, TC-LSTMs give more strong interactions
among different dimensions while LC-LSTMs en-
sures the two sequences interact closely without be-
ing conflated using two separated LSTMs.

Comparison of LC-LSTMs and word-by-word
Attention LSTMs The characteristic of attention
LSTMs is that they obtain the attention weighted
representation of one sentence considering he align-
ment between the two sentences, which is asymmet-
ric unidirectional encoding. Nevertheless, in LC-

LSTM, each hidden state of each step is obtained
with the consideration of interaction between two
sequences with symmetrical encoding fashion.

4 End-to-End Architecture for Sentence
Matching

In this section, we present an end-to-end deep ar-
chitecture for matching two sentences, as shown in
Figure 2.

4.1 Embedding Layer

To model the sentences with neural model, we firstly
need transform the one-hot representation of word
into the distributed representation. All words of
two sequences X = x1, x2, · · · , xn and Y =
y1, y2, · · · , ym will be mapped into low dimensional
vector representations, which are taken as input of
the network.

4.2 Stacked Coupled-LSTMs Layers

A basic block consists of five layers. We firstly use
four directional coupled-LSTMs to model the local
interactions with different information flows. And
then we sum the outputs of these LSTMs by aggre-
gation layer. To increase the learning capabilities of
the coupled-LSTMs, we stack the basic block on top
of each other.

4.2.1 Four Directional Coupled-LSTMs Layers
The C-LSTMs is defined along a certain pre-

defined direction, we can extend them to access to
the surrounding context in all directions. Similar
to bi-directional LSTM, there are four directions in
coupled-LSTMs.

(h1i,j , c
1
i,j) = C-LSTMs(hi−1,j ,hi,j−1, ci−1,j , ci,j−1,xi,yj),

(h2i,j , c
2
i,j) = C-LSTMs(hi−1,j ,hi,j+1, ci−1,j , ci,j+1,xi,yj),

(h3i,j , c
3
i,j) = C-LSTMs(hi+1,j ,hi,j+1, ci+1,j , ci,j+1,xi,yj),

1706



(h4i,j , c
4
i,j) = C-LSTMs(hi+1,j ,hi,j−1, ci+1,j , ci,j−1,xi,yj).

4.2.2 Aggregation Layer
The aggregation layer sums the outputs of four di-

rectional coupled-LSTMs into a vector.

ĥi,j =
4∑

d=1

hdi,j , (13)

where the superscript t of hi,j denotes the different
directions.

4.2.3 Stacking C-LSTMs Blocks
To increase the capabilities of network of learning

multiple granularities of interactions, we stack sev-
eral blocks (four C-LSTMs layers and one aggrega-
tion layer) to form deep architectures.

4.3 Pooling Layer

The output of stacked coupled-LSTMs layers is a
tensor H ∈ Rn×m×d, where n andm are the lengths
of sentences, and d is the number of hidden neurons.
We apply dynamic pooling to automatically extract
Rp×q subsampling matrix in each slice Hi ∈ Rn×m,
similar to (Socher et al., 2011).

More formally, for each slice matrix Hi, we par-
tition the rows and columns of Hi into p×q roughly
equal grids. These grid are non-overlapping. Then
we select the maximum value within each grid
thereby obtaining a p× q × d tensor.

4.4 Fully-Connected Layer

The vector obtained by pooling layer is fed into a full
connection layer to obtain a final more abstractive
representation.

4.5 Output Layer

The output layer depends on the types of the tasks,
we choose the corresponding form of output layer.
There are two popular types of text matching tasks in
NLP. One is ranking task, such as community ques-
tion answering. Another is classification task, such
as textual entailment.

1. For ranking task, the output is a scalar matching
score, which is obtained by a linear transforma-
tion after the last fully-connected layer.

MQA RTE
Embedding size 100 100
Hidden layer size 50 50
Initial learning rate 0.05 0.005
Regularization 5E−5 1E−5
Pooling (p, q) (2,1) (1,1)

Table 1: Hyper-parameters for our model on two
tasks.

2. For classification task, the outputs are the prob-
abilities of the different classes, which is com-
puted by a softmax function after the last fully-
connected layer.

5 Training

Our proposed architecture can deal with different
sentence matching tasks. The loss functions varies
with different tasks. More concretely, we use max-
margin loss (Bordes et al., 2013; Socher et al., 2013)
for ranking task and cross-entropy loss for classifi-
cation task.

To minimize the objective, we use stochastic gra-
dient descent with the diagonal variant of AdaGrad
(Duchi et al., 2011). To prevent exploding gradients,
we perform gradient clipping by scaling the gradient
when the norm exceeds a threshold (Graves, 2013).

6 Experiment

In this section, we investigate the empirical perfor-
mances of our proposed model on two different text
matching tasks: classification task (recognizing tex-
tual entailment) and ranking task (matching of ques-
tion and answer).

6.1 Hyperparameters and Training

The word embeddings for all of the models are ini-
tialized with the 100d GloVe vectors (840B token
version, (Pennington et al., 2014)) and fine-tuned
during training to improve the performance. The
other parameters are initialized by randomly sam-
pling from uniform distribution in [−0.1, 0.1].

For each task, we take the hyperparameters which
achieve the best performance on the development set
via an small grid search over combinations of the ini-
tial learning rate [0.05, 0.0005, 0.0001], l2 regular-
ization [0.0, 5E−5, 1E−5, 1E−6] and the threshold

1707



value of gradient norm [5, 10, 100]. The final hyper-
parameters are set as Table 1.

6.2 Competitor Methods
• Neural bag-of-words (NBOW): Each sequence

as the sum of the embeddings of the words it
contains, then they are concatenated and fed to
a MLP.

• Single LSTM: A single LSTM to encode the
two sequences, which is used in (Rocktäschel
et al., 2015).

• Parallel LSTMs: Two sequences are encoded
by two LSTMs separately, then they are con-
catenated and fed to a MLP.

• Attention LSTMs: An attentive LSTM to en-
code two sentences into a semantic space,
which used in (Hermann et al., 2015;
Rocktäschel et al., 2015).

• Word-by-word Attention LSTMs: An improve-
ment of attention LSTM by introducing word-
by-word attention mechanism, which used in
(Hermann et al., 2015; Rocktäschel et al.,
2015).

6.3 Experiment-I: Recognizing Textual
Entailment

Recognizing textual entailment (RTE) is a task to de-
termine the semantic relationship between two sen-
tences. We use the Stanford Natural Language In-
ference Corpus (SNLI) (Bowman et al., 2015). This
corpus contains 570K sentence pairs, and all of the
sentences and labels stem from human annotators.
SNLI is two orders of magnitude larger than all other
existing RTE corpora. Therefore, the massive scale
of SNLI allows us to train powerful neural networks
such as our proposed architecture in this paper.

6.3.1 Results
Table 2 shows the evaluation results on SNLI. The

3rd column of the table gives the number of param-
eters of different models without the word embed-
dings.

Our proposed two C-LSTMs models with four
stacked blocks outperform all the competitor mod-
els, which indicates that our thinner and deeper net-
work does work effectively.

Model k |θ|M Test
NBOW 100 80K 75.1
single LSTM
(Rocktäschel et al., 2015)

100 111K 80.9

parallel LSTMs
(Bowman et al., 2015)

100 221K 77.6

Attention LSTMs
(Rocktäschel et al., 2015)

100 252K 82.3

Attention(w-by-w) LSTMs
(Rocktäschel et al., 2015)

100 252K 83.5

LC-LSTMs (Single Direction) 50 45K 80.5
LC-LSTMs 50 45K 80.9
four stacked LC-LSTMs 50 135K 84.3
TC-LSTMs (Single Direction) 50 77.5K 80.1
TC-LSTMs 50 77.5K 81.6
four stacked TC-LSTMs 50 190K 85.1

Table 2: Results on SNLI corpus.

Besides, we can see both LC-LSTMs and TC-
LSTMs benefit from multi-directional layer, while
the latter obtains more gains than the former. We at-
tribute this discrepancy between two models to their
different mechanisms of controlling the information
flow from depth dimension.

Compared with attention LSTMs, our two mod-
els achieve comparable results to them using much
fewer parameters (nearly 1/5). By stacking C-
LSTMs2 , the performance of them are improved
significantly, and the four stacked TC-LSTMs
achieve 85.1% accuracy on this dataset.

Moreover, we can see TC-LSTMs achieve better
performance than LC-LSTMs on this task, which
need fine-grained reasoning over pairs of words as
well as phrases.

6.3.2 Understanding Behaviors of Neurons in
C-LSTMs

To get an intuitive understanding of how the C-
LSTMs work on this problem, we examined the neu-
ron activations in the last aggregation layer while
evaluating the test set using TC-LSTMs. We find
that some cells are bound to certain roles.

Let hi,j,k denotes the activation of the k-th neu-
ron at the position of (i, j), where i ∈ {1, . . . , n}
and j ∈ {1, . . . ,m}. By visualizing the hidden state
hi,j,k and analyzing the maximum activation, we

2To make a fair comparison, we also train a stacked
attention-based LSTM with the same setting as our models,
while it does not make significant improvement with 83.7% ac-
curacy.

1708



Index of Cell Word or Phrase Pairs
3-th (in a pool, swimming), (near a fountain, next to the ocean), (street, outside)
9-th (doing a skateboard, skateboarding), (sidewalk with, inside), (standing, seated)
17-th (blue jacket, blue jacket), (wearing black, wearing white), (green uniform, red uniform)
25-th (a man, two other men), (a man, two girls), (an old woman, two people)

Table 3: Multiple interpretable neurons and the word-pairs/phrase-pairs captured by these neurons.

A

pe
rs

on i
s

w
ea

rin
g a

gr
ee

n

sh
ir

t .

. 
over 
hunched 
pants 
black 
and 
shirt 
red 
a 
in 
person 
A 

−0.5 0 0.5

(a) 3rd neuron

A

pe
rs

on i
s

ou
ts

id
e .

. 

street 

the 

down 

walking 

is 

jeans 

wearing 

woman 

A 

0 0.2 0.4 0.6

(b) 17th neuron

Figure 3: Illustration of two interpretable neurons
and some word-pairs capture by these neurons. The
darker patches denote the corresponding activations
are higher.

can find that there exist multiple interpretable neu-
rons. For example, when some contextualized local
perspectives are semantically related at point (i, j)
of the sentence pair, the activation value of hidden
neuron hi,j,k tend to be maximum, meaning that the
model could capture some reasoning patterns.

Figure 3 illustrates this phenomenon. In Fig-
ure 3(a), a neuron shows its ability to monitor
the local contextual interactions about color. The
activation in the patch, including the word pair
“(red, green)”, is much higher than others.
This is informative pattern for the relation predic-
tion of these two sentences, whose ground truth
is contradiction. An interesting thing is there
are two words describing color in the sentence
“ A person in a red shirt and black
pants hunched over.”. Our model ignores
the useless word “black”, which indicates that this
neuron selectively captures pattern by contextual un-
derstanding, not just word level interaction.

In Figure 3(b), another neuron shows that it
can capture the local contextual interactions,
such as “(walking down the street,

outside)”. These patterns can be easily captured
by pooling layer and provide a strong support for
the final prediction.

Table 3 illustrates multiple interpretable neurons
and some representative word or phrase pairs which
can activate these neurons. These cases show that
our models can capture contextual interactions be-
yond word level.

6.3.3 Error Analysis
Although our models C-LSTMs are more sen-

sitive to the discrepancy of the semantic capacity
between two sentences, some semantic mistakes at
the phrasal level still exist. For example, our models
failed to capture the key informative pattern when
predicting the entailment sentence pair “A girl
takes off her shoes and eats blue
cotton candy/The girl is eating
while barefoot.”

Besides, despite the large size of the training
corpus, it’s still very different to solve some
cases, which depend on the combination of the
world knowledge and context-sensitive infer-
ences. For example, given an entailment pair
“a man grabs his crotch during a
political demonstration/The man
is making a crude gesture”, all models
predict “neutral”. This analysis suggests that
some architectural improvements or external world
knowledge are necessary to eliminate all errors
instead of simply scaling up the basic model.

6.4 Experiment-II: Matching Question and
Answer

Matching question answering (MQA) is a typical
task for semantic matching. Given a question, we
need select a correct answer from some candidate
answers.

In this paper, we use the dataset collected from
Yahoo! Answers with the getByCategory function

1709



Model k P@1(5) P@1(10)
Random Guess - 20.0 10.0
NBOW 50 63.9 47.6
single LSTM 50 68.2 53.9
parallel LSTMs 50 66.9 52.1
Attention LSTMs 50 73.5 62.0
Attention LSTMs (w-by-w) 50 75.1 64.0
LC-LSTMs (Single Direction) 50 75.4 63.0
LC-LSTMs 50 76.1 64.1
three stacked LC-LSTMs 50 78.5 66.2
TC-LSTMs (Single Direction) 50 74.3 62.4
TC-LSTMs 50 74.9 62.9
three stacked TC-LSTMs 50 77.0 65.3

Table 4: Results on Yahoo question-answer pairs
dataset.

provided in Yahoo! Answers API, which produces
963, 072 questions and corresponding best answers.
We then select the pairs in which the length of ques-
tions and answers are both in the interval [4, 30], thus
obtaining 220, 000 question answer pairs to form the
positive pairs.

For negative pairs, we first use each question’s
best answer as a query to retrieval top 1, 000 results
from the whole answer set with Lucene, where 4 or
9 answers will be selected randomly to construct the
negative pairs.

The whole dataset is divided into training, vali-
dation and testing data with proportion 20 : 1 : 1.
Moreover, we give two test settings: selecting the
best answer from 5 and 10 candidates respectively.

6.4.1 Results

Results of MQA are shown in the Table 4. For our
models, due to stacking block more than three layers
can not make significant improvements on this task,
we just use three stacked C-LSTMs.

By analyzing the evaluation results of question-
answer matching in table 4, we can see strong in-
teraction models (attention LSTMs, our C-LSTMs)
consistently outperform the weak interaction mod-
els (NBOW, parallel LSTMs) with a large margin,
which suggests the importance of modelling strong
interaction of two sentences.

Our proposed two C-LSTMs surpass the competi-
tor methods and C-LSTMs augmented with multi-
directions layers and multiple stacked blocks fully
utilize multiple levels of abstraction to directly boost
the performance.

Additionally, LC-LSTMs is superior to TC-
LSTMs. The reason may be that MQA is a relative
simple task, which requires less reasoning abilities,
compared with RTE task. Moreover, the parameters
of LC-LSTMs are less than TC-LSTMs, which en-
sures the former can avoid suffering from overfitting
on a relatively smaller corpus.

7 Related Work

Our architecture for sentence pair encoding can be
regarded as strong interaction models, which have
been explored in previous models.

An intuitive paradigm is to compute similari-
ties between all the words or phrases of the two
sentences. Socher et al. (2011) firstly used this
paradigm for paraphrase detection. The represen-
tations of words or phrases are learned based on re-
cursive autoencoders.

A major limitation of this paradigm is the inter-
action of two sentence is captured by a pre-defined
similarity measure. Thus, it is not easy to in-
crease the depth of the network. Compared with
this paradigm, we can stack our C-LSTMs to model
multiple-granularity interactions of two sentences.

Rocktäschel et al. (2015) used two LSTMs
equipped with attention mechanism to capture the it-
eration between two sentences. This architecture is
asymmetrical for two sentences, where the obtained
final representation is sensitive to the two sentences’
order.

Compared with the attentive LSTM, our proposed
C-LSTMs are symmetrical and model the local con-
textual interaction of two sequences directly.

8 Conclusion and Future Work

In this paper, we propose an end-to-end deep archi-
tecture to capture the strong interaction information
of sentence pair. Experiments on two large scale text
matching tasks demonstrate the efficacy of our pro-
posed model and its superiority to competitor mod-
els. Besides, we present an elaborate qualitative
analysis of our models, which gives an intuitive un-
derstanding how our model worked.

In future work, we would like to incorporate some
gating strategies into the depth dimension of our pro-
posed models, like highway or residual network, to
enhance the interactions between depth and other di-

1710



mensions thus training more deep and powerful neu-
ral networks.

Acknowledgments
We would like to thank the anonymous reviewers for
their valuable comments. This work was partially
funded by National Natural Science Foundation of
China (No. 61532011 and 61672162), the National
High Technology Research and Development Pro-
gram of China (No. 2015AA015408).

References
D. Bahdanau, K. Cho, and Y. Bengio. 2014. Neural ma-

chine translation by jointly learning to align and trans-
late. ArXiv e-prints, September.

Antoine Bordes, Nicolas Usunier, Alberto Garcia-Duran,
Jason Weston, and Oksana Yakhnenko. 2013. Trans-
lating embeddings for modeling multi-relational data.
In NIPS.

Samuel R. Bowman, Gabor Angeli, Christopher Potts,
and Christopher D. Manning. 2015. A large anno-
tated corpus for learning natural language inference.
In Proceedings of the 2015 Conference on Empirical
Methods in Natural Language Processing.

Wonmin Byeon, Thomas M Breuel, Federico Raue, and
Marcus Liwicki. 2015. Scene labeling with lstm re-
current neural networks. In Proceedings of the IEEE
Conference on Computer Vision and Pattern Recogni-
tion, pages 3547–3555.

John Duchi, Elad Hazan, and Yoram Singer. 2011.
Adaptive subgradient methods for online learning and
stochastic optimization. The Journal of Machine
Learning Research, 12:2121–2159.

Jeffrey L Elman. 1990. Finding structure in time. Cog-
nitive science, 14(2):179–211.

Alex Graves and Jürgen Schmidhuber. 2005. Frame-
wise phoneme classification with bidirectional lstm
and other neural network architectures. Neural Net-
works, 18(5):602–610.

Alex Graves and Jürgen Schmidhuber. 2009. Offline
handwriting recognition with multidimensional recur-
rent neural networks. In Advances in Neural Informa-
tion Processing Systems, pages 545–552.

Alex Graves, Santiago Fernández, and Jürgen Schmid-
huber. 2007. Multi-dimensional recurrent neural net-
works. In Artificial Neural Networks–ICANN 2007,
pages 549–558. Springer.

Alex Graves. 2013. Generating sequences with recurrent
neural networks. arXiv preprint arXiv:1308.0850.

Hua He, Kevin Gimpel, and Jimmy Lin. 2015. Multi-
perspective sentence similarity modeling with convo-
lutional neural networks. In Proceedings of the 2015

Conference on Empirical Methods in Natural Lan-
guage Processing, pages 1576–1586.

Karl Moritz Hermann, Tomas Kocisky, Edward Grefen-
stette, Lasse Espeholt, Will Kay, Mustafa Suleyman,
and Phil Blunsom. 2015. Teaching machines to read
and comprehend. In Advances in Neural Information
Processing Systems, pages 1684–1692.

Sepp Hochreiter and Jürgen Schmidhuber. 1997. Long
short-term memory. Neural computation, 9(8):1735–
1780.

Baotian Hu, Zhengdong Lu, Hang Li, and Qingcai Chen.
2014. Convolutional neural network architectures for
matching natural language sentences. In Advances in
Neural Information Processing Systems.

Nal Kalchbrenner, Edward Grefenstette, and Phil Blun-
som. 2014. A convolutional neural network for mod-
elling sentences. In Proceedings of ACL.

Nal Kalchbrenner, Ivo Danihelka, and Alex Graves.
2015. Grid long short-term memory. arXiv preprint
arXiv:1507.01526.

PengFei Liu, Xipeng Qiu, Xinchi Chen, Shiyu Wu, and
Xuanjing Huang. 2015. Multi-timescale long short-
term memory neural network for modelling sentences
and documents. In Proceedings of the Conference on
EMNLP.

Pengfei Liu, Xipeng Qiu, Jifan Chen, and Xuanjing
Huang. 2016. Deep fusion LSTMs for text seman-
tic matching. In Proceedings of Annual Meeting of the
Association for Computational Linguistics.

Jeffrey Pennington, Richard Socher, and Christopher D
Manning. 2014. Glove: Global vectors for word
representation. Proceedings of the Empiricial Meth-
ods in Natural Language Processing (EMNLP 2014),
12:1532–1543.

Xipeng Qiu and Xuanjing Huang. 2015. Convolutional
neural tensor network architecture for community-
based question answering. In Proceedings of Interna-
tional Joint Conference on Artificial Intelligence.

Tim Rocktäschel, Edward Grefenstette, Karl Moritz Her-
mann, Tomáš Kočiskỳ, and Phil Blunsom. 2015. Rea-
soning about entailment with neural attention. arXiv
preprint arXiv:1509.06664.

Mike Schuster and Kuldip K Paliwal. 1997. Bidirec-
tional recurrent neural networks. Signal Processing,
IEEE Transactions on, 45(11):2673–2681.

Richard Socher, Eric H Huang, Jeffrey Pennin, Christo-
pher D Manning, and Andrew Y Ng. 2011. Dynamic
pooling and unfolding recursive autoencoders for para-
phrase detection. In Advances in Neural Information
Processing Systems.

Richard Socher, Danqi Chen, Christopher D Manning,
and Andrew Ng. 2013. Reasoning with neural ten-
sor networks for knowledge base completion. In

1711



Advances in Neural Information Processing Systems,
pages 926–934.

Ilya Sutskever, Oriol Vinyals, and Quoc VV Le. 2014.
Sequence to sequence learning with neural networks.
In Advances in Neural Information Processing Sys-
tems, pages 3104–3112.

Shengxian Wan, Yanyan Lan, Jiafeng Guo, Jun Xu, Liang
Pang, and Xueqi Cheng. 2016. A deep architecture for
semantic matching with multiple positional sentence
representations. In AAAI.

Wenpeng Yin and Hinrich Schütze. 2015. Convolutional
neural network for paraphrase identification. In Pro-
ceedings of the 2015 Conference of the North Ameri-
can Chapter of the Association for Computational Lin-
guistics: Human Language Technologies, pages 901–
911.

Wenpeng Yin, Hinrich Schütze, Bing Xiang, and Bowen
Zhou. 2015. Abcnn: Attention-based convolutional
neural network for modeling sentence pairs. arXiv
preprint arXiv:1512.05193.

1712


