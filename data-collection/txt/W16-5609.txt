



















































Identifying Stance by Analyzing Political Discourse on Twitter


Proceedings of 2016 EMNLP Workshop on Natural Language Processing and Computational Social Science, pages 66–75,
Austin, TX, November 5, 2016. c©2016 Association for Computational Linguistics

Identifying Stance by Analyzing Political Discourse on Twitter

Kristen Johnson
Purdue University

West Lafayette, IN 47907, USA
john1187@purdue.edu

Dan Goldwasser
Purdue University

West Lafayette, IN 47907, USA
dgoldwas@purdue.edu

Abstract

Politicians often use Twitter to express their
beliefs, stances on current political issues,
and reactions concerning national and inter-
national events. Since politicians are scruti-
nized for what they choose or neglect to say,
they craft their statements carefully. Thus de-
spite the limited length of tweets, their content
is highly indicative of a politician’s stances.
We present a weakly supervised method for
understanding the stances held by politicians,
on a wide array of issues, by analyzing how
issues are framed in their tweets and their
temporal activity patterns. We combine these
components into a global model which collec-
tively infers the most likely stance and agree-
ment patterns.

1 Introduction

Recently the popularity of traditional media outlets
such as television and printed press has decreased,
causing politicians to turn their attention to social
media outlets, which allow them to directly access
the public, express their beliefs, and react to cur-
rent events. This trend emerged during the 2008 U.S.
presidential election campaign and has since moved
to the mainstream – in the 2016 campaign, all can-
didates employ social media platforms. One of the
most notable examples of this trend is the micro-
blogging outlet Twitter, which unlike its predeces-
sors, requires candidates to compress their ideas,
political stances, and reactions into 140 character
long tweets. As a result, candidates have to cleverly
choose how to frame controversial issues, as well as
react to events and each other (Mejova et al., 2013;
Tumasjan et al., 2010).

In this work we present a novel approach for mod-
eling the microblogging activity of presidential can-
didates and other prominent politicians. We look
into two aspects of the problem, stance prediction
over a wide array of issues, as well as agreement
and disagreement patterns between politicians over
these issues. While the two aspects are related, we
argue they capture different information, as identify-
ing agreement patterns reveals alliances and rivalries
between candidates, across and inside their party.
We show that understanding the political discourse
on microblogs requires modeling both the content
of posted messages as well as the social context in
which they are generated, and suggest a joint model
capturing both aspects.

Converse to other works predicting stance per in-
dividual tweet (SemEval, 2016), we use the over-
all Twitter behavior to predict a politician’s stance
on an issue. We argue that these settings are bet-
ter suited for the political arena on Twitter. Given
the limit of 140 characters, the stance relevance of
a tweet is not independent of the social context in
which it was generated. In an extreme case, even the
lack of Twitter activity on certain topics can be in-
dicative of a stance. Additionally, framing issues in
order to create bias towards their stance is a tool
often used by politicians to contextualize the dis-
cussion (Tsur et al., 2015; Card et al., 2015; Boyd-
stun et al., 2014). Previous works exploring fram-
ing analyze text in traditional settings, such as con-
gressional speeches or newspaper articles. To apply
framing analysis to Twitter data, we allow tweets to
hold multiple frames when necessary, as we find that
on average many tweets are relevant to two frames
per issue. This approach allows our model to make
use of changing and similar framing patterns over

66



(1) Hillary Clinton (@HillaryClinton): We need to keep guns

out of the hands of domestic abusers and convicted stalkers .

(2) Donald Trump (@realDonaldTrump): Politicians are trying

to chip away at the 2nd Amendment . I won’t let them take away

our guns !

(3) Bernie Sanders (@SenSanders): We need sensible

gun-control legislation which prevents guns from being used

by people who should not have them .

Figure 1: Tweets on the issue of gun control, highlight-
ing issue indicators in green and different frame indica-
tors in yellow.

politicians’ timelines in order to increase our pre-
diction accuracy.

For example, consider the issue of gun control.
Figure 1 shows three issue-related tweets by three
politicians. To correctly identify the stance taken by
each of the politicians, our model must combine
three aspects. First, the relevance of these tweets
to the question can be identified using issue indi-
cators (marked in green). Second, the similarity be-
tween the stances taken by two of the three politi-
cians can be identified by observing how the issue is
framed (marked in yellow). In this example, tweets
(1) and (3) frame the issue of gun control as a mat-
ter of safety, while (2) frames it as an issue related
to personal freedom, thus revealing the agreement
and disagreement patterns between them. Finally,
we note the strong negative sentiment of tweet (1).
Notice that each aspect individually might not con-
tain sufficient information for correct classification,
but combining all three, by propagating the stance
bias (derived from analyzing the negative sentiment
of (1)) to politicians likely to hold similar or oppos-
ing views (derived from frame analysis), leads to a
more reliable prediction.

Given the dynamic nature of this domain, we de-
sign our approach to use minimal supervision and
naturally adapt to new issues. Our model builds on
several weakly supervised local learners that use
a small seed set of issue and frame indicators to
characterize the stance of tweets (based on lexical
heuristics (O’Connor et al., 2010) and framing di-
mensions (Card et al., 2015)) and activity statistics
which capture temporally similar patterns between
politicians’ Twitter activity. Our final model repre-
sents agreement and stance bias by combining these
weak models into a weakly supervised joint model
through Probabilistic Soft Logic (PSL), a recently

1

DEM (P1)
TWEETS (P1,GUN)
FRAMEGUN (P1, SAFETY)

SAME_PARTY (P1,P3) ~SAME_PARTY (P1,P2)

~SAME_PARTY (P2,P3)
SAME_STANCEGUN(P2,P3) ?

Pr o(P1,GUN) ?

3

DEM (P3)
Tweet s (p3,Gun )
FRAMEGUN (P3, SAFETY)

Pr o(P3,Gun) ?

2

~DEM (P2)
TWEETS (P2,GUN )
Fr ameGUN (P2, Fr eedom)

Pr o(P2,Gun) ?

SAME_STANCEGUN (P1,P3) ? SAME_STANCEGUN (P1,P2) ?

asdasd

Figure 2: Relational Representation of Politicians’
Twitter Activity. P1, P2, and P3 represent 3 different
politicians. GUN refers to the issue of gun control;
SAFETY and FREEDOM refer to different frames. Predic-
tion target predicates are marked in red.

introduced probabilistic modeling framework (Bach
et al., 2013). PSL combines these aspects declar-
atively by specifying high level rules over a rela-
tional representation of the politicians’ activities (as
shown in Figure 2), which is further compiled into
a graphical model called a hinge-loss Markov ran-
dom field (Bach et al., 2013), and used to make pre-
dictions about stance and agreement between politi-
cians.

We analyze the Twitter activity of 32 prominent
U.S. politicians, some of which were candidates for
the U.S. 2016 presidential election. We collected
their recent tweets and stances on 16 different is-
sues, which were used for evaluation purposes. Our
experiments demonstrate the effectiveness of our
global modeling approach which outperforms the
weak learners that provide the initial supervision.

2 Related Work

To the best of our knowledge this is the first work
to use Twitter data, specifically content, frames, and
temporal activity, to predict politicians’ stances.
Previous works (Sridhar et al., 2015; Hasan and Ng,
2014; Abu-Jbara et al., 2013; Walker et al., 2012;
Abbott et al., 2011; Somasundaran and Wiebe, 2010;
Somasundaran and Wiebe, 2009) have studied min-
ing opinions and predicting stances in online debate
forum data, exploiting argument and threaded con-
versation structures, or analyzed social interaction

67



and group structure (Sridhar et al., 2015; Abu-Jbara
et al., 2013; West et al., 2014). In our Twitter dataset,
there were few “@” mention or retweet examples
forming a conversation concerning the investigated
issues, thus we did not have access to argument or
conversation structures for analysis. Works which
focus on inferring signed social networks (West et
al., 2014), stance classification (Sridhar et al., 2015),
social group modeling (Huang et al., 2012), and PSL
collective classification (Bach et al., 2015) are clos-
est to our work, but these typically operate in su-
pervised settings. In this work, we use PSL without
direct supervision, to assign soft values (in the range
of 0 to 1) to output variables 1.

Using Twitter to analyze political discourse and
influence has gained in popularity over recent years.
Predicting characteristics of Twitter users, including
political party affiliation has been explored (Volkova
et al., 2015; Volkova et al., 2014; Conover et al.,
2011). Previous works have also focused on sen-
timent analysis (Pla and Hurtado, 2014; Bakliwal
et al., 2013), predicting ideology (Djemili et al.,
2014), analyzing types of tweets and Twitter net-
work effects around political events (Maireder and
Ausserhofer, 2013), automatic polls based on Twit-
ter sentiment and political forecasting using Twit-
ter (Bermingham and Smeaton, 2011; O’Connor et
al., 2010; Tumasjan et al., 2010), as well as uses of
distant supervision (Marchetti-Bowick and Cham-
bers, 2012).

Analyzing political tweets specifically has also
attracted considerable interest. Recently, SemEval
Task 6 (SemEval, 2016) aimed to detect the stance
of individual tweets. Unlike this task and most re-
lated work on stance prediction (e.g., those men-
tioned above), we do not assume that each tweet ex-
presses a stance. Instead, we combine tweet content
and temporal indicators into a representation of a
politician’s overall Twitter behavior, to determine if
these features are indicative of a politician’s stance.
This approach allows us to capture when politicians
fail to tweet about a topic, which indicates a lack of
stance as well.

To the best of our knowledge, this work is also the
first attempt to analyze issue framing in Twitter data.
To do so we used the frame guidelines developed
by Boydstun et al. (2014). Issue framing is related to

1Conversely, Markov Logic Networks assign hard (0 or 1)
values to model variables.

both analyzing biased language (Greene and Resnik,
2009; Recasens et al., 2013) and subjectivity (Wiebe
et al., 2004). Several previous works have explored
topic framing of public statements, congressional
speeches, and news articles (Tsur et al., 2015; Card
et al., 2015; Baumer et al., 2015) . Other works fo-
cus on identifying and measuring political ideolo-
gies (Iyyer et al., 2014; Bamman and Smith, 2015;
Sim et al., 2013; Lewenberg et al., 2016) and poli-
cies (Gerrish and Blei, 2012; Nguyen et al., 2015;
Grimmer, 2010).

Finally, unsupervised and weakly supervised
models of Twitter data for several various tasks have
been suggested, such as user profile extraction (Li et
al., 2014b), life event extraction (Li et al., 2014a),
and conversation modeling (Ritter et al., 2010). Fur-
ther, Eisenstein (2013) discusses methods for deal-
ing with the unique language used in micro-blogging
platforms.

3 Data and Problem Setting
REPUBLICAN POLITICIANS

Jeb Bush, Ben Carson, Chris Christie, Ted Cruz, Carly Fio-
rina, Lindsey Graham, Mike Huckabee, Bobby Jindal, John
Kasich, George Pataki, Rand Paul, Rick Perry, Marco Ru-
bio, Rick Santorum, Donald Trump, Scott Walker

DEMOCRATIC POLITICIANS

Joe Biden, Lincoln Chafee, Hillary Clinton, Kirsten Gilli-
brand, John Kerry, Ben Lujan, Ed Markey, Martin O’Malley,
Nancy Pelosi, Harry Reid, Bernie Sanders, Chuck Schumer,
Jon Tester, Mark Warner, Elizabeth Warren, Jim Webb

Table 1: Politicians tracked in this study.

Collection and Pre-Processing of Tweets: We
collected tweets for the 32 politicians listed in Ta-
ble 1, initially beginning with those politicians par-
ticipating in the 2016 U.S. presidential election (16
Republicans and 5 Democrats). To increase rep-
resentation of Democrats, we collected tweets of
Democrats who hold leadership roles within their
party. For all 32 politicians we have a total of 99,161
tweets, with an average of 3,000 per person. There
are 39,353 Democrat and 59,808 Republican tweets.

Using tweets from both parties, we compiled a
set of frequently appearing keywords for each is-
sue, with an average of seven keywords per issue.
A Python script was then used on these preselected
keywords to filter all tweets, keeping only those that
represent our 16 political issues of interest (shown in

68



ISSUE QUESTION
ABORTION Do you support abortion?
ACA Do you support the Patient Protection and Affordable Care Act (Obamacare)?
CONFEDERATE Should the federal government allow states to fly the confederate flag?
DRUGS Do you support the legalization of Marijuana?
ENVIRONMENT Should the federal government continue to give tax credits and subsidies to the wind power industry?
GUNS Do you support increased gun control?
IMMIGRATION Do you support stronger measures to increase our border security?
IRAN Should the U.S. conduct targeted airstrikes on Irans nuclear weapons facilities?
ISIS Should the U.S. formally declare war on ISIS?
MARRIAGE Do you support the legalization of same sex marriage?
NSA Do you support the Patriot Act?
PAY Should employers be required to pay men and women, who perform the same work, the same salary?
RELIGION Should a business, based on religious beliefs, be able to deny service to a customer?
SOCIAL SECURITY Should the government raise the retirement age for Social Security?
STUDENT Would you support increasing taxes on the rich in order to reduce interest rates for student loans?
TPP Do you support the Trans-Pacific Partnership?

Table 2: Issues taken from ISideWith.com and their corresponding Yes/No questions. Each issue serves as a
prediction target for each politician. For example, for each politician we predict if they are for (PRO) or against
(¬PRO) increased gun control (GUNS), as well as if every pair of politicians shares the same stance for that issue
(SAMESTANCEI ).

Table 2), and automatically eliminating all irrelevant
tweets (e.g., those about personal issues).

Annotating Stances and Agreement: We used
ISideWith.com, a popular website that matches
users to politicians based on their answers to a series
of 58 questions, to choose 16 of these issues (shown
in Table 2) for our prediction goals. ISideWith.
com uses a range of yes/no answers in their ques-
tions and provides proof of the politician’s stance
on that issue, if available, through public informa-
tion such as quotes. Since we use the stances as the
ground truth for evaluating our prediction, all politi-
cians with unavailable answers or those not listed on
the site were manually annotated via online searches
of popular newspapers, political channels, and vot-
ing records. Since ISideWith.com does not con-
tain answers to all questions for all politicians, es-
pecially those that are less popular, we design our
approach to be generalizable to such situations by
requiring minimal supervision.

Predicting Stance and Agreement: Based on the
collected stances, which represent our ground truth
of whether a politician is for or against an issue,
we define two target predicates using PSL notation
(see Section 4.1) to capture the desired output as soft
truth assignments to these predicates. The first pred-
icate, PRO(P1, ISSUE) captures the idea that politi-
cian P1 is in support of an ISSUE. Consequently,

an opposing stance would be captured by the nega-
tion: ¬PRO(P1, ISSUE). In this work, we do not
make use of stance correlations among party mem-
bers (Lewenberg et al., 2016; Maddox and Lilie,
1984). For example, in U.S. politics Republicans
are known to be against gun control and abortion,
while Democrats support both issues. Since we are
interested in determining the effectiveness of our lo-
cal models (described in Section 4.2) to capture the
stance of each politician, we do not encode such
cross-issue information into the models. Addition-
ally, in a weakly supervised setting, we assume we
do not have access to such information.

The second target predicate, SAMESTANCEI (P1,
P2) classifies if two politicians share a stance for
a given issue, i.e., if both are for or against an is-
sue, where I represents 1 of the 16 issues being in-
vestigated. Although the two predicates are clearly
inter-dependent, we model them as separate predi-
cates since they can depend on different Twitter be-
havioral and content cues and we can often identify
indicators of shared stance, without mention of the
actual stance.

4 From Local to Global Models of Twitter
Activity

Our approach uses a collection of weakly super-
vised local models to capture the similarities be-
tween stance bias, tweet content, and temporal ac-

69



tivity patterns of users’ timelines. These local mod-
els are used to provide the initial bias when learn-
ing the parameters of the global PSL model, which
uses PSL to combine all of the local models together
into a joint global model. In addition to the PSL lo-
cal model predicates (described below), we also use
directly observed information: party affiliation, de-
noted DEM(P1) for Democrat and ¬DEM(P1) for
Republican, and SAMEPARTY(P1, P2) to denote if
two politicians belong to the same party. As shown
by the baseline measurements in Section 5, local
information alone is not strong enough to capture
stance or agreement for politicians. However, by us-
ing PSL, we are able to build connections between
each local model in order to increase the overall ac-
curacy of each global model’s prediction.

4.1 Global Modeling using PSL
PSL is a recent declarative language for specify-
ing weighted first-order logic rules. A PSL model
is specified using a set of weighted logical for-
mulas, which are compiled into a special class of
graphical model, called a hinge-loss MRF, defining a
probability distribution over the possible continuous
value assignments to the model’s random variables
and allowing the model to scale easily (Bach et al.,
2015). The defined probability density function has
the form:

P (Y | X) = 1
Z

exp

(
−

M∑
r=1

λrφr(Y , X)

)

where λ is the weight vector, Z is a normalization
constant, and

φr(Y,X) = (max{lr(Y, X), 0})ρr

is the hinge-loss potential corresponding to the
instantiation of a rule, specified by a linear func-
tion lr, and an optional exponent ρr ∈ 1, 2. The
weights of the rules are learned using maximum-
likelihood estimation, which in our weakly super-
vised setting was estimated using the Expectation-
Maximization algorithm. For more details we refer
the reader to Bach et al. (2015).

Specified PSL rules have the form:

λ1 : P1(x) ∧ P2(x, y)→ P3(y),
λ2 : P1(x) ∧ P4(x, y)→ ¬P3(y)

where P1, P2, P3, P4 are predicates, and x, y are
variables. Each rule is associated with a weight λ,
which indicates its importance in the model. Given
concrete constants a, b respectively instantiating the

variables x, y, the mapping of the model’s atoms
to soft [0,1] assignments will be determined by the
weights assigned to each one of the rules. For ex-
ample, if λ1 > λ2, the model will prefer P3(b)
to its negation. This contrasts with “classical” or
other probabilistic logical models in which rules are
strictly true or false. In our work, the constant sym-
bols correspond to politicians and predicates repre-
sent party affiliation, Twitter activities, and similari-
ties between politicians based on Twitter behaviors.

4.2 Local Models of Basic Twitter Activity
Issue: We use a keyword based heuristic, simi-
lar to the approach described in O’Connor et al.
(2010), to capture which issues politicians are tweet-
ing about. Each issue is associated with a small set
of keywords, which may be mutually exclusive, such
as those concerning Iran or Environment. However,
some may fall under multiple issues at once (e.g.,
religion may indicate the tweet refers to ISIS, Reli-
gion, or Marriage). The majority of matching key-
words determines the issue of the tweet, with rare
cases of ties manually resolved. The output of this
classifier is all of the issue-related tweets of a politi-
cian, which are used as input for the PSL predi-
cate TWEETS(P1, ISSUE). This binary predicate in-
dicates if politician P1 has tweeted about the issue
or not.

Sentiment Analysis: Based on the idea that the
sentiment of a tweet can help expose a politician’s
stance on a certain issue, we use OpinionFinder
2.0 (Wilson et al., 2005) to label each politician’s
issue-related tweets as positive, negative, or neutral.
We observed, however, that for all politicians, a ma-
jority of tweets will be labeled as neutral. This may
be caused by the difficulty of labeling sentiment for
Twitter data. If a politician has no positive or nega-
tive tweets, they are assigned their party’s majority
sentiment assignment for that issue. This output is
used as input to the PSL predicates TWEETPOS(P1,
ISSUE) and TWEETNEG(P1, ISSUE).

Agreement and Disagreement: To determine
how well tweet content similarity can capture stance
agreement, we computed the pair-wise cosine simi-
larity between all of the politicians. Due to the us-
age of similar words per issue, most politicians are
grouped together, even across different parties. To
overcome this noise, we compute the frequency of
similar words within tweets about each issue. For

70



PSL MODEL EXAMPLE OF PSL RULE
LOCAL BASELINE (LB) LOCALSAMESTANCEI (P1, P2) →SAMESTANCEI (P1, P2)

TWEETS(P1,ISSUE) ∧TWEETPOS(P1,ISSUE) → PRO(P1, ISSUE)
MODEL 1 (M1) SAMEPARTY(P1, P2) ∧DEM(P1) →PRO(P2, ISSUE)

SAMEPARTY(P1, P2) →SAMESTANCEI (P1, P2)
MODEL 2 (M2) TWEETS(P1, ISSUE) ∧DEM(P1) →PRO(P1, ISSUE)

TWEETPOS(P1, ISSUE) ∧TWEETPOS(P2, ISSUE) →SAMESTANCEI (P1, P2)
MODEL 3 (M3) LOCALSAMESTANCEI (P1, P2) ∧PRO(P1, ISSUE) →PRO(P2, ISSUE)

SAMETEMPORALACTIVITYI (P1, P2) ∧SAMEPARTY(P1, P2) → SAMESTANCEI (P1, P2)
FRAME(P1, ISSUE) ∧FRAME(P2, ISSUE) →SAMESTANCEI (P1, P2)

Table 3: Subset of examples of the rules that are used by each PSL model. Each model also contains negated versions
of rules, as well as similar rules where DEM has been replaced by ¬DEM to represent Republicans.

each issue, all of a politician’s tweets are aggre-
gated and the frequency of each word is compared
to all other politicians’ word frequencies. Politi-
cians, P1 and P2, are considered to have a similar
LOCALSAMESTANCEI (P1, P2) if their frequency
counts per word for an issue are within the same
range.

4.3 Baseline PSL Model: Using Local Models
Directly

Previous stance classification works typically pre-
dict stance based on a single piece of text (e.g., fo-
rum posts or tweets) in a supervised setting, mak-
ing it difficult to directly compare to our approach.
To provide some comparison, we implement a base-
line model which, as expected, has a weaker perfor-
mance than our models. The baseline model does not
take advantage of the global modeling framework,
but instead learns weights over the rules listed in the
first two lines of Table 3. These rules directly map
the output of the local noisy models to PSL target
predicates.

4.4 PSL Model 1: Party Based Agreement

The tendency of politicians to vote with their politi-
cal party on most issues is encoded via the Model
1 PSL rules listed in Table 3, which aim to cap-
ture party based agreement. For some issues we ini-
tially assume Democrats (DEM) are for an issue,
while Republicans (¬DEM) are against that issue, or
vice versa. In the latter case, the rules of the model
would change, e.g. the second rule would become:
¬DEM(P1)→PRO(P1, ISSUE), and likewise for all
other rules. Similarly, if two politicians are in the
same party, we expect them to have the same stance,
or agree, on an issue. For all PSL rules, the reverse
also holds, e.g., if two politicians are not in the same

party, we expect them to have different stances.

4.5 PSL Model 2: Basic Twitter Activity

Model 2 builds upon the initial party line bias of
Model 1. In addition to political party based infor-
mation, we also include representations of the politi-
cian’s Twitter activity, as shown in Table 3. This in-
cludes whether or not a politician tweets about an
issue (TWEETS) and what sentiment is expressed in
those tweets. The predicate TWEETPOS models if
a politician tweets positively on the issue, whereas
TWEETNEG models negative sentiment. Two differ-
ent predicates are used instead of the negation of
TWEETPOS, which would cause all politicians for
which there are no tweets (or sentiment) on that is-
sue to also be considered.

4.6 Local Models of High Level Twitter
Activity

Temporal Activity Patterns: We observed from
reading Twitter feeds that most politicians will com-
ment on an event the day it happens. For general
issues, politicians comment as often as desired to
express their support or lack thereof for a particu-
lar issue. To capture patterns between politicians, we
align their timelines based on days where they have
tweeted about an issue. When two or more politi-
cians tweet about the same issue on the same day,
they are considered to have similar temporal activ-
ity, which may indicate stance agreement. This in-
formation is used as input for our PSL predicate
SAMETEMPORALACTIVITYI (P1, P2).

Political Framing: The way politicians choose to
contextualize their tweets on an issue is strongly in-
dicative of their stance on that issue. To investigate
this, we compiled a list of unique keywords for each

71



political framing dimension as described in Boyd-
stun et al. (2014) and Card et al. (2015). We use
the keyword matching approach described in Sec-
tion 4.2 to classify all tweets into a political frame
with some tweets belonging to multiple frames. We
sum over the total number of each frame type and
use the frame with the maximum and second largest
count as that politician’s frames for that issue. In
the event of a tie we assign the frame that ap-
pears most frequently within that politician’s party.
These frames are used as input to the PSL predicate
FRAME(P1, ISSUE).

4.7 PSL Model 3: Agreement Patterns
The last three lines of Table 3 present a sub-
set of the rules used in Model 3 to incorporate
higher level Twitter information into the model.
Our intuition is that politicians who tweet in a
similar manner would also have similar stances
on issues, which we represent with the predicate
LOCALSAMESTANCEI . SAMETEMPORALACTIV-
ITY represents the idea that if politicians tweet about
an issue around the same times then they also share
a stance for that issue. Finally, FRAME indicates the
frame used by that politician for different issues.
The use of these rules allows Model 3 to overcome
Model 2 inconsistencies between stance and senti-
ment (e.g., if someone attacks their opposition).

5 Experiments

Experimental Settings: As described in Section
4, the data generated from the local models is used
as input to the PSL models. Stances collected in Sec-
tion 3 are used as the ground truth for evaluation of
the results of the PSL models. We initialize Model
1, as described in Section 4.4, using political party
affiliation knowledge. Model 2 builds upon Model
1 by adding the results of the issue and sentiment
analysis local models. Model 3 combines all previ-
ous models with higher level Twitter activities: tweet
agreement, temporal activity, and frames. We imple-
ment our PSL models to have an initial bias that can-
didates do not share a stance and are against an issue.

Experimental Results By Issue: Table 4 presents
the results of using our three proposed PSL models.
Local Baseline (LB) refers to using only the weak
local models for prediction with no additional in-
formation about party affiliation. We observe that
for prediction of stance (PRO) LB performs better

than random chance in 11 of 16 issues; for predic-
tion of agreement (SAMESTANCEI ), LB performs
much lower overall, with only 5 of 16 issues pre-
dicted above chance.

Using Model 1 (M1), we improve stance predic-
tion accuracy for 11 of the issues and agreement ac-
curacy for all issues. Model 2 (M2) further improves
the stance and agreement predictions for an addi-
tional 8 and 10 issues, respectively. Model 3 (M3)
increases the stance prediction accuracy of M2 for 4
issues and the agreement accuracy for 9 issues. The
final agreement predictions of M3 are significantly
improved over the initial LB for all issues.

The final stance predictions of M3 are improved
over all issues except Guns, Iran, and TPP. For Guns,
the stance prediction remains the same through-
out all models, meaning additional party informa-
tion does not boost the initial predictions determined
from Twitter behaviors. For Iran, the addition of M1
and M2 lower the accuracy, but the behavioral fea-
tures from M3 are able to restore it to the origi-
nal prediction. For TPP, this trend is likely due to
the fact that all models incorporate party informa-
tion and the issue of TPP is the most heavily divided
within and across parties, with 8 Republicans and
4 Democrats in support of TPP and 8 Republicans
and 12 Democrats opposed. Even in cases where M1
and/or M2 lowered the initial baseline result (e.g.
stance for Religion or agreement for Environment),
the final prediction by M3 is still higher than that of
the baseline.

Framing and Temporal Information: As shown
in Table 4, performance for some issues does not
improve in Model 3. Upon investigation, we found
that for all issues, except Abortion which improves
in agreement, one or both of the top frames for the
party are the same across party lines. For example,
for ACA both Republicans and Democrats have the
Economic and Health and Safety frames as their top
two frames. For TPP, both parties share the Eco-
nomic frame.

In addition to similar framing overlap, the Twit-
ter timeline for ACA also exhibits overlap, as shown
in Figure 3(a). This figure highlights one week be-
fore and after the Supreme Court ruling (seen as
the peak of activity, 6/25/2015) to uphold the ACA.
Conversely, Abortion, which shares no frames be-
tween parties (Democrats frame Abortion with Con-
stitutionality and Health and Safety frames; Repub-

72



Issue
STANCE AGREEMENT

LB M 1 M 2 M 3 LB M 1 M 2 M 3
ABORTION 81.25 96.88 96.88 96.88 49.31 93.75 93.75 95.36
ACA 96.88 100 100 100 51.61 100 100 100
CONFEDERATE 34.38 78.12 87.5 84.38 51.31 69.6 77.7 80.18
DRUGS 87.5 78.12 96.88 88.88 50.42 63.6 84.07 84.07
ENVIRONMENT 53.12 78.12 78.13 81.08 45.16 68.75 65.59 69.28
GUNS 93.75 93.75 93.75 93.75 48.59 68.54 99.59 99.59
IMMIGRATION 37.5 81.25 81.25 86.36 53.62 68.55 69.06 69.56
IRAN 84.38 65.62 65.63 84.38 35.57 79.73 100 100
ISIS 40.32 76.28 93.75 93.75 59.68 76.28 76.28 90.04
MARRIAGE 62.5 90.62 90.62 90.62 50.57 87.12 87.43 87.43
NSA 37.5 53.12 53.12 61.54 34.15 49.2 56.66 59.65
PAY 84.38 84.38 90.62 89.47 64.30 72.92 80.31 74.31
RELIGION 75 68.75 81.25 81.25 47.62 86.24 76.46 79.44
SOCIAL SECURITY 28.12 78.12 78.13 78.13 53.76 73.25 90.03 90.88
STUDENT 93.75 96.88 96.88 96.88 51.61 100 100 100
TPP 62.5 62.5 62.5 62.5 45.43 48.39 54.64 65.32

Table 4: Stance and Agreement Accuracy by Issue. LB uses weak local models, M1 represents party line agreement,
M2 adds Twitter activity, and M3 adds higher level Twitter behaviors.

06/
18/

15

06/
25/

15

07/
01/

15
0
5

10
15
20
25
30
35
40
45

T
w

e
e
ts

 C
o
u

n
t

(a) ACA
07/

27/
15

08/
03/

15

08/
09/

15
0
2
4
6
8

10
12
14
16

T
w

e
e
ts

 C
o
u

n
t

(b) Abortion
Figure 3: Temporal Twitter Activity by Party. Republi-
can (red) and Democrat (blue) event based temporal over-
laps.

licans use Economic and Capacity and Resources
frames), exhibits a timeline with greater fluctuation.
The peak of Figure 3(b) is 8/3/2015, which is the
day that the budget was passed to include funding
for Planned Parenthood. Overall both parties have
different patterns over this time range, allowing M3
to increase agreement prediction accuracy by 1.61%.

6 Conclusion

In this paper we take a first step towards under-
standing the dynamic microblogging behavior of
politicians. Though we concentrate on a small set
of politicians and issues in this work, this frame-
work can be modified to handle additional politi-
cians or issues, as well as those in other coun-
tries, by incorporating appropriate domain knowl-
edge (e.g., using new keywords for different issues
in other countries), which we leave as future work.
Unlike previous works, which tend to focus on one

aspect of this complex microblogging behavior, we
build a holistic model connecting temporal behav-
iors, party-line bias, and issue frames into a single
predictive model used to identify fine-grained pol-
icy stances and agreement. Despite having no ex-
plicit supervision, and using only intuitive “rules-
of-thumb” to bootstrap our global model, our ap-
proach results in a strong prediction model which
helps shed light on political discourse framing in-
side and across party lines.

Acknowledgements

We thank the anonymous reviewers for their
thoughtful comments and suggestions.

References
Rob Abbott, Marilyn Walker, Pranav Anand, Jean E.

Fox Tree, Robeson Bowmani, and Joseph King. 2011.
How can you say such things?!?: Recognizing dis-
agreement in informal political argument. In Proc. of
the Workshop on Language in Social Media.

Amjad Abu-Jbara, Ben King, Mona Diab, and Dragomir
Radev. 2013. Identifying opinion subgroups in arabic
online discussions. In Proc. of ACL.

Stephen H. Bach, Bert Huang, Ben London, and Lise
Getoor. 2013. Hinge-loss Markov random fields:
Convex inference for structured prediction. In Proc.
of UAI.

Stephen H Bach, Matthias Broecheler, Bert Huang, and
Lise Getoor. 2015. Hinge-loss markov random

73



fields and probabilistic soft logic. arXiv preprint
arXiv:1505.04406.

Akshat Bakliwal, Jennifer Foster, Jennifer van der Puil,
Ron O’Brien, Lamia Tounsi, and Mark Hughes. 2013.
Sentiment analysis of political tweets: Towards an ac-
curate classifier. In Proc. of ACL.

David Bamman and Noah A Smith. 2015. Open extrac-
tion of fine-grained political statements. In Proc. of
EMNLP.

Eric Baumer, Elisha Elovic, Ying Qin, Francesca Polletta,
and Geri Gay. 2015. Testing and comparing computa-
tional approaches for identifying the language of fram-
ing in political news. In Proc. of ACL.

Adam Bermingham and Alan F Smeaton. 2011. On us-
ing twitter to monitor political sentiment and predict
election results.

Amber Boydstun, Dallas Card, Justin H. Gross, Philip
Resnik, and Noah A. Smith. 2014. Tracking the de-
velopment of media frames within and across policy
issues.

Dallas Card, Amber E. Boydstun, Justin H. Gross, Philip
Resnik, and Noah A. Smith. 2015. The media frames
corpus: Annotations of frames across issues. In Proc.
of ACL.

Michael D Conover, Bruno Gonçalves, Jacob Ratkiewicz,
Alessandro Flammini, and Filippo Menczer. 2011.
Predicting the political alignment of twitter users. In
Proc. of Privacy, Security, Risk and Trust (PASSAT)
and SocialCom.

Sarah Djemili, Julien Longhi, Claudia Marinica, Dimitris
Kotzinos, and Georges-Elia Sarfati. 2014. What does
twitter have to say about ideology? In NLP 4 CMC:
Natural Language Processing for Computer-Mediated
Communication.

Jacob Eisenstein. 2013. What to do about bad language
on the internet. In Proc. of NAACL.

Sean Gerrish and David M Blei. 2012. How they
vote: Issue-adjusted models of legislative behavior. In
Advances in Neural Information Processing Systems,
pages 2753–2761.

Stephan Greene and Philip Resnik. 2009. More than
words: Syntactic packaging and implicit sentiment. In
Proc. of NAACL.

Justin Grimmer. 2010. A bayesian hierarchical topic
model for political texts: Measuring expressed agen-
das in senate press releases. In Political Analysis.

Kazi Saidul Hasan and Vincent Ng. 2014. Why are you
taking this stance? identifying and classifying reasons
in ideological debates. In Proc. of EMNLP.

Bert Huang, Stephen H. Bach, Eric Norris, Jay Pujara,
and Lise Getoor. 2012. Social group modeling with
probabilistic soft logic. In NIPS Workshops.

Mohit Iyyer, Peter Enns, Jordan L Boyd-Graber, and
Philip Resnik. 2014. Political ideology detection us-
ing recursive neural networks. In Proc. of ACL.

Yoad Lewenberg, Yoram Bachrach, Lucas Bordeaux, and
Pushmeet Kohli. 2016. Political dimensionality esti-
mation using a probabilistic graphical model. In Proc.
of UAI.

Jiwei Li, Alan Ritter, Claire Cardie, and Eduard H Hovy.
2014a. Major life event extraction from twitter based
on congratulations/condolences speech acts. In Proc.
of EMNLP.

Jiwei Li, Alan Ritter, and Eduard H Hovy. 2014b.
Weakly supervised user profile extraction from twitter.
In Proc. of ACL.

William Maddox and Stuart Lilie. 1984. Beyond liberal
and conservative: Reassessing the political spectrum.

Axel Maireder and Julian Ausserhofer. 2013. National
politics on twitter: Structures and topics of a net-
worked public sphere. In Information, Communica-
tion, and Society.

Micol Marchetti-Bowick and Nathanael Chambers.
2012. Learning for microblogs with distant supervi-
sion: Political forecasting with twitter. In Proc. of
EACL.

Yelena Mejova, Padmini Srinivasan, and Bob Boynton.
2013. Gop primary season on twitter: popular political
sentiment in social media. In WSDM.

Viet-An Nguyen, Jordan Boyd-Graber, Philip Resnik,
and Kristina Miler. 2015. Tea party in the house: A hi-
erarchical ideal point topic model and its application to
republican legislators in the 112th congress. In Proc.
of ACL.

Brendan O’Connor, Ramnath Balasubramanyan, Bryan R
Routledge, and Noah A Smith. 2010. From tweets to
polls: Linking text sentiment to public opinion time
series. In Proc. of ICWSM.

Ferran Pla and Lluı́s F Hurtado. 2014. Political tendency
identification in twitter using sentiment analysis tech-
niques. In Proc. of COLING.

Marta Recasens, Cristian Danescu-Niculescu-Mizil, and
Dan Jurafsky. 2013. Linguistic models for analyzing
and detecting biased language. In Proc. of ACL.

Alan Ritter, Colin Cherry, and Bill Dolan. 2010. Unsu-
pervised modeling of twitter conversations. In Proc.
of NAACL.

SemEval. 2016. Task 6. http://alt.qcri.org/
semeval2016/task6/.

Yanchuan Sim, Brice DL Acree, Justin H Gross, and
Noah A Smith. 2013. Measuring ideological propor-
tions in political speeches. In Proc. of EMNLP.

Swapna Somasundaran and Janyce Wiebe. 2009. Recog-
nizing stances in online debates. In Proc. of ACL.

74



Swapna Somasundaran and Janyce Wiebe. 2010. Recog-
nizing stances in ideological on-line debates. In Proc.
of NAACL Workshops.

Dhanya Sridhar, James Foulds, Bert Huang, Lise Getoor,
and Marilyn Walker. 2015. Joint models of disagree-
ment and stance in online debate. In Proc. of ACL.

Oren Tsur, Dan Calacci, and David Lazer. 2015. A frame
of mind: Using statistical models for detection of fram-
ing and agenda setting campaigns. In Proc. of ACL.

Andranik Tumasjan, Timm Oliver Sprenger, Philipp G
Sandner, and Isabell M Welpe. 2010. Predicting elec-
tions with twitter: What 140 characters reveal about
political sentiment.

Svitlana Volkova, Glen Coppersmith, and Benjamin
Van Durme. 2014. Inferring user political preferences
from streaming communications. In Proc. of ACL.

Svitlana Volkova, Yoram Bachrach, Michael Armstrong,
and Vijay Sharma. 2015. Inferring latent user proper-
ties from texts published in social media. In Proc. of
AAAI.

Marilyn A. Walker, Pranav Anand, Robert Abbott, and
Ricky Grant. 2012. Stance classification using dia-
logic properties of persuasion. In Proc. of NAACL.

Robert West, Hristo S Paskov, Jure Leskovec, and
Christopher Potts. 2014. Exploiting social net-
work structure for person-to-person sentiment analy-
sis. TACL.

Janyce Wiebe, Theresa Wilson, Rebecca Bruce, Matthew
Bell, and Melanie Martin. 2004. Learning subjective
language. Computational linguistics.

Theresa Wilson, Paul Hoffmann, Swapna Somasun-
daran, Jason Kessler, Janyce Wiebe, Yejin Choi, Claire
Cardie, Ellen Riloff, and Siddharth Patwardhan. 2005.
Opinionfinder: A system for subjectivity analysis. In
Proc. of EMNLP.

75


