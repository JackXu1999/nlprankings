



















































Implicit Subjective and Sentimental Usages in Multi-sense Word Embeddings


Proceedings of the 9th Workshop on Computational Approaches to Subjectivity, Sentiment and Social Media Analysis, pages 8–13
Brussels, Belgium, October 31, 2018. c©2018 Association for Computational Linguistics

https://doi.org/10.18653/v1/P17

8

Implicit Subjective and Sentimental Usages
in Multi-sense Word Embeddings

Yuqi Sun 1 Haoyue Shi 1,∗ Junfeng Hu1,2,†
1: School of EECS, Peking University, Beijing, China

2: MOE Key Lab of Computational Linguistics, School of EECS, Peking University
{sun yq, hyshi, hujf}@pku.edu.cn

Abstract

In multi-sense word embeddings, contextual
variations in corpus may cause a univocal
word to be embedded into different sense vec-
tors. Shi et al. (2016) show that this kind of
pseudo multi-senses can be eliminated by lin-
ear transformations. In this paper, we show
that pseudo multi-senses may come from a
uniform and meaningful phenomenon such as
subjective and sentimental usage, though they
are seemingly redundant.

In this paper, we present an unsupervised algo-
rithm to find a linear transformation which can
minimize the transformed distance of a group
of sense pairs. The major shrinking direction
of this transformation is found to be related
with subjective shift. Therefore, we can not
only eliminate pseudo multi-senses in multi-
sense embeddings, but also identify these sub-
jective senses and tag the subjective and senti-
mental usage of words in the corpus automati-
cally.

1 Introduction

Multi-sense word embeddings are popular choices
to represent polysemous words (Reisinger and
Mooney, 2010; Huang et al., 2012; Neelakantan
et al., 2014; Cheng and Kartsaklis, 2015; Lee and
Chen, 2017). These methods learn senses of words
automatically by clustering contexts they appear
in. However, contextual variation in corpus may
cause a univocal word be embedded into different
senses (Shi et al., 2016). For example, the context
of “another” in this sentence is normal and narra-
tive:

“South Trust, another large bank head-
quartered in Birmingham, was acquired
by Wachovia in 2004.”
∗Now at Toyota Technological Institute at Chicago,

freda@ttic.edu.
† Corresponding author.

sentimental context

normal context

std-sense
senti-sense

sentimental shift vector

Figure 1: The relation between sentimental sense
(senti-sense) and normal sense (std-sense) of a word.
The vector differences are considered as sentimental
shift vectors.

In the second sentence, the word “another” locates
in subjective and emotional context with intense
feelings:

“He committed suicide after the woman
he loved married another man.”

The word “another” in these two sentences have
the same meaning, but they are often embedded
into two different senses by existing multi-sense
word embedding models.

Shi et al. (2016) used linear transformation to
eliminate the vector differences between corre-
sponding sense pairs with the same meaning, and
improved the performance on downstream tasks
such as contextual word similarity (Huang et al.,
2012). Such pairs were called pseudo multi-sense
pairs. However, they did not give any explicit ex-
planation of the eliminated vector difference in a
pseudo multi-sense pair.

In this paper, we propose to explain the so-
called pseudo multi-senses by slightly modifying



9

the linear transformation proposed by Shi et al.
(2016). We find that a large number of pseudo
multi-senses can be viewed as pairs of i) a normal
sense and ii) a subjective or sentimental sense. In
addition, as shown in Figure 1, a group of words
may have similar normal-subjective/sentimental
difference vector, indicating subjectivity and sen-
timent are general sources of pseudo multi-senses.

In the first step of our approach, we identify the
multi-sense pairs that are generated by an uniform
contextual variation. Then we regress a linear
transformation which can minimize the average
Euclidean distance between two opposite groups
in embedding space. We analyze the major shrink-
ing directions in the embedding space w.r.t. the
linear transformation, and find it consistent that
one of such directions is relevant to subjective and
sentimental usage.

The motivation of our approach is that a group
of pseudo multi-senses is often generated sys-
tematically, i.e., pseudo multi-senses in the same
group come from the same reason. Therefore, a
linear transformation that eliminate the shift and
minimize distance between senses may reflect a
salient language phenomenon. In addition to giv-
ing explicit explanation to pseudo multi-senses,
experimental results also show that our approach
can contribute to some NLP tasks such as subjec-
tive and sentimental analysis.

In Section 2, we introduce some related work.
In Section 3, we present a method to mine a linear
transformation that eliminates semantic shift gen-
erating ‘pseudo multi-senses’. In Section 4, we an-
alyze the language phenomenon represented and
eliminated by that linear transformation, namely
subjective and sentimental usage, and do some
evaluations on the subjective shift. Finally in Sec-
tion 6 we draw conclusions and propose future
work left to be done.

2 Related Work

Subjectivity and sentiment analysis have been
investigated by many researches with different
methods(Turney, 2002; Wiebe, 2000; Pang and
Lee, 2004, 2008; Liu, 2010; Cambria et al., 2013;
Maas et al., 2011; Lin and He, 2009; Abdul-
Mageed et al., 2014; Dasgupta and Ng, 2009; Pak
and Paroubek, 2010). Many works contribute to
language resources for subjectivity and sentimen-
tal analysis. (Baccianella et al., 2010) Recent
works also provide many training methods to per-

fect the language models’ performance on bench-
mark datasets (Kouloumpis et al., 2011; Maas
et al., 2011; Taboada et al., 2011; Wang and Man-
ning, 2012; Labutov and Lipson, 2013; Lan et al.,
2016; Ren et al., 2016; Tang et al., 2016). Our
work provides a unsupervised method of subjec-
tivity analysis based on multi-sense word embed-
ding.

Multi-sense word embedding is also a popular
way to represent polysemous words(Reisinger and
Mooney, 2010; Huang et al., 2012; Neelakantan
et al., 2014; Guo et al., 2014; Li and Jurafsky,
2015; Iacobacci et al., 2015; Cheng and Kartsak-
lis, 2015; Lee and Chen, 2017). However, these
method using contextual difference for sense clus-
tering to decide senses are so sensitive to contex-
tual variation and usage of word, therefore may
embed a single sense into several vectors. We aim
to mine such contextual variations. While super-
vised methods rely on external knowledge with
manually definition of senses(Chen et al., 2014;
Cao et al., 2017).

Singular value decomposition is used on la-
tent semantic indexing by factorizing a term-
document matrix and constructing a ”semantic
space”(Deerwester et al., 1990). We use a sim-
ilar approach to extract language phenomena we
mine.

3 Methodology

The general framework of our method includes the
following four steps:

1. We start with random selected “pseudo multi-
senses” as initial seeds, training a linear
transformation to minimize transformed dis-
tance of these sense pairs.

Let M denote the transformation matrix and
(x,y) denote a vector pair of pseudo multi-
sense. Define the set of pairs sharing a uni-
form semantic shift as P. Ideally, we expect
M to transform x closer to y and keep y un-
moved for all (x,y) ∈ P. Therefore we de-
rive the following loss function:

L(M) =
∑

(x,y)∈P

‖Mx−y‖22+‖My−y‖22

2. Update pseudo multi-senses iteratively w.r.t.
the loss function.



10

According to the hypothesis that a systemati-
cal contextual variation may generate a group
of pseudo multi-senses, the linear transfor-
mation that eliminate this variation can be
used to pick out most typical pseudo multi-
senses. We define shrinking rate, which re-
veals the degree of a pair of senses being
combined by transformation M :

ρM (x,y) =
‖Mx−My‖2
‖x− y‖2

(1)

The smaller ρM (x,y) is, the more likely
(x,y) are generated by this contextual vari-
ation. Thus we optimize the set of pseudo
multi-senses, and use it to retrain transforma-
tion matrix. This algorithm can eventually
converge to an optimal solution.

3. Extract eigen-directions of linear transforma-
tion M with a singular value decomposition
algorithm.

The major shrinking directions are semantic
directions shrunk and eliminated by the lin-
ear transformation. If there exists an obvi-
ous explanation of such directions, they can
be viewed as the representative directions for
specific language phenomena (e.g., subjec-
tive or sentimental usage) in the embedding
space.

4. Observe KNN of major shrinking directional
vectors so as to reveal the language phenom-
ena corresponding to contextual variations.

The pseudo code of this procedure is shown in
Algorithm 1.

4 Intuitive Results

Our experiments are based on the multi-sense
skip-gram(MSSG) model (Neelakantan et al.,
2014) and Wikipedia Corpus, training a 50-
dimensional multi-sense embedding space.

With this method, we train a linear transforma-
tion with random seeds. Results show that trans-
formation will converge to a stable point, which
verifies the existence of systematical contextual
variation in corpus.

Furthermore, to understand the language phe-
nomena that generate ‘pseudo’ mutli-senses, we
observe nearest neighbours of eigen-directional
vectors. We find each eigen-direction is meaning-
ful. The eigenvalues of these eigen-directions are

Algorithm 1 Train a transformation matrix.
Require: The multi-sense embedding space V ;

Set of random selected sense pairs, S0; Loss
function, L(M); Shrinking rate function
ρM (x,y); Size of word set k.

Ensure: Transformation matrix, M ;
Set of candidate sense pairs, S

1: Initialize S with S0
2: while not converged do
3: Train transformation matrix M by mini-

mizing L(M) using gradient descent algo-
rithm

4: Choose the k-most shrunk pairs as S
5: end while
6: returnM,S

subj-vec’s KNN feelings, song, strange, love, every-
thing, emotional, never,something,
girlfriend, always, eyes, smell, dia-
logue, smile, really, movie, sounds,
things, sexual, mind, script

Reversed subj-
vec’s KNN

regional, administrative, township,
located, racial, lies, avenue, vir-
ginia, approximately, historic, reg-
ister, pennsylvania, municipality,
served, delaware, situated, politi-
cian, operates, terminus, unincor-
porated

Table 1: Top 20 KNN for subj-vec are shown in ‘subj-
vec’s KNN’. Top 20 KNN for reversed subj-vec are
shown in ‘Reversed subj-vec’s KNN’.

expansion multipliers of these dimensions. There-
fore the eigen-direction with smallest eigenvalue
represents the most salient language phenomenon.
Interestingly, nearest neighbours are words about
sentiments and emotions. Under our observation,
this major shrinking directional vector is likely to
be the vector representing subjective usage. We
denote this vector as subj-vec. The KNN of subj-
vec is shown in Table 1.

Therefore we found that the subjective usage of
words is a salient language phenomenon in multi-
sense embedding space. Interestingly, we also ob-
serve that the reversed direction of subj-vec is re-
lated to some regional and political topics, which
is matched with human intuition.

5 Evaluations

5.1 Sentence Classification
We take subjectivity and sentiment analysis tasks
to evaluate the function of subj-vec.

We take two text classification tasks: SUBJ



11

(Pang and Lee, 2004), a subjectivity status de-
tection task and MPQA (Wiebe et al., 2005),
an opinion polarity classification task. We use
the LR(logistic regression) classifier and sentence
level features to do the classification tasks. We use
word/sense embedding as encoder, and decide the
sense of every instance by Equation2 .

Sense(C(w)) = argmax
sense

cos(Vcontext, Vsense)

(2)
We express the sentence-level features with

contextual vector, denoted as context-vec, which
is the sum of sense embeddings in a sentence. We
provide four groups of evaluation results with dif-
ferent encoders:
1. context-vec with original embeddings.
2. context-vec with the embedding space whose
subjective direction is stretched. We stretch sub-
jective direction by Equation 3, in which embed-
ding of a sense s is denoted as v(s), and embed-
ding in the stretched space is denoted as v′(s).

v′(s) = v(s) + v(s) · subj-vec ∗ subj-vec (3)

In Equation 3, subj-vec is the directional vector of
subjective usage. Each embedding is added by a
bias in subjective direction.
3. context-vec with the embedding space whose
subjective direction is eliminated by Equation 4.

v′(s) = v(s)− v(s) · subj-vec ∗ subj-vec (4)

4. context-vec + context-vec · subj-vec ∗ subj-
vec. We extract information about subjective us-
age by combining context-vec and the dot product
of context-vec and subj-vec of every sentence.

The results are shown in Table 2.
Obviously subjective direction of embedding

space improves its performance on subjective and
sentimental analysis. Such improvement doesn’t
appear on single-sense embeddings. Meanwhile
eliminating subjective direction worsen the perfor-
mances on every listed tasks.

Li and Jurafsky (2015) argued that multi-senses
word embedding does not outperform single-sense
word embedding in several language tasks. In fact,
we found by add features on the sentimental di-
mension, multi-sense embeddings can achieve bet-
ter performance on subjectivity and sentimental
analysis tasks.

Sense Space SUBJ-multi SUBJ-single
Original Space 88.36 88.73
Subj-Stretched 88.39 88.72
Subj-Eliminated 87.22 88.35
Context + Subj 88.4 88.73
Sense Space MPQA-multi MPQA-single
Original Space 82.05 82.73
Subj-Stretched 82.07 82.23
Subj-Eliminated 81.79 82.13
Context + Subj 83.05 82.29

Table 2: The performance of the original MSSG50D
on Wiki Corpus embeddings(Original Space), the
sentimental-stretched space(‘Subj-stretched’), the
sentimental-eliminated space(‘Subj-eliminated’) and
the original contextual vector in combination with its
projection on sentimental direction(‘Context + Subj’)
on text classification tests including MPQA (Wiebe
et al., 2005), an opinion polarity classification task,
and SUBJ (Pang and Lee, 2004), a subjectivity status
detection task. We use equation 2 to decide the sense
of every instance.

words KNN
science astronomy, psychology, librarianship,

research, anthropology, sciences,
parapsychology, literature

+subj-vec science, literature, journalism, para-
psychology, photography, filmmak-
ing, literary, graphic, writing

fight fights, duel, fighting, confrontation,
battling, surprise, revenge, vengeance,
foe

+ subj-vec revenge, fights, confrontation,
vengeance, furious, grodd, surprise,
vicious, infuriated

rich cultivated, lush, cultivating, shady,
landscapes, growing, alfalfa, abun-
dance, herb, landscape

+ subj-vec delight, shady, good, riches, im-
mensely, arth, charm, curious, little-
known, wonderful

country nation, scandinavia, europe, america,
countries, turkey, uk, sweden, interna-
tionally, ireland

+subj-vec nation, clubbers, welcome, coming,
enjoying, mam, wigwam, pride, eu-
rope, americ

Table 3: KNN for words before and after adding a sub-
jective bias.

5.2 Analogies

Moreover, since subj-vec represents subjective us-
age, we add it to some embeddings in multi-sense
embedding space to observe the effect of subj-vec
on semantic shift. Table 3 illustrates the KNN for
the original words and the words with a subjective
bias. The table shows that by adding subjective
subj-vec, the subjective and sentimental properties
for words are changed. In general, more emotional



12

and subjective words appear in the KNNs of the
new location. This is another interesting property
of subj-vec.

6 Conclusions And Future Work

In this article we propose a methodology to rep-
resent language phenomena such as subjective us-
age by a uniform bias vector of sense pairs, and
provide an unsupervised approach to mine it. We
also use evaluations to explore its functions and
find subj-vec can relatively improve multi-sense
embeddings performance on subjective and sen-
timental analysis tasks. Furthermore, there are
many linguistic phenomena left to be mined.

Acknowledgements

We thank the anonymous reviewers for their valu-
able feedback. This work is supported by National
Natural Science Foundation of China (61472017).

References
Muhammad Abdul-Mageed, Mona Diab, and San-

dra Kübler. 2014. Samar: Subjectivity and Senti-
ment Analysis for Arabic Social Media. Computer
Speech & Language.

Stefano Baccianella, Andrea Esuli, and Fabrizio Sebas-
tiani. 2010. Sentiwordnet 3.0: An Enhanced Lexical
Resource for Sentiment Analysis and Opinion Min-
ing. In Proc. of LREC.

Erik Cambria, Bjorn Schuller, Yunqing Xia, and
Catherine Havasi. 2013. New Avenues in Opinion
Mining and Sentiment Analysis. IEEE Intelligent
Systems.

Yixin Cao, Lifu Huang, Heng Ji, Xu Chen, and Juanzi
Li. 2017. Bridge Text and Knowledge by Learn-
ing Multi-prototype Entity Mention Embedding. In
Proc. of ACL.

Xinxiong Chen, Zhiyuan Liu, and Maosong Sun. 2014.
A Unified Model for Word Sense Representation
and Disambiguation. In Proc. of EMNLP.

Jianpeng Cheng and Dimitri Kartsaklis. 2015. Syntax-
aware multi-sense word embeddings for deep com-
positional models of meaning. arXiv preprint
arXiv:1508.02354.

Sajib Dasgupta and Vincent Ng. 2009. Mine the Easy,
Classify the Hard: a Semi-supervised Approach to
Automatic Sentiment Classification. In Proc. of
ACL-IJCNLP.

Scott Deerwester, Susan T. Dumais, George W Furnas,
Thomas K Landauer, and Richard Harshman. 1990.
Indexing by latent semantic analysis. Journal of the
American society for information science.

Jiang Guo, Wanxiang Che, Haifeng Wang, and Ting
Liu. 2014. Learning Sense-Specific Word Embed-
dings by Exploiting Bilingual Resources. In Proc.
of COLING.

Eric H. Huang, Richard Socher, Christopher D. Man-
ning, and Andrew Y. Ng. 2012. Improving Word
Representations via Global Context and Multiple
word Prototypes. In Proc. of ACL.

Ignacio Iacobacci, Mohammad Taher Pilehvar, and
Roberto Navigli. 2015. Sensembed: Learning Sense
Embeddings for Word and Relational Similarity. In
Proc. of ACL-IJCNLP.

Efthymios Kouloumpis, Theresa Wilson, and Jo-
hanna D Moore. 2011. Twitter Sentiment Analysis:
The Good the Bad and the OMG! In Proc. of the
AAAI Conference on Weblogs and Social Media.

Igor Labutov and Hod Lipson. 2013. Re-embedding
Words. In Proc. of ACL.

Man Lan, Zhihua Zhang, Yue Lu, and Ju Wu. 2016.
Three convolutional neural network-based models
for learning sentiment word vectors towards senti-
ment analysis. In Proc. of IJCNN.

Guang-He Lee and Yun-Nung Chen. 2017. MUSE:
Modularizing Unsupervised Sense Embeddings. In
Proc. of EMNLP.

Jiwei Li and Dan Jurafsky. 2015. Do Multi-sense Em-
beddings Improve Natural Language Understand-
ing? In Proc. of EMNLP.

Chenghua Lin and Yulan He. 2009. Joint Senti-
ment/Topic Model for Sentiment Analysis. In Proc.
of CIKM.

Bing Liu. 2010. Sentiment Analysis and Subjectivity.
Handbook of Natural Language Processing.

Andrew L. Maas, Raymond E. Daly, Peter T. Pham,
Dan Huang, Andrew Y. Ng, and Christopher Potts.
2011. Learning word vectors for sentiment analysis.
In Proc. of ACL.

Arvind Neelakantan, Jeevan Shankar, Alexandre Pas-
sos, and Andrew McCallum. 2014. Efficient Non-
parametric Estimation of Multiple Embeddings per
Word in Vector Space. In Proc. of EMNLP.

Alexander Pak and Patrick Paroubek. 2010. Twitter as
a Corpus for Sentiment Analysis and Opinion Min-
ing. In Proc. of LREC.

Bo Pang and Lillian Lee. 2004. A Sentimental Educa-
tion: Sentiment Analysis using Subjectivity Summa-
rization based on Minimum Cuts. In Proc. of ACL.

Bo Pang and Lillian Lee. 2008. Opinion Mining and
Sentiment Analysis. Now Publishers, Inc.

Joseph Reisinger and Raymond J Mooney. 2010.
Multi-prototype Vector-Space Models of Word
Meaning. In Proc. of NAACL-HLT.



13

Yafeng Ren, Yue Zhang, Meishan Zhang, and
Donghong Ji. 2016. Improving Twitter Senti-
ment Classification using Topic-Enriched Multi-
Prototype Word Embeddings. In AAAI, pages 3038–
3044.

Haoyue Shi, Caihua Li, and Junfeng Hu. 2016. Real
multi-sense or pseudo multi-sense: An approach to
improve word representation. Proc. of the Workshop
on Computational Linguistics for Linguistic Com-
plexity.

Maite Taboada, Julian Brooke, Milan Tofiloski, Kim-
berly Voll, and Manfred Stede. 2011. Lexicon-based
Methods for Sentiment Analysis. Computational
Linguistics.

Duyu Tang, Furu Wei, Bing Qin, Nan Yang, Ting Liu,
and Ming Zhou. 2016. Sentiment embeddings with
applications to sentiment analysis. IEEE Trans. on
Knowledge and Data Engineering.

Peter D. Turney. 2002. Thumbs Up or Thumbs Down?:
Semantic Orientation applied to unsupervised classi-
fication of reviews. In Proc. of ACL.

Sida Wang and Christopher D Manning. 2012. Base-
lines and Bigrams: Simple, Good Sentiment and
Topic Classification. In Proc. of ACL.

Janyce Wiebe. 2000. Learning Subjective Adjectives
from Corpora. In Proc. of AAAI.

Janyce Wiebe, Theresa Wilson, and Claire Cardie.
2005. Annotating Expressions of Opinions and
Emotions in Language. Language Resource and
Evaluation.


