




































Results of the WMT19 Metrics Shared Task: Segment-Level and Strong MT Systems Pose Big Challenges


Proceedings of the Fourth Conference on Machine Translation (WMT), Volume 2: Shared Task Papers (Day 1) pages 62–90
Florence, Italy, August 1-2, 2019. c©2019 Association for Computational Linguistics

62

Results of the WMT19 Metrics Shared Task:
Segment-Level and Strong MT Systems Pose Big Challenges

Qingsong Ma
Tencent-CSIG, AI Evaluation Lab

qingsong.mqs@gmail.com

Johnny Tian-Zheng Wei
UMass Amherst, CICS

jwei@umass.edu

Ondřej Bojar
Charles University, MFF ÚFAL

bojar@ufal.mff.cuni.cz

Yvette Graham
Dublin City University, ADAPT

graham.yvette@gmail.com

Abstract

This paper presents the results of the
WMT19 Metrics Shared Task. Par-
ticipants were asked to score the out-
puts of the translations systems compet-
ing in the WMT19 News Translation
Task with automatic metrics. 13 research
groups submitted 24 metrics, 10 of which
are reference-less “metrics” and constitute
submissions to the joint task with WMT19
Quality Estimation Task, “QE as a Met-
ric”. In addition, we computed 11 baseline
metrics, with 8 commonly applied base-
lines (BLEU, SentBLEU, NIST, WER,
PER, TER, CDER, and chrF) and 3 reim-
plementations (chrF+, sacreBLEU-BLEU,
and sacreBLEU-chrF). Metrics were evalu-
ated on the system level, how well a given
metric correlates with the WMT19 offi-
cial manual ranking, and segment level,
how well the metric correlates with human
judgements of segment quality. This year,
we use direct assessment (DA) as our only
form of manual evaluation.

1 Introduction

To determine system performance in machine
translation (MT), it is often more practical
to use an automatic evaluation, rather than
a manual one. Manual/human evaluation can
be costly and time consuming, and so an au-
tomatic evaluation metric, given that it suffi-
ciently correlates with manual evaluation, can
be useful in developmental cycles. In studies
involving hyperparameter tuning or architec-
ture search, automatic metrics are necessary
as the amount of human effort implicated in
manual evaluation is generally prohibitively
large. As objective, reproducible quantities,
metrics can also facilitate cross-paper compar-

isons. The WMT Metrics Shared Task1 annu-
ally serves as a venue to validate the use of
existing metrics (including baselines such as
BLEU), and to develop new ones; see Koehn
and Monz (2006) through Ma et al. (2018).

In the setup of our Metrics Shared Task,
an automatic metric compares an MT sys-
tem’s output translations with manual ref-
erence translations to produce: either (a)
system-level score, i.e. a single overall score
for the given MT system, or (b) segment-level
scores for each of the output translations, or
both.

This year we teamed up with the organizers
of the QE Task and hosted “QE as a Metric” as
a joint task. In the setup of the Quality Esti-
mation Task (Fonseca et al., 2019), no human-
produced translations are provided to estimate
the quality of output translations. Quality es-
timation (QE) methods are built to assess MT
output based on the source or based on the
translation itself. In this task, QE developers
were invited to perform the same scoring as
standard metrics participants, with the excep-
tion that they refrain from using a reference
translation in production of their scores. We
then evaluate the QE submissions in exactly
the same way as regular metrics are evalu-
ated, see below. From the point of view of
correlation with manual judgements, there is
no difference in metrics using or not using ref-
erences.

The source, reference texts, and MT sys-
tem outputs for the Metrics task come from
the News Translation Task (Barrault et al.,
2019, which we denote as Findings 2019). The
texts were drawn from the news domain and
involve translations of English (en) to/from

1http://www.statmt.org/wmt19/metrics-task.
html

qingsong.mqs@gmail.com
jwei@umass.edu
bojar@ufal.mff.cuni.cz
graham.yvette@gmail.com
http://www.statmt.org/wmt19/metrics-task.html
http://www.statmt.org/wmt19/metrics-task.html


63

Czech (cs), German (de), Finnish (fi), Gu-
jarati (gu), Kazakh (kk), Lithuanian (lt), Rus-
sian (ru), and Chinese (zh), but excluding cs-
en (15 language pairs). Three other language
pairs not including English were also manu-
ally evaluated as part of the News Translation
Task: German→Czech and German↔French.
In total, metrics could participate in 18 lan-
guage pairs, with 10 target languages.

In the following, we first give an overview of
the task (Section 2) and summarize the base-
line (Section 3) and submitted (Section 4) met-
rics. The results for system- and segment-level
evaluation are provided in Sections 5.1 and
5.2, respectively, followed by a joint discussion
Section 6.

2 Task Setup

This year, we provided task participants with
one test set for each examined language pair,
i.e. a set of source texts (which are commonly
ignored by MT metrics), corresponding MT
outputs (these are the key inputs to be scored)
and a reference translation (held out for the
participants of “QE as a Metric” track).

In the system-level, metrics aim to corre-
late with a system’s score which is an aver-
age over many human judgments of segment
translation quality produced by the given sys-
tem. In the segment-level, metrics aim to
produce scores that correlate best with a hu-
man ranking judgment of two output trans-
lations for a given source segment (more on
the manual quality assessment in Section 2.3).
Participants were free to choose which lan-
guage pairs and tracks (system/segment and
reference-based/reference-free) they wanted to
take part in.

2.1 Source and Reference Texts
The source and reference texts we use are
newstest2019 from this year’s WMT News
Translation Task (see Findings 2019). This
set contains approximately 2,000 sentences for
each translation direction (except Gujarati,
Kazakh and Lithuanian which have approx-
imately 1,000 sentences each, and German
to/from French which has 1701 sentences).

The reference translations provided in new-
stest2019 were created in the same direc-
tion as the MT systems were translating.

The exceptions are German→Czech where
both sides are translations from English and
German↔French which followed last years’
practice. Last year and the years before, the
dataset consisted of two halves, one originat-
ing in the source language and one in the tar-
get language. This however lead to adverse
artifacts in MT evaluation.

2.2 System Outputs
The results of the Metrics Task are affected
by the actual set of MT systems participating
in a given translation direction. On one hand,
if all systems are very close in their transla-
tion quality, then even humans will struggle
to rank them. This in turn will make the task
for MT metrics very hard. On the other hand,
if the task includes a wide range of systems
of varying quality, correlating with humans
should be generally easier, see Section 6.1 for
a discussion on this. One can also expect that
if the evaluated systems are of different types,
they will exhibit different error patterns and
various MT metrics can be differently sensi-
tive to these patterns.

This year, all MT systems included in the
Metrics Task come from the News Translation
Task (see Findings 2019). There are however
still noticeable differences among the various
language pairs.

• Unsupervised MT Systems. The
German→Czech research systems were
trained in an unsupervised fashion, i.e.
without the access to parallel Czech-
German texts (except for a couple of
thousand sentences used primarily for val-
idation). We thus expect the research
German-Czech systems to be “more cre-
ative” and depart further away from the
references. The online systems in this
language directions are however standard
MT systems so the German-Czech evalu-
ation could be to some extent bimodal.

• EU Election. The French↔German
translation was focused on a sub-domain
of news, namely texts related EU Elec-
tion. Various MT system developers may
have invested more or less time to the do-
main adaptation.

• Regular News Tasks Systems. These



64

are all the other MT systems in the
evaluation; differing in whether they are
trained only on WMT provided data
(“Constrained”, or “Unconstrained”) as
in the previous years. All the freely avail-
able web services (online MT systems) are
deemed unconstrained.

Overall, the results are based on 233 systems
across 18 language pairs.2

2.3 Manual Quality Assessment
Direct Assessment (DA, Graham et al., 2013,
2014a, 2016) was employed as the source of the
“golden truth” to evaluate metrics again this
year. The details of this method of human
evaluation are provided in Findings 2019.

The basis of DA is to collect a large number
of quality assessments (a number on a scale of
1–100, i.e. effectively a continuous scale) for
the outputs of all MT systems. These scores
are then standardized per annotator.

In the past years, the underlying man-
ual scores were reference-based (human judges
had access to the same reference translation
as the MT quality metric). This year, the of-
ficial WMT19 scores are reference-based (or
“monolingual”) for some language pairs and
reference-free (or “bilingual”) for others.3

Due to these different types of golden truth
collection, reference-based language pairs are
in a closer match with the standard reference-
based metrics, while the reference-free lan-
guage pairs are better fit for the “QE as a
metric” subtask.

Note that system-level manual scores are
different than those of the segment-level. Since
for segment-level evaluation, collecting enough
DA judgements for each segment is infeasible,
so we resort to converting DA judgements to

2This year, we do not use the artificially constructed
“hybrid systems” (Graham and Liu, 2016) because the
confidence on the ranking of system-level metrics is suf-
ficient even without hybrids.

3Specifically, the reference-based language pairs
were those where the anticipated translation quality
was lower or where the manual judgements were ob-
tained with the help of anonymous crowdsourcing.
Most of these cases were translations into English (fi-
en, gu-en, kk-en, lt-en, ru-en and zh-en) and then the
language pairs not involving English (de-cs, de-fr and
fr-de). The reference-less (bilingual) evaluations were
those where mainly MT researchers themselves were
involved in the annotations: en-cs, en-de, en-fi, en-gu,
en-kk, en-lt, en-ru, en-zh.

golden truth expressed as relative rankings, see
Section 2.3.2.

The exact methods used to calculate corre-
lations of participating metrics with the golden
truth are described below, in the two sections
for system-level evaluation (Section 5.1) and
segment-level evaluation (Section 5.2).

2.3.1 System-level Golden Truth: DA
For the system-level evaluation, the collected
continuous DA scores, standardized for each
annotator, are averaged across all assessed seg-
ments for each MT system to produce a scalar
rating for the system’s performance.

The underlying set of assessed segments is
different for each system. Thanks to the fact
that the system-level DA score is an average
over many judgments, mean scores are consis-
tent and have been found to be reproducible
(Graham et al., 2013). For more details see
Findings 2019.

2.3.2 Segment-level Golden Truth:
daRR

Starting from Bojar et al. (2017), when WMT
fully switched to DA, we had to come up with a
solid golden standard for segment-level judge-
ments. Standard DA scores are reliable only
when averaged over sufficient number of judg-
ments.4

Fortunately, when we have at least two DA
scores for translations of the same source in-
put, it is possible to convert those DA scores
into a relative ranking judgement, if the dif-
ference in DA scores allows conclusion that
one translation is better than the other. In
the following, we denote these re-interpreted
DA judgements as “daRR”, to distinguish
it clearly from the relative ranking (“RR”)
golden truth used in the past years.5

4For segment-level evaluation, one would need to
collect many manual evaluations of the exact same seg-
ment as produced by each MT system. Such a sampling
would be however wasteful for the evaluation needed by
WMT, so only some MT systems happen to be evalu-
ated for a given input sentence. In principle, we would
like to return to DA’s standard segment-level evalua-
tion in future, where a minimum of 15 human judge-
ments of translation quality are collected per transla-
tion and combined to get highly accurate scores for
translations, but this would increase annotation costs.

5Since the analogue rating scale employed by DA is
marked at the 0-25-50-75-100 points, we use 25 points
as the minimum required difference between two sys-
tem scores to produce daRR judgements. Note that we



65

DA>1 Ave DA pairs daRR
de-en 2,000 16.0 239,220 85,365
fi-en 1,996 9.5 83,168 38,307

gu-en 1,016 11.0 55,880 31,139
kk-en 1,000 11.0 55,000 27,094
lt-en 1,000 11.0 55,000 21,862

ru-en 1,999 11.9 131,766 46,172
zh-en 2,000 10.1 95,174 31,070
en-cs 1,997 9.1 75,560 27,178
en-de 1,997 19.1 347,109 99,840
en-fi 1,997 8.1 59,129 31,820
en-gu 998 6.9 21,854 11,355
en-kk 998 9.0 37,032 18,172
en-lt 998 9.0 36,435 17,401
en-ru 1,997 8.7 69,503 24,334
en-zh 1,997 9.8 87,501 18,658
de-cs 1,997 8.5 65,039 35,793
de-fr 1,605 4.1 12,055 4,862
fr-de 1,224 3.0 4,258 1,369

newstest2019

Table 1: Number of judgements for DA converted
to daRR data; “DA>1” is the number of source
input sentences in the manual evaluation where at
least two translations of that same source input
segment received a DA judgement; “Ave” is the
average number of translations with at least one
DA judgement available for the same source input
sentence; “DA pairs” is the number of all possi-
ble pairs of translations of the same source input
resulting from “DA>1”; and “daRR” is the num-
ber of DA pairs with an absolute difference in DA
scores greater than the 25 percentage point mar-
gin.

From the complete set of human assess-
ments collected for the News Translation Task,
all possible pairs of DA judgements attributed
to distinct translations of the same source were
converted into daRR better/worse judge-
ments. Distinct translations of the same
source input whose DA scores fell within 25
percentage points (which could have been
deemed equal quality) were omitted from the
evaluation of segment-level metrics. Conver-
sion of scores in this way produced a large set
of daRR judgements for all language pairs,
rely on judgements collected from known-reliable vol-
unteers and crowd-sourced workers who passed DA’s
quality control mechanism. Any inconsistency that
could arise from reliance on DA judgements collected
from low quality crowd-sourcing is thus prevented.

shown in Table 1 due to combinatorial ad-
vantage of extracting daRR judgements from
all possible pairs of translations of the same
source input. We see that only German-French
and esp. French-German can suffer from in-
sufficient number of these simulated pairwise
comparisons.

The daRR judgements serve as the golden
standard for segment-level evaluation in
WMT19.

3 Baseline Metrics
In addition to validating popular metrics, in-
cluding baselines metrics serves as comparison
and prevents “loss of knowledge” as mentioned
by Bojar et al. (2016).

Moses scorer6 is one of the MT evaluation
tools that aggregated several useful metrics
over the time. Since Macháček and Bojar
(2013), we have been using Moses scorer to
provide most of the baseline metrics and kept
encouraging authors of well-performing MT
metrics to include them in Moses scorer.7

The baselines we report are:

BLEU and NIST The metrics BLEU
(Papineni et al., 2002) and NIST
(Doddington, 2002) were computed
using mteval-v13a.pl8 from the
OpenMT Evaluation Campaign.
The tool includes its own tokeniza-
tion. We run mteval with the flag
--international-tokenization.9

TER, WER, PER and CDER. The met-
rics TER (Snover et al., 2006), WER,
PER and CDER (Leusch et al., 2006)
were produced by the Moses scorer, which
is used in Moses model optimization.
We used the standard tokenizer script as
available in Moses toolkit for tokeniza-
tion.

sentBLEU. The metric sentBLEU is com-
puted using the script sentence-bleu,
a part of the Moses toolkit. It is a

6https://github.com/moses-smt/mosesdecoder/
blob/master/mert/evaluator.cpp

7If you prefer standard BLEU, we recommend sacre-
BLEU (Post, 2018a), found at https://github.com/
mjpost/sacreBLEU.

8http://www.itl.nist.gov/iad/mig/tools/
9International tokenization is found to perform

slightly better (Macháček and Bojar, 2013).

https://github.com/moses-smt/mosesdecoder/blob/master/mert/evaluator.cpp
https://github.com/moses-smt/mosesdecoder/blob/master/mert/evaluator.cpp
https://github.com/mjpost/sacreBLEU
https://github.com/mjpost/sacreBLEU
http://www.itl.nist.gov/iad/mig/tools/


66

Sc
or

in
g

Le
ve

l
M

et
ri

c
Fe

at
ur

es
Le

ar
ne

d?
Se

g
Sy

s
C

it
at

io
n/

P
ar

ti
ci

pa
nt

A
va

ila
bi

lit
y

Baselines

se
nt

B
LE

U
n-

gr
am

s
•

−
(m

os
es

de
co

de
r)

me
rt

/s
en

te
nc

e-
bl

eu
B

LE
U

n-
gr

am
s

−
•

Pa
pi

ne
ni

et
al

.(
20

02
)

(m
os

es
de

co
de

r)
sc

ri
pt

s/
ge

ne
ri

c/
mt

ev
al

-v
13

a.
pl

N
IS

T
n-

gr
am

s
−

•
D

od
di

ng
to

n
(2

00
2)

(m
os

es
de

co
de

r)
sc

ri
pt

s/
ge

ne
ri

c/
mt

ev
al

-v
13

a.
pl

W
E

R
Le

ve
ns

ht
ei

n
di

st
an

ce
−

•
Le

us
ch

et
al

.(
20

06
)

(m
os

es
de

co
de

r)
me

rt
/e

va
lu

at
or

T
E

R
ed

it
di

st
an

ce
,e

di
t

ty
pe

s
−

•
Sn

ov
er

et
al

.(
20

06
)

(m
os

es
de

co
de

r)
me

rt
/e

va
lu

at
or

P
E

R
ed

it
di

st
an

ce
,e

di
t

ty
pe

s
−

•
Le

us
ch

et
al

.(
20

03
)

(m
os

es
de

co
de

r)
me

rt
/e

va
lu

at
or

C
D

E
R

ed
it

di
st

an
ce

,e
di

t
ty

pe
s

−
•

Le
us

ch
et

al
.(

20
06

)
(m

os
es

de
co

de
r)

me
rt

/e
va

lu
at

or
ch

rF
ch

ar
ac

te
r

n-
gr

am
s

•
⊘

Po
po

vi
ć

(2
01

5)
ht

tp
:/

/g
it

hu
b.

co
m/

m-
po

po
vi

c/
ch

rF
ch

rF
+

ch
ar

ac
te

r
n-

gr
am

s
•

⊘
Po

po
vi

ć
(2

01
7)

ht
tp

:/
/g

it
hu

b.
co

m/
m-

po
po

vi
c/

ch
rF

sa
cr

eB
LE

U
-B

LE
U

n-
gr

am
s

−
•

Po
st

(2
01

8a
)

ht
tp

:/
/g

it
hu

b.
co

m/
mj

po
st

/s
ac

re
BL

EU
sa

cr
eB

LE
U

-c
hr

F
n-

gr
am

s
−

•
Po

st
(2

01
8a

)
ht

tp
:/

/g
it

hu
b.

co
m/

mj
po

st
/s

ac
re

BL
EU

Metrics

B
E

E
R

ch
ar

.
n-

gr
am

s,
pe

rm
ut

at
io

n
tr

ee
s

ye
s

•
⊘

U
ni

v.
of

A
m

st
er

da
m

,I
LC

C
(S

ta
no

je
vi

ć
an

d
Si

m
a’

an
,2

01
5)

ht
tp

:/
/g

it
hu

b.
co

m/
st

an
oj

ev
ic

/b
ee

r
B

E
R

T
r

co
nt

ex
tu

al
wo

rd
em

be
dd

in
gs

•
⊘

U
ni

v.
of

M
el

bo
ur

ne
(M

at
hu

r
et

al
.,

20
19

)
ht

tp
:/

/g
it

hu
b.

co
m/

ni
ti

ka
m/

mt
ev

al
-i

n-
co

nt
ex

t
ch

ar
ac

T
E

R
ch

ar
.

ed
it

di
st

an
ce

,e
di

t
ty

pe
s

•
⊘

RW
T

H
A

ac
he

n
U

ni
v.

(W
an

g
et

al
.,

20
16

a)
ht

tp
:/

/g
it

hu
b.

co
m/

rw
th

-i
6/

Ch
ar

ac
TE

R
E

E
D

ch
ar

.
ed

it
di

st
an

ce
,e

di
t

ty
pe

s
•

⊘
RW

T
H

A
ac

he
n

U
ni

v.
(S

ta
nc

he
v

et
al

.,
20

19
)

ht
tp

:/
/g

it
hu

b.
co

m/
rw

th
-i

6/
Ex

te
nd

ed
Ed

it
Di

st
an

ce
E

SI
M

le
ar

ne
d

ne
ur

al
re

pr
es

en
ta

tio
ns

ye
s

•
⊘

U
ni

v.
of

M
el

bo
ur

ne
(M

at
hu

r
et

al
.,

20
19

)
ht

tp
:/

/g
it

hu
b.

co
m/

ni
ti

ka
m/

mt
ev

al
-i

n-
co

nt
ex

t
LE

P
O

R
a

su
rfa

ce
lin

gu
ist

ic
fe

at
ur

es
•

⊘
D

ub
lin

C
ity

U
ni

ve
rs

ity
,A

D
A

PT
(H

an
et

al
.,

20
12

,2
01

3)
ht

tp
:/

/g
it

hu
b.

co
m/

po
et

ha
n/

LE
PO

R
LE

P
O

R
b

su
rfa

ce
lin

gu
ist

ic
fe

at
ur

es
•

⊘
D

ub
lin

C
ity

U
ni

ve
rs

ity
,A

D
A

PT
(H

an
et

al
.,

20
12

,2
01

3)
ht

tp
:/

/g
it

hu
b.

co
m/

po
et

ha
n/

LE
PO

R
M

et
eo

r+
+

_
2.

0
(s

yn
ta

x)
wo

rd
al

ig
nm

en
ts

•
⊘

Pe
ki

ng
U

ni
ve

rs
ity

(G
uo

an
d

H
u,

20
19

)
−

M
et

eo
r+

+
_

2.
0

(s
yn

ta
x+

co
py

)
wo

rd
al

ig
nm

en
ts

•
⊘

Pe
ki

ng
U

ni
ve

rs
ity

(G
uo

an
d

H
u,

20
19

)
−

P
R

eP
ps

ue
do

-r
ef

er
en

ce
s,

pa
ra

ph
ra

se
s

•
⊘

To
ky

o
M

et
ro

po
lit

an
U

ni
v.

(Y
os

hi
m

ur
a

et
al

.,
20

19
)

ht
tp

:/
/g

it
hu

b.
co

m/
ko

ke
ma

n/
PR

eP
W

M
D

O
wo

rd
m

ov
er

di
st

an
ce

•
⊘

Im
pe

ria
lC

ol
le

ge
Lo

nd
on

(C
ho

w
et

al
.,

20
19

a)
−

Y
iS

i-0
se

m
an

tic
sim

ila
rit

y
•

⊘
N

R
C

(L
o,

20
19

)
ht

tp
:/

/g
it

hu
b.

co
m/

ch
ik

iu
lo

/Y
iS

i
Y

iS
i-1

se
m

an
tic

sim
ila

rit
y

•
⊘

N
R

C
(L

o,
20

19
)

ht
tp

:/
/g

it
hu

b.
co

m/
ch

ik
iu

lo
/Y

iS
i

Y
iS

i-1
_

sr
l

se
m

an
tic

sim
ila

rit
y

•
⊘

N
R

C
(L

o,
20

19
)

ht
tp

:/
/g

it
hu

b.
co

m/
ch

ik
iu

lo
/Y

iS
i

QESystems

IB
M

1-
m

or
ph

em
e

LM
lo

g
pr

ob
s.,

ib
m

1
le

xi
co

n
•

⊘
D

ub
lin

C
ity

U
ni

ve
rs

ity
,A

D
A

PT
(P

op
ov

ic
,2

01
2)

−
IB

M
1-

po
s4

gr
am

LM
lo

g
pr

ob
s.,

ib
m

1
le

xi
co

n
•

⊘
D

ub
lin

C
ity

U
ni

ve
rs

ity
,A

D
A

PT
(P

op
ov

ic
,2

01
2)

−
LP

co
nt

ex
tu

al
wo

rd
em

b.
,M

T
lo

g
pr

ob
.

ye
s

•
⊘

U
ni

v.
of

Ta
rt

u
(Y

an
ko

vs
ka

ya
et

al
.,

20
19

)
−

LA
SI

M
co

nt
ex

tu
al

wo
rd

em
be

dd
in

gs
ye

s
•

⊘
U

ni
v.

of
Ta

rt
u

(Y
an

ko
vs

ka
ya

et
al

.,
20

19
)

−
U

N
I

?
?

•
⊘

?
?

U
N

I+
?

?
•

⊘
?

?
U

SF
D

?
?

•
⊘

U
ni

v.
of

Sh
effi

el
d

?
U

SF
D

-T
L

?
?

•
⊘

U
ni

v.
of

Sh
effi

el
d

?
Y

iS
i-2

se
m

an
tic

sim
ila

rit
y

•
⊘

N
R

C
(L

o,
20

19
)

ht
tp

:/
/g

it
hu

b.
co

m/
ch

ik
iu

lo
/Y

iS
i

Y
iS

i-2
_

sr
l

se
m

an
tic

sim
ila

rit
y

•
⊘

N
R

C
(L

o,
20

19
)

ht
tp

:/
/g

it
hu

b.
co

m/
ch

ik
iu

lo
/Y

iS
i

Ta
bl

e
2:

Pa
rt

ic
ip

an
ts

of
W

M
T

19
M

et
ric

sS
ha

re
d

Ta
sk

.
“•

”
de

no
te

st
ha

tt
he

m
et

ric
to

ok
pa

rt
in

(s
om

e
of

th
e

la
ng

ua
ge

pa
irs

)o
ft

he
se

gm
en

t-
an

d/
or

sy
st

em
-le

ve
l

ev
al

ua
tio

n.
“⊘

”
in

di
ca

te
st

ha
tt

he
sy

st
em

-le
ve

ls
co

re
sa

re
im

pl
ie

d,
sim

pl
y

ta
ki

ng
ar

ith
m

et
ic

(m
ac

ro
-)

av
er

ag
e

of
se

gm
en

t-
le

ve
ls

co
re

s.
“−

”
in

di
ca

te
st

ha
tt

he
m

et
ric

di
dn

’t
pa

rt
ic

ip
at

e
th

e
tr

ac
k

(S
eg

/S
ys

-le
ve

l).
A

m
et

ric
is

le
ar

ne
d

if
it

is
tr

ai
ne

d
on

a
Q

E
or

m
et

ric
ev

al
ua

tio
n

da
ta

se
t

(i.
e.

pr
et

ra
in

in
g

or
pa

rs
er

s
do

n’
t

co
un

t,
bu

t
tr

ai
ni

ng
on

W
M

T
20

17
m

et
ric

s
ta

sk
da

ta
do

es
).

Fo
r

th
e

ba
se

lin
e

m
et

ric
s

av
ai

la
bl

e
in

th
e

M
os

es
to

ol
ki

t,
pa

th
s

ar
e

re
la

tiv
e

to
ht

tp
:/

/g
it

hu
b.

co
m/

mo
se

s-
sm

t/
mo

se
sd

ec
od

er
/.

mert/sentence-bleu
scripts/generic/mteval-v13a.pl
scripts/generic/mteval-v13a.pl
mert/evaluator
mert/evaluator
mert/evaluator
mert/evaluator
http://github.com/m-popovic/chrF
http://github.com/m-popovic/chrF
http://github.com/mjpost/sacreBLEU
http://github.com/mjpost/sacreBLEU
http://github.com/stanojevic/beer
http://github.com/nitikam/mteval-in-context
http://github.com/rwth-i6/CharacTER
http://github.com/rwth-i6/ExtendedEditDistance
http://github.com/nitikam/mteval-in-context
http://github.com/poethan/LEPOR
http://github.com/poethan/LEPOR
http://github.com/kokeman/PReP
http://github.com/chikiulo/YiSi
http://github.com/chikiulo/YiSi
http://github.com/chikiulo/YiSi
http://github.com/chikiulo/YiSi
http://github.com/chikiulo/YiSi
http://github.com/moses-smt/mosesdecoder/
http://github.com/moses-smt/mosesdecoder/


67

smoothed version of BLEU for scoring
at the segment-level. We used the stan-
dard tokenizer script as available in Moses
toolkit for tokenization.

chrF and chrF+. The metrics chrF and
chrF+ (Popović, 2015, 2017) are com-
puted using their original Python im-
plementation, see Table 2. We ran
chrF++.py with the parameters -nw 0 -b
3 to obtain the chrF score and with
-nw 1 -b 3 to obtain the chrF+ score.
Note that chrF intentionally removes all
spaces before matching the n-grams, deto-
kenizing the segments but also concate-
nating words.10

sacreBLEU-BLEU and sacreBLEU-
chrF. The metrics sacreBLEU-BLEU
and sacreBLEU-chrF (Post, 2018a)
are re-implementation of BLEU and chrF
respectively. We ran sacreBLEU-chrF
with the same parameters as chrF, but
their scores are slightly different. The sig-
nature strings produced by sacreBLEU
for BLEU and chrF respectively are
BLEU+case.lc+lang.de-en+numrefs.1+
smooth.exp+tok.intl+version.1.3.6
and chrF3+case.mixed+lang.de-en
+numchars.6+numrefs.1+space.False+
tok.13a+version.1.3.6.

The baselines serve in system and segment-
level evaluations as customary: BLEU, TER,
WER, PER, CDER, sacreBLEU-BLEU
and sacreBLEU-chrF for system-level only;
sentBLEU for segment-level only and chrF
for both.

Chinese word segmentation is unfortunately
not supported by the tokenization scripts men-
tioned above. For scoring Chinese with base-
line metrics, we thus pre-processed MT out-
puts and reference translations with the script
tokenizeChinese.py11 by Shujian Huang,
which separates Chinese characters from each
other and also from non-Chinese parts.

10We originally planned to use the chrF implemen-
tation which was recently made available in Moses
Scorer but it mishandles Unicode characters for now.

11http://hdl.handle.net/11346/WMT17-TVXH

4 Submitted Metrics

Table 2 lists the participants of the WMT19
Shared Metrics Task, along with their metrics
and links to the source code where available.
We have collected 24 metrics from a total of 13
research groups, with 10 reference-less “met-
rics” submitted to the joint task “QE as a Met-
rich” with WMT19 Quality Estimation Task.

The rest of this section provides a brief sum-
mary of all the metrics that participated.

4.1 BEER
BEER (Stanojević and Sima’an, 2015) is a
trained evaluation metric with a linear model
that combines sub-word feature indicators
(character n-grams) and global word order fea-
tures (skip bigrams) to achieve a language ag-
nostic and fast to compute evaluation metric.
BEER has participated in previous years of
the evaluation task.

4.2 BERTr
BERTr (Mathur et al., 2019) uses contextual
word embeddings to compare the MT output
with the reference translation.

The BERTr score of a translation is the
average recall score over all tokens, us-
ing a relaxed version of token matching
based on BERT embeddings: namely, com-
puting the maximum cosine similarity be-
tween the embedding of a reference to-
ken against any token in the MT out-
put. BERTr uses bert_base_uncased em-
beddings for the to-English language pairs,
and bert_base_multilingual_cased embed-
dings for all other language pairs.

4.3 CharacTER
CharacTER (Wang et al., 2016b,a), identi-
cal to the 2016 setup, is a character-level met-
ric inspired by the commonly applied transla-
tion edit rate (TER). It is defined as the mini-
mum number of character edits required to ad-
just a hypothesis, until it completely matches
the reference, normalized by the length of the
hypothesis sentence. CharacTER calculates
the character-level edit distance while per-
forming the shift edit on word level. Unlike
the strict matching criterion in TER, a hy-
pothesis word is considered to match a refer-
ence word and could be shifted, if the edit dis-

http://hdl.handle.net/11346/WMT17-TVXH


68

tance between them is below a threshold value.
The Levenshtein distance between the refer-
ence and the shifted hypothesis sequence is
computed on the character level. In addition,
the lengths of hypothesis sequences instead of
reference sequences are used for normalizing
the edit distance, which effectively counters
the issue that shorter translations normally
achieve lower TER.

Similarly to other character-level metrics,
CharacTER is generally applied to non-
tokenized outputs and references, which also
holds for this year’s submission with one ex-
ception. This year tokenization was carried
out for en-ru hypotheses and references be-
fore calculating the scores, since this results in
large improvements in terms of correlations.
For other language pairs, no tokenizer was
used for pre-processing.

4.4 EED

EED (Stanchev et al., 2019) is a character-
based metric, which builds upon CDER. It
is defined as the minimum number of opera-
tions of an extension to the conventional edit
distance containing a “jump” operation. The
edit distance operations (insertions, deletions
and substitutions) are performed at the char-
acter level and jumps are performed when a
blank space is reached. Furthermore, the cov-
erage of multiple characters in the hypothesis
is penalised by the introduction of a coverage
penalty. The sum of the length of the refer-
ence and the coverage penalty is used as the
normalisation term.

4.5 ESIM

Enhanced Sequential Inference Model (ESIM;
Chen et al., 2017; Mathur et al., 2019) is a
neural model proposed for Natural Language
Inference that has been adapted for MT evalu-
ation. It uses cross-sentence attention and sen-
tence matching heuristics to generate a repre-
sentation of the translation and the reference,
which is fed to a feedforward regressor. The
metric is trained on singly-annotated Direct
Assessment data that has been collected for
evaluating WMT systems: all WMT 2018 to-
English data for the to-English language pairs,
and all WMT 2018 data for all other language
pairs.

4.6 hLEPORb_baseline,
hLEPORa_baseline

The submitted metric hLEPOR_baseline is
a metric based on the factor combination of
length penalty, precision, recall, and position
difference penalty. The weighted harmonic
mean is applied to group the factors together
with tunable weight parameters. The system-
level score is calculated with the same formula
but with each factor weighted using weight es-
timated at system-level and not at segment-
level.

In this submitted baseline version, hLE-
POR_baseline was not tuned for each lan-
guage pair separately but the default weights
were applied across all submitted language
pairs. Further improvements can be achieved
by tuning the weights according to the devel-
opment data, adding morphological informa-
tion and applying n-gram factor scores into
it (e.g. part-of-speech, n-gram precision and
n-gram recall that were added into LEPOR
in WMT13.). The basic model factors and
further development with parameters setting
were described in the paper (Han et al., 2012)
and (Han et al., 2013).

For sentence-level score, only hLE-
PORa_baseline was submitted with scores
calculated as the weighted harmonic mean
of all the designed factors using default
parameters.

For system-level score, both
hLEPORa_baseline and hLE-
PORb_baseline were submitted, where
hLEPORa_baseline is the the average
score of all sentence-level scores, and hLE-
PORb_baseline is calculated via the same
sentence-level hLEPOR equation but replac-
ing each factor value with its system-level
counterpart.

4.7 Meteor++_2.0 (syntax),
Meteor++_2.0 (syntax+copy)

Meteor++ 2.0 (Guo and Hu, 2019) is
a metric based on Meteor (Denkowski and
Lavie, 2014) that takes syntactic-level para-
phrase knowledge into consideration, where
paraphrases may sometimes be skip-grams.
i.e. (protect...from, protect...against). As
the original Meteor-based metrics only pay
attention to consecutive string matching,



69

they perform badly when reference-hypothesis
pairs contain skip n-gram paraphrases. Me-
teor++ 2.0 extracts the knowledge from the
Paraphrase Database (PPDB; Bannard and
Callison-Burch, 2005) and integrates it into
Meteor-based metrics.

4.8 PReP
PReP (Yoshimura et al., 2019) is a method for
filtering pseudo-references to achieve a good
match with a gold reference.

At the beginning, the source sentence is
translated with some off-the-shelf MT sys-
tems to create a set of pseudo-references.
(Here the MT systems were Google Translate
and Microsoft Bing Translator.) The pseudo-
references are then filtered using BERT (De-
vlin et al., 2019) fine-tuned on the MPRC
corpus (Dolan and Brockett, 2005), estimat-
ing the probability of the paraphrase between
gold reference and pseudo-references. Thanks
to the high quality of the underlying MT sys-
tems, a large portion of their outputs is indeed
considered as a valid paraphrase.

The final metric score is calculated sim-
ply with SentBLEU with these multiple ref-
erences.

4.9 WMDO
WMDO (Chow et al., 2019b) is a metric based
on distance between distributions in the se-
mantic vector space. Matching in the seman-
tic space has been investigated for translation
evaluation, but the constraints of a transla-
tion’s word order have not been fully explored.
Building on the Word Mover’s Distance metric
and various word embeddings, WMDO intro-
duces a fragmentation penalty to account for
fluency of a translation. This word order ex-
tension is shown to perform better than stan-
dard WMD, with promising results against
other types of metrics.

4.10 YiSi-0, YiSi-1, YiSi-1_srl, YiSi-2,
YiSi-2_srl

YiSi (Lo, 2019) is a unified semantic MT qual-
ity evaluation and estimation metric for lan-
guages with different levels of available re-
sources.

YiSi-1 is a MT evaluation metric that mea-
sures the semantic similarity between a ma-
chine translation and human references by

aggregating the idf-weighted lexical semantic
similarities based on the contextual embed-
dings extracted from BERT and optionally in-
corporating shallow semantic structures (de-
noted as YiSi-1_srl).

YiSi-0 is the degenerate version of YiSi-1
that is ready-to-deploy to any language. It
uses longest common character substring to
measure the lexical similarity.

YiSi-2 is the bilingual, reference-less version
for MT quality estimation, which uses the con-
textual embeddings extracted from BERT to
evaluate the crosslingual lexical semantic simi-
larity between the input and MT output. Like
YiSi-1, YiSi-2 can exploit shallow semantic
structures as well (denoted as YiSi-2_srl).

4.11 QE Systems
In addition to the submitted standard metrics,
10 quality estimation systems were submitted
to the “QE as a Metric” track. The submitted
QE systems are evaluated in the same settings
as metrics to facilitate comparison. Their de-
scriptions can be found in the Findings of the
WMT 2019 Shared Task on Quality Estima-
tion (Fonseca et al., 2019).

5 Results
We discuss system-level results for news task
systems in Section 5.1. The segment-level re-
sults are in Section 5.2.

5.1 System-Level Evaluation
As in previous years, we employ the Pearson
correlation (r) as the main evaluation measure
for system-level metrics. The Pearson correla-
tion is as follows:

r =

∑n
i=1(Hi −H)(Mi −M)√∑n

i=1(Hi −H)2
√∑n

i=1(Mi −M)2
(1)

where Hi are human assessment scores of all
systems in a given translation direction, Mi
are the corresponding scores as predicted by
a given metric. H and M are their means,
respectively.

Since some metrics, such as BLEU, aim to
achieve a strong positive correlation with hu-
man assessment, while error metrics, such as
TER, aim for a strong negative correlation we
compare metrics via the absolute value |r| of a



70

de-en fi-en gu-en kk-en lt-en ru-en zh-en
n 16 12 11 11 11 14 15
Correlation |r| |r| |r| |r| |r| |r| |r|

BEER 0.906 0.993 0.952 0.986 0.947 0.915 0.942
BERTr 0.926 0.984 0.938 0.990 0.948 0.971 0.974
BLEU 0.849 0.982 0.834 0.946 0.961 0.879 0.899
CDER 0.890 0.988 0.876 0.967 0.975 0.892 0.917
CharacTER 0.898 0.990 0.922 0.953 0.955 0.923 0.943
chrF 0.917 0.992 0.955 0.978 0.940 0.945 0.956
chrF+ 0.916 0.992 0.947 0.976 0.940 0.945 0.956
EED 0.903 0.994 0.976 0.980 0.929 0.950 0.949
ESIM 0.941 0.971 0.885 0.986 0.989 0.968 0.988
hLEPORa_baseline − − − 0.975 − − 0.947
hLEPORb_baseline − − − 0.975 0.906 − 0.947
Meteor++_2.0(syntax) 0.887 0.995 0.909 0.974 0.928 0.950 0.948
Meteor++_2.0(syntax+copy) 0.896 0.995 0.900 0.971 0.927 0.952 0.952
NIST 0.813 0.986 0.930 0.942 0.944 0.925 0.921
PER 0.883 0.991 0.910 0.737 0.947 0.922 0.952
PReP 0.575 0.614 0.773 0.776 0.494 0.782 0.592
sacreBLEU.BLEU 0.813 0.985 0.834 0.946 0.955 0.873 0.903
sacreBLEU.chrF 0.910 0.990 0.952 0.969 0.935 0.919 0.955
TER 0.874 0.984 0.890 0.799 0.960 0.917 0.840
WER 0.863 0.983 0.861 0.793 0.961 0.911 0.820
WMDO 0.872 0.987 0.983 0.998 0.900 0.942 0.943
YiSi-0 0.902 0.993 0.993 0.991 0.927 0.958 0.937
YiSi-1 0.949 0.989 0.924 0.994 0.981 0.979 0.979
YiSi-1_srl 0.950 0.989 0.918 0.994 0.983 0.978 0.977
QE as a Metric:
ibm1-morpheme 0.345 0.740 − − 0.487 − −
ibm1-pos4gram 0.339 − − − − − −
LASIM 0.247 − − − − 0.310 −
LP 0.474 − − − − 0.488 −
UNI 0.846 0.930 − − − 0.805 −
UNI+ 0.850 0.924 − − − 0.808 −
YiSi-2 0.796 0.642 0.566 0.324 0.442 0.339 0.940
YiSi-2_srl 0.804 − − − − − 0.947

newstest2019

Table 3: Absolute Pearson correlation of to-English system-level metrics with DA human assessment in
newstest2019; correlations of metrics not significantly outperformed by any other for that language pair
are highlighted in bold.



71

en-cs en-de en-fi en-gu en-kk en-lt en-ru en-zh
n 11 22 12 11 11 12 12 12
Correlation |r| |r| |r| |r| |r| |r| |r| |r|

BEER 0.990 0.983 0.989 0.829 0.971 0.982 0.977 0.803
BLEU 0.897 0.921 0.969 0.737 0.852 0.989 0.986 0.901
CDER 0.985 0.973 0.978 0.840 0.927 0.985 0.993 0.905
CharacTER 0.994 0.986 0.968 0.910 0.936 0.954 0.985 0.862
chrF 0.990 0.979 0.986 0.841 0.972 0.981 0.943 0.880
chrF+ 0.991 0.981 0.986 0.848 0.974 0.982 0.950 0.879
EED 0.993 0.985 0.987 0.897 0.979 0.975 0.967 0.856
ESIM − 0.991 0.957 − 0.980 0.989 0.989 0.931
hLEPORa_baseline − − − 0.841 0.968 − − −
hLEPORb_baseline − − − 0.841 0.968 0.980 − −
NIST 0.896 0.321 0.971 0.786 0.930 0.993 0.988 0.884
PER 0.976 0.970 0.982 0.839 0.921 0.985 0.981 0.895
sacreBLEU.BLEU 0.994 0.969 0.966 0.736 0.852 0.986 0.977 0.801
sacreBLEU.chrF 0.983 0.976 0.980 0.841 0.967 0.966 0.985 0.796
TER 0.980 0.969 0.981 0.865 0.940 0.994 0.995 0.856
WER 0.982 0.966 0.980 0.861 0.939 0.991 0.994 0.875
YiSi-0 0.992 0.985 0.987 0.863 0.974 0.974 0.953 0.861
YiSi-1 0.962 0.991 0.971 0.909 0.985 0.963 0.992 0.951
YiSi-1_srl − 0.991 − − − − − 0.948
QE as a Metric:
ibm1-morpheme 0.871 0.870 0.084 − − 0.810 − −
ibm1-pos4gram − 0.393 − − − − − −
LASIM − 0.871 − − − − 0.823 −
LP − 0.569 − − − − 0.661 −
UNI 0.028 0.841 0.907 − − − 0.919 −
UNI+ − − − − − − 0.918 −
USFD − 0.224 − − − − 0.857 −
USFD-TL − 0.091 − − − − 0.771 −
YiSi-2 0.324 0.924 0.696 0.314 0.339 0.055 0.766 0.097
YiSi-2_srl − 0.936 − − − − − 0.118

newstest2019

Table 4: Absolute Pearson correlation of out-of-English system-level metrics with DA human assessment
in newstest2019; correlations of metrics not significantly outperformed by any other for that language
pair are highlighted in bold.



72

de-en fi-en gu-en

Y
iS

i.1
_s

rl
Y

iS
i.1

E
S

IM
B

E
R

Tr
ch

rF
ch

rF
.

sa
cr

eB
LE

U
.c

hr
F

B
E

E
R

E
E

D
Y

iS
i.0

C
ha

ra
cT

E
R

M
et

eo
r..

_2
.0

.s
yn

ta
x.

co
py

.
C

D
E

R
M

et
eo

r..
_2

.0
.s

yn
ta

x.
P

E
R

T
E

R
W

M
D

O
W

E
R

U
N

I.
B

LE
U

U
N

I
sa

cr
eB

LE
U

.B
LE

U
N

IS
T

Y
iS

i.2
_s

rl
Y

iS
i.2

P
R

eP
LP

.1
ib

m
1.

m
or

ph
em

e
ib

m
1.

po
s4

gr
am

LA
S

IM

LASIM
ibm1.pos4gram
ibm1.morpheme
LP.1
PReP
YiSi.2
YiSi.2_srl
NIST
sacreBLEU.BLEU
UNI
BLEU
UNI.
WER
WMDO
TER
PER
Meteor.._2.0.syntax.
CDER
Meteor.._2.0.syntax.copy.
CharacTER
YiSi.0
EED
BEER
sacreBLEU.chrF
chrF.
chrF
BERTr
ESIM
YiSi.1
YiSi.1_srl

M
et

eo
r..

_2
.0

.s
yn

ta
x.

co
py

.
M

et
eo

r..
_2

.0
.s

yn
ta

x.
E

E
D

B
E

E
R

Y
iS

i.0
ch

rF
.

ch
rF

P
E

R
C

ha
ra

cT
E

R
sa

cr
eB

LE
U

.c
hr

F
Y

iS
i.1

_s
rl

Y
iS

i.1
C

D
E

R
W

M
D

O
N

IS
T

sa
cr

eB
LE

U
.B

LE
U

T
E

R
B

E
R

Tr
W

E
R

B
LE

U
E

S
IM

U
N

I
U

N
I.

ib
m

1.
m

or
ph

em
e

Y
iS

i.2
P

R
eP

PReP
YiSi.2
ibm1.morpheme
UNI.
UNI
ESIM
BLEU
WER
BERTr
TER
sacreBLEU.BLEU
NIST
WMDO
CDER
YiSi.1
YiSi.1_srl
sacreBLEU.chrF
CharacTER
PER
chrF
chrF.
YiSi.0
BEER
EED
Meteor.._2.0.syntax.
Meteor.._2.0.syntax.copy.

Y
iS

i.0
W

M
D

O
E

E
D

ch
rF

sa
cr

eB
LE

U
.c

hr
F

B
E

E
R

ch
rF

.
B

E
R

Tr
N

IS
T

Y
iS

i.1
C

ha
ra

cT
E

R
Y

iS
i.1

_s
rl

P
E

R
M

et
eo

r..
_2

.0
.s

yn
ta

x.
M

et
eo

r..
_2

.0
.s

yn
ta

x.
co

py
.

T
E

R
E

S
IM

C
D

E
R

W
E

R
B

LE
U

sa
cr

eB
LE

U
.B

LE
U

P
R

eP
Y

iS
i.2

YiSi.2
PReP
sacreBLEU.BLEU
BLEU
WER
CDER
ESIM
TER
Meteor.._2.0.syntax.copy.
Meteor.._2.0.syntax.
PER
YiSi.1_srl
CharacTER
YiSi.1
NIST
BERTr
chrF.
BEER
sacreBLEU.chrF
chrF
EED
WMDO
YiSi.0

kk-en lt-en ru-en

W
M

D
O

Y
iS

i.1
Y

iS
i.1

_s
rl

Y
iS

i.0
B

E
R

Tr
E

S
IM

B
E

E
R

E
E

D
ch

rF
ch

rF
.

hL
E

P
O

R
a_

ba
se

lin
e

hL
E

P
O

R
b_

ba
se

lin
e

M
et

eo
r..

_2
.0

.s
yn

ta
x.

M
et

eo
r..

_2
.0

.s
yn

ta
x.

co
py

.
sa

cr
eB

LE
U

.c
hr

F
C

D
E

R
C

ha
ra

cT
E

R
B

LE
U

sa
cr

eB
LE

U
.B

LE
U

N
IS

T
T

E
R

W
E

R
P

R
eP

P
E

R
Y

iS
i.2

YiSi.2
PER
PReP
WER
TER
NIST
sacreBLEU.BLEU
BLEU
CharacTER
CDER
sacreBLEU.chrF
Meteor.._2.0.syntax.copy.
Meteor.._2.0.syntax.
hLEPORb_baseline
hLEPORa_baseline
chrF.
chrF
EED
BEER
ESIM
BERTr
YiSi.0
YiSi.1_srl
YiSi.1
WMDO

E
S

IM
Y

iS
i.1

_s
rl

Y
iS

i.1
C

D
E

R
B

LE
U

W
E

R
T

E
R

sa
cr

eB
LE

U
.B

LE
U

C
ha

ra
cT

E
R

B
E

R
Tr

P
E

R
B

E
E

R
N

IS
T

ch
rF

.
ch

rF
sa

cr
eB

LE
U

.c
hr

F
E

E
D

M
et

eo
r..

_2
.0

.s
yn

ta
x.

M
et

eo
r..

_2
.0

.s
yn

ta
x.

co
py

.
Y

iS
i.0

hL
E

P
O

R
b_

ba
se

lin
e

W
M

D
O

P
R

eP
ib

m
1.

m
or

ph
em

e
Y

iS
i.2

YiSi.2
ibm1.morpheme
PReP
WMDO
hLEPORb_baseline
YiSi.0
Meteor.._2.0.syntax.copy.
Meteor.._2.0.syntax.
EED
sacreBLEU.chrF
chrF
chrF.
NIST
BEER
PER
BERTr
CharacTER
sacreBLEU.BLEU
TER
WER
BLEU
CDER
YiSi.1
YiSi.1_srl
ESIM

Y
iS

i.1
Y

iS
i.1

_s
rl

B
E

R
Tr

E
S

IM
Y

iS
i.0

M
et

eo
r..

_2
.0

.s
yn

ta
x.

co
py

.
M

et
eo

r..
_2

.0
.s

yn
ta

x.
E

E
D

ch
rF

ch
rF

.
W

M
D

O
N

IS
T

C
ha

ra
cT

E
R

P
E

R
sa

cr
eB

LE
U

.c
hr

F
T

E
R

B
E

E
R

W
E

R
C

D
E

R
B

LE
U

sa
cr

eB
LE

U
.B

LE
U

U
N

I.
U

N
I

P
R

eP
LP

.1
Y

iS
i.2

LA
S

IM

LASIM
YiSi.2
LP.1
PReP
UNI
UNI.
sacreBLEU.BLEU
BLEU
CDER
WER
BEER
TER
sacreBLEU.chrF
PER
CharacTER
NIST
WMDO
chrF.
chrF
EED
Meteor.._2.0.syntax.
Meteor.._2.0.syntax.copy.
YiSi.0
ESIM
BERTr
YiSi.1_srl
YiSi.1

zh-en en-cs en-de

E
S

IM
Y

iS
i.1

Y
iS

i.1
_s

rl
B

E
R

Tr
ch

rF
.

ch
rF

sa
cr

eB
LE

U
.c

hr
F

P
E

R
M

et
eo

r..
_2

.0
.s

yn
ta

x.
co

py
.

E
E

D
M

et
eo

r..
_2

.0
.s

yn
ta

x.
hL

E
P

O
R

b_
ba

se
lin

e
hL

E
P

O
R

a_
ba

se
lin

e
Y

iS
i.2

_s
rl

W
M

D
O

C
ha

ra
cT

E
R

B
E

E
R

Y
iS

i.2
Y

iS
i.0

N
IS

T
C

D
E

R
sa

cr
eB

LE
U

.B
LE

U
B

LE
U

T
E

R
W

E
R

P
R

eP

PReP
WER
TER
BLEU
sacreBLEU.BLEU
CDER
NIST
YiSi.0
YiSi.2
BEER
CharacTER
WMDO
YiSi.2_srl
hLEPORa_baseline
hLEPORb_baseline
Meteor.._2.0.syntax.
EED
Meteor.._2.0.syntax.copy.
PER
sacreBLEU.chrF
chrF
chrF.
BERTr
YiSi.1_srl
YiSi.1
ESIM

sa
cr

eB
LE

U
.B

LE
U

C
ha

ra
cT

E
R

E
E

D
Y

iS
i.0

ch
rF

.
B

E
E

R
ch

rF
C

D
E

R
sa

cr
eB

LE
U

.c
hr

F
W

E
R

T
E

R
P

E
R

Y
iS

i.1
B

LE
U

N
IS

T
ib

m
1.

m
or

ph
em

e
Y

iS
i.2

U
N

I

UNI
YiSi.2
ibm1.morpheme
NIST
BLEU
YiSi.1
PER
TER
WER
sacreBLEU.chrF
CDER
chrF
BEER
chrF.
YiSi.0
EED
CharacTER
sacreBLEU.BLEU

E
S

IM
Y

iS
i.1

Y
iS

i.1
_s

rl
C

ha
ra

cT
E

R
E

E
D

Y
iS

i.0
B

E
E

R
ch

rF
.

ch
rF

sa
cr

eB
LE

U
.c

hr
F

C
D

E
R

P
E

R
sa

cr
eB

LE
U

.B
LE

U
T

E
R

W
E

R
Y

iS
i.2

_s
rl

Y
iS

i.2
B

LE
U

LA
S

IM
ib

m
1.

m
or

ph
em

e
U

N
I

LP
.1

ib
m

1.
po

s4
gr

am
N

IS
T

U
S

F
D

U
S

F
D

.T
L

USFD.TL
USFD
NIST
ibm1.pos4gram
LP.1
UNI
ibm1.morpheme
LASIM
BLEU
YiSi.2
YiSi.2_srl
WER
TER
sacreBLEU.BLEU
PER
CDER
sacreBLEU.chrF
chrF
chrF.
BEER
YiSi.0
EED
CharacTER
YiSi.1_srl
YiSi.1
ESIM

en-fi en-gu en-kk

B
E

E
R

E
E

D
Y

iS
i.0

ch
rF

ch
rF

.
P

E
R

T
E

R
sa

cr
eB

LE
U

.c
hr

F
W

E
R

C
D

E
R

Y
iS

i.1
N

IS
T

B
LE

U
C

ha
ra

cT
E

R
sa

cr
eB

LE
U

.B
LE

U
E

S
IM

U
N

I
Y

iS
i.2

ib
m

1.
m

or
ph

em
e

ibm1.morpheme
YiSi.2
UNI
ESIM
sacreBLEU.BLEU
CharacTER
BLEU
NIST
YiSi.1
CDER
WER
sacreBLEU.chrF
TER
PER
chrF.
chrF
YiSi.0
EED
BEER

C
ha

ra
cT

E
R

Y
iS

i.1
E

E
D

T
E

R
Y

iS
i.0

W
E

R
ch

rF
.

sa
cr

eB
LE

U
.c

hr
F

ch
rF

hL
E

P
O

R
a_

ba
se

lin
e

hL
E

P
O

R
b_

ba
se

lin
e

C
D

E
R

P
E

R
B

E
E

R
N

IS
T

B
LE

U
sa

cr
eB

LE
U

.B
LE

U
Y

iS
i.2

YiSi.2
sacreBLEU.BLEU
BLEU
NIST
BEER
PER
CDER
hLEPORb_baseline
hLEPORa_baseline
chrF
sacreBLEU.chrF
chrF.
WER
YiSi.0
TER
EED
YiSi.1
CharacTER

Y
iS

i.1
E

S
IM

E
E

D
ch

rF
.

Y
iS

i.0
ch

rF
B

E
E

R
hL

E
P

O
R

a_
ba

se
lin

e
hL

E
P

O
R

b_
ba

se
lin

e
sa

cr
eB

LE
U

.c
hr

F
T

E
R

W
E

R
C

ha
ra

cT
E

R
N

IS
T

C
D

E
R

P
E

R
sa

cr
eB

LE
U

.B
LE

U
B

LE
U

Y
iS

i.2

YiSi.2
BLEU
sacreBLEU.BLEU
PER
CDER
NIST
CharacTER
WER
TER
sacreBLEU.chrF
hLEPORb_baseline
hLEPORa_baseline
BEER
chrF
YiSi.0
chrF.
EED
ESIM
YiSi.1

en-lt en-ru en-zh

T
E

R
N

IS
T

W
E

R
E

S
IM

B
LE

U
sa

cr
eB

LE
U

.B
LE

U
C

D
E

R
P

E
R

B
E

E
R

ch
rF

.
ch

rF
hL

E
P

O
R

b_
ba

se
lin

e
E

E
D

Y
iS

i.0
sa

cr
eB

LE
U

.c
hr

F
Y

iS
i.1

C
ha

ra
cT

E
R

ib
m

1.
m

or
ph

em
e

Y
iS

i.2

YiSi.2
ibm1.morpheme
CharacTER
YiSi.1
sacreBLEU.chrF
YiSi.0
EED
hLEPORb_baseline
chrF
chrF.
BEER
PER
CDER
sacreBLEU.BLEU
BLEU
ESIM
WER
NIST
TER

T
E

R
W

E
R

C
D

E
R

Y
iS

i.1
E

S
IM

N
IS

T
B

LE
U

C
ha

ra
cT

E
R

sa
cr

eB
LE

U
.c

hr
F

P
E

R
sa

cr
eB

LE
U

.B
LE

U
B

E
E

R
E

E
D

Y
iS

i.0
ch

rF
.

ch
rF

U
N

I
U

N
I.

U
S

F
D

LA
S

IM
U

S
F

D
.T

L
Y

iS
i.2

LP
.1

LP.1
YiSi.2
USFD.TL
LASIM
USFD
UNI.
UNI
chrF
chrF.
YiSi.0
EED
BEER
sacreBLEU.BLEU
PER
sacreBLEU.chrF
CharacTER
BLEU
NIST
ESIM
YiSi.1
CDER
WER
TER

Y
iS

i.1
Y

iS
i.1

_s
rl

E
S

IM
C

D
E

R
B

LE
U

P
E

R
N

IS
T

ch
rF

ch
rF

.
W

E
R

C
ha

ra
cT

E
R

Y
iS

i.0
E

E
D

T
E

R
B

E
E

R
sa

cr
eB

LE
U

.B
LE

U
sa

cr
eB

LE
U

.c
hr

F
Y

iS
i.2

_s
rl

Y
iS

i.2

YiSi.2
YiSi.2_srl
sacreBLEU.chrF
sacreBLEU.BLEU
BEER
TER
EED
YiSi.0
CharacTER
WER
chrF.
chrF
NIST
PER
BLEU
CDER
ESIM
YiSi.1_srl
YiSi.1

Figure 1: System-level metric significance test results for DA human assessment for into English and
out-of English language pairs (newstest2019): Green cells denote a statistically significant increase in
correlation with human assessment for the metric in a given row over the metric in a given column
according to Williams test.



73

given metric’s correlation with human assess-
ment.

5.1.1 System-Level Results
Tables 3, 4 and 5 provide the system-level cor-
relations of metrics evaluating translation of
newstest2019. The underlying texts are part
of the WMT19 News Translation test set (new-
stest2019) and the underlying MT systems are
all MT systems participating in the WMT19
News Translation Task.

As recommended by Graham and Bald-
win (2014), we employ Williams significance
test (Williams, 1959) to identify differences
in correlation that are statistically significant.
Williams test is a test of significance of a dif-
ference in dependent correlations and there-
fore suitable for evaluation of metrics. Corre-
lations not significantly outperformed by any
other metric for the given language pair are
highlighted in bold in Tables 3, 4 and 5.

Since pairwise comparisons of metrics may
be also of interest, e.g. to learn which metrics
significantly outperform the most widely em-
ployed metric BLEU, we include significance
test results for every competing pair of metrics
including our baseline metrics in Figure 1 and
Figure 2.

This year, the increased number of systems
participating in the news tasks has provided a
larger sample of system scores for testing met-
rics. Since we already have sufficiently con-
clusive results on genuine MT systems, we do
not need to generate hybrid system results as
in Graham and Liu (2016) and past metrics
tasks.

5.2 Segment-Level Evaluation
Segment-level evaluation relies on the man-
ual judgements collected in the News Trans-
lation Task evaluation. This year, again we
were unable to follow the methodology out-
lined in Graham et al. (2015) for evaluation of
segment-level metrics because the sampling of
sentences did not provide sufficient number of
assessments of the same segment. We there-
fore convert pairs of DA scores for compet-
ing translations to daRR better/worse prefer-
ences as described in Section 2.3.2.

We measure the quality of metrics’ segment-
level scores against the daRR golden truth us-
ing a Kendall’s Tau-like formulation, which is

an adaptation of the conventional Kendall’s
Tau coefficient. Since we do not have a to-
tal order ranking of all translations, it is not
possible to apply conventional Kendall’s Tau
(Graham et al., 2015).

Our Kendall’s Tau-like formulation, τ , is as
follows:

τ =
|Concordant| − |Discordant|
|Concordant|+ |Discordant|

(2)

where Concordant is the set of all human com-
parisons for which a given metric suggests the
same order and Discordant is the set of all
human comparisons for which a given metric
disagrees. The formula is not specific with re-
spect to ties, i.e. cases where the annotation
says that the two outputs are equally good.

The way in which ties (both in human and
metric judgement) were incorporated in com-
puting Kendall τ has changed across the years
of WMT Metrics Tasks. Here we adopt the
version used in WMT17 daRR evaluation.
For a detailed discussion on other options, see
also Macháček and Bojar (2014).

Whether or not a given comparison of a pair
of distinct translations of the same source in-
put, s1 and s2, is counted as a concordant
(Conc) or disconcordant (Disc) pair is defined
by the following matrix:

Metric
s1 < s2 s1 = s2 s1 > s2

H
um

an s1 < s2 Conc Disc Disc
s1 = s2 − − −
s1 > s2 Disc Disc Conc

In the notation of Macháček and Bojar
(2014), this corresponds to the setup used in
WMT12 (with a different underlying method
of manual judgements, RR):

Metric
WMT12 < = >

H
um

an < 1 -1 -1
= X X X
> -1 -1 1

The key differences between the evaluation
used in WMT14–WMT16 and evaluation used
in WMT17–WMT19 were (1) the move from
RR to daRR and (2) the treatment of ties. In
the years 2014-2016, ties in metrics scores were
not penalized. With the move to daRR, where
the quality of the two candidate translations



74

de-cs de-fr fr-de
n 11 11 10
Correlation |r| |r| |r|

BEER 0.978 0.941 0.848
BLEU 0.941 0.891 0.864
CDER 0.864 0.949 0.852
CharacTER 0.965 0.928 0.849
chrF 0.974 0.931 0.864
chrF+ 0.972 0.936 0.848
EED 0.982 0.940 0.851
ESIM 0.980 0.950 0.942
hLEPORa_baseline 0.941 0.814 −
hLEPORb_baseline 0.959 0.814 −
NIST 0.954 0.916 0.862
PER 0.875 0.857 0.899
sacreBLEU-BLEU 0.869 0.891 0.869
sacreBLEU-chrF 0.975 0.952 0.882
TER 0.890 0.956 0.895
WER 0.872 0.956 0.894
YiSi-0 0.978 0.952 0.820
YiSi-1 0.973 0.969 0.908
YiSi-1_srl − − 0.912
QE as a Metric:
ibm1-morpheme 0.355 0.509 0.625
ibm1-pos4gram − 0.085 0.478
YiSi-2 0.606 0.721 0.530

newstest2019

Table 5: Absolute Pearson correlation of system-level metrics for language pairs not involving English
with DA human assessment in newstest2019; correlations of metrics not significantly outperformed by
any other for that language pair are highlighted in bold.

de-cs de-fr fr-de

E
E

D
E

S
IM

Y
iS

i.0
B

E
E

R
sa

cr
eB

LE
U

.c
hr

F
ch

rF
Y

iS
i.1

ch
rF

.
C

ha
ra

cT
E

R
hL

E
P

O
R

b_
ba

se
lin

e
N

IS
T

B
LE

U
hL

E
P

O
R

a_
ba

se
lin

e
T

E
R

P
E

R
W

E
R

sa
cr

eB
LE

U
.B

LE
U

C
D

E
R

Y
iS

i.2
ib

m
1.

m
or

ph
em

e

ibm1.morpheme
YiSi.2
CDER
sacreBLEU.BLEU
WER
PER
TER
hLEPORa_baseline
BLEU
NIST
hLEPORb_baseline
CharacTER
chrF.
YiSi.1
chrF
sacreBLEU.chrF
BEER
YiSi.0
ESIM
EED

Y
iS

i.1
W

E
R

T
E

R
Y

iS
i.0

sa
cr

eB
LE

U
.c

hr
F

E
S

IM
C

D
E

R
B

E
E

R
E

E
D

ch
rF

.
ch

rF
C

ha
ra

cT
E

R
N

IS
T

B
LE

U
sa

cr
eB

LE
U

.B
LE

U
P

E
R

hL
E

P
O

R
b_

ba
se

lin
e

hL
E

P
O

R
a_

ba
se

lin
e

Y
iS

i.2
ib

m
1.

m
or

ph
em

e
ib

m
1.

po
s4

gr
am

ibm1.pos4gram
ibm1.morpheme
YiSi.2
hLEPORa_baseline
hLEPORb_baseline
PER
sacreBLEU.BLEU
BLEU
NIST
CharacTER
chrF
chrF.
EED
BEER
CDER
ESIM
sacreBLEU.chrF
YiSi.0
TER
WER
YiSi.1

E
S

IM
Y

iS
i.1

_s
rl

Y
iS

i.1
P

E
R

T
E

R
W

E
R

sa
cr

eB
LE

U
.c

hr
F

sa
cr

eB
LE

U
.B

LE
U

ch
rF

B
LE

U
N

IS
T

C
D

E
R

E
E

D
C

ha
ra

cT
E

R
B

E
E

R
ch

rF
.

Y
iS

i.0
ib

m
1.

m
or

ph
em

e
Y

iS
i.2

ib
m

1.
po

s4
gr

am

ibm1.pos4gram
YiSi.2
ibm1.morpheme
YiSi.0
chrF.
BEER
CharacTER
EED
CDER
NIST
BLEU
chrF
sacreBLEU.BLEU
sacreBLEU.chrF
WER
TER
PER
YiSi.1
YiSi.1_srl
ESIM

Figure 2: System-level metric significance test results for DA human assessment in newstest2019 for
German to Czech, German to French and French to German; green cells denote a statistically significant
increase in correlation with human assessment for the metric in a given row over the metric in a given
column according to Williams test.



75

de-en fi-en gu-en kk-en lt-en ru-en zh-en
Human Evaluation daRR daRR daRR daRR daRR daRR daRR
n 85,365 38,307 31,139 27,094 21,862 46,172 31,070
BEER 0.128 0.283 0.260 0.421 0.315 0.189 0.371
BERTr 0.142 0.331 0.291 0.421 0.353 0.195 0.399
CharacTER 0.101 0.253 0.190 0.340 0.254 0.155 0.337
chrF 0.122 0.286 0.256 0.389 0.301 0.180 0.371
chrF+ 0.125 0.289 0.257 0.394 0.303 0.182 0.374
EED 0.120 0.281 0.264 0.392 0.298 0.176 0.376
ESIM 0.167 0.337 0.303 0.435 0.359 0.201 0.396
hLEPORa_baseline − − − 0.372 − − 0.339
Meteor++_2.0(syntax) 0.084 0.274 0.237 0.395 0.291 0.156 0.370
Meteor++_2.0(syntax+copy) 0.094 0.273 0.244 0.402 0.287 0.163 0.367
PReP 0.030 0.197 0.192 0.386 0.193 0.124 0.267
sentBLEU 0.056 0.233 0.188 0.377 0.262 0.125 0.323
WMDO 0.096 0.281 0.260 0.420 0.300 0.162 0.362
YiSi-0 0.117 0.271 0.263 0.402 0.289 0.178 0.355
YiSi-1 0.164 0.347 0.312 0.440 0.376 0.217 0.426
YiSi-1_srl 0.199 0.346 0.306 0.442 0.380 0.222 0.431
QE as a Metric:
ibm1-morpheme −0.074 0.009 − − 0.069 − −
ibm1-pos4gram −0.153 − − − − − −
LASIM −0.024 − − − − 0.022 −
LP −0.096 − − − − −0.035 −
UNI 0.022 0.202 − − − 0.084 −
UNI+ 0.015 0.211 − − − 0.089 −
YiSi-2 0.068 0.126 −0.001 0.096 0.075 0.053 0.253
YiSi-2_srl 0.068 − − − − − 0.246

newstest2019

Table 6: Segment-level metric results for to-English language pairs in newstest2019: absolute Kendall’s
Tau formulation of segment-level metric scores with DA scores; correlations of metrics not significantly
outperformed by any other for that language pair are highlighted in bold.



76

en-cs en-de en-fi en-gu en-kk en-lt en-ru en-zh
Human Evaluation daRR daRR daRR daRR daRR daRR daRR daRR
n 27,178 99,840 31,820 11,355 18,172 17,401 24,334 18,658

BEER 0.443 0.316 0.514 0.537 0.516 0.441 0.542 0.232
CharacTER 0.349 0.264 0.404 0.500 0.351 0.311 0.432 0.094
chrF 0.455 0.326 0.514 0.534 0.479 0.446 0.539 0.301
chrF+ 0.458 0.327 0.514 0.538 0.491 0.448 0.543 0.296
EED 0.431 0.315 0.508 0.568 0.518 0.425 0.546 0.257
ESIM − 0.329 0.511 − 0.510 0.428 0.572 0.339
hLEPORa_baseline − − − 0.463 0.390 − − −
sentBLEU 0.367 0.248 0.396 0.465 0.392 0.334 0.469 0.270
YiSi-0 0.406 0.304 0.483 0.539 0.494 0.402 0.535 0.266
YiSi-1 0.475 0.351 0.537 0.551 0.546 0.470 0.585 0.355
YiSi-1_srl − 0.368 − − − − − 0.361
QE as a Metric:
ibm1-morpheme −0.135 −0.003 −0.005 − − −0.165 − −
ibm1-pos4gram − −0.123 − − − − − −
LASIM − 0.147 − − − − −0.24 −
LP − −0.119 − − − − −0.158 −
UNI 0.060 0.129 0.351 − − − 0.226 −
UNI+ − − − − − − 0.222 −
USFD − −0.029 − − − − 0.136 −
USFD-TL − −0.037 − − − − 0.191 −
YiSi-2 0.069 0.212 0.239 0.147 0.187 0.003 −0.155 0.044
YiSi-2_srl − 0.236 − − − − − 0.034

newstest2019

Table 7: Segment-level metric results for out-of-English language pairs in newstest2019: absolute
Kendall’s Tau formulation of segment-level metric scores with DA scores; correlations of metrics not
significantly outperformed by any other for that language pair are highlighted in bold.

de-cs de-fr fr-de
Human Evaluation daRR daRR daRR
n 35,793 4,862 1,369

BEER 0.337 0.293 0.265
CharacTER 0.232 0.251 0.224
chrF 0.326 0.284 0.275
chrF+ 0.326 0.284 0.278
EED 0.345 0.301 0.267
ESIM 0.331 0.290 0.289
hLEPORa_baseline 0.207 0.239 −
sentBLEU 0.203 0.235 0.179
YiSi-0 0.331 0.296 0.277
YiSi-1 0.376 0.349 0.310
YiSi-1_srl − − 0.299
QE as a Metric:
ibm1-morpheme 0.048 −0.013 −0.053
ibm1-pos4gram − −0.074 −0.097
YiSi-2 0.199 0.186 0.066

newstest2019

Table 8: Segment-level metric results for language
pairs not involving English in newstest2019: ab-
solute Kendall’s Tau formulation of segment-level
metric scores with DA scores; correlations of met-
rics not significantly outperformed by any other for
that language pair are highlighted in bold.

is deemed substantially different and no ties
in human judgements arise, it makes sense to
penalize ties in metrics’ predictions in order to
promote discerning metrics.

Note that the penalization of ties makes our
evaluation asymmetric, dependent on whether
the metric predicted the tie for a pair where
humans predicted <, or >. It is now impor-
tant to interpret the meaning of the compar-
ison identically for humans and metrics. For
error metrics, we thus reverse the sign of the
metric score prior to the comparison with hu-
man scores: higher scores have to indicate bet-
ter translation quality. In WMT19, the origi-
nal authors did this for CharacTER.

To summarize, the WMT19 Metrics Task
for segment-level evaluation:

• ensures that error metrics are first con-
verted to the same orientation as the hu-
man judgements, i.e. higher score indi-
cating higher translation quality,

• excludes all human ties (this is already
implied by the construction of daRR
from DA judgements),



77

de-en fi-en gu-en

Y
iS

i.1
_s

rl
E

S
IM

Y
iS

i.1
B

E
R

Tr
B

E
E

R
ch

rF
.

ch
rF

E
E

D
Y

iS
i.0

C
ha

ra
cT

E
R

W
M

D
O

M
et

eo
r..

_2
.0

.s
yn

ta
x.

co
py

.
M

et
eo

r..
_2

.0
.s

yn
ta

x.
Y

iS
i.2

Y
iS

i.2
_s

rl
se

nt
B

LE
U

P
R

eP U
N

I
U

N
I.

LA
S

IM
ib

m
1.

m
or

ph
em

e
LP

ib
m

1.
po

s4
gr

am

ibm1−pos4gram
LP
ibm1−morpheme
LASIM
UNI+
UNI
PReP
sentBLEU
YiSi−2_srl
YiSi−2
Meteor++_2.0(syntax)
Meteor++_2.0(syntax+copy)
WMDO
CharacTER
YiSi−0
EED
chrF
chrF+
BEER
BERTr
YiSi−1
ESIM
YiSi−1_srl

Y
iS

i.1
Y

iS
i.1

_s
rl

E
S

IM
B

E
R

Tr
ch

rF
.

ch
rF

B
E

E
R

W
M

D
O

E
E

D
M

et
eo

r..
_2

.0
.s

yn
ta

x.
M

et
eo

r..
_2

.0
.s

yn
ta

x.
co

py
.

Y
iS

i.0
C

ha
ra

cT
E

R
se

nt
B

LE
U

U
N

I.
U

N
I

P
R

eP
Y

iS
i.2

ib
m

1.
m

or
ph

em
e

ibm1−morpheme
YiSi−2
PReP
UNI
UNI+
sentBLEU
CharacTER
YiSi−0
Meteor++_2.0(syntax+copy)
Meteor++_2.0(syntax)
EED
WMDO
BEER
chrF
chrF+
BERTr
ESIM
YiSi−1_srl
YiSi−1

Y
iS

i.1
Y

iS
i.1

_s
rl

E
S

IM
B

E
R

Tr
E

E
D

Y
iS

i.0
W

M
D

O
B

E
E

R
ch

rF
.

ch
rF

M
et

eo
r..

_2
.0

.s
yn

ta
x.

co
py

.
M

et
eo

r..
_2

.0
.s

yn
ta

x.
P

R
eP

C
ha

ra
cT

E
R

se
nt

B
LE

U
Y

iS
i.2

YiSi−2
sentBLEU
CharacTER
PReP
Meteor++_2.0(syntax)
Meteor++_2.0(syntax+copy)
chrF
chrF+
BEER
WMDO
YiSi−0
EED
BERTr
ESIM
YiSi−1_srl
YiSi−1

kk-en lt-en ru-en

Y
iS

i.1
_s

rl
Y

iS
i.1

E
S

IM
B

E
R

Tr
B

E
E

R
W

M
D

O
Y

iS
i.0

M
et

eo
r..

_2
.0

.s
yn

ta
x.

co
py

.
M

et
eo

r..
_2

.0
.s

yn
ta

x.
ch

rF
.

E
E

D
ch

rF
P

R
eP

se
nt

B
LE

U
hL

E
P

O
R

a_
ba

se
lin

e
C

ha
ra

cT
E

R
Y

iS
i.2

YiSi−2
CharacTER
hLEPORa_baseline
sentBLEU
PReP
chrF
EED
chrF+
Meteor++_2.0(syntax)
Meteor++_2.0(syntax+copy)
YiSi−0
WMDO
BEER
BERTr
ESIM
YiSi−1
YiSi−1_srl

Y
iS

i.1
_s

rl
Y

iS
i.1

E
S

IM
B

E
R

Tr
B

E
E

R
ch

rF
.

ch
rF

W
M

D
O

E
E

D
M

et
eo

r..
_2

.0
.s

yn
ta

x.
Y

iS
i.0

M
et

eo
r..

_2
.0

.s
yn

ta
x.

co
py

.
se

nt
B

LE
U

C
ha

ra
cT

E
R

P
R

eP
Y

iS
i.2

ib
m

1.
m

or
ph

em
e

ibm1−morpheme
YiSi−2
PReP
CharacTER
sentBLEU
Meteor++_2.0(syntax+copy)
YiSi−0
Meteor++_2.0(syntax)
EED
WMDO
chrF
chrF+
BEER
BERTr
ESIM
YiSi−1
YiSi−1_srl

Y
iS

i.1
_s

rl
Y

iS
i.1

E
S

IM
B

E
R

Tr
B

E
E

R
ch

rF
.

ch
rF

Y
iS

i.0
E

E
D

M
et

eo
r..

_2
.0

.s
yn

ta
x.

co
py

.
W

M
D

O
M

et
eo

r..
_2

.0
.s

yn
ta

x.
C

ha
ra

cT
E

R
se

nt
B

LE
U

P
R

eP
U

N
I.

U
N

I
Y

iS
i.2

LA
S

IM LP

LP
LASIM
YiSi−2
UNI
UNI+
PReP
sentBLEU
CharacTER
Meteor++_2.0(syntax)
WMDO
Meteor++_2.0(syntax+copy)
EED
YiSi−0
chrF
chrF+
BEER
BERTr
ESIM
YiSi−1
YiSi−1_srl

zh-en en-cs en-de

Y
iS

i.1
_s

rl
Y

iS
i.1

B
E

R
Tr

E
S

IM
E

E
D

ch
rF

.
B

E
E

R
ch

rF
M

et
eo

r..
_2

.0
.s

yn
ta

x.
M

et
eo

r..
_2

.0
.s

yn
ta

x.
co

py
.

W
M

D
O

Y
iS

i.0
hL

E
P

O
R

a_
ba

se
lin

e
C

ha
ra

cT
E

R
se

nt
B

LE
U

P
R

eP
Y

iS
i.2

Y
iS

i.2
_s

rl

YiSi−2_srl
YiSi−2
PReP
sentBLEU
CharacTER
hLEPORa_baseline
YiSi−0
WMDO
Meteor++_2.0(syntax+copy)
Meteor++_2.0(syntax)
chrF
BEER
chrF+
EED
ESIM
BERTr
YiSi−1
YiSi−1_srl

Y
iS

i.1

ch
rF

.

ch
rF

B
E

E
R

E
E

D

Y
iS

i.0

se
nt

B
LE

U

C
ha

ra
cT

E
R

Y
iS

i.2

U
N

I

ib
m

1.
m

or
ph

em
e

ibm1−morpheme

UNI

YiSi−2

CharacTER

sentBLEU

YiSi−0

EED

BEER

chrF

chrF+

YiSi−1

Y
iS

i.1
_s

rl
Y

iS
i.1

E
S

IM
ch

rF
.

ch
rF

B
E

E
R

E
E

D
Y

iS
i.0

C
ha

ra
cT

E
R

se
nt

B
LE

U
Y

iS
i.2

_s
rl

Y
iS

i.2
LA

S
IM

U
N

I
ib

m
1.

m
or

ph
em

e
U

S
F

D
U

S
F

D
.T

L
LP

ib
m

1.
po

s4
gr

am

ibm1−pos4gram
LP
USFD−TL
USFD
ibm1−morpheme
UNI
LASIM
YiSi−2
YiSi−2_srl
sentBLEU
CharacTER
YiSi−0
EED
BEER
chrF
chrF+
ESIM
YiSi−1
YiSi−1_srl

en-fi en-gu en-kk

Y
iS

i.1

ch
rF

.

B
E

E
R

ch
rF

E
S

IM

E
E

D

Y
iS

i.0

C
ha

ra
cT

E
R

se
nt

B
LE

U

U
N

I

Y
iS

i.2

ib
m

1.
m

or
ph

em
e

ibm1−morpheme

YiSi−2

UNI

sentBLEU

CharacTER

YiSi−0

EED

ESIM

chrF

BEER

chrF+

YiSi−1

E
E

D

Y
iS

i.1

Y
iS

i.0

ch
rF

.

B
E

E
R

ch
rF

C
ha

ra
cT

E
R

se
nt

B
LE

U

hL
E

P
O

R
a_

ba
se

lin
e

Y
iS

i.2

YiSi−2

hLEPORa_baseline

sentBLEU

CharacTER

chrF

BEER

chrF+

YiSi−0

YiSi−1

EED

Y
iS

i.1

E
E

D

B
E

E
R

E
S

IM

Y
iS

i.0

ch
rF

.

ch
rF

se
nt

B
LE

U

hL
E

P
O

R
a_

ba
se

lin
e

C
ha

ra
cT

E
R

Y
iS

i.2

YiSi−2

CharacTER

hLEPORa_baseline

sentBLEU

chrF

chrF+

YiSi−0

ESIM

BEER

EED

YiSi−1

en-lt en-ru en-zh

Y
iS

i.1

ch
rF

.

ch
rF

B
E

E
R

E
S

IM

E
E

D

Y
iS

i.0

se
nt

B
LE

U

C
ha

ra
cT

E
R

Y
iS

i.2

ib
m

1.
m

or
ph

em
e

ibm1−morpheme

YiSi−2

CharacTER

sentBLEU

YiSi−0

EED

ESIM

BEER

chrF

chrF+

YiSi−1

Y
iS

i.1
E

S
IM

E
E

D
ch

rF
.

B
E

E
R

ch
rF

Y
iS

i.0
se

nt
B

LE
U

C
ha

ra
cT

E
R

U
N

I
U

N
I.

U
S

F
D

.T
L

U
S

F
D

Y
iS

i.2 LP
LA

S
IM

LASIM
LP
YiSi−2
USFD
USFD−TL
UNI+
UNI
CharacTER
sentBLEU
YiSi−0
chrF
BEER
chrF+
EED
ESIM
YiSi−1

Y
iS

i.1
_s

rl

Y
iS

i.1

E
S

IM

ch
rF

ch
rF

.

se
nt

B
LE

U

Y
iS

i.0

E
E

D

B
E

E
R

C
ha

ra
cT

E
R

Y
iS

i.2

Y
iS

i.2
_s

rl

YiSi−2_srl

YiSi−2

CharacTER

BEER

EED

YiSi−0

sentBLEU

chrF+

chrF

ESIM

YiSi−1

YiSi−1_srl

Figure 3: daRR segment-level metric significance test results for into English and out-of English language
pairs (newstest2019): Green cells denote a significant win for the metric in a given row over the metric
in a given column according bootstrap resampling.



78

de-cs de-fr fr-de

Y
iS

i.1

E
E

D

B
E

E
R

E
S

IM

Y
iS

i.0

ch
rF

.

ch
rF

C
ha

ra
cT

E
R

hL
E

P
O

R
a_

ba
se

lin
e

se
nt

B
LE

U

Y
iS

i.2

ib
m

1.
m

or
ph

em
e

ibm1−morpheme

YiSi−2

sentBLEU

hLEPORa_baseline

CharacTER

chrF

chrF+

YiSi−0

ESIM

BEER

EED

YiSi−1

Y
iS

i.1
E

E
D

Y
iS

i.0
B

E
E

R
E

S
IM

ch
rF

.
ch

rF
C

ha
ra

cT
E

R
hL

E
P

O
R

a_
ba

se
lin

e
se

nt
B

LE
U

Y
iS

i.2
ib

m
1.

m
or

ph
em

e
ib

m
1.

po
s4

gr
am

ibm1−pos4gram
ibm1−morpheme
YiSi−2
sentBLEU
hLEPORa_baseline
CharacTER
chrF
chrF+
ESIM
BEER
YiSi−0
EED
YiSi−1

Y
iS

i.1
Y

iS
i.1

_s
rl

E
S

IM
ch

rF
.

Y
iS

i.0
ch

rF
E

E
D

B
E

E
R

C
ha

ra
cT

E
R

se
nt

B
LE

U
Y

iS
i.2

ib
m

1.
m

or
ph

em
e

ib
m

1.
po

s4
gr

am

ibm1−pos4gram
ibm1−morpheme
YiSi−2
sentBLEU
CharacTER
BEER
EED
chrF
YiSi−0
chrF+
ESIM
YiSi−1_srl
YiSi−1

Figure 4: daRR segment-level metric significance test results for German to Czech, German to French
and French to German (newstest2019): Green cells denote a significant win for the metric in a given row
over the metric in a given column according bootstrap resampling.

• counts metric’s ties as a Discordant pairs.

We employ bootstrap resampling (Koehn,
2004; Graham et al., 2014b) to estimate con-
fidence intervals for our Kendall’s Tau for-
mulation, and metrics with non-overlapping
95% confidence intervals are identified as hav-
ing statistically significant difference in perfor-
mance.

5.2.1 Segment-Level Results
Results of the segment-level human evaluation
for translations sampled from the News Trans-
lation Task are shown in Tables 6, 7 and 8,
where metric correlations not significantly out-
performed by any other metric are highlighted
in bold. Head-to-head significance test results
for differences in metric performance are in-
cluded in Figures 3 and 4.

6 Discussion
This year, human data was collected from
reference-based evaluations (or “monolin-
gual”) and reference-free evaluations (or
“bilingual”). The reference-based (mono-
lingual) evaluations were obtained with the
help of anonymous crowdsourcing, while
the reference-less (bilingual) evaluations were
mainly from MT researchers who committed
their time contribution to the manual evalua-
tion for each submitted system.

6.1 Stability across MT Systems
The observed performance of metrics depends
on the underlying texts and systems that par-
ticipate in the News Translation Task (see Sec-
tion 2). For the strongest MT systems, distin-
guishing which system outputs are better is

468101214161820

sacreBLEU-BLEU

Figure 5: Pearson correlations of sacreBLEU-
BLEU for English-German system-level evalua-
tion for all systems (left) down to only top 4 sys-
tems (right). The y-axis spans from -1 to +1, base-
line metrics for the language pair in grey.

hard, even for human assessors. On the other
hand, if the systems are spread across a wide
performance range, it will be easier for metrics
to correlate with human judgements.

To provide a more reliable view, we created
plots of Pearson correlation when the under-
lying set of MT systems is reduced to top n
ones. One sample such plot is in Figure 5, all
language pairs and most of the metrics are in
Appendix A.

As the plot documents, the official correla-
tions reported in Tables 3 to 5 can lead to
wrong conclusions. sacreBLEU-BLEU cor-
relates at .969 when all systems are considered,
but as we start considering only the top n sys-
tems, the correlation falls relatively quickly.
With 10 systems, we are below .5 and when
only the top 6 or 4 systems are considered,
the correlation falls even to the negave val-
ues. Note that correlations point estimates
(the value in the y-axis) become noiser with
the decreasing number of the underlying MT
systems.

Figure 6 explains the situation and illus-



79

 0.15

 0.2

 0.25

 0.3

 0.35

 0.4

 0.45

 0.5

-2 -1.5 -1 -0.5  0  0.5

Sa
cr

eB
LE

U-
BL

EU

DA

Top 4
Top 6
Top 8

Top 10
Top 12
Top 15

All systems

Figure 6

trates the sensitivity of the observed correla-
tions to the exact set of systems. On the full
set of systems, the single outlier (the worst-
performing system called en_de_task) helps
to achieve a great positive correlation. The
majority of MT systems however form a cloud
with Pearson correlation around .5 and the top
4 systems actually exhibit a negative corre-
lation of the human score and sacreBLEU-
BLEU.

In Appendix A, baseline metrics are plotted
in grey in all the plots, so that their trends can
be observed jointly. In general, most baselines
have similar correlations, as most baselines use
similar features (n-gram or word-level features,
with the exception of chrF). In a number of
language pairs (de-en, de-fr, en-de, en-kk, lt-
en, ru-en, zh-en), baseline correlations tend to-
wards 0 (no correlation) or even negative Pear-
son correlation. For a widely applied metric
such as sacreBLEU-BLEU, our analysis re-
veals weak correlation in comparing top state-
of-the-art systems in these language pairs, es-
pecially in en-de, de-en, ru-en, and zh-en.

We will restrict our analysis to those lan-
guage pairs where the baseline metrics have an
obvious downward trend (de-en, de-fr, en-de,
en-kk, lt-en, ru-en, zh-en). Examining the top-
n correlation in the submitted metrics (not in-
cluding QE systems), most metrics show the
same degredation in correlation as the base-
lines. We note BERTr as the one exception
consistently degrading less and retaining pos-
itive correlation compared to other submitted
metrics and baselines, in the language pairs
where it participated.

For QE systems, we noticed that in some in-
stances, QE systems have upward correlation
trends when other metrics and baselines have
downward trends. For instance, LP, UNI, and
UNI+ in the de-en language pair, YiSi-2 in
en-kk, and UNI and UNI+ in ru-en. These
results suggest that QE systems such as UNI
and UNI+ perform worse on judging systems
of wide ranging quality, but better for top per-
forming systems, or perhaps for systems closer
in quality.

If our method of human assessment is sound,
we should believe that BLEU, a widely ap-
plied metric, is no longer a reliable metric for
judging our best systems. Future investiga-
tions are needed to understand when BLEU
applies well, and why BLEU is not effective
for output from our state of the art models.

Metrics and QE systems such as BERTr,
ESIM, YiSi that perform well at judging
our best systems often use more semantic
features compared to our n-gram/char-gram
based baselines. Future metrics may want to
explore a) whether semantic features such as
contextual word embeddings are achieving se-
mantic understanding and b) whether seman-
tic understanding is the true source of a met-
ric’s performance gains.

It should be noted that some language pairs
do not show the strong degrading pattern with
top-n systems this year, for instance en-cs, en-
gu, en-ru, or kk-en. English-Chinese is partic-
ularly interesting because we see a clear trend
towards better correlations as we reduce the
set of underlying systems to the top scoring
ones.

6.2 Overall Metric Performance
6.2.1 System-Level Evaluation
In system-level evaluation, the series of YiSi
metrics achieve the highest correlations in sev-
eral language pairs and it is not significantly
outperformed by any other metrics (denoted
as a “win” in the following) for almost all lan-
guage pairs.

The new metric ESIM performs best on 5
language languages (18 language pairs) and
obtains 11 “wins” out of 16 language pairs in
which ESIM participated.

The metric EED performs better for lan-
guage pairs out-of English and excluding En-



80

glish compared to into-English language pairs,
achieving 7 out of 11 “wins” there.

6.2.2 Segment-Level Evaluation

For segment-level evaluation, most language
pairs are quite discerning, with only one or
two metrics taking the “winner” position (of
not being significantly surpassed by others).
Only French-German differs, with all metrics
performing similarly except the significantly
worse sentBLEU.

YiSi-1_srl stands out as the “winner” for
all language pairs in which it participated.
The excluded language pairs were probably
due to the lack of semantic information re-
quired by YiSi-1_srl. YiSi-1 participated
all language pairs and its correlations are com-
parable with those of YiSi-1_srl.

ESIM obtain 6 “winners” out of all 18 lan-
guages pairs.

Both YiSi and ESIM are based on neu-
ral networks (YiSi via word and phrase em-
beddings, as well as other types of available
resources, ESIM via sentence embeddings).
This is a confirmation of a trend observed last
year.

6.2.3 QE Systems as Metrics

Generally, correlations for the standard
reference-based metrics are obviously better
than those in “QE as a Metric” track, both
when using monolingual and bilingual golden
truth.

In system-level evaluation, correlations for
“QE as a Metric” range from 0.028 to 0.947
across all language pairs and all metrics but
they are very unstable. Even for a single
metric, take UNI for example, the correla-
tions range from 0.028 to 0.930 across language
pairs.

In segment-level evaluation, correlations for
QE metrics range from -0.153 to 0.351 across
all language pairs and show the same instabil-
ity across language pairs for a given metric.

In either case, we do not see any pattern
that could explain the behaviour, e.g. whether
the manual evaluation was monolingual or
bilingual, or the characteristics of the given
language pair.

6.3 Dependence on Implementation
As it already happened in the past, we had
multiple implementations for some metrics,
BLEU and chrF in particular.

The detailed configuration of BLEU and
sacreBLEU-BLEU differ and hence their
scores and correlation results are different.

chrF and sacreBLEU-chrF use the same
parameters and should thus deliver the same
scores but we still observe some differences,
leading to different correlations. For instance
for German-French Pearson correlation, chrF
obtains 0.931 (no win) but sacreBLEU-
chrF reaches 0.952, tying for a win with other
metrics.

We thus fully support the call for clarity by
Post (2018b) and invite authors of metrics to
include their implementations either in Moses
scorer or sacreBLEU to achieve a long-term
assessment of their metric.

7 Conclusion

This paper summarizes the results of WMT19
shared task in machine translation evaluation,
the Metrics Shared Task. Participating met-
rics were evaluated in terms of their correla-
tion with human judgement at the level of
the whole test set (system-level evaluation),
as well as at the level of individual sentences
(segment-level evaluation).

We reported scores for standard metrics re-
quiring the reference as well as quality estima-
tion systems which took part in the track “QE
as a metric”, joint with the Quality Estimation
task.

For system-level, best metrics reach over
0.95 Pearson correlation or better across sev-
eral language pairs. As expected, QE sys-
tems are visibly in all language pairs but they
can also reach high system-level correlations,
up to .947 (Chinese-English) or .936 (English-
German) by YiSi-1_srl or over .9 for multi-
ple language pairs by UNI.

An important caveat is that the correlations
are heavily affected by the underlying set of
MT systems. We explored this by reducing
the set of systems to top-n ones for various ns
and found out that for many language pairs,
system-level correlations are much worse when
based on only the better performing systems.
With both good and bad MT systems partic-



81

ipating in the news task, the metrics results
can be overly optimistic compared to what we
get when evaluating state-of-the-art systems.

In terms of segment-level Kendall’s τ re-
sults, the standard metrics correlations varied
between 0.03 and 0.59, and QE systems ob-
tained even negative correlations.

The results confirm the observation from the
last year, namely metrics based on word or
sentence-level embeddings (YiSi and ESIM),
achieve the highest performance.

Acknowledgments
Results in this shared task would not be pos-
sible without tight collaboration with organiz-
ers of the WMT News Translation Task. We
would like to thank Marcin Junczys-Dowmunt
for the suggestion to examine metrics perfor-
mance across varying subsets of MT systems,
as we did in Appendix A.

This study was supported in parts by the
grants 19-26934X (NEUREM3) of the Czech
Science Foundation, ADAPT Centre for Dig-
ital Content Technology (www.adaptcentre.
ie) at Dublin City University funded un-
der the SFI Research Centres Programme
(Grant 13/RC/2106) co-funded under the
European Regional Development Fund, and
Charles University Research Programme “Pro-
gres” Q18+Q48.

References
Colin Bannard and Chris Callison-Burch. 2005.

Paraphrasing with bilingual parallel corpora. In
Proceedings of the 43rd Annual Meeting on Asso-
ciation for Computational Linguistics, ACL ’05,
pages 597–604, Stroudsburg, PA, USA. Associ-
ation for Computational Linguistics.

Loïc Barrault, Ondřej Bojar, Marta R. Costa-
jussà, Christian Federmann, Mark Fishel,
Yvette Graham, Barry Haddow, Matthias Huck,
Philipp Koehn, Shervin Malmasi, Christof
Monz, Mathias Müller, Santanu Pal, Matt
Post, and Marcos Zampieri. 2019. Findings of
the 2019 Conference on Machine Translation
(WMT19). In Proceedings of the Fourth Con-
ference on Machine Translation, Florence, Italy.
Association for Computational Linguistics.

Ondřej Bojar, Christian Federmann, Barry Had-
dow, Philipp Koehn, Matt Post, and Lucia Spe-
cia. 2016. Ten Years of WMT Evaluation Cam-
paigns: Lessons Learnt. In Proceedings of the
LREC 2016 Workshop “Translation Evaluation

– From Fragmented Tools and Data Sets to an
Integrated Ecosystem”, pages 27–34, Portorose,
Slovenia.

Ondřej Bojar, Yvette Graham, and Amir Kamran.
2017. Results of the WMT17 metrics shared
task. In Proceedings of the Second Confer-
ence on Machine Translation, Volume 2: Shared
Tasks Papers, Copenhagen, Denmark. Associa-
tion for Computational Linguistics.

Qian Chen, Xiaodan Zhu, Zhen-Hua Ling, Si Wei,
Hui Jiang, and Diana Inkpen. 2017. Enhanced
lstm for natural language inference. In Proceed-
ings of the 55th Annual Meeting of the Associ-
ation for Computational Linguistics (Volume 1:
Long Papers), pages 1657–1668.

Julian Chow, Pranava Madhyastha, and Lucia
Specia. 2019a. Wmdo: Fluency-based word
mover’s distance for machine translation eval-
uation. In Proceedings of Fourth Conference on
Machine Translation.

Julian Chow, Lucia Specia, and Pranava Mad-
hyastha. 2019b. WMDO: Fluency-based Word
Mover’s Distance for Machine Translation Eval-
uation. In Proceedings of the Fourth Conference
on Machine Translation, Florence, Italy. Asso-
ciation for Computational Linguistics.

Michael Denkowski and Alon Lavie. 2014. Meteor
Universal: Language Specific Translation Evalu-
ation for Any Target Language. In Proceedings
of the Ninth Workshop on Statistical Machine
Translation, pages 376–380, Baltimore, Mary-
land, USA. Association for Computational Lin-
guistics.

Jacob Devlin, Ming-Wei Chang, Kenton Lee, and
Kristina Toutanova. 2019. BERT: Pre-training
of deep bidirectional transformers for language
understanding. In Proceedings of the 2019 Con-
ference of the North American Chapter of the
Association for Computational Linguistics: Hu-
man Language Technologies, Volume 1 (Long
and Short Papers), pages 4171–4186, Minneapo-
lis, Minnesota. Association for Computational
Linguistics.

George Doddington. 2002. Automatic Evalua-
tion of Machine Translation Quality Using N-
gram Co-occurrence Statistics. In Proceedings
of the Second International Conference on Hu-
man Language Technology Research, HLT ’02,
pages 138–145, San Francisco, CA, USA. Mor-
gan Kaufmann Publishers Inc.

William B. Dolan and Chris Brockett. 2005. Au-
tomatically constructing a corpus of sentential
paraphrases. In Proceedings of the Third Inter-
national Workshop on Paraphrasing (IWP2005).

Erick Fonseca, Lisa Yankovskaya, André F. T.
Martins, Mark Fishel, and Christian Feder-
mann. 2019. Findings of the WMT 2019 Shared

www.adaptcentre.ie
www.adaptcentre.ie
https://doi.org/10.3115/1219840.1219914
http://www.aclweb.org/anthology/W/W14/W14-3348
http://www.aclweb.org/anthology/W/W14/W14-3348
http://www.aclweb.org/anthology/W/W14/W14-3348
https://www.aclweb.org/anthology/N19-1423
https://www.aclweb.org/anthology/N19-1423
https://www.aclweb.org/anthology/N19-1423
http://dl.acm.org/citation.cfm?id=1289189.1289273
http://dl.acm.org/citation.cfm?id=1289189.1289273
http://dl.acm.org/citation.cfm?id=1289189.1289273
https://www.aclweb.org/anthology/I05-5002
https://www.aclweb.org/anthology/I05-5002
https://www.aclweb.org/anthology/I05-5002


82

Task on Quality Estimation. In Proceedings of
the Fourth Conference on Machine Translation,
Florence, Italy. Association for Computational
Linguistics.

Yvette Graham and Timothy Baldwin. 2014. Test-
ing for Significance of Increased Correlation with
Human Judgment. In Proceedings of the 2014
Conference on Empirical Methods in Natural
Language Processing (EMNLP), pages 172–176,
Doha, Qatar. Association for Computational
Linguistics.

Yvette Graham, Timothy Baldwin, Alistair Mof-
fat, and Justin Zobel. 2013. Continuous Mea-
surement Scales in Human Evaluation of Ma-
chine Translation. In Proceedings of the 7th Lin-
guistic Annotation Workshop & Interoperability
with Discourse, pages 33–41, Sofia, Bulgaria. As-
sociation for Computational Linguistics.

Yvette Graham, Timothy Baldwin, Alistair Mof-
fat, and Justin Zobel. 2014a. Is Machine Trans-
lation Getting Better over Time? In Proceed-
ings of the 14th Conference of the European
Chapter of the Association for Computational
Linguistics, pages 443–451, Gothenburg, Swe-
den. Association for Computational Linguistics.

Yvette Graham, Timothy Baldwin, Alistair Mof-
fat, and Justin Zobel. 2016. Can machine trans-
lation systems be evaluated by the crowd alone.
Natural Language Engineering, FirstView:1–28.

Yvette Graham and Qun Liu. 2016. Achieving Ac-
curate Conclusions in Evaluation of Automatic
Machine Translation Metrics. In Proceedings of
the 15th Annual Conference of the North Amer-
ican Chapter of the Association for Computa-
tional Linguistics: Human Language Technolo-
gies, San Diego, CA. Association for Computa-
tional Linguistics.

Yvette Graham, Nitika Mathur, and Timothy
Baldwin. 2014b. Randomized significance tests
in machine translation. In Proceedings of the
ACL 2014 Ninth Workshop on Statistical Ma-
chine Translation, pages 266–274. Association
for Computational Linguistics.

Yvette Graham, Nitika Mathur, and Timo-
thy Baldwin. 2015. Accurate Evaluation of
Segment-level Machine Translation Metrics. In
Proceedings of the 2015 Conference of the North
American Chapter of the Association for Com-
putational Linguistics Human Language Tech-
nologies, Denver, Colorado.

Yinuo Guo and Junfeng Hu. 2019. Meteor++
2.0: Adopt Syntactic Level Paraphrase Knowl-
edge into Machine Translation Evaluation. In
Proceedings of the Fourth Conference on Ma-
chine Translation, Florence, Italy. Association
for Computational Linguistics.

Aaron L.-F. Han, Derek F. Wong, and Lidia S.
Chao. 2012. Lepor: A robust evaluation metric
for machine translation with augmented factors.
In Proceedings of the 24th International Con-
ference on Computational Linguistics (COLING
2012), pages 441–450. Association for Computa-
tional Linguistics.

Aaron L.-F. Han, Derek F. Wong, Lidia S. Chao,
Liangye He, Yi Lu, Junwen Xing, and Xiaodong
Zeng. 2013. Language-independent model for
machine translation evaluation with reinforced
factors. In Machine Translation Summit XIV,
pages 215–222. International Association for
Machine Translation.

Philipp Koehn. 2004. Statistical significance tests
for machine translation evaluation. In Proc. of
Empirical Methods in Natural Language Process-
ing, pages 388–395, Barcelona, Spain. Associa-
tion for Computational Linguistics.

Philipp Koehn and Christof Monz. 2006. Manual
and Automatic Evaluation of Machine Trans-
lation Between European Languages. In Pro-
ceedings of the Workshop on Statistical Ma-
chine Translation, StatMT ’06, pages 102–121,
Stroudsburg, PA, USA. Association for Compu-
tational Linguistics.

Gregor Leusch, Nicola Ueffing, and Hermann Ney.
2003. A novel string-to-string distance measure
with applications to machine translation evalu-
ation. In Proceedings of Mt Summit IX, pages
240–247.

Gregor Leusch, Nicola Ueffing, and Hermann Ney.
2006. CDER: Efficient MT Evaluation Using
Block Movements. In In Proceedings of EACL,
pages 241–248.

Chi-kiu Lo. 2019. YiSi - a Unified Semantic MT
Quality Evaluation and Estimation Metric for
Languages with Different Levels of Available Re-
sources. In Proceedings of the Fourth Conference
on Machine Translation, Florence, Italy. Asso-
ciation for Computational Linguistics.

Qingsong Ma, Ondřej Bojar, and Yvette Graham.
2018. Results of the WMT18 metrics shared
task: Both characters and embeddings achieve
good performance. In Proceedings of the Third
Conference on Machine Translation, Volume 2:
Shared Task Papers, Brussels, Belgium. Associ-
ation for Computational Linguistics.

Matouš Macháček and Ondřej Bojar. 2014. Re-
sults of the WMT14 metrics shared task. In
Proceedings of the Ninth Workshop on Statisti-
cal Machine Translation, pages 293–301, Balti-
more, MD, USA. Association for Computational
Linguistics.

Matouš Macháček and Ondřej Bojar. 2013. Results
of the WMT13 Metrics Shared Task. In Proceed-

http://www.aclweb.org/anthology/D14-1020
http://www.aclweb.org/anthology/D14-1020
http://www.aclweb.org/anthology/D14-1020
http://www.aclweb.org/anthology/E14-1047
http://www.aclweb.org/anthology/E14-1047
https://doi.org/10.1017/S1351324915000339
https://doi.org/10.1017/S1351324915000339
http://dl.acm.org/citation.cfm?id=1654650.1654666
http://dl.acm.org/citation.cfm?id=1654650.1654666
http://dl.acm.org/citation.cfm?id=1654650.1654666
http://www.aclweb.org/anthology/W13-2202
http://www.aclweb.org/anthology/W13-2202


83

ings of the Eighth Workshop on Statistical Ma-
chine Translation, pages 45–51, Sofia, Bulgaria.
Association for Computational Linguistics.

Nitika Mathur, Tim Baldwin, and Trevor Cohn.
2019. Putting evaluation in context: Contextual
embeddings improve machine translation evalu-
ation. In Proc. of ACL (short papers). To ap-
pear.

Kishore Papineni, Salim Roukos, Todd Ward, and
Wei-Jing Zhu. 2002. BLEU: A Method for Au-
tomatic Evaluation of Machine Translation. In
Proceedings of the 40th Annual Meeting on Asso-
ciation for Computational Linguistics, ACL ’02,
pages 311–318.

Maja Popovic. 2012. Morpheme- and POS-based
IBM1 and language model scores for translation
quality estimation. In Proceedings of the Sev-
enth Workshop on Statistical Machine Trans-
lation, WMT@NAACL-HLT 2012, June 7-8,
2012, Montréal, Canada, pages 133–137.

Maja Popović. 2015. chrF: character n-gram F-
score for automatic MT evaluation. In Proceed-
ings of the Tenth Workshop on Statistical Ma-
chine Translation, Lisboa, Portugal. Association
for Computational Linguistics.

Maja Popović. 2017. chrF++: words helping char-
acter n-grams. In Proceedings of the Second
Conference on Machine Translation, Volume 2:
Shared Tasks Papers, Copenhagen, Denmark.
Association for Computational Linguistics.

Matt Post. 2018a. A call for clarity in reporting
BLEU scores. In Proceedings of the Third Con-
ference on Machine Translation: Research Pa-
pers, pages 186–191, Belgium, Brussels. Associ-
ation for Computational Linguistics.

Matt Post. 2018b. A call for clarity in reporting
bleu scores. In Proceedings of the Third Confer-
ence on Machine Translation, Belgium, Brus-
sels. Association for Computational Linguistics.

Matthew Snover, Bonnie Dorr, Richard Schwartz,
Linnea Micciulla, and John Makhoul. 2006. A
study of translation edit rate with targeted hu-
man annotation. In In Proceedings of Associa-
tion for Machine Translation in the Americas,
pages 223–231.

Peter Stanchev, Weiyue Wang, and Hermann Ney.
2019. EED: Extended Edit Distance Measure
for Machine Translation. In Proceedings of the
Fourth Conference on Machine Translation, Flo-
rence, Italy. Association for Computational Lin-
guistics.

Miloš Stanojević and Khalil Sima’an. 2015. BEER
1.1: ILLC UvA submission to metrics and tun-
ing task. In Proceedings of the Tenth Workshop
on Statistical Machine Translation, Lisboa, Por-
tugal. Association for Computational Linguis-
tics.

Weiyue Wang, Jan-Thorsten Peter, Hendrik
Rosendahl, and Hermann Ney. 2016a. Charac-
ter: Translation edit rate on character level. In
ACL 2016 First Conference on Machine Trans-
lation, pages 505–510, Berlin, Germany.

Weiyue Wang, Jan-Thorsten Peter, Hendrik
Rosendahl, and Hermann Ney. 2016b. Charac-
Ter: Translation Edit Rate on Character Level.
In Proceedings of the First Conference on Ma-
chine Translation, Berlin, Germany. Association
for Computational Linguistics.

Evan James Williams. 1959. Regression analysis,
volume 14. Wiley New York.

Elizaveta Yankovskaya, Andre Tättar, and Mark
Fishel. 2019. Quality Estimation and Transla-
tion Metrics via Pre-trained Word and Sentence
Embeddings. In Proceedings of the Fourth Con-
ference on Machine Translation, Florence, Italy.
Association for Computational Linguistics.

Ryoma Yoshimura, Hiroki Shimanaka, Yukio Mat-
sumura, Hayahide Yamagishi, and Mamoru Ko-
machi. 2019. Filtering Pseudo-References by
Paraphrasing for Automatic Evaluation of Ma-
chine Translation. In Proceedings of the Fourth
Conference on Machine Translation, Florence,
Italy. Association for Computational Linguis-
tics.

http://aclweb.org/anthology/W/W12/W12-3116.pdf
http://aclweb.org/anthology/W/W12/W12-3116.pdf
http://aclweb.org/anthology/W/W12/W12-3116.pdf
http://aclweb.org/anthology/W15-3049
http://aclweb.org/anthology/W15-3049
https://www.aclweb.org/anthology/W18-6319
https://www.aclweb.org/anthology/W18-6319
http://aclweb.org/anthology/W15-3050
http://aclweb.org/anthology/W15-3050
http://aclweb.org/anthology/W15-3050


84

A Correlations for Top-N Systems

A.1 de-cs

1

0

1 BEER CharacTER EED ESIM LEPORa

1

0

1 LEPORb NIST PER TER YiSi-0

46891

0

1 YiSi-1

4689

YiSi-2

4689

ibm1-morpheme

4689

sacreBLEU-BLEU

4689

sacreBLEU-chrF

A.2 de-en

1

0

1 BEER BERTr CharacTER EED ESIM

1

0

1 LASIM LP Meteor++_2.0 Meteor++_2.0(+copy)NIST

1

0

1 PER PReP TER UNI UNI+

1

0

1 WMDO YiSi-0 YiSi-1 YiSi-1_srl YiSi-2

4681012141

0

1 YiSi-2_srl

468101214

ibm1-morpheme

468101214

ibm1-pos4gram

468101214

sacreBLEU-BLEU

468101214

sacreBLEU-chrF

A.3 de-fr

1

0

1 BEER CharacTER EED ESIM LEPORa

1

0

1 LEPORb NIST PER TER YiSi-0

1

0

1 YiSi-1

4689

YiSi-2

4689

ibm1-morpheme

4689

ibm1-pos4gram

4689

sacreBLEU-BLEU

46891

0

1 sacreBLEU-chrF



85

A.4 en-cs

1

0

1 BEER CharacTER EED NIST PER

1

0

1 TER UNI YiSi-0

4689

YiSi-1

4689

YiSi-2

46891

0

1 ibm1-morpheme

4689

sacreBLEU-BLEU

4689

sacreBLEU-chrF

A.5 en-de

1

0

1 BEER CharacTER EED ESIM LASIM

1

0

1 LP NIST PER TER UNI

1

0

1 USFD USFD-TL YiSi-0 YiSi-1 YiSi-1_srl

1

0

1 YiSi-2

468101214161820

YiSi-2_srl

468101214161820

ibm1-morpheme

468101214161820

ibm1-pos4gram

468101214161820

sacreBLEU-BLEU

4681012141618201

0

1 sacreBLEU-chrF

A.6 en-fi

1

0

1 BEER CharacTER EED ESIM NIST

1

0

1 PER TER UNI YiSi-0

46810

YiSi-1

468101

0

1 YiSi-2

46810

ibm1-morpheme

46810

sacreBLEU-BLEU

46810

sacreBLEU-chrF



86

A.7 en-gu

1

0

1 BEER CharacTER EED LEPORa LEPORb

1

0

1 NIST PER TER

4689

YiSi-0

4689

YiSi-1

46891

0

1 YiSi-2

4689

sacreBLEU-BLEU

4689

sacreBLEU-chrF

A.8 en-kk

1

0

1 BEER CharacTER EED ESIM LEPORa

1

0

1 LEPORb NIST PER TER

4689

YiSi-0

46891

0

1 YiSi-1

4689

YiSi-2

4689

sacreBLEU-BLEU

4689

sacreBLEU-chrF

A.9 en-lt

1

0

1 BEER CharacTER EED ESIM LEPORb

1

0

1 NIST PER TER YiSi-0

46810

YiSi-1

468101

0

1 YiSi-2

46810

ibm1-morpheme

46810

sacreBLEU-BLEU

46810

sacreBLEU-chrF



87

A.10 en-ru

1

0

1 BEER CharacTER EED ESIM LASIM

1

0

1 LP NIST PER TER UNI

1

0

1 UNI+ USFD USFD-TL

46810

YiSi-0

46810

YiSi-1

468101

0

1 YiSi-2

46810

sacreBLEU-BLEU

46810

sacreBLEU-chrF

A.11 en-zh

1

0

1 BEER CharacTER EED ESIM NIST

1

0

1 PER TER YiSi-0 YiSi-1

46810

YiSi-1_srl

468101

0

1 YiSi-2

46810

YiSi-2_srl

46810

sacreBLEU-BLEU

46810

sacreBLEU-chrF

A.12 fi-en

1

0

1 BEER BERTr CharacTER EED ESIM

1

0

1 Meteor++_2.0 Meteor++_2.0(+copy)NIST PER PReP

1

0

1 TER UNI UNI+ WMDO YiSi-0

1

0

1 YiSi-1

46810

YiSi-1_srl

46810

YiSi-2

46810

ibm1-morpheme

46810

sacreBLEU-BLEU

468101

0

1 sacreBLEU-chrF



88

A.13 fr-de

1

0

1 BEER CharacTER EED ESIM NIST

1

0

1 PER TER YiSi-0 YiSi-1 YiSi-1_srl

4681

0

1 YiSi-2

468

ibm1-morpheme

468

ibm1-pos4gram

468

sacreBLEU-BLEU

468

sacreBLEU-chrF

A.14 gu-en

1

0

1 BEER BERTr CharacTER EED ESIM

1

0

1 Meteor++_2.0 Meteor++_2.0(+copy)NIST PER PReP

1

0

1 TER WMDO YiSi-0

4689

YiSi-1

4689

YiSi-1_srl

46891

0

1 YiSi-2

4689

sacreBLEU-BLEU

4689

sacreBLEU-chrF

A.15 kk-en

1

0

1 BEER BERTr CharacTER EED ESIM

1

0

1 LEPORa LEPORb Meteor++_2.0 Meteor++_2.0(+copy)NIST

1

0

1 PER PReP TER WMDO YiSi-0

46891

0

1 YiSi-1

4689

YiSi-1_srl

4689

YiSi-2

4689

sacreBLEU-BLEU

4689

sacreBLEU-chrF



89

A.16 lt-en

1

0

1 BEER BERTr CharacTER EED ESIM

1

0

1 LEPORb Meteor++_2.0 Meteor++_2.0(+copy)NIST PER

1

0

1 PReP TER WMDO YiSi-0 YiSi-1

46891

0

1 YiSi-1_srl

4689

YiSi-2

4689

ibm1-morpheme

4689

sacreBLEU-BLEU

4689

sacreBLEU-chrF

A.17 ru-en

1

0

1 BEER BERTr CharacTER EED ESIM

1

0

1 LASIM LP Meteor++_2.0 Meteor++_2.0(+copy)NIST

1

0

1 PER PReP TER UNI UNI+

1

0

1 WMDO YiSi-0

4681012

YiSi-1

4681012

YiSi-1_srl

4681012

YiSi-2

46810121

0

1 sacreBLEU-BLEU

4681012

sacreBLEU-chrF



90

A.18 zh-en

1

0

1 BEER BERTr CharacTER EED ESIM

1

0

1 LEPORa LEPORb Meteor++_2.0 Meteor++_2.0(+copy)NIST

1

0

1 PER PReP TER WMDO YiSi-0

1

0

1 YiSi-1

468101213

YiSi-1_srl

468101213

YiSi-2

468101213

YiSi-2_srl

468101213

sacreBLEU-BLEU

4681012131

0

1 sacreBLEU-chrF


