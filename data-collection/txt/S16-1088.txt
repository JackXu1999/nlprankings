



















































UTA DLNLP at SemEval-2016 Task 1: Semantic Textual Similarity: A Unified Framework for Semantic Processing and Evaluation


Proceedings of SemEval-2016, pages 584–587,
San Diego, California, June 16-17, 2016. c©2016 Association for Computational Linguistics

UTA DLNLP at SemEval-2016 Task 1: Semantic Textual Similarity: A
Unified Framework for Semantic Processing and Evaluation

Peng Li
Computer Science and Engineering

University of Texas at Arlington
jerryli1981@gmail.com

Heng Huang∗
Computer Science and Engineering

University of Texas at Arlington
heng@uta.edu

Abstract

In this paper, we propose a deep neural net-
work based natural language processing sys-
tem for semantic textual similarity prediction.
We leverage multi-layer bidirectional LSTM
to learn sentence representation. After that, we
construct matching features followed by High-
way Multilayer Perceptron to make predic-
tions. Experimental results demonstrate that
this approach can’t get better results on stan-
dard evaluation datasets.

1 Introduction

Traditional approaches (Lai and Hockenmaier,
2014; Zhao et al., 2014; Jimenez et al., 2014) for
semantic textual similarity prediction usually build
the supervised model using a variety of hand crafted
features. Hundreds of features generated at differ-
ent linguistic levels are exploited to boost classifica-
tion. With the success of deep learning in many ma-
chine learning related applications, there has been
much interest in applying deep neural network based
techniques to further improve the prediction tasks
in natural language processing (NLP) (Socher et al.,
2011b; Iyyer et al., 2014; Tai et al., 2015).

A key component of deep neural network is word
embeddings which serves as a lookup table to get
word representations. From low-level NLP tasks
such as language modeling, POS tagging, name en-
tity recognition, and semantic role labeling, to high-

∗To whom all correspondence should be addressed. This
work was partially supported by NSF-IIS 1117965, NSF-IIS
1302675, NSF-IIS 1344152, NSF-DBI 1356628, NIH R01
AG049371.

level tasks such as machine translation, informa-
tion retrieval and semantic analysis (Kalchbrenner
and Blunsom, 2013; Socher et al., 2011a; Tai et
al., 2015). Deep word representation learning has
demonstrated its importance for these tasks. All
the tasks get performance improvement via further
learning either word level representations or sen-
tence level representations.

In this work, we focus on deep neural network
based semantic textual similarity prediction. We use
multi-layer bidirectional LSTM (Long Short Term
Memory) (Graves et al., 2013) to learn sentence rep-
resentations. After that, we construct matching fea-
tures followed by Highway Multilayer Perceptron to
learn high-level hidden matching feature representa-
tions. Below, we will briefly introduce Multi-Layer
Bidirectional LSTM.

2 Multi-Layer Bidirectional LSTM

2.1 RNN vs LSTM

Recurrent neural networks (RNNs) are capable of
modeling sequences of varying lengths via the re-
cursive application of a transition function on a hid-
den state. For example, at each time step t, an RNN
takes the input vector xt ∈ Rn and the hidden state
vector ht−1 ∈ Rm, then applies affine transforma-
tion followed by an element-wise nonlinearity such
as hyperbolic tangent function to produce the next
hidden state vector ht:

ht = tanh(Wxt +Uht−1 + b) . (1)

A major issue of RNNs using these transition
functions is that it is difficult to learn long-range de-

584



pendencies during training step because the compo-
nents of the gradient vector can grow or decay expo-
nentially (Bengio et al., 1994).

The LSTM architecture (Hochreiter and Schmid-
huber, 1998) addresses the problem of learning long
range dependencies by introducing a memory cell
that is able to preserve state over long periods of
time. Concretely, at each time step t, the LSTM unit
can be defined as a collection of vectors in Rd: an
input gate it, a forget gate ft, an output gate ot, a
memory cell ct and a hidden state ht. We refer to d as
the memory dimensionality of the LSTM. One step
of an LSTM takes as input xt, ht−1, ct−1 and pro-
duces ht, ct via the following transition equations:

it = σ(W
(i)xt +U

(i)ht−1 + b(i)) ,

ft = σ(W
(f)xt +U

(f)ht−1 + b(f)) ,

ot = σ(W
(o)xt +U

(o)ht−1 + b(o)) ,

ut = tanh(W
(u)xt +U

(u)ht−1 + b(u)) ,

ct = it � ut + ft � ct−1 ,
ht = ot � tanh(ct) ,

(2)

where σ(·) and tanh(·) are the element-wise sig-
moid and hyperbolic tangent functions, � is the
element-wise multiplication operator.

2.2 Model Description
One shortcoming of conventional RNNs is that they
are only able to make use of previous context. In se-
mantic text similarity prediction task, the decision
is made after the whole sentence pair is digested.
Therefore, exploring future context would be better
for sequence meaning representation. Bidirectional
RNNs architecture (Graves et al., 2013) proposed a
solution of making prediction based on future words.
At each time step t, the model maintains two hid-
den states, one for the left-to-right propagation

−→
ht

and the other for the right-to-left propagation
←−
ht.

The hidden state of the Bidirectional LSTM is the
concatenation of the forward and backward hidden
states. The following equations illustrate the main
ideas:

−→
ht = tanh(

−→
Wxt +

−→
U
−→
h t−1 +

−→
b )

←−
ht = tanh(

←−
Wxt +

←−
U
←−
h t+1 +

←−
b ) ,

(3)

Deep RNNs can be created by stacking multiple
RNN hidden layer on top of each other, with the

output sequence of one layer forming the input se-
quence for the next. Assuming the same hidden layer
function is used for allN layers in the stack, the hid-
den vectors hn are iteratively computed from n = 1
to N and t = 1 to T :

hnt = tanh(Wh
n−1
t +Uh

n
t−1 + b) . (4)

Multilayer bidirectional RNNs can be imple-
mented by replacing each hidden vector hn with the
forward and backward vectors

−→
hn and

←−
hn, and en-

suring that every hidden layer receives input from
both the forward and backward layers at the level
below. Furthermore, we can apply LSTM memory
cell to hidden layers to construct multilayer bidirec-
tional LSTM.

Finally, we can concatenate sequence hidden ma-
trix
−→
M ∈ Rn×d and reversed sequence hidden ma-

trix
←−
M ∈ Rn×d to form the sentence representation.

Here n is the number of layers, d is the memory di-
mensionality of the LSTM. In the next section, we
will use the two matrices to generate matching fea-
ture planes via linear algebra operations.

3 Learning from Matching Features

Inspired by (Tai et al., 2015), we apply element-wise
merge to first sentence matrix M1 ∈ Rn×2d and sec-
ond sentence matrix M2 ∈ Rn×2d. Similar to pre-
vious method, we can define two simple matching
feature planes (FPs) with below equations:

FP1 =M1 �M2 ,
FP2 = |M1 −M2| ,

(5)

where � is the element-wise multiplication. The
FP1 measure can be interpreted as an element-wise
comparison of the signs of the input representations.
The FP2 measure can be interpreted as the distance
between the input representations.

3.1 Highway MLP

Inspired by (Kim et al., 2016), we build Highway
Multilayer Perceptron (HMLP) layer to further en-
hance representation learning. Conventional MLP
applies an affine transformation followed by a non-
linearity to obtain a new set of features:

z = g(Wy + b) . (6)

585



One layer of a highway network does the following:

z = t� g(WHy + bH) + (1− t)� y , (7)

where g is a nonlinearity, t = σ(WTy + bT ) is
called as the transform gate, and (1− t) is called as
the carry gate. Similar to the memory cells in LSTM
networks, highway layers allow adaptively carrying
some dimensions of the input directly to the input
for training deep networks.

4 Experiments

We use all previous dataset to train our LSTM classi-
fier. The total number of training examples is 12912,
and the number of dev examples is 680. Note that we
didn’t use cross validation to find the best model. Ta-
ble 1 shows the Pearson correlation results of STS
task.

4.1 Hyperparameters and Training Details
We first initialize our word representations using
publicly available 300-dimensional Glove word vec-
tors 1. LSTM memory dimension is 100, the number
of layers is 2. Training is done through stochastic
gradient descent over shuffled mini-batches with the
AdaGrad update rule (Duchi et al., 2011). The learn-
ing rate is set to 0.05. The mini-batch size is 25.
The model parameters were regularized with a per-
minibatch L2 regularization strength of 10−4. Note
that word embeddings were fixed during training.

4.2 Objective Functions
The task of semantic similarity prediction tries to
measure the degree of semantic similarity of a sen-
tence pair by assigning a similarity score ranging
from 1 (completely unrelated) to 5 (semantically
equivalent). Inspired by (Tai et al., 2015), given a
sentence pair, we wish to predict a real-valued sim-
ilarity score in a range of [1,K], where K > 1
is an integer. The sequence 1, 2, ...,K is the ordi-
nal scale of similarity, where higher scores indicate
greater degrees of similarity. We can predict the sim-
ilarity score ŷ by predicting the probability that the
learned hidden representation xh belongs to the or-
dinal scale. This is done by projecting an input rep-
resentation onto a set of hyperplanes, each of which
corresponds to a class. The distance from the input

1http://nlp.stanford.edu/projects/glove/

to a hyperplane reflects the probability that the input
will located in corresponding scale.

Mathematically, the similarity score ŷ can be writ-
ten as:

ŷ = rT · p̂θ(y|xh)
= rT · softmax(W · xh + b)

= rT · e
Wixh+bi

∑
j e

Wjxh+bj
(8)

where rT = [1 2 . . .K] and the weight matrix W
and b are parameters.

In order to introduce the task objective function,
we define a sparse target distribution p that satisfies
y = rT p:

pi =





y − byc, i = byc+ 1
byc − y + 1, i = byc
0 otherwise

(9)

where 1 ≤ i ≤ K. The objective function then
can be defined as the regularized KL-divergence be-
tween p and pθ:

J(θ) = − 1
m

m∑

k=1

KL(p(k)||pkθ) +
λ

2
||θ||22 , (10)

where m is the number of training pairs and the su-
perscript k indicates the k-th sentence pair (Tai et
al., 2015).

5 Conclusions and Discussions

In this paper, we propose a deep neural network ar-
chitecture that leverages pre-trained word embed-
dings to learn sentence meanings. Our approach
first generates word sequence representations as in-
puts into a multilayer bidirectional LSTM to learn
sentence representations. After that, we construct
matching features followed by highway MLP to
learn high-level hidden matching feature represen-
tations. Experimental results on benchmark datasets
demonstrate that our model didn’t achieved the
state-of-the-art performance compared with other
approaches. Our approach is above the median
scores only on question-question domain. We sus-
pect our model have worse capability of domain
adaption. Also the Highway MLP may increase the
model complexity and lead to worse performance.

586



Method All answer-
answer

headlines plagiarism postediting question-
question

Our Run: 100-1 0.64965 0.46391 0.74499 0.74003 0.71947 0.58083
Our Run: 150-1 0.64500 0.43042 0.72133 0.71620 0.74471 0.62006
Our Run: 150-3 0.63698 0.41871 0.72485 0.70296 0.69652 0.65543

Median 0.68923 0.48018 0.76439 0.78949 0.81241 0.57140
Best 0.77807 0.69235 0.82749 0.84138 0.86690 0.74705

Table 1: The pearson correlation score comparison on STS Task, Here 100 and 150 are LSTM memory dimension. 1 and 3 are the
number of LSTM layers

References
Yoshua Bengio, Patrice Simard, and Paolo Fransconi.

1994. Learning long-term dependencies with gradient
descent is difficult. In IEEE Transactions on Neural
Networks 5(2).

John Duchi, Elad Hazan, and Yoram Singer. 2011.
Adaptive subgradient methods for online learning and
stochastic optimization. The Journal of Machine
Learning Research, 12:2121–2159.

Alex Graves, Navdeep Jaitly, and Abdel rahman Mo-
hamed. 2013. Hybrid speech recognition with
deep bidirectional lstm. In IEEE Workshop on
Au- tomatic Speech Recognition and Understanding
(ASRU), pages 273–278.

Sepp Hochreiter and Jürgen Schmidhuber. 1998. Long
short-term memory. In Neural Computation 9(8).

Mohit Iyyer, Jordan Boyd-Graber, Leonardo Claudino,
Richard Socher, and Hal Daumé III. 2014. A neu-
ral network for factoid question answering over para-
graphs. In Empirical Methods in Natural Language
Processing.

Sergio Jimenez, George Duenas, Julia Baquero, and
Alexander Gelbukh. 2014. UNAL-NLP: Combin-
ing soft cardinality features for semantic textual sim-
ilarity, relatedness and entailment. In Proceedings of
SemEval 2014: International Workshop on Semantic
Evaluation.

Nal Kalchbrenner and Phil Blunsom. 2013. Recur-
rent continuous translation models. In Proceedings of
the 2013 Conference on Empirical Methods in Nat-
ural Language Processing, pages 1700–1709, Seat-
tle, Washington, USA. Association for Computational
Linguistics.

Yoon Kim, Yacine Jernite, David Sontag, and Alexan-
der M Rush. 2016. Character-aware neural language
models. In Thirtieth AAAI Conference on Artificial In-
telligence.

Alice Lai and Julia Hockenmaier. 2014. Illinois-lh: A
denotational and distributional approach to semantics.
In Proceedings of SemEval 2014: International Work-
shop on Semantic Evaluation.

Richard Socher, Eric H Huang, Jeffrey Pennin, Christo-
pher D Manning, and Andrew Y Ng. 2011a. Dynamic
pooling and unfolding recursive autoencoders for para-
phrase detection. In Advances in Neural Information
Processing Systems, pages 801–809.

Richard Socher, Jeffrey Pennington, Eric H Huang, An-
drew Y Ng, and Christopher D Manning. 2011b.
Semi-supervised recursive autoencoders for predicting
sentiment distributions. In Proceedings of the Con-
ference on Empirical Methods in Natural Language
Processing, pages 151–161. Association for Compu-
tational Linguistics.

Kai Sheng Tai, Richard Socher, and Christopher D Man-
ning. 2015. Improved semantic representations
from tree-structured long short-term memory net-
works. arXiv preprint arXiv:1503.00075.

Jiang Zhao, Tian Tian Zhu, and Man Lan. 2014. ECNU:
One stone two birds: Ensemble of heterogenous mea-
sures for semantic relatedness and textual entailment.
In Proceedings of SemEval 2014: International Work-
shop on Semantic Evaluation.

587


