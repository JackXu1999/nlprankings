



















































INESC-ID at SemEval-2016 Task 4-A: Reducing the Problem of Out-of-Embedding Words


Proceedings of SemEval-2016, pages 238–242,
San Diego, California, June 16-17, 2016. c©2016 Association for Computational Linguistics

INESC-ID at SemEval-2016 Task 4-A: Reducing the Problem of
Out-of-Embedding Words

Silvio Amir, Ramon F. Astudillo, Wang Ling, Mário J. Silva, Isabel Trancoso
Instituto de Engenharia de Sistemas e Computadores Investigação e Desenvolvimento

Rua Alves Redol 9
Lisbon, Portugal

{samir,ramon.astudillo,wlin,mjs,isabel.trancoso}@inesc-id.pt

Abstract

We present the INESC-ID system for the 2016
edition of SemEval Twitter Sentiment Analy-
sis shared task (subtask 4-A). The system was
based on the Non-Linear Sub-space Embed-
ding (NLSE) model developed for last year’s
competition. This model trains a projection
of pre-trained embeddings into a small sub-
space using the supervised data available. De-
spite its simplicity, the system attained perfor-
mances comparable to the best systems of last
edition with no need for feature engineering.
One limitation of this model was the assump-
tion that a pre-trained embedding was avail-
able for every word. In this paper, we investi-
gated different strategies to overcome this lim-
itation by exploiting character-level embed-
dings and learning representations for out-of-
embedding vocabulary words. The resulting
approach outperforms our previous model by
a relatively small margin, while still attaining
strong results and a consistent good perfor-
mance across all the evaluation datasets.

1 Introduction

Pre-trained word embeddings provide a simple
means to attain semi-supervised learning in Natu-
ral Language Processing (NLP) tasks (Collobert et
al., 2011). They can be trained with large amounts
of unsupervised data and be fine tuned as the initial
building block of a semi-supervised system. How-
ever, in domains with a significant number of typos,
use of slang and abbreviations, such as social me-
dia, the high number of singletons leads to a poor
fine tuning of the embeddings. In previous work,

we addressed this by learning a projection of the
embeddings into a small sub-space (Astudillo et
al., 2015b). This allowed us to attain representa-
tions also for Out-Of-Vocabulary (OOV) words, pro-
vided that embeddings for those words are avail-
able. However, even if the embeddings are esti-
mated from large amounts of unlabeled text, in noisy
domains, such as Twitter, a significant number of
words will not be seen and therefore will not have
an embedding. We refer to those words as the Out-
of-Embedding Vocabulary (OOEV).

In this paper, we focus on the problem of obtain-
ing good representations for OOEV words. We ex-
perimented with character to word models (C2W)
and investigated different strategies for initializing
and updating OOEVs from the available training
data. The best results were attained by using the la-
beled data to perform small updates to these repre-
sentations in the first few epochs of training. The re-
sulting system outperforms that of the previous eval-
uation, although by a small margin. It ranks fourth
in the 2016 evaluation with a consistently high per-
formance in all years.

2 NLSE Model Overview

In this section, we briefly review the approach in-
troduced in the 2015 evaluation (Astudillo et al.,
2015a). For a particular regression or classification
task, only a subset of all the latent aspects captured
by the word embeddings will be useful. Therefore,
instead of updating the embeddings directly with the
available labeled data, we estimate a projection of
these embeddings into a low dimensional sub-space.
This simple method brings two fundamental advan-

238



tages. Firstly, the lower dimensional embeddings re-
quire fewer parameters fitting the complexity of the
target task and the available training data. Secondly,
the learned projection can be used to adapt the rep-
resentations for all words with an embedding, even
if they do not occur in the labeled dataset.

Assuming we are given a matrix of pre-trained
embeddings, where each column represents a word
from a vocabulary V , let such matrix be denoted by
E ∈ Re×|V|, where e is the number of latent di-
mensions. We define the adapted embedding ma-
trix as the factorization S · E, where S ∈ Rs×e,
with s � e. The parameters of matrix S are es-
timated using the labeled dataset, while E is kept
fixed. In other words, we determine the optimal pro-
jection of the embedding matrix E into a sub-space
of dimension s. In what follows, we will refer to
this approach as Non-Linear Sub-space Embedding
(NLSE) model.

The NLSE can be interpreted as a simple feed-
forward neural network model (Rumelhart et al.,
1985) with one single hidden layer utilizing the em-
bedding sub-space approach. Let

m = [w1 · · ·wn] (1)

denote a message of n words. Each column
w ∈ {0, 1}v×1 of m represents a word in one-hot
form, that is, a vector of zeros of the size of the vo-
cabulary v with a 1 on the i-th entry of the vector.
Let y denote a categorical random variable over K
classes. The NLSE model, estimates the probabil-
ity of each possible category y = k ∈ K given a
message m as

p(y = k|m; θ) ∝ exp (Yk · h · 1) (2)

with parameters θ = {S,Y}. Here, h ∈ [0, 1]e×n
are the activations of the hidden layer for each word,
given by

h = σ (S ·E ·m) (3)
where σ() is a sigmoid function acting on each ele-
ment of the matrix. The matrix Y ∈ R3×s maps the
embedding sub-space to the classification space and
1 ∈ 1n×1 is a matrix of ones that sums the scores
for all words together, prior to normalization. This
is equivalent to a bag-of-words assumption. Finally,
the model computes a probability distribution over
the K classes, using the softmax function.

3 Out-of-Embedding Vocabulary Words

Despite the fact that word embeddings are typi-
cally estimated from very large amounts of unla-
beled data, it is often the case that a number of
words appearing on the training or test sets are not
present on the unlabeled corpus. These words will
not be represented in E. This problem is even more
significant in social media environments like Twit-
ter, where there is a significant lexical variation and
where novel words, expressions and slang can be in-
troduced over time. In Table 1, we show the per-
centage of OOV and OOEV words on each Twitter
dataset.

The näive way of dealing with this issue, is to sim-
ply set the embeddings of unknown words to zero,
essentially ignoring them. As will see later, a better
approach is to treat these words as model parameters
and use the training signal to learn a better represen-
tation for them.

3.1 Character-level Embeddings

One natural way of avoiding OOEV in neural net-
work models, is to learn character-level embeddings
and define a composition function to combine them
into word representations, thus obtaining represen-
tations for any given word.

We experimented using C2W, a simple compo-
sitional model for learning word representations,
from character embeddings. Given a word w, the
C2W model generates a set of character n-grams
{c1, . . . , cm}, and projects each n-gram ci into a
vector eci ∈ Rd, where d is the number of latent di-
mensions. The individual character representations
are then combined to obtain a fixed-size representa-
tion for word w as ew = ec1 ⊕ . . . ⊕ ecm , where
⊕ denotes pointwise sum. These word representa-
tions can be used as the input to a standard neural
language model where the parameters are estimated
from unlabeled data by learning to predict words
within a context.

3.2 Mapping C2W to SSG Embeddings

Unfortunately the C2W embeddings performed very
poorly in our model. Therefore, to have embeddings
for all the words we employed an approach similar
to (Mikolov et al., 2013). We learn a mapping be-
tween the embedding spaces induced by C2W and

239



2013 2014 2015 2016
OOV 70.9% 37.9% 39.3% 65.1%
OOEV 15.0% 11.2% 11.5% 22%
OOV & OOEV 14.8% 11.0% 11.3% 21.8%

Table 1: Out Of Vocabulary (OOV) and Ouf Of
Embedding Vocabulary (OOEV) statistics for the
different SemEval Task4-B datasets. Embeddings
reported are the Structured Skipgram embeddings
used in the experiments.

System 2013 2014 2015
2015 hyperparameters 0.618 0.646 0.591
+lower neutral cost 0.706 0.702 0.669
+shuffle per epoch 0.723 0.721 0.649
+update OOEVs 2 iter 0.725 0.729 0.657
Best SemEval 2015 0.722 0.727 0.652

Table 2: Effect of the improvements on the NLSE
model.

Structured Skip-Gram embeddings (SSG) (Ling et
al., 2015), allowing us compute an approximate SSG
embedding for all the words. To this end, we first
obtained C, the set of words present in the two em-
beddings spaces. Then, we learned a linear map T
by solving for the following objective:

T← argmin
T

∑

w∈C
||T · cw − sw||2 (4)

where, cw denotes the C2W embedding for word w
and sw denotes the SSG embedding for wordw. This
mapping, was then used to compute a SSG embed-
dings for each OOEV as sw′ = T · cw′ .

3.3 Partial Update of Embeddings during
Training

Given the small amount of supervised data, directly
updating the embeddings with the SemEval train set
leads to very poor results. It is however possible to
update only the OOEV words present in the training
set simultaneously to the computation of the sub-
space (Astudillo et al., 2015a). To obtain positive
results with this approach, it was also necessary to
reduce the effect of training by lowering the learn-
ing rate to 0.001 and updating the embeddings only
in the first two iterations.

4 Main Improvements over the 2015 NLSE

One complication with Twitter-based evaluations is
the need of the participant to retrieve the tweets
themselves, since some of the tweets may no longer
be available. The INESC-ID system presented in
2015 employed a train set of 8604 tweets, consid-
erably smaller than the original dataset (with 11328
tweets). For this edition, it was possible to get ahold
of the full dataset, as utilized by Severyn and Mos-
chitti (2015). For reproducibility and comparison
purposes our systems this year were developed with
this dataset.

The system presented in 2015 was very sim-
ple both in its structure and the number hyper-
parameters. Furthermore, tunning and selection of
candidate systems was also performed without au-
tomatic grid-search. It was therefore expected that
our previous setup would outright produce better re-
sults by training on a larger dataset. Disappointingly,
this was not the case. In fact, the NLSE optimized
for the 2015 competition seemed to be sitting on a
local optimum that was difficult to come out from.
To overcome this problem, we introduced two mod-
ifications in the training procedure1. The NLSE is
trained by minimizing the negative log-likelihood.
This cost function is sub-optimal taking into account
the evaluation metric, as it weights equally positive,
negative and neutral predictions. A simple improve-
ment over this cost is an asymmetric weighting that
penalizes the predictions of neutral tweets. This was
incorporated as a multiplicative factor on the log-
likelihood of values 4/3, 4/3 and 1/3 for the pos-
itive, negative and neutral classes, respectively. To
reduce the risk of getting trapped into a local mini-
mum, the train data was shuffled before each train-
ing epoch. The asymmetric cost and randomization
led to a slower, less consistent convergence. For this
reason the number of iterations was increased from
8 to 12. The learning rate was changed from 0.01 to
0.005. Table 2 shows the effect of the improvements
on the submitted system.

After introducing these two improvements, we in-
vestigated different methods to address the prob-
lem of OOEV as described in the previous sec-

1After paper revision the model in https://github.
com/ramon-astudillo/NLSE will be updated to reflect
the new system.

240



tion. Namely those exploiting C2W embeddings,
mapping C2W embeddings to SSG embeddings and
training the embeddings for OOEVs. The results of
these strategies are displayed in Table 3.

System 2013 2014 2015 2016
baseline 0.721 0.721 0.649 0.609
C2W embeddings 0.659 0.689 0.613 0.543
C2W→ SSG 0.724 0.715 0.652 0.613
update OOEVs 2 iter 0.723 0.728 0.656 0.610

Table 3: Comparision of strategies to address the
problem of OOEV

5 The Submitted System

As mentioned in the previous section, the system
submitted is an improvement over our 2015 sys-
tem (Astudillo et al., 2015b). It therefore shares the
same training characteristics as the previous model.
The 52 million tweets used by Owoputi et al. (2013)
and the tokenizer described in the same work were
used to train the word embeddings Structured Skip-
Gram (SSG). For this submission, the C2W em-
beddings were also trained using a publicly avail-
able toolkit2. For the annotated SemEval training
data, the messages were previously pre-processed as
follows: lower-casing, replacing Twitter user men-
tions and URLs with special tokens and reducing
any character repetition to at most 3 characters. Fol-
lowing Astudillo et al. (2015a), we used embeddings
with 600 dimensions and set the sub-space size to 10
dimensions.

To train the model, the development set was split
into 80% for parameter learning and 20% for model
evaluation and selection, maintaining the original
relative class proportions in each set. The weights
were all randomly initialized uniformly with ranges
of [−0.001, 0.001], [−0.1, 0.1] and [−0.7, 0.7] for
the OOEVs, subspace and classification layers re-
spectively. The training procedure entailed mini-
mizing the negative log-likelihood over the train-
ing data with respect to the parameters, using stan-
dard Stochastic Gradient Descent (Rumelhart et al.,
1985) with a fixed learning rate of 0.005 and mini-
batch of size 1, i.e., updating the weights after each
message was processed. We reshuffled the training

2https://github.com/wlin12/wang2vec

System 2013 2014 2015 2016 Avg
SwissCheese 0.7005 0.7165 0.6711 0.6331 0.6802
SENSEI-LIF 0.7064 0.7442 0.6622 0.6302 0.6861
unimelb 0.6877 0.7067 0.6514 0.6173 0.6654
INESC-ID 0.7232 0.7273 0.6573 0.6104 0.6793
aueb 0.6668 0.7086 0.6237 0.6055 0.6515

Table 4: Official test-set results for the top five sys-
tems in SemEval 2016 Task 4-B. Subscript number
indicates position in general ranking.

examples after each training epoch and performed
model selection by early stopping after 12 iterations.
The candidate for submission was manually selected
by observing the performance across 2013, 2014 and
2015 datasets. Priority was given to models that
presented a consistent high performance in all the
datasets. In retrospect, this was most probably a sub-
optimal decision judging from the evaluation results.

Table 4 displays the performance for the top 5 sys-
tems in SemEval 2016 task 4-B (Nakov et al., 2016).
The NLSE system (labeled INESC-ID) ranks forth
with a stable performance across all years. The re-
sults are particularly strong for 2013 with a differ-
ence of 0.017 points over the next best performing
system on the top five. This is consistent with the
divide noticed during system selection between per-
formance in 2013 and 2015. High-performing sys-
tems in 2014, and particularly in 2013, do not appear
to be equally performing in recent years.

6 Conclusions

We presented the INESC-ID system for the SemEval
2016 task 4-A, built on top of the successful Non-
Linear Subspace Embedding model. We found that
training with a larger dataset required a more careful
procedure to avoid overfitting. Reproducing the best
results obtained in SemEval 2015 required shuffling
the data before each training epoch and adapting the
cost function to better reflect the evaluation metric.

To address the problem of out-of-embedding
words, we tried to introduce character-level embed-
dings in our model but found these to be detrimental.
We obtained better results by learning embeddings
for these words during the training. Even though the
performance gains were not very pronounced, our
system still attained very strong results across all the
evaluation datasets.

241



Acknowledgments

This work was partially supported by Fundação para
a Ciência e Tecnologia (FCT), through contracts
UID/CEC/50021/2013, EXCL/EEI-ESS/0257/2012
(DataStorm), grant number SFRH/BPD/68428/2010
and Ph.D. scholarship SFRH/BD/89020/2012.

References

Ramón Astudillo, Silvio Amir, Wang Ling, Mario Silva,
and Isabel Trancoso. 2015a. Learning word repre-
sentations from scarce and noisy data with embedding
subspaces. In Proceedings of the 53rd Annual Meet-
ing of the Association for Computational Linguistics
and the 7th International Joint Conference on Natu-
ral Language Processing (Volume 1: Long Papers),
pages 1074–1084, Beijing, China, July. Association
for Computational Linguistics.

Ramon F. Astudillo, Silvio Amir, Wang Ling, Bruno
Martins, Mário Silva, and Isabel Trancoso. 2015b.
Inesc-id: Sentiment analysis without hand-coded fea-
tures or liguistic resources using embedding sub-
spaces. In Proceedings of the 9th International Work-
shop on Semantic Evaluation, SemEval ’2015, Denver,
Colorado, June. Association for Computational Lin-
guistics.

Ronan Collobert, Jason Weston, Léon Bottou, Michael
Karlen, Koray Kavukcuoglu, and Pavel Kuksa. 2011.
Natural language processing (almost) from scratch.
The Journal of Machine Learning Research, 12:2493–
2537.

Wang Ling, Chris Dyer, Alan Black, and Isabel Tran-
coso. 2015. Two/too simple adaptations of word2vec
for syntax problems. In Proceedings of the 2015 Con-
ference of the North American Chapter of the Associa-
tion for Computational Linguistics: Human Language
Technologies. Association for Computational Linguis-
tics.

Tomas Mikolov, Quoc V Le, and Ilya Sutskever. 2013.
Exploiting similarities among languages for machine
translation. arXiv preprint arXiv:1309.4168.

Preslav Nakov, Alan Ritter, Sara Rosenthal, Veselin Stoy-
anov, and Fabrizio Sebastiani. 2016. SemEval-2016
task 4: Sentiment analysis in Twitter. In Proceedings
of the 10th International Workshop on Semantic Eval-
uation (SemEval 2016).

Olutobi Owoputi, Chris Dyer, Kevin Gimpel, Nathan
Schneider, and Noah A. Smith. 2013. Improved part-
of-speech tagging for online conversational text with
word clusters. In In Proceedings of NAACL.

David E Rumelhart, Geoffrey E Hinton, and Ronald J
Williams. 1985. Learning internal representations by
error propagation. Technical report, DTIC Document.

Aliaksei Severyn and Alessandro Moschitti. 2015.
Unitn: Training deep convolutional neural network for
twitter sentiment classification. In Proceedings of the
9th International Workshop on Semantic Evaluation,
SemEval ’2015, Denver, Colorado, June. Association
for Computational Linguistics.

242


