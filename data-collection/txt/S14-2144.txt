



















































UW-MRS: Leveraging a Deep Grammar for Robotic Spatial Commands


Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 812–816,
Dublin, Ireland, August 23-24, 2014.

UW-MRS: Leveraging a Deep Grammar for Robotic Spatial Commands

Woodley Packard

University of Washington

sweaglesw@sweaglesw.org

Abstract

This paper describes a deep-parsing ap-

proach to SemEval-2014 Task 6, a novel

context-informed supervised parsing and

semantic analysis problem in a controlled

domain. The system comprises a hand-

built rule-based solution based on a pre-

existing broad coverage deep grammar of

English, backed up by a off-the-shelf data-

driven PCFG parser, and achieves the best

score reported among the task participants.

1 Introduction

SemEval-2014 Task 6 involves automatic transla-

tion of natural language commands for a robotic

arm into structured “robot control language”

(RCL) instructions (Dukes, 2013a). Statements of

RCL are trees, with a fixed vocabulary of content

words like prism at the leaves, and markup like

action: or destination: at the nonterminals.

The yield of the tree largely aligns with the words

in the command, but there are frequently substitu-

tions, insertions, and deletions.

A unique and interesting property of this task

is the availability of highly relevant machine-

readable descriptions of the spatial context of each

command. Given a candidate RCL fragment de-

scribing an object to be manipulated, a spatial

planner provided by the task organizers can auto-

matically enumerate the set of task-world objects

that match the description. This information can

be used to resolve some of the ambiguity inherent

in natural language.

The commands come from the Robot Com-

mands Treebank (Dukes, 2013a), a crowdsourced

corpus built using a game with a purpose (von

Ahn, 2006). Style varies considerably, with miss-

ing determiners, missing or unexpected punc-

This work is licensed under a Creative Commons At-
tribution 4.0 International Licence. Page numbers and pro-
ceedings footer are added by the organizers. Licence details:
http://creativecommons.org/licenses/by/4.0/

tuation, and missing capitalization all common

(Dukes, 2013b). Examples (1) and (2) show typi-

cal commands from the dataset.

(1) drop the blue cube

(2) Pick yellow cube and drop it on top of blue cube

Although the natural language commands vary

in their degree of conformance to what might be

called standard English, the hand-built gold stan-

dard RCL annotations provided with them (e.g.

Figure 1) are commendable in their uniformity and

accuracy, in part because they have been automat-

ically verified against the formal before and after

scene descriptions using the spatial planner.

(event: (action: drop)

(entity: (color: blue)

(type: cube))

Figure 1: RCL corresponding to Example (1).

2 Related Work

Automatic interpretation of natural language is

a difficult and long-standing research problem.

Some approaches have taken a relatively shal-

low view; for instance, ELIZA (Weizenbaum,

1966) used pattern matching to somewhat con-

vincingly participate in an English conversation.

Approaches taking a deeper view tend to parse

utterances into structured representations. These

are usually abstract and general-purpose in na-

ture, e.g. the syntax trees produced by main-

stream PCFG parsers and the DRS produced by

the Boxer system (Bos, 2008). As a notable ex-

ception, Dukes (2014) presents a novel method to

produce RCL output directly.

The English Resource Grammar (ERG;

Flickinger, 2000) employed as a component in

the present work is a broad-coverage precision

hand-written unification grammar of English,

following the Head-driven Phrase Structure

Grammar theory of syntax (Pollard & Sag, 1994).

The ERG produces Minimal Recursion Semantics

812



(MRS; Copestake et al., 2005) analyses, which

are flat structures that explicitly encode predicate

argument relations (and other data). A simplified

MRS structure is shown in Figure 2. With minor

modifications to allow determinerless NPs and

some unexpected measure noun lexemes (as in

“two squares to the left”, etc), the ERG yields

analyses for 99% of the commands in the training

portion of the Robot Command Treebank.(
INDEX = e,

{
pron(x), cube n(y),

drop v cause(e, x, y), blue a( , y)
})

Figure 2: Highly simplified view of the MRS pro-

duced by the ERG for Example (1).

3 ERG-based RCL Synthesis

This section outlines the method my sys-

tem employs to synthesize RCL outputs from

the MRS analyses produced by the ERG.

The ERG provides a ranked list of candidate

MRS analyses for each input. As a first step,

grossly inappropriate analyses are ruled out, e.g.

those proposing non-imperative main verbs or

domain-inappropriate parts of speech (“block” as

a verb). An attempt is made to convert each re-

maining analysis into a candidate RCL statement.

If conversion is successful, the result is tested for

coherence with respect to the known world state,

using the supplied spatial planner. An RCL state-

ment is incoherent if it involves picking up or mov-

ing an entity which does not exist, or if its com-

mand type (take, move, drop) is incompatible

with the current state of the robot arm, e.g. drop

is incoherent when the robot arm is not holding

anything. Processing stops as soon as a coherent

result is found.1

3.1 From MRS to RCL

Given an individual (imperative) MRS structure,

the first step in conversion to RCL is to iden-

tify the sequence of top-level verbal predications.

The INDEX property of the MRS provides an en-

try point. In a simple command like Example (1),

the INDEX will point to a single verbal predica-

tion, whereas in a compound command such as

1Practically speaking, conversion from MRS to RCL is
accomplished by a relatively short C program embodying
these rules and steps (about 1500 lines in the final version):
http://sweaglesw.org/svn/semeval-2014-task6/tags/dublin

Example (2), the INDEX will point to a coordina-

tion predication, which itself will have left and

right arguments which must be visited recursively.

Each verbal predication visited in this manner gen-

erates an event: RCL statement whose action:

property is determined by a looking up the ver-

bal predicate in a short hand-written table (e.g.

drop v cause maps to action: drop). If the

predicate is not found in the table, the most com-

mon action move is guessed.

Every RCL event: element must have an

entity: subelement, representing the object to

be moved by the action. Although in princi-

ple MRS makes no guarantees about the gener-

alizability of the semantic interpretation of argu-

ment roles across different predicates, in prac-

tice the third argument of every verbal predicate

relevant to this domain represents the object to

be moved; hence, synthesis of an event: pro-

ceeds by inspecting the third argument of the

MRS predicate which gave rise to it. Some types

of event: also involve a destination: subele-

ment, which encodes the location where the en-

tity should come to rest. When present, a verbal

predicate’s fourth argument almost always iden-

tifies a prepositional predication holding this in-

formation, although there are exceptions (e.g. for

move v from-to rel it is the fifth). When no such

resultative role is present, the first prepositional

modifier (if any) of the verbal event variable is

used for the destination: subelement.

Synthesis of an entity: element from a

referential index like y in Figure 2 or a
spatial-relation: element from a preposi-

tional predication proceeds in much the same way:

the RCL type: or relation: is determined by

a simple table lookup, and subelements are built

based on connections indicated in the MRS. One

salient difference is the treatment of predicates

that are not found in their respective lookup ta-

bles. Whereas unknown command predicates de-

fault to the most common action move, unknown

modifying spatial relations are simply dropped,2

and unknown entity types cause conversion to fail,

on the theory that an incorrect parse is likely. Pru-

dent rejection of suspect parses only rarely elim-

inates all available analyses, and generally helps

to find the most appropriate one. On development

data, the first analysis produced by the ERG was

2If the spatial relation is part of a mandatory
destination: element, this can then cause conversion to fail.

813



convertible for 87% of commands, and the first

RCL hypothesis was spatially coherent for 96% of

commands. These numbers indicate that the parse

ranking component of the ERG works quite well.

3.2 Polishing the Rules

I split the 2500 task-supplied annotated commands

into a randomly-divided training set (2000 com-

mands) and development set (500 commands).

Throughout this work, the development set was

only used for estimating performance on unseen

data and tuning system combination settings; the

contents of the development set were never in-

spected for rule writing or error analysis pur-

poses. Although the conversion architecture out-

lined above constitutes an effective framework,

there were quite a few details to be worked

through, such as the construction of the lookup ta-

bles, identification of cases requiring special han-

dling, elimination of undesirable parses, modest

extension of the ERG, etc. An error-analysis

tool which performed a fine-grained comparison

of the synthesized RCL statements with the gold-

standard ones and agglomerated common error

types proved invaluable when writing rules. 3 Pol-

ishing the system in this manner took about two

weeks of part-time effort; I maintained a log giv-

ing a short summary of each tweak (e.g. “map

center n of rel to type: region”). These

tweaks required varying amounts of time to imple-

ment, from a few seconds up to perhaps an hour;

system accuracy as a function of the number of

such tweaks is shown in Figure 3.

3.3 Anaphora and Ellipsis

Some commands use anaphora to evoke the iden-

tity or type of previously mentioned entities. Typ-

ically, the pronoun “it” refers to a specific entity

while the pronoun “one” refers to the type of an

entity (e.g. “Put the red cube on the blue one.”).

Empirically, the antecedent is nearly always the

first entity: element in the RCL statement, and

this heuristic works well in the system. A small

fraction of commands (< 0.5% of the training
data) elide the pronoun, in commands like “Take

the blue tetrahedron and place in front left corner.”

In principle these could be detected and accommo-

dated through the addition of a simple mal-rule to

3The error-analysis tool walks the system and gold
RCL trees in tandem, recording differences and printing the
most common mismatches. It consists of about 100 lines of
Python and shell script, and took perhaps an hour to build.

0

20

40

60

80

100

10 20 30 40 50 60 70 80

A
cc
u
ra
cy

Approx. Number of Tweaks

Training Set
Development Set

Figure 3: Tuning the MRS-to-RCL conversion

system by tweaking/adding rules. Development-

set accuracy was only checked occasionally during

rule-writing to avoid over-fitting.

the ERG (Bender et al., 2004), but for simplicity

my system ignores this problem, leading to errors.

4 Robustness Strategies

If none of the analyses produced by the ERG result

in coherent RCL statements, the system produces

no output. On the one hand this results in quite a

high precision: on the training data, 96.75% of the
RCL statements produced are exactly correct. On

the other hand, in some scenarios a lower precision

result may be preferable to no result. The ERG-

based system fails to produce any output for 3.1%
of the training data inputs, a number that should be

expected to increase for unseen data (since conver-

sion can sometimes fail when the MRS contains

unrecognized predicates).

In order to produce a best-guess answer for

these remaining items, I employed the Berkeley

parser (Petrov et al., 2006), a state-of-the-art data-

driven system that induces a PCFG from a user-

supplied corpus of strings annotated with parse

trees. The RCL treebank is not directly suitable as

training material for the Berkeley parser, since the

yield of an RCL tree is not identical to (or even

in 1-to-1 correspondence with) the words of the

input utterance. In the interest of keeping things

simple, I produced a phrase structure translation

of the RCL treebank by simply discarding the el-

ements of the RCL trees that did not correspond

to any input, and inserting (X word) nodes for in-

put words that were not aligned to any RCL frag-

ment. The question of where in the tree to insert

these X nodes is presumably of considerable im-

portance, but again in the interest of simplicity I

simply clustered them together with the first RCL-

814



S

event:

✟✟
✟✟

❍❍
❍❍

action:

drop

drop

entity:

✟✟✟ ❍❍❍
color:

✟✟ ❍❍
X

the

blue

blue

type:

cube

cube

Figure 4: Automatic phrase structure tree transla-

tion of the RCL statement shown in Figure 1.

aligned word appearing after them. Unaligned in-

put tokens at the end of the sentence were added

as siblings of the root node. Figure 4 shows the

phrase structure tree resulting from the translation

of the RCL statement shown in Figure 1.

Using this phrase structure treebank, the Berke-

ley parser tools make it possible to automatically

derive a similar phrase structure tree for any input

string, and indeed when the input string is a com-

mand such as the ones of interest in this work, the

resulting tree is quite close to an RCL statement.

Deletion of the X nodes yields a robust system

that frequently produces the exact correct RCL,

at least for those items where only input-aligned

RCL leaves are required. The most common type

of non-input-aligned RCL fragment is the id: el-

ement, identifying the antecedent of an anaphor.

As with the ERG-based system, a heuristic select-

ing the first entity as the antecedent whenever an

anaphor is present works quite well.

Improving the output of the statistical system

via tweaks of the type used in the ERG-based sys-

tem was much more challenging, due to the rel-

ative impoverishedness of the information made

available by the parser. Accurately detecting situ-

ations to improve without causing collateral dam-

age proved difficult. However, the base accu-

racy of the statistical system was quite good, and

when used as a back-off it improved overall sys-

tem scores considerably, as shown in Table 5.

5 Results and Discussion

The combined system performs best on both por-

tions of the data. Over the development data, the

MRS-based system performs considerably better

than the statistical system, in part due to the use

of spatial planning in the MRS-based system (time

did not permit adding spatial planning to the statis-

Dev Eval
System P R P R

MRS-only (−SP) 90.7 88.0 92.1 80.3
MRS-only (+SP) 95.4 92.2 96.1 82.4

Robust-only (−SP) 88.2 88.2 81.5 81.5
Combined (−SP) 90.8 90.8 90.5 90.5
Combined (+SP) 95.0 95.0 92.5 92.5

ERG coverage 98.6 91.0

Figure 5: Evaluation results. ±SP indicates
whether or not spatial planning was used. The ro-

bust and combined systems always returned a re-

sult, so P = R.

tical system). The statistical system has a slightly

higher recall than the MRS-only system without

spatial planning, but the MRS-only system has a

higher precision — markedly so on the evalua-

tion data. This is consistent with previous find-

ings combining precision grammars with statisti-

cal systems (Packard et al., 2014).

ERG coverage dropped precipitously from

roughly 99% on the development data to 91%

on the evaluation data. This is likely the major

cause of the 10% absolute drop in the recall of the

MRS-only system. The fact that the robust sta-

tistical system encounters a comparable drop on

the evaluation data suggests that the text is qual-

itatively different from the (also held-out) devel-

opment data. One possible explanation is that

whereas the development data was randomly se-

lected from the 2500 task-provided training com-

mands, the evaluation data was taken as the se-

quentially following segment of the treebank, re-

sulting in the same distribution of game-with-a-

purpose participants (and hence writing styles) be-

tween the training and development sets but a dif-

ferent distribution for the evaluation data. 4

Dukes (2014) reports an accuracy of 96.53%,

which appears to be superior to the present system;

however, that system appears to have used more

training data than was available for the shared task,

and averaged scores over the entire treebank, mak-

ing direct comparison difficult.

Acknowledgements

I am grateful to Dan Flickinger, Emily Bender and

Stephan Oepen for their many helpful suggestions.

4Reviewers suggested dropping sentence initial punctua-
tion and reading “cell” as “tile.” This trick boosts the MRS-
only recall to 91.1% and the combined system to 94.5%,
demonstrating both the frailty of NLP systems to unexpected
inputs and the presence of surprises in the evaluation data.
ERG coverage rose from 91.0% to 98.6%.

815



References

Bender, E. M., Flickinger, D., Oepen, S., Walsh, A., &

Baldwin, T. (2004). Arboretum: Using a preci-

sion grammar for grammar checking in CALL.

In Instil/icall symposium 2004.

Bos, J. (2008). Wide-coverage semantic analysis with

boxer. In J. Bos & R. Delmonte (Eds.), Seman-

tics in text processing. step 2008 conference pro-

ceedings (pp. 277–286). College Publications.

Copestake, A., Flickinger, D., Pollard, C., & Sag, I.

(2005). Minimal recursion semantics: An intro-

duction. Research on Language & Computation,

3(2), 281–332.

Dukes, K. (2013a). Semantic annotation of robotic

spatial commands. In Language and Technology

Conference (LTC). Poznan, Poland.

Dukes, K. (2013b). Train robots: A dataset for

natural language human-robot spatial interac-

tion through verbal commands. In Interna-

tional Conference on Social Robotics (ICSR).

Embodied Communication of Goals and Inten-

tions Workshop.

Dukes, K. (2014). Contextual Semantic Parsing using

Crowdsourced Spatial Descriptions. Computa-

tion and Language. arXiv:1405.0145 [cs.CL].

Flickinger, D. (2000). On building a more efficient

grammar by exploiting types. Natural Language

Engineering, 6(01), 15-28.

Packard, W., Bender, E. M., Read, J., Oepen, S., & Dri-

dan, R. (2014). Simple negation scope reso-

lution through deep parsing: A semantic solu-

tion to a semantic problem. In Proceedings of

the 52nd annual meeting of the Association for

Computational Linguistics. Baltimore, USA.

Petrov, S., Barrett, L., Thibaux, R., & Klein, D. (2006).

Learning accurate, compact, and interpretable

tree annotation. In Proceedings of the 21st Inter-

national Conference on Computational Linguis-

tics and the 44th annual meeting of the Asso-

ciation for Computational Linguistics (pp. 433–

440).

Pollard, C., & Sag, I. A. (1994). Head-Driven Phrase

Structure Grammar. Chicago, USA: The Uni-

versity of Chicago Press.

von Ahn, L. (2006). Games with a purpose. Computer,

39(6), 92–94.

Weizenbaum, J. (1966). ELIZA — a computer pro-

gram for the study of natural language commu-

nication between man and machine. Communi-

cations of the ACM, 9(1), 36–45.

816


