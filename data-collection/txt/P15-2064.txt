



















































Cross-lingual Transfer of Named Entity Recognizers without Parallel Corpora


Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics
and the 7th International Joint Conference on Natural Language Processing (Short Papers), pages 390–396,

Beijing, China, July 26-31, 2015. c©2015 Association for Computational Linguistics

Cross-lingual Transfer of Named Entity Recognizers
without Parallel Corpora

Ayah Zirikly∗
Department of Computer Science

The George Washington University
Washington DC, USA
ayaz@gwu.edu

Masato Hagiwara
Duolingo, Inc.

Pittsburgh PA, USA
masato@duolingo.com

Abstract

We propose an approach to cross-lingual
named entity recognition model transfer
without the use of parallel corpora. In ad-
dition to global de-lexicalized features, we
introduce multilingual gazetteers that are
generated using graph propagation, and
cross-lingual word representation map-
pings without the use of parallel data. We
target the e-commerce domain, which is
challenging due to its unstructured and
noisy nature. The experiments have shown
that our approaches beat the strong MT
baseline, where the English model is trans-
ferred to two languages: Spanish and Chi-
nese.

1 Introduction

Named Entity Recognition (NER) is usually
solved by a supervised learning approach, where
sequential labeling models are trained from a large
amount of manually annotated corpora. However,
such rich annotated data only exist for resource-
rich languages such as English, and building NER
systems for the majority of resource-poor lan-
guages, or specific domains in any languages, still
poses a great challenge.

Annotation projection through parallel text
(Yarowsky et al., 2001), (Das and Petrov, 2011),
(Wang and Manning, 2014) has been traditionally
used to overcome this issue, where the annotated
tags in the source (resource-rich) language are pro-
jected via word-aligned bilingual parallel text (bi-
text) and used to train sequential labeling mod-
els in the (resource-poor) target language. How-
ever, this could lead to two issues: firstly, word

∗This work has been performed while the authors were
at Rakuten Institute of Technology, New York. The authors
would like to thank Prof. Satoshi Sekine at New York Univer-
sity and other members of Rakuten Institute of Technology
for their support during the project.

alignment and projected tags are potentially noisy,
making the trained models sub-optimal. Instead of
projecting noisy labels explicitly, Wang and Man-
ning (2014) project posterior marginals expecta-
tions as soft constraints. Das and Petrov (2011)
projected POS tags from source language types
to target language trigarms using graph propaga-
tion and used the projected label distribution to
train robust POS taggers. Secondly, the availabil-
ity of such bitext is limited especially for resource-
poor languages and domains, where it is often the
case that available resources are moderately-sized
monolingual/comparable corpora and small bilin-
gual dictionaries.

Instead, we seek a direct transfer approach
(Figure 1) to cross-lingual NER (also classified
as transductive transfer learning (Pan and Yang,
2010) and closely related to domain adaptation).
Specifically, we only assume the availability of
comparable corpora and small-sized bilingual dic-
tionaries, and use the same sequential tagging
model trained on the source corpus for tagging
the target corpus. Direct transfer approaches are
extensively studied for cross-lingual dependency
parser transfer. For example, Zeman et al. (2008)
built a constituent parser using direct transfer be-
tween closely related languages, namely, Danish
and Swedish. McDonald et al. (2011) trained
de-lexicalized dependency parsers in English and
then “re-lexicalized” the parser. However, cross-
lingual transfer of named entity taggers have not
been studied enough, and this paper, to the best of
the authors’ knowledge, is the first to apply direct
transfer learning to NER.

Transfer of NER taggers poses a difficult chal-
lenge that is different from syntax transfer: most
of the past work deals with de-lexicalized parsers,
yet one of the most important clues for NER,
gazetteers, is inherently lexicalized. Also, various
features used for dependency parsing (Universal
POS tags, unsupervised clustering, etc.) are yet to

390



Figure 1: System Framework

be proven useful for direct transfer of NER. There-
fore, the contributions of this paper is as follows:

1. We show that direct transfer approach for
multilingual NER actually works and per-
forms better than the strong MT baseline
(Shah et al., 2010), where the system’s out-
put in the source language is simply machine
translated into the target language.

2. We explore various non-lexical features,
namely, Universal POS tags and Brown clus-
ter mapping, which are deemed effective
for multilingual NER transfer. Although
brown cluster mapping (Täckström et al.,
2012), Universal POS Tagset (Petrov et al.,
2011), and re-lexicalization and self train-
ing (Täckström et al., 2013) are shown to
be effective for direct transfer of dependency
parsers, there have been no studies exploring
these features for NER transfer.

3. We show that gazetteers can actually be
generated only from the source language
gazetteers and a comparable corpus, through
a technique which we call gazetteer expan-
sion based on semi-supervised graph prop-
agation (Zhu et al., 2003). Gazetteer ex-
pansion has been used for various other pur-
poses, including POS tagging (Alexandrescu
and Kirchhoff, 2007) and dependency parsers
(Durrett et al., 2012).

2 Approach

In this paper we propose a direct transfer learning
approach to train NER taggers in a multilingual
setting. Our goal is to identify named entities in a

target language LT , given solely annotated data in
the source language LS . Previous approaches rely
on parallel data to transfer the knowledge from one
language to another. However, parallel data is very
expensive to construct and not available for all lan-
guage pairs in all domains. Thus, our approach
loosens the constraint and only requires in-domain
comparable corpora.

2.1 Monolingual NER in Source Language

Our framework is based on direct transfer ap-
proach, where we extract abstract, language-
independent and non lexical features FS and FT
in LS and LT . A subset of FT is generated us-
ing a mapping scheme discussed in Section 2.2,
then, directly apply LS NER model on LT using
FT . We adopt Conditional Random Field (CRF)
sequence labeling (Lafferty et al., 2001) to train
our system and generate the English model.

Monolingual Features 1) Token position: In-
stead of using token exact position, we use token
relative position in addition to position’s binary
features such as token is in: first, second, and last
third of the sentence. These features are based on
the observation that certain tokens, such as brand
names in title or description of a product, tend to
appear at the beginning of the sentence, while oth-
ers toward the end.

2) Word Shape: We use a list of binary fea-
tures: is-alphanumerical, is-number, is-alpha, is-
punctuation, the number length (if is-num is true),
pattern-based features (e.g. regular expressions to
capture certain patterns such as products model
numbers), latin-only features (first-is-capital, all-
capitals, all-small);

391



3) In-Title: A binary feature that specifies
whether the token is in the product’s title or de-
scription. For instance, brand names mostly ap-
pear in the beginning of titles, while this does not
hold in descriptions;

4) Preceding/Proceeding keywords within win-
dow: some NEs are often preceded by certain
keywords. For instance, often a product size is
preceded by certain keywords such as dimension,
height or word“size.” In our work we use a manu-
ally created list of keywords for two classes Color
and Size. Although the keyword list is domain de-
pendent, it is often short and can be easily updated.

5) Universal Part of Speech Tags: Part of
Speech (POS) tags have been widely used in many
NER systems. However, each language has its
own POS tagset that often has limited overlap with
other POS languages’ tagsets. Thus, we use a
coarse-grained layer of POS tags called Universal
POS, as proposed in (Petrov et al., 2011).

6) Token is a unit: A binary feature that is set to
true if it matches an entry in the units dictionary
(e.g., “cm.”)

7) Gazetteers: Building dictionaries for every
LT of interest is expensive; thus, we propose
a method, described in Section 3, to generate
gazetteers in LT given ones in LS .

8) Brown Clustering (BC): Word representa-
tions, especially Brown Clustering (Brown et al.,
1992), are used in many NLP tasks and are proven
to improve NER performance (Turian et al., 2010).
In this work, we use cluster IDs of variable pre-
fix lengths in order to retrieve word similarities on
different granularity levels.

2.2 Multilingual NER in Target Language

Our goal is to transfer each feature from LS to LT
space. The main challenge resides in transferring
features 7 and 8 without the use of external re-
sources and parallel data for every target language.

2.2.1 Brown Clustering Mapping
Given i) Vocabulary in the source/target lan-
guages VS = {wS1 , wS2 , ..., wSNS} and VT =
{wT1 , wT2 , ..., vTNT }; ii) The output of brown clus-
tering on LS and LT : CS = {cS1 , ..., cSKS} and
CT = {cT1 , ..., cTKL}, we aim to find the best
mapping cS∗ that maximizes the cluster similarity
simC for each target cluster (Equation 1), and for
each metric discussed in the following. We cal-
culate the cluster similarity simC as the weighted

average of the word similarity simW of the mem-
bers of the two clusters (Equation 2).

cS∗ = arg max
cS∈CS

simC(c
S , cT ) for each cT ∈ CT (1)

simC(ct, cs) =
1

|cS ||cT |
∑

wS∈cS ,wT∈cT
simW (wS , wT )

(2)

Clusters Similarity Metrics The similarity
metrics used can be summarized in:

a) String Similarity (external resources indepen-
dent): This metric works only on languages that
share the same alphabet, as it is based on the in-
tuition that most NEs conserve the name’s shape
or present minor changes that can be identified us-
ing edit distance in closely related languages (we
use Levenshtein distance (Levenshtein, 1966)).
The two variations of string similarity metrics
used are: i) Exact match: simW (wi, wj) =
1 if wi = wj ; ii) Edit distance: simW (wi, wj) =
1 if levenshtein-distance(wi, wj) < θ.

b) Dictionary-based similarity: We present two
similarity metrics using BabelNet synsets (Nav-
igli and Ponzetto, 2012): i) Binary co-occurence:
simbinaryW (wi, wj) = 1 if wj ∈ synset(wi),
where synset(wi) is the set of words in the
BabelNet synset of wi; ii) Frequency weighted:
Weighted version of the binary similarity that
is based on the observation that less frequent
words tend to be less reliable in brown clustering:
simweightedW (wi, wj) = [log f(wi) + log f(wj)]×
simbinaryW (wi, wj) where f(w) is the frequency
of word w. Unlike String similarity metrics, this
metric is not limited to similar languages due to
the use of multilingual dictionaries i.e., BabelNet,
which covers 271 languages.

3 Gazetteer expansion

In our approach, we use graph-based semi-
supervised learning to expand the gazetteers in
the source language to the target. Figure 2 il-
lustrates the motivation of our approach. Sup-
pose we have “New York” in the GPE gazetteer
in LS (English in this case), and we would like
to bootstrap the corresponding GPE gazetteer in
LT (Spanish). Although there is no direct link
between “New York” and “Nueva York,” you can
infer that “Puerto Rico” (in English) is similar to
“New York” based on some intra-language seman-
tic similarity model, then “Puerto Rico” is actually

392



Figure 2: Gazeteer expansion

identical in both languages, then finally “Nueva
York” is similar to “Puerto Rico” (in Spanish)
again based on the Spanish intra-language similar-
ity model. This indirect inference of beliefs from
the source gazetteers to the target can be modeled
by semi-supervised graph propagation (Zhu et al.
2003), where graph nodes are VS ∪ VT , positive
labels are entries in the LS gazetteer (e.g., GPE)
which we wish to expand to LT , and negative la-
bels are entries in other gazetteers (e.g., PERSON)
in LS . The edge weights between same-language
nodes wi and wj are given by exp(−σ||wi−wj ||)
where wi is the distributed vector representation
of word wi computed by word2vec (Mikolov et al.,
2013). The edge weights between node wi ∈ VS
and vj ∈ VT are defined 1 if the spelling of these
two words are identical and 0 otherwise. Note that
this spelling based similarity propagation is still
available for language pairs with different writing
systems such as English and Chinese, because ma-
jor NEs (e.g., brand names) are often written in
Roman alphabets even in Chinese products. Since
the analytical solution to this propagation involves
the computation of n×n (n is the number of unla-
beled nodes) matrix, we approximated it by run-
ning three propagation steps iteratively, namely,
LS → LS , LS → LT , and LT → LT . After
the propagation, we used all the nodes with their
propagated values f(wi) > θ as entities in the new
gazetteer.

4 Experiments

4.1 Datasets

The targeted dataset contains a list of products (ti-
tles and descriptions). The titles of products are
≈ 10 words long and poorly structured, adding
more difficulties to our task. On the other hand,

Color Brand Material Model Type Size
EN 358 814 733 203 1238 427
ES 207 425 301 172 606 126
ZH 416 60 381 24 690 306

Table 1: Language-Tags Numbers Stats

the length of product descriptions ranges from
12-130 words. The e-commerce genre poses the
need to introduce new NE tagset as opposed to
the conventional ones, thus we introduce 6 tag
types: 1) Color; 2) Brand names; 3) Size; 4) Type:
e.g. “camera,” “shirt”; 5) Material: e.g. “plas-
tic”, “cotton”; 6) Model: the model number of a
product: e.g., “A1533.”. For the rest of the ex-
periments, English (EN) is the source language,
whereas we experiment with Spanish (ES) and
Chinese (ZH) as target languages. The datasets
used are: i) Training data: 1800 annotated English
products from Rakuten.com shopping (Rakuten,
2013a); ii) Test data: 300 ES products from
Rakuten Spain (Rakuten, 2013b) and 500 prod-
ucts from Rakuten Taiwan (Rakuten, 2013c); iii)
Brown clustering: English: Rakuten shopping
2013 dump (19m unique products with 607m to-
kens); Spanish: Rakuten Spain 2013 dump (700K
unique products that contains 41m tokens) in addi-
tion to Spanish Wikipedia dump (Al-Rfou’, 2013);
Chinese: Wikipedia Chinese 2014 dump (147m
tokens) plus 16k products crawled from Rakuten
Taiwan. Table 1 shows the numbers of tags per
category for each language.

4.2 Baseline

To the best of our knowledge, there is no previ-
ous work that proposes transfer learning for NER
without the use of parallel data. Thus, we ought to
generate a strong baseline to compare our results
to. Given the language pair (LS , LT ), we use Mi-
crosoft Bing Translate API to generate LT → LS
translation. Then, we apply LS NER model on the
translated text and evaluate by mapping the tagged
tokens back to LT using the word alignments gen-
erated by Bing Translate. We choose Bing trans-
late as opposed to Google translate due to its free-
to-use API that provides word alignment informa-
tion on the character level.

4.3 Results & Discussion

For each studied language we use Stanford
CoreNLP (Manning et al., 2014) for EN and ZH,
and TreeTagger (Schmid, 1994) for ES to produce

393



Color Brand Material Model Type Size Micro-Avg
EN-Mono 68.45 71.91 50.94 59.78 53.73 45.42 61.12
ES-Baseline 24.23 3.44 13.08 14.51 12.5 6.61 13.79
ES-TL 18.00 9.37 8.05 16.99 18.26 10.64 39.46
ES-GT 38.49 13.31 33.5 2.27 36.43 1.16 30.20
ZH-Baseline 19.16 2.79 11.96 None 9.35 6.34 12.58
ZH-TL 9.36 1.02 1.81 None 17.28 17.74 23.43

Table 2: F-score Results

the tokens and the POS tags. However, we ap-
ply extra processing steps to the tokenizer due to
the nature of the domain’s data (e.g., avoid tok-
enizing models instances), in addition to normaliz-
ing URLs, numbers, and elongation. We also map
POS tags for all the source and target languages to
the universal POS tagset as explained in 2.1.

Based on Table 2, we note that English mono-
lingual performance (80:20 train/test split and 5-
folds cross variation) is considerably lower than
state-of-the-art English NER systems, which is
due to the nature of our targeted domain, the newly
proposed NE tagset, and most importantly, the
considerably small training data (1280 products).
These factors also affects the baseline and our pro-
posed system performance.

Table 2 illustrates the results for the English
monolingual NER system (EN-Mono), baseline
for ES and ZH (ES-Baseline and ZH-Baseline,
respectively), our proposed transfer learning ap-
proach with the gazetteer expansion (ES-TL and
ZH-TL). Additionally, we added the results of our
proposed approach where the gazetteers used are
machine translated using Google translate from
the English gazetteers to Spanish (ES-MT), in or-
der to evaluate our gazetteer expansion approach
performance to the translated gazetteers.

We note that ES-Baseline and ZH-Baseline are
considerably low due to the poor word alignment
generated by Bing Translator, which results in in-
correct tag projection. The quality of mapping is
mainly due to the noisy nature of the domain’s
data, which can be very expensive to fix.

Although the performance of our proposed sys-
tem is low (39.46% for ES and 23.43% for ZH),
but it surpasses the baseline performance in most
of the tag classes and yields an overall improve-
ment on the micro-average F-score of ≈ 23% in
ES and 11% in ZH. We note that one of the rea-
sons behind ZH Brand low performance is that
universal-POS for brands in EN are mostly proper

noun as opposed to noun in ZH, additionally the
considerably low number of brands in ZH test
data (60). On the other hand, it is intuitive that
Model yields one of the best performance among
the tags, since it is the most language independent
tag (as depicted in ES-TL). However, this does not
hold true in ZH due to the very small number of
Model instances (24). Type produces the best per-
formance in ES and ZH, due to the high cover-
age of the new expanded gazetteer over Type in-
stances, in addition to the large number of train-
ing instances (1238), in comparison to the other
tags. After conducting leave-out experiments on
Brown clustering and gazetteers features in ES, we
note that both shows an improvement of≈ 4% and
≈ 8% respectively.

Our system surpasses the MT-based gazetter ex-
pansion by ≈ 9%, when comparing ES-TL to ES-
MT. However, as expected the main improvement
is in Model and Size tags as opposed to other tags
(e.g. Brand and Color) where MT provides more
accurate gazetteers. In our system output, colors
that are included in LT expanded gazetteers (e.g.
“azul” in ES) and have a high similarity score in
our proposed BC mapping, are correctly tagged.
On the other hand OOV Brand have a very large
prediction error rate due to the small training data.

5 Conclusion and Future Works

In this paper, we propose a cross-lingual NER
transfer learning approach which does not depend
on parallel corpora. Our experiments showed the
ability to transfer NER model to latin (ES) and
non latin (ZH) languages. For the future work, we
would like to investigate the generality of our ap-
proach in broader languages and domains.

394



References
Rami Al-Rfou’. 2013. Spanish wikipedia dump. url =

https://sites.google.com/site/rmyeid/projects/polyglot.

Andrei Alexandrescu and Katrin Kirchhoff. 2007.
Data-driven graph construction for semi-supervised
graph-based learning in NLP. In Human Language
Technologies 2007: The Conference of the North
American Chapter of the Association for Computa-
tional Linguistics; Proceedings of the Main Confer-
ence, pages 204–211, Rochester, New York, April.
Association for Computational Linguistics.

Peter F Brown, Peter V Desouza, Robert L Mercer,
Vincent J Della Pietra, and Jenifer C Lai. 1992.
Class-based n-gram models of natural language.
Computational linguistics, 18(4):467–479.

Dipanjan Das and Slav Petrov. 2011. Unsuper-
vised part-of-speech tagging with bilingual graph-
based projections. In Proceedings of the 49th An-
nual Meeting of the Association for Computational
Linguistics: Human Language Technologies, pages
600–609, Portland, Oregon, USA, June. Association
for Computational Linguistics.

Greg Durrett, Adam Pauls, and Dan Klein. 2012. Syn-
tactic transfer using a bilingual lexicon. In Pro-
ceedings of the 2012 Joint Conference on Empirical
Methods in Natural Language Processing and Com-
putational Natural Language Learning, pages 1–11.
Association for Computational Linguistics.

John Lafferty, Andrew McCallum, and Fernando CN
Pereira. 2001. Conditional random fields: Prob-
abilistic models for segmenting and labeling se-
quence data.

VI Levenshtein. 1966. Binary Codes Capable of Cor-
recting Deletions, Insertions and Reversals. Soviet
Physics Doklady, 10:707.

Christopher D. Manning, Mihai Surdeanu, John Bauer,
Jenny Finkel, Steven J. Bethard, and David Mc-
Closky. 2014. The Stanford CoreNLP natural lan-
guage processing toolkit. In Proceedings of 52nd
Annual Meeting of the Association for Computa-
tional Linguistics: System Demonstrations, pages
55–60.

Ryan McDonald, Slav Petrov, and Keith Hall. 2011.
Multi-source transfer of delexicalized dependency
parsers. In Proceedings of the Conference on Em-
pirical Methods in Natural Language Processing,
EMNLP ’11, pages 62–72, Stroudsburg, PA, USA.
Association for Computational Linguistics.

Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Cor-
rado, and Jeff Dean. 2013. Distributed representa-
tions of words and phrases and their compositional-
ity. In Advances in Neural Information Processing
Systems, pages 3111–3119.

Roberto Navigli and Simone Paolo Ponzetto. 2012.
BabelNet: The automatic construction, evaluation

and application of a wide-coverage multilingual se-
mantic network. Artificial Intelligence, 193:217–
250.

Sinno Jialin Pan and Qiang Yang. 2010. A survey on
transfer learning. IEEE Trans. on Knowl. and Data
Eng., 22(10):1345–1359, October.

Slav Petrov, Dipanjan Das, and Ryan McDonald. 2011.
A universal part-of-speech tagset. arXiv preprint
arXiv:1104.2086.

Rakuten. 2013a. Rakuten shopping. url =
http://www.rakuten.com/.

Rakuten. 2013b. Rakuten spanish.

Rakuten. 2013c. Rakuten taiwanese.

Helmut Schmid. 1994. Probabilistic part-of-speech
tagging using decision trees.

Rushin Shah, Bo Lin, Anatole Gershman, and Robert
Frederking. 2010. Synergy: A named entity recog-
nition system for resource-scarce languages such as
swahili using online machine translation. In Pro-
ceedings of the Second Workshop on African Lan-
guage Technology (AfLaT 2010), pages 21–26.

Oscar Täckström, Ryan McDonald, and Jakob Uszko-
reit. 2012. Cross-lingual word clusters for direct
transfer of linguistic structure. In Proceedings of
the 2012 Conference of the North American Chap-
ter of the Association for Computational Linguis-
tics: Human Language Technologies, pages 477–
487, Montréal, Canada, June. Association for Com-
putational Linguistics.

Oscar Täckström, Ryan McDonald, and Joakim Nivre.
2013. Target language adaptation of discrimina-
tive transfer parsers. In Proceedings of the 2013
Conference of the North American Chapter of the
Association for Computational Linguistics: Human
Language Technologies, pages 1061–1071, Atlanta,
Georgia, June. Association for Computational Lin-
guistics.

Joseph Turian, Lev Ratinov, and Yoshua Bengio. 2010.
Word representations: A simple and general method
for semi-supervised learning. In Proceedings of the
48th Annual Meeting of the Association for Compu-
tational Linguistics, pages 384–394. Association for
Computational Linguistics.

Mengqiu Wang and Christopher D Manning. 2014.
Cross-lingual projected expectation regularization
for weakly supervised learning.

David Yarowsky, Grace Ngai, and Richard Wicen-
towski. 2001. Inducing multilingual text analy-
sis tools via robust projection across aligned cor-
pora. In Proceedings of the First International Con-
ference on Human Language Technology Research,
HLT ’01, pages 1–8, Stroudsburg, PA, USA. Asso-
ciation for Computational Linguistics.

395



Daniel Zeman, Univerzita Karlova, and Philip Resnik.
2008. Cross-language parser adaptation between re-
lated languages. In In IJCNLP-08 Workshop on NLP
for Less Privileged Languages, pages 35–42.

Xiaojin Zhu, Zoubin Ghahramani, and John Lafferty.
2003. Semi-supervised learning using gaussian
fields and harmonic functions. In IN ICML, pages
912–919.

396


