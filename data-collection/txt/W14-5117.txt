



















































Proceedings of the...


D S Sharma, R Sangal and J D Pawar. Proc. of the 11th Intl. Conference on Natural Language Processing, pages 107–112,
Goa, India. December 2014. c©2014 NLP Association of India (NLPAI)

Multiobjective Optimization and Unsupervised Lexical Acquisition for
Named Entity Recognition and Classification

Govind
IIT Patna, India

govind.mc12@iitp.ac.in

Asif Ekbal
IIT Patna, India

asif@iitp.ac.in

Chris Biemann
TU Darmstadt, Germany

biem@cs.tu-darmstadt.de

Abstract

In this paper, we investigate the util-
ity of unsupervised lexical acquisition
techniques to improve the quality of
Named Entity Recognition and Classi-
fication (NERC) for the resource poor
languages. As it is not a priori clear
which unsupervised lexical acquisition
techniques are useful for a particular task
or language, careful feature selection is
necessary. We treat feature selection as a
multiobjective optimization (MOO) prob-
lem, and develop a suitable framework
that fits well with the unsupervised lexical
acquisition. Our experiments show per-
formance improvements for two unsuper-
vised features across three languages.

1 Introduction

Named Entity Recognition and Classification
(NERC) (Nadeau and Sekine, 2007) is a subtask of
information extraction that has great importance in
many Natural Language Processing (NLP) appli-
cation areas. The objective of NERC is to find and
assign tokens in unstructured text to pre-defined
classes such as the names of organizations, per-
sons, locations, miscellaneous (e.g. date-times,
quantities, monetary expression etc.); and other-
than-NE.

There have been a good number of research
works in NERC area but these are mostly lim-
ited to the resource-rich languages such as En-
glish, the majority of the European languages and
a few Asian languages like Japanese, Chinese and
Korean. Research in NLP relating to the resource-
scarce languages like the Indian ones is still evolv-
ing and poses some interesting problems. Some
of the problems outlined previously in (Ekbal and
Saha, 2011b) with reference to a specific NERC
task include the absence of capitalization informa-
tion, appearance of named entities (NEs) in the

dictionary with other word classes, and the non-
availability of various NLP resources and process-
ing technology for non-Latin resource-poor lan-
guages.

In present work, we propose some novel meth-
ods based on the concepts of unsupervised lex-
ical acquisition and multiobjective optimization
(MOO) (Deb, 2001) for solving the problems of
NERC for several languages. While we eval-
uate the proposed method with only three lan-
guages, the technique is generic and language-
independent, and thus should adapt well to other
languages or domains.

1.1 Multiobjective Optimization

The multiobjective optimization problem (MOOP)
can be stated as follows: find the vectors x of de-
cision variables that simultaneously optimize the
M objective values f1(x), f2(x), ..., fM (x), while
satisfying the constraints, if any. An important
concept of MOO is that of domination. In the con-
text of a maximization problem, a solution xi is
said to dominate xj if ∀k ∈ 1, 2, . . . ,M, fk(xi) ≥
fk(xj) and ∃k ∈ 1, 2, . . . ,M, such that fk(xi) >
fk(xj).In general, a MOO algorithm usually ad-
mits a set of solutions that are not dominated by
any solution encountered by it.

Genetic algorithms (GAs) are known to be
more effective than classical methods such as
weighted metrics, goal programming (Deb, 2001),
for solving multiobjective problems primarily be-
cause of their population-based nature. Evolution-
ary approaches have also been used to solve few
NLP problems including NERC (Ekbal and Saha,
2011a; Sofianopoulos and Tambouratzis, 2010).

1.2 Unsupervised Lexical Acquisition

One of the major problems in applying machine
learning algorithms for solving information ex-
traction problems is the availability of large an-
notated corpora. We explore possibilities aris-107



ing from the use of unsupervised part-of-speech
(PoS) induction (Biemann, 2009) and lexical ex-
pansion (Miller et al., 2012) with distributional
thesauri (Riedl and Biemann, 2013). Unsuper-
vised PoS induction is a technique that induces
lexical-syntactic categories through the statistical
analysis of large, raw text corpora. As shown in
(Biemann et al., 2007a), using these induced cat-
egories as features results in improved accuracies
for a variety of NLP tasks, including NERC.

Lexical expansion (Miller et al., 2012) is also
an unsupervised technique that needs a large cor-
pus for the induction, and is based on the com-
putation of a distributional thesaurus (DT), see
(Riedl and Biemann, 2013; Lin, 1998). While
(Miller et al., 2012) used a DT for expanding
lexical representations and showed performance
gains in knowledge-based word sense disambigua-
tion (WSD), the expansion technique can also be
used in other text processing applications includ-
ing NERC: especially for rare words and unseen
instances, lexical expansion can provide a useful
back-off technique as it performs a generalization
of the training and test data.

2 Technical Background

Unlike supervised techniques, unsupervised PoS
tagging (Christodoulopoulos et al., 2010) tech-
niques require no pre-existing manually tagged
corpus to build a tagging model and hence highly
suitable for the resource poor languages.

There have been various approaches to unsuper-
vised PoS induction. One such approach, reported
in (Brown et al., 1992) is based on the class based
n-gram models. In (Clark, 2003) distributional and
morphological information is used for PoS induc-
tion. We use the unsupervised PoS tagging system
of (Biemann, 2009) because of its availability as
an open source software. We use web-based cor-
pus of 34 million tokens for Bengali (Ekbal and
Bandyopadhyay, 2008), and the datasets reported
in (Biemann et al., 2007b) for Hindi and German.
These datasets were used for unsupervised lexical
acquisition.

A Distributional Thesaurus (DT) is an automat-
ically computed resource that relates words ac-
cording to their similarity. A DT contains, for
every sufficiently frequent word, the most simi-
lar words as computed over the similarity of con-
texts these words appear in, which implements
the distributional hypothesis (Harris, 1951). We

use the scalable, open source implementation of
(Riedl and Biemann, 2013), based on the MapRe-
duce paradigm.

Feature selection is the vital task which involves
selecting a subset of relevant features for build-
ing robust classifier by eliminating the redundant
and irrelevant features. It therefore, reduces the
time complexity of the learning algorithm and im-
proves performance. Overall results as reported
in (Biemann, 2009) suggest that unsupervised PoS
tagging provides an additional word-level feature,
which can be computed for any language and do-
main, and has been proven to be useful in do-
main adaptation and in situations where we have
scarcity of labelled training data. In our work, we
employ unsupervised PoS tags as one of the im-
portant language independent features which can
benefit NERC task for various Indian languages
and German.

We also investigate the use of features based
on distributional similarity. We incorporate three
most similar words to a particular token as three
features in training and test datasets. As an exam-
ple, Figure 1 shows the three most similar words
for tokens in a Hindi language sentence.

Figure 1: Lexical expansion of tokens in Hindi
language with ITRANS transliteration to English.
Here, ND denotes the ”not defined”.

3 Named Entity Features

Following features constitute the available feature
set for building the various models based on a first
order Conditional Random Field (CRF) (Lafferty
et al., 2001) classifier. Most of the following
features do not require any language and domain
specific resources or rules for their computation.
Context words: These denote the local contexts
surrounding the current token.
Word suffix and prefix: Fixed length character
sequences stripped from the leftmost and right108



most positions of words.
First word: A binary valued feature which takes
the value 1 when the current word is the first token
of the sentence and 0 for the other case.
Length of the word: This feature takes the value
1 when the number of characters in a token is
greater than a predetermined threshold value
(here, set to 5).
Infrequent word: A binary valued feature which
checks whether frequency of current word in the
training set exceeds a threshold value (here, set to
5).
Last word of sentence: This binary valued
feature checks whether the word is the last word
of a sentence or not and turn on/off accordingly.
Capitalization: This binary valued feature checks
whether the word starts with a capital letter or not
and takes values accordingly. This feature is used
only for German.
Part-of-speech (POS) information: PoS tags of
the current and/or the surrounding token(s).
Chunk information: Chunk of the current and/or
surrounding tokens. This is used only for German.
Digit features: These features are defined based
upon the presence and/or the number of digits
and/or symbols in a token.
Unsupos: Unsupervised PoS tag as obtained from
the system developed in (Biemann, 2009) is used
as a feature.
Unsupervised DT features: Three most similar
word from the DT for each token in training and
test dataset.

4 Feature Selection using MOO

In this section we formulate feature selection as
an optimization problem that involves choosing
an relevant feature subset for NERC. Mutiobjec-
tive optimization (MOO) can be effective for solv-
ing the problem of feature selection. Here we de-
velop a feature selection method based on a popu-
lar MOO based technique, namely non-dominated
sorting genetic algorithm (NSGA-II) (Deb, 2001).
In order to implement our MOO-based feature
selection we make use of NSGA–II (Deb et al.,
2002). As a supervised learner we used Condi-
tional Random Field (CRF) (Lafferty et al., 2001),
and carried out experiments using its CRF++1 im-
plementation.

1CRF++:Yet another CRF toolkit http://crfpp.googlecode.
com/svn/trunk/doc/index.html

4.1 Formulation of feature selection problem
Let us denote the N number of available features
by f1, f2, . . . , fN and suppose that the set of all
features be denoted by F = fi : i = 1, 2 . . . N .
Then the problem of feature selection can be stated
as follows: Find a set of features G that will op-
timize a function O(F ) such that: G ⊆ F . Here,
O is a measure of classification efficiency for the
classifier trained using the features set G. The
feature selection problem can be formulated un-
der the MOO framework as: Find a set of features
G such that maximize [O1(G), O2(G)], where
O1, O2 ∈ recall, precision, F-measure, −(feature
count). Here, we choose O1 = F-measure and
O2 = −(feature count)

4.2 Problem Encoding
Let the total number of features is N and size of
the population is P . The length of the chromo-
some is determined from the number of available
features and hence its size is N . If the ith posi-
tion of chromosome is 0, then it represents that ith

feature does not participate in feature template set
for construction of CRF-based classifier and oppo-
site in case of 1.All the P number of chromosomes
of initial population are initialized with a random
sequence of 0 and 1.

4.3 Fitness Computation
For the fitness computation, the following steps
are executed.

• There are |G| number of features present in a
particular chromosome (i.e., total |G| number
of 1’s are there in the chromosome).

• Build a CRF classifier with only these |G|
features. We perform 3-fold cross validation
and compute the F-measure value.

• Our objective is to maximize F-measure and
minimize the feature count. NSGA-II (Deb,
2001) is used for optimization process using
these two objective functions.

4.4 Selecting a single solution
The MOO based feature selection technique pro-
duces a set of solutions on the Pareto front. All
these are best in their own and incomparable on
the basis of aforementioned two objectives collec-
tively. But in order to report the final results we
build a CRF classifier with that particular feature
combination that yields the highest F1 measure109



Language Set #tokens
Bengali Training 328,064

Test 34,200
Hindi Training 462,120

Test 60,810
German Training 220,187

Test 54,711

Table 1: Statistics of annotated training and test
datasets

value among all the solutions of the final popu-
lation.

5 Datasets and Experimental Setup

We use the web-based Bengali news corpus for
our NERC experiments (Ekbal and Bandyopad-
hyay, 2008) in Bengali. A part of this corpus
was manually annotated with four MUC NE cat-
egories, namely PER (Person name), LOC (Loca-
tion name), ORG (Organization name) and MISC
(Miscellaneous name). The Miscellaneous name
includes date, time, number, percentages, mon-
etary expressions and measurement expressions
(Ekbal and Bandyopadhyay, 2008). In addition we
also use the NER on South and South East Asian
Languages (NERSSEAL)2 Shared Task datasets
of Bengali after mapping the fine-grained tagset to
our coarse-grained form. For German we use the
datasets obtained from datasets from the CoNLL
2003 challenge (Tjong Kim Sang and De Meul-
der, 2003). Statistics of training and test datasets
are reported in Table 5.

The feature selection algorithm is run three
times with different set of available features.
Specifically we design three experiments, one with
only basic lexical features, the second with lexical
features along with unsupervised PoS tag, and the
third experiment with three features from DT in
addition to unsupervised PoS tag and lexical fea-
tures. In order to properly denote the boundaries
of a NE, we follow the IOB2 encoding scheme of
the CoNLL-2003 shared task3.

6 Evaluation of NERC for the Indian
Languages

In this section we present the results along with
the analysis for NERC on two Indian languages,
namely Hindi and Bengali. For each of the lan-
guages, we extracted the features as defined in
Section 3 including the token itself. We also in-
corporate features from the immediate contextual

2http://ltrc.iiit.ac.in/ner-ssea-08
3http://www.cnts.ua.ac.be/conll2003/ner/

tokens (i.e. preceding token and following token).
So the available number of features becomes equal
to 27*3=81, and our goal is to find the best feature
subset from this available feature set which opti-
mizes our objective functions.

In all the experiments, we set the following pa-
rameter values for NSGA-II algorithm: population
size = 32, number of generations = 50, probability
of crossover = 0.8 and probability of mutation =
0.0125. The values of these parameters were de-
termined using a held-out dataset (created by tak-
ing a portion from the training dataset).

Table 2 depicts the detailed evaluation results
for NERC task on Hindi dataset. Results show
that without using any lexical acquisition feature,
we obtain the best results with a set of 41 features
represented in the final population of MOO based
feature selection algorithm. These results are con-
sidered as baseline for our further experiments on
NERC.

In the next experiment we incorporate unsuper-
vised PoS tag in the available set of features and
apply the algorithm. It is observed that includ-
ing unsuporvised PoS, recall increases but at the
cost of precision. However this causes a small im-
provement in F1 measure. This improvement is
attributed because of the incorporation of unsuper-
vised PoS tags for training the classifier. Thus, un-
supos features generalize over the vocabulary, and
subsume part of the lower-level features. We ob-
serve that the presence of the unsupervised PoS tag
reduces the optimized feature set from 41 down to
25 features while at the same time improving in
F1.

Features Tag Precision Recall F1 FC
Syntactic
features
only(Baseline)

LOC 82.71% 47.97% 60.72

MISC 83.37% 74.22% 78.53
ORG 52.63% 29.85% 38.10
PER 70.72% 29.15% 41.29

Overall 80.15% 52.19% 63.22 41
Syntactic + Un-
supos features LOC 82.20% 49.24% 61.59

MISC 83.00% 76.78% 79.77
ORG 62.50% 29.85% 40.40
PER 67.42% 32.14% 43.53

Overall 79.22% 54.45% 64.54 25
Syntactic + Un-
supos + DT fea-
tures

LOC 72.88% 63.39% 67.81

MISC 80.08% 82.76% 81.40
ORG 55.13% 56.95% 56.03
PER 63.87% 43.96% 52.08

Overall 73.26% 66.44% 69.68 32

Table 2: NERC performance for Hindi data–set,
No. of generations=50, Size of population=32,
FC= Feature Count110



Next, we explore DT features by adding them
to the pool of features. Algorithm for feature se-
lection is again run with these additional features,
and the results are reported in Table 2. With these
DT features, recall goes up rapidly, but at the cost
of precision. Again, we see a drop in precision,
yet a relative recall increase of approximately 12%
causes the F-measure to increase approximately
by 5 percentage points.

The feature selection algorithm determines 32
features to be most relevant for the task. This fea-
ture combination includes several lexical expan-
sion features that include the first two expansions
of the preceding token and all the three expansions
of the current token. It seems that the CRF profits
rather from the expansion of contexts than from
the expansions themselves. These DT in combi-
nation with unsupervised PoS features improve a
total of 6 points F-measure over the baseline.

Thereafter we experiment with the Bengali
datasets and its results are shown in Table 3. It
shows how the performance can be improved with
the use of unsupervised PoS tag and DT features.
Although there is not much difference in the scores
between the results obtained in the first two exper-
iments, there is substantial reduction in the feature
count. Again, recall is increased at cost of preci-
sion, as unsupervised features add coverage, but
also noise at subsuming lower-level features. The
performance obtained using unsupervised features
are quite encouraging and comparable to the exist-
ing works (for both Hindi and Bengali). This also
open a new direction for performing similar kinds
of works in the resource-poor languages.

Feature set F1–measure FC
No unsupervised PoS Tag 72.44 30
and DT features
With unsupervised PoS Tag 72.72 14
With unsupervised PoS Tag 73.50 21
and DT features

Table 3: NERC performance for Bengali data–set,
No. of Generations=50, Size of population=52

7 Experiments for NERC on German

In this section we report on our experiments for
NERC in German language. For each token we ex-
tract twelve features including lexical features, un-
supervised PoS tag and three most similar words
from DT. We compute the values of these features
at the preceding and succeeding tokens. We use
the default parameter values of CRF and set of the

parameters of NSGA-II as mentioned in the previ-
ous section.

Table 4 depicts the performance for NERC task
on German dataset for the baseline model, which
is constructed without using any unsupervised
lexical acquisition features. Table also presents
the results of the models which are constructed
after incorporation of lexical acquisition features.
For the baseline model, feature selection algo-
rithm selects the solution representing 20 features
for training CRF classifier. We obtain precision,
recall and F1 measure of 80.43%, 64.11% and
71.35%, respectively.

Features Tag Precision Recall F1 FC
Syntactic fea-
tures only
(Baseline)

LOC 77.36% 67.94% 72.34

MISC 80.52% 30.10% 43.82
ORG 73.47% 59.76% 65.91
PER 86.83% 68.68% 76.70

Overall 80.43% 64.11% 71.35 20
Syntactic + DT
features

LOC 81.40% 69.93% 75.23
MISC 79.22% 29.61% 43.11
ORG 74.50% 57.02% 64.60
PER 88.31% 72.40% 79.56

Overall 82.89% 65.72% 73.31 19
Syntactic + DT
+ Unsupervised
PoS features

LOC 84.87% 72.60% 78.26

MISC 79.75% 30.58% 44.21
ORG 74.64% 61.99% 67.73
PER 93.07% 82.15% 87.27

Overall 86.21% 71.52% 78.18 21

Table 4: NERC performance for German data–set,
No. of Generations=50, Size of population=52

In the next experiment on German dataset with
DT features incorporated, we obtain improve-
ments in both precision and recall, which causes
substantial improvement in F1. Lexical expan-
sion reduces the chances of unseen instances dur-
ing testing, which results in higher F1 measure
with one less number of features. The third ex-
periment includes three DT features as well as the
unsupervised PoS tag in the available set of fea-
tures for feature selection. It is evident that we ob-
tain significant improvements for both recall and
precision, which in turn causing higher F1 mea-
sure. Over the baseline we obtain an improvement
of 6.83 in F1 measure with the 21 most relevant
features. The best solution includes all the four
unsupervised lexical acquisition features.

8 Conclusion

In this present work, we proposed a unsuper-
vised lexical acquisition and MOO-based tech-111



nique for building NERC systems. It has been
consistently observed that incorporation of un-
supervised lexical acquisition features and using
MOO-based feature selection result in significant
improvement in NERC performance for a vari-
ety of languages. The performance of our models
compares favourably with other works in the lit-
erature (Tjong Kim Sang and De Meulder, 2003).
Also, we present a framework that can easily be
transferred to the other languages and applica-
tions.

In future we would like to include more lan-
guage independent features. Rather than selecting
a single best-fitting feature set from best popula-
tion produced by MOO algorithm, we would like
to combine an ensemble of several classification
systems based on different feature sets and/or dif-
ferent classification techniques.

References
Chris Biemann, Claudio Giuliano, and Alfio Gliozzo.

2007a. Unsupervised part-of-speech tagging sup-
porting supervised methods. In Proceedings of
RANLP, volume 7.

Chris Biemann, Gerhard Heyer, Uwe Quasthoff, and
Matthias Richter. 2007b. The Leipzig Corpora Col-
lection - Monolingual corpora of standard size. In
Proceedings of Corpus Linguistics 2007, Birming-
ham, UK.

Chris Biemann. 2009. Unsupervised part-of-speech
tagging in the large. Research on Language and
Computation, 7(2-4):101–135.

Peter F. Brown, Peter V. deSouza, Robert L. Mer-
cer, Vincent J. Della Pietra, and Jenifer C. Lai.
1992. Class-based n-gram models of natural lan-
guage. Comput. Linguist., 18(4):467–479, Decem-
ber.

Christos Christodoulopoulos, Sharon Goldwater, and
Mark Steedman. 2010. Two decades of unsuper-
vised POS induction: How far have we come? In
Proceedings of EMNLP, pages 575–584.

Alexander Clark. 2003. Combining distributional and
morphological information for part of speech induc-
tion. In In proceedings of European chapter of the
Association for Computational Linguistics (EACL-
03), pages 59–66.

Kalyanmoy Deb, Amrit Pratap, Sameer Agarwal, and
T. Meyarivan. 2002. A fast and elitist multiobjective
genetic algorithm: NSGA-II. IEEE Transactions on
Evolutionary Computation, 6(2):181–197.

Kalyanmoy Deb. 2001. Multi-objective Optimization
Using Evolutionary Algorithms. John Wiley and
Sons, Ltd, England.

Asif Ekbal and Sivaji Bandyopadhyay. 2008. A
web-based Bengali news corpus for named entity
recognition. Language Resources and Evaluation,
42(2):173–182.

Asif Ekbal and Sriparna Saha. 2011a. Multiobjective
Optimization for Classifier Ensemble and Feature
Selection: An Application to Named Entity Recog-
nition. International Journal on Document Analysis
and Recognition (IJDAR), 8.

Asif Ekbal and Sriparna Saha. 2011b. Weighted vote-
based classifier ensemble for named entity recog-
nition: A genetic algorithm-based approach. ACM
Trans. Asian Lang. Inf. Process., 10(2):9.

Zellig S. Harris. 1951. Methods in Structural Linguis-
tics. University of Chicago Press, Chicago.

John D. Lafferty, Andrew McCallum, and Fernando
C. N. Pereira. 2001. Conditional Random Fields:
Probabilistic Models for Segmenting and Labeling
Sequence Data. In ICML, pages 282–289.

Dekang Lin. 1998. Automatic retrieval and clus-
tering of similar words. In 36th Annual Meet-
ing of the Association for Computational Linguis-
tics and the 17th International Conference on Com-
putational Linguistics, pages 768–774, Stroudsburg,
USA. ACM Press.

Tristan Miller, Chris Biemann, Torsten Zesch, and
Iryna Gurevych. 2012. Using distributional similar-
ity for lexical expansion in knowledge-based word
sense disambiguation. Proceedings of COLING–12,
Mumbai, India.

David Nadeau and Satoshi Sekine. 2007. A sur-
vey of named entity recognition and classification.
Lingvisticae Investigationes, 30(1):3–26.

Martin Riedl and Chris Biemann. 2013. Scaling to
large3 data: An efficient and effective method to
compute distributional thesauri. In EMNLP, pages
884–890.

Sokratis Sofianopoulos and George Tambouratzis.
2010. Multi-objective optimisation of real-valued
parameters of a hybrid mt system using genetic algo-
rithms. Pattern Recognition Letters, 31(12):1672–
1682.

Erik F. Tjong Kim Sang and Fien De Meulder.
2003. Introduction to the CoNLL-2003 shared task:
Language-independent named entity recognition. In
Walter Daelemans and Miles Osborne, editors, Pro-
ceedings of CoNLL-2003, pages 142–147. Edmon-
ton, Canada.

112


