




































Modeling Recurrence for Transformer


Proceedings of NAACL-HLT 2019, pages 1198–1207
Minneapolis, Minnesota, June 2 - June 7, 2019. c©2019 Association for Computational Linguistics

1198

Modeling Recurrence for Transformer

Jie Hao
Florida State University
haoj8711@gmail.com

Xing Wang
Tencent AI Lab

brightxwang@tencent.com

Baosong Yang
University of Macau

nlp2ct.baosong@gmail.com

Longyue Wang
Tencent AI Lab

vinnylywang@tencent.com

Jinfeng Zhang
Florida State University

jinfeng@stat.fsu.edu

Zhaopeng Tu∗
Tencent AI Lab

zptu@tencent.com

Abstract

Recently, the Transformer model (Vaswani
et al., 2017) that is based solely on attention
mechanisms, has advanced the state-of-the-art
on various machine translation tasks. How-
ever, recent studies reveal that the lack of re-
currence hinders its further improvement of
translation capacity (Chen et al., 2018; De-
hghani et al., 2019). In response to this
problem, we propose to directly model recur-
rence for Transformer with an additional re-
currence encoder. In addition to the stan-
dard recurrent neural network, we introduce
a novel attentive recurrent network to lever-
age the strengths of both attention and recur-
rent networks. Experimental results on the
widely-used WMT14 English⇒German and
WMT17 Chinese⇒English translation tasks
demonstrate the effectiveness of the proposed
approach. Our studies also reveal that the
proposed model benefits from a short-cut that
bridges the source and target sequences with
a single recurrent layer, which outperforms its
deep counterpart.

1 Introduction

Recently, Transformer (Vaswani et al., 2017) – a
new network architecture based solely on atten-
tion mechanisms, has advanced the state-of-the-art
on various translation tasks across language pairs.
Compared with the conventional recurrent neu-
ral network (RNN) (Schuster and Paliwal, 1997)
based model that leverages recurrence as the ba-
sic building module (Sutskever et al., 2014; Bah-
danau et al., 2015; Chen et al., 2018), Transformer
replaces RNN with self-attention network (SAN)
to model the dependencies among input elements.
One appealing strength of SAN is that it breaks

∗ Zhaopeng Tu is the corresponding author of the paper.
This work was conducted when Jie Hao and Baosong Yang
were interning at Tencent AI Lab.

down the sequential assumption to obtain the abil-
ity of highly parallel computation: input elements
interact with each other simultaneously without
regard to their distance.

However, prior studies empirically show that
the lack of recurrence modeling hinders Trans-
former from further improvement of translation
quality (Dehghani et al., 2019). Modeling re-
currence is crucial for capturing several essen-
tial properties of input sequence, such as struc-
tural representations (Tran et al., 2016) and posi-
tional encoding (Shaw et al., 2018), which are ex-
actly the weaknesses of SAN (Tran et al., 2018).
Recently, Chen et al. (2018) show that the rep-
resentations learned by SAN-based and RNN-
based encoders are complementary to each other,
and merging them can improve translation perfor-
mance for RNN-based NMT models.

Starting from these findings, we propose to di-
rectly model recurrence for Transformer with an
additional recurrence encoder. The recurrence en-
coder recurrently reads word embeddings of input
sequence and outputs a sequence of hidden states,
which serves as an additional information source
to the Transformer decoder. In addition to the stan-
dard RNN, we propose to implement recurrence
modeling with a novel attentive recurrent network
(ARN), which combines advantages of both SAN
and RNN. Instead of recurring over the individ-
ual symbols of sequences like RNN, the ARN re-
currently revises its representations over a set of
feature vectors, which are extracted by an atten-
tion model from the input sequence. Accordingly,
ARN combines the strong global modeling capac-
ity of SAN with the recurrent bias of RNN.

We evaluate the proposed approach on widely-
used WMT14 English⇒German and WMT17
Chinese⇒English translation tasks. Experimen-
tal results show that the additional recurrence en-
coder, implemented with either RNN or ARN,



1199

Source

Multi-Head 
Attention

Feed 
Forward

Add & Norm

Add & Norm

⊕☯Positional Encoding

N×

Softmax

Output Probabilities

Recurrence 
Modeling

Feed 
Forward

Norm

Add & Norm

Source 
Embedding

Multi-Head Attention

Add & Norm

N
th

 D
ec

od
er

 L
ay

er

Se
lf-

A
tte

nt
io

n 
E

nc
od

er

R
ec

ur
re

nc
e 

E
nc

od
er

Source 
Embedding

Source

Multi-Head 
Attention

Feed 
Forward

Add & Norm

Add & Norm

⊕☯Positional Encoding

N×

Target 
Embedding

Target

Multi-Head 
Attention

Feed 
Forward

Add & Norm

Add & Norm

⊕ ☯ Positional Encoding

N×Multi-Head 
Attention

Add & Norm

Softmax

Output 
Probabilities

[AAAI2019] Recurrence Encoder

Multi-Head Attention

Add & Norm

Feed Forward

Add & Norm

Multi-Head Attention

Add & Norm

Figure 1: The architecture of Transformer.

consistently improves translation performance,
demonstrating the necessity of modeling recur-
rence for Transformer. Specifically, the ARN im-
plementation outperforms its RNN counterpart,
which confirms the strength of ARN.

Further analyses reveal that our approach ben-
efits from a short-cut that bridges the source and
target sequences with shorter path. Among all the
model variants, the implementation with shortest
path performs best, in which the recurrence en-
coder is single layer and its output is only fed
to the top decoder layer. It consistently out-
performs its multiple deep counterparts, such as
multiple-layer recurrence encoder and feeding the
output of recurrence encoder to all the decoder
layers. In addition, our approach indeed gener-
ates more informative encoder representations, es-
pecially representative on syntactic structure fea-
tures, through conducting linguistic analyses on
probing tasks (Conneau et al., 2018).

2 Background

Figure 1 shows the model architecture of Trans-
former. The encoder is composed of a stack of N
identical layers, each of which has two sub-layers.
The first sub-layer is a self-attention network, and
the second one is a position-wise fully connected
feed-forward network. A residual connection (He
et al., 2016) is employed around each of two sub-
layers, followed by layer normalization (Ba et al.,

2016). Formally, the output of the first sub-layer
Cne and the second sub-layer H

n
e are sequentially

calculated as:

Cne = LN
(
SELF-ATT(Hn−1e ) +H

n−1
e

)
, (1)

Hne = LN
(
FFN(Cne ) +C

n
e

)
, (2)

where SELF-ATT(·), LN(·), and FFN(·) are re-
spectively self-attention mechanism, layer nor-
malization, and feed-forward network with ReLU
activation in between.

In transformer, SELF-ATT(·) computes atten-
tion over the input Hn−1e as follows:

SELF-ATT(Hn−1e ) = softmax(
QK>√
dk

)V (3)

where {Q,K,V} are query, key and value vectors
that are transformed from the input representations
Hn−1e .

√
dk is the scaling factor where the dk is

the dimension size of the query and key vectors.
The decoder is also composed of a stack of N

identical layers. In addition to two sub-layers in
each decoder layer, the decoder inserts a third sub-
layer Dnd to perform attention over the output of
the encoder HNe :

Cnd = LN
(
SELF-ATT(Hn−1d ) +H

n−1
d

)
, (4)

Dnd = LN
(
ATT(Cnd ,H

N
e ) +C

n
d

)
, (5)

Hnd = LN
(
FFN(Dnd ) +D

n
d

)
, (6)

where ATT(Cnd ,H
N
e ) denotes attending the top

encoder layer HNe with C
n
d as query. The top layer

of the decoder HNd is used to generate the final
output sequence.

3 Approach

In this section, we first describe the architecture
of the introduced recurrence encoder and elaborate
two types of neural network that are used as recur-
rence encoder in this work. Then we introduce the
integration of recurrence encoder into the Trans-
former. Specifically, two strategies are presented
to fuse the representations produced by the recur-
rence encoder and the conventional encoder. Fi-
nally we present the short-cut connection between
the recurrence encoder and the decoder that we
found very effective to use the learned representa-
tion to improve the translation performance under
the proposed architecture.



1200

Output of  
Transformer Encoder

Output of  
Recurrence Encoder

Source

Multi-Head 
Attention

Feed 
Forward

Add & Norm

Add & Norm

⊕☯Positional Encoding

N×

Recurrence 
Modeling

Feed 
Forward

Norm

Add & Norm

Source 
Embedding

Se
lf-

A
tte

nt
io

n 
E

nc
od

er

R
ec

ur
re

nc
e 

E
nc

od
er

e1 e2 e3 e4 e5 e6

h1 h2 h3 h4 h5 h6!

e1 e2 e3 e4 e5 e6

c1 c2

h1 h2!

N×

Figure 2: The architecture of Transformer augmented
with an additional recurrence encoder, the output of
which is directly fed to the top decoder layer.

3.1 Recurrence Modeling

Figure 2 shows the architecture of the introduced
recurrence encoder which reads word embeddings
of source words and outputs a sequence of hidden
states that embeds recurrent information. Similar
to the Transformer encoder, it has a stack of N
identical layers, each of which has two sub-layers.
The first one is a recurrence modeling network and
the second is a fully connected feed-forward net-
work:

Cnr = LN(REC(H
n−1
r ) +H

n−1
r ), (7)

Hnr = LN(FFN(C
n
r ) +C

n
r ), (8)

where REC(·) is the function of recurrence mod-
eling. Note that at the bottom layer of the recur-
rence encoder (N=1), we do not employ a resid-
ual connection on the recurrence sub-layer (i.e.
Equation 7), which releases the constraint that C1r
should share the same length with input embed-
dings sequence Ein1. This offers a more flexible
choice of the recurrence functions.

There are many possible ways to implement the
general idea of recurrence modeling REC(·). The
aim of this paper is not to explore this whole space
but simply to show that some fairly straightfor-
ward implementations work well. In this work, we
investigate two representative implementations,
namely RNN and its variation attentive current
network that combines advantages of both RNN
and attention models, as shown in Figure 3.

1The input of the lowest layer in the recurrence encoder is
the word embeddings of input sequence Ein.

Output of  
Transformer Encoder

Output of  
Recurrence Encoder

Source

Multi-Head 
Attention

Feed 
Forward

Add & Norm

Add & Norm

⊕☯Positional Encoding

N×

Recurrence 
Modeling

Feed 
Forward

Norm

Add & Norm

Source 
Embedding

Se
lf-

A
tte

nt
io

n 
E

nc
od

er

R
ec

ur
re

nc
e 

E
nc

od
er

e1 e2 e3 e4 e5 e6

h1 h2 h3 h4 h5 h6

e1 e2 e3 e4 e5 e6

c1 c2

h1 h2

N×

h0

h0

(a) Recurrent Neural Network

Output of  
Transformer Encoder

Output of  
Recurrence Encoder

Source

Multi-Head 
Attention

Feed 
Forward

Add & Norm

Add & Norm

⊕☯Positional Encoding

N×

Recurrence 
Modeling

Feed 
Forward

Norm

Add & Norm

Source 
Embedding

Se
lf-

A
tte

nt
io

n 
E

nc
od

er

R
ec

ur
re

nc
e 

E
nc

od
er

e1 e2 e3 e4 e5 e6

h1 h2 h3 h4 h5 h6

e1 e2 e3 e4 e5 e6

c1 c2

h1 h2

N×

h0

h0

(b) Attentive Recurrent Network

Figure 3: Two implementations of recurrence model-
ing: (a) standard RNN, and (b) the proposed ARN.

Recurrent Neural Network (RNN) An intu-
itive choice of recurrence modeling is RNN, which
is a standard network to model sequence orders. In
this work, we use a bidirectional RNN (BiRNN),
which is widely applied in RNN-based NMT mod-
els (Bahdanau et al., 2015; Chen et al., 2018).
Each hidden state in the output representations
HnRNN = {hn1 , . . . ,hnJ} is calculated as

hnj =
[−→
h j ;
←−
h j
]
, (9)

−→
h j =

−→
f (
−→
h j−1,h

n−1
j ), (10)

←−
h j =

←−
f (
←−
h j+1,h

n−1
j ), (11)

where
−→
f (·) and

←−
f (·) are the activation functions

of forward and backward RNN respectively, which
can be implemented as LSTM (Hochreiter and
Schmidhuber, 1997) or GRU (Cho et al., 2014).
hn0 is the initial state of RNN, which is the mean
of Hn−1RNN . H

0
RNN represents the word embeddings

of the input sequence.

Attentive Recurrent Network (ARN) We can
also extend RNN by recurring over a set of feature
vectors extracted with an attention model, which
allows the model to learn a compact, abstractive
feature vectors over the input sequence. Specifi-
cally, the ARN performs T recurrent steps on the
attentive output of the input representation Hn−1r :

hnt = f(h
n
t−1, c

n
t ), (12)

cnt = ATT(h
n
t−1,H

n−1
r ). (13)

The output representations HnARN =
{hn1 , . . . ,hnT } are fed to the subsequent modules.
Analogous to Equations 9-11, ARN can be
extended to the bidirectional variant, i.e. BiARN,



1201

Softmax

Output Probabilities

Multi-Head Attention

Add & Norm

[AAAI2019] Integration

Multi-Head Attention

Add & Norm

Feed Forward

Add & Norm

Multi-Head Attention

Add & Norm

Multi-Head 
Attention

Add & Norm

Multi-Head 
Attention

Add & Norm

⊕

Multi-Head Attention

Add & Norm

Gated Sum

Softmax

Output Probabilities

Feed Forward

Add & Norm

Target 
Embedding

Target

⊕ ☯
PosEnc

Target 
Embedding

Target

⊕ ☯Output of  
Transformer  
Encoder

Output of  
Recurrence  

Encoder

PosEncOutput of  
Transformer  
Encoder

Output of  
Recurrence  

Encoder

N× N×

(a) Gated Sum

Softmax

Output Probabilities

Multi-Head Attention

Add & Norm

[AAAI2019] Integration

Multi-Head Attention

Add & Norm

Feed Forward

Add & Norm

Multi-Head Attention

Add & Norm

Multi-Head 
Attention

Add & Norm

Multi-Head 
Attention

Add & Norm

⊕

Multi-Head Attention

Add & Norm

Gated Sum

Softmax

Output Probabilities

Feed Forward

Add & Norm

Target 
Embedding

Target

⊕ ☯
PosEnc

Target 
Embedding

Target

⊕ ☯Output of  
Transformer  
Encoder

Output of  
Recurrence  

Encoder

PosEncOutput of  
Transformer  
Encoder

Output of  
Recurrence  

Encoder

N× N×

(b) Stack

Figure 4: Different strategies to integrate the output of the additional recurrence encoder into the decoder.

except that the input is the attentive context vector
cnt rather than the individual representation vector
of the input sequence.

Note that, the number of recurrence step T is
allowed to be unequal to the length of input se-
quence J . In contrast to RNN which recurs over
the individual symbols of the input sequences,
ARN recurrently revises its representations of all
symbols in the sequence with an attention model.

3.2 Integrating into Transformer

Since the output of recurrence encoder unneces-
sarily shares the same length with that of Trans-
former encoder (e.g. when ARN is used as recur-
rence function), combination strategy on the en-
coder side, such as concatenating the outputs of
both encoders (Chen et al., 2018), is not an uni-
versal solution in this scenario. Accordingly, we
feed the information of the additional recurrence
encoder into the decoder of Transformer. Specifi-
cally, we serve an additional attention layer Rnd as
the fourth sub-layer in each decoder block to per-
form attention over the output of the recurrence
encoder HNr . As shown in Figure 4, we present
two strategies to integrate Rnd , namely gated sum
and stack, which differ at how Rnd interacts with
the output of attention over the Transformer en-
coder, i.e., Dnd in Equation 5.

Gated Sum The first strategy combines the out-
puts of the two attention sub-layers in a gating fu-
sion (Figure 4(a)), in which the outputs of both
encoders are attended simultaneously:

Rnd = LN
(
ATT(Cnd ,H

N
r ) +C

n
d

)
, (14)

D̂nd = λnD
n
d + (1− λn)Rnd , (15)

Hnd = LN
(
FFN(D̂nd ) + D̂

n
d

)
, (16)

where λn is an interpolation weight calculated by
a logistic sigmoid function:

λn = sigmoid(D
n
d ,R

n
d ) (17)

As seen, the output of self-attention layer Cnd
serves as a query to attend the outputs of both en-
coders (Equations 5 and 14), and the outputs of
both attention models {Dnd ,Rnd} are combined via
a gated sum (Equation 15), which is subsequently
fed to the feed-forward layer (Equation 16).

Stack We can also arrange the sub-layers in a
stack (Figure 4(b)), in which the outputs of both
encoders are attended sequentially:

Rnd = LN
(
ATT(Dnd ,H

N
r ) +D

n
d

)
, (18)

Hnd = LN
(
FFN(Rnd ) +R

n
d

)
, (19)

The decoder first attends the output of Trans-
former encoder, and the attention output Dnd
serves as the query to attend the output of recur-
rence encoder (Equation 18).



1202

3.3 Short-Cut Effect

The introduced recurrence encoder provides an
additional computation path ranging from the
input sequence to the output sequence. Chung
et al. (2017) and Shen et al. (2019) have shown
that a shortcut for gradient back-propagation ben-
efits language modeling. Inspired from them, we
use a shorter path to transform the learned recur-
rence. We call this the “short-cut effect”.

Among all the model variants, we implement
shortest path as: the recurrence encoder is single
layer and its output is only fed to the top decoder
layer while the first N − 1 decoder layers perform
the same as the standard Transformer (e.g. Equa-
tions 4-6). Accordingly, the computation path is
Ein → Hr → RNd → HNd , then the decoder
uses HNd to make a target word prediction. It is
much simpler than that of the conventional Trans-
former, which transfers information learned from
input sequences across multiple stacking encoder
and decoder layers. We expect it outperforms its
multiple deep counterparts, such as multiple-layer
recurrence encoder and feeding the output of re-
currence encoder to all the decoder layers.

4 Related Work

Improving Transformer Encoder From the
perspective of representation learning, there has
been an increasing amount of work on improving
the representation power of SAN encoder. Baw-
den et al. (2018) and Voita et al. (2018) exploit
external context for SAN encoder, while Yang
et al. (2019) leverage the intermediate representa-
tions to contextualize the transformations in SAN.
A number of recent efforts have explored ways
to improve multi-head SAN by encouraging in-
dividual attention heads to extract distinct infor-
mation (Strubell et al., 2018; Li et al., 2018).
Concerning multi-layer SAN encoder, Dou et al.
(2018, 2019) and Wang et al. (2018) propose to
aggregate the multi-layer representations, and De-
hghani et al. (2019) recurrently refine these rep-
resentations. Our approach is complementary to
theirs, since they focus on improving the repre-
sentation power of SAN encoder, while we aim to
complement SAN encoder with an additional re-
currence encoder.

Along the direction of modeling recurrence for
SAN, Vaswani et al. (2017) and Shaw et al. (2018)
inject absolute position encoding and relative po-
sitional encoding to consider the position informa-

tion respectively. Shen et al. (2018) introduce a di-
rectional self-attention network (DiSAN), which
allows each token to attend to previous (or fol-
lowing) tokens only. Both studies verify the ne-
cessity of modeling recurrence for SAN. We re-
implemented these approaches on top of Trans-
former, and experimental results show that our ap-
proach outperforms them by explicitly augment-
ing Transformer with an additional recurrence en-
coder. It should be emphasized that our approach
is complementary to theirs, and combining them
together is expected to further improve perfor-
mance, which we leave for future work.

Closely related to our work, Chen et al. (2018)
propose to combine SAN encoder with an addi-
tional RNN encoder. The main differences be-
tween our work and theirs are: 1) we enhance the
state-of-the-art Transformer with recurrence infor-
mation, while Chen et al. (2018) augment RNN-
based models with SAN encoder. To this end,
we propose a novel attentive recurrent network
to implement the additional recurrence encoder in
Transformer. We re-implemented the approach
proposed by Chen et al. (2018) on top of Trans-
former. Experimental results indicate the superi-
ority of our approach, which confirms our claim.
In addition, we elaborately design the integration
strategy to effectively feed the recurrence informa-
tion to the decoder, and empirically show that the
proposed model benefits from the short-cut effect.

Comparison to Reviewer Network Attentive
recurrent network are inspired by the reviewer net-
work, which is proposed by Yang et al. (2016) for
the image caption generation task. There are two
key differences which reflect how we have gener-
alized from the original model. First, we perform
attention steps over the source embeddings instead
of the encoder representations. The main reason
is that the Transformer encoder is implemented as
multiple layers, and higher layers generally en-
code global information, as indicated by Peters
et al. (2018). Second, we feed the feature vec-
tors together with the original encoder represen-
tations to the decoder. In image caption genera-
tion, the source side (i.e. image) contains much
more information than the target side (i.e. cap-
tion) (Tu et al., 2017). Therefore, they aim at
learning a compact and abstractive representation
from the source information, which serves as the
only input to the decoder. In this work, we focus
on leveraging the attention model to better learn



1203

the recurrence, which we expect to complement
the Transformer model. In our preliminary exper-
iments, attending over the encoder representations
does not improve performance, while feeding the
feature vectors only to the decoder seriously harms
performance.

5 Experiment

5.1 Setup

We conducted experiments on the widely-
used WMT14 English-to-German (4.6M sentence
pairs, En⇒De) and WMT17 Chinese-to-English
(20.6M sentence pairs, Zh⇒En) translation tasks.
All the data had been tokenized and segmented
into subword symbols using byte-pair encoding
(Sennrich et al., 2016) with 32K merge opera-
tions2. We used case-sensitive NIST BLEU score
(Papineni et al., 2002) as the evaluation metric,
and bootstrap resampling (Koehn et al., 2003) for
statistical significance test.

We implemented the proposed approaches on
top of the Transformer model (Vaswani et al.,
2017). Both in our model and related model of
Subsection 5.3, the RNN is implemented with
GRU (Cho et al., 2014) for fair comparison.
We followed the configurations in Vaswani et al.
(2017), and reproduced their reported results on
the En⇒De task.

We initialized parameters of the proposed mod-
els by the pre-trained baseline model. We have
tested both Base and Big models, which differ at
hidden size (512 vs. 1024), filter size (2048 vs.
4096), and number of attention heads (8 vs. 16).
In consideration of computation cost, we stud-
ied model variations with Base model on En⇒De
task, and evaluated overall performances with both
Base and Big models on both En⇒De and Zh⇒En
translation tasks.

5.2 Impact of Components

In this subsection, we conducted ablation studies
to evaluate the different implementations of the
proposed model, e.g., recurrence encoder and inte-
gration strategy, under the proposed architecture.

Effect of Recurrence Modeling We first inves-
tigated the effect of recurrence encoder imple-
mentations, as listed in Table 1. We observed
that introducing an additional recurrence encoder
improves translation performance in all cases.

2https://github.com/rsennrich/subword-nmt

Model Rec. Encoder Speed BLEU
BASE n/a 1.28 27.31

OURS

6-Layer BIRNN 1.10 27.54
6-Layer BIARN 1.09 27.72
3-Layer BIARN 1.15 28.10
1-Layer BIARN 1.24 28.21

Table 1: Evaluation of recurrence encoder implemen-
tations. The output of recurrence encoder is fed to the
top decoder layer in a stack fusion. “Speed” denotes
the training speed (steps/second).

Model Integration to Dec. BLEU
BASE n/a n/a 27.31

OURS

Gated Sum Top 28.12
Gated Sum All 28.02
Stack Top 28.21
Stack All 27.93

Table 2: Evaluation of decoder integration strategies.

Among all model variations, BIARN outperforms
its BIRNN counterpart.

Concerning BIARN models, reducing the lay-
ers consistently improves performance. Specifi-
cally, the 1-Layer BIARN achieves the best per-
formances in both translation quality and train-
ing speed. This confirms the claim that the pro-
posed approach benefits from a short-cut on gra-
dient back-propagation. Accordingly, we adopted
1-Layer BIARN as the default setting in the fol-
lowing experiments.

Effect of Integration Strategies We then tested
the effect of different integration strategies, as
showed in Table 2. We have two observations.
First, feeding only to the top decoder layer con-
sistently outperforms feeding to all decoder lay-
ers with different integration strategies. This em-
pirically reconfirms the short-cut effect. Second,
the stack strategy marginally outperforms its gated
sum counterpart. Therefore, in the following ex-
periments, we adopted the “Stack + Top” model in
Table 2 as defaulting setting.

5.3 Results

Performances across Languages Finally, we
evaluated the proposed approach on the widely
used WMT17 Zh⇒En and WMT14 En⇒De data,
as listed in Table 3.

To make the evaluation convincing, we re-
viewed the prior reported systems, and built strong



1204

System Architecture Zh⇒En En⇒De
# Para. BLEU # Para. BLEU

Existing NMT systems

(Vaswani et al., 2017)
TRANSFORMER-BASE n/a n/a 65M 27.3
TRANSFORMER-BIG n/a n/a 213M 28.4

(Hassan et al., 2018) TRANSFORMER-BIG n/a 24.2 n/a n/a
(Chen et al., 2018) RNMT + SAN Encoder n/a n/a n/a 28.84

Our NMT systems

this work

TRANSFORMER-BASE 107.9M 24.13 88.0M 27.31
+ 1-Layer BIARN +9.4M 24.70⇑ +9.4M 28.21⇑

TRANSFORMER-BIG 303.9M 24.56 264.1M 28.58
+ 1-Layer BIARN +69.4M 25.10⇑ +69.4M 28.98↑

Table 3: Comparing with the existing NMT systems on WMT17 Zh⇒En and WMT14 En⇒De test sets. “↑ / ⇑”:
significant over the conventional self-attention counterpart (p < 0.05/0.01), tested by bootstrap resampling.

Model BLEU
TRANSFORMER-BASE 27.31
+ RELPOS 27.64
+ DISAN 27.58
+ RNN Encoder 27.47
+ BIARN Encoder (OURS) 28.21

Table 4: Comparison with re-implemented related
work: “RELPOS”: relative position encoding (Shaw
et al., 2018), “DISAN”: directional SAN (Shen et al.,
2018), “RNN Encoder”: combining SAN and RNN en-
coders with multi-column strategy (Chen et al., 2018).

baselines which outperform the reported results
on the same data. As seen in Table 3, model-
ing recurrence consistently improves translation
performance across model variations (BASE and
BIG models) and language pairs (Zh⇒En and
En⇒De), demonstrating the effectiveness and uni-
versality of our approach.

Comparison with Previous Work In order to
directly compare our approach with the previous
work on modeling recurrence, we re-implemented
their approaches on top of the TRANSFORMER-
BASE in WMT14 En⇒De translation task. For
relative position encoding, we used unique edge
representations per layer and head with clipping
distance k = 16. For the DiSAN strategy, we
applied a mask to the TRANSFORMER encoder,
which constrains the SAN to focus on forward
or backward elements. For the multi-column en-
coder, we re-implemented the additional encoder
with six RNN layers.

Table 4 lists the results. As seen, all the re-

currence enhanced approaches achieve improve-
ments over the baseline model TRANSFORMER-
BASE, which demonstrates the necessity of model-
ing recurrence for TRANSFORMER. Among these
approaches, our approach (i.e., 1-Layer BIARN
Encoder) achieves the best performance.

5.4 AnalysisBL
EU

25

26

27

28

29

30

Length of Source Sentence

(0,
 15

]

(15
, 3

0]

(30
, 4

5]
> 4

5

Ours
Base

B
LE

U

26.6

26.7

26.8

26.9

27.0

Number of Output Capsules

1 2 4 8 16 32 64 12
8

25
6

51
2

B
LE

U

27.0

27.3

27.6

27.9

28.2

28.5

Number of Recurrent Steps

1 4 8 16 J

27.31
O
URS
1 step
27.50
4 step
27.99
8 step
28.21
16 step
27.92
equal to sentence length
27.33

Figure 5: Effect of recurrent steps. The recurrence en-
coder is implemented as a single-layer BIARN. J de-
notes the length of the input sequence.

Effect of Recurrent Steps To verify the re-
currence effect on the proposed model, we con-
ducted experiments with different recurrent steps
on single-layer BIARN model. As shown in Fig-
ure 5, the BLEU score typically goes up with the
increase of the recurrent steps, while the trend
does not hold when T > 8. This finding is consis-
tent with Yang et al. (2016), which indicates that
conducting too many recurrent steps fails to gen-
erate a compact representation. This is exactly one
of the ARN’s strengths.



1205

Model
Surface Syntactic Semantic

SeLen WC TrDep ToCo BShif Tense SubN ObjN SoMo CoIn

BASE 92.20 63.00 44.74 79.02 71.24 89.24 84.69 84.53 52.13 62.47
6-Layer BIRNN 89.90 77.46 44.47 79.55 71.53 89.17 85.99 84.96 51.75 61.92
6-Layer BIARN 89.78 72.02 44.45 79.21 71.31 88.38 85.64 85.00 53.27 62.38
3-Layer BIARN 89.80 72.61 44.28 79.43 71.84 88.93 85.79 84.99 53.30 62.42
1-Layer BIARN 90.91 73.68 45.15 79.62 72.21 89.00 85.54 84.54 53.44 62.71

Table 5: Classification accuracies on 10 probing tasks of evaluating linguistics embedded in the encoder outputs.

Linguistic Analyses In this section, we con-
ducted 10 probing tasks3 to study what linguistic
properties are captured by the encoders (Conneau
et al., 2018). A probing task is a classification
problem that focuses on simple linguistic proper-
ties of sentences. ‘SeLen’ predicts the length of
sentences in terms of number of words. ‘WC’
tests whether it is possible to recover information
about the original words given its sentence embed-
ding. ‘TrDep’ checks whether an encoder infers
the hierarchical structure of sentences. In ‘ToCo’
task, sentences should be classified in terms of
the sequence of top constituents immediately be-
low the sentence node. ‘BShif’ tests whether two
consecutive tokens within the sentence have been
inverted. ‘Tense’ asks for the tense of the main-
clause verb. ‘SubN’ focuses on the number of the
main clause’s subject. ‘ObjN’ tests for the number
of the direct object of the main clause. In ‘SoMo’,
some sentences are modified by replacing a ran-
dom noun or verb with another one and the classi-
fier should tell whether a sentence has been modi-
fied. ‘CoIn’ contains sentences made of two coor-
dinate clauses. Half of sentences are inverted the
order of the clauses and the task is to tell whether
a sentence is intact or modified.

We used the pre-trained encoders of model vari-
ations in Table 1 to generate the sentence represen-
tations of input, which are used to carry out prob-
ing tasks. For the TRANSFORMER-BASE model,
the mean of the encoder top layer representations
is used as the sentence representation. For the pro-
posed models, which have two encoders, two sen-
tence representations are generated from the same
way in base model. To make full use of the learned
representations, we combined these two sentence
representations via a gate as the final sentence rep-
resentation to conduct the experiments.

3https://github.com/facebookresearch/SentEval/tree/master
/data/probing

Table 5 lists the results. Clearly, the proposed
models significantly improve the classification ac-
curacies, although there is still considerable differ-
ence among different variants. More specifically,

• Concerning surface properties, among the
ARN variants, multi-layer ARN inversely de-
creases the accuracies, while 1-layer ARN
consistently improves the accuracies. Con-
sidering the related results presented in Ta-
ble 1 (Row 3-5), we believe that ARN bene-
fits from the shallow structure.

• ARN tends to capture deeper linguistic prop-
erties, both syntactic and semantic. Espe-
cially, among these probing tasks, ‘TrDep’
and ‘Toco’ tasks are related to syntactic struc-
ture modeling. As expected, TRANSFORMER
augmented with an additional encoders out-
performs the baseline model, which demon-
strates that the proposed models successfully
model the syntactic structure.

6 Conclusion

In this work, we propose to directly model re-
currence for Transformer with an additional re-
currence encoder. We implement the recurrence
encoder with a novel attentive recurrent network
as well as RNN. The recurrence encoder is used
to generate recurrence representations for the in-
put sequence. To effectively feed the recurrence
representations to the decoder to guide the out-
put sequence generation, we study two strategies
to integrate the recurrence encoder into the Trans-
former. To evaluate the effectiveness of the pro-
posed model, we conduct experiments on large-
scale WMT14 EN⇒DE and WMT17 ZH⇒EN
datasets. Experimental results on two language
pairs show that the proposed model achieves sig-
nificant improvements over the baseline TRANS-
FORMER. Linguistic analyses on probing tasks



1206

further show that our model indeed generates more
informative representations, especially representa-
tive on syntactic structure features.

Future work includes validating the proposed
model in other tasks, such as reading comprehen-
sion, language inference, and sentence classifica-
tion. Another promising direction is to directly
augment Transformer encoder on recurrence mod-
eling without the additional encoder.

Acknowledgments

J.Z. was supported by the National Institute of
General Medical Sciences of the National Institute
of Health under award number R01GM126558.
We thank the anonymous reviewers for their in-
sightful comments.

References
Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hin-

ton. 2016. Layer normalization. arXiv preprint
arXiv:1607.06450.

Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Ben-
gio. 2015. Neural machine translation by jointly
learning to align and translate. In ICLR.

Rachel Bawden, Rico Sennrich, Alexandra Birch, and
Barry Haddow. 2018. Evaluating Discourse Phe-
nomena in Neural Machine Translation. In NAACL.

Mia Xu Chen, Orhan Firat, Ankur Bapna, Melvin
Johnson, Wolfgang Macherey, George Foster, Llion
Jones, Mike Schuster, Noam Shazeer, Niki Parmar,
Ashish Vaswani, Jakob Uszkoreit, Lukasz Kaiser,
Zhifeng Chen, Yonghui Wu, and Macduff Hughes.
2018. The best of both worlds: Combining recent
advances in neural machine translation. In ACL.

Kyunghyun Cho, Bart Van Merriënboer, Caglar Gul-
cehre, Dzmitry Bahdanau, Fethi Bougares, Holger
Schwenk, and Yoshua Bengio. 2014. Learning
phrase representations using RNN encoder-decoder
for statistical machine translation. In EMNLP.

JunYoung Chung, Sungjin Ahn, and Yoshua Bengio.
2017. Hierarchical Multiscale Recurrent Neural
Networks. In ICLR.

Alexis Conneau, German Kruszewski, Guillaume
Lample, Loı̈c Barrault, and Marco Baroni. 2018.
What you can cram into a single $&!#∗ vector:
Probing sentence embeddings for linguistic proper-
ties. In ACL.

Mostafa Dehghani, Stephan Gouws, Oriol Vinyals,
Jakob Uszkoreit, and Lukasz Kaiser. 2019. Univer-
sal transformers. In ICLR.

Ziyi Dou, Zhaopeng Tu, Xing Wang, Shuming Shi, and
Tong Zhang. 2018. Exploiting deep representations
for neural machine translation. In EMNLP.

Ziyi Dou, Zhaopeng Tu, Xing Wang, Longyue Wang,
Shuming Shi, and Tong Zhang. 2019. Dynamic
layer aggregation for neural machine translation. In
AAAI.

Hany Hassan, Anthony Aue, Chang Chen, Vishal
Chowdhary, Jonathan Clark, Christian Feder-
mann, Xuedong Huang, Marcin Junczys-Dowmunt,
William Lewis, Mu Li, et al. 2018. Achieving hu-
man parity on automatic chinese to english news
translation. arXiv preprint arXiv:1803.05567.

Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian
Sun. 2016. Deep residual learning for image recog-
nition. In CVPR.

Sepp Hochreiter and Jürgen Schmidhuber. 1997.
Long short-term memory. Neural computation,
9(8):1735–1780.

Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In ACL.

Jian Li, Zhaopeng Tu, Baosong Yang, Michael R. Lyu,
and Tong Zhang. 2018. Multi-Head Attention with
Disagreement Regularization. In EMNLP.

Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a method for automatic eval-
uation of machine translation. In ACL.

Matthew E. Peters, Mark Neumann, Mohit Iyyer, Matt
Gardner, Christopher Clark, Kenton Lee, and Luke
Zettlemoyer. 2018. Deep contextualized word rep-
resentations. In NAACL.

Mike Schuster and Kuldip K Paliwal. 1997. Bidirec-
tional recurrent neural networks. IEEE Transactions
on Signal Processing, 45(11):2673–2681.

Rico Sennrich, Barry Haddow, and Alexandra Birch.
2016. Neural machine translation of rare words with
subword units. In ACL.

Peter Shaw, Jakob Uszkoreit, and Ashish Vaswani.
2018. Self-Attention with Relative Position Repre-
sentations. In NAACL.

Tao Shen, Tianyi Zhou, Guodong Long, Jing Jiang,
Shirui Pan, and Chengqi Zhang. 2018. DiSAN: di-
rectional self-attention network for RNN/CNN-free
language understanding. In AAAI.

Yikang Shen, Shawn Tan, Alessandro Sordoni, and
Aaron Courville. 2019. Ordered neurons: Integrat-
ing tree structures into recurrent neural networks. In
ICLR.

Emma Strubell, Patrick Verga, Daniel Andor,
David Weiss, and Andrew McCallum. 2018.
Linguistically-Informed Self-Attention for Seman-
tic Role Labeling. In EMNLP.



1207

Ilya Sutskever, Oriol Vinyals, and Quoc V Le. 2014.
Sequence to sequence learning with neural net-
works. In NIPS.

Ke Tran, Arianna Bisazza, and Christof Monz. 2016.
Recurrent memory networks for language modeling.
In NAACL.

Ke Tran, Arianna Bisazza, and Christof Monz. 2018.
The importance of being recurrent for modeling hi-
erarchical structure. In EMNLP.

Zhaopeng Tu, Yang Liu, Zhengdong Lu, Xiaohua Liu,
and Hang Li. 2017. Context gates for neural ma-
chine translation. TACL.

Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob
Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz
Kaiser, and Illia Polosukhin. 2017. Attention is all
you need. In NIPS.

Elena Voita, Pavel Serdyukov, Rico Sennrich, and Ivan
Titov. 2018. Context-aware neural machine transla-
tion learns anaphora resolution. In ACL.

Qiang Wang, Fuxue Li, Tong Xiao, Yanyang Li, Yin-
qiao Li, and Jingbo Zhu. 2018. Multi-layer repre-
sentation fusion for neural machine translation. In
COLING.

Baosong Yang, Jian Li, Derek F. Wong, Lidia S. Chao,
Xing Wang, and Zhaopeng Tu. 2019. Context-aware
self-attention networks. In AAAI.

Zhilin Yang, Ye Yuan, Yuexin Wu, Ruslan Salakhutdi-
nov, and William W Cohen. 2016. Review networks
for caption generation. In NIPS.


