1036

Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 1036–1041,

Austin, Texas, November 1-5, 2016. c(cid:13)2016 Association for Computational Linguistics

Joint Transition-based Dependency Parsing and Disﬂuency Detection for

Automatic Speech Recognition Texts

Masashi Yoshikawa and Hiroyuki Shindo and Yuji Matsumoto

Graduate School of Information and Science

Nara Institute of Science and Technology

8916-5, Takayama, Ikoma, Nara, 630-0192, Japan

{ masashi.yoshikawa.yh8, shindo, matsu }@is.naist.jp

Abstract

Joint dependency parsing with disﬂuency de-
tection is an important task in speech lan-
guage processing. Recent methods show high
performance for this task, although most au-
thors make the unrealistic assumption that in-
put texts are transcribed by human annota-
tors. In real-world applications, the input text
is typically the output of an automatic speech
recognition (ASR) system, which implies that
the text contains not only disﬂuency noises but
also recognition errors from the ASR system.
In this work, we propose a parsing method that
handles both disﬂuency and ASR errors us-
ing an incremental shift-reduce algorithm with
several novel features suited to ASR output
texts. Because the gold dependency informa-
tion is usually annotated only on transcribed
texts, we also introduce an alignment-based
method for transferring the gold dependency
annotation to the ASR output texts to con-
struct training data for our parser. We con-
ducted an experiment on the Switchboard cor-
pus and show that our method outperforms
conventional methods in terms of dependency
parsing and disﬂuency detection.

1 Introduction
Spontaneous speech is different from written text in
many ways, one of which is that it contains disﬂuen-
cies, that is, parts of the utterance that are corrected
by the speaker during the utterance. NLP system
performance is reported to deteriorate when there
are disﬂuencies, for example, with SMT (Cho et al.,
2014). Therefore, it is desirable to preprocess the
speech before passing it to other NLP tasks.

There are a number of studies that address the
problem of detecting disﬂuencies. Some of these
studies include dependency parsing (Honnibal and
Johnson, 2014; Wu et al., 2015; Rasooli and
Tetreault, 2014), whereas others are dedicated sys-
tems (Qian and Liu, 2013; Ferguson et al., 2015;
Hough and Purver, 2014; Hough and Schlangen,
2015; Liu et al., 2003). Among these studies, Hon-
nibal (2014) and Wu (2015) address this problem by
adding a new action to transition-based dependency
parsing that removes the disﬂuent parts of the in-
put sentence from the stack. Using this approach,
they achieved high performance in terms of both de-
pendency parsing and disﬂuency detection on the
Switchboard corpus.

However, the authors assume that the input texts
to parse are transcribed by human annotators, which,
in practice, is unrealistic. In real-world applications,
in addition to disﬂuencies, the input texts contain
ASR errors; these issues might degrade the parsing
performance. For example, proper nouns that are not
contained in the ASR system vocabulary may break
up into smaller pieces, yielding a difﬁcult problem
for the parsing unit (Cheng et al., 2015):

REF: what can we get at Litanfeeth
HYP: what can we get it leaks on feet

In this work, we propose a method for joint de-
pendency parsing and disﬂuency detection that can
robustly parse ASR output texts. Our parser handles
both disﬂuencies and ASR errors using an incremen-
tal shift-reduce algorithm, with novel features that
consider recognition errors of the ASR system.

Furthermore, to evaluate dependency parsing per-

1037

Figure 1: Examples of three problematic cases. Above shows the gold transcription and its tree, below shows the aligned ASR
output and its newly transferred tree, where the dotted edges are ASR error edges.

formance on real human utterances, we create a tree-
annotated corpus that contains ASR errors. 1

2 Data Creation
To evaluate dependency parsing performance on real
speech texts, we must create a tree-annotated corpus
of ASR output texts.

Given a corpus that consists of speech data, tran-
scription text and its syntactic annotation (e.g., the
Switchboard corpus), we ﬁrst apply the ASR sys-
tem to the speech data. Next, we perform alignment
between the ASR output texts and the transcription.
Then, we transfer the gold syntactic annotation to
the ASR output texts based on this alignment (Fig-
ure 1). The alignment is performed by minimizing
the edit distance between the two sentences. We in-
clude “NULL” tokens in this alignment to allow for
some tokens not having an aligned conterpart (“N”
tokens in the Figure 1).

In the constructed trees, there are three problem-
atic cases based on how an ASR output text and its
transcription are aligned with each other: (1) a word
in the ASR output text aligns with a NULL token
in the transcription (ASR-to-NULL), (2) a word in
the gold transcription aligns with a NULL in the
ASR output (Trans-to-NULL), and (3) two words
align, but do not match exactly in terms of characters
(NOT MATCH). To create a consistent dependency
tree that spans the entire sentence, we must address
each of these cases.

1There are also studies that tackle the problem of disﬂuency
detection in the context of speech recognition such as (Liu et al.,
2003). Our work is novel in that our aim is to extend the joint
method of disﬂuency detection with dependency parsing so that
it can be applicable to the output of ASR system.

2.1 ASR-to-NULL
In the case of ASR-to-NULL, a token from the ASR
system has no corresponding token in the gold tran-
scription. In this case, we automatically annotate a
dependency relation with an “error” label such that
the token’s head becomes the previous word token.
In
the ﬁgure, the words “old” and “way” have no cor-
responding words in the gold transcription. Thus,
we automatically annotate the dependency relations
between (“old”, “uh”) and (“way”, “old”), respec-
tively, with the “error” label.

Figure 1(a) shows an example of this case.

2.2 Trans-to-NULL
Although NULL tokens are introduced to facilitate
alignment, as these tokens in the ASR output are not
actual words, we must remove them in the ﬁnal tree.
Without any treatment, the gold transcription tokens
aligned to these tokens are also deleted along with
them. This causes the child tokens in the sentence
not to have heads; consequently, these child tokens
are not included in the syntactic tree. To avoid this
problem, we instead attach them to the head of the
deleted token.

For example, in Figure 1(b), the word “are” is
missing in the ASR hypothesis. Then, this token’s
children lose their head in the transfer process. Thus,
we rescue these children by attaching them to the
head of “are”, which, in this case, is ROOT token.

If the head of the removed token is also of the
Trans-to-NULL type, then we look for an alternative
head by climbing the tree in a recursive manner, un-
til reaching ROOT. We also label the newly created
edges in this process as “error”.

ROOT

(a)
Trans:

they  may      flip      flop  when  they  get  to  be  uh    N     N   older

they  made  slipped  flop  when  they  get  to  be  uh  old  way  older

(b)

Trans-to-NULL:
Transcription 
token aligns to 
NULL in ASR 
output text.

error

error

ASR:

ROOT

NOT MATCH:
Aligned tokens
does not match 
on character.

 

error error

ASR-to-NULL:
ASR output 
token aligns to 
NULL in gold 
transcription.

ROOT

what  age  are  your  children

what  age   N   your  children

error

error

ROOT
 

ROOT

1038

2.3 NOT MATCH
In cases in which two aligned tokens do not match
exactly on the character level, the mismatch is re-
garded as an instance of a substitution type of ASR
error. Therefore, we encode this fact in the label of
the arc from the token to its head.

In Figure 1(a), the words “made” and “slipped” in
the ASR hypothesis do not match the gold transcrip-
tion tokens, “may” and “ﬂip”, respectively. There-
fore, we automatically re-label the arc from each to-
ken to its head as “error”.

3 Transition-based Dependency Parsing

To parse texts that contain disﬂuencies and ASR er-
rors, we extend the ArcEager shift-reduce depen-
dency parser of (Zhang and Nivre, 2011). Our pro-
posed parser adopts the same Shift, Reduce, LeftArc,
and RightArc actions as ArcEager. To this parser we
add three new actions, i.e., Edit, LeftArcError, and
RightArcError, to handle disﬂuencies and ASR er-
rors.

Edit action removes a disﬂuent token when it is
the ﬁrst element of the stack. This is different from
Honnibal (2014)’s Edit action:
theirs accumulates
consecutive disﬂuent tokens on the top of the stack
and removes them all at once, whereas our method
removes this kind of token one-by-one. Use of this
Edit action guarantees that the length of the action
sequence is always 2n− 1. This property is advanta-
geous because the parser can use the standard beam
search and does not require normalization, such as
those adopted in (Honnibal and Johnson, 2014) and
(Zhu et al., 2013).

LeftArcError and RightArcError act in the same
way as LeftArc and RightArc, except that these act
only on ASR error tokens, whereas the original Left-
Arc and RightArc are reserved for non ASR error to-
kens. Using two different kinds of Arc actions for
the two types of tokens (ASR error or not) allows
for the weights not to be shared between them, and
is expected to yield improved performance.

In the experiment below, we train all of the mod-
els using structured perceptron with max violation
(Huang et al., 2012). The feature set is mainly based
on (Honnibal and Johnson, 2014), such as the dis-
ﬂuency capturing features to inquire whether the to-
ken sequence inside the two speciﬁc spans match on

word forms or POS tags. We adjusted these features
to inspect the content of the buffer more carefully,
because our parser decides if the word token is dis-
ﬂuent or not every time new token is shifted and
hints for the decision lies much more in the buffer.

3.1 Backoff Action Feature
With the newly proposed LeftArcError
and
RightArcError actions, we fear that the relatively
low frequency of “error” tokens may cause the
weights for these actions to be updated too infre-
quently to be accurately generalized. We resort
to using the “backoff action feature” to avoid
this situation. This means that, for each action
the score of
a ∈ {Lef tArc, Lef tArcError},
performing it in a state s is calculated as follow:
SCORE(a, s) = w · f (a, s) + w · f (a′, s)

(1)

LeftArcBackoff

where a′ = LeftArcBackoff, w is the weight vec-
tor and f (·,·) is the feature representation, respec-
tively.
is not actual action per-
formed by our parser, rather it is used to provide the
common feature representation which both LeftArc
and LeftArcError can “back off” to. RightArc and
RightArcError actions also calculate their scores as
in Eq.(1), with a′ = RightArcBackoff. The scores
for all the other actions are calculated in the normal
way: SCORE(a, s) = w · f (a, s).
3.2 WCN Feature
To better capture which parts of the texts are likely
to be ASR errors, we use additional features ex-
tracted from a word confusion network (WCN) gen-
erated by ASR models. Marin (2015) reports his
observation that WCN slots with more arcs tend to
correspond to erroneous region. Following (Marin,
2015), we use mean and standard deviation of arc
posteriors and the highest arc posterior in each WCN
slot corresponding to each word token. We include
in the feature vector these real-valued features for to-
kens on top of the stack and the ﬁrst three elements
of the buffer.

4 Experiment
We conducted experiments using both the proposed
parsing method and the tree-annotated corpus based
on the ASR output texts. Our experiments were per-
formed using the Switchboard corpus (Godfrey et

1039

al., 1992). This corpus consists of speech data and
its transcription texts, and subset of which is anno-
tated with POS tags, syntactic trees and disﬂuency
information (repair, discourse marker and interjec-
tion) based on (Shriberg, 1994). 2

4.1 ASR Settings
To obtain the ASR output
texts of the corpus,
we used the off-the-shelf NeuralNet recipe (Zhang
et al., 2014) presented by Kaldi.3 We used the
jackknife method to obtain the ASR output texts
throughout the syntactically annotated part of the
corpus. 4

From these ASR output texts, we created the
tree-annotated corpus by applying the data creation
method introduced in §2. Out of all 857,493 word
tokens,
there are 32,606 ASR-to-NULL, 34,952
Trans-to-NULL, and 93,138 NOT MATCH cases,
meaning 15.6% of all word tokens had “error” la-
beled arcs.

4.2 Parsing Settings
We assigned POS tags to the created corpus using
the Stanford POS tagger (Toutanova et al., 2003)
trained on a part of the gold Switchboard corpus. 5
We adopt the same train/dev/test split as in (Hon-
nibal and Johnson, 2014), although the data size re-
duces slightly during the process of data creation.
We report the unlabeled attachment score (UAS),
which indicates how many heads of ﬂuent tokens are
correctly predicted. As for disﬂuency detection, we
report precision/recall/F1-score values following the
previous work in the literature.

As a baseline (To which we refer as Base in the
following), we use an ArcEager parser with our pro-
posed Edit action and the disﬂuency capturing fea-
tures, trained on the train part of the gold Switch-
board corpus. Using this parser on ASR output test
data can be seen as reproducing the typical situation,
2We converted the phrase structure trees to dependency ones

using the Stanford converter (de Marneffe et al., 2006).

3http://kaldi-asr.org/
4The average Word Error Rate of resulting models were
13.9 % on the Switchboard part of HUB5 evaluation dataset:
https://catalog.ldc.upenn.edu/LDC2002S09

5We used a part of the corpus that is annotated with POS in-
formation but not syntactic one. The performance of the tagger
is evaluated on the syntactically annotated part of the corpus;
the tagger has an accuracy score of 95.0%.

Model

Base
+ ErrorAct
+ Backoff
+ WCN

Dep
UAS
72.7
76.3
76.4
76.2

Disﬂ
Prec. Rec.
62.2
58.6
57.6
66.0
57.3
65.6
67.9
57.9

F1
60.3
61.5
61.1
62.5

Table 1: Dependency parsing and disﬂuency detection results
of the proposed methods. We used our created corpus as both
train and test data.

Dep
UAS
89.7
74.7
72.7
76.2

Disﬂ
Rec.
76.8
65.6
62.2
57.9

Train

Test

Model

Base
Base
Base
Ours

Prec.
90.4
58.5
58.6
67.9

T rans
ASR
ASR
ASR

T rans
T rans
ASR
ASR

F1
83.1
61.8
60.3
62.5
Table 2: Parsing result on different train-test settings. T rans
refers to original Switchboard transcription text, ASR the text
created through the data creation in §4.1. Ours is our proposed
parser: Base + ErrorAct + Backoff + WCN.

in which a parser is trained on ASR-error-free texts,
but nevertheless needs to parse the ASR output texts.

4.3 Results and Analysis
In Table 1, based on the baseline Base parser, we
report scores with the additional (and additive) use
of Left/RightArcError actions (ErrorAct), the WCN
feature (WCN), and the backoff action feature (Back-
off), on our created corpus. Using ErrorAct resulted
in 3.6% and 1.2% improvement in UAS and disﬂu-
ency detection F1, respectively. Backoff contributes
to further improved UAS, whereas WCN cause an
increase in disﬂuency detection accuracy.

Table 2 reports performance on various train and
test data settings.
In Table 2, the Train and Test
columns represent which data to use in training
and testing; T rans refers to the gold transcription
text of the Switchboard corpus, and ASR the text
created through the data creation in §4.1. When
evaluated on the ASR texts, the parser trained on
the ASR texts showed degraded performance com-
pared to the parser trained on the gold transcription
((Train, Test) = (ASR, ASR)). Although both the
train and test data are ASR texts and share character-
istics, we did not observe domain adaptation effect.
We hypothesized that the drop in the performance is
due to the noisy nature of our corpus, which is cre-
ated from the texts with ASR errors. Having ASR-

1040

error-speciﬁc actions, Left/RightArcError mitigates
this problem by separately treating the ASR error
tokens and non ASR error tokens. Finally, with the
newly proposed features, the parser trained on ASR
texts outperforms the parser trained on the transcrip-
tion texts with the improvement of 1.5% and 0.7%
for UAS and disﬂuency detection, respectively.

However, when compared with the case of
(Train, Test) = (T rans, T rans), we observe sig-
niﬁcant decreases in performance in both of the
tasks conducted on ASR texts. This result clearly
poses a new challenge for the disﬂuency detection
community.

5 Conclusion
In this work, we have proposed a novel
joint
transition-based dependency parsing method with
disﬂuency detection. Using new actions, and new
feature set, the proposed parser can parse ASR out-
put texts robustly. We have also introduced a data
construction method to evaluate dependency parsing
and disﬂuency detection performance for real speech
data. As the experimental results for ASR texts is
signiﬁcantly lower than that achieved for the gold
transcription texts, we have clariﬁed the need to de-
velop a method that is robust to recognition errors in
the ASR system.

6 Acknowledgements
We thank the three anonymous reviewers for their
detailed and insightful comments on an earlier draft
of this paper. This work was supported by JSPS
KAKENHI Grant Number 15K16053, 26240035.

References
Hao Cheng, Hao Fang, and Mari Ostendorf. 2015. Open-
domain name error detection using a multi-task rnn.
In Proceedings of the 2015 Conference on Empirical
Methods in Natural Language Processing, pages 737–
746. Association for Computational Linguistics.

Eunah Cho, Jan Niehues, and Alex Waibel. 2014. Tight
integration of speech disﬂuency removal into smt. In
Proceedings of the 14th Conference of the European
Chapter of the Association for Computational Linguis-
tics, volume 2: Short Papers (EACL), pages 43–47.
Association for Computational Linguistics.

Marie-Catherine de Marneffe, Bill MacCartney, and
Christopher D. Manning. 2006. Generating typed

dependency parses from phrase structure parses.
In
In Proceedings of the 5th International Conference on
Language Resources and Evaluation (LREC).

James Ferguson, Greg Durrett, and Dan Klein. 2015.
Disﬂuency detection with a semi-markov model and
prosodic features.
In Proceedings of the 2015 Con-
ference of the North American Chapter of the Asso-
ciation for Computational Linguistics: Human Lan-
guage Technologies (NAACL), pages 257–262. Asso-
ciation for Computational Linguistics.

J. J. Godfrey, E. C. Holliman, and J. McDaniel. 1992.
“ switchboard: Telephone speech corpus for research
and development ”.
In Acoustics, Speech, and Sig-
nal Processing, 1992. ICASSP-92., 1992 IEEE Inter-
national Conference on (Volume:1 ). Proc. IEEE Int.
Conf. Acoust. Speech Sig. Proc.

Matthew Honnibal and Mark Johnson. 2014. Joint incre-
mental disﬂuency detection and dependency parsing.
In Transactions of the Association of Computational
Linguistics Volume 2, Issue 1 (TACL), pages 131–142.
Association for Computational Linguistics.

Julian Hough and Matthew Purver. 2014. Strongly in-
cremental repair detection. In Proceedings of the 2014
Conference on Empirical Methods in Natural Lan-
guage Processing (EMNLP), pages 78–89. Associa-
tion for Computational Linguistics.

Julian Hough and David Schlangen. 2015. Recurrent
neural networks for incremental disﬂuency detection.
Interspeech 2015.

Liang Huang, Suphan Fayong, and Yang Guo. 2012.
Structured perceptron with inexact search. In Proceed-
ings of the 2012 Conference of the North American
Chapter of the Association for Computational Linguis-
tics: Human Language Technologies. Association for
Computational Linguistics.

Yang Liu, Elizabeth Shriberg, and Andreas Stolcke.
2003. Automatic disﬂuency identiﬁcation in coversa-
tional speech using multiple knowledge sources. In In
Proceedings of the 8th Eurospeech Conference.

Marius Alexandru Marin. 2015.

In Effective Use of
Cross-Domain Parsing in Automatic Speech Recogni-
tion and Error Detection. Ph.D. thesis. University of
Washington.

Xian Qian and Yang Liu. 2013. Disﬂuency detection
using multi-step stacked learning. In Proceedings of
the 2013 Conference of the North American Chapter
of the Association for Computational Linguistics: Hu-
man Language Technologies. Association for Compu-
tational Linguistics.

Mohammad Sadegh Rasooli and Joel Tetreault. 2014.
Non-monotonic parsing of ﬂuent umm i mean disﬂuent
sentences. In Proceedings of the 14th Conference of
the European Chapter of the Association for Compu-
tational Linguistics, volume 2: Short Papers (EACL),

1041

pages 48–53. Association for Computational Linguis-
tics.

Elizabeth Shriberg. 1994.

In Preliminaries to a The-
ory of Speech Disﬂuencies. Ph.D. thesis. University of
California, Berkeley.

Kristina Toutanova, Dan Klein, Christopher Manning,
and Singer Yoram. 2003. Feature-rich part-of-speech
tagging with a cyclic dependency network. In In Pro-
ceedings of the 2003 Human Language Technology
Conference of the North American Chapter of the As-
sociation for Computational Linguistics, pages 173–
180. Association for Computational Linguistics.

Shuangzhi Wu, Dongdong Zhang, Ming Zhou, and
Tiejun Zhao. 2015. Efﬁcient disﬂuency detection with
transition-based parsing.
In Proceedings of the 53rd
Annual Meeting of the Association for Computational
Linguistics and the 7th International Joint Conference
on Natural Language Processing (Volume 1: Long Pa-
pers) (ACL), pages 495–503. Association for Compu-
tational Linguistics.

Yue Zhang and Joakim Nivre. 2011. Transition-based
dependency parsing with rich non-local features.
In
Proceedings of the 49th Annual Meeting of the Asso-
ciation for Computational Linguistics: Human Lan-
guage Technologies (ACL), pages 188–193. Associa-
tion for Computational Linguistics.

Xiaohui Zhang, Jan Trmal, Daniel Povey, and San-
jeev Khudanpur. 2014.
Improving deep neural net-
work acoustic models using generalized maxout net-
works. In IEEE International Conference on Acous-
tics, Speech and Signal Processing (ICASSP).

Muhua Zhu, Yue Zhang, Wenliang Chen, Miu Zhang, and
Jingbo Zhu. 2013. Fast and accurate shift-reduce con-
stituent parsing. In In Proceedings of the 51st Annual
Meeting of the Association for Computational Linguis-
tics (ACL), pages 434–443. Association for Computa-
tional Linguistics.

