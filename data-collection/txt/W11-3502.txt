















































Discriminative Method for Japanese Kana-Kanji Input Method


Proceedings of the Workshop on Advances in Text Input Methods (WTIM 2011), pages 10–18,
Chiang Mai, Thailand, November 13, 2011.

Discriminative Method for Japanese Kana-Kanji Input Method

Hiroyuki Tokunaga Daisuke Okanohara
Preferred Infrastructure Inc.

2-40-1 Hongo, Bunkyo-ku, Tokyo
113-0033, Japan

{tkng,hillbig}@preferred.jp

Shinsuke Mori
Kyoto University

Yoshidahonmachi, Sakyo-ku, Kyoto
606-8501, Japan

forest@i.kyoto-u.ac.jp

Abstract

The most popular type of input method
in Japan is kana-kanji conversion, conver-
sion from a string of kana to a mixed kanji-
kana string.

However there is no study using discrim-
inative methods like structured SVMs for
kana-kanji conversion. One of the reasons
is that learning a discriminative model
from a large data set is often intractable.
However, due to progress of recent re-
searches, large scale learning of discrim-
inative models become feasible in these
days.

In the present paper, we investigate
whether discriminative methods such as
structured SVMs can improve the accu-
racy of kana-kanji conversion. To the best
of our knowledge, this is the first study
comparing a generative model and a dis-
criminative model for kana-kanji conver-
sion. An experiment revealed that a dis-
criminative method can improve the per-
formance by approximately 3%.

1 Introduction

Kana-kanji conversion is conversion from kana
characters to kanji characters, the most popular
way of inputting Japanese text from keyboards.
Generally one kana string corresponds to many
kanji strings, proposing what the user wants to in-
put is not trivial. We showed how input keys are
processed in Figure 1.

Two types of problems are encountered in kana-
kanji conversion. One is how to reduce conversion
errors, and the other is how to correct smoothly
when a conversion failure has occurred. In the
present study, we focus on the problem of reduc-
ing conversion errors.

Early kana-kanji conversion systems employed
heuristic rules, such as maximum-length-word
matching.

With the growth of statistical natural language
processing, data-driven methods were applied for
kana-kanji conversion. Most existing studies on
kana-kanji conversion have used probability mod-
els, especially generative models. Although gen-
erative models have some advantages, a number
of studies on natural language tasks have shown
that discriminative models tend to overcome gen-
erative models with respect to accuracy.

However, there have been no studies using only
a discriminative model for kana-kanji conversion.
One reason for this is that learning a discriminative
model from a large data set is often intractable.
However, due to progress in recent research on
machine learning, large-scale learning of discrim-
inative models has become feasible.

The present paper describes how to apply a dis-
criminative model for kana-kanji conversion. The
remainder of the present paper is organized as
follows. In the next section, we present a brief
description of Japanese text input and define the
kana-kanji conversion problem. In Section 3, we
describe related researches. Section 4 provides the
baseline method of the present study, i.e., kana-
kanji conversion based on a probabilistic language
model. In Section 5, we describe how to apply
the discriminative method for kana-kanji conver-
sion. Section 6 presents the experimental results,
and conclusions are presented in Section 7.

2 Japanese Text Input and Kana-Kanji
Conversion

Japanese text is composed of several scripts. The
primary components are hiragana, katakana, (we
refer them as kana) and kanji.

The number of kana is less than hundred. In
many case, we input romanized kana characters
from the keyboard. One of the task of a Japanese

10



BOS

苦科学 咎

学習

終

か　  が　　く　　と　　が　　く　　し　　ゅ　　う

が蚊 苦 と
EOS

input (kana):

火 蛾

ka　  ga　  ku　　to　  ga　　ku　　shu　　       uinput (romanized):

constructed graph:

Figure 1: A graph constructed from an input string. Some nodes are omitted for simplicity.

input method is transliterating them to kana.
The problem is how to input kanji. The num-

ber of kanji is more than ten thousand, the ordinal
keyboards in Japan do not have a sufficient number
of keys to enable the user to input such characters
directly.

In order to address this problem, a number
of methods have been proposed and investigated.
Handwriting recognition systems, for example,
have been successfully implemented. Another
successful method is kana-kanji conversion, which
is currently the most commonly used method for
inputting Japanese text due to its fast input speed
and low initial cost to learn.

2.1 Kana-kanji conversion
Kana-kanji conversion is the problem of convert-
ing a given kana string into a mixed kanji-kana
string. For simplicity, in the following we describe
a mixed kanji-kana string as a kanji string.

What should be noted here is that kana-kanji
conversion is the conversion from a kana sentence
to a kanji sentence. One of the key points of kana-
kanji conversion is that an entire sentence can be
converted at once. This is why kana-kanji conver-
sion is great at input speed.

Since an input string is non-segmented sen-
tence, every kana-kanji conversion system must be
able to segment a kana sentence into words. This
is not a trivial problem, recent kana-kanji conver-
sion softwares often employ the statistical meth-
ods.

Although there is a measure of freedom with re-
spect to the design of a kana-kanji conversion sys-
tem, in the present paper, we discuss kana-kanji
conversion systems they comply with the follow-
ing procedures:

1. Construct a graph from the input string.

We must first construct a graph that repre-
sents all possible conversions. An example
of such a graph is given in Figure 1.

We must use a dictionary in order to construct
this graph.

Since all edges are directed from the start
node to the goad node, the graph is a directed
acyclic graph (DAG).

2. Select the most likely path in the graph.

This task is broken into two parts. The first is
setting the cost of each vertex and edge. The
cost is often determined by supervised learn-
ing methods. The second is finding the most
likely path from the graph. Viterbi algorithm
is used for this task.

Formally, the task of kana-kanji conversion can
be defined as follows. Let x be an input, an un-
segmented sentence written in kana. Let y be a
path, i.e., a sequence of words. When we write
the jth word in y as yj , y can be denoted as
y = y1y2 . . . yn, where n is the number of words
in path y.

Let Y be a set of candidate paths in the DAG
built from the input sentence x. The goal is to
select a correct path y∗ from all candidate paths in
Y .

2.2 Parameter estimation with a probabilistic
language model

If we defined the kana-kanji conversion as a graph
search problem, all edges and vertices must have
costs. There are two ways to estimate these pa-
rameters. One is to use a language model, which
was proposed in the 1990s, and the other is to use
a discriminative model, as in the present study.

11



This subsection describes kana-kanji conver-
sion based on probabilistic language models,
treated as a baseline in the present paper.

In this method, given a kana string, we calcu-
late the probabilities for each conversion candidate
and then present the candidate that has the highest
probability.

We denote the probability P of a conversion
candidate y given x by P (y|x).

Using Bayes’ theorem, we can transform this
expression as follows:

P (y|x) = P (x|y)P (y)
P (x)

∝ P (y)P (x|y),

where P (y) is the language model, and P (x|y) is
the kana-kanji model.

The language model P (y) assigns a high prob-
ability to word sequences that are likely to occur.
Word n-gram models or class n-gram models are
often used in the natural language processing.

The definition of n-gram model is denoted as
follows:

P (y) =
∏

j

P (yj |yj−1, yj−2, . . . , yj−n+1),

where n−1 indicates the length of the history used
to estimate the probability.

If we adopt a word bigram (n = 2) model, then
P (y) is decomposed into the following form:

P (y) =
∏

j

P (yj |yj−1), (1)

where P (yj |yj−1) is the probability of the appear-
ance of yj after yj−1. Let c(y) be the number of
occurrences of y in the corpus, and let c(y1, y2)
be the number of occurrences of y1 after y2 in the
corpus. The probability P (yj |yj−1) is calculated
as follows:

P (yj |yj−1) =
c(yj , yj−1)

c(yj−1)
.

The kana-kanji model P (x|y) is assigned a
high probability if x corresponds to y several
times. In Japanese, most characters have multiple
pronunciations. For example, 雨 (rain) could be
read as “ame”, “same” or “u”. In the case of 雨,
most of Japanese expect the reading to be “ame”.
Therefore P (ame|雨) should be assigned a higher
probability than for “same” or “u”.

Mori et al. (1999) proposed the following de-
composition model for kana-kanji model:

P (x|y) =
∏

j

P (xj |yj)

where each P (xj |yj) is a fraction of how many
times the word yj is read as xj . Given that
d(xj , yj) is the number of times word yj is read
as xj , P (xj |yj) can be written as follows:

P (xj |yj) =
d(xj , yj)∑
i d(xi, yj)

.

2.3 Smoothing

In Eq. (1), if any P (yj |yj−1) is zero, P (y) is also
estimated to zero. This means that P (y) is always
zero if a word that does not appear in the training
corpus is appeared in y. Since we use a language
model to determine which sentence is more natu-
ral as a Japanese sentence, both probabilities be-
ing zero does not help us. This is referred to as the
zero frequency problem.

We apply smoothing to prevent this prob-
lem. In the present paper, we implemented both
of additive smoothing and interpolated Kneser-
Ney smoothing (Chen and Goodman, 1998; Teh,
2006). Surprisingly, the additive smoothing over-
came the interpolated Kneser-Ney smoothing.
Therefore we adopt the additive smoothing in the
experiments.

3 Related Research

Mori et al. (1999) proposed a kana-kanji con-
version method based on a probabilistic language
model. They used the class bigram as their lan-
guage model.

Gao et al. (2006) reported the results of ap-
plying a discriminative language model for kana-
kanji conversion. Their objective was domain
adaptation using a discriminative language model
for reranking of top-k conversion candidates enu-
merated by a generative model.

Kana-kanji conversion and morphological anal-
ysis are similar in some respects. Most notably,
both are the same type of extension of the se-
quential labeling problem. Therefore, it would be
worthwhile to consider studies on morphological
analysis.

Nagata (1994) showed that a pos-trigram
language model-based morphological analyzer
achieved approximately 95% precision and recall,

12



which was a state-of-the-art result at that time.
Ten years later, Kudo el al. (2004) applied the
conditional random field (CRF) to Japanese mor-
phological analysis. They reported that this major
discriminative probabilistic model does not suffer
from label bias and length bias and is superior to
the hidden Markov model (HMM) and maximum
entropy Markov model (MEMM) with respect to
accuracy.

The purpose of the present study is to investi-
gate whether this increase in performance for mor-
phological analysis also applies to kana-kanji con-
version.

4 Kana-Kanji Conversion Based on
Discriminative Methods

In this section, we present a description of kana-
kanji conversion based on discriminative methods.

In discriminative methods, we calculate a score
for each conversion candidate y1, y2, y3 . . . for in-
put x. The candidate that has the highest score is
presented to the user.

We herein restrict the score function such
that the score function can be decomposed into
weighted sum of K feature functions Ψ, where Ψ
is a vector of each feature function Ψk. We also re-
strict arguments of feature functions to x, yj−1, yj
in order to use the Viterbi algorithm for fast
conversion. A feature function Ψk(x, yj−1, yj)
returns 1 if the feature is enabled, otherwise
Ψk(x, yj−1, yj) returns 0.

The score of a conversion candidate y over x is
calculated as follows:

f(x, y) =
∑

j

∑

k

wkΨk(x, yj−1, yj),

where wk is the weight of feature function Ψk, and
w is a vector of each feature weight wk.

Since the output of kana-kanji conversion is a
vector, the problem is a structured output problem,
which can be addressed in a number of ways, in-
cluding the use of CRF (Lafferty et al., 2001) or
SSVM (Tsochantaridis et al., 2005; Ratliff et al.,
2007).

The performances of CRF, SSVM and other
learner models are similar if all of the models use
the same feature set (Keerthi and Sundararajan,
2007). We use the SSVM as the learner because
it is somewhat easier to implement.

4.1 Structured SVM
The SSVM is a natural extension of the SVM for
structured output problems.

We denote the ith datum as (x(i), y(i)). Here,
Li(y) is the loss for the ith datum, and we as-
sume that Li(y) ≥ 0 for all y 6= y(i), and that
Li(y(i)) = 0. Note that the value of Li is zero if
and only if y = y(i). This means that all of other
y are treated as negative examples.

We adopt the following loss function, which is
similar to Hamming loss:

Li(y) =
∑

j

l(yj),

where l(yj) is 1 if the path is incorrect, otherwise
l(yj) is 0.

The objective function of SSVM is expressed as
follows:

1

n

n∑

i=1

ri(w) + λ‖w‖, (2)

where ri(w) is the risk function, which is defined
as follows:

max
y∈Y

(wf(x(i), y)+L(y))−wf(x(i),y(i)). (3)

Note that the loss and the risk are differentiated
in the structured output problems, while they are
often not in binary classification problems.

Here, ‖w‖, which is the norm of w, is referred
to as a regularization term. The most commonly
used norms are L1-norm and L2-norm. We used
L1-norm in the experiment because it tends to
find sparse solutions. Since input methods are ex-
pected to work with modest amounts of RAM, this
property is important. L1-norm is calculated as
follows:

‖w‖1 =
∑

k

|wk|. (4)

The positive real number λ is a parameter that
trades off between the loss term and the regular-
ization term.

In order to minimize this objective function,
we used FOBOS as the parameter optimization
method (Duchi and Singer, 2009).

4.2 Learning of the SSVM using FOBOS
FOBOS is a versatile optimization method for
non-smooth convex problems, which can be used
both online and batch-wise. We used online FO-
BOS for parameter optimization.

13



Algorithm 1 Learning of SSVM
for (x(i), y(i)) do

y∗ = argmaxy f(x
(i), y) + L(x(i), y)

if y∗ 6= y(i) then
wi+

1
2 = w(i) − ηt ∇(f(x(i), y∗) − f(x(i), y(i)))

for k ∈ K do
w

(i+1)
k = sign(w

i+ 1
2

k )max{|w
(i+ 1

2
)

k | − ληt, 0}

end for
end if

end for

In the case of online FOBOS, FOBOS is viewed
as an extension of the stochastic gradient decent
and the subgradient method.

FOBOS alternates between two phases. The
first step processes the (sub)gradient descent, the
second step processes the regularization term in a
manner similar to projected gradients.

The first step of FOBOS is as follows:

w(i+
1
2
) = w(i) − ηigfi , (5)

where ηt is the learning rate and g
f
t is a subgradi-

ent of the risk function.
The second step of FOBOS is defined as the fol-

lowing optimization problem:

w(i+1) =

argmin
w

{
1

2
‖w − w(i+ 12 )‖2 + ηi+ 1

2
‖w‖

}
.

(6)

If the regularization term is L1-norm or L2-
norm, the closed-form solutions are easily derived.
For the case of the L1-norm, each element of vec-
tor w(i+1) is calculated as follows:

w
(i+1)
k = sign(w

i+ 1
2

k ) max{|w
(i+ 1

2
)

k | − ληt, 0},
(7)

where sign is a function which returns 1 if the ar-
gument is greater than 0 and otherwise returns −1.

We present a pseudo code of SSVM by FOBOS
as Algorithm 1.

In general, execution of the following expres-
sion needs exponential amount of calculation.

y∗ = argmax
y

f(x(i),y) + L(x(i), y). (8)

Name # sentences Data Source
OC 6,476 Yahoo! Chiebukuro

(Q&A site in Yahoo! Japan)

OW 5,934 White Book
PN 17,007 News Paper
PB 10,347 Book

Table 1: Details of Data Set

However, we restricted the form of our feature
functions to Ψk(x, yj−1, yj) so that we can use the
Viterbi algorithm to obtain y∗. The time complex-
ity of the Viterbi algorithm is linear, proportional
to the length of y.

Here, ∇f denotes a subgradient of function f .
The derived subgradient for f is as follows:

∇f(x, y) =
∑

j

∑

k

Ψk(x, yj−1, yj).

Therefore, the first parameter update rule of FO-
BOS can be rewritten as follows:

wi+
1
2 =w(i) − ηt(

∑

j

∑

k

Ψk(x, y
∗
j−1, y

∗
j )

−
∑

j

∑

k

Ψk(x, y
(i)
j−1, y

(i)
j )).

(9)

5 Evaluation

In order to evaluate our method, we compared the
generative model explained in Section 2 and our
discriminative model explained in Section 4 using
a popular data set.

5.1 Settings

As the data set, we used the balanced cor-
pus of contemporary written Japanese (BCCWJ)
(Maekawa, 2008). The corpus contains sev-
eral different data set, and we used human-
annotated data sets in our experiments. The
human-annotated part of the BCCWJ consists of
four parts, referred to as OC, OW, PN and PB.
Each data set is summarized in Table 1.

In addition, we constructed a data set (referred
to as ALL) that is the concatenation of OC, OW,
PN and PB.

The baseline method we used herein is a lan-
guage model based generative model. The lan-
guage model is the linear sum of logarithm of a

14



Type Template
Word Unigram 〈yi〉
Word Bigram 〈yi−1, yi〉
Class Bigram 〈POSi−1, POSi〉
Word and the Read 〈yi, xi〉

Table 2: Feature Templates

class bigram probability, a word bigram probabil-
ity and a word unigram probability. The smooth-
ing method is additive smoothing, where δ =
10−5. The performance was insensitive to the δ
when the value was small enough.

As the discriminative model, we used SSVM.
The learning loop of the SSVM was repeated until
convergence, i.e., 30 times for each data set. The
learning rate η is a fixed float number, 0.1. We
used L1-norm with λ = 10−7 as the regularization
term.

We implemented our SSVM learner in C lan-
guage, the calculation time for the ALL data set
was approximately 43 minutes and 20 seconds us-
ing an Intel Core 2 Duo (3.16GHz).

All of the experiments were carried out by five-
fold cross validation, and each data set was ran-
domly shuffled before being dividing into five data
sets.

5.2 Feature functions

We summarized feature functions which we used
in the experiments in Table 2. We used the second
level part of speech (In some Japanese dictionar-
ies, part of speech is designed to have hierarchical
structure) as classes.

5.3 Criteria

We evaluated these methods based on precision,
recall, and F-score, as calculated from the given
answers and system outputs.

In order to compute the precision and recall,
we must define true positive. In the present pa-
per, we use the longest common subsequence of
a given answer sentence and a system output sen-
tence as true positive. Let NLCS be the length of
the longest common subsequence of a given an-
swer and a system output. Let NDAT be the length
of the given answer sentence, and let NSY S be the
length of the system output sentence.

The definitions of precision, recall, and F-score

are as follows:

precision =
NLCS
NDAT

,

recall =
NLCS
NSY S

,

F-score = 2
precision · recall
precision + recall

.

5.4 Difference between the discriminative
model and the generative model

We compared the performance of the SSVM and
the generative method based on a language model
(baseline).

The results of the experiment were shown in Ta-
ble 3. The precision, recall, and F-score for the
SSVM and a baseline model are listed.

The experimental results revealed that the
SSVM performed better than the baseline model
for all data sets. However, the increase in per-
formance for each data set was not uniform.
The largest increase in performance was obtained
for PN, whose data source is newspaper arti-
cles. Since newspaper articles are written by well-
schooled newspersons, sentences are clear and
consistent. Compared to newspapers, other data
sets are noisy and inconsistent. The small im-
provement in performance is interpreted as being
due to the data set being noisy and the relative
difficulty in improving the performance scores as
compared to newspapers.

5.5 Relationship between data set size and
performance

In order to investigate the effect on performance
change related to data set size, we examined the
performance of the SSVM and the baseline model
for each data set size. The ALL data set is used in
this experiment.

The results are shown in Figure 2. The horizon-
tal axis denotes the number of sentences used for
training, and the vertical axis denotes the F-score.

The SSVM consistently outperforms the base-
line model whereas the number of sentences is
more than roughly 1000.

Interestingly, the baseline model performed bet-
ter than the SSVM, where the data set size is rel-
atively small. Generative models are said to be
superior to discriminative models if there is only a
small amount of data (Ng and Jordan, 2002). The
result of the present experiment agrees naturally
with their observations.

15



baseline SSVM
Precision Recall F-score Precision Recall F-score

avg. SD avg. SD avg. SD avg. SD avg. SD avg. SD
OC 87.4 0.31 86.9 0.17 87.2 0.22 88.0 0.41 89.1 0.27 88.5 0.33
OW 93.7 0.09 93.1 0.12 93.4 0.10 96.1 0.09 96.4 0.13 96.2 0.10
PN 87.4 0.11 86.4 0.16 86.9 0.13 91.1 0.24 91.6 0.17 91.4 0.20
PB 87.8 0.13 86.9 0.15 87.3 0.13 89.5 0.24 90.3 0.28 89.9 0.24
ALL 88.6 0.08 87.2 0.12 87.9 0.10 92.2 0.16 92.4 0.21 92.3 0.19

Table 3: Performance comparison for the SSVM and the baseline with the language model. (SD: standard
deviation.) Performance is measured by precision, recall and F-score.

 0.5

 0.55

 0.6

 0.65

 0.7

 0.75

 0.8

 0.85

 0.9

 0.95

 10  100  1000  10000

F
-s

co
re

number of sentences

ssvm
language model

Figure 2: F-score vs. data set size. With the in-
crease of the data set size, SSVM has overcome
baseline method.

5.6 Examples of misconversions
In this subsection, we present examples of miscon-
versions, which are categorized into four types.

Mori et al. (1999) categorized misconversions
into these three types, and their categorization was
also applicable to the proposed system. In addi-
tion, we present a number of misconversions that
could not be categorized into three categories.

5.6.1 Homonym failures
Japanese has numerous homonyms. For correct
conversion, syntactically and semantically correct
homonyms must be chosen.

Corpus 多分衛星放送でやっているのだと
思います。
Perhaps that show is broadcast by satellite.

System 多分衛生放送でやっているのだと
思います。
Perhaps that show is broadcast by health.

The system failed to recognize the compound
word satellite broadcast. This type of errors will
decrease as the data set size increases.

Corpus 暴力団の抗争も激化。
Bloody conflicts of gang is also escalated.

System 暴力団の構想も激化。
Concept of gang is also escalated.

Since ‘暴力団’ (gang) and ‘抗争’ (bloody con-
flicts) are strongly correlated, this problem would
be solved if we can use a feature which considers
long-distance information.

In principle, some fraction of homonym failures
could not be solved because of ambiguity of kana
string. A typical example is the name of a per-
son. The number of conversion candidates of ’Hi-
royuki’ is over 100.

5.6.2 Unknown word failures
It is difficult to convert a word which is not in the
dictionary. This type of misconversions cannot be
reduced with the SSVM of the present study. In
the following, we present an example of unknown
word failure.

Corpus 家でチキンナゲット作れますか？
Can you make chicken nuggets at home?

System 家でチキンなゲット作れますか？
Can you make chicken ??? at home?

The underlined part of system output is broken
as Japanese, and it cannot be converted into En-
glish. This type of errors often causes misdetec-
tions of word boundaries. This error is caused by
the absence of particlesナゲット from the dictio-
nary.

5.6.3 Orthographic variant failures
Some words have only one meaning and one pro-
nunciation, but have multiple expressions. Num-
bers are examples of such words. As a typical
case, six could be denoted as six or 6 or VI. In

16



addition, in Japanese, six can also be denoted as
‘六’ (roku) or ‘陸’ (roku).

Some of these expression are misconversions.
For example, ‘陸’ is seldom used, and in most
cases converting ’roku’ as ‘陸’ would be con-
sidered a misconversion. Nevertheless, with
the exception of human judgment, we have no
way to distinguish misconversions from non-
misconversions. Thus, in the present paper, all or-
thographic variants are treated as failures.

Corpus 一歳年下の弟は中学三年になるところ
だった．

System １歳年下の弟は中学三年になる所だ
った．

5.6.4 Other failures
Failures that do not fit into any of the above three
categories are salient in these experiments. The
following are two examples:

Corpus 太すぎず、細すぎないジーンズ。
Not too thick, not too thin jeans.

System 太すぎ図、細すぎないジーンズ。
Too thick figure, not too thin jeans.

The reason of misconversion is that the score for
‘図’ is too high.

Corpus ようやく来たかって感じです。
I feel that it has finally come.

System ようや茎たかって感じです。

The score for ‘茎’ (kuki/caulome) is too high.
In this case, misconversion is accompanied by se-
rious word boundary detection errors, and most of
the system output is difficult to interpret.

These errors are caused by poorly estimated pa-
rameters.

5.6.5 Discussion of misconversions
Although the discriminative method could im-
proves the performance of kana-kanji conversion
if there is sufficient data, there are still misconver-
sions.

Based on the investigation of the misconver-
sions, if a much larger data set is used, several
misconversions will be vanish. In fact, there are
several errors that do not exist in the closed test
results.

However, there are some types of errors that can
not be eliminated just by using a large amount of

data. Some of these errors will vanish if we can
use long-distance information in the feature func-
tions.

6 Conclusion

In the present paper, we suggested the possibility
of the discriminative methods for kana-kanji con-
version.

We proposed a system using a SSVM with FO-
BOS for parameter optimization. The experiments
of the present study revealed that the discrimina-
tive method is 1 to 4% superior with respect to pre-
cision and recall.

One of the advantages of the discriminative
methods is the flexibility allowing the inclusion of
a variety of feature functions. However, we used
only the set of a kana, a word surface and a class
(part of speech) in the experiments. using the en-
tire input string is expected to reduce homonym
failures, and further exploration of this area would
be interesting.

The data set used in the present study was mod-
est size. The increase in performance due to a
large data set should be investigated in the fu-
ture. In general, a large annotated data set is dif-
ficult to obtain. There are numbers of ways to
tackle the problem. There are two important op-
tions, one is applying of semi-supervised learning,
another is use of a morphological analyzer. We
will choose the latter option because building an
affective semi-supervised discriminative learning
model would be difficult for the case of kana-kanji
conversion.

Since the optimization method used in the
present study is online learning, the optimization
method can also be used for personalization from
the correction operations log of the user. There
have been few studies on this subject, and there
has been no report of discriminative models be-
ing used. Learning from the correction log of the
user is difficult because users often make mistakes.
Kana-kanji conversion software users sometimes
complain about the degradation of the conversion
performance as a side effect of personalization.
Therefore, an error detection mechanism will be
important. In the future, we plan to implement a
complete Japanese input method that embeds the
kana-kanji conversion system developed for the
present paper. Moreover, we intend to take into
account statistics and investigate input errors.

17



References
Stanley F. Chen and Joshua T. Goodman. 1998. An

empirical study of smoothing techniques for lan-
guage modeling. Technical Report Technical Report
TR-10-98, Computer Science Group, Harvard Uni-
versity.

John Duchi and Yoram Singer. 2009. Efficient online
and batch learning using forward backward splitting.
Journal of Machine Learning Research, 10:2899–
2934.

Jianfeng Gao, Hisami Suzuki, and Bin Yu. 2006. Ap-
proximation lasso methods for language modeling.
In Annual Meeting of the Association for Computa-
tional Linguistics, Sydney, Australia, July.

Selvaraj Sathiya Keerthi and Sellamanickam Sun-
dararajan. 2007. Crf versus svm-struct for sequence
labeling. Yahoo Research Technical Report.

Taku Kudo, Kaoru Yamamoto, and Yuji Matsumoto.
2004. Appliying conditional random fields to
japanese morphological analysis. In Conference on
Empirical Methods in Natural Language Process-
ing, Barcelona, Spain, July.

John Lafferty, Andrew McCallum, and Fernando
Pereira. 2001. Conditional random fields: Prob-
abilistic models for segmenting and labeling se-
quence data. In Proceedings of the Eighteenth In-
ternational Conference on Machine Learning.

Kikuo Maekawa. 2008. Balanced corpus of con-
temporary written japanese. In Proceedings of the
6th Workshop on Asian Language Resources, pages
101–102.

Shinsuke Mori, Tsuchiya Masatoshi, Osamu Yamaji,
and Makoto Nagao. 1999. Kana-kanji conver-
sion by a stochastic model. Transactions of IPSJ,
40(7):2946–2953. (in Japanese).

Masaaki Nagata. 1994. A stochastic japanese morpho-
logical analyzer using a forward-dp backward-a* n-
best search algorithm. pages 201–207, 8.

Andrew Y. Ng and Michael. I. Jordan. 2002. On dis-
criminative vs. generative classifiers: A comparison
of logistic regression and naive bayes. In Proceed-
ings of the Advances in Neural Information Process-
ing Systems.

Nathan Ratliff, J. Andrew Bagnell, and Martin A.
Zinkevich. 2007. (online) subgradient methods for
structured prediction. In International Conference
on Artificial Intelligence and Statistics, March.

Yee Whye Teh. 2006. A bayesian interpretation of
interpolated kneser-ney. Technical Report Technical
Report TRA2/06, NUS School of Computing.

Ioannis Tsochantaridis, Thorsten Joachims, Thomas
Hofmann, and Yasemin Altun. 2005. Large margin
methods for structured and interdependent output
variables. Journal of Machine Learning Research,
6:1453–1484.

18


