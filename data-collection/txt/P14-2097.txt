



















































Nonparametric Method for Data-driven Image Captioning


Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 592–598,
Baltimore, Maryland, USA, June 23-25 2014. c©2014 Association for Computational Linguistics

Nonparametric Method for Data-driven Image Captioning

Rebecca Mason and Eugene Charniak
Brown Laboratory for Linguistic Information Processing (BLLIP)

Brown University, Providence, RI 02912
{rebecca,ec}@cs.brown.edu

Abstract

We present a nonparametric density esti-
mation technique for image caption gener-
ation. Data-driven matching methods have
shown to be effective for a variety of com-
plex problems in Computer Vision. These
methods reduce an inference problem for
an unknown image to finding an exist-
ing labeled image which is semantically
similar. However, related approaches for
image caption generation (Ordonez et al.,
2011; Kuznetsova et al., 2012) are ham-
pered by noisy estimations of visual con-
tent and poor alignment between images
and human-written captions. Our work
addresses this challenge by estimating a
word frequency representation of the vi-
sual content of a query image. This al-
lows us to cast caption generation as an
extractive summarization problem. Our
model strongly outperforms two state-of-
the-art caption extraction systems accord-
ing to human judgments of caption rele-
vance.

1 Introduction

Automatic image captioning is a much studied
topic in both the Natural Language Processing
(NLP) and Computer Vision (CV) areas of re-
search. The task is to identify the visual content
of the input image, and to output a relevant natural
language caption.

Much prior work treats image captioning as
a retrieval problem (see Section 2). These ap-
proaches use CV algorithms to retrieve similar im-
ages from a large database of captioned images,
and then transfer text from the captions of those
images to the query image. This is a challenging
problem for two main reasons. First, visual simi-
larity measures do not perform reliably and do not

Query Image: Captioned Images:

1. 2. 3.

4. 5. 6.

1.) 3 month old baby girl with blue eyes in her crib
2.) A photo from the Ismail’s portrait shoot

3.) A portrait of a man, in black and white

4.) Portrait in black and white with the red rose

5.) I apparently had this saved in black and white as well

6.) Portrait in black and white

Table 1: Example of a query image from the SBU-
Flickr dataset (Ordonez et al., 2011), along with
scene-based estimates of visually similar images.
Our system models visual content using words that
are frequent in these captions (highlighted) and ex-
tracts a single output caption.

capture all of the relevant details which humans
might describe. Second, image captions collected
from the web often contain contextual or back-
ground information which is not visually relevant
to the image being described.

In this paper, we propose a system for transfer-
based image captioning which is designed to ad-
dress these challenges. Instead of selecting an out-
put caption according to a single noisy estimate
of visual similarity, our system uses a word fre-
quency model to find a smoothed estimate of vi-
sual content across multiple captions, as Table 1
illustrates. It then generates a description of the
query image by extracting the caption which best
represents the mutually shared content.

The contributions of this paper are as follows:

592



1. Our caption generation system effectively lever-
ages information from the massive amounts of
human-written image captions on the internet. In
particular, it exhibits strong performance on the
SBU-Flickr dataset (Ordonez et al., 2011), a noisy
corpus of one million captioned images collected
from the web. We achieve a remarkable 34%
improvement in human relevance scores over a
recent state-of-the-art image captioning system
(Kuznetsova et al., 2012), and 48% improvement
over a scene-based retrieval system (Patterson et
al., 2014) using the same computed image fea-
tures.

2. Our approach uses simple models which can
be easily reproduced by both CV and NLP re-
searchers. We provide resources to enable com-
parison against future systems.1

2 Image Captioning by Transfer

The IM2TEXT model by Ordonez et al. (2011)
presents the first web-scale approach to image cap-
tion generation. IM2TEXT retrieves the image
which is the closest visual match to the query im-
age, and transfers its description to the query im-
age. The COLLECTIVE model by Kuznetsova et
al. (2012) is a related approach which uses trained
CV recognition systems to detect a variety of vi-
sual entities in the query image. A separate de-
scription is retrieved for each visual entity, which
are then fused into a single output caption. Like
IM2TEXT, their approach uses visual similarity as
a proxy for textual relevance.

Other related work models the text more di-
rectly, but is more restrictive about the source
and quality of the human-written training data.
Farhadi et al. (2010) and Hodosh et al. (2013)
learn joint representations for images and cap-
tions, but can only be trained on data with very
strong alignment between images and descriptions
(i.e. captions written by Mechanical Turkers). An-
other line of related work (Fan et al., 2010; Aker
and Gaizauskas, 2010; Feng and Lapata, 2010)
generates captions by extracting sentences from
documents which are related to the query image.
These approaches are tailored toward specific do-
mains, such as travel and news, where images tend
to appear with corresponding text.

1See http://bllip.cs.brown.edu/
download/captioning_resources.zip or ACL
Anthology.

3 Dataset

In this paper, we use the SBU-Flickr dataset2. Or-
donez et al. (2011) query Flickr.com using a
huge number of words which describe visual en-
tities, in order to build a corpus of one million
images with captions which refer to image con-
tent. However, further analysis by Hodosh et al.
(2013) shows that many captions in SBU-Flickr
(∼67%) describe information that cannot be ob-
tained from the image itself, while a substantial
fraction (∼23%) contain almost no visually rel-
evant information. Nevertheless, this dataset is
the only web-scale collection of captioned images,
and has enabled notable research in both CV and
NLP.3

4 Our Approach

4.1 Overview

For a query image Iq, our task is to generate a rele-
vant description by selecting a single caption from
C, a large dataset of images with human-written
captions. In this section, we first define the feature
space for visual similarity, then formulate a den-
sity estimation problem with the aim of modeling
the words which are used to describe visually sim-
ilar images to Iq. We also explore methods for
extractive caption generation.

4.2 Measuring Visual Similarity

Data-driven matching methods have shown to be
very effective for a variety of challenging prob-
lems (Hays and Efros, 2008; Makadia et al.,
2008; Tighe and Lazebnik, 2010). Typically these
methods compute global (scene-based) descriptors
rather than object and entity detections. Scene-
based techniques in CV are generally more robust,
and can be computed more efficiently on large
datasets.

The basic IM2TEXT model uses an equally
weighted average of GIST (Oliva and Torralba,
2001) and TinyImage (Torralba et al., 2008) fea-
tures, which coarsely localize low-level features
in scenes. The output is a multi-dimensional
image space where semantically similar scenes
(e.g. streets, beaches, highways) are projected
near each other.

2http://tamaraberg.com/CLSP11/
3In particular, papers stemming from the 2011 JHU-CLSP

Summer Workshop (Berg et al., 2012; Dodge et al., 2012;
Mitchell et al., 2012) and more recently, the best paper award
winner at ICCV (Ordonez et al., 2013).

593



Patterson and Hays (2012) present “scene at-
tribute” representations which are characterized
using low-level perceptual attributes as used by
GIST (e.g. openness, ruggedness, naturalness),
as well as high-level attributes informed by open-
ended crowd-sourced image descriptions (e.g., in-
door lighting, running water, places for learning).
Follow-up work (Patterson et al., 2014) shows
that their attributes provide improved matching for
image captioning over IM2TEXT baseline. We
use their publicly available4 scene attributes for
our experiments. Training set and query images
are represented using 102-dimensional real-valued
vectors, and similarity between images is mea-
sured using the Euclidean distance.

4.3 Density Estimation
As shown in Bishop (2006), probability density
estimates at a particular point can be obtained by
considering points in the training data within some
local neighborhood. In our case, we define some
region R in the image space which contains Iq.
The probability mass of that space is

P =
∫
R
p(Iq)dIq (1)

and if we assume thatR is small enough such that
p(Iq) is roughly constant in R, we can approxi-
mate

p(Iq) ≈ k
img

nimgV img
(2)

where kimg is the number of images within R in
the training data, nimg is the total number of im-
ages in the training data, and V img is the volume
ofR. In this paper, we fix kimg to a constant value,
so that V img is determined by the training data
around the query image.5

At this point, we extend the density estima-
tion technique in order to estimate a smoothed
model of descriptive text. Let us begin by consid-
ering p(w|Iq), the conditional probability of the
word6 w given Iq. This can be described using a

4https://github.com/genp/sun_
attributes

5As an alternate approach, one could fix the value of
V img and determine kimg from the number of points in R,
giving rise to the kernel density approach (a.k.a. Parzen
windows). However we believe the KNN approach is more
appropriate here, because the number of samples is nearly
10000 times greater than the number of dimensions in the
image representation.

6Here, we use word to refer to non-function words, and
assume all function words have been removed from the cap-
tions.

Bayesian model:

p(w|Iq) = p(Iq|w)p(w)
p(Iq)

(3)

The prior for w is simply its unigram frequency in
C, where ntxtw and ntxt are word token counts:

p(w) =
ntxtw
ntxt

(4)

Note that ntxt is not the same as nimg because a
single captioned image can have multiple words
in its caption. Likewise, the conditional density

p(Iq|w) ≈ k
txt
w

ntxtw V
img

(5)

considers instances of observed words within R,
although the volume of R is still defined by the
image space. ktxtw is the number of times w is used
withinR while ntxtw is the total number of times w
is observed in C.

Combining Equations 2, 4, and 5 and canceling
out terms gives us the posterior probability:

p(w|Iq) = k
txt
w

kimg
· n

img

ntxt
(6)

If the number of words in each caption is inde-
pendent of its image’s location in the image space,
then p(w|Iq) is approximately the observed uni-
gram frequency for the captions insideR.
4.4 Extractive Caption Generation

We compare two selection methods for extractive
caption generation:

1. SumBasic SumBasic (Nenkova and Vander-
wende, 2005) is a sentence selection algorithm for
extractive multi-document summarization which
exclusively maximizes the appearance of words
which have high frequency in the original docu-
ments. Here, we adapt SumBasic to maximize the
average value of p(w|Iq) in a single extracted cap-
tion:

output = arg max
ctxt∈R

∑
w∈ctxt

1
|ctxt|p(w|Iq) (7)

The candidate captions ctxt do not necessarily
have to be observed in R, but in practice we did
not find increasing the number of candidate cap-
tions to be more effective than increasing the size
ofR directly.

594



Figure 1: BLEU scores vs k for SumBasic extrac-
tion.

2. KL Divergence We also consider a KL
Divergence selection method. This method out-
performs the SumBasic selection method for ex-
tractive multi-document summarization (Haghighi
and Vanderwende, 2009). It also generates the best
extractive captions for Feng and Lapata (2010),
who caption images by extracting text from a re-
lated news article. The KL Divergence method is

output = arg min
ctxt∈R

∑
w

p(w|Iq) log p(w|Iq)
p(w|ctxt)

(8)

5 Evaluation

5.1 Automatic Evaluation
Although BLEU (Papineni et al., 2002) scores
are widely used for image caption evaluation, we
find them to be poor indicators of the quality of
our model. As shown in Figure 1, our system’s
BLEU scores increase rapidly until about k = 25.
Past this point we observe the density estimation
seems to get washed out by oversmoothing, but the
BLEU scores continue to improve until k = 500
but only because the generated captions become
increasingly shorter. Furthermore, although we
observe that our SumBasic extracted captions ob-
tain consistently higher BLEU scores, our per-
sonal observations find KL Divergence captions to
be better at balancing recall and precision. Never-
theless, BLEU scores are the accepted metric for
recent work, and our KL Divergence captions with
k = 25 still outperform all other previously pub-
lished systems and baselines. We omit full results
here due to space, but make our BLEU setup with
captions for all systems and baselines available for
documentary purposes.

System Relevance
COLLECTIVE 2.38 (σ = 1.45)
SCENE ATTRIBUTES 2.15 (σ = 1.45)
SYSTEM 3.19 (σ = 1.50)
HUMAN 4.09 (σ = 1.14)

Table 2: Human evaluations of relevance: mean
ratings and standard deviations. See Section 5.2.

5.2 Human Evaluation

We perform our human evaluation of caption rele-
vance using a similar setup to that of Kuznetsova
et al. (2012), who have humans rate the image cap-
tions on a 1-5 scale (5: perfect, 4: almost per-
fect, 3: 70-80% good, 2: 50-70% good, 1: to-
tally bad). Evaluation is performed using Amazon
Mechanical Turk. Evaluators are shown both the
caption and the query image, and are specifically
instructed to ignore errors in grammaticality and
coherence.

We generate captions using our system with KL
Divergence sentence selection and k = 25. We
also evaluate the original HUMAN captions for
the query image, as well as generated captions
from two recently published caption transfer sys-
tems. First, we consider the SCENE ATTRIBUTES
system (Patterson et al., 2014), which represents
both the best scene-based transfer model and a
k = 1 nearest-neighbor baseline for our system.
We also compare against the COLLECTIVE system
(Kuznetsova et al., 2012), which is the best object-
based transfer model.

In order to facilitate comparison, we use the
same test/train split that is used in the publicly
available system output for the COLLECTIVE sys-
tem7. However, we remove some query images
which have contamination between the train and
test set (this occurs when a photographer takes
multiple shots of the same scene and gives all the
images the exact same caption). We also note that
their test set is selected based on images where
their object detection systems had good perfor-
mance, and may not be indicative of their perfor-
mance on other query images.

Table 2 shows the results of our human study.
Captions generated by our system have 48%
improvement in relevance over the SCENE AT-
TRIBUTES system captions, and 34% improve-

7http://www.cs.sunysb.edu/
˜pkuznetsova/generation/cogn/captions.
html

595



COLLECTIVE: One of the birds seen in
company of female and
juvenile.

View of this woman sit-
ting on the sidewalk in
Mumbai by the stained
glass. The boy walk-
ing by next to match-
ing color walls in gov t
building.

Found this mother bird
feeding her babies in
our maple tree on the
phone.

Found in floating grass
spotted alongside the
scenic North Cascades
Hwy near Ruby arm a
black bear.

SCENE
ATTRIBUTES:

This small bird is pretty
much only found in the
ancient Caledonian pine
forests of the Scottish
Highlands.

me and allison in front
of the white house

The sand in this beach
was black...I repeat
BLACK SAND

Not the green one, but
the almost ghost-like
white one in front of it.

SYSTEM: White bird found in
park standing on brick
wall

by the white house pine tree covered in ice
:)

Pink flower in garden w/
moth

HUMAN: Some black head bird
taken in bray head.

Us girls in front of the
white house

Male cardinal in snowy
tree knots

Black bear by the road
between Ucluelet and
Port Alberni, B.C.,
Canada

Table 3: Example query images and generated captions.

ment over the COLLECTIVE system captions. Al-
though our system captions score lower than the
human captions on average, there are some in-
stances of our system captions being judged as
more relevant than the human-written captions.

6 Discussion and Examples

Example captions are shown in Table 3. In many
instances, scene-based image descriptors provide
enough information to generate a complete de-
scription of the image, or at least a sufficiently
good one. However, there are some kinds of
images for which scene-based features alone are
insufficient. For example, the last example de-
scribes the small pink flowers in the background,
but misses the bear.

Image captioning is a relatively novel task for
which the most compelling applications are prob-
ably not yet known. Much previous work in im-
age captioning focuses on generating captions that
concretely describe detected objects and entities
(Kulkarni et al., 2011; Yang et al., 2011; Mitchell
et al., 2012; Yu and Siskind, 2013). However,
human-generated captions and annotations also
describe perceptual features, contextual informa-
tion, and other types of content. Additionally, our
system is robust to instances where entity detec-
tion systems fail to perform. However, one could

consider combined approaches which incorporate
more regional content structures. For example,
previous work in nonparametric hierarchical topic
modeling (Blei et al., 2010) and scene labeling
(Liu et al., 2011) may provide avenues for further
improvement of this model. Compression meth-
ods for removing visually irrelevant information
(Kuznetsova et al., 2013) may also help increase
the relevance of extracted captions. We leave these
ideas for future work.

References
Ahmet Aker and Robert Gaizauskas. 2010. Generating

image descriptions using dependency relational pat-
terns. In Proceedings of the 48th Annual Meeting of
the Association for Computational Linguistics, ACL
’10, pages 1250–1258, Stroudsburg, PA, USA. As-
sociation for Computational Linguistics.

Alexander C Berg, Tamara L Berg, Hal Daume, Jesse
Dodge, Amit Goyal, Xufeng Han, Alyssa Mensch,
Margaret Mitchell, Aneesh Sood, Karl Stratos, et al.
2012. Understanding and predicting importance in
images. In Computer Vision and Pattern Recog-
nition (CVPR), 2012 IEEE Conference on, pages
3562–3569. IEEE.

Christopher M Bishop. 2006. Pattern recognition and
machine learning, volume 1. Springer New York.

David M. Blei, Thomas L. Griffiths, and Michael I. Jor-
dan. 2010. The nested chinese restaurant process

596



and bayesian nonparametric inference of topic hier-
archies. J. ACM, 57(2):7:1–7:30, February.

Jesse Dodge, Amit Goyal, Xufeng Han, Alyssa Men-
sch, Margaret Mitchell, Karl Stratos, Kota Yam-
aguchi, Yejin Choi, Hal Daumé III, Alexander C.
Berg, and Tamara L. Berg. 2012. Detecting visual
text. In North American Chapter of the Association
for Computational Linguistics (NAACL).

Xin Fan, Ahmet Aker, Martin Tomko, Philip Smart,
Mark Sanderson, and Robert Gaizauskas. 2010.
Automatic image captioning from the web for gps
photographs. In Proceedings of the international
conference on Multimedia information retrieval,
MIR ’10, pages 445–448, New York, NY, USA.
ACM.

Ali Farhadi, Mohsen Hejrati, Mohammad Amin
Sadeghi, Peter Young, Cyrus Rashtchian, Julia
Hockenmaier, and David Forsyth. 2010. Every pic-
ture tells a story: generating sentences from images.
In Proceedings of the 11th European conference on
Computer vision: Part IV, ECCV’10, pages 15–29,
Berlin, Heidelberg. Springer-Verlag.

Yansong Feng and Mirella Lapata. 2010. How many
words is a picture worth? automatic caption genera-
tion for news images. In Proceedings of the 48th An-
nual Meeting of the Association for Computational
Linguistics, ACL ’10, pages 1239–1249, Strouds-
burg, PA, USA. Association for Computational Lin-
guistics.

Aria Haghighi and Lucy Vanderwende. 2009. Ex-
ploring content models for multi-document summa-
rization. In Proceedings of Human Language Tech-
nologies: The 2009 Annual Conference of the North
American Chapter of the Association for Computa-
tional Linguistics, pages 362–370. Association for
Computational Linguistics.

James Hays and Alexei A Efros. 2008. Im2gps: esti-
mating geographic information from a single image.
In Computer Vision and Pattern Recognition, 2008.
CVPR 2008. IEEE Conference on, pages 1–8. IEEE.

Micah Hodosh, Peter Young, and Julia Hockenmaier.
2013. Framing image description as a ranking task:
Data, models and evaluation metrics. Journal of Ar-
tificial Intelligence Research, 47:853–899.

Girish Kulkarni, Visruth Premraj, Sagnik Dhar, Siming
Li, Yejin Choi, Alexander C. Berg, and Tamara L.
Berg. 2011. Baby talk: Understanding and gener-
ating simple image descriptions. In CVPR, pages
1601–1608.

Polina Kuznetsova, Vicente Ordonez, Alexander C.
Berg, Tamara L. Berg, and Yejin Choi. 2012. Col-
lective generation of natural image descriptions. In
ACL.

Polina Kuznetsova, Vicente Ordonez, Alexander Berg,
Tamara Berg, and Yejin Choi. 2013. Generalizing
image captions for image-text parallel corpus. In
ACL.

Ce Liu, Jenny Yuen, and Antonio Torralba. 2011.
Nonparametric scene parsing via label transfer.
Pattern Analysis and Machine Intelligence, IEEE
Transactions on, 33(12):2368–2382.

Ameesh Makadia, Vladimir Pavlovic, and Sanjiv Ku-
mar. 2008. A new baseline for image annotation.
In Computer Vision–ECCV 2008, pages 316–329.
Springer.

Margaret Mitchell, Jesse Dodge, Amit Goyal, Kota Ya-
maguchi, Karl Stratos, Xufeng Han, Alyssa Men-
sch, Alexander C. Berg, Tamara L. Berg, and Hal
Daumé III. 2012. Midge: Generating image de-
scriptions from computer vision detections. In Euro-
pean Chapter of the Association for Computational
Linguistics (EACL).

Ani Nenkova and Lucy Vanderwende. 2005. The im-
pact of frequency on summarization.

Aude Oliva and Antonio Torralba. 2001. Modeling the
shape of the scene: A holistic representation of the
spatial envelope. International Journal of Computer
Vision, 42:145–175.

V. Ordonez, G. Kulkarni, and T.L. Berg. 2011.
Im2text: Describing images using 1 million cap-
tioned photographs. In NIPS.

Vicente Ordonez, Jia Deng, Yejin Choi, Alexander C
Berg, and Tamara L Berg. 2013. From large scale
image categorization to entry-level categories. In In-
ternational Conference on Computer Vision.

Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a method for automatic eval-
uation of machine translation. In Proceedings of
the 40th Annual Meeting on Association for Com-
putational Linguistics, ACL ’02, pages 311–318,
Stroudsburg, PA, USA. Association for Computa-
tional Linguistics.

Genevieve Patterson and James Hays. 2012. Sun at-
tribute database: Discovering, annotating, and rec-
ognizing scene attributes. In Computer Vision and
Pattern Recognition (CVPR), 2012 IEEE Confer-
ence on, pages 2751–2758. IEEE.

Genevieve Patterson, Chen Xu, Hang Su, and James
Hays. 2014. The sun attribute database: Beyond
categories for deeper scene understanding. Interna-
tional Journal of Computer Vision.

Joseph Tighe and Svetlana Lazebnik. 2010. Su-
perparsing: scalable nonparametric image parsing
with superpixels. In Computer Vision–ECCV 2010,
pages 352–365. Springer.

Antonio Torralba, Robert Fergus, and William T Free-
man. 2008. 80 million tiny images: A large data
set for nonparametric object and scene recognition.
Pattern Analysis and Machine Intelligence, IEEE
Transactions on, 30(11):1958–1970.

597



Yezhou Yang, Ching Lik Teo, Hal Daumé III, and Yian-
nis Aloimonos. 2011. Corpus-guided sentence gen-
eration of natural images. In Empirical Methods
in Natural Language Processing (EMNLP), Edin-
burgh, Scotland.

Haonan Yu and Jeffrey Mark Siskind. 2013. Grounded
language learning from video described with sen-
tences. In Proceedings of the 51st Annual Meet-
ing of the Association for Computational Linguistics
(Volume 1: Long Papers), volume 1, pages 53–63,
Sofia, Bulgaria. Association for Computational Lin-
guistics.

598


