



















































STransE: a novel embedding model of entities and relationships in knowledge bases


Proceedings of NAACL-HLT 2016, pages 460–466,
San Diego, California, June 12-17, 2016. c©2016 Association for Computational Linguistics

STransE: a novel embedding model of entities and relationships
in knowledge bases

Dat Quoc Nguyen1, Kairit Sirts1, Lizhen Qu2 and Mark Johnson1

1 Department of Computing, Macquarie University, Sydney, Australia
dat.nguyen@students.mq.edu.au, {kairit.sirts, mark.johnson}@mq.edu.au

2 NICTA, ACT 2601, Australia
lizhen.qu@nicta.com.au

Abstract

Knowledge bases of real-world facts about
entities and their relationships are useful re-
sources for a variety of natural language pro-
cessing tasks. However, because knowledge
bases are typically incomplete, it is useful to
be able to perform link prediction, i.e., pre-
dict whether a relationship not in the knowl-
edge base is likely to be true. This paper com-
bines insights from several previous link pre-
diction models into a new embedding model
STransE that represents each entity as a low-
dimensional vector, and each relation by two
matrices and a translation vector. STransE is
a simple combination of the SE and TransE
models, but it obtains better link prediction
performance on two benchmark datasets than
previous embedding models. Thus, STransE
can serve as a new baseline for the more com-
plex models in the link prediction task.

1 Introduction

Knowledge bases (KBs), such as WordNet (Fell-
baum, 1998), YAGO (Suchanek et al., 2007), Free-
base (Bollacker et al., 2008) and DBpedia (Lehmann
et al., 2015), represent relationships between en-
tities as triples (head entity, relation, tail entity).
Even very large knowledge bases are still far from
complete (Socher et al., 2013; West et al., 2014).
Link prediction or knowledge base completion sys-
tems (Nickel et al., 2015) predict which triples not
in a knowledge base are likely to be true (Taskar et
al., 2004; Bordes et al., 2011). A variety of differ-
ent kinds of information is potentially useful here,

including information extracted from external cor-
pora (Riedel et al., 2013; Wang et al., 2014a) and
the other relationships that hold between the enti-
ties (Angeli and Manning, 2013; Zhao et al., 2015).
For example, Toutanova et al. (2015) used informa-
tion from the external ClueWeb-12 corpus to signif-
icantly enhance performance.

While integrating a wide variety of information
sources can produce excellent results, there are sev-
eral reasons for studying simpler models that di-
rectly optimize a score function for the triples in
a knowledge base, such as the one presented here.
First, additional information sources might not be
available, e.g., for knowledge bases for specialized
domains. Second, models that don’t exploit external
resources are simpler and thus typically much faster
to train than the more complex models using addi-
tional information. Third, the more complex mod-
els that exploit external information are typically
extensions of these simpler models, and are often
initialized with parameters estimated by such sim-
pler models, so improvements to the simpler mod-
els should yield corresponding improvements to the
more complex models as well.

Embedding models for KB completion associate
entities and/or relations with dense feature vectors
or matrices. Such models obtain state-of-the-art per-
formance (Nickel et al., 2011; Bordes et al., 2011;
Bordes et al., 2012; Bordes et al., 2013; Socher et
al., 2013; Wang et al., 2014b; Guu et al., 2015) and
generalize to large KBs (Krompa et al., 2015). Ta-
ble 1 summarizes a number of prominent embedding
models for KB completion.

Let (h, r, t) represent a triple. In all of the models

460



Model Score function fr(h, t) Opt.

SE ‖Wr,1h−Wr,2t‖`1/2 ; Wr,1, Wr,2 ∈ Rk×k SGD
Unstructured ‖h− t‖`1/2 SGD
TransE ‖h + r− t‖`1/2 ; r ∈ Rk SGD
DISTMULT h>Wrt ; Wr is a diagonal matrix ∈ Rk×k AdaGrad
NTN u>r tanh(h>Mrt + Wr,1h + Wr,2t + br) ; ur , br ∈ Rd; Mr ∈ Rk×k×d; Wr,1, Wr,2 ∈ Rd×k L-BFGS
TransH ‖(I− rpr>p )h + r− (I− rpr>p )t‖`1/2 ; rp, r ∈ Rk ; I: Identity matrix size k × k SGD
TransD ‖(I + rph>p )h + r− (I + rpt>p )t‖`1/2 ; rp, r ∈ Rd ; hp, tp ∈ Rk ; I: Identity matrix size d× k AdaDelta
TransR ‖Wrh + r−Wrt‖`1/2 ; Wr ∈ Rd×k ; r ∈ Rd SGD
Our STransE ‖Wr,1h + r−Wr,2t‖`1/2 ; Wr,1, Wr,2 ∈ Rk×k; r ∈ Rk SGD

Table 1: The score functions fr(h, t) and the optimization methods (Opt.) of several prominent embedding models
for KB completion. In all of these the entities h and t are represented by vectors h and t ∈ Rk respectively.

discussed here, the head entity h and the tail entity
t are represented by vectors h and t ∈ Rk respec-
tively. The Unstructured model (Bordes et al., 2012)
assumes that h ≈ t. As the Unstructured model
does not take the relationship r into account, it can-
not distinguish different relation types. The Struc-
tured Embedding (SE) model (Bordes et al., 2011)
extends the unstructured model by assuming that h
and t are similar only in a relation-dependent sub-
space. It represents each relation r with two matri-
ces Wr,1 and Wr,2 ∈ Rk×k, which are chosen so
that Wr,1h ≈ Wr,2t. The TransE model (Bordes et
al., 2013) is inspired by models such as Word2Vec
(Mikolov et al., 2013) where relationships between
words often correspond to translations in latent fea-
ture space. The TransE model represents each rela-
tion r by a translation vector r ∈ Rk, which is cho-
sen so that h + r ≈ t.

The primary contribution of this paper is that
two very simple relation-prediction models, SE and
TransE, can be combined into a single model, which
we call STransE. Specifically, we use relation-
specific matrices Wr,1 and Wr,2 as in the SE model
to identify the relation-dependent aspects of both h
and t, and use a vector r as in the TransE model
to describe the relationship between h and t in this
subspace. Specifically, our new KB completion
model STransE chooses Wr,1, Wr,2 and r so that
Wr,1h + r ≈ Wr,2t. That is, a TransE-style rela-
tionship holds in some relation-dependent subspace,
and crucially, this subspace may involve very dif-
ferent projections of the head h and tail t. So Wr,1
and Wr,2 can highlight, suppress, or even change the

sign of, relation-specific attributes of h and t. For
example, for the “purchases” relationship, certain
attributes of individuals h (e.g., age, gender, mari-
tal status) are presumably strongly correlated with
very different attributes of objects t (e.g., sports car,
washing machine and the like).

As we show below, STransE performs better than
the SE and TransE models and other state-of-the-art
link prediction models on two standard link predic-
tion datasets WN18 and FB15k, so it can serve as
a new baseline for KB completion. We expect that
the STransE will also be able to serve as the basis
for extended models that exploit a wider variety of
information sources, just as TransE does.

2 Our approach

Let E denote the set of entities and R the set of re-
lation types. For each triple (h, r, t), where h, t ∈ E
and r ∈ R, the STransE model defines a score func-
tion fr(h, t) of its implausibility. Our goal is to
choose f such that the score fr(h, t) of a plausi-
ble triple (h, r, t) is smaller than the score fr′(h′, t′)
of an implausible triple (h′, r′, t′). We define the
STransE score function f as follows:

fr(h, t) = ‖Wr,1h + r−Wr,2t‖`1/2

using either the `1 or the `2-norm (the choice is made
using validation data; in our experiments we found
that the `1 norm gave slightly better results). To
learn the vectors and matrices we minimize the fol-
lowing margin-based objective function:

461



L =
∑

(h,r,t)∈G
(h′,r,t′)∈G′

(h,r,t)

[γ + fr(h, t)− fr(h′, t′)]+

where [x]+ = max(0, x), γ is the margin hyper-
parameter, G is the training set consisting of correct
triples, and G′(h,r,t) = {(h′, r, t) | h′ ∈ E , (h′, r, t) /∈
G} ∪ {(h, r, t′) | t′ ∈ E , (h, r, t′) /∈ G} is the set
of incorrect triples generated by corrupting a correct
triple (h, r, t) ∈ G.

We use Stochastic Gradient Descent (SGD) to
minimize L, and impose the following constraints
during training: ‖h‖2 6 1, ‖r‖2 6 1, ‖t‖2 6 1,
‖Wr,1h‖2 6 1 and ‖Wr,2t‖2 6 1.
3 Related work

Table 1 summarizes related embedding models for
link prediction and KB completion. The models
differ in the score functions fr(h, t) and the algo-
rithms used to optimize the margin-based objective
function, e.g., SGD, AdaGrad (Duchi et al., 2011),
AdaDelta (Zeiler, 2012) and L-BFGS (Liu and No-
cedal, 1989).

DISTMULT (Yang et al., 2015) is based on a
Bilinear model (Nickel et al., 2011; Bordes et al.,
2012; Jenatton et al., 2012) where each relation is
represented by a diagonal rather than a full matrix.
The neural tensor network (NTN) model (Socher et
al., 2013) uses a bilinear tensor operator to repre-
sent each relation. Similar quadratic forms are used
to model entities and relations in KG2E (He et al.,
2015) and TATEC (Garcia-Duran et al., 2015b).

The TransH model (Wang et al., 2014b) asso-
ciates each relation with a relation-specific hyper-
plane and uses a projection vector to project en-
tity vectors onto that hyperplane. TransD (Ji et al.,
2015) and TransR/CTransR (Lin et al., 2015b) ex-
tend the TransH model using two projection vec-
tors and a matrix to project entity vectors into a
relation-specific space, respectively. TransD learns
a relation-role specific mapping just as STransE, but
represents this mapping by projection vectors rather
than full matrices, as in STransE. Thus STransE can
be viewed as an extension of the TransR model,
where head and tail entities are associated with their
own project matrices, rather than using the same ma-
trix for both, as in TransR and CTransR.

Dataset #E #R #Train #Valid #Test
WN18 40,943 18 141,442 5,000 5,000
FB15k 14,951 1,345 483,142 50,000 59,071

Table 2: Statistics of the experimental datasets used in
this study (and previous works). #E is the number of
entities, #R is the number of relation types, and #Train,
#Valid and #Test are the numbers of triples in the training,
validation and test sets, respectively.

Recently, Lao et al. (2011), Neelakantan et al.
(2015), Gardner and Mitchell (2015), Luo et al.
(2015), Lin et al. (2015a), Garcia-Duran et al.
(2015a) and Guu et al. (2015) showed that rela-
tion paths between entities in KBs provide richer in-
formation and improve the relationship prediction.
Nickel et al. (2015) reviews other approaches for
learning from KBs and multi-relational data.

4 Experiments

For link prediction evaluation, we conduct experi-
ments and compare the performance of our STransE
model with published results on the benchmark
WN18 and FB15k datasets (Bordes et al., 2013). In-
formation about these datasets is given in Table 2.

4.1 Task and evaluation protocol

The link prediction task (Bordes et al., 2011; Bordes
et al., 2012; Bordes et al., 2013) predicts the head or
tail entity given the relation type and the other entity,
i.e. predicting h given (?, r, t) or predicting t given
(h, r, ?) where ? denotes the missing element. The
results are evaluated using the ranking induced by
the score function fr(h, t) on test triples.

For each test triple (h, r, t), we corrupted it by
replacing either h or t by each of the possible en-
tities in turn, and then rank these candidates in as-
cending order of their implausibility value computed
by the score function. Following the protocol de-
scribed in Bordes et al. (2013), we remove any cor-
rupted triples that appear in the knowledge base, to
avoid cases where a correct corrupted triple might
be ranked higher than the test triple. We report the
mean rank and the Hits@10 (i.e., the proportion of
test triples in which the target entity was ranked in
the top 10 predictions) for each model. Lower mean
rank or higher Hits@10 indicates better link predic-
tion performance.

462



Following TransR/CTransR (Lin et al., 2015b),
TransD (Ji et al., 2015), TATEC (Garcia-Duran et
al., 2015b), RTransE (Garcia-Duran et al., 2015a)
and PTransE (Lin et al., 2015a), we used the en-
tity and relation vectors produced by TransE (Bor-
des et al., 2013) to initialize the entity and relation
vectors in STransE, and we initialized the relation
matrices with identity matrices. Following Wang et
al. (2014b), Lin et al. (2015b), He et al. (2015), Ji
et al. (2015) and Lin et al. (2015a), we applied the
“Bernoulli” trick for generating head or tail entities
when sampling incorrect triples. We ran SGD for
2,000 epochs to estimate the model parameters. Fol-
lowing Bordes et al. (2013) we used a grid search on
validation set to choose either the l1 or l2 norm in the
score function f , as well as to set the SGD learning
rate λ ∈ {0.0001, 0.0005, 0.001, 0.005, 0.01}, the
margin hyper-parameter γ ∈ {1, 3, 5} and the num-
ber of vector dimensions k ∈ {50, 100}. The lowest
mean rank on the validation set was obtained when
using the l1 norm in f on both WN18 and FB15k,
and when λ = 0.0005, γ = 5, and k = 50 for
WN18, and λ = 0.0001, γ = 1, and k = 100 for
FB15k.

4.2 Main results

Table 3 compares the link prediction results of
our STransE model with results reported in prior
work, using the same experimental setup. The
first twelve rows report the performance of mod-
els that do not exploit information about alterna-
tive paths between head and tail entities. The next
two rows report results of the RTransE and PTransE
models, which are extensions of the TransE model
that exploit information about relation paths. The
last row presents results for the log-linear model
Node+LinkFeat (Toutanova and Chen, 2015) which
makes use of textual mentions derived from the large
external ClueWeb-12 corpus.

It is clear that Node+LinkFeat with the additional
external corpus information obtained best results. In
future work we plan to extend the STransE model
to incorporate such additional information. Table 3
also shows that models RTransE and PTransE em-
ploying path information achieve better results than
models that do not use such information. In terms of
models not exploiting path information or external
information, the STransE model scores better than

Method WN18 FB15k
MR H10 MR H10

SE (Bordes et al., 2011) 985 80.5 162 39.8
Unstructured (Bordes et al., 2012) 304 38.2 979 6.3
TransE (Bordes et al., 2013) 251 89.2 125 47.1
TransH (Wang et al., 2014b) 303 86.7 87 64.4
TransR (Lin et al., 2015b) 225 92.0 77 68.7
CTransR (Lin et al., 2015b) 218 92.3 75 70.2
KG2E (He et al., 2015) 348 93.2 59 74.0
TransD (Ji et al., 2015) 212 92.2 91 77.3
TATEC (Garcia-Duran et al., 2015b) - - 58 76.7
NTN (Socher et al., 2013) - 66.1+ - 41.4+

DISTMULT (Yang et al., 2015) - 94.2+ - 57.7+

Our STransE model 206 93.4 69 79.7
RTransE (Garcia-Duran et al., 2015a) - - 50 76.2
PTransE (Lin et al., 2015a) - - 58 84.6
NLFeat (Toutanova and Chen, 2015) - 94.3 - 87.0

Table 3: Link prediction results. MR and H10 denote
evaluation metrics of mean rank and Hits@10 (in %), re-
spectively. “NLFeat” abbreviates Node+LinkFeat. The
results for NTN (Socher et al., 2013) listed in this table
are taken from Yang et al. (2015) since NTN was origi-
nally evaluated on different datasets. The results marked
with + are obtained using the optimal hyper-parameters
chosen to optimize Hits@10 on the validation set; trained
in this manner, STransE obtains a mean rank of 244 and
Hits@10 of 94.7% on WN18, while producing the same
results on FB15k.

the other models on WN18 and produces the highest
Hits@10 score on FB15k. Compared to the closely
related models SE, TransE, TransR, CTransR and
TransD, STransE does better than these models on
both WN18 and FB15k.

Following Bordes et al. (2013), Table 4 analyzes
Hits@10 results on FB15k with respect to the re-
lation categories defined as follows: for each rela-
tion type r, we computed the averaged number ah of
heads h for a pair (r, t) and the averaged number at
of tails t for a pair (h, r). If ah < 1.5 and at < 1.5,
then r is labeled 1-1. If ah ≥ 1.5 and at < 1.5, then
r is labeled M-1. If ah < 1.5 and at ≥ 1.5, then r is
labeled as 1-M. If ah ≥ 1.5 and at ≥ 1.5, then r is
labeled as M-M. 1.4%, 8.9%, 14.6% and 75.1% of
the test triples belong to a relation type classified as
1-1, 1-M, M-1 and M-M, respectively.

Table 4 shows that in comparison to prior models
not using path information, STransE obtains high-
est Hits@10 result for M-M relation category at
(80.1%+83.1%)/2 = 81.6%. In addition, STransE

463



Method Predicting head h Predicting tail t
1-1 1-M M-1 M-M 1-1 1-M M-1 M-M

SE 35.6 62.6 17.2 37.5 34.9 14.6 68.3 41.3
Unstr. 34.5 2.5 6.1 6.6 34.3 4.2 1.9 6.6
TransE 43.7 65.7 18.2 47.2 43.7 19.7 66.7 50.0
TransH 66.8 87.6 28.7 64.5 65.5 39.8 83.3 67.2
TransR 78.8 89.2 34.1 69.2 79.2 37.4 90.4 72.1
CTransR 81.5 89.0 34.7 71.2 80.8 38.6 90.1 73.8
KG2E 92.3 94.6 66.0 69.6 92.6 67.9 94.4 73.4
TransD 86.1 95.5 39.8 78.5 85.4 50.6 94.4 81.2
TATEC 79.3 93.2 42.3 77.2 78.5 51.5 92.7 80.7
STransE 82.8 94.2 50.4 80.1 82.4 56.9 93.4 83.1

Table 4: Hits@10 (in %) by the relation category on
FB15k. “Unstr.” abbreviates Unstructured.

also performs better than TransD for 1-M and M-1
relation categories. We believe the improved per-
formance of the STransE model is due to its use of
full matrices, rather than just projection vectors as
in TransD. This permits STransE to model diverse
and complex relation categories (such as 1-M, M-1
and especially M-M) better than TransD and other
similiar models. However, STransE is not as good
as TransD for the 1-1 relations. Perhaps the ex-
tra parameters in STransE hurt performance in this
case (note that 1-1 relations are relatively rare, so
STransE does better overall).

5 Conclusion and future work

This paper presented a new embedding model for
link prediction and KB completion. Our STransE
combines insights from several simpler embed-
ding models, specifically the Structured Embedding
model (Bordes et al., 2011) and the TransE model
(Bordes et al., 2013), by using a low-dimensional
vector and two projection matrices to represent each
relation. STransE, while being conceptually sim-
ple, produces highly competitive results on standard
link prediction evaluations, and scores better than
the embedding-based models it builds on. Thus it
is a suitable candidate for serving as future baseline
for more complex models in the link prediction task.

In future work we plan to extend STransE to ex-
ploit relation path information in knowledge bases,
in a manner similar to Lin et al. (2015a), Garcia-
Duran et al. (2015a) or Guu et al. (2015).

Acknowledgments

This research was supported by a Google award
through the Natural Language Understanding Fo-
cused Program, and under the Australian Re-
search Council’s Discovery Projects funding scheme
(project number DP160102156).

NICTA is funded by the Australian Government
through the Department of Communications and the
Australian Research Council through the ICT Centre
of Excellence Program. The first author is supported
by an International Postgraduate Research Scholar-
ship and a NICTA NRPA Top-Up Scholarship.

References
Gabor Angeli and Christopher Manning. 2013. Philoso-

phers are Mortal: Inferring the Truth of Unseen
Facts. In Proceedings of the Seventeenth Conference
on Computational Natural Language Learning, pages
133–142.

Kurt Bollacker, Colin Evans, Praveen Paritosh, Tim
Sturge, and Jamie Taylor. 2008. Freebase: A Col-
laboratively Created Graph Database for Structuring
Human Knowledge. In Proceedings of the 2008 ACM
SIGMOD International Conference on Management of
Data, pages 1247–1250.

Antoine Bordes, Jason Weston, Ronan Collobert, and
Yoshua Bengio. 2011. Learning Structured Em-
beddings of Knowledge Bases. In Proceedings of
the Twenty-Fifth AAAI Conference on Artificial Intelli-
gence, pages 301–306.

Antoine Bordes, Xavier Glorot, Jason Weston, and
Yoshua Bengio. 2012. A Semantic Matching Energy
Function for Learning with Multi-relational Data. Ma-
chine Learning, 94(2):233–259.

Antoine Bordes, Nicolas Usunier, Alberto Garcia-Duran,
Jason Weston, and Oksana Yakhnenko. 2013.
Translating Embeddings for Modeling Multi-relational
Data. In Advances in Neural Information Processing
Systems 26, pages 2787–2795.

John Duchi, Elad Hazan, and Yoram Singer. 2011.
Adaptive Subgradient Methods for Online Learning
and Stochastic Optimization. The Journal of Machine
Learning Research, 12:2121–2159.

Christiane D. Fellbaum. 1998. WordNet: An Electronic
Lexical Database. MIT Press.

Alberto Garcia-Duran, Antoine Bordes, and Nicolas
Usunier. 2015a. Composing Relationships with
Translations. In Proceedings of the 2015 Conference
on Empirical Methods in Natural Language Process-
ing, pages 286–290.

464



Alberto Garcia-Duran, Antoine Bordes, Nicolas Usunier,
and Yves Grandvalet. 2015b. Combining Two And
Three-Way Embeddings Models for Link Prediction in
Knowledge Bases. CoRR, abs/1506.00999.

Matt Gardner and Tom Mitchell. 2015. Efficient and Ex-
pressive Knowledge Base Completion Using Subgraph
Feature Extraction. In Proceedings of the 2015 Con-
ference on Empirical Methods in Natural Language
Processing, pages 1488–1498.

Kelvin Guu, John Miller, and Percy Liang. 2015.
Traversing Knowledge Graphs in Vector Space. In
Proceedings of the 2015 Conference on Empirical
Methods in Natural Language Processing, pages 318–
327.

Shizhu He, Kang Liu, Guoliang Ji, and Jun Zhao. 2015.
Learning to Represent Knowledge Graphs with Gaus-
sian Embedding. In Proceedings of the 24th ACM In-
ternational on Conference on Information and Knowl-
edge Management, pages 623–632.

Rodolphe Jenatton, Nicolas L. Roux, Antoine Bordes,
and Guillaume R Obozinski. 2012. A latent factor
model for highly multi-relational data. In Advances
in Neural Information Processing Systems 25, pages
3167–3175.

Guoliang Ji, Shizhu He, Liheng Xu, Kang Liu, and Jun
Zhao. 2015. Knowledge Graph Embedding via Dy-
namic Mapping Matrix. In Proceedings of the 53rd
Annual Meeting of the Association for Computational
Linguistics and the 7th International Joint Conference
on Natural Language Processing (Volume 1: Long Pa-
pers), pages 687–696.

Denis Krompa, Stephan Baier, and Volker Tresp. 2015.
Type-Constrained Representation Learning in Knowl-
edge Graphs. In Proceedings of the 14th International
Semantic Web Conference, pages 640–655.

Ni Lao, Tom Mitchell, and William W. Cohen. 2011.
Random Walk Inference and Learning in a Large Scale
Knowledge Base. In Proceedings of the Conference on
Empirical Methods in Natural Language Processing,
pages 529–539.

Jens Lehmann, Robert Isele, Max Jakob, Anja Jentzsch,
Dimitris Kontokostas, Pablo N. Mendes, Sebastian
Hellmann, Mohamed Morsey, Patrick van Kleef,
Sören Auer, and Christian Bizer. 2015. DBpedia - A
Large-scale, Multilingual Knowledge Base Extracted
from Wikipedia. Semantic Web, 6(2):167–195.

Yankai Lin, Zhiyuan Liu, Huanbo Luan, Maosong Sun,
Siwei Rao, and Song Liu. 2015a. Modeling Rela-
tion Paths for Representation Learning of Knowledge
Bases. In Proceedings of the 2015 Conference on
Empirical Methods in Natural Language Processing,
pages 705–714.

Yankai Lin, Zhiyuan Liu, Maosong Sun, Yang Liu, and
Xuan Zhu. 2015b. Learning Entity and Relation Em-
beddings for Knowledge Graph Completion. In Pro-
ceedings of the Twenty-Ninth AAAI Conference on Ar-
tificial Intelligence Learning, pages 2181–2187.

D. C. Liu and J. Nocedal. 1989. On the Limited Memory
BFGS Method for Large Scale Optimization. Mathe-
matical Programming, 45(3):503–528.

Yuanfei Luo, Quan Wang, Bin Wang, and Li Guo.
2015. Context-Dependent Knowledge Graph Embed-
ding. In Proceedings of the 2015 Conference on
Empirical Methods in Natural Language Processing,
pages 1656–1661.

Tomas Mikolov, Wen-tau Yih, and Geoffrey Zweig.
2013. Linguistic Regularities in Continuous Space
Word Representations. In Proceedings of the 2013
Conference of the North American Chapter of the As-
sociation for Computational Linguistics: Human Lan-
guage Technologies, pages 746–751.

Arvind Neelakantan, Benjamin Roth, and Andrew Mc-
Callum. 2015. Compositional Vector Space Models
for Knowledge Base Completion. In Proceedings of
the 53rd Annual Meeting of the Association for Com-
putational Linguistics and the 7th International Joint
Conference on Natural Language Processing (Volume
1: Long Papers), pages 156–166.

Maximilian Nickel, Volker Tresp, and Hans-Peter
Kriegel. 2011. A Three-Way Model for Collective
Learning on Multi-Relational Data. In Proceedings of
the 28th International Conference on Machine Learn-
ing, pages 809–816.

Maximilian Nickel, Kevin Murphy, Volker Tresp, and Ev-
geniy Gabrilovich. 2015. A Review of Relational Ma-
chine Learning for Knowledge Graphs. Proceedings
of the IEEE, to appear.

Sebastian Riedel, Limin Yao, Andrew McCallum, and
Benjamin M. Marlin. 2013. Relation Extraction with
Matrix Factorization and Universal Schemas. In Pro-
ceedings of the 2013 Conference of the North Ameri-
can Chapter of the Association for Computational Lin-
guistics: Human Language Technologies, pages 74–
84.

Richard Socher, Danqi Chen, Christopher D Manning,
and Andrew Ng. 2013. Reasoning With Neural Ten-
sor Networks for Knowledge Base Completion. In Ad-
vances in Neural Information Processing Systems 26,
pages 926–934.

Fabian M. Suchanek, Gjergji Kasneci, and Gerhard
Weikum. 2007. YAGO: A Core of Semantic Knowl-
edge. In Proceedings of the 16th International Con-
ference on World Wide Web, pages 697–706.

Ben Taskar, Ming fai Wong, Pieter Abbeel, and Daphne
Koller. 2004. Link Prediction in Relational Data. In

465



Advances in Neural Information Processing Systems
16, pages 659–666.

Kristina Toutanova and Danqi Chen. 2015. Observed
Versus Latent Features for Knowledge Base and Text
Inference. In Proceedings of the 3rd Workshop on
Continuous Vector Space Models and their Composi-
tionality, pages 57–66.

Kristina Toutanova, Danqi Chen, Patrick Pantel, Hoifung
Poon, Pallavi Choudhury, and Michael Gamon. 2015.
Representing Text for Joint Embedding of Text and
Knowledge Bases. In Proceedings of the 2015 Con-
ference on Empirical Methods in Natural Language
Processing, pages 1499–1509.

Zhen Wang, Jianwen Zhang, Jianlin Feng, and Zheng
Chen. 2014a. Knowledge Graph and Text Jointly Em-
bedding. In Proceedings of the 2014 Conference on
Empirical Methods in Natural Language Processing
(EMNLP), pages 1591–1601.

Zhen Wang, Jianwen Zhang, Jianlin Feng, and Zheng
Chen. 2014b. Knowledge Graph Embedding by
Translating on Hyperplanes. In Proceedings of the
Twenty-Eighth AAAI Conference on Artificial Intelli-
gence, pages 1112–1119.

Robert West, Evgeniy Gabrilovich, Kevin Murphy, Shao-
hua Sun, Rahul Gupta, and Dekang Lin. 2014.
Knowledge Base Completion via Search-based Ques-
tion Answering. In Proceedings of the 23rd Interna-
tional Conference on World Wide Web, pages 515–526.

Bishan Yang, Wen-tau Yih, Xiaodong He, Jianfeng Gao,
and Li Deng. 2015. Embedding Entities and Relations
for Learning and Inference in Knowledge Bases. In
Proceedings of the International Conference on Learn-
ing Representations.

Matthew D. Zeiler. 2012. ADADELTA: An Adaptive
Learning Rate Method. CoRR, abs/1212.5701.

Yu Zhao, Sheng Gao, Patrick Gallinari, and Jun Guo.
2015. Knowledge Base Completion by Learn-
ing Pairwise-Interaction Differentiated Embeddings.
Data Mining and Knowledge Discovery, 29(5):1486–
1504.

466


