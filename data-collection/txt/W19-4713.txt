



















































Predicting Historical Phonetic Features using Deep Neural Networks: A Case Study of the Phonetic System of Proto-Indo-European


Proceedings of the 1st International Workshop on Computational Approaches to Historical Language Change, pages 98–108
Florence, Italy, August 2, 2019. c©2019 Association for Computational Linguistics

98

Predicting historical phonetic features using deep neural networks: A case
study of the phonetic system of Proto-Indo-European

Frederik Hartmann
University of Konstanz

frederik.hartmann@uni-konstanz.de

Abstract

Traditional historical linguistics lacks the pos-
sibility to empirically assess its assumptions
regarding the phonetic systems of past lan-
guages and language stages beyond traditional
methods such as comparative tools to gain
insights into phonetic features of sounds in
proto- or ancestor languages. The paper at
hand presents a computational method based
on deep neural networks to predict phonetic
features of historical sounds where the exact
quality is unknown and to test the overall co-
herence of reconstructed historical phonetic
features. The method utilizes the principles of
coarticulation, local predictability and statis-
tical phonological constraints to predict pho-
netic features by the features of their immedi-
ate phonetic environment. The validity of this
method will be assessed using New High Ger-
man phonetic data and its specific application
to diachronic linguistics will be demonstrated
in a case study of the phonetic system Proto-
Indo-European.

1 Introduction

Since the beginning of historical linguistics, one
of the main aims of historical phonology and pho-
netics has been to reveal phonetic features of now
lost sounds and phonological systems of past lan-
guages. The study of the phonetic system of ear-
lier stages of languages is a crucial prerequisite
to uncover sound change and effects on sound
change precisely. The methods, however, are lim-
ited for every language whose speakers cannot
be invited to a phonetics lab for detailed test-
ing. The temporal scope of inquiries into lan-
guage change would be fairly limited if we could
only examine language change as far back as voice
recording and experimental testing methods were
present. If we want to study language change
over thousands of years, we must rely on robust

techniques to approximate historical phonetic fea-
tures as well as possible. The most prominent
methods in historical linguistics so far to achieve
this goal are based on comparative approaches (cf.
Campbell, 2013; Beekes and Vaan, 2011; Meier-
Brügger et al., 2010). Especially for the recon-
struction of proto-languages, historical phonolo-
gists use the comparative method to estimate the
approximate quality of sounds by investigating
their outcomes and effects in the descendant lan-
guages. However, approaching historical phonet-
ics by comparative means bears the disadvantage
that the more the daughter languages disagree in
certain respects, the less precise are the estimates
scholars can make for the respective proto-sounds.
For some problems for which comparative tech-
niques yield imprecise results, there is a need for
alternative methods to tackle these issues. More-
over, there is also no alternative method for cross-
checking assumptions obtained through the tradi-
tional methods as such an alternative would need
to operate on a basis different from diachronic
comparison. Thus the method proposed in this
paper makes use of synchronic structures and fea-
tures of a language’s phonology and feeds this data
into a deep neural network to predict the phonetic
features of unknown sounds. The data the network
can draw upon is the direct phonetic environment
of each sound with the goal to predict its features
only by the features of its environment.

The reason for the predictability of sound fea-
tures in the context of their environment is due
to coarticulatory effects, statistical constraints and
local predictability. Coarticulation refers to the
observation that sounds tend to both influence
and be influenced by their environment phoneti-
cally (see e.g. Kühnert and Nolan, 1999; Ohala,
1993a; Hardcastle and Hewlett, 2006; Fowler,
1980). This reciprocal influence can be detected
synchronically which makes it a possible alterna-



99

tive to be used for historical phonology if applied
to historical language stages or proto-languages:
In theory, sounds constantly influence their envi-
ronment and are affected by it at the same time
so that a tight net of interlaced dependencies be-
tween sounds and their environment arises. There
are indications that sound changes which better fit
into this phonetic structure in their initial stage are
more likely to become widely adapted (Donegan
and Nathan, 2015; Blevins, 2015; Ohala, 1993a,b;
Hale, 2003).

Similarly, and partially originating from coar-
ticulatory processes, we find certain types of
phonological constraints in languages, be it syl-
lable composition constraints or the prevention
of certain consonant clusters which make up a
language’s phonotactics. These constraints can
be both absolute and statistical, whereby abso-
lute constraints are rules which are never violated,
whereas statistical constraints constitute a strong
dominance of one phonological shape over oth-
ers. The network can utilize a language’s phono-
tactics, constraints and coarticulatory effects to
predict the phonetic features of a target sound.
Feature predictions from environmental properties
have already been studied in quantitative phonet-
ics and proven to be possible to some degree due
to local predictability effects (see e.g. Priva, 2015;
Van Son and Van Santen, 2005; Raymond et al.,
2006).

It is important to keep in mind that local pre-
dictability on the basis of the phonetic environ-
ment is, in fact, not contradictory to the observa-
tion that different sounds can occur in the same
environments which can be demonstrated using
minimal pairs. Predictability in this context does
not mean that a certain environment of a given
sound always yields certain phonetic properties, it
is rather a probabilistic observation that environ-
ments tend to occur paired with certain phonetic
features and that this tendency of forming patterns
is what can be predicted using probabilistic mod-
els and machine learning algorithms.

2 The deep neural network approach

Using machine learning algorithms is not new to
the field of linguistics, though it is one of the
more recent methods.1 While these approaches
are found in an increasing number of studies in lin-

1See e.g. Chollet (2018); Nielsen (2015) for a general
introduction to deep learning.

guistics in general, in historical linguistics in par-
ticular the method is less used although some stud-
ies have been published in this or adjacent fields
such as cladistics (Jäger et al., 2017; Jäger and
Sofroniev, 2016). Since this approach of predict-
ing sound features by the features in the phonetic
environment only works synchronically, the deep
neural network used for this needs to be trained on
better known phonological features as the basis for
predicting unknown features.

The data fed to the network must therefore con-
tain a dataset where the phonetic environment
serves as the input that is mapped on the tar-
get sound. To achieve this, the lexical corpus
data needs to be split into trigrams or pentagrams
of phonetic segments which are then categorized
with regard to their phonetic features. Afterwards,
the middle or target sound is removed and the re-
maining environment passed through the network
with the respective target sound features as la-
bels. Doing this trains the model to detect the cor-
rect phonetic features for the target sound given
its environment. If the network has successfully
trained, the environments of unknown sounds can
be passed to the model which will, in turn, pre-
dict the features of the sounds on the basis of its
weights and biases obtained in the training pro-
cess. When the network performs well on the
training data, we have little reason for it perform-
ing worse on the prediction of unknown sounds.
Deep neural networks are especially suited for this
task since other methods such as random forests or
support vector machines have performed worse on
this classification in preliminary tests I conducted
beforehand. These three approaches, Deep neu-
ral networks, random forests and support vector
machines, are entirely different approaches to ma-
chine learning classification tasks: While random
forest classificators aim at finding the best decision
tree by partitioning the data in subgroups, support
vector machines establish the best splitting func-
tion, a hyperplane, to classify new samples ac-
cording to their position in the multi-dimensional
space. Deep neural networks on the other hand
aim at optimizing the decision function through
means of building abstract representation of the
data and ‘learning’ the occurrence patterns of data
features. It is not always possible to determine
why some algorithms perform worse on some
datasets and better on others. In the task at hand
we can merely state that deep neural networks



100

seem to find the global minimum, or a better lo-
cal minimum, of the decision function well while
other algorithms do not perform on the same level,
presumably due to their characteristics not being
ideal for this particular case. In the following sec-
tion, a case study on Proto-Indo-European shall
function as an example study that can be con-
ducted using neural networks.

3 Case study: The phonetic system of
Proto-Indo-European

The phonetic system of Proto-Indo-European
(PIE) is an ideal field to demonstrate the capabil-
ities of this neural network approach for several
reasons: (1) while the phonetic inventory of PIE,
along with its phonotactics, has been reasonably
well investigated (Clackson, 2007, 64-71; Meier-
Brügger et al., 2010, 272-275; Byrd, 2015; Ringe,
2017, 13-17; Fortson IV, 2011, 62-64), there are
still unknown aspects that lead to scholarly discus-
sions and diverging theories such as the Glottalic
theory.2 (2) three sounds of PIE, the so-called la-
ryngeals, are still a matter of debate since they are
only scarcely attested in PIE’s daughter languages
and sometimes only through their effects on neigh-
bouring sounds. The case study will therefore
aim to propose an attempt to predict the laryn-
geals and to uncover possible inconsistencies in
the phonetic system of PIE. The three laryngeals
(h1, h2, and h3) are reconstructed sounds in PIE
whose exact phonetic value is unknown. Apart
from some direct evidence of laryngeal reflexes in
the Anatolian languages, most of our knowledge
of those sounds stems from structural and pho-
netic patterns the laryngeals induced in the daugh-
ter languages before they faded altogether. Pre-
vious research interprets the laryngeals h1 : h2 :
h3 as [P]/[h] : [X]/[x]/[G]/[Q] : [Gw]/[Qw]/[K] .(Ras-
mussen, 1994; Kümmel, 2007; Meier-Brügger
et al., 2010; Beekes, 1994; Bomhard, 2004; Gip-
pert, 1994; Weiss, 2016; Mayrhofer and Cowgill,
1986)

3.1 The data
One of the best resources to obtain reconstructed
word data that is already digital is the English
version of Wiktionary.3 Its validity as a reposi-
tory of data for linguistic research has been as-

2See Byrd (2015); Beekes and Vaan (2011); Clackson
(2007) for a comprehensive overview of the scientific debate.

3https://en.wiktionary.org, accessed: 2019-
03-13

sessed by multiple studies and many other stud-
ies have already used its database for linguistic in-
quiry (e.g. Chiarcos et al., 2013; Navarro et al.,
2009; de Melo, 2015; Zesch et al., 2008; Meyer
and Gurevych, 2012). Especially regarding recon-
structed language data, Wiktionary has the deci-
sive advantage that the reconstructions follow cer-
tain guidelines (see Wiktionary contributors) un-
like data collected from various different tradi-
tional dictionaries.

For this study, I extracted all PIE reconstruc-
tions found in page headings from the English
Wiktionary .xml dump on 20.10.2018. Such a
dump file contains all English Wiktionary pages
including page and edit histories. The lemmas that
were extracted were subsequently split into seg-
ments of trigrams: preceding sound, target sound
and following sound with a final trigram count
of 7782. Where a trigram contained a root end-
ing, ‘-’ was used as following sound to encode
the root ending, cases of word-final or word-initial
were added as ‘zero’ in the preceding or follow-
ing sound slot, respectively. Each sound was ulti-
mately classified according to its place and manner
of its articulation according to the reconstructed
phonetic inventory of PIE most scholars agree on
(e.g. Clackson, 2007, 34; Beekes and Vaan, 2011,
119; Ringe, 2017, 8) without considering the glot-
talic theory.4

3.2 Approaches to verify the method

Before we are able to apply any machine learn-
ing techniques to the data, we need to establish
whether coarticulatory and statistical constraint ef-
fects exist in PIE and that the method is actu-
ally feasible for predicting sound features in gen-
eral. Although there have been studies suggesting
the existence of such effects as mentioned above,
a preliminary analysis needs to be conducted to
demonstrate the data shows these effects and that
a deep neural network can indeed ‘learn’ them and
make correct predictions on the basis of the ob-
served patterns.

For this reason, I set up a generalized linear
logistic regression model as an example to deter-
mine the phonetic effects on the occurrence of the
feature aspirated in PIE. The model was fit for
best AIC through both top-down and bottom-up
fitting. Before fitting, aliases were removed as

4For the full list of features used in this study, please refer
to the appendix.

https://en.wiktionary.org


101

well as collinear predictors up to a cutoff-point of
Variance Inflation Factor (VIF) greater than 4.

Estimate Std. Error z value Pr(>|z|)
(Intercept) -2.302 0.091 -25.312 0.000
labial preceding -1.504 0.240 -6.269 0.000
sibilant preceding -2.310 0.587 -3.938 0.000
liquid preceding -0.843 0.245 -3.446 0.001
syllabic cons. preceding 1.137 0.380 2.994 0.003
back vowel preceding -1.131 0.267 -4.232 0.000
mid vowel preceding -1.022 0.178 -5.730 0.000
close vowel preceding -0.947 0.468 -2.021 0.043
h1 preceding -1.519 0.518 -2.933 0.003
h2 preceding -2.106 0.513 -4.103 0.000
h3 preceding -1.185 0.598 -1.981 0.048
word boundary following 1.439 0.154 9.336 0.000
voiceless cons. following -2.279 0.390 -5.848 0.000
nasal following -1.691 0.512 -3.304 0.001
liquid following 0.444 0.175 2.543 0.011
syllabic cons. following 0.381 0.185 2.055 0.040
velar following 1.487 0.509 2.919 0.004
back vowel following 0.526 0.170 3.091 0.002
plosive following -1.041 0.415 -2.510 0.012
h2 following -2.494 1.006 -2.481 0.013

Table 1: Generalized linear logistic regression for the
occurrence of the feature aspirated

It can be observed in table 1 that several pre-
dictors were significant. E.g. preceding sibilant
reduces the probability of the target sound being
aspirated whereas following velar increases this
probability. As suggested by this model, the data
contains information on coarticulatory and statisti-
cal constraint effects the neural network can draw
upon.

As a second approach to ensure that the pre-
sented method and data is suitable for predicting
sound features, I conducted a preliminary study
using the same method to predict the features of
New High German sounds. For this analysis, I
utilized the German phonology lemma data from
CELEX2 (Baayen et al., 1995) in the syllabified
phonetic lemma transcription with stress in the
DISC character set (PhonStrsDISC). After extrac-
tion from the CELEX2 file, the data were pre-
pared using the same process as for the PIE data
with a final sample size of 441236 German tri-
grams. The method was simultaneously tested
with a dataset in which each lemma was oversam-
pled proportional to its frequency of occurrence in
the ‘Mannheimer Korpus’ provided by CELEX2
(Mann_Freq) (see Gulikers et al., 1995). While
this approach would ideally proportion the dataset
more realistically and could, in theory, improve
model training, it did not enhance the performance
of the network and was therefore discarded.

Each sound of those trigrams was classified
according to 38 phonetic features (e.g. conso-

nant, nasal, plosive) where 0 and 1 indicate the
absence/presence of a particular feature, respec-
tively.5 Note, that these 38 features contain some
redundancies (e.g. vowels are entirely contained
in the feature continuant). This is due to the fact
that a deep neural network performs best on as
many input features as possible since there might
be some relevant signal in a seemingly redun-
dant or unimportant feature vector. Accordingly,
specifying two complementary features like e.g.
voiced and voiceless can increase the network’s
performance since the two categories only apply
to consonants. Otherwise, a single binary fea-
ture [+voice] would not only encode voiced con-
sonants but also all vowels and therefore decrease
the ability of the network to detect voiced conso-
nants specifically. Redundancy itself is also not a
problem as redundant or irrelevant information in
the data is weighted less important during training
while the network focuses on those features that
have predictive power.

Also, only basic features (13 features in total
for consonants and 10 features for vowels) such
as e.g. consonant, velar and labial were used
as target features for the prediction of German
sound features. The reason for this decision was
that the more fine-grained the distinctions become,
the fewer occurrences of the feature there are on
which the network can train. Therefore, although
the feature liquid containing German r and l was
further divided into rhotic and lateral as features
contained in the classification of the phonetic en-
vironments, only liquid was tested as a target fea-
ture. If rhotic were tested as target feature on a
sound with unknown features, the network would
train only on the sound r and therefore not neces-
sarily train on the feature rhotic but rather learn to
discriminate r from all other sounds which has in
turn little explanatory power when predicting the
rhotic feature for other sounds.

The method was tested on the German sounds
p, r, E:, a: as an arbitrary preliminary selection that
ideally is representative of all other sounds in the
New High German phonetic inventory. Therefore,
four datasets were prepared, where the respective
sound was removed as target sound and its pres-
ence in any phonetic environment was indicated
by adding a new feature only for this sound. For
example when the phonetic environment in a par-

5For the full list of features used in this study, please refer
to the appendix.



102

ticular trigram contained r while r was the sound
to be later predicted by the network, r was clas-
sified in a dummy feature category that only en-
codes presence/absence of this particular sound.
This procedure is necessary since removing all in-
stances of the particular sound, r in this case, in the
phonetic environment would reduce the number of
environments and therefore distort the data.

After data preparation, a single network was
set up for each feature and trained one feature at
a time with a binary output to predict the pres-
ence or absence of the feature. I.e. this binary
network was trained to detect a particular feature
and to predict its presence or absence for unseen
sound environment data. After the entire data were
shuffled and the test and validation data were sep-
arated from the training sets using the Stratified
ShuffleSplit cross-validator included in the python
package scikit-learn (Pedregosa et al., 2011), the
training sets were over-sampled before each run
to counter class imbalance with the SMOTE al-
gorithm (Chawla et al., 2002) implemented in the
‘Imbalanced-learn’ (Lemaître et al., 2017) python
package. The network was trained for 30 epochs
using the optimizer Adam with a learning rate of
0.01 with a batch size of 250 samples with the
layer configuration displayed in table 2.

Layer Layer size Activation
Dense layer 1 256 ReLU
Dense layer 2 128 ReLU
Dense layer 3 64 ReLU
Dense layer 4 32 ReLU
Output layer 2 softmax

Table 2: Network architecture for the German feature
prediction task

For the subsequent evaluation of the model per-
formance, weights and biases were used form the
epoch at which the network performed best on
the validation data during training using the Keras
callback ModelCheckpoint (Chollet et al., 2015).
This procedure minimizes the risk of the model
being stuck at a local minimum in the search space
at the time training stops after an arbitrarily cho-
sen number of epochs. It has been established
in preliminary tests that the model performance
was enhanced when training on an all-consonant
or all-vowel subset of the data: First, a model was
trained to predict the feature [± consonant] and af-
ter the prediction, the main model was trained on
consonant or vowel data according to the predic-
tion of the preliminary model. After each train-
ing, the network performance was evaluated and

subsequently tasked with predicting the particular
feature for the respective test sound. The results
are presented in tables 3, 4, 5, and 6 which show
which number of samples in the test sets were clas-
sified correctly or incorrectly. I.e. 24656 conso-
nant samples in the column TP means that 24656
samples of all positive samples in the test set were
correctly classified as positive. Similarly, in ta-
ble 3 in the first row, 7211 samples in prediction:
feature present denote that 7211 of all tested in-
stances of p were classified as [+consonant].

Note that model accuracy metrics such as F1
score, precision, or recall are not given here since
these measures only evaluate a classifier’s perfor-
mance on a mixed dataset. Because the method
proposed here aims at performing well on deter-
mining whether a sound shows a given feature and
since this feature is either present in all samples
of this sound or absent in all samples, the main
goal is that the deep network yields more true pos-
itives than false negatives and more true negatives
than false positives. Applied to the example in
table 3 this means that since German p is [+con-
sonant], ideally the majority of classified samples
will be classified as such. If after model evaluation
the number of false negatives were higher than the
number of true positives, the model would likely
not be able to classify the majority of samples cor-
rectly. More samples would end up being incor-
rectly labeled as negatives as a result of the poor
model training yielding more false negatives than
true positives. Therefore, a high false positive or
false negative count is not a concern in itself as
long as the ratio of true positives to false negatives
and true negatives to false positives is always in fa-
vor of true positives or true negatives, respectively.

Feature TP FN FP TN Pred: feat. present Pred: feat. absent
consonant 24656 2184 859 15541 7211 1627

nasal 3885 1553 4255 17148 2609 6229
plosive 5417 1984 4341 15099 4860 3978
affricate 732 172 6750 19187 2352 6486
fricative 7156 3627 4272 11786 2394 6444
liquid 4698 1615 5361 15167 1670 7168

sibilant 2148 1072 5676 17945 1634 7204
voiced 11560 3681 3442 8158 3582 5256
labial 3447 864 7656 14874 5507 3331

dental/alveolar 8747 4093 3834 10167 4155 4683
palatal 1019 270 4497 21055 2373 6465

velar/uvular 4896 3035 4200 14710 1972 6866
glottal 428 43 5481 20889 1856 6982

Table 3: Network evaluations and predictions for Ger-
man p

The results show that all 13 tested features of
p are predicted correctly, r is correctly predicted
to be a voiced liquid, yet regarding place of artic-
ulation, which in German r-allophones is ranging



103

Feature TP FN FP TN Pred: feat. present Pred: feat. absent
consonant 21986 1739 1311 15089 39071 920

nasal 3722 1716 2702 15585 16238 23753
plosive 5524 2761 3082 12358 6333 33658
affricate 732 172 6339 16482 7972 32019
fricative 4881 1903 4314 12627 11352 28639
liquid 1604 710 5259 16152 22942 17049

sibilant 2172 1048 4683 15822 8997 30994
voiced 8006 3236 3568 8915 30068 9923
labial 3907 1288 6205 12325 12071 27920

dental/alveolar 8974 3865 2844 8042 28021 11970
palatal 1006 283 3138 19298 3895 36096

velar/uvular 2916 1015 4937 14857 11004 28987
glottal 432 39 4728 18526 8695 31296

Table 4: Network evaluations and predictions for Ger-
man r

Feature TP FN FP TN Pred: feat. present Pred: feat. absent
consonant 25884 1840 1111 15105 204 1638

front vowel 3658 1963 2714 7881 589 1253
central vowel 4546 1667 2529 7474 858 984
back vowel 1920 1032 3391 9873 831 1011

round 1395 653 3845 10323 898 944
close 3054 1729 2132 9301 493 1349
mid 5790 1670 2525 6231 585 1257
open 2582 1391 3110 9133 1219 623

diphthong 1097 333 2776 12010 464 1378
long 6595 1544 2365 5712 1248 594

Table 5: Network evaluations and predictions for Ger-
man E:

from alveolar to uvular (cf. Meinhold and Stock,
1982, 131-133), only dental/alveolar is predicted
which makes a total of 11 out of 13 features. The
German vowels were less well detected, with a
total of 8 out of 10 for E: and 6 out of 10 for
a:. Although the model performs on some sounds
and features better than on others, it performs bet-
ter than expected by chance. Since these results
stem from a selected set of sounds in a prelimi-
nary study, specific questions as to which features
are detected better than others and why some fea-
tures are incorrectly predicted for certain kinds of
sounds need to be established in further research.

3.3 The deep learning method applied to
Proto-Indo-European

To prepare the PIE data for training, the data were
randomly shuffled and split into training and test
set using the Stratified ShuffleSplit cross-validator
included in the python package scikit-learn (Pe-
dregosa et al., 2011). Afterwards, the training
set was first oversampled with the SMOTE algo-
rithm and subsequently under-sampled by remov-
ing Tomek links using SMOTETomek (Batista
et al., 2003) implemented in the ‘Imbalanced-
learn’ (Lemaître et al., 2017) python package to
counter class imbalance in the dataset. Yet the
SMOTE over-sampling process performed on the
minority group increases the dataset’s variation,
so to cope with this variation and to make sure
that findings were not due to random biases dur-

Feature TP FN FP TN Pred: feat. present Pred: feat. absent
consonant 25694 2030 1102 14445 597 7936

front vowel 3864 1941 2457 7285 4691 3842
central vowel 3996 1364 2302 7885 2760 5773
back vowel 1877 1075 2846 9749 2720 5813

round 1377 671 3550 9949 3848 4685
close 3177 1606 2282 8482 2403 6130
mid 5953 1691 2303 5600 5151 3382
open 2070 1050 2827 9600 3104 5429

diphthong 1164 266 2810 11307 2178 6355
long 5834 1636 2121 5956 4839 3694

Table 6: Network evaluations and predictions for Ger-
man a:

ing oversampling or stratification, I ran each net-
work 100 times to have a representative number
of slightly varying model outputs. Each of these
runs yields a confusion matrix with the count of
true positive, false negative, false positive and true
negative predictions of the test samples. To de-
termine whether the model performs significantly
better than expected by a random class assign-
ment, all confusion matrices were compared using
Wilcoxon signed rank tests with continuity correc-
tion. For each model, I performed this test on the
output of the 100 runs of true positives vs. false
negatives to determine whether the network can
clearly find a present feature and a second test on
the 100 runs of false positives vs. true negatives to
determine whether the network can clearly find the
absence of a feature. When the Wilcoxon signed
rank test is significant, the tested groups are ‘non-
identical’ populations.

3.4 Example 1: The phonetic quality of the
PIE laryngeals

In the following stage, a deep neural network can
be set up to learn to detect the feature aspirated
and to subsequently predict whether the laryngeals
had this feature.

The network was trained for 50 epochs using the
optimizer Adam with a learning rate of 0.01 and a
batch size of 64 samples with the layer configura-
tion displayed in table 7.

Layer Layer size Activation
Dense layer 1 128 ReLU

Dropout layer 1 0.25 dropout rate
Dense layer 2 64 ReLU

Dropout layer 2 0.25 dropout rate
Dense layer 3 32 ReLU
Output layer 2 softmax

Table 7: Network architecture for the feature aspirated

The dropout layers in this network architec-
ture were implemented to reduce the effect of
over-fitting due to the limited amount of training
samples. Analogous to the training on the mod-



104

ern German dataset above, only weights and bi-
ases form the epoch at which the network per-
formed best on the validation data during training
were used. As mentioned above, the network was
trained and evaluated 100 times in order to further
minimize the effect of accidental findings in sin-
gle runs. The results are listed in table 8 which
is a summary of all test set prediction confusion
matrices obtained in the 100 runs.6

True positives False negatives False positives True negatives
Mean 58.32 19.68 219.5 602.5

Median 58 20 221 601
Std. dv. 2.044 6.920

Table 8: Statistics of the confusion matrices from 100
runs for classifying the feature aspirated

Subsequently, a Wilcoxon signed rank tests
with continuity correction with the alternative hy-
pothesis H1: True positives greater than false neg-
atives gives W = 10000.00 p < 0.00001. A second
Wilcoxon signed rank tests with continuity cor-
rection with the alternative hypothesis H1: True
negatives greater than false positives gives W =
10000.00 p < 0.00001. These test statistics show
that in these 100 runs, the network was able to de-
tect the feature aspirated reliably and, most im-
portantly, when presented with an unseen dataset
which either contains sounds that have the feature
aspirated or sounds that do not, the network will
correctly identify over 70 percent of the samples.
The variance in the prediction accuracy in table
8 can be explained by, as previously addressed,
noise in the data and variation in the partition-
ing and subsequent oversampling of the training
set. Having established the functioning network,
the model can be used to predict the target feature
for sounds with unknown qualities. Since the la-
ryngeals cannot be assigned a phonetic value by
means of the comparative method, their properties
can be predicted. To achieve this, the phonetic en-
vironment was passed through the networks after
training at the end of each of the 100 runs. The
output of every prediction is a classification matrix
for each of the three laryngeals. Table 9 shows the
summary of these classification matrices.

To determine the significance of these findings,
Wilcoxon signed rank tests with continuity cor-
rection were applied to the predictions. Table 10

6The figures in the tables provided here and below repre-
sent the number of classified samples from the test set. E.g.
a mean of 58.32 in true positives means that from all posi-
tive samples in the test set, an average of 58.32 samples were
classified correctly as positive.

h1 h2 h3
Positives Negatives Positives Negatives Positives Negatives

Mean 113.8 85.2 160.5 187.5 54.06 51.94
Median 115 84 163 187 53 53
Std. dv. 4.141 4.079 1.879

Table 9: Prediction results by the trained model for the
laryngeal feature aspirated

shows the test results. The networks trained on de-

h1 h2 h3
H1 W p-value W p-value W p-value

P greater N 9991.00 < 0.00001 0 1 7477.00 < 0.00001
N greater P 9.00 1 10000 < 0.00001 2523.00 1

Table 10: Results of Wilcoxon signed rank test with
continuity correction applied to the predictions for each
laryngeal with H1: positives greater than negatives and
H1: negatives greater than positives

tecting the feature aspirated clearly predict the as-
pirated feature for h1. For h2, the model clearly
rejects the feature aspirated. In the case of h3,
the statistical tests indicate that the laryngeal pos-
sessed the feature aspirated, however because of
the thin difference in the number of predicted sam-
ples, we still need to treat this finding with caution,
since the feature is not as clearly predicted for h3
as it is for h1. It is likely that the aspiration present
in h3 is weaker than or different from that of h1.

3.5 Example 2: The internal coherence of
PIE nasals

Besides predicting phonetic features of unknown
sounds, the deep neural networks can moreover
detect inconsistencies or idiosyncrasies in PIE.
One such example is the feature nasal which is
present in both PIE non-syllabic (*m, *n) and syl-
labic nasals (*m. , *n. ). While both are regarded
to be phonetically identical and only differing in
their syllabicity (Clackson, 2007, 35), an inves-
tigation using the deep neural network approach
gives some insights into their relationship to one
another: To analyze this feature, a deep neural net-
work was set up with the architecture displayed in
table 11.

Layer Layer size Activation
Dense layer 1 128 ReLU

Dropout layer 1 0.25 dropout rate
Dense layer 2 64 ReLU

Dropout layer 2 0.25 dropout rate
Dense layer 3 32 ReLU
Output layer 2 softmax

Table 11: Network architecture for the feature nasal

The method used in this case is equal to the
training and evaluation procedure of the model



105

used in 3.4. The resulting confusion matrices ob-
tained after each evaluation of the 100 training
runs are summarized in table 12.

True positives False negatives False positives True negatives
Mean 49.32 53.68 135.5 661.5

Median 48 55 118.5 678.5
Std. dv. 6.689 38.951

Table 12: Statistics of the confusion matrices from 100
runs for classifying the feature nasal

As this summary shows, the neural network had
more difficulties learning the properties of nasal
than it had learning the feature aspirated. The
classifier only detects the feature less than 50 per-
cent of the time it is presented with nasal sounds,
which is approximately what could be expected
by randomly classifying the rest samples. More-
over, a Wilcoxon signed rank test with continu-
ity correction with the alternative hypothesis H1:
True positives greater than false negatives gives
W = 3144 p = 1. As a result, it was not possi-
ble to successfully train the network on this fea-
ture. Given the large discrepancy in performance
between this and the previous network and the fact
that both models were optimized using the same
methods, the problem must be data inherent. This
finding raises the question of why exactly this se-
ries differs from the other features. This leaves
three possible explanations: (1) The data contain-
ing the nasals is noisier compared to the other pho-
netic features so that the classifier cannot train on
a consistent set of properties. Although data can
be varying degrees of noisy, it is unlikely that this
feature is overly affected by noise. (2) The nasal
feature was weakly articulated in PIE and thus it
had little effect on its environment. An effect so
small that it did not leave stable traces the classi-
fier could detect. (3) The third explanation is that
the nasal series does not possess internal coher-
ence. This reason is arguably the most probable
given that the nasals consist of two different sets
of nasals that contrast in their syllabicity, espe-
cially since syllabic and non-syllabic resonants are
also allophones and are therefore in complemen-
tary distribution (cf. Schindler, 1977). Yet since
the model was trained on detecting nasality – not
syllabicity – while there were other syllabic con-
sonants in the non-nasal group, it is also possible
that the model is not solely misled by the differ-
ence in syllabicity and their complementary distri-
bution. There might also be a difference in nasal-
ity itself which results in the feature not forming a

consistent, classifiable group. In other words, the
syllabic and non-syllabic nasals might additionally
have also differed in their nasality (i.e. nasality be-
ing differently articulated in both cases), yet this
observation needs to be further investigated before
one can make more substantiated claims.

4 Conclusion

As has been demonstrated in this paper, using
deep neural networks in historical phonetics is a
viable method to predict unknown features and
to uncover previously unnoticed inconsistencies
within a language’s phonetic system. The tool
is specifically powerful for historical linguistics
since it does not rely on diachronic methods such
as the comparative method to analyze and deter-
mine phonetic features but can draw upon syn-
chronic phonetic patterns arising from coarticu-
lation and statistical constraints. The results ob-
tained through the machine learning technique
presented in this paper are moreover reproducible
and empirical, and can therefore be seen as com-
plementary to previous results obtained by other
empirical approaches such as the comparative
method. However, the specific strengths and
weaknesses of this method need to be further in-
vestigated in future research.

References
R Baayen, Richard Piepenbrock, and Léon Gulikers.

1995. Celex2 ldc96l14. Web Download. Philadel-
phia: Linguistic Data Consortium.

Gustavo EAPA Batista, Ana LC Bazzan, and
Maria Carolina Monard. 2003. Balancing training
data for automated annotation of keywords: a case
study. In WOB, pages 10–18.

R. S. P. Beekes. 1994. Who were the laryngeals. In
Jens Elmegård Rasmussen, editor, In honorem Hol-
ger Pedersen, pages 450–454. Reichert, Wiesbaden.

R. S. P. Beekes and Michiel de Vaan. 2011. Compar-
ative Indo-European linguistics: An introduction, 2.
ed. edition. Benjamins, Amsterdam.

Juliette Blevins. 2015. Evolutionary phonology: A
holistic approach to sound change typology. In
Patrick Honeybone and Joseph Curtis Salmons, edi-
tors, The Oxford handbook of historical phonology,
Oxford handbooks in linguistics, pages 485–500.
Oxford University Press, Oxford.

Allan R. Bomhard. 2004. The proto-indo-european la-
ryngeals. In Adam Hyllested and Jens Elmegård



106

Rasmussen, editors, Per aspera ad asteriscos, Inns-
brucker Beiträge zur Sprachwissenschaft, pages 69–
80. Institut für Sprachen und Literaturen der Univer-
sität Innsbruck, Innsbruck.

Andrew Byrd. 2015. The Indo-European Syllable.
Brill, Leiden.

Lyle Campbell. 2013. Historical linguistics. Edin-
burgh University Press, Oxford.

Nitesh V Chawla, Kevin W Bowyer, Lawrence O Hall,
and W Philip Kegelmeyer. 2002. Smote: synthetic
minority over-sampling technique. Journal of artifi-
cial intelligence research, 16:321–357.

Christian Chiarcos, John McCrae, Philipp Cimiano,
and Christiane Fellbaum. 2013. Towards Open Data
for Linguistics: Linguistic Linked Data, pages 7–25.
Springer Berlin Heidelberg, Berlin, Heidelberg.

François Chollet. 2018. Deep learning with Python.
Manning Publications Company, Shelter Island.

François Chollet et al. 2015. Keras. https://
keras.io.

James Clackson. 2007. Indo-European linguistics: An
introduction / James Clackson. Cambridge text-
books in linguistics. Cambridge University Press,
Cambridge.

Patricia J. Donegan and Geoffrey S. Nathan. 2015.
Natural phonology and sound change. In Patrick
Honeybone and Joseph Curtis Salmons, editors, The
Oxford handbook of historical phonology, Oxford
handbooks in linguistics, pages 431–449. Oxford
University Press, Oxford.

Benjamin W Fortson IV. 2011. Indo-European lan-
guage and culture: An introduction, volume 30.
John Wiley & Sons, Hoboken.

Carol A Fowler. 1980. Coarticulation and theories of
extrinsic timing. Journal of Phonetics, 8(1):113–
133.

Jost Gippert. 1994. Zur phonetik der laryngale. In
Jens Elmegård Rasmussen, editor, In honorem Hol-
ger Pedersen, pages 455–466. Reichert, Wiesbaden.

Léon Gulikers, Gilbert Rattink, and Richard Piepen-
brock. 1995. German linguistic guide. The CELEX
Lexical Database (CD-ROM). Linguistic Data Con-
sortium, Philadelphia, PA.

Mark Hale. 2003. Neogrammarian sound change. In
Brian D. Joseph, editor, The handbook of histori-
cal linguistics, Blackwell handbooks in linguistics,
pages 343–368. Blackwell, Malden, MA.

William J. Hardcastle and Nigel Hewlett. 2006. Coar-
ticulation: Theory, data and techniques. Cambridge
University Press, Cambridge.

Gerhard Jäger, Johann-Mattis List, and Pavel
Sofroniev. 2017. Using support vector ma-
chines and state-of-the-art algorithms for phonetic
alignment to identify cognates in multi-lingual
wordlists. In Proceedings of Conference: Volume 1:
Long Papers,ACL, pages 1204–1226.

Gerhard Jäger and Pavel Sofroniev. 2016. Auto-
matic cognate classification with a support vec-
tor machine. In Proceedings of the 13th Con-
ference on Natural Language Processing (KON-
VENS), Bochumer Linguistische Arbeitsberichte,
pages 128–134.

Barbara Kühnert and Francis Nolan. 1999. The
origin of coarticulation. In Nigel Hewlett and
William J. Hardcastle, editors, Coarticulation, Cam-
bridge studies in speech science and communica-
tion, pages 7–30. Cambridge University Press, Cam-
bridge.

Martin Joachim Kümmel. 2007. Konsonantenwan-
del: Bausteine zu einer Typologie des Lautwandels
und ihre Konsequenzen für die vergleichende Rekon-
struktion: Teilw. zugl.: Freiburg, Univ., Habil.-Schr.,
2005. Reichert, Wiesbaden.

Guillaume Lemaître, Fernando Nogueira, and Chris-
tos K. Aridas. 2017. Imbalanced-learn: A python
toolbox to tackle the curse of imbalanced datasets
in machine learning. Journal of Machine Learning
Research, 18(17):1–5.

Manfred Mayrhofer and Warren Cowgill. 1986. In-
dogermanische Grammatik. Bd 1. Winter, Heidel-
berg.

Michael Meier-Brügger, Matthias Fritz, and Man-
fred Mayrhofer. 2010. Indogermanische Sprachwis-
senschaft, 9., durchgesehene und ergänzte auflage
2010 edition. De Gruyter Studium. De Gruyter,
Berlin.

Gottfried Meinhold and Eberhard Stock. 1982.
Phonologie der deutschen Gegenwartssprache, sec-
ond edition edition. Bibliographisches Institut,
Leipzig.

Gerard de Melo. 2015. Wiktionary-based word embed-
dings. Proceedings of MT Summit XV, pages 346–
359.

Christian M. Meyer and Iryna Gurevych. 2012. Wik-
tionary: A new rival for expert-built lexicons? ex-
ploring the possibilities of collaborative lexicogra-
phy. In Magali Gragner, Sylviana;Paquot, editor,
Electronic Lexicography. Oxford University Press,
Oxford.

Emmanuel Navarro, Franck Sajous, Bruno Gaume,
Laurent Prévot, Hsieh ShuKai, Kuo Tzu-Yi, Pierre
Magistry, and Huang Chu-Ren. 2009. Wiktionary
and nlp: Improving synonymy networks. In Pro-
ceedings of the 2009 Workshop on The People’s Web
Meets NLP: Collaboratively Constructed Semantic

https://doi.org/10.1007/978-3-642-31782-8_2
https://doi.org/10.1007/978-3-642-31782-8_2
https://keras.io
https://keras.io
https://doi.org/10.1017/CBO9780511486395.002
https://doi.org/10.1017/CBO9780511486395.002
http://jmlr.org/papers/v18/16-365.html
http://jmlr.org/papers/v18/16-365.html
http://jmlr.org/papers/v18/16-365.html
http://site.ebrary.com/lib/alltitles/docDetail.action?docID=10498675
http://site.ebrary.com/lib/alltitles/docDetail.action?docID=10498675
http://dl.acm.org/citation.cfm?id=1699765.1699768
http://dl.acm.org/citation.cfm?id=1699765.1699768


107

Resources, People’s Web ’09, pages 19–27, Strouds-
burg, PA, USA. Association for Computational Lin-
guistics.

Michael A. Nielsen. 2015. Neural Networks and Deep
Learning. Determination Press.

J. J. Ohala. 1993a. Coarticulation and phonology. Lan-
guage and speech, 36 ( Pt 2-3):155–170.

J. J. Ohala. 1993b. The phonetics of sound change. In
Charles Jones, editor, Historical linguistics, Long-
man linguistics library, pages 237–278. Longman,
London.

F. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel,
B. Thirion, O. Grisel, M. Blondel, P. Pretten-
hofer, R. Weiss, V. Dubourg, J. Vanderplas, A. Pas-
sos, D. Cournapeau, M. Brucher, M. Perrot, and
E. Duchesnay. 2011. Scikit-learn: Machine learning
in Python. Journal of Machine Learning Research,
12:2825–2830.

Uriel Cohen Priva. 2015. Informativity affects con-
sonant duration and deletion rates. Laboratory
Phonology, 6(2):243–278.

Jens Elmegård Rasmussen, editor. 1994. In honorem
Holger Pedersen: Kolloquium der Indogermanis-
chen Gesellschaft vom 25. bis 28. März 1993 in
Kopenhagen. Reichert, Wiesbaden.

William Raymond, Robin Dautricourt, and Elizabeth
Hume. 2006. Word-medial /t,d/ deletion in spon-
taneous speech: Modeling the effects of extra-
linguistic, lexical, and phonological factors. Lan-
guage Variation and Change, 18:55—-97.

Donald A. Ringe. 2017. From Proto-Indo-European
to Proto-Germanic, second edition edition, volume
volume I of A linguistic history of English. Oxford
University Press, Oxford.

Jochem Schindler. 1977. Notizen zum sieversschen
gesetz. Die Sprache, 23(1):56–65.

Rob JJH Van Son and Jan PH Van Santen. 2005. Dura-
tion and spectral balance of intervocalic consonants:
A case for efficient communication. Speech Com-
munication, 47(1-2):100–123.

Michael Weiss. 2016. The proto-indo-european laryn-
geals and the name of cilicia in the iron age. In An-
drew Miles Byrd, Jessica DeLisi, and Mark Wen-
the, editors, Tavet tat satyam, pages 331–340. Beech
Stave Press, Ann Arbor and New York.

Wiktionary contributors. Wiktionary:about proto-indo-
european — Wiktionary, the free dictionary. Ac-
cessed: 2019-03-13.

Torsten Zesch, Christof Müller, and Iryna Gurevych.
2008. Using wiktionary for computing semantic re-
latedness. In AAAI, volume 8, pages 861–866.

A Appendices

word boundary zero
nasal m, n, N

plosive p, t, d, k, g, b
affricate pf, ts, tS
fricative f, v, s, z, S, Z, x/ç, h
liquid l, r
rhotic r
lateral l
sibilant s, z, S, Z
voiced m, n, N, d, g, Z, v, l, r, z, b

voiceless p, t, k, pf, tS, S, s, f, x/ç, h
labial m, p, pf, f, v, b

bilabial m, p, b
labiodental pf, f, v

dental/alveolar n, t, d, ts, s, z, l, r
palatal S, Z, tS, r

velar/uvular N, k, g, x/ç, r
glottal h

obstruent p, b, t, d, k, g, pf, ts, tS, f, v, s, z,
S, Z, x/ç, h, r

sonorant m, n, N, I, i, E, e, y, Y, ø, œ, æ, @,
a, A, u, U, o, O, aI

“
, aU

“
, OY

“occlusive p, b, t, d, k, g, m, n, N, pf, ts, tS
continuant f, v, s, z, S, Z, x/ç, h, r, I, i, E, e,

aI
“
, OY

“
, y, Y, ø, œ, æ, @, a, A, u, U,

o, O
consonant m, n, N, d, g, Z, v, l, r, b, p, t, k,

pf, tS, S, s, z, f, x/ç, h
front vowel I, i, E, e, y, Y, ø, œ, æ

central vowel @, a, A
back vowel u, U, o, O

close i, I, u, U, y, Y
mid E, e, @, o, O, ø, œ, æ
open a, A

diphthong aI
“
, aU

“
, OY

“open diphthong aI
“
, OY

“mid diphthong aU
“front diphthong aI
“
, aU

“back diphthong OY
“round o, O, y, Y, ø, œ

unround I, i, E, e, @, a, A, u, U, æ
long i, e, a, u, o, æ, œ, y
short I, E, U, A, Y, ø, @

Table 13: Phonetic feature assignment of each consid-
ered New High German sound

https://doi.org/10.1177/002383099303600303
https://doi.org/10.1093/oso/9780198792581.001.0001
https://doi.org/10.1093/oso/9780198792581.001.0001
https://en.wiktionary.org/wiki/Wiktionary:About_Proto-Indo-European
https://en.wiktionary.org/wiki/Wiktionary:About_Proto-Indo-European


108

root ending -
word boundary final/initial zero

voiced *bh, *dh, *ǵh, *gh, *gw, *b, *d,
*ǵ, *g, *gw, *m, *m. , *n, *r, *l,
*l., *r., *y, *w

voiceless *p, *t, *s, *ḱ, *k, *kw

nasal *m, *m. , *n, *n.
aspirated *bh, *dh, *ǵh, *gh, *gwh

labial/labialized *m, *m. , *p, *b, *bh, *w, *kw,
*gw, *gwh

sibilant *s
liquid *r, *r., *l., *l

syllabic *r., *l., *m. , *n. , *i, *u, *ū
coronal *n, *n. , *t, *d, *dh, *s, *r, *l,

*l., *r.
postvelar *k, *g, *gh, *kw, *gw, *gwh

velar *ḱ, *ǵ, *ǵh

palatal *y
front vowel *e, *ē, *i
back vowel *o, *ō, *u, *ū

center vowel *a, *ā
short vowel *e, *o, *u, *a, *i
long vowel *ē, *ō, *ū, *ā
open vowel *a, *ā
close vowel *u, *ū, *i
laryngeal 1 *h1
laryngeal 2 *h2
laryngeal 3 *h3

unspecified laryngeal *H
consonant *bh, *dh, *ǵh, *gh, *gw, *b, *d,

*ǵ, *g, *gw, *m, *m. , *n, *r, *l,
*l., *r., *y, *w, *p, *t, *s, *ḱ,
*k, *kw

back consonant *k, *g, *gh, *kw, *gw, *gwh,
*ḱ, *ǵ, *ǵh

front consonant *m, *m. , *p, *b, *bh, *w, *s,
*r, *r., *l., *l, *n, *n. , *t, *d, *dh

stop *ḱ,*b, *bh, *p, *ǵ, *ǵh, *k, *g,
*gh, *kw, *gw, *gwh, *t, *d,
*dh

obstruent *ḱ, *p, *b, *bh, *ǵ, *ǵh, *k,
*g, *gh, *kw, *gw, *gwh, *t,
*d, *dh, *s

sonorant *m, *m. , *n, *n. , *r, *r., *y, *w,
*e, *o, *u, *a, *i, *ē, *ō, *ū,
*ā

occlusive *ḱ, *p, *b, *bh, *ǵ, *ǵh, *k,
*g, *gh, *kw, *gw, *gwh, *t,
*d, *dh, *m, *m. , *n, *n.

continuant *s, *y, *w, *e, *o, *u, *a, *i,
*ē, *ō, *ū, *ā

Table 14: Phonetic feature assignment of each consid-
ered PIE sound


