















































Context Dependent Semantic Parsing over Temporally Structured Data


Proceedings of NAACL-HLT 2019, pages 3576–3585
Minneapolis, Minnesota, June 2 - June 7, 2019. c©2019 Association for Computational Linguistics

3576

Context-Dependent Semantic Parsing over Temporally Structured Data

Charles Chen and Razvan Bunescu
School of Electrical Engineering and Computer Science, Ohio University

lc971015@ohio.edu, bunescu@ohio.edu

Abstract

We describe a new semantic parsing setting
that allows users to query the system using
both natural language questions and actions
within a graphical user interface. Multiple time
series belonging to an entity of interest are
stored in a database and the user interacts with
the system to obtain a better understanding of
the entity’s state and behavior, entailing se-
quences of actions and questions whose an-
swers may depend on previous factual or nav-
igational interactions. We design an LSTM-
based encoder-decoder architecture that mod-
els context dependency through copying mech-
anisms and multiple levels of attention over
inputs and previous outputs. When trained
to predict tokens using supervised learning,
the proposed architecture substantially outper-
forms standard sequence generation baselines.
Training the architecture using policy gradient
leads to further improvements in performance,
reaching a sequence-level accuracy of 88.7%
on artificial data and 74.8% on real data.

1 Introduction and Motivation

Wearable sensors are being increasingly used in
medicine to monitor important physiological pa-
rameters. Patients with type I diabetes, for exam-
ple, wear a sensor inserted under the skin which
provides measurements of the interstitial blood glu-
cose level (BGL) every 5 minutes. Sensor bands
provide a non-invasive solution to measuring addi-
tional physiological parameters, such as tempera-
ture, skin conductivity, heart rate, and acceleration
of body movements. Patients may also self-report
information about discrete life events such as meals,
sleep, or stressful events, while an insulin pump
automatically records two types of insulin inter-
ventions: a continuous stream of insulin called the
basal rate, and discrete self-administered insulin
dosages called boluses. The data acquired from sen-

sors and patients accumulates rapidly and leads to
a substantial data overload for the health provider.

To help doctors more easily browse the wealth of
generated patient data, we built a graphical user in-
terface (GUI) that displays the various time series
of measurements corresponding to a patient. As
shown in Figure 1, the GUI displays the data cor-
responding to one day, whereas buttons allow the
user to move to the next or previous day. While the
graphical interface was enthusiastically received by
doctors, it soon became apparent that the doctor-
GUI interaction could be improved substantially
if the tool also allowed for natural language (NL)
interactions. Most information needs are highly
contextual and local. For example, if the blood glu-
cose spiked after a meal, the doctor would often
want to know more details about the meal or about
the bolus that preceded the meal. The doctor often
found it easier to express their queries in natural
language (e.g. “show me how much he ate", “did he
bolus before that"), resulting in a sub-optimal situ-
ation where the doctor would ask this type of local
questions in English while a member of our team
would perform the clicks required to answer the
question, e.g. click on the meal event, to show de-
tails such as amount of carbohydrates. Furthermore,
there were also global questions, such as “How of-
ten does the patient go low in the morning and the
evening", whose answers would require browsing
the entire patient history in the worst case, which
would be very inefficient. This motivated us to start
work on a new system component that would allow
the doctor to interact using both natural language
queries and direct actions within the GUI. A suc-
cessful solution to the task described in this paper
has the potential for applications in many areas of
medicine where sensor data and life events are per-
vasive. Intelligent user interfaces for the proposed
task will also benefit the exploration and interpreta-
tion of data in other domains such as experimental



3577

Figure 1: GUI window displaying 1 day worth of data.

physics, where large amounts of time series data
are generated from high-throughput experiments.

2 Task Definition

Given an input from the user (a NL query or a direct
GUI interaction), the aim is to parse it into a logical
form representation that can be run by an inference
engine in order to automatically extract the answer
from the database. Table 1 shows sample inputs
paired with their logical forms. For each input, the
examples also show relevant previous inputs from
the interaction sequence. In the following sections
we describe a number of major features that, on
their own or through their combination, distinguish
this task from other semantic parsing tasks.

2.1 Time is essential

All events and measurements in the knowledge
base are organized in time series. Consequently,
many queries contain time expressions, such as
the relative “midnight" or the coreferential “then",
and temporal relations between relevant entities,
expressed through words such as “after" or “when".
This makes processing of temporal relations essen-
tial for a good performance. Furthermore, the GUI
serves to anchor the system in time, as most of the
information needs expressed in local questions are
relative to the day shown in the GUI, or the last
event that was clicked.

2.2 GUI interactions vs. NL questions

The user can interact with the system 1) directly
within the GUI (e.g. mouse clicks); 2) through natu-
ral language questions; or 3) through a combination
of both, as shown in Examples 1 and 2 in Table 1.
Although the result of every direct interaction with
the GUI can also be obtained using natural lan-
guage questions, sometimes it can be more conve-

Example 1
Click on Exercise event at 9:29am.
Click(e) ∧ e.type = Exercise ∧ e.time = 9:29am
Click on Miscellaneous event at 9:50am
Click(e) ∧ e.type = Misc ∧ e.time = 9:50am
Q1: What was she doing mid afternoon

when her heart rate went up?
Answer(e) ∧Behavior(e1.value,Up)

∧Around(e.time, e1.time)
∧e.type == DiscreteType
∧e1.type == HeartRate
∧e1.time == MidAfternoon()

Q2: What time did that start?
Answer(e(−1).time)

Example 2
Click on Bolus at 8:03pm.
Click(e) ∧ e.type = Bolus ∧ e.time = 8:03pm
Q3: What did she eat for her snack?
Answer(e.food) ∧ e.kind == Snack

Example 3
Click on Exercise at 7:52pm.
Click(e) ∧ e.type = Exercise ∧ e.time = 7:52pm
Q4: What did she do then?
Answer(e(−1).kind)
Q5: Did she take a bolus before then?
Answer(Any(d.type == Bolus

∧Before(d.time, e(−1).time)))
Example 4

Q6: What is the first day they have heart rate reported?
Answer(e.date)

∧Order(e, 1, Sequence(d, d.type == HeartRate))
Example 5

Q7: Is there another day he goes low in the morning?
Answer(Any(Hypo(d1) ∧ x! = CurrentDate
∧x.type == Date ∧ d1.time == Morning(x))

Table 1: Examples of interactions and logical forms.

nient to use the GUI directly, especially when all
events of interest are in the same area of the screen
and thus easy to move the mouse or hand from one
to the other. For example, a doctor interested in
what the patient ate that day can simply click on
the blue squares at the top of the bottom pane in
Figure 1, one after another. Sometimes a click can
be used to anchor the system at a particular time
during the day, after which the doctor can ask short
questions implicitly focused on that region in time.
An example of such hybrid behavior is shown in
Example 2, where a click on a Bolus event is fol-
lowed by a question about a snack, which implicitly
should be the meal right after the bolus.

2.3 Factual queries vs. GUI commands

Most of the time, doctors have information needs
that can be satisfied by clicking on an event shown
in the GUI or by asking factual questions about a
particular event of interest from that day. In con-



3578

trast, a different kind of interaction happens when
the doctor wants to change what is shown in the
tool, such as toggling on/off particular time series
(e.g. “toggle on heart rate"), or navigating to a dif-
ferent day (e.g. “go to next day", “look at the pre-
vious day"). Sometimes, a question may be a com-
bination of both, as in “What is the first day they
have a meal without a bolus?", for which the expec-
tation is that the system navigates to that day and
also clicks on the meal event to show additional
information and anchor the system at the time of
that meal.

2.4 Sequential dependencies

The user interacts with the system through a se-
quence of questions or clicks. The logical form of a
question, and implicitly its answer, may depend on
the previous interaction with the system. Examples
1 to 3 in Table 1 are all of this kind. In example 1,
the pronoun “that" in question 2 refers to the an-
swer to question 1. In example 2, the snack refers
to the meal around the time of the bolus event that
was clicked previously – this is important, as there
may be multiple snacks that day. In example 3, the
adverb “then" in question 5 refers to the time of the
event that is the answer of the previous question. As
can be seen from these examples, sequential depen-
dencies can be expressed as coreference between
events from different questions. Coreference may
also happen within questions, as in question 4 for
example. Overall, solving coreferential relations
will be essential for good performance.

3 Semantic Parsing Datasets

To train and evaluate semantic parsing approaches,
we created two datasets of sequential interactions: a
dataset of real interactions (Section 3.1) and a much
larger dataset of artificial interactions (Section 3.2).

3.1 Real Interactions

We recorded interactions with the GUI in real time,
using data from 9 patients, each with around 8
weeks worth of time series data. In each record-
ing session, the tool was loaded with data from
one patient and the physician was instructed to ex-
plore the data in order to understand the patient
behavior as usual, by asking NL questions or inter-
acting directly with the GUI. Whenever a question
was asked, a member of our study team found the
answer by navigating in and clicking on the cor-
responding event. After each session, the question

Event Types
Physiological Parameters:

BGL, BasalRate, TemporaryBasal, Carbs, GSR, InfusionSet,
AirTemperature, SkinTemperature, HeartRate, StepCount.

Life Events:
FingerSticks, Bolus, Hypo, HypoAction, Misc, Illness,
Meal, Exercise, ReportedSleep, Wakeup, Work, Stressors.

Constants
Up, Down, On, Off, Monday, Tuesday, ..., Sunday.

Functions
Interval(t1, t2), Before(t), After(t), ...

return corresponding intervals (default lengths).
Morning([d]), Afternoon([d]), Evening([d]), ...

return corresponding intervals for day d.
WeekDay(d):

return the day of the week of date d.
Sequence(var, statements):

return a chronologically ordered sequence
of possible values for var that satisfy statements.

Count(var[, statements]):
returns the number of possible values
for var that satisfy statements.

Predicates
Answer(e), Click(e)
Morning(t), Afternoon(t), Evening(t), ...
Overlap(t1, t2), Before(t1, t2), Around(t1, t2), ...
Behavior(variable, direction):

whether variable increases, if direction is Up,
(or decrease if direction is Down).

High(variable), Low(variable):
whether variable has some low value.

Order(event, ordinal, sequence[, attribute]):
whether the event is at place ordinal in sequence

Commands
DoClick, DoToggle, DoSetDate, DoSetTime, ...

Table 2: Vocabulary for logical forms.

segments were extracted manually from the speech
recordings, transcribed, and timestamped. All di-
rect interactions (e.g. mouse clicks) were recorded
automatically by the tool, timestamped, and ex-
ported into an XML file. The sorted list of ques-
tions and the sorted list of mouse clicks were then
merged using the timestamps as key, resulting in a
chronologically sorted list of questions and GUI in-
teractions. Mouse clicks were automatically trans-
lated into logical forms, whereas questions were
parsed into logical forms manually.

A snapshot of the vocabulary for logical forms
is shown in Table 2, showing the Event Types, Con-
stants, Functions, Predicates, and Commands. Ev-
ery life event or physiological measurement stored
in the database is represented in the logical forms
as an event object e with 3 major attributes: e.type,
e.date, and e.time. Depending on its type, an event
object may contain additional fields. For example,
if e.type = BGL, then it has an attribute e.value.
If e.type = Meal, then it has attributes e.food
and e.carbs. We use e(−i) to represent the event



3579

appearing in the ith previous logical form (LF).
Thus, to reference the event mentioned in the previ-
ous LF, we use e(−1), as shown for question Q5. If
more than one event appears in the previous LF, we
use an additional index j to match the event index
in the previous LF. Coreference between events
is represented simply using the equality operator,
e.g. e = e(−1).The dataset contains logical forms
for 237 interactions: 74 mouse clicks and 163 NL
queries.

3.2 Artificial Interactions

The number of annotated real interactions is too
small for training an effective semantic parsing
model. To increase the number of training exam-
ples, we designed and implemented an artificial
data generator that simulates user-GUI interactions,
with sentence templates defining the skeleton of
each entry in order to maintain high-quality sen-
tence structure and grammar. This approach is simi-
lar to (Weston et al., 2015), with the difference that
we need a much higher degree of variation such
that the machine learning model does not mem-
orize all possible sentences, and consequently a
much richer template database. We therefore imple-
mented a template language with recursive gram-
mar, that can be used to define as many templates
and generate as many data examples as desired.
We used the same vocabulary as for the real inter-
actions dataset. To generate contextual dependen-
cies (e.g. event coreference), the implementation
allows for more complex combo templates where
a sequence of templates are instantiated together.
A more detailed description of the template lan-
guage and the simulator implementation is given in
(Chen et al., 2019) and Appendix A, together with
illustrative examples. The simulator was used to
generate 1,000 interactions and their logical forms:
312 mouse clicks and 688 NL queries.

4 Baseline Models for Semantic Parsing

This section describes two baseline models: a stan-
dard LSTM encoder-decoder for sequence gen-
eration SeqGen (Section 4.1) and its attention-
augmented version SeqGen+Att2In (Section 4.2).
This last model will be used later in Section 5 as
a component in the context-dependent semantic
parsing architecture.

Figure 2: The SeqGen model takes a sequence of inter-
actions as input X = x1, . . . , xn and encodes it with a
Bi-LSTM (left). The decoder LSTM (right) generates a
logical form Ŷ = ŷ1, . . . , ŷT .

4.1 SeqGen

As shown in Figure 2, the sequence-generation
model SeqGen uses Long Short-Term Memory
(LSTM) (Hochreiter and Schmidhuber, 1997) units
in an encoder-decoder architecture (Bahdanau
et al., 2017; Cho et al., 2014), composed of a bi-
directional LSTM for the encoder over the input
sequence X and an LSTM for the decoder of the
output LF sequence Y . We use Yt = y1, . . . , yt to
denote the sequence of output tokens up to position
t. We use Ŷ to denote the generated logical form.

The initial state s0 is created by running the
bi-LSTM encoder over the input sequence X and
concatenating the last hidden states. Starting from
the initial hidden state s0, the decoder produces a
sequence of states s1, . . . , sT , using embeddings
e(yt) to represent the previous tokens in the se-
quence. A softmax is used to compute token proba-
bilities at each position as follows:

p(yt|Yt−1, X) = softmax(W hst) (1)
st = h(st−1, e(yt−1))

The transition function h is implemented by the
LSTM unit.

4.2 SeqGen+Att2In

This model (Figure 3) is similar to SeqGen, except
that it attends to the current input (NL query or
mouse click) during decoding. Equation 2 defines
the corresponding attention mechanism Att2In used
to create the context vector dt:

etj = v
T
a tanh(W af j +Uast−1) (2)

αtj =
exp(etj)∑m
k=1 exp(etk)

, dt=ct=
n∑

j=1

αtjf j

Here f j is the j-th hidden states for Bi-LSTM
corresponding to xj and αtj is an attention weight.



3580

Both the context vector dt and st are used to predict
the next token ŷt in the logical form:

ŷt ∼ softmax(W hst +W ddt)

Figure 3: The SeqGen+Att2In model augments the Se-
qGen model with an attention mechanism. At each de-
coding step t, it attends to all input tokens in order to
compute a context vector dt.

5 Context-Dependent Semantic Parsing

In Figure 4 we show our proposed semantic pars-
ing model, SP+Att2All+Copy (SPAAC). Similar to
the baseline models, we use a bi-directional LSTM
to encode the input and another LSTM as the de-
coder. Context-dependency is modeled using two
types of mechanisms: attention and copying. The
attention mechanism (Section 5.1) is comprised of
3 models: Att2HisIn attending to the previous in-
put, Att2HisLF attending to the previous logical
form, and the Att2In introduced in Section 4.2 that
attends to the current input. The copying mecha-
nism (Section 5.2) is comprised of two models: one
for handling unseen tokens, and one for handling
coreference to events in the current and previous
logical forms.

5.1 Attention Mechanisms
At decoding step t, the Att2HisIn attention model
computes the context vector ĉt as follows:

êtk = v
T
b tanh(W brk +U bst−1) (3)

βtk =
exp(êtk)∑m2
l=1 exp(êtl)

, ĉt =
n∑

k=1

βtk · rk

where rk is the encoder hidden state corresponding
to xk in the previous input X−1, ĉt is the context
vector, and βtk is an attention weight.

Similarly, the Att2HisLF model computes the
context vector c̃t as follows:

ẽtj = vc
T tanh(W clj +U cst−1) (4)

γtj =
exp(ẽtj)∑n
j=1 exp(ẽtj)

, c̃t =

n∑
j=1

γtj · lj

where lj is the j-th hidden state of the decoder for
the previous logical form Y −1.

The context vector used in the decoder is com-
prised of the context vectors from the three atten-
tion models Att2In, Att2HisIn and Att2HisLF:

dt = concat(ct, ĉt, c̃t) (5)

5.2 Copying Mechanisms

In order to handle out-of-vocabulary (OOV) tokens
and coreference (REF) between entities in the cur-
rent and the previous logical forms, we add two
special tokens OOV and REF to the vocabulary.
Inspired by the copying mechanism in (Gu et al.,
2016), we train the model to learn which token in
the current input X = {xj} is an OOV by mini-
mizing the following loss:

Loov(Y ) = −
Y.l∑
t=1

X.l∑
j=1

log po(Oj |sXj , sYt ) (6)

where X.l is the length of current input, Y.l is
the length of the current logical form, sXj is the
LSTM state for xj and sYt is the LSTM state for
yt, Oj ∈ {0, 1} is a label indicating whether xj
is an OOV. We use logistic regression to compute
the OOV probability, i.e. po(Oj = 1|sXj , sYt ) =
σ(wTo [s

X
j , s

Y
t ]).

Similarly, to solve coreference, the model is
trained to learn which entity in the previously gen-
erated logical form Ŷ −1 = {ŷj} is coreferent with
the entity in the current logical form by minimizing
the following loss:

Lref (Y )=−
Y.l∑
t=1

Ŷ −1.l∑
j=1

log pr(Rj |sŶ
−1

j , s
Y
t ) (7)

where Ŷ −1.l is the length of the previous gen-
erated logical form, Y.l is the length of the cur-
rent logical form, sŶ

−1
j is the LSTM state at po-

sition j in Ŷ −1 and sYt is the LSTM state for
position t in Y , and Rj ∈ {0, 1} is a label indi-
cating whether ŷj is an entity referred by yt in
the next logical form Y . We use logistic regres-
sion to compute the coreference probability, i.e.
pr(Rj = 1|sŶ

−1
j , s

Y
t ) = σ(w

T
r [s

Ŷ −1
j , s

Y
t ]).

Finally, we use “Teacher forcing” (Williams and
Zipser, 1989) to train the model to learn which
token in the vocabulary (including special tokens
OOV and REF) should be generated, by minimizing



3581

Figure 4: Context-dependent semantic parsing architecture. We use a Bi-LSTM (left) to encode the input and a
LSTM (right) as the decoder. We show only parts of the LF to save space. The complete generated LF at time T-1
is Y −1 = [Answer, (, e, ), ∧, Around, (, e, ., time, OOV, ), ∧, e, ., type, ==, DiscreteType]. The token 10am is copied
from the input to replace the generated OOV token (solid green arrow). The complete generated LF at time T is Y
= [Answer, (, REF, ., time, )]. The entity token e is copied from the previous LF to replace the generated REF token
(solid green arrow). Orange dash arrows attend to historical input. Blue dash arrows attend to current input. Purple
dash arrows attend to previous logical form.

the following token generation loss:

Lgen(Y ) = −
Y.l∑
t=1

log p(yt|Yt−1, X) (8)

where Y.l is the length of the current logical form.

5.3 Supervised Learning: SPAAC-MLE
The supervised learning model SPAAC-MLE is ob-
tained by training the semantic parsing architecture
from Figure 4 to minimize the sum of the 3 negative
log-likelihood losses:

LMLE(Y )=Lgen(Y )+Loov(Y )+Lref (Y ) (9)

At inference time, beam search is used to generate
the LF sequence (Ranzato et al., 2015; Wiseman
and Rush, 2016). During inference, if the generated
token at position t is OOV, we copy the token from
the current input X that has the maximum OOV
probability, i.e. argmaxj po(Oj = 1|sXj , sYt ).
Similarly, if the generated entity token at position
t is REF, we copy the entity token from the pre-
vious LF Y −1 that has the maximum coreference
probability, i.e. argmaxj pr(Rj = 1|sY

−1
j , s

Y
t ).

5.4 Reinforcement Learning: SPAAC-RL
All models described in this paper are evaluated
using sequence-level accuracy, a discrete metric
where a generated logical form is considered to
be correct if it is equivalent with the ground truth

logical form. This is a strict evaluation measure in
the sense that it is sufficient for a token to be wrong
to invalidate the entire sequence. At the same time,
there can be many generated sequences that are
correct, e.g. any reordering of the clauses from
the ground truth sequence is correct. The large
number of potentially correct generations can lead
MLE-trained models to have sub-optimal perfor-
mance (Paulus et al., 2017; Rennie et al., 2017;
Zeng et al., 2016; Norouzi et al., 2016). Further-
more, although “teacher forcing” (Williams and
Zipser, 1989) is widely used for training sequence
generation models, it leads to exposure bias (Ran-
zato et al., 2015): the network has knowledge of
the ground truth LF tokens up to the current token
during training, but not during testing, which can
lead to propagation of errors at generation time.

Like Paulus et al. (2017), we address these prob-
lems by using policy gradient to train a token
generation policy that aims to directly maximize
sequence-level accuracy. We use the self-critical
policy gradient training algorithm proposed by Ren-
nie et al. (2017). We model the sequence gener-
ation process as a sequence of actions taken ac-
cording to a policy, which takes an action (to-
ken ŷt) at each step t as a function of the current
state (history Ŷt−1), according to the probability
p(ŷt|Ŷt−1). The algorithm uses this probability to
define two policies: a greedy, baseline policy πb



3582

that takes the action with the largest probability, i.e.
πb(Ŷt−1) = argmaxŷt p(ŷt|Ŷt−1); and a sampling
policy πs that samples the action according to the
same distribution, i.e. πs(Ŷt−1) ∝ p(ŷt|Ŷt−1).

The baseline policy is used to generate a se-
quence Ŷ b, whereas the sampling policy is used
to generate another sequence Ŷ s. The reward
R(Ŷ s) is then defined as the difference between
the sequence-level accuracy (A) of the sampled
sequence Ŷ s and the baseline sequence Ŷ b. The
corresponding self-critical policy gradient loss is:

LRL = −R(Ŷ s)× LMLE(Ŷ s)

= −
(
A(Ŷ s)−A(Ŷ b)

)
× LMLE(Ŷ s) (10)

Thus, minimizing the RL loss is equivalent to max-
imizing the likelihood of the sampled Ŷ s if it ob-
tains a higher sequence-level accuracy than the
baseline Ŷ b.

6 Experimental Evaluation

All models are implemented in Tensorflow using
dropout to deal with overfitting. For both datasets,
10% of the data is put aside for validation. After
tuning on the artificial validation data, the feed-
forward neural networks dropout rate was set to
0.5 and the LSTM units dropout rate was set to
0.3. The word embeddings had dimensionality of
64 and were initialized at random. Optimization
is performed with the Adam algorithm. For each
dataset, we use five-fold cross evaluation, where
the data is partitioned into five folds, one fold is
used for testing and the other folds for training. The
process is repeated five times to obtain test results
on all folds. We use an early-stop strategy on the
validation set. The number of gradient updates is
typically more than 20,000. All the experiments are
performed on a single NVIDIA GTX1080 GPU.

The models are trained and evaluated on the ar-
tificial interactions first. To evaluate on real inter-
actions, the models are pre-trained on the entire
artificial dataset and then fine-tuned using real in-
teractions. SPAAC-RL is pre-trained with MLE loss
to provide more efficient policy exploration. We
use sequence level accuracy as evaluation metric
for all models: a generated sequence is considered
correct if and only if all the generated tokens match
the ground truth tokens.

We report experimental evaluations of the pro-
posed models SPAAC-MLE and SPAAC-RL and
baseline models SeqGen, SeqGen+Att2In on the

Models Artificial Real

SeqGen 51.8 22.2
SeqGen+Att2In 72.7 35.4
SPAAC-MLE 84.3 66.9
SPAAC-RL 88.7 74.8

Table 3: Sequence-level accuracy on the 2 datasets.

Well the Finger Stick is 56.
T&MLE&RL:
e.type == Fingerstick ∧ e.value == 56
It looks like she suspended her pump.
T&MLE&RL:
Suspended(e) ∧ around(e.time, e(−1).time)
Let’s look at the next day.
T&MLE&RL: DoSetDate(currentdate+ 1)

See if he went low.
T&MLE&RL: Answer(any(e, hypo(e)))

Let’s see what kind of exercise that is,
where the steps are high?
T&RL:Answer(e.kind) ∧ e.type == exercise
∧around(e.time, e1.time) ∧ e1.type == stepcount
∧high(e1.value)
MLE: Answer(e.kind) ∧ e.type == exercise
∧around(e.time, e1.time) ∧ e1.type == exercise
∧e1.type == exercise
Click on the exercise.
T&RL:DoClick(e) ∧ e.type == exercise
MLE: Answer(e) ∧ e.type == exercise

Table 4: Examples generated by SPAAC-MLE and
SPAAC-RL using real interactions. T: true logical forms.
MLE: logical forms by SPAAC-MLE. RL: logical forms
by SPAAC-RL.

Real and Artificial Interactions Datasets in Table 3.
We also report examples generated by the SPAAC
models in Tables 4 and 5.

6.1 Discussion
The results in Table 3 demonstrate the importance
of modeling context-dependency, as the two SPAAC
models outperform the baselines on both datasets.
The RL model also obtains substantially better ac-
curacy than the MLE model. The improvement in
performance over the MLE model for the real data
is statistically significant at p = 0.05 in a one-tailed
paired t-test.

Analysis of the generated logical forms revealed
that one common error made by SPAAC-MLE is the
generation of incorrect event types. Some of these
errors are fixed by the current RL model. How-
ever, there are instances where even the RL-trained
model outputs the wrong event type. By comparing



3583

Does he always get some sleep around 4:30pm?
T&MLE&RL: Answer(cond(around(x, 4 : 30pm)
=> any(e.type == reportedsleep ∧ e.time == x)))

Is it the first week of the patient?
T&MLE&RL: Answer(week(currentdate) == x)
∧order(x, 1, sequence(e, e.type == week))

Does she ever get some rest around 5:37pm?
T&MLE&RL: Answer(any(e.type == reportedsleep
∧around(e.time, 5 : 37pm)))

When is the first time he changes his infusion set?
T&MLE&RL: Answer(e.date)
∧order(e, 1, sequence(e, e.type == infusionset))

How many months she has multiple exercises?
T&RL: Answer(count(x, count(e, e.type == exercise
∧e.date == x) > 1 ∧ x.type == month))
MLE: Answer(count(x, count(e, e.type == exercise
∧e.date == x) > 1 ∧ x.type == week))
Toggle so we can see fingersticks.
T&RL: DoToggle(on,fingersticks)
MLE: DoToggle(on, bgl)

Table 5: Examples generated by SPAAC-MLE and
SPAAC-RL using artificial interactions. T: true logical
forms. MLE: logical forms generated by SPAAC-MLE.
RL: logical forms generated by SPAAC-RL.

the sampled logical forms Ŷ s and the generated
baseline logical forms Ŷ b, we found that some-
times the sampled tokens for event types are the
same as those in the baseline. An approach that we
plan to investigate in future work is to utilize more
advanced sampling methods to generate Ŷ s, in or-
der to achieve a better balance between exploration
and exploitation.

7 Related Work

Question Answering has been the topic of recent
research (Yih et al., 2014; Dong et al., 2015; An-
dreas et al., 2016; Hao et al., 2017; Abujabal et al.,
2017; Chen and Bunescu, 2017). Semantic parsing,
which maps text in natural language to meaning
representations in formal logic, has emerged as an
important component for building QA systems, as
in (Liang, 2016; Jia and Liang, 2016a; Zhong et al.,
2017). Context-dependent processing has been ex-
plored in complex, interactive QA (Harabagiu et al.,
2005; Kelly and Lin, 2007) and semantic parsing
(Zettlemoyer and Collins, 2009; Artzi and Zettle-
moyer, 2011; Iyyer et al., 2017; Suhr et al., 2018;
Long et al., 2016). Although these approaches take
into account sequential dependencies between ques-
tions or sentences, the setting in our work has a
number of significant distinguishing features, such
as the importance of time – data is represented nat-

urally as multiple time series of events – and the
anchoring on a graphical user interface that also
enables direct interactions through mouse clicks
and a combination of factual queries and interface
commands.

Dong and Lapata (2016) use an attention-
enhanced encoder-decoder architecture to learn
the logical forms from natural language without
using hand-engineered features. Their proposed
Seq2Tree architecture can capture the hierarchical
structure of logical forms. Jia and Liang (2016b)
train a sequence-to-sequence RNN model with
a novel attention-based copying mechanism to
learn the logical forms from questions. The copy-
ing mechanism has been investigated by Gu et al.
(2016) and Gulcehre et al. (2016) in the context of
a wide range of NLP applications. These semantic
parsing models considered sentences in isolation.
In contrast, generating correct logical forms in our
task required modeling sequential dependencies be-
tween logical forms. In particular, coreference is
modeled between events mentioned in different log-
ical forms by repurposing the copying mechanism
originally used for modeling out-of-vocabulary to-
kens.

8 Conclusion

We introduced a new semantic parsing setting in
which users can query a system using both natu-
ral language and direct interactions (mouse clicks)
within a graphical user interface. Correspondingly,
we created a dataset of real interactions and a much
larger dataset of artificial interactions. The correct
interpretation of a natural language query often re-
quires knowledge of previous interactions with the
system. We proposed a new sequence generation
architecture that modeled this context dependency
through multiple attention models and a copy-
ing mechanism for solving coreference. The pro-
posed architecture is shown to outperform standard
LSTM encoder-decoder architectures that are con-
text agnostic. Furthermore, casting the sequence
generation process in the framework of reinforce-
ment learning alleviates the exposure bias and leads
to substantial improvements in sequence-level ac-
curacy.

The two datasets and the implementation of
the systems presented in this paper are made
publicly available at https://github.com/
charleschen1015/SemanticParsing.
The data visualization GUI is available under



3584

the name OHIOT1DMVIEWER at http://
smarthealth.cs.ohio.edu/nih.html.

Acknowledgments

This work was partly supported by grant
1R21EB022356 from the National Institutes of
Health. We would like to thank Frank Schwartz
and Cindy Marling for contributing real interac-
tions, Quintin Fettes and Yi Yu for their help with
recording and pre-processing the interactions, and
Sadegh Mirshekarian for the design of the artificial
data generation. We would also like to thank the
anonymous reviewers for their useful comments.

References
Abdalghani Abujabal, Rishiraj Saha Roy, Mohamed

Yahya, and Gerhard Weikum. 2017. Quint: Inter-
pretable question answering over knowledge bases.
In Proceedings of the 2017 Conference on Empiri-
cal Methods in Natural Language Processing: Sys-
tem Demonstrations, pages 61–66.

Jacob Andreas, Marcus Rohrbach, Trevor Darrell, and
Dan Klein. 2016. Learning to compose neural net-
works for question answering. In Proceedings of
NAACL-HLT, pages 1545–1554.

Yoav Artzi and Luke Zettlemoyer. 2011. Bootstrap-
ping semantic parsers from conversations. In Pro-
ceedings of the conference on empirical methods in
natural language processing, pages 421–432. Asso-
ciation for Computational Linguistics.

Dzmitry Bahdanau, Philemon Brakel, Kelvin Xu,
Anirudh Goyal, Ryan Lowe, Joelle Pineau, Aaron
Courville, and Yoshua Bengio. 2017. An actor-critic
algorithm for sequence prediction. In ICLR.

Charles Chen and Razvan Bunescu. 2017. An explo-
ration of data augmentation and rnn architectures
for question ranking in community question answer-
ing. In Proceedings of the Eighth International Joint
Conference on Natural Language Processing (Vol-
ume 2: Short Papers), volume 2, pages 442–447.

Charles Chen, Sadegh Mirshekarian, Razvan Bunescu,
and Cindy Marling. 2019. From physician queries to
logical forms for efficient exploration of patient data.
In 2019 IEEE 13th International Conference on Se-
mantic Computing (ICSC), pages 371–374. IEEE.

Kyunghyun Cho, Bart van Merrienboer, Caglar Gul-
cehre, Dzmitry Bahdanau, Fethi Bougares, Holger
Schwenk, and Yoshua Bengio. 2014. Learning
phrase representations using rnn encoder–decoder
for statistical machine translation. In Proceedings of
the 2014 Conference on Empirical Methods in Nat-
ural Language Processing (EMNLP), pages 1724–
1734.

Li Dong and Mirella Lapata. 2016. Language to logi-
cal form with neural attention. In Proceedings of the
54th Annual Meeting of the Association for Compu-
tational Linguistics (Volume 1: Long Papers), vol-
ume 1, pages 33–43.

Li Dong, Furu Wei, Ming Zhou, and Ke Xu. 2015.
Question answering over freebase with multi-
column convolutional neural networks. In Proceed-
ings of the 53rd Annual Meeting of the Association
for Computational Linguistics and the 7th Interna-
tional Joint Conference on Natural Language Pro-
cessing (Volume 1: Long Papers), volume 1, pages
260–269.

Jiatao Gu, Zhengdong Lu, Hang Li, and Victor OK
Li. 2016. Incorporating copying mechanism in
sequence-to-sequence learning. In Proceedings of
the 54th Annual Meeting of the Association for Com-
putational Linguistics (Volume 1: Long Papers), vol-
ume 1, pages 1631–1640.

Caglar Gulcehre, Sungjin Ahn, Ramesh Nallapati,
Bowen Zhou, and Yoshua Bengio. 2016. Point-
ing the unknown words. In Proceedings of the
54th Annual Meeting of the Association for Compu-
tational Linguistics (Volume 1: Long Papers), vol-
ume 1, pages 140–149.

Yanchao Hao, Yuanzhe Zhang, Kang Liu, Shizhu He,
Zhanyi Liu, Hua Wu, and Jun Zhao. 2017. An end-
to-end model for question answering over knowl-
edge base with cross-attention combining global
knowledge. In Proceedings of the 55th Annual Meet-
ing of the Association for Computational Linguistics
(Volume 1: Long Papers), volume 1, pages 221–231.

Sanda Harabagiu, Andrew Hickl, John Lehmann, and
Dan Moldovan. 2005. Experiments with interactive
question-answering. In Proceedings of the 43rd an-
nual meeting on Association for Computational Lin-
guistics, pages 205–214. Association for Computa-
tional Linguistics.

Sepp Hochreiter and Jürgen Schmidhuber. 1997.
Long short-term memory. Neural computation,
9(8):1735–1780.

Mohit Iyyer, Wen-tau Yih, and Ming-Wei Chang. 2017.
Search-based neural structured learning for sequen-
tial question answering. In Proceedings of the
55th Annual Meeting of the Association for Compu-
tational Linguistics (Volume 1: Long Papers), vol-
ume 1, pages 1821–1831.

Robin Jia and Percy Liang. 2016a. Data recombination
for neural semantic parsing. In Proceedings of the
54th Annual Meeting of the Association for Compu-
tational Linguistics (Volume 1: Long Papers), pages
12–22, Berlin, Germany. Association for Computa-
tional Linguistics.

Robin Jia and Percy Liang. 2016b. Data recombina-
tion for neural semantic parsing. In Proceedings of



3585

the 54th Annual Meeting of the Association for Com-
putational Linguistics (Volume 1: Long Papers), vol-
ume 1, pages 12–22.

Diane Kelly and Jimmy Lin. 2007. Overview of the
TREC 2006 ciQA task. In ACM SIGIR Forum, vol-
ume 41, pages 107–116. ACM.

Percy Liang. 2016. Learning executable semantic
parsers for natural language understanding. Commu-
nications of the ACM, 59(9):68–76.

Reginald Long, Panupong Pasupat, and Percy Liang.
2016. Simpler context-dependent logical forms via
model projections. In Proceedings of the 54th An-
nual Meeting of the Association for Computational
Linguistics (Volume 1: Long Papers), volume 1,
pages 1456–1465.

Mohammad Norouzi, Samy Bengio, Navdeep Jaitly,
Mike Schuster, Yonghui Wu, Dale Schuurmans, et al.
2016. Reward augmented maximum likelihood for
neural structured prediction. In Advances In Neural
Information Processing Systems, pages 1723–1731.

Romain Paulus, Caiming Xiong, and Richard Socher.
2017. A deep reinforced model for abstractive sum-
marization. arXiv preprint arXiv:1705.04304.

Marc’Aurelio Ranzato, Sumit Chopra, Michael Auli,
and Wojciech Zaremba. 2015. Sequence level train-
ing with recurrent neural networks. arXiv preprint
arXiv:1511.06732.

Steven J Rennie, Etienne Marcheret, Youssef Mroueh,
Jerret Ross, and Vaibhava Goel. 2017. Self-critical
sequence training for image captioning. In 2017
IEEE Conference on Computer Vision and Pattern
Recognition (CVPR), pages 1179–1195. IEEE.

Alane Suhr, Srinivasan Iyer, and Yoav Artzi. 2018.
Learning to map context-dependent sentences to ex-
ecutable formal queries. In Proceedings of the 2018
Conference of the North American Chapter of the
Association for Computational Linguistics: Human
Language Technologies, Volume 1 (Long Papers),
volume 1, pages 2238–2249.

Jason Weston, Antoine Bordes, Sumit Chopra, and
Tomas Mikolov. 2015. Towards AI-complete ques-
tion answering: A set of prerequisite toy tasks.
CoRR, abs/1502.05698.

Ronald J Williams and David Zipser. 1989. A learn-
ing algorithm for continually running fully recurrent
neural networks. Neural computation, 1(2):270–
280.

Sam Wiseman and Alexander M Rush. 2016.
Sequence-to-sequence learning as beam-search
optimization. In Proceedings of the 2016 Confer-
ence on Empirical Methods in Natural Language
Processing, pages 1296–1306.

Wen-tau Yih, Xiaodong He, and Christopher Meek.
2014. Semantic parsing for single-relation question
answering. In Proceedings of the 52nd Annual Meet-
ing of the Association for Computational Linguistics
(Volume 2: Short Papers), volume 2, pages 643–648.

Wenyuan Zeng, Wenjie Luo, Sanja Fidler, and Raquel
Urtasun. 2016. Efficient summarization with
read-again and copy mechanism. arXiv preprint
arXiv:1611.03382.

Luke S Zettlemoyer and Michael Collins. 2009. Learn-
ing context-dependent mappings from sentences to
logical form. In Proceedings of the Joint Confer-
ence of the 47th Annual Meeting of the ACL and the
4th International Joint Conference on Natural Lan-
guage Processing of the AFNLP: Volume 2-Volume
2, pages 976–984. Association for Computational
Linguistics.

Victor Zhong, Caiming Xiong, and Richard Socher.
2017. Seq2SQL: Generating structured queries
from natural language using reinforcement learning.
arXiv preprint arXiv:1709.00103.


