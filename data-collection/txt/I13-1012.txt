










































Multilingual Mention Detection for Coreference Resolution


International Joint Conference on Natural Language Processing, pages 100–108,
Nagoya, Japan, 14-18 October 2013.

Multilingual Mention Detection for Coreference Resolution

Olga Uryupina
DISI, University of Trento, Italy
uryupina@gmail.com

Alessandro Moschitti
QCRI, Qatar Foundation, Doha, Qatar

amoschitti@qf.org.qa

Abstract

This paper proposes a novel algorithm for
multilingual mention detection: we ex-
tract mentions from parse trees via kernel-
based SVM learning. Our approach al-
lows for straightforward mention detection
for any language where (not necessary per-
fect) parsing resources are available, with-
out any complex language-specific rule
engineering. We also investigate possi-
bilities for incorporating automatically ac-
quired mentions into an end-to-end coref-
erence resolution system. We evaluate our
approach on the Arabic and Chinese por-
tions of the CoNLL-2012 dataset, showing
a significant improvement over the system
with the baseline mention detection.

1 Introduction

Accurate mention detection (MD) is a vital prereq-
uisite for a variety of Natural Language Processing
tasks, in particular, for Relation Extraction (RE)
and Coreference Resolution (CR). If a toolkit can-
not extract mentions reliably, it will obviously be
unable to assign them to relations or entities.

Many studies on RE and CR report evaluation
figures on gold mentions: in such a setting, a sys-
tem is supplied with correct mention boundaries
and/or semantic classes or other relevant prop-
erties. It can, in theory, be argued that such a
methodology provides better insights on perfor-
mance of RE and CR algorithms per se. It has been
demonstrated, however, that evaluation results on
gold mentions are misleading: for example, Ng
(2008) shows that unsupervised CR algorithms ex-
hibit promising results on gold mentions, that are
not mirrored in a more realistic evaluation on au-
tomatically detected mentions.

The exact scope of the mention detection task
varies considerably depending on the annotation

guidelines. Thus, some corpora consider all the
(non-embedding) NPs to be mentions, some cor-
pora do not allow for non-referential mentions and
some do not mark singleton referential mentions,
that do not participate in coreference relations. In
addition, some guidelines may restrict the annota-
tion to specific semantic types.

A number of linguistic studies focus on various
syntactic, semantic and discourse clues that might
help identify nominal constructions that cannot
participate in coreference relations. Possible fea-
tures include, among others, specific syntactic
constructions for expletive pronouns, negation,
modality and quantification (Karttunen, 1976).
Several algorithms have been proposed recently,
trying to tackle some of the addressed phenom-
ena within a computational approach. Thus, a
number of algorithms have been developed re-
cently to identify expletive usages of “it” (Evans,
2001; Boyd et al., 2005; Bergsma and Yarowsky,
2011). While these approaches are potentially
beneficial for mention detection in English, for
other languages, neither theoretical nor compu-
tational studies are available at the moment. In
this paper, we use tree kernels to extract relevant
syntactic patterns automatically, without assuming
any prior knowledge of the input language.

In this paper, we propose a learning-based so-
lution to the mention detection task. We use
SVMs (Joachims, 1999) with syntactic tree ker-
nels (Collins and Duffy, 2001; Moschitti, 2008;
Moschitti, 2006) to classify parse tree nodes as
±mentions. Our approach does not require any
language- or corpus-specific engineering and thus
can be easily adapted to cover new languages or
mention annotation schemes.

The rest of paper is organized as follows. In the
next section, we define the task and discuss our
tree and vector representations. Section 4 presents
MD evaluation figures. Finally, in Section 5 we
incorporate our MD module into an end-to-end

100



coreference resolution system.

2 Related Work

Until recently, most RE and CR toolkits have
been evaluated on the ACE datasets (Doddington
et al., 2004). The ACE guidelines restrict possi-
ble mentions to be considered to specific semantic
types (PERSON, LOCATION and so on). More-
over, mentions are annotated with their minimal
and maximal span, allowing for relaxed matching
between gold and automatically extracted bound-
aries. In such a setting, the mention detection task
can be cast as a tagging problem, similar to the
named entity recognition and classification task.
A number of systems have followed this scenario,
demonstrating reliable performance (Florian et al.,
2004; Ittycheriah et al., 2003; Zitouni and Florian,
2008).

In the past years, however, several corpora have
been created from a more linguistic perspective:
for example, the OntoNotes dataset (Hovy et al.,
2006; Pradhan et al., 2012) provides annotation
for unrestricted coreference. The guidelines differ
significantly from the ACE scheme: mentions cor-
respond to parse nodes and can be of any semantic
type, the systems are expected to recover mention
boundaries exactly. The OntoNotes mentions—
unlike ACE ones–correspond to large NP struc-
tures (embedding NP nodes in gold parse trees),
so a traditional approach (e.g., one of those men-
tioned above), which aims at identifying basic NP
chunks, would not be applicable here. There-
fore, any MD method for OntoNotes would rely
on parsing.

The OntoNotes corpus has been used for eval-
uating end-to-end CR systems at two CoNLL
shared tasks (2011 and 2012). At the 2011 shared
task, the participants relied on rule-based mod-
ules for extracting mention boundaries from parse
trees. This was relatively straightforward, as the
task was devoted to CR in English and most par-
ticipants could use their in-house MD modules
developed and refined in the past decade. At
the 2012 shared task, however, the systems were
expected to provide end-to-end coreference res-
olution for Arabic and Chinese. As it turned
out, most groups could not adapt their MD rules
to cover these two languages and fell back to
very simple baselines (e.g., “use all NP nodes as
mentions”). Kummerfeld et al. (2011) investi-
gated various post- and pre-filtering heuristics for

adapting their mention detection algorithm to the
OntoNotes English data in a semi-automatic way,
reporting mixed results.

3 Mention extraction from parse trees

We recast MD as a node filtering task: each candi-
date node is classified as either mention or not. In
this study, we consider all “NP” nodes to be candi-
dates for MD. As Table 1 shows, this is a reason-
able assumption for the OntoNotes dataset, as al-
most 90% of all the mentions for both Arabic and
Chinese correspond to NP nodes. The remaining
11-14% of mentions can mostly be attributed to
parsing errors: as we aim at end-to-end process-
ing with no gold information available, we run our
system on automatically extracted parse trees, it is
therefore possible that a mention corresponds to a
gold NP node that has not been labeled correctly
in an automatic parse tree.

train development
NP-nodes % NP-nodes %

ARB 24068 87.23 2916 87.91
CHN 88523 85.96 12572 88.52

Table 1: NP-nodes in OntoNotes for Arabic
(ARB) and Chinese (CHN): total numbers and
percentage of mentions that are NP-nodes.

Not all the NP nodes, however, correspond to a
mention. Such non-mention NPs fall into several
categories:

• Embedded NPs. When an NP is embedded
into another one, only the outer NP is used to
represent a mention:

(1) [MENTION−NP [NP This type] of
earthquake] has no precursors. 1

A number of heuristics have been proposed
for English to identify and discard embedded
NPs, based on available head-finding algo-
rithms, e.g., (Collins, 1999). For other lan-
guages, however, the task of finding a head
of a given NP in a constituency tree is not
trivial.

• Non-referential NPs. Depending on the an-
notation guidelines, non-referential NPs can

1We use English OntoNotes examples throughout this pa-
per to illustrate discussed phenomena, as our approach is
language-independent. The evaluation, however, is done on
Arabic and Chinese.

101



either be marked as mentions or not. In
OntoNotes, non-referential NPs should not
be annotated:

(2) This type of earthquake has [NP no
precursors].

• Singleton NPs. In some CR corpora (for ex-
ample, ACE), mentions are annotated even
if they do not participate in any corefer-
ence relations. In other corpora (MUC and
OntoNotes), such singletons are not marked.
When singletons are not marked, the MD
tasks becomes considerably more difficult:
the performance of an MD component cannot
be measured and optimized directly, but only
in conjunction with a coreference resolver.

• Erroneous NPs. When we evaluate an end-
to-end system, we expect it to process raw in-
put and thus rely on automatically extracted
parse trees. Some NP-nodes might be incor-
rect, not corresponding to any NP in the gold
tree. Such nodes cannot be mentions:

(3) At the meeting, Huang Xiangning
read [NP the earthquake prediction]
that they had previously issued.

“The earthquake prediction” is considered to
be an NP node by the parser. In the gold
data, however, this node does not exist at all.
And even if it existed, the mention should
correspond to its embedding NP node, “the
earthquake prediction that the had previously
issued” (cf. example 1 above). While this
problem is less crucial for English, parsing
resources for other languages are still scarce
and less reliable.

3.1 Tree Representation
We use kernel-based SVMs to classify nodes as
±mentions. This requires representing a relevant
fragment of a tree with a specific node marked as
“C-NP” (candidate). We start from a straightfor-
ward representation: using automatically gener-
ated parse trees provided within the CoNLL data
distribution, we generate one example for each NP
node: the example corresponds to the entire parse
tree with just a single node re-labeled as “C-NP”.
The assigned class label reflects the fact that this
particular node corresponds to some gold mention
or not. For example, the full parse tree for our sen-
tence (1-2) will generate one positive (for “This

type of earthquake”, shown on Figure 1) and three
negative examples (for “This type”, “earthquake”
and “no precursors”).

While this representation might work for our
toy example, for a longer sentence it would pro-
vide irrelevant information. Consider again the
tree on Figure 1. To generate a training example
we append “C-” to one NP node, keeping all the
remaining nodes as-is. The tree kernel operates
on subtrees of the given structure, so, effectively,
it will consider a lot of tree fragments that do not
contain the marked node. These fragments will
affect the treatment of different examples, possi-
bly with conflicting class labels. It will not only
make learning slow but also introduce spurious ev-
idence, decreasing the system’s performance. We
have therefore investigated two possibilities for
pruning our trees.

Our first pruning algorithm (“up-down”) starts
from the node of interest (C-NP) and goes up for
u nodes. From each node on the path, it considers
all its children up to the depth d. The first part of
Figure 2 shows a pruned tree for u = 2, d = 1 for
the node “This type of earthquake.”

Our second pruning algorithm (“radial”) starts
from the node of interest and considers all the
nodes in the tree that are reachable from it via at
most n edges. The second part of Figure 2 shows
a pruned tree for n = 2 for the same node.

3.2 Vector Representation

In addition to (pruned) trees, we also provide vec-
tor representations of our NPs. For each NP, we
extract its basic properties: number, gender, per-
son, mention type (name, nominal or pronoun)
and the number of other NPs in the document that
have the same surface form. To extract mention
properties, we have to compute the head. How-
ever, the goal of our study is to provide an MD
algorithm that is adaptable to different languages
without extensive engineering. We have therefore
deliberately relied on an over-simplistic heuristic
for finding an NP head: either the last or the first
noun in an NP is considered a head, depending on
some very basic information on a word order in a
specific language. Given the head, we extract its
properties from the CoNLL data in a straightfor-
ward way (for example, we have compiled a list
of pronouns with their gender, number and person
values from the training data and so on). This is
done fully automatically and doesn’t require any

102



S

PP

C-NP

NP

DT
This

NN
type

PP

IN
of

NP

NN
earthquake

VP

VDZ
has

NP

DT
no

NNS
precursors

Figure 1: Parse tree for “This type of earthquake”, examples (1-2): before pruning.

S

PP

C-NP

NP PP

VP

S

PP

C-NP

NP

DT
This

NN
type

PP

IN
of

NP

NN
earthquake

Figure 2: Up-down (left) vs. radial (right) pruning for “This type of earthquake,” examples (1-2)

language-specific manual engineering.
Table 2 lists the features for our vector repre-

sentation. Nominal values are binarized, leading
to 10 binary or continuous features.

feature possible values
Gender F,M,Unknown
Definiteness Yes, No
Number Sg,Pl,Du,Unknown
MentionType Name,Nominal,Pronoun
#same-surface NPs continuous (normalized)

in the doc

Table 2: Features used for Mention Detection:
each feature describes an individual NP

4 Evaluating MD

In this section we provide evaluation results on
both Arabic and Chinese. We reserve a small por-
tion of the CoNLL training data (around 20k in-
stances for each language) for training an MD sys-
tem. Another small subset (around 5k instances)
is reserved for fitting the system parameters. The
evaluation results are reported on the CoNLL de-
velopment data. Note that we evaluate the NP-

node classifier, so the system receives no penalty
for missing mentions that are not NPs. In Section
5 below, however, we will assess the impact of MD
on the end-to-end CR system and thus penalize for
missing non-NP mentions.

As a baseline (“all-NP”), we consider all the NP
nodes to be mentions. Table 3 below compares this
baseline against mentions extracted automatically
from different representations. We use Syntactic
Tree Kernels (TK) implemented within the SVM-
TK toolkit2 to induce the classification.

As our results suggest, vector representation
does not provide enough information for robust
mention detection.3 Indeed, without tree kernels,
the system is only able to learn a major class label-
ing. This highlights the importance of a model that
is able to handle structured input, learning relevant
patterns directly from parse trees.

As discussed in Section 3 above, full trees con-
tain too much misleading evidence. A single parse
tree might contain several dozens of NP nodes, so,

2http://disi.unitn.it/moschitti/
Tree-Kernel.htm

3As a pilot experiment, we also added bag-of-words fea-
tures to our vector representations, but this didn’t yield any
improvement.

103



representation pruning R P F
Arabic

all-NP N/A 100 18.0 30.5
vectors N/A 0 N/A N/A
trees - 53.9 45.5 49.4
trees d=3, u=1 59.5 50.4 54.6
trees n=2 59.6 64.7 62.0
trees+vectors d=3, u=1 65.1 52.9 58.4
trees+vectors n=2 66.0 66.1 66.1

Chinese
all-NP N/A 100 27.9 43.6
vectors N/A 0 N/A N/A
trees - 65.0 55.8 60.0
trees d=1, u=1 73.0 56.9 63.9
trees n=2 69.7 63.7 66.6
trees+vectors d=1, u=1 75.6 60.7 67.4
trees+vectors n=3 71.3 68.9 70.1

Table 3: Performance of the MD classifier on the
development set

using a full sentence tree to represent a particular
candidate node provides confusing input for the
classifier. This is reflected with a low classifier
performance on full representations.

Both pruning strategies have resulted in a sub-
stantial improvement in the performance level.
The radial pruning has significantly outperformed
the up-down strategy. Moreover, the radial prun-
ing depends on just one parameter and can there-
fore be optimized faster.

Finally, joint vector and tree representation fur-
ther outperforms a plain tree-based model. It must
be noted, however, that our MD features (Table 2)
require at least some minimal amount of language-
specific engineering.

5 Incorporating TK-based Mention
Detection into an end-to-end
coreference resolution system

For our experiments, we use BART – a modu-
lar toolkit for coreference resolution that supports
state-of-the-art statistical approaches to the task
and enables efficient feature engineering (Vers-
ley et al., 2008). BART has originally been cre-
ated and tested for English, but its flexible modu-
lar architecture ensures its portability to other lan-
guages and domains.

In our evaluation experiments, we follow a
very simple model of coreference, namely, the
mention-pair approach advocated by Soon et al.

(2001) and adopted in many studies ever since.
We believe, however, that more complex models
of coreference will also benefit from our MD algo-
rithm: most state-of-the-art CR systems treat men-
tion detection as a preprocessing step that is not af-
fected by further processing and therefore we ex-
pect them to yield better performance when such a
preprocessing is achieved in a more robust way.

Creating a robust coreference resolver for
a new language requires linguistic expertise
and language-specific engineering. This cannot
and, moreover, should not be avoided by fully
language-agnostic methods. Our approach to end-
to-end coreference resolution relies on a universal
MD component that requires no linguistic engi-
neering – it facilitates the development of coref-
erence resolvers in the narrow sense, by provid-
ing them with input mentions. We must stress that
the resolvers themselves are not supposed to be
universal: in fact, a number of linguistic studies
on coreference address various language-specific
challenging problems (e.g., zero pronouns, differ-
ent marking of information status etc).

Below we describe the adjustments we made
to BART to cover Arabic and Chinese and then
report on our experiments for integrating kernel-
based MD into BART to provide an end-to-end
coreference resolution for these languages.

5.1 Adapting BART to Arabic and Chinese

The modularity of the BART toolkit enables its
straightforward adaptation to different languages.
This includes creating meaningful linguistic repre-
sentations of mentions (“mention properties”) and,
optionally, some experiments on feature selection
and engineering.

We extracted some properties (sentence bound-
aries, lemmata, speaker id) for Arabic and Chi-
nese directly from the CoNLL/OntoNotes layers4.
Mention types are inferred from PoS tags.

We compiled lists of pronouns for both Arabic
and Chinese from the training and development
data. For Arabic, we used gold PoS tags to clas-
sify pronouns into subtypes, person, number and
gender. For Chinese, no such information is avail-
able, so we consulted several grammar sketches
and lists of pronouns on the web. Finally, we ex-
tracted a list of gender affixes for Arabic along

4Recall that all the layers, apart from the Arabic lemma,
were computed using state-of-the-art preprocessing tools by
the CoNLL organizers and do not contain gold information

104



with a list of gender-classified lemmata from the
training data.

We assessed the list of features, supported by
BART, discarding those that require unavailable
information (for example, the aliasing feature
relies on semantic types for named entities that are
not available within the CoNLL/OntoNotes distri-
bution for languages other than English). We also
created two additional features: LemmataMatch
(similar to string match, but uses lemmata instead
of tokens) and NumberAgreementDual (simi-
lar to commonly used number agreement features,
but supports dual number). Both features are ex-
pected to provide important information for coref-
erence in Arabic, a morphologically rich language.

We ran a feature selection experiment to fur-
ther remove irrelevant features (BART were only
tested on European languages, thus several fea-
tures reflected patterns more common for Ger-
manic and Romance languages). This resulted in
two feature sets, one for each language, listed in
Table 4. For comparison, we also show the base-
line features (cf. below).

5.2 Incorporating Kernel-based MD into a
Coreference Resolver

Coreference resolution systems have different tol-
erance for precision and recall MD errors. If a spu-
rious mention is introduced, the CR system might
still assign it to no coreference chain and thus dis-
card from the output partition. If a correct men-
tion is missed, however, the system has no chance
of recovering it as it does not even start processing
such a mention. This suggests that an MD module
should be tuned to yield better recall.

To assess the impact of MD precision and recall
errors on the performance of our coreference re-
solver, we run a simulation experiment. We start
from the upper bound baseline: the MD module
considers all the true (gold) NP mentions to be
positive and all the spurious ones – to be neg-
ative. We then randomly distort this baseline,
adding spurious mentions and removing correct
ones, to arrive at a predefined performance level.
The resulting MD output is then sent to our coref-
erence resolution system and its performance is
measured. As a measure of the CR system per-
formance, we use the MELA F-score – an aver-
age of MUC, B3 and CEAFe metrics, the official
performance measure at the CoNLL shared task
(Pradhan et al., 2012).

Figure 3 shows the results of our simulation ex-
periment on the development data. Each line on
the figure corresponds to a single MD recall level
(varying from 100% to 70%). On the horizon-
tal axis, we plot the MD precision (from 10% to
100%) and on the vertical axis – the end-to-end
system MELA F-score. The curves support our in-
tuition that reliable MD recall is crucial for coref-
erence: when the MD recall drops to around 70%,
the MELA score remains at the baseline level even
for very high MD precision. It must be noted that
our simulation experiment relies on an unrealistic
assumption: we assume all the errors to be inde-
pendent. In a more practical setting, the MELA
F-score for a given combination of MD precision
and recall can be higher, because the coreference
system might fail to resolve the same NPs that
are problematic for the MD module. Nevertheless,
the curves illustrate the fact that any MD module
should be strongly biased towards recall in order
to be useful for coreference resolution.

We therefore reran our optimization experi-
ments to fit more parameters of the MD module.
Recall from Section 4 that we already used a small
amount of CoNLL training data to fit our d, u and
n values. We expanded the set of parameters, us-
ing the end-to-end performance (MELA F-score)
to select optimal values on the same subset. Table
5 lists all the parameters of our MD module.

d, u up-down pruning thresholds
n radial pruning threshold
j precision-recall trade-off (SVM-TK)
c cost factor (SVM-TK)
s size of MD vs. CR data splits
r tree vs. tree+vector representation

Table 5: Parameters optimized on a held-out data

Our experiments reveal that, indeed, a recall-
oriented version of our MD classifier yields the
most reliable end-to-end resolution. Table 6 shows
the MD performance of the best classifier selected
according to the MELA score. While the F-scores
of these biased classifiers are, obviously, much
lower than their unbiased counterparts, they still
manage to filter out a substantial amount of noun
phrases, at the same time maintaining a very high
recall level.

Finally, Tables 7 and 8 show the MELA score of
BART on the CoNLL-2012 development and test
sets respectively. To evaluate the impact of our

105



feature Baseline Arabic Chinese
StringMatch Mi and Mj have the same surface form + + +
MentionType relevant types of Mi and Mj , (cf. Soon et al.) + + +
GenderAgree Mi and Mj agree in gender + + +
NumberAgree Mi and Mj agree in number + +
NumberAgreeDual -*-, supports dual number +
AnimacyAgree Mi and Mj agree in animacy + +
Compatible Mi and Mj don’t disagree or overlap +
Alias heuristical NE-matching +
DistanceSentence distance in sentences between Mi and Mj + +
Appositive Mi and Mj are in an apposition +
First Mention Mi is the first mention in its sentence +
DistanceMarkable distance in mentions between Mi and Mj +
FirstSecondPerson Mi/j is a pronoun of the 1st/second person +
NonProSalience for non-pro Mi, # preceding same-head mentions +
SpeakerAlias heuristics for 1/2 pers. pro, use “speaker” layer +

Table 4: Features used for Coreference Resolution in Arabic and Chinese: each feature describes a pair
of mentions {Mi, Mj}, i < j, where Mi is a candidate antecedent and Mj is a candidate anaphor

kernel-based MD (TKMD), we compare its per-
formance against two baselines. The lower bound,
“all-NP”, considers all the NP-nodes in a parse
tree to be candidate mentions. The upper bound,
“gold-NP” only considers gold NP-nodes to be
candidate mentions. Note that the upper bound
does not include mentions that do not correspond
to NP-nodes at all (around 12% of all the mentions
in the development data, cf. Table 1 above).

Tables 7 and 8 also show the performance level
of BART’s rule-based MD module that was de-
veloped for English. Although this heuristic has
proved reliable on the English data, for example,
at the CoNLL 2011 and 2012 shared tasks, it is
not robust enough to be ported as-is to other lan-
guages: indeed, the performance of the heuristic
MD on Arabic and Chinese is lower than the all-
NP baseline. This highlights the importance of
a learning-based approach: while rule-based MD
shows good results for English, we cannot expect
spending ten more years on designing similar sys-
tems for other languages.

R P F
Arabic 90.67 31.07 46.28
Chinese 98.37 38.27 55.1

Table 6: Performance of the recall-oriented MD
classifier on the CoNLL development set.

For both languages, the performance goes up

Soon et al. (2001) Table 4
features features

Arabic
all-NP 46.15 46.32
English MD 43.46 43.49
TK-MD 48.13† 50.02†

gold-NP 63.27† 64.55†

Chinese
all-NP 51.04 51.40
English MD 46.77 46.77
TK-MD 53.40† 53.86†

gold-NP 57.30† 57.98†

Table 7: Evaluating the impact of MD and lin-
guistic knowledge: MELA F-score on the devel-
opment set, significant improvement over the cor-
responding all-NP baseline shown with †.

drastically when one shifts from a realistic eval-
uation (the “all-NP” baseline) to gold NP men-
tions. Kernel-based MD is able to recover part
of this difference, providing significant improve-
ments over the baseline (t-test on individual docu-
ments, p < 0.05).

Another important point is the difference be-
tween our basic feature set and more specific fea-
tures (cf, Table 4). The contribution of extra fea-
tures is relatively small and not significant, which
is not surprising given the fact that all of them are
very naı̈ve and do not address any coreference-

106



 40

 45

 50

 55

 60

 65

 70

 0.2  0.3  0.4  0.5  0.6  0.7  0.8  0.9  1

C
O

R
E

F
 M

E
LA

 F

EMD precision

Arabic EMD, randomly distorting goldnp baseline

R=1

R=0.95

R=0.9

R=0.85

R=0.8

R=0.75

R=0.7

 44

 46

 48

 50

 52

 54

 56

 58

 0.2  0.3  0.4  0.5  0.6  0.7  0.8  0.9  1

C
O

R
E

F
 M

E
LA

 F

EMD precision

Chinese EMD, randomly distorting goldnp baseline

R=1

R=0.95

R=0.9

R=0.85

R=0.8

R=0.75

Figure 3: Performance of an end-to-end corefer-
ence resolution system for different values of MD
Recall and Precision in a simulation experiment:
MELA F-score on the Arabic and Chinese devel-
opment data.

related phenomena specific for Arabic and Chi-
nese. However, the extra features help more when
the MD improves. This suggests that a robust
MD module is an essential prerequisite for further
work on coreference in new languages: a more ac-
curate set of mentions provides a better testbed for
manually engineered language-specific features or
constraints.

6 Conclusion and Future Work

In this paper we have investigated possibilities
for language-independent mention detection based
on syntactic tree kernels. We have shown that a
kernel-based approach can provide a robust pre-
processing system that is a vital prerequisite for
fast and efficient development of end-to-end mul-
tilingual coreference resolvers.

We have evaluated different tree and vector rep-
resentations, showing that the best performance is

Soon et al. (2001) Table 4
features features

Arabic
all-NP 46.79 47.36
English MD 43.77 43.65
TK-MD 48.38† 51.54†

gold-NP 63.07† 65.57†

Chinese
all-NP 53.26 53.24
English MD 48.99 48.99
TK-MD 58.11† 58.15†

gold-NP 59.97† 60.04†

Table 8: Evaluating the impact of MD and lin-
guistic knowledge: MELA F-score on the official
CoNLL-2012 test set, significant improvement
over the corresponding all-NP baseline shown
with †.

achieved by applying radial pruning to parse trees
and augmenting the resulting representation with
feature vectors, encoding very basic and shallow
properties of candidate NPs.

We have investigated possibilities of incorporat-
ing our MD module to an end-to-end coreference
resolution system. Our evaluation results show
significant improvement over the system relying
on the “all-NP” baseline for both Arabic and Chi-
nese. It should be stressed that no other baseline is
available without using deep linguistic expertise.

In the future, we plan to follow two directions
to further improve our algorithm. First, we want
to consider more global models of MD, providing
joint inference over sets of NP nodes, and, possi-
bly, incorporating CR predictions as well. Several
studies (Daume III and Marcu, 2005; Denis and
Baldridge, 2009) followed this direction recently,
showing promising results for joint MD and CR
modeling.

Second, we want to combine our learning-
based MD with more traditional heuristic systems.
While our approach provides a fast reliable testbed
and allows CR researchers to specifically focus on
coreference, rule-based MD modules have been
created for a variety of languages, especially for
European ones, in the past decade. We believe that
by combining such systems with our kernel-based
algorithm, we can build MD modules that show a
high performance level and, at the same time, are
more robust and portable to different domains and
corpora.

107



Acknowledgments

The research described in this paper has been par-
tially supported by the European Community’s
Seventh Framework Programme (FP7/2007-2013)
under the grant #288024: LIMOSINE – Linguis-
tically Motivated Semantic aggregation engiNes.

References
Shane Bergsma and David Yarowsky. 2011. NADA:

A robust system for non-referential pronoun detec-
tion. In Proc. DAARC, pages 12–23, Faro, Portugal,
October.

Adriane Boyd, Whitney Gegg-Harrison, and Donna
Byron. 2005. Identifying nonreferential it: A
machine learning approach incorporating linguisti-
cally motivated patterns. In Proceedingd of the
ACL Workshop on Feature Engineering for Machine
Learning in Natural Language Processing,, pages
40–47.

Michael Collins and Nigel Duffy. 2001. Convolution
kernels for natural language. In Advances in Neural
Information Processing Systems 14, pages 625–632.
MIT Press.

Michael Collins. 1999. Head-Driven Statistical Mod-
els for Natural Language Parsing. Ph.D. thesis,
University of Pennsylvania.

Pascal Denis and Jason Baldridge. 2009. Global joint
models for coreference resolution and named entity
classification. In Procesamiento del Lenguaje Natu-
ral 42, Barcelona: SEPLN.

George Doddington, Alexis Mitchell, Mark Przybocki,
Lance Ramshaw, Stephanie Strassell, and Ralph
Weischedel. 2004. The automatic content extrac-
tion (ACE) program–tasks, data, and evaluation. In
Proceedings of the Language Resources and Evalu-
ation Conference.

Richard Evans. 2001. Applying machine learning to-
ward an automatic classification of it. Literary and
Linguistic Computing, 16(1):45–57.

Radu Florian, Hany Hassan, Abraham Ittycheriah,
Hongyan Jing, Xiaoqiang Luo, Nicolas Nicolov, and
Salim Roukos. 2004. A statistical model for multi-
lingual entity detection and tracking. In Proceedings
of the Human Language Technology Conference of
the North American Chapter of the Association for
Computational Linguistics, pages 1–8.

Eduard Hovy, Mitchell Marcus, Martha Palmer,
Lance Ramshaw, and Ralph Weischedel. 2006.
OntoNotes: The 90% solution. In Proceedings of
HLT/NAACL.

Hal Daume III and Daniel Marcu. 2005. A large-scale
exploration of effective global features for a joint en-
tity detection and tracking model. In Proceedings of

the 2005 Conference on Empirical Methods in Nat-
ural Language Processing.

Abraham Ittycheriah, Lucian Vlad Lita, Nanda Kamb-
hatla, Nicolas Nicolov, Salim Roukos, and Margo
Stys. 2003. Identifying and tracking entity men-
tions in a maximum entropy framework. In Pro-
ceedings of the Conference of the North American
Chapter of the Association for Computational Lin-
guistics on Human Language Technology.

Thorsten Joachims. 1999. Making large-scale SVM
learning practical. In B. Schölkopf, C. Burges, and
A. Smola, editors, Advances in Kernel Methods -
Support Vector Learning. MIT-Press.

Lauri Karttunen. 1976. Discourse referents. In
J. McKawley, editor, Sytax and Semantics, vol-
ume 7, pages 361–385. Academic Press.

Jonathan K Kummerfeld, Mohit Bansal, David Burkett,
and Dan Klein. 2011. Mention detection: Heuris-
tics for the OntoNotes annotations. In Proceedings
of the Fifteenth Conference on Computational Nat-
ural Language Learning: Shared Task, pages 102–
106, Portland, Oregon, USA, June. Association for
Computational Linguistics.

Alessandro Moschitti. 2006. Efficient convolution ker-
nels for dependency and constituent syntactic trees.
In Proceedings of European Conference on Machine
Learning, pages 318–329.

Alessandro Moschitti. 2008. Kernel methods, syntax
and semantics for relational text categorization. In
Proceeding of the International Conference on In-
formation and Knowledge Management, NY, USA.

Vincent Ng. 2008. Unsupervised models for corefer-
ence resolution. In Proceedings of the 2008 Con-
ference on Empirical Methods in Natural Language
Processing, pages 640–649.

Sameer Pradhan, Alessandro Moschitti, Nianwen Xue,
Olga Uryupina, and Yuchen Zhang. 2012. CoNLL-
2012 shared task: Modeling multilingual unre-
stricted coreference in OntoNotes. In Proceedings
of the Sixteenth Conference on Computational Natu-
ral Language Learning (CoNLL 2012), Jeju, Korea.

Wee Meng Soon, Hwee Tou Ng, and Daniel
Chung Yong Lim. 2001. A machine learning ap-
proach to coreference resolution of noun phrases.
Computational Linguistic, 27(4):521–544.

Yannick Versley, Simone Paolo Ponzetto, Massimo
Poesio, Vladimir Eidelman, Alan Jern, Jason Smith,
Xiaofeng Yang, and Alessandro Moschitti. 2008.
BART: a modular toolkit for coreference resolution.
In Proceedings of the 46th Annual Meeting of the As-
sociation for Computational Linguistics on Human
Language Technologies, pages 9–12.

Imed Zitouni and Radu Florian. 2008. Mention detec-
tion crossing the language barrier. In Proceedings of
the 2008 Conference on Empirical Methods in Nat-
ural Language Processing.

108


