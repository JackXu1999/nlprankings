




































Ranking and Selecting Multi-Hop Knowledge Paths to Better Predict Human Needs


Proceedings of NAACL-HLT 2019, pages 3671–3681
Minneapolis, Minnesota, June 2 - June 7, 2019. c©2019 Association for Computational Linguistics

3671

Ranking and Selecting Multi-Hop Knowledge Paths to Better Predict

Human Needs

Debjit Paul

Research Training Group AIPHES
Institute for Computational Linguistics

Heidelberg University
paul@cl.uni-heidelberg.de

Anette Frank

Research Training Group AIPHES
Institute for Computational Linguistics

Heidelberg University
frank@cl.uni-heidelberg.de

Abstract

To make machines better understand senti-
ments, research needs to move from polarity
identification to understanding the reasons that
underlie the expression of sentiment. Cate-
gorizing the goals or needs of humans is one
way to explain the expression of sentiment in
text. Humans are good at understanding sit-
uations described in natural language and can
easily connect them to the character’s psycho-
logical needs using commonsense knowledge.
We present a novel method to extract, rank, fil-
ter and select multi-hop relation paths from a
commonsense knowledge resource to interpret
the expression of sentiment in terms of their
underlying human needs. We efficiently inte-
grate the acquired knowledge paths in a neural
model that interfaces context representations
with knowledge using a gated attention mech-
anism. We assess the model’s performance on
a recently published dataset for categorizing
human needs. Selectively integrating knowl-
edge paths boosts performance and establishes
a new state-of-the-art. Our model offers inter-
pretability through the learned attention map
over commonsense knowledge paths. Human
evaluation highlights the relevance of the en-
coded knowledge.

1 Introduction

Sentiment analysis and emotion detection are es-
sential tasks in human-computer interaction. Due
to its broad practical applications, there has been
rapid growth in the field of sentiment analysis
(Zhang et al., 2018). Although state-of-the-art
sentiment analysis can detect the polarity of text
units (Hamilton et al., 2016; Socher et al., 2013),
there has been limited work towards explaining the
reasons for the expression of sentiment and emo-
tions in texts (Li and Hovy, 2017). In our work,
we aim to go beyond the detection of sentiment,
toward explaining sentiments. Such explanations

can range from detecting overtly expressed expla-
nations or reasons for sentiments towards specific
aspects of, e.g., products or films, as in user re-
views to the explanation of the underlying reasons
for emotional reactions of characters in a narrative
story. The latter requires understanding of stories
and modeling the mental state of characters. Re-
cently, Ding and Riloff (2018) proposed to catego-
rize affective events with categories based on hu-
man needs, to provide explanations of people’s at-
titudes towards such events. Given an expression
such as I broke my leg, they categorize the reason
for the expressed negative sentiment as being re-
lated to a need concerning ‘health’.

In this paper we focus on the Modelling Naive
Psychology of Characters in Simple Common-

sense Stories dataset of Rashkin et al. (2018),
which contains annotations of a fully-specified
chain of motivations and emotional reactions of
characters for a collection of narrative stories. The
stories are annotated with labels from multiple the-
ories of psychology (Reiss, 2004; Maslow, 1943;
Plutchik, 1980) to provide explanations for the
emotional reactions of characters.

Similar to Ding and Riloff (2018), we hypothe-
size that emotional reactions (joy, trust, fear, etc.)
of characters can be explained by (dis)satisfaction
of their psychological needs. However, predict-
ing categories of human needs that underlie the
expression of sentiment is a difficult task for a
computational model. It requires not only detect-
ing surface patterns from the text, but also requires
commonsense knowledge about how a given situ-
ation may or may not satisfy specific human needs
of a character. Such knowledge can be diverse
and complex, and will typically be implicit in the
text. In contrast, human readers can make use of
relevant information from the story and associate
it with their knowledge about human interaction,
desires and human needs, and thus will be able



3672

to infer underlying reasons for emotions indicated
in the text. In this work, we propose a computa-
tional model that aims to categorize human needs
of story characters by integrating commonsense
knowledge from ConceptNet (Speer and Havasi,
2012). Our model aims to imitate human under-
standing of a story, by (i) learning to select rele-
vant words from the text, (ii) extracting pieces of
knowledge from the commonsense inventory and
(iii) associating them with human need categories
put forth by psychological theories. Our assump-
tion is that by integrating commonsense knowl-
edge in our model we will be able to overcome
the lack of textual evidence in establishing rela-
tions between expressed emotions in specific sit-
uations and the inferable human needs of story
characters. In order to provide such missing as-
sociations, we leverage the graph structure of the
knowledge source. Since these connections can be
diverse and complex, we develop a novel approach
to extract and rank multi-hop relation paths from
ConceptNet using graph-based methods.

Our contributions are: (i) We propose a novel
approach to extract and rank multi-hop relation
paths from a commonsense knowledge resource
using graph-based features and algorithms. (ii) We
present an end-to-end model enhanced with atten-
tion and a gated knowledge integration component
to predict human needs in a given context. To the
best of our knowledge, our model is the first to ad-
vance commonsense knowledge for this task. (iii)
We conduct experiments that demonstrate the ef-
fectiveness of the extracted knowledge paths and
show significant performance improvements over
the prior state-of-the-art. (iv) Our model provides
interpretability in two ways: by selecting relevant
words from the input text and by choosing relevant
knowledge paths from the imported knowledge. In
both cases, the degree of relevance is indicated via
an attention map. (v) A small-scale human eval-
uation demonstrates that the extracted multi-hop
knowledge paths are indeed relevant. Our code is
made publicly available.1

2 Related Work

Sentiment Analysis and Beyond. Starting with
Pang et al. (2002), sentiment analysis and emo-
tion detection has grown to a wide research field.
Researchers have investigated polarity classifica-

1
https://github.com/debjitpaul/

Multi-Hop-Knowledge-Paths-Human-Needs

tion, sentiment and emotion detection and clas-
sification (Tang et al., 2015; Yin et al., 2017; Li
et al., 2017) on various levels (tokens, phrases,
sentences or documents), as well as structured pre-
diction tasks such as the identification of holders
and targets (Deng and Wiebe, 2015) or sentiment
inference (Choi et al., 2016). Our work goes be-
yond the analysis of overtly expressed sentiment
and aims at identifying goals, desires or needs un-
derlying the expression of sentiment. Li and Hovy
(2017) argued that the goals of an opinion holder
can be categorized by human needs. There has
been work related to goals, desires, wish detec-
tion (Goldberg et al., 2009; Rahimtoroghi et al.,
2017). Most recently, Ding and Riloff (2018) pro-
pose to categorize affective events into physiolog-
ical needs to explain people’s motivations and de-
sires. Rashkin et al. (2018) published a dataset
for tracking emotional reactions and motivations
of characters in stories. In this work, we use this
dataset to develop a knowledge-enhanced system
that ‘explains’ sentiment in terms of human needs.

Integrating Structured Knowledge into Neu-

ral NLU Systems. Neural models aimed at solv-
ing NLU tasks have been shown to profit from the
integration of knowledge, using different methods:
Xu et al. (2017) show that injecting loosely struc-
tured knowledge with a recall-gate mechanism is
beneficial for conversation modeling; Mihaylov
and Frank (2018) and Weissenborn et al. (2017)
propose integration of commonsense knowledge
for reading comprehension: the former explic-
itly encode selected triples from ConceptNet us-
ing attention mechanisms, the latter enriches ques-
tion and context embeddings by encoding triples
as mapped statements extracted from ConceptNet.
Concurrently to our work, Bauer et al. (2018)
proposed a heuristic method to extract multi-hop
paths from ConceptNet for a reading comprehen-
sion task. They construct paths starting from con-
cepts appearing in the question to concepts appear-
ing in the context, aiming to emulate multi-hop
reasoning. Tamilselvam et al. (2017) use Concept-
Net relations for aspect-based sentiment analysis.
Similar to our approach, Bordes et al. (2014) make
use of knowledge bases to obtain longer paths con-
necting entities appearing in questions to answers
in a QA task. They also provide a richer repre-
sentation of answers by building subgraphs of en-
tities appearing in answers. In contrast, our work
aims to provide information about missing links



3673

Figure 1: Maslow and Reiss: Theories of Psychology
as presented in Rashkin et al. (2018).

between sentiment words in a text and underly-
ing human needs by extracting relevant multi-hop
paths from structured knowledge bases.

3 Selecting and Ranking Commonsense

Knowledge to Predict Human Needs

Our task is to automatically predict human needs
of story characters given a story context. In this
task, following the setup of Rashkin et al. (2018),
we explain the probable reasons for the expression
of emotions by predicting appropriate categories
from two theories of psychology: Hierarchy of
needs (Maslow, 1943) and basic motives (Reiss,
2002). The task is defined as a multi-label classifi-
cation problem with five coarse-grained (Maslow)
and 19 fine-grained (Reiss) categories, respec-
tively (see Fig. 1).1 We start with a Bi-LSTM en-
coder with self-attention as a baseline model, to
efficiently categorize human needs. We then show
how to select and rank multi-hop commonsense
knowledge paths from ConceptNet that connect
textual expressions with human need categories.
Finally, we extend our model with a gated knowl-
edge integration mechanism to incorporate rel-
evant multi-hop commonsense knowledge paths
for predicting human needs. An overview of the
model is given in Figure 2. We now describe each
component in detail.

3.1 A Bi-LSTM Encoder with Attention to

Predict Human Needs

Our Bi-LSTM encoder takes as input a sentence
S consisting of a sequence of tokens, denoted as
ws1, w

s
2, ...., w

s
n, or ws1:n and its preceding context

Cxt, denoted as wcxt1 , wcxt2 , ...., wcxtm , or wcxt1:m. As
further input we read the name of a story character,
which is concatenated to the input sentence. For

1Details about the labels are given in the Supplement.

Si
gm

oi
d 

la
ye

r

BILSTMs

De
ns

e 
la

ye
r

Co
nt

ex
t

.…
….

.…
….

Attention Weights

Co
m

m
on

 Se
ns

e 
Pa

th
s Encoding layer

w1cxt
w2cxt
w3cxt

::::
wmcxt

char
w1s
w2s
w3s
:::
wms

Encoding layer Self-attention layer

Se
nt

en
ce

En
co

di
ng

En
co

di
ng

En
co

di
ng

En
co

di
ng

cr1k

cr2k

cr3k
:::
:::
:::

crmk

Co
nc

at
en

at
e 

La
ye

r(o
)

In
pu

t 
re

pr
es

en
ta

tio
nxs

xcxt

xk

Gated Layer

Figure 2: Attention over multi-hop knowledge paths.

this input the model is tasked to predict appropri-
ate human need category labels z 2 Z, according
to a predefined inventory.

Embedding Layer: We embed each word from
the sentence and the context with a contextualized
word representation using character-based word
representations (ELMo) (Peters et al., 2018). The
embedding of each word wi in the sentence and
context is represented as esi and e

cxt
i , respectively.

Encoding Layer: We use a single-layer Bi-
LSTM (Hochreiter and Schmidhuber, 1997) to ob-
tain sentence and context representations hs and
hcxt, which we form by concatenating the final
states of the forward and backward encoders.

hs = BiLSTM(es1:n);h
cxt = BiLSTM(ecxt1:m)

(1)
A Self-Attention Layer allows the model to

dynamically control how much each token con-
tributes to the sentence and context representation.
We use a modified version of self-attention pro-
posed by Rei and Søgaard (2018), where both in-
put representations are passed through a feedfor-
ward layer to generate scalar values for each word
in context vcxti and sentence v

s
i (cf. (2-5)).

asi = ReLU(W
s
i h

s
i + b

s
i ), (2)

acxti = ReLU(W
cxt
i h

cxt
i + b

cxt
i ) (3)

vsi = W
s
v ia

s
i + b

s
vi (4)

vcxti = W
cxt
v ia

cxt
i + b

cxt
v i (5)

where, W s, bs,W cxt, bcxt,W sv ,W cxtv are train-
able parameters. We calculate the soft attention
weights for both sentence and context:

evi =
1

1 + exp(�vi)
; v̂i =

eviPN
k=1 evk

(6)



3674

where, evi is the output of the sigmoid function,
therefore evi is in the range [0,1] and v̂i is the nor-
malized version of evi. Values v̂i are used as atten-
tion weights to obtain the final sentence and con-
text representations xs and xcxt, respectively:

xs =
NX

i=1

v̂i
shsi (7) x

cxt =
MX

i=1

v̂i
cxthcxti

(8)

with N and M the number of tokens in S and
Cxt. The output of the self-attention layer is gen-
erated by concatenating xs and xcxt. We pass this
representation through a FF layer of dimension Z:

y = ReLU(Wy[x
s;xcxt] + by) (9)

where Wy, by are trainable parameters and ’;’ de-
notes concatenation of two vectors. Finally, we
feed the output layer y to a logistic regression layer
to predict a binary label for each class z 2 Z,
where Z is the set of category labels for a particu-
lar psychological theory (Maslow/Reiss, Fig. 1).

3.2 Extracting Commonsense Knowledge

To improve the prediction capacity of our model,
we aim to leverage external commonsense know-
ledge that connects expressions from the sen-
tence and context to human need categories. For
this purpose we extract multi-hop commonsense
knowledge paths that connect words in the textual
inputs with the offered human need categories, us-
ing as resource ConceptNet (Speer and Havasi,
2012), a large commonsense knowledge inven-
tory. Identifying contextually relevant information
from such a large knowledge base is a non-trivial
task. We propose an effective two-step method
to extract multi-hop knowledge paths that asso-
ciate concepts from the text with human need cat-
egories: (i) collect all potentially relevant knowl-
edge relations among concepts and human needs
in a subgraph for each input sentence; (ii) rank, fil-
ter and select high-quality paths using graph-based
local measures and graph centrality algorithms.

3.2.1 Construction of Sub-graphs

ConceptNet is a graph G = (V,E) whose nodes
are concepts and edges are relations between con-
cepts (e.g. CAUSES, MOTIVATEDBY). For each
sentence S we induce a subgraph G0 = (V 0, E0)
where V 0 comprises all concepts c 2 V that ap-
pear in S and the directly preceding sentence in

context Cxt. V 0 also includes all concepts c 2 V
that correspond to one of the human need cate-
gories in our label set Z. Fig. 3 shows an example.
The sub-graph is constructed as follows:

Shortest Paths: In a first step, we find all short-
est paths p0 from ConceptNet that connect any
concept ci 2 V 0 to any other concept cj 2 V 0
and to each human needs concept z 2 Z. We fur-
ther include in V 0 all the concepts c 2 V which
are contained in the above shortest paths p0.

Neighbours: To better represent the meaning
of the concepts in V 0, we further include in V 0 all
concepts c 2 V that are directly connected to any
c 2 V 0 that is not already included in V 0.

Sub-graph: We finally construct a connected
sub-graph G0 = (V 0, E0) from V 0 by defining E0

as the set of all ConceptNet edges e 2 E that di-
rectly connect any pair of concepts (ci, cj) 2 V 0.

Overall, we obtain a sub-graph that contains re-
lations and concepts which are supposed to be use-
ful to “explain” why and how strongly concepts ci
that appear in the sentence and context are associ-
ated with any of the human needs z 2 Z.

3.2.2 Ranking and Selecting Multi-hop Paths

We could use all possible paths p contained in the
sub-graph G0, connecting concepts ci from the text
and human needs concepts z contained in G0, as
additional evidence to predict suitable human need
categories. But not all of them may be relevant.
In order to select the most relevant paths, we pro-
pose a two-step method: (i) we score each vertex
with a score (Vscore) that reflects its importance in
the sub-graph and on the basis of the vertices’ Vs-
cores we determine a path score Pscore, as shown
in Figure 3; (ii) we select the top-k paths with re-
spect to the computed path score (Pscore) .

(i) Vertex Scores and Path Scores: We hy-
pothesize that the most useful commonsense rela-
tion paths should include vertices that are impor-
tant with respect to the entire extracted subgraph.
We measure the importance of a vertex using dif-
ferent local graph measures: the closeness central-
ity measure, page rank or personalized page rank.

Closeness Centrality (CC) (Bavelas, 1950) re-
flects how close a vertex is to all other vertices in
the given graph. It measures the average length of
the shortest paths between a given vertex vi and
all other vertices in the given graph G0. In a con-
nected graph, the closeness centrality CC(vi) of a



3675

vertex vi 2 G0 is computed as

V scoreCC(vi) =
| V 0 |P

j d (vj , vi)
(10)

where | V 0 | represents the number of vertices in
the graph G0 and d(vj , vi) represents the length of
the shortest path between vi and vj . For each path
we compute the normalized sum of VscoreX of all
vertices vj contained in the path, for any measure
X 2 {CC,PR,PPR}.

PscoreX =

P
j V scoreX(vj)

N
(11)

We rank the paths according to their PscoreCC ,
assuming that relevant paths will contain vertices
that are close to the center of the sub-graph G0.

PageRank (PR) (Brin and Page, 1998) is a
graph centrality algorithm that measures the rel-
ative importance of a vertex in a graph. The Page-
Rank score of a vertex vi 2 G0 is computed as:

V scorePR(vi) = ↵
X

j

uji
vj
Lj

+
1� ↵
n

(12)

where Lj =
P

i uji is the number of neighbors
of vertex j, ↵ is a damping factor representing the
probability of jumping from a given vertex vi to
another random vertex in the graph and n repre-
sents the number of vertices in G0. We calculate
PscorePR using Eq. 11 and order the paths ac-
cording to their PscorePR, assuming that relevant
paths will contain vertices with high relevance, as
reflected by a high number of incoming edges.

Personalized PageRank (PPR) (Haveliwala,
2002) is used to determine the importance of a ver-
tex with respect to a certain topic (set of vertices).
Instead of assigning equal probability for a ran-
dom jump 1�↵n , PPR assigns stronger probability
to certain vertices to prefer topical vertices. The
PPR score of a vertex v 2 G0 is computed as:

V scorePPR(vi) = ↵
X

j

uji
vj
Lj

+ (1� ↵)T

(13)
where T = 1|Tj | if nodes vi belongs to topic
Tj and otherwise T = 0. In our setting, Tj
will contain concepts from the text and human
needs, to assign them higher probabilities. We
calculate PscorePPR using Eq. 11 and order the
paths according to their scores, assuming that rel-
evant paths should contain vertices holding im-
portance with respect to vertices representing con-
cepts from the text and human needs.

Stewart has always been a big gamer since the age of 5. 
One day while at the mall he saw a sign for a video game 
tournament. He promptly signed up for the tournament.

Previous Sentence

He came home with a gold medal.

Sentence

Concepts mentioned in the text (c)

The following week he ended 
up winning the tournament.

Narrative Story

week wingold medal tournament

Path connecting human need (z) and concepts (c) 
with Vscores and Pscores

gold

medal jewelry

week time fine

position first gold medal

gold medal

Pscores

0.12 0.09 0.15

0.10 0.09 0.15

0.08 0.10 0.08

0.15 0.12 0.13

0.15

0.15

0.12

0.11

0.10

0.1375

Character: ‘Stewart’, Emotion: ‘Joy’, Human need = ‘status’ 

gold 
medal

medal

first 
place

first

position

status

jewelry

gold

Subgraph

status

status

statusjewelry

status

Figure 3: Illustration of commonsense path selection.
Top: Context and sentence, Bottom: Selected knowl-
edge paths with Vscores and Pscores (left) and the
corresponding subgraph. Concepts from the text are
marked with green dashed lines; blue boxes show the
human need label status assigned to Stewart.

(ii) Path Selection: We rank knowledge paths
based on their Pscore using the above relevance
measures, and construct ranked lists of paths of
two types: (i) paths connecting a human needs
concept z 2 Z to a concept mentioned in the text
(pc�z) 2 and (ii) paths connecting concepts in the
text (pc�c) 3. Ranked lists of paths are constructed
individually for concepts that constitute the start
or endpoint of a path: a human needs concept for
pc�z or any concept from the text for pc�c.

Figure 3 illustrates an example where the char-
acter Stewart felt joy after winning a gold medal.
The annotated human need label is status. We
show the paths selected by our algorithm that con-
nect concepts from the text and the human need
status. We select the top-k paths of type pc�z
for each human need to capture relevant knowl-
edge about human needs in relation to concepts in
the text. Similarly, we select the top-k paths of
type pc�c for each ci to capture relevant knowl-
edge about the text (not shown in Fig. 3).

3.3 Extending the Model with Knowledge

We have seen how to obtain a ranked list of com-
monsense knowledge paths from a subgraph ex-
tracted from ConceptNet that connect concepts
from the textual input and possible human needs
categories that are the system’s classification tar-

2pc�z denotes path connecting a human needs concept
z 2 Z and a concept c mentioned in the text.

3pc�c denotes path connecting a concept c and another
concept c mentioned in the text.



3676

gets. Our intuition is that the extracted common-
sense knowledge paths will provide useful evi-
dence for our model to link the content expressed
in the text to appropriate human need categories.
Paths that are selected by the model as a relevant
connection between the input text and the labeled
human needs concept can thus provide explana-
tions for emotions or goals expressed in the text in
view of a human needs category. We thus integrate
these knowledge paths into our model, (i) to help
the model making correct predictions and (ii) to
provide explanations of emotions expressed in the
text in view of different human needs categories.
For each input, we represent the extracted ranked
list of n commonsense knowledge paths p as a list
crk,1, crk,2, ...., crk,n, where each crk,i1:l represents
a path consisting of concepts and relations, with l
the length of the path. We embed all concepts and
relations in crk,i1:l with pretrained GloVe (Penning-
ton et al., 2014) embeddings.

Encoding Layer: We use a single-layer BiL-
STM to obtain encodings (hk,i) for each knowl-
edge path

hk,i = BiLSTM(ek,i1:n) (14)

where hk represents the output of the BiLSTM for
the knowledge path and i its the ranking index.

Attention Layer: We use an attention layer,
where each encoded commonsense knowledge
path interacts with the sentence representation xs

to receive attention weights (ĥk,i):

ehk,i = �(xshk,i), ĥk,i =
ehk,i

PN
i=1

ehk,i
(15)

In Eq. 15, we use sigmoid to calculate the atten-
tion weights, similar to Eq. 6. However, this time
we compute attention to highlight which knowl-
edge paths are important for a given input repre-
sentation (xs being the final state hidden repre-
sentation over the input sentence, Eq. 7). To ob-
tain the sentence-aware commonsense knowledge
representation xk, we pass the output of the atten-
tion layer through a feedforward layer. Wk, bk are
trainable parameters.

xk = ReLU(Wk(
NX

i=1

ĥk,ihk,i) + bk) (16)

3.4 Distilling Knowledge into the Model

In order to incorporate the selected and weighted
knowledge into the model, we concatenate the sen-

Classification Train Dev Test

Reiss 5432 1469 5368
Reiss without belonging class 5431 1469 5366
Maslow 6873 1882 6821

Table 1: Dataset Statistics: nb. of instances (sentences
with annotated characters and human need labels).

tence xs, context xcxt and knowledge xk represen-
tation and pass it through a FF layer.

oi = ReLU(Wz[x
s
i ;x

cxt
i ;x

k
i ] + bz) (17)

We employ a gating mechanism to allow the
model to selectively incorporate relevant informa-
tion from commonsense knowledge xk and from
the joint input representation yi (see Eq. 9) sep-
arately. We finally pass it to a logistic regression
classifier to predict a binary label for each class z
in the set Z of category labels

zi = �(Weyz(oi � yi + oi � xki ) + beyz) (18)

where � represents element-wise multiplication,
beyz , Weyz are trainable parameters.

4 Experimental Setup

Dataset: We evaluate our model on the Mod-
eling Naive Psychology of Characters in Sim-

ple Commonsense Stories (MNPCSCS) dataset
(Rashkin et al., 2018). It contains narrative sto-
ries where each sentence is annotated with a
character and a set of human need categories
from two inventories: Maslow’s (with five coarse-
grained) and Reiss’s (with 19 fine-grained) cat-
egories (Reiss’s labels are considered as sub-
categories of Maslow’s). The data contains the
original worker annotations. Following prior work
we select the annotations that display the “major-
ity label” i.e., categories voted on by � 2 work-
ers. Since no training data is available, similar to
prior work we use a portion of the devset as train-
ing data, by performing a random split, using 80%
of the data to train the classifier, and 20% to tune
parameters. Data statistics is reported in Table 1.

Rashkin et al. (2018) report that there is low
annotator agreement i.a. between the belonging
and the approval class. We also find high co-
occurrence of the belonging, approval and so-
cial contact classes, where belonging and so-
cial contact both pertain to the Maslow class
Love/belonging while approval belongs to the



3677

Maslow class Esteem. This indicates that belong-
ing interacts with Love/belonging and Esteem in
relation to social contact. We further observed
during our study that in the Reiss dataset the
number of instances annotated with the belong-
ing class is very low (no. of instances in training
is 24, and in dev 5). The performance for this
class is thus severely hampered, with 4.7 F1 score
for BiLSTM+Self-Attention and 7.1 F1 score for
BiLSTM+Self-Attention+Knowledge. After es-
tablishing benchmark results with prior work (cf.
Table 2, including belonging), we perform all fur-
ther experiments with a reduced Reiss dataset, by
eliminating the belonging class from all instances.
This impacts the overall number of instances only
slightly: by one instance for training and two in-
stances for test, as shown in Table 1.

Training: During training we minimize the
weighted binary cross entropy loss,

L =
ZX

z=1

wzyzlogeyz + (1� wz)(1� yz)log(1� eyz)

(19)

wz =
1

1� exp�
p

P (yz)
(20)

where Z is the number of class labels in the clas-
sification tasks and wz is the weight. P (yz) is the
marginal class probability of a positive label for z
in the training set.

Embeddings: To compare our model with prior
work we experiment with pretrained GloVe (100d)
embeddings (Pennington et al., 2014). Otherwise
we used GloVe (300d) and pretrained ELMo em-
beddings (Peters et al., 2018) to train our model.

Hyperparameters for Knowledge Inclusion:

We compute ranked lists of knowledge paths of
two types: pc�z and pc�c. We use the top-3 pc�z
paths for each z using our best ranking strategy
(Closeness Centrality + Personalized PageRank)
in our best system results (Tables 2, 3, 5), and also
considered paths pc�c (top-3 per pair) when eval-
uating different path selection strategies (Table 4).

Evaluation Metrics: We predict a binary label
for each class using a binary classifier so the pre-
diction of each label is conditionally independent
of the other classes given a context representation
of the sentence. In all prediction tasks we report
the micro-averaged Precision (P), Recall (R) and
F1 scores by counting the number of positive in-
stances across all of the categories. All reported
results are averaged over five runs. More informa-

Reiss Maslow
Model WE P R F1 P R F1

BiLSTM⇧ G100d 18.35 27.61 22.05 31.29 33.85 32.52
CNN⇧ G100d 18.89 31.22 23.54 27.47 41.01 32.09
REN⇧ G100d 16.79 22.20 19.12 26.24 42.14 32.34
NPN⇧ G100d 13.13 26.44 17.55 24.27 44.16 31.33
BM G100d 25.08 28.25 26.57 47.65 60.98 53.54
BM + K| G100d 28.47 39.13 32.96 50.54 64.54 56.69
BM ELMo 29.50 44.28 35.41±0.23 53.86 67.23 59.81±0.23
BM + K| ELMo 31.74 43.51 36.70±0.14 57.90 66.07 61.72±0.11
BM? ELMo 31.45 44.29 37.70
BM + K?| ELMo 36.76 42.53 39.44

Table 2: Multi-label Classification Results: ⇧: results in
Rashkin et al.; ?: w/o belonging; BM: BiLSTM+Self-
Att.; +K:w/ knowledge, |:ranking method CC+PPR.

tion on the dataset, metrics and all other training
details are given in the Supplement.

5 Results

Our experiment results are summarized in Table
2. We benchmark our baseline BiLSTM+Self-
Attention model (BM, BM w/ knowledge) against
the models proposed in Rashkin et al. (2018): a
BiLSTM and a CNN model, and models based on
the recurrent entity network (REN) (Henaff et al.,
2016) and neural process networks (NPN) (Bosse-
lut et al., 2017). The latter differ from the basic
encoding models (BiLSTM, CNN) and our own
models by explicitly modeling entities. We find
that our baseline model BM outperforms all prior
work, achieving new state-of-the-art results. For
Maslow we show improvement of 21.02 pp. F1
score. For BM+K this yields a boost of 6.39 and
3.15 pp. F1 score for Reiss and Maslow, respec-
tively. When using ELMo with BM we see an im-
provement in recall. However, adding knowledge
on top improves the precision by 2.24 and 4.04
pp. for Reiss and Maslow. In all cases, injecting
knowledge improves the model’s precision and F1
score.

Table 2 (bottom) presents results for the reduced
dataset, after eliminating Reiss’ label belonging.
Since belonging is a rare class, we observe fur-
ther improvements. We see the same trend: adding
knowledge improves the precision of the model.

5.1 Model Ablations

To obtain better insight into the contributions of
individual components of our models, we perform
an ablation study (Table 3). Here and in all later
experiments we use richer (300d) GloVe embed-
dings and the dataset w/o belonging. We show
results including and not including self-attention



3678

WE Atten K Gated P R F1

G300d - - - 23.31 34.69 27.89
G300d X - - 26.09 35.59 30.11
G300d X X - 27.99 37.73 32.14
G300d X X X 28.65 39.42 33.19
ELMo - - - 32.35 42.66 36.80
ELMo X - - 31.45 44.29 37.70
ELMo X X - 32.65 45.60 38.05
ELMo X X X 36.76 42.53 39.44

Table 3: Model ablations for Reiss Classification on
MNPCSCS dataset w/o belonging.

Path Ranking P R F1

S+M(Pc�z+ Pc�c) None 32.51 42.70 36.90
S+M(Pc�z+ Pc�c) Random 31.63 43.35 36.57

Single Hop(Pc�z) CC + PPR 33.00 44.63 37.94
S+M(Pc�c + Pc�z) CC + PPR 35.30 44.11 39.21

S+M(Pc�z) CC 33.45 47.93 39.40
S+M(Pc�z) PR 35.51 42.82 38.82
S+M(Pc�z) PPR 36.23 43.09 39.34
S+M(Pc�z) CC + PPR 36.76 42.53 39.44

Table 4: Results for different path selection strategies
on MNPCSCS w/o belonging; S+M:Single+Multi hop.

and knowledge components. We find that using
self-attention over sentences and contexts is highly
effective, which indicates that learning how much
each token contributes helps the model to improve
performance. We observe that integrating knowl-
edge improves the overall F1 score and yields a
gain in precision with ELMo. Further, integrat-
ing knowledge using the gating mechanism we see
a considerable increase of 3.58 and 1.74 pp. F1
score improvement over our baseline model for
GloVe and ELMo representations respectively.

5.2 Commonsense Path Selection

We further examine model performance for (i) dif-
ferent variants of selecting commonsense knowl-
edge, including (ii) the effectiveness of the rele-
vance ranking strategies discussed in §3.2.2. In
Table 4, rows 3-4 use our best ranking method:
CC+PPR; rows 5-8 show results when using the
top-3 ranked pc�z paths for each human need z
with different ranking measures. None shows re-
sults when no selection is applied to the set of
extracted knowledge paths (i.e., using all possi-
ble paths from pc�z and pc�c). Random randomly
selects 3 paths for each human need from the set
of paths used in None. This yields only a slight
drop in performance. This suggests that not ev-
ery path is relevant. We evaluate the performance
when only considering single-hop paths (now top-
3 ranked using CC+PPR) (Single-Hop). We see an
improvement over random paths and no selection,

but not important enough. In contrast, using both
single and multi-hop paths in conjunction with rel-
evance ranking improves the performance consid-
erably (rows 4-8). This demonstrates that multi-
hop paths are informative. We also experimented
with pc�c+pc�z . We find improvement in recall,
however the overall performance decreases by 0.2
F1 score compared to paths pc�z ranked using CC
+ PPR. Among different ranking measures preci-
sion for Personalized PageRank performs best in
comparison with CC and PR in isolation, and re-
call for CC in isolation is highest. Combining CC
and PPR yields the best results among the different
ranking strategies (rows 5-8).

6 Analysis

6.1 Performance per Human Need

Categories

We examined the model performance on each cat-
egory (cf. Figure 4). The model performs well for
basic needs like food, safety, health, romance, etc.
We note that inclusion of knowledge improves the
performance for most classes (only 5 classes do
not profit from knowledge compared to only using
ELMo), especially for labels which are rare like
honor, idealism, power. We also found that the
annotated labels can be subjective. For instance,
Tom lost his job is annotated with order while our
model predicts savings, which we consider to be
correct. Similar to Rashkin et al. (2018) we ob-
serve that preceding context helps the model to
better predict the characters’ needs, e.g., Context:
Erica’s [..] class had a reading challenge [..]. If

she was able to read 50 books [..] she won a pizza

party!; Sentence: She read a book every day for

the entire semester is annotated with competition.
Without context the predicted label is curiosity,
however when including context, the model pre-
dicts competition, curiosity. We measure the mod-
els performance when applying it only to the first
sentence of each story (i.e., without the context).
As shown in Table 5, also in this setting the in-
clusion of knowledge improves the performance.

Model WE P R F1

BM ELMo 33.39 45.15 38.39
BM+K ELMo 36.36 44.02 39.83

Table 5: Multi-label classification on MNPCSCS w/o
belonging class and w/o context (1st sentence only)

.



3679

Figure 4: Best model’s performance per human needs
(F1 scores) for Reiss on MNPCSCS dataset.

Context: Timmy had to renew his driver’s license. He went to his local DMV. He
waited in line for nearly 2 hours. He took a new picture for his driver’s license.
Sentence: He drove back home after an exhausting day.
True Label: rest

Predicted Label (BM): status, approval, order

Predicted Label (BM+K): rest

Figure 5: Interpreting the attention weights on sentence
representation and selected commonsense paths.

6.2 Human Evaluation of Extracted Paths

We conduct human evaluation to test the effec-
tiveness and relevance of the extracted common-
sense knowledge paths. We randomly selected
50 sentence-context pairs with their gold labels
from the devset and extracted knowledge paths
that contain the gold label (using CC+PPR for
ranking). We asked three expert evaluators to
decide whether the paths are relevant to provide
information about the missing links between the
concepts in the sentence and the human need (gold
label). The inter-annotator agreement had a Fleiss’
= 0.76. The result for this evaluation shows that
in 34% of the cases computed on the basis of ma-
jority agreement, our algorithm was able to select
a relevant commonsense path. More details about
the human evaluation are given in the Supplement.

6.3 Interpretabilty

Finally we study the learned attention distributions
of the interactions between sentence representa-
tion and knowledge paths, in order to interpret how
knowledge is employed to make predictions. Vi-
sualization of the attention maps gives evidence of
the ability of the model to capture relevant knowl-
edge that connects human needs to the input text.
The model provides interpretability in two ways:
by selecting tokens from the input text using Eq.6
and by choosing knowledge paths from the im-
ported knowledge using Eq.15 as shown in Fig-
ure 5. Figure 5 shows an example where including
knowledge paths helped the model to predict the
correct human need category. The attention map
depicts which exact paths are selected to make the
prediction. In this example, the model correctly
picks up the token “exhausting” from the input
sentence and the knowledge path “exhausting is
a fatigue causes desire rest”. We present more
examples of extracted knowledge and its attention
visualization in the Supplement.

7 Conclusion

We have introduced an effective new method to
rank multi-hop relation paths from a common-
sense knowledge resource using graph-based algo-
rithms. Our end-to-end model incorporates multi-
hop knowledge paths to predict human needs. Due
to the attention mechanism we can analyze the
knowledge paths that the model considers in pre-
diction. This enhances transparency and inter-
pretability of the model. We provide quantitative
and qualitative evidence of the effectiveness of the
extracted knowledge paths. We believe our rele-
vance ranking strategy to select multi-hop knowl-
edge paths can be beneficial for other NLU tasks.
In future work, we will investigate structured and
unstructured knowledge sources to find explana-
tions for sentiments and emotions.

Acknowledgements

This work has been supported by the German Re-
search Foundation as part of the Research Training
Group Adaptive Preparation of Information from
Heterogeneous Sources (AIPHES) under grant
No. GRK 1994/1. We thank NVIDIA Corporation
for donating GPUs used in this research. We thank
Éva Mújdricza-Maydt, Esther van den Berg and
Angel Daza for evaluating the paths, and Todor
Mihaylov for his valuable feedback.



3680

References

Lisa Bauer, Yicheng Wang, and Mohit Bansal. 2018.
Commonsense for Generative Multi-Hop Question
Answering Tasks. In Proceedings of the 2018 Con-
ference on Empirical Methods in Natural Language

Processing, pages 4220–4230.

Alex Bavelas. 1950. Communication patterns in task-
oriented groups. The Journal of the Acoustical So-
ciety of America, 22(6):725–730.

Antoine Bordes, Sumit Chopra, and Jason Weston.
2014. Question Answering with Subgraph Embed-
dings. In Proceedings of the 2014 Conference on
Empirical Methods in Natural Language Processing

(EMNLP), pages 615–620.

Antoine Bosselut, Omer Levy, Ari Holtzman, Corin
Ennis, Dieter Fox, and Yejin Choi. 2017. Simulat-
ing action dynamics with neural process networks.
CoRR, abs/1711.05313.

Sergey Brin and Lawrence Page. 1998. The anatomy of
a large-scale hypertextual web search engine. Com-
puter networks and ISDN systems, 30(1-7):107–117.

Eunsol Choi, Hannah Rashkin, Luke Zettlemoyer, and
Yejin Choi. 2016. Document-level sentiment infer-
ence with social, faction, and discourse context. In
Proceedings of the 54th Annual Meeting of the As-

sociation for Computational Linguistics (Volume 1:

Long Papers), volume 1, pages 333–343.

Lingjia Deng and Janyce Wiebe. 2015. Joint prediction
for entity/event-level sentiment analysis using prob-
abilistic soft logic models. In Proceedings of the
2015 Conference on Empirical Methods in Natural

Language Processing, pages 179–189.

Haibo Ding and Ellen Riloff. 2018. Human needs cate-
gorization of affective events using labeled and unla-
beled data. In Proceedings of the 2018 Conference
of the North American Chapter of the Association

for Computational Linguistics: Human Language

Technologies, Volume 1 (Long Papers), volume 1,
pages 1919–1929.

Andrew B Goldberg, Nathanael Fillmore, David An-
drzejewski, Zhiting Xu, Bryan Gibson, and Xiaojin
Zhu. 2009. May all your wishes come true: A study
of wishes and how to recognize them. In Proceed-
ings of Human Language Technologies: The 2009

Annual Conference of the North American Chap-

ter of the Association for Computational Linguistics,
pages 263–271.

William L Hamilton, Kevin Clark, Jure Leskovec, and
Dan Jurafsky. 2016. Inducing domain-specific senti-
ment lexicons from unlabeled corpora. In Proceed-
ings of the Conference on Empirical Methods in Nat-

ural Language Processing. Conference on Empirical

Methods in Natural Language Processing, volume
2016, page 595. NIH Public Access.

Taher H Haveliwala. 2002. Topic-sensitive pagerank.
In Proceedings of the 11th international conference
on World Wide Web, pages 517–526. ACM.

Mikael Henaff, Jason Weston, Arthur Szlam, Antoine
Bordes, and Yann LeCun. 2016. Tracking the
world state with recurrent entity networks. CoRR,
abs/1612.03969.

Sepp Hochreiter and Jürgen Schmidhuber. 1997.
Long short-term memory. Neural Computation,
9(8):1735–1780.

Diederik P Kingma and Jimmy Lei Ba. 2014. Adam:
Amethod for stochastic optimization. In Proc. 3rd
Int. Conf. Learn. Representations.

Jiwei Li and Eduard Hovy. 2017. Reflections on senti-
ment/opinion analysis. In A Practical Guide to Sen-
timent Analysis, pages 41–59. Springer.

Zheng Li, Yu Zhang, Ying Wei, Yuxiang Wu, and
Qiang Yang. 2017. End-to-end adversarial memory
network for cross-domain sentiment classification.
In Proceedings of the International Joint Conference
on Artificial Intelligence (IJCAI 2017).

Abraham Harold Maslow. 1943. A theory of human
motivation. Psychological review, 50(4):370.

Todor Mihaylov and Anette Frank. 2018. Knowledge-
able reader: Enhancing cloze-style reading compre-
hension with external commonsense knowledge. In
Proceedings of the 56th Annual Meeting of the As-

sociation for Computational Linguistics (Volume 1:

Long Papers), pages 821–832.

Bo Pang, Lillian Lee, and Shivakumar Vaithyanathan.
2002. Thumbs up?: sentiment classification using
machine learning techniques. In Proceedings of the
ACL-02 conference on Empirical methods in natural

language processing-Volume 10, pages 79–86.

Jeffrey Pennington, Richard Socher, and Christopher
Manning. 2014. Glove: Global vectors for word
representation. In Proceedings of the 2014 confer-
ence on empirical methods in natural language pro-

cessing (EMNLP), pages 1532–1543.

Matthew Peters, Mark Neumann, Mohit Iyyer, Matt
Gardner, Christopher Clark, Kenton Lee, and Luke
Zettlemoyer. 2018. Deep contextualized word rep-
resentations. In Proceedings of the 2018 Confer-
ence of the North American Chapter of the Associ-

ation for Computational Linguistics: Human Lan-

guage Technologies, Volume 1 (Long Papers), pages
2227–2237.

Robert Plutchik. 1980. A general psychoevolutionary
theory of emotion. Theories of emotion, 1(4):3–31.

Elahe Rahimtoroghi, Jiaqi Wu, Ruimin Wang, Pranav
Anand, and Marilyn Walker. 2017. Modelling pro-
tagonist goals and desires in first-person narrative.
In Proceedings of the 18th Annual SIGdial Meeting
on Discourse and Dialogue, pages 360–369.



3681

Hannah Rashkin, Antoine Bosselut, Maarten Sap,
Kevin Knight, and Yejin Choi. 2018. Modeling
naive psychology of characters in simple common-
sense stories. In Proceedings of the 56th Annual
Meeting of the Association for Computational Lin-

guistics (Volume 1: Long Papers), pages 2289–
2299.

Marek Rei and Anders Søgaard. 2018. Zero-shot se-
quence labeling: Transferring knowledge from sen-
tences to tokens. In Proceedings of the 2018 Con-
ference of the North American Chapter of the Asso-

ciation for Computational Linguistics: Human Lan-

guage Technologies, Volume 1 (Long Papers), vol-
ume 1, pages 293–302.

Steven Reiss. 2002. Who am I?: 16 basic desires that
motivate our actions define our persona. Penguin.

Steven Reiss. 2004. Multifaceted nature of intrinsic
motivation: The theory of 16 basic desires. Review
of general psychology, 8(3):179.

Richard Socher, Alex Perelygin, Jean Wu, Jason
Chuang, Christopher D Manning, Andrew Ng, and
Christopher Potts. 2013. Recursive deep models
for semantic compositionality over a sentiment tree-
bank. In Proceedings of the 2013 conference on
empirical methods in natural language processing,
pages 1631–1642.

Robert Speer and Catherine Havasi. 2012. Represent-
ing General Relational Knowledge in ConceptNet 5.
In LREC, pages 3679–3686.

Srikanth Tamilselvam, Seema Nagar, Abhijit Mishra,
and Kuntal Dey. 2017. Graph Based Sentiment Ag-
gregation using ConceptNet Ontology. In Proceed-
ings of the Eighth International Joint Conference on

Natural Language Processing (Volume 1: Long Pa-

pers), volume 1, pages 525–535.

Duyu Tang, Bing Qin, and Ting Liu. 2015. Docu-
ment modeling with gated recurrent neural network
for sentiment classification. In Proceedings of the
2015 conference on empirical methods in natural

language processing, pages 1422–1432.

Dirk Weissenborn, Tomáš Kočiskỳ, and Chris Dyer.
2017. Dynamic Integration of Background Knowl-
edge in Neural NLU Systems. arXiv preprint
arXiv:1706.02596.

Zhen Xu, Bingquan Liu, Baoxun Wang, Chengjie
Sun, and Xiaolong Wang. 2017. Incorporating
loose-structured knowledge into conversation mod-
eling via recall-gate LSTM. In Neural Networks
(IJCNN), 2017 International Joint Conference on,
pages 3506–3513. IEEE.

Yichun Yin, Yangqiu Song, and Ming Zhang. 2017.
Document-level multi-aspect sentiment classifica-
tion as machine comprehension. In Proceedings of
the 2017 Conference on Empirical Methods in Nat-

ural Language Processing, pages 2044–2054.

Lei Zhang, Shuai Wang, and Bing Liu. 2018. Deep
learning for sentiment analysis: A survey. Wiley In-
terdisciplinary Reviews: Data Mining and Knowl-

edge Discovery.


