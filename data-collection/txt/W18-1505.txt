



















































Towards Controllable Story Generation


Proceedings of the First Workshop on Storytelling, pages 43–49
New Orleans, Louisiana, June 5, 2018. c©2018 Association for Computational Linguistics

Towards Controllable Story Generation

Nanyun Peng Marjan Ghazvininejad Jonathan May Kevin Knight
Information Sciences Institute & Computer Science Department

University of Southern California
{npeng,ghazvini,jonmay,knight}@isi.edu

Abstract

We present a general framework of analyzing
existing story corpora to generate controllable
and creative new stories. The proposed frame-
work needs little manual annotation to achieve
controllable story generation. It creates a new
interface for humans to interact with comput-
ers to generate personalized stories. We apply
the framework to build recurrent neural net-
work (RNN)-based generation models to con-
trol story ending valence1 (Egidi and Gerrig,
2009) and storyline. Experiments show that
our methods successfully achieve the control
and enhance the coherence of stories through
introducing storylines. with additional control
factors, the generation model gets lower per-
plexity, and yields more coherent stories that
are faithful to the control factors according to
human evaluation.

1 Introduction

Storytelling is an important task in natural lan-
guage generation, which plays a crucial role in the
generation of various types of texts, such as nov-
els, movies, and news articles. Automatic story
generation efforts started as early as the 1970s
with the TALE-SPIN system (Meehan, 1977).
Early attempts in this field relied on symbolic
planning (Meehan, 1977; Lebowitz, 1987; Turner,
1993; Bringsjord and Ferrucci, 1999; Perez and
Sharples, 2001; Riedl and Young, 2010), case-
based reasoning (Gervas et al., 2005), or gener-
alizing knowledge from existing stories to assem-
ble new ones (Swanson and Gordon, 2012; Li
et al., 2013). In recent years, deep learning mod-
els are used to capture higher level structure in
stories. Roemmele et al. (2017) use skip-thought
vectors (Kiros et al., 2015) to encode sentences,
and a Long Short-Term Memory (LSTM) net-
work (Hochreiter and Schmidhuber, 1997) to gen-

1Happy or sad endings.

Stories

Analyzer

Generator

Human
Inputs

Structured
Control	Factors

Sam	was	a	star	athlete.
He	ran	track	at	college.	
There	was	a	big	race	
coming	up.	
Everyone	was	sure	he	
would	win.	

sad	
ending

Sam	got	very	nervous	
and	lost	the	game.

happy	
ending

He	got	the	first	prize!

Figure 1: An overview (upper) and an example (lower) of
the proposed analyze-to-generate story framework.

erate stories. Martin et al. (2017) train a recurrent
encoder-decoder neural network (Sutskever et al.,
2014) to predict the next event in the story.

Despite significant progress in automatic story
generation, there has been less emphasis on con-
trollability: having a system takes human inputs
and composes stories accordingly. With the recent
successes on controllable generation of images
(Chen et al., 2016; Siddharth et al., 2017; Lample
et al., 2017), dialog responses (Wang et al., 2017),
poems (Ghazvininejad et al., 2017), and different
styles of text (Hu et al., 2017; Ficler and Goldberg,
2017; Shen et al., 2017; Fu et al., 2017). people
would want to control a story generation system
to produce interesting and personalized stories.

This paper emphasizes the controllability as-
pect. We propose a completely data-driven ap-
proach towards controllable story generation by
analyzing the existing story corpora. First, an an-
alyzer extracts control factors from existing sto-
ries, and then a generator learns to generate sto-
ries according to the control factors. This creates
an excellent interface for humans to interact: the
generator can take human-supplied control factors
to generate stories that reflect a user’s intent. Fig-

43



ure 1 gives the overview (upper) and an example
(lower) of the framework. The instantiations of the
analyzer and the generator are flexible and can be
easily applied to different scenarios. We explore
two control factors: (1) ending valence (happy or
sad ending) and (2) storyline keywords. We use
supervised classifiers and rule-based keyword ex-
tractors for analysis, and conditional RNNs for
generation.

The contributions of the paper are two-fold:

1. We propose a general framework enabling in-
teractive story generation by analyzing exist-
ing story corpora.

2. We apply the framework to control story end-
ing valence and storyline, and show that with
these additional control factors, our models
generate stories that are both more coherent
and more faithful to human inputs.

2 Controllable Story Generation

As a pilot study, we explore the control of 1) end-
ing valence, which is an abstract, style-level ele-
ment of stories, and 2) storyline, which is a more
concrete, content-level concept for stories.

2.1 Ending Valence Control
Prior work has explored manipulating emotion in
interactive storytelling (Cavazza et al., 2009). For
simplicity, we refine our scope to manipulating the
ending valence for controllable story generation.
We categorize ending valence into happyEnding,
sadEnding, or cannotTell.

Analyzer. The analyzer for the ending valence
control is a classifier that labels each story as hap-
pyEnding, sadEnding, or cannotTell. Formally,
given a story corpus X = {x1,x2, · · · ,xN} with
N stories, the ending valence analyzer is a func-
tion fv that maps each story xi to a label li:

li = f
v(xi),

where i indexes instances. Since there is no prior
work on analyzing story ending valence, we build
our own analyzer by collecting some annotations
for story ending valence from Amazon Mechani-
cal Turk (AMT) and building a supervised classi-
fier. We employ an LSTM-based logistic regres-
sion classifier as it learns feature representations
that capture long-term dependencies between the
words, and has been shown efficient in text classi-
fication tasks (Tang et al., 2015).

Specifically, we use a bidirectional-LSTM to
encode an input story into a sequence of vector
representations hi = {hi,1, hi,2, · · · , hi,T }, where
hi = BiLSTM(xi) = [

−→
h i;
←−
h i], T denotes the

story length, [, :, ] denotes element-wise concate-
nation.

−→
h i and

←−
h i are sequences of vectors com-

puted by a forward and a backward LSTM. an
LSTM-cell is applied at each step to complete the
following computations:




ii,t
fi,t
oi,t
ci,t


 =




σ
σ
σ

tanh


U

(
Ewxi,t
hi,t−1

)
(1a)

c̃i,t = fi,t � c̃i,t−1 + ii,t � ci,t (1b)
hi,t = LSTM -cell(xi,t, hi,t−1) (1c)

= oi,t � tanh(c̃i,t) (1d)

ii,t, fi,t, oi,t, ci,t are the input, forget, output gates,
and a contemporary central memory that control
the information flow of the previous contexts and
the current input. σ and tanh denotes element-
wise sigmoid and tanh function. Ew is an embed-
ding matrix that maps an input word xi,t to a x-
dimensional vector. Each hi,t is a d-dimensional
vector that can be viewed as the contextual repre-
sentation of word xi,t.

To obtain the sentence representation, we take
a max pooling over the sentence, where for each
dimension j of the vector ĥi, we have:

ĥji = maxt∈[1,...,T ] h
j
i,t, j = 1, ..., d. (2)

The final classifier is defined as:

fv(xi) = g(Wĥi + b), (3)

where g() is the softmax function, W and b are
model parameters that are jointly learned with the
BiLSTM parameters.

Generator. The generator for the ending
valence-controlled story generation is a con-
ditional language model, where the probabil-
ity of generating each word is denoted as
p(wt|wt−11 , l; θ); l represents the ending valence
label and θ represents model parameters. We learn
valence embeddings for the ending valence labels
to facilitate the computation. Formally, we learn
an embedding matrix El to map each label lk into
a vector:

ekl = E
l[lk],

44



whereEl is am×pmatrix that maps each label (p
of them) into a m-dimensional vector. The ending
valence embeddings dimension are made the same
as the word embedding dimension for simplicity.

We add the ending valence as follows:

p(wt|wt−11 , l; θ) =
{
g(V F(el,F(wt−1, ht−1))), t = s
g(V F(wt−1, ht−1)), t = others (4)

where s denotes the position right before the end-
ing sentence, g() is the softmax function, F means
the computations of an LSTM-cell, and V denotes
parameters that perform a linear transformation.
We treat the ending valence as an additional input
to the story. The valence embeddings are jointly
learned with other model parameters.

2.2 Storyline Control
Li et al. (2013) introduced plot graphs which con-
tain events and their relations to represent story-
line. Although the representation is rich, these
plot graphs are hard to define and curate without
highly specialized knowledge. In this pilot study,
we follow what Yan (2016) did for poetry, to use
a sequence of words as the storyline. We further
confine the words to appear in the original story.

Analyzer. The analyzer for storyline control
is an extractor that extracts a sequence of words
ki = {ki,1, ki,2, . . . , ki,r} from each story xi. The
kis are ordered according to their order in the
story. We adapt the RAKE algorithm (Rose et al.,
2010) for keyword extraction, which builds docu-
ment graphs and weights the importance of each
word combining several word-level and graph-
level criteria. We extract the most important word
from each sentence as the storyline.

Generator. The generator for storyline-
controlled generation is also a conditional lan-
guage model. Specifically, we employ the seq2seq
model with attention (Bahdanau et al., 2014) im-
plemented in OpenNMT (Klein et al., 2017).
Specifically, the storyline words are encoded into
vectors by a BiLSTM: hk = BiLSTM(k) =
[
−→
h k;
←−
h k], and the decoder generate each word ac-

cording to the probability:

p(wt|wt−11 ,hk; θ) = g(V lst) (5a)
st = F att(wt−1, st−1, ct) (5b)

ct =

r∑

j=1

αtjh
k
j (5c)

αtj =
exp(a(st−1, hkj ))∑r
p=1 exp(a(st−1, h

k
p))

(5d)

Agreement experiment Cases Agreement
Researcher vs. Researcher 150 83%
Turkers vs. Researcher 150 78%
Classifier vs. Turkers 3980 69%
Always happyEnding 3980 58%

Table 1: Annotation agreement for labeling story ending
valence. Labels are happyEnding, sadEnding, or cannotTell.
The automatic classifier trained on 3980 turker annotated sto-
ries achieved much better results than the majority baseline
on 5-fold cross-validation.

Method PPL
uncontrolled model 24.63
Storyline controlled 18.36

Table 2: Perplexities on the ROCstories development data.
When storylines are given, the controlled models achieve
lower perplexity than the uncontrolled one.

g() again denotes the softmax function, and V l

denotes parameters that perform a linear transfor-
mation. F att() in Equation 5b denotes the com-
putations of an LSTM-cell with attention mech-
anism, where the context vector ct is computed
by an weighted summation of the storyline words
vectors as in Equation 5c, and the weights are
computed from some alignment function a() as in
Equation 5d.

3 Experimental Setup

We conduct experiments on the ROCstories
dataset (Mostafazadeh et al., 2016), which consists
of 98,162 five-line stories for training, and 1871
stories each for the development and test sets. We
treat the first four sentences of each story as the
body and the last sentence as the ending. We build
analyzers to annotate the ending valence and the
storyline for every story, and train the two con-
trolled generators with 98,162 annotated stories.

3.1 Ending Valence Annotation

We conduct a three-stage data collection proce-
dure to gather ending valence annotations and
train a classifier to analyze the whole corpora. We
classify all the stories into happyEnding, sadEnd-
ing, or cannotTell. Table 1 summarizes the results.

In the first stage, two researchers annotate 150
stories to gauge the feasibility of the task. It is
nontrivial, as the agreement between the two re-
searchers is only 83%, mainly because of the can-
notTell case2. The second stage collects larger-

2The inter-annotator agreement is 95% if we exclude the
instances that at least one person chose cannotTell.

45



Methods
Ending Valence Storyline

Faithfulness Coherence Faithfulness Coherence
avg score % win avg score % win avg score % win avg score % win

Retrieve Human 3.20 19.6 2.89 16.4 3.47 42.4 3.44 17.6
Uncontrolled 3.26 27.8 3.54 41.8 2.82 20.8 3.54 29.2

Controlled-Generated 3.44 52.6 3.37 41.8 3.08 36.8 3.74 53.2

Table 3: Human evaluation for the ending valence (left) and storyline (right) controlled generation. Scores range in [1,5].
Three stories (one from each method) are grouped together so that people can give comparative scores. Faithfulness survey
asks people to rate whether the generated stories reflect the given control factors. Coherence asks people to rate the coherence
of the stories without considering the control factors. % win measures how often the generated result by one method is rated
higher than others, excluding the instances that tie on the highest score.

scale annotations from AMT. We gather 3980 an-
notated stories with the turker-researcher agree-
ment at 78%. A classifier as described in Sec-
tion 2.1 is then trained to analyze the whole ROC-
stories corpora. Using 5-fold cross-validation, we
estimate the accuracy of the classifier to be 69%3,
which, while not terribly impressive, is an 11%
improvement over the majority baseline (hap-
pyEnding). Considering the low inter-annotator
agreement on this problem, we consider this a de-
cent analyzer.

4 Experimental Results

We compare the controlled generation under our
proposed framework with the uncontrolled gener-
ation. We design the experiments to answer the
following research questions:

1. How does the controlled generation frame-
work affect the generation quantitatively?

2. Does the proposed framework enables con-
trols to the stories while maintaining the co-
herence of the stories?

To answer the former question, we design auto-
matic evaluations that measure the perplexity of
the models given appropriate and inappropriate
controls. For the latter question, we design human
evaluations to compare the generated stories from
controlled and uncontrolled versions in terms of
the document-level coherence and the faithfulness
to the control factors.

4.1 Automatic Evaluation
The advantages of the automatic evaluation is that
it can be conducted at scale and gives panoramic
views of the systems. We compute the perplex-
ities of different models on the ROCstories de-
velopment dataset. Table 2 shows the results for

3We included the cannotTell cases and conducted a 3-
class classification.

the storyline experiments. With the additional sto-
ryline information, it is easier for the generation
model to guess what will happen next in a story,
thus yield lower perplexities. We conduct the same
experiments for ending valence controlled genera-
tion and observe the same. However, since ending
valence is only one bit of information, the perplex-
ity difference is only 0.8.

4.2 Human Evaluation

We conduct a human evaluation with 1000 story
groups for each setting. Each group consists of
stories from: (1) the uncontrolled LSTM gener-
ation model, (2) controlled generation with our
framework, and (3) a contrastive method which re-
trieves and re-ranks existing sentences in the train-
ing data. Users are asked to rate the three stories
on a 1-5 scale with respect to faithfulness (whether
stories reflect the control factor), and coherence.
All the evaluations are conducted on Amazon Me-
chanic Turk. We compute the average score and
percentage win of each method. Table 3 summa-
rizes the results.

Ending Valence For the ending valence control,
we supply each system with the first 4 sentences
from ROCStories test set and an ending valence
randomly assigned by a human. The systems gen-
erate endings4. We only let the systems generate
happyEnding or sadEnding stories, with the ratio
around 1:1. Faithfulness is defined as whether the
generated stories reflect the given ending valence.

The contrastive method retrieves existing happy
or sad endings from the training data instead of
generating new sentences. Specifically, we gather
all the stories that are annotated with happyEnd-
ings from the 3980 annotated stories in one set,
and all the sadEndings in another set. When the
given ending valence is happyEnding, the sys-

4The uncontrolled LSTM generation model has no way to
take the ending valence input.

46



Story Body Setting Ending
sarah had been dreaming of
visiting europe for years. she
had finally saved enough for
the trip. she landed in
spain and traveled east across
the continent. she did n’t
like how different everything
was.

Human sarah decided she preferred home over europe.

No-Valence-Control sarah was so happy that she had a great time.

Happy-Generated sarah was happy to go home.

Sad-Generated sarah was very disappointed.

Happy-Retrieved they both had a lot of fun.

Sad-Retrieved I had to go to the hospital.

samuel loved reading old sci-
ence fiction stories. he col-
lected books by hg wells and
jules verne. his favorite was
hg wells. his father gave him
a copy of the island of dr
moreau.

Human he loved it!

No-Valence-Control he was so happy to see the book.

Happy-Generated he was very happy to see the new book.

Sad-Generated he was so sad that he couldn’t find his book.

Happy-Retrieved by the end of the day, he was glad he had gone!

Sad-Retrieved bob got very sick and had to go to the hospital.

Table 4: Samples of endings generated by given the first four sentences of a story and an ending valence. The uncontrolled
version cannot take the designated ending valence into account, while both the controlled versions reflect the correct valence.

Users’ Storyline Generated Story
john angry meeting
park happy

john was a very angry person. he was angry at his job. john decided to go
to the park. john went to the park. john was very happy with his new job.

john angry meeting
park wolf

john was very angry. he had a meeting at the park. he went to the park. he
saw a wolf. john was very angry.

jennifer angry meeting
park happy

jennifer was very angry. she had a meeting at the park. jennifer was not
happy. she went to the park. jennifer was happy to be meeting her friend.

jennifer angry meeting
park wolf scared

jennifer was very angry. she had a meeting at the park. she went to the
park. she saw a wolf. jennifer was very angry.

Table 5: Samples of stories generated from user-given storyline words. The generated stories usually contains the words in
the storyline. Changing one word in the storyline can result in another creative story.

tem scores all the sentences from the happenEnd-
ing set by combining it with the given 4 sen-
tences, and using a trained uncontrolled gener-
ation model to compute the likelihood. This
chooses the most coherent happyEnding for the
given story. Similarly for the sadEnding sto-
ries. Table 3 shows that the proposed analyze-
to-generate framework (“Controlled-Generated”)
achieves the highest faithfulness score while re-
taining similar coherence as the uncontrolled one.

Storyline For storyline control, we supply each
system with 5 words as a storyline. The systems
generate stories accordingly. Storyline words are
extracted from the ROCstories test set. The uncon-
trolled generation model cannot take this input; it
generates random stories. Faithfulness is defined
as whether the generated stories follow the given
storyline.

The contrastive method retrieves human writ-

ten sentences in the training data to compose sto-
ries. Specifically, it follows the given storyline
words order to retrieve sentences from the training
data. The trained uncontrolled generation model
scores each sentence based on existing previous
sentences and choose the highest scoring sentence
for each word in the storyline. If a word in the sto-
ryline has never appeared in the training data, we
simply skip it.

As shown in Table 3, the contrastive method
achieves the highest faithfulness, probably be-
cause it guarantees the words in the storyline ap-
pear in the stories while the other systems cannot.
However, the coherence of the contrastive method
is lowest, because it is constrained by the exist-
ing sentences in the training data. Although an
uncontrolled generation model is employed to en-
courage document-level coherence, the available
choices are restricted. Our method achieves the

47



best coherence and higher faithfulness score than
the uncontrolled version.

4.3 Generation Samples
Table 4 shows two examples of the ending valence
controlled generation. The uncontrolled model
“No-Valence-Control” can generate coherent end-
ings; However, it cannot alter the ending valence.
On the other hand, the two controlled models can
generate different endings based on different end-
ing valence. The contrastive retrieval method, re-
stricted by the existing happyEnding and sadEnd-
ing in the training data, obtains endings that are
not coherent with the whole story.

Table 5 demonstrates some examples from the
storyline controlled generation. The storyline
words are user supplied. We can see that this pro-
vides fun interactions: changing one word in the
storyline can result in a creative new story.

5 Conclusion

We proposed an analyze-to-generate framework
that enables controllable story generation. The
framework is generally applicable for many con-
trol factors. In this paper, two instantiations of
the framework are explored to control the end-
ing valence and the storyline of stories. Exper-
iments show that our framework enables human
controls while achieving better coherence than an
uncontrolled generation models. In the future,
we will explore other control factors and better
controllable generation models to adding the con-
trol factors into the generated stories. The cur-
rent analyze-to-generate framework is done in a
pipeline fashion. We also plan to explore the joint
training of the analyzer and the generator to im-
prove the quality of both.

Acknowledgements

We thank the anonymous reviewers for the use-
ful comments and the suggestion for the right ter-
minology of “ending valence”. This work is sup-
ported by Contract W911NF-15-1-0543 with the
US Defense Advanced Research Projects Agency
(DARPA).

References
Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Ben-

gio. 2014. Neural machine translation by jointly
learning to align and translate. arXiv preprint
arXiv:1409.0473 .

Selmer Bringsjord and David Ferrucci. 1999. Artificial
intelligence and literary creativity: Inside the mind
of brutus, a storytelling machine. Psychology Press.

Marc Cavazza, David Pizzi, Fred Charles, Thurid Vogt,
and Elisabeth André. 2009. Emotional input for
character-based interactive storytelling. In Proceed-
ings of The 8th International Conference on Au-
tonomous Agents and Multiagent Systems-Volume
1. International Foundation for Autonomous Agents
and Multiagent Systems, pages 313–320.

Xi Chen, Yan Duan, Rein Houthooft, John Schulman,
Ilya Sutskever, and Pieter Abbeel. 2016. Infogan:
Interpretable representation learning by information
maximizing generative adversarial nets. In Proceed-
ings of Advances in Neural Information Processing
Systems.

Giovanna Egidi and Richard J Gerrig. 2009. How va-
lence affects language processing: Negativity bias
and mood congruence in narrative comprehension.
Memory & cognition 37(5):547–555.

Jessica Ficler and Yoav Goldberg. 2017. Controlling
linguistic style aspects in neural language genera-
tion. In Proceedings of the Workshop on Stylistic
Variation. pages 94–104.

Zhenxin Fu, Xiaoye Tan, Nanyun Peng, Dongyan
Zhao, and Rui Yan. 2017. Style transfer in text:
Exploration and evaluation. In Proceedings of The
32nd AAAI Conference on Artificial Intelligence.

Pablo Gervas, Belen Diaz-Agudo, Federico Peinado,
and Raquel Hervas. 2005. Story plot genera-
tion based on CBR. Knowledge-Based Systems
18(4):235–242.

Marjan Ghazvininejad, Xing Shi, Jay Priyadarshi, and
Kevin Knight. 2017. Hafez: an interactive poetry
generation system. Proceedings of 55th Annual
Meeting of the Association for Computational Lin-
guistics, System Demonstrations pages 43–48.

Sepp Hochreiter and Jurgen Schmidhuber. 1997.
Long short-term memory. Neural computation
9(8):1735–1780.

Zhiting Hu, Zichao Yang, Xiaodan Liang, Ruslan
Salakhutdinov, and Eric P Xing. 2017. Toward con-
trolled generation of text. In Proceedings of the In-
ternational Conference on Machine Learning. pages
1587–1596.

Ryan Kiros, Yukun Zhu, Ruslan Salakhutdinov,
Richard Zemel, Raquel Urtasun, Antonio Torralba,
and Sanja Fidler. 2015. Skip-thought vectors. In
Proceedings of Advances in neural information pro-
cessing systems.

Guillaume Klein, Yoon Kim, Yuntian Deng, Jean
Senellart, and Alexander M. Rush. 2017. Open-
NMT: Open-source toolkit for neural machine trans-
lation. In ACL.

48



Guillaume Lample, Neil Zeghidour, Nicolas
Usunier, Antoine Bordes, Ludovic Denoyer,
and Marc’Aurelio Ranzato. 2017. Fader networks:
Manipulating images by sliding attributes. In Pro-
ceedings of 31st Conference on Neural Information
Processing Systems.

Michael Lebowitz. 1987. Planning stories. In Proceed-
ings of the cognitive science society, Hillsdale.

Boyang Li, Stephen Lee-Urban, George Johnston, and
Mark Riedl. 2013. Story generation with crowd-
sourced plot graphs. In Proceedings of The 28th
AAAI Conference on Artificial Intelligence.

Lara Martin, Prithviraj Ammanabrolu, William Han-
cock, Shruti Singh, Brent Harrison, and Mark Riedl.
2017. Event representations for automated story
generation with deep neural nets. In Proceedings
of the Thirty-Second AAAI Conference on Artificial
Intelligence.

James Meehan. 1977. TALE-SPIN, an interactive pro-
gram that writes stories. In Proceedings of The In-
ternational Joint Conferences on Artificial Intelli-
gence Organization.

Nasrin Mostafazadeh, Nathanael Chambers, Xiaodong
He, Devi Parikh, Dhruv Batra, Lucy Vanderwende,
Pushmeet Kohli, and James Allen. 2016. A cor-
pus and cloze evaluation for deeper understanding
of commonsense stories. In Proceedings of the Con-
ference of the North American Chapter of the Asso-
ciation for Computational Linguistics: Human Lan-
guage Technologies.

Rafael Perez and Mike Sharples. 2001. MEXICA: A
computer model of a cognitive account of creative
writing. Journal of Experimental & Theoretical Ar-
tificial Intelligence 13(2):119–139.

Mark Riedl and Robert Young. 2010. Narrative plan-
ning: Balancing plot and character. Journal of Arti-
ficial Intelligence Research 39:217–268.

Melissa Roemmele, Sosuke Kobayashi, Naoya Inoue,
and Andrew M Gordon. 2017. An RNN-based bi-
nary classifier for the story cloze test. LSDSem 2017
page 74.

Stuart Rose, Dave Engel, Nick Cramer, and Wendy
Cowley. 2010. Automatic keyword extraction from
individual documents. Text Mining: Applications
and Theory pages 1–20.

Tianxiao Shen, Tao Lei, Regina Barzilay, and Tommi
Jaakkola. 2017. Style transfer from non-parallel text
by cross-alignment. In Proceedings of Advances in
neural information processing systems.

N Siddharth, Brooks Paige, Van de Meent, Alban Des-
maison, Frank Wood, Noah D Goodman, Pushmeet
Kohli, Philip HS Torr, et al. 2017. Learning disen-
tangled representations with semi-supervised deep
generative models. In Proceedings of Advances in
Neural Information Processing Systems.

Ilya Sutskever, Oriol Vinyals, and Quoc Le. 2014. Se-
quence to sequence learning with neural networks.
In Proceedings of Advances in neural information
processing systems.

Reid Swanson and Andrew Gordon. 2012. Say any-
thing: Using textual case-based reasoning to enable
open-domain interactive storytelling. ACM Trans-
actions on Interactive Intelligent Systems 2(3):16.

Duyu Tang, Bing Qin, and Ting Liu. 2015. Document
modeling with gated recurrent neural network for
sentiment classification. In Proceedings of the Con-
ference on Empirical Methods in Natural Language
Processing. pages 1422–1432.

Scott R. Turner. 1993. Minstrel: A Computer Model of
Creativity and Storytelling. Ph.D. thesis, Los Ange-
les, CA, USA. UMI Order no. GAX93-19933.

Di Wang, Nebojsa Jojic, Chris Brockett, and Eric Ny-
berg. 2017. Steering output style and topic in neu-
ral response generation. In Proceedings of the Con-
ference on Empirical Methods in Natural Language
Processing. pages 2130–2140.

Rui Yan. 2016. i, poet: Automatic poetry composition
through recurrent neural networks with iterative pol-
ishing schema. In Proceedings of The International
Joint Conferences on Artificial Intelligence Organi-
zation. pages 2238–2244.

49


