



















































Generating Patient Problem Lists from the ShARe Corpus using SNOMED CT/SNOMED CT CORE Problem List


Proceedings of the 2014 Workshop on Biomedical Natural Language Processing (BioNLP 2014), pages 54–58,
Baltimore, Maryland USA, June 26-27 2014. c©2014 Association for Computational Linguistics

Generating Patient Problem Lists from the ShARe Corpus using
SNOMED CT/SNOMED CT CORE Problem List

Danielle Mowery
Janyce Wiebe

University of Pittsburgh
Pittsburgh, PA
dlm31@pitt.edu

wiebe@cs.pitt.edu

Mindy Ross
University of California

San Diego
La Jolla, CA
mkross@ucsd.edu

Sumithra Velupillai
Stockholm University

Stockholm, SE
sumithra@dsv.su.se

Stephane Meystre
Wendy W Chapman

University of Utah
Salt Lake City, UT
stephane.meystre,

wendy.chapman@utah.edu

Abstract
An up-to-date problem list is useful for
assessing a patient’s current clinical sta-
tus. Natural language processing can help
maintain an accurate problem list. For in-
stance, a patient problem list from a clin-
ical document can be derived from indi-
vidual problem mentions within the clin-
ical document once these mentions are
mapped to a standard vocabulary. In
order to develop and evaluate accurate
document-level inference engines for this
task, a patient problem list could be gen-
erated using a standard vocabulary. Ad-
equate coverage by standard vocabularies
is important for supporting a clear rep-
resentation of the patient problem con-
cepts described in the texts and for interop-
erability between clinical systems within
and outside the care facilities. In this
pilot study, we report the reliability of
domain expert generation of a patient
problem list from a variety of clinical
texts and evaluate the coverage of anno-
tated patient problems against SNOMED
CT and SNOMED Clinical Observation
Recording and Encoding (CORE) Prob-
lem List. Across report types, we learned
that patient problems can be annotated
with agreement ranging from 77.1% to
89.6% F1-score and mapped to the CORE
with moderate coverage ranging from
45%-67% of patient problems.

1 Introduction

In the late 1960’s, Lawrence Weed published
about the importance of problem-oriented medi-
cal records and the utilization of a problem list
to facilitate care provider’s clinical reasoning by
reducing the cognitive burden of tracking cur-
rent, active problems from past, inactive problems

from the patient health record (Weed, 1970). Al-
though electronic health records (EHR) can help
achieve better documentation of problem-specific
information, in most cases, the problem list is
manually created and updated by care providers.
Thus, the problem list can be out-of-date con-
taining resolved problems or missing new prob-
lems. Providing care providers with problem list
update suggestions generated from clinical docu-
ments can improve the completeness and timeli-
ness of the problem list (Meystre and Haug, 2008).

In recent years, national incentive and standard
programs have endorsed the use of problem lists
in the EHR for tracking patient diagnoses over
time. For example, as part of the Electronic Health
Record Incentive Program, the Center for Medi-
care and Medicaid Services defined demonstra-
tion of Meaningful Use of adopted health infor-
mation technology in the Core Measure 3 objec-
tive as “maintaining an up-to-date problem list of
current and active diagnoses in addition to histor-
ical diagnoses relevant to the patients care” (Cen-
ter for Medicare and Medicaid Services, 2013).
More recently, the Systematized Nomenclature of
Medicine Clinical Terms (SNOMED CT) has be-
come the standard vocabulary for representing and
documenting patient problems within the clinical
record. Since 2008, this list is iteratively refined
four times each year to produce a subset of gen-
eralizable clinical problems called the SNOMED
CT CORE Problem List. This CORE list repre-
sents the most frequent problem terms and con-
cepts across eight major healthcare institutions in
the United States and is designed to support in-
teroperability between regional healthcare institu-
tions (National Library of Medicine, 2009).

In practice, there are several methodologies ap-
plied to generate a patient problem list from clin-
ical text. Problem lists can be generated from
coded diagnoses such as the International Statis-
tical Classification of Disease (ICD-9 codes) or

54



concept labels such as Unified Medical Language
System concept unique identifiers (UMLS CUIs).
For example, Meystre and Haug (2005) defined 80
of the most frequent problem concepts from coded
diagnoses for cardiac patients. This list was gen-
erated by a physician and later validated by two
physicians independently. Coverage of coded pa-
tient problems were evaluated against the ICD-9-
CM vocabulary. Solti et al. (2008) extended the
work of Meystre and Haug (2005) by not limit-
ing the types of patient problems from any list
or vocabulary to generate the patient problem list.
They observed 154 unique problem concepts in
their reference standard. Although both studies
demonstrate valid methods for developing a pa-
tient problem list reference standard, neither study
leverages a standard vocabulary designed specifi-
cally for generating problem lists.

The goals of this study are 1) determine how
reliably two domain experts can generate a pa-
tient problem list leveraging SNOMED CT from
a variety of clinical texts and 2) assess the cover-
age of annotated patient problems from this corpus
against the CORE Problem List.

2 Methods

In this IRB-approved study, we obtained the
Shared Annotated Resource (ShARe) corpus
originally generated from the Beth Israel Dea-
coness Medical Center (Elhadad et al., un-
der review) and stored in the Multiparameter
Intelligent Monitoring in Intensive Care, ver-
sion 2.5 (MIMIC II) database (Saeed et al.,
2002). This corpus consists of discharge sum-
maries (DS), radiology (RAD), electrocardiogram
(ECG), and echocardiogram (ECHO) reports from
the Intensive Care Unit (ICU). The ShARe cor-
pus was selected because it 1) contains a variety of
clinical text sources, 2) links to additional patient
structured data that can be leveraged for further
system development and evaluation, and 3) has en-
coded individual problem mentions with semantic
annotations within each clinical document that can
be leveraged to develop and test document-level
inference engines. We elected to study ICU pa-
tients because they represent a sensitive cohort that
requires up-to-date summaries of their clinical sta-
tus for providing timely and effective care.

2.1 Annotation Study

For this annotation study, two annotators - a physi-
cian and nurse - were provided independent train-
ing to annotate clinically relevant problems e.g.,
signs, symptoms, diseases, and disorders, at the
document-level for 20 reports. The annotators
were given feedback based on errors over two it-
erations. For each patient problem in the remain-
ing set, the physician was instructed to review the
full text, span the a problem mention, and map the
problem to a CUI from SNOMED-CT using the
extensible Human Oracle Suite of Tools (eHOST)
annotation tool (South et al., 2012). If a CUI did
not exist in the vocabulary for the problem, the
physician was instructed to assign a “CUI-less” la-
bel. Finally, the physician then assigned one of
five possible status labels - Active, Inactive, Re-
solved, Proposed, and Other - based on our pre-
vious study (Mowery et al., 2013) to the men-
tion representing its last status change at the con-
clusion of the care encounter. Patient problems
were not annotated as Negated since patient prob-
lem concepts are assumed absent at a document-
level (Meystre and Haug, 2005). If the patient
was healthy, the physician assigned “Healthy - no
problems” to the text. To reduce the cognitive bur-
den of annotation and create a more robust refer-
ence standard, these annotations were then pro-
vided to a nurse for review. The nurse was in-
structed to add missing, modify existing, or delete
spurious patient problems based on the guidelines.

We assessed how reliably annotators agreed
with each other’s patient problem lists using inter-
annotator agreement (IAA) at the document-level.
We evaluated IAA in two ways: 1) by problem
CUI and 2) by problem CUI and status. Since
the number of problems not annotated (i.e., true
negatives (TN)) are very large, we calculated F1-
score as a surrogate for kappa (Hripcsak and Roth-
schild, 2005). F1-score is the harmonic mean of
recall and precision, calculated from true posi-
tive, false positive, and false negative annotations,
which were defined as follows:

true positive (TP) = the physician and nurse prob-
lem annotation was assigned the same CUI
(and status)

false positive (FP) = the physician problem anno-
tation (and status) did not exist among the
nurse problem annotations

55



false negative (FN) = the nurse problem anno-
tation (and status) did not exist among the
physician problem annotations

Recall =
TP

(TP + FN)
(1)

Precision =
TP

(TP + FP )
(2)

F1-score =

2
(Recall ∗ Precision)
(Recall + Precision)

(3)

We sampled 50% of the corpus and determined
the most common errors. These errors with
examples were programmatically adjudicated
with the following solutions:

Spurious problems: procedures
solution: exclude non-problems via guidelines

Problem specificity: CUI specificity differences
solution: select most general CUIs

Conflicting status: negated vs. resolved
solution: select second reviewer’s status

CUI/CUI-less: C0031039 vs. CUI-less
solution: select CUI since clinically useful

We split the dataset into about two-thirds train-
ing and one-third test for each report type. The re-
maining data analysis was performed on the train-
ing set.

2.2 Coverage Study
We characterized the composition of the reference
standard patient problem lists against two stan-
dard vocabularies SNOMED-CT and SNOMED-
CT CORE Problem List. We evaluated the cover-
age of patient problems against the SNOMED CT
CORE Problem List since the list was developed
to support encoding clinical observations such as
findings, diseases, and disorders for generating pa-
tient summaries like problem lists. We evaluated
the coverage of patient problems from the corpus
against the SNOMED-CT January 2012 Release
which leverages the UMLS version 2011AB. We
assessed recall (Eq 1), defining a TP as a patient
problem CUI occurring in the vocabulary and a

FN as a patient problem CUI not occurring in the
vocabulary.

3 Results

We report the results of our annotation study on
the full set and vocabulary coverage study on the
training set.

3.1 Annotation Study

The full dataset is comprised of 298 clinical doc-
uments - 136 (45.6%) DS, 54 (18.1%) ECHO,
54 (18.1%) RAD, and 54 (18.1%) ECG. Seventy-
four percent (221) of the corpus was annotated by
both annotators. Table 1 shows agreement overall
and by report, matching problem CUI and prob-
lem CUI with status. Inter-annotator agreement
for problem with status was slightly lower for all
report types with the largest agreement drop for
DS at 15% (11.6 points).

Report Type CUI CUI + Status
DS 77.1 65.5
ECHO 83.9 82.8
RAD 84.7 82.8
ECG 89.6 84.8

Table 1: Document-level IAA by report type for problem
(CUI) and problem with status (CUI + status)

We report the most common errors by frequency
in Table 2. By report type, the most common er-
rors for ECHO, RAD, and ECG were CUI/CUI-
less, and DS was Spurious Concepts.

Errors DS ECHO RAD ECG
SP 423 (42%) 26 (23%) 30 (35%) 8 (18%)
PS 139 (14%) 31 (27%) 8 (9%) 0 (0%)
CS 318 (32%) 9 (8%) 8 (9%) 14 (32%)
CC 110 (11%) 34 (30%) 37 (44%) 22 (50%)
Other 6 (>1%) 14 (13%) 2 (2%) 0 (0%)

Table 2: Error types by frequency - Spurious Problems (SP),
Problem Specificity (PS), Conflicting status (CS), CUI/CUI-
less (CC)

3.2 Coverage Study

In the training set, there were 203 clinical docu-
ments - 93 DS, 37 ECHO, 38 RAD, and 35 ECG.
The average number of problems were 22±10 DS,
10±4 ECHO, 6±2 RAD, and 4±1 ECG. There
are 5843 total current problems in SNOMED-CT
CORE Problem List. We observed a range of
unique SNOMED-CT problem concept frequen-
cies: 776 DS, 63 ECHO, 113 RAD, and 36 ECG

56



by report type. The prevalence of covered prob-
lem concepts by CORE is 461 (59%) DS, 36
(57%) ECHO, 71 (63%) RAD, and 16 (44%)
ECG. In Table 3, we report coverage of patient
problems for each vocabulary. No reports were
annotated as “Healthy - no problems”. All reports
have SNOMED CT coverage of problem mentions
above 80%. After mapping problem mentions to
CORE, we observed coverage drops for all report
types, 24 to 36 points.

Report Patient Annotated with Mapped to
Type Problems SNOMED CT CORE
DS 2000 1813 (91%) 1335 (67%)
ECHO 349 300 (86%) 173 (50%)
RAD 190 156 (82%) 110 (58%)
ECG 95 77(81%) 43 (45%)

Table 3: Patient problem coverage by SNOMED-CT and
SNOMED-CT CORE

4 Discussion

In this feasibility study, we evaluated how reliably
two domain experts can generate a patient problem
list and assessed the coverage of annotated patient
problems against two standard clinical vocabular-
ies.

4.1 Annotation Study

Overall, we demonstrated that problems can be re-
liably annotated with moderate to high agreement
between domain experts (Table 1). For DS, agree-
ment scores were lowest and dropped most when
considering the problem status in the match crite-
ria. The most prevalent disagreement for DS was
Spurious problems (Table 2). Spurious problems
included additional events (e.g., C2939181: Mo-
tor vehicle accident), procedures (e.g., C0199470:
Mechanical ventilation), and modes of administra-
tion (e.g., C0041281: Tube feeding of patient) that
were outside our patient problem list inclusion cri-
teria. Some pertinent findings were also missed.
These findings are not surprising given on average
more problems occur in DS and the length of DS
documents are much longer than other document
types. Indeed, annotators are more likely to miss
a problem as the number of patient problems in-
crease.

Also, status differences can be attributed to mul-
tiple status change descriptions using expressions
of time e.g., “cough improved then” and modal-
ity “rule out pneumonia”, which are harder to

track and interpret over a longer document. The
most prevalent disagreements for all other doc-
ument types were CUI/CUI-less in which iden-
tifying a CUI representative of a clinical obser-
vation proved more difficult. An example of
Other disagreement was a sidedness mismatch
or redundant patient problem annotation. For
example, C0344911: Left ventricular dilatation
vs. C0344893: Right ventricular dilatation or
C0032285: Pneumonia was recorded twice.

4.2 Coverage Study

We observed that DS and RAD reports have higher
counts and coverage of unique patient problem
concepts. We suspect this might be because other
document types like ECG reports are more likely
to have laboratory observations, which may be
less prevalent findings in CORE. Across document
types, coverage of patient problems in the corpus
by SNOMED CT were high ranging from 81%
to 91% (Table 3). However, coverage of patient
problems by CORE dropped to moderate cover-
ages ranging from 45% to 67%. This suggests that
the CORE Problem List is more restrictive and
may not be as useful for capturing patient prob-
lems from these document types. A similar report
of moderate problem coverage with a more restric-
tive concept list was also reported by Meystre and
Haug (2005).

5 Limitations

Our study has limitations. We did not apply a tra-
ditional adjudication review between domain ex-
perts. In addition, we selected the ShARe corpus
from an ICU database in which vocabulary cover-
age of patient problems could be very different for
other domains and specialties.

6 Conclusion

Based on this feasibility study, we conclude that
we can generate a reliable patient problem list
reference standard for the ShARe corpus and
SNOMED CT provides better coverage of patient
problems than the CORE Problem List. In fu-
ture work, we plan to evaluate from each ShARe
report type, how well these patient problem lists
can be derived and visualized from the individ-
ual disease/disorder problem mentions leveraging
temporality and modality attributes using natu-
ral language processing and machine learning ap-
proaches.

57



Acknowledgments

This work was partially funded by NLM
(5T15LM007059 and 1R01LM010964), ShARe
(R01GM090187), Swedish Research Council
(350-2012-6658), and Swedish Fulbright Com-
mission.

References
Center for Medicare and Medicaid Services. 2013.

EHR Incentive Programs-Maintain Problem
List. http://www.cms.gov/Regulations-and-
Guidance/Legislation/EHRIncentivePrograms/
downloads/3 Maintain Problem ListEP.pdf.

Noemie Elhadad, Wendy Chapman, Tim OGorman,
Martha Palmer, and Guergana. Under Review
Savova. under review. The ShARe Schema for
the Syntactic and Semantic Annotation of Clinical
Texts.

George Hripcsak and Adam S. Rothschild. 2005.
Agreement, the F-measure, and Reliability in In-
formation Retrieval. J Am Med Inform Assoc,
12(3):296–298.

Stephane Meystre and Peter Haug. 2005. Automation
of a Problem List using Natural Language Process-
ing. BMC Medical Informatics and Decision Mak-
ing, 5(30).

Stephane M. Meystre and Peter J. Haug. 2008. Ran-
domized Controlled Trial of an Automated Problem
List with Improved Sensitivity. International Jour-
nal of Medical Informatics, 77:602–12.

Danielle L. Mowery, Pamela W. Jordan, Janyce M.
Wiebe, Henk Harkema, John Dowling, and
Wendy W. Chapman. 2013. Semantic Annotation
of Clinical Events for Generating a Problem List. In
AMIA Annu Symp Proc, pages 1032–1041.

National Library of Medicine. 2009. The
CORE Problem List Subset of SNOMED-
CT. Unified Medical Language System 2011.
http://www.nlm.nih.gov/research/umls/SNOMED-
CT/core subset.html.

Mohammed Saeed, C. Lieu, G. Raber, and Roger G.
Mark. 2002. MIMIC II: a massive temporal ICU
patient database to support research in intelligent pa-
tient monitoring. Comput Cardiol, 29.

Imre Solti, Barry Aaronson, Grant Fletcher, Magdolna
Solti, John H. Gennari, Melissa Cooper, and Thomas
Payne. 2008. Building an Automated Problem List
based on Natural Language Processing: Lessons
Learned in the Early Phase of Development. pages
687–691.

Brett R. South, Shuying Shen, Jianwei Leng, Tyler B.
Forbush, Scott L. DuVall, and Wendy W. Chapman.

2012. A prototype tool set to support machine-
assisted annotation. In Proceedings of the 2012
Workshop on Biomedical Natural Language Pro-
cessing, BioNLP ’12, pages 130–139. Association
for Computational Linguistics.

Lawrence Weed. 1970. Medical Records, Med-
ical Education and Patient Care: The Problem-
Oriented Record as a Basic Tool. Medical Pub-
lishers: Press of Case Western Reserve University,
Cleveland: Year Book.

58


