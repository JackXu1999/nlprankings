



















































Modularization of Regular Growth Automata


Proceedings of the 9th International Workshop on Finite State Methods and Natural Language Processing, pages 3–11,
Blois (France), July 12-15, 2011. c©2011 Association for Computational Linguistics

Modularization of Regular Growth Automata

Christian Wurm
Fakultät für Linguistik und Literaturwissenschaften, Universität Bielefeld

CITEC Bielefeld
cwurm@uni-bielefeld.de

Abstract

Regular growth automata form a class of
infinite machines, in which all local com-
putations are performed by finite state au-
tomata. We present some results which are
relevant to application in practice; apart
from runtime, the most important one is
modularization, that is, abstraction over
subroutines. We use the new techniques
to prove some results on substitution.

1 Introduction

We recently introduced the concept of regu-
lar growth automata (RGA, we refer the
reader to (2) for background, examples and dis-
cussion of related work; for some related work
see (2),(2)). Whereas previously we focused on
mathematical results and provided some formal
examples, we will here focus on adapting the
concept to make it useful for linguistic applica-
tion. This will be accomplished via modulariza-
tion of automata; we study how large automata
can be reasonably split up into smaller and sim-
pler ones.

Organization is as follows: in the next section,
we present definitions and give an overview of
the most important formal features of regular
growth automata. The third section is about
treating regular growth automata in a modular
fashion, while preserving determinism, which is
a very favorable property of RGA. In our view,
this is the key to making the concept applica-
ble on a broader scale. As we argue in the last
section, this might also provide new insights in

linguistic theory, as it gives us a new perspective
on syntactic structures.

2 Formal Concepts

2.1 Definitions

In a sense, regular growth automata are infinite
extensions of finite state automata. In FSA,
states are atomic symbols like letters;1 in reg-
ular growth automata, the state set Q ⊆ Ω∗ is
a stringset formed out of a finite alphabet. The
most important property of infinite state ma-
chines in general is the computability of state
transitions. We will require that transition rela-
tions on Q be regular.2 The subsequent defini-
tions follow (2).3

Definition 1 Put Σ⊥ := Σ ∪ {⊥}, for ⊥/∈ Σ.
The convolution of a tuple of strings 〈~x1, ~x2〉 ∈
(Σ∗)2, written as ⊗〈~x1, ~x2〉 of length max({|~xi| :
i ∈ {1, 2}}) is defined as follows: the kth com-
ponent of ⊗〈~x1, ~x2〉 is 〈σ1, σ2〉, where σi is the
k-th letter of ~xi provided that k ≤ |~xi|, and ⊥
otherwise.
The convolution of a relation R ⊆ (Σ∗)2,
written ⊗R, is the the set {⊗〈~x1, ~x2〉: 〈~x1, ~x2〉 ∈
R}.
Definition 2 A relation R ∈ (Σ∗)2 is called
(synchronous)4 regular, if there is a finite state
automaton over (Σ⊥)2 recognizing ⊗R.

1For this reason, they have also been called the inter-
nal alphabet in early work on the topic.

2For mathematical background see (2).
3We confine ourselves to binary relations, but the con-

cept generalizes without complication.
4There is some confusing usage, as some authors use

3



Regular relations as defined here form a
proper subset of the relations defined by finite
state transducers in general. We use them for
three reasons: firstly, they are closed under
Boolean operations (contrary to transducer de-
fined relations), secondly, they allow at most a
constant growth of strings, a property whose im-
portance will become clear later on, and thirdly,
we have not met any case where the additional
power provided by transducer relations would
have been of any use. We call the class of trans-
ducers which computes regular relations syn-
chronous transducers. In the sequel we will
restrict ourselves to this subclass, so we will omit
the adjective, when there is no danger of confu-
sion.

Definition 3 We define a regular
growth automaton (RGA) as a tuple
〈�,Q, F, δ,Σ, OpΣ,Ω〉, where Ω is a finite set of
symbols, the state alphabet, Q ⊆ Ω∗ is the state
set, � ∈ Q is the initial state, F ⊆ Ω∗ is the set
of accepting states, δ ⊆ Q×Σ×Q the transition
relation, Σ a finite input alphabet; OpΣ is a set
of synchronous transducers, with one opx for
each x ∈ Σ, where Ω is the input and output
alphabet for all opx ∈ OpΣ. In the sequel, we
identify the opx ∈ OpΣ with the relations they
induce on Ω∗. In addition, the following hold:

1. F is a regular set;

2. for every transducer opσ, ((qi, σ), qj) ∈ δ
exactly if qj ∈ opσ(qi);

3. Q is recursively defined as the smallest set
such that (i) � is in Q, (ii) if α is in Q,
then for all σ ∈ Σ, opσ(α) is also in Q.

We call the transducers in OpΣ letter opera-
tors.

A regular growth automaton is deterministic
exactly if the letter operators represent (par-
tial) functions on Ω∗, the RGA is total ex-
actly if the letter operators represent total func-
tions/relations. We can easily totalize RGAs
by totalizing the letter operators, sending all

the term regular for “finite state computable”, while oth-
ers use the term in our sense. We will usually simply say
regular, and mean it in the sense defined here.

previously undefined inputs to a new absorb-
ing state.5 We strongly conjecture that de-
terministic and non-deterministic automata are
non-equivalent, but still lack a conclusive proof
(we will see some evidence later on).6 We will
write RGA or regular growth automaton if we
make statements valid for both cases; we will
write DGA for deterministic, NGA for non-
deterministic automata. We will focus on the
deterministic case, which has many favourable
properties, as we will see.

Note that there is some redundancy in our
presentation, for the letter operators serve to
specify the transition function and state set. We
keep this redundancy for ease of presentation: if
we fix the letter operators, Q and δ are fixed,
as well as Σ and Ω. The only additional infor-
mation we need is the set of accepting states F .
When we construct automata, we will thus con-
struct letter operators and an accepting state
set, nothing more. We write δ̂ for the transi-
tion function generalized from letters to string
in the obvious fashion. In case we are handling
with several automata, we mark automata and
their components with a subscript as in δRGA
to ensure unique reference. Then we have the
following:

Definition 4 A regular growth automa-
ton RGA recognizes a language L, if
L = {~x : δ̂RGA(�, ~x) ∈ FRGA}.

We write L(RA) for the language a given au-
tomaton recognizes. There are two ways to con-
ceive of a regular growth automaton: the first
one is to think of it as an automaton with an
infinite state set which is already specified; al-
ternatively we can think of it as a machine which
constructs its own state set only when given an
input.

Definition 5 Two strings ~x, ~y are Nerode
equivalent in a language L, in symbols ~x ∼L ~y,
if for all ~z, ~x~z ∈ L iff ~y~z ∈ L.

5We call an absorbing a state which is not accepting
and from which all transitions point at the state itself.

6The classical determinization algorithm via power-
set construction does not preserve the regularity of the
automaton.

4



From a language theoretic perspective note
that, as an ordinary finite state machine, it
computes Nerode-equivalences. From a practi-
cal perspective, the most remarkable property
is that the automaton constructs its state set in
runtime; assuming that its input has some up-
per bound, it will be nothing but a finite state
machine. But it is a finite state machine which
is constructed on the fly, that is, given an in-
put, the necessary states and only the necessary
states are generated.

In this paper, we will add another property to
the one of being dynamic (in the above sense),
namely the one of being modular : we will show
that in regular growth automata, we can ab-
stract over certain subsystems of language, fac-
torizing them into modules. This remedies a
notorious weakness of classical finite state ap-
proaches: the lack of abstraction ((2)).

2.2 Recognizing Power

We call the class of languages recognized by
some DGA (NGA) LDGA (LNGA). As we have
shown previously, LDGA forms a Boolean Al-
gebra. It contains languages which are not
context-free, not mildly context sensitive and
not semilinear, however there is strong evidence
that it does not contain all context-free lan-
guages (see below, and contrary to LNGA, for
there is an algorithm which converts any CFG
into a NGA). LDGA gives thus a true cut across
the Chomsky hierarchy, which we judge to be
possibly relevant for formal linguistics.

2.3 Runtime

Definition 6 A finite state transducer T is
functional, if for any given input T computes
at most one output. It is deterministic, if
δT (Q× Ω) is a (partial) function.

As is well-known, the latter implies the for-
mer, but the former does not imply the latter;
for DGA, we do require our transducers to be
functional, but not to be deterministic.

Lemma 7 A synchronous functional trans-
ducer T computes the output for a given input
of length n in O(n) steps.

Proof. This is obvious, if the transducer is
deterministic.7 If a synchronous transducer is
non-deterministic, then there is a constant up-
per bound to non-determinism: the states qi ∈
δ̂(q0, ~w,~v) are less than k for all ~w,~v ∈ Σ∗, that
is, |δ̂(q0, ~w,~v)| ≤ k. This is obviously due to the
fact that T has finitely many states. We show
that also the set of pairs of states and output
strings 〈~v, qi〉 ∈ δ̂(q0, ~w) is constantly bounded
for any ~w, that is, for all ~w, |δ̂(q0, ~w)| ≤ k.
This is because the set of states we can go to
is bounded, and for a given input, if we go into
one state, we need to write at most one output.
For if we have two output words for a prefix
by going to a single state, then we can discard
both: if it were possible to reach an accepting
state from the current one, then the transducer
would no longer be functional, contrary to our
assumptions. Therefore, for each letter we read,
we have to compute at most k transitions and
write an output on at most k possible output
words. The output word for an input of length
n is thus computed in ≤ kn + l steps (l is the
length of the constantly bounded suffix we might
add). a
Theorem 8 For any string ~w, |~w| = n, a given
DGA accepts or rejects ~x in O(n2) steps.

Proof. As our transducers are synchronous
and functional, the length of the states8 grows
(at most) proportionally to the size of the input
string. Put l := max({n : n = |opσ(~x)| − |~x| :
σ ∈ Σ}). For lemma 7, the transducers compute
the transitions for states of length m in ≤ km
steps; as the DGA is deterministic, the letter
operators are functional. To calculate the state
~x, for ~x = δ̂DGA(�, ~w) and |~w| = n, we thus
have to perform at most k × l + k × 2l + ... +
k × nl = ∑ni=1 k(i× l) steps. Checking whether
the state is accepting takes at most l × n steps;
so accepting or rejecting takes O(

∑n
i=1 kil+ ln)

steps. This is proportional to n2. a
Note that this is strong evidence for the con-

jecture that LDGA does not contain the context
free languages, for it is widely believed that the
lowest possible bound for parsing CFLs is higher

7A proof of this can be found in (2).
8Recall that RGA-states are strings!

5



than quadratic (see (2)).
For non-deterministic automata, things are

much worse: we have to assume that in the
worst case runtime is exponential in terms of
input strings: for in general, a word of length n
might lead us to kn different states, and so we
might have to perform exponentially many com-
putations. This is a very good reason to take
care that a DGA remains deterministic under
various transformations.

3 Automaton Construction and
Regular Growth Subroutines

3.1 Automaton Construction

For application, the most urgent question is how
to construct a regular growth automaton given
a language. We have no way of defining directly
the state set: we can only define it indirectly via
the letter operators, and so we have to be aware
of the consequences in the infinite of the interac-
tion of our finite state transducers. Construct-
ing a large set of transducers which interact in
the desired way can be a tremendous task.

In this section, we propose a procedure to fa-
cilitate the construction of regular growth au-
tomata: we will factorize languages into cer-
tain sublanguages, and adress these by interact-
ing subroutines. For example, in natural lan-
guage, this means that for a given category, say,
a NP[nom], we construct a subroutine, which
covers all its possible contents regardless of the
contexts in which it occurs; and we construct a
matrix automaton in which it is embedded and
which provides its possible contexts, as main
clause, complement clause etc. This is not a
trivial task, given that we do not have an a pri-
ori notion of constituency in our approach! We
thus have two problems: care for the distribu-
tion of an NP[nom], and care for its internal
structure. However, each of these seem to be
much more feasible problems than both at once.

We will show how to construct transducers
out of subroutines, such that the letter operators
compute the union of all subroutines they occur
in, but where it depends on the context (i.e., the
current state) which subroutine is called. We
will do this first for simple and then for recur-

sive substitution. If we want to preserve deter-
minism when merging subroutines in the general
case, there is however an important condition:
there must not be a locally unbounded ambi-
guity on which subroutine is called at a certain
point.

3.2 Regular Growth Subroutines:
Equivalence

Definition 9 A (deterministic) regular
growth subroutine is a (D)GA,

1. with a regular set of initial states I ⊆ Ω∗
instead of �, and

2. for all opσ ∈ OpΣ, all qi ∈ I and ~w ∈ Ω∗,
opσ(qi ~w) = qi~v for some ~v ∈ Ω∗; that is,
operators only write and change suffixes to
initial states.

A (deterministic) regular growth subroutine
(RGS/DGS) is thus a generalization of a DGA;
a DGA is a DGS with the empty string as initial
state. A note on the second condition: if sub-
routines only write suffixes to their initial states,
this facilitates their global interaction when we
embed them into larger automata. We use pre-
fixes in I to encode global states, and the suffix
to encode the state of the subroutine. Impor-
tantly, we do not add any additional recognizing
power to our automata by generalizing them to
subroutines in the above way:

Theorem 10 For any language L, L =
L(DGS) for some DGS exactly if there is a
DGA such that L = L(DGA).

Note that for the nondeterministic case, this
result would be trivial, but it is not for the de-
terministic one. The idea of the proof is quite
simple: we use a partition of I to simulate it on
a finite tuple. As the proof itself is quite lengthy,
we present it in the appendix. The concept of
subroutines is very useful from the following per-
spective: if we merge several DGA into into a
single automaton, the resulting automaton will
recognize simply the union of their languages.
If we merge several DGS with a DGA, this will
not be necessarily the case: the initial state sets
provide us with a tool to control when a DGS

6



is called. We can thus convert DGA into DGS,
to be able to merge them in a controlled fash-
ion; the equivalence is important for it means
we can conceive and check DGA and DGS inde-
pendently and equivalently. Recall the the defi-
nition of ∼L, the Nerode-equivalence in L:
Definition 11 We say a subroutine S is em-
bedded within a RGA, if

1. IS ∩QRGA 6= ∅ and

2. FS ∩QRGA 6= ∅

3. if δ̂RGA(q0, ~u) ∈ IS, then for all ~v, ~w ∈
L(S), ~u~v ∼L(RGA) ~u~w,

that is, S must be reachable, it must lead back
into the main automaton, and for all prefixes
which call the subroutine, the stringset the sub-
routine computes forms an equivalence class
with respect to the language of the matrix ma-
chine.

Note that with this definition, we do not say
anything on the internal structure of the au-
tomata or the operators, nor of the indepen-
dent existence of a subroutine in the above sense.
Embedded subroutines can be implicit, that is,
they are computed, but at no point written out
as such.

Definition 12 For a subroutine S embedded
within an RGA, if q ∈ IS, we say that q calls
S. If there is a q ∈ Q such that opa(q) ⊆ IS, we
say that a calls S. A characteristic prefix
~a of a subroutine S, written as ~a ∈ cp(S), is a
word for which holds: for all q ∈ Q, if δ̂(q,~a) is
defined,9 then δ̂(q,~a) calls S.

A subroutine S is called characteristic if for
all q ∈ I, we have q = δ̂(qj ,~a) for some ~a ∈
cp(S), where there is a constant k such that for
all ~a ∈ cp(S), |~a| ≤ k. A (sub)language L
is characteristic if L = {~x~y : ~x ∈ cp(S) and
~y ∈ L(S)}; S (and L) is strictly characteristic
if in addition a word in L(S) is not the prefix of
another word in L(S).

9And not an absorbing state, we should add, but we
leave the issue of total versus partial automata aside; it
does not pose any serious problems.

An embedded subroutine is thus characteristic
if we always know from a prefix when it is called;
it is strictly characteristic if in addition we know
when it ends.

Definition 13 Two subroutines S1 and S2 (two
languages L1 and L2) are called distinct, if they
are characteristic and have disjoint sets of char-
acteristic prefixes; they are strictly distinct, if
in addition they are strictly characteristic.

It is crucial to see that the distinction of sub-
routines is orthogonal to the distinction between
letter operators: if subroutines embedded in a
DGA share an alphabet, we have different sub-
routines within one letter operator. We write
opSiσ for the function of a subroutine Si com-
puted by one operator. opSiσ (~x) thus means that
opσ computes its function in Si on a substring ~x.

ǫstart
S1 S2

S3

LS1,LS1

LS2,LS2

LS3,LS3

Ω,Ω Fig.1: Blueprint of a letter opera-tor omputing three subroutines; itomputes the identity, until somemarker in the input ativates asubroutine. Note that these mark-ers are not harateristi pre�xes;rather, the harateristi pre�x-es put the marker into the state-string.
3.3 Example I: Simple Substitution

As an example of how to model constituent-like
dependencies, we show how to use subroutines
for simple letter substitution. Note that insert-
ing subroutines is quite trivial; the problem is to
use subroutines and preserve determinism of the
automaton. The treatment will not be very de-
tailed; in particular, we will not look “inside”
the transducers and spell out the operations.
This is due to the fact that firstly, we do not re-
ally need to - which is one of the greatest merits
of our approach; and secondly, because trans-
ducers quickly result to be large and hard to
read. We encourage the skeptical reader to check
that all conditions we state below can indeed be
checked by synchronous finite state transducers.
In the sequel, we will make an abuse of ontology
for the sake of readibility and construct regu-
lar expressions over languages. Recall that dis-
tinct languages have decompositions into a set

7



of bounded prefixes, which call a unique subrou-
tine, and the language of the subroutine itself.

Proposition 14 Let Σ be an alphabet, and let
{Lσ : σ ∈ Σ} be a set of pairwise distinct lan-
guages, such that Lσ = cp(DGSσ) · L(DGSσ)
for all σ ∈ Σ. Now let i : Σ → {Lσ : σ ∈ Σ}
be a one-to-one mapping, where i(σ) = Lσ. For
every LM ⊆ Σ∗, if there is a DGAM such that
L(DGAM ) = LM , then there is a DGAi(M) such
that L(DGAi(M)) = i[LM ].

Proof. First we change the state alphabet
ΩM of DGAM to make it disjoint from all other
Ωx, x 6= M . The new operators in OpDGAi(M)Σ
are constructed as follows: first we take care of
the characteristic prefixes; for simplicity we as-
sume they are single letters. If they are not, they
are constantly bounded, and so we have some fi-
nite amount of non-determinism; this can always
be determinized with a tuple construction.10 To
make the treatment more compact, we leave this
construction to the reader.

The characteristic prefixes are the letters
which compute the interaction of the routines;
we add a letter Sσ to Ωi(M) for each σ ∈ Σ,
where for all α 6= i(M), Sσ /∈ Ωα, and define the
operators as follows:

1. For all a ∈ cp(Sσ) : σ ∈ Σ, opi(M)a is defined
on all and only on states in (�|(Ω∗M ((Fσ) :
σ ∈ Σ)∗Sx((Fσ) : σ ∈ Σ)),

2. If a ∈ cp(Sσ), then opi(M)a (�) =
(opMσ (�))Sσ(opSσa (�)),

3. If a ∈ cp(Sσ), then opi(M)a (~c~xSτ~z) =
opMσ (~c)~x~zSσopSσa (�), provided it is defined,
and where ~c is the longest prefix of the state
in ΩM .

That is, operators for characteristic prefixes
(i) only operate on states which consist of a pre-
fix pM ∈ Ω∗M , followed by a (possibly empty)
sequence of accepting states Fσ of the subrou-
tines. (ii) They operate on pM as the letters

10Alternatively, we can conceive of characteristic pre-
fix strings as chunks: as regular relations are closed un-
der composition, we can only let the composition com-
pute the desired function; technically, this amounts to
the same, namely a tuple construction.

to for which they are substituted, (iii) they put
or change and postpone a distinguished marker
Sσ at the end of the string, which marks the
beginning of the operating scope of a substitu-
tion language, and (iv) then start computing the
subroutine.

All other letter operators simply compute
their previous function as subroutines, where
each subroutine Sσ computes Lσ, and where the
initial state set for Sσ is Ω∗Sσ. Before they read
the marker, they simply compute the identity,
so:

(1) opi(M)a (~xSσ~y) = ~xSσopSσa (~y),
Note that our characteristic prefixes make

sure there is only one marker Sσ in any state-
string. The set of final states Fi(L) is now sim-
ply the set of all sequences of only accepting
states for all languages/subroutines, with possi-
bly a marker Sτ :
(2) Fi(L) := FM · ((Fτ ) : τ ∈ T )∗Sx((Fτ ) :

τ ∈ T ). a
Note that in this case, we need the condition of
distinctness of sublanguages, to make sure we
know when to call a subroutine. When we treat
recursive substitution, we will see that we need
strict distinctness, to also know when we can
stop a subroutine.

3.4 Example II: Recursive Substitution

We are surely not only interested in simple sub-
stitution, but also recursive substitution, that
is, we want to be able to call new subroutines
during the execution of subroutines. As an ex-
ample, we will prove the following:

Proposition 15 Let P ⊆ Σ; let (Lρ) : ρ ∈ P
be a set of strictly distinct languages, such that
Lρ = cp(DGSρ) · L(DGSρ) and Lρ ⊆ Σ∗ for all
ρ ∈ P . Define i : P → (Lρ) : ρ ∈ P as a one-to-
one mapping, where i(ρ) = Lρ. Let LM ⊆ Σ∗ be
a language such that LM = L(DGAM ). Then
there is a DGAP which recognizes LP , which is
defined as follows:

1. If ~x ∈ LM , then ~x ∈ LP .
2. If ~xρ~y ∈ LP , then ~xi(ρ)~y ∈ LP .

8



Proof. We construct DGAP . First we
change the state alphabet ΩM such that is dis-
junct from all other Ωx, x 6= M . The new opera-
tors OpDGAPΣ are constructed as follows: first we
take care of the characteristic prefixes; for sim-
plicity we assume again they are single letters,
and use letters Sρ as before.11

1. If a ∈ cp(Sρ), then opPa (~x) =
opMρ (~x)SρopSρa (�), if ~x ∈ Ω∗M .

2. If a ∈ cp(Sρ), then opPa (~xSσ~y~z) =
~xSσ(opSσρ (~y))~zSρ(opSρa (�)),
where ~z ∈ (�|(Sρ : ρ ∈ P )Ω∗P ), and ~y is
the rightmost substring matching this con-
ditions which does not contain any symbol
Sρ and is not in any Fρ′ .

Though notation becomes a bit opaque at
this point, the concept is quite simple: we sim-
ply make sure that in our DGAP states every
subroutine has a clearly distinguished operating
scope, where the most deeply embedded subrou-
tine is written as the rightmost one in the state
string. We thus translate a structure of the form
[1[2[3]]] into a form [1][2][3]. This is works be-
cause sublanguages are strictly distinct, so we
know exactly when a subroutine is completed,
and we can ignore all substrings in some Fρ. The
other letters operate as follows:

1. opPτ (~xSσ~y~z) = ~xSσ(opSστ (~y))~z, where ~z ∈
(�|(Sρ : ρ ∈ P )Ω∗P ), and ~y is the right-
most substring which satisfies this condi-
tions which does not contain a Sρ and is
not in Fρ′ ;

2. opPτ (~x~y) = (op
M
τ (~x))~y), where ~x ∈ Ω∗M and

~y ∈ ((SρFρ) : ρ ∈ P )∗

The set of accepting states is

(3) FP := FM · ((SρFρ) : ρ ∈ P )∗. a
Note that, while we do not require that any of
the languages involved be context-free, this sub-
stitution is in a precise sense “context-free”,12

11There is a Sρ for each ρ ∈ P and Sρ /∈ Ωα, for all
α 6= P .

12More p precisely: deterministic context-free.

as the distribution of the substituted language
equals the distribution of a single letter. How-
ever, this is of course only one particular case of
substitution; recall that the sets of initial states
and final states of a subroutine are regular sets;
so we can take any substitution point which can
be defined by a regular set of states. We see that
we can decouple the complexity of the substitu-
tion/upcalling of subroutines from the complex-
ity of the routines itself. We thus can reasonably
encode a difference between local complexity (in
the sense of subroutine) and a global complexity
(in the sense of substitution conditions), and we
can restrict them independently.

4 Outlook: Regular Growth
Automata and Linguistic Theory

In the last section we saw how the factoriza-
tion was useful to find a (quite) simple solution
to complex problems of substitution. The main
goal was to show that factorization of automata
into subautomata makes them easier to handle
in application. We want to underline that the
reason we think modular treatment is much eas-
ier for linguistic application is not only the struc-
ture of regular growth automata, but in partic-
ular the structure of natural languages. This
is partly due to the fact that we often13 find
some kind of constituent structure in natural
languages; but there is more to that: we often
find very complex local structures, whose scope
however is often quite limited. If we look for
example at systems of clitic pronouns, we find a
very unpredictable behaviour, and a large num-
ber of complex rules which possibly even have to
take into account segmental and suprasegmental
phonology. If we look at constituents which have
a (possibly) very complex internal structure, as
clausal complements, we find that their distri-
bution is highly regular and follows very simple
rules.14

This observation is by no means new: for ex-
ample, the well-known work of John Hawkins
is mostly concerned with these and similar ob-

13Some people say: always.
14This asymmetry gets even more striking if we also

consider morphology as a part of syntactic structure.

9



servations. We will not go into detail here, but
we think that principles as early immediate con-
stituents, minimize domains etc. (see (2),(2))
can be roughly restated, with a slightly shifted
perspective, in the following way:

(4) The more complex a constituent is, the
more regular15 is its distribution.

We think that this important observation has
had unduly little impact on linguistic theory
proper. The main reason for this seems to us
that usually we posit fully specified constituent
structures (of some kind or other) for all utter-
ances: and once we make this step, we are un-
able to partially redeem it. One could see it as
a disadvantage to regular growth automata that
they cannot be mapped onto such a constituent
structure, contrary to, for example, pushdown
automata. However, in this context there is
an equal advantage: for a pushdown automa-
ton, the notion of a subroutine does not make
much sense, for all configurations are equally
well-suited and, in fact, can be easily treated
as subroutines. For our automata this is not the
case: because the notion of a constituent is so
problematic, the notion of an embedded subrou-
tine is truly meaningful. It allows to capture the
asymmetry of local and global structure. This is
the reason we think our approach is well suited
for natural language descriptions not only from
a practical, but also a theoretical point of view:
though we give up the notion of a fully spec-
ified constituent structure, we can much more
easily treat the rich local structure of natural
languages with compact local subroutines, and
its much more austere global structure by means
of very simple interactions of routines. This in
turn might facilitate a more realistic view on the
organization of linguistic knowledge.

Acknowledgements

The author wants to thank Marcus Kracht and
Jens Michaelis for their support, and three
anonymous reviewers for their helpful com-
ments.

15Here we can read regular both in its colloquial and
in its technical meaning.

5 Appendix: Proof of Theorem 10

Proof. One direction is immediate. To see the
if-direction, we show how to convert a DGS into a
DGA. By definition, for the prefixes of states which
are in I, the output equals the input, and as we have
one output and the transducer is functional, there is
a single state we go to (this is the inverse reasoning
from lemma 7).16 Assume without loss of generality
that our letter operators are total. We write

(5) ~x ∼opσ ~y if δ̂opσ (q0, ~x) = (~x′, qi) and
δ̂opσ (q0, ~y) = (~y

′, qi) for some ~x′, ~y′.

For the above reasons, ∼opσ is an equivalence re-
lation. We form partitions I with the equivalence
classes induced by ∼opσ , for each opσ ∈ OpΣ: put
Popσ := I/ ∼opσ . As our operators are finite,
|I/ ∼opσ | ≤ k for each σ ∈ Σ. Next, by intersecting
the elements across the different partitions, we con-
struct a more fine-grained partition, which forms a
partition (though not maximal) with respect to every
∼opσ : opσ ∈ OpΣ:

(6) J := ⋂opσ∈OpΣ Popσ .17
J is a partition of I, and for each J ∈ J and for
each opσ ∈ OpΣ there is a unique state q such that
δ̂opσ (q0,~j) = (~j

′, q) for some ~j′, and all ~j ∈ J . Cru-
cially, while its elements might be of infinite cardi-
nalitiy, J itself is of finite cardinality. Put l = |J |.
The initial state of our DGA is an l-tupel, where
each πi : i ≤ l is assigned a J ∈ J via a bijection
φ(J )→M ,where M ⊂ N and for all m ∈M , m ≤ l.
We construct the new letter operators opDGAσ , which
operate on (Ωk)∗, as follows:

(7) opDGAσ (〈~x1, ~x2, ..., ~xl〉 =
〈opqiσ (~x1), opqjσ (~x2), ..., opqmσ (~xk)〉,

where opqxσ is opσ with its initial state changed to
qx; and for every πi : i ≤ l, we make sure that if
φ−1(i) = J , then for all ~j ∈ J , δ̂opσ (q0,~i) = 〈~w, qx〉
for some ~w.

We thus simulate the possibly infinite set I with a
finite set of tuples, where each projection represents a
set of strings for which all opσ go into a unique state.
The operations on projections are still synchronous
regular, and as the DGS was deterministic, the tuple
size of the DGA remains constant.

16Provided the transducer is not redundant; if it is,
then the states can be merged.

17Note that this is a somewhat sloppy notation, as we
do not intersect the partitions, but their elements.

10



Now to FDGA. Regular languages are closed under
projection and cylindrification; so we take as accept-
ing states the set of all l-cylindrifications of FDGS ,
that is, the set of all l-tuples of which one projec-
tion is in FDGS . Call this set FDGS′ . Finally, we
need to take care of the case where a prefix in I
has some influence on membership in F . Here we
use the fact that regular languages are closed un-
der prefixation: for two languages, L1, L2, we put
L1\L2 := {~v : ∃~w ∈ L1 : ~w~v ∈ L2}. It is well-known
that if L1 and L2 are regular, then so is L1\L2, the
set of strings which, given a prefix from L1, result in a
word in L2. We thus have to put FDGA := I\FDGA′ .
The resulting DGA does the job as required. a

References

Didier Caucal. Synchronization of regular automata.
In Rastislav Královic and Damian Niwinski, ed-
itors, MFCS, volume 5734 of Lecture Notes in
Computer Science, pages 2–23. Springer, 2009.

Christiane Frougny and Jacques Sakarovitch. Syn-
chronized rational relations of finite and infinite
words. Theor. Comput. Sci., 108(1):45–82, 1993.

John A. Hawkins. A Performance Theory of Order
and Constituency. Cambridge Univ. Pr., Cam-
bridge, 1994.

John A. Hawkins. Efficiency and Complexity in
Grammars. Oxford Univ. Pr., Oxford, 2004.

Lillian Lee. Fast context-free grammar parsing re-
quires fast boolean matrix multiplication. J.
ACM, 49(1):1–15, 2002.

Mehryar Mohri. Finite-state transducers in language
and speech processing. Computational Linguis-
tics, 23(2):269–311, 1997.

Sasha Rubin. Automata presenting structures: A
survey of the finite string case. Bulletin of Sym-
bolic Logic, 14(2):169–209, 2008.

Shuly Wintner. Strengths and weaknesses of finite-
state technology: a case study in morphological
grammar development. Natural Language Engi-
neering, 14(4):457–469, 2008.

Christian Wurm. Regular growth automata: Proper-
ties of a class of finitely induced infinite machines.
In Proceedings of the 12th Mathematics of Lan-
guage, Springer LNCS, 2011.

11


