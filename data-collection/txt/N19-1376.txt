




































Topic Spotting using Hierarchical Networks with Self Attention


Proceedings of NAACL-HLT 2019, pages 3755–3761
Minneapolis, Minnesota, June 2 - June 7, 2019. c©2019 Association for Computational Linguistics

3755

Topic Spotting using Hierarchical Networks with Self Attention

Pooja Chitkara1, Ashutosh Modi1,
Pravalika Avvaru1, Sepehr Janghorbani1,2, Mubbasir Kapadia1,2

1Disney Research, 2Rutgers University
pchitkar@andrew.cmu.edu

ashutosh.modi@disneyresearch.com
pavvaru@andrew.cmu.edu

sepehr.janghorbani@rutgers.edu
mubbasir.kapadia@rutgers.edu

Abstract

Success of deep learning techniques have re-
newed the interest in development of dialogue
systems. However, current systems struggle to
have consistent long term conversations with
the users and fail to build rapport. Topic spot-
ting, the task of automatically inferring the
topic of a conversation, has been shown to be
helpful in making a dialog system more en-
gaging and efficient. We propose a hierarchi-
cal model with self attention for topic spotting.
Experiments on the Switchboard corpus show
the superior performance of our model over
previously proposed techniques for topic spot-
ting and deep models for text classification.
Additionally, in contrast to offline processing
of dialog, we also analyze the performance of
our model in a more realistic setting i.e. in
an online setting where the topic is identified
in real time as the dialog progresses. Results
show that our model is able to generalize even
with limited information in the online setting.

1 Introduction
Recently, a number of commercial conversation

systems have been introduced e.g. Alexa, Google
Assistant, Siri, Cortana, etc. Most of the avail-
able systems perform well on goal-oriented con-
versations which spans over few utterances in a
dialogue. However, with longer conversations (in
open domains), existing systems struggle to re-
main consistent and tend to deviate from the cur-
rent topic during the conversation. This hinders
the establishment of long term social relationship
with the users (Dehn and Van Mulken, 2000). In
order to have coherent and engaging conversations
with humans, besides other relevant natural lan-
guage understanding (NLU) techniques (Jokinen
and McTear, 2009), a system, while responding,
should take into account the topic of the current
conversation i.e. Topic Spotting.

Topic spotting has been shown to be important

in commercial dialog systems (Bost et al., 2013;
Jokinen et al., 2002) directly dealing with the cus-
tomers. Topical information is useful for speech
recognition systems (Iyer and Ostendorf, 1999)
as well as in audio document retrieval systems
(Hazen et al., 2007; Hazen, 2011). Importance
of topic spotting can be gauged from the work
of Alexa team (Guo et al., 2018), who have pro-
posed topic based metrics for evaluating the qual-
ity of conversational bots. The authors empirically
show that topic based metrics correlate with hu-
man judgments.

Given the importance of topical information in
a dialog system, this paper proposes self attention
based hierarchical model for predicting topics in
a dialog. We evaluate our model on Switchboard
(SWBD) corpus (Godfrey et al., 1992) and show
that our model supersedes previously applied tech-
niques for topic spotting. We address the evalu-
ative limitations of the current SWBD corpus by
creating a new version of the corpus referred as
SWBD2. We hope that SWBD2 corpus would
provide a new standard for evaluating topic spot-
ting models. We also experiment with an online
setting where we examine the performance of our
topic classifier as the length of the dialog is varied
and show that our model can be used in a real time
dialog system as well.

2 Related Work
Topic spotting is the task of detecting the topic

of a dialog (Hazen et al., 2007). Topic spotting has
been an active area of research over the past few
decades both in the NLP community as well as in
the speech community. In this section we briefly
outline some of the main works in this area. For
a detailed survey of prior research in this area, the
reader is referred to Hazen (2011).

Most of the methods proposed for topic spotting
use features extracted from transcribed text as in-



3756

wk,1 wk,2

…… ……

w1,1 : w1,L0 wk,L

……
E E E

… …

a1 a2 aL

…

T<latexit sha1_base64="FjsLb5IuxN/Kp+xQz1FBQUFnWZA=">AAAB8XicbVDLSgNBEOyNrxhfUY9eBoPgKeyKoDcDXjxGyAuTJcxOepMhs7PLzKwQlvyFlxyU4NUv8De8+TdOHgdNLGgoqrrp6g4SwbVx3W8nt7G5tb2T3y3s7R8cHhWPTxo6ThXDOotFrFoB1Si4xLrhRmArUUijQGAzGN7P/OYzKs1jWTOjBP2I9iUPOaPGSk+diJqBDrPauFssuWV3DrJOvCUp3X1OJlMAqHaLX51ezNIIpWGCat323MT4GVWGM4HjQifVmFA2pH1sWypphNrP5onH5MIqPRLGypY0ZK7+nshopPUoCmznPOGqNxP/89qpCW/9jMskNSjZYlGYCmJiMjuf9LhCZsTIEsoUt1kJG1BFmbFPKtgneKsnr5PGVdlzy96jV6pcwwJ5OINzuAQPbqACD1CFOjCQ8AKv8OZoZ+JMnfdFa85ZzpzCHzgfP6AFk8I=</latexit><latexit sha1_base64="hbhZMlAjkUdy33rAl72tevAC25c=">AAAB8XicbVDLSgNBEOyJrxhfUY9eBoPgKeyKoDcDXjxGyAuTJcxOZpMhs7PLzKwQlvyFlwiKePUL/A1v/o2zmxw0saChqOqmq9uPBdfGcb5RYW19Y3OruF3a2d3bPygfHrV0lCjKmjQSker4RDPBJWsabgTrxIqR0Bes7Y9vM7/9yJTmkWyYScy8kAwlDzglxkoPvZCYkQ7SxrRfrjhVJwdeJe6CVG4+Zxme6/3yV28Q0SRk0lBBtO66Tmy8lCjDqWDTUi/RLCZ0TIasa6kkIdNemiee4jOrDHAQKVvS4Fz9PZGSUOtJ6NvOPOGyl4n/ed3EBNdeymWcGCbpfFGQCGwinJ2PB1wxasTEEkIVt1kxHRFFqLFPKtknuMsnr5LWRdV1qu69W6ldwhxFOIFTOAcXrqAGd1CHJlCQ8AQv8Io0mqE39D5vLaDFzDH8Afr4Af1ClYc=</latexit><latexit sha1_base64="hbhZMlAjkUdy33rAl72tevAC25c=">AAAB8XicbVDLSgNBEOyJrxhfUY9eBoPgKeyKoDcDXjxGyAuTJcxOZpMhs7PLzKwQlvyFlwiKePUL/A1v/o2zmxw0saChqOqmq9uPBdfGcb5RYW19Y3OruF3a2d3bPygfHrV0lCjKmjQSker4RDPBJWsabgTrxIqR0Bes7Y9vM7/9yJTmkWyYScy8kAwlDzglxkoPvZCYkQ7SxrRfrjhVJwdeJe6CVG4+Zxme6/3yV28Q0SRk0lBBtO66Tmy8lCjDqWDTUi/RLCZ0TIasa6kkIdNemiee4jOrDHAQKVvS4Fz9PZGSUOtJ6NvOPOGyl4n/ed3EBNdeymWcGCbpfFGQCGwinJ2PB1wxasTEEkIVt1kxHRFFqLFPKtknuMsnr5LWRdV1qu69W6ldwhxFOIFTOAcXrqAGd1CHJlCQ8AQv8Io0mqE39D5vLaDFzDH8Afr4Af1ClYc=</latexit><latexit sha1_base64="WBVcOY/JwJ318Rf7YS5SPbZt5Jo=">AAAB8XicbVDLSsNAFL2pr1pfVZduBovgqiQi6LLgxmWFvrANZTKdtEMnkzBzI5TQv3DjQhG3/o07/8ZpmoW2Hhg4nHMvc+4JEikMuu63U9rY3NreKe9W9vYPDo+qxycdE6ea8TaLZax7ATVcCsXbKFDyXqI5jQLJu8H0buF3n7g2IlYtnCXcj+hYiVAwilZ6HEQUJybMWvNhtebW3RxknXgFqUGB5rD6NRjFLI24QiapMX3PTdDPqEbBJJ9XBqnhCWVTOuZ9SxWNuPGzPPGcXFhlRMJY26eQ5OrvjYxGxsyiwE7mCVe9hfif108xvPUzoZIUuWLLj8JUEozJ4nwyEpozlDNLKNPCZiVsQjVlaEuq2BK81ZPXSeeq7rl178GrNa6LOspwBudwCR7cQAPuoQltYKDgGV7hzTHOi/PufCxHS06xcwp/4Hz+ANtnkPg=</latexit>
softmax

| {z }
uk

wN,1 : wN,L00

�!
h1

(1)  �h1(1)
�!
h2

(1)
 �
h2

(1) �!hL(1)
 �
hL

(1)

…

softmax

| {z }
uN

| {z }
u1 utterances

BiLSTM              Layer

utterance 
embedding

attention weights

tanh

BiLSTM
Layer �!

h1
(2) �!h2(2)

�!
hN

(2)  �hN (2)
 �
h2

(2) �h1(2)

�!
h1

(1)

…
 �
h1

(1)

�!
hL

(1)

 �
hL

(1)

BiLSTM
Layer

BiLSTM
Layer

attention 
weights

…… ……

Embedding
Layer

Embedding
Layer

BiLSTM Hidden Layers

vk,2
<latexit sha1_base64="OuGlh/uVnUWLTeHXQQWMXIY9w20=">AAAB+XicbVDLSsNAFL2pr1pfUZduBovgopSkG10W3LgRK9gHtCFMppN26GQSZiaFEvInblwo4tY/cefOT3HSdqGtBwYO59zLPXOChDOlHefLKm1sbm3vlHcre/sHh0f28UlHxakktE1iHstegBXlTNC2ZprTXiIpjgJOu8HkpvC7UyoVi8WjniXUi/BIsJARrI3k2/YgwnochNnUzya1Rp77dtWpO3OgdeIuSbVp391/A0DLtz8Hw5ikERWacKxU33US7WVYakY4zSuDVNEEkwke0b6hAkdUedk8eY4ujDJEYSzNExrN1d8bGY6UmkWBmSxyqlWvEP/z+qkOr72MiSTVVJDFoTDlSMeoqAENmaRE85khmEhmsiIyxhITbcqqmBLc1S+vk06j7jp198GtNmuwQBnO4BwuwYUraMIttKANBKbwBC/wamXWs/VmvS9GS9Zy5xT+wPr4AQu0lVo=</latexit><latexit sha1_base64="qvm+No4aw12KMy0E19kmg50dSkI=">AAAB+XicbVDLSsNAFL2pr1pfUZdugkVwUUrSjYKbghs3YgX7wDaEyWTSDp1MwsykUEL+xI0LRdz6J+5c+StO2i609cDA4Zx7uWeOnzAqlW1/GaW19Y3NrfJ2ZWd3b//APDzqyDgVmLRxzGLR85EkjHLSVlQx0ksEQZHPSNcfXxd+d0KEpDF/UNOEuBEachpSjJSWPNMcREiN/DCbeNm41shzz6zadXsGa5U4C1Jtmrd331fBY8szPwdBjNOIcIUZkrLv2IlyMyQUxYzklUEqSYLwGA1JX1OOIiLdbJY8t860ElhhLPTjypqpvzcyFEk5jXw9WeSUy14h/uf1UxVeuhnlSaoIx/NDYcosFVtFDVZABcGKTTVBWFCd1cIjJBBWuqyKLsFZ/vIq6TTqjl137p1qswZzlOEETuEcHLiAJtxAC9qAYQJP8AKvRmY8G2/G+3y0ZCx2juEPjI8fVvKWUg==</latexit><latexit sha1_base64="qvm+No4aw12KMy0E19kmg50dSkI=">AAAB+XicbVDLSsNAFL2pr1pfUZdugkVwUUrSjYKbghs3YgX7wDaEyWTSDp1MwsykUEL+xI0LRdz6J+5c+StO2i609cDA4Zx7uWeOnzAqlW1/GaW19Y3NrfJ2ZWd3b//APDzqyDgVmLRxzGLR85EkjHLSVlQx0ksEQZHPSNcfXxd+d0KEpDF/UNOEuBEachpSjJSWPNMcREiN/DCbeNm41shzz6zadXsGa5U4C1Jtmrd331fBY8szPwdBjNOIcIUZkrLv2IlyMyQUxYzklUEqSYLwGA1JX1OOIiLdbJY8t860ElhhLPTjypqpvzcyFEk5jXw9WeSUy14h/uf1UxVeuhnlSaoIx/NDYcosFVtFDVZABcGKTTVBWFCd1cIjJBBWuqyKLsFZ/vIq6TTqjl137p1qswZzlOEETuEcHLiAJtxAC9qAYQJP8AKvRmY8G2/G+3y0ZCx2juEPjI8fVvKWUg==</latexit><latexit sha1_base64="WAD4Vlp0pIegzMiB8bnwBBkRRZI=">AAAB+XicbVDLSsNAFL2pr1pfUZduBovgopSkm7osuHFZwT6gDWEynbRDJ5MwMymUkD9x40IRt/6JO//GSZuFth4YOJxzL/fMCRLOlHacb6uys7u3f1A9rB0dn5ye2ecXfRWnktAeiXkshwFWlDNBe5ppToeJpDgKOB0E8/vCHyyoVCwWT3qZUC/CU8FCRrA2km/b4wjrWRBmCz+bN1p57tt1p+msgLaJW5I6lOj69td4EpM0okITjpUauU6ivQxLzQineW2cKppgMsdTOjJU4IgqL1slz9GNUSYojKV5QqOV+nsjw5FSyygwk0VOtekV4n/eKNXhnZcxkaSaCrI+FKYc6RgVNaAJk5RovjQEE8lMVkRmWGKiTVk1U4K7+eVt0m81XafpPrr1TqOsowpXcA234EIbOvAAXegBgQU8wyu8WZn1Yr1bH+vRilXuXMIfWJ8/n06Tjg==</latexit>

sk
<latexit sha1_base64="yUKfgTFIknPb8I2zxRf6STJJB2A=">AAAB7HicbVBNS8NAEJ3Ur1q/qh69LBbBU0mkUI8FLx4rmLbQhrLZTtqlm03Y3Qgl9Dd48aCIV3+QN/+N2zYHbX0w8Hhvhpl5YSq4Nq777ZS2tnd298r7lYPDo+OT6ulZRyeZYuizRCSqF1KNgkv0DTcCe6lCGocCu+H0buF3n1BpnshHM0sxiOlY8ogzaqzk62E+nQ+rNbfuLkE2iVeQGhRoD6tfg1HCshilYYJq3ffc1AQ5VYYzgfPKINOYUjalY+xbKmmMOsiXx87JlVVGJEqULWnIUv09kdNY61kc2s6Ymole9xbif14/M9FtkHOZZgYlWy2KMkFMQhafkxFXyIyYWUKZ4vZWwiZUUWZsPhUbgrf+8ibp3NQ9t+49NGqtRhFHGS7gEq7Bgya04B7a4AMDDs/wCm+OdF6cd+dj1Vpyiplz+APn8wccmY7T</latexit><latexit sha1_base64="yUKfgTFIknPb8I2zxRf6STJJB2A=">AAAB7HicbVBNS8NAEJ3Ur1q/qh69LBbBU0mkUI8FLx4rmLbQhrLZTtqlm03Y3Qgl9Dd48aCIV3+QN/+N2zYHbX0w8Hhvhpl5YSq4Nq777ZS2tnd298r7lYPDo+OT6ulZRyeZYuizRCSqF1KNgkv0DTcCe6lCGocCu+H0buF3n1BpnshHM0sxiOlY8ogzaqzk62E+nQ+rNbfuLkE2iVeQGhRoD6tfg1HCshilYYJq3ffc1AQ5VYYzgfPKINOYUjalY+xbKmmMOsiXx87JlVVGJEqULWnIUv09kdNY61kc2s6Ymole9xbif14/M9FtkHOZZgYlWy2KMkFMQhafkxFXyIyYWUKZ4vZWwiZUUWZsPhUbgrf+8ibp3NQ9t+49NGqtRhFHGS7gEq7Bgya04B7a4AMDDs/wCm+OdF6cd+dj1Vpyiplz+APn8wccmY7T</latexit><latexit sha1_base64="yUKfgTFIknPb8I2zxRf6STJJB2A=">AAAB7HicbVBNS8NAEJ3Ur1q/qh69LBbBU0mkUI8FLx4rmLbQhrLZTtqlm03Y3Qgl9Dd48aCIV3+QN/+N2zYHbX0w8Hhvhpl5YSq4Nq777ZS2tnd298r7lYPDo+OT6ulZRyeZYuizRCSqF1KNgkv0DTcCe6lCGocCu+H0buF3n1BpnshHM0sxiOlY8ogzaqzk62E+nQ+rNbfuLkE2iVeQGhRoD6tfg1HCshilYYJq3ffc1AQ5VYYzgfPKINOYUjalY+xbKmmMOsiXx87JlVVGJEqULWnIUv09kdNY61kc2s6Ymole9xbif14/M9FtkHOZZgYlWy2KMkFMQhafkxFXyIyYWUKZ4vZWwiZUUWZsPhUbgrf+8ibp3NQ9t+49NGqtRhFHGS7gEq7Bgya04B7a4AMDDs/wCm+OdF6cd+dj1Vpyiplz+APn8wccmY7T</latexit><latexit sha1_base64="yUKfgTFIknPb8I2zxRf6STJJB2A=">AAAB7HicbVBNS8NAEJ3Ur1q/qh69LBbBU0mkUI8FLx4rmLbQhrLZTtqlm03Y3Qgl9Dd48aCIV3+QN/+N2zYHbX0w8Hhvhpl5YSq4Nq777ZS2tnd298r7lYPDo+OT6ulZRyeZYuizRCSqF1KNgkv0DTcCe6lCGocCu+H0buF3n1BpnshHM0sxiOlY8ogzaqzk62E+nQ+rNbfuLkE2iVeQGhRoD6tfg1HCshilYYJq3ffc1AQ5VYYzgfPKINOYUjalY+xbKmmMOsiXx87JlVVGJEqULWnIUv09kdNY61kc2s6Ymole9xbif14/M9FtkHOZZgYlWy2KMkFMQhafkxFXyIyYWUKZ4vZWwiZUUWZsPhUbgrf+8ibp3NQ9t+49NGqtRhFHGS7gEq7Bgya04B7a4AMDDs/wCm+OdF6cd+dj1Vpyiplz+APn8wccmY7T</latexit>

W(1)
<latexit sha1_base64="k+mab+VoRgDvO2Ue1m/xz+mrK7Y=">AAAB+XicbVDLSsNAFL2pr1pfUZduBotQNyWRQl0W3LisYB/QxjKZTtqhk0mYmRRKyJ+4caGIW//EnX/jpM1CWw8MHM65l3vm+DFnSjvOt1Xa2t7Z3SvvVw4Oj45P7NOzrooSSWiHRDySfR8rypmgHc00p/1YUhz6nPb82V3u9+ZUKhaJR72IqRfiiWABI1gbaWTbwxDrqR+kvae05l5n2ciuOnVnCbRJ3IJUoUB7ZH8NxxFJQio04VipgevE2kux1IxwmlWGiaIxJjM8oQNDBQ6p8tJl8gxdGWWMgkiaJzRaqr83UhwqtQh9M5nnVOteLv7nDRId3HopE3GiqSCrQ0HCkY5QXgMaM0mJ5gtDMJHMZEVkiiUm2pRVMSW461/eJN2buuvU3YdGtdUo6ijDBVxCDVxoQgvuoQ0dIDCHZ3iFNyu1Xqx362M1WrKKnXP4A+vzBwSckzI=</latexit><latexit sha1_base64="k+mab+VoRgDvO2Ue1m/xz+mrK7Y=">AAAB+XicbVDLSsNAFL2pr1pfUZduBotQNyWRQl0W3LisYB/QxjKZTtqhk0mYmRRKyJ+4caGIW//EnX/jpM1CWw8MHM65l3vm+DFnSjvOt1Xa2t7Z3SvvVw4Oj45P7NOzrooSSWiHRDySfR8rypmgHc00p/1YUhz6nPb82V3u9+ZUKhaJR72IqRfiiWABI1gbaWTbwxDrqR+kvae05l5n2ciuOnVnCbRJ3IJUoUB7ZH8NxxFJQio04VipgevE2kux1IxwmlWGiaIxJjM8oQNDBQ6p8tJl8gxdGWWMgkiaJzRaqr83UhwqtQh9M5nnVOteLv7nDRId3HopE3GiqSCrQ0HCkY5QXgMaM0mJ5gtDMJHMZEVkiiUm2pRVMSW461/eJN2buuvU3YdGtdUo6ijDBVxCDVxoQgvuoQ0dIDCHZ3iFNyu1Xqx362M1WrKKnXP4A+vzBwSckzI=</latexit><latexit sha1_base64="k+mab+VoRgDvO2Ue1m/xz+mrK7Y=">AAAB+XicbVDLSsNAFL2pr1pfUZduBotQNyWRQl0W3LisYB/QxjKZTtqhk0mYmRRKyJ+4caGIW//EnX/jpM1CWw8MHM65l3vm+DFnSjvOt1Xa2t7Z3SvvVw4Oj45P7NOzrooSSWiHRDySfR8rypmgHc00p/1YUhz6nPb82V3u9+ZUKhaJR72IqRfiiWABI1gbaWTbwxDrqR+kvae05l5n2ciuOnVnCbRJ3IJUoUB7ZH8NxxFJQio04VipgevE2kux1IxwmlWGiaIxJjM8oQNDBQ6p8tJl8gxdGWWMgkiaJzRaqr83UhwqtQh9M5nnVOteLv7nDRId3HopE3GiqSCrQ0HCkY5QXgMaM0mJ5gtDMJHMZEVkiiUm2pRVMSW461/eJN2buuvU3YdGtdUo6ijDBVxCDVxoQgvuoQ0dIDCHZ3iFNyu1Xqx362M1WrKKnXP4A+vzBwSckzI=</latexit><latexit sha1_base64="k+mab+VoRgDvO2Ue1m/xz+mrK7Y=">AAAB+XicbVDLSsNAFL2pr1pfUZduBotQNyWRQl0W3LisYB/QxjKZTtqhk0mYmRRKyJ+4caGIW//EnX/jpM1CWw8MHM65l3vm+DFnSjvOt1Xa2t7Z3SvvVw4Oj45P7NOzrooSSWiHRDySfR8rypmgHc00p/1YUhz6nPb82V3u9+ZUKhaJR72IqRfiiWABI1gbaWTbwxDrqR+kvae05l5n2ciuOnVnCbRJ3IJUoUB7ZH8NxxFJQio04VipgevE2kux1IxwmlWGiaIxJjM8oQNDBQ6p8tJl8gxdGWWMgkiaJzRaqr83UhwqtQh9M5nnVOteLv7nDRId3HopE3GiqSCrQ0HCkY5QXgMaM0mJ5gtDMJHMZEVkiiUm2pRVMSW461/eJN2buuvU3YdGtdUo6ijDBVxCDVxoQgvuoQ0dIDCHZ3iFNyu1Xqx362M1WrKKnXP4A+vzBwSckzI=</latexit>

W(2)
<latexit sha1_base64="thiZP403hYYWRm+zF+IHR7/Hn/U=">AAAB+XicbVDLSsNAFL2pr1pfUZduBotQNyUpBV0W3LisYB/QxjKZTtqhk0mYmRRKyJ+4caGIW//EnX/jpM1CWw8MHM65l3vm+DFnSjvOt1Xa2t7Z3SvvVw4Oj45P7NOzrooSSWiHRDySfR8rypmgHc00p/1YUhz6nPb82V3u9+ZUKhaJR72IqRfiiWABI1gbaWTbwxDrqR+kvae01rjOspFdderOEmiTuAWpQoH2yP4ajiOShFRowrFSA9eJtZdiqRnhNKsME0VjTGZ4QgeGChxS5aXL5Bm6MsoYBZE0T2i0VH9vpDhUahH6ZjLPqda9XPzPGyQ6uPVSJuJEU0FWh4KEIx2hvAY0ZpISzReGYCKZyYrIFEtMtCmrYkpw17+8SbqNuuvU3YdmtdUs6ijDBVxCDVy4gRbcQxs6QGAOz/AKb1ZqvVjv1sdqtGQVO+fwB9bnDwYjkzM=</latexit><latexit sha1_base64="thiZP403hYYWRm+zF+IHR7/Hn/U=">AAAB+XicbVDLSsNAFL2pr1pfUZduBotQNyUpBV0W3LisYB/QxjKZTtqhk0mYmRRKyJ+4caGIW//EnX/jpM1CWw8MHM65l3vm+DFnSjvOt1Xa2t7Z3SvvVw4Oj45P7NOzrooSSWiHRDySfR8rypmgHc00p/1YUhz6nPb82V3u9+ZUKhaJR72IqRfiiWABI1gbaWTbwxDrqR+kvae01rjOspFdderOEmiTuAWpQoH2yP4ajiOShFRowrFSA9eJtZdiqRnhNKsME0VjTGZ4QgeGChxS5aXL5Bm6MsoYBZE0T2i0VH9vpDhUahH6ZjLPqda9XPzPGyQ6uPVSJuJEU0FWh4KEIx2hvAY0ZpISzReGYCKZyYrIFEtMtCmrYkpw17+8SbqNuuvU3YdmtdUs6ijDBVxCDVy4gRbcQxs6QGAOz/AKb1ZqvVjv1sdqtGQVO+fwB9bnDwYjkzM=</latexit><latexit sha1_base64="thiZP403hYYWRm+zF+IHR7/Hn/U=">AAAB+XicbVDLSsNAFL2pr1pfUZduBotQNyUpBV0W3LisYB/QxjKZTtqhk0mYmRRKyJ+4caGIW//EnX/jpM1CWw8MHM65l3vm+DFnSjvOt1Xa2t7Z3SvvVw4Oj45P7NOzrooSSWiHRDySfR8rypmgHc00p/1YUhz6nPb82V3u9+ZUKhaJR72IqRfiiWABI1gbaWTbwxDrqR+kvae01rjOspFdderOEmiTuAWpQoH2yP4ajiOShFRowrFSA9eJtZdiqRnhNKsME0VjTGZ4QgeGChxS5aXL5Bm6MsoYBZE0T2i0VH9vpDhUahH6ZjLPqda9XPzPGyQ6uPVSJuJEU0FWh4KEIx2hvAY0ZpISzReGYCKZyYrIFEtMtCmrYkpw17+8SbqNuuvU3YdmtdUs6ijDBVxCDVy4gRbcQxs6QGAOz/AKb1ZqvVjv1sdqtGQVO+fwB9bnDwYjkzM=</latexit><latexit sha1_base64="thiZP403hYYWRm+zF+IHR7/Hn/U=">AAAB+XicbVDLSsNAFL2pr1pfUZduBotQNyUpBV0W3LisYB/QxjKZTtqhk0mYmRRKyJ+4caGIW//EnX/jpM1CWw8MHM65l3vm+DFnSjvOt1Xa2t7Z3SvvVw4Oj45P7NOzrooSSWiHRDySfR8rypmgHc00p/1YUhz6nPb82V3u9+ZUKhaJR72IqRfiiWABI1gbaWTbwxDrqR+kvae01rjOspFdderOEmiTuAWpQoH2yP4ajiOShFRowrFSA9eJtZdiqRnhNKsME0VjTGZ4QgeGChxS5aXL5Bm6MsoYBZE0T2i0VH9vpDhUahH6ZjLPqda9XPzPGyQ6uPVSJuJEU0FWh4KEIx2hvAY0ZpISzReGYCKZyYrIFEtMtCmrYkpw17+8SbqNuuvU3YdmtdUs6ijDBVxCDVy4gRbcQxs6QGAOz/AKb1ZqvVjv1sdqtGQVO+fwB9bnDwYjkzM=</latexit>

W(f)
<latexit sha1_base64="q/JWAI7hd1WJa5AVDnfLVsFD7V4=">AAAB+XicbVDLSsNAFL2pr1pfUZduBotQNyWRQl0W3LisYB/QxjKZTtqhk0mYmRRKyJ+4caGIW//EnX/jpM1CWw8MHM65l3vm+DFnSjvOt1Xa2t7Z3SvvVw4Oj45P7NOzrooSSWiHRDySfR8rypmgHc00p/1YUhz6nPb82V3u9+ZUKhaJR72IqRfiiWABI1gbaWTbwxDrqR+kvae0Flxn2ciuOnVnCbRJ3IJUoUB7ZH8NxxFJQio04VipgevE2kux1IxwmlWGiaIxJjM8oQNDBQ6p8tJl8gxdGWWMgkiaJzRaqr83UhwqtQh9M5nnVOteLv7nDRId3HopE3GiqSCrQ0HCkY5QXgMaM0mJ5gtDMJHMZEVkiiUm2pRVMSW461/eJN2buuvU3YdGtdUo6ijDBVxCDVxoQgvuoQ0dIDCHZ3iFNyu1Xqx362M1WrKKnXP4A+vzB1WPk2c=</latexit><latexit sha1_base64="q/JWAI7hd1WJa5AVDnfLVsFD7V4=">AAAB+XicbVDLSsNAFL2pr1pfUZduBotQNyWRQl0W3LisYB/QxjKZTtqhk0mYmRRKyJ+4caGIW//EnX/jpM1CWw8MHM65l3vm+DFnSjvOt1Xa2t7Z3SvvVw4Oj45P7NOzrooSSWiHRDySfR8rypmgHc00p/1YUhz6nPb82V3u9+ZUKhaJR72IqRfiiWABI1gbaWTbwxDrqR+kvae0Flxn2ciuOnVnCbRJ3IJUoUB7ZH8NxxFJQio04VipgevE2kux1IxwmlWGiaIxJjM8oQNDBQ6p8tJl8gxdGWWMgkiaJzRaqr83UhwqtQh9M5nnVOteLv7nDRId3HopE3GiqSCrQ0HCkY5QXgMaM0mJ5gtDMJHMZEVkiiUm2pRVMSW461/eJN2buuvU3YdGtdUo6ijDBVxCDVxoQgvuoQ0dIDCHZ3iFNyu1Xqx362M1WrKKnXP4A+vzB1WPk2c=</latexit><latexit sha1_base64="q/JWAI7hd1WJa5AVDnfLVsFD7V4=">AAAB+XicbVDLSsNAFL2pr1pfUZduBotQNyWRQl0W3LisYB/QxjKZTtqhk0mYmRRKyJ+4caGIW//EnX/jpM1CWw8MHM65l3vm+DFnSjvOt1Xa2t7Z3SvvVw4Oj45P7NOzrooSSWiHRDySfR8rypmgHc00p/1YUhz6nPb82V3u9+ZUKhaJR72IqRfiiWABI1gbaWTbwxDrqR+kvae0Flxn2ciuOnVnCbRJ3IJUoUB7ZH8NxxFJQio04VipgevE2kux1IxwmlWGiaIxJjM8oQNDBQ6p8tJl8gxdGWWMgkiaJzRaqr83UhwqtQh9M5nnVOteLv7nDRId3HopE3GiqSCrQ0HCkY5QXgMaM0mJ5gtDMJHMZEVkiiUm2pRVMSW461/eJN2buuvU3YdGtdUo6ijDBVxCDVxoQgvuoQ0dIDCHZ3iFNyu1Xqx362M1WrKKnXP4A+vzB1WPk2c=</latexit><latexit sha1_base64="q/JWAI7hd1WJa5AVDnfLVsFD7V4=">AAAB+XicbVDLSsNAFL2pr1pfUZduBotQNyWRQl0W3LisYB/QxjKZTtqhk0mYmRRKyJ+4caGIW//EnX/jpM1CWw8MHM65l3vm+DFnSjvOt1Xa2t7Z3SvvVw4Oj45P7NOzrooSSWiHRDySfR8rypmgHc00p/1YUhz6nPb82V3u9+ZUKhaJR72IqRfiiWABI1gbaWTbwxDrqR+kvae0Flxn2ciuOnVnCbRJ3IJUoUB7ZH8NxxFJQio04VipgevE2kux1IxwmlWGiaIxJjM8oQNDBQ6p8tJl8gxdGWWMgkiaJzRaqr83UhwqtQh9M5nnVOteLv7nDRId3HopE3GiqSCrQ0HCkY5QXgMaM0mJ5gtDMJHMZEVkiiUm2pRVMSW461/eJN2buuvU3YdGtdUo6ijDBVxCDVxoQgvuoQ0dIDCHZ3iFNyu1Xqx362M1WrKKnXP4A+vzB1WPk2c=</latexit>

U
tte

ra
nc

e 
En

co
de

r
Di

al
og

 E
nc

od
er

a = [a1, ..., aL]
<latexit sha1_base64="WDFg6st08nGebQNXZDOFuTRVAz4=">AAACB3icbZDLSsNAFIZPvNZ6i5edIINFcFFCIoJuhIIbFy4q2AukIU6mk3bo5MLMRCghOze+ihsXirj1Fdz5Nk4vC239YeDjP+cw5/xByplUtv1tLCwuLa+sltbK6xubW9vmzm5TJpkgtEESnoh2gCXlLKYNxRSn7VRQHAWctoLB1ajeeqBCsiS+U8OUehHuxSxkBCtt+eZhJ8KqH4Q5LtAlcrGfO0XVsqyqppvC882KbdljoXlwplCp7YfhPQDUffOr001IFtFYEY6ldB07VV6OhWKE06LcySRNMRngHnU1xjii0svHdxToWDtdFCZCv1ihsft7IseRlMMo0J2jreVsbWT+V3MzFV54OYvTTNGYTD4KM45UgkahoC4TlCg+1ICJYHpXRPpYYKJ0dGUdgjN78jw0Ty3Htpxbp1I7g4lKcABHcAIOnEMNrqEODSDwCM/wCm/Gk/FivBsfk9YFYzqzB39kfP4A4AGZcQ==</latexit><latexit sha1_base64="Oemvv3EJlsqQ0sZwivlFuGYf2lA=">AAACB3icbZDLSsNAFIYn9VbrLV52ggwWwUUJGRF0IxTcuHBRwV4gDWEynbRDJxdmJkIJ2bnxVdy4UMStr+DOt3GSdqGtPwx8/Occ5pzfTziTyra/jcrS8srqWnW9trG5tb1j7u51ZJwKQtsk5rHo+VhSziLaVkxx2ksExaHPadcfXxf17gMVksXRvZok1A3xMGIBI1hpyzOP+iFWIz/IcA6voIO9DOUNy7Iamm5z1zPrtmWXgouAZlBvHgSlWp751R/EJA1ppAjHUjrITpSbYaEY4TSv9VNJE0zGeEgdjREOqXSz8o4cnmhnAINY6BcpWLq/JzIcSjkJfd1ZbC3na4X5X81JVXDpZixKUkUjMv0oSDlUMSxCgQMmKFF8ogETwfSukIywwETp6Go6BDR/8iJ0zixkW+gO1ZvnYKoqOATH4BQgcAGa4Aa0QBsQ8AiewSt4M56MF+Pd+Ji2VozZzD74I+PzB4D7mqk=</latexit><latexit sha1_base64="Oemvv3EJlsqQ0sZwivlFuGYf2lA=">AAACB3icbZDLSsNAFIYn9VbrLV52ggwWwUUJGRF0IxTcuHBRwV4gDWEynbRDJxdmJkIJ2bnxVdy4UMStr+DOt3GSdqGtPwx8/Occ5pzfTziTyra/jcrS8srqWnW9trG5tb1j7u51ZJwKQtsk5rHo+VhSziLaVkxx2ksExaHPadcfXxf17gMVksXRvZok1A3xMGIBI1hpyzOP+iFWIz/IcA6voIO9DOUNy7Iamm5z1zPrtmWXgouAZlBvHgSlWp751R/EJA1ppAjHUjrITpSbYaEY4TSv9VNJE0zGeEgdjREOqXSz8o4cnmhnAINY6BcpWLq/JzIcSjkJfd1ZbC3na4X5X81JVXDpZixKUkUjMv0oSDlUMSxCgQMmKFF8ogETwfSukIywwETp6Go6BDR/8iJ0zixkW+gO1ZvnYKoqOATH4BQgcAGa4Aa0QBsQ8AiewSt4M56MF+Pd+Ji2VozZzD74I+PzB4D7mqk=</latexit><latexit sha1_base64="q5ty6+uBkHIUIpmqXQeFlC2nJCg=">AAACB3icbZDLSsNAFIYnXmu9RV0KMlgEFyVkRNCNUHDjwkUFe4E2hMl00g6dTMLMRCghOze+ihsXirj1Fdz5Nk7aLLT1h4GP/5zDnPMHCWdKu+63tbS8srq2Xtmobm5t7+zae/ttFaeS0BaJeSy7AVaUM0FbmmlOu4mkOAo47QTj66LeeaBSsVjc60lCvQgPBQsZwdpYvn3Uj7AeBWGGc3gFe9jPUF53HKdu6Db3fLvmOu5UcBFQCTVQqunbX/1BTNKICk04VqqH3ER7GZaaEU7zaj9VNMFkjIe0Z1DgiCovm96RwxPjDGAYS/OEhlP390SGI6UmUWA6i63VfK0w/6v1Uh1eehkTSaqpILOPwpRDHcMiFDhgkhLNJwYwkczsCskIS0y0ia5qQkDzJy9C+8xBroPuUK1xXsZRAYfgGJwCBC5AA9yAJmgBAh7BM3gFb9aT9WK9Wx+z1iWrnDkAf2R9/gDZdZfx</latexit>

Figure 1: Model Architecture

put to a classifier (typically Naı̈ve Bayes or SVM ).
Extracted features include: Bag of Words (BoW),
TF-IDF (Sparck Jones, 1972; Schütze et al., 2008),
n-grams, and word co-occurrences (Hazen, 2011;
Myers et al., 2000). Some approaches (in addi-
tion to word co-occurrences features) incorporate
background world knowledge using Wikipedia
(Gupta and Ratinov, 2007). In our work, we do
not explicitly extract the features but learn these
during training. Moreover, unlike previous ap-
proaches, we explicitly model the dependencies
between utterances via self attention mechanism
and hierarchical structure.

Topic spotting has been explored in depth in the
speech processing community (see for example,
Wright et al. (1996); Kuhn et al. (1997); Nöth
et al. (1997); Theunissen (2002)). Researchers in
this community have attempted to predict the topic
directly from the audio signals using phoneme
based features. However, the performance of word
based models supersedes those of audio models
(Hazen et al., 2007).

Recently, there has been lot of work in deep
learning community for text classification (Kalch-
brenner et al., 2014; Zhang et al., 2015; Lai
et al., 2015; Lin et al., 2015; Tang et al., 2015).
These deep learning models use either RNN-
LSTM based neural networks (Hochreiter and
Schmidhuber, 1997) or CNN based neural net-
works (Kim, 2014) for learning representation of
words/sentences. We follow similar approach for
topic spotting. Our model is related to the Hier-
archical Attention Network (HN-ATT) model pro-

posed by Yang et al. (2016) for document clas-
sification. HN-ATT models the document hierar-
chically by composing words (with weights deter-
mined by first level of attention mechanism) to get
sentence representations and then combines the
sentence representations with help of second level
attention to get document representation which is
then used for classification.

The aim of this paper is not to improve text
classification but to improve topic spotting. Topic
spotting and text classification differ in various as-
pects. We are among the first to show the use
of hierarchical self attention (HN-SA) model for
topic spotting. It is natural to consider applying
text classification techniques for topic spotting.
However, as we empirically show in this paper,
text classification techniques do not perform well
in this setting. Moreover, for the dialog corpus
simple BoW approaches perform better than more
recently proposed HN-ATT model (Yang et al.,
2016).

3 Hierarchical Model with Self Attention
We propose a hierarchical model with self at-

tention (HN-SA) for topic spotting. We are given
a topic label for each dialog and we want to learn
a model mapping from space of dialogues to the
space of topic labels. We learn a prediction model
by minimizing Negative Log Likelihood (NLL)
of the data.

3.1 Model Architecture
We propose a hierarchical architecture as shown

in Figure 1. An utterance encoder takes each



3757

utterance in the dialog and outputs the corre-
sponding utterance representation. A dialog en-
coder processes the utterance representations to
give a compact vector representation for the dia-
log which is used to predict the topic of the dialog.
Utterance Encoder: Each utterance in the dia-
log is processed sequentially using single layer Bi-
directional Long Short Term Memory (BiLSTM)
(Dyer et al., 2015) network and self-attention
mechanism (Vaswani et al., 2017) to get the utter-
ance representation. In particular, given an utter-
ance with one-hot encoding for the tokens, uk =
{wk,1,wk,2, ....,wk,L}, each token is mapped to
a vector vk,i = Ewk,i ; i = 1, 2, ...L using pre-
trained embeddings (matrix E).

Utterance representation (sk = aTH(1)) is the
weighted sum of the forward and backward di-
rection concatenated hidden states at each step
of the BiLSTM (H(1) = [h(1)1 , ....,h

(1)
L ]

T where

h
(1)
i = [

−→
hi

(1)
:
←−
hi

(1)
] = BiLSTM(vk,i) ). The

weights of the combination (a = softmax(h(2)a ))
are determined using self-attention mechanism
proposed by Vaswani et al. (2017) by measur-
ing the similarity between the concatenated hid-
den states (h(2)a = W

(2)
a h

(1)
a + b

(2)
a and h

(1)
a =

tanh(W(1)a H(1) + b
(1)
a )) at each step in the utter-

ance sequence. Self-attention computes the simi-
larity of a token in the context of an utterance and
thus, boosts the contribution of some keywords to
the classifier. It also mitigates the need for a sec-
ond layer of attention at a dialog level reducing
the number of parameters, reducing the confusion
of the classifier by not trying to reweigh individual
utterances and reducing the dependence on having
all utterances (full future context) for an accurate
prediction. A simple LSTM based model (HN)
and HN-ATT perform worse than the model us-
ing self attention (§5), indicating the crucial role
played by self-attention mechanism.
Dialog Encoder: Utterance embeddings (repre-
sentations) are sequentially encoded by a sec-
ond single layer BiLSTM to get the dialog

representation (h(2)k = [
−→
hk

(2)
:
←−
hk

(2)
] =

BiLSTM(sk) ; k = 1, 2, ...N ). Bidirectional
concatenated hidden state corresponding to the
last utterance (i.e. last step of BiLSTM) is used
for making a prediction via a linear layer followed
by softmax activation (p(T|D) = softmax(hD)
where hD = Wfh

(2)
N ).

# Dialogues # Topics Avg. # Utterances
SWBD SWBD2 SWBD SWBD2 SWBD SWBD2

Train 1024 877 66 42 192.27 194.33
Dev 112 49 48 33 180.52 177.02
Test 19 98 12 42 237.58 201.97

Table 1: Corpus statistics for both versions of SWBD

4 Experimental Setup
As in previous work (§2), we use Switchboard

(SWBD) (Godfrey et al., 1992) corpus for training
our model. SWBD is a corpus of human-human
conversations, created by recording (and later tran-
scribing) telephonic conversations between two
participants who were primed with a topic. Ta-
ble 1 gives the corpus statistics. Topics in SWBD
range over a variety of domains, for example, pol-
itics, health, sports, entertainment, hobbies, etc.,
making the task of topic spotting challenging.

Dialogues in the test set of the original SWBD
cover a limited number of topics (12 vs 66). The
test set is not ideal for evaluating topic spotting
system. We address this shortcoming by creating
a new split and we refer to this version of the cor-
pus as SWBD2. The new split provides opportu-
nity for more rigorous evaluation of a topic spot-
ting system. SWBD2 was created by removing in-
frequent topics (< 10 dialogues) from the corpus
and then randomly moving dialogues between the
train/development set and the test set, in order to
have instances of each topic in the test set. The
majority class baseline in SWBD2 is around 5%.

In transcribed SWBD corpus some punctuation
symbols such as #, ?, have special meanings and
non-verbal sounds have been mapped to special
symbols e.g. <Laughter>. To preserve the mean-
ings of special symbols we performed minimal
preprocessing. Dialog Corpora is different from
text classification corpora (e.g. product reviews).
If we roughly equate a dialog to a document and an
utterance to a sentence, dialogs are very long doc-
uments with short sentences. Moreover, the vo-
cabulary distribution in a dialog corpus is funda-
mentally different, e.g. presence of back-channel
words like ‘uhm’ and ‘ah’.
Model Hyper-parameters: We use GloVe em-
beddings (Pennington et al., 2014) with dimen-
sionality of 300. The embeddings are updated dur-
ing training. Each of the LSTM cell in the utter-
ance and dialog encoder uses hidden state of di-
mension 256. The weight matrices in the atten-
tion network have dimension of 128. The hyper-
parameters were found by experimenting with the



3758

Models SWBD SWBD2
BoW + Logsitic 78.95 87.76
BoW + SVM 73.68 90.82
Bigram + SVM 52.63 79.59
BoW + TF-IDF + Logistic 52.63 81.63
nGram + Logistic 52.63 78.57
nGram + TF-IDF + Logistic 57.89 87.76
Bag of Means + Logistic 78.95 87.76
Avg. Skipgram + Logistic 26.32 59.18
Doc2Vec + SVM 73.68 86.73
HN 31.58 54.08
HN-ATT (Yang et al., 2016) 73.68 85.71
CNN (Kim, 2014) 84.21 93.87
HN-SA (our model) 89.47 95.92

Table 2: Accuracy (in %) of our model and other text
classification models on both versions of SWBD.

development set. We trained the model by min-
imizing the cross-entropy loss using Adam op-
timizer (Kingma and Ba, 2014) with an initial
learning rate of 0.001. The learning rate was re-
duced by half when development set accuracy did
not change over successive epochs. Model took
around 30 epochs to train.

5 Experiments and Results
We compare the performance of our model

(Table 2) with traditional Bag of Words (BoW),
TF-IDF, and n-grams features based classifiers.
We also compare against averaged Skip-Gram
(Mikolov et al., 2013), Doc2Vec (Le and Mikolov,
2014), CNN (Kim, 2014), Hierarchical Attention
(HN-ATT) (Yang et al., 2016) and hierarchical net-
work (HN) models. HN it is similar to our model
HN-SA but without any self attention.
Analysis: As is evident from the experiments on
both the versions of SWBD, our model (HN-SA)
outperforms traditional feature based topic spot-
ting models and deep learning based document
classification models. It is interesting to see that
simple BoW and n-gram baselines are quite com-
petitive and outperform some of the deep learn-
ing based document classification model. Simi-
lar observation has also been reported by Mesnil
et al. (2014) for the task of sentiment analysis.
The task of topic spotting is arguably more chal-
lenging than document classification. In the topic
spotting task, the number of output classes (66/42
classes) is much more than those in document
classification (5/6 classes), which is done mainly
on the texts from customer reviews. Dialogues
in SWBD have on an average 200 utterances and
are much longer texts than customer reviews. Ad-
ditionally, the number of dialogues available for
training the model is significantly lesser than cus-

tomer reviews. We further investigated the per-
formance on SWBD2 by examining the confusion
matrix of the model. Figure 2 shows the heatmap
of the normalized confusion matrix of the model
on SWBD2. For most of the classes the classi-
fier is able to predict accurately. However, the
model gets confused between the classes which
are semantically close (w.r.t. terms used) to each
other, for example, the model gets confused be-
tween pragmatically similar topics e.g. HOBBIES
vs GARDENING, MOVIES vs TV PROGRAMS,
RIGHT TO PRIVACY vs DRUG TESTING.
Online Setting: In an online conversational sys-
tem, a topic spotting model is required to predict
the topic accurately and as soon as possible dur-
ing the dialog. We investigated the relationship
between dialog length (in terms of number of ut-
terances) and accuracy. This would give us an idea
about how many utterances are required to reach a
desirable level of accuracy. For this experiment,
we varied the length of the dialogues from the test
set that was available to the model for making pre-
diction. We created sub-dialogues of length start-
ing with 1/32 of the dialog length and increasing
it in multiples of 2, up to the full dialog. Figure 3
shows both the absolute accuracy and the accuracy
relative to that on the full dialog. With just a few
(3.125%) initial utterances available, the model is
already 72% confident about the topic. This may
be partly due to the fact that in a discussion, the
first few utterances explicitly talk about the topic.
However, as we have seen, since SWBD covers
many different topics which are semantically close
to each other but are assigned distinct classes, it
is equally challenging to predict the topic with the
same model. By the time the system has processed
half the dialog in SWBD2 it is already within 99%
accuracy of the full system. The experiment shows
the possibility of using the model in an online set-
ting where the model predicts the topic with high
confidence as the conversation progresses.

6 Conclusion and Future Work
In this paper we presented a hierarchical model

with self attention for topic spotting. The model
outperforms the conventional topic spotting tech-
niques as well as deep learning techniques for text
classification. We empirically show that the pro-
posed model can also be used in an online setting.
We also introduced a new version of SWBD cor-
pus: SWBD2. We hope that it will serve as the
new standard for evaluating topic spotting models.



3759

Figure 2: Normalized Confusion Matrix in form of heatmap for model predictions on SWBD2. Vertical axis is the
target class.

57.89

68.42
73.68

78.95
84.21

89.47
64.70

76.47

82.35

88.24

94.12

100.00

69.39
76.53

90.82 91.84
94.90 95.92

72.34

79.79

94.68 95.75
98.94 100.00

40.00

50.00

60.00

70.00

80.00

90.00

100.00

1/32 1/16 1/8 1/4 1/2 1

Fraction of Dialog Length

SWBD Absolute SWBD Relative SWBD2 Absolute SWBD2 Relative

Figure 3: Effect of Dialog Length on Accuracy. Plot shows both the absolute accuracy and relative accuracy (w.r.t.
full model) for different fractions of the data.

Moving forward, we would like to explore a more
realistic multi-modal topic spotting system. Such
a system should fuse two modalities: audio and
transcribed text to make topic predictions.

Acknowledgments
We would like to thank anonymous reviewers

for their insightful comments. Mubbasir Kapa-

dia has been funded in part by NSF IIS-1703883,
NSF S&AS-1723869, and DARPA SocialSim-
W911NF-17-C-0098.

References
Xavier Bost, Marc El-Beze, and Renato De Mori. 2013.

Multiple topic identification in telephone conversa-



3760

tions. In Interspeech.

Doris M Dehn and Susanne Van Mulken. 2000. The
impact of animated interface agents: a review of
empirical research. International journal of human-
computer studies, 52(1):1–22.

Chris Dyer, Miguel Ballesteros, Wang Ling, Austin
Matthews, and Noah A Smith. 2015. Transition-
based dependency parsing with stack long short-
term memory. arXiv preprint arXiv:1505.08075.

John J Godfrey, Edward C Holliman, and Jane Mc-
Daniel. 1992. Switchboard: Telephone speech cor-
pus for research and development. In ICASSP-92.

Fenfei Guo, Angeliki Metallinou, Chandra Khatri,
Anirudh Raju, Anu Venkatesh, and Ashwin Ram.
2018. Topic-based evaluation for conversational
bots. CoRR, abs/1801.03622.

Rakesh Gupta and Lev Ratinov. 2007. Topic spotting in
dialogues using knowledge transfer. In NIPS Work-
shop on Learning Problem Design.

Timothy J Hazen. 2011. Topic identification. Spoken
Language Understanding: Systems for Extracting
Semantic Information from Speech, pages 319–356.

Timothy J Hazen, Fred Richardson, and Anna Margo-
lis. 2007. Topic identification from audio recordings
using word and phone recognition lattices. In Auto-
matic Speech Recognition & Understanding, 2007.
ASRU. IEEE Workshop on, pages 659–664. IEEE.

Sepp Hochreiter and Jürgen Schmidhuber. 1997.
Long short-term memory. Neural computation,
9(8):1735–1780.

Rukmini M Iyer and Mari Ostendorf. 1999. Modeling
long distance dependence in language: Topic mix-
tures versus dynamic cache models. IEEE Transac-
tions on speech and audio processing.

Kristiina Jokinen, Antti Kerminen, Mauri Kaipainen,
Tommi Jauhiainen, Graham Wilcock, Markku Tu-
runen, Jaakko Hakulinen, Jukka Kuusisto, and
Krista Lagus. 2002. Adaptive dialogue systems-
interaction with interact. In Proceedings of the
3rd SIGdial workshop on Discourse and dialogue-
2002,Volume 2.

Kristiina Jokinen and Michael McTear. 2009. Spoken
dialogue systems. synthesis lectures on human lan-
guage technologies. Morgan and Claypool.

Nal Kalchbrenner, Edward Grefenstette, and Phil Blun-
som. 2014. A convolutional neural network for
modelling sentences. CoRR, abs/1404.2188.

Yoon Kim. 2014. Convolutional neural net-
works for sentence classification. arXiv preprint
arXiv:1408.5882.

Diederik P Kingma and Jimmy Ba. 2014. Adam: A
method for stochastic optimization. arXiv preprint
arXiv:1412.6980.

Roland Kuhn, Peter Nowell, and Caroline Drouin.
1997. Approaches to phoneme-based topic spotting:
An experimental comparison. In ICASSP.

Siwei Lai, Liheng Xu, Kang Liu, and Jun Zhao.
2015. Recurrent convolutional neural networks for
text classification. In Proceedings of the Twenty-
Ninth AAAI Conference on Artificial Intelligence,
AAAI’15, pages 2267–2273. AAAI Press.

Quoc Le and Tomas Mikolov. 2014. Distributed rep-
resentations of sentences and documents. In Inter-
national Conference on Machine Learning, pages
1188–1196.

Rui Lin, Shujie Liu, Muyun Yang, Mu Li, Ming Zhou,
and Sheng Li. 2015. Hierarchical recurrent neural
network for document modeling. In EMNLP.

Grégoire Mesnil, Tomas Mikolov, Marc’Aurelio Ran-
zato, and Yoshua Bengio. 2014. Ensemble of
generative and discriminative techniques for senti-
ment analysis of movie reviews. arXiv preprint
arXiv:1412.5335.

Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Cor-
rado, and Jeff Dean. 2013. Distributed representa-
tions of words and phrases and their compositional-
ity. In NIPS.

Kary Myers, Michael Kearns, Satinder Singh, and Mar-
ilyn A Walker. 2000. A boosting approach to topic
spotting on subdialogues. Family Life, 27(3):1.

Elmar Nöth, Stefan Harbeck, Heinrich Niemann, and
Volker Warnke. 1997. A frame and segment based
approach for topic spotting. In Fifth European Con-
ference on Speech Communication and Technology.

Jeffrey Pennington, Richard Socher, and Christopher
Manning. 2014. Glove: Global vectors for word
representation. In EMNLP.

Hinrich Schütze, Christopher D Manning, and Prab-
hakar Raghavan. 2008. Introduction to information
retrieval. Cambridge University Press.

Karen Sparck Jones. 1972. A statistical interpretation
of term specificity and its application in retrieval.
Journal of documentation.

Duyu Tang, Bing Qin, and Ting Liu. 2015. Document
modeling with gated recurrent neural network for
sentiment classification. pages 1422–1432. The As-
sociation for Computational Linguistics.

Marthinus Wilhelmus Theunissen. 2002. Phonene-
based topic spotting on the switchboard corpus.
Ph.D. thesis.

Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob
Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz
Kaiser, and Illia Polosukhin. 2017. Attention is all
you need.



3761

Jerry H Wright, Michael J Carey, and Eluned S Par-
ris. 1996. Statistical models for topic identification
using phoneme substrings. In ICASSP.

Zichao Yang, Diyi Yang, Chris Dyer, Xiaodong He,
Alex Smola, and Eduard Hovy. 2016. Hierarchi-
cal attention networks for document classification.
In Proceedings of the 2016 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics: Human Language Technologies,
pages 1480–1489.

Xiang Zhang, Junbo Zhao, and Yann LeCun. 2015.
Character-level convolutional networks for text clas-
sification. In Advances in neural information pro-
cessing systems, pages 649–657.


