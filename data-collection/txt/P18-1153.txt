
























































pun_camera_ready.pdf


Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers), pages 1650–1660
Melbourne, Australia, July 15 - 20, 2018. c©2018 Association for Computational Linguistics

1650

A Neural Approach to Pun Generation

Zhiwei Yu and Jiwei Tan and Xiaojun Wan
Institute of Computer Science and Technology, Peking University

The MOE Key Laboratory of Computational Linguistics, Peking University
{yuzw,tanjiwei,wanxiaojun}@pku.edu.cn

Abstract

Automatic pun generation is an inter-
esting and challenging text generation
task. Previous efforts rely on templates
or laboriously manually annotated pun
datasets, which heavily constrains the
quality and diversity of generated puns.
Since sequence-to-sequence models pro-
vide an effective technique for text gener-
ation, it is promising to investigate these
models on the pun generation task. In this
paper, we propose neural network mod-
els for homographic pun generation, and
they can generate puns without requiring
any pun data for training. We first train
a conditional neural language model from
a general text corpus, and then generate
puns from the language model with an
elaborately designed decoding algorithm.
Automatic and human evaluations show
that our models are able to generate homo-
graphic puns of good readability and qual-
ity.

1 Introduction

Punning is an ingenious way to make conversation
enjoyable and plays important role in entertain-
ment, advertising and literature. A pun is a means
of expression, the essence of which is in the given
context the word or phrase can be understood
in two meanings simultaneously (Mikhalkova and
Karyakin, 2017). Puns can be classified according
to various standards, and the most essential dis-
tinction for our research is between homographic
and homophonic puns. A homographic pun ex-
ploits distinct meanings of the same written word
while a homophonic pun exploits distinct mean-
ings of the same spoken word. Puns can be homo-

graphic, homophonic, both, or neither (Miller and
Gurevych, 2015).

Puns have the potential to combine novelty and
familiarity appropriately, which can induce pleas-
ing effect to advertisement (Valitutti et al., 2008).
Using puns also contributes to elegancy in liter-
ary writing, as laborious manual counts revealed
that puns are one of the most commonly used
rhetoric of Shakespeare, with the frequency in cer-
tain of his plays ranging from 17 to 85 instances
per thousand lines (Miller and Gurevych, 2015).
It is not an overstatement to say that pun genera-
tion has significance in human society. However,
as a special branch of humor, generating puns is
not easy for humans, let alone automatically gen-
erating puns with artificial intelligence techniques.
While text generation is a topic of interest in the
natural language processing community, pun gen-
eration has received little attention.

Recent sequence-to-sequence (seq2seq) frame-
work is proved effective on text generation tasks
including machine translation (Sutskever et al.,
2014), image captioning (Vinyals et al., 2015),
and text summarization (Tan et al., 2017). The
end-to-end framework has the potential to train
a language model which can generate fluent and
creative sentences from a large corpus. Great
progress has achieved on the tasks with sufficient
training data like machine translation, achieving
state-of-the-art performance. Unfortunately, due
to the limited puns which are deemed insuffi-
cient for training a language model, there has
not been any research concentrated on generating
puns based on the seq2seq framework as far as we
know.

The inherent property of humor makes the
pun generation task more challenging. Despite
decades devoted to theories and algorithms for hu-
mor, computerized humor still lacks of creativ-
ity, sophistication of language, world knowledge,



1651

empathy and cognitive mechanisms compared to
humans, which are extremely difficult to model
(Hossain et al., 2017).

In this paper, we study the challenging task of
generating puns with seq2seq models without us-
ing a pun corpus for training. We propose a brand-
new method to generate homographic puns us-
ing normal text corpus which can result in good
quality of language model and avoid considerable
expense of human annotators on the limited pun
resources. Our proposed method can generate
puns according to the given two senses of a tar-
get word. We achieve this by first proposing an
improved language model that is able to generate
a sentence containing a given word with a specific
sense. Based on the improved language model, we
are able to generate a pun sentence that is suit-
able for two specified senses of a homographic
word, using a novel joint beam search algorithm
we propose. Moreover, based on the observed
characteristics of human generated puns, we fur-
ther enhance the model to generate puns highlight-
ing intended word senses. The proposed method
demonstrates the ability to generate homographic
puns containing the assigned two senses of a target
word.

Our approach only requires a general text cor-
pus, and we use the Wikipedia corpus in our ex-
periment. We introduce both manual ways and
automatic metrics to evaluate the generated puns.
Experimental results demonstrate that our meth-
ods are powerful and inspiring in generating ho-
mographic puns.

The contributions of our work are as follows:

• To our knowledge, our work is the first at-
tempt to adopt neural language models on
pun generation. And we do not use any tem-
plates or pun data sets in training the model.

• We propose a brand-new algorithm to gen-
erate sentences containing assigned distinct
senses of a target word.

• We further ameliorate our model with asso-
ciative words and multinomial sampling to
produce better pun sentences.

• Our approach yields substantial results on
generating homographic puns with high ac-
curacy of assigned senses and low perplexity.

2 Related Work

2.1 Pun Generation

In recent decades, exploratory research into com-
putational humor has developed to some extent,
but seldom is research specifically concerned with
puns. Miller and Gurevych (2015) found that most
previous studies on puns tend to focus on phono-
logical or syntactic pattern rather than semantic
pattern. In this subsection we briefly review some
prior work on pun generation.

Lessard and Levison (1992) devised a pro-
gram to create Tom Swifty, a type of pun which
is present in a quoted utterance followed by a
punning adverb. Binsted and Ritchie (1994)
came up with an early prototype of pun-generator
Joke Analysis and Production Engine (JAPE). The
model generates question-answer punning with
two types of structures: schemata for determin-
ing relationships between key words in a joke, and
templates for producing the surface form of the
joke. Later its successor JAPE-2 (Binsted, 1996;
Binsted et al., 1997) and STANDUP (Ritchie et al.,
2007) introduced constructing descriptions. The
Homonym Common Phrase Pun generator (Ve-
nour, 1999) could create two-utterance texts: a
one-sentence set-up and a punch-line. Venour
(1999) used schemata to specify the required lexi-
cal items and their intern relations, and used tem-
plates to indicate where to fit the lexical items in
a skeleton text (Ritchie, 2004). McKay (2002)
proposed WISCRAIC program which can pro-
duce puns in three forms: question-answer form,
single sentence and a two-sentence sequence.
The Template-Based Pun Extractor and Genera-
tor (Hong and Ong, 2009) utilized phonetic and
semantic linguistic resources to extract word rela-
tionships in puns automatically. The system stores
the extracted knowledge in template form and re-
sults in computer-generated puns.

Most previous research on pun generation is
based on templates which is convenient but lacks
linguistic subtlety and can be inflexible. None of
the systems aimed to be creative as the skeletons of
the sentences are fixed and the generation process
based on lexical information rarely needs world
knowledge or reasoning (Ritchie, 2004). Recently
more and more work focuses on pun detection
and interpretation (Miller et al., 2017; Miller and
Gurevych, 2015; Doogan et al., 2017), rather than
pun generation.



1652

2.2 Natural Language Generation

Natural language generation is an important area
of NLP and it is an essential foundation for the
tasks like machine translation, dialogue response
generation, summarization and of course pun gen-
eration.

In the past, text generation is usually based
on the techniques like templates or rules, proba-
bilistic models like n-gram or log-linear models.
Those models are fairly interpretable and well-
behaved but require infeasible amounts of hand-
engineering to scale with the increasing training
data (Xie, 2017). In most cases larger corpus re-
veals better what matters, so it is natural to tackle
large scale modeling (Józefowicz et al., 2016).

Recently, neural network language models
(Bengio et al., 2003) have shown the good ability
to model language and fight the curse of dimen-
sionality. Cho et al. (2014) propose the encoder-
decoder structure which proves very efficient to
generate text. The encoder produces a fixed-length
vector representation of the input sequence and
the decoder uses the representation to generate an-
other sequence of symbols. Such model has a sim-
ple structure and maps the source to the target di-
rectly, which outperforms the prior models in text
generation tasks.

3 Our Models

The goal of our pun generation model is to gen-
erate a sentence containing a given target word as
homographic pun. Give two senses of the target
word (a polyseme) as input, our model generates
a sentence where both senses of the word are ap-
propriate in the sentence. We adopt the encoder-
decoder framework to train a conditional language
model which can generate sentences containing
each given sense of the target word. Then we pro-
pose a joint beam search algorithm to generate an
appropriate sentence to convey both senses of the
target word. We call this Joint Model whose ba-
sic structure is illustrated in Figure 1. We further
propose an improved model to highlight the dif-
ferent senses of the target word in one sentence,
by reminding people the specific senses of the tar-
get word, which may not easily come to mind. We
achieve this by using Pointwise Mutual Informa-
tion (PMI) to find the associative words of each
sense of the target word and increase their proba-
bility of appearance while decoding. To improve
the diversity of the generated sentence, we use

multinomial sampling to decode words in the de-
coding process. The improved model is named the
Highlight Model.

3.1 Joint Model

3.1.1 Conditional Language Model
For a given word as input, we would like to gen-
erate a natural sentence containing the target word
with the specified sense. We improve the neural
language model to achieve this goal, and name it
conditional language model.

The conditional language model for pun gener-
ation is similar to the seq2seq model with an in-
put of only one word. We use Long Short-Term
Memory (LSTM) as encoder to map the input se-
quence (target word) to a vector of a fixed dimen-
sionality, and then another LSTM network as de-
coder to decode the target sequence from the vec-
tor (Sutskever et al., 2014).

Our goal is to generate a sentence contain-
ing the target word. However, vanilla seq2seq
model cannot guarantee the target word to ap-
pear in the generated sequence all the time. To
solve this problem, we adopt the asynchronous
forward/backward generation model proposed by
Mou et al. (2015), which employs a mechanism
to guarantee some word to appear in the output
in seq2seq models. The model first generates the
backward sequence starting from the target word
wt at position t of the sentence (i.e., the words be-
fore wt), and ending up with “</s>” at the po-
sition 0 of the sentence. The probability of the
backward sequence is denoted as p(w1t ). Then
we reverse the output of the backward sequence
as the input to the forward model. In this pro-
cess, the goal of the encoder is to map the gener-
ated half sentence to a vector representation and
the decoder will generate the latter part accord-
ingly. The probability of the forward sequence
is denoted as p(wnt ). Then the input and output
of the forward model are concatenated to form
the generated sentence. In the asynchronous for-
ward/backward model, the probability of the out-
put sentence can be decomposed as:

p(
w1t=
wnt

)= p(wt)
t∏

i=0
p(bw)(wt−i|·)

m−t+1∏
i=0

p(fw)(wt+i|·),
(1)

where p(�) denotes the probability of a particu-
lar backward/forward sequence (Mou et al., 2015).
p(bw)(wt|·) or p(fw)(wt|·) denotes the probabil-



1653

input

input

Figure 1: Framework of the proposed Joint Model. (Top) Two senses of the target word input1 and
input2 (e.g. “countv01” and “countv08”) are firstly provided to the backward model, to generate the
backward sequence starting from the target senses and ending up with “</s>”. (Bottom) Then the
backward sequence are reversed and inputted to the forward model, to generate the forward sequence.
The inputs and outputs of the forward model are concatenated to form the final output sentence. Joint
beam search algorithm is used to generate each word that has the potential to make the generated sentence
suitable for both input senses.

ity of wt given previous sequence · in the back-
ward or forward model respectively. The above
model can only guarantee the target word to ap-
pear in the generated sentence. Since we hope to
generate a sentence containing the specified word
sense, we treat different senses of the same word
as independent new pseudo-words. We label the
senses of words with Word Sense Disambigua-
tion (WSD) tools, and then we train the language
model using the corpus with labeled senses so
that for each word sense we can generate a sen-
tence accordingly. We use the Python Implemen-
tations of WSD Technologies1 for WSD. This tool
can return the most possible sense for the target
word based on WordNet (Miller, 1995). We at-
tach the sense label to the word and form a new
pseudo-word accordingly. Taking “count” for ex-
ample, “countv01” means “determine the number

1https://github.com/alvations/pywsd

or amount of ”, while “countv08” means “have
faith or confidence in”.

3.1.2 Decoding with Joint Beam Search
Algorithm

Beam search is a frequently-used algorithm in the
decoding stage of seq2seq models to generate the
output sequence. It can be viewed as an adaptation
of branch-and-bound search that uses an inadmis-
sible pruning rule. In the beam search algorithm,
only the most promising nodes at each level of the
search graph are selected and the rest nodes are
permanently removed. This strategy makes beam
search able to find a solution within practical time
or memory limits and work well in practical tasks
(Zhou and Hansen, 2005; Freitag and Al-Onaizan,
2017).

We also use beam search in our pun genera-
tion model. According to the definition of homo-
graphic puns, at least two senses of the target word



1654

should be interpreted in one sentence. We hope to
generate a same sentence for distinct senses of the
same word, and in this way the target word in the
sentence can be interpreted as various senses. Pro-
vided with two senses of a target word as inputs
to the encoder in the backward generation pro-
cess, e.g. “countv01” as input1 and “countv08”
as input2, we decode two output sentences in par-
allel, and the two sentences should be the same
except for the input pseudo-words. Assume h(s)t,i
denotes the hidden state of the i-th beam at time
step t, when given the s-th pseudo-word as input
(s =1 or 2). In the traditional beam search algo-
rithm, softmax layer is applied on the hidden state
to get the probability distribution on the vocabu-
lary, and the log likelihood of the probability is
used to get a word score distribution d(s)t,i :

d
(s)
t,i = log(softmax layer(h

(s)
t,i )). (2)

The accumulated score distribution on the i-th
beam is:

p
(s)
t,i = u

(s)
t−1,i + d

(s)
t,i , (3)

|V | denotes the vocabulary size. u(s)t−1,i is a |V |-
dimensional vector whose values are all equal to
the accumulated score of the generated sequence
till time step t − 1. Assume the beam width is b,
p
(s)
t is the concatenation of p

(s)
t,i on all beams and

its dimension size is |V |∗b. The beam search algo-
rithm selects b candidate words at each time step
according to p(s)t (s =1 or 2). When decoding for
input1 and input2 in parallel, at each time step
there will be b candidates for each input according
to p(1)t and p

(2)
t respectively. Since input1 and

input2 are different, the candidates for two inputs
will hardly be the same. However, our goal is to
choose candidate words which have the potential
to result in candidate sentences suitable for both
senses. Our joint beam search algorithm selects
b candidates while decoding for the two inputs ac-
cording to the joint score distribution on all beams.
The joint score distribution on the i-th beam is:

ot,i=p
(1)
t,i + p

(2)
t,i . (4)

The summation of the log scores can be viewed
as the product of original probabilities, which rep-
resents the joint probability if the two probabil-
ity distributions are viewed independent. Given
the b candidates selected according to the joint
score distribution, our joint beam search algorithm

Algorithm 1 Joint Beam Search Algorithm
b denotes the beam width. l denotes the number of unfinished
beams. BeamId records which beams the candidates come
from. WordId records the indices of candidates in the vo-
cabulary where 1 is the index of “<s>”. BEAM t[i] denotes
the i-th beam history till time step t. |V | denotes the vocabu-
lary size. Copy(m,n) aims to make an n-dimensional vector
by replicating m for n times. The initial states of the decoder
(h(1)−1,i,h

(2)
−1,i) are equal to the final states of the encoder ac-

cordingly. m � n denotes appending n to m.
BEAM−1[i]= [], i=0, 1, ..., b− 1
u

(1)
−1,i = u

(2)
−1,i = Copy(0, |V |),i = 0, 1, ..., b− 1

BeamId = [0, 1, ..., b− 1]
WordId= [1, .., 1] ∈ Rb
Outputs= []; t = 0; l = b
while l > 0 do
o=[]
for i= 0 to b− 1 do

xt,i is the word embedding corresponding to
WordId[i]

h(1)t,i=LSTM(xt,i,h
(1)
t−1,i)

h
(2)
t,i=LSTM(xt,i,h

(2)
t−1,i)

p(1)t,i = u
(1)
t−1,i + log(softmax layer(h

(1)
t,i ))

p(2)t,i = u
(2)
t−1,i + log(softmax layer(h

(2)
t,i ))

ot,i = p
(1)
t,i + p

(2)
t,i

o � ot,i
end for
WordId = the indices of words with the top b scores in o
BeamId = the indices of source beams w.r.t. WordId
for i= 0 to b− 1 do
BEAMt[i] = BEAMt−1[BeamId[i]] � WordId[i]
u

(1)
t,i = u

(2)
t,i = Copy(ot,BeamId[i][WordId[i]], |V |)

if WordId[i] represents “</s>”
l = l − 1
Outputs = Outputs �BEAMt[i]

end if
end for
t = t+ 1

return top b items in Outputs

is similar to the vanilla beam search algorithm,
which generates the candidate sequences step by
step. If any beam selects “</s>” as the candi-
date, we regard this branch has finished decod-
ing. The decoding process will be finished after all
the beams have selected “</s>”. The joint beam
search algorithm is described in Algorithm 1.

3.2 Highlight Model
3.2.1 Word Association
The joint model we described above is able to
generate sentences suitable for both given senses
of the target word. But we found this model is
prone to generate monotonous sentences, making
it difficult to discover that the target word in the
sentence can be understood in two ways. For ex-
ample, in the sentence “He couldn’t count on his
friends”, people can easily realize that the com-
mon meaning “have faith or confidence in” of the



1655

word “count”, but may ignore other senses of the
word. If we add some words and modify the sen-
tence as “The inept mathematician couldn’t count
on his friends”, people can also come up with
the meaning “determine the number or amount
of ” due to the word “mathematician”. Comparing
the examples above, the two senses are proper in
both sentences, but people may interpret “count”
in the two sentences differently. Based on such ob-
servations, we improve the pun generation model
by adding some keywords to the sentence which
could remind people some special sense of the
target word. We call those keywords associative
words, and the improved model is named as High-
light Model.

To extract associative words of each sense of the
target word, we first build word association norms
in our corpus by using pointwise mutual informa-
tion (PMI). As mutual information compares the
probability of observing w1 and w2 together (the
joint probability) with the probabilities of observ-
ing w1 and w2 independently (chance) (Church
and Hanks, 1990), positive PMI scores indicate
that the words occur together more than would be
expected under an independence assumption, and
negative scores indicate that one word tends to ap-
pear solely when the other does not (Islam and
Inkpen, 2006). In this case we take top k asso-
ciative words for each sense with relatively high
positive PMI scores, which are calculated as fol-
lows:

PMI(w1, w2) = log2
p(w1, w2)

p(w1) · p(w2) . (5)

During decoding we increase the probability of
the associative words to be chosen according to
their PMI scores. For each sense of the target
word, we normalize the PMI scores of the asso-
ciative words as follows:

Asso(w
(s)
t , cp) = σ(

PMI(w
(s)
t , cp)

maxcjPMI(w
(s)
t , cj)

),

(6)
where w(s)t represents the s-th sense of the tar-
get word wt, and cp is the p-th associative word
for w(s)t . To smooth the PMI scores we use sig-
moid function σ which is differentiable and widely
used in the neural network models. The final
PMI score for each associative word is denoted as
Asso(w

(s)
t , cp). As we choose candidates accord-

ing to a score distribution on the whole vocabulary,

we need a PMI score distribution (S(w(s)t )) rather
than single scores, and the value at position q is
supposed to be:

S
(
w

(s)
t

)
[q]=

{
Asso

(
w

(s)
t ,vq

)
, vq∈AssoTK(w(s)t );

0, else,
(7)

where vq denotes the q-th word in the vocabulary,
and AssoTK(w(s)t ) denotes the top k associative
words of w(s)t .

3.2.2 Multinomial Sampling
In our highlight model, we add S(w(1)t ) and
S(w

(2)
t ) to ot,i , as:

õt,i=ot,i+α1 ·S(w(1)t )+α2 ·S(w(2)t ), (8)

where we use α1 and α2 as coefficient weights to
balance the PMI scores of the two assigned senses
and the joint score. In the Highlight Model, we
first select 2b candidates according to the scores of
words from Eq. 8. Then we use multinomial sam-
pling to select the final b candidates. Sampling is
useful in cases where we may want to get a variety
of outputs for a particular input. One example of a
situation where sampling is meaningful would be
in a seq2seq model for a dialog system (Neubig,
2017). In our pun generation model we hope to
produce relatively more creative sentences, so we
use multinomial sampling to increase the uncer-
tainty when generating the sentence. The multi-
nomial distribution can be seen as a multivariate
generalization of the binomial distribution and it
is prone to choose the words with relatively high
probabilities. If an associative word of one sense
has been selected, we decay the scores for all as-
sociative words of this sense. In this way we can
prevent the sentence obviously being prone to re-
flect one sense of the target word.

4 Experiments

4.1 Data Preprocessing
Most text generation tasks using seq2seq model
require large amount of training data. However,
for many tasks, like pun generation, it is difficult
to get adequate data to train a seq2seq model. In
this study, our pun generation model does not rely
on training data of puns. We only require a text
corpus to train the conditional language model,



1656

which is very cheap to get. In this paper, we use
the English Wikipedia corpus to train the language
model. The corpus texts are firstly lowercased and
tokenized, and all numeric characters are replaced
with “#”. We split the texts into sentences and
discard the sentences whose length is less than 5
words or more than 50 words. We then select pol-
ysemes appearing in the homographic pun data set
(Miller et al., 2017) and pun websites. Those pol-
ysemes in the corpus are replaced by the labeled
sense. We restrict that each sentence can be la-
beled with at most two polysemes in order to train
a reliable language model. If there are more pol-
ysemes in one sentence, we keep the last two be-
cause in our observation we found pun words tend
to occur near the end of a sentence. After label-
ing, we keep the 105,000 most frequently occur-
ring words and other words are replaced with the
“<unk>” token. We discard the sentences with
two or more “<unk>” tokens. There are totally
3,974 distinct labeled senses corresponding to a
total of 772 distinct polysemes. We assume those
reserved senses are more likely to generate puns
of good quality.

While training the language model we use
2,595,435 sentences as the training set, and
741,551 sentences as the development set to de-
cide when to stop training.

4.2 Training Details
The number of LSTM layers we use in the seq2seq
model is 2 and each layer has 128 units. To avoid
overfitting, we set the dropout rate to 0.2. We
use Stochastic Gradient Descent (SGD) with a de-
creasing learning rate schedule as optimizer. The
initial learning rate is 1.0 and is halved every 1k
steps after training for 8k steps, which is the same
as Luong et al. (2017). We set beam size b = 5
while decoding. For each sense we select at most
30 associative words (k=30). To increase the prob-
ability of choosing the associative words, we set
α1 = 6.0 and α2 = 6.0. If an associative word
of some sense of a target word has been chosen,
its corresponding α will be set to zero for all the
associative words of this sense.

4.3 Baselines
Since there is no existing neural model applied on
this special task, we implement two baseline mod-
els for comparison. We select 100 target words
and two senses for each word to test the quality of
those models.

Normal Language Model: It is trained with
an encoder-decoder model and uses beam search
while decoding. In the training process, inputs are
unlabeled target words and outputs are sentences
containing the target words.

Pun Language Model: We use the data set of
homographic puns from Miller et al. (2017). The
model is trained on the data set in asynchronous
forward/backward way. As the pun data set is
limited, the pun language model has no creativity,
which means if we input a word appearing in the
training data, then the output will usually be an ex-
isting sentence from the training data. Therefore,
we remove the sentences which contain words in
the 100 target words from the pun data set, and
then train the model for test.

4.4 Automatic Evaluation

We select 100 target words and two senses for
each word for test. We use the language mod-
eling toolkit SRILM2 to train a trigram model
with another 7,746,703 sentences extracted from
Wikipedia, which are different from the data set
used before. The perplexity scores (PPL) of our
models and baseline models are estimated based
on the trained language model, as shown in Ta-
ble 1. Normal Language Model has no constraint
of generating sentences suitable for both senses.
This means at each time step the beam search algo-
rithm can select the candidates with highest prob-
abilities. And thus it is natural that it obtains the
lowest perplexity. Taking the constraint of senses
into consideration, the perplexity scores of Joint
Model and Highlight Model are still comparable
to that of Normal Language Model. However, Pun
Language Model could not be trained well con-
sidering the limit of the pun training data, so it
gets the highest perplexity score. This result re-
veals that it is not feasible to build a homographic
pun generation system based on the pun data set
since pun data is far from enough. In the table, We
further compare the diversity of the generated sen-
tences of four models following Li et al. (2016).
Distinct-1 (d.-1) and distinct-2 (d.-2) are the ra-
tios of the distinct unigrams and bigrams in gen-
erated sentences, i.e., the number of distinct uni-
grams or bigrams divided by the total number of
unigrams or bigrams. The results show our mod-
els are more creative than Normal Language and

2http://www.speech.sri.com/projects/srilm/



1657

Model PPL d.-1(%) d.-2(%)
Highlight 91.80 27.13 62.85

Joint 63.48 22.13 50.59
Normal Language 62.66 19.60 41.62

Pun Language 889.07 14.78 23.11

Table 1: Results of automatic evaluation.

Figure 2: Results of human evaluation.

Pun Language models, and Highlight Model can
generate sentences with the best diversity.

4.5 Human Evaluation

Because of the subtle and delicate structure of
puns, automatic evaluation is not enough. So we
sample one sentence for each word from four mod-
els mentioned above and then get 100 sentences of
each model generated from the target words, to-
gether with 100 puns containing the same target
words from homographic pun data set in Miller
et al. (2017). We ask judges on Amazon Mechan-
ical Turk to evaluate all the sentences and the rat-
ing score ranges from 1 to 5. Five native English
speakers are asked to give a score on each sen-
tence in three aspects with the following informa-
tion: Readability indicates whether the sentence
is easy to understand semantically; Accuracy in-
dicates whether the given senses are suitable in a
sentence; Fluency indicates whether the sentence
is fluent and consistent with the rules of grammar.

The results in Figure 2 show that pun data is
not enough to train an ideal language model, while
Normal Language Model has enough corpus to
train a good language model. But Normal Lan-
guage Model is unable to make the given two
senses appear in one sentence and in a few cases
even can not assure the appearance of the target
words. Joint Model and Highlight Model can gen-
erate fluent sentences for the assigned two senses.
Although Highlight Model could remind people

Model # sentences avg. score
Highlight 15 0.98

Joint 12 0.87
Gold Puns 28 1.38

Table 2: Results of Soft Turing Test.

specific senses of the target words in most cases, in
few cases sampled words make the whole sentence
unsatisfactory and get a relatively lower score of
accuracy. As to the Readability, the Joint Model
performs better than other three models. Both
Joint model and Highlight model outperform Nor-
mal Language Model and Pun Language Model.

To test the potential of the sentences generated
by our models to be homographic puns, we fur-
ther design a Soft Turing Test. We select 30 sen-
tences generated by Joint Model and 30 sentences
generated by Highlight Model independently, to-
gether with 30 gold puns from the homographic
pun data set. We mix them up, and give the def-
inition of homographic pun and ask 10 people on
Amazon Mechanical Turk to judge each sentence.
People can judge each sentence as one of three cat-
egories: definitely by human, might by human and
definitely by machine. The three categories cor-
respond to the scores of 2, 1 and 0, respectively.
If the average score of one sentence is equal or
greater than 1, we regard it as judged to be gener-
ated by human. The number of sentences judged
as by human for each model and the average score
for each model are shown in Table 2.

Due to the flexible language structure of High-
light Model, the generated homographic puns out-
perform those generated by Joint Model in the Soft
Turing Test, however still far from gold-standard
puns. Our models are adept at generating homo-
graphic puns containing assigned senses but weak
in making homographic puns humorous.

4.6 Examples

We show some examples generated by differ-
ent models in Table 3. For the two senses of
“pitch”, Highlight Model generates a sentence
which uses “high” to remind readers the sense re-
lated to sound and uses “player” to highlight the
sense related to throwing a baseball. Joint Model
returns a sentence that can be understood in both
way roughly only if we give the two senses in ad-
vance, otherwise readers may only think of the



1658

Model Sample
pitch: 1) the property of sound that arise with variation in the frequency of vibration;

2) the act of throwing a baseball by a pitcher to a batter.
Highlight in one that denotes player may have had a high pitch in the world
Joint the object of the game is based on the pitch of the player
Normal Language this is a list of high pitch plot
Pun Language our bikinis are exciting they are simply the tops on the mouth
Gold Puns if you sing while playing baseball you won’t get a good pitch
square: 1) a plane rectangle with four equal sides and four right angles, a four-sided regular polygon;

2) someone who doesn’t understand what is going on.
Highlight little is known when he goes back to the square of the football club
Joint there is a square of the family
Normal Language the population density was # people per square mile
Pun Language when the pirate captain’s ship ran aground he couldn’t fathom why
Gold Puns my advanced geometry class is full of squares
problem: 1) a source of difficulty;

2) a question raised for consideration or solution.
Highlight you do not know how to find a way to solve the problem which in the state
Joint he is said to be able to solve the problem as he was a professor
Normal Language in # he was appointed a member of the new york stock exchange
Pun Language those who iron clothes have a lot of pressing veteran
Gold Puns math teachers have lots of problems

Table 3: Examples of outputs by different models.

sense related to baseball. For Normal Language
Model, it is difficult to be interpreted in two senses
we assigned. Pun Language Model has no ability
to return a sentence containing the assigned word
at all. Observing the gold pun, the context de-
scribes a more vivid scene which we need to pay
attention to. For “square”, sentences generated
by Highlight Model and Joint Model can be inter-
preted in two senses and Highlight Model results
in a sentence with dexterity. Normal Language
Model give a sentence where “square” means nei-
ther of the two given senses. Pun Language Model
cannot return a sentence we need with no sur-
prise. For “problem”, both Highlight Model and
Joint Model can generate sentences containing as-
signed two senses while Normal Language Model
and Pun Language Model can not return sentences
with the target word. Compare to our generated
sentences, we find gold puns are more concise and
accurate, which takes us into consideration on the
delicate structure of puns and the conclusion is
still in exploration.

5 Conclusion and Future Work

In this paper, we proposed two models for pun
generation without using training data of puns.
Joint Model makes use of conditional language
model and the joint beam search algorithm, which
can assure the assigned senses of target words suit-
able in one sentence. Highlight Model takes asso-
ciative words into consideration, which makes the
distinct senses more obvious in one sentence. The
produced puns are evaluated using automatic eval-
uation and human evaluation, and they outperform
the sentences generated by our baseline models.

For future work, we hope to improve the results
by using the pun data and design a more proper
way to select candidates from associative words.

Acknowledgment

This work was supported by National Natural Sci-
ence Foundation of China (61772036, 61331011)
and Key Laboratory of Science, Technology and
Standard in Press Industry (Key Laboratory of In-
telligent Press Media Technology). We thank the
anonymous reviewers for their helpful comments.
Xiaojun Wan is the corresponding author.



1659

References
Yoshua Bengio, Réjean Ducharme, Pascal Vin-

cent, and Christian Janvin. 2003. A neu-
ral probabilistic language model. Journal
of Machine Learning Research 3:1137–1155.
http://www.jmlr.org/papers/v3/bengio03a.html.

Kim Binsted. 1996. Machine humour: An imple-
mented model of puns .

Kim Binsted, Helen Pain, and Graeme D Ritchie. 1997.
Children’s evaluation of computer-generated pun-
ning riddles. Pragmatics & Cognition 5(2):305–
354.

Kim Binsted and Graeme Ritchie. 1994. An im-
plemented model of punning riddles. In Pro-
ceedings of the 12th National Conference on
Artificial Intelligence, Seattle, WA, USA, July
31 - August 4, 1994, Volume 1.. pages 633–638.
http://www.aaai.org/Library/AAAI/1994/aaai94-
096.php.

Kyunghyun Cho, Bart van Merrienboer, Çaglar
Gülçehre, Fethi Bougares, Holger Schwenk, and
Yoshua Bengio. 2014. Learning phrase representa-
tions using RNN encoder-decoder for statistical ma-
chine translation. arXiv preprint arXiv:1406.1078.
http://arxiv.org/abs/1406.1078.

Kenneth Ward Church and Patrick Hanks. 1990. Word
association norms, mutual information, and lexicog-
raphy. Computational Linguistics 16(1):22–29.

Samuel Doogan, Aniruddha Ghosh, Hanyang Chen,
and Tony Veale. 2017. Idiom savant at semeval-
2017 task 7: Detection and interpretation of en-
glish puns. In Proceedings of the 11th International
Workshop on Semantic Evaluation, SemEval@ACL
2017, Vancouver, Canada, August 3-4, 2017. pages
103–108. https://doi.org/10.18653/v1/S17-2011.

Markus Freitag and Yaser Al-Onaizan. 2017. Beam
search strategies for neural machine transla-
tion. In Proceedings of the First Work-
shop on Neural Machine Translation, NMT@ACL
2017, Vancouver, Canada, August 4, 2017.
pages 56–60. https://aclanthology.info/papers/W17-
3207/w17-3207.

Bryan Anthony Hong and Ethel Ong. 2009. Automati-
cally extracting word relationships as templates for
pun generation. In Proceedings of the Workshop
on Computational Approaches to Linguistic Cre-
ativity. Association for Computational Linguistics,
Stroudsburg, PA, USA, CALC ’09, pages 24–31.
http://dl.acm.org/citation.cfm?id=1642011.1642015.

Nabil Hossain, John Krumm, Lucy Vanderwende,
Eric Horvitz, and Henry A. Kautz. 2017. Fill-
ing the blanks (hint: plural noun) for mad
libs humor. In Proceedings of the 2017
Conference on Empirical Methods in Natural
Language Processing, EMNLP 2017, Copen-
hagen, Denmark, September 9-11, 2017. pages

638–647. https://aclanthology.info/papers/D17-
1067/d17-1067.

Aminul Islam and Diana Inkpen. 2006. Second or-
der co-occurrence PMI for determining the semantic
similarity of words. In Proceedings of the Fifth In-
ternational Conference on Language Resources and
Evaluation, LREC 2006, Genoa, Italy, May 22-28,
2006.. pages 1033–1038.

Rafal Józefowicz, Oriol Vinyals, Mike Schuster, Noam
Shazeer, and Yonghui Wu. 2016. Exploring
the limits of language modeling. arXiv preprint
arXiv:1602.02410. http://arxiv.org/abs/1602.02410.

Greg Lessard and Michael Levison. 1992. Computa-
tional modelling of linguistic humour: Tom swifties.
In In ALLC/ACH Joint Annual Conference, Oxford.
pages 175–178.

Jiwei Li, Michel Galley, Chris Brockett, Jianfeng
Gao, and Bill Dolan. 2016. A diversity-promoting
objective function for neural conversation mod-
els. In NAACL HLT 2016, The 2016 Con-
ference of the North American Chapter of the
Association for Computational Linguistics: Hu-
man Language Technologies, San Diego Califor-
nia, USA, June 12-17, 2016. pages 110–119.
http://aclweb.org/anthology/N/N16/N16-1014.pdf.

Minh-Thang Luong, Eugene Brevdo, and Rui Zhao.
2017. Neural machine translation (seq2seq) tutorial.
https://github.com/tensorflow/nmt .

Justin McKay. 2002. Generation of idiom-based witti-
cisms to aid second language learning. In In Stock
et al.. pages 77–87.

Elena Mikhalkova and Yuri Karyakin. 2017. Pun-
fields at semeval-2017 task 7: Employing ro-
get’s thesaurus in automatic pun recognition and
interpretation. arXiv preprint arXiv:1707.05479.
http://arxiv.org/abs/1707.05479.

George A Miller. 1995. Wordnet: a lexical database for
english. Communications of the ACM 38(11):39–
41.

Tristan Miller and Iryna Gurevych. 2015. Automatic
disambiguation of english puns. In Proceedings
of the 53rd Annual Meeting of the Association for
Computational Linguistics and the 7th International
Joint Conference on Natural Language Process-
ing of the Asian Federation of Natural Language
Processing, ACL 2015, July 26-31, 2015, Beijing,
China, Volume 1: Long Papers. pages 719–729.
http://aclweb.org/anthology/P/P15/P15-1070.pdf.

Tristan Miller, Christian F. Hempelmann, and Iryna
Gurevych. 2017. SemEval-2017 Task 7: Detec-
tion and interpretation of English puns. In Proceed-
ings of the 11th International Workshop on Semantic
Evaluation (SemEval-2017). pages 59–69.



1660

Lili Mou, Rui Yan, Ge Li, Lu Zhang, and Zhi
Jin. 2015. Backbone language modeling for con-
strained natural language generation. arXiv preprint
arXiv:1512.06612. http://arxiv.org/abs/1512.06612.

Graham Neubig. 2017. Neural machine trans-
lation and sequence-to-sequence models: A
tutorial. arXiv preprint arXiv:1703.01619.
http://arxiv.org/abs/1703.01619.

Graeme Ritchie. 2004. The linguistic analysis of jokes.
Routledge.

Graeme Ritchie, Ruli Manurung, Helen Pain, Annalu
Waller, Rolf Black, and Dave O’Mara. 2007. A
practical application of computational humour. In
Proceedings of the 4th International Joint Confer-
ence on Computational Creativity. pages 91–98.

Ilya Sutskever, Oriol Vinyals, and Quoc V. Le. 2014.
Sequence to sequence learning with neural net-
works. In Advances in Neural Information Process-
ing Systems 27: Annual Conference on Neural In-
formation Processing Systems 2014, December 8-
13 2014, Montreal, Quebec, Canada. pages 3104–
3112. http://papers.nips.cc/paper/5346-sequence-
to-sequence-learning-with-neural-networks.

Jiwei Tan, Xiaojun Wan, and Jianguo Xiao. 2017.
Abstractive document summarization with a graph-
based attentional neural model. In Proceedings
of the 55th Annual Meeting of the Association for
Computational Linguistics, ACL 2017, Vancouver,
Canada, July 30 - August 4, Volume 1: Long Papers.
Association for Computational Linguistics, pages
1171–1181. https://doi.org/10.18653/v1/P17-1108.

Alessandro Valitutti, Carlo Strapparava, and Oliviero
Stock. 2008. Textual affect sensing for compu-
tational advertising. In Creative Intelligent Sys-
tems, Papers from the 2008 AAAI Spring Sympo-
sium, Technical Report SS-08-03, Stanford, Califor-
nia, USA, March 26-28, 2008. pages 117–122.

Chris Venour. 1999. The computational generation of
a class of puns. In Master’s thesis, Queen’s Univer-
sity,Kingston, Ontario.

Oriol Vinyals, Alexander Toshev, Samy Bengio, and
Dumitru Erhan. 2015. Show and tell: A neu-
ral image caption generator. In IEEE Confer-
ence on Computer Vision and Pattern Recogni-
tion, CVPR 2015, Boston, MA, USA, June 7-12,
2015. IEEE Computer Society, pages 3156–3164.
https://doi.org/10.1109/CVPR.2015.7298935.

Ziang Xie. 2017. Neural text generation: A practical
guide. arXiv preprint arXiv:1711.09534 .

Rong Zhou and Eric A. Hansen. 2005. Beam-
stack search: Integrating backtracking with
beam search. In Proceedings of the Fifteenth
International Conference on Automated Plan-
ning and Scheduling (ICAPS 2005), June 5-10
2005, Monterey, California, USA. pages 90–98.
http://www.aaai.org/Library/ICAPS/2005/icaps05-
010.php.


