



















































SimCompass: Using Deep Learning Word Embeddings to Assess Cross-level Similarity


Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 560–565,
Dublin, Ireland, August 23-24, 2014.

SimCompass: Using Deep Learning Word Embeddings
to Assess Cross-level Similarity

Carmen Banea, Di Chen,
Rada Mihalcea∗

University of Michigan
Ann Arbor, MI

Claire Cardie
Cornell University

Ithaca, NY

Janyce Wiebe
University of Pittsburgh

Pittsburgh, PA

Abstract

This article presents our team’s partici-
pating system at SemEval-2014 Task 3.
Using a meta-learning framework, we
experiment with traditional knowledge-
based metrics, as well as novel corpus-
based measures based on deep learning
paradigms, paired with varying degrees of
context expansion. The framework en-
abled us to reach the highest overall per-
formance among all competing systems.

1 Introduction

Semantic textual similarity is one of the key
components behind a multitude of natural lan-
guage processing applications, such as informa-
tion retrieval (Salton and Lesk, 1971), relevance
feedback and text classification (Rocchio, 1971),
word sense disambiguation (Lesk, 1986; Schutze,
1998), summarization (Salton et al., 1997; Lin
and Hovy, 2003), automatic evaluation of machine
translation (Papineni et al., 2002), plagiarism de-
tection (Nawab et al., 2011), and more.

To date, semantic similarity research has pri-
marily focused on comparing text snippets of simi-
lar length (see the semantic textual similarity tasks
organized during *Sem 2013 (Agirre et al., 2013)
and SemEval 2012 (Agirre et al., 2012)). Yet,
as new challenges emerge, such as augmenting a
knowledge-base with textual evidence, assessing
similarity across different context granularities is
gaining traction. The SemEval Cross-level seman-
tic similarity task is aimed at this latter scenario,
and is described in more details in the task paper
(Jurgens et al., 2014).

∗{carmennb,chenditc,mihalcea}@umich.edu
This work is licensed under a Creative Commons At-

tribution 4.0 International Licence. Page numbers and pro-
ceedings footer are added by the organisers. Licence details:
http://creativecommons.org/licenses/by/4.0/

2 Related Work

Over the past years, the research community has
focused on computing semantic relatedness us-
ing methods that are either knowledge-based or
corpus-based. Knowledge-based methods derive
a measure of relatedness by utilizing lexical re-
sources and ontologies such as WordNet (Miller,
1995) or Roget (Rog, 1995) to measure defi-
nitional overlap, term distance within a graph-
ical taxonomy, or term depth in the taxonomy
as a measure of specificity. There are many
knowledge-based measures that were proposed in
the past, e.g., (Leacock and Chodorow, 1998;
Lesk, 1986; Resnik, 1995; Jiang and Conrath,
1997; Lin, 1998; Jarmasz and Szpakowicz, 2003;
Hughes and Ramage, 2007).

On the other side, corpus-based measures such
as Latent Semantic Analysis (LSA) (Landauer
and Dumais, 1997), Explicit Semantic Analy-
sis (ESA) (Gabrilovich and Markovitch, 2007),
Salient Semantic Analysis (SSA) (Hassan and
Mihalcea, 2011), Pointwise Mutual Informa-
tion (PMI) (Church and Hanks, 1990), PMI-IR
(Turney, 2001), Second Order PMI (Islam and
Inkpen, 2006), Hyperspace Analogues to Lan-
guage (Burgess et al., 1998) and distributional
similarity (Lin, 1998) employ probabilistic ap-
proaches to decode the semantics of words. They
consist of unsupervised methods that utilize the
contextual information and patterns observed in
raw text to build semantic profiles of words. Un-
like knowledge-based methods, which suffer from
limited coverage, corpus-based measures are able
to induce the similarity between any two words, as
long as they appear in the corpus used for training.

3 System Description

3.1 Generic Features

Our system employs both knowledge and corpus-
based measures as detailed below.

560



Knowledge-based features
Knowledge-based metrics were shown to provide
high correlation scores with the goldstandard in
text similarity tasks (Agirre et al., 2012; Agirre et
al., 2013). We used three WordNet-based simi-
larity measures that employ information content.
We chose these metrics because they are able to
incorporate external information derived from a
large corpus: Resnik (Resnik, 1995) (RES), Lin
(Lin, 1998) (LIN ), and Jiang & Conrath (Jiang
and Conrath, 1997) (JCN ).

Corpus based features
Our corpus based features are derived from a
deep learning vector space model that is able to
“understand” word meaning without human in-
put. Distributed word embeddings are learned us-
ing a skip-gram recurrent neural net architecture
running over a large raw corpus (Mikolov et al.,
2013b; Mikolov et al., 2013a). A primary advan-
tage of such a model is that, by breaking away
from the typical n-gram model that sees individual
units with no relationship to each other, it is able to
generalize and produce word vectors that are simi-
lar for related words, thus encoding linguistic reg-
ularities and patterns (Mikolov et al., 2013b). For
example, vec(Madrid)-vec(Spain)+vec(France) is
closer to vec(Paris) than any other word vec-
tor (Mikolov et al., 2013a). We used the pre-
trained Google News word2vec model (WTV )
built over a 100 billion words corpus, and con-
taining 3 million 300-dimension vectors for words
and phrases. The model is distributed with the
word2vec toolkit. 1

Since the methods outlined above provide similar-
ity scores at the sense or word level, we derive text
level metrics by employing two methods.
VectorSum. We add the vectors corresponding to
the non-stopwords tokens in bag of words (BOW)
A and B, resulting in vectors VA and VB , respec-
tively. The assumption is that these vectors are
able to capture the semantic meaning associated
with the contexts, enabling us to gauge their relat-
edness using cosine similarity.
Align. Given two BOW A and B as input, we
compare them using a word-alignment-based sim-
ilarity measure (Mihalcea et al., 2006). We calcu-
late the pairwise similarity between the words in
A and B, and match each word in A with its most
similar counterpart in B. For corpus-based fea-

1https://code.google.com/p/word2vec/

tures, the similarity measure represents the aver-
age over these scores, while for knowledge-based
measures, we consider the top 40% ranking pairs.

We use the DKPro Similarity package (Bär et
al., 2013) to compute knowledge-based metrics,
and the word2vec implementation from the Gen-
sim toolkit (Rehurek and Sojka, 2010).

3.2 Feature Variations

Since our system participated in all four lexical
levels evaluations, we describe below the modifi-
cations pertaining to each.
word2sense. At the word2sense level, we em-
ploy both knowledge and corpus-based features.
Since the information available in each pair is ex-
tremely limited (only a word and a sense key)
we infuse contextual information by drawing on
WordNet (Miller, 1995). In WordNet, the sense
of each word is encapsulated in a uniquely iden-
tifiable synset, consisting of the definition (gloss),
usage examples and its synonyms. We can derive
three variations (where the word and sense com-
ponents are represented by BOW A and B, respec-
tively): a) no expansion (A={word}, B={sense}),
b) expand right (R) (A={word}, B={sense gloss
& example}), c) expand left (L) & right (R)
(A={word glosses & examples}, B={sense gloss
& example}). After applying the Align method,
we obtain measures JNC, LIN , RES and
WTV 1; VectorSum results in WTV 2.
phrase2word. As this lexical level also suf-
fers from low context, we adapt the above vari-
ations, where the phrase and word components
are represented by BOW A and BOW B, re-
spectively. Thus, we have: a) no expan-
sion (A={phrase}, B={word}), b) expand R
(A={phrase}, B={word glosses and examples}),
c) expand L & R (A={phrase glosses & exam-
ples}, B={word glosses and examples}). We ex-
tract the same measures as for word2sense.
sentence2phrase. For this variation, we use only
corpus based measures; BOW A represents the
sentence component, B, the phrase. Since there is
sufficient context available, we follow the no ex-
pansion variation, and obtain metrics WTV 1 (by
applying Align) and WTV 2 (using VectorSum).
paragraph2sentence. At this level, due to the
long context that entails one-to-many mappings
between the words in the sentence and those in
the paragraph, we use a text clustering technique
prior to calculating the features’ weights.

561



a) no clustering. We use only corpus based mea-
sures, where the paragraph represents BOW A,
and the sentence represents BOW B. Then we ap-
ply Align and VectorSum, resulting in WTV 1 and
WTV 2, respectively.
b) paragraph centroids extraction. Since the
longer text contains more information compared
to the shorter one, we extract k topic vectors after
K-means clustering the left context.2 These cen-
troids are able to model topics permeating across
sentences, and by comparing them with the word
vectors pertaining to the short text, we seek to cap-
ture how much of the information is covered in the
shorter text. Each word is paired with the centroid
that it is closest to, and the average is computed
over these scores, resulting in WTV 3.
c) sentence centroids extraction. Under a dif-
ferent scenario, assuming that one sentence cov-
ers only a few strongly expressed topics, unlike
a paragraph that may digress and introduce unre-
lated noise, we apply clustering on the short text.
The centroids thus obtained are able to capture
the essence of the sentence, so when compared to
every word in the paragraph, we can gauge how
much of the short text is reflected in the longer
one. Each centroid is paired with the word that it is
most similar to, and we average these scores, thus
obtaining WTV 4. In a way, methods b) and c)
provide a macro, respectively micro view of how
the topics are reflected across the two spans of text.

3.3 Meta-learning

The measures of similarity described above pro-
vide a single score per each long text - short text
pair in the training and test data. These scores then
become features for a meta-learner, which is able
to optimize their impact on the prediction process.
We experimented with multiple regression algo-
rithms by conducting 10 fold cross-validation on
the training data. The strongest performer across
all lexical levels was Gaussian processes with a
radial basis function (RBF) kernel. Gaussian pro-
cesses regression is an efficient probabilistic pre-
diction framework that assumes a Gaussian pro-
cess prior on the unobservable (latent) functions
and a likelihood function that accounts for noise.
An individual classifier3 was trained for each lex-
ical level and applied to the test data sets.

2Implementation provided in the Scikit library (Pedregosa
et al., 2011), where k is set to 3.

3Implementation available in the WEKA machine learn-
ing software (Hall et al., 2009) using the default parameters.

4 Evaluations & Discussion

Our system participated in all cross-level subtasks
under the name SimCompass, competing with 37
other systems developed by 20 teams.

Figure 1 highlights the Pearson correlations at
the four lexical levels between the gold standard
and each similarity measure introduced in Section
3, as well as the predictions ensuing as a result
of meta-learning. The left and right histograms in
each subfigure present the scores obtained on the
train and test data, respectively.

In the case of word2sense train data, we no-
tice that expanding the context provides additional
information and improves the correlation results.
For corpus-based measures, the correlations are
stronger when the expansion involves only the
right side of the tuple, namely the sense. We
notice an increase of 0.04 correlation points for
WTV1 and 0.09 for WTV2. As soon as the word
is expanded as well, the context incorporates too
much noise, and the correlation levels drop. In
the case of knowledge-based measures, expanding
the context does not seem to impact the results.
However, these trends do not carry out to the test
data, where the corpus-based features without ex-
pansion reach a correlation higher than 0.3, while
the knowledge-based features score significantly
lower (by 0.16). Once all these measures are used
as features in a meta learner (All) using Gaus-
sian processes regression (GP), the correlation in-
creases over the level attained by the best perform-
ing individual feature, reaching 0.45 on the train
data and 0.36 on the test data. SimCompass ranks
second in this subtask’s evaluations, falling short
of the leading system by 0.025 correlation points.

Turning now to the phrase2word subfigure, we
notice that the context already carries sufficient
information, and expanding it causes the perfor-
mance to drop (the more extensive the expan-
sion, the steeper the drop). Unlike the scenario
encountered for word2sense, the trend observed
here on the training data also gets mirrored in the
test data. Same as before, knowledge-based mea-
sures have a significantly lower performance, but
deep learning-based features based on word2vec
(WTV) only show a correlation variation by at
most 0.05, proving their robustness. Leveraging
all the features in a meta-learning framework en-
ables the system to predict stronger scores for both
the train and the test data (0.48 and 0.42, respec-
tively). Actually, for this variation, SimCompass

562



 0

 0.1

 0.2

 0.3

 0.4

 0.5
w

o
rd

2
se

n
se

Train Test

 0

 0.1

 0.2

 0.3

 0.4

 0.5

B
L

JN
C

L
IN

R
E
S

W
T
V

1
W

T
V

2
G

P

JN
C

L
IN

R
E
S

W
T
V

1
W

T
V

2
G

P

p
h

ra
se

2
w

o
rd

No expansion Expansion R Expansion L&R All

 0

 0.2

 0.4

 0.6

 0.8

se
n

te
n

ce
2

p
h

ra
se

Train Test

B
L

W
T
V

1
W

T
V

2
W

T
V

3
W

T
V

4
G

P

W
T
V

1
W

T
V

2
W

T
V

3
W

T
V

4
G

P

 0

 0.2

 0.4

 0.6

 0.8

p
ar

ag
ra

p
h

2
se

n
te

n
ce

Figure 1: Pearson correlation of individual measures on the train and test data sets. As these measures be-
come features in a regression algorithm (GP), prediction correlations are included as well. BL represents
the baseline computed by the organizers.

obtains the highest score among all competing sys-
tems, surpassing the second best by 0.10.

Noticing that expansion is not helpful when suf-
ficient context is available, for the next variations
we use the original tuples. Also, due to the re-
duced impact of knowledge-based features on the
training outcome, we only focus on deep learning
features (WTV1, WTV2, WTV3, WTV4).

Shifting to sentence2phrase, WTV2 (con-
structed using VectorSum) is the top perform-
ing feature, surpassing the baseline by 0.19,
and attaining 0.69 and 0.73 on the train and
test sets, respectively. Despite also considering
a lower performing feature (WTV1), the meta-
learner maintains high scores, surpassing the cor-
relation achieved on the train data by 0.04 (from
0.70 to 0.74). In this variation, our system ranks
fifth, at 0.035 from the top system.

For the paragraph2sentence variation, due to
the availability of longer contexts, we introduce
WTV3 and WTV4 that are based on clustering the
left and the right sides of the tuple, respectively.
WTV2 fares slightly better than WTV3 and WTV4.
WTV1 surpasses the baseline this time, leaving its
mark on the decision process. When training the
GP learner on all features, we obtain 0.78 correla-
tion on the train data, and 0.81 on test data, 0.10
higher than those attained by the individual fea-
tures alone. SimCompass ranks seventh in perfor-
mance on this subtask, at 0.026 from the first.

Considering the overall system performance,
SimCompass is remarkably versatile, ranking

among the top at each lexical level, and taking the
first place in the SemEval Task 3 overall evalu-
ation with respect to both Pearson (0.58 average
correlation) and Spearman correlations.

5 Conclusion

We described SimCompass, the system we partic-
ipated with at SemEval-2014 Task 3. Our exper-
iments suggest that traditional knowledge-based
features are cornered by novel corpus-based word
meaning representations, such as word2vec, which
emerge as efficient and strong performers under
a variety of scenarios. We also explored whether
context expansion is beneficial to the cross-level
similarity task, and remarked that only when the
context is particularly short, this enrichment is vi-
able. However, in a meta-learning framework, the
information permeating from a set of similarity
measures exposed to varying context expansions
can attain a higher performance than possible with
individual signals. Overall, our system ranked first
among 21 teams and 38 systems.

Acknowledgments

This material is based in part upon work sup-
ported by National Science Foundation CAREER
award #1361274 and IIS award #1018613 and
by DARPA-BAA-12-47 DEFT grant #12475008.
Any opinions, findings, and conclusions or recom-
mendations expressed in this material are those of
the authors and do not necessarily reflect the views

563



of the National Science Foundation or the Defense
Advanced Research Projects Agency.

References
Eneko Agirre, Daniel Cer, Mona Diab, and Aitor

Gonzalez-Agirre. 2012. SemEval-2012 Task 6: A
pilot on semantic textual similarity. In Proceedings
of the 6th International Workshop on Semantic Eval-
uation (SemEval 2012), in conjunction with the First
Joint Conference on Lexical and Computational Se-
mantics (*SEM 2012), Montreal, Canada.

Eneko Agirre, Daniel Cer, Mona Diab, Aitor Gonzalez-
Agirre, and Weiwei Guo. 2013. *SEM 2013 Shared
Task: Semantic Textual Similarity, including a Pi-
lot on Typed-Similarity. In The Second Joint Con-
ference on Lexical and Computational Semantics
(*SEM 2013).

Daniel Bär, Torsten Zesch, and Iryna Gurevych. 2013.
DKPro Similarity: An open source framework for
text similarity. In Proceedings of the 51st Annual
Meeting of the Association for Computational Lin-
guistics: System Demonstrations, pages 121–126,
Sofia, Bulgaria.

Curt Burgess, Kay Livesay, and Kevin Lund. 1998.
Explorations in context space: words, sentences,
discourse. Discourse Processes, 25(2):211–257.

Kenneth Church and Patrick Hanks. 1990. Word as-
sociation norms, mutual information, and lexicogra-
phy. Computational Linguistics, 16(1):22–29.

Evgeniy Gabrilovich and Shaul Markovitch. 2007.
Computing semantic relatedness using Wikipedia-
based explicit semantic analysis. In Proceedings of
the 20th International Joint Conference on Artificial
Intelligence, pages 1606–1611, Hyderabad, India.

Mark Hall, Eibe Frank, Geoffrey Holmes, Bernhard
Pfahringer, Peter Reutemann, and Ian H. Witten.
2009. The WEKA Data Mining Software: An Up-
date. SIGKDD Explorations, 11(1).

Samer Hassan and Rada Mihalcea. 2011. Measuring
semantic relatedness using salient encyclopedic con-
cepts. Artificial Intelligence, Special Issue.

Thad Hughes and Daniel Ramage. 2007. Lexical se-
mantic knowledge with random graph walks. In
Proceedings of the Conference on Empirical Meth-
ods in Natural Language Processing, Prague, Czech
Republic.

Aminul Islam and Diana Zaiu Inkpen. 2006. Second
order co-occurrence PMI for determining the seman-
tic similarity of words. In Proceedings of the Fifth
Conference on Language Resources and Evaluation,
volume 2, pages 1033–1038, Genoa, Italy, July.

Mario Jarmasz and Stan Szpakowicz. 2003. Roget’s
thesaurus and semantic similarity. In Proceedings

of the conference on Recent Advances in Natural
Language Processing RANLP-2003, Borovetz, Bul-
garia, September.

Jay J. Jiang and David W. Conrath. 1997. Semantic
similarity based on corpus statistics and lexical tax-
onomy. In Proceeding of the International Confer-
ence Research on Computational Linguistics (RO-
CLING X), Taiwan.

David Jurgens, Mohammad Taher Pilehvar, and
Roberto Navigli. 2014. SemEval-2014 Task 3:
Cross-Level Semantic Similarity. In Proceedings of
the 8th International Workshop on Semantic Evalu-
ation (SemEval-2014), Dublin, Ireland.

Thomas K. Landauer and Susan T. Dumais. 1997.
A solution to plato’s problem: The latent semantic
analysis theory of acquisition, induction, and repre-
sentation of knowledge. Psychological Review, 104.

Claudia Leacock and Martin Chodorow. 1998. Com-
bining local context and WordNet sense similarity
for word sense identification. In WordNet, An Elec-
tronic Lexical Database. The MIT Press.

Michael E. Lesk. 1986. Automatic sense disambigua-
tion using machine readable dictionaries: How to
tell a pine cone from an ice cream cone. In Pro-
ceedings of the SIGDOC Conference 1986, Toronto,
June.

Chin-Yew Lin and Eduard Hovy. 2003. Auto-
matic evaluation of summaries using n-gram co-
occurrence statistics. In Proceedings of Human Lan-
guage Technology Conference (HLT-NAACL 2003),
Edmonton, Canada, May.

Dekang Lin. 1998. An information-theoretic defini-
tion of similarity. In Proceedings of the Fifteenth In-
ternational Conference on Machine Learning, pages
296–304, Madison, Wisconsin.

Rada Mihalcea, Courtney Corley, and Carlo Strappa-
rava. 2006. Corpus-based and knowledge-based
measures of text semantic similarity. In American
Association for Artificial Intelligence (AAAI-2006),
Boston, MA.

Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey
Dean. 2013a. Distributed Representations of Words
and Phrases and their Compositionality . In NIPS,
pages 3111–3119.

Tomas Mikolov, Wen-tau Yih, and Geoffrey Zweig.
2013b. Linguistic regularities in continuous space
word representations. In NAACL HLT, pages 746–
751, Atlanta, GA, USA.

George A. Miller. 1995. WordNet: a Lexical database
for English. Communications of the Association for
Computing Machinery, 38(11):39–41.

Rao Muhammad Adeel Nawab, Mark Stevenson, and
Paul Clough. 2011. External plagiarism detection
using information retrieval and sequence alignment:

564



Notebook for PAN at CLEF 2011. In Proceedings
of the 5th International Workshop on Uncovering
Plagiarism, Authorship, and Social Software Misuse
(PAN 2011).

Kishore. Papineni, Salim Roukos, Todd Ward, and
Wei-Jing Zhu. 2002. Bleu: A method for automatic
evaluation of machine translation. In Proceedings of
the 40th Annual Meeting of the Association for Com-
putational Linguistics, pages 311–318, Philadelphia,
PA.

Fabian Pedregosa, Gaël Varoquaux, Alexandre Gram-
fort, Vincent Michel, Bertrand Thirion, Olivier
Grisel, Mathieu Blondel, Peter Prettenhofer, Ron
Weiss, Vincent Dubourg, Jake Vanderplas, Alexan-
dre Passos, David Cournapeau, Matthieu Brucher,
Matthieu Perrot, and Édouard Duchesnay. 2011.
Scikit-learn: Machine learning in Python. The Jour-
nal of Machine Learning Research, 12:2825–2830.

Radim Rehurek and Petr Sojka. 2010. Software frame-
work for topic modelling with large corpora. In Pro-
ceedings of the LREC 2010 Workshop on New Chal-
lenges for NLP Frameworks, pages 45–50, Valletta,
Malta, May. ELRA.

Philip Resnik. 1995. Using information content to
evaluate semantic similarity in a taxonomy. In Pro-
ceedings of the 14th International Joint Conference
on Artificial Intelligence, pages 448–453, Montreal,
Quebec, Canada. Morgan Kaufmann Publishers Inc.

J. Rocchio, 1971. Relevance feedback in information
retrieval. Prentice Hall, Ing. Englewood Cliffs, New
Jersey.

1995. Roget’s II: The New Thesaurus. Houghton Mif-
flin.

Gerard Salton and Michael E. Lesk, 1971. The SMART
Retrieval System: Experiments in Automatic Doc-
ument Processing, chapter Computer evaluation of
indexing and text processing. Prentice Hall, Ing. En-
glewood Cliffs, New Jersey.

Gerard Salton, Amit Singhal, Mandar Mitra, and Chris
Buckley. 1997. Automatic text structuring and sum-
marization. Information Processing and Manage-
ment, 2(32).

Hinrich Schutze. 1998. Automatic word sense dis-
crimination. Computational Linguistics, 24(1):97–
124.

Peter D. Turney. 2001. Mining the Web for Synonyms:
PMI-IR versus LSA on TOEFL. In Proceedings of
the 12th European Conference on Machine Learning
(ECML’01), pages 491–502, Freiburg, Germany.

565


