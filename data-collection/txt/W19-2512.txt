



















































Computational Linguistics Applications for Multimedia Services


Proc. of the 3rd Joint SIGHUM Workshop on Computational Linguistics for Cultural Heritage, Social Sciences, Humanities and Literature, pp. 91–97
Minneapolis, MN, USA, June 7, 2019. c©2019 Association for Computational Linguistics

91

Computational Linguistics Applications for Multimedia Services

Kyeongmin Rim Kelley Lynch

Department of Computer Science
Brandeis University
Waltham MA USA

{krim,kmlynch,jamesp}@brandeis.edu

James Pustejovsky

Abstract

We present Computational Linguistics Appli-
cations for Multimedia Services (CLAMS),
a platform that provides access to computa-
tional content analysis tools for archival mul-
timedia material that appear in different me-
dia, such as text, audio, image, and video.
The primary goal of CLAMS is: (1) to de-
velop an interchange format between multi-
modal metadata generation tools to ensure in-
teroperability between tools; (2) to provide
users with a portable, user-friendly workflow
engine to chain selected tools to extract mean-
ingful analyses; and (3) to create a public soft-
ware development kit (SDK) for developers
that eases deployment of analysis tools within
the CLAMS platform. CLAMS is designed
to help archives and libraries enrich the meta-
data associated with their mass-digitized mul-
timedia collections, that would otherwise be
largely unsearchable.

1 Introduction and Motivation
Since the invention of the phonograph and mov-
ing pictures, audiovisual materials have been one
of the primary methods of recording modern his-
tory alongside textual records. Many historical
events, important persons, social issues, and major
conflicts over the last several decades have been
recorded on such mass media. Researchers in both
media studies and the social sciences, as well as
historians have long recognized the value of audio
and visual records as evidence about the past (e.g.,
Boykoff and Boykoff, 2007; Dalton and Charnigo,
2004; Doms and Morin, 2004). Likewise, edu-
cators have appreciated the ability of multimedia
materials to make history and cultural heritage ar-
tifacts come alive in the classroom setting (e.g. Ott
and Pozzi, 2011; Antonaci et al., 2013). Recently,
with the advent of large digital storage, there have
been many large-scale projects aimed at the mass-
digitization of books (Christenson, 2011), newspa-

pers (NDNP, 2005), oral history (Oard et al., 2002;
NYPL, 2013), and public broadcasting (MDPI,
2014; AAPB, 2015). Selections of results from
these projects are publicly available through web-
based digital libraries, often accompanied by a
search interface. However, users of such digital li-
brary resources can be frustrated by the difficulties
associated with accessing these historical audiovi-
sual records, not because of any lack of accessi-
bility to the digital media themselves, but because
of the lack of accessibility to the contents of the
media (Schaffner, 2009). Audiovisual media, un-
like textual records, are opaque to even the sim-
plest text-based search capability. Finding content
relevant to one’s research question among thou-
sands of hours of audiovisual records, hence, is
time-consuming, involving watching or listening
to hours of contents. Therefore, a key to mak-
ing a digital multimedia archive useful and ac-
cessible is to generate and deploy rich metadata
of collection items (Cariani et al., 2015). The
availability of such descriptive, structured, textual
metadata about the content of the collections and
the included items radically improve the searcha-
bility and discoverability of the material (Puste-
jovsky et al., 2017). Yet, manually cataloging
meaningful and suitably robust metadata is a gen-
eral challenge across digital archives, as it will
also be time-consuming and laborious, involving
archivists watching and listening to items.

In this paper, we describe the CLAMS1 plat-
form, developed for libraries and archivists to help
enrich item-level descriptive metadata by provid-
ing with automatically extracted information from
time-based multimedia collections utilizing com-
putational analysis tools for text, audio, and video
(Pustejovsky, 2018). These tools for different
modalities will be orchestrated via CLAMS work-

1http://www.clams.ai

http://www.clams.ai


92

flow engine that provides a common interchange
format ensuring syntactic and semantic interoper-
ability between these tools.

2 Prior Work
Multilingual Access to Large Spoken Archives
(MALACH) (Oard et al., 2002) was one of the
early studies that used computational linguistics
tools to build an automatic metadata extraction
system. In MALACH, oral history recording data
was processed through automatic speech recogni-
tion (ASR) and natural language processing (NLP)
pipelines that extracted relevant information for
cataloging. In prototyping its World Service
Archive (Raimond et al., 2014), the BBC devel-
oped COMMA, an metadata extraction and linked
data-based interlinking system for public radio
broadcasts. Its outcome is now in use by the BBC
(BBC, 2015), however it is not publicly avail-
able. More recently, the EU funded Media in Con-
text (MiCO) project (Aichroth et al., 2015). This
project aimed at accomplishing a media analysis
platform for multimodal media that supports cus-
tomized workflows leveraging on assorted open
and closed source content analysis tools. An in-
teroperability layer, MiCO Broker, was developed
based on RDF and XML structures to chain dif-
ferent tools. Among the latest work, Audiovi-
sual Metadata Platform (AMP) is noteworthy as
it plans to design and develop a platform that ex-
ploits chains of automated tools and human-in-
the-loop to generate and manage metadata at in-
stitutional scale (Dunn et al., 2018). We actively
seek collaboration with others in order to move
closer to achieving a “global laboratory” for lan-
guage applications.

In the computational linguistics (CL) commu-
nity, UIMA (Ferrucci et al., 2009) and GATE
(Cunningham et al., 2013) have been long-
standing popular tool-chaining platforms for re-
searchers and NLP developers. Particularly,
UIMA provides an extremely general model of
type systems and annotations that can be applied
upon multimedia source data. However, there
is stiff learning curve behind its high generality,
combined with its tight binding with XML syn-
tax and Java programming language. More re-
cently, web-based workflow engines such as the
LAPPS Grid (Ide et al., 2014) and WebLicht
(Hinrichs et al., 2010) provide user friendly web
interfaces. Particularly, these web-based plat-
forms not only offer tool repositories of various

levels of state-of-the-art NLP tools for textual
data, such as CoreNLP (Manning et al., 2014),
OpenNLP (OpenNLP, 2017), but also implement
open source SDK for tool developers to promote
adoption. These workflow engines can operate dif-
ferent tools which are separately developed only
because of the underlying data interchange for-
mats that impose common I/O language between
those tools. For such an interchange format,
The LAPPS Grid uses LAPPS Interchange Format
(LIF) rooted on JSON-LD serialization (Verhagen
et al., 2015), while the WebLicht uses XML-based
Text Corpus Format (TCF) (Heid et al., 2010).
Additionally the LAPPS Grid defines a semantic
linked data vocabulary that ensures semantic in-
teroperability (Ide et al., 2015). Having imple-
mented in-platform interoperability has led to a
multi-platform collaboration between LAPPS and
CLARIN (Hinrichs et al., 2018).

3 Project Description
Figure 1 shows the overall structure of the plat-
form in a working environment as delivered to
an archive. As a platform, the primary goals of
CLAMS are 1) to develop an interchange format
between multimodal annotations that allows anal-
ysis tools for different modalities to work together
when chained into a single workflow, and 2) to
provide libraries and archivists a portable work-
flow engine software with a user-friendly interface
to select available tools and create workflows and
run them, and lastly 3) to offer various analysis
tools alongside a public SDK for developers of the
tools that allows easy adoption of the interchange
format and streamlined deployment to the work-
flow engine. In the rest of this section, we will
discuss how we address each of aforementioned
goals.

3.1 Multimodal Interoperability
To implement the platform with interoperating
analysis tools, we developed Multi-Media Inter-
change Format (MMIF) as the common tongue
of CLAMS. MMIF consists of two parts – it
adopts the already successful JSON-LD as syn-
tax, and an open linked data vocabulary for the
semantics of the terminology. The vocabulary is
re-using the LAPPS Grid vocabulary as its lin-
guistic terminology, while extending it further to
cover audiovisual concepts such as timeFrame,
or boundingBox.

Typologically, multimodal annotations in



93

Figure 1: Architectural sketch of CLAMS platform. Archives pull the containerized platform and services. The
platform runs as an orchestrated set of containers that are connected to local storage to grant access to the data
repository. Archivists interact with services to create, edit, and execute workflows only via the web-based front-
end workflow engine.

CLAMS are first categorized by the anchor type
on which the annotation is placed. That is, an
annotation can be placed on 1) character offsets
of a text, 2) time segments of time-based media,
3) two-dimensional (width × height) or three-
dimensional (w × h × duration) bounding boxes
on video frames, and 4) other annotation. For
instance, a named entity recognition
(NER) annotation can anchor on a token
annotation that in turn anchored on character
offsets. Furthermore, the characters can be from
primary text data or from other annotations (such
as ASR or optical character recognition (OCR)).
Next, annotations are further categorized by the
semantic types that are hierarchically defined in
CLAMS vocabulary. For example, white noise
detection and blank screen detection tools both
produce subcategories of the noisyFrame
annotation.

To address the complexity of additional anno-
tation types and I/O constraints on tools, a lay-
ered annotation structure proved to be the best
implementation choice for the interchange for-
mat based on many precedents, including LIF and
TCF. Specifically, in MMIF, each tool generates a
view object that contains all annotations as well
as information about the production of the view
(producer, production time, version, included an-
notation types, etc.). As a result, downstream tools
can precisely locate any required input annotations
from the input MMIF.

Last but not least, each tool deployed as a ser-
vice on CLAMS must expose an application pro-
gramming interface (API) to return its tool meta-
data, which contains information of the I/O con-
straints it poses. This tool metadata is used by the
workflow engine to validate tool chains before cre-
ation and execution of workflows.

3.2 Workflow Engine
In order to facilitate the development of metadata
generation workflows, we are using the Galaxy
platform. The Galaxy platform was originally de-
veloped for genomic research, but has successfully
been used for the deployment and integration of
NLP tools (Giardine et al., 2005; Ide et al., 2016).
Galaxy provides a web-based graphical user in-
terface which will allow archivists to import data,
construct complex multimodal workflows, and ex-
plore and visualize the metadata generated by ap-
plying workflows to their data.

3.3 CLAMS SDK and Services
We start with a number of fundamental analy-
sis tools for text, image, audio, and video as
CLAMS microservices. Users can easily config-
ure a CLAMS instance with various tools based
on specific needs, and then deploy it on a server
where the archival data is stored. Figure 2 shows
an example of a CLAMS instance configured with
a set of video services. It also shows creation of a
workflow of an ordered application of services to
a specific set of input data.



94

Figure 2: An example workflow created using the Galaxy workflow engine

The SDK including core APIs used in the devel-
opment and deployment of tools will release on an
open repository under open source license.

3.3.1 Text Services
As the design of the interoperability layer, the
MMIF, of CLAMS is largely inspired by and ex-
panding that of the LAPPS Grid platform (LIF).
The LAPPS Grid offers a wide range of text anal-
ysis services via its web-based SOAP API, and
re-using them in CLAMS can be done by map-
ping these SOAP messages out to CLAMS API.
These text analysis services include NER, pars-
ing, relation extraction, and coreference resolu-
tion. Used on audio transcripts and OCR results,
they will capture important entities, events, par-
ticipants, and relations that can be included in the
descriptive metadata.

3.3.2 Audiovisual Filtering Services
In spite of recent achievements in computer vision
(CV) and ASR, such tools are still very expen-
sive with respect to time and space to run. How-
ever, a video clip can include completely content-
less blank frames or SMPTE bars as well as non-
speech audio (music, natural sounds, beep, etc).
Thus, blindly feeding those expensive CV and
ASR tools with the entire clip can be not only a
waste of computing resources, but can also result
in introducing unnecessary noisy annotations. To
address this problem, we added a range of less ex-
pensive filtering services such as blank screen de-
tection, SMPTE bar detection, and HiPSTAS au-

dio tagger (Clement et al., 2014).

3.3.3 ASR and Forced Alignment
The platform will include open source tools to pro-
cess speech and audio from video and audio data.
Audio processing will include Kaldi-based ASR
which generates a transcript of the data that can
then be processed with NLP tools. Additionally,
CLAMS can provide forced alignment services
such as the Montreal Forced Aligner, which gener-
ates time-aligned transcriptions from raw text tran-
scripts (McAuliffe et al., 2017). These speech ser-
vices in particular are very important for multi-
modal annotation, as they provide alignment be-
tween a time-based modality and a character-
based modality.

3.3.4 Computer Vision Tools
Various types of metadata can be found in text
displayed in frames of a video. Slates are video
frames which display metadata such as air date,
director, producer, and title. This metadata can be
extracted by constructing a pipeline of computer
vision and NLP tools. Text localization tools can
detect the bounding boxes of text in a frame which
can then be used to label a section of a video as
a slate. Slate frames are then fed to a preprocess-
ing tool and an OCR tool. The OCR tool generates
unstructured text. Since the text generated through
OCR is likely to contain significant errors, a subse-
quent tool processes this text to correct spelling er-
rors and extract structured metadata from the cor-
rected text.



95

In news programs, when a reporter or guest is
introduced, it is common for their name and title to
be displayed at the bottom of the frame in a chyron
or “lower-third”. By applying OCR to chyrons, we
can identify names of people appearing in a video.
End credits contain production metadata such as
cast and crew which can also be recognized by ap-
plying OCR tools.

Face detection and recognition (FDR) can be
used to detect the location of faces in frames of
video and to cluster detected faces so that indi-
viduals can be identified across different scenes
within a video.

By integrating multiple vision and text based
tools into a pipeline, it is possible to generate
more robust metadata. For example, once clusters
of detected faces are identified, this metadata can
be combined with metadata from applying OCR
to chyrons. By combining these two metadata
sources, it will be possible to identify people in a
video even after the chyron is no longer displayed.
This metadata will be useful for researchers and
archivists who are searching for all of the video
segments in a dataset in which a particular person
appears.

4 On-going and Future Work
We are currently collaborating with the Ameri-
can Archive of Public Broadcasting (AAPB) at
WGBH Boston. The expertise of their archivists
and librarians, as well as their perspective as target
users, can provide us with insight towards select-
ing the analysis tools and phenomena of interest
that can potentially push forward the state-of-the-
art CL and CV technologies, within the vast unex-
plored collections of multimedia data. We actively
seek collaboration with others in order to move
closer to achieving an open platform for multi-
meida analysis.

We also believe that the platform can be used
in academic settings with multimodal research
datasets, such as MPII Movie Description dataset
(Rohrbach et al., 2015), oral histories (StoryCorps,
2003; Telling Their Stories, 2005), and the The
CHILDES Project (MacWhinney, 2014). For
more technically literate users in research commu-
nities, we plan to develop a scriptable workflow
engine extending the current SDK.

5 Conclusion
In this paper, we have presented CLAMS, a plat-
form for multimodal computational analysis tools

that provides interoperability between tools and
a portable graphical user interface (GUI) work-
flow engine. Together, these tools can be used to
automatically extract important information, such
as timestamps (airing time, event time), people,
companies, or historical events and relations, from
time-based audiovisual material. We believe that
archivists can use CLAMS over the digital mul-
timedia collections they have to enrich item-level
metadata of their collections and, in turn, greatly
enhance the searchability and discoverability of
their assets.

References
AAPB. 2015. American Archive of Public Broadcast-

ing. http://americanarchive.org/. Ac-
cessed: 2019-02-20.

Patrick Aichroth, Christian Weigel, Thomas Kurz,
Horst Stadler, Frank Drewes, Johanna Björklund,
Kai Schlegel, Emanuel Berndl, Antonio Perez, Alex
Bowyer, et al. 2015. Mico-media in context. In
2015 IEEE International Conference on Multimedia
& Expo Workshops (ICMEW), pages 1–4. IEEE.

Alessandra Antonaci, Michela Ott, and Francesca
Pozzi. 2013. Virtual museums, cultural heritage ed-
ucation and 21st century skills. Learning & Teach-
ing with Media & Technology, 185.

BBC. 2015. COMMA - BBC R & D. https://
www.bbc.co.uk/rd/projects/comma. Ac-
cessed: 2019-02-20.

Maxwell T Boykoff and Jules M Boykoff. 2007. Cli-
mate change and journalistic norms: A case-study
of us mass-media coverage. Geoforum, 38(6):1190–
1204.

Karen Cariani, Sadie Roosa, Jack Brighton, and Brian
Grane. 2015. Accelerating exposure of audiovisual
collections: What’s next? In Innovation, Collabo-
ration, and Models: Proceedings of the CLIR Cat-
aloging Hidden Special Collections and Archives
Symposium.

Heather Christenson. 2011. Hathitrust. Library Re-
sources & Technical Services, 55(2):93–102.

Tanya E Clement, David Tcheng, Loretta Auvil, and
Tony Borries. 2014. High performance sound tech-
nologies for access and scholarship (hipstas) in the
digital humanities. Proceedings of the American
Society for Information Science and Technology,
51(1):1–10.

Hamish Cunningham, Valentin Tablan, Angus Roberts,
and Kalina Bontcheva. 2013. Getting more out of
biomedical documents with gate’s full lifecycle open
source text analytics. PLoS computational biology,
9(2):e1002854.

http://americanarchive.org/
https://www.bbc.co.uk/rd/projects/comma
https://www.bbc.co.uk/rd/projects/comma


96

Margaret Stieg Dalton and Laurie Charnigo. 2004.
Historians and their information sources. College
& Research Libraries, 65(5):400–425.

Mark Doms and Norman J. Morin. 2004. Consumer
sentiment, the economy, and the news media. Fi-
nance and Economics Discussion Series 2004-51,
Board of Governors of the Federal Reserve System
(US).

Jon W Dunn, Juliet L Hardesty, Tanya Clement, Chris
Lacinak, and Amy Rudersdorf. 2018. Audiovisual
metadata platform (amp) planning project: Progress
report and next steps. Technical report.

David Ferrucci, Adam Lally, Karin Verspoor, and Eric
Nyberg. 2009. Unstructured information manage-
ment architecture (UIMA) version 1.0. OASIS Stan-
dard.

Belinda Giardine, Cathy Riemer, Ross C Hardison,
Richard Burhans, Laura Elnitski, Prachi Shah,
Yi Zhang, Daniel Blankenberg, Istvan Albert, James
Taylor, et al. 2005. Galaxy: a platform for interac-
tive large-scale genome analysis. Genome research,
15(10):1451–1455.

Ulrich Heid, Helmut Schmid, Kerstin Eckart, and Er-
hard Hinrichs. 2010. A corpus representation for-
mat for linguistic web services: The d-spin text cor-
pus format and its relationship with iso standards.
In LREC2010, Valletta, Malta. European Language
Resources Association (ELRA).

Erhard Hinrichs, Marie Hinrichs, and Thomas Zas-
trow. 2010. Weblicht: Web-based LRT services for
german. In Proceedings of the ACL 2010 System
Demonstrations, pages 25–29. Association for Com-
putational Linguistics.

Erhard Hinrichs, Nancy Ide, James Pustejovsky, Jan
Hajic, Marie Hinrichs, Mohammad Fazleh Elahi,
Keith Suderman, Marc Verhagen, Kyeongmin Rim,
Pavel Stranak, and Jozef Misutka. 2018. Bridg-
ing the LAPPS Grid and CLARIN. In LREC2018,
Miyazaki, Japan. European Language Resources
Association (ELRA).

Nancy Ide, James Pustejovsky, Christopher Cieri, Eric
Nyberg, Di Wang, Keith Suderman, Marc Verhagen,
and Jonathan Wright. 2014. The language applica-
tion grid. In LREC2014, Reykjavik, Iceland. Euro-
pean Language Resources Association (ELRA).

Nancy Ide, James Pustejovsky, Keith Suderman, Marc
Verhagen, Christopher Cieri, and Eric Nyberg. 2016.
The Language Application Grid and Galaxy. In
LREC 2016, pages 51–70.

Nancy Ide, Keith Suderman, Marc Verhagen, and
James Pustejovsky. 2015. The language applica-
tion grid web service exchange vocabulary. In Inter-
national Workshop on Worldwide Language Service
Infrastructure, pages 18–32. Springer.

Brian MacWhinney. 2014. The CHILDES project:
Tools for analyzing talk, Volume I: Transcription for-
mat and programs. Psychology Press.

Christopher D. Manning, Mihai Surdeanu, John Bauer,
Jenny Finkel, Steven J. Bethard, and David Mc-
Closky. 2014. The Stanford CoreNLP natural lan-
guage processing toolkit. In Association for Compu-
tational Linguistics (ACL) System Demonstrations,
pages 55–60.

Michael McAuliffe, Michaela Socolof, Sarah Mi-
huc, Michael Wagner, and Morgan Sonderegger.
2017. Montreal forced aligner: Trainable text-
speech alignment using kaldi. In INTERSPEECH.

MDPI. 2014. Media Digitization & Preservation Ini-
tiative. https://mdpi.iu.edu/. Accessed:
2019-02-20.

NDNP. 2005. National digital newspaper program
ndnp: a partnership between the library of congress
and the national endowment for the humanities.
https://lccn.loc.gov/2005567119. Ac-
cessed: 2019-02-20.

NYPL. 2013. The New York Public Library’s
Community Oral History Project. http://
oralhistory.nypl.org/. Accessed: 2019-
02-20.

Douglas W Oard, Dina Demner-Fushman, Jan Hajič,
Bhuvana Ramabhadran, Samuel Gustman, William J
Byrne, Dagobert Soergel, Bonnie Dorr, Philip
Resnik, and Michael Picheny. 2002. Cross-language
access to recorded speech in the malach project. In
International Conference on Text, Speech and Dia-
logue, pages 57–64. Springer.

OpenNLP. 2017. Apache OpenNLP. https://
opennlp.apache.org/. Accessed: 2019-02-
20.

Michela Ott and Francesca Pozzi. 2011. Towards a
new era for cultural heritage education: Discussing
the role of ict. Computers in Human Behavior,
27(4):1365–1371.

James Pustejovsky. 2018. Enhancing access to me-
dia collections and archives using computational lin-
guistic tools. In Proceedings of Enhancing Explo-
ration of Audiovisual Collections with Computer-
based Annotation Techniques, Workshop at AMIA.

James Pustejovsky, Nancy Ide, Marc Verhagen, and
Keith Suderman. 2017. Enhancing access to me-
dia collections and archives using computational lin-
guistic tools. In Proceedings of the Corpora for
Digital Humanities Workshop, pages 19–28. Asso-
ciation for Computational Linguistics.

Yves Raimond, Tristan Ferne, Michael Smethurst, and
Gareth Adams. 2014. The bbc world service archive
prototype. Web Semantics: Science, Services and
Agents on the World Wide Web, 27:2–9.

https://EconPapers.repec.org/RePEc:fip:fedgfe:2004-51
https://EconPapers.repec.org/RePEc:fip:fedgfe:2004-51
https://docs.oasis-open.org/uima/v1.0/uima-v1.0.html
https://docs.oasis-open.org/uima/v1.0/uima-v1.0.html
http://aclweb.org/anthology/P10-4005
http://aclweb.org/anthology/P10-4005
https://doi.org/10.1007/978-3-319-31468-6_4
http://www.aclweb.org/anthology/P/P14/P14-5010
http://www.aclweb.org/anthology/P/P14/P14-5010
https://mdpi.iu.edu/
https://lccn.loc.gov/2005567119
http://oralhistory.nypl.org/
http://oralhistory.nypl.org/
https://opennlp.apache.org/
https://opennlp.apache.org/


97

Anna Rohrbach, Marcus Rohrbach, Niket Tandon, and
Bernt Schiele. 2015. A dataset for movie descrip-
tion. In Proceedings of the IEEE Conference on
Computer Vision and Pattern Recognition (CVPR).

Jennifer Schaffner. 2009. The metadata is the interface:
Better description for better discovery of archives
and special collections, synthesized from user
studies. http://www.oclc.org/programs/
publications/reports/2009-06.pdf.

StoryCorps. 2003. Storycorps - stories from peo-
ple of all backgrounds and beliefs. https://
storycorps.org/. Accessed: 2019-04-04.

Telling Their Stories. 2005. Telling their stories
oral history archives project. http://www.
tellingstories.org/. Accessed: 2019-04-
04.

Marc Verhagen, Keith Suderman, Di Wang, Nancy
Ide, Chunqi Shi, Jonathan Wright, and James Puste-
jovsky. 2015. The lapps interchange format. In In-
ternational Workshop on Worldwide Language Ser-
vice Infrastructure, pages 33–47. Springer.

http://www.oclc.org/programs/publications/reports/2009-06.pdf
http://www.oclc.org/programs/publications/reports/2009-06.pdf
https://storycorps.org/
https://storycorps.org/
http://www.tellingstories.org/
http://www.tellingstories.org/

