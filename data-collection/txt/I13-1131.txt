










































Structure Cognizant Pseudo Relevance Feedback


International Joint Conference on Natural Language Processing, pages 982–986,
Nagoya, Japan, 14-18 October 2013.

Structure Cognizant Pseudo Relevance Feedback

Arjun Atreya V, Yogesh Kakde, Pushpak Bhattacharyya, Ganesh Ramakrishnan
CSE Department, IIT Bombay, Mumbai

{arjun,pb,ganesh}@cse.iitb.ac.in,yrkakde@gmail.com

Abstract

We propose a structure cognizant frame-
work for pseudo relevance feedback
(PRF). This has an application, for ex-
ample, in selecting expansion terms for
general search from subsets such as
Wikipedia, wherein documents typically
have a minimally fixed set of fields, viz.,
Title, Body, Infobox and Categories. In
existing approaches to PRF based expan-
sion, weights of expansion terms do not
depend on their field(s) of origin. This,
we feel, is a weakness of current PRF ap-
proaches. We propose a per field EM for-
mulation for finding the importance of the
expansion terms, in line with traditional
PRF. However, the final weight of an ex-
pansion term is found by weighting these
importance based on whether the term be-
longs to the title, the body, the infobox
or the category field(s). In our experi-
ments with four languages, viz., English,
Spanish, Finnish and Hindi, we find that
this structure-aware PRF yields a 2% to
30% improvement in performance (MAP)
over the vanilla PRF. We conduct ablation
tests to evaluate the importance of vari-
ous fields. As expected, results from these
tests emphasize the importance of fields in
the order of title, body, categories and in-
fobox.

1 Introduction

The ruling paradigm for Information retrieval (IR)
(Manning et al., 2009) is Pseudo Relevance feed-
back (PRF). In PRF, an assumption is made that
the top retrieved documents are relevant to the
query for picking expansion terms. Zhai and Laf-
ferty (2001) show that using pseudo relevance
feedback on monolingual retrieval improves the

overall result considerably over the retrieval with-
out PRF. In case of retrieval for languages with lit-
tle web content, Chinnakotla et al., (2010) show
that taking help of another language to expand
query helps in better performance.

The motivation for our work is as follows. Ev-
ery document in the web collection has certain
structure associated with it viz., title, body, links,
etc. Each of these fields has different level of
importance in the document. For instance, docu-
ment title broadly describes the whole document,
whereas the body of the document contains the de-
tails. Content in these fields have different scales
of contribution in uniquely representing that doc-
ument in the collection. Hence it is important to
consider the structure of a document while extract-
ing expansion terms from it.

Structure based PRF, of course, draws on the
basic theory of PRF as in Zhai and Lafferty
(2001), which is based on expectation maximiza-
tion (EM). We formulate a per field EM to get the
weights of expansion terms and subsequently take
their weighted sum in a spirit similar to mixture
models.

2 Related Work

Approaches based on the use of external resources
like wordnet for query expansion, though ex-
tensively studied, have been eventually dropped
(Gong et al., 2005; Qiu and Frei, 1993). Sev-
eral works have also used structure of documents
for query expansion. These works propose the
technique of first choosing relevant documents and
finding expansion terms, therefrom, using cooc-
curence, meta tags etc. Al-Shboul and Myaeng
(2011) use categories of Wikipedia pages to clus-
ter documents and retrieve the relevant cluster for
query. This approach gives better recall at the cost
of precision.

Anchor texts in Wikipedia pages pointing to a
category same as the query category are picked

982



as expansion terms in Ganesh and Verma (2009).
This work exploits the structure only in the form
of anchor texts and category information.

Techniques to disambiguate query terms based
on disambiguation pages of Wikipedia are pro-
posed in (Xu et al., 2009; Lin et al., 2010). Once
disambiguated, the page is considered for picking
expansion terms. Other literatures that deal with
PRF based IR are (Milne. et al., 2007; Lin and
Wu, 2008; Lv and Zhai, 2010; Jiang, 2011).

3 Our System

We make use of Wikipedia as an external docu-
ment collection for picking expansion terms. Rea-
sons for this are: a) open source b) well-defined
structure c) authenticity due to crowdsourcing and
review, d) coverage across domains and languages
e) ever growing. Four fields from the Wikipedia
document are considered viz., title, body, cate-
gories and infobox.

Our problem statement is:

Given a query Q in a language L, re-
trieve relevant results from any docu-
ment collection (WWW/dataset) in L us-
ing Wikipedia documents in L for gener-
ating expansion terms.

The process of PRF based retrieval involves the
following steps.

1. Retrieve ranked list of Wikipedia documents
for a given query Q- RetrievalModel (Sec-
tion 3.1)

2. Pick expansion terms from the top k retrieved
documents-ExpansionModel (Section 3.2)

3. Obtain a modified query Q′ by combining
the expansion terms with the query terms-
AggregationModel (Section 3.3)

4. Retrieve ranked list of documents for the
modified query Q′- RetrievalModel (Sec-
tion 3.1)

3.1 Retrieval Model
Language model based retrieval is used in (Ponte
and Croft, 1998) and (Croft, 2003). For every
document D, θD is the probability distribution of
terms. Similarly, θQ is for the query Q. The "dis-
tance" between the query and a document, DKL is
calculated as equation 1.

DKL (θQ|θD) = −
∑
w

P (w|θQ)logP (w|θD)

(1)

The more the relevance of D, the less is
DKL (θQ|θD).

3.2 Expansion Model

This model picks expansion terms that get com-
bined with the query. Choosing expansion terms
involves selecting a set of relevant documents and
identifying terms that uniquely represent them.
We use the retrieval model mentioned in section
3.1 to pick top k documents.

There exist many off-the-shelf expansion mod-
els to choose expansion terms from (Ganesh and
Verma, 2009; Al-Shboul and Myaeng, 2011).
None of these, however, exploit the structure of
relevant documents. (Zhai and Lafferty, 2001) ex-
plain one of the state of art techniques to choose
expansion terms using EM algorithm without con-
sidering the structure of a document. In Zhai and
Lafferty (2001), a set of relevant documents R is
retrieved and all terms in these documents are con-
sidered as observations. Since R is a subset of the
document collection C, all terms in R also appear
in C. Both R and C act as sources for generating
terms.

Given a document, the content in each field of
the document represents the document with dif-
ferent levels of importance. In our expansion
model, we use Wikipedia as the source of expan-
sion terms. Every Wikipedia document is com-
posed of four fields title, body, category and in-
fobox.

Expansion terms are picked independently from
each field of the Wikipedia document. We run
EM algorithm on each field as explained in Zhai
and Lafferty (2001). We formulate an EM algo-
rithm for pickng expansion terms from Title field
instead from a document as the whole. Body,
Categories and Infobox fields follow the same
formulation. The probability of all title terms in R
(PRtk) is maximized using EM algorithm. Sim-
ilarly, body terms, category terms and infobox
terms are also maximized.

The output of interest in an iterative EM algo-
rithm is the set of expansion terms for every field.
EM algorithm gives the weights of the expansion
terms, indicating their importance. Weighted com-
bination of these sets of expansion terms from
different fields of the document leads to the fi-
nal set of expansion terms. Empirically decided
weights (α’s) are used for combining expansion
terms from different fields as shown in the equa-

983



Dataset Query set No.of documents
English FIRE 2010 76-125(50) 1,25,586
Spanish ELRA-E0036 41-200(160) 4,54,045
Finnish ELRA-E0036 91-250(160) 55,344
Hindi FIRE 2010 76-125(50) 95,216

Table 1: Details of Experimental Setup; numbers
is parenthesis indicate the number of queries

tion 2. αx indicates the importance given to the
document field x.

PRk = αt ·PRtk +αb ·PRbk +αc ·PRck +αi ·PRik
(2)

where αt + αb + αc + αi = 1

3.3 Aggregation Model
Once expansion terms are picked from Wikipedia
documents, they are merged with initial query
terms. Introducing expansion terms increases the
possibility of topic drift for the intended informa-
tion need. Hence, it is important to give more
weight to query terms compared to expansion
terms. The equation 3 indicates the aggregation
of query Q with the expansion terms E to get the
modified query Q′ with λ as the weight given to
the query over the expansion terms.

Q′ = λQ+ (1− λ)E (3)

4 Experimental Setup

We conduct experiments to evaluate the effect
of document structure on expansion terms, us-
ing ELRA-E00361(part of CLEF) and FIRE 20102

datasets. Experiments are done in four languages,
English, Spanish, Finnish and Hindi. Following
are the set of experiments conducted:

NORF- No relevance feedback: This is the sim-
plest form of retrieval without using any expan-
sion.

PRF- Pseudo relevance feedback without using
the structure of a document: This is traditional
PRF. All terms in Wikipedia are considered to be
equally important, and the naive expansion model
of (Zhai and Lafferty, 2001) is used to find expan-
sion terms.

StructPRF- Pseudo relevance feedback using
the structure of a document: This is our proposed
model. Structure of Wikipedia documents is used
for finding expansion terms using the model de-
scribed in section 3.2.

1http://catalog.elra.info/product_info.php?products_id=1127
2http://www.isical.ac.in/∼fire/data.html

NORF PRF StructPRF
English 0.1758 0.2022 (+15%) 0.2189 (+24.5%)
Spanish 0.0433 0.1352 (+212%) 0.1778 (+310%)
Finnish 0.1532 0.2477 (+61.6%) 0.2517 (+64.3%)
Hindi 0.2321 0.2364 (+1.8%) 0.2529 (+9%)

Table 2: MAP scores; plus(+) indicates improve-
ment over NORF

NORF PRF StructPRF
English (2761) 1888 2080 2138
Spanish (2694) 391 1818 1919
Finnish (1377) 243 875 974

Hindi (915) 748 780 785

Table 3: Relevant documents retrieved; numbers
in parenthesis indicate the actual relevant docu-
ments

Table 1 describes the experimental details. For
every query, 1000 results are retrieved and used
for evaluation. All languages use their respective
Wikipedia content for picking expansion terms.

5 Results

MAP scores are shown in table 2. StructPRF
has an overall improvement in MAP of 8% for En-
glish, 30% for Spanish, 2% for Finnish and 7% for
Hindi over PRF . Figure 1 shows average preci-
sion values of all queries at different result posi-
tions for all languages. It is observed that there
is a definite improvement in precision values for
StructPRF over PRF. As we go down the list of
retrievals (P@k, with k increasing), the improve-
ment in StructPRF decreases but never gets below
PRF and NORF .

Figure 2 depicts precision vs. recall curves for
all languages. The results indicate that the Struct-
PRF has a better precision for most recall val-

 0

 0.05

 0.1

 0.15

 0.2

 0.25

 0.3

 0.35

 0.4

 50  100  150  200  250  300  350  400  450  500

P
re

c
is

io
n

Results at position ’k’

StructPRF
PRF

NORF

(a) English

 0

 0.05

 0.1

 0.15

 0.2

 0.25

 50  100  150  200  250  300  350  400  450  500

P
re

c
is

io
n

Results at position ’k’

StructPRF
PRF

NORF

(b) Spanish

 0

 0.05

 0.1

 0.15

 0.2

 0.25

 0.3

 20  40  60  80  100  120  140  160  180  200

P
re

c
is

io
n

Results at position ’k’

StructPRF
PRF

NORF

(c) Finnish

 0

 0.05

 0.1

 0.15

 0.2

 0.25

 0.3

 0.35

 0.4

 20  40  60  80  100  120  140  160  180  200

P
re

c
is

io
n

Results at position ’k’

StructPRF
PRF

NORF

(d) Hindi

Figure 1: P@k Values

984



English Spanish Finnish Hindi
NoTitle 0.1953(-11%) 0.1179(-33%) 0.1914(-23%) 0.2086(-17%)
NoBody 0.2059(-6%) 0.1383(-22%) 0.2333(-8%) 0.2185(-13%)

NoCategories 0.2172(-0.7%) 0.1436(-19%) 0.2358(-7%) 0.2209(-12%)
NoInfobox 0.2178(-0.5%) 0.1467(-17%) 0.2449(-3%) 0.2234(-11%)

Table 4: MAP scores for ablation tests; minus(-) indicates percentage decrease from StructPRF

 0

 0.1

 0.2

 0.3

 0.4

 0.5

 0.6

 0.7

 0.8

 0  0.2  0.4  0.6  0.8  1

P
re

c
is

io
n

Recall

StructPRF
PRF

NORF

(a) English

 0

 0.1

 0.2

 0.3

 0.4

 0.5

 0.6

 0.7

 0  0.2  0.4  0.6  0.8  1

P
re

c
is

io
n

Recall

StructPRF
PRF

NORF

(b) Spanish

 0.1

 0.2

 0.3

 0.4

 0.5

 0.6

 0.7

 0  0.2  0.4  0.6  0.8  1

P
re

c
is

io
n

Recall

StructPRF
PRF

NORF

(c) Finnish

 0

 0.1

 0.2

 0.3

 0.4

 0.5

 0.6

 0.7

 0.8

 0.9

 0  0.2  0.4  0.6  0.8  1

P
re

c
is

io
n

Recall

StructPRF
PRF

NORF

(d) Hindi

Figure 2: Precision-Recall Curve

ues. At 60% to 80% recall, precision of PRF
is better than StructPRF in English. This in-
dicates that most of the relevant documents are
pushed higher up the order in the result set. For
Spanish and Finnish, StructPRF consistently
outperforms PRF . In Hindi, between 40% to
60% recall, PRF has a higher precision than
StructPRF . This is again because of the rele-
vant documents being pushed higher in the ranked
list.

Analyzing query wise performances ofNORF ,
PRF and StructPRF for all languages, we ob-
served that StructPRF has best precision com-
pared to other two for ≈60% of queries in all lan-
guages.

Table 3 shows that there is an improvement in
the number of relevant documents retrieved by
StructPRF compared to PRF for all languages.
StructPRF has an improvement of 2.8%, 5%,
11% and 0.8% recall in English, Spanish, Finnish
and Hindi respectively over PRF .

From these results it is evident that structure
cognizant PRF benefits retrieval performance in
terms of both precision and recall.

6 Ablation Tests

In ablation tests, we "disable" one field, that is, do
not take expansion terms from a field, and get the
MAP score. For instance, NoTitle has body, cat-

egories and infobox with equal weights (i.e., 1/3)
and weight of the title field as 0.

Table 4 lists the MAP scores for all cases of ab-
lation. The name of each of these cases indicates
the field "disabled". It is observed that the worst
degradation in MAP occurs on disabling the Title
field. This happens for all languages. The degra-
dation decreases in the order of Title, Body, Cate-
gories and Infobox.

The above observation translates to setting val-
ues for αt, αb, αc and αi described in section 3.2
as αt > αb > αc > αi with αt+αb+αc+αi = 1.
Hence the choice of α’s for experimentation are
0.4, 0.3, 0.2 and 0.1 for αt, αb, αc and αi respec-
tively.

The fields being important in the order of Ti-
tle, Body, Categories and Infobox is quite intuitive.
This is because the Title represents the content of
the document with a few words. Hence, the Title
field has a larger impact as compared to the Body
field. Though Categories and Infobox have
lesser words, like Title, they refer to a generic
context of the query.

7 Conclusions and Future Direction

In this paper, we have explored the usage of doc-
ument structure for PRF. We proposed an expan-
sion model that considers each field of the doc-
ument with different levels of importance in pick-
ing expansion terms. This structure cognizant PRF
is compared with both traditional PRF and with
no-feedback, for four languages, English, Span-
ish, Finnish and Hindi. Experimental results show
that using structure helps in getting considerable
improvement in both precision and recall over tra-
ditional PRF. Ablation tests reveal the relative im-
portance of the fields, with "title" field proving
more important than others.

In our work, we combine expansion terms ob-
tained from every field of a document in a decou-
pled way, that is, through separate per field EMs.
In future, we would like to explore tight coupling
of document fields (EM over individual per-field
EM).

985



References
Bashar Al-Shboul and Sung-Hyon Myaeng. 2011.

Query phrase expansion using wikipedia in patent
class search. In AIRS, pages 115–126.

Manoj K. Chinnakotla, Karthik Raman, and Pushpak
Bhattacharyya. 2010. Multilingual prf: english
lends a helping hand. In Proceedings of the 33rd in-
ternational ACM SIGIR conference on Research and
development in information retrieval, SIGIR ’10,
pages 659–666, New York, NY, USA. ACM.

W Bruce Croft. 2003. Language models for informa-
tion retrieval. In Proceedings of 19th international
conference on data engineering, pages 3–7.

Surya Ganesh and Vasudeva Verma. 2009. Exploit-
ing structure and content of wikipedia for query ex-
pansion in the context. In International Conference
RANLP, pages 103–106.

Zhiguo Gong, Chan Wa Cheang, and U Leong Hou.
2005. Web query expansion by wordnet. In In
DEXA, pages 166–175.

Xue Jiang. 2011. Query expansion based on a seman-
tic graph model. In Proceedings of the 34th inter-
national ACM SIGIR conference on Research and
development in Information Retrieval, SIGIR ’11,
pages 1315–1316, New York, NY, USA. ACM.

Tien-Chien Lin and Shih-Hung Wu. 2008. Query ex-
pansion via wikipedia link. In ITIA’08:The 2008 In-
ternational Conference on Information Technology
and Industrial Application.

Meng-Chun Lin, Ming-Xiang Li, Chih-Chuan Hsu,
and Shih-Hung Wu. 2010. Query expansion from
wikipedia and topic web crawler on clir. In Proceed-
ings of NTCIR-8 Workshop Meeting, June 15-18.

Yuanhua Lv and ChengXiang Zhai. 2010. Positional
relevance model for pseudo-relevance feedback. In
Proceedings of the 33rd international ACM SIGIR
conference on Research and development in infor-
mation retrieval, SIGIR ’10, pages 579–586, New
York, NY, USA. ACM.

Christopher D. Manning, Prabhakar Raghavan, and
Hinrich Schütze. 2009. An Inroduction to informa-
tion Retrieval. Cambridge University Press, Cam-
bridge, England.

D Milne., Witten. I.H, and Nichols. D.M. 2007.
A knowledge-based search engine powered by
wikipedia. In ACM Conference on Information and
Knowledge Management.

Jay M Ponte and W Bruce Croft. 1998. A language
modeling approach to information retrieval. In Pro-
ceedings of 21st annual international ACM SIGIR
conference on research and development in informa-
tion retrieval, pages 275–281.

Yonggang Qiu and Hans-Peter Frei. 1993. Concept
based query expansion. In Proceedings of the 16th
annual international ACM SIGIR conference on Re-
search and development in information retrieval, SI-
GIR ’93, pages 160–169, New York, NY, USA.
ACM.

Yang Xu, Gareth J.F. Jones, and Bin Wang. 2009.
Query dependent pseudo-relevance feedback based
on wikipedia. In Proceedings of the 32nd inter-
national ACM SIGIR conference on Research and
development in information retrieval, SIGIR ’09,
pages 59–66, New York, NY, USA. ACM.

Chengxiang Zhai and John Lafferty. 2001. Model-
based feedback in the language modeling approach
to information retrieval. In Proceedings of the tenth
international conference on Information and knowl-
edge management, CIKM ’01, pages 403–410, New
York, NY, USA. ACM.

986


