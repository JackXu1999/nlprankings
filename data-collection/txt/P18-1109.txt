



















































Gaussian Mixture Latent Vector Grammars


Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers), pages 1181–1189
Melbourne, Australia, July 15 - 20, 2018. c©2018 Association for Computational Linguistics

1181

Gaussian Mixture Latent Vector Grammars

Yanpeng Zhao, Liwen Zhang, Kewei Tu
School of Information Science and Technology,

ShanghaiTech University, Shanghai, China
{zhaoyp1,zhanglw1,tukw}@shanghaitech.edu.cn

Abstract

We introduce Latent Vector Grammars
(LVeGs), a new framework that extends la-
tent variable grammars such that each non-
terminal symbol is associated with a con-
tinuous vector space representing the set
of (infinitely many) subtypes of the non-
terminal. We show that previous models
such as latent variable grammars and com-
positional vector grammars can be inter-
preted as special cases of LVeGs. We then
present Gaussian Mixture LVeGs (GM-
LVeGs), a new special case of LVeGs that
uses Gaussian mixtures to formulate the
weights of production rules over subtypes
of nonterminals. A major advantage of us-
ing Gaussian mixtures is that the partition
function and the expectations of subtype
rules can be computed using an extension
of the inside-outside algorithm, which en-
ables efficient inference and learning. We
apply GM-LVeGs to part-of-speech tag-
ging and constituency parsing and show
that GM-LVeGs can achieve competitive
accuracies. Our code is available at
https://github.com/zhaoyanpeng/lveg.

1 Introduction

In constituency parsing, refining coarse syntactic
categories of treebank grammars (Charniak, 1996)
into fine-grained subtypes has been proven effec-
tive in improving parsing results. Previous ap-
proaches to refining syntactic categories use tree
annotations (Johnson, 1998), lexicalization (Char-
niak, 2000; Collins, 2003), or linguistically mo-
tivated category splitting (Klein and Manning,
2003). Matsuzaki et al. (2005) introduce latent
variable grammars, in which each syntactic cate-
gory (represented by a nonterminal) is split into

a fixed number of subtypes and a discrete latent
variable is used to indicate the subtype of the
nonterminal when it appears in a specific parse
tree. Since the latent variables are not observ-
able in treebanks, the grammar is learned using
expectation-maximization. Petrov et al. (2006)
present a split-merge approach to learning la-
tent variable grammars, which hierarchically splits
each nonterminal and merges ineffective splits.
Petrov and Klein (2008b) further allow a nonter-
minal to have different splits in different produc-
tion rules, which results in a more compact gram-
mar.

Recently, neural approaches become very pop-
ular in natural language processing (NLP). An im-
portant technique in neural approaches to NLP
is to represent discrete symbols such as words
and syntactic categories with continuous vectors
or embeddings. Since the distances between
such vector representations often reflect the sim-
ilarity between the corresponding symbols, this
technique facilitates more informed smoothing in
learning functions of symbols (e.g., the probability
of a production rule). In addition, what a symbol
represents may subtly depend on its context, and a
continuous vector representation has the potential
of representing each instance of the symbol in a
more precise manner. For constituency parsing,
recursive neural networks (Socher et al., 2011)
and their extensions such as compositional vector
grammars (Socher et al., 2013) can be seen as rep-
resenting nonterminals in a context-free grammar
with continuous vectors. However, exact inference
in these models is intractable.

In this paper, we introduce latent vector gram-
mars (LVeGs), a novel framework of grammars
with fine-grained nonterminal subtypes. A LVeG
associates each nonterminal with a continuous
vector space that represents the set of (infinitely
many) subtypes of the nonterminal. For each in-

https://github.com/zhaoyanpeng/lveg


1182

stance of a nonterminal that appears in a parse
tree, its subtype is represented by a latent vector.
For each production rule over nonterminals, a non-
negative continuous function specifies the weight
of any fine-grained production rule over subtypes
of the nonterminals. Compared with latent vari-
able grammars which assume a small fixed num-
ber of subtypes for each nonterminal, LVeGs as-
sume an unlimited number of subtypes and are
potentially more expressive. By having weight
functions of varying smoothness for different pro-
duction rules, LVeGs can also control the level
of subtype granularity for different productions,
which has been shown to improve the parsing ac-
curacy (Petrov and Klein, 2008b). In addition,
similarity between subtypes of a nonterminal can
be naturally modeled by the distance between the
corresponding vectors, so by using continuous and
smooth weight functions we can ensure that simi-
lar subtypes will have similar syntactic behaviors.

We further present Gaussian Mixture LVeGs
(GM-LVeGs), a special case of LVeGs that uses
mixtures of Gaussian distributions as the weight
functions of fine-grained production rules. A ma-
jor advantage of GM-LVeGs is that the partition
function and the expectations of fine-grained pro-
duction rules can be computed using an extension
of the inside-outside algorithm. This makes it pos-
sible to efficiently compute the gradients during
discriminative learning of GM-LVeGs. We evalu-
ate GM-LVeGs on part-of-speech tagging and con-
stituency parsing on a variety of languages and
corpora and show that GM-LVeGs achieve com-
petitive results.

It shall be noted that many modern state-of-
the-art constituency parsers predict how likely a
constituent is based on not only local information
(such as the production rules used in composing
the constituent), but also contextual information
of the constituent. For example, the neural CRF
parser (Durrett and Klein, 2015) looks at the words
before and after the constituent; and RNNG (Dyer
et al., 2016) looks at the constituents that are al-
ready predicted (in the stack) and the words that
are not processed (in the buffer). In this paper,
however, we choose to focus on the basic frame-
work and algorithms of LVeGs and leave the in-
corporation of contextual information for future
work. We believe that by laying a solid foundation
for LVeGs, our work can pave the way for many
interesting extensions of LVeGs in the future.

2 Latent Vector Grammars

A latent vector grammar (LVeG) considers sub-
types of nonterminals as continuous vectors and
associates each nonterminal with a latent vector
space representing the set of its subtypes. For each
production rule, the LVeG defines a weight func-
tion over the subtypes of the nonterminal involved
in the production rule. In this way, it models the
space of refinements of the production rule.

2.1 Model Definition

A latent vector grammar is defined as a 5-tuple
G = (N,S,Σ, R,W ), where N is a finite set of
nonterminal symbols, S ∈ N is the start sym-
bol, Σ is a finite set of terminal symbols such that
N∩Σ = ∅,R is a set production rules of the form
X � γ where X ∈ N and γ ∈ (N ∪ Σ)∗, W is a
set of rule weight functions indexed by production
rules in R (to be defined below). In the following
discussion, we consider R in the Chomsky normal
form (CNF) for clarity of presentation. However,
it is straightforward to extend our formulation to
the general case.

Unless otherwise specified, we always use cap-
ital letters A,B,C, . . . for nonterminal symbols
and use bold lowercase letters a,b, c, . . . for their
subtypes. Note that subtypes are represented
by continuous vectors. For a production rule
of the form A � BC, its weight function is
WA�BC(a,b, c). For a production rule of the
form A � w where w ∈ Σ, its weight func-
tion is WA�w(a). The weight functions should
be non-negative, continuous and smooth, and
hence fine-grained production rules of similar sub-
types of a nonterminal would have similar weight
assignments. Rule weights can be normalized
such that

∑
B,C

∫
b,cWA�BC(a,b, c)dbdc = 1,

which leads to a probabilistic context-free gram-
mar (PCFG). Whether the weights are normalized
or not leads to different model classes and accord-
ingly different estimation methods. However, the
two model classes are proven equivalent by Smith
and Johnson (2007).

2.2 Relation to Other Models

Latent variable grammars (LVGs) (Matsuzaki
et al., 2005; Petrov et al., 2006) associate
each nonterminal with a discrete latent vari-
able, which is used to indicate the subtype
of the nonterminal when it appears in a parse
tree. Through nonterminal-splitting and the



1183

expectation-maximization algorithm, fine-grained
production rules can be automatically induced
from a treebank.

We show that LVGs can be seen as a special case
of LVeGs. Specifically, we can use one-hot vec-
tors in LVeGs to represent latent variables in LVGs
and define weight functions in LVeGs accordingly.
Consider a production rule r : A � BC. In a
LVG, each nonterminal is split into a number of
subtypes. Suppose A, B, and C are split into nA,
nB , and nC subtypes respectively. ax is the x-th
subtype of A, by is the y-th subtype of B, and cz
is the z-th subtype of C. ax � bycz is a fine-
grained production rule of A � BC, where x =
1, . . . , nA, y = 1, . . . , nB , and z = 1, . . . , nC .
The probabilities of all the fine-grained produc-
tion rules can be represented by a rank-3 tensor
ΘA�BC ∈ RnA×nB×nC . To cast the LVG as a
LVeG, we require that the latent vectors in the
LVeG must be one-hot vectors. We achieve this by
defining weight functions that output zero if any
of the input vectors is not one-hot. Specifically,
we define the weight function of the production
rule A � BC as:

Wr(a,b, c) =
∑
x,y,z

ΘA�BCcba× (δ(a− ax)

× δ(b− by)× δ(c− cz)) , (1)

where δ(·) is the Dirac delta function, ax ∈ RnA ,
by ∈ RnB , cz ∈ RnC are one-hot vectors (which
are zero everywhere with the exception of a single
1 at the x-th index of ax, the y-th index of by,
and the z-th index of cz) and ΘA�BC is multiplied
sequentially by c, b, and a.

Compared with LVGs, LVeGs have the follow-
ing advantages. While a LVG contains a finite,
typically small number of subtypes for each non-
terminal, a LVeG uses a continuous space to rep-
resent an infinite number of subtypes. When
equipped with weight functions of sufficient com-
plexity, LVeGs can represent more fine-grained
syntactic categories and production rules than
LVGs. By controlling the complexity and smooth-
ness of the weight functions, a LVeG is also ca-
pable of representing any level of subtype gran-
ularity. Importantly, this allows us to change the
level of subtype granularity for the same nonter-
minal in different production rules, which is sim-
ilar to multi-scale grammars (Petrov and Klein,
2008b). In addition, with a continuous space of
subtypes in a LVeG, similarity between subtypes

can be naturally modeled by their distance in the
space and can be automatically learned from data.
Consequently, with continuous and smooth weight
functions, fine-grained production rules over simi-
lar subtypes would have similar weights in LVeGs,
eliminating the need for the extra smoothing steps
that are necessary in training LVGs.

Compositional vector grammars (CVGs)
(Socher et al., 2013), an extension of recursive
neural networks (RNNs) (Socher et al., 2011),
can also be seen as a special case of LVeGs.
For a production rule r : A � BC, a CVG can
be interpreted as specifying its weight function
Wr(a,b, c) in the following way. First, a neural
network f indexed byB and C is used to compute
a parent vector p = fBC(b, c). Next, the score of
the parent vector is computed using a base PCFG
and a vector vBC :

s(p) = vTBCp + logP (A � BC) , (2)

where P (A � BC) is the rule probability from
the base PCFG. Then, the weight function of the
production rule A � BC is defined as:

Wr(a,b, c) = exp (s(p))× δ(a− p) . (3)

This form of weight functions in CVGs leads
to point estimation of latent vectors in a parse
tree, i.e., for each nonterminal in a given parse
tree, only one subtype in the whole subtype space
would lead to a non-zero weight of the parse. In
addition, different parse trees of the same sub-
string typically lead to different point estimations
of the subtype vector at the root nonterminal. Con-
sequently, CVGs cannot use dynamic program-
ming for inference and hence have to resort to
greedy search or beam search.

3 Gaussian Mixture LVeGs

A major challenge in applying LVeGs to parsing is
that it is impossible to enumerate the infinite num-
ber of subtypes. Previous work such as CVGs re-
sorts to point estimation and greedy search. In this
section we present Gaussian Mixture LVeGs (GM-
LVeGs), which use mixtures of Gaussian distribu-
tions as the weight functions in LVeGs. Because
Gaussian mixtures have the nice property of being
closed under product, summation, and marginal-
ization, we can compute the partition function and
the expectations of fine-grained production rules
using dynamic programming. This in turn makes
efficient learning and parsing possible.



1184

3.1 Representation

In a GM-LVeG, the weight function of a produc-
tion rule r is defined as a Gaussian mixture con-
taining Kr mixture components:

Wr(r) =

Kr∑
k=1

ρr,kN (r|µr,k,Σr,k) , (4)

where r is the concatenation of the latent vectors
of the nonterminals in r, which denotes a fine-
grained production rule of r. ρr,k > 0 is the k-th
mixture weight (the mixture weights do not nec-
essarily sum up to 1), N (r|µr,k,Σr,k) is the k-th
Gaussian distribution parameterized by mean µr,k
and covariance matrix Σr,k, and Kr is the num-
ber of mixture components, which can be differ-
ent for different production rules. Below we write
N (r|µr,k,Σr,k) as Nr,k(r) for brevity. Given a
production rule of the form A � BC, the GM-
LVeG expects r = [a; b; c] and a,b, c ∈ Rd,
where d is the dimension of the vectors a,b, c. We
use the same dimension for all the subtype vectors.

For the sake of computational efficiency, we
use diagonal or spherical Gaussian distributions,
whose covariance matrices are diagonal, so that
the inverse of covariance matrices in Equation 15–
16 can be computed in linear time. A spherical
Gaussian has a diagonal covariance matrix where
all the diagonal elements are equal, so it has fewer
free parameters than a diagonal Gaussian and re-
sults in faster learning and parsing. We empiri-
cally find that spherical Gaussians lead to slightly
better balance between the efficiency and the pars-
ing accuracy than diagonal Gaussians.

3.2 Parsing

The goal of parsing is to find the most probable
parse tree T ∗ with unrefined nonterminals for a
sentence w of n words w1:n = w1 . . . wn. This
is formally defined as:

T ∗ = argmax
T∈G(w)

P (T |w) , (5)

where G(w) denotes the set of parse trees with
unrefined nonterminals for w. In a PCFG, T ∗ can
be found using dynamic programming such as the
CYK algorithm. However, parsing becomes in-
tractable with LVeGs, and even with LVGs, the
special case of LVeGs.

A common practice in parsing with LVGs is to
use max-rule parsing (Petrov et al., 2006; Petrov

and Klein, 2007). The basic idea of max-rule
parsing is to decompose the posteriors over parses
into the posteriors over production rules approx-
imately. This requires calculating the expected
counts of unrefined production rules in parsing
the input sentence. Since Gaussian mixtures are
closed under product, summation, and marginal-
ization, in GM-LVeGs the expected counts can be
calculated using the inside-outside algorithm in
the following way. Given a sentence w1:n, we
first calculate the inside score sAI (a, i, j) and out-
side score sAO(a, i, j) for a nonterminal A over a
span wi:j using Equation 6 and Equation 7 in Ta-
ble 1 respectively. Note that both sAI (a, i, j) and
sAO(a, i, j) are mixtures of Gaussian distributions
of the subtype vector a. Next, using Equation 8 in
Table 1, we calculate the score s(A � BC, i, k, j)
(1 ≤ i ≤ k < j ≤ n), where 〈A � BC, i, k, j〉
represents a production ruleA � BC with nonter-
minalsA,B, andC spanning wordswi:j ,wi,k, and
wk+1:j respectively in the sentence w1:n. Then the
expected count (or posterior) of 〈A � BC, i, k, j〉
is calculated as:

q(A � BC, i, k, j) = s(A � BC, i, k, j)
sI(S, 1, n)

, (9)

where sI(S, 1, n) is the inside score for the start
symbol S spanning the whole sentence w1:n. Af-
ter calculating all the expected counts, we can use
the MAX-RULE-PRODUCT algorithm (Petrov and
Klein, 2007) for parsing, which returns a parse
with the highest probability that all the production
rules are correct. Its objective function is given by

T ∗q = argmax
T∈G(w)

∏
e∈T

q(e) , (10)

where e ranges over all the 4-tuples
〈A � BC, i, k, j〉 in the parse tree T . This
objective function can be efficiently solved
by dynamic programming such as the CYK
algorithm.

Although the time complexity of the inside-
outside algorithm with GM-LVeGs is polynomial
in the sentence length and the nonterminal num-
ber, in practice the algorithm is still slow because
the number of Gaussian components in the inside
and outside scores increases dramatically with the
recursion depth. To speed up the computation, we
prune Gaussian components in the inside and out-
side scores using the following technique. Sup-
pose we have a minimum pruning threshold kmin



1185

sAI (a, i, j) =
∑

A�BC∈R

∑
k=i,··· ,j−1

∫∫
WA�BC(a,b, c)× sBI (b, i, k)× sCI (c, k + 1, j) dbdc .(6)

sAO(a, i, j) =
∑

B�CA∈R

∑
k=1,··· ,i−1

∫∫
WB�CA(b, c,a)× sBO(b, k, j)× sCI (c, k, i− 1) dbdc

+
∑

B�AC∈R

∑
k=j+1,··· ,n

∫∫
WB�AC(b,a, c)× sBO(b, i, k)× sCI (c, j + 1, k) dbdc .(7)

s(A � BC, i, k, j) =
∫∫∫

WA�BC(a,b, c)× sAO(a, i, j)× sBI (b, i, k)× sCI (c, k + 1, j) dadbdc .(8)

Table 1: Equation 6: sAI (a, i, j) is the inside score of a nonterminal A over a span wi:j in the sentence w1:n, where 1 ≤
i < j ≤ n. Equation 7: sAO (a, i, j) is the outside score of a nonterminal A over a span wi:j in the sentence w1:n, where
1 ≤ i ≤ j ≤ n. Equation 8: s(A � BC, i, k, j) is the score of a production rule A � BC with nonterminals A, B, and C
spanning words wi:j , wi,k, and wk+1:j respectively in the sentence w1:n, where 1 ≤ i ≤ k < j ≤ n.

and a maximum pruning threshold kmax. Given
an inside or outside score with kc Gaussian com-
ponents, if kc ≤ kmin, then we do not prune
any Gaussian component; otherwise, we compute
kallow = min{kmin + floor(kϑc ), kmax} (0 ≤ ϑ ≤
1 is a constant) and keep only kallow components
with the largest mixture weights.

In addition to component pruning, we also em-
ploy two constituent pruning techniques to reduce
the search space during parsing. The first tech-
nique is used by Petrov et al. (2006). Before
parsing a sentence with a GM-LVeG, we run the
inside-outside algorithm with the treebank gram-
mar and calculate the posterior probability of ev-
ery nonterminal spanning every substring. Then
a nonterminal would be pruned from a span if
its posterior probability is below a pre-specified
threshold pmin. When parsing with GM-LVeGs,
we only consider the unpruned nonterminals for
each span.

The second constituent pruning technique is
similar to the one used by Socher et al. (2013).
Note that for a strong constituency parser such as
the Berkeley parser (Petrov and Klein, 2007), the
constituents in the top 200 best parses of a sen-
tence can cover almost all the constituents in the
gold parse tree. So we first use an existing con-
stituency parser to run k-best parsing with k =
200 on the input sentence. Then we parse with a
GM-LVeG and only consider the constituents that
appear in the top 200 parses. Note that this method
is different from the re-ranking technique because
it may produce a parse different from the top 200
parses.

3.3 Learning
Given a training dataset D = {(Ti,wi) | i =
1, . . . ,m} containing m samples, where Ti is the
gold parse tree with unrefined nonterminals for the
sentence wi, the objective of discriminative learn-
ing is to minimize the negative log conditional
likelihood:

L(Θ) = − log
m∏
i=1

P (Ti|wi; Θ) , (11)

where Θ represents the set of parameters of the
GM-LVeG.

We optimize the objective function using the
Adam (Kingma and Ba, 2014) optimization algo-
rithm. The derivative with respect to Θr, the pa-
rameters of the weight function Wr(r) of an un-
refined production rule r, is calculated as follows
(the derivation is in the supplementary material):

∂L(Θ)
∂Θr

=

m∑
i=1

∫ (
∂Wr(r)

∂Θr
(12)

×
EP (t|wi)[fr(t)]− EP (t|Ti)[fr(t)]

Wr(r)

)
dr ,

where t indicates a parse tree with nonterminal
subtypes, and fr(t) is the number of occurrences
of the unrefined rule r in the unrefined parse tree
that is obtained by replacing all the subtypes in t
with the corresponding nonterminals. The two ex-
pectations in Equation 12 can be efficiently com-
puted using the inside-outside algorithm. Because
the second expectation is conditioned on the parse
tree Ti, in Equation 6 and Equation 7 we can skip
all the summations and assign the values of B, C,
and k according to Ti.



1186

In GM-LVeGs, Θr is the set of parameters in a
Gaussian mixture:

Θr = {(ρr,k,µr,k,Σr,k)|k = 1, . . . ,Kr} . (13)

According to Equation 12, we need to take the
derivatives ofWr(r) respect to ρr,k, µr,k, and Σr,k
respectively:

∂Wr(r)/∂ρr,k = Nr,k(r) , (14)
∂Wr(r)/∂µr,k = ρr,kNr,k(r)Σ−1r,k(r− µr,k) ,(15)

∂Wr(r)/∂Σr,k = ρr,kNr,k(r)Σ−1r,k
1

2

(
− I (16)

+ (r− µr,k)(r− µr,k)TΣ−1r,k
)
.

Substituting Equation 14–16 into Equation 12, we
have the full gradient formulations of all the pa-
rameters. In spite of the integral in Equation 12,
we can derive a closed-form solution for the gradi-
ent of each parameter, which is shown in the sup-
plementary material.

In order to keep each mixture weight ρr,k posi-
tive, we do not directly optimize ρr,k; instead, we
set ρr,k = exp(θρr,k) and optimize θρr,k by gradi-
ent descent. We use a similar trick to keep each
covariance matrix Σr,k positive definite.

Since we use the inside-outside algorithm de-
scribed in Section 3.2 to calculate the two ex-
pectations in Equation 12, we face the same ef-
ficiency problem that we encounter in parsing. To
speed up the computation,we again use both com-
ponent pruning and constituent pruning introduced
in Section 3.2.

Because gradient descent is often sensitive to
the initial values of the parameters, we employ the
following informed initialization method. Mixture
weights are initialized using the treebank gram-
mar. Suppose in the treebank grammar P (r) is
the probability of a production rule r. We initial-
ize the mixture weights in the weight function Wr
by ρr,k = α · P (r) where α > 1 is a constant.
We initialize all the covariance matrices to iden-
tity matrices and initialize each mean with a value
uniformly sampled from [−0.05, 0.05].

4 Experiment

We evaluate the GM-LVeG on part-of-speech
(POS) tagging and constituency parsing and com-
pare it against its special cases such as LVGs and
CVGs. It shall be noted that in this paper we focus
on the basic framework of LVeGs and aim to show

its potential advantage over previous special cases.
It is therefore not our goal to compete with the
latest state-of-the-art approaches to tagging and
parsing. In particular, we currently do not incor-
porate contextual information of words and con-
stituents during tagging and parsing, while such
information is critical in achieving state-of-the-art
accuracy. We will discuss future improvements of
LVeGs in Section 5.

4.1 Datasets
Parsing. We use the Wall Street Journal corpus
from the Penn English Treebank (WSJ) (Marcus
et al., 1994). Following the standard data splitting,
we use sections 2 to 21 for training, section 23 for
testing, and section 22 for development. We pre-
process the treebank using a right-branching bina-
rization procedure to obtain an unannotated X-bar
grammar, so that there are only binary and unary
production rules. To deal with the problem of un-
known words in testing, we adopt the unknown
word features used in the Berkeley parser and set
the unknown word threshold to 1. Specifically, any
word occurring less than two times is replaced by
one of the 60 unknown word categories.
Tagging. (1) We use Wall Street Journal corpus
from the Penn English Treebank (WSJ) (Marcus
et al., 1994). Following the standard data split-
ting, we use sections 0 to 18 for training, sections
22 to 24 for testing, and sections 19 to 21 for de-
velopment. (2) The Universal Dependencies tree-
bank 1.4 (UD) (Nivre et al., 2016), in which En-
glish, French, German, Russian, Spanish, Indone-
sian, Finnish, and Italian treebanks are used. We
use the original data splitting of these corpora for
training and testing. For both WSJ and UD En-
glish treebanks, we deal with unknown words in
the same way as we do in parsing. For the rest of
the data, we use only one unknown word category
and the unknown word threshold is also set to 1.

4.2 POS Tagging
POS tagging is the task of labeling each word in
a sentence with the most probable part-of-speech
tag. Here we focus on POS tagging with Hidden
Markov Models (HMMs). Because HMMs are
equivalent to probabilistic regular grammars, we
can extend HMMs with both LVGs and LVeGs.
Specifically, the hidden states in HMMs can be
seen as nonterminals in regular grammars and
therefore can be associated with latent variables
or latent vectors.



1187

We implement two training methods for LVGs.
The first (LVG-G) is generative training us-
ing expectation-maximization that maximizes the
joint probability of the sentence and the tags. The
second (LVG-D) is discriminative training using
gradient descent that maximizes the conditional
probability of the tags given the sentence. In both
cases, each nonterminal is split into a fixed num-
ber of subtypes. In our experiments we test 1, 2,
4, 8, and 16 subtypes of each nonterminal. Due
to the limited space, we only report experimental
results of LVG with 16 subtypes for each nonter-
minal. Full experimental results can be found in
the supplementary material.

We experiment with two different GM-LVeGs:
GM-LVeG-D with diagonal Gaussians and GM-
LVeG-S with spherical Gaussians. In both cases,
we fix the number of Gaussian components Kr to
4 and the dimension of the latent vectors d to 3.
We do not use any pruning techniques in learning
and inference because we find that our algorithm
is fast enough with the current setting of Kr and
d. We train the GM-LVeGs for 20 epoches and se-
lect the models with the best token accuracy on the
development data for the final testing.

We report both token accuracy and sentence ac-
curacy of POS tagging in Table 2. It can be seen
that, on all the testing data, GM-LVeGs consis-
tently surpass LVGs in terms of both token ac-
curacy and sentence accuracy. GM-LVeG-D is
slightly better than GM-LVeG-S in sentence ac-
curacy, producing the best sentence accuracy on
5 of the 9 testing datasets. GM-LVeG-S performs
slightly better than GM-LVeG-D in token accuracy
on 5 of the 9 datasets. Overall, there is not sig-
nificant difference between GM-LVeG-D and GM-
LVeG-S. However, GM-LVeG-S admits more effi-
cient learning than GM-LVeG-D in practice since
it has fewer parameters.

4.3 Parsing

For efficiency, we train GM-LVeGs only on sen-
tences with no more than 50 words (totally 39115
sentences). Since we have found that spherical
Gaussians are better than diagonal Gaussians con-
sidering both model performance and learning ef-
ficiency, here we use spherical Gaussians in the
weight functions. The dimension of latent vectors
d is set to 3, and all the Gaussian mixtures have
Kr = 4 components. We use α = 8 in initializing
mixture weights. We train the GM-LVeG for 15

epoches and select the model with the highest F1
score on the development data for the final testing.
We use component pruning in both learning and
parsing, with kmax = 50 and ϑ = 0.35 in both
learning and parsing, kmin = 40 in learning and
kmin = 20 in parsing. During learning we use the
first constituent pruning technique with the prun-
ing threshold pmin = 1e − 5, and during parsing
we use the second constituent pruning technique
based on the Berkeley parser which produced 133
parses on average for each testing sentence. As
can be seen, we use weaker pruning during train-
ing than during testing. This is because in training
stronger pruning (even if accurate) results in worse
estimation of the first expectation in Equation 12,
which makes gradient computation less accurate.

We compare LVeGs with CVGs and several
variants of LVGs: (1) LVG-G-16 and LVG-D-16,
which are LVGs with 16 subtypes for each nonter-
minal with discriminative and generative training
respectively (accuracies obtained from Petrov and
Klein (2008a)); (2) Multi-scale grammars (Petrov
and Klein, 2008b), trained without using the
span features in order for a fair comparison; (3)
Berkeley parser (Petrov and Klein, 2007) (accura-
cies obtained from Petrov and Klein (2008b) be-
cause Petrov and Klein (2007) do not report exact
match scores). The experimental results are shown
in Table 3. It can be seen that GM-LVeG-S pro-
duces the best F1 scores on both the development
data and the testing data. It surpasses the Berkeley
parser by 0.92% in F1 score on the testing data.
Its exact match score on the testing data is only
slightly lower than that of LVG-D-16.

We further investigate the influence of the la-
tent vector dimension and the Gaussian compo-
nent number on the efficiency and the parsing ac-
curacy . We experiment on a small dataset (statis-
tics of this dataset are in the supplemental mate-
rial). We first fix the component number to 4 and
experiment with the dimension 2, 3, 4, 5, 6, 7, 8,
9. Then we fix the dimension to 3 and experiment
with the component number 2, 3, 4, 5, 6, 7, 8, 9.
F1 scores on the development data are shown in
the first row in Figure 1. Average time consumed
per epoch in learning is shown in the second row
in Figure 1. When Kr = 4, the best dimension is
5; when d = 3, the best Gaussian component num-
ber is 3. A higher dimension or a larger Gaussian
component number hurts the model performance
and requires much more time for learning. Thus



1188

Model
WSJ English French German Russian Spanish Indonesian Finnish Italian

T S T S T S T S T S T S T S T S T S

LVG-D-16 96.62 48.74 92.31 52.67 93.75 34.90 87.38 20.98 81.91 12.25 92.47 24.82 89.27 20.29 83.81 19.29 94.81 45.19
LVG-G-16 96.78 50.88 93.30 57.54 94.52 34.90 88.92 24.05 84.03 16.63 93.21 27.37 90.09 21.19 85.01 20.53 95.46 48.26

GM-LVeG-D 96.99 53.10 93.66 59.46 94.73 39.60 89.11 24.77 84.21 17.84 93.76 32.48 90.24 21.72 85.27 23.30 95.61 50.72
GM-LVeG-S 97.00 53.11 93.55 58.11 94.74 39.26 89.14 25.58 84.06 18.44 93.52 30.66 90.12 21.72 85.35 22.07 95.62 49.69

Table 2: Token accuracy (T) and sentence accuracy (S) for POS tagging on the testing data.

Model
dev (all) test ≤ 40 test (all)

F1 F1 EX F1 EX

LVG-G-16 88.70 35.80
LVG-D-16 89.30 39.40
Multi-Scale 89.70 39.60 89.20 37.20

Berkeley Parser 90.60 39.10 90.10 37.10
CVG (SU-RNN) 91.20 91.10 90.40

GM-LVeG-S 91.24 91.38 41.51 91.02 39.24

Table 3: Parsing accuracy on the testing data of WSJ. EX
indicates the exact match score.

our choice ofKr = 4 and d = 3 in GM-LVeGs for
parsing is a good balance between the efficiency
and the parsing accuracy.

83.5

84.0

84.5

85.0

85.5

F1
 S

co
re

2 4 6 8
d: dimension

50

75

100

125

150

175

Ti
m

e 
(m

in
) P

er
 E

po
ch

2 4 6 8
kr: # of Gaussian components

Figure 1: F1 score and average time (min) consumed per
epoch in learning. Left: # of Gaussian components fixed to
4 with different dimensions; Right: dimension of Gaussians
fixed to 3 with different # of Gaussian components.

5 Discussion

It shall be noted that in this paper we choose to
focus on the basic framework and algorithms of
LVeGs, and therefore we leave a few important
extensions for future work. One extension is to
incorporate contextual information of words and
constituents. which is a crucial technique that can
be found in most state-of-the-art approaches to
parsing or POS tagging. One possible way to uti-

lize contextual information in LVeGs is to allow
the words in the context of an anchored produc-
tion rule to influence the rule’s weight function.
For example, we may learn neural networks to pre-
dict the parameters of the Gaussian mixture weight
functions in a GM-LVeG from the pre-trained em-
beddings of the words in the context.

In GM-LVeGs, we currently use the same num-
ber of Gaussian components for all the weight
functions. A more desirable way would be au-
tomatically determining the number of Gaussian
components for each production rule based on the
ideal refinement granularity of the rule, e.g., we
may need more Gaussian components for NP �
DT NN than for NP � DT JJ, since the latter is
rarely used. There are a few possible ways to learn
the component numbers such as greedy addition
and removal, the split-merge method, and sparsity
priors over mixture weights.

An interesting extension beyond LVeGs is to
have a single continuous space for subtypes of all
the nonterminals. Ideally, subtypes of the same
nonterminal or similar nonterminals are close to
each other. The benefit is that similarity between
nonterminals can now be modeled.

6 Conclusion

We present Latent Vector Grammars (LVeGs) that
associate each nonterminal with a latent continu-
ous vector space representing the set of subtypes
of the nonterminal. For each production rule, a
LVeG defines a continuous weight function over
the subtypes of the nonterminals involved in the
rule. We show that LVeGs can subsume latent vari-
able grammars and compositional vector gram-
mars as special cases. We then propose Gaus-
sian mixture LVeGs (GM-LVeGs). which formu-
late weight functions of production rules by mix-
tures of Gaussian distributions. The partition func-
tion and the expectations of fine-grained produc-
tion rules in GM-LVeGs can be efficiently com-
puted using dynamic programming, which makes
learning and inference with GM-LVeGs feasible.



1189

We empirically show that GM-LVeGs can achieve
competitive accuracies on POS tagging and con-
stituency parsing.

Acknowledgments

This work was supported by the National Natu-
ral Science Foundation of China (61503248), Ma-
jor Program of Science and Technology Com-
mission Shanghai Municipal (17JC1404102), and
Program of Shanghai Subject Chief Scientist (A
type) (No.15XD1502900). We would like to thank
the anonymous reviewers for their careful reading
and useful comments.

References
Eugene Charniak. 1996. Tree-bank grammars. In Pro-

ceedings of the 30th National Conference on Artifi-
cial Intelligence, volume 2, pages 1031–1036.

Eugene Charniak. 2000. A maximum-entropy-inspired
parser. In Proceedings of the 1st Meeting of the
North American Chapter of the Association for
Computational Linguistics, pages 132–139. Associ-
ation for Computational Linguistics.

Michael Collins. 2003. Head-driven statistical mod-
els for natural language parsing. Computational lin-
guistics, 29(4):589–637.

Greg Durrett and Dan Klein. 2015. Neural CRF pars-
ing. In Proceedings of the 53rd Annual Meeting
of the Association for Computational Linguistics,
pages 302–312. Association for Computational Lin-
guistics.

Chris Dyer, Adhiguna Kuncoro, Miguel Ballesteros,
and Noah A Smith. 2016. Recurrent neural network
grammars. In Proceedings of the 2016 Conference
of the North American Chapter of the Association
for Computational Linguistics: Human Language
Technologies, pages 199–209. Association for Com-
putational Linguistics.

Mark Johnson. 1998. PCFG models of linguistic
tree representations. Computational Linguistics,
24(4):613–632.

Diederik P Kingma and Jimmy Ba. 2014. Adam: A
method for stochastic optimization. arXiv preprint
arXiv:1412.6980.

Dan Klein and Christopher D Manning. 2003. Accu-
rate unlexicalized parsing. In Proceedings of the
41st annual meeting on Association for Computa-
tional Linguistics, pages 423–430. Association for
Computational Linguistics.

Mitchell Marcus, Grace Kim, Mary Ann
Marcinkiewicz, Robert MacIntyre, Ann Bies,

Mark Ferguson, Karen Katz, and Britta Schas-
berger. 1994. The penn treebank: annotating
predicate argument structure. In Proceedings of
the workshop on Human Language Technology,
pages 114–119. Association for Computational
Linguistics.

Takuya Matsuzaki, Yusuke Miyao, and Jun’ichi Tsujii.
2005. Probabilistic CFG with latent annotations. In
Proceedings of the 43rd annual meeting on Associ-
ation for Computational Linguistics, pages 75–82.
Association for Computational Linguistics.

Joakim Nivre, Marie-Catherine de Marneffe, Filip
Ginter, Yoav Goldberg, Jan Hajic, Christopher D.
Manning, Ryan T. McDonald, Slav Petrov, Sampo
Pyysalo, Natalia Silveira, Reut Tsarfaty, and Daniel
Zeman. 2016. Universal dependencies v1: A mul-
tilingual treebank collection. In Proceedings of
the 10th International Conference on Language Re-
sources and Evaluation, pages 1659–1666.

Slav Petrov, Leon Barrett, Romain Thibaux, and Dan
Klein. 2006. Learning accurate, compact, and in-
terpretable tree annotation. In Proceedings of the
44th annual meeting of the Association for Compu-
tational Linguistics, pages 433–440. Association for
Computational Linguistics.

Slav Petrov and Dan Klein. 2007. Improved infer-
ence for unlexicalized parsing. In Proceedings of the
2007 Conference of the North American Chapter of
the Association for Computational Linguistics: Hu-
man Language Technologies, pages 404–411. Asso-
ciation for Computational Linguistics.

Slav Petrov and Dan Klein. 2008a. Discriminative log-
linear grammars with latent variables. In Advances
in Neural Information Processing Systems 20, pages
1153–1160.

Slav Petrov and Dan Klein. 2008b. Sparse multi-scale
grammars for discriminative latent variable parsing.
In Proceedings of the 2008 Conference on Empiri-
cal Methods in Natural Language Processing, pages
867–876. Association for Computational Linguis-
tics.

Noah A Smith and Mark Johnson. 2007. Weighted
and probabilistic context-free grammars are equally
expressive. Computational Linguistics, 33(4):477–
491.

Richard Socher, John Bauer, Christopher D Manning,
et al. 2013. Parsing with compositional vector gram-
mars. In Proceedings of the 51st Annual Meeting of
the Association for Computational Linguistics, vol-
ume 1, pages 455–465. Association for Computa-
tional Linguistics.

Richard Socher, Cliff C Lin, Chris Manning, and An-
drew Y Ng. 2011. Parsing natural scenes and nat-
ural language with recursive neural networks. In
Proceedings of the 28th International Conference on
Machine Learning, pages 129–136.

http://www.aaai.org/Papers/AAAI/1996/AAAI96-153.pdf
http://www.aclweb.org/anthology/A00-2018
http://www.aclweb.org/anthology/A00-2018
https://doi.org/10.1162/089120103322753356
https://doi.org/10.1162/089120103322753356
http://aclweb.org/anthology/P/P15/P15-1030.pdf
http://aclweb.org/anthology/P/P15/P15-1030.pdf
http://aclweb.org/anthology/N/N16/N16-1024.pdf
http://aclweb.org/anthology/N/N16/N16-1024.pdf
https://aclanthology.info/pdf/J/J98/J98-4004.pdf
https://aclanthology.info/pdf/J/J98/J98-4004.pdf
http://arxiv.org/abs/1412.6980
http://arxiv.org/abs/1412.6980
http://aclweb.org/anthology/P/P03/P03-1054.pdf
http://aclweb.org/anthology/P/P03/P03-1054.pdf
http://aclweb.org/anthology/H/H94/H94-1020.pdf
http://aclweb.org/anthology/H/H94/H94-1020.pdf
http://aclweb.org/anthology/P/P05/P05-1010.pdf
http://www.lrec-conf.org/proceedings/lrec2016/pdf/348_Paper.pdf
http://www.lrec-conf.org/proceedings/lrec2016/pdf/348_Paper.pdf
http://aclweb.org/anthology/P06-1055
http://aclweb.org/anthology/P06-1055
http://www.aclweb.org/anthology/N07-1051
http://www.aclweb.org/anthology/N07-1051
http://papers.nips.cc/paper/3337-discriminative-log-linear-grammars-with-latent-variables.pdf
http://papers.nips.cc/paper/3337-discriminative-log-linear-grammars-with-latent-variables.pdf
http://www.aclweb.org/anthology/D08-1091
http://www.aclweb.org/anthology/D08-1091
https://doi.org/10.1162/coli.2007.33.4.477
https://doi.org/10.1162/coli.2007.33.4.477
https://doi.org/10.1162/coli.2007.33.4.477
http://aclweb.org/anthology/P/P13/P13-1045.pdf
http://aclweb.org/anthology/P/P13/P13-1045.pdf
http://www.icml-2011.org/papers/125_icmlpaper.pdf
http://www.icml-2011.org/papers/125_icmlpaper.pdf

