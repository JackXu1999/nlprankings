










































Untitled


Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 4033–4041
Florence, Italy, July 28 - August 2, 2019. c©2019 Association for Computational Linguistics

4033

Towards Language Agnostic Universal Representations

Armen Aghajanyan ∗

Facebook / Seattle, WA

armenag@fb.com

Xia Song and Saurabh Tiwary

Microsoft / Bellevue, WA

{xiaso, satiwary}@microsoft.com

Abstract

When a bilingual student learns to solve word

problems in math, we expect the student to

be able to solve these problem in both lan-

guages the student is fluent in, even if the math

lessons were only taught in one language.

However, current representations in machine

learning are language dependent. In this work,

we present a method to decouple the language

from the problem by learning language ag-

nostic representations and therefore allowing

training a model in one language and apply-

ing to a different one in a zero shot fashion.

We learn these representations by taking in-

spiration from linguistics, specifically the Uni-

versal Grammar hypothesis and learn univer-

sal latent representations that are language ag-

nostic (Chomsky, 2014; Montague, 1970). We

demonstrate the capabilities of these represen-

tations by showing that models trained on a

single language using language agnostic rep-

resentations achieve very similar accuracies in

other languages.

1 Introduction

Anecdotally speaking, fluent bilingual speakers

rarely face trouble translating a task learned in

one language to another. For example, a bilingual

speaker who is taught a math problem in English

will trivially generalize to other known languages.

Furthermore there is a large collection of evidence

in linguistics arguing that although separate lexi-

cons exist in multilingual speakers the core repre-

sentations of concepts and theories are shared in

memory (Altarriba, 1992; Mitchel, 2005; Bentin

et al., 1985). The fundamental question we’re in-

terested in answering is on the learnability of these

shared representations within a statistical frame-

work.

We approached this problem from a linguis-

tics perspective. Languages have vastly varying

syntactic features and rules. Linguistic Relativ-

ity studies the impact of these syntactic variations

on the formations of concepts and theories (Au,

1983). Within this framework of study, the two

schools of thoughts are linguistic determinism and

weak linguistic influence. Linguistic determinism

argues that language entirely forms the range of

cognitive processes, including the creation of var-

ious concepts, but is generally agreed to be false

(Hoijer, 1954; Au, 1983). Although there exists

some weak linguistic influence, it is by no means

fundamental (Ahearn, 2016). The superfluous na-

ture of syntactic variations across languages brings

forward the argument of principles and param-

eters (PnP) which hypothesizes the existence of

a small distributed parameter representation that

captures the syntactic variance between languages

denoted by parameters (e.g. head-first or head-

final syntax), as well as common principles shared

across all languages (Culicover, 1997). Univer-

sal Grammar (UG) is the study of principles and

the parameters that are universal across languages

(Montague, 1970).

The ability to learn these universalities would

allow us to learn representations of language that

are fundamentally agnostic of the specific lan-

guage itself. Doing so would allow us to learn

a task in one language and reap the benefits of

all other languages without needing multilingual

datasets. We take inspiration from the UG hy-

pothesis and learn latent representations that are

language agnostic which allow us to solve down-

stream problems in new languages without the

need of any language-specific training data. We do

not make any claims about the Universal Grammar

hypothesis, but simply take inspiration from it.

∗Work done while at Microsoft.



4034

2 Related Work

Our work attempts to unite universal (task agnos-

tic) representations with multilingual (language

agnostic) representations (Peters et al., 2018; Mc-

Cann et al., 2017). The recent trend in univer-

sal representations has been moving away from

context-less unsupervised word embeddings to

context-rich representations. Deep contextualized

word representations (ELMo) trains an unsuper-

vised language model on a large corpus of data

and applies it to a large set of auxiliary tasks (Pe-

ters et al., 2018). These unsupervised represen-

tations boosted the performance of models on a

wide array of tasks. Along the same lines (Mc-

Cann et al., 2017) showed the power of using la-

tent representations of translation models as fea-

tures across other non-translation tasks. In gen-

eral, initializing models with pre-trained language

models shows promise against the standard ini-

tialization with word embeddings. Even further,

(Radford et al., 2017) show that an unsupervised

language model trained on a large corpus will con-

tain a neuron that strongly correlates with senti-

ment without ever training on a sentiment task im-

plying that unsupervised language models maybe

picking up informative and structured signals.

In the field of multilingual representations, a

fair bit of work has been done on multilingual

word embeddings. (Ammar et al., 2016) explored

the possibility of training massive amounts of

word embeddings utilizing either parallel data or

bilingual dictionaries via the SkipGram paradigm.

Later on an unsupervised approach to multilin-

gual word representations was proposed by (Chen

and Cardie, 2018) which utilized an adversarial

training regimen to place word embeddings into

a shared latent space. Although word embeddings

show great utility, they fall behind methods which

exploit sentence structure as well as words. Less

work has been done on multilingual sentence rep-

resentations. Most notably both (Schwenk and

Douze, 2017) and (Artetxe et al., 2017) propose a

way to learn multilingual sentence representation

through a translation task.

We train downstream models using language

agnostic universal representations on a set of tasks

and show the ability for the downstream models to

generalize to languages that we did not train on.

3 Optimization Problem

Statistical language models approximate the prob-

ability distribution of a series of words by predict-

ing the next word given a sequence of previous

words.

p(w0, ..., wn) =
n∏

i=1

p(wi | w0, ..., wi−1)

where wi are indices representing words in an ar-

bitrary vocabulary.

Learning grammar is equivalent to language

modeling, as the support of p will represent the set

of all grammatically correct sentences. Further-

more, let jα represent a particular language. Let

pjα(·) represent the language model for the jα
th

language and wjα represents a word from the jα
th

language. Let kjα represent a distributed represen-

tation of a specific language along the lines of the

PnP argument (Culicover, 1997). UG, through the

lens of statistical language modeling, hypothesizes

the existence of a factorization of pjα(·) contain-
ing a language agnostic segment. The factoriza-

tion used throughout this paper is the following (◦
denotes function composition):

b = u ◦ ejα(w
jα
0
, ..., w

jα
i ) (1)

pjα(wi | w0, ..., wi−1) = e
−1

jα
◦ h(b, kjα) (2)

s.t. d(p(b | jα) || p(b | jβ)) ≤ ǫ (3)

The distribution matching constraint d, insures

that the representations across languages are com-

mon as hypothesized by the UG argument.

Function ejα : N
i → Ri×d is a language spe-

cific function which takes an ordered set of in-

tegers representing tokens (token meaning w
jα
i )

and outputs a vector of size d per token. Func-

tion u : Ri×d → Ri×d takes the language spe-
cific representation and attempts to embed into a

language agnostic representation. Function h :
(Ri×d,Rf ) → Ri×d takes the universal represen-
tation as well as a distributed representation of the

language of size f and returns a language specific

decoded representation. e−1jα maps our decoded

representation back to the token space.

For the purposes of distribution matching we

utilize the GAN framework. Following recent suc-

cesses we use Wasserstein-1 as our distance func-

tion d (Arjovsky et al., 2017).

Given two languages jα and jβ the distribution

of the universal representations should be within



4035



4036

4.2 Training

We trained UG-WGAN with a variety of lan-

guages depending on the downstream task.

For each language we utilized the respective

Wikipedia dump. From the wikipedia dump we

extract all pages using the wiki2text∗ utility and

build language specific vocabularies consisting of

16k BPE tokens (Sennrich et al., 2015). During

each batch we uniform sample random documents

from our set of languages which are approximately

the same length, therefore a batch will be mixed

with respect to language. We train our language

model via BPTT where the truncation length pro-

gressively grows from 15 to 50 throughout train-

ing. The critic is updated 10 times for every update
of the language model. We trained each language

model for 14 days on a NVidia Titan X. For each

language model we would do a sweep over λ, but

in general we have found that λ = 0.1 works suf-
ficiently well for minimizing both perplexity and

Wasserstein distance.

4.3 Exploration

A couple of interesting questions arise from the

described training procedure. Is the distribu-

tion matching constraint necessary or will simple

joint language model training exhibit the proper-

ties we’re interested in? Can this optimization

process fundamentally learn individual languages

grammar while being constrained by a universal

channel? What commonalities between languages

can we learn and are they informative enough to

be exploited?

We can test out the usefulness of the distribution

matching constraint by running an ablation study

on the λ hyper-parameter. We trained UG-WGAN

on English, Spanish and Arabic wikidumps fol-

lowing the procedure described above. We kept

all the hyper-parameters consistent apart for aug-

menting λ from 0 to 10. The results are shown in

Figure 2. Without any weight on the distribution

matching term the critic trivially learns to sepa-

rate the various languages and no further training

reduces the wasserstein distance. The joint lan-

guage model internally learns individual language

models who are partitioned in the latent space. We

can see this by running a t-SNE plot on the univer-

sal (u(·)) representation of our model and seeing
existence of clusters of the same language as we

did in Figure 3 (Maaten and Hinton, 2008). An

∗https://github.com/rspeer/wiki2text

universal model satisfying the distribution match-

ing constrain would mix all languages uniformly

within it’s latent space.

To test the universality of UG-WGAN represen-

tations we will apply them to a set of orthogonal

NLP tasks. We will leave the discussion on the

learnability of grammar to the Discussion section

of this paper.

5 Experiments

By introducing a universal channel in our lan-

guage model we reduced a representations depen-

dence on a single language. Therefore we can uti-

lize an arbitrary set of languages in training an

auxiliary task over UG encodings. For example

we can train a downstream model only on one lan-

guages data and transfer the model trivially to any

other language that UG-WGAN was trained on.

5.1 Sentiment Analysis

To test the universality of UG-WGAN representa-

tion we first trained UG-WGAN in English, Chi-

nese and German following the procedure de-

scribed in Section 4. The embedding size of

the table was 300 and the internal LSTM hidden
size was 512. A dropout rate of 0.1 was used
and trained with the ADAM optimization method

(Kingma and Ba, 2014). Since we are interested

in the zero-shot capabilities of our representation,

we trained our sentiment analysis model only on

the English IMDB Large Movie Review dataset

and tested it on the Chinese ChnSentiCorp dataset

and German SB-10K (Maas et al., 2011; Tan and

Zhang, 2008). We binarize the label’s for all the

datasets.

Our sentiment analysis model ran a bi-

directional LSTM on top of fixed UG representa-

tions from where we took the last hidden state and

computed a logistic regression. This was trained

using standard SGD with momentum.

We also compare against encodings learned as

a by-product of multi-encoder and decoder neu-

ral machine translation as a baseline (Klein et al.,

2017). We see that UG representations are use-

ful in situations when there is a lack of data in an

specific language. The language agnostics prop-

erties of UG embeddings allows us to do success-

ful zero-shot learning without needing any paral-

lel corpus, furthermore the ability to generalize

from language modeling to sentiment attests for

the universal properties of these representations.



4037



4038

Method IMDB ChnSentiCorp SB-10K

NMT + Logistic (Schwenk and Douze, 2017) 12.44% 20.12% 22.92%

FullUnlabeledBow (Maas et al., 2011) 11.11% * *

NB-SVM TRIGRAM (Mesnil et al., 2014) 8.54% 18.20% 19.40%

UG-WGAN λ = 0.1 + Logistic (Ours) 8.01% 15.40% 17.32%
UG-WGAN λ = 0.0 + Logistic (Ours) 7.80% 53.00% 49.38%
Sentiment Neuron (Radford et al., 2017) 7.70% * *

SA-LSTM (Dai and Le, 2015) 7.24% * *

Table 1: Zero-shot capability of UG and OpenNMT representation from English training. For all other methods

we trained on the available training data. Table shows error of sentiment model.

Method sNLI(en) sNLI (ru)

Densely-Connected Recurrent and Co-Attentive Network Ensemble (Kim et al.,
2018)

9.90% *

UG-WGAN (λ = 0.1) + Densely-Connected Recurrent and Co-Attentive Net-
work (Kim et al., 2018)

12.25% 21.00%

UG-WGAN (λ = 0.1) + Multiway Attention Network (Tan et al., 2018) 21.50% 34.25%
UG-WGAN (λ = 0.0) + Multiway Attention Network (Tan et al., 2018) 13.50% 65.25%
UG-WGAN (λ = 0.0) + Densely-Connected Recurrent and Co-Attentive Network
(Kim et al., 2018)

11.50% 68.25%

Unlexicalized features + Unigram + Bigram features (Bowman et al., 2015) 21.80% 55.00%

Table 2: Error in terms of accuracy for the following methods. For Unlexicalized features + Unigram + Bigram

features we trained on 200 out of the 400 Russian samples and tested on the other 200 as a baseline.

showed the best perplexity. The NLI specific

model was the Densely-Connected Recurrent and

Co-Attentive Network.

Looking at Figure 4 we see that increasing λ

doesn’t seem to have a significant impact on the

generalization gap but has a large impact on test

error. Our hypothesis is that a large λ doesn’t pro-

vide the model with enough freedom to learn use-

ful representations since the optimizations focus

would largely be on minimizing the Wasserstein

distance, while a small λ permits this freedom.

One reason we might be seeing this generaliza-

tion gap might be due to the way we satisfy the

Lipschitz constraint. It’s been shown that there

are better constraints than clipping parameters to

a compact space such as a gradient penalty (Gul-

rajani et al., 2017). This is a future direction that

can be explored.

6 Discussion

Universal Grammar also comments on the learn-

ability of grammar, stating that statistical infor-

mation alone is not enough to learn grammar

and some form of native language faculty must

exist, sometimes titled the poverty of stimulus

(POS) argument (Chomsky, 2010; Lewis and El-

man, 2001). The goal of our paper is not to make

a statement on the Universal Grammar hypothesis.

But from a machine learning perspective, we’re in-

terested in extracting informative features. That

being said it is of interest to what extent language

models capture grammar and furthermore the ex-

tent to which models trained with our objective

learn grammar.

One way to measure universality is by studying

perplexity of our multi-lingual language model as

we increase the number of languages. To do so

we trained 6 UG-WGAN models on the follow-

ing languages: English, Russian, Arabic, Chinese,

German, Spanish, French. We maintain the same

procedure as described above. The hidden size of

the language model was increased to 1024 with

16K BPE tokens being used. The first model was

trained on English Russian, second was trained on

English Russian Arabic and so on. For Arabic we

still trained from left to right even though naturally

the language is read from right to left. We report

the results in Figure 5. As we increase the num-

ber of languages the perplexity gap between con-

strained and unconstrained UG-WGAN (λ = 0.0)
decreases which implies while controlling capac-

ity, our constrained (universal λ = 0.1) language
model, models language (almost) as well as jointly

trained language models with no universal con-



4039



4040

λ = 0.0 λ = 0.1

en earth’s oxide is a monopoly that occurs towing of the
carbon-booed trunks, resulting in a beam containing
of oxygen through the soil, salt, warm waters, and
the different proteins.

the practice of epimatic behaviours may be required
in many ways of all non-traditional entities.

the groove and the products are numeric because
they are called ”pressibility” (ms) nutrients contain-
ing specific different principles that are available
from the root of their family, including a wide va-
riety of molecular and biochemical elements.

a state line is a self-government environment for sta-
tistical cooperation, which is affected by the monks
of canada, the east midland of the united kingdom.

however, compared to the listing of special defini-
tions, it has evolved to be congruent with structural
introductions, allowing to form the chemical form.

the vernacular concept of physical law is not as an
objection (the whis) but as a universal school.

es la revista más reciente varió el manuscrito original-
mente por primera vez en la revista publicada en
1994.

en el municipio real se localiza al mar del norte y
su entorno en escajárı́os alto, con mayor variedad de
cı́clica población en forma de cerca de 1070 km2.

de hecho la primera canción de ”blebe cantas”,
pahka zanjiwtryinvined cot de entre clases de
fanáticas, apareció en el ornitólogo sello triusion, jr.,
en la famosa publicación playboy de john allen.

fue el último habitantes de suecia, con tres hijos,
atasaurus y aminkinano (nuestra).

The names of large predators in charlesosaurus in-
clude bird turtles hibernated by aerial fighters and
ignored fish.

jaime en veracruz fue llamado papa del conde mayor
de valdechio, hijo de diego de zúñiga.

Table 3: Example of samples from UG-WGAN with λ = 0.0 and λ = 0.1

References

Laura M Ahearn. 2016. Living language: An intro-
duction to linguistic anthropology, volume 2. John
Wiley & Sons.

Jeanette Altarriba. 1992. The representation of trans-
lation equivalents in bilingual memory. Cognitive
processing in bilinguals, 83:157–174.

Waleed Ammar, George Mulcaire, Yulia Tsvetkov,
Guillaume Lample, Chris Dyer, and Noah A Smith.
2016. Massively multilingual word embeddings.
arXiv preprint arXiv:1602.01925.

Martin Arjovsky, Soumith Chintala, and Léon Bot-
tou. 2017. Wasserstein gan. arXiv preprint
arXiv:1701.07875.

Mikel Artetxe, Gorka Labaka, Eneko Agirre, and
Kyunghyun Cho. 2017. Unsupervised neural ma-
chine translation. arXiv preprint arXiv:1710.11041.

Terry Kit-Fong Au. 1983. Chinese and english counter-
factuals: the sapir-whorf hypothesis revisited. Cog-
nition, 15(1-3):155–187.

Shlomo Bentin, Gregory McCarthy, and Charles C
Wood. 1985. Event-related potentials, lexical deci-
sion and semantic priming. Electroencephalogra-
phy and clinical Neurophysiology, 60(4):343–355.

Samuel R Bowman, Gabor Angeli, Christopher Potts,
and Christopher D Manning. 2015. A large anno-
tated corpus for learning natural language inference.
arXiv preprint arXiv:1508.05326.

Danqi Chen, Adam Fisch, Jason Weston, and An-
toine Bordes. 2017. Reading wikipedia to an-
swer open-domain questions. arXiv preprint
arXiv:1704.00051.

Xilun Chen and Claire Cardie. 2018. Unsupervised
multilingual word embeddings. arXiv preprint
arXiv:1808.08933.

Noam Chomsky. 2010. Poverty of stimulus: Unfin-
ished business. Transcript of a presentation given at
Johannes-Gutenberg University, Mainz.

Noam Chomsky. 2014. Aspects of the Theory of Syn-
tax, volume 11. MIT press.

Peter W Culicover. 1997. Principles and parameters:
An introduction to syntactic theory. Oxford Univer-
sity Press.

Andrew M Dai and Quoc V Le. 2015. Semi-supervised
sequence learning. In Advances in neural informa-
tion processing systems, pages 3079–3087.

Yarin Gal and Zoubin Ghahramani. 2016. A theoret-
ically grounded application of dropout in recurrent
neural networks. In Advances in neural information
processing systems, pages 1019–1027.

Ishaan Gulrajani, Faruk Ahmed, Martin Arjovsky, Vin-
cent Dumoulin, and Aaron C Courville. 2017. Im-
proved training of wasserstein gans. In Advances



4041

in Neural Information Processing Systems, pages
5767–5777.

James Higginbotham. 1987. The autonomy of syntax
and semantics.

Sepp Hochreiter and Jürgen Schmidhuber. 1997.
Long short-term memory. Neural computation,
9(8):1735–1780.

Harry Hoijer. 1954. The sapir-whorf hypothesis. Lan-
guage in culture, pages 92–105.

Sergey Ioffe and Christian Szegedy. 2015. Batch nor-
malization: Accelerating deep network training by
reducing internal covariate shift. arXiv preprint
arXiv:1502.03167.

Seonhoon Kim, Jin-Hyuk Hong, Inho Kang, and No-
jun Kwak. 2018. Semantic sentence matching with
densely-connected recurrent and co-attentive infor-
mation. arXiv preprint arXiv:1805.11360.

Diederik P Kingma and Jimmy Ba. 2014. Adam: A
method for stochastic optimization. arXiv preprint
arXiv:1412.6980.

Guillaume Klein, Yoon Kim, Yuntian Deng, Jean
Senellart, and Alexander M. Rush. 2017. Open-
NMT: Open-source toolkit for neural machine trans-
lation. In Proc. ACL.

John D Lewis and Jeffrey L Elman. 2001. Learnability
and the statistical structure of language: Poverty of
stimulus arguments revisited. In Proceedings of the
26th Annual Conference on Language Development.

Andrew L. Maas, Raymond E. Daly, Peter T. Pham,
Dan Huang, Andrew Y. Ng, and Christopher Potts.
2011. Learning word vectors for sentiment analy-
sis. In Proceedings of the 49th Annual Meeting of
the Association for Computational Linguistics: Hu-
man Language Technologies, pages 142–150, Port-
land, Oregon, USA. Association for Computational
Linguistics.

Laurens van der Maaten and Geoffrey Hinton. 2008.
Visualizing data using t-sne. Journal of machine
learning research, 9(Nov):2579–2605.

Bryan McCann, James Bradbury, Caiming Xiong, and
Richard Socher. 2017. Learned in translation: Con-
textualized word vectors. In Advances in Neural In-
formation Processing Systems, pages 6294–6305.

Grégoire Mesnil, Tomas Mikolov, Marc’Aurelio Ran-
zato, and Yoshua Bengio. 2014. Ensemble of
generative and discriminative techniques for senti-
ment analysis of movie reviews. arXiv preprint
arXiv:1412.5335.

Aaron Mitchel. 2005. Do bilinguals access a shared or
separate conceptual store? creating false memories
in a mixed-language paradigm.

Richard Montague. 1970. Universal grammar. Theo-
ria, 36(3):373–398.

Matthew E. Peters, Mark Neumann, Mohit Iyyer, Matt
Gardner, Christopher Clark, Kenton Lee, and Luke
Zettlemoyer. 2018. Deep contextualized word rep-
resentations. CoRR, abs/1802.05365.

Alec Radford, Rafal Jozefowicz, and Ilya Sutskever.
2017. Learning to generate reviews and discovering
sentiment. arXiv preprint arXiv:1704.01444.

Holger Schwenk and Matthijs Douze. 2017. Learn-
ing joint multilingual sentence representations
with neural machine translation. arXiv preprint
arXiv:1704.04154.

Rico Sennrich, Barry Haddow, and Alexandra Birch.
2015. Neural machine translation of rare words with
subword units. arXiv preprint arXiv:1508.07909.

Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky,
Ilya Sutskever, and Ruslan Salakhutdinov. 2014.
Dropout: a simple way to prevent neural networks
from overfitting. The Journal of Machine Learning
Research, 15(1):1929–1958.

Chuanqi Tan, Furu Wei, Wenhui Wang, Weifeng Lv,
and Ming Zhou. 2018. Multiway attention networks
for modeling sentence pairs. In IJCAI, pages 4411–
4417.

Songbo Tan and Jin Zhang. 2008. An empirical study
of sentiment analysis for chinese documents. Expert
Systems with applications, 34(4):2622–2629.

Cédric Villani. 2008. Optimal transport: old and new,
volume 338. Springer Science & Business Media.


