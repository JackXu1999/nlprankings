



















































Improving Knowledge Base Construction from Robust Infobox Extraction


Proceedings of NAACL-HLT 2019, pages 138–148
Minneapolis, Minnesota, June 2 - June 7, 2019. c©2019 Association for Computational Linguistics

138

Improving Knowledge Base Construction from Robust Infobox Extraction

Boya Peng∗ Yejin Huh
Apple Inc.

1 Apple Park Way
Cupertino, CA, USA

{yejin.huh,xiaoling}@apple.com

Xiao Ling Michele Banko∗

Sentropy Technologies

{emma,mbanko}@sentropy.io∗

Abstract

A capable, automatic Question Answering
(QA) system can provide more complete
and accurate answers using a comprehen-
sive knowledge base (KB). One important
approach to constructing a comprehensive
knowledge base is to extract information from
Wikipedia infobox tables to populate an ex-
isting KB. Despite previous successes in the
Infobox Extraction (IBE) problem (e.g., DB-
pedia), three major challenges remain: 1) De-
terministic extraction patterns used in DBpe-
dia are vulnerable to template changes; 2)
Over-trusting Wikipedia anchor links can lead
to entity disambiguation errors; 3) Heuristic-
based extraction of unlinkable entities yields
low precision, hurting both accuracy and com-
pleteness of the final KB. This paper presents
a robust approach that tackles all three chal-
lenges. We build probabilistic models to pre-
dict relations between entity mentions directly
from the infobox tables in HTML. The en-
tity mentions are linked to identifiers in an ex-
isting KB if possible. The unlinkable ones
are also parsed and preserved in the final out-
put. Training data for both the relation extrac-
tion and the entity linking models are auto-
matically generated using distant supervision.
We demonstrate the empirical effectiveness of
the proposed method in both precision and re-
call compared to a strong IBE baseline, DBpe-
dia, with an absolute improvement of 41.3%
in average F1. We also show that our ex-
traction makes the final KB significantly more
complete, improving the completeness score
of list-value relation types by 61.4%.

1 Introduction

Most existing knowledge bases (KBs) are largely
incomplete. This can be seen in Wikidata (Vran-
dečić and Krötzsch, 2014), which is a widely used

∗ Work performed at Apple Inc.

knowledge graph created largely by human edi-
tors. Only 46% of person entities in Wikidata
have birth places available 1. An estimate of 584
million facts are maintained in Wikipedia, not in
Wikidata (Hellmann, 2018). A downstream appli-
cation such as Question Answering (QA) will suf-
fer from this incompleteness, and fail to answer
certain questions or even provide an incorrect an-
swer especially for a question about a list of enti-
ties due to a closed-world assumption. Previous
work on enriching and growing existing knowl-
edge bases includes relation extraction on nat-
ural language text (Wu and Weld, 2007; Mintz
et al., 2009; Hoffmann et al., 2011; Surdeanu et al.,
2012; Koch et al., 2014), knowledge base reason-
ing from existing facts (Lao et al., 2011; Guu et al.,
2015; Das et al., 2017), and many others (Dong
et al., 2014).

Wikipedia (https://wikipedia.org)
has been one of the key resources used for
knowledge base construction. In many Wikipedia
pages, a summary table of the subject, called an
infobox table, is presented in the top right region
of the page (see the leftmost table in Figure 1 for
the infobox table of The_Beatles). Infobox
tables offer a unique opportunity for extracting
information and populating a knowledge base.
An infobox table is structurally formatted as an
HTML table and therefore it is often not necessary
to parse the text into a syntactic parse tree as
in natural language extraction. Intra-Wikipedia
anchor links are prevalent in infobox tables,
often providing unambiguous references to en-
tities. Most importantly, a significant amount
of information represented in the infobox tables
are not otherwise available in a more traditional
structured knowledge base, such as Wikidata.

We are not the first to use infobox tables

1as of June 2018

https://wikipedia.org


139

for knowledge base completion. The pioneering
work of DBpedia (Auer et al., 2007; Lehmann
et al., 2015)2 extracts canonical knowledge triples
(subject, relation type, object) from infobox
tables with a manually created mapping from Me-
diawiki 3 templates to relation types. Despite the
success of the DBpedia project, three major chal-
lenges remain. First, deterministic mappings are
sensitive to template changes. If Wikipedia modi-
fies an infobox template (e.g., the attribute “birth-
Date” is renamed to “dateOfBirth”), the DBpe-
dia mappings need to be manually updated. Sec-
ondly, while Wikipedia anchor links facilitate dis-
ambiguation of string values in the infobox ta-
bles, blindly trusting the anchor links can cause
errors. For instance, both “Sasha” and “Malia,”
children of Barack Obama, are linked to a sec-
tion of the Wikipedia page of Barack_Obama,
rather than their own pages. Finally, little atten-
tion has been paid to the extraction of unlinkable
entities. For example, Larry King has married
seven women, only one of which can be linked to
a Wikipedia page. A knowledge base without the
information of the other six entities will provide
an incorrect answer to the question “How many
women has Larry King married?”

In this paper, we present a system, RIBE, to
tackle all three challenges: 1) We build proba-
bilistic models to predict relations and object en-
tity links. The learned models are more robust
to changes in the underlying data representation
than manually maintained mappings. 2) We incor-
porate the information from HTML anchors and
build an entity linking system to link string values
to entities rather than fully relying on the anchor
links. 3) We produce high-quality extractions even
when the objects are unlinkable, which improves
the completeness of the final knowledge base.

We demonstrate that the proposed method is ef-
fective in extracting over 50 relation types. Com-
pared to a strong IBE baseline, DBpedia, our
extractions achieve significant improvements on
both precision and recall, with a 41.3% increase in
average F1 score. We also show that the extracted
triples add a great value to an existing knowledge

2Throughout the paper, we use “DBpedia” to refer to
its infobox extraction component rather than the DBpedia
knowledge base unless specified. We use Wikidata as the
baseline knowledge base for our experiments since it is up-
dated more frequently. The last release DBpedia knowledge
base was in 2016.

3Mediawiki is a markup language that defines the page
layout and presentation of Wikipedia pages.

base, Wikidata, improving an average recall of
list-value relation types by 61.4%.

To summarize, our contributions are three-fold:

• RIBE produces high-quality extractions,
achieving higher precision and recall com-
pared to DBpedia.
• Our extractions make Wikidata more com-

plete by adding a significant number of triples
for 51 relation types.
• RIBE extracts relations with unlinkable enti-

ties, which are crucial for the completeness
of list-value relation types and the question
answering capability from a knowledge base.

2 Related Work

Auer and Lehmann (2007) proposed to extract in-
formation from infobox tables by pattern match-
ing against Mediawiki templates and parsing and
transforming them into RDF triples. However, re-
lation types of the triples remain lexical and can
be ambiguous. Lehmann et al. (2015) introduced
an ontology to reduce the ambiguity in relation
types. A mapping from infobox attributes to the
ontology 4 is manually created, which is brittle to
template changes. In contrast, RIBE is much more
robust. It trains statistical models with distant su-
pervision from Wikidata to automatically learn the
mapping from infobox attributes to Wikidata re-
lation types. RIBE properly parses infobox val-
ues into separate object mentions, instead of rely-
ing on existing Mediawiki boundaries as in (Auer
and Lehmann, 2007; Lehmann et al., 2015). The
RIBE entity linker learns to make entity link pre-
dictions rather than a direct translation of anchor
links (Auer and Lehmann, 2007; Lehmann et al.,
2015) vulnerable to human edit errors. While
there is other relevant work, due to space con-
straints, it is discussed throughout the paper and
in the appendices as appropriate.

3 Problem Definition

We define a relation as a triple (e1, r, e2) or
r(e1, e2), where r is the relation type and e1,
e2 are the subject and the object entities respec-
tively 5. We define an entity mention m as the
surface form of an entity, and a relation mention

4DBpedia Mappings Wiki at http://mappings.
dbpedia.org .

5e2 can also be a literal value such as a number or a time
expression unless specified.

http://mappings.dbpedia.org
http://mappings.dbpedia.org


140

as a pair of entity mentions (m1,m2) for a rela-
tion type r. We denote the set of infobox tables
by T = {t1, t2, ..., tn} where ti appears in the
Wikipedia page of the entity ei 6. The Infobox
Extraction problem studied in this paper aims at
extracting a set of relation triples r(e1, e2) from
an input of Wikipedia infobox tables T .

4 System Overview

We describe the RIBE system in this section.
Wikidata (Vrandečić and Krötzsch, 2014) is em-
ployed as the base external KB. We draw distant
supervision from it by matching candidate men-
tions against Wikidata relation triples for training
statistical models. We also link our mentions to
Wikidata entities 7 and compare our extractions to
it for evaluation (Tables 4, 8). The final output of
RIBE is a set of relation triples R = {ri(ei1, ei2)}.

4.1 Relation Extraction
Figure 1 depicts an overview of the relation extrac-
tor. It extracts relation mentions r(m1,m2) from
each infobox table in T in four stages: entity men-
tion generation, feature generation, distant super-
vision, and model training and inference.

4.1.1 Mention Generation
We parse each infobox table rendered in HTML,
instead of the raw Mediawiki template, to gen-
erate object mentions. As the infobox templates
evolve over time, different Mediawiki templates
have been used to describe the same type of things.
Despite the difference in templates, the rendered
infobox tables in HTML displays a surprisingly
consistent format: each row contains an attribute
cell and an attribute value cell.

We chunk the text of each attribute value cell
and generate object mentions using a few heuris-
tics such as HTML tag boundaries. Table 5 in
Appendix A shows the heuristics. Each pair of
subject and object mentions becomes one relation
mention candidate. Note that an off-the-shelf noun
phrase chunker (Sang and Buchholz, 2000) or a
Named Entity Recognition (NER) tagger (Finkel
et al., 2005) doesn’t work well in this case as they
are often trained on grammatical sentences instead
of the textual segments seen in infobox tables.

6We assume that only one representative infobox table is
available on each page.

7Note that each Wikipedia page has a corresponding
Wikidata entry. Exceptions exist but are negligible. In this
paper, we use “Wikipedia entity” and “Wikidata entity” in-
terchangeably for the same real-world entity.

4.1.2 Feature Generation
For each relation mention, we generate features
similar to the ones from Mintz et al. (2009), with
some modifications. Since the subject of a rela-
tion mention is outside the infobox, most gener-
ated features focus on the object (e.g., the word
shape of the object mention), and its context (e.g.,
the infobox attribute value). Table 6 in Appendix
A lists the complete set of features.

4.1.3 Distant Supervision
Instead of manually collecting training examples,
we use distant supervision (Craven and Kumlien,
1999; Bunescu and Mooney, 2007; Mintz et al.,
2009) from Wikidata to automatically generate
training data for relation extraction. We assume
that a pair of mentions (me1 ,m2) expresses the
relation type r if we are able to match m2 to
e2 where r(e1, e2) is a Wikidata relation. Since
the object mention m2 is non-canonical, we do a
string match between the entity name and the men-
tion text or a direct entity match if an anchor link
exists for the mention. We construct the negative
training data for a relation type r by combining
positive examples of other relation types and a ran-
dom sample of the unlabeled entity mention pairs.

4.1.4 Model Training and Inference
We train a binary logistic regression classi-
fier (Friedman et al., 2001) for each relation type.
The classifier predicts how likely a pair of entity
mentions is to express a relation type r. We only
output relations with probabilities higher than a
threshold θ (0.9 is used empirically). Otherwise, a
mention pair is deemed to not express any relation
type. We choose one binary classifier for each type
rather than a single multi-class classifier because
one pair of mentions can express multiple relation
types. For example, the mention pair (“The Beat-
les”, “England”) under the attribute “Origin” (see
Fig. 1) expresses two relation types: country of
origin and location of formation.

4.2 Entity Linking & Normalization

Figure 2 provides an overview of the entity linker
in four stages: candidate generation, feature gen-
eration, distant supervision, and model training
and inference. The subject mei of each extracted
relation mention r(mei ,m2) is trivially linked to
the subject entity ei. The entity linker links the
object mention to a Wikidata entity unless no cor-
responding entity exists, in which case we mark



141

Figure 1: Relation Extractor Overview

The Beatles

P740

Liverpool

Relation Triple
Liverpool, UK

Liverpool F.C.

University of Liverpool

...

Knowledge base

Liverpool, UK

Liverpool F.C.

University of Liverpool

...

Origin Liverpool, England

Genres Rock, pop

...

Anchor links

Distant supervision

PROP=origin

TYPE=LOC

POSITION=0

...

PROP=origin

TYPE=LOC

POSITION=0

...

REL=origin

TYPE=LOC

POSITION=0

...

Feature generation

Classifier

Linking

The Beatles

P740

England

England football team

Kingdom of England

England (country)

...

Prediction

Input

Training Inference

Linking candidates generation

Figure 2: Entity Linker Overview

m2 an unlinkable entity. We normalize the literal
values when m2 is not an entity reference.8

4.2.1 Candidate Generation
We generate candidate entities {ẽi2} form2 in each
r(me1 ,m2) extracted. Candidate entities are gen-
erated from the anchor link associated with the
mention if present, matching the surface form text
of anchor links harvested from other pages for the
same relation type r, and type-aware name match-
ing with Wikidata entities. We determine the en-
tity type(s) of the object for a relation type r from
the statistics of existing relation triples in Wiki-
data. In this work, we use a set of coarse-grained
entity types: person, band, school, organization,
location, event, creative work, and award.9

4.2.2 Feature Generation
We generate features for each candidate entity of
m2, including lexical features of m2, textual sim-
ilarity between the entity name and m2, how the
candidate entity is generated (e.g., anchor link or
name matching), and for name matching, the kind
of names the mention is matched against (e.g., har-

8For relation types with quantity objects such as popu-
lation and height, date objects such as birth date, or string
values such as website, we normalize them into a canonical
form with respect to each data type.

9When the required entity type is absent from this set
(e.g., an object value of the language spoken relation type),
no candidate is provided from type-aware name matching.

vested surface form text or an entity alias). Con-
textual information surrounding m2 in the same
infobox is also used for disambiguation.10

4.2.3 Distant Supervision
Similar to Sec. 4.1.3, we use Wikidata to distantly
supervise the entity linker. We compare each
r(me1 , ẽ

i
2) triple with all r(e1, e2) triples from

Wikidata. The candidate entity in which ẽi2 = e2
is labeled as a positive example, while the rest of
the candidate entities are considered negative ex-
amples. Note that the supervision is noisy because
there may be multiple valid or very similar canoni-
cal entities for the same r(me1 ,m2), especially for
relation types such as genre. Additional heuristic
rules are used to denoise the supervision (cf. la-
beling functions in Ratner et al. (2016)).

4.2.4 Model training & Inference
We use a logistic regression model to predict the
probability of each candidate for a mention. The
candidate entity with the highest probability for a
mention is chosen. If the highest probability does
not meet a threshold (0.5 is chosen empirically),
we mark a mention an unlinkable entity. For
location-type objects, we make a collective pre-
diction that the entity choices of neighboring men-
tions are decided altogether (see Appendix B.2).

10See feature examples in Table 7 of Appendix A.



142

5 Experiments

In this section, we conduct experiments to answer
the following questions:

• Does RIBE produce high-quality extrac-
tions? (Sec. 5.2)
• Are these extractions a significant addition to

Wikidata? (Sec. 5.3)
• How does the extraction with unlinkable en-

tities affect the quality of the KB especially
for the list-valued relation types? (Sec. 5.4)

5.1 Data
We use Wikipedia infobox tables and Wikidata 11

to construct our training and evaluation data set.
We test RIBE on both coarse-grained entity types
such as person, location and event as well as fine-
grained ones such as band and school. We denote
the set of entity types ET . A set of 51 Wikidata
relation types RT is used as extraction targets, a
sample of which is shown in Table 4 12.

To assign types to entities, we first recursively
traverse Wikidata to collect all sub-types of a tar-
get entity type t ∈ ET via the subclass of relation.
For example, both city and state are subclasses of
type location. Next, we use the instance of rela-
tion type to identify types for entities. An entity is
deemed type t if and only if it appears as the sub-
ject of an instance of relation, where the object is
one of the sub-types of t. Around 1.2 million in-
fobox tables remain after a subject type filtering.
Table 1 displays a type breakdown.

5.2 Extraction Quality
We compare RIBE with a strong IBE baseline,
DBpedia (Lehmann et al., 2015). We obtain ex-
tractions from DBpedia by running their extrac-
tor13 on the same set of 1.2 million infobox ta-
bles. We identified 74 mappings of relation types
between DBpedia and Wikidata from the DBpe-
dia ontology14. 23 of them overlap with our target
relation set RT (see Table 3). We divide DBpe-
dia relation types by subject types. For instance,
we create two sub-types person:record_label and
band:record_label from the DBpedia relation type
recordLabel, where the first one includes record-
Label relations with subject type person, and the
second one with band. Table 2 lists the complete

11An 11/2018 Wikipedia and a 06/2018 Wikidata dump.
12The full list is provided in Table 8 in Appendix C
13
https://github.com/dbpedia/extraction-framework/

14
https://wiki.dbpedia.org/Downloads2014#h395-1

Subject entity type Percentage (%)
Person 81.5
Event 10.4

Location 3.3
Band 2.7

School 1.9
Table 1: Infobox subject entity type breakdown.

Subject:Relation type Wikidata DBpedia
per:team P54 team

per:place_of_birth P19 birthPlace
per:date_of_birth P569 birthDate

per:occupation P106 occupation
per:place_of_death P20 deathPlace
per:date_of_death P570 deathDate
per:educated_at P69 almaMater

band:has_member P527

bandMember
formerBandMember

currentMember
pastMember

per:spouse P26 spouse
per:award P166 award
per:child P40 child

per:political_party P102 party
per:partner P551 residence

per:record_label P264 recordLabel*
per:parent K206 parent
band:genre P136 genre

per:instrument P1303 instrument
band:record_label P264 recordLabel*

per:employer P108 employer
per:burial_place P119 placeOfBurial

school:student_count P2196 numberOfStudents
band:country P17 country
event:country P17 country

loc:largest_city K223 largestCity
loc:ethic_group P172 ethnicGroup*

Table 2: Mappings from Wikidata relation types to DB-
pedia ones. The relation types followed by * are miss-
ing from the output generated by the DBpedia extrac-
tion code but present in the latest public DBpedia data
release. We use the latter for those relation types for a
fair comparison.

mappings of relation types from Wikidata to DB-
pedia.

5.2.1 Evaluation Methodology
We evaluate the extraction quality for each rela-
tion type r ∈ RT using four metrics: yield, pre-
cision, recall, and F1 score. Yield (Y) is de-
fined as the number of all uniquely extracted re-
lations r(e1, e2). To compute precision and re-
call, we first collect a ground truth set of relations
G = {Gr|r ∈ RT}. 15 We create a union of ex-
tractions from RIBE and DBpedia, and randomly
sample around 100 entities with at least one re-
lation extracted from either system. This is sim-
ilar to the pooling methodology used in the TAC
KBP evaluation process (TAC, 2017). The sam-
pled extractions are graded by human annotators.

15If both systems have low true recall, this ground truth set
will have low recall as well. While we leave a better estimate
of recall for future work, anecdotally we see that that is not
the case here.

https://github.com/dbpedia/extraction-framework/
https://wiki.dbpedia.org/Downloads2014#h395-1


143

Subject Relation Type Yield P / R / F1 (%) C (%)RIBE DBpedia RIBE DBpedia RIBE DBpedia
person team 1,606,179 27598 97.9 / 99.4 / 98.6 42.8 / 0.8 / 1.5 97.6 1.1
person place of birth 1,222,396 733498 99.3 / 80.5 / 88.9 100.0 / 51.6 / 68.0 74.0 39.4
person date of birth 878,537 478383 100.0 / 98.1 / 99.0 100.0 / 48.6 / 65.4 - -
person occupation 541,901 355723 99.4 / 98.9 / 99.1 34.4 / 16.5 / 22.3 97.8 13.9
person place of death 445,128 226721 98.7 / 97.5 / 98.1 98.8 / 54.0 / 69.8 95.6 45.0
person birth name 325,594 89574 98.9 / 97.0 / 97.9 100.0 / 25.0 / 40.0 97.0 25.0
person date of death 317,489 187762 100.0 / 98.0 / 98.9 100.0 / 57.6 / 73.0 - -
person educated at 279,661 100916 93.3 / 96.2 / 94.7 100.0 / 36.9 / 53.9 94.0 32.6
band has member 186,090 49485 97.6 / 98.8 / 98.1 97.4 / 19.2 / 32.0 92.7 11.4

person spouse 156,984 40312 96.6 / 98.3 / 97.4 92.5 / 21.4 / 34.7 97.8 17.3
person award 119,136 91158 91.5 / 92.6 / 92.0 73.6 / 58.3 / 65.0 85.7 52.3
person child 115,582 30441 97.6 / 96.2 / 96.8 98.1 / 25.5 / 40.4 93.4 25.0
person political party 111,061 45528 100.0 / 96.0 / 97.9 100.0 / 36.6 / 53.5 95.5 31.4
person partner 94,541 57354 100.0 / 89.8 / 94.6 100.0 / 56.7 / 72.3 87.7 51.1
person record label 82,478 46440 99.1 / 96.2 / 97.6 100.0 / 52.8 / 69.1 90.7 38.1
person parent 79,497 36105 98.0 / 94.4 / 96.1 93.6 / 36.9 / 52.9 92.6 33.6
band genre 78,742 66843 99.2 / 98.8 / 99.0 99.1 / 76.7 / 86.4 96.9 84.6

person instrument 69,816 42150 100.0 / 94.2 / 97.0 96.1 / 40.9 / 57.3 91.8 36.7
band record label 64,148 37883 98.5 / 97.2 / 97.8 97.6 / 59.2 / 73.7 93.4 46.7

person employer 62,183 8726 93.2 / 96.8 / 94.9 92.5 / 16.1 / 27.4 95.7 20.0
person place of burial 54,430 1141 98.6 / 98.7 / 98.6 100.0 / 4.0 / 7.6 97.8 4.3
school student count 20,410 24624 98.6 / 79.6 / 88.1 98.7 / 83.9 / 90.7 - -
event country 12,625 9028 100.0 / 89.3 / 94.3 50.0 / 23.8 / 32.2 88.7 23.7

location largest city 914 3205 100.0 / 24.5 / 39.3 100.0 / 89.8 / 94.6 - -
location ethnic group 753 147 91.1 / 90.8 / 90.9 83.9 / 16.8 / 27.9 84.1 18.8

Average 97.9 / 91.9 / 93.8 89.9 / 40.3 / 52.5 92.4 31.0
Table 3: Comparison with DBpedia. P /R/F1 denotes Precision/Recall/F1 measures. C denotes the completeness score for
list-value relation types. An asterisk marks the relation types that are missing from the output generated by the DBpedia
extraction code but present in the last public DBpedia release at https://wiki.dbpedia.org/develop/datasets/
dbpedia-version-2016-10. We use the latter for those relation types to conduct a fair comparison.

According to our annotation guidelines, we mark
an extraction incorrect if one of the the following
is met.

• The relation is not expressed in the infobox.
• The object entity has an incorrect identifier.
• The object of a relation triple is unlinked by

the system but should be linked in the ground
truth. For instance, a string “United States”
not linked to its entity identifier in Wikidata
is considered incorrect.

• The object is incorrectly parsed. For exam-
ple, “Sasha and Malia” would be an incorrect
extraction for the child relation of Barack
Obama.

The final set Gr consists of all correct relations
of the sampled entities from both approaches. The
number of labels varies from 83 for event:country
to 557 for band:has_member, resulting in a total
of 4,858 labels on triples and an average of 194
per relation type. The Precision (P) of a system s,
RIBE or DBpedia, is computed as P sr =

|Usr∩Gr|
|Usr |

,
whereU sr is the set of all extracted relations of r by
system s. An absolute Recall (R) of the universe
is difficult to compute. We compute an estimated
recallRsr =

|Usr∩Gr|
|Gr| w.r.t the ground truth set. The

standard F1 Score (F1) is computed as a harmonic
mean of Pr and Rr.

5.2.2 Results

Table 3 shows that RIBE achieves better precision,
recall, F1, and yield for almost all relation types.
The DBpedia extractor underperforms for two rea-
sons. First, the extraction fully relies on Wikipedia
anchor links for entity linking, which not only
hurts the precision due to erroneous links, but also
results in a low linked ratio since mentions with-
out anchor links will not be linked. Secondly, it
treats each row in an infobox table as one mention
without proper chunking in the absence of anchor
links. This approach hurts both precision and re-
call, since an extracted string value of “Sasha and
Malia” for child not only misses the correct enti-
ties for both Sasha and Malia, but also provides
false information that “Sasha and Malia” repre-
sents one single person. In contrast, RIBE identi-
fies object mentions from infobox rows and pre-
dicts entity links even when no anchor link ex-
ists. Also, RIBE is able to consistently link to enti-
ties whose types are compatible to the target rela-
tion type. For example, we consistently link to an
occupation object (e.g., Lawyer) for occupation
rather than to a discipline (e.g., Law).

https://wiki.dbpedia.org/develop/datasets/dbpedia-version-2016-10
https://wiki.dbpedia.org/develop/datasets/dbpedia-version-2016-10


144

Relation type (Wikidata ID) Wikidata yield RIBE yield +Yield (%) Linked (%) Precision (%)
student count (P2196) 1,325 23,440 1758.5 N/A 94.78

doctoral student (P185) 2,998 9,131 263.0 78.8 97.96
has part (P527)* 105,825 185,804 166.2 18.9 100.00

recurring date (P837) 785 1,282 126.5 89.4 95.65
instrument (P1303) 56,877 69,800 93.3 98.6 100.00

member of sports team (P54) 1,190,242 1,600,285 51.3 95.0 96.69
unmarried partner (P451) 5,263 3,108 44.5 53.1 98.97
doctoral advisor (P184) 14,300 10,688 35.2 73.9 97.12

destination point (P1444) 4,942 1,824 19.1 98.8 100.00
award received (P166) 498,505 118,509 16.3 77.4 93.18
official website (P856) 525,496 100,623 11.0 N/A 100.00

population (P1082) 695,577 53,071 4.9 N/A 100.00
sibling (P3373) 188,328 12,129 4.1 75.4 100.00

country of citizenship (P27) 2,687,600 239,798 2.4 98.6 100.00

Table 4: Comparison with Wikidata for a sample of relation types. See Table 8 in Appendix C for the full list. The column
Wikidata yield shows the total number of relation triples in Wikidata per relation type. RIBE yield shows the total number of
relation triples extracted by RIBE. +Yield represents the number of relation triples we extract that are not in Wikidata divided
by Wikidata counts. Data is added to Wikidata organically by editors and the source is not limited to infoboxes. Therefore yield
comparison shows that extractions from infoboxes may complement Wikidata to construct a better knowledge graph.Linked
(%) shows that the percentage of relation triples with their objects linked to Wikidata entities (“N/A” if the object type is a
literal value). ∗ We use P527 to represent band has member where the subject entity is a band and the object entity is a current
or past member of the band.

5.3 Complement to Wikidata
We compare RIBE to Wikidata using the same sub-
ject type filter on Wikidata relation types. Table 4
shows the evaluation for a sample of relation types
(see Table 8 in Appendix C for the complete list).
To evaluate the quality of extra yield, we compute
per-relation-type precision by randomly sampling
100 relation triples that do not exist in Wikidata
(around 5.1k labels in total). The predictions are
graded by human annotators and precision is com-
puted as described in Sec. 5.2.1. RIBE achieves a
significant increase in yield over Wikidata (17 out
of 51 relation types have 100%+ increase), while
maintaining higher than 95% precision for almost
all relation types. This indicates that the extracted
triples are high-quality and a critical complement
to Wikidata.16 We observe that relation types with
person object type have a lower linked ratio since
not all objects have corresponding entities in Wiki-
data (e.g., children of celebrities).

5.4 Completeness of list-value relation types
A list-value relation type allows multiple objects
for the same subject (e.g., a person can have mul-
tiple children). In order to measure the complete-
ness of extractions for list-value relation types, we
define a completeness score C for each relation
type r using a set equality by comparing the ex-
tracted set of object values for each subject en-
tity to the ground truth set. To compute Cr, we
average the completeness scores over the sam-
pled subject entities. The “C (%)” column of Ta-

16Wikidata is mostly curated by human edits and therefore
the Wikidata yield and RIBE yield is not directly comparable.

ble 3 shows that RIBE consistently produces sub-
stantially more complete extractions than DBpedia
does.

6 Conclusion and Future Directions

We proposed a novel system, RIBE, that extracts
knowledge triples from Wikipedia infobox tables.
The proposed system produces high-quality data
and improves the average F1 score over 51 rela-
tion types by 41.3% compared to a strong IBE
system, DBpedia. We also empirically show the
added value of the extracted knowledge with re-
spect to Wikidata. Additionally, RIBE takes into
account unlinkable entities, dramatically improv-
ing the completeness of list-value relation types.

In future work, we would like to investigate
its effectiveness and robustness in a cross-lingual
setting. We would like to work on Entity Dis-
covery (Hoffart et al., 2014; Wick et al., 2013)
to discover and disambiguate the unlinkable enti-
ties. We would also like to jointly model the rela-
tion extractor and the entity linker to improve the
model performance.
Acknowledgments
We would like to thank Mike Cafarella, Pablo
Mendes, Stephen Pulman, Chris DuBois,
Lawrence Cayton, Matthias Paulik, Madian
Khabsa, and Jo Daiber for providing valuable
feedback and discussion, Mark Biswas for proof-
reading our manuscript, Thomas Semere for
leading the annotation project, and Eric Chahin,
Vivek Raghuram, and Eric Choi for the engineer-
ing support. We also appreciate the comments
from the anonymous reviewers.



145

References
Sören Auer, Christian Bizer, Georgi Kobilarov, Jens

Lehmann, Richard Cyganiak, and Zachary G. Ives.
2007. Dbpedia: A nucleus for a web of open data.
In Proceedings of the 6th International Semantic
Web Conference and 2nd Asian Semantic Web Con-
ference (ISWC/ASWC-2007), pages 722–735.

Sören Auer and Jens Lehmann. 2007. What have inns-
bruck and leipzig in common? extracting semantics
from wiki content. In ESWC.

Razvan Bunescu and Raymond Mooney. 2007. Learn-
ing to extract relations from the web using mini-
mal supervision. In Annual meeting-association for
Computational Linguistics, volume 45, page 576.

M. Craven and J. Kumlien. 1999. Constructing bio-
logical knowledge bases by extracting information
from text sources. In Proceedings of the Seventh
International Conference on Intelligent Systems for
Molecular Biology, August 6-10, 1999, Heidelberg,
Germany, pages 77–86. AAAI.

S. Cucerzan. 2007. Large-scale named entity disam-
biguation based on wikipedia data. In Proceedings
of EMNLP-CoNLL, volume 2007, pages 708–716.

Rajarshi Das, Arvind Neelakantan, David Belanger,
and Andrew McCallum. 2017. Chains of reasoning
over entities, relations, and text using recurrent neu-
ral networks. In EACL.

Xin Dong, Evgeniy Gabrilovich, Geremy Heitz, Wilko
Horn, Ni Lao, Kevin Murphy, Thomas Strohmann,
Shaohua Sun, and Wei Zhang. 2014. Knowledge
vault: a web-scale approach to probabilistic knowl-
edge fusion. In KDD.

J.R. Finkel, T. Grenager, and C. Manning. 2005. In-
corporating non-local information into information
extraction systems by gibbs sampling. In Proceed-
ings of the 43rd Annual Meeting on Association for
Computational Linguistics, pages 363–370. Associ-
ation for Computational Linguistics.

Jerome Friedman, Trevor Hastie, and Robert Tibshi-
rani. 2001. The elements of statistical learning, vol-
ume 1. Springer series in statistics New York, NY,
USA:.

Kelvin Guu, John Miller, and Percy Liang. 2015.
Traversing knowledge graphs in vector space. In
EMNLP.

Sebastian Hellmann. 2018. Wikidata Adoption
in Wikipedia. https://lists.wikimedia.
org/pipermail/wikidata/2018-December/
012681.html.

Johannes Hoffart, Yasemin Altun, and Gerhard
Weikum. 2014. Discovering emerging entities with
ambiguous names. In Proceedings of the 23rd in-
ternational conference on World wide web, pages
385–396. International World Wide Web Confer-
ences Steering Committee.

Raphael Hoffmann, Congle Zhang, Xiao Ling, Luke
Zettlemoyer, and Daniel S Weld. 2011. Knowledge-
based weak supervision for information extraction
of overlapping relations. In Proceedings of the 49th
Annual Meeting of the Association for Computa-
tional Linguistics: Human Language Technologies,
volume 1, pages 541–550.

Mitchell Koch, John Gilmer, Stephen Soderland, and
Daniel S Weld. 2014. Type-aware distantly super-
vised relation extraction with linked arguments. In
EMNLP.

Sayali Kulkarni, Amit Singh, Ganesh Ramakrishnan,
and Soumen Chakrabarti. 2009. Collective annota-
tion of Wikipedia entities in web text. In Proceed-
ings of the 15th ACM SIGKDD international con-
ference on Knowledge discovery and data mining,
pages 457–466. ACM.

Ni Lao, Tom M. Mitchell, and William W. Cohen.
2011. Random walk inference and learning in a
large scale knowledge base. In EMNLP.

Jens Lehmann, Robert Isele, Max Jakob, Anja
Jentzsch, Dimitris Kontokostas, Pablo N Mendes,
Sebastian Hellmann, Mohamed Morsey, Patrick
Van Kleef, Sören Auer, et al. 2015. Dbpedia–a
large-scale, multilingual knowledge base extracted
from wikipedia. Semantic Web, 6(2):167–195.

Xiao Ling, Sameer Singh, and Daniel S. Weld. 2015.
Design challenges for entity linking. TACL, 3:315–
328.

Mike Mintz, Steven Bills, Rion Snow, and Dan Juraf-
sky. 2009. Distant supervision for relation extrac-
tion without labeled data. In Proceedings of the
Joint Conference of the 47th Annual Meeting of the
ACL and the 4th International Joint Conference on
Natural Language Processing of the AFNLP: Vol-
ume 2 - Volume 2, ACL ’09, pages 1003–1011,
Stroudsburg, PA, USA. Association for Computa-
tional Linguistics.

Alexander Ratner, Christopher De Sa, Sen Wu, Daniel
Selsam, and Christopher Ré. 2016. Data program-
ming: Creating large training sets, quickly. Ad-
vances in neural information processing systems,
29:3567–3575.

Erik F. Tjong Kim Sang and Sabine Buchholz. 2000.
Introduction to the conll-2000 shared task chunking.
In CoNLL/LLL.

Mihai Surdeanu, Julie Tibshirani, Ramesh Nallapati,
and Christopher D Manning. 2012. Multi-instance
multi-label learning for relation extraction. In Pro-
ceedings of the 2012 Joint Conference on Empirical
Methods in Natural Language Processing and Com-
putational Natural Language Learning, pages 455–
465. Association for Computational Linguistics.

TAC. 2017. Cold start knowledge base population at
tac 2017 task description. Technical report.

https://lists.wikimedia.org/pipermail/wikidata/2018-December/012681.html
https://lists.wikimedia.org/pipermail/wikidata/2018-December/012681.html
https://lists.wikimedia.org/pipermail/wikidata/2018-December/012681.html
http://dl.acm.org/citation.cfm?id=1690219.1690287
http://dl.acm.org/citation.cfm?id=1690219.1690287
https://tac.nist.gov/2017/KBP/ColdStart/guidelines/TAC_KBP_2017_ColdStartTaskDescription_1.0.pdf
https://tac.nist.gov/2017/KBP/ColdStart/guidelines/TAC_KBP_2017_ColdStartTaskDescription_1.0.pdf


146

Denny Vrandečić and Markus Krötzsch. 2014. Wiki-
data: A free collaborative knowledgebase. Com-
mun. ACM, 57(10):78–85.

Michael Wick, Sameer Singh, Harshal Pandya, and An-
drew McCallum. 2013. A joint model for discover-
ing and linking entities. In Proceedings of the 2013
workshop on Automated knowledge base construc-
tion, pages 67–72. ACM.

Fei Wu and Daniel S. Weld. 2007. Autonomously se-
mantifying wikipedia. In Proceedings of the Inter-
national Conference on Information and Knowledge
Management (CIKM-2007), pages 41–50.

A Mention Generation Heuristics and
Example Features

We present the heuristics used in mention gener-
ation of the relation extractor in Table 5 and ex-
ample features in Table 6 and 7 for the relation
extractor and the entity linker respectively.

B Implementation Details

In this appendix, we describe a few implementa-
tion details in addressing the noise from distant
supervision and making collective entity link pre-
diction.

B.1 Denoise Distant Supervision

Distant supervision for relation extraction assumes
that every occurrence of two entities that par-
ticipate in a known relation expresses the re-
lation. However, the assumption does not al-
ways hold. For example, in the infobox table of
John Lennon (https://en.wikipedia.
org/wiki/John_Lennon), the object men-
tion “Yoko Ono” appears under both Spouse(s)
and Associated acts. Both occurrences of
(“John Lennon”, “Yoko Ono”) will be labeled as
positive instances of the spouse relation type in
Wikidata, despite the second occurrence being un-
related. Therefore, we introduce a whitelist Ar
of normalized infobox attributes 17 if the relation
type r is vulnerable to noise. A relation mention
r(m1,m2) (e.g., the second occurrence of John
Lennon and Yoko Ono) will be removed if the
attribute a (e.g., associated_act) is not inAr
(e.g., {spouse, wife, husband}).

B.2 Collective Entity Linking

Locations are often presented in multiple levels,
such as “San Jose, CA, USA”. There are many
candidate cities named “San Jose” in Wikidata.
One candidate San Jose, CA has a relation lo-
cated in 18 with California, which is a can-
didate for the mention “CA”. Another candidate
San Jose, Costa Rica does not have such
a relation with the neighboring mention and there-
fore is less likely to be the correct entity. Similarly,
if two cities are mentioned together, the candidate
entities that are in the same country are more likely
to be the correct prediction at the same time. This

17We normalize a raw infobox attribute by singularizing
and converting all tokens to lower case.

18It is equivalent to the Wikidata relation type P131, lo-
cated in the administrative territorial entity.

https://doi.org/10.1145/2629489
https://doi.org/10.1145/2629489
https://en.wikipedia.org/wiki/John_Lennon
https://en.wikipedia.org/wiki/John_Lennon


147

Heuristic Input example Output example
Keep anchor linked text intact Earnest & Young [Earnest & Young]

Split and reconstruct dates Jan 12-14th, 2013 [2013-01-12, 2013-01-14]
Split on special characters Monday/Tuesday [Monday, Tuesday]

Split on stop words Alice, and Bob [Alice, Bob]

Table 5: Heuristics used for mention generation in the relation extractor.

Feature Type Feature Example
Lexical Normalized infobox attribute of the object mention PROP=member
Lexical Position of the mention in the list of object values POS=0
Lexical Head and tail tokens of the object mention t_0=Paul
Lexical Window of k tokens to the left of the object mention t-1=Lennon
Lexical Window of k tokens to the right of the object mention t+1=George
Lexical Word shape of the object mention NUM:ALL
Lexical Type of the object mention TYPE:LOC
Lexical Whether all tokens in the mention are upper-cased OBJ_UPPER

Conjunctive Conjunction of two features PROP=member&POS=0

Table 6: Example features used in the relation extractor.

Feature Type Feature Example
CandGen Source of the candidate entity source=anchor_link
CandGen Match key between entity and mention phrase match_key=alias
CandGen Conjunctive Source and Match key P740:s=linkˆmk=phrase
CandGen Candidate Generator P740:link_match_w_phrase

Type Object entity type P740:LOC
Phrase Coarse-grained similarity score fuzzy_score=3
Context Entity and neighboring phrases Q5355602ˆphrase1=england
Context Overlap in page links and Wikidata triples has_connection_P413

Table 7: Example features used in the entity linker.

also applies to relation types such as educated at,
which expects schools as object entities. A loca-
tion mention following the school name may help
disambiguate.

We perform this collective entity linking ap-
proach (Cucerzan, 2007; Kulkarni et al., 2009;
Ling et al., 2015) in the following way: for can-
didate entities ei and ej of two neighboring men-
tions, we represent the relation between the two by
rij . From this we calculate the normalized score
(NS) for a location phrase with token length ` de-
fined as

NS =

α
∑`

i pi (` = 1)
1
`

∑`
i pi +

1

(`2)

∑`
i,j ri,j (` > 1)

, (1)

where

rij =

{
1.0 located_in(ei, ej)

0.5 same_country(ei, ej)
, (2)

pi is the local probability of ei being the correct
entity and α is a hyperparameter empirically set to

1.5. For ` > 1, the maximum NS allowed is 2. A
score larger than 1 implies a presence of either lo-
cated in or same country relation. If NS is above a
threshold, say 1.3, we sort by (`, NS) in descend-
ing order and choose the top set of entities. While
we prefer entities that are consistent with a longer
phrase, the threshold accounts for the incomplete-
ness of the relation located in. If there is no candi-
date with a NS above the threshold, we choose the
candidate with the highest NS.

Conceptually the same method (with differ-
ent link types rij) can be applied to all object
types. However in practice we found that for
non-location entities the impact was small and not
worth the increased computation.

C Supplementary Experimental Results

In this appendix, we present the complete version
of Table 4 in Table 8.



148

Relation name (Wikidata ID) Wikidata yield RIBE Yield +Yield (%) Linked (%) Precision (%)
student count (P2196) 1,325 23,440 1758.5 N/A 94.78

work period (start) (P2031) 11,154 157,273 1384.6 N/A 100.00
statistical leader (P3279) 1,699 20,637 1125.1 90.6 98.08

allegiance (P945) 4,725 37,497 721.7 99.7 98.82
largest city *** 242 919 279.7 99.9 100.00

doctoral student (P185) 2,998 9,131 263.0 78.8 97.96
record label (P264) 46,900 146,259 236.7 66.4 97.20

location (P276) 52,320 128,419 216.0 95.6 99.17
has part (P527) * 105,825 185,804 166.2 18.9 100.00
winner (P1346) 28,182 52,016 156.7 97.9 100.00

spouse (P26) 91,458 155,795 137.2 25.8 98.98
end time (P582) 19,762 33,682 136.0 N/A 98.95

recurring date (P837) 785 1,282 126.5 89.4 95.65
genre (P136) 134,712 228,276 123.6 98.2 96.77

start time (P580) 20,611 31,557 121.9 N/A 100.00
residence (P551) 61,040 94,003 121.3 97.6 100.00

instrument (P1303) 56,877 69,800 93.3 98.6 100.00
location of formation (P740) 23,405 30,878 84.5 99.8 98.10

consecrator (P1598) 4,934 5,315 83.8 92.1 98.35
child (P40) 147,599 115,621 61.2 28.7 96.91

member of sports team (P54) 1,190,242 1,600,285 51.3 95.0 96.69
place of burial (P119) 88,068 53,584 49.6 80.9 100.00

conflict (P607) 92,886 73,931 49.2 98.5 98.15
dissolved (P576) 32,589 16,409 48.8 N/A 99.04

unmarried partner (P451) 5,263 3,108 44.5 53.1 98.97
place of death (P20) 637,832 441,445 41.7 96.2 97.12

musical conductor (P3300) 340 236 40.3 60.3 100.00
place of birth (P19) 1,685,695 1,218,098 37.1 96.5 100.00

doctoral advisor (P184) 14,300 10,688 35.2 73.9 97.12
parent ** 141,409 79,372 33.9 48.7 100.00

family (P53) 22,245 14,590 33.6 84.6 88.78
student (P802) 12,665 3,813 26.3 68.1 100.00

destination point (P1444) 4,942 1,824 19.1 98.8 100.00
start point (P1427) 5,115 1,912 18.6 99.1 98.95
employer (P108) 247,406 61,741 17.1 90.9 95.54

member of political party (P102) 238,962 110,263 16.6 97.7 95.96
award received (P166) 498,505 118,509 16.3 77.4 93.18

religion (P140) 54,654 9,519 14.4 99.2 98.99
educated at (P69) 752,542 277,301 14.3 95.1 93.16

point in time (P585) 164,951 34,960 13.4 N/A 98.95
official website (P856) 525,496 100,623 11.0 N/A 100.00

occupation (P106) 3,624,331 539,557 8.5 93.4 98.99
inception (P571) 349,012 49,202 8.0 N/A 98.98

population (P1082) 695,577 53,071 4.9 N/A 100.00
date of birth (P569) 2,685,493 876,610 4.6 N/A 100.00

sibling (P3373) 188,328 12,129 4.1 75.4 100.00
date of death (P570) 1,309,427 315,381 3.9 N/A 99.00

country of citizenship (P27) 2,687,600 239,798 2.4 98.6 100.00
country (P17) 1,690,459 34,707 1.6 99.9 95.90

languages (P1412) 611,498 6,891 0.4 99.4 98.90
sport (P641) 689,697 5,466 0.2 100.0 100.00

Table 8: Comparison with Wikidata. The column Wikidata yield shows the total number of relation triples in
Wikidata per relation type. RIBE yield shows the total number of relation triples extracted by RIBE. +Yield shows
the number of relation triples we extract that are not in Wikidata. Linked (%) shows that the percentage of relation
triples with their objects linked to Wikidata entities. ∗ We use P527 to represent band has member where the
subject entity is a band and the object entity is a current or past member of the band. Note that parent and largest
city in Table 8 do not currently exist in Wikidata. ** We combine relations for Wikidata predicates father (P22)
and mother (P25) to form the parent relation type. *** We collect all the cities located in every location entity, e,
in Wikidata that contains cities and use the city with the largest and latest population number as the largest city of
e. This way, we construct a total of 242 largest city relations from Wikidata.


