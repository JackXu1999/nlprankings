




































Code-Switching for Enhancing NMT with Pre-Specified Translation


Proceedings of NAACL-HLT 2019, pages 449–459
Minneapolis, Minnesota, June 2 - June 7, 2019. c©2019 Association for Computational Linguistics

449

Code-Switching for Enhancing NMT with Pre-Specified Translation

Kai Song1,2, Yue Zhang3, Heng Yu2, Weihua Luo2, Kun Wang1, Min Zhang1
1 Soochow University, Suzhou, China

2 Alibaba DAMO Academy, Hangzhou, China
3 School of Engineering, Westlake University, Hangzhou, China

songkai.sk@alibaba-inc.com, zhangyue@wias.org.cn
{yuheng.yh,weihua.luowh}@alibaba-inc.com

kwang1994@stu.suda.edu.cn, minzhang@suda.edu.cn

Abstract

Leveraging user-provided translation to con-
strain NMT has practical significance. Exist-
ing methods can be classified into two main
categories, namely the use of placeholder tags
for lexicon words and the use of hard con-
straints during decoding. Both methods can
hurt translation fidelity for various reasons.
We investigate a data augmentation method,
making code-switched training data by replac-
ing source phrases with their target transla-
tions. Our method does not change the NMT
model or decoding algorithm, allowing the
model to learn lexicon translations by copy-
ing source-side target words. Extensive exper-
iments show that our method achieves consis-
tent improvements over existing approaches,
improving translation of constrained words
without hurting unconstrained words.

1 Introduction

One important research question in domain-
specific machine translation (Luong and Manning,
2015) is how to impose translation constraints
(Crego et al., 2016; Hokamp and Liu, 2017; Post
and Vilar, 2018). As shown in Figure 1 (a), the
word “breadboard” can be translated into “切面
包板 (a wooden board that is used to cut bread
on)” in the food domain, but “电路板 (a con-
struction base for prototyping of electronics)” in
the electronic domain. To enhance translation
quality, a lexicon can be leveraged for domain-
specific or user-provided words (Arthur et al.,
2016; Hasler et al., 2018). We investigate the
method of leveraging pre-specified translation for
NMT using such a lexicon.

For leveraging pre-specified translation, one ex-
isting approach uses placeholder tags to substitute
named entities (Crego et al., 2016; Li et al., 2016;
Wang et al., 2017b) or rare words (Luong et al.,

Input:  I want a breadboard 

Output:     

Constrained:    

I

breadboard
user-provided or domain-specific dictionary:

Input: I want a breadboard 

Code-switched: I want a  

Output:    

(a) Constrained NMT (b) Constrained NMT: Our Method

want a

I want a I want a

Figure 1: Constrained NMT

2014) on both the source and target sides during
training, so that a model can translate such words
by learning to translate placeholder tags. For ex-
ample, the i-th named entity in the source sentence
is replaced with “tagi”, as well as its correspond-
ing translation in the target side. Placeholder tags
in the output are replaced with pre-specified trans-
lation as a post-processing step. One disadvantage
of this approach, however, is that the meaning of
the original words in the pre-specified translation
is not fully retained, which can be harmful to both
adequacy and fluency of the output.

Another approach (Hokamp and Liu, 2017; Post
and Vilar, 2018) imposes pre-specified transla-
tion via lexical constraints, making sure such con-
straints are satisfied by modifying NMT decod-
ing. This method ensures that pre-specified trans-
lations appear in the output. A problem of this
method is that it does not explicitly explore the
correlation between pre-specified translations and
their corresponding source words during decod-
ing, and thus can hurt translation fidelity (Hasler
et al., 2018). There is not a mechanism that allows
the model to learn constraint translations during
training, which the placeholder method allows.

We investigate a novel method based on data
augmentation, which combines the advantages of
both methods above. The idea is to construct syn-
thetic parallel sentences from the original paral-



450

lel training data. The synthetic sentence pairs re-
semble code-switched source sentences and their
translations, where certain source words are re-
placed with their corresponding target transla-
tions. The motivation is to make the model learn to
“translate” embedded pre-specified translations by
copying them from the modified source. During
decoding, the source is similarly modified as a pre-
processing step. As shown in Figure 1 (b), trans-
lation is executed over the code-switched source,
without further constraints or post-processing.

In contrast to the placeholder method, our
method keeps lexical semantic information (i.e.
target words v.s. placeholder tags) in the source,
which can lead to more adequate translations.
Compared with the lexical constraint method, pre-
specified translation is learned because such in-
formation is available both in training and de-
coding. As a data augmentation method, it can
be used on any NMT architecture. In addi-
tion, our method enables the model to translate
code-switched source sentences, and preserve its
strength in translating un-replaced sentences.

To further strengthen copying, we propose two
model-level adjustments: First, we share target-
side embeddings with source-side target words, so
that target vocabulary words have a unique embed-
ding in the NMT system. Second, we integrate
pointer network (Vinyals et al., 2015; Gulcehre
et al., 2016; Gu et al., 2016; See et al., 2017) into
the decoder. The copy mechanism was firstly pro-
posed to copy source words. In our method, it is
further used to copy source-side target words.

Results on large scale English-to-Russian (En-
Ru) and Chinese-to-English (Ch-En) tasks show
that our method outperforms both placeholder and
lexical constraint methods over a state-of-the-art
Transformer (Vaswani et al., 2017) model on var-
ious test sets across different domains. We also
show that shared embedding and pointer network
can lead to more successful applications of the
copying mechanism. We release four high-quality
En-Ru e-commerce test sets translated by Russian
language experts, totalling 7169 sentences with an
average length of 211.

2 Related Work

Using placeholders. Luong et al. (2014) use an-
notated unk tags to present the unk symbols in

1To best of our knowledge, this is the first public e-
commerce test set.

training corpora, where the correspondence be-
tween source and target unk symbols are obtained
from word alignment (Brown et al., 1993). Output
unk tags are replaced through a post-processing
stage by looking up a pre-specified dictionary or
copying the corresponding source word. Crego
et al. (2016) extended unk tags symbol to spe-
cific symbols that can present name entities. Wang
et al. (2017b) and Li et al. (2016) use a similar
method. This method is limited when constrain
NMT with pre-specified translations consisting of
more general words, due to the loss of word mean-
ing when representing them with placeholder tags.
In contrast to their work, word meaning is fully
kept in modified source in our work.

Lexical constraints. Hokamp and Liu (2017)
propose an altered beam search algorithm, namely
grid beam search, which takes target-side pre-
specified translations as lexical constraints during
beam search. A potential problem of this method
is that translation fidelity is not specifically con-
sidered, since there is no indication of a match-
ing source of each pre-specific translation. In
addition, decoding speed is significantly reduced
(Post and Vilar, 2018). Hasler et al. (2018) use
alignment to gain target-side constraints’ corre-
sponding source words, simultaneously use finite-
state machines and multi-stack (Anderson et al.,
2016) decoding to guide beam search. Post and
Vilar (2018) give a fast version of Hokamp and
Liu (2017), which limits the decoding complex-
ity linearly by altering the beam search algorithm
through dynamic beam allocation.

In contrast to their methods, our method does
not make changes to the decoder, and therefore
decoding speed remains unchanged. Translation
fidelity of pre-specified source words is achieved
through a combination of training and decod-
ing procedure, where replaced source-side words
still contain their target-side meaning. As a soft
method of inserting pre-specified translation, our
method does not guarantee that all lexical con-
straints are satisfied during decoding, but has bet-
ter overall translation quality compared to their
method.

Using probabilistic lexicons. Aiming at mak-
ing use of one-to-many phrasal translations, the
following work is remotely related to our work.
Tang et al. (2016) use a phrase memory to pro-
vide extra information for their NMT encoder, dy-
namically switching between word generation and



451

phrase generation during decoding. Wang et al.
(2017a) use SMT to recommend prediction for
NMT, which contains not only translation opera-
tions of a SMT phrase table, but also alignment in-
formation and coverage information. Arthur et al.
(2016) incorporate discrete lexicons by converting
lexicon probabilities into predictive probabilities
and linearly interpolating them with NMT proba-
bility distributions.

Our method is similar in the sense that external
translations of source phrases are leveraged. How-
ever, their tasks are different. In particular, these
methods regard one-to-many translation lexicons
as a suggestion. In contrast, our task aims to con-
strain NMT translation through one-to-one pre-
specified translations. Lexical translations can be
used to generate code-switched source sentences
during training, but we do not modify NMT mod-
els by integrating translation lexicons. In addition,
our data augmentation method is more flexible,
because it is model-free.

Alkhouli et al. (2018) simulate a dictionary-
guided translation task to evaluate NMT’s align-
ment extraction. A one-to-one word translation
dictionary is used to guide NMT decoding. In their
method, a dictionary entry is limited to only one
word on both the source and target sides. In addi-
tion, a pre-specified translation can come into ef-
fect only if the corresponding source-side word is
successfully aligned during decoding.

On translating named entities, Currey et al.
(2017) augment the training data by copying
target-side sentences to the source-side, resulting
in augmented training corpora where the source
and the target sides contain identical sentences.
The augmented data is shown to improve transla-
tion performance, especially for proper nouns and
other words that are identical in the source and tar-
get languages.

3 Data augmentation

Our method is based on data augmentation. Dur-
ing training, augmented data are generated by re-
placing source words or phrases directly with their
corresponding target translations. The motivation
is to sample as many code-switched translation
pairs as possible. During decoding, given pre-
specified translations, the source sentence is mod-
ified by replacing phrases with their pre-specified
translations, so that the trained model can directly
copy embedded target translations in the output.

3.1 Training
Given a bilingual training corpus, we sample aug-
mented sentence pairs by leveraging a SMT phrase
table, which can be trained over the same bilin-
gual corpus or a different large corpus. We extract
source-target phrase pairs2 from the phrase table,
replacing source-side phrases of source sentences
using the following sampling steps:

1. Indexing between source-target phrase pairs
and training sentences: (a) For each source-
target phrase pair, we record all the match-
ing bilingual sentences that contain both the
source and target. Word alignment can be
used to ensure the phrase pairs that are mu-
tual translation. (b) We also sample bilin-
gual sentences that match two source-target
phrase pairs. In particular, given a combina-
tion of two phrase pairs, we index bilingual
sentences that match both simultaneously.

2. Sampling: (a) For each source-target phrase
pair, we keep at most k1 randomly selected
matching sentences. The source-side phrase
is replaced with its target-side translation. (b)
For each combination of two source-target
phrase pairs, we randomly sample at most
k2 matching sentences. Both source-side
matching phrases are replaced with their tar-
get translations.3

The sampled training data is added to the origi-
nal training data to form a final set of training sen-
tences.

3.2 Decoding
We impose target-side pre-specified translations to
the source by replacing source phrases with their
translations. Lexicons are defined in the form of
one-to-one source-target phrase pairs. Different
from training, the number of replaced phrases in
a source sentence is not necessarily restricted to
one or two, which will be discussed in Section
5.5. In practice, pre-specified translations can be
provided by customers or through user feedback,
which contains one identified translation for spec-
ified source segment.

4 Model

Transformer (Vaswani et al., 2017) uses self-
attention network for both encoding and decod-

2Source-side phrase is at most trigram.
3We set k1 = 100, k2 = 30 empirically.



452

ing. The encoder is composed of n stacked neu-
ral layers. For time step i in layer j, the hidden
state hi,j is calculated by employing self-attention
over the hidden states in layer j − 1, which are
{h1,j−1, h2,j−1, ..., hm,j−1}, where m is the num-
ber of source-side words.

In particular, hi,j is calculated as follows: First,
a self-attention sub-layer is employed to encode
the context. Then attention weights are computed
as scaled dot product between the current query
hi,j−1 and all keys {h1,j−1, h2,j−1, ..., hm,j−1},
normalized with a softmax function. Af-
ter that, the context vector is represented as
weighted sum of the values projected from hid-
den states in the previous layer, which are
{h1,j−1, h2,j−1, ..., hm,j−1}. The hidden state in
the previous layer and the context vector are then
connected by residual connection, followed by a
layer normalization function (Ba et al., 2016), to
produce a candidate hidden state h

′
i,j . Finally, an-

other sub-layer including a feed-forward network
(FFN) layer, followed by another residual connec-
tion and layer normalization, are used to obtain the
hidden state hi,j .

In consideration of translation quality, multi-
head attention is used instead of single-head at-
tention as mentioned above, positional encoding
is also used to compensate the missing of position
information in this model.

The decoder is also composed of n stacked
layers. For time step t in layer j, a self-
attention sub-layer of hidden state st,j is calcu-
lated by employing self-attention mechanism over
hidden states in previous target layer, which are
{s1,j−1, s2,j−1, ..., st−1,j−1}, resulting in candi-
date hidden state s

′
t,j . Then, a second target-to-

source sub-layer of hidden state st,j is inserted
above the target self-attention sub-layer. In par-
ticular, the queries(Q) are projected from s

′
t,j , and

the keys(K) and values(V ) are projected from the
source hidden states in the last layer of encoder,
which are {h1,n, h2,n, ..., hm,n}. The output state
is another candidate hidden state s

′′
t,j . Finally, a

last feed-forward sub-layer of hidden state st,j is
calculated by employing self-attention over s

′′
t,j .

A softmax layer based on decoder’s last layer
st,n is used to gain a probability distribution
Ppredict over target-side vocabulary.

p(yt|y1, ..., yt−1, x) = softmax(st,n ∗ W), (1)

where W is the weight matrix which is learned, x

i want

h1,1 h2,1 h3,1
Encoder Layer n

[ ] [ ] [ ]
s4,n

[ ]
[ ]

[ ]
[ ]

Source Embeddings Target Embeddings

i
want

: target-to-source attention weights

Linear & Softmax

: target-side vocabulary probability distribution

(1− gpred )*Pcopy

Pcopy Ppredict

gpred *Ppredict

probability distribution over source-side words and target-side vocabulary

Decoder Layer n[ ]

h4,1

[ ] a [ ]

a

iwant a

Figure 2: Shared embeddings and pointer network

represent the source sentence, {y1, y2, ..., yt} rep-
resent target words.

4.1 Shared Target Embeddings

Shared target embeddings enforces the correspon-
dence between source-side and target-side expres-
sions on the embedding level. As shown in Fig-
ure 2, during encoding, source-side target word
embeddings are identical to their embeddings in
the target-side vocabulary embedding matrix. This
makes it easier for the model to copy source-side
target words to the output.

4.2 Pointer Network

To strengthen copying through locating source-
side target words, we integrate pointer network
(Gulcehre et al., 2016) into the decoder, as shown
in Figure 2. At each decoding time step t, the
target-to-source attention weights αt,1, ...,αt,m
are utilized as a probability distribution Pcopy,
which models the probability of copying a word
from the i-th source-side position. The i-th
source-side position may represent a source-side
word or a source-side target word. Pcopy is added
to Ppredict, the probability distribution over target-
side vocabulary, to gain a new distribution over
both the source and the target side vocabulary4:

P = (1− gpred) ∗ Pcopy + gpred ∗ Ppredict , (2)

where gpred is used to control the contribution of
two probability distributions. For time step t, gpred
is calculated from the context vector ct and the
current hidden state of the decoder’s last layer st,n:

4For the words which belong to the source-side vocab-
ulary but are not appeared in the source-side sentence, the
probabilities are set to 0.



453

gpred = σ(ct ∗Wp + st,n ∗Wq + br), (3)

where Wp, Wq, and br are parameters trained and
σ is the sigmoid function. In addition, the context
vector ct is calculated as ct =

󰁓m
i=1 αt,i ∗ hi,n,

where αt,i is attention weight mentioned earlier.
{h1,n, h2,n, ..., hm,n} are the source-side hidden
states of the encoder’s last layer.

5 Experiments

We compare our method with strong baselines
on large-scale En-Ru and Ch-En tasks on var-
ious test sets across different domains, using a
strongly optimized Transformer (Vaswani et al.,
2017). BLEU (Papineni et al., 2002) is used for
evaluation.

5.1 Data

Our training corpora are taken from the
WMT2018 news translation task.

En-Ru. We use 13.88M sentences as base-
line training data, containing both a real bilin-
gual corpus and a synthetic back-translation cor-
pus (Sennrich et al., 2015a). The synthetic corpus
is translated from “NewsCommonCrawl”, which
can be obtained from the WMT task. The
news domain contains four different test sets
published by WMT2018 over the recent years,
namely “news2015”, “news2016”, “news2017”,
and “news2018”, respectively, each having one
reference. The e-commerce domain contains
four files totalling 7169 sentences, namely “sub-
ject17”, “desc17”, “subject18”, and “desc18”, re-
spectively, each having one reference. The sen-
tences are extracted from e-commerce websites,
in which “subject”s are the goods names shown
on a listing page. “desc”s refer to information in
a commodity’s description page. “subject17” and
“desc17” are released5. Our development set is
“news2015”.

Ch-En. We use 7.42M sentences as our base-
line training data, containing both real bilingual
corpus and synthetic back-translation corpus (Sen-
nrich et al., 2015a). We use seven public devel-
opment and test data sets, four in the news do-
main, namely “NIST02”, “NIST03”, “NIST04”,
“NIST05”, respectively, each with four references,
and three in the spoken language domain, namely

5https://github.com/batman2013/
e-commerce_test_sets

“CSTAR03”, “IWSLT2004”, “IWLST2005”, re-
spectively, each with 16 references. “NIST03” is
used for development.

5.2 Experimental Settings

We use six self-attention layers for both the en-
coder and the decoder. The embedding size and
the hidden size are set to 512. Eight heads are
used for self-attention. A feed-forward layer with
2048 cells and Swish (Ramachandran et al., 2018)
is used as the activation function. Adam (Kingma
and Ba, 2014) is used for training; warmup step is
16000; the learning rate is 0.0003. We use label
smoothing (Junczys-Dowmunt et al., 2016) with a
confidence score of 0.9, and all the drop-out (Gal
and Ghahramani, 2016) probabilities are set to 0.1.

We extract a SMT phrase table on the bilin-
gual training corpus by using moses (Koehn et al.,
2007) with default setting, which is used for
matching sentence pairs to generate augmented
training data. We apply count-based pruning
(Zens et al., 2012) to the phrase table, the thresh-
old is set to 10.

During decoding, similar to Hasler et al.
(2018), Alkhouli et al. (2018) and Post and Vi-
lar (2018), we make use of references to obtain
gold constraints. Following previous work, pre-
specified translations for each source sentence are
sampled from references and used by all systems
for fair comparison.

In all the baseline systems, the vocabulary size
is set to 50K on both sides. For “Data augmenta-
tion”, to allow the source-side dictionary to cover
target-side words, the target- and source-side vo-
cabularies are merged for a new source vocabu-
lary. For “Shared embeddings”, the source vo-
cabulary remains the same as the baselines, where
the source-side target words use embeddings from
target-side vocabulary.

5.3 System Configurations

We use an in-house reimplementation of Trans-
former, similar to Google’s Tensor2Tensor. For
the baselines, we reimplement Crego et al. (2016),
as well as Post and Vilar (2018). BPE (Sennrich
et al., 2015b) is used for all experiments, the oper-
ation is set to 50K. Our test sets cover news and e-
commerce domains on En-Ru, and news and spo-
ken language domains on Ch-En.

Baseline 1: Using Placeholder. We combine
Luong et al. (2014) and Crego et al. (2016). For



454

news15 news16 news17 news18 △ subject17 desc17 subject18 desc18 △

Marian 33.27 31.91 36.18 32.11 -0.15 8.03 23.21 11.02 27.94 -0.46

Transformer 33.29 31.95 36.57 32.27 - 8.56 23.53 11.95 27.90 -
+ Placeholder 33.14 32.07 36.24 32.03 -0.15 9.81 24.04 13.84 29.34 +1.27
+ Lexi. Cons. 33.50 32.62 36.65 32.88 +0.39 9.24 23.67 13.1 29.83 +0.98

Data Aug. 34.71 33.69 38.43 33.51 +1.57 10.63 25.56 14.26 30.92 +2.36
+ Share 35.28 34.37 39.02 34.44 +2.26 10.82 25.84 15.20 30.97 +2.72
+ Share&Point 36.44 35.31 40.23 35.43 +3.33 11.58 26.53 16.08 32.17 +3.61

Table 1: Results on En-Ru, one or two source phrases of each sentence have pre-specified translation. “Trans-
former” is our in-house vanilla Transformer baseline. “Marian” is the implementation of Transformer by Junczys-
Dowmunt et al. (2018), which is used as a reference of our Transformer implementation.

CSTAR03 IWSLT04 IWSLT05 △ NIST02 NIST03 NIST04 NIST05 △

Transformer 53.03 56.52 64.72 - 40.52 37.85 40.12 39.26 -
+ Placeholder 52.51 56.15 64.44 -0.39 40.01 37.16 39.96 38.87 -0.44
+ Lexi. Cons. 53.30 56.95 65.63 +0.54 40.36 38.02 40.44 39.72 +0.20

Data Aug. 53.82 57.28 65.54 +0.79 40.85 38.41 40.81 40.29 +0.65
+Share 53.90 57.67 65.59 +0.96 41.06 38.57 41.22 40.38 +0.87
+Share&Point 53.79 57.29 65.65 +0.82 41.11 38.7 41.3 40.4 +0.94

Table 2: Results on Ch-En, one or two source phrases of each sentence have pre-specified translation.

generating placeholder tags during training, fol-
lowing Crego et al. (2016), we use a named en-
tity translation dictionary which is extracted from
Wikidata6. The dictionary is released together
with e-commerce test sets, which is mentioned be-
fore. For Ch-En, the dictionary contains 285K per-
son names, 746K location names and 1.6K orga-
nization names. For En-Ru, the dictionary con-
tains 471K person names, 254K location names
and 1.5K organization names. Additionally, we
manually corrected a dictionary which contains
142K brand names and product names translation
for En-Ru. By further leveraging word alignment
in the same way as Luong et al. (2014), the place-
holder tags are annotated with indices. We use
FastAlign (Dyer et al., 2013) to generate word
alignment. The amount of sentences containing
placeholder tags is controlled to a ratio of 5% of
the corpus. During decoding, pre-specified trans-
lations described in Section 5.2 are used.

Baseline 2: Lexical Constraints. We re-
implement Post and Vilar (2018), integrating their
algorithm into our Transformer. Target-side words
or phrases of pre-specified translations mentioned
in Section 5.2 are used as lexical constraints.

6https://www.wikidata.org

Our System. During training, we use the
method described in Section 3.1 to obtain the
augmented training data. The SMT phrase table
mentioned in Section 5.2 is used for “Indexing”
and “Sampling”. During decoding, pre-specified
translations mentioned in Section 5.2 are used.
The augmented data contain sampled sentences
with one or two replacements on the source side.
By applying the two sampling steps described in
Section 3.1, about 10M and 6M augmented Ch-En
and En-Ru sentences are generated, respectively.
The final training corpora consists of both the aug-
mented training data and the original training data.

5.4 Results

Comparison with Baselines. Our Transformer
implementation can give comparable performance
with state-of-the-art NMT (Junczys-Dowmunt
et al., 2018), see “Transformer” and “Marian” in
Table 1, which also shows a comparison of dif-
ferent methods on En-Ru. The lexical constraint
method gives improvements on both the news
and the e-commerce domains, compared with the
Transformer baseline. The placeholder method
also gives an improvement on the e-commerce



455

Figure 3: Sample outputs.

domain. The average improvement is calculated
over all the test set results in each domain. In
the news domain, the average improvement of
our method is 3.48 BLEU higher compared with
placeholder, and 2.94 over lexical constraints. In
the e-commerce domain, the average improvement
of our method is 1.34 BLEU compared with place-
holder, and 2.63 with lexical constraints. Both
shared embedding and pointer network are effec-
tive. Table 2 shows the same comparison on Ch-
En. In the spoken language domain, the average
improvement is 1.35 BLEU compared with place-
holder, and 0.42 with lexical constraints. In the
news domain, the average improvement is 1.38
BLEU compared with placeholder, and 0.74 with
lexical constraints.

We find that the placeholder method can only
bring improvements on the En-Ru e-commerce
test sets, since the pre-specified translations of the
four e-commerce test sets are mostly entities, such
as brand names or product names. Using place-
holder tags to represent these entities leads to rel-
atively little loss of word meaning. But on many
of the other test sets, pre-specified translations are
mostly vocabulary words. The placeholder tags
fail to keep their word meaning during translation,
leading to lower results.

The speed contrast between unconstrained
NMT, lexical constraint and our method is shown
in Table 3. The decoding speed of our method is
equal to unconstrained NMT, and faster than the
lexical constraint method, which confirms our in-

Beam Size 5 10 20 30

Unconstrained & Ours 416 312 199 146
Lexical Constraint 102 108 74 50

Table 3: Decoding speed (words/sec), Ch-En dev set.

tuition introduced earlier.
Sample Outputs. Figure 3 gives a comparison

of different system’s translations. Given a Chi-
nese source sentence, the baseline system fails to
translate “计划生育” adequately, as “family plan-
ning” is not a correct translation of “计划生育”.
In the pre-specified methods, the correct trans-
lation (“计划生育” to “planned parenthood”) is
achieved through different ways.

For the placeholder method, the source phrase
“计划生育” is replaced with the placeholder tag
“tag1” during pre-processing. After translation,
output “tag1” is replaced with “planned parent-
hood” as a post-processing step. However, the
underlined word “program” is generated before
“planned parenthood”, which has no relationship
with any source-side word. The source-side word
“协会”, which means “association”, is omitted in
translation. Through deeper analysis, the specific
phrase “program tag1” occurs frequently in the
training data. During decoding, using the hard
tag leads to the loss of the source phrase’s origi-
nal meaning. As a result, the word “program” is
incorrectly generated along with “tag1”.

The lexical constraints method regards the tar-



456

Figure 4: Increased BLEU on Ch-En test sets.

Figure 5: Copy success rate on Ch-En test sets.

get side of the pre-specified translation as a lex-
ical constraint. Here the altered beam search al-
gorithm fails to predict the constraint “planned
parenthood” during previous decoding steps. Al-
though the constraint finally comes into effect,
over translation occurs, which is highlighted by
the underlined words. This is because the method
enforces hard constraints, preventing decoding to
stop until all constraints are met.

Our method makes use of pre-specified transla-
tion by replacing the source-side phrase “计划生
育” with the target-side translation “planned par-
enthood”, copying the desired phrase to the out-
put along with the decoding procedure. The trans-
lation “association of planned parenthood from
providing” is the exact translation of the source-
side phrase “计划(planned) 生育(parenthood) 协
会(association)提供(providing)”, and agrees with
the reference, “planned parenthood to provide”.

5.5 Analysis

Effect of Using More Pre-specified Transla-
tions. Even though the augmented training data
have only one or two replacements on the source
side, the model can translate a source sentence
with up to five replacements. Figure 4 shows
that compared with unconstrained Transformer,
the translation quality of our method keeps in-
creasing when the number of replacements in-
creases, since more pre-specified translations are
used.

We additionally measure the effect on the Ch-
En WMT test sets, namely “newsdev2017”, “new-
stest2017”, “newstest2018”, respectively, each
having only one reference instead of four. The
baseline BLEU scores on these three test sets are
18.49, 20.01 and 19.05, respectively. Our method
gives BLEU scores of 20.56, 22.3, 21.08, respec-
tively, when using one or two pre-specified trans-
lations for each sentence. The increased BLEU
when utilizing different number of pre-specified
translations is shown in Figure 4. We found that
the improvements on WMT test sets are more sig-
nificant than on NIST, since pre-specified transla-
tions are sampled from one reference only, enforc-
ing the output to match this reference. The place-
holder method does not give consistent improve-
ments on news test sets, due to the same reason as
mentioned earlier.

As shown in Figure 5, the copy success rate of
our method does not decrease significantly when
the number of replacements grows. Here, a copy
success refers a pre-specified target translation that
can occur in the output. The placeholder method
achieves a higher copy success rate than ours when
the number of replacements is 1, but the copy suc-
cess rate decreases when using more pre-specified
translations. The copy success rate of the lexi-
cal constraint method is always 100%, since it im-
poses hard constraints rather than soft constraints.
However, as discussed earlier, overall translation
quality can be harmed as a cost of satisfying de-
coding constraints by their method.

In the presented experiment results, the highest
copy success rate of our method is 90.54%, which
means a number of source-side target words or
phrases are not successfully copied to the trans-
lation output. This may be caused by the lack of
training samples for certain target-side words or
phrases. In En-Ru, we additionally train a model
with augmented data that is obtained by matching



457

NIST02 NIST03 NIST04 NIST05

Data Aug. 83.89% 85.71% 86.71% 87.45%
+Share&Point 87.72% 88.31% 89.18% 90.54%

Table 4: Copy success rate on Ch-En test sets.

news15 news16 news17 news18

Baseline 33.29 31.95 36.57 32.27
Ours 33.53 32.29 36.54 32.47

Table 5: BLEU scores of non code-switched (original)
input on En-Ru test sets.

an SMT phrase table without any pruning strategy.
The copy success rate can reach 98%, even with-
out using “shared embedding” and “pointer net-
work” methods.

Effect of Shared Embeddings and Pointer
Network. The gains of shared embeddings and
pointer network are reflected in both the copy suc-
cess rate and translation quality. As shown in Ta-
ble 4, when using one pre-specified translation for
each source sentence, the copy success rate im-
proves on various test sets by integrating shared
embeddings and pointer network, demonstrating
that more pre-specified translations come into ef-
fect. Table 1 and Table 2 earlier show the improve-
ment of translation quality.

Translating non Code-Switched Sentences.
Our method preserves its strength on translating
non code-switched sentences. As shown in Ta-
ble 5, the model trained on the augmented cor-
pus has comparable strength on translating un-
replaced sentences as the model trained on the
original corpus. In addition, on some test sets, our
method is slightly better than the baseline when
translating non code-switched source sentences.
This can be explained from two aspects: First,
the augmented data make the model more robust
to perturbed inputs; Second, the pointer network
makes the model better by copying certain source-
side words (Gulcehre et al., 2016), such as non-
transliterated named entities.

6 Conclusion

We investigated a data augmentation method for
constraining NMT with pre-specified translations,
utilizing code-switched source sentences and their
translations as augmented training data. Our
method allows the model to learn to translate

source-side target phrases by “copying” them to
the output, achieving consistent improvements
over previous lexical constraint methods on large
NMT test sets. To the best of our knowledge, we
are the first to leverage code switching for NMT
with pre-specified translations.

7 Future Work

In the future, we will study how the copy suc-
cess rate and the BLEU scores interact when dif-
ferent sampling strategies are taken to obtain aug-
mented training corpus and when the amount of
augmented data grows. Another direction is to
validate the performance when applying this ap-
proach to language pairs that contain a number of
identical letters in their alphabets, such as English
to French and English to Italian.

Acknowledgments

We thank the anonymous reviewers for their de-
tailed and constructed comments. Yue Zhang is
the corresponding author. The research work is
supported by the National Natural Science Foun-
dation of China (61525205). Thanks for Shao-
hui Kuang, Qian Cao, Zhongqiang Huang and Fei
Huang for their useful discussion.

References
Tamer Alkhouli, Gabriel Bretschner, and Hermann

Ney. 2018. On the alignment problem in multi-head
attention-based neural machine translation. arXiv
preprint arXiv:1809.03985.

Peter Anderson, Basura Fernando, Mark Johnson, and
Stephen Gould. 2016. Guided open vocabulary
image captioning with constrained beam search.
CoRR, abs/1612.00576.

Philip Arthur, Graham Neubig, and Satoshi Naka-
mura. 2016. Incorporating discrete translation lexi-
cons into neural machine translation. arXiv preprint
arXiv:1606.02006.

Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hin-
ton. 2016. Layer normalization. arXiv preprint
arXiv:1607.06450.

Peter F Brown, Vincent J Della Pietra, Stephen A Della
Pietra, and Robert L Mercer. 1993. The mathemat-
ics of statistical machine translation: Parameter esti-
mation. Computational linguistics, 19(2):263–311.

Josep Crego, Jungi Kim, Guillaume Klein, An-
abel Rebollo, Kathy Yang, Jean Senellart, Egor
Akhanov, Patrice Brunelle, Aurelien Coquard,



458

Yongchao Deng, et al. 2016. Systran’s pure neu-
ral machine translation systems. arXiv preprint
arXiv:1610.05540.

Anna Currey, Antonio Valerio Miceli Barone, and Ken-
neth Heafield. 2017. Copied monolingual data im-
proves low-resource neural machine translation. In
Proceedings of the Second Conference on Machine
Translation, pages 148–156.

Chris Dyer, Victor Chahuneau, and Noah A Smith.
2013. A simple, fast, and effective reparameteriza-
tion of ibm model 2. In Proceedings of the 2013
Conference of the North American Chapter of the
Association for Computational Linguistics: Human
Language Technologies, pages 644–648.

Yarin Gal and Zoubin Ghahramani. 2016. A theoret-
ically grounded application of dropout in recurrent
neural networks. In Advances in neural information
processing systems, pages 1019–1027.

Jiatao Gu, Zhengdong Lu, Hang Li, and Victor OK
Li. 2016. Incorporating copying mechanism in
sequence-to-sequence learning. arXiv preprint
arXiv:1603.06393.

Caglar Gulcehre, Sungjin Ahn, Ramesh Nallap-
ati, Bowen Zhou, and Yoshua Bengio. 2016.
Pointing the unknown words. arXiv preprint
arXiv:1603.08148.

Eva Hasler, Adrià De Gispert, Gonzalo Iglesias, and
Bill Byrne. 2018. Neural machine translation de-
coding with terminology constraints. arXiv preprint
arXiv:1805.03750.

Chris Hokamp and Qun Liu. 2017. Lexically con-
strained decoding for sequence generation using grid
beam search. arXiv preprint arXiv:1704.07138.

Marcin Junczys-Dowmunt, Tomasz Dwojak, and Rico
Sennrich. 2016. The amu-uedin submission to the
wmt16 news translation task: Attention-based nmt
models as feature functions in phrase-based smt.
arXiv preprint arXiv:1605.04809.

Marcin Junczys-Dowmunt, Roman Grundkiewicz,
Tomasz Dwojak, Hieu Hoang, Kenneth Heafield,
Tom Neckermann, Frank Seide, Ulrich Germann,
Alham Fikri Aji, Nikolay Bogoychev, André F. T.
Martins, and Alexandra Birch. 2018. Marian: Fast
neural machine translation in C++. In Proceedings
of ACL 2018, System Demonstrations, pages 116–
121, Melbourne, Australia. Association for Compu-
tational Linguistics.

Diederik P Kingma and Jimmy Ba. 2014. Adam: A
method for stochastic optimization. arXiv preprint
arXiv:1412.6980.

Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, et al. 2007. Moses: Open source

toolkit for statistical machine translation. In Pro-
ceedings of the 45th annual meeting of the ACL on
interactive poster and demonstration sessions, pages
177–180. Association for Computational Linguis-
tics.

Xiaoqing Li, Jiajun Zhang, and Chengqing Zong. 2016.
Neural name translation improves neural machine
translation. arXiv preprint arXiv:1607.01856.

Minh-Thang Luong and Christopher D Manning. 2015.
Stanford neural machine translation systems for spo-
ken language domains. In Proceedings of the In-
ternational Workshop on Spoken Language Transla-
tion, pages 76–79.

Minh-Thang Luong, Ilya Sutskever, Quoc V Le, Oriol
Vinyals, and Wojciech Zaremba. 2014. Addressing
the rare word problem in neural machine translation.
arXiv preprint arXiv:1410.8206.

Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a method for automatic eval-
uation of machine translation. In Proc. ACL, pages
311–318, Philadelphia, Pennsylvania, USA.

Matt Post and David Vilar. 2018. Fast lexically
constrained decoding with dynamic beam alloca-
tion for neural machine translation. arXiv preprint
arXiv:1804.06609.

Prajit Ramachandran, Barret Zoph, and Quoc V Le.
2018. Searching for activation functions.

Abigail See, Peter J Liu, and Christopher D Man-
ning. 2017. Get to the point: Summarization
with pointer-generator networks. arXiv preprint
arXiv:1704.04368.

Rico Sennrich, Barry Haddow, and Alexandra Birch.
2015a. Improving neural machine translation mod-
els with monolingual data. Computer Science.

Rico Sennrich, Barry Haddow, and Alexandra Birch.
2015b. Neural machine translation of rare
words with subword units. arXiv preprint
arXiv:1508.07909.

Yaohua Tang, Fandong Meng, Zhengdong Lu, Hang Li,
and Philip LH Yu. 2016. Neural machine transla-
tion with external phrase memory. arXiv preprint
arXiv:1606.01792.

Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob
Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz
Kaiser, and Illia Polosukhin. 2017. Attention is all
you need. CoRR, abs/1706.03762.

Oriol Vinyals, Meire Fortunato, and Navdeep Jaitly.
2015. Pointer networks. In Advances in Neural In-
formation Processing Systems, pages 2692–2700.

Xing Wang, Zhengdong Lu, Zhaopeng Tu, Hang
Li, Deyi Xiong, and Min Zhang. 2017a. Neural
machine translation advised by statistical machine
translation. In AAAI, pages 3330–3336.



459

Yuguang Wang, Shanbo Cheng, Liyang Jiang, Jia-
jun Yang, Wei Chen, Muze Li, Lin Shi, Yanfeng
Wang, and Hongtao Yang. 2017b. Sogou neural ma-
chine translation systems for wmt17. In Proceed-
ings of the Second Conference on Machine Transla-
tion, pages 410–415.

Richard Zens, Daisy Stanton, and Peng Xu. 2012. A
systematic comparison of phrase table pruning tech-
niques. In Proceedings of the 2012 Joint Confer-
ence on Empirical Methods in Natural Language
Processing and Computational Natural Language
Learning, pages 972–983. Association for Compu-
tational Linguistics.


