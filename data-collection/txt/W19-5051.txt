



















































ANU-CSIRO at MEDIQA 2019: Question Answering Using Deep Contextual Knowledge


Proceedings of the BioNLP 2019 workshop, pages 478–487
Florence, Italy, August 1, 2019. c©2019 Association for Computational Linguistics

478

ANU-CSIRO at MEDIQA 2019:
Question Answering Using Deep Contextual Knowledge

Vincent Nguyen
Australian National University

CSIRO Data61
vincent.nguyen@anu.edu.au

Sarvnaz Karimi
CSIRO Data61

Sydney, Australia
sarvnaz.karimi@csiro.au

Zhenchang Xing
Australian National University

Canberra, Australia
zhenchang.xing@anu.edu.au

Abstract

We report on our system for textual inference
and question entailment in the medical do-
main for the ACL BioNLP 2019 Shared Task,
MEDIQA. Textual inference is the task of find-
ing the semantic relationships between pairs
of text. Question entailment involves iden-
tifying pairs of questions which have similar
semantic content. To improve upon medical
natural language inference and question en-
tailment approaches to further medical ques-
tion answering, we propose a system that in-
corporates open-domain and biomedical do-
main approaches to improve semantic under-
standing and ambiguity resolution. Our mod-
els achieve 80% accuracy on medical natural
language inference (6.5% absolute improve-
ment over the original baseline), 48.9% accu-
racy on recognising medical question entail-
ment, 0.248 Spearman’s rho for question an-
swering ranking and 68.6% accuracy for ques-
tion answering classification.

1 Introduction

Medical health search is the second most searched
thematic query, representing 5% of all queries on
Google (Cocco et al., 2018). However, many
queries are semantically identical and are poten-
tially already answered by experts (Abacha and
Demner-Fushman, 2016). However, these ques-
tions may not be directly retrievable due to se-
mantic ambiguity involving abbreviations (Wu
et al., 2017), patient colloquialism (Graham and
Brookey, 2008) or esoteric terminology (Lee et al.,
2019). Furthermore, in regards to disease, tempo-
rality is a key factor in determining the relevance
of retrieved answers (Lee et al., 2019). For ex-
ample, it is more appropriate to retrieve answers
relating to the summer cold in the summer.

As a means to retrieve these questions that
are already answered by experts, question en-
tailment has been proposed to discern relation-
ships between pairs of questions. Recognising

Question Entailment (RQE) is the task of deter-
mining the relationship between a question pair,
RQE(Q1, Q2), as either entailment or not en-
tailment, where Abacha and Demner-Fushman
(2016) define question entailment as the situation
where “a question, Q1, entails another question,
Q2, if every answer to Q2 is also a complete or
partial answer to Q1.”

Natural Language Inference (NLI) is determin-
ing the relationship between pairs of sentences,
not just questions. NLI is the task of determin-
ing whether a hypothesis, H , is inferred (entail-
ment), not inferred (contradiction) or neither (neu-
tral), given a premise. In the context of question
answering (QA), it can be used to validate if the
answer can be inferred from the question.

Though RQE and NLI have thrived in the open-
domain setting (Bowman et al., 2015; Rajpurkar
et al., 2016), there are unique challenges in apply-
ing these tasks directly to the biomedical question
answering field. Previous models in the medical
domain that used NLI and RQE relied on mod-
els which were shallowly bidirectional (Romanov
and Shivade, 2018) or rule-based approaches with
shallow keyword matching techniques (Abacha
and Demner-Fushman, 2016) which would not
generalise well.

The MEDIQA (Ben Abacha et al., 2019) chal-
lenge, as part of the ACL BioNLP workshop, aims
to further research efforts in NLI and RQE by in-
troducing their applications to Biomedical QA.

In this paper, we detail our approach in
MEDIQA which addresses some of the problems
with biomedical text such as utilising deep contex-
tual relationships between words within a sentence
for semantic understanding and ambiguity associ-
ated with esoteric terminology, abbreviations, and
patient colloquialism. We combine biomedical
and open-domain approaches as a means to im-
prove generalisation and bridge the gap between
patient colloquialism and biomedical terminology.



479

2 Datasets

MEDIQA 2019 (Ben Abacha et al., 2019) pro-
vides datasets to be used for three different tasks.

Task 1: Natural Language Inference The
MEDNLI dataset is used for this task (Romanov
and Shivade, 2018). A collection of 11232 med-
ical premise-hypothesis pairs are used for train-
ing, 2817 pairs for validation and 405 for testing.
We preprocessed the text to remove punctuation,
that were designed to ensure patient anonymity as
a means to reduce noise while ensuring that sen-
tence integrity was not broken.

For example, cerebrovascular accident in
[**2948**]→ cerebrovascular accident in 2948.
Furthermore, we expand all medical abbreviations
using the ADAM database (Wu et al., 2017). For
example, On arrival to the ED T97 BP 184/94
HR 92→ On arrival to the emergency department
Temperature 97 Blood Pressure 184/94 Heart rate
92.

Task 2: Recognizing Question Entailment For
RQE, a collection of 8588 medical question pairs
for training, 302 pairs for validation (Abacha and
Demner-Fushman, 2016) and 230 pairs for testing
is released. The RQE collection aims to match
consumer health questions from the National Li-
brary of Medicine with Frequently Asked Ques-
tions (FAQs) from NIH websites.

Task 3: Question Answering Two sepa-
rate training datasets were provided from the
MEDIQA challenge (Ben Abacha et al., 2019):

LiveQAMed: 104 consumer health questions
covering different types of questions about dis-
eases and drugs alongside their associated an-
swers.

Alexa: 104 simple questions about the most fre-
quent diseases and associated answers.

No external data was used for any of the tasks
as a conscious decision in order to assess the fine-
tuning performance of our models. However, ex-
ternal data has shown to be useful in knowledge-
based approaches (Romanov and Shivade, 2018)
and we leave this as future work.

3 Our System

Due to the similarity of our approaches in the three
tasks, we first describe a shared model that was
utilised by all the tasks. Our approach extends
upon the current state-of-the-art models (Lee et al.,

Algorithm 1: Ensemble Approach for NLI,
RQE and QA
Input: Training Data, x ∈ X , Test Data,

z ∈ Z, Hyperparameters Θ,
Pre-trained Models MBrt and MBio

Output: Label Predictions, y ∈ Y
X ← PreprocessText(X);
Z ← PreprocessText(Z);
while numEpochs < totalEpochs do

for bx ∈ X do
//bx is a minibatch of X
MBioFT ← Train(MBio, bx,Θ);
MBrtFT ← Train(MBrt, bx,Θ);
//MFT denotes the fine-tuned model

end
numEpochs++;

end
for x ∈ X do

PredxBio ← Predict(MBioFT , x);
PredxBrt ← Predict(MBrtFT , x);
//Pred is the softmax score outputs from
each model
SVM ← Train(PredxBio ⊕ PredxBrt)

end
PredZBio ← Predict(MBioFT , Z);
PredZBrt ← Predict(MBrtFT , Z);
Y = Predict(SVM,PredZBio ⊕ PredZBrt);
return Y

2019; Devlin et al., 2019) in the open-domain
and apply them to the MEDIQA biomedical tasks.
As the state-of-the-art models currently employ
transfer learning, we modelled an ensemble trans-
fer learning approach used in the medical com-
puter vision domain (Menegola et al., 2017; Ku-
mar et al., 2017).

BERT As part of our strategy to combine open-
domain approaches to a biomedical focused one,
we elected to use a current state-of-the-art open-
domain approach, BERT (Devlin et al., 2019),
that is based on deeply bidirectional, unsupervised
language representation that has been trained on
Wikipedia.

BioBERT From the biomedical focused ap-
proach, we used BioBERT (Lee et al., 2019), a ver-
sion of BERT that has been pre-trained using ad-
ditional biomedical datasets, including PubMED
and PMC.



480

Table 1: Hyperparamters used for each run for Tasks 1 & 2.

Task 1 Task 2
Run Model Learning

Rate
Batch Size Epochs Learning

Rate
Batch Size Epochs

1
BioBERT 2e-5 64 1 2e-5 64 1
BERT 8e-6 32 1 8e-6 32 1

2
BioBERT 2e-5 64 40 2e-5 64 40
BERT 8e-6 32 40 8e-6 32 40

3
BioBERT x3 2e-5 64 40 2e-5 64 40
BERT 8e-6 32 40 8e-6 32 40

4
BioBERT x3 - - - 2e-5 64 -
BERT - - - 2e-5 32 -

5
BioBERT x3 1e-6 32 100 1e-6 32 100
BERT 1e-6 32 100 1e-6 32 100

Table 2: Tokenisation statistics for all Tasks.

Task Statistic Training Validation Testing
1 Average Sequence Length 386 190 64
2 Average Sequence Length 176 276 230

3
Average Sequence Length 605 632 582
Portion of Docs >512 Sequence Length 0.32 0.37 0.32

Support Vector Machine We combined our
predictions from our open-domain and biomedi-
cal domain approaches using a support vector ma-
chine (Cortes and Vapnik, 1995), which here, is
akin to using a data-driven weighting function.

Learning-to-Rank We also used learning-to-
rank models such as LambdaRank (Burges et al.,
2007) and RankNet (Burges et al., 2005), which
were implemented in Tensorflow Ranking1 for the
ranking portion of the challenge.

Sentence Embeddings When encoding our fea-
tures into sentence embeddings, we used bert-
as-service2 in conjunction with BioBERT to cre-
ate context-rich embeddings of text. In one of
our post-challenge runs, we used a biomedical
word2vec word embedding model (Chiu et al.,
2016).

Hyperparameters For all three tasks, we exper-
imented with batch sizes (2N , n ∈ {3, 4, 5, 6, 7})
and learning rates (A × 10B , A ∈ {1, 2, 3...10},
B ∈ {2, 3, 4, 5, 6}) and selected the parameters
that maximised performance on the validation set.
We used the default sequence length of 64 for
training, validation and testing of all three tasks.

1Tensorflow Ranking
2Bert-As-Service Sentence Embeddings

Algorithm For the classification tasks in the
challenge, we used an ensemble approach (see Al-
gorithm 1). First, the text training data, X , and
testing data, Z, is preprocessed. This preprocess-
ing is done differently depending on the submis-
sion and task. Preprocessing includes punctuation
removal and abbreviation expansion. This training
data is used to train the BERT and BioBERT mod-
els using hyperparameters, Θ. The softmax scores
for each training example, X , predicted by the fi-
nal fine-tuned models are concatenated (denoted
by ⊕ and used to train an SVM). The final predic-
tions for the testing set, Z, are collected by first
using the fine-tuned models to predict the softmax
scores. These softmax scores are concatenated and
fed as input into the SVM which outputs predic-
tions, Y , for the test set.

Task 1: Natural Language Inference

The models were trained as follows: For the first
and second run, BERT3 is trained for a single
epoch with a learning rate of 8e-6 with a batch size
of 32, while the BioBERT4 models were trained
with a learning rate of 2e-5 with a batch size of
64. The models had their predictions combined

3BERT Base Model
4BioBERT Pretrained Models

https://github.com/tensorflow/ranking
https://github.com/hanxiao/bert-as-service
https://storage.googleapis.com/bert_models/2018_10_18/uncased_L-24_H-1024_A-16.zip
https://github.com/naver/BioBERT-pretrained/releases/tag /v1.0-PubMed-pmc


481

via an SVM (sklearn-pandas, version 1.8.0) with
a penalty of 1.0, RBF kernel and gamma with the
’auto’ parameter, which was then used as a data-
driven weighting function. The code used for this
portion was based on the following code from the
BERT repository.5

For run 1, we established a baseline approach
with no preprocessing and the models were trained
only for one epoch. From run 2 onwards, prepro-
cessing was done to the text to remove punctuation
used for patient anonymity and expand medical
abbreviations as mentioned previously. For runs 3
and 5, instead of using a single BioBERT model,
the three variants of BioBERT were trained indi-
vidually using the same parameters as in run 2.
However, in the fourth run, early stop validation
was used to select the best models that maximised
validation accuracy. However, we excluded this
run because it had the same predictions as run 3.
In the final run, the learning rate was lowered and
trained over a larger number of epochs.

Task 2: Recognizing Question Entailment

We use the same runs as Task 1. However, we did
not do any preprocessing for any runs as it did not
have any benefit on the validation set.

Task 3: Question Answering Task 3 was a 2-
part challenge where answer snippets needed to be
ranked and classified as relevant or irrelevant.

Algorithm 2: Ensemble Approach for Rank-
ing QA
Input: Alexa Training Data, TA, LiveQA

Training Data, TL, Test Data, Z
Output: Ranked List, RL
while numEpochs < totalEpochs do

for ba, bl ∈ (TA, TL) do
//ba is a minibatch of TA
MAlexa ← Train(FE(ba),Θ);
MLiveQA ← Train(FE(bl),Θ);
//FE is a feature extractor that

vectorizes input
end
numEpochs++;

end
RLAlexa ← Predict(MAlexa, Z);
RLLiveQA ← Predict(MLiveQA, Z);
RL← RankScore(RLAlexa, RLLiveQA)
return RL

5Sentence Classification Bert Code

In this task, for the ranking task, we mainly
used an ensemble of two separate learning-to-rank
models that were trained on LiveQA and Alexa
(see Algorithm 2). We used the following features
as input to the model:

1. BioBERT sentence embedding of Question
2. BioBERT sentence embedding of Answer
3. BioBERT sentence embedding of Entailed

Answer from MedQUAD
4. NLI predictions over all candidates summed
5. NLI predictions over all candidates averaged

The first two features were embeddings that
were encoded using BioBERT, as mentioned pre-
viously. The third feature was found through the
following steps:

1. Use BM25 (Stephen Robertson, 1994) to find
the question candidates in MedQUAD, M ,
which are most related to a Question, Q.

2. Set a cut-off value, ρ to minimise the num-
ber of candidates for RQE/NLI. For the chal-
lenge, we set rho = 4.

3. Predict the question entailment between
all questions, Q and candidates M us-
ing the RQE model, predrqe(Q,m) =
RQE(Q,m ∈M).

4. Retain all candidate answers, R, that had
questions predicted to be entailed to the
Question.

5. Perform NLI on the answers in the origi-
nal ranked list, L, and all candidate answers
extracted from MedQUAD, prednli(l, r) =
NLI(l ∈ L, r ∈ R).

6. Use the answer with the highest BM25 score
for the third feature.

The fourth and fifth features were performed by
summing NLI predictions,

∑
prednli(l ∈ L, r ∈

R), and averaging, 1|R|
∑
prednli(l ∈ L, r ∈ R).

The features were fed into Tensorflow learning-
to-rank models (RankNet for run 1 and Lamb-
daRank for runs 3 and 4) with 2307 features us-
ing the Adam optimizer (Kingma and Ba, 2015), a
group size of 2 and a learning rate of 0.001.

We ensembled predictions from the two models
in two different ways. We used simple averaging
for Run 1. However, for subsequent runs, we used
RankScore (Li et al., 2013), which we define as:

https://github.com/google-research/bert/blob/master/run_classifier.py


482

Table 3: Results for all 3 tasks in the MEDIQA shared task, additional post challenge runs are included. Note:
With the exception of Task 1, all post challenge runs were evaluated using the official evaluation script.

Task 1 Task 2 Task 3
Run Accuracy Accuracy Accuracy Spearman’s Rho Precision@1
1 0.751 0.481 0.581 0.093 0.580
2 0.800 0.485 0.584 0.122 0.640
3 0.796 0.481 0.584 -0.007 0.520
4 - 0.489 0.584 -0.043 0.533
5 0.768 0.485 0.577 0.162 0.593

Post Challenge Runs
Task Description Accuracy Spearman’s Rho Precision
1 Run 5 + Maximum Sequence

Length (Validation Set)
0.827 (+0.016) - -

2 Run 5 + Maximum Seq. Length 0.489 (+0.004) - -
3 Run 5 (Corrected Submission) 0.686 (+0.109) 0.0513 (-0.111) 0.771 (+0.178)
3 Run 5 (Corrected Submission) +

Max Seq. Length
0.663 (-0.023) 0.0971 (+0.046) 0.749 (-0.022)

3 Run 1 with word2vec embed-
ding

- 0.284 (+0.189) -

3 Run 5 (Corrected Submission)
with UMLS concept expansion

0.659 (-0.027) 0.0200 (-0.0313) 0.749 (-0.022)

Rs(d ∈ D) = 1/dr. We use RankScore to score
each item in the ranked lists of Alexa and LiveQA
models. We then combine the items by summing
the documents RankScore from each model and
sorting.

For classification, the same architecture from
Tasks 1 and 2 for Runs 3 - 5 was used (4-ensemble
with SVM layer). For runs 2 and 5, we use soft-
max scores output from the classification to rank
documents.

4 Results and Discussion

Ensembles have been successfully utilised in other
biomedical domains (Kumar et al., 2017; Bri-
jesh and Zahid, 2011), with the main idea behind
using these being to incorporate complementary
strengths of the members of the ensemble. Thus,
BERT is used in conjunction with BioBERT in or-
der to correct the mistakes that the model makes
by injecting non-domain specific knowledge. This
idea was supported in our baseline experiments
on task 1 where BioBERT scored 0.7913 on val-
idation, while BERT scored 0.7715 on validation,
but ensembling resulted in a higher final score of
0.7950.

NLI Baseline System Problems Our baseline
system made characteristic mistakes on the vali-

dation set, which is shown in Table 4 for Task 1.
We found that our system had trouble with nu-
merical interpretation and, for instance, was not
able to determine the difference between type 1
and type 2 diabetes. Furthermore, this problem
is exacerbated when abbreviations and numeri-
cal interpretation are required in phrases such as
T97 BP 184/94. Thus, to aid the system in dis-
ambiguating abbreviations, we expanded all ab-
breviations using the ADAM database of com-
mon clinical abbreviations and resulted in an 0.049
increase in accuracy. Furthermore, the system
would struggle with medical forms of negation.
However, due to the use of BERT/BioBERT, con-
ventional techniques such as NegEx or removal
would break sentence integrity and reduce com-
prehension, thereby affecting word context, and
thus were not viable. Furthermore, punctuation,
in terms of patient anonymisation, is also a prob-
lem as the punctuation does not carry meaningful
semantic content and will confuse the classifiers.

RQE Baseline System Problems In task 2, we
found that our baseline system made similar mis-
takes for different reasons (see Table 5). We found
examples of what we consider near miss where the
definition of partial entailment depends on inter-
pretation. For example, in this question, the user



483

Table 4: Common mistakes made by the baseline system in Task 1.

Type Premise Hypothesis
Numerical
Interpretation

PAST MEDICAL HISTORY: Type 2 diabetes
mellitus.

the patient has type 1 diabetes

Abbreviation
and Numerical
Interpretation

On arrival to the ED T97 BP 184/94 HR 92
RR 24 88% on RA ->98% on NRB.

The patient was hypertensive in
the ED

Negation He denied headache or nausea or vomiting. He has no head pain
Semantic Gap HISTORY OF PRESENT ILLNESS:,The pa-

tient is a 54 year old male with endstage re-
nal disease secondary to type 1 diabetes who
presents for kidney transplant from wife.

patient is on insulin

wants information on hypertension (high blood
pressure). However, according to the gold stan-
dard, this is not a form of entailment, partial or
otherwise. We hypothesise that this lies on the
borderline of the entailment definition or may be
due to bias. Furthermore, our system struggles
with abbreviations. However, the examples in the
second task dataset are more related to problems
with co-reference resolution where abbreviations
appear in the original question but not in the FAQ
question.

Furthermore, phrases like “come out of” should
be aligned to terms such as “discharge”, which is
an example of a semantic gap and require com-
mon sense comprehension. This is problematic as
BERT is known to struggle with this sort of rea-
soning (Talmor et al., 2018). Also, we did not
adjust the sequence length parameter (set to 64),
which may have been a source of error. How-
ever, a later investigation through a post-challenge
run that shows that only Task 1 benefits from an
increase in sequence length (see Table 3). Fi-
nally, patient colloquialism presents a unique chal-
lenge where “hole in lung” is to be interpreted as
“pleurisy” (lung inflammation). Although we did
not address this complex problem, it could be po-
tentially solved through crowd-sourcing of medi-
cal forum data. This may be suitable as an area to
investigate for future work.

We found that in all our submissions on the
test set of the challenge, although our system was
able to achieve high results on the validation set of
79%, the models were not well suited for the test
set. Our model predicted entailment 92% of the
time on the test set, suggesting that the model is
overfitting, even though our baseline was trained
for only one epoch. We found that the cases where

the models make errors are cases where the ques-
tion contains words such as diagnosis and the dis-
ease is mentioned, but the semantic content of the
question might be about treatment rather than the
diagnosis. This is very different from the training
and validation datasets that were provided, which
were much more straightforward and did not re-
quire as much comprehension. An example il-
lustrating this difficulty is Question A: Glaucoma:
Can you mail me patient information about Glau-
coma, I was recently diagnosed and want to learn
all I can about the disease.” and Question B: How
is glaucoma diagnosed?

Question Answering Submission Problems
For the third Task, we incorrectly trained our mod-
els to recognise documents with a relevance score
of one as irrelevant. In contrast, the task is defined
to classify documents of relevance score one and
two as irrelevant. By fixing this error, we found
that we had over a 10% increase in accuracy (Ta-
ble 3). However, interestingly, we found that the
ranking quality (shown through Spearman’s Rho)
decreased. Upon investigation, we found two rea-
sons why this problem occurred: (1) our system
was able to differentiate the relevance of one from
the other three labels much better than differenti-
ating between labels of one/two against three/four.
This was reflected in the validation accuracy of our
initial incorrect model, which achieved an accu-
racy of 95% whereas the corrected model scores
only 70% on the validation set, (2) we found that
the longer the models were trained, the worse the
ranking quality became. We hypothesise that the
problem is due to how cross entropy loss and soft-
max functions work. Since the models are min-
imising KL-Divergence, the softmax scores be-
come more extreme, falling close to 1 or very



484

Table 5: Common mistakes made by the baseline system in Task 2.

Type Question A Question B
Near Miss I want more information on Hypertension and

fibromyalgia, I seem to be getting only topics
on diabetes and I do not have this. I enjoy
reading the current info.

What is high blood pressure?

Abbreviation Hi I have retinitis pigmentosa for 3years, Im
suffering from this disease. Please intoduce
me any way to treat mg eyes such as stem cell
... Thank you

Are there treatments for RP?

Semantic Gap Which drug we I take to stop water come out
of my nipple

How to Treat Nipple Discharge

Sequence
Length

... The problem is my binocular vision is not
good enough ... is there any operation that can
fix this?

What is Vision Therapy When
and why is it needed [for binoc-
ular vision]?

Patient Collo-
quialism

Cure for hole in lung. I certainly would like
to request for medical for hole in the lung

How Are Pleurisy and Other
Pleural Disorders Treated?

close to 0. This results in the differences between
scores of the documents to be very low (forming
dense clusters) which reduces ranking quality as
the ranking becomes more sensitive to noise and
uncertainty (Siddhant and Lipton, 2018).

Question Answering Baseline System Problems
Due to the error of our submissions for Task 3, we
will not discuss the mistakes that occurred within
the challenge for the pointwise ranking runs. In-
stead, we will look at the mistakes that the post-
challenge run encountered for those. However, for
the pairwise runs within the challenge, we found
that it performed much worse than expected. We
attribute this ranking deficit to two important fac-
tors.

The first is that BERT sentence embeddings are
not useful to represent sentences because the vec-
tor space is too condensed (vector representations
are very close together). The second is that our
vector representations were too large, with BERT
sentence embeddings producing embeddings up to
800 dimensions. Using 3 of these embeddings re-
sults in a very large input which would take too
long to train or hinder convergence. This effect
was observed in a post-challenge run where we
used Chiu et al. (2016)’s biomedical word2vec
embeddings and achieved a much higher Spear-
man’s Rho. The second factor was that the Lamb-
daLoss (Burges et al., 2007) function was not a
suitable objective function as the RankNet model
performed better.

From Table 2, we find that Task 3 is more ver-

bose than the other two tasks and presents unique
challenges as almost a third of the documents will
have information loss due to the limitation of max-
imum sequence length by BERT being 512 due
to quadratic memory explosion (Liu et al., 2018).
However, we did a post-challenge run where we
increased the sequence length with no noticeable
difference. This is because the majority of infor-
mation in these long sequence can be safely dis-
carded. Furthermore, the BERT truncation strat-
egy is to truncate from the end of the sentence, im-
plying that the important information is typically
at the start of the answer.

We also find that there are unique challenges
in Task 3 due to the use of real patient ques-
tions shown in Table 6. We found that problems
such as typos, grammar and spellings mistakes
were not directly fixed by the BERT/BioBERT
ensemble as the collections were pretrained on
academic or formal language (Pubmed, PMC and
Wikipedia). However, problems such as synonyms
(for example, abetalipoproteinemia and Bassen-
Kornzweig syndrome) which should be addressed
by the model were also not addressable due to a
limitation in the vocabulary of the models, which
is discussed below. Furthermore, we found cases
of near miss, for example, the model identifies
anemia and treatment options, but it is not the tar-
get disease of the question. To address these prob-
lems, we use a heuristic to expand UMLS terms in
the question and answer, and add these to the start
of the sentence to combat the mentioned problems.



485

Table 6: Common mistakes made by baseline system in Task 3

Type Question Answer
Typo abetalipoproteimemia hi, I would like to know

if there is any support for those suffering with
abetalipoproteinemia ... keen to learn how to
get it diagnosed...

abetalipoproteinemia: Abetal-
ipoproteinemia is an inherited
disorder that affects the absorp-
tion of dietary fats, cholesterol,
and fat-soluble vitamins...

Synonyms abetalipoproteimemia hi, I would like to know
if there is any support for those suffering with
abetalipoproteinemia...

Bassen-Kornzweig syndrome
(Exams and Tests): There may
be damage to the retina of
the eye (retinitis pigmentosa).
Tests that may be done to
help diagnose this condition
include...

Near miss about thalassemia treatment sir,my friend is
suffering from thalassemia ,in that majorly
red blood anemia,white blood anemia and the
blood is comming out from mouth when she
got cough .her condition is very severe...

Anemia (Treatment): Anemia
treatment depends on the cause.
- Iron deficiency anemia. Treat-
ment for this form of anemia...

Grammar
and spelling
mistakes

Absence seizures Does any damage occurre
from these spells. Mental or physical

Seizures: A seizure is a sud-
den, uncontrolled electrical dis-
turbance in the brain. It can
cause changes in your behavior,
movements or feelings, and in
levels of consciousness. If you
have two...

Semantic Gap Bad Breath I have very bad breath and at
times it can make myself and others sick. I
need some advice as to what I need to do.

Breath odor (Home Care): Use
proper dental hygiene, espe-
cially flossing. Remember that
mouthwashes are not effective
in treating the underlying prob-
lem...

We found that the model performs better on the
validation set than any of the post-challenge runs
(79% accuracy, a 5% absolute increase over the
other runs), but did not perform substantially bet-
ter on the test set (see Table 3).

Problems with Underlying Models One prob-
lem in using models such as BERT and BioBERT
is the limitation in the maximum sequence length.
This is demonstrated in the test portion of the chal-
lenge, where test set answers were much longer
than those seen in the training and validation col-
lection. These sequences were longer than the 512
sequence length limit allowed by the BERT ar-
chitecture, which is constrained due to a problem
known as the quadratic memory explosion (Liu
et al., 2018) leading to exponentially longer train-
ing times and memory usage.

Though there are ways to overcome these re-
strictions such as striding the sentences pairs and
labels, this results in contextual information being
lost and label imbalance. This restriction also hin-
ders the encoding of long-range dependencies be-
tween sequences as only contexts within a fixed
length can be considered (Dai et al., 2019).

In addition, we use BioBERT as a means of con-
tributing deep clinical contextual understanding of
sentences. However, we find that during Word-
Piece Tokenisation (Devlin et al., 2019), medical
terms are always split into their sub-word rep-
resentations as they are out-of-vocabulary, e.g.,
arthralgias → art hra al gia s. Wordpiece to-
kenisation relies on the idea that morphemes carry
meaning. However, due to the use of this non-
medical vocabulary, specific medical related mor-



486

phemes are not being learned. For instance, arthr-
(where - denotes prefix), means joints and -algias
means pain, so the correct tokenisation should be
arthralgias→ arthr algia s so that the model can
currently learn the semantic meaning behind the
morpheme. We find that these limitations hindered
the use of these models and their application to the
MEDIQA tasks.

We emphasise that there is a real-world appli-
cation with the models and methods in this chal-
lenge. However, if we were to scale our ap-
proach to real-world application, we would require
external data. Therefore for future work, given
more time, we would like to use external datasets
such as emrQA (Pampari et al., 2018) and explore
multi-task learning due to the similarity of the
three tasks and aim to incorporate other medical
tasks for a better generalisation of the biomedical
question answering. We would also want to train
the BERT models on biomedical-focused vocabu-
lary and additional data in the future as a baseline
to compare against multi-task learning.

5 Conclusions

In this shared task, we use and improve upon NLI
and RQE techniques for medical question answer-
ing. Our approach involves utilising deep con-
textual relationships between words emphasising
semantic understanding and resolving ambiguity.
We combine biomedical and open-domain strate-
gies to improve generalisation and bridge the gap
between the open-domain and biomedical domain
question answering.

Acknowledgements

This research is supported by the Australian Re-
search Training Program and the CSIRO Postgrad-
uate Scholarship.

References

Ben Abacha and Demner-Fushman. 2016. Recogniz-
ing Question Entailment for Medical Question An-
swering. American Medical Informatics Association
Annual Symposium Proceedings, 2016:310–318.

Asma Ben Abacha, Chaitanya Shivade, and Dina
Demner-Fushman. 2019. Overview of the MEDIQA
2019 shared task on textual inference, question en-
tailment and question answering. In Proceedings of
the BioNLP 2019 workshop, Florence, Italy. Associ-
ation for Computational Linguistics.

Samuel Bowman, Gabor Angeli, Christopher Potts, and
Christopher Manning. 2015. A large annotated cor-
pus for learning natural language inference. In Pro-
ceedings of the 2015 Conference on Empirical Meth-
ods in Natural Language Processing (EMNLP), Lis-
bon, Portugal. Association for Computational Lin-
guistics.

Verma Brijesh and Hassan Syed Zahid. 2011. Hybrid
ensemble approach for classification. Applied Intel-
ligence, 34(2):258–278.

Chris Burges, Tal Shaked, Erin Renshaw, Ari Lazier,
Matt Deeds, Nicole Hamilton, and Greg Hullender.
2005. Learning to rank using gradient descent. In
Proceedings of the 22Nd International Conference
on Machine Learning, ICML ’05, pages 89–96, New
York, NY. ACM.

Christopher Burges, Robert Ragno, and Quoc Le. 2007.
Learning to rank with nonsmooth cost functions. In
Advances in Neural Information Processing Systems
19, pages 193–200.

Billy Chiu, Gamal Crichton, Anna Korhonen, and
Sampo Pyysalo. 2016. How to train good word em-
beddings for biomedical NLP. In Proceedings of
the 15th Workshop on Biomedical Natural Language
Processing, pages 166–174, Berlin, Germany.

Anthony Cocco, Rachel Zordan, David Taylor, Tracey
Weiland, Stuart Dilley, Joyce Kant, Mahesha Dom-
bagolla, Andreas Hendarto, Fiona Lai, and Jennie
Hutton. 2018. Dr Google in the ED: searching for
online health information by adult emergency de-
partment patients. The Medical Journal of Aus-
tralia, 209:342–347.

Corinna Cortes and Vladimir Vapnik. 1995. Support-
vector networks. Machine Learning, 20(3):273–
297.

Zihang Dai, Zhilin Yang, Yiming Yang, Jaime Car-
bonell, Quoc Le, and Ruslan Salakhutdinov. 2019.
Transformer-xl: Attentive language models beyond
a fixed-length context. Computing Research Repos-
itory, abs/1901.02860.

Jacob Devlin, Ming-Wei Chang, Kenton Lee, and
Kristina Toutanova. 2019. BERT: Pre-training of
deep bidirectional transformers for language un-
derstanding. In Proceedings of the Conference of
the North American Chapter of the Association for
Computational Linguistics: Human Language Tech-
nologies, Minneapolis, MN.

Suzanne Graham and John Brookey. 2008. Do patients
understand? The Permanente journal, 12(3):67–69.

Diederik Kingma and Jimmy Ba. 2015. Adam: A
method for stochastic optimization. In 3rd Interna-
tional Conference on Learning Representations, San
Diego, CA.

https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5333286/
https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5333286/
https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5333286/
http://nlp.stanford.edu/pubs/snli_paper.pdf
http://nlp.stanford.edu/pubs/snli_paper.pdf
https://doi.org/10.1007/s10489-009-0194-7
https://doi.org/10.1007/s10489-009-0194-7
https://doi.org/10.1145/1102351.1102363
http://papers.nips.cc/paper/2971-learning-to-rank-with-nonsmooth-cost-functions.pdf
https://doi.org/10.18653/v1/W16-2922
https://doi.org/10.18653/v1/W16-2922
https://doi.org/10.5694/mja17.00889
https://doi.org/10.5694/mja17.00889
https://doi.org/10.5694/mja17.00889
https://doi.org/10.1023/A:1022627411411
https://doi.org/10.1023/A:1022627411411
http://arxiv.org/abs/1901.02860
http://arxiv.org/abs/1901.02860
http://arxiv.org/abs/1810.04805
http://arxiv.org/abs/1810.04805
http://arxiv.org/abs/1810.04805
https://www.ncbi.nlm.nih.gov/pubmed/21331214
https://www.ncbi.nlm.nih.gov/pubmed/21331214
http://arxiv.org/abs/1412.6980
http://arxiv.org/abs/1412.6980


487

Ashnil Kumar, Jinman Kim, David Lyndon, Michael
Fulham, and Dagan Feng. 2017. An ensemble of
fine-tuned convolutional neural networks for medi-
cal image classification. IEEE Journal of Biomedi-
cal and Health Informatics, 21(1):31–40.

Jinhyuk Lee, Wonjin Yoon, Sungdong Kim,
Donghyeon Kim, Sunkyu Kim, Chan Ho So,
and Jaewoo Kang. 2019. BioBERT: A pre-trained
biomedical language representation model for
biomedical text mining. arXiv e-prints, page
arXiv:1901.08746.

Vincent Li, Paul Thomas, and David Hawking. 2013.
Merging algorithms for enterprise search. ACM
International Conference Proceeding Series, pages
42–49.

Peter Liu, Mohammad Saleh, Etienne Pot, Ben
Goodrich, Ryan Sepassi, Lukasz Kaiser, and Noam
Shazeer. 2018. Generating wikipedia by summariz-
ing long sequences. Computing Research Reposi-
tory, abs/1801.10198.

Afonso Menegola, Julia Tavares, Michel Fornaciali,
Lin Li, Sandra Fontes de Avila, and Eduardo Valle.
2017. Recod titans at isic challenge 2017. Comput-
ing Research Repository, abs/1703.04819.

Anusri Pampari, Preethi Raghavan, Jennifer Liang, and
Jian Peng. 2018. emrqa: A large corpus for question
answering on electronic medical records. Comput-
ing Research Repository, abs/1809.00732.

Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and
Percy Liang. 2016. Squad: 100, 000+ questions
for machine comprehension of text. Computing Re-
search Repository, abs/1606.05250.

Alexey Romanov and Chaitanya Shivade. 2018.
Lessons from natural language inference in the
clinical domain. Computing Research Repository,
abs/1808.06752.

Aditya Siddhant and Zachary Lipton. 2018. Deep
bayesian active learning for natural language pro-
cessing: Results of a large-scale empirical study.
In Proceedings of the 2018 Conference on Empiri-
cal Methods in Natural Language Processing, pages
2904–2909, Brussels, Belgium.

Susan Jones Micheline Hancock-Beaulieu Mike Gat-
ford Stephen Robertson, Steve Walker. 1994. Okapi
at trec-3. In Proceedings of the Third Text REtrieval
Conference (TREC 1994), Gaithersburg, MD.

Alon Talmor, Jonathan Herzig, Nicholas Lourie, and
Jonathan Berant. 2018. Commonsenseqa: A
question answering challenge targeting common-
sense knowledge. Computing Research Repository,
abs/1811.00937.

Yonghui Wu, Joshua Denny, Rosenbloom Trent, Ran-
dolph Miller, Dario Giuse, Lulu Wang, Carmelo
Blanquicett, Ergin Soysal, Jun Xu, and Hua Xu.

2017. A long journey to short abbreviations: de-
veloping an open-source framework for clinical ab-
breviation recognition and disambiguation (CARD).
Journal of the American Medical Informatics Asso-
ciation, 24(e1):e79–e86.

https://doi.org/10.1109/JBHI.2016.2635663
https://doi.org/10.1109/JBHI.2016.2635663
https://doi.org/10.1109/JBHI.2016.2635663
http://arxiv.org/abs/1901.08746
http://arxiv.org/abs/1901.08746
http://arxiv.org/abs/1901.08746
https://doi.org/10.1145/2537734.2537750
http://arxiv.org/abs/1801.10198
http://arxiv.org/abs/1801.10198
http://arxiv.org/abs/1703.04819
http://arxiv.org/abs/1809.00732
http://arxiv.org/abs/1809.00732
http://arxiv.org/abs/1606.05250
http://arxiv.org/abs/1606.05250
http://arxiv.org/abs/1808.06752
http://arxiv.org/abs/1808.06752
https://www.aclweb.org/anthology/D18-1318
https://www.aclweb.org/anthology/D18-1318
https://www.aclweb.org/anthology/D18-1318
https://www.microsoft.com/en-us/research/publication/okapi-at-trec-3/
https://www.microsoft.com/en-us/research/publication/okapi-at-trec-3/
http://arxiv.org/abs/1811.00937
http://arxiv.org/abs/1811.00937
http://arxiv.org/abs/1811.00937
https://www.ncbi.nlm.nih.gov/pubmed/27539197
https://www.ncbi.nlm.nih.gov/pubmed/27539197
https://www.ncbi.nlm.nih.gov/pubmed/27539197

