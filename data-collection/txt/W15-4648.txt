



















































Dialogue Management based on Multi-domain Corpus


Proceedings of the SIGDIAL 2015 Conference, pages 364–373,
Prague, Czech Republic, 2-4 September 2015. c©2015 Association for Computational Linguistics

Dialogue Management based on Multi-domain Corpus

Wendong Ge
Institute of Automation

Chinese Academy of Sciences
Beijing, China

wendong.ge@ia.ac.cn

Bo Xu
Institute of Automation

Chinese Academy of Sciences
Beijing, China

xubo@ia.ac.cn

Abstract

Dialogue Management (DM) is a key is-
sue in Spoken Dialogue System. Most
of the existing data-driven DM schemes
train the dialogue policy for some specif-
ic domain (or vertical domain), only using
the dialogue corpus in this domain, which
might suffer from the scarcity of dialogue
corpus in some domains. In this paper,
we divide Dialogue Act (DA), as seman-
tic representation of utterance, into DA
type and slot parameter, where the former
one is domain-independent and the latter
one is domain-specific. Firstly, based on
multiple-domain dialogue corpus, the DA
type prediction model is trained via Re-
current Neutral Networks (RNN). More-
over, DA type decision problem is mod-
eled as a multi-order POMDP, and trans-
formed to be a one-order MDP with con-
tinuous states, which is solved by Natu-
ral Actor Critic (NAC) algorithm and ap-
plicable for every domain. Furthermore,
a slot parameter selection scheme is de-
signed to generate a complete machine DA
according to the features of specific do-
main, which yields the Multi-domain Cor-
pus based Dialogue Management (MCD-
M) scheme. Finally, extensive experimen-
tal results illustrate the performance im-
provement of the MCDM scheme, com-
pared with the existing schemes.

1 Introduction

With the fast development of Automatic Speech
Recognition (ASR) and Natural Language Pro-
cessing (NLP), a lot of Spoken Dialogue System-
s (SDS) appear in our lives as information assis-
tants. In SDS, Dialogue Management (DM), as
one of the most important modules, not only deter-

mines the current machine reaction, but also con-
trols the process of future dialogue. Thus, it is im-
portant to study DM in the establishment of SD-
S.(Michael, 2002)

A lot of studies have been done on DM.
(Thomson, 2010) introduces a new POMDP-based
framework for building spoken dialogue systems
by using Bayesian updates of the dialogue state.
(Olivier, 2011) explores the possibility of using
a set of approximate dynamic programming algo-
rithms for policy optimization in SDS, which are
combined to a method for learning a sparse repre-
sentation of the value function. (Annemiek, 2012)
analyzes current dialogue management in operat-
ing unmanned systems and develops a more ad-
vanced way of dialogue management and accom-
panying dialogue manager. (Yuan, 2012) propos-
es a task ontology model for domain independent
dialogue management, where the knowledge of a
specific task is modeled in its task ontology which
is independent from dialogue control. (Daubigney,
2012) proposes to apply the Kalman Temporal D-
ifferences (KTD) framework to the problem of di-
alogue strategy optimization so as to address all
these issues in a comprehensive manner with a s-
ingle framework. (Emmanuel, 2013) proposes a
scheme to utilize a socially-based reward function
for Reinforcement Learning and uses it to fit the
user adaptation issue for dialogue management.
(Daniele, 2013) describes an architecture for a di-
alogue management system to be employed in se-
rious games for natural language interaction with
non-player characters. (Young et al., 2013) pro-
vides an overview of the current state of the art
in the development of POMDP-based spoken di-
alog systems. (Hao, 2014) presents a dialogue
manager based on a log-linear probabilistic model
and uses context-free grammars to impart hierar-
chical structure to variables and features. (Kallir-
roi, 2014) uses single-agent Reinforcement Learn-
ing and multi-agent Reinforcement Learning for

364



learning dialogue policies in a resource allocation
negotiation scenario. To sum up, most of these
previous studies establish a specific-domain DM
model, only using the dialogue corpus in this do-
main, which might suffer from scarcity of dialogue
corpus in some vertical domains.

In this paper, we mainly consider the domain-
s about slot-filling tasks such as hotel reservation,
flight ticket booking, and shopping guidance. We
utilize dialogue act (DA) as semantic representa-
tion of utterance, and divide it into DA type and
slot parameter, where the former one is domain-
independent and the latter one is domain-specific.
Based on the dialogue corpus in multiple domain-
s, we train the current machine DA type predic-
tion model and the next user DA type prediction
model via Recurrent Neutral Networks (RNN).
With these two prediction models, the current ma-
chine DA type decision problem is modeled as a
multi-order POMDP, and transformed to be a one-
order MDP with continuous states, which could
be solved by Natural Actor Critic (NAC) algorith-
m. This general DA type decision model could be
applied to multiple domains. After calculating the
machine DA type, we design a slot parameter s-
election scheme to generate a complete machine
DA according to the features of vertical domain,
which yields the Multi-domain Corpus based Dia-
logue Management (MCDM) scheme. The advan-
tages of this scheme are as follows.

• The MCDM scheme separates DA into DA
type and slot parameter, where DA type
is domain-independent. It utilizes multi-
domain corpus to train a general DA type de-
cision model that is applicable to every do-
main. Namely, it extracts general dialogue
knowledge from all the domains and put it
into vertical domain DM model. Even for
some vertical domain with insufficient dia-
logue corpus, it could work well.

• The MCDM scheme encodes the dialogue
historical information into history vector vi-
a RNN, and utilizes this history vector to es-
timate the distribution over possible curren-
t machine DA type and the distribution over
possible next user DA. Theoretically, the his-
tory vector contains the whole dialogue his-
tory, even the information of utterances in the
first turn.

• The MCDM scheme models the machine DA

type decision problem as a POMDP, which
makes a decision in the limitation of unreli-
able ASR and NLP, and achieves a tradeof-
f between dialogue popularity (frequency of
dialogue pattern) and slot-filling efficiency.

• The MCDM scheme designs a slot parameter
selection method for generated machine DA
type, according to the features of vertical do-
main.

The rest of this paper is organized as follows.
In Section 2, system model is introduced. Sec-
tion 3 establishes the current machine DA type
prediction model and the next user DA type pre-
diction model via RNN, and Section 4 models the
DA type decision problem as a POMDP. Section
5 describes slot selection scheme for the given DA
type and slot filling process. Extensive experimen-
tal results are provided in Section 6 to illustrate the
performance comparison, and Section 7 concludes
this study.

2 System Model

Generally, the SDS operates as follows. Receiving
user voice input, Natural Language Understanding
(NLU) module transforms it into semantic repre-
sentation such as DA. There are two steps in NLU:
the first is Automatic Speech Recognition (ASR)
that turns voice into text (Willie, 2004) (Vinyal-
s, 2012); the second is Semantic Decoder (SD)
that extracts DA from text (Hea, 2006) (Mairesse,
2009). NLU is hardly able to analyze the exact
DA of user input due to inevitable ambiguity, un-
certainty and errors in ASR and SD. Thus, the dis-
tribution of possible DAs is utilized to represent
the result of NLU. According to this distribution
and dialogue history, Dialogue Management (DM)
module calculates the optimal output DA. Final-
ly, Natural Language Generation (NLG) module
transforms output DA into voice, including sen-
tence generation that generates sentence based on
DA (Mairesse, 2007) and Text To Speech (TTS)
that turns sentence text into output voice (Clark,
2004) (Zen, 2007).

In this paper, we focus on DM in SDS for the
slot-filling task. Firstly, we collect the dialogue
corpus in multiple domains such as hotel reserva-
tion, flight ticket booking and shopping guidance.
We label the dialogue corpus with DA set intro-
duced in (Stolcke, 2000). This set includes 42 DA
labels, which is wildly used and cited over 600

365



Do you have a room tonight?

YES-NO-QUESTION(room_quantity=1, checkin_time=tonight)

U1:

U1_DA:

Yes, we have. What kind of room type you prefer?

YES-ANSWERS() + WH-QUESTION(room_type)

M1:

M1_DA:

A double room.

STATEMENT(room_type=double room, room_quantity=1)

U2:

U2_DA:

OK. What is your checkout time?

ACKNOWLEDGE() + WH-QUESTION(checkout_time)

M2:

M2_DA:

U3:

M3_DA:

Hotel Reservation

Figure 1: an example of labeled dialogue

times in Google Scholar. Fig.1 is an example of
labeled dialogue. Additionally, DA is divided into
two parts: DA type and slot parameters. For exam-
ple, for the DA “WH-QUESTION (room type)”,
the DA type is “WH-QUESTION”, and the slot
parameter is “room type”. It is observed that DA
type is domain-independent while slot parameter
is domain-specific.

Based on these labeled dialogues, we design the
multi-domain DM scheme, which could be divid-
ed into two steps:

• DA type decision: The dialogue historical in-
formation is encoded into history vector via
RNN. Based on this vector, we estimate the
possible current machine DA types and pos-
sible next user DA types, which will be in-
troduced in section 3. With these DA type
estimations, the DA type decision process is
modeled as a POMDP, which will be intro-
duced in section 4. This DA type decision
model is applicable to every vertical domain.

• Slot parameter selection: After determining
the DA type, the slot parameter selection
scheme is designed according to the features
of vertical domain, in order to generate a
complete machine output DA.

3 RNN based Prediction Model

In this section, we introduce current machine DA
type prediction model and next user DA type pre-
diction model. (Nagata, 1994) utilizes N -gram
Language Model to predict DA type. As quanti-
ty of DA type combination in historical epoches
grows exponentially with the increment of N , the
parameter N could not be too big, namely Bi-
gram and Tri-gram are usually used. Thus, N-
gram based DA type prediction model could not

 !t O
V
  !t H

 !1t "H

W
 

 !2t "H

 !2
u
t "I

 !2
m
t "I

2

u

t
!

"

W
 

2

m

t
!

"

 !
u
tI

 !
m
tI

u
U

 

m
U

 

 !1
u
t "I

 !1
m
t "I

1

u

t
!

"

1

m

t
!

"

m

t
!

u

t
!

3

m

t
!

"

u
U

 

m
U

 

u
U

 

m
U

 

Figure 2: RNN for the current machine DA pre-
diction

consider the dialogue historical information effi-
ciently. In order to solve this problem, we utilize
RNN to predict the DA type. The details of pre-
diction model are as follows.

Firstly, the sentences in dialogue corpus are di-
vided into two sets: sentence set spoken by ma-
chine (or service provider such as customer ser-
vice representative in hotel reservation) and sen-
tence set spoken by user (customer). We count the
DA type combination in these two sets respective-
ly, where the machine DA type combination set is
denoted as Cm and the user DA type combination
set is denoted as Cu.

Secondly, we predict the probability distribu-
tion over current machine DA types. We denote
the combination of DA type corresponding to us-
er and machine sentences in t-th turn as ξmt and
ξut , where ξ

m
t ∈ Cm and ξut ∈ Cu. The probabil-

ity distribution over current machine DA types is
determined by the current user DA type, the last
machine DA type and the previous dialogue his-
torical information, which is denoted as

Pr
{
ξmt

∣∣ξut , ξmt−1, ξut−1, · · · , ξm1 , ξu1 } (1)
We utilize RNN to estimate the conditional prob-
ability in equation (1). The architecture of this
RNN is illustrated in Fig. 2. The inputs of RNN
in the t-th turn are ξut and ξ

m
t−1. The input layer-

s in this turn are one-hot representations (Turian,
2010) of ξut and ξ

m
t−1, denoted as Iu (t) and Im (t).

(The size of Iu (t) or Im (t) is equivalent to |Cu|

366



or |Cm|. There is only one 1 in Iu (t) or Im (t)
corresponding to the ξut or ξ

m
t−1 position, and oth-

er elements are zeros.) We denote hidden layer as
Hα (t) and output layer as Oα (t). Thus, Oα (t)
is the probability distribution of current machine
DA type combination, which could be calculated
as (Mikolov, 2010)

Hα (t) = f (UαuIu (t) + U
α
mIm (t)

+WαHα (t− 1)) (2)

and
Oα (t) = g (VαHα (t)) (3)

where f (·) is a sigmoid function, namely f (x) =
1/(1 + e−x) and g (·) is a soft-max function,
namely g (xi) = exi

/∑Ng
i=1 e

xi . The parameters
of this RNN could be trained by the Back Propa-
gation Through Time (BPTT) algorithm (Mikolov,
2012).

Thirdly, we predict the probability distribution
over next user DA types based on the current ma-
chine DA type, the current user DA type and the
previous dialogue historical information, which is
denoted as

Pr
{
ξut+1

∣∣ξmt , ξut , ξmt−1, ξut−1, · · · , ξm1 , ξu1 } (4)
We also utilize the RNN with the same architec-
ture mentioned above to predict this conditional
probability, but inputs and outputs are differen-
t. The inputs in the t-th turn are ξmt and ξ

u
t , and

the output is the probability distribution of ξut+1,
which is illustrated in Fig 3. The parameters of
this RNN could be also trained by BPTT.

Besides, in different vertical domains, the pat-
tern of DA type evolution might be different. For
example, there might be a lot of question-answer
exchanges in hotel reservation domain, because
machine needs to collect a lot of information about
reservation such as room type, check-in time and
client name, and user also needs to inquire a lot
of information about room and hotel such as room
price and hotel address. While in other domain-
s such as restaurant catering, the slots requested
by machine are more than slots requested by us-
er, which might lead to less question-answer ex-
changes. Thus, in order to solve this problem,
when training some specific domain (target do-
main), we copy the dialogue corpus in the target
domain repeatedly and control the size of target-
domain corpus to be Kd times than the size of cor-
pus in other domains, which increases the size of

 !t O
V

  !t H

 !1t "H

W
 

 !2t "H

 !2
m
t "I

 !2
u
t "I

2

m

t
!

"

W
 

1

u

t
!

"

 !
m
tI

 !
u
tI

m
U

 

u
U

 

 !1
m
t "I

 !1
u
t "I

1

m

t
!

"

u

t
!

1

u

t
!

#

m

t
!

2

u

t
!

"

m
U

 

u
U

 

m
U

 

u
U

 

Figure 3: RNN for the next user DA prediction

the corpus in the target domain and makes DA type
prediction model fit for the features of the target
domain.

4 Model DM as POMDP

In this section, we use POMDP (Littman, 2009)
to model DM problem, illustrated in Fig.4. State
is defined as the combination of user DA types in
each turn, namely st = ξut ∈ Cu. Action is defined
as the combination of machine DA types in each
turn, namely at = ξmt ∈ Cm. As the user DAs
in (t + 1)-th turn are not only determined by the
user and machine DAs in t-th turn, but also related
to the previous DAs, we define τ as a window size
for this kind of relevance. Thus, the state transition
probability could be represented as

Pr {st+1 |at, st, · · · , a1, s1 }
= Pr {st+1 |at, st, · · · , at−τ+1, st−τ+1 }
= Pr

{
ξut+1

∣∣ξmt , ξut , · · · , ξmt−τ+1, ξut−τ+1 } (5)
This conditional probability could be estimated by
RNN in section 3, which is denoted as πut+1. Ob-
servation is defined as user input voice in each
turn, denoted as ot ∈ O . As st could not be ob-
served directly, ot is utilized to estimate st, name-
ly Pr {st |ot }, which could be obtained from ASR
and SD and denoted as pot . The reward function
includes two parts: slot-filling efficiency and dia-
logue popularity, which is denoted as

rt (st, at, st+1) = λ1F (st, at, st+1)
+ λ2G (st, at, st+1)

(6)

367



1t
s
 t

s
1t

s
!

1t
r
 

1t
o
 1t

a
 

t
r

t
o

t
a

1t
o
!

1t
r
!

1t
a
!

Step t-1 Step t Step t+1

Figure 4: POMDP

where F (·) is a function mapping from the cur-
rent user DA type, the current machine DA type
and the next user DA type to the normalized quan-
tity of filled slots that will be introduced in section
5, G (·) is the normalized quantity of sequence
(st, at, st+1) that could be counted from dialogue
corpus and represent dialogue popularity, λ1 and
λ2 are the weights of slot filling reward and pop-
ularity reward, and λ1 + λ2 = 1. The policy is
defined as a mapping from observation to action,
which is denoted as ζ ∈ Z : O → A . Thus,
the DM problem is to find out the optimal policy
to maximize the total expected discount reward,
which is shown as

max
ζ∈Z

Eζ

[
T∑

t=1
βrt (st, at, st+1)

]
s.t.
Pr {st+1 |at, st, · · · , at−τ+1, st−τ+1 } = πut+1
Pr {st |ot } = pot

(7)
where β is a time discount factor. This problem
is a τ order POMDP, which is difficult to slove
directly. In the following, it will be transformed to
be a MDP with continuous states.

We define belief state as bt ∈ B to represen-
t the distribution over possible states in the t-th
turn, not only based on the current voice input, but
also based on the dialogue history. The belief state
updating process is the process of calculating bt+1
according to {bt, bt−1, · · · , bt−τ+1}, which could

be represented as

bt+1 = κ · Pr {ot+1 |st+1 }
∑
st

· · · ∑
st−τ+1

Pr {st+1

|st, at, · · · , st−τ+1, at−τ+1 }
t∏

i=t−τ+1
bi

(8)
where κ is normalization constant. The deduc-
tion of this updating processing will be found in
Appendix A. As user input voice is a continuous
signal and different people have different habits
of pronunciation and semantic representation, it is
hard to estimate Pr {ot+1 |st+1 } directly. Thus,
according to Bayes Rules, Pr {ot+1 |st+1 } could
be shown as

Pr {ot+1 |st+1 } = Pr {st+1 |ot+1 }Pr {ot+1}Pr {st+1}
(9)

where Pr {st+1 |ot+1 } could be estimated by AS-
R and SD, Pr {st+1} is prior distribution that
could be counted in corpus, denoted as pst+1, and
Pr {ot+1} is the same for different st+1 that could
be deleted. For belief state, the reward function
could be redefined as

rt (bt, at, · · · , bt−τ+1, at−τ+1) =∑
st

· · · ∑
st−τ+1

(rt (st, at, st+1) · Pr {st+1

|st, at, · · · , st−τ+1, at−τ+1 }
t∏

i=t−τ+1
bi+

Pr {at |st, at−1, · · · , st−τ+1, at−τ }
t∏

i=t−τ+1
bi

)
(10)

where the first part is the belief form of state re-
ward and the second part is the expectation of the
current machine DA type probability estimated by
RNN in the section 3. We redefine the policy as a
mapping from belief state to action, which is de-
noted as ζ ′ ∈ Z ′ : B → A . Thus, the problem
(7) could be reformulated as

max
ζ′∈Z ′

Eζ′

[
T∑

t=1
βrt (bt, at, · · · , bt−τ+1, at−τ+1)

]
s.t.

bt+1 = κ · p
o
t

pst+1

∑
st

· · · ∑
st−τ+1

Pr {st+1 |st, at,

· · · , st−τ+1, at−τ+1}
t∏

i=t−τ+1
bi,

b0 = po0.
(11)

This problem is a τ order MDP with continuous
states, which will be transformed to be one order
MDP.

368



We redefine new state as the sequence of be-
lief state and action from the (t− τ + 1)-th turn
to the t-th turn, which is denoted as s̄t =
{bt, at−1, bt−1, · · · , at−τ+1, bt−τ+1} and s̄t ∈ S̄ .
Thus, the state transition probability could be
shown as

Pr {s̄t+1 |s̄t , at}
= Pr {bt+1, at, bt, · · · , at−τ+2, bt−τ+2
|bt, at−1, bt−1, · · · , at−τ+1, bt−τ+1 , at}

= Pr {bt+1, |bt, at, · · · , bt−τ+1, at−τ+1 }
(12)

which could be obtained from equation (8) and
denoted as π̄s̄t+1s̄t,at . The reward function could be
rewritten as

r̄t (s̄t, at) = rt (bt, at, · · · , bt−τ+1, at−τ+1)
(13)

We redefine the policy as a mapping from new s-
tate to action, which is denoted as ζ̄ ∈ Z̄ : S̄ →
A . Thus, the problem (11) could be reformulated
as

max
ζ̄∈Z̄

Eζ

[
T∑

t=1
βr̄t (s̄t, at)

]
s.t.

Pr {s̄t+1 |s̄t , at} = π̄s̄t+1s̄t,at

(14)

This problem is a one order MDP with continu-
ous states, which could be solved by Natural Actor
Critic algorithm (Peters, 2008) (Bhatnagar, 2009).

5 Slot Selection and Slot-filling

After determining the DA type of machine, the
next step is selecting slot parameter for it to yield
a complete output DA. Firstly, the parameters for
DAs could be classified as follows.

• ∅: some DAs have no parameters, such as
YES-ANSWERS ()

• slot: parameter of some DA is a slot, such as
WH-QUESTION (room type)

• slot = value: parameter of some DA is
a slot value pair , such as STATEMENT
(double room price= $100)

Additionally, The slots in human-machine dia-
logue could be divided into two categories, illus-
trated in Fig.5:

• Slots requested from machine to users, such
as room type, checkin time, which is denote
as Qm. The values of these slots are un-
known for machine before the dialogue. In

1

m

Q

2

m

Q

slot value

room_type double_room

room_quantity

checkin_time tomorrow_evening

checkout_time

client_name

client_phone

Lucy

... ...

m

Q

u

Q

slot value

single_room_price $100

double_room_price

hotel_address No.95, East St. 

... ...

$150

Figure 5: slot classification

dialogue processing, we denote unfilled slots
as Q1m and filled slots as Q

2
m.

• Slots requested from users to machine, such
as double room price, hotel address, which
is denote as Qu. The values of theses slot
are known for machine before the dialogue.

The purpose of human-machine dialogue is to ex-
change these slot information. For example, in ho-
tel reservation, machine is to request values of s-
lots in Qm, while user is to request values of slots
in Qu in order to determine the values of slots in
Qm that user will inform to machine. Besides, it
is obvious that Q1m is a set of slots, Q

2
m and Qu

are sets of slot value pairs.
Thus, there are three situations in the slot selec-

tion for a machine DA type

• If the parameter classification corresponding
to the machine DA type is ∅, it is no need to
select slot.

• If the parameter classification corresponding
to the machine DA type is a slot, it is selected
from Q1m.

• If the parameter classification correspond-
ing to the machine DA type is a slot value

369



pair, it is selected from Q2m and Qu. For
example, for “STATEMENT”, it is selected
from Qu; for “DECLARATIVE YES-NO-
QUESTION”, it is selected from Q2m.

In slot selection process, the orders of slots in Q1m,
Q2m and Qu ought to be learned based on the di-
alogue corpus in vertical domain such as slot se-
quence in the task, slot dependency, slots that user
request, domain expertise knowledge and so forth.

After obtaining a complete the machine, the last
task is filling the slots according to the current DA
and historical DA sequence. In this paper, we use
handcrafted rules to fill the slots. For example,
according to the DA sequence

user: STATEMENT (room type = double room)
machine: DECLARATIVE YES-NO
-QUESTION (room type = double room)
user: YES ANSWER ()
machine: ACKNOWLEDGE ()

The slot “room type” is filled by the value “dou-
ble room”. This knowledge could be represented
by the first order logic (Smullyan, 1995) as follow.

STATEMENT (X = A) ∧ DECLARATIVE
YES-NO-QUESTION (X = A) ∧ YES AN-
SWER () ∧ ACKNOWLEDGE () ⇒ fill (X, A)

6 Experimental Results

In this section, we compare the performance of
the proposed DM schemes and the existing DM
scheme. The DM scheme proposed in this paper
is named as the RNN-MCDM scheme. In the N-
Gram-MCDM scheme, the DA type is estimated
by N-gram model, and other parts are the same as
the RNN-MCDM scheme. In the existing scheme,
the DM model in each domain is designed accord-
ing to (Young et al., 2013), using the dialogue
corpus in its own domain. Namely, for a given
domain, the existing scheme does not utilize dia-
logue corpus in other domains.

The dialogue corpus for experiments cover-
s five vertical domains, including hotel reserva-
tion (171 dialogues), shopping guidance (71 di-
alogues), banking service (64 dialogues), restau-
rant catering (46 dialogues), and taxi service (33
dialogues). Several slots are defined for each
vertical domain. For example, in hotel reserva-
tion, the slots requested from machine to users in-
clude “room type”, “room quantity”, “client quan-
tity”, “checkin time”, “checkout time”, “break-
fast demand”(yes or no), “breakfast type”, “clien-

hotel reservation shopping guidance banking service restaurant catering taxi service
0

0.2

0.4

0.6

0.8

1

1.2

1.4

1.6

1.8

2

no
rm

al
iz

ed
 a

ve
ra

ge
 tu

rn
s 

of
 te

st
in

g 
di

al
og

ue
s

 

 

 the existing DM scheme
 the N−Gram−MCDM scheme
 the RNN−MCDM scheme

Figure 6: comparison of normalized average turn
in different domains

t name” and “client phone”, while the slots re-
quested from users to machine include “hotel ad-
dress = No.95 East St.”, “room type set = sin-
gle room, double room, and deluxe room”, “s-
ingle room price = $80”, “double room price =
$100”, “deluxe room price = $150”, “breakfast
type set = Chinese breakfast, American breakfast,
and Japanese breakfast”, “Chinese breakfast price
= $12”, “American breakfast price = $15” and
“Japanese breakfast price = $10”. Besides, we al-
so define 8 slots for shopping guidance, 9 slots for
banking service, 6 slots for restaurant catering and
4 slots for taxi service. The details of these slot-
s are not described due to the limitation of pages.
Besides, Kd is set to be 10.

The dialogues in corpus are divided into two
parts: 70% corpus for training the DM model and
30% corpus for user simulation to test the system-
s. The simulated users are built via Bayesian Net-
works according to (Pietquin, 2005). There are t-
wo performance indices for SDS evaluation: aver-
age turn and success rate. Average turn is defined
as the average dialogue turn cost for task com-
pletion. Generally, in different vertical domains,
the dialogue turns are directly proportional to the
quantities of slots. Thus, we define the normalized
average turn as the ratio of average dialogue turn
to slot number. In addition, success rate is defined
as the ratio of the dialogues that complete the task
in the threshold turns to all the dialogues. Here,
we define the threshold as double of slot number.

Fig. 6 illustrates the normalized average turn
in the RNN-MCDM scheme, the N-Gram-MCDM
scheme and the existing DM scheme. The ver-

370



hotel reservation shopping guidance banking service restaurant catering taxi service
0

0.2

0.4

0.6

0.8

1

1.2
su

cc
es

s 
ra

te

 

 

the existing DM scheme
the N−Gram−MCDM scheme
the RNN−MCDM scheme

Figure 7: comparison of success rate in different
domains

tical domains for comparison are hotel reserva-
tion, shopping guidance, banking service, restau-
rant catering and taxi service. From this picture,
we have the following conclusions. For the exist-
ing DM scheme, in the vertical domain with more
dialogue corpus it has lower normalized average
turn, while it has higher normalized average turn
in the vertical domain with less dialogue corpus.
The reasons are as follows. The existing scheme
only uses the dialogue corpus in one domain. It-
s trained DM model might not contain the abun-
dant states if the size of dialogue corpus is smal-
l. Thus, when being in a unknown state, it could
not calculate the optimal action, which might be
detrimental to the efficiency of slot filling. How-
ever, the MCDM schemes have stable and better
performance of normalized average turn, which
should be ascribed to the fact that the proposed
schemes train the general DM model based on
the dialogue corpus in all the domains, and lean-
ing general dialogue knowledge to guide the dia-
logue evolution. In addition, the N-Gram-MCDM
scheme has lower normalized average turn than
the existing scheme. Especially in the vertical do-
main with less dialogue corpus, performance im-
provement is more obvious. The reason is that the
N-Gram-MCDM scheme could learn the general
dialogue knowledge from all the domains, espe-
cially in the domain with less corpus it could use a
part of other domain knowledge to train its optimal
dialogue policy. Furthermore, the RNN-MCDM
scheme has the lowest normalized average turn in
every domain, because the RNN-MCDM scheme
use RNN to learning history vector for DA pre-

diction that takes the whole dialogue history in-
to account. Namely, the RNN-MCDM scheme
utilizes dialogue historical information more ef-
ficiently than the N-Gram-MCDM scheme, and
RNN-based prediction model is smoother than N-
Gram-based prediction model.

Fig. 7 compares the success rate among
the RNN-MCDM scheme, the N-Gram-MCDM
scheme and the existing DM scheme. From this
picture, we can find out that the RNN-MCDM
scheme has the highest success rate, and the suc-
cess rate in the existing scheme is lower than the
N-Gram-MCDM scheme, the gap become huge
in the vertical domain with less dialogue corpus,
which should be ascribed to the same reasons in
Fig. 6.

7 Conclusion

In this paper, we proposed the DM scheme based
on Multi-domain Corpus. In this scheme, DA is
divided into DA type and slot parameter, where the
former one is domain-independent and the latter
one is domain-specific. We used RNN to estimate
the probability distributions of next user DA type
and current machine DA type with dialogue cor-
pus in all the domains, and established a POMDP-
based current machine DA type decision model
that is applicable to all the vertical domains. Ad-
ditionally, we designed a slot parameter selection
scheme to generate a complete machine DA ac-
cording to the features of vertical domain, which
yields the MCDM scheme. Finally, extensive ex-
perimental results indicated that the proposed DM
scheme is superior to the existing scheme.

Acknowledgments

This work is supported by the National Pro-
gram on Key Basic Research Project (973 Pro-
gram), basic theories and methods of Chinese Lan-
guage Processing and Deep Computing in Inter-
net environment, multi-lingual Automatic Speech
Recognition for complex environments. (No.
2013CB329302)

Appendix A

In this section, we deduce the belief state updat-
ing process in equation (8). The belief state in the
(t + 1)-th turn could be represented as

bt+1 = Pr {st+1 |ot+1, bt, at, · · · , bt−τ+1, at−τ+1 }
(15)

371



If we denote bt, at, · · · , bt−τ+1, at−τ+1 as φ, bt+1
could be written as

bt+1 =
Pr {st+1, ot+1, φ}

Pr {ot+1, φ}

=
Pr {ot+1 |st+1, φ}Pr {st+1 |φ}Pr {φ}

Pr {ot+1 |φ}Pr {φ}

=
Pr {ot+1 |st+1, φ}Pr {st+1 |φ}

Pr {ot+1 |φ} (16)

According to (Thomson, 2009),
Pr {ot+1 |st+1, φ} = Pr {ot+1 |st+1 }. In
addition, Pr {st+1 |φ} could be shown as

Pr {st+1 |φ} =
∑
st

· · · ∑
st−τ+1

Pr {st+1 |st, at,
· · · , st−τ+1, at−τ+1}Pr {st, · · · , st−τ+1 |φ}

(17)
where

Pr {st, · · · , st−τ+1 |φ} =
t∏

i=t−τ+1
bi (18)

Besides, Pr {ot+1 |φ} could be shown as

Pr {ot+1 |φ} =
∑
st+1

Pr {ot+1 |st+1 }Pr {st+1 |φ}

(19)
Accordingly,

bt+1 =
Pr {ot+1 |st+1 }Pr {st+1 |φ}∑

st+1

Pr {ot+1 |st+1 }Pr {st+1 |φ}

= κ · Pr {ot+1 |st+1 }
∑
st

· · ·
∑

st−τ+1

Pr {st+1

|st, at, · · · , st−τ+1, at−τ+1 }
t∏

i=t−τ+1
bi (20)

where

κ =
1∑

st+1

Pr {ot+1 |st+1 }Pr {st+1 |φ} (21)

is a normalization factor.

References
S. Bhatnagar, R. S. Sutton, M. Ghavamzadeh, M. Lee.

2009 Natural actorCcritic algorithms. Automatica,
45(11), 2471-2482, 2009.

R. A. Clark, K. Richmond, S. King. 2004 Festival
2-Build Your Own General Purpose Unit Selection
Speech Synthesiser. In Fifth ISCA Workshop on
Speech Synthesis, 2004.

L. Daubigney, M. Geist, S. Chandramohan, O.
Pietquin. 2012. A Comprehensive Reinforcement
Learning Framework for Dialogue Management Op-
timization. IEEE Journal of Selected Topics in Sig-
nal Processing, Vol. 6 No.8, pp: 891-902, 2012.

Annemiek van Drunen. 2012. Dialogue management
and automation in interaction with unmanned sys-
tems. Proceedings of the 2nd International Confer-
ence on Application and Theory of Automation in
Command and Control System, May 2012.

Emmanuel Ferreira, Fabrice Lefvre. 2013. Social sig-
nal and user adaptation in reinforcement learning-
based dialogue management. MLIS ’13, August
2013.

Kallirroi Georgila, Claire Nelson, David Traum. 2014.
Single-Agent vs. Multi-Agent Techniques for Con-
current Reinforcement Learning of Negotiation Dia-
logue Policies. The 52nd Annual Meeting of the As-
sociation for Computational Linguistics, Jun, 2014.

Yulan Hea, Steve Young. 2006. Spoken language
understanding using the Hidden Vector State Mod-
el. Speech Communication, Volume 48, Issues 3C4,
Pages 262C275, 2006

M. L. Littman. 2009 A tutorial on partially observable
Markov decision processes. Journal of Mathemati-
cal Psychology, 53(3), 119-125, 2009.

F. Mairesse, M. Gasic, F. Jurcicek, S. Keizer, B. Thom-
son, K. Yu, S. Young. 2009. Spoken language un-
derstanding from unaligned data using discrimina-
tive classificati.on models. IEEE International Con-
ference on Acoustics, Speech and Signal Processing,
2009.

F. Mairesse, M. Walker. 2007 PERSONAGE: Person-
ality generation for dialogue. IEEE International
Conference on Acoustics, Speech and Signal Pro-
cessing, 2007.

Michael F. Mctear. 2002. Spoken dialogue technolo-
gy: enabling the conversational user interface. ACM
computing Survey, volume 34, No. 1, pages: 90-169,
2002.

T. Mikolov. 2012 Statistical language models based on
neural networks. Presentation at Google, Mountain
View, 2012.

T. Mikolov, M. Karafit, L. Burget, J. Cernocky, S. Khu-
danpur. 2010 Recurrent neural network based lan-
guage model. 11th Annual Conference of the Inter-
national Speech Communication Association, Sep,
2010.

372



Daniele Mori, Riccardo Berta, Alessandro De Glori-
a, Valentina Fiore, Lauto Magnani. 2013. An easy
to author dialogue management system for serious
games. Journal on Computing and Cultural Her-
itage, Vol.6 no.2 May 2013.

M. Nagata, T. Morimoto. 1994 First steps towards s-
tatistical modeling of dialogue to predict the speech
act type of the next utterance. Speech Communica-
tion, 15(3), 193-203.

Reithinger Norbert, Elisabeth Maier. 1995 Utilizing s-
tatistical dialogue act processing in verbmobil. Pro-
ceedings of ACL, Jun, 1995.

J. Peters, S. Schaal. 2008 Natural actor-critic. Neuro-
computinge, 71(7), 1180-1190, 2008.

O. Pietquin and T. Dutoit. 2005 A probabilistic
framework for dialog simulation and optimal strat-
egy learning. IEEE Transactions on Speech and
Audio Processing, Special Issue on Data Mining of
Speech, Audio and Dialog, 2005.

Olivier Pietquin, Matthieu Geist, Senthilkumar Chan-
dramohan, Herv Frezza-Buet. 2011. Sample-
efficient batch reinforcement learning for dialogue
management optimization. ACM Transactions on
Speech and Language Processing, Vol. 7 No. 3, May
2011.

R. M. Smullyan. 1995 First-order logic. Courier Cor-
poration, 1995

A. Stolcke, K. Ries, N. Coccaro, E. Shriberg, R. Bates,
D. Jurafsky. 2000 Dialogue act modeling for au-
tomatic tagging and recognition of conversational
speech. Computational linguistics, 26(3), 339-373.

Hao Tang, S. Watanabe, T.K. Marks, J.R. Hershey.
2014. Log-linear dialog manager. 2014 IEEE Inter-
national Conference on Acoustics, Speech and Sig-
nal Processing (ICASSP), pp: 4092 C 4096, May
2014.

B. Thomson. 2009 Statistical methods for spoken dia-
logue management. Ph.D. dissertation, Cambridge,
2009.

B. Thomson and S. Young. 2010. Bayesian update
of dialogue state: A POMDP framework for spoken
dialogue systems. Comput. Speech Lang, vol. 24,
no. 4, pp. 562C588, 2010.

J. Turian, L. Ratinov, Y. Bengio. 2010 Word repre-
sentations: a simple and general method for semi-
supervised learning. In Proceedings of the 48th an-
nual meeting of the association for computational
linguistics, Jul, 2010.

O. Vinyals, S.V. Ravuri, D. Povey. 2012. Revis-
iting Recurrent Neural Networks for robust ASR.
2012 IEEE International Conference on Acoustics,
Speech and Signal Processing (ICASSP), 2012.

Willie Walker, Paul Lamere, Philip Kwok. 2004.
Sphinx-4: a flexible open source framework for
speech recognition. Technical Report, Sun Mi-
crosystems

S. Young, M. Gasic, B. Thomson, J. D. Williams.
(2013). 2013. Pomdp-based statistical spoken di-
alog systems: A review. Proceedings of the IEEE,
101(5), pages: 1160-1179, 2013.

Xiaobu Yuan, Guoying Liu. 2012. A task ontolo-
gy model for domain independent dialogue manage-
ment. IEEE International Conference on Virtual En-
vironments Human-Computer Interfaces and Mea-
surement Systems, 2012.

H. Zen, T. Nose, J. Yamagishi, S. Sako, T. Masuko,
A. W. Black, K. Tokuda. 2007 The HMM-based
speech synthesis system version 2.0. In Proc. 6th
ISCA Workshop on Speech Synthesis, Aug, 2007.

373


