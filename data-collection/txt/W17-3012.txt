



















































Understanding Abuse: A Typology of Abusive Language Detection Subtasks


Proceedings of the First Workshop on Abusive Language Online, pages 78–84,
Vancouver, Canada, July 30 - August 4, 2017. c©2017 Association for Computational Linguistics

Understanding Abuse:
A Typology of Abusive Language Detection Subtasks

Zeerak Waseem
Department of Computer Science

University of Sheffield
United Kingdom

z.w.butt@sheffield.ac.uk

Thomas Davidson
Department of Sociology

Cornell University
Ithica, NY

trd54@cornell.edu

Dana Warmsley
Department for Applied Mathematics

Cornell University
Ithica, NY

dw457@cornell.edu

Ingmar Weber
Qatar Computing Research Institute

HBKU
Doha, Qatar

iweber@hbku.edu.qa

Abstract

As the body of research on abusive lan-
guage detection and analysis grows, there
is a need for critical consideration of the
relationships between different subtasks
that have been grouped under this label.
Based on work on hate speech, cyberbully-
ing, and online abuse we propose a typol-
ogy that captures central similarities and
differences between subtasks and we dis-
cuss its implications for data annotation
and feature construction. We emphasize
the practical actions that can be taken by
researchers to best approach their abusive
language detection subtask of interest.

1 Introduction

There has been a surge in interest in the detec-
tion of abusive language, hate speech, cyberbully-
ing, and trolling in the past several years (Schmidt
and Wiegand, 2017). Social media sites have also
come under increasing pressure to tackle these is-
sues. Similarities between these subtasks have
led scholars to group them together under the
umbrella terms of “abusive language”, “harmful
speech”, and “hate speech” (Nobata et al., 2016;
Faris et al., 2016; Schmidt and Wiegand, 2017)
but little work has been done to examine the rela-
tionship between them. As each of these subtasks
seeks to address a specific yet partially overlap-
ping phenomenon, we believe that there is much
to gain by studying how they are related.

The overlap between subtasks is illustrated by
the variety of labels used in prior work. For
example, in annotating for cyberbullying events,
Van Hee et al. (2015b) identifies discriminative
remarks (racist, sexist) as a subset of “insults”,
whereas Nobata et al. (2016) classifies similar re-
marks as “hate speech” or “derogatory language”.
Waseem and Hovy (2016) only consider “hate
speech” without regard to any potential overlap
with bullying or otherwise offensive language,
while Davidson et al. (2017) distinguish hate
speech from generally offensive language. Wul-
czyn et al. (2017) annotates for personal attacks,
which likely encompasses identifying cyberbully-
ing, hate speech, and offensive language. The
lack of consensus has resulted in contradictory an-
notation guidelines - some messages considered
as hate speech by Waseem and Hovy (2016) are
only considered derogatory and offensive by No-
bata et al. (2016) and Davidson et al. (2017).

To help to bring together these literatures and
to avoid these contradictions, we propose a typol-
ogy that synthesizes these different subtasks. We
argue that the differences between subtasks within
abusive language can be reduced to two primary
factors:

1. Is the language directed towards a specific
individual or entity or is it directed towards
a generalized group?

2. Is the abusive content explicit or implicit?

Each of the different subtasks related to abu-

78



sive language occupies one or more segments of
this typology. Our aim is to clarify the similarities
and differences between subtasks in abusive lan-
guage detection to help researchers select appro-
priate strategies for data annotation and modeling.

2 A typology of abusive language

Much of the work on abusive language subtasks
can be synthesized in a two-fold typology that con-
siders whether (i) the abuse is directed at a specific
target, and (ii) the degree to which it is explicit.

Starting with the targets, abuse can either be di-
rected towards a specific individual or entity, or
it can be used towards a generalized Other, for
example people with a certain ethnicity or sex-
ual orientation. This is an important sociological
distinction as the latter references a whole cate-
gory of people rather than a specific individual,
group, or organization (see Brubaker 2004, Wim-
mer 2013) and, as we discuss below, entails a lin-
guistic distinction that can be productively used
by researchers. To better illustrate this, the first
row of Table 1 shows examples from the literature
of directed abuse, where someone is either men-
tioned by name, tagged by a username, or refer-
enced by a pronoun.1 Cyberbullying and trolling
are instances of directed abuse, aimed at individ-
uals and online communities respectively. The
second row shows cases with abusive expressions
towards generalized groups such as racial cate-
gories and sexual orientations. Previous work has
identified instances of hate speech that are both
directed and generalized (Burnap and Williams,
2015; Waseem and Hovy, 2016; Davidson et al.,
2017), although Nobata et al. (2016) come clos-
est to making a distinction between directed and
generalized hate.

The other dimension is the extent to which
abusive language is explicit or implicit. This is
roughly analogous to the distinction in linguis-
tics and semiotics between denotation, the lit-
eral meaning of a term or symbol, and connota-
tion, its sociocultural associations, famously ar-
ticulated by Barthes (1957). Explicit abusive lan-
guage is that which is unambiguous in its potential
to be abusive, for example language that contains
racial or homophobic slurs. Previous research
has indicated a great deal of variation within such
language (Warner and Hirschberg, 2012; David-

1All punctuation is as reported in original papers. We
have added all the * symbols.

son et al., 2017), with abusive terms being used
in a colloquial manner or by people who are
victims of abuse. Implicit abusive language is
that which does not immediately imply or denote
abuse. Here, the true nature is often obscured by
the use of ambiguous terms, sarcasm, lack of pro-
fanity or hateful terms, and other means, generally
making it more difficult to detect by both anno-
tators and machine learning approaches (Dinakar
et al., 2011; Dadvar et al., 2013; Justo et al., 2014).
Social scientists and activists have recently been
paying more attention to implicit, and even uncon-
scious, instances of abuse that have been termed
“micro-aggressions” (Sue et al., 2007). As the ex-
amples show, such language may nonetheless have
extremely abusive connotations. The first column
of Table 1 shows instances of explicit abuse, where
it should be apparent to the reader that the content
is abusive. The messages in the second column are
implicit and it is harder to determine whether they
are abusive without knowing the context. For ex-
ample, the word “them” in the first two examples
in the generalized and implicit cell refers to an eth-
nic group, and the words “skypes” and “Google”
are used as euphemisms for slurs about Jews and
African-Americans respectively. Abuse using sar-
casm can be even more elusive for detection sys-
tems, for instance the seemingly harmless com-
ment praising someone’s intelligence was a sar-
castic response to a beauty pageant contestants un-
satisfactory answer to a question (Dinakar et al.,
2011).

3 Implications for future research

In the following section we outline the implica-
tions of this typology, highlighting where the ex-
isting literatures indicate how we can understand,
measure, and model each subtype of abuse.

3.1 Implications for annotation

In the task of annotating documents that contain
bullying, it appears that there is a common un-
derstanding of what cyberbullying entails: an in-
tentionally harmful electronic attack by an indi-
vidual or group against a victim, usually repeti-
tive in nature (Dadvar et al., 2013). This consen-
sus allows for a relatively consistent set of annota-
tion guidelines across studies, most of which sim-
ply ask annotators to determine if a post contains
bullying or harassment (Dadvar et al., 2014; Kon-
tostathis et al., 2013; Bretschneider et al., 2014).

79



Explicit Implicit
D

ir
ec

te
d “Go kill yourself”, “You’re a sad little f*ck” (Van Hee et al., 2015a),

“@User shut yo beaner ass up sp*c and hop your f*ggot ass back across

the border little n*gga” (Davidson et al., 2017),
“Youre one of the ugliest b*tches Ive ever fucking seen” (Kontostathis

et al., 2013).

“Hey Brendan, you look gorgeous today. What beauty salon did you

visit?” (Dinakar et al., 2012),

“(((@User))) and what is your job? Writing cuck articles and slurping

Google balls? #Dumbgoogles” (Hine et al., 2017),

“you’re intelligence is so breathtaking!!!!!!” (Dinakar et al., 2011)

G
en

er
al

iz
ed

“I am surprised they reported on this crap who cares about another dead

n*gger?”, “300 missiles are cool! Love to see um launched into Tel Aviv!

Kill all the g*ys there!” (Nobata et al., 2016),

“So an 11 year old n*gger girl killed herself over my tweets? ˆ ˆ thats

another n*gger off the streets!!” (Kwok and Wang, 2013).

“Totally fed up with the way this country has turned into a haven for

terrorists. Send them all back home.” (Burnap and Williams, 2015),

“most of them come north and are good at just mowing lawns” (Dinakar

et al., 2011),

“Gas the skypes” (Magu et al., 2017)

Table 1: Typology of abusive language.

High inter-annotator agreement on cyberbullying
tasks (93%) (Dadvar et al., 2013) further indicates
a general consensus around the features of cyber-
bullying (Van Hee et al., 2015b). After bullying
has been identified annotators are typically asked
more detailed questions about the extremity of the
bullying, the identification of phrases that indi-
cate bullying, and the roles of users as bully/victim
(Dadvar et al., 2014; Van Hee et al., 2015b; Kon-
tostathis et al., 2013).

We expect that consensus may be due to the di-
rected nature of the phenomenon. Cyberbullying
involves a victim whom annotators can identify
and relatively easily discern whether statements
directed towards the victim should be considered
abusive. In contrast, in work on annotating harass-
ment, offensive language, and hate speech there
appears to be little consensus on definitions and
lower inter-annotator agreement (κ ≈ 0.60−0.80)
(Ross et al., 2016; Waseem, 2016a; Tulkens et al.,
2016; Bretschneider and Peters, 2017) are ob-
tained. Given that these tasks are often broadly
defined and the target is often generalized, all else
being equal, it is more difficult for annotators to
determine whether statements should be consid-
ered abusive. Future work in these subtasks should
aim to have annotators distinguish between tar-
geted and generalized abuse so that each subtype
can be modeled more effectively.

Annotation (via crowd-sourcing and other
methods) tends to be more straightforward when
explicit instances of abusive language can be iden-
tified and agreed upon (Waseem, 2016b), but is
considerably more difficult when implicit abuse is
considered (Dadvar et al., 2013; Justo et al., 2014;
Dinakar et al., 2011). The connotations of lan-
guage can be difficult to classify without domain-

specific knowledge. Furthermore, while some ar-
gue that detailed guidelines can help annotators
to make more subtle distinctions (Davidson et al.,
2017), others find that they do not improve the re-
liability of non-expert classifications (Ross et al.,
2016). In such cases, expert annotators with do-
main specific knowledge are preferred as they tend
to produce more accurate classifications (Waseem,
2016a).

Ultimately, the nature of abusive language can
be extremely subjective, and researchers must en-
deavor to take this into account when using hu-
man annotators. Davidson et al. (2017), for in-
stance, show that annotators tend to code racism
as hate speech at a higher rate than sexism. As
such, it is important that researchers consider the
social biases that may lead people to disregard cer-
tain types of abuse.

The type of abuse that researchers are seeking
to identify should guide the annotation strategy.
Where subtasks occupy multiple cells in our ty-
pology, annotators should be allowed to make nu-
anced distinctions that differentiate between dif-
ferent types of abuse. In highlighting the major
differences between different abusive language de-
tection subtasks, our typology indicates that differ-
ent annotation strategies are appropriate depend-
ing on the type of abuse.

3.2 Implications for modeling

Existing research on abusive language online has
used a diverse set of features. Moving forward,
it is important that researchers clarify which fea-
tures are most useful for which subtasks and which
subtasks present the greatest challenges. We do
not attempt to review all the features used (see
Schmidt and Wiegand 2017 for a detailed review)

80



but make suggestions for which features could be
most helpful for the different subtasks. For each
aspect of the typology, we suggest features that
have been shown to be successful predictors in
prior work. Many features occur in more than one
form of abuse. As such, we do not propose that
particular features are necessarily unique to each
phenomenon, rather that they provide different in-
sights and should be employed depending on what
the researcher is attempting to measure.

Directed abuse. Features that help to identify
the target of abuse are crucial to directed abuse de-
tection. Mentions, proper nouns, named entities,
and co-reference resolution can all be used in dif-
ferent contexts to identify targets. Bretschneider
and Peters (2017) use a multi-tiered system, first
identifying offensive statements, then their sever-
ity, and finally the target. Syntactical features have
also proven to be successful in identifying abu-
sive language. A number of studies on hate speech
use part-of-speech sequences to model the expres-
sion of hatred (Warner and Hirschberg, 2012; Gi-
tari et al., 2015; Davidson et al., 2017). Typed de-
pendencies offer a more sophisticated way to cap-
ture the relationship between terms (Burnap and
Williams, 2015). Overall, there are many tools
that researchers can use to model the relationship
between abusive language and targets, although
many of these require high-quality annotations to
use as training data.

Generalized abuse. Generalized abuse online
tends to target people belonging to a small set of
categories, primarily racial, religious, and sexual
minorities (Silva et al., 2016). Researchers should
consider identifying forms of abuse unique to each
target group addressed, as vocabularies may de-
pend on the groups targeted. For example, the
language used to abuse trans-people and that used
against Latin American people are likely to differ,
both in the nouns used to denote the target group
and the other terms associated with them. In some
cases a lexical method may therefore be an appro-
priate strategy. Further research is necessary to de-
termine if there are underlying syntactic structures
associated with generalized abusive language.

Explicit abuse Explicit abuse, whether directed
or generalized, is often indicated by specific key-
words. Hence, dictionary-based approaches may
be well suited to identify this type of abuse
(Warner and Hirschberg, 2012; Nobata et al.,
2016), although the presence of particular words

should not be the only criteria, even terms that
denote abuse may be used in a variety of differ-
ent ways (Kwok and Wang, 2013; Davidson et al.,
2017). Negative polarity and sentiment of the text
are also likely indicators of explicit abuse that can
be leveraged by researchers (Gitari et al., 2015).

Implicit abuse. Building a specific lexicon may
prove impractical, as in the case of the appropri-
ation of the term “skype” in some forums (Magu
et al., 2017). Still, even partial lexicons may be
used as seeds to inductively discover other key-
words by use of a semi-supervised method pro-
posed by King et al. (2017). Additionally, charac-
ter n-grams have been shown to be apt for abu-
sive language tasks due to their ability to cap-
ture variation of words associated with abuse (No-
bata et al., 2016; Waseem, 2016a). Word embed-
dings are also promising ways to capture terms
associated with abuse (Djuric et al., 2015; Bad-
jatiya et al., 2017), although they may still be in-
sufficient for cases like 4Chan’s connotation of
“skype” where a word has a dominant meaning
and a more subversive one. Furthermore, as some
of the above examples show, implicit abuse often
takes on complex linguistic forms like sarcasm,
metonymy, and humor. Without high quality la-
beled data to learn these representations, it may be
difficult for researchers to come up with models of
syntactic structure that can help to identify implicit
abuse. To overcome these limitations researchers
may find it prudent to incorporate features beyond
just textual analysis, including the characteristics
of the individuals involved (Dadvar et al., 2013)
and other extra-textual features.

4 Discussion

This typology has a number of implications for fu-
ture work in the area.

First, we want to encourage researchers work-
ing on these subtasks to learn from advances in
other areas. Researchers working on purportedly
distinct subtasks are often working on the same
problems in parallel. For example, the field of hate
speech detection can be strengthened by interac-
tions with work on cyberbullying, and vice versa,
since a large part of both subtasks consists of iden-
tifying targeted abuse.

Second, we aim to highlight the important dis-
tinctions within subtasks that have hitherto been
ignored. For example, in much hate speech re-
search, diverse types of abuse have been lumped

81



together under a single label, forcing models to ac-
count for a large amount of within-class variation.
We suggest that fine-grained distinctions along the
axes allows for more focused systems that may
be more effective at identifying particular types of
abuse.

Third, we call for closer consideration of how
annotation guidelines are related to the phe-
nomenon of interest. The type of annotation and
even the choice of annotators should be motivated
by the nature of the abuse. Further, we welcome
discussion of annotation guidelines and the an-
notation process in published work. Many exist-
ing studies only tangentially mention these, some-
times never explaining how the data were anno-
tated.

Fourth, we encourage researchers to consider
which features are most appropriate for each sub-
task. Prior work has found a diverse array of fea-
tures to be useful in understanding and identify-
ing abuse, but we argue that different feature sets
will be relevant to different subtasks. Future work
should aim to build a more robust understanding
of when to use which types of features.

Fifth, it is important to emphasize that not all
abuse is equal, both in terms of its effects and its
detection. We expect that social media and web-
site operators will be more interested in identify-
ing and dealing with explicit abuse, while activists,
campaigners, and journalists may have more in-
centive to also identify implicit abuse. Targeted
abuse such as cyberbullying may be more likely
to be reported by victims and thus acted upon
than generalized abuse. We also expect that im-
plicit abuse will be more difficult to detect and
model, although methodological advances may
make such tasks more feasible.

5 Conclusion

We have presented a typology that synthesizes the
different subtasks in abusive language detection.
Our aim is to bring together findings in these dif-
ferent areas and to clarify the key aspects of abu-
sive language detection. There are important an-
alytical distinctions that have been largely over-
looked in prior work and through acknowledging
these and their implications we hope to improve
abuse detection systems and our understanding of
abusive language.

Rather than attempting to resolve the “defini-
tional quagmire” (Faris et al., 2016) involved in

neatly bounding and defining each subtask we en-
courage researchers to think carefully about the
phenomena they want to measure and the appro-
priate research design. We intend for our typol-
ogy to be used both at the stage of data collection
and annotation and the stage of feature creation
and modeling. We hope that future work will be
more transparent in discussing the annotation and
modeling strategies used, and will closely exam-
ine the similarities and differences between these
subtasks through empirical analyses.

References
Pinkesh Badjatiya, Shashank Gupta, Manish Gupta,

and Vasudeva Varma. 2017. Deep learning for hate
speech detection in tweets. In Proceedings of the
26th International Conference on World Wide Web
Companion. pages 759–760.

Roland Barthes. 1957. Mythologies. Seuil.

Uwe Bretschneider and Ralf Peters. 2017. Detect-
ing offensive statements towards foreigners in social
media. In Proceedings of the 50th Hawaii Interna-
tional Conference on System Sciences.

Uwe Bretschneider, Thomas Whner, and Ralf Peters.
2014. Detecting online harassment in social net-
works. In ICIS 2014 Proceedings: Conference
Theme Track: Building a Better World through IS.

Rogers Brubaker. 2004. Ethnicity without groups. Har-
vard University Press.

Pete Burnap and Matthew L Williams. 2015. Cyber
hate speech on twitter: An application of machine
classification and statistical modeling for policy and
decision making. Policy & Internet 7(2):223–242.

Maral Dadvar, Dolf Trieschnigg, and Franciska
de Jong. 2014. Experts and machines against bul-
lies: a hybrid approach to detect cyberbullies. In
Conference on Artificial Intelligence. Springer Inter-
national Publishing.

Maral Dadvar, Dolf Trieschnigg, Roeland Ordelman,
and Franciska de Jong. 2013. Improving cyberbul-
lying detection with user context. In European Con-
ference on Information Retrieval. Springer, pages
693–696.

Thomas Davidson, Dana Warmsley, Micheel Macy,
and Ingmar Weber. 2017. Automated hate speech
detection and the problem of offensive language. In
Proceedings of the Eleventh International Confer-
ence on Web and Social Media. Montreal, Canada,
pages 512–515.

Karthik Dinakar, Birago Jones, Catherine Havasi,
Henry Lieberman, and Rosalind Picard. 2012. Com-
mon sense reasoning for detection, prevention, and

82



mitigation of cyberbullying. ACM Transactions on
Interactive Intelligent Systems (TiiS) 2(3):18.

Karthik Dinakar, Roi Reichart, and Henry Lieberman.
2011. Modeling the detection of textual cyberbully-
ing. The Social Mobile Web 11(02).

Nemanja Djuric, Jing Zhou, Robin Morris, Mihajlo Gr-
bovic, Vladan Radosavljevic, and Narayan Bhamidi-
pati. 2015. Hate speech detection with comment
embeddings. In Proceedings of the 24th Interna-
tional Conference on World Wide Web. ACM, pages
29–30.

Robert Faris, Amar Ashar, Urs Gasser, and Daisy Joo.
2016. Understanding harmful speech online. Berk-
man Klein Center Research Publication 21.

Njagi Dennis Gitari, Zhang Zuping, Hanyurwimfura
Damien, and Jun Long. 2015. A lexicon-based
approach for hate speech detection. International
Journal of Multimedia and Ubiquitous Engineering
10(4):215–230.

Gabriel Emile Hine, Jeremiah Onaolapo, Emiliano De
Cristofaro, Nicolas Kourtellis, Ilias Leontiadis, Rig-
inos Samaras, Gianluca Stringhini, and Jeremy
Blackburn. 2017. A longitudinal measurement
study of 4chan’s politically incorrect forum and its
effect on the web. In Proceedings of the Eleventh
International Conference on Web and Social Media.
Montreal, Canada, pages 92–101.

Raquel Justo, Thomas Corcoran, Stephanie M. Lukin,
Marilyn Walker, and M. Ins Torres. 2014. Extract-
ing relevant knowledge for the detection of sarcasm
and nastiness in the social web. Knowledge-Based
Systems 69:124 – 133.

Gary King, Patrick Lam, and Margaret E Roberts.
2017. Computer-assisted keyword and document set
discovery from unstructured text. American Journal
of Political Science .

April Kontostathis, Kelly Reynolds, Andy Garron, and
Lynne Edwards. 2013. Detecting cyberbullying:
Query terms and techniques. In Proceedings of the
5th Annual ACM Web Science Conference. ACM,
New York, NY, USA, WebSci ’13, pages 195–204.

Irene Kwok and Yuzhou Wang. 2013. Locate the hate:
Detecting tweets against blacks. In Proceedings of
the Twenty-Seventh AAAI Conference on Artificial
Intelligence. AAAI Press, AAAI’13, pages 1621–
1622.

Rijul Magu, Kshitij Joshi, and Jiebo Luo. 2017. De-
tecting the hate code on social media. In Proceed-
ings of the Eleventh International Conference on
Web and Social Media. Montreal, Canada, pages
608–612.

Chikashi Nobata, Joel Tetreault, Achint Thomas,
Yashar Mehdad, and Yi Chang. 2016. Abusive lan-
guage detection in online user content. In Proceed-
ings of the 25th International Conference on World
Wide Web. pages 145–153.

Björn Ross, Michael Rist, Guillermo Carbonell, Ben-
jamin Cabrera, Nils Kurowsky, and Michael Wo-
jatzki. 2016. Measuring the Reliability of Hate
Speech Annotations: The Case of the European
Refugee Crisis. In Proceedings of NLP4CMC III:
3rd Workshop on Natural Language Processing for
Computer-Mediated Communication. pages 6–9.

Anna Schmidt and Michael Wiegand. 2017. A survey
on hate speech detection using natural language pro-
cessing. In Proceedings of the Fifth International
Workshop on Natural Language Processing for So-
cial Media. Association for Computational Linguis-
tics, Valencia, Spain, pages 1–10.

Leandro Araújo Silva, Mainack Mondal, Denzil Cor-
rea, Fabrı́cio Benevenuto, and Ingmar Weber. 2016.
Analyzing the targets of hate in online social media.
In Proceedings of the Tenth International Confer-
ence on Web and Social Media. Cologne, Germany,
pages 687–690.

Derald Wing Sue, Christina M Capodilupo, Gina C
Torino, Jennifer M Bucceri, Aisha Holder, Kevin L
Nadal, and Marta Esquilin. 2007. Racial microag-
gressions in everyday life: implications for clinical
practice. American Psychologist 62(4):271–286.

Stéphan Tulkens, Lisa Hilte, Elise Lodewyckx, Ben
Verhoeven, and Walter Daelemans. 2016. The au-
tomated detection of racist discourse in dutch social
media. CLIN Journal 6:3–20.

Cynthia Van Hee, Els Lefever, Ben Verhoeven, Julie
Mennes, Bart Desmet, Guy De Pauw, Walter Daele-
mans, and Veronique Hoste. 2015a. Detection and
fine-grained classification of cyberbullying events.
In Proceedings of the International Conference Re-
cent Advances in Natural Language Processing.
Hissar, Bulgaria, pages 672–680.

Cynthia Van Hee, Ben Verhoeven, Els Lefever, Guy
De Pauw, Véronique Hoste, and Walter Daelemans.
2015b. Guidelines for the fine-grained analysis of
cyberbullying. Technical report, LT3, Ghent Uni-
versity, Belgium.

William Warner and Julia Hirschberg. 2012. Detecting
hate speech on the world wide web. In Proceed-
ings of the Second Workshop on Language in Social
Media. Association for Computational Linguistics,
LSM ’12, pages 19–26.

Zeerak Waseem. 2016a. Are you a racist or am i seeing
things? annotator influence on hate speech detection
on twitter. In Proceedings of the First Workshop on
NLP and Computational Social Science. Association
for Computational Linguistics, Austin, Texas, pages
138–142.

Zeerak Waseem. 2016b. Automatic Hate Speech De-
tection. Master’s thesis, University of Copenhagen.

Zeerak Waseem and Dirk Hovy. 2016. Hateful sym-
bols or hateful people? predictive features for hate
speech detection on twitter. In Proceedings of the

83



NAACL Student Research Workshop. Association for
Computational Linguistics, San Diego, California,
pages 88–93.

Andreas Wimmer. 2013. Ethnic boundary making:
Institutions, power, networks. Oxford University
Press.

Ellery Wulczyn, Nithum Thain, and Lucas Dixon.
2017. Ex machina: Personal attacks seen at scale.
In Proceedings of the 26th International Conference
on World Wide Web.

84


