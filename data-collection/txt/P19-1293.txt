



















































Translating Translationese: A Two-Step Approach to Unsupervised Machine Translation


Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 3057–3062
Florence, Italy, July 28 - August 2, 2019. c©2019 Association for Computational Linguistics

3057

Translating Translationese: A Two-Step Approach to Unsupervised
Machine Translation

Nima Pourdamghani♣ Nada Aldarrab♠ Marjan Ghazvininejad♦
Kevin Knight♥ Jonathan May♠

♣ Amazon ♠ USC Information Sciences Institute
♦ Facebook AI Research ♥ DiDi Labs

nimpourd@amazon.com aldarrab@isi.edu ghazvini@fb.com
kevinknight@didiglobal.com jonmay@isi.edu

Abstract

Given a rough, word-by-word gloss of a source
language sentence, target language natives can
uncover the latent, fully-fluent rendering of the
translation. In this work we explore this in-
tuition by breaking translation into a two step
process: generating a rough gloss by means of
a dictionary and then ‘translating’ the resulting
pseudo-translation, or ‘Translationese’ into a
fully fluent translation. We build our Trans-
lationese decoder once from a mish-mash of
parallel data that has the target language in
common and then can build dictionaries on de-
mand using unsupervised techniques, resulting
in rapidly generated unsupervised neural MT
systems for many source languages. We ap-
ply this process to 14 test languages, obtain-
ing better or comparable translation results on
high-resource languages than previously pub-
lished unsupervised MT studies, and obtain-
ing good quality results for low-resource lan-
guages that have never been used in an unsu-
pervised MT scenario.

1 Introduction

Quality of machine translation, especially neural
MT, highly depends on the amount of available
parallel data. For a handful of languages, where
parallel data is abundant, MT quality has reached
quite good performance (Wu et al., 2016; Hassan
et al., 2018). However, the quality of translation
rapidly deteriorates as the amount of parallel data
decreases (Koehn and Knowles, 2017). Unfortu-
nately, many languages have close to zero parallel
texts. Translating texts from these languages re-
quires new techniques.

Hermjakob et al. (2018) presented a hybrid
human/machine translation tool that uses lexical
translation tables to gloss a translation and relies
on human language and world models to propa-
gate glosses into fluent translations. Inspired by
that work, this work investigates the following

question: Can we replace the human in the loop
with more technology? We provide the following
two-step solution to unsupervised neural machine
translation:

1. Use a bilingual dictionary to gloss the input
into a pseudo-translation or ‘Translationese’.

2. Translate the Translationese into target lan-
guage, using a model built in advance from
various parallel data, with the source side
converted into Translationese using Step 1.

The notion of separating adequacy from fluency
components into a pipeline of operations dates
back to the early days of MT and NLP research,
where the inadequacy of word-by-word MT was
first observed (Yngve, 1955; Oswald, 1952). A
subfield of MT research that seeks to improve flu-
ency given disfluent but adequate first-pass trans-
lation is automatic post-editing (APE) pioneered
by Knight and Chander (1994). Much of the cur-
rent APE work targets correction of black-box MT
systems, which are presumed to be supervised.

Early approaches to unsupervised machine
translation include decipherment methods (Nuhn
et al., 2013; Ravi and Knight, 2011; Pourdamghani
and Knight, 2017), which suffer from a huge hy-
pothesis space. Recent approaches to zero-shot
machine translation include pivot-based methods
(Chen et al., 2017; Zheng et al., 2017; Cheng
et al., 2016) and multi-lingual NMT methods (Fi-
rat et al., 2016a,b; Johnson et al., 2017; Ha et al.,
2016, 2017). These systems are zero-shot for a
specific source/target language pair, but need par-
allel data from source to a pivot or multiple other
languages.

More recently, totally unsupervised NMT meth-
ods are introduced that use only monolingual data
for training a machine translation system. Lam-
ple et al. (2018a,c), Artetxe et al. (2018), and



3058

Yang et al. (2018) use iterative back-translation
to train MT models in both directions simultane-
ously. Their training takes place on massive mono-
lingual data and requires a long time to train as
well as careful tuning of hyperparameters.

The closest unsupervised NMT work to ours is
by Kim et al. (2018). Similar to us, they break
translation into glossing and correction steps.
However, their correction step is trained on ar-
tificially generated noisy data aimed at simulat-
ing glossed source texts. Although this correction
method helps, simulating noise caused by natural
language phenomena is a hard task and needs to
be tuned for every language.

Previous zero-shot NMT work compensates for
a lack of source/target parallel data by either
using source/pivot parallel data, extremely large
monolingual data, or artificially generated data.
These requirements and techniques limit the meth-
ods’ applicability to real-world low-resource lan-
guages. Instead, in this paper we propose us-
ing parallel data from high-resource languages to
learn ‘how to translate’ and apply the trained sys-
tem to low resource settings. We use off-the-
shelf technologies to build word embeddings from
monolingual data (Bojanowski et al., 2017) and
learn a source-to-target bilingual dictionary us-
ing source and target embeddings (Lample et al.,
2018b). Given a target language, we train source-
to-target dictionaries for a diverse set of high-
resource source languages, and use them to con-
vert the source side of the parallel data to Transla-
tionese. We combine this parallel data and train a
Translationese-to-target translator on it. Later, we
can build source-to-target dictionaries on-demand,
generate Translationese from source texts, and use
the pre-trained system to rapidly produce machine
translation for many languages without requiring
a single line of source-target parallel data.

We introduce the following contributions in this
paper:

• Following Hermjakob et al. (2018), we pro-
pose a two step pipeline for building a rapid
neural MT system for many languages. The
pipeline does not require parallel data or pa-
rameter fine-tuning when adapting to new
source languages.

• The pipeline only requires a comprehensive
source to target dictionary. We show that this
dictionary can be easily obtained using off-
the shelf tools within a few hours.

• We use this system to translate test texts from
14 languages into English. We obtain bet-
ter or comparable quality translation results
on high-resource languages than previously
published unsupervised MT studies, and ob-
tain good quality results for low-resource lan-
guages that have never been used in an un-
supervised MT scenario. To our knowledge,
this is the first unsupervised NMT work that
shows good translation results on such a large
number of languages.

2 Method

We introduce a two-step pipeline for unsupervised
machine translation. In the first step a source
text is glossed into a pseudo-translation or Trans-
lationese, while in the second step a pre-trained
model translates the Translationese into target. We
introduce a fully unsupervised method for convert-
ing the source into Translationese, and we show
how to train a Translationese to target system in
advance and apply it to new source languages.

2.1 Building a Dictionary

The first step of our proposed pipeline includes
a word-by-word translation of the source texts.
This requires a source/target dictionary. Manually
constructed dictionaries exist for many language
pairs, however cleaning these dictionaries to get a
word to word lexicon is not trivial, and these dic-
tionaries often cover a small portion of the source
vocabulary, focusing on stems and specifically ex-
cluding inflected variants. In order to have a com-
prehensive, word to word, inflected bi-lingual dic-
tionary we look for automatically built ones.

Automatic lexical induction is an active field of
research (Fung, 1995; Koehn and Knight, 2002;
Haghighi et al., 2008; Lample et al., 2018b). A
popular method for automatic extraction of bilin-
gual dictionaries is through building cross-lingual
word embeddings. Finding a shared word rep-
resentation space between two languages enables
us to calculate the distance between word embed-
dings of source and target, which helps us to find
translation candidates for each word.

We follow this approach for building the bilin-
gual dictionaries. For a given source and target
language, we start by separately training source
and target word embeddings S and T , and use
the method introduced by Lample et al. (2018b)
to find a linear mapping W that maps the source



3059

embedding space to the target: SW = T .
Lample et al. (2018b) propose an adversarial

method for estimating W, where a discriminator is
trained to distinguish between elements randomly
sampled fromWS and T, andW is trained to pre-
vent the discriminator from making accurate clas-
sifications. Once the initial mapping matrix W is
trained, a number of refinement steps is performed
to improve performance over less frequent words
by changing the metric of the space.

We use the trained matrix W to map the source
embeddings into the space of the target embed-
dings. Then we find the k-nearest neighbors
among the target words for each source word, ac-
cording to the cosine distance metric. These near-
est neighbors represent our translation options for
that source word.

2.2 Source to Translationese
Once we have the translation options for tokens in
the source vocabulary we can perform a word by
word translation of the source into Translationese.
However, a naive translation of each source token
to its top translation option without considering
the context is not the best way to go. Given dif-
ferent contexts, a word should be translated differ-
ently.

We use a 5-gram target language model to look
at different translation options for a source word
and select one based on its context. This language
model is trained in advance on large target mono-
lingual data.

In order to translate a source sentence into
Translationese we apply a beam search with a
stack size of 100 and assign a score equal to
αPLM + βd(s, t) to each translation option t for a
source token s, where PLM is the language model
score, and d(s, t) is the cosine distance between
source and target words. We set α = 0.01 and
β = 0.5

2.3 Translationese to Target
We train a transformer model (Vaswani et al.,
2017) on parallel data from a diverse set of high-
resource languages to translate Translationese into
a fluent target. For each language we convert the
source side of the parallel data to Translationese
as described in Section 2.2. Then we combine and
shuffle all the Translationese/target parallel data
and train the model on the result. Once the model
is trained, we can apply it to the Translationese
coming from any source language.

We use the tensor2tensor implementa-
tion1 of the transformer model with the
transformer base set of hyperparame-
ters (6 layers, hidden layer size of 512) as our
translation model.

3 Data and Parameters

For all our training and test languages, we use
the pre-trained word embeddings2 trained on
Wikipedia data using fastText (Bojanowski et al.,
2017). These embeddings are used to train bilin-
gual dictionaries.

We select English as the target language. In or-
der to avoid biasing the trained system toward a
language or a specific type of parallel data, we use
diverse parallel data on a diverse set of languages
to train the Translationese to English system. We
use Arabic, Czech, Dutch, Finnish, French, Ger-
man, Italian, Russian, and Spanish as the set of
out training languages.

We use roughly 2 million sentence pairs per
language and limit the length of the sentences
to 100 tokens. For Dutch, Finnish, and Ital-
ian we use Europarl (Koehn, 2005) for parallel
data. For Arabic we use MultiUN (Tiedemann,
2012). For French we use CommonCrawl. For
German we use a mix of CommonCrawl (1.7M),
and NewsCommentary (300K). The numbers in
parentheses show the number of sentences for
each dataset. For Spanish we use CommonCrawl
(1.8M), and Europarl (200K). For Russian we use
Yandex (1M), CommonCrawl (800K), and News-
Commentary (200K), and finally for Czech we use
a mix of ParaCrawl (1M), Europarl (640K), News-
Commentary (200K), and CommonCrawl (160K).

We train one model on these nine languages and
apply it to test languages not in this set. Also, to
test on each of the training languages, we train a
model where the parallel data for that language is
excluded from the training data. In each experi-
ment we use 3000 blind sentences randomly se-
lected out of the combined parallel data as the de-
velopment set.

We use the default parameters in Lample et al.
(2018b) to find the cross-lingual embedding vec-
tors. In order to create the dictionary we limit the
size of the source and target (English) vocabulary

1https://github.com/tensorflow/
tensor2tensor

2https://github.com/facebookresearch/
fastText/blob/master/pretrained-vectors.
md

https://github.com/tensorflow/tensor2tensor
https://github.com/tensorflow/tensor2tensor
https://github.com/facebookresearch/fastText/blob/master/pretrained-vectors.md
https://github.com/facebookresearch/fastText/blob/master/pretrained-vectors.md
https://github.com/facebookresearch/fastText/blob/master/pretrained-vectors.md


3060

to 100K tokens. For each source token we find 20
nearest neighbors in the target language. We use
a 5-gram language model trained on 4 billion to-
kens of Gigaword to select between the translation
options for each token. We use Moses scripts for
tokenizing and lowercasing the data. We do not
apply BPE (Sennrich et al., 2016) on the data. In
order to be comparable to Kim et al. (2018) we
split German compound words only for the new-
stest2016 test data. We use the CharSplit3 python
package for this purpose. We use tensor2tensor’s
transformer base hyperparameters to train
the transformer model on a single gpu for each lan-
guage.

4 Experiments

We report translation results on newstest2013
for Spanish, newstest2014 for French, and new-
stest2016 for Czech, German, Finnish, Roma-
nian, and Russian. We also report results on
the first 3000 sentences of GlobalVoices20154

for Dutch, Bulgarian, Danish, Indonesian, Pol-
ish, Portuguese, and Catalan. In each experiment
we report the quality of the intermediate Transla-
tionese as well as the scores for our full model.

fr-en de-en ru-en ro-en
Lample et al.
(2018a)

14.3 13.3 - -

Artetxe et al.
(2018)

15.6 10.2 - -

Yang et al.
(2018)

15.6 14.6 - -

Lample et al.
(2018c) (trans-
former)

24.2 21.0 9.1 19.4

Kim et al.
(2018)

16.5 17.2 - -

Translationese 11.6 13.8 5.7 8.1
Full Model 21.0 18.7 12.0 16.3

Table 1: Comparing translation results on new-
stest2014 for French, and newstest2016 for Russian,
German, and Romanian with previous unsupervised
NMT methods. Kim et al. (2018) is the method closest
to our work. We report the quality of Translationese as
well as the scores for our full model.

We compare our results against all the exist-
ing fully unsupervised neural machine translation

3https://github.com/dtuggener/
CharSplit

4http://opus.nlpl.eu/GlobalVoices.php

methods in Table 1 and show better results on
common test languages compared to all of them
except Lample et al. (2018c) where, compared to
their transformer model,5 we improve results for
Russian, but not for other languages.

The first four methods that we compare against
are based on back-translation. These methods re-
quire huge monolingual data and large training
time to train a model per test language. The fifth
method, which is most similar to our approach
(Kim et al., 2018), can be trained quickly, but
still is fine tuned for each test language and per-
forms worse than our method. Unlike the previous
works, our model can be trained once and applied
to any test language on demand. Besides this,
these methods use language-specific tricks and de-
velopment data for training their models while our
system is trained totally independent of the test
language.

We also show acceptable BLEU scores for ten
other languages for which no previous unsuper-
vised NMT scores exist, underscoring our ability
to produce new systems rapidly (Table 2).

cs-en es-en fi-en nl-en bg-en
Translationese 7.4 12.7 3.8 16.9 10.0
Full Model 13.7 22.2 7.2 22.0 16.8

da-en id-en pl-en pt-en ca-en
Translationese 13.6 7.4 8.3 15.2 10.1
Full Model 18.5 13.7 14.8 23.1 19.8

Table 2: Translation results on ten new languages:
Czech, Spanish, Finnish, Dutch, Bulgarian, Danish, In-
donesian, Polish, Portuguese, and Catalan

5 Conclusion

We propose a two step pipeline for building a rapid
unsupervised neural machine translation system
for any language. The pipeline does not require re-
training the neural translation model when adapt-
ing to new source languages, which makes its
application to new languages extremely fast and
easy. The pipeline only requires a comprehensive
source-to-target dictionary. We show how to easily
obtain such a dictionary using off-the shelf tools.
We use this system to translate test texts from 14
languages into English. We obtain better or com-
parable quality translation results on high-resource
languages than previously published unsupervised

5They present better results when combining their trans-
former model with an unsupervised phrase-based translation
model.

https://github.com/dtuggener/CharSplit
https://github.com/dtuggener/CharSplit
http://opus.nlpl.eu/GlobalVoices.php


3061

MT studies, and obtain good quality results for ten
other languages that have never been used in an
unsupervised MT scenario.

Acknowledgements

The research is based upon the work that took
place in Information Sciences Institute (ISI) which
was supported by the Office of the Director of Na-
tional Intelligence (ODNI), Intelligence Advanced
Research Projects Activity (IARPA), via AFRL
Contract FA8650-17-C-9116 and by the Defense
Advanced Research Projects Agency (DARPA)
via contract HR0011-15-C-0115. The views and
conclusions contained herein are those of the au-
thors and should not be interpreted as necessarily
representing the official policies or endorsements,
either expressed or implied, of the ODNI, IARPA,
DARPA, or the U.S. Government. The U.S. Gov-
ernment is authorized to reproduce and distribute
reprints for Governmental purposes notwithstand-
ing any copyright annotation thereon.

References
Mikel Artetxe, Gorka Labaka, Eneko Agirre, and

Kyunghyun Cho. 2018. Unsupervised neural ma-
chine translation. In Proc. ICLR.

Piotr Bojanowski, Edouard Grave, Armand Joulin, and
Tomas Mikolov. 2017. Enriching word vectors with
subword information. Transactions of the Associa-
tion for Computational Linguistics, 5:135–146.

Yun Chen, Yang Liu, Yong Cheng, and Victor O.K.
Li. 2017. A teacher-student framework for zero-
resource neural machine translation. In Proc. ACL.

Yong Cheng, Yang Liu, Qian Yang, Maosong Sun, and
Wei Xu. 2016. Neural machine translation with
pivot languages. arXiv preprint arXiv:1611.04928.

Orhan Firat, Kyunghyun Cho, and Yoshua Bengio.
2016a. Multi-way, multilingual neural machine
translation with a shared attention mechanism. In
Proc. NAACL.

Orhan Firat, Baskaran Sankaran, Yaser Al-Onaizan,
Fatos T. Yarman Vural, and Kyunghyun Cho. 2016b.
Zero-resource translation with multi-lingual neural
machine translation. In Proc. EMNLP.

Pascale Fung. 1995. Compiling bilingual lexicon en-
tries from a non-parallel English-Chinese corpus. In
Workshop on Very Large Corpora.

Thanh-Le Ha, Jan Niehues, and Alexander Waibel.
2016. Toward multilingual neural machine trans-
lation with universal encoder and decoder. arXiv
preprint arXiv:1611.04798.

Thanh-Le Ha, Jan Niehues, and Alexander Waibel.
2017. Effective strategies in zero-shot neural ma-
chine translation. arXiv preprint arXiv:1711.07893.

Aria Haghighi, Percy Liang, Taylor Berg-Kirkpatrick,
and Dan Klein. 2008. Learning bilingual lexicons
from monolingual corpora. In Proc. ACL.

Hany Hassan, Anthony Aue, Chang Chen, Vishal
Chowdhary, Jonathan Clark, Christian Feder-
mann, Xuedong Huang, Marcin Junczys-Dowmunt,
William Lewis, Mu Li, et al. 2018. Achieving hu-
man parity on automatic Chinese to English news
translation. arXiv preprint arXiv:1803.05567.

Ulf Hermjakob, Jonathan May, Michael Pust, and
Kevin Knight. 2018. Translating a language you
don’t know in the Chinese room. In Proc. ACL, Sys-
tem Demonstrations.

Melvin Johnson, Mike Schuster, Quoc V. Le, Maxim
Krikun, Yonghui Wu, Zhifeng Chen, Nikhil Thorat,
Fernanda Viégas, Martin Wattenberg, Greg Corrado,
Macduff Hughes, and Jeffrey Dean. 2017. Google’s
multilingual neural machine translation system: En-
abling zero-shot translation. Transactions of the As-
sociation for Computational Linguistics, 5:339–351.

Yunsu Kim, Jiahui Geng, and Hermann Ney. 2018.
Improving unsupervised word-by-word translation
with language model and denoising autoencoder. In
Proc. EMNLP.

Kevin Knight and Ishwar Chander. 1994. Automated
postediting of documents. In Proc AAAI.

Philipp Koehn. 2005. Europarl: A parallel corpus for
statistical machine translation. In Proc. MT summit.

Philipp Koehn and Kevin Knight. 2002. Learning a
translation lexicon from monolingual corpora. In
Proc. ACL workshop on Unsupervised lexical acqui-
sition.

Philipp Koehn and Rebecca Knowles. 2017. Six chal-
lenges for neural machine translation. In Proc. ACL
Workshop on Neural Machine Translation.

Guillaume Lample, Alexis Conneau, Ludovic Denoyer,
and Marc’Aurelio Ranzato. 2018a. Unsupervised
machine translation using monolingual corpora only.
In Proc. ICLR.

Guillaume Lample, Alexis Conneau, MarcÁurelio
Ranzato, Ludovic Denoyer, and Hervé Jégou.
2018b. Word translation without parallel data. In
Proc. ICLR.

Guillaume Lample, Myle Ott, Alexis Conneau, Lu-
dovic Denoyer, and Marc’Aurelio Ranzato. 2018c.
Phrase-based & neural unsupervised machine trans-
lation. In Proc. EMNLP.

Malte Nuhn, Julian Schamper, and Hermann Ney.
2013. Beam search for solving substitution ciphers.
In Proc. ACL.



3062

Victor Oswald. 1952. Word-by-word translation. In
Proc. intervention à la Conférence du MIT.

Nima Pourdamghani and Kevin Knight. 2017. Deci-
phering related languages. In Proc. EMNLP.

Sujith Ravi and Kevin Knight. 2011. Deciphering for-
eign language. In Proc. ACL.

Rico Sennrich, Barry Haddow, and Alexandra Birch.
2016. Neural machine translation of rare words with
subword units. In Proc. ACL.

Jörg Tiedemann. 2012. Parallel data, tools and inter-
faces in OPUS. In Proc. Lrec.

Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob
Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz
Kaiser, and Illia Polosukhin. 2017. Attention is all
you need. In Proc. NIPS.

Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V
Le, Mohammad Norouzi, Wolfgang Macherey,
Maxim Krikun, Yuan Cao, Qin Gao, Klaus
Macherey, et al. 2016. Google’s neural ma-
chine translation system: Bridging the gap between
human and machine translation. arXiv preprint
arXiv:1609.08144.

Zhen Yang, Wei Chen, Feng Wang, and Bo Xu.
2018. Unsupervised neural machine translation with
weight sharing. In Proc. ACL.

Victor H. Yngve. 1955. Sentence-for-sentence transla-
tion. Mechanical Translation, 2(2):29–37.

Hao Zheng, Yong Cheng, and Yang Liu. 2017.
Maximum expected likelihood estimation for zero-
resource neural machine translation. In Proc. IJCAI.


