



















































M2L at SemEval-2016 Task 8: AMR Parsing with Neural Networks


Proceedings of SemEval-2016, pages 1154–1159,
San Diego, California, June 16-17, 2016. c©2016 Association for Computational Linguistics

M2L at SemEval-2016 Task 8:
AMR Parsing with Neural Networks

Yevgeniy Puzikov, Daisuke Kawahara, Sadao Kurohashi
Graduate School of Informatics, Kyoto University

Yoshida-honmachi, Sakyo-ku
Kyoto, 606-8501, Japan

puzikov@nlp.ist.i.kyoto-u.ac.jp
{kuro,dk}@i.kyoto-u.ac.jp

Abstract

This paper describes our contribution to the
SemEval 2016 Workshop. We participated in
the Shared Task 8 on Meaning Representa-
tion parsing using a transition-based approach,
which builds upon the system of Wang et al.
(2015a) and Wang et al. (2015b), with addi-
tions that utilize a Feedforward Neural Net-
work classifier and an enriched feature set. We
observed that exploiting Neural Networks in
Abstract Meaning Representation parsing is
challenging and we could not benefit from it,
while the feature enhancements yielded an im-
proved performance over the baseline model.

1 Introduction

Abstract Meaning Representation (AMR) (Ba-
narescu et al., 2013; Dorr et al., 1998) is a semantic
formalism which represents sentence meaning in a
form of a rooted directed acyclic graph. AMR graph
nodes represent concepts, labelled directed edges
between the nodes show the relationships between
concepts. The AMR formalism was created in or-
der to explore the semantics behind natural language
units for further analysis and application in various
tasks.

At the time of writing this paper two AMR parsers
are publicly available: graph-based JAMR (Flani-
gan et al., 2014) and transition-based CAMR (Wang
et al., 2015a; Wang et al., 2015b). The latter has
served as our baseline model, which we tried to im-
prove by incorporating additional features defined
for a wider conditioning context and a neural net-
work (NN) classifier.

Inspired by the results of Chen and Manning
(2014) and Weiss et al. (2015), who obtained
state-of-the-art results in transition-based depen-
dency parsing using Feedforward Neural Networks
(FFNN), and taking into account the transition na-
ture of the CAMR model, we performed a series of
experiments in the same direction. Neural networks
have been successfully applied to many NLP fields
and we were curious to examine their potential in the
task of AMR parsing. Specifically, we investigated
the possibility of constraining the averaged percep-
tron algorithm (Collins, 2002) predictions by those
of an FFNN at the initial step of the inference pro-
cess.

2 Preprocessing

We used the Stanford CoreNLP v3.6.0 toolkit (Man-
ning et al., 2014) to get named entity (NE) and de-
pendency information, the latter in the form of Stan-
ford dependencies was obtained from the NN depen-
dency parser (Chen and Manning, 2014).

We used a publicly available semantic role la-
belling (SRL) system with a predicate disambigua-
tion module (Björkelund et al., 2009). The system
is a part of MATE tools1, which also include a lem-
matizer, a part of speech (POS) tagger, and a de-
pendency parser. We use them to obtain lemmas and
POS tags. All the tools were used with the pretrained
models.

Using MALT dependencies instead of or in tan-
dem with Stanford dependencies did not change the

1https://storage.googleapis.com/
google-code-archive-downloads/v2/code.
google.com/mate-tools

1154



Topic
word lemma POS NE dep integer feature vector

(3) hE1 = tanh(W
E
1 h

E
0 + b

E
1 )

(1) hE0 = [x
w, xl, xp, xn, xd]

p = softmax(W3h3) (7)

h3 = tanh(W2h2 + b2) (6)

h2 = [h
E
1 , h

I
1] (5)

hI1 = tanh(W
I
1 h

I
0 + b

I
1) (4)

hI0 = [s
cap
0 , s

len
0 , ..., b

cap
0 , b

len
0 , ...] (2)

Figure 1: Schematic view of our NN model.

overall performance of the AMR parser. Due to time
limitations, we did not perform a full analysis of the
accuracy of both tools, which would be an interest-
ing and important point to investigate further.

All information extracted after the preprocess-
ing step was combined into one CoNLL-format file
(Hajič et al., 2009). Finally, we used an AMR graph-
to-sentence aligner of Flanigan et al. (2014) to map
word spans to concept fragments in the AMR graph.

3 Parsing Algorithm

We use the same set of transitions and a parsing
algorithm as Wang et al. (2015b). We skip the
full description due to space limitations and refer
to the original paper. It forms a quadruple Q =
(S, T, s0, St), where S is a set of parsing states (or
configurations), T is a set of parsing transitions (or
actions), s0 is the initial state and St is a set of
terminal states of the parser. Each state is a triple
(σ, β,G), where σ denotes a stack, storing indices
of the nodes which have not been processed yet; it’s
top element is σ0. β is a buffer, storing the chil-
dren of σ0. Finally, G is a partially built AMR graph
aligned with the sentence tokens.

At the beginning of the parsing procedure, σ is
initialized with a post-order traversal of the input de-
pendency tree with topmost element σ0; β is initial-
ized with σ0’s children or set to null if σ0 is a leaf
node. G is initialized with the nodes and edges of
the dependency tree, but all node and edge labels
are set to null. The parser processes all nodes and
their outgoing edges in the tree in a bottom-up left-
right manner, applying some transition to the current
node or edge. Parsing terminates when both σ and β
are empty.

There are nine basic transitions (NEXT-EDGE,
SWAP, REATTACH, REPLACE-HEAD, REEN-
TRANCE, MERGE, NEXT-NODE, DELETE-
NODE, INFER), some of which result in assigning
either a concept label or an edge label. The sets
of concept labels for aligned nodes, unaligned
nodes and edge labels (Saligned tag, Sunaligned tag,
Sedge, respectively) are constructed during the
preprocessing stage and are later used to provide
candidate concept tags or edge labels for the
respective transitions. Let’s also introduce a set
Stotal = Saligned tag

⋃
Sunaligned tag

⋃
Sedge.

|Stotal| determines the number of classes for our
classification algorithm to choose from.

We propose a parsing strategy which first uses
an NN classifier to constrain the space of candidate
transitions by choosing an unlabelled version of a
transition, and then forces the perceptron algorithm
to make a prediction only on the label of the chosen
transition. We have also tried to completely substi-
tute the averaged perceptron algorithm with an NN,
experimenting with various types and architectures
of the latter, but experimental results were unsatis-
factory. This question is yet to be analysed in full,
but presumably we failed to provide our NN classi-
fiers with a sufficiently rich input representation: we
used a very simple technique of concatenating em-
bedding vectors (Section 4), which apparently did
not capture enough information for an NN to make
accurate predictions.

4 Neural Network Architecture

During preprocessing, we create a vocabulary V
which stores all unique words, lemmas, POS tags,
NE tags and dependency labels from the training set.

1155



Perceptron model features
Singular features Feature combination

{σ0, β0, βhead+0 }.{w, l, n, p, d, brown4, brown6, brown10, brown20} {σ0.w, σ0.l, σ0.p, σ0.n, σ0.d, σ0.len}+ t.tag
σhead0 .{w, l, p, n, d}, σ0.tag σ0.d+ β0.brown20, σ0.brown4 + β0.brown20, σ0.brown20 + β0.brown4, σ0.brown20 + β0.d
σ0.isnom, β0.len, β0.rph σ0.n+ β0.n, σ0.d+ β0.l, σ0.p+ β0.l, σ0.l + β0.d, σ0.l + β0.p

distσ0,β0 , distβ0,βhead+0
β0.l + β

rsb
0 .d, β0.l + β0.numswp, β0.l + β

head+
0 .d, β0.l + β

head+
0 .p

{β0, βhead+0 }.{arglabel, prdlabel}, σ0.verb sense ? β0.n+ βhead+0 .n, β0.tag + βhead+0 .tag, β0.n+ βhead+0 .tag, σ0.l + β0.tag + βhead+0 .tag
σ0.eq verb sense, {β0, βhead+0 }.{isarg, isprd} ? pathβ0,σ0 + β0.l + σ0.l, pathβ0,βhead+0 + β0.l + β

head+
0 .l

{σ0, σp0 , σlsb0 , σrsb0 , σrsb20 , σprs10 , σprs20 }.{w, l, p, n, d} • distσ0,β0 + pathσ0,β0 , distσ0,β0 + pathβhead+0 ,β0
{β0, βp0 , βlsb0 , βrsb0 , βrsb20 , βprs10 , βprs20 }.{w, l, p, n, d} • σ0.p+ tis verb, β0.d+ β0.arglabel ?

NN model features
Embedding features:{σ0, σp0 , σlsb0 , σrsb0 , σrsb20 , σprs10 , σprs20 }.{w, l, p, n, d} Numerical features: {σ0, β0}.{cap, arg0, arg1, arg2, len, nech, isnom, isleaf}
{β0, βp0 , βlsb0 , βrsb0 , βrsb20 , βprs10 , βprs20 }.{w, l, p, n, d}

Table 1: Feature sets. σhead0 is the head of σ0 in the dependency tree; βhead+0 is the node, to which β0 could be attached to as a
result of either reentrance or reattachment transition; t is a transition under consideration – during feature extraction we consider the

tag which might be assigned as a result of transition (t.tag) or whether this candidate is a verb sense tag (t.tagis verb). The isnom

feature checks whether the lemma of a corresponding element is in the NomBank dictionary. nech is the number of an element’s

children which have an NE label from the set {“PERSON”, “LOCATION”, “ORGANIZATION”, “MISC”}. Other features are
self-explanatory.

Each of them is mapped to the same D-dimensional
vector space (ewi , e

l
i, e

p
i , e

n
i , e

d
i ∈ RD). We create

one embedding matrix E ∈ R|V |×D, where |V | is
the total size of the vocabulary and D is the dimen-
sionality of dense embedding vectors.

In each parsing configuration we consider a set
of elements which might be useful in the prediction
task. These elements are σ0, β0 and their neighbour-
ing nodes, which for σ0 include σ

p
0 (the parent of

σ0 in the dependency tree), σlsb0 (the left sibling),
σrsb0 , σ

rsb2
0 (the first and the second right siblings),

σprs10 , σ
prs2
0 (the first and the second previous tokens

in the sentence); neighbouring nodes for β0 are de-
fined in a similar manner. Thus, the total number of
relevant elements is nelem = 14.

The overall architecture of the network is depicted
in Figure 1. The input layer of the network con-
sists of two components. The first one is formed by
concatenating all the corresponding embedding vec-
tors for each element’s feature (Figure 1 (1)). Each
of the x components is, in turn, a concatenation of
the embeddings of the configuration elements for a
particular type of annotation. For example, xw is
an RN vector, where N = d × nelem. We form
xl, xp, xn, xd in a similar manner and concatenate
xw, xl, xp, xn, xd to form the input vector hE0 . The
second component of the input vector is hI0 (Figure
1 (2)), a concatenation of vectors, representing non-
embedding numerical features (see Table 1).

We separately map two parts of the input layer to
hidden layers using the tanh activation function: hE0
to hE1 , h

I
0 to h

I
1 (Figure 1 (3) and (4)) . We then con-

catenate layers hE1 and h
I
1 (Figure 1 (5)) and pass the

resultant vector to the last hidden layer h3, applying
the tanh function again (Figure 1 (6)). Finally, a
softmax layer is added on top of h3 in order to cal-
culate probabilities of the output classes (Figure 1
(7)).

5 Feature Sets

We have designed two separate feature sets for the
NN and perceptron classifiers. The feature set for
the latter is roughly the same as in (Wang et al.,
2015b) (Table 1). Following the authors of CAMR,
we also make use of the NomBank 1.0 dictionary
(Meyers et al., 2004) 2. Unfortunately, we could
not obtain the copy of the same SRL system which
was used by the authors. Therefore, we also mea-
sure accuracy improvement from incorporating the
semantic features defined in the original paper but
extracted after processing the data with a different
SRL system (marked with a ?).

We also measure the improvement from incorpo-
rating the features extracted from a wider configura-
tion context – they were not included into the base-
line model and are marked with a •.

In the case of the NN classifier we follow a
standard feature extraction procedure and discard
transition-specific features. Apart from the embed-
ding features, we also include a number of numeri-
cal features, which proved to be useful in our exper-

2http://nlp.cs.nyu.edu/meyers/NomBank.
html

1156



iments.

6 Training Procedure and Parsing Policy

We trained the perceptron with a weight-averaging
procedure, described in (Collins, 2002).

For the NN classifier we first prepared a training
set {(xi, yi)}ni=1, where xi denotes a feature repre-
sentation of configuration i and yi is the unlabelled
version of the correct transition. The training ob-
jective is to minimize L2-regularized negative log-
likelihood of the model:

L(θ) =−
n∑

i=1

logP (yi|xi, θ) + λ||θ||22,

θ = [E,WE1 ,W
I
1 ,W2,W3].

We randomly initialized the embeddings within
(−0.01, 0.01) and fixed their dimensionality at 32.
All weight matrices were initialized using the nor-
malized initialization technique of Glorot and Ben-
gio (2010). Hidden layer sizes were fixed as follows:
hE0 = 2, 240, h

I
0 = 32, h

E
1 = 100, h

I
1 = 16, h2 =

116, h3 = 64. All the biases were set to 0.02.
We train our network using mini-batches of size

200 under the early stopping settings with RM-
Sprop3 as an optimization algorithm. The learning
rate of 5× 10−4 and λ = 1.5× 10−5 were found to
perform best on the validation set.

Given a feature representation of the current con-
figuration s, the parsing algorithm first provides a
pool of legal transitions T ∈ Stotal, |T | � |Stotal|.
We compute the probabilities of nine unlabelled
transitions using the NN. If the network is confident
about its prediction (we set an empirically chosen
probability threshold of 0.9), we choose the highest
scoring candidate and force the perceptron to pre-
dict the label of the transition, if it is a transition
which assigns a concept tag or edge label. Each
candidate t ∈ T is being scored by a linear scoring
function score(t, s) = ω · φ(t, s), where ω denotes
a weight vector for a particular candidate transition
and φ(t, s) is a feature function mapping a (t, s) pair
to a real-valued feature vector. The best scoring tran-
sition is chosen and applied to the configuration. If
the NN chooses a transition which does not assign

3http://www.cs.toronto.edu/˜tijmen/
csc321/slides/lecture_slides_lec6.pdf

Model P R F1
Perceptron (baseline) 0.56 0.66 0.60
Perceptron + SRL 0.57 0.68 0.62
Perceptron + wide context 0.58 0.69 0.63
Perceptron + SRL + wide context 0.58 0.68 0.63
NN + Perceptron 0.56 0.66 0.60
NN + Perceptron + SRL + wide context 0.58 0.68 0.63

Table 2: Experimental results.

labels, we apply this transition without asking for the
perceptron prediction. Finally, if the network is not
confident about the prediction – that is, if the proba-
bility for the highest scoring candidate is lower than
0.9 – we disregard the NN prediction and choose the
prediction given by the perceptron algorithm.

7 Experiments

All the experiments were performed on the
LDC2015E86 dataset, provided by the organiz-
ers. In our experiments we followed the standard
train/dev/test split (16, 833, 1, 368 and 1, 371 sen-
tences, respectively). Parser performance was evalu-
ated with the Smatch (Cai and Knight, 2013) scoring
script v2.0.2 4 (Table 2).

As expected, using SRL features resulted in bet-
ter performance compared to the baseline model
(roughly a 2 F1 points gain). Conditioning on a
wider context was also beneficial – widening the
context to include more configuration elements is of-
ten a good feature expansion technique (Toutanova
et al., 2003; Chen et al., 2014). In contrast to
our expectations, the NN classifier did not improve
the parser performance. This might be due to the
higher complexity of the AMR parsing task or the
peculiarities of the underlying parsing algorithm (as
mentioned in Section 3, we discarded some action-
specific features due to the difficulty of their integra-
tion into the NN model). Further investigation on
this matter is required to draw ground conclusions.

8 Conclusion

We have performed a range of experiments which re-
sulted in improving the performance of the baseline
AMR parsing system. The results show that a richer
feature set is very likely to lead to more accurate pre-
dictions. Unfortunately, our attempts to further im-

4http://alt.qcri.org/semeval2016/task8/
index.php?id=data-and-tools

1157



prove the system using NN were not that successful.
This goes against the hypothesis that a small num-
ber of dense vector embedding features are sufficient
to capture the information necessary for accurate in-
ference, which in traditional approaches is achieved
by using a large amount of sparse hand-crafted fea-
tures (Chen and Manning, 2014). The obtained re-
sults will be used in our further investigation on this
matter.

Acknowledgments

We thank the anonymous reviewers for their insight-
ful comments and suggestions. The first author is
supported by a Japanese Government (MEXT) re-
search scholarship.

References

Laura Banarescu, Claire Bonial, Shu Cai, Madalina
Georgescu, Kira Griffitt, Ulf Hermjakob, Kevin
Knight, Philipp Koehn, Martha Palmer, and Nathan
Schneider. 2013. Abstract meaning representation
for sembanking. In Proceedings of the 7th Linguistic
Annotation Workshop and Interoperability with Dis-
course, pages 178–186.

Anders Björkelund, Love Hafdell, and Pierre Nugues.
2009. Multilingual semantic role labeling. In Pro-
ceedings of the Thirteenth Conference on Compu-
tational Natural Language Learning: Shared Task,
CoNLL ’09, pages 43–48, Stroudsburg, PA, USA. As-
sociation for Computational Linguistics.

Shu Cai and Kevin Knight. 2013. Smatch: an evalu-
ation metric for semantic feature structures. In Pro-
ceedings of the 51st Annual Meeting of the Associa-
tion for Computational Linguistics (Volume 2: Short
Papers), pages 748–752, Sofia, Bulgaria, August. As-
sociation for Computational Linguistics.

Danqi Chen and Christopher Manning. 2014. A fast and
accurate dependency parser using neural networks.
In Proceedings of the 2014 Conference on Empirical
Methods in Natural Language Processing (EMNLP),
pages 740–750, Doha, Qatar, October. Association for
Computational Linguistics.

Wenliang Chen, Yue Zhang, and Min Zhang. 2014. Fea-
ture embedding for dependency parsing. In Proceed-
ings of COLING 2014, the 25th International Confer-
ence on Computational Linguistics: Technical Papers,
pages 816–826, Dublin, Ireland, August. Dublin City
University and Association for Computational Lin-
guistics.

Michael Collins. 2002. Discriminative training methods
for hidden markov models: Theory and experiments
with perceptron algorithms. In Proceedings of the
ACL-02 Conference on Empirical Methods in Natural
Language Processing - Volume 10, EMNLP ’02, pages
1–8, Stroudsburg, PA, USA. Association for Compu-
tational Linguistics.

Bonnie Dorr, Nizar Habash, and David Traum. 1998.
A thematic hierarchy for efficient generation from
lexical-conceptual structure. In Proceedings of the
Third Conference of the Association for Machine
Translation in the Americas, AMTA-98, in Lecture
Notes in Artificial Intelligence, pages 333–343.

Jeffrey Flanigan, Sam Thomson, Jaime Carbonell, Chris
Dyer, and A. Noah Smith. 2014. A discriminative
graph-based parser for the abstract meaning represen-
tation. In Proceedings of the 52nd Annual Meeting
of the Association for Computational Linguistics (Vol-
ume 1: Long Papers), pages 1426–1436.

Xavier Glorot and Yoshua Bengio. 2010. Understanding
the difficulty of training deep feedforward neural net-
works. In Proceedings of the International Conference
on Artificial Intelligence and Statistics (AISTATS10).
Society for Artificial Intelligence and Statistics.

Jan Hajič, Massimiliano Ciaramita, Richard Johans-
son, Daisuke Kawahara, Maria Antònia Martı́, Lluı́s
Màrquez, Adam Meyers, Joakim Nivre, Sebastian
Padó, Jan Štěpánek, Pavel Straňák, Mihai Surdeanu,
Nianwen Xue, and Yi Zhang. 2009. The conll-
2009 shared task: Syntactic and semantic dependen-
cies in multiple languages. In Proceedings of the Thir-
teenth Conference on Computational Natural Lan-
guage Learning: Shared Task, CoNLL ’09, pages 1–
18, Stroudsburg, PA, USA. Association for Computa-
tional Linguistics.

Christopher D. Manning, Mihai Surdeanu, John Bauer,
Jenny Finkel, Steven J. Bethard, and David McClosky.
2014. The Stanford CoreNLP natural language pro-
cessing toolkit. In Association for Computational Lin-
guistics (ACL) System Demonstrations, pages 55–60.

A. Meyers, R. Reeves, C. Macleod, R. Szekely, V. Zielin-
ska, B. Young, and R. Grishman. 2004. The nombank
project: An interim report. In A. Meyers, editor, HLT-
NAACL 2004 Workshop: Frontiers in Corpus Annota-
tion, pages 24–31, Boston, Massachusetts, USA, May
2 - May 7. Association for Computational Linguistics.

Kristina Toutanova, Dan Klein, Christopher D. Manning,
and Yoram Singer. 2003. Feature-rich part-of-speech
tagging with a cyclic dependency network. In Pro-
ceedings of the 2003 Conference of the North Ameri-
can Chapter of the Association for Computational Lin-
guistics on Human Language Technology - Volume 1,
NAACL ’03, pages 173–180, Stroudsburg, PA, USA.
Association for Computational Linguistics.

1158



Chuan Wang, Nianwen Xue, and Sameer Pradhan.
2015a. A transition-based algorithm for amr pars-
ing. In Proceedings of the 2015 Conference of the
North American Chapter of the Association for Com-
putational Linguistics: Human Language Technolo-
gies, pages 366–375, Denver, Colorado, May–June.
Association for Computational Linguistics.

Chuan Wang, Nianwen Xue, and Sameer Pradhan.
2015b. Boosting transition-based amr parsing with
refined actions and auxiliary analyzers. In Proceed-
ings of the 53rd Annual Meeting of the Association
for Computational Linguistics and the 7th Interna-
tional Joint Conference on Natural Language Process-
ing (Volume 2: Short Papers), pages 857–862, Beijing,
China, July. Association for Computational Linguis-
tics.

David Weiss, Chris Alberti, Michael Collins, and Slav
Petrov. 2015. Structured training for neural network
transition-based parsing. In Proceedings of the 53rd
Annual Meeting of the Association for Computational
Linguistics and the 7th International Joint Conference
on Natural Language Processing (Volume 1: Long Pa-
pers), pages 323–333, Beijing, China, July. Associa-
tion for Computational Linguistics.

1159


