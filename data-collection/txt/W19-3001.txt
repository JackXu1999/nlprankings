



















































Towards augmenting crisis counselor training by improving message retrieval


Proceedings of the Sixth Workshop on Computational Linguistics and Clinical Psychology, pages 1–11
Minneapolis, Minnesota, June 6, 2019. c©2019 Association for Computational Linguistics

1

Towards Augmenting Crisis Counselor Training by Improving Message
Retrieval

Orianna DeMasi
University of California

Berkeley

Marti A. Hearst
University of California

Berkeley

Benjamin Recht
University of California

Berkeley

Abstract

A fundamental challenge when training coun-
selors is presenting novices with the opportu-
nity to practice counseling distressed individ-
uals without exacerbating a situation. Rather
than replacing human empathy with an auto-
mated counselor, we propose simulating an in-
dividual in crisis so that human counselors in
training can practice crisis counseling in a low-
risk environment. Towards this end, we collect
a dataset of suicide prevention counselor role-
play transcripts and make initial steps towards
constructing a CRISISbot for humans to coun-
sel while in training. In this data-constrained
setting, we evaluate the potential for message
retrieval to construct a coherent chat agent
in light of recent advances with text embed-
ding methods. Our results show that embed-
dings can considerably improve retrieval ap-
proaches to make them competitive with gen-
erative models. By coherently retrieving mes-
sages, we can help counselors practice chat-
ting in a low-risk environment.

1 Introduction

Suicide prevention hotlines can provide immedi-
ate care in critical times of need (Gould et al.,
2012, 2013; Ramchand et al., 2016). These hot-
lines are expanding services to text to meet grow-
ing demands and adapt to shifts in communication
trends (Smith and Page, 2015). Crisis helplines
rely on counselors who are trained in a variety of
skills, such as empathy, active listening, assessing
risk of suicide, de-escalation, and connecting in-
dividuals to longer term solutions (Gould et al.,
2013; Paukert et al., 2004).

Properly training counselors is critical yet diffi-
cult as, resource costs aside, counselors need to
practice and develop expertise in realistic envi-
ronments that are low-risk, i.e., they do not put
distressed individuals in danger. Because novice

counselors are unable to assume full responsibil-
ity for a crisis situation until they have some ex-
perience, training often includes human-to-human
role-playing (American Association of Suicidol-
ogy, 2012; Suicide Prevention Resource Center,
2007). Role-playing has been shown to improve
crisis intervention training (Cross et al., 2011).
However, such training takes a lot of human time,
which centers struggle to provide.

Instead of attempting to scale services by re-
placing human counselors and trying to automate
the generation of empathetic responses, we seek to
build a training tool that can augment hotline train-
ing and empower more counselors. As a first com-
ponent, we develop a chat interface where novices
can practice formulating responses by interacting
with a simulated distressed individual.

To build such a system, we collect synthetic
role-play transcripts that provide example scenar-
ios and example messages.Because real transcripts
may contain scenarios that cannot be fully de-
identified, we hope that synthetic transcripts will
enable the development of a training system with-
out violating the confidentiality of anyone contact-
ing a real hotline. Here, we consider the one-sided
case of simulating the individual in distress with
the intention of eventually providing a training en-
vironment for novice counselors to practice coun-
seling without putting anyone in danger.

In the application we consider, and in many sim-
ilarly data-constrained applications, language gen-
eration methods may be challenged by the limited
data that can initially be collected. To surmount
this issue, we explore the extent to which retrieval
methods can be improved to provide an engaging
chat experience. More specifically, we consider
whether improved embedding methods, which en-
able better representation of text, improve retrieval
models through better comparisons of text similar-
ity. Briefly stated, we ask two research questions:



2

RQ1 Do improved embedding methods retrieve
coherent responses to a single turn of context
more often than commonly-used TF-IDF or
generative models?

RQ2 Can we extend retrieval baseline models to
consider more than one turn of context when
selecting a response?

Our results show that recent developments in
embedding methods have considerably improved
dialogue retrieval, which is promising for the use
of these methods in data-limited applications. We
also find that extending retrieval to consider ad-
ditional messages of context does improve base-
lines. This indicates the potential for retrieval
methods to benefit data-limited dialogue systems
and the need to re-evaluate baselines for genera-
tive models. Within the setting that we study, our
results provide promise for building a chat module
that can enable crisis counselors to practice before
interacting with individuals in need.

2 Related Work

Considerable potential for automating a counselor
was shown with the initial rule-based Eliza sys-
tem (Weizenbaum, 1966) and recent developments
have sought to target systems for delivering cogni-
tive behavioral therapy (Fitzpatrick et al., 2017).
Other studies have looked at the effect of sui-
cide prevention counselor training (Gould et al.,
2013), identifying patterns of successful crisis hot-
line counselors (Althoff et al., 2016), automating
counselor evaluation (Pérez-Rosas et al., 2017),
and building a dashboard for crisis counselors (Di-
nakar et al., 2015). There is additional work to
identify supportive and distressed behaviors and
language in online forums (Balani and De Choud-
hury, 2015; De Choudhury and De, 2014; Wang
and Jurgens, 2018) and support forum moderators
(Hussain et al., 2015). Most similar to our study,
was one study that showed the potential for an
avatar system to help train medical doctors to de-
liver news to patients (Andrade et al., 2010). How-
ever, this study did not target counselors or train
conversation strategies. To our knowledge, there
has been no work on automating the individual
seeking help to improve counselor training.

2.1 Text Retrieval for Dialogue Systems

Previous systems have explored the use of retriev-
ing messages from related contexts for continuing

I did not know who 
to talk to

This is a safe place to talk. 
Tell me more about what is 
going on to make you 
feel sad and stressed

Well, my parents have been 
fighting a lot for the past few 
months and I got a C 
on a test today

Visitor

Counselor

mi

ci

ri

Figure 1: A conversation snippet showing a visitor’s
response ri to a counselor’s messagemi with preceding
context, i.e., a visitor’s message ci.

dialogue. Some studies have looked at defining
or learning scoring functions over IDF weights to
construct retrieval scores (Krause et al., 2017; Rit-
ter et al., 2011). Most similar to our work is a
system that considered similarities of full histories
of dialogues in addition to a previous turn of con-
text (Banchs and Li, 2012) and another study that
hand-tuned weights in a scoring function on IDF
weights to include additional messages of context
(Sordoni et al., 2015). However, these works used
similarities calculated over TF-IDF (Baeza-Yates
et al., 2011) and bag-of-words of representations,
instead of more recent embedding methods (Bo-
janowski et al., 2016; Conneau et al., 2017; Pen-
nington et al., 2014; Peters et al., 2018; Subrama-
nian et al., 2018), which we explore.

3 Dataset

We collected a dataset of synthetic chat transcripts
between suicide prevention counselors and hotline
visitors. An example of such a conversation is
shown in Figure 1 and additional examples are dis-
cussed in the Results section. Artificial or role-
play transcripts were generated by trained coun-
selors in order to protect the identity of any indi-
viduals who may contact crisis hotlines. We chose
this approach because retrieval should not be used
on datasets consisting of real conversations. Such
datasets have been explored in prior work to un-
derstand effective hotline conversations (Althoff
et al., 2016).



3

Role-playing between experienced and novice
counselors is a common tool for crisis counselor
training, and is a task counselors are often ex-
posed to before being approved to work on a hot-
line (American Association of Suicidology, 2012;
Kalafat et al., 2007). In addition to expecting
role-playing to be a natural task for hotline coun-
selors, prior work on short, unstructured social di-
alogues between peers found that self-dialogues,
i.e., where an individual would produce both sides
of a two-person dialogue, generated high quality
and creative example conversations (Krause et al.,
2017). We followed this work and asked experi-
enced counselors to self-role-play scenarios of a
counselor working with a hotline visitor. We col-
lected transcripts in three phases: full role-plays,
visitor-only role-plays, and counselor-paraphrase
role-plays.

3.1 Collection

After consenting to participate in the study, coun-
selors were invited to the first of three phases. In
the first phase, counselors were asked to role-play
both sides of a potential crisis text conversation.
To be representative of common demographic of
individuals who contact a helpline over text, coun-
selors were prompted to role-play a youth expe-
riencing trouble in school and with their parents.
This persona was chosen to represent a common
scenario that a counselor may encounter in a text-
based conversation. The counselors were able to
decide if the fictional youth was experiencing sui-
cidal thoughts, specific issues they were having,
and if they felt better by the end of the conversa-
tion. Transcripts were required to be 20 turns for
each counselor and visitor (40 turns total). How-
ever, participants were able to extend the conver-
sation to at most 60 turns total, if they chose. Mes-
sages were unconstrained in length, but it was sug-
gested that they resemble SMS messages.

Counselors who participated in a second phase
of the study were given the counselor’s side of a
transcript generated in the first phase of the study
and asked to role-play only the youth experiencing
trouble in a way that fit with the counselor’s mes-
sages. Participants in the third phase of the study
were given a full transcript generated in the first
phase and asked to generate counselor paraphrases
that reworded and possibly improved the original
counselor messages. The second and third phases
were designed to increase the variety of responses

Phase Count
Unique conversations 1 254
Visitor-only role-plays 2 182
Counselor-only role-plays 3 118
Visitor messages 1-2 9062
Counselor messages 2 5320
Counselor paraphrases 3 2999

Table 1: Statistics on role-play transcripts. Phase in-
dicates the study phase during which each set of data
was collected. Each counselor paraphrase reworded a
single counselor message.

that might be made.
Additional data were collected for evaluating

models, as will be discussed below. All study
methods were approved by the university’s Inter-
nal Review Board.

3.2 Dataset Statistics
In total, 32 crisis counselors participated in the
study and wrote example messages. In general, the
transcripts represent a broad range of scenarios.
Statistics on the resulting dataset are in Table 1. In
the following results, we do not include messages
generated in the second phase of the study.

4 Methods

After preprocessing, we consider two tasks: how
to return a visitor response to a single input coun-
selor message and how to return a visitor response
when considering a counselor input message and
preceding conversation context. For responding
to a single counselor input message, we consider
two approaches: one based on cosine similarity
of vector representations and the other based on
likelihood. For responding to a counselor message
when considering additional conversation context,
we extend retrieval to consider additional mes-
sages of context, i.e., an additional message pre-
ceding the counselor’s last message. For gener-
ating responses, we consider a popular Seq2Seq
model (Sutskever et al., 2014; Vinyals and Le,
2015) and a hierarchical neural model (Park et al.,
2018).

4.1 Data Preprocessing
Names were standardized to be popular Ameri-
can male or female baby names from the last 5
decades. Entire messages were tokenized with ap-
propriate tokenizers for each embedding method
and converted to lowercase, as appropriate.



4

4.2 Response Retrieval Considering a Single
Message

For the first retrieval approach we consider, let
a message input to the system be mi. Let MN
and RN be all the N messages and responses, re-
spectively, in the training set and mj and rj in-
dicate individual messages and responses in the
training set. The first method considers all the
messages in the training set and returns the re-
sponse rj′ to the messagemj′ that shares the high-
est cosine similarity with the input message, i.e.,
j′ = argmaxj sim(mi,mj) where j indexes over
the messages in the training set.

Similarity is commonly calculated as cosine
similarity between TF-IDF vector representations
of the input (i.e., counselor) message mi and mes-
sages in the training set. We compare the TF-
IDF representation with additional vector repre-
sentations of the counselor input. Exhaustive com-
parison of embedding methods is not feasible, so
we chose popular, successful, and diverse embed-
dings: GloVe (Pennington et al., 2014), FastText
(Bojanowski et al., 2016), Attract-Repel (Vulić
et al., 2017), and ELMo (Peters et al., 2018; Gard-
ner et al., 2018). We also consider two sentence
embeddings: InferSent (Conneau et al., 2017) and
GenSen (Subramanian et al., 2018). Messages are
embedded by summing the embeddings of their el-
ements, e.g., across words or sentences for appro-
priate embeddings.

For the second retrieval approach, we select the
response from the training data that is most prob-
able, i.e, j′ = argmaxj P (rj |mi) where mi is
again the input message and j indexes over train-
ing examples. With this approach, which we will
refer to as S2S-retrieve, the probability of a re-
sponse is calculated by a Seq2Seq model trained
on counselor-visitor message-response pairs. All
Seq2Seq models were trained in the OpenNMT
framework (Klein et al., 2017).

4.2.1 Response Retrieval Considering More
than One Message of Context

When multiple messages of context are present,
we propose including the additional context in the
retrieval methods in three ways. For this work,
we consider only one message in the conversation
that precedes the counselor’s input message to be
additional context, as indicated in Figure 1.

First, we consider the response from the train-
ing data rj′ that has the highest similarity calcu-

lated over the sum of the previous messages em-
beddings, i.e., considering contexts ci and cj that
precede a test message mi and a training mes-
sage mj respectively, we choose rj′ such that j′ =
argmaxj sim(mi + ci,mj + cj).

As a second approach, we measure context sim-
ilarity as the weighted sum of context and mes-
sage similarities: j′ = argmaxj sim(mi,mj) +
λsim(ci, cj). The weight parameter λ is found via
cross-validation to optimize the similarity of em-
bedded responses returned with true responses on
a development set.

Third, for the likelihood based model, we again
consider the response from the training set that
returns the highest likelihood, as calculated by a
Seq2Seq model. To include an additional context
message, we concatenate preceding messages be-
fore encoding and decoding.

4.3 Response Generation

For generating a response to a single coun-
selor message, we consider a Seq2Seq model
(Sutskever et al., 2014).

When considering an additional message of
context, we first use the Seq2Seq model with the
preceding messages concatenated into a single in-
put. Second, we use a Variational Hierarchical
Conversation RNN (VHCR) that explicitly models
prior conversation state with a hierarchical struc-
ture of latent variables (Park et al., 2018). This
model has been shown to improve on other models
that adjust for context when there is more than one
preceding utterance (Park et al., 2018). Seq2Seq
and VHCR model embeddings are initialized with
GloVe vectors (Pennington et al., 2014).

5 Experiments

For the two response selection tasks, we randomly
separated transcripts into training, development,
and test sets, with the training set accounting for
80% of the conversations and the rest evenly dis-
tributed between development and test sets. Coun-
selor paraphrases were assigned to the set that
their original message was assigned to. Messages
were not randomly shuffled, but separated by con-
versation, to avoid training on data related to the
test data. For both research questions, a response
was either generated from a model trained on the
training set or retrieved from the bank of train-
ing examples for every counselor message or para-
phrased counselor message in the test set.



5

Method Embedding
unit

Selection
metric

Percent that
made sense

Avg. tokens
in response

Avg. tokens
in MS

Random – – 25.30 15.1 12.6
re

tr
ie

va
l

TF-IDF word cos-sim 60.34 13.1 12.4
Attract-Repel word cos-sim 58.50 18.3 16.2
ELMo word cos-sim 65.88 14.5 14.0
FastText word cos-sim 62.71 16.2 15.5
GloVe word cos-sim 58.63 15.9 15.1
GenSen sentence cos-sim 64.16 14.5 14.2
InferSent sentence cos-sim 61.79 14.9 14.0
S2S-retrieve – likelihood 67.46 8.8 8.2

ge
n. S2S-generate – – 64.16 11.7 10.8

Ground truth – – 89.33 14.6 14.6

Table 2: Performance of methods used to return a response to a single input message. MS indicates the set of
responses that crowdworkers judged as making sense in context, rather than all the responses that the method
returned. Both the best performing method and ground truth results are in bold.

5.1 Evaluation

To evaluate the overall quality of responses that
methods returned, we follow prior work that in-
dicated there is currently no automatic equivalent
and used human judges (Liu et al., 2016). These
judges were crowdworkers on Amazon Mechani-
cal Turk1 who had been granted Masters status and
were located in the United States. Crowdwork-
ers were presented with instructions, labeled ex-
amples, and batches of 10 cases where they were
asked to judge responses to messages.

To evaluate methods for the first research ques-
tion, crowdworkers were given a single message
and a response and asked to judge the response.
For the second research question, crowdworkers
were given two messages of context and a high-
lighted response and asked to judge the response.

In contrast to studies that rank on scales (Lowe
et al., 2017), we directly asked the workers to de-
cide if a response made sense or not. In addition
to indicating that a response did or did not make
sense, we allowed a third class for workers to in-
dicate if they were unsure without additional con-
text. We found these classes to be sufficiently de-
scriptive to consistently label messages between
researchers. In preliminary trials with crowdwork-
ers, there was insufficient agreement on labels.
This instability of labels could stem from a vari-
ety of causes, including uncertainty about whether
a change of topic should be considered a coherent
response. To surmount this ambiguity, we asked

1https://www.mturk.com/

two crowdworkers to label each response and a
third crowdworker to break any ties. All cases
where crowdworkers indicated that they were un-
sure were considered to be labeled as not coherent.
With this voting approach, on a trial set of mes-
sage and response pairs, crowdworker labels cor-
responded with researcher determined labels with
a Cohen’s Kappa of 0.69 (Cohen, 1968), indicat-
ing considerable agreement.

5.2 Performance Metrics

To assess the quality of a method at returning re-
sponses, we take messages from a held-out test set
and return a response to it by either selecting a
message from the training set or generating a re-
sponse with a model trained on the message and
response pairs in the training set. The split into
training, development, and test sets is held con-
stant across methods. We ask crowdworkers to
judge whether each response makes sense as a pos-
sible response to the given message and aggregate
multiple crowdworker decisions into a single label
for each returned response. We then use the per-
cent of responses returned by a method that were
labeled as making sense as an indicator of method
performance. The higher percent of messages that
made sense as responses, the better the method is
at responding coherently. We also consider the
number of tokens in each response returned by a
method and average the number across all the re-
sponses returned as a surrogate for how interesting
the responses are. Presumably, longer messages
are more interesting than short responses.

https://www.mturk.com/


6

Decision Subcategory Count

Makes sense

Answers the counselor’s question(s) 17
Logical response, fits the conversation 15
Not perfect, but conceivable someone could respond this way 7
Agrees/disagrees with counselor’s statement 2

Mismatched

Doesn’t answer or respond to the question 11
Messages are unrelated 9
Doesn’t fit, seem right, or make sense 4
Responses answers a different question 3
Response is a bad, incoherent message 3
Message is from a different part of the conversation 2

Unclear

Response is vague or confusing 4
Worker just didn’t know 3
Can’t tell without more context 2
Explanation of why worker is unsure 1

Other Researchers were unsure what rationale meant 13Description of message content 4

Table 3: Themes in crowdworker rationales for why a response made sense or not. The count is the number of
rationales out of a subset of 100 pairs that shared the theme.

5.3 Random and Ground Truth Baselines

For the first research question, we included a
method that randomly selected responses from the
training set to messages in the test set. This
method is intended as a baseline for how easy the
task was for a method to guess responses.

For both the first and second research questions,
we included a method that returned ground truth
visitor responses from the test set as an indicator
of how hard the task was for humans to determine
response quality without additional context.

5.4 Assessing Why Responses Are Coherent

To understand how crowdworkers decided if a
response was coherent, we asked crowdworkers
to evaluate responses on a set of 100 message-
response pairs and additionally provide a ratio-
nale for their decision. For each of 50 test mes-
sages, we made two pairs: one with a response
randomly selected from the training messages and
the other with the ground truth response from the
test set. These two methods where chosen to gen-
erate pairs that were not likely and likely to be co-
herent. We directly asked whether the response
was coherent and “Why did you choose that op-
tion?” with an open text box for crowdworkers to
enter a rationale. We read and grouped the ratio-
nales into themes of why responses did or did not
make sense.

6 Results

We present results on two tasks corresponding to
our two research questions: retrieving a response
to a counselor’s message and extending retrieval

to consider an additional message of context. We
also consider rationales for why responses do or
do not make sense.

6.1 Comparing Retrieval Methods for a
Single Message of Context

Retrieval methods showed a clear benefit over ran-
domly selecting responses, i.e., retrieval methods
returned a higher percent of coherent messages, as
judged by crowdworkers (Table 2). ELMo embed-
dings and three other embeddings (FastText, In-
ferSent, and GenSen) improved on the commonly
used TF-IDF retrieval baseline. This suggests that
retrieval methods with recent embeddings provide
stronger baselines for generative methods and may
continue to improve alongside better embedding
methods. However, only the ELMo embeddings
surpassed the Seq2Seq generative model in per-
cent of coherent messages returned, indicating the
potential of generative models even in this data-
limited setting.

Retrieving responses with a Seq2Seq likelihood
score returned the highest percent of coherent re-
sponses, but the shortest messages on average.
Short messages may indicate that, while coherent,
the messages may be less engaging or interesting
for an interactive environment.

When assessing crowdworker rationales for de-
ciding if a response was coherent, we found 16
themes or reasons why crowdworkers would de-
cide whether a response made sense (Table 3).
Crowdworkers cited more ways that responses did
not make sense than ways a response would be co-
herent. Examples of messages in the test set and
the corresponding responses that each method re-



7

Method Example 1 Example 2 Example 3

M
es

sa
ge

Wow, not even 18 but al-
ready paying the bills. How
old are you?

Sounds like a great plan,
take care Jessica... we’re
here 24/7 in case you need to
talk to us again

Hi, I’m Michael! What is
your name? Location?

R
es

po
ns

es

Random I took a small amount. (N) Sometimes I have to. They
don’t really like hanging out
with him. (N)

Yes, please. (N)

Tf-idf I’m 17. (Y) Thanks, it’s good to talk about
this stuff. I have to go eat din-
ner now (Y)

Hi Michael, this is Christo-
pher. I’m in Daly City and I
need help. (Y)

Attract-Repel I feel lonely (N) Maybe. Maybe next time. (N) I’m Jennifer (Y)
ELMo 14 (Y) ok thx (Y) i’d rather not say my name. is

this space confidential (Y)
FastText 14 (Y) ok thx (Y) i’d rather not say my name. is

this space confidential (Y)
GloVe Im 15, i have a twin but hes a

boy. Im a girl. They let him
get away with evvvverything
at home and school (Y)

Maybe. Maybe next time. (N) i’d rather not say my name. is
this space confidential (Y)

GenSen 14 (Y) Okay. I got it. Thank you.
You were really helpful. I was
feeling really awful but I’m
feeling better now. (Y)

i’d rather not say my name. is
this space confidential (Y)

InferSent 14 (Y) Yes please. I’ll be right back.
(N)

i’d rather not say my name. is
this space confidential (Y)

S2S-retrieve I’m 15 and I’m pretty sure.
Life is lame and I can’t go on
like this anymore. (Y)

Maybe (Y) i’d rather not say my name. is
this space confidential (Y)

S2S-generate I’m 15 and I’m pretty sure .
Life is lame and I can’t go on
like this anymore . (Y)

Okay, I will message you to-
morrow. Thank you (Y)

i’d rather not say my name. is
this space confidential (Y)

Ground truth Yea it’s awkward. Im 17, be
18 in 4mo (Y)

You too (Y) My name is Christopher and
I’m in Golden Gate Park. (Y)

Table 4: Examples of three counselor messages and the corresponding visitor response output from each method.
These examples are from the first research question, where only one preceding counselor message is considered.
Whether crowdworkers thought a response made sense or not is indicated parentheses as ”Y” and ”N”, respectively.

turned for them are shown in Table 4.

6.2 Extending Retrieval to Include
Additional Messages of Context

Providing crowdworkers with an additional mes-
sage of context appeared to impact their impres-
sion of whether responses made sense in context.
When presented with an additional message of
context, i.e, one visitor message and one coun-
selor message, crowdworkers found a larger per-
cent of the ground truth responses from the test
set to make sense (Table 5). In contrast, when
provided with an additional message of context to
evaluate a response, crowdworkers judged a lower
percent of responses returned by the ELMo-based
retrieval method to be coherent (61.40%, Table 5)
than when they were only presented with a single
message of context (65.88%, Table 2). Incorporat-
ing a previous message of context into a similarity
score increased the percent of coherent messages
returned, but by less than 1%. We only consid-

ered the ELMo embeddings, as they were found to
perform best in the first research question. Three
out of four retrieval methods returned a higher per-
cent of coherent messages than both generative
models, indicating that including more context for
generative models is challenging. Again using the
Seq2Seq likelihood to retrieve responses returned
the highest percent of messages that made sense.
However, these responses also had the fewest to-
kens, implying generic, short messages that might
score low on a qualitative scale of how engaging
an interactive system is.

7 Discussion

In contrast to many popular dialogue datasets (Ser-
ban et al., 2015), the transcripts we collected have
a relatively high number of turns (minimum 40
total turns per conversation), implying rich con-
versations. These conversations are also interest-
ing for their unique position of having distinct
roles for participants, a counselor and a distressed



8

Method Incorporation of addi-
tional context

Percent that
made sense

Avg. tokens
in response

Avg. tokens
in MS

re
tr

ie
va

l
ELMo – 61.40 14.6 13.6
ELMo-sum Measure similarity of sum

of embedded messages
51.78 15.6 15.2

ELMo-weight Weight similarities of pre-
vious messages

61.66 14.9 13.9

S2S-retrieve Concatenate context 65.48 5.5 4.6

ge
n. S2S-generate Concatenate context 58.89 8.3 7.3

VHCR-generate Models conversation 55.07 10.8 8.4
Ground truth – 91.30 14.6 14.7

Table 5: Performance of methods used to retrieve or generate responses when an additional message of context is
considered, i.e., two total messages. MS denotes only responses that were considered to make sense in context.
Both the best performing method and ground truth results are in bold.

youth, and related themes. We find retrieval to
be a competitive approach with generative mod-
els and return responses that make sense for more
than 60% of input messages. We also find themes
for how responses can seem to be coherent.

Giving crowdworkers an additional message of
context to judge whether a response was coher-
ent or not affected their decisions. It appeared
that ground truth responses were easier to distin-
guish as coherent and fewer retrieved messages
were judged as coherent if an additional message
of context was presented. This indicates the im-
portance of context, especially during evaluation.

The results we present are on a specific, data-
limited setting, but the implications of our results
may be broader both for other important applica-
tions, which commonly have data limitations, and
for retrieval baselines that are used to assess gen-
erative models. As embeddings have improved,
so too have retrieval baselines, which need to be
updated for appropriate evaluation of generative
models in any language generation setting.

Our results are not without limitations. The
data-limited setting presented a challenge to train-
ing generative models, and perhaps extensive
hyper-parameter tuning could influence results.
However, limited data and non-exhaustive param-
eter tuning are common limitations. Further, as
datasets increase in size, so does the potential for
relevant, related contexts to be present and thus
the potential for successful retrieval increases as
well. Thus, even on larger datasets, competitive
retrieval models, such as those we have presented,
should be considered for baseline comparisons.

Another limitation of our approach is the extent

to which we have considered context so far. Be-
cause the conversations we collected are long rel-
ative to some other datasets it is likely more con-
text will be necessary to produce a coherent sim-
ulation. We have begun to methodically look at
the effects of incrementally including more con-
text and extending retrieval models beyond a sin-
gle message. These initial steps indicate the im-
pact context has and provide important baselines
for comparing future, more general models.

8 Conclusion

Our work shows promise that data-limited applica-
tions may build initial systems with retrieval meth-
ods powered by recently developed embeddings.
By collecting role-play transcripts and showing re-
sults in a data-limited context, we have demon-
strated the potential to develop a successful sim-
ulation of a hotline visitor that novice counselors
can practice with during training. We found that
retrieval methods became more competitive with
improved embedding methods and surpassed gen-
erative methods when more context was consid-
ered. We also found that context had impact on
how difficult it was for crowdworkers to evaluate
responses.

As a next step, we plan to explore better leverag-
ing rich structure in the conversations, with a focus
on the protocol that the counselors are trained to
follow. There has been increased interest in blend-
ing retrieval and generation approaches by mod-
ifying prototypes retrieved from training data (Li
et al., 2018; Weston et al., 2018). It is possible
that such an approach would enable modifying and
thus tailoring responses to similar contexts.



9

References
Tim Althoff, Kevin Clark, and Jure Leskovec. 2016.

Large-scale Analysis of Counseling Conversations:
An Application of Natural Language Processing to
Mental Health. Transactions of the Association for
Computational Linguistics.

American Association of Suicidology. 2012. Organi-
zation Accreditation Standards Manual.

Allen D Andrade, Anita Bagri, Khin Zaw, Bernard A
Roos, and Jorge G Ruiz. 2010. Avatar-mediated
training in the delivery of bad news in a virtual
world. Journal of palliative medicine, 13(12):1415–
1419.

Ricardo Baeza-Yates, Berthier de Araújo Neto Ribeiro,
et al. 2011. Modern information retrieval. New
York: ACM Press; Harlow, England: Addison-
Wesley,.

Sairam Balani and Munmun De Choudhury. 2015. De-
tecting and characterizing mental health related self-
disclosure in social media. In Proceedings of the
33rd Annual ACM Conference Extended Abstracts
on Human Factors in Computing Systems, pages
1373–1378. ACM.

Rafael E Banchs and Haizhou Li. 2012. Iris: a chat-
oriented dialogue system based on the vector space
model. In Proceedings of the ACL 2012 System
Demonstrations, pages 37–42. Association for Com-
putational Linguistics.

Piotr Bojanowski, Edouard Grave, Armand Joulin,
and Tomas Mikolov. 2016. Enriching word vec-
tors with subword information. arXiv preprint
arXiv:1607.04606.

Jacob Cohen. 1968. Weighted kappa: Nominal scale
agreement provision for scaled disagreement or par-
tial credit. Psychological bulletin, 70(4):213.

Alexis Conneau, Douwe Kiela, Holger Schwenk, Loı̈c
Barrault, and Antoine Bordes. 2017. Supervised
learning of universal sentence representations from
natural language inference data. In Proceedings of
the 2017 Conference on Empirical Methods in Nat-
ural Language Processing.

Wendi F Cross, David Seaburn, Danette Gibbs, Karen
Schmeelk-Cone, Ann Marie White, and Eric D
Caine. 2011. Does practice make perfect? a ran-
domized control trial of behavioral rehearsal on sui-
cide prevention gatekeeper skills. The journal of
primary prevention, 32(3-4):195.

Munmun De Choudhury and Sushovan De. 2014.
Mental health discourse on reddit: Self-disclosure,
social support, and anonymity. In ICWSM.

Karthik Dinakar, Jackie Chen, Henry Lieberman, Ros-
alind Picard, and Robert Filbin. 2015. Mixed-
initiative real-time topic modeling & visualization

for crisis counseling. In Proceedings of the 20th in-
ternational conference on intelligent user interfaces,
pages 417–426. ACM.

Kathleen Kara Fitzpatrick, Alison Darcy, and Molly
Vierhile. 2017. Delivering cognitive behavior ther-
apy to young adults with symptoms of depression
and anxiety using a fully automated conversational
agent (woebot): a randomized controlled trial. JMIR
mental health, 4(2).

Matt Gardner, Joel Grus, Mark Neumann, Oyvind
Tafjord, Pradeep Dasigi, Nelson F. Liu, Matthew
Peters, Michael Schmitz, and Luke S. Zettlemoyer.
2018. AllenNLP: A deep semantic natural language
processing platform. In ACL workshop for NLP
Open Source Software.

Madelyn S Gould, Wendi Cross, Anthony R Pisani,
Jimmie Lou Munfakh, and Marjorie Kleinman.
2013. Impact of applied suicide intervention skills
training on the national suicide prevention lifeline.
Suicide and Life-Threatening Behavior, 43(6):676–
691.

Madelyn S Gould, Jimmie LH Munfakh, Marjorie
Kleinman, and Alison M Lake. 2012. National sui-
cide prevention lifeline: enhancing mental health
care for suicidal individuals and other people in
crisis. Suicide and Life-Threatening Behavior,
42(1):22–35.

M Sazzad Hussain, Juchen Li, Louise A Ellis, Laura
Ospina-Pinillos, Tracey A Davenport, Rafael A
Calvo, and Ian B Hickie. 2015. Moderator assistant:
A natural language generation-based intervention to
support mental health via social media. Journal of
Technology in Human Services, 33(4):304–329.

John Kalafat, Madelyn S Gould, Jimmie Lou Harris
Munfakh, and Marjorie Kleinman. 2007. An evalu-
ation of crisis hotline outcomes. part 1: Nonsuicidal
crisis callers. Suicide and Life-threatening behav-
ior, 37(3):322–337.

Guillaume Klein, Yoon Kim, Yuntian Deng, Jean
Senellart, and Alexander M. Rush. 2017. Open-
NMT: Open-source toolkit for neural machine trans-
lation. In Proceedings of ACL.

Ben Krause, Marco Damonte, Mihai Dobre, Daniel
Duma, Joachim Fainberg, Federico Fancellu, Em-
manuel Kahembwe, Jianpeng Cheng, and Bon-
nie Webber. 2017. Edina: Building an open do-
main socialbot with self-dialogues. arXiv preprint
arXiv:1709.09816.

Juncen Li, Robin Jia, He He, and Percy Liang. 2018.
Delete, retrieve, generate: a simple approach to sen-
timent and style transfer. In Proceedings of the 2018
Conference of the North American Chapter of the
Association for Computational Linguistics: Human
Language Technologies, pages 1865–1874.

https://www.suicidology.org/LinkClick.aspx?fileticket=Nb6eC6r2HX0%3D&portalid=14
https://www.suicidology.org/LinkClick.aspx?fileticket=Nb6eC6r2HX0%3D&portalid=14
https://doi.org/10.18653/v1/P17-4012
https://doi.org/10.18653/v1/P17-4012
https://doi.org/10.18653/v1/P17-4012


10

Chia-Wei Liu, Ryan Lowe, Iulian Serban, Mike Nose-
worthy, Laurent Charlin, and Joelle Pineau. 2016.
How not to evaluate your dialogue system: An em-
pirical study of unsupervised evaluation metrics for
dialogue response generation. In Proceedings of the
2016 Conference on Empirical Methods in Natural
Language Processing, pages 2122–2132. Associa-
tion for Computational Linguistics.

Ryan Lowe, Michael Noseworthy, Iulian Vlad Ser-
ban, Nicolas Angelard-Gontier, Yoshua Bengio, and
Joelle Pineau. 2017. Towards an automatic turing
test: Learning to evaluate dialogue responses. In
Proceedings of the 55th Annual Meeting of the Asso-
ciation for Computational Linguistics, pages 1116–
1126.

Yookoon Park, Jaemin Cho, and Gunhee Kim. 2018. A
hierarchical latent structure for variational conversa-
tion modeling. In Proceedings of the 2018 Confer-
ence of the North American Chapter of the Associ-
ation for Computational Linguistics: Human Lan-
guage Technologies, pages 1792–1801.

Amber Paukert, Brian Stagner, and Kerry Hope. 2004.
The assessment of active listening skills in helpline
volunteers. Stress, Trauma, and Crisis, 7(1):61–76.

Jeffrey Pennington, Richard Socher, and Christo-
pher D. Manning. 2014. Glove: Global vectors for
word representation. In Empirical Methods in Nat-
ural Language Processing (EMNLP), pages 1532–
1543.

Verónica Pérez-Rosas, Rada Mihalcea, Kenneth Resni-
cow, Satinder Singh, Lawrence Ann, Kathy J Gog-
gin, and Delwyn Catley. 2017. Predicting coun-
selor behaviors in motivational interviewing encoun-
ters. In Proceedings of the 15th Conference of the
European Chapter of the Association for Compu-
tational Linguistics: Volume 1, Long Papers, vol-
ume 1, pages 1128–1137.

Matthew E. Peters, Mark Neumann, Mohit Iyyer, Matt
Gardner, Christopher Clark, Kenton Lee, and Luke
Zettlemoyer. 2018. Deep contextualized word rep-
resentations. In Proc. of NAACL.

Rajeev Ramchand, Lisa Jaycox, Pat Ebener, Mary Lou
Gilbert, Dionne Barnes-Proby, and Prodyumna
Goutam. 2016. Characteristics and proximal out-
comes of calls made to suicide crisis hotlines in cal-
ifornia. Crisis.

Alan Ritter, Colin Cherry, and William B Dolan. 2011.
Data-driven response generation in social media. In
Proceedings of the conference on empirical methods
in natural language processing, pages 583–593. As-
sociation for Computational Linguistics.

Iulian Vlad Serban, Ryan Lowe, Peter Henderson, Lau-
rent Charlin, and Joelle Pineau. 2015. A survey of
available corpora for building data-driven dialogue
systems. arXiv preprint arXiv:1512.05742.

Aaron Smith and Dana Page. 2015. Us smartphone use
in 2015. Pew Research Center, 1.

Alessandro Sordoni, Michel Galley, Michael Auli,
Chris Brockett, Yangfeng Ji, Margaret Mitchell,
Jian-Yun Nie, Jianfeng Gao, and Bill Dolan. 2015.
A neural network approach to context-sensitive gen-
eration of conversational responses. In Proceed-
ings of the 2015 Conference of the North Ameri-
can Chapter of the Association for Computational
Linguistics: Human Language Technologies, pages
196–205.

Sandeep Subramanian, Adam Trischler, Yoshua Ben-
gio, and Christopher J Pal. 2018. Learning gen-
eral purpose distributed sentence representations via
large scale multi-task learning. In ICLR.

Suicide Prevention Resource Center. 2007. Applied
suicide intervention skills training (ASIST).

Ilya Sutskever, Oriol Vinyals, and Quoc V Le. 2014.
Sequence to sequence learning with neural net-
works. In Advances in neural information process-
ing systems, pages 3104–3112.

Oriol Vinyals and Quoc Le. 2015. A neural conversa-
tional model. arXiv preprint arXiv:1506.05869.

Ivan Vulić, Nikola Mrkšić, Roi Reichart, Diarmuid Ó
Séaghdha, Steve Young, and Anna Korhonen. 2017.
Morph-fitting: Fine-tuning word vector spaces with
simple language-specific rules. In Proceedings of
ACL, pages 56–68.

Zijian Wang and David Jurgens. 2018. Its going to be
okay: Measuring access to support in online com-
munities. In Proceedings of the 2018 Conference on
Empirical Methods in Natural Language Process-
ing, pages 33–45.

Joseph Weizenbaum. 1966. Elizaa computer program
for the study of natural language communication be-
tween man and machine. Communications of the
ACM, 9(1):36–45.

Jason Weston, Emily Dinan, and Alexander H Miller.
2018. Retrieve and refine: Improved sequence
generation models for dialogue. arXiv preprint
arXiv:1808.04776.

A Appendices

Modified model parameters are shared below for
reproducibility.

A.1 Seq2Seq Model Parameters
More information on model parameters can be
found in the OpenNMT-py online documentation2.

-dynamic dict on
2http://opennmt.net/OpenNMT-py/index.

html

http://www.aclweb.org/anthology/D14-1162
http://www.aclweb.org/anthology/D14-1162
http://opennmt.net/OpenNMT-py/index.html
http://opennmt.net/OpenNMT-py/index.html


11

-share vocab on

-src seq length = 200

-tgt seq length = 200

-rnn size = 500

-src word vec size = 300

-tgt word vec size = 300

-share embeddings on

-encoder type = brnn

-decoder type = rnn

-rnn type = LSTM

-layers = 2

-global attention = general

-optim = adam

-learning rate = 0.001

-batch size = 4

pre-trained embedding glove.840B.300d.txt

A.2 VHCR Model Parameters
More info can be found about model parameters in
the online repository3.

-model = VHCR

-batch size = 4

-embedding size = 300

-encoder hidden size = 500

-decoder hidden size = 500

-context size = 500

-z sent size = 50

-z conv size = 50

pre-trained embedding glove.840B.300d.txt

-max sentence length = 60

-max conversation length = 5

-min vocab frequency = 3

3https://github.com/ctr4si/
A-Hierarchical-Latent-Structure\
-for-Variational-Conversation-Modeling

https://github.com/ctr4si/A-Hierarchical-Latent-Structure\ -for-Variational-Conversation-Modeling
https://github.com/ctr4si/A-Hierarchical-Latent-Structure\ -for-Variational-Conversation-Modeling
https://github.com/ctr4si/A-Hierarchical-Latent-Structure\ -for-Variational-Conversation-Modeling

