



















































Large-Scale Categorization of Japanese Product Titles Using Neural Attention Models


Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics: Volume 2, Short Papers, pages 663–668,
Valencia, Spain, April 3-7, 2017. c©2017 Association for Computational Linguistics

Large-Scale Categorization of Japanese Product Titles Using Neural
Attention Models

Yandi Xia, Aaron Levine, Pradipto Das
Giuseppe Di Fabbrizio, Keiji Shinzato and Ankur Datta
Rakuten Institute of Technology, Boston, MA, 02110 - USA

{ts-yandi.xia, aaron.levine, pradipto.das,
giuseppe.difabbrizio, keiji.shinzato, ankur.datta}@rakuten.com

Abstract

We propose a variant of Convolutional
Neural Network (CNN) models, the Atten-
tion CNN (ACNN); for large-scale catego-
rization of millions of Japanese items into
thirty-five product categories. Compared
to a state-of-the-art Gradient Boosted Tree
(GBT) classifier, the proposed model re-
duces training time from three weeks to
three days while maintaining more than
96% accuracy. Additionally, our proposed
model characterizes products by imputing
attentive focus on word tokens in a lan-
guage agnostic way. The attention words
have been observed to be semantically
highly correlated with the predicted cate-
gories and give us a choice of automatic
feature extraction for downstream process-
ing.

1 Introduction

E-commerce sites provide product catalogs with
millions of items that are continuously updated by
thousands of merchants. To list new products in an
e-commerce marketplace and expose them to on-
line users, merchants must supply several pieces
of meta-data. Rakuten Ichiba1 is an example of
such a large-scale e-commerce platform in Japan,
hosting more than 239 million products from over
44, 000 merchants. To improve search relevance
and catalog navigation, products must be catego-
rized into a taxonomy tree with thousands of nodes
several levels deep (e.g., 6 levels with more than
43, 000 nodes for Rakuten Ichiba).

For such a large taxonomy, manual item catego-
rization is often inaccurate and inconsistent across
merchants. Automatic categorization into a full
taxonomy tree is feasible, although a layered ap-
proach is more practical for scalability and accu-

1Ichiba http://www.rakuten.co.jp

racy reasons. For instance, Shen et al. (2012b)
uses a two level strategy to combat imbalance. Das
et al. (2017) also exploits a similar 2-step cascade
categorization.

This work focuses on large-scale categorization
of Japanese products for the top-level categories of
the Rakuten Ichiba catalog taxonomy. Examples
of top-level product categories include Clothing,
Electronics, Shoes, and Books & Media, as well as
less represented categories such as Travel, Com-
munication, and Cars & Motorbikes. We com-
pare Convolutional Neural Network (CNN), At-
tention CNN (ACNN), and state-of-the-art Gra-
dient Boosted Tree (GBT) classification models
trained on more than 18 million catalog items.
ACNN model performance is comparable to that
of the GBT model with a 7-fold reduction in train-
ing time without the need for feature engineer-
ing. Additionally, ACNN’s attention mechanism
selects salient words that are semantically relevant
to identifying categories and potentially useful for
automatic language-agnostic feature extraction.

2 Related Work
Research on large-scale product categorization has
recently come into focus (Shen et al., 2011; Shen
et al., 2012b; Shen et al., 2012a; Yu et al.,
2013; Chen and Warren, 2013; Sun et al., 2014;
Kozareva, 2015). Most contemporary work in this
area points out the noise issues that arise in large
product datasets and address the problem with a
combination of a wide variety of features and stan-
dard classifiers. However, the existing methods
for noisy product classification have only been ap-
plied to English. Their efficacy for moraic and
agglutinative languages such as Japanese remains
unknown.

Application of deep learning techniques is gain-
ing grounds for text categorization applications
(Kim, 2014; Ma et al., 2015; Yang et al., 2016),
however, their application to product data has only
been recently reported. Pyo et al. (2016) uses Re-

663



current Neural Networks (RNNs) without word
embeddings. Furthermore, unlike our proposed
model, RNNs cannot impute tokens in title text
with attention weights that can be helpful in down-
stream applications.

Dependency-based deep learning (Ma et al.,
2015) has proven useful for sentence classifica-
tion, but product titles, whether in English or
Japanese, are not beholden to the same grammat-
ical rigor. We do not use deeper linguistic tech-
niques such as parsing or Part-of-Speech tagging
due to the language-agnostic nature of our catego-
rization techniques. Attention-based deep learning
models have been used in the image domain (Xu
et al., 2015) and in the generic text classification
domain (Yang et al., 2016). However, to the best
of our knowledge, this is the first work on simul-
taneous categorization and attention based salient
token selection on Japanese product data.

3 Dataset Characteristics

The data we use is a selection of product list-
ings from Rakuten Ichiba, a large Japanese E-
commerce service for thousands of merchants.
Each merchant submits their own product data,
leading to item names with inconsistent formats
and disagreements on genres for the same sets of
items. Our training set consists of 18, 199, 420
listings and the test set of 2, 274, 928 listings, for
a 90/10% split. The training data is uniformly
sampled before the split. Due to the popularity
of certain product types, the balance is unevenly
distributed between 35 top-level categories: There
are 1, 869, 471 in the largest category, but only 925
in the smallest.

Statistics Training set Test set

Mean character count/title 62.510 62.506
Standard deviation 31.496 31.492

Mean word count/title 23.187 23.188
Standard deviation 11.945 11.942

Mean character count/word 05.800 05.799

Table 1: Word and character level statistics for our
Rakuten Ichiba dataset.

Table 1 shows character and word level statis-
tics per product title in the training and test set.
It is evident from the mean word and character
counts that, on average, Japanese product titles in
our dataset are quite verbose. We thus expect that
convolutional neural network based models that
rely on full context of the input text, to work better
for the categorization task.

4 Modeling Approaches
4.1 Attention Neural Networks
Our ACNN model is related to the work described
in Yang et al. (2016), which has been more suitable
for well-formed document classification tasks with
a limited number of categories.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

+

!" !# !$ !%

&'

(" ($ (%(#

)

Convolutional
Embeddings

*" *# *$ *%

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

+

&+

(′" (′$ (′%(′#

Fully connected network

Softmax

&'-

!" !# !$ !%

.

Figure 1: Attentional CNN model architecture

The gating mechanisms shown in the left mod-
ule of Fig. 1 are akin to the hierarchical model in
(Yang et al., 2016). However, their model ends at
that module, at which point it is connected to the
softmax output layer. In our model, the left mod-
ule acts as a context encoder and the right module
acts as an attention mechanism that is dependent
on the encoded context and input.

Generally speaking, the local convolutional op-
erations are unaware of the existence of preced-
ing or succeeding convolutions over the text se-
quence. The context module enables propagation
of stronger shared parameters through a context
embedding, leading to the higher weighting of at-
tention over specific parts of the inputs. The prop-
agation strength in the network builds up based
on the pattern of context present in the input se-
quences across several training examples and the
training loss incurred for the encoding.

The filters of the convolutional layer (LeCun
and Bengio, 1995; Kim, 2014) are convolved with
with a window of consecutive observations (char-
acters or words), and produce an encoding of the
window. In Fig. 1, ei is the encoding of ith

window, where, each window is defined over a
word or character in the input sequence. For
our ACNN model, the input sequence is treated

664



as a sequence of words and then as a separate
sequence of characters with the two distinct se-
quences being concatenated as a single input se-
quence, {v0, v1, ...vL}. The value of L is three
in Fig. 1. Each variable, ui, encodes the non-
linearity of the linear manifold on ei over a set
of shared parameters, WU, and biases, bU, as
ui = tanh

(
WTUei + bU

)
. The variables ei actu-

ally correspond to parts of the data and ui help ag-
gregate the values of ei projected along the learned
directions in the parameter space for (WU,bU).
Each value of ui is computed independent of uj 6=i.
The shared parameter WU is a D × F matrix,
where D is a hyper parameter chosen for the at-
tention mechanism and F is the number of convo-
lution filters, both chosen during cross-validation.
The variables ei and bU are F dimensional vec-
tors.

The inputs to the context encoding vector, rep-
resented by the variable c, are local softmax func-
tions of the form:

αi =
exp(uTi wu)∑
j exp(uTj wu)

(1)

The encoded context vector c is then simply
c =

∑
i αiei. Obtaining the input encoding for

the attention module is similar to context encoding
except that u′i depends on a separate set of shared
parameters, WU′ as well as Wc, for the context
and corresponding bias term b. In this case, we
have:

u′i = tanh
(
WTU′ei + W

T
c c + b

)
(2)

The softmax functions α′i are similarly defined
as in Equ. 1, but w.r.t. u′i and wu′ . The α′is can
be thought as the maximum of the relevance of the
variables u′i, according to the context c. The out-
put, z, from the attention module is the weighted
arithmetic mean,

∑
i α
′
iei, where the weight rep-

resent the relevance for each input variable vi,
through ei, according to the context c.

We use windows over both word and character
observation embeddings of the input text (either
tokens or single Japanese characters). We concate-
nate the word and character encoding vectors and
input it to a fully connected layer. A cross-entropy
loss is imposed at the output layer.

4.2 Gradient Boosted Trees
GBTs (Friedman, 2000) optimize a loss func-
tional: L = Ey[L(y, F (x)|X)] where F (x) can
be a mathematically difficult to characterize func-
tion, e.g., a decision tree f(x). The optimal

value of the function is expressed as F ?(x) =∑M
m=0 fm(x,a,w), where f0(x,a,w) is the ini-

tial guess and {fm(x,a,w)}Mm=1 are additive
boosts on x defined by the optimization method.
The parameter am of fm(x,a,w) denotes split
points of predictor variables and wm denotes the
boosting weights on the leaf nodes of the decision
trees corresponding to the partitioned training set
Xj for region j.

Each boosting round m updates the weights
wm,j on the leaves and creates a new tree. The
optimal selection of decision tree parameters is
based on optimizing the fm(x,a,w) using a lo-
gistic loss.

5 Experimental Setup and Results

5.1 Data Preprocessing

Tokenization of Japanese product titles is done us-
ing MeCab2. The tokenizer is trained using fea-
tures that are augmented with in-house product
keyword dictionaries. Romaji words written us-
ing Latin characters are separated from Kanji and
Kana words. All brackets are normalized to square
brackets and punctuations from non-numeric to-
kens are removed. We remove anything outside
of standard Japanese UTF-8 character ranges. Fi-
nally, canonical normalization changes the code
points of the resulting Japanese text into an NFKC
normalized3 form.

For GBT, we use several features – at the to-
kenized word level, we use counts of word uni-
grams and word bi-grams. For character features,
the product title is first normalized as discussed
above. Character 2, 3, and 4-grams are then ex-
tracted with their counts, where extractions in-
clude single spaces appearing at the end of word
boundaries. Feature engineering for GBT uses
cross-validation to identify the best set of feature
combinations and is thus time consuming.

The embedding representation of words and
characters for the CNN-based classifiers is per-
formed over the normalized input on which fea-
ture extraction for GBT is done. To reduce GPU
memory consumption, the CNN-based models are
trained on titles from which words and characters
that appear in less than 20 titles in the training set
are removed. Such rare token removal is not per-
formed on the training data for the GBT models
since they are trained on CPU servers.

2
https://sourceforge.net/projects/mecab/

3
http://unicode.org/reports/tr15/

665



5.2 Classifier Comparison
In this section, we compare categorization perfor-
mance of a baseline CNN model w.r.t. our pro-
posed model and a state-of-the-art GBT classifier.
We use 10-fold cross-validation over 90% of the
training data to perform parameter tuning.

ACNN model parameter setup - The words
and characters are in an embedding vector space
of dimension 300. These embeddings are trained
on the product title training corpus. We use four
different window sizes 1, 3, 4, 5 for words and an-
other of size 4 for characters. The dimension of
the filter encoders ei and e′i is 250, which is the
same as the number of filters. The hidden layer
size is the number of window sizes times the num-
ber of filters i.e., 1, 250 and we also use a dropout
with 0.5 probability on the hidden layer. The CNN
models are run for a maximum of three days on a
server with 8 Nvidia TitanX GPUs and the best
model corresponding to the iteration for the low-
est validation error is used for test set evaluation.

GBT model parameter setup - For each cate-
gory, the boosted stumps for the GBT (Chen and
Guestrin, 2016) models are allowed to grow up
to a maximum depth of 500. The initial learn-
ing rate is assigned a value of 0.05 and the num-
ber of boosting rounds is set to 50. For leaf node
weights, we use L2 regularization with a regu-
larization constant of 0.5. The GBT models are
trained on a 64-core CPU server.

Models Micro-F1 Training Time

GBT 96.23 3 weeks
CNN 95.90 3 days
ACNN-word 96.00 3 days
ACNN-word-character 96.27 3 days

Table 2: Micro-F1 measures for evaluated mod-
els. The Micro-precision scores (not shown here)
are very similar to the micro-F1 scores with occa-
sional differences in the third and fourth decimal
places.

Table 2 shows that our proposed ACNN model
– the CNN model augmented with word and char-
acter based attention mechanisms, improves over
the baseline CNN model by an absolute 0.37%,
which translates to more than 8, 000 test titles be-
ing correctly classified additionally. Although, the
improvement of the proposed model is not signifi-
cant when using a stringent p-value of 0.0001 (i.e.,
a typical value used in industrial setting), we em-
phasize that in practice any increase in accuracy

helps (e.g., an additional million items when con-
sidering the whole Ichiba catalog).

Both GBT and our proposed ACNN model per-
form well for top level categorization of Japanese
product titles. However, do the models make sim-
ilar mistakes on the test set?

To this end, we computed the ratio of the sum of
the number of listings in the test set per category
for which both GBT and ACNN mis-classify but
agree on the wrong predicted category, to the to-
tal number of mis-classifications from ACNN. The
upper bound of this ratio is 1.0, which means that
ACNN would make the same mistakes as GBT
would. However, from our experiments, the ratio
turned out to be 0.37, which means that GBT and
ACNN make different mistakes more than 60% of
the time. The relatively low value of the ratio indi-
cates that we can gain major benefits for the final
top level categorization by using an ensemble of
GBT and ACNN models. The ACNN model does
worse than GBT on 17 categories with a mean er-
ror difference, µ, of 0.78 and standard deviation,
σ, of 1.15 and it does better than GBT on the rest
of the 18 categories with µ = 0.39 and σ = 0.41.

Statistics from test set 8000 titles 18 categories

Mean word count/title 20.930 20.120
Mean character count/word 09.245 05.815
Mean rare word count/title 00.158 00.354

Table 3: Word and character level statistics for:
1) The 8000 titles in the test set, for which ACNN
predicts correctly over CNN (Middle column);
and 2) The 18 categories in the test set for which
ACNN performs better than GBT (Rightmost col-
umn).

Table 3 sheds some insights on why the ACNN
model may be doing better over the CNN model,
for the 8000 titles in the test set. We compare
the average number of characters in the words of
the 8000 titles in the test set for which our ACNN
model provides correct predictions over the CNN
model, to that for the overall test set from Table 1.
The count for the former case turns out to be 9.245
that is substantially higher than that for the latter
case, which is 5.799. It is thus highly likely that
the ACNN model is performing better than CNN
by leveraging the longer word and character con-
texts for these 8000 titles.

On the other hand, removal of the rare tokens
(words appearing in less than 20 titles) seem to

666



Reference	category Predicted	category Tokens	
1 ml
Japanese	Sake	&	Shochu Japanese	Sake	&	Shochu Anno potato shochu Mujinzo Anno ml

2 TOEIC TOEIC
Learning,	Service	&	Insurance Book,	magazine	&	comics Shiho	Hayashi editor TOEIC test preperation Hayashi method first TOEIC test speed english study guide

3 1000
Travel	&	tickets Toys,	hobbies	&	games Kan	Otake 1000 th strike-out achievement commemoration ball Yomiuri Giants Yomiuri Giants club

4 CD DVD gz suzuki gz custom ukawa
Car	&	moter	bikes CD,	DVD	&	musical	instruments Price	drop used import Suzuki gz Custom Suzuki gz Custom ukawa

Anno	potato	shochu	Mujinzo	Anno	ml	[Anno	is	a	region	that	grows	potato]

Editor	Shiho	Hayashi	TOEIC	test	Hayashi	method	preperation	for	first	TOEIC	test	speed	english	study	guide

Kan	Otake	1000th	strike	out	commemoration	ball	Yomiuri	Giants	Yomiuri	Giants	club

Price	drop	Used	import	Suzuki	GZ	custom	Suzuki	GZ	custom	ukawa

Manual	translation	of	the	Japanese	product	title	into	English:

Manual	translation	of	the	Japanese	product	title	into	English:

Manual	translation	of	the	Japanese	product	title	into	English:

Manual	translation	of	the	Japanese	product	title	into	English:

Figure 2: Examples of attention tokens for correct and incorrect classifications with English translations
for tokens, product titles, and categories. Gradient colors are coded by attention model weights. Darker
shades of blue have higher attention.

have negligible effect on the context of the titles
from the subset of 8000 titles. However, the effect
is a little more pronounced for the context of the ti-
tles from the subset of the 18 categories for which
ACNN does better than GBT, but, with a mean er-
ror difference of only half of that for the other 17
categories on which it does worse.

5.3 Paying Attention Pays Off!
One of the most important aspects of the ACNN
model is the ability to highlight words and char-
acters in sequential text tokens automatically
through the attention mechanism. Examples of
such selected word tokens from test titles can be
observed in Fig. 2.

In order to visualize the importance of the words
related to the categorization label contribution, we
use the attention vectors (e.g., α′ scores) gener-
ated by the model. The word attention scores ac-
curately localize words that are closely related to
the classification labels. For instance, in Figure
2, line 1, the first word highlighted in the product
description (higher score) is potato, which is one
of the main ingredients in the Japanese alcoholic
beverage, Shōchū (焼酎), that is referred to in the
product title.

For the second example, there is ambiguity be-
tween the reference and the predicted category
since the product title can be applied to both. In
this case, the attention model is highlighting words
like editor, English, and guide that may apply to
both Learning services and Books.

The third example in Fig. 2 is an annotation
mistake that was correctly captured by the model.
Here the attention model is extracting the salient
words Giants, strike-out, and Kan Otake, which
are related to the predicted category.

Finally, in the fourth example, the attention

mechanism assigns high scores to the words price
drop, import, and Suzuki where Suzuki is a popular
car manufacturer and music curriculum in Japan.
“Suzuki” is thus inherently ambiguous and our
model fails to put attention on context clues like
the token “gz”, which is a motorbike model.

6 Concluding Remarks
We propose a variant of the popular CNN model,
the Attention CNN (ACNN) model, for the task of
large-scale categorization of millions of Japanese
product titles into thirty-five top level categories.
The proposed model can leverage GPUs to reduce
training time from three weeks for a state-of-the-
art GBT classifier to three days while maintaining
more than 96% accuracy.

Our language agnostic attention model can
highlight salient tokens, which are semantically
highly correlated to predicted categories. This
helps in dimensionality reduction without the
need for feature engineering.

As future work, we will experiment with ensem-
ble methods to exploit differences in prediction er-
rors from the different models, thereby improving
overall classification performance.

References

Tianqi Chen and Carlos Guestrin. 2016. Xgboost: A
scalable tree boosting system. In Proceedings of the
22nd ACM SIGKDD International Conference on
Knowledge Discovery and Data Mining, San Fran-
cisco, CA, USA, August 13-17, 2016, pages 785–
794.

Jianfu Chen and David Warren. 2013. Cost-sensitive
learning for large-scale hierarchical classification.
In Proceedings of the 22Nd ACM International Con-
ference on Information & Knowledge Management,
CIKM ’13, pages 1351–1360.

667



Pradipto Das, Yandi Xia, Aaron Levine, Giuseppe Di
Fabbrizio, and Ankur Datta. 2017. Web-scale
language-independent cataloging of noisy product
listings for e-commerce. In Proceedings of the 15th
Conference of the European Chapter of the Asso-
ciation for Computational Linguistics, EACL 2017,
April 3-7, 2017, Valencia, Spain.

Jerome H. Friedman. 2000. Greedy function approx-
imation: A gradient boosting machine. Annals of
Statistics, 29:1189–1232.

Yoon Kim. 2014. Convolutional neural networks
for sentence classification. In Proceedings of the
2014 Conference on Empirical Methods in Natu-
ral Language Processing (EMNLP), pages 1746–
1751, Doha, Qatar, October. Association for Com-
putational Linguistics.

Zornitsa Kozareva. 2015. Everyone likes shopping!
multi-class product categorization for e-commerce.
In NAACL HLT 2015, The 2015 Conference of
the North American Chapter of the Association for
Computational Linguistics: Human Language Tech-
nologies, Denver, Colorado, USA, May 31 - June 5,
2015, pages 1329–1333.

Y. LeCun and Y. Bengio. 1995. Convolutional net-
works for images, speech, and time-series. In M. A.
Arbib, editor, The Handbook of Brain Theory and
Neural Networks. MIT Press.

Mingbo Ma, Liang Huang, Bing Xiang, and Bowen
Zhou. 2015. Dependency-based convolutional neu-
ral networks for sentence embedding. In Proceed-
ings of the 53st Annual Meeting of the Association
for Computational Linguistics (ACL), pages 174–
179, Beijing, China, July. Association for Compu-
tational Linguistics.

Hyuna Pyo, Jung-Woo Ha, and Jeonghee Kim. 2016.
Large-scale item categorization in e-commerce us-
ing multiple recurrent neural networks. In Proceed-
ings of the 22nd ACM SIGKDD International Con-
ference on Knowledge Discovery and Data Mining,
KDD ’16, New York, NY, USA. ACM.

Dan Shen, Jean David Ruvini, Manas Somaiya, and
Neel Sundaresan. 2011. Item categorization in the
e-commerce domain. In Proceedings of the 20th
ACM International Conference on Information and
Knowledge Management, CIKM ’11, pages 1921–
1924, New York, NY, USA. ACM.

Dan Shen, Jean-David Ruvini, Rajyashree Mukherjee,
and Neel Sundaresan. 2012a. A study of smoothing
algorithms for item categorization on e-commerce
sites. Neurocomput., 92:54–60, September.

Dan Shen, Jean-David Ruvini, and Badrul Sarwar.
2012b. Large-scale item categorization for e-
commerce. In Proceedings of the 21st ACM In-
ternational Conference on Information and Knowl-
edge Management, CIKM ’12, pages 595–604, New
York, NY, USA. ACM.

Chong Sun, Narasimhan Rampalli, Frank Yang, and
AnHai Doan. 2014. Chimera: Large-scale classi-
fication using machine learning, rules, and crowd-
sourcing. Proc. VLDB Endow., 7(13), August.

Kelvin Xu, Jimmy Ba, Ryan Kiros, Kyunghyun Cho,
Aaron Courville, Ruslan Salakhudinov, Rich Zemel,
and Yoshua Bengio. 2015. Show, attend and tell:
Neural image caption generation with visual atten-
tion. In David Blei and Francis Bach, editors, Pro-
ceedings of the 32nd International Conference on
Machine Learning (ICML-15), pages 2048–2057.
JMLR Workshop and Conference Proceedings.

Zichao Yang, Diyi Yang, Chris Dyer, Xiaodong He,
Alexander J. Smola, and Eduard H. Hovy. 2016.
Hierarchical attention networks for document clas-
sification. In HLT-NAACL.

Hsiang-Fu Yu, Chia-Hua Ho, Yu-Chin Juan, and Chih-
Jen Lin. 2013. LibShortText: A Library for Short-
text Classication and Analysis. Technical report,
Department of Computer Science, National Taiwan
University, Taipei 106, Taiwan.

668


