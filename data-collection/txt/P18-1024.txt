



















































LinkNBed: Multi-Graph Representation Learning with Entity Linkage


Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers), pages 252–262
Melbourne, Australia, July 15 - 20, 2018. c©2018 Association for Computational Linguistics

252

LinkNBed: Multi-Graph Representation Learning with Entity Linkage

Rakshit Trivedi ∗
College of Computing

Georgia Tech

Christos Faloutsos
SCS, CMU

and Amazon.com

Bunyamin Sisman
Amazon.com

Hongyuan Zha
College of Computing

Georgia Tech

Jun Ma
Amazon.com

Xin Luna Dong
Amazon.com

Abstract

Knowledge graphs have emerged as an im-
portant model for studying complex multi-
relational data. This has given rise to
the construction of numerous large scale
but incomplete knowledge graphs encod-
ing information extracted from various re-
sources. An effective and scalable ap-
proach to jointly learn over multiple graphs
and eventually construct a unified graph
is a crucial next step for the success of
knowledge-based inference for many down-
stream applications. To this end, we pro-
pose LinkNBed, a deep relational learning
framework that learns entity and relation-
ship representations across multiple graphs.
We identify entity linkage across graphs as
a vital component to achieve our goal. We
design a novel objective that leverage en-
tity linkage and build an efficient multi-task
training procedure. Experiments on link
prediction and entity linkage demonstrate
substantial improvements over the state-of-
the-art relational learning approaches.

1 Introduction

Reasoning over multi-relational data is a key con-
cept in Artificial Intelligence and knowledge graphs
have appeared at the forefront as an effective tool
to model such multi-relational data. Knowledge
graphs have found increasing importance due to
its wider range of important applications such as
information retrieval (Dalton et al., 2014), natural
language processing (Gabrilovich and Markovitch,
2009), recommender systems (Catherine and Co-
hen, 2016), question-answering (Cui et al., 2017)

∗Correspondence: rstrivedi@gatech.edu and
bunyamis@amazon.com. Work done when Rakshit
Trivedi interned at Amazon.

and many more. This has led to the increased ef-
forts in constructing numerous large-scale Knowl-
edge Bases (e.g. Freebase (Bollacker et al., 2008),
DBpedia (Auer et al., 2007), Google’s Knowledge
graph (Dong et al., 2014), Yago (Suchanek et al.,
2007) and NELL (Carlson et al., 2010)), that can
cater to these applications, by representing infor-
mation available on the web in relational format.

All knowledge graphs share common drawback
of incompleteness and sparsity and hence most ex-
isting relational learning techniques focus on using
observed triplets in an incomplete graph to infer
unobserved triplets for that graph (Nickel et al.,
2016a). Neural embedding techniques that learn
vector space representations of entities and relation-
ships have achieved remarkable success in this task.
However, these techniques only focus on learning
from a single graph. In addition to incompleteness
property, these knowledge graphs also share a set of
overlapping entities and relationships with varying
information about them. This makes a compelling
case to design a technique that can learn over mul-
tiple graphs and eventually aid in constructing a
unified giant graph out of them. While research on
learning representations over single graph has pro-
gressed rapidly in recent years (Nickel et al., 2011;
Dong et al., 2014; Trouillon et al., 2016; Bordes
et al., 2013; Xiao et al., 2016; Yang et al., 2015),
there is a conspicuous lack of principled approach
to tackle the unique challenges involved in learning
across multiple graphs.

One approach to multi-graph representation
learning could be to first solve graph alignment
problem to merge the graphs and then use exist-
ing relational learning methods on merged graph.
Unfortunately, graph alignment is an important but
still unsolved problem and there exist several tech-
niques addressing its challenges (Liu and Yang,
2016; Pershina et al., 2015; Koutra et al., 2013;
Buneman and Staworko, 2016) in limited settings.



253

The key challenges for the graph alignment prob-
lem emanate from the fact that the real world data
are noisy and intricate in nature. The noisy or
sparse data make it difficult to learn robust align-
ment features, and data abundance leads to com-
putational challenges due to the combinatorial per-
mutations needed for alignment. These challenges
are compounded in multi-relational settings due to
heterogeneous nodes and edges in such graphs.

Recently, deep learning has shown significant
impact in learning useful information over noisy,
large-scale and heterogeneous graph data (Rossi
et al., 2017). We, therefore, posit that combin-
ing graph alignment task with deep representation
learning across multi-relational graphs has poten-
tial to induce a synergistic effect on both tasks.
Specifically, we identify that a key component
of graph alignment process—entity linkage—also
plays a vital role in learning across graphs. For
instance, the embeddings learned over two knowl-
edge graphs for an actor should be closer to one
another compared to the embeddings of all the
other entities. Similarly, the entities that are already
aligned together across the two graphs should pro-
duce better embeddings due to the shared context
and data. To model this phenomenon, we propose
LinkNBed, a novel deep learning framework that
jointly performs representation learning and graph
linkage task. To achieve this, we identify key chal-
lenges involved in the learning process and make
the following contributions to address them:

• We propose novel and principled approach to-
wards jointly learning entity representations
and entity linkage. The novelty of our frame-
work stems from its ability to support linkage
task across heterogeneous types of entities.

• We devise a graph-independent inductive
framework that learns functions to capture
contextual information for entities and rela-
tions. It combines the structural and semantic
information in individual graphs for joint in-
ference in a principled manner.

• Labeled instances (specifically positive in-
stances for linkage task) are typically very
sparse and hence we design a novel multi-task
loss function where entity linkage task is tack-
led in robust manner across various learning
scenarios such as learning only with unlabeled
instances or only with negative instances.

• We design an efficient training procedure to
perform joint training in linear time in the
number of triples. We demonstrate superior
performance of our method on two datasets cu-
rated from Freebase and IMDB against state-
of-the-art neural embedding methods.

2 Preliminaries

2.1 Knowledge Graph Representation
A knowledge graph G comprises of set of facts
represented as triplets (es, r, eo) denoting the re-
lationship r between subject entity es and object
entity eo. Associated to this knowledge graph, we
have a set of attributes that describe observed char-
acteristics of an entity. Attributes are represented
as set of key-value pairs for each entity and an at-
tribute can have null (missing) value for an entity.
We follow Open World Assumption - triplets not
observed in knowledge graph are considered to be
missing but not false. We assume that there are no
duplicate triplets or self-loops.

2.2 Multi-Graph Relational Learning
Definition. Given a collection of knowledge
graphs G, Multi-Graph Relational Learning refers
to the the task of learning information rich represen-
tations of entities and relationships across graphs.
The learned embeddings can further be used to in-
fer new knowledge in the form of link prediction or
learn new labels in the form of entity linkage. We
motivate our work with the setting of two knowl-
edge graphs where given two graphs G1, G2 ∈ G,
the task is to match an entity eG1 ∈ G1 to an entity
eG2 ∈ G2 if they represent the same real-world
entity. We discuss a straightforward extension of
this setting to more than two graphs in Section 7.

Notations. Let X and Y represent realization of
two such knowledge graphs extracted from two dif-
ferent sources. Let nXe and n

Y
e represent number

of entities in X and Y respectively. Similarly, nXr
and nYr represent number of relations in X and Y .
We combine triplets from both X and Y to obtain
set of all observed triplets D = {(es, r, eo)p}Pp=1
where P is total number of available records across
from both graphs. Let E andR be the set of all enti-
ties and all relations in D respectively. Let |E| = n
and |R| = m. In addition to D, we also have set
of linkage labels L for entities between X and Y .
Each record in L is represented as triplet (eX ∈ X ,
eY ∈ Y , l ∈ {0, 1}) where l = 1 when the entities
are matched and l = 0 otherwise.



254

3 Proposed Method: LinkNBed

We present a novel inductive multi-graph relational
learning framework that learns a set of aggregator
functions capable of ingesting various contextual
information for both entities and relationships in
multi-relational graph. These functions encode the
ingested structural and semantic information into
low-dimensional entity and relation embeddings.
Further, we use these representations to learn a
relational score function that computes how two
entities are likely to be connected in a particular
relationship. The key idea behind this formulation
is that when a triplet is observed, the relationship
between the two entities can be explained using
various contextual information such as local neigh-
borhood features of both entities, attribute features
of both entities and type information of the entities
which participate in that relationship.

We outline two key insights for establishing the
relationships between embeddings of the entities
over multiple graphs in our framework:
Insight 1 (Embedding Similarity): If the two en-
tities eX ∈ X and eY ∈ Y represent the same
real-world entity then their embeddings eX and eY

will be close to each other.
Insight 2 (Semantic Replacement): For a given
triplet t = (es, r, eo) ∈ X , denote g(t) as the func-
tion that computes a relational score for t using
entity and relation embeddings. If there exists
a matching entity es

′ ∈ Y for es ∈ X , denote
t′ = (es

′
, r, eo) obtained after replacing es with es

′
.

In this case, g(t) ∼ g(t′) i.e. score of triplets t and
t′ will be similar.

For a triplet (es, r, eo) ∈ D, we describe en-
coding mechanism of LinkNBed as three-layered
architecture that computes the final output represen-
tations of zr, ze

s
, ze

o
for the given triplet. Figure 1

provides an overview of LinkNBed architecture
and we describe the three steps below:

3.1 Atomic Layer
Entities, Relations, Types and Attributes are first
encoded in its basic vector representations. We use
these basic representations to derive more complex
contextual embeddings further.
Entities, Relations and Types. The embedding
vectors corresponding to these three components
are learned as follows:

ve
s
= f(WEes) ve

o
= f(WEeo) (1)

vr = f(WRr) vt = f(WTt) (2)

where ve
s
,ve

o ∈ Rd. es, eo ∈ Rn are “one-hot”
representations of es and eo respectively. vr ∈
Rk and r ∈ Rm is “one-hot” representation of r.
vt ∈ Rq and t ∈ Rz is ”one-hot” representation
of t . WE ∈ Rd×n, WR ∈ Rk×m and WT ∈
Rq×z are the entity, relation and type embedding
matrices respectively. f is a nonlinear activation
function (Relu in our case). WE, WR and WT

can be initialized randomly or using pre-trained
word embeddings or vector compositions based on
name phrases of components (Socher et al., 2013).
Attributes. For a given attribute a represented
as key-value pair, we use paragraph2vec (Le and
Mikolov, 2014) type of embedding network to learn
attribute embedding. Specifically, we represent
attribute embedding vector as:

a = f(Wkeyakey + W
valaval) (3)

where a ∈ Ry, akey ∈ Ru and aval ∈ Rv.
Wkey ∈ Ry×u and Wval ∈ Ry×v. akey will be
“one-hot” vector and aval will be feature vector.
Note that the dimensions of the embedding vectors
do not necessarily need to be the same.

3.2 Contextual Layer

While the entity and relationship embeddings de-
scribed above help to capture very generic latent
features, embeddings can be further enriched to
capture structural information, attribute informa-
tion and type information to better explain the exis-
tence of a fact. Such information can be modeled
as context of nodes and edges in the graph. To this
end, we design the following canonical aggregator
function that learns various contextual information
by aggregating over relevant embedding vectors:

c(z) = AGG({z′,∀z′ ∈ C(z)}) (4)

where c(z) is the vector representation of the ag-
gregated contextual information for component z.
Here, component z can be either an entity or a rela-
tion. C(z) is the set of components in the context
of z and z′ correspond to the vector embeddings
of those components. AGG is the aggregator func-
tion which can take many forms such Mean, Max,
Pooling or more complex LSTM based aggrega-
tors. It is plausible that different components in
a context may have varied impact on the compo-
nent for which the embedding is being learned.
To account for this, we employ a soft attention
mechanism where we learn attention coefficients



255

Figure 1: LinkNBed Architecture Overview - one step score computation for a given triplet (es, r, eo).
The Attribute embeddings are not simple lookups but they are learned as shown in Eq 3

to weight components based on their impact before
aggregating them. We modify Eq. 4 as:

c(z) = AGG(q(z) ∗ {z′, ∀z′ ∈ C(z)}) (5)

where

q(z) =
exp(θz)∑

z′∈C(z)
exp(θz′)

(6)

and θz’s are the parameters of attention model.
Following contextual information is modeled in
our framework:

Entity Neighborhood Context Nc(e) ∈ Rd.
Given a triplet (es, r, eo), the neighborhood
context for an entity es will be the nodes located
near es other than the node eo. This will capture
the effect of local neighborhood in the graph
surrounding es that drives es to participate in fact
(es, r, eo). We use Mean as aggregator function.
As there can be large number of neighbors, we
collect the neighborhood set for each entity as a
pre-processing step using a random walk method.
Specifically, given a node e, we run k rounds of
random-walks of length l following (Hamilton
et al., 2017) and create set N (e) by adding all
unique nodes visited across these walks. This
context can be similarly computed for object entity.

Entity Attribute Context Ac(e) ∈ Ry. For
an entity e, we collect all attribute embeddings
for e obtained from Atomic Layer and learn
aggregated information over them using Max
operator given in Eq. 4.

Relation Type Context Tc(r) ∈ Rq. We use type
context for relation embedding i.e. for a given
relationship r, this context aims at capturing the
effect of type of entities that have participated
in this relationship. For a given triplet (es, r, eo),
type context for relationship r is computed by
aggregation with mean over type embeddings
corresponding to the context of r. Appendix C
provides specific forms of contextual information.

3.3 Representation Layer

Having computed the atomic and contextual em-
beddings for a triplet (es, r, eo), we obtain the final
embedded representations of entities and relation
in the triplet using the following formulation:

ze
s
= σ( W1v

es︸ ︷︷ ︸
Subject Entity Embedding

+ W2Nc(e
s)︸ ︷︷ ︸

Neighborhood Context

+ W3Ac(e
s))︸ ︷︷ ︸

Subject Entity Attributes

(7)

ze
o
= σ( W1v

eo︸ ︷︷ ︸
Object Entity Embedding

+ W2Nc(e
o)︸ ︷︷ ︸

Neighborhood Context

+ W3Ac(e
o))︸ ︷︷ ︸

Object Entity Attributes

(8)

zr = σ( W4v
r︸ ︷︷ ︸

Relation Embedding

+ W5Tc(r))︸ ︷︷ ︸
Entity Type Context

(9)

where W1,W2 ∈ Rd×d, W3 ∈ Rd×y, W4 ∈
Rd×k and W5 ∈ Rd×q. σ is nonlinear activation
function – generally Tanh or Relu.



256

Following is the rationale for our formulation: An
entity’s representation can be enriched by encoding
information about the local neighborhood features
and attribute information associated with the en-
tity in addition to its own latent features. Parame-
ters W1,W2,W3 learn to capture these different
aspects and map them into the entity embedding
space. Similarly, a relation’s representation can
be enriched by encoding information about entity
types that participate in that relationship in addi-
tion to its own latent features. Parameters W4,W5
learn to capture these aspects and map them into
the relation embedding space. Further, as the ulti-
mate goal is to jointly learn over multiple graphs,
shared parameterization in our model facilitate the
propagation of information across graphs thereby
making it a graph-independent inductive model.
The flexibility of the model stems from the ability
to shrink it (to a very simple model considering
atomic entity and relation embeddings only) or ex-
pand it (to a complex model by adding different
contextual information) without affecting any other
step in the learning procedure.

3.4 Relational Score Function

Having observed a triplet (es, r, eo), we first use Eq.
7, 8 and 9 to compute entity and relation represen-
tations. We then use these embeddings to capture
relational interaction between two entities using
the following score function g(·):

g(es, r, eo) = σ(zr
T · (zes � zeo)) (10)

where zr, ze
s
, ze

o ∈ Rd are d-dimensional repre-
sentations of entity and relationships as described
below. σ is the nonlinear activation function and �
represent element-wise product.

4 Efficient Learning Procedure

4.1 Objective Function

The complete parameter space
of the model can be given by:
Ω = {{Wi}5i=1,WE,WR,Wkey,Wval,Wt,Θ}.
To learn these parameters, we design a novel multi-
task objective function that jointly trains over two
graphs. As identified earlier, the goal of our model
is to leverage the available linkage information
across graphs for optimizing the entity and relation
embeddings such that they can explain the ob-
served triplets across the graphs. Further, we want
to leverage these optimized embeddings to match

entities across graphs and expand the available link-
age information. To achieve this goal, we define
following two different loss functions catering to
each learning task and jointly optimize over them
as a multi-task objective to learn model parameters:

Relational Learning Loss. This is conven-
tional loss function used to learn knowledge
graph embeddings. Specifically, given a p-th
triplet (es, r, eo)p from training set D, we sample
C negative samples by replacing either head or
tail entity and define a contrastive max margin
function as shown in (Socher et al., 2013):

Lrel =
C∑
c=1

max(0, γ − g(esp, rp, eop)

+ g′(esc, rp, e
o
p))

(11)

where, γ is margin, esc represent corrupted entity
and g′(esc, rp, e

o
p) represent corrupted triplet score.

Linkage Learning Loss: We design a novel
loss function to leverage pairwise label set L.
Given a triplet (esX , rX , e

o
X) from knowledge

graph X , we first find the entity e+Y from graph Y
that represent the same real-world entity as esX .
We then replace esX with e

+
Y and compute score

g(e+Y , rX , e
o
X). Next, we find set of all entities E

−
Y

from graph Y that has a negative label with entity
esX . We consider them analogous to the negative
samples we generated for Eq. 11. We then propose
the label learning loss function as:

Llab =
Z∑

z=1

max(0, γ − g(e+Y , rX , e
o
X)

+ (g′(e−Y , rX , e
o
X)z))

(12)

where, Z is the total number of negative labels
for eX . γ is margin which is usually set to 1
and e−Y ∈ E

−
Y represent entity from graph Y with

which entity esX had a negative label. Please note
that this applies symmetrically for the triplets that
originate from graph Y in the overall dataset. Note
that if both entities of a triplet have labels, we will
include both cases when computing the loss. Eq. 12
is inspired by Insight 1 and Insight 2 defined earlier
in Section 2. Given a set D of N observed triplets
across two graphs, we define complete multi-task
objective as:

L(Ω) =

N∑
i=1

[b·Lrel+(1−b)·Llab]+λ ‖Ω‖22 (13)



257

Algorithm 1 LinkNBed mini-batch Training
Input: Mini-batch M, Negative Sample Size
C, Negative Label Size Z, Attribute data
att data, Neighborhood data nhbr data, Type
data type data, Positive Label Dict pos dict,
Negative Label Dict neg dict
Output: Mini-batch Loss LM.
LM = 0
score pos = []; score neg = []; score pos lab =
[]; score neg lab = []
for i = 0 to size(M) do

input tuple =M[i] = (es, r, eo)
sc = compute triplet score(es, r, eo) (Eq. 10)
score pos.append(sc)
for j = 0 to C do

Select esc from entity list such that e
s
c 6= es

and esc 6= eo and (esc, r, eo) /∈ D
sc neg = compute triplet score(esc, r, e

o)
score neg.append(sc neg)

end for
if es in pos dict then
es+ = positive label for es

sc pos l = compute triplet score(es+, r, eo)
score pos lab.append(sc pos l)

end if
for k = 0 to Z do

Select es− from neg dict
sc neg l = compute triplet score(es−, r, eo)
score neg lab.append(sc neg l)

end for
end for
LM += compute minibatch loss(score pos,
score neg, score pos lab, score neg lab)
(Eq. 13)
Back-propagate errors and update parameters Ω
return LM

where Ω is set of all model parameters and λ is
regularization hyper-parameter. b is weight hyper-
parameter used to attribute importance to each task.
We train with mini-batch SGD procedure (Algo-
rithm 1) using Adam Optimizer.
Missing Positive Labels. It is expensive to ob-
tain positive labels across multiple graphs and
hence it is highly likely that many entities will
not have positive labels available. For those en-
tities, we will modify Eq. 12 to use the original
triplet (esX , rX , e

o
X) in place of perturbed triplet

g(e+Y , rX , e
o
X) for the positive label. The rationale

here again arises from Insight 2 wherein embed-
dings of two duplicate entities should be able to

replace each other without affecting the score.
Training Time Complexity. Most contextual in-
formation is pre-computed and available to all train-
ing steps which leads to constant time embedding
lookup for those context. But for attribute network,
embedding needs to be computed for each attribute
separately and hence the complexity to compute
score for one triplet is O(2a) where a is number
of attributes. Also for training, we generate C
negative samples for relational loss function and
use Z negative labels for label loss function. Let
k = C + Z. Hence, the training time complexity
for a set of n triplets will be O(2ak ∗ n) which is
linear in number of triplets with a constant factor as
ak << n for real world knowledge graphs. This is
desirable as the number of triplets tend to be very
large per graph in multi-relational settings.
Memory Complexity. We borrow notations
from (Nickel et al., 2016a) and describe the pa-
rameter complexity of our model in terms of the
number of each component and corresponding
embedding dimension requirements. Let Ha =
2∗NeHe+NrHr+NtHt+NkHk+NvHv. The pa-
rameter complexity of our model is: Ha ∗ (Hb+1).
Here, Ne, Nr, Nt, Nk, Nv signify number of enti-
ties, relations, types, attribute keys and vocab size
of attribute values across both datasets. Here Hb is
the output dimension of the hidden layer.

5 Experiments

5.1 Datasets
We evaluate LinkNBed and baselines on two real
world knowledge graphs: D-IMDB (derived from
large scale IMDB data snapshot) and D-FB (de-
rived from large scale Freebase data snapshot). Ta-
ble 5.1 provides statistics for our final dataset used
in the experiments. Appendix B.1 provides com-
plete details about dataset processing.

Dataset # Entities # Relations # Attributes # Entity # Available
Name Types Triples

D-IMDB 378207 38 23 41 143928582
D-FB 39667 146 69 324 22140475

Table 1: Statistics for Datasets: D-IMDB and D-FB

5.2 Baselines
We compare the performance of our method
against state-of-the-art representation learning
baselines that use neural embedding techniques to
learn entity and relation representation. Specif-
ically, we consider compositional methods of



258

RESCAL (Nickel et al., 2011) as basic matrix fac-
torization method, DISTMULT (Yang et al., 2015)
as simple multiplicative model good for capturing
symmetric relationships, and Complex (Trouillon
et al., 2016), an upgrade over DISTMULT that can
capture asymmetric relationships using complex
valued embeddings. We also compare against
translational model of STransE that combined
original structured embedding with TransE and has
shown state-of-art performance in benchmark test-
ing (Kadlec et al., 2017). Finally, we compare with
GAKE (Feng et al., 2016), a model that captures
context in entity and relationship representations.

In addition to the above state-of-art models,
we analyze the effectiveness of different compo-
nents of our model by comparing with various
versions that use partial information. Specifically,
we report results on following variants:
LinkNBed - Embed Only. Only use entity
embeddings, LinkNBed - Attr Only. Only use
Attribute Context, LinkNBed - Nhbr Only. Only
use Neighborhood Context, LinkNBed - Embed +
Attr. Use both Entity embeddings and Attribute
Context, LinkNBed - Embed + Nhbr. Use both
Entity embeddings and Neighbor Context and
LinkNBed - Embed All. Use all three Contexts.

5.3 Evaluation Scheme

We evaluate our model using two inference tasks:
Link Prediction. Given a test triplet (es, r, eo),
we first score this triplet using Eq. 10. We then
replace eo with all other entities in the dataset
and filter the resulting set of triplets as shown in
(Bordes et al., 2013). We score the remaining set
of perturbed triplets using Eq. 10. All the scored
triplets are sorted based on the scores and then
the rank of the ground truth triplet is used for the
evaluation. We use this ranking mechanism to
compute HITS@10 (predicted rank ≤ 10) and
reciprocal rank ( 1rank ) of each test triplet. We
report the mean over all test samples.

Entity Linkage. In alignment with Insight
2, we pose a novel evaluation scheme to perform
entity linkage. Let there be two ground truth test
sample triplets: (eX , e+Y , 1) representing a positive
duplicate label and (eX , e−Y , 0) representing a
negative duplicate label. Algorithm 2 outlines
the procedure to compute linkage probability or
score q (∈ [0, 1]) for the pair (eX , eY ). We use
L1 distance between the two vectors analogous

Algorithm 2 Entity Linkage Score Computation
Input: Test pair – (eX ∈ X, eY ∈ Y ).
Output: Linkage Score – q.

1. Collect all triplets involving eX from graph
X and all triplets involving eY from graph Y
into a combined set O. Let |O| = k.
2. Construct Sorig ∈ Rk.
For each triplet o ∈ O, compute score g(o) using
Eq. 10 and store the score in Sorig.
3. Create triplet set O′ as following:
if o ∈ O contain eX ∈ X then

Replace eX with eY to create perturbed triplet
o′ and store it in O′

end if
if o ∈ O contain eY ∈ Y then

Replace eY with eX to create perturbed triplet
o′ and store it in O′

end if
4. Construct Srepl ∈ Rk.
For each triplet o′ ∈ O′, compute score g(o′)
using Eq. 10 and store the score in Srepl.
5. Compute q.
Elements in Sorig and Srepl have one-one corre-
spondence so take the mean absolute difference:
q = |Sorig - Srepl|1
return q

to Mean Absolute Error (MAE). In lieu of
hard-labeling test pairs, we use score q to compute
Area Under the Precision-Recall Curve (AUPRC).

For the baselines and the unsupervised version
(with no labels for entity linkage) of our model,
we use second stage multilayer Neural Network as
classifier for evaluating entity linkage. Appendix
B.2 provides training configuration details.

5.4 Predictive Analysis

Link Prediction Results. We train LinkNBed
model jointly across two knowledge graphs and
then perform inference over individual graphs to re-
port link prediction reports. For baselines, we train
each baseline on individual graphs and use parame-
ters specific to the graph to perform link prediction
inference over each individual graph. Table 5.4
shows link prediction performance for all meth-
ods. Our model variant with attention mechanism
outperforms all the baselines with 4.15% improve-
ment over single graph state-of-the-art Complex
model on D-IMDB and 8.23% improvement on D-
FB dataset. D-FB is more challenging dataset to



259

Method D-IMDB-HITS10 D-IMDB-MRR D-FB-HITS10 D-FB-MRR

RESCAL 75.3 0.592 69.99 0.147
DISTMULT 79.5 0.691 72.34 0.556

Complex 83.2 0.752 75.67 0.629
STransE 80.7 0.421 69.87 0.397
GAKE 69.5 0.114 63.22 0.093

LinkNBed-Embed Only 79.9 0.612 73.2 0.519
LinkNBed-Attr Only 82.2 0.676 74.7 0.588
LinkNBed-Nhbr Only 80.1 0.577 73.4 0.572

LinkNBed-Embed + Attr 84.2 0.673 78.39 0.606
LinkNBed-Embed + Nhbr 81.7 0.544 73.45 0.563

LinkNBed-Embed All 84.3 0.725 80.2 0.632
LinkNBed-Embed All (Attention) 86.8 0.733 81.9 0.677

Improvement (%) 4.15 1.10 8.23 7.09

Table 2: Link Prediction Results on both datasets

learn as it has a large set of sparse relationships,
types and attributes and it has an order of magni-
tude lesser relational evidence (number of triplets)
compared to D-IMDB. Hence, LinkNBed’s pro-
nounced improvement on D-FB demonstrates the
effectiveness of the model. The simplest version of
LinkNBed with only entity embeddings resembles
DISTMULT model with different objective func-
tion. Hence closer performance of those two mod-
els aligns with expected outcome. We observed
that the Neighborhood context alone provides only
marginal improvements while the model benefits
more from the use of attributes. Despite being
marginal, attention mechanism also improves accu-
racy for both datasets. Compared to the baselines
which are obtained by trained and evaluated on in-
dividual graphs, our superior performance demon-
strates the effectiveness of multi-graph learning.

Entity Linkage Results. We report entity link-
age results for our method in two settings: a.) Su-
pervised case where we train using both the objec-
tive functions. b.) Unsupervised case where we
learn with only the relational loss function. The
latter case resembles the baseline training where
each model is trained separately on two graphs in
an unsupervised manner. For performing the entity
linkage in unsupervised case for all models, we
first train a second stage of simple neural network
classifier and then perform inference. In the super-
vised case, we use Algorithm 2 for performing the
inference. Table 5.4 demonstrates the performance
of all methods on this task. Our method signifi-
cantly outperforms all the baselines with 33.86%
over second best baseline in supervised case and
17.35% better performance in unsupervised case.
The difference in the performance of our method in
two cases demonstrate that the two training objec-
tives are helping one another by learning across the
graphs. GAKE’s superior performance on this task
compared to the other state-of-the-art relational
baselines shows the importance of using contex-

Method AUPRC (Supervised) AUPRC (Unsupervised)

RESCAL - 0.327
DISTMULT - 0.292

Complex - 0.359
STransE - 0.231
GAKE - 0.457

LinkNBed-Embed Only 0.376 0.304
LinkNBed-Attr Only 0.451 0.397
LinkNBed-Nhbr Only 0.388 0.322

LinkNBed-Embed + Attr 0.512 0.414
LinkNBed-Embed + Nhbr 0.429 0.356

LinkNBed-Embed All 0.686 0.512
LinkNBed-Embed All (Attention) 0.691 0.553

Improvement (%) 33.86 17.35

Table 3: Entity Linkage Results - Unsupervised
case uses classifier at second step

tual information for entity linkage. Performance of
other variants of our model again demonstrate that
attribute information is more helpful than neigh-
borhood context and attention provides marginal
improvements. We provide further insights with
examples and detailed discussion on entity linkage
task in Appendix A.

6 Related Work

6.1 Neural Embedding Methods for
Relational Learning

Compositional Models learn representations by
various composition operators on entity and rela-
tional embeddings. These models are multiplica-
tive in nature and highly expressive but often suf-
fer from scalability issues. Initial models include
RESCAL (Nickel et al., 2011) that uses a relation
specific weight matrix to explain triplets via pair-
wise interactions of latent features, Neural Tensor
Network (Socher et al., 2013), more expressive
model that combines a standard NN layer with a bi-
linear tensor layer and (Dong et al., 2014) that em-
ploys a concatenation-projection method to project
entities and relations to lower dimensional space.
Later, many sophisticated models (Neural Associa-
tion Model (Liu et al., 2016), HoLE (Nickel et al.,
2016b)) have been proposed. Path based composi-
tion models (Toutanova et al., 2016) and contextual
models GAKE (Feng et al., 2016) have been re-
cently studied to capture more information from
graphs. Recently, model like Complex (Trouil-
lon et al., 2016) and Analogy (Liu et al., 2017)
have demonstrated state-of-the art performance on
relational learning tasks. Translational Models
( (Bordes et al., 2014), (Bordes et al., 2011), (Bor-
des et al., 2013), (Wang et al., 2014), (Lin et al.,
2015), (Xiao et al., 2016)) learn representation by



260

employing translational operators on the embed-
dings and optimizing based on their score. They of-
fer an additive and efficient alternative to expensive
multiplicative models. Due to their simplicity, they
often loose expressive power. For a comprehensive
survey of relational learning methods and empiri-
cal comparisons, we refer the readers to (Nickel
et al., 2016a), (Kadlec et al., 2017), (Toutanova and
Chen, 2015) and (Yang et al., 2015). None of these
methods address multi-graph relational learning
and cannot be adapted to tasks like entity linkage
in straightforward manner.

6.2 Entity Resolution in Relational Data

Entity Resolution refers to resolving entities avail-
able in knowledge graphs with entity mentions
in text. (Dredze et al., 2010) proposed entity
disambiguation method for KB population, (He
et al., 2013) learns entity embeddings for resolu-
tion, (Huang et al., 2015) propose a sophisticated
DNN architecture for resolution, (Campbell et al.,
2016) proposes entity resolution across multiple
social domains, (Fang et al., 2016) jointly embeds
text and knowledge graph to perform resolution
while (Globerson et al., 2016) proposes Attention
Mechanism for Collective Entity Resolution.

6.3 Learning across multiple graphs

Recently, learning over multiple graphs have
gained traction. (Liu and Yang, 2016) divides
a multi-relational graph into multiple homoge-
neous graphs and learns associations across them
by employing product operator. Unlike our work,
they do not learn across multiple multi-relational
graphs. (Pujara and Getoor, 2016) provides logic
based insights for cross learning, (Pershina et al.,
2015) does pairwise entity matching across multi-
relational graphs and is very expensive, (Chen et al.,
2017) learns embeddings to support multi-lingual
learning and Big-Align (Koutra et al., 2013) tackles
graph alignment problem efficiently for bipartite
graphs. None of these methods learn latent rep-
resentations or jointly train graph alignment and
learning which is the goal of our work.

7 Concluding Remarks and Future Work

We present a novel relational learning framework
that learns entity and relationship embeddings
across multiple graphs. The proposed representa-
tion learning framework leverage an efficient learn-
ing and inference procedure which takes into ac-
count the duplicate entities representing the same

real-world entity in a multi-graph setting. We
demonstrate superior accuracies on link predic-
tion and entity linkage tasks compared to the exist-
ing approaches that are trained only on individual
graphs. We believe that this work opens a new
research direction in joint representation learning
over multiple knowledge graphs.

Many data driven organizations such as Google
and Microsoft take the approach of constructing a
unified super-graph by integrating data from multi-
ple sources. Such unification has shown to signifi-
cantly help in various applications, such as search,
question answering, and personal assistance. To
this end, there exists a rich body of work on linking
entities and relations, and conflict resolution (e.g.,
knowledge fusion (Dong et al., 2014). Still, the
problem remains challenging for large scale knowl-
edge graphs and this paper proposes a deep learning
solution that can play a vital role in this construc-
tion process. In real-world setting, we envision
our method to be integrated in a large scale system
that would include various other components for
tasks like conflict resolution, active learning and
human-in-loop learning to ensure quality of con-
structed super-graph. However, we point out that
our method is not restricted to such use cases—one
can readily apply our method to directly make infer-
ence over multiple graphs to support applications
like question answering and conversations.

For future work, we would like to extend the
current evaluation of our work from a two-graph
setting to multiple graphs. A straightforward ap-
proach is to create a unified dataset out of more
than two graphs by combining set of triplets as
described in Section 2, and apply learning and in-
ference on the unified graph without any major
change in the methodology. Our inductive frame-
work learns functions to encode contextual informa-
tion and hence is graph independent. Alternatively,
one can develop sophisticated approaches with it-
erative merging and learning over pairs of graphs
until exhausting all graphs in an input collection.

Acknowledgments

We would like to give special thanks to Ben Lon-
don, Tong Zhao, Arash Einolghozati, Andrew
Borthwick and many others at Amazon for helpful
comments and discussions. We thank the reviewers
for their valuable comments and efforts towards
improving our manuscript. This project was sup-
ported in part by NSF(IIS-1639792, IIS-1717916).



261

References
Sören Auer, Christian Bizer, Georgi Kobilarov, Jens

Lehmann, Richard Cyganiak, and Zachary Ives.
2007. DBpedia: A nucleus for a web of open data.
In The Semantic Web.

Kurt Bollacker, Colin Evans, Praveen Paritosh, Tim
Sturge, and Jamie Taylor. 2008. Freebase: a col-
laboratively created graph database for structuring
human knowledge. In SIGMOD Conference.

Antoine Bordes, Xavier Glorot, Jason Weston, and
Yoshua Bengio. 2014. A semantic matching energy
function for learning with multi-relational data. Ma-
chine Learning.

Antoine Bordes, Nicolas Usunier, Alberto Garcia-
Duran, Jason Weston, and Oksana Yakhnenko.
2013. Translating embeddings for modeling multi-
relational data. In Advances in neural information
processing systems, pages 2787–2795.

Antoine Bordes, Jason Weston, Ronan Collobert, and
Yoshua Bengio. 2011. Learning structured embed-
dings of knowledge bases. In AAAI.

Peter Buneman and Slawek Staworko. 2016. Rdf graph
alignment with bisimulation. Proc. VLDB Endow.

W. M. Campbell, Lin Li, C. Dagli, J. Acevedo-
Aviles, K. Geyer, J. P. Campbell, and C. Priebe.
2016. Cross-domain entity resolution in social me-
dia. arXiv:1608.01386v1.

Andrew Carlson, Justin Betteridge, Bryan Kisiel, Burr
Settles, Estevam R. Hruschka Jr., and Tom M.
Mitchell. 2010. Toward an architecture for never-
ending language learning. In Proceedings of the
Twenty-Fourth Conference on Artificial Intelligence
(AAAI 2010).

Rose Catherine and William Cohen. 2016. Personal-
ized recommendations using knowledge graphs: A
probabilistic logic programming approach. In Pro-
ceedings of the 10th ACM Conference on Recom-
mender Systems.

Muhao Chen, Yingtao Tian, Mohan Yang, and Carlo
Zaniolo. 2017. Multilingual knowledge graph em-
beddings for cross-lingual knowledge alignment.

Wanyun Cui, Yanghua Xiao, Haixun Wang, Yangqiu
Song, Seung-won Hwang, and Wei Wang. 2017.
Kbqa: Learning question answering over qa corpora
and knowledge bases. Proc. VLDB Endow.

Jeffrey Dalton, Laura Dietz, and James Allan. 2014.
Entity query feature expansion using knowledge
base links. In Proceedings of the 37th International
ACM SIGIR Conference on Research &#38; Devel-
opment in Information Retrieval.

Xin Dong, Evgeniy Gabrilovich, Geremy Heitz, Wilko
Horn, Ni Lao, Kevin Murphy, Thomas Strohmann,
Shaohua Sun, and Wei Zhang. 2014. Knowledge

vault: A web-scale approach to probabilistic knowl-
edge fusion. In Proceedings of the 20th ACM
SIGKDD International Conference on Knowledge
Discovery and Data Mining, pages 601–610.

Mark Dredze, Paul McNamee, Delip Rao, Adam Ger-
ber, and Tim Finin. 2010. Entity disambiguation
for knowledge base population. In Proceedings
of the 23rd International Conference on Computa-
tional Linguistics.

Wei Fang, Jianwen Zhang, Dilin Wang, Zheng Chen,
and Ming Li. 2016. Entity disambiguation by knowl-
edge and text jointly embedding. In CoNLL.

Jun Feng, Minlie Huang, Yang Yang, and Xiaoyan Zhu.
2016. Gake: Graph aware knowledge embedding.
In COLING.

Evgeniy Gabrilovich and Shaul Markovitch. 2009.
Wikipedia-based semantic interpretation for natural
language processing. J. Artif. Int. Res.

Amir Globerson, Nevena Lazic, Soumen Chakrabarti,
Amarnag Subramanya, Michael Ringaard, and Fer-
nando Pereira. 2016. Collective entity resolution
with multi-focal attention. In Proceedings of the
54th Annual Meeting of the Association for Compu-
tational Linguistics.

William L. Hamilton, Rex Ying, and Jure Leskovec.
2017. Representation learning on graphs: Methods
and applications. arXiv:1709.05584.

Zhengyan He, Shujie Liu, Mu Li, Ming Zhou, Longkai
Zhang, and Houfeng Wang. 2013. Learning entity
representation for entity disambiguation. In Pro-
ceedings of the 51st Annual Meeting of the Associ-
ation for Computational Linguistics.

Hongzhao Huang, Larry Heck, and Heng Ji.
2015. Leveraging deep neural networks and
knowledge graphs for entity disambiguation.
arXiv:1504.07678v1.

Rudolph Kadlec, Ondrej Bajgar, and Jan Kleindienst.
2017. Knowledge base completion: Baselines strike
back. In Proceedings of the 2nd Workshop on Rep-
resentation Learning for NLP.

Danai Koutra, HangHang Tong, and David Lubensky.
2013. Big-align: Fast bipartite graph alignment. In
2013 IEEE 13th International Conference on Data
Mining.

Quoc Le and Tomas Mikolov. 2014. Distributed repre-
sentations of sentences and documents. In Proceed-
ings of the 31st International Conference on Ma-
chine Learning.

Yankai Lin, Zhiyuan Liu, Maosong Sun, and Xuan Zhu.
2015. Learning entity and relation embeddings for
knowledge graph completion. AAAI Conference on
Artificial Intelligence.



262

Hanxiao Liu, Yuexin Wu, and Yimin Yang. 2017. Ana-
logical inference for multi-relatinal embeddings. In
Proceedings of the 34th International Conference on
Machine Learning.

Hanxiao Liu and Yimin Yang. 2016. Cross-graph learn-
ing of multi-relational associations. In Proceedings
of the 33rd International Conference on Machine
Learning.

Quan Liu, Hui Jiang, Andrew Evdokimov, Zhen-Hua
Ling, Xiaodan Zhu, Si Wei, and Yu Hu. 2016. Prob-
abilistic reasoning via deep learning: Neural associ-
ation models. arXiv:1603.07704v2.

Maximilian Nickel, Kevin Murphy, Volker Tresp, and
Evgeniy Gabrilovich. 2016a. A review of relational
machine learning for knowledge graphs. Proceed-
ings of the IEEE.

Maximilian Nickel, Lorenzo Rosasco, and Tomaso
Poggio. 2016b. Holographic embeddings of knowl-
edge graphs.

Maximilian Nickel, Volker Tresp, and Hans-Peter
Kriegel. 2011. A three-way model for collective
learning on multi-relational data. In Proceedings
of the 28th International Conference on Machine
Learning (ICML-11), pages 809–816.

Maria Pershina, Mohamed Yakout, and Kaushik
Chakrabarti. 2015. Holistic entity matching across
knowledge graphs. In 2015 IEEE International Con-
ference on Big Data (Big Data).

Jay Pujara and Lise Getoor. 2016. Generic statisti-
cal relational entity resolution in knowledge graphs.
In Sixth International Workshop on Statistical Rela-
tional AI.

Ryan A. Rossi, Rong Zhou, and Nesreen K.
Ahmed. 2017. Deep feature learning for graphs.
arXiv:1704.08829.

Richard Socher, Danqi Chen, Christopher D Manning,
and Andrew Ng. 2013. Reasoning with neural ten-
sor networks for knowledge base completion. In
Advances in Neural Information Processing Systems,
pages 926–934.

Fabian M. Suchanek, Gjergji Kasneci, and Gerhard
Weikum. 2007. Yago: A core of semantic knowl-
edge. In Proceedings of the 16th International Con-
ference on World Wide Web.

Kristina Toutanova and Danqi Chen. 2015. Observed
versus latent features for knowledge base and text
inference. In ACL.

Kristina Toutanova, Xi Victoria Lin, Wen-tau Yih, Hoi-
fung Poon, and Chris Quirk. 2016. Compositional
learning of embeddings for relation paths in knowl-
edge bases and text. In Proceedings of the 54th An-
nual Meeting of the Association for Computational
Linguistics, volume 1, pages 1434–1444.

Theo Trouillon, Johannes Welbl, Sebastian Riedel, Eric
Gaussier, and Guillaume Bouchard. 2016. Complex
embeddings for simple link prediction. In Proceed-
ings of the 33rd International Conference on Ma-
chine Learning.

Zhen Wang, Jianwen Zhang, Jianlin Feng, and Zheng
Chen. 2014. Knowledge graph embedding by trans-
lating on hyperplanes.

Han Xiao, Minlie Huang, and Xiaoyan Zhu. 2016.
Transg: A generative model for knowledge graph
embedding. In Proceedings of the 54th Annual
Meeting of the Association for Computational Lin-
guistics.

Bishan Yang, Wen-tau Yih, Xiaodong He, Jianfeng
Gao, and Li Deng. 2015. Embedding entities and
relations for learning and inference in knowledge
bases. arXiv:1412.6575.


