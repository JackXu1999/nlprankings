



















































The Impact of Listener Gaze on Predicting Reference Resolution


Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics
and the 7th International Joint Conference on Natural Language Processing (Short Papers), pages 812–817,

Beijing, China, July 26-31, 2015. c©2015 Association for Computational Linguistics

The Impact of Listener Gaze on Predicting Reference Resolution

Nikolina Koleva1 Martı́n Villalba2 Maria Staudte1

1Embodied Spoken Interaction Group, Saarland University, Saarbrücken, Germany
2 Department of Linguistics, University of Potsdam, Potsdam, Germany

{nikkol | masta}@coli.uni-saarland.de
{martin.villalba | alexander.koller}@uni-potsdam.de

Alexander Koller2

Abstract

We investigate the impact of listener’s
gaze on predicting reference resolution in
situated interactions. We extend an ex-
isting model that predicts to which entity
in the environment listeners will resolve
a referring expression (RE). Our model
makes use of features that capture which
objects were looked at and for how long,
reflecting listeners’ visual behavior. We
improve a probabilistic model that consid-
ers a basic set of features for monitoring
listeners’ movements in a virtual environ-
ment. Particularly, in complex referential
scenes, where more objects next to the tar-
get are possible referents, gaze turns out to
be beneficial and helps deciphering listen-
ers’ intention. We evaluate performance at
several prediction times before the listener
performs an action, obtaining a highly sig-
nificant accuracy gain.

1 Introduction

Speakers tend to follow the listener’s behavior in
order to determine whether their communicated
message was received and understood. This phe-
nomenon is known as grounding, it is well estab-
lished in the dialogue literature (Clark, 1996), and
it plays an important role in collaborative tasks
and goal–oriented conversations. Solving a col-
laborative task in a shared environment is an ef-
fective way of studying the alignment of commu-
nication channels (Clark and Krych, 2004; Hanna
and Brennan, 2007).

In situated spoken conversations ambiguous lin-
guistic expressions are common, where additional
modalities are available. While Gargett et al.
(2010) studied instruction giving and following in
virtual environments, Brennan et al. (2013) ex-
amined pedestrian guidance in outdoor real envi-
ronments. Both studies investigate the interaction

of human interlocutors but neither study exploits
listeners’ eye movements. In contrast, Koller et
al. (2012) designed a task in which a natural lan-
guage generation (NLG) system gives instructions
to a human player in virtual environment whose
eye movements were tracked. They outperformed
similar systems in both successful reference res-
olution and listener confusion. Engonopoulos et
al. (2013) attempted to predict the resolution of
an RE, achieving good performance by combining
two probabilistic log–linear models: a semantic
model Psem that analyzes the semantics of a given
instruction, and an observational model Pobs that
inspects the player’s behavior. However, they did
not include listener’s gaze. They observed that the
accuracy for Pobs reaches its highest point at a rel-
atively late stage in an interaction. Similar obser-
vations are reported by Kennington and Schlangen
(2014): they compare listener gaze and an incre-
mental update model (IUM) as predictors for the
resolution of an RE, noting that gaze is more ac-
curate before the onset of an utterance, whereas
the model itself is more accurate afterwards.

In this paper we report on the extension of the
Pobs model to also consider listener’s visual be-
haviour. More precisely we implement features
that encode listener’s eye movement patterns and
evaluate their performance on a multi–modal data
collection. We show that such a model as it
takes an additional communication channel pro-
vides more accurate predictions especially when
dealing with complex scenes. We also expand on
concepts from the IUM, by applying the conclu-
sions drawn from its behaviour to a dynamic task
with a naturalistic interactive scenario.

2 Problem definition

We address the research question of how to auto-
matically predict an RE resolution, i.e., answer-
ing the question of which entity in a virtual en-
vironment has been understood by the listener af-

812



ter receiving an instruction. While the linguistic
material in instructions carries a lot of informa-
tion, even completely unambiguous descriptions
may be misunderstood. A robust NLG system
should be capable of detecting misunderstandings
and preventing its users from making mistakes.

Language comprehension is mirrored by inter-
locutors’ non verbal behavior, and this can help
when decoding the listener’s interpretation. Pre-
cise automatic estimates may be crucial when de-
veloping a real–time NLG system, as such a mech-
anism would be more robust and capable at avoid-
ing misunderstandings. As mentioned in section 1,
Engonopoulos et al. (2013) propose two statistical
models to solve that problem: a semantic model
Psem based on the linguistic content, and an ob-
servation model Pobs based on listener behavior
features.

More formally, let’s assume a system generates
an expression r that aims to identify a target ob-
ject ot among a setO of possible objects, i.e. those
available in the scene view. Given the state of the
world s at time point t, and the observed listener’s
behavior σ(t) of the user at time t ≥ tb (where
tb denotes the end of an interaction), we estimated
the conditional probability p(op|r, s, σ(t)) that in-
dicates how probable it is that the listener resolved
r to op. This probability can be also expressed as
follows:

P (op|r, s, σ(t)) ∝ Psem(op|r, s)Pobs(op|σ(t))
P (op)

Following Engonopoulos et al. (2013) we make
the simplifying assumption that the distribution of
the probability among the possible targets is uni-
form and obtain:

P (op|r, s, σ(t)) ∝ Psem(op|r, s)Pobs(op|σ(t))
We expect an NLG system to compute and out-
put an expression that maximizes the probability
of op. Due to the dynamic nature of our scenar-
ios, we also require the probability value to be up-
dated at certain time intervals throughout an in-
teraction. Tracking the probability changes over
time, an NLG system could proactively react to
changes in its environment. Henderson and Smith
(2007) show that accounting for both fixation lo-
cation and duration are key to identify a player’s
focus of attention.

The technical contribution of this paper is to ex-
tend the Pobs model of Engonopoulos et al. (2013)
with gaze features to account for these variables.

3 Episodes and feature functions

The data for our experiment was obtained from
the GIVE Challenge (Koller et al., 2010), an inter-
active task in a 3D virtual environment in which
a human player (instruction follower, IF) is navi-
gated through a maze, locating and pressing but-
tons in a predefined order aiming to unlock a safe.
While pressing the wrong button in the sequences
doesn’t always have negative effects, it can also
lead to restarting or losing the game. The IF re-
ceives instructions from either another player or
an automated system (instruction giver, IG). The
IF’s behavior was recorded every 200ms, along
with the IG’s instructions and the state of the
virtual world. The result is an interaction cor-
pus comprising over 2500 games and spanning
over 340 hours of interactions. These interactions
were mainly collected during the GIVE-2 and the
GIVE-2.5 challenges. A laboratory study con-
ducted by Staudte et al. (2012) comprises a data
collection that contains eye-tracking records for
the IF. Although the corpus contains both success-
ful and unsuccessful games, we have decided to
consider only the successful ones.

We define an episode over this corpus as a typ-
ically short sequence of recorded behavior states,
beginning with a manipulation instruction gener-
ated by the IG and ending with a button press by
the IF (at time point tb). In order to make sure
that the recorded button press is a direct response
to the IG’s instruction, an episode is defined such
that it doesn’t contain further utterances after the
first one. Both the target intended by the IG (ot)
and the one selected by the IF (op) were recorded.

Figure 1: The structure of the interactions.

Figure 1 depicts the structure of an episode
when eye-tracking data is available. Each episode

813



can be seen as a sequence of interaction states
(s1, . . . , sn), and each state has a set of visible
objects ({o1, o2, o3, o10, o12}). We then compute
the subset of fixated objects ({o2, o3, o12}). We
update both sets of visible and fixated objects dy-
namically in each interaction state with respect to
the change in visual scene and the corresponding
record of the listener’s eye movements.

We developed feature functions over these
episodes. Along with the episode’s data, each
function takes two parameters: an object op for
which the function is evaluated, and a parameter
d seconds that defines how much of the episode’s
data is the feature allowed to analyze. Each feature
looks only at the behavior that happens in the time
interval −d to 0. Henceforth we refer to the value
of a feature function over this interval as its value
at time −d. The value of a feature function evalu-
ated on episodes with length less than d seconds is
undefined.

4 Prediction models

Given an RE uttered by an IG, the semantic model
Psem estimates the probability for each possible
object in the environment to have been understood
as the referent, ranks all candidates and selects the
most probable one in a current scene. This prob-
ability represents the semantics of the utterance,
and is evaluated at a single time point immediately
after the instruction (e.g. “press the blue button”)
has been uttered. The model takes into account
features that encode the presence or absence of ad-
jectives carrying information about the spatial or
color properties (like the adjective “blue”), along
with landmarks appearing as post modifiers of the
target noun.

In contrast to the semantic model, the observa-
tional model Pobs evaluates the changes in the vi-
sual context and player’s behavior after an instruc-
tion has been received. The estimated probabil-
ity is updated constantly before an action, as the
listener in our task–oriented interactions is con-
stantly in motion, altering the visual context. The
model evaluates the distance of the listener posi-
tion to a potential target, whether it is visible or
not, and also how salient an object is in that par-
ticular time window.

As we have seen above, eye movements pro-
vide useful information indicating language com-
prehension, and also how to map a semantic repre-
sentation to an entity in a shared environment. In-

terlocutors constantly interact with their surround-
ing and point to specific entities with gestures.
Gaze behaviour is also driven by the current state
of an interaction. Thus, we extend the basic set
of Pobs features and implement eye–tracking fea-
tures that capture gaze information. We call this
the extended observational model PEobs and con-
sider the following additional features:

1. Looked at: feature counts the number of
interaction states in which an object has
been fixated at least once during the current
episode.

2. Longest Sequence: detects the longest con-
tinuous sequence of interaction states in
which a particular object has been fixated.

3. Linear Distance: returns the euclidean dis-
tance dist on screen between the gaze cursor
and the center of an object.

4. Inv-Squared Distance: returns 1
1+dist2

.

5. Update Fixated Objects: expands the list of
fixated objects in order to consider the IF’s
focus of attention. It successively searches in
10 pixel steps and stops as soon as an object
is found (the threshold is 100 pixels). This
feature evaluates to 1 if the list of fixated ob-
jects is been expanded and 0 otherwise.

When training our model at time −dtrain, we
generate a feature matrix. Given a training
episode, each possible (located in the same room)
object op is added as a new row, where each col-
umn contains the value of a different feature func-
tion for op over this episode at time −dtrain. Fi-
nally, the row based on the target selected by the
IF is marked as a positive example. We then train
a log-linear model, where the weights assigned
to each feature function are learned via optimiza-
tion with the L-BFGS algorithm. By training our
model to correctly predict a target button based
only on data observed up until −dtrain seconds
before the actual action tb, we expect our model to
reliably predict which button the user will select.
Analogously, we define accuracy at testing time
−dtest as the percentage of correctly predicted tar-
get objects when predicting over episodes at time
−dtest. This pair of training and test parameters is
denoted as the tuple (dtrain, dtest).

814



5 Dataset

We evaluated the performance of our improved
model over data collected by Staudte et al. (2012)
using the GIVE Challenge platform. Both training
and testing were performed over a subset of the
data obtained during a collection task involving
worlds created by Gargett et al. (2010), designed
to provide the task with varying levels of diffi-
culty. This corpus provides recorded eye-tracking
data, collected with a remote faceLAB system. In
contrast, the evaluation presented by Engonopou-
los et al. (2013) uses only games collected for the
GIVE 2 and GIVE 2.5 challenges, for which no
eye-tracking data is available. Here, we do not in-
vestigate the performance of Psem and concentrate
on the direct comparison between Pobs and PEobs
in order to find out if and when eye–tracking can
improve the prediction of an RE resolution.

We further filtered our corpus in order to re-
move noisy games following Koller et al. (2012),
considering only interactions for which the eye-
tracker calibration detected inspection of either the
target or another button object in at least 75% of all
referential scenes in an interaction. The resulting
corpus comprises 75 games, for a combined length
of 8 hours. We extracted 761 episodes from this
corpus, amounting to 47m 58s of recorded interac-
tions, with an average length per episode of 3.78
seconds (σ = 3.03sec.). There are 261 episodes
shorter than 2 sec., 207 in the 2-4 sec. range, 139
in the 4-6 sec. range, and 154 episodes longer than
6 sec.

6 Evaluation and results

The accuracy of our probabilistic models depends
on the parameters (dtrain, dtest). At different
stages of an interaction the difficulty to predict an
intended target varies as the visual context changes
and in particular the number of visible objects. As
the weights of the features are optimized at time
−dtrain, it would be expected that testing also at
time −dtest = −dtrain yields the highest accu-
racy. However, the difficulty to make a predic-
tion decreases as tb − dtest approaches tb, i.e. as
the player moves towards the intended target. We
expect that testing at −dtrain works best, but we
need to be able to update continuously. Thus we
also evaluate at other timepoints and test several
combinations of the (dtrain, dtest) parameters.

Given the limited amount of eye-tracking data
available in our corpus, we replaced the cross-

corpora-challenge test setting from the original
Pobs study with a ten fold cross validation setup.
As training and testing were performed over in-
stances of a certain minimum length according to
(dtrain, dtest), we first removed all instances with
length less than max(dtrain, dtest), and then per-
form the cross validation split. In this way we
ensure that the number of instances in the folds
are not unbalanced. Moreover, each instance was
classified as easy or hard depending on the num-
ber of visible objects at time tb. An instance
was considered easy if no more than three objects
were visible at that point, or hard otherwise. For
−dtest = 0, 59.5% of all instances are considered
hard, but this proportion decreases as −dtest in-
creases. At −dtest = −6, the number of hard in-
stances amounts to 72.7%.

We evaluated both the original Pobs model and
the PEobs model on the same data set. We also cal-
culated accuracy values for each feature function,
in order to test whether a single function could out-
perform Pobs. We included as baselines two ver-
sions of Pobs using only the features InRoom and
Visual Salience proposed by Engonopoulos et al.
(2013).

The accuracy results on Figure 2 show our ob-
servations for−6 ≤ −dtrain ≤ −2 and−dtrain ≤
−dtest ≤ 0. The graph shows that PEobs performs
similarly as Pobs on the easy instances, i.e. the
eye-tracking features are not contributing in those
scenarios. However, PEobs shows a consistent im-
provement on the hard instances over Pobs.

For each permutation of the training and test-
ing parameters (dtrain, dtest), we obtain a set of
episodes that fulfil the length criteria for the given
parameters. We apply Pobs and PEobs on the ob-
tained set of instances and measure two corre-
sponding accuracy values. We compared the ac-
curacy values of Pobs and PEobs over all 25 differ-
ent (dtrain, dtest) pairs, using a paired samples t-
test. The test indicated that the PEobs performance
(M = 83.72, SD = 3.56) is significantly better
than the Pobs performance (M = 79.33, SD =
3.89), (t(24) = 9.51, p < .001, Cohen′s d =
1.17). Thus eye-tracking features seem to be par-
ticularly helpful for predicting to which entity an
RE is resolved in hard scenes.

The results also show a peak in accuracy near
the -3 seconds mark. We computed a 2x2 con-
tingency table that contrasts correct and incorrect
predictions for Pobs and PEobs, i.e. whether oi was

815



Figure 2: Accuracy as a function of training and testing time.

classified as target object or not. Data for this ta-
ble was collected from all episode judgements for
models trained at times in the [−6 sec.,−3 sec.]
range and tested at -3 seconds. McNemar’s test
showed that the marginal row and column frequen-
cies are significantly different (p < 0.05). This
peak is related to the average required time be-
tween an utterance and the resulting target manip-
ulation. This result shows that our model is more
accurate precisely at points in time when we ex-
pect fixations to a target object.

7 Conclusion

In this paper we have shown that listener’s gaze
is useful by showing that accuracy improves over
Pobs in the context of predicting the resolution of
an RE. In addition, we observed that our model
PEobs proves to be more robust than Pobs when the
time interval between the prediction (tb − dtest)
and the button press (tb) increases, i.e. gaze is
especially beneficial in an early stage of an in-
teraction. This approach shows significant ac-
curacy improvement on hard referential scenes
where more objects are visible.

We have also established that gaze is particu-
larly useful when combined with some other sim-
ple features, as the features that capture listeners
visual behaviour are not powerful enough to out-
perform even the simplest baseline. Gaze only
benefits the model when it is added on top of fea-
tures that capture the visual context, i.e. the current
scene.

The most immediate future line of research is
the combination of our PEobs model with the se-

mantic model Psem, in order to test the impact of
the extended features in a combined model. If suc-
cessful, such a model could provide reliable pre-
dictions for a significant amount of time before an
action takes place. This is of particular importance
when it comes to designing a system that auto-
matically generates and online outputs feedback to
confirm correct and reject incorrect intentions.

Testing with users in real time is also an area
for future research. An implementation of the Pobs
model is currently in the test phase, and an exten-
sion for the PEobs model would be the immediate
next step. The model could be embedded in an
NLG system to improve the automatic language
generation in such scenarios.

Given that our work refers only to NLG sys-
tems, there’s no possible analysis of speaker’s
gaze. However, it may be interesting to ask
whether a human IG could benefit from the pre-
dictions of PEobs. We could study whether pre-
dictions based on the gaze (mis-)match between
both interlocutors are more effective than simply
presenting the IF’s gaze to the IG and trusting the
IG to correctly interpret this data. If such a sys-
tem proved to be effective, it could point misun-
derstandings to the IG before either of the partici-
pants becomes aware of them.

Acknowledgements

This work was funded by the Cluster of Excel-
lence on “Multimodal Computing and Interaction”
of the German Excellence Initiative and the SFB
632 “Information Structure”.

816



References
Susan E. Brennan, Katharina S. Schuhmann, and

Karla M. Batres. 2013. Entrainment on the move
and in the lab: The walking around corpus. In Pro-
ceedings of the 35th Annual Conference of the Cog-
nitive Science Society, Berlin, Germany.

Herbert H. Clark and Meredyth A. Krych. 2004.
Speaking while monitoring addressees for under-
standing. Journal of Memory and Language,
50(1):62–81, January.

Herbert H. Clark. 1996. Using Language. Cambridge
University Press, May.

Nikos Engonopoulos, Martı́n Villalba, Ivan Titov, and
Alexander Koller. 2013. Predicting the resolution
of referring expressions from user behavior. In Pro-
ceedings of the Conference on Empirical Methods in
Natural Language Processing (EMNLP), Seattle.

Andrew Gargett, Konstantina Garoufi, Alexander
Koller, and Kristina Striegnitz. 2010. The give-
2 corpus of giving instructions in virtual environ-
ments. In Proceedings of the Seventh International
Conference on Language Resources and Evaluation
(LREC’10), Valletta, Malta, May. European Lan-
guage Resources Association (ELRA).

Joy E. Hanna and Susan E. Brennan. 2007. Speakers’
eye gaze disambiguates referring expressions early
during face-to-face conversation. Journal of Mem-
ory and Language, 57(4):596–615, November.

John M. Henderson and Tim J. Smith. 2007. How
are eye fixation durations controlled during scene
viewing? further evidence from a scene onset delay
paradigm. Visual Cognition, 17(6-7):1055–1082.

Casey Kennington and David Schlangen. 2014. Com-
paring listener gaze with predictions of an incremen-
tal reference resolution model. RefNet workshop on
Psychological and Computational Models of Refer-
ence Comprehension and Production.

Alexander Koller, Kristina Striegnitz, Andrew Gargett,
Donna Byron, Justine Cassell, Robert Dale, Johanna
Moore, and Jon Oberlander. 2010. Report on the
Second NLG Challenge on Generating Instructions
in Virtual Environments (GIVE-2). In Proceedings
of the 6th International Natural Language Genera-
tion Conference (INLG).

Alexander Koller, Maria Staudte, Konstantina Garoufi,
and Matthew Crocker. 2012. Enhancing referen-
tial success by tracking hearer gaze. In Proceed-
ings of the 13th Annual Meeting of the Special In-
terest Group on Discourse and Dialogue, SIGDIAL
’12, pages 30–39, Stroudsburg, PA, USA. Associa-
tion for Computational Linguistics.

Maria Staudte, Alexander Koller, Konstantina Garoufi,
and Matthew Crocker. 2012. Using listener gaze to
augment speech generation in a virtual 3D environ-
ment. In Proceedings of the 34th Annual Meeting of
the Cognitive Science Society (CogSci), Sapporo.

817


