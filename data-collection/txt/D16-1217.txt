



















































Does well-being translate on Twitter?


Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 2042–2047,
Austin, Texas, November 1-5, 2016. c©2016 Association for Computational Linguistics

Does ‘well-being’ translate on Twitter?

Laura K. Smith1 Salvatore Giorgi1 Rishi Solanki2 Johannes C. Eichstaedt1
H. Andrew Schwartz3 Muhammad Abdul-Mageed4 Anneke Buffone1 and Lyle H. Ungar5

1Department of Psychology, University of Pennsylvania
2Electrical and Systems Engineering, University of Pennsylvania

3Computer Science, Stony Brook University
4Library, Archival and Information Studies, University of British Columbia

5Computer and Information Science, University of Pennsylvania
lasm@sas.upenn.edu, sgiorgi@sas.upenn.edu

Abstract

We investigate whether psychological well-
being translates across English and Span-
ish Twitter, by building and comparing
source language and automatically translated
weighted lexica in English and Spanish. We
find that the source language models perform
substantially better than the machine trans-
lated versions. Moreover, manually correct-
ing translation errors does not improve model
performance, suggesting that meaningful cul-
tural information is being lost in translation.
Further work is needed to clarify when au-
tomatic translation of well-being lexica is ef-
fective and how it can be improved for cross-
cultural analysis.

1 Introduction

Interest in sentiment analysis spans academic and
commercial domains, with wide-ranging applica-
tions (Pang and Lee, 2008; Liu, 2012). While the
majority of tools for sentiment analysis have been
developed for English text, ideally sentiment and
emotion could be analyzed across many languages.
Does one need to build models for each language of
interest, or can models be applied cross-culturally?
More generally, how do cultures differ in the lan-
guage they use to express sentiment and feeling?

Sentiment in resource-poor languages has com-
monly been assessed by first translating text into En-
glish and then applying an English sentiment model
(Mohammad et al., 2016). This approach is eco-
nomical and efficient, as building each model of in-
terest in every target language is resource-intensive.
Yet it is not clear how much culturally specific

information and accuracy are lost in the transla-
tion process, and specifically how this varies across
languages, cultures, linguistic content, and corpora
(e.g., social media vs. news). While extensive work
has demonstrated that automatic machine transla-
tion (MT) methods are competitive when translat-
ing opinion in news and blogs, less research has ex-
amined the translation of sentiment on social me-
dia, and specifically on Twitter, known for its re-
striction of individual exchanges to short samples of
text (140 characters) and informal language. More-
over, research has not focused on translating subjec-
tive well-being specifically.

Beyond sentiment, this paper investigates how ex-
pressions of personal well-being translate between
English and Spanish on Twitter. We have English
and Spanish speakers annotate Tweets in their na-
tive language for five components of subjective well-
being (positive emotion, engagement, positive re-
lationships, meaning, and accomplishment) (Selig-
man, 2011). We then compare how well models
trained and tested in the same language compare
to (a) models developed in one language, and then
translated (using Google Translate) to the other lan-
guage (e.g., how well English models translated to
Spanish work on Spanish Tweets) and (b) how well
models developed in one language work on Tweets
translated from another language (e.g., how well En-
glish models work on Tweets translated from Span-
ish to English).

2 Related Work

There is a vast literature on sentiment analysis which
space precludes us from surveying; see (Liu, 2012)

2042



for an excellent overview. A small but rapidly grow-
ing camp is developing methods to estimate per-
sonality and emotion, asking “how does she feel?”
rather than “how much does she like the product?”
(Mohammad and Kiritchenko, 2015; Park et al.,
2014). In social media, the well-being of individuals
as well as communities has been studied, on various
platforms such as Facebook and Twitter (Bollen et
al., 2011; Schwartz et al., 2013; Eichstaedt et al.,
2015; Schwartz et al., 2016).

2.1 Translating sentiment

Past work has, on the whole, regarded state-of-the-
art automatic translation for sentiment analysis opti-
mistically. In assessing statistical MT, (Balahur and
Turchi, 2012) found that modern SMT systems can
produce reliable training data for languages other
than English. Comparative evaluations between En-
glish and Romanian (Mihalcea et al., 2007) and En-
glish and both Spanish and Romanian (Banea et al.,
2008) based on the English MPQA sentiment data
suggest that, in spite of word ambiguity in either
the source or target language, automatic translation
is a viable alternative to the construction of mod-
els in target languages. Wan (2008) shows that it
is useful to improve a system in a target language
(Chinese) by applying ensemble methods exploit-
ing sentiment-specific data and lexica from the tar-
get language and a source language (English). More
recent work has examined how sentiment changes
with translation between English and Arabic, also
finding that automatic translation of English texts
yields competitive results (Abdul-Mageed and Diab,
2014; Mohammad et al., 2016). However, translated
texts tend to lose sentiment information such that the
translated data is more neutral than the source lan-
guage (Salameh et al., 2015).

It is less obvious how well expressions of emo-
tion or subjective well-being translate between lan-
guages and cultures; the words for liking a phone
or TV may be more similar across cultures than the
ones for finding life and relationships satisfying, or
work meaningful and engaging.

2.2 Well-being

In contrast to classic sentiment analysis, well-being
is not restricted to positive and negative emo-
tion. In 2011, the psychologist Martin Selig-

man proposed PERMA (Seligman, 2011), a five-
dimensional model of well-being where ‘P’ stands
for positive emotion, ‘E’ is engagement, ‘R’ is pos-
itive relationships, ‘M’ is meaning, and ‘A’ is a
sense of accomplishment. PERMA is of interest to
this translation context because while the ‘P’ dimen-
sion maps relatively cleanly onto traditional con-
ceptions of sentiment (i.e., positive and negative
emotion), PERMA also includes social and cogni-
tive components which may be expressed with more
variation across languages and cultures. In recent
work, Schwartz et al. (2016) developed an English
PERMA model using Facebook data. In this pa-
per, we adopt a similar method when building our
message-level models over Tweets.

Governments around the world are increasingly
dedicating resources to the measurement of well-
being to complement traditional economic indica-
tors such as gross domestic product. Being able to
measure well-being across regions is not only be-
coming more important for institutions and policy-
makers, but also for private sector entities that want
to assess and promote the well-being of their orga-
nizations and customers. This raises the importance
of translation, given that resources for the measure-
ment of well-being are disproportionately available
in English.

3 Methods

We collected Spanish data using the Twitter API,
gathering 15.3 million geolocated Tweets between
September and November 2015 using a lati-
tude/longitude bounding box around Spain. This set
was reduced to messages containing only Spanish
using the Language Identification (LangID) Python
package (Lui and Baldwin, 2012). We restricted to
messages with an 80% or higher Spanish confidence
score as given by LangID. This resulted in 6.1 mil-
lion Tweets from 290,000 users. We selected 5,100
random messages from this set for annotation. En-
glish Tweets were similarly collected using the Twit-
ter API, restricted to the US, and filtered to be (pri-
marily) in English.

3.1 Annotating message-level data

Amazon’s Mechanical Turk (MTurk) was used to
annotate the 5,000 random English (American)

2043



Tweets1. CrowdFlower, an online crowdsourcing
platform similar to MTurk, but more widely used
in Europe, was used to annotate our 5,100 random
Spanish Tweets1. As the Tweets exclusively came
from Spain, raters were restricted to fluent Spanish
speakers who live in Spain.

On both MTurk and CrowdFlower, separate anno-
tation tasks were set up for each of the 10 PERMA
components (positive and negative dimensions for
the 5 components). Workers were given the defini-
tion of the PERMA construct, directions on how to
perform the task, and were presented with an exam-
ple annotation task. During the task workers were
asked to indicate “to what extent does this message
express” the construct in question on a scale from 1
(“Not at all”) to 7 (“Extremely”). Directions were
presented in English for the English task, and in
Spanish for the Spanish task. The Spanish instruc-
tions were translated manually from English by a
bilingual English-Spanish speaker and verified by an
additional bilingual speaker.

In the English task, two raters assessed each mes-
sage. If the raters disagreed by more than 3 points,
a rating was obtained from a third rater. It proved
more difficult to get raters for the Spanish task, even
on CrowdFlower. In some cases we were unable to
obtain even a single annotation for a given Tweet and
PERMA component.

3.2 Developing weighted lexica

Tweets were tokenzied using an emoticon-aware to-
kenizer, ‘happy fun tokenizer’1. We then extracted
unigrams and bigrams from each corpus, yielding
vocabularies of 5,430 and 4,697 ‘words’ in English
and Spanish, respectively. The presence/absence of
these unigrams and bigrams in each Tweet were used
as features in Lasso (L1 penalized regression) (Tib-
shirani, 1996) models to predict the average anno-
tation score for each of the crowdsourced PERMA
labels. Separate models, each consisting of regres-
sion weights for each term in the lexicon, were built
for each of the ten (five positive and five nega-
tive) PERMA components in both English and Span-
ish1. Each model was validated using 10-fold cross
validation, with Pearson correlations averaged over
the 10 positive/negative PERMA components. Re-

1 Available at www.wwbp.org.

sults are presented in Table 1. The models were
then transformed into a predictive lexicon using the
methods described in (Sap et al., 2014), where the
weights in the lexicon were derived from the above
Lasso regression model.

Model r
Spanish 0.36
English 0.36

Table 1: Performance as measured by Pearson r correlation av-
eraged over the 10 positive/negative PERMA components using

10-fold cross validation.

3.3 Translating the models
We used Google Translate to translate both the orig-
inal English and Spanish Tweets and the words in
the models. We also created versions of the trans-
lated models in which we manually corrected appar-
ent translation errors for 25 terms with the largest
regression coefficients for each of the 10 PERMA
components (the top 250 terms for each model).

3.4 Comparative evaluation
We evaluated how well the different models
worked, computing the Pearson correlations be-
tween message-level PERMA scores predicted from
the different models and the ground-truth annota-
tions. Lexica were built on 80% of the messages
and then evaluated on the remaining 20%. Figure
1 shows test accuracies. Comparing the English
and Spanish source language and machine translated
models, we observe substantially better performance
when models were built over the same language they
are applied to, i.e., using models built in Spanish to
predict on Spanish Tweets. Translating the mod-
els (e.g., translating an English model to Spanish
and using it on Spanish Tweets) or translating the
Tweets (e.g., translating Spanish Tweets to English
and using an English model) work substantially less
well, with translating the Tweets giving marginally
better performance than translating the models. Fi-
nally, we translate both the model and Tweets, giv-
ing slightly better performance than translating the
Tweets alone. Complete PERMA lexica were then
built over the entire message sets for public release.

3.5 Error Analysis
To quantify the errors in translation, we took the
25 top-weighted words in each component of the

2044



Figure 1: Performance (Pearson r correlation) between ground-
truth annotations and predicted lexica scores averaged over the

10 PERMA components.

PERMA lexicon (250 terms total) and manually
translated them with the help of a native Spanish
speaker. The manual translations were then com-
pared against the automatic translations. Out of the
top 25 words we calculated the percentage of cor-
rect automatic translations (when manual and auto-
matic translations matched) and averaged the per-
centages across positive and negative PERMA com-
ponents. The average percentage of correct transla-
tions is listed in Table 2 as correct trans.

These correctly translated terms were then com-
pared to the terms in the opposite source model (i.e.,
after translating English PERMA to Spanish, we
compared the translations with Spanish PERMA).
We calculated the percentage of the top 250 trans-
lated words missing in the 250 top words of the
source lexicon for each PERMA component and av-
eraged over the 10 components. This value is re-
ported in Table 2 as missing terms. For terms that
appeared in both the translated and source lexica we
compared their respective weights, calculating both
percentage of terms in which the weights were of
different signs and percentage of terms with sub-
stantially different weights. Again, these percent-
ages were averaged over the 10 PERMA compo-
nents. Percentages are reported in Table 2 as opp
sign and weight diff, respectively. To be considered
“substantially different” the two weights must differ
by a factor of 2. It is worth noting that at no point
were the translated and source weights equal (within
a tolerance of 10−5).

We then looked at the errors term by term. Out of
the 500 terms considered (top 250 words per source

source
lang

correct
trans

missing
terms

opp
sign

weight
diff

English 83% 81% 0.5% 6.9%
Spanish 74% 91% 0.0% 4.8%

Table 2: Summary of translation errors. Percentages are av-
eraged over the 10 PERMA components. Source lang is the

language of the model which was translated, correct trans is the

percentage of correct automatically translated words, missing

terms is the percentage of correct automatic translations within

the 250 top terms that did not appear in the top 250 words of

other source model, opp sign is the percentage of terms whose

sign switched between models, and weight diff is the percent-

age of terms whose weights between the two models were off

by a factor of two.

PERMA term weight(en)
weight

(es)
%

chg
POS M

(en)
mundo*
(world) 0.42 -0.18 143

NEG A
(en)

odio**
(hate) 0.29 2.19 87

NEG M
(en)

nadie***
(no one) 0.23 0.24 4.2

NEG R
(es)

sad**
(triste) 1.70 0.0012 100

NEG P
(es)

hate***
(odio) 1.81 1.75 3.3

Table 3: Examples of specific errors. Error types are denoted
by asterisks: * denotes a change in sign, ** denotes the largest

change in weight and *** denotes the smallest change in weight

per source model. Language listed under each PERMA cate-

gory is the language of the source model that was translated.

The % chg column is percentage change relative to the larger

weight. For clarity, under each term we include its translation.

language) only one term weight changed signs be-
tween models: “mundo” (world). The weight for
this term in the translated English to Spanish model
was 0.42 whereas the weight in the Spanish model
was -0.18, amounting to a 140% change. Next, for
each source model we report terms with the largest
and smallest differences in weight. These terms and
weights are reported in Table 3. The language ab-
breviation (“en” or “es”) listed under each PERMA
component is used to denote the source language we
translated from. For example, (en) indicates that
we started with English PERMA, translated it into
Spanish and then compared to Spanish PERMA.

2045



4 Discussion

The difference in performance between source and
machine translated models can be attributed to a few
main problems. First, the translation might be in-
accurate (e.g., from our corpus, “te” is not in fact
“tea”). We manually corrected translation errors in
the prediction models with the help of a native Span-
ish speaker, but found that translation error accounts
for marginal discrepancy between the source lan-
guage and machine translated models.

A second source of errors are translations which
are technically accurate, yet do not translate cultur-
ally. For instance, even though “andaluces” trans-
lated correctly into “Andalusians,” “Andalusia” (an
autonomous community in Spain) does not invoke
the same cultural meaning in English as it does for
Spaniards. A machine would be hard-pressed to
translate “Andalusia” into a relevant region within
the U.S. that might invoke similar popular sentiment.
Although Spanish and American people share some
holidays, musicians, and sports heroes, many of
these differ (e.g., “Iker Casillas” is not well known
in the U.S. and “La selectividad” may be similar to
the “SATs,” but this is not captured in MT).

A third source of error stems from cultural dif-
ferences, with certain topics resonating differently
cross-culturally. For instance, when comparing the
highest weighted positive terms across PERMA, re-
ligious language (e.g., “god,” “blessed”) appears in
English but not Spanish, fitting with the popular no-
tion that Americans are more religious than Euro-
peans. Spanish PERMA’s positive emotion com-
ponent contains multiple highly weighted instances
of laughter; none have high weights in the English
model. Highly weighted English negative emo-
tion terms are marked by general aggression (e.g.,
“kill,” “stupid”) whereas the highest weighted Span-
ish terms include derogatory terms for disliked peo-
ple (e.g., “douchebag,” “fool”). The American posi-
tive relationship component is marked by words like
“friend” and “friends,” while “sister” is weighted
more highly in Spanish PERMA.

Note that this is fundamentally a problem of do-
main adaptation rather than MT, as our error analy-
sis revealed that the majority of top-weighted terms
were exclusive to one source model. Different cul-
tures use different words (or at least vastly different

word frequencies) when revealing the same kind of
well-being. Exploring where the sentiment around a
similar concept diverges across languages can pro-
vide insight to researchers studying cross-cultural
variation.

4.1 Limitations

This work has significant limitations. First, the En-
glish and Spanish annotation processes, though kept
as similar as possible, were not identical; annota-
tions were gathered on different platforms, and due
to our difficulty in recruiting Spanish raters, our total
annotations per message varied across tasks. Addi-
tionally, the models were built over relatively small
corpora of 5,000 English Tweets and 5,100 Span-
ish Tweets. These Tweets came from different time
periods, which may further reduce similarity be-
tween the Spanish and English corpora. Finally, our
method does not account for the presence of various
sub-cultures within the United States and Spain.

5 Conclusion

In this work, we investigated how well expressions
of subjective well-being translate across English and
Spanish Twitter, finding that the source language
models performed substantially better than the ma-
chine translated versions. Moreover, manually cor-
recting translation errors in the top 250 terms of the
lexica did not improve model performance, suggest-
ing that meaningful cultural information was lost in
translation.

Our findings suggest that further work is
needed to understand when automatic translation of
language-based models will lead to competitive sen-
timent translation on social media and how such
translations can be improved. Cultural differences
seem more important than language differences, at
least for the tasks we studied here. We expect that
language indicators of personality and emotion will
similarly translate poorly, but that remains to be
studied.

Acknowledgments

The authors acknowledge support from the Temple-
ton Religion Trust (grant TRT-0048) and Bioibérica.

2046



References

Muhammad Abdul-Mageed and Mona T Diab. 2014.
Sana: A large scale multi-genre, multi-dialect lexicon
for arabic subjectivity and sentiment analysis. In Pro-
ceedings of the 9th edition of the Language Resources
and Evaluation Conference, LREC, pages 1162–1169.

Alexandra Balahur and Marco Turchi. 2012. Multilin-
gual sentiment analysis using machine translation? In
Proceedings of the 3rd Workshop on Computational
Approaches to Subjectivity and Sentiment Analysis,
WASSA, pages 52–60.

Carmen Banea, Rada Mihalcea, Janyce Wiebe, and
Samer Hassan. 2008. Multilingual subjectivity analy-
sis using machine translation. In Proceedings of the
2008 Conference on Empirical Methods in Natural
Language Processing, EMNLP, pages 127–135.

Johan Bollen, Huina Mao, and Alberto Pepe. 2011.
Modeling public mood and emotion: Twitter sentiment
and socio-economic phenomena. In Proceedings of
the Fifth International Conference on Weblogs and So-
cial Media, ICWSM, pages 450–453.

Johannes C Eichstaedt, H Andrew Schwartz, Margaret L
Kern, Gregory Park, Darwin R Labarthe, Raina M
Merchant, et al. 2015. Psychological language on
twitter predicts county-level heart disease mortality.
Psychological Science, 26(2):159–169.

Bing Liu. 2012. Sentiment analysis and opinion min-
ing. Synthesis lectures on human language technolo-
gies, 5(1):1–167.

Marco Lui and Timothy Baldwin. 2012. langid. py: An
off-the-shelf language identification tool. In Proceed-
ings of the ACL 2012 system demonstrations, ACL,
pages 25–30.

Rada Mihalcea, Carmen Banea, and Janyce Wiebe. 2007.
Learning multilingual subjective language via cross-
lingual projections. In Proceedings of the 45th Annual
Meeting of the Association for Computational Linguis-
tics, ACL, pages 976–983.

Saif M Mohammad and Svetlana Kiritchenko. 2015. Us-
ing hashtags to capture fine emotion categories from
tweets. Computational Intelligence, 31(2):301–326.

Saif M Mohammad, Mohammad Salameh, and Svetlana
Kiritchenko. 2016. How translation alters sentiment.
Journal of Artificial Intelligence Research, 55:95–130.

Bo Pang and Lillian Lee. 2008. Opinion mining and
sentiment analysis. Foundations and Trends in Infor-
mation Retrieval, 2(1–2):1–135.

Greg Park, H Andrew Schwartz, Johannes C Eichstaedt,
Margaret L Kern, David J Stillwell, Michal Kosinski,
et al. 2014. Automatic personality assessment through
social media language. Journal of Personality and So-
cial Psychology, 108:934–952.

Mohammad Salameh, Saif M Mohammad, and Svetlana
Kiritchenko. 2015. Sentiment after translation: A
case-study on arabic social media posts. In Proceed-
ings of the 2015 Conference of the North American
Chapter of the Association for Computational Linguis-
tics: Human Language Technologies, NAACL, pages
767–777.

Maarten Sap, Greg Park, Johannes C Eichstaedt, Mar-
garet L Kern, David J Stillwell, Michal Kosinski, et al.
2014. Developing age and gender predictive lexica
over social media. In Proceedings of the 2014 Con-
ference on Empirical Methods In Natural Language
Processing, EMNLP, pages 1146–1151.

H Andrew Schwartz, Johannes C Eichstaedt, Margaret L
Kern, Lukasz Dziurzynski, Richard E Lucas, Megha
Agrawal, et al. 2013. Characterizing geographic vari-
ation in well-being using tweets. In Proceedings of the
7th International AAAI Conference on Weblogs and
Social Media, ICWSM.

H Andrew Schwartz, Maarten Sap, Margaret L Kern,
Johannes C Eichstaedt, Adam Kapelner, Megha
Agrawal, et al. 2016. Predicting individual well-being
through the language of social media. In Biocom-
puting 2016: Proceedings of the Pacific Symposium,
pages 516–527.

Martin EP Seligman. 2011. Flourish. Free Press, New
York, NY.

Robert Tibshirani. 1996. Regression shrinkage and se-
lection via the lasso. Journal of the Royal Statistical
Society. Series B (Methodological), pages 267–288.

Xiaojun Wan. 2008. Using bilingual knowledge and en-
semble techniques for unsupervised Chinese sentiment
analysis. In Proceedings of the 2008 Conference on
Empirical Methods in Natural Language Processing,
EMNLP, pages 553–561.

2047


