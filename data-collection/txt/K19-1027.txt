



















































Unsupervised Neural Machine Translation with Future Rewarding


Proceedings of the 23rd Conference on Computational Natural Language Learning, pages 281–290
Hong Kong, China, November 3-4, 2019. c©2019 Association for Computational Linguistics

281

Unsupervised Neural Machine Translation with Future Rewarding

Xiangpeng Wei1,2, Yue Hu1,2∗, Luxi Xing1,2, Li Gao3
1Institute of Information Engineering, Chinese Academy of Sciences, Beijing, China

2School of Cyber Security, University of Chinese Academy of Sciences, Beijing, China
3Platform & Content Group, Tencent, Beijing, China

{weixiangpeng, huyue, xingluxi}@iie.ac.cn
leolgao@tencent.com

Abstract

In this paper, we alleviate the local optimality
of back-translation by learning a policy (takes
the form of an encoder-decoder and is defined
by its parameters) with future rewarding under
the reinforcement learning framework, which
aims to optimize the global word predictions
for unsupervised neural machine translation.
To this end, we design a novel reward function
to characterize high-quality translations from
two aspects: n-gram matching and semantic
adequacy. The n-gram matching is defined
as an alternative for the discrete BLEU met-
ric, and the semantic adequacy is used to mea-
sure the adequacy of conveying the meaning
of the source sentence to the target. During
training, our model strives for earning higher
rewards by learning to produce grammatically
more accurate and semantically more ade-
quate translations. Besides, a variational infer-
ence network (VIN) is proposed to constrain
the corresponding sentences in two languages
have the same or similar latent semantic code.
On the widely used WMT’14 English-French,
WMT’16 English-German and NIST Chinese-
to-English benchmarks, our models respec-
tively obtain 27.59/27.15, 19.65/23.42 and
22.40 BLEU points without using any labeled
data, demonstrating consistent improvements
over previous unsupervised NMT models.

1 Introduction

Neural Machine Translation (Sutskever et al.,
2014; Bahdanau et al., 2015) directly models
the entire translation process through training an
encoder-decoder model that has achieved remark-
able performance (Wu et al., 2016; Gehring et al.,
2017; Vaswani et al., 2017) when provided with
massive amounts of parallel corpora. However, the
lack of large-scale parallel data is a serious prob-
lem for the vast majority of language pairs.

∗Corresponding Author.

As a result, several works have recently tried to
get rid of the dependence on parallel corpora us-
ing unsupervised setting, in which the NMT model
only has access to two independent monolingual
corpora with one for each language (Lample et al.,
2018a; Artetxe et al., 2018b; Yang et al., 2018).
Among these works, the encoder and decoder act
as a standard auto-encoder (AE) that are trained to
reconstruct the inputs from their noised versions.
Due to the lack of cross-language signals, unsu-
pervised NMT usually requires pseudo parallel
data generated with the back-translation method
for achieving the final goal of translating between
source and target languages.

Back-translation typically uses beam
search (Sennrich et al., 2016a) or just greedy
search (Lample et al., 2018a,b) to generate
synthetic sentences. Both are approximate al-
gorithms to identify the maximum a posteriori
(MAP) output, i.e. the sentence with the highest
estimated probability given an input. Although
back-translation with MAP prediction has been
proved to be successful, it suffers from several
apparent issues when trained with maximum
likelihood estimation (MLE) only, including ex-
posure bias and loss-evaluation mismatch. Thus,
this method often fails to produce the optimal
synthetic sentences for the subsequent training.

In this paper, we address the problem men-
tioned above with future rewarding for unsuper-
vised NMT. The basic idea is to model the future
direction of a translation and optimize the global
word predictions under the policy gradient rein-
forcement learning framework. More concretely,
we sample N translations via the policy for each
input sentence and build a new objective func-
tion by combining the cross-entropy loss used in
prior works with sequence-level rewards from pol-
icy gradient reinforcement learning. We consider
the sequence-level reward from two aspects: 1) n-



282

gram matching, which is the precision or recall
of all sub-sequences of 1, 2, 3 and 4 tokens in
generated sequence and is responsible for mea-
suring the accuracy of surface word predictions;
2) semantic adequacy, which is the similarity be-
tween the underlying semantic representations of
the generated translation and the input sentence.
These two aspects of rewards are inspired by the
general criteria of what properties a high-quality
translation should have and are complementary to
each other. Additionally, a variational inference
network (VIN) is proposed to model the under-
lying semantics of monolingual sentences explic-
itly. It is used to map the source and target lan-
guages into a shared semantic space during auto-
encoding, as well as constrain the sentences and
their translated counterparts have the same or sim-
ilar semantic code during cross-language training.

The major contributions of this paper can be
summarized as follows:

• We propose a novel learning paradigm for un-
supervised NMT that models future rewards
to optimize the global word predictions via
policy gradient reinforcement learning. To
enforce the underlying semantic space, we in-
troduce a VIN into our model.

• We introduce an effective reward function
that jointly accounts for the n-gram match-
ing and the semantic adequacy of generated
translations.

• We conduct extensive experiments on
English-French, English-German and NIST
Chinese-to-English translation tasks. Ex-
perimental results show that the proposed
approach achieves significant improvements
across different language pairs.

2 Unsupervised Neural Machine
Translation

In this section, we first describe the composition of
the introduced model and then give details of the
newly proposed unsupervised training method.

2.1 Model Composition
The introduced translation model consists of six
components: including two encoders with shar-
ing last few layers, two completely independent
decoders with one for each language, and two
newly introduced VINs with one for each lan-
guage. For the encoders and decoders, we follow

Figure 1: Illustration of Variational Denosing Auto-
Encoding. The newly introduced VIN is highlighted in
red. Two aspects of losses are respectively abbreviated
as Llz and Llrec.

the recently emerged Transformer (Vaswani et al.,
2017). Specifically, each encoder is composed of
a stack of four identical layers, and each layer con-
sists of a multi-head self-attention sub-layer and a
fully connected feed-forward sub-layer. The en-
coders of the source and target languages are re-
spectively parameterized as Θencsrc and Θ

enc
tgt , and

the encoding operation is denoted as e(xl; Θencl ),
xl is the input sequence of word embeddings,
l ∈ {src, tgt}. The decoders are also composed
of four identical layers. In addition to the two sub-
layers in each encoder layer, the decoder inserts a
third sub-layer, which performs multi-head atten-
tion over the output of the encoder stack, the de-
tails we refer the reader to (Vaswani et al., 2017).
Similar to encoders, we denote source decoder as
Θdecsrc, target decoder as Θ

dec
tgt , and decoding oper-

ation as d(xl; Θdecl ), l ∈ {src, tgt}. For VINs,
each of them is composed of a standard Gaussian
distributionN (0,1) as the prior, and a neural pos-
terior that is implemented as feed-forward neural
network and parameterized by ψl, l ∈ {src, tgt}.

In this work, the entire model is trained in an un-
supervised manner by optimizing two objectives:
1) variational denoising auto-encoding; 2) cross-
language training with future rewarding.

2.2 Variational Denoising Auto-Encoding

Firstly, two auto-encoders are respectively trained
to learn to reconstruct their inputs. In this form,
each encoder should learn to compose the input
sentence of its corresponding language, and each
decoder is expected to learn to recover the original
input sentence from this composition. However,
without any constraint, the auto-encoder would
make very literal word-by-word copies, without
capturing any internal structure of the input sen-
tence involved. To address this issue, prior works



283

often adapt the same strategy as Denosing Auto-
Encoding (DAE) (Vincent et al., 2008), and add
some noise to the input sentences (Hill et al.,
2016). As shown in Figure 1, we augment the
DAE with a variational inference network (VIN)
to model underlying semantics of monolingual
sentences explicitly, which assumes that there ex-
ists a latent variable z from this semantic space.
And this variable, together with the noised input
sentence, guides the decoding process. With this
assumption, we define the objective function of re-
construction as follow:

Llrec = logPΘl→l(xl|z, C(xl)) (1)

where Θl→l = Θencl ◦Θdecl ◦ψl represents the com-
bination of Θencl , Θ

dec
l and ψl, l ∈ {src, tgt}. C

denotes a stochastic noise model, in which we ap-
ply the same method as in (Lample et al., 2018a).

The continuous latent variable z, acts as the un-
derlying semantics here, is approximated by a neu-
ral posterior inference network qψl(z|xl). Fol-
lowing (Kingma and Welling, 2014; Kingma et al.,
2014), the posterior approximation is regarded as a
diagonal Gaussian N (µ, diag(σ2)), and its mean
µ and variance σ2 are parameterized with deep
neural networks. We also reparameterize z as a
function of µ and σ (i.e., z = µ + σ � ε, ε is
a standard Gaussian variable that plays a role of
introducing noises) rather than using the standard
sampling method. We aim to map source and tar-
get languages into a shared semantic space and use
the following objective function for VINs:

Llz = −KL(qψl(z|xl)||N (0,1)) (2)

where l ∈ {src, tgt}. KL(Q||P ) is the Kullback-
Leibler divergence between Q and P .

We finally incorporate the auto-encoder and the
VIN into an end-to-end neural network, and the
overall training objective of auto-encoding is to
minimize the following loss function:

Llae = −(Llz + Llrec) (3)

2.3 Cross-language Training with Future
Rewarding

In spite of the auto-encoding, the second objective
of unsupervised NMT is to constrain the model to
be able to map an input sentence from the source
(target) language to the target (source) language.

Due to the lack of alignment information be-
tween two independent monolingual corpora, the

back-translation (Sennrich et al., 2016a) method
is used to synthetise a pseudo parallel corpus for
cross-language training. More concretely, given
an input sentence in one language, which can be
firstly translated into the other language (i.e. use
the corresponding encoder and the decoder of the
other language) by applying the model in infer-
ence mode with greedy decoding. And then, the
model is trained to reconstruct the original sen-
tence from this translation. The most widely used
method in previous works to train the model for
sequence generation, called maximum likelihood
estimation (MLE for short), it assumes that the
ground-truth is provided at each step during train-
ing. The objective of MLE is defined as the maxi-
mization of the following log-likelihood:

Ll1mle = logPΘl2→l1 (xl1 |zp, x̃l2) (4)

where Θl2→l1 = Θ
enc
l2
◦ Θdecl1 ◦ ψl2 represents

the combination of Θencl2 , Θ
dec
l1

and ψl2 . zp is ap-
proximated by the introduced VIN (i.e., reparam-
eterized from the Gaussian qψl2 (zp|x̃l2)). x̃l2 =
d(e(xl1 ; Θ

enc
l1

); Θdecl2 ) is obtained by greedy de-
coding in inference mode (l1 = src, l2 = tgt or
l1 = tgt, l2 = src).

2.3.1 Future Rewarding
Unfortunately, maximizing Ll1mle does not always
produce the best results on discrete evaluation
metrics such as BLEU (Papineni et al., 2002), as
the accumulation of errors caused by exposure
bias as well as the inconsistency between train-
ing and testing measurements lead to the models
tend to be short-sighted. We bridge the discrep-
ancy between training and testing modes caused
by MLE through learning a policy to model future
rewards, which can directly optimize the global
word predictions and is made possible with rein-
forcement learning, as illustrated in Figure 2. To
reduce the variance of the model, we use the self-
critical policy gradient learning algorithm (Rennie
et al., 2017).

For self-critical policy gradient learning, we
produce two separate output sequences at each
training iteration: x̂, the sampled translation,
which is obtained by sampling from the final out-
put probability distribution, and x̂g, the baseline
output, obtained by performing a greedy search.
Thus, the objective function of cross-language
training can be redefined as the expected advan-
tages of the sampled sequence over the baseline



284

Figure 2: Illustration of the proposed method for cross-language training with future rewarding. Three aspects
of losses are respectively abbreviated as Ll1

z′
, Ll1mle and L

l1
rl. And L

l1
z′

is an auxiliary function that constrains the
sentences and their translated counterparts in other language have the same or similar semantic codes.

sequence:

Ll1rl =EPΘl2→l1 (x̂l1 |zp,x̃l2 )[r(x̂l1)− r(x̂
g
l1

)]

=logPΘl2→l1 (x̂l1 |zp, x̃l2)× [r(x̂l1)− r(x̂
g
l1

)]

(5)

where a terminal reward r is observed after the
generation reaches the end of each sentence. It
is worth noting that considering a baseline reward
into training objective can reduce the variance of
the model. And we can see that maximizing Lrl
is equivalent to maximizing the conditional like-
lihood of the sampled sequence x̂ if it obtains a
higher reward than the baseline x̂g, thus increas-
ing the expected reward of our model.

2.3.2 Reward
r in Equation 5 denotes the sequence-level reward
that evaluates the quality of generated translations.
In this subsection, we discuss two major factors
that contribute to the success of a translation, that
is, n-gram matching and semantic adequacy, and
describe how to approximate these factors through
computable reward functions.

N-gram matching For a translation generated
by a NMT model, we need to measure the accu-
racy of surface word predictions. For that purpose,
the BLEU (Papineni et al., 2002) score is often
utilized in previous works. However, the BLEU
score has some undesirable properties when used
for single sentences, as it was designed to be a cor-
pus measure. Thus, we apply the smoothed ver-
sion of GLEU (Wu et al., 2016) as the reward for
measuring n-gram precision or recall. More con-
cretely, given a generated translation x̂l1 in one
language and the ground-truth reference xl1 , we
record all sub-sequences of 1, 2, 3 and 4 tokens in
x̂l1 and xl1 , and start all n-gram counts from 1 in-
stead of 0. Then we compute a recallRgleu, which
is the ratio of the number of matching n-grams to

the number of total n-grams in xl1 (ground-truth),
and a precision Pgleu, which is the ratio of the
number of matching n-grams to the number of to-
tal n-grams in x̂l1 (generated output). Finally, the
reward of the generated translation x̂l1 on n-gram
matching is defined as:

r1(x̂l1) = min{Rgleu, Pgleu} (6)

where r1 ranges from zero to one and it is sym-
metrical when switching x̂ and x.

Semantic adequacy We want the model can ad-
equately convey the meaning of the source sen-
tence to the target as much as possible. Thus, we
introduce another crucial reward function that is
used to measure the semantic adequacy of the gen-
erated translations. More concretely, for a gener-
ated translation x̂l1 in one language, we compute
the representation of x̂l1 as:

ei = TFIDF(wi), wi ∈ x̂l1
wi = ei/Sum(e1, e2, ..., eTx̂l1

)

cx̂l1 =

Tx̂l1∑
i=1

wix̂
i
l1

(7)

Identically, for the corresponding input sen-
tence in another language, its representation cx̃l2
can be extracted from the embedding matrix x̃l2 .
As the source and target word embeddings are of-
ten mapped to a shared-latent space in unsuper-
vised NMT, we therefore can directly use the fol-
lowing cosine similarity as the reward for semantic
adequacy:

r2(x̂l1) =
(cx̂l1 , cx̃l2 )

‖cx̂l1‖ · ‖cx̃l2‖
(8)

where (, ) indicates the dot product operation.



285

The final reward for a translation x̂l1 is a linear
combination of the rewards discussed above:

r(x̂l1) = r1(x̂l1) + r2(x̂l1) (9)

where r1(x̂l1) and r2(x̂l1) complement to each
other and work jointly to guide the learning of our
model. Note that the combination of these two as-
pects of rewards helps because it can prevent the
cases that the generated translation with high n-
gram matching but low semantic adequacy to have
relatively high rewards, and vice versa.

2.3.3 Overall Objective Function
In addition to the aforementioned MLE objective
function (Eq. 4) and the RL objective function
(Eq. 5), there is an auxiliary function that con-
strains the sentences and their translated counter-
parts have the same or similar semantic code and
is defined as:

Ll1
z′

= −KL(qψl1 (zq|xl1)||qψl2 (zp|x̃l2)) (10)

Finally, the overall training objective of cross-
language training is to minimize the following loss
function with hyperparameters η:

Ll1cl = −((1− η)(L
l1
mle + L

l1
z′

) + ηLl1rl) (11)

where η is a scaling factor. In the beginning of
the training η = 0, while as we move on with the
training we can increase the η to slowly reduce the
effect of MLE loss. And η is updated as follows:

η = min(0.8,max(0.0,
steps− ns
ne − ns

)) (12)

where steps is the global steps that the model has
been updated, ns and ne are the start and end steps
for increasing η respectively.

2.4 Training Procedure
There are two stages in the proposed unsupervised
training. In the first stage, we pre-train the pro-
posed model with denoising auto-encoding and
cross-language training, until no improvement is
achieved on the development set. This ensures
that the model starts with a much better policy
than random because now the model can focus on
the good part of the search space. In the second
state, we use an annealing schedule to teach the
model to produce stable sequences gradually. That
is, after the initial pre-training steps, we continue
training the model with future rewarding. During

each iteration, we perform one batch of denoising
auto-encoding and cross-language training for the
source as well as target languages alternately.

For model selection, we randomly extract 3000
source and target sentences to form a development
set. Following (Lample et al., 2018a), we trans-
late the source sentences to the target language and
then convert the resulting sentences back to the
source language. The quality of the model is then
evaluated by computing the BLEU score over the
original inputs and their reconstructions via this
two-step translation process. The performance is
finally averaged over two directions, and the se-
lected model is the one with the highest score.

3 Experiments

We mainly evaluate the proposed approach on the
widely used English-German, English-French and
NIST Chinese-to-English1 translation tasks.

3.1 Datasets

For English-French and English-German, we use
30M sentences from the WMT monolingual News
Crawl datasets from years 2007 through 2017.
We use the publicly available implementation of
Moses2 scripts for tokenization. Besides, we use a
shared vocabulary for source and target languages
with 60K subword tokens based on byte-pair en-
coding (Sennrich et al., 2016b). We remove sen-
tences longer than 50 subword-tokens. Experi-
mental results are reported on newstest2014 for
English-French translation and newstest2016 for
English-German translation. We adopt the same
method as in (Lample et al., 2018b) to obtain
cross-lingual embeddings.

For NIST Chinese-to-English translation, our
training data consists of 1.6M sentence pairs ran-
domly extracted from LDC corpora3, which has
been widely utilized by previous works. Simi-
lar to (Yang et al., 2018), we build the monolin-
gual dataset by randomly shuffling the Chinese
and English sentences respectively since the data
set is not big enough. We set the vocabulary size
to 30K for both Chinese and English. The av-
erage BLEU score over NIST02∼06 is reported

1The reason that we do not conduct experiments on
English-to-Chinese translation is that we do not get public
test sets for English-to-Chinese.

2http://www.statmt.org/moses/
3LDC2002E18, LDC2003E07, LDC2003E14, the

Hansards portion of LDC2004T07, LDC2004T08, and
LDC2005T06



286

en→fr fr→en en→de de→en zh→en
Existing Unsupervised NMT

Artetxe et al. (2018b) 15.13 15.56 - - -
Lample et al. (2018a) 15.05 14.31 9.64 13.33 -
Yang et al. (2018) 16.97 15.58 10.86 14.62 14.52
Lample et al. (2018b), NMT 25.14 24.18 17.16 21.00 -
Wu et al. (2019) 27.56 26.90 19.55 23.29 -
Song et al. (2019) 27.41 27.09 18.21 23.37 -

This work
MLE 25.47 24.51 17.04 21.13 18.26
(+Future Rewarding) 27.59 27.15 19.65 23.42 22.40

Table 1: Results of the proposed method in comparison to existing unsupervised NMT systems (BLEU).

in this paper. To pre-train cross-lingual embed-
dings, we utilize the monolingual corpora to train
the embeddings for each language independently
by using word2vec (Mikolov et al., 2013). Then
we apply the public implementation4 proposed
by Artetxe et al. (2017) to map these embeddings
into a shared latent space and keep the mapped
embeddings fixed during training.

For NIST Chinese-to-English, we apply case-
insensitive NIST BLEU computed by the script
mteval-v13a.pl to evaluate the translation perfor-
mance. For English-German and English-French,
we evaluate the translation performance with the
script multi-belu.pl.

3.2 Hyper-parameters
We set the following hyper-parameters: word em-
bedding dimension as 512, hidden size of self-
attention as 512, hidden size of fully connected
layers as 1024 and the head number as 8. We
share the last one layer of encoders in both lan-
guages. The dropout rate is set as 0.1, 0.3 and
0.2 during the training for En-Fr, En-De and Zh-
to-En, respectively. We perform a fixed number
of iterations (500K) to train each model, and set
ns = 300K, ne = 400K, for gradually increasing
the effect of future rewarding. We use the Adam
optimizer with a simple learning rate schedule: we
start with a learning rate of 10−4, after 300K up-
dates, we begin to halve the learning rate every
100K steps. We set the mini-batch size as 64. At
decoding time, we use greedy search.

3.3 Overall Results
Our method is compared with several previous un-
supervised NMT systems (Artetxe et al., 2018b;

4https://github.com/artetxem/vecmap

Lample et al., 2018a,b; Yang et al., 2018; Wu et al.,
2019; Song et al., 2019). Although, Song et al.
(2019) have achieved comparable results with su-
pervised NMT systems with larger monolingual
data (Wikipedia data) and bigger model5, we still
list the results that obtained with the same data
and model as ours for fair comparison. We also
consider a “Baseline” model, with the same ar-
chitecture as described in Section 2.1 except for
the variational inference network and is trained
using MLE only. We directly copy the experi-
mental results of previous models reported in their
papers and report the BLEU scores on English-
French, English-German and NIST Chinese-to-
English test sets in Table 1.

As shown in Table 1, our approach achieves
BLEU score of 27.59 and 27.15 on En→Fr and
Fr→En translations respectively, which outper-
forms Lample et al. (2018b) by more than 2 BLEU
points on both En→Fr and Fr→En. For the En-
De, we achieve 19.65 and 23.42 BLEU scores
on En→De and De→En respectively, with up to
+10.09 BLEU points improvement over previous
unsupervised NMT models. For the Chinese-to-
English translation, the proposed method leads to
a substantial improvement (up to 54%) over the
previous system showed in Yang et al. (2018).
Compared to baseline, our approach demonstrates
significant improvements by more than 2 BLEU
points over three benchmarks. These results indi-
cate that the newly proposed training method that
models future rewards to optimize global word
predictions for unsupervised NMT is promising
and enables the model to generate quality trans-
lations.

5Our model can also adopt such an advanced pre-training
technique, we leave this for feature work.



287

3.4 Analysis

In this section, we conduct some analysis over the
proposed method by taking English-French trans-
lation as an example.

3.4.1 Ablation Study
To understand the importance of different compo-
nents of the proposed system, we perform an ab-
lation study by training multiple versions of our
model with some missing components: the varia-
tional inference network and the future rewarding
method. Results are reported in Table 2. From
the table, we can see that removing the future
rewarding, and the accuracy drops by 0.98/1.02
BLEU points. Without the variational inference
networks, the accuracy decreases with 0.62/0.69
BLEU points. These findings demonstrate that
both the future rewarding and the VIN are impor-
tant, and both contribute to the improvement of
translation accuracy. The more critical component
is the future rewarding technology, which is vital
to optimize the global word predictions.

en-fr fr-en
Full Model 27.59 27.15
Without VINs 26.97 26.46
Without Future Rewarding 26.61 26.13

Table 2: Ablation study of our method on English-
French translation task.

3.4.2 Qualitative Comparison of
Back-translating

We perform qualitative evaluation on the pseudo
parallel data generated with the back-translation
method. To this end, we conduct a “round-trip”
translation (e.g., src → ˜tgt → ˆsrc), where src
and ˜tgt form a pseudo parallel corpus, ˆsrc is the
reconstruction from ˜tgt. We explore three settings
for qualitative evaluation: 1) UNKs, the ratio of the
number of unknown words to the number of total
words in ˜tgt; 2) the average over all sentences in
˜tgt with respect of their semantic adequacy, de-

noted as SA; 3) the BLEU scores over the origi-
nal inputs and their reconstructions, denoted as r-
BLEU. All settings are finally averaged over two
directions.

Results are shown in Table 3. The proposed
training method introduces significant boosts in all
of the three settings, with reducing 1.34% of un-
known words, increasing the semantic adequacy

UNKs SA r-BLEU
Baseline 3.51% 0.794 54.23
+Future Rewarding 2.17% 0.882 60.08

Table 3: Qualitative comparison of the generated
pseudo parallel sentences from the models trained with
MLE only and with the proposed training method on
English-French test set.

Better than Baseline
S: He put together a real feast for his fans to mark the

occasion.
R: Pour l’occasion, il a concocté un vrai festin pour

ses fans.
B: Il a mis en scène un vrai festin pour son public pour

marquer le souvenir.
O: Il a mis un vrai festin pour ses fans pour marquer

la circonstance.
S: Des scientifiques viennent de mettre en lumière la

façon dont les mouvements de la queue d’un chien
sont liés à son humeur.

R: Scientists have shed more light on how the move-
ments of a dog’s tail are linked to its mood.

B: Scientists come out of light the way the movements
of the tail of a dog are linked to his spirits.

O: Scientists come to light the way of the movements
of a dog’s tail are related to its mood.

Worse than Baseline
S: The recalled models were built between August 1

and September 10.
R: Les modèles rappelés ont été construits entre le 1er

août et le 10 septembre.
B: Les modèles rappelés ont été construits entre le 1er

août et le 10 septembre.
O: Les modèles de raconté ont été construits entre le

1er août et le 10 septembre.
S: Elles connaissent leur entreprise mieux que per-

sonne.
R: They know their business better than anyone else.
B: They know their business better than anyone else.
O: They know their company better than anyone.

Table 4: Translation examples from English-French
test set (English-to-French is above the dotted line and
French-to-English is below the dotted line). B: the
baseline model; O: our proposed model.

by 0.088 and improving r-BLEU points by 5.85.
This is in line with our expectations, as the pro-
posed future rewarding method is not optimized to
predict the next token, but rather to increase long-
term reward.

3.4.3 Example Translations
Table 4 shows four example translations. The
first part shows examples for which the proposed
model reached a higher BLEU score than the base-
line model. We find that the translation produced



288

by the baseline model doesn’t adequately con-
vey the meaning of the source sentence to the
target. By contrast, the proposed future reward-
ing method enables the model to generate trans-
lations that are more diversity while ensuring the
meaning of the source sentences, such as “circon-
stance” and “come to light”. The possible rea-
son is that we apply the semantic adequacy to re-
ward translations that have different syntax struc-
tures and expressions but share the same meaning
as the ground-truth sentence. The second part con-
tains examples where the baseline achieved better
BLEU score than our model, that is, in a few cases,
our model chooses inappropriate words that under
the same topic as reference words.

4 Related Work

In order to reduce the exposure bias and opti-
mize the metrics used to evaluate sequence mod-
eling tasks (like BLEU, ROUGE or METEOR)
directly, reinforcement learning (RL) has been
widely used in many of recent works on machine
translation (Ranzato et al., 2016; Shen et al., 2016;
He et al., 2017; Bahdanau et al., 2017; Li et al.,
2017), text summarization (Paulus et al., 2018; Wu
and Hu, 2018; Li et al., 2018; Wang et al., 2018),
dialogue generation (Li et al., 2016), and question
answering (Hu et al., 2018). However, our pro-
posed method is the first use in combination with
reinforcement learning for unsupervised NMT to
explicitly enhance back-translation.

Recently, motivated by the success of cross-
lingual embeddings (Artetxe et al., 2016; Zhang
et al., 2017; Conneau et al., 2017), several works
have tried to train NMT or SMT models using un-
supervised setting, in which the model only has
access to unlabeled data. For example, Lample
et al. (2018a) propose a model that consists of
a single encoder and a single decoder for both
languages, respectively responsible for encoding
source and target sentences to a shared latent space
and to decode from that latent space to the source
or target domain. Different from (Lample et al.,
2018a), Artetxe et al. (2018b) introduce a shared
encoder but two independent decoders with one
for each language. Both of these two works
mentioned above utilize denoising auto-encoding
to reconstruct their noisy inputs and incorporate
back-translation into cross-language training pro-
cedure. Further, Yang et al. (2018) extend the sin-
gle encoder by using two independent encoders

but sharing some partial weights, which are re-
sponsible for alleviating the weakness in keep-
ing language-specific characteristics of the shared
encoder. And the entire system is fine-tuned by
introducing two global GANs with one for each
language. More recently, Artetxe et al. (2018a)
and Lample et al. (2018b) propose an alternative
approach based on phrase-based statistical ma-
chine translation, which profits from the modu-
lar architecture of SMT. In addition, Lample et al.
(2018b) also introduce a novel cross-lingual em-
bedding training method which is particularly suit-
able for related languages (e.g., English-French
and English-German). Ren et al. (2019) intro-
duce SMT models as posterior regularization, in
which SMT and NMT models boost each other
through iterative back-translation in a unified EM
training algorithm. Wu et al. (2019) propose an
alternative for back-translation, , extract-edit, to
extract and then edit real sentences from the tar-
get monolingual corpora. Lample and Conneau
(2019) and Song et al. (2019) propose to pretrain
cross-lingual language models for the initializa-
tion stage of unsupervised neural machine trans-
lation, which is critical to the performance of their
proposed model. In contrast to theirs, we pro-
pose an effective training method for unsupervised
NMT that models future rewards to optimize the
global word predictions via neural policy rein-
forcement learning, which can be applied to arbi-
trary architectures and language pairs easily.

5 Conclusion

In this paper, we have proposed a novel learning
paradigm for unsupervised NMT that models fu-
ture rewards to optimize the global word predic-
tions via reinforcement learning, in which we de-
sign an effective reward function that jointly ac-
counts for the n-gram matching and the seman-
tic adequacy of generated translations. To con-
strain the corresponding sentences in two lan-
guages have the same or similar semantic code,
we also introduce a variational inference network
into the proposed model.

We test the proposed model on WMT’14
English-French, WMT’16 English-German and
NIST Chinese-to-English translation tasks. Ex-
periment results show that our approach leads to
significant improvements over various language
pairs, especially on distantly-related languages
such as Chinese and English.



289

6 Acknowledgments

We would like to thank the anonymous review-
ers for their valuable comments and suggestions.
This work was supported by the National Key Re-
search and Development Program of China (No.
2017YFB0803301). Yue Hu is the corresponding
author.

References
Mikel Artetxe, Gorka Labaka, and Eneko Agirre. 2016.

Learning principled bilingual mappings of word em-
beddings while preserving monolingual invariance.
In Proceedings of the 2016 Conference on Empirical
Methods in Natural Language Processing, EMNLP
2016, pages 2289–2294.

Mikel Artetxe, Gorka Labaka, and Eneko Agirre. 2017.
Learning bilingual word embeddings with (almost)
no bilingual data. In Proceedings of the 55th An-
nual Meeting of the Association for Computational
Linguistics, ACL 2017, pages 451–462.

Mikel Artetxe, Gorka Labaka, and Eneko Agirre.
2018a. Unsupervised statistical machine translation.
In Proceedings of the 2018 Conference on Empirical
Methods in Natural Language Processing, EMNLP
2018, pages 3632–3642.

Mikel Artetxe, Gorka Labaka, Eneko Agirre, and
Kyunghyun Cho. 2018b. Unsupervised neural ma-
chine translation. In ICLR 2018.

Dzmitry Bahdanau, Philemon Brakel, Kelvin Xu,
Anirudh Goyal, Ryan Lowe, Joelle Pineau, Aaron
Courville, and Yoshua Bengio. 2017. An actor-critic
algorithm for sequence prediction. In ICLR 2017.

Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Ben-
gio. 2015. Neural machine translation by jointly
learning to align and translate. In ICLR 2015.

Alexis Conneau, Guillaume Lample, Marc’Aurelio
Ranzato, Ludovic Denoyer, and Hervé Jégou.
2017. Word translation without parallel data. In
arXiv:1710.04087.

Jonas Gehring, Michael Auli, David Grangier, De-
nis Yarats, and Yann N Dauphin. 2017. Con-
volutional sequence to sequence learning. In
arXiv:1705.03122.

Di He, Hanqing Lu, Yingce Xia, Tao Qin, Liwei Wang,
and Tieyan Liu. 2017. Decoding with value net-
works for neural machine translation. In Advances
in Neural Information Processing Systems, NIPS
2017, pages 178–187.

Felix Hill, Kyunghyun Cho, and Anna Korhonen. 2016.
Learning distributed representations of sentences
from unlabelled data. In Proceedings of NAACL-
HLT 2016, pages 1367–1377.

Minghao Hu, Yuxing Peng, Zhen Huang, Xipeng Qiu,
Furu Wei, and Ming Zhou. 2018. Reinforced
mnemonic reader for machine reading comprehen-
sion. In Proceedings of the Twenty-Seventh Inter-
national Joint Conference on Artificial Intelligence,
IJCAI 2018, pages 4099–4106.

Diederik P Kingma, Shakir Mohamed, Danilo Jimenez
Rezende, and Max Welling. 2014. Semi-supervised
learning with deep generative models. In Advances
in Neural Information Processing Systems, NIPS
2014, pages 3581–3589.

Diederik P Kingma and Max Welling. 2014. Auto–
encoding variational bayes. In ICLR 2014.

Guillaume Lample and Alexis Conneau. 2019.
Cross–lingual language model pretraining. In
arXiv:1901.07291.

Guillaume Lample, Alexis Conneau, Ludovic De-
noyer, and Marc’Aurelio Ranzato. 2018a. Unsu-
pervised machine translation using monolingual cor-
pora only. In ICLR 2018.

Guillaume Lample, Myle Ott, Alexis Conneau, Lu-
dovic Denoyer, and Marc’Aurelio Ranzato. 2018b.
Phrase-based & neural unsupervised machine trans-
lation. In Proceedings of the 2018 Conference on
Empirical Methods in Natural Language Process-
ing, EMNLP 2018, pages 5039–5049.

Jiwei Li, Will Monroe, and Dan Jurafsky. 2017.
Learning to decode for future success. In
arXiv:1701.06549.

Jiwei Li, Will Monroe, Alan Ritter, Michel Galley,
Jianfeng Gao, and Dan Jurafsky. 2016. Deep rein-
forcement learning for dialogue generation. In Pro-
ceedings of the 2016 Conference on Empirical Meth-
ods in Natural Language Processing, ENMLP 2016,
page 1192–1202.

Piji Li, Lidong Bing, and Wai Lam. 2018. Actor-
critic based training framework for abstractive sum-
marization. In arXiv:1803.11070.

Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg Cor-
rado, and Jeffrey Dean. 2013. Distributed represen-
tations of words and phrases and their composition-
ality. In Advances in Neural Information Processing
Systems, NIPS 2013, pages 3111–3119.

Kishore Papineni, Salim Roukos, Todd Ward, and
Wei Jing Zhu. 2002. Bleu: a method for automatic
evaluation of machine translation. In Proceedings of
the 40th annual meeting on association for compu-
tational linguistics, ACL 2002, pages 311–318.

Romain Paulus, Caiming Xiong, and Richard Socher.
2018. A deep reinforced model for abstractive sum-
marization. In ICLR 2018.

Marc’Aurelio Ranzato, Sumit Chopra, Michael Auli,
and Wojciech Zaremba. 2016. Sequence level train-
ing with recurrent neural networks. In ICLR 2016.

http://aclweb.org/anthology/D/D16/D16-1250.pdf
http://aclweb.org/anthology/D/D16/D16-1250.pdf
https://doi.org/10.18653/v1/P17-1042
https://doi.org/10.18653/v1/P17-1042
https://aclanthology.info/papers/D18-1399/d18-1399
https://arxiv.org/pdf/1710.11041.pdf
https://arxiv.org/pdf/1710.11041.pdf
https://arxiv.org/abs/1607.07086
https://arxiv.org/abs/1607.07086
https://arxiv.org/abs/1409.0473
https://arxiv.org/abs/1409.0473
http://arxiv.org/abs/1710.04087
https://arxiv.org/abs/1705.03122
https://arxiv.org/abs/1705.03122
http://papers.nips.cc/paper/6622-decoding-with-value-networks-for-neural-machine-translation
http://papers.nips.cc/paper/6622-decoding-with-value-networks-for-neural-machine-translation
http://www.aclweb.org/anthology/N16-1162
http://www.aclweb.org/anthology/N16-1162
https://doi.org/10.24963/ijcai.2018/570
https://doi.org/10.24963/ijcai.2018/570
https://doi.org/10.24963/ijcai.2018/570
http://papers.nips.cc/paper/5352-semi-supervised-learning-with-deep-generative-models.pdf
http://papers.nips.cc/paper/5352-semi-supervised-learning-with-deep-generative-models.pdf
https://arxiv.org/abs/1312.6114
https://arxiv.org/abs/1312.6114
https://arxiv.org/abs/1901.07291
https://arxiv.org/pdf/1711.00043.pdf
https://arxiv.org/pdf/1711.00043.pdf
https://arxiv.org/pdf/1711.00043.pdf
https://aclanthology.info/papers/D18-1549/d18-1549
https://aclanthology.info/papers/D18-1549/d18-1549
http://arxiv.org/abs/1701.06549
http://anthology.aclweb.org/D/D16/D16-1127.pdf
http://anthology.aclweb.org/D/D16/D16-1127.pdf
https://arxiv.org/abs/1803.11070
https://arxiv.org/abs/1803.11070
https://arxiv.org/abs/1803.11070
http://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality
http://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality
http://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality
http://www.anthology.aclweb.org/P/P02/P02-1040.pdf
http://www.anthology.aclweb.org/P/P02/P02-1040.pdf
https://arxiv.org/abs/1705.04304
https://arxiv.org/abs/1705.04304
https://arxiv.org/abs/1511.06732
https://arxiv.org/abs/1511.06732


290

Shuo Ren, Zhirui Zhang, Shujie Liu, Ming Zhou, and
Shuai Ma. 2019. Unsupervised neural machine
translation with SMT as posterior regularization. In
arXiv:1901.04112.

Steven J Rennie, Etienne Marcheret, Youssef Mroueh,
Jarret Ross, and Vaibhava Goel. 2017. Self-critical
sequence training for image captioning. In 2017
IEEE Conference on Computer Vision and Pattern
Recognition, CVPR 2017, pages 1179–1195.

Rico Sennrich, Barry Haddow, and Alexandra Birch.
2016a. Improving neural machine translation mod-
els with monolingual data. In Proceedings of the
54th Annual Meeting of the Association for Compu-
tational Linguistics, ACL 2016, pages 86–96.

Rico Sennrich, Barry Haddow, and Alexandra Birch.
2016b. Neural machine translation of rare words
with subword units. In Proceedings of the 54th An-
nual Meeting of the Association for Computational
Linguistics, ACL 2016, pages 1715–1725.

Shiqi Shen, Yong Cheng, Zhongjun He, Wei He, Hua
Wu, Maosong Sun, and Yang Liu. 2016. Minimum
risk training for neural machine translation. In Pro-
ceedings of the 54th Annual Meeting of the Asso-
ciation for Computational Linguistics, ACL 2016,
pages 1683–1692.

Kaitao Song, Xu Tan, Tao Qin, Jianfeng Lu, and Tie-
Yan Liu. 2019. MASS: masked sequence to se-
quence pre-training for language generation. In
arXiv:1905.02450.

Ilya Sutskever, Oriol Vinyals, and Quoc V. Le. 2014.
Sequence to sequence learning with neural net-
works. In Advances in Neural Information Process-
ing Systems, NIPS 2014, pages 3104–3112.

Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob
Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz
Kaiser, and Illia Polosukhin. 2017. Attention is all
you need. In arXiv:1706.03762.

Pascal Vincent, Hugo Larochelle, Yoshua Bengio, and
Pierre Antoine Manzagol. 2008. Extracting and
composing robust features with denoising autoen-
coders. In Machine Learning, Proceedings of the
Twenty-Fifth International Conference, ICML 2008,
page 1096–1103.

Li Wang, Junlin Yao, Yunzhe Tao, Li Zhong, Wei Liu,
and Qiang Du. 2018. A reinforced topic-aware con-
volutional sequence-to-sequence model for abstrac-
tive text summarization. In arXiv:1805.03616.

Jiawei Wu, Xin Wang, and William Yang Wang. 2019.
Extract and edit: An alternative to back-translation
for unsupervised neural machine translation. In
arXiv:1904.02331.

Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V
Le, Mohammad Norouzi, Wolfgang Macherey,
Maxim Krikun, Yuan Cao, Qin Gao, and Klaus

Macherey. 2016. Google’s neural machine transla-
tion system: Bridging the gap between human and
machine translation. In arXiv:1609.08144.

Yuxiang Wu and Baotian Hu. 2018. Learning to extract
coherent summary via deep reinforcement learning.
In Proceedings of the Thirty-Second AAAI Confer-
ence on Artificial Intelligence, AAAI 2018, pages
5602–5609.

Zhen Yang, Wei Chen, Feng Wang, and Bo Xu.
2018. Unsupervised neural machine translation with
weight sharing. In arXiv:1804.09057.

Meng Zhang, Yang Liu, Huanbo Luan, and Maosong
Sun. 2017. Adversarial training for unsupervised
bilingual lexicon induction. In Proceedings of the
55th Annual Meeting of the Association for Compu-
tational Linguistics, ACL 2017, pages 1959–1970.

http://arxiv.org/abs/1901.04112
http://arxiv.org/abs/1901.04112
https://doi.org/10.1109/CVPR.2017.131
https://doi.org/10.1109/CVPR.2017.131
http://www.aclweb.org/anthology/P16-1009
http://www.aclweb.org/anthology/P16-1009
http://aclweb.org/anthology/P/P16/P16-1162.pdf
http://aclweb.org/anthology/P/P16/P16-1162.pdf
http://aclweb.org/anthology/P/P16/P16-1159.pdf
http://aclweb.org/anthology/P/P16/P16-1159.pdf
http://arxiv.org/abs/1905.02450
http://arxiv.org/abs/1905.02450
http://www.anthology.aclweb.org/D/D14/D14-1179.pdf
http://www.anthology.aclweb.org/D/D14/D14-1179.pdf
https://arxiv.org/abs/1706.03762
https://arxiv.org/abs/1706.03762
https://doi.org/10.1145/1390156.1390294
https://doi.org/10.1145/1390156.1390294
https://doi.org/10.1145/1390156.1390294
https://arxiv.org/abs/1805.03616
https://arxiv.org/abs/1805.03616
https://arxiv.org/abs/1805.03616
http://arxiv.org/abs/1904.02331
http://arxiv.org/abs/1904.02331
https://arxiv.org/pdf/1609.08144
https://arxiv.org/pdf/1609.08144
https://arxiv.org/pdf/1609.08144
https://www.aaai.org/ocs/index.php/AAAI/AAAI18/paper/view/16838
https://www.aaai.org/ocs/index.php/AAAI/AAAI18/paper/view/16838
https://arxiv.org/abs/1804.09057
https://arxiv.org/abs/1804.09057
https://doi.org/10.18653/v1/P17-1179
https://doi.org/10.18653/v1/P17-1179

