



















































Using Centroids of Word Embeddings and Word Mover's Distance for Biomedical Document Retrieval in Question Answering


Proceedings of the 15th Workshop on Biomedical Natural Language Processing, pages 114–118,
Berlin, Germany, August 12, 2016. c©2016 Association for Computational Linguistics

Using Centroids of Word Embeddings and Word Mover’s Distance for
Biomedical Document Retrieval in Question Answering

Georgios-Ioannis Brokos1, Prodromos Malakasiotis1,2 and Ion Androutsopoulos1,2
1Department of Informatics, Athens University of Economics and Business, Greece

Patission 76, GR-104 34 Athens, Greece
http://nlp.cs.aueb.gr

2Institute for Language and Speech Processing, Research Center ‘Athena’, Greece
Artemidos 6 & Epidavrou, GR-151 25 Maroussi, Athens, Greece

http://www.ilsp.gr

Abstract

We propose a document retrieval method
for question answering that represents
documents and questions as weighted cen-
troids of word embeddings and reranks
the retrieved documents with a relax-
ation of Word Mover’s Distance. Using
biomedical questions and documents from
BIOASQ, we show that our method is com-
petitive with PUBMED. With a top-k ap-
proximation, our method is fast, and easily
portable to other domains and languages.

1 Introduction

Biomedical experts (e.g., researchers, clinical doc-
tors) routinely need to search the biomedical lit-
erature to support research hypotheses, treat rare
syndromes, follow best practices etc. The most
widely used biomedical search engine is PUBMED,
with more than 24 million biomedical references
and abstracts, mostly of journal articles.1 To im-
prove their performance, biomedical search en-
gines often use large, manually curated ontolo-
gies, e.g., to identify biomedical terms and expand
queries with related terms.2 Biomedical experts,
however, report that search engines often miss rel-
evant documents and return many irrelevant ones.3

There is also growing interest for biomedical
question answering (QA) systems (Athenikos and
Han, 2010; Bauer and Berleant, 2012; Tsatsaro-
nis et al., 2015), which allow their users to specify
their information needs more precisely, as natural
language questions rather than Boolean queries,

1See http://www.ncbi.nlm.nih.gov/pubmed.
2PUBMED uses UMLS (http://www.nlm.nih.gov/

research/umls/). See also the GoPubMed search engine
(http://www.gopubmed.com/).

3Malakasiotis et al. (2014) summarize the findings of in-
terviews that investigated how biomedical experts search.

and aim to produce more concise answers. Docu-
ment retrieval is particularly important in biomed-
ical QA, since most of the information sought re-
sides in documents and is essential in later stages.

We propose a new document retrieval method.
Instead of representing documents and questions
as bags of words, we represent them as the cen-
troids of their word embeddings (Mikolov et al.,
2013; Pennington et al., 2014) and retrieve the
documents whose centroids are closer to the cen-
troid of the question. This allows retrieving rele-
vant documents that may have no common terms
with the question without query expansion. Using
biomedical questions from the BIOASQ competi-
tion (Tsatsaronis et al., 2015), we show that our
method combined with a relaxation of the recently
proposed Word Mover’s Distance (WMD) (Kusner
et al., 2015) is competitive with PUBMED. We also
show that with a top-k approximation, our method
is particularly fast, with no significant decrease
in effectiveness. Given that it does not require
ontologies, term extractors, or manually labeled
training data, our method could be easily ported
to other domains (e.g., legal texts) and languages.

2 The proposed method

The word embeddings and document centroids are
pre-computed. For each question, its centroid is
computed and the documents with the top-k near-
est (in terms of cosine similarity) centroids are re-
trieved (Fig. 1). The retrieved documents are then
optionally reranked using a relaxation of WMD.

2.1 Centroids of documents and questions

In the simplest case, the centroid ~t of a text t is
the sum of the embeddings of the tokens of t di-
vided by the number of tokens in t. Previous
work on hierarchical biomedical document clas-
sification (Kosmopoulos et al., 2016) reported im-

114



 

Irrelevant 
Relevant 
Relevant 

Relevant 

Irrelevant 

< 0.4, -3, …, 5> 

Which genes have been found mutated  
in grey platelet syndrome patients? 

 
… 

RWMD 

< 0.5, -3,…, 6> 

< 0.7, -12,…, 1> 

Relevant 

Relevant 
Irrelevant 

… 

Irrelevant 

Relevant 
< 0.4, -3, …, 5> 

< -0.7, -13,…, 1> 

1 

2 
3 

k-1 
k 

1 
2 
3 
4 

k 

question centroid 

centroids of documents 

Figure 1: Illustration of the proposed method.

proved performance when the IDF scores of the to-
kens are also taken into account as follows:

~t =

|V |∑
j=1

~wj · TF(wj , t) · IDF(wj)
|V |∑
j=1

TF(wj , t) · IDF(wj)
(1)

where |V | is the vocabulary size (approx. 1.7 mil-
lion words, ignoring stop words), wj is the j-th
vocabulary word, ~wj its embedding, TF(wj , t) the
term frequency of wj in t, and IDF(wj) the inverse
document frequency of wj (Manning et al., 2008).
We use the 200-dimensional word embeddings
of BIOASQ, obtained by applying WORD2VEC
(Mikolov et al., 2013) to approx. 11 million ab-
stracts from PubMed.4 The IDF scores are com-
puted on the 11 million abstracts.

2.2 Document retrieval and reranking
Given a question with centroid ~q, identifying the
documents with the k nearest centroids requires
computing the distance between ~q and each docu-
ment centroid, which is impractical for large doc-
ument collections. Efficient approximate top-k al-
gorithms, however, exist. They divide the vector
space into subspaces and use trees to index the in-
stances in each subspace (Arya et al., 1998; In-
dyk and Motwani, 1998; Andoni and Indyk, 2006;
Muja and Lowe, 2009). We show that with an
approximate top-k algorithm, document retrieval
is very fast, with no significant decrease in per-
formance. The top-k retrieved documents di are
ranked by decreasing (cosine) similarity of their
centroids to ~q. We call this method Cent when the

4The skipgram model of WORD2VEC was used, with hier-
archical softmax, 5-word windows, and default other param-
eters. See http://participants-area.bioasq.
org/info/BioASQword2vec/ for further details.

simple (no IDF) centroids are used, and CentIDF
when the IDF-weighted centroids (Eq. 1) are used.

The top-k documents are optionally reranked
with an approximation of the WMD distance. WMD
measures the total distance the word embeddings
of two texts (in our case, question and document)
have to travel to become identical. In its full form,
WMD allows each word embedding to be partially
aligned (travel) to multiple word embeddings of
the other text, which requires solving a linear pro-
gram and is too slow for our purposes. Kusner et
al. (2015) reported promising results in text classi-
fication using WMD as the distance of a k-NN clas-
sifier. They also introduced relaxed, much faster
WMD versions. In our case, the first relaxation
(RWMD-Q) sums the distances the word embed-
dings ~w of the question q have to travel to the clos-
est word embeddings ~w′ of the document d:

RWMD-Q(q, d) =
∑
w∈q

min
w′∈d

dist(~w, ~w′) (2)

Following Kusner et al., we use the Euclidean dis-
tance as dist(~w, ~w′). Similarly, the second relaxed
form (RWMD-D) sums the distances of the word
embeddings of d to the closest embeddings of q. If
we set dist(~w, ~w′) = 1 if w, w′ are identical and 0
otherwise, RWMD-Q counts how many words of q
are present in d, and RWMD-D counts the words of
d that are present in q. Kusner et al. found the max-
imum of RWMD-Q and RWMD-D (RWMD-MAX) to
be the best relaxation of WMD. In our case, where
q is much shorter than d, RWMD-Q works much
better, because d contains many irrelevant words
that have no close counter-parts in q, and their
long distances dominate in RWMD-D and RWMD-
MAX.5 We call CentIDF-RWMD-Q and CentIDF-
RWMD-D the CentIDF method with the additional
reranking by RWMD-Q or RWMD-D, respectively.

3 Experiments

3.1 Data
We used the 1,307 training questions and the gold
relevant PUBMED document ids of the fourth year
of BIOASQ (Task 4b).6 The questions were writ-
ten by biomedical experts, who also identified the

5We do not report results with RWMD-MAX reranking, be-
cause they are as bad as results with RWMD-D.

6The questions and gold document ids are available from
http://participants-area.bioasq.org/. The
1,307 questions are all the training and test questions of the
previous years of BIOASQ, which were available to the par-
ticipants of the fourth year. We use all the 1,307 questions for
testing, since our method is unsupervised.

115



Recall
0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1

MI
P

0
0.05
0.1

0.15
0.2

0.25
0.3

0.35
0.4

0.45
0.5

c
c idf
p m se
c idf rwmd q
c idf rwmd d
p m se rwmd q
h

Figure 2: Mean Interpolated Precision at 11 recall
points, for k (documents to retrieve) set to 1,000.

gold relevant documents using PUBMED, and re-
flect real needs (Tsatsaronis et al., 2015). We pass
each question to our methods (after tokenization
and stop-word removal) or the PUBMED search
engine (hereafter PubMedSE), which performs its
own tokenization and query expansion.7

The document collection that we search con-
tains approx. 14 million article abstracts and titles
from the November 2015 PUBMED dump, which
was also used in the fourth year of BIOASQ.8 Our
methods view each document as a concatenation
of the title and abstract of an article.9 The titles
and abstracts have an average length of approx. 13
and 143 tokens, respectively. When comparing
against PubMedSE, we ignore documents returned
by PubMedSE that are not in the dump, but this is
very rare and does not affect the results.

3.2 Experimental results
Figures 2–4 show Mean Interpolated Precision
(MIP) at 11 recall levels, Mean Average Interpo-
lated Precision (MAIP), Mean Average Precision
(MAP), and Normalized Discounted Cumulative
Gain (nDCG).10 Roughly speaking, MAIP is the
area under the MIP curve, MAP is the same area
without interpolation, and nDCG is an alternative

7We use relevance ranking (not recency) in PubMedSE.
8The dump is available from https://www.nlm.

nih.gov/databases/license/license.html.
The 14 million articles do not include approx. 10 million
articles for which only titles are provided. There are hardly
any title-only gold relevant documents, and PubMedSE very
rarely returns title-only documents.

9It is unclear to us if PUBMED also searches the full texts
of the articles, which may put our methods at a disadvantage.

10All measures are widely used (Manning et al., 2008). We
use binary relevance in nDCG, as in the BIOASQ dataset.

5.7
1%

4.9
4%

9.0
9%

8.0
2%10.

00%

8.6
6%

16.
04%

14.
35%15.
96%

14.
28%

4.2
6%

3.6
8%

12.
53%

11.
16%

17.
43%

15.
54%17.

37%

15.
49%

MAIP MAP

c
c idf
p m se
c idf rwmd q
ann c idf rwmd q
c idf rwmd d
p m se rwmd q
h
ann h

Figure 3: MAIP and MAP scores, for k (documents
to retrieve) set to 1,000.

to MAIP. Unless otherwise stated, the number of
retrieved documents is set to k = 1,000.

Figure 2 shows that Cent performs much worse
than CentIDF. At low recall, CentIDF is as good
as PubMedSE, but PubMedSE outperforms CentIDF
at high recall. Reranking the top-k documents
of CentIDF by RWMD-Q has a significant impact,
leading to a system (CentIDF-RWMD-Q) that per-
forms better or as good as PubMedSE up to 0.7 re-
call. Reranking the top-k documents of PubMedSE
by RWMD-Q (PubMedSE-RWMD-Q) also improves
the performance of PubMedSE. Reranking the
top-k documents of CentIDF by RWMD-D (or
RWMD-MAX, not shown) leads to much worse re-
sults (CentIDF-RWMD-D), for reasons already ex-
plained.11 Similar conclusions are reached by ex-
amining the MAIP, MAP, and nDCG scores.

Keyword-based information retrieval may miss
relevant documents that use different terms
than the question, even with query expan-
sion. PubMedSE retrieves no documents for
35% (460/1307) of our questions.12 Further ex-
periments (not reported), however, indicate that
PubMedSE has higher precision than CentIDF-
RWMD-Q, when PubMedSE returns documents, at
the expense of lower recall. Hence, there is scope
to combine PubMedSE with our methods. As a
first, crude step, we tested a method (Hybrid) that
returns the documents of CentIDF-RWMD-Q when
PubMedSE retrieves no documents, and those of

11The same holds when the top-k documents of PubMedSE
are reranked by RWMD-D or RWMD-MAX (not shown).

12The experts that identified the gold relevant documents
used simple keyword, Boolean, and advanced PubMedSE
queries, whereas we used the English questions as queries.

116



nDCG@20 nDCG@100

9.3
8% 12.

45%13.
58% 17

.27
%

13.
63% 15.
33%

23.
23% 27

.04
%

23.
14% 26

.93
%

6.9
7% 10

.93
%17

.20
%

18.
17%

24.
32% 26.
60%

24.
26% 26.
52%

cent
centidf
pubmedse
centidf-rwmd-q
ann-centidf-rwmd-q
centidf-rwmd-d
pubmedse-rwmd-q
hybrid
ann-hybrid

Figure 4: nDCG@k, for k = 20 and k = 100.

System Search Reranking Total

CentIDF-RWMD-Q 47.41
(±1.22)

14.45
(±6.15) 61.86

ANN-CentIDF-RWMD-Q 0.36
(±0.04)

14.24
(±6.06) 14.60

Table 1: Average times (in seconds) over all the
questions of the dataset (k = 1000).

PubMedSE-RWMD-Q otherwise. Hybrid had the
best results in our experiments; the only excep-
tion was its nDCG@100 score, which was slightly
lower than the score of CentIDF-RWMD-Q.

Table 1 shows that an approximate top-k algo-
rithm (ANN) in CentIDF-RWMD-Q (ANN-CentIDF-
RWMD-Q) reduces dramatically the time to obtain
the top-k documents, with a very small decrease in
MAIP, MAP, and nDCG scores (Figures 3 and 4).13

We also compared against the other participants
of the second year of BIOASQ; the participant re-
sults of later years are not yet available.14 The
official BIOASQ score is MAP; MIP, MAIP, and
nDCG scores are not provided. Our best method
was again Hybrid (avg. MAP over the five batches
of the second year 16.18%). It performed over-
all better than the BIOASQ ‘baselines’ (best avg.
MAP 15.60%) and all eight participants, except for
the best one (avg. MAP 28.20%). The best sys-
tem (Choi and Choi, 2014) used dependency IR
models (Metzler and Croft, 2005), combined with
UMLS and query expansion heuristics (e.g., adding

13We use Annoy (https://github.com/spotify/
annoy), 100 trees, 1,000 neighbors, search-k = 10 · |trees| ·
|neighbors|. Times on a server with 4 Intel Xeon E5620 CPUs
(16 cores total), at 2.4 GHz, with 128 GB RAM.

14We used the evaluation platform of BIOASQ (http://
participants-area.bioasq.org/oracle).

the titles of the top-k initially retrieved documents
to the query). The ‘baselines’ are actually very
competitive; no system beat them in the first year,
and only one was better in the second year. They
are PubMedSE, but using BIOASQ-specific heuris-
tics (e.g., instructing PubMedSE to ignore types of
articles the experts did not consider). Our system
is simpler and does not use heuristics; hence, it can
be ported more easily to other domains.

4 Other related work

Kosmopoulos et al. (2016) reports that a k-NN
classifier that represents articles as IDF-weighted
centroids (Eq. 1) of 200-dimensional word embed-
dings (200 features) is as good at assigning seman-
tic labels (MeSH headings) to biomedical articles
as when using millions of bag-of-word features,
reducing significantly the training and classifica-
tion times. To our knowledge, our work is the first
attempt to use IDF-weighted centroids of word em-
beddings in information retrieval, and the first to
use WMD to rerank the retrieved documents. More
elaborate methods to encode texts as vectors have
been proposed (Le and Mikolov, 2014; Kiros et
al., 2015; Hill et al., 2016) and they could be used
as alternatives to centroids of word embeddings,
though the latter are simpler and faster to compute.

The OHSUMED dataset (Hersh et al., 1994) is
often used in biomedical information retrieval ex-
periments. It is much smaller (101 queries, ap-
prox. 350K documents) than the BIOASQ dataset
that we used, but we plan to experiment with
OHSUMED in future work for completeness.

5 Conclusions and future work

We proposed a new QA driven document retrieval
method that represents documents and questions
as IDF-weighted centroids of word embeddings.
Combined with a relaxation of the WMD distance,
our method is competitive with PUBMED, without
ontologies and query expansion. Combined with
PUBMED, it performs better than PUBMED on its
own. With a top-k approximation, it is fast, and
easily portable to other domains and languages.

We plan to consider alternative dense vector en-
codings of documents and queries, textual entail-
ment (Bowman et al., 2015; Rocktäschel et al.,
2016), and full-text documents, where it may be
necessary to extend RWMD-Q to take into account
the proximity (density) of the words of the (now
longer) document the query words are mapped to.

117



Acknowledgments

The work of the second author was funded by
the Athens University of Economics and Business
Research Support Program 2014-2015, “Action 2:
Support to Post-doctoral Researchers”.

References
A. Andoni and P. Indyk. 2006. Near-optimal hashing

algorithms for approximate nearest neighbor in high
dimensions. In Proc. of the 47th Annual IEEE Sym-
posium on Foundations of Computer Science, pages
459–468, Washington, DC.

S. Arya, D. M. Mount, N. S. Netanyahu, R. Silverman,
and A. Y. Wu. 1998. An optimal algorithm for ap-
proximate nearest neighbor searching fixed dimen-
sions. Journal of the ACM, 45(6):891–923.

S. J. Athenikos and H. Han. 2010. Biomedical ques-
tion answering: A survey. Computer Methods and
Programs in Biomedicine, 99(1):1–24.

M. A. Bauer and D. Berleant. 2012. Usability survey
of biomedical question answering systems. Human
Genomics, 6(1)(17).

S. R. Bowman, G. Angeli, C. Potts, and C. D. Manning.
2015. A large annotated corpus for learning natu-
ral language inference. In Proc. of the 2015 Con-
ference on Empirical Methods in Natural Language
Processing, Lisbon, Portugal.

S. Choi and J. Choi. 2014. Classification and retrieval
of biomedical literatures: SNUMedinfo at CLEF
QA track BioASQ 2014. In Proc. of the QA Lab
of the 5th Conference and Labs of the Evaluation
Forum, pages 1283–1295, Valencia, Spain.

W. Hersh, C. Buckley, T. J. Leone, and D. Hickam.
1994. OHSUMED: An interactive retrieval evalu-
ation and new large test collection for research. In
Proc. of the 17th Annual International ACM SIGIR
Conference on Research and Development in Infor-
mation Retrieval, pages 192–201, Dublin, Ireland.

F. Hill, K. Cho, and A. Korhonen. 2016. Learning
distributed representations of sentences from unla-
belled data. arXiv preprint 1602.03483.

P. Indyk and R. Motwani. 1998. Approximate nearest
neighbors: Towards removing the curse of dimen-
sionality. In Proc. of the 30th Annual ACM Sympo-
sium on Theory of Computing, pages 604–613, Dal-
las, TX.

R. Kiros, Y. Zhu, R. R. Salakhutdinov, R. Zemel, R. Ur-
tasun, A. Torralba, and S. Fidler. 2015. Skip-
thought vectors. In Advances in Neural Information
Processing Systems 28, pages 3276–3284. Montréal,
Canada.

A. Kosmopoulos, I. Androutsopoulos, and
G. Paliouras. 2016. Biomedical semantic in-
dexing using dense word vectors in BioASQ.
Journal Of Biomedical Semantics, Supplement On
Biomedical Information Retrieval. To appear.

M. Kusner, Y. Sun, N. Kolkin, and K. Q. Weinberger.
2015. From word embeddings to document dis-
tances. In Proc. of the 32nd International Confer-
ence on Machine Learning, pages 957–966, Lille,
France.

Q. Le and T. Mikolov. 2014. Distributed representa-
tions of words and phrases. In Proc. of the 31st In-
ternational Conference on Machine Learning, pages
1188–1196, Beijing, China.

P. Malakasiotis, I. Androutsopoulos, A. Bernadou,
N. Chatzidiakou, E. Papaki, P. Constantopoulos,
I. Pavlopoulos, A. Krithara, Y. Almyrantis, D. Poly-
chronopoulos, A. Kosmopoulos, G. Balikas, I. Parta-
las, G. Tsatsaronis, and N. Heino. 2014. Challenge
Evaluation Report 2 and Roadmap. BioASQ deliv-
erable D5.4.

C.D. Manning, P. Raghavan, and H. Schütze. 2008.
Introduction to Information Retrieval. Cambridge
University Press.

D. Metzler and W.B. Croft. 2005. A Markov Ran-
dom Field model for term dependencies. In Proc. of
the 28th Annual International ACM SIGIR confer-
ence on Research and Development in Information
Retrieval, pages 472–479, Salvador, Brazil.

T. Mikolov, W. Yih, and G. Zweig. 2013. Distributed
representations of words and phrases and their com-
positionality. In Proc. of the Conference on Neural
Information Processing Systems, Lake Tahoe, NV.

M. Muja and D. G. Lowe. 2009. Fast approximate
nearest neighbors with automatic algorithm config-
uration. In Proc. of the International Conference
on Computer Vision Theory and Applications, pages
331–340, Lisboa, Portugal.

J. Pennington, R. Socher, and C. D. Manning. 2014.
GloVe: Global vectors for word representation. In
Proc. of the Conference on Empirical Methods on
Natural Language Processing, Doha, Qatar.

T. Rocktäschel, E. Grefenstette, K. M. Hermann,
T. Kočiskỳ, and P. Blunsom. 2016. Reasoning about
entailment with neural attention. In International
Conference on Learning Representations, San Juan,
Puerto Rico.

G. Tsatsaronis, G. Balikas, P. Malakasiotis, I. Parta-
las, M. Zschunke, M.R. Alvers, D. Weissenborn,
A. Krithara, S. Petridis, D. Polychronopoulos,
Y. Almirantis, J. Pavlopoulos, N. Baskiotis, P. Galli-
nari, T. Artieres, A. Ngonga, N. Heino, E. Gaussier,
L. Barrio-Alvers, M. Schroeder, I. Androutsopou-
los, and G. Paliouras. 2015. An overview of the
BioASQ large-scale biomedical semantic indexing
and question answering competition. BMC Bioin-
formatics, 16(138).

118


