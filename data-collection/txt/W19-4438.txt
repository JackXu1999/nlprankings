



















































Measuring Text Complexity for Italian as a Second Language Learning Purposes


Proceedings of the Fourteenth Workshop on Innovative Use of NLP for Building Educational Applications, pages 360–368
Florence, Italy, August 2, 2019. c©2019 Association for Computational Linguistics

360

Measuring Text Complexity for Italian as a Second Language Learning
Purposes

Luciana Forti1, Alfredo Milani2, Luisa Piersanti2,
Filippo Santarelli1, Valentino Santucci1, Stefania Spina1

1Department of Humanities and Social Sciences,
University for Foreigners of Perugia, Perugia (Italy)

2Department of Mathematics and Computer Science,
University of Perugia, Perugia (Italy)

{luciana.forti,valentino.santucci,stefania.spina}@unistrapg.it
luisa.piersanti@studenti.unipg.it, alfredo.milani@unipg.it

filippo.santarelli@unicam.it

Abstract
The selection of texts for second language
learning purposes typically relies on teach-
ers’ and test developers’ individual judgment
of the observable qualitative properties of a
text. Little or no consideration is generally
given to the quantitative dimension within an
evidence-based framework of reproducibility.
This study aims to fill the gap by evaluating the
effectiveness of an automatic tool trained to as-
sess text complexity in the context of Italian as
a second language learning. A dataset of texts
labeled by expert test developers was used to
evaluate the performance of three classifier
models (decision tree, random forest, and sup-
port vector machine), which were trained us-
ing linguistic features measured quantitatively
and extracted from the texts. The experimen-
tal analysis provided satisfactory results, also
in relation to which kind of linguistic trait con-
tributed the most to the final outcome.

1 Introduction

The task of automatically classifying a text ac-
cording to its different levels of complexity has
had various applications in a number of different
fields. It is key in mood and sentiment analysis,
in the detection of hate speech, in text simplifica-
tion, and also in the assessment of text readability
in relation to both native and non-native readers.

Being able to select a text and compare it with
others is a central concern in the field of second
language learning. When choosing a text to be
used in a lesson or as part of a language test, a
teacher and/or language test developer will gener-
ally assess the suitability of that text on the basis
of several aspects: the need to adhere to a specific
syllabus and curriculum, as well as general guide-
lines and test specifications. Other aspects that are
considered include learner-related variables such

as their linguistic needs, their educational back-
ground, and their age, all elements involving other
aspects such as text genre, text type, tasks to be
assigned to the text, and so on.

According to the literature, there is wide con-
sensus on specific characteristics that can influ-
ence the difficulty of a text in the context of a
reading comprehension task. These characteris-
tics have a role in terms of the cognitive demands
that a text will impose on its reader (Bachman
and Palmer, 2010)(Purpura, 2014). These char-
acteristics are text length, grammatical complex-
ity, word frequency, cohesion, rhetorical organiza-
tion, genre, text abstractness, subject knowledge
and cultural knowledge. All these aspects relate
to readability, and are often evaluated intuitively
and subjectively by individual experienced teach-
ers, who will then use a given text deemed to be
representative of a certain proficiency level in a
lesson or as part of a test.

Although this kind of sensitivity to the text is
extremely valuable, especially when adapting a
lesson or test item to the specific needs of a group
of learners, its limitations are evident for at least
two reasons: the evaluation is performed by sin-
gle teachers or test developers at the one time and
it is not reproducible; the evaluation is conducted
solely on the basis of observable qualitative fea-
tures of a text.

In the context of language assessment, text se-
lection for the purposes of a reading comprehen-
sion task has considerable implications with re-
gard to the interpretation of test scores: a text sub-
jectively deemed suitable for a given proficiency
level, which would have objectively been deemed
otherwise, will inevitably hinder the validity of the
overall testing process. The same can be argued
for text selection aimed at lesson planning: an in-



361

adequate text chosen for a given class will hinder
the whole learning process.

As a result, an automatic system able to used ex-
tract objective and reproducible information about
a text, combining qualitative and quantitative data,
is highly desirable in the field of second language
learning, though still largely lacking, especially
for the Italian language and in relation to differ-
ent proficiency levels. The Common European
Framework of Reference for Languages (CEFR)
descriptors are unable to provide this kind of sup-
port in relation to the readability of a text.

In this study, we assess the effectiveness of an
automatic classification tool for the evaluation of
text complexity in Italian. We used a dataset of
texts used at CVCL, Centro Valutazione Certifi-
cazioni Linguistiche, one of the main Italian lan-
guage testing centres with sections all over the
world, based at the University for Foreigners of
Perugia. Each text in the dataset was labeled by
test development experts according to the CEFR
descriptors. The dataset was used to train a clas-
sification model, enabling it to automatically pre-
dict the proficiency level of any text in input. The
dataset was used to test three different classifiers:
decision tree, random forest and support vector
machine. The main difference between this study
and the related work in the field that will be de-
scribed in the following paragraph is that a set
of linguistic features is used to distinguish texts
from the perspective of CEFR levels. Therefore,
linguistic features measured quantitatively and ex-
tracted from the texts are used to train the classi-
fication models that, in turn, allow to predict the
proficiency level of an unseen text.

The rest of the article is organized as follows.
The literature related to this work is described in
Section 2. The architecture of the system is intro-
duced in Section 3, while the definitions of the lin-
guistic features adopted in the study are provided
in Section 4. Experiments are discussed in Sec-
tion 5, while the conclusions are drawn in Section
6 together with future lines of research.

2 Related Work

The assessment of text readability in relation to
its complexity has been a central research topic
for many decades now. In particular, advances
in computational linguistics and the development
of corpora, along with the availability of sophis-
ticated language technologies, allow the capturing

of a wide variety of increasingly complex linguis-
tic features that are able to affect the readability of
a text.

A number of studies aimed at developing au-
tomatic readability measures have focused on the
English language, both for the simplification of
administrative texts and for the purposes of first
and second language learning. In more recent
years, these studies have also involved other lan-
guages, such as French, Swedish, Dutch, German
and Portuguese.

The texts used as a gold standard to train the
classification models vary. For French, the corpus
of texts was a second language coursebook corpus,
containing texts developed by expert teachers and
materials’ designers (François and Fairon, 2012).
A similar approach has been used for Swedish
(Pilán et al., 2016), with the subsequent addition
of a corpus of texts produced by second language
learners (Pilán and Volodina, 2018). Other stud-
ies have used exams texts (Branco et al., 2014) or
a combination of exam texts and native texts (Xia
et al., 2011). One study on the readability of Dutch
texts (Velleman and Van der Geest, 2014) uses a
set of reference texts calibrated in order to repre-
sent a range of reading skills, while another one
tailored for English (Vajjala and Meurers, 2016)
includes a wide range graded corpora to cater for
both natives’ and learners’ reading skills in both
general and specialist language needs.

In terms of features, a number of systems have
been developed, such as the Flesch-Kincaid (Kin-
caid and Lieutenant Robert, 1975), Coh-metrix
(Graesser et al., 2004) and CTAP (Xiaobin and
Meurers, 2016). In relation to the Italian lan-
guage, three main approaches have been ex-
plored: the Flesch-Vacca formula, an adapta-
tion of the Flesch-Kincaid formula for English
(Franchina and Vacca, 1986), the GulpEase index
(Lucisano and Piemontese, 1988), and READ-IT
(Dell’Orletta et al., 2011).

In the Flesch-Kincaid formula, and its Italian
counterpart, text complexity is measured with ref-
erence to the average length of words, based on
syllables, and the average length of sentences,
based on words. In addition to this, the for-
mula provides an output indicating the approxi-
mate number of years spent in the education sys-
tem that are necessary to comprehend a given text.
The GulpEase index provides information that is
similar to the Flesch-Kincaid formula, though it



362

is based on considerably different characteristics.
First, it is created directly on and for the Italian
language. Second, though it includes the average
length of words as well as the average length of
sentences, the former is calculated on the basis
of letters, not syllables, which aids the automatic
treatment of the text.

For both of these measures, values range from 1
to 100, namely the highest and lowest textual com-
plexity levels respectively. READ-IT, on the other
hand, is based on a number of raw text, lexical,
morpho-syntactic, and syntactic features, based on
Support Vector Machines. This set of features is
used together with a training corpus in order to de-
velop a statistical model that is able to assess the
complexity of newly inputted texts. The training
corpus is formed by newspaper articles and a sim-
plified reader of newspaper articles.

The aim of these measures developed for the
Italian language have so far concerned the require-
ments of text simplification of administrative texts
or other typically complex texts, in order to meet
the needs of people with low literacy levels or with
mild cognitive disorders. To the best of our knowl-
edge, this study represents the first attempt to au-
tomatically classify Italian texts on the basis of a
wide set of linguistic features, and in relation to
the CEFR levels. In this respect, it lays the ground-
work for a new resource in the context of Italian as
a second language learning and teaching.

3 The System’s Architecture

The problem of automatically measuring text
complexity through the CEFR proficiency levels
has been cast to a supervised classification prob-
lem.

We collected a dataset of texts labeled by the
experts of the CVCL center of the University for
Foreigners of Perugia. This dataset is used to train
a classification model that, in turn, allows to auto-
matically predict the proficiency level of any text
in input.

As it is possible to see from the system’s ar-
chitecture depicted in Figure 1, the classification
model does not directly work with the texts in their
pure form. Indeed, any text is converted to a vec-
tor of numeric features and then passed on to the
classification model (both for training or level pre-
diction).

This scheme allows, on the one hand to adopt
the most common classification models available

in the machine learning literature (Shalev-Shwartz
and Ben-David, 2014) and, on the other hand, to
build a classification model based only on the lin-
guistic features of the text that, we think, are what
discriminate texts from the point-of-view of profi-
ciency classes.

Therefore, the most important part of our sys-
tem is the component performing the ”Linguis-
tic Features Extraction” phase. This component
has been implemented on top of Natural Language
Processing (NLP) tools for the Italian language. In
particular, we have adopted the NLP pipeline tools
provided by Tint (Palmero Aprosio and Moretti,
2016), i.e., the Italian counterpart of the widely
known Stanford CoreNLP tool (Manning et al.,
2014). The linguistic features used in this work
are detailed in Section 4.

Although most of the recently proposed works
in NLP use semantically based features (Santucci
et al., 2018), such as the well known word embed-
dings system introduced in (Mikolov et al., 2013)
and (Bojanowski et al., 2016), it is worthwhile to
note that here we chose lexical and syntactic fea-
tures because they are the key linguistic traits for
distinguishing different CEFR levels.

Regarding the classification model, we ran ex-
periments on our system with three widely known
models: decision tree (DT), random forest (RF),
and support vector machine (SVM)1. While DT
and RF provide more interpretable models, that
can be analyzed ex post by linguistic scholars, we
expect that SVM should reach a larger accuracy.

In our prototypical system, we have used the
implementations of DT, RF and SVM available in
the widely adopted ”Sci-Kit Learn” library (Pe-
dregosa et al., 2011).

4 Linguistic Features

The features used for predicting the text level are
inspired by those adopted in (Dell’Orletta et al.,
2011). These linguistics features have been di-
vided into four main categories, and are described
as follows.

4.1 Raw text features

Raw text features are the most elementary type of
features considered here and they were computed
through the tokenization of the text in input. In
particular, they are:

1See (Shalev-Shwartz and Ben-David, 2014) for a de-
scription of the models employed here



363

Figure 1: Architecture of the System

• Word Length, computed as the average num-
ber of characters per word, and

• Sentence Length, i.e., the average number of
tokens per sentence.

4.2 Lexical Features

Lexical features are mainly computed through the
lemmatization of the text’s tokens and with refer-
ence to the New Basic Italian Vocabulary (NBIV)
(De Mauro and Chiari, forthcoming) which in-
cludes the following three reference wordlists:

• fundamental (F) words (i.e.: the first 2000
most frequent words), such as cane, gatto;

• high usage (HU) words (i.e: occurring be-
tween frequency ranks 2000 and 4300) , such
as accademia, incapace, orribile;

• high availability (HA) words (i.e.: identi-
fied in De Mauro and Chiari (forthcoming)
through a native speaker judgment question-
naire), such as affannato, mandarino, sal-
vadanaio.

Therefore, the considered lexical features are:

• Lemmas in NBIV, i.e., the percentage of text’s
lemmas belonging to NBIV;

• Lemmas’ distribution with respect to NBIV,
i.e., the distribution of the text’s lemmas in
the previous point among the three subsets F,
HU and HA of NBIV (this feature is a vector
of three numbers);

• Type Token Ratio (TTR), computed as the ra-
tio between the number of different lemmas
and the number of tokens in the text; how-
ever, since TTR is highly influenced by the
text length, we restricted the computation to
the first 100 tokens of every text in input.

4.3 Morpho-syntactic features
Morpho-syntactic features are computed basing on
the Part-of-speech (POS) tagging and the morpho-
logical analysis performed on the text in input. In
particular, we have considered the following fea-
tures:

• POS Tags Distribution, i.e., for each POS tag
p, it is computed the ratio between the num-
ber of tokens with type p and the total number
of tokens in the text;



364

• Lexical Density, computed as the ratio be-
tween the number of content words (i.e.,
words tagged as nouns, adjectives, adverbs or
verbs) and the total number of tokens in the
text;

• Verbal Moods Distribution, i.e., the distribu-
tion of the seven verbal moods among the
verbal tokens of the text.

4.4 Syntactic Features
The text in input is syntactically parsed by Tint
and, for each sentence of the text, a dependency
parse tree is produced. The syntactic features,
described in the following, are then computed
through dependency trees.

• Dependency Types Distribution, i.e., for each
possible dependency type d, we computed
the ratio between the number of dependen-
cies with type d and the total number of de-
pendencies, considering all the parse trees.

• Depth of the Parse Trees, computed as the
maximum depth among all the parse trees.

• Length of non-Verbal Chains, i.e., the aver-
age length of the paths without verbal nodes
in the parse trees.

• Verbal Roots, i.e., the percentage of parse
trees with a verbal root.

• Average Verbal Arity, where the verbal arity
of a verbal node v is the number of depen-
dency links with v as a head.

• Subordinates, i.e., the number of subordinate
clauses in the parse trees.

• Average Length of the Dependency Links,
where the length of a dependency link be-
tween two tokens t1 and t2 is the distance, in
terms of number of tokens, between t1 and t2
in the syntagmatic dimension of the sentence.

5 Experiments

Experiments have been held in order to: analyze
the effectiveness and the robustness of the pro-
totypical classification system here proposed, and
gain useful insights about which features discrim-
inate more the texts.

The rest of the section is organized as follows.
Section 5.1 describes the corpus of texts and the

B1 B2 C1 C2 Total
2C 0 129 0 97 226
4C 194 129 103 97 523

Table 1: Distribution of proficiency levels across
datasets 2C and 4C.

datasets used in our experimentation. The exper-
imental design is described in Section 5.2. The
effectiveness of our system is analyzed in Sec-
tion 5.3, while its robustness is discussed in Sec-
tion 5.4. Finally, Section 5.5 analyzes the con-
tribution of the different features selected for this
work.

5.1 Corpus and Datasets
An important preliminary step to the experiments
was the creation of a reliable corpus of labeled
texts. In regard to this, we collected 523 texts with
CEFR levels manually marked by expert language
test developers. The corpus contains texts for the
four CEFR levels B1, B2, C1 and C2.

Two different datasets, namely 4C and 2C, have
been extracted from the corpus. While 4C (i.e.,
four classes) corresponds to the whole corpus, the
smaller dataset 2C (i.e., two classes) contains the
subset of 226 texts belonging to the levels B2 and
C2. Table 1 provides the distribution of the differ-
ent levels for both datasets.

Two main reasons motivated the introduction of
the smaller dataset 2C. First, the distribution of
the proficiency levels in 4C is unbalanced, there-
fore a smaller and more balanced dataset such as
2C can be more reliable in terms of representative-
ness. Second, as the classification models do not
treat the four levels as part of an ordinal scale, thus
ignoring the natural ordering characterizing them,
choosing two levels instead of four eliminates this
issue.

5.2 Experimental Design
We tested three classifier models, namely, Deci-
sion Tree (DT), Random Forest (RF) and SVM,
on both the datasets 2C and 4C.

For each dataset, the effectiveness and ro-
bustness of each model was evaluated using the
nested cross-validation scheme (Varma and Si-
mon, 2006). Two nested cross-validation loops
were performed: the outer loop aims at estimat-
ing the effectiveness of the model setting which is
calibrated in the inner loop. Both loops use 5 strat-
ified folds. The inner loop performs an exhaustive



365

Parameters Name Values

Feature-wise normalization
NN, SS, L2,
RS

Split quality measure
(criterion)

GI, IE

Min. impurity decrease
(min impurity decrease)

0, 2

Min. samples to split
(min samples split) 2, 10,

NS
10

Min. samples per leaf
(min samples leaf)

1, 5, 10

Max. number of features
(max features)

√
NF , NF

Max. number of leaf nodes
(max leaf nodes)

2, 5

Weights associated with classes
(class weight)

1,
NS

NC · nk

Table 2: Parameters space for the Decision Tree and the
Random Forest classifiers. The original name of each
parameter in the Sci-Kit documentation is in typewriter
font within brackets. The function used to measure the
quality of a split can either be the Gini impurity (GI)
or the Information entropy (IE). NF is the number of
features, NS and NC are the number of samples and
the number of classes in the dataset, respectively; nk is
the number of samples in the k-th class of the dataset.

grid search on the hyper-parameters space, cross-
validated on the training and validation sets ob-
tained by the outer loop. Every grid search returns
the setting of hyper-parameters which maximizes
the (macro-averaged) F1-score. Then, the general-
ization ability of the selected model setting is as-
sessed on the test sets generated by the outer loop
and using the classic metrics accuracy, precision,
recall and F1-score.

The linguistic features described in Section 4
may have different scales, hence we introduced a
preprocessing step to normalize them. Four nor-
malization methods were considered: no normal-
ization at all (NN), L2 normalization (L2), stan-
dardization (SS), and robust standardization (RS)
(which, with respect to SS, do not consider the out-
lier values). Hence, the choice of the normaliza-
tion method is a further hyper-parameter which is
tuned by the grid search.

The calibrated hyper-parameters and their
ranges are provided in Table 2 for DT and RF, and
Table 3 for SVM.

Finally, in order to reduce the computational ef-

Parameters Name Values

Penalty parameter (C)
0.5, 0.75,
1.0, 1.25,
1.5

Kernel coefficient γ (gamma) 10−3, 10−4

One-vs-rest or one-vs-one
(decision function shape)

OvO, OvR

Weights associated with classes
(class weight)

1,
NS

NC · nk

Table 3: Parameters space for the SVM classifier. The
original name of each parameter in the Sci-Kit docu-
mentation is in typewriter font within brackets.

2C A P R F1
DT 0.9292 0.9265 0.9303 0.9281
RF 0.9292 0.9278 0.9278 0.9278

SVM 0.9336 0.9355 0.9291 0.9318

4C A P R F1
DT 0.7189 0.6908 0.6888 0.6885
RF 0.7495 0.7136 0.7186 0.7130

SVM 0.7725 0.7400 0.7407 0.7398

Table 4: Accuracy A, precision P , recall R and F1-
score for Decision Tree (D.T.), Random Forest (R.F.)
and Support Vector Machine (SVM) on 2C (upper ta-
ble) and 4C (lower table). Such measures are first com-
puted for each class, then their unweighted mean is
computed.

fort, we fixed the following hyper-parameters:

• the maximum tree depth of DT and RF has
been set to

⌊√
NF

⌋
, where NF is the number

of features;

• the number of trees in RF has been set to 100;

• the SVM uses the radial basis function as ker-
nel type.

5.3 Results

For each dataset we tested all three classifiers, and
we report the results, in terms of accuracy, preci-
sion, recall and F1-score, in Table 4 for datasets
2C and 4C.

For the 2C dataset the differences between De-
cision Trees and Random Forests are irrelevant,
indeed the precision slightly increases, the recall
decreases comparably, and the F1-score conse-
quently remains pretty much the same. SVMs



366

Actual
Predicted B1 B2 C1 C2

B1 174 20 0 0
B2 13 95 20 1
C1 0 29 45 29
C2 0 3 16 78

Table 5: Confusion Matrix for Random Forests on 4C.

achieve better results, although the performances
are satisfactory for all classifiers.

For the 4C dataset the situation is different:
there is an improvement switching from Decision
Trees to Random Forests, as is expected, and also
from these ones to SVMs, that outperform the
other models by 2.3% in terms of accuracy. How-
ever, as can be expected, the performances ob-
tained on the 2C dataset are generally better than
those obtained on 4C.

As previously reported, our models do not take
into account the levels’ ordering. Hence, we ana-
lyze how much this aspect influenced the perfor-
mances. In particular, we analyze the results ob-
tained by the RF model on the 4C dataset. With
this aim, Table 5 shows the confusion matrix (for
RF on 4C) with the levels ordered according to
their natural ordering. From these data, it is ev-
ident that only four misclassified texts are two
classes away from the actual class. Therefore, the
classifier model does not seem to be very sensitive
to the levels’ ordering.

5.4 Robustness Analysis

Here we analyze the robustness of the RF model
by considering the differences among the five set-
tings of hyper-parameters obtained by the calibra-
tions performed in the outer-loop of the nested
cross-validation.

Most notably, every setting has the
same assignment for the hyper-parameters
criterion, min impurity decrease
and max leaf nodes. Moreover,
the hyper-parameters class weight,
min samples leaf, max features and
min samples split have been assigned to the
same value in three settings out of five. The only
unstable parameter is the normalization method
which assumes all the possible values.

These results, despite some minor differences,
shows the robustness of the proposed approach.

5.5 Analysis of Linguistic Features

The choice of Random Forests classifiers come in
handy when we want to analyze how the classi-
fier works internally, how the linguistic features
are used and how they contribute to the final re-
sult.

The importance of a feature used by the Ran-
dom Forests classifier can be quantified by means
of the loss of Gini impurity due to each node where
the splitting is performed according to that feature.

As already pointed out, a single run of nested
cross-validation for Random Forests provides 5
different sets of parameters for each dataset; we
analyzed them all separately, however, due to
space constraints, in Figure 2, we only show the
features’ importance for one of them. The impor-
tance of linguistic features remains quite steady
across the different 5 folds of the nested cross-
validation both for 2C and 4C, thus proving the ro-
bustness of our architecture, and providing a sound
assessment of the contribution of a feature to the
final outcome.

6 Conclusion and Future Work

In this work we introduced an automatic classifi-
cation system for assessing the proficiency level of
an Italian text used for second language learning
purposes.

A dataset of texts labeled by expert test develop-
ers was used to evaluate the performance of three
classifier models (decision tree, random forest,
and support vector machine), which were trained
using linguistic features measured quantitatively
and extracted from the texts.

Experiments were held in order to analyze the
effectiveness and robustness of the proposed pro-
totypical classification system, and to gain useful
insight about which features contribute the more
to discriminate the texts from the point of view of
CEFR levels.

Overall, considering the preliminary nature of
the work, the classification accuracy we obtained
is satisfactory. Moreover, we derived interesting
indications about the contribution of the different
linguistic features we considered.

This work can be extended along several future
research avenues: integrating more linguistic fea-
tures, considering the natural ordering among the
proficiency levels, including more classification
models, and artificially augmenting the dataset of
texts.



367

References
L. Bachman and A.S. Palmer. 2010. Language Assess-

ment in Practice. Oxford University Press.

Piotr Bojanowski, Edouard Grave, Armand Joulin,
and Tomas Mikolov. 2016. Enriching word vec-
tors with subword information. arXiv preprint
arXiv:1607.04606.

António Branco, João Rodrigues, Francisco Costa,
João Silva, and Rui Vaz. 2014. Rolling out text cat-
egorization for language learning assessment sup-
ported by language technology. Computational Pro-
cessing of the Portuguese Language, pages 256–
261.

T. De Mauro and I. Chiari. forthcoming. Il Nuovo Vo-
cabolario di Base della Lingua Italiana.

Felice Dell’Orletta, Simonetta Montemagni, and Giu-
lia Venturi. 2011. Read–it: Assessing readability
of italian texts with a view to text simplification.
In Proceedings of the Second Workshop on Speech
and Language Processing for Assistive Technolo-
gies, pages 73–83, Edinburgh, Scotland, UK. Asso-
ciation for Computational Linguistics.

V. Franchina and R. Vacca. 1986. Adaptation of flesh
readability index on a bilingual text written by the
same author both in italian and english languages.
Linguaggi, 3:47–49.

Thomas François and Cedrik Fairon. 2012. An ”ai
readability” formula for french as a foreign lan-
guage. Proceedings of the 2012 Joint Conference on
Empirical Methods in Natural Language Process-
ing and Computational Natural Language Learning,
pages 466–477.

A.C. Graesser, D.S. McNamara, M.M. Louwerse, and
Z. Cai. 2004. Coh-metrix: Analysis of text on co-
hesion and language. Behavior Research Methods,
Instruments, And Computers, 36:193–202.

P.J. Kincaid and Fishburne R. Lieutenant Robert, P.
1975. Derivation of new readability formulas for
navy enlisted personnel. Research Branch Report,
Millington, TN: Chiefof Naval Training, pages 8–75.

P. Lucisano and M.E. Piemontese. 1988. Gulpease:
una formula per la predizione della difficoltà dei testi
in lingua italiana. Scuola e città, 31(3):110–124.

Christopher D. Manning, Mihai Surdeanu, John Bauer,
Jenny Finkel, Steven J. Bethard, and David Mc-
Closky. 2014. The Stanford CoreNLP natural lan-
guage processing toolkit. In Association for Compu-
tational Linguistics (ACL) System Demonstrations,
pages 55–60.

Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Cor-
rado, and Jeff Dean. 2013. Distributed representa-
tions of words and phrases and their composition-
ality. In C. J. C. Burges, L. Bottou, M. Welling,

Z. Ghahramani, and K. Q. Weinberger, editors, Ad-
vances in Neural Information Processing Systems
26, pages 3111–3119. Curran Associates, Inc.

A. Palmero Aprosio and G. Moretti. 2016. Italy goes
to Stanford: a collection of CoreNLP modules for
Italian. ArXiv e-prints.

F. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel,
B. Thirion, O. Grisel, M. Blondel, P. Pretten-
hofer, R. Weiss, V. Dubourg, J. Vanderplas, A. Pas-
sos, D. Cournapeau, M. Brucher, M. Perrot, and
E. Duchesnay. 2011. Scikit-learn: Machine learning
in Python. Journal of Machine Learning Research,
12:2825–2830.

Ildiko Pilán and Elena Volodina. 2018. Predicting pro-
ficiency levels in learner writings by transferring
a linguistic complexity model from expert-written
coursebooks. Proceedings of the Workshop on Lin-
guistic Complexity and Natural Language Process-
ing, pages 49–58.

Ildikó Pilán, Sowmya Vajjala, and Elena Volodina.
2016. A readable read: Automatic assessment of
language learning materials based on linguistic com-
plexity. International Journal of Computational
Linguistics and Applications, 7 (1), pages 143–159.

J.E. Purpura. 2014. Cognition and language assess-
ment. In The Companion to Language Assessment
volume III, pages 1,453–1,476.

Valentino Santucci, Stefania Spina, Alfredo Milani,
Giulio Biondi, and Gabriele Di Bari. 2018. Detect-
ing hate speech for italian language in social media.
In EVALITA 2018, co-located with the Fifth Italian
Conference on Computational Linguistics (CLiC-it
2018), volume 2263.

Shai Shalev-Shwartz and Shai Ben-David. 2014. Un-
derstanding machine learning: From theory to algo-
rithms. Cambridge university press.

Sowmya Vajjala and Detmar Meurers. 2016.
Readability-based sentence ranking for evalu-
ating text simplification. Int. Jour. of Applied
Linguistics, pages 194–222.

Sudhir Varma and Richard Simon. 2006. Bias in error
estimation when using cross-validation for model
selection. BMC Bioinformatics, 7(1):91.

Eric Velleman and Thea Van der Geest. 2014. On-
line test tool to determine the cefr reading compre-
hension level of text. Procedia Computer Science,
pages 350–358.

Menglin Xia, Ekaterina Kochmar, and Ted Briscoe.
2011. Text readability assessment for second lan-
guage learners. Proc. of the 11th Works. on Innov.
Use of NLP for Building Educ. Appl., pages 12–22.

C. Xiaobin and D. Meurers. 2016. Ctap: A web-
based tool supporting automatic complexity analy-
sis. Proc. of the Workshop on Computational Lin-
guistics for Linguistic Complexity, pages 113–119.

https://www.aclweb.org/anthology/W11-2308
https://www.aclweb.org/anthology/W11-2308
http://www.aclweb.org/anthology/P/P14/P14-5010
http://www.aclweb.org/anthology/P/P14/P14-5010
http://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf
http://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf
http://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf
http://arxiv.org/abs/1609.06204
http://arxiv.org/abs/1609.06204
http://arxiv.org/abs/1609.06204
https://doi.org/10.1186/1471-2105-7-91
https://doi.org/10.1186/1471-2105-7-91
https://doi.org/10.1186/1471-2105-7-91


368

Lexical  Density
Subordinates

Average Verbal Arity
Depth  of  the  Parse  Trees

TTR
Sentence Length

Verbal  Roots
Verbal Moods Distribution

Word Length
Length of non-Verbal Chains

Lemmas in NBIV
Lemmas  distribution  wrt  NBIV
Dependency Types Distribution

Avg  Length  of  the  Dependency  Links
POS Tags Distribution

0.001 0.01 0.1 1

Average Verbal Arity
Depth  of  the  Parse  Trees

TTR
Sentence Length

Subordinates
Lexical  Density

Word Length
Length of non-Verbal Chains

Verbal Moods Distribution
Verbal  Roots

Dependency Types Distribution
Lemmas in NBIV

Lemmas  distribution  wrt  NBIV
Avg  Length  of  the  Dependency  Links

POS Tags Distribution

0.001 0.01 0.1 1

Figure 2: Relative importance of the features used by the Random Forest classifier trained on 2C (top plot) and 4C
(bottom plot), in logarithmic scale.


