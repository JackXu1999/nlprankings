



















































UIUC at SemEval-2018 Task 1: Recognizing Affect with Ensemble Models


Proceedings of the 12th International Workshop on Semantic Evaluation (SemEval-2018), pages 377–384
New Orleans, Louisiana, June 5–6, 2018. ©2018 Association for Computational Linguistics

UIUC at SemEval-2018 Task 1: Recognizing Affect with Ensemble Models

Abhishek Narwekar
Department of Computer Science

University of Illinois, Urbana Champaign
USA 61820

abhisheknkar@gmail.com

Roxana Girju
Linguistics Department,

Computer Science Department,
Beckman Research Institute,

University of Illinois, Urbana Champaign
USA 61820

girju@illinois.edu

Abstract

Our submission to the SemEval-2018 Task1:
Affect in Tweets shared task competition is a
supervised learning model relying on standard
lexicon features coupled with word embed-
ding features. We used an ensemble of di-
verse models, including random forests, gra-
dient boosted trees, and linear models, cor-
rected for training-development set mismatch.
We submitted the system’s output for sub-
tasks 1 (emotion intensity prediction), 2 (emo-
tion ordinal classification), 3 (valence inten-
sity regression) and 4 (valence ordinal clas-
sification), for English tweets. We placed
25th, 19th, 24th and 15th in the four subtasks
respectively. The baseline considered was an
SVM (Support Vector Machines) model with
linear kernel on the lexicon and embedding
based features. Our system’s final perfor-
mance measured in Pearson correlation scores
outperformed the baseline by a margin of 2.2%
to 14.6% across all tasks.

1 Introduction

Affective computing deals with the recognition,
interpretation, processing, and simulation of hu-
man affects. It is a highly interdisciplinary field
at the heart of a broad range of technological ap-
plications in health care, media & advertisement,
automotive, and others.

Although emotions are a fundamental feature of
human experience, they have been long ignored
by technology development mainly due to their
complex and subjective nature, as well as the lack
of learning capabilities to detect them. Current
affective computing systems focus mainly on fa-
cial expressions, body language, speech (tone of
voice, rhythm, etc.), keystroke as well as physio-
logical input (e.g., heart rate, body temperature) to
capture and process changes in a user’s emotional
state. However, in environments such as social

media and Internet forums, most often the only
signal is written language. And since language
per se is the smallest portion of human commu-
nication (Mehrabian, 1981), emotions are not easy
to detect.

Although emotion detection is directly related
to the more popular task of sentiment analysis,
they differ in many respects. Sentiment Analy-
sis aims to detect the positive, neutral, or negative
orientation of the text, while emotion detection
focuses on recognizing and classifying text snip-
pets into a set of predefined, more or less univer-
sal emotions. Various such classification models
have been proposed, two famous ones being Ek-
man’s (Ekman, 1997) six basic emotions (anger,
happiness, surprise, disgust, sadness, and fear)
and Plutchik’s wheel of eight emotions (Plutchik,
2001), where each primary emotion has a polar op-
posite (joy, trust, fear, surprise, sadness, anticipa-
tion, anger, and disgust).

To date, there are many freely available tools
for sentiment polarity classification of input text,
yet not so many exist for emotion detection. Ma-
jor challenges are: (1) the difficulty in establish-
ing ground truth for various emotions, (2) the high
variability, vagueness, ambiguity, and implicitness
of language that can make the detection very prob-
lematic, (3) the scarcity of non-verbal clues in
written communication, as well as (4) the chal-
lenge of getting access to and being able to process
the right type of context. This can be explained by
the ”7% Rule” (Mehrabian, 1981): only 7% of hu-
man communication is verbal while over 90% is
comprised of tone of voice (38%) and body lan-
guage (55%).

This year, SemEval 2018 hosts Task1: Affect in
Tweets (Mohammad et al., 2018) - a shared task
competition aiming to predict emotions and sen-
timent in tweets. There are five sub-tasks (Table
1). The participating systems have to automati-

377



cally determine the intensity of emotions (E) and
intensity of sentiment (i.e., valence V) from a col-
lection of tweets, as experienced by the authors of
these tweets. The organizers also include a multi-
label emotion classification task for tweets. For
each task, separate training and test data sets for
each language considered are provided to the par-
ticipants.

ID Task Input Output
Label

1 El-reg Tweet (t), Intensity(e, t) ∈ (0, 1)
Emotion (e)

2 El-oc Tweet (t), 0 ≤ Intensity(e, t) ≤ 3,
Emotion (e) Intensity(e, t) ∈N

3 V-reg Tweet (t), Intensity(s, t) ∈ (0, 1)
Sentiment (s)

4 V-oc Tweet (t), -3 ≤ Intensity(s, t) ≤ 3,
Sentiment (s) Intensity(s, t) ∈N

5 E-c Tweet (t), Class: neutral/
Emotions (s) no emotion/

multiple emotions

Table 1: Description of the five sub-tasks of Task1: Af-
fect in Tweets at SemEval 2018.

The contributions of the UIUC system are as
follows: (1) In this competition, we demon-
strate the use of a system that uses lexicon- and
embedding-based features in an ensemble model
of diverse approaches such as random forests, gra-
dient boosted trees, and linear classifiers. We
demonstrate how their combination in the final en-
semble outperforms each of the individual meth-
ods. (2) We account for the train-development
mismatch in the dataset by training a separate
model to learn this mismatch. (3) We analyze the
UIUC system and several variants of it, some of
which improve on its performance. (4) We also
perform an error analysis of “difficult” tweets, and
explore areas for improvement of the model.

2 Related Work

Word-Emotion Lexicons: Word-emotion lexi-
cons are a mapping between the words in the vo-
cabulary to an emotion rating. Some lexicons map
words to discrete emotions, such as General In-
quirer (Stone et al., 1962), Wordnet Affect (Strap-
parava et al., 2004) and the NRC-10 Emotion Lex-
icon (Mohammad and Turney, 2013). Others, such
as Affective Norms for English Words (ANEW)
(Bradley and Lang, 1999) and WKB Corpus (War-
riner et al., 2013), map them to dimensions such as
valence, arousal and dominance.

Sentence-Level Labeled Corpora: Large
scale corpora annotated with sentence-level emo-

tion labels are uncommon in the literature. Affec-
tive Text (Strapparava and Mihalcea, 2007), cre-
ated for SemEval 2007, contains emotion annota-
tions headlines of news articles. Alm et al. anno-
tated about 185 children’s stories with the Ekman
labels. Aman and Szpakowicz created annotated
5,000 sentences with additional labels for intensity
and emotion bearing phrases. Preotiuc-Pietro et al.
annotated 3,000 social media posts for valence and
arousal, making this one of the few datasets that
contains annotations based on the VAD model.

Approaches: Rule-based approaches incorpo-
rate domain knowledge. This can include term-
based n-gram features, distance between certain
terms or pre-specified POS patterns. Early work
in this area focused mainly on linguistic heuristics
(Hatzivassiloglou and McKeown, 1997). How-
ever, a major drawback of these rule-based ap-
proaches is that they are unable to detect novel ex-
pression of sentiment. Keyword based approaches
classify text based on the detection of unambigu-
ous words in language. They depend on large scale
lexicons with affective labels for words, such NRC
(Mohammad and Turney, 2013). Knowledge-
based approaches use web ontologies or seman-
tic networks. A major advantage of such systems
is that they enable the system to use conceptual
ideas derived from world knowledge (Cambria and
Hussain, 2012). Recently, distributed approaches
have been proposed that leverage word embed-
dings and train deep neural networks on the em-
bedding space (Mohammad and Bravo-Marquez,
2017a).

Shared evaluations have encouraged the com-
munity to create benchmarks over shared tasks,
and have been organized frequently. The Af-
fective Text task at SemEval 2007 (Strapparava
and Mihalcea, 2007) asked its participants to pre-
dict emotion labels for headlines of news articles.
More recently, the Shared Task on Emotion In-
tensity (EmoInt) at WASSA 2017 (Mohammad
and Bravo-Marquez, 2017a), had 22 participating
teams who were given a corpus of 3,960 English
tweets annotated with a continuous intensity score
for each of four of Ekman’s basic emotions: anger,
fear, joy and sadness.

3 Dataset

Tasks 1 and 2 share the same training and devel-
opment data sets: a total of 7,500 sentences in
training and about 1,600 sentences in development

378



across the four emotions: anger, fear, joy, sadness.
It is interesting to note that the training data sets
for the emotions of fear, anger and sadness overlap
significantly: all pairs have a Jaccard similarity of
over 0.5. This means that over 67% of the data sets
across these emotions contain the same tweets.

Tasks 3 and 4 share the same data sets as well,
for a total of 1,200 tweets in training and 450
tweets in development across the four emotions.

Another interesting overlap is between the tweet
collections for Tasks 5 and 1 (and therefore Task
2): The data set for Task 5 appears to be made
up largely of the tweets for Task 1, for both the
training and development sets. These overlaps of
the training and development data sets across all
emotions gave us the idea to tackle all tasks using a
common set of features. For instance, Tasks 2 and
4 may be solved by simply transforming the output
of Tasks 1 and 3, respectively. Task 5 involves
a multi-label classification and thus, needs more
thought.

In the test set, with the exception of the first
1,000 or so sentences, nearly 95% of the total sen-
tences for Tasks 1A and 3A (i.e., for English) are
the so called “mystery” sentences – meaning, es-
sentially neutral sentences without any emotional
content. The scores reported by the organizers
are for the non-mystery sentences only (i.e., non-
neutral).

4 The UIUC System

Our system takes as input features from affective
lexicons and word embeddings trained on affective
Twitter corpora. We then train an ensemble of di-
verse models over these features. Given that the
training and development labels are not directly
comparable, we also model the mismatch between
the two sets. Moreover, we also describe addi-
tional models that we constructed after the com-
petition deadline (section 4.4). We report results
for tasks 1A, 2A, 3A and 4A (where ’A’ identifies
the target language: English).

4.1 Feature Space

We have used the AffectiveTweets (Mohammad
and Bravo-Marquez, 2017b), a package in Weka
(Hall et al., 2009) for extracting certain fea-
tures from a tweet. The features extracted
are: MPQA (Wilson et al., 2005), BingLiu
(Bauman et al., 2017), AFINN (Nielsen, 2011),
Sentiment-140 Emoticon (Kiritchenko et al.,

2014), NRC Hashtag Emotion Lexicon (Mo-
hammad and Kiritchenko, 2015), NRC emo-
tion lexicon (wordlevel) (Mohammad and Tur-
ney, 2013), SentiWordNet (Baccianella et al.,
2010), NegatingWordList(Mohammad and Bravo-
Marquez, 2017b). We call these lexicon features.
In addition, we also extract Embedding based fea-
tures (Twitter Edinburgh 100D / 400D corpus) us-
ing the AffectiveTweets package.

4.2 Models

The UIUC system contains an ensemble con-
structed using stacking of several base learners.
A schematic of this ensemble is shown in figure
1. We obtained out-of-fold predictions for each
of these three models using 5-fold cross validation
on layer 1. These predictions were concatenated
and provided as input to layer 2. Parameters of the
models in this ensemble are detailed below:

Layer 1
Random Forests:
n estimators=100, max features=

√
F (F=total

features), max depth=5, min samples leaf=2
XGB1: max depth=5, min child weight=150,
gamma=0, n estimators=150, reg alpha=0.01,
reg lambda=0.87, learning rate=0.1
SVM:kernel=linear, C=0.1

Layer 2
XGB: max depth=3, min child weight=1,
gamma=0, n estimators=100, reg alpha=0.1,
reg lambda=1, learning rate=0.1, random state=0

4.3 Modeling the Mismatch Between the
Training and Development Sets

According to the organizers, the training set for
the task was created from an annotation effort in
2016 (Mohammad and Bravo-Marquez, 2017a).
The development and test sets were created from
a common 2017 annotation effort. As a result, the
scores for tweets across the training and develop-
ment sets or across the training and test sets are not
directly comparable. We therefore devise a model
that can predict and eliminate the mismatch be-
tween the two sets of labels. As a means to model
the mismatch in the distributions of the two label
sets, we train a linear model that, for the labels
in the development set, learns a function between
the predictions made for the development set and

1Note: XGB stands for the XGBoost implementation of
gradient boosted decision trees. SVM was implmented using
sklearn (http://scikit-learn.org/stable/).

379



Figure 1: Ensemble used in the UIUC system.

the ground truth. This learner does not affect the
training in any way, but is a way to transform the
predictions made for the development set so that
they are comparable to the ground truth labels.

4.4 Additional Models

After the competition deadline, we built and eval-
uated additional models. The overall model was
an ensemble with the same structure as the offi-
cial submission. Additions include implementa-
tion of neural models of computation. In particu-
lar, we implement feed-forward neural networks
(using the average word embeddings as input),
LSTM-CNN (using individual word embeddings)
and character level LSTM (using the character
stream). The neural networks were implemented
in Keras (Chollet et al., 2015) with the Tensorflow
(Abadi et al., 2016) backend. Details about these
additional models are shown below.

Layer 1:
SVM (layer 1): C=0.1, kernel=RBF
XGB: max depth=5, reg lambda=0.87,
min child weight=150, n estimators=150
FFNN (feed forward neural network): Dense
(256, sigmoid), Dropout (0.2, sigmoid), Dense
(64, sigmoid), Dense (32, sigmoid), Dense (1,
relu)
LSTM-CNN: Conv1D (300, 3, relu), Dropout
(0.2), LSTM (150), Dropout (0.2), Dense (32,
sigmoid), Dense (1, relu)
Character level language model (Char): LSTM
(150), Dropout (0.2), Dense (64, sigmoid), Dense
(1, relu)

Layer 2:
SVM: C=1, kernel=RBF

5 Results

In this section, we describe the results of our offi-
cial submission to SemEval 2018 (subsection 5.1)
as well as the results of experiments on additional
models constructed after the competition deadline
(section 5.2).

5.1 Performance of the UIUC system

Tables 2 and 3 show the performance of our model
for Tasks 1A, 2A, 3A and 4A, respectively. We
have shown the results by comparing our model
against the baseline, which has been trained using
an SVM with linear kernel on the lexicon and em-
bedding based features. Our submission outper-
forms the baseline in nearly all the task-emotion
pairs.

In particular, we observe that the results for the
prediction of data points in the 0.5 – 1 range are
poorer than in the overall range. The reason for
this is that the finer prediction is a harder task than
the overall prediction, and exactly predicting the
emotion intensity given that it is high has signifi-
cant variance. Scores for Task 2A are worse than
those for Task 1A in spite of the similarity of the
tasks. This is because in Task 2A, we essentially
discretize the output, thereby either increasing or
decreasing the absolute error between the inten-
sity predicted and the actual intensity, depending
on whether the discretized output is correct or not.
On the whole, evidently, the correlation drops as
the effect of the latter case (increase in the abso-
lute error) dominates over the former.

Tasks 3 and 4 follow similar trends as Tasks 1
and 2 respectively, but we see a higher correla-
tion for these tasks as compared to Tasks 1 and
2, respectively. This leads to the conclusion that
predicting the sentiment is an easier task than pre-

380



Subtask Submission Pearson (all instances) Alternate evaluationmacro-avg anger fear joy sadness macro-avg anger fear joy sadness

1
UIUC System 0.647 0.663 0.646 0.649 0.628 0.463 0.514 0.431 0.422 0.485

Baseline 0.630 0.652 0.625 0.632 0.610 0.462 0.523 0.418 0.424 0.481

2
UIUC System 0.518 0.514 0.449 0.576 0.533 0.463 0.414 0.392 0.562 0.484

Baseline 0.448 0.512 0.258 0.527 0.493 0.334 0.335 0.219 0.415 0.366

Table 2: Results of the UIUC system for subtasks 1a (emotion intensity regression) and 2a (emotion ordinal
classification) and comparison with baseline. The alternate evaluation is Pearson correlation for tweets with scores
between 0.5 and 1 for subtask 1 and the Cohen Kappa (Cohen, 1960) for subtask 2.

Subtask Model Pearson Alternate evaluation

3 UIUC system 0.762 0.582Baseline 0.746 0.565

4 UIUC system 0.724 0.694Baseline 0.688 0.673

Table 3: Results of the UIUC system for tasks 3a (va-
lence intensity regression) and 4a (valence ordinal clas-
sification) and comparison with baseline. The alter-
nate evaluation was Pearson correlation for tweets with
score 0.5-1 for subtask 3a and Cohen’s Kappa for sub-
task 4a.

dicting the intensity of a given emotion.

5.2 Performance of Additional Models
Ablation Study for Task 1: Given the multiple
subsections of data, it is difficult to optimize the
architecture and parameters for all emotions for all
subtasks. Therefore, we focus on optimizing the
architecture and parameters for only the first sub-
task (emotion intensity prediction) for the emotion
anger. Given the many models developed and pre-
sented here, it is interesting to see how they per-
form individually on this subtask. Table 4 shows
the performance of various feature-model combi-
nations. Note that L and E in the Features column
indicate lexicon-based and embedding-based fea-
tures respectively.

Feature Model CV Dev Test

L

SVM 0.646 0.616 0.654
XGB 0.648 0.646 0.634
FFNN 0.699 0.674 0.664

SVM+XGB 0.662 0.651 0.663
SVM+XGB+FFNN [M1] 0.695 0.674 0.673

E

SVM 0.564 0.553 0.555
LSTM 0.640 0.635 0.633

LSTM-CNN 0.641 0.639 0.635
LSTM-CNN (Att) [M2] 0.651 0.642 0.644

L+E
M1+M2 0.733 0.713 0.701

M1+M2+Char 0.735 0.711 0.704

Table 4: An ablation study of various features and mod-
els for subtask 1: emotion intensity prediction for the
specific case of the emotion anger.

We use the SVM trained on lexical fea-
tures as the baseline. We can see that the
SVM+XGB+FFNN (referred to as M1) performs

better than the SVM alone. LSTM-CNN with at-
tention (referred to as M2) performs similarly to
the SVM baseline. However, when combined to-
gether, the model M1+M2+Char performs better
than each of the individual models on the test set.
This means that the different models capture com-
plementary information about the input, and work
better in unison, thus demonstrating the efficacy of
the idea of ensembling.

Henceforth, we use M1 to refer to the SVM
+ XGBoost + Feedforward Neural Network ar-
chitecture trained on lexical features, M2 to re-
fer to the LSTM-CNN architecture with attention
trained on the embedding features and Char to re-
fer to the character level LSTM model trained on
the individual characters.

Task Features Model Pearson Correlation CoefficientAnger Fear Joy Sadness

1

L SVM 0.654 0.646 0.649 0.628
L M1 0.673 0.668 0.698 0.642
E M2 0.644 0.659 0.685 0.644

L+E M1+M2+Char 0.704 0.688 0.713 0.652

2

L SVM 0.514 0.449 0.576 0.533
L M1 0.549 0.462 0.58 0.557
E M2 0.544 0.455 0.571 0.542

L+E M1+M2+Char 0.558 0.461 0.601 0.566

Table 5: Evaluation for subtask 1 (emotion intensity
prediction) and subtask 2 (emotion ordinal classifica-
tion) for all emotions with various features and models.

Tasks 1 and 2 with with Additional Models:
Table 5 shows the performance of the models de-
scribed above to the first two subtasks: emotion
intensity prediction and emotion ordinal classifi-
cation. We have shown the results for all the four
emotions. As we can see, here too, the model com-
bination M1+M2+Char combination performs the
best for all emotions in subtask 1. The perfor-
mance of the model is the best for the emotion joy,
and the worst for the emotion fear.

Tasks 3 and 4 with with Additional Models:
Coming to subtasks 3 and 4 (valence intensity pre-
diction and valence ordinal classification respec-
tively), Table 6 shows the performance of the var-

381



Task Features Model Alternate Evaluation

3

L SVM 0.762
L M1 0.78
E M2 0.764

L+E M1+M2+Char 0.784

4

L SVM 0.724
L M1 0.733
E M2 0.745

L+E M1+M2+Char 0.75

Table 6: Evaluation for subtask 3 (emotion valence re-
gression) and subtask 4 (valence ordinal classification)
for various features and models. The alternate evalu-
ation is the Pearson correlation for tweets with scores
0.5 - 1 for subtask 3 and the Cohen’s Kappa for subtask
4.

ious models on these tasks. Consistent with the
results of subtasks 1 and 2, the combined model
M1+M2+Char performs the best for both tasks.

In general, we note that the correlation is signif-
icantly higher on valence prediction tasks as com-
pared to the emotion intensity tasks. This is likely
because the emotion intensity prediction is a fine
grained task, requiring the model to observe pat-
terns specific to an emotion. Valence is more of an
“aggregated” effect of all the emotions.

Had the best model in additional experiments
for all subtasks been submitted to SemEval with
all other factors constant, its rank based on the
macro-average for the first four subtasks would
have been 15th, 15th, 18th and 13th respectively.

6 Discussion

In order to identify areas where the model can im-
prove, it is necessary to study cases where it per-
forms poorly. To do so, we select 5 sentences
where the baseline SVM model performs very
poorly while predicting anger intensity (based on
absolute error) and 1 sentence where it performs
well. We have restricted the number of sentences
to 6 for brevity. In particular, for sentences 1 and
2, the model significantly overestimates the inten-
sity, for sentence 3, the model predicts the inten-
sity accurately. For sentences 4, 5 and 6, the model
significantly underestimates the intensity. Table 7
shows the sentences considered and the true value
of emotion intensity for the emotion anger.

We then compare the absolute error between
the true value and model prediction for various
models. This comparison is shown in Table 8.
Given that 5 of the 6 sentences are “difficult”
for the models, we observe that there is no clear

# Tweet Intensity
1 never had a dull moment with u guys 0.078
2 Fast and furious marathon soon! 0.118
3 They cancelled Chewing Gum. #devastated 0.625
4 Its taking apart my lawn! GET OFF MY LAWN! 0.797
5 I need a beer #irritated 0.806
6 Working with alergies is the most miserable shit 0.856

in the world #miserable #alergies

Table 7: Test Examples for Error Analysis with inten-
sity annotations for anger.

winner over these sentences. However, we ob-
serve that for sentences 1 and 2, the model M1
performs relatively well. For sentences 4, 5 and
6, the models involving M2 perform relatively
well. This suggests that M1 is better at predict-
ing the lower intensities, while M2 is better at the
higher intensities. This may explain why though
the overall scores for the two models was simi-
lar, the ensembled model outperformed the indi-
vidual models. Another interesting observation is
that for sentence 4, the presence of the capital let-
ters is the reason for the high intensity. The model
M1+M2+Char is able to identify this well, and
contributes to reducing the error significantly as
compared to all the other models.

Features Model Sentence-wise error
1 2 3 4 5 6

L SVM 0.310 0.308 0.004 -0.377 -0.326 -0.327
L M1 0.305 0.287 -0.067 -0.391 -0.286 -0.245
E M2 0.344 0.366 0.051 -0.265 -0.241 -0.199

L+E M1+M2+Char 0.339 0.373 0.071 -0.213 -0.242 -0.203

Table 8: Absolute error values for various features and
models for subtask 1: emotion intensity prediction for
emotion anger.

7 Conclusion

In this paper we presented the UIUC system
that performs regression and ordinal classifica-
tion of the emotion and sentiment present in En-
glish tweets. Our system comprised an ensem-
ble trained on lexicon based and embedding based
features. We also provided an account for the
training and development mismatch in a given data
set by training an adaptive model between the
model predictions and the final test predictions.
We finally perform an error analysis over the var-
ious models to identify potential sources of im-
provement to the model.

382



References
Martı́n Abadi, Paul Barham, Jianmin Chen, Zhifeng

Chen, Andy Davis, Jeffrey Dean, Matthieu Devin,
Sanjay Ghemawat, Geoffrey Irving, Michael Isard,
et al. 2016. Tensorflow: A system for large-scale
machine learning. In OSDI, volume 16, pages 265–
283.

Cecilia Ovesdotter Alm, Dan Roth, and Richard
Sproat. 2005. Emotions from text: machine learning
for text-based emotion prediction. In Proceedings of
the conference on human language technology and
empirical methods in natural language processing,
pages 579–586. Association for Computational Lin-
guistics.

Saima Aman and Stan Szpakowicz. 2007. Identify-
ing expressions of emotion in text. In International
Conference on Text, Speech and Dialogue, pages
196–205. Springer.

Stefano Baccianella, Andrea Esuli, and Fabrizio Sebas-
tiani. 2010. Sentiwordnet 3.0: An enhanced lexical
resource for sentiment analysis and opinion mining.
In LREC. European Language Resources Associa-
tion.

Konstantin Bauman, Bing Liu, and Alexander Tuzhilin.
2017. Aspect based recommendations: Recom-
mending items with the most valuable aspects based
on user reviews. In Proceedings of the 23rd ACM
SIGKDD, pages 717–725. ACM.

Margaret M Bradley and Peter J Lang. 1999. Affective
norms for English words (ANEW): Instruction man-
ual and affective ratings. Technical report, Technical
report C-1, the center for research in psychophysiol-
ogy, University of Florida.

Erik Cambria and Amir Hussain. 2012. Sentic comput-
ing: Techniques, tools, and applications, volume 2.
Springer Science & Business Media.

François Chollet et al. 2015. Keras.

Jacob Cohen. 1960. A coefficient of agreement for
nominal scales. Journal of Educational and Psycho-
logical Measurement, 20(1):37.

Paul Ekman. 1997. Basic Emotions. Handbook of
Cognition and Emotion, John Wiley & SOns, Sus-
sex, UK.

Mark Hall, Eibe Frank, Geoffrey Holmes, Bernhard
Pfahringer, Peter Reutemann, and Ian H Witten.
2009. The weka data mining software: an update.
ACM SIGKDD explorations newsletter, 11(1):10–
18.

Vasileios Hatzivassiloglou and Kathleen R McKeown.
1997. Predicting the semantic orientation of adjec-
tives. In Proceedings of the 35th annual meeting
of the association for computational linguistics and
eighth conference of the european chapter of the as-
sociation for computational linguistics, pages 174–
181. Association for Computational Linguistics.

Svetlana Kiritchenko, Xiaodan Zhu, and Saif M Mo-
hammad. 2014. Sentiment analysis of short in-
formal texts. Journal of Artificial Intelligence Re-
search, 50:723–762.

Albert Mehrabian. 1981. Silent messages: Implicit
communication of emotions and attitudes (2nd ed).
Wadsworth Pub. Co., Belmont, California.

Saif Mohammad and Felipe Bravo-Marquez. 2017a.
Wassa shared task on emotion intensity. In Pro-
ceedings of the 8th Workshop on Computational Ap-
proaches to Subjectivity, Sentiment and Social Me-
dia Analysis (WASSA), pages 34–49.

Saif M Mohammad and Felipe Bravo-Marquez. 2017b.
Emotion intensities in tweets. arXiv preprint
arXiv:1708.03696.

Saif M. Mohammad, Felipe Bravo-Marquez, Mo-
hammad Salameh, and Svetlana Kiritchenko. 2018.
Semeval-2018 Task 1: Affect in tweets. In Proceed-
ings of International Workshop on Semantic Evalu-
ation (SemEval), New Orleans, LA, USA.

Saif M Mohammad and Svetlana Kiritchenko. 2015.
Using hashtags to capture fine emotion cate-
gories from tweets. Computational Intelligence,
31(2):301–326.

Saif M Mohammad and Peter D Turney. 2013. Crowd-
sourcing a word–emotion association lexicon. Com-
putational Intelligence, 29(3):436–465.

Finn Årup Nielsen. 2011. A new ANEW: Evaluation
of a word list for sentiment analysis in microblogs.
arXiv preprint arXiv:1103.2903.

Robert Plutchik. 2001. The nature of emotions. Amer-
ican Scientist, 89.

Daniel Preotiuc-Pietro, H Andrew Schwartz, Gregory
Park, Johannes C Eichstaedt, Margaret Kern, Lyle
Ungar, and Elizabeth P Shulman. 2016. Modelling
valence and arousal in facebook posts. In Proceed-
ings of NAACL-HLT, pages 9–15.

Philip J Stone, Robert F Bales, J Zvi Namenwirth, and
Daniel M Ogilvie. 1962. The general inquirer: A
computer system for content analysis and retrieval
based on the sentence as a unit of information. Be-
havioral Science, 7(4):484–498.

Carlo Strapparava and Rada Mihalcea. 2007. Semeval-
2007 task 14: Affective text. In Proceedings of
the 4th International Workshop on Semantic Evalu-
ations, pages 70–74. Association for Computational
Linguistics.

Carlo Strapparava, Alessandro Valitutti, et al. 2004.
Wordnet affect: an affective extension of wordnet.
In LREC, volume 4, pages 1083–1086. Citeseer.

Amy Beth Warriner, Victor Kuperman, and Marc Brys-
baert. 2013. Norms of valence, arousal, and dom-
inance for 13,915 english lemmas. Behavior re-
search methods, 45(4):1191–1207.

383



Theresa Wilson, Janyce Wiebe, and Paul Hoff-
mann. 2005. Recognizing contextual polarity in
phrase-level sentiment analysis. In Proceedings of
HLT/EMNLP, pages 347–354. Association for Com-
putational Linguistics.

384


