



















































AdvEntuRe: Adversarial Training for Textual Entailment with Knowledge-Guided Examples


Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers), pages 2418–2428
Melbourne, Australia, July 15 - 20, 2018. c©2018 Association for Computational Linguistics

2418

AdvEntuRe: Adversarial Training for Textual Entailment
with Knowledge-Guided Examples

Dongyeop Kang1 Tushar Khot2 Ashish Sabharwal2 Eduard Hovy1
1School of Computer Science, Carnegie Mellon University, Pittsburgh, PA, USA

2Allen Institute for Artificial Intelligence, Seattle, WA, USA
fdongyeok,hovyg@cs.cmu.edu ftushark,ashishsg@allenai.org

Abstract

We consider the problem of learning tex-
tual entailment models with limited super-
vision (5K-10K training examples), and
present two complementary approaches
for it. First, we propose knowledge-guided
adversarial example generators for incor-
porating large lexical resources in entail-
ment models via only a handful of rule
templates. Second, to make the entailment
model—a discriminator—more robust, we
propose the first GAN-style approach for
training it using a natural language ex-
ample generator that iteratively adjusts
based on the discriminator’s performance.
We demonstrate effectiveness using two
entailment datasets, where the proposed
methods increase accuracy by 4.7% on
SciTail and by 2.8% on a 1% training
sub-sample of SNLI. Notably, even a sin-
gle hand-written rule, negate, improves
the accuracy on the negation examples in
SNLI by 6.1%.

1 Introduction

The impressive success of machine learning mod-
els on large natural language datasets often does
not carry over to moderate training data regimes,
where models often struggle with infrequently ob-
served patterns and simple adversarial variations.
A prominent example of this phenomenon is tex-
tual entailment, the fundamental task of decid-
ing whether a premise text entails (�) a hypoth-
esis text. On certain datasets, recent deep learn-
ing entailment systems (Parikh et al., 2016; Wang
et al., 2017; Gong et al., 2018) have achieved
close to human level performance. Nevertheless,
the problem is far from solved, as evidenced by
how easy it is to generate minor adversarial ex-

Table 1: Failure examples from the SNLI dataset:
negation (Top) and re-ordering (Bottom). P is
premise, H is hypothesis, and S is prediction made
by an entailment system (Parikh et al., 2016).

P: The dog did not eat all of the chickens.
H: The dog ate all of the chickens.
S: entails (score 56:5%)
P: The red box is in the blue box.
H: The blue box is in the red box.
S: entails (score 92:1%)

amples that break even the best systems. As Ta-
ble 1 illustrates, a state-of-the-art neural system
for this task, namely the Decomposable Attention
Model (Parikh et al., 2016), fails when faced with
simple linguistic phenomena such as negation, or
a re-ordering of words. This is not unique to a
particular model or task. Minor adversarial exam-
ples have also been found to easily break neural
systems on other linguistic tasks such as reading
comprehension (Jia and Liang, 2017).

A key contributor to this brittleness is the use
of specific datasets such as SNLI (Bowman et al.,
2015) and SQuAD (Rajpurkar et al., 2016) to drive
model development. While large and challenging,
these datasets also tend to be homogeneous. E.g.,
SNLI was created by asking crowd-source work-
ers to generate entailing sentences, which then
tend to have limited linguistic variations and an-
notation artifacts (Gururangan et al., 2018). Con-
sequently, models overfit to sufficiently repetitive
patterns—and sometimes idiosyncrasies—in the
datasets they are trained on. They fail to cover
long-tail and rare patterns in the training distribu-
tion, or linguistic phenomena such as negation that
would be obvious to a layperson.

To address this challenge, we propose to train
textual entailment models more robustly using ad-



2419

versarial examples generated in two ways: (a)
by incorporating knowledge from large linguistic
resources, and (b) using a sequence-to-sequence
neural model in a GAN-style framework.

The motivation stems from the following ob-
servation. While deep-learning based textual en-
tailment models lead the pack, they generally do
not incorporate intuitive rules such as negation,
and ignore large-scale linguistic resources such
as PPDB (Ganitkevitch et al., 2013) and Word-
Net (Miller, 1995). These resources could help
them generalize beyond specific words observed
during training. For instance, while the SNLI
dataset contains the pattern two men � people, it
does not contain the analogous pattern two dogs �
animals found easily in WordNet.

Effectively integrating simple rules or linguis-
tic resources in a deep learning model, however,
is challenging. Doing so directly by substantially
adapting the model architecture (Sha et al., 2016;
Chen et al., 2018) can be cumbersome and limit-
ing. Incorporating such knowledge indirectly via
modified word embeddings (Faruqui et al., 2015;
Mrkšić et al., 2016), as we show, can have little
positive impact and can even be detrimental.

Our proposed method, which is task-specific
but model-independent, is inspired by data-
augmentation techniques. We generate new
training examples by applying knowledge-guided
rules, via only a handful of rule templates, to the
original training examples. Simultaneously, we
also use a sequence-to-sequence or seq2seq model
for each entailment class to generate new hypothe-
ses from a given premise, adaptively creating new
adversarial examples. These can be used with any
entailment model without constraining model ar-
chitecture.

We also introduce the first approach to train a
robust entailment model using a Generative Ad-
versarial Network or GAN (Goodfellow et al.,
2014) style framework. We iteratively improve
both the entailment system (the discriminator)
and the differentiable part of the data-augmenter
(specifically the neural generator), by training
the generator based on the discriminator’s perfor-
mance on the generated examples. Importantly,
unlike the typical use of GANs to create a strong
generator, we use it as a mechanism to create a
strong and robust discriminator.

Our new entailment system, called AdvEntuRe,
demonstrates that in the moderate data regime,

adversarial iterative data-augmentation via only a
handful of linguistic rule templates can be sur-
prisingly powerful. Specifically, we observe 4.7%
accuracy improvement on the challenging SciTail
dataset (Khot et al., 2018) and a 2.8% improve-
ment on 10K-50K training subsets of SNLI. An
evaluation of our algorithm on the negation ex-
amples in the test set of SNLI reveals a 6.1% im-
provement from just a single rule.

2 Related Work

Adversarial example generation has recently re-
ceived much attention in NLP. For example, Jia
and Liang (2017) generate adversarial examples
using manually defined templates for the SQuAD
reading comprehension task. Glockner et al.
(2018) create an adversarial dataset from SNLI
by using WordNet knowledge. Automatic meth-
ods (Iyyer et al., 2018) have also been proposed to
generate adversarial examples through paraphras-
ing. These works reveal how neural network sys-
tems trained on a large corpus can easily break
when faced with carefully designed unseen ad-
versarial patterns at test time. Our motivation is
different. We use adversarial examples at train-
ing time, in a data augmentation setting, to train a
more robust entailment discriminator. The gener-
ator uses explicit knowledge or hand written rules,
and is trained in a end-to-end fashion along with
the discriminator.

Incorporating external rules or linguistic re-
sources in a deep learning model generally re-
quires substantially adapting the model architec-
ture (Sha et al., 2016; Liang et al., 2017; Kang
et al., 2017). This is a model-dependent approach,
which can be cumbersome and constraining. Sim-
ilarly non-neural textual entailment models have
been developed that incorporate knowledge bases.
However, these also require model-specific engi-
neering (Raina et al., 2005; Haghighi et al., 2005;
Silva et al., 2018).

An alternative is the model- and task-
independent route of incorporating linguis-
tic resources via word embeddings that are
retro-fitted (Faruqui et al., 2015) or counter-
fitted (Mrkšić et al., 2016) to such resources. We
demonstrate, however, that this has little positive
impact in our setting and can even be detrimen-
tal. Further, it is unclear how to incorporate
knowledge sources into advanced representations
such as contextual embeddings (McCann et al.,



2420

2017; Peters et al., 2018). We thus focus on a
task-specific but model-independent approach.

Logical rules have also been defined to label ex-
isting examples based on external resources (Hu
et al., 2016). Our focus here is on generating new
training examples.

Our use of the GAN framework to create a bet-
ter discriminator is related to CatGANs (Wang and
Zhang, 2017) and TripleGANs (Chongxuan et al.,
2017) where the discriminator is trained to classify
the original training image classes as well as a new
‘fake’ image class. We, on the other hand, gener-
ate examples belonging to the same classes as the
training examples. Further, unlike the earlier fo-
cus on the vision domain, this is the first approach
to train a discriminator using GANs for a natural
language task with discrete outputs.

3 Adversarial Example Generation

We present three different techniques to create ad-
versarial examples for textual entailment. Specifi-
cally, we show how external knowledge resources,
hand-authored rules, and neural language genera-
tion models can be used to generate such exam-
ples. Before describing these generators in detail,
we introduce the notation used henceforth.

We use lower-case letters for single instances
(e.g., x; p; h), upper-case letters for sets of in-
stances (e.g., X;P;H ), blackboard bold for mod-
els (e.g., D), and calligraphic symbols for discrete
spaces of possible values (e.g., class labels C). For
the textual entailment task, we assume each exam-
ple is represented as a triple (p, h, c), where p is
a premise (a natural language sentence), h is a hy-
pothesis, and c is an entailment label: (a) entails
(v) if h is true whenever p is true; (b) contradicts
(f) if h is false whenever p is true; or (c) neu-
tral (#) if the truth value of h cannot be concluded
from p being true.1

We will introduce various example generators
in the rest of this section. Each such generator, G�,
is defined by a partial function f� and a label g�. If
a sentence s has a certain property required by f�
(e.g., contains a particular string), f� transforms
it into another sentence s0 and g� provides an en-
tailment label from s to s0. Applied to a sentence
s, G� thus either “fails” (if the pre-requisite isn’t
met) or generates a new entailment example triple,�
s; f�(s); g�

�
. For instance, consider the generator

1The symbols are based on Natural Logic (Lakoff, 1970)
and use the notation of MacCartney and Manning (2012).

Source ρ f�(s) g�
Knowledge Base, GKB

WordNet

hyper(x; y) v
anto(x, y) f

syno(x, y) Replace xwith y in s v

PPDB x � y v
SICK c(x; y) c

Hand-authored, GH
Domain knowledge neg negate(s) f

Neural Model, Gs2s
Training data (s2s, c) Gs2sc (s) c

Table 2: Various generators G� characterized by
their source, (partial) transformation function f�
as applied to a sentence s, and entailment label g�

for �:=hypernym(car, vehicle) with the (partial)
transformation function f�:=“Replace car with
vehicle” and the label g�:=entails. f� would fail
when applied to a sentence not containing the
word “car”. Applying f� to the sentence s=“A
man is driving the car” would generate s’=“A
man is driving the vehicle”, creating the example
(s; s0; entails).

The seven generators we use for experimenta-
tion are summarized in Table 2 and discussed in
more detail subsequently. While these particu-
lar generators are simplistic and one can easily
imagine more advanced ones, we show that train-
ing using adversarial examples created using even
these simple generators leads to substantial accu-
racy improvement on two datasets.

3.1 Knowledge-Guided Generators

Large knowledge-bases such as WordNet and
PPDB contain lexical equivalences and other re-
lationships highly relevant for entailment models.
However, even large datasets such as SNLI gen-
erally do not contain most of these relationships
in the training data. E.g., that two dogs entails
animals isn’t captured in the SNLI data. We de-
fine simple generators based on lexical resources
to create adversarial examples that capture the un-
derlying knowledge. This allows models trained
on these examples to learn these relationships.

As discussed earlier, there are different ways
of incorporating such symbolic knowledge into
neural models. Unlike task-agnostic ways of ap-
proaching this goal from a word embedding per-
spective (Faruqui et al., 2015; Mrkšić et al., 2016)



2421

or the model-specific approach (Sha et al., 2016;
Chen et al., 2018), we use this knowledge to gener-
ate task-specific examples. This allows any entail-
ment model to learn how to use these relationships
in the context of the entailment task, helping them
outperform the above task-agnostic alternative.

Our knowledge-guided example genera-
tors, GKB� , use lexical relations available in a
knowledge-base: � := r(x; y) where the relation
r (such as synonym, hypernym, etc.) may differ
across knowledge bases. We use a simple (partial)
transformation function, f�(s):=“Replace x in s
with y”, as described in an earlier example. In
some cases, when part-of-speech (POS) tags are
available, the partial function requires the tags for
x in s and in r(x; y) to match. The entailment
label g� for the resulting examples is also defined
based on the relation r , as summarized in Table 2.

This idea is similar to Natural Logic Inference
or NLI (Lakoff, 1970; Sommers, 1982; Angeli
and Manning, 2014) where words in a sentence
can be replaced by their hypernym/hyponym to
produce entailing/neutral sentences, depending on
their context. We propose a context-agnostic use
of lexical resources that, despite its simplicity, al-
ready results in significant gains. We use three
sources for generators:

WordNet (Miller, 1995) is a large, hand-
curated, semantic lexicon with synonymous words
grouped into synsets. Synsets are connected by
many semantic relations, from which we use hy-
ponym and synonym relations to generate entailing
sentences, and antonym relations to generate con-
tradicting sentences2. Given a relation r(x; y), the
(partial) transformation function f� is the POS-tag
matched replacement of x in s with y, and requires
the POS tag to be noun or verb. NLI provides a
more robust way of using these relations based on
context, which we leave for future work.

PPDB (Ganitkevitch et al., 2013) is a large
resource of lexical, phrasal, and syntactic para-
phrases. We use 24,273 lexical paraphrases in
their smallest set, PPDB-S (Pavlick et al., 2015),
as equivalence relations, x � y. The (partial)
transformation function f� for this generator is
POS-tagged matched replacement of x in s with
y, and the label g� is entails.

2A similar approach was used in a parallel work to gener-
ate an adversarial dataset from SNLI (Glockner et al., 2018).

SICK (Marelli et al., 2014) is dataset with en-
tailment examples of the form (p; h; c), created to
evaluate an entailment model’s ability to capture
compositional knowledge via hand-authored rules.
We use the 12,508 patterns of the form c(x; y) ex-
tracted by Beltagy et al. (2016) by comparing sen-
tences in this dataset, with the property that for
each SICK example (p; h; c), replacing (when ap-
plicable) x with y in p produces h. For simplic-
ity, we ignore positional information in these pat-
terns. The (partial) transformation function f� is
replacement of x in s with y, and the label g� is c.

3.2 Hand-Defined Generators

Even very large entailment datasets have no or
very few examples of certain otherwise common
linguistic constructs such as negation,3 causing
models trained on them to struggle with these con-
structs. A simple model-agnostic way to allevi-
ate this issue is via a negation example generator
whose transformation function f�(s) is negate(s),
described below, and the label g� is contradicts.
negate(s): If s contains a ‘be’ verb (e.g., is,

was), add a “not” after the verb. If not, also add
a “did” or “do” in front based on its tense. E.g.,
change “A person is crossing” to “A person is not
crossing” and “A person crossed” to “A person
did not cross.” While many other rules could be
added, we found that this single rule covered a
majority of the cases. Verb tenses are also consid-
ered4 and changed accordingly. Other functions
such as dropping adverbial clauses or changing
tenses could be defined in a similar manner.

Both the knowledge-guided and hand-defined
generators make local changes to the sentences
based on simple rules. It should be possible to ex-
tend the hand-defined rules to cover the long tail
(as long as they are procedurally definable). How-
ever, a more scalable approach would be to extend
our generators to trainable models that can cover
a wider range of phenomena than hand-defined
rules. Moreover, the applicability of these rules
generally depends on the context which can also
be incorporated in such trainable generators.

3.3 Neural Generators

For each entailment class c, we use a trainable
sequence-to-sequence neural model (Sutskever

3Only 211 examples (2.11%) in the SNLI training set con-
tain negation triggers such as not, ’nt, etc.

4https://www.nodebox.net/code/index.php/Linguistics



2422

et al., 2014; Luong et al., 2015) to generate an en-
tailment example (s; s0; c) from an input sentence
s. The seq2seq model, trained on examples la-
beled c, itself acts as the transformation function
f� of the corresponding generator Gs2sc . The la-
bel g� is set to c. The joint probability of seq2seq
model is:

Gs2sc (Xc ;�c) = G
s2s
c (Hc ; Pc ;�c) (1)

= ΠiP (hi;cjpi;c ;�c)P (hi ) (2)

The loss function for training the seq2seq is:

O�c = argmin
�c

L(Hc ;Gs2sc (Xc ;�c)) (3)

where L is the cross-entropy loss between the
original hypothesisHc and the predicted hypothe-
sis. Cross-entropy is computed for each predicted
word wi against the same in Hc given the se-
quence of previous words in Hc . O�c are the op-
timal parameters in Gs2sc that minimize the loss
for class c. We use the single most likely output
to generate sentences in order to reduce decoding
time.

3.4 Example Generation
The generators described above are used to cre-
ate new entailment examples from the training
data. For each example (p; h; c) in the data, we
can create two new examples:

�
p; f�(p); g�

�
and�

h; f�(h); g�
�
.

The examples generated this way using GKB
and GH can, however, be relatively easy, as the
premise and hypothesis would differ by only a
word or so. We therefore compose such simple
(“first-order”) generated examples with the orig-
inal input example to create more challenging
“second-order” examples. We can create second-
order examples by composing the original exam-
ple (p; h; c) with a generated sentence from hy-
pothesis, f�(h) and premise, f�(p). Figure 1 de-
picts how these two kinds of examples are gener-
ated from an input example (p; h; c).

First, we consider the second-order example be-
tween the original premise and the transformed
hypothesis: (p; f�(h);

L
(c; g�)), where

L
, de-

fined in the left half of Table 3, composes the input
example label c (connecting p and h) and the gen-
erated example label g� to produce a new label.
For instance, if p entails h and h entails f�(h),
p would entail f�. In other words,

L
(v;v) is

v. For example, composing (“A man is playing

P H

P' H'

Entailment in data (x)

Generation (z) 

First/Second-order 
entailment between z & x

Figure 1: Generating first-order (blue) and
second-order (red) examples.

p) h h) h′ p) h′ p) h p) p′ p′) h
c g�

L
c g�

N
v v v v v ?
v f f v f ?
v # # v # #
f v ? f v ?
f f ? f f ?
f # # f # #
# v # # v #
# f # # f #
# # # # # #

Table 3: Entailment label composition functionsL
(left) and

N
(right) for creating second-order

examples. c and g� are the original and generated
labels, resp. v: entails,f: contradicts, #: neutral,
?: undefined

soccer”, “A man is playing a game”, v) with a
generated hypothesis f�(h): “A person is playing
a game.” will give a new second-order entailment
example: (“A man is playing soccer”, “A person is
playing a game”, v).

Second, we create an example from the
generated premise to the original hypothesis:
(f�(p); h;

N
(g�; c)). The composition function

here, denoted
N

and defined in the right half of
Table 3, is often undetermined. For example, if
p entails f�(p) and p entails h, the relation be-
tween f�(p) and h is undetermined i.e.

N
(v;v

) =?. While this particular composition
N

often
leads to undetermined or neutral relations, we use
it here for completeness. For example, compos-
ing the previous example with a generated neu-
tral premise, f�(p): “A person is wearing a cap”
would generate an example (“A person is wearing
a cap”, “A man is playing a game”, #)

The composition function
L

is the same as
the “join” operation in natural logic reason-
ing (Icard III and Moss, 2014), except for two dif-
ferences: (a) relations that do not belong to our



2423

three entailment classes are mapped to ‘?’, and
(b) the exclusivity/alternation relation is mapped
to contradicts. The composition function

N
, on

the other hand, does not map to the join operation.

3.5 Implementation Details

Given the original training examples X, we gener-
ate the examples from each premise and hypothe-
sis in a batch using GKB and GH. We also generate
new hypothesis per class for each premise using
Gs2sc . Using all the generated examples to train the
model would, however, overwhelm the original
training set. For examples, our knowledge-guided
generators GKB can be applied in 17,258,314 dif-
ferent ways.

To avoid this, we sub-sample our synthetic ex-
amples to ensure that they are proportional to the
input examplesX , specifically they are bounded to
˛jX j where ˛ is tuned for each dataset. Also, as
seen in Table 3, our knowledge-guided generators
are more likely to generate neutral examples than
any other class. To make sure that the labels are
not skewed, we also sub-sample the examples to
ensure that our generated examples have the same
class distribution as the input batch. The SciTail
dataset only contains two classes: entails mapped
to v and neutral mapped to f. As a result, gen-
erated examples that do not belong to these two
classes are ignored.

The sub-sampling, however, has a negative side-
effect where our generated examples end up us-
ing a small number of lexical relations from the
large knowledge bases. On moderate datasets, this
would cause the entailment model to potentially
just memorize these few lexical relations. Hence,
we generate new entailment examples for each
mini-batch and update the model parameters based
on the training+generated examples in this batch.

The overall example generation procedure goes
as follows: For each mini-batch X (1) randomly
choose 3 applicable rules per source and sentence
(e.g., replacing men with people based on PPDB
in premise is one rule), (2) produce examplesZal l
using GKB, GH and Gs2s, (3) randomly sub-select
examples Z from Zal l to ensure the balance be-
tween classes and jZj= ˛jX j.

4 AdvEntuRe

Figure 2 shows the complete architecture of our
model, AdvEntuRe (ADVersarial training for tex-
tual ENTailment Using Rule-based Examples.).

The entailment model D is shown with the white
box and two proposed generators are shown using
black boxes. We combine the two symbolic un-
trained generators, GKB and GH into a single Grule
model. We combine the generated adversarial ex-
amples Z with the original training examples X to
train the discriminator. Next, we describe how the
individual models are trained and finally present
our new approach to train the generator based on
the discriminator’s performance.

4.1 Discriminator Training
We use one of the state-of-the-art entailment mod-
els (at the time of its publication) on SNLI, de-
composable attention model (Parikh et al., 2016)
with intra-sentence attention as our discriminator
D. The model attends each word in hypothesis
with each word in the premise, compares each pair
of the attentions, and then aggregates them as a fi-
nal representation. This discriminator model can
be easily replaced with any other entailment model
without any other change to the AdvEntuRe archi-
tecture. We pre-train our discriminator D on the
original dataset, X=(P, H, C) using:

D(X ; � ) = argmax
OC

D( OC jP;H ; � ) (4)

O� = argmin
�

L(C;D(X ; � )) (5)

where L is cross-entropy loss function between
the true labels, Y and the predicted classes, and
O� are the learned parameters.

4.2 Generator Training
Our knowledge-guided and hand-defined genera-
tors are symbolic parameter-less methods which
are not currently trained. For simplicity, we will
refer to the set of symbolic rule-based generators
as Grule := GKB [GH. The neural generator Gs2s,
on the other hand, can be trained as described ear-
lier. We leave the training of the symbolic models
for future work.

4.3 Adversarial Training
We now present our approach to iteratively train
the discriminator and generator in a GAN-style
framework. Unlike traditional GAN (Goodfellow
et al., 2014) on image/text generation that aims
to obtain better generators, our goal is to build
a robust discriminator regularized by the genera-
tors (Gs2s and Grule). The discriminator and gen-
erator are iteratively trained against each other to



2424

[H → H’, WordNet("part of” → “piece of”), C] The 
chromosomes are a piece of our body cells
[P → P’, NEG, C] Humans don’t have 23 chromosome 
pairs

Data

[P → H, C] The chromosomes are pulled to the two pairs 
of chromosomes, that are identical
[P → H, C] The chromosomes are a part of our body 
cells

x z

G
rule

G
s2s

D

[P] Humans have 23 
chromosome pairs 
[H] The chromosomes are a 
part of our body cells

C

ρPPDB/WordNet
SICK/Hand

Figure 2: Overview of AdvEntuRe, our model for knowledge-guided textual entailment.

Algorithm 1 Training procedure for AdvEntuRe.

1: pretrain discriminator D( O� ) on X;
2: pretrain generators Gs2sc ( O�) on X;
3: for number of training iterations do
4: for mini-batch B  X do
5: generate examples from G
6: ZG(G(B;�),
7: balance X and ZG s.t. jZG j � ˛jX j
8: optimize discriminator:
9: O� = argmin� LD(X +ZG ; � )

10: optimize generator:
11: O� = argmin� LGs2s(ZG ;LD;�)
12: Update �  O� ;�  O�

achieve better discrimination on the augmented
data from the generator and better example gen-
eration against the learned discriminator. Algo-
rithm 1 shows our training procedure.

First, we pre-train the discriminator D and the
seq2seq generators Gs2s on the original data X .
We alternate the training of the discriminator and
generators over K iterations (set to 30 in our ex-
periments).

For each iteration, we take a mini-batch B from
our original data X . For each mini-batch, we
generate new entailment examples, ZG using our
adversarial examples generator. Once we collect
all the generated examples, we balance the ex-
amples based on their source and label (as de-
scribed in Section 3.5). In each training itera-
tion, we optimize the discriminator against the
augmented training data, X + ZG and use the
discriminator loss to guide the generator to pick
challenging examples. For every mini-batch of
examples X + ZG , we compute the discrimina-

tor loss L(C ;D(X + ZG ; � )) and apply the neg-
ative of this loss to each word of the generated
sentence in Gs2s. In other words, the discrimina-
tor loss value replaces the cross-entropy loss used
to train the seq2seq model (similar to a REIN-
FORCE (Williams, 1992) reward). This basic ap-
proach uses the loss over the entire batch to update
the generator, ignoring whether specific examples
were hard or easy for the discriminator. Instead,
one could update the generator per example based
on the discriminator’s loss on that example. We
leave this for future work.

5 Experiments

Our empirical assessment focuses on two key
questions: (a) Can a handful of rule templates im-
prove a state-of-the-art entailment system, espe-
cially with moderate amounts of training data? (b)
Can iterative GAN-style training lead to an im-
proved discriminator?

To this end, we assess various models on the
two entailment datasets mentioned earlier: SNLI
(570K examples) and SciTail (27K examples).5 To
test our hypothesis that adversarial example based
training prevents overfitting in small to moderate
training data regimes, we compare model accura-
cies on the test sets when using 1%, 10%, 50%,
and 100% subsamples of the train and dev sets.

We consider two baseline models: D, the De-
composable Attention model (Parikh et al., 2016)
with intra-sentence attention using pre-trained
word embeddings (Pennington et al., 2014); and
Dretro which extends D with word embeddings
initialized by retrofitted vectors (Faruqui et al.,
2015). The vectors are retrofitted on PPDB, Word-

5SNLI has a 96.4%/1.7%/1.7% split and SciTail has a
87.3%/4.8%/7.8% split on train, valid, and test sets, resp.



2425

Table 4: Test accuracies with different subsam-
pling ratios on SNLI (top) and SciTail (bottom).

SNLI 1% 10% 50% 100%
D 57.68 75.03 82.77 84.52
Dretro 57.04 73.45 81.18 84.14
AdvEntuRe
x D + Gs2s 58.35 75.66 82.91 84.68
x D + Grule 60.45 77.11 83.51 84.40
x D + Grule + Gs2s 59.33 76.03 83.02 83.25

SciTail 1% 10% 50% 100%
D 56.60 60.84 73.24 74.29
Dretro 59.75 67.99 69.05 72.63
AdvEntuRe
x D + Gs2s 65.78 70.77 74.68 76.92
x D + Grule 61.74 66.53 73.99 79.03
x D + Grule + Gs2s 63.28 66.78 74.77 78.60

Net, FrameNet, and all of these, with the best re-
sults for each dataset reported here.

Our proposed model, AdvEntuRe, is evaluated
in three flavors: D augmented with examples gen-
erated by Grule, Gs2s, or both, where Grule =
GKB[GH. In the first two cases, we create new ex-
amples for each batch in every epoch using a fixed
generator (cf. Section 3.5). In the third case (D +
Grule + Gs2s), we use the GAN-style training.

We uses grid search to find the best hyper-
parameters for D based on the validation set: hid-
den size 200 for LSTM layer, embedding size 300,
dropout ratio 0.2, and fine-tuned embeddings.

The ratio between the number of generated vs.
original examples, ˛ is empirically chosen to be
1.0 for SNLI and 0.5 for SciTail, based on vali-
dation set performance. Generally, very few gen-
erated examples (small ˛) has little impact, while
too many of them overwhelm the original dataset
resulting in worse scores (cf. Appendix for more
details).

5.1 Main Results

Table 4 summarizes the test set accuracies of the
different models using various subsampling ratios
for SNLI and SciTail training data.

We make a few observations. First, Dretro is in-
effective or even detrimental in most cases, except
on SciTail when 1% (235 examples) or 10% (2.3K
examples) of the training data is used. The gain in
these two cases is likely because retrofitted lexical
rules are helpful with extremely less data training
while not as data size increases.

On the other hand, our method always achieves

Table 5: Test accuracies across various rules R
and classes C. Since SciTail has two classes, we
only report results on two classes of Gs2s

R/C SNLI (5%) SciTail (10%)

D
+
G

ru
le

D 69.18 60.84
+ PPDB 72.81 (+3.6%) 65.52 (+4.6%)
+ SICK 71.32 (+2.1%) 67.49 (+6.5%)
+ WordNet 71.54 (+2.3%) 64.67 (+3.8%)
+ HAND 71.15 (+1.9%) 69.05 (+8.2%)
+ all 71.31 (+2.1%) 64.16 (+3.3%)

D
+
G

s2
s

D 69.18 60.84
+ positive 71.21 (+2.0%) 67.49 (+6.6%)
+ negative 71.76 (+2.6%) 68.95 (+8.1%)
+ neutral 71.72 (+2.5%) -
+ all 72.28 (+3.1%) 70.77 (+9.9%)

the best result compared to the baselines (D and
Dretro). Especially, significant improvements are
made in less data setting: +2.77% in SNLI (1%)
and 9.18% in SciTail (1%). Moreover, D + Grule’s
accuracy on SciTail (100%) also outperforms
the previous state-of-the-art model (DGEM (Khot
et al., 2018), which achieves 77.3%) for that
dataset by 1.7%.

Among the three different generators combined
with D, both Grule and Gs2s are useful in Sci-
Tail, while Grule is much more useful than Gs2s on
SNLI. We hypothesize that seq2seq model trained
on large training sets such as SNLI will be able
to reproduce the input sentences. Adversarial ex-
amples from such a model are not useful since
the entailment model uses the same training exam-
ples. However, on smaller sets, the seq2seq model
would introduce noise that can improve the robust-
ness of the model.

5.2 Ablation Study

To evaluate the impact of each generator, we per-
form ablation tests against each symbolic genera-
tor in D + Grule and the generator Gs2sc for each
entailment class c. We use a 5% sample of SNLI
and a 10% sample of SciTail. The results are sum-
marized in Table 5.

Interestingly, while PPDB (phrasal para-
phrases) helps the most (+3.6%) on SNLI, simple
negation rules help significantly (+8.2%) on Sc-
iTail dataset. Since most entailment examples in
SNLI are minor rewrites by Turkers, PPDB often
contains these simple paraphrases. For SciTail, the
sentences are authored independently with lim-
ited gains from simple paraphrasing. However, a
model trained on only 10% of the dataset (2.3K



2426

Table 6: Given a premise P (underlined), examples of hypothesis sentences H’ generated by seq2seq
generators Gs2s, and premise sentences P’ generated by rule based generators Grule, on the full SNLI
data. Replaced words or phrases are shown in bold. This illustrates that even simple, easy-to-define
rules can generate useful adversarial examples.

P a person on a horse jumps over a broken down airplane
H’: Gs2sc=v a person is on a horse jumps over a rail, a person jumping over a plane
H’: Gs2sc=f a person is riding a horse in a field with a dog in a red coat
H’: Gs2sc=# a person is in a blue dog is in a park
P (or H) a dirt bike rider catches some air going off a large hill
P’: GKB(PPDB)�=�;g�=v a dirt motorcycle rider catches some air going off a large hill
P’: GKB(SICK)�=c;g�=# a dirt bike man on yellow bike catches some air going off a large hill
P’: GKB(WordNet)�=syno;g�=v a dirt bike rider catches some atmosphere going off a large hill
P’: GHand�=neg;g�=f a dirt bike rider do not catch some air going off a large hill

examples) would end up learning a model relying
on purely word overlap. We believe that the sim-
ple negation examples introduce neutral examples
with high lexical overlap, forcing the model to find
a more informative signal.

On the other hand, using all classes for Gs2s re-
sults in the best performance, supporting the ef-
fectiveness of the GAN framework for penaliz-
ing or rewarding generated sentences based on
D’s loss. Preferential selection of rules within the
GAN framework remains a promising direction.

5.3 Qualitative Results

Table 6 shows examples generated by various
methods in AdvEntuRe. As shown, both seq2seq
and rule based generators produce reasonable sen-
tences according to classes and rules. As ex-
pected, seq2seq models trained on very few exam-
ples generate noisy sentences. The quality of our
knowledge-guided generators, on the other hand,
does not depend on the training set size and they
still produce reliable sentences.

5.4 Case Study: Negation

For further analysis of the negation-based gener-
ator in Table 1, we collect only the negation ex-
amples in test set of SNLI, henceforth referred to
as nega-SNLI. Specifically, we extract examples
where either the premise or the hypothesis con-
tains “not”, “no”, “never”, or a word that ends with
“n’t’. These do not cover more subtle ways of ex-
pressing negation such as “seldom” and the use of
antonyms. nega-SNLI contains 201 examples with
the following label distribution: 51 (25.4%) neu-

tral, 42 (20.9%) entails, 108 (53.7%) contradicts.
Table 7 shows examples in each category.

Table 7: Negation examples in nega-SNLI

v
P: several women are playing volleyball.
H: this doesn’t look like soccer.

#

P: a man with no shirt on is performing
with a baton.
H: a man is trying his best at the national
championship of baton.

f

P: island native fishermen reeling in their
nets after a long day’s work.
H: the men did not go to work today but
instead played bridge.

While D achieves an accuracy of only 76.64%6
on nega-SNLI, D + GH with negate is substan-
tially more successful (+6.1%) at handling nega-
tion, achieving an accuracy of 82.74%.

6 Conclusion

We introduced an adversarial training architec-
ture for textual entailment. Our seq2seq and
knowledge-guided example generators, trained in
an end-to-end fashion, can be used to make any
base entailment model more robust. The effec-
tiveness of this approach is demonstrated by the
significant improvement it achieves on both SNLI
and SciTail, especially in the low to medium data
regimes. Our rule-based generators can be ex-
panded to cover more patterns and phenomena,
and the seq2seq generator extended to incorporate
per-example loss for adversarial training.

6This is much less than the full test accuracy of 84.52%.



2427

References
Gabor Angeli and Christopher D Manning. 2014. Nat-

uralLI: Natural logic inference for common sense
reasoning. In EMNLP, pages 534–545.

Islam Beltagy, Stephen Roller, Pengxiang Cheng, Ka-
trin Erk, and Raymond J. Mooney. 2016. Repre-
senting meaning with a combination of logical and
distributional models. Computational Linguistics,
42:763–808.

Samuel R. Bowman, Gabor Angeli, Christopher Potts,
and Christopher D. Manning. 2015. A large anno-
tated corpus for learning natural language inference.
In EMNLP.

Qian Chen, Xiaodan Zhu, Zhen-Hua Ling, and Diana
Inkpen. 2018. Natural language inference with ex-
ternal knowledge. In ACL.

LI Chongxuan, Taufik Xu, Jun Zhu, and Bo Zhang.
2017. Triple generative adversarial nets. In NIPS,
pages 4091–4101.

Manaal Faruqui, Jesse Dodge, Sujay K Jauhar, Chris
Dyer, Eduard Hovy, and Noah A Smith. 2015.
Retrofitting word vectors to semantic lexicons.
NAACL.

Juri Ganitkevitch, Benjamin Van Durme, and Chris
Callison-Burch. 2013. PPDB: The paraphrase
database. In NAACL-HLT, pages 758–764.

Max Glockner, Vered Shwartz, and Yoav Goldberg.
2018. Breaking nli systems with sentences that re-
quire simple lexical inferences. In ACL.

Yichen Gong, Heng Luo, and Jian Zhang. 2018. Natu-
ral language inference over interaction space. ICLR.

Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza,
Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron
Courville, and Yoshua Bengio. 2014. Generative ad-
versarial nets. In NIPS, pages 2672–2680.

Suchin Gururangan, Swabha Swayamdipta, Omer
Levy, Roy Schwartz, Samuel R Bowman, and
Noah A Smith. 2018. Annotation artifacts in natural
language inference data. In NAACL.

Aria Haghighi, Andrew Ng, and Christopher Manning.
2005. Robust textual inference via graph matching.
In EMNLP.

Zhiting Hu, Xuezhe Ma, Zhengzhong Liu, Eduard
Hovy, and Eric Xing. 2016. Harnessing deep neu-
ral networks with logic rules. ACL.

Thomas Icard III and Lawrence Moss. 2014. Recent
progress in monotonicity. LiLT (Linguistic Issues in
Language Technology), 9.

Mohit Iyyer, John Wieting, Kevin Gimpel, and Luke S.
Zettlemoyer. 2018. Adversarial example generation
with syntactically controlled paraphrase networks.
In NAACL.

R. Jia and Percy Liang. 2017. Adversarial examples
for evaluating reading comprehension systems. In
EMNLP.

Dongyeop Kang, Varun Gangal, Ang Lu, Zheng Chen,
and Eduard Hovy. 2017. Detecting and explaining
causes from text for a time series event. In EMNLP.

Tushar Khot, Ashish Sabharwal, and Peter Clark. 2018.
SciTail: A textual entailment dataset from science
question answering. AAAI.

George Lakoff. 1970. Linguistics and Natural Logic.
Synthese, 22(1-2):151–271.

Chen Liang, Jonathan Berant, Quoc Le, Kenneth D.
Forbus, and Ni Lao. 2017. Neural symbolic ma-
chines: Learning semantic parsers on freebase with
weak supervision. In ACL.

Thang Luong, Hieu Pham, and Christopher D. Man-
ning. 2015. Effective approaches to attention-based
neural machine translation. In EMNLP.

Bill MacCartney and Christopher D. Manning. 2012.
Natural logic and natural language inference. In
Computing Meaning. Text, Speech and Language
Technology, volume 47.

Marco Marelli, Stefano Menini, Marco Baroni, Luisa
Bentivogli, Raffaella Bernardi, and Roberto Zam-
parelli. 2014. A SICK cure for the evaluation of
compositional distributional semantic models. In
LREC, pages 216–223.

Bryan McCann, James Bradbury, Caiming Xiong, and
Richard Socher. 2017. Learned in translation: Con-
textualized word vectors. In NIPS.

George A Miller. 1995. WordNet: a lexical
database for english. Communications of the ACM,
38(11):39–41.

Nikola Mrkšić, Diarmuid O Séaghdha, Blaise Thom-
son, Milica Gašić, Lina Rojas-Barahona, Pei-Hao
Su, David Vandyke, Tsung-Hsien Wen, and Steve
Young. 2016. Counter-fitting word vectors to lin-
guistic constraints. In HLT-NAACL.

Ankur P. Parikh, Oscar Täckström, Dipanjan Das, and
Jakob Uszkoreit. 2016. A decomposable attention
model for natural language inference. In EMNLP.

Ellie Pavlick, Pushpendre Rastogi, Juri Ganitkevitch,
Benjamin Van Durme, and Chris Callison-Burch.
2015. PPDB 2.0: Better paraphrase ranking, fine-
grained entailment relations, word embeddings, and
style classification. In ACL.

Jeffrey Pennington, Richard Socher, and Christopher
Manning. 2014. GloVe: Global vectors for word
representation. In EMNLP, pages 1532–1543.

Matthew E Peters, Mark Neumann, Mohit Iyyer, Matt
Gardner, Christopher Clark, Kenton Lee, and Luke
Zettlemoyer. 2018. Deep contextualized word rep-
resentations. In NAACL.



2428

Rajat Raina, Aria Haghighi, Christopher Cox, Jenny
Finkel, Jeff Michels, Kristina Toutanova, Bill Mac-
Cartney, Marie-Catherine de Marneffe, Christo-
pher D Manning, and Andrew Y Ng. 2005. Robust
textual inference using diverse knowledge sources.
In 1st PASCAL Recognition Textual Entailment
Challenge Workshop.

Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and
Percy Liang. 2016. SQuAD: 100,000+ questions for
machine comprehension of text. In EMNLP.

Lei Sha, Sujian Li, Baobao Chang, and Zhifang Sui.
2016. Recognizing textual entailment via multi-task
knowledge assisted lstm. In Chinese Computational
Linguistics and Natural Language Processing Based
on Naturally Annotated Big Data, pages 285–298.
Springer.

Vivian S Silva, André Freitas, and Siegfried Hand-
schuh. 2018. Recognizing and justifying text entail-
ment through distributional navigation on definition
graphs. In AAAI.

Fred Sommers. 1982. The logic of natural language.

Ilya Sutskever, Oriol Vinyals, and Quoc V Le. 2014.
Sequence to sequence learning with neural net-
works. In NIPS, pages 3104–3112.

Shanshan Wang and Lei Zhang. 2017. CatGAN:
Coupled adversarial transfer for domain generation.
CoRR, abs/1711.08904.

Zhiguo Wang, Wael Hamza, and Radu Florian. 2017.
Bilateral multi-perspective matching for natural lan-
guage sentences. In IJCAI.

Ronald J Williams. 1992. Simple statistical gradient-
following algorithms for connectionist reinforce-
ment learning. In Reinforcement Learning, pages
5–32. Springer.


