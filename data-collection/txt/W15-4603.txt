



















































Knowledge transfer between speakers for personalised dialogue management


Proceedings of the SIGDIAL 2015 Conference, pages 12–21,
Prague, Czech Republic, 2-4 September 2015. c©2015 Association for Computational Linguistics

Knowledge transfer between speakers for personalised dialogue
management

Iñigo Casanueva, Thomas Hain, Heidi Christensen, Ricard Marxer and Phil Green
Department of Computer Science, University of Sheffield, United Kingdom
{i.casanueva, t.hain, h.christensen, r.marxer,

p.green}@sheffield.ac.uk

Abstract
Model-free reinforcement learning has
been shown to be a promising data driven
approach for automatic dialogue policy
optimization, but a relatively large amount
of dialogue interactions is needed be-
fore the system reaches reasonable perfor-
mance. Recently, Gaussian process based
reinforcement learning methods have been
shown to reduce the number of dialogues
needed to reach optimal performance, and
pre-training the policy with data gathered
from different dialogue systems has fur-
ther reduced this amount. Following this
idea, a dialogue system designed for a sin-
gle speaker can be initialised with data
from other speakers, but if the dynamics of
the speakers are very different the model
will have a poor performance. When data
gathered from different speakers is avail-
able, selecting the data from the most sim-
ilar ones might improve the performance.
We propose a method which automatically
selects the data to transfer by defining a
similarity measure between speakers, and
uses this measure to weight the influence
of the data from each speaker in the pol-
icy model. The methods are tested by sim-
ulating users with different severities of
dysarthria interacting with a voice enabled
environmental control system.

1 Introduction
Partially observable Markov decision processes
(POMDP) (Young et al., 2013) are a popular
framework to model dialogue management as
a reinforcement learning (RL) problem. In a
POMDP, a state tracker (Thomson and Young,
2010)(Williams, 2014) maintains a distribution
over possible user goals (states), called the be-
lief state, and RL methods (Sutton and Barto,

1998) are used to optimize a metric called cumu-
lative reward, a score that combines dialogue suc-
cess rate and dialogue length. However, existing
model-based RL approaches become intractable
for real world sized dialogue systems (Williams
and Young, 2007), and model-free approaches of-
ten need a large number of dialogues to converge
to the optimal policy (Jurčı́ček et al., 2012).

Recently, Gaussian process (GP) based RL (En-
gel et al., 2005) has been proposed for dialogue
policy optimization, reducing the number of in-
teractions needed to converge to the optimal pol-
icy by an order of magnitude with respect to other
POMDP models, allowing the policy to be learned
directly from real users interactions (Gašić et al.,
2013 a). In addition, using transfer learning meth-
ods (Taylor and Stone, 2009) to initialise the pol-
icy with data gathered from dialogue systems in
different domains has increased the learning speed
of the policy further (Gašić et al., 2013 b), and
provided an acceptable system performance when
there is no domain specific data available. In the
case of dialogue managers personalised for a sin-
gle speaker, data gathered from other “source”
speakers can be used to pre-train the policy, but if
the dynamics of the other speakers are very differ-
ent, this data will have a different distribution than
the data of the current “target” speaker, and there-
fore, using this data to train the policy model does
not have any benefit. In the context of speaker
specific acoustic models for users with dysarthria
(a speech impairment), Christensen et al. (2014)
demonstrated that using a speaker similarity met-
ric to select the data to train the acoustic mod-
els improves ASR performance. Taking this idea
into dialogue management, if a similarity metric
is defined between different speakers, this metric
can be used to select which data from the source
speakers is used to train the model, and even to
weight the influence of the data from each speaker
in the model. As GP-RL is a non-parametric

12



method, a straightforward way to transfer knowl-
edge is to directly initialise the GP model for the
target speaker using data from source speakers,
and update the GP with the data from the tar-
get speaker as this is gathered through interaction.
But GP-RL soon becomes intractable as the data
amount increases, limiting the amount of data that
can be transferred. Gašić et al. (2013 a) proposes
to transfer knowledge between domains by using
the source data to train a prior GP, whose pos-
terior is used as prior mean in the new GP. An-
other option is to use a GP approximation method
(Quiñonero and Rasmussen, 2005) which permits
data selection, use the speaker similarity metric to
select the source data to initialise the policy, and
then discard source data points as data points from
the target speaker become available, keeping the
number of data points up to a maximum.

This paper investigates knowledge transfer be-
tween speakers in the context of a spoken environ-
mental control system personalised for speakers
with dysarthria (Christensen et al., 2013), where
the ASR is adapted as speaker specific data is
gathered (Christensen et al., 2012), thus improv-
ing the ASR performance with usage. The pa-
per is organised as follows: Section 2 gives the
background of GP-RL and defines the methods to
select and weight the transferred data. Section
3 presents the experimental setup of the environ-
mental control system and the different dysarthric
simulated users, as well as the different features
used to define the speaker similarities. In Section
4 the results of the experiments are presented and
explained and Section 5 concludes the paper.

2 GPs for reinforcement learning
The objective of a POMDP based dialogue man-
ager is to find the policy π(b) = a that maximizes
the expected cumulative reward ci defined as the
sum of immediate rewards from time step i until
the dialogue is finished, where a ∈ A is the action
taken by the manager, and the belief state b is a
probability distribution over a discrete set of states
S . The Q-function defines the expected cumula-
tive reward when the dialogue is in belief state bi
and action ai is taken, following policy π:

Q(bi, ai) = Eπ[ci] ; where ci =
N∑
n=i

γn−irn (1)

where N is the time step at which the terminal ac-
tion is taken (end of the dialogue), ri is the im-
mediate reward given by the reward function, and

0 ≤ γ ≤ 1 is the discount factor, which weights
future rewards. If ci is considered to be a random
variable, it can be modelled as a mean plus a resid-
ual, ci = Q(bi, ai) + ∆Q(bi, ai). Then the im-
mediate reward ri can be written recursively as the
temporal difference (TD) between Q at time i and
i+ 1:

ri = Q(bi, ai) + ∆Q(bi, ai)
−γiQ(bi+1, ai+1)− γi∆Q(bi+1, ai+1)(2)

where γi = 0 if ai is a terminal action1, and the
discount factor γ otherwise. Given a set of ob-
served belief-action points (bi, ai), with their re-
spective ri values, the set of linear equations can
be represented in matrix form as:

rt−1 = Htqt + Ht∆qt (3)

where qt=[Q(b1, a1), Q(b2, a2), ..., Q(bt, at)]>,
∆qt=[∆Q(b1, a1),∆Q(b2, a2), ...,∆Q(bt, at)]>

, rt−1 = [r1, r2, ..., rt−1]> and

Ht =


1 −γ1 . . . 0 0
0 1 . . . 0 0
...

. . . . . .
...

...
0 0 . . . 1 −γt−1


If the random variables qt are assumed to have
a joint Gaussian distribution with zero mean and
∆Q(bi, ai) ∼ N (0, σ2), the system can be mod-
elled as a GP (Rasmussen and Williams, 2005),
with the covariance matrix determined by a kernel
function defined independently over the belief and
the action space (Engel et al., 2005):

ki,j = k((bi, ai), (bj , aj)) = kb(bi,bj)ka(ai, aj)
(4)

To simplify the notation, from now on xi =
(bi, ai) will be defined as each belief-action point,
and KY,Y ′ as the matrix of size |Y| × |Y′| whose
elements are computed by the kernel function (eq.
4) between any set of points Y and Y′. For a new
belief-action point x∗ = (b∗, a∗), the posterior of
the expected cumulative reward can be computed:

Q(x∗)|Xt, rt−1 ∼ N (Q̄(x∗), Q̂(x∗))
Q̄(x∗) = K∗,XH>t (HtKX,XH

>
t + Σt)

−1rt−1
Q̂(x∗) = k(x∗,x∗)

−K∗,XH>t (HtKX,XH>t + Σt)−1HtKX,∗
(5)

1As dialogue management is an episodic RL problem,
the temporal difference relationship between 2 consecutive
belief-action points only happens if the points belong to the
same dialogue.

13



where Xt is the set of size t of all the previously
visited (bi, ai) points, ∗ denotes the set of size
1 composed by the new belief-action point to be
evaluated and Σt = σ2HtH>t . Q̄ and Q̂ represent
the mean and the variance of Q respectively.

To further simplify the notation it is possible to
redefine eq. 5 by defining a kernel in the tempo-
ral difference space instead of in the belief-action
space. If the set of belief-action points Xt is rede-
fined2 as Zt where zi = (bi, ai,bi+1, ai+1), with
bi+1 and ai+1 set to any default values if ai is a
terminal action, a kernel function between 2 tem-
poral difference points can be defined as:

ktdi,j = k
td(zi, zj)

= ktd((bi, ai,bi+1, ai+1), (bj , aj ,bj+1, aj+1))
= (ki,j + γiγjki+1,j+1 − γiki+1,j − γjki,j+1)

(6)
where ki,j is the kernel function in the belief-
action space (eq. 4) and γi = 0 and γj = 0 if ai
and aj are terminal actions respectively, or the dis-
count factor γ otherwise (as in eq. 2). When ai is
a terminal action, the value of ai+1 and bi+1 in zi
is irrelevant, as it will be multiplied by γi = 0. In
the same way, when this kernel is used to compute
the covariance vector between a new test point
and the set Zt, as the new point x∗ = (b∗, a∗)
lies in the belief-action space, it is redefined as
z∗ = (b∗, a∗,b∗+1, a∗+1) with b∗+1 and a∗+1 set
to default values. Then, a∗ is considered a terminal
action, so b∗+1 and a∗+1 won’t affect the value of
ktdi∗ due to γ∗ = 0. A more detailed derivation of
the temporal difference kernel is given in appendix
A. Using the temporal difference kernel defined in
eq. 6, eq. 5 can be rewritten as:

Q(z∗)|Zt, rt−1 ∼ N (Q̄(z∗), Q̂(z∗))
Q̄(z∗) = Ktd∗,Z(K

td
Z,Z + Σt)

−1rt−1
Q̂(z∗) = ktd(z∗, z∗)−Ktd∗,Z(KtdZ,Z + Σt)−1KtdZ,∗

(7)
where KtdY,Y ′ is the covariance matrix computed
with the temporal difference kernel between any
set of TD points Y and Y′. With this notation,
the shape of the equation for the posterior of Q is
equivalent to classic GP regression models. Thus,
it is straightforward to apply a wide range of well
studied GP techniques, such as sparse methods.
Redefining the belief-action set of points Xt as the
set of temporal difference points Zt also simplifies
the selection of data points (e.g. to select inducing

2Take into account that |Zt| = |Xt| − 1

points in sparse models), because the dependency
between consecutive points is well defined.

The GP literature proposes various sparse meth-
ods which select a subset of inducing points U
of size m < t from the set of training points Z
(Quiñonero and Rasmussen, 2005). In this pa-
per the deterministic training conditional (DTC)
method is used. Once the subset of points
has been selected and assuming ∆Q(bi, ai) −
γi∆Q(bi+1, ai+1) ∼ N (0, σ2) as in (Engel et al.,
2003), the GP posterior can be approximated in
O(t ·m2) with the DTC method as:
Qdtc(z∗)|Zt, rt−1 ∼ N (Q̄dtc(z∗), Q̂dtc(z∗))
Q̄dtc(z∗) = σ−2Ktd∗,UΛK

td
U,Zrt−1

Q̂dtc(z∗) = ktd(z∗, z∗)−Φ + Ktd∗,UΛKtdU,∗
(8)

where Λ = (σ−2KtdU,ZK
td
Z,U +K

td
U,U )

−1 and Φ =
Ktd∗,U (K

td
U,U )

−1KtdU,∗.
Once the posterior for any new belief-action

point can be computed with eq. 7 or eq. 8, the pol-
icy π(b) = a can be computed as the action a that
maximizes the Q-function from the current belief
state b∗, but in order to avoid getting stuck in
a local optimum, an exploration-exploitation ap-
proach should be taken. One of the advantages of
GPs is that they compute the uncertainty of the ex-
pected cumulative reward in form of a variance,
which can be used as a metric for active explo-
ration (Geist and Pietquin, 2011) to speed up the
learning of the policy with an �-greedy approach:

π(b∗) =
{ arg max

a∈A
Q̄(b∗, a) with prob. (1− �)

arg max
a∈A

Q̂(b∗, a) with prob. �

(9)
where � controls the exploration rate. The pol-
icy optimization loop is performed following the
Episodic GP-Sarsa algorithm defined by (Gašić
and Young, 2014).
2.1 Transfer learning with GP-RL
The scenario where a statistical model for a spe-
cific “target” task must be trained, but only data
from different but related “source” tasks is avail-
able, is known as transfer learning (Pan and Yang,
2010). In the context of this paper the different
tasks will be dialogues with different speakers, and
three points of transfer learning will be addressed:

• How to transfer the knowledge

• In the case of multiple source speakers, which
data to transfer, and

14



• How to weight data from different sources.

In the context of reinforcement learning (Taylor
and Stone, 2009) and dialogue policy optimization
(Gašić et al., 2013 a), transfer learning has been
shown to increase the performance of the system
in the initial stages of use and to speed up the pol-
icy learning, requiring a smaller amount of target
data to reach the optimal policy.

2.1.1 Knowledge transfer
The most straightforward way to transfer the data
in GP-RL is to initialise the set of temporal differ-
ence points Zt of the GP with the source points
and then continue updating it with target data
points as they are gathered through interaction.
However, this approach has a few shortcomings.
First, as GP-RLs complexity increases with the
number of data points, the model might quickly
become intractable if it is initialised with too many
source points. Also, when data points from the tar-
get speaker are gathered through interaction, the
source points may not improve the performance
of the system, while increasing the model com-
plexity. Second, as the computation of the vari-
ance for a new point depends on the number of
close points already visited, the variance of the
new belief-action points will be reduced by the ef-
fect of the source points close in the belief-action
space. If the distribution of the source data points
is unbalanced, the effectiveness of the policy of
eq. 9 will be affected. Gašić et al. (2013 a) pro-
poses to use the source points to train a prior GP,
and use its posterior as mean function for the GP
trained with the target points. With this approach,
the mean of the posterior in eq. 7 will be modified
as:

Q̄(z∗) = m(z∗)+Ktd∗,Z(K
td
Z,Z+Σ)

−1(rt−1−mt)
(10)

where m(z∗) is the mean of the posterior of the
Q-function given by the prior GP and mt =
[m(z0), ...,m(zt)]>. If the DTC approach (eq. 8)
is taken, the posterior Q-function mean becomes:

Q̄dtc(z∗) = m(z∗)+σ−2Ktd∗,UΛK
td
U,Z(rt−1−mt)

(11)
This approach has the advantage of being com-

putationally cheaper than the former method while
modelling the uncertainty for new target points
more accurately, but at the cost of not taking into
account the correlation between source and target
points, which might reduce the performance when
there is a small amount of target data.

A third approach combines the two previous
methods, using a portion of the transfer points to
train a GP for the prior mean function, while the
rest is used to initialise the set Zt of the GP that
will be updated with target points. This method
will be computationally cheaper than the first one
while increasing the performance of the second
method with a small amount of target data.

2.1.2 Transfer data selection

As non-parametric models, the complexity of GPs
will increase with the number of data points, lim-
iting the amount of source data that can be trans-
ferred. Additionally, if the points come from
multiple sources, it is possible that the data dis-
tribution from some sources is more similar to
the target speaker than others, hence transferring
data from these sources will increase performance.
We propose to extract a speaker feature vector s
from each speaker and define a similarity function
f(s, s′) between speakers (see sec. 3.4). The data
can be selected by choosing the points from the
source speakers more similar to the target.

With the DTC approach (eq. 8), a subset of
inducing points Um must be selected. The most
straightforward way is to select the most similar
points to the speaker from the transferred points.
As the user interacts with the system and target
data points are gathered, these points may be used
as inducing points. This approach acts like an-
other layer of data selection; the reduced com-
plexity will allow for the transfer of more source
points, while using the target points as inducing
points will mean that only the source points that
lie in the same part of the belief-action space as
the target points have influence on the model.

2.1.3 Transfer data weighting

When transferring data from multiple sources,
the similarity between each source and the target
speaker might be different. Thus the data from a
source more similar to the target should have more
influence in the model than less similar ones. As a
GP is defined by computing covariances between
data points through a kernel function, one way to
weight the data from different sources is to ex-
tend the belief-action vector used to compute the
covariance with the speaker feature vector s ex-
plained in the previous section as xi = (bi, ai, si),
and then extend the kernel (eq. 4) by multiplying
it by a new kernel in the speaker space ks as:

15



kexti,j = k((bi, ai, si), (bj , aj , sj))

= kb(bi,bj)ka(ai, aj)ks(si, sj)
(12)

By adding this extra space to the data points, the
covariance between points will not only depend on
the similarity between points in the belief-action
space, but also in the speaker space, reducing the
covariance between two points that lie in differ-
ent parts of the speaker space. This approach will
also help to partially deal with the variance com-
puting problem of the first model in sec. 2.1.1, as
the source points will lie on a different part of the
speaker space than the new target points, thus hav-
ing less influence in the variance computation.
3 Experimental setup
To test the system in a scenario with high vari-
ability between the dynamics of the speakers,
the experiments are performed within the con-
text of a voice-enabled control system designed
to help speakers with dysarthria to interact with
their home devices (TV, radio, lamps...), where
the speakers have different severities of dysarthria
(this is an instance of the homeService application
(Christensen et al., 2013)). The system has a vo-
cabulary of 36 commands and is organised in a
tree setup where each node in the tree represents
either a device (e.g. “TV”), a property of that de-
vice (e.g. “channel”), or actions that trigger some
change in one of the devices (e.g. “one”, child
of “channel”, will change the TV to channel one).
When the system transitions to one of the terminal
nodes that trigger an action, the action associated
with this node is performed, and subsequently the
system returns to the root node. In the following
experiments a dialogue will be considered finished
when one of the terminal node actions is carried
out. In the non-terminal nodes, the user may ei-
ther speak one of the commands available in that
node (defined by its children nodes) to transition
to them, or say the meta-command “back” to re-
turn to its parent node. The ASR is configured
to recognise single words, so there is no need for
a language understanding system, as the concepts
are just a direct mapping from the ASR output. A
more detailed explanation of the system is given
in (Casanueva et al., 2014) and two example dia-
logues are presented in Appendix B.

3.1 Simulated dysarthric users
In the homeService application, each system is
personalised for a single speaker by adapting the

ASR system’s acoustic model as more data is gath-
ered through interaction, thus increasing the accu-
racy of the ASR over time. In the following exper-
iments, the system is tested by interacting with a
set of simulated users with dysarthria, where each
user interacts with a set of different ASR simu-
lators, arising from the different amounts of data
used to adapt the ASR. To train the ASR simula-
tor for these users, data from a dysarthric speech
database (UASpeech database (Kim et al., 2008))
has been used. Table 1 shows the characteristics
of the 15 speakers of the database, and the ASR
accuracy for each speaker in the 36 word vocabu-
lary of the system without adaptation and adapted
with 500 words from that speaker. Additionally,
an intelligibility measure assessment is presented
for each speaker as the percentage of words spo-
ken by each speaker which are understood by un-
familiar speakers; these are shown in the second
column in table 1.

The system is tested with 6 different simulated
users trained with data from low and medium in-
telligibility3 speakers. Each user interacts with
4 different ASRs, adapted with 0, 150, 300 and
500 words respectively. For a more detailed ex-
planation of the simulated users configuration, the
reader may refer to (Casanueva et al., 2014).

3.2 POMDP setup

Each non-terminal node in the tree is modelled as
an independent POMDP where the state set S is
the set of possible goals of the node and the ac-
tion setA is the set of actions associated with each
goal plus an “ask” action, which requests the user
to repeat his last command. The reward function
for all the POMDPs is -1 for the “ask” action, and
+10 for each other action if it corresponds to the
user goal, or -10 otherwise, and γ = 0.95. The
state tracker is a logistic regression classifier (Pe-
dregosa et al., 2011), where classes are the set of
states S. The belief state b is computed as the pos-
terior over the states given the last 5 observations
(N-best lists with normalised confidence scores).
For each speaker, the state tracker has been trained
with data from the other 14 speakers.

3In (Casanueva et al., 2014) it was shown that, with a
36 command setup, statistical DM is most useful for low
and medium intelligibility speakers. For high intelligibility
speakers, the ASR accuracy is close to 100% so the improve-
ment obtained from DM is small, and for very low intelligi-
bility speakers, the absolute performance is not high enough
to make the system useful.

16



Speaker Range of Number of Speaker independent Adapted ASR
intelligibility int. measures speakers ASR accuracy range accuracy range

Very low 2% - 15% 4 12.04% - 46.80% 23.06% - 74.37%
Low 28% - 43% 3 27.04% - 55.99% 80.52% - 95.28%

Medium 58% - 62% 3 55.34% - 68.34% 85.93% - 89.61%
High 86% - 95% 5 68.14% - 97.76% 95.38% - 100.00%

Table 1: Stats for the UASpeech database

3.3 Policy models
The DTC approach (eq. 8) is used to compute the
Q-function for the policy (eq. 9) with Gaussian
noise variance σ2 = 5. The kernel over the belief
space is a radial basis function kernel (RBF):

kb(bi,bj) = σ2k exp
(
− ||bi − bj ||

2

2l2k

)
(13)

with variance σ2k = 25 and lengthscale l
2
k = 0.5.

The delta kernel is used over the action space:

ka(ai, aj) = δ(ai, aj) =
{

1 if ai = aj
0 otherwise

(14)

and the kernels over the speaker space are defined
in section 3.4. The size of the inducing set Um is
500 and the maximum size of the TD points set Zt
is 2000. Whenever a new data point is observed
from the target speaker, it is added to the set of in-
ducing points Um, and the first point of the set Um
(which, due to the ordering done by data selection,
corresponds to the least similar source point or to
the oldest target point) is discarded from the in-
ducing set. Whenever a new data point is observed
and the size of the set of temporal difference points
|Zt| = 2000, the first point of this set is discarded.
Three variations of the DTC approach are used:

• DTC: Equation 8 is used to compute the Q
posterior for the policy (eq. 9) and the set
of temporal difference points Zt is initialised
with the source points.

• Prior: Equation 11 is used to compute the Q
posterior for the policy (eq. 9) and the prior
GP is trained with the source points.

• Hybrid: Equation 11 is used to compute the
Q posterior for the policy (eq. 9), the prior
GP is trained with half of the source points
and the set of temporal difference points Zt
is initialised with the other half.

3.4 Speaker similarities
To compute the similarities between speakers a
vector of speaker features s must be extracted.
Different kinds of features may be extracted, such

as meta-data based features, acoustic features, fea-
tures related to the ASR performance, etc. In this
paper, we explore 3 different methods to extract s;

• Intelligibility assessment: The intelligibility
assessment for each speaker in the UASpeech
database (table 1) can be used as a single di-
mensional feature.

• I-vectors: Martı́nez et al. (2013) showed that
i-vectors (Dehak et al., 2011) can be used
to predict the intelligibility of a dysarthric
speaker. For each speaker, s is defined as a
400 dimensional vector corresponding to the
mean i-vector extracted from each utterance
from that speaker. For more information on
the i-vector extraction and characteristics, re-
fer to (Martı́nez et al., 2014).

• ASR accuracy: The performance statistics
of the ASR (e.g. accuracy) can be used as
speaker features. In this paper we use the ac-
curacy per word (command), defining s as a
36 dimensional vector where each element is
the ASR accuracy for each of the 36 com-
mands.

The kernel over the speaker space ks (eq. 12),
is defined as an RBF kernel (eq. 13). This ker-
nel is used both to compute the similarity between
speakers in order to select data (section 2.1.2),
and to weight the data from each source speaker
(section 2.1.3). ks has variance σ2k = 1 and the
lengthscale l2k varies depending on the features.
For intelligibility features l2k = 0.5, for i-vectors
l2k = 8.0 and for ASR accuracy features l

2
k = 4.0

4 Results
In the following experiments the reward is com-
puted as -1 for each dialogue turn, +20 if the dia-
logue was successful4. The system has been tested

4Because of the variable depth tree structure of the spoken
dialogue system, the sum or average of cumulative rewards
obtained in each sub-dialogue is not a good measure of the
overall system performance. If the dialogue gets stuck in a
loop going back and forth between two sub-dialogues, the
extra amount of turns spent in this loop would not be reflected
in the average of rewards

17



0 100 200 300 400
training dialogues

60

65

70

75

80

85 Success rate (%)

0 100 200 300 400
training dialogues

7

8

9

10

11

12

13 Reward

DTC-Conv
DTC-1000
DTC-2000
pri-1000
pri-2000
hyb-1000
hyb-2000

Figure 1: different policy models compared

with the 24 speaker-ASR pairs explained in sec-
tion 3.1, and in the following figures, each plotted
line is the average results for these 24 speaker-
ASR pairs. As the behaviour of the simulated
user and some data selection methods partially de-
pend on random variables, each experiment has
been initialised with four different seeds and all
the results presented are the average of the four
seeds tested over 500 dialogues. In all the experi-
ments the data to initialise each POMDP is trans-
ferred from a pool of 4200 points corresponding
to 300 points from each speaker in table 1 except
the speaker being tested, where each data pool is
different for each seed.

Figure 1 compares the different policy models
presented in section 3.3 using the intelligibility
measure based similarity to select and weight the
data. The dotted line named DTC-conv shows the
performance of the DTC policy when trained un-
til convergence with the target speaker by simulat-
ing 1200 sub-dialogues in each node. DTC-1000
and DTC-2000 show the performance of the basic
DTC approach when 1000 and 2000 source points
are transferred respectively. It can be observed
that, transferring more points boosts the perfor-
mance, but at the cost of increasing the complex-
ity. pri-1000 and pri-2000 show the performance
of the prior policy with 1000 and 2000 transfer
points respectively. The success rate is above the
DTC policy but the learning rate for the reward is
slower. This might be because the small amount
of target data points make the predictions of the
Q-function given by the GP unreliable. Hyb-1000
and hyb-2000 show the performance of the hybrid
model, showing the best behaviour on success rate
after 100 dialogues, and for hyb-2000 even outper-
forming DTC-2000 in reward after 400 dialogues.

In figure 2 the different approaches to com-
pute the speaker similarities for data selection

and weighting presented in section 3.4 are com-
pared, using the DTC model with 1000 transfer
points (named DTC-1000 in the previous figure).
DTC-int uses the intelligibility measure based fea-
tures, DTC-iv the i-vector features and DTC-acc
the ASR accuracy based features. DTC-iv outper-
forms the other two features, followed closely by
DTC-acc. The performance of DTC-int is way
below the other two metrics, suggesting that the
information given by intelligibility assessments is
a weak feature for source speaker selection (as it
is done by humans, it might be very noisy). As
DTC-acc uses information about the ASR statis-
tics (which is the input for the dialogue manager),
it might be expected that it will outperform the
rest, but in this case a purely acoustic based mea-
sure such as the DTC-iv works better. The reason
for this might be that these features are not corre-
lated to the ASR performance, so hidden variables
are used to better organise the data. To investi-
gate the usefulness of similarity based data selec-
tion, two different data selection methods which
do not weight the transferred data have been tried.
DTC-randspk selects the ordering of the speak-
ers from whom the data is transferred at random,
and has a much worse performance than the sim-
ilarity based method, but DTC-allspk selects the
1000 source points from all the speakers, select-
ing 1000 points at random from the pool of 4200
points and, as it can be seen, the reward obtained
by this method is slightly better than with DTC-iv,
even if the success rate is lower. This suggests that
transferring points from more speakers rather than
from just the closest ones is a better strategy, prob-
ably because points selected by this method are
distributed more uniformly over the belief-action
space. A method which does a trade-off between
filling the belief-action space while selecting the
most similar points could be a better option.

To further investigate the effect of selection and
weighting of the data, figure 3 plots the results
for the DTC policy model using the i-vector based
similarity to weight the data but different data se-
lection methods. iv-clo selects the closest speakers
with respect to the i-vector metric, iv-randspk or-
ders the speakers at random, and iv-allspk selects
the 1000 transfer points from all the speakers but
the tested one. As in the previous figure, selecting
speakers by similarity works better than selecting
speakers at random, but selecting the points from
all the speakers and weighting them with the i-
vector metric outperforms all the previous meth-

18



0 100 200 300 400
training dialogues

66

68

70

72

74

76

78

80

82

84 Success rate (%)

0 100 200 300 400
training dialogues

8.5

9.0

9.5

10.0

10.5

11.0

11.5

12.0

12.5 Reward

DTC-Conv
DTC-int
DTC-iv
DTC-acc
DTC-allspk
DTC-randspk

Figure 2: different similarity metrics for data se-
lection and weighting compared

ods. This might be because weighting the data
does a kind of data selection, as the data points
from source speakers closer to the target will have
more influence than the further ones, while trans-
ferring points from all the speakers covers a big-
ger part of the belief-action space. acc-allspk and
allspk-uw show the results of weighting the data
with the ASR accuracy metric and not weighting
the data respectively, when selecting the data from
all speakers. The accuracy metric performs worse
than the i-vector metric once again, but it still out-
performs not weighting the data, suggesting that
data weighting works for different metrics. Finally
iv-allspk-hyb plots the performance of the hybrid
model when selecting the data from all the speak-
ers and weighting it with the i-vector based simi-
larity. Even if it is computationally cheaper, it out-
performs iv-allspk after 100 dialogues, suggesting
that with a good similarity metric and data selec-
tion method, the hybrid model in section 3.3 is the
best option to take.

5 Conclusions
When transferring knowledge between speakers in
a GP-RL based policy, weighting the data by us-
ing a similarity metric between speakers, and to
a lesser extent, selecting the data using this sim-
ilarity, improves the performance of the dialogue
manager. By defining a kernel between temporal
difference points and interpreting the Q-function
as a GP regression problem where data points are
in the TD space, sparse methods that allow the se-
lection of the subset of inducing points such as
DTC can be applied. In a transfer learning sce-
nario, DTC permits a larger number of data points
to be transferred and the selection of points col-
lected from the target speaker as inducing points.

We showed that using part of the transferred
data to train a prior GP for the mean function,

0 100 200 300 400
training dialogues

68

70

72

74

76

78

80

82

84 Success rate (%)

0 100 200 300 400
training dialogues

8.5

9.0

9.5

10.0

10.5

11.0

11.5

12.0

12.5 Reward

DTC-Conv
iv-clo
iv-allspk
acc-allspk
allspk-uw
iv-allspk-hyb
iv-randspk

Figure 3: different transfer data selection methods
compared

and the rest to initialize the set of points of the
GP, improves the performance of each of these ap-
proaches. Transferring data points from a larger
number of speakers outperformed selecting the
data points only from the more similar ones, prob-
ably because the belief-action space is covered
better. This suggests that more complex data se-
lection algorithms that trade-off between selecting
the data points by similarity and covering more
uniformly the belief-action space should be used.
Also, increasing the amount of data transferred in-
creased the performance, but the complexity in-
crease of GP-RL limits the amount of data that
can be transferred. More computationally efficient
ways to transfer the data could be studied.

Of the three metrics based on speaker features
tested (speaker intelligibility, i-vectors and ASR
accuracy), i-vectors outperformed the rest. This
suggest that i-vectors are a potentially good fea-
ture for speaker specific dialogue management and
could be used in other tasks such as state tracking.
ASR accuracy based metrics also outperformed
the intelligibility based one, and as ASR accuracy
and i-vector are uncorrelated features, a combina-
tion of them could give further improvement.

Finally, as the models were tested with simu-
lated users in a hierarchically structured dialogue
system (following the structure of the homeSer-
vice application), future work directions include
evaluating the policy models in a mixed initiative
dialogue system and testing them with real users.
Acknowledgements

The research leading to these results was sup-
ported by the University of Sheffield studentship
network PIPIN and EPSRC Programme Grant
EP/I031022/1 (Natural Speech Technology). The
authors would like to thank David Martı́nez for
providing the i-vectors used in this paper.

19



References
I. Casanueva, H. Christensen, T. Hain, and P. Green.

2014. Adaptive speech recognition and dialogue
management for users with speech disorders. Pro-
ceedings of Interspeech.

H. Christensen, S. Cunningham, C. Fox, P. Green, and
T. Hain. 2012. A comparative study of adaptive, au-
tomatic recognition of disordered speech. Proceed-
ings of Interspeech.

H. Christensen, I. Casanueva , S. Cunningham, P.
Green, and T. Hain. 2013. homeService: Voice-
enabled assistive technology in the home using
cloud-based automatic speech recognition. Pro-
ceedings of SLPAT.

H. Christensen, I. Casanueva , S. Cunningham, P.
Green, and T. Hain. 2014. Automatic selection of
speakers for improved acoustic modelling: recogni-
tion of disordered speech with sparse data. Proceed-
ings of SLT.

N. Dehak, P. Kenny, R. Dehak, P. Dumouchel, P. Ouel-
let. 2011. Front-end factor analysis for speaker ver-
ification. IEEE Transactions on Audio, Speech, and
Language Processing.

Y. Engel, S. Mannor, R. Meir. 2003. Bayes Meets Bell-
man: The Gaussian Process Approach to Temporal
Difference Learning. Proceedings of ICML.

Y. Engel, S. Mannor, R. Meir. 2005. Reinforcement
learning with Gaussian processes. Proceedings of
ICML.

M. Gašić, C. Breslin, M. Henderson, D. Kim, M.
Szummer, B. Thomson, P. Tsiakoulis and S. Young.
2013. On-line policy optimisation of Bayesian spo-
ken dialogue systems via human interaction. Pro-
ceedings of ICASSP.

M. Gašić, C. Breslin, M. Henderson, D. Kim, M.
Szummer, B. Thomson, P. Tsiakoulis and S. Young.
2013. POMDP-based dialogue manager adaptation
to extended domains. Proceedings of SIGDIAL.

M. Gašić and S. Young. 2014. Gaussian Processes
for POMDP-based dialogue manager opimisation.
IEEE Transactions on Audio, Speech and Language
Processing.

M. Geist and O. Pietquin. 2011. Managing uncertainty
within the KTD framework. Proceedings of JMLR.

F. Jurčı́ček, B. Thomson, and S. Young. 2012. Rein-
forcement learning for parameter estimation in sta-
tistical spoken dialogue systems. Computer Speech
and Language.

H. Kim, M. Hasegawa-Johnson, A. Perlman, J. Gun-
derson, T. Huang, K. Watkin, and S. Frame. 2008.
Dysarthric speech database for universal access re-
search. Proceedings of Interspeech.

D. Martı́nez, P. Green and H. Christensen. 2013.
Dysarthria Intelligibility Assessment in a Factor
Analysis Total Variability Space. Proceedings of In-
terspeech.

D. Martı́nez, E. Lleida, P. Green, H. Christensen, A.
Ortega and A. Miguel. 2015. Intelligibility Assess-
ment and Speech Recognizer Word Accuracy Rate
Prediction for Dysarthric Speakers in a Factor Anal-
ysis Subspace. ACM Transactions on Accessible
Computing (TACCESS), Volume 6 Number 3. (Ac-
cepted)

S. Pan and Q. Yang. 2010. A Survey on Transfer
Learning. IEEE Transactions on Knowledge and
Data Engineering.

F. Pedregosa et al. 2011. Scikit-learn: Machine Learn-
ing in Python. Journal of Machine Learning Re-
search.

J. Quiñonero and C. Rasmussen. 2005. A Unifying
View of Sparse Approximate Gaussian Process Re-
gression. Journal of Machine Learning Research.

C. Rasmussen and C. Williams. 2005. Gaussian Pro-
cesses for Machine Learning,. MIT Press.

R. Sutton and G. Barto. 1998. Introduction to Rein-
forcement Learning. MIT Press.

M. Taylor, and P. Stone. 2009. Transfer learning
for reinforcement learning domains: A survey. The
Journal of Machine Learning Research.

B. Thomson, and S. Young. 2010. Bayesian update
of dialogue state: A POMDP framework for spoken
dialogue systems. Computer Speech and Language.

J. Williams and S. Young. 2007. Partially observ-
able Markov decision processes for spoken dialog
systems. Computer Speech and Language.

J. Williams. 2014. Web-style Ranking and SLU Com-
bination for Dialog State Tracking. Proceedings of
SIGDIAL.

S. Young, M. Gašić, B. Thomson and J. D. Williams.
2013. POMDP-Based Statistical Spoken Dialog
Systems: A Review. Proceedings of the IEEE.

20



Appendix A. Temporal difference kernel
In equation 5, a linear transformation from

the belief-action space to the temporal difference
space is applied to the to the covariance vector
K∗,X and to the covariance matrix KX,X by mul-
tiplying them by the matrix Ht. Deriving the
term HtKX,XH>t we obtain the matrix in eq. 15
(page bottom), where ki,j is the kernel function be-
tween two belief-action points xi = (bi, ai) and
xj = (bj , aj), defined in eq. 4. The transformed
matrix (eq. 15) has the form of a covariance ma-
trix where each element is a sum of kernel func-
tions ki,j between belief-action points on time i or
i + 1 weighted by the discount factors. So each
element of this matrix can be defined as a function
of 2 temporal differences between belief-action
points (TD points), zi = (bi, ai,bi+1, ai+1) and
zj = (bj , aj ,bj+1, aj+1) in the form of (eq. 6):

ktdi,j = (ki,j + γiγjki+1,j+1− γiki+1,j − γjki,j+1)
(16)

where γi and γj will be 0 if ai and aj are terminal
actions respectively. Deriving the term K∗,XH>t
(and HtKX,∗) we obtain:

K∗,XH>t =[
(k1,∗

−γ1k2,∗)
(k2,∗

−γ2k3,∗) . . .
(kt−1,∗

−γt−1kt,∗)
] (17)

which is a vector with ktdi,∗ = (ki,∗ − γiki+1,∗)
for each term. This is equivalent to equation 16 if
the action of the new point a∗ is considered a ter-
minal action, thus γ∗ = 0. Then, redefining the
set of belief-action points Xt as the set of belief-
action temporal difference points denoted as Zt,
and defining Ktd as the covariance matrix com-
puted with the kernel function between two tem-
poral difference points (eq. 6), eq. 7 can be de-
rived from eq. 5 by doing the following substitu-
tions: K∗,XH>t = Ktd∗,Z , HtKX,∗ = K

td
Z,∗ and

HtKX,XH>t = KtdZ,Z .

Appendix B. Example homeService dialogues
For a more detailed description of the hierarchi-

cal structure of the homeService environment, this
appendix presents two example dialogues between
an user and the system. The second column rep-
resents the actions taken either by the user (com-
mands) or by the system (actions)

Dialogue 1: Goal = {TV, Channel, One}
Dialogue starts in node “Devices”
Sub-dialogue “Devices”

User TV ( Speaks the command “TV”)
System Ask (Requests to repeat last command)
User TV (Repeats his last command)
System TV (Dialogue transitions to node “TV”)

Sub-dialogue “TV”
User Chan. (Command “Channel”)
System Chan. (Transitions to node “Channel”)

Sub-dialogue “Channel”
User One (Command “One”)
System One (Performs action TV-Channel-One)

As an action has been taken in a terminal node,
the dialogue ends.

Dialogue 2: Goal = {Hi-fi, On}
Dialogue starts in node “Devices”
Sub-dialogue “Devices”

User Hi-fi (Command “Hi-fi”)
System Light (transitions to node Light)

Sub-dialogue “Light”
User Back (Requests to go to previous node)
System Back (transitions to node Devices)

Sub-dialogue “Devices”
User Hi-fi (Command “Hi-fi”)
System Hi-fi (transitions to node Hi-fi)

Sub-dialogue “Hi-fi”
User On (Command “On”)
System Off (Performs action Hifi-Off)

As the action taken in the terminal node does not
match the goal, it is a failed dialogue.

21


