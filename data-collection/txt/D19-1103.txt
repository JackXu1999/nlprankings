











































Target Language-Aware Constrained Inference for Cross-lingual Dependency Parsing


Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing
and the 9th International Joint Conference on Natural Language Processing, pages 1117–1128,
Hong Kong, China, November 3–7, 2019. c©2019 Association for Computational Linguistics

1117

Target Language-Aware Constrained Inference for Cross-lingual
Dependency Parsing

Tao Meng
University of California, Los Angeles

tmeng@cs.ucla.edu

Nanyun Peng
University of Southern California

npeng@isi.edu

Kai-Wei Chang
University of California, Los Angeles

kw@kwchang.net

Abstract
Prior work on cross-lingual dependency pars-
ing often focuses on capturing the common-
alities between source and target languages
and overlooks the potential of leveraging lin-
guistic properties of the languages to facil-
itate the transfer. In this paper, we show
that weak supervisions of linguistic knowl-
edge for the target languages can improve a
cross-lingual graph-based dependency parser
substantially. Specifically, we explore several
types of corpus linguistic statistics and com-
pile them into corpus-wise constraints to guide
the inference process during the test time. We
adapt two techniques, Lagrangian relaxation
and posterior regularization, to conduct infer-
ence with corpus-statistics constraints. Ex-
periments show that the Lagrangian relaxation
and posterior regularization inference improve
the performances on 15 and 17 out of 19 target
languages, respectively. The improvements
are especially significant for target languages
that have different word order features from
the source language.

1 Introduction

Natural language processing (NLP) techniques
have achieved remarkable performance in a vari-
ety of tasks when sufficient training data is avail-
able. However, obtaining high-quality annota-
tions for low-resource language tasks is challeng-
ing, and this poses great challenges to process
low-resource languages. To bridge the gap, cross-
lingual transfer has been proposed to transfer mod-
els trained on high-resource languages (e.g., En-
glish) to low-resource languages (e.g., Tamil) to
combat the resource scarcity problem. Recent
studies have demonstrated successes of transfer-
ring models across languages without retraining
for NLP tasks, such as named entity recognition
(Xie et al., 2018), dependency parsing (Tiede-
mann, 2015; Agić et al., 2014), and question

answering (Joty et al., 2017), using a shared
multi-lingual word embedding space (Smith et al.,
2017) or delexicalization approaches (Zeman and
Resnik, 2008; McDonald et al., 2013).

One key challenge for cross-lingual transfer is
the differences among languages; for example,
languages may have different word orders. When
transferring a model learned from a source lan-
guage to target languages, the performance may
drop significantly due to the differences. To tackle
this problem, various approaches have been pro-
posed to better capture the commonalities between
the source and the target languages (McDonald
et al., 2011; Guo et al., 2016; Täckström et al.,
2013; Agić, 2017; Ahmad et al., 2019); however,
they overlook the potential to leverage linguistic
knowledge about the target language to account
for the differences between the source and the tar-
get languages to facilitate the transfer.

In this paper, we propose a complementary ap-
proach that studies how to leverage the linguis-
tic knowledge about the target languages to help
the transfer. Specifically, we use corpus linguistic
statistics of the target languages as weak supervi-
sion signals to guide the test-time inference pro-
cess when parsing with a graph-based parser. This
approach is effective as the model only need to be
trained once on the source language and applied to
many target languages using different constraints
without retraining the model.

We argue that certain corpus linguistic statistics
such as the word order (e.g., how often an adjec-
tive appears before or after a noun) can be easily
obtained from available resources such as World
Atlas of Language Structures (WALS) (Dryer and
Haspelmath, 2013). To incorporate the corpus lin-
guistic statistics to a cross-lingual parser, we com-
pile them into corpus-wise constraints and adopt
two families of methods: 1) Lagrangian relax-
ation (LR) and 2) posterior regularization (PR) to



1118

solve the constrained inference problem. The al-
gorithms take the original graph-based parsing in-
ference as a sub-routine, and LR iteratively adjusts
the pair-wise potentials until the constraints are
(loosely) satisfied, while PR finds a feasible dis-
tribution and do inference based on that. The con-
strained inference framework is general and sup-
ports any knowledge that can be formulated as a
first-order logic (Roth and Yih, 2004).

We evaluate the proposed approach under the
single-source transfer setting using English as
the source language and test on 19 target lan-
guages covering a broad range of language fam-
ilies with low-resource languages such as Tamil
and Welsh. We demonstrate that by adding
three simple corpus-wise constraints derived from
WALS features, the performances improve in 15
and 17 out of 19 languages when using Lagrangian
relaxation and posterior regularization techniques,
respectively. The improvements are especially
substantial when the target language features are
distant from the source language. For example,
our framework improves the UAS score of Urdu
by 15.7%, and Tamil by 7.3%.1

2 Constrained Cross-Lingual Parsing
Our work focuses on the graph-based dependency
parser (McDonald et al., 2005) in the zero-shot
single-source transfer setting as in Ahmad et al.
(2019). However, the proposed algorithms can be
extended other transfer settings. Given a trained
model, we derive corpus-statistics constraints and
apply them to correct errors caused by word order
differences between the source and the target lan-
guage during the inference time. Figure 1 shows
an example of how constraints can influence the
inference results.

In this section, we first give a quick review
of the graph-based parser and introduce the nota-
tions. We then discuss how to formulate corpus-
wise constraints based on corpus linguistic statis-
tics for guiding the graph-based parser.

2.1 Background: Graph-Based Parser
A graph-based parser learns a scoring function for
every pair of words in a sentence and conducts in-
ference to derive a directed spanning tree with the
highest accumulated score. Formally, given the k-
th sentence wk = (wk1, . . . , wkL(k)) where L(k)

1The code and data are available at
https://github.com/MtSomeThree/
CrossLingualDependencyParsing.

denotes the length of the k-th sentence, a graph-
based parser learns a score matrix S(k), where
S(k)
ij

denotes the score to form an arc from word
wki to word wkj . Let yk be an indicator function
that yk(i, j) 2 {0, 1} denotes the arc from wki to
wkj . The maximum directed spanning tree infer-
ence can be formulated as an integer linear pro-
gramming (ILP) problem:

y⇤k = arg max
yk2Yk

X

i,j

S(k)
ij

yk(i, j), (1)

where Yk is the set of legal dependency trees of
sentence k. In recent years, neural network ap-
proaches (Kiperwasser and Goldberg, 2016; Wang
and Chang, 2016; Kuncoro et al., 2016; Dozat and
Manning, 2017) have been applied to modeling the
scoring matrix S(k) and have achieved great per-
formance in dependency parsing.

From the probabilistic point of view, if we
assume for different i, j, the edge probabilities
P (yk(i, j) = 1|wk) are mutually conditional in-
dependent, the probability of a whole parse tree
can be written as

P (yk|wk) =
Y

i,j

P (yk(i, j) = 1|wk)yk(i,j). (2)

If we set S(k)
ij

= logP (yk(i, j) = 1|wk) + Z 0j ,
where Z 0

j
is a constant term, then Eq. (1) can be

regarded as the following maximum a posteriori
(MAP) inference problem:

y⇤k = arg max
yk2Yk

P (yk|wk)

= arg max
yk2Yk

X

i,j

logP (yk(i, j) = 1|wk)yk(i, j).

(3)

2.2 Corpus-Wise Constraints
Given the inference problems as in equations (1)
and (3), additional constraints can be imposed to
incorporate expert knowledge about the languages
to help yield a better parser. Instance-level con-
straints have been explored in the literature of de-
pendency parsing, both in the monolingual (Dryer,
2007) and cross-lingual transfer (Täckström et al.,
2013) settings. However, most word order features
for a language are non-deterministic and cannot be
compiled into instance-wise constraints.

In this work, we introduce corpus-wise con-
straints to leverage the non-deterministic features
for cross-lingual parser. We compile the follow-
ing two types of corpus-wise constraints based on
corpus linguistics statistics:

https://github.com/MtSomeThree/CrossLingualDependencyParsing.
https://github.com/MtSomeThree/CrossLingualDependencyParsing.


1119

• Unary constraints consider statistics regard-
ing a particular POS tag (POS).

• Binary constraints consider statistics regard-
ing a pair of POS tags (POS1, POS2).

Specifically, a unary constraint specifies the ratio
r of the heads of a particular POS appears on the
left of that POS.2 Similarly, a binary constraint
specifies the ratio r of POS1 being on the left of
POS2 when there is an arc between POS1 and
POS2.

The ratios r for the constraints are called cor-
pus statistics, which can be estimated in one of the
following ways: a) leveraging existing linguistics
resources or consulting linguists; b) leveraging a
higher-resource language that is similar to the tar-
get language (e.g., Finnish and Estonian) to collect
the statistics. In this paper, we explore the first op-
tion and leverage the WALS features, which pro-
vide a reference for word order typology, to esti-
mate the ratios.

Compile Constraints From WALS Features.
For a particular language, once we collect the
corpus-statistics of a pair of POS tags, we can
formulate a binary constraint. There are different
ways to estimate the corpus-statistics. For exam-
ple, Östling (2015) utilizes a small amount of par-
allel data to estimate the dominant word orders.
In this paper, we simply utilize a small subset of
WALS features that show the dominant order of
some POS pairs (e.g. adjective and noun) in a lan-
guage. They can be directly compiled into binary
constraints.

Similarly, we can estimate the ratio for unary
constraints based on WALS features. For a partic-
ular POS tag, we choose all WALS features related
to it to formulate a feature vector f . The mapping
from the vector f to the unary constraint ratio r is
learnable: for each language with annotated data,
we can get a WALS feature vector flang and a ratio
rlang from the annotation. We only need a small
amount of data to estimate rlang well. Given a set
of languages with feature vectors and estimated ra-
tios, we can learn the mapping by a simple linear
regression, and apply it to estimate the ratio of any
target language to compile a unary constraint.

2.3 Formulate Constraints
In the following, we mathematically formulate
the corpus-wise constraints. Note that these con-

2The ratio for the head being on the right of that POS is
thereby 1� r.

NOUN   ADP   NOUN          PROPN   PROPN VERB    AUX  PUNCT
भोज  का  खर्च भारतीय सेना उठाती है ।

Enjoyment   of   expenditure       Indian      army       bears       is          .

Constraint: In an ADP-NOUN arc in Hindi, ADP 
is more likely to be on the right. 

NOUN   ADP   NOUN          PROPN   PROPN VERB    AUX  PUNCT
भोज  का  खर्च भारतीय सेना उठाती है ।

Enjoyment   of   expenditure       Indian      army       bears       is          .

Figure 1: An running example of Hindi. On the top
there is the inference result of a baseline model trained
on English. In English, ADP is mostly on the left of the
NOUN, so the potential of the correct ADP-NOUN arc
is lower. With the help of corpus-statistics constraints,
the potential is adjusted and the model gets correct in-
ference result as shown in the bottom. The dashed lines
highlight the difference.

straints are based on the statistics over the entire
corpus. For a unary constraint C(POS)u , let P de-
notes a set of word with part-of-speech tag POS.
We define C+u := {(k, i, j)|wkj 2 P ^ i < j} as
the set of arcs where the head of word in P is on
its left and C�u := {(k, i, j)|wkj 2 P ^ i > j},
conversely.

For a binary constraint C(POS1,POS2)
b

, we de-
notes P1, P2 as a set of word with part-of-speech
tag POS1, POS2, respectively. We then define
C+
b

as the set of arcs with two ends wki 2 P1 and
wkj 2 P2, and wki is on the left of wkj . We define
C�
b

Similarly. Formally,

C+
b
:={(k, i, j)|wki 2 P1 ^ wkj 2 P2 ^ i < j}
[ {(k, j, i)|wki 2 P1 ^ wkj 2 P2 ^ i < j},

C�
b
:={(k, i, j)|wki 2 P1 ^ wkj 2 P2 ^ i > j}
[ {(k, j, i)|wki 2 P1 ^ wkj 2 P2 ^ i > j}.

For notational simplicity, we use C to denote
all constraints including both the unary and binary
ones. The ratio function R(C, Y ) for a constraint
C given the parse trees Y can be defined as:

R(C, Y )=

P
k

P
(i,j):(k,i,j)2C+ yk(i, j)P

k

P
(i,j):(k,i,j)2C+[C� yk(i, j)

.

We want to enforce the ratio R(C, Y ) estimated
from Y to be consistent with a value r (see Sec.
2.2), which formulates a constraint

r � ✓  R(C, Y )  r + ✓

where ✓ is a tolerance margin. Note that the
instance-level hard constraint is a special case of



1120

Sentence !"

Score Matrix 
#$%" /

Prob. Dist. 
&' ("(*, ,) !"

Feasible Dist. 
."∗(*, ,)

Dependency 
Tree 0"

1.. (7)

MAP Inference

Graph-Based Parser

Constrained
Inference

Original
Inference

Figure 2: The pipelines of the baseline method (left),
Lagrangian relaxation (right) and posterior regulariza-
tion (middle). Lagrangian relaxation converts con-
strained inference to an unconstrained optimization
problem using Lagrange’s method. Posterior regular-
ization method is working on the distribution space.
For a given distribution, PR finds the closest feasible
distribution and conduct MAP inference.
the corpus-statistics constraint when r = 0 or
r = 1.

Given a set of corpus-statistics constraints
C = {C1, C2, . . . , Cn} with corresponding corpus
statistics r = {r1, r2, . . . , rn}, the objective of the
constrained inference is:

max
Y 2Y

X
k

X
i,jL(k)

S(k)
ij

yk(i, j),

s.t. ri�✓i  R(Ci, Y )ri+✓i, i 2 [N ],
(4)

where Y denotes the set of all possible dependency
trees. As all the constraints can be written as a
linear inequality with respect to yk(i, j). Eq. (4)
is an ILP.

3 Inference with Corpus-Statistics
Constraints

The ILP problem in Eq. (4) is in general an NP-
hard problem; especially, it involves variables as-
sociated with the entire corpus. Without con-
straints, Eq. (4) can be decoupled into K sub-
problems, and the inference with respect to each
sentence can be solved independently as in Eq.
(1). In this way, an efficient inference algorithm
such as maximum directed spanning tree algo-
rithm (Chu and Liu, 1965) can be used.

However, with the corpus-wise constraints, di-
rectly solving Eq. (4) is infeasible. Therefore, we
explore two algorithms for inference with corpus-
statistics constraints: Lagrangian relaxation and

posterior regularization (Ganchev et al., 2010).
The Lagrangian relaxation algorithm introduces
Lagrangian multipliers to relax the constraint op-
timization problem to an unconstrained optimiza-
tion problem, and estimates the Lagrangian mul-
tipliers with gradient-based methods. The poste-
rior regularization algorithm uses the constraints
of the target language to define a feasible set of
parse tree distributions, and find a feasible distri-
bution that is closest to the parse tree distribution
trained on the source language by minimizing the
KL-divergence. The constrained inference prob-
lem can then be converted into an MAP inference
problem on the best feasible distribution. Figure
2 illustrates the procedure of the original infer-
ence, Lagrangian relaxation, and posterior regu-
larization.

3.1 Lagrangian Relaxation

Lagrangian relaxation has been applied in various
NLP applications (Rush and Collins, 2012, 2011).
In Eq. (4), each constraint Ci involves two in-
equality constraints: R(Ci, Y ) � ri + ✓ � 0,
and ri + ✓ � R(Ci, Y ) � 0. Instead of treat-
ing these two constraints separately, we consider
a heuristic to optimize with equality constraints
R(Ci, Y ) = ri, i 2 [N ] and terminate earlier
when constraints in Eq. (4) are satisfied. De-
spite this approach does not guarantee the solu-
tion is optimal if all the constraints are satisfied as
the original Lagrangian relaxation algorithm does,
in practice, the inference converges faster (as the
number of Lagrangian multipliers is half) and the
parsing performance maintains.

In the following, we derive the constrained
inference algorithm for corpus-statistics con-
straints. First, we rewrite the equality constraint
R(C, Y ) = r by substituting R(C, Y ) with
Eq. (2.3):

(1� r)
P
k

P

(i,j):(k,i,j)2C+i

yk(i, j)

�r
P
k

P

(i,j):(k,i,j)2C�i

yk(i, j) = 0, (5)

We use F(C) to denote the left-hand-side of
Eq. (5), which is linear w.r.t. yk. Then, the La-
grangian relaxation of the constrained inference
problem can be written as:

L(Y,�; C) =
X

k

X

i,j

S(k)
ij

yk(i, j)+
X

i2[N ]

�iF (Ci),



1121

Algorithm 1 Lagrangian Relaxation for Con-
straint Inference
Input: Constraints C = {Ci}Ni=1, corresponding

ratio r = {ri}Ni=1, tolerance margin ✓ = {✓i}Ni=1,
learning rate decay ⌘, initial learning rate ↵0
Output: parse trees Ŷ
1: ↵ ↵0
2: �i  0, i 2 [N ]
3: repeat
4: Ŷ  argmaxY 2Y L(Y,�; C)
5: r̂i  R(Ci, Ŷ ), i 2 [N ]
6: if 8i, abs(ri � r̂i)  ✓ then
7: return Ŷ
8: �i  ↵(r̂i � ri), i 2 [N ]
9: ↵ ⌘↵

10: until MAX ITER times
11: return Ŷ

where �i is called Lagrangian multiplier. It is
well-known that we can solve the dual form of the
constrained inference problem:

max
Y 2Y

min
��0

L(Y,�; C).

To solve the dual form, we initialize �i to be 0.
At iteration t, we firstly conduct an constraint-
augmented inference with a fixed �(t):

Ŷ (t) = max
Y 2Y

L(Y,�(t); C). (6)

As F (C) is a linear function w.r.t yk, we com-
bine it with S(k)

ij
yk(i, j). In this way, the inference

problem Eq. (6) can be treated as a special case
of Eq. (1) with a different scoring matrix S(k).
In this way, we can treat the inference on every
sentence independently and leverage existing in-
ference techniques.

After solving the constraint-augmented infer-
ence, we compute the ratio of every constraint
r̂i

(t) = R(Ci, Ŷ (t)), and use gradient ascent al-
gorithm to update the Lagrangian multipliers

�(t+1)
i

= �(t)
i

+ ↵(t)(ri � r̂i).

Here ↵(t) denote the step size at iteration t. The
algorithm is shown in Algorithm 1.

3.2 Posterior Regularization
From a probabilistic point of view, the parser
model learns parameters ✓ to realize Eq. (2). Dur-
ing the inference, the model predict a probabil-
ity distribution p✓(Y|W) over possible parse trees

given a sentence. The posterior regularization al-
gorithm first defines a feasible set of the proba-
bility distributions w.r.t the given constraints, and
looks for the closest feasible distribution q⇤(Y) to
the model distribution p✓(Y|W). The best parse
tree is given by argmaxY q⇤(Y). Specifically, we
define the feasible set as:

Q={q(Y)|ri�✓iR(Ci, q(Y))ri+✓i, i2 [N ]},

where R(C, q) =
P

k

P
(i,j):(k,i,j)2C+ qk(i,j)P

k

P
(i,j):(k,i,j)2C+[C� qk(i,j)

,

with qk(i, j) = EY⇠q(Y )[yk(i, j)].
To measure the distance between two distribu-

tion, we use the KL-divergence, and find the best
feasible distribution q⇤(Y):

q⇤(Y) = argmin
q2Q

KL(q(Y)kp✓(Y|W)). (7)

If the feasible set has the expectation form:

{q|EY⇠q[�(Y)]  b}, (8)

Eq. (7) has a simple close form solution (Ganchev
et al., 2010):

q⇤(Y) =
p✓(Y|W) exp(��⇤ · �(Y))

Z(�⇤)
, (9)

where �⇤ is the solution of

�⇤=argmax
��0
�b · �� logZ(�),

Z(�)=
X

Y0

p✓(Y
0|W) exp(��⇤ · �(Y0)). (10)

In the rest of this section, we first show that the
feasible set Q we considered above can be refor-
mulated in the form of Eq. (8), and then we dis-
cuss how to solve Eq. (10). To show that the in-
equality R(C, q(Y))  r, in Q can be formulated
in the form of Eq. (8), we set

�C(Y) =
X

k,i,j

yk(i, j)�C(k, i, j),

and

�C(k, i, j)=

8
><

>:

1� r (k, i, j)2C+

�r (k, i, j)2C�

0 (k, i, j) /2C+[C�.
(11)

Similarly, we can derive R(C, q(Y)) � r, into the
same form and rewrite Q as

Q={q(Y) | EY⇠q�(Y)  0},



1122

Algorithm 2 Posterior Regularization
Input: Constraints C = {Ci}Ni=1, corresponding

ratio r = {ri}Ni=1, tolerance margin ✓ = {✓i}Ni=1
Output: parse trees Ŷ
1: p✓(yk(i, j) = 1) normalize

⇣
exp

⇣
S(k)
ij

⌘⌘

2: � defined by Eq. (11)
3: �i  0, i 2 [N ]
4: repeat
5: estimate g @ logZ(�)/@� in a batch
6: update � based on g
7: until MAX ITER times
8: for each (k, i, j) do
9: qk(i, j) defined by Eq. (12)

10: Ŷ  MAP inference based on q
11: return Ŷ

where � = (�C1 ,�C2 , ...,�CN ) is a collection of
the constraints. The detailed derivations can be
found in Appendix A.

We solve Eq. (10) by sub-gradient descent3.
Noting that there can be exponential number of
terms in Z(�), we firstly need to factorize Z(�)
from corpus level to instance level and arc level,
and compute the gradient. The technical details
are in Appendix A. With the optimal �⇤, we can
compute the feasible distribution q⇤(Y) given p✓
and W. Noting that the solution Eq. (9) can also
be factorized to arc-level:

q⇤k(i, j) / p✓(yk(i, j)|wk) exp(��⇤ · �(k, i, j)),
(12)

here q⇤
k
(i, j) denote the arc-level distribution

q⇤(yk(i, j) = 1) satisfying Eq. (3). We then
do MAP inference based on q, which is actually
a minimal spanning tree problem same as before.
Algorithm 2 summarizes the process.

4 Experiments

In this section, we evaluate the proposed al-
gorithms by transferring an English dependency
parser to 19 target languages covering 13 language
families of real low-resource languages. We first
introduce the experimental setup including data
selection and constraint details and then discuss
the results as well as in-depth analysis.

4.1 Setup
Model and Data We train the best performing
Att-Graph parser proposed in Ahmad et al. (2019)

3In implementation, we use stochastic gradient descent
with Adam optimizer (Kingma and Ba, 2015)

on English and transfer it to 19 target languages
in UD Tree Bank v2.2 (Nivre et al., 2018).4 The
model takes words and predicted POS tags5 as in-
put, and achieve transfer by leveraging pre-trained
multi-lingual FastText (Bojanowski et al., 2017)
embeddings that project the word embeddings
from different languages into the same space us-
ing an offline transformation method (Smith et al.,
2017; Conneau et al., 2018). The SelfAtt-Graph
model uses a Transformer (Vaswani et al., 2017)
with relative position embedding as the encoder
and a deep biaffine scorer (Dozat and Manning,
2017) as the decoder. We follow the setting in
Ahmad et al. (2019) to train and tune only on the
source language (English) and directly transfer to
all the target languages. We modify their decoder
to incorporate constraints with the proposed con-
strained inference algorithms during the transfer
phase without retraining the model. All the hyper-
parameters are specified in Appendix Table 4 to-
gether with hyper-parameters for the inference al-
gorithms in Appendix Table 5.

Constraints We consider two types of con-
straints: 1) instance-level projective constraints
for avoiding creating crossing arcs in the de-
pendency trees, 2) corpus-statistics constraints
constructed by the process described in Sec-
tion 2.2. We consider the following three corpus-
statistics constraints: C1 = (NOUN), C2 =
(NOUN,ADP ), C3 = (NOUN,ADJ); intu-
itively, C1 concerns about the ratio of nouns be-
ing on the right of their heads; C2 concerns about
the ratio of nouns being on the left of adposi-
tions among all noun-adposition arcs; C3 con-
cerns about the ratio of nouns being on the left of
adjectives among all noun-adjective arcs.

For binary constraints, C2 and C3 can be di-
rectly compiled from WALS feature 85A and 87A
respectively. We encode “dominant order” spec-
ified in WALS as the ratio being always greater
than 0.75 (i.e., r = 0.875 and ✓ = 0.125). If there
is no dominant order or the feature is missing, we
set r = 0.5 and ✓ = 0.25. Some WALS features
like 82A, 83A are also about word order, but we
need to specify the arc types to utilize them. For
simplicity, we only consider forming constraints
from the POS tags in this paper. To estimate the

4We make the selection to prioritize the coverage of lan-
guage families and low resource languages. The language
family information can be found in Table 1.

5We use predicted POS tags provided in UD v2.2.



1123

Family Lang. Features Baseline Lagrangian Relaxation Posterior RegularizationOracle WALS �WALS Oracle WALS �WALS
IE.Germanic en 1,1,1 90.5 90.3 90.4 -0.1 90.4 90.6 +0.1
IE.Indic ur -1,-1,1 18.3 35.2 34.0 +15.7 35.0 33.7 +15.4
IE.Indic hi -1,-1,1 34.3 52.4 53.4 +19.1 51.3 49.1 +14.8
Dravidian ta -1,-1,1 36.1 42.8 43.4 +7.3 43.1 43.0 +6.9
Turkic tr -1,-1,1 31.2 35.2 37.1 +5.9 35.1 36.3 +5.1
Afro-Asiatic ar 1, 1, -1 38.5 47.3 45.3 +6.8 45.8 43.7 +5.2
Afro-Asiatic he 1, 1, -1 55.7 58.8 57.6 +1.9 58.3 57.6 +1.9
Austronesian id 1, 1, -1 49.3 53.1 52.3 +3.0 52.3 51.9 +2.6
Korean ko -1,-1,1 34.0 37.1 37.2 +3.2 36.3 36.4 +2.4
IE.Celtic cy 1, 1, -1 47.3 54.2 51.7 +4.4 53.8 50.0 +2.7
IE.Romance ca 1, 1, -1 73.9 74.9 73.8 -0.1 74.9 74.7 +0.8
IE.Romance fr 1, 1, -1 77.8 79.1 78.7 +0.9 79.0 79.0 +1.2
Uralic et 1, -1,1 65.3 65.5 65.8 +0.5 65.7 66.0 +0.7
Uralic fi 1, -1,1 66.7 67.1 67.0 +0.3 66.9 67.1 +0.4
IE.Slavic hr 1, 1, 1 62.2 63.7 63.2 +1.0 63.6 63.4 +1.2
IE.Slavic bg 1, 1, 1 79.6 79.7 79.2 +0.0 79.7 79.7 +0.1
IE.Baltic lv 1, 1, 1 70.3 70.7 69.5 -0.8 70.5 69.9 -0.4
IE.Latin la ?, ?, ? 47.4 48.0 45.6 -1.8 48.1 47.3 -0.1
IE.Germanic da 1, 1, 1 76.6 76.6 76.5 -0.1 76.6 76.6 +0.0
IE.Germanic nl 0, 1, 1 67.5 67.6 67.5 +0.0 67.9 67.9 +0.4
Average Performance 54.3 58.4 57.8 +3.5 58.1 57.5 +3.1

Table 1: Cross-lingual transfer performances for dependency parsing on 19 languages from 13 different fami-
lies, with the performance on the source language (English) as a reference. Performances are reported per UAS
(we observe similar trends for LAS and details can be found in the appendix Table 7). We compare the baseline
model (Ahmad et al., 2019) with our two algorithms (Lagrangian relaxation and posterior regularization) consid-
ering the oracle constraints, and the corpus-statistics constraints compiled from WALS. Columns �WALS denote
the improvements bring by leveraging WALS feature as constraints. We also create a Features column to show
three WALS features [83A,85A,87A] for each language. The values {1, -1, 0, ?} stand for the same as English,
opposite to English, no dominant order, and feature missing, respectively.

ratio for unary constraint C1, we use the WALS
features 82A, 83A, 85A, 86A, 87A, 88A, 89A that
are related to NOUN to form feature vectors, and
do regression on languages in the test set except
the target language to predict the constraint ratio.
The process guarantees the target language remain
unseen during the ratio estimation process. The
ratios on the regression training languages are es-
timated by sampling 100 sentences in the training
set per language.

We also consider an oracle setting where we
collect a “ground-truth” ratio of each constraint for
the target language to estimate an upper bound of
our inference algorithms. In the oracle setting, we
estimate the ratio on the whole training corpus of
the target language and set the margin to ✓ = 0.01.

4.2 Parsing Performances
We first compare the performances of the cross-
lingual dependency parser with or without con-

straints. Table 1 illustrates the results for the 19
target languages we selected,6 along with the per-
formance on the source language (English). The
performance on English is not as high as the de-
pendency parsers specialized for English, because
to achieve transfer, we have to freeze the pre-
trained multi-lingual word embeddings. Yet this
parser achieved the best single-source transfer per-
formances according to Ahmad et al. (2019).

As is shown in Table 1, the improvements by
our constrained inference algorithms are dramatic
in a few languages that have very distinct word
order features from the source language. For ex-
ample, the parsing performance of Hindi (hi) im-
proves about 15% in UAS with WALS features
via both Lagrangian relaxation and posterior reg-
ularization inference. The improvements are less

6We also run on all languages in Ahmad et al. (2019) for
completeness and observe similar trends. The results can be
found in Appendix Table 7.



1124

obvious for languages that are in the same fam-
ily as English such as Danish(da) and Dutch(nl).
This is expected as the corpus linguistic statistics
of these languages are similar to English thus the
constraints are mostly satisfied with the baseline
parser. Comparing Lagrangian relaxation and pos-
terior regularization, we find posterior regulariza-
tion being more robust and less sensitive to the er-
rors in the corpus-statistics estimation, while La-
grangian relaxation gives a higher improvement
on average. Overall, the two proposed constrained
inference algorithms improved the transfer perfor-
mance by 3.5% and 3.1% per UAS on average on
19 target languages.

For languages like Finnish (fi) and Estonian (et),
the WALS setting works even better than the ora-
cle. We suspect the reason being the large margin
we set in the WALS setting. When the estimated
corpus-statistics is different from the real ratio
in the test set, the large margin relaxes the con-
straints, thus could result in better performances.

Discussion. Despite the major experiments and
analysis are conducted using English as the only
source language, our approach is general and does
not have restriction on the choice of the source lan-
guage(s). To verify this claim, we run experiments
with Hebrew as the source language. Under the or-
acle setting, Lagrangian relaxation and posterior
regularization improve the baseline by 4.4% and
4.1%, respectively.

We observed that if we compile WALS features
into hard constraints (i.e., set r = 0 or 1), the con-
straint inference framework only improves perfor-
mance on half of the languages. For example, in
Estonian (et), the performance drops about 3%.
This is because WALS only provides the domi-
nant order. Therefore, treating WALS as hard con-
straints introduces error to the inference.

Finally, we assume if we can access to native
speakers, the corpus-statistics can be estimated by
a few partial annotations of parse trees. In our sim-
ulation, using less than 300 arcs, we can achieve
the same performance as using the oracle.

4.3 Contributions of Individual Constraints
We analyze the contribution of each constraint
demonstrated in Table 2. Here we use the or-
acle setting to reduce the noise introduced by
corpus-statistics estimation errors. The results are
based on Lagrangian relaxation inference. As
shown in Table 2, Despite some languages have

Model UAS coverage �
baseline 54.3 N/A N/A
+Proj. 54.6 N/A +0.3

+Proj.+C1 57.0 0.24 +2.4
+Proj.+C2 55.7 0.08 +1.1
+Proj.+C3 55.0 0.07 +0.4

oracle 58.4 N/A +4.1

Table 2: Ablation study: average UAS of baseline
model with different sets of constraints. Proj. represent
projective constraints. C1-C3 and oracle are introduced
in 4.1. The improvements for projective constraint and
oracle are compared to baseline. For the other three
constraint sets the improvement is compared to model
with projective constraint.

Const. statistics improvement
+Proj. N/A +0.1

C1 0.30/0.36/0.94 +6.9
C2 0.00/0.06/1.00 +11.3
C3 0.14/0.27/0.12 +0.5
All N/A +18.1

Table 3: Contribution of individual constraints and
their statistics in Hindi. The second column lists the ra-
tios estimated from oracle in English/ baseline in Hindi/
oracle in Hindi, respectively. The improvement is mea-
sured in UAS. The improvement of constraints is com-
puted same as Table 2

non-projective dependencies, we observed perfor-
mance improvements on almost all the languages
when the projective constraint is enforced. All the
constraints we formulated have positive contribu-
tions to the performance improvements. C1 =
(NOUN) brings the largest gain probably be-
cause its widest coverage.

Table 1 shows that the performance of Hindi im-
proves from 34% to over 51% per UAS for both
inference algorithms. To better understand where
the improvements come from, we conduct an anal-
ysis to breakdown the contribution of each indi-
vidual constraint for Hindi. Table 3 shows the re-
sults. We can see that since the corpus linguis-
tic statistics between Hindi and English are dis-
tinct, the baseline model only achieves low perfor-
mance. With the constrained inference, especially
the postposition constraint (C2), the proposed in-
ference algorithm bring significant improvement.

To verify the effectiveness of the constraints,
we analyze the relation between the performance
improvements and corpus statistics ratio gaps be-
tween the source and the target languages. To
quantify the ratio gap, we weight constraints by



1125

their coverage rate and compute the weighted av-
erage of the ratio difference between source and
target languages. Results show that the perfor-
mance improvement is highly related to the ra-
tio gap. The Pearson Correlation Coefficient is
0.938. The figure showing the correlation be-
tween performance gap (as per UAS) and the cor-
pus statistics ratio gap is in the Appendix Figure 3.

5 Related Work

Cross-Lingual Transfer for Parsing Many ap-
proaches have been developed to transfer a de-
pendency parser. However, they mainly focus on
better capture information from the source lan-
guage(s). McDonald et al. (2011); Guo et al.
(2016); Täckström et al. (2013); Chen et al. (2019)
consider transferring a parser trained on multi-
ple source languages. Agić (2017); Lin et al.
(2019) selects good source languages by com-
paring part-of-speech tags sequences. Søgaard
(2011); Täckström et al. (2013) chooses suitable
data points from the source language. Pires et al.
(2019) uses multilingual BERT to leverage lan-
guage features from multiple languages. Ahmad
et al. (2019) design an order-free model to take
out the order features from the source language.
Xiao and Guo (2014); Guo et al. (2015) learn
an alignment from source words to target words.
Ponti et al. (2018) learn an anisomorphism from
the source parsing tree to target. Rasooli and
Collins (2019) reorder the source data before train-
ing. In contrast, we focus on incorporating linguis-
tic properties in the target languages.

Constrained Inference for Parsing Several
previous studies show that adding constraints in
inference time improves the performance of mod-
els. Grave and Elhadad (2015) consider incorpo-
rating constraints to promote popular types of arcs
in an unsupervised setting. Naseem et al. (2010);
Li et al. (2019) train a parser with constraints com-
piled from the frequency of particular arcs. Com-
pared with the previous work, we focus on cross-
lingual transfer with word order constraints.

Finally, prior studies have noticed that the word
order information is significant for parsing and
use it as features (Ammar et al., 2016; Naseem
et al., 2012; Rasooli and Collins, 2017; Zhang and
Barzilay, 2015; Dryer, 2007). Täckström et al.
(2013) further propose to decompose these fea-
tures from models for adapting target languages.
Wang and Eisner (2018a) use the statistics of sur-

face part-of-speech (POS) tags of target languages
to learn the word order. Wang and Eisner (2018b)
use POS tags of target languages together with a
similar language, and design a stochastic permu-
tation process to synthetic the word order. How-
ever, none of them consider using the word order
features as constraints.

Incorporating Constraints In NLP Tasks
Constraints are widely incorporated in variety of
NLP tasks. To name a few, Roth and Yih (2004)
propose to formulate constrained inferences in
NLP as integer linear programming problems. To
solve the intractable structure, Rush and Collins
(2012) decompose the structure and incorporate
constraints on some composite tasks. To improve
the performance of a model, Chang and Collins
(2011); Peng et al. (2015) incorporate constraints
on exact decoding tasks and inference tasks on
graphical models, and Chang et al. (2013); Dalvi
(2015); Martins (2015) incorporate corpus-level
constraints on semi-supervised multilabel classi-
fication and coreference resolution. Zhao et al.
(2017) incorporate corpus-level constraints to
avoid amplifying gender bias on visual semantic
role labeling and multilabel classification. In
contrast to previous work, we incorporate corpus-
level constraints to facilitate dependency parser in
the cross-lingual transfer setting.

6 Conclusion

We propose to leverage corpus-linguistic statis-
tics to guide the inference of cross-lingual depen-
dency parsing. We compile these statistics into
corpus-statistic constraints and design two infer-
ence algorithms on top of a graph-based parser
based on Lagrangian relaxation and posterior reg-
ularization. Experiments on 19 languages show
that our approach improves the performance of
the cross-lingual parser substantially. In the fu-
ture, we plan to study the design and incorporation
of fine-grained constraints considering multipule
languages for cross-lingual transfer. We also plan
to adapt this constrained inference framework to
other cross-lingual structured prediction problems,
such as semantic role labeling.

Acknowledgement This work was supported in
part by National Science Foundation Grant IIS-
1760523 and an NIH R01 (LM012592). We thank
anonymous reviewers and members of the UCLA-
NLP lab for their feedback.



1126

References
Željko Agić. 2017. Cross-lingual parser selection

for low-resource languages. In Proceedings of the
NoDaLiDa Workshop on Universal Dependencies,
UDW@NoDaLiDa 2017, Gothenburg, Sweden, May
22, 2017, pages 1–10. Association for Computa-
tional Linguistics.

Željko Agić, Jörg Tiedemann, Kaja Dobrovoljc, Si-
mon Krek, Danijela Merkler, and Sara Može. 2014.
Cross-lingual dependency parsing of related lan-
guages with rich morphosyntactic tagsets. In
EMNLP 2014 Workshop on Language Technology
for Closely Related Languages and Language Vari-
ants.

Wasi Uddin Ahmad, Zhisong Zhang, Xuezhe Ma,
Eduard Hovy, Kai-Wei Chang, and Nanyun Peng.
2019. On difficulties of cross-lingual transfer with
order differences: A case study on dependency pars-
ing. In Proceedings of the 2019 Conference of
the North American Chapter of the Association for
Computational Linguistics.

Waleed Ammar, George Mulcaire, Miguel Ballesteros,
Chris Dyer, and Noah A Smith. 2016. Many lan-
guages, one parser. Transactions of the Association
for Computational Linguistics, 4:431–444.

Piotr Bojanowski, Edouard Grave, Armand Joulin, and
Tomas Mikolov. 2017. Enriching word vectors with
subword information. Transactions of the Associa-
tion for Computational Linguistics, 5:135–146.

Kai-Wei Chang, S. Sundararajan, and S. Sathiya
Keerthi. 2013. Tractable semi-supervised learn-
ing of complex structured prediction models. In
Machine Learning and Knowledge Discovery in
Databases - European Conference, ECML PKDD
2013, Prague, Czech Republic, September 23-27,
2013, Proceedings, Part III, volume 8190 of Lec-
ture Notes in Computer Science, pages 176–191.
Springer.

Yin-Wen Chang and Michael Collins. 2011. Exact de-
coding of phrase-based translation models through
lagrangian relaxation. In Proceedings of the 2011
Conference on Empirical Methods in Natural Lan-
guage Processing, EMNLP 2011, 27-31 July 2011,
John McIntyre Conference Centre, Edinburgh, UK,
A meeting of SIGDAT, a Special Interest Group of
the ACL, pages 26–37. ACL.

Xilun Chen, Ahmed Hassan Awadallah, Hany Has-
san, Wei Wang, and Claire Cardie. 2019. Multi-
source cross-lingual model transfer: Learning what
to share. In Proceedings of the 57th Conference of
the Association for Computational Linguistics, ACL
2019, Florence, Italy, July 28- August 2, 2019, Vol-
ume 1: Long Papers, pages 3098–3112. Association
for Computational Linguistics.

Y. J. Chu and T. H. Liu. 1965. On the shortest arbores-
cence of a directed graph. Science Sinica, 14.

Alexis Conneau, Guillaume Lample, Marc’Aurelio
Ranzato, Ludovic Denoyer, and Hervé Jégou. 2018.
Word translation without parallel data. Internation
Conference on Learning Representations.

Bhavana Bharat Dalvi. 2015. Constrained Semi-
supervised Learning in the Presence of Unantici-
pated Classes. Ph.D. thesis, Google Research.

Timothy Dozat and Christopher D Manning. 2017.
Deep biaffine attention for neural dependency pars-
ing. Internation Conference on Learning Represen-
tations.

Matthew S Dryer. 2007. Word order. Language typol-
ogy and syntactic description, 1:61–131.

Matthew S. Dryer and Martin Haspelmath, editors.
2013. WALS Online. Max Planck Institute for Evo-
lutionary Anthropology, Leipzig.

Kuzman Ganchev, Jennifer Gillenwater, Ben Taskar,
et al. 2010. Posterior regularization for structured
latent variable models. Journal of Machine Learn-
ing Research, 11(Jul):2001–2049.

Edouard Grave and Noémie Elhadad. 2015. A con-
vex and feature-rich discriminative approach to de-
pendency grammar induction. In Proceedings of the
53rd Annual Meeting of the Association for Compu-
tational Linguistics and the 7th International Joint
Conference on Natural Language Processing (Vol-
ume 1: Long Papers), volume 1, pages 1375–1384.

Jiang Guo, Wanxiang Che, David Yarowsky, Haifeng
Wang, and Ting Liu. 2015. Cross-lingual depen-
dency parsing based on distributed representations.
In Proceedings of the 53rd Annual Meeting of the
Association for Computational Linguistics and the
7th International Joint Conference on Natural Lan-
guage Processing (Volume 1: Long Papers), vol-
ume 1, pages 1234–1244.

Jiang Guo, Wanxiang Che, David Yarowsky, Haifeng
Wang, and Ting Liu. 2016. A representation learn-
ing framework for multi-source transfer parsing. In
Proceedings of the Thirtieth AAAI Conference on
Artificial Intelligence, AAAI’16, pages 2734–2740.

Shafiq Joty, Preslav Nakov, Lluı́s Màrquez, and Israa
Jaradat. 2017. Cross-language learning with ad-
versarial neural networks. In Proceedings of the
21st Conference on Computational Natural Lan-
guage Learning (CoNLL 2017), pages 226–237.

Diederik P. Kingma and Jimmy Ba. 2015. Adam: A
method for stochastic optimization. In 3rd Inter-
national Conference on Learning Representations,
ICLR 2015, San Diego, CA, USA, May 7-9, 2015,
Conference Track Proceedings.

Eliyahu Kiperwasser and Yoav Goldberg. 2016. Sim-
ple and accurate dependency parsing using bidirec-
tional lstm feature representations. Transactions
of the Association for Computational Linguistics,
4:313–327.



1127

Adhiguna Kuncoro, Miguel Ballesteros, Lingpeng
Kong, Chris Dyer, and Noah A Smith. 2016. Distill-
ing an ensemble of greedy dependency parsers into
one mst parser. In Proceedings of the 2016 Con-
ference on Empirical Methods in Natural Language
Processing, pages 1744–1753.

Bowen Li, Jianpeng Cheng, Yang Liu, and Frank
Keller. 2019. Dependency grammar induction with
a neural variational transition-based parser. In Pro-
ceedings of the AAAI Conference on Artificial Intel-
ligence, volume 33, pages 6658–6665.

Yu-Hsiang Lin, Chian-Yu Chen, Jean Lee, Zirui Li,
Yuyan Zhang, Mengzhou Xia, Shruti Rijhwani,
Junxian He, Zhisong Zhang, Xuezhe Ma, Antonios
Anastasopoulos, Patrick Littell, and Graham Neu-
big. 2019. Choosing transfer languages for cross-
lingual learning. In Proceedings of the 57th Confer-
ence of the Association for Computational Linguis-
tics, ACL 2019, Florence, Italy, July 28- August 2,
2019, Volume 1: Long Papers, pages 3125–3135.
Association for Computational Linguistics.

André F. T. Martins. 2015. Transferring coreference
resolvers with posterior regularization. In Proceed-
ings of the 53rd Annual Meeting of the Association
for Computational Linguistics and the 7th Interna-
tional Joint Conference on Natural Language Pro-
cessing of the Asian Federation of Natural Language
Processing, ACL 2015, July 26-31, 2015, Beijing,
China, Volume 1: Long Papers, pages 1427–1437.
The Association for Computer Linguistics.

Ryan McDonald, Koby Crammer, and Fernando
Pereira. 2005. Online large-margin training of de-
pendency parsers. In Proceedings of ACL-2005,
pages 91–98, Ann Arbor, Michigan, USA.

Ryan McDonald, Joakim Nivre, Yvonne Quirmbach-
Brundage, Yoav Goldberg, Dipanjan Das, Kuzman
Ganchev, Keith Hall, Slav Petrov, Hao Zhang, Os-
car Täckström, et al. 2013. Universal dependency
annotation for multilingual parsing. In Proceed-
ings of the 51st Annual Meeting of the Association
for Computational Linguistics (Volume 2: Short Pa-
pers), volume 2, pages 92–97.

Ryan McDonald, Slav Petrov, and Keith Hall. 2011.
Multi-source transfer of delexicalized dependency
parsers. In Proceedings of the conference on empir-
ical methods in natural language processing, pages
62–72. Association for Computational Linguistics.

Tahira Naseem, Regina Barzilay, and Amir Globerson.
2012. Selective sharing for multilingual dependency
parsing. In Proceedings of the 50th Annual Meet-
ing of the Association for Computational Linguis-
tics: Long Papers-Volume 1, pages 629–637. Asso-
ciation for Computational Linguistics.

Tahira Naseem, Harr Chen, Regina Barzilay, and Mark
Johnson. 2010. Using universal linguistic knowl-
edge to guide grammar induction. In Proceedings of

the 2010 Conference on Empirical Methods in Nat-
ural Language Processing, pages 1234–1244. Asso-
ciation for Computational Linguistics.

Joakim Nivre, Mitchell Abrams, Željko Agić, and
et al. 2018. Universal dependencies 2.2. LIN-
DAT/CLARIN digital library at the Institute of For-
mal and Applied Linguistics (ÚFAL), Faculty of
Mathematics and Physics, Charles University.

Robert Östling. 2015. Word order typology through
multilingual word alignment. In Proceedings of the
53rd Annual Meeting of the Association for Compu-
tational Linguistics and the 7th International Joint
Conference on Natural Language Processing (Vol-
ume 2: Short Papers), volume 2, pages 205–211.

Nanyun Peng, Ryan Cotterell, and Jason Eisner. 2015.
Dual decomposition inference for graphical models
over strings. In Proceedings of the 2015 Conference
on Empirical Methods in Natural Language Pro-
cessing, EMNLP 2015, Lisbon, Portugal, Septem-
ber 17-21, 2015, pages 917–927. The Association
for Computational Linguistics.

Telmo Pires, Eva Schlinger, and Dan Garrette. 2019.
How multilingual is multilingual bert? In Pro-
ceedings of the 57th Conference of the Association
for Computational Linguistics, ACL 2019, Florence,
Italy, July 28- August 2, 2019, Volume 1: Long Pa-
pers, pages 4996–5001. Association for Computa-
tional Linguistics.

Edoardo Maria Ponti, Roi Reichart, Anna Korhonen,
and Ivan Vulić. 2018. Isomorphic transfer of syntac-
tic structures in cross-lingual NLP. In Proceedings
of the 56th Annual Meeting of the Association for
Computational Linguistics, ACL 2018, Melbourne,
Australia, July 15-20, 2018, Volume 1: Long Pa-
pers, pages 1531–1542. Association for Computa-
tional Linguistics.

Mohammad Sadegh Rasooli and Michael Collins.
2017. Cross-lingual syntactic transfer with limited
resources. Transactions of the Association for Com-
putational Linguistics, 5:279–293.

Mohammad Sadegh Rasooli and Michael Collins.
2019. Low-resource syntactic transfer with unsu-
pervised source reordering. In Proceedings of the
2019 Conference of the North American Chapter
of the Association for Computational Linguistics:
Human Language Technologies, NAACL-HLT 2019,
Minneapolis, MN, USA, June 2-7, 2019, Volume 1
(Long and Short Papers), pages 3845–3856. Associ-
ation for Computational Linguistics.

Dan Roth and Wen-tau Yih. 2004. A linear program-
ming formulation for global inference in natural lan-
guage tasks. Technical report, ILLINOIS UNIV AT
URBANA-CHAMPAIGN DEPT OF COMPUTER
SCIENCE.



1128

Alexander M Rush and Michael Collins. 2011. Exact
decoding of syntactic translation models through La-
grangian relaxation. In Proceedings of the 49th An-
nual Meeting of the Association for Computational
Linguistics: Human Language Technologies, pages
72–82.

Alexander M Rush and MJ Collins. 2012. A tutorial
on dual decomposition and lagrangian relaxation for
inference in natural language processing. Journal of
Artificial Intelligence Research, 45:305–362.

Samuel L Smith, David HP Turban, Steven Hamblin,
and Nils Y Hammerla. 2017. Offline bilingual word
vectors, orthogonal transformations and the inverted
softmax. Internation Conference on Learning Rep-
resentations.

Anders Søgaard. 2011. Data point selection for cross-
language adaptation of dependency parsers. In Pro-
ceedings of the 49th Annual Meeting of the Associ-
ation for Computational Linguistics: Human Lan-
guage Technologies: short papers-Volume 2, pages
682–686. Association for Computational Linguis-
tics.

Oscar Täckström, Ryan McDonald, and Joakim Nivre.
2013. Target language adaptation of discriminative
transfer parsers. In Proceedings of the 2013 Con-
ference of the North American Chapter of the Asso-
ciation for Computational Linguistics: Human Lan-
guage Technologies, pages 1061–1071. Association
for Computational Linguistics.

Jörg Tiedemann. 2015. Cross-lingual dependency
parsing with universal dependencies and predicted
pos labels. In Proceedings of the Third International
Conference on Dependency Linguistics (Depling
2015), pages 340–349.

Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob
Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz
Kaiser, and Illia Polosukhin. 2017. Attention is all
you need. In Advances in Neural Information Pro-
cessing Systems, pages 5998–6008.

Dingquan Wang and Jason Eisner. 2018a. Surface
statistics of an unknown language indicate how to
parse it. Transactions of the Association for Compu-
tational Linguistics (TACL).

Dingquan Wang and Jason Eisner. 2018b. Synthetic
data made to order: The case of parsing. In Proceed-
ings of the 2018 Conference on Empirical Methods
in Natural Language Processing, pages 1325–1337.

Wenhui Wang and Baobao Chang. 2016. Graph-based
dependency parsing with bidirectional lstm. In Pro-
ceedings of the 54th Annual Meeting of the Associa-
tion for Computational Linguistics (Volume 1: Long
Papers), volume 1, pages 2306–2315.

Min Xiao and Yuhong Guo. 2014. Distributed word
representation learning for cross-lingual dependency
parsing. In Proceedings of the Eighteenth Confer-
ence on Computational Natural Language Learning,
pages 119–129.

Jiateng Xie, Zhilin Yang, Graham Neubig, Noah A.
Smith, and Jaime Carbonell. 2018. Neural cross-
lingual named entity recognition with minimal re-
sources. In Proceedings of the 2018 Conference on
Empirical Methods in Natural Language Process-
ing, pages 369–379. Association for Computational
Linguistics.

Daniel Zeman and Philip Resnik. 2008. Cross-
language parser adaptation between related lan-
guages. In Proceedings of the IJCNLP-08 Workshop
on NLP for Less Privileged Languages.

Yuan Zhang and Regina Barzilay. 2015. Hierarchical
low-rank tensors for multilingual transfer parsing.
Association for Computational Linguistics.

Jieyu Zhao, Tianlu Wang, Mark Yatskar, Vicente Or-
donez, and Kai-Wei Chang. 2017. Men also like
shopping: Reducing gender bias amplification us-
ing corpus-level constraints. In Proceedings of the
2017 Conference on Empirical Methods in Natural
Language Processing, EMNLP 2017, Copenhagen,
Denmark, September 9-11, 2017, pages 2979–2989.
Association for Computational Linguistics.


