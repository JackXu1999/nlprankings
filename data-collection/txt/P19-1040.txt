



















































Evidence-based Trustworthiness


Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 413–423
Florence, Italy, July 28 - August 2, 2019. c©2019 Association for Computational Linguistics

413

Evidence-based Trustworthiness

Yi Zhang Zachary G. Ives
Department of Computer and Information Science

University of Pennsylvania
{yizhang5, zives, danroth}@cis.upenn.edu

Dan Roth

Abstract

The information revolution brought with it
information pollution. Information retrieval
and extraction help us cope with abundant in-
formation from diverse sources. But some
sources are of anonymous authorship, and
some are of uncertain accuracy, so how can
we determine what we should actually be-
lieve? Not all information sources are equally
trustworthy, and simply accepting the majority
view is often wrong.

This paper develops a general framework for
estimating the trustworthiness of information
sources in an environment where multiple
sources provide claims and supporting evi-
dence, and each claim can potentially be pro-
duced by multiple sources. We consider two
settings: one in which information sources
directly assert claims, and a more realistic
and challenging one, in which claims are in-
ferred from evidence provided by sources, via
(possibly noisy) NLP techniques. Our key
contribution is to develop a family of proba-
bilistic models that jointly estimate the trust-
worthiness of sources, and the credibility of
claims they assert. This is done while ac-
counting for the (possibly noisy) NLP needed
to infer claims from evidence supplied by
sources. We evaluate our framework on sev-
eral datasets, showing strong results and sig-
nificant improvement over baselines.

1 Introduction

The emergence of social networks and news ag-
gregators — combined with ill-informed posts,
deliberate efforts to create and spread sensation-
alized information, and a strongly polarized po-
litical environment — makes it very difficult to
establish what is really known. Therefore, fact
checking seeks to assess whether the claim is true
or false, or to provide a confidence level for the
claim given textual evidence (Hassan et al., 2017;

Wang, 2017; Wang et al., 2018). A typical fact
checking pipeline consists of document retrieval,
sentence-level evidence selection, and textual en-
tailment stages (Thorne et al., 2018). However,
this pipeline is local in that it applies to a given
claim. The missing step here is to assess the trust-
worthiness of the sources producing the claims
and evidence. This is a global step that, in princi-
ple, accounts for all claims made by a source and
all sources making a claim.

Previous work has studied how to estimate
the trustworthiness or credibility of information
sources for fact-finding (Vydiswaran et al.; Paster-
nack and Roth, 2013), truth discovery (Dong et al.;
Pochampally et al., 2014; Dong et al., 2015; Li
et al., 2016) and crowdsourcing (Sabou et al.,
2012; Hovy et al., 2013; Gao et al., 2015). Usu-
ally, given a list of conflicting facts, e.g. “source
s asserts claim c”, or “annotator x labels data item
t by label y”, we detect the true claims or correct
labels for the data item by resolving conflicts, and
then compute the trustworthiness of sources.

However, many sources do not directly assert
claims, but rather generate articles as evidence,
expecting readers to infer claims from this evi-
dence. In practice, given a claim of interest, peo-
ple may search for related articles from multiple
sources and collect evidence for the claim; they
can then determine the veracity of the claim by de-
ciding whether the evidence found supports or re-
futes the claim. However, most existing work that
attempted to study trustworthiness of sources as-
sumed that sources make assertions directly. Even
when intermediate text was accounted for (Vydis-
waran et al.; Nakashole and Mitchell, 2014), it was
assumed that clean evidence and clear connections
between evidence and conflicting claims are pro-
vided, disregarding the fact that NLP systems at-
tempting to support these tasks are noisy.

This paper considers two situations when eval-



414

Claim: Tom Brokaw wants 
Brian Williams fired.

Source: pagesix.com
Stance: For

Source: foxnews.com
Related Articles:
… NBC's Tom Brokaw reportedly 
wants Brian Williams fired over 
fabricated Iraq helicopter story …

Source: m.huffpost.com
Related Articles:
… Brian Williams' Future Uncertain 
As NBC News Investigates Iraq, 
Katrina Coverage …

Direct 
Assertion

Indirect 
Assertion

Textual 
Entailment 

for 
Indirect 

Assertion

For
(Entailment) 

Against
(Contradiction)

Figure 1: Claim with assertions from multiple sources
(from http://www.emergent.info/). Direct
assertions specify their stance; indirect assertions pro-
vide related articles, and we can leverage (noisy) text
entailment tools to collect their stances. We want to
assess whether to believe the stance and articles.

uating the trustworthiness of information sources:
(1) the source directly asserts claims, and (2) the
source indirectly asserts claims by proposing evi-
dence. The first case is similar to previous work;
the second case is more challenging but more im-
portant in practice. Both cases are depicted in Fig-
ure 1. A multitude of sources is given and each
may assert multiple claims or propose multiple
pieces of evidence. At the same time, multiple
claims are observed, some of which are directly
asserted by sources and some are supported by ev-
idence.

Our goals are to identify true claims and to es-
timate the trustworthiness of each source. The
key challenge is that this global inference task
is influenced by the knowledge of which claims
are made by which sources; however, establish-
ing links – from evidence generated by a source
to claims – requires NLP techniques such as tex-
tual entailment (TE) (Dagan et al., 2013). Such
TE tools, which assess whether a given textual ev-
idence (premise) entails a given claim (hypothe-
sis), are often noisy — making the evaluation of
sources more difficult.

The key contributions of this work are as
follows: (1) It proposes a probabilistic model,
JELTA, which jointly estimates the credibility of
claims and the trustworthiness of sources, when
claims are made by sources directly, indirectly, or
both. (2) Our framework incorporates a TE model
as part of the global inference framework as a way
to link evidence (and thus, sources) to claims. (3)

This is the first work to distinguish between di-
rect and indirect assertions made by information
sources.

Our experiments on both synthetic and natural
datasets show solid results that are significantly
better than baselines.

2 Trustworthiness Analysis

Our goal is to evaluate the trustworthiness of infor-
mation sources by detecting the true claims while
accounting for noise in the links between claims
and evidence for them. While direct assertions
are straightforward to deal with (since it is clear
which source generates which claim), the chal-
lenge is to incorporate “noisy assertions” into our
problem formulation. We first describe our setting,
and then elaborate on the probabilistic modeling.

Figure 2: Our solution considers two settings: (1)
source si directly asserts multiple claims cj ; (2) the
source provides evidence ek by multiple articles, and
the proposed evidence can support or refute claims via
some noisy NLP tool.

2.1 Noisy Assertions
We are given a set of claims to validate and a
text corpus (pieces of evidence) generated by mul-
tiple sources that are believed to have generated
the claims. Given the claim text, we issue a set
of searches over the corpus, to find evidence in
support of the claims. The result is a a set of
(noisy) assertions. A (noisy) assertion consists of
a claim, a sentence in the corpus, and a label (“en-
tailment”, “contradiction”, “neutral”). The claim
is a real world input we attempt to determine the
truth value of. E.g., in Figure 1, “Tom Brokaw

http://www.emergent.info/


415

wants Brian Williams fired” is such a
claim. An assertion, on the other hand, is an ar-
tifact of our framework. As we search the cor-
pus generated by the sources for evidence sup-
porting the claim, we identify candidate sentences
(‘Related Articles” in the figure) and use a pre-
trained textual entailment model (e.g. the decom-
posable attention model (Parikh et al., 2016)) to
provide an entailment label and complete the triple
(claim, sentence, label). The generation of noisy
assertions as described above follows a typical
fact-checking pipeline mentioned in Thorne et al.
(2018).

Table 1: Notation Table

Notation Description
s an information source
c a claim
m a mutually exclusion set of claims
e an evidence
ym the true claim in m
bs,c The (observed) probability of c asserted by s

bs,e,c
The (observed) probability of c
asserted by e from s

ws,m if s asserts claims of m
ws,e if s provides e
we,m if e supports or refutes claims of m
Hs the probability s makes an honest claim

Ps
the (hidden) probability s produces
a true evidence

Rs
the probability s recalls a true evidence
(true-positive rate)

Qs
the probability s recalls a false evidence
(false-positive rate)

Xi Set of all (observed) direct assertions
Xd Set of all (observed) indirect assertions
Y Set of all true claims

Given noisy assertions, Figure 2 illustrates our
problem setting. Overall, there are two situations.
In the upper part of the figure, we show the case
in which information sources make direct asser-
tions: the source directly states that some claims
are true or false. The alternative case, indicated
in the lower part of the figure, involves the source
indirectly asserting claims by making noisy asser-
tions: the source first generates articles that con-
tain sentences, and the sentences may entail or re-
fute related claims. An entailment tool can then
be used to assert the claims to be true or false,
based on those sentences. A claim can be sup-
ported by multiple sources or multiple pieces of
evidence from different sources. We now propose
our model, JELTA, which handles both cases de-
scribed above.

2.2 Fundamentals

Our probabilistic model denotes an information
source as s ∈ S, a claim as c ∈ C, and m as
a mutual exclusion set of claims (exactly one of
the claims in each mutual exclusion set is true).
Here m is a fact to be checked, and c is a state-
ment that m is true or false. ws,m, ws,e and we,m
are binary indicators — respectively telling us if
s asserts claims of m, if s provides evidence e,
and if evidence e supports claims in m. We de-
note evidence e ∈ E, and for each entailment
result, we use bs,c and bs,e,c to represent the ob-
served probability that s asserts c and s provides e
to assert c respectively. Here,

∑
c∈m bs,c = 1 and∑

c∈m bs,e,c = 1. We summarize our notation in
Table 1.

2.3 JELTA

Our work models a joint distribution that reflects a
“story” of how sources generate observations. In-
tuitively, given an estimation of the verdict of the
claims and the factors, including the trustworthi-
ness of sources providing claims and evidence, we
want to maximize the probability that we can ob-
serve the claims and evidence.

We represent the verdict of a claimm as a latent
variable, ym, and associate a parameter Hs with
each source s, reflecting the probability of s telling
the truth, which we use to measure the trustwor-
thiness of s. We now describe how ym and Hs
are used to compute the probability of observing
the claims and evidence. Starting with the proba-
bility that source s makes a direct assertion: intu-
itively, if s asserts a true claim c = ĉ inm, then the
probability that s asserts c is Hs, the probability s
telling the truth. Otherwise, s chooses uniformly
from other claims in m with probability 1−Hs|m|−1 .

Besides the term Hs, we require another (hid-
den) factor related to s, namely, the probability of
s telling the truth as evidence. We denote this as
Ps, the precision of s generating evidence. Here
we allow Ps can be different with Hs, since pro-
viding true evidence for a true claim is more dif-
ficult than just providing a true claim. However,
considering that those all reflect the trustworthi-
ness of s, we assume they share a similar distribu-
tion over sources in our problem.
Ps can then be represented by two other param-

eters, Rs and Qs (Dong et al., 2015). These rep-
resent the true- and false-positive rates of s pro-
ducing evidence, respectively. We denote γ as the



416

Figure 3: Plate diagram for probabilistic model de-
scribing the generation of direct and indirect assertions.
Shaded parts are the observations, ym is the latent vari-
able, and Hs, Rs, Qs and φw are groups of parameters.
Dotted lines describe the interactions between Hs and
Rs, Qs.

probability of a claim being true, then Ps can be
represented by Qs and Rs as:

Ps =
γRs

γRs + (1− γ)Qs
(1)

We assume that the probability of the claim be-
ing true or false is equal. Since Hs and Ps share
similar distributions, Hs relates to Qs and Rs as
follows:

Hs ∼ Ps =
Rs

Rs +Qs
(2)

Now we discuss how to compute the probabil-
ity of observing the noisy assertions. Intuitively,
when source s wants to assert a claim with the
NLP tool (textual entailment model) by proposing
evidence: if s wants to support a true claim indi-
rectly, s will recall true evidence with probability
Rs. This requires the NLP tool to do textual en-
tailment correctly, otherwise s will also uniformly
choose false or unrelated evidence.

This paper considers the simplest way to gener-
ate a false claim or false evidence, and the choice
may not always follow random sampling in prac-
tice. Our prior work Pasternack and Roth (2013)
discusses some other options, which could alter-
natively be used here.

In the remainder of this section, we formally
model those processes.

Direct Assertion. Modeling the generative pro-
cess of direct assertions by sources is very similar
to Simple LCA (Pasternack and Roth, 2013). As
above, if the claim c ∈ m asserted by s is the true
claim ym, the probability of observing the source s

asserting claim c of m is Hs. Otherwise, the prob-
ability of s asserting a false claim of m is 1−Hs|m|−1 .

Therefore, the joint probability of the observa-
tion over Xd and ym can be modeled as follows:

P (ym, Xd|Hs) =

P (ym)
(
H
bs,ym
s

∏
c∈m\ym

(
1−Hs
|m| − 1

)1−bs,c
)ws,m

(3)
Then, given all sources S and θ = {Hs}, we

can write the full joint of direct observations as:

P (Y,Xd|θ) =∏
m

P (ym)
∏
s

(
H
bs,ym
s (

1−Hs
|m| − 1

)1−bs,ym
)ws,m

(4)
Note that we simplify the expression by leveraging∑

c∈m\ym bs,c = 1− bs,ym .

Indirect Assertion. Here the sources provide
articles containing possible evidence rather than
making direct assertions. Besides the parameters
Qs and Rs, the observation also depends on the
noisy entailment results given by the textual en-
tailment model. Therefore, we introduce a func-
tion φw(e,m, c) ∈ R1 to measure the reliability of
an entailment result. Here φw(e,m, c) is a linear
combination of feature values in a sigmoid func-
tion, so that we can scale it to [0, 1]:

φw(e,m, c) =
exp(

∑
iwizi)

1 + exp(
∑

iwizi)
(5)

where zi is a feature for each given 〈e,m, c〉, and
w = {wi} are the weights of each zi learned by
our model.

For each observation 〈s, e,m, c〉, the source
generates true evidence with probability Rs, and
with probability of φw(e,m, c), the proposed evi-
dence e supports claim c of m. This means that
we have probability of Rs · φw(e,m, c) to ob-
serve the tuple when c = ym. If c 6= ym, ei-
ther the source does not provide true evidence, or
the entailment model provides an unreliable en-
tailment result — which means we have probabil-
ity of 1N

(
1 − Ps · φw(e,m, c)

)
to observe a false

evidence-claim pair. Here N is the total number
of such false evidence-claim pairs.

Therefore, the joint probability of the observa-
tion over ym and Xi (indirect assertion observa-



417

tions) is as follows:

P (ym, Xi|Rs, Qs,W ) =

P (ym)
∏
e

((
Rs · φw(e,m, c)

)bs,e,ym
(1− RsRs+Qs · φw(e,m, c)

N

)1−bs,e,ym)ws,e,we,m
(6)

Here, we also use
∑

c∈m\ym bs,e,c = 1−bs,e,ym .
Then, given all sources S and θ = {Qs, Rs,W},
the full joint probability of indirect assertions is:

P (Y,Xi|θ) =∏
m

P (ym)
∏
s

∏
e

((
Rs · φw(e,m, c)

)bs,e,ym
(1− RsRs+Qs · φw(e,m, c)

N

)1−bs,e,ym)ws,ewe,m
(7)

Joint Modeling. Now to consider direct and in-
direct assertions together, we multiply Equations 4
and 7 together with two hyper-parameters, ηd and
ηi, which give different weights to direct and indi-
rect assertions. If ηd > ηi, this means we believe
that a direct assertion is more accurate than an in-
direct assertion, and vice versa.

Therefore, observing that all sources propose
their evidence and make their assertions indepen-
dently, and taking θ = {Hs, Rs, Qs,W}, we can
write the full joint as:

P (X,Y |θ) =
∏
m

P (ym)
∏
s

(
H
bs,ym
s

(
1−Hs
|m| − 1

)1−bs,ym
)ws,mηd∏

e

((
Rs · φ

)bs,e,ym
(1− RsRs+Qs · φ

N

)1−bs,e,ym)ws,ewe,mηi
(8)

where φ = φw(e,m, c) for abbreviation. Mean-
while, since Hs ∼ RsRs+Qs , we model it by min-
imizing their KL divergence. Therefore, we also
minimize:

EHs [log
Hs
Ps

] =
∑
s

Hs log
Hs
Ps

=
∑
s

Hs log
Hs(Rs +Qs)

Rs

(9)

2.4 Inference
The true claim, ym, is a latent variable that is
unknown in our problem, so we solve this ap-

proximately by using the EM algorithm (Demp-
ster et al., 1977) to first estimate the true claim,
then find the maximum a posterior point estimate
of the parameters. Therefore, the E-step is ∀m:

P (ym = c|X, θt) =
P (ym = c|X, θt)∑
v∈m P (ym = v|X, θt)

(10)
In the M-step, besides maximizing the posterior

of parameters, we should also consider the inter-
actions between Hs and Rs, Qs. We include it as
a regularization term with a parameter λ that con-
trols the importance of the interactions. Thus, the
M-step is as follows:

θt+1 = argmaxθEY |X,θt [logP (X,Y |θ)P (θ)]

− λEHs [log
Hs
Ps

]

(11)
Since there are no closed form solutions for

those parameters, we use gradient ascent to solve
them parameter-by-parameter. We leave the com-
putation of derivatives to the appendix.

2.5 Measuring Entailment Results
In our model, φw(e,m, c) evaluates the reliabil-
ity of an entailment result given by the entail-
ment model. As we described in Section 2.3,
φw(e,m, c) is a sigmoid function of a linear com-
bination of feature values, and we include follow-
ing features:

Entailment Score. For each prediction of the
given entailment model, the model will predict a
label, i.e. entailment, contradiction or neutral as
well as a score to support its conclusion.

Text Similarity. This feature is computed by
the cosine similarity between numerical represen-
tations of the evidence and the claim. In this work,
we use tf-idf and Glove (Pennington et al., 2014)
to represent sentences respectively. To represent
a sentence, we use the pre-trained Glove 1 with a
simple method proposed in (Arora et al., 2017).

Entity Similarity. We identify entities for each
pair of evidence and claim, and compute the over-
lap of entities by jaccard similarity and entity sim-
ilarity by NESim (Do et al., 2009) as two features.

3 Experimental Evaluation

We evaluate the effectiveness of our joint model
JELTA and compare it with baselines. We first de-

1https://nlp.stanford.edu/projects/
glove/

https://nlp.stanford.edu/projects/glove/
https://nlp.stanford.edu/projects/glove/


418

scribe our datasets and the methods we compare
with, then elaborate on the results.

3.1 Experimental Settings

Data Sets We use both synthetic and natural
datasets to evaluate our models.

Synthetic Dataset: FEVER. We use the train-
ing file of FEVER2 to create the synthetic dataset.
FEVER is a dataset for verification of claims. We
augment FEVER with sources and other informa-
tion using following steps.

Step 1: Assign Veracity for Claims. Fever pro-
vides evidence-claim pairs with their textual en-
tailment labels. Considering our running example,
Fever provides sentence pairs such as “...NBC’s
Tom Brokaw reportedly...” and “Tom Brokaw
wants Brian Williams fired.” as evidence and
claim. For each experimental round, we sample
200 claims from those pairs, then randomly assign
half as true and half as false.These labels will be
the ground truth of claims’ veracity.

Step 2: Create Sources with Accuracy. Next, we
create sources with corresponding accuracy as the
ground truth of trustworthiness. In our each exper-
imental round, we create 200 sources and for each
source si, we associate an accuracy denoted as
Hsi . To generate Hsi , we sample a decimal num-
ber from a normal distribution (µ = 0.5, σ = 1) in
[0, 1]. A normal distribution is used here because
we assume most sources mix true and false claims,
and a few of them are highly trustworthy or totally
unbelievable.

Step 3: Associate Sources with Evidence and
Claims. The last step is to assign claims and ev-
idence to each source. In our experiments, each
source makes 30 assertions. Each source si, with
probability Hsi , picks a true claim; otherwise it
picks a false claim. For evidence, since we as-
sume that the distribution of precision generating
evidence over sources shares a similar distribu-
tion with {Hsi}, the source si picks a piece of
evidence either supporting a true claim or refut-
ing a false claim with Hsi + �, where epsilon is a
small Gaussian noise (µ = 0, σ = 1). Considering
the running example, if claim “Tom Brokaw wants
Brian Williams fired.” is associated with “True”
in Step 1, and Fever provides the pair with la-
bel “Entailment”, “...NBC’s Tom Brokaw report-
edly...” is therefore a piece of evidence supporting
a true claim. Otherwise si picks a piece of evi-

2http://fever.ai/data.html

dence supporting a false claim or refuting a true
claim. Note we assume that each source provides
more pieces of evidence than claims, and set the
ratio of direct assertions to indirect assertions as 14
in our expeirments.

We run 10 rounds of experiments and report the
average performance.

Natural Dataset: Emergent. We use Emer-
gent (Ferreira and Vlachos, 2016) directly; it is
derived from a digital journalism project for ru-
mor debunking. It contains 300 rumored claims
and 2,595 associated news articles from different
websites, collected and labeled by journalists with
an estimate of their veracity (true, false or unver-
ified). We eliminated the claims that are unveri-
fied, leaving 201 claims and 589 effective sources.
For each source, the dataset provides the claims
it supports or refutes, which we use as direct as-
sertions. It also provides the articles generated by
the source, and we use them as possible evidence
repository that may support or refute the claims.
The ground truth of the trustworthiness is gener-
ated by computing the accuracy of sources based
on the veracity label provided.

Entailment Model. We need a textual entail-
ment model to tell us if evidence (a sentence) sup-
ports or refutes a claim. We use a pre-trained de-
composable attention model (Parikh et al., 2016)
with Elmo embedding (Peters et al., 2018) trained
on the SNLI dataset 3. The model’s performance
is not good on either FEVER or Emergent: when
we use majority voting over the evidence to esti-
mate the veracity of a claim, the accuracy is under
40%. To improve the textual entailment model, we
adapt the pre-trained model with additional train-
ing data. For the experiment on FEVER, we ran-
domly sample 100 training examples from labeled
development dataset of FEVER. (There is no over-
lap between the additional training data and our
created test data.) For the experiments on Emer-
gent, we construct additional training data by arti-
cle headlines and article headline stance provided
by Emergent. Here, the article headline is gen-
erated by each article, and the dataset tells us if
the article headline can support or refute the claim,
which is a good source of additional training data.

Metrics To evaluate the performance of our
method as well as the baselines, we evaluate

3https://nlp.stanford.edu/projects/
snli/

http://fever.ai/data.html
https://nlp.stanford.edu/projects/snli/
https://nlp.stanford.edu/projects/snli/


419

(a) Veracity accuracy of claims on

FEVER

(b) Veracity accuracy of claims on

Emergent
(c) Veracity varying noise rate in textual

entailment results

Figure 4: The performance of estimating veracity of claims by different methods. On both FEVER and Emergent, JELTA
achieves the highest accuracy, and a low standard deviation in the 10 rounds evaluation on FEVER. (c) reports the accuracy
variation when we add different rate of noise in the textual entailment results, and JELTA is consistently better.

(a) Evaluating correlation estimation on

FEVER

(b) Evaluating correlation estimation on

Emergent
(c) Correlation varying noise rate in

textual entailment results

Figure 5: The performance of estimating the trustworthiness of sources by Peasrson and Spearman score. The methods
considering both direct and indirect assertions are much better than those considering one of them only, and JELTA can achieve
an additional improvement. (c) reports the performance varying different rate of noises being added to the entailment results,
and JELTA is also consistently better than other methods.

(1) the accuracy of the estimated veracity of the
claims, (2) the accuracy of the estimated trustwor-
thiness of the source. Here, we evaluate trust-
worthiness by two typical correlation scores, the
Spearman correlation coefficient and Pearson cor-
relation coefficient (Fieller et al., 1957). Spear-
man’s correlation assesses monotonic relation-
ships, whereas Pearson’s correlation is the covari-
ance of two random variables — thus when com-
puting Pearson’s correlation, we normalize the es-
timated accuracy of the sources.

Baselines
MJ-Claim. In this case, we only consider direct

assertions made by sources, and for each claim we
collect all related assertions and do majority vote
to estimate the veracity of the claims. Once we get
an estimation of their veracity, we can compute the
accuracy for each source.

MJ-EVI. We only consider indirect assertions in
this case. With the textual entailment model out-
put, each evidence provided by the article will ei-

ther support, refute or abstain the claim. Here,
we also use majority vote to estimate the verac-
ity of the claims, and use the mean ratio between
the number of evidence supporting the true claim
and the total number of evidence for each claim to
estimate the trustworthiness of the source.

Sim-LCA. We leverage the model proposed in
(Pasternack and Roth, 2013) to estimate the credi-
bility of the sources. Here, the model only consid-
ers direct assertions.

Sim-Com. We propose a simple solution that
considers both direct and indirect assertions. Here,
we use MJ-EVI to estimate the truth of the claims,
based on which we calculate the accuracy for each
source. Note that the results are the same com-
pared with MJ-EVI when estimating the veracity
of the claims, while the estimation of trustworthi-
ness over sources is different.

3.2 Results

Accuracy of Veracity. Figure 4 (a) reports (for
each method) the accuracy of estimation for



420

the claims’ veracity, over our synthetic dataset.
JELTA achieves the highest accuracy, by around
5%, and shows a low standard deviation over the
10 rounds of experiments. Figure 4 (b) reports
the accuracy on Emergent. We again observe a
4% improvement in accuracy compared with MJ-
EVI, and around 16% improvement vs the meth-
ods considering direct assertions only. It makes
sense that evidence-based method (leveraging in-
direct assertions) can beat the claim-based method
(leveraging direct assertions only) by using more
information to reduce potential noise. However,
using sources and claims only is more noisy, espe-
cially with many bad information sources. Fig-
ure 4 (a) shows that when the distribution of
sources changes, the performance of MJ-Claim
and Sim-LCA also varies a lot: their performance
greatly depends on the distribution of trustworthi-
ness over sources. Besides offering higher accu-
racy, evidence-based methods are more robust to
varying sources’ trustworthiness.

Trustworthiness Estimation. Figure 5 (a) re-
ports the performance by Pearson and Spearman
score, for each method’s estimate of the trustwor-
thiness of each source on FEVER. JELTA’s ac-
curacy is consistently better than other baselines,
whenever we use the Spearman or Pearson score
to compute the correlation between the estimation
and the ground truth. JELTA also has a lower stan-
dard deviation over different rounds. This result
is consistent with the results shown when we are
estimating the veracity of claims. It reveals that
evidence-based methods are relatively more sta-
ble than the methods considering direct assertions
only. MJ-Claim and Sim-LCA highly depend on
the trustworthiness distribution over sources. If
most of the sources are more trustworthy, we can
both estimate the true claims more accurately and
better estimate the trustworthiness of sources; and
vice versa. That is why both MJ-Claim and Sim-
LCA have high standard deviations over different
rounds. Based on the results of MJ-EVI, we can
observe that simply calculating accuracy by esti-
mated “correct” evidence cannot achieve a highly
correlated estimation of sources: the entailment
tool provides noisy evidence. However, Sim-Com,
which directly counts estimated “correct” claims
by MJ-EVI, can improve the estimation. Thus, if
we can estimate the veracity of claims accurately,
estimating the trustworthiness by claims is more
accurate than doing that by noisy evidence. This

is also why we can significantly improve the per-
formance by joint modeling. Intuitively, we use
evidence to better estimate the veracity of claims,
and leverage claims to better estimate the trust-
worthiness of sources, in an iterative fashion. Fig-
ure 5 (b) leads to similar conclusions. Since there
are more trustworthy sources, the performance of
claim-based methods is better than MJ-EVI.

Influence of textual entailment model. Fig-
ures 4 and 5 show that our method, which jointly
considers direct and indirect assertions, signifi-
cantly improves the estimation. Among different
factors, evidence contributes the most when esti-
mating the veracity of claims, which can also help
the estimation of the trustworthiness. However,
the usefulness of evidence highly depends on the
quality of the NLP tool. To quantify the amount of
noise introduced, we report the Pearson and Spear-
man score varying a noise rate r. Given r, for each
entailment result, with probability r, we will flip
the answer of the textual entailment. For exam-
ple, if the result is “entailment”, we will change
it randomly to either “contradiction” or “neutral”,
and vice versa. The results are shown in (c) of Fig-
ure 4 and 5. As noise increases, the accuracy, Pear-
son and Spearman score drop lower. However, the
JELTA method is consistently better than the alter-
natives. JELTA’s accuracy decreases more slowly,
and its correlation remains positive, even though
we flip 95% of the entailment results. This demon-
strates that jointly considering direct and indirect
assertions can better avoid the skewness caused by
either evidence or claims.

4 Related Work

Evaluating the trustworthiness of sources has
been studied for fact-finding, truth discovery and
crowdsourcing. In the context of fact-finding (Vy-
diswaran et al.; Pasternack and Roth, 2013) and
truth discovery (Yin et al., 2008; Dong et al.; Zhao
et al., 2012; Li et al., 2014; Pochampally et al.,
2014; Dong et al., 2015; Li et al., 2016), the solu-
tions estimate the trustworthiness or credibility of
sources, by resolving the conflicts of claims pro-
vided by multiple sources. The claims are usually
in structured form, and conflicting values can be
easily captured without noise. Works in (Vydis-
waran et al.; Nakashole and Mitchell, 2014; Popat
et al., 2017) further take text into consideration,
however, in (Vydiswaran et al.; Nakashole and
Mitchell, 2014), they still depend on a structured



421

input form and thus the connection between evi-
dence and conflicting claims are given, which is
usually not practical. Popat et al. (2017) lever-
ages text as evidence to do fact-checking, while
their estimation of credibility of sources neglects
the reliability of sources generating evidence. In
crowdsourced labeling (Sabou et al., 2012; Hovy
et al., 2013; Gao et al., 2015), the system is given
noisy labels which are annotated by different an-
notators. The input is again in structured form, and
there is no evidence to consider. This is a limited
setting compared with our problem. Our problem
is also related to fact-checking (Wang et al., 2018;
Thorne et al., 2018; Yin and Roth, 2018; Zhao
et al., 2018), however they only consider if the ev-
idence can support the claim without tracking the
source of the claim and evidence.

5 Conclusions and Future Work

This paper studied the problem of estimating the
trustworthiness of given information sources. The
sources make direct claims or indirect claims by
generating evidence that implies these claims.

We proposed a probabilistic framework, JELTA,
which jointly considers both kinds of assertions to
better estimate claims’ veracity and sources’ trust-
worthiness. We evaluated JELTA over both syn-
thetic and real datasets, and our results show sig-
nificant improvements over baselines.

While we presented the framework here as ap-
plying to claims with two truth values, we believe
that this framework can apply more broadly. For
example, rather than considering a claim as being
True or False, (Chen et al., 2019) suggests that one
needs to view a claim from a diverse, yet compre-
hensive, set of perspectives. Our framework can
be extended to deal with sources that generate a
spectrum of perspectives, each with a stance rela-
tive to claim and with evidence supporting it. We
leave this for future work.

Acknowledgments

This work is partly supported by a Google gift and
by DARPA, under agreement number HR0011-
18-2-0052.

References
Sanjeev Arora, Yingyu Liang, and Tengyu Ma. 2017.

A simple but tough-to-beat baseline for sentence em-
beddings. ICLR.

Sihao Chen, Daniel Khashabi, Wenpeng Yin, Chris
Callison-Burch, and Dan Roth. 2019. Seeing things
from a different angle: Discovering diverse perspec-
tives about claims. In NAACL.

Ido Dagan, Dan Roth, Mark Sammons, and Fabio Mas-
simo Zanzoto. 2013. Recognizing textual entail-
ment: Models and applications.

Arthur P Dempster, Nan M Laird, and Donald B Rubin.
1977. Maximum likelihood from incomplete data
via the em algorithm. Journal of the royal statistical
society. Series B (methodological), pages 1–38.

Quang Do, Dan Roth, Mark Sammons, Yuancheng Tu,
and V Vydiswaran. 2009. Robust, light-weight ap-
proaches to compute lexical similarity. Computer
Science Research and Technical Reports.

Xin Luna Dong, Laure Berti-Equille, and Divesh Sri-
vastava. Integrating conflicting data: the role of
source dependence. Proceedings of the VLDB En-
dowment.

Xin Luna Dong, Evgeniy Gabrilovich, Kevin Murphy,
Van Dang, Wilko Horn, Camillo Lugaresi, Shaohua
Sun, and Wei Zhang. 2015. Knowledge-based trust:
Estimating the trustworthiness of web sources. Pro-
ceedings of the VLDB Endowment, 8(9):938–949.

William Ferreira and Andreas Vlachos. 2016. Emer-
gent: a novel data-set for stance classification. In
Proceedings of the 2016 conference of the North
American chapter of the association for computa-
tional linguistics: Human language technologies,
pages 1163–1168.

Edgar C Fieller, Herman O Hartley, and Egon S Pear-
son. 1957. Tests for rank correlation coefficients. i.
Biometrika, 44(3/4):470–481.

Jing Gao, Qi Li, Bo Zhao, Wei Fan, and Jiawei Han.
2015. Truth discovery and crowdsourcing aggre-
gation: A unified perspective. Proceedings of the
VLDB Endowment, 8(12):2048–2049.

Naeemul Hassan, Gensheng Zhang, Fatma Arslan, Jo-
sue Caraballo, Damian Jimenez, Siddhant Gawsane,
Shohedul Hasan, Minumol Joseph, Aaditya Kulka-
rni, Anil Kumar Nayak, et al. 2017. Claimbuster:
the first-ever end-to-end fact-checking system. Pro-
ceedings of the VLDB Endowment, 10(12):1945–
1948.

Dirk Hovy, Taylor Berg-Kirkpatrick, Ashish Vaswani,
and Eduard Hovy. 2013. Learning whom to trust
with mace. In Proceedings of the 2013 Conference
of the North American Chapter of the Association
for Computational Linguistics: Human Language
Technologies, pages 1120–1130.

Qi Li, Yaliang Li, Jing Gao, Bo Zhao, Wei Fan, and
Jiawei Han. 2014. Resolving conflicts in heteroge-
neous data by truth discovery and source reliability
estimation. In Proceedings of the 2014 ACM SIG-
MOD international conference on Management of
data, pages 1187–1198. ACM.



422

Yaliang Li, Jing Gao, Chuishi Meng, Qi Li, Lu Su,
Bo Zhao, Wei Fan, and Jiawei Han. 2016. A sur-
vey on truth discovery. ACM SIGKDD Explorations
Newsletter, 17(2):1–16.

Ndapandula Nakashole and Tom M Mitchell. 2014.
Language-aware truth assessment of fact candidates.
In Proceedings of the 52nd Annual Meeting of the
Association for Computational Linguistics (Volume
1: Long Papers), volume 1, pages 1009–1019.

Ankur P Parikh, Oscar Täckström, Dipanjan Das, and
Jakob Uszkoreit. 2016. A decomposable attention
model for natural language inference. arXiv preprint
arXiv:1606.01933.

Jeff Pasternack and Dan Roth. 2013. Latent credibility
analysis. In Proceedings of the 22nd international
conference on World Wide Web, pages 1009–1020.
ACM.

Jeffrey Pennington, Richard Socher, and Christopher
Manning. 2014. Glove: Global vectors for word
representation. In Proceedings of the 2014 confer-
ence on empirical methods in natural language pro-
cessing (EMNLP), pages 1532–1543.

Matthew E Peters, Mark Neumann, Mohit Iyyer, Matt
Gardner, Christopher Clark, Kenton Lee, and Luke
Zettlemoyer. 2018. Deep contextualized word rep-
resentations. arXiv preprint arXiv:1802.05365.

Ravali Pochampally, Anish Das Sarma, Xin Luna
Dong, Alexandra Meliou, and Divesh Srivastava.
2014. Fusing data with correlations. In Proceedings
of the 2014 ACM SIGMOD international conference
on Management of data, pages 433–444. ACM.

Kashyap Popat, Subhabrata Mukherjee, Jannik
Strötgen, and Gerhard Weikum. 2017. Where the
truth lies: Explaining the credibility of emerging
claims on the web and social media. In Proceedings
of the 26th International Conference on World Wide
Web Companion, pages 1003–1012. International
World Wide Web Conferences Steering Committee.

Marta Sabou, Kalina Bontcheva, and Arno Scharl.
2012. Crowdsourcing research opportunities:
lessons from natural language processing. In Pro-
ceedings of the 12th International Conference on
Knowledge Management and Knowledge Technolo-
gies, page 17. ACM.

James Thorne, Andreas Vlachos, Christos
Christodoulopoulos, and Arpit Mittal. 2018.
Fever: a large-scale dataset for fact extraction and
verification. arXiv preprint arXiv:1803.05355.

VG Vydiswaran, ChengXiang Zhai, and Dan Roth.
Content-driven trust propagation framework. In
Proceedings of the 17th ACM SIGKDD interna-
tional conference on Knowledge discovery and data
mining.

William Yang Wang. 2017. ” liar, liar pants on fire”:
A new benchmark dataset for fake news detection.
arXiv preprint arXiv:1705.00648.

Xuezhi Wang, Cong Yu, Simon Baumgartner, and Flip
Korn. 2018. Relevant document discovery for fact-
checking articles. In Companion of the The Web
Conference 2018 on The Web Conference 2018,
pages 525–533. International World Wide Web Con-
ferences Steering Committee.

Wenpeng Yin and Dan Roth. 2018. Twowingos: A
two-wing optimization strategy for evidential claim
verification. arXiv preprint arXiv:1808.03465.

Xiaoxin Yin, Jiawei Han, and S Yu Philip. 2008.
Truth discovery with multiple conflicting informa-
tion providers on the web. IEEE Transactions on
Knowledge and Data Engineering, 20(6):796–808.

Bo Zhao, Benjamin IP Rubinstein, Jim Gemmell, and
Jiawei Han. 2012. A bayesian approach to dis-
covering truth from conflicting sources for data in-
tegration. Proceedings of the VLDB Endowment,
5(6):550–561.

Shuai Zhao, Bo Cheng, Hao Yang, et al. 2018. An
end-to-end multi-task learning model for fact check-
ing. In Proceedings of the First Workshop on Fact
Extraction and VERification (FEVER), pages 138–
144.



423

A Appredix

A.1 Inference
To infer the value of latent variables and param-
eters in our model, we use EM algorithm to first
estimate the true claim, and then find the maxi-
mum a posterior point estimate of the parameters.
As shown in Section 2.4, given parameters θt and
X , E-step is easy to compute, while the M-step
is more complicated. Since there are no closed
form solutions for those parameters, we use gradi-
ent ascent to solve them and do them parameter-
by-parameter.

For Hs, we have:

∂P (X,Y |θ)
∂Hs

=

∑
m

∑
ym

P (ym|X, θt)ws,mηd(bs,ym −Hs)
Hs −Hs2

+ λ
(
log

Rs
Rs +Qs

− logHs − 1
)

(12)

Then, for Rs, Qs and W , the derivatives are as
follows:

∂P (X,Y |θ)
∂Rs

=
∑
m

∑
ym

P (ym|X, θt)ηi
∑
e

ws,ewe,m

[ bs,e,ym
Rs

+
(1− bs,e,ym)φw(e,m, c)Qs

Rs(Qs +Rs)φw(e,m, c)− (Qs +Rs)2
]

+ λ ·Hs
Qs

Rs(Qs +Rs)
(13)

∂P (X,Y |θ)
∂Qs

=
∑
m

∑
ym

P (ym|X, θt)ηi

∑
e

ws,ewe,m
(1− bs,e,ym)φw(e,m, c)Rs

(Qs +Rs)2 −Rs(Qs +Rs)φw(e,m, c)

− λ · Hs
Rs +Qs

(14)

∂P (X,Y |θ)
∂wi

=
∑
m

∑
ym

P (ym|X, θt)ηi
∑
s

∑
e

ws,ewe,m

[
bs,e,ym +

Hs(1− bs,e,ym)φw(e,m, c)
Hs · φw(e,m, c)− 1

](
1− φw(e,m, c)

)
zi

(15)


