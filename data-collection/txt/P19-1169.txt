



















































SphereRE: Distinguishing Lexical Relations with Hyperspherical Relation Embeddings


Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 1727–1737
Florence, Italy, July 28 - August 2, 2019. c©2019 Association for Computational Linguistics

1727

SphereRE: Distinguishing Lexical Relations with Hyperspherical Relation
Embeddings

Chengyu Wang1, Xiaofeng He1∗, Aoying Zhou2
1School of Computer Science and Software Engineering, East China Normal University

2School of Data Science and Engineering, East China Normal University
chywang2013@gmail.com, xfhe@sei.ecnu.edu.cn

ayzhou@dase.ecnu.edu.cn

Abstract
Lexical relations describe how meanings of
terms relate to each other. Typical relations in-
clude hypernymy, synonymy, meronymy, etc.
Automatic distinction of lexical relations is
vital for NLP applications, and is also chal-
lenging due to the lack of contextual signals
to discriminate between such relations. In
this work, we present a neural representation
learning model to distinguish lexical relations
among term pairs based on Hyperspherical Re-
lation Embeddings (SphereRE). Rather than
learning embeddings for individual terms, the
model learns representations of relation triples
by mapping them to the hyperspherical em-
bedding space, where relation triples of dif-
ferent lexical relations are well separated. We
further introduce a Monte-Carlo based sam-
pling and learning algorithm to train the model
via transductive learning. Experiments over
several benchmarks confirm SphereRE outper-
forms state-of-the-arts.

1 Introduction

Lexical relations are relations between terms in
lexicons. Types of lexical relations include hyper-
nymy, synonymy, meronymy, etc. Such relations
are treated as key resources for various NLP ap-
plications, e.g., question answering (Yang et al.,
2017), taxonomy induction (Shen et al., 2018),
machine translation (Zhang et al., 2018), natural
language inference (Inkpen et al., 2018), lexical
database construction (Speer et al., 2017), etc.

Due to its importance, automatic acquisition
of lexical relations is a research focus in NLP.
In early years, lexical relations in WordNet
were manually compiled by linguists (Miller,
1995). Recently, path-based and distributional
approaches are two major paradigms to classify
a term pair into a fixed inventory of lexical re-
lations, or to predict it as random (meaning the

∗Corresponding author.

two terms are un-related) (Shwartz and Dagan,
2016; Wang et al., 2017a). Path-based approaches
use dependency paths connecting two terms to
infer lexical relations (Washio and Kato, 2018a;
Roller et al., 2018). The paths usually describe
relations between terms explicitly, but require the
two terms co-occur in a sentence, leading to the
“low coverage” problem. Apart from Hearst pat-
terns (Hearst, 1992), there are few high-quality
textual patterns to recognize lexical relations other
than hypernymy. Distributional approaches con-
sider the global contexts of terms to predict lexi-
cal relations using word embeddings (Baroni et al.,
2012; Glavas and Vulic, 2018). They are reported
to outperform several path-based approaches, but
can suffer from the “lexical memorization” prob-
lem (Levy et al., 2015; Shwartz and Dagan, 2016).
This is because some supervised distributional ap-
proaches learn properties of two terms separately,
instead of how two terms relate to each other in the
embedding space.

(a) Term Embedding Space (b) Relation Embedding Space

Car

Auto

Automobile

Vehicle

Engine

Wheel

Hypernymy
Synonymy
Meronymy

(Car, Hypernymy, 
Vehicle)

(Car, Meronymy, Engine)

(Car, Meronymy, Wheel)

(Car, Synonymy, Auto)

(Car, Synonymy, 
Automobile)

O Ox x

y y

Figure 1: An example of hyperspherical learning w.r.t.
the term car and three types of lexical relations.

In this paper, we aim at improving distributional
approaches by learning lexical relation representa-
tions in hyperspherical embedding space, named
hyperSpherical Relation Embeddings (SphereRE).
Consider the example w.r.t. car in Figure 1. Word
embeddings of these terms are similar to each
other due to their contextual similarity. Hence,
embedding offsets of term pairs can not distin-



1728

guish the three types of lexical relations well
(i.e., hypernymy, synonymy and meronymy). In-
stead of learning individual term embeddings, we
directly map all the relation triples to the hy-
perspherical embedding space such that different
types of lexical relations have diverse embeddings
in terms of angles. For example, the angle between
embeddings of (car, hypernymy, vehicle) and (car,
synonymy, auto) is large. In contrast, that of (car,
synonymy, automobile) and (car, synonymy, auto)
is small. As a result, different types of lexical rela-
tions can be distinguished. Moreover, by learning
representations of lexical relation triples explicitly,
our work addresses “lexical memorization” (Levy
et al., 2015) from a distributional aspect.

To learn SphereRE vectors for lexical relation
triples, we minimize embedding distances of term
pairs that are likely to share the same lexical rela-
tion in both labeled and unlabeled data, and max-
imize embedding distances of different lexical re-
lations. The distances in the hyperspherical space
are defined based on the angles of embeddings. In
this work, we first propose a relation-aware se-
mantic projection model to estimate probabilis-
tic distributions of lexical relations over unlabeled
data. The SphereRE vectors are efficiently learned
by Monte-Carlo techniques by transductive learn-
ing. Finally, a neural network based classifier is
trained using all the features to make the final pre-
dictions of lexical relations over all unlabeled data.

We evaluate SphereRE over four benchmark
datasets and the CogALex-V shared task (Santus
et al., 2016a), and confirm that SphereRE is highly
effective, outperforming state-of-the-art. We also
evaluate the embedding quality of SphereRE.

The rest of this paper is organized as follows.
Section 2 summarizes the related work. We
present SphereRE in Section 3. Experiments are
illustrated in Section 4, with the conclusion shown
in Section 5.

2 Related Work

We briefly overview related work on lexical rela-
tion classification and hyperspherical learning.

2.1 Lexical Relation Classification

Among all methods, path-based and distributional
approaches are two major paradigms (Shwartz and
Dagan, 2016). For hypernymy relations, Hearst
patterns (Hearst, 1992) are lexical patterns fre-
quently employed, summarized in Wang et al.

(2017a). Shwartz et al. (2016) employ an LSTM-
based neural network to learn representations of
dependency paths. Roller et al. (2018) use Hearst
pattern based statistics derived from a large text
corpus to detect hypernymy relations. For other
lexical relations, LexNET (Shwartz and Dagan,
2016) extends Shwartz et al. (2016) to classify
multiple types of lexical relations based on an in-
tegrated neural network. This type of methods re-
quires that the two terms co-occur in a sentence.
Washio and Kato (2018a) address the “low cover-
age” issue by augmenting dependency paths.

Distributional approaches employ term repre-
sentations to predict lexical relations, which ex-
ploit global contexts of terms. Traditional meth-
ods use a combination of two terms’ embeddings
as the representation, such as vector concatena-
tion (Baroni et al., 2012; Roller and Erk, 2016),
vector difference (Roller et al., 2014; Weeds et al.,
2014; Vylomova et al., 2016), etc. After that, a
classifier is trained to predict lexical relations. Al-
though distributional methods do not require the
co-occurrence of two terms, they suffer from “lex-
ical memorization” (Levy et al., 2015). It means
the algorithms only learn the properties of the two
terms, rather than the relations between them. Re-
cently, more complicated neural networks have
been proposed. Glavas and Vulic (2018) propose
a Specialization Tensor Model to discriminate be-
tween four lexical relations. The model learns dif-
ferent specializations of input distributional em-
beddings w.r.t. term pairs in order to predict dif-
ferent types of lexical relations. Attia et al. (2016)
employ a convolutional neural network in a multi-
task setting. Nguyen et al. (2016, 2017b) dis-
tinguish antonymy and synonymy via word em-
beddings and path-based neural networks. Similar
research is presented in Hashimoto et al. (2015);
Washio and Kato (2018b); Chen et al. (2018);
Bouraoui et al. (2018). A few works learn relation
embeddings for other NLP applications (Jameel
et al., 2018; Joshi et al., 2018).

Another research direction is to learn special-
izing embeddings. Yu et al. (2015); Luu et al.
(2016); Nguyen et al. (2017a); Vulic and Mrksic
(2018) (and a few others) learn hypernymy embed-
dings considering hierarchical structure of hyper-
nymy relations. For other lexical relations, Mrk-
sic et al. (2017) present the model Attract-Repel
to improve qualities of word embeddings for syn-
onymy recognition. However, they focus on one



1729

particular lexical relation, not capable of distin-
guishing multiple types of lexical relations.

2.2 Hyperspherical Learning

The work of hyperspherical learning is mostly in
computer vision. Liu et al. (2017) propose a hper-
spherical network (SphereNet) for image classi-
fication. It learns angular representations on hy-
perspheres using hyperspherical convolution units.
Wang et al. (2017c) apply the L2 hypersphere em-
bedding technique to face verification, optimiz-
ing cosine similarity for feature normalization. In
NLP, hyperspherical learning has not been exten-
sively used. Masumura et al. (2017) introduce hy-
perspherical query likelihood models for informa-
tion retrieval. Mei and Wang (2016) leverage hy-
perspherical clustering for document categoriza-
tion. Lv et al. (2018) consider sphere represen-
tations as knowledge graph embeddings. To our
knowledge, few methods employ hyperspherical
learning to learn representations for NLP applica-
tions. In our work, we focus on lexical relation
classification and present the SphereRE model to
address this problem.

3 The SphereRE Model

We introduce the Hyperspherical Relation Embed-
ding (SphereRE) model in detail.

3.1 Learning Objective

We start with some basic notations. Let D and
U be the labeled and unlabeled sets, consisting of
term pairs (xi, yi). Each pair (xi, yi) corresponds
to a pre-defined lexical relation type ri ∈ R.1 The
task of our work is to predict the lexical relation
type ri for each pair (xi, yi) ∈ U based on D.

Denote ~xi (or ~yi) as the embedding of word
xi (or yi), pre-trained using any neural language
models. For each lexical relation type rm ∈ R, we
learn a mapping function fm(~xi) that maps the re-
lation subject xi to the relation object yi in the em-
bedding space if xi and yi have the lexical relation
type rm. Hence, we aim at minimizing the objec-
tive function Jf with I(·) as the indicator function:

Jf =

|D|∑
i=1

∑
rm∈R

I(ri = rm)‖fm(~xi)− ~yi‖2

1If the dataset contains term pairs of several lexical re-
lation types, together with random, unrelated term pairs, we
consider “random” as a special lexical relation type.

To represent lexical relation triples in the (origi-
nal) embedding space, we utilize the vector differ-
ence model (Roller et al., 2014; Weeds et al., 2014;
Vylomova et al., 2016). Combining the model Jf ,
given a term pair (xi, yi) with lexical relation type
ri, the representation of the triple is fi(~xi)− ~xi.

Next, we consider the hyperspherical learning
objective. Based on the assumption in Figure 1,
we define a symmetric function g(·, ·) to quantify
the distance between two representations of lex-
ical relation triples in the SphereRE space. Fol-
lowing Roller et al. (2014); Weeds et al. (2014);
Vylomova et al. (2016), we employ the vector dif-
ference model to represent the relation embedding
of a term pair. Because we aim at learning repre-
sentations for both labeled and unlabeled data in
order to make predictions, for two pairs (xi, yi)
and (xj , yj) with lexical relation types ri and rj
((xi, yi), (xj , yj) ∈ D ∪ U ), we minimize the fol-
lowing function:

δ(ri, rj)g(fi(~xi)− ~xi, fj(~xj)− ~xj)

where δ(ri, rj) is the sign function that returns 1 if
the two pairs share the same lexical relation type
(i.e., ri = rj) and -1 otherwise. Hence, embed-
ding distances of term pairs that share the same
lexical relation type are minimized. Embedding
distances of term pairs with different lexical rela-
tion types are maximized. Refer to Figure 2 for a
geometric interpretation of the objective.

Car

Auto

Automobile

Car

Vehicle

Wheel

v(car, auto)

v(car, automobile)

Goal: Minimizing the angle Goal: Maximizing the angle

v(car, wheel)

v(car, vehicle)

Case i) Same Lexical Relation Type Case ii) Different Lexical Relation Types

Figure 2: A geometric interpretation of hyperspherical
learning. “v(car, auto)” is the embedding vector w.r.t.
the term pair “(auto, car)” (i.e., fi(~xi) − ~xi) based on
vector difference and Jf , characterizing the lexical re-
lation of the two terms in the original embedding space.
For simplicity, we use an arrow to represent the embed-
ding fi(~xi)− ~xi.

In summary, the objective function of lexical re-
lation representation learning in the hyperspheri-
cal embedding space Jg is defined as follows:

Jg =

D∪U∑
i,j

δ(ri, rj)g(fi(~xi)− ~xi, fj(~xj)− ~xj)



1730

Let Θ be all parameters in the model. The gen-
eral objective of SphereRE is defined as follows,
with λ1 and λ2 as balancing hyperparameters:

J(Θ) = Jf + λ1Jg + λ2‖Θ‖2

It is computationally intractable to minimize
J(Θ). The reasons are twofold: i) The lexical re-
lation types ri of all pairs (xi, yi) ∈ U should be
predicted before we can minimize J(Θ). ii) The
definition of Jg does not directly determine how
to generate the representations of lexical relation
triples. Additionally, minimizing J(Θ) requires
the traversal of D and U in quadratic time, lead-
ing to the high computational cost.

In the following, we present a relation-aware se-
mantic projection model as the function fm(·). It
is employed to approximate ri (for all (xi, yi) ∈
U ). Next, the representation learning process of
lexical relation triples and the lexical relation clas-
sification algorithms are introduced in detail.

3.2 Relation-aware Semantic Projection
For each pair (xi, yi) ∈ U , we approximate ri
from a probabilistic perspective, as an initial pre-
diction step. Following Wang and He (2016); Ya-
mane et al. (2016); Wang et al. (2017b), for each
lexical relation type rm ∈ R, we utilize a map-
ping matrix Mm ∈ Rd×d as fm(~xi) where d is the
dimension of pre-trained word embeddings. After
adding a Tikhonov regularizer on Mm, the learn-
ing objective function Jm w.r.t. one specific lexi-
cal relation type rm ∈ R over D can be re-written
as follows:

Jm =

|D|∑
i=1

I(ri = rm)‖Mm~xi − ~yi‖2 + µ‖Mm‖2F

Therefore, Jf =
∑

rm∈R Jm. The minimiza-
tion of Jm has a closed-form solution. The opti-
mal solution M∗m is as follows:

M∗m = arg min
Mm

Jm = (X
T
mXm + µE)

−1XTmYm

(1)
where Xm and Ym are two nm × d data matrices,
with nm being the number of term pairs that have
the lexical relation type rm ∈ R in D. The i-th
rows of Xm and Ym are the embedding vectors of
the i-th sample (xi, yi) ∈ D that has the lexical
relation type rm ∈ R. E is a d×d identity matrix.

For each lexical relation type rm ∈ R, we train
a semantic projection model based on Eq. (1). Af-

ter that, a simple lexical relation prediction clas-
sifier is trained over D based on the following
|R| × d-dimensional feature vector F(xi, yi):2

F(xi, yi) = (M1~xi − ~yi)⊕ · · · ⊕ (M|R|~xi − ~yi)

where ⊕ is the vector concatenation operator.
M1, · · · ,M|R| are projection matrices w.r.t. |R|
lexical relation types r1, · · · , r|R|.

Based on Jm, if (xi, yi) has the lexical relation
type rm, the norm of Mm~xi − ~yi is likely to be
small. On the contrary, the norms of Mn~xi −
~yi(1 ≤ n ≤ |R|, n 6= m) are likely to be large.
Therefore, the features are highly discriminative
for lexical relation classification.

For each pair (xi, yi) ∈ U , the classifier outputs
an |R|-dimensional probabilistic distribution over
all lexical relation types R. In this work, we de-
note pi,m as the probability of (xi, yi) ∈ U having
the lexical relation type rm ∈ R.

3.3 Relation Representation Learning

After we have computed the probability pi,m for
all (xi, yi) ∈ U and all rm ∈ R, we focus on the
objective Jg. The goal is to learn a dr-dimensional
vector ~ri for each (xi, yi) ∈ D ∪ U , regarded the
representation of the lexical relation triple (named
the SphereRE vector).

To avoid the high complexity and the propaga-
tion effect of predicted errors, inspired by Perozzi
et al. (2014); Grover and Leskovec (2016), we re-
formulate Jg and the function g(·, ·) via the Skip-
gram model (Mikolov et al., 2013a) over neigh-
boring graphs. Let Nb(xi, yi) be the neighbors of
a term pair (xi, yi) in the SphereRE space, where
each term pair (xj , yj) ∈ Nb(xi, yi) is likely to
share the same lexical relation type as (xi, yi). To
ensure that term pairs with the same lexical rela-
tion type have similar SphereRE vectors, the prob-
lem of optimizing Jg can be reformulated by max-
imizing the probability of predicting the neighbors
of (xi, yi) given its SphereRE vector ~ri. There-
fore, we define a new objective function J

′
g to re-

2In practice, we employ the multiclass logistic regression
model as the underlying classifier. This is because it gener-
ates well calibrated probabilistic distributions, reflecting the
model prediction confidence. In contrast, the outputs of more
complicated models such as deep neural networks are not
well calibrated. See Guo et al. (2017) for details.



1731

Condition Value of wi,j
(xi, yi) ∈ D, (xj , yj) ∈ D, ri = rj 1
(xi, yi) ∈ D, (xj , yj) ∈ D, ri 6= rj 0
(xi, yi) ∈ D, (xj , yj) ∈ U, ri = rm 12pj,m(cos(Mm~xi − ~xi,Mm~xj − ~xj) + 1)
(xi, yi) ∈ U, (xj , yj) ∈ D, rj = rm 12pi,m(cos(Mm~xi − ~xi,Mm~xj − ~xj) + 1)
(xi, yi) ∈ U, (xj , yj) ∈ U 12

∑
rm∈R pi,mpj,m · (cos(Mm~xi − ~xi,Mm~xj − ~xj) + 1)

Table 1: The choice of wi,j according to different conditions.

place Jg based on the negative log likelihood:

J
′
g =

−
∑

(xi,yi)∈D∪U

∑
(xj ,yj)∈Nb(xi,yi)

log Pr((xj , yj)|~ri)

(2)

A remaining problem is to define the neighbor-
hood Nb(xi, yi) properly, to preserve the hyper-
spherical similarity property of the distance func-
tion g(fi(~xi) − ~xi, fj(~xj) − ~xj). In this work,
we introduce a weight factor wi,j ∈ [0, 1] w.r.t.
two pairs (xi, yi) and (xj , yj) in D ∪U that quan-
tifies the similarity between the two pairs in the
SphereRE space. If (xi, yi) ∈ D and (xj , yj) ∈
D, because the true lexical relation types are
known, we simply have: wi,j = I(ri = rj).

We continue to discuss other conditions. If i)
(xi, yi) ∈ D has the lexical relation type rm, and
ii) the lexical relation type of (xj , yj) ∈ U is un-
known but is predicted to be rm with probability
pj,m, the similarity between (xi, yi) and (xj , yj)
in terms of angles is defined using the weighted
cosine similarity function in the range of (0, 1):

wi,j =
1

2
pj,m(cos(Mm~xi − ~xi,Mm~xj − ~xj) + 1)

A similar case holds for (xi, yi) ∈ U and
(xj , yj) ∈ D. If (xi, yi) ∈ U and (xj , yj) ∈ U ,
because the lexical relation types of both pairs are
unknown, we compute the weight wi,j by sum-
ming up all the weighted cosine similarities over
all possible lexical relation types in R:

wi,j =
1

2

∑
rm∈R

pi,mpj,m·

(cos(Mm~xi − ~xi,Mm~xj − ~xj) + 1)

Readers can also refer to Table 1 for a summa-
rization of the choices of wi,j .

To reduce computational complexity, we pro-
pose a Monte-Carlo based sampling and learning
method to learn SphereRE vectors based on the

values of wi,j . The algorithm is illustrated in Al-
gorithm 1. It starts with the random initialization
of SphereRE vector ~ri for each (xi, yi) ∈ D ∪ U .
An iterative process randomly selects one pair
(xi, yi) as the starting point. The next pair (xj , yj)
is selected with probability as follows:

Pr((xj , yj)|(xi, yi)) =
wi,j∑

(x
′
j ,y
′
j)∈Dmini

wi,j′

(3)
where Dmini is a mini-batch of term pairs ran-
domly selected from D ∪U . In this way, the algo-
rithm only needs to traverse |Dmini| pairs instead
of |D|+ |U | pairs. This process continues, result-
ing in a sequence of pairs, denoted as S: S =
{(x1, y1), (x2, y2), · · · , (x|S|, y|S|)}. Denote l as
the window size. We approximate J

′
g in Eq. (2) by

−
∑

(xi,yi)∈S
∑i+l

j=i−l(j 6=i) log Pr((xj , yj)|~ri) us-
ing the negative sampling training technique of the
Skip-gram model (Mikolov et al., 2013a,b).

The values of SphereRE vectors ~ri are contin-
uously updated until all the iterations stop. We
can see that ~ris are the low-dimensional represen-
tations of lexical relation triples, encoded in the
hyperspherical space. The process is shown in Al-
gorithm 1.

Algorithm 1 SphereRE Learning
1: for each (xi, yi) ∈ D ∪ U do
2: Randomly initialize SphereRE vector ~ri;
3: end for
4: for i = 1 to max iteration do
5: Sample a sequence based on Eq. (3):

S = {(x1, y1), (x2, y2), · · · , (x|S|, y|S|)};
6: Update all SphereRE vectors ~ri by minimizing

−
∑

(xi,yi)∈S
∑i+l

j=i−l(j 6=i) log Pr((xj , yj)|~ri);
7: end for

In practice, we find that there is a drawback of
the sampling process. Because the predictions for
all (xi, yi) ∈ U are probabilistic, it leads to the
situation where the algorithm prefers to choose
term pairs in D to form the sequence S . The low
sampling rate of U results in the poor represen-
tation learning quality of these pairs. Here, we
employ a boosting approach to increase chances



1732

of (xi, yi) ∈ U being selected based on stratified
sampling. The values of all probabilities pi,m are
multiplied by a factor γ > 1, i.e., pi,m ← pi,mγ. 3

3.4 Lexical Relation Classification

Finally, we train a lexical relation classifier. For
each pair (xi, yi) ∈ D, we train a classifier over
(|R| × d+ dr)-dimensional feature set F∗(xi, yi):

F∗(xi, yi) = F(xi, yi)⊕ ~ri

where F(xi, yi) are |R| × d-dimensional
projection-based features. ~ri is the SphereRE
vector of (xi, yi) that encodes the relation triple in
the SphereRE space.

We follow the work (Shwartz and Dagan, 2016)
by using a fully-connected feed-forward neural
network, shown in Figure 3. The input layer has
|R| × d + dr nodes. We add only one hidden
layer, followed by an |R|-dimensional output layer
with softmax as the prediction function. The neu-
ral network is trained using the stochastic gradient
descent algorithm, and is employed to predict the
lexical relations for all (xi, yi) ∈ U . The high-
level procedure is summarized in Algorithm 2.

… … ……… …

…

SphereRE VectorProjection-based Features w.r.t. |R| Lexical Relation Types

Hidden Layer

Output Layer

Figure 3: The neural network architecture.

Algorithm 2 Lexical Relation Classification
1: for each lexical relation type rm ∈ R do
2: Compute M∗m by Eq. (1);
3: end for
4: Train a classifier over D over F(xi, yi);
5: for each pair (xi, yi) ∈ U do
6: Predict distribution pi,m by the classifier;
7: end for
8: Learning ~ri for all (xi, yi) ∈ D ∪ U by Algorithm 1;
9: Train a neural network over D by features F∗(xi, yi);

10: for each pair (xi, yi) ∈ U do
11: Predict the lexical relation ri by the neural network;
12: end for

3Note that although we do not explicitly optimize Jg or
construct the SphereRE space directly, the SphereRE vectors
learned by Algorithm 1 (i.e., ~ri) reflect the clear distinctions
of triples with different lexical relation types. Further analy-
sis of SphereRE vectors will be shown in experiments.

4 Experiments

In this section, we conduct extensive experiments
to evaluate SphereRE and compare it with state-
of-the-art to make the convincing conclusion.

4.1 Datasets and Experimental Settings

In the experiments, we train a fastText model (Bo-
janowski et al., 2017) over the English Wikipedia
corpus to generate term embeddings. The dimen-
sionality d is set to 300. To evaluate the effec-
tiveness of SphereRE, we use four public datasets
for multi-way classification of lexical relations:
K&H+N (Necsulescu et al., 2015), BLESS (Ba-
roni and Lenci, 2011), ROOT09 (Santus et al.,
2016b) and EVALution (Santus et al., 2015). We
also evaluate SphereRE over the subtask 2 of the
CogALex-V shared task (Santus et al., 2016a).
The statistics are summarized in Table 2.

We follow the exact same experimental settings
to partition the four public datasets into training,
validation and testing sets as in (Shwartz and Da-
gan, 2016). The partition of the CogALex dataset
is the same as those in the default settings of the
CogALex-V shared task (Santus et al., 2016a).
The default settings for SphereRE are as follows:
µ = 0.001, dr = 300, |Dmini| = 20, |S| = 100,
γ = 2 and l = 3. We run Algorithm 1 in 500 iter-
ations. We also report how the changes of the neu-
ral network architecture and parameters affect the
performance over the validation sets afterwards. It
should be further noted that we do not set the val-
ues of λ1 and λ2 in the implementation because
we employ sampling based techniques to learn ~ri,
instead of directly optimizing J(Θ).

4.2 Experiments over Four Public Datasets

We report the results of SphereRE and compare it
with state-of-the-art over four public datasets.

4.2.1 General Performance
To compare SphereRE with others, we consider
following baselines:

• Concat (Baroni et al., 2012), Diff (Weeds
et al., 2014): They are classical distributional
methods using vector concatenation and vec-
tor difference as features. A neural network
without hidden layers is trained.
• NPB (Shwartz et al., 2016): It uses a path-

based LSTM neural network to classify lex-
ical relations. It is implemented by Shwartz



1733

Relation K&H+N BLESS ROOT09 EVALution CogALex
Antonym - - - 1,600 601
Attribute - 2,731 - 1,297 -
Co-hyponym 25,796 3,565 3,200 - -
Event - 3,824 - - -
Holonym - - - 544 -
Hypernym 4,292 1,337 3,190 1,880 637
Meronym 1,043 2,943 - 654 387
Random 26,378 12,146 6,372 - 5,287
Substance meronym - - - 317 -
Synonym - - - 1,086 402
All 57,509 26,546 12,762 7,378 7,314

Table 2: Statistics of all datasets. Relation names in all datasets have been mapped to relation names in WordNet.

and Dagan (2016) and only considers depen-
dency paths.
• LexNET (Shwartz and Dagan, 2016): It is

built upon Shwartz et al. (2016), which com-
bines representations of dependency paths
and word embeddings for classification.
• Concath, Diffh, LexNETh: They are variants

of Concat, Diff and LexNET, with one hidden
layer between the input and the output layer.
• NPB+Aug, LexNET+Aug (Washio and Kato,

2018a): They are variants of NPB and
LexNET. The dependency paths used in the
two original systems have been augmented in
order to improve the pattern coverage.

The results of SphereRE and all the baselines
are summarized in Table 3. We compute the Preci-
sion, Recall and F1 score for each lexical relation,
and report the average scores over all the relations,
weighted by the support. We can see that clas-
sification distributional approaches perform worse
than integrated neural networks (such as Shwartz
and Dagan (2016)), because they are not capable
of learning the true relations between terms. The
proposed approach SphereRE consistently outper-
forms all the baselines over the four datasets in
terms of F1 scores. When the type of lexical re-
lations becomes larger (e.g., EVALution), the im-
provement of SphereRE are less significant than
that of other datasets (e.g., BLESS, ROOT09).
The most possible cause is that errors induced by
relation-aware semantic projection are more likely
to propagate to subsequent steps.

4.2.2 Study on Neural Network Architectures
We adjust the neural network architecture (shown
in Figure 3) and report the performance over the
validation sets in Figure 4. As shown, adding
more hidden layers does not improve the perfor-
mance of lexical relation classification. In some
datasets (e.g., EVALution), the performance even
drops, indicating a sign of overfitting. We change

the number of hidden nodes when we use one hid-
den layer in the network. The results show that the
setting does not affect the performance greatly.

0 1 2 3 4 5
0.5

0.6

0.7

0.8

0.9

1.0

Number of hidden layers

F1
 S

co
re

K&H+N
BLESS
ROOT09
EVALution

(a) Varying #hidden layers

100 200 300 400 500

0.6

0.7

0.8

0.9

1.0

Number of nodes in the hidden layer

F1
 S

co
re

K&H+N
BLESS
ROOT09
EVALution

(b) Varying #nodes

Figure 4: Network structure analysis.

4.2.3 Study on Monte-Carlo Sampling

We continue to study how the settings of Monte-
Carlo sampling affect the quality of the SphereRE
vectors. We adjust the number of iterations and the
parameter γ. The performance is shown in Fig-
ure 5. As seen, more iterations contribute to the
higher quality of embeddings. After a sufficient
number of iterations (> 500), the performance be-
comes stable. As for the choice of γ, smaller val-
ues lead to the low sampling rates of unlabeled
data, hence lower the prediction performance. In
contrast, an overly large γ induces too many errors
in relation-aware semantic projection to the sam-
pling process. Hence, a balanced setting of γ is
required.

200 400 600 800 1000

0.6

0.7

0.8

0.9

1.0

Number of iterations

F1
 S

co
re

K&H+N
BLESS
ROOT09
EVALution

(a) Varying #iterations

1 2 3 4 5
0.5

0.6

0.7

0.8

0.9

1.0

γ

F1
 S

co
re

K&H+N
BLESS
ROOT09
EVALution

(b) Varying γ

Figure 5: MC sampling analysis.



1734

Method↓ Dataset→ K&H+N BLESS ROOT09 EVALution
Pre Rec F1 Pre Rec F1 Pre Rec F1 Pre Rec F1

Concat 0.909 0.906 0.904 0.811 0.812 0.811 0.636 0.675 0.646 0.531 0.544 0.525
Concath 0.983 0.984 0.983 0.891 0.889 0.889 0.712 0.721 0.716 0.57 0.573 0.571
Diff 0.888 0.886 0.885 0.801 0.803 0.802 0.627 0.655 0.638 0.521 0.531 0.528
Diffh 0.941 0.942 0.941 0.861 0.859 0.860 0.683 0.692 0.686 0.536 0.54 0.539
NPB 0.713 0.604 0.55 0.759 0.756 0.755 0.788 0.789 0.788 0.53 0.537 0.503
LexNET 0.985 0.986 0.985 0.894 0.893 0.893 0.813 0.814 0.813 0.601 0.607 0.6
LexNETh 0.984 0.985 0.984 0.895 0.892 0.893 0.812 0.816 0.814 0.589 0.587 0.583
NPB+Aug - - 0.897 - - 0.842 - - 0.778 - - 0.489
LexNET+Aug - - 0.970 - - 0.927 - - 0.806 - - 0.545
SphereRE 0.990 0.989 0.990 0.938 0.938 0.938 0.860 0.862 0.861 0.62 0.621 0.62
Improvement - - 0.5%↑ - - 1.1%↑ - - 4.7%↑ - - 2.0%↑

Table 3: Performance comparison of lexical relation classification over four public datasets.

Features↓ Dataset→ K&H+N BLESS ROOT09 EVALution
w/o. SphereRE vectors 0.968 0.918 0.82 0.581
w. SphereRE vectors 0.990 0.938 0.861 0.62
Improvement +2.2% +2.0% +4.1% +3.9%

Table 4: Feature analysis in terms of F1 score.

Method↓ Relation→ SYN ANT HYP MER All
Attia et al. (2016) 0.204 0.448 0.491 0.497 0.423
Shwartz and Dagan (2016) 0.297 0.425 0.526 0.493 0.445
Glavas and Vulic (2018) 0.221 0.504 0.498 0.504 0.453
SphereRE 0.286 0.479 0.538 0.539 0.471

Table 5: Performance comparison over the CogALex-
V shared task. (Due to space limitation, we only list
the performance of top systems in CogALex-V.)

4.2.4 Feature Analysis
We further study whether adding the SphereRE
vectors contributes to lexical relation classifica-
tion. We remove all the these embeddings and use
the rest of the features to make prediction based
on the same neural architecture and parameter set-
tings. The results are shown in Table 4. By learn-
ing the SphereRE vectors and adding them to the
classifier, the performance improves in all four
datasets.

4.3 Experiments over the CogALex-V Shared
Task

We evaluate SphereRE over the CogALex-V
shared task (Santus et al., 2016a), where partici-
pants are asked to classify 4,260 term pairs into
5 lexical relations: synonymy, antonymy, hyper-
nymy, meronymy and random. The training set
contains 3,054 pairs. This task is the most chal-
lenging because i) it considers random relations as
noise, discarding it from the averaged F1 score; ii)
the training set is small; and iii) it enforces lexi-
cal spilt of the training and testing sets, disabling
“lexical memorization” (Levy et al., 2015).

In this shared task, GHHH (Attia et al., 2016)
and LexNET (Shwartz and Dagan, 2016) are top-

two systems with the highest performance. The
most recent work on CogALex-V is STM (Glavas
and Vulic, 2018). SphereRE achieves the aver-
aged F1 score of 47.1% (excluding the random
relations), outperforming state-of-the-art. Addi-
tionally, as reported in previous studies, the “lex-
ical memorization” effect (Levy et al., 2015) is
rather severe for hypernymy relations. Although
SphereRE is fully distributional, it achieves the
highest F1 score of 53.8%.

4.4 Analysis of SphereRE Vector Qualities

We conduct additional experiments to evaluate the
qualities of Sphere vectors. The first set of exper-
iments evaluates whether top-k most similar re-
lation triples of a given relation triple share the
same lexical relation type. This task is called top-
k similar lexical relation retrieval. In this task, the
similarity between two relation triples is quanti-
fied by the cosine similarity of the two correspond-
ing SphereRE vectors. The score is reported by
Precision@k. Higher Precision@k scores indicate
SphereRE vectors with better quality, because lex-
ical relation triples with the same lexical relation
type should have similar Sphere vectors. In the ex-
periments, we compute the Precision@k over all
the labeled (training) and unlabeled (testing) sets
of all five datasets. The results are shown in Ta-
ble 6 in terms of Average Precision@k (AP@k)
(with k = 1, 5, 10).

As seen, SphereRE has near perfect perfor-
mance (over 95% for AP@1, over 90% for AP@5
and AP@10) over training sets of all five datasets.
This is because in representation learning, all the
labels (i.e., lexical relation types) of these term
pairs are already known. Hence, SphereRE pre-
serves distributional characteristics of these la-
beled datasets well. As for unlabeled datasets, the
performance drops slightly over K&H+N, BLESS
and ROOT09. The performance is not very satis-



1735

(a) ROOT09 (Training) (b) ROOT09 (Testing) (c) EVALution (Training) (d) EVALution (Testing)

Figure 6: Visualization of SphereRE vectors by t-SNE (Maaten and Hinton, 2008).

Dataset AP@1 AP@5 AP@10 AP@1 AP@5 AP@10
Training Set Testing Set

K&H+N 0.972 0.954 0.951 0.862 0.844 0.839
BLESS 0.962 0.950 0.948 0.868 0.830 0.825
ROOT09 0.987 0.993 0.989 0.814 0.789 0.828
EVALution 0.988 0.987 0.982 0.653 0.650 0.697
CogALex 0.953 0.904 0.918 0.631 0.628 0.649

Table 6: Performance of top-k similar relation retrieval over five datasets in terms of Average Precision@k.

Term Pairs Predicted Relation True Relation
(heart, courage) Random Synonym
(wing, animal) Random Meronym
(mint, pennyroyal) Random Hypernym
(handlebar, bike) Co-hyponym Meronym
(grenade, object) Attribute Hypernym

Table 7: Cases of prediction errors. All the relation
names are mapped to relation names in WordNet.

factory over EVALution and CogALex, due to the
internal challenges of lexical relation classification
over the two datasets. This is because they contain
a relatively large number of lexical relation types
and random, unrelated term pairs.

To have a more intuitive understanding of these
learned SphereRE vectors, we plot the embed-
dings in Figure 6 by t-SNE (Maaten and Hin-
ton, 2008). Due to space limitation, we only plot
SphereRE vectors in part of the training and test-
ing sets from ROOT09 and EVALution. For train-
ing data, we can see a clear separation of different
lexical relation types. The slight “messiness” w.r.t.
testing data indicates learning errors.

4.5 Error Analysis
For error analysis, we randomly sample 300 cases
of prediction errors and ask human annotators to
analyze the most frequent causes. We present sev-
eral cases in Table 7. The largest number of er-
rors (approximately 42%) occur due to the ran-
dom relations in K&H+N, BLESS, ROOT09 and
CogALex. These relations are large in quantity
and blurry in semantics, misleading the classifier
to predict other lexical relations as random.

Another large proportion of errors (about 31%)

are related to unbalanced ratio of relations (apart
from random). The number of some types of lexi-
cal relation triples in the training set is small (e.g.,
Meronym in EVALution, Synonym in CogALex).
As a result, the representation learning w.r.t. these
relation triples is relatively of lower quality.

5 Conclusion and Future Work

In this paper, we present a representation learning
model to distinguish lexical relations based on Hy-
perspherical Relation Embeddings (SphereRE). It
learns representations of lexical relation triples by
mapping them to the hyperspherical embedding
space. The lexical relations between term pairs are
predicted using neural networks over the learned
embeddings. Experiments over four benchmark
datasets and CogALex-V show SphereRE outper-
forms state-of-the-art methods.

In the future, we will improve our model to
deal with datasets containing a relatively large
number of lexical relation types and random term
pairs. Additionally, the mapping technique used
for relation-aware semantic projection can be fur-
ther improved to model different linguistic prop-
erties of lexical relations (e.g., the “one-to-many”
mappings for meronymy).

Acknowledgements

This work is supported by the National Key Re-
search and Development Program of China under
Grant No. 2016YFB1000904.



1736

References
Mohammed Attia, Suraj Maharjan, Younes Samih,

Laura Kallmeyer, and Thamar Solorio. 2016.
Cogalex-v shared task: GHHH - detecting se-
mantic relations via word embeddings. In Co-
gALex@COLING, pages 86–91.

Marco Baroni, Raffaella Bernardi, Ngoc-Quynh Do,
and Chung-chieh Shan. 2012. Entailment above the
word level in distributional semantics. In EACL,
pages 23–32.

Marco Baroni and Alessandro Lenci. 2011. How
we blessed distributional semantic evaluation. In
GEMS, pages 1–10.

Piotr Bojanowski, Edouard Grave, Armand Joulin, and
Tomas Mikolov. 2017. Enriching word vectors with
subword information. TACL, 5:135–146.

Zied Bouraoui, Shoaib Jameel, and Steven Schockaert.
2018. Relation induction in word embeddings revis-
ited. In COLING, pages 1627–1637.

Hong-You Chen, Cheng-Syuan Lee, Keng-Te Liao,
and Shou-de Lin. 2018. Word relation autoencoder
for unseen hypernym extraction using word embed-
dings. In EMNLP, pages 4834–4839.

Goran Glavas and Ivan Vulic. 2018. Discriminating
between lexico-semantic relations with the special-
ization tensor model. In NAACL.

Aditya Grover and Jure Leskovec. 2016. node2vec:
Scalable feature learning for networks. In KDD,
pages 855–864.

Chuan Guo, Geoff Pleiss, Yu Sun, and Kilian Q. Wein-
berger. 2017. On calibration of modern neural net-
works. In ICML, pages 1321–1330.

Kazuma Hashimoto, Pontus Stenetorp, Makoto Miwa,
and Yoshimasa Tsuruoka. 2015. Task-oriented
learning of word embeddings for semantic relation
classification. In CoNLL, pages 268–278.

Marti A. Hearst. 1992. Automatic acquisition of hy-
ponyms from large text corpora. In COLING.

Diana Inkpen, Xiaodan Zhu, Zhen-Hua Ling, Qian
Chen, and Si Wei. 2018. Neural natural language in-
ference models enhanced with external knowledge.
In ACL, pages 2406–2417.

Shoaib Jameel, Zied Bouraoui, and Steven Schockaert.
2018. Unsupervised learning of distributional rela-
tion vectors. In ACL, pages 23–33.

Mandar Joshi, Eunsol Choi, Omer Levy, Daniel S.
Weld, and Luke Zettlemoyer. 2018. pair2vec: Com-
positional word-pair embeddings for cross-sentence
inference. CoRR, abs/1810.08854.

Omer Levy, Steffen Remus, Chris Biemann, and Ido
Dagan. 2015. Do supervised distributional methods
really learn lexical inference relations? In NAACL,
pages 970–976.

Weiyang Liu, Yan-Ming Zhang, Xingguo Li, Zhen Liu,
Bo Dai, Tuo Zhao, and Le Song. 2017. Deep hyper-
spherical learning. In NIPS, pages 3953–3963.

Anh Tuan Luu, Yi Tay, Siu Cheung Hui, and See-Kiong
Ng. 2016. Learning term embeddings for taxonomic
relation identification using dynamic weighting neu-
ral network. In EMNLP, pages 403–413.

Xin Lv, Lei Hou, Juanzi Li, and Zhiyuan Liu. 2018.
Differentiating concepts and instances for knowl-
edge graph embedding. In EMNLP, pages 1971–
1979.

Laurens Van Der Maaten and Geoffrey Hinton. 2008.
Visualizing data using t-sne. JMLR, 9(2605):2579–
2605.

Ryo Masumura, Taichi Asami, Hirokazu Masataki, Ku-
gatsu Sadamitsu, Kyosuke Nishida, and Ryuichiro
Higashinaka. 2017. Hyperspherical query likeli-
hood models with word embeddings. In IJCNLP,
pages 210–216.

Jian-Ping Mei and Yangtao Wang. 2016. Hyperspheri-
cal fuzzy clustering for online document categoriza-
tion. In FUZZ-IEEE, pages 1487–1493.

Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey
Dean. 2013a. Efficient estimation of word represen-
tations in vector space. CoRR, abs/1301.3781.

Tomas Mikolov, Ilya Sutskever, Kai Chen, Gregory S.
Corrado, and Jeffrey Dean. 2013b. Distributed rep-
resentations of words and phrases and their compo-
sitionality. In NIPS, pages 3111–3119.

George A. Miller. 1995. Wordnet: A lexical database
for english. Commun. ACM, 38(11):39–41.

Nikola Mrksic, Ivan Vulic, Diarmuid Ó Séaghdha, Ira
Leviant, Roi Reichart, Milica Gasic, Anna Korho-
nen, and Steve J. Young. 2017. Semantic special-
ization of distributional word vector spaces using
monolingual and cross-lingual constraints. TACL,
5:309–324.

Silvia Necsulescu, Sara Mendes, David Jurgens, Núria
Bel, and Roberto Navigli. 2015. Reading between
the lines: Overcoming data sparsity for accurate
classification of lexical relationships. In *SEM.

Kim Anh Nguyen, Maximilian Köper, Sabine Schulte
im Walde, and Ngoc Thang Vu. 2017a. Hierarchical
embeddings for hypernymy detection and direction-
ality. In EMNLP, pages 233–243.

Kim Anh Nguyen, Sabine Schulte im Walde, and
Ngoc Thang Vu. 2016. Integrating distributional
lexical contrast into word embeddings for antonym-
synonym distinction. In ACL.

Kim Anh Nguyen, Sabine Schulte im Walde, and
Ngoc Thang Vu. 2017b. Distinguishing antonyms
and synonyms in a pattern-based neural network. In
EAL, pages 76–85.



1737

Bryan Perozzi, Rami Al-Rfou, and Steven Skiena.
2014. Deepwalk: online learning of social repre-
sentations. In KDD, pages 701–710.

Stephen Roller and Katrin Erk. 2016. Relations such
as hypernymy: Identifying and exploiting hearst pat-
terns in distributional vectors for lexical entailment.
In EMNLP, pages 2163–2172.

Stephen Roller, Katrin Erk, and Gemma Boleda. 2014.
Inclusive yet selective: Supervised distributional hy-
pernymy detection. In COLING, pages 1025–1036.

Stephen Roller, Douwe Kiela, and Maximilian Nickel.
2018. Hearst patterns revisited: Automatic hyper-
nym detection from large text corpora. In ACL,
pages 358–363.

Enrico Santus, Anna Gladkova, Stefan Evert, and
Alessandro Lenci. 2016a. The cogalex-v shared task
on the corpus-based identification of semantic rela-
tions. In CogALex@COLING, pages 69–79.

Enrico Santus, Alessandro Lenci, Tin-Shing Chiu, Qin
Lu, and Chu-Ren Huang. 2016b. Nine features in
a random forest to learn taxonomical semantic rela-
tions. In LREC.

Enrico Santus, Frances Yung, Alessandro Lenci, and
Chu-Ren Huang. 2015. Evalution 1.0: an evolving
semantic dataset for training and evaluation of dis-
tributional semantic models. In LDL@IJCNLP.

Jiaming Shen, Zeqiu Wu, Dongming Lei, Chao Zhang,
Xiang Ren, Michelle T. Vanni, Brian M. Sadler, and
Jiawei Han. 2018. Hiexpan: Task-guided taxon-
omy construction by hierarchical tree expansion. In
KDD, pages 2180–2189.

Vered Shwartz and Ido Dagan. 2016. Path-based vs.
distributional information in recognizing lexical se-
mantic relations. In CogALex@COLING.

Vered Shwartz, Yoav Goldberg, and Ido Dagan. 2016.
Improving hypernymy detection with an integrated
path-based and distributional method. In ACL.

Robert Speer, Joshua Chin, and Catherine Havasi.
2017. Conceptnet 5.5: An open multilingual graph
of general knowledge. In AAAI, pages 4444–4451.

Ivan Vulic and Nikola Mrksic. 2018. Specialising word
vectors for lexical entailment. In NAACL, pages
1134–1145.

Ekaterina Vylomova, Laura Rimell, Trevor Cohn, and
Timothy Baldwin. 2016. Take and took, gaggle and
goose, book and read: Evaluating the utility of vec-
tor differences for lexical relation learning. In ACL.

Chengyu Wang and Xiaofeng He. 2016. Chinese
hypernym-hyponym extraction from user generated
categories. In COLING, pages 1350–1361.

Chengyu Wang, Xiaofeng He, and Aoying Zhou.
2017a. A short survey on taxonomy learning from
text corpora: Issues, resources and recent advances.
In EMNLP, pages 1190–1203.

Chengyu Wang, Junchi Yan, Aoying Zhou, and Xi-
aofeng He. 2017b. Transductive non-linear learn-
ing for chinese hypernym prediction. In ACL, pages
1394–1404.

Feng Wang, Xiang Xiang, Jian Cheng, and Alan Lod-
don Yuille. 2017c. Normface: L2 hypersphere em-
bedding for face verification. In ACM MM.

Koki Washio and Tsuneaki Kato. 2018a. Filling miss-
ing paths: Modeling co-occurrences of word pairs
and dependency paths for recognizing lexical se-
mantic relations. In NAACL, pages 1123–1133.

Koki Washio and Tsuneaki Kato. 2018b. Neural latent
relational analysis to capture lexical semantic rela-
tions in a vector space. In EMNLP, pages 594–600.

Julie Weeds, Daoud Clarke, Jeremy Reffin, David J.
Weir, and Bill Keller. 2014. Learning to distinguish
hypernyms and co-hyponyms. In COLING, pages
2249–2259.

Josuke Yamane, Tomoya Takatani, Hitoshi Yamada,
Makoto Miwa, and Yutaka Sasaki. 2016. Distribu-
tional hypernym generation by jointly learning clus-
ters and projections. In COLING, pages 1871–1879.

Shuo Yang, Lei Zou, Zhongyuan Wang, Jun Yan, and
Ji-Rong Wen. 2017. Efficiently answering technical
questions - A knowledge graph approach. In AAAI,
pages 3111–3118.

Zheng Yu, Haixun Wang, Xuemin Lin, and Min Wang.
2015. Learning term embeddings for hypernymy
identification. In IJCAI, pages 1390–1397.

Wen Zhang, Jiawei Hu, Yang Feng, and Qun Liu. 2018.
Refining source representations with relation net-
works for neural machine translation. In COLING,
pages 1292–1303.


