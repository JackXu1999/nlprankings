



















































Entity Hierarchy Embedding


Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics
and the 7th International Joint Conference on Natural Language Processing, pages 1292–1300,

Beijing, China, July 26-31, 2015. c©2015 Association for Computational Linguistics

Entity Hierarchy Embedding

Zhiting Hu, Poyao Huang, Yuntian Deng, Yingkai Gao, Eric P. Xing
School of Computer Science
Carnegie Mellon University
Pittsburgh, PA 15213, USA

{zhitingh,poyaoh,yuntiand,yingkaig,epxing}@cs.cmu.edu

Abstract

Existing distributed representations are
limited in utilizing structured knowledge
to improve semantic relatedness modeling.
We propose a principled framework of em-
bedding entities that integrates hierarchi-
cal information from large-scale knowl-
edge bases. The novel embedding model
associates each category node of the hi-
erarchy with a distance metric. To cap-
ture structured semantics, the entity sim-
ilarity of context prediction are measured
under the aggregated metrics of relevant
categories along all inter-entity paths. We
show that both the entity vectors and cat-
egory distance metrics encode meaningful
semantics. Experiments in entity linking
and entity search show superiority of the
proposed method.

1 Introduction

There has been a growing interest in distributed
representation that learns compact vectors (a.k.a
embedding) for words (Mikolov et al., 2013a),
phrases (Passos et al., 2014), and concepts (Hill
and Korhonen, 2014), etc. The induced vectors
are expected to capture semantic relatedness of
the linguistic items, and are widely used in senti-
ment analysis (Tang et al., 2014), machine transla-
tion (Zhang et al., 2014), and information retrieval
(Clinchant and Perronnin, 2013), to name a few.

Despite the impressive success, existing work
is still limited in utilizing structured knowledge
to enhance the representation. For instance, word
and phrase embeddings are largely induced from
plain text. Though recent knowledge graph em-
beddings (Lin et al., 2015; Wang et al., 2014) inte-
grate the relational structure among entities, they
primarily target at link prediction and lack an ex-
plicit relatedness measure.

In this paper, we propose to improve the dis-
tributed representations of entities by integrating
hierarchical information from large-scale knowl-
edge bases (KBs). An entity hierarchy groups en-
tities into categories which are further organized
to form a taxonomy. It provides rich structured
knowledge on entity relatedness (Resnik, 1995).
Our work goes beyond the previous heuristic use
of entity hierarchy which relies on hand-crafted
features (Kaptein and Kamps, 2013; Ponzetto
and Strube, 2007), and develops a principled
optimization-based framework. We learn a dis-
tance metric for each category node, and mea-
sure entity-context similarity under the aggregat-
ed metrics of all relevant categories. The met-
ric aggregation encodes the hierarchical property
that nearby entities tend to share common seman-
tic features. We further provide a highly-efficient
implementation in order to handle large complex
hierarchies.

We train a distributed representation for the w-
hole entity hierarchy of Wikipedia. Both the entity
vectors and the category distance metrics capture
meaningful semantics. We deploy the embedding
in both entity linking (Han and Sun, 2012) and
entity search (Demartini et al., 2010) tasks. Hi-
erarchy embedding significantly outperforms that
without structural knowledge. Our methods also
show superiority over existing competitors.

To the best of our knowledge, this is the first
work to learn distributed representations that in-
corporates hierarchical knowledge in a principled
framework. Our model that encodes hierarchy by
distance metric learning and aggregation provides
a potentially important and general scheme for u-
tilizing hierarchical knowledge.

The rest of the paper is organized as follows:
§2 describes the proposed embedding model; §3
presents the application of the learned embed-
ding; §4 evaluates the approach; §5 reviews related
work; and finally, §6 concludes the paper.

1292



e1 
...e2….e3… 
……….. 

Text Context 

e1 e2 

e3 

Entity Hierarchy 

h2 

h1 
𝑀ℎ1 

𝑀ℎ2 

𝑣𝑒1 

Entity pairs 

  𝑀𝑒1.𝑒3  = 𝜋ℎ1𝑀ℎ1 + 𝜋ℎ2𝑀ℎ2 𝑣𝑒2 

𝑣𝑒3 

 (𝑒1,   𝑒3 ) 

𝑒1,  𝑒2   

Dist. metrics 

𝑀𝑒1,𝑒2 = 𝑀ℎ2 

Figure 1: The model architecture. The text context of an entity is based on its KB encyclopedia article.
The entity hierarchical structure is incorporated through distance metric learning and aggregation.

2 Entity Hierarchy Embedding

The objective of the embedding model is to find
a representation for each entity that is useful for
predicting other entities occurring in its contex-
t. We build entity’s context upon KB encyclope-
dia articles, where entity annotations are readily
available. We further incorporate the entity hierar-
chical structure in the context prediction through
distance metric learning and aggregation, which
encodes the rich structured knowledge in the in-
duced representations. Our method is flexible and
efficient to model large complex DAG-structured
hierarchies. Figure 1 shows an overview of the
model architecture.

2.1 Model Architecture

Our architecture builds on the skip-gram word
embedding framework (Mikolov et al., 2013b).
In the skip-gram model, a set of (target, con-
text) word pairs are extracted by sliding a fixed-
length context window over a text corpus, and the
word vectors are learned such that the similarity
of the target- and context-word vectors is maxi-
mized. We generalize both the context definition
and the similarity measure for entity hierarchy em-
bedding.

Unlike words that can be directly extracted from
plain text, entities are hidden semantics underly-
ing their surface forms. In order to avoid manual
annotation cost, we exploit the text corpora from
KBs where the referent entities of surface text are
readily annotated. Moreover, since a KB encyclo-
pedia article typically focuses on describing one
entity, we naturally extend the entity’s context as
its whole article, and obtain a set of entity pairs
D = {(eT , eC)}, where eT denotes the target-
entity and eC denotes the context-entity occurring
in entity eT ’s context.

Let E be the set of entities. For each entity e ∈

E , the model learns both a “target vector” ve ∈ Rn
and “context vector” v̄e ∈ Rn, by maximizing the
training objective

L = 1|D|
∑

(eT ,eC)∈D
log p(eC |eT ), (1)

where the prediction probability is defined as a
softmax:

p(eC |eT ) = exp {−d (eT , eC)}∑
e∈E exp {−d (eT , e)}

. (2)

Here d(e, e′) is the distance between the target
vector of e (i.e., ve) and the context vector of e′

(i.e., v̄e′). We present the design in the following.

2.2 Hierarchical Extension
An entity hierarchy takes entities as leaf nodes and
categories as internal nodes, which provides key
knowledge sources on semantic relatedness that 1)
far-away entities in the hierarchy tend to be se-
mantically distant, and 2) nearby entities tend to
share common semantic features. We aim to en-
code this knowledge in our representations. As K-
B hierarchies are large complex DAG structures,
we develop a highly-efficient scheme to enable
practical training.

Specifically, we associate a separate distance
metric Mh ∈ Rn×n with each category h in the
hierarchy. A distance metric is a positive semidef-
inite (PSD) matrix. We then measure the distance
between two entities under some aggregated dis-
tance metric as detailed below. The local metrics
thus not only serve to capture the characteristics
of individual categories, but also make it possible
to share the representation across entities through
metric aggregation of relevant categories.

Metric aggregation Given two entities e and e′,
let Pe,e′ be the path between them. One obvious
way to define the aggregated metricMe,e′ ∈ Rn×n
is through a combination of the metrics on the

1293



e 

h2 

h1 

h3 h4 
e' 

𝑒 → 𝑒′path 1 

𝑒 → 𝑒′path 2 

turning node 

Figure 2: Paths in a DAG-structured hierarchy. A
path P is defined as a sequence of non-duplicated
nodes with the property that there exists a turning
node t ∈ P such that any two consecutive nodes
before t are (child, parent) pairs, while consecu-
tive nodes after t are (parent, child) pairs. Thus a
turning node is necessarily a common ancestor.

path:
∑

h∈Pe,e′ Mh, leading to a nice property that
the more nodes a path has, the more distant the en-
tities tend to be (as Mh is PSD). This simple strat-
egy, however, can be problematic when the hier-
archy has a complex DAG structure, in that there
can be multiple paths between two entities (Fig-
ure 2). Though the shortest path can be selected, it
ignores other related category nodes and loses rich
information. In contrast, an ideal scheme should
not only mirror the distance in the hierarchy, but
also take into account all possible paths in order to
capture the full aspects of relatedness.

However, hierarchies in large KBs can be com-
plex and contains combinationally many paths be-
tween any two entities. We propose an efficien-
t approach that avoids enumerating paths and in-
stead models the underlying nodes directly. In par-
ticular, we extend Pe,e′ as the set of all category
nodes included in any of the e → e′ paths, and
define the aggregate metric as

Me,e′ = γe,e′
∑

h∈Pe,e′
πee′,hMh, (3)

where {πee′,h} are the relative weights of the cat-
egories such that

∑
h∈Pe,e′ πee′,h = 1. This serves

to balance the size of P across different entity
pairs. We set πee′,h ∝ ( 1sh↓e + 1sh↓e′ ) with sh↓e
being the average #steps going down from node
h to node e in the hierarchy (infinite if h is not
an ancestor of e). This implements the intuition
that an entity (e.g., “Iphone”) is more relevant to
its immediate categories (e.g., “Mobile phones”)
than to farther and more generic ancestors (e.g.,
“Technology”). The scaling factor γe,e′ encodes
the distance of the entities in the hierarchy and can

be of various choices. We set γe,e′ = minh{sh↓e+
sh↓e′} to mirror the least common ancestor.

In Figure 2, Pe,e′ = {h2, h3, h4}, and the rel-
ative weights of the categories are πee′,h2 ∝ 3/2
and πee′,h3 = πee′,h4 ∝ 1. Category h2 is the least
common ancestor and γe,e′ = 3.

Based on the aggregated metric, the distance be-
tween a target entity eT and a context entity eC can
then be measured as

d (eT , eC) = (veT − v̄eC )>MeT ,eC (veT − v̄eC ). (4)

Note that nearby entities in the hierarchy tend to
share a large proportion of local metrics in Eq 3,
and hence can exhibit common semantic features
when measuring distance with others.

Complexity of aggregation As computing dis-
tance is a frequent operation in both training and
application stages, a highly efficient aggregation
algorithm is necessary in order to handle complex
large entity hierarchies (with millions of nodes).
Our formulation (Eq 3) avoids exhaustive enumer-
ation over all paths by modeling the relevant nodes
directly. We show that this allows linear complex-
ity in the number of children of two entities’ com-
mon ancestors, which is efficient in practice.

The most costly operation is to find Pe,e′ , i.e.,
the set of all category nodes that can occur in any
of e→ e′ paths. We use a two-step procedure that
(1) finds all common ancestors of entity e and e′

that are turning nodes of any e → e′ paths (e.g.,
h2 in Figure 2), denoted asQe,e′ ; (2) expands from
Qe,e′ to construct the full Pe,e′ . For the first step,
the following theorem shows each common ances-
tor can be efficiently assessed by testing only its
children nodes. For the second step, it is straight-
forward to see that Pe,e′ can be constructed by ex-
panding Qe,e′ with its descendants that are ances-
tors of either e or e′. Other parameters (πee′ and
γe,e′) of aggregation can be computed during the
above process.

We next provide the theorem for the first step.
LetAe be the ancestor nodes of entity e (including
e itself). For a node h ∈ Ae ∪ Ae′ , we define
its critical node th as the nearest (w.r.t the length
of the shortest path) descendant of h (including h
itself) that is in Qe,e′ ∪ {e, e′}. E.g., in Figure 2,
th1 = h2; th2 = h2; th3 = e. Let Ch be the set of
immediate child nodes of h.

Theorem 1. ∀h ∈ Ae ∩Ae′ , h ∈ Qe,e′ iff it satis-
fies the two conditions: (1) |Ch∩(Ae ∪ Ae′) | ≥ 2;
(2) ∃a, b ∈ Ch s.t. ta 6= tb.

1294



Proof. We outline the proof here, and provide the
details in the appendix.

Sufficiency: Note that e, e′ /∈ Qe,e′ . We prove
the sufficiency by enumerating possible situation-
s: (i) ta = e, tb = e′; (ii) ta = e, tb ∈ Qe,e′ ;
(iii) ta, tb ∈ Qe,e′ . For (i): as ta = e, there exists
a path e → · · · → a → h where any two con-
secutive nodes is a (child, parent) pair. Similarly,
there is a path h → b → · · · → e′ where any t-
wo consecutive nodes is a (parent, child) pair. It
is provable that the two paths intersect only at h,
and thus can be combined to form an e→ e′ path:
e → · · · → a → h → b → · · · → e′, yielding h
as a turning node. The cases (ii) and (iii) can be
proved similarly.

Necessity: We prove by contradiction. Suppose
that ∀a, b ∈ Ch ∩ (Ae ∪ Ae′) we have ta = tb.
W.l.o.g. we consider two cases: (i) ta = tb = e,
and (ii) ta = tb ∈ Qe,e′ . It is provable that both
cases will lead to contradiction.

Therefore, by checking common ancestors from
the bottom up, we can construct Qe,e′ with time
complexity linear to the number of all ancestors’
children.

2.3 Learning

For efficiency, we use negative sampling to refor-
mulate the training objective, which is then opti-
mized through coordinate gradient ascent.

Specifically, given the training data {(eT , eC)}
extracted from KB corpora, the representation
learning is formulated as maximizing the objec-
tive in Eq 1, subject to PSD constraints on distance
metrics Mh � 0, and ‖ve‖2 = ‖v̄e‖2 = 1 to avoid
scale ambiguity.

The likelihood of each data sample is defined
as a softmax in Eq 2, which iterates over all en-
tities in the denominator and is thus computation-
ally prohibitive. We apply the negative sampling
technique as in conventional skip-gram model, by
replacing each log probability log p(eC |eT ) with

log σ(−d (eT , eC)) +
∑k

i=1
Eei∼P (e) [log σ(−d (eT , ei))] ,

where σ(x) = 1/(1 + exp(−x)) is the sigmoid
function; and for each data sample we draw k neg-
ative samples from the noise distribution P (e) ∝
U(e)3/4 with U(e) being the unigram distribution
(Mikolov et al., 2013b).

The negative sampling objective is optimized
using coordinate gradient ascent, as shown in Al-

gorithm 1. To avoid overfitting and improve effi-
ciency, in practice we restrict the distance metrics
Mh to be diagonal (Xing et al., 2002). Thus the
PSD project of Mh (line 17) is simply taking the
positive part for each diagonal elements.

Algorithm 1 Entity Hierarchy Embedding
Input: The training data D = {(eT , eC)},

Entity hierarchy,
Parameters: n – dimension of the embedding

k – number of negative samples
η – gradient learning rate
B – minibatch size

1: Initialize v, v̄,M randomly such that ‖v‖2 = ‖v̄‖2 = 1
and M � 0.

2: repeat
3: Sample a batch B = {(eT , eC)i}Bi=1 from D
4: for all (eT , eC) ∈ B do
5: Compute {P,π, γ}eT ,eC for metric aggregation
6: Sample negative pairs {(eT , ei)}ki=1
7: Compute {{P,π, γ}eT ,ei}ki=1 for metric aggrega-

tion
8: end for
9: repeat

10: for all e ∈ E included in B do
11: ve = ve + η ∂L∂ve
12: v̄e = v̄e + η ∂L∂v̄e
13: ve, v̄e = Project to unit sphere(ve, v̄e)
14: end for
15: until convergence
16: repeat
17: for all h included in B do
18: Mh = Mh + η ∂L∂Mh
19: Mh = Project to PSD(Mh)
20: end for
21: until convergence
22: until convergence
Output: Entity vectors v, v̄, and category dist. metricsM

3 Applications

One primary goal of learning semantic embedding
is to improve NLP tasks. The compact represen-
tations are easy to work with because they en-
able efficient computation of semantic relatedness.
Compared to word embedding, entity embedding
is particularly suitable for various language under-
standing applications that extract underlying se-
mantics of surface text. Incorporating entity hier-
archies further enriches the embedding with struc-
tured knowledge.

In this section, we demonstrate how the learned
entity hierarchy embedding can be utilized in t-
wo important tasks, i.e., entity linking and enti-
ty search. In both tasks, we measure the seman-
tic relatedness between entities as the reciprocal
distance defined in Eq 4. This greatly simpli-
fies previous methods which have used various
hand-crafted features, and leads to improved per-
formance as shown in our experiments.

1295



3.1 Entity Linking

The entity linking task is to link surface form-
s (mentions) of entities in a document to entities
in a reference KB. It is an essential first step for
downstream tasks such as semantic search and K-
B construction. The quality of entity relatedness
measure is critical for entity linking performance,
because of the key observation that entities in a
document tend to be semantically coherent. For
example, in sentence “Apple released an operating
system Lion”, The mentions “Apple” and “Lion”
refer to Apple Inc. and Mac OS X Lion, respec-
tively, as is more coherent than other configura-
tions like (fruit apple, animal lion).

Our algorithm finds the optimal configuration
for the mentions of a document by maximizing
the overall relatedness among assigned entities, to-
gether with the local mention-to-entity compatibil-
ity. Specifically, we first construct a mention-to-
entity dictionary based on Wikipedia annotations.
For each mention m, the dictionary contains a set
of candidate entities and for each candidate entity
e a compatibility score P (e|m) which is propor-
tional to the frequency that m refers to e. For effi-
ciency we only consider the top-5 candidate enti-
ties according to P (e|m). Given a set of mentions
M = {mi}Mi=1 in a document, let A = {emi}Mi=1
be a configuration of its entity assignments. The
score of A is formulated as probability

P (A|M) ∝
∏M

i=1
P (emi |mi)

∑M
j=1
j 6=i

1

d
(
emi , emj

)
+ �

,

where for each entity assignment we define its
global relatedness to other entity assignments as
the sum of the reciprocal distances (� = 0.01 is
a constant used to avoid divide-by-zero). Direct
enumeration of all potential configurations is com-
putationally prohibitive, we therefore use simulat-
ed annealing to search for an optimal solution.

3.2 Entity Search

Entity search has attracted a growing interest
(Chen et al., 2014b; Balog et al., 2011). Unlike
conventional web search that finds unorganized
web pages, entity search retrieves knowledge di-
rectly by generating a list of relevant entities in re-
sponse to a search request. The input of the entity
search task is a natural language questionQ along
with one or more desired entity categories C. For
example, a query can be Q =“films directed by
Akira Kurosawa” and C ={Japanese films}.

Previous methods typically score candidate en-
tities by measuring both the similarity between en-
tity content and the query question Q (text match-
ing), and the similarity between categories of en-
tities and the query categories C (category match-
ing).

We apply a similar category matching strate-
gy as in previous work (Chen et al., 2014b) that
assesses lexical (e.g., head words) similarity be-
tween category names, while replacing the text
matching with entity relatedness measure. Specif-
ically, we first extract the underlying entities men-
tioned inQ through entity linking, then score each
candidate entity by its average relatedness to the
entities in Q. For instance, the entity Rashomon
will obtain a high score in the above example as
it is highly related with the entity Akira Kurosawa
in the query. This scheme not only avoids complex
document processing (e.g., topic modeling) in tex-
t matching, but also implicitly augments the short
query text with background knowledge, and thus
improves the accuracy and robustness.

4 Experiments

We validate the quality of our entity representa-
tion by evaluating its applications of entity linking
and entity search on public benchmarks. In the
entity linking task, our approach improves the F1
score by 10% over state-of-the-art results. We al-
so validate the advantage of incorporating hierar-
chical structure. In the entity search task, our sim-
ple algorithm shows competitive performance. We
further qualitatively analyze the entity vectors and
category metrics, both of which capture meaning-
ful semantics, and can potentially open up a wide
rage of other applications.

Knowledge base We use the Wikipedia snap-
shot from Jan 12nd, 2015 as our training data and
KB. After pruning administrative information we
obtain an entity hierarchy including about 4.1M
entities and 0.8M categories organized into 12
layers. Loops in the original hierarchy are re-
moved by deleting bottom-up edges, yielding a
DAG structure. We extract a set of 87.6M entity
pairs from the wiki links on Wikipedia articles.

We train 100-dimensional vector represen-
tations for the entities and distance metrics
(100×100 diagonal matrixes) for the categories
(we would study the impact of dimensionality in
the future). We set the batch size B = 500, the
initial learning rate η = 0.1 and decrease it by a

1296



factor of 5 whenever the objective value does not
increase, and the negative sample size k = 5. The
model is trained on a Linux machine with 128G
RAM and 16 cores. It takes 5 days to converge.

4.1 Entity Linking

4.1.1 Setup
Dataset As our entities based on English
Wikipedia include not only named entities (e.g.,
persons, organizations) but also general concepts
(e.g., “computer” and “human”), we use a stan-
dard entity linking dataset IITB1 where mentions
of Wikipedia entities are manually annotated ex-
haustively. The dataset contains about 100 docu-
ments and 17K mentions in total. As in the base-
line work, we use only the mentions whose refer-
ent entities are contained in Wikipedia.

Criteria We adopt the common criteria, preci-
sion, recall, and F1. Let A∗ be the golden stan-
dard entity annotations, and A be the annotations
by entity linking model, then

precision =
|A∗ ∩ A|
|A| recall =

|A∗ ∩ A|
|A∗| .

The F1 score is then computed based on the aver-
age precision and recall across all documents.

Baselines We compare our algorithm with the
following approaches. All the competitors are de-
signed to be able to link general concept mentions
to Wikipedia.

CSAW (Kulkarni et al., 2009) has a similar
framework as our algorithm. It measures entity
relatedness using a variation of Jaccard similarity
on Wikipedia page incoming links.

Entity-TM (Han and Sun, 2012) models an en-
tity as a distribution over mentions and words, and
sets up a probabilistic generative process for the
observed text.

Ours-NoH. To validate the advantage of incor-
porating hierarchical structure, we design a base-
line that relies on entity embedding without enti-
ty hierarchy. That is, we obtain entity vectors by
fixing the distance metric in Eq 4 as an identity
matrix.

4.1.2 Results
Table 1 shows the performance of the competitors.
Our algorithm using the entity hierarchy embed-
ding gets 21% to 10% improvement in F1, and

1http://www.cse.iitb.ac.in/soumen/doc/CSAW/Annot

Methods Precision Recall F1
CSAW 0.65 0.74 0.69

Entity-TM 0.81 0.80 0.80
Ours-NoH 0.78 0.85 0.81

Ours 0.87 0.94 0.90

Table 1: Entity linking performance

over 6% and 14% improvements in Precision and
Recall, respectively. The CSAW model devises
a set of entity features based on text content and
link structures of Wikipedia pages, and combines
them to measure relatedness. Compared to these
hand-crafted features which are essentially heuris-
tic and hard to verify, our embedding model in-
duces semantic representations by optimizing a s-
ingle well-defined objective. Note that the em-
bedding actually also encodes the Wikipedia inter-
page network, as we train on the entity-context
pairs which are extracted from wiki links.

The Entity-TM model learns a representation
for each entity as a word distribution. However, as
noted in (Baroni et al., 2014), the counting-based
distributional model usually shows inferior perfor-
mance than context-predicting methods as ours.
Moreover, in addition to the text context, our mod-
el integrates the entity hierarchical structure which
provides rich knowledge of semantic relatedness.
The comparison between Ours and Ours-NoH fur-
ther reveals the effect of integrating the hierarchy
in learning entity vectors. With entity hierarchy,
we obtain more semantically meaningful represen-
tations that achieve 9% F1 improvement over en-
tity vectors without hierarchical knowledge.

4.2 Entity Search

4.2.1 Setup

Dataset We use the dataset from INEX 2009 en-
tity ranking track2, which contains 55 queries. The
golden standard results of each query contains a
set of relevant entities each of which corresponds
to a Wikipedia page.

Criteria We use the common criteria of
precision@k, i.e., the percentage of relevant
entities in the top-k results (we set k = 10), as
well as precision@R where R is the number of
golden standard entities for a query.

2http://www.inex.otago.ac.nz/tracks/entity-
ranking/entity-ranking.asp

1297



Baselines We compare our algorithm with the
following recent competitors.

Balog (Balog et al., 2011) develops a proba-
bilistic generative model which represents entities,
as well as the query, as distributions over both
words and categories. Entities are then ranked
based on the KL-divergence between the distribu-
tions.

K&K (Kaptein and Kamps, 2013) exploits
Wikipedia entity hierarchy to derive the content of
each category, which is in turn used to measure
relatedness with the query categories. It further
incorporates inter-entity links for relevance propa-
gation.

Chen (Chen et al., 2014b) creates for each en-
tity a context profile leveraging both the whole
document (long-range) and sentences around en-
tity (short-range) context, and models query text
by a generative model. Categories are weighted
based on the head words and other features. Our
algorithm exploits a similar method for category
matching.

Methods Precision@10 Precision@R
Balog 0.18 0.16
K&K 0.31 0.28
Chen 0.55 0.42
Ours 0.57 0.46

Table 2: Entity search performance.

4.2.2 Results

Table 2 lists the entity search results of the com-
petitors. Our algorithm shows superiority over the
previous best performing methods. Balog con-
structs representations for each entity merely by
counting (and smoothing) its co-occurrence be-
tween words and categories, which is inadequate
to capture relatedness accurately. K&K leverages
the rich resources in Wikipedia such as text, hi-
erarchy, and link structures. However, the hand-
crafted features are still suboptimal compared with
our learned representations.

Chen performs well by combining both long-
and short-context of entities, as well as catego-
ry lexical similarity. Our algorithm replaces it-
s text matching component with a semantic en-
richment step, i.e., grounding entity mentions in
the query text onto KB entities. This augments
the short query with rich background knowledge,
facilitating accurate relatedness measure based on
our high-quality entity embedding.

4.3 Qualitative Analysis

We qualitatively inspect the learned representa-
tions of the entity hierarchy. The results show that
both the entity vectors and the category distance
metrics capture meaningful semantics, and can po-
tentially boost a wide range of applications such as
recommendation and knowledge base completion.

Entity vectors Table 3 shows a list of target en-
tities, and their top-4 nearest entities in the whole
entity set or subsets belonging to given categories.
Measuring under the whole set (column 2) results
in nearest neighbors that are strongly related with
the target entity. For instance, the nearest enti-
ties for “black hole” are “faster-than-light”, “event
horizon”, “white hole”, and “time dilation”, all of
which are concepts from physical cosmology and
the theory of relativity. Similar results can be ob-
served from other 3 examples.

Even more interesting is to specify a category
and search for the most related entities under the
category. The third column of Table 3 shows sev-
eral examples. E.g., our model found that the most
related Chinese websites to Youtube are “Tudou”,
“56.com”, “Youku” (three top video hosting ser-
vices in China), and “YinYueTai” (a major MV
sharing site in China). The high-quality results
show that our embedding model is able to discov-
er meaningful relationships between entities from
the complex entity hierarchy and plain text. This
can be a useful feature in a wide range of appli-
cations such as semantic search (e.g., looking for
movies about black hole), recommendation (e.g.,
suggesting TV series of specific genre for kids),
and knowledge base completion (e.g., extracting
relations between persons), to name a few.

Target entity Most related entities

black hole

overall:
faster-than-light
event horizon
white hole
time dilation

American films:
Hidden Universe 3D
Hubble (film)
Quantum Quest
Particle Fever

Youtube

overall:
Instagram
Twitter
Facebook
Dipdive

Chinese websites:
Tudou
56.com
Youku
YinYueTai

Harvard
University

overall:
Yale University
University of Pennsylvania
Princeton University
Swarthmore College

businesspeople in software:
Jack Dangermond
Bill Gates
Scott McNealy
Marc Chardon

X-Men: Days of
Future Past (film)

overall:
Marvel Studios
X-Men: The Last Stand
X2 (film)
Man of Steel (film)

children’s television series:
Ben 10: Race Against Time
Kim Possible: A Sitch in Time
Ben 10: Alien Force
Star Wars: The Clone Wars

Table 3: Most related entities under specific cate-
gories. “Overall” represents the most general cat-
egory that includes all the entities.

1298



Xbox 360 games 

Xbox 
Xbox games 

Xbox live arcade  games 

Windows 98 

Windows me 

Windows 7 

Windows server 2008 r2 

Windows 2000 

snapshots of Windows Vista 
snapshots of Microsoft Office 

snapshots of Windows 2000 

snapshots of Microsoft Windows 

snapshots of Windows softwares 

Figure 3: Distance metric visualization for the
subcategories of the category “Microsoft”. The t-
SNE (Van der Maaten and Hinton, 2008) algorith-
m is used to map the high-dimensional (diagonal)
matrixes into the 2D space.

Category distance metrics In addition to learn-
ing vector representations of entities, we also as-
sociate with each category a local distance metric
to capture the features of individual category. As
we restrict the distance metrics to be diagonal ma-
trixes, the magnitude of each diagonal value can
be viewed as how much a category is character-
ized by the corresponding dimension. Categories
with close semantic meanings are expected to have
similar metrics.

Figure 3 visualizes the metrics of all subcate-
gories under the category “Microsoft”, where we
amplify some parts of the figure to showcase the
clustering of semantically relevant categories. For
instance, the categories of Microsoft Windows op-
erating systems, and those of the Xbox games,
are embedded close to each other, respectively.
The results validate that our hierarchy embedding
model can not only encode relatedness between
leaf entities, but also capture semantic similarity
of the internal categories. This can be helpful in
taxonomy refinement and relation discovery.

5 Related Work

Distributed representation There has been a
growing interest in distributed representation of
words. Skip-gram model (Mikolov et al., 2013a)
is one of the most popular methods to learn word
representations. The model aims to find a rep-
resentation for each word that is useful for pre-
dicting its context words. Word-context simi-
larity is measured by simple inner product. A
set of recent works generalizing the basic skip-
gram to incorporate dependency context (Levy
and Goldberg, 2014), word senses (Chen et al.,
2014a), and multi-modal data (Hill and Korho-
nen, 2014). However, these work leverages lim-

ited structured knowledge. Our proposed method
goes beyond skip-gram significantly such that we
measures entity-context similarity under aggregat-
ed distance metrics of hierarchical category n-
odes. This effectively captures the structured
knowledge. Another research line learn knowl-
edge graph embedding (Lin et al., 2015; Wang et
al., 2014; Bordes et al., 2013), which models enti-
ties as vectors and relations as some operations on
the vector space (e.g., translation). These work-
s aim at relation prediction for knowledge graph
completion, and can be viewed as a supplement to
the above that extracts semantics from plain text.

Utilizing hierarchical knowledge Semantic hi-
erarchies are key sources of knowledge. Previ-
ous works (Ponzetto and Strube, 2007; Leacock
and Chodorow, 1998) use KB hierarchies to de-
fine relatedness between concepts, typically based
on path-length measure. Recent works (Yogatama
et al., 2015; Zhao et al., 2011) learn representa-
tions through hierarchical sparse coding that en-
forces similar sparse patterns between nearby n-
odes. Category hierarchies have also been wide-
ly used in classification (Xiao et al., 2011; Wein-
berger and Chapelle, 2009). E.g., in (Verma et al.,
2012) category nodes are endowed with discrim-
inative power by learning distance metrics. Our
approach differs in terms of entity vector learning
and metric aggregation on DAG hierarchy.

6 Conclusion
In this paper, we proposed to learn entity hierarchy
embedding to boost semantic NLP tasks. A princi-
pled framework was developed to incorporate both
text context and entity hierarchical structure from
large-scale knowledge bases. We learn a distance
metric for each category node, and measure enti-
ty vector similarity under aggregated metrics. A
flexible and efficient metric aggregation scheme
was also developed to model large-scale hierar-
chies. Experiments in both entity linking and enti-
ty search tasks show superiority of our approach.

The qualitative analysis indicates that our model
can be potentially useful in a wide range of other
applications such as knowledge base completion
and ontology refinement. Another interesting as-
pect of future work is to incorporate other sources
of knowledge to further enrich the semantics.

Acknowledgments
This research is supported by NSF IIS-1218282,
IIS-12511827, and IIS-1450545.

1299



References
Krisztian Balog, Marc Bron, and Maarten De Rijke.

2011. Query modeling for entity search based on
terms, categories, and examples. TOIS, 29(4):22.

Marco Baroni, Georgiana Dinu, and Germán
Kruszewski. 2014. Dont count, predict! a
systematic comparison of context-counting vs.
context-predicting semantic vectors. In Proc. of
ACL, volume 1, pages 238–247.

Antoine Bordes, Nicolas Usunier, Alberto Garcia-
Duran, Jason Weston, and Oksana Yakhnenko.
2013. Translating embeddings for modeling multi-
relational data. In Proc. of NIPS, pages 2787–2795.

Xinxiong Chen, Zhiyuan Liu, and Maosong Sun.
2014a. A unified model for word sense representa-
tion and disambiguation. In Proc. of EMNLP, pages
1025–1035.

Yueguo Chen, Lexi Gao, Shuming Shi, Xiaoyong Du,
and Ji-Rong Wen. 2014b. Improving context and
category matching for entity search. In Proc. of
AAAI.

Stéphane Clinchant and Florent Perronnin. 2013. Ag-
gregating continuous word embeddings for informa-
tion retrieval. page 100.

Gianluca Demartini, Tereza Iofciu, and Arjen P
De Vries. 2010. Overview of the inex 2009 entity
ranking track. In Focused Retrieval and Evaluation,
pages 254–264. Springer.

Xianpei Han and Le Sun. 2012. An entity-topic model
for entity linking. In Proc. of EMNLP, pages 105–
115. Association for Computational Linguistics.

Felix Hill and Anna Korhonen. 2014. Learning ab-
stract concept embeddings from multi-modal data:
Since you probably can’t see what i mean. In Proc.
of EMNLP, pages 255–265.

Rianne Kaptein and Jaap Kamps. 2013. Exploiting the
category structure of wikipedia for entity ranking.
Artificial Intelligence, 194:111–129.

Sayali Kulkarni, Amit Singh, Ganesh Ramakrishnan,
and Soumen Chakrabarti. 2009. Collective anno-
tation of wikipedia entities in web text. In Proc. of
KDD, pages 457–466. ACM.

Claudia Leacock and Martin Chodorow. 1998. Com-
bining local context and wordnet similarity for word
sense identification. WordNet: An electronic lexical
database, 49(2):265–283.

Omer Levy and Yoav Goldberg. 2014. Dependency-
based word embeddings. In Proc. of ACL, volume 2,
pages 302–308.

Yankai Lin, Zhiyuan Liu, Maosong Sun, Yang Liu, and
Xuan Zhu. 2015. Learning entity and relation em-
beddings for knowledge graph completion. In Proc.
of AAAI.

Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey
Dean. 2013a. Efficient estimation of word represen-
tations in vector space.

Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Cor-
rado, and Jeff Dean. 2013b. Distributed representa-
tions of words and phrases and their compositional-
ity. In Proc. of NIPS, pages 3111–3119.

Alexandre Passos, Vineet Kumar, and Andrew McCal-
lum, 2014. Lexicon Infused Phrase Embeddings for
Named Entity Resolution, pages 78–86. Association
for Computational Linguistics.

Simone Paolo Ponzetto and Michael Strube. 2007.
Knowledge derived from wikipedia for computing
semantic relatedness. JAIR, 30(1):181–212.

Philip Resnik. 1995. Using information content to e-
valuate semantic similarity in a taxonomy. In Proc.
of IJCAI, pages 448–453.

Duyu Tang, Furu Wei, Nan Yang, Ming Zhou, Ting Li-
u, and Bing Qin. 2014. Learning sentiment-specific
word embedding for twitter sentiment classification.
In Proc. of ACL, pages 1555–1565.

Laurens Van der Maaten and Geoffrey Hinton. 2008.
Visualizing data using t-sne. JMLR, 9(2579-
2605):85.

Nakul Verma, Dhruv Mahajan, Sundararajan Sella-
manickam, and Vinod Nair. 2012. Learning hier-
archical similarity metrics. In Proc. of CVPR, pages
2280–2287. IEEE.

Zhen Wang, Jianwen Zhang, Jianlin Feng, and Zheng
Chen. 2014. Knowledge graph and text jointly em-
bedding. In Proc. of EMNLP.

Kilian Q Weinberger and Olivier Chapelle. 2009.
Large margin taxonomy embedding for document
categorization. In Proc. of NIPS, pages 1737–1744.

Lin Xiao, Dengyong Zhou, and Mingrui Wu. 2011.
Hierarchical classification via orthogonal transfer.
In Proc. of ICML, pages 801–808.

Eric P Xing, Michael I Jordan, Stuart Russell, and An-
drew Y Ng. 2002. Distance metric learning with
application to clustering with side-information. In
Proc. of NIPS, pages 505–512.

Dani Yogatama, Manaal Faruqui, Chris Dyer, and Noah
Smith. 2015. Learning word representations with
hierarchical sparse coding. Proc. of ICML.

Jiajun Zhang, Shujie Liu, Mu Li, Ming Zhou, and
Chengqing Zong. 2014. Bilingually-constrained
phrase embeddings for machine translation. In Proc.
of ACL.

Bin Zhao, Fei Li, and Eric P Xing. 2011. Large-scale
category structure aware image categorization. In
Proc. of NIPS, pages 1251–1259.

1300


