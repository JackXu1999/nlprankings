



















































Text Similarity Estimation Based on Word Embeddings and Matrix Norms for Targeted Marketing


Proceedings of NAACL-HLT 2019, pages 1827–1836
Minneapolis, Minnesota, June 2 - June 7, 2019. c©2019 Association for Computational Linguistics

1827

Text Similarity Estimation Based on Word Embeddings and Matrix
Norms for Targeted Marketing

Tim vor der Brück
School of Information Technology

Lucerne University of
Applied Sciences and Arts

Switzerland
tim.vorderbrueck@hslu.ch

Marc Pouly
School of Information Technology

Lucerne University of
Applied Sciences and Arts

Switzerland
marc.pouly@hslu.ch

Abstract

The prevalent way to estimate the similarity
of two documents based on word embeddings
is to apply the cosine similarity measure to
the two centroids obtained from the embed-
ding vectors associated with the words in each
document. Motivated by an industrial appli-
cation from the domain of youth marketing,
where this approach produced only mediocre
results, we propose an alternative way of com-
bining the word vectors using matrix norms.
The evaluation shows superior results for most
of the investigated matrix norms in compari-
son to both the classical cosine measure and
several other document similarity estimates.

1 Introduction

Estimating semantic document similarity is of ut-
most importance in a lot of different areas, like
plagiarism detection, information retrieval, or text
summarization. We will focus here on an NLP
application that has been less researched, i.e., the
assignment of people to the best matching target
group to allow for running precise and customer-
oriented marketing campaigns.

Until recently, similarity estimates were pre-
dominantly based either on deep semantic ap-
proaches or on typical information retrieval tech-
niques like Latent Semantic Analysis. In the last
couple of years, however, so-called word and sen-
tence embeddings became state-of-the-art.

The prevalent approach to document similarity
estimation based on word embeddings consists in
measuring the similarity between the vector rep-
resentations of the two documents derived as fol-
lows:

1. The word embeddings (often weighted by
the tf-idf coefficients of the associated words
(Brokos et al., 2016)) are looked up in a
hashtable for all the words in the two doc-
uments to compare. These embeddings are

determined beforehand on a very large cor-
pus typically using either the skip gram or
the continuous bag of words variant of the
Word2Vec model (Mikolov et al., 2013).

2. The centroid over all word embeddings be-
longing to the same document is calculated
to obtain its vector representation.

If vector representations of the two documents to
compare were successfully established, a similar-
ity estimate can be obtained by applying the cosine
measure to the two vectors.

Let x1, . . . , xm and y1, . . . , yn be the word vec-
tors of two documents. The cosine similarity value
between the two document centroids C1 und C2 is
given by:

cos(∠(
1

m

m∑
i=1

xi,
1

n

n∑
i=1

yi))

=

∑m
i=1

∑n
j=1〈xi, yj〉

mn‖C1‖‖C2‖

(1)

Hence, potentially small values of 〈xi, yj〉 can
have in aggregate a considerable influence on the
total similarity estimate, which makes this esti-
mate vulnerable to noise in the data. We propose
an alternative approach that is based on matrix
norms and which proved to be more noise-robust
by focusing primarily on high word similarities.

Finally, we conducted an evaluation where we
achieved with our method superior accuracy in
target group assignments than several traditional
word embedding based methods.

2 Related Work

The most popular method to come up with word
vectors is Word2Vec, which is based on a 3 layer
neural network architecture in which the word
vectors are obtained as the weights of the hid-
den layer. Alternatives to Word2Vec are GloVe



1828

(Pennington et al., 2014), which is based on ag-
gregated global word co-occurrence statistics and
the Explicit Semantic Analysis (or shortly ESA)
(Gabrilovic and Markovitch, 2009), in which each
word is represented by the column vector in the
tf-idf matrix over Wikipedia.

The idea of Word2Vec can be transferred to the
level of sentences as well. In particular, the so-
called Skip Thought Vector model (STV) (Kiros
et al., 2015) derives a vector representation of
the current sentence by predicting the surrounding
sentences.

(Song and Roth, 2015) propose an alternative
approach to applying the cosine measure to the
two word vector centroids for ESA word embed-
dings. In particular, they establish a bipartite graph
consisting of the best matching vector compo-
nents by solving a linear optimization problem.
The similarity estimate for the documents is then
given by the global optimum of the objective func-
tion. However, this method is only useful for
sparse vector representations. In case of dense
vectors, (Mijangos et al., 2017) suggested to ap-
ply the Frobenius kernel to the embedding matri-
ces, which contain the embedding vectors for all
document components (usually either sentences or
words) (cf. also (Hong et al., 2015)). However,
crucial limitations are that the Frobenius kernel
is only applicable if the number of words (sen-
tences respectively) in the compared documents
coincide and that a word from the first document is
only compared with its counterpart from the sec-
ond document. Thus, an optimal matching has to
be established already beforehand. In contrast, the
matrix norm approach as presented here applies to
arbitrary embedding matrices. Since it conducts
a pairwise comparison of all words contained in
the two documents, there is also no need for any
matching method.

Another simlarity estimate that employs the en-
tire embedding matrix is the word mover‘s dis-
tance (Kusner et al., 2015), which is a special case
of the earth mover‘s distance, a well studied trans-
portation problem. Basically, this approach deter-
mines the minimum effort (with respect to em-
bedding vector changes) to transform the words
of one text into the words of another text. The
word mover‘s distance requires a linear optimiza-
tion problem to be solved. Linear optimization is
usually tackled by the simplex method, which has
in the worst case, which rarely occurs however, ex-

Name Definition

Frob. norm ‖A‖F :=
√∑m

i=1

∑n
j=1 |Aij |2

2-norm ‖A‖2 :=
√
ρ(A>A)

L1,1-norm ‖A‖L1,1 :=
∑m

i=1

∑n
j=1 |Aij |

1-norm ‖A‖1 := max1≤j≤n
∑m

i=1 |Aij |
∞-norm ‖A‖∞ := max1≤i≤m

∑n
j=1 |Aij |

Table 1: Examples of matrix norms; A is an m × n
matrix; ρ(X) denotes the largest absolute eigenvalue
of a squared matrix X.

ponential runtime complexity.

A drawback of conventional similarity esti-
mates as described above is that slightly related
word pairs can have in aggregate a considerable
influence on their values, i.e., these estimates are
sensitive to noise in the data. In contrast, several
of our matrix norm based similarity estimates fo-
cus primarily on strongly related word pairs and
are therefore less vulnerable to noise.

3 Similarity Measure / Matrix Norm

Before going more into detail, we want to review
some concepts that are crucial for the remainder
of this paper. According to (Belanche and Orozco,
2011), a similarity measure on some set X is
an upper bounded, exhaustive and total function
s : X × X → I ⊂ R with |I| > 1 (therefore
I is upper bounded and sup I exists). Addition-
ally, it should fulfill the properties of reflexivity
(the supremum is reached if an item is compared
to itself) and symmetry. We call such a measure
normalized if the supremum equals 1 (Attig and
Perner, 2011). Note that an asymmetric similarity
measure can easily be converted into a symmet-
ric by taking the geometric or arithmetic mean of
the asymmetric measure applied twice to the same
arguments in switched order.

A norm is a function f : V → R over some vec-
tor space V that is absolutely homogeneous, pos-
itive definite and fulfills the triangle inequality. It
is called matrix norm if its domain is a set of ma-
trices and if it is sub-multiplicative, i.e., ‖AB‖ ≤
‖A‖·‖B‖. Several popular matrix norms are given
in Table 1. Note that the Frobenius norm can also
be represented by ‖A‖F =

√
tr(AA>).



1829

4 Document Similarity Measure based
on Matrix Norms

For an arbitrary document t we define the em-
beddings matrix E(t) as follows: E(t)ij is the i-
th component of the normalized embeddings vec-
tor belonging to the j-th word of the document t.
Let t, u be two arbitrary documents, then the en-
try (i, j) of a product E(t)>E(u) specifies the re-
sult of the cosine measure estimating the seman-
tic similarity between word i of document t and
word j of document u. The value of a matrix norm
‖E(t)>E(u)‖ is then a measure for the similar-
ity of the two documents. Since the vector com-
ponents obtained by Word2Vec can be negative,
the cosine measure between two word vectors can
also assume negative values (rather rarely in prac-
tice though). Negative cosine values indicate neg-
atively correlated words and should be handled
akin to the uncorrelated case. Because a matrix
norm usually treats negative and positive matrix
entries alike, we replace all negative values in the
matrix by zeros. Finally, since our measure should
be restricted to values from zero to one, we have
to normalize it.

Formally, we define our similarity measure
sn(t, u) as follows :

‖K(E(t)>E(u))‖√
‖K(E(t)>E(t))‖ · ‖K(E(u)>E(u))‖

where E(t) is the embeddings matrix belong-
ing to document t, where all embedding column
vectors are normalized. K(M) is the matrix,
where all negative entries are replaced by zero, i.e.
K(M)ij = max{0,Mij}.
Proposition 1. If the cosine similarity values be-
tween all embedding vectors of words occurring
in any of the documents are non-negative, i.e., if
K(E(t)>E(u)) = E(t)>E(u) for all document
pairs (t, u), then sn is a normalized similarity
measure for the 2-norm, the Frobenius norm and
the L1,1-norm.

Proof. We give the proof for the 2-norm here and
for the other two norms in the appendix.

Symmetry At first, we focus on the symmetry
condition.

Let A := E(t), B := E(u), where t and u are
arbitrary documents. Symmetry directly follows,
if we can show that

‖Z‖ = ‖Z>‖

for arbitrary matrices Z, since with this property
we have

sn(t, u) =
‖A>B‖√

‖A>A‖ · ‖B>B‖

=
‖(B>A)>‖√
‖B>B‖ · ‖A>A‖

=
‖B>A‖√

‖B>B‖ · ‖A>A‖
= sn(u, t)

(2)

Let M and N be arbitrary matrices such that
MN and NM are both defined and quadratic,
then (see (Chatelin, 1993))

ρ(MN) = ρ(NM) (3)

where ρ(X) denotes the largest absolute eigen-
value of a squared matrix X.

Using identity 3 one can easily infer that:

‖Z‖2 =
√
ρ(Z>Z) =

√
ρ(ZZ>) = ‖Z>‖2

(4)

Boundedness

Proof. The following property needs to be veri-
fied:

‖A>B‖2√
‖A>A‖2 · ‖B>B‖2

≤ 1 (5)

In the proof, we exploit the fact that for ev-
ery positive-semidefinite matrix X, the following
equation holds

ρ(X2) = ρ(X)2 (6)

We observe that for the denominator

‖A>A‖2 · ‖B>B‖2
=
√
ρ((A>A)>A>A)

√
ρ((B>B)>B>B)

=
√
ρ((A>A)>(A>A)>)

√
ρ((B>B)>(B>B)>)

=
√
ρ([(A>A)>]2)

√
ρ([(B>B)>]2)

(6)
=
√
ρ((A>A)>)2

√
ρ((B>B)>)2

=ρ((A>A)>)ρ((B>B)>)

(4)
=‖A‖22 · ‖B‖22

(7)

Putting things together we finally obtain

‖A>B‖2√
‖A>A‖2‖B>B‖2

sub-mult.
≤ ‖A

>‖2 · ‖B‖2√
‖A>A‖2‖B>B‖2

(4)
=

‖A‖2 · ‖B‖2√
‖A>A‖2‖B>B‖2

(7)
=
‖A‖2 · ‖B‖2√
‖A‖22 · ‖B‖22

= 1

(8)



1830

However, proposition 1 is not sufficient in all
cases, since negative cosine similarity values can
occur in practice. Therefore, we also prove a
stronger claim stated in the following proposition.

Proposition 2. If the cosine measure values be-
tween embedding vectors belonging to words of
the same document are all non-negative, then sn
is a normalized similarity measure for the Frobe-
nius and the L1,1-norm.

Proof. The proof of symmetry and reflexivity is
analogous to proposition 1. So we only prove
boundedness of sn . Since the cosine measure
for two embedding vectors emb belonging to
words of the same document cannot be nega-
tive, we have 〈emb(wi), emb(wk)〉 ≥ 0 for
i,k with 1 ≤ i ≤ k ≤ |t| and therefore
K(E(t)>E(t)) = E(t)>E(t). We furthermore
have ‖K(E(t)>E(u))‖ ≤ ‖E(t)>E(u)‖ for the
Frobenius and L1,1-norm, since replacing a zero
entry with another value can never decrease the
value of the norm. Thus,

‖K(E(t)>E(u))‖√
‖K(E(t)>E(t))‖ · ‖K(E(u)>E(u))‖

≤

‖E(t)>E(u)‖√
‖E(t)>E(t)‖ · ‖E(u)>E(u)‖

≤ 1.
(9)

The last inequality follows from proposition
1.

However, the proposed normalization factor√
‖K(E(t)>E(t))‖ · ‖K(E(u)>E(u))‖ is not

eligible for all types of matrix norms, which is an
immediate consequence of the following proposi-
tion.

Proposition 3. There exist no mean value function
mean : R× R→ R such that

sn1 :=
‖K(E(t0)>E(u0))‖1

m

m :=mean(‖K(E(t0)>E(t0))‖1,

‖K(E(u0)>E(u0))‖1)

(10)

is bounded by one. Hence, sn1 cannot be a nor-
malized similarity measure.

Proof. We give a counter-example for the maxi-
mum mean, for which we show that sn1 exceeds
the value of 1:

E(t0) =

0.1644 0.50250 0.5025
0.9864 0.7035



Corpus # Words

German Wikipedia 651 880 623
Frankfurter Rundschau 34 325 073
News journal 20 Minutes 8 629 955

Table 2: Corpus sizes measured by number of words.

E(u0) =

0.1204 0.9759 0.27220.2408 0.0976 0.9526
0.9631 0.1952 0.1361


Since the maximum mean meanmax (a, b) =

max{a, b} is greater or equal to all other means
(including the geometric mean), we have that:

‖K(E(t0)>E(u0))‖1
mean(‖K(E(t0)>E(t0))‖1, ‖K(E(u0)>E(u0))‖1)

≥ ‖K(E(t0)
>E(u0))‖1

max{‖K(E(t0)>E(t0))‖1, ‖K(E(u0)>E(u0))‖1}
= 1.0284 > 1

(11)
for arbitrary type of means mean .

Note that the matrices used in the counter-
example can be extended to any number of em-
bedding dimensions by adding additional zeros.

A further issue is, whether the similarity mea-
sure is invariant to word permutations. Actually,
this is the case for our matrix norm similarity es-
timates, which is stated in the following proposi-
tion.

Proposition 4. The obtained similarity estimate
for all of the considered matrix norms is indepen-
dent of the word sequence of the input texts.

This property is quite beneficial in our scenario
since one of the texts to compare constitutes of an
unordered key word list (see more details in the
next section).

Proof. We focus in this proof on the 2-norm, for
which this property is not directly obvious like for
the other regarded norms. For simplicity, we first
concentrate on the special case that all cosine val-
ues between word embeddings are non-negative.
This proof can easily be extended to the general
case, too. In particular, we show that the similar-
ity estimate does not change, if two columns of
the first matrix are exchanged, which can be ex-
pressed by postmultiplying this matrix with a per-
mutation matrix P. By employing symmetry and
induction this proof can be applied to arbitrary se-
quence permutations and to the second argument



1831

matrix as well. With this, the similarity estimate is
given as:

sn2(t, u) =‖((AP)>B)‖2
=
√
ρ(((AP)>B)>((AP)>B))

=
√
ρ(B>APP>A>B)

(P is an orthogonal matrix)

=
√
ρ(B>AIA>B)

=
√
ρ(A>B)>(A>B)

=‖A>B‖2

(12)

By exploiting that K(MP) = K(M)P for ar-
bitary matrices M, this proof can be generalized
to negative cosine measure values as well.

The question remains, how the similarity mea-
sure value induced by matrix norms performs in
comparison to the usual centroid method. Let us
first focus on L11 and the Frobenius norm. Actu-
ally, both are special cases of a norm that raises
the absolute values of the matrix components to a
certain power e. If this exponent e becomes large,
then:

snLe,1(t, u)

=
‖E(u)>E(t)‖Le,1√

‖E(t)>E(t)‖Le,1 · ‖E(u)>E(u)‖Le,1

≈ (#p)
1/e√√√√√√

∥∥∥∥∥∥∥
1 0 . . .. . .
0 . . . 1


∥∥∥∥∥∥∥
Le,1

·

∥∥∥∥∥∥∥
1 0 . . .. . .
0 . . . 1


∥∥∥∥∥∥∥
Le,1

(mainly the diagonal elements of the

matrices in the denominator assume 1)

≈
(

#p√
nm

)1/e
(13)

where #p denotes the number of perfect matches
(similarity value of 1.0) between words of the two
documents and n (m) is the number of words in
text t (u). Thus, with an increasing exponent,
snLe,1 tends to focus on very good matches and
disregards the others. This property is quite bene-
ficial in our scenario, where often only one or two
words of the contest answers (cf. next section) in-
dicate the right target group.

General statements about the 2-norm based sim-
ilarity measure are difficult, but we can draw some
conclusions, if we restrict to the case, where A>B

is a square diagonal matrix. Hereby, one word of
the first text is very similar to exactly one word of
the second text and very dissimilar to all remain-
ing words. The similarity estimate is then given
by the largest eigenvalue (also called the spectral
radius) of A>B, which equals the largest cosine
measure value. Thus, the 2-norm based similarity
estimate is able to filter out noise (low word simi-
larity values) akin to the Frobenius norm.

Let us now take a look at the similarity measure
sn1, which is induced by the 1-norm. sn1 assumes
high values, if there is one word of the second doc-
ument that matches very well with all words of the
first document. All other less matching words of
the second document do not contribute to the as-
sumed similarity estimate at all.

5 Application to Targeted Marketing

Market segmentation is one of the key tasks of
a marketer. Usually, it is accomplished by clus-
tering over demographic variables, geographic
variables, psychographic variables and behaviors
(Lynn, 2011). In this paper, we will describe an
alternative approach based on unsupervised natu-
ral language processing. In particular, our busi-
ness partner operates a commercial youth plat-
form for the Swiss market, where registered mem-
bers get access to third-party offers such as dis-
counts and special events like concerts or cast-
ings. Actually, several hundred online contests per
year are launched over this platform sponsored by
other firms, an increasing number of them require
the members to write short free-text snippets, e.g.
to elaborate on a perfect holiday at a destination
of their choice in case of a contest sponsored by
a travel agency. Based on the results of a broad
survey, the platform provider’s marketers assume
five different target groups (called milieus) being
present among the platform members: progressive
postmodern youth (people primarily interested in
culture and arts), young performers (people striv-
ing for a high salary with a strong affinity to lux-
ury goods), freestyle action sportsmen, hedonists
(rather poorly educated people who enjoy party-
ing and disco music) and conservative youth (tra-
ditional people with a strong concern for security).
A sixth milieu called special groups comprises all
those who cannot be assigned to one of the up-
per five milieus. For each milieu (with the excep-
tion of special groups) a keyword list was man-
ually created to describe its main characteristics.



1832

Method Contest
1 2 3 All

Random 0.167 0.167 0.167 0.167
ESA 0.357 0.254 0.288 0.335
ESA2 0.355 0.284 0.227 0.330
W2VC 0.347 0.328 0.227 0.330
WW2VC 0.347 0.299 0.197 0.322
GloVe 0.350 0.269 0.258 0.328
STV 0.157 0.313 0.258 0.189

snL1,1 0.337 0.328 0.197 0.318
GM 0.377 0.313 0.227 0.350
sn1 0.372 0.299 0.212 0.343
sn∞ 0.243 0.254 0.273 0.248
sn2 0.370 0.299 0.288 0.350
snF 0.367 0.254 0.242 0.337

Table 3: Obtained accuracy values for similarity mea-
sures induced by different matrix norms and for four
baseline methods. GM = Geometric Mean of sn1 and
sn∞. (W)W2VC=(tf-idf-weighted) Word2Vec Em-
bedding Centroids.

For triggering marketing campaigns, an algorithm
shall be developed that automatically assigns each
contest answer to the most likely target group:
we propose the youth milieu as best match for a
contest answer, for which the estimated semantic
similarity between the associated keyword list and
user answer is maximal. In case the highest sim-
ilarity estimate falls below the 10 percent quan-
tile for the distribution of highest estimates, the
special groups milieu is selected. Since the key-
word list typically consists of nouns (in the Ger-
man language capitalized) and the user contest an-
swers might contain a lot of adjectives and verbs
as well, which do not match very well to nouns in
the Word2Vec vector representation, we actually
conduct two comparisons for our Word2Vec based
measures, one with the unchanged user contest an-
swers and one by capitalizing every word before-
hand. The final similarity estimate is then given as
the maximum value of both individual estimates.

6 Evaluation

For evaluation, we selected three online contests
(language: German), where people elaborated on
their favorite travel destination for an example,
speculated about potential experiences with a pair
of fancy sneakers (contest 2) and explained why
they emotionally prefer a certain product out of

Method Contest
1 2 3

Min kap. 0.123 0.295/0.030 0.110/0.101
Max. kap. 0.178 0.345/0.149 0.114/0.209

sn2 0.128 0.049/0.065 0.060/0.064
sn1 0.124 -0.032/0.033 0.024/0.017
snF 0.129 0.041/0.042 0.039/0.045

# Entr. 1544 100 100

Table 4: Minimum and maximum average inter-
annotator agreements (Cohen’s kappa) / average inter-
annotator agreement values for our automated match-
ing method, FN=Frobenius norm.

 0
 200
 400
 600
 800

 1000
 1200
 1400
 1600

 0  0.2  0.4  0.6  0.8  1  1.2  1.4  1.6

Distribution of sn1

Figure 1: Distribution of sn1 determined on contest 1.

four available candidates. We experimented with
different keyword list sizes but obtained the best
results with rather few and therefore precise key-
words. In particular, we used the following num-
ber of keywords for the individual milieus:
• Action Sportsman: 3
• Young Performer: 4
• Hedonist: 7
• Conservative Youth: 4
• Progressive Postmodern Youth: 6
In order to provide a gold standard, three pro-

fessional marketers from different youth market-
ing companies annotated independently the best
matching youth milieus for every contest answer.
We determined for each annotator individually
his/her average inter-annotator agreement with the
others (Cohen’s kappa). The minimum and maxi-
mum of these average agreement values are given
in Table 4. Since for contest 2 and contest 3,
some of the annotators annotated only the first
50 entries (last 50 entries respectively), we spec-
ified min/max average kappa values for both parts.
We further compared the youth milieus proposed



1833

by our unsupervised matching algorithm with the
majority votes over the human experts’ answers
(see Table 3)1. Moreover, we computed its aver-
age inter-annotator agreement with the human an-
notators (see again Table 4), quasi treating the pre-
dictions like additional annotations.

The Word2Vec word embeddings were trained
on the German Wikipedia (dump originating from
20 February 2017) merged with a Frankfurter
Rundschau newspaper Corpus and 34 249 articles
of the news journal 20 minutes2, where the latter
is targeted to the Swiss market and freely avail-
able at various Swiss train stations (see Table 2
for a comparison of corpus sizes). By employing
articles from 20 minutes, we want to ensure the
reliability of word vectors for certain Switzerland
specific expressions like Velo or Glace, which are
underrepresented in the German Wikipedia and
the Frankfurter Rundschau corpus. ESA is usu-
ally trained on Wikipedia, since the authors of
the original ESA paper suggest that the articles of
the training corpus should represent disjoint con-
cepts, which is only guaranteed for encyclopedias.
However, Stein and Anerka (Gottron et al., 2011)
challenged this hypothesis and demonstrated that
promising results can be obtained by applying
ESA on other types of corpora like the popular
Reuters newspaper corpus as well. Unfortunately,
the implementation we use (Wikiprep-ESA3) ex-
pects its training data to be a Wikipedia Dump.
Furthermore, Wikiprep-ESA only indexes words
that are connected by hyperlinks, which are usu-
ally lacking in ordinary newspaper articles. So we
could train Wikiprep-ESA on Wikipedia only but
additionally have developed a version of ESA that
can be applied on arbitrary corpora (in the follow-
ing referred to as ESA2) and which was trained
on the full corpus (Wikipedia+Frankfurter Rund-
schau+20 minutes). The STVs were also trained
on the same corpus as our matrix norms based es-
timates and Word2Vec embedding centroids. The
actual document similarity estimation is accom-
plished by the usual centroid approach (we did not
evaluate matrix norms here). An issue we were
faced with is that STVs are not bag of word mod-
els but actually take the sequence of the words
into account and therefore the obtained similar-

1Note that the geometric mean of the 1- and∞-norm as
specified in Table 3 is not a matrix norm itself, since it lacks
submultipicativity.

2http://www.20min.ch
3https://github.com/faraday/wikiprep-esa

Method Accuracy

ESA 0.672
STV 0.716

W2VC 0.726
sn2 0.731

snL1,1 0.741
snF 0.781

Table 5: Accuracy value otbained for matching a sen-
tence of the first to the associated sentence of the sec-
ond translation.

ity estimate between milieu keyword list and con-
test answer would be dependent on the keyword
ordering. However, this order could have arbi-
trarily been chosen by the marketers and might
be completely random. A possible solution is to
compare the contest answers with all possible per-
mutation of keywords and determine the maxi-
mum value over all those comparisons. However,
such an approach would be infeasible already for
medium keyword list sizes. Therefore, we use a
beam search approach instead, which extends the
keyword list iteratively and keeps only the n-best
performing permutations.

Finally, to verify the general applicability of
our approach, we conducted a second experiment,
where a novel from Edgar Allen Poe (The pur-
loined letter) was independently translated by two
translators into German. We aim to match a sen-
tence from the first translation to the associated
sentence of the second by looking for the assign-
ment with the highest semantic relatedness disre-
garding the sentence order. The obtained accuracy
values based on the first 200 sentences of both
translations are given in Table 5. To guarantee
an 1:1 sentence mapping, periods were partly re-
placed by semicolons.

7 Discussion

The evaluation showed that the inter-annotator
agreement values vary strongly for contest 2 part
2 (minimum average annotator agreement accord-
ing to Cohen’s kappa of 0.03 while the maxi-
mum is 0.149, see Table 4). On this contest part,
our matrix norm-based matching (2-norm and
Frobenius-norm) obtains a considerably higher av-
erage agreement than one of the annotators. Re-
garding baseline systems, the most relevant com-
parison is naturally the one with Word2Vec cen-



1834

 0

 0.2

 0.4

 0.6

 0  0.2  0.4  0.6

(a) W2VC vs snL11

 0
 0.2
 0.4
 0.6
 0.8

 0  0.2  0.4  0.6

(b) W2VC vs sn2

 0

 0.2

 0.4

 0.6

 0  0.2  0.4  0.6

(c) W2VC vs snF

Figure 2: Scatter plots of cosine between centroids of Word2Vec embeddings (W2VC) vs sn .

troids, since it employs the same type of data.
Hereby we reached higher accuracy values for the
best performing matrix norms on two of the three
contests including the largest contest 1. Note that
the elimination of negative values from the embed-
ding matrix product proved to be important. If we
omit this step, the obtained accuracy of snf for in-
stance will drop by around 0.023 determined over
all three contests (column: all).

It is quite striking that, although sn1 lacks
two properties of a normalized similarity measure
(boundedness by 1 and symmetry), it reaches quite
good results on contest 1. As you can see in Fig-
ure 1, which shows the distribution of sn1 in con-
test 1, the value of 1 is indeed exceeded several
times (the maximum value is 1.5), but this occurs
rather rarely in our experiment. Actually, 99% of
its values fall into the interval [0,1]. Thus, the non-
boundedness is much less a problem in practice
than the theoretical results indicate.

Finally, we determined the scatter plots (see
Figure 2) showing cosine of Word2Vec embed-
dings (W2VC) vs several matrix norm based sim-
ilarity estimates. These scatter plots exhibits that
the score distributions of snf and sn2 are quite
similar and their values often exceed the cosine
measure value due to the fact that a few very strong
word matches can already result in a high simi-
larity estimate. The scatter plot for snL11 reveals
that this measure is much closer to W2VC than the
other two matrix norm based similarity estimates.

Note that a downside of our approach in rela-
tion to the usual Word2Vec centroids method is
the increased runtime, since it requires the pair-
wise comparison of all words contained in the in-
put documents. In our scenario with rather short
text snippets and keyword lists, this was not much
of an issue. However, for large documents, such
a comprehensive comparison could become soon

infeasible. One possible solution for this perfor-
mance issue is to apply our proposed estimates
to sentence embeddings instead of word embed-
dings, which on the one hand would reduce the
dimensionality of the embedding matrices and on
the other hand would take word order into account.

8 Conclusion

We proposed a novel similarity measure to com-
pare word embeddings from different documents,
which makes use of matrix norms. This measure
was evaluated on the task to assign users to the best
matching marketing target groups. We obtained
superior results compared to the usual centroid /
cosine measure similarity estimation for most of
the investigated matrix norm especially for the
largest contest 1. Furthermore, we proved elemen-
tary properties for our proposed similarity mea-
sure regarding its well-definedness and its perfor-
mance in comparison to the usual centroid-based
approach.

Acknowledgement

Hereby we thank the Jaywalker GmbH as well as
the Jaywalker Digital AG for their support regard-
ing this publication and especially for annotating
the contest data with the best-fitting youth milieus.

References
Anja Attig and Petra Perner. 2011. The problem of

normalization and a normalized similarity measure
by online data. Transactions on Case-Based Rea-
soning, 4(1).

Lluı́s A. Belanche and Jorge Orozco. 2011. Things
to know about a (dis)similarity measure. In Pro-
ceedings of the 15th International Conference on
Knowledge-Based and Intelligent Information &
Engineering Systems, Karlsruhe, Germany.



1835

Georgios-Ioannis Brokos, Prodromos, and Ion An-
droutsopoulos. 2016. Using centroids of word em-
beddings and word mover‘s distance for biomedical
document retrieval in question answering. In Pro-
ceedings of the 15th Workshop on Biomedical Nat-
ural Language Processing, pages 114–118, Berlin,
Germany.

Françoise Chatelin. 1993. Eigenvalues of Matrices -
Revised Edition. Society for Industrial and Applied
Mathematics, Philadelphia, Pennsylvania.

Evgeniy Gabrilovic and Shaul Markovitch. 2009.
Wikipedia-based semantic interpretation for natural
language processing. Journal of Artificial Intelli-
gence Research, 34.

Thomas Gottron, Maik Anderka, and Benno Stein.
2011. Insights into explicit semantic analysis. In
Proceedings of the 20th ACM international confer-
ence on Information and knowledge management,
pages 1961–1964, Glasgow, UK.

Ki-Joo Hong, Gai-Hui Lee, and Han-Joon Kom. 2015.
Enhanced document clustering using wikipedia-
based document representation. In Proceedings of
the 2015 International Conference on Applied Sys-
tem Innovation (ICASI).

Ryand Kiros, Yukun Zhu, Rusland Salakhudinov,
Richard S. Zemel, Antonio Torralba, Raquel Urta-
sun, and Sanja Fiedler. 2015. Skip-thought vec-
tors. In Proceedings of the Conference on Neural
Information Processing Systems (NIPS), Montréal,
Canada.

Matt J. Kusner, Yu Sun, Nicholas I. Kolkin, and Kil-
ias Q. Weinberger. 2015. From word embeddings to
document distances. In Proceedings of the 32nd In-
ternational Conference on Machine Learning, pages
957–966.

Michael Lynn. 2011. Segmenting and target-
ing your market: Strategies and limitations.
Technical report, Cornell University. Online:
http://scholorship.sha.cornell.
edu/articles/243.

Victor Mijangos, Gerardo Sierra, and Azuncena
Montes. 2017. Sentence level matrix representation
for document spectral clustering. Pattern Recogni-
tion Letters, 85.

Tomas Mikolov, Ilya Sutskever, Chen Ilya, Greg Cor-
rado, and Jeffrey Dean. 2013. Distributed represen-
tations of words and phrases and their composition-
ality. In Proceedings of the Conference on Neu-
ral Information Processing Systems (NIPS), pages
3111–3119, Lake Tahoe, Nevada.

Jeffrey Pennington, Richard Socher, and Christo-
pher D. Manning. 2014. Glove: Global vectors for
word representation. In Proceedings of the Con-
ference on Empirical Methods in Natural Language
Processing (EMNLP), Doha, Katar.

Yangqiu Song and Dan Roth. 2015. Unsupervised
sparse vector densification for short text similarity.
In Proceedings of the Conference of the North Amer-
ican Chapter of the Association for Computational
Linguistics (NAACL), Denver, Colorado.

A Proof of Proposition 1

In this section we give the proof of proposition 1
for Frobenius and L1,1-norm.

Reflexivity is trivially fulfilled for Frobenius,
2- and L1,1-norm. Therefore, we only prove that
the induced similarity measures are symmetric and
upper-bounded by 1.

Let A := E(t), B := E(u), where t and u are
arbitrary documents.

A.1 Frobenius norm
Symmetry The trace of a matrix is known to
be invariant under cyclic permutations. With this
property, the symmetry of snF can easily be de-
duced.

Boundedness

Proof. We need to show that:

‖A>B‖F√
‖A>A‖F · ‖B>B‖F

≤ 1 (14)

For that we leverage the Cauchy-Schwarz in-
equality that states that for an inner product space
H we have

|〈x, y〉| ≤
√
〈x, x〉 · 〈y, y〉 (15)

for all x, y ∈ H, where equality holds if, and only
if, x is a scalar multiple of y. In particular, the
function f(Y,X) = tr(Y>X) is such an inner
product.

Let X = AA> and Y = BB> such that
X,Y ∈ Rm×m. Then we can infer
tr(BB>AA>) = tr((BB>)>AA>)

= tr(Y>X)

(15)
≤
√

tr(X>X) · tr(Y>X)

=
√

tr((AA>)>AA>) · tr((BB>)>BB>)

=
√

tr(AA>AA>) · tr(BB>BB>)
(16)

Next, we observe that for the numerator

‖A>B‖F =
√

tr(A>B(A>B)>)

=
√

tr(A>BB>A)

=
√

tr(BB>AA>)

(16)
≤ 4

√
tr(AA>AA>) · tr(BB>BB>)

= 4
√

tr(A>A>A>A) · tr(B>BB>B)

=
√
‖A>A‖F · ‖B>B‖F

(17)

http://scholorship.sha.cornell.edu/articles/243
http://scholorship.sha.cornell.edu/articles/243


1836

Hence, we finally obtain

‖A>B‖F√
‖A>A‖F · ‖B>B‖F

(17)
≤

√
‖A>A‖F · ‖B>B‖F√
‖A>A‖F · ‖B>B‖F

= 1.

A.2 L1,1-norm
Proof for symmetry. For the L1,1-norm, we have
that

‖A‖L1,1 =
m∑
i=1

n∑
j=1

|Aij | =
n∑

j=1

m∑
i=1

|A>ji|

=‖A>‖L1,1

(18)

The boundedness follows directly from the
Cauchy-Schwarz inequality, since the induced
similarity estimate snL1,1 is an inner product.


