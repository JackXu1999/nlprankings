



















































Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics


Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics, pages 277–287
Vancouver, Canada, July 30 - August 4, 2017. c©2017 Association for Computational Linguistics

https://doi.org/10.18653/v1/P17-1026

Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics, pages 277–287
Vancouver, Canada, July 30 - August 4, 2017. c©2017 Association for Computational Linguistics

https://doi.org/10.18653/v1/P17-1026

A* CCG Parsing with a Supertag and Dependency Factored Model

Masashi Yoshikawa and Hiroshi Noji and Yuji Matsumoto
Graduate School of Information and Science

Nara Institute of Science and Technology
8916-5, Takayama, Ikoma, Nara, 630-0192, Japan

{ masashi.yoshikawa.yh8, noji, matsu }@is.naist.jp

Abstract

We propose a new A* CCG parsing model
in which the probability of a tree is decom-
posed into factors of CCG categories and
its syntactic dependencies both defined on
bi-directional LSTMs. Our factored model
allows the precomputation of all probabil-
ities and runs very efficiently, while mod-
eling sentence structures explicitly via de-
pendencies. Our model achieves the state-
of-the-art results on English and Japanese
CCG parsing.1

1 Introduction

Supertagging in lexicalized grammar parsing is
known as almost parsing (Bangalore and Joshi,
1999), in that each supertag is syntactically infor-
mative and most ambiguities are resolved once a
correct supertag is assigned to every word. Re-
cently this property is effectively exploited in A*
Combinatory Categorial Grammar (CCG; Steed-
man (2000)) parsing (Lewis and Steedman, 2014;
Lewis et al., 2016), in which the probability of
a CCG tree y on a sentence x of length N is
the product of the probabilities of supertags (cate-
gories) ci (locally factored model):

P (y|x) =
∏

i∈[1,N ]
Ptag(ci|x). (1)

By not modeling every combinatory rule in a
derivation, this formulation enables us to employ
efficient A* search (see Section 2), which finds the
most probable supertag sequence that can build a
well-formed CCG tree.

Although much ambiguity is resolved with this
supertagging, some ambiguity still remains. Fig-
ure 1 shows an example, where the two CCG

1 Our software and the pretrained models are available at:
https://github.com/masashi-y/depccg.

(a)

a house in Paris in France

NP (NP\NP)/NP NP (NP\NP)/NP NP
> >

NP\NP NP\NP
<

NP
<

NP
(b)

a house in Paris in France

NP (NP\NP)/NP NP (NP\NP)/NP NP
>

NP\NP
<

NP
>

NP\NP
<

NP

Figure 1: CCG trees that are equally likely under
Eq. 1. Our model resolves this ambiguity by mod-
eling the head of every word (dependencies).

parses are derived from the same supertags. Lewis
et al.’s approach to this problem is resorting to
some deterministic rule. For example, Lewis et al.
(2016) employ the attach low heuristics, which
is motivated by the right-branching tendency of
English, and always prioritizes (b) for this type
of ambiguity. Though for English it empirically
works well, an obvious limitation is that it does not
always derive the correct parse; consider a phrase
“a house in Paris with a garden”, for which the
correct parse has the structure corresponding to (a)
instead.

In this paper, we provide a way to resolve these
remaining ambiguities under the locally factored
model, by explicitly modeling bilexical dependen-
cies as shown in Figure 1. Our joint model is still
locally factored so that an efficient A* search can
be applied. The key idea is to predict the head of
every word independently as in Eq. 1 with a strong
unigram model, for which we utilize the scoring
model in the recent successful graph-based depen-
dency parsing on LSTMs (Kiperwasser and Gold-
berg, 2016; Dozat and Manning, 2016). Specif-

277

https://doi.org/10.18653/v1/P17-1026
https://doi.org/10.18653/v1/P17-1026


ically, we extend the bi-directional LSTM (bi-
LSTM) architecture of Lewis et al. (2016) predict-
ing the supertag of a word to predict the head of it
at the same time with a bilinear transformation.

The importance of modeling structures beyond
supertags is demonstrated in the performance gain
in Lee et al. (2016), which adds a recursive com-
ponent to the model of Eq. 1. Unfortunately, this
formulation loses the efficiency of the original one
since it needs to compute a recursive neural net-
work every time it searches for a new node. Our
model does not resort to the recursive networks
while modeling tree structures via dependencies.

We also extend the tri-training method of Lewis
et al. (2016) to learn our model with dependen-
cies from unlabeled data. On English CCGbank
test data, our model with this technique achieves
88.8% and 94.0% in terms of labeled and unla-
beled F1, which mark the best scores so far.

Besides English, we provide experiments on
Japanese CCG parsing. Japanese employs freer
word order dominated by the case markers and a
deterministic rule such as the attach low method
may not work well. We show that this is actually
the case; our method outperforms the simple ap-
plication of Lewis et al. (2016) in a large margin,
10.0 points in terms of clause dependency accu-
racy.

2 Background

Our work is built on A* CCG parsing (Section
2.1), which we extend in Section 3 with a head
prediction model on bi-LSTMs (Section 2.2).

2.1 Supertag-factored A* CCG Parsing

CCG has a nice property that since every category
is highly informative about attachment decisions,
assigning it to every word (supertagging) resolves
most of its syntactic structure. Lewis and Steed-
man (2014) utilize this characteristics of the gram-
mar. Let a CCG tree y be a list of categories
⟨c1, . . . , cN ⟩ and a derivation on it. Their model
looks for the most probable y given a sentence x
of length N from the set Y (x) of possible CCG
trees under the model of Eq. 1:

ŷ = arg max
y∈Y (x)

∑

i∈[1,N ]
log Ptag(ci|x).

Since this score is factored into each supertag, they
call the model a supertag-factored model.

Exact inference of this problem is possible by
A* parsing (Klein and D. Manning, 2003), which
uses the following two scores on a chart:

b(Ci,j) =
∑

ck∈ci,j
log Ptag(ck|x),

a(Ci,j) =
∑

k∈[1,N ]\[i,j]
max

ck
log Ptag(ck|x),

where Ci,j is a chart item called an edge, which
abstracts parses spanning interval [i, j] rooted by
category C. The chart maps each edge to the
derivation with the highest score, i.e., the Viterbi
parse for Ci,j . ci,j is the sequence of categories on
such Viterbi parse, and thus b is called the Viterbi
inside score, while a is the approximation (upper
bound) of the Viterbi outside score.

A* parsing is a kind of CKY chart parsing aug-
mented with an agenda, a priority queue that keeps
the edges to be explored. At every step it pops the
edge e with the highest priority b(e) + a(e) and
inserts that into the chart, and enqueue any edges
that can be built by combining e with other edges
in the chart. The algorithm terminates when an
edge C1,N is popped from the agenda.

A* search for this model is quite efficient be-
cause both b and a can be obtained from the uni-
gram category distribution on every word, which
can be precomputed before search. The heuris-
tics a gives an upper bound on the true Viterbi
outside score (i.e., admissible). Along with this
the condition that the inside score never increases
by expansion (monotonicity) guarantees that the
first found derivation on C1,N is always optimal.
a(Ci,j) matches the true outside score if the one-
best category assignments on the outside words
(arg maxck log Ptag(ck|x)) can comprise a well-
formed tree with Ci,j , which is generally not true.

Scoring model For modeling Ptag, Lewis and
Steedman (2014) use a log-linear model with fea-
tures from a fixed window context. Lewis et al.
(2016) extend this with bi-LSTMs, which encode
the complete sentence and capture the long range
syntactic information. We base our model on this
bi-LSTM architecture, and extend it to modeling a
head word at the same time.

Attachment ambiguity In A* search, an edge
with the highest priority b+a is searched first, but
as shown in Figure 1 the same categories (with the
same priority) may sometimes derive more than

278



one tree. In Lewis and Steedman (2014), they pri-
oritize the parse with longer dependencies, which
they judge with a conversion rule from a CCG
tree to a dependency tree (Section 4). Lewis et al.
(2016) employ another heuristics prioritizing low
attachments of constituencies, but inevitably these
heuristics cannot be flawless in any situations. We
provide a simple solution to this problem by ex-
plicitly modeling bilexical dependencies.

2.2 Bi-LSTM Dependency Parsing
For modeling dependencies, we borrow the idea
from the recent graph-based neural dependency
parsing (Kiperwasser and Goldberg, 2016; Dozat
and Manning, 2016) in which each dependency
arc is scored directly on the outputs of bi-LSTMs.
Though the model is first-order, bi-LSTMs enable
conditioning on the entire sentence and lead to the
state-of-the-art performance. Note that this mech-
anism is similar to modeling of the supertag distri-
bution discussed above, in that for each word the
distribution of the head choice is unigram and can
be precomputed. As we will see this keeps our
joint model still locally factored and A* search
tractable. For score calculation, we use an ex-
tended bilinear transformation by Dozat and Man-
ning (2016) that models the prior headness of each
token as well, which they call biaffine.

3 Proposed Method

3.1 A* parsing with Supertag and
Dependency Factored Model

We define a CCG tree y for a sentence x =
⟨xi, . . . , xN ⟩ as a triplet of a list of CCG cat-
egories c = ⟨c1, . . . , cN ⟩, dependencies h =
⟨h1, . . . , hN ⟩, and the derivation, where hi is the
head index of xi. Our model is defined as follows:

P (y|x) =
∏

i∈[1,N ]
Ptag(ci|x)

∏

i∈[1,N ]
Pdep(hi|x).

(2)

The added term Pdep is a unigram distribution of
the head choice.

A* search is still tractable under this model.
The search problem is changed as:

ŷ = arg max
y∈Y (x)

( ∑

i∈[1,N ]
log Ptag(ci|x)

+
∑

i∈[1,N ]
log Pdep(hi|x)

)
,

  

John met
NP S\NP/NP NP

Mary

b(e2) b(e1) 
             b(e3) = 
         b(e1) + b(e2)
 + logPdep(met → John)
NP   S\NP/NP    NP
John   saw    Mary

NP S\NP
S

Figure 2: Viterbi inside score for edge e3 under
our model is the sum of those of e1 and e2 and the
score of dependency arc going from the head of e2
to that of e1 (the head direction changes according
to the child categories).

and the inside score is given by:

b(Ci,j) =
∑

ck∈ci,j
log Ptag(ck|x) (3)

+
∑

k∈[i,j]\{root(hCi,j)}
log Pdep(hk|x),

where hCi,j is a dependency subtree for the Viterbi
parse on Ci,j and root(h) returns the root index.
We exclude the head score for the subtree root to-
ken since it cannot be resolved inside [i, j]. This
causes the mismatch between the goal inside score
b(C1,N ) and the true model score (log of Eq. 2),
which we adjust by adding a special unary rule that
is always applied to the popped goal edge C1,N .

We can calculate the dependency terms in Eq. 3
on the fly when expanding the chart. Let the cur-
rently popped edge be Ai,k, which will be com-
bined with Bk,j into Ci,j . The key observation is
that only one dependency arc (between root(hAi,k)
and root(hBk,j)) is resolved at every combination
(see Figure 2). For every rule C → A B we
can define the head direction (see Section 4) and
Pdep is obtained accordingly. For example, when
the right child B becomes the head, b(Ci,j) =
b(Ai,k) + b(Bk,j) + log Pdep(hl = m|x), where
l = root(hAi,k) and m = root(h

B
k,j) (l < m).

The Viterbi outside score is changed as:

a(Ci,j) =
∑

k∈[1,N ]\[i,j]
max

ck
log Ptag(ck|x)

+
∑

k∈L
max

hk
log Pdep(hk|x),

where L = [1, N ] \ [k′|k′ ∈ [i, j], root(hCi,j) ̸=
k′]. We regard root(hCi,j) as an outside word since
its head is undefined yet. For every outside word
we independently assign the weight of its argmax

279



head, which may not comprise a well-formed de-
pendency tree. We initialize the agenda by adding
an item for every supertag C and word xi with the
score a(Ci,i) =

∑
k∈I\{i} max log Ptag(ck|x) +∑

k∈I max log Pdep(hk|x). Note that the depen-
dency component of it is the same for every word.

3.2 Network Architecture
Following Lewis et al. (2016) and Dozat and Man-
ning (2016), we model Ptag and Pdep using bi-
LSTMs for exploiting the entire sentence to cap-
ture the long range phenomena. See Figure 3 for
the overall network architecture, where Ptag and
Pdep share the common bi-LSTM hidden vectors.

First we map every word xi to their hidden vec-
tor ri with bi-LSTMs. The input to the LSTMs
is word embeddings, which we describe in Sec-
tion 6. We add special start and end tokens to each
sentence with the trainable parameters following
Lewis et al. (2016). For Pdep, we use the biaffine
transformation in Dozat and Manning (2016):

gdepi = MLP
dep
child(ri),

gdephi = MLP
dep
head(rhi),

Pdep(hi|x) (4)
∝ exp((gdepi )TWdepg

dep
hi

+ wdepg
dep
hi

),

where MLP is a multilayered perceptron.
Though Lewis et al. (2016) simply use an MLP
for mapping ri to Ptag, we additionally utilize the
hidden vector of the most probable head hi =
arg maxh′i Pdep(h

′
i|x), and apply ri and rhi to a

bilinear function:2

gtagi = MLP
tag
child(ri),

gtaghi = MLP
tag
head(rhi), (5)

ℓ = (gtagi )
TUtagg

tag
hi

+ Wtag

[
gtagi
gtaghi

]
+ btag,

Ptag(ci|x) ∝ exp(ℓc),

where Utag is a third order tensor. As in Lewis et
al. these values can be precomputed before search,
which makes our A* parsing quite efficient.

4 CCG to Dependency Conversion

Now we describe our conversion rules from a CCG
tree to a dependency one, which we use in two pur-

2 This is inspired by the formulation of label prediction in
Dozat and Manning (2016), which performs the best among
other settings that remove or reverse the dependence between
the head model and the supertag model.

  

LSTM LSTM LSTM LSTM

LSTM LSTM LSTM LSTM

concat concat concat concat

x1 x2 x3 x4

Bilinear Biaffine

S  NP  S/S ..
                  ..

x1   x2   x3 ..
                  ..

r1 r2 r3 r4

PdepPtag

Figure 3: Neural networks of our supertag and
dependency factored model. First we map every
word xi to a hidden vector ri by bi-LSTMs, and
then apply biaffine (Eq. 4) and bilinear (Eq. 5)
transformations to obtain the distributions of de-
pendency heads (Pdep) and supertags (Ptag).

poses: 1) creation of the training data for the de-
pendency component of our model; and 2) extrac-
tion of a dependency arc at each combinatory rule
during A* search (Section 3.1). Lewis and Steed-
man (2014) describe one way to extract dependen-
cies from a CCG tree (LEWISRULE). Below in
addition to this we describe two simpler alterna-
tives (HEADFIRST and HEADFINAL), and see the
effects on parsing performance in our experiments
(Section 6). See Figure 4 for the overview.

LEWISRULE This is the same as the conversion
rule in Lewis and Steedman (2014). As shown in
Figure 4c the output looks a familiar English de-
pendency tree.

For forward application and (generalized) for-
ward composition, we define the head to be the
left argument of the combinatory rule, unless it
matches either X/X or X/(X\Y ), in which case
the right argument is the head. For example, on
“Black Monday” in Figure 4a we choose Mon-
day as the head of Black. For the backward rules,
the conversions are defined as the reverse of the
corresponding forward rules. For other rules, Re-
movePunctuation (rp) chooses the non punctua-
tion argument as the head, while Conjunction (Φ)
chooses the right argument.3

3When applying LEWISRULE to Japanese, we ignore the
feature values in determining the head argument, which we
find often leads to a more natural dependency structure. For
example, in “tabe ta” (eat PAST), the category of auxiliary
verb “ta” is Sf1\Sf2 with f1 ̸= f2, and thus Sf1 ̸= Sf2 . We
choose “tabe” as the head in this case by removing the feature
values, which makes the category X\X .

280



No , it was n′t Black Monday .
S/S , NP (S\NP)/NP (S\NP )\(S\NP ) NP/NP NP .

<B× >
(S\NP)/NP NP

>
S\NP

<
S

rp
S

>
S

rp
S

(a) English sentence

I SUB English ACC speak want .
Boku wa eigo wo hanasi tai .

NP NP\NP NP NP\NP (S\NP)\NP S\S S\S
< < <B2

NP NP (S\NP)\NP
<

S\NP
<

S
<

S

(b) Japanese sentence “I want to speak English.”

No , it was n’t Black Monday .

(c) LEWISRULE

No , it was n’t Black Monday .

(d) HEADFIRST

Boku wa eigo wo hanasi tai .

(e) HEADFINAL

Figure 4: Examples of applying conversion rules in Section 4 to English and Japanese sentences.

One issue when applying this method for ob-
taining the training data is that due to the mis-
match between the rule set of our CCG parser, for
which we follow Lewis and Steedman (2014), and
the grammar in English CCGbank (Hockenmaier
and Steedman, 2007) we cannot extract dependen-
cies from some of annotated CCG trees.4 For this
reason, we instead obtain the training data for this
method from the original dependency annotations
on CCGbank. Fortunately the dependency annota-
tions of CCGbank matches LEWISRULE above in
most cases and thus they can be a good approxi-
mation to it.

HEADFINAL Among SOV languages, Japanese
is known as a strictly head final language, mean-
ing that the head of every word always follows
it. Japanese dependency parsing (Uchimoto et al.,
1999; Kudo and Matsumoto, 2002) has exploited
this property explicitly by only allowing left-to-
right dependency arcs. Inspired by this tradition,
we try a simple HEADFINAL rule in Japanese
CCG parsing, in which we always select the right
argument as the head. For example we obtain the
head final dependency tree in Figure 4e from the
Japanese CCG tree in Figure 4b.

HEADFIRST We apply the similar idea as
HEADFINAL into English. Since English has the
opposite, SVO word order, we define the simple
“head first” rule, in which the left argument always
becomes the head (Figure 4d).

4 For example, the combinatory rules in Lewis and Steed-
man (2014) do not contain Nconj → N N in CCGbank.
Another difficulty is that in English CCGbank the name of
each combinatory rule is not annotated explicitly.

Though this conversion may look odd at first
sight it also has some advantages over LEWIS-
RULE. First, since the model with LEWISRULE
is trained on the CCGbank dependencies, at infer-
ence, occasionally the two components Pdep and
Ptag cause some conflicts on their predictions. For
example, the true Viterbi parse may have a lower
score in terms of dependencies, in which case
the parser slows down and may degrade the ac-
curacy. HEADFIRST, in contract, does not suffer
from such conflicts. Second, by fixing the direc-
tion of arcs, the prediction of heads becomes eas-
ier, meaning that the dependency predictions be-
come more reliable. Later we show that this is in
fact the case for existing dependency parsers (see
Section 5), and in practice, we find that this simple
conversion rule leads to the higher parsing scores
than LEWISRULE on English (Section 6).

5 Tri-training

We extend the existing tri-training method to our
models and apply it to our English parsers.

Tri-training is one of the semi-supervised meth-
ods, in which the outputs of two parsers on un-
labeled data are intersected to create (silver) new
training data. This method is successfully applied
to dependency parsing (Weiss et al., 2015) and
CCG supertagging (Lewis et al., 2016).

We simply combine the two previous ap-
proaches. Lewis et al. (2016) obtain their sil-
ver data annotated with the high quality supertags.
Since they make this data publicly available 5, we
obtain our silver data by assigning dependency

5https://github.com/uwnlp/taggerflow

281



structures on top of them.6

We train two very different dependency parsers
from the training data extracted from CCGbank
Section 02-21. This training data differs depend-
ing on our dependency conversion strategies (Sec-
tion 4). For LEWISRULE, we extract the orig-
inal dependency annotations of CCGbank. For
HEADFIRST, we extract the head first dependen-
cies from the CCG trees. Note that we cannot an-
notate dependency labels so we assign a dummy
“none” label to every arc. The first parser is
graph-based RBGParser (Lei et al., 2014) with
the default settings except that we train an unla-
beled parser and use word embeddings of Turian
et al. (2010). The second parser is transition-based
lstm-parser (Dyer et al., 2015) with the de-
fault parameters.

On the development set (Section 00), with
LEWISRULE dependencies RBGParser shows
93.8% unlabeled attachment score while that of
lstm-parser is 92.5% using gold POS tags.
Interestingly, the parsers with HEADFIRST de-
pendencies achieve higher scores: 94.9% by
RBGParser and 94.6% by lstm-parser, sug-
gesting that HEADFIRST dependencies are easier
to parse. For both dependencies, we obtain more
than 1.7 million sentences on which two parsers
agree.

Following Lewis et al. (2016), we include 15
copies of CCGbank training set when using these
silver data. Also to make effects of the tri-train
samples smaller we multiply their loss by 0.4.

6 Experiments

We perform experiments on English and Japanese
CCGbanks.

6.1 English Experimental Settings

We follow the standard data splits and use Sections
02-21 for training, Section 00 for development,
and Section 23 for final evaluation. We report la-
beled and unlabeled F1 of the extracted CCG se-
mantic dependencies obtained using generate
program supplied with C&C parser.

For our models, we adopt the pruning strate-
gies in Lewis and Steedman (2014) and allow at
most 50 categories per word, use a variable-width
beam with β = 0.00001, and utilize a tag dictio-
nary, which maps frequent words to the possible

6We annotate POS tags on this data using Stanford POS
tagger (Toutanova et al., 2003).

supertags7. Unless otherwise stated, we only al-
low normal form parses (Eisner, 1996; Hocken-
maier and Bisk, 2010), choosing the same subset
of the constraints as Lewis and Steedman (2014).

We use as word representation the concatena-
tion of word vectors initialized to GloVe8 (Pen-
nington et al., 2014), and randomly initialized pre-
fix and suffix vectors of the length 1 to 4, which
is inspired by Lewis et al. (2016). All affixes ap-
pearing less than two times in the training data are
mapped to “UNK”.

Other model configurations are: 4-layer bi-
LSTMs with left and right 300-dimensional
LSTMs, 1-layer 100-dimensional MLPs with
ELU non-linearity (Clevert et al., 2015) for all
MLP depchild, MLP

dep
head, MLP

tag
child and MLP

tag
head,

and the Adam optimizer with β1 = 0.9, β2 = 0.9,
L2 norm (1e−6), and learning rate decay with the
ratio 0.75 for every 2,500 iteration starting from
2e−3, which is shown to be effective for training
the biaffine parser (Dozat and Manning, 2016).

6.2 Japanese Experimental Settings
We follow the default train/dev/test splits of
Japanese CCGbank (Uematsu et al., 2013). For
the baselines, we use an existing shift-reduce CCG
parser implemented in an NLP tool Jigg9 (Noji
and Miyao, 2016), and our implementation of the
supertag-factored model using bi-LSTMs.

For Japanese, we use as word representation
the concatenation of word vectors initialized to
Japanese Wikipedia Entity Vector10, and 100-
dimensional vectors computed from randomly
initialized 50-dimensional character embeddings
through convolution (dos Santos and Zadrozny,
2014). We do not use affix vectors as affixes are
less informative in Japanese. All characters ap-
pearing less than two times are mapped to “UNK”.
We use the same parameter settings as English for
bi-LSTMs, MLPs, and optimization.

One issue in Japanese experiments is evalua-
tion. The Japanese CCGbank is encoded in a dif-
ferent format than the English bank, and no stan-
dalone script for extracting semantic dependen-
cies is available yet. For this reason, we evaluate
the parser outputs by converting them to bunsetsu

7We use the same tag dictionary provided with their bi-
LSTM model.

8http://nlp.stanford.edu/projects/
glove/

9https://github.com/mynlp/jigg
10http://www.cl.ecei.tohoku.ac.jp/

˜m-suzuki/jawiki_vector/

282



Method Labeled Unlabeled
CCGbank

LEWISRULE w/o dep 85.8 91.7
LEWISRULE 86.0 92.5
HEADFIRST w/o dep 85.6 91.6
HEADFIRST 86.6 92.8

Tri-training
LEWISRULE 86.9 93.0
HEADFIRST 87.6 93.3

Table 1: Parsing results (F1) on English develop-
ment set. “w/o dep” means that the model discards
dependency components at prediction.

Method Labeled Unlabeled # violations
CCGbank

LEWISRULE w/o dep 85.8 91.7 2732
LEWISRULE 85.4 92.2 283
HEADFIRST w/o dep 85.6 91.6 2773
HEADFIRST 86.8 93.0 89

Tri-training
LEWISRULE 86.7 92.8 253
HEADFIRST 87.7 93.5 66

Table 2: Parsing results (F1) on English develop-
ment set when excluding the normal form con-
straints. # violations is the number of combina-
tions violating the constraints on the outputs.

dependencies, the syntactic representation ordi-
nary used in Japanese NLP (Kudo and Matsumoto,
2002). Given a CCG tree, we obtain this by first
segment a sentence into bunsetsu (chunks) using
CaboCha11 and extract dependencies that cross a
bunsetsu boundary after obtaining the word-level,
head final dependencies as in Figure 4b. For ex-
ample, the sentence in Figure 4e is segmented as
“Boku wa | eigo wo | hanashi tai”, from which we
extract two dependencies (Boku wa) ← (hanashi
tai) and (eigo wo) ← (hanashi tai). We perform
this conversion for both gold and output CCG trees
and calculate the (unlabeled) attachment accuracy.
Though this is imperfect, it can detect important
parse errors such as attachment errors and thus can
be a good proxy for the performance as a CCG
parser.

6.3 English Parsing Results
Effect of Dependency We first see how the de-
pendency components added in our model affect
the performance. Table 1 shows the results on the
development set with the several configurations,
in which “w/o dep” means discarding the depen-

11http://taku910.github.io/cabocha/

Method Labeled Unlabeled
CCGbank
C&C (Clark and Curran, 2007) 85.5 91.7
w/ LSTMs (Vaswani et al., 2016) 88.3 -
EasySRL (Lewis et al., 2016) 87.2 -
EasySRL reimpl 86.8 92.3
HEADFIRST w/o NF (Ours) 87.7 93.4

Tri-training
EasySRL (Lewis et al., 2016) 88.0 92.9
neuralccg (Lee et al., 2016) 88.7 93.7
HEADFIRST w/o NF (Ours) 88.8 94.0

Table 3: Parsing results (F1) on English test
set (Section 23).

dency terms of the model and applying the attach
low heuristics (Section 1) instead (i.e., a supertag-
factored model; Section 2.1). We can see that for
both LEWISRULE and HEADFIRST, adding de-
pendency terms improves the performance.

Choice of Dependency Conversion Rule To
our surprise, our simple HEADFIRST strategy al-
ways leads to better results than the linguistically
motivated LEWISRULE. The absolute improve-
ments by tri-training are equally large (about 1.0
points), suggesting that our model with dependen-
cies can also benefit from the silver data.

Excluding Normal Form Constraints One ad-
vantage of HEADFIRST is that the direction of
arcs is always right, making the structures sim-
pler and more parsable (Section 5). From another
viewpoint, this fixed direction means that the con-
stituent structure behind a (head first) dependency
tree is unique. Since the constituent structures of
CCGbank trees basically follow the normal form
(NF), we hypothesize that the model learned with
HEADFIRST has an ability to force the outputs in
NF automatically. We summarize the results with-
out the NF constraints in Table 2, which shows
that the above argument is correct; the number
of violating NF rules on the outputs of HEAD-
FIRST is much smaller than that of LEWISRULE
(89 vs. 283). Interestingly the scores of HEAD-
FIRST slightly increase from the models with NF
(e.g., 86.8 vs. 86.6 for CCGbank), suggesting that
the NF constraints hinder the search of HEAD-
FIRST models occasionally.

Results on Test Set Parsing results on the test
set (Section 23) are shown in Table 3, where we
compare our best performing HEADFIRST depen-
dency model without NF constraints with the sev-
eral existing parsers. In the CCGbank experi-

283



EasySRL reimpl neuralccg Ours
Tagging 24.8 21.7 16.6
A* Search 185.2 16.7 114.6
Total 21.9 9.33 14.5

Table 4: Results of the efficiency experiment,
where each number is the number of sentences
processed per second. We compare our proposed
parser against neuralccg and our reimplemen-
tation of EasySRL.

ment, our parser shows the better result than all
the baseline parsers except C&C with an LSTM
supertagger (Vaswani et al., 2016). Our parser
outperforms EasySRL by 0.5% and our reimple-
mentation of that parser (EasySRL reimpl) by
0.9% in terms of labeled F1. In the tri-training
experiment, our parser shows much increased per-
formance of 88.8% labeled F1 and 94.0% unla-
beled F1, outperforming the current state-of-the-
art neuralccg (Lee et al., 2016) that uses recur-
sive neural networks by 0.1 point and 0.3 point in
terms of labeled and unlabeled F1. This is the best
reported F1 in English CCG parsing.

Efficiency Comparison We compare the ef-
ficiency of our parser with neuralccg and
EasySRL reimpl.12 The results are shown
in Table 4. For the overall speed (the third
row), our parser is faster than neuralccg al-
though lags behind EasySRL reimpl. Inspect-
ing the details, our supertagger runs slower than
those of neuralccg and EasySRL reimpl,
while in A* search our parser processes over 7
times more sentences than neuralccg. The
delay in supertagging can be attributed to sev-
eral factors, in particular the differences in net-
work architectures including the number of bi-
LSTM layers (4 vs. 2) and the use of bilin-
ear transformation instead of linear one. There
are also many implementation differences in our
parser (C++A* parser with neural network model
implemented with Chainer (Tokui et al., 2015))
and neuralccg (Java parser with C++ Tensor-
Flow (Abadi et al., 2015) supertagger and recur-
sive neural model in C++ DyNet (Neubig et al.,
2017)).

6.4 Japanese Parsing Result
We show the results of the Japanese parsing exper-
iment in Table 5. The simple application of Lewis

12This experiment is performed on a laptop with 4-thread
2.0 GHz CPU.

Method Category Bunsetsu Dep.
Noji and Miyao (2016) 93.0 87.5
Supertag model 93.7 81.5
LEWISRULE (Ours) 93.8 90.8
HEADFINAL (Ours) 94.1 91.5

Table 5: Results of Japanese CCGbank.

Yesterday buy−PAST curry−ACC eat−PAST
Kinoo kat − ta karee − wo tabe − ta
S/S S NP S\NP

>
S un

NP/NP
>

NP
<

S

Yesterday buy−PAST curry−ACC eat−PAST
Kinoo kat − ta karee − wo tabe − ta
S/S S NP S\NP

un
NP/NP

>
NP

<
S

>
S

Figure 5: Examples of ambiguous Japanese sen-
tence given fixed supertags. The English transla-
tion is “I ate the curry I bought yesterday”.

et al. (2016) (Supertag model) is not effective for
Japanese, showing the lowest attachment score of
81.5%. We observe a performance boost with our
method, especially with HEADFINAL dependen-
cies, which outperforms the baseline shift-reduce
parser by 1.1 points on category assignments and
4.0 points on bunsetsu dependencies.

The degraded results of the simple application
of the supertag-factored model can be attributed to
the fact that the structure of a Japanese sentence
is still highly ambiguous given the supertags (Fig-
ure 5). This is particularly the case in construc-
tions where phrasal adverbial/adnominal modi-
fiers (with the supertag S/S) are involved. The
result suggests the importance of modeling depen-
dencies in some languages, at least Japanese.

7 Related Work

There is some past work that utilizes dependencies
in lexicalized grammar parsing, which we review
briefly here.

For Head-driven Phrase Structure Gram-
mar (HPSG; Pollard and Sag (1994)), there are
studies to use the predicted dependency structure
to improve HPSG parsing accuracy. Sagae et al.
(2007) use dependencies to constrain the form
of the output tree. As in our method, for every
rule (schema) application they define which child
becomes the head and impose a soft constraint
that these dependencies agree with the output of
the dependency parser. Our method is different

284



in that we do not use the one-best dependency
structure alone, but rather we search for a CCG
tree that is optimal in terms of dependencies
and CCG supertags. Zhang et al. (2010) use the
syntactic dependencies in a different way, and
show that dependency-based features are useful
for predicting HPSG supertags.

In the CCG parsing literature, some work op-
timizes a dependency model, instead of supertags
or a derivation (Clark and Curran, 2007; Xu et al.,
2014). This approach is reasonable given that the
objective matches the evaluation metric. Instead
of modeling dependencies alone, our method finds
a CCG derivation that has a higher dependency
score. Lewis et al. (2015) present a joint model
of CCG parsing and semantic role labeling (SRL),
which is closely related to our approach. They
map each CCG semantic dependency to an SRL
relation, for which they give the A* upper bound
by the score from a predicate to the most proba-
ble argument. Our approach is similar; the largest
difference is that we instead model syntactic de-
pendencies from each token to its head, and this is
the key to our success. Since dependency parsing
can be formulated as independent head selections
similar to tagging, we can build the entire model
on LSTMs to exploit features from the whole sen-
tence. This formulation is not straightforward in
the case of multi-headed semantic dependencies in
their model.

8 Conclusion

We have presented a new A* CCG parsing
method, in which the probability of a CCG tree
is decomposed into local factors of the CCG cat-
egories and its dependency structure. By explic-
itly modeling the dependency structure, we do not
require any deterministic heuristics to resolve at-
tachment ambiguities, and keep the model locally
factored so that all the probabilities can be pre-
computed before running the search. Our parser
efficiently finds the optimal parse and achieves the
state-of-the-art performance in both English and
Japanese parsing.

Acknowledgments

We are grateful to Mike Lewis for answering
our questions and your Github repository from
which we learned many things. We also thank
Yuichiro Sawai for the faster LSTM implementa-
tion. This work was in part supported by JSPS

KAKENHI Grant Number 16H06981, and also by
JST CREST Grant Number JPMJCR1301.

References
Martı́n Abadi, Ashish Agarwal, Paul Barham, Eugene

Brevdo, Zhifeng Chen, Craig Citro, Greg S. Cor-
rado, Andy Davis, Jeffrey Dean, Matthieu Devin,
Sanjay Ghemawat, Ian Goodfellow, Andrew Harp,
Geoffrey Irving, Michael Isard, Yangqing Jia, Rafal
Jozefowicz, Lukasz Kaiser, Manjunath Kudlur, Josh
Levenberg, Dan Mané, Rajat Monga, Sherry Moore,
Derek Murray, Chris Olah, Mike Schuster, Jonathon
Shlens, Benoit Steiner, Ilya Sutskever, Kunal Tal-
war, Paul Tucker, Vincent Vanhoucke, Vijay Va-
sudevan, Fernanda Viégas, Oriol Vinyals, Pete War-
den, Martin Wattenberg, Martin Wicke, Yuan Yu,
and Xiaoqiang Zheng. 2015. TensorFlow: Large-
Scale Machine Learning on Heterogeneous Sys-
tems. Software available from tensorflow.org.
http://tensorflow.org/.

Srinivas Bangalore and Aravind K Joshi. 1999. Su-
pertagging: An Approach to Almost Parsing. Com-
putational linguistics 25(2):237–265.

Stephen Clark and James R. Curran. 2007. Wide-
Coverage Efficient Statistical Parsing with CCG
and Log-Linear Models. Computational Lin-
guistics, Volume 33, Number 4, December 2007
http://aclweb.org/anthology/J07-4004.

Djork-Arné Clevert, Thomas Unterthiner, and
Sepp Hochreiter. 2015. Fast and Accurate
Deep Network Learning by Exponential Lin-
ear Units (ELUs). CoRR abs/1511.07289.
http://arxiv.org/abs/1511.07289.

Cı́cero Nogueira dos Santos and Bianca Zadrozny.
2014. Learning Character-level Representations for
Part-of-Speech Tagging. ICML.

Timothy Dozat and Christopher D. Manning.
2016. Deep Biaffine Attention for Neural
Dependency Parsing. CoRR abs/1611.01734.
http://arxiv.org/abs/1611.01734.

Chris Dyer, Miguel Ballesteros, Wang Ling, Austin
Matthews, and A. Noah Smith. 2015. Transition-
Based Dependency Parsing with Stack Long Short-
Term Memory. In Proceedings of the 53rd Annual
Meeting of the Association for Computational Lin-
guistics and the 7th International Joint Conference
on Natural Language Processing (Volume 1: Long
Papers). Association for Computational Linguistics,
pages 334–343. https://doi.org/10.3115/v1/P15-
1033.

Jason Eisner. 1996. Efficient Normal-Form Parsing for
Combinatory Categorial Grammar. In 34th Annual
Meeting of the Association for Computational Lin-
guistics. http://aclweb.org/anthology/P96-1011.

285



Julia Hockenmaier and Yonatan Bisk. 2010. Normal-
form parsing for Combinatory Categorial Grammars
with generalized composition and type-raising. In
Proceedings of the 23rd International Conference
on Computational Linguistics (Coling 2010). Col-
ing 2010 Organizing Committee, pages 465–473.
http://aclweb.org/anthology/C10-1053.

Julia Hockenmaier and Mark Steedman. 2007. CCG-
bank: A Corpus of CCG Derivations and Depen-
dency Structures Extracted from the Penn Tree-
bank. Computational Linguistics 33(3):355–396.
http://www.aclweb.org/anthology/J07-3004.

Eliyahu Kiperwasser and Yoav Goldberg. 2016.
Simple and Accurate Dependency Parsing
Using Bidirectional LSTM Feature Repre-
sentations. Transactions of the Association
for Computational Linguistics 4:313–327.
https://www.transacl.org/ojs/index.php/tacl/article/view/885.

Dan Klein and Christopher D. Manning. 2003. A*
Parsing: Fast Exact Viterbi Parse Selection. In
Proceedings of the 2003 Human Language Tech-
nology Conference of the North American Chapter
of the Association for Computational Linguistics.
http://aclweb.org/anthology/N03-1016.

Taku Kudo and Yuji Matsumoto. 2002. Japanese
Dependency Analysis using Cascaded Chunking.
In Proceedings of the 6th Conference on Natu-
ral Language Learning, CoNLL 2002, Held in
cooperation with COLING 2002, Taipei, Taiwan,
2002. http://aclweb.org/anthology/W/W02/W02-
2016.pdf.

Kenton Lee, Mike Lewis, and Luke Zettlemoyer.
2016. Global Neural CCG Parsing with Op-
timality Guarantees. In Proceedings of the
2016 Conference on Empirical Methods in
Natural Language Processing. Association for
Computational Linguistics, pages 2366–2376.
http://aclweb.org/anthology/D16-1262.

Tao Lei, Yu Xin, Yuan Zhang, Regina Barzilay, and
Tommi Jaakkola. 2014. Low-Rank Tensors for
Scoring Dependency Structures. In Proceedings
of the 52nd Annual Meeting of the Association for
Computational Linguistics (Volume 1: Long Pa-
pers). Association for Computational Linguistics,
pages 1381–1391. https://doi.org/10.3115/v1/P14-
1130.

Mike Lewis, Luheng He, and Luke Zettlemoyer. 2015.
Joint A* CCG Parsing and Semantic Role Labelling.
In Proceedings of the 2015 Conference on Empiri-
cal Methods in Natural Language Processing. Asso-
ciation for Computational Linguistics, pages 1444–
1454. https://doi.org/10.18653/v1/D15-1169.

Mike Lewis, Kenton Lee, and Luke Zettlemoyer.
2016. LSTM CCG Parsing. In Proceedings
of the 2016 Conference of the North American

Chapter of the Association for Computational Lin-
guistics: Human Language Technologies. Associa-
tion for Computational Linguistics, pages 221–231.
https://doi.org/10.18653/v1/N16-1026.

Mike Lewis and Mark Steedman. 2014. A* CCG Pars-
ing with a Supertag-factored Model. In Proceed-
ings of the 2014 Conference on Empirical Methods
in Natural Language Processing (EMNLP). Asso-
ciation for Computational Linguistics, pages 990–
1000. https://doi.org/10.3115/v1/D14-1107.

Graham Neubig, Chris Dyer, Yoav Goldberg, Austin
Matthews, Waleed Ammar, Antonios Anastasopou-
los, Miguel Ballesteros, David Chiang, Daniel
Clothiaux, Trevor Cohn, Kevin Duh, Manaal
Faruqui, Cynthia Gan, Dan Garrette, Yangfeng Ji,
Lingpeng Kong, Adhiguna Kuncoro, Gaurav Ku-
mar, Chaitanya Malaviya, Paul Michel, Yusuke
Oda, Matthew Richardson, Naomi Saphra, Swabha
Swayamdipta, and Pengcheng Yin. 2017. DyNet:
The Dynamic Neural Network Toolkit. arXiv
preprint arXiv:1701.03980 .

Hiroshi Noji and Yusuke Miyao. 2016. Jigg:
A Framework for an Easy Natural Language
Processing Pipeline. In Proceedings of ACL-
2016 System Demonstrations. Association for
Computational Linguistics, pages 103–108.
https://doi.org/10.18653/v1/P16-4018.

Jeffrey Pennington, Richard Socher, and Christo-
pher D. Manning. 2014. GloVe: Global Vectors for
Word Representation. In Empirical Methods in Nat-
ural Language Processing (EMNLP). pages 1532–
1543. http://www.aclweb.org/anthology/D14-1162.

Carl Pollard and Ivan A Sag. 1994. Head-driven
phrase structure grammar. University of Chicago
Press.

Kenji Sagae, Yusuke Miyao, and Jun’ichi Tsujii.
2007. HPSG Parsing with Shallow Dependency
Constraints. In Proceedings of the 45th Annual
Meeting of the Association of Computational Lin-
guistics. Association for Computational Linguistics,
pages 624–631. http://aclweb.org/anthology/P07-
1079.

Mark Steedman. 2000. The Syntactic Process. The
MIT Press.

Seiya Tokui, Kenta Oono, Shohei Hido, and Justin
Clayton. 2015. Chainer: a Next-Generation
Open Source Framework for Deep Learn-
ing. In Proceedings of Workshop on Ma-
chine Learning Systems (LearningSys) in The
Twenty-ninth Annual Conference on Neu-
ral Information Processing Systems (NIPS).
http://learningsys.org/papers/LearningSys 2015 paper 33.pdf.

Kristina Toutanova, Dan Klein, Christopher D. Man-
ning, and Yoram Singer. 2003. Feature-Rich Part-
of-Speech Tagging with a Cyclic Dependency Net-
work. In Proceedings of the 2003 Human Lan-
guage Technology Conference of the North Ameri-
can Chapter of the Association for Computational

286



Linguistics. http://www.aclweb.org/anthology/N03-
1033.

Joseph Turian, Lev-Arie Ratinov, and Yoshua Ben-
gio. 2010. Word Representations: A Simple and
General Method for Semi-Supervised Learning. In
Proceedings of the 48th Annual Meeting of the As-
sociation for Computational Linguistics. Associa-
tion for Computational Linguistics, pages 384–394.
http://aclweb.org/anthology/P10-1040.

Kiyotaka Uchimoto, Satoshi Sekine, and Hitoshi
Isahara. 1999. Japanese Dependency Structure
Analysis Based on Maximum Entropy Models.
In Ninth Conference of the European Chapter
of the Association for Computational Linguistics.
http://aclweb.org/anthology/E99-1026.

Sumire Uematsu, Takuya Matsuzaki, Hiroki Hanaoka,
Yusuke Miyao, and Hideki Mima. 2013. Inte-
grating Multiple Dependency Corpora for Induc-
ing Wide-coverage Japanese CCG Resources. In
Proceedings of the 51st Annual Meeting of the
Association for Computational Linguistics (Vol-
ume 1: Long Papers). Association for Compu-
tational Linguistics, Sofia, Bulgaria, pages 1042–
1051. http://www.aclweb.org/anthology/P13-1103.

Ashish Vaswani, Yonatan Bisk, Kenji Sagae, and Ryan
Musa. 2016. Supertagging With LSTMs. In Pro-
ceedings of the 2016 Conference of the North Amer-
ican Chapter of the Association for Computational
Linguistics: Human Language Technologies. Asso-
ciation for Computational Linguistics, pages 232–
237. https://doi.org/10.18653/v1/N16-1027.

David Weiss, Chris Alberti, Michael Collins, and
Slav Petrov. 2015. Structured Training for Neu-
ral Network Transition-Based Parsing. In Proceed-
ings of the 53rd Annual Meeting of the Associa-
tion for Computational Linguistics and the 7th In-
ternational Joint Conference on Natural Language
Processing (Volume 1: Long Papers). Associa-
tion for Computational Linguistics, pages 323–333.
https://doi.org/10.3115/v1/P15-1032.

Wenduan Xu, Stephen Clark, and Yue Zhang. 2014.
Shift-Reduce CCG Parsing with a Dependency
Model. In Proceedings of the 52nd Annual Meet-
ing of the Association for Computational Lin-
guistics (Volume 1: Long Papers). Association
for Computational Linguistics, pages 218–227.
https://doi.org/10.3115/v1/P14-1021.

Yao-zhong Zhang, Takuya Matsuzaki, and Jun’ichi
Tsujii. 2010. A Simple Approach for HPSG Su-
pertagging Using Dependency Information. In Hu-
man Language Technologies: The 2010 Annual
Conference of the North American Chapter of the
Association for Computational Linguistics. Associ-
ation for Computational Linguistics, pages 645–648.
http://aclweb.org/anthology/N10-1090.

287


	A* CCG Parsing with a Supertag and Dependency Factored Model

