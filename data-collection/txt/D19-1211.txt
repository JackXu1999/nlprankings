



















































UR-FUNNY: A Multimodal Language Dataset for Understanding Humor


Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing
and the 9th International Joint Conference on Natural Language Processing, pages 2046–2056,
Hong Kong, China, November 3–7, 2019. c©2019 Association for Computational Linguistics

2046

UR-FUNNY: A Multimodal Language Dataset for Understanding Humor

Md Kamrul Hasan1*, Wasifur Rahman1*, Amir Zadeh2, Jianyuan Zhong1,
Md Iftekhar Tanveer1 †, Louis-Philippe Morency2, Mohammed (Ehsan) Hoque1

1 - Department of Computer Science, University of Rochester, USA
2 - Language Technologies Institute, SCS, CMU, USA

{mhasan8,echowdh2}@cs.rochester.edu, abagherz@cs.cmu.edu,
jzhong9@u.rochester.edu, itanveer@cs.rochester.edu,

morency@cs.cmu.edu, mehoque@cs.rochester.edu

I can put a bizarre 
new idea in your 
mind right now.

I can say,  imagine a 
jellyfish waltzing in a 
library while thinking 

about quantum 
mechanics.

Now, if everything has gone 
relatively well in your life, 

you probably haven’t had that 
thought before. 

So because of this 
ability, we humans 
are able to transmit 
knowledge across 

time.

Your brain takes 
those vibrations from 

your eardrums and 
transforms them into 

thoughts. 

Vi
de
o

Au
di
o

Te
xt

Humor?
Yes
No

Context Punchline

t=0:41 t=1:15 t=1:20t=1:09t=1:03t=0:52

Figure 1: An example of the UR-FUNNY dataset. UR-FUNNY presents a framework to study the dynamics of
humor in multimodal language. Machine learning models are given a sequence of sentences with the accompanying
modalities of visual and acoustic. Their goal is to detect whether or not the sequence will trigger immediate
laughter after the punchline.

Abstract

Humor is a unique and creative communica-
tive behavior often displayed during social in-
teractions. It is produced in a multimodal man-
ner, through the usage of words (text), gestures
(visual) and prosodic cues (acoustic). Un-
derstanding humor from these three modali-
ties falls within boundaries of multimodal lan-
guage; a recent research trend in natural lan-
guage processing that models natural language
as it happens in face-to-face communication.
Although humor detection is an established
research area in NLP, in a multimodal con-
text it has been understudied. This paper
presents a diverse multimodal dataset, called
UR-FUNNY, to open the door to understand-
ing multimodal language used in expressing
humor. The dataset and accompanying stud-
ies, present a framework in multimodal humor
detection for the natural language processing
community. UR-FUNNY is publicly available
for research.

1 Introduction

Humor is a unique communication skill that re-
moves barriers in conversations. Research shows
that effective use of humor allows a speaker
to establish rapport (Stauffer, 1999), grab atten-
tion (Wanzer et al., 2010), introduce a difficult
concept without confusing the audience (Garner,
2005) and even to build trust (Vartabedian and
Vartabedian, 1993). During face-to-face com-
munications, humor involves multimodal com-
municative channels including effective use of
words (text), accompanying gestures (visual) and
prosodic cues (acoustic). Being able to mix
and align those modalities appropriately is often
unique to individuals, attributing to many differ-
ent styles. Styles include gradually building up
to a punchline using words, gestures and prosodic
cues, a sudden twist to the story with an unex-

* - Equal contribution ; †- Currently affiliated with
Comcast Applied AI Research, Washington DC, USA.



2047

pected punchline (Ramachandran, 1998), creating
a discrepancy between modalities (e.g., something
funny being said without any emotion), or just
laughing with the speech to stimulate the audience
to mirror the laughter (Provine, 1992).

Modeling humor using a computational frame-
work is inherently challenging due to factors such
as: 1) Idiosyncrasy: often humorous people are
also the most creative ones (Hauck and Thomas,
1972). This creativity in turn adds to the dynamic
complexity of how humor is expressed in a multi-
modal manner. Use of words, gestures, prosodic
cues and their (mis)alignments are choices that a
creative user often experiments with. 2) Contex-
tual Dependencies: humor often develops through
time as speakers plan for a punchline in advance.
There is a gradual build up in the story with a
sudden twist using a punchline (Ramachandran,
1998). Some punchlines when viewed in isola-
tion (as illustrated in Figure 1) may not appear
funny. The humor stems from the prior build up,
cross-referencing multiple sources, and its deliv-
ery. Therefore, a full understanding of humor re-
quires analyzing the context of the punchline.

Understanding the unique dependencies across
modalities and its impact on humor require knowl-
edge from multimodal language; a recent research
trend in the field of natural language processing
(Zadeh et al., 2018b). Studies in this area aim to
explain natural language from three modalities of
text, visual and acoustic. In this paper, alongside
computational descriptors for text, gestures such
as smile or vocal properties such as loudness are
measured and put together in a multimodal frame-
work to define humor recognition as a multimodal
task.

The main contribution of this paper to the NLP
community is introducing the first multimodal lan-
guage (including text, visual and acoustic modal-
ities) dataset of humor detection named “UR-
FUNNY”. This dataset opens the door to un-
derstanding and modeling humor in a multimodal
framework. The studies in this paper present per-
formance baselines for this task and demonstrate
the impact of using all three modalities together
for humor modeling.

2 Background

The dataset and experiments in this paper are con-
nected to the following areas:
Humor Analysis: Humor analysis has been

Dataset #Pos #Neg Mod type #spk
16000 One-Liners 16000 16000 {t} joke -
Pun of the Day 2423 2423 {t} pun -
PTT Jokes 1425 2551 {t} political -
Ted Laughter 4726 4726 {t} speech 1192
Big Bang Theory 18691 24981 {t,a} tv show <50
UR-FUNNY 8257 8257 {t,a,v} speech 1741

Table 1: Comparison between UR-FUNNY and no-
table humor detection datasets in the NLP community.
Here, ‘#’,‘pos’, ’neg’ , ‘mod’ and ‘spk’ denote number,
positive, negative, modalities and speaker respectively.

among active areas of research in both natural lan-
guage processing and affective computing. No-
table datasets in this area include “16000 One-
Liners” (Mihalcea and Strapparava, 2005), “Pun
of the Day” (Yang et al., 2015), “PTT Jokes”
(Chen and Soo, 2018), “Ted Laughter” (Chen and
Lee, 2017), and “Big Bang Theory” (Bertero
et al., 2016). The above datasets have studied
humor from different perspectives. For example,
“16000 One-Liner” and “Pun of the Day” focus
on joke detection (binary task), while “Ted Laugh-
ter” focuses on punchline detection (whether or
not punchline triggers laughter). Similar to “Ted
Laughter”, UR-FUNNY focuses on punchline de-
tection. Furthermore, punchline is accompanied
by context sentences to properly model the build
up of humor. Unlike previous datasets where neg-
ative samples were drawn from a different do-
main, UR-FUNNY uses a challenging negative
sampling case where samples are drawn from the
same videos. Furthermore, UR-FUNNY is the
only humor detection dataset which incorporates
all three modalities of text, visual and acoustic.
Table 1 shows a comparison between previously
proposed datasets and UR-FUNNY dataset.

From modeling aspect, humor detection is done
using hand-crafted and non-neural models (Yang
et al., 2015), neural based RNN and CNN models
on Yelp (de Oliveira et al., 2017) and TED talks
data (Chen and Lee, 2017). Newer approaches
have used (Chen and Soo, 2018) highway net-
works on “16000 One-Liner” and “Pun of the
Day” datasets. There have been very few attempts
at using extra modalities alongside language for
detecting humor, mostly limited to adding simple
audio features (Rakov and Rosenberg, 2013; Bert-
ero et al., 2016) or the description of cartoon im-
ages (Shahaf et al., 2015). Furthermore, these at-
tempts have been restricted to certain topics and
domains (such as “Big Bang Theory” TV show



2048

(Bertero et al., 2016)).
Multimodal Language Analysis: Studying nat-
ural language from modalities of text, visual and
acoustic is a recent research trend in natural lan-
guage processing (Zadeh et al., 2018b). Notable
works in this area present novel multimodal neu-
ral architectures (Wang et al., 2019; Pham et al.,
2019; Hazarika et al., 2018; Poria et al., 2017;
Zadeh et al., 2017), multimodal fusion approaches
(Liang et al., 2018; Tsai et al., 2018; Liu et al.,
2018; Zadeh et al., 2018a; Barezi et al., 2018)
as well as resources (Poria et al., 2018a; Zadeh
et al., 2018c, 2016; Park et al., 2014; Rosas et al.,
2013; Wöllmer et al., 2013). Multimodal lan-
guage datasets mostly target multimodal sentiment
analysis (Poria et al., 2018b), emotion recognition
(Zadeh et al., 2018c; Busso et al., 2008), and per-
sonality traits recognition (Park et al., 2014). UR-
FUNNY dataset is similar to the above datasets in
diversity (speakers and topics) and size, with the
main task of humor detection. Beyond the scope
of multimodal language analysis, the dataset and
studies in this paper have similarities to other ap-
plications in multimodal machine learning such
language and vision studies, robotics, image cap-
tioning, and media description (Baltrušaitis et al.,
2019).

3 UR-FUNNY Dataset

In this section we present the UR-FUNNY dataset
1. We first discuss the data acquisition process,
and subsequently present statistics of the dataset
as well as multimodal feature extraction and vali-
dation.

3.1 Data Acquisition

A suitable dataset for the task of multimodal hu-
mor detection should be diverse in a) speakers:
modeling the idiosyncratic expressions of humor
may require a dataset with large number of speak-
ers, and b) topics: different topics exhibit different
styles of humor as the context and punchline can
be entirely different from one topic to another.

TED talks 2 are among the most diverse idea
sharing channels, in both speakers and topics.
Speakers from various backgrounds, ethnic groups
and cultures present their thoughts through a

1UR-FUNNY Dataset: https://github.com/
ROC-HCI/UR-FUNNY

2Videos on www.ted.com are publicly available for
download.

widely popular channel 3. The topics of these pre-
sentations are diverse; from scientific discoveries
to everyday ordinary events. As a result of diver-
sity in speakers and topics, TED talks span across
a broad spectrum of humor. Therefore, this plat-
form presents a unique resource for studying the
dynamics of humor in a multimodal setup.

TED videos include manual transcripts and au-
dience markers. Transcriptions are highly reliable,
which in turn allow for aligning the text and audio.
This property makes TED talks a unique resource
for newest continuous fusion trends (Chen et al.,
2017). Transcriptions also include reliably anno-
tated markers for audience behavior. Specifically,
the “laughter” marker has been used in NLP stud-
ies as an indicator of humor (Chen and Lee, 2017).
Previous studies have identified the importance of
both punchline and context in understanding and
modeling the humor. In a humorous scenario, con-
text is the gradual build up of a story and punch-
line is a sudden twist to the story which causes
laughter (Ramachandran, 1998). Using the pro-
vided laughter marker, the sentence immediately
before the marker is considered as the punchline
and the sentences prior to punchline (but after pre-
vious laughter marker) are considered context.

We collect 1866 videos as well as their tran-
scripts in English from TED portal. These 1866
videos are chosen from 1741 different speakers
and across 417 topics. The laughter markup is
used to filter out 8257 humorous punchlines from
the transcripts (Chen and Lee, 2017). The con-
text is extracted from the prior sentences to the
punchline (until the previous humor instances or
the beginning of video is reached). Using a sim-
ilar approach, 8257 negative samples are chosen
at random intervals where the last sentence is not
immediately followed by a laughter marker. The
last sentence is assumed a punchline and similar to
the positive instances, the context is chosen. This
negative sampling uses sentences from the same
distribution, as opposed to datasets which use sen-
tences from other distributions or domains as neg-
ative sample (Yang et al., 2015; Mihalcea and
Strapparava, 2005). After this negative sampling,
there is a homogeneous 50% split in the dataset
between positive and negative examples.

Using forced alignment, we mark the beginning
and end of each sentence in the video as well as

3More than 12 million subscribers on YouTube https:
//www.youtube.com/user/TEDtalksDirector

https://github.com/ROC-HCI/UR-FUNNY
https://github.com/ROC-HCI/UR-FUNNY
www.ted.com
https://www.youtube.com/user/TEDtalksDirector
https://www.youtube.com/user/TEDtalksDirector


2049

  

  (a) Distribution of Punchline Sentence Length in Number of Words

(c) Distribution of Context Length
 in Number of Sentences

               (d) Distribution of Punchline (left) and Context (right) 
             Sentence Duration in Seconds

(e) Ted Talk Categories 

<4 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39>39
0

0.01

0.02

0.03

0.04

0.05

0.06

0.07

0.08

0 1 2 3 4 >=5
0

0.05

0.1

0.15

0.2

0.25

0.3

0.35

0.4

slee
p

HIV

Guns

toy

Iran

God

pain

hack

pia
no

jazz

bee
s

novel fu
nn

y

evil

ants

iraq

sme
ll

AIDS

Islam

eb
ola

urban

PTSD

vocals

cyborg

meme

Bran
d

Brazil

glacier

De
ba

te

Syria

suicide

markets

mi
nin

g
tel

ec
om

reso
urce

s

sin
ge

r

street art

ori
ga

mi

violin

TED-Ed

(b) Distribution of Context Sentence Length in Number of Words
<4 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39>39

0

0.01

0.02

0.03

0.04

0.05

0.06

0.07

  humor       non-humor   humor        non-humor
0

2

4

6

8

10

12

14

16

0

2

4

6

8

10

12

14

non-humor
humor

non-humor
humor

non-humor
humor

Figure 2: Overview of UR-FUNNY dataset statistics. (a) the distribution of punchline sentence length for humor
and non-humor cases. (b) the distribution of context sentence length for humor and non-humor cases. (c) distri-
bution of the number of sentences in the context. (d) distribution of the duration (in seconds) of punchline and
context sentences. (e) topics of the videos in UR-FUNNY dataset. Best viewed in zoomed and color.

words and phonemes in the sentences (Yuan and
Liberman, 2008). Therefore, an alignment is es-
tablished between text, visual and acoustic. Uti-
lizing this alignment, the timing of punchline as
well as context is extracted for all instances in the
dataset.

3.2 Dataset Statistics

The high level statistics of UR-FUNNY dataset
are presented in Table 2. Total duration of the
entire dataset is 90.23 hours. There are a total
of 1741 distinct speakers and a total of 417 dis-
tinct topics in the UR-FUNNY dataset. Figure 2.e
shows the word cloud of the topics based on log-
frequency of the topic. The top most five frequent
topics are technology, science, culture, global is-
sues and design 4. There are in total 16514 video
segments of humor and not humor instances (equal
splits of 8257). The average duration of each data
instance is 19.67 seconds, with context an average
of 14.7 and punchline with an average of 4.97 sec-
onds. The average number of words in punchline
is 16.14 and the average number of words in con-
text sentences is 14.80.

Figure 2 shows an overview for some of the
important statistics of UR-FUNNY dataset. Fig-
ure 2.a demonstrates the distribution of punchline
for humor and non-humor cases based on number

4Metadata collected from www.ted.com

General
total #videos 1866
total duration in hour 90.23
total #distinct speakers 1741
total #distinct topics 417
total #humor instances 8257
total #non-humor instances 8257
total #words 965573
total #unique words 32995
total #sentences 63727
avg length of sentences in words 15.15
avg duration of sentences (s) 4.64
Punchline
#sentences in punchline 1
avg #words in punchline 16.14
avg #words in humorous punchline 15.17
avg #words in non-humorous punchline 17.10
avg duration of punchline (s) 4.97
avg duration of humorous punchline (s) 4.58
avg duration of non-humorous punchline (s) 5.36
Context
avg total #words in context 42.33
avg #words in context sentences 14.80
avg #sentences in context 2.86
avg #sentences in humorous context 2.82
avg #sentences in non-humorous context 2.90
avg duration of context (s) 14.7
avg duration of humorous context (s) 13.79
avg duration of non-humorous context (s) 15.62
avg duration of context sentences (s) 4.25
avg duration of humorous context sentences (s) 4.79
avg duration of non-humorous context sentences (s) 4.52

Table 2: Summary of the UR-FUNNY dataset statis-
tics. Here, ‘#’ denotes number, ‘avg’ denotes average
and ‘s’ denotes seconds

www.ted.com


2050

Train Val Test
#humor instances 5306 1313 1638
#not humor instances 5292 1313 1652
#videos used 1166 300 400
#speakers 1059 294 388
avg #words in punchline 15.81 16.94 16.55
avg #words in context 41.69 42.86 43.94
avg #sentences in context 2.84 2.81 2.95
punchline avg duration(second) 4.85 5.25 5.15
context avg duration(second) 14.39 14.91 15.54

Table 3: Statistics of train, validation & test folds of
UR-FUNNY dataset. Here, ‘avg’ denotes average and
‘#’ denotes number.

of words. There is no clear distinction between
humor and non-humor punchlines as both follow
similar distribution. Similarly, Figure 2.b shows
the distribution of number of words per context
sentence. Both humor and non-humor context
sentences follow the same distribution. Majority
(≥ 90%) of punchlines have length less than 32.
In terms of number of seconds, Figure 2.d shows
the distribution of punchline and context sentence
length in terms of seconds. Figure 2.c demon-
strates the distribution of number of context sen-
tences per humor and non-humor data instances.
Number of context sentences per humor and non-
humor case is also roughly the same. The statistics
in Figure 2 show that there is no trivial or degen-
erate distinctions between humor and non-humor
cases. Therefore, classification of humor versus
non-humor cases cannot be done based on simple
measures (such as number of words); it requires
understanding the content of sentences.

Table 3 shows the standard train, validation and
test folds of the UR-FUNNY dataset. These folds
share no speaker with each other - hence standard
folds are speaker independent (Zadeh et al., 2016).
This minimizes the chance of overfitting to iden-
tity of the speakers or their communication pat-
terns.

3.3 Extracted Features

For each modality, the extracted features are as
follows:

Language (text): Glove word embeddings
(Pennington et al., 2014) are used as pre-trained
word vectors for the text features. P2FA forced
alignment model (Yuan and Liberman, 2008)
is used to align the text and audio on phoneme
level. From the force alignment, we extract the
timing annotations of context and punchline on

word level. Then, the acoustic and visual cues
are aligned on word level by interpolation (Chen
et al., 2017).

Acoustic: COVAREP software (Degottex et al.,
2014) is used to extraction acoustic features at
the rate of 30 frame/sec. We extract follow-
ing 81 features: fundamental frequency (F0),
Voiced/Unvoiced segmenting features (VUV)
(Drugman and Alwan, 2011), normalized am-
plitude quotient (NAQ), quasi open quotient
(QOQ) (Kane and Gobl, 2013), glottal source
parameters (H1H2, Rd,Rd conf) (Drugman et al.,
2012; Alku et al., 2002, 1997), parabolic spectral
parameter (PSP), maxima dispersion quotient
(MDQ), spectral tilt/slope of wavelet responses
(peak/slope), Mel cepstral coefficient (MCEP
0-24), harmonic model and phase distortion mean
(HMPDM 0-24) and deviations (HMPDD 0-12),
and the first 3 formants. These acoustic features
are related to emotions and tone of speech.

Visual: OpenFace facial behavioral analysis
tool (Baltrušaitis et al., 2016) is used to extract
the facial expression features at the rate of 30
frame/sec. We extract all facial Action Units
(AU) features based on the Facial Action Coding
System (FACS) (Ekman, 1997). Rigid and non-
rigid facial shape parameters are also extracted
(Baltrušaitis et al., 2016). We observed that the
camera angle and position changes frequently
during TED presentations. However, for the
majority of time, the camera stays focused on
the presenter. Due to the volatile camera work,
the only consistently available source of visual
information was the speaker’s face.

UR-FUNNY dataset is publicly available for
download alongside all the extracted features.

4 Multimodal Humor Detection

In this section, we first outline the problem for-
mulation for performing binary multimodal hu-
mor detection on UR-FUNNY dataset. We then
proceed to study the UR-FUNNY dataset through
the lens of a contextualized extension of Memory
Fusion Network (MFN) (Zadeh et al., 2018a) - a
state-of-the-art model in multimodal language.



2051

!",$,%

!",$,$

...

!",$,&'(

!),$,%

!),$,$

...

!),$,&'(

!*,$,%

!*,$,$

...

!*,$,&'(

+ = 1 + = 2

...

...

...

+ = /0

+ = 2Context
LSTMt LSTMv LSTMa

ℎ*,$ ℎ",$ ℎ),$

2

Figure 3: The structure of Unimodal Context Network
as outlined in Section 4.2.1. For demonstration pur-
pose, we show the case for n = 2 (second context sen-
tence). After n = NC , the output H (outlined by blue)
is complete. Best viewed in color.

4.1 Problem Formulation

UR-FUNNY dataset is a multimodal dataset with
three modalities of text, visual and acoustic. We
denote the set of these modalities as M = {t, v, a}.
Each of the modalities come in a sequential form.
We assume word-level alignment between modal-
ities (Yuan and Liberman, 2008). Since frequency
of the text modality is less than visual and acous-
tic (i.e. visual and acoustic have higher sampling
rate), we use expected visual and acoustic descrip-
tors for each word (Chen et al., 2017). After
this process, each modality has the same sequence
length (each word has a single visual and acoustic
vector accompanied with it).

Each data sample in the UR-FUNNY can be de-
scribed as a triplet (l, P,C) with l being a binary
label for humor or non-humor, P is the punchline
and C is the context. Both punchline and con-
text have multiple modalities P = {Pm;m ∈ M},
C = {Cm;m ∈ M}. If there are NC context sen-
tences accompanying the punchline, then Cm =
[Cm,1,Cm,2, . . . ,Cm,NC ] - are context sentences
start from first sentence to the last (NC) sentence.
KP is the number of words in the punchline and

KCn∣NCn=1 is the number of words in each of the
context sentences respectively. Using this nota-
tion, Pm,k refers to the kth entry in the modality
m of the punchline. Similarly, Cm,n,k refers to the
kth entry in the modality m of the nth context sen-
tence.

Models developed on UR-FUNNY dataset are
trained on triplets of (l, P,C). During testing only
a tuple (P,C) is given to predict the l. Here l is
the label for laughter, specifically whether or not
the inputs P,C are likely to trigger a laughter.

4.2 Contextual Memory Fusion Baseline

Memory Fusion Network (MFN) is among the
state-of-the-art models for several multimodal
datasets (Zadeh et al., 2018a). We devise an exten-
sion of the MFN model, named Contextual Mem-
ory Fusion Network(C-MFN), as a baseline for
humor detection on UR-FUNNY dataset. This is
done by introducing two components to allow the
involvement of context in the MFN model: 1) Uni-
modal Context Network, where information from
each modality is encoded using M Long-short
Term Memories (LSTM), 2) Multimodal Context
Network, where unimodal context information are
fused (using self-attention) to extract the multi-
modal context information. We discuss the com-
ponents of the C-MFN model in the following
such sections.

4.2.1 Unimodal Context Network
To model the context, we first model each modal-
ity within the context. Unimodal Context Net-
work (Figure 3) consists of M LSTMs, one for
each modality m ∈ M denoted as LSTMm. For
each context sentence n of each modality m ∈ M ,
LSTMm is used to encode the information into a
single vector hm,n. This single vector is the last
output of the LSTMm over Cm,n as input. The
recurrence step for each LSTM is the utterance
of each word (due to word-level alignment visual
and acoustic modalities also follow this time-step).
The output of the Unimodal Context Network is
the set H = {hm,n;m ∈M,1 ≤ n < NC}.

4.2.2 Multimodal Context Network
Multimodal Context Network (Figure 4) learns a
multimodal representation of the context based on
the output H of the Unimodal Context Network.
Sentences and modalities in the context can form
complex asynchronous spatio-temporal relations.
For example, during the gradual buildup of the



2052

!

" = 1 " = 2

...

...

...

" = &'

Embedding

Self-Attention

Residual

Feed Forward

Residual

En
co

de
r

(!

Figure 4: The structure of Multimodal Context Net-
work as outlined in Section 4.2.2. The output H of the
Unimodal Context Network is connected to an encoder
module to get the multimodal output Ĥ . For the details
of components outlined in orange please refer to the
authors’ original paper. (Vaswani et al., 2017). Best
viewed in color.

context, the speaker’s facial expression may be im-
pacted due to an arbitrary previously uttered sen-
tence. Transformers (Vaswani et al., 2017) are a
family of neural models that specialize in find-
ing various temporal relations between their inputs
through self-attention. By concatenating represen-
tations hm∈M,n (i.e. for all M modalities of the
nth context), self-attention model can be applied
to find asynchronous spatio-temporal relations in
the context. We use an encoder with 6 intermedi-
ate layers to derive a multimodal representation Ĥ
conditioned on H . Ĥ is also spatio-temporal (as
produced output of encoders in a transformer are).
The output of Multimodal Context Network is the
output Ĥ of the encoder.

4.2.3 Memory Fusion Network (MFN)
After learning unimodal (H) and multimodal (Ĥ)
representations of context, we use a Memory Fu-
sion Network (MFN) (Zadeh et al., 2018a) to
model the punchline (Figure 5). MFN contains
2 types of memories: a System of LSTMs with
M unimodal memories to model each modality in
punchline, and a Multi-view Gated Memory which

System of LSTMs

Multi-view Gated
Memory

Delta-memory 
Attention

ℎ",$%& %'( )"
ℎ*,$%& %'( )*
ℎ+,$%& %'( )+

,- ∈/,$ ,- ∈/,0 ,- ∈/,12. . .

34

4

)

Figure 5: The initialization and recurrence process of
Memory Fusion Network (MFN). The outputs of Uni-
modal and Multimodal Context Networks (H and Ĥ)
are used initializing the MFN neural components. For
the details of components outlined in orange please re-
fer to the authors’ original paper (Zadeh et al., 2018a).
Best viewed in color.

stores multimodal information. We use a simple
trick to combine the Context Networks (Unimodal
and Multimodal) with the MFN: we initialize the
memories in the MFN using the outputs H (uni-
modal representation) and Ĥ (multimodal repre-
sentation). For System of LSTMs, this is done
by initializing the LSTM cell state of modality m
with Dm(hm,1≤n<NC). Dm is a fully connected
neural network that maps the information from
hm,1≥j≥NC (mth modality in context) to the cell
state of the mth LSTM in the System of LSTMs.
The Multi-view Gated Memory is initialized based
on a non-linear projection D(Ĥ) where D is a
fully connected neural network. Similar to con-
text where modalities are aligned at word level,
punchline is also aligned the same way. There-
fore a word-level implementation of the MFN is
used, where a word and accompanying visual and
acoustic descriptors of punchline are used as input
to the System of LSTMs at each time-step. The
Multi-view Gated Memory is updated iteratively
at every recurrence of the System of LSTMs using
a Delta-memory Attention Network.

The final prediction of humor is conditioned on
the last state of the System of LSTMs and Multi-
view Gated Memory using an affine mapping with
Sigmoid activation.



2053

5 Experiments

In the experiments of this paper, our goal is
to establish a performance baseline for the
UR-FUNNY dataset. Furthermore, we aim to
understand the role of context and punchline, as
well as role of individual modalities in the task
of humor detection. For all the experiments, we
use the proposed contextual extension of Memory
Fusion Network (MFN), called C-MFN (Section
4.2). Aside the proposed C-MFN model, the
following variants are also studied:

C-MFN (P): This variant of the C-MFN uses
only punchline with no contextual information.
Essentially, this is equivalent to a MFN model
since initialization trick is not used.

C-MFN (C): This variant of the C-MFN uses
only contextual information without punchline.
Essentially, this is equivalent to removing the
MFN and directly conditioning the humor pre-
diction on the Unimodal and Multimodal Context
Network outputs (Sigmoid activated neuron after
applying DM ;m ∈M on H and D on Ĥ).

The above variants of the C-MFN allow for
studying the importance of punchline and con-
text in modeling humor. Furthermore, we com-
pare the performance of the C-MFN variants in
the following scenarios: (T) a only text modal-
ity is used without visual and acoustic, (T+V) text
and visual modalities are used without acoustic,
(T+A) text and acoustic modalities are used with-
out visual, (A+V) only visual and acoustic modal-
ities are used, (T+A+V) all modalities are used to-
gether.

We compare the performance of C-MFN vari-
ants across the above scenarios. This allows for
understanding the role of context and punchline in
humor detection, as well as the importance of dif-
ferent modalities. All the models for our experi-
ments are trained using categorical cross-entropy.
This measure is calculated between the output of
the model and ground-truth labels. We also com-
pare the performance of C-MFN to a Random For-
est classifier as another strong non-neural base-
line. We use the summary features of punchline
and context for the Random Forest classifier.

Modality T A+V T+A T+V T+A+V
C-MFN (P) 62.85 53.3 63.28 63.22 64.47
C-MFN (C) 57.96 50.23 57.78 57.99 58.45
C-MFN 64.44 57.99 64.47 64.22 65.23

Table 4: Binary accuracy for different variants of C-
MFN and training scenarios outlined in Section 5. The
best performance is achieved using all three modalities
of text (T), visual (V) and acoustic (A).

6 Results and Discussion

The results of our experiments are presented in Ta-
ble 4. Results demonstrate that both context and
punchline information are important as C-MFN
outperforms C-MFN (P) and C-MFN (C) models.
Punchline is the most important component for de-
tecting humor as the performance of C-MFN (P) is
significantly higher than C-MFN (C).

Models that use all modalities (T+A+V) out-
perform models that use only one or two modal-
ities (T, T+A, T+V, A+V). Between text (T) and
nonverbal behaviors (A+V), text shows to be the
most important modality. Most of the cases, both
modalities of visual and acoustic improve the per-
formance of text alone (T+V, T+A).

Based on the above observations, each neural
component of the C-MFN model is useful in im-
proving the prediction of humor. The results also
indicate that modeling humor from a multimodal
perspective yields successful results. Furthermore,
both context and punchline are important in under-
standing humor.

The highest accuracy achieved by Random For-
est baseline (after hyper parameter tuning & us-
ing same folds as C-MFN) was 57.78%, which
is higher than random baseline but lower than C-
MFN (65.23%). In addition, C-MFN achieves
higher accuracy than similar notable unimodal
previous work (58.9%) where only punchline and
textual information were used (Chen and Lee,
2017) . The human performance 5 on the UR-
FUNNY dataset is 82.5%.

The results from Table 4 demonstrate that while
a state-of-the-art model can achieve a reason-
able level of success in modeling humor, there is
still a large gap between human-level performance
with state of the art. Therefore, UR-FUNNY

5This is calculated by averaging the performance of two
annotators over a shuffled set of 100 humor and 100 non-
humor cases. The annotators are given the same input as
the machine learning models (similar context and punchline).
The annotators agree 84% of times.



2054

dataset presents new challenges to the field of
NLP, specifically research areas of humor detec-
tion and multimodal language analysis.

7 Conclusion

In this paper, we presented a new multimodal
dataset for humor detection called UR-FUNNY.
This dataset is the first of its kind in the NLP com-
munity. Humor detection is done from the per-
spective of predicting laughter - similar to (Chen
and Lee, 2017). UR-FUNNY is diverse in both
speakers and topics. It contains three modalities
of text, visual and acoustic. We study this dataset
through the lens of a Contextualized Memory Fu-
sion Network (C-MFN). Results of our experi-
ments indicate that humor can be better modeled
if all three modalities are used together. Further-
more, both context and punchline are important in
understanding humor. The dataset and the accom-
panying experiments are publicly available.

Acknowledgment

This research was supported in part by grant
W911NF-15-1-0542 and W911NF-19-1-0029
with the US Defense Advanced Research Projects
Agency (DARPA) and the Army Research Office
(ARO).

This material is based upon work partially
supported by the National Science Foundation
(Awards #1734868 #1722822) and National In-
stitutes of Health. Any opinions, findings, and
conclusions or recommendations expressed in this
material are those of the author(s) and do not
necessarily reflect the views of National Science
Foundation or National Institutes of Health, and
no official endorsement should be inferred.

References
Paavo Alku, Tom Bäckström, and Erkki Vilk-

man. 2002. Normalized amplitude quotient for
parametrization of the glottal flow. the Journal of
the Acoustical Society of America, 112(2):701–710.

Paavo Alku, Helmer Strik, and Erkki Vilkman. 1997.
Parabolic spectral parametera new method for quan-
tification of the glottal flow. Speech Communica-
tion, 22(1):67–79.

Tadas Baltrušaitis, Chaitanya Ahuja, and Louis-
Philippe Morency. 2019. Multimodal machine
learning: A survey and taxonomy. IEEE Transac-
tions on Pattern Analysis and Machine Intelligence,
41(2):423–443.

Tadas Baltrušaitis, Peter Robinson, and Louis-Philippe
Morency. 2016. Openface: an open source fa-
cial behavior analysis toolkit. In 2016 IEEE Win-
ter Conference on Applications of Computer Vision
(WACV), pages 1–10. IEEE.

Elham J Barezi, Peyman Momeni, Pascale Fung, et al.
2018. Modality-based factorization for multimodal
fusion. arXiv preprint arXiv:1811.12624.

Dario Bertero, Pascale Fung, X Li, L Wu, Z Liu,
B Hussain, Wc Chong, Km Lau, Pc Yue, W Zhang,
et al. 2016. Deep learning of audio and language
features for humor prediction. In LREC.

Carlos Busso, Murtaza Bulut, Chi-Chun Lee, Abe
Kazemzadeh, Emily Mower, Samuel Kim, Jean-
nette N Chang, Sungbok Lee, and Shrikanth S
Narayanan. 2008. Iemocap: Interactive emotional
dyadic motion capture database. Language re-
sources and evaluation, 42(4):335.

Lei Chen and Chong Min Lee. 2017. Predicting audi-
ence’s laughter during presentations using convolu-
tional neural network. In Proceedings of the 12th
Workshop on Innovative Use of NLP for Building
Educational Applications, pages 86–90.

Minghai Chen, Sen Wang, Paul Pu Liang, Tadas Bal-
trušaitis, Amir Zadeh, and Louis-Philippe Morency.
2017. Multimodal sentiment analysis with word-
level fusion and reinforcement learning. In Proceed-
ings of the 19th ACM International Conference on
Multimodal Interaction, pages 163–171. ACM.

Peng-Yu Chen and Von-Wun Soo. 2018. Humor recog-
nition using deep learning. In Proceedings of the
2018 Conference of the North American Chapter of
the Association for Computational Linguistics: Hu-
man Language Technologies, Volume 2 (Short Pa-
pers), volume 2, pages 113–117.

Gilles Degottex, John Kane, Thomas Drugman, Tuomo
Raitio, and Stefan Scherer. 2014. Covarepa col-
laborative voice analysis repository for speech tech-
nologies. In 2014 ieee international conference on
acoustics, speech and signal processing (icassp),
pages 960–964. IEEE.

Thomas Drugman and Abeer Alwan. 2011. Joint ro-
bust voicing detection and pitch estimation based on
residual harmonics. In Twelfth Annual Conference
of the International Speech Communication Associ-
ation.

Thomas Drugman, Mark Thomas, Jon Gudnason,
Patrick Naylor, and Thierry Dutoit. 2012. Detec-
tion of glottal closure instants from speech signals:
A quantitative review. IEEE Transactions on Audio,
Speech, and Language Processing, 20(3):994–1006.

Rosenberg Ekman. 1997. What the face reveals: Basic
and applied studies of spontaneous expression using
the Facial Action Coding System (FACS). Oxford
University Press, USA.



2055

Randy Garner. 2005. Humor, analogy, and metaphor:
Ham it up in teaching. Radical Pedagogy, 6(2):1.

William E Hauck and John W Thomas. 1972. The re-
lationship of humor to intelligence, creativity, and
intentional and incidental learning. The journal of
experimental education, 40(4):52–55.

Devamanyu Hazarika, Soujanya Poria, Amir Zadeh,
Erik Cambria, Louis-Philippe Morency, and Roger
Zimmermann. 2018. Conversational memory net-
work for emotion recognition in dyadic dialogue
videos. In Proceedings of the 2018 Conference of
the North American Chapter of the Association for
Computational Linguistics: Human Language Tech-
nologies, Volume 1 (Long Papers), volume 1, pages
2122–2132.

John Kane and Christer Gobl. 2013. Wavelet maxima
dispersion for breathy to tense voice discrimination.
IEEE Transactions on Audio, Speech, and Language
Processing, 21(6):1170–1179.

Paul Pu Liang, Ziyin Liu, Amir Zadeh, and Louis-
Philippe Morency. 2018. Multimodal language
analysis with recurrent multistage fusion. arXiv
preprint arXiv:1808.03920.

Zhun Liu, Ying Shen, Varun Bharadhwaj Lakshmi-
narasimhan, Paul Pu Liang, Amir Zadeh, and Louis-
Philippe Morency. 2018. Efficient low-rank multi-
modal fusion with modality-specific factors. arXiv
preprint arXiv:1806.00064.

Rada Mihalcea and Carlo Strapparava. 2005. Making
computers laugh: Investigations in automatic humor
recognition. In Proceedings of the Conference on
Human Language Technology and Empirical Meth-
ods in Natural Language Processing, pages 531–
538. Association for Computational Linguistics.

Luke de Oliveira, ICME Stanford, and Alfredo Láinez
Rodrigo. 2017. Humor detection in yelp reviews.

Sunghyun Park, Han Suk Shim, Moitreya Chatterjee,
Kenji Sagae, and Louis-Philippe Morency. 2014.
Computational analysis of persuasiveness in social
multimedia: A novel dataset and multimodal predic-
tion approach. In Proceedings of the 16th Interna-
tional Conference on Multimodal Interaction, pages
50–57. ACM.

Jeffrey Pennington, Richard Socher, and Christopher
Manning. 2014. Glove: Global vectors for word
representation. In Proceedings of the 2014 confer-
ence on empirical methods in natural language pro-
cessing (EMNLP), pages 1532–1543.

Hai Pham, Paul Pu Liang, Thomas Manzini, Louis-
Philippe Morency, and Barnabas Poczos. 2019.
Found in translation: Learning robust joint repre-
sentations by cyclic translations between modalities.
arXiv preprint arXiv:1812.07809.

Soujanya Poria, Erik Cambria, Devamanyu Haz-
arika, Navonil Mazumder, Amir Zadeh, and Louis-
Philippe Morency. 2017. Multi-level multiple at-
tentions for contextual multimodal sentiment analy-
sis. In 2017 IEEE International Conference on Data
Mining (ICDM), pages 1033–1038. IEEE.

Soujanya Poria, Devamanyu Hazarika, Navonil Ma-
jumder, Gautam Naik, Erik Cambria, and Rada Mi-
halcea. 2018a. Meld: A multimodal multi-party
dataset for emotion recognition in conversations.
arXiv preprint arXiv:1810.02508.

Soujanya Poria, Amir Hussain, and Erik Cambria.
2018b. Multimodal Sentiment Analysis, volume 8.
Springer.

Robert R Provine. 1992. Contagious laughter: Laugh-
ter is a sufficient stimulus for laughs and smiles.
Bulletin of the Psychonomic Society, 30(1):1–4.

Rachel Rakov and Andrew Rosenberg. 2013. ” sure, i
did the right thing”: a system for sarcasm detection
in speech. In Interspeech, pages 842–846.

Vilayanur S Ramachandran. 1998. The neurology and
evolution of humor, laughter, and smiling: the false
alarm theory. Medical hypotheses, 51(4):351–354.

Verónica Pérez Rosas, Rada Mihalcea, and Louis-
Philippe Morency. 2013. Multimodal sentiment
analysis of spanish online videos. IEEE Intelligent
Systems, 28(3):38–45.

Dafna Shahaf, Eric Horvitz, and Robert Mankoff.
2015. Inside jokes: Identifying humorous cartoon
captions. In Proceedings of the 21th ACM SIGKDD
International Conference on Knowledge Discovery
and Data Mining, pages 1065–1074. ACM.

David Stauffer. 1999. Let the good times roll: Build-
ing a fun culture. Harvard Management Update,
4(10):4–6.

Yao-Hung Hubert Tsai, Paul Pu Liang, Amir Zadeh,
Louis-Philippe Morency, and Ruslan Salakhutdinov.
2018. Learning factorized multimodal representa-
tions. arXiv preprint arXiv:1806.06176.

Robert A Vartabedian and Laurel Klinger Vartabedian.
1993. Humor in the workplace: A communication
challenge.

Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob
Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz
Kaiser, and Illia Polosukhin. 2017. Attention is all
you need. In Advances in Neural Information Pro-
cessing Systems, pages 5998–6008.

Yansen Wang, Ying Shen, Zhun Liu, Paul Pu Liang,
Amir Zadeh, and Louis-Philippe Morency. 2019.
Words can shift: Dynamically adjusting word rep-
resentations using nonverbal behaviors. arXiv
preprint arXiv:1811.09362.



2056

Melissa B Wanzer, Ann B Frymier, and Jeffrey Irwin.
2010. An explanation of the relationship between
instructor humor and student learning: Instructional
humor processing theory. Communication Educa-
tion, 59(1):1–18.

Martin Wöllmer, Felix Weninger, Tobias Knaup, Björn
Schuller, Congkai Sun, Kenji Sagae, and Louis-
Philippe Morency. 2013. Youtube movie reviews:
Sentiment analysis in an audio-visual context. IEEE
Intelligent Systems, 28(3):46–53.

Diyi Yang, Alon Lavie, Chris Dyer, and Eduard Hovy.
2015. Humor recognition and humor anchor extrac-
tion. In Proceedings of the 2015 Conference on Em-
pirical Methods in Natural Language Processing,
pages 2367–2376.

Jiahong Yuan and Mark Liberman. 2008. Speaker
identification on the scotus corpus. Journal of the
Acoustical Society of America, 123(5):3878.

Amir Zadeh, Minghai Chen, Soujanya Poria, Erik
Cambria, and Louis-Philippe Morency. 2017. Ten-
sor fusion network for multimodal sentiment analy-
sis. arXiv preprint arXiv:1707.07250.

Amir Zadeh, Paul Pu Liang, Navonil Mazumder,
Soujanya Poria, Erik Cambria, and Louis-Philippe
Morency. 2018a. Memory fusion network for multi-
view sequential learning. In Thirty-Second AAAI
Conference on Artificial Intelligence.

Amir Zadeh, Paul Pu Liang, Louis-Philippe Morency,
Soujanya Poria, Erik Cambria, and Stefan Scherer.
2018b. Proceedings of grand challenge and work-
shop on human multimodal language (challenge-
hml). In Proceedings of Grand Challenge
and Workshop on Human Multimodal Language
(Challenge-HML).

Amir Zadeh, Rowan Zellers, Eli Pincus, and Louis-
Philippe Morency. 2016. Mosi: multimodal cor-
pus of sentiment intensity and subjectivity anal-
ysis in online opinion videos. arXiv preprint
arXiv:1606.06259.

AmirAli Bagher Zadeh, Paul Pu Liang, Soujanya Po-
ria, Erik Cambria, and Louis-Philippe Morency.
2018c. Multimodal language analysis in the wild:
Cmu-mosei dataset and interpretable dynamic fu-
sion graph. In Proceedings of the 56th Annual Meet-
ing of the Association for Computational Linguistics
(Volume 1: Long Papers), volume 1, pages 2236–
2246.


