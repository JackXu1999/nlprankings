



















































Duluth at SemEval-2017 Task 7 : Puns Upon a Midnight Dreary, Lexical Semantics for the Weak and Weary


Proceedings of the 11th International Workshop on Semantic Evaluations (SemEval-2017), pages 416–420,
Vancouver, Canada, August 3 - 4, 2017. c©2017 Association for Computational Linguistics

Duluth at SemEval-2017 Task 7:

Puns Upon a Midnight Dreary, Lexical Semantics for the Weak and Weary

Ted Pedersen

Department of Computer Science

University of Minnesota

Duluth, MN 55812 USA

tpederse@d.umn.edu

Abstract

This paper describes the Duluth systems

that participated in SemEval-2017 Task 7

: Detection and Interpretation of English

Puns. The Duluth systems participated in

all three subtasks, and relied on methods

that included word sense disambiguation

and measures of semantic relatedness.

1 Introduction

Puns represent a broad class of humorous word

play. This paper focuses on two types of puns,

homographic and heterographic.

A homographic pun is characterized by an os-

cillation between two senses of a single word, each

of which leads to a different but valid interpreta-

tion:

I’d like to tell you a chemistry joke but

I’m afraid of your reaction.

Here the oscillation is between two senses of

reaction. The first that comes to mind is perhaps

that of a person revealing their true feelings about

something (how they react), but then the relation-

ship to chemistry emerges and the reader realizes

that reaction can also mean the chemical sense,

where substances change into others.

Homographic puns can also be created via com-

pounding:

He had a collection of candy that was in

mint condition.

The pun relies on the oscillation between the fla-

vor mint and the compound mint condition, where

candy interacts with mint and mint condition inter-

acts with collection.

A heterographic pun relies on a different kind of

oscillation, that is between two words that nearly

sound alike, rhyme, or are nearly spelled the same.

The best angle from which to solve a

problem is the try angle.

Here the oscillation is between try angle and tri-

angle, where try suggests that the best way to solve

a problem is to try harder, and triangle is (perhaps)

the best kind of angle.

This example illustrates one of the main chal-

lenges of heterographic puns, and that is identify-

ing multi word expressions that are used as a kind

of compound, but without being a standard or typ-

ical compound (like the very non-standard try an-

gle). One reading treats try angle as a kind of mis-

spelled version of triangle while the other treats

them as two distinct words (try and angle). There

is also a kind of oscillation between senses here,

since try angle can waver back and forth between

the geometric sense and the one of making effort.

During our informal study of both hetero-

graphic and homographic puns, we observed a

fairly clear pattern where a punned word will oc-

cur towards the end of a sentence and has a sense

that is semantically related to an earlier word, and

another sense that fits the immediate context in

which it occurs. It often seemed that the sense that

fits the immediate context is a more conventional

usage (as in afraid of your reaction) and the more

amusing sense is that which connects to an earlier

word via some type of semantic relation (chemi-

cal reaction). This is more complicated in the case

of heterographic puns since the punned word can

rely on pronunciation or spelling to create the ef-

fect (i.e., try angle versus triangle). In this work

we focused on exploiting these long distance se-

mantic relations, although in future work we plan

to consider the use of language models to identify

more conventional usages.

We used two versions of the WordNet SenseRe-

late word sense disambiguation algorithm1 : Tar-

1http://senserelate.sourceforge.net

416



getWord (Patwardhan et al., 2005) and AllWords

(Pedersen and Kolhatkar, 2009). Both have the

goal of finding the assignment of senses in a con-

text that maximizes their overall semantic related-

ness (Patwardhan et al., 2003) according to mea-

sures in WordNet::Similarity2 (Pedersen et al.,

2004). We relied on the Extended Gloss

Overlaps measure (lesk) (Banerjee and Pedersen,

2003) and the Gloss vector measure (vector)

(Patwardhan and Pedersen, 2006).

The intuition behind a Lesk measure is that re-

lated words will be defined using some of the same

words, and that recognizing these overlaps can

serve as a means of identifying relationships be-

tween words (Lesk, 1986). The Extended Gloss

overlap measure (hereafter simply lesk) extends

this idea by considering not only the definitions

of the words themselves, but also concatenates the

definitions of words that are directly related via

hypernym, hyponym, and other relations accord-

ing to WordNet.

The Gloss Vector measure (hereafter simply

vector) extends this idea by representing each

word in a concatenated definition with a vector of

co-occurring words, and then creating a represen-

tation of this definition by averaging together all of

these vectors. The relatedness between two word

senses can then be measured by finding the cosine

between their respective vectors.

2 Systems

The evaluation data for each subtask was individ-

ual sentences that are independent of each other.

All sentences were tokenized so that each al-

phanumeric string was separated from any adja-

cent punctuation, and all text was converted to

lowercase. Multi-word expressions (compounds)

found in WordNet were identified.

SemEval–2017 Task 7 (Miller et al., 2017) fo-

cused on pun identification, and was divided into

three subtasks.

2.1 Subtask 1

The problem in Subtask 1 was to identify if a sen-

tence contains a pun (or not). We relied on the

premise that a sentence will have one unambigu-

ous assignment of senses, and that this should be

true even as the parameters of a word sense disam-

biguation algorithm are varied. Thus, if a sentence

has multiple possible assignments of senses based

2http://wn-similarity.sourceforge.net

on the results of different runs of a word sense dis-

ambiguation algorithm, then there is a possibility

that a pun exists. To investigate this hypothesis

we ran the WordNet::SenseRelate::AllWords algo-

rithm using four different configurations, and then

compared the four sense tagged sentences with

each other. If there were more than two differ-

ences in the sense assignments that resulted from

these different runs, then the sentence is presumed

to contain a pun.

WordNet::SenseRelate::AllWords takes mea-

sures of semantic relatedness between all the pair-

wise combinations of words in a sentence that oc-

cur within a certain number of positions of each

other (the window size), and assigns the sense

to each content word that results in the maxi-

mum relatedness among the words in that win-

dow. The assumption that underlies this method

is that words in a window will be semantically re-

lated, at least to an extent, so when choices among

word senses are made, those that are most related

to other words in the window will be selected.

The four configurations include two where the

window of context is the entire sentence (a wide

window) and another two where the window of

context is only one word to the left and one

word to the right (a narrow window). In ad-

dition these two configurations were carried out

with and without compounding of words being

performed prior to disambiguation. In all four

configurations the Gloss Vector measure Word-

Net::Similarity::vector was used as the measure

of semantic relatedness. If more than two sense

changes result from these different configurations,

then we say that a pun has occurred in the sen-

tence.

2.2 Subtask 2

In Subtask 2 the evaluation data consists of the in-

stances from Subtask 1 that contain puns. The task

is to identify the punning word.

We took two approaches to this subtask, how-

ever both were informed by our observation that

punned words often occur later in sentences. The

first (run 1) was to rely on our word sense disam-

biguation results from Subtask 1 and identify the

last word which changed senses between different

runs of the WordNet::SenseRelate::AllWords dis-

ambiguation algorithm. We relied on two of the

four configurations used in Subtask 1. We used the

narrow and wide contexts from Subtask 1 without

417



finding compounds. We realized that this might

cause us to miss some cases where a pun was

created with a compound, but our intuition was

that the more common cases (especially for homo-

graphic puns) would be those without compounds.

Our second approach (run 2) was a simple base-

line where the last content word in the sentence

was simply assumed to be the punned word.

2.3 Subtask 3

The evaluation data for Subtask 3 includes hetero-

graphic and homographic instances from Subtask

2 where the word being punned has been identi-

fied. The task is to determine which two senses of

the punned word are creating the pun.

We used the word sense disambiguation algo-

rithm WordNet::SenseRelate::TargetWord, which

assigns a sense to a single word in context

(whereas AllWords assigns a sense to every word

in a context). However, both TargetWord and All-

Words have the same underlying premise, and that

is that words in a sentence should be assigned the

senses that are most related to the senses of other

words in that sentence.

We tried various combinations of TargetWord

configurations, where each would produce their

own verdict on the sense of the punned word.

We took the two most frequent senses assigned

by these variations and used them as the sense of

the punned word. Note that for the heterographic

puns there was an additional step, where alterna-

tive spellings of the target word were included in

the disambiguation algorithm. For example :

The dentist had a bad day at the orifice.

Orifice is already identified as the punned word,

and one of the intended senses would be that of an

opening, but the other is the somewhat less obvi-

ous spelling variation office, as in a bad day at the

office.

For the first variation (run 1) we used both the

local and global options from TargetWord. The

local option measures the semantic relatedness of

the target word with all of the other members of

the window of context, whereas the global op-

tion measures the relatedness among all of the

words in the window of context (not just the tar-

get word). We also varied whether the lesk or vec-

tor measure was used, if a narrow or wide win-

dow was used, and if compounds were identified.

We took all possible combinations of these varia-

tions, which resulted in 16 possible configurations.

To this we added a WordNet sense one baseline

with and without finding compounds, and a ran-

domly assigned sense baseline. Thus, there were

19 variations in our run 1 ensemble. We took this

approach with both the homographic and hetero-

graphic puns, although for the heterographic puns

we also replaced the target word with all of the

words known to WordNet that differed by one edit

distance. The premise of this was to detect mi-

nor misspellings that might enable a heterographic

pun.

For run 2 we only used the local window of

context with WordNet::SenseRelate::TargetWord,

but added to lesk and vector the Resnik measure

(res) and the shortest path (path) measure. We car-

ried out each of these with and without identifying

compounds, which gives us a total of eight differ-

ent combinations. We also tried a much more am-

bitious substitution method for the heterographic

puns, where we queried the Datamuse API in or-

der to find words that were rhymes, near rhymes,

homonyms, spelled like, sound like, related, and

means like words for the target word. This created

a large set of candidate target words, and all of

these were disambiguated to find out which sense

of which target word was most related to the sur-

rounding context.

3 Results

We review our results in the three subtasks in this

section. Table 1 refers to homographic results as

hom and heterographic as het. Thus the first run

of the Duluth systems on homographic data is de-

noted as Duluth-hom1, and the first run on het-

erographic data is Duluth-het1. The highest rank-

ing system is indicated via High-hom and High-

het. P and R as column headers stand for preci-

sion and recall, A stands for accuracy, and C is for

coverage. Rank x/y indicates that this system was

ranked x of y participating systems.

3.1 Subtask 1

Puns were found in 71% (1,271) of the hetero-

graphic and 71% of the homographic instances

(1,607). This suggests this subtask would have a

relatively high baseline performance, for example

if a system simply predicted that every sentence

contained a pun. Given this we do not want to

make too strong a claim about our approach, but

it does seem that focusing on sentences that have

multiple possible (and valid) sense assignments

418



Table 1: Subtask 1, 2, 3 results

Subtask 1 P R A F1 rank

High-hom .97 .80 .84 .87 1 / 9

Duluth-hom1 .87 .78 .74 .83 2 / 9

High-het .87 .82 .78 .84 1 / 7

Duluth-het1 .87 .74 .69 .80 3 / 7

Subtask 2 P R C F1 rank

High-hom .66 .66 1.0 .66 1 / 15

Duluth-hom1 .37 .36 .99 .37 7 / 15

Duluth-hom2 .44 .44 1.0 .44 6 / 15

High-het .80 .80 1.0 .80 1 / 11

Duluth-het1 .18 .18 .99 .18 11 / 11

Duluth-het2 .53 .53 1.0 .53 4 / 11

Subtask 3 P R C F1 rank

High-hom .17 .14 .86 .16 1 / 8

Duluth-hom2 .17 .14 .86 .16 1 / 8

Duluth-hom1 .15 .15 1.0 .15 3 / 8

High-het .08 .07 .83 .08 1 / 6

Duluth-het1 .03 .03 1.0 .03 3 / 6

Duluth-het2 .001 .001 .98 .001 6 / 6

is promising for pun identification. Our method

tended to over-predict puns, reporting that a pun

occurred in 84% (1,489 of 1,780 instances) of the

heterographic data, and 80% (1,791 of 2,250 in-

stances) of the homographic.

3.2 Subtask 2

Subtask 2 consists of all the instances from Sub-

task 1 that included a pun. This leads to 1,489

heterographic puns and 1,791 homographic.

We see that our simple baseline method of

choosing the last content word as the punned word

(run 2) significantly outperformed our more elab-

orate method (run 1) of identifying which word

experienced more changes of senses across mul-

tiple variations of the disambiguation algorithm.

We can also see that run 1 did not fare very well

with heterographic puns. In general we believe

the difficulty that run 1 experienced was due to

the overall noisiness that is characteristic of word

sense disambiguation algorithms.

3.3 Subtask 3

Subtask 3 consists of 1,298 homograph instances

and 1,098 heterographic instances. We see that

for homographs our method fared very well, and

was the top ranked of participating systems. On

the other hand our heterographic approach was

not terribly successful. We believe that the idea

of generating alternative target words for hetero-

graphic puns is necessary, since without this it

would be impossible to identify one of the senses

of the punned word. However, our run 1 ap-

proach of simply using target word variations with

an edit distance of one did not capture the vari-

ations present in heterographic puns (e.g., orifice

and office have an edit distance of 2). Our run 2

approach of finding many different target words

via the Datamuse API resulted in an overwhelm-

ing number of possibilities where the intended tar-

get word was very difficult to identify.

4 Discussion and Future Work

One limitation of our approach is the uncertain

level of accuracy of word sense disambiguation al-

gorithms, which vary from word to word and do-

main to domain. Finding multiple possible senses

for a single word may signal a pun or expose the

limits of a particular WSD algorithm.

In addition, the contexts used in this evalua-

tion were all single sentences, and were relatively

short. Whether or not having more context avail-

able would help or hinder these approaches is an

interesting question.

Heterographic puns posed a host of challenges,

in particular mapping clever near spellings and

near pronunciations into the intended form (e.g.,

try angle as triangle). Simply trying to assign

senses to try angle will obviously miss the pun,

and so the ability to map similar sounding phrases

to the intended word is a capability that our sys-

tems were not terribly successful with. However,

we were better able to identify compounds in ho-

mographic puns (e.g., mint condition) since those

were written literally and could be found (if in

WordNet) via a simple subsequence search.

While our reliance on word sense disambigua-

tion and semantic relatedness served us well for

homographic puns, it was clearly not sufficient

for heterographic. Moving forward it seems im-

portant to have a reliable mechanism to map the

spelling and pronunciation variations that charac-

terize heterographic puns to their intended forms.

While dictionaries of rhyming and sound-alike

words are certainly helpful, they typically intro-

duce too many possibilities from which to make a

reliable selection. Language modeling seems like

a promising way to winnow that space, so that we

can get from a try angle to a triangle.

419



References

Satanjeev Banerjee and Ted Pedersen. 2003. Extended
gloss overlaps as a measure of semantic related-
ness. In Proceedings of the Eighteenth Interna-
tional Joint Conference on Artificial Intelligence.
Acapulco, pages 805–810.

M.E. Lesk. 1986. Automatic sense disambiguation us-
ing machine readable dictionaries : How to tell a
pine cone from an ice cream cone. In Proceedings
of the 5th Annual International Conference on Sys-
tems Documentation. ACM Press, pages 24–26.

Tristan Miller, Christian F. Hempelmann, and Iryna
Gurevych. 2017. SemEval-2017 Task 7: Detec-
tion and interpretation of English puns. In Proceed-
ings of the 11th International Workshop on Semantic
Evaluation (SemEval-2017). Vancouver, BC.

S. Patwardhan, S. Banerjee, and T. Pedersen. 2003. Us-
ing measures of semantic relatedness for word sense
disambiguation. In Proceedings of the Fourth Inter-
national Conference on Intelligent Text Processing
and Computational Linguistics. Mexico City, pages
241–257.

S. Patwardhan, S. Banerjee, and T Pedersen. 2005.
SenseRelate::TargetWord - a generalized framework
for word sense disambiguation. In Proceedings of
the Demonstration and Interactive Poster Session
of the 43rd Annual Meeting of the Association for
Computational Linguistics. Ann Arbor, MI, pages
73–76.

S. Patwardhan and T. Pedersen. 2006. Using WordNet-
based Context Vectors to Estimate the Semantic Re-
latedness of Concepts. In Proceedings of the EACL
2006 Workshop on Making Sense of Sense: Bring-
ing Computational Linguistics and Psycholinguis-
tics Together. Trento, Italy, pages 1–8.

T. Pedersen and V. Kolhatkar. 2009. Word-
Net::SenseRelate::AllWords - a broad coverage
word sense tagger that maximizes semantic related-
ness. In Proceedings of the North American Chap-
ter of the Association for Computational Linguistics
- Human Language Technologies 2009 Conference.
Boulder, CO, pages 17–20.

T. Pedersen, S. Patwardhan, and J. Michelizzi. 2004.
Wordnet::Similarity - Measuring the relatedness of
concepts. In Proceedings of Fifth Annual Meeting
of the North American Chapter of the Association
for Computational Linguistics. Boston, MA, pages
38–41.

420


