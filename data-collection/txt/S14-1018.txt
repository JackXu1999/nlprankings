








































Cognitive Compositional Semantics using Continuation Dependencies

William Schuler
Department of Linguistics
The Ohio State University
schuler@ling.osu.edu

Adam Wheeler
Department of Linguistics
The Ohio State University
wheeler@ling.osu.edu

Abstract

This paper describes a graphical semantic
representation based on bottom-up ‘con-
tinuation’ dependencies which has the im-
portant property that its vertices define a
usable set of discourse referents in work-
ing memory even in contexts involving
conjunction in the scope of quantifiers. An
evaluation on an existing quantifier scope
disambiguation task shows that non-local
continuation dependencies can be as reli-
ably learned from annotated data as repre-
sentations used in a state-of-the-art quanti-
fier scope resolver, suggesting that contin-
uation dependencies may provide a natural
representation for scope information.

1 Introduction

It is now fairly well established that at least shal-
low semantic interpretation informs parsing deci-
sions in human sentence processing (Tanenhaus et
al., 1995; Brown-Schmidt et al., 2002), and re-
cent evidence points to incremental processing of
quantifier implicatures as well (Degen and Tanen-
haus, 2011). This may indicate that inferences
about the meaning of quantifiers are processed di-
rectly in working memory. Human working mem-
ory is widely assumed to store events (includ-
ing linguistic events) as re-usable activation-based
states, connected by a durable but rapidly mutable
weight-based memory of cued associations (Marr,
1971; Anderson et al., 1977; Murdock, 1982; Mc-
Clelland et al., 1995; Howard and Kahana, 2002).
Complex dependency structures can therefore be
stored in this associative memory as graphs, with
states as vertices and cued associations as directed
edges (e.g. Kintsch, 1988). This kind of represen-
tation is necessary to formulate and evaluate algo-
rithmic claims (Marr, 1982) about cued associa-
tions and working memory use in human sentence

processing (e.g. van Schijndel and Schuler, 2013).
But accounting for syntax and semantics in this
way must be done carefully in order to preserve
linguistically important distinctions. For example,
positing spurious local dependencies in filler-gap
constructions can lead to missed integrations of
dependency structure in incremental processing,
resulting in weaker model fitting (van Schijndel et
al., 2013). Similar care may be necessary in cases
of dependencies arising from anaphoric corefer-
ence or quantifier scope.

Unfortunately, most existing theories of compo-
sitional semantics (Montague, 1973; Barwise and
Cooper, 1981; Bos, 1996; Baldridge and Kruijff,
2002; Koller, 2004; Copestake et al., 2005) are
defined at the computational level (Marr, 1982),
employing beta reduction over complete or under-
specified lambda calculus expressions as a precise
description of the language processing task to be
modeled, not at the algorithmic level, as a model
of human language processing itself. The struc-
tured expressions these theories generate are not
intended to represent re-usable referential states
of the sort that could be modeled in current theo-
ries of associative memory. As such, it should not
be surprising that structural adaptations of lambda
calculus expressions as referential states exhibit a
number of apparent deficiencies:

First, representations based on lambda calculus
expressions lack topologically distinguishable ref-
erents for sets defined in the context of outscop-
ing quantifiers. For example, a structural adapta-
tion of a lambda calculus expression for the sen-
tence Every line contains two numbers, shown in
Figure 1a (adapted from Koller, 2004), contains
referents for the set of all document lines (sL) and
for the set of all numbers (sN) which can be iden-
tified by cued associations to predicate constants
like Number, but it is not clear how a referent for
the set of numbers in document lines can be dis-
tinguished from a referent for the set of numbers



a) pL

Every

0

sL

λ

0

dL

1
eL

Line

0

2

1
s′L

λ

0
d′L

1
pN

Two

0

sN

λ

0

dN

1
eN

Number

0

2

1
s′N

λ

0
d′N

1
eC

Contain

0

2

2

2

2

1

1

1

2

b) pL

Every

0

sL

λ

0

dL

1
eL

Line

0

2

1
s′L

λ

0
d′L

1
pA

pS

A

0
sS

λ

0

dS

1
eS

Space

0

2

1
s′S

λ

0
d′S

1
eB

BeginsWith

0

2

2

1

And

0
pN

Two

0

sN

λ

0

dN

1
eN

Number

0

2

1
s′N

λ

0
d′N

1
eC

Contain

0

2

2

2

2

2

1

1

1

2 1

1

2

c) (Every pL sL s′L) ∧ (Set sL dL eL) ∧ (Line eL dL) ∧ (Set s′L d′L pN) ∧
(Two pN sN s′N) ∧ (Set sN dN eN) ∧ (Number eN dN) ∧ (Set s′N d′N eC) ∧ (Contain eC d′L d′N)

Figure 1: Semantic dependency graph in a ‘direct’ (top-down) style, adapted from a disambiguated rep-
resentation of Koller (2004), excluding quantifiers over eventualities. The semantic dependency structure
for the sentence Every line contains two numbers (a), with flat logical form (c), is not a subgraph of the
semantic dependency structure for Every line begins with a space and contains two numbers (b), because
the structure is interrupted by the explicit conjunction predicate ‘And’.

in each document line (s′N) using local topological
features of the dependency graph, as would be re-
quired to accurately recall assertions about total or
average quantities of numbers in document lines.1

Second, graphs based on traditional lambda
calculus representations do not model conjuncts
as subgraphs of conjunctions. For example, the
graphical representation of the sentence Every line

1This graph matching can be implemented in a vectorial
model of associative memory by comparing the (e.g. cosine)
similarity of superposed vectors resulting from cueing in-
coming and outgoing dependencies with all possible labels
in increasingly longer paths from one or more constant vec-
tor states (e.g. vectors for predicate constants). This graph
matching does not necessarily preclude the introduction of
monotonicity constraints from matched quantifiers. For ex-
ample, More than two perl scripts work, can entail More
than two scripts work, using a subgraph in the first argu-
ment, but Fewer than two scripts work, can entail Fewer than
two perl scripts work, using a supergraph in the first argu-
ment. This consideration is similar to those observed in rep-
resentations based on natural logic (MacCartney and Man-
ning, 2009) which also uses low-level matching to perform
some kinds of inference, but representations based on natural
logic typically exclude other forms of inference, whereas the
present model does not.

This matching also assumes properties of nuclear scope
variables are inherited from associated restrictor variables,
e.g. through a set of dependencies from nuclear scope sets
to restrictor sets not shown in the figure. This assumption
will be revisited in Section 3.

begins with a space and contains two numbers
shown in Figure 1b does not contain the graphical
representation of the sentence Every line contains
two numbers shown in Figure 1a as a connected
subgraph. Although one might expect a query
about a conjunct to be directly answerable from
a knowledge base containing the conjoined repre-
sentation, the pattern of dependencies that make
up the conjunct in a graphical representation of a
lambda calculus expression does not match those
in the larger conjunction.

Finally, representations based on lambda calcu-
lus expressions contain vertices that do not seem
to correspond to viable discourse referents. For
example, following the sentence Every line con-
tains two numbers, using the lambda expression
shown in Figure 1b, dL may serve as a referent of
it in but it has only one underscore, sN may serve
as a referent of they in but they are not negative,
eC may serve as a referent of that in but that was
before it was edited, and pL may serve as a ref-
erent of that in but the compiler doesn’t enforce
that, but it is not clear what if anything would nat-
urally refer to the internal conjunction pA. Predi-
cations over such conjunctions (e.g. Kim believes
that every line begins with a space and contains



two numbers) are usually predicated at the outer
proposition pL, and in any case do not have truth
values that are independent of the same predica-
tion at each conjunct. One of the goals of Minimal
Recursion Semantics (Copestake et al., 2005) was
to eliminate similar kinds of superfluous conjunc-
tion structure.

Fortunately, lambda calculus expressions like
those shown in Figure 1 are not the only way to
represent compositional semantics of sentences.
This paper defines a graphical semantic depen-
dency representation that can be translated into
lambda calculus, but has the important property
that its vertices define a usable set of discourse
referents in working memory even in contexts in-
volving conjunction in the scope of quantifiers.
It does this by reversing the direction of de-
pendencies from parent-to-child subsumption in
a lambda-calculus tree to a representation sim-
ilar to the inside-out structure of function def-
initions in a continuation-passing style (Barker,
2002; Shan and Barker, 2006)2 so that sets are de-
fined in terms of their context, and explicit ‘And’
predicates are no longer required, leaving noth-
ing to get in the way of an exact pattern match.3

The learnability of the non-local continuation de-
pendencies involved in this representation is then
evaluated on an existing quantifier scope disam-
biguation task using a dependency-based statisti-
cal scope resolver, with results comparable to a
state-of-the-art unrestricted graph-based quantifier
scope resolver (Manshadi et al., 2013).

2 Continuation Dependencies

This paper explores the use of a bottom-up depen-
dency representation, inspired by the inside-out
structure of function definitions in a continuation-
passing style (Barker, 2002; Shan and Barker,
2006), which creates discourse referents for sets
that are associated with particular scoping con-
texts. This dependency representation preserves
the propositions, sets, eventualities, and ordinary

2This representation also has much in common with gen-
eralized Skolem terms of Steedman (2012), which also repre-
sent dependencies to outscoped terms, but here continuation
dependencies are applied to all quantifiers, including univer-
sals.

3This also holds for explicit disjunction predicates, which
can be cast as conjunction through application of de Morgan’s
law and manipulation of the polarity of adjacent quantifiers.
For example, Every line begins with at least one space or
contains at least two numbers, is equivalent to No line be-
gins with fewer than one space and contains fewer than two
numbers.

discourse referents of a ‘direct’ representation (the
p, s, e, and d nodes in Figure 1), but replaces the
downward dependencies departing set referents
with upward dependencies to context sets (high-
lighted in Figure 2).

Figures 1c and 2c also show flat logical forms
composed of elementary predications, adapted
from Kruijff (2001) and Copestake et al. (2005),
for the sentence Every line contains two numbers,
which are formed by identifying the function as-
sociated with the predicate constant (e.g. Contain)
that is connected to each proposition or eventual-
ity referent (e.g. eC) by a dependency labeled ‘0’,
then applying that function to this referent, fol-
lowed by the list of arguments connected to this
referent by functions numbered ‘1’ and up: e.g.
(Contain eC d′L d

′
N). These dependencies can also

be defined by numbered dependency functions fn
from source instance j to destination instance i,
notated (fn j) = i. This notation will be used
in Section 4 to define constraints in the form of
equations. For example, the subject (first argu-
ment) of a lexical item may be constrained to be
the subject (first argument) of that item’s senten-
tial complement (second argument), as in an in-
stance of subject control, using the dependency
equation (f1 i) = (f1 (f2 i)).

Since continuation dependencies all flow up the
tree, any number of conjuncts can impinge upon a
common outscoping continuation, so there is no
longer any need for explicit conjunction nodes.
The representation is also attractive in that it lo-
cally distinguishes queries about, say, the cardi-
nality of the set of numbers in each document line
(Set s′N d

′
N s
′
L) from queries about the cardinal-

ity of the set of numbers in general (Set s′N d
′
N s
′
⊥)

which is crucial for successful inference by pattern
matching. Finally, connected sets of continuation
dependencies form natural ‘scope graphs’ for use
in graph-based disambiguation algorithms (Man-
shadi and Allen, 2011; Manshadi et al., 2013),
which will be used to evaluate this representation
in Section 6.

3 Mapping to Lambda Calculus

It is important for this representation not only
to have attractive graphical subsumption proper-
ties, but also to be sufficiently expressive to de-
fine corresponding expressions in lambda calcu-
lus. When continuation dependencies are filled in,
the resulting dependency structure can be trans-



a)

eL

Line

pLEvery
0

sL

λ

0

dL

1

1

s′L

λ

0

d′L

1

2

eN

Number

pNTwo
0

sN

λ

0

dN

1

1

s′N

λ

0

d′N

1

2

pCSome
0

sC

λ

0

eC

Contain

0

1

1

s′C

λ

0

e′C

1

2

1 1
1

2

22

b)

eL

Line

pLEvery
0

sL

λ

0

dL

1

1

s′L

λ

0

d′L

1

2

eS

Space

pSA
0

sS

λ

0

dS

1

1

s′S

λ

0

d′S

1

2

pBSome
0

sB

λ

0

eB

BeginWith

0

1

1

s′B

λ

0

e′B

1

2

eN

Number

pNTwo
0

sN

λ

0

dN

1

1

s′N

λ

0

d′N

1

2

pCSome
0

sC

λ

0

eC

Contain

0

1

1

s′C

λ

0

e′C

1

2

1 1 1 2 1
1

2

22 2

2

c) (Every pL sL s′L) ∧ (Set sL dL s⊥) ∧ (Line eL dL) ∧ (Set s′L d′L s⊥) ∧
(Two pN sN s′N) ∧ (Set sN dN s⊥) ∧ (Number eN dN) ∧ (Set s′N d′N s′L) ∧ (Contain eC d′L d′N)

Figure 2: Semantic dependency graph in a ‘continuation-passing’ (bottom-up) style, including quantifiers
over eventualities for verbs (in gray). The semantic dependency structure for the sentence Every line
contains two numbers (a), with flat logical form (c), is now contained by the semantic dependency
structure for Every line begins with a space and contains two numbers (b).

lated into a lambda calculus expression by a de-
terministic algorithm which traverses sequences of
continuation dependencies and constructs accord-
ingly nested terms in a manner similar to that de-
fined for DRT (Kamp, 1981). This graphical rep-
resentation can be translated into lambda calculus
by representing the source graph as a set Γ of ele-
mentary predications ( f i0 .. iN) and the target as
a set ∆ of translated lambda calculus expressions,
e.g. (λi (h f i0 .. i .. iN)). The set ∆ can then be de-
rived from Γ using the following natural deduction
rules:4

• Initialize ∆ with lambda terms (sets) that have
no outscoped sets in Γ:

Γ, (Set s i ) ; ∆
Γ, (Set s i ) ; (λi True),∆

(Set s ) < Γ

• Add constraints to appropriate sets in ∆:
4Here, set predications are defined with an additional final

argument position, which is defined to refer in a nuclear scope
set to the restrictor set that is its sibling, and in a restrictor set
to refer to s⊥.

Γ, ( f i0 .. i .. iN) ; (λi o),∆
Γ ; (λi o ∧ (h f i0 .. i .. iN)),∆

i0 ∈ E

• Add constraints of supersets as constraints on
subsets in ∆:

Γ, (Set s i ), (Set s′ i′ s′′s) ;
(λi o ∧ (h f i0 .. i .. iN)), (λi′ o′),∆
Γ, (Set s i ), (Set s′ i′ s′′s) ;
(λi o ∧ (h f i0 .. i .. iN)),

(λi′ o′ ∧ (h f i0 .. i′ .. iN)),∆

• Add quantifiers over completely constrained
sets in ∆:
Γ, (Set s i ), ( f p s′ s′′),

(Set s′ i′ s ), (Set s′′i′′s′ s′) ;
(λi o), (λi′ o′), (λi′′ o′′),∆

Γ, (Set s i ) ;
(λi o ∧ (h f (λi′ o′) (λi′′ o′′))),∆

p ∈ P,
( f ′.. i′..) < Γ,
( f ′′.. i′′..) < Γ.

For example, the graph in Figure 2 can be trans-
lated into the following lambda calculus expres-
sion (including quantifiers over eventualities in the
source graph, to eliminate unbound variables):



(Every (λdLSome (λeLLine eL dL))
(λd′LTwo (λdN Some (λeN Number eN dN))

(λd′N Some (λeC Contain eC d
′
L d
′
N))))

4 Derivation of Syntactic and Semantic
Dependencies

The semantic dependency representation defined
in this paper assumes semantic dependencies other
than those representing continuations are derived
compositionally by a categorial grammar. In par-
ticular, this definition assumes a Generalized Cat-
egorial Grammar (GCG) (Bach, 1981; Oehrle,
1994), because it can be used to distinguish argu-
ment and modifier compositions (from which re-
strictor and nuclear scope sets are derived in a tree-
structured continuation graph), and because large
GCG-annotated corpora defined with this distinc-
tion are readily available (Nguyen et al., 2012).
GCG category types c ∈ C each consist of a prim-
itive category type u ∈ U, typically labeled with
the part of speech of the head of a category (e.g.
V, N, A, etc., for phrases or clauses headed by
verbs, nouns, adjectives, etc.), followed by one or
more unsatisfied dependencies, each consisting of
an operator o ∈ O (-a and -b for adjacent argument
dependencies preceding and succeeding a head, -c
and -d for adjacent conjunct dependencies preced-
ing and succeeding a head, -g for filler-gap depen-
dencies, -r for relative pronoun dependencies, and
some others), each followed by a dependent cate-
gory type from C. For example, the category type
for a transitive verb would be V-aN-bN, since it is
headed by a verb, and has unsatisfied dependen-
cies to satisfied noun-headed categories preced-
ing and succeeding it (for the subject and direct
object noun phrase, respectively). This formula-
tion has the advantage for semantic dependency
calculation that it distinguishes modifier and ar-
gument attachment. Since the semantic represen-
tation described in this paper makes explicit dis-
tinctions between restrictor sets and scope sets
(which is necessary for coherent interpretation of
quantifiers) it is necessary to consistently apply
predicate-argument constraints to discourse refer-
ents in the nuclear scope set of a quantifier and
modifier-modificand constraints to discourse ref-
erents in the restrictor set of a quantifier. For ex-
ample, in Sentence 1:

(1) Everything is [A-aN open].

the predicate open constrains the nuclear scope set
of every, but in Sentence 2:

(2) Everything [A-aN open] is finished.

the predicate open constrains the restrictor set.
These constraints can be consistently applied in
the argument and modifier attachment rules of a
GCG.

Like a Combinatory Categorial Grammar
(Steedman, 2000), a GCG defines syntactic depen-
dencies for compositions that are determined by
the number and kind of unsatisfied dependencies
of the composed category types. These are similar
to dependencies for subject, direct object, prepo-
sition complement, etc., of Stanford dependencies
(de Marneffe et al., 2006), but are reduced to num-
bers based on the order of the associated depen-
dencies in the category type of the lexical head.

These syntactic dependencies are then associ-
ated with semantic dependencies, with the refer-
ent of a subject associated with the first argument
of an eventuality, the referent of a direct object as-
sociated with the second argument, and so on, for
all verb forms other than passive verbs. In the case
of passive verbs, the referent of a subject is asso-
ciated with the second argument of an eventuality,
the referent of a direct object associated with the
third argument, and so on.

In order to have a consistent treatment of ar-
gument and modifier attachment across all cate-
gory types, and also in order to model referents
of verbs as eventualities which can be quantified
by adverbs like never, once, twice, etc. (Parsons,
1990), it is desirable for eventualities associated
with verbs to also be quantified. Outgoing seman-
tic dependencies to arguments of eventualities are
then applied as constraints to the discourse refer-
ent variable of the restrictor sets of these quanti-
fiers. Incoming dependencies to eventualities and
other discourse referents used as modificands of
modifiers are also applied as constraints to dis-
course referent variables of restrictor sets, but in-
coming dependencies to discourse referents used
as arguments of predicates are applied as con-
straints to discourse referent variables of nuclear
scope sets. This assignment to restrictor or nuclear
scope sets depends on the context of the relevant
(argument or modifier attachment) parser opera-
tion, so associations between syntactic and seman-
tic dependencies must be left partially undefined
in lexical entries. Lexical entries are therefore de-
fined with separate syntactic and semantic depen-
dencies, using even numbers for syntactic depen-
dencies from lexical items, and odd numbers for



a) containing

s′

d′
1

iC

pC

sC

eC

Contain

0

1

1

1

s′′

d′′
1

1 2

3 5

0

b)

i′

p′

s′
2

1

i

3

2

c)

i′

p′

s′
1

1

i

3

2

Figure 3: Example lexical semantic dependencies for the verb containing (a), and dependency equations
for argument attachment (b) and modifier attachment (c) in GCG deduction rules. Lexical dependencies
are shown in gray. Even numbered edges departing lexical items denote lexical syntactic dependen-
cies, and odd numbered edges departing lexical items are lexical semantic dependencies. Argument
attachments constrain semantic arguments to the nuclear scope sets of syntactic arguments, and modifier
attachments constrain semantic arguments to the restrictor sets of syntactic arguments.

semantic dependencies from lexical items. For ex-
ample, a lexical mapping for the finite transitive
verb contains might be associated with the pred-
icate Contain, and have the discourse referent of
its first lexical semantic argument (f1 (f3 i)) as-
sociated with the first argument of the eventuality
discourse referent of the restrictor set of its propo-
sition (f1 (f1 (f1 (f1 i)))), and the discourse referent
of its second lexical semantic argument (f1 (f5 i))
associated with the second argument of the even-
tuality discourse referent of the restrictor set of its
proposition (f2 (f1 (f1 (f1 i)))):

contains ⇒ V-aN-bN : λi (f0 i)=contains
∧ (f0 (f1 (f1 (f1 i))))=Contain
∧ (f1 (f1 (f1 (f1 i))))=(f1 (f3 i))
∧ (f2 (f1 (f1 (f1 i))))=(f1 (f5 i))

A graphical representation of these dependencies
is shown in Figure 3a. These lexical semantic con-
straints are then associated with syntactic depen-
dencies by grammar rules for argument and modi-
fier attachment, as described below.

4.1 Inference rules for argument attachment
In GCG, as in other categorial grammars, infer-
ence rules for argument attachment apply functors
of category c-ad or c-bd to preceding or succeed-
ing arguments of category d:

d : g c-ad : h⇒ c : (fc-ad g h) (Aa)
c-bd : g d : h⇒ c : (fc-bd g h) (Ab)

where fuϕ1...ϕn are composition functions for u∈U
and ϕ∈{-a, -b, -c, -d}×C, which connect the lexi-
cal item (f2n i) of a preceding child function g as
the 2nth argument of lexical item i of a succeeding
child function h, or vice versa:

fuϕ1..n−1-ad
def
= λg h i (g (f2n i)) ∧ (h i)
∧ (f2n+1 i)=(f2 (f1 (f2n i))) (1a)

fuϕ1..n−1-bd
def
= λg h i (g i) ∧ (h (f2n i))
∧ (f2n+1 i)=(f2 (f1 (f2n i))) (1b)

as shown in Figure 3b. This associates the lex-
ical semantic argument of the predicate (f2n+1 i)
with the nuclear scope of the quantifier propo-
sition associated with the syntactic argument
(f2 (f1 (f2n i))). For example, the following infer-
ence attaches a subject to a verb:

every line
N : λi (f0 i)=line ..

contains two numbers
V-aN : λi (f0 i)=contains ..

V : λi (f0 (f2 i))=line .. ∧ (f0 i)=contains ..
∧ (f3 i)=(f2 (f1 (f2 i)))

Aa

4.2 Inference rules for modifier attachment
This grammar also uses distinguished inference
rules for modifier attachment. Inference rules for
modifier attachment apply preceding or succeed-
ing modifiers of category u-ad to modificands of
category c, for u ∈ U and c, d ∈ C:

u-ad : g c : h⇒ c : (fPM g h) (Ma)
c : g u-ad : h⇒ c : (fSM g h) (Mb)



eL

Line

0

lines

iL

pLSome
0

sL

λ

0

dL

1

1

s′L

λ

0

d′L

1

2

1

containing

iC

pCSome
0

sC

λ

0

eC

Contain

0

1

1

s′C

λ

0

e′C

1

2

1

eN

Number

0

numbers

iN

pNSome
0

sN

λ

0

dN

1

1

s′N

λ

0

d′N

1

2

1

0 0 02 4

3 5

2
2

1
1 2

1

Figure 4: Compositional analysis of noun phrase lines containing numbers exemplifying both argument
attachment (to numbers) and modifier attachment (to lines). Lexical dependencies are shown in gray, and
continuation dependencies (which do not result from syntactic composition) are highlighted.

where fPM and fSM are category-independent com-
position functions for preceding and succeeding
modifiers, which return the lexical item of the ar-
gument ( j) rather than of the predicate (i):

fPM
def
= λg h j ∃i (f2 i)= j ∧ (g i) ∧ (h j)

∧ (f3 i)=(f1 (f1 (f2 i))) (2a)

fSM
def
= λg h j ∃i (f2 i)= j ∧ (g j) ∧ (h i)

∧ (f3 i)=(f1 (f1 (f2 i))) (2b)

as shown in Figure 3c. This allows categories
for predicates to be re-used as modifiers. Unlike
argument attachment, modifier attachment asso-
ciates the lexical semantic argument of the mod-
ifier (f2n+1 i) with the restrictor of the quantifier
proposition of the modificand (f1 (f1 (f2n i))). For
example, the following inference attaches an ad-
jectival modifier to the quantifier proposition of a
noun phrase:

every line
N:λi (f0 i)=line ..

containing two numbers
A-aN:λi (f0 i)=containing ..

N : λi (f0 i)=line .. ∧ ∃ j (f0 j)=containing ..
∧ (f2 j)=i ∧ (f3 j)=(f1 (f1 (f2 j)))

Mb

An example of argument and modifier attachment
is shown in Figure 4.

5 Estimation of Scope Dependencies

Semantic dependency graphs obtained from GCG
derivations as described in Section 4 are scopally
underspecified. Scope disambiguations must then

be obtained by specifying continuation dependen-
cies from every set referent to some other set ref-
erent (or to a null context, indicating a top-level
set). In a sentence processing model, these non-
local continuation dependencies would be incre-
mentally calculated in working memory in a man-
ner similar to coreference resolution.5 However, in
this paper, in order to obtain a reasonable estimate
of the learnability of such a system, continuation
dependencies are assigned post-hoc by a statistical
inference algorithm.

The disambiguation algorithm first defines a
partition of the set of reified set referents into
sets {s, s′, s′′} of reified set referents s whose dis-
course referent variables (f1 s) are connected by
semantic dependencies. For example, sL, sC and
s′N in Figure 4 are part of the same partition, but s

′
L

is not.
Scope dependencies are then constructed from

these partitions using a greedy algorithm which
starts with an arbitrary set from this partition in

5Like any other dependency, a continuation dependency
may be stored during incremental processing when both its
cue (source) and target (destination) referents have been hy-
pothesized. For example, upon processing the word numbers
in the sentence Every line contains two numbers, a continu-
ation dependency may be stored from the nuclear scope set
associated with this word to the nuclear scope set of the sub-
ject every line, forming an in-situ interpretation with some
amount of activation (see Figure 4), and with some (proba-
bly smaller) amount of activation, a continuation dependency
may be stored from the nuclear scope set of this subject to
the nuclear scope set of this word, forming an inverted inter-
pretation. See Schuler (2014) for a model of how sentence
processing in associative memory might incrementally store
dependencies like these as cued associations.



the dependency graph, then begins connecting it,
selecting the highest-ranked referent of that par-
tition that is not yet attached and designating it
as the new highest-scoping referent in that parti-
tion, attaching it as the context of the previously
highest-scoping referent in that partition if one ex-
ists. This proceeds until:

1. the algorithm reaches a restrictor or nuclear
scope referent with a sibling (superset or sub-
set) nuclear scope or restrictor referent that
has not yet served as the highest-scoping ref-
erent in its partition, at which point the algo-
rithm switches to the partition of that sibling
referent and begins connecting that; or

2. the algorithm reaches a restrictor or nuclear
scope referent with a sibling nuclear scope or
restrictor referent that is the highest-scoping
referent in its partition, in which case it con-
nects it to its sibling with a continuation de-
pendency from the nuclear scope referent to
the restrictor referent and merges the two sib-
lings’ partitions.

In this manner, all set referents in the dependency
graph are eventually assembled into a single tree
of continuation dependencies.

6 Evaluation

This paper defines a graphical semantic represen-
tation with desirable properties for storing sen-
tence meanings as cued associations in associa-
tive memory. In order to determine whether this
representation of continuation dependencies is re-
liably learnable, the set of test sentences from the
QuanText corpus (Manshadi et al., 2011) was au-
tomatically annotated with these continuation de-
pendencies and evaluated against the associated
set of gold-standard quantifier scopes. The sen-
tences in this corpus were collected as descrip-
tions of text editing tasks using unix tools like sed
and awk, collected from online tutorials and from
graduate students asked to write and describe ex-
ample scripts. Gold-standard scoping relations in
this corpus are specified over bracketed sequences
of words in each sentence. For example, the sen-
tence Print every line that starts with a number
might be annotated:

Print [1 every line] that starts with [2 a number] .

scoping relations: 1 > 2

meaning that the quantifier over lines, referenced
in constituent 1, outscopes the quantifier over
numbers, referenced in constituent 2. In order to
isolate the learnablility of the continuation depen-
dencies described in this paper, both training and
test sentences of this corpus were annotated with
hand-corrected GCG derivations which are then
used to obtain semantic dependencies as described
in Section 4. Continuation dependencies are then
inferred from these semantic dependencies us-
ing the algorithm described in Section 5. Gold-
standard scoping relations are considered success-
fully recalled if a restrictor (f1 (f1 i)) or nuclear
scope (f2 (f1 i)) referent of any lexical item i within
the outscoped span is connected by a sequence of
continuation dependencies (in the appropriate di-
rection) to any restrictor or nuclear scope referent
of any lexical item within the outscoping span.

First, the algorithm was run without any lexical-
ization on the 94 non-duplicate sentences of the
QuanText test set. Results of this evaluation are
shown in the third line of Table 1 using the per-
sentence complete recall accuracy (‘AR’) defined
by Manshadi et al. (2013).

The algorithm was then run using bilexical
weights based on the frequencies F̃(h, h′) with
which a word h′ occurs as a head of a category
outscoped by a category headed by word h in the
350-sentence training set of the QuanText corpus.
For example, since quantifiers over lines are often
outscoped by quantifiers over files in the training
data, the system learns to rank continuation de-
pendencies to referents associated with the word
lines ahead of continuation dependencies to ref-
erents associated with the word files in bottom-
up inference. These lexical features may be par-
ticularly helpful because continuation dependen-
cies are generated only between directly adjacent
sets. Results for scope disambiguation using these
rankings are shown in the fourth line of Table 1.
This increase is statistically significant (p = 0.001
by two-tailed McNemar’s test). This significance
for local head-word features on continuation de-
pendencies shows that these dependencies can be
reliably learned from training examples, and sug-
gests that continuation dependencies may be a nat-
ural representation for scope information.

Interestingly, effects of lexical features for
quantifiers (the word each, or definite/indefinite
distinctions) were not substantial or statistically
significant, despite the relatively high frequencies



System AR
Manshadi and Allen (2011) baseline 63%
Manshadi et al. (2013) 72%
This system, w/o lexicalized model 61%
This system, w. lexicalized model 72%

Table 1: Per-sentence complete recall accuracy
(‘AR’) of tree-based algorithm as compared to
Manshadi and Allen (2011) and Manshadi et al.
(2013) on explicit NP chunks in the QuanText test
set, correcting for use of gold standard trees as de-
scribed in footnote 19 of Manshadi et al. (2013).

of the words each and the in the test corpus (oc-
curring in 16% and 68% of test sentences, respec-
tively), which suggests that these words may often
be redundant with syntactic and head-word con-
straints. Results using preferences that rank refer-
ents quantified by the word each after other refer-
ents achieve a numerical increase in accuracy over
a model with no preferences (up 5 points, to 66%),
but it is not statistically significant (p = .13). Re-
sults using preferences that rank referents quanti-
fied by the word the after other referents achieve a
numerical increase in accuracy over a model with
no preferences (up 1 point, to 62%), but this is
even less significant (p = 1). Results are even
weaker in combination with head-word features
(up 1 point, to 73%, for each; down two points,
to 70%, for the). This suggests that world knowl-
edge (in the form of head-word information) may
be more salient to quantifier scope disambiguation
than many intuitive linguistic preferences.

7 Conclusion

This paper has presented a graphical semantic de-
pendency representation based on bottom-up con-
tinuation dependencies which can be translated
into lambda calculus, but has the important prop-
erty that its vertices define a usable set of discourse
referents in working memory even in contexts in-
volving conjunction in the scope of quantifiers.
An evaluation on an existing quantifier scope dis-
ambiguation task shows that non-local continua-
tion dependencies can be as reliably learned from
annotated data as representations used in a state-
of-the-art quantifier scope resolver. This suggests
that continuation dependencies may be a natural
representation for scope information.

Continuation dependencies as defined in this
paper provide a local representation for quantifi-

cational context. This ensures that graphical repre-
sentations match only when their quantificational
contexts match. When used to guide a statistical
or vectorial representation, it is possible that this
local context will allow certain types of inference
to be defined by simple pattern matching, which
could be implemented in existing working mem-
ory models. Future work will explore the use of
this graph-based semantic representation as a ba-
sis for vectorial semantics in a cognitive model of
inference during sentence processing.

8 Acknowledgements

The authors would like to thank Mehdi Manshadi
for assistance in obtaining the QuanText corpus.
The authors would also like to thank Erhard Hin-
richs, Craige Roberts, the members of the OSU
LLIC Reading Group, and the three anonymous
*SEM reviewers for their helpful comments about
this work.

References
James A. Anderson, Jack W. Silverstein, Stephen A.

Ritz, and Randall S. Jones. 1977. Distinctive fea-
tures, categorical perception and probability learn-
ing: Some applications of a neural model. Psycho-
logical Review, 84:413–451.

Emmon Bach. 1981. Discontinuous constituents in
generalized categorial grammars. Proceedings of
the Annual Meeting of the Northeast Linguistic So-
ciety (NELS), 11:1–12.

Jason Baldridge and Geert-Jan M. Kruijff. 2002. Cou-
pling CCG and hybrid logic dependency seman-
tics. In Proceedings of the 40th Annual Meeting of
the Association for Computational Linguistics (ACL
2002), Philadelphia, Pennsylvania.

Chris Barker. 2002. Continuations and the nature
of quantification. Natural Language Semantics,
10:211–242.

Jon Barwise and Robin Cooper. 1981. Generalized
quantifiers and natural language. Linguistics and
Philosophy, 4.

Johan Bos. 1996. Predicate logic unplugged. In Pro-
ceedings of the 10th Amsterdam Colloquium, pages
133–143.

Sarah Brown-Schmidt, Ellen Campana, and Michael K.
Tanenhaus. 2002. Reference resolution in the wild:
Online circumscription of referential domains in a
natural interactive problem-solving task. In Pro-
ceedings of the 24th Annual Meeting of the Cogni-
tive Science Society, pages 148–153, Fairfax, VA,
August.



Ann Copestake, Dan Flickinger, Carl Pollard, and Ivan
Sag. 2005. Minimal recursion semantics: An intro-
duction. Research on Language and Computation,
pages 281–332.

Marie-Catherine de Marneffe, Bill MacCartney, and
Christopher D. Manning. 2006. Generating typed
dependency parses from phrase structure parses. In
Proceedings of LREC 2006.

Judith Degen and Michael K. Tanenhaus. 2011. Mak-
ing inferences: The case of scalar implicature pro-
cessing. In Proceedings of the 33rd Annual Confer-
ence of the Cognitive Science Society, pages 3299–
3304.

Marc W. Howard and Michael J. Kahana. 2002. A dis-
tributed representation of temporal context. Journal
of Mathematical Psychology, 45:269–299.

Hans Kamp. 1981. A theory of truth and semantic
representation. In Jeroen A. G. Groenendijk, Theo
M. V. Janssen, and Martin B. J. Stokhof, editors,
Formal Methods in the Study of Language: Math-
ematical Centre Tracts 135, pages 277–322. Mathe-
matical Center, Amsterdam.

Walter Kintsch. 1988. The role of knowledge in dis-
course comprehension: A construction-integration
model. Psychological review, 95(2):163–182.

Alexander Koller. 2004. Constraint-based and graph-
based resolution of ambiguities in natural language.
Ph.D. thesis, Universität des Saarlandes.

Geert-Jan M. Kruijff. 2001. A Categorial-Modal
Architecture of Informativity: Dependency Gram-
mar Logic and Information Structure. Ph.D. thesis,
Charles University.

Bill MacCartney and Christopher D. Manning. 2009.
An Extended Model of Natural Logic. In Proceed-
ings of the Eighth International Conference on Com-
putational Semantics, IWCS-8 ’09, pages 140–156.
Association for Computational Linguistics.

Mehdi Manshadi and James F. Allen. 2011. Unre-
stricted quantifier scope disambiguation. In Graph-
based Methods for Natural Language Processing,
pages 51–59.

Mehdi Manshadi, James F. Allen, and Mary Swift.
2011. A corpus of scope-disambiguated english
text. In Proceedings of ACL, pages 141–146.

Mehdi Manshadi, Daniel Gildea, and James F. Allen.
2013. Plurality, negation, and quantification: To-
wards comprehensive quantifier scope disambigua-
tion. In Proceedings of ACL, pages 64–72.

David Marr. 1971. Simple memory: A theory
for archicortex. Philosophical Transactions of the
Royal Society (London) B, 262:23–81.

David Marr. 1982. Vision. A Computational Investiga-
tion into the Human Representation and Processing
of Visual Information. W.H. Freeman and Company.

J. L. McClelland, B. L. McNaughton, and R. C.
O’Reilly. 1995. Why there are complementary
learning systems in the hippocampus and neocortex:
Insights from the successes and failures of connec-
tionist models of learning and memory. Psychologi-
cal Review, 102:419–457.

Richard Montague. 1973. The proper treatment
of quantification in ordinary English. In J. Hin-
tikka, J.M.E. Moravcsik, and P. Suppes, editors,
Approaches to Natural Langauge, pages 221–242.
D. Riedel, Dordrecht. Reprinted in R. H. Thoma-
son ed., Formal Philosophy, Yale University Press,
1994.

B.B. Murdock. 1982. A theory for the storage and
retrieval of item and associative information. Psy-
chological Review, 89:609–626.

Luan Nguyen, Marten van Schijndel, and William
Schuler. 2012. Accurate unbounded dependency re-
covery using generalized categorial grammars. In
Proceedings of the 24th International Conference
on Computational Linguistics (COLING ’12), pages
2125–2140, Mumbai, India.

Richard T. Oehrle. 1994. Term-labeled categorial type
systems. Linguistics and Philosophy, 17(6):633–
678.

Terence Parsons. 1990. Events in the Semantics of
English. MIT Press.

William Schuler. 2014. Sentence processing in a
vectorial model of working memory. In Fifth An-
nual Workshop on Cognitive Modeling and Compu-
tational Linguistics (CMCL 2014).

Chung-chieh Shan and Chris Barker. 2006. Explaining
crossover and superiority as left-to-right evaluation.
Linguistics and Philosophy, 29:91–134.

Mark Steedman. 2000. The syntactic process. MIT
Press/Bradford Books, Cambridge, MA.

Mark Steedman. 2012. Taking Scope - The Natural
Semantics of Quantifiers. MIT Press.

Michael K. Tanenhaus, Michael J. Spivey-Knowlton,
Kathy M. Eberhard, and Julie E. Sedivy. 1995. In-
tegration of visual and linguistic information in spo-
ken language comprehension. Science, 268:1632–
1634.

Marten van Schijndel and William Schuler. 2013. An
analysis of frequency- and recency-based processing
costs. In Proceedings of NAACL-HLT 2013. Associ-
ation for Computational Linguistics.

Marten van Schijndel, Luan Nguyen, and William
Schuler. 2013. An analysis of memory-based pro-
cessing costs using incremental deep syntactic de-
pendency parsing. In Proceedings of CMCL 2013.
Association for Computational Linguistics.


