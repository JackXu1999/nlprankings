



















































Multi-document summarization using distortion-rate ratio


Proceedings of the ACL 2014 Student Research Workshop, pages 64–70,
Baltimore, Maryland USA, June 22-27 2014. c©2014 Association for Computational Linguistics

Multi-Document Summarization Using Distortion-Rate Ratio

Ulukbek Attokurov
Department of Computer Engineering

Istanbul Technical University
attokurov@itu.edu.tr

Ulug Bayazit
Department of Computer Engineering

Istanbul Technical University
ulugbayazit@itu.edu.tr

Abstract
The current work adapts the optimal tree
pruning algorithm(BFOS) introduced by
Breiman et al.(1984) and extended by
Chou et al.(1989) to the multi-document
summarization task. BFOS algorithm is
used to eliminate redundancy which is one
of the main issues in multi-document sum-
marization. Hierarchical Agglomerative
Clustering algorithm(HAC) is employed
to detect the redundancy. The tree de-
signed by HAC algorithm is successively
pruned with the optimal tree pruning al-
gorithm to optimize the distortion vs. rate
cost of the resultant tree. Rate parameter is
defined to be the number of the sentences
in the leaves of the tree. Distortion is the
sum of the distances between the represen-
tative sentence of the cluster at each node
and the other sentences in the same clus-
ter. The sentences assigned to the leaves of
the resultant tree are included in the sum-
mary. The performance of the proposed
system assessed with the Rouge-1 metric
is seen to be better than the performance
of the DUC-2002 winners on DUC-2002
data set.

1 Introduction

Nowadays, the massive amount of information
available in the form of digital media over the in-
ternet makes us seek effective ways of accessing
this information. Textual documents, audio and
video materials are uploaded every second. For in-
stance, the number of Google’s indexed web pages
has exceeded 30 billion web pages in the last two
years. Extraction of the needed information from
a massive information pool is a challenging task.
The task of skimming all the documents in their
entirety before deciding which information is rel-
evant is very time consuming.

One of the well known and extensively studied
methods for solving this problem is summariza-
tion. Text summarization produces a short ver-
sion of a document that covers the main topics in
it (Mani and Hahn, 2000). It enables the reader
to determine in a timely manner whether a given
document satisfies his/her needs or not.

A single document summarization system pro-
duces a summary of only one document whereas
a multi-document summarization system produces
a summary based on multiple documents on the
same topic. Summarization systems can also be
categorized as generic or query-based . A generic
summary contains general information about par-
ticular documents. It includes any information
supposed to be important and somehow linked to
the topics of the document set. In contrast, a query
based summary comprises information relevant to
the given query. In this case, query is a rule ac-
cording to which a summary is to be generated.

Summarization systems can be also classified
as extractive or abstractive. In extractive systems,
a summary is created by selecting important sen-
tences from a document. Here, only sentences
containing information related to the main topics
of the document are considered to be important.
These sentences are added to the summary with-
out any modification. On the other hand, abstrac-
tive systems can modify the existing sentences or
even generate new sentences to be included in the
summary. Therefore, abstractive summarization
is typically more complex than extractive summa-
rization.

The main goal in multi-document summariza-
tion is redundancy elimination. Since the docu-
ments are related to the same topics, similar text
units(passages, sentences etc.) are encountered
frequently in different documents. Such text units
that indicate the importance of the topics discussed
within them should be detected in order to re-
duce the redundancy. Some of the well-known ap-

64



proaches that address this problem are briefly ex-
plained in the following section.

Although much work has been done to elim-
inate the redundancy in multi-document summa-
rization, the problem is still actual and addressed
in the current work as well. The current work
proposes to integrate the generalized BFOS algo-
rithm (Breiman et al., 1984) adopted by Chou et.al
(1989) for pruned tree structured quantizer design
with the HAC (Hierarchical Agglomerative Clus-
tering) algorithm. The two main parameters (dis-
tortion and rate) in the latter work are adopted to
the multi-document summarization task. Distor-
tion can be succinctly defined as the information
loss in the meaning of the sentences due to their
representation with other sentences. More specif-
ically, in the current context, distortion contribu-
tion of a cluster is taken to be the sum of the dis-
tances between the vector representations of the
sentences in the cluster and representative sen-
tence of that cluster. Rate of a summary is de-
fined to be the number of sentences in the sum-
mary, but more precise definitions involving word
or character counts are also possible. BFOS based
tree pruning algorithm is applied to the tree built
with the HAC algorithm. HAC algorithm is used
for clustering purposes since BFOS algorithm gets
tree structured data as an input. It is found that
the suggested approach yields better results in
terms of the ROUGE-1 Recall measure (Lin et
al., 2003) when compared to 400 word extractive
summaries(400E) included in DUC-2002 data set.
Also, the results with the proposed method are
higher than the ones obtained with the best sys-
tems of DUC-2002 in terms of sentence recall and
precision(Harabagiu, 2002; Halteren, 2002).

2 Related Works

Term frequency (Luhn, 1958), lexical chains
(Barzilay and Elhadad, 1997), location of the sen-
tences (Edmundson, 1969) and the cue phrases
(Teufel et al., 1997) are used to determine the im-
portant lexical units. Goldstein et al. (2000) pro-
posed a measure named Maximal Marginal Rel-
evance which assigns a high priority to the pas-
sages relevant to the query and has minimal sim-
ilarity to the sentences in the summary. Radev
et al. (2001) developed a system called MEAD
based on the centroid of the cluster. The words
that are most relevant to the main topics are in-
cluded in the centroid. Lin et al. designed a

statistic-based summarization system (Summarist)
which incorporated NLP(Natural Language Pro-
cessing) and IR(Information Retrieval) methods.
LSA(Latent Semantic Analysis) (Landauer et al.,
1998) has also been used extensively in recent
years for multi-document summarization. By ap-
plying SVD(Singular Value Decomposition) to the
term-document matrix, it determines the most im-
portant topics and represents the term and docu-
ments in the reduced space (Murray et al., 2005;
Steinberger and Jezek , 2004; Geiss, 2011). Rachit
Arora et al. (2008) combined LDA(Latent Dirich-
let Allocation) and SVD. In this approach, LDA is
used to detect topics and SVD is applied to select
the sentences representing these topics.

Clustering of the sentences has also been used
to determine the redundant information. In this
approach, the sentences are first clustered. The
sentences in each cluster share common informa-
tion about the main topics of the documents to be
summarized. Then a sentence is selected (Radev
et al., 2004) or generated (McKeown et al., 1999)
from each cluster that represents the sentences in
the cluster. Finally, selected sentences are added
to the summary until a predetermined length is ex-
ceeded (Aliguliyev, 2006; Hatzivassiloglou et al.,
1999; Hatzivassiloglou et al., 2001).

3 Background

3.1 Generalized BFOS Algorithm

Let us assume that we have a tree T with the set of
leaves T̃ . Also let us denote a sub-tree of T rooted
at any node of T as S. The leaves of the sub-trees
may happen to be the inner nodes of T . If the root
node of the sub-tree S is not identical to the root
node of T and the set of leaves S̃ is a sub-set of
T̃ then S is called a branch. But if the sub-tree S
is rooted at the root node of T then S is named a
pruned sub-tree of T . Function defined on the tree
T and on any sub-tree S is called a tree functional.
Monotonic tree functional is a class of functional
where it increases or decreases depending on the
tree size. In our case, tree size is the number of the
nodes of T .

Two main tree functionals(u1 and u2) need to
be defined in the generalized BFOS algorithm.
They are adapted to the problem under considera-
tion. In regression trees, u1 is the number of the
leaves and u2 is the mean squared distortion er-
ror. In TSVQ(Tree Structured Vector Quantiza-
tion), u1 and u2 are the length of the code and

65



the expected distortion, respectively. In the cur-
rent context, distortion(D) and rate(R) defined in
the next section are used as the tree functionals u1
and u2.

As shown in Chou et al., the set of distortion and
rate points of the pruned sub-trees of T generate a
convex hull if distortion is an increasing and rate
is a decreasing function. Also it is stated that if
the tree T is pruned off until the root node remains,
then it is possible to generate the sub-trees which
correspond to the vertices on the lower boundary
of the convex hull. Thus it is sufficient to consider
the sub-trees corresponding to the vertices of the
boundary to trade off between rate and distortion.

A parameter λ = −∆D∆R may be used to locate
the vertices on the lower boundary of the convex
hull. ∆D and ∆R indicate the amount of distor-
tion increase and rate decrease when branch sub-
tree S is pruned off. It can be shown that a step
on the lower boundary can be taken by pruning off
at least one branch sub-tree rooted at a particular
inner node. The λ value of this sub-tree is mini-
mal among all the other branch sub-trees rooted at
various inner nodes of T , because it is a slope of
the lower boundary. At each pruning iteration, the
algorithm seeks the branch sub-tree rooted at an
inner node with the minimal lambda and prunes
it off the tree. After each pruning step, the in-
ner node at which the pruned branch sub-tree is
rooted becomes a leaf node. The pruning itera-
tions continue until the root node remains or the
pruned sub-tree meets a certain stopping criterion.

4 The Proposed Summarization System

In the current work, BFOS and HAC algorithm
were incorporated to the multi-document sum-
marization system. Generalized version of the
BFOS algorithm discussed in the work of Chou
et al. (1989) with previous applications to TSVQ,
speech recognition etc. was adapted for the pur-
pose of pruning the large tree designed by the
HAC algorithm. Generalized BFOS algorithm
was preferred in the current context because it is
believed that the generated optimal trees yield the
best trade-off between the semantic distortion and
rate (the summary length in terms of number of
sentences).

The proposed system consists of the following
stages: preprocessing, redundancy detection, re-
dundancy elimination and the summary genera-
tion.

In preprocessing stage, the source documents
are represented in the vector space. Towards this
end, the sentences are parsed, stemmed and a fea-
ture set is created (terms (stems or words, n-grams
etc.) that occur in more than one document are
extracted). The sentences of the document set are
then represented by a sentence X term matrix with
n columns and m rows, where n is the number of
the sentences and m is the number of the terms in
the feature set. TF-IDF is used to determine the
values of the matrix elements. TF-IDF assigns a
value according to the importance of the terms in
the collection of the sentences. If the term t occurs
frequently in the current document but the oppo-
site is true for other documents then tf-idf value of
t is high.

TF − IDF = TF ∗ log N
DF

(1)

where TF is the term frequency, DF is the docu-
ment frequency and N is the number of sentences.
Term frequency is the number of the occurrences
of the term in the sentence. Document frequency
is the number of the sentences in which the term is
found.

Redundancy detection is facilitated by applying
the Hierarchical Agglomerative Clustering(HAC)
algorithm. Initially, individual sentences are con-
sidered to be singletons in the HAC algorithm.
The most similar clusters are then successively
merged to form a new cluster that contains the
union of the sentences in the merged clusters. At
each step, a new (inner) node is created in the tree
as the new cluster appears and contains all the sen-
tences in the union of the merged clusters. HAC
merge operations continue until a single cluster re-
mains. The tree built after HAC operation is re-
ferred to as the HAC tree.

The third stage is the redundancy elimination.
To this end, generalized BFOS algorithm dis-
cussed previously is applied to the HAC tree. In
order to adapt the generalized BFOS algorithm to
the current context, distortion contribution of each
cluster (node) is defined as follows:

D =
∑

s∈cluster
d(rs, s) (2)

where d is the distance between the representative
sentence(rs) and a sentence(s) in the cluster.

By definition, the distortion contribution of each
leaf node of the HAC tree is zero.

66



Rate is defined to be the number of sentences
in the leaves of the tree. A branch sub-tree is
removed at each pruning step of the generalized
BFOS algorithm. Correspondingly, the sentences
at the leaf nodes of the pruned branch subtree are
eliminated. As a result, the rate decreases to the
number of leaf nodes remaining after pruning.

The centroid of the cluster can be used as the
representative sentence of the cluster. Centroid
can be constituted of the important (with TF-IDF
values exceeding a threshold) words of the cluster
(Radev et al., 2004) or can be generated using Nat-
ural language processing techniques (McKeown et
al., 1999). In the current work, the simpler ap-
proach of selecting the sentence from the cluster
yielding the minimal distortion as the representa-
tive sentence is employed.
λ parameter is used to determine the branch

sub-trees that are successively pruned. In each
pruning step, the branch sub-tree with minimum
λ is identified to minimize the increase in total
distortion(∆D) per discarded sentence(∆R).

In accordance with the definition of rate given
above, ∆R is the change in the number of sen-
tences in the summary before and after the prun-
ing of the branch sub-tree. It also equals to the
number of pruned leaf nodes, because rate equals
to the number of the sentences stored in the leaf
nodes of the current tree. For instance, let us as-
sume that the number of sentences before pruning
is 10 and a sub-tree A is cut off. If A has 4 leaf
nodes, than 3 of them is eliminated and one is left
to represent the cluster of sentences corresponding
to the sub-tree A. Since 3 leaf nodes are removed
and each leaf node is matched to the certain sen-
tence, the current rate equals to 7. The increase in
total distortion is written as

∆D = Dpost −Dprev (3)
where Dprev is set equal to the sum of distortions
in the leaves of the tree before pruning and Dpost
is set equal to the sum of distortions in the leaves
of the tree after pruning.

The application of the generalized BFOS algo-
rithm to the HAC tree can be recapped as follows.
At the initial step, a representative sentence is se-
lected for each inner node and λ is determined for
each inner node. At each generic pruning step, the
node with the minimum lambda value is identified,
the sub-tree rooted at that node is pruned, the root
node of the sub-tree is converted to a leaf node.

After each pruning step, the λ values of the ances-
tor nodes of this new leaf node are updated. We
summarize the generalized BFOS algorithm with
a pseudocode in Algorithm 1.

Algorithm 1: PRUNING THE TREE. Prunes a
tree T created by using Hierarchical Agglom-
erative Clustering Algorithm
Input: A tree T produced by using

Hierarchical Clustering Algorithm
Output: Optimal sub-tree O obtained by

pruning T
1 For each leaf node,
λ←∞,distortion(D)← 0

2 For each inner node calculate λ = ∆D∆R ,
where ∆D and ∆R are change in
distortion(D) and rate(R) respectively

3 rate(R)← the number of the leaves of T
4 while the number of the nodes > 1 do
5 find a node A with minimum λ value

among the inner nodes
6 prune the sub-tree S rooted at the node A
7 convert the pruned inner node A to the

leaf node containing the representative
sentence of the sub-tree S

8 update the ancestor nodes of the node A:
update ∆D, ∆R and λ

9 update rate(R)

10 return O

A summary of desired length can be created by
selecting a threshold based on rate (the number of
remaining sentences after pruning, the number of
leaf nodes of the pruned tree). Another possibil-
ity for the choice of the stopping criterion may
be based on the λ parameter which monotonically
increases with pruning iterations. When a large
enough λ value is reached, it may be assumed that
shortening the summary further eliminates infor-
mative sentences.

The proposed method of summarization has a
few drawbacks. The main problem is that the
pruning algorithm is highly dependent on the dis-
tortion measure. If the distortion measure is not
defined appropriately, the representative sentence
can be selected incorrectly. Another issue is the
inclusion of the irrelevant sentences into the sum-
mary. This problem may occur if the sentences
remaining after pruning operation are included in
the summary without filtering.

67



5 Evaluation

The testing of the system performed on DUC-2002
data set (Document Understanding Conference,
2002) since the proposed system is designed to
produce a generic summary without specified in-
formation need of users or predefined user profile.
This data set contains 59 document sets. For each
document set extraction based summaries with the
length 200 and 400 words are provided. Document
sets related to the single event are used for testing
purposes.

Evaluation of the system is carried out using
ROUGE package (Lin C, 2004). Rouge is a sum-
mary evaluation approach based on n-gram co-
occurrence , longest common subsequence and
skip bigram statistics (Lin et al., 2003). The per-
formance of the summarizing system is measured
with Rouge-1 Recall, Rouge-1 Precision and F1
measure(Table 1). 400E stood for the extrac-
tive 400 word summary provided by DUC-2002
data set. It was created manually as an extrac-
tive summary for evaluation purposes. Candidate
summary(CS) was produced by the proposed sys-
tem. Both summaries were compared against a
200 word abstractive summary included in DUC-
2002 data set. 200 word abstractive summary
was considered as the model summary in ROUGE
package. As shown, the summary of the proposed
system gives better results in Rouge-1 recall mea-
sure. However, the highest precision is achieved
in the 400E summary. Generally, the proposed
system outperforms the 400E summary, since F1-
score which takes into account precision and recall
is higher.

In addition, the performance of the system was
compared with the best systems(BEST) of DUC-
2002(Halteren, 2002; Harabagiu, 2002)(Table 2).
The results of the best systems(BEST) in terms
of sentence recall and sentence precision are pro-
vided by DUC-2002. Sentence recall and sentence
precision of the candidate summary(produced by
the proposed system) were calculated by using 400
word extract based summary(provided by DUC-
2002) and a candidate summary. Sentence recall
and sentence precision are defined as follows:

sentence recall =
M

B
(4)

sentence precision =
M

C
(5)

where M is the number of the sentences included

summary P R F1
400E 0.313 0.553 0.382
candidate 0.3 0.573 0.394

Table 1: ROUGE-1 Results. Candidate sum-
mary(produced by the proposed system) and 400E
summary provided by DUC 2002 are compared
with 200 word abstract created manually.

in both of the summaries(a candidate and 400
word summary provided by DUC-2002(400E)),
C,B are the number of the sentences in the can-
didate summary and in a 400E summary, respec-
tively.

summary Sentence
Precision

Sentence
Recall

BEST 0.271 0.272
candidate 0.273 0.305

Table 2: Results. The best systems of DUC-2002
results and the results of the proposed system. Pro-
posed system is compared with 400 word extracts
provided by DUC-2002.

As shown, the proposed system performs bet-
ter than the best systems of DUC-2002 in terms
of sentence recall. We are more interested in sen-
tence recall because it states the ratio of the impor-
tant sentences contained in the candidate summary
if the sentences included in the 400E summary are
supposed to be important ones. Furthermore, sen-
tence precision is affected from the length of the
candidate summary.

Figure 1: The relationship between distortion and
rate. While rate is decreasing distortion is increas-
ing.

Summarizing the text can be considered as the
compression of the text. Thus it is possible to de-
pict the graph of dependence of distortion on rate
(Figure 1). The graph shows that as rate decreases
distortion increases monotonically. Therefore, if
distortion is assumed to be the information loss oc-

68



curred when the original text is summarized then
the summaries of different quality can be produced
by restricting rate (the number of sentences).

Another graph shows the change of the lambda
value(Figure 2). The iteration number of the prun-
ing is on X axis and lambda value is on Y one. If λ
value of the pruned points are sorted in ascending
order and then the graph of ordered λ values is de-
picted according to their order then the graph iden-
tical to the one shown below is obtained(Figure 2).
This indicates that the node with minimal lambda
value is selected in each iteration. Consequently,
the sentences are eliminated so that increase in dis-
tortion is minimal for decrease in rate.

Figure 2: λ value of the pruned node. The change
of λ value has upward tendency.

All in all, the quantitative analyses show that the
proposed system can be used as one of the redun-
dancy reduction methods. However, in order to
achieve the good results, the parameters of BFOS
algorithm have to be set appropriately.

6 Conclusion

In this paper , the combination of tree pruning and
clustering is explored for the purpose of multi-
document summarization. Redundancy in the text
detected by the HAC algorithm is eliminated by
the generalized BFOS algorithm. It is shown that
if the parameters(distortion and rate) are set prop-
erly, generalized BFOS algorithm can be used to
reduce the redundancy in the text. The depicted
graph (Figure 1) shows that the proposed defi-
nitions of distortion and rate are eligible for the
multi-document summarization purpose.

The performance evaluation results in terms of
ROUGE-1 metric suggest that the proposed sys-
tem can perform better with additional improve-
ments (combining with LSI). Also it is stated that
distance measure selection and noisy sentence in-
clusion have significance impact on the summa-
rization procedure.

Future research will deal with the abstraction. A

new sentence will be created(not extracted) when
two clusters are merged. It will represent the clus-
ter of sentences as well as summarize the other
sentences in the same cluster.

Acknowledgments

We thank Google for travel and conference sup-
port for this paper.

References
Aliguliyev R. 2006. A Novel Partitioning-Based Clus-

tering Method and Generic Document Summariza-
tion. In WI-IATW 06: Proceedings of the 2006
IEEE/WIC/ACM international conference on Web
Intelligence and Intelligent Agent Technology,pages
626–629,Washington,DC, USA.

Arora R. and Ravindran B. 2008. Latent Dirichlet Al-
location Based Multi-Document Summarization. In
Proceedings of the Second Workshop on Analytics
for Noisy Unstructured Text Data (AND 2008),91-
97.

Barzilay R. and Elhadad M. 1997. Using Lexical
Chains for Text Summarization. In Proceedings of
the ACL/EACL’97 Workshop on Intelligent Scalable
Text Summarization,pages 10-17.

Barzilay R. 2003. Information fusion for multi-
document summarization: Paraphrasing and gener-
ation, PhD thesis, DigitalCommons@Columbia.

Breiman L., Friedman J.H., Olshen R.A., and
Stone C.J. 1984. Classification and Regression
Trees. The Wadsworth Statistics/Probability Se-
ries,Belmont, CA: Wadsworth.

Chou A. Philip, Tom Lookabaugh, and Gray M.
Robert. 1989. Optimal Pruning with Applica-
tions to Tree-Structured Source Coding and Model-
ing. IEEE transactions on information theory, vol-
ume 35, no 2.

DUC–2002. 2002. Document Understanding Confer-
ence.

Edmundson H. P. 1969. New methods in automatic
extracting. Journal of the ACM,16:264-285.

Goldstein J., Mittal V., Carbonell J., and Kantrowitz M.
2000. Multi-document summarization by sentence
extraction. In Proceedings of the ANLP/NAACL
Workshop on Automatic Summarization,pages 40-
48.

H. van Halteren. 2002. Writing style recognition and
sentence extraction. In Proceedings of the workshop
on automatic summarization,pages 66–70.

Harabagiu S.M. and Lacatusu F. 2002. Generating
single and multi-document summaries with gistex-
ter. In Proceedings of the workshop on automatic
summarization,pages 30–38.

69



Hatzivassiloglou V., Klavans J. L., Holcombe M.
L., Barzilay R., Kan M.-Y., and McKeown K. R.
1999. Detecting text similarity over short pas-
sages: Exploring Linguistic Feature Combinations
via Machine Learning. In Proceedings of the 1999
Joint SIGDAT Conference on empirical Methods in
Natural Language Processing and very large cor-
pora,pages 203-212. College Park, MD, USA.

Hatzivassiloglou V., Klavans J. L., Holcombe M. L.,
Barzilay R., Kan M.-Y., and McKeown K. R. 2001.
SIMFINDER: A Flexible Clustering Tool for Sum-
marization. In NAACL Workshop on Automatic
Summarization,pages 41-49. Pittsburgh, PA, USA.

Hahn U. and Mani I. 2000. Computer. The
challenges of automatic summarization. IEEE
Computer,33(11),29–36.

Hovy E. and Lin C.Y. 1999. Automated Text Sum-
marization in SUMMARIST. Mani I and May-
bury M (eds.), Advances in Automatic Text Summa-
rization,pages 81–94. The MIT Press.

Johanna Geiss. 2011. Latent semantic sentence clus-
tering for multi-document summarization, PhD the-
sis. Cambridge University.

“Towards Multidocument Summarization by Refor-
mulation: Progress and Prospects”,

Kathleen McKeown, Judith Klavans, Vasilis Hatzivas-
siloglou, Regina Barzilay, Eleazar Eskin. 1999. To-
wards Multidocument Summarization by Reformu-
lation: Progress and Prospects. In Proceedings of
AAAI,Orlando, Florida.

Landauer T.K., Foltz P.W., and Laham D. 1998. In-
troduction to Latent Semantic Analysis. Discourse
Processes,25,pages 259–284.

Lin C.Y. and Hovy E. 2003. Automatic Evaluation
of Summaries Using N-gram Co-occurrence Statis-
tics. In North American Chapter of the Association
for Computational Linguistics on Human Language
Technology (HLTNAACL- 2003),pages 71-78.

Lin C–Y. 2004. Rouge: A package for automatic
evaluation of summaries. In Proceedings of Work-
shop on Text Summarization Branches Out, Post-
Conference Workshop of ACL 2004.

Luhn H.P. 1958. The Automatic Creation of
Literature Abstracts. IBM Journal of Research
Development,2(2):159-165.

Murray G., Renals S., and Carletta J. 2005. Extrac-
tive summarization of meeting recordings. In Pro-
ceedings of the 9th European Conference on Speech
Communication and Technology.

Radev D. R., Jing H., and Budzikowska M. 2000.
Centroid-based summarization of multiple docu-
ments: sentence extraction, utility-based evaluation,
and user studies, pages 21-29. In ANLP/NAACL
Workshop on Summarization, Morristown, NJ, USA.

Radev R., Blair–goldensohn S,Zhang Z. 2001. Exper-
iments in Single and Multi-Docuemtn Summariza-
tion using MEAD. InFirst Document Under- stand-
ing Conference,New Orleans,LA.

Radev D. R., Jing H., Stys M., and Tam D.
2004. Centroid-based summarization of mul-
tiple documents. Information Processing and
Management,40:919-938.

Scott Deerwester, Dumais T. Susan, Furnas W George ,
Landauer Thomas K., and Richard Harshman. 1990.
Indexing by latent semantic analysis. Journal of the
American Society of Information Science,41(6):391-
407.

Steinberger J. and Jezek K. 2004. Using Latent Se-
mantic Analysis in Text Summarization and Sum-
mary Evaluation. Proceedings of ISIM ’04, pages
93-100.

Teufel, Simone, and Marc Moens. 1997. Sentence
extraction as a classification task. ACL/EACL
workshop on Intelligent and scalable Text
summarization,58-65.

70


