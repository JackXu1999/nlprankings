



















































On the Evaluation of Semantic Phenomena in Neural Machine Translation Using Natural Language Inference


Proceedings of NAACL-HLT 2018, pages 513–523
New Orleans, Louisiana, June 1 - 6, 2018. c©2018 Association for Computational Linguistics

On the Evaluation of Semantic Phenomena in
Neural Machine Translation Using Natural Language Inference

Adam Poliak1 Yonatan Belinkov2 James Glass2 Benjamin Van Durme1
1Center for Language and Speech Processing

Johns Hopkins University, Baltimore, MD 21218
2Computer Science and Artificial Intelligence Laboratory

Massachusetts Institute of Technology, Cambridge, MA 02139
{azpoliak,vandurme}@cs.jhu.edu, {belinkov,glass}@mit.edu

Abstract

We propose a process for investigating the ex-
tent to which sentence representations arising
from neural machine translation (NMT) sys-
tems encode distinct semantic phenomena. We
use these representations as features to train a
natural language inference (NLI) classifier ba-
sed on datasets recast from existing semantic
annotations. In applying this process to a re-
presentative NMT system, we find its enco-
der appears most suited to supporting inferen-
ces at the syntax-semantics interface, as com-
pared to anaphora resolution requiring world-
knowledge. We conclude with a discussion on
the merits and potential deficiencies of the
existing process, and how it may be improved
and extended as a broader framework for eva-
luating semantic coverage.1

1. Introduction

What do neural machine translation (NMT) mo-
dels learn about semantics? Many researchers sug-
gest that state-of-the-art NMT models learn re-
presentations that capture the meaning of senten-
ces (Gu et al., 2016; Johnson et al., 2017; Zhou
et al., 2017; Andreas and Klein, 2017; Neubig,
2017; Koehn, 2017). However, there is limited un-
derstanding of how specific semantic phenome-
na are captured in NMT representations beyond
this broad notion. For instance, how well do the-
se representations capture Dowty (1991)’s the-
matic proto-roles? Are these representations suf-
ficient for understanding paraphrastic inference?
Do the sentence representations encompass com-
plex anaphora resolution? We argue that existing
semantic annotations recast as Natural Langua-
ge Inference (NLI) can be leveraged to investi-
gate whether sentence representations encoded by
NMT models capture these semantic phenomena.

1Code developed and data used are available at https:
//github.com/boknilev/nmt-repr-analysis.

Sara adopted Jill, she wanted a child
DPR

Sara adopted Jill, Jill wanted a child
7

Iran possesses five research reactors
FN+

Iran has five research reactors
3

Berry Rejoins WPP Group
SPR

Berry was sentient
3

Figure 1: Example sentence pairs for the different se-
mantic phenomena. DPR deals with complex anaphora
resolution, FN+ is concerned with paraphrastic inferen-
ce, and SPR covers Reisinger et al. (2015)’s semantic
proto-roles. 3 / 7 indicates that the first sentence entails
/ does not entail the second.

We use sentence representations from pre-
trained NMT encoders as features to train classi-
fiers for NLI, the task of determining if one sen-
tence (a hypothesis) is supported by another (a
context).2 If the sentence representations learned
by NMT models capture distinct semantic pheno-
mena, we hypothesize that those representations
should be sufficient to perform well on NLI da-
tasets that test a model’s ability to capture these
phenomena. Figure 1 shows example NLI senten-
ce pairs with their respective labels and semantic
phenomena.

We evaluate NMT sentence representations of
4 NMT models from 2 domains on 4 different
NLI datasets to investigate how well they captu-
re different semantic phenomena. We use White
et al. (2017)’s Unified Semantic Evaluation Fra-
mework (USEF) that recasts three semantic phe-
nomena NLI: 1) semantic proto-roles, 2) paraph-
rastic inference, 3) and complex anaphora resolu-
tion. Additionally, we evaluate the NMT sentence
representations on 4) Multi-NLI, a recent exten-
sion of the Stanford Natural Language Inference
dataset (SNLI) (Bowman et al., 2015) that inclu-
des multiple genres and domains (Williams et al.,

2Sometimes referred to as recognizing textual entail-
ment (Dagan et al., 2006, 2013).

513



2017). We contextualize our results with a stan-
dard neural encoder described in Bowman et al.
(2015) and used in White et al. (2017).

Based on the recast NLI datasets, our investiga-
tion suggests that NMT encoders might learn mo-
re about semantic proto-roles than anaphora reso-
lution or paraphrastic inference. We note that the
target-side language affects how an NMT source-
side encoder captures these semantic phenomena.

2. Motivation

Why use recast NLI? We focus on NLI, as op-
posed to a wide range of NLP taks, as a uni-
fied framework that can capture a variety of se-
mantic phenomena based on arguments by Whi-
te et al. (2017). Their recast dataset enables us to
study whether NMT encoders capture “distinct ty-
pes of semantic reasoning” under just one task. We
choose these specific semantic phenomena for two
reasons. First, a long term goal is to understand
how combinations of different corpora and neural
architectures can contribute to a system’s ability to
perform general language understanding. As hu-
mans can understand (annotate consistently) the
sentence pairs used in our experiments, we would
similarly like our final system to have this same
capability. We posit that it is necessary but not ne-
cessarily sufficient for a language understanding
system to be able to capture the semantic pheno-
mena considered here. Second, we believe these
semantic phenomena might be relevant for trans-
lation. We demonstrate this with a few examples.

Anaphora Anaphora resolution connects to-
kens, typically pronouns, to their referents. Anap-
hora resolution should occur when translating
from morphologically poor languages into so-
me morphologically rich languages. For example,
when translating “The parent fed the child becau-
se she was hungry,” a Spanish translation should
describe the child as la niña (fem.) and not el ni-
ño (masc.) since she refers to the child. Becau-
se world knowledge is often required to perform
anaphora resolution (Rahman and Ng, 2012; Ja-
vadpour, 2013), this may enable evaluating whet-
her an NMT encoder learns world knowledge. In
this example, she refers to the child and not the
parent since world knowledge dictates that parents
often feed children when children are hungry.

Proto-roles Dowty (1991)’s proto-roles may be
expressed differently in different languages, and

so correctly identifying them can be important for
translation. For example, English does not usually
explicitly mark volition, a proto-role, except by
using adverbs like intentionally or accidentally.
Other languages mark volitionality by using spe-
cial affixes (e.g., Tibetan and Sesotho, a Bantu lan-
guage), case marking (Hindi, Sinhalese), or au-
xiliaries (Japanese).3 Correctly generating these
markers may require the MT system to encode vo-
litionality on the source side.

Paraphrases Callison-Burch (2007) discusses
how paraphrases help statistical MT (SMT) when
alignments from source words to target-language
words are unknown. If the alignment model can
map a paraphrase of the source word to a word
in the target language, then the SMT model can
translate the original word based on its paraph-
rase.4 Paraphrases are also used by professional
translators to deal with non-equivalence of words
in the source and target languages (Baker, 2018).

3. Methodology

We use NMT models based on bidirectio-
nal long short-term memory (Bi-LSTM) encoder-
decoders with attention (Sutskever et al., 2014;
Bahdanau et al., 2015), trained on a parallel cor-
pus. Given an NLI context-hypothesis pair, we
pass each sentence independently through a trai-
ned NMT encoder to extract their respective vec-
tor representations. We represent each sentence by
concatenating the last hidden state from the for-
ward and backward encoders, resulting in v and u
(in R2d) for the context and hypothesis.5 We fo-
llow the common practice of feeding the conca-
tenation (v,u) ∈ R4d to a classifier (Rocktäs-
chel et al., 2016; Bowman et al., 2015; Mou et al.,
2016; Liu et al., 2016; Cheng et al., 2016; Munkh-
dalai and Yu, 2017).

Sentence pair representations are fed into a clas-
sifier with a softmax layer that maps onto the num-
ber of labels. Experiments with both linear and
non-linear classifiers have not shown major diffe-
rences, so we report results with the linear classi-
fier unless noted otherwise. We report implemen-
tation details in Appendix B.

3For references and examples, see: en.wikipedia.
org/wiki/Volition_(linguistics).

4Using paraphrases can help NMT models generate text in
the target language in some settings (Sekizawa et al., 2017).

5We experimented with other sentence representations
and their combinations, and did not see differences in ove-
rall conclusions. See Appendix A for these experiments.

514



Train

Test DPR: 50.0 SPR: 65.4 FN+: 57.5
ar es zh de USEF ar es zh de USEF ar es zh de USEF

DPR 49.8 50.0 50.0 50.0 49.5 45.4 57.1 47.0 43.9 65.2 48.0 55.9 51.0 46.8 19.2
SPR 50.1 50.3 50.1 49.9 50.7 72.1 74.2 73.6 73.1 80.6 56.3 57.0 56.9 56.1 65.8
FN+ 50.0 50.0 50.4 50.0 49.5 57.3 63.6 54.5 60.7 60.0 56.2 56.1 54.3 55.5 80.5

Table 1: Accuracy on NLI with representations generated by encoders of English→{ar,es,zh,de} NMT models.
Rows correspond to the training and validation sets and major columns correspond to the test set. The column
labeled “USEF” refers to the test accuracies reported in White et al. (2017). The numbers on the top row represents
each dataset’s majority baseline. Bold numbers indicate the highest performing model for the given dataset.

4. Data

MT data We train NMT models on four langua-
ge pairs: English → {Arabic (ar), Spanish (es),
Chinese (zh), and German (de)}. See Appendix B
for training details. The first three pairs use the
United Nations parallel corpus (Ziemski et al.,
2016) and for English-German, we use the WMT
dataset (Bojar et al., 2014). Although the entail-
ment classifier only uses representations extracted
from the English-side encoders as features, using
multiple language pairs allows us to explore whet-
her different target languages affect what semantic
phenomena are captured by an NMT encoder.

Natural Language Inference data We use
four distinct datasets to train classifiers: Multi-
NLI (Williams et al., 2017), a recent expansion
of SNLI containing a broad array of domains that
was used in the 2017 RepEval shared task (Nan-
gia et al., 2017), and three recast NLI datasets
from The JHU Decompositional Semantics Initia-
tive (Decomp)6 released by White et al. (2017).
Sentence-pairs and labels were recast, i.e. auto-
matically converted, from existing semantic an-
notations: FrameNet Plus (FN+) (Pavlick et al.,
2015), Definite Pronoun Resolution (DPR) (Rah-
man and Ng, 2012), and Semantic Proto-Roles
(SPR) (Reisinger et al., 2015). The FN+ portion
contains sentence pairs based on paraphrastic in-
ference, DPR’s sentence pairs focus on identif-
ying the correct antecedent for a definite pronoun,
and SPR’s sentence pairs test whether the semantic
proto-roles from Reisinger et al. (2015) apply ba-
sed on a given sentence.7 Recasting makes it easy
to determine how well an NLI method captures the
fine-grained semantics inspired by Dowty (1991)’s
thematic proto-roles, paraphrastic inference, and
complex anaphora resolutions. Table 2 includes
the datasets’ statistics.

6decomp.net
7We refer the reader to White et al. (2017) for detailed

discussion on how the existing datasets were recast as NLI.

DPR SPR FN+ MNLI

Train 2K 123K 124K 393K
Dev .4K 15K 15K 9K
Test 1K 15K 14K 9K

Table 2: Number of sentences in NLI datasets.

5. Results

Table 1 shows results of NLI classifiers trained
on representations from different NMT encoders.
We also report the majority baseline and the results
of Bowman et al.’s 3-layer deep 200 dimensional
neural network used by White et al. (“USEF”).

Paraphrastic entailment (FN+) Our classifiers
predict FN+ entailment worse than the majority
baseline, and drastically worse than USEF when
trained on FN+’s training set. Since FN+ tests pa-
raphrastic inference and NMT models have been
shown to be useful to generate sentential paraphra-
se pairs (Wieting and Gimpel, 2017; Wieting et al.,
2017), it is surprising that our classifiers using the
representations from the NMT encoder perform
poorly. Although the sentences in FN+ are much
longer than in the other datasets, sentence length
does not seem to be responsible for the poor FN+
results. The classifiers do not noticeably perform
better on shorter sentences than longer ones, as no-
ted in Appendix C.

Upon manual inspection, we noticed that in
many not-entailed examples, swapped paraphra-
ses had different part-of-speech (POS) tags. This
begs the question of whether different POS tags
for swapped paraphrases affects the accuracies.
Using Stanford CoreNLP (Manning et al., 2014),
we partition our validation set based on whether
the paraphrases share the same POS tag. Table 3
reports dev set accuracies using classifiers trained
on FN+. Classifiers using features from NMT en-
coders trained on the three languages from the UN
corpus noticeably perform better on cases where
paraphrases have different POS tags compared to
paraphrases with the same POS tags. These dif-

515



ar es zh de

Same Tag 52.9 52.6 52.6 50.2
Different Tag 55.8 59.1 53.4 46.0

Table 3: Accuracies on FN+’s dev set based on whether
the swapped paraphrases share the same POS tag.

ferences might suggest that the recast FN+ might
not be an ideal dataset to test how well NMT enco-
ders capture paraphrastic inference. The sentence
representations may be impacted more by ungram-
maticality caused by different POS tags as oppo-
sed to poor paraphrases.

Anaphora entailment (DPR) The low accura-
cies for predicting NLI targeting anaphora resolu-
tion are similar to White et al. (2017)’s findings.
They suggest that the model has difficulty in cap-
turing complex anaphora resolution. By using con-
trastive evaluation pairs, Bawden et al. (2017) re-
cently suggested as well that NMT models are
poorly suited for co-reference resolution. Our re-
sults are not surprising given that DPR tests whet-
her a model contains common sense knowled-
ge (Rahman and Ng, 2012). In DPR, syntactic
cues for co-reference are purposefully balanced
out as each pair of pro-nouns appears in at least
two context-hypothesis pairs (Table 9). This forces
the model’s decision to be informed by semantics
and world knowledge – a model cannot use syn-
tactic cues to help perform anaphora resolution.8

Although the poor performance of NMT represen-
tations may be explained by a variety of reasons,
e.g. training data, architectures, etc., we would still
like ideal MT systems to capture the semantics of
co-reference, as evidenced in the example in §2.

Even though the classifiers perform poorly
when predicting paraphrastic entailment, they sur-
prisingly outperform USEF by a large margin
(around 25–30%) when using a model trained on
DPR.9 This might suggest that an NMT encoder
can pick up on how pronouns may be used as a ty-
pe of lexical paraphrase (Bhagat and Hovy, 2013).

Proto-role entailment (SPR) When predicting
SPR entailments using a classifier trained on SPR
data, we noticeably outperform the majority base-
line but are below USEF. Both ours and USEF’s
accuracies are lower than Teichert et al. (2017)’s
best reported numbers. This is not surprising as
Teichert et al. condition on observed semantic role
labels when predicting proto-role labels.

8Appendix D includes some illustrative examples.
9This is seen in the last columns of the top row in Table 1.

Proto-Role ar es zh de avg MAJ

physically existed 70.6 70.8 77.2 70.8 72.4† 65.9
sentient 78.5 82.2 80.5 81.7 80.7† 75.5
aware 75.9 77.0 76.6 76.7 76.6† 60.9
volitional 74.3 76.8 74.7 73.7 74.9† 64.5
existed before 68.4 70.5 66.5 68.4 68.5† 64.8
caused 69.4 74.1 72.2 72.7 72.1† 63.4

changed 64.2 62.4 63.8 62.0 63.1 65.1
location 91.1 90.1 90.4 90.2 90.4 91.7
moved 90.6 88.8 90.1 90.3 89.9 93.3
used in 34.9 38.1 31.8 34.2 34.7 55.2
existed after 62.7 69.0 65.6 65.2 65.7 69.7
chang. state 61.8 60.7 60.9 60.7 61.0 65.2
chang. possession 89.6 88.6 89.9 88.3 89.1 93.9
stationary during 86.3 84.4 90.5 86.0 86.8 96.3
physical contact 85.0 82.0 84.5 84.4 84.0 85.8
existed during 59.3 71.8 60.8 64.4 64.1 84.7

Table 4: Accuracies on the SPR test set broken down
by each proto-role. “avg” represents the score for the
proto-role averaged across target languages. Bold and
† respectively indicate the best results for each proto-
role and whether all of our classifiers outperformed the
proto-role’s majority baseline.

Table 4 reports accuracies for each proto-role.
Whenever one of the classifiers outperforms the
baseline for a proto-role, all the other classifiers
do as well. The classifiers outperform the majo-
rity baseline for 6 of the reported 16 proto-roles.
We observe these 6 properties are more associated
with proto-agents than proto-patients.

The larger improvements over the majority ba-
seline for SPR compared to FN+ and DPR is not
surprising. Dowty (1991) posited that proto-agent,
and -patient should correlate with English syntac-
tic subject, and object, respectively, and empiri-
cally the necessity of [syntactic] parsing for pre-
dicate argument recognition has been observed
in practice (Gildea and Palmer, 2002; Punyaka-
nok et al., 2008). Further, recent work is sugges-
tive that LSTM-based frameworks implicitly may
encode syntax based on certain learning objecti-
ves (Linzen et al., 2016; Shi et al., 2016; Belin-
kov et al., 2017b). It is unclear whether NMT en-
coders capture semantic proto-roles specifically or
just underlying syntax that affects the proto-roles.

NMT target language Our experiments show
differences based on which target language was
used to train the NMT encoder, in capturing se-
mantic proto-roles and paraphrastic inference. In
Table 1, we notice a large improvement using sen-
tence representations from an NMT encoder that
was trained on en-es parallel text. The improve-
ments are most profound when a classifier trained
on DPR data predicts entailment focused on se-

516



ar es zh de MAJ

MNLI-1 45.9 45.7 46.6 48.0 35.6
MNLI-2 46.6 46.7 48.2 48.9 36.5

Table 5: Accuracies for MNLI test sets. MNLI-1 refers
to the matched case and MNLI-2 is the mismatched.

mantic proto-roles or paraphrastic inference. We
also note that using the NMT encoder trained on
en-es parallel text results in the highest results in
5 of the 6 proto-roles in the top portion of Ta-
ble 4. When using other sentence representations
(Appendix A), we notice that using representa-
tions from English-German encoders consistently
outperforms using the other encoders (Tables 6
and 7). This prevents us from making generaliza-
tions regarding specific target side languages.

NLI across multiple domains Though our main
focus is exploring what NMT encoders learn about
distinct semantic phenomena, we would like to
know how useful NMT models are for general
NLI across multiple domains. Therefore, we also
evaluate the sentence representations with Multi-
NLI. As indicated by Table 5, the representa-
tions perform noticeably better than a majority ba-
seline. However, our results are not competitive
with state-of-the-art systems trained specifically
for Multi-NLI (Nangia et al., 2017).

6. Related Work

In concurrent work, Poliak et al. (2018) explore
whether NLI datasets contain statistical irregulari-
ties by training a model with access to only hypot-
heses. Their model significantly outperforms the
majority baseline and our results on Multi-NLI,
SPR, and FN+. They suggest that these, among ot-
her NLI datasets, contain statistical irregularities.
Their findings illuminate issues with the recast da-
tasets we consider, but do not invalidate our ap-
proach of using recast NLI to determine whether
NMT encoders capture distinct semantic pheno-
mena. Instead, they force us to re-evaluate the ma-
jority baseline as an indicator of whether encoders
learn distinct semantics and to what extent we can
make conclusions based on these recast datasets.

Prior work has focused on the relationship bet-
ween semantics and machine translation. MEANT
and its extension XMEANT evaluate MT systems
based on semantics (Lo and Wu, 2011; Lo et al.,
2014). Others have focused on incorporating se-
mantics directly in MT. Chan et al. (2007) use
word sense disambiguation to help statistical MT,

Gao and Vogel (2011) add semantic-roles to im-
prove phrase-based MT, and Carpuat et al. (2017)
demonstrate how filtering parallel sentences that
are not parallel in meaning improves translation.
Recent work explores how representations learned
by NMT systems can improve semantic tasks. Mc-
Cann et al. (2017) show improvements in many
tasks by using contextualized word vectors extrac-
ted from a LSTM encoder trained for MT. Their
goal is to use NMT to improve other tasks whi-
le we focus on using NLI to determine what NMT
models learn about different semantic phenomena.

Researchers have explored what NMT models
learn about other linguistic phenomena, such as
morphology (Dalvi et al., 2017; Belinkov et al.,
2017a), syntax (Shi et al., 2016), and lexical se-
mantics (Belinkov et al., 2017b), including word
senses (Marvin and Koehn, 2018; Liu et al., 2018)

7. Conclusion and Future Work

Researchers suggest that NMT models learn
sentence representations that capture meaning. We
inspected whether distinct types of semantics are
captured by NMT encoders. Our experiments sug-
gest that NMT encoders might learn the most
about semantic proto-roles, do not focus on anap-
hora resolution, and may poorly capture paraph-
rastic inference. We conclude by suggesting that
target-side language affects how well an NMT en-
coder captures these semantic phenomena.

In future work, we would like to study how
well NMT encoders capture other semantic pheno-
mena, possibly by recasting other datasets. Com-
paring how semantic phenomena are represen-
ted in different NMT architectures, e.g. purely
convolutional (Gehring et al., 2017) or attention-
based (Vaswani et al., 2017), may shed light on
whether different architectures may better captu-
re semantic phenomena. Finally, investigating how
multilingual systems learn semantics can bring a
new perspective to questions of universality of re-
presentation (Schwenk and Douze, 2017).

Acknowledgements

This work was supported by JHU-HLTCOE,
DARPA LORELEI, and Qatar Computing Re-
search Institute. We thank anonymous reviewers.
Views and conclusions contained in this publica-
tion are those of the authors and should not be in-
terpreted as representing official policies or endor-
sements of DARPA or the U.S. Government.

517



References
Jacob Andreas and Dan Klein. 2017. Analogs of lin-

guistic structure in deep representations. In Pro-
ceedings of the 2017 Conference on Empirical Met-
hods in Natural Language Processing. Association
for Computational Linguistics, pages 2893–2897.

Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Ben-
gio. 2015. Neural machine translation by jointly
learning to align and translate. In International Con-
ference on Learning Representations (ICLR).

Mona Baker. 2018. In other words: A coursebook on
translation. Routledge.

Rachel Bawden, Rico Sennrich, Alexandra Birch, and
Barry Haddow. 2017. Evaluating discourse pheno-
mena in neural machine translation. arXiv preprint
arXiv:1711.00513 .

Yonatan Belinkov, Nadir Durrani, Fahim Dalvi, Hassan
Sajjad, and James Glass. 2017a. What do Neural
Machine Translation Models Learn about Morpho-
logy? In Proceedings of the 55th Annual Meeting of
the Association for Computational Linguistics (Vo-
lume 1: Long Papers). Association for Computatio-
nal Linguistics, pages 861–872. https://doi.
org/10.18653/v1/P17-1080.

Yonatan Belinkov, Lluís Màrquez, Hassan Sajjad, Na-
dir Durrani, Fahim Dalvi, and James Glass. 2017b.
Evaluating Layers of Representation in Neural Ma-
chine Translation on Part-of-Speech and Semantic
Tagging Tasks. In Proceedings of the Eighth In-
ternational Joint Conference on Natural Language
Processing (Volume 1: Long Papers). Asian Fede-
ration of Natural Language Processing, Taipei, Tai-
wan, pages 1–10.

Rahul Bhagat and Eduard Hovy. 2013. What is a
paraphrase? Computational Linguistics 39(3):463–
472.

Ondrej Bojar, Christian Buck, Christian Federmann,
Barry Haddow, Philipp Koehn, Johannes Leveling,
Christof Monz, Pavel Pecina, Matt Post, Herve
Saint-Amand, Radu Soricut, Lucia Specia, and Aleš
Tamchyna. 2014. Findings of the 2014 workshop
on statistical machine translation. In Proceedings of
the Ninth Workshop on Statistical Machine Trans-
lation. Association for Computational Linguistics,
Baltimore, Maryland, USA, pages 12–58.

Samuel R. Bowman, Gabor Angeli, Christopher Potts,
and Christopher D. Manning. 2015. A large an-
notated corpus for learning natural language infe-
rence. In Proceedings of the 2015 Conference on
Empirical Methods in Natural Language Processing
(EMNLP). Association for Computational Linguis-
tics.

Chris Callison-Burch. 2007. Paraphrasing
and Translation. Ph.D. thesis, University
of Edinburgh, Edinburgh, Scotland. http:
//cis.upenn.edu/~ccb/publications/
callison-burch-thesis.pdf.

Marine Carpuat, Yogarshi Vyas, and Xing Niu. 2017.
Detecting cross-lingual semantic divergence for
neural machine translation. In Proceedings of the
First Workshop on Neural Machine Translation.
Association for Computational Linguistics, pages
69–79. http://aclweb.org/anthology/
W17-3209.

Seng Yee Chan, Tou Hwee Ng, and David Chiang.
2007. Word Sense Disambiguation Improves Sta-
tistical Machine Translation. In Proceedings of the
45th Annual Meeting of the Association of Compu-
tational Linguistics. Association for Computational
Linguistics, pages 33–40.

Jianpeng Cheng, Li Dong, and Mirella Lapata. 2016.
Long short-term memory-networks for machine
reading. In Proceedings of the 2016 Conference
on Empirical Methods in Natural Language Pro-
cessing. Association for Computational Linguistics,
Austin, Texas, pages 551–561.

Kyunghyun Cho, Bart van Merrienboer, Dzmitry Bah-
danau, and Yoshua Bengio. 2014. On the properties
of neural machine translation: Encoder–decoder ap-
proaches. In Proceedings of SSST-8, Eighth Works-
hop on Syntax, Semantics and Structure in Statisti-
cal Translation. Association for Computational Lin-
guistics, Doha, Qatar, pages 103–111. http://
www.aclweb.org/anthology/W14-4012.

Ronan Collobert, Koray Kavukcuoglu, and Clément
Farabet. 2011. Torch7: A Matlab-like Environment
for Machine Learning. In BigLearn, NIPS works-
hop. EPFL-CONF-192376.

Alexis Conneau, Douwe Kiela, Holger Schwenk, Loïc
Barrault, and Antoine Bordes. 2017. Supervised
learning of universal sentence representations from
natural language inference data. In Proceedings of
the 2017 Conference on Empirical Methods in Natu-
ral Language Processing. Association for Compu-
tational Linguistics, pages 670–680. http://
aclweb.org/anthology/D17-1070.

Ido Dagan, Oren Glickman, and Bernardo Magnini.
2006. The pascal recognising textual entailment
challenge. In Machine learning challenges. evalua-
ting predictive uncertainty, visual object classifica-
tion, and recognising tectual entailment, Springer,
pages 177–190.

Ido Dagan, Dan Roth, Mark Sammons, and Fabio Mas-
simo Zanzotto. 2013. Recognizing textual entail-
ment: Models and applications. Synthesis Lectures
on Human Language Technologies 6(4):1–220.

Fahim Dalvi, Nadir Durrani, Hassan Sajjad, Yonatan
Belinkov, and Stephan Vogel. 2017. Understanding
and improving morphological learning in the neu-
ral machine translation decoder. In Proceedings of
the Eighth International Joint Conference on Natu-
ral Language Processing (Volume 1: Long Papers).
Asian Federation of Natural Language Processing,
Taipei, Taiwan, pages 142–151.

518



David Dowty. 1991. Thematic proto-roles and argu-
ment selection. Language pages 547–619.

Qin Gao and Stephan Vogel. 2011. Utilizing Target-
Side Semantic Role Labels to Assist Hierarchical
Phrase-based Machine Translation. In Proceedings
of Fifth Workshop on Syntax, Semantics and Structu-
re in Statistical Translation. Association for Compu-
tational Linguistics, pages 107–115.

Jonas Gehring, Michael Auli, David Grangier, Denis
Yarats, and Yann N. Dauphin. 2017. Convolutio-
nal Sequence to Sequence Learning. In Doina Pre-
cup and Yee Whye Teh, editors, Proceedings of the
34th International Conference on Machine Lear-
ning. PMLR, International Convention Centre, Syd-
ney, Australia, volume 70 of Proceedings of Machi-
ne Learning Research, pages 1243–1252.

Daniel Gildea and Martha Palmer. 2002. The neces-
sity of parsing for predicate argument recognition.
In Proceedings of the 40th Annual Meeting on Asso-
ciation for Computational Linguistics. Association
for Computational Linguistics, pages 239–246.

Jiatao Gu, Zhengdong Lu, Hang Li, and Victor O.K.
Li. 2016. Incorporating copying mechanism in
sequence-to-sequence learning. In Proceedings of
the 54th Annual Meeting of the Association for
Computational Linguistics (Volume 1: Long Pa-
pers). Association for Computational Linguistics,
Berlin, Germany, pages 1631–1640.

Seyedeh Leili Javadpour. 2013. Resolving pronomi-
nal anaphora using commonsense knowledge. Ph.D.
thesis, Louisiana State University.

Melvin Johnson, Mike Schuster, Quoc Le, Maxim Kri-
kun, Yonghui Wu, Zhifeng Chen, Nikhil Thorat, Fer-
nand a ViÃ c©gas, Martin Wattenberg, Greg Corra-
do, Macduff Hughes, and Jeffrey Dean. 2017. Goo-
gle’s multilingual neural machine translation sys-
tem: Enabling zero-shot translation. Transactions
of the Association for Computational Linguistics
5:339–351.

Philipp Koehn. 2017. Neural machine translation. ar-
Xiv preprint arXiv:1709.07809 .

Tal Linzen, Emmanuel Dupoux, and Yoav Goldberg.
2016. Assessing the Ability of LSTMs to Learn
Syntax-Sensitive Dependencies. Transactions of the
Association of Computational Linguistics 4(1):521–
535.

Frederick Liu, Han Lu, and Graham Neubig. 2018.
Handling Homographs in Neural Machine Transla-
tion. In Proceedings of the 16th Annual Conferen-
ce of the North American Chapter of the Associa-
tion for Computational Linguistics: Human Langua-
ge Technologies.

Yang Liu, Chengjie Sun, Lei Lin, and Xiaolong Wang.
2016. Learning natural language inference using bi-
directional lstm model and inner-attention. arXiv
preprint arXiv:1605.09090 .

Chi-kiu Lo, Meriem Beloucif, Markus Saers, and De-
kai Wu. 2014. Xmeant: Better semantic mt eva-
luation without reference translations. In Procee-
dings of the 52nd Annual Meeting of the Association
for Computational Linguistics (Volume 2: Short Pa-
pers). volume 2, pages 765–771.

Chi-kiu Lo and Dekai Wu. 2011. Meant: an inexpensi-
ve, high-accuracy, semi-automatic metric for evalua-
ting translation utility via semantic frames. In Pro-
ceedings of the 49th Annual Meeting of the Associa-
tion for Computational Linguistics: Human Langua-
ge Technologies-Volume 1. Association for Compu-
tational Linguistics, pages 220–229.

Christopher D. Manning, Mihai Surdeanu, John Bauer,
Jenny Finkel, Steven J. Bethard, and David Mc-
Closky. 2014. The Stanford CoreNLP natural
language processing toolkit. In Association for
Computational Linguistics (ACL) System Demons-
trations. pages 55–60. http://www.aclweb.
org/anthology/P/P14/P14-5010.

Rebecca Marvin and Philipp Koehn. 2018. Explo-
ring Word Sense Disambiguation Abilities of Neu-
ral Machine Translation Systems. In Proceedings of
the 13th Conference of The Association for Machi-
ne Translation in the Americas (Volume 1: Research
Track. pages 125–131.

Bryan McCann, James Bradbury, Caiming Xiong, and
Richard Socher. 2017. Learned in translation: Con-
textualized word vectors. In I. Guyon, U. V. Lux-
burg, S. Bengio, H. Wallach, R. Fergus, S. Vishwa-
nathan, and R. Garnett, editors, Advances in Neu-
ral Information Processing Systems 30, Curran As-
sociates, Inc., pages 6297–6308.

Lili Mou, Rui Men, Ge Li, Yan Xu, Lu Zhang, Rui Yan,
and Zhi Jin. 2016. Natural language inference by
tree-based convolution and heuristic matching. In
Proceedings of the 54th Annual Meeting of the As-
sociation for Computational Linguistics (Volume 2:
Short Papers). Association for Computational Lin-
guistics, Berlin, Germany, pages 130–136. http:
//anthology.aclweb.org/P16-2022.

Tsendsuren Munkhdalai and Hong Yu. 2017. Neural
tree indexers for text understanding. In Proceedings
of the 15th Conference of the European Chapter of
the Association for Computational Linguistics: Vo-
lume 1, Long Papers. Association for Computational
Linguistics, Valencia, Spain, pages 11–21.

Nikita Nangia, Adina Williams, Angeliki Lazaridou,
and Samuel Bowman. 2017. The repeval 2017
shared task: Multi-genre natural language inferen-
ce with sentence representations. In Proceedings of
the 2nd Workshop on Evaluating Vector Space Re-
presentations for NLP. pages 1–10.

Graham Neubig. 2017. Neural machine translation and
sequence-to-sequence models: A tutorial. arXiv pre-
print arXiv:1703.01619 .

519



Ellie Pavlick, Travis Wolfe, Pushpendre Rastogi,
Chris Callison-Burch, Mark Dredze, and Benjamin
Van Durme. 2015. Framenet+: Fast paraphrastic tri-
pling of framenet. In Proceedings of the 53rd An-
nual Meeting of the Association for Computational
Linguistics and the 7th International Joint Confe-
rence on Natural Language Processing (Volume 2:
Short Papers). Association for Computational Lin-
guistics, Beijing, China, pages 408–413.

Adam Poliak, Jason Naradowsky, Aparajita Haldar, Ra-
chel Rudinger, and Benjamin Van Durme. 2018.
Hypothesis only baselines for natural language in-
ference. In The Seventh Joint Conference on Lexical
and Computational Semantics (*SEM).

Vasin Punyakanok, Dan Roth, and Wen-tau Yih. 2008.
The importance of syntactic parsing and inference in
semantic role labeling. Computational Linguistics
34(2):257–287.

Altaf Rahman and Vincent Ng. 2012. Resolving com-
plex cases of definite pronouns: The winograd sche-
ma challenge. In Proceedings of the 2012 Joint Con-
ference on Empirical Methods in Natural Language
Processing and Computational Natural Language
Learning. Association for Computational Linguis-
tics, Jeju Island, Korea, pages 777–789. http://
www.aclweb.org/anthology/D12-1071.

Drew Reisinger, Rachel Rudinger, Francis Ferraro,
Craig Harman, Kyle Rawlins, and Benjamin Van
Durme. 2015. Semantic proto-roles. Transactions
of the Association for Computational Linguistics
3:475–488.

Tim Rocktäschel, Edward Grefenstette, Karl Moritz
Hermann, Tomas Kocisky, and Phil Blunsom. 2016.
Reasoning about entailment with neural attention.
In International Conference on Learning Represen-
tations (ICLR).

Holger Schwenk and Matthijs Douze. 2017. Learning
joint multilingual sentence representations with neu-
ral machine translation. In Proceedings of the 2nd
Workshop on Representation Learning for NLP. pa-
ges 157–167.

Yuuki Sekizawa, Tomoyuki Kajiwara, and Mamoru
Komachi. 2017. Improving japanese-to-english
neural machine translation by paraphrasing the tar-
get language. In Proceedings of the 4th Workshop
on Asian Translation (WAT2017). Asian Federation
of Natural Language Processing, Taipei, Taiwan, pa-
ges 64–69.

Xing Shi, Inkit Padhi, and Kevin Knight. 2016. Does
string-based neural mt learn source syntax? In
Proceedings of the 2016 Conference on Empirical
Methods in Natural Language Processing. Asso-
ciation for Computational Linguistics, Austin, Te-
xas, pages 1526–1534. https://aclweb.org/
anthology/D16-1159.

Ilya Sutskever, Oriol Vinyals, and Quoc V Le. 2014.
Sequence to sequence learning with neural net-
works. In Advances in neural information proces-
sing systems. pages 3104–3112.

Adam Teichert, Adam Poliak, Benjamin Van Durme,
and Matthew R Gormley. 2017. Semantic proto-role
labeling. In Thirty-First AAAI Conference on Artifi-
cial Intelligence (AAAI-17).

Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob
Uszkoreit, Llion Jones, Aidan N Gomez, Ł ukasz
Kaiser, and Illia Polosukhin. 2017. Attention is All
you Need. In I. Guyon, U. V. Luxburg, S. Bengio,
H. Wallach, R. Fergus, S. Vishwanathan, and R. Gar-
nett, editors, Advances in Neural Information Pro-
cessing Systems 30, Curran Associates, Inc., pages
5998–6008.

Aaron Steven White, Pushpendre Rastogi, Kevin Duh,
and Benjamin Van Durme. 2017. Inference is
everything: Recasting semantic resources into a uni-
fied evaluation framework. In Proceedings of the
Eighth International Joint Conference on Natural
Language Processing (Volume 1: Long Papers).
Asian Federation of Natural Language Processing,
Taipei, Taiwan, pages 996–1005.

John Wieting and Kevin Gimpel. 2017. Pushing the
limits of paraphrastic sentence embeddings with mi-
llions of machine translations. arXiv preprint ar-
Xiv:1711.05732 .

John Wieting, Jonathan Mallinson, and Kevin Gim-
pel. 2017. Learning paraphrastic sentence em-
beddings from back-translated bitext. In Procee-
dings of the 2017 Conference on Empirical Met-
hods in Natural Language Processing. Association
for Computational Linguistics, Copenhagen, Den-
mark, pages 274–285. https://www.aclweb.
org/anthology/D17-1026.

Adina Williams, Nikita Nangia, and Samuel R Bow-
man. 2017. A broad-coverage challenge corpus for
sentence understanding through inference. arXiv
preprint arXiv:1704.05426 .

Hao Zhou, Zhaopeng Tu, Shujian Huang, Xiaohua Liu,
Hang Li, and Jiajun Chen. 2017. Chunk-based bi-
scale decoder for neural machine translation. In
Proceedings of the 55th Annual Meeting of the As-
sociation for Computational Linguistics (Volume 2:
Short Papers). Association for Computational Lin-
guistics, pages 580–586. https://doi.org/
10.18653/v1/P17-2092.

Michał Ziemski, Marcin Junczys-Dowmunt, and Bruno
Pouliquen. 2016. The united nations parallel cor-
pus v1.0. In Nicoletta Calzolari (Conference Chair),
Khalid Choukri, Thierry Declerck, Sara Goggi, Mar-
ko Grobelnik, Bente Maegaard, Joseph Mariani, He-
lene Mazo, Asuncion Moreno, Jan Odijk, and Ste-
lios Piperidis, editors, Proceedings of the Tenth In-
ternational Conference on Language Resources and
Evaluation (LREC 2016). European Language Re-
sources Association (ELRA), Paris, France.

520



A. Sentence Representations

In the experiments reported in the main paper,
we used a simple sentence representation, the first
and last hidden states of the forward and back-
ward encoders. We concatenated them for both
the context and the hypothesis and fed to a li-
near classifier. Here we compare the results of
InferSent (Conneau et al., 2017), a more in-
volved representation that was found to provide
a good sentence representation based on NLI da-
ta. Specifically, we concatenate the forward and
backward encodings for each sentence, and max-
pool over the length of the sentence, resulting in v
and u (in R2d) for the context and hypothesis. The
InferSent representation is defined by

(u,v, |u− v|,u ∗ v) ∈ R8d

where the product and subtraction are ca-
rried element-wise and commas denote vector-
concatenation.

The pair representation is fed into a multi-
layered perceptron (MLP) with one hidden layer
and a ReLU non-linearity. We set the hidden layer
size to 500 dimensions, similarly to Conneau et al.
(2017). The softmax layer maps onto the number
of labels, which is either 2 or 3 depending on the
dataset.

InferSent results Table 6 shows the results
of the classifier trained on NMT representations
with the InferSent architecture. Here, the repre-
sentations from NMT encoders trained on the
English-German parallel corpus slightly outper-
forms the others. Since this data used a different
corpus compared to the other language pairs, we
cannot determine whether the improved results are
due to the different target side language or corpus.
The main difference with respects to the simpler
sentence representation (Concat) is improved re-
sults on FN+. Table 7 shows the results on Multi-
NLI. It is interesting to note that, when using
the sentence representations from NMT encoders,
concatenating the sentence vectors outperformed
the InferSent method on Multi-NLI.

B. Implementation & Experimental
Details

We use 4-layer NMT systems with 500-
dimensional word embeddings and LSTM states
(i.e., d = 500). The vocabulary size is 75K words.

FN+ DPR SPRL

NMT Concat

en-ar 56.2 49.8 72.1
en-es 56.1 50.0 74.2
en-zh 54.3 50.0 73.1
en-de 55.5 50.0 73.1

NMT
InferSent

en-ar 57.9 50.0 73.6
en-es 58.0 50.0 72.7
en-zh 57.8 49.8 72.4
en-de 58.3 50.1 73.7

Majority 57.5 50.0 65.4
(White et al., 2017) 80.5 49.5 80.6

Table 6: NLI results on fine-grained semantic pheno-
mena. FN+ = paraphrases; DPR = pronoun resolution;
SPRL = proto-roles. NMT representations are com-
bined with either a simple concatenation (results co-
pied from Table 2) or the InferSent representation.
State-of-the-art (SOTA) is from White et al. (2017).

MNLI-1 MNLI-2

NMT
Concat

en-ar 45.9 46.6
en-es 45.7 46.7
en-zh 46.6 48.2
en-de 48.0 48.9

NMT
Infer-
Sent

en-ar 40.1 41.8
en-es 44.9 40.8
en-zh 43.7 42.1
en-de 41.3 41.1

Majority 35.6 36.5
SOTA 81.10 83.21

Table 7: Results on language inference on MultiN-
LI (Williams et al., 2017), matched/mismatched sce-
nario (MNLI1/2).

We train NMT models until convergence and ta-
ke the models that performed best on the develop-
ment set for generating representations to feed into
the entailment classifier. We use the hidden states
from the top encoding layer for obtaining senten-
ce representations since it has been hypothesized
that higher layers focus on word meaning, as op-
posed to syntax (Belinkov et al., 2017a,b). We re-
move long sentences (> 50 words) when training
both the classifier and the NMT model, as is com-
mon NMT practice (Cho et al., 2014). During tes-
ting, we use all test sentences regardless of senten-
ce length. Our implementation extends Belinkov
et al. (2017a)’s implementation in Torch (Collo-
bert et al., 2011).

We train English→Arabic/Spanish/Chinese
NMT models on the first 2 million senten-
ces of the United Nations parallel corpus
training set (Ziemski et al., 2016), and the
English→German model on the WMT data-

521



set (Bojar et al., 2014). We use the official
training/development/test splits.

In our NLI experiments, we do not train on
Multi-NLI and test on the recast datasets, or vice-
versa, since Multi-NLI since Multi-NLI uses a 3-
way classification (entailment, neutral, and con-
tradictions) while the recast datasets use just two
labels (entailed and not-entailed). In preliminary
experiments, we also used a 3-layered MLP. Alt-
hough the results slightly improved, we noted si-
milar trends to the linear classifier.

C. Sentence length

The average sentence in the FN+ test dataset is
31 words and almost 10% of the test sentences
are longer than 50 words. In SPR and DPR, each
premise sentence has on average 21 and 15 words
respectively and only 1% of sentences in SPR ha-
ve more than 50 words. No DPR sentences have
> 50 words.

Table 8 reports accuracies for ranges of
sentence lengths in FN+’s development set.
When trained on sentence representations form
an English→Chinese,German NMT encoder, the
NLI accuracies steadily decrease. When using
English→Arabic, the accuracies stay consistent
until sentences have between 70–80 tokens while
the results from English→Spanish quickly drops
from 0–10 to 10–20 but then stays relatively con-
sistent.

D. World Knowledge in DPR

When released, Rahman and Ng (2012)’s DPR
dataset confounded the best co-reference models
because “its difficulty stems in part from its relian-
ce on sophisticated knowledge sources.” Table 9
includes examples that demonstrate how world
knowledge is needed to accurately predict these
recast NLI sentence-pairs.

Lorem ipsum dolor sit amet, consectetuer adi-
piscing elit. Ut purus elit, vestibulum ut, placerat
ac, adipiscing vitae, felis. Curabitur dictum gravi-
da mauris. Nam arcu libero, nonummy eget, con-
sectetuer id, vulputate a, magna. Donec vehicula
augue eu neque. Pellentesque habitant morbi tristi-
que senectus et netus et malesuada fames ac turpis
egestas. Mauris ut leo. Cras viverra metus rhon-
cus sem. Nulla et lectus vestibulum urna fringilla
ultrices. Phasellus eu tellus sit amet tortor gravi-
da placerat. Integer sapien est, iaculis in, pretium
quis, viverra ac, nunc. Praesent eget sem vel leo

Sentence length ar es zh de total

0-10 46.8 63.7 66.0 65.4 526

10-20 49.0 53.3 57.4 56.5 2739

20-30 48.4 54.0 53.2 54.9 4889

30-40 48.4 54.1 51.2 53.9 4057

40-50 47.7 59.0 55.0 58.7 2064

50-60 49.1 56.1 54.5 57.5 877

60-70 46.4 53.6 43.9 44.1 444

70-80 59.9 51.6 43.3 43.3 252

Table 8: Accuracies on FN+’s dev set based on sen-
tence length. The first column represents the range of
sentences length: first number is inclusive and second
is exclusive. The last column represents how many con-
text sentences have lengths that are in the given row’s
range.

ultrices bibendum. Aenean faucibus. Morbi dolor
nulla, malesuada eu, pulvinar at, mollis ac, nulla.
Curabitur auctor semper nulla. Donec varius or-
ci eget risus. Duis nibh mi, congue eu, accumsan
eleifend, sagittis quis, diam. Duis eget orci sit amet
orci dignissim rutrum.

522



Chris was running after John, because he stole his watch
I Chris was running after John, because John stole his watch 3
I Chris was running after John, because Chris stole his watch 7
Chris was running after John, because he wanted to talk to him
I Chris was running after John, because Chris wanted to talk to him 3
I Chris was running after John, because John wanted to talk to him 7
The plane shot the rocket at the target, then it hit the target
I The plane shot the rocket at the target, then the rocket hit the target 3
I The plane shot the rocket at the target, then the target hit the target 7
Professors do a lot for students, but they are rarely thankful
I Professors do a lot for students, but students are rarely thankful 3
I Professors do a lot for students, but Professors are rarely thankful 7
MIT accepted the students, because they had good grades
I MIT accepted the students, because the students had good grades 3
I MIT accepted the students, because MIT had good grades 7
Obama beat John McCain, because he was the better candidate
I Obama beat John McCain, because Obama was the better candidate 3
I Obama beat John McCain, because John McCain was the better candidate 7
Obama beat John McCain, because he failed to win the majority of the
electoral votes
I Obama beat John McCain, because John McCain failed to win

the majority of the electoral votes 3
I Obama beat John McCain, because Obama failed to win

the majority of the electoral vote 7

Table 9: Examples from DPR’s dev set. The first line in each section is a context and lines with I are corresponding
hypotheses. 3 (7) in the last column indicates whether the hypothesis is entailed (or not) by the context.

523


