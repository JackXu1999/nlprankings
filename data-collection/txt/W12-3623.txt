










































Dependency Treebank of Urdu and its Evaluation


Proceedings of the 6th Linguistic Annotation Workshop, pages 157–165,
Jeju, Republic of Korea, 12-13 July 2012. c©2012 Association for Computational Linguistics

A Dependency Treebank of Urdu and its Evaluation

Riyaz Ahmad Bhat
LTRC, IIIT Hyderabad

riyaz.bhat@research.iiit.ac.in

Dipti Misra Sharma
LTRC, IIIT Hyderabad
dipti@iiit.ac.in

Abstract

In this paper we describe a currently un-
derway treebanking effort for Urdu-a South
Asian language. The treebank is built from
a newspaper corpus and uses a Karaka based
grammatical framework inspired by Paninian
grammatical theory. Thus far 3366 sen-
tences (0.1M words) have been annotated with
the linguistic information at morpho-syntactic
(morphological, part-of-speech and chunk in-
formation) and syntactico-semantic (depen-
dency) levels. This work also aims to evalu-
ate the correctness or reliability of this man-
ual annotated dependency treebank. Evalua-
tion is done by measuring the inter-annotator
agreement on a manually annotated data set of
196 sentences (5600 words) annotated by two
annotators. We present the qualitative analy-
sis of the agreement statistics and identify the
possible reasons for the disagreement between
the annotators. We also show the syntactic
annotation of some constructions specific to
Urdu like Ezafe and discuss the problem of
word segmentation (tokenization).

1 Introduction

Hindi and Urdu1 are often socially considered dis-
tinct language varieties, but linguistically the divi-
sion between the two varieties is not well-founded.
(Masica, 1993, p. 27) explains that while they are
different languages officially, they are not even dif-
ferent dialects or sub-dialects in a linguistic sense;
rather, they are different literary styles based on the

1Hindi-Urdu is an Indo-Aryan language spoken mainly in
North India and Pakistan.

same linguistically defined sub-dialect. He further
explains that at colloquial level, Hindi and Urdu are
nearly identical, both in terms of core vocabulary
and grammar. However, at formal and literary levels,
vocabulary differences begin to loom much larger
(Hindi drawing its higher lexicon from Sanskrit and
Urdu from Persian and Arabic) to the point where
the two styles/languages become mutually unintelli-
gible. In written form not only lexical items but the
way Urdu and Hindi is written makes one believe
that they are two separate languages. They are writ-
ten in separate orthographies, Hindi being written
in Devanagari, and Urdu in a modified Perso-Arabic
script. Under the treebanking effort for Indian lan-
guages, two separate treebanks are being built for
both Hindi and Urdu. Among the two, however,
Hindi treebank has matured and grown considerably
(Bhatt et al., 2009), (Palmer et al., 2009).

The paper is arranged as follows, next Section
gives a brief overview of the related works on syn-
tactic treebanking. Section 3 describes the grammat-
ical formalism chosen for the annotation. In Section
4 we discuss treebanking pipeline of Urdu followed
by some of the Urdu specific issues in Section 5. In
Section 6 we discuss the empirical results of inter-
annotator agreement. Section 7, concludes the pa-
per.

2 Related Work

A treebank is a text corpus annotated with syntactic,
semantic and sometimes even inter sentential rela-
tions (Hajičová et al., 2010). Treebanks are of multi-
fold importance, they are an invaluable resource for
testing linguistic theories on which they are built

157



and are used for a number of NLP tasks like train-
ing and testing syntactic parsers. Owing to their
great importance, a number of syntactic treebank-
ing projects have been initiated for many different
languages. Among the treebanks include Penn tree-
bank (PTB) (Marcus et al., 1993), Prague Depen-
dency treebank (PDT) (Hajicová, 1998) for Czech,
(Rambow et al., 2002) for English, Alpino (Van der
Beek et al., 2002) for Dutch, TUT (Bosco and Lom-
bardo, 2004) for Italian, TIGER (Brants et al., 2002)
for German and many others. Currently existing
treebanks mainly differ in the grammatical formal-
ism adopted. Dependency based formalism com-
pared with the constituency based formalism is as-
sumed to suit better for representing syntactic struc-
tures of free word order languages, its represen-
tation does not crucially rely on the position of a
syntactic unit in a sentence thus easily handles the
scrambling of arguments in such languages (Shieber,
1985), (Bharati et al., 1995), (Hajič, 1998), (Haji-
cová, 1998), (Oflazer et al., 2003). Not only are
dependency-based representations suitable for less
configurational languages, they are also favorable
for a number of natural language processing appli-
cations (Culotta and Sorensen, 2004), (Reichartz et
al., 2009).

Structural relations like subject and direct object
are believed to be less relevant for the grammatical
description of Indian languages (ILs) because of the
less configurational nature of these languages (Bhat,
1991). Indian languages are morphologically rich
and have a relatively free constituent order. (Begum
et al., 2008) have argued in favor of using Karaka
relations instead of structural relations for the syn-
tactic analysis of ILs. They proposed an annotation
scheme for the syntactic treebanking of ILs based
on the Computational Paninian Grammar (CPG), a
formalism inspired by Paninian grammatical theory.
Currently dependency treebanks of four ILs, namely
Hindi, Urdu, Bangla and Telegu, are under develop-
ment following this annotation scheme. The depen-
dency structures in all the four treebanks are, under
this annotation scheme, annotated with the Karaka
relations. Although English does not belong to the
free word order languages, a number of attempts
have been made to study the applicability of CPG
based syntactic analysis to it as well (Bharati et al.,
1996), (Vaidya et al., 2009), (Chaudhry and Sharma,

2011).

3 CPG Formalism

The CPG formalism, inspired by the grammatical
theory of Panini, the fifth century B.C. grammarian
of Sanskrit, is a dependency grammar. As in other
dependency grammars, the syntactic structures in
this formalism essentially consists of a set of binary,
asymmetric relations between words of a sentence.
A dependency relation is defined between a depen-
dent, a syntactically subordinate word and a head
word on which it depends. In this formalism verb is
treated as the primary modified (the root of the de-
pendency tree) and the elements (nominals) modify-
ing the verb participate in the activity specified by it.
The relation that holds between a verb and its mod-
ifier is called a karaka relation. There are six basic
karakas defined by Panini namely (i) karta ‘agent’,
(ii) karma ‘theme’, (iii) karana ‘instrument’, (iv)
sampradaan ‘recipient’, (v) apaadaan ‘source’, and
(vi) adhikarana ‘location’. Besides karaka relations
that hold between a verb and the participants of
the action specified by the verb, dependency rela-
tions also exist between nouns (genitives), between
nouns and their modifiers (adjectival modification,
relativization), between verbs and their modifiers
(adverbial modification including clausal subordina-
tion). A detailed tag-set containing all these differ-
ent kinds of dependency relations has been defined
in the annotation scheme based on the CPG formal-
ism (Bharati et al., 2009). Examples (1) and (2)
depict some of the karaka relations (k1 ‘karta’, k2
‘karma’, k3 ‘karana’) of verbs A�K
A

�
ê» ‘eat’ and A

�
�
KA

�
¿

‘cut’ respectively while example (3) shows a geni-
tive relation between two nouns, 	á�
A

�
K
 ‘Yasin’ and

ÕÎ
�
¯ ‘pen’.

(1) A�K
A
�
ê» I. �
 ÿ

	
�

	á�
A
�
K


yAsIn-ne
Yasin-ERG

saeb
apple-NOM

khAyA
eat-PST+PERF

‘Yasin ate an apple.’

158



A
�
K
A

�
ê»

I. �


k2

ÿ
	

�
	á�
A

�
K


k1

(2) A
�
�
KA

�
¿ I. �


�
 ÿ� ñ» A

�
g� ÿ

	
�

	á�
A
�
K


yAsIn-ne
Yasin-ERG

chAku-se
knife-INST

saeb
apple-NOM

kAtA
eat-PST+PERF
‘Yasin cut the apple with a knife.’

A
�
�
KA

�
¿

I. �


k2

ÿ� ñ» A
�

g�

k3

ÿ
	

�
	á�
A

�
K


k1

(3) ÕÎ
�
¯ A

�
¿

	á�
A
�
K


yAsIn-kA
Yasin-GEN

qalam
pen

‘Yasin’s pen.’

ÕÎ
�
¯ A

�
¿

	á�
A
�
K


��

r6

4 Annotation Pipeline

The dependency treebanks for Indian languages
based on CPG formalism are developed following
a generic pipeline. The process of treebank devel-
opment under the pipeline consists of a series of
steps namely (i) Tokenization, (ii) Morph-Analysis,
(iii) POS-tagging, (iv) Chunking, and (v) Depen-
dency annotation. Annotation process begins with
the tokenization of raw text. The tokens obtained
during tokenization are, in the next steps, annotated
with morphological and POS tag information. After
morph-analysis and POS-tagging correlated, insep-
arable words are grouped into chunks. The process-
ing at the steps mentioned thus far are automated
by highly accurate tools built in-house (tokenizer,

morph analyzer, POS-tagger and chunker). The out-
put of each tool is, however, manually corrected and
validated by the human annotators. The final step in
the pipeline is the manual dependency annotation.
Only the inter-chunk dependencies are marked leav-
ing the dependencies between words in a chunk un-
specified because the intra-chunk dependencies are
observed to be highly predictive given the head of a
chunk and can be easily generated by a set of rules
at a later stage.

UDT is steadily being developed following this
treebanking pipeline by annotating the newspaper
articles by a team of annotators with expertise in lin-
guistics. The tool being used for the annotation is
a part of Sanchay2 (Singh, 2006). The annotations
are represented in Shakti Standard Format (SSF)
(Bharati et al., 2007). Hitherto, 3226 sentences
(around 0.1M words) have been annotated with de-
pendency structure. Each sentence contains an av-
erage of 29 words and an average of 13.7 chunks of
average length 2.0.

5 Languages Specific Issues

5.1 Word segmentation
Urdu is written in a Nastaliq style cursive Arabic
script. In this script an individual letter acquires
different shapes upon joining with the adjacent let-
ters. There are four possible shapes a letter can
acquire namely initial,medial, final form in a
connected sequence of letters or an isolated form.
The letters acquiring all these four shapes depend-
ing on the context of their occurrence are called as
joiners. An another set of letters, however, called
as non − joiners do not adhere to this four-way
shaping. They only join with the letters before them
and have only final and isolated forms. An ex-
ample of a joiner is Arabic Letter ‘Teh’ �H and a

non-joiner is Arabic letter ‘waaw’ ð.
The concept of space as a word boundary marker

is not present in Urdu writing (Durrani and Hussain,
2010), (Lehal, 2010). Space character is primar-
ily required to generate correct shaping of words.
For example a space is necessary within the word
Y

	
J
�
Ó

�
H �Pð �Qå

�	
 “needy” to generate the visually cor-

rect and acceptable form of this word. Without
2http://apps.sanchay.co.in/latest-builds/

159



space it appears as Y	J �Ü
�
ß �Pð �Qå

�	
 which is visually in-

correct. In contrast to this, writers of Urdu find it
unnecessary to insert a space between the two words
	Q»Q

�
Ó ð

�
XP@ “Urdu Center”, because the correct shap-

ing is produced automatically as the first word ends
with a non-joiner. Therefore 	Q»Q�Ó ð �XP@ and 	Q»Q�Óð �XP@
look identical. Although space character is primar-
ily used to generate correct shapes of words, it is
now being used as a word separator as well. This
two-way function of space character in Urdu makes
it an unreliable cue for word boundary which poses
challenges to the process of tokenization. In UDT
pipeline raw text is tokenized into individual tokens
using a tokenizer which uses space as word bound-
ary. The generation of erroneous tokens (single
words broken into multiple fragments) is obvious,
since, as mentioned above, space not only marks
word boundary it is also used to generate correct
shaping of a word. To ensure that only valid tokens
are processed in the further stages of the pipeline, to-
kenization is followed by human post-editing. The
fragments of a word are joined using an underscore
‘ ’. This ensures that such words retain their visually
correct shape. For example two fragments �H �Pð �Qå

�	


and Y	J �Ó of a single word Y	J �Ó �H �Pð �Qå
�	
 generated by

the tokenizer will be joined into single word with an
‘ ’ as Y	J �Ó_ �H �Pð �Qå

�	
.

5.2 Ezafe
Ezafe is an enclitic short vowel e which joins two
nouns, a noun and an adjective or an adposition and
a noun into a possessive relationship. In Urdu ezafe
is a loan construction from Persian, it originated
from an Old Iranian relative pronoun −hya, which
in Middle Iranian changed into y/i a device for nom-
inal attribution (Bögel et al., 2008). The Urdu ezafe
construction functions similarly to that of its Persian
counter part. In both the languages the ezafe con-
struction is head-initial which is different from the
typical head-final nature of these languages. As in
Persian the Urdu ezafe lacks prosodic independence,
it is attached to a word to its left which is the head
of the ezafe construction. It is pronounced as a unit
with the head and licenses a modifier to its right.
This is in contrast to the Urdu genitive construction,
which conforms to the head-final pattern typical for

Urdu. The genitive marker leans on the modifier of
the genitive construction not on the head and is pro-
nounced as a unit with it. Example (4) is a typi-
cal genitive construction in Urdu while (5) shows an
ezafe construction.

(4) ÕÎ
�
¯ A

�
¿

	á�
A
�
K


yAsIn-kA
Yasin-GEN

qalam
pen

‘Yasin’s pen.’

(5) 	àA
��
J» A

�
K�

�
I
�

Óñºk

hukummat-e
government-Ez

Pakistan
Pakistan

‘Government of Pakistan.’

The ezafe construction in Urdu can also indi-
cate relationships other than possession. In current
Urdu treebank when an ezafe construction is used
to show possessive relationship, it is annotated sim-
ilar to genitive constructions indicating possession
with an “r6” label as shown in example (6), the
head noun I. kA

�
 ‘owner’ ‘possesses’ the modi-

fying noun �Iêº
��
K ‘throne’. However, in example

(7) ezafe does not indicate a possessive meaning, in
such cases “NMOD” (noun modifier) is used instead
of “r6”, the adjective 	á �ðP ‘bright’ does not stand

in a possession relation to the 	P
�
ðP ‘day’, but simply

modifies the head noun in an attributive manner.

(6) �Iêº
��
K I.�

kA
�



sahb-e
owner-Ez

takht
throne

‘The owner of the throne.’

�
Iêº

��
K I.�

kA
�



r6

��

(7) 	á �ðP 	P
�
ðP

rooz-e
day-Ez

rooshan
bright

‘Bright day.’

160



	á
�

ðP 	P
�
ðP

nmod

��

6 Agreement Analysis

In order to ensure the reliability of manual depen-
dency annotations in UDT, we did an agreement
analysis using a data set of 5600 words annotated
by two annotators, without either annotator knowing
other’s decisions. A good agreement on the data set
will assure that the annotations in UDT are reliable.
The data set used contains 2595 head-dependent de-
pendency chains marked with dependency relations
belonging to a tag-set of 39 tags. The agreement
measured is chunk based; for each chunk in a sen-
tence agreement was measured with regard to its re-
lation with the head it modifies.

Inter-annotator agreement was measured using
Cohen’s kappa (Cohen and others, 1960) which is
the mostly used agreement coefficient for annotation
tasks with categorical data. Kappa was introduced to
the field of computational linguistics by (Carletta et
al., 1997) and since then many linguistics resources
have been evaluated using the matrix such as (Uria
et al., 2009), (Bond et al., 2008), (Yong and Foo,
1999). The kappa statistics show the agreement be-
tween the annotators and the reproducibility of their
annotated data sets. Similar results produced by the
annotators on a given data set proves the similarity
in their understanding of the annotation guidelines.
However, a good agreement does not necessarily en-
sure validity, since annotators can make similar kind
of mistakes and errors.

The kappa coefficient κ is calculated as:

κ =
Pr(a)− Pr(e)

1− Pr(e)
(8)

Pr(a) is the observed agreement among the coders,
and Pr(e) is the expected agreement, that is, Pr(e)
represents the probability that the coders agree by
chance.

Based on the interpretation matrix of kappa value
proposed by Landis and Koch (Landis and Koch,
1977) as presented in Table 1, we consider that the
agreement as presented in Table 2, between the an-
notators on the data set used for the evaluation, is
reliable. There is a substantial amount of agreement

Kappa Statistic Strength of agreement
<0.00 Poor
0.0-0.20 Slight
0.21-0.40 Fair
0.41-0.60 Moderate
0.61-0.80 Substantial
0.81-1.00 Almost perfect

Table 1: Coefficients for the agreement-rate based on
(Landis and Koch, 1977).

No. of Annotations Agreement Pr(a) Pr(e) Kappa
2595 1921 0.74 0.097 0.71

Table 2: Kappa statistics

between the annotators which implies their similar
understanding of the annotation guidelines and of
the linguistic phenomenon present in the language.

Urdu as discussed earlier is a morphologically
rich language, information concerning the arrange-
ment of words into syntactic units or cues to syn-
tactic relations, is expressed at word level through
case clitics (Mohanan, 1990). Because information
about the relations between syntactic elements is ex-
pressed at word level, the prediction of the syntac-
tic relations becomes easier for an annotator. How-
ever, as mentioned in Table 3 case markers and case
roles don not have a one to one mapping, each
case marker is distributed over a number of case
roles, this phenomenon is called as case syncretism.
Among the 6 case markers viz ÿ 	� (ergative), ñ»

(dative), ñ» (accusative), ÿ� (instrumental),

ÿ� (ablative), A
�
¿ (genitive) and ÿ×, QK� (locative)

only ÿ 	� (ergative) is unambiguous, all others are
ambiguous between different roles. This syncretism
is one of the reason for the disagreement between
the annotators. Out of 965 case marked nominals
735 are agreed upon by both the annotators and for
230 nominals both disagreed. Examples below show
syncretism in case marker ñ» ‘ko’. ñ» marks the ‘re-
cipient’, ‘theme’ and the ‘experiencer’ of the main
verbs in sentences (9), (10) and (11) respectively.

161



ÿ
	

� (ne) ñ» (ko) A
�
¿ (kA) ÿ� (se) ÿ× (mem) QK� (par)

k1 100 22 1 0 0 0
k2 0 46 1 15 0 0
k3 0 0 0 2 0 0
k4 0 17 0 19 0 0
k4a 0 2 0 0 0 0
k5 0 0 0 14 0 0
k7 0 0 1 1 60 70
k7t 0 5 2 11 6 0
k7p 0 0 0 0 19 10
r6 0 0 89 0 0 0
rh 0 0 0 5 0 0

Table 3: Agreement among the Annotators on Karaka
roles given a Case Marker.

The nominals carrying ñ» in these sentences will be
labeled in UDT as k4 ‘recipient’, k2 ‘theme’ and k4a
‘experiencer’ respectively.

(9) øX H. A
��
J» ñ»

	á�
A
�
K
 ÿ

	
� A

�
K
XA

�	
K

Nadiya-ne
Nadya-ERG

Yasin-ko
Yasin-DAT

kitab
book-NOM

di.
give-PST+PRF
‘Nadiya gave Yasin a book.’

(10) A�K
C
�
�
K. ñ»

	á�
A
�
K
 ÿ

	
� A

�
K
XA

�	
K

Nadiya-ne
Nadya-ERG

Yasin-ko
Yasin-ACC

bhulaayaa.
call-PST+PRF

‘Nadiya called Yasin.’

(11) úG
�
@ XA

�
K
 ú

	
GA

�
î
f
» ñ»

	á�
A
�
K


Yasin-ko
Yasin-Dat

kahani
story-NOM

yaad
memory

aayi.
come-PST+PRF
‘Yasin remembered the story.’

Table 5 shows the statistics of the annotation-the
number of labels used by each annotator and the

frequency of agreement and disagreement per la-
bel. Statistics in Table 4 and 5 show that a consid-
erable amount of confusion is between ‘k1’ (agent)
and ‘k2’ (theme); ‘k1’ (agent) and ‘pof’ (part of);
‘k1s’ (noun complement) and ‘pof’ (part of) and ‘k2’
(theme) and ‘pof’ (part of). Out of 110 disagree-
ments for label ‘pof’, the annotators differ 81 (74%)
times in marking a given dependency structure either
with a ‘pof’ relation or with ‘k1, ‘k1s’ or ‘k2’. Sim-
ilarly for ‘k1’ 38% disagreements are between ‘k2’
and ‘pof’ and for ‘k2’ 49% disagreements are be-
tween ‘k1’ and ‘pof’. The high number of disagree-
ments among the members of this small subset of
labels (k1, k2, k1s, pof) suggest the validity of the
disagreement that is to say that the disagreements
are not random or by chance and can be attributed
to the ambiguity or some complex phenomenon in
the language. All the disagreements involving ‘pof’
relation occur due to the complexity of identifying
the complex predicates in Urdu. The challenges in
the identification of complex predicates (Begum et
al., 2011) coupled with similar syntactic distribution
of these Karaka roles explain the differences among
the annotators for these relations. Take for example
the case of sentences (12) and (13) both X

�
YÓ ‘help’

and úG
.
A

�
g� ‘key’ have similar syntactic context, but in

(12) X
�
YÓ ‘help’ is part of the complex predicate and

has a ‘pof’ (part of complex predicate) relation with
the light verb úÍ ‘take’ while in (13) úG

.
A

�
g� ‘key’ is

the ‘theme’ of the main verb úÍ ‘take’ and will be

marked as its ‘k2’. Similarly in (14) and (15) ú¾ÒëX

‘threat’ and H. A
��
J» ‘book’ have similar context, simi-

lar to X
�
YÓ ‘help’ in (12), ú¾ÒëX ‘threat’ has a ‘pof’

relation with the verb øX ‘give’ and H. A
��
J» ‘book’ in

(15) is its ‘theme’ marked with the label ‘k2’.

(12) úÍ X
�
YÓ ÿ�

	á�
A
�
K
 ÿ

	
� A

�
K
XA

�	
K

Nadiya-ne
Nadya-ERG

Yasin-se
Yasin-ABL

madad
help

li.
take-PST+PRF
‘Nadiya took help from yasin.’

(13) úÍ úG
.
A

�
g� ÿ�

	á�
A
�
K
 ÿ

	
� A

�
K
XA

�	
K

162



Nadiya-ne
Nadya-ERG

Yasin-se
Yasin-ABL

chaabi
key-NOM

li.
take-PST+PRF
‘Nadiya took key from Yasin.’

(14) øX ú¾ÒëX ñ» 	á�
A
�
K
 ÿ

	
� A

�
K
XA

�	
K

Nadiya-ne
Nadya-ERG

Yasin-ko
Yasin-ACC

dhamki
threaten

di.
give-PST+PRF
‘Nadiya threatened Yasin.’

(15) øX H. A
��
J» ñ»

	á�
A
�
K
 ÿ

	
� A

�
K
XA

�	
K

Nadiya-ne
Nadya-ERG

Yasin-ko
Yasin-DAT

kitab
book-NOM

di.
give-PST+PRF
‘Nadiya gave Yasin a book.’

k1 k1s k2 k2s k3 k4 k4a k5 k7 k7p k7t pof
k1 0 1 5 0 1 5 1 0 2 1 0 11
k1s 2 0 2 0 0 0 0 0 0 0 0 16
k2 43 2 0 1 0 1 0 3 2 0 1 38
k2s 0 1 8 0 0 0 0 0 0 0 0 2
k3 0 0 1 0 0 0 0 0 0 0 0 0
k4 2 0 6 0 0 0 1 0 0 0 0 0

k4a 1 0 0 0 0 0 0 0 0 0 0 0
k5 0 0 1 0 1 2 0 0 0 3 0 0
k7 0 0 0 0 0 0 0 0 0 3 3 1

k7p 0 0 0 0 0 0 0 0 8 0 0 0
k7t 0 0 0 0 0 0 0 0 2 0 0 0
pof 1 9 6 1 0 0 0 0 2 0 0 0

Table 4: Confusion Matrix between the Annotators.

7 Conclusion

In this paper we have discussed an ongoing effort of
building a dependency treebank for Urdu based on
CPG framework. We discussed some of the Urdu
specific issues like Ezafe construction and word
segmentation encountered during the treebank de-
velopment. We also discussed the evaluation of de-
pendency level annotation by measuring the inter-
annotator agreement using the Kappa statistics. The

Relations Ann.1 Ann.2 Agr. Disagr.
1 ras− k4 0 1 0 1
2 ras− k1 4 6 3 4
3 ras− k2 1 3 0 4
4 pof idiom 1 0 0 1
5 r6− k1 10 8 4 10
6 r6− k2 63 50 43 27
7 rbmod 2 0 0 2
8 pof 325 271 243 110
9 rt 43 48 38 15
10 k3 11 8 6 7
11 rs 1 8 1 7
12 k2s 21 30 17 17
13 k2p 4 3 2 3
14 k1 346 320 254 158
15 rd 13 3 2 12
16 k2 249 298 179 189
17 nmod relc 27 30 13 31
18 k7 160 156 123 70
19 jjmod 23 8 8 15
20 k5 15 28 12 19
21 k4 46 50 34 28
22 nmod k2inv 2 3 2 1
23 rh 21 15 7 22
24 k4a 10 12 7 8
25 k7a 5 6 4 3
26 adv 47 45 30 32
27 nmod k1inv 0 1 0 1
28 fragof 6 7 5 3
29 k7p 46 44 29 32
30 k7t 67 71 53 32
31 nmod emph 1 2 0 3
32 k1s 62 70 41 50
33 r6 297 335 258 116
34 k1u 0 1 0 1
35 vmod 102 98 63 74
36 nmod 91 96 48 91
37 ccof 436 486 389 144
38 sent− adv 1 0 0 1
39 r6v 5 5 3 4

Table 5: Agreement and Disagreement between the An-
notators.

163



agreement as presented in this work is considered to
be reliable and substantial ensuring that the syntac-
tic annotations in the treebank are consistent and are
annotated by the annotators with a substantial clarity
of the annotation guidelines.

8 Acknowledgement

We would like to thank Anjum Parveen for annotat-
ing the data set used for the experiments of inter-
annotator agreement. The work reported in this pa-
per is supported by the NSF grant (Award Number:
CNS 0751202; CFDA Number: 47.070). Any opin-
ions, findings, and conclusions or recommendations
expressed in this material are those of the author(s)
and do not necessarily reflect the views of the Na-
tional Science Foundation.

References
R. Begum, S. Husain, A. Dhwaj, D.M. Sharma, L. Bai,

and R. Sangal. 2008. Dependency annotation scheme
for indian languages. In Proceedings of IJCNLP. Cite-
seer.

R. Begum, K. Jindal, A. Jain, S. Husain, and
D. Misra Sharma. 2011. Identification of conjunct
verbs in hindi and its effect on parsing accuracy. Com-
putational Linguistics and Intelligent Text Processing,
pages 29–40.

A. Bharati, V. Chaitanya, R. Sangal, and KV Ramakrish-
namacharyulu. 1995. Natural Language Processing:
A Paninian Perspective. Prentice-Hall of India.

A. Bharati, M. Bhatia, V. Chaitanya, and R. Sangal.
1996. Paninian grammar framework applied to en-
glish. Technical report, Technical Report TRCS-96-
238, CSE, IIT Kanpur.

A. Bharati, R. Sangal, and D.M. Sharma. 2007. Ssf:
Shakti standard format guide. Technical report, Tech-
nical report, IIIT Hyderabad.

A. Bharati, D.M. Sharma, S. Husain, L. Bai, R. Begum,
and R. Sangal. 2009. Anncorra: Treebanks for in-
dian languages guidelines for annotating hindi tree-
bank (version–2.0).

D.N.S. Bhat. 1991. Grammatical relations: the evidence
against their necessity and universality. Psychology
Press.

R. Bhatt, B. Narasimhan, M. Palmer, O. Rambow, D.M.
Sharma, and F. Xia. 2009. A multi-representational
and multi-layered treebank for hindi/urdu. In Pro-
ceedings of the Third Linguistic Annotation Workshop,
pages 186–189. Association for Computational Lin-
guistics.

T. Bögel, M. Butt, and S. Sulger. 2008. Urdu ezafe
and the morphology-syntax interface. Proceedings of
LFG08.

F. Bond, S. Fujita, and T. Tanaka. 2008. The hinoki syn-
tactic and semantic treebank of japanese. Language
Resources and Evaluation, 42(2):243–251.

C. Bosco and V. Lombardo. 2004. Dependency and re-
lational structure in treebank annotation. In Proceed-
ings of Workshop on Recent Advances in Dependency
Grammar at COLING’04.

S. Brants, S. Dipper, S. Hansen, W. Lezius, and G. Smith.
2002. The tiger treebank. In Proceedings of the work-
shop on treebanks and linguistic theories, pages 24–
41.

J. Carletta, S. Isard, G. Doherty-Sneddon, A. Isard, J.C.
Kowtko, and A.H. Anderson. 1997. The reliability of
a dialogue structure coding scheme. Computational
linguistics, 23(1):13–31.

H. Chaudhry and D.M. Sharma. 2011. Annotation and
issues in building an english dependency treebank.

J. Cohen et al. 1960. A coefficient of agreement for nom-
inal scales. Educational and psychological measure-
ment, 20(1):37–46.

A. Culotta and J. Sorensen. 2004. Dependency tree
kernels for relation extraction. In Proceedings of the
42nd Annual Meeting on Association for Computa-
tional Linguistics, page 423. Association for Compu-
tational Linguistics.

N. Durrani and S. Hussain. 2010. Urdu word segmen-
tation. In Human Language Technologies: The 2010
Annual Conference of the North American Chapter of
the Association for Computational Linguistics, pages
528–536. Association for Computational Linguistics.

E. Hajicová. 1998. Prague dependency treebank: From
analytic to tectogrammatical annotation. Proceedings
of TSD98, pages 45–50.

J. Hajič. 1998. Building a syntactically annotated cor-
pus: The prague dependency treebank. Issues of va-
lency and meaning, pages 106–132.

E. Hajičová, A. Abeillé, J. Hajič, J. Mı́rovský, and
Z. Urešová. 2010. Treebank annotation. In Nitin In-
durkhya and Fred J. Damerau, editors, Handbook of
Natural Language Processing, Second Edition. CRC
Press, Taylor and Francis Group, Boca Raton, FL.
ISBN 978-1420085921.

J.R. Landis and G.G. Koch. 1977. The measurement of
observer agreement for categorical data. Biometrics,
pages 159–174.

G.S. Lehal. 2010. A word segmentation system for han-
dling space omission problem in urdu script. In 23rd
International Conference on Computational Linguis-
tics, page 43.

164



M.P. Marcus, M.A. Marcinkiewicz, and B. Santorini.
1993. Building a large annotated corpus of en-
glish: The penn treebank. Computational linguistics,
19(2):313–330.

C.P. Masica. 1993. The Indo-Aryan Languages. Cam-
bridge Univ Pr, May.

T.W. Mohanan. 1990. Arguments in Hindi. Ph.D. thesis,
Stanford University.

K. Oflazer, B. Say, D.Z. Hakkani-Tür, and G. Tür. 2003.
Building a turkish treebank. Abeillé (Abeillé, 2003),
pages 261–277.

M. Palmer, R. Bhatt, B. Narasimhan, O. Rambow, D.M.
Sharma, and F. Xia. 2009. Hindi syntax: Annotating
dependency, lexical predicate-argument structure, and
phrase structure. In The 7th International Conference
on Natural Language Processing, pages 14–17.

O. Rambow, C. Creswell, R. Szekely, H. Taber, and
M. Walker. 2002. A dependency treebank for english.
In Proceedings of LREC, volume 2.

F. Reichartz, H. Korte, and G. Paass. 2009. Dependency
tree kernels for relation extraction from natural lan-
guage text. Machine Learning and Knowledge Dis-
covery in Databases, pages 270–285.

S.M. Shieber. 1985. Evidence against the context-
freeness of natural language. Linguistics and Philoso-
phy, 8(3):333–343.

L. Uria, A. Estarrona, I. Aldezabal, M. Aranzabe, A. Dı́az
de Ilarraza, and M. Iruskieta. 2009. Evaluation of the
syntactic annotation in epec, the reference corpus for
the processing of basque. Computational Linguistics
and Intelligent Text Processing, pages 72–85.

A. Vaidya, S. Husain, P. Mannem, and D. Sharma. 2009.
A karaka based annotation scheme for english. Com-
putational Linguistics and Intelligent Text Processing,
pages 41–52.

L. Van der Beek, G. Bouma, R. Malouf, and G. Van No-
ord. 2002. The alpino dependency treebank. Lan-
guage and Computers, 45(1):8–22.

C. Yong and S.K. Foo. 1999. A case study on inter-
annotator agreement for word sense disambiguation.

165


