










































CuteForce - Deep Deterministic HPSG Parsing


Proceedings of the 12th International Conference on Parsing Technologies, pages 186–197,
October 5-7, 2011, Dublin City University. c© 2011 Association for Computational Linguistics

CuteForce – Deep Deterministic HPSG Parsing

Gisle Ytrestøl
Department of Informatics / University of Oslo

gisley@ifi.uio.no

Abstract

We present a deterministic HPSG parser ca-
pable of processing text incrementally with
very fast parsing times. Our system demon-
strates an efficient data-driven approach that
achieves a high level of precision. Through
a series of experiments in different config-
urations, we evaluate our system and com-
pare it to current state-of-the-art within the
field, and show that high quality determin-
istic parsing is realistic even for a ‘deep’
unification-based precision grammar.

1 Motivation

The traditional chart parsing paradigm for text
parsing has been to process one sentence at a time,
and through some type of dynamic programming
algorithm derive the most probable analysis. For
a formalism like HPSG, even short sentences can
license hundreds of valid (i.e. grammatical) analy-
ses, and at some point a statistical model has to
be employed to reduce and pin down an n-best
candidate list. In order to circumvent a costly ex-
traction of the full parse forest, deterministic in-
cremental parsing has enjoyed an increased inter-
est in the research community. Deterministic pars-
ing can mitigate both the time and space complex-
ity challenges often associated with probabilistic
chart parsing methods. The deterministic incre-
mental approach is attractive both from a compu-
tational and a cognitive standpoint. Whereas tra-
ditional chart parsing approaches require the entire
sentence as input before producing an analysis, an
incremental algorithm will incrementally expand a
syntactic/semantic derivation as it reads the input
sentence one word/token at a time. There are sev-
eral attractive features to this approach. The time-
complexity will be linear when the algorithm is
deterministic, i.e. when it does not allow for later
changes to the partial derivation. For a number of
applications, e.g. speech recognition, the ability to

process input on the fly per word, and not per sen-
tence, can also be vital.

This paper introduces a ‘deep’ incremental de-
terministic HPSG parser, dubbed CuteForce. It
has an optional unification mode, ensuring that all
parse derivations are valid HPSG analyses. Uni-
fication is a costly operation, and for certain ap-
plications it may be a desirable trade-off to gain a
significant speed-up by replacing unification with
less expensive constraint checking.

2 Related Work

While there is a rich research tradition in statistical
parsing, the predominant approach derives from
chart parsing and is inherently non-deterministic.
In the following, we review alternative approaches
to statistical parsing, with a focus on deterministic
and incremental processing.

2.1 Deterministic Parsing

Deterministic parsing aims to derive one single
analysis provided the input string and the gram-
mar. As almost any medium to long sentence car-
ries substantial inherent ambiguity given even a
moderately simple grammar, this would in prac-
tice mean to disambiguate on the fly by making a
sequence of local choices that continuously pursue
the globally optimal derivation.

Dependency Grammar Kudo and Matsumoto
(2002) introduced an efficient deterministic de-
pendency parser for Japanese. Their parser out-
performed previous probabilistic models with re-
spect to accuracy and efficiency. Yamada and Mat-
sumoto (2003) applied a similar method for En-
glish, and received a near state of the art accuracy
compared to other dependency grammar parsers.

The method was further investigated with Malt-
Parser (Nivre et al., 2007; Nivre and Scholz,
2004). MaltParser is a platform for data-driven
dependency parsing, which can induce a parsing

186



model from treebank data and parse new input us-
ing a pre-compiled parsing model. The parser is
formalized as a transition-based instantiation of a
shift-reduce parser. It has been applied to a range
of different languages, and Nivre et al. (2007) re-
port that dependency accuracy consistently ranges
from 80 to 90 %, “usually with less than a 5%
increase in error rate compared to state-of-the art
parsers for the language in question.”

CFG Sagae and Lavie (2005) present a de-
terministic CFG parser grounded in the same
classifier-based strategy that was pursued by Malt-
Parser. Given that the search-space of constituent
trees is larger than for dependency trees, the poten-
tial efficiency gain of deterministic parsing could
be higher than for dependency parsing, but the
margin of error would also be larger, given that
a deterministic CFG parse is likely to reach more
decision points, with more alternative paths than
the same sentence would have encountered in de-
pendency parsing. Although they do not reach the
same precision/accuracy as ‘classic’ state-of-the-
art parsers (e.g. Collins (1997), Charniak (2000),
Charniak and Johnson (2005)), “the simplicity and
efficiency of deterministic parsers make them at-
tractive in a number of situations requiring fast,
light-weight parsing, or parsing of large amounts
of data.” (Sagae and Lavie, 2005)

HPSG The increased complexity of unification-
based grammars like HPSG could potentially li-
cense a substantial increase in efficiency by pars-
ing deterministically, but the inherent hard con-
straints of unification-based grammar could cause
a high number of parse failures. Parallel to the
development of the CuteForce parser, Ninomiya
et al. (2010; 2009) provide a deterministic shift-
reduce parser for HPSG where this issue is ad-
dressed. To mitigate the problem of parse failures,
they suggest default unification (Ninomiya et al.,
2002; Copestake, 1993; Carpenter, 1993) by over-
writing inconsistent constraints in the grammar,
outlining a deterministic and a non-deterministic
configuration.

Ninomiya et al. (2010) evaluate their parser
on Enju2.3ß, an HPSG for English, which is ex-
tracted from Section 02-21 of the Penn Treebank
(Miyao and Tsujii, 2005; Miyao et al., 2004).
Their deterministic parser using default unification
reaches a coverage of 95%. They further conclude
that “[t]he experiments revealed that determinis-

tic parsing with default unification achieved a high
degree of accuracy of 87.6 per cent for the Penn
Treebank with gold part-of-speech (POS) tags.”
(Ninomiya et al., 2010)

Further, Matsuzaki et al. (2007) provide a fast,
partially deterministic shift-reduce HPSG parser.
The parser requires a preceding non-deterministic
supertagging and CFG-filtering stage prior to the
unification-based parsing which is done through
a deterministic shift-reduce algorithm, and gives
“comparable accuracy with a speed-up by a factor
of six (30 msec/sentence) compared with the best
published result using the same grammar.” (Mat-
suzaki et al., 2007)

2.2 Traditional Unification-Based Parsing

Whereas the parser proposed by Ninomiya et al.
(2010; 2009) is to our knowledge the only other
effort to deterministically parse HPSG, traditional
chart parsing approaches for unification-based
grammars have been presented in a number of re-
search efforts (e.g. Riezler et al. (2000), Kaplan et
al. (2004), Malouf and van Noord (2004), Miyao
and Tsujii (2005), Toutanova et al. (2005)). Since
unification is a destructive, hence non-reversible
operation, a non-deterministic parsing algorithm
will need to preserve the original feature struc-
ture in order to keep the entire parse forest intact.
Keeping the parse forest intact is crucial when
constructing an n-best list over the most probable
HPSG analyses that is licensed by the grammar,
as the individual parse tree will contain diverg-
ing feature structures. This has been addressed
in several research papers, as this could poten-
tially be a major bottleneck for non-deterministic
HPSG parsing (Callmeier, 2000; Oepen and Car-
roll, 2000; Zhang et al., 2007).

PET HPSG Parser The PET platform was
developed as a tool for experimentation with
HPSG processing and implementation techniques
(Callmeier, 2000). It has further been developed
through subsumption-based packing (Oepen and
Carroll, 2000), selective unpacking and statistical
parse ranking (Zhang et al., 2007). It is main-
tained by DELPH-IN1, which is a cooperation be-
tween academic institutions and researchers work-
ing with deep linguistic processing in HPSG.

The PET HPSG Parser will for each input sen-
tence use a hand-crafted grammar (e.g. LinGO
ERG) to retrieve the candidate analyses that are

1http://www.delph-in.net/

187



grounded in the grammar, thus being well-formed.
During parsing it attempts to create the full packed
parse forest for each input sentence, and produces
an n-best list based on statistical parse ranking.
Parsing will fail if the full parse forest cannot be
constructed within a preset time limit (normally 60
seconds).

3 LinGO ERG and Redwoods Treebank

There exists a number of different HPSG gram-
mars used for automatic parsing. The parser pre-
sented in this paper uses LinGO (Linguistic Gram-
mars Online) ERG (English Resource Grammar),
which is a broad-coverage, linguistically pre-
cise handcrafted grammar for English (Flickinger,
2000). It is continuously being developed and ex-
panded to increase coverage for new domains.

The Redwoods treebank is a collection of an-
notated corpora from a variety of domains, in-
cluding tourism guides, transcribed scheduling di-
alogs, ecommerce emails and Wikipedia articles
(Oepen et al., 2002). Each analysis in the treebank
is manually disambiguated and assigned an HPSG
analysis in accordance with ERG. Its use in this
project is recorded in Table 1. HPSG signs corre-
sponding to lexical types are developed manually
and assigned in accordance with the lexical cover-
age of the grammar. Each derivation within Red-
woods is grounded in ERG, ensuring that the anal-
yses from each individual treebank are kept con-
sistent with the overall grammar.

WikiWoods WikiWoods is a collection of au-
tomatically annotated Wikipedia articles, in to-
tal 1.3 million articles and 55 million utterances.
WeScience (ws) can be seen as a manually an-
notated subset of WikiWoods (Flickinger et al.,
2010; Ytrestøl et al., 2009). The WikiWoods cor-
pus is parsed with the PET HPSG parser using the
1010 release of ERG, and will contain a propor-
tion of incorrect analyses. In a manual inspec-
tion of 1,000 random utterances, roughly 82 % of
the analyses were deemed correct or nearly correct
(Flickinger et al., 2010).

In this paper we augment the training data with
derivations from WikiWoods, using the deriva-
tions from Section 2000 and onwards. As demon-
strated in this paper, both the supertagger as well
as CuteForce benefits greatly from the augmented
training data.

3.1 Statistics and Example

Table 1 provides an overview of the Redwoods
treebanks which are used in this paper.

Name #Sent ∼Length Usage
ws1-11 7636 14.4 train
ws12 601 15.6 dev
ws13 785 14.1 test
logon 8501 13.3 train
vm 11116 6.8 train
sc 2564 15.1 train

Table 1: The corpora in Redwoods treebank used for
training and testing in this paper.
#Sent refers to the number of sentences. ∼Length
is the average number of tokens per sentence for the
treebank.
ws – WeScience, Wikipedia articles
logon – Tourism guides
vm – Verbmobil corpus, transcribed dialogs
sc – SemCor, subset of English Brown Corpus

Figure 1 presents an HPSG derivation for a
simple sentence. This tree of ERG rules can be
presented as a feature structure, and a semantic
MRS representation can be derived directly from
its structure.

Lexical Rules and Lexical Types The deriva-
tion in Figure 1 is composed of lexical and syn-
tactic rules. Further, each token is assigned a lexi-
cal type, which is conceptually equivalent to a su-
pertag (see Section 4). From the leaves in Figure
1 we see that punctuation is considered a part of
the token, e.g. for RSS-. The lexical types end
with the le suffix. The lexical type for “RSS-”,
n - pn le, provides the HPSG sign template for a
proper noun. The conceptual sign is further aug-
mented by the lexical ERG rules, which end with
*lr. The hyphen in “RSS-” is hence annotated with
w hyphen plr.

For “specialized” the lexical type v np* le de-
notes the lexical category verb (v), with an op-
tional noun phrase subcategorization argument.
v pas odlr denotes a passive verb, and v j-nb-pas-
tr dlr derives an attributive adjective from a tran-
sitive passive verb. Altogether there are 50 lexical
rules in the ERG, and around 1,000 lexical types.

Syntactic Rules Syntactic ERG rules have a c
suffix. The rules can be unary or binary. The root
node in Figure 1, sb-hd mc c, denotes the con-
ventional head-subject main clause in HPSG, in

188



sb-hd mc c

sp-hd n c

d - prt-div le

“some”

aj-hdn norm c

v j-nb-pas-tr dlr

v pas odlr

v np* le

“specialized”

n ms ilr

n - m le

“software”

hd-cmp u c

v vp mdl-p le

“can”

hd-cmp u c

v n3s-bse ilr

v np* le

“narrate”

hdn bnp c

np-hdn cpd c

hdn bnp-pn c

w hyphen plr

n - pl le

“RSS-”

w period plr

n pl olr

n - mc le

“feeds.”

Figure 1: HPSG derivation from WeScience.

turn connecting a head+specifier (sp-hd n c) and
a head+complement (hd-cmp u c) phrase in a bi-
nary production. There are in total 145 binary and
55 unary syntactic rules in the ERG.

4 Preprocessing and Supertagging

CuteForce assumes an input buffer consisting of
the tokenized words, part-of-speech tags and lex-
ical types. For tokenization we use the gold stan-
dard ERG tokenization provided by the treebank.
HPSG lexical types are supertags providing a lex-
ical template for the input token (Bangalore and
Joshi, 1999). Currently, there are 1186 lexical
types in the grammar, and these templates can be
seen as an extension to the token, enriching the
word with HPSG lexical information, correspond-
ing to the HPSG sense of a word sign.2 Dri-
dan (2009) showed that supertagging contributes
to both speed-up and increased coverage for the
PET HPSG Parser. Although it would be feasible
to integrate supertagging into the parsing oracle,
assigning supertags incrementally during parsing,
we have opted for a pre-processing stage for per-
formance reasons, a design choice which is paral-
lel to MaltParser.

Dridan (2009) investigated supertagging of lex-
ical types within ERG. The thesis concludes that
the TnT tagger (Brants, 2000) performs better
than C&C Tagger (Clark and Curran, 2007) when

2892 different lexical types are found in the training data.

trained on the limited training data that was avail-
able in 2009. However, the learning curve for
the TnT tagger seemed to flatten out at around
150,000 tokens, whereas the learning curve for
C&C was still rising.

We trained the C&C tagger on the Redwoods
treebank, and augmented it with sentences from
the WikiWoods corpus (see Table 1). The find-
ings in Dridan (2009) suggest that non-gold stan-
dard data may improve the C&C tagger. Addi-
tionally we developed a customized feature model
trained with SV Mhmm, which is an implemen-
tation of structural SVMs for sequence tagging
which learns a hidden Markov model (Joachims et
al., 2009). Whereas the C&C tagger is optimized
for supertagging, and has a relatively fixed config-
uration, SV Mhmm requires that the user designs
features specifically aimed for the classification
task. This allows the user to address certain un-
conventional aspects of ERG supertagging, most
especially the tokenization that includes punctua-
tion. On smaller sets of training data, SV Mhmm

outperformed C&C. However, C&C trains much
faster, and is capable of training on much larger
amounts of data than SV Mhmm.

Results The best single tag accuracy in Dridan
(2009) refers to Section 2 of WeScience, since
this was released prior to the final release of We-
Science. In Table 2 we present the lexical type ac-
curacies (lt acc.) on WeScience Section 13 for the

189



best configurations for SV Mhmm and the C&C
tagger, along with the best single lexical type accu-
racy for WeScience 2 reported in Dridan (2009). #
sent refers to the number of sentences in the train-
ing data, and cpu hours is the number of hours it
took to train the model. Although we were able to
train even larger models for the C&C tagger, the
accuracy seemed to flatten out at approximately
6,800,000 tokens on our development set, so we
have applied this model. For the C&C tagger, we
used the default parameter settings.

TnT 3 C&C SV Mhmm

lt acc. 0.864 0.953 0.934
# sent ≈10,000 6,800,000 300,000
cpu hours <0.1 314 480

Table 2: Single tag accuracy on WeScience Section 2
(TnT) and Section 13 (C&C and SV Mhmm).

Consistent with the assumption in Dridan
(2009), augmenting the training data contributes
substantially to the performance of the tagger.
C&C and SV Mhmm (Table 2) are trained on
6,800,000 and 300,000 sentences respectively, ap-
proximately 94 million/4 million tokens. C&C has
a comparatively low accuracy when the amount of
training data is limited. We assume the punctu-
ation regime in ERG works is C&C’s disadvan-
tage when the training data is limited, since it will
result in a large lexicon with comparatively low
frequency per lexical entry. When increasing the
training data, this sparsity problem will have less
impact – it is even plausible that this contributes to
the overall performance of C&C when the amount
of training data is high, because the punctuation
provides enriched information about the token.

Part-of-speech tags are assigned prior to su-
pertagging. POS tags are not part of the final
HPSG derivation, but are used both by the pars-
ing oracle in CuteForce and by the supertagger.
The POS tags used in training and test data are
tagged using the TnT POS tagger, applying the
pre-compiled English WSJ model bundled with
the tagger.

3From the best single tag accuracy reported in Dridan
(2009) on Section 2 of ws. Training time is not stated in Dri-
dan (2009), but in TnT it should only be a matter of minutes
for such a small data set. This model is trained on 1,941 sen-
tences from WeScience, and additional data from the Red-
woods treebank, approximately 10,000 sentences, and We-
Science should hence be considered (mostly) out-of-domain
for this model, compared to the models used in the training
of the C&C and SV Mhmm tagger.

5 CuteForce – Deterministic Incremental
HPSG Parsing

Our parser, CuteForce, employs a classifier-based
oracle to guide the shift-reduce parser that in-
crementally builds a syntactic/semantic HPSG
derivation that conforms to the LinGO English Re-
source Grammar (ERG).

Parser Layout The sentence input buffer β is a
list of tuples with token, part-of-speech tags and
HPSG lexical types (supertags).

Given a set of ERG rules R and a sentence
buffer β, a parser configuration is a tuple c =
(α, β, ι, π) where:

• α is a stack of “active” edges4

• β is a list of tuples of word forms
W , part of speech tags POS and lexical
types LT derived from a sentence x =
((W1, POS1, LT1), ...(Wn, POSn, LTn)).

• ι is the current input position in β

• π is a stack of passive edges instantiating an
ERG rule

The stack of passive edges π makes up the
full HPSG representation of the input string if the
string is accepted.

Transition System The shift-reduce parser has
four different transitions, two of which are param-
eterized with a unary or binary ERG rule, which
are added to the passive edges, hence building the
HPSG structure. The four transitions are:

• ACTIVE – (adds an active edge to stack α,
and increments ι)

• UNIT(R1) – (adds unary passive edge to π
instantiating unary ERG rule (R1))

• PASSIVE(R2) – (pops α and adds binary
passive edge to π instantiating binary ERG
rule (R2))

• ACCEPT – (terminates the parse of the sen-
tence. π represents the HPSG derivation of
the sentence)

4An “active” edges in our sense is a hypothesis of an ap-
plication of a binary rule where the left daughter is known (an
element of π), and the specific binary ERG rule and the right
daughter is yet to be found.

190



Parsing Configuration Mode The parser can
operate in three oracle configurations: HPSG Uni-
fication mode, CFG approximation mode and un-
restricted mode.

In HPSG Unification mode, the parser validates
that each transition implies a valid unification.
This is done through an XML-RPC interface to
LKB (Copestake, 2002). We expect that a substan-
tial speedup could be obtained if this implementa-
tion was done natively in CuteForce5. All UNIT
and PASSIVE transitions are implicit unifications.
For each parsing stage, the parsing oracle returns a
ranked list of transitions. The highest-ranked tran-
sition not violating a unification constraint will be
executed. If no transitions yield a valid unifica-
tion, parsing fails for the given sentence. All non-
failing parses (i.e. parses that terminates with the
ACCEPT transition) are ensured to produce a valid
HPSG derivation.

In CFG mode, a naive CFG approximation
of the ERG is employed to guide the oracle.
The CFG approximation consists of CFG rules
harvested from the parser’s training data, aug-
mented with derivations from WikiWoods, in total
300,000 sentences. Each ERG rule instantiation,
using the identifiers shown in Figure 1 as non-
terminal symbols, will be treated as a CFG rule,
and each parser action will be validated against the
set of CFG rules. If the parser action yields a CFG
projection not found among the valid CFG rules in
the CFG approximation, the CFG filter will block
this transition. If the parser arrives at a state where
the CFG filter blocks all further transitions, pars-
ing fails.

In unrestricted mode, the oracle chooses the
highest scoring transition without any further re-
strictions imposed. In this setting, the parser typ-
ically reaches close to 100 % coverage – the only
sentences not covered in this setting are instances
where the parser enters an infinite unit production
loop, and the sentence is dismissed.6

5Specifically, the current unification back-end performs
non-destructive unification, i.e. it does not take advantage of
the deterministic nature of CuteForce.

6One would require additional heuristics to avoid unary
loops in an incremental parsing scheme that allows for such
productions. In Sagae and Lavie (2005) they force a non-
unary production after three consecutive unary transitions in
order to break a potential loop.

5.1 Machine Learning Model

A discriminative machine learning model is used
to predict the parser action. The model is trained
and tested on the WeScience Treebank, a branch
of the hand-annotated LinGO Redwoods treebank
(Ytrestøl et al., 2009; Oepen et al., 2002). Section
1-11 is used for training, Section 12 is used in de-
velopment and Section 13 is held-out for testing.
The training data is further augmented with addi-
tional sentences from other Redwoods treebanks
(see Table 1), and derivations from the automati-
cally annotated Wikiwoods corpus. Parsing results
for CuteForce initially improve when the size of
the training data increases, but the full extent of
the effect of training on partially incorrect training
data is not yet clear. Section 3 provides a more
in-depth presentation of the training data used in
this paper. The parsing is reduced to a classifica-
tion problem. Each HPSG analysis is a derivation
from one unique sequence of parser actions T =
t1, t2 . . . tn The input histories are feature vectors
representing a parser state, and the output class is
a transition t. This can be seen as a deterministic
implementation of a History-based model, intro-
duced by Black et al. (1993). Formally, a deriva-
tion D is a sequence of the highest scoring transi-
tions T :

arg maxti P (ti | Φ(t1, .., ti−1))

The function Φ maps the current state, or his-
tory, to a feature vector. The vector is further de-
fined through a set of feature functions, see Sec-
tion 5.2.

For training we use LibLinear (Fan et al., 2008),
which provides a number of solvers. In CuteForce,
we have used the implementation of SVM multi-
class classification of Crammer and Singer’s for-
mula (Vapnik, 1995; Keerthi et al., 2008; Cram-
mer and Singer, 2002), which has given better re-
sults than other learners evaluated during the de-
velopment.

5.2 Feature Model

CuteForce is equipped with a rich feature model
optimized for a large (100,000+) set of training
derivations. In our training data of 150,000 train-
ing derivations, we have approximately 6 million
training instances, where each instance represents
a parser action and is mapped to a feature vector.
We distinguish between static and dynamic fea-
tures, where the static features are defined prior

191



to parsing and only depend on the properties of
the input buffer, whereas the dynamic features are
defined by the HPSG derivation that is partially
built during parsing. Part-of-speech tags and ERG
lexical types (supertags) are annotated in a prepro-
cessing stage.

For the history-based model, we expand the for-
mal parser definition in Section 5 with a position
index j for α and π, where 0 denotes the top of the
stack, -1 denotes the next stack element etc.

Feature Functions We define a set of feature
functions to describe the features used in the fea-
ture vector:

• β(ι) is the ιth W /POS/LT tuple in the input
buffer, where ι denotes the current buffer po-
sition.

• BL is the length of buffer β

• W(ι) is the ιth word form in β.

• LT(ι) is the ιth lexical type in β.

• POS(ι) is the ιth Part-of-Speech tag in β, an-
notated by the preprocessor (TNT).

• LC(ι) is the lexical category tag derived from
LT(ι).

• SubCat(ι) is the Subcategorization field de-
rived from LT(ι).

• IP(ι) and FP(ι) denote word-initial and
word-final punctuation in the word W(ι), re-
spectively.

• α(j) is the jth unification candidate edge
from the top of the active edge stack.

• π(j) is the jth edge from the top of the passive
edge stack.

• l(e) is the left-branched daughter of the edge
e.

• r(e) is the right-branched daughter of the
(passive) edge e.

• h(e) is the HPSG head of the edge e. h
∗

(e)
denotes the head-relation down to the pre-
terminal.

• ER(e) is the ERG rule of the (passive) edge
e.

• S(α/π) denotes the size of the α and π stacks.

Feature Templates Each training instance maps
70 features to the feature vector. In this paper we
limit ourselves to presenting the feature functions
corresponding to the atomic features extracted for
each parsing state (see Table 3). In the feature vec-
tor, most of the atomic features occur in conjunc-
tion with other features, and only a few of the fea-
tures will occur by themselves. A combination of
two or more features is necessary to represent the
inherent dependence many features have on one
another, given that we train the model linearly. For
our model derived from 150,000 sentences, we ex-
tracted approximately 6 million features, using a
frequency cutoff of 3.

SF W(ι−1,ι,ι+1),POS(ι−1,ι,ι+1,ι+2)
LC(ι−1,ι,ι+1,ι+2),ι, BL− ι
LT(ι−1,ι,ι+1,ι+2),FP(ι),IP(ι)
SubCat(ι,ι+1,ι+2)

DF S(α),ER(π(0)),ER(h(π(0))),ER(h(h(π(0))))

ER(h∗
(π(0))

),ER(h(l(α(0))))
,ER(h(h(l(α(0)))

))

ER(h∗
(l(α(0))

)
),ER(l(π(0))),ER(r(π(0)))

ER(l(l(α(0)))
),ER(r(l(α(0))))

,ER(l(α(−1)))

Table 3: Static and dynamic features.

5.3 Training Data

Given the available treebanks and corpora we have
at our disposal (see Section 3), we evaluated a
number of different training data configurations.
In addition to corpora from the Redwoods tree-
bank, we can extend the training model with Wiki-
Woods data.

In Figure 2 we observe that the accuracy of
CuteForce improves when extending the number
of training derivations. Figure 2 presents results
for parsing without CFG or Unification filtering
using gold standard lexical types.

Training data from the WeScience treebank
amounts to 7,636 sentences. When training on the
same amount of sentences from other (out of do-
main) treebanks from Redwoods, we see a clear
drop in precision – both the in-domain WeScience
treebank, and maybe more surprisingly, the an-
notated WikiWoods corpus is a better source for
training than the out-of-domain, yet gold standard,
Redwoods data.

Training solely on WikiWoods annotation
yields lower performance when the amount of
training data is limited, but from 30,000 sen-
tences there are only minor differences in the per-

192



 0.15

 0.2

 0.25

 0.3

 0.35

 0.4

 0.45

 0.5

 0  20000  40000  60000  80000  100000  120000  140000  160000  180000  200000

E
xa

ct
 M

at
ch

 P
ro

po
rt

io
n

Training Derivations (Sentences)

CuteForce (Exact Match Score) on ws12

Redwoods Treebank + Wikiwoods
WeScience Treebank + Wikiwoods

 Wikiwoods

Figure 2: The exact match proportion (proportion of sentences with an HPSG analysis identical to the gold stan-
dard) increases when adding more training sentences. The WeScience Treebank augumented with WikiWoods
derivations, totally 150,000 sentences, reaches the highest accuracy.

formance for the two in-domain configurations.
There are at least two trends in Figure 2 that
seem obvious: First, in-domain data is a better
training source than out-of-domain data – even
when the training data contains a proportion of er-
rors. Second, when the amount of training data is
very limited, training on the WeScience treebank
yields the best results. However, when we ex-
tend the amount of WikiWoods derivations in the
training data, these differences diminish, and from
100,000 training sentences onwards, there seems
to be no substantial differences between the train-
ing data configurations. At about 150,000 deriva-
tions, the curve appears to flatten out, and even
starting to decline. For the remainder of the pa-
per, we continue using the model trained on We-
Science, augmented with Wikiwoods annotation,
totally 150,000 derivations, as this configuration
achieved the highest accuracy on the development
data set.

In this iteration we have indiscriminately used
WikiWoods sentences from Section 2000 and on-
wards. It could prove beneficial to reject sentences
in the training data that are obvious outliers. This
could be very short (one word) sentences, very
long sentences, sentences partially or completely
in a foreign language7 , or sentences that have other
properties that may imply that they are unsuited to
use as training data. This will be subject for fur-

7Certain articles in the English Wikipedia contain large
amount of foreign texts, for example articles concerning a
specific language.

ther research.

6 Evaluation

CuteForce is evalutated on Section 13 from We-
Science (ws13), and the experiments were carried
out on an Intel Xenon(R) server with 2 GHz CPU.

6.1 Deterministic HPSG Parsers

To our knowledge, the parser outlined in Ni-
nomiya et al. (2010) is the only effort in incremen-
tal deterministic unification-based parsing, along
side with CuteForce. However, a complete head-
to-head comparison of the parse analyses pro-
duced by the parsers is not possible due to formal
and technical difference in the underlying gram-
mars. Whereas the HPSG Enju grammar used
by Ninomiya et al. (2010) is induced from the
Penn Treebank, the ERG is a handcrafted gram-
mar. Where the granularity of an induced gram-
mar like Enju is largely determined by informa-
tion available in the treebank, the ERG includes
more fine-grained and richer analyses, for example
with respect to subcategorization patterns (includ-
ing relational nouns and adjectives), multi-word
expressions and other subtle linguistic properties
that cannot easily be extracted from a treebank
with heuristics alone.

Even if the parser proposed by Ninomiya et al.
(2010) had been publicly available, adapting it to
ERG would not be straightforward, because the
typed feature structure logic assumed in the ERG
is formally richer than that of Enju. Alternatively,

193



one could opt for a partial evaluation of the analy-
ses generated by the two parsers, e.g. by extracting
dependency relations (Clark and Curran (2007),
Cahill et al. (2008)). This would be subject for
further investigation.

Instead of directly comparing the two determin-
istic parsers, we can however see how they relate
to traditional non-deterministic parsers. Ninomiya
et al. (2010) present an extensive evaluation of
their deterministic parser, compared to other non-
deterministic Enju HPSG parsers. In the follow-
ing, we will compare CuteForce to the PET HPSG
Parser, which is the de facto HPSG parser for the
DELPH-IN consortium.

6.2 CuteForce vs PET HPSG Parser

The PET HPSG Parser is the most widely used
parser for ERG. Its design is distinctly different
from CuteForce (see Section 2.2). Comparing
the two parsers based on available testing data
is non-trivial: Whereas Ytrestøl et al. (2009) re-
port a parsing coverage of 86 % for the sentences
annotated in WeScience, it is only the sentences
that were actually covered that appear in the We-
Science Treebank. Hence, the PET Parser will
have a 100 % coverage in all the test sentences we
are able to evaluate. To accommodate for this, we
have evaluated the parsers on test sentences where
the coverage overlaps for both parsers, hence we
have divided the results according to the parser
mode in which CuteForce is operating. Table 4
presents the parser scores on ws13 when Cute-
Force is using gold standard supertags, whereas
Table 5 presents the same score when CuteForce
is applying supertagged input.

6.3 Parsing Results

The oracle mode determines the coverage for
CuteForce, hence the subset of the test data that
is evaluated for each configuration. M in Table 4
and 5 refers to the CuteForce oracle mode, where
N in unrestricted, C is CFG approximation mode,
and U is unification mode. Ex is the proportion of
sentences that was parsed identically to the gold
standard derivation. Cov refers to the correspond-
ing coverage, only relevant for CuteForce8. F1 is
the Parseval F1-score when treating the gold stan-
dard and the parsed analysis as a CFG tree struc-

8Since the treebanks only consist of sentences that the
PET parser was able to parse, PET will have 100 % cover-
age in all configurations.

ture. TA is the lexical type (supertag) accuracy. All
tree derivation scores are computed by evalb9.

Val is the proportion of the parsed analyses that
are valid HPSG derivations, and ∼SL refers to the
average sentence length for the subset. mps is the
average number of millisecond per sentence used
by the parser (this excludes preprocessing time
done by the supertagger).

In Table 4 and 5 we see that with gold stan-
dard supertags, CuteForce is comparable, and even
more accurate than PET in certain validation ma-
trices. When parsing with supertagged input, PET
is in overall better. However, when considering the
F1-score, it seems clear that the parse derivations
produced by CuteForce have high quality. Pars-
ing time is consistently 15 msec/sentence unless
we apply unification. This is to our knowledge the
fastest parsing time reported by a parser on a ma-
jor HPSG grammar.

We see that the proportion of sentences that are
well-formed HPSG derivations are in the range
from 0.47-0.59, depending on the configuration.
Although a fairly high share of the derivations will
not yield a unifiable HPSG derivation, it is how-
ever likely that one could extract a partially well-
formed semantic structure through robust unifica-
tion (Fouvry, 2003). This is also addressed in
Zhang and Krieger (2011), and will be evaluated
in further studies.

7 Future Work

There are alternative paths that could be pursued
to attempt to improve on the current configura-
tion. Instead of using single tags, one could let the
supertagger assign multiple supertags. Zhang and
Clark (2011) present a shift-reduce parser for CCG
where the parsing oracle chooses between a set of
candidate supertags assigned by the supertagger.
Hence they are able to take syntactic information
into account when choosing the supertag. A simi-
lar approach could be beneficial for CuteForce.

Allowing for non-determinism would open for
a number of strategies. Ytrestøl (2011) evaluated
a backtracking algorithm where ranking is applied
to locate and redo an incorrect transition done by
CuteForce. Alternatively, one could consider a
beam-search strategy in line with Ninomiya et al.
(2010). This will be subject for further study.

9http://nlp.cs.nyu.edu/evalb/

194



M Cov Ex F1 TA Val ∼SL mps
CF N 0.99 0.42 0.86 1 0.51 14.1 15
PET - 1 0.48 0.87 0.965 1 14.1 2.3k
CF C 0.81 0.52 0.89 1 0.59 12.0 15
PET - 1 0.55 0,88 0.966 1 12.0 2.3k
CF U 0.57 0.77 0.94 1 1 8.6 1.6k
PET - 1 0.66 0.90 0.972 1 8.6 2.3k

Table 4: Parsing results for CuteForce (gold standard lexical types) and PET HPSG Parser on ws13, evaluated on
sentences where the coverage overlaps for both parsers.

M Cov Ex F1 TA Val ∼SL mps
CF N 0.99 0.36 0.82 0.953 0.47 14.0 15
PET - 1 0.48 0.87 0.965 1 14.0 2.3k
CF C 0.79 0.45 0.85 0.959 0.59 11.9 15
PET - 1 0.55 0.88 0.966 1 11.9 2.3k
CF U 0.52 0.70 0.92 0.974 1 8.1 1.6k
PET - 1 0.69 0.90 0.976 1 8.1 2.3k

Table 5: Parsing results for CuteForce (supertagged lexical types) and PET HPSG Parser on ws13, evaluated on
sentences where the coverage overlaps for both parsers.

8 Concluding Remarks

We have presented an efficient deterministic pars-
ing approach to HPSG. Whereas unification-based
parsing traditionally has been associated with non-
deterministic parsers, we have demonstrated a de-
terministic system capable of achieving a high
level of precision with very fast parsing times. Al-
though it is questionable if a deterministic sys-
tem could ever reach the same precision-level as
state-of-the-art non-deterministic systems, deter-
ministic parsing would be attractive for applica-
tions where it is desirable to trade some precision
for high-speed incremental parsing.

Acknowledgement

The author would like to thank the anonymous re-
viewers for their helpful suggestions, and Stephan
Oepen (University of Oslo) and Joakim Nivre (Up-
psala University) for their valued input and inspir-
ing feedback during the writing of this paper, and
in the PhD project in general. Experimentation
and engineering was made possible through access
to the TITAN high-performance computing facil-
ities at the University of Oslo (UiO), and we are
grateful to the Scientific Computation staff at UiO,
as well as to the Norwegian Metacenter for Com-
putational Science.

References

Srinivas Bangalore and Aravind K. Joshi. 1999.
Supertagging: an approach to almost parsing.

Computational Linguistics, pages 237–265.

Ezra Black, Fred Jelinek, John Lafferty, David M.
Magerman, Robert Mercer, and Salim Roukos.
1993. Towards history-based grammars: Using
richer models for probabilistic parsing. In Pro-
ceedings of the 31st Meeting of the Association
for Computational Linguistics, pages 31–37.

Thorsten Brants. 2000. TnT - a statistical part-of-
speech tagger. In Proceedings of the 6th ACL
Conference on Applied Natural Language Pro-
cessing, pages 224–231.

Aoife Cahill, Michael Burke, Ruth O’Donovan,
Stefan Riezler, Josef van Genabith, and Andy
Way. 2008. Wide-coverage deep statistical
parsing using automatic dependency structure
annotation. 34:81–124.

Ulrich Callmeier. 2000. PET — A platform for
experimentation with efficient HPSG process-
ing techniques. Natural Language Engineer-
ing, 6 (1) (Special Issue on Efficient Processing
with HPSG):99 – 108.

Bob Carpenter. 1993. Skeptical and credulous de-
fault unification with applications to templates
and inheritance. In Inheritance, defaults and
the lexicon, pages 13–37. Cambridge University
Press.

Eugene Charniak and Mark Johnson. 2005.
Coarse-to-fine n-best parsing and maxent dis-
criminative reranking. In Proceedings of the

195



43th Meeting of the Association for Computa-
tional Linguistics, pages 173–180.

Eugene Charniak. 2000. A maximum-entropy-
inspired parser. In Proceedings of the 1st Con-
ference of the North American Chapter of the
ACL, pages 132–139. Morgan Kaufmann Pub-
lishers Inc.

Stephen Clark and James Curran. 2007.
Formalism-independent parser evaluation with
ccg and depbank. In Proceedings of the 45th
Meeting of the Association for Computational
Linguistics, pages 248–255.

Michael Collins. 1997. Three generative, lexi-
calised models for statistical parsing. In Pro-
ceedings of the 35th Meeting of the Association
for Computational Linguistics and the 7th Con-
ference of the European Chapter of the ACL,
pages 16–23.

Ann Copestake. 1993. Defaults in lexical rep-
resentation. In Inheritance, defaults and the
lexicon, pages 223–245. Cambridge University
Press.

Ann Copestake. 2002. Implementing Typed Fea-
ture Structure Grammars. CSLI Publications.

Koby Crammer and Yoram Singer. 2002. On
the algorithmic implementation of multiclass
kernel-based vector machines. The Journal of
Machine Learning Research, 2:265–292.

Rebecca Dridan. 2009. Using Lexical Statistics to
Improve HPSG Parsing. Ph.D. thesis, Saarland
University.

Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh,
Xiang-Rui Wang, and Chih-Jen Lin. 2008. Li-
blinear: A library for large linear classifica-
tion. The Journal of Machine Learning Re-
search, 9:1871–1874.

Dan Flickinger, Stephan Oepen, and Gisle
Ytrestøl. 2010. Wikiwoods: Syntacto-semantic
annotation for english wikipedia. In Proceed-
ings of the 7th International Conference on
Language Resources and Evaluation. European
Language Resources Association (ELRA).

Dan Flickinger. 2000. On building a more ef-
ficient grammar by exploiting types. Natural
Language Engineering, 6 (1):15 – 28.

Frederik Fouvry. 2003. Robust Processing for
Constraint-Based Grammar Formalisms. Ph.D.
thesis, Univeristy of Essex.

Thorsten Joachims, Thomas Finley, and Chun-
Nam John Yu. 2009. Cutting-plane training of
structural SVMs. Machine Learning, 77:27–59.

Ronald M. Kaplan, Stefan Riezler, Tracy H.
King, John T. Maxwell III, and Er Vasserman.
2004. Speed and accuracy in shallow and deep
stochastic parsing. In Proceedings of the main
conference on Human Language Technology
Conference of the North American Chapter of
the Association of Computational Linguistics.

S. Sathiya Keerthi, S. Sundararajan, Kai-Wei
Chang, Cho-Jui Hsieh, and Chih-Jen Lin. 2008.
A sequential dual method for large scale multi-
class linear SVMs. In Proceeding of the
14th ACM SIGKDD international conference
on Knowledge discovery and data mining, KDD
’08, pages 408–416. ACM.

Taku Kudo and Yuji Matsumoto. 2002. Japanese
dependency analysis using cascaded chunking.
In Proceedings of the 6th conference on Natu-
ral language learning - Volume 20, COLING-
02, pages 1–7. Association for Computational
Linguistics.

Robert Malouf and Gertjan van Noord. 2004.
Wide coverage parsing with stochastic attribute
value grammars. In Proceedings of the IJCNLP
workshop Beyond Shallow Analysis.

Takuya Matsuzaki, Yusuke Miyao, and Jun’ichi
Tsujii. 2007. Efficient HPSG parsing with su-
pertagging and cfg-filtering. In Proceedings of
the 20th International Joint Conference on Ar-
tificial Intelligence, pages 1671–1676.

Yusuke Miyao and Jun’ichi Tsujii. 2005.
Probabilistic disambiguation models for wide-
coverage hpsg parsing. In Proceedings of the
43rd Annual Meeting on Association for Com-
putational Linguistics, pages 83–90. Associa-
tion for Computational Linguistics.

Yusuke Miyao, Takashi Ninomiya, and Jun ichi
Tsujii. 2004. Corpus-oriented grammar de-
velopment for acquiring a head-driven phrase
structure grammar from the penn treebank. In

196



Proceedings of the 1nd International Joint Con-
ference on Natural Language Processing, pages
684–693.

Takashi Ninomiya, Yusuke Miyao, and Jun’ichi
Tsujii. 2002. Lenient default unification for ro-
bust processing within unification based gram-
mar formalisms. In Proceedings of the 19th In-
ternational Conference on Computational Lin-
guistics, pages 1–7.

Takashi Ninomiya, Nobuyuki Shimizu, Takuya
Matsuzaki, and Hiroshi Nakagawa. 2009. De-
terministic shift-reduce parsing for unification-
based grammars by using default unification. In
Proceedings of the 12th Conference of the Eu-
ropean Chapter of the ACL, pages 603–611. As-
sociation for Computational Linguistics.

Takashi Ninomiya, Takuya Matsuzaki, Nobuyuki
Shimizu, and Hiroshi Nakagawa. 2010. De-
terministic shift-reduce parsing for unification-
based grammars. Natural Language Engineer-
ing, pages 1–35.

Joakim Nivre and Mario Scholz. 2004. Deter-
ministic dependency parsing of English text. In
Proceedings of the 20th International Confer-
ence on Computational Linguistics. Association
for Computational Linguistics.

Joakim Nivre, Johan Hall, Jens Nilsson, Atanas
Chanev, Gülsen Eryigit, Sandra Kübler, Sve-
toslav Marinov, and Erwin Marsi. 2007. Malt-
parser: A language-independent system for
data-driven dependency parsing. Natural Lan-
guage Engineering, 13(2):95–135.

Stephan Oepen and John Carroll. 2000. Ambi-
guity packing in constraint-based parsing. Prac-
tical results. In Proceedings of the 1st Confer-
ence of the North American Chapter of the ACL,
pages 162 – 169.

Stephan Oepen, Kristina Toutanova, Stuart
Shieber, Chris Manning, Dan Flickinger, and
Thorsten Brants. 2002. The LinGO Redwoods
treebank. Motivation and preliminary applica-
tions. In Proceedings of the 19th International
Conference on Computational Linguistics.

Stefan Riezler, Detlef Prescher, Jonas Kuhn, and
Mark Johnson. 2000. Lexicalized stochastic
modeling of constraint-based grammars using

log-linear measures and EM training. In Pro-
ceedings of the 38th Meeting of the Association
for Computational Linguistics, pages 480 – 487.

Kenji Sagae and Alon Lavie. 2005. A classifier-
based parser with linear run-time complexity.
In Proceedings of the 9th International Work-
shop on Parsing Technologies, pages 125–132.
Association for Computational Linguistics.

Kristina Toutanova, Christopher D. Manning, Dan
Flickinger, and Stephan Oepen. 2005. Stochas-
tic HPSG Parse Disambiguation using the Red-
woods Corpus. In Research in Language and
Computation.

Vladimir N. Vapnik. 1995. The nature of statisti-
cal learning theory. Springer.

Hiroyasu Yamada and Yuji Matsumoto. 2003.
Statistical dependency analysis with support
vector machines. In Proceedings of the 8th In-
ternational Workshop on Parsing Technologies,
pages 195–206.

Gisle Ytrestøl, Dan Flickinger, and Stephan
Oepen. 2009. Extracting and Annotating
Wikipedia Sub-Domains — Towards a New
eScience Community Resource. In Proceed-
ings of the 8th Workshop on Treebanks and Lin-
guistic Theories, pages 185 – 197.

Gisle Ytrestøl. 2011. Optimistic backtracking:
a backtracking overlay for deterministic incre-
mental parsing. In Proceedings of the ACL
2011 Student Session, HLT-SS ’11, pages 58–
63.

Yue Zhang and Stephen Clark. 2011. Shift-reduce
ccg parsing. In Proceedings of the 49th Meeting
of the Association for Computational Linguis-
tics, pages 683–692.

Yi Zhang and Hans-Ulrich Krieger. 2011. Large-
scale corpus-driven pcfg approximation of an
hpsg. In Proceedings of the 12th International
Workshop on Parsing Technologies.

Yi Zhang, Stephan Oepen, and John Carroll.
2007. Efficiency in unification-based n-best
parsing. In Proceedings of the 10th Inter-
national Workshop on Parsing Technologies,
pages 48–59. Association for Computational
Linguistics.

197


