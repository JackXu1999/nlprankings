










































Connectionist-Inspired Incremental PCFG Parsing


In: R. Levy & D. Reitter (Eds.), Proceedings of the 3rd Workshop on Cognitive Modeling and Computational Linguistics (CMCL 2012), pages 51–60,
Montréal, Canada, June 7, 2012. c©2012 Association for Computational Linguistics

Connectionist-Inspired Incremental PCFG Parsing

Marten van Schijndel

The Ohio State University

vanschm@ling.ohio-state.edu

Andy Exley

University of Minnesota

exley@cs.umn.edu

William Schuler

The Ohio State University

schuler@ling.ohio-state.edu

Abstract

Probabilistic context-free grammars (PCFGs)

are a popular cognitive model of syntax (Ju-

rafsky, 1996). These can be formulated to

be sensitive to human working memory con-

straints by application of a right-corner trans-

form (Schuler, 2009). One side-effect of the

transform is that it guarantees at most a sin-

gle expansion (push) and at most a single re-

duction (pop) during a syntactic parse. The

primary finding of this paper is that this prop-

erty of right-corner parsing can be exploited to

obtain a dramatic reduction in the number of

random variables in a probabilistic sequence

model parser. This yields a simpler structure

that more closely resembles existing simple

recurrent network models of sentence compre-

hension.

1 Introduction

There may be a benefit to using insights from human

cognitive modelling in parsing. Evidence for in-

cremental processing can be seen in garden pathing

(Bever, 1970), close shadowing (Marslen-Wilson,

1975), and eyetracking studies (Tanenhaus et al.,

1995; Allopenna et al., 1998; Altmann and Kamide,

1999), which show humans begin attempting to pro-

cess a sentence immediately upon receiving lin-

guistic input. In the cognitive science community,

this incremental interaction has often been mod-

elled using recurrent neural networks (Elman, 1991;

Mayberry and Miikkulainen, 2003), which utilize

a hidden context with a severely bounded repre-

sentational capacity (a fixed number of continuous

units or dimensions), similar to models of activation-

based memory in the prefrontal cortex (Botvinick,

2007), with the interesting possibility that the dis-

tributed behavior of neural columns (Horton and

Adams, 2005) may directly implement continuous

dimensions of recurrent hidden units. This paper

presents a refinement of a factored probabilistic se-

quence model of comprehension (Schuler, 2009) in

the direction of a recurrent neural network model

and presents some observed efficiencies due to this

refinement.

This paper will adopt an incremental probabilis-

tic context-free grammar (PCFG) parser (Schuler,

2009) that uses a right-corner variant of the left-

corner parsing strategy (Aho and Ullman, 1972)

coupled with strict memory bounds, as a model of

human-like parsing. Syntax can readily be approxi-

mated using simple PCFGs (Hale, 2001; Levy, 2008;

Demberg and Keller, 2008), which can be easily

tuned (Petrov and Klein, 2007). This paper will

show that this representation can be streamlined to

exploit the fact that a right-corner parse guarantees

at most one expansion and at most one reduction can

take place after each word is seen (see Section 2.2).

The primary finding of this paper is that this prop-

erty of right-corner parsing can be exploited to ob-

tain a dramatic reduction in the number of random

variables in a probabilistic sequence model parser

(Schuler, 2009) yielding a simpler structure that

more closely resembles connectionist models such

as TRACE (McClelland and Elman, 1986), Short-

list (Norris, 1994; Norris and McQueen, 2008), or

recurrent models (Elman, 1991; Mayberry and Mi-

ikkulainen, 2003) which posit functional units only

for cognitively-motivated entities.

The rest of this paper is structured as follows:

Section 2 gives the formal background of the right-

corner parser transform and probabilistic sequence

51



model parsing. The simplification of this model is

described in Section 3. A discussion of the interplay

between cognitive theory and computational mod-

elling in the resulting model may be found in Sec-

tion 4. Finally, Section 5 demonstrates that such

factoring also yields large benefits in the speed of

probabilistic sequence model parsing.

2 Background

2.1 Notation

Throughout this paper, PCFG rules are defined over

syntactic categories subscripted with abstract tree

addresses (cηι). These addresses describe a node’s
location as a path from a given ancestor node. A 0
on this path represents a leftward branch and a 1 a
rightward branch. Positions within a tree are repre-

sented by subscripted η and ι so that cη0 is the left
child of cη and cη1 is the right child of cη. The set of
syntactic categories in the grammar is denoted by C.
Finally, JφK denotes an indicator probability which
is 1 if φ and 0 otherwise.

2.2 Right-Corner Parsing

Parsers such as that of Schuler (2009) model hierar-

chically deferred processes in working memory us-

ing a coarse analogy to a pushdown store indexed

by an embedding depth d (to a maximum depth D).
To make efficient use of this store, a CFG G must
be transformed using a right-corner transform into

another CFG G′ with no right recursion. Given an
optionally arc-eager attachment strategy, this allows

the parser to clear completed parse constituents from

the set of incomplete constituents in working mem-

ory much earlier than with a conventional syntactic

structure. The right-corner transform operates de-

terministically over a CFG following three mapping

rules:

cη → cη0 cη1 ∈ G

cη/cη1 → cη0 ∈ G
′

(1)

cηι → cηι0 cηι1 ∈ G, cη ∈ C

cη/cηι1 → cη/cηι cηι0 ∈ G
′

(2)

cηι → xηι ∈ G, cη ∈ C

cη → cη/cηι cηι ∈ G
′

(3)

A bottom-up incremental parsing strategy com-

bined with the way the right-corner transform pulls

each subtree into a left-expanding hierarchy ensures

at most a single expansion (push) will occur at

any given observation. That is, each new observa-

tion will be the leftmost leaf of a right-expanding

subtree. Additionally, by reducing multiply right-

branching subtrees to single rightward branches, the

transform also ensures that at most a single reduc-

tion (pop) will take place at any given observation.

Schuler et al. (2010) show near complete cover-

age of the Wall Street Journal portion of the Penn

Treebank (Marcus et al., 1993) can be achieved with

a right-corner incremental parsing strategy using no

more than four incomplete contituents (deferred pro-

cesses), in line with recent estimates of human work-

ing memory capacity (Cowan, 2001).

Section 3 will show that, in addition to being de-

sirable for bounded working memory restrictions,

the single expansion/reduction guarantee reduces

the search space between words to only two decision

points — whether to expand and whether to reduce.

This allows rapid processing of each candidate parse

within a sequence modelling framework.

2.3 Model Formulation

This transform is then extended to PCFGs and inte-

grated into a sequence model parser. Training on

an annotated corpus yields the probability of any

given syntactic state executing an expansion (creat-

ing a syntactic subtree) or a reduction (completing

a syntactic subtree) to transition from every suffi-

ciently probable (in this sense active) hypothesis in

the working memory store.

The probability of the most likely sequence of

store states q̂1..D1..T can then be defined as the prod-
uct of nonterminal θQ, preterminal θP,d, and termi-
nal θX factors:

q̂1..D1..T
def
= argmax

q1..D1..T

T
∏

t=1

PθQ(q
1..D
t | q

1..D
t−1 pt−1)

· PθP,d′ (pt | b
d′

t ) · PθX (xt | pt) (4)

where all incomplete constituents qdt are factored
into active adt and awaited b

d
t components:

qdt
def
= adt /b

d
t (5)

and d′ determines the deepest non-empty incomplete
constituent of q1..Dt :

52



d′
def
= max{d | qdt 6= ‘–’} (6)

The preterminal model θP,d denotes the expecta-
tion of a subtree containing a given preterminal, ex-

pressed in terms of side- and depth-specific gram-

mar rules PθGs,d(cη → cη0 cη1) and expected counts

of left progeny categories EθG∗,d(cη
∗

→ cηι ...) (see
Appendix A):

PθP,d(cηι | cη)
def
= EθG∗,d(cη

∗

→ cηι ...)

·
∑

xηι

PθGL,d(cηι → xηι) (7)

and the terminal model θX is simply:

PθX (xη | cη)
def
=

PθG(cη → xη)
∑

xη
PθG(cη → xη)

(8)

The Schuler (2009) nonterminal model θQ is
computed from a depth-specific store element

model θQ,d and a large final state model θF,d:

PθQ(q
1..D
t | q

1..D
t−1 pt−1)

def
=

∑

f1..Dt

D
∏

d=1

PθF,d(f
d
t | f

d+1
t q

d
t−1 q

d−1
t−1 )

· PθQ,d(q
d
t | f

d+1
t f

d
t q

d
t−1 q

d−1
t ) (9)

After each time step t and depth d, θQ generates
a set of final states to generate a new incomplete

constituent qdt . These final states f
d
t are factored

into categories cfdt
and boolean variables (0 or 1)

encoding whether a reduction has taken place at

depth d and time step t. The depth-specific final state
model θF,d gives the probability of generating a final
state fdt from the preceding q

d
t and q

d−1
t which is the

probability of executing a reduction or consolidation

of those incomplete constituents:

PθF,d(f
d
t | f

d+1
t q

d
t−1 q

d−1
t−1 )

def
=

{

if fd+1t = ‘–’ : Jf
d
t = 0K

if fd+1t 6= ‘–’ : PθF,d,R(f
d
t | q

d
t−1 q

d−1
t−1 )

(10)

With these depth-specific fdt in hand, the model can
calculate the probabilities of each possible qdt for

each d and t based largely on the probability of tran-
sitions (θQ,d,T ) and expansions (θQ,d,E) from the in-
complete constituents at the previous time step:

PθQ,d(q
d
t | f

d+1
t f

d
t q

d
t−1 q

d−1
t−1 )

def
=







if fd+1t = ‘–’, f
d
t = ‘–’ : Jq

d
t = q

d
t−1K

if fd+1t 6= ‘–’, f
d
t = ‘–’ : PθQ,d,T (q

d
t | f

d+1
t f

d
t q

d
t−1 q

d−1
t )

if fd+1t 6= ‘–’, f
d
t 6= ‘–’ : PθQ,d,E (q

d
t | q

d−1
t )

(11)

This model is shown graphically in Figure 1.

The probability distributions over reductions

(θF,d,R), transitions (θQ,d,T ) and expansions
(θQ,d,E) are then defined, also in terms of side- and
depth-specific grammar rules PθGs,d(cη → cη0 cη1)
and expected counts of left progeny cate-

gories EθG∗,d(cη
∗

→ cηι ...) (see Appendix A):

PθQ,d,T (q
d
t | f

d+1
t f

d
t q

d
t−1q

d−1
t )

def
=

{

if fdt 6= ‘–’: PθQ,d,A(q
d
t | q

d−1
t f

d
t )

if fdt = ‘–’: PθQ,d,B (q
d
t | q

d
t−1f

d+1
t )

(12)

PθF,d,R(f
d
t | f

d+1
t q

d
t−1q

d−1
t−1 )

def
=

{

if c
fd+1t

6=xt : Jf
d
t = ‘–’K

if c
fd+1t

=xt : PθF,d,R(f
d
t | q

d
t−1q

d−1
t−1 )

(13)

PθQ,d,E (cηι/c
′

ηι | /cη)
def
=

EθG∗,d(cη
∗

→ cηι ...) · Jxηι = c
′

ηι = cηιK (14)

The subcomponent models are obtained by ap-

plying the transform rules to all possible trees pro-

portionately to their probabilities and marginalizing

over all constituents that are not used in the models:

• for active transitions (from Transform Rule 1):

PθQ,d,A(cηι/cηι1 | /cη cηι0)
def
=

EθG∗,d(cη
∗

→ cηι ...) · PθGL,d(cηι → cηι0 cηι1)

EθG∗,d(cη
+
→ cηι0 ...)

(15)

53



p1

x1

q11

q21

q31

q41

p2

x2

q12

q22

q32

q42

p3

x3

q13

q23

q33

q43

p4

x4

q14

q24

q34

q44

p5

x5

q15

q25

q35

q45

p6

x6

q16

q26

q36

q46

p7

x7

q17

q27

q37

q47

f12

f22

f32

f42

f13

f23

f33

f43

f14

f24

f34

f44

f15

f25

f35

f45

f16

f26

f36

f46

f17

f27

f37

f47

=
D
T

=
th
e

=
0,
D
T

=
N
P/
N
N

=
N
N

=
fu
nd

=
0,
N
P

=
S/
V
P

=
V
B

=
bo
ug
ht

=
0,
V
B =

S/
V
P

=
V
P/
N
P

=
D
T

=
tw
o

=
1,
D
T

=
S/
V
P

=
V
P/
N
N

=
JJ

=
re
gi
on
al

=
1,
JJ

=
S/
V
P

=
V
P/
N
N

=
N
N

=
ba
nk
s

=
1,
V
P =

S/
R
B

=
R
B

=
to
da
y

Figure 1: Schuler (2009) Sequence Model

• for awaited transitions (Transform Rule 2):

PθQ,d,B (cη/cηι1 | c
′

η/cηι cηι0)
def
=

Jcη = c
′

ηK ·
PθGR,d(cηι → cηι0 cηι1)

EθG∗,d(cηι
0
→ cηι0 ...)

(16)

• for reductions (from Transform Rule 3):

PθF,d,R(cηι,1 | /cη c
′

ηι/ )
def
=

Jcηι = c
′

ηιK ·
EθG∗,d(cη

0
→ cηι ...)

EθG∗,d(cη
∗

→ cηι ...)
(17)

PθF,d,R(cηι,0 | /cη c
′

ηι/ )
def
=

Jcηι = c
′

ηιK ·
EθG∗,d(cη

+
→ cηι ...)

EθG∗,d(cη
∗

→ cηι ...)
(18)

3 Simplified Model

As seen in the previous section, the right-corner

parser of Schuler (2009) makes the center embed-

ding depth explicit and each memory store element

is modelled as a combination of an active and an

awaited component. Each input can therefore either

increase (during an expansion) or decrease (during

a reduction) the store of incomplete constituents or

it can alter the active or awaited component of the

deepest incomplete constituent (the affectable ele-

ment). Alterations of the awaited component of the

affectable element can be thought of as the expan-

sion and immediate reduction of a syntactic con-

stituent. The grammar models transitions in the ac-

tive component implicitly, so these are conceptual-

ized as consisting of neither an expansion nor a re-

duction.

Removing some of the variables in this model re-

sults in one that looks much more like a neural net-

work (McClelland and Elman, 1986; Elman, 1991;

Norris, 1994; Norris and McQueen, 2008) in that

all remaining variables have cognitive correllates —

in particular, they correspond to incomplete con-

stituents in working memory—while still maintain-

ing the ability to explicitly represent phrase struc-

ture. This section will demonstrate how it is possi-

ble to exploit this to obtain a large reduction in the

number of modelled random variables.

In the Schuler (2009) sequence model, eight ran-

dom variables are used to model the hidden states

at each time step (see Figure 1). Half of these vari-

ables are joint consisting of two further (active and

awaited) constituent variables, while the other half

are merely over intermediate final states. Although

the entire store is carried from time step to time

step, only one memory element is affectable at any

one time, and this element may be reduced zero or

54



one times (using an intermediate final state), and ex-

panded zero or one times (using an incomplete con-

stituent state), yielding four possible combinations.

This means the model only actually needs one of its

intermediate final states.

The transition model θQ can therefore be simpli-
fied with terms θF,d for the probability of expand-
ing the incomplete constituent at d, and terms θA,d
and θB,d for reducing the resulting constituent
(defining the active and awaited components of

a new incomplete constituent), along with terms

for copying incomplete constituents above this af-

fectable element, and for emptying the elements be-

low it:

PθQ(q
1..D
t | q

1..D
t−1 pt−1)

def
=PθF,d′ (‘+’ | b

d′

t−1 pt−1) · PθA,d′ (‘–’ | b
d′−1
t−1 a

d′

t−1)

· Jad
′
−1

t =a
d′−1
t−1 K · PθB,d′−1(b

d′−1
t | b

d′−1
t−1 a

d′

t−1)

· Jq1..d
′
−2

t =q
1..d′−2
t−1 K · Jq

d′..D
t = ‘–’K

+PθF,d′ (‘+’ | b
d′

t−1 pt−1) · PθA,d′ (a
d′

t | b
d′−1
t−1 a

d′

t−1)

· PθB,d′ (b
d′

t | a
d′

t a
d′+1
t−1 )

· Jq1..d
′
−1

t =q
1..d′−1
t−1 K · Jq

d′+1..D
t = ‘–’K

+PθF,d′ (‘–’ | b
d′

t−1 pt−1) · PθA,d′ (‘–’ | b
d′

t−1 pt−1)

· Jad
′

t =a
d′

t−1K · PθB,d′ (b
d′

t | b
d′

t−1 pt−1)

· Jq1..d
′
−1

t =q
1..d′−1
t−1 K · Jq

d′+1..D
t = ‘–’K

+PθF,d′ (‘–’ | b
d′

t−1 pt−1) · PθA,d′ (a
d′+1
t | b

d′

t−1 pt−1)

· PθB,d′ (b
d′+1
t | a

d′+1
t pt−1)

· Jq1..d
′

t =q
1..d′

t−1 K · Jq
d′+2..D
t = ‘–’K (19)

The first element of the sum in Equation 19 com-

putes the probability of a reduction with no expan-

sion (decreasing d′). The second corresponds to the
probability of a store undergoing neither an expan-

sion nor a reduction (a transition to a new active con-

stituent at the same embedding depth). In the third

is the probability of an expansion and a reduction

(a transition among awaited constituents at the same

embedding depth). Finally, the last term yields the

probability of an expansion without a reduction (in-

creasing d′).
From Equation 19 it may be seen that the unaf-

fected store elements of each time step are main-

tained sans change as guaranteed by the single-

reduction feature of the right-corner parser. This re-

sults in a large representational economy by mak-

ing the majority of store state decisions determinis-

tic. This representational economy will later trans-

late into computational efficiencies (see section 5).

In this sense, cognitive modelling contributes to a

practical speed increase.

Since the bulk of the state remains the same,

the recognizer can access the affectable variable

and operate solely over the transition possibili-

ties from that variable to calculate the distribu-

tion over store states for the next time step to ex-

plore. Reflecting this change, the hidden states

now model a single final-state variable (f) for
results of the expansion decision, and the af-

fectable variable resulting from the reduction de-

cision (both its active (a) and awaited (b) cate-
gories), as well as the preterminal state (p) defined
in the previous section. These models are again ex-

pressed in terms of side- and depth-specific grammar

rules PθGs,d(cη → cη0 cη1) and expected counts of

left progeny categories EθG∗,d(cη
∗

→ cηι ...) (see
Appendix A).

Expansion probabilities are modelled as a binary

decision depending on whether or not the awaited

component of the affectable variable cη is likely to
expand immediately into an anticipated pretermi-

nal cηι (resulting in a non-empty final state: ‘+’) or
if intervening embeddings are necessary given the

affectable active component (yielding no final state:

‘–’):

PθF,d(f | cη cηι)
def
=



















if f= ‘+’ :
EθG∗,d

(cη
0
→cηι ...)

EθG∗,d
(cη

∗

→cηι ...)

if f= ‘–’ :
EθG∗,d

(cη
+
→cηι ...)

EθG∗,d
(cη

∗

→cηι ...)

(20)

The active component category cηι is defined as de-
pending on the category of the awaited component

above it cη and its left-hand child cηι0:

PθA,d(cηι | cη cηι0)
def
=

EθG∗,d
(cη

1
→cηι0 ...)

EθG∗,d
(cη

+
→cηι0 ...)

· Jcηι= ‘–’K

+
EθG∗,d

(cη
+
→cηι ...)·PθGL,d (cηι→cηι0 ...)

EθG∗,d
(cη

+
→cηι0 ...)

(21)

The awaited component category cη1 is defined as

55



depending on the category of its parent cη and the
preceding sibling cη0:

PθB,d(cη1 | cη cη0)
def
=

PθGR,d
(cη→cη0 cη1)

EθG∗,d
(cη

1
→cη0 ...)

(22)

Both of these make sense given the manner in which

the right-corner parser shifts dependencies to the left

down the tree in order to obtain incremental infor-

mation about upcoming constituents.

3.1 Graphical Representation

In order to be represented graphically, the working

memory store θQ is factored into a single expansion
term θF and a product of depth-specific reduction
terms θQ,d:

PθQ(q
1..D
t | q

1..D
t−1 pt−1)

def
=

∑

ft

PθF (ft | q
1..D
t−1 )

·

D
∏

d=1

PθQ,d(q
d
t | q

1..D
t−1 pt−1 ft q

d+1
t ) (23)

and the depth-specific reduction model θQ,d is fac-
tored into individual decisions over each random

variable:

PθQ,d(q
d
t | q

1..D
t−1 pt−1 ft q

d+1
t )

def
=



































































if qd+1t = ‘–’, ft 6= ‘–’, d=d
′−1 :

Jadt =a
d
t−1K · PθB,d(b

d
t | b

d
t−1 a

d+1
t−1 )

if qd+1t = ‘–’, ft 6= ‘–’, d=d
′ :

PθA,d(a
d
t | b

d−1
t−1 a

d
t−1) · PθB,d(b

d
t | a

d
t a

d
t−1)

if qd+1t = ‘–’, ft= ‘–’, d=d
′ :

Jadt =a
d
t−1K · PθB,d(b

d
t | b

d
t−1 pt−1)

if qd+1t = ‘–’, ft= ‘–’, d=d
′+1 :

PθA,d(a
d
t | b

d−1
t−1 pt−1) · PθB,d(b

d
t | a

d
t pt−1)

if qd+1t 6= ‘–’ : Jq
d
t =q

d
t−1K

otherwise : Jqdt = ‘–’K

(24)

This dependency structure is represented graphically

in Figure 2.

The first conditional in Equation 24 checks

whether the input causes a reduction but no expan-

sion (completing a subtree parse). In this case, d′ is
reduced from the previous t, and the relevant qdt−1 is
copied to qdt except the awaited constituent is altered

to reflect the completion of its preceding awaited

subtree. In the second case, the parser makes an

active transition as it completes a left subtree and

begins exploring the right subtree. The third case

is similar to the first except it transitions between

two like depths (awaited transition), and depends on

the preterminal just seen to contrive a new subtree

to explore. In the fourth case, d′ is incremented as
another incomplete constituent opens up in working

memory. The final two cases simply update the un-

affected store states to reflect their previous states at

time t− 1.

4 Discussion

This factoring of redundant hidden states out of the

Schuler (2009) probabilistic sequence model shows

that cognitive modelling can more closely approx-

imate a simple recurrent network model of lan-

guage processing (Elman, 1991). Probabilistic se-

quence model parsers have previously been mod-

elled with random variables over incomplete con-

stituents (Schuler, 2009). In the current implementa-

tion, each variable can be thought of as a bank of ar-

tificial neurons. These artificial neurons inhibit one

another through the process of normalization. Con-

versely, they activate artificial neurons at subsequent

time steps by contributing probability mass through

the transformed grammar. This point was made by

Norris and McQueen (2008) with respect to lexical

access; this model extends it to parsing.

Recurrent networks can parse simple sentences

but run into problems when running over more com-

plex datasets. This limitation comes from the unsu-

pervised methods typically used to train them, which

have difficulty scaling to sufficiently large training

sets for more complex constructions. The approach

described in this paper uses a hidden context simi-

lar to that of a recurrent network to inform the pro-

gression of the parse, except that the context is in

terms of random variables with distributions over a

set of explicit syntactic categories. By framing the

variable domains in a linguistically-motivated fash-

ion, the problem of acquisition can be divested from

the problem of processing. This paper then uses the

semi-supervised grammar training of Petrov et al.

(2006) in order to develop a simple, accurate model

for broad-coverage parsing independent of scale.

56



p1

x1

q11

q21

q31

q41

p2

x2

q12

q22

q32

q42

p3

x3

q13

q23

q33

q43

p4

x4

q14

q24

q34

q44

p5

x5

q15

q25

q35

q45

p6

x6

q16

q26

q36

q46

p7

x7

q17

q27

q37

q47

f2 f3 f4 f5 f6 f7 f8=
DT

=th
e

=N
P/N

N

=N
N

=fu
nd

=+

=S/
VP

=V
B

=bo
ugh

t

=S/
VP

=V
P/N

P

=D
T

=tw
o

=S/
VP

=V
P/N

N

=JJ

=re
gion

al

=S/
VP

=V
P/N

N

=N
N

=ba
nks

=+

=S/
RB

=RB

=to
day

=+

Figure 2: Parse using Simplified Model

Like Schuler (2009), the incremental parser dis-

cussed here operates in O(n) time where n is the
length of the input. Further, by its incremental na-

ture, this parser is able to run continuously on a

stream of input, which allows any other processes

dependent on the input (such as discourse integra-

tion) to run in parallel regardless of the length of the

input.

5 Computational Benefit

Due to the decreased number of decisions required

by this simplified model, it is substantially faster

than previous similar models. To test this speed in-

crease, the simplified model was compared with that

of Schuler (2009). Both parsers used a grammar that

had undergone 5 iterations of the Petrov et al. (2006)

split-merge-smooth algorithm as found to be opti-

mal by Petrov and Klein (2007), and both used a

beam-width of 500 elements. Sections 02-21 of the

Wall Street Journal Treebank were used in training

the grammar induction for both parsers according to

Petrov et al. (2006), and Section 23 was used for

evaluation. No tuning was done as part of the trans-

form to a sequence model. Speed results can be seen

in Table 1. While the speed is not state-of-the-art in

the field of parsing at large, it does break new ground

for factored sequence model parsers.

To test the accuracy of this parser, it was com-

pared using varying beam-widths to the Petrov and

Klein (2007) and Roark (2001) parsers. With the

exception of the Roark (2001) parser, all parsers

used 5 iterations of the Petrov et al. (2006) split-

System Sec/Sent

Schuler 2009 74

Current Model 12

Table 1: Speed comparison with an unfactored proba-

bilistic sequence model using a beam-width of 500 ele-

ments

System P R F

Roark 2001 86.6 86.5 86.5

Current Model (500) 86.6 87.3 87.0

Current Model (2000) 87.8 87.8 87.8

Current Model (5000) 87.8 87.8 87.8

Petrov Klein (Binary) 88.1 87.8 88.0

Petrov Klein (+Unary) 88.3 88.6 88.5

Table 2: Accuracy comparison with state-of-the-art mod-

els. Numbers in parentheses are number of parallel acti-

vated hypotheses

merge-smooth algorithm, and the training and test-

ing datasets remained the same. These results may

be seen in Table 2. Note that the Petrov and Klein

(2007) parser allows unary branching within the

phrase structure, which is not well-defined under the

right-corner transform. To obtain a fair comparison,

it was also run with strict binarization. The cur-

rent approach achieves comparable accuracy to the

Petrov and Klein (2007) parser assuming a strictly

binary-branching phrase structure.

57



6 Conclusion

The primary goal of this paper was to demonstrate

that a cognitively-motivated factoring of an exist-

ing probabilistic sequence model parser (Schuler,

2009) is not only more attractive from a modelling

perspective but also more efficient. Such factor-

ing yields a much slimmer model where every vari-

able has cognitive correlates to working memory el-

ements. This also renders several transition prob-

abilities deterministic and the ensuing representa-

tional economy leads to a 5-fold increase in pars-

ing speed. The results shown here suggest cognitive

modelling can lead to computational benefits.

References

Alfred V. Aho and Jeffery D. Ullman. 1972. The The-

ory of Parsing, Translation and Compiling; Volume. I:

Parsing. Prentice-Hall, Englewood Cliffs, New Jersey.

P. D. Allopenna, J. S. Magnuson, and M. K. Tanenhaus.

1998. Tracking the time course of spoken word recog-

nition using eye movements: evidence for continuous

mapping models. Journal of Memory and Language,

38:419–439.

G. T. M. Altmann and Y. Kamide. 1999. Incremental

interpretation at verbs: restricting the domain of sub-

sequent reference. Cognition, 73:247–264.

Richard Bellman. 1957. Dynamic Programming.

Princeton University Press, Princeton, NJ.

Thomas G. Bever. 1970. The cognitive basis for lin-

guistic structure. In J.R̃. Hayes, editor, Cognition and

the Development of Language, pages 279–362. Wiley,

New York.

Matthew Botvinick. 2007. Multilevel structure in behav-

ior and in the brain: a computational model of fusters

hierarchy. Philosophical Transactions of the Royal So-

ciety, Series B: Biological Sciences, 362:1615–1626.

Nelson Cowan. 2001. The magical number 4 in short-

term memory: A reconsideration of mental storage ca-

pacity. Behavioral and Brain Sciences, 24:87–185.

Vera Demberg and Frank Keller. 2008. Data from eye-

tracking corpora as evidence for theories of syntactic

processing complexity. Cognition, 109(2):193–210.

Jeffrey L. Elman. 1991. Distributed representations,

simple recurrent networks, and grammatical structure.

Machine Learning, 7:195–225.

John Hale. 2001. A probabilistic earley parser as a psy-

cholinguistic model. In Proceedings of the Second

Meeting of the North American Chapter of the Associ-

ation for Computational Linguistics, pages 159–166,

Pittsburgh, PA.

Jonathan C Horton and Daniel L Adams. 2005. The cor-

tical column: a structure without a function. Philo-

sophical Transactions of the Royal Society of London

- Series B: Biological Sciences, 360(1456):837–862.

Daniel Jurafsky. 1996. A probabilistic model of lexical

and syntactic access and disambiguation. Cognitive

Science: A Multidisciplinary Journal, 20(2):137–194.

Roger Levy. 2008. Expectation-based syntactic compre-

hension. Cognition, 106(3):1126–1177.

Mitchell P. Marcus, Beatrice Santorini, and Mary Ann

Marcinkiewicz. 1993. Building a large annotated cor-

pus of English: the Penn Treebank. Computational

Linguistics, 19(2):313–330.

William D. Marslen-Wilson. 1975. Sentence per-

ception as an interactive parallel process. Science,

189(4198):226–228.

Marshall R. Mayberry, III and Risto Miikkulainen. 2003.

Incremental nonmonotonic parsing through semantic

self-organization. In Proceedings of the 25th Annual
Conference of the Cognitive Science Society, pages

798–803, Boston, MA.

James L. McClelland and Jeffrey L. Elman. 1986. The

trace model of speech perception. Cognitive Psychol-

ogy, 18:1–86.

Dennis Norris and James M. McQueen. 2008. Shortlist

b: A bayesian model of continuous speech recognition.

Psychological Review, 115(2):357–395.

Dennis Norris. 1994. Shortlist: A connectionist model

of continuous speech recognition. Cognition, 52:189–

234.

Slav Petrov and Dan Klein. 2007. Improved infer-

ence for unlexicalized parsing. In Proceedings of

NAACL HLT 2007, pages 404–411, Rochester, New

York, April. Association for Computational Linguis-

tics.

Slav Petrov, Leon Barrett, Romain Thibaux, and Dan

Klein. 2006. Learning accurate, compact, and inter-

pretable tree annotation. In Proceedings of the 44th

Annual Meeting of the Association for Computational

Linguistics (COLING/ACL’06).

Brian Roark. 2001. Probabilistic top-down parsing

and language modeling. Computational Linguistics,

27(2):249–276.

William Schuler, Samir AbdelRahman, Tim Miller, and

Lane Schwartz. 2010. Broad-coverage incremental

parsing using human-like memory constraints. Com-

putational Linguistics, 36(1):1–30.

William Schuler. 2009. Parsing with a bounded stack

using a model-based right-corner transform. In Pro-

ceedings of NAACL/HLT 2009, NAACL ’09, pages

344–352, Boulder, Colorado. Association for Compu-

tational Linguistics.

58



Michael K. Tanenhaus, Michael J. Spivey-Knowlton,

Kathy M. Eberhard, and Julie E. Sedivy. 1995. Inte-

gration of visual and linguistic information in spoken

language comprehension. Science, 268:1632–1634.

A Grammar Formulation

Given D memory elements indexed by d (see Sec-

tion 2.2) and a PCFG θG, the probability θ
(k)
Ts,d of a

tree rooted at a left or right sibling s ∈ {L, R} of
category cη ∈ C requiring d ∈ 1..D memory ele-
ments is defined recursively over paths of increasing

length k:

P
θ
(0)
Ts,d

(1 | cη)
def
= 0 (25)

P
θ
(k)
TL,d

(1 | cη)
def
=

∑

xη

PθG(cη → xη)

+
∑

cη0,cη1

PθG(cη → cη0 cη1)

· P
θ
(k−1)
TL,d

(1 | cη0) · Pθ(k−1)
TR,d

(1 | cη1)

(26)

P
θ
(k)
TR,d

(1 | cη)
def
=

∑

xη

PθG(cη → xη)

+
∑

cη0,cη1

PθG(cη → cη0 cη1)

· P
θ
(k−1)
TL,d+1

(1 | cη0) · Pθ(k−1)
TR,d

(1 | cη1)

(27)

Note that the center embedding depth d increases
only for left children of right children. This is be-

cause in a binary branching structure, center embed-

dings manifest as zigzags. Since the model is also

sensitive to the depth d of each decomposition, the
side- and depth-specific probabilities of θGL,d and

θGR,d are defined as follows:

PθGL,d(cη → xη)
def
=

PθG(cη → xη)

P
θ
(∞)
TL,d

(1 | cη)
(28)

PθGR,d(cη → xη)
def
=

PθG(cη → xη)

P
θ
(∞)
TR,d

(1 | cη)
(29)

PθGL,d(cη → cη0 cη1)
def
= PθG(cη → cη0 cη1)

· P
θ
(∞)
TL,d

(1 | cη0) · Pθ(∞)
TR,d

(1 | cη1)

· P
θ
(∞)
TL,d

(1 | cη)
−1 (30)

PθGR,d(cη → cη0 cη1)
def
= PθG(cη → cη0 cη1)

· P
θ
(∞)
TL,d+1

(1 | cη0) · Pθ(∞)
TR,d

(1 | cη1)

· P
θ
(∞)
TR,d

(1 | cη)
−1 (31)

The model will also need an expected count

EθG∗,d(cη
∗

→ cηι ...) of the given child constituent
cηι dominating a prefix of constituent cη. Expected
versions of these counts may later be used to derive

probabilities of memory store state transitions (see

Sections 2.3, 3).

EθG∗,d(cη
0
→ cη ...)

def
=

∑

xη

PθGR,d(cη → xη)

(32)

EθG∗,d(cη
1
→ cη0 ...)

def
=

∑

cη1

PθGR,d(cη → cη0 cη1)

(33)

EθG∗,d(cη
k
→ cηι0 ...)

def
=

∑

cηι

EθG∗,d(cη
k−1
→ cηι ...)

·
∑

cηι1

PθGL,d(cηι → cηι0 cηι1)

(34)

EθG∗,d(cη
∗

→ cηι ...)
def
=

∞
∑

k=0

EθG∗,d(cη
k
→ cηι ...)

(35)

EθG∗,d(cη
+
→ cηι ...)

def
=

∞
∑

k=1

EθG∗,d(cη
k
→ cηι ...)

(36)

Equation 32 gives the probability of a constituent

appearing as an observation, and Equation 33 gives

the probability of a constituent appearing as a left

59



child. Equation 34 extends the previous two equa-

tions to account for a constituent appearing at an ar-

bitrarily deep embedded path of length k. Taking
the sum of all k path lengths (as in Equation 35)
allows the model to account for constituents any-

where in the left progeny of the dominated subtree.

Similarly, Equation 36 gives the expectation that the

constituent is non-immediately dominated by cη. In
practice the infinite sum is estimated to some con-

stant K using value iteration (Bellman, 1957).

60


