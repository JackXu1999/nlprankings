



















































Reusing Weights in Subword-Aware Neural Language Models


Proceedings of NAACL-HLT 2018, pages 1413–1423
New Orleans, Louisiana, June 1 - 6, 2018. c©2018 Association for Computational Linguistics

Reusing Weights in Subword-aware Neural Language Models

Zhenisbek Assylbekov
School of Science and Technology

Nazarbayev University
zhassylbekov@nu.edu.kz

Rustem Takhanov
School of Science and Technology

Nazarbayev University
rustem.takhanov@nu.edu.kz

Abstract

We propose several ways of reusing subword
embeddings and other weights in subword-
aware neural language models. The pro-
posed techniques do not benefit a competi-
tive character-aware model, but some of them
improve the performance of syllable- and
morpheme-aware models while showing sig-
nificant reductions in model sizes. We dis-
cover a simple hands-on principle: in a multi-
layer input embedding model, layers should be
tied consecutively bottom-up if reused at out-
put. Our best morpheme-aware model with
properly reused weights beats the competitive
word-level model by a large margin across
multiple languages and has 20%–87% fewer
parameters.

1 Introduction

A statistical language model (LM) is a model
which assigns a probability to a sequence of
words. It is used in speech recognition, ma-
chine translation, part-of-speech tagging, informa-
tion retrieval and other applications. Data sparsity
is a major problem in building traditional n-gram
language models, which assume that the proba-
bility of a word only depends on the previous n
words. To deal with potentially severe problems
when confronted with any n-grams that have not
explicitly been seen before, some form of smooth-
ing is necessary.

Recent progress in statistical language mod-
eling is connected with neural language models
(NLM), which tackle the data sparsity problem by
representing words as vectors. Typically this is
done twice: at input (to embed the current word
of a sequence into a vector space) and at output
(to embed candidates for the next word of a se-
quence). Especially successful are the models in
which the architecture of the neural network be-
tween input and output is recurrent (Mikolov et al.,

2010), which we refer to as recurrent neural net-
work language models (RNNLM).

Tying input and output word embeddings in
word-level RNNLM is a regularization technique,
which was introduced earlier (Bengio et al., 2001;
Mnih and Hinton, 2007) but has been widely used
relatively recently, and there is empirical evidence
(Press and Wolf, 2017) as well as theoretical justi-
fication (Inan et al., 2017) that such a simple trick
improves language modeling quality while de-
creasing the total number of trainable parameters
almost two-fold, since most of the parameters are
due to embedding matrices. Unfortunately, this
regularization technique is not directly applicable
to subword-aware neural language models as they
receive subwords at input and return words at out-
put. This raises the following questions: Is it pos-
sible to reuse embeddings and other parameters in
subword-aware neural language models? Would
it benefit language modeling quality? We exper-
imented with different subword units, embedding
models, and ways of reusing parameters, and our
answer to both questions is as follows: There are
several ways to reuse weights in subword-aware
neural language models, and none of them im-
prove a competitive character-aware model, but
some of them do benefit syllable- and morpheme-
aware models, while giving significant reductions
in model sizes. A simple morpheme-aware model
that sums morpheme embeddings of a word ben-
efits most from appropriate weight tying, showing
a significant gain over the competitive word-level
baseline across different languages and data set
sizes. Another contribution of this paper is the dis-
covery of a hands-on principle that in a multi-layer
input embedding model, layers should be tied con-
secutively bottom-up if reused at output.

The source code for the morpheme-aware
model is available at https://github.com/
zh3nis/morph-sum.

1413



2 Related Work

Subword-aware NLM: There has been a large
number of publications in the last 2–3 years on
subword-level and subword-aware NLMs,1 espe-
cially for the cases when subwords are charac-
ters (Ling et al., 2015; Kim et al., 2016; Verwimp
et al., 2017) or morphemes (Botha and Blunsom,
2014; Qiu et al., 2014; Cotterell and Schütze,
2015). Less work has been done on syllable-level
or syllable-aware NLMs (Mikolov et al., 2012; As-
sylbekov et al., 2017; Yu et al., 2017). For a thor-
ough and up-to-date review of the previous work
on subword-aware neural language modeling we
refer the reader to the paper by Vania and Lopez
(2017), where the authors systematically compare
different subword units (characters, character tri-
grams, BPE, morphs/morphemes) and different
representation models (CNN, Bi-LSTM, summa-
tion) on languages with various morphological ty-
pology.
Tying weights in NLM: Reusing embeddings
in word-level neural language models is a tech-
nique which was used earlier (Bengio et al., 2001;
Mnih and Hinton, 2007) and studied in more de-
tails recently (Inan et al., 2017; Press and Wolf,
2017). However, not much work has been done on
reusing parameters in subword-aware or subword-
level language models. Jozefowicz et al. (2016)
reused the CharCNN architecture of Kim et al.
(2016) to dynamically generate softmax word em-
beddings without sharing parameters with an in-
put word-embedding sub-network. They man-
aged to significantly reduce the total number of
parameters for large models trained on a huge
dataset in English (1B tokens) with a large vo-
cabulary (800K tokens) at the expense of deteri-
orated performance. Labeau and Allauzen (2017)
used similar approach to augment the output word
representations with subword-based embeddings.
They experimented with characters and morpho-
logical decompositions, and tried different compo-
sitional models (CNN, Bi-LSTM, concatenation)
on Czech dataset consisting of 4.7M tokens. They
were not tying weights between input and out-
put representations, since their preliminary exper-
iments with tied weights gave worse results.

Our approach differs in the following aspects:

1Subword-level LMs rely on subword-level inputs and
make predictions at the level of subwords; subword-aware
LMs also rely on subword-level inputs but make predictions
at the level of words.

we focus on the ways to reuse weights at output,
seek both model size reduction and performance
improvement in subword-aware language models,
try different subword units (characters, syllables,
and morphemes), and make evaluation on small
(1M–2M tokens) and medium (17M–51M tokens)
data sets across multiple languages.

3 Recurrent Neural Language Model

LetW be a finite vocabulary of words. We assume
that words have already been converted into in-
dices. Let EinW ∈ R|W|×dW be an input embedding
matrix for words — i.e., it is a matrix in which the
wth row (denoted as w) corresponds to an embed-
ding of the word w ∈ W .

Based on word embeddings w1:k =
w1, . . . ,wk for a sequence of words w1:k, a
typical word-level RNN language model produces
a sequence of states h1:k according to

ht = RNNCell(wt,ht−1), h0 = 0. (1)

The last state hk is assumed to contain informa-
tion on the whole sequence w1:k and is further
used for predicting the next word wk+1 of a se-
quence according to the probability distribution

Pr(wk+1|w1:k) = softmax(hkEoutW + b), (2)
where EoutW ∈ RdLM×|W| is an output embedding
matrix, b ∈ R|W| is a bias term, and dLM is a state
size of the RNN.
Subword-based word embeddings: One of the
more recent advancements in neural language
modeling has to do with segmenting words at in-
put into subword units (such as characters, syl-
lables, morphemes, etc.) and composing each
word’s embedding from the embeddings of its sub-
words. Formally, let S be a finite vocabulary of
subwords,2 and let EinS ∈ R|S|×dS be an input em-
bedding matrix for subwords. Any word w ∈ W
is a sequence of its subwords (s1, s2, . . . , snw) =
σ(w), and hence can be represented as a sequence
of the corresponding subword vectors:

[s1, s2, . . . , snw ]. (3)

A subword-based word embedding model
E(·; EinS ,Θin) with parameters (EinS ,Θin) con-
structs a word vector x from the sequence of
subword vectors (3), i.e.

x = E(σ(w); EinS ,Θ
in), (4)

2As in the case of words, we assume that subwords have
already been converted into indices.

1414



unconstitutional conditions on

subword-based softmax

word-level
RNNLM

word vector

subword-based embedding of a word

subword
vectors

un con sti tu tional

imposes unconstitutional conditions

Figure 1: Subword-aware RNNLM with subword-
based softmax.

which is then fed into a RNNLM (1) instead of
a plain embedding w. The additional parameters
Θin correspond to the way the embedding model
constructs the word vector: for instance, in the
CharCNN model of Kim et al. (2016), Θin are the
weights of the convolutional and highway layers.
Reusing word embeddings: Another recent tech-
nique in word-level neural language modeling is
tying input and output word embeddings:

EinW =
(
EoutW

)T
,

under the assumption that dW = dLM. Although
being useful for word-level language modeling
(Press and Wolf, 2017; Inan et al., 2017), this reg-
ularization technique is not directly applicable to
subword-aware language models, as they receive
subword embeddings at input and return word em-
beddings at output. In the next section we describe
a simple technique to allow reusing subword em-
beddings EinS as well as other parameters Θ

in in a
subword-aware RNNLM.

4 Reusing Weights

Let EoutS be an output embedding matrix for sub-
words and let us modify the softmax layer (2) so
that it utilizes EoutS instead of the word embedding
matrix EoutW . The idea is fairly straightforward: we
reuse an embedding model (4) to construct a new

word embedding matrix:

ÊoutW = [E(σ(w); E
out
S ,Θ

out) for w ∈ W], (5)
and use ÊoutW instead of E

out
W in the softmax layer

(2). Such modification of the softmax layer will be
referred to as subword-based softmax. The over-
all architecture of a subword-aware RNNLM with
subword-based softmax is given in Figure 1. Such
a model allows several options for reusing embed-
dings and weights, which are discussed below.
• Reusing neither subword embeddings nor em-

bedding model weights: As was shown by Joze-
fowicz et al. (2015), this can significantly re-
duce the total number of parameters for large
models trained on huge datasets (1B tokens)
with large vocabularies (800K tokens). How-
ever, we do not expect significant reductions on
smaller data sets (1-2M tokens) with smaller vo-
cabularies (10-30K tokens), which we use in our
main experiments.
• Reusing subword embeddings (RE) can be done

by setting EoutS = E
in
S in (5). This will give a

significant reduction in model size for models
with |EinS | � |Θin|,3 such as the morpheme-
aware model of Botha and Blunsom (2014).
• Reusing weights of the embedding model (RW)

can be done by setting Θout = Θin. Unlike the
previous option, this should significantly reduce
sizes of models with |EinS | � |Θin|, such as the
character-aware model of Kim et al. (2016).
• Reusing both subword embeddings and weights

of the embedding model (RE+RW) can be done
by setting EoutS = E

in
S and Θ

out = Θin simul-
taneously in (5). This should significantly re-
duce the number of trainable parameters in any
subword-aware model. Here we use exactly the
same word representations both at input and at
output, so this option corresponds to the reusing
of plain word embeddings in pure word-level
language models.

5 Experimental Setup

Data sets: All models are trained and evaluated on
the PTB (Marcus et al., 1993) and the WikiText-
2 (Merity et al., 2017) data sets. For the PTB
we utilize the standard training (0-20), validation
(21-22), and test (23-24) splits along with pre-
processing per Mikolov et al. (2010). WikiText-2
is an alternative to PTB, which is approximately
two times as large in size and three times as large

3|A| denotes number of elements in A.

1415



in vocabulary (Table 1).

Data set T |W| |S| |M|
PTB 0.9M 10K 5.9K 3.4K
WikiText-2 2.1M 33K 19.5K 8.8K

Table 1: Corpus statistics. T = number of tokens in
training set; |W| = word vocabulary size; |S| = sylla-
ble vocabulary size; |M| = morph vocabulary size.

Subword-based embedding models: We experi-
ment with existing representational models which
have previously proven effective for language
modeling.
• CharCNN (Kim et al., 2016) is a character-

aware convolutional model, which performs on
par with the 2014–2015 state-of-the-art word-
level LSTM model (Zaremba et al., 2014) de-
spite having 60% fewer parameters.
• SylConcat is a simple concatenation of sylla-

ble embeddings suggested by Assylbekov et al.
(2017), which underperforms CharCNN but has
fewer parameters and is trained faster.
• MorphSum is a summation of morpheme em-

beddings, which is similar to the approach of
Botha and Blunsom (2014) with one important
difference: the embedding of the word itself is
not included into the sum. We do this since
other models do not utilize word embeddings.

In all subword-aware language models we inject
a stack of two highway layers (Srivastava et al.,
2015) right before the word-level RNNLM as done
by Kim et al. (2016), and the non-linear activa-
tion in any of these highway layers is a ReLU. The
highway layer size is denoted by dHW.
Word-level RNNLM: There is a large variety of
RNN cells to choose from in (1). To make our
results directly comparable to the previous work
of Inan et al. (2017), Press and Wolf (2017) on
reusing word embeddings we select a rather con-
ventional architecture – a stack of two LSTM cells
(Hochreiter and Schmidhuber, 1997).
Hyperparameters: We experiment with two con-
figurations for the state size dLM of the word-level
RNNLM: 200 (small models) and 650 (medium-
sized models). In what follows values outside
brackets correspond to small models, and values
within brackets correspond to medium models.
• CharCNN: We use the same hyperparameters as

in the work of Kim et al. (2016), where “large
model” stands for what we call “medium-sized
model”.

• SylConcat: dS = 50 (200), dHW = 200 (800).
These choices are guided by the work of Assyl-
bekov et al. (2017).
• MorphSum: dS = dHW = 200 (650). These

choices are guided by Kim et al. (2016).
Optimizaton method is guided by the previous
works (Zaremba et al., 2014; Gal and Ghahra-
mani, 2016) on word-level language modeling
with LSTMs. See Appendix A for details.
Syllabification and morphological segmenta-
tion: True syllabification of a word requires
its grapheme-to-phoneme conversion and then
its splitting up into syllables based on some
rules. True morphological segmentation requires
rather expensive morphological analysis and dis-
ambiguation tools. Since these are not always
available for under-resourced languages, we de-
cided to utilize Liang’s widely-used hyphenation
algorithm (Liang, 1983) and an unsupervised mor-
phological segmentation tool, Morfessor 2.0 (Vir-
pioja et al., 2013), as approximations to syllabi-
fication and morphological segmentation respec-
tively. We use the default configuration of Morfes-
sor 2.0. Syllable and morpheme vocabulary sizes
for both PTB and WikiText-2 are reported in Ta-
ble 1.

6 Results

In order to investigate the extent to which each of
our proposed options benefits the language mod-
eling task, we evaluate all four modifications (no
reusing, RE, RW, RE+RW) for each subword-
aware model against their original versions and
word-level baselines. The results of evaluation are
given in Table 2. We have both negative and posi-
tive findings which are summarized below.

Negative results:
• The ‘no reusing’ and RW options should never

be applied in subword-aware language models
as they deteriorate the performance.
• Neither of the reusing options benefits Char-

CNN when compared to the original model with
a plain softmax layer.

Positive results:
• The RE+RW option puts CharCNN’s perfor-

mance close to that of the original version, while
reducing the model size by 30–75%.
• The RE and RE+RW are the best reusing options

for SylConcat, which make it on par with the
original CharCNN model, despite having 35–
75% fewer parameters.

1416



PTB Wikitext-2

Model
Small Medium Small Medium

Size PPL Size PPL Size PPL Size PPL

Word 4.7M 88.1 19.8M 79.8 14M 111.9 50.1M 95.7
Word + reusing word emb’s 2.7M 86.6 13.3M 74.5 7.3M 104.1 28.4M 89.9

CharCNN (original) 4.1M 87.3 19.4M 77.1 8.7M 101.6 34.5M 88.7
CharCNN 3.3M 97.5 18.5M 89.2 3.3M 110.6 18.5M —
CharCNN + RE 3.3M 99.1 18.5M 82.9 3.3M 110.2 18.5M —
CharCNN + RW 2.2M 93.5 13.6M 103.2 2.2M 111.5 13.6M —
CharCNN + RE + RW 2.2M 91.0 13.6M 79.9 2.2M 101.8 13.6M —

SylConcat (original) 3.2M 89.0 18.7M 77.9 8.5M 105.7 36.6M 91.4
SylConcat 1.7M 96.9 17.7M 90.5 3.1M 118.1 23.2M 114.8
SylConcat + RE 1.4M 87.4 16.6M 75.7 2.1M 101.0 19.3M 94.2
SylConcat + RW 1.6M 99.9 15.2M 96.2 2.9M 118.9 19.4M 112.1
SylConcat + RE + RW 1.2M 88.4 12.7M 76.2 1.9M 101.0 15.5M 86.7

MorphSum (original) 3.5M 87.5 17.2M 78.5 9.3M 101.9 35.8M 90.1
MorphSum 2.4M 89.0 14.5M 82.4 4.5M 100.3 21.7M 86.7
MorphSum + RE 1.6M 85.5 12.3M 74.1 2.8M 97.6 15.9M 81.2
MorphSum + RW 2.2M 89.6 12.8M 81.0 4.4M 101.4 20.0M 86.6
MorphSum + RE + RW 1.5M 85.1 10.7M 72.2 2.6M 96.5 14.2M 77.5

Table 2: Results. The pure word-level models and original versions of subword-aware models (with regular soft-
max) serve as baselines. Reusing the input embedding architecture at output in CharCNN leads to prohibitively
slow models when trained on WikiText-2 (≈800 tokens/sec on NVIDIA Titan X Pascal); we therefore abandoned
evaluation of these configurations.

• The RE and RE+RW configurations benefit
MorphSum making it not only better than its
original version but also better than all other
models and significantly smaller than the word-
level model with reused embeddings.

In what follows we proceed to analyze the ob-
tained results.

6.1 CharCNN is biased towards surface form

We hypothesize that the reason CharCNN does not
benefit from tied weights is that CNN over char-
acter embeddings is an excessively flexible model
which learns to adapt to a surface form more than
to semantics. To validate this hypothesis we pick
several words4 from the English PTB vocabulary
and consider their nearest neighbors under cosine
similarity as produced by the medium-sized mod-
els (with the regular softmax layer) at input (Ta-
ble 3). As we can see from the examples, the
CharCNN model is somewhat more biased to-
wards surface forms at input than SylConcat and

4We pick the same words as Kim et al. (2016).

MorphSum.5 When CharCNN is reused to gener-
ate a softmax embedding matrix this bias is prop-
agated to output embeddings as well (Table 3).

6.2 Tying weights bottom-up

From Table 2 one can notice that tying weights
without tying subword embeddings (RW) always
results in worse performance than the tying both
weights and embeddings (RE+RW). Recall that
subword embedding lookup is done before the
weights of subword-aware embedding model are
used (see Figure 1). This leads us to the following

Conjecture. Let EinS = Θ
in
0 ,Θ

in
1 ,Θ

in
2 , . . . ,Θ

in
n

be the parameters of the consecutive layers of a
subword-aware input embedding model (4), i.e.
x = x(n) = fn

(
x(n−1); Θinn

)
, . . . , x(1) =

f1
(
x(0); Θin1

)
, x(0) = f0

(
σ(w); EinS

)
and let

EoutS = Θ
out
0 ,Θ

out
1 ,Θ

out
2 , . . . ,Θ

out
n be the parame-

ters of the consecutive layers of a subword-aware
embedding model used to generate the output pro-
jection matrix (5). LetA be a subword-aware neu-

5A similar observation for character-aware NLMs was
made by Vania and Lopez (2017).

1417



Model In Vocabulary Out-of-Vocabularywhile his you richard trading computer-aided misinformed looooook
IN

PU
T

E
M

B
E

D
D

IN
G

S
chile hhs god graham traded computer-guided informed look

CharCNN whole its we harold tradition computerized performed looks
(original) meanwhile her your edward heading computer-driven formed looking

although this i ronald eroding black-and-white confirmed looked
although my kemp thomas printing computer-guided reinforced —

SylConcat though historic welch robert working computer-driven surprised —
(original) when your i stephen lending computerized succeeding —

mean irish shere alan recording computer succeed —
although mystery i stephen program-trading cross-border informed nato

MorphSum whenever my ghandi leonard insider-trading bank-backed injured lesko
(original) when whiskey we william relations pro-choice confined imo

1980s sour cadillac robert insurance government-owned formed swapo

I/
O

E
M

B
’S thi her we gerard trades computer-guided informed look

CharCNN when its your gerald trader large-scale performed outlook
+ RE + RW after the young william traders high-quality outperformed looks

above heir why edward trade futures-related confirmed looked

Table 3: Nearest neighbors based on cosine similarity. We underline character ngrams in words which are close
to the given word orthographically rather than semantically. The pyphen syllabifier, which is used in SylConcat,
failed to segment the word ‘looooook’ into syllables, and therefore its neighbors are not available.

ral language model in which the first (j+1) layers
of input and output embedding sub-networks have
tied weights:

∀i = 0, j : Θini = Θouti ,
and let B be a model in which at least one layer
below the (j + 1)th layer has untied weights:

∃i = 0, j − 1 : Θini 6= Θouti , Θinj = Θoutj .
Then model B performs at most as well as model
A, i.e. PPLA ≤ PPLB .

To test this conjecture empirically, we conduct
the following experiments: in all three embedding
models (CharCNN, SylConcat, and MorphSum),
we reuse different combinations of layers. If an
embedding model has n layers, there are 2n ways
to reuse them, as each layer can either be tied or
untied at input and output. However, there are two
particular configurations for each of the embed-
ding models that do not interest us: (i) when nei-
ther of the layers is reused, or (ii) when only the
very first embedding layer is reused. Hence, for
each model we need to check 2n − 2 configura-
tions. For faster experimentation we evaluate only
small-sized models on PTB. The results are re-
ported in Table 4. As we can see, the experiments
in general reject our conjecture: in SylConcat leav-
ing an untied first highway layer between tied em-
bedding and second highway layers (denote this as
HW2+Emb) turned out to be slightly better than
tying all three layers (HW2+HW1+Emb). Recall,
that a highway is a weighted average between non-
linear and identity transformations of the incom-

ing vector:

x 7→ t� ReLU(xA + b) + (1− t)� x,
where t = σ(xW + c) is a transform gate, A,
W, b and c are trainable parameters, and � is
the element-wise multiplication operator. To find
out why leaving an untied highway below a tied
one is beneficial in SylConcat, we compare the
distributions of the transform gate values t from
the first highway layers of both configurations,
HW2+Emb and HW2+HW1+Emb, in SylConcat
and MorphSum (Figure 2).

We can see that SylConcat heavily relies on
nonlinearity in the first highway layer, while
MorphSum does not utilize much of it. This means
that in MorphSum, the highway is close to an iden-
tity operator (t ≈ 0), and does not transform the
sum of morpheme vectors much, either at input
or at output. Therefore, tying the first highway
layer is natural to Morh-Sum. SylConcat, on the
other hand, applies non-linear transformations to
the concatenation of syllable vectors, and hence
makes additional preparations of the word vector
for the needs of the RNNLM at input and for Soft-
max prediction at output. These needs differ from
each other (as shown in the next subsection). This
is why SylConcat benefits from an additional de-
gree of freedom when the first highway is left un-
tied.

Despite not being true in all cases, and due
to being true in many cases, we believe that the
above-mentioned conjecture is still useful. In
short it can be summarized as a practical hands-

1418



HW2 HW1 CNN Emb PPL
X 94.1
X X 92.8

X 94.6
X X 94.5
X X 93.1
X X X 90.1

X 94.9
X X 99.2
X X 94.1
X X X 92.5
X X 94.3
X X X 97.8
X X X 96.3
X X X X 91.0

HW2 HW1 Emb PPL
X 95.4
X X 87.4

X 99.0
X X 87.9
X X 96.2
X X X 88.4

HW2 HW1 Emb PPL
X 90.0
X X 84.7

X 89.9
X X 85.7
X X 89.4
X X X 85.1

Table 4: Reusing different combinations of layers in small CharCNN (left), small SylConcat (top right) and small
MorphSum on PTB data. “X” means that the layer is reused at output.

0.0 0.2 0.4 0.6 0.8 1.0

0
2

4
6

8

Transform gate values

D
e

n
s
it
y

Input

Output

Tied

0.0 0.2 0.4 0.6 0.8 1.0

0
2

4
6

8

Transform gate values

D
e

n
s
it
y

Input

Output

Tied

Figure 2: Kernel density estimations of the transform gate values of the first highway layers in SylConcat (left) and
MorphSum. Values corresponding to ‘Input’ and ‘Output’ curves come from the HW2+Emb configurations, while
those corresponding to ‘Tied’ curves come from the HW2+HW1+Emb configurations.

on rule:

Layers should be tied consecutively bottom-up,

i.e. one should not leave untied layer(s) below a
tied one. Keep in mind that this rule does not guar-
antee a performance increase as more and more
layers are tied. It only says that leaving untied
weights below the tied ones is likely to be worse
than not doing so.

6.3 Difference between input and output
embeddings

One can notice from the results of our experi-
ments (Table 4) that having an untied second high-
way layer above the first one always leads to bet-
ter performance than when it is tied. This means
that there is a benefit in letting word embeddings
slightly differ at input and output, i.e. by spe-
cializing them for the needs of RNNLM at input
and of Softmax at output. This specialization is
quite natural, as input and output representations
of words have two different purposes: input rep-

resentations send a signal to the RNNLM about
the current word in a sequence, while output rep-
resentations are needed to predict the next word
given all the preceding words. The difference be-
tween input and output word representations is
discussed in greater detail by Garten et al. (2015)
and Press and Wolf (2017). Here we decided to
verify the difference indirectly: we test whether
intrinsic dimensionality of word embeddings sig-
nificantly differs at input and output. For this, we
apply principal component analysis to word em-
beddings produced by all models in “no reusing”
mode. The results are given in Figure 3, where we
can see that dimensionalities of input and output
embeddings differ in the word-level model, Char-
CNN, and SylConcat models, but the difference is
less significant in MorphSum model. Interestingly,
in word-level and MorphSum models the output
embeddings have more principal components than
the input ones. In CharCNN and SylConcat, how-
ever, results are to other way around. We defer the
study of this phenomenon to the future work.

1419



0 100 200 300 400 500 600

0
.2

0
.4

0
.6

0
.8

1
.0

Input

Output

0 100 200 300 400 500

0
.0

0
.2

0
.4

0
.6

0
.8

1
.0

Input

Output

0 50 100 150 200

0
.0

0
.2

0
.4

0
.6

0
.8

1
.0

Input

Output

0 50 100 150 200

0
.2

0
.4

0
.6

0
.8

1
.0

Input

Output

Figure 3: PCA applied to input and word embeddings produced by different models. Horizontal axis corresponds
to number of principal components, vertical axis corresponds to percentage of total variance to retain. From left to
right: word-level model, CharCNN, SylConcat, MorphSum.

6.4 CharCNN generalizes better than
MorphSum

One may expect larger units to work better than
smaller units, but smaller units to generalize better
than larger units. This certainly depends on how
one defines generalizability of a language model.
If it is an ability to model unseen text with un-
seen words, then, indeed, character-aware models
may perform better than syllable- or morpheme-
aware ones. This can be partially seen from Ta-
ble 3, where the OOV words are better handled
by CharCNN in terms of in-vocabulary nearest
neighbors. However, to fully validate the above-
mentioned expectation we conduct additional ex-
periments: we train two models, CharCNN and
MorphSum, on PTB and then we evaluate them on
the test set of Wikitext-2 (245K words, 10K word-
types). Some words in Wikitext-2 contain char-
acters or morphemes that are not present in PTB,
and therefore such words cannot be embedded by
CharCNN or MorphSum correspondingly. Such
words were replaced by the <unk> token, and we
call them new OOVs6. The results of our experi-
ments are reported in Table 5. Indeed, CharCNN

Model # new OOVs PPL

CharCNN + RE + RW 3659 306.8
MorphSum + RE + RW 4195 316.2

Table 5: Training on PTB and testing on Wikitext-2.

faces less OOVs on unseen text, and thus general-
izes better than MorphSum.

6.5 Performance on non-English Data
According to Table 2, MorphSum+RE+RW com-
fortably outperforms the strong baseline Word+RE

6These are “new” OOVs, since the original test set of
Wikitext-2 already contains “old” OOVs marked as <unk>.

Model FR ES DE CS RU

S Word+RE 218 205 305 514 364

D
-SMorphSum+RE+RW 188 171 246 371 237

M

Word+RE 205 193 277 488 351
MorphSum+RE+RW 172 157 222 338 210

S Word+RE 167 149 285 520 267

D
-MMorphSum+RE+RW 159 143 242 463 229

Table 6: Evaluation on non-English data.
MorphSum+RE+RW has significantly less pa-
rameters than Word+RE (Appendix B). S — small
model, M — medium model, D-S — small data, D-M
— medium data; FR — French, ES — Spanish, DE —
German, CS — Czech, RU — Russian.

(Inan et al., 2017). It is interesting to see whether
this advantage extends to non-English languages
which have richer morphology. For this purpose
we conduct evaluation of both models on small
(1M tokens) and medium (17M–51M tokens) data
in five languages (see corpora statistics in Ap-
pendix B). Due to hardware constraints we only
train the small models on medium-sized data. We
used the same architectures for all languages and
did not perform any language-specific tuning of
hyperparameters, which are specified in Appendix
A. The results are provided in Table 6. As one can
see, the advantage of the morpheme-aware model
over the word-level one is even more pronounced
for non-English data. Also, we can notice that the
gain is larger for small data sets. We hypothesize
that the advantage of MorphSum+RE+RW over
Word+RE diminishes with the decrease of type-
token ratio (TTR). A scatterplot of PPL change
versus TTR (Figure 4) supports this hypothesis.
Moreover, there is a strong correlation between
these two quantities: ρ̂(∆PPL,TTR) = 0.84, i.e.
one can predict the mean decrease in PPL from the
TTR of a text with a simple linear regression:

∆PPL ≈ 2, 109× TTR.

1420



Model PTB WT-2 CS DE ES FR RU

AWD-LSTM-Word w/o emb. dropout 61.38 68.50 410 241 145 151 232
AWD-LSTM-MorphSum + RE + RW 61.17 66 .92 253 177 126 140 162

Table 7: Replacing LSTM with AWD-LSTM.

10 20 30 40 50 60

0
2
0

4
0

6
0

8
0

1
0
0

1
2
0

1
4
0

TTR × 1000

∆P
P

L

FR
ES

DE

CS

RU

FR
ES

DE

CS
RU

PTB

WT2

small data

medium data

Figure 4: PPL improvement vs TTR. ∆PPL =
PPLWord+RE − PPLMorphSum+RE+RW.

6.6 Replacing LSTM with AWD-LSTM

The empirical perplexities in Table 2 are way
above the current state-of-the-art on the same
datasets (Melis et al., 2018). However, the ap-
proach of Melis et al. (2018) requires thousands
of evaluations and is feasible for researchers who
have access to hundreds of GPUs. Unfortunately,
we do not have such access. Also, the authors
do not disclose the optimal hyperparameters they
found, and thus we could not reproduce their mod-
els. There is another state-of-the-art language
model, AWD-LSTM (Merity et al., 2018), which
has open-source code. We replaced this model’s
word embedding layer with the MorphSum sub-
network and fully reused morpheme embed-
dings and other weights of MorphSum at out-
put. We refer to such modification as AWD-LSTM-
MorphSum + RE + RW. We trained both models
without fine-tuning (due to time constraints) and
we did not use embedding dropout (section 4.3
of Merity et al. (2018)) in either model, as it is
not obvious how embeddings should be dropped
in the case of AWD-LSTM-MorphSum. The results
of evaluation on the PTB, Wikitext-2, and non-
English datasets are given in Table 7.

Although AWD-LSTM-MorphSum is on par
with AWD-LSTM-Word on PTB and is slightly
better on Wikitext-2, replacing plain word em-
beddings with the subword-aware model with ap-
propriately reused parameters is crucial for non-

English data. Notice that AWD-LSTM under-
performs LSTM (used by us) on Czech dataset
(cf. Table 6). We think that the hyperparame-
ters of AWD-LSTM in Merity et al. (2018) are
thoroughly tuned for PTB and Wikitext-2 and may
poorly generalize to other datasets.

7 Conclusion

There is no single best way to reuse parameters
in all subword-aware neural language models: the
reusing method should be tailored to each type of
subword unit and embedding model. However, in-
stead of testing an exponential (w.r.t. sub-network
depth) number of configurations, it is sufficient to
check only those where weights are tied consecu-
tively bottom-up.

Despite being similar, input and output embed-
dings solve different tasks. Thus, fully tying input
and output embedding sub-networks in subword-
aware neural language models is worse than let-
ting them be slightly different. This raises the
question whether the same is true for pure word-
level models, and we defer its study to our future
work.

One of our best configurations, a simple
morpheme-aware model which sums morpheme
embeddings and fully reuses the embedding sub-
network, outperforms the competitive word-level
language model while significantly reducing the
number of trainable parameters. However, the
performance gain diminishes with the increase of
training set size.

Acknowledgements

We gratefully acknowledge the NVIDIA Corpora-
tion for their donation of the Titan X Pascal GPU
used for this research. The work of Zhenisbek
Assylbekov has been funded by the Committee
of Science of the Ministry of Education and Sci-
ence of the Republic of Kazakhstan, contract #
346/018-2018/33-28, IRN AP05133700. The au-
thors would like to thank anonymous reviewers for
their valuable feedback, and Dr. J. N. Washington
for proofreading an early version of the paper.

1421



References
Zhenisbek Assylbekov, Rustem Takhanov, Bagdat

Myrzakhmetov, and Jonathan N. Washington. 2017.
Syllable-aware neural language models: A failure to
beat character-aware ones. In Proc. of EMNLP.

Yoshua Bengio, Réjean Ducharme, Pascal Vincent, and
Christian Jauvin. 2001. A neural probabilistic lan-
guage model http://www.iro.umontreal.
ca/œlisa/pointeurs/nips00_lm.ps.

Jan Botha and Phil Blunsom. 2014. Compositional
morphology for word representations and language
modelling. In Proc. of ICML.

Wenlin Chen, David Grangier, and Michael Auli. 2016.
Strategies for training large vocabulary neural lan-
guage models. In Proc. of ACL.

Ryan Cotterell and Hinrich Schütze. 2015. Morpho-
logical word-embeddings. In Proc. of HLT-NAACL.

Yarin Gal and Zoubin Ghahramani. 2016. A theoret-
ically grounded application of dropout in recurrent
neural networks. In Proc. of NIPS.

Justin Garten, Kenji Sagae, Volkan Ustun, and Morteza
Dehghani. 2015. Combining distributed vector rep-
resentations for words. In In Proc. of VS@HLT-
NAACL.

Sepp Hochreiter and Jürgen Schmidhuber. 1997.
Long short-term memory. Neural computation
9(8):1735–1780.

Hakan Inan, Khashayar Khosravi, and Richard Socher.
2017. Tying word vectors and word classifiers: A
loss framework for language modeling. In Proc. of
ICLR.

Sébastien Jean, Kyunghyun Cho, Roland Memisevic,
and Yoshua Bengio. 2015. On using very large tar-
get vocabulary for neural machine translation. In
Proc. of ACL-IJCNLP.

Rafal Jozefowicz, Oriol Vinyals, Mike Schuster, Noam
Shazeer, and Yonghui Wu. 2016. Exploring
the limits of language modeling. arXiv preprint
arXiv:1602.02410 .

Rafal Jozefowicz, Wojciech Zaremba, and Ilya
Sutskever. 2015. An empirical exploration of recur-
rent network architectures. In Proc. of ICML.

Yoon Kim, Yacine Jernite, David Sontag, and Alexan-
der M Rush. 2016. Character-aware neural language
models. In Proc. of AAAI.

Matthieu Labeau and Alexandre Allauzen. 2017. Char-
acter and subword-based word representation for
neural language modeling prediction. In Proc. of
SCLeM@EMNLP.

Franklin Mark Liang. 1983. Word Hy-phen-a-tion by
Com-put-er. Citeseer.

Wang Ling, Chris Dyer, Alan W Black, Isabel Tran-
coso, Ramon Fermandez, Silvio Amir, Luis Marujo,
and Tiago Luis. 2015. Finding function in form:
Compositional character models for open vocabu-
lary word representation. In Proc. of EMNLP.

Mitchell P Marcus, Mary Ann Marcinkiewicz, and
Beatrice Santorini. 1993. Building a large annotated
corpus of english: The penn treebank. Computa-
tional linguistics 19(2):313–330.

Gábor Melis, Chris Dyer, and Phil Blunsom. 2018. On
the state of the art of evaluation in neural language
models. In Proc. of ICLR.

Stephen Merity, Nitish Shirish Keskar, and Richard
Socher. 2018. Regularizing and optimizing lstm lan-
guage models .

Stephen Merity, Caiming Xiong, James Bradbury, and
Richard Socher. 2017. Pointer sentinel mixture
models. In Proc. of ICLR.

Tomas Mikolov, Martin Karafiát, Lukas Burget, Jan
Cernockỳ, and Sanjeev Khudanpur. 2010. Recur-
rent neural network based language model. In Proc.
of INTERSPEECH.

Tomáš Mikolov, Ilya Sutskever, Anoop Deoras, Hai-
Son Le, Stefan Kombrink, and Jan Cernocky.
2012. Subword language modeling with neu-
ral networks. preprint (http://www. fit. vutbr.
cz/imikolov/rnnlm/char. pdf) .

Andriy Mnih and Geoffrey Hinton. 2007. Three new
graphical models for statistical language modelling.
In Proc. of ICML.

Ofir Press and Lior Wolf. 2017. Using the output em-
bedding to improve language models. In Proc. of
EACL.

Siyu Qiu, Qing Cui, Jiang Bian, Bin Gao, and Tie-Yan
Liu. 2014. Co-learning of word representations and
morpheme representations. In Proc. of COLING.

Rupesh K Srivastava, Klaus Greff, and Jürgen Schmid-
huber. 2015. Training very deep networks. In Proc.
of NIPS.

Clara Vania and Adam Lopez. 2017. From characters
to words to in between: Do we capture morphology?
In Proc. of ACL.

Lyan Verwimp, Joris Pelemans, Patrick Wambacq,
et al. 2017. Character-word lstm language models.
In Proc. of EACL.

Sami Virpioja, Peter Smit, Stig-Arne Grönroos, Mikko
Kurimo, et al. 2013. Morfessor 2.0: Python imple-
mentation and extensions for morfessor baseline .

Paul J Werbos. 1990. Backpropagation through time:
what it does and how to do it. Proc. of the IEEE
78(10):1550–1560.

1422



Seunghak Yu, Nilesh Kulkarni, Haejun Lee, and Ji-
hie Kim. 2017. Syllable-level neural language
model for agglutinative language. In Proc. of
SCLeM@EMNLP.

Wojciech Zaremba, Ilya Sutskever, and Oriol Vinyals.
2014. Recurrent neural network regularization.
arXiv preprint arXiv:1409.2329 .

A Optimization

Training the models involves minimizing the neg-
ative log-likelihood over the corpus w1:K :

−∑Tk=1 log Pr(wk|w1:k−1) −→ min,
by truncated BPTT (Werbos, 1990). We backprop-
agate for 35 time steps using stochastic gradient
descent where the learning rate is initially set to

• 1.0 in small word-level models,
• 0.5 in small and medium CharCNN, medium

SylConcat (SS, SS+RW) models,
• 0.7 in all other models,
and start decaying it with a constant rate after a
certain epoch. This is 5 and 10 for the small
word-level and all other networks respectively ex-
cept CharCNN, for which it is 12. The decay rate
is 0.9. The initial values for learning rates were
tuned as follows: for each model we start with
1.0 and decrease it by 0.1 until there is conver-
gence at the very first epoch. We use a batch size
of 20. We train for 70 epochs. Parameters of
the models are randomly initialized uniformly in
[−0.1, 0.1] and in [−0.05, 0.05] for the small and
medium networks, except the forget bias of the
word-level LSTM, which is initialized to 1, and
the transform bias of the highway layer, which is
initialized to values around −2. For regulariza-
tion we use a variant of variational dropout (Gal
and Ghahramani, 2016) proposed by Inan et al.
(2017). For PTB, the dropout rates are 0.3 and 0.5
for the small and medium models. For Wikitext-2,
the dropout rates are 0.2 and 0.4 for the small and
medium models. We clip the norm of the gradients
(normalized by minibatch size) at 5.

For non-English small-sized data sets (Data-S)
we use the same hyperparameters as for PTB. To
speed up training on non-English medium-sized
data (Data-M) we use a batch size of 100 and sam-
pled softmax (Jean et al., 2015) with the number of
samples equal to 20% of the vocabulary size (Chen
et al., 2016).

Data set T |W| |M|

Sm
al

l

French (FR) 1M 25K 6K
Spanish (ES) 1M 27K 7K
German (DE) 1M 37K 8K
Czech (CS) 1M 46K 10K
Russian (RU) 1M 62K 12K

M
ed

iu
m

French (FR) 57M 137K 26K
Spanish (ES) 56M 152K 26K
German (DE) 51M 339K 39K
Czech (CS) 17M 206K 34K
Russian (RU) 25M 497K 56K

Table 8: Non-English corpora statistics. T = number
of tokens in training set; |W| = word vocabulary size;
|M| = morph vocabulary size.

Model FR ES DE CS RU

Sm
al

l Word + RE 5.6 6.1 8.0 10.0 13.4

D
at

a-
S

MorphSum 2.1 2.3 2.5 2.8 3.2+ RE + RW

M
ed

iu
m Word + RE 23.0 24.3 30.6 36.9 48.0

MorphSum 12.6 13.2 13.8 15.0 16.1+ RE + RW
Sm

al
l Word + RE 28.2 31.2 68.8 42.0 100.6

D
at

a-
M

MorphSum 6.2 6.1 8.9 7.7 12.6+ RE + RW

Table 9: Model sizes in millions of trainable parame-
ters.

B Non-English corpora statistics and
model sizes

The non-English small and medium data comes
from the 2013 ACL Workshop on Machine Trans-
lation7 with pre-processing per Botha and Blun-
som (2014). Corpora statistics is provided in Ta-
ble 8.

Model sizes for Word+RE and
MorphSum+RE+RW, which were evaluated
on non-English data sets, are given in Table 9.
MorphSum+RE+RW requires 45%–87% less
parameters than Word+RE.

7http://www.statmt.org/wmt13/
translation-task.html

1423


