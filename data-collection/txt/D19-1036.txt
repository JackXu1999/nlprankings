



















































CaRe: Open Knowledge Graph Embeddings


Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing
and the 9th International Joint Conference on Natural Language Processing, pages 378–388,
Hong Kong, China, November 3–7, 2019. c©2019 Association for Computational Linguistics

378

CaRe: Open Knowledge Graph Embeddings

Swapnil Gupta
Indian Institute of Science

Bangalore, India
swapnilgupta.229@gmail.com

Sreyash Kenkre
Walmart Labs

Bangalore, India
sreyash@gmail.com

Partha Talukdar
Indian Institute of Science

Bangalore, India
ppt@iisc.ac.in

Abstract
Open Information Extraction (OpenIE) meth-
ods are effective at extracting (noun phrase, re-
lation phrase, noun phrase) triples from text,
e.g., (Barack Obama, took birth in, Honolulu).
Organization of such triples in the form of
a graph with noun phrases (NPs) as nodes
and relation phrases (RPs) as edges results in
the construction of Open Knowledge Graphs
(OpenKGs). In order to use such OpenKGs
in downstream tasks, it is often desirable to
learn embeddings of the NPs and RPs present
in the graph. Even though several Knowl-
edge Graph (KG) embedding methods have
been recently proposed, all of those methods
have targeted Ontological KGs, as opposed
to OpenKGs. Straightforward application of
existing Ontological KG embedding methods
to OpenKGs is challenging, as unlike Onto-
logical KGs, OpenKGs are not canonicalized,
i.e., a real-world entity may be represented us-
ing multiple nodes in the OpenKG, with each
node corresponding to a different NP refer-
ring to the entity. For example, nodes with
labels Barack Obama, Obama, and President
Obama may refer to the same real-world en-
tity Barack Obama. Even though canonical-
ization of OpenKGs has received some at-
tention lately, output of such methods has
not been used to improve OpenKG embed-
dings. We fill this gap in the paper and
propose Canonicalization-infused Representa-
tions (CaRe) for OpenKGs. Through exten-
sive experiments, we observe that CaRe en-
ables existing models to adapt to the chal-
lenges in OpenKGs and achieve substantial
improvements for the link prediction task.

1 Introduction

Open Information Extraction (OpenIE) methods
such as ReVerb (Fader et al., 2011), OLLIE
(Mausam et al., 2012), BONIE (Saha et al., 2017)
and CALMIE (Saha and Mausam, 2018) can au-
tomatically extract (noun phrase, relation phrase,

Sentences in Text Corpus: 
• Barack Obama, 44th president of USA, took birth in Honolulu.
• Michelle Obama wife of Barack was born in Chicago.

OpenIE

OpenKG

Barack  
Obama

USA Michelle  
Obama

Chicago

44
th

 p
re

sid
en

t o
f

Honolulu

Took birth in

Barack

W
ife

 o
f

Was born in

First lady of

?   

Figure 1: Challenges in OpenKG. KG embedding
methods are effective in downstream tasks such as Link
Prediction. Existing KG embedding methods are in-
effective on OpenKGs as they are not canonicalized.
CaRe can effectively utilize canonicalization informa-
tion to learn better embeddings in OpenKGs. Green
dotted line represents the missing NP canonicalization
information. Please see Section 1 for more details.

noun phrase) triples from text, e.g., (Barack
Obama, took birth in, Honolulu), (Michelle
Obama, wife of, Barack), etc. An Open Knowl-
edge Graph (OpenKG) can be constructed out of
such triples by representing noun phrases (NPs) as
nodes and relation phrases (RPs) as edges connect-
ing them. Example of an OpenKG is shown in Fig-
ure 1. OpenKGs do not require pre-specified on-
tology, making them highly adaptable. In order to
use OpenKGs in downstream tasks such as Ques-
tion Answering, Document Classification, etc., it
is often necessary to learn embeddings of NPs and
RPs present as nodes and edges in an OpenKG.

Even though Knowledge Graph (KG) embed-



379

ding has been an active area of research (Bor-
des et al., 2013; Yang et al., 2014), all the pro-
posed KG embedding methods have focused on
embedding Ontological KGs, such as WikiData
(Vrandečić and Krötzsch, 2014), DBpedia (Auer
et al., 2007), YAGO (Suchanek et al., 2007),
NELL (Mitchell et al., 2018), and Freebase (Bol-
lacker et al., 2008). Existing KG embedding mod-
els train representation of each node and edge la-
bel based on the context of triples they are present
in. Doing this is suitable for ontological KGs as
they are canonicalized. However, in OpenKGs,
the same latent entity may be represented in dif-
ferent nodes labelled with different NPs. Simi-
larly, the same latent relation can be represented
with different RPs. For example, in Figure 1,
Barack Obama the entity is represented using two
NP nodes: Barack Obama and Barack. Similarly,
the two RPs – took birth in and was born in – re-
fer to the same underlying relation. Hence, the
paradigm of learning embeddings for each node
and edge label only from the context of the triples
they appear in is ineffective for OpenKGs.

A possible solution is to canonicalize the
OpenKGs. This involves identifying NPs and
RPs that refer to the same entity and relation,
and assigning them unique IDs. Nodes in the
OpenKG having the same ID are merged, leading
to a clean and canonicalized graph. Recent works
that automatically canonicalize OpenKGs, includ-
ing (Galárraga et al., 2014) and CESI (Vashishth
et al., 2018), pose canonicalization as a clustering
task of the NPs and RPs. However, due to auto-
matic generation, the output clusters often contain
some incorrectly canonicalized elements. Thus,
directly merging nodes or relations with the same
IDs would result in the propagation of errors in the
canonicalization step to down-stream tasks.

Our premise is that the output of these auto-
matic canonicalization models can be utilized to
improve OpenKG embedding. Instead of explic-
itly merging nodes with common IDs, KG em-
bedding models can be designed to judiciously
account for mistakes during the canonicalization
step. Towards establishing this premise, we pro-
pose a flexible OpenKG embedding approach to
integrate and utilize the output of a canonicaliza-
tion model in an error-conscious manner. Our con-
tributions are as follows:

• We draw attention to an important but rela-
tively unexplored problem of learning repre-

sentations for OpenKGs.

• We propose Canonicalization-infused Repre-
sentations (CaRe) for Open KGs - a novel ap-
proach to enrich OpenKG embedding models
with the output of a canonicalization model.
To the best of our knowledge, this is the first
model of its kind.

• Through extensive experiments on real-world
datasets, we establish CaRe’s effectiveness in
embedding OpenKGs.

CaRe source code is available at https://
github.com/malllabiisc/CaRE.

2 Related Work

OpenKG extraction and canonicalization: Mul-
tiple OpenIE systems have been developed over
the years. These include TextRunner (Yates
et al., 2007), (Angeli et al., 2015), ReVerb (Fader
et al., 2011), OLLIE (Mausam et al., 2012), SR-
LIE (Christensen et al., 2011), RelNoun (Pal and
Mausam, 2016) and ClauseIE (Del Corro and
Gemulla, 2013). A recent survey on the progress
in OpenIE systems is presented in (Mausam,
2016).

To perform automatic canonicalization of
OpenKGs, (Galárraga et al., 2014) first cluster
NPs over manually defined feature spaces. These
are then passed to AMIE (Galárraga et al., 2013)
for RP clustering. Whereas, CESI (Vashishth
et al., 2018) jointly learns vector representations of
NPs and RPs by infusing side information in KG
embedding models which are then used to cluster
NPs and RPs. In this work, we use CESI to gener-
ate canonicalization clusters for our datasets.
KG Embedding Methods: KG embedding meth-
ods aim to learn low dimensional vector represen-
tations for the nodes and edge labels encoding the
graph topology. All these methods train the em-
beddings by optimizing a link prediction based ob-
jective. They primarily differ in their way of math-
ematically modelling the likelihood of a triple be-
ing true. Translation-based hypothesis which re-
gards relation vector for any (subject, relation, ob-
ject) triple as a translation from the subject vec-
tor to the object vector is used in methods like
TransE (Bordes et al., 2013) and TransH (Wang
et al., 2014). Semantic matching models, such as
DistMult (Yang et al., 2014), ComplEx (Trouillon
et al., 2016) and HolE (Nickel et al., 2016), use

https://github.com/malllabiisc/CaRE
https://github.com/malllabiisc/CaRE


380

USA

United States of  
America

United States

An NP cluster from  
canonicalization  

modelIs the president of

United States 
of America

Washington, D.C.

North America USA

Is the capital of

Is in

Donald TrumpUnited States

Original OpenKG

(a)

United States

Is the president of

Donald TrumpUSANorth America

Is in

Washington, D.C.

United States  
of America

Is the capital of

Canonicalization Augmented OpenKG 
(Canonicalization edges represented as dotted lines)

(b)

Figure 2: CaRe Step 1: First CaRe augments the original OpenKG with the output of a canonicalization model.
(a) OpenKG and NP clusters from a canonicalization model. (b) Augmented OpenKG by adding undirected edges
between canonical NPs (represented as dotted lines). Please refer to Section 4.2 for more details.

similarity-based scoring functions to measure the
likelihood of a fact. Multi-Layer neural network
models, ConvE (Dettmers et al., 2018) and R-
GCN (Schlichtkrull et al., 2018) have shown better
expressive strength. R-GCN adapts graph convo-
lutional network (GCN) (Kipf and Welling, 2016)
to a relational graph proposing an auto-encoder
model for the link prediction task. Implicit in all
these approaches is the assumption that each node
in the graph is a different entity, and distinct edge
labels refer to distinct relations. This assumption
does not hold in OpenKGs. Existing KG embed-
dings are thus unsuitable for a task like link pre-
diction on OpenKGs. CaRe addresses these limi-
tations by infusing the output of a canonicalization
model with the KG embedding models.

3 Background

In this section, we present a quick overview of
a few basic methods which are useful for under-
standing the rest of the paper, especially the exper-
iments section. We first start with the notations.

Notations: OpenKG is denoted as G =
(N,R,T+), where N and R are the set of NPs and
RPs, respectively, and T+ = {(s, r, o)|s ∈ N, r ∈
R, o ∈ N} is the set of observed triples. Here s, o
are the subject and object phrase, respectively.
TransE: Given a triple (s, r, o), consider
es, rr, eo ∈ Rd as the d-dimensional vector rep-
resentations of subject (s), relation (r) and object
(o) respectively. TransE follows a translation
based triple scoring function ψ(.):

ψ(s, r, o) = −‖es + rr − eo‖p .

Here, ‖.‖ denotes norm and p is either 1 or
2. For training parameters, TransE uses margin-

based pairwise ranking loss with negative sam-
pling.
ConvE: ConvE (Dettmers et al., 2018) first re-
shapes es and rr to ēs and r̄r , respectively, and
passes them through a 2D convolution layer to
compute the score corresponding to a triple:

ψ(s, r, o) = f(vec(f([ēs; r̄r] ∗ w))W )eo.

Here, ∗ and w denote the convolution opera-
tor and convolution filters, f represents an ac-
tivation function and W , the weights of the fi-
nal linear layer. For training, ConvE uses binary
cross-entropy loss with correct samples consid-
ered as positive instances while negative instances
are generated through negative sampling.
Graph Neural Networks (GNN): GNNs were in-
troduced in (Gori et al., 2005) and (Scarselli et al.,
2009) as a generalization of recursive neural net-
works. Later, the generalization of CNN to graph-
structured data, popularly known as Graph Con-
volution Network (GCN), was proposed in (Bruna
et al., 2014). A first-order formulation of GCN
as proposed in (Kipf and Welling, 2016). Under
GCN formulation, the representation of a node n
after lth layer is defined as follows.

el+1n = f

 ∑
i∈N (n)

(
W leli + b

l
) , ∀n ∈ N (1)

Here, W l, bl are layer parameters, N (n) corre-
sponds to the immediate neighborhood of n and f
denotes an activation function.

GAT: GAT uses the attention mechanism to
determine the weights of a node’s neighbors
(Veličković et al., 2018). Further, it uses multi-
head attentions and defines the representation of



381

S es<latexit sha1_base64="Iv8ppuEMd3JY9AlltU5FT+jTP5o=">AAAB6nicbVBNS8NAEJ3Ur1q/qh69LBbBU0m0oMeiF48V7Qe0oWy2k3bpZhN2N0IJ/QlePCji1V/kzX/jts1BWx8MPN6bYWZekAiujet+O4W19Y3NreJ2aWd3b/+gfHjU0nGqGDZZLGLVCahGwSU2DTcCO4lCGgUC28H4dua3n1BpHstHM0nQj+hQ8pAzaqz0gH3dL1fcqjsHWSVeTiqQo9Evf/UGMUsjlIYJqnXXcxPjZ1QZzgROS71UY0LZmA6xa6mkEWo/m586JWdWGZAwVrakIXP190RGI60nUWA7I2pGetmbif953dSE137GZZIalGyxKEwFMTGZ/U0GXCEzYmIJZYrbWwkbUUWZsemUbAje8surpHVR9S6r3n2tUr/J4yjCCZzCOXhwBXW4gwY0gcEQnuEV3hzhvDjvzseiteDkM8fwB87nD1PCjdI=</latexit>

Canonical NPs for s as per 
Canonicalization model 

Oeo
<latexit sha1_base64="W4bNWa5sdYY2pmghkc5oDULuIyA=">AAAB6nicbVA9SwNBEJ2LXzF+RS1tFoNgFe5U0DJoYxnRmEByhL3NXLJkb/fY3RNCyE+wsVDE1l9k579xk1yhiQ8GHu/NMDMvSgU31ve/vcLK6tr6RnGztLW9s7tX3j94NCrTDBtMCaVbETUouMSG5VZgK9VIk0hgMxreTP3mE2rDlXywoxTDhPYljzmj1kn32FXdcsWv+jOQZRLkpAI56t3yV6enWJagtExQY9qBn9pwTLXlTOCk1MkMppQNaR/bjkqaoAnHs1Mn5MQpPRIr7UpaMlN/T4xpYswoiVxnQu3ALHpT8T+vndn4KhxzmWYWJZsvijNBrCLTv0mPa2RWjByhTHN3K2EDqimzLp2SCyFYfHmZPJ5Vg/NqcHdRqV3ncRThCI7hFAK4hBrcQh0awKAPz/AKb57wXrx372PeWvDymUP4A+/zB02yjc4=</latexit>

Canonical NPs for o as per 
Canonicalization model 

Triples Scoring Function

Triple Loss Function
T

Base Model (B)

e
0
s

<latexit sha1_base64="p8mI8iz+QRO2bJdS1FFoKqTg/Sk=">AAAB8HicbVBNS8NAEJ34WetX1aOXYBE9lUQFPRa9eKxgP6StZbOdtEt3N2F3I5SQX+HFgyJe/Tne/Ddu2xy09cHA470ZZuYFMWfaeN63s7S8srq2Xtgobm5t7+yW9vYbOkoUxTqNeKRaAdHImcS6YYZjK1ZIRMCxGYxuJn7zCZVmkbw34xi7ggwkCxklxkoP2Et19pieZL1S2at4U7iLxM9JGXLUeqWvTj+iiUBpKCdat30vNt2UKMMox6zYSTTGhI7IANuWSiJQd9PpwZl7bJW+G0bKljTuVP09kRKh9VgEtlMQM9Tz3kT8z2snJrzqpkzGiUFJZ4vChLsmciffu32mkBo+toRQxeytLh0SRaixGRVtCP78y4ukcVbxzyv+3UW5ep3HUYBDOIJT8OESqnALNagDBQHP8ApvjnJenHfnY9a65OQzB/AHzucP+MSQgw==</latexit>

e
0
o

<latexit sha1_base64="ELmZvNiAsFl8ScmLFG7iPjetQBM=">AAAB8HicbVDLSgNBEOz1GeMr6tHLYBA9hV0V9Bj04jGCeUgSw+xkNhkyj2VmVgjLfoUXD4p49XO8+TdOkj1oYkFDUdVNd1cYc2as7397S8srq2vrhY3i5tb2zm5pb79hVKIJrRPFlW6F2FDOJK1bZjltxZpiEXLaDEc3E7/5RLVhSt7bcUy7Ag8kixjB1kkPtJeq7DE9yXqlsl/xp0CLJMhJGXLUeqWvTl+RRFBpCcfGtAM/tt0Ua8sIp1mxkxgaYzLCA9p2VGJBTTedHpyhY6f0UaS0K2nRVP09kWJhzFiErlNgOzTz3kT8z2snNrrqpkzGiaWSzBZFCUdWocn3qM80JZaPHcFEM3crIkOsMbEuo6ILIZh/eZE0zirBeSW4uyhXr/M4CnAIR3AKAVxCFW6hBnUgIOAZXuHN096L9+59zFqXvHzmAP7A+/wB8qCQfw==</latexit>rr
<latexit sha1_base64="e08VsO9M3ITMlrHUiCajIwPKoj4=">AAAB7HicbVBNS8NAEJ3Ur1q/qh69LBbBU0lU0GPRi8cKpi20oWy2m3bpZhN2J0IJ/Q1ePCji1R/kzX/jts1BWx8MPN6bYWZemEph0HW/ndLa+sbmVnm7srO7t39QPTxqmSTTjPsskYnuhNRwKRT3UaDknVRzGoeSt8Px3cxvP3FtRKIecZLyIKZDJSLBKFrJ1/1cT/vVmlt35yCrxCtIDQo0+9Wv3iBhWcwVMkmN6XpuikFONQom+bTSywxPKRvTIe9aqmjMTZDPj52SM6sMSJRoWwrJXP09kdPYmEkc2s6Y4sgsezPxP6+bYXQT5EKlGXLFFouiTBJMyOxzMhCaM5QTSyjTwt5K2IhqytDmU7EheMsvr5LWRd27rHsPV7XGbRFHGU7gFM7Bg2towD00wQcGAp7hFd4c5bw4787HorXkFDPH8AfO5w8q5o7q</latexit>

Input Triple: ( s , r , o)

Relation Phrase (r)
…

Canonical Cluster 
Encoder Network (CN)Phrase Encoder 

Network (PN) 

Canonical Cluster 
Encoder Network (CN)

w1
<latexit sha1_base64="mx/VFezHqvMY4WbFRc+kdsKvuk4=">AAAB6nicbVBNS8NAEJ3Ur1q/qh69LBbBU0lU0GPRi8eK9gPaUDbbTbt0swm7E6WE/gQvHhTx6i/y5r9x2+agrQ8GHu/NMDMvSKQw6LrfTmFldW19o7hZ2tre2d0r7x80TZxqxhsslrFuB9RwKRRvoEDJ24nmNAokbwWjm6nfeuTaiFg94DjhfkQHSoSCUbTS/VPP65UrbtWdgSwTLycVyFHvlb+6/ZilEVfIJDWm47kJ+hnVKJjkk1I3NTyhbEQHvGOpohE3fjY7dUJOrNInYaxtKSQz9fdERiNjxlFgOyOKQ7PoTcX/vE6K4ZWfCZWkyBWbLwpTSTAm079JX2jOUI4toUwLeythQ6opQ5tOyYbgLb68TJpnVe+86t1dVGrXeRxFOIJjOAUPLqEGt1CHBjAYwDO8wpsjnRfn3fmYtxacfOYQ/sD5/AELJo2i</latexit>

w2
<latexit sha1_base64="qmpmSMS1f2q0FkcTJm+uMnFv//o=">AAAB6nicbVBNS8NAEJ34WetX1aOXxSJ4KkkV9Fj04rGi/YA2lM120y7dbMLuRCmhP8GLB0W8+ou8+W/ctjlo64OBx3szzMwLEikMuu63s7K6tr6xWdgqbu/s7u2XDg6bJk414w0Wy1i3A2q4FIo3UKDk7URzGgWSt4LRzdRvPXJtRKwecJxwP6IDJULBKFrp/qlX7ZXKbsWdgSwTLydlyFHvlb66/ZilEVfIJDWm47kJ+hnVKJjkk2I3NTyhbEQHvGOpohE3fjY7dUJOrdInYaxtKSQz9fdERiNjxlFgOyOKQ7PoTcX/vE6K4ZWfCZWkyBWbLwpTSTAm079JX2jOUI4toUwLeythQ6opQ5tO0YbgLb68TJrVinde8e4uyrXrPI4CHMMJnIEHl1CDW6hDAxgM4Ble4c2Rzovz7nzMW1ecfOYI/sD5/AEMqo2j</latexit>

wT
<latexit sha1_base64="VmJPJFkY7hBPN74Ldn84MVnzqoE=">AAAB6nicbVBNS8NAEJ34WetX1aOXxSJ4KokKeix68VixX9CGstlu2qWbTdidKCX0J3jxoIhXf5E3/43bNgdtfTDweG+GmXlBIoVB1/12VlbX1jc2C1vF7Z3dvf3SwWHTxKlmvMFiGet2QA2XQvEGCpS8nWhOo0DyVjC6nfqtR66NiFUdxwn3IzpQIhSMopUennr1XqnsVtwZyDLxclKGHLVe6avbj1kacYVMUmM6npugn1GNgkk+KXZTwxPKRnTAO5YqGnHjZ7NTJ+TUKn0SxtqWQjJTf09kNDJmHAW2M6I4NIveVPzP66QYXvuZUEmKXLH5ojCVBGMy/Zv0heYM5dgSyrSwtxI2pJoytOkUbQje4svLpHle8S4q3v1luXqTx1GAYziBM/DgCqpwBzVoAIMBPMMrvDnSeXHenY9564qTzxzBHzifP0AyjcU=</latexit>

Figure 3: CaRe Step 2: In this step, CaRe learns KG embeddings from the augmented OpenKG (Figure 2). Base
model can be any existing KG embedding model (e.g., TransE, ConvE). RP embeddings are parameterized by
encoding vector representations of the word sequence composing them. This enables CaRe to capture semantic
similarity of RPs. Embeddings of NPs are made more context rich by updating them with the represenations of
canonical NPs (connected with dotted lines). Please see Section 3 and 4 for details.

nth the node as follows

el+1n = ||Kk=1f

 ∑
i∈N (n)

(
αk(e

l
n, e

l
i)W

l
ke

l
i

) .
(2)

Here, α(.) denotes attention weights and || de-
notes concatenation.

4 CaRe: Proposed Method

4.1 Overview
CaRe consists of two steps. We first give an
overview of the two steps below and then provide
a detailed description of each step.

• Step 1: In this step, the given OpenKG is
augmented by adding edges from a canoni-
calization model. This step is outlined in Fig-
ure 2 and described in detail in Section 4.2.

• Step 2: In this step, embeddings of nodes
and edges present in the augmented OpenKG
obtained in Step 1 are learned. This step is
outlined in Figure 3. The system consists of
three components: Base Model, Phrase En-
coder Network, and Canonical Cluster En-
coder Network. We describe these compo-
nents in Section 4.3, Section 4.4, and Sec-
tion 4.5, respectively.

CaRe architecture and its components are de-
picted in Figure 3.

4.2 Step 1: Canonicalization Augmented
OpenKG

As discussed in Section 1, based on the automatic
clusters generated by a canonicalization model,
merging NPs with a single ID can lead to the prop-
agation of errors in the graph. Hence, we propose
a soft integration scheme by adding an unlabeled
and undirected edge between any two NPs which
are canonical as per the canonicalization model
as shown in Figure 2. More formally, suppose
n1, n2 are two NPs that have been identified to be
the same in the canonicalization step. We add an
undirected and unlabelled edge (n1, n2) between
the nodes n1 and n2, and call them canonicaliza-
tion edges. If the canonicalization step produces
a confidence score for n1 and n2 being the same,
then this score can be incorporated as a weight on
the canonicalization edge. If no such score is pro-
duced, then the edges are kept unweighted. We
then collect all these canonicalization edges into
the set C. Finally, these edges are added to the
original OpenKGG to get a canonicalization aug-
mented OpenKG, G′ = (N,R,T+ ∪ C)



382

4.3 Step 2: Base Model (B)
This component decides the way parameters are
trained in CaRe based on the relational edges T+.
While CaRe is flexible to accommodate any KG
embedding model as the base model, for the ex-
periments in paper, we used TransE and ConvE
(please see Section 3).

4.4 Step 2: Phrase Encoder Network (PN)
We generate embeddings of RPs by encoding
the vector representations of the words compos-
ing them. Consider an RP as a sequence of T
words (w1, w2, ..., wT ) and their corresponding
word vectors as (x1, x2, ..., xT ). They are passed
through a Phrase Encoder Network module (Fig-
ure 3) for which we use a bidirectional GRU (Cho
et al., 2014) model with last pooling as described
below.

(
−→
h1,
−→
h2, ...,

−→
hT ) =

−−−→
GRU(x1, x2, ..., xT )

(
←−
h1,
←−
h2, ...,

←−
hT ) =

←−−−
GRU(x1, x2, ..., xT ).

Finally, rr = [
−→
hT :

←−
h1] is concatenation of fi-

nal hidden states in both the directions. This ap-
proach allows parameter sharing across RPs with
word overlaps while leveraging the rich semantic
information from pre-trained word embeddings.

4.5 Step 2: Canonical Cluster Encoder
Network (CN)

The canonicalization information is incorpo-
rated in the NP embeddings by utilizing the
canonicalization-induced edges C.

Each NP n ∈ N is assigned a vector en. As NP
canonicalization is expressed as a clustering step,
for each NP, its canonical NPs are a single edge
away in C (see Figure 3). Hence, a single layer
of network aggregation is sufficient. We propose
a non-parametric message passing and update net-
work which works in the following two steps:
Context vector: First for each NP, n ∈ N a con-
text vector ecn is generated by the following mes-
sage passing scheme from its canonical neighbors.

ecn =

 ∑
i∈N (n)

1

|N (n)|
ei

 ,∀n ∈ N.
Here,N (n) = {i|i ∈ N, (n, i) ∈ C}, is the canon-
ical neighborhood of n.
Updating NP embeddings: The updated embed-
dings for each NP is computed as below:

e
′
n =

en
2

+
ecn
2

which are passed to the decoding stage. For
the case of weighted canonicalization edges, the
weights can be used while generating the context
vector of an NP, by making the contribution of its
canonical NPs proportional to the edge weights
joining them. The proposed approach can be in-
terpreted as a local embedding smoothening of
canonical NPs and we call this network as Lo-
cal Averaging Network (LAN). We also exper-
imented with more sophisticated attention-based
context vector generation and gating mechanism
for embedding update stage, but they resulted in a
performance drop.

Note that, our approach to embed NPs is in-
spired by the auto-encoder framework used in R-
GCN (Schlichtkrull et al., 2018) but with a key
difference. In R-GCN, the same set of edges are
utilized at both the encoding and decoding stages.
Whereas in CaRe, the canonicalization edges C
are used at the encoder (Canonical Cluster En-
coder Network) while the decoder (Base Model)
operates on the relational edges T+.
CaRe nomenclature: We introduce a generic
nomenclature for the possible variants of the CaRe
framework based on the choice of the Base Model
(B), Phrase Encoder Network (PN) and Canon-
ical Cluster Encoder Network (CN) as CaRe(B,
PN, CN). For e.g., CaRe(B=ConvE, PN=Bi-GRU,
CN=LAN) corresponds to a CaRe model with
ConvE as base model, Bi-GRU network with
last pooling (Section 4.4) as the Phrase Encoder
Network and Local Averaging Network (Sec-
tion 4.5) as the Canonical Cluster Encoder Net-
work. We define Bi-GRU and LAN as default val-
ues for the PN and CN arguments respectively. In
case, values for these arguments are not explic-
itly mentioned, they take the default values. Thus,
CaRe(B=ConvE) represents the same network as
CaRe(B=ConvE, PN=Bi-GRU, CN=LAN).

5 Experiments

5.1 Datasets

Statistics of the two datasets used in the experi-
ments of this paper are summarized in Table 1. In
order to build these datasets, we first obtained the
three graph datasets – Base, Ambiguous and Re-
Verb45K. While Base and Ambiguous datasets are
created in (Galárraga et al., 2014), ReVerb45K is



383

Datasets ReVerb45K ReVerb20K

# NPs 27K 11.1K
# RPs 21.6K 11.1K

# Gold NP Clusters 18.6K 10.8K
# Train Triples 36K 15.5K
# Test Triples 5.4K 2.4K

# Validation Triples 3.6K 1.6K

Table 1: Details of datasets used. Please see Section 5.1
for details.

introduced in (Vashishth et al., 2018). ReVerb20K
is created by combining the two smaller datasets
Base and Ambiguous. All the datasets are con-
structed through ReVerb Open KB (Fader et al.,
2011). We refer the readers to the respective pa-
pers for the construction details.

To generate the train, validation and test triples
the following procedure is adopted. First, the en-
tire set of triples is divided in 80 : 20 ratio en-
suring that each NP and RP in the triples of the
smaller set is at least present once in the triples
of the bigger set. This consideration is essential
because of the transductive nature of existing KG
embedding models. The bigger set is considered
as the train dataset and the smaller set is further
randomly divided in 30 : 70 to get the valida-
tion and test datasets respectively. Both datasets
contain gold canonicalization clusters for the NPs
extracted through the Freebase entity linking in-
formation (Gabrilovich et al., 2013) which enables
automatic evaluation in the canonicalization task.

5.2 Open KG Link Prediction Evaluation

In the typical link prediction evaluation, an unseen
triple (s, r, o) is taken and partial triples (s, r, ?)
and (?, r, o) are shown to the model. It ranks all
the entities in the graph for their likelihood to be
the missing entity and the rank assigned to the true
missing entity is considered.

However, while this is suitable for ontological
KGs, it is not valid for our setting. In OpenKGs,
instead of entities, NPs are present and several of
them can refer to the same entity. This means that,
even when predicting correct entity, a model will
be unfairly penalized if the prediction is a different
canonical form of the entity than the one present
in the considered triple. We, therefore propose to
rank gold NP clusters, available in the dataset, in-
stead of ranking each NP. We do this in the follow-
ing manner:

• List all the NPs in decreasing order of their
likelihood to be the missing part of the triple.

• Prune this list by keeping the best ranked NPs
for each gold cluster. This gives us the ranked
list of gold clusters.

• Consider the rank of the cluster to which the
true missing NP belongs.

We provide results using three commonly used
evaluation metrics: mean rank (MR), mean re-
ciprocal rank (MRR) and Hits@n with n =
{10, 30, 50}. Filtered setting as introduced in
(Bordes et al., 2013) is followed.

5.3 Experimental Setup

We run CESI on both the datasets to generate the
NP canonical clusters. Note that, none of the ex-
isting automatic canonicalization models output
edge weights. Hence, for all the experiments,
the canonicalization-induced edges are kept un-
weighted. For Phrase Encoder Network, we found
single layer in the Bi-GRU model works best.
CaRe allows the use of any pre-trained embed-
dings. However, (Vashishth et al., 2018) demon-
strates the effectiveness of pre-trained GloVE
(Pennington et al., 2014) vectors for initializing
representations for the same datasets. Hence, the
word vectors are initialized with 300-dimensional
pre-trained GloVE embeddings and are kept train-
able. We use PyTorch Geometric library (Fey and
Lenssen, 2019) for the Canonical Cluster Encoder
Network module.

The choice of optimizer and regularization
based hyper-parameters is directly adopted from
the ones proposed in the original work of the base
models. Both the NP and RP embedding size is
kept fixed at 300, while the learning rate is selected
through a grid search over {0.1,0.01,0.001}.

Models like ConvE introduce inverse relations.
For our experiments we generate inverse phrase
for an RP by adding a phrase “inverse of” to that
RP. In ComplEx the embeddings have both real
and imaginary parts. For this, we use two separate
Phrase Encoder Network and Graph Neural Net-
work modules for the real and imaginary parts of
RP and NP embeddings respectively.

6 Results

In this section we evaluate the following ques-
tions:



384

Method ReVerb45K ReVerb20K

MR MRR Hits@10 Hits@30 Hits@50 MR MRR Hits@10 Hits@30 Hits@50

TransE 2955.8 .193 .361 .446 .478 1425.8 .126 .299 .411 .468
TransH 2998.2 .194 .362 .442 .478 1464.4 .129 .303 .409 .467
DistMult 8988.8 .051 .051 .052 .065 6260.0 .033 .044 .055 .060
ComlEx 7786.5 .047 .047 .048 .073 5502.2 .037 .058 .075 .085
R-GCN 2866.8 .042 .046 .091 .113 1204.3 .122 .187 .263 .305
ConvE 2650.8 .233 .338 .401 .429 1014.5 .294 .402 .491 .541

CaRe(B=ConvE) 1308.0 .324 .456 .543 .579 973.2 .318 .439 .525 .566

Table 2: Link Prediction results. CaRe(B=ConvE) substantially outperforms all the existing KG embedding
models. For all the experiments, evaluation strategy described in Section 5.2 is followed. B, CN, PN are the
arguments of CaRe framework (Section 4 and Figure 3). For the reported results: PN=Bi-GRU and CN=LAN.

Method ReVerb45K ReVerb20K

MR MRR Hits@10 Hits@30 Hits@50 MR MRR Hits@10 Hits@30 Hits@50

TransE 2955.8 .193 .361 .446 .478 1425.8 .126 .299 .411 .468
CaRe(B=TransE, CN=φ) 2522.3 .195 .378 .457 .488 978.2 .286 .411 .515 .565

ComplEx 7786.5 .047 .047 .048 .073 5502.2 .037 .058 .075 .085
CaRe(B=ComplEx, CN=φ) 5205.4 .185 .225 .235 .325 3010.0 .216 .288 .345 .376

R-GCN 2866.8 .042 .046 .091 .113 1204.3 .122 .187 .263 .305
CaRe(B=R-GCN, CN=φ) 2508.1 .145 .203 .260 .305 1210.3 .195 .275 .340 .370

ConvE 2650.8 .233 .338 .401 .429 1014.5 .294 .402 .491 .541
CaRe(B=ConvE, CN=φ) 1656.1 .293 .401 .477 .509 966.9 .307 .419 .514 .556

Table 3: Impact of parameterizing RP embeddings. B, CN, PN are the arguments of CaRe framework (Section 4
and Figure 3). φ value for CN argument implies that Canonical Cluster Encoder Network module is not used in
these experiments. PN=Bi-GRU in all these experiments. Please refer to Section 6.2.

Q1. Is CaRe effective for the link prediction task
in OpenKGs? (Section 6.1)

Q2. What is the quantitave and qualitative impact
of parameterizing RP embeddings in CaRe?
(Section 6.2 and Section 6.4)

Q3. How does the local embedding smoothening
approach adopted in CaRe compare against
other competitive baselines? (Section 6.3)

6.1 Overall Performance
As shown in Figure 3, any existing KG embed-
ding model can be used as the base model in
the CaRe framework. We experimented with sev-
eral KG embedding models and found the CaRe
achieved substantial improvements in comparison
to standalone use of the KG embedding model
in each case. We achieved overall best perfor-
mance working with ConvE as the base model,
CaRe(B=ConvE). The experimental results are
presented in Table 2.

Table 1 shows that in both the datasets, on an
average, the number of train triples for each NP

and RP is less than 2. In contrast, FB15k (Bordes
et al., 2013), an ontological KG, has on an average
32 triples for each entity and 360 triples for each
relation. This highlights the extremely sparse and
fragmented nature of OpenKGs. Hence, the su-
perior performance of CaRe supports the hypoth-
esis that the flow of information while learning
the representations of canonical NPs and RPs in
OpenKGs is beneficial.

Also, a comparison of the number of unique
NPs and gold NP clusters for the two datasets (Ta-
ble 1) shows that the number of NPs canonical to
each other is more significant in ReVerb45K than
in ReVerb20K. Hence, a more prominent improve-
ment due to CaRe in ReVerb45K as compared to
ReVerb20K proves the effectiveness of CaRe in
utilizing the canonicalization information.

6.2 Impact of parameterizing RP
embeddings

As described in Section 4.4, a Phrase Encoder
Network is used to parametrize RP embeddings,
which allows parameter sharing while learning
embeddings. Table 3 provides a quantitative anal-



385

(a) ReVerb45K (b) ReVerb20K

Figure 4: Performance comparison of CaRe framework for differnt values of the CN (as described in Section 6.3).
B=ConvE (left group) and B=TransE (right group) in both the plots. PN=Bi-GRU. Mean Reciprocal Rank (MRR)
is plotted on the y axis (higher is better).

(a) ConvE (b) CaRe(B=ConvE)

Figure 5: t-SNE visualization of RP embeddings. RP embeddings learned by CaRe(B=ConvE) are able to
capture the semantic similarity of the RPs whereas in ConvE this information is lost. Please refer to Section 6.4.

ysis of the impact of parameterizing RP embed-
dings across several KG embedding models. In
these experiments, the CN argument of CaReis
kept φ, which implies no canonicalization infor-
mation is used. The NP embeddings are trained
in the same manner as the base KG embedding
model.

Comparing the performance of
CaRe(B=ConvE,CN=φ) in Table 3 with the
performance of CaRe(B=ConvE) in Table 2,
it can be noticed that a major part of the per-
formance boost of CaRe can be attributed to
parameterization of RP embeddings.

6.3 Different ways to utilize Canonicalization
edges

There can be several methods through which the
canonicalization edges integrated into the graph
(as described in Figure 2), can be utilized in the

model. In Section 4.5, we described a local em-
bedding smoothening method adopted in CaRe.
In this section, we present a comparative analysis
with the following competitive baselines:

• CaRe(CN=GCN): A single layer of GCN
(Kipf and Welling, 2016) for the Canonical
Cluster Encoder Network. Refer to Equa-
tion 1.

• CaRe(CN=GAT): A single layer of GAT
(Veličković et al., 2018) for the Canonical
Cluster Encoder Network. Refer to Equa-
tion 2.

• CaRe(CN=edge): Here the canonicalization
edges in G

′
are labelled with a symmetric RP

“is canonical to”. This adds a new edge type
in the graph and is treated by KG embedding
models like any other edge type.



386

We show these comparisons using both TransE
and ConvE as base models in Figure 4. The
GCN and GAT architectures have an adverse ef-
fect on the performance. We believe this can
be attributed to the complex nature of these ar-
chitectures in an already over-parameterized and
noisy setting. In CaRe(CN=edge), the model is
expected to learn the meaning of two NPs being
canonical and encode it in the vector embedding
of the new edge type. The local averaging net-
work CaRe(CN=LAN) used in CaRe is a GNN
architecture with fixed weights. The choice of
weights follows from the prior belief that canon-
ical NP embeddings should be close in the vector
space, inducing a useful inductive bias. The above
results indicate that this leads to better perfor-
mance. Additionally, unlike the GNN based meth-
ods, CaRe(CN=edge) has a limitation in its mod-
elling capacity as it provides no way to handle the
case where canonicalization edges are weighted.

6.4 Qualitative Analysis

Figure 5 demonstrates a qualitative compari-
son of RP embeddings between ConvE and
CaRe(B=ConvE). For this experiment, we se-
lected seven RPs, and for each RP, through human
judgement, we selected two more RPs with similar
meaning. Thus, there are seven different clusters.
The figure shows t-SNE (van der Maaten and Hin-
ton, 2008) visualization of the embeddings learnt
for these RPs by CaRe(B=ConvE) and ConvE. t-
SNE is a non-linear transformation which tries to
map points in high dimensional space to a lower
dimension preserving the local relationships be-
tween the points. The figure verifies the hypoth-
esis that due to the parameterization of RP embed-
dings and utilizing pre-trained word embeddings,
CaRe is able to better capture the semantic similar-
ity of the RPs in comparison to the base models.
Due to the explicit integration of NP canonicaliza-
tion in CaRe, we observed a similar desirable im-
pact on the embeddings learned for NPs as well.

7 Conclusion

Open Information Extraction (OpenIE) provides
an effective way to bootstrap Open Knowledge
Graphs (OpenKGs) from text corpus. OpenKGs
consist of noun phrases (NPs) as nodes and re-
lations phrases (RPs) as edges. In spite of
this advantage, OpenKGs are often sparse and
non-canonicalized, i.e., the same entity could

be expressed using multiple nodes in the graph
(and similarly for relations). This renders exist-
ing Ontological KG embedding methods ineffec-
tive in learning embeddings of NPs and RPs in
OpenKGs. In spite of this limitation, there has
been no prior work which has focused on OpenKG
embedding. We fill this gap in the paper and
propose CaRe. CaRe infuses canonicalization in-
formation combined with the neighborhood graph
structure to learn rich representations of NPs. Fur-
ther, it captures the semantic similarity of RPs by
utilizing the word sequence information in these
relation phrases to parameterize the RP embed-
dings. Through extensive experiments on real-
world datasets, we demonstrate the effectiveness
of embeddings learned by CaRe.

As part of future work, we hope to extend CaRe
to also utilize RP canonicalization information.
Utilizing OpenKG embeddings in tasks beyond
link prediction is another avenue of further work.

Acknowledgments

We thank the anonymous reviewers for their
constructive comments. This work is supported
by the Ministry of Human Resource Development
(MHRD), Government of India. Finally, we thank
all the members of MALL Lab, IISc for their
invaluable suggestions.

References
Gabor Angeli, Melvin Jose Johnson Premkumar, and

Christopher D. Manning. 2015. Leveraging linguis-
tic structure for open domain information extraction.
In Proceedings of the 53rd Annual Meeting of the
Association for Computational Linguistics and the
7th International Joint Conference on Natural Lan-
guage Processing (Volume 1: Long Papers), pages
344–354, Beijing, China. Association for Computa-
tional Linguistics.

Sören Auer, Christian Bizer, Georgi Kobilarov, Jens
Lehmann, Richard Cyganiak, and Zachary Ives.
2007. Dbpedia: A nucleus for a web of open data.
In The Semantic Web, pages 722–735, Berlin, Hei-
delberg. Springer Berlin Heidelberg.

Kurt Bollacker, Colin Evans, Praveen Paritosh, Tim
Sturge, and Jamie Taylor. 2008. Freebase: A col-
laboratively created graph database for structuring
human knowledge. In Proceedings of the 2008
ACM SIGMOD International Conference on Man-
agement of Data, SIGMOD ’08, pages 1247–1250,
New York, NY, USA. ACM.

https://doi.org/10.3115/v1/P15-1034
https://doi.org/10.3115/v1/P15-1034
https://doi.org/10.1145/1376616.1376746
https://doi.org/10.1145/1376616.1376746
https://doi.org/10.1145/1376616.1376746


387

Antoine Bordes, Nicolas Usunier, Alberto Garcia-
Durán, Jason Weston, and Oksana Yakhnenko.
2013. Translating embeddings for modeling multi-
relational data. In Proceedings of the 26th Interna-
tional Conference on Neural Information Process-
ing Systems - Volume 2, NIPS’13, pages 2787–2795,
USA. Curran Associates Inc.

Joan Bruna, Wojciech Zaremba, Arthur Szlam, and
Yann Lecun. 2014. Spectral networks and lo-
cally connected networks on graphs. In Inter-
national Conference on Learning Representations
(ICLR2014), CBLS, April 2014.

Kyunghyun Cho, Bart van Merrienboer, Caglar Gul-
cehre, Dzmitry Bahdanau, Fethi Bougares, Holger
Schwenk, and Yoshua Bengio. 2014. Learning
phrase representations using RNN encoder–decoder
for statistical machine translation. In Proceedings of
the 2014 Conference on Empirical Methods in Nat-
ural Language Processing (EMNLP), pages 1724–
1734, Doha, Qatar. Association for Computational
Linguistics.

Janara Christensen, Mausam, Stephen Soderland, and
Oren Etzioni. 2011. An analysis of open informa-
tion extraction based on semantic role labeling. In
Proceedings of the Sixth International Conference
on Knowledge Capture, K-CAP ’11, pages 113–120,
New York, NY, USA. ACM.

Luciano Del Corro and Rainer Gemulla. 2013. Clausie:
Clause-based open information extraction. In Pro-
ceedings of the 22Nd International Conference on
World Wide Web, WWW ’13, pages 355–366, New
York, NY, USA. ACM.

Tim Dettmers, Minervini Pasquale, Stenetorp Pon-
tus, and Sebastian Riedel. 2018. Convolutional 2d
knowledge graph embeddings. In Proceedings of
the 32th AAAI Conference on Artificial Intelligence,
pages 1811–1818.

Anthony Fader, Stephen Soderland, and Oren Etzioni.
2011. Identifying relations for open information ex-
traction. In Proceedings of the 2011 Conference on
Empirical Methods in Natural Language Process-
ing, pages 1535–1545, Edinburgh, Scotland, UK.
Association for Computational Linguistics.

Matthias Fey and Jan E. Lenssen. 2019. Fast graph
representation learning with PyTorch Geometric.
In ICLR Workshop on Representation Learning on
Graphs and Manifolds.

Evgeniy Gabrilovich, Michael Ringgaard, and Amar-
nag Subramanya. 2013. Facc1: Freebase an-
notation of clueweb corpora, version 1 (re-
lease date 2013-06-26, format version 1, cor-
rection level 0). Note: http://lemurproject.
org/clueweb09/FACC1/Cited by, 5.

Luis Galárraga, Geremy Heitz, Kevin Murphy, and
Fabian M Suchanek. 2014. Canonicalizing open
knowledge bases. In Proceedings of the 23rd acm

international conference on conference on informa-
tion and knowledge management, pages 1679–1688.
ACM.

Luis Antonio Galárraga, Christina Teflioudi, Katja
Hose, and Fabian Suchanek. 2013. Amie: Associ-
ation rule mining under incomplete evidence in on-
tological knowledge bases. In Proceedings of the
22Nd International Conference on World Wide Web,
WWW ’13, pages 413–422, New York, NY, USA.
ACM.

Marco Gori, Gabriele Monfardini, and Franco
Scarselli. 2005. A new model for learning in graph
domains. In Proceedings. 2005 IEEE International
Joint Conference on Neural Networks, 2005., vol-
ume 2, pages 729–734. IEEE.

Thomas N Kipf and Max Welling. 2016. Semi-
supervised classification with graph convolutional
networks. arXiv preprint arXiv:1609.02907.

Laurens van der Maaten and Geoffrey Hinton. 2008.
Visualizing data using t-SNE. Journal of Machine
Learning Research, 9:2579–2605.

Mausam, Michael Schmitz, Stephen Soderland, Robert
Bart, and Oren Etzioni. 2012. Open language learn-
ing for information extraction. In Proceedings of
the 2012 Joint Conference on Empirical Methods
in Natural Language Processing and Computational
Natural Language Learning, pages 523–534, Jeju
Island, Korea. Association for Computational Lin-
guistics.

Mausam Mausam. 2016. Open information extraction
systems and downstream applications. In Proceed-
ings of the Twenty-Fifth International Joint Con-
ference on Artificial Intelligence, pages 4074–4077.
AAAI Press.

T. Mitchell, W. Cohen, E. Hruschka, P. Talukdar,
B. Yang, J. Betteridge, A. Carlson, B. Dalvi,
M. Gardner, B. Kisiel, J. Krishnamurthy, N. Lao,
K. Mazaitis, T. Mohamed, N. Nakashole, E. Pla-
tanios, A. Ritter, M. Samadi, B. Settles, R. Wang,
D. Wijaya, A. Gupta, X. Chen, A. Saparov,
M. Greaves, and J. Welling. 2018. Never-ending
learning. Commun. ACM, 61(5):103–115.

Maximilian Nickel, Lorenzo Rosasco, and Tomaso
Poggio. 2016. Holographic embeddings of knowl-
edge graphs. In Proceedings of the Thirtieth AAAI
Conference on Artificial Intelligence, AAAI’16,
pages 1955–1961. AAAI Press.

Harinder Pal and Mausam. 2016. Demonyms and com-
pound relational nouns in nominal open IE. In Pro-
ceedings of the 5th Workshop on Automated Knowl-
edge Base Construction, pages 35–39, San Diego,
CA. Association for Computational Linguistics.

Jeffrey Pennington, Richard Socher, and Christopher
Manning. 2014. Glove: Global vectors for word
representation. In Proceedings of the 2014 Con-
ference on Empirical Methods in Natural Language

http://dl.acm.org/citation.cfm?id=2999792.2999923
http://dl.acm.org/citation.cfm?id=2999792.2999923
https://doi.org/10.3115/v1/D14-1179
https://doi.org/10.3115/v1/D14-1179
https://doi.org/10.3115/v1/D14-1179
https://doi.org/10.1145/1999676.1999697
https://doi.org/10.1145/1999676.1999697
https://doi.org/10.1145/2488388.2488420
https://doi.org/10.1145/2488388.2488420
https://arxiv.org/abs/1707.01476
https://arxiv.org/abs/1707.01476
https://www.aclweb.org/anthology/D11-1142
https://www.aclweb.org/anthology/D11-1142
https://doi.org/10.1145/2488388.2488425
https://doi.org/10.1145/2488388.2488425
https://doi.org/10.1145/2488388.2488425
http://www.jmlr.org/papers/v9/vandermaaten08a.html
https://www.aclweb.org/anthology/D12-1048
https://www.aclweb.org/anthology/D12-1048
https://doi.org/10.1145/3191513
https://doi.org/10.1145/3191513
http://dl.acm.org/citation.cfm?id=3016100.3016172
http://dl.acm.org/citation.cfm?id=3016100.3016172
https://doi.org/10.18653/v1/W16-1307
https://doi.org/10.18653/v1/W16-1307
https://doi.org/10.3115/v1/D14-1162
https://doi.org/10.3115/v1/D14-1162


388

Processing (EMNLP), pages 1532–1543, Doha,
Qatar. Association for Computational Linguistics.

Swarnadeep Saha and Mausam. 2018. Open informa-
tion extraction from conjunctive sentences. In Pro-
ceedings of the 27th International Conference on
Computational Linguistics, pages 2288–2299, Santa
Fe, New Mexico, USA. Association for Computa-
tional Linguistics.

Swarnadeep Saha, Harinder Pal, and Mausam. 2017.
Bootstrapping for numerical open IE. In Proceed-
ings of the 55th Annual Meeting of the Association
for Computational Linguistics (Volume 2: Short Pa-
pers), pages 317–323, Vancouver, Canada. Associa-
tion for Computational Linguistics.

Franco Scarselli, Marco Gori, Ah Chung Tsoi, Markus
Hagenbuchner, and Gabriele Monfardini. 2009. The
graph neural network model. IEEE Transactions on
Neural Networks, 20(1):61–80.

Michael Sejr Schlichtkrull, Thomas N. Kipf, Peter
Bloem, Rianne van den Berg, Ivan Titov, and Max
Welling. 2018. Modeling relational data with graph
convolutional networks. In ESWC, volume 10843 of
Lecture Notes in Computer Science, pages 593–607.
Springer.

Fabian M Suchanek, Gjergji Kasneci, and Gerhard
Weikum. 2007. Yago: a core of semantic knowl-
edge. In Proceedings of the 16th international con-
ference on World Wide Web, pages 697–706. ACM.

Théo Trouillon, Johannes Welbl, Sebastian Riedel, Éric
Gaussier, and Guillaume Bouchard. 2016. Com-
plex embeddings for simple link prediction. In In-
ternational Conference on Machine Learning, pages
2071–2080.

Shikhar Vashishth, Prince Jain, and Partha Talukdar.
2018. Cesi: Canonicalizing open knowledge bases
using embeddings and side information. In Pro-
ceedings of the 2018 World Wide Web Conference
on World Wide Web, pages 1317–1327. International
World Wide Web Conferences Steering Committee.

Petar Veličković, Guillem Cucurull, Arantxa Casanova,
Adriana Romero, Pietro Liò, and Yoshua Bengio.
2018. Graph Attention Networks. International
Conference on Learning Representations. Accepted
as poster.

Denny Vrandečić and Markus Krötzsch. 2014. Wiki-
data: A free collaborative knowledgebase. Com-
mun. ACM, 57(10):78–85.

Zhen Wang, Jianwen Zhang, Jianlin Feng, and Zheng
Chen. 2014. Knowledge graph embedding by trans-
lating on hyperplanes. In Twenty-Eighth AAAI con-
ference on artificial intelligence.

Bishan Yang, Wen-tau Yih, Xiaodong He, Jianfeng
Gao, and Li Deng. 2014. Embedding entities and
relations for learning and inference in knowledge
bases. arXiv preprint arXiv:1412.6575.

Alexander Yates, Michael Cafarella, Michele Banko,
Oren Etzioni, Matthew Broadhead, and Stephen
Soderland. 2007. Textrunner: open information
extraction on the web. In Proceedings of Human
Language Technologies: The Annual Conference of
the North American Chapter of the Association for
Computational Linguistics: Demonstrations, pages
25–26. Association for Computational Linguistics.

https://www.aclweb.org/anthology/C18-1194
https://www.aclweb.org/anthology/C18-1194
https://doi.org/10.18653/v1/P17-2050
https://openreview.net/forum?id=rJXMpikCZ
https://doi.org/10.1145/2629489
https://doi.org/10.1145/2629489

