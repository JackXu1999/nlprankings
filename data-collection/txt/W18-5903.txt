




































Identifying Depression on Reddit: The Effect of Training Data


Proceedings of the 3rd Social Media Mining for Health Applications (SMM4H) Workshop & Shared Task, pages 9–12
Brussels, Belgium, October 31, 2018. c©2018 Association for Computational Linguistics

9

Identifying depression on Reddit: the effect of training data

Inna Pirina
Department of Linguistics
University of Tübingen

inna.pyrina@gmail.com

Çağrı Çöltekin
Department of Linguistics
University of Tübingen

ccoltekin@sfs.uni-tuebingen.de

Abstract
This paper presents a set of classification ex-
periments for identifying depression in posts
gathered from social media platforms. In ad-
dition to the data gathered previously by other
researchers, we collect additional data from
the social media platform Reddit. Our exper-
iments show promising results for identifying
depression from social media texts. More im-
portantly, however, we show that the choice of
corpora is crucial in identifying depression and
can lead to misleading conclusions in case of
poor choice of data.

1 Introduction

Clinical depression, also referred to as major de-
pressive disorder, is a serious mental condition
that can interfere with normal daily life activities.
One of the many risks of clinical depression is sui-
cide – research has indicated that approximately
two-thirds of people who die by suicide were deal-
ing with depression at the time of death (Richards
and O’Hara, 2014). Meanwhile, according to the
World Health Organization, nearly 50% of peo-
ple with clinical depression worldwide remain un-
treated. One of the main reasons why the disor-
der is ignored is believed to be under-diagnosis
(Sheenan, 2004).
One of the many ways that the condition could

be manifested is in the way people write: the
words they choose and the general tone of the pro-
duced text are affected by the disorder (Reece et al.,
2017). Due to the stigma around clinical depres-
sion, people tend to turn to the Internet, thus, mak-
ing the data gathered from social media a valuable
source of literary cues that could help identify de-
pression from texts. Moreover, the ease of obtain-
ing data makes Internet an attractive source for the
purpose.
Early research on the relation between language

and depression has beenmostly theoretical, mainly

focusing on the linguistic features that are mani-
fested in ‘depressed language’, such as negatively-
valenced words Beck et al. (1987), and frequent
use of first-person pronouns Pyszczynski et al.
(1987). These observations have been verified us-
ing corpus studies (Rude et al., 2004; Pennebaker
et al., 2008), indicating, indeed, certain aspects of
linguistic output is related to the speaker’s or au-
thor’s mental state.
A challenge in investigating the link between de-

pression and the linguistic output is obtaining suit-
able data. And, one of the easy (and fruitful) data
source for this purpose has been the Internet, in par-
ticular social media platforms (Ramirez-Esparza
et al., 2008; Coppersmith et al., 2015).
Most of the earlier works have been focused on

analyzing the language used by depressed individ-
uals, and/or finding linguistic correlates of the de-
pression. A more applicable approach to monitor-
ing public or individual mental health requires ex-
plicit identification of depression from the linguis-
tic samples. Such an application can complement
the conventional diagnosis methods, and, if proven
successful, it can be useful for diagnosis where
conventional methods are not applicable. Similar
to some of the recent studies (Coppersmith et al.,
2015; Yates et al., 2017; Lynn et al., 2018), our aim
in this paper is identifying depression from linguis-
tic data. Using (mainly) corpora we gather from
the social media platform Reddit, we experiment
with a number of different classification models.
Our focus here is on selection of corpora for reli-
able and generalizable analysis or identification of
depression from the social media data.

2 Methods

2.1 Data

In part of our experiments, we use the data col-
lected by Ramirez-Esparza et al. (2008), which



10

consists of 400 forum posts by depressed individ-
uals. Ramirez-Esparza et al. (2008) used a simi-
larly sized data set from a breast cancer forum as
the ‘control group’ in their analyses. Since the con-
trol data was not available to us, we report results
using an alternative set of documents collected by
Gorbunova (2017) as the negative class in our clas-
sification experiments.
Our data was gathered from Reddit, which hosts

over 10 000 online communities (also known as
‘subreddits’) of anonymous users united by com-
mon interests or discussion topics. In all data sets
described below, we only collect the original posts,
‘submissions’, not the comments.

As an approximation to the data collection
method of Ramirez-Esparza et al. (2008), we col-
lect data from a relatively large subreddit that is
devoted to depression, where authors seek support
from the community. Similar to Ramirez-Esparza
et al. (2008), we also collect the number of posts
from subreddit devoted to breast cancer discussion,
as the control set (or negative class). Since the
differences between depression and breast cancer
may involve serious topical differences, we also
collect yet another set of posts from ‘family’ and
‘friendship advice’ subreddits, which we presume
is topically more similar to depression subreddit.
In all of the cases above, however, the posts in

the both positive and negative classes are topically
specific. In practice, we would like to identify de-
pression from everyday language, not necessarily
language used for talking about depression, and
seeking community support. As our more realistic
example, we collected a number of posts follow-
ing a protocol similar to Coppersmith et al. (2015)
and Yates et al. (2017). First, we looked for ex-
pressions like ‘I was just diagnosed with depres-
sion’, on the depression subreddit. Unlike Yates
et al. (2017), we do not manually check the sam-
pled texts. As a result, a certain number of false
positives are expected. For each author mention-
ing a diagnosis, we searched for the postings of
the same author within a month of the original post
in other subreddits, excluding some potentially re-
lated ones like ‘Anxiety’, ‘mentalhealth’ and ‘de-
pression_help’. The resulting posts are written by
authors with (likely) depression, and to a large ex-
tend topically different than that of depression sub-
reddit. To keep the training set size the same as
the other data sets we use, we randomly sample
400 posts obtained in this manner for training, and

another 400 posts for testing. Another difference
from Yates et al. (2017) is that our training and test
instances are the documents, not the authors. We
also sample randomly the same amount of posts
as our texts with authors without depression, from
the same set of subreddits, but excluding the au-
thors that posted in the depression subreddit during
the time period we used for our investigation. For
each setting, we pick only one document for each
author.
In sum, we experiment with 8 data sets:

DSF Posts from Depression Support Forums
(Ramirez-Esparza et al., 2008)

DND Posts from Non Depression Forums (Gor-
bunova, 2017)

DS Posts from Depression Support subreddit
BC Posts from Breast Cancer subreddit
FF Posts from subreddits related to Family and

Friends
DO Posts from authors with (probable) Depres-

sion posted on Other forums
ND Posts from authors with (probably) No Depres-

sion
All data sets have 400 training set items, and the

data sets DO and ND also have additional 400 posts
used as a reasonable test set.

2.2 Classifiers and tuning

We have experimented with a relatively large num-
ber of classification methods, including logistic re-
gression and recurrent neural networks, in a num-
ber of different settings. In our experiments, the
support vector machines (SVMs) with a combina-
tion of character and word n-grams of various sizes
performed the best. We only report the experi-
ments with SVM models.
In all cases we used linear SVMs with bag-of-n-

grams features. SVMs are known to work well in a
number of other text classification problems in this
setting. The character andword n-grams of various
sizes are extracted from the texts, and weighted us-
ing BM25 algorithm (Robertson et al., 2009). We
optimized maximum order of character and word
n-grams as well as the SVM margin parameter C
through random search. A 5-fold cross-validation
is performed for each parameter setting explored.
For each experiment we report the setting where
average scores over the 5-fold cross validation is
the highest. The BM25 parameters ‘k1’ and ‘b’
were not optimized, and set to 0.75 and 2.0 respec-
tively. For experiments with class imbalance, we



11

Model 5-fold F1 Test set F1

DSF–NDF 94.75 64.05
DS–BC 98.62 56.88
DS–FF 92.25 55.62
DS–ND 91.75 56.48
DO–ND 68.12 67.49
allD–allND 91.40 58.28

Table 1: Best 5-fold CV results obtained on each clas-
sification setting, together with the performance of the
system on the test set. The model descriptions list data
used for positive and negative class respectively. The
data sets are explained in Section 2.1. The last row com-
bined all ‘positive’ and ‘negative’ data sets, except the
DO and ND sets. The scores are percentages.

used class weights during training to overcome the
class imbalance problem.
All models were implemented in Python pro-

gramming language, using scikit-learn package
(Pedregosa et al., 2011). The source code
for the classification models and data collection
scripts are available at https://github.com/
Inusette/Identifying-depression.

2.3 Evaluation

For evaluating the models, we report the standard
measures of F1 score (harmonic mean of precision
and recall). We use the ‘binary’ version of the
scores with positive class being text from authors
with depression.

2.4 Experiments and results

We train 6 SVM classifiers, using different data
sets descried in Section 2.1. Table 1 presents the
performance comparison of the classifier on a num-
ber of different settings.
Each row in Table 1 presents F1-score of a bi-

nary SVM classifier on the data set as well as the
performance of the same system on the test set con-
sisting of DO and ND. Since each model is tuned for
F1-score, the precision and recall values are rather
balanced, and are not reported in Table 1. In gen-
eral, 5-fold cross validation results are rather high,
especially if both data sets are specific. Best results
are obtained when both data sets are very specific,
as in DS–BC case. The success of the classifier goes
down as the texts belonging to the negative class
comes from less specific domains. And in fact,
the worse in-dataset results are obtained in our tar-
get setting, during which the classification of doc-
uments written by authors with diagnosed depres-

sion in non-depression related topics (DO) against
the documents on the similar topics written by (pre-
sumably) healthy authors (ND). The gap between
all other settings and DO–ND setting is rather large.
We also observe a very sharp drop of perfor-

mance between the 5-fold cross validation results
and the results on the test set. Interestingly, the
most successful model (except DO–ND) on the test
data is the forum data which is expected to be
rather different from the all others which came
from Reddit.
The last row of Table 1 reports the performance

of a model where positive/negative instances of all
other (except DO and ND) settings are combined.
The resulting model is trained on more data, how-
ever, its data sources are not as harmonized as in
other settings. As a result, it performs comparably,
but worse than other specific models. However,
the non-specificity seems to slightly help in the test
set, resulting in better than all others (except DSF–
NDF setting).

3 Discussion and conclusions

In this paper we reported a number of experiments
on detecting depression from language samples
collected from social media. Being able to detect
depression from linguistic material is interesting
both theoretically, and due to its potential applica-
tions as a diagnostic aid or for monitoring of pub-
lic or individual mental health. These goals are vi-
able only if we can identify depression to a suc-
cessful degree. There has been a number promis-
ing results for detecting depression from the writ-
ing samples, particularly from social media texts
(Ramirez-Esparza et al., 2008; Coppersmith et al.,
2015; Reece et al., 2017; Lynn et al., 2018).
In this study, our focus has been the selection

of sources for successful detection of depression
from social media text. Our results clearly show
that careful selection of sources is important for
not obtaining illusionary results. This is particu-
larly important if one intends to use the resulting
systems in practical applications. However, it may
be equally important, not to get wrong conclusions
for more theoretically oriented research as well.
Another important contribution of our works is

the use of Reddit for the purpose. There has been
relatively few studies using Reddit for investigat-
ing linguistic aspects of mental health (De Choud-
hury and De, 2014; Yates et al., 2017). We believe
Reddit’s emphasis on anonymity is useful for ob-

https://github.com/Inusette/Identifying-depression
https://github.com/Inusette/Identifying-depression


12

taining less biased results. Reddit corpora also has
the advantage of availability,1 which can help re-
producing the earlier results. Furthermore, not hav-
ing length limitation like Twitter, a popular choice
in other studies, may also be important in some
cases. The F1-scores we obtained on Reddit cor-
pus, although higher than the results reported in
Yates et al. (2017), is lower than the earlier results
on Twitter (Coppersmith et al., 2015). This could
potentially be due to the small number of training
instances in our study. However, further investiga-
tion is needed for understanding the differences.
In this study we only reported results from lin-

ear classifiers, using simple character and word
bag-of-n-gram features. These models are simple,
fast, language independent, and performed better
than other systems we experimented with, includ-
ing a number of deep learning architectures (this
is in line with some earlier work where same mod-
els and methodolgy is used on similar tasks, e.g.,
Çöltekin and Rama, 2016, 2018). Furthermore, al-
though our focus in this paper has been their per-
formance, the linear models are also more open to
analysis, allowing investigation of (types) of fea-
tures that are useful for the task.

References
Aaron T Beck, A John Rush, Brian F Shaw, and Gary

Emery. 1987. Cognitive Therapy of Depression.
Guilford Press.

Çağrı Çöltekin and Taraka Rama. 2016. Discriminat-
ing similar languages with linear svms and neural
networks. In Proceedings of the Third Workshop on
NLP for Similar Languages, Varieties and Dialects
(VarDial3), pages 15–24, Osaka, Japan.

Çağrı Çöltekin and Taraka Rama. 2018. Tübingen-
Oslo at SemEval-2018 task 2: SVMs perform better
than RNNs at emoji prediction. In Proceedings of
the 12th International Workshop on Semantic Eval-
uation (SemEval-2018), New Orleans, LA, United
States.

Glen Coppersmith, Mark Dredze, Craig Harman,
Kristy Hollingshead, and Margaret Mitchell. 2015.
Clpsych 2015 shared task: Depression and ptsd on
twitter. In Proceedings of the 2nd Workshop on
Computational Linguistics and Clinical Psychology:
From Linguistic Signal to Clinical Reality, pages 31–
39. Association for Computational Linguistics.

Munmun De Choudhury and Sushovan De. 2014. Men-
tal health discourse on Reddit: Self-disclosure, so-
cial support, and anonymity. In Proceedings of the
Eight International AAAI Conference on Weblogs
and Social Media (ICWSM), Ann Arbor, Michigan,
USA.
1The corpus we use is publicly available at https://

files.pushshift.io/reddit/submissions/.

Anastasia Gorbunova. 2017. Predicting depression
from online communication: comparison of three
classification techniques. Master’s thesis, Univer-
sity of Tübingen, Tübingen, Germany.

Veronica Lynn, Alissa Goodman, Kate Niederhof-
fer, Kate Loveys, Philip Resnik, and H. Andrew
Schwartz. 2018. Clpsych 2018 shared task: Pre-
dicting current and future psychological health from
childhood essays. In Proceedings of the Fifth Work-
shop on Computational Linguistics and Clinical Psy-
chology: FromKeyboard to Clinic, pages 37–46. As-
sociation for Computational Linguistics.

Fabian Pedregosa, Gaël Varoquaux, Alexandre Gram-
fort, Vincent Michel, Bertrand Thirion, Olivier
Grisel, Mathieu Blondel, Peter Prettenhofer, Ron
Weiss, Vincent Dubourg, Jake Vanderplas, Alexan-
dre Passos, David Cournapeau, Matthieu Brucher,
Matthieu Perrot, and Édouard Duchesnay. 2011.
Scikit-learn: Machine learning in Python. Journal
of Machine Learning Research, 12:2825–2830.

JamesW. Pennebaker, Cindy K. Chung, EwaKacewicz,
and Nairan Ramirez-esparza. 2008. The psychology
of word use in depression forums in English and in
Spanish: Testing two text analytic approaches. In
ICWSM.

Tom Pyszczynski, Kathleen Holt, and Jeff Greenberg.
1987. Depression, self-focused attention, and ex-
pectancies for positive and negative future life events
for self and others. Journal of Personality and Social
Psychology, 52(5):994–1001.

Nairan Ramirez-Esparza, Cindy K Chung, Ewa
Kacewicz, and JamesW Pennebaker. 2008. The psy-
chology of word use in depression forums in English
and in Spanish: Testing two text analytic approaches.
In International Conference on Weblogs and Social
Media, pages 102–108.

Andrew G. Reece, Andrew J. Reagan, Katharina L. M.
Lix, Peter Sheridan Dodds, ChristopherM. Danforth,
and Ellen J. Langer. 2017. Forecasting the onset and
course of mental illness with twitter data. Scientific
Reports.

C Steven Richards and Michael W O’Hara. 2014. The
Oxford Handbook of Depression and Comorbidity.
Oxford Library of Psychology. Oxford University
Press.

Stephen Robertson, Hugo Zaragoza, et al. 2009. The
probabilistic relevance framework: BM25 and be-
yond. Foundations and Trends® in Information Re-
trieval, 3(4):333–389.

Stephanie Rude, Eva-Maria Gortner, and James Pen-
nebaker. 2004. Language use of depressed and
depression-vulnerable college students. Cognition
and Emotion, 18(8):1121–1133.

DV Sheenan. 2004. Depression: underdiagnosed, un-
dertreated, underappreciated. 13.

Andrew Yates, Arman Cohan, and Nazli Goharian.
2017. Depression and self-harm risk assessment in
online forums. In Proceedings of the 2017 Con-
ference on Empirical Methods in Natural Language
Processing, pages 2968–2978, Copenhagen, Den-
mark. Association for Computational Linguistics.

https://files.pushshift.io/reddit/submissions/
https://files.pushshift.io/reddit/submissions/

