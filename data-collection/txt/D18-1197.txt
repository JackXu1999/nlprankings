



















































What It Takes to Achieve 100 Percent Condition Accuracy on WikiSQL


Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 1702–1711
Brussels, Belgium, October 31 - November 4, 2018. c©2018 Association for Computational Linguistics

1702

What It Takes to Achieve 100% Condition Accuracy on WikiSQL

Semih Yavuz and Izzeddin Gur and Yu Su and Xifeng Yan
Department of Computer Science, University of California, Santa Barbara

{syavuz,izzeddingur,ysu,xyan}@cs.ucsb.com

Abstract

WikiSQL is a newly released dataset for study-
ing the natural language sequence to SQL
translation problem. The SQL queries in Wik-
iSQL are simple: Each involves one relation
and does not have any join operation. De-
spite of its simplicity, none of the publicly
reported structured query generation models
can achieve an accuracy beyond 62%, which
is still far from enough for practical use. In
this paper, we ask two questions, “Why is the
accuracy still low for such simple queries?”
and “What does it take to achieve 100% ac-
curacy on WikiSQL?” To limit the scope of
our study, we focus on the WHERE clause in
SQL. The answers will help us gain insights
about the directions we should explore in or-
der to further improve the translation accuracy.
We will then investigate alternative solutions
to realize the potential ceiling performance on
WikiSQL. Our proposed solution can reach up
to 88.6% condition accuracy on the WikiSQL
dataset.

1 Introduction

A large amount of world’s data is stored in re-
lational databases, which require users to master
structured query languages such as SQL to query
data. It might not be convenient for many users
who do not have programming background. To-
wards removing this huge barrier between non-
expert users and data, building reliable natural lan-
guage interfaces to databases has been a long-
standing open problem (Woods, 1973; Androut-
sopoulos et al., 1995; Popescu et al., 2003).

Recently, there is a renewed interest in natu-
ral language interfaces to databases due to the
advance in deep learning and the new release
of large-scale annotated data such as WikiSQL
(Zhong et al., 2017). WikiSQL includes a large
collection of questions and their corresponding
SQL queries. While the queries in WikiSQL are

quite simple: Each involves one relation and does
not have join operations, none of the publicly
reported SQL generation models (Zhong et al.,
2017; Xu et al.) can achieve an accuracy beyond
62%, which is far from enough for practical use.
It is not clear yet what level of parsing capabilities
are needed to achieve high performance, ideally
close to 100% accuracy, on this task.

In this paper, we aim to figure out the level
of language understanding required to perform
well on the WikiSQL task. Specifically, we fo-
cus on the WHERE clause generation, which is the
most challenging part of this task as reported in
(Xu et al.): The accuracies for other clauses like
SELECT and AGGREGATE are over 90% whereas
the accuracy for WHERE is only around 70%. We
aim to conduct the following two studies: (i) Un-
derstanding the difficulties of the task and (ii) In-
vestigating alternative solutions to realize the po-
tential ceiling performance.

To this end, we first conduct a careful analysis
on a subset of the WikiSQL data to identify the
main challenges. This analysis leads to two im-
portant observations: (i) ∼17% of the questions
are either too ambiguous (hard) or require exter-
nal knowledge to answer, (ii) ∼68% of the ques-
tions can be answered by exact match or simple
paraphrasing, however we surprisingly find that
the current best system (Xu et al.) can only get
less than 80% accuracy on such simple questions.
Deeper analysis on the second observation leads
to the conclusion that most of the errors in this
category are due to wrongly generated condition
values as shown by the example in Figure 1. One
can resort to soft/hard look-up based approaches
over table content as in previous work (Mou et al.,
2017; Iyer et al., 2017) or user interaction (which
is also applicable to the first observation) to more
accurately recognize the entities in questions for
better condition value generation.



1703

Which award has the category of the best 
direction of a musical?

Table Question:

1986 Tony Award Best Musical Best Musical Nominated

1986 Tony Award Best Direction of a Musical Bob Fosse Nominated

1986 Tony Award Best Choreography Bob Fosse Won

1986
Drama Desk 

Award
Outstanding Actor in a 

Musical
Cleavant 
Derricks

Nominated

1986
Drama Desk 

Award
Outstanding Director of a 

Musical
Bob Fosse Nominated

1986
Drama Desk 

Award
Outstanding Choreography Bob Fosse Won

Correct SQL:

SELECT award
WHERE category = best direction of a musical

SQLNet Prediction:

SELECT award
WHERE category = the best direction award a musical

Figure 1: An example from WikiSQL. SQLNet makes a wrong prediction on the condition value of the WHERE clause.

As a second contribution, we propose to use ta-
ble content as additional data source to address
the aforementioned wrongly generated condition
value problem, and investigate solutions to real-
ize the potential performance limit (upper bound)
on WikiSQL. Note that neither Seq2SQL (Zhong
et al., 2017) nor SQLNet (Xu et al.) utilize table
content. However, we show that it is not straight-
forward to achieve a high accuracy even in the sce-
nario where table content is available as an op-
timal external knowledge through our model ab-
lation results and error analysis. We demonstrate
that our proposed solutions can reach up to 88.6%
WHERE condition accuracy, almost matching the
performance on SELECT and AGGREGATE.

2 Background

The WikiSQL dataset introduced in (Zhong et al.,
2017) is created from a large number of tables ex-
tracted from Wikipedia, employing Amazon Me-
chanical Turk for annotation. An example from
the dataset is provided in Figure 1: It consists of a
table t, a SQL query s, and its corresponding nat-
ural language question q. There can be multiple
conditions in the WHERE clause of a SQL query,
each of which consists of a table column, an oper-
ator (=, <,>, etc.), and a condition value.

Instead of asking annotators to write SQL
queries for given questions and tables, the authors
(Zhong et al., 2017) facilitate the annotation pro-
cess by paraphrasing generated questions. This
raises concerns that the resulting dataset is limited
to only simple queries. We acknowledge this con-
cern. However, it is still a large, valuable dataset
towards the goal of building ultimate natural lan-
guage interfaces to databases. If the existing or
newly proposed solutions can not solve this task
with high accuracy, how can we advance to more
complicated ones? Any insight and solution to this
task can help us build more advanced SQL synthe-
sis algorithms in future.

3 WikiSQL Data Analysis

In this section, we aim to deliver a thorough anal-
ysis of the WikiSQL dataset. (Zhong et al., 2017;
Xu et al.) made an assumption that only table
schema is available to the model, and table content
(i.e., table cell values) is not available. We want to
answer the following question: As the dataset cre-
ation process involves several heuristics and pre-
defined templates to simplify the annotation task,
what kind of capabilities does it still require to an-
swer the resulting questions successfully? To this
end, we randomly sample 100 examples from the
development set of WikiSQL for analysis.

3.1 Categorization

We manually categorize these 100 examples in
terms of the capability needed to predict the cor-
rect WHERE clause as described below. We also
provide illustrative examples for each category in
Table 1. If an example belongs to multiple cate-
gories, we include it in all the categories that ap-
ply. In Table 2, we present the break down of the
examples over these categories.
Exact match. For each condition in the SQL
query, its column name appears in the question
around the neighborhood of its condition value
with exactly the same surface form.
Paraphrase. For at least one of the conditions in
the SQL query, its column name is paraphrased in
the question, hence the inference requires certain
paraphrasing capabilities.
Partial clue. For at least one of the conditions
in the SQL query, its column name is not explic-
itly mentioned in the question, not even in para-
phrased form. However, there are still partial se-
mantic clues for inference.
External knowledge. For at least one of the con-
ditions in the SQL query, there is no clue in the
question to infer its column name. Inferring this
column name from the question requires exter-
nal knowledge regarding the type of its condition



1704

Category Question Pseudo SQL Query Table Columns

EXACT MATCH In what state was the electorate
fowler?

SELECT state
WHERE electorate EQL fowler

member, party, elec-
torate, state, term in of-
fice

PARAPHRASE What was the date of the game
after week 5 against the Hous-
ton Oilers?

SELECT date
WHERE week GT 5
AND opponent EQL houston oilers

week, date, opponent,
result, attendance

PARTIAL CLUE Who had the most points in the
game on March 7?

SELECT high points
WHERE date EQL march 7

game, date, team,
score, high points, ...

EXTERNAL-
KNOWLEDGE

Name the callback date for
amway arena

SELECT callback date
WHERE audition venue EQL amway arena

episode air data, au-
dition city, audition
venue, callback date, ...

AMBIGUOUS List the branding for krca-tv SELECT branding
WHERE callsign EQL krca-tv

branding, callsign,
channel, power (kw), ...

Table 1: Representative examples for each category. EQL and GT correspond to the = and > operators. Highlighted parts of
the question and the pseudo SQL query are provided to indicate clues for the category.

Category # Questions Percentage

EXACT MATCH 59 54.1%
PARAPHRASE 16 14.7%
PARTIAL CLUE 15 13.8%
EXTERNAL KNOWLEDGE 11 10.1%
AMBIGUOUS 8 7.3%

Table 2: Number of examples in different categories.
Category Seq2set+CA Seq2set+CA+WE

EXACT MATCH 67.8% 72.9%
PARAPHRASE 75.0% 68.8%
PARTIAL CLUE 80.0% 80.0%
EXTERNAL KNOWLEDGE 54.6% 45.5%
AMBIGUOUS 37.5% 37.5%
TOTAL 67.0% 67.9%

Table 3: Accuracy breakdown of SQLNet.

value that can be detected from the question.
Ambiguous. For this category, even with the ex-
ternal knowledge it is almost impossible (even for
humans) to confidently infer the correct condition
column from the question.

3.2 Performance Breakdown of SQLNet

In Table 3, we show the performance breakdown
of SQLNet (Xu et al.), the state-of-the-art model
for WikiSQL, on the selected 100 examples. It
has two variants with comparable performance, so
we show both of them. As expected, both variants
of SQLNet perform poorly on examples that are
either too ambiguous or require external knowl-
edge. However, it is surprising that the accuracy
on examples that need only exact matching or sim-
ple paraphrasing is also not very high, especially
considering the paraphrasing capabilities of deep
learning models gained from distributed represen-
tations. We find that most of these errors are
due to wrongly generated condition values as il-
lustrated in Figure 1, where SQLNet fails to even

produce a valid phrase. This is indeed important
prior knowledge that can be effectively outsourced
by resorting to soft/hard look-up based approaches
(Mou et al., 2017; Iyer et al., 2017) instead of fully
relying on models to precisely generate it.

The above observations exhibit an opportunity
to incorporate external knowledge in the condition
generation process. We opt to use the table content
as the knowledge source to address the wrong con-
dition value problem, which is not used in the ex-
isting models (Zhong et al., 2017; Xu et al.). Next
we will show that it is not trivial to leverage table
content.

4 Our Solutions

As motivated in the previous sections, our main
objective here is to investigate solutions to re-
alize the potential ceiling performance on Wik-
iSQL when using the table content as an additional
knowledge source. We first describe an attentional
RNN-based model that serves as our baseline, and
propose several variants for WHERE clause gener-
ation, each addressing a specific weakness.

4.1 Task Formulation

Given a question q and a table t, our objective is
to generate the WHERE clause of the SQL query
corresponding to the question. In addition, we as-
sume that the table t can be queried while gen-
erating the WHERE clause. Each condition in
the WHERE clause is represented as a triple of
(COLUMN,OP,VALUE). SQLNet (Xu et al.) gen-
erates each of these individual components by first
predicting COLUMN and OP, and then generating
VALUE using pointer networks (Vinyals et al.). In



1705

Figure 2: Model overview.

contrast, we propose a two step approach to tackle
this problem in the reverse way, taking advantage
of the content of table t as additional knowledge:
(i) Generate the condition VALUE from the ques-
tion, and (ii) predict which COLUMN and OP apply
to this VALUE.

4.1.1 Candidate Generation
Our objective in candidate generation process is
to produce a set of (COLUMN,VALUE) pairs from
question q and table t, where VALUE is an n-gram
in q and COLUMN is a column in t. Similar to prior
work (Zhong et al., 2017; Xu et al.), we assume
that VALUE occurs in the question.

We first generate the set N of all n-grams from
the question. Then, for each candidate v ∈ N
and each column c ∈ t, we query table t to check
whether value v is contained in any row of col-
umn c. Then, we create a set C = {(c, v) : v ∈ c}
of (COLUMN,VALUE) pairs as our final candidates
for the WHERE clause. We note here that VALUE
may not necessarily be an n-gram in question
for other potential external knowledge or NLIDB
tasks. In such scenarios, we can alternatively re-
sort to soft look-up based approaches like the ones
proposed in (Mou et al., 2017; Bordes et al., 2015).

4.1.2 Condition Prediction
Given a question q, SQL table t, and a set C of
(COLUMN,VALUE) pair as candidates for WHERE
clause, our objective is to learn two mappings:

fcond :(q, c, v) 7→ {0, 1} (1)
fop :(q, c, v) 7→ {=, >,<} (2)

where fcond determines whether to select (c, v) ∈
C as a condition in WHERE clause and fop pre-
dicts the operator OP ∈ {=, >,<}. These two
mappings together can fully determine the final
WHERE clause. Note that there can be multiple
conditions in the WHERE clause.

4.2 Model Overview
We design a neural network model (Figure 2) to
instantiate the mappings fcond and fop. We first
describe the general structure of the model, con-
sisting of the following steps:
Value Context. We define value context as the
context of the value in the question for a given pair
of question q and value v. Later the question en-
coder will condition on this information to make
predictions. We will investigate different options
for value context in Section 4.3. For now, we de-
fine it as a transformation gcontext which maps each
(q, v) to its value context.
Value Abstraction. Inspired by (Yavuz et al.,
2016), we define value abstraction as a trans-
formation gabstract that replaces the surface form
of VALUE in the question with a single token
ENTITY. For the running example in Figure 1,
applying value abstraction maps the question to
“Which award has the category of the ENTITY?”
It further informs the question encoder regarding
the location of the VALUE in the question.
Encoding. Given question q = (q1, q2, . . . , qm),
column c = (c1, c2, . . . , cn), and value v =
(v1, v2, . . . , vk). Before encoding, we first apply
the aforementioned textual transformations and
obtain value context

q′ = (q′1, q
′
2, . . . , q

′
l) = gcontext ◦ gabstract(q, v).

We first encode all the words into a vector space
using an embedding matrix E ∈ Rd×|V|, where
V denotes the vocabulary and d is the embedding
dimension. Let q′i denote the embedding of word
q′i. To obtain the contextual embeddings, we use a
bi-directional LSTM with hidden unit size h:

−→
hqi = LSTM

q
fwd(
−→
hqi−1,q

′
i) (3)

←−
hqi = LSTM

q
bwd(
←−
hqi+1,q

′
i) (4)

with
−→
h 0 = 0 and

←−
h l+1 = 0. We com-

bine forward and backward contextual embed-



1706

dings for each word in the question to obtain hqi =
concat(

−→
hqi,
←−
hqi) ∈ R2h.

Similarly, we obtain the contextual embedding
hcolj ∈ R2h for each word cj in column c with
another bi-directional LSTM but shared word em-
bedding matrix.
Distilled Attention. The objective of this step is
to distill the most relevant information from both
the value context and the column for the final pre-
diction. We first compute the attention score of
each word cj ∈ c on the words q′i ∈ q′:

S
(col→q)
j,i = h

col
j W

(col→q)h
q
i (5)

P
(col→q)
j = softmaxi S

(col→q)
j,i (6)

where W(col→q) ∈ R2h×2h is a model parameter
to be learned and P (col→q)j ∈ Rl represents an at-
tention probability distribution of word cj over the
words of value context q′. Let P (col→q) ∈ Rn×l
denote the column-wise concatenation of P (col→q)j
indicating the unified representation of attention
matrix from column words onto value context.
Similarly, we compute the attention from value
context (question) to column and get P (q→col) ∈
Rl×n.

The intuition behind distilled attention is to al-
low two-way comparison to clean up the attention
weights. To exemplify this point better, consider
a scenario where a word cj ∈ c attends on a word
q′i ∈ q′ with high probability, but q′i is much more
relevant to another word cj′ ∈ c. We leverage
reverse attention P (q→col) to distill the attention
weights of column words on the value context. To
this end, we define distilled attention weights as

P = (P (q→col))> � P (col→q) (7)

where P ∈ Rn×l becomes our final attention
weights and � indicates the Hadamard product.
Value Context and Column Representations.
Having defined how the encodings and attention
weights are computed, we now describe the final
value context and column representations. First,
we compute a value context vector for each word
cj ∈ c using the distilled attention weights by

h
(q→col)
j =

l∑
i=1

Pj,i h
q
i (8)

and fuse it with the corresponding column context
vector by

hcondj = tanh(W
cond
0 h

col
j + W

cond
1 h

(q→col)
j ) (9)

h
op
j = tanh(W

op
0 h

col
j + W

op
1 h

(q→col)
j ) (10)

where Wcond0 ,W
cond
1 ,W

op
0 ,W

op
1 ∈ R2h×2h are

trainable model parameters. Finally, we apply a
pooling layer to obtain fixed-size representations

hcond =
1

n

∑
j

hcondj and h
op =

1

n

∑
j

h
op
j (11)

for condition and operator predictions.
Prediction. So far we have obtained two differ-
ent representations hcond ∈ R2h and hop ∈ R2h as
the latent unified representations of question, col-
umn, and value triple (q, c, v). We than use these
representations to make the final predictions:

pcond = softmax(Ucond hcond) ∈ R2 (12)
pop = softmax(Uop hop) ∈ R3 (13)

where Ucond ∈ R2×2h and Uop ∈ R3×2h are
model parameters. The final prediction mappings
are then defined as:

fcond(q, c, v) = arg max
i

pcondi (14)

fop(q, c, v) = arg max
i

p
op
i (15)

Training Objective. Let T = {(q, c, v, l, o)} de-
note a set of training tuples where l ∈ {0, 1} de-
notes whether to include a condition with (c, v) as
(COLUMN,VALUE) pair and if so, o ∈ {=, >,<}
indicates which OP to apply. Our loss function
consists of two components:

J(Θ) = Jcond(Θ) + l Jop(Θ) (16)

where Jcond(Θ) = − log(pcondl ) and Jop(Θ) =
− log(popo ) are simply negative log-likelihood
losses for condition and operator predictions.
Inference. We also employ an inference schema
where we make a simple assumption that a con-
dition value v may be a part of only one condi-
tion. Hence, we group candidate {(c, v)} pairs
by value v, and create a candidate column set
Cv : {c : (c, v)} for each unique candidate value
v. Based on the probabilities pcond(c, v) computed
by the trained model, we select the column c ∈ Cv
with the maximum probability for each candidate
value v, hence form the set of (COLUMN,VALUE)
pairs to be included in condition this way.

4.3 Model Variants
In this section, we discuss variants of our model
with different choices for the value context. Con-
sider the running example in Figure 1 and assume



1707

p1’ = smaller than 0.5

p2’ = a large end QPs3

p3’ = has NP

s2

s1

Figure 3: Partial view of parse tree for question “Which ta-
per/ft that has a Large end smaller than 0.5” illustrating parse
tree based value contexts.

hypothetically that award is a candidate condi-
tion column for value best direction of a musi-
cal. When we use the whole question as the value
context, eliminating award from being a condition
COLUMN for this VALUE becomes very challeng-
ing for the model as it is not informed enough re-
garding the finer context of the value.

4.3.1 Base Model
The base model simply uses identity mapping for
value context. More precisely, we use the whole
question as the context for the candidate condition
VALUE. So, gcontext(q, v) = q.

4.3.2 Window-based model
The objective of window-based model is to get
value contexts that can provide more clean con-
text information by leveraging the context window
around the candidate value. In this case, we first
identify the span [start, end] for value v in the
question q. Based on its span and a predetermined
context window size w, we define gcontext(q, v) =
(qstart−w, . . . , qstart, . . . , qend, . . . , qend+w).

4.3.3 Parse tree-based model
We also analyze a simple parse tree based model
that hierarchically split up the value context into
multiple contexts based on the constituency parse
tree of the question. To motivate this, consider the
question in Figure 3. Window-based value con-
text for 0.5 is “a large end smaller than ENTITY”
and candidate table columns to apply this value are
“large end” and “small end”. Based on this con-
text, a model will likely assign a higher probability
to “large end” than “small end”. The syntactic
structure of the question can potentially help re-
duce such ambiguities.

Parse tree-based value contexts. As highlighted
in Figure 3, we use nested phrase-level subtrees
that contain the candidate VALUE. Moreover, to
better inform the model regarding the type of
phrases, we use phrase level constituent tags of
subtrees in two ways: (i) to inform the parent tree
with the type of its subtree containing candidate
VALUE, (ii) to apply a tag-specific affine transfor-
mation on phrase representations. To this end, we
first select the r-lowest subtrees s1 ⊂ s2 ⊂ . . . ⊂
sr of the question’s parse tree containing VALUE
along with their corresponding phrase-level con-
stituency tags1 t1, t2, . . . , tr, respectively. We then
iteratively form multiple values contexts as nested
phrases of p′1, p

′
2, . . . , p

′
r shown in Figure 3 as fol-

lows: (i) p′1 is equal to the phrase formed by the
words at the leaf nodes of subtree s1, and then
(ii) iteratively form p′i as the phrase formed by
the words under subtree si by replacing its sub-
phrase corresponding to subtree si−1 with its con-
stituency tag ti−1 for i > 1.
Modifications to General Model. We now de-
scribe how the general model defined in Section
4.2 is adapted to accommodate the multiple nested
value contexts of different types. In this process,
we aim to capture two types of important infor-
mation: (i) syntactic phrase-level types of value
contexts, and (ii) their distance to VALUE.

We first compute a value context vector
h

(q→col)
j,k ∈ R

2h for each word cj ∈ c and each
value context p′k as in Eq. 8 using the same en-
coding and attention mechanisms. However, we
replace the affine transformation layers defined in
Eq. 9 and 10 with tag and distance specific ones
as follows:

hcondj = tanh(W
cond
0 h

col
j +

1

r

r∑
k=1

Wcondk,tk h
(q→col)
j,k )

(17)

h
op
j = tanh(W

op
0 h

col
j +

1

r

r∑
k=1

W
op
k,tk

h
(q→col)
j,k )

(18)

where Wcond0 ,W
cond
k,tk

,W
op
0 ,W

op
k,tk
∈ Rd×2h for

k = 1, 2, . . . , r are model parameters. It is impor-
tant to note here that k determines the distance of
value context to VALUE and tk indicates its tag,
effectively making the fusion layers above tag and

1We use the Penn treebank annotation conven-
tions that are described in Bies et al. (1995) at
http://languagelog.ldc.upenn.edu/myl/PennTreebank1995.pdf



1708

Model Dev Test

No External Knowledge
SEQ2SQL (Zhong et al., 2017) 62.1% 60.2%
SQLNET-(Seq2Set + CA) 72.1% 70.0%
SQLNET-(Seq2Set + CA + WE) 74.1% 71.9%
SQLNET*-(Seq2Set + CA) 72.3% 70.9%
SQLNET*-(Seq2Set + CA + WE) 73.8% 71.7%
In Our Scenario
SQLNET*-(Seq2Set + CA) 82.2% 80.9%
SQLNET*-(Seq2Set + CA + WE) 83.5% 81.8%
STAMP + RL (Sun et al., 2018) 77.3% 76.3%
TYPESQL + TC (w/ Freebase) (Yu et al., 2018) 92.8% 87.9%
OUR BASELINE 77.2% 77.1%
SQLMASTER 84.8% 83.9%
SQLMASTER + VA 86.2% 86.1%
SQLMASTER (Window-Based) 87.4% 87.1%
SQLMASTER (Window-Based) + VA 87.9% 87.6%
SQLMASTER (Tree-Based) 88.7% 88.3%
SQLMASTER (Tree-Based) + VA 88.9% 88.6%
OUR UPPERBOUND 92.1% 91.4%

Table 4: WHERE clause accuracy. +VA denotes that Value
Abstraction is applied. SQLNET* refers to our reimplemen-
tation of SQLNet. SQLMASTER refers to our proposed mod-
els. Only TYPESQL + TC leverages Freebase on top of table
content among the models reported in our scenario. WHERE
clause accuracy of TYPESQL (w/o Freebase) is not reported.

distance specific. The rest of the model exactly
follows the Eq. from 11 through 15 with the same
training objective and inference schemes.

5 Experiments

In this section, we discuss the details of the exper-
iments and present our main results.

5.1 Training Details

For training our neural networks, we only keep the
words appearing at least 3 times in the whole train-
ing data and the rest of the words are replaced with
UNK token. Word embeddings are initialized with
pretrained GloVe (Pennington et al., 2014) vec-
tors2, and updated during the training. We take
the dimension of word embeddings and the size of
LSTM hidden layer both equal to 100. The model
parameters are optimized using Adam (Kingma
and Ba, 2015) with batch size of 32 and a decaying
learning rate starting with 0.001. We apply gradi-
ent clipping to 5 when its norm exceeds this value.
We use early stopping based on the model accu-
racy on the development set. We report our results
with a model snapshot achieving the best accuracy
on the development set. Our models are imple-
mented in tensorflow (Abadi et al., 2016). The
code is available at https://github.com/
semihyavuzz/sql_master.

2More specifically, we use 100D vectors from
http://nlp.stanford.edu/data/glove.6B.zip

Model Dev Test
SQLNET (Xu et al.) 63.2% 61.3%
DIALSQL (Gur et al., 2018) 70.9% 69.0%
TYPESQL (w/o Freebase) (Yu et al., 2018) 66.5% 64.9%
TYPESQL + TC (w/ Freebase) (Yu et al., 2018) 79.2% 75.4%
SQLMASTER (Ours, w/o Freebase) 73.1% 72.4%

Table 5: Full query-match (QM) accuracy. For SQLMAS-
TER, we combine our WHERE clause predictions with the
SELECT and AGGREGATE clause predictions of SQLNet.
QM result for TYPESQL + TC (w/o Freebase) is not reported.

5.2 Main Results

In Table 4, we present our main results in com-
parison with the related works. BASELINE refers
to a baseline for our models where the WHERE
clause accuracy is computed by assuming that
each candidate (COLUMN,VALUE) pair is in-
cluded with corresponding OP being equality. On
the other hand, UPPERBOUND accuracy is com-
puted by assuming fcond and fop makes 100% cor-
rect mapping of whether to include a candidate
(COLUMN,VALUE) pair and which OP to apply on
this condition. In other words, errors in UPPER-
BOUND exist due to wrong candidate generation.

As shown in Table 4, our models surpass the
previous results by a large margin as well as its
variants improving upon each other. A portion of
these improvements definitely come from assum-
ing and using the table itself as the optimal ex-
ternal knowledge. Acknowledging this fact, we
make the following more important conclusions
from Table 4, 5, and 6: (i) Comparison of the
performance results across scenarios (SQLNET vs.
SQLMASTER or TYPESQL) reveals that there is
a large room for improvement when an external
knowledge base is used, (ii) Comparison of our
own models with its variants demonstrates that
each component/extension incorporated brings a
considerable performance improvement, justify-
ing its potential power to be used in other re-
lated NLP tasks, (iii) Comparing our models with
SQLNET and TYPESQL+TC within our scenario
provides further clues/justifications towards effec-
tiveness of our proposed Tree-Based model, (iv)
SQLMASTER performs comparably to TYPESQL
+ TC (Yu et al., 2018) on WHERE condition pre-
dictions despite the fact that we do not use Free-
base, which is exploited in (Yu et al., 2018) to
identify named entities of certain Freebase types
(e.g., person, place, country, organization, sport),
(v) Our SQLMASTER (Tree-Based) + VA model
achieves 88.6% on test portion of WikiSQL, which
almost reaches the upper bound of 91.4%, demon-
strating a great promise for future work in this do-

https://github.com/semihyavuzz/sql_master
https://github.com/semihyavuzz/sql_master


1709

Category SQLNET SQLMASTER

EXACT MATCH 72.9% 86.4%
PARAPHRASE 68.8% 93.8%
PARTIAL CLUE 80.0% 93.3%
EXTERNAL KNOWLEDGE 45.5% 90.9%
AMBIGUOUS 37.5% 75.0%
TOTAL 67.9% 88.1%

Table 6: Accuracy breakdown of SQLNET compared to
SQLMASTER over the categories obtained by hand-analysis
on the randomly selected examples as explained in Section 3.

main, and finally (vi) When the performance of
our model is compared to SQLNET over the cat-
egories as shown in Table 6, we observe that it
consistently improves the performance of over all
the categories, but most noticeably on EXTERNAL
KNOWLEDGE and AMBIGUOUS ones which were
the main categories inspiring this work and pro-
posed approaches motivated by the analysis pro-
vided in Section 3.
Evaluation in Our Scenario. We adapt SQL-
Net (Xu et al.) results to our scenario via a post-
processing step as follows: For each of their pre-
dicted condition in WHERE clause, if column c and
operator o are both correct, but value v is wrong,
then we replace this value with the one in our gen-
erated candidates that maps to the column c when
the mapping is unique (only one value maps to c).

5.3 Error Analysis

In this section, we provide an error analysis of our
models to better understand what are the remain-
ing challenges to achieve UPPERBOUND perfor-
mance. To this end, we analyzed 100 randomly
sampled examples from development set on which
our best model fails. 41% of these errors are due
to our models not being able to perform a good se-
mantic understanding of the value context. 36%
of the errors correspond to ambiguous questions
that lack sufficient information to disambiguate
between correct and wrong column. A good rep-
resentative example for this category is “Who was
the director of king’s speech?”, where our model
predicts “winner and nominees” column for the
condition value “king’s speech” while the correct
column is “original title”. The remaining 18% and
5% of the errors are caused by sparsity of column
names and wrong labelling problems, respectively.

6 Related Work

Research on natural language interfaces to
databases (NLIDBs) and semantic parsing has
spanned several decades. Early rule-based

NLIDBs (Woods, 1973; Androutsopoulos et al.,
1995; Popescu et al., 2003) employ carefully de-
signed rules to map natural language questions to
formal representations like SQL queries. While
having a high precision, rule-based systems are
brittle when facing with language variations and
usually only admit inputs from a restricted vo-
cabulary. The rise of statistical models (Zettle-
moyer and Collins, 2005; Kate et al., 2005; Berant
et al., 2013) and neural network models (Yih et al.,
2015; Dong and Lapata, 2016; Sun et al., 2016;
Zhong et al., 2017; Xu et al.) has enabled NLIDBs
that are more robust to language variations. Such
systems allow users to formulate questions with
greater flexibility instead of having to probe and
adapt to the boundary of rule-based systems.

Along with the advance in modeling is the de-
velopment of benchmarks for training and test-
ing NLIDB models. Early benchmarks are mostly
curated by experts (Zelle and Ray, 1996; Zettle-
moyer and Collins, 2007). State-of-the-art models
(Dong and Lapata, 2016) have achieved a high ac-
curacy of 80% to 90% on these benchmarks. In
the recent years, a number of large-scale, crowd-
sourced benchmarks have been constructed with
the goal to train and test NLIDBs in a more
real-world setting, notably WebQuestions (Berant
et al., 2013) and GraphQuestions (Su et al., 2016)
for knowledge bases, and WikiSQL (Zhong et al.,
2017) for SQL queries to relational databases. The
best accuracies on these benchmarks are still far
from enough for real use, typically in the range of
20% to 60%.

Besides releasing WikiSQL, Zhong et al. (2017)
propose an approach (Seq2SQL) to solve this
task. Seq2SQL leverages the pointer-networks
(Vinyals et al.) to generate linearized SQL queries
token-by-token using the input question and ta-
ble schema. They report significant performance
improvement over (Dong and Lapata, 2016), a
generic sequence-to-tree approach proposed for
semantic parsing. More recently, Xu et al. propose
a sketch-based sequence-to-set approach (SQL-
Net) eliminating sequence-to-sequence structure
employed in (Zhong et al., 2017), when the order
does not matter. In our work, we provide a careful
analysis of SQLNet results to better understand the
limitations of this model on the WikiSQL task. In-
spired by this analysis, we propose novel solutions
to realize close to upper-bound condition accuracy
in the scenario where SQL table is available as an



1710

optimal external knowledge. Another recent work
(Yu et al., 2018) also focuses on using external
knowledge (Freebase) along with the table content
to generate SQL queries in a type aware fashion.
A concurrent line of related work exploits graph-
to-sequence neural models with the aim to better
exploit syntactic information in the input question
(Xu et al., 2018a,b). On the other hand, Gur et al.
(2018) takes an orthogonal approach and intro-
duces a dialogue-based query refinement mecha-
nism where a candidate SQL query (generated by
any black-box model) is refined by interactively
validating and updating modular segments of the
query with users. The authors show that by having
successful interactions with users, not only the ac-
curacy of the candidate queries can be improved
but also new insights into limitations of current
query generation systems can be gained.

There are also a number of recent studies on
semantic parsing for semi-structured tables. For
example, Pasupat and Liang (2015) develop the
WikiTableQuestions benchmark, where the task is
to find table cells in HTML table to answer ques-
tions, while Jauhar et al. (2016) focus on multi-
choice questions. On the other hand, Sun et al.
(2016) study how to answer user questions with
table cells from millions of HTML tables. These
studies directly find cells of semi-structured tables
as answers, instead of generating SQL queries for
relational databases.

7 Conclusion

In this paper, we thoroughly analyzed the recently
released WikiSQL dataset and the performance
breakdown of SQLNet. Through the analysis, we
identified an opportunity/need to further explore
the potentials of incorporating external knowledge
in the structured query generation process. In this
direction, we developed alternative solutions to
explore the potential performance limits for this
task in the scenario where table content can be
used. We showed that our proposed systems can
reach up to 88.6% accuracy in condition genera-
tion and provided a discussion regarding what the
remaining challenges were through an error anal-
ysis. We consider solving the WikiSQL task as a
necessary preliminary step towards realizing natu-
ral language interfaces to databases in full fledge.

Acknowledgements

This research was sponsored in part by the Army
Research Laboratory under cooperative agree-
ments W911NF09-2-0053 and NSF 1528175. The
views and conclusions contained herein are those
of the authors and should not be interpreted as rep-
resenting the official policies, either expressed or
implied, of the Army Research Laboratory or the
U.S. Government. The U.S. Government is autho-
rized to reproduce and distribute reprints for Gov-
ernment purposes notwithstanding any copyright
notice herein.

References
Martı́n Abadi, Paul Barham, Jianmin Chen, Zhifeng

Chen, Andy Davis, Jeffrey Dean, Matthieu Devin,
Sanjay Ghemawat, Geoffrey Irving, Michael Isard,
Manjunath Kudlur, Josh Levenberg, Rajat Monga,
Sherry Moore, Derek G. Murray, Benoit Steiner,
Paul Tucker, Vijay Vasudevan, Pete Warden, Martin
Wicke, Yuan Yu, and Xiaoqiang Zheng. 2016. Ten-
sorflow: A system for large-scale machine learning.
In 12th USENIX Symposium on Operating Systems
Design and Implementation (OSDI 16).

Ion Androutsopoulos, Graeme D Ritchie, and Peter
Thanisch. 1995. Natural language interfaces to
databases–an introduction. Natural language engi-
neering, 1(1):29–81.

Jonathan Berant, Andrew Chou, Roy Frostig, and Percy
Liang. 2013. Semantic parsing on Freebase from
question-answer pairs. In Empirical Methods on
Natural Language Processing (EMNLP).

Ann Bies, Mark Ferguson, Karen Katz, and Robert
MacIntyre. 1995. Bracketing guidelines for tree-
bank ii style penn treebank project. In Linguistic
Data Consortium.

Antoine Bordes, Nicolas Usunier, Sumit Chopra, and
Jason Weston. 2015. Large-scale simple question
answering with memory networks. arXiv preprint
arXiv:1506.02075.

Li Dong and Mirella Lapata. 2016. Language to logical
form with neural attention. In Annual Meeting of the
Association for Computational Linguistics (ACL).

Izzeddin Gur, Semih Yavuz, Yu Su, and Xifeng Yan.
2018. Dialsql: Dialogue based structured query
generation. In Annual Meeting of the Association
for Computational Linguistics (ACL).

Srinivasan Iyer, Ioannis Konstas, Alvin Cheung,
Jayant Krishnamurthy, and Luke Zettlemoyer. 2017.
Learning a neural semantic parser from user feed-
back. In Annual Meeting of the Association for
Computational Linguistics (ACL).



1711

Sujay Kumar Jauhar, Peter Turney, and Eduard Hovy.
2016. Tables as semi-structured knowledge for
question answering. In Annual Meeting of the As-
sociation for Computational Linguistics (ACL).

Rohit J Kate, Yuk Wah Wong, and Raymond J Mooney.
2005. Learning to transform natural to formal lan-
guages. In AAAI Conference on Artificial Intelli-
gence (AAAI).

Diederik P. Kingma and Jimmy Ba. 2015. Adam: A
method for stochastic optimization. In International
Conference on Learning Representations (ICLR).

Lili Mou, Zhengdong Lu, Hang Li, and Zhi Jin. 2017.
Coupling distributed and symbolic execution for nat-
ural language queries. In International Conference
on Machine Learning (ICML).

Panupong Pasupat and Percy Liang. 2015. Compo-
sitional semantic parsing on semi-structured tables.
In Annual Meeting of the Association for Computa-
tional Linguistics (ACL).

Jeffrey Pennington, Richard Socher, and Christopher D
Manning. 2014. Glove: Global vectors for word
representation. In Empirical Methods on Natural
Language Processing (EMNLP).

Ana-Maria Popescu, Oren Etzioni, and Henry Kautz.
2003. Towards a theory of natural language inter-
faces to databases. In Proceedings of the 8th in-
ternational conference on Intelligent user interfaces,
pages 149–157. ACM.

Yu Su, Huan Sun, Brian Sadler, Mudhakar Srivatsa,
Izzeddin Gur, Zenghui Yan, and Xifeng Yan. 2016.
On generating characteristic-rich question sets for qa
evaluation. In Empirical Methods on Natural Lan-
guage Processing (EMNLP).

Huan Sun, Hao Ma, Xiaodong He, Wen-tau Yih, Yu Su,
and Xifeng Yan. 2016. Table cell search for question
answering. In World Wide Web (WWW).

Yibo Sun, Duyu Tang, Nan Duan, Jianshu Ji, Guihong
Cao, Xiaocheng Feng, Bing Qin, Ting Liu, and Ming
Zhou. 2018. Semantic parsing with syntax- and
table-aware sql generation. In Annual Meeting of the
Association for Computational Linguistics (ACL).

Oriol Vinyals, Meire Fortunato, and Navdeep Jaitly.
Pointer networks. In Advances in Neural Informa-
tion Processing Systems (NIPS).

William A Woods. 1973. Progress in natural language
understanding: an application to lunar geology. In
Proceedings of the American Federation of Informa-
tion Processing Societies Conference.

Kun Xu, Lingfei Wu, Zhiguo Wang, and Vadim
Sheinin. 2018a. Graph2seq: Graph to se-
quence learning with attention-based neural net-
works. arXiv preprint arXiv:1804.00823.

Kun Xu, Lingfei Wu, Zhiguo Wang, Mo Yu, Li-
wei Chen, and Vadim Sheinin. 2018b. Exploiting
rich syntactic information for semantic parsing with
graph-to-sequence model. In Empirical Methods on
Natural Language Processing (EMNLP).

Xiaojun Xu, Chang Liu, and Dawn Song. Sqlnet:
Generating structured queries from natural language
without reinforcement learning. arXiv preprint
arXiv:1711.04436.

Semih Yavuz, Izzeddin Gur, Yu Su, Mudhakar Srivatsa,
and Xifeng Yan. 2016. Improving semantic parsing
via answer type inference. In Empirical Methods on
Natural Language Processing (EMNLP).

Scott Wen-tau Yih, Ming-Wei Chang, Xiaodong He,
and Jianfeng Gao. 2015. Semantic parsing via
staged query graph generation: Question answering
with knowledge base. In Annual Meeting of the As-
sociation for Computational Linguistics (ACL).

Tao Yu, Zifan Li, Zilin Zhang, Rui Zhang, and
Dragomir Radev. 2018. Typesql: Knowledge-based
type-aware neural text-to-sql generation. In Confer-
ence of the North American Chapter of the Associ-
ation for Computational Linguistics: Human Lan-
guage Technologies (NAACL-HLT).

John M Zelle and Mooney Ray. 1996. Learning to
parse database queries using inductive logic pro-
gramming. In Proceedings of the AAAI Conference
on Artificial Intelligence.

Luke Zettlemoyer and Michael Collins. 2007. Online
learning of relaxed ccg grammars for parsing to log-
ical form. In Proceedings of the 2007 Joint Con-
ference on Empirical Methods in Natural Language
Processing and Computational Natural Language
Learning (EMNLP-CoNLL).

Luke S. Zettlemoyer and Michael Collins. 2005.
Learning to map sentences to logical form: Struc-
tured classification with probabilistic categorial
grammars. In Proceedings of the Conference on Un-
certainty in Artificial Intelligence (UAI).

Victor Zhong, Caiming Xiong, and Richard Socher.
2017. Seq2sql: Generating structured queries
from natural language using reinforcement learning.
arXiv preprint arXiv:1709.00103.


