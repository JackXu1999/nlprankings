




































Rethinking Complex Neural Network Architectures for Document Classification


Proceedings of NAACL-HLT 2019, pages 4046–4051
Minneapolis, Minnesota, June 2 - June 7, 2019. c©2019 Association for Computational Linguistics

4046

Rethinking Complex Neural Network Architectures for
Document Classification

Ashutosh Adhikari,∗ Achyudh Ram,∗ Raphael Tang, and Jimmy Lin
David R. Cheriton School of Computer Science

University of Waterloo
{adadhika, arkeshav, r33tang, jimmylin}@uwaterloo.ca

Abstract
Neural network models for many NLP tasks
have grown increasingly complex in recent
years, making training and deployment more
difficult. A number of recent papers have
questioned the necessity of such architectures
and found that well-executed, simpler mod-
els are quite effective. We show that this is
also the case for document classification: in
a large-scale reproducibility study of several
recent neural models, we find that a simple
BiLSTM architecture with appropriate regu-
larization yields accuracy and F1 that are ei-
ther competitive or exceed the state of the art
on four standard benchmark datasets. Sur-
prisingly, our simple model is able to achieve
these results without attention mechanisms.
While these regularization techniques, bor-
rowed from language modeling, are not novel,
to our knowledge we are the first to apply them
in this context. Our work provides an open-
source platform and the foundation for future
work in document classification.

1 Introduction

Recent developments in neural architectures for
a wide range of NLP tasks can be character-
ized as a drive towards increasingly complex net-
work components and modeling techniques. Wor-
ryingly, these new models are accompanied by
smaller and smaller improvements in effectiveness
on standard benchmark datasets, which leads us
to wonder if observed improvements are “real”.
There is, however, ample evidence to the contrary.
To provide a few examples: Melis et al. (2018)
report that standard LSTM architectures outper-
form more recent models when properly tuned.
Vaswani et al. (2017) show that sequence trans-
duction using encoder–decoder networks with at-
tention mechanisms work just as well with the at-
tention module only, making most of the complex
∗ Equal contribution.

neural machinery unnecessary. Mohammed et al.
(2018) show that simple RNN- and CNN-based
models yield accuracies rivaling far more complex
architectures in simple question answering over
knowledge graphs.

Perhaps most damning are the indictments
of Sculley et al. (2018), who lament the lack of
empirical rigor in our field and cite even more ex-
amples where improvements can be attributed to
far more mundane reasons (e.g., hyperparameter
tuning) or are simply noise. Lipton and Stein-
hardt (2018) concur with these sentiments, adding
that authors often use fancy mathematics to ob-
fuscate or to impress (reviewers) rather than to
clarify. Complex architectures are more difficult
to train, more sensitive to hyperparameters, and
brittle with respect to domains with different data
characteristics—thus both exacerbating the “crisis
of reproducibility” and making it difficult for prac-
titioners to deploy networks that tackle real-world
problems in production environments.

Like the papers cited above, we question the
need for overly complex neural architectures, fo-
cusing on the problem of document classification.
Starting with a large-scale reproducibility study
of several recent neural models, we find that a
simple bi-directional LSTM (BiLSTM) architec-
ture with appropriate regularization yields accu-
racy and F1 that are either competitive or exceed
the state of the art on four standard benchmark
datasets. As the closest comparison point, we
find no benefit to the hierarchical modeling pro-
posed by Yang et al. (2016) and we are able to
achieve good classification results without atten-
tion mechanisms. While these regularization tech-
niques, borrowed from language modeling, are not
novel, we are to our knowledge the first to apply
them in this context. Our work provides an open-
source platform and the foundation for future work
in document classification.



4047

2 Background and Related Work

2.1 Document Classification
Over the last few years, deep neural networks have
achieved the state of the art in document classifi-
cation. One popular model, hierarchical attention
network (HAN), uses word- and sentence-level
attention in classifying documents (Yang et al.,
2016). Although this model nicely captures the in-
tuition that modeling word sequences in sentences
should be handled separately from sentence-level
discourse modeling, one wonders if such complex
architectures are really necessary, especially given
the size of training data available today.

An important variant of document classifica-
tion is the multi-label, multi-class case. Liu et al.
(2017) develop XML-CNNs for multi-label text
classification, basing the architecture on Kim-
CNN (Kim, 2014) with increased filter sizes and
an additional fully-connected layer. They also in-
corporate dynamic adaptive max-pooling (Chen
et al., 2015) instead of the vanilla max-pooling
over time in KimCNN. The paper compares with
CNN-based approaches for the multi-label task,
but only reports precision and disregards recall.
Yang et al. (2018) instead adopts encoder–decoder
sequence generation models (SGMs) for generat-
ing multiple labels for each document. Similar to
our critique of HAN, we opine against the high
complexity of these multi-label approaches.

2.2 Regularizing RNNs
There have been attempts to extend dropout (Sri-
vastava et al., 2014) from feedforward neural net-
works to recurrent ones. Unfortunately, direct
application of dropout on the hidden units of an
RNN empirically harms its ability to retain long-
term information (Zaremba et al., 2014). Re-
cently, however, Merity et al. (2018) success-
fully apply dropout-like techniques to regularize
RNNs for language modeling, achieving competi-
tive word-level perplexity on multiple datasets. In-
spired by this development, we adopt two of their
regularization techniques, embedding dropout and
weight-dropped LSTMs, to our task of document
classification.
Weight-dropped LSTM. LSTMs comprise eight
total input–hidden and hidden–hidden weight ma-
trices; in weight dropping, Merity et al. (2018)
regularize the four hidden–hidden matrices with
DropConnect (Wan et al., 2013). The operation
is applied only once per sequence, using the same

m
ax-pool

ba c d f

e g
1

2

n

Figure 1: Illustration of the model architecture, where
the labels are the following: (a) input word embed-
dings (b) BiLSTM (c, d) concatenated forward hf1:n and
backward hb1:n hidden features (e) max-pooling over
time (f) document feature vector (g) softmax or sig-
moid output.

dropout mask across multiple timesteps. Conve-
niently, this allows practitioners to use fast, out-of-
the-box LSTM implementations without affecting
the RNN formulation or training performance.

Embedding Dropout. Introduced in Gal and
Ghahramani (2016) and successfully employed for
neural language modeling (Merity et al., 2018),
embedding dropout performs dropout on entire
word embeddings, effectively removing some of
the words at each training iteration. As a result,
the technique conditions the model to be robust
against missing input; for document classification,
this discourages the model from relying on a small
set of input words for prediction.

3 BiLSTM Model

We design our model to be minimalistic: First,
we feed the word embeddings w1:n of a document
to a single-layer BiLSTM, extracting concatenated
forward and backward word-level context vectors
h1:n = h

f
1:n ⊕ hb1:n. Subsequently, we max-pool

h1:n across time to yield document vector d—see
Figure 1, labels a–f. Finally, we feed d to a sig-
moid or a softmax layer over the labels, depend-
ing on if the task type is multi-label or single-label
classification (label g).

Contrary to prior art, our approach refrains from
attention, hierarchical structure, and sequence
generation, each of which increases model com-
plexity. For one, hierarchical structure requires
sentence-level tokenization and multiple RNNs.
For another, sequence generation uses an encoder–
decoder architecture, reducing computational par-
allelism. All three methods add depth to the
model; our approach instead uses a single-layer
BiLSTM with trivial max-pooling and concatena-



4048

tion operations, which makes for both simple im-
plementation and resource-efficient inference.

4 Experimental Setup

We conduct a large-scale reproducibility study in-
volving HAN, XML-CNN, KimCNN, and SGM.
These are compared to our proposed model, re-
ferred to as LSTMreg, as well as an ablated variant
without regularization, denoted LSTMbase. The
implementation of our model as well as from-
scratch reimplementations of all the comparison
models (except for SGM) are provided in our
toolkit called Hedwig, which we make publicly
available to serve as the foundation for future
work.1 In addition, we compare the neural ap-
proaches to logistic regression (LR) and support
vector machines (SVMs). The LR model is trained
using a one-vs-rest multi-label objective, while the
SVM is trained with a linear kernel. Both of these
methods use word-level tf–idf vectors of the doc-
uments as features.

All of our experiments are performed on Nvidia
GTX 1080 and RTX 2080 Ti GPUs, with PyTorch
0.4.1 as the backend framework. We use Scikit-
learn 0.19.2 for computing the tf–idf vectors and
implementing LR and SVMs.

4.1 Datasets

We evaluate our models on the following four
datasets: Reuters-21578, arXiv Abstract Paper
dataset (AAPD), IMDB, and Yelp 2014. Reuters
and AAPD are multi-label datasets, whereas
IMDB and Yelp are single-label ones. For IMDB
and Yelp, we use random sampling to split the
dataset such that 80% is used for training, 10% for
validation, and 10% for test. We use the standard
ModApte splits (Apté et al., 1994) for the Reuters
dataset, and author-defined splits for AAPD (Yang
et al., 2018). We summarize the statistics of these
datasets in Table 1.

Unfortunately, there is little consensus within
the natural language processing community for
choosing the splits of IMDB and Yelp 2014. Fur-
thermore, they are often unreported in modeling
papers, hence preventing direct comparison with
past results. We are not able to find the exact
splits Yang et al. (2016) use; for consistency, we
use the same proportion the authors report, but of
course this yields different samples in each split.

1 http://hedwig.ca

Dataset C N W S

Reuters 90 10,789 144.3 6.6
AAPD 54 55,840 167.3 1.0
IMDB 10 135,669 393.8 14.4
Yelp 2014 5 1,125,386 148.8 9.1

Table 1: Summary of the datasets. C denotes the num-
ber of classes in the dataset, N the number of samples,
and W and S the average number of words and sen-
tences per document, respectively.

For the multi-label datasets, we report the well-
known micro-averaged F1 score, which is the
class-weighted harmonic mean between recall and
precision. For the single-label datasets, we com-
pare the models using accuracy.

4.2 Training and Hyperparameters
To ensure a fair comparison, we tune the hyper-
parameters for all baseline models. For HAN, we
use a batch size of 32 across all the datasets, with
a learning rate of 0.01 for Reuters and 0.001 for
the rest. To train XML-CNN, we select a dy-
namic pooling window length of eight, a learn-
ing rate of 0.001, and 128 output channels, with
batch sizes of 32 and 64 for single-label and multi-
label datasets, respectively. For KimCNN, we use
a batch size of 64 with a learning rate of 0.01. For
training SGM on Reuters, we use the source code
provided by the authors2 and follow the same hy-
perparameters in their paper (Yang et al., 2018).
For the LR and SVM models, we use the default
set of hyperparameters in Scikit-learn.

For LSTMreg and LSTMbase, we use the Adam
optimizer with a learning rate of 0.01 on Reuters
and 0.001 on the rest of the datasets, using batch
sizes of 32 and 64 for multi-label and single-label
tasks, respectively. For LSTMreg, we also apply
temporal averaging (TA): as shown in Kingma and
Ba (2014), TA reduces both generalization error
and stochastic noise in recent parameter estimates
from stochastic approximation. We set the default
TA exponential smoothing coefficient of βEMA to
0.99. We choose 512 hidden units for the Bi-
LSTM models, whose max-pooled output is regu-
larized using a dropout rate of 0.5. We also reg-
ularize the input–hidden and hidden–hidden Bi-
LSTM connections using embedding dropout and
weight dropping, respectively, with dropout rates
of 0.1 and 0.2.
2 https://github.com/lancopku/SGM



4049

# Model Reuters AAPD IMDB Yelp ’14

Val. F1 Test F1 Val. F1 Test F1 Val. Acc. Test Acc. Val. Acc. Test Acc.

1 LR 77.0 74.8 67.1 64.9 43.1 43.4 61.1 60.9
2 SVM 89.1 86.1 71.1 69.1 42.5 42.4 59.7 59.6
3 KimCNN Repl. 83.5 ±0.4 80.8 ±0.3 54.5 ±1.4 51.4 ±1.3 42.9 ±0.3 42.7 ±0.4 66.5 ±0.1 66.1 ±0.6
4 KimCNN Orig. – – – – – 37.6†† – 61.0††

5 XML-CNN Repl. 88.8 ±0.5 86.2 ±0.3 70.2 ±0.7 68.7 ±0.4 – – – –
6 HAN Repl. 87.6 ±0.5 85.2 ±0.6 70.2 ±0.2 68.0 ±0.6 51.8 ±0.3 51.2 ±0.3 68.2 ±0.1 67.9 ±0.1
7 HAN Orig. – – – – – 49.4‡ – 70.5‡
8 SGM Orig. 82.5 ±0.4 78.8 ±0.9 – 71.0† – – – –

9 LSTMbase 87.6 ±0.2 84.9 ±0.3 72.1 ±0.4 69.6 ±0.4 52.5 ±0.2 52.1 ±0.3 68.6 ±0.1 68.4 ±0.1
10 LSTMreg 89.1 ±0.8 87.0 ±0.5 73.1 ±0.4 70.5 ±0.5 53.4 ±0.2 52.8 ±0.3 69.0 ±0.1 68.7 ±0.1

Table 2: Results for each model on the validation and test sets; best values are bolded in blue. Repl. reports mean
± SD of five runs from our reimplementations; Orig. refers to point estimates from †Yang et al. (2018), ‡Yang
et al. (2016), and ††Tang et al. (2015).

For our optimization objective, we use cross-
entropy and binary cross-entropy loss for single-
label and multi-label tasks, respectively. On
all datasets and models, we use 300-dimensional
word vectors (Mikolov et al., 2013) pre-trained on
Google News. We train all neural models for 30
epochs with five random seeds, reporting the mean
validation set scores and their corresponding test
set results.

Toward Robust Baselines. Recently, repro-
ducibility is becoming a growing concern for the
NLP community (Crane, 2018). Indeed, very few
of the papers that we consider in this study re-
port validation set results, let alone run on mul-
tiple seeds. In order to address these issues, we
report scores on both validation and test sets for
our reimplementations; doing so is good practice,
since it reinforces the validity of the experimen-
tal results and claims. We also provide the stan-
dard deviation of the scores across different seeds
to demonstrate the stability of our results. This is
in line with previous papers (Zhang and Wallace,
2017; Reimers and Gurevych, 2017; Crane, 2018)
that emphasize reporting variance for robustness
against potentially spurious conclusions.

5 Results and Discussion

We report the mean and standard deviation (SD) of
the F1 scores and accuracy for all five runs in Ta-
ble 2. For HAN and KimCNN, we include results
from the original papers to validate our reimple-
mentation. We fail to replicate the reported results
of SGM on AAPD using the authors’ codebase

and data splits.3 As a result, we simply copy the
value reported in Yang et al. (2018) in Table 2, row
8, which represents their maximum F1 score. To
verify the correctness of our HAN and KimCNN
reimplementations, we compare the differences in
F1 and accuracy on the appropriate datasets. We
attribute the small differences to using different
dataset splits (see Section 4.1) and reporting mean
values.

Baseline Comparison. We see that our sim-
ple LSTMreg model achieves state of the art on
Reuters and IMDB (see Table 2, rows 9 and 10),
establishing mean scores of 87.0 and 52.8 for F1
score and accuracy on the test sets of Reuters
and IMDB, respectively. This highlights the ef-
ficacy of proper regularization and optimization
techniques for the task. We observe that LSTMreg
consistently improves upon the performance of
LSTMbase across all of the tasks—see rows 9 and
10, where, on average, regularization yields in-
creases of 1.5 and 0.5 points for F1 score and ac-
curacy, respectively.

A few of our LSTMreg runs attain state-of-the-
art test F1 scores on AAPD. However, in the in-
terest of robustness, we report the mean value, as
mentioned in Section 4.2. We also find the accu-
racy of LSTMreg and our reimplemented version
of HAN on Yelp 2014 to be almost two points
lower than the copied result of HAN (rows 6, 7,
and 10) from Yang et al. (2016). On the other
hand, both of the models surpass the original re-
sult by nearly two points for the IMDB dataset.
We cannot rule out that these disparities are caused

3 The authors did not answer our e-mails seeking assistance.



4050

by the absence of any widely-accepted splits for
evaluation on Yelp 2014 and IMDB (as opposed
to model or implementation differences).

Interestingly, the non-neural LR and SVM base-
lines perform remarkably well. On Reuters, for
example, the SVM beats many neural baselines,
including our non-regularized LSTMbase (rows 2–
9). On AAPD, the SVM either ties or beats the
other models, losing only to SGM (rows 2–8).
Compared to the SVM, the LR baseline appears
better suited for the single-label datasets IMDB
and Yelp 2014, where it achieves better accuracy
than the SVM does.

6 Conclusions and Future Work

In this paper, we question the complexity of ex-
isting neural network architectures for document
classification. To demonstrate the effectiveness of
proper regularization and optimization, we apply
embedding dropout, weight dropping, and tem-
poral averaging when training a simple BiLSTM
model, establishing either competitive or state-of-
the-art results on multiple datasets.

One potential extension of this work is to con-
duct a comprehensive ablation study to determine
the relative contribution of each of the regulariza-
tion and optimization techniques. Furthermore, it
would be interesting to compare these techniques
to the recent line of research in deep language
representation models, such as Embeddings from
Language Models (ELMo; Peters et al., 2018) and
pre-trained transformers (Devlin et al., 2018; Rad-
ford, 2018). Finally, the examined regularization
and optimization methods deserve exploration in
other NLP tasks as well.

Acknowledgments

This research was supported by the Natu-
ral Sciences and Engineering Research Council
(NSERC) of Canada. We thank Nabiha Asghar for
providing us with the Yelp 2014 dataset. We also
thank the anonymous reviewers for their valuable
comments.

References
Chidanand Apté, Fred Damerau, and Sholom M Weiss.

1994. Automated learning of decision rules for text
categorization. ACM Transactions on Information
Systems, 12(3):233–251.

Yubo Chen, Liheng Xu, Kang Liu, Daojian Zeng,
and Jun Zhao. 2015. Event extraction via dy-

namic multi-pooling convolutional neural networks.
In Proceedings of the 53rd Annual Meeting of the
Association for Computational Linguistics and the
7th International Joint Conference on Natural Lan-
guage Processing (Volume 1: Long Papers), vol-
ume 1, pages 167–176.

Matt Crane. 2018. Questionable answers in question
answering research: reproducibility and variability
of published results. Transactions of the Association
for Computational Linguistics, 6:241–252.

Jacob Devlin, Ming-Wei Chang, Kenton Lee, and
Kristina Toutanova. 2018. BERT: Pre-training of
deep bidirectional transformers for language under-
standing. CoRR, abs/1810.04805.

Yarin Gal and Zoubin Ghahramani. 2016. A theoret-
ically grounded application of dropout in recurrent
neural networks. In Advances in neural information
processing systems, pages 1019–1027.

Yoon Kim. 2014. Convolutional neural networks for
sentence classification. In Proceedings of the 2014
Conference on Empirical Methods in Natural Lan-
guage Processing (EMNLP), pages 1746–1751.

Diederik P. Kingma and Jimmy Ba. 2014. Adam: A
method for stochastic optimization. In International
Conference on Learning Representations.

Zachary C. Lipton and Jacob Steinhardt. 2018.
Troubling trends in machine learning scholarship.
arXiv:1807.03341v2.

Jingzhou Liu, Wei-Cheng Chang, Yuexin Wu, and
Yiming Yang. 2017. Deep learning for extreme
multi-label text classification. In Proceedings of the
40th International ACM SIGIR Conference on Re-
search and Development in Information Retrieval,
pages 115–124.

Gbor Melis, Chris Dyer, and Phil Blunsom. 2018. On
the state of the art of evaluation in neural language
models. In International Conference on Learning
Representations.

Stephen Merity, Nitish Shirish Keskar, and Richard
Socher. 2018. Regularizing and optimizing LSTM
language models. In International Conference on
Learning Representations.

Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Cor-
rado, and Jeff Dean. 2013. Distributed representa-
tions of words and phrases and their compositional-
ity. In Advances in neural information processing
systems, pages 3111–3119.

Salman Mohammed, Peng Shi, and Jimmy Lin. 2018.
Strong baselines for simple question answering over
knowledge graphs with and without neural net-
works. In Proceedings of the 2018 Conference of
the North American Chapter of the Association for
Computational Linguistics: Human Language Tech-
nologies, Volume 2 (Short Papers), pages 291–296.



4051

Matthew Peters, Mark Neumann, Mohit Iyyer, Matt
Gardner, Christopher Clark, Kenton Lee, and Luke
Zettlemoyer. 2018. Deep contextualized word rep-
resentations. In Proceedings of the 2018 Confer-
ence of the North American Chapter of the Asso-
ciation for Computational Linguistics: Human Lan-
guage Technologies, Volume 1 (Long Papers), pages
2227–2237.

Alec Radford. 2018. Improving language understand-
ing by generative pre-training.

Nils Reimers and Iryna Gurevych. 2017. Reporting
score distributions makes a difference: performance
study of LSTM-networks for sequence tagging. In
Proceedings of the 2017 Conference on Empirical
Methods in Natural Language Processing, pages
338–348.

D. Sculley, Jasper Snoek, Alex Wiltschko, and Ali
Rahimi. 2018. Winner’s curse? On pace, progress,
and empirical rigor. In Proceedings of the 6th Inter-
national Conference on Learning Representations,
Workshop Track (ICLR 2018).

Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky,
Ilya Sutskever, and Ruslan Salakhutdinov. 2014.
Dropout: a simple way to prevent neural networks
from overfitting. Journal of Machine Learning Re-
search, 15(1):1929–1958.

Duyu Tang, Bing Qin, and Ting Liu. 2015. Document
modeling with gated recurrent neural network for
sentiment classification. In Proceedings of the 2015
Conference on Empirical Methods in Natural Lan-
guage Processing, pages 1422–1432.

Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob
Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz
Kaiser, and Illia Polosukhin. 2017. Attention is all
you need. In Advances in Neural Information Pro-
cessing Systems, pages 5998–6008.

Li Wan, Matthew Zeiler, Sixin Zhang, Yann Le Cun,
and Rob Fergus. 2013. Regularization of neural net-
works using dropconnect. In International Confer-
ence on Machine Learning, pages 1058–1066.

Pengcheng Yang, Xu Sun, Wei Li, Shuming Ma, Wei
Wu, and Houfeng Wang. 2018. SGM: Sequence
generation model for multi-label classification. In
Proceedings of the 27th International Conference on
Computational Linguistics, pages 3915–3926.

Zichao Yang, Diyi Yang, Chris Dyer, Xiaodong He,
Alex Smola, and Eduard Hovy. 2016. Hierarchi-
cal attention networks for document classification.
In Proceedings of the 2016 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics: Human Language Technologies,
pages 1480–1489.

Wojciech Zaremba, Ilya Sutskever, and Oriol Vinyals.
2014. Recurrent neural network regularization.
arXiv:1409.2329.

Ye Zhang and Byron Wallace. 2017. A sensitivity anal-
ysis of (and practitioners’ guide to) convolutional
neural networks for sentence classification. In Pro-
ceedings of the Eighth International Joint Confer-
ence on Natural Language Processing (Volume 1:
Long Papers), volume 1, pages 253–263.


