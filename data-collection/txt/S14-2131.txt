



















































UNAL-NLP: Combining Soft Cardinality Features for Semantic Textual Similarity, Relatedness and Entailment


Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 732–742,
Dublin, Ireland, August 23-24, 2014.

UNAL-NLP: Combining Soft Cardinality Features for Semantic
Textual Similarity, Relatedness and Entailment

Sergio Jimenez, George Dueñas,
and Julia Baquero

Universidad Nacional de Colombia
Ciudad Universitaria, edificio 453,

oficina 114, Bogotá, Colombia
[sgjimenezv,geduenasl,
jmbaquerov]@unal.edu.co

Alexander Gelbukh
Center for Computing Research (CIC),
Instituto Politécnico Nacional (IPN),
Av. Juan Dios Bátiz, Av. Mendizábal,

Col. Nueva Industrial Vallejo,
Mexico City, Mexico
www.gelbukh.com

Abstract

This paper describes our participation in
the SemEval-2014 tasks 1, 3 and 10. We
used an uniform approach for addressing
all the tasks using the soft cardinality for
extracting features from text pairs, and
machine learning for predicting the gold
standards. Our submitted systems ranked
among the top systems in all the task and
sub-tasks in which we participated. These
results confirm the results obtained in pre-
vious SemEval campaigns suggesting that
the soft cardinality is a simple and useful
tool for addressing a wide range of natural
language processing problems.

1 Introduction

The semantic textual similarity is a core prob-
lem in the computational linguistic field. Con-
sequently, the previous evaluation campaigns of
this task in SemEval have attracted the attention
of many research groups worldwide (Agirre et al.,
2012; Agirre et al., 2013).This year, 3 tasks related
to this problem have been proposed exploring dif-
ferent facets such as semantic relatedness, entail-
ment , multilingualism, lack of training data and
imbalance in the amount of information.

The soft cardinality (Jimenez et al., 2010) is a
simple concept that generalizes the classical set
cardinality by considering the similarities among
the elements in a collection for a more intuitive
quantification of the number of elements in that
collection. This approach can be applied to text
applications representing texts as collections of
words and providing a similarity function that
compares two words. Varying this word-to-word
similarity function the soft cardinality can reflect

This work is licensed under a Creative Commons At-
tribution 4.0 International Licence. Page numbers and pro-
ceedings footer are added by the organisers. Licence details:
http://creativecommons.org/licenses/by/4.0/

notions of syntactic similarity, semantic related-
ness, among others. We (and others) have used
this approach to address with success the semantic
textual similarity and other tasks in previous Se-
mEval editions (Jimenez et al., 2012b; Jimenez et
al., 2012a; Jimenez et al., 2013a; Jimenez et al.,
2013b; Jimenez et al., 2013c; Croce et al., 2013).

In this paper we describe our participating sys-
tems in the SemEval-2014 tasks 1, 3, and 10,
which used the soft cardinality as core approach.

2 Features from Soft Cardinalities

The cardinality of a collection of elements is the
counting of non-repeated elements in it. This def-
inition is intrinsically associated with the notion
of set, which is a collection of non-repeated ele-
ments.Thus, the cardinality of a collection or set
A is denoted as |A|. Clearly, the cardinality of a
collection with repeated elements treats groups of
identical elements as a single instance contribut-
ing only with a unit (1) to the element counting.
Jimenez et al. (2010) proposed the soft cardinal-
ity that uses a notion of similarity among elements
for grouping not only identical elements but simi-
lar too. That notion of similarity among elements
is provided by a similarity function that compares
two elements ai and aj and returns a score in [0,1]
interval, having sim(ai, ai) = 1. Although, it
is not necessary that sim fulfills another metric
properties aside of identity, symmetry is also de-
sirable. Thus, the soft cardinality of a collection
A, whose elements a1, a2, . . . , a|A| are compara-
ble with a similarity function sim(ai, aj), is de-
noted as |A|sim. This soft cardinality is given by
the following expression:

|A|sim =
|A|∑
i=1

wai∑|A|
j=1 sim(ai, aj)p

(1)

It is trivial to see that |A| = |A|sim either if
p → ∞ or when the function sim is a crisp com-

732



Basic Derived
|A| |A ∩B| = |A|+ |B| − |A ∪B|
|B| |A4B| = |A ∪B| − |A ∩B||
|A ∪B| |A \B| = |A| − |A ∩B|

|B \A| = |B| − |A ∩B|
Table 1: The 7 basic and derived cardinalities for
two sets comparison.

parator, i.e. one that returns 1 for identical ele-
ments and 0 otherwise. This property shows that
the soft cardinality generalizes the classical cardi-
nality and that the parameter p controls its degree
of “softness”, whose default value is 1. The values
wai are optional “importance” weights associated
with each element ai, by default those weights can
be assigned to 1.

For the tasks at hand, we represent each short
text (lets say A) as a collection of words ai and
the sim function can be any operator that com-
pares pairs of words. The motivation for using the
soft cardinality is that the sim function can reflect
any dimension of word similarity (e.g. syntactic,
semantic) and the soft cardinality projects that no-
tion at sentence level. For instance, if sim pro-
vides the degree of semantic relatedness between
two words using WordNet, two texts A and B
could be compared by computing |A|sim, |B|sim
and |A∪B|sim. Given that A∩B could be empty,
the soft cardinality of the intersection must be ap-
proximated by |A ∩ B|sim ≈ |A|sim + |B|sim −
|A ∪ B|sim instead of being computed directly
from A ∩ B using equation 1. Using that approx-
imation, the commonality (intersection) between
A and B is induced by the pair-wise similarities
provided by sim among the words in A and B.

Since more than a century when Jaccard (1901)
proposed his well-known index, the classical set
cardinality has been used to build similarity func-
tions for set comparison. Any binary-cardinality-
based similarity function is an algebraic combina-
tion of |A|, |B| and either |A ∩ B| or |A ∪ B|
(e.g. Jaccard, Dice, Tversky, overlap and cosine
indexes). These three cardinalities describes un-
ambiguously all the regions in the Venn’s diagram
when comparing two sets. Thus, in this scenario 4
possible cardinalities can be derived from these 3
basic cardinalities, see Table 1. Clearly, the same
set of cardinalities can be obtained for the soft car-
dinality.

When training data is available, which is the

# Feature expression
1 |A|/|A∪B|
2 |A|−|A∩B|/|A|
3 |A|−|A∩B|/|A∪B|
4 |B|/|A∪B|
5 |B|−|A∩B|/|B|
6 |B|−|A∩B|/|A∪B|
7 |A∩B|/|A|
8 |A∩B|/|B|
9 |A∩B|/|A∪B|
10 |A∪B|−|A∩B|/|A∩B|

Table 2: Extended set of 10 rational features.

case for tasks 1, 3 and 10 in SemEval 2014, it
is possible to think that instead of using an ad-
hoc expression (e.g. Jaccard, Dice) the similar-
ity function can be obtained using the cardinalities
in Table 1 as features for a machine-learning re-
gression algorithm. Our hypothesis is that such
learnt function should predict in a more accurate
way the gold standard variable than any other ad-
hoc function. However, these cardinality features
are intrinsically correlated with the length of the
texts where they were obtained. This correlation
makes that the performance of the learnt similar-
ity function could be dependent of the length of
the texts. For instance, if the function was trained
using long texts it is plausible to think that this
function would be more effective when tested with
long texts than with shorter ones. Having this in
mind, an extended set of rational features is pro-
posed, whose values are standardized in [0,1] in-
terval aiming to reduce the effect of the length of
the texts. These features are presented in Table 2.

The soft cardinality has proven to overcome
the classic cardinality in the semantic textual
similarity (STS) task in previous SemEval cam-
paigns (Jimenez et al., 2012b; Jimenez et al.,
2013a). Even using a simplistic function sim
based on q-grams of characters, the soft cardinal-
ity method ranked third among 89 participating
systems (Agirre et al., 2012). Thus, our participat-
ing systems in the SemEval 2014 campaign were
based on the previously described set of 17 fea-
tures, obtained from the soft cardinality with dif-
ferent sim functions for comparing pairs of words.
Each sim function produced a different set of fea-
tures, which were combined with a regression al-
gorithm for similarity and relatedness tasks. Sim-
ilarly, a classification algorithm was used for the

733



entailment task.

3 Systems Description

In this section the different feature sets used for
each submitted system to the different task and
subtask are described. Besides, the data used for
training, parameters and other preprocessing de-
tails are described for each system.

3.1 Task 1: Textual Relatedness and
Entailment

The task 1 is based on the SICK (Sentences
Involving Compositional Knowledge) data set
(Marelli et al., 2014), which contains nearly
10,000 pairs of sentences manually labeled by re-
latedness and entailment. The relatedness gold la-
bels range from 1 to 5, having 1 the minimum level
of relatedness between the texts and 5 for the max-
imum. The entailment labels have three categori-
cal values: neutral, contradiction and entailment.
The two sub tasks consist of predicting the related-
ness and entailment gold standards using approxi-
mately the 50% of the text pairs as training and the
other part as test bed.

Our overall approach consists in extracting 4
different sets of features using the method pre-
sented in section 2 and training a machine learn-
ing algorithm for predicting the gold standard la-
bels in the test data. Each feature set is described
in the following 4 subsections and the subsection
3.1.6 provides details of the used combination of
features, machine learning algorithm and prepro-
cessing details.

3.1.1 String-Matching Features
First, all texts in the SICK data set where prepro-
cessed by lower casing, tokenizing and stop-word
removal (using the NLTK1). Then each word was
reduced to its stem using the Porter’s algorithm
(Porter, 1980) and a idf weight (Jones, 2004) was
associated to each stem (wai weights in eq. 1) us-
ing the very SICK data set as document collec-
tion. Next, for each instance in the data, which
is composed of two texts A and B, the 17 fea-
tures listed in Tables 1 and 2 where extracted using
eq.1. The used word-to-word similarity function
sim decomposes each word in bags of 3-grams
of characters, which are compared using the sym-
metrical Tversky’s index (Tversky, 1977; Jimenez
et al., 2013a). Thus, the similarity between two

1http://www.nltk.org/

pairs of words w1 and w2, represented each one as
a collection of 3-grams of characters, is given by
the following expression:

sim(w1, w2) =
|c|

β(α|wmin|+ (1− α)|wmax|) + |c|
(2)

|c| = |w1 ∩ w2|+ biassim,
|wmin| = min[|w1 \ w2|, |w2 \ w1|],
|wmax| = max[|w1 \ w2|, |w2 \ w1|].

The values used for the parameters were α =
1.9, β = 2.36, bias = −0.97, and p = 0.39
(where p corresponds to eq.1). The motivation and
justification for these parameters can be found in
(Jimenez et al., 2013a). These values were ob-
tained by building a text similarity function us-
ing the Dice’s coefficient and the soft cardinali-
ties plugging eq.2 in eq.1. Next, this text similar-
ity function is evaluated in the 5,000 training text
pairs and the obtained scores are compared against
the relatedness gold-standard using the Pearson’s
correlation.
waiare not training parameters, but they are

weights associated with the words. These weights
could have been obtained from a larger corpus,
but we use the training texts to obtain them. This
process is repeated iteratively exploring the search
space defined by these 4 parameters using a hill-
climbing approach until a maximum correlation is
reached. We observe that the optimal values of the
parameters p, α, β, and bias vary considerably be-
tween the data sets and for the different sim func-
tions of word-to-word similarity. We do not yet
understand from which factors of the data and the
sim functions depend on these parameters. This
issue will be the objective of further research.

Henceforth, the set of 17 string-based features
described in this subsection will be referred as
SM.

3.1.2 ESA Features
For this set of features we used the idea proposed
by Gabrilovich and Markovitch (2007) of enrich-
ing the representation of a text by representing
each word by its textual definition in a knowl-
edge base, i.e. explicit semantic analysis (ESA).
For that, we used as knowledge base the synset’s
textual definitions provided by WordNet. First,
in order to determine the textual definition asso-
ciated to each word, the texts were tagged using

734



the maximum entropy POS tagger included in the
NLTK. Next, the adapted Lesk algorithm (Baner-
jee and Pedersen, 2002) for word sense disam-
biguation was applied in the texts disambiguating
one word at the time. The software package used
for this disambiguation process was pywsd2. The
arguments needed for the disambiguation of each
word are the POS tag of the target word and the
entire sentence as context. Once all the words are
disambiguated with their corresponding WordNet
synsets, each word is replaced by all the words in
their textual definition jointly with the same word
and its lemma. The final result of this stage is that
each text in the data set is replaced by a longer
text including the original text and some related
words. The motivation of this procedure is that the
extended versions of each pair of texts have more
chance of sharing common words that the original
texts.

The extended versions of these texts were used
to obtain another 17 features with the same proce-
dure described in the previous subsection (3.1.1).
This feature subset will henceforth be referred as
ESA.

3.1.3 Features for each part-of-speech
category

This set of features is motivated by the idea pro-
posed by Corley and Mihalcea (2005) of group-
ing words by their POS category before being
compared for semantic textual similarity. Our ap-
proach consist in provide a version of each text
pair in the data set for each POS category in-
cluding only the words belonging to that cate-
gory. For instance, the pair of texts {“A beauti-
ful girl is playing tennis”, “A nice and handsome
boy is playing football”} produce new pairs such
as: {“beautiful”, “nice handsome”} for the ADJ
tag, {“girl tennis”, “boy football”} for NOUN and
{“is playing”, “is playing”} for VERB.

Again, the POS tags were provided by the
NLTK’s max entropy tagger. The 28 POS cate-
gories were simplified to 9 categories in order to
avoid an excessive number of features and hence
sparseness; the used mapping is shown in Table 3.
Next, for each one of the 9 new POS categories a
set of 17 features (SM) is extracted reusing again
the method proposed in subsection 3.1.1. The only
difference with the method described in that sub-
section is that the stop-words were not removed

2https://github.com/alvations/pywsd

Reduced tag set NLTK’s POS tag set
ADJ JJ,JJR,JJS

NOUN NN,NNP,NNPS,NNS

ADV RB,RBR,RBS,WRB

VERB VB,VBD,VBG,VBN,VBP,VBZ

PRO WP,WP$,PRP,PRP$

PREP RP,IN

DET PDT,DT,WDT

EX EX

CC CC

Table 3: Mapping reduction of the POS tag set.

and the stemming process was not performed. The
motivation for generating this feature sets by POS
category is that the machine learning algorithms
could weight differently each category. The intu-
ition behind this is that it is reasonable that cat-
egories such as VERB and NOUN could play a
more important role for the task at hand than oth-
ers such as ADJ or PREP. Using these categorized
features, such discrimination among POS cate-
gories can be discovered from the training data.

Finally, the total number of features in this set is
153 (17 features× 9 POS categories). This feature
set will be referred as POS.

3.1.4 Features From Dependencies
The syntactic soft cardinality (Croce et al., 2012;
Croce et al., 2013) extend the soft cardinality
approach by representing texts as bags of de-
pendencies instead of bags of words. Each de-
pendency is a 3-tuple composed of two syntac-
tically related words and the type of their rela-
tionship. For instance, the sentence “The boy
plays football” can be represented with 3 depen-
dencies: [det,“boy”,“The”], [subj,“plays”,“boy”]
and [obj,“plays”,“football”]. Clearly, this repre-
sentation distinguish pairs of texts such as {“The
dog bites a boy”,“The boy bites a dog”}, which
are indistinguishable when they are represented as
bags of words. This representation can be obtained
automatically using the Stanford Parser (De Marn-
effe et al., 2006), which in addition provides a de-
pendency identifying the root word in a sentence.
We used the version 3.3.13 of that parser to obtain
such representation.

Once the texts are represented as bags of de-
pendencies, it is necessary to provide a similar-
ity function between two dependency tuples in or-

3http://nlp.stanford.edu/software/lex-parser.shtml

735



der to use the soft cardinality (eq. 1) and hence
to obtain the 17 cardinality features in Tables 1
and 2. Such function can be obtained using the
sim function (eq. 2) for comparing the first and
second words between the dependencies and even
the labels of the dependency types. Let’s consider
two dependencies tuples d = [ddep, dw1 , dw2 ] and
p = [pdep, pw1 , pw2 ] where ddep and pdep are the
labels of the dependency type; dw1 and pw1 are
the first words on each dependency tuple; and dw2
and pw2 are the second words. The similarity func-
tion for comparing two dependency tuples can be a
linear combination of the sim scores between the
corresponding elements of the dependency tuples
by the following expression:

simdep(d, p) =

γsim(ddep, pdep) + δsim(dw1 , pw2) + λsim(dw2 , pw2)

Although, it is unusual to compare the depen-
dencies’ type labels ddep and pdep with a similar-
ity function designed for words, we observed ex-
perimentally that this approach yield better overall
performance in the relatedness task in comparison
with a simple crisp comparison. The optimal val-
ues for the parameters γ = −3, δ = 10 and λ = 3
were determined with the same methodology used
in subsection 3.1.1 for determining α, β and bias.
Clearly, the fact that δ > λ means that the first
words in the dependency tuples plays a more im-
portant role than the second ones for the task at
hand. However, the fact that γ < 0 is counter intu-
itive because it means that the lower the similarity
between the dependency type labels is, the larger
the similarity between the two dependencies. Up
to date we have been unable to find a plausible ex-
planation for this phenomenon. This set of 17 fea-
tures will be referred hereinafter as DEP.

3.1.5 Additional Features
In addition to the feature sets based in soft car-
dinality, we designed some features aimed to ad-
dress linguistic phenomena such as antonymy, hy-
pernymy and negation.

Antonymy: Consider the following text pair
from the test data {“A man is emptying a container
made of plastic”,“A man is filling a container
made of plastic” }, which is labeled as a contra-
diction with a relatedness score of 3.91. Clearly,
these labels are explained by the antonymy rela-
tion between “emptying” and “filling”. Given that
none of the features presented above address this
issue, a list of 11,028 pairs of antonym words was

gathered from several web sites (see Table 4) and
from the antonymy relationships in WordNet, in
order to detect these cases. That list was used to
count the number of occurrences of pairs antonym
words between pairs of texts and in each one of
the texts. Thus, for any pair of texts A and B (rep-
resented as sets of words), three features (referred
henceforth as ANT) were extracted:

antonym AB Counts the number of occurrences
of pairs of antonyms in A × B (Cartesian
product) or in B ×A .

antonym AA Counts the number of occurrences
of pairs of antonyms in A×A.

antonym BB Counts the number of occurrences
of pairs of antonyms in B ×B.

Hypernymy: Consider the following text pair
from the test data {“A man is sitting comfortably
at a table”,“A person is sitting comfortably at the
table” }, which is labeled as an entailment with
a relatedness score of 3.96. In this case, the en-
tailment is based on the hypernymy between “per-
son” and “man”. In order to capture this linguis-
tic factor 3 features similar to the previously de-
scribed antonym features were proposed. First,
word sense disambiguation was performed (as de-
scribed in subsection 3.1.2) for obtaining a synset
label for each word. Secondly, we build a bi-
nary function hyp(ss1, ss2) that takes two Word-
Net synsets as arguments and returns 1 if ss1 is
a hypernym of ss2 with a maximum depth in the
WordNet’s is-a hierarchy of 6 steps, and 0 oth-
erwise. This hypernymy function was build us-
ing the WordNet interface provided by the NLTK.
Next, based on that synset-to-synset function, a
text-to-text function that captures the degree or hy-
pernymy in a text or in a pair of texts was build us-
ing the Monge-Elkan measure (Monge and Elkan,
1996). Thus, for two texts A and B represented
as sets of synset labels, the following expression
measures their degree of hypernymy:

HY P (A,B) =
1
|A|

|A|∑
i=1

|B|
max
j=1

hyp(ai, bj)

Using the function HY P (∗, ∗), 3 features are
extracted from each pair of text (referred hence-
forth as HYP):

hypernym AB from HY P (A,B)

736



http://www.myenglishpages.com/site php files/vocabulary-lesson-opposites-adjectives.php

http://www.allaboutspace.com/wordlist/opposites.shtml

http://www.michigan-proficiency-exams.com/antonym-list.html

http://examples.yourdictionary.com/examples-of-antonyms.html

http://www.synonyms-antonyms.com/antonyms.html

http://englishwilleasy.com/word-must-know/vocabulary/vocabulary-list-by-opposites-or-antonyms/

http://www.meridianschools.org/staff/districtcurriculum/moreresources/languagearts/all grades/antonyms.doc

http://mrsbrower.weebly.com/uploads/1/3/2/4/13243672/antonymlist.pdf

https://foxhugh.wordpress.com/word-lists/list-of-antonyms/

http://www.paulnoll.com/Books/Clear-English/English-antonyms-1.html

http://wordnet.princeton.edu/wordnet/download/

Table 4: URLs used for the list of 11,028 antonym pairs (accessed on March 20, 2014).

hypernym AA from HY P (A,A)

hypernym BB from HY P (B,B)

Negation: Negations play an important role in
the task at hand. For instance, consider this pair
of texts {“A person is rinsing a steak with wa-
ter”,“A man is not rinsing a large steak”} labeled
as a contradiction. In that example the negation of
the verb “rising” is the main factor of contradic-
tion. In order to capture this linguistic feature we
build a simple function that detects the occurrence
of a verb negation if the text contains one of the
following words: “not”, “n’t”, “nor”, “null”, “nei-
ther”, “either”, “barely”, “scarcely” and “hardly”.
Similarly, noun negation is detected looking for
the words: “no”, “none”, “nobody”, “nowhere”,
“nothing” and “never”. Thus, for two texts A and
B, 4 features are extracted (referred henceforth as
NEG):

verb neg A if verb negation is detected in A

verb neg B if verb negation is detected in B

noun neg A if noun negation is detected in A

noun neg B if noun negation is detected in B

3.1.6 Submitted Runs and Results
RUN1 (PRIMARY) This system produced pre-
dictions by extracting all the features described
previously (SM, ESA, POS, DEP, ANT,
HYP and NEG) from all the texts in the SICK
data set. Next, two machine learning models were
obtained (WEKA (Hall et al., 2009) was used
for that) using the training part of SICK, one for
regression (relatedness) and another for classifi-
cation (entailment). The regression model was

a reduced-error pruning tree (REPtree) (Quin-
lan, 1987) boosted with 20 iterations of bagging
(Breiman, 1996). The classification model was a
J48Graft tree also boosted with 20 bagging itera-
tions. These two models produced the predictions
for the test part of SICK.

RUN2 This system is similar to the one used
in RUN1, but it used only the feature sets SM and
NEG. Another difference is that a linear regres-
sion was used instead of the REPtree and no bag-
ging was performed.

RUN3 The same as RUN1, but again, linear
regression was used instead of the REPtree and no
bagging was performed.

RUN4 The same as RUN2, but the models
were boosted with 20 iterations of bagging.

RUN5 The same as RUN3, but 30 iterations of
bagging were used instead of 20.

The official results obtained by these systems
(prefixed UNAL-NLP) are shown in Table 5
jointly with those obtained by other 3 top sys-
tems among the 18 participating systems. Our
primary run (RUN1) obtained pretty competitive
results ranking 3th and 4th in the entailment and
relatedness tasks. The RUN4 obtained a remark-
able performance (it would be ranked 6th for en-
tailment and 8th for relatedness) in spite of the
fact that is a system purely based on string match-
ing. The comparison of our runs 1, 3 and 5, which
mainly differs by the use of bagging, shows that
this boosting method provides considerable im-
provements. In fact, comparing RUN3 (all fea-
tures, no bagging) and RUN4 (SM and NEG fea-
ture sets boosted with bagging), they performed
similarly in spite of the considerable larger num-
ber of features used in RUN3. Besides, the RUN5
slightly outperformed our primary run (RUN1) us-

737



Entailment Relatedness
system accuracy official rank Pearson Spearman MSE official rank

UNAL-NLP run1 (primary) 83.05% 3rd/18 0.8043 0.7458 0.3593 4th/17

UNAL-NLP run2 79.81% - 0.7482 0.7033 0.4487 -

UNAL-NLP run3 80.15% - 0.7747 0.7286 0.4081 -

UNAL-NLP run4 80.21% - 0.7662 0.7142 0.4210 -

UNAL-NLP run5 83.24% - 0.8070 0.7489 0.3550 -

ECNU run1 83.64% 2nd/18 0.8280 0.7689 0.3250 1st/17
Stanford run5 74.49% 12th/18 0.8272 0.7559 0.3230 2nd/17

Illinois-LH run1 84.58% 1st/18 0.7993 0.7538 0.3692 5th/17

Table 5: Results for task 1.

ing 10 additional iterations of bagging.

3.1.7 Error Analysis
Our primary run for the task 1 failed in 835 pairs of
sentences out of 4,927 in the entailment subtask.
We wanted to understand in why our system failed
in these 835 instances, so we classified manually
these instances in 4 error categories (each instance
could be assigned to several categories).

Paraphrase not detected (NP): exam-
ple={“Two groups of people are playing football”,
“Two teams are competing in a football match”},
gold standard=entailment, prediction=neutral,
number of occurrences= 420 (50.3%). The system
failed to detect the paraphrase between “groups of
people” and “teams”.

Negation not detected (NN) : exam-
ple={“There is no one playing the guitar”,
“Someone is playing the guitar”}, gold stan-
dard=contradiction, prediction=neutral, number
of occurrences=94 (11.3%). The system failed to
detect that the contradiction is due to the negation
in the first text.

False similarity between words (NSS) : ex-
ample={“Two dogs are playing by a tree”,
“Two dogs are sleeping by a tree”}, gold stan-
dard=neutral, prediction=entailment, number of
occurrences=413 (49.5%). The only difference
between these 2 sentences is the gerund “playing”
vs. “sleeping”, which the system erroneously con-
sidered as similar.

Antonym not detected (NA): exam-
ple={“Three children are running down hill”,
“Three children are running up hill”}, gold
standard=contradiction, prediction=entailment,
number of occurrences=40 (4.8%). The only
difference between these 2 sentences is the
words “down” vs. “up”. In spite that this pair
of antonyms was included in the antonym list,

Error category NP NN NSS NA
NP 420 5 125 0

NN - 94 1 0

NSS - - 413 22

NA - - - 40

Table 6: Co-ocurrences of types of errors in RUN1
(task1).

the system failed to distinguish the contradiction
between the texts.

The matrix in Table 6 reports the number of
co-occurrences of error categories in the 835 in-
stances erroneously classified.

3.2 Task 3: Cross-level Semantic Similarity
The SemEval 2014 task 3 (cross-level semantic
similarity) (Jurgens et al., 2014) proposed the se-
mantic textual similarity task but across differ-
ent textual levels, namely paragraph-to-sentence,
sentence-to-phrase, phrase-to-word and word-to-
sense. As usual, the goal is to predict the gold sim-
ilarity scores for each pair of texts. For each one
of these cross-level comparison types there were
proposed a separated training and test data sets.
Basically, we addressed this task using the set of
features SM presented in subsection 3.1.1 in com-
bination with a text expansion approach similar to
the method presented in subsection 3.1.2.

3.2.1 Paragraph-to-sentence and
Sentence-to-phrase

For these two cross-level comparison types we
extracted the SM feature set using the pro-
vided texts. The model parameters obtained for
paragraph-to-sentence were α = 0.1, β = 1.75,
bias = −1.35, p = 1.55; and for sentence-to-
phrase were α = 0.68, β = 0.92, bias = −0.92,
p = 2.49.

738



The system for the RUN2 used the SM fea-
ture set and a machine learning model build with
the provided training data for generating the simi-
larity score predictions for the test data. For the
paragraph-to-sentence data set the model was a
REPtree for regression boosted with 40 bagging it-
erations. Similarly, the model for the sentence-to-
phrase data set was a linear regressor also boosted
with 40 bagging iterations.

Unlike RUN2, RUN1 does not make use of any
machine learning algorithm. Instead, we used the
only the basic cardinalities (see Table 1) from the
SM feature set in combination with an ad-hoc re-
semblance coefficient, i.e. the Dice’s coefficient
2|A∩B|/|A|+|B| for the paragraph-to-sentence data
set. In turn, for sentence-to-phrase the overlap co-
efficient, i.e. |A∩B|/min[|A|,|B|], was used.

3.2.2 Phrase-to-word and Word-to-sense
Before applying the same procedure used in the
previous subsection, the texts in the phrase-to-
word and word-to-sense data sets were expanded
with a similar approach to that was used in subsec-
tion 3.1.2.

Phrase-to-word expansion: First, the “word”
was expanded finding its corresponding WordNet
synset using the adapted Lesk’s algorithm provid-
ing as context the “phrase”. Then, once the word’s
synset is obtained, the “word” text is extended
with the textual definition of the synset. Simi-
larity, this procedure is repeated for each word in
the “phase” obtaining and extended version of the
phrase. Finally, these two texts are used for ex-
tracting the SM feature set. The model param-
eters were α = 0.8, β = 1.9, bias = −0.8,
p = 1.5.

Word-to-sense expansion: First, the “sense”
(i.e. synset) is replaced by its textual definition
and its lemma. At this point the pair word-sense
becomes a pair word-sentence. Then, the synset
of the “word” is obtained performing the adapted
Lesk’s algorithm. Next, the “word” is extended
with textual definition of the synset. Finally, these
two texts are used for extracting the SM feature
set obtaining the following model parameters were
α = 0.59, β = 0.9, bias = −0.89, p = 3.91.
3.2.3 Results
The official results obtained by the two submitted
runs jointly with other 3 top systems are shown in
Table 7. Our submissions (prefixed with UNAL-
NLP) ranked 3rd and 5th among 38 participating

test data train data
OnWN (en) OnWN 2012/2013 test

headlines (en) headlines 2013 test

images (en) MSRvid 2012 train and test

deft-news (en) MSRpar 2013 train and test

deft-forum (en)
MSRvid 2012 train and test

OnWN 2012/2013 test

tweet-news (en)
SMTeuroparl 2012 test

SMTnews 2012 test

Wikipedia (es) SMTeuroparl 2012 train

news (es) SMTeuroparl 2012 train

Table 8: Training data used for the STS-2014 data
sets (task 10).

systems, showing that the SM (string-matching)
feature set is effective for the prediction of sim-
ilarity scores. Particularly, in the paragraph-to-
sentence data set, which has the longest text,
RUN2 obtained the best official score. In contrast,
the scores obtained for the phrase-to-word and
word-to-sense data sets were considerably lower
in comparison with the top system, but still com-
petitive against most of the other participating sys-
tems.

3.3 Task 10: Multilingual Semantic
Similarity

The SemEval-2014 task 10 (multilingual seman-
tic similarity) (Agirre et al., 2014) is the sequel of
the semantic textual similarity (STS) evaluations
at SemEval in the past two years (Agirre et al.,
2012; Agirre et al., 2013). This year 6 test data
sets were proposed in English and 2 data sets in
Spanish. Similarly to the 2013 campaign, there is
not explicit training data for each data set. Conse-
quently, different data sets from the previous STS
evaluations were selected to be used as training
data for the new data sets. The selection criterion
was the average character length and type of the
texts. The Table 8 shows the training data used for
each test data set.

3.3.1 English Subtask
The RUN1 for the English data sets was produced
with a parameterized similarity function based on
the SM feature set and the symmetrized Tversky’s
index (Tversky, 1977; Jimenez et al., 2013a). For
a detailed description of this function and its pa-
rameters, please refer to the STSsim feature in
the system description paper of the NTNU team
(Lynum et al., 2014). The parameters used in that

739



System Para-2-Sent Sent-2-Phr Phr-2-Word Word-2-Sense Official Rank
SimCompass run1 0.811 0.742 0.415 0.356 1st/38

ECNU run1 0.834 0.771 0.315 0.269 2nd/38
UNAL-NLP run2 0.837 0.738 0.274 0.256 3rd/38

SemantiKLUE run1 0.817 0.754 0.215 0.314 4th/38

UNAL-NLP run1 0.817 0.739 0.252 0.249 5th/38

Table 7: Official results for task 3 (Pearson’s correlation).

Data α β bias p α′ β′ bias′

OnWN 0.53 -0.53 1.01 1.00 -4.89 0.52 0.46

headlines 0.36 -0.29 4.17 0.85 -4.50 0.43 0.19

images 1.12 -1.11 0.93 0.64 -0.98 0.50 0.11

deft-news 3.36 -0.64 1.37 0.44 2.36 0.72 0.02

deft-forum 1.01 -1.01 0.24 0.93 -2.71 0.42 1.63

tweet-news 0.13 0.14 2.80 0.01 2.66 1.74 0.45

Table 9: Optimal parameters used for task 10 in
English.

function are reported in Table 9. Unlike subsec-
tion 3.1.1 where the Dice’s coefficient was used as
the text similarity function, here the symmetrical
Tversky’s index (eq. 2) was reused generating the
three additional parameters marked with apostro-
phe (α′, β′ and bias′).

For the RUN2 the SM feature set was extracted
from all the data sets in English (en) listed in Table
8. Then, a REPtree (Quinlan, 1987) boosted with
50 bagging iterations (Breiman, 1996) was trained
using the training data sets selected for each test
data set. Finally, these machine learning models
produced the similarity score predictions for each
test data set.

The RUN3 was identical to the RUN2 but in-
cluded additional feature sets apart from SM,
namely: ESA, POS and WN. The WN feature
set is the same as SM, but replacing the word-to-
word similarity function in eq. 2 by the path mea-
sure from the WordNet::Similarity package (Ped-
ersen et al., 2004).

3.3.2 Spanish Subtask
The Spanish system was based entirely in the SM
feature set with some small changes for adapt-
ing the system to Spanish. Basically, the list of
English stop-words was replaced by the Spanish
stop-words provided by the NLTK. In addition,
the Porter stemmer was replaced by its Spanish
equivalent, i.e. the Snowball stemmer for Span-
ish. The RUN1 is equivalent to the RUN1 for the

data set run1 run2 run3
deft-forum 0.5043 0.3826 0.4607

deft-news 0.7205 0.7305 0.7216

headlines 0.7616 0.7645 0.7605

images 0.8071 0.7706 0.7782

OnWN 0.7823 0.8268 0.8426

tweet-news 0.6145 0.4028 0.6583

mean (en) 0.7113 0.6573 0.7209

official rank (en) 12th/38 22th/38 9th/38

Wikipedia 0.7804 0.7566 0.6894
news 0.8154 0.7829 0.7965

mean (es) 0.8013 0.7723 0.7533

official rank (es) 3rd/22 9th/22 12th/22

Table 10: Official results for the task 10 (Pearson’s
correlation).

English subtask described in the previous subsec-
tion. The parameters used for the text similarity
function were α = 1.16, β = 1.08, bias = 0.02,
p = 1.02, α′ = 1.54, β′ = 0.08 and bias′ = 1.37.
The description and meaning of these parameters
can be found in (Lynum et al., 2014) associated to
the STSsim feature.

The RUN2 was obtained using the SM feature
set and a linear regressor for generating the simi-
larity score predictions. Similarity, RUN3 used the
same feature set SM in combination with a REP-
tree boosted with 30 bagging iterations.

3.3.3 Results

The results for the 3 submitted runs correspond-
ing to the 2 sub tasks (English and Spanish) are
shown in Table 10. It is important to note that
the RUN1 for the Wikipedia data set in Spanish
was the top system among 22 participating sys-
tems. This result is remarkable given that this sys-
tem was trained with a data set in English showing
the domain adaptation ability of the soft cardinal-
ity approach.

740



4 Conclusions

We participated in the SemEval-2014 task 1, 3 and
10 with an uniform approach based on soft cardi-
nality features, obtaining pretty satisfactory results
in all data sets, tasks and sub tasks. This approach
has been used since SemEval-2012 in all versions
of the following tasks: semantic textual similar-
ity (Jimenez et al., 2012b; Jimenez et al., 2013a),
typed similarity (Croce et al., 2013), cross-lingual
textual entailment (Jimenez et al., 2012a; Jimenez
et al., 2013c), student response analysis (Jimenez
et al., 2013b), and multilingual semantic textual
similarity (Lynum et al., 2014). In the majority
of the cases, the systems based on soft cardinality,
built by us and other teams, have been among the
top systems. Given the uniformity of the approach,
the consistency of the results, the few computa-
tional resources required and the overall concep-
tual simplicity, the soft cardinality is established
as a useful tool for a wide spectrum of applications
in natural language processing.

References
Eneko Agirre, Daniel Cer, Mona Diab, and Gonzalez-

Agirre Aitor. 2012. SemEval-2012 task 6: A pilot
on semantic textual similarity. In Proceedings of the
6th International Workshop on Semantic Evaluation
(SemEval@*SEM 2012), Montreal,Canada.

Eneko Agirre, Daniel Cer, Mona Diab, Aitor Gonzalez-
Agirre, and Weiwei Guo. 2013. *SEM 2013 shared
task: Semantic textual similarity, including a pilot
on typed-similarity. Atlanta, Georgia, USA.

Eneko Agirre, Carmen Banea, Claire Cardie, Daniel
Cer, Mona Diab, Aitor Gonzalez-Agirre, Weiwei
Guo, Rada Mihalcea, German Rigau, and Janyce
Weibe. 2014. SemEval-2014 task 10: Multilingual
semantic textual similarity. In Proceedings of the
8th International Workshop on Semantic Evaluation
(SemEval-2014), Dublin, Ireland, August.

Satanjeev Banerjee and Ted Pedersen. 2002. An
adapted lesk algorithm for word sense disambigua-
tion using WordNet. In Computational linguis-
tics and intelligent text processing, page 136–145.
Springer.

Leo Breiman. 1996. Bagging predictors. Machine
Learning, 24(2):123–140.

Courtney Corley and Rada Mihalcea. 2005. Measur-
ing the semantic similarity of texts. In Proceedings
of the ACL Workshop on Empirical Modeling of Se-
mantic Equivalence and Entailment, EMSEE ’05,
page 13–18, Stroudsburg, PA, USA.

Danilo Croce, Valerio Storch, P. Annesi, and Roberto
Basili. 2012. Distributional compositional seman-
tics and text similarity. In 2012 IEEE Sixth Interna-
tional Conference on Semantic Computing (ICSC),
pages 242–249, September.

Danilo Croce, Valerio Storch, and Roberto Basili.
2013. UNITOR-CORE TYPED: Combining text
similarity and semantic filters through SV regres-
sion. In Second Joint Conference on Lexical and
Computational Semantics (*SEM), Volume 1: Pro-
ceedings of the Main Conference and the Shared
Task: SemanticTextual Similarity, page 59, Atlanta,
Georgia, USA.

Marie-Catherine De Marneffe, Bill MacCartney, and
Christopher D. Manning. 2006. Generating typed
dependency parses from phrase structure parses. In
Proceedings of LREC, volume 6, page 449–454,
Genoa, Italy, May.

Evgeniy Gabrilovich and Shaul Markovitch. 2007.
Computing semantic relatedness using wikipedia-
based explicit semantic analysis. In Proceedings of
the 20th International Joint Conference on Artifical
Intelligence, IJCAI’07, page 1606–1611, San Fran-
cisco, CA, USA. Morgan Kaufmann Publishers Inc.

Mark Hall, Frank Eibe, Geoffrey Holmes, and Bern-
hard Pfahringer. 2009. The WEKA data min-
ing software: An update. SIGKDD Explorations,
11(1):10–18.

Paul Jaccard. 1901. Etude comparative de la distribu-
tion florare dans une portion des alpes et des jura.
Bulletin de la Société Vaudoise des Sciences Na-
turelles, pages 547–579.

Sergio Jimenez, Fabio Gonzalez, and Alexander Gel-
bukh. 2010. Text comparison using soft cardi-
nality. In Edgar Chavez and Stefano Lonardi, ed-
itors, String Processing and Information Retrieval,
volume 6393 of LNCS, pages 297–302. Springer,
Berlin, Heidelberg.

Sergio Jimenez, Claudia Becerra, and Alexander Gel-
bukh. 2012a. Soft cardinality: A parameterized
similarity function for text comparison. In SemEval
2012, Montreal, Canada.

Sergio Jimenez, Claudia Becerra, and Alexander Gel-
bukh. 2012b. Soft cardinality+ ML: Learning adap-
tive similarity functions for cross-lingual textual en-
tailment. In SemEval 2012, Montreal, Canada.

Sergio Jimenez, Claudia Becerra, and Alexander Gel-
bukh. 2013a. SOFTCARDINALITY-CORE: Im-
proving text overlap with distributional measures for
semantic textual similarity. In *SEM 2013, Atlanta,
Georgia, USA, June.

Sergio Jimenez, Claudia Becerra, and Alexander Gel-
bukh. 2013b. SOFTCARDINALITY: Hierarchical
text overlap for student response analysis. In Se-
mEval 2013, Atlanta, Georgia, USA, June.

741



Sergio Jimenez, Claudia Becerra, and Alexander Gel-
bukh. 2013c. SOFTCARDINALITY: Learning
to identify directional cross-lingual entailment from
cardinalities and SMT. In SemEval 2013, Atlanta,
Georgia, USA, June.

Karen Spärck Jones. 2004. A statistical interpretation
of term specificity and its application in retrieval.
Journal of Documentation, 60(5):493–502, October.

David Jurgens, Mohammad T. Pilehvar, and Roberto
Navigli. 2014. SemEval-2014 task 3: Cross-
level semantic similarity. In Proceedings of the
8th International Workshop on Semantic Evaluation
(SemEval-2014), Dublin, Ireland, August.

André Lynum, Partha Pakray, Björn Gambäck, and
Sergio Jimenez. 2014. NTNU: Measuring se-
mantic similarity with sublexical feature represen-
tations and soft cardinalty. In Proceedings of the
8th International Workshop on Semantic Evaluation
(SemEval-2014), Dublin, Ireland, August.

Marco Marelli, Stefano Menini, Marco Baroni, Lucia
Bentivogli, Raffaella Bernardi, and Roberto Zam-
parelli. 2014. A SICK cure for the evaluation of
compositional distributional semantic models. In
Proceedings of LREC, Reykjavik, Iceland, May.

Alvaro E. Monge and Charles Elkan. 1996. The field
matching problem: Algorithms and applications. In
Proceeding of the 2nd International Conference on
Knowledge Discovery and Data Mining (KDD-96),
pages 267–270, Portland, OR.

Ted Pedersen, Siddharth Patwardhan, and Jason Miche-
lizzi. 2004. WordNet::similarity: measuring the
relatedness of concepts. In Proceedings HLT-
NAACL–Demonstration Papers, Stroudsburg, PA,
USA.

Martin Porter. 1980. An algorithm for suffix stripping.
Program, 3(14):130–137, October.

J. Ross Quinlan. 1987. Simplifying decision
trees. International journal of man-machine studies,
27(3):221–234.

Amos Tversky. 1977. Features of similarity. Psycho-
logical Review, 84(4):327–352, July.

742


