















































Cross-Language Entity Linking


Proceedings of the 5th International Joint Conference on Natural Language Processing, pages 255–263,
Chiang Mai, Thailand, November 8 – 13, 2011. c©2011 AFNLP

Cross-Language Entity Linking

Paul McNamee and James Mayfield
HLTCOE & Applied Physics Laboratory

Johns Hopkins University
{paul.mcnamee,james.mayfield}@jhuapl.edu

Dawn Lawrie
Loyola University Maryland

lawrie@cs.loyola.edu

Douglas W. Oard
iSchool & UMIACS

University of Maryland, College Park
oard@umd.edu

David Doermann
UMIACS

University of Maryland, College Park
doermann@umiacs.umd.edu

Abstract
There has been substantial recent interest
in aligning mentions of named entities in
unstructured texts to knowledge base de-
scriptors, a task commonly called entity
linking. This technology is crucial for
applications in knowledge discovery and
text data mining. This paper presents ex-
periments in the new problem of cross-
language entity linking, where documents
and named entities are in a different lan-
guage than that used for the content of
the reference knowledge base. We have
created a new test collection to evaluate
cross-language entity linking performance
in twenty-one languages. We present ex-
periments that examine issues such as: the
importance of transliteration; the utility
of cross-language information retrieval;
and, the potential benefit of multilingual
named entity recognition. Our best model
achieves performance which is 94% of a
strong monolingual baseline.

1 Introduction

Entity Linking involves aligning a textual mention
of a named entity to the entry in a knowledge base
(KB) that represents the mentioned entity, if it is
present. The problem has two main complicating
features: entities can be referred to using multi-
ple name variants (e.g., aliases or misspellings);
and several entities can share the same name (e.g.,
many people are named Marı́a Sánchez). Ap-
plications of entity linking include linking pa-
tient health records from separate hospitalizations,
maintaining personal credit files, preventing iden-
tity crimes, and supporting law enforcement.

Starting in 2009 the NIST Text Analysis
Conference (TAC) began conducting evaluations

of technologies for knowledge base population
(KBP). Systems addressing the entity linking sub-
task take as input a name string from a document
and produce as output the knowledge base node, if
any, corresponding to the mentioned entity. This
capability is vital for knowledge discovery; with-
out it, extracted information cannot be properly in-
serted in the correct KB node. We follow the TAC-
KBP problem formulation and use its reference
knowledge base, which was derived from a 2008
snapshot of English Wikipedia. In the present
work our focus is on person entities. We seek
to develop and evaluate technologies for matching
foreign language names to the appropriate knowl-
edge base descriptor (or kbid) in the English KB.

To support this research we created what we
believe to be the first cross-language entity link-
ing test collection. Our dataset includes twenty-
one languages in addition to English, and covers
five writing systems. Compared to the problem of
monolingual (English) entity linking, a solution to
the cross-language variant requires both a method
to match foreign names to their English equiva-
lents, and a way to compare contextual features
from the non-English source document with con-
textual information about known entities stored in
the KB. Figure 1 illustrates the process.

The rest of this paper is structured as follows. In
Section 2 we discuss related work in entity link-
ing and cross-language name matching. In Sec-
tion 3 we present our approach to monolingual en-
tity linking and describe the adaptations that are
required to address the cross-language problem.
Section 4 discusses the TAC-KBP evaluation and
the construction of our test collection. Sections 5
and 6 present experiments exploring the effects of
transliteration and cross-language content match-
ing on the problem. Section 7 summarizes the
main contributions of this work.

255



Figure 1: Linking an Arabic query referring to
Tony Blair to a Wikipedia-derived KB using name
matching and context matching features.
〈entity wiki title=“Tony Blair” type=“PER” id=“E0481157” name=“Tony Blair”〉
〈facts class=“Infobox Prime Minister”〉
〈fact name=“honorific-prefix”〉The Right Honourable〈/fact〉
〈fact name=“name”〉Tony Blair〈/fact〉
〈fact name=“order”〉Prime Minister of the United Kingdom〈/fact〉
〈fact name=“monarch”〉〈link entity id=“E0699345”〉Elizabeth II〈/link〉〈/fact〉
〈fact name=“predecessor”〉〈link entity id=“E0614790”〉John Major〈/link〉〈/fact〉
〈fact name=“successor”〉〈link entity id=“E0455080”〉Gordon Brown〈/link〉〈/fact〉
〈fact name=“birth date”〉6 May 1953 (1953-05-06) (age56)〈/fact〉
〈fact name=“spouse”〉〈link entity id=“E0629105”〉Cherie Booth〈/link〉〈/fact〉
〈fact name=“children”〉Euan, Nicholas, Kathryn, Leo〈/fact〉
〈fact name=“alma mater”〉〈link〉St John’s College, Oxford〈/link〉〈/fact〉
〈fact name=“profession”〉〈link entity id=“E0701822”〉Lawyer〈/link〉〈/fact〉
〈/facts〉
〈wiki text〉〈![CDATA[Tony Blair
Anthony Charles Lynton “Tony” Blair (born 6 May 1953) is a British politician who was
Prime Minister of the United Kingdom from 2 May 1997 to 27 June 2007. He was Leader
of the Labour Party from 1994 to 2007 and the Member of Parliament for Sedgefield from
1983 to 2007...

Figure 2: Excerpt from the KB entry for Tony
Blair (E0481157). From LDC2009E58.

2 Related Work

Three types of named entity resolution are found
in the literature: identity resolution, which
matches structured or semi-structured entity de-
scriptions, such as database records; coreference
resolution, which clusters textual entity descrip-
tions; and entity linking, which matches a textual
description to a structured or semi-structured de-
scription. All three types of resolution have signif-
icant literature on monolingual processing; cross-
language matching is less well studied.

Identity resolution and its closely related cousin
record linkage grew out of the database commu-
nity, which needs to determine when two database
records represent the same entity. When matching
records are found, identity resolution merges the
two records, while record linkage simply notes the
correspondence. Brizan and Tansel (2006) present
a short overview of work in these fields. Typical
approaches combine algorithmic matching of indi-
vidual column values with hand-coded heuristics
to combine the column scores and threshold the

result.

Coreference resolution operates over text, deter-
mining when two entity mentions refer to the same
entity. Approaches to within-document corefer-
ence resolution typically exploit syntactic, gram-
matical and discourse-level features, information
that is not available when trying to resolve ref-
erences across documents. Ng (2010) presents
a comprehensive review of recent approaches to
within-document coreference resolution. In con-
trast, cross-document coreference resolution typ-
ically assumes that within-document references
have been resolved, and tries to place all such
mention chains that refer to the same entity into
a single cluster that represents that entity. Because
the kinds of document-specific features that guide
within-document coreference resolution are miss-
ing, research in cross-document coreference reso-
lution tends to be more directly applicable to entity
linking (which also lacks those features). The Web
People Search Evaluation Workshop (Artiles et al.,
2010) has been one of the recent drivers of re-
search in cross-document coreference resolution,
defining a clustering task that groups Web pages
for multiple people bearing the same name.

Entity linking is a hybrid of the preceding two
types of named entity resolution, matching a tex-
tual entity mention to a set of structured entity rep-
resentations (usually called the knowledge base).
Ji and Grishman (2011) present a good overview
of the state of the art in monolingual entity linking,
as practiced in the TAC evaluation. TAC data sets
use a subset of Wikipedia entities for the knowl-
edge base, manually curated query names, and
ground truth identified by human assessors with-
out pooling. Wikipedia has been another signif-
icant source of training and test data. Adafre and
de Rijke (2005) explore automatically adding links
between Wikipedia pages (albeit without focus-
ing specifically on named entities). Bunescu and
Pasca (2006) trained an SVM to predict whether a
query entity matches a Wikipedia page by using
hyperlinks within Wikipedia itself as the source
of training and test data. Cucerzan (2007) stud-
ied identifying entity mentions in text and map-
ping them to Wikipedia articles. Mihalcea and
Csomai (2007) and Milne and Witten (2008) each
attempt to identify and properly induce hyper-
links for informative terms in Wikipedia articles
(again without specific focus on named entities).
Cross-language entity linking has not yet been

256



widely explored. TAC1 and NTCIR.2 have both
for the first time announced plans for shared tasks
for cross-language entity linking. Steinberger
and Pouliquen (2007) describe a system that uses
multilingual named entity recognition and cross-
language name matching to automatically analyze
tens of thousands of news stories daily; however,
they do not conduct a formal evaluation of their
name merging algorithm.

Contributing to each of these three kinds of
named entity resolution are two essential un-
derlying technologies: name matching and con-
text matching. In name matching we ask the
question, “Do two different strings represent the
same name?” For example, we might like to
know whether “Gadhafi” and “Khadafy” are two
spellings of the same name. When used as a fea-
ture for machine learning, we ask the related ques-
tion, “How similar are two name strings?”

Cross-language name matching is closely re-
lated to name transliteration. Indeed, transliterat-
ing a name to the language of the knowledge base,
then performing monolingual name matching in
that language, is a reasonable approach to cross-
language name matching. Name transliteration
has an extensive literature; Karimi et al. (2011)
present a comprehensive survey of the topic.

Name matching does not demand translitera-
tion though; transliteration is a generative pro-
cess, and name matching requires only that a
known name pair be given a score represent-
ing the degree of match. Snae (2007) presents
a survey of popular name matching algorithms
from the record linkage perspective. Monolin-
gually, Levenshtein distance (1966) and its vari-
ants are used for basic string matching in many
contexts. Cross-language approaches typically
combine cross-language mappings of some sort
with edit distance metrics. For example, Mani
et al. (2008) demonstrate a machine learning ap-
proach to the problem.

The second crucial underlying technology is
context matching. Monolingually, context match-
ing can match on many contextual attributes, in-
cluding words, entities, topics, or graph struc-
tures. Context matching in the translingual set-
ting is closely related to cross-language informa-
tion retrieval (CLIR); both tasks attempt to esti-
mate the degree of similarity between texts written

1http://nlp.cs.qc.cuny.edu/kbp/2011/
2http://ntcir.nii.ac.jp/CrossLink/

in different languages. Kishida (2005) presents an
overview of the key methods in CLIR.

3 Cross-Language Entity Linking

Our approach to entity linking breaks the problem
down into two main parts: candidate identifica-
tion and candidate ranking. Candidate identifica-
tion quickly identifies a small set of KB nodes that
with high probability contain the correct answer,
if it is present. Candidate ranking then consid-
ers each candidate in greater detail, producing a
ranked list. We give a description of each of these
steps in this section; complete details of our En-
glish entity linking approach, including descrip-
tions of all of the features used and performance
on the TAC-KBP datasets can be found in (Mc-
Namee, 2010).

3.1 Candidate Identification

As a KB may contain a large number of entries, we
prefer to avoid brute force comparisons between
the query and all KB entities. To identify the en-
tries that might reasonably correspond to the input
named entity, we rely on a set of fast name match-
ing techniques. We have found that it is possible
to achieve high recall without resorting to contex-
tual features. We create indexes for the names in
the KB to support fast lookup of potential matches.
The specific techniques that we use include:

• Exact match of query and candidate names
• Known alias or nickname lookup
• Number of character 4-grams in common be-

tween query and candidate

• Sum of IDF-weighted words in common be-
tween query and candidate3

In tests on the TAC-KBP 2009 test collection,
this approach achieved 97.1% recall. For only
2.9% of the queries, the proper KB referent for the
query was not one of the candidates. These cases
were particularly challenging because they in-
volved ambiguous organization names or obscure
personal nicknames. Our methods are similar to
methods used in the database community, some-
times known as blocking (Whang et al., 2009) or
canopies (McCallum et al., 2000).

3Inverse document frequency weights enable us to ef-
fectively match, for example, Q: Mary Elizabeth Surratt and
KB: Mary Surratt, since Surratt is a highly discriminating
term even though Mary is not.

257



Chinese 306165 Czech 6101
German 34101 Finnish 5639
French 23834 Swedish 5526
Arabic 19347 Danish 2648
Bulgarian 17383 Turkish 2581
Spanish 14406 Macedonian 2469
Italian 12093 Romanian 1981
Dutch 10853 Croatian 1527
Serbian 10020 Urdu 987
Greek 9590 Albanian 257
Portuguese 6335

Table 1: Number of training pairs for transliterat-
ing to English from each language.

To perform cross-language candidate identifica-
tion, we transliterate4 the query name to English,
then apply our monolingual English heuristics.
We used the multilingual transliteration system
and training data developed by Irvine et al. (2010)
in their experiments in orthographic translitera-
tion. The number of training name/transliteration
pairs varied by language and is given in Ta-
ble 1. The source for most of this training data
is Wikipedia, which contains links between article
pages in multiple languages.

3.2 Candidate Ranking

The second phase in our approach is to score each
viable candidate using supervised machine learn-
ing, and to select the highest scoring one as output.
Each entity linking query is represented by a fea-
ture vector x, where x ∈ Rk, and each candidate y
is a member of Y, the set of entities in the knowl-
edge base. Individual feature functions, fi(x, y),
are based on intrinsic properties of the query x,
intrinsic properties of a specific KB candidate y,
and most commonly, on comparisons between the
query and candidate. For each query our goal is
to select a single KB entity y or choose NIL if the
mentioned entity is not represented in the KB.

Thus, we desire that the correct knowledge base
entity y′ for a query x receives a higher score than
any other knowledge base entities y ∈ Y, y 6= y′.
We chose a soft maximum margin approach to
learning and used the ranking Support Vector Ma-
chine approach described by Joachims (2002) and
implemented in the SVMrank tool. We selected a
linear kernel for training speed, and set the slack
parameter C to be 0.01 times the number of train-
ing examples.

In our system absence from the knowledge base
4We use transliteration in a broad sense, to include situa-

tions where word translation rather than character translitera-
tion is warranted.

is treated as a distinct ranked candidate, the so-
called NIL candidate. NIL prediction is integrated
into the process by including features that are in-
dicative of no other candidate being correct. Con-
sidering absence as a ranked candidate eliminates
the need to select a threshold below which NIL
will be returned.

The classes of feature functions we use include:

• Name matching features between the query
name (Qname) and KB candidate (KBname)
• Text comparisons between the query docu-

ment (Qdoc) and the text associated with the
KB candidate
• Relation features, chiefly evidence from rela-

tions in the KB being evidenced in the Qdoc
• Co-occurring entities, detected by running

named entity recognition (NER) on the Qdoc
and finding matching names in the candi-
date’s KB entry
• Features pertaining to the entity type of the

KB candidate
• Indications that no candidate is correct and

that NIL is therefore the appropriate response

3.2.1 Name matching
A variety of string similarity features are incorpo-
rated to account for misspellings, name variants,
or partially specified names when trying to match
the query name and KB entry. Christen (2006) dis-
cusses a variety of name matching features, sev-
eral of which we adopt. One of the most useful is
the Dice score over sets of character bigrams.

3.2.2 Cross-language name equivalence
In all of our cross-language experiments we added
name matching features designed to directly cal-
culate the likelihood that a given non-English
name is equivalent to a given English name. The
model is based on projections of character n-grams
across languages (McNamee, 2008).

3.2.3 Contextual Similarity
We measure monolingual document similarity be-
tween Qdoc and the KB text (KBdoc) in two ways:
using cosine similarity with TF/IDF weighting;
and using the Dice coefficient over bags of words.
IDF values are approximated using counts from
the Google 5-gram dataset following the method
of Klein and Nelson (2008). We also used fea-
tures such as whether the query string occurs in
the KBdoc and the KBname occurs in the Qdoc.

258



To match contexts when the query document
and KB are in different languages we treat cross-
language context linking as a CLIR problem in
which the query is created from the words in the
vicinity of mentions of the query name. We adopt
Probabilistic Structured Queries (PSQ) (Darwish
and Oard, 2003), the key idea of which is to treat
alternate translations of a query term as synonyms
and to weight the contributions of each “synonym”
using a statistical translation model. We index
the Wikipedia articles in our test collection using
a publicly available IR tool (Indri), learn isolated
word translation probabilities from a parallel text
using the Berkeley aligner5 and Joshua,6 and im-
plement PSQ using Indri’s #wsyn operator. Based
on initial tests on training data, we use a contex-
tual window size of± 40 terms to the left and right
of the query name mention as the source language
query. In Roman alphabet languages, untranslated
terms are retained in a character-normalized form.

3.2.4 Relation Features
As can be seen in Figure 2, the KB contains a set
of attributes and relations associated with each en-
tity (e.g., age, employer, spouses, etc.). While one
could run a relation extractor over the query docu-
ment and look for relational equivalences, or con-
tradictions, we chose a more straightforward ap-
proach: we simply treat the words from all facts
as a surrogate “document” and calculate document
similarity with the query document.

3.2.5 Named Entity Features
We applied the named entity tagger by Ratinov
and Roth (2009) to query documents and created
features from the tagger output, including: the per-
centage of NEs present in KBdoc; the percentage
of words from all NEs that are present in KBdoc;
and, the number of co-occurring NEs from Qdoc
that are present in KBdoc. Except for an experi-
ment described in Section 6.1, these features are
only used in our monolingual English runs.

3.2.6 Entity Type Features
In English experiments the type of the query entity
is determined from the NER output for the query
document. Since the reference knowledge base
provides a class (e.g., scientist) and a type (e.g.,
PER) for most entities, we can check whether the
type of the KB entity is consistent with the query.

5http://code.google.com/p/berkeleyaligner/
6http://sourceforge.net/projects/joshua/

This helps discourage selection of eponymous en-
tries named after famous people (e.g., the USS
Abraham Lincoln (CVN-72), a nuclear-powered
aircraft carrier named after the 16th US president).

3.2.7 NIL Features
Some features can indicate whether it is likely
or unlikely that there is a matching KB entry for
a query. For example, if many candidates have
strong name matches, it is reasonable to believe
that one of them is correct. Conversely, if no can-
diate has high textual similarity with the query, or
overlap between KB facts and the query document
text, it becomes more plausible to believe that the
entity is missing from the KB.

4 Building a Test Collection for Entity
Linking in Twenty-One Languages

The TAC-KBP entity linking test collections from
2009 and 2010 include the following resources:
(a) a large collection of English documents; (b)
approximately 7,000 queries comprising English
name mentions from those documents; (c) a refer-
ence knowledge base with over 818K entries; and
(d) a set of annotations that identify the appro-
priate KB entry for each query, or absence (Mc-
Namee and Dang, 2009; Ji et al., 2010). The
KB was created by processing a 2008 dump of
English Wikipedia; each entry includes structured
attributes obtained from Wikipedia’s infoboxes in
addition to the unstructured article text. A sample
KB entry is shown in Figure 2. We use the TAC
KB in all of our experiments.

Since the TAC-KBP queries and documents are
only available in English, these data are not di-
rectly usable for cross-language entity linking.
One approach would be to manually translate the
TAC documents and queries into each desired lan-
guage. This would be prohibitively expensive. In-
stead, we use parallel document collections and
crowdsourcing to generate ground truth in other
languages. A fundamental insight on which our
work is based is that if we build an entity linking
test collection using the English half of a parallel
text collection, we can make use of readily avail-
able annotators and tools developed specifically
for English, then project the English results onto
the other language. Thus, we apply English NER
to find person names in text (Ratinov and Roth,
2009), our English entity linking system to iden-
tify candidate entity IDs, and English annotators
on Amazon’s Mechanical Turk to select the correct

259



Language Collection Queries Non-NIL
Albanian (sq) SETimes 4,190 2,274
Arabic (ar) LDC2004T18 2,829 661
Bulgarian (bg) SETimes 3,737 2,068
Chinese (zh) LDC2005T10 1,958 956
Croatian (hr) SETimes 4,139 2,257
Czech (cs) ProjSynd 1,044 722
Danish (da) Europarl 2,105 1,096
Dutch (nl) Europarl 2,131 1,087
Finnish (fi) Europarl 2,038 1,049
French (fr) ProjSynd 885 657
German (de) ProjSynd 1,086 769
Greek (el) SETimes 3,890 2,129
Italian (it) Europarl 2,135 1,087
Macedonian (mk) SETimes 3,573 1,956
Portuguese (pt) Europarl 2,119 1,096
Romanian (ro) SETimes 4,355 2,368
Serbian (sr) SETimes 3,943 2,156
Spanish (es) ProjSynd 1,028 743
Swedish (sv) Europarl 2,153 1,107
Turkish (tr) SETimes 3,991 2,169
Urdu (ur) LDC2006E110 1,828 1,093
Total 55,157 29,500

Table 2: Language coverage in our collection.

kbid for each name. Finally, we use standard sta-
tistical word alignment techniques implemented in
the Berkeley Word Aligner (Haghighi et al., 2009)
to map from English name mentions to the corre-
sponding names in the non-English documents.

The six parallel collections we used came from
the LDC and online sources. Together, these col-
lections contain 196,717 non-English documents
in five different scripts and twenty-one different
languages. The final size of the query sets by lan-
guage is shown in Table 2. We partitioned these
queries and their associated documents into three
sets per language: 60% for training, 20% for de-
velopment, and 20% for test. In other work we
give additional details about the creation of our
test collection (Mayfield et al., 2011).

5 Experimental Results

5.1 English Baselines

Since all of our documents are from parallel cor-
pora, every query is available in English and at
least one other language. To serve as a point
of comparison, we ran our monolingual entity
linking system using the English version of the
queries. We also determined performance of a
baseline that predicts a kbid if its entity’s name is
a unique, exact match for the English query string,
and NIL otherwise.

To compare approaches we calculate the per-
centage of time that the top-ranked prediction
from a system is correct, which we call Precision-

at-rank-one (P@1).7 For the exact match base-
line (Exact), the mean P@1 accuracy across all
query sets is 0.897; on the TAC-KBP 2010 person
queries, this baseline achieves a score of 0.832,
which is lower most likely because of the inten-
tional efforts at TAC to artificially increase query
name ambiguity. Results for both English base-
lines are included in Table 3.

5.2 Cross-Language Name Matching

Table 3 also reports cross-language experiments
in twenty languages where cross-language name
matching is used to project the non-English query
name into English (NameMatch), but the docu-
ment remains untranslated. If the correct translit-
eration is known from our transliteration training
data, we perform table lookup; otherwise the 1-
best transliteration produced by our model is used.

Name matching alone produces serviceable per-
formance. Averaged over all languages, perfor-
mance on all queries is 93% of the monolingual
English baseline. Losses tend to be small in the
languages that use a Latin alphabet.

To investigate how errors in automated translit-
eration affect the system, we also conducted an ex-
periment where the human-produced translations
of the entity name were obtained from the English
side of the parallel data. In Table 4 we report how
this condition (PerfectTrans) performs relative to
the monolingual baseline. Perfect name transla-
tion reduces the error rate dramatically, and per-
formance of 99.2% of monolingual is obtained.

5.3 Name Matching and Context Matching

Table 3 also reports the use of both name match-
ing and context matching using CLIR (+Context).
Over all queries, performance rises from 92.9%
to 93.9% of the English baseline. Bigger gains
are evident on non-NIL queries. In fact, the pan-
language average hides the fact that much larger
gains are observed for non-NILs in Arabic, Czech,
Macedonian, Serbian, and Turkish. We checked
whether these gains are significant compared to
NameMatch using the sign test; values indicating
significant gains (p < 0.05) are emboldened.

5.4 Learning Rate

Our approach depends on having a quantity of la-
belled data on which to train a classifier. To in-
vestigate the effect that the number of training ex-

7At TAC this metric is called micro-averaged accuracy.

260



All Queries Non-NIL Queries
English Cross-Language English Cross-Language

Set N Mono Exact NameMatch +Context N Mono Exact NameMatch +Context
ar 577 0.948 0.886 (93%) 0.901 (95%) 0.926 (98%) 136 0.838 0.552 (66%) 0.706 (84%) 0.787 (94%)
bg 770 0.982 0.918 (94%) 0.892 (91%) 0.892 (91%) 430 0.972 0.854 (88%) 0.821 (84%) 0.833 (86%)
cs 203 0.931 0.764 (82%) 0.828 (89%) 0.862 (93%) 136 0.985 0.669 (68%) 0.772 (78%) 0.838 (85%)
da 428 0.988 0.963 (97%) 0.965 (98%) 0.963 (97%) 225 0.982 0.933 (95%) 0.938 (95%) 0.933 (95%)
de 217 0.931 0.756 (81%) 0.871 (94%) 0.876 (94%) 154 0.987 0.675 (68%) 0.857 (87%) 0.877 (89%)
el 776 0.979 0.928 (95%) 0.833 (85%) 0.851 (87%) 423 0.972 0.868 (89%) 0.714 (73%) 0.745 (77%)
es 208 0.909 0.760 (84%) 0.889 (98%) 0.894 (98%) 149 0.960 0.685 (71%) 0.873 (91%) 0.899 (94%)
fi 425 0.986 0.965 (98%) 0.927 (94%) 0.941 (95%) 220 0.982 0.936 (95%) 0.868 (88%) 0.900 (92%)
fr 186 0.930 0.742 (80%) 0.909 (98%) 0.876 (94%) 135 0.978 0.659 (67%) 0.904 (92%) 0.911 (93%)
hr 846 0.980 0.924 (94%) 0.930 (95%) 0.920 (94%) 470 0.972 0.864 (89%) 0.889 (91%) 0.866 (89%)
it 443 0.984 0.966 (98%) 0.907 (92%) 0.914 (93%) 227 0.978 0.938 (96%) 0.833 (85%) 0.859 (88%)

mk 720 0.978 0.932 (95%) 0.822 (84%) 0.850 (87%) 391 0.967 0.875 (90%) 0.706 (73%) 0.749 (78%)
nl 441 0.984 0.964 (98%) 0.955 (97%) 0.955 (97%) 224 0.978 0.933 (95%) 0.924 (95%) 0.933 (95%)
pt 443 0.987 0.964 (98%) 0.982 (100%) 0.977 (99%) 230 0.978 0.935 (96%) 0.974 (100%) 0.961 (98%)
ro 878 0.976 0.924 (95%) 0.961 (98%) 0.961 (98%) 480 0.967 0.860 (89%) 0.935 (97%) 0.933 (97%)
sq 849 0.972 0.927 (95%) 0.889 (92%) 0.913 (94%) 465 0.955 0.867 (91%) 0.809 (85%) 0.860 (90%)
sr 799 0.976 0.920 (94%) 0.804 (82%) 0.840 (86%) 447 0.966 0.857 (89%) 0.653 (68%) 0.743 (77%)
sv 448 0.987 0.964 (98%) 0.958 (97%) 0.960 (97%) 231 0.978 0.935 (96%) 0.935 (96%) 0.944 (96%)
tr 804 0.980 0.923 (94%) 0.954 (97%) 0.968 (99%) 440 0.973 0.859 (88%) 0.925 (95%) 0.953 (98%)
ur 363 0.973 0.862 (89%) 0.810 (83%) 0.840 (86%) 215 0.967 0.772 (80%) 0.707 (73%) 0.763 (79%)
x̄ 541 0.968 0.897 (93%) 0.899 (93%) 0.909 (94%) 291 0.967 0.826 (85%) 0.837 (87%) 0.864 (89%)

Table 3: P@1 for a variety of experimental conditions. The left half of the table presents aggregate
results for all queries; on the right performance is given for just non-NIL queries. Percentages are with
respect to the monolingual English condition.

Set Mono NM PerfectTrans English NEs
P@1 P@1 P@1 % Mono P@1 % Mono

ar 0.948 0.901 0.941 99.3% 0.901 95.1%
bg 0.982 0.892 0.986 100.4% 0.8558 87.0%
cs 0.931 0.828 0.882 94.7% 0.897 96.3%
da 0.988 0.965 0.986 99.8% 0.972 98.4%
de 0.931 0.871 0.899 96.5% 0.917 98.5%
el 0.979 0.833 0.978 99.9% 0.872 89.1%
es 0.909 0.889 0.914 100.5% 0.861 94.7%
fi 0.986 0.927 0.988 100.2% 0.955 96.9%
fr 0.930 0.909 0.909 97.7% 0.914 98.3%
hr 0.980 0.930 0.972 99.2% 0.963 98.3%
it 0.984 0.907 0.987 100.2% 0.930 94.5%

mk 0.978 0.822 0.976 99.9% 0.881 90.1%
nl 0.984 0.955 0.982 99.8% 0.964 97.9%
pt 0.987 0.982 0.987 100.0% 0.977 99.1%
ro 0.976 0.961 0.976 100.0% 0.960 98.4%
sq 0.972 0.889 0.976 100.5% 0.933 96.0%
sr 0.976 0.804 0.974 99.7% 0.977 100.1%
sv 0.987 0.958 0.984 99.8% 0.975 98.9%
tr 0.980 0.954 0.984 100.4% 0.963 98.2%
ur 0.973 0.810 0.931 95.7% 0.876 90.1%
x̄ 0.968 0.899 0.961 99.2% 0.927 95.8%

Table 4: Cross-language effectiveness (P@1) over
all queries with optimal (a) transliteration and (b)
named entity recognition. Simply having access
to perfect transliterations achieves 99% of mono-
lingual performance, on average. Providing lists
of named entities mentioned in the document, in
English, also improves performance. Bold values
indicate statistically significant gains compared to
the NameMatch (NM) run.

0.80 

0.82 

0.84 

0.86 

0.88 

0.90 

0.92 

0.94 

0.96 

0.98 

1.00 

0 500 1000 1500 2000 2500 

A
cc

ur
ac

y 

Training Queries 

tr 

hr 

ar 

sq 

de 

mk 

Figure 3: Classifier accuracy and training set size.

emplars has on classifier accuracy we built classi-
fiers using fixed numbers of training queries. Fig-
ure 3 shows these results for selected languages.
Each curve was produced by generating a random
permutation of the training data, selecting the first
k queries, and averaging the results over five tri-
als. Note that the total amount of available train-
ing data differs by language. In all cases, accuracy
rises quickly for the first 500 queries, and little im-
provement is observed after 1000 examples.

261



0.70 

0.75 

0.80 

0.85 

0.90 

0.95 

1.00 

ar/ar ur/ar ur/ur ar/ur 
0.70 

0.75 

0.80 

0.85 

0.90 

0.95 

1.00 

bg/bg mk/bg mk/mk bg/mk 
0.70 

0.75 

0.80 

0.85 

0.90 

0.95 

1.00 

ro/ro tr/ro tr/tr ro/tr 

Figure 4: Training with annotations from another language using the same writing system. A label of
xx/yy indicates that feature weights were trained using labelled data from language xx and then applied
to queries on language yy.

6 Additional Experiments

6.1 Multilingual NER and Transliteration

We believe that the entities in a document that co-
occur with a target entity are important clues for
disambiguating entities. However, while we had
ready access to named entity recognizers in En-
glish, we did not have NER capability in all of
the languages of our collection. We would like
to know how useful multilingual NER could be.
To simulate this using our test collection, where
all documents are from parallel texts with an En-
glish translation, we conducted an experiment that
used the English documents only to recognize En-
glish named entities that co-occur with the query
string; in every other respect, the untranslated for-
eign document was used by the system. The En-
glish NER may make errors, but we use it to
simulate the performance of excellent non-English
NER coupled with perfect entity translation.

Table 4 shows that co-occurring entities are a
very helpful feature. Compared to name matching
alone, average P@1 rises from 89.9% to 92.7%.

6.2 Cross-Language Training

Although we have demonstrated an efficient
method for building a test collection for cross-
language entity linking, it may still be diffi-
cult to obtain training data and tools for some
less-resourced languages. Our process and fea-
ture set is largely language-independent, and we
would like to know how feasible it is to make
predictions without any language-specific train-
ing data by exploiting entity linking annotations
from a related language. We examined pairs of
languages using the same script – Arabic/Urdu,
Bulgarian/Macedonian, and Romanian/Turkish –
and trained classifiers using labeled data for the

other language. Figure 4 shows that performance
is not dramatically different when using annota-
tions from a language sharing a common alphabet.
This suggests that it is plausible to build a cross-
language entity linking system without manually-
produced annotations for a particular language.

7 Conclusions

In this paper we introduced a new problem, cross-
language entity linking, and we described an ap-
proach to this problem that uses statistical translit-
eration and cross-language information retrieval.
Using a newly-developed test collection for this
task,9 we demonstrated the success of the ap-
proach in twenty languages. Our best model us-
ing both name and context matching achieves av-
erage performance across twenty languages which
is 94% of a strong monolingual English baseline,
with individual languages ranging from 86% to
99%. Additionally, we characterized the number
of training exemplars needed, demonstrated the
feasibility of off-language training, and illustrated
performance gains that are possible if combined
multilingual NER/transliteration is available.

Acknowledgments

We are grateful to Chris Callison-Burch and Ann
Irvine for their support with machine translation
and orthographic transliteration, and to Tan Xu
and Mohammad S. Raunak for their help in data
curation. Support for one of authors was provided
in part by NSF grant CCF 0916081.

8The English NEs run was significantly worse vs.
NameMatch in Bulgarian (bg).

9The test collection is available at http://web.jhu.
edu/HLTCOE/datasets.html.

262



References
Sisay Fissaha Adafre and Maarten de Rijke. 2005.

Discovering missing links in Wikipedia. In
LinkKDD ’05: Proceedings of the 3rd international
workshop on Link discovery, pages 90–97. ACM.

Javier Artiles, Andrew Borthwick, Julio Gonzalo,
Satoshi Sekine, and Enrique Amigo. 2010.
Overview of the web people search clustering and at-
tribute extraction tasks. In CLEF Third WEPS Eval-
uation Workshop.

David Guy Brizan and Abdullah Uz Tansel. 2006.
A survey of entity resolution and record link-
age methodologies. Communications of the IIMA,
6(3):41–50.

Razvan C. Bunescu and Marius Pasca. 2006. Us-
ing encyclopedic knowledge for named entity dis-
ambiguation. In European Chapter of the Assoca-
tion for Computational Linguistics (EACL).

Peter Christen. 2006. A comparison of personal name
matching: Techniques and practical issues. Techni-
cal Report TR-CS-06-02, Australian National Uni-
versity.

Silviu Cucerzan. 2007. Large-scale named entity dis-
ambiguation based on Wikipedia data. In Empirical
Methods in Natural Language Processing.

Kareem Darwish and Douglas W. Oard. 2003. Prob-
abilistic structured query methods. In ACM SIGIR,
pages 338–344. ACM.

Aria Haghighi, John Blitzer, John DeNero, and Dan
Klein. 2009. Better word alignments with super-
vised ITG models. In Proceedings of the Joint Con-
ference of the 47th Annual Meeting of the ACL and
the 4th International Joint Conference on Natural
Language Processing, pages 923–931. ACL.

Ann Irvine, Chris Callison-Burch, and Alexandre Kle-
mentiev. 2010. Transliterating from all languages.
In AMTA.

Heng Ji and Ralph Grishman. 2011. Knowledge base
population: Successful approaches and challenges.
In Association for Computational Linguistics.

Heng Ji, Ralph Grishman, Hoa Trang Dang, Kira Grif-
fitt, and Joe Ellis. 2010. Overview of the TAC 2010
Knowledge Base Population track. In Text Analysis
Conference (TAC).

Thorsten Joachims. 2002. Optimizing search engines
using clickthrough data. In Knowledge Discovery
and Data Mining (KDD).

Sarvnaz Karimi, Falk Scholer, and Andrew Turpin.
2011. Machine transliteration survey. ACM Com-
puting Surveys, 43(4):1–57.

Kazuaki Kishida. 2005. Technical issues of cross-
language information retrieval: a review. Informa-
tion Processing and Management, 41(3):433 – 455.
Cross-Language Information Retrieval.

Martin Klein and Michael L. Nelson. 2008. A com-
parison of techniques for estimating IDF values to
generate lexical signatures for the web. In WIDM
’08, pages 39–46. ACM.

Vladimir I. Levenshtein. 1966. Binary codes capable
of correcting deletions, insertions and reversals. So-
viet Physics–Doklady, 10(8):707–710.

Inderjeet Mani, Alex Yeh, and Sherri Condon. 2008.
Learning to match names across languages. In
MMIES ’08, pages 2–9. ACL.

James Mayfield, Dawn Lawrie, Paul McNamee, and
Douglas W. Oard. 2011. Building a cross-language
entity linking collection in twenty-one languages. In
Cross-Language Evaluation Forum (CLEF).

Andrew McCallum, Kamal Nigam, and Lyle Ungar.
2000. Efficient clustering of high-dimensional data
sets with application to reference matching. In
Knowledge Discovery and Data Mining (KDD).

Paul McNamee and Hoa Trang Dang. 2009. Overview
of the TAC 2009 Knowledge Base Population track.
In Text Analysis Conference (TAC).

Paul McNamee. 2008. Textual Representations for
Corpus-Based Bilingual Retrieval. Ph.D. thesis,
University of Maryland Baltimore County, Balti-
more, MD.

Paul McNamee. 2010. HLTCOE efforts in entity link-
ing at TAC KBP 2010. In Text Analysis Conference
(TAC), Gaithersburg, Maryland, November.

Rada Mihalcea and Andras Csomai. 2007. Wikify!:
linking documents to encyclopedic knowledge. In
CIKM, pages 233–242.

David N. Milne and Ian H. Witten. 2008. Learning to
link with wikipedia. In CIKM, pages 509–518.

Vincent Ng. 2010. Supervised noun phrase corefer-
ence research: The first fifteen years. In Proceed-
ings of the 48th Annual Meeting of the Association
for Computational Linguistics, pages 1396–1411.

Lev Ratinov and Dan Roth. 2009. Design chal-
lenges and misconceptions in named entity recog-
nition. In Proceedings of the Thirteenth Confer-
ence on Computational Natural Language Learning
(CoNLL-2009), pages 147–155, Boulder, Colorado,
June. Association for Computational Linguistics.

Chakkrit Snae. 2007. A comparison and analysis of
name matching algorithms. Proceedings of World
Academy of Science, Engineering and Technology,
21:252–257, January.

Ralf Steinberger and Bruno Pouliquen. 2007. Cross-
lingual named entity recognition. Linguisticae In-
vestigationes, 30(1):135–162, January.

Steven Euijong Whang, David Menestrina, Georgia
Koutrika, Martin Theobald, and Hector Garcia-
Molina. 2009. Entity resolution with iterative
blocking. In SIGMOD 2009, pages 219–232. ACM.

263


