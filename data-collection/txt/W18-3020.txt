



















































Learning Hierarchical Structures On-The-Fly with a Recurrent-Recursive Model for Sequences


Proceedings of the 3rd Workshop on Representation Learning for NLP, pages 154–158
Melbourne, Australia, July 20, 2018. c©2018 Association for Computational Linguistics

154

Learning Hierarchical Structures On-The-Fly with a
Recurrent-Recursive Model for Sequences

Athul Paul Jacob∗†
MILA

University of Waterloo

Zhouhan Lin∗†
MILA

University of Montréal
AdeptMind Scholar

Alessandro Sordoni
Microsoft Research
Montréal, Canada

Yoshua Bengio
MILA

University of Montréal, CIFAR

Abstract

We propose a hierarchical model for se-
quential data that learns a tree on-the-
fly, i.e. while reading the sequence. In
the model, a recurrent network adapts its
structure and reuses recurrent weights in
a recursive manner. This creates adaptive
skip-connections that ease the learning of
long-term dependencies. The tree struc-
ture can either be inferred without super-
vision through reinforcement learning, or
learned in a supervised manner. We pro-
vide preliminary experiments in a novel
Math Expression Evaluation (MEE) task,
which is explicitly crafted to have a hier-
archical tree structure that can be used to
study the effectiveness of our model. Ad-
ditionally, we test our model in a well-
known propositional logic and language
modelling tasks. Experimental results
show the potential of our approach.

1 Introduction

Many kinds of sequential data such as language
or math expressions naturally come with a hierar-
chical structure. Sometimes the structure is hid-
den deep in the semantics of the sequence, like the
syntax tree in natural language; Other times the
structure is more explicit, as in math expressions,
where the tree is determined by the parentheses.

Recurrent neural networks (RNNs) have shown
tremendous success in modeling sequential data,
such as natural language (Mikolov et al., 2010;
Merity et al., 2017). However, RNNs process the
observed data as a linear sequence of observations:

∗Equal contribution (Ordering determined by coin flip).
Corresponding authors: zhouhan.lin@umontreal.ca, apja-
cob@edu.uwaterloo.ca

†Work done while at Microsoft Research, Montreal.

the length of the computational path between any
two words is a function of their position in the ob-
served sequence, instead of their semantic or syn-
tactic roles, leading to the appearance of difficult-
to-learn long-term dependencies and stimulating
research on strategies to deal with that (Bengio
et al., 1994; El Hihi and Bengio, 1996; Hochreiter
and Schmidhuber, 1997). Hierarchical, tree-like
structures may alleviate this problem by creating
shortcuts between distant inputs and by simulat-
ing compositionality of the sequence, compressing
the sequence into higher-level abstractions. Mod-
els that use tree as prior knowledge (e.g. (Socher
et al., 2013; Tai et al., 2015; Bowman et al., 2016))
have shown improved performances over sequen-
tial models, validating the value of tree structure.
For example, TreeLSTM (Tai et al., 2015) learns a
bottom-up encoder, but requires the model to have
access to the entire sentence as well as its parse
tree before encoding it, which limits its applica-
tion in some cases, e.g. language modeling. There
has been various efforts to learn the tree structure
as a supervised training target (Dyer et al., 2016;
Socher et al., 2010; Zhou et al., 2017; Zhang et al.,
2015), which free the model from relying on an
external parser.

More recent efforts learn the best tree structure
without supervision, by minimizing the log like-
lihood of the observed corpus, or by optimizing
over a downstream task (Williams et al., 2017).
These models usually take advantage of a binary
tree assumption on the inferred tree, which im-
poses restrictions on the flexibility of inferred tree
structure, for example, Gumbel TreeLSTM (Choi
et al., 2017; Yogatama et al., 2016).

We propose a model that reads sequences us-
ing a hierarchical, tree-structured process (Fig. 1):
it creates a tree on-the-fly, in a top-down fashion.
Our model sits in between fully recursive models
that have access to the whole sequence, such as



155

hl

hdownhl-1

F2 F1 hlhl-1
F3

hl-1

hl

hdown

F4hmMLP

SR M R
x0 x6

x1
^

x7
^

SR

RR R

R

ROOT

     
 

     
 

(a) split cell (b) recurrent cell (c) merge cell

(d) Recurrent-Recursive network (unrolled) (e) Inferred tree

x2

x1 x5

x2
^

x3
^

R R

x3

x4
^

R

x4

x5
^

x6
^

Figure 1: (a) - (c) are the 3 different cells. (d) is a sample model structure resulted from a sequence of
decisions. ”R”, ”S” and ”M” stand for recurrent cell, split cell, and merge cell, respectively. Note that
the ”S” and ”M” node can take inputs in datasets where splitting and merging signals are part of the
sequence. For example, in math data the splitting signal are related to the brackets. (e) is the tree inferred
from (d).

TreeLSTMs (Tai et al., 2015), and vanilla recur-
rent models that “flatten” input sequence, such as
LSTMs (Schmidhuber, 1992). At each time-step
in the sequence, the model chooses either to cre-
ate a new sub-tree (split), to return and merge in-
formation into the parent node (merge), or to pre-
dict the next word in the sequence (recur). On
split, a new sub-tree is created which takes control
on which operation to perform. Merge operations
end the current computation and return a represen-
tation of the current sub-tree to the parent node,
which composes it with the previously available
information on the same level. Recurrent opera-
tions use information from the siblings to perform
predictions. Operations at every level in the tree
use shared weights, thus sharing the recursive na-
ture of TreeLSTMs. In contrast to TreeLSTMs
however, the tree is created on-the-fly, which es-
tablishes skip-connections with previous tokens in
the sequence and forms compositional represen-
tations of the past. The branching decisions can
either be trained through supervised learning, by
providing the true branching signals, or by policy
gradients (Williams, 1992) which maximizes the
log-likelihood of the observed sequence. As op-
posed to previous models, these three operations
constantly change the structure of the model in an
online manner.

Experimental evaluation aims to analyze vari-
ous aspects of the model such as: how does the
model generalize on sequences of different lengths
than those seen during training? how hard is the
tree learning problem? To answer those questions,

we propose a novel multi-operation math expres-
sion evaluation (MEE) dataset, with a standard set
of tasks with varying levels of difficulty, where the
difficulty scales up with respect to the length of the
sequence.

2 Model

Similar to a standard RNN, our model modifies a
hidden state hl for each step of the input sequence
x = {x1, . . . , xN} by means of split, merge and
recurrent operations. Denote the sequence of op-
erations by z = {z1, . . . , zL}, where L may be
greater than the number of tokens N since only
recurrent operations consume input tokens (see
Fig. 1). Each operation is parametrized by a dif-
ferent “cell”. A policy network controls which op-
eration to perform during sequence generation.

split (S) The split cell creates a sub-tree by tak-
ing the previous state hl−1 as input and generating
two outputs hl and hdown. hl is used for further
computation, while hdown (the small blue rectan-
gle in Fig. 1(d)) is pushed into a stack for future
use. In our model, hdown = F1(hl−1, xt) and
hl = F2(hl−1, xt) where the F1 and F2 are LSTM
units (Hochreiter and Schmidhuber, 1997), and xt
is the current input.

recurrent (R) This cell is a standard LSTM
unit that takes as input the previous state hl−1 and
the current token xt, and outputs the hidden state
hl, which will be used to predict the next output
x̂t+1. After application of this cell, the counter t is
incremented and input xt is consumed.



156

merge (M) The merge cell closes a sub-tree and
returns control to its parent node. It does so by
merging the previous hidden state hl−1 with the
top of the stack hdown into a new hidden state
hm = MLP (hl−1, hdown) (Fig. 1(c)). hm is then
used as input to another LSTM unit (F4) to yield
hl = F4(xt, hm), the new hidden state of the over-
all network. Intuitively, hl−1 summarizes the con-
tents within the sub-tree. This is merged with in-
formation obtained before the model entered the
sub-tree hdown into the new state hl.

Policy Network We consider the decision at
each timestep zt ∈ {S,M,R} as a categorical
variable sampled from a policy network pπ, condi-
tioned on the hidden state ht and the input embed-
ding et of the current input xt. In the supervised
setting, pπ(zt|et, ht) is trained by maximizing the
likelihood of the true branching labels, while in
the unsupervised setting, we resort to the REIN-
FORCE algorithm using − log p(yt|C) as a re-
ward, where yt is the task target (i.e. the next word
in language modeling), and C is the representation
learnt by the model up until time t.

3 Experimental Results

We conduct our experiments on a math induction
task, a propositional logic inference task (Bow-
man et al., 2016) and language modelling. First
of all, we aim to investigate whether a) our hier-
archical model may help in tasks that explicitly
exhibit hierarchical structure, and then b) whether
the trees learned without supervision correspond
to the ground-truth trees, c) how our model fare
with respect to hierarchical models that have ac-
cess to the whole sequence with a pre-determined
tree structure and finally, d) are there any limita-
tions for models that are not capable of learning
hierarchical structures on-the-fly.

3.1 Math Induction
Our math expression evaluation dataset (MEE)
consists of parenthesized mathematical expres-
sions and their corresponding evaluations. The
math expressions contain bracketing symbols
(”()”), four different kinds of operations, ”+-*%”,
where ”%” is the modulo operation, and digits
from 0 to 9. The “length” of an expression is the
number of operations in the expression and its re-
sult is restricted to be a positive, two-digit integer
(Table 1). We randomly generate expressions of
different lengths and for each length the resulting

Length Expression Value
4 ((9+(2+6))+(1*3)) 20
5 (((7-2)%((3%1)+6))*9) 45
6 ((((3-0)+(7-6))*(0+9))-7) 29
7 ((4*(6+(7*(2*8))))%(9+(3+7))) 16

Table 1: Sample expressions from MEE dataset

sub-dataset is divided into 100,000, 10,000 and
10,000 expressions as training, valid and test sets.
We make sure that there is no overlap between the
splits and every expression is made unique across
the whole dataset.

We use an encoder-decoder approach where the
encoder reads the characters in the expression and
produces the encoding as input to the decoder,
which in turn sequentially generates 2 digits as
the predicted value. We experiment on various
encoders, including our model, and compare their
performances. We use the same decoder architec-
ture to ensure a fair comparison. The output of
the encoder is provided as the initial hidden state
of the decoder LSTM. To test the generalization
of our model, for all the experiments shown in
this subsection, we train the model on expressions
of length 4 and 5, and evaluate on expressions of
length 4 to 7 in the test sets.

For this task, our baseline is a simple LSTM en-
coder (which corresponds to our model with only
recur operations). We compare two versions of
our RRNet encoder. In the supervised setting,
we force the model to split and merge when it
reads “(” and “)”, respectively, and recur other-
wise. This gives us an idea on how well the model
would perform if it had access to the ground-truth
tree. In the unsupervised setting, we learn the tree
using policy gradient, where the reward is the ac-
curacy of the math result prediction.

The results are in Table 2. The supervised RR-
Net yields the best performance showing that (a)
it is important to exploit the hierarchical struc-
ture of the observed data, corroborating previous
work (Williams et al., 2017), and (b) our model is
effective at capturing that information. The unsu-
pervised RRNet model also outperforms the base-
line LSTM: the model learn to exploit branching
operations to achieve better performance. We ob-
serve that the trees produced by the model do not
correspond to the ground-truth trees. In order to
assess whether the additional parameters of split
and merge operations, rather than the learned tree



157

Model Train TestL=4 L=5 L=6 L=7
LSTM 75.80 81.32 67.65 52.70 41.35
Uns RRNet 86.00 89.42 77.96 61.34 50.46
Sup RRNet 93.70 93.28 86.69 79.09 72.70

Table 2: Prediction accuracy on MEE dataset.

Figure 2: Test accuracy of the models, trained on
sequences of length ≤ 6 in logic data. The hor-
izontal axis indicates the length of the sequence,
and the vertical axis indicates the accuracy of
model’s performance on the corresponding test
set.

structure, produce better results, we measured the
performance of our model trained with “random”
deterministic policies (associating each of the in-
put characters to either a split, merge or recur op-
eration). We see that “random” policies perform
worse than a “learnt” policy on the task, effec-
tively similar to the baseline LSTM. In turn, the
model with the learnt policy underperforms the
model trained with ground-truth trees. Even in this
seemingly easy task, it has appeared difficult for
the model to learn the optimal branching policy.

3.2 Logical inference

In the next task, we analyze performance on the
artificial language as described in Bowman et al.
(2015b). This language has six word types {a,
b, c, d, e, f} and three logical operations {or,
and, not}. There are seven mutually exclusive
logical relations that describe the relationship be-
tween two sentences: entailment (<, =), equiva-
lence (≡), exhaustive and non-exhaustive contra-
diction (∧, |), and two types of semantic indepen-
dence (#, `). The train/dev/test dataset ratios are

set to 0.8/0.1/0.1 as described 1 with the number
of logical operations ranging from 1 to 12.

From Figure 2, we report the performance
of our model when trained with ground-truth
trees as input. It is encouraging to see that
our recurrent-recursive encoder improves perfor-
mance over Transformer (FAN) (Tran et al., 2018)
and LSTM, especially for long sequences. The
best performance on this dataset is given by
TreeLSTM, which has access to the whole se-
quence (Bowman et al., 2015b) and does not en-
code sequences on-the-fly.

3.3 Language Modelling

In language modeling, architectures such as
TreeLSTM aren’t directly applicable since their
structure isn’t computed on-the-fly, while read-
ing the sentence. We perform preliminary exper-
iments using the Penn Treebank Corpus dataset
(Marcus et al., 1993), which has a vocabulary
of 10,000 unique words and 929k, 73k and 82k
words in training, validation and test set respec-
tively. Our cells use one layer and the hidden di-
mensionality is 350. Our model yields test per-
plexity of 107.28 as compared to the LSTM base-
line which gets 113.4 (Dyer et al., 2016). This pre-
liminary result shows that the endeavor to exploit
explicit hierarchical structures for language mod-
eling, although challenging, may be promising.

4 Final Considerations

In this work, we began exploring properties of
a recurrent-recursive neural network architecture
that learns to encode the sequence on-the-fly, i.e.
while reading. We argued this may be an impor-
tant feature for tasks such as language modeling.
We additionally proposed a new mathematical ex-
pression evaluation dataset (MEE) as a toy prob-
lem for validating the performance of sequential
models to learn from hierarchical data. We empir-
ically observed that, in this task, our model per-
forms better than a standard LSTM architecture
with no explicit structure and also outperforms
the baseline LSTM and FAN architectures on the
propositional logic task.

We hope to further study the properties of this
model by either more thorough architecture search
(recurrent dropouts, layer norm, hyper-parameter
sweeps), different variation of RL algorithms such

1https://github.com/sleepinyourhat/
vector-entailment

https://github.com/sleepinyourhat/vector-entailment
https://github.com/sleepinyourhat/vector-entailment


158

as deep Q-learning (Mnih et al., 2013) and em-
ploying this model on various other tasks such
as SNLI (Bowman et al., 2015a) and semi-
supervised parsing.

Acknowledgement

The authors would like to thank Compute
Canada for providing the computational resources.
Zhouhan Lin would like to thank AdeptMind for
generously supporting his research via scholar-
ship.

References
Yoshua Bengio, Patrice Simard, and Paolo Frasconi.

1994. Learning long-term dependencies with gradi-
ent descent is difficult. IEEE transactions on neural
networks 5(2):157–166.

Samuel R. Bowman, Gabor Angeli, Christopher Potts,
and Christopher D. Manning. 2015a. A large an-
notated corpus for learning natural language infer-
ence. In Proceedings of the 2015 Conference on
Empirical Methods in Natural Language Processing
(EMNLP). Association for Computational Linguis-
tics.

Samuel R Bowman, Jon Gauthier, Abhinav Ras-
togi, Raghav Gupta, Christopher D Manning, and
Christopher Potts. 2016. A fast unified model for
parsing and sentence understanding .

Samuel R. Bowman, Christopher D. Manning, and
Christopher Potts. 2015b. Tree-structured compo-
sition in neural networks without tree-structured ar-
chitectures. CoRR abs/1506.04834.

Jihun Choi, Kang Min Yoo, and Sang-goo Lee. 2017.
Learning to compose task-specific tree structures.
arXiv preprint arXiv:1707.02786 .

Chris Dyer, Adhiguna Kuncoro, Miguel Ballesteros,
and Noah A Smith. 2016. Recurrent neural network
grammars. Proceedings of ACL .

Salah El Hihi and Yoshua Bengio. 1996. Hierarchical
recurrent neural networks for long-term dependen-
cies. In Proceedings of NIPS.

Sepp Hochreiter and Jürgen Schmidhuber. 1997.
Long short-term memory. Neural computation
9(8):1735–1780.

Mitchell P Marcus, Mary Ann Marcinkiewicz, and
Beatrice Santorini. 1993. Building a large annotated
corpus of english: The penn treebank. Computa-
tional linguistics 19(2):313–330.

Stephen Merity, Nitish Shirish Keskar, and Richard
Socher. 2017. Regularizing and optimizing lstm lan-
guage models. arXiv preprint arXiv:1708.02182 .

Tomas Mikolov, Martin Karafit, Luks Burget, Jan Cer-
nock, and Sanjeev Khudanpur. 2010. Recurrent
neural network based language model. In Takao
Kobayashi, Keikichi Hirose, and Satoshi Nakamura,
editors, INTERSPEECH. ISCA, pages 1045–1048.

Volodymyr Mnih, Koray Kavukcuoglu, David Sil-
ver, Alex Graves, Ioannis Antonoglou, Daan Wier-
stra, and Martin Riedmiller. 2013. Playing atari
with deep reinforcement learning. arXiv preprint
arXiv:1312.5602 .

Jürgen Schmidhuber. 1992. Learning complex, ex-
tended sequences using the principle of history com-
pression. Neural Computation 4(2).

Richard Socher, Christopher D Manning, and An-
drew Y Ng. 2010. Learning continuous phrase
representations and syntactic parsing with recursive
neural networks. In Proceedings of the NIPS-2010
Deep Learning and Unsupervised Feature Learning
Workshop. pages 1–9.

Richard Socher, Alex Perelygin, Jean Wu, Jason
Chuang, Christopher D Manning, Andrew Ng, and
Christopher Potts. 2013. Recursive deep models
for semantic compositionality over a sentiment tree-
bank. In Proceedings of the 2013 conference on
empirical methods in natural language processing.
pages 1631–1642.

Kai Sheng Tai, Richard Socher, and Christopher D
Manning. 2015. Improved semantic representations
from tree-structured long short-term memory net-
works. arXiv preprint arXiv:1503.00075 .

K. Tran, A. Bisazza, and C. Monz. 2018. The Impor-
tance of Being Recurrent for Modeling Hierarchical
Structure. ArXiv e-prints .

Adina Williams, Andrew Drozdov, and Samuel R Bow-
man. 2017. Learning to parse from a semantic ob-
jective: It works. is it syntax? arXiv preprint
arXiv:1709.01121 .

Ronald J Williams. 1992. Simple statistical gradient-
following algorithms for connectionist reinforce-
ment learning. Machine learning 8(3-4):229–256.

Dani Yogatama, Phil Blunsom, Chris Dyer, Edward
Grefenstette, and Wang Ling. 2016. Learning to
compose words into sentences with reinforcement
learning. arXiv preprint arXiv:1611.09100 .

Xingxing Zhang, Liang Lu, and Mirella Lapata. 2015.
Top-down tree long short-term memory networks.
arXiv preprint arXiv:1511.00060 .

Ganbin Zhou, Ping Luo, Rongyu Cao, Yijun Xiao,
Fen Lin, Bo Chen, and Qing He. 2017. Generative
neural machine for tree structures. arXiv preprint
arXiv:1705.00321 .


