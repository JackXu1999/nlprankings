



















































Eyes Don't Lie: Predicting Machine Translation Quality Using Eye Movement


Proceedings of NAACL-HLT 2016, pages 1082–1088,
San Diego, California, June 12-17, 2016. c©2016 Association for Computational Linguistics

Eyes Don’t Lie:
Predicting Machine Translation Quality Using Eye Movement

Hassan Sajjad, Francisco Guzmán, Nadir Durrani, Ahmed Abdelali,
Houda Bouamor†, Irina Temnikova, Stephan Vogel

Qatar Computing Research Institute, HBKU, Qatar
†Carnegie Mellon University, Qatar

Abstract

Poorly translated text is often disfluent and
difficult to read. In contrast, well-formed
translations require less time to process. In
this paper, we model the differences in reading
patterns of Machine Translation (MT) evalua-
tors using novel features extracted from their
gaze data, and we learn to predict the qual-
ity scores given by those evaluators. We test
our predictions in a pairwise ranking scenario,
measuring Kendall’s tau correlation with the
judgments. We show that our features provide
information beyond fluency, and can be com-
bined with BLEU for better predictions. Fur-
thermore, our results show that reading pat-
terns can be used to build semi-automatic met-
rics that anticipate the scores given by the
evaluators.

1 Introduction

Human evaluation has been the preferred method for
tracking the progress of MT systems. In the past,
the prevalent criterion was to judge the quality of a
translation in terms of fluency and adequacy, on an
absolute scale (White et al., 1994). However, dif-
ferent evaluators focused on different aspects of the
translations, which increased the subjectivity of their
judgments. As a result, evaluations suffered from
low inter- and intra-annotator agreements (Turian et
al., 2003; Snover et al., 2006). This caused a shift to-
wards a ranking-based approach (Callison-Burch et
al., 2007). Unfortunately, the disagreement between
evaluators is still a challenge that cannot be easily
resolved due to the non-transparent thought-process
that evaluators follow to make a judgment.

The eye-mind hypothesis (Just and Carpenter,
1980; Potter, 1983) states that when completing a
task, people cognitively process objects that are in
front of their eyes (i.e. where they fixate their gaze).1

Based on this assumption, it has been possible to
study reading behavior and patterns (Rayner, 1998;
Garrod, 2006; Hansen and Ji, 2010).

The overall difficulty of a sentence and its syn-
tactic complexity affects reading behavior (Coco
and Keller, 2015). Ill-formed sentences take longer
to process, and may cause the reader to jump back
while reading. Hence, by looking into how evalu-
ators read the translations and their accompanying
references, we can learn about: (i) the complexity of
a reference sentence, and (ii) the quality of a trans-
lation sentence.

Using reading patterns from evaluators could be a
useful tool for MT evaluation: (i) to shed light into
the evaluation process: e.g. the general reading be-
havior that evaluators follow to complete their task;
(ii) to understand which parts of a translation are
more difficult for the annotator; and (iii) to develop
semi-automatic evaluation systems that use reading
patterns to predict translation quality.

In this paper, we make a first step towards (iii): us-
ing reading patterns as a method for distinguishing
between good and bad translations. Our hypothesis
is that bad translations are difficult to read, which
may be reflected by the reading patterns of the eval-
uators. Motivated by the notion of reading diffi-
culty, we extracted novel features from the evalua-
tor’s gaze data, and used them to model and predict
the quality of translations as perceived by evaluators.

1Except in cases of covert attention.

1082



2 Features and Model

A perfectly grammatical sentence can be difficult
to read for several reasons: unfamiliar vocabulary,
complex syntactic structure, syntactic or semantic
ambiguity, etc. (Harley, 2013). Reading automatic
translations is even more challenging due to untrans-
lated words, incorrect word order, morphological
disagreements, etc. Cognitively processing difficult
sentences generally results in modified reading pat-
terns (Garrod, 2006; Coco and Keller, 2015).

In this paper, we analyze the reading patterns
of human judges in terms of the word transitions
(jumps), and the time spent on each word (dwell
time); and use them as features to predict the quality
score of a specific translation. For the sake of sim-
plicity, as recommended by Guzmán et al. (2015),
we only consider a monolingual evaluation scenario
and ignore the source text . However, our features
and experimental setup can be extended to include
source-side features.

2.1 Features

Jump features While reading text, the gaze of a
person does not visit every single word, but it ad-
vances in jumps called saccades. These jumps can
go forwards (progressions) or backwards (regres-
sions). The number of regressions correlates with
the reading difficulty of a sentence (Garrod, 2006;
Schotter et al., 2014; Metzner, 2015). In an evalu-
ation scenario, a fluent reading would mean mono-
tonic gaze movement. On the contrary, the reader
may need to jump back multiple times while reading
a poor translation. We classify the word-transitions
according to the direction of the jump and distance
between the start and end words. For subsequent
words n, n+ 1, this would mean a forward jump of
distance equal to 1. All jumps with distance greater
than 4 were sorted into a 5+ bucket. Additionally,
we separate the features for reference and translation
jumps. We also count the total number of jumps.

Total jump distance We additionally aggregate
jump distances2 to count the total distance covered
while evaluating a sentence. We have reference dis-
tance and translation distance features. Again, the

2Jump count and distance features have also shown to be
useful in SMT decoders (Durrani et al., 2011).

idea is that for a well-formed sentence, gaze distance
should be less, compared to a poorly-formed one.

Inter-region jumps While reading a translation,
evaluators can jump between the translation and a
reference to compare them. Intuitively, more jumps
of this type could signify that the translation is
harder to evaluate. Here we count the number of
transitions between reference and translation.

Dwell time The amount of time a person fixates on
a region is a crucial marker for processing difficulty
in sentence comprehension (Clifton et al., 2007) and
moderately correlates with the quality of a transla-
tion (Doherty et al., 2010). Our feature counts the
time spent by the reader on each particular word. We
separate reference and translation features.

Lexicalized Features The features discussed
above do not associate gaze movements with the
words being read. We believe that this information
can be critical to judge the overall difficulty of the
reference sentence, and to evaluate which transla-
tion fragments are problematic to the reader. To
compute the lexicalized features, we extract streams
of reference and translation lexical sequences based
on the gaze jumps, and score them using a tri-gram
language model. Let Ri = r1, r2, . . . , rm be a
sub-sequence of gaze movement over reference and
there are R1, R2, . . . , Rn sequences, the lex feature
is computed as follows:

lex(R) =
n∑
i

log p(Ri)
|Ri|

p(Ri) =
m∑
j

p(rj |rj−1, rj−2)

The normalization factor |Ri| is used to make
the probabilities comparable. We also use un-
normalized scores as additional feature. A similar
set of features lex(T ) is computed for the transla-
tions. All features are normalized by the length of
the sentence.

2.2 Model

For predicting the quality scores given by an eval-
uator, we use a linear regression model with ridge

1083



regularization. The ridge coefficient β̂ is the value
of β that minimizes the error:

∑
i

(yi − xTi β)2 + λ
p∑

j=1

β2j

Here the parameter λ controls the amount of shrink
applied to regression coefficients. A high value of λ
shrinks the coefficients close to zero (Hastie et al.,
2001). We used the implementation provided in the
glmnet package of R (Friedman et al., 2010), which
inherits a cross-validation mechanism that finds the
best value of λ on the training data.

3 Experimental Setup

We used a subset of the Spanish-English portion
of the WMT’12 Evaluation task. We selected 60
medium-length sentences which have been evalu-
ated previously by at least 2 different annotators.
For each sentence we selected the best and worst
translations according to a human evaluation score
based on the expected wins (Callison-Burch et al.,
2012). As a result, we had 60 references with two
corresponding translations each, adding up to a total
of 120 evaluation tasks. Each evaluation task was
performed by 6 different evaluators, resulting in 720
evaluations.

The annotators were presented with a translation-
reference pair at a time. The two evaluation tasks
corresponding to the same reference were presented
at two different times with at least 40 other tasks
in-between. This was done to prevent any possi-
ble spurious effects that may arise from remember-
ing the content of a first translation, when evaluating
the second translation of the same sentence. During
each evaluation task, the evaluators were asked to as-
sess the quality of a translation by providing a score
between 0–100 (Graham et al., 2013). The observed
inter-annotator agreement (Cohen’s kappa) among
our annotators was 0.321. This is slightly higher
than the overall inter-annotator agreement of 0.284
reported in WMT’12 for the Spanish-English.3 For
reading patterns we use the EyeTribe eye-tracker at

3For a rough comparison only. Note that these two num-
bers are not exactly comparable given that they are calculated
on different subsets of the same data. Still, there is a fair agree-
ment between the our evaluators and the expected wins from
WMT’12 (avg. pairwise kappa of 0.381)

a sampling frequency of 30Hz. Please refer to Ab-
delali et al. (2016) for our Eye-Tracking setup and
to know about iAppraise, an evaluation environment
that supports eye-tracking.

3.1 Evaluation

In our evaluation, we used eye-tracking features to
predict the quality of a translation in a pairwise sce-
nario in a protocol similar to the one from WMT’12.
First, we obtained the predicted scores ŷkA, ŷ

k
B for

translations A and B when evaluated by evaluator
k. Then, we computed the agreements w.r.t. the
scores ykA, y

k
B provided by the evaluator for the

same pair of translations. That is, we considered an
agreement when rankings were in order, e.g. ŷkA >
ŷkB ⇐⇒ ykA > ykB . Otherwise, we considered
it a disagreement. Finally, we computed Kendall’s
tau correlation coefficient as follows: τ = agg−disagg+dis .
We evaluated the performance using a 10-fold cross-
validation. While the folds were selected randomly,
we ensured that all translations corresponding to the
same sentence were included in the same fold, to
prevent any overlap between train and test.

4 Results

In this section, we first analyze the results of co-
herent feature sets to measure their predictive power
and to validate the intuitions about the information
they capture. Later, we use combination of features
and assess their suitability as evaluation metrics.

4.1 Gaze as a translation quality predictor

In Table 1, we show the results for the predictive
models trained on different feature sets. For simplic-
ity, we divide the feature groups in: reference only
features (I), translation only features (II), translation
and reference features (III); and lexicalized features
(IV). In the last group, we also add a tri-gram lan-
guage model scores for comparison purposes.

Reference only features In section I of the ta-
ble, we observe the prediction results for the mod-
els that only used features from the references. Un-
surprisingly, most of these features lack the predic-
tive power to determine whether translationA is bet-
ter than translation B (τ from 0.06 to 0.13). One
would expect that important phenomena that can be

1084



observed only on the reference (e.g. the overall dif-
ficulty of the sentence), are neutralized in a pairwise
setting, because an evaluator would read both in-
stances of the reference text similarly.4

However, some features like the dwell time (τ =
0.13) yield better results than others. This could be
explained by the need to go back to the reference,
when reading a confusing translation, thus spending
more time reading the reference.

Translation only features In section II, we ob-
serve the results for the translation features. At a
first glance, we realize that the correlation results
are much higher than for the reference features (τ
from 0.17 to 0.23). This supports the hypothesis
that reading patterns can help to distinguish good
from bad translations. Furthermore, it also supports
specific intuitions about these reading patterns. For
example, the fluency of a sentence is important (for-
ward jumps, τ = 0.17), but the number of regres-
sions are better predictors of the quality of a sen-
tence (τ = 0.22). Additionally, the time spent read-
ing a translation (dwell time) is a good predictor of
the quality (τ = 0.22). All of the above validate
the intuition that reading patterns capture informa-
tion about the quality of a translation. In general,
using translation eye-tracking features in a pairwise
evaluation, can help to predict which translation is
better.

Translation and reference features Reference
and translation features are not independent. Inter-
region jumps capture the number of times that eval-
uators go between translation and references before
making judgment. In section III, we observe that
these features can be useful to predict the quality of
a translation (τ = 0.18).

Lexicalized features In the last rows of the ta-
ble, we show that reading patterns help to evaluate
more than just the fluency of a translation. A simple
language model score (BLM ), is a weaker quality
predictor (τ = 0.17) than most of the eye-tracking
translation features. Using the lexicalized version of
the jump features gives additional predictive power
(τ = 0.22). Furthermore, by adding the total num-

4Although there could be differences based on correspond-
ing translation, which may result in different values for the ref-
erence features.

SYS Feature Sets (total features) τ

I. Eye-tracking: Reference
EyeReffj Forward jumps (5) 0.06
EyeRefbj Backward jumps (5) 0.11
EyeRefdist Total jump distance (1) 0.09
EyeRefvisit Total number of jumps (1) 0.10
EyeReftime Dwell time (1) 0.13

II. Eye-tracking: Translation
EyeTrafj Forward jumps (5) 0.17
EyeTrabj Backward jumps (5) 0.22
EyeTradist Total jump distance (1) 0.19
EyeTravisit Total number of jumps(1) 0.23
EyeTratime Dwell time (1) 0.22

III. Eye-tracking: Inter-region
EyeInter Jumps b/w regions (2) 0.18

IV. Lexicalized features
BLM Language model (6) 0.17
EyeLexall Lexicalized gaze jumps combined (6) 0.22

Table 1: Results of individual eye-tracking features
based on reference region, translation region, inter-
region and lexicalized information

ber of jumps and backward jumps to the LM fea-
tures, we would obtain a considerable gain in corre-
lation (τ = 0.30). This suggests that the reading
patterns capture information about more than just
fluency.

4.2 Gaze to build an evaluation metric

So far, we’ve shown that the individual sets of
features based on reading patterns can help to
predict translation quality, and that this goes beyond
simple fluency. One question that remains to be
answered is whether these features could be used
as a whole to evaluate the quality of a translation
semi-automatically. That is, whether we can use the
gaze information, and other lexical information to
anticipate the score that an evaluator will assign to
a translation. Here, we present evaluation results
combining several of these gaze features, and
compare them against BLEU (Papineni et al., 2002),
which uses lexical information and is designed to
measure not only fluency but also adequacy.

In Table 2, we present results in the following way:
in (I) we present the best non-lexicalized feature

1085



combinations that improve the predictive power of
the model. In (II) we re-introduce the results of lex-
icalized jumps feature. In (III) we present results
of BLEU and the combination of eye-tracking fea-
tures with it. Finally in (IV) we present the human-
to-human agreement measured in average Kendall’s
tau and in max human-to-human Kendall’s tau.

Combinations of translation jumps In section I
we present several combinations of features. All of
them include the backward jumps feature. This fea-
ture provides predictive power (τ = 0.22), which
is orthogonal to other features. This is in line with
our initial hypothesis that for a bad translation, an
evaluator needs to go back and forth several times to
understand it. Combining the backward jumps with
the total number of jumps (CTJ1) slightly increases
the correlation to τ = 0.25. Adding the jump dis-
tance (CTJ2) also increases its τ to 0.27. While this
correlation is lower than BLEU (τ = 0.34), it does
showcase the predictive power of the reading pat-
terns.

Combinations with BLEU When we combined
BLEU with the translation jumps, we observed an
increment in the τ to 0.37. Combining BLEU with
the lexicalized jumps, yields the best combination
(τ = 0.42). Although moderate, these increments
suggest that the reading patterns could be capturing
additional phenomenon besides adequacy and flu-
ency, such as structural complexity. These phenom-
ena remain to be explored in future work.

Human performance On average, evaluators
agreements with each other are fair (τ = 0.33) and
below the best combination (CB3), while the maxi-
mum agreement of any two evaluators is relatively
higher (τ = 0.53). This tells us that on average
the semi-automatic approach to evaluation that we
propose here is already competitive to predictions
done by another (average) human. However, there is
still room for improvement with respect to the most-
agreeing pair of evaluators.

5 Related Work

Eye-tracking devices have been used previously
in the MT research. Stymne et al. (2012) used
eye-tracking to identify and classify MT errors.

SYS Feature Sets τ

I. Combination of translation jumps
EyeTrabj Backward jumps 0.22
CTJ1 Backward jumps, total jumps 0.25
CTJ2 Backward jumps, total jumps, distance 0.27

II. Eye-tracking: Best Lexicalized
EyeLexall Lexicalized gaze jumps 0.22

III. Combinations with BLEU
Bbleu BLEU 0.34
CB1 Bbleu + EyeTrabj 0.38
CB2 Bbleu + CTJ2 0.39
CB3 Bbleu + EyeLexall 0.42

IV. Human performance
Avg Avg. human-to-human agreement 0.33
Max Max. human-to-human agreement 0.53

Table 2: Result of combining several jump and lex-
icalized features with BLEU. The column Feature
Sets shows the name of the systems whose features
are combined for that particular run. We also in-
cluded the average and maximum observed tau be-
tween any two evaluators, as a reference.

Doherty et al. (2010) conducted a study using eye-
tracking for MT evaluation and showed correlation
between fixations and BLEU scores. Doherty and
O’Brien (2014) evaluated the quality of machine
translation output in terms of its usability by an
end user. Guzmán et al. (2015) used eye-tracking
to show that having monolingual environment
improves the consistency of the evaluation.

Our work is different, as we: i) proposed novel eye-
tracking features and ii) model gaze movements to
predict human judgment.

6 Conclusion

We have shown that the reading patterns detected
through eye-tracking can be used to predict human
judgments of automatic translations. To this end, we
extracted novel lexicalized and non-lexicalized fea-
tures from the eye-tracking data motivated by no-
tions of reading difficulty, and used them to predict
the quality of a translation. We have shown that
these features capture more than just the fluency of
a translation, and provide complementary informa-
tion to BLEU. In combination, these features can
be used to produce semi-automatic metrics with im-
proved the correlation with human judgments.

1086



In the future, we plan to extend our experiments to
a large set of users and different language pairs. Ad-
ditionally we plan to improve the feature set to take
into account phenomena such as early termination,
i.e. when an evaluator makes a judgment before fin-
ishing reading a translation. We plan to deepen our
analysis to determine what kind of information is be-
ing used beyond fluency and adequacy.

References

Ahmed Abdelali, Nadir Durrani, and Francisco Guzmán.
2016. iAppraise: A Manual Machine Translation
Evaluation Environment Supporting Eye-tracking. In
Proceedings of the 2016 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics: Human Language Technologies,
San Diego, California.

Chris Callison-Burch, Cameron Fordyce, Philipp Koehn,
Christof Monz, and Josh Schroeder. 2007. (Meta-)
Evaluation of Machine Translation. In Proceedings of
the Second Workshop on Statistical Machine Transla-
tion, Prague, Czech Republic.

Chris Callison-Burch, Philipp Koehn, Christof Monz,
Matt Post, Radu Soricut, and Lucia Specia. 2012.
Findings of the 2012 Workshop on Statistical Machine
Translation. In Proceedings of the Seventh Work-
shop on Statistical Machine Translation, pages 10–
51, Montréal, Canada. Association for Computational
Linguistics.

Charles Clifton, Adrian Staub, and Keith Rayner. 2007.
Eye Movements in Reading Words and Sentences. Eye
Movements: A Window on Mind and Brain, pages
341–372.

Moreno I. Coco and Frank Keller. 2015. The Interac-
tion of Visual and Linguistic Saliency during Syntac-
tic Ambiguity Resolution. The Quarterly Journal of
Experimental Psychology, 68(1):46–74.

Stephen Doherty and Sharon O’Brien. 2014. Assess-
ing the Usability of Raw Machine Translated Out-
put: A User-Centered Study Using Eye Tracking. In-
ternational Journal of Human-Computer Interaction,
30(1):40–51.

Stephen Doherty, Sharon O’Brien, and Michael Carl.
2010. Eye Tracking as an Automatic MT Evaluation
Technique. Machine translation, 24(1):1–13.

Nadir Durrani, Helmut Schmid, and Alexander Fraser.
2011. A Joint Sequence Translation Model with In-
tegrated Reordering. In Proceedings of the Associa-
tion for Computational Linguistics: Human Language
Technologies (ACL-HLT’11), Portland, OR, USA.

Jerome Friedman, Trevor Hastie, and Rob Tibshirani.
2010. Regularization Paths for Generalized Linear
Models via Coordinate Descent. Journal of Statistical
Software, 33(1):1–22.

Simon Garrod. 2006. Psycholinguistic Research Meth-
ods. The Encyclopedia of Language and Linguistics,
2:251–257.

Yvette Graham, Timothy Baldwin, Alistair Moffat, and
Justin Zobel. 2013. Continuous Measurement Scales
in Human Evaluation of Machine Translation. In Pro-
ceedings of the 7th Linguistic Annotation Workshop
and Interoperability with Discourse, Sofia, Bulgaria.

Francisco Guzmán, Ahmed Abdelali, Irina Temnikova,
Hassan Sajjad, and Stephan Vogel. 2015. How do Hu-
mans Evaluate Machine Translation. In Proceedings
of the 10th Workshop on Statistical Machine Transla-
tion, Lisbon, Portugal.

Dan Witzner Hansen and Qiang Ji. 2010. In the Eye
of the Beholder: A Survey of Models for Eyes and
Gaze. Pattern Analysis and Machine Intelligence,
IEEE Transactions on, 32(3):478–500.

Trevor A Harley. 2013. The Psychology of Language:
From Data to Theory. Psychology Press.

Trevor Hastie, Robert Tibshirani, and Jerome Friedman.
2001. The Elements of Statistical Learning. Springer
Series in Statistics. Springer New York Inc., New
York, NY, USA.

Marcel A. Just and Patricia A. Carpenter. 1980. A The-
ory of Reading: From Eye Fixations to Comprehen-
sion. Psychological review, 87(4):329.

Paul-Philipp Metzner. 2015. Eye Movements and Brain
Responses in Natural Reading. Ph.D. thesis, Univer-
sity of Potsdam.

Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: a method for automatic eval-
uation of machine translation. In Proceedings of the
Association for Computational Linguistics, ACL ’02,
pages 311–318, Philadelphia, PA, USA.

Mary C. Potter. 1983. Representational Buffers: The
Eye-Mind Hypothesis in Picture Perception, Reading,
and Visual Search. Eye Movements in Reading: Per-
ceptual and Language Processes, pages 423–437.

Keith Rayner. 1998. Eye Movements in Reading and
Information Processing: 20 Years of Research. Psy-
chological bulletin, 124(3):372.

Elizabeth R. Schotter, Randy Tran, and Keith Rayner.
2014. Dont Believe What You Read (Only
Once) Comprehension Is Supported by Regressions
During Reading. Psychological science, page
0956797614531148.

Matthew Snover, Bonnie Dorr, Richard Schwartz, Lin-
nea Micciulla, and John Makhoul. 2006. A Study of
Translation Edit Rate with Targeted Human Annota-
tion. In Proceedings of the 7th Biennial Conference of

1087



the Association for Machine Translation in the Ameri-
cas, Cambridge, Massachusetts, USA.

Sara Stymne, Henrik Danielsson, Sofia Bremin,
Hongzhan Hu, Johanna Karlsson, Anna Prytz Lillkull,
and Martin Wester. 2012. Eye Tracking as a Tool
for Machine Translation Error Analysis. In Proceed-
ings of the International Conference on Language
Resources and Evaluation, Istanbul, Turkey.

Joseph Turian, Luke Shen, and I. Dan Melamed. 2003.
Evaluation of Machine Translation and its Evaluation.
In Proceedings of Machine Translation Summit IX,
New Orleans, LA, USA.

John White, Theresa O’Connell, and Francis O’Mara.
1994. The ARPA MT Evaluation Methodologies:
Evolution, Lessons, and Future Approaches. In Pro-
ceedings of the Association for Machine Translation in
the Americas Conference, Columbia, Maryland, USA.

1088


