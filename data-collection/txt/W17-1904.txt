



















































Improving Clinical Diagnosis Inference through Integration of Structured and Unstructured Knowledge


Proceedings of the 1st Workshop on Sense, Concept and Entity Representations and their Applications, pages 31–36,
Valencia, Spain, April 4 2017. c©2017 Association for Computational Linguistics

Improving Clinical Diagnosis Inference through Integration of Structured
and Unstructured Knowledge

Yuan Ling, Yuan An
College of Computing & Informatics

Drexel University
Philadelphia, PA, USA

{yl638,ya45}@drexel.edu

Sadid A. Hasan
Artificial Intelligence Laboratory
Philips Research North America

Cambridge, MA, USA
sadid.hasan@philips.com

Abstract

This paper presents a novel approach to
the task of automatically inferring the
most probable diagnosis from a given
clinical narrative. Structured Knowledge
Bases (KBs) can be useful for such com-
plex tasks but not sufficient. Hence, we
leverage a vast amount of unstructured
free text to integrate with structured KBs.
The key innovative ideas include building
a concept graph from both structured and
unstructured knowledge sources and rank-
ing the diagnosis concepts using the en-
hanced word embedding vectors learned
from integrated sources. Experiments on
the TREC CDS and HumanDx datasets
showed that our methods improved the re-
sults of clinical diagnosis inference.

1 Introduction and Related Work

Clinical diagnosis inference is the problem of au-
tomatically inferring the most probable diagno-
sis from a given clinical narrative. Many health-
related information retrieval tasks can greatly ben-
efit from the accurate results of clinical diagnosis
inference. For example, in recent Text REtrieval
Conference (TREC) Clinical Decision Support
track (CDS1), diagnosis inference from medical
narratives has improved the accuracy of retriev-
ing relevant biomedical articles (Roberts et al.,
2015; Hasan et al., 2015; Goodwin and Harabagiu,
2016).

Solutions to the clinical diagnostic inferencing
problem require a significant amount of inputs
from domain experts and a variety of sources (Fer-
rucci et al., 2013; Lally et al., 2014). To ad-
dress such complex inference tasks, researchers
(Yao and Van Durme, 2014; Bao et al., 2014;
Dong et al., 2015) have utilized structured KBs

1http://www.trec-cds.org/

that store relevant information about various en-
tity types and relation triples. Many large-scale
KBs have been constructed over the years, such
as WordNet (Miller, 1995), Yago (Suchanek et
al., 2007), Freebase (Bollacker et al., 2008), DB-
pedia (Auer et al., 2007), NELL (Carlson et al.,
2010), UMLS Metathesaurus (Bodenreider, 2004)
etc. However, using KBs alone for inference tasks
(Bordes et al., 2014) has certain limitations such as
incompleteness of knowledge, sparsity, and fixed
schema (Socher et al., 2013; West et al., 2014).

On the other hand, unstructured textual re-
sources such as free texts from Wikipedia gen-
erally contain more information than structured
KBs. As a supplementary knowledge to mitigate
the limitations of structured KBs, unstructured text
combined with structured KBs provides improved
results for related tasks, for example, clinical ques-
tion answering (Miller et al., 2016). For pro-
cessing text, word embedding models (e.g. skip-
gram model (Mikolov et al., 2013b; Mikolov et
al., 2013a)) can efficiently discover and represent
the underlying patterns of unstructured text. Word
embedding models represent words and their rela-
tionships as continuous vectors. To improve word
embedding models, previous works have also suc-
cessfully leveraged structured KBs (Bordes et al.,
2011; Weston et al., 2013; Wang et al., 2014; Zhou
et al., 2015; Liu et al., 2015).

Motivated by the superior power of the integra-
tion of structured KBs and unstructured free text,
we propose a novel approach to clinical diagno-
sis inference. The novelty lies in the ways of in-
tegrating structured KBs with unstructured text.
Experiments showed that our methods improved
clinical diagnosis inference from different aspects
(Section 5.4). Previous work on diagnosis in-
ference from clinical narratives either formulates
the problem as a medical literature retrieval task
(Zheng and Wan, 2016; Balaneshin-kordan and
Kotov, 2016) or as a multiclass multilabel classi-

31



fication problem in a supervised setting (Hasan et
al., 2016; Prakash et al., 2016). To the best of our
knowledge, there is no work on diagnoses infer-
ence from clinical narratives conducted in an un-
supervised way. Thus, we build such baselines for
this task.

2 Overview of the Approach

Our approach includes four steps in general: 1)
extracting source concepts, q, from clinical narra-
tives, 2) iteratively identifying corresponding ev-
idence concepts, a, from KBs and unstructured
text, 3) representing both source and evidence
concepts in a weighted graph via a regularizer-
enhanced skip-gram model, and 4) ranking the rel-
evant evidence concepts (i.e. diagnoses) based on
their association with the source concepts, S(q, a)
(computed by weighted dot product of two vec-
tors), to generate the final output. Figure 1 shows
the overview using an illustrative example.

Given source concepts as input, we build an
edge-weighted graph representing the connections
among all the concepts by iteratively retrieving
evidence concepts from both KBs and unstruc-
tured text. The weights of the edges represent the
strengths of the relationships between concepts.
Each concept is represented as a word embedding
vector. We combine all the source concept vec-
tors into a single vector representing a clinical sce-
nario. Source concepts are differentiated accord-
ing to the weighting scheme in Section 4.2. Evi-
dence concepts are also represented as vectors and
ranked according to their relevance to the source
concepts. For each clinical case, we find the most
probable diagnoses from the top-ranked evidence
concepts.

3 Knowledge Sources of Evidence
Concepts

In this study, we use UMLS Metathesaurus (Bo-
denreider, 2004) and Freebase (Bollacker et al.,
2008) as the structured KBs. Both KBs pro-
vide semantic relation triples in the following
format: <concept1, relation, concept2>. We
select UMLS relation types that are relevant
to the problem of clinical diagnosis inference.
These types include disease-treatment, disease-
prevention, disease-finding, sign or symptom,
causes etc. Freebase contains a large number
of triples from multiple domains. We select
61,243 triples from freebase that are classified as

medicine relation types. There are 19 such rela-
tion types in total. Most of them fall under the
“medicine.disease” category.

For unstructured text, we use articles from
Wikipedia and MayoClinic corpus as the supple-
mentary knowledge source. Important clinical
concepts mentioned in a Wikipedia/MayoClinic
page can serve as a critical clue to a clinical di-
agnosis. For example, in Figure 1, we see that
“dyspnea”, “shortness of breath”, “tachypnea” etc.
are the related signs and symptoms of the “Pul-
monary Embolism” diagnosis. We select 37,245
Wikipedia pages under the clinical diseases and
medicine category in this study. Most of the
page titles represent disease names. In addition,
MayoClinic2 disease corpus contains 1,117 pages,
which include sections of Symptoms, Causes,
Risk Factors, Treatments and Drugs, Prevention,
etc.

4 Methodology

4.1 Building Weighted Concept Graph
Both the source and the evidence concepts are
represented as nodes in a graph. A clinical case
is represented as a set of source concept nodes:
q = {q1, q2, . . .}. We build a weighted concept
graph from source concepts using Algorithm 1.

Algorithm 1: Build Concept Graph
Input : source concept nodes q
Output: graphG = (V,E)

1 S = q and V = q;
2 while S 6= ∅ do
3 for each qi in S do
4 if distance(qi, q) > 2 then
5 continue;
6 end
7 if triple< qi, r, aj > in KBs then
8 wij = 1;
9 e = (qi, aj) and e.value = wij ;

10 insert aj to V and S;
11 insert e toE;
12 end
13 Use qi as query, search in Unstructured Text Corpora, get

ResultR;
14 for each page-similarity pair (p, sij) inR do
15 e = (qi, title(p)) and e.value = sij ;
16 insert title(p) to V and S;
17 insert e toE;
18 end
19 remove qi from S;
20 end
21 end

Two kinds of evidence concept nodes are added
to the graph: 1) the entities from KBs (UMLS and
Freebase) (step 7-12 in Algorithm 1), and 2) the
entities from unstructured text pages (step 13-18).
If there exists a triple < qi, r, aj > in KBs, where

2http://www.mayoclinic.org/diseases-conditions

32



Figure 1: Overview of our system.

r refers to a relation, an edge is used to connect
node qi and node aj . wij represents the weight for
that edge, and let wij = 1, if the corresponding
triple occurs at least once. Due to the incomplete-
ness of the KBs, there may exist multiple missing
connections between a potential evidence concept
aj and a source concept qi. Unstructured knowl-
edge from Wikipedia and MayoClinic can replen-
ish these missing connections. For each page p,
the page title represents an evidence concept aj .
We use each source concept qi as a query, and
page p as a document, and then calculate a query-
document similarity to measure the edge weight
wij between node aj and node qi. We only take
evidence concepts as all nodes connected to source
concepts in a distance of at most 2 (step 4-6).

4.2 Representing Clinical Case

We combine the source concepts q and get a sin-
gle vector vq to represent the clinical case narra-
tive. The source concepts from narratives for clin-
ical diagnosis inference should be differentiated.
Some source concepts are major symptoms for a
diagnosis, while others are less critical. These ma-
jor source concepts should be identified and given
higher weight values. We develop two kinds of
weighting schema for the differential expression
of the source concepts. The source concept is rep-
resented as vq = 1N

∑
qi∈q γivqi . N is the total

number of source concepts. vqi is the vector rep-
resentation for one source concept qi.

(1) A longer concept usually convey more infor-
mation (e.g. malar rash vs. rash), so it should be
given more weights. We define this weight value
as γ1 = #Words inConcept.

(2) For some commonly seen concepts (e.g.
fever), usually, there are more edges connected to
them. Sometimes, a common concept is less im-
portant for diagnosis inference, while some unique
concepts are critical to infer a specific diagnosis.
We define this weight value for each concept as
γ2 = 1#Connected Edges . A higher weight value
means the source concept is more unique.

4.3 Inferring Concepts for Diagnosis

Extracting Potential Evidence Concepts: From
source concept nodes q, we find their con-
nected concepts in the graph as evidence concepts.
Traversing all edges in a graph is computation-
ally expensive and often unnecessary for finding
potential diagnoses. The solution is to use a sub-
graph. We follow the idea proposed in Bordes et
al. (2014). The evidence concepts are defined as
all nodes connected to source concepts in a dis-
tance of at most 2.

Ranking Evidence Concepts: We rank each
evidence concept a′ according to its matching
score S(q, a′) to the source concepts. The match-
ing score S(q, a′) is a dot product of embedding
representation of the evidence concept a′ and the
source concept q by taking the edge weights wij
into consideration. S(q, a′) = wijva′ · vq. va′ and

33



vq are embedding representations for a′ and q. The
embedding E ∈ Rk×N for concepts are trained
using embedding models (Section 4.4). N is the
total number of concepts and k is the predefined
dimensions for the embedding vector. Each con-
cept in the graph can find a k dimensional vector
representation in E. For a set of source concepts
and evidence concepts A(q), the top-ranked evi-
dence concept can be computed as:

a = argmax(a′∈A(q))S(q, a
′) (1)

4.4 Word Embedding Models

We use the skip-gram model as the basic model.
The skip-gram model predicts surrounding words
wt−c, . . . , wt−1, wt+1, . . . , wt+c given the current
center word wt. We further enhance the skip-gram
model by adding a graph regularizer. Given a se-
quence of training words w1, w2, . . . , wT , the ob-
jective function is:

J = max
1

T

T∑
t=1

(1−λ)
∑

−c≤j≤c,j 6=0
log p(wt+j |wt)−λ

R∑
r=1

D(vt, vr),

(2)

where vt and vr are the representation vectors for
wordwt and wordwr. λ is a parameter to leverage
the graph regularizer and original objective. Sup-
pose, wordwt is mentioned having relations with a
set of other wordswr, r ∈ {1, . . . , R} in KBs. The
graph regularizer λ

∑R
r=1D(vt, vr) integrates ex-

tra knowledge about semantic relationships among
words within the graph structure. D(vt, vr) repre-
sents the distance between vt and vr. In our ex-
periments, the distance between two concepts is
measured using KL-Divergence. D(vt, vr) can be
calculated using any other types of distance met-
rics. By minimizing D(vt, vr), we expect if two
concepts have a close relation in KBs, their vector
representations will also be close to each other.

5 Experiments

5.1 Datasets for Clinical Diagnosis Inference

Our first dataset is from the 2015 TREC CDS
track (Roberts et al., 2015). It contains 30 top-
ics, where each topic is a medical case narrative
that describes a patient scenario. Each case is as-
sociated with the ground truth diagnosis. We use
MetaMap3 to extract the source concepts from a
narrative and then manually refine them to remove
redundancy.

3https://metamap.nlm.nih.gov/

Our second dataset is curated from HumanDx4,
a project to foster integrating efforts to map health
problems to their possible diagnoses. We curate
diagnosis-findings relationships from HumanDx
and create a dataset with 459 diagnosis-findings
entries. Note that, the findings from this dataset
are used as the given source concepts for a clinical
scenario.

5.2 Training Data for Word Embeddings

We curate a biomedical corpus of around 5M
sentences from two data sources: PubMed Cen-
tral5 from the 2015 TREC CDS snapshot6 and
Wikipedia articles under the “Clinical Medicine”
category7. After sentence splitting, word tok-
enization, and stop words removal, we train our
word embedding models on this corpus. UMLS
Metathesaurus and Freebase are used as KBs to
train the graph regularizer. We use stochastic gra-
dient descent (SGD) to maximize the objective
function and set the parameters empirically.

5.3 Results

We use Mean Reciprocal Rank (MRR) and Aver-
age Precision at 5 (P@5) to evaluate our models.
MRR is a statistical measure to evaluate a process
that generates a list of possible responses to a sam-
ple of queries, ordered by probability of correct-
ness. Average P@5 is calculated as precision at
top 5 predicted results divided by the total number
of topics. Since our dataset only has one correct
diagnosis for each topic, all results have poor Av-
erage P@5 scores.

Table 1 presents the results for our experi-
ments. We report two baselines: Skip-gram refers
to the basic word embedding model, and Skip-
gram* refers to the graph-regularized model us-
ing KBs. We also show the results for using dif-
ferent unstructured knowledge sources and differ-
ent weighting schema. We can see that the best
scores are obtained by the graph-regularized mod-
els with both the unstructured knowledge sources
with variable weighting schema (Section 4.2).

5.4 Discussion

Unstructured text is a critical supplement: We
analyze the source concepts and the corresponding
evidence concepts for CDS topics, and investigate

4https://www.humandx.org/
5https://www.ncbi.nlm.nih.gov/pmc/
6http://www.trec-cds.org/2015.html#documents
7https://en.wikipedia.org/wiki/Category:Clinical medicine

34



TREC CDS HumanDx
Method MRR Average P@5 MRR Average P@5

Baselines
Skip-gram 21.66 8.88 18.56 5.08
Skip-gram* 22.60 8.88 18.63 5.15

Skip-gram* + Different Unstructured Text Datasets
Wikipedia 26.01 8.96 19.42 5.76
MayoClinic 32.64 9.52 19.46 5.80
Both 32.29 9.60 19.12 5.76

Skip-gram* + Both Text Datasets + Different Weights
γ1 32.22 10.40 21.09 5.88
γ2 32.77 12.00 20.86 5.93

Table 1: Evaluation results.

the origin of the correct diagnoses. 70% of the
correct diagnoses can be inferred from Wikipedia,
60% of the correct diagnoses from MayoClinic,
56% of the correct diagnoses from Freebase, and
only 7% are from UMLS. Hence, Wikipedia and
MayoClinic are very important sources for finding
the correct diagnoses.

Source concepts should be differentiated: In
clinical narratives, some concepts are more critical
than others for the clinical diagnosis inference. We
developed two weighting schema to assign higher
weight values to more important concepts. The re-
sults in Table 1 show that differentiating the source
concepts with different weight values has a large
impact on the model performance.

Enhanced skip-gram is better: We propose
the enhanced skip-gram model by using a graph
regularizer to integrate the semantic relationships
among concepts from KBs. Experimental results
show that diagnosis inference is improved by us-
ing word embedding representations from the en-
hanced skip-gram model.

6 Conclusion

We proposed a novel approach to the task of clin-
ical diagnosis inference from clinical narratives.
Our method overcomes the limitations of struc-
tured KBs by making use of the integrated struc-
tured and unstructured knowledge. Experimen-
tal results showed that the enhanced skip-gram
model with differential expression of source con-
cepts improved the performance on two bench-
mark datasets.

References
Sören Auer, Christian Bizer, Georgi Kobilarov, Jens

Lehmann, Richard Cyganiak, and Zachary Ives.
2007. Dbpedia: A nucleus for a web of open data.
In The semantic web, pages 722–735. Springer.

Saeid Balaneshin-kordan and Alexander Kotov. 2016.
Optimization method for weighting explicit and la-
tent concepts in clinical decision support queries.
In Proceedings of the 2016 ACM on International
Conference on the Theory of Information Retrieval,
pages 241–250. ACM.

Junwei Bao, Nan Duan, Ming Zhou, and Tiejun Zhao.
2014. Knowledge-based question answering as ma-
chine translation. Cell, 2(6).

Olivier Bodenreider. 2004. The unified medical lan-
guage system (umls): integrating biomedical termi-
nology. Nucleic acids research, 32(suppl 1):D267–
D270.

Kurt Bollacker, Colin Evans, Praveen Paritosh, Tim
Sturge, and Jamie Taylor. 2008. Freebase: a col-
laboratively created graph database for structuring
human knowledge. In Proceedings of the 2008 ACM
SIGMOD international conference on Management
of data, pages 1247–1250. ACM.

Antoine Bordes, Jason Weston, Ronan Collobert, and
Yoshua Bengio. 2011. Learning structured embed-
dings of knowledge bases. In Conference on artifi-
cial intelligence.

Antoine Bordes, Sumit Chopra, and Jason Weston.
2014. Question answering with subgraph embed-
dings. arXiv preprint arXiv:1406.3676.

Andrew Carlson, Justin Betteridge, Bryan Kisiel,
Burr Settles, Estevam R Hruschka Jr, and Tom M
Mitchell. 2010. Toward an architecture for never-
ending language learning. In AAAI, volume 5,
page 3.

Li Dong, Furu Wei, Ming Zhou, and Ke Xu.
2015. Question answering over freebase with multi-
column convolutional neural networks. In Proceed-
ings of Association for Computational Linguistics,
pages 260–269.

David Ferrucci, Anthony Levas, Sugato Bagchi, David
Gondek, and Erik T Mueller. 2013. Watson: beyond
jeopardy! Artificial Intelligence, 199:93–105.

Travis R. Goodwin and Sanda M. Harabagiu. 2016.
Medical question answering for clinical decision

35



support. In Proceedings of the 25th ACM Inter-
national on Conference on Information and Knowl-
edge Management, pages 297–306. ACM.

Sadid A. Hasan, Yuan Ling, Joey Liu, and Oladimeji
Farri. 2015. Using neural embeddings for diag-
nostic inferencing in clinical question answering. In
TREC.

Sadid A. Hasan, Siyuan Zhao, Vivek Datla, Joey Liu,
Kathy Lee, Ashequl Qadir, Aaditya Prakash, and
Oladimeji Farri. 2016. Clinical question answering
using key-value memory networks and knowledge
graph. In TREC.

Adam Lally, Sugato Bachi, Michael A. Barborak,
David W. Buchanan, Jennifer Chu-Carroll, David A.
Ferrucci, Michael R. Glass, Aditya Kalyanpur,
Erik T. Mueller, J. William Murdock, et al. 2014.
Watsonpaths: scenario-based question answering
and inference over unstructured information. York-
town Heights: IBM Research.

Quan Liu, Hui Jiang, Si Wei, Zhen-Hua Ling, and
Yu Hu. 2015. Learning semantic word embeddings
based on ordinal knowledge constraints. In Pro-
ceedings of the 53rd Annual Meeting of the Associ-
ation for Computational Linguistics and the 7th In-
ternational Joint Conference on Natural Language
Processing (ACL-IJCNLP), pages 1501–1511.

Tomas Mikolov, Kai Chen, Greg Corrado, and Jef-
frey Dean. 2013a. Efficient estimation of word
representations in vector space. arXiv preprint
arXiv:1301.3781.

Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S. Cor-
rado, and Jeff Dean. 2013b. Distributed representa-
tions of words and phrases and their compositional-
ity. In Advances in neural information processing
systems, pages 3111–3119.

Alexander Miller, Adam Fisch, Jesse Dodge, Amir-
Hossein Karimi, Antoine Bordes, and Jason We-
ston. 2016. Key-value memory networks for di-
rectly reading documents. CoRR, abs/1606.03126.

George A. Miller. 1995. Wordnet: a lexical
database for english. Communications of the ACM,
38(11):39–41.

Aaditya Prakash, Siyuan Zhao, Sadid A. Hasan, Vivek
Datla, Kathy Lee, Ashequl Qadir, Joey Liu, and
Oladimeji Farri. 2016. Condensed memory net-
works for clinical diagnostic inferencing. arXiv
preprint arXiv:1612.01848.

Kirk Roberts, Matthew S. Simpson, Ellen Voorhees,
and William R. Hersh. 2015. Overview of the trec
2015 clinical decision support track. In TREC.

Richard Socher, Danqi Chen, Christopher D. Manning,
and Andrew Ng. 2013. Reasoning with neural ten-
sor networks for knowledge base completion. In Ad-
vances in Neural Information Processing Systems,
pages 926–934.

Fabian M. Suchanek, Gjergji Kasneci, and Gerhard
Weikum. 2007. Yago: a core of semantic knowl-
edge. In Proceedings of the 16th international con-
ference on World Wide Web, pages 697–706. ACM.

Zhen Wang, Jianwen Zhang, Jianlin Feng, and Zheng
Chen. 2014. Knowledge graph and text jointly em-
bedding. In EMNLP, pages 1591–1601. Citeseer.

Robert West, Evgeniy Gabrilovich, Kevin Murphy,
Shaohua Sun, Rahul Gupta, and Dekang Lin. 2014.
Knowledge base completion via search-based ques-
tion answering. In Proceedings of the 23rd inter-
national conference on World wide web, pages 515–
526. ACM.

Jason Weston, Antoine Bordes, Oksana Yakhnenko,
and Nicolas Usunier. 2013. Connecting language
and knowledge bases with embedding models for re-
lation extraction. arXiv preprint arXiv:1307.7973.

Xuchen Yao and Benjamin Van Durme. 2014. Infor-
mation extraction over structured data: Question an-
swering with freebase. In ACL (1), pages 956–966.
Citeseer.

Ziwei Zheng and Xiaojun Wan. 2016. Graph-based
multi-modality learning for clinical decision sup-
port. In Proceedings of the 25th ACM International
on Conference on Information and Knowledge Man-
agement, pages 1945–1948. ACM.

Guangyou Zhou, Tingting He, Jun Zhao, and Po Hu.
2015. Learning continuous word embedding with
metadata for question retrieval in community ques-
tion answering. In Proceedings of ACL, pages 250–
259.

36


