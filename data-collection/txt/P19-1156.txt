



















































Better Character Language Modeling through Morphology


Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 1606–1613
Florence, Italy, July 28 - August 2, 2019. c©2019 Association for Computational Linguistics

1606

Better Character Language Modeling Through Morphology

Terra Blevins1 and Luke Zettlemoyer1,2
1Paul G. Allen School of Computer Science & Engineering, University of Washington

2Facebook AI Research
{blvns, lsz}@cs.washington.edu

Abstract

We incorporate morphological supervision
into character language models (CLMs) via
multitasking and show that this addition im-
proves bits-per-character (BPC) performance
across 24 languages, even when the morphol-
ogy data and language modeling data are dis-
joint. Analyzing the CLMs shows that in-
flected words benefit more from explicitly
modeling morphology than uninflected words,
and that morphological supervision improves
performance even as the amount of language
modeling data grows. We then transfer mor-
phological supervision across languages to im-
prove language modeling performance in the
low-resource setting.

1 Introduction

Character language models (CLMs) are distri-
butions over sequences of characters (Sutskever
et al., 2011), in contrast to traditional language
models which are distributions over sequences of
words. CLMs eliminate the need for a fixed
word vocabulary, and modeling text at the charac-
ter level gives the CLM access to subword infor-
mation. These attributes suggest that CLMs can
model regularities that exist within words, such
as morphological inflection. However, even large
language modeling (LM) corpora have sparse cov-
erage of inflected forms for morphologically-rich
languages, which has been shown to make word
and character language modeling more difficult
(Gerz et al., 2018b; Cotterell et al., 2018). Be-
cause to this, we hypothesize that accurately mod-
eling morphology improves language modeling,
but that it is difficult for CLMs to learn this from
text alone.

Motivated by this hypothesis, we add morphol-
ogy supervision to character language modeling
and show that, across two benchmark datasets,
multitasking morphology with CLMs improves

bits-per-character (BPC) performance on twenty-
four languages, even when the annotated morphol-
ogy features and language modeling data do not
overlap. We also show that models augmented
by multitasking achieve better BPC improvements
on inflected forms than on uninflected forms, and
that increasing the amount of language modeling
data does not diminish the gains from morphol-
ogy. Furthermore, to augment morphology anno-
tations in low-resource languages, we also transfer
morphology information between pairs of high-
and low-resource languages. In this cross-lingual
setting, we see that morphology supervision from
the high-resource language improves BPC perfor-
mance on the low-resource language over both
the low-resource multitask model and over adding
language modeling data from the high-resource
language alone.

2 Approach

Language Modeling Given a sequence of char-
acters c = c1, c2, ..., cn, our character-level lan-
guage models calculate the probability of c as

p(c) =
|c|∏
i=1

p(ci|c1, c2, ..., ci−1) (1)

Each distribution is an LSTM (Hochreiter and
Schmidhuber, 1997) trained such that at each time
step t, the model takes in a character ct and es-
timates the probability of the next character ct+1
as

p(ct+1|c≤t) = g(LSTM(wt,ht−1)) (2)

where ht−1 is the previous hidden state of the
LSTM, wt is the character embedding learned by
the model for ct, and g is a softmax over the char-
acter vocabulary space.

We calculate the loss function of our language
model LLM as the negative log-likelihood of the



1607

CS DE EN ES FI FR RU
dev test dev test dev test dev test dev test dev test dev test

HCLM 2.010 1.984 1.605 1.588 1.591 1.538 1.548 1.498 1.754 1.711 1.499 1.467 1.777 1.761
LM 2.013 1.972 1.557 1.538 1.543 1.488 1.571 1.505 1.725 1.699 1.357 1.305 1.745 1.724

MTL 1.938 1.900 1.249 1.241 1.313 1.256 1.260 1.196 1.698 1.669 1.211 1.167 1.645 1.619
∆ 0.075 0.072 0.308 0.297 0.230 0.232 0.311 0.309 0.027 0.030 0.146 0.138 0.100 0.105

Table 1: Results on Multilingual Wikipedia Corpus (MWC) in bits per character (BPC). ∆ calculated improvement
in BPC from the baseline LM to MTL. HCLM is the best model from Kawakami et al. (2017).

model on the character sequence c:

LLM(c) = NLL(c) = −
|c|∑
i=1

log p(ci|c<i) (3)

We then evaluate the trained model’s perfor-
mance with bits-per-character (BPC):

BPC(c) = − 1
|c|

|c|∑
i=1

log p(ci|c<i) (4)

Multitask Learning To add morphology fea-
tures as supervision, we use a multitask learn-
ing (MTL) objective (Collobert and Weston, 2008)
that combines loss functions for predicting differ-
ent morphological tags with the language model-
ing objective. Since morphological features are
annotated at the word-level, we convert these an-
notations to the character level by placing each
annotated word’s tags as supervision on the first
character (which we found to outperform super-
vising the last character in preliminary results).

This early placement allows the model to have
access to the morphological features while decod-
ing the rest of the characters in the word. There-
fore, our morphology data m = m1,m2, ...,mn
is a sequence of labeled pairs in the form mi =
(x, y) where x is a character and y is a set of mor-
phology tags for that character. For example, “cats
ran” would be given to our model as the sequence
(‘c’, Number=Pl), (‘a’, -), (‘t’, -), (‘s’, -), (‘ ’, -),
(‘r’, Tense=Past), (‘a’, -), (‘n’, -).

We modify the model’s loss function to

L(c,m) = LLM(c) + δ
n∑

i=1

Li(m) (5)

where n is the number of morphological features
we have annotated in a language, δ is a weight-
ing parameter between the primary and auxiliary
losses, LLM is the original language modeling
loss, and Li are the additional losses for each mor-
phological feature (e.g., tense, number, etc). Be-

cause we include a separate loss for each morpho-
logical feature, each feature is predicted indepen-
dently.

3 Experimental Setup

Datasets We obtain morphological annotations
for 24 languages (Table 2) from Universal Depen-
dencies (UD; v.2.3), which consists of dependency
parsing treebanks with morphology annotations on
a large number of languages (Nivre et al., 2018).
These languages were chosen based on the size
of their treebanks (to ensure a sufficient amount
of morphology annotations); we also exclude lan-
guages that do not have morphology features an-
notated in the treebank.

For language modeling supervision, we train
two sets of models. One set is trained with the
text from the UD treebanks; the other set of mod-
els is trained on the Multilingual Wikipedia Cor-
pus (MWC) (Kawakami et al., 2017). This lan-
guage modeling dataset consists of Wikipedia data
across seven languages (Czech, German, English,
Spanish, Finnish, French, and Russian).

Model architecture Our models each consist of
a stacked LSTM with 1024 hidden dimensions
and a character embedding layer of 512 dimen-
sions. We include two hidden layers in the lan-
guage models trained on UD, and three hidden lay-
ers in those trained on MWC. The parameters that
integrate multitasking into the model (the layer at
which we multitask morphology and the weight-
ing we give the morphology losses, δ) are tuned
individually for each language. Further hyperpa-
rameter and training details are given in the sup-
plement.

4 Language Modeling Results

Distant MTL We first train CLMs where the
language modeling data (from MWC) and mor-
phology data (from UD) do not overlap (Table 1).1

1Since both of these datasets draw from Wikipedia, we
verified that no sentences overlap between the MWC test set



1608

(a) (b) (c)

Figure 1: Improvement of MTL over the LM baseline (a) over the inflection rate of each UD language, (b) over
the quantity of training data for each UD language, and (c) for inflected and uninflected words in the UD dev set.

Lang ISO %Infl LM MTL ∆
Bulgarian BG 39% 1.890 1.887 0.003
Catalan CA 31% 1.653 1.599 0.054
Czech CS 43% 2.045 1.832 0.213
Danish DA 30% 2.152 2.135 0.017
German DE 33% 1.917 1.881 0.036
English EN 15% 2.183 2.173 0.010
Spanish ES 28% 1.801 1.763 0.038

Farsi FA 27% 2.213 2.205 0.008
French FR 32% 1.751 1.712 0.039
Hindi HI 28% 1.819 1.773 0.046

Croatian HR 49% 1.866 1.841 0.025
Italian IT 36% 1.595 1.554 0.041

Latvian LV 47% 2.243 2.217 0.026
Dutch NL 19% 1.989 1.972 0.017
Polish PL 42% 2.218 2.154 0.064

Portuguese PT 31% 1.787 1.785 0.002
Romanian RO 42% 1.840 1.798 0.042
Russian RU 42% 1.993 1.824 0.169
Slovak SK 45% 2.705 2.686 0.019

Ukranian UK 40% 2.359 2.338 0.021
Estonian ET 49% 2.089 1.993 0.096
Finnish FI 55% 1.981 1.921 0.060
Arabic AR 86% 1.724 1.708 0.016
Hebrew HE 42% 2.293 2.282 0.011

Table 2: BPC results on the Universal Dependencies
(UD) test set. %Infl is the inflection rate in each lan-
guage. Languages are grouped by fusional, agglutina-
tive, and introflexive typologies, respectively.

In this setting, we only train on the morphology
features from UD and do not include this data as
additional language modeling supervision. These
models are trained on alternating batches from the
two disjoint datasets. LM is a language modeling
baseline with no multitask objective; MTL adds
morphology supervision.

We find that for all seven languages, the MTL
model outperforms our baseline trained only on
MWC. Our model also outperforms the strongest
model from Kawakami et al. (2017), HCLM-
cache, which is a hierarchical language model

and the UD treebanks for each of the seven languages.

with caching. Thus, adding morphology supervi-
sion to our character language models allows us to
achieve lower BPCs than a more complicated LM
architecture. Surprisingly, we see a larger gain on
languages with more LM data (EN, DE, ES, FR)
than those with less data (but are considered to be
more morphologically rich, e.g., CS, DE, and RU);
we explore this phenomenon more in Section 5.

Fully Supervised MTL We then train CLMs us-
ing UD for both langauge modeling and morphol-
ogy supervision on more languages (Table 2). We
again find that adding morphology supervision im-
proves BPC. In general, we see smaller improve-
ments between the LM and MTL models than un-
der distant supervision, even though the UD LM
data is fully annotated with morphology tags; this
is likely due to the smaller training sets in UD (on
average) than in MWC. On languages where the
size of the two datasets are comparable, such as
Russian and Czech, we see larger improvements
on the fully supervised models than we do in the
distant LM setting.

To investigate these results, we compare the rate
of inflected words on the development set (which
we use as a rough measure of morphological com-
plexity of the language) in a language against BPC
improvement by MTL model (Fig. 1(a)). The
rate at which each language is inflected is given
in Table 2. We unexpectedly find that how much
a language benefits from morphology supervision
is only weakly correlated with the inflection rate
of the language (r=0.15). This is surprising, be-
cause one would expect that additional morpho-
logical supervision would help languages that en-
code more morphological features in the forms
(i.e., with higher inflection rates).

We then examine the effect of training dataset



1609

(a)

% Train # Chars LM MTL ∆

CS

5% 0.31M 2.829 2.793 0.036
10% 0.61M 2.625 2.581 0.044
25% 1.5M 2.379 2.303 0.076
50% 3.1M 2.191 2.120 0.071

100% 6.1M 2.013 1.938 0.075
MWC-LG 10.2M 1.835 1.729 0.106

RU

5% 0.47M 2.492 2.486 0.006
10% 0.93M 2.305 2.283 0.022
25% 2.3M 2.066 2.033 0.033
50% 4.7M 1.935 1.898 0.037

100% 9.3M 1.745 1.645 0.100
MWC-LG 18.2M 1.554 1.377 0.177

(b)

% Train # Chars BPC

CS

0% (LM) 2.013
5% 0.34M 2.040
10% 0.69M 2.019
25% 1.7M 2.000
50% 3.4M 1.984

100% 6.9M 1.938

RU

0% (LM) 1.745
5% 0.27M 1.758
10% 0.53M 1.761
25% 1.3M 1.673
50% 2.7M 1.700

100% 5.3M 1.645

(c)

LM data Morph. data BPC

SK

None 2.806
SK 2.779
CS 2.752

CS+SK 2.777

CS+SK None 2.668CS+SK 2.446

UK

None 2.369
UK 2.348
RU 2.348

RU+UK 2.351

RU+UK None 2.495RU+UK 2.316

Table 3: (a) BPC on MWC development set with varied amounts of LM training data from MWC. The last line
is from training on MWC-large dataset, (b) BPC on MWC development set with varied amounts of supervised
morphology data from UD train set (compared against the baseline LM), and (c) Cross-lingual transfer on UD,
evaluated on low-resource language’s development set: from Czech (CS; 6.9M characters in training set) to Slovak
(SK; 0.4M) and from Russian (RU; 5.3M) to Ukrainian (UK; 0.5M)

size on BPC improvement between the LM and
the multitasked model (Fig. 1(b)). We find that
more training data (which adds both morpholog-
ical and LM supervision) is strongly correlated
with larger gains over the baseline LM (r=0.93).
Therefore, it seems that any potential correlation
between morphological complexity and the bene-
fit of multitasking morphology is overwhelmed by
differences in dataset size.

5 Analysis Experiments

Modeling Inflected Words We hypothesized
that morphology supervision would be most ben-
eficial to words whose form is dependent on their
morphology, e.g. inflected words. To investigate
this, we calculate BPC of our UD models on in-
flected and uninflected forms in the UD develop-
ment set. We determine whether or not a word is
inflected by comparing it to the (annotated) lemma
given in the UD treebank. We find that on 16 of the
24 languages for which we train models on UD,
the MTL model improves more on inflected words
than uninflected words, and that the average delta
between LM and MTL models is 31% greater for
inflected words than uninflected. A comparison
of the improvements in six of these languages are
given in Fig. 1(c). We show results for the ag-
glutinative (ET, FI) and introflexive (AR, HE) lan-
guages and pick two fusional languages (EN, RU)
against which to compare.

Effect of Training Data One caveat to the ob-
served gain from morphology is that the CLMs
may capture this information if given more lan-
guage modeling data, which is much cheaper to

obtain than morphology annotations. To test this,
we train CLMs on Czech (CS) and Russian (RU)
on varied amounts of language modeling data
from the MWC corpus (Table 2(a)). We find that
for both RU and CS, increasing the amount of
LM data does not eliminate the gains we see from
multitasking with morphology. Instead, we see
that increasing LM data leads to larger improve-
ments in the MTL model. Even when we train the
CLMs on twice as much LM data (obtained from a
larger version of the MWC dataset, MWC-large),
we continue to see large improvements via multi-
tasking.

We then investigate how the amount of an-
notated morphology data affects performance on
these models (Table 2(b)). We find that, as ex-
pected, increasing the amount of morphological
data the language model is trained on improves
BPC performance. For both Czech and Russian,
the MTL models mulitasked with 25% or more of
the annotated data still outperform the LM base-
line, but MTL models trained on smaller subsets
of the morphology data performed worse than the
baseline. This is in line with our findings in Sec-
tion 4 that the amount of annotated morphology
data is closely tied with how much multitasking
helps.

Cross-lingual Transfer In the previous section,
we showed that the amount of training data (both
for LM and for morphology) the CLM sees is cru-
cial for better performance. Motivated by this, we
extend our models to the cross-lingual setting, in
which we use data from high-resource languages
to improve performance on closely-related, low-
resource ones. We train models on the (high,



1610

low) language pairs of (Russian, Ukrainian) and
(Czech, Slovak) and transfer both LM and mor-
phological supervision (Table 2(c)). We find the
best performance for each low-resource language
is achieved by using both the high-resource LM
data and morphology annotations to augment the
low-resource data. In Slovak (SK), this gives us
a 0.333 BPC improvement over the MTL model
on SK data alone, and in Ukranian (UK), we see a
improvement of 0.032 in this setting over the MTL
trained only on UK.

6 Related Work

Prior work has investigated to what degree neu-
ral models capture morphology when trained on
language modeling (Vania and Lopez, 2017) and
on machine translation (Belinkov et al., 2017;
Bisazza and Tump, 2018). Other work has looked
into how the architecture of language models can
be improved for morphologically-rich languages
(Gerz et al., 2018a). In particular, both Kawakami
et al. (2017) and Mielke and Eisner (2019) pro-
posed hybrid open-vocabulary LM architectures to
deal with rare words in morphologically-rich lan-
guages on MWC.2

Another line of work has investigated the use of
morphology to improve models trained on other
NLP tasks. These approaches add morphology as
an input to the model, either with gold labels on
the LM dataset (Vania and Lopez, 2017) or by la-
beling the data with a pretrained morphological
tagger (Botha and Blunsom, 2014; Matthews et al.,
2018). This approach to adding morphology as in-
put features to models has also been applied to de-
pendency parsers (Vania et al., 2018) and semantic
role labeling models (Şahin and Steedman, 2018).
Unlike these approaches, however, our technique
does not require the morphology data to overlap
with the training data of the primary task or de-
pend on automatically labeled features. More sim-
ilarly to our work, Dalvi et al. (2017) find that
incorporating morphological supervision into the
decoder of an NMT system via multitasking im-
proves performance by up to 0.58 BLEU points
over the baseline for English-German, English-
Czech, and German-English.

2Results comparing against Mielke and Eisner (2019) are
given in the supplement, due to a different character vocabu-
lary from Kawakami et al. (2017).

7 Conclusion

We incorporate morphological supervision into
character language models via multitask learning
and find that this addition improves BPC on 24
languages. Furthermore, we observe this gain
even when the morphological annotations and lan-
guage modeling data are disjoint, providing a sim-
ple way to improve language modelsing without
requiring additional annotation efforts. Our anal-
ysis finds that the addition of morphology bene-
fits inflected forms more than uninflected forms
and that training our CLMs on additional language
modeling data does not diminish these gains in
BPC. Finally, we show that these gains can also
be projected across closely related languages by
sharing morphological annotations. We conclude
that this multitasking approach helps the CLMs
capture morphology better than the LM objective
alone.

Acknowledgements

This material is based upon work supported by the
National Science Foundation Graduate Research
Fellowship Program under Grant No. DGE-
1762114. We thank Victor Zhong, Sewon Min,
and the anonymous reviewers for their helpful
comments.

References
Yonatan Belinkov, Nadir Durrani, Fahim Dalvi, Hassan

Sajjad, and James Glass. 2017. What do neural ma-
chine translation models learn about morphology?
In Proceedings of the 55th Annual Meeting of the As-
sociation for Computational Linguistics, pages 861–
872.

Arianna Bisazza and Clara Tump. 2018. The lazy en-
coder: A fine-grained analysis of the role of mor-
phology in neural machine translation. In Proceed-
ings of the 2018 Conference on Empirical Methods
in Natural Language Processing, pages 2871–2876.

Jan Botha and Phil Blunsom. 2014. Compositional
morphology for word representations and language
modelling. In International Conference on Machine
Learning, pages 1899–1907.

Ronan Collobert and Jason Weston. 2008. A unified
architecture for natural language processing: Deep
neural networks with multitask learning. In Pro-
ceedings of the 25th International Conference on
Machine Learning, pages 160–167. ACM.

Ryan Cotterell, Sebastian J. Mielke, Jason Eisner, and
Brian Roark. 2018. Are all languages equally hard

https://arxiv.org/pdf/1704.03471.pdf
https://arxiv.org/pdf/1704.03471.pdf
http://www.aclweb.org/anthology/D18-1313
http://www.aclweb.org/anthology/D18-1313
http://www.aclweb.org/anthology/D18-1313
http://proceedings.mlr.press/v32/botha14.pdf
http://proceedings.mlr.press/v32/botha14.pdf
http://proceedings.mlr.press/v32/botha14.pdf
https://ronan.collobert.com/pub/matos/2008_nlp_icml.pdf
https://ronan.collobert.com/pub/matos/2008_nlp_icml.pdf
https://ronan.collobert.com/pub/matos/2008_nlp_icml.pdf
http://aclweb.org/anthology/N18-2085


1611

to language-model? In Proceedings of the 2018
Conference of the North American Chapter of the
Association for Computational Linguistics: Human
Language Technologies, Volume 2 (Short Papers),
pages 536–541. Association for Computational Lin-
guistics.

Fahim Dalvi, Nadir Durrani, Hassan Sajjad, Yonatan
Belinkov, and Stephan Vogel. 2017. Understanding
and improving morphological learning in the neu-
ral machine translation decoder. In Proceedings of
the Eighth International Joint Conference on Natu-
ral Language Processing (Volume 1: Long Papers),
volume 1, pages 142–151.

Daniela Gerz, Ivan Vulić, Edoardo Ponti, Jason Narad-
owsky, Roi Reichart, and Anna Korhonen. 2018a.
Language modeling for morphologically rich lan-
guages: Character-aware modeling for word-level
prediction. Transactions of the Association of Com-
putational Linguistics, 6:451–465.

Daniela Gerz, Ivan Vulić, Edoardo Maria Ponti, Roi
Reichart, and Anna Korhonen. 2018b. On the rela-
tion between linguistic typology and (limitations of)
multilingual language modeling. In Proceedings of
the 2018 Conference on Empirical Methods in Nat-
ural Language Processing, pages 316–327.

Sepp Hochreiter and Jürgen Schmidhuber. 1997.
Long short-term memory. Neural computation,
9(8):1735–1780.

Kazuya Kawakami, Chris Dyer, and Phil Blunsom.
2017. Learning to create and reuse words in open-
vocabulary neural language modeling. In Proceed-
ings of the 55th Annual Meeting of the Association
for Computational Linguistics.

Diederik P. Kingma and Jimmy Lei Ba. 2015. Adam:
A method for stochastic optimization. In Proceed-
ings of 3rd International Conference of Learning
Representations.

Austin Matthews, Graham Neubig, and Chris Dyer.
2018. Using morphological knowledge in open-
vocabulary neural language models. In Proceed-
ings of the 2018 Conference of the North American
Chapter of the Association for Computational Lin-
guistics: Human Language Technologies, Volume 1
(Long Papers), volume 1, pages 1435–1445.

Sebastian J Mielke and Jason Eisner. 2019. Spell once,
summon anywhere: A two-level open-vocabulary
language model. In Proceedings of the Thirty-Third
AAAI Conference on Artifical Intelligence.

Joakim Nivre et al. 2018. Universal dependencies 2.3.
LINDAT/CLARIN digital library at the Institute of
Formal and Applied Linguistics (ÚFAL), Faculty of
Mathematics and Physics, Charles University.

Gözde Gül Şahin and Mark Steedman. 2018.
Character-level models versus morphology in
semantic role labeling. In Proceedings of the 56th
Annual Meeting of the Association for Computa-
tional Linguistics.

Ilya Sutskever, James Martens, and Geoffrey E Hin-
ton. 2011. Generating text with recurrent neural
networks. In Proceedings of the 28th International
Conference on Machine Learning (ICML-11), pages
1017–1024.

Clara Vania, Andreas Grivas, and Adam Lopez. 2018.
What do character-level models learn about mor-
phology? the case of dependency parsing. In Pro-
ceedings of the 2018 Conference on Empirical Meth-
ods in Natural Language Processing, pages 2573–
2583. Association for Computational Linguistics.

Clara Vania and Adam Lopez. 2017. From character to
words to in between: Do we capture morphology?
In Proceedings of the 55th Annual Meeting of the
Association for Computational Linguistics (Volume
1: Long Papers), pages 2016–2027. Association for
Computational Linguistics.

A Appendix: Languages and Datasets

The languages we use from Universal Dependen-
cies and details about their treebanks are given in
Table 4. Most of the treebanks we used in this pa-
per are manually annotated (and then possibly au-
tomatically converted to their current format), ex-
cept for German, English, and French, which are
automatically annotated. For models trained in the

Num Chars
Lang ISO Treebank Train Dev Test

Bulgarian BG BTB 0.7M 90K 88K
Catalan CA AnCora 2.2M 0.3M 0.3M
Czech CS PDT 6.9M 0.9M 1.0M
Danish DA DDT 0.4M 56K 54K
German DE GSD 1.6M 75K 0.1M
English EN EWT 1.0M 0.1M 0.1M
Spanish ES GSD 2.1M 0.2M 65K

Farsi FA Seraji 0.6M 77K 78K
French FR GSD 1.9M 0.2M 52K
Hindi HI HDTB 1.3M 0.2M 0.2M

Croatian HR SET 0.9M 0.1M 0.1M
Italian IT ISDT 1.6M 67K 59K
Latvian LV LVTB 0.7M 0.1M 0.1M
Dutch NL Alpino 1.1M 65K 67K
Polish PL LFG 0.6M 74K 74K

Portuguese PT Bosque 1.1M 58K 55K
Romanian RO RRT 1.1M 98K 93K
Russian RU SynTagRus 5.3M 0.7M 0.7M
Slovak SK SNK 0.4M 76K 80K

Ukranian UK IU 0.5M 71K 98K
Estonian ET EDT 2.2M 0.2M 0.3M
Finnish FI TDT 1.2M 0.1M 0.2M
Arabic AR PADT 1.3M 0.2M 0.2M
Hebrew HE HTB 0.8M 63K 68K

Table 4: Dataset statistics for Universal Dependencies
(UD; v.2.3). Languages are grouped by typology, from
top to bottom: fusional, agglutinative, and introflexive

http://aclweb.org/anthology/N18-2085
http://www.aclweb.org/anthology/I17-1015
http://www.aclweb.org/anthology/I17-1015
http://www.aclweb.org/anthology/I17-1015
https://www.mitpressjournals.org/doi/pdf/10.1162/tacl_a_00032
https://www.mitpressjournals.org/doi/pdf/10.1162/tacl_a_00032
https://www.mitpressjournals.org/doi/pdf/10.1162/tacl_a_00032
http://aclweb.org/anthology/D18-1029
http://aclweb.org/anthology/D18-1029
http://aclweb.org/anthology/D18-1029
http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.676.4320&rep=rep1&type=pdf
https://arxiv.org/abs/1704.06986
https://arxiv.org/abs/1704.06986
https://arxiv.org/pdf/1412.6980
https://arxiv.org/pdf/1412.6980
http://aclweb.org/anthology/N18-1130.pdf
http://aclweb.org/anthology/N18-1130.pdf
https://arxiv.org/pdf/1804.08205.pdf
https://arxiv.org/pdf/1804.08205.pdf
https://arxiv.org/pdf/1804.08205.pdf
http://hdl.handle.net/11234/1-2895
https://arxiv.org/pdf/1805.11937.pdf
https://arxiv.org/pdf/1805.11937.pdf
https://www.cs.utoronto.ca/~ilya/pubs/2011/LANG-RNN.pdf
https://www.cs.utoronto.ca/~ilya/pubs/2011/LANG-RNN.pdf
http://aclweb.org/anthology/D18-1278
http://aclweb.org/anthology/D18-1278
http://aclweb.org/anthology/P17-1184
http://aclweb.org/anthology/P17-1184


1612

Num Chars
Lang Vocab Train Dev Test

CS 238 6.1M 0.4M 0.5M
DE 298 13.6M 1.2M 1.3M
EN 307 15.6M 1.5M 1.3M
ES 307 11.0M 1.0M 1.3M
FI 246 6.4M 0.7M 0.6M
FR 272 12.4M 1.3M 1.6M
RU 273 9.3M 1.0M 0.9M

Table 5: Dataset statistics for Multilingual Wikipedia
Corpus (MWC). Vocabulary size is based on the char-
acter vocabulary given in (Kawakami et al., 2017).

fully-supervised MTL setting where UD is used
for both LM and morphology supervision, we cal-
culate the character vocabulary for each language
by including any character that occurs more than
5 times in the training set of the language’s UD
treebank.

Dataset statistics for the Multilingual Wikipedia
Corpus (MWC) are given in Table 5. When
analyzing the effect of LM training dataset size
on Czech and Russian, we also train models on
the training portion of a larger version of the
MWC corpus, MWC-large, which contains ap-
proximately twice as much training data as the
standard MWC dataset. Specifically, MWC-large
contains 10.2M training characters for Czech and
18.2M for Russian. There is no prior work that we
know of that reports BPC on this larger dataset.

For models trained on the disjoint supervi-
sion setting, we use the character vocabulary pro-
vided for each language in the MWC dataset (see
Kawakami et al. (2017) for preprocessing details).
In cases where we use two sources of supervision
for the model – LM supervision from MWC and
morphology supervision from UD – we use the
MWC character vocabulary for all inputs, so that
BPC results across models are comparable. This
only affects a small number of the character types
(11 or fewer for each language) in the UD training
data.

The character vocabulary provided in the MWC
dataset and used for the distant supervision setting
differs from the vocabulary calculated by includ-
ing the characters that occur more than 25 times
in the MWC training set.3 Because of this, our
distant supervision setting on MWC is not compa-
rable with Mielke and Eisner (2019), which uses
the second vocabulary setting. Therefore, we re-

3On English, this preprocessing difference decreases the
character vocabulary size from 307 in the provided vocabu-
lary to 167.

train our character LM baselines and multitasked
models in this vocabulary setting (Table 6). We
find that our LM and MTL models generally ob-
tain slightly better performance on this setting, and
we continue to see improvement from multitask-
ing morphology over the character LM baseline.

B Appendix: Model Parameters and
Training

To train all models presented in this paper, we
use the Adam optimizer (Kingma and Ba, 2015)
with an initial learning rate of 0.002 and clip
the norm of the gradient to 5. We also apply
a dropout of 0.5 to each layer. We train each
model on sequences of 150 characters and use
early stopping with a patience of 10. We only
use the language modeling performance (BPC)
on the development set for early stopping and
hyperparameter selection (and do not consider
the morphology losses). For the UD language
models, we train models with two hidden layers
for 150 epochs with a batch size of 10. The
models trained on MWC contain three hidden
layers and are trained for 250 epochs with a batch
size of 32. All of our models are implemented in
Pytorch.4

For each language, we individually tuned the
level at which we multitask the morphology
objectives and the weighting ratio between the
primary and auxiliary losses δ. We consider
multitasking the morphology objective at either
the first or second hidden layer (as all of our
models have two hidden layers), and tune for
each language δ = {0.01, 0.1, 0.5, 1, 1.5, 2}. The
parameters chosen for each language and setting
(fully supervised or distant MTL) are given in
Table 7.

C Appendix: Additional Results

We provide the full set of results for our experi-
ments in Section 5 on how well our CLMs model
inflected forms versus uninflected forms across all
24 UD languages (Table 8).

4https://pytorch.org/



1613

CS DE EN ES FI FR RU
dev test dev test dev test dev test dev test dev test dev test

Mielke BPE 1.88 1.856 1.45 1.414 1.45 1.386 1.42 1.362 1.70 1.652 1.36 1.317 1.63 1.598
Mielke Full 1.95 1.928 1.51 1.465 1.45 1.387 1.42 1.363 1.79 1.751 1.36 1.319 1.74 1.709

LM 2.01 1.975 1.52 1.493 1.45 1.395 1.55 1.482 1.74 1.705 1.60 1.565 1.72 1.692
MTL 1.81 1.771 1.43 1.414 1.32 1.262 1.33 1.268 1.69 1.658 1.15 1.104 1.62 1.596

Table 6: Results on Multilingual Wikipedia Corpus (MWC) in bits per character (BPC), trained on the vocabulary
from Mielke and Eisner (2019).

Distant MTL Fully-Supervised
Lang MTL layer δ MTL layer δ
BG - - 2 1.0
CA - - 1 2.0
CS 2 1.5 2 1.5
DA - - 2 0.5
DE 2 2.0 1 1.0
EN 2 1.0 2 1.0
ES 2 1.0 1 1.5
FA - - 1 0.01
FR 3 1.0 1 2.0
HI - - 1 2.0
HR - - 2 1.0
IT - - 1 2.0
LV - - 2 1.0
NL - - 2 1.5
PL - - 2 1.0
PT - - 2 1.5
RO - - 2 0.5
RU 3 1.0 2 1.5
SK - - 2 2.0
UK - - 2 1.0
ET - - 2 2.0
FI 1 0.5 2 1.0
AR - - 2 0.5
HE - - 2 0.5

Table 7: Language specific parameters for multitasked
models trained in the distant MTL setting and the fully-
supervised MTL setting

Lang %Infl Word Type LM MTL ∆

BG 39% inflected 2.092 2.085 0.008uninflected 2.333 2.330 0.002

CA 31% inflected 1.849 1.783 0.066uninflected 2.007 1.943 0.064

CS 43% inflected 2.205 1.940 0.265uninflected 2.539 2.322 0.217

DA 30% inflected 2.411 2.387 0.024uninflected 2.559 2.552 0.007

DE 33% inflected 1.916 1.868 0.048uninflected 2.323 2.263 0.060

EN 15% inflected 2.235 2.216 0.019uninflected 2.579 2.571 0.008

ES 28% inflected 1.742 1.700 0.042uninflected 2.053 2.010 0.043

FA 27% inflected 2.874 2.859 0.016uninflected 2.499 2.492 0.007

FR 32% inflected 1.856 1.809 0.047uninflected 2.228 2.174 0.054

HI 28% inflected 1.996 1.941 0.053uninflected 2.270 2.228 0.042

HR 49% inflected 2.055 2.021 0.035uninflected 2.507 2.487 0.021

IT 36% inflected 1.897 1.852 0.045uninflected 2.056 2.010 0.046

LV 47% inflected 2.387 2.361 0.027uninflected 2.782 2.758 0.024

NL 19% inflected 2.161 2.493 0.030uninflected 2.131 2.468 0.025

PL 42% inflected 2.522 2.462 0.060uninflected 2.633 2.578 0.054

PT 31% inflected 2.071 2.065 0.007uninflected 2.214 2.205 0.009

RO 42% inflected 2.037 1.987 0.050uninflected 2.373 2.316 0.057

RU 42% inflected 2.130 1.920 0.210uninflected 2.583 2.424 0.159

SK 45% inflected 2.976 2.969 0.007uninflected 3.545 3.535 0.010

UK 40% inflected 2.580 2.553 0.027uninflected 2.553 2.956 0.009

ET 49% inflected 2.397 2.692 0.112uninflected 2.285 2.629 0.063

FI 55% inflected 2.152 2.084 0.068uninflected 2.402 2.339 0.063

AR 86% inflected 2.036 2.013 0.023uninflected 3.856 3.828 0.027

HE 42% inflected 3.426 3.360 0.066uninflected 2.168 2.211 -0.043

Table 8: BPC performance on the UD development set
on inflected versus uninflected words. Bold delta val-
ues for each language indicate whether than language
improves more on inflected or uninflected words by
when multitasking morphology is added.


