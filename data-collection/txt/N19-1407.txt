




































Convolutional Self-Attention Networks


Proceedings of NAACL-HLT 2019, pages 4040–4045
Minneapolis, Minnesota, June 2 - June 7, 2019. c©2019 Association for Computational Linguistics

4040

Convolutional Self-Attention Networks

Baosong Yang† Longyue Wang‡ Derek F. Wong† Lidia S. Chao† Zhaopeng Tu‡∗
†NLP2CT Lab, Department of Computer and Information Science, University of Macau

‡Tencent AI Lab
†nlp2ct.baosong@gmail.com, {derekfw,lidiasc}@umac.mo

‡{vinnylywang,zptu}@tencent.com

Abstract

Self-attention networks (SANs) have drawn
increasing interest due to their high paral-
lelization in computation and flexibility in
modeling dependencies. SANs can be fur-
ther enhanced with multi-head attention by
allowing the model to attend to information
from different representation subspaces. In
this work, we propose novel convolutional
self-attention networks, which offer SANs the
abilities to 1) strengthen dependencies among
neighboring elements, and 2) model the inter-
action between features extracted by multiple
attention heads. Experimental results of ma-
chine translation on different language pairs
and model settings show that our approach
outperforms both the strong Transformer base-
line and other existing models on enhancing
the locality of SANs. Comparing with prior
studies, the proposed model is parameter free
in terms of introducing no more parameters.

1 Introduction

Self-attention networks (SANs) (Parikh et al.,
2016; Lin et al., 2017) have shown promising
empirical results in various natural language pro-
cessing (NLP) tasks, such as machine transla-
tion (Vaswani et al., 2017), natural language in-
ference (Shen et al., 2018a), and acoustic model-
ing (Sperber et al., 2018). One appealing strength
of SANs lies in their ability to capture dependen-
cies regardless of distance by explicitly attending
to all the elements. In addition, the performance
of SANs can be improved by multi-head atten-
tion (Vaswani et al., 2017), which projects the in-
put sequence into multiple subspaces and applies
attention to the representation in each subspace.

Despite their success, SANs have two major
limitations. First, the model fully take into ac-

∗Zhaopeng Tu is the corresponding author of the paper.
This work was conducted when Baosong Yang was interning
at Tencent AI Lab.

count all the elements, which disperses the atten-
tion distribution and thus overlooks the relation of
neighboring elements and phrasal patterns (Yang
et al., 2018; Wu et al., 2018; Guo et al., 2019).
Second, multi-head attention extracts distinct lin-
guistic properties from each subspace in a parallel
fashion (Raganato and Tiedemann, 2018), which
fails to exploit useful interactions across differ-
ent heads. Recent work shows that better features
can be learned if different sets of representations
are present at feature learning time (Ngiam et al.,
2011; Lin et al., 2014).

To this end, we propose novel convolutional
self-attention networks (CSANs), which model lo-
cality for self-attention model and interactions be-
tween features learned by different attention heads
in an unified framework. Specifically, in order to
pay more attention to a local part of the input se-
quence, we restrict the attention scope to a window
of neighboring elements. The localness is there-
fore enhanced via a parameter-free 1-dimensional
convolution. Moreover, we extend the convolu-
tion to a 2-dimensional area with the axis of at-
tention head. Thus, the proposed model allows
each head to interact local features with its adja-
cent subspaces at attention time. We expect that
the interaction across different subspaces can fur-
ther improve the performance of SANs.

We evaluate the effectiveness of the proposed
model on three widely-used translation tasks:
WMT14 English-to-German, WMT17 Chinese-
to-English, and WAT17 Japanese-to-English. Ex-
perimental results demonstrate that our approach
consistently improves performance over the strong
TRANSFORMER model (Vaswani et al., 2017)
across language pairs. Comparing with previous
work on modeling locality for SANs (e.g. Shaw
et al., 2018; Yang et al., 2018; Sperber et al.,
2018), our model boosts performance on both
translation quality and training efficiency.



4041

Bush held a talk with Sharon

Bush held a talk with Sharon

Bush held a talk with SharonBush held a talk with Sharon

Bush held a talk with Sharon

(a) Vanilla SANs

Bush held a talk with Sharon

Bush held a talk with Sharon

Bush held a talk with SharonBush held a talk with Sharon

Bush held a talk with Sharon

(b) 1D-Convolutional SANs

Bush held a talk with Sharon

Bush held a talk with Sharon

Bush held a talk with SharonBush held a talk with Sharon

Bush held a talk with Sharon

(c) 2D-Convolutional SANs

Figure 1: Illustration of (a) vanilla SANs; (b) 1-dimensional convolution with the window size being 3; and (c)
2-dimensional convolution with the area being 3 × 3. Different colors and patterns represent different subspaces
modeled by multi-head attention, and transparent colors denote masked tokens that are invisible to SANs.

2 Multi-Head Self-Attention Networks

SANs produce representations by applying atten-
tion to each pair of tokens from the input se-
quence, regardless of their distance. Vaswani et al.
(2017) found it is beneficial to capture different
contextual features with multiple individual atten-
tion functions. Given an input sequence X =
{x1, . . . ,xI} ∈ RI×d, the model first transforms
it into queries Q, keys K, and values V:

Q,K,V = XWQ,XWK ,XWV ∈ RI×d (1)

where {WQ,WK ,WV } ∈ Rd×d are trainable
parameters and d indicates the hidden size. The
three types of representations are split into H dif-
ferent subspaces, e.g., [Q1, . . . ,QH ] = Q with
Qh ∈ RI×

d
H . In each subspace h, the element

ohi in the output sequence O
h = {oh1 , . . . ,ohI } is

calculated by

ohi = ATT(q
h
i ,K

h)Vh ∈ R
d
H (2)

where ATT(·) is an attention model (Bahdanau
et al., 2015; Vaswani et al., 2017) that retrieves
the keys Kh with the query qhi . The final output
representation O is the concatenation of outputs
generated by multiple attention models:

O = [O1, . . . ,OH ] ∈ RI×d (3)

3 Approach

As shown in Figure 1(a), the vanilla SANs use
the query qhi to compute a categorical distribution
over all elements from Kh (Equation 2). It may in-
herit the attention to neighboring information (Yu
et al., 2018; Yang et al., 2018; Guo et al., 2019). In
this work, we propose to model locality for SANs
by restricting the model to attend to a local re-
gion via convolution operations (1D-CSANs, Fig-
ure 1(b)). Accordingly, it provides distance-aware

information (e.g. phrasal patterns), which is com-
plementary to the distance-agnostic dependencies
modeled by the standard SANs (Section 3.1).

Moreover, the calculation of output oh are re-
stricted to the a single individual subspace, over-
looking the richness of contexts and the dependen-
cies among groups of features, which have proven
beneficial to the feature learning (Ngiam et al.,
2011; Wu and He, 2018). We thus propose to
convolute the items in adjacent heads (2D-CSANs,
Figure 1(c)). The proposed model is expected to
improve performance through interacting linguis-
tic properties across heads (Section 3.2).

3.1 Locality Modeling via 1D Convolution
For each query qhi , we restrict its attention re-
gion (e.g., Kh = {kh1 , . . . ,khi , . . . ,khI }) to a local
scope with a fixed size M + 1 (M ≤ I) centered
at the position i:

K̂h = {kh
i−M

2

, . . . ,khi , . . . ,k
h
i+M

2

} (4)

V̂h = {vh
i−M

2

, . . . ,vhi , . . . ,v
h
i+M

2

} (5)

Accordingly, the calculation of corresponding out-
put in Equation (2) is modified as:

ohi = ATT(q
h
i , K̂

h)V̂h (6)

As seen, SANs are only allowed to attend to the
neighboring tokens (e.g., K̂h, V̂h), instead of all
the tokens in the sequence (e.g., Kh, Vh).

The SAN-based models are generally imple-
mented as multiple layers, in which higher lay-
ers tend to learn semantic information while lower
layers capture surface and lexical information (Pe-
ters et al., 2018; Raganato and Tiedemann, 2018).
Therefore, we merely apply locality modeling to
the lower layers, which same to the configuration
in Yu et al. (2018) and Yang et al. (2018). In



4042

this way, the representations are learned in a hi-
erarchical fashion (Yang et al., 2017). That is, the
distance-aware and local information extracted by
the lower SAN layers, is expected to complement
distance-agnostic and global information captured
by the higher SAN layers.

3.2 Attention Interaction via 2D Convolution
Mutli-head mechanism allows different heads to
capture distinct linguistic properties (Raganato
and Tiedemann, 2018; Li et al., 2018), especially
in diverse local contexts (Yang et al., 2018). We
hypothesis that exploiting local properties across
heads can further improve the performance of
SANs. To this end, we expand the 1-dimensional
window to a 2-dimensional area with the new di-
mension being the index of attention head. Sup-
pose that the area size is (N + 1) × (M + 1)
(N ≤ H), the keys and values in the area are:

K̃h =
⋃

[K̂h−
N
2 , . . . , K̂h, . . . , K̂h+

N
2 ] (7)

Ṽh =
⋃

[V̂h−
N
2 , . . . , V̂h, . . . , V̂h+

N
2 ] (8)

where K̂h, V̂h are elements in the h-th subspace,
which are calculated by Equations 4 and 5 respec-
tively. The union operation

⋃
means combining

the keys and values in different subspaces. The
corresponding output is calculated as:

ohi = ATT(q
h
i , K̃

h)Ṽh (9)

The 2D convolution allows SANs to build rel-
evance between elements across adjacent heads,
thus flexibly extract local features from different
subspaces rather than merely from an unique head.

The vanilla SAN models linearly aggregate fea-
tures from different heads, and this procedure lim-
its the extent of abstraction (Fukui et al., 2016; Li
et al., 2019). Multiple sets of representations pre-
sented at feature learning time can further improve
the expressivity of the learned features (Ngiam
et al., 2011; Wu and He, 2018).

4 Related Work

Self-Attention Networks Recent studies have
shown that SANs can be further improved by cap-
turing complementary information. For exam-
ple, Hao et al. (2019) complemented SANs with
recurrence modeling, while Yang et al. (2019)
modeled contextual information for SANs.

Concerning modeling locality for SANs, Yu
et al. (2018) injected several CNN layers (Kim,

2014) to fuse local information, the output of
which is fed to the subsequent SAN layer. Several
researches proposed to revise the attention distri-
bution with a parametric localness bias, and suc-
ceed on machine translation (Yang et al., 2018)
and natural language inference (Guo et al., 2019).
While both models introduce additional parame-
ters, our approach is a more lightweight solution
without introducing any new parameters. Closely
related to this work, Shen et al. (2018a) applied a
positional mask to encode temporal order, which
only allows SANs to attend to the previous or fol-
lowing tokens in the sequence. In contrast, we
employ a positional mask (i.e. the tokens outside
the local window is masked as 0) to encode the
distance-aware local information.

In the context of distance-aware SANs, Shaw
et al. (2018) introduced relative position encod-
ing to consider the relative distances between se-
quence elements. While they modeled locality
from position embedding, we improve locality
modeling from revising attention scope. To make
a fair comparison, we re-implemented the above
approaches under a same framework. Empirical
results on machine translation tasks show the su-
periority of our approach in both translation qual-
ity and training efficiency.

Multi-Head Attention Multi-head attention
mechanism (Vaswani et al., 2017) employs
different attention heads to capture distinct fea-
tures (Raganato and Tiedemann, 2018). Along
this direction, Shen et al. (2018a) explicitly
used multiple attention heads to model different
dependencies of the same word pair, and Strubell
et al. (2018) employed different attention heads
to capture different linguistic features. Li et al.
(2018) introduced disagreement regularizations to
encourage the diversity among attention heads.
Inspired by recent successes on fusing informa-
tion across layers (Dou et al., 2018, 2019), Li
et al. (2019) proposed to aggregate information
captured by different attention heads. Based on
these findings, we model interactions among
attention heads to exploit the richness of local
properties distributed in different heads.

5 Experiments

We conducted experiments with the Transformer
model (Vaswani et al., 2017) on English⇒German
(En⇒De), Chinese⇒English (Zh⇒En) and
Japanese⇒English (Ja⇒En) translation tasks.



4043

For the En⇒De and Zh⇒En tasks, the mod-
els were trained on widely-used WMT14 and
WMT17 corpora, consisting of around 4.5 and
20.62 million sentence pairs, respectively. Con-
cerning Ja⇒En, we used the first two sections
of WAT17 corpus as the training data, which
consists of 2M sentence pairs. To reduce the
vocabulary size, all the data were tokenized and
segmented into subword symbols using byte-pair
encoding (Sennrich et al., 2016) with 32K merge
operations. Following Shaw et al. (2018), we
incorporated the proposed model into the encoder,
which is a stack of 6 SAN layers. Prior studies
revealed that modeling locality in lower layers can
achieve better performance (Shen et al., 2018b;
Yu et al., 2018; Yang et al., 2018), we applied our
approach to the lowest three layers of the encoder.
About configurations of NMT models, we used
the Base and Big settings same as Vaswani et al.
(2017), and all models were trained on 8 NVIDIA
P40 GPUs with a batch of 4096 tokens.

5.1 Effects of Window/Area Size
We first investigated the effects of window
size (1D-CSANs) and area size (2D-CSANs) on
En⇒De validation set, as plotted in Figure 2. For
1D-CSANs, the local size with 11 is superior to
other settings. This is consistent with Luong et al.
(2015) who found that 10 is the best window size
in their local attention experiments. Then, we
fixed the number of neighboring tokens being 11
and varied the number of heads. As seen, by
considering the features across heads (i.e. > 1),
2D-CSANs further improve the translation qual-
ity. However, when the number of heads in at-
tention goes up, the translation quality inversely
drops. One possible reason is that the model still
has the flexibility of learning a different distribu-
tion for each head with few interactions, while a
large amount of interactions assumes more heads
make “similar contributions” (Wu and He, 2018).

5.2 Accuracy of Phrase Translation
One intuition of our approach is to capture useful
phrasal patterns via modeling locality. To evalu-
ate the accuracy of phrase translations, we calcu-
late the improvement of the proposed approaches
over multiple granularities of n-grams, as shown
in Figure 3. Both the two model variations con-
sistently outperform the baseline on larger granu-
larities, indicating that modeling locality can raise
the ability of self-attention model on capturing the

(a) 1D-CSANs

(b) 2D-CSANs

Figure 2: Effects of (a) window size on 1D-CSANs,
and (b) attended head numbers on 2D-CSANs. For 2D-
CSANs, the window size dimension is fixed to be 11.

Figure 3: Performance improvement on different n-
grams. “Gap of BLEU” denotes the improvement
achieved by the proposed models over the baseline.

phrasal information. Furthermore, the dependen-
cies among heads can be complementary to the lo-
calness modeling, which reveals the necessity of
the interaction of features in different subspaces.

5.3 Comparison to Related Work
We re-implemented and compared several exiting
works (Section 4) upon the same framework. Ta-
ble 1 lists the results on the En⇒De translation
task. As seen, all the models improve translation
quality, reconfirming the necessity of modeling lo-
cality and distance information. Besides, our mod-
els outperform all the existing works, indicating
the superiority of the proposed approaches. In par-
ticular, CSANs achieve better performance than



4044

Model Parameter Speed BLEU 4
TRANSFORMER-BASE (Vaswani et al., 2017) 88.0M 1.28 27.31 -
+ BI DIRECT (Shen et al., 2018a) +0.0M -0.00 27.58 +0.27
+ REL POS (Shaw et al., 2018) +0.1M -0.11 27.63 +0.32
+ NEIGHBOR (Sperber et al., 2018) +0.4M -0.06 27.60 +0.29
+ LOCAL HARD (Luong et al., 2015) +0.4M -0.06 27.73 +0.42
+ LOCAL SOFT (Yang et al., 2018) +0.8M -0.09 27.81 +0.50
+ BLOCK (Shen et al., 2018b) +6.0M -0.33 27.59 +0.28
+ CNNs (Yu et al., 2018) +42.6M -0.54 27.70 +0.39
+ 1D-CSANs +0.0M -0.00 27.86 +0.55
+ 2D-CSANs +0.0M -0.06 28.18 +0.87

Table 1: Comparing with the existing approaches on WMT14 En⇒De translation task. For a fair comparison,
we re-implemented the existing locality approaches under the same framework. “Parameter” denotes the number
of model parameters (M = million) and “Speed” denotes the training speed (steps/second). “4” column denotes
performance improvements over the Transformer baseline.

Model WMT14 En⇒De WMT17 Zh⇒En WAT17 Ja⇒En
Speed BLEU Speed BLEU Speed BLEU

TRANSFORMER-BASE 1.28 27.31 1.21 24.13 1.33 28.10
+ CSANs 1.22 28.18⇑ 1.16 24.80⇑ 1.28 28.50↑

TRANSFORMER-BIG 0.61 28.58 0.58 24.56 0.65 28.41
+ CSANs 0.50 28.74 0.48 25.01↑ 0.55 28.73↑

Table 2: Experimental results on WMT14 En⇒De, WMT17 Zh⇒En and WAT17 Ja⇒En test sets. “Speed”
denotes the training speed (steps/second). “↑ / ⇑” indicates statistically significant difference from the vanilla
self-attention counterpart (p < 0.05/0.01), tested by bootstrap resampling (Koehn, 2004).

CNNs, revealing that extracting local features with
dynamic weights is superior to assigning fixed pa-
rameters. Moreover, while most of the existing ap-
proaches (except for Shen et al. (2018a)) introduce
new parameters, our methods are parameter-free
and thus only marginally affect training efficiency.

5.4 Universality of The Proposed Model

To validate the universality of our approach on MT
tasks, we evaluated the proposed approach on dif-
ferent language pairs and model settings. Table 2
lists the results on En⇒De, Zh⇒En and Ja⇒En
translation tasks. As seen, our model consistently
improves translation quality across language pairs,
which demonstrates the effectiveness and univer-
sality of the proposed approach. It is encouraging
to see that CSANs with base setting yields compa-
rable performance with TRANSFORMER-BIG.

6 Conclusion

In this paper, we propose a parameter-free convo-
lutional self-attention model to enhance the fea-
ture extraction of neighboring elements across

multiple heads. Empirical results of machine
translation task on a variety of language pairs
demonstrate the effectiveness and universality of
the proposed methods. The extensive analyses
suggest that: 1) modeling locality is beneficial
to SANs; 2) interacting features across multiple
heads at attention time can further improve the
performance; and 3) to some extent, the dynamic
weights are superior to their fixed counterpart (i.e.
CSANs vs. CNNs) on local feature extraction.
Moreover, it is interesting to validate the proposed
model in other sequence modeling tasks.

Acknowledgments

The work was partly supported by the National
Natural Science Foundation of China (Grant No.
61672555), the Joint Project of Macao Sci-
ence and Technology Development Fund and Na-
tional Natural Science Foundation of China (Grant
No. 045/2017/AFJ) and the Multiyear Research
Grant from the University of Macau (Grant No.
MYRG2017-00087-FST). We thank the anony-
mous reviewers for their insightful comments.



4045

References
Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Ben-

gio. 2015. Neural Machine Translation by Jointly
Learning to Align and Translate. In ICLR.

Ziyi Dou, Zhaopeng Tu, Xing Wang, Shuming Shi, and
Tong Zhang. 2018. Exploiting Deep Representa-
tions for Neural Machine Translation. In EMNLP.

Ziyi Dou, Zhaopeng Tu, Xing Wang, Longyue Wang,
Shuming Shi, and Tong Zhang. 2019. Dynamic
Layer Aggregation for Neural Machine Translation.
In AAAI.

Akira Fukui, Dong Huk Park, Daylen Yang, Anna
Rohrbach, Trevor Darrell, and Marcus Rohrbach.
2016. Multimodal Compact Bilinear Pooling for Vi-
sual Question Answering and Visual Grounding. In
EMNLP.

Maosheng Guo, Yu Zhang, and Ting Liu. 2019. Gaus-
sian Transformer: A Lightweight Approach for Nat-
ural Language Inference. In AAAI.

Jie Hao, Xing Wang, Baosong Yang, Longyue Wang,
Jinfeng Zhang, and Zhaopeng Tu. 2019. Modeling
Recurrence for Transformer. In NAACL.

Yoon Kim. 2014. Convolutional Neural Networks for
Sentence Classification. In EMNLP.

Philipp Koehn. 2004. Statistical Significance Tests for
Machine Translation Evaluation. In EMNLP.

Jian Li, Zhaopeng Tu, Baosong Yang, Michael R. Lyu,
and Tong Zhang. 2018. Multi-Head Attention with
Disagreement Regularization. In EMNLP.

Jian Li, Baosong Yang, Zi-Yi Dou, Xing Wang,
Michael R. Lyu, and Zhaopeng Tu. 2019. Infor-
mation Aggregation for Multi-Head Attention with
Routing-by-Agreement. In NAACL.

Min Lin, Qiang Chen, and Shuicheng Yan. 2014. Net-
work in Network. In ICLR.

Zhouhan Lin, Minwei Feng, Cicero Nogueira dos San-
tos, Mo Yu, Bing Xiang, Bowen Zhou, and Yoshua
Bengio. 2017. A Structured Self-Aattentive Sen-
tence Embedding. In ICLR.

Thang Luong, Hieu Pham, and Christopher D. Man-
ning. 2015. Effective Approaches to Attention-
based Neural Machine Translation. In EMNLP.

Jiquan Ngiam, Aditya Khosla, Mingyu Kim, Juhan
Nam, Honglak Lee, and Andrew Y Ng. 2011. Mul-
timodal Deep Learning. In ICML.

Ankur Parikh, Oscar Täckström, Dipanjan Das, and
Jakob Uszkoreit. 2016. A Decomposable Attention
Model for Natural Language Inference. In EMNLP.

Matthew E Peters, Mark Neumann, Mohit Iyyer, Matt
Gardner, Christopher Clark, Kenton Lee, and Luke
Zettlemoyer. 2018. Deep Contextualized Word Rep-
resentations. In NAACL.

Alessandro Raganato and Jörg Tiedemann. 2018.
An Analysis of Encoder Representations in
Transformer-Based Machine Translation. In
EMNLP Workshop BlackboxNLP: Analyzing and
Interpreting Neural Networks for NLP.

Rico Sennrich, Barry Haddow, and Alexandra Birch.
2016. Neural Machine Translation of Rare Words
with Subword Units. In ACL.

Peter Shaw, Jakob Uszkoreit, and Ashish Vaswani.
2018. Self-Attention with Relative Position Repre-
sentations. In NAACL.

Tao Shen, Tianyi Zhou, Guodong Long, Jing Jiang,
Shirui Pan, and Chengqi Zhang. 2018a. DiSAN:
Directional Self-Attention Network for RNN/CNN-
Free Language Understanding. In AAAI.

Tao Shen, Tianyi Zhou, Guodong Long, Jing Jiang, and
Chengqi Zhang. 2018b. Bi-Directional Block Self-
Attention for Fast and Memory-Efficient Sequence
Modeling. In ICLR.

Matthias Sperber, Jan Niehues, Graham Neubig, Se-
bastian Stüker, and Alex Waibel. 2018. Self-
Attentional Acoustic Models. Interspeech.

Emma Strubell, Patrick Verga, Daniel Andor,
David Weiss, and Andrew McCallum. 2018.
Linguistically-Informed Self-Attention for Seman-
tic Role Labeling. In EMNLP.

Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob
Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz
Kaiser, and Illia Polosukhin. 2017. Attention is All
You Need. In NIPS.

Wei Wu, Houfeng Wang, Tianyu Liu, and Shuming
Ma. 2018. Phrase-level Self-Attention Networks for
Universal Sentence Encoding. In EMNLP.

Yuxin Wu and Kaiming He. 2018. Group Normaliza-
tion. arXiv:1803.08494.

Baosong Yang, Jian Li, Derek F. Wong, Lidia S. Chao,
Xing Wang, and Zhaopeng Tu. 2019. Context-
Aware Self-Attention Networks. In AAAI.

Baosong Yang, Zhaopeng Tu, Derek F. Wong, Fandong
Meng, Lidia S. Chao, and Tong Zhang. 2018. Mod-
eling Localness for Self-Attention Networks. In
EMNLP.

Baosong Yang, Derek F Wong, Tong Xiao, Lidia S
Chao, and Jingbo Zhu. 2017. Towards Bidirec-
tional Hierarchical Representations for Attention-
based Neural Machine Translation. In EMNLP.

Adams Wei Yu, David Dohan, Minh-Thang Luong, Rui
Zhao, Kai Chen, Mohammad Norouzi, and Quoc V
Le. 2018. QANet: Combining Local Convolution
with Global Self-attention for Reading Comprehen-
sion. In ICLR.


