




































Enhancement of Encoder and Attention Using Target Monolingual Corpora in Neural Machine Translation


Proceedings of the 2nd Workshop on Neural Machine Translation and Generation, pages 55–63
Melbourne, Australia, July 20, 2018. c©2018 Association for Computational Linguistics

55

Enhancement of Encoder and Attention
Using Target Monolingual Corpora in Neural Machine Translation

Kenji Imamura, Atsushi Fujita, and Eiichiro Sumita
National Institute of Information and Communications Technology

3-5 Hikaridai, Seika-cho, Soraku-gun, Kyoto 619-0289, Japan
{kenji.imamura,atsushi.fujita,eiichiro.sumita}@nict.go.jp

Abstract

A large-scale parallel corpus is required
to train encoder-decoder neural machine
translation. The method of using synthetic
parallel texts, in which target monolingual
corpora are automatically translated into
source sentences, is effective in improving
the decoder, but is unreliable for enhanc-
ing the encoder. In this paper, we propose
a method that enhances the encoder and at-
tention using target monolingual corpora
by generating multiple source sentences
via sampling. By using multiple source
sentences, diversity close to that of hu-
mans is achieved. Our experimental re-
sults show that the translation quality is
improved by increasing the number of syn-
thetic source sentences for each given tar-
get sentence, and quality close to that us-
ing a manually created parallel corpus was
achieved.

1 Introduction

In recent years, neural machine translation (NMT)
based on encoder-decoder models (Sutskever
et al., 2014; Bahdanau et al., 2014) has become
the mainstream approach for machine translation.
In this method, the encoder converts an input sen-
tence into numerical vectors called “states,” and
the decoder generates a translation on the basis of
these states. Although the encoder-decoder mod-
els can generate high-quality translations, they re-
quire large amounts of parallel texts for training.

On the other hand, monolingual corpora are
readily available in large quantities. Sennrich et al.
(2016a) proposed a method using synthetic paral-
lel texts, in which target monolingual corpora are
translated back into the source language (Figure
1). The advantage of this method is that the de-

coder is accurately trained because the target side
of the synthetic parallel texts consists of manually
created (correct) sentences. Consequently, this
method provides steady improvements. However,
this approach may not contribute to the improve-
ment of the encoder because the source side of
the synthetic parallel texts are automatically gen-
erated.

In this paper, we extend the method proposed by
Sennrich et al. (2016a) to enhance the encoder and
attention using target monolingual corpora. Our
proposed method generates multiple source sen-
tences by sampling when each target sentence is
translated back. By using multiple source sen-
tences, we aim to achieve the following.

• To average errors in individual synthetic sen-
tences and reduce their harmful effects.

• To ensure diversity as human translations.
This is a countermeasure against machine-
translated sentences that have less variety.

The remainder of this paper is organized as fol-
lows. Section 2 provides an overview of related
work that uses monolingual corpora in NMT. Sec-
tion 3 describes the proposed method, and Section
4 evaluates the proposed method through experi-
ments. In addition, Section 5 proposes the appli-
cation of our method as a self-training approach.
Finally, Section 6 concludes the paper.

2 Related Work

One approach of using target monolingual cor-
pora is to construct a recurrent neural network lan-
guage model and combine the model with the de-
coder (Gülçehere et al., 2015; Sriram et al., 2017).
Similarly, there is a method of training language
models, jointly with the translator, using multi-
task learning (Domhan and Hieber, 2017). These



56

Target Monolingual Corpus Base Parallel CorpusSynthetic Source Sentences Synthetic Parallel Corpus (Forward) TranslatorSource → TargetBack-TranslatorTarget → Source TranslationTest SentenceTrainingTrainingFilter
Figure 1: Flow of Our Approach

methods only enhance the decoder and require a
modification of the NMT.

Another approach of using monolingual corpora
of the target language is to learn models using syn-
thetic parallel sentences. The method of Sennrich
et al. (2016a) generates synthetic parallel corpora
through back-translation and learns models from
such corpora. Our proposed method is an exten-
sion of this method. Currey et al. (2017) gener-
ated synthetic parallel sentences by copying tar-
get sentences to the source. This method utilizes a
feature in which some words, such as named enti-
ties, are often identical across the source and target
languages and do not require translation. How-
ever, this method provides no benefits to language
pairs having different character sets, such as En-
glish and Japanese.

On the other hand, the basis of source mono-
lingual corpora, a pre-training method based on
an autoencoder has been proposed to enhance
the encoder (Zhang and Zong, 2016). How-
ever, the decoder is not enhanced by this method.
Cheng et al. (2016) trained two autoencoders us-
ing source and target monolingual corpora, while
translation models are trained using a parallel cor-
pus. This method enhances both the encoder and
decoder, but it requires two monolingual corpora,
respectively. Our proposed method enhances not
only the decoder but also the encoder and atten-
tion using target monolingual corpora.

3 Proposed Method

3.1 Synthetic Source Sentences

The back-translator used in this study is an NMT
trained on a small parallel corpus (hereinafter re-
ferred to as the base parallel corpus). Each sen-
tence in a target monolingual corpus is translated

LSTM LSTMWord Distribution ���� Word Distribution ��
States

Output Word ���ContextsOutput Word �����Sampling Sampling
�����

Generator &Attention Mech. Generator &Attention Mech.Contexts
Figure 2: Decoding Process of Back-Translator

by the back-translator to generate synthetic source
sentences. The back-translator does not output
only high-likelihood sentences but generates sen-
tences by random sampling.

Figure 2 illustrates the decoding process of the
back-translator. When the decoder generates a
sentence word-by-word, it also generates the pos-
terior probability distribution of an output word
Pr(yt) through the decoding process. We call this
a word distribution. In a usual decoding process,
the output word ŷt is determined by selecting a
word with the highest probability (if the decoder
outputs 1-best translation by greedy search). 1

ŷt = argmax
yt

Pr(yt|y<t,x), (1)

where y<t and x are the history of the output
words and the input word sequence, respectively.

In contrast, the back-translator in this paper de-
termines the output word by sampling based on the

1In translation, an output sentence is generally generated
from multiple hypotheses using beam search. However, it is
the same that the beam search selects high-likelihood words.



57

Log-likelihood Synthetic Source Sentence
-2.25 what should i do when i get injured or sick in japan ?
-2.38 what should i do if i get injured or sick in japan ?
-5.20 what should i do if i get injured or illness in japan ?
-5.52 what should we do when we get injured or sick in japan ?
-13.87 if i get injured or a sickness in japan , what shall i do ?

Target Sentence 日本で怪我や病気をしたときはどうすればいいのでしょう
か ?

Manual Back-Translation what should i do when i get injured or sick in japan ?

Table 1: Examples of Synthetic Source Sentences (English-Japanese Translation):
The italicized words indicate differences with the manual back-translation.

word distribution.

ŷt = sampling
yt

(Pr(yt|y<t,x)), (2)

where samplingy(P ) denotes the sampling oper-
ation of y based on the probability distribution P .
The decoding continues until the end-of-sentence
symbol is generated.2 We repeat the above process
to generate multiple synthetic sentences. Note that
this generation method is the same as that of the
minimum risk training (Shen et al., 2016).

In NMT, even if a low-probability word is
selected by the sampling, the subsequent word
would become fluent because it is conditioned
by the history. Table 1 presents examples of
the synthetic source sentences produced by the
back-translator. Most of the synthetic source sen-
tences are identical, or close to, the manual back-
translation (i.e., the reference translation). On the
other hand, the last example is quite different from
the perspective of word order because the clauses
are inverted. Such a synthetic sentence is usually
not produced by the n-best translation because of
the low likelihood. However, it is possible to gen-
erate diverse source sentences by sampling.

The sampling occasionally generates identical
sentences as a result. However, we did not remove
the duplication to reflect the original probability
distribution.

3.2 Training

The synthetic source sentences are paired with the
target sentences to construct the synthetic parallel
corpus. The NMT model is trained on a mixture of
the synthetic corpus and the base parallel corpus.

2The back-translator does not use the beam search be-
cause the sampling is independently performed for each
word.

In the training, we must deal with the two dif-
ferent types of sentence pairs. In addition, if we
use multiple source sentences for a given target
sentence, the model will be biased toward the syn-
thetic corpus. To avoid this problem, we adjust the
learning rate according to the size of the corpora.
Specifically, we first configure two mini-batch sets
each from the base and synthetic corpora. There-
after, the learning rate η/N is applied to the mini-
batches of the synthetic corpus, in contrast to the
learning rate η for those of the base corpus, where
N denotes the number of synthetic source sen-
tences per target sentence. Finally, the two sets
are shuffled and used for training.

The training time increases along with the in-
crease of data. However, the translation speed
does not change because the model structure is not
changed.

It must be noted that if the domains of the base
parallel and the target monolingual corpora are
different, it is better to perform “further training”
using the base parallel corpus for domain adapta-
tion (Freitag and Al-Onaizan, 2016; Servan et al.,
2016). 3

3.3 Filtering of Synthetic Parallel Sentences

The synthetic source sentences contain errors. A
direct approach to reduce such errors involves fil-
tering the sentence pairs according to their qual-
ity. In this paper, we consider the following three
methods.

3.3.1 Likelihood Filtering
The first method is filtering by the likelihood out-
put from the back-translator. We consider the
likelihood as an indicator of translation quality,
and low-likelihood synthetic sentences are filtered

3We did not perform “further training” in this paper.



58

out. Note that the likelihood is corrected with
the length of the synthetic source sentence. We
call this the length biased log-likelihood lllen (Oda
et al., 2017).

lllen(y|x) =
∑
t

log Pr(yt|x,y<t)+WP ·T, (3)

where the first term on the right-hand side is
the log-likelihood, WP denotes the word penalty
(WP ≥ 0), and T denotes the number of words in
the synthetic source sentence.

NMTs tend to generate shorter translations than
the expectation (Morishita et al., 2017). The word
penalty works to increase the likelihood of long
hypotheses when it is set to a positive value. With
an appropriate value, we can obtain synthetic sen-
tences that are almost of the same length as the
manual back-translation. We set the word penalty
such that the lengths of the translation and refer-
ence translation on the development set are ap-
proximately equal, using line search.

3.3.2 Confidence Filtering
The second method involves filtering with the con-
fidence of translation used in the translation qual-
ity estimation task. We use the data provided by
Fujita and Sumita (2017), which is a collection of
manual labels indicating whether the translation is
acceptable or not. We train the support vector ma-
chines (SVMs) on the sentence-level data and re-
gard the classifier’s score as the confidence score.

The features of the SVM classifier include
the 17 basic features of QuEst++ (Specia et al.,
2015).4 They are roughly categorized into the fol-
lowing two types.

• Language model features of each of the
source and target sentences.

• Features based on the parallel sentences such
as the average number of translation hypothe-
ses per word.

In addition, we add the source and target word em-
beddings. The sentence features are computed by
averaging all word embeddings (Shah et al., 2016).
The hyperparameters for the training are set using
the grid search on the development set.

In the expriments of Section 4, features are ex-
tracted from the base parallel corpus.

4http://www.quest.dcs.shef.ac.uk/

Type # Sentences
Parallel Base 400,000

Development 2,000
Test 2,000

Monolingual GCP Corpus 1,552,475
(Japanese) BCCWJ 4,791,336

Table 2: Corpus Statistics

3.3.3 Random Filtering

The third method is random filtering. This is iden-
tical to the reduction of the number of synthetic
source sentences to be generated.

4 Experiments

4.1 Experimental Settings

Corpora The corpus sizes used here are shown
in Table 2. We used the global communica-
tion plan corpus (the GCP corpus, (Imamura and
Sumita, 2018)), which is an in-house parallel cor-
pus of daily life conversations and consists of
Japanese (Ja), English (En), and Chinese (Zh).
The experiments were performed on English-
to-Japanese and Chinese-to-Japanese translation
tasks. We randomly selected 400K sentences for
the base parallel corpus, and the remaining (1.55M
sentences) were used as the Japanese monolingual
corpus. The reason for dividing the parallel corpus
into two corpora is to measure the upper-bound
of quality improvement by using existing paral-
lel texts on the same domain as the manual back-
translation.

We also used the Balanced Corpus of Contem-
porary Written Japanese (BCCWJ)5 as a monolin-
gual corpus from a different domain. We used ap-
proximately 4.8M sentences, each of which con-
tains less than 1024 characters. We assume practi-
cal situations in which the domains of parallel and
monolingual corpora are not identical.

All sentences were segmented into words using
an in-house word segmenter. The words were fur-
ther segmented into 16K sub-words based on the
byte-pair encoding rules (Sennrich et al., 2016b)
acquired from the base parallel corpus for each
language independently.

Translation System The translation system
used in this study was OpenNMT (Klein et al.,
2017). We modified it to accept Sections 3.1 and
3.2.

5http://pj.ninjal.ac.jp/corpus center/bccwj/en/



59

The encoder was comprised of a two-layer Bi-
LSTM (500 + 500 units), the decoder included a
two-layer LSTM (1,000 units), and the stochastic
gradient descent was used for optimization. The
learning rate for the base parallel corpus was 1.0
for the first 14 epochs, followed by the annealing
of 6 epochs while decreasing the learning rate by
half. The mini-batch size was 64.

At the translation stage, we generated 10-best
translations and selected the best among them on
the basis of the length reranking (Morishita et al.,
2017). Equation 3 was used as the score func-
tion for the reranking. By correcting the trans-
lation length, the translation quality can be com-
pared without the effect of the brevity penalty of
the BLEU score.

The back-translator was comprised of the same
system. We generated 10 synthetic source sen-
tences per target sentence using the method de-
scribed in Section 3.1, and filtered them to create
synthetic parallel sentences.

Competing Methods In this paper, we consider
the case in which only the base parallel corpus
is used as the baseline, and the case in which
the manual back-translation of the GCP corpus is
added as the upper-bound of the translation qual-
ity. Thereafter, we compare the following methods
and settings:

• Various numbers of synthetic source sen-
tences for a given target sentence

• The methods for generating synthetic source
sentences: sampling vs. n-best generation

• The three filtering methods described in Sec-
tion 3.3

Evaluation BLEU (Papineni et al., 2002) was
used for the evaluation. The multeval tool (Clark
et al., 2011)6 was used for statistical testing at a
significance level of 5% (p < 0.05).

4.2 Results with GCP Corpus

Figures 3 and 4 depict the relationship between
the number of synthetic source sentences and the
BLEU score on the GCP corpus of En-Ja and
Zh-Ja translation tasks, respectively. The graphs
and tables in the figures present the same data
for overviews and for analyzing the data in detail.
Note that the method of Sennrich et al. (2016a)
corresponds to the case of one synthetic source

6https://github.com/jhclark/multeval

sentence of the n-best generation (i.e., 1-best gen-
eration).

In both En-Ja and Zh-Ja translation, the score
was improved when multiple synthetic sentences
were given. Even though the method of Sennrich
et al. (2016a) achieved improvements of +2.42
and +2.38 BLEU points from the base corpus only
for En-Ja and Zh-Ja translations, respectively, fur-
ther improvements were observed by using multi-
ple synthetic sentences. Since the target sentences
were the same in all cases except for the base cor-
pus only, we can conclude that providing multiple
source sentences is effective for improving the en-
coder and attention.7

The improvements from the base corpus only
to the manual back-translation reached +4.86 and
+5.29 BLEU points in En-Ja and Zh-Ja transla-
tions, respectively. When we focus on the case in
which the number of synthetic source sentences is
6, for example, the improvements in the proposed
methods (the likelihood, confidence, and random
filtering) were achieved at least +4.08 and +5.01
BLEU points. This means that more than 80%
of improvements with the manual back-translation
were achieved using only monolingual corpora.
Nevertheless, all methods did not reach the BLEU
score of the manual back-translation; thus, we can-
not substitute parallel corpora with monolingual
corpora.

When we compared the three filtering meth-
ods, the BLEU scores were almost equivalent in
most cases. In fact, there were no significant dif-
ferences among filtering methods in all cases of
Zh-Ja translation. In En-Ja translation, there were
some significantly different cases, but the signifi-
cance was not consistently derived.

When the synthetic source generation was
changed to the n-best generation, the BLEU scores
were visibly degraded relative to the proposed
method (i.e., sampling). We speculate that the
likelihood and confidence filtering were ineffec-
tive because of the high-quality back-translator,
and the diversity of the synthetic source sentences
contributed considerably to quality improvement.

4.3 Results with BCCWJ

Table 3 shows the results using BCCWJ as a
monolingual corpus (the results of the GCP cor-

7Unfortunately, it is unknown in this experiment whether
the encoder or attention were enhanced. We plan to investi-
gate which module is enhanced by freezing parameters (Zoph
et al., 2016) of the encoder and attention through the training.



60

25.026.027.028.029.030.031.032.0 0 2 4 6 8 10BLEU Number of Synthetic Source SentencesLikelihood FilteringConfidence FilteringRandom FilteringN-best GenerationManual Back-TranslationBase Corpus OnlySennrich et al. (2016)
En-Ja

# of Synthetic Likelihood Confidence Random N-best
Sentences Filtering Filtering Filtering Generation

Base Corpus Only 26.19
Sennrich et al. (2016a) 28.61 (+2.42)

1 29.01 (+2.82) 28.49 (+2.30) 28.85 (+2.66) 28.61 (+2.42)
2 30.16 (+3.97) 29.26 (+3.07) 30.30 (+4.11) 29.61 (+3.42)
4 30.99 (+4.80) 30.26 (+4.07) 30.08 (+3.89) 29.51 (+3.32)
6 30.41 (+4.22) 30.59 (+4.40) 30.27 (+4.08) 29.62 (+3.43)
8 30.39 (+4.20) 30.53 (+4.34) 30.22 (+4.03) 30.39 (+4.20)

10 30.66 (+4.47) 30.66 (+4.47) 30.66 (+4.47) 29.70 (+3.51)
Manual Back-Translation 31.05 (+4.86)

Figure 3: The BLEU scores using the GCP corpus (English-Japanese translation) represented by a graph
and table. The bracketed values of the table indicate differences from those of the base corpus only.36.037.038.039.040.041.042.043.0 0 2 4 6 8 10BLEU Number of Synthetic Source SentencesLikelihood FilteringConfidence FilteringRandom FilteringN-best GenerationBase Corpus OnlyManual Back-TranslationSennrich et al. (2016)

Zh-Ja
# of Synthetic Likelihood Confidence Random N-best

Sentences Filtering Filtering Filtering Generation
Base Corpus Only 37.08

Sennrich et al. (2016a) 39.46 (+2.38)
1 40.63 (+3.55) 40.34 (+3.26) 40.88 (+3.80) 39.46 (+2.38)
2 41.35 (+4.27) 41.73 (+4.65) 41.68 (+4.60) 40.22 (+3.14)
4 41.92 (+4.84) 42.01 (+4.93) 41.63 (+4.55) 41.22 (+4.14)
6 42.14 (+5.06) 42.22 (+5.14) 42.09 (+5.01) 40.77 (+3.69)
8 42.31 (+5.23) 41.89 (+4.81) 42.30 (+5.22) 40.84 (+3.76)

10 41.80 (+4.72) 41.80 (+4.72) 41.80 (+4.72) 40.79 (+3.71)
Manual Back-Translation 42.37 (+5.29)

Figure 4: The BLEU scores using the GCP corpus (Chinese-Japanese translation) represented by a graph
and table. The bracketed values of the table indicate differences from those of the base corpus only.



61

# of Synthetic BCCWJ GCP Corpus
Source Sentences BLEU BLEU

0 (Base Corpus Only) 26.19
1 29.84 29.01
2 29.94 30.16
4 30.66 30.99

Manual Back-Translation - 31.05

Table 3: The BLEU scores of the BCCWJ and
GCP corpora according to the number of synthetic
source sentences (En-Ja, the random filtering).

Sampling N-best Gen.
BLEU 15.05 21.55
Edit Distance

A) between SYN and MAN 9.73 8.52
B) among SYNs 9.34 3.90

Table 4: The BLEU scores and the edit distances
of synthetic source sentences based on 10 syn-
thetic sentences and manual back-translation for
the same 1,000 target sentences in the GCP cor-
pus.

pus are also shown for reference). In this study,
we only performed random filtering experiments
on En-Ja translation due to resource limitations.

In the case of BCCWJ, the BLEU scores in-
creased with the number of synthetic source sen-
tences, similar to the GCP corpus. We cannot di-
rectly compare the scores of the two corpora; how-
ever, similar improvement was achieved when we
used a several-fold size of the different domain
monolingual corpus.

4.4 Analysis

The above experiments consider diversity under
the following two assumptions.

• The number of synthetic source sentences in-
dicates the diversity.

• The diversity of the synthetic sentences by
sampling is higher than that of the n-best gen-
eration.

In this section, we quantify the diversity using the
edit distance among the systhetic source sentences
to compare the generation methods.

We sampled 1,000 Japanese sentences from the
GCP corpus in En-Ja translation with their ten cor-
responding back-translations generated by each
method. Table 4 shows the results. The BLEU
scores were computed regarding the 10,000 sen-
tences as a document. The edit distances were

computed for the following two cases, setting the
insertion, deletion, and substitution costs to 1.0.

A) The average distance between a synthetic
sentence (SYN) and the manual back-
translation (MAN; i.e., reference translation).
Note that this value also indicates translation
quality because it is a source for computing
the word error rate (smaller value represents
better quality).

B) The average distance among synthetic source
sentences of a target sentence (10C2 = 45
combinations per target sentence).

As for the BLEU scores in Table 4, the sampling
method achieved a lower score than that of the n-
best generation. Similarly, the edit distance A of
the sampling had a larger value than that of the n-
best generation. These results imply that the sam-
pling generates poor synthetic sentences. How-
ever, these scores are influenced by the diversity
because they naturally become worse along with
the variety of synthetic sentences when they are
computed using a single reference.

On the other hand, as for edit distance B, the
distance of the n-best generation was less than half
of that of the sampling, even though sentences of
the sampling generation can include identical sen-
tences. Intuitively, the n-best generation generates
similar sentences where only few words are differ-
ent. As shown in Table 4, the distances of the syn-
thetic sentences by sampling were almost the same
as those from the manual back-translation, and the
distances by the n-best generation were not. This
result verifies that the generation by sampling in-
creases the diversity of the synthetic source sen-
tences.

5 Application to Self-Training Using
Parallel Corpora

In this paper, we enhanced the encoder and atten-
tion using target monolingual corpora. Our pro-
posed method can be applied to a self-training
method only using parallel corpora. Specifically,
we train a back-translator using a given parallel
corpus, and the target side of the parallel corpus
is translated into the source. Then, the original
and synthetic parallel corpora are mixed. We fi-
nally train the forward translator using this corpus
to enhance the encoder.



62

# of Source Sentences BLEU
1 (Manual Bitexts Only) 31.05

2 (Manual Bitexts + 1 Syn. Sentence) 31.29
4 (Manual Bitexts + 3 Syn. Sentences) 31.65
6 (Manual Bitexts + 5 Syn. Sentences) 31.75
8 (Manual Bitexts + 7 Syn. Sentences) 32.25
10 (Manual Bitexts + 9 Syn. Sentences) 32.28

Table 5: The effect of self-training (En-Ja transla-
tion)

5.1 Settings

We confirm whether the quality can be improved
from the upper-bound of the experiments in Sec-
tion 4.

The experimental settings were the same as
those of Section 4 except for the corpora. We
considered the mixture of the base and GCP cor-
pora (including the manual back-translation) in
Table 2 as the original parallel corpus, with 1.95M
sentences. The monolingual corpus was the tar-
get side of the entire parallel corpus. The back-
translator generated nine synthetic source sen-
tences, and they were randomly filtered. The orig-
inal and synthetic parallel corpora were concate-
nated to train the forward translator. Namely, the
number of source sentences per target sentence
was at most ten.

In this experiment, we used the learning rate η
for the original parallel corpus and η/N for the
synthetic parallel corpus, where N denotes the
number of synthetic source sentences per target
sentence. The learning rate was η = 0.5, which
means 1.0 for a target sentence in total.

5.2 Results

Table 5 shows the BLEU scores in the En-Ja trans-
lation according to the number of source sen-
tences. Similar to the results in Section 4, the
BLEU scores increased along with the increase in
the number of source sentences. When we added
nine synthetic source sentences, the BLEU score
was improved by +1.23 points in comparison to
the manual bitext only. Therefore, by increasing
the diversity of the manual translation using syn-
thetic sentences, we can further enhance the en-
coder and attention.

6 Conclusions

In this paper, we enhanced the encoder and atten-
tion by using multiple synthetic source sentences,
in which target monolingual corpora were trans-

lated by sampling. During the training, we used
different learning rates for the base and synthetic
parallel corpora to avoid overfitting to the syn-
thetic corpus. As a result, the translation qual-
ity was improved by increasing the number of
synthetic source sentences for a given target sen-
tence, and the quality approached that of the man-
ual back-translation. In addition, we confirmed the
generation by sampling synthesized diverse source
sentences and consequently improved the transla-
tion quality in comparison with the n-best genera-
tion. We also attempted some filtering methods on
the synthetic source sentences to obtain improved
parallel sentences, but we could not confirm their
effectiveness in our experiments.

Our future work is to clarify the other con-
ditions where the proposed method is effective,
such as the relationship between qualities of the
backward and forward translations, experiments
on public data sets, and comparison with the num-
ber of synthetic sentences and monolingual cor-
pus size at the same training time. In addition, we
plan to consider other applications, such as apply-
ing our methods to smaller parallel corpora and
using source monolingual corpora.

Acknowledgments

The authors appreciate anonymous reviewers for
their helpful comments.

This work was supported by “Promotion of
Global Communications Plan — Research and
Development and Social Demonstration of Mul-
tilingual Speech Translation Technology,” a pro-
gram of the Ministry of Internal Affairs and Com-
munications, Japan.

References
Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua

Bengio. 2014. Neural machine translation by
jointly learning to align and translate. CoRR,
abs/1409.0473.

Yong Cheng, Wei Xu, Zhongjun He, Wei He, Hua
Wu, Maosong Sun, and Yang Liu. 2016. Semi-
supervised learning for neural machine translation.
In Proceedings of the 54th Annual Meeting of the As-
sociation for Computational Linguistics (Volume 1:
Long Papers), pages 1965–1974, Berlin, Germany.

Jonathan H. Clark, Chris Dyer, Alon Lavie, and
Noah A. Smith. 2011. Better hypothesis testing for
statistical machine translation: Controlling for op-
timizer instability. In Proceedings of the 49th An-
nual Meeting of the Association for Computational

http://arxiv.org/abs/1409.0473
http://arxiv.org/abs/1409.0473
http://www.aclweb.org/anthology/P16-1185
http://www.aclweb.org/anthology/P16-1185
http://www.aclweb.org/anthology/P11-2031
http://www.aclweb.org/anthology/P11-2031
http://www.aclweb.org/anthology/P11-2031


63

Linguistics: Human Language Technologies, pages
176–181, Portland, Oregon, USA.

Anna Currey, Antonio Valerio Miceli Barone, and Ken-
neth Heafield. 2017. Copied monolingual data im-
proves low-resource neural machine translation. In
Proceedings of the Second Conference on Machine
Translation, pages 148–156, Copenhagen, Den-
mark.

Tobias Domhan and Felix Hieber. 2017. Using target-
side monolingual data for neural machine transla-
tion through multi-task learning. In Proceedings of
the 2017 Conference on Empirical Methods in Natu-
ral Language Processing, pages 1500–1505, Copen-
hagen, Denmark.

Markus Freitag and Yaser Al-Onaizan. 2016. Fast
domain adaptation for neural machine translation.
CoRR, abs/1612.06897.

Atsushi Fujita and Eiichiro Sumita. 2017. Japanese
to English/Chinese/Korean datasets for translation
quality estimation and automatic post-editing. In
Proceedings of the 4th Workshop on Asian Trans-
lation (WAT2017), pages 79–88, Taipei, Taiwan.

Çaglar Gülçehere, Orhan Firat, Kelvin Xu, Kyunghyun
Cho, Loı̈c Barrault, Huei-Chi Ling, Fethi Bougares,
Holger Schwenk, and Yoshua Bengio. 2015. On us-
ing monolingual corpora in neural machine transla-
tion. In CoRR, abs/1503.03535.

Kenji Imamura and Eiichiro Sumita. 2018. Multilin-
gual parallel corpus for global communication plan.
In Proceedings of the Eleventh International Confer-
ence on Language Resources and Evaluation (LREC
2018), pages 3453–3458, Miyazaki, Japan.

Guillaume Klein, Yoon Kim, Yuntian Deng, Jean
Senellart, and Alexander M. Rush. 2017. Open-
NMT: Open-source toolkit for neural machine trans-
lation. In Proceedings of ACL 2017, System Demon-
strations, pages 67–72, Vancouver, Canada.

Makoto Morishita, Jun Suzuki, and Masaaki Nagata.
2017. NTT neural machine translation systems at
WAT 2017. In Proceedings of the 4th Workshop on
Asian Translation (WAT2017), pages 89–94, Taipei,
Taiwan.

Yusuke Oda, Katsuhito Sudoh, Satoshi Nakamura,
Masao Utiyama, and Eiichiro Sumita. 2017. A
simple and strong baseline: NAIST-NICT neural
machine translation system for WAT2017 English-
Japanese translation task. In Proceedings of the 4th
Workshop on Asian Translation (WAT2017), pages
135–139, Taipei, Taiwan.

Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a method for automatic
evaluation of machine translation. In Proceedings
of the 40th Annual Meeting of the Association for
Computational Linguistics (ACL), pages 311–318,
Philadelphia, Pennsylvania, USA.

Rico Sennrich, Barry Haddow, and Alexandra Birch.
2016a. Improving neural machine translation mod-
els with monolingual data. In Proceedings of the
54th Annual Meeting of the Association for Compu-
tational Linguistics (ACL-2016, Volume 1: Long Pa-
pers), pages 86–96, Berlin, Germany.

Rico Sennrich, Barry Haddow, and Alexandra Birch.
2016b. Neural machine translation of rare words
with subword units. In Proceedings of the 54th An-
nual Meeting of the Association for Computational
Linguistics (Volume 1: Long Papers), pages 1715–
1725, Berlin, Germany.

Christophe Servan, Josep Maria Crego, and Jean Senel-
lart. 2016. Domain specialization: a post-training
domain adaptation for neural machine translation.
CoRR, abs/1612.06141.

Kashif Shah, Fethi Bougares, Loı̈c Barrault, and Lucia
Specia. 2016. Shef-lium-nn: Sentence level qual-
ity estimation with neural network features. In Pro-
ceedings of the First Conference on Machine Trans-
lation, pages 838–842, Berlin, Germany.

Shiqi Shen, Yong Cheng, Zhongjun He, Wei He, Hua
Wu, Maosong Sun, and Yang Liu. 2016. Minimum
risk training for neural machine translation. In Pro-
ceedings of the 54th Annual Meeting of the Associa-
tion for Computational Linguistics (Volume 1: Long
Papers), pages 1683–1692, Berlin, Germany.

Lucia Specia, Gustavo Paetzold, and Carolina Scar-
ton. 2015. Multi-level translation quality predic-
tion with quest++. In Proceedings of ACL-IJCNLP
2015 System Demonstrations, pages 115–120, Bei-
jing, China.

Anuroop Sriram, Heewoo Jun, Sanjeev Satheesh, and
Adam Coates. 2017. Cold fusion: Training seq2seq
models together with language models. CoRR,
abs/1708.06426.

Ilya Sutskever, Oriol Vinyals, and Quoc V. Le. 2014.
Sequence to sequence learning with neural net-
works. In Proceedings of Advances in Neural Infor-
mation Processing Systems 27 (NIPS 2014), pages
3104–3112.

Jiajun Zhang and Chengqing Zong. 2016. Exploit-
ing source-side monolingual data in neural machine
translation. In Proceedings of the 2016 Conference
on Empirical Methods in Natural Language Pro-
cessing (EMNLP-2016), pages 1535–1545, Austin,
Texas.

Barret Zoph, Deniz Yuret, Jonathan May, and Kevin
Knight. 2016. Transfer learning for low-resource
neural machine translation. In Proceedings of the
2016 Conference on Empirical Methods in Natu-
ral Language Processing, pages 1568–1575, Austin,
Texas.

http://www.aclweb.org/anthology/W17-4715
http://www.aclweb.org/anthology/W17-4715
https://www.aclweb.org/anthology/D17-1158
https://www.aclweb.org/anthology/D17-1158
https://www.aclweb.org/anthology/D17-1158
http://arxiv.org/abs/1612.06897
http://arxiv.org/abs/1612.06897
http://www.aclweb.org/anthology/W17-5705
http://www.aclweb.org/anthology/W17-5705
http://www.aclweb.org/anthology/W17-5705
https://arxiv.org/abs/1503.03535
https://arxiv.org/abs/1503.03535
https://arxiv.org/abs/1503.03535
http://www.lrec-conf.org/proceedings/lrec2018/pdf/104.pdf
http://www.lrec-conf.org/proceedings/lrec2018/pdf/104.pdf
http://aclweb.org/anthology/P17-4012
http://aclweb.org/anthology/P17-4012
http://aclweb.org/anthology/P17-4012
http://www.aclweb.org/anthology/W17-5706
http://www.aclweb.org/anthology/W17-5706
http://www.aclweb.org/anthology/W17-5712
http://www.aclweb.org/anthology/W17-5712
http://www.aclweb.org/anthology/W17-5712
http://www.aclweb.org/anthology/W17-5712
http://aclweb.org/anthology/P02-1040
http://aclweb.org/anthology/P02-1040
https://doi.org/10.18653/v1/P16-1009
https://doi.org/10.18653/v1/P16-1009
http://aclweb.org/anthology/P16-1162
http://aclweb.org/anthology/P16-1162
http://arxiv.org/abs/1612.06141
http://arxiv.org/abs/1612.06141
http://www.aclweb.org/anthology/W16-2392
http://www.aclweb.org/anthology/W16-2392
http://www.aclweb.org/anthology/P16-1159
http://www.aclweb.org/anthology/P16-1159
http://www.aclweb.org/anthology/P15-4020
http://www.aclweb.org/anthology/P15-4020
http://arxiv.org/abs/1708.06426
http://arxiv.org/abs/1708.06426
http://papers.nips.cc/paper/5346-sequence-to-sequence-learning-with-neural-networks.pdf
http://papers.nips.cc/paper/5346-sequence-to-sequence-learning-with-neural-networks.pdf
https://aclweb.org/anthology/D16-1160
https://aclweb.org/anthology/D16-1160
https://aclweb.org/anthology/D16-1160
https://aclweb.org/anthology/D16-1163
https://aclweb.org/anthology/D16-1163

