



















































Biases in Predicting the Human Language Model


Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 7–12,
Baltimore, Maryland, USA, June 23-25 2014. c©2014 Association for Computational Linguistics

Biases in Predicting the Human Language Model

Alex B. Fine
University of Illinois at Urbana-Champaign

abfine@illinois.edu

Austin F. Frank
Riot Games

aufrank@riotgames.com

T. Florian Jaeger
University of Rochester

fjaeger@bcs.rochester.edu

Benjamin Van Durme
Johns Hopkins University
vandurme@cs.jhu.edu

Abstract

We consider the prediction of three hu-
man behavioral measures – lexical deci-
sion, word naming, and picture naming –
through the lens of domain bias in lan-
guage modeling. Contrasting the predic-
tive ability of statistics derived from 6 dif-
ferent corpora, we find intuitive results
showing that, e.g., a British corpus over-
predicts the speed with which an Amer-
ican will react to the words ward and
duke, and that the Google n-grams over-
predicts familiarity with technology terms.
This study aims to provoke increased con-
sideration of the human language model
by NLP practitioners: biases are not lim-
ited to differences between corpora (i.e.
“train” vs. “test”); they can exist as well
between corpora and the intended user of
the resultant technology.

1 Introduction

Computational linguists build statistical language
models for aiding in natural language processing
(NLP) tasks. Computational psycholinguists build
such models to aid in their study of human lan-
guage processing. Errors in NLP are measured
with tools like precision and recall, while errors in
psycholinguistics are defined as failures to model
a target phenomenon.

In the current study, we exploit errors of the lat-
ter variety—failure of a language model to predict
human performance—to investigate bias across
several frequently used corpora in computational
linguistics. The human data is revealing because
it trades on the fact that human language process-
ing is probability-sensitive: language processing

reflects implicit knowledge of probabilities com-
puted over linguistic units (e.g., words). For ex-
ample, the amount of time required to read a word
varies as a function of how predictable that word is
(McDonald and Shillcock, 2003). Thus, failure of
a language model to predict human performance
reveals a mismatch between the language model
and the human language model, i.e., bias.

Psycholinguists have known for some time that
the ability of a corpus to explain behavior depends
on properties of the corpus and the subjects (cf.
Balota et al. (2004)). We extend that line of work
by directly analyzing and quantifying this bias,
and by linking the results to methodological con-
cerns in both NLP and psycholinguistics.

Specifically, we predict human data from
three widely used psycholinguistic experimental
paradigms—lexical decision, word naming, and
picture naming—using unigram frequency esti-
mates from Google n-grams (Brants and Franz,
2006), Switchboard (Godfrey et al., 1992), spoken
and written English portions of CELEX (Baayen
et al., 1995), and spoken and written portions
of the British National Corpus (BNC Consor-
tium, 2007). While we find comparable overall
fits of the behavioral data from all corpora un-
der consideration, our analyses also reveal spe-
cific domain biases. For example, Google n-
grams overestimates the ease with which humans
will process words related to the web (tech, code,
search, site), while the Switchboard corpus—a
collection of informal telephone conversations be-
tween strangers—overestimates how quickly hu-
mans will react to colloquialisms (heck, darn) and
backchannels (wow, right).

7



Figure 1: Pairwise correlations between log frequency es-
timates from each corpus. Histograms show distribution over
frequency values from each corpus. Lower left panels give
Pearson (top) and Spearman (bottom) correlation coefficients
and associated p-values for each pair. Upper right panels plot
correlations

2 Fitting Behavioral Data

2.1 Data
Pairwise Pearson correlation coefficients for log
frequency were computed for all corpora under
consideration. Significant correlations were found
between log frequency estimates for all pairs (Fig-
ure 1). Intuitive biases are apparent in the corre-
lations, e.g.: BNCw correlates heavily with BNCs
(0.91), but less with SWBD (0.79), while BNCs
correlates more with SWBD (0.84).1

Corpus Size (tokens)

Google n-grams (web release) ∼ 1 trillion
British National Corpus (written, BNCw) ∼ 90 million
British National Corpus (spoken, BNCs) ∼ 10 million
CELEX (written, CELEXw) ∼ 16.6 million
CELEX (spoken, CELEXs) ∼ 1.3 million
Switchboard (Penn Treebank subset 3) ∼ 800,000

Table 1: Summary of the corpora under consideration.

2.2 Approach
We ask whether domain biases manifest as sys-
tematic errors in predicting human behavior. Log
unigram frequency estimates were derived from
each corpus and used to predict reaction times
(RTs) from three experiments employing lexical

1BNCw and BNCs are both British, while BNCs and
SWBD are both spoken.

decision (time required by subjects to correctly
identify a string of letters as a word of English
(Balota et al., 1999)); word naming (time required
to read aloud a visually presented word (Spieler
and Balota, 1997); (Balota and Spieler, 1998));
and picture naming (time required to say a pic-
ture’s name (Bates et al., 2003)). Previous work
has shown that more frequent words lead to faster
RTs. These three measures provide a strong test
for the biases present in these corpora, as they
span written and spoken lexical comprehension
and production.

To compare the predictive strength of log fre-
quency estimates from each corpus, we fit mixed
effects regression models to the data from each
experiment. As controls, all models included (1)
mean log bigram frequency for each word, (2)
word category (noun, verb, etc.), (3) log mor-
phological family size (number of inflectional and
derivational morphological family members), (4)
number of synonyms, and (5) the first principal
component of a host of orthographic and phono-
logical features capturing neighborhood effects
(type and token counts of orthographic and phono-
logical neighbors as well as forward and backward
inconsistent words; (Baayen et al., 2006)). Mod-
els of lexical decision and word naming included
random intercepts of participant age to adjust for
differences in mean RTs between old (mean age
= 72) vs. young (mean age = 23) subjects, given
differences between younger vs. older adults’ pro-
cessing speed (cf. (Ramscar et al., 2014)). (All
participants in the picture naming study were col-
lege students.)

2.3 Results

For each of the six panels corresponding to fre-
quency estimates from a corpus A, Figure 2 gives
the χ2 value resulting from the log-likelihood ra-
tio of (1) a model containing A and an estimate
from one of the five remaining corpora (given on
the x axis) and (2) a model containing just the cor-
pus indicated on the x axis. Thus, for each panel,
each bar in Figure 2 shows the explanatory power
of estimates from the corpus given at the top of the
panel after controlling for estimates from each of
the other corpora.

Model fits reveal intuitive, previously undocu-
mented biases in the ability of each corpus to pre-
dict human data. For example, corpora of British
English tend to explain relatively little after con-

8



trolling for other British corpora in modeling lexi-
cal decision RTs (yellow). Similarly, Switchboard
provides relatively little explanatory power over
the other corpora in predicting picture naming
RTs (blue bars), possibly because highly image-
able nouns and verbs frequent in everyday interac-
tions are underrepresented in telephone conversa-
tions between people with no common visual ex-
perience. In other words, idiosyncratic facts about
the topics, dialects, etc. represented in each cor-
pus lead to systematic patterns in how well each
corpus can predict human data relative to the oth-
ers. In some cases, the predictive value of one
corpus after controlling for another—apparently
for reasons related to genre, dialect—can be quite
large (cf. the χ2 difference between a model with
both Google and Switchboard frequency estimates
compared to one with only Switchboard [top right
yellow bar]).

In addition to comparing the overall predictive
power of the corpora, we examined the words
for which behavioral predictions derived from the
corpora deviated most from the observed behav-
ior (word frequencies strongly over- or under-
estimated by each corpora). First, in Table 2 we
give the ten words with the greatest relative differ-
ence in frequency for each corpus pair. For exam-
ple, fife is deemed more frequent according to the
BNC than to Google.2

These results suggest that particular corpora
may be genre-biased in systematic ways. For in-
stance, Google appears to be biased towards termi-
nology dealing with adult material and technology.
Similarly, BNCw is biased, relative to Google, to-
wards Britishisms. For these words in the BNC
and Google, we examined errors in predicted lexi-
cal decision times. Figure 3 plots errors in the lin-
ear model’s prediction of RTs for older (top) and
younger (bottom) subjects.

The figure shows a positive correlation between
how large the difference is between the lexical de-
cision RT predicted by the model and the actu-
ally observed RT, and how over-estimated the log
frequency of that word is in the BNC relative to
Google (left panel) or in Google relative to the
BNC (right panel). The left panel shows that BNC
produces a much greater estimate of the log fre-

2Surprisingly, fife was determined to be one of the words
with the largest frequency asymmetry between Switchboard
and the Google n-grams corpus. This was a result of lower-
casing all of the words in in the analyses, and the fact that
Barney Fife was mentioned several times in the BNC.

quency of the word lee relative to Google, which
leads the model to predict a lower RT for this word
than is observed (i.e., the error is positive; though
note that the error is less severe for older relative to
younger subjects). By contrast, the asymmetry be-
tween the two corpora in the estimated frequency
of sir is less severe, so the observed RT deviates
less from the predicted RT. In the right panel, we
see that Google assigns a much greater estimate
of log frequency to the word tech than the BNC,
which leads a model predicting RTs from Google-
derived frequency estimates to predict a far lower
RT for this word than observed.

3 Discussion

Researchers in computational linguistics often as-
sume that more data is always better than less
data (Banko and Brill, 2001). This is true in-
sofar as larger corpora allow computational lin-
guists to generate less noisy estimates of the av-
erage language experience of the users of compu-
tational linguistics applications. However, corpus
size does not necessarily eliminate certain types of
biases in estimates of human linguistic experience,
as demonstrated in Figure 3.

Our analyses reveal that 6 commonly used cor-
pora fail to reflect the human language model in
various ways related to dialect, modality, and other
properties of each corpus. Our results point to
a type of bias in commonly used language mod-
els that has been previously overlooked. This bias
may limit the effectiveness of NLP algorithms in-
tended to generalize to a linguistic domains whose
statistical properties are generated by humans.

For psycholinguists these results support an im-
portant methodological point: while each corpus
presents systematic biases in how well it predicts
human behavior, all six corpora are, on the whole,
of comparable predictive value and, specifically,
the results suggest that the web performs as well
as traditional instruments in predicting behavior.
This has two implications for psycholinguistic re-
search. First, as argued by researchers such as
Lew (2009), given the size of the Web compared to
other corpora, research focusing on low-frequency
linguistic events—or requiring knowledge of the
distributional characteristics of varied contexts—
is now more tractable. Second, the viability of
the web in predicting behavior opens up possibil-
ities for computational psycholinguistic research
in languages for which no corpora exist (i.e., most

9



CELEX written BNC written Google

CELEX spoken BNC spoken Switchboard

0

40

80

120

0

10

20

30

40

0

10

20

30

0

10

20

30

0

10

20

30

40

0

5

10

C
E

LE
X

 w
rit

te
n

B
N

C
 w

rit
te

n

G
oo

gl
e

C
E

LE
X

 s
po

ke
n

B
N

C
 s

po
ke

n

S
w

itc
hb

oa
rd

C
E

LE
X

 w
rit

te
n

B
N

C
 w

rit
te

n

G
oo

gl
e

C
E

LE
X

 s
po

ke
n

B
N

C
 s

po
ke

n

S
w

itc
hb

oa
rd

C
E

LE
X

 w
rit

te
n

B
N

C
 w

rit
te

n

G
oo

gl
e

C
E

LE
X

 s
po

ke
n

B
N

C
 s

po
ke

n

S
w

itc
hb

oa
rd

Comparison

χ Λ2

task

lexical decision

picture naming

word naming

Pairwise model comparisons

Figure 2: Results of log likelihood ratio model comparisons. Large values indicate that the reference predictor (panel title)
explained a large amount of variance over and above the predictor given on the x-axis.

Google and BNC written

Standardized difference score

E
rr

or
 in

 li
ne

ar
 m

od
el

centcent
damedame

doledoledukeduke

fifefife

glenglen
godgod

gulfgulf
hallhall

hankhank

kingking

leelee

lordlord
marchmarchmarchmarchmarchmarchmarchmarch

nicknick

primeprime

princeprince

sirsir
wardward

centcent damedame
doledole

dukeduke

fifefife
glenglen

godgod

gulfgulf

hallhall
hankhank

kingking

leelee

lordlord marchmarchmarchmarchmarchmarchmarchmarch

nicknick

primeprimeprinceprince

sirsir
wardward

assass

binbin

bugbug

buttbuttbuttbuttbuttbuttbuttbutt cartcart
chatchat clickclick

codecode
darndarn

denden

dialdialdikedikefilefileflipflip gaygayheckheckhophop hunkhunk
linklink loglog mailmail

mapmap pagepage

peepee
prepprep

printprint
quotequote

ranchranchscriptscript

searchsearch

selfself
sexsex

sitesite

skipskipslotslotstorestore
sucksuck

tagtag

techtech

teensteens

threadthread
tiretire

toetoe

twaintwain

webwebwhizwhiz

wowwow

zipzip

assass

binbin

bugbugbuttbuttbuttbuttbuttbuttbuttbutt

cartcartchatchat clickclick

codecode
darndarndenden
dialdial

dikedike

filefile

flipflip

gaygay
heckheck

hophop hunkhunk
linklink loglog mailmailmapmap pagepage

peepeeprepprep
printprint quotequoteranchranchscriptscript

searchsearch

selfself
sexsex

sitesite

skipskip

slotslot
storestore

sucksucktagtag

techtech

teensteens
threadthread

tiretire
toetoe

twaintwain webweb

whizwhiz
wowwow

zipzip

-0.1

0.0

0.1

0.2

0.3

0.4

-0.1

0.0

0.1

0.2

0.3

0.4

old
young

-3.5 -3.0 -2.5 2.5 3.0 3.5 4.0 4.5 5.0 5.5

Google < BNC written Google > BNC written

goog.f
-4

-2

0

2

Figure 3: Errors in the linear model predicting lexical decision RTs from log frequency are plotted against the standardized
difference in log frequency in the Google n-grams corpus versus the written portion of the BNC. Top and bottom panels show
errors for older and younger subjects, respectively. The left panel plots words with much greater frequency in the written
portion of the BNC relative to Google; the right panel plots words occurring more frequently in Google. Errors in the linear
model are plotted against the standardized difference in log frequency across the corpora, and word color encodes the degree to
which each word is more (red) or less (blue) frequent in Google. That the fit line in each graph is above 0 in the y-axis means
that on average these biased words in each domain are being over-predicted, i.e., the corpus frequencies suggest humans will
react (sometimes much) faster than they actually did in the lab.

10



Greater Lesser Top-10
google bnc.s web, ass, gay, tire, text, tool, code, woe, site, zip
google bnc.w ass, teens, tech, gay, bug, suck, site, cart, log, search
google celex.s teens, cart, gay, zip, mail, bin, tech, click, pee, site
google celex.w web, full, gay, bin, mail, zip, site, sake, ass, log
google swbd gay, thread, text, search, site, link, teens, seek, post, sex
bnc.w google fife, lord, duke, march, dole, god, cent, nick, dame, draught
bnc.w bnc.s pact, corps, foe, tract, hike, ridge, dine, crest, aide, whim
bnc.w celex.s staff, nick, full, waist, ham, lap, knit, sheer, bail, march
bnc.w celex.w staff, lord, last, nick, fair, glen, low, march, should, west
bnc.w swbd rose, prince, seek, cent, text, clause, keen, breach, soul, rise
celex.s google art, yes, pound, spoke, think, mean, say, thing, go, drove
celex.s bnc.s art, hike, pact, howl, ski, corps, peer, spoke, jazz, are
celex.s bnc.w art, yes, dike, think, thing, sort, mean, write, pound, lot
celex.s celex.w yes, sort, thank, think, jazz, heck, tape, well, fife, get
celex.s swbd art, cell, rose, spoke, aim, seek, shall, seed, text, knight
celex.w google art, plod, pound, shake, spoke, dine, howl, sit, say, draught
celex.w bnc.s hunch, stare, strife, hike, woe, aide, rout, yell, glaze, flee
celex.w bnc.w dike, whiz, dine, shake, grind, jerk, whoop, say, are, cram
celex.w celex.s wrist, pill, lawn, clutch, stare, spray, jar, shark, plead, horn
celex.w swbd art, rose, seek, aim, rise, burst, seed, cheek, grin, lip
swbd google mow, kind, lot, think, fife, corps, right, cook, sort, do
swbd bnc.s creek, mow, guess, pact, strife, tract, hank, howl, foe, nap
swbd bnc.w stuff, whiz, tech, lot, kind, creek, darn, dike, bet, kid
swbd celex.s wow, sauce, mall, deck, full, spray, flute, rib, guy, bunch
swbd celex.w heck, guess, right, full, stuff, lot, last, well, guy, fair

Table 2: Examples of words with largest difference in z-transformed log frequencies (e.g., the relative frequencies of fife,
lord, and duke, in the BNC are far greater than in Google).

languages). This furthers the arguments of the “the
web as corpus” community (Kilgarriff and Grefen-
stette, 2003) with respect to psycholinguistics.

Finally, combining multiple sources of fre-
quency estimates is one way researchers may be
able to reduce the prediction bias from any sin-
gle corpus. This relates to work in automatically
building domain specific corpora (e.g., Moore and
Lewis (2010), Axelrod et al. (2011), Daumé III
and Jagarlamudi (2011), Wang et al. (2014), Gao
et al. (2002), and Lin et al. (1997)). Those efforts
focus on building representative document collec-
tions for a target domain, usually based on a seed
set of initial documents. Our results prompt the
question: can one use human behavior as the tar-
get in the construction of such a corpus? Con-
cretely, can we build corpora by optimizing an ob-
jective measure that minimizes error in predicting
human reaction times? Prior work in building bal-
anced corpora used either rough estimates of the
ratio of genre styles a normal human is exposed to
daily (e.g., the Brown corpus (Kucera and Fran-
cis, 1967)), or simply sampled text evenly across
genres (e.g., COCA: the Corpus of Contemporary
American English (Davies, 2009)). Just as lan-
guage models have been used to predict reading
grade-level of documents (Collins-Thompson and
Callan, 2004), human language models could be

used to predict the appropriateness of a document
for inclusion in an “automatically balanced” cor-
pus.

4 Conclusion

We have shown intuitive, domain-specific biases
in the prediction of human behavioral measures
via corpora of various genres. While some psy-
cholinguists have previously acknowledged that
different corpora carry different predictive power,
this is the first work to our knowledge to system-
atically document these biases across a range of
corpora, and to relate these predictive errors to do-
main bias, a pressing issue in the NLP community.
With these results in hand, future work may now
consider the automatic construction of a “prop-
erly” balanced text collection, such as originally
desired by the creators of the Brown corpus.

Acknowledgments

The authors wish to thank three anonymous ACL
reviewers for helpful feedback. This research
was supported by a DARPA award (FA8750-13-2-
0017) and NSF grant IIS-0916599 to BVD, NSF
IIS-1150028 CAREER Award and Alfred P. Sloan
Fellowship to TFJ, and an NSF Graduate Research
Fellowship to ABF.

11



References
A. Axelrod, X. He, and J. Gao. 2011. Domain adap-

tation via pseudo in-domain data selection. In Pro-
ceedings of the Conference on Empirical Methods in
Natural Language Processing (EMNLP 11).

R. H. Baayen, R. Piepenbrock, and L. Gulikers. 1995.
The CELEX Lexical Database (Release 2). Linguis-
tic Data Consortium, Philadelphia.

R. H. Baayen, L. F. Feldman, and R. Schreuder.
2006. Morphological influences on the recognition
of monosyllabic monomorphemic words. Journal of
Memory and Language, 53:496–512.

D. A. Balota and D. H. Spieler. 1998. The utility of
item-level analyses in model evaluation: A reply to
Seidenberg & Plaut (1998). Psychological Science.

D. A. Balota, M. J. Cortese, and M. Pilotti. 1999. Item-
level analyses of lexical decision performance: Re-
sults from a mega-study. In Abstracts of the 40th An-
nual Meeting of the Psychonomics Society, page 44.

D. Balota, M. Cortese, S. Sergent-Marshall, D. Spieler,
and M. Yap. 2004. Visual word recognition for
single-syllable words. Journal of Experimental Psy-
chology:General, (133):283316.

M. Banko and E. Brill. 2001. Mitigating the paucity of
data problem. Human Language Technology.

E. Bates, S. D’Amico, T. Jacobsen, A. Szkely, E. An-
donova, A. Devescovi, D. Herron, CC Lu, T. Pech-
mann, C. Plh, N. Wicha, K. Federmeier, I. Gerd-
jikova, G. Gutierrez, D. Hung, J. Hsu, G. Iyer,
K. Kohnert, T. Mehotcheva, A. Orozco-Figueroa,
A. Tzeng, and O. Tzeng. 2003. Timed picture nam-
ing in seven languages. Psychonomic Bulletin & Re-
view, 10(2):344–380.

BNC Consortium. 2007. The British National Corpus,
version 3 (BNC XML Edition). Distributed by Ox-
ford University Computing Services on behalf of the
BNC Consortium.

T. Brants and A. Franz. 2006. Web 1T 5-gram Version
1. Linguistic Data Consortium (LDC).

Kevyn Collins-Thompson and James P. Callan. 2004.
A language modeling approach to predicting reading
difficulty. In HLT-NAACL, pages 193–200.

H. Daumé III and J. Jagarlamudi. 2011. Domain
adaptation for machine translation by mining unseen
words. In Proceedings of the 49th Annual Meet-
ing of the Association for Computational Linguis-
tics: Human Language Technologies (ACL-HLT 11).

M. Davies. 2009. The 385+ million word corpus of
contemporary american english (19902008+): De-
sign, architecture, and linguistic insights. Inter-
national Journal of Corpus Linguistics, 14(2):159–
190.

J. Gao, J. Goodman, M. Li, and K. F. Lee. 2002. To-
ward a unified approach to statistical language mod-
eling for chinese. In Proceedings of the ACM Trans-
actions on Asian Language Information Processing
(TALIP 02).

J. Godfrey, E. Holliman, and J. McDaniel. 1992.
SWITCHBOARD: Telephone Speech Corpus for
Research and Development. In Proceedings of
ICASSP-92, pages 517–520.

A. Kilgarriff and G. Grefenstette. 2003. Introduction
to the special issue on the web as corpus. Computa-
tional Linguistics, 29(3):333–348.

H. Kucera and W.N. Francis. 1967. Computational
analysis of present-day american english. provi-
dence, ri: Brown university press.

R. Lew, 2009. Contemporary Corpus Linguistics,
chapter The Web as corpus versus traditional cor-
pora: Their relative utility for linguists and language
learners, pages 289–300. London/New York: Con-
tinuum.

S. C. Lin, C. L. Tsai, L. F. Chien, K. J. Chen, and
L. S. Lee. 1997. Chinese language model adapta-
tion based on document classification and multiple
domain-specific language models. In Proceedings
of the 5th European Conference on Speech Commu-
nication and Technology.

S.A. McDonald and R.C. Shillcock. 2003. Eye
movements reveal the on-line computation of lexical
probabilities during reading. Psychological science,
14(6):648–52, November.

R. C. Moore and W. Lewis. 2010. Intelligent selection
of language model training data. In Proceedings of
the 48th Annual Meeting of the Association for Com-
putational Linguistics (ACL 10).

M. Ramscar, P. Hendrix, C. Shaoul, P. Milin, and R. H.
Baayen. 2014. The myth of cognitive decline: non-
linear dynamics of lifelong learning. Topics in Cog-
nitive Science, 32:5–42.

D. H. Spieler and D. A. Balota. 1997. Bringing com-
putational models of word naming down to the item
level. 6:411–416.

L. Wang, D.F. Wong, L.S. Chao, Y. Lu, and J. Xing.
2014. A systematic comparison of data selection
criteria for smt domain adaptation. The Scientific
World Journal.

12


