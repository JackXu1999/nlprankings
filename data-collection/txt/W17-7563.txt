



















































Proceedings of the...


S Bandyopadhyay, D S Sharma and R Sangal. Proc. of the 14th Intl. Conference on Natural Language Processing, pages 523–532,
Kolkata, India. December 2017. c©2016 NLP Association of India (NLPAI)

Semantic Enrichment Across Language: A Case Study of Czech
Bibliographic Databases

Pavel Smrz
Brno University of Technology

Faculty of Information Technology
Bozetechova 2, 612 66 Brno, Czechia

smrz@fit.vutbr.cz

Lubomir Otrusina
Brno University of Technology

Faculty of Information Technology
Bozetechova 2, 612 66 Brno, Czechia
iotrusina@fit.vutbr.cz

Abstract

This paper deals with semantic enrichment
of textual resources by means of automat-
ically generated named entity recognizers-
linkers and advanced indexing and search-
ing mechanisms that can be integrated into
various information retrieval and informa-
tion extraction systems. It introduces a
new system transforming Wikipedia and
other available sources into task-specific
knowledge bases and employs contex-
tual information to build state-of-the-art
entity disambiguation components. Al-
though some components are language-
dependent (for example, that responsible
for the morphology analysis or the seman-
tic role identification), they can be eas-
ily replaced by existing tools providing
specific functions. As a case study, we
demonstrate an instantiation of the sys-
tem for the task of semantic annotation of
Czech bibliographic databases in the con-
text of the CPK project. We particularly
stress the role of problem-specific knowl-
edge sources that can be easily integrated
into our system and play a key role in the
success of the tool in real applications.

1 Introduction

Semantic enrichment of textual resources is a
well-studied field with applications in many dif-
ferent domains, such as news, market analysis,
environmental studies or cultural heritage. Vari-
ous general-purpose tools have appeared in recent
years (some of them are discussed in the next sec-
tion). Existing systems can often recognize basic
entities and provide a link to a knowledge base
(KB), but they usually lack additional information
on entities that is critical for specialised applica-

tions. Indeed, it is not guaranteed that a referred
KB entry contains complete and pertinent infor-
mation, such as task-relevant entity (sub-)type and
attributes.

Moreover, it is almost impossible to re-purpose
or re-target existing semantic enrichment systems
to a new domain or a new context or to adapt and
extend it for another language. Clearly, tools for
specific domains call for an integration of specific
information sources. For example, given a per-
son, a bibliographical system needs information
about publications written by him or her, or a list
of publications mentioning the person. Although
domain-specific knowledge can substantially im-
prove results of entity disambiguation, it is very
difficult to make the existing systems use such in-
formation.

The ultimate goal of the work reported in this
paper is to build a state-of-the-art semantic en-
richment system that will be more flexible than
existing tools and will need only a minimal ef-
fort to adapt to new environments and tasks. The
background KB is derived directly from Wikipedia
dumps and domain-specific knowledge sources.
Hence, it has always up-to-date information and
can benefit from recent updates (as opposed to,
e. g., DBpedia-based systems relying on the re-
source updated twice a year which is usually built
from Wikipedia versions several months old).

The rest of this paper is structured as follows.
The next section surveys existing systems for se-
mantic enrichment and points out differences in
the approach they follow. Section 3 deals with the
process of KB creation and compares results of the
developed method with alternative solutions. Sec-
tion 4 describes a case study of the semantic en-
richment of Czech texts and bibliographic meta-
data. Last section concludes our work and dis-
cusses future directions of our research.523



2 Related Work

The need to bridge the semantic gap between
the semi-structured “web of documents” and the
structured “web of knowledge” (Buitelaar and
Cimiano, 2008) has led to the development of var-
ious semantic enrichment systems in recent years.
Named entity recognition (NER), linking (NEL)
and disambiguation (NED) present a key compo-
nent of the process of semantic enrichment.

Tools such as DBpedia Spotlight (Daiber et
al., 2013), Illinois Wikifier (Ratinov et al., 2011),
AIDA (Hoffart et al., 2011), or Babelfy (Moro et
al., 2014) enable annotating mentions of named
entities in a plain text and “anchoring” the anno-
tations in linked open data resources (most fre-
quently in DBpedia/Wikipedia).

State-of-the-art NED methods have to cope with
trade-offs among output accuracy, run-time effi-
ciency and scalability. Fast methods, like Illinois
Wikifier or DBpedia Spotlight use relatively sim-
ple contextual features. These methods perform
well on standard texts that deal with prominent
entities, but their accuracy is rather low on more
complex inputs with highly ambiguous names.
On the other hand, sophisticated systems, such
as AIDA or Babelfy, rely on rich contextual fea-
tures, such as key phrases and joint-inference algo-
rithms. Consequently, they tend to be rather slow.

DBpedia Spotlight is a system for automati-
cally annotating text documents with DBpedia1

URIs. Its disambiguation algorithm is based on
the cosine similaritiy and a modification of TF-
IDF weights. The main phrase spotting algorithm
relies on exact string matching, which uses Ling-
Pipes 2 Aho-Corasick implementation.

Illinois Wikifier combines local clues and
global coherence of the joint cross-linking assign-
ment by analysing Wikipedia link structure and es-
timating pairwise article relatedness. It aims at
linking all possible concepts to their correspond-
ing Wikipedia pages.

AIDA employs the YAGO knowledge base3 as
an entity catalogue and a source of entity types and
semantic relationships among entities. It uses co-
occurrence information obtained from large, syn-
tactically parsed corpora as a similarity measure.

1http://dbpedia.org/
2http://alias-i.com/lingpipe/
3https://www.mpi-inf.

mpg.de/departments/
databases-and-information-systems/
research/yago-naga/yago/

The AIDA system provides an integrated NED
method using popularity, similarity, and graph-
based coherence. AIDA-light (Nguyen et al.,
2014) is a lightweight version of AIDA. It is a
complete system for NED, which is orders of mag-
nitude faster than AIDA while achieving compara-
ble output quality.

Babelfy provides a unified approach to word
sense disambiguation and entity linking. It is
knowledge-based and exploits semantic relations
between word meanings and named entities from
BabelNet4 (Navigli and Ponzetto, 2012) – a multi-
lingual semantic network consisting of more than
13 million concepts and named entities in 271 lan-
guages. It is based on a loose identification of
candidate meanings (substring matching instead of
exact matching) coupled with a densest subgraph
heuristic which selects high-coherence semantic
interpretations.

Czech named entity recognition has become a
well-established field as well. In (Straková et
al., 2016), authors present a completely feature-
less, language-agnostic named entity recognition
system. The system uses only surface forms of
words, lemmata and tags as its input. Despite
that, it surpasses the precision of current state-of-
the-art Czech NER systems, which use manually
designed rule-based classification features, such
as first character capitalization, existence of spe-
cial characters in the word, or regular expressions
designed to reveal particular named entity types.
The system is based on artificial neural networks
with parametric rectified linear units, word embed-
dings and character-level embeddings, which do
not need manually designed classification features
or gazetteers.

Ni and Florian (2017) proposed a new class
of approaches that utilize Wikipedia entity type
mappings to improve multilingual NER systems.
They apply a maximum entropy classifier on En-
glish Wikipedia to construct an entity type map-
ping. To build multilingual mappings, they use
the inter-language links of Wikipedia. The sys-
tem has a fine-grained entity type set containing
51 types (such as person, organization, location,
title work, facility, event, date, time. . . ). They use
both – structured information, such as title and in-
fobox, and unstructured information, such as text
in a Wikipedia page, as features. The classifier
trained with all the features identifies the correct

4http://babelnet.org/524



category for English with overall F1 score of 90.1.
Their approach improved the baseline NER sys-
tems for English, Portuguese, Japanese, Spanish,
Dutch and German.

Various evaluation campaigns have also re-
cently appeared that compare quality of the NE
annotation on collected datasets. Initiatives such
as NIST TAC KBP5 – Knowledge Base Popula-
tion – Entity Discovery and Linking Track (Ji et
al., 2015), NEEL6 – Named Entity rEcognition
and Linking Challenge on Microposts (Rizzo et
al., 2015), or ERD7 – Entity Recognition and Dis-
ambiguation Challenge (Carmel et al., 2014) rank
participating systems based on their overall perfor-
mance on collections of specific textual fragments
(selected sentences, tweets. . . ) that had been man-
ually annotated. As the manual dataset prepara-
tion is tedious, the provided training and test data
is usually limited to few thousands of entity men-
tions. Developers of NER tools can measure im-
provements in annotation quality w.r.t. a particular
available dataset or they can use specific bench-
marking frameworks such as NERD (Rizzo and
Troncy, 2011) or Gerbil (Cornolti et al., 2013),
embracing several datasets.

3 Knowledge Base Creation

As mentioned above, we aim at creating a domain-
specific knowledge-reference store with the high-
est possible coverage of entities and specific at-
tributes relevant for a task in hand. Due to its lim-
ited applicability across languages and across con-
texts, we cannot simply apply the approach fol-
lowed by the DBpedia extraction team (Lehmann
et al., 2015). Indeed, specific hand-crafted extrac-
tion patterns that are often based on Wikipedia fea-
tures rare in some languages (e.g., Infoboxes in the
case of the Czech Wikipedia) are not easily adapt-
able to our purposes. On the other hand, we do not
want to ignore the Wikipedia structure as a key
indicator of entity grouping (and types) and rely
solely on pure natural language processing (NLP)
of entity definitions. The NLP approach is gener-
ally difficult to transfer from one language to an-
other and its success hinges on the discipline au-
thors of Wikipedia articles follow when creating
the content.

5http://nlp.cs.rpi.edu/kbp/
6http://www.lancaster.ac.uk/

Microposts2015
7http://web-ngram.research.microsoft.

com/ERD2014/

As opposed to manual approaches, our method
employs a straightforward learning technique that
capitalizes on the most frequent Wikipedia fea-
tures in each individual language (most frequently,
categories and lists a Wikipedia page is assigned
to). The learning process involves two steps of the
extraction – identification of entities (more pre-
cisely, Wikipedia articles / domain-specific source
entries referring to a specific entity) and slot fill-
ing of attribute values relevant to a particular task.
The overall schema of the Knowledge Base Cre-
ation component is presented in Figure 1. Let us
focus on the initial step first.

The system enables users to specify a ba-
sic set of entity types to be recognized (general
ones, such as person, location, event, or domain-
specific, for example, visual artists related to a par-
ticular place). It then expects a set of examples of
(prototypical) representatives of each type. It is
also possible to extend the input by negative ex-
amples, e.g., entities that are known to be often
misrecognized as belonging to a given entity type.
The positive examples can be either extracted from
other sources such as existing lists of entities of
interest, or they can be identified by the user. If
the entity type exists in a knowledge source in an-
other language in which entities have been already
identified, the system can also take advantage of
inter-language links and consider all entities of a
given type linked to the language as positive ex-
amples for the automatic extraction process. Note
that inter-language links are treated as features of
the automatic technique described and evaluated
below so that one can directly see what value they
bring as examples of specific entity types.

Even if the initial set of examples is limited, our
method can find a significant portion of all entities
of the given type. This can be demonstrated by
the results in Table 1 showing the numbers of ex-
amples necessary to reach the coverage of 90 % or
higher for selected types in English Wikipedia. It
is clear that less than 20 examples usually suffice
to cover almost all entities of particular types so
that even fully manual instantiation of the system
does not present a tedious job.

If no negative examples are explicitly provided,
the system uses entities of all other types as nega-
tive examples for a current type, while considering
mutual exclusivity constraints that can be specified
by the user (all categories are considered mutually
exclusive by default). Depending on the complex-525



Figure 1: Structure of the KB creation.

Entity type Min. numberof Examples Coverage

Beverage 10 90.0 %
Musical Work 20 90.3 %
Video Game 15 96.9 %

Table 1: The numbers of examples necessary to
reach the coverage of 90 % or higher for selected
types in English Wikipedia.

ity of the type, the filtration based on negative ex-
amples can bring significant improvements to the
automatic extraction.

When dealing with Wikipedia, the learning al-
gorithm explores six types of features:

1. a particular category is assigned to an article;

2. an entity forms a part of a list;

3. a Wikipedia page contains a specific infobox;

4. the key word / phrase of the definition (the
first sentence of an article) corresponds to a
given list;

5. the first sentence corresponds to a given pat-
tern (for example, an asterisk before a date in
parenthesis);

6. the article has an equivalent in a language
where the type corresponds to the expected
one.

The system generates hypotheses based on pro-
vided examples (for example, “all entities in cate-
gory American Merchants have type Person) and
evaluates their relative value. A simple logit model
is used to combine the features contributing the
most. To find an optimal combination of features,
standard measures of the association rule mining
are used – support, confidence, lift, and convic-
tion (Bayardo Jr and Agrawal, 1999). Support in-
dicates how frequently an individual feature or a
set of features (a feature set) appears in Wikipedia.
The confidence measures how often a feature set
truly correlates with an entity type, i.e., the pro-
portion of articles corresponding to a feature set F
which are known to correspond to type t. The lift
is defined as the ratio of the observed support to
that expected if F and t were independent. The526



conviction can be interpreted as the frequency that
the feature set makes an incorrect prediction of the
entity type – the ratio of the expected frequency
that F occurs with entities of other types than t.

Let symbol 7→ denote the mapping of a feature
set to an entity and E be the set of all entities (or,
more precisely, all articles that can deal with an
entity). The above-mentioned measures can be
formally defined as:

supp(F ) =
|{e ∈ E;F 7→ e}|

|E|

conf(F ⇒ T ) = supp(F ∪ T )
supp(F )

lift(F ⇒ T ) = supp(F ∪ T )
supp(F )× supp(T )

conv(F ⇒ T ) = 1− supp(T )
1− conf(F ⇒ T )

Table 2 show examples of hypotheses generated
by the system for person identification from En-
glish Wikipedia along with their support, confi-
dence, lift, and conviction values.

The entity type presents just the initial attribute
the automatic system extracts from Wikipedia
and/or domain-specific knowledge sources. The
user can define a full set of additional attributes
corresponding to the entity type that are relevant
for a given task. The attributes can reflect the
structure of a known template for a given entity
type (for example, attributes of a Wikipedia in-
fobox or all potential relations an entity of the
type can have in DBpedia). On the other hand,
the attribute can be also specific to a given con-
text. For example, our previous work in the cul-
tural heritage domain (Smrz et al., 2013) utilised
attributes of art influences and travel experiences
of visual artists that could not be directly mapped
to a pre-identified relation in DBpedia/Wikipedia
infoboxes.

Existing extraction frameworks provide a very
limited quality in terms of the completeness of
attributes being correctly filled based on infor-
mation contained in the source. To reach a
high coverage and precision, the system applies
a learning approach again. Feature detection
does not work with the textual content directly.

It rather deals with word embeddings (a combi-
nation of Word2Vec (Mikolov et al., 2013) and
GLOVE (Pennington et al., 2014) vectors) and
generalizes the structure of textual fragments cor-
responding to attribute-filling examples provided
by the user.

The resulting knowledge base can be further
supplemented by additional information necessary
for the actual identification of entities in text,
unique identification of entities and their linking
to other authoritative knowledge sources, disam-
biguation information and entity visual represen-
tation (if available). As discussed in the follow-
ing section, entity mentions are matched in text
by means of a finite-state technology. The KB
should contain all alternative names that can re-
fer to an entity and all forms of names one can
expect in the text. For the KB generated from En-
glish Wikipedia, we collect and process all alter-
native names (redirections) and generate known
variants of the name forms (for example, short-
ened or omitted middle names). We also use inter-
language links to record language variants of en-
tity names and consider transcription to charac-
ters without diacritics (through Unicode equiva-
lent classes) and transliterations. The generation
of name forms in a morphologically rich language
(Czech) is discussed in the next section.

We store and later index entity identifiers in
their original sources and known interlinks to
other LOD (Linked Open Data) resources (for ex-
ample, Wikipedia URI in both forms with numer-
ical article IDs as well as titles, links to Freebase,
DBpedia, etc.). To be able to correctly assign the
KB link for an entity with an ambiguous name, we
store a vector characterizing words and multiword
expressions appearing with an actual meaning of
each name. The disambiguation algorithm then
reads this data and classifies the entity according
to an observed context. If a Wikipedia article re-
ferring to an entity contains an image / images, we
store this information to be able to represent the
entity in a visual way. Figure 2 demonstrates one
form of visualization based on such KB.

As already mentioned, the KB resulting from
our system covers significantly more entities (cor-
rectly assigns the type to more entities) than that
of alternative solutions. This is true not only for
specific types and less frequent languages such as
Czech (see the next section) but also for standard
entity types and attributes in English. Table 3 com-527



hypotheses support confidence lift conviction
all entities in category ending with
births have type Person

23.22 99.51 400.08 15581

all entities whose page contains section
History don’t have type Person

10.17 99.52 132.47 5196

all entities in category ending with
deaths have type Person

10.79 99.81 401.27 40540

all entities in category starting with
People have type Person

11.10 99.34 399.40 11536

all entities whose page contains tem-
plate coord don’t have type Person

6.48 99.97 133.07 100933

Table 2: Examples of hypotheses for person identification from English Wikipedia.

Figure 2: Visualization of an entity.

pares numbers of entities of common general types
that can be found using a SPARQL query (often
rather a complicated one) in the current DBpedia
and the corresponding numbers resulting from our
system. It is obvious that if a task requires high
precision, it cannot easily rely only on the types
identified by DBpedia.

4 Semantic Enrichment of Czech
Bibliographic Databases

This section presents a case study of the semantic
enrichment system applied to Czech bibliographic
databases. It forms a part of our work in a large na-
tional project – CPK: Using Semantic Technolo-

type number of entities
in DBpedia in our KB

person 1,348,346 1,624,602
place 806,418 738,328
organization 253,215 606,712
video game 20,910 44,862

Table 3: Numbers of entities of common types that
can be found using a SPARQL query in the current
DBpedia and the corresponding numbers resulting
from our system.

gies to Access Cultural Heritage Through the Cen-
tral Portal of Czech Libraries. The primary ob-528



jective of the project is to research and develop
methods leading to a significantly improved ac-
cess to cultural heritage available in Czech li-
braries as well as applying results of this research
to the creation of the Central portal of Czech li-
braries (CPK), using advanced semantic technolo-
gies. The portal is to be developed in the form
of a technological system combining existing and
newly developed component applications and a
specialized public database (index) of resources
integrated into the portal. CPK indexes not only
metadata records from contributing institutions,
but also full texts of documents digitized by li-
braries and other information sources.

The fulltexts Czech libraries collect and provide
to readers include historical as well as contem-
porary newspapers and other periodicals, mono-
graphs, and various other material of a cultural
value. As opposed to bibliographic records as-
sociated with monographs that generally contain
subject references, including links to authorita-
tive entity knowledge bases (discussed in detail
below), the search in the content of newspapers
cannot take advantages of the semantically anno-
tated metadata (only a simple fulltext search is
supported). That is why our work primary fo-
cuses on the available content of periodicals and
its automatic semantic enrichment. Note that it is
expected that later phases of the project will ex-
tend this towards particular categories of mono-
graphs (such as historical non-fiction and biogra-
phies) and will scale-up the techniques developed
for periodicals to a wide range of the cultural her-
itage artefacts.

Most of the modules integrated into the process-
ing pipeline for the described use case can be eas-
ily adapted and used for the same task in any lan-
guage. We specifically identify the parts that are
language-dependent and that need to be replaced
by a tool or a resource tailored to the task when the
system is to be transferred to another language.

The discussed use case also builds on task-
specific knowledge sources. The Czech libraries
work with a collection of controlled vocabularies
and thesauri, referred to as National Authorities
(or simply Authorities), that include named enti-
ties (persons, geographical entities, events, etc.).
The Authorities are uniquely identified, they pro-
vide an official- as well as alternative names,
a brief description of the entity, and a link to
the Czech Wikipedia (if available). In addition

to the information that could be extracted from
Wikipedia, the knowledge source can be also used
to identify all monographs that deal with particular
entities (either in their fulltext form or, at least, as
a metadata record listing the title, authors, other
subject references, etc.). For the entries corre-
sponding to people, the works authored by a given
person can be also identified (currently, only titles
and classifications of the work are considered in
such cases).

It is critical to recognize that the task-specific
knowledge source can bring invaluable quality to
the semantic enrichment process and can enable
extracting entities and relations not be covered by
Wikipedia and other general-purpose resources.
This can be demonstrated by the statistics summa-
rized in Table 4. It is obvious that monographs
in Czech libraries refer to many entities not ad-
dressed by the Czech Wikipedia. To be able to rec-
ognize mentions of relevant entities in Czech peri-
odicals and monograph, one cannot simply reckon
on Wikipedia only.

A back side of the integration of domain-
specific resources lies in a significant increase of
ambiguity of names. While there are only 7.1 % of
ambiguous person names in the Czech Wikipedia,
the knowledge base combining this resource with
all Authority files has the ambiguity of 13.3 %.
Fortunately, the links to metadata records and full-
texts of monographs dealing with referred Au-
thorities can provide additional learning contexts
for the entity disambiguation process. This is
also true for entities covered by both Wikipedia
and national Authorities so that the additional
data actually improves the quality of entity disam-
biguation models. Indeed, combined Wikipedia
pages, referred web links and all available Wik-
iLinks (Otrusina and Smrz, 2016) are not usually
a match for an entire book dedicated to a person,
location, event, or other type of entities.

Czech is a morphologically-rich (inflectional)
language. It is not easy to generate all poten-
tial forms in which entity names can appear in
text. For example, nouns and adjectives distin-
guish 7 cases (sometimes with 2 forms per case)
in singular and 7 cases in plural and multi-word
names of entities come in various forms with com-
plex agreement rules (e.g., genitive phrases with
prepositional groups). Moreover, not all parts of
multi-word names are capitalized (for example,
the Czech Republic would be Česká republika in529



type number of entities covered by
in national authorities Czech Wikipedia

person 629,122 14,758 (2.35 %)
place 29,041 3,386 (11.65 %)

Table 4: Entities from Czech Authorities covered by Czech Wikipedia.

Czech) so that recognition of the boundary of an
unknown (new) name can be complicated. The
generation of various morphological forms of en-
tity names is clearly language-dependent.

The generation of the name forms was divided
into two steps. First, we identified all single-word
names and single-string parts of multi-word names
that were not covered by an existing morpholog-
ical database/analyser. We employed a statisti-
cal learning method that considers various mor-
phological features (esp. word endings and fre-
quency of similar word paradigms) and estimates
the most probable morphological paradigm ac-
cording to which the word would decline. We then
generated all potential wordforms and correspond-
ing morphological tags for single-word names and
prepared the same for filtering multi-word combi-
nations by means of group agreement rules.

The second step dealt with multi-word entity
names only. Known names served as training
examples for statistical name-structure prediction
method. It showed to be beneficial to recognize
not only the structures necessary for name-form
generation, but also to estimate precise meaning
of the name components in this phase. The algo-
rithm was trained to distinguish the most probable
given names from surnames, titles and name speci-
fiers (such as the younger), numerical components
of the names (WWI), etc. This was critical for or-
ganizing the names into hierarchies as well as for
generating variants with name initials and short-
ened forms, acronym matching, etc.

The process of semantic enrichment of the
Czech text is realized in the form of a pipeline
combining various text- and language analysing
tools and task-tailored classifiers. The input can
be provided as a plain text (for example, the con-
tent of historical periodicals obtained by means
of OCR) or in the HTML form. We employ
CLD3 8 for language identification (currently fil-
tering out all non-Czech documents) and JustText
(Pomikálek, 2013) to identify and eliminate po-
tential boilerplate in HTML Depending on the fur-

8https://github.com/google/cld3

ther planned processing, the text can be tokenized
and verticalized in this phase and task-relevant
links from the HTML can be stored to be con-
sidered later too. To enable linguistically-oriented
search in the indexed material, the text can be also
lemmatized, PoS tagged and parsed. The rele-
vant tools can be easily plugged-in as well as re-
placed if necessary. We take advantage of UD-
pipe (Straka and Straková, 2017) – a linguistic
processing pipeline providing results in various
languages.

The actual entity recognition and disambigua-
tion is realized by the SEC component developed
by our team (Dytrych and Smrz, 2016). The tool
is publicly available9 and can be easily instanti-
ated for other contexts and new languages. It takes
the knowledge base (in the form described the
previous section) and extracts all potential entity
names (a primary reference name, all alternative
names, morphological forms, and generated vari-
ants). A minimum finite-state automaton is con-
structed from all the name strings and correspond-
ing indices in the knowledge base by means of an
algorithm described in (Daciuk et al., 2000). The
SEC recognizer is thus able to associate a textual
fragment that corresponds (can correspond) to a
name with all potential KB entries it can refer to.
Using the information stored in the KB (for ex-
ample, statistics on Wikipedia article popularity)
it can also provide prior probabilities of the poten-
tial entity linking. The SEC can also plug-in other
entity disambiguation modules that take advan-
tage of additional contextual information stored in
the KB. The current version for Czech combines
a rule-based disambiguation module implement-
ing strict application-specific restrictions (for ex-
ample, preferring entities covered by national Au-
thority files) and a learning-based disambiguator
trained on all the relevant material available in the
CPK project. To enable users to easily extract se-
mantic relations and to search the resource seman-
tically, the SEC engine can also incorporate coref-
erence resolution tools, semantic role labellers and

9http://sec.fit.vutbr.cz/sec/530



other language processing components.

5 Conclusions and Future Directions

The semantic enrichment system introduced in
this paper stresses the easiness of system cus-
tomization and transfer to other languages. We
showed that it is worth to pay attention to prepara-
tion of the knowledge base component generated
from Wikipedia and domain-specific resources.
The presented learning-based extraction system
leads to better coverage and generates data di-
rectly applicable in the entity recognition and link-
ing task. Moreover, entity types and attributes
are not “hardwired”, they are fully defined by
application needs. Results of the case study on
semantic enrichment of Czech texts collected in
the CPK project further demonstrate that existing
knowledge-reference systems bring a significant
value to the semantic enrichment process and that
a system based just on general-purpose knowledge
sources would not satisfy the need of specific ap-
plications.

There are several directions of research we will
explore in our future work and apply in the CPK
project. We are going to extend interlinks with
DBpedia, KBpedia, and other resources and take
advantages of established hierarchical structures
(ontologies) to initialize the set of prototypical
attributes in order to simplify the task of type-
specific attribute definition. We will also inte-
grate an advanced classifier of specific entity types
and attributes that are not covered by the KB.
Less known people, places, events and other en-
tities are often introduced in newspaper texts (usu-
ally the first time they appear) so that the system
could suggest extensions to the national Authority
database adding the most cited new entities. As
for the search interface, a lot needs to be done to
make the system more intuitive and accessible to
laymen.

Acknowledgments

The work reported in this paper has been
supported by the Ministry of Culture of the
Czech Republic, programme NAKI II, project
DG16P02R006 CPK: Using Semantic Technolo-
gies to Access Cultural Heritage Through the Cen-
tral Portal of Czech Libraries.

References
Roberto J Bayardo Jr and Rakesh Agrawal. 1999. Min-

ing the most interesting rules. In Proceedings of
the fifth ACM SIGKDD international conference on
Knowledge discovery and data mining, pages 145–
154. ACM.

Paul Buitelaar and Philipp Cimiano. 2008. Ontology
learning and population: bridging the gap between
text and knowledge, volume 167. IOS Press.

David Carmel, Ming-Wei Chang, Evgeniy Gabrilovich,
Bo-June Paul Hsu, and Kuansan Wang. 2014.
ERD’14: Entity recognition and disambiguation
challenge. In ACM SIGIR Forum, volume 48, pages
63–77. ACM.

Marco Cornolti, Paolo Ferragina, and Massimiliano
Ciaramita. 2013. A framework for benchmarking
entity-annotation systems. In Proceedings of the
22nd International Conference on World Wide Web,
WWW ’13, pages 249–260, New York, NY, USA.
ACM.

Jan Daciuk, Stoyan Mihov, Bruce W Watson, and
Richard E Watson. 2000. Incremental construction
of minimal acyclic finite-state automata. Computa-
tional linguistics, 26(1):3–16.

Joachim Daiber, Max Jakob, Chris Hokamp, and
Pablo N. Mendes. 2013. Improving efficiency and
accuracy in multilingual entity extraction. In Pro-
ceedings of the 9th International Conference on Se-
mantic Systems (I-Semantics).

Jaroslav Dytrych and Pavel Smrz. 2016. Interaction
patterns in computer-assisted semantic annotation of
text - an empirical evaluation. In Proceedings of the
8th International Conference on Agents and Artifi-
cial Intelligence - Volume 2: ICAART,, pages 74–84.
INSTICC, SciTePress.

Johannes Hoffart, Mohamed Amir Yosef, Ilaria Bor-
dino, Hagen Fürstenau, Manfred Pinkal, Marc Span-
iol, Bilyana Taneva, Stefan Thater, and Gerhard
Weikum. 2011. Robust Disambiguation of Named
Entities in Text. In Conference on Empirical Meth-
ods in Natural Language Processing, EMNLP 2011,
Edinburgh, Scotland, pages 782–792.

Heng Ji, Joel Nothman, Ben Hachey, and Radu Florian.
2015. Overview of TAC-KBP2015 tri-lingual entity
discovery and linking.

Jens Lehmann, Robert Isele, Max Jakob, Anja
Jentzsch, Dimitris Kontokostas, Pablo N Mendes,
Sebastian Hellmann, Mohamed Morsey, Patrick
Van Kleef, Sören Auer, et al. 2015. Dbpedia–a
large-scale, multilingual knowledge base extracted
from wikipedia. Semantic Web, 6(2):167–195.

Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Cor-
rado, and Jeff Dean. 2013. Distributed representa-
tions of words and phrases and their compositional-
ity. In Advances in neural information processing
systems, pages 3111–3119.531



Andrea Moro, Alessandro Raganato, and Roberto Nav-
igli. 2014. Entity Linking meets Word Sense Dis-
ambiguation: a Unified Approach. Transactions
of the Association for Computational Linguistics
(TACL), 2:231–244.

Roberto Navigli and Simone Paolo Ponzetto. 2012.
BabelNet: The automatic construction, evaluation
and application of a wide-coverage multilingual se-
mantic network. Artificial Intelligence, 193:217–
250.

Dat Ba Nguyen, Johannes Hoffart, Martin Theobald,
and Gerhard Weikum. 2014. Aida-light:
High-throughput named-entity disambiguation. In
LDOW.

Jian Ni and Radu Florian. 2017. Improving multilin-
gual named entity recognition with wikipedia entity
type mapping. arXiv preprint arXiv:1707.02459.

Lubomir Otrusina and Pavel Smrz. 2016. Wtf-
lod - a new resource for large-scale ner evaluation.
In Nicoletta Calzolari (Conference Chair), Khalid
Choukri, Thierry Declerck, Sara Goggi, Marko Gro-
belnik, Bente Maegaard, Joseph Mariani, Helene
Mazo, Asuncion Moreno, Jan Odijk, and Stelios
Piperidis, editors, Proceedings of the Tenth Interna-
tional Conference on Language Resources and Eval-
uation (LREC 2016), Paris, France, may. European
Language Resources Association (ELRA).

Jeffrey Pennington, Richard Socher, and Christopher
Manning. 2014. Glove: Global vectors for word
representation. In Proceedings of the 2014 confer-
ence on empirical methods in natural language pro-
cessing (EMNLP), pages 1532–1543.

J Pomikálek. 2013. justext: Heuristic based boiler-
plate removal tool. Available: Google code, online
http://code. google. com/p/justext.

Lev Ratinov, Dan Roth, Doug Downey, and Mike
Anderson. 2011. Local and global algorithms
for disambiguation to wikipedia. In Proceedings
of the 49th Annual Meeting of the Association
for Computational Linguistics: Human Language
Technologies-Volume 1, pages 1375–1384. Associ-
ation for Computational Linguistics.

Giuseppe Rizzo and Raphaël Troncy. 2011. Nerd:
evaluating named entity recognition tools in the web
of data.

Giuseppe Rizzo, Amparo E Cano, Bianca Pereira, and
Andrea Varga. 2015. #microposts2015 named en-
tity recognition & linking challenge. In 5th Interna-
tional Workshop on Making Sense of Microposts.

Pavel Smrz, Lubomir Otrusina, Jan Kouril, and
Jaroslav Dytrych. 2013. Decipher deliverable
D4.3.1: Semantic Annotator. Technical report, Brno
University of Technology, Faculty of Information
Technology.

Milan Straka and Jana Straková. 2017. Tokenizing,
pos tagging, lemmatizing and parsing ud 2.0 with
udpipe. In Proceedings of the CoNLL 2017 Shared
Task: Multilingual Parsing from Raw Text to Univer-
sal Dependencies, pages 88–99, Vancouver, Canada,
August. Association for Computational Linguistics.

Jana Straková, Milan Straka, and Jan Hajič. 2016.
Neural networks for featureless named entity recog-
nition in czech. In International Conference on Text,
Speech, and Dialogue, pages 173–181. Springer.

532


