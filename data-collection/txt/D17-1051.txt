



















































Identifying Humor in Reviews using Background Text Sources


Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 492–501
Copenhagen, Denmark, September 7–11, 2017. c©2017 Association for Computational Linguistics

Identifying Humor in Reviews using Background Text Sources

Alex Morales and ChengXiang Zhai
Department of Computer Science

University of Illinois, Urbana-Champaign
amorale4@illinois.edu
czhai@illinois.edu

Abstract

We study the problem of automatically
identifying humorous text from a new
kind of text data, i.e., online reviews.
We propose a generative language model,
based on the theory of incongruity, to
model humorous text, which allows us to
leverage background text sources, such as
Wikipedia entry descriptions, and enables
construction of multiple features for iden-
tifying humorous reviews. Evaluation of
these features using supervised learning
for classifying reviews into humorous and
non-humorous reviews shows that the fea-
tures constructed based on the proposed
generative model are much more effective
than the major features proposed in the ex-
isting literature, allowing us to achieve al-
most 86% accuracy. These humorous re-
view predictions can also supply good in-
dicators for identifying helpful reviews.

1 Introduction

The growth of online feedback systems, such as
online reviews in which users can write about their
preferences and opinions, has allowed for creativ-
ity in the written communication of user ideas. As
such, these feedback systems have become ubiq-
uitous, and it’s not difficult to imagine a future
with smart systems reacting to user’s behaviour
in a human-like manner (Nijholt, 2014). An es-
sential component for personal communication is
the expression of humor. Although many peo-
ple have studied the theory of humor, it still re-
mains loosely defined (Ritchie, 2009), this leads
to difficulties in modelling humor. While the task
for identifying humor in text has been previously
studied, most approaches have focused on shorter
text such as Twitter data (Mihalcea and Strappar-

ava, 2006; Reyes et al., 2012, 2010) (see Section 6
for a more complete review of related work). In
this paper, we study the problem of automatically
identifying humorous text from a new kind of text
data, i.e., online reviews. In order to quantitatively
test whether the review is humorous, we devise a
novel approach, using the theory of incongruity, to
model the reviewer’s humorous intent when writ-
ing the review. The theory of incongruity states
that we laugh because there is something incon-
gruous (Attardo, 1994), in other words, there is a
change from our expectation.

Specifically, we propose a general generative
language model to model the generation of humor-
ous text. The proposed model is a mixture model
with multinomial distributions as component mod-
els (i.e., models of topics), similar to Probabilis-
tic Latent Semantic Analysis (Hofmann, 1999).
However, the main difference is that the compo-
nent word distributions (i.e., component language
models) are all assumed to be known in our model,
and they are designed to model the two types of
language used in a humorous text, including 1) the
general background model estimated using all the
reviews, and 2) the reference language models of
all the topical aspects covered in the review that
capture the typical words used when each of the
covered aspects is discussed. Thus the model only
has the parameters indicating the relative cover-
age of these component language models. The
idea here is to use these parameters to assess how
well a review can be explained by collectively by
the reference language models corresponding to
all the topical aspects covered in the review, which
are estimated using an external text source (e.g.,
Wikipedia).

We construct multiple features based on the
generative model and evaluate them using super-
vised learning for classifying reviews into humor-
ous and non-humorous reviews. Experiment re-

492



sults on a Yelp1 review data set show that the fea-
tures constructed based on the proposed generative
model are much more effective than the major fea-
tures proposed in the existing literature, allowing
us to achieve almost 86% accuracy. We also exper-
imented with using the results of humorous review
prediction to further predict helpful reviews, and
the results show that humorous review prediction
can supply good indicators for identifying helpful
reviews for consumers.

2 Referential Humor and Incongruity

In this section we describe some observations in
our data that have motivated our approach to solv-
ing the problem. In particular, we show that hu-
morous reviews tend to reference aspects which
deviate from what is expected. That is, in funny re-
views, the authors tend to use referential humor, in
which specific concepts or entities are referenced
to produce comedic effects, which we call aspects.
Here we define referential humor to be a humor-
ous piece of text which references aspects outside
of the typical context, in our case restaurant re-
views. For the rest of the paper we use humorous
and funny interchangeably.

Our study uses review data from Yelp. Yelp
has become a popular resource for identifying high
quality restaurants. A Yelp user is able to submit
reviews rating the overall experience of the restau-
rants. The reviews submitted to Yelp tend to have
similar context, in particular they mention several
aspects rating the quality of the restaurant such as
food, price, service and so on. This information is
expected from the reviewer in their review, how-
ever it is not always the case since there is no re-
quirement for writing the review. Yelp users are
able to vote for a review in several criterion, such
as funny, cool, and useful. This gives the users an
incentive for not only creating informative reviews
but possibly entertaining reviews.

In Figure 1, we show a humorous review, ran-
domly sampled by using our classifier with a high
probability of being funny, where the reviewer as-
serts that the food has extreme medicinal prop-
erties. The reviewer refers to “Nyquil” a com-
mon cold medicine to express the food’s incredible
ability to cure ailments. This appears almost sur-
prising since it would not normally be mentioned
in restaurants reviews. To identify the intended
humor, we can use the references the reviewer

1www.yelp.com

Figure 1: A funny review (left), with Kd = 3,
aspect topics (right) contain words in their corre-
sponding language model, probabilities removed
for clarity, the colored (bracketed) word corre-
spond to a different aspect assignment.

makes, e.g. Nyquil, as clues to what she is empha-
sising, e.g. the savory soondubu, by making such
comparisons, e.g. the heavenly taste and amazing
price. Yelp users seem to consider funny reviews
which tended to deviate from what was expected
into things which would seem out of place.

3 Language Models as a Proxy for
Incongruity

Motivated by the observations discussed in the
previous section (i.e., reviewers tend to reference
some entities which seem unexpected in the con-
text of the topic of the review), we propose a gen-
erative language model based on the theory of in-
congruity to model the generation of potentially
humorous reviews. Following previous work on
humor, we use the definition of incongruity in hu-
mor as “what people find unexpected” (Mihalcea
and Strapparava, 2006), where “unexpected” con-
cepts are those concepts which people do not con-
sider to be the norm in some domain, later we for-
malize unexpectedness using our model.

We now describe the proposed model in more
detail. Suppose we observe the following refer-
ences toKd topical aspectsAd = {r1, r2, ..., rKd}
in a review Rd = [w1, w2, ..., wNd ], where each ri
corresponds to an aspect reference (i.e. NyQuil in
our running example), andwi ∈ V , where V is the
vocabulary set. The model generates a word, for
some review, at a time, which talks about a specific
aspect or is related to the language used in Yelp
more broadly; we call the latter the background
language model. Thus a word is generated from a
mixture model, and its probability is an interpola-

493



Figure 2: Generation model for reviews, where
the dth review has Kd aspects in the review. The
shaded nodes here are the observed data and the
light node z are the latent variables corresponding
to aspect assignments.

tion of the background language and the language
of the references as shown in Figure 2.

These aspects provide some context to the un-
derling meaning of a review; the reviewers use
these aspects for creative writing when describ-
ing their dining experience. These aspects allow
us to use external information as the context, thus
we develop measures for incongruity addressing
the juxtaposition of the aspect’s context and the
review. The review construction process is repre-
sented in a generative model, see Figure 2, where
the shaded nodes represent our observations, we
have observed the words as well as the referenced
aspects which the reviewer has mentioned in their
review. The light nodes are the labels for the as-
pect which has generated the corresponding word.
Since the background language model, denoted by
θB , is review independent, we can simplify the
generative model by copying the background lan-
guage model for each review, thus we can focus on
the parameter estimation for each review in paral-
lel.

A key component to the success of our fea-
tures is the mesh of background text from external
sources, or background text sources, and the re-
views. In our example, Figure 1, Nyquil is a criti-
cal component for understanding the humor. How-
ever it is difficult to understand some references a
reviewer makes without any prior knowledge. To
do so, we incorporate external background knowl-
edge in the form of language models for the refer-
enced aspect present in the reviews. If the reviewer
has made Kd references to different aspects Ad in

review Rd, then for each ri there is a correspond-
ing language model θriw = P (w|θri) over the vo-
cabulary w ∈ V . For simplicity, we describe the
model for each document, and use the notation θiw
and θi for the corresponding language model of ri.

3.1 Incorporating Background Text Sources

As described before, some features we will use to
describe incongruity correspond to the weights of
the mixture model used to generate the words in
the review, which take into account the language
of the references she will make or allude, as shown
in Figure 2. The probability that an author will
generate a word w, for the dth review given corre-
sponding aspects Θ = {θB, θ1, ..., θKd}, is

P (w, d,Θ) =
Kd∑
z=0

P (w, z, d,Θ) =

Kd∑
z=0

P (w|z,Θ)P (z|d) = λθBw + (1− λ)
Kd∑
i=1

πiθ
i
w

Note Kd indicates the different aspects the re-
viewer will mention in a review, Rd, and hence
it can vary between reviews. θBw = P (w|z =
0,Θ) is the probability that the word will appear
when writing a review (e.g. background language
model) and θiw can be interpreted as word distri-
butions over aspect i. Here λ = P (z = 0|d) is
the weight for the background language model and

πi =
P (z = i|d)

1− P (z = 0|d) denotes the relative weights
of the referenced aspect’s language models used
in the review. We denote our parameters for re-
view Rd as ΛRd = {π1, ..., πKd , λ}. Note that
the parameter set varies depending on how many
references the review makes. In order to estimate
P (w|θi), we first need to find the aspects that the
user is mentioning in their reviews. In general as-
pects can be defined as any topics explicitly de-
fined in external background text data; in our ex-
periments we define aspects as Wikipedia entities.
In subsection 5.1, we describe one way of obtain-
ing these aspects, but first we describe the estima-
tion methodology.

3.2 Parameter Estimation

To estimate our parameters ΛRd , we would like to
maximize the likelihood of P (Rd), which is the
same as maximizing the log-likelihood of P (Rd).

494



That is

Λ̂ = argmaxΛ logP (Rd|Λ)
= argmaxΛ

∑
w∈V

c(w,Rd) log (P (w, d,Θ))

Here c(w,Rd) represents the number of occur-
rences of the word w in Rd. In order to maxi-
mize the log-likelihood we use the EM algorithm
(Dempster et al., 1977), to compute the update
rules for the parameters λ and π1, ...πKd . For the
E-Step, at the n+ 1th iteration we have

P (zw = 0) =
θBwλ

(n)(∑Kd
l=1 θ

l
wπ

(n)
l

)
(1− λ(n)) + θBwλ(n)

P (zw = j) =
θjwπ

(n)
j∑Kd

l=1 θ
l
wπ

(n)
l

Where zw is a hidden variable indicating whether
we have selected any of the aspect language mod-
els, or the background language model, when gen-
erating the word w. The update rules for the M-
Step are as follows:

λ(n) =
∑

w∈V c(w,Rd)P (zw = 0)
n

, π
(n)
j =∑

w∈V c(w,Rd)P (zw = j)(1− P (zw = 0))∑Kd
l=1

∑
w∈V c(w,Rd)P (zw = l)(1− P (zw = 0))

We ran EM until the parameters converged or a
small threshold was reached. Note there is some
similarity to other topic modelling approaches like
PLSA (Hofmann, 1999). PLSA is a way to soft
cluster the documents into several topics, in doing
so a word distribution for each topic is learned. In
our work we make the assumption that the “topics”
are fixed, namely they are the aspects which the
reviewer mentions in their review. Note that, we
can similarly derive update rules for an different
topic model such as LDA (Blei et al., 2003), how-
ever prior work, (Lu et al., 2011), shows that LDA
does not show superior performance over PLSA
empirically for a number of tasks.

4 Features construction

Since we are interested in studying discriminative
features for humorous and non-humorous reviews,
we set up a classification problem to classify a re-
view into either humorous or non-humorous. In
classification problems the data plays a critical
role; here the labels are obtained from the funny

votes in our Yelp dataset, and we describe how we
created the ground-truth in Section 5. Here in this
section, we discuss the new features we can con-
struct based on the proposed language model and
estimated parameter values.

4.1 Incongruity features

A natural feature in our incongruity model is the
estimated background weight, λ, since it indicates
how much emphasis the reviewer puts in their re-
view to describe the referenced aspects, we de-
note this feature by A1. Another feature is based
on the relative weights for the referenced aspect’s
language models. There tends to be more ‘sur-
prise’ in a review when the reviewer talks about
multiple aspects equally, this is because the more
topics the reviewer writes about the more intricate
the review becomes. We use the entropy of the
weightsH(Rd) = −

∑Kd
i=1 πi log πi as another in-

congruity score and label this feature as A2.

4.2 Unexpectedness features

Humor often relies on introducing concepts which
seem out of place to produce a comedic ef-
fect. Thus we want to measure this divergence
from the references and the language expected
in the reviews. Hence a natural measure is the
KL-divergence measure the distance between the
background language model and the aspect lan-
guage models. We use the largest deviation,
maxi{DKL(θi||θB)} as feature D2. For this fea-
ture we tried different combinations such as a
weighted average, but both features seemed to per-
form equally so we only describe one of them.

By considering the context of the references in
the reviews we can distinguish which statements
should be considered as humorous, thus we also
use the relative weight for each aspect to mea-
sure unexpectedness. Formally we have Uj =
πjDKL(θj ||θB), lastly we will denote maxi{Ui}
these set of features as U2.

4.3 Baseline features from previous work

For completeness, we also include a description of
all the baseline features used in our experiments;
they represent the state of the art in defining fea-
tures for this task. These features described be-
low do not use any external text sources (leverag-
ing external text sources is a novel aspect of our
work), and they are more contextual and syntacti-
cal based features. We describe some of the most

495



promising features, which have previously shown
to be useful in identifying humor in text.
Context features: Due to the popular success of
context features by Mihalcea and Pulman (2007)
we tried the following features content related fea-
tures: C1: the uni-grams in the review.2 C2:
length of the review. C3: average word length.
C4: the ratio of uppercase and lowercase charac-
ters to other characters in the review text.
Alliteration: Inspired by the success that Mihal-
cea and Strapparava (2006) had using the presence
and absence of alliteration in jokes, we developed
a similar feature for identifying funny reviews. We
used CMU’s pronunciation dictionary 3 to extract
the pronunciation to identify alliteration chains,
and rhyme chains in sentences. A chain is a con-
secutive set of words which have similar pronun-
ciation, for example if the words words “scenery”
and “greenery” are consecutive they would form
a rhyme chain. Similarly, “vini, vidi, visa” also
forms another chain this time an alliteration chain.
We used the review’s total number of alliteration
chains and rhyme chains and denote it by E1. Note
that there could be different lengths of chains, we
experimented with some variations but they per-
formed roughly the same, for simplicity we did not
describe them here.
Ambiguity: Ambiguity in word interpretation has
also been found to be useful in finding jokes. The
reasoning is that if a word has multiple interpreta-
tion it is possible that the author intended another
interpretation of the word instead of the more com-
mon one. We restricted the words in the reviews
to only nouns and used Wordnet 4 to extract the
synsets for these words. Then we counted the av-
erage number of synsets for each of these words,
finally we took the mean score for all the words in
the reviews. We call these features lexical ambi-
guity and denote it by E2.

5 Experimental Results

For our experiments we obtained the reviews from
the Yelp Dataset Challenge5, this dataset con-
tains over 1.6 million reviews from 10 different
cities. We also crawled reviews from Yelp in the
Los Angeles area which is not included in the

2We also considered content-based features derived from
PLSA topic weights, however the unigram features outper-
form these features, thus we exclude them for lack of space.

3www.speech.cs.cmu.edu/cgi-bin/cmudict
4http://wordnet.princeton.edu/
5http://www.yelp.com/dataset_challenge

[0
, 
1
]

(1
, 
2
]

(2
, 
3
]

(3
, 
4
]

(4
, 
5
]

Star Ratings

0

20

40

60

80

100

120

140

160

M
e
a
n
 A
v
e
ra
g
e
 N
u
m
b
e
r 
o
f 
R
e
v
ie
w
s

(a)

0 10 20 30 40 50 60 70 80
Number of votes

0

2

4

6

8

10

12

14

Lo
g
-F
re
q
u
e
n
cy

(b)

[0
, 
1
]

(1
, 
2
]

(2
, 
3
]

(3
, 
4
]

(4
, 
5
]

Star Ratings

0.0

0.5

1.0

1.5

2.0

2.5

M
e
a
n
 A
v
e
ra
g
e
 J
u
d
g
e
m
e
n
ts

(c)

Figure 3: (a) Mean average number of reviews
for restaurants falling in five different star rating
ranges. (b) Log occurrences of funny votes per
review. (c) Mean average voting judgements for
restaurants in different star ratings.

Yelp Dataset Challenge. This dataset was particu-
larly interesting since the readers are able to vote
whether a review is considered cool, funny, and/or
helpful. It also allows the flexibility for the review-
ers to write longer pieces of text to express their
overall rating of a restaurant.

5.1 Identifying Aspects in Reviews

We use recent advancements in Wikification,
which aims to connect important entities and con-
cepts in text to Wikipedia, it is also known as dis-
ambiguation to Wikipedia. In particular we use the
work of Ratinov et al. (2011), in order to obtain
the Wikipedia pages of the entities in the reviews,
we call these aspects of the review. Using the
Wikipedia description of the aspects we can com-
pute the language models for each aspect. Using
mitlm, the MIT language modeling toolkit by Hsu
and Glass (2008), we apply Modified Kneser-Ney
smoothing to obtain the language models from the
Wikipedia pages obtained from review’s aspects.

5.2 Preliminaries and Groundtruth
Construction

In Figure 3 we give an account of data statistics
based on a random sample of 500,000 reviews, fo-
cusing on the funny voting judgements and the
star rating distributions. In Figure 3a, we no-
tice that on average the highly rated restaurants
tend to have more reviews. Since users would

496



Features Classifiers
Naive Bayes Perceptron AdaBoost

Content Related Features

C1 69.92 (0.545) 57.62 (1.084) 69.44 (0.485)
C2 51.33 (1.250) 50.35 (0.763) 50.56 (1.155)
C3 50.86 (0.812) 50.00 (0.012) 50.59 (1.122)
C4 53.85 (0.486) 50.03 (0.172) 51.41 (1.205)

Alliteration E1 50.81 (0.408) 50.11 (0.301) 50.28 (1.195)
Ambiguity E2 51.53 (0.677) 50.39 (0.857) 51.78 (1.533)

Incongruity
A1 81.32 (0.974) 81.32 (0.974) 81.32 (0.974)
A2 83.68 (0.623) 83.68 (0.623) 83.68 (0.623)

Divergence Features D2 84.55 (0.550) 83.68 (0.627) 84.23 (0.561)
Unexpectedness U2 83.68 (0.627) 83.68 (0.627) 83.68 (0.627)

Combination features

A1 + D2 84.55 (0.549) 83.68 (0.627) 84.35 (0.548)
A2 + D2 84.55 (0.549) 84.00 (0.579) 84.41 (0.496)
D2 + U2 84.55 (0.549) 84.00 (0.579) 84.40 (0.549)

A2 + D2 + U2 84.55 (0.550) 83.89 (0.593) 84.35 (0.590)
D2 + U2 + C1 78.28 (0.545) 79.63 (0.534) 83.18 (1.109)
A2 + D2 + C1 78.87 (0.546) 82.68 (0.353) 85.61 (0.900)

A1 + D2+U2+C1 78.62 (0.671) 79.63 (0.528) 85.77 (0.843)
A2 + D2+U2+C1 78.87 (0.546) 81.60 (0.703) 85.60 (0.968)

Table 1: Classification accuracies, using 5-fold cross validation, the 95% confidence is given inside the
parenthesis.

prefer to dine in a restaurant expecting to get a
better overall experience, they create a feedback
on the reviews for those highly rated restaurants.
This “rich-get-richer” effect has been also been re-
cently observed in other social networks (Su et al.,
2016) and a more detailed analysis is out of scope
of this paper. We observe that most of the re-
views receive a low number of funny votes in Fig-
ure 3b, with µ = 0.55, where µ is the average
funny rating. Computing the restaurant’s average
funny votes, then taking the mean by the star rat-
ings for each category range, see Figure 3c, which
seems to be consistently increasing across the dif-
ferent star ratings. Note that this also includes the
restaurants with zero funny votes, by excluding
these we found that the ratings were more con-
sistently stable on about 2.1 votes. Thus regard-
less of restaurant rating, the funny reviews dis-
tribution are stable on average. Considering the
prevalence of noise in the voting process, we also
analysed those reviews with more than one funny
vote (µ = 3.90), and with more than two votes
(µ = 5.54).

To construct our ground-truth data, we took all
of the reviews at least five funny votes, which indi-
cates the review was collectively funny, and con-
sidered those as humorous reviews, we consid-

ered all the reviews with zero funny votes as non-
humorous reviews. We obtained 17,769 humorous
reviews and 856,202 non-humorous, from which
we sampled 12,000 reviews from each category,
and another 5,000 reviews was left for a develop-
ment dataset, to obtain a corpus with 34,000 re-
views total. In total we collected 2,747 wikipedia
pages with an average of about 247 sentences per
page. In our work we focused on identifying dis-
tinguishing features and relative improvement in
a balanced dataset and while the true distribution
may be skewed, we leave the unbalance distribu-
tion study for future work.

Finally we use five-fold cross validation to eval-
uate all the methods. Due to the success of linear
classifiers in text classification tasks we were in-
terested in studying the Perceptron and Adaboost
algorithms, we also used a Naive Bayes classi-
fier which has been shown to perform relatively
well in humor recognition tasks (Mihalcea and
Strapparava, 2006). We used the Learning Based
Java (LBJava) toolkit by Rizzolo and Roth (2010)
for the implementation of all the classifiers and
used their recommended parameter settings. For
the Averaged Perceptron implementation, we used
a learning rate of 0.05 and thickness of 5. In
Adaboost, we choose BinaryMIRA as our weak

497



learner to do our boosting on. We also considered
SparseWinnow and SparseConfidenceWeighted to
be our weak learner as well, but the boosting per-
formance for those two learners is marginal on the
development set.6 All experiments were run on an
Intel Core i5-4200U CPU with 1.60GHz running
Ubuntu.

5.3 Predicting Funny Reviews

We report the results of the features in Table 1.
First we can compare the accuracies of the indi-
vidual features. For the content related features
we see that the best features is C1, which is con-
sistent to what others have found in humor recog-
nition research (Mihalcea and Pulman, 2007). The
other content related features are based on some
popular features for detecting useful reviews, how-
ever we notice that in the humor context it is not
very effective. The performance of the contextual
features could indicate that humor is not specific
to a particular context and thus comparing differ-
ent context between humorous and non-humorous
text will not always work.

For the alliteration and ambiguity features
which were reported to be very useful in short text,
such as one-liners and on Twitter, are not as useful
in detecting humours reviews. The reason is pretty
clear since when writing a funny review, the re-
viewer does not worry about the limitation of text
and thus their humor does not rush to a punch-line.
Instead the reviewer is able to write a longer more
creative piece, adhering to less structure. The fea-
tures based on incongruity and unexpectedness, do
really well in distinguishing the funny and non-
funny reviews. For incongruity the best feature is
A2, achieving about the same accuracy as unex-
pectedness features of about 83% accuracy.

The best feature was D2 achieving an accuracy
of around 84% accuracy. The features seem to be
consistent over all of our classifiers. This indi-
cates that incorporating background text sources
to identify humor in reviews is crucial, and our
features we can indirectly capture some common
knowledge, e.g. prior knowledge. In particu-
lar it provides evidence that humor in online re-
views can be better categorized as referential hu-
mor (Ritchie, 2009) rather then shorter jokes. The
results also suggest that we can use these features

6Since our main goal is to understand the effectiveness
of various features we did not further tune these parame-
ters since they are presumably orthogonal to the question we
study.

to help predict the style of humorous text.
Exploring this would be an interesting venue for

future work. When we combine our features for
the classification task and find that the best com-
bination is the incongruity features with the diver-
gence features. We do not report the results for
features E1, E2 and other context features, C2,
C3, C4, since their performance when combined
with other features did not add to the accuracy of
the more discriminant feature. The divergence fea-
ture D2 plays a big role in the accuracy perfor-
mance. This is in line with our hypothesis that the
more uncommon language used the more it is pos-
sible to be for a humorous purpose.

It is interesting to see that AdaBoost performed
the best out of all three classifiers achieving about
86% accuracy, especially when more features
were added, the classifier was able to use this in-
formation for improvement. While Naive Bayes
and the Perceptron algorithm did not make such
improvement achieving about 85% accuracy.

5.4 Ranking Funny Reviews

From the data we noticed that funny reviews tend
to be voted highly useful,in particular we noticed a
correlation coefficient of 0.77. Although it would
have been easy to use the useful votes as a fea-
ture to determine whether the review is funny/not
funny, these scores are only available after people
have been exposed to these reviews. To test how
well the features worked when identifying help-
ful reviews, in a more realistic setting, we formu-
lated a retrieval problem. Given a set of reviews,
D = {R1, R2, ..., Rm} and relevant scores based
on usefulness, U = {u1, u2, ..., um}, is it possible
to develop a scoring function such that we rank the
useful reviews higher? For this task we used the
classification output of Naive Bayes, P (funny|Ri)
where i is the current example under considera-
tion, for our scoring function and trained with the
best performing features in the original dataset.
We used a with-held dataset crawled from restau-
rants in Yelp in the Los Angeles area containing
about 1,360 reviews with 260 reviews labelled as
helpful and the other reviews labelled as not help-
ful. To obtain the ground truth we used the useful
votes in Yelp similar to how we constructed the
funny labels, using a threshold of 5 votes mini-
mum to be considered helpful. This experiment
reveals two things about our features for detect-
ing humorous reviews. First we see that the preci-

498



K Precision @ K
1 1.00

10 0.50
25 0.48
50 0.44
100 0.45
200 0.54

Table 2: Precision of useful reviews.

sion is around 50%, see Table 2, this is more than
two times better than random guess which is about
19% and second that our features can be used to
filter out some useful reviews.

6 Related Work

Although there has been much work in the the-
ory of humor by many linguists, philosophers and
mathematicians (Paulos, 2008), the definition of
humor is still a debated topic of research (Attardo,
1994). There have been many applications from
computational humor research; for instance, cre-
ating embodied agents using humor, such as chat
bots, which could allow for more engaging inter-
actions and can impact many domains in education
(Binsted et al., 2006). Existing work on computa-
tional humor research can typically be divided into
humor recognition and humor generation.

In humor generation, some systems have suc-
cessfully generated jokes and puns by exploiting
some lexical structure in the pun/joke (Lessard and
Levison, 1992; Manurung et al., 2008; McKay,
2002). The HAHAcronym project was able to take
user inputs and output humorous acronyms and it
achieves comical effects by exploiting incongruity
(Stock and Strapparava, 2002). Work in automatic
generation of humor is limited to particular do-
mains, usually only generating short funny texts.

One of the earliest work on humor recognition
in text data is the work of Mihalcea and Strap-
parave (2006), trying to identify one-liners, short
sentences with a humorous effect. They frame the
problems as a classification problem and develop
surface features (alliteration, antonym, and adult
slang) as well as context related features. They ul-
timately proposed that additional knowledge such
as, irony, ambiguity, incongruity, and common
sense knowledge among other things would be
beneficial in humor recognition, but they do not
further pursue these avenues. Although they are
able to distinguish between humorous and non-

humorous one liners, in longer of texts such as re-
views it is not so clear that these features suffice.
Instead we make use of the creative writing struc-
ture of the reviewers by looking at the referenced
entities in their reviews.

Although verbal irony can be humorous, and an
active topic of research (Wallace, 2013), it is of-
ten defined as the “opposite to what the speaker
means”, and combining features for identifying
both humor and irony has been studied (see, e.g.,
Reyes et al. (2012)). In the work by Reyes
et al. (2012), the authors defined the unexpected-
ness feature as semantic relatedness of concepts in
Wordnet and assuming that the less the semantic
relatedness of concepts the funnier the text. In our
work we use a similar definition but applying it to
the “topical” relatedness of the referenced aspects
and the background language model. The authors
demonstrate that irony and humor share some sim-
ilar characteristics and thus we can potentially use
similar features to discriminate them. There has
been some early work in identifying humor fea-
tures in web comments (Reyes et al., 2010), in
these comments the users are able to create humor
through dialogue thus making the problem more
complex. More recently there was a workshop
in SemEval-2017 7, which focus is on identifying
humorous tweets which are related, typically as a
punchline, to a particular hashtag.

Kiddon and Brun (2011) aimed to understand
“That’s what she said” (TWSS) jokes, which they
classify as double entendres. They frame the prob-
lem as metaphor identification and notice that the
source nouns are euphemisms for sexually explicit
nouns. They also make use of the common struc-
ture of the TWSS jokes to the erotic domains to
improve 12% in precision over word-based fea-
tures. In our work we try to explicitly model
the incongruity of the reviewer, by doing so we
are able to distinguish the separate language used
by the user when introducing humorous concepts.
Recently there has been work in consumer re-
search, to identify the prevalence of humor in so-
cial media (McGraw et al., 2015). The main focus
was to examine the benign violation theory, which
“suggest that things are humorous when people
perceive something as wrong yet okay”. One of
their finding suggests that humor is more preva-
lent in complaints than in praise, thus motivating

7http://alt.qcri.org/semeval2017/
task6/

499



the usage of automatic humor identification meth-
ods for restaurants regardless of its popularity.

While there is a breadth of work in identifying
helpful reviews and opinion spam in reviews (Jin-
dal and Liu, 2008) as well as deceptive opinion
spam (Ott et al., 2011), and synthetic opinion spam
(Sun et al., 2013); we show that humour can also
be used to identify helpful reviews.

7 Conclusion

We have studied humorous text identification in a
novel setting involving online reviews. This task
has not been studied in the previous work and is
different than detecting humorous jokes or one-
liners, this allows for creative and expressive writ-
ing since the reviewer is not limited in text. In this
problem we cannot directly apply the ideas that
others have developed in order to identify the hu-
morous reviews. Instead features that are based on
the theory of incongruity are shown to outperform
previous features and are effective in the classi-
fication task. Our model introduces a novel and
way to incorporate external text sources for humor
identification task, and which can be applied to
any natural language provided there is a reference
database, i.e. news articles or Wikipedia pages,
in that language. We also show that the features
developed can also be used to identify helpful re-
views. This is very useful in the online review set-
ting since there tends to be a cumulative advan-
tage, that is the “rich get richer” effect which lim-
its the exposure that the users get to other helpful
reviews. Thus identifying these types of review
early can potentially diversify the types of reviews
that the users read.

Although we used a background language
model on the entire corpus to capture a sense of
expectation, there could be other ways to do this.
For example, we could develop neural network
embeddings to capture the entities descriptions in
the reviews. Another direction would be to use
topic models and see whether reviewers are more
inclined to compare different types of references
when talking about certain aspects of restaurants
or other products. A different approach to identi-
fying helpful reviews would be to create entertain-
ing and informative summaries.

Acknowledgments

The first author was supported by the University of
Illinois, Urbana-Champaign College of Engineer-

ing’s Support for Underrepresented Groups in En-
gineering (SURGE) Fellowship and the Graduate
College’s Graduate Distinguished Fellowship.

References
Salvatore Attardo. 1994. Linguistic theories of humor,

volume 1. Walter de Gruyter.

Kim Binsted, Anton Nijholt, Oliviero Stock, Carlo
Strapparava, G Ritchie, R Manurung, H Pain, An-
nalu Waller, and D O’Mara. 2006. Computational
humor. Intelligent Systems, IEEE, 21(2):59–69.

David M Blei, Andrew Y Ng, and Michael I Jordan.
2003. Latent dirichlet allocation. Journal of ma-
chine Learning research, 3(Jan):993–1022.

Arthur P Dempster, Nan M Laird, and Donald B Rubin.
1977. Maximum likelihood from incomplete data
via the em algorithm. Journal of the royal statistical
society. Series B (methodological), pages 1–38.

Thomas Hofmann. 1999. Probabilistic latent semantic
analysis. In Proceedings of the Fifteenth conference
on Uncertainty in artificial intelligence, pages 289–
296. Morgan Kaufmann Publishers Inc.

Bo-June Hsu and James Glass. 2008. Iterative lan-
guage model estimation: efficient data structure &
algorithms. In Proceedings of Interspeech, vol-
ume 8, pages 1–4.

Nitin Jindal and Bing Liu. 2008. Opinion spam and
analysis. In Proceedings of the 2008 International
Conference on Web Search and Data Mining, pages
219–230. ACM.

Chloe Kiddon and Yuriy Brun. 2011. That’s what she
said: double entendre identification. In Proceedings
of the 49th Annual Meeting of the Association for
Computational Linguistics: Human Language Tech-
nologies: short papers-Volume 2, pages 89–94. As-
sociation for Computational Linguistics.

Greg Lessard and Michael Levison. 1992. Computa-
tional modelling of linguistic humour: Tom swifties.
In ALLC/ACH Joint Annual Conference, Oxford,
pages 175–178.

Yue Lu, Qiaozhu Mei, and ChengXiang Zhai. 2011. In-
vestigating task performance of probabilistic topic
models: an empirical study of plsa and lda. Infor-
mation Retrieval, 14(2):178–203.

Ruli Manurung, Graeme Ritchie, Helen Pain, An-
nalu Waller, Dave O’Mara, and Rolf Black. 2008.
The construction of a pun generator for language
skills development. Applied Artificial Intelligence,
22(9):841–869.

A Peter McGraw, Caleb Warren, and Christina Kan.
2015. Humorous complaining. Journal of Con-
sumer Research, 41(5):1153–1171.

500



Justin McKay. 2002. Generation of idiom-based wit-
ticisms to aid second language learning. Stock et
al.(2002), pages 77–87.

Rada Mihalcea and Stephen Pulman. 2007. Character-
izing humour: An exploration of features in humor-
ous texts. In Computational Linguistics and Intelli-
gent Text Processing, pages 337–347. Springer.

Rada Mihalcea and Carlo Strapparava. 2006. Learn-
ing to laugh (automatically): Computational models
for humor recognition. Computational Intelligence,
22(2):126–142.

Anton Nijholt. 2014. Towards humor modelling and
facilitation in smart environments. Advances in Af-
fective and Pleasurable Design, pages 260–269.

Myle Ott, Yejin Choi, Claire Cardie, and Jeffrey T
Hancock. 2011. Finding deceptive opinion spam
by any stretch of the imagination. In Proceed-
ings of the 49th Annual Meeting of the Association
for Computational Linguistics: Human Language
Technologies-Volume 1, pages 309–319. Association
for Computational Linguistics.

John Allen Paulos. 2008. Mathematics and humor: A
study of the logic of humor. University of Chicago
Press.

Lev Ratinov, Dan Roth, Doug Downey, and Mike
Anderson. 2011. Local and global algorithms
for disambiguation to wikipedia. In Proceedings
of the 49th Annual Meeting of the Association
for Computational Linguistics: Human Language
Technologies-Volume 1, pages 1375–1384. Associ-
ation for Computational Linguistics.

Antonio Reyes, Martin Potthast, Paolo Rosso, and
Benno Stein. 2010. Evaluating humour features on
web comments. In LREC.

Antonio Reyes, Paolo Rosso, and Davide Buscaldi.
2012. From humor recognition to irony detection:
The figurative language of social media. Data &
Knowledge Engineering, 74:1–12.

Graeme Ritchie. 2009. Can computers create humor?
AI Magazine, 30(3):71.

Nick Rizzolo and Dan Roth. 2010. Learning based java
for rapid development of nlp systems. In LREC.

Oliviero Stock and Carlo Strapparava. 2002. Ha-
hacronym: Humorous agents for humorous
acronyms. Stock, Oliviero, Carlo Strapparava, and
Anton Nijholt. Eds, pages 125–135.

Jessica Su, Aneesh Sharma, and Sharad Goel. 2016.
The effect of recommendations on network struc-
ture. In Proceedings of the 25th International Con-
ference on World Wide Web, pages 1157–1167. In-
ternational World Wide Web Conferences Steering
Committee.

Huan Sun, Alex Morales, and Xifeng Yan. 2013. Syn-
thetic review spamming and defense. In Proceed-
ings of the 19th ACM SIGKDD international con-
ference on Knowledge discovery and data mining,
pages 1088–1096. ACM.

Byron C Wallace. 2013. Computational irony: A sur-
vey and new perspectives. Artificial Intelligence Re-
view, pages 1–17.

501


