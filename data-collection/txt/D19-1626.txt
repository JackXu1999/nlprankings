



















































Aggregating Bidirectional Encoder Representations Using MatchLSTM for Sequence Matching


Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing
and the 9th International Joint Conference on Natural Language Processing, pages 6059–6063,
Hong Kong, China, November 3–7, 2019. c©2019 Association for Computational Linguistics

6059

Aggregating Bidirectional Encoder Representations Using MatchLSTM
for Sequence Matching

Bo Shao†‡, Yeyun Gong‡, Weizhen Qi§‡, Nan Duan‡, Xiaola Lin†
†Sun Yat-sen University
‡Microsoft Research Asia

§University of Science and Technology of China
shaobo2@mail2.sysu.edu.cn

{yegong, nanduan}@microsoft.com
weizhen@mail.ustc.edu.cn
linxl@mail.sysu.edu.cn

Abstract

In this work, we propose an aggregation
method to combine the Bidirectional Encoder
Representations from Transformer (BERT)
with a MatchLSTM layer for Sequence Match-
ing. Given a sentence pair, we extract the out-
put representations of it from BERT. Then we
extend BERT with a MatchLSTM layer to get
further interaction of the sentence pair for se-
quence matching tasks. Taking natural lan-
guage inference as an example, we split BERT
output into two parts, which is from premise
sentence and hypothesis sentence. At each
position of the hypothesis sentence, both the
weighted representation of the premise sen-
tence and the representation of the current to-
ken are fed into LSTM. We jointly train the
aggregation layer and pre-trained layer for se-
quence matching. We conduct an experiment
on two publicly available datasets, WikiQA
and SNLI. Experiments show that our model
achieves significant improvement compared
with state-of-the-art methods on both datasets.

1 Introduction

Text sequence matching aims to infer the relation-
ship of two text sequences which is a critical prob-
lem used in many NLP tasks such as answer se-
lection (Yang et al., 2015), natural language infer-
ence (Bowman et al., 2015), and paraphrase iden-
tification (Dolan et al., 2004).

Recent years, various deep learning models
have been proposed for text sequence matching
tasks (Wang and Jiang, 2016; Tay et al., 2018; Ty-
moshenko and Moschitti, 2018; Kim et al., 2018).
Most previous works are learned from task spe-
cific supervised datasets, which are always limited
to the amount, due to the cost of annotation. Re-
cently, learning a general representation of words
based on language models attracts great attention,
and are successfully applied to a range of NLP
tasks (McCann et al., 2017; Peters et al., 2018,

2017; Alec Radford, 2018; Devlin et al., 2018). In
these works, they first learn general language rep-
resentations through training a language model us-
ing large scale unlabeled data. Then they use these
representations to the downstream tasks. Previous
works show that the representations with linguis-
tic information perform obvious effectiveness and
robustness applied to downstream tasks.

In these methods, there are two typical strate-
gies using pre-trained models, feature-based (Pe-
ters et al., 2017) and fine-tuning approach (De-
vlin et al., 2018; Radford et al., 2018). Our
model is based on the fine-tuning approach and
use BERT (Devlin et al., 2018) as our pre-trained
language model. BERT has been directly used
in many tasks and achieved significantly improve-
ment compared with other methods. However,
BERT only uses a standard softmax layer for many
different specific tasks. Intuitively, for more com-
plex tasks, like sequence matching in this work,
we need a better strategy for extending the BERT
to infer the relationship of the sequence pair. Thus,
we incorporate a MatchLSTM layer (Wang and
Jiang, 2015) to BERT, and make it better to adapt
to sequence matching.

In our model, we design a MatchLSTM layer
to aggregate representations of words in the se-
quences, and then we concatenate the hidden
state of first token in BERT to the output of
MatchLSTM (Wang and Jiang, 2015). This
method can combine the features extracted from
BERT and MatchLSTM effectively, and it is much
better than directly using the outputs of BERT as
pre-trained word representations to MatchLSTM.

The main contributions of this work are as fol-
lows:

• We extend BERT with a MatchLSTM layer.
It can be applied to a variety of sequence
matching tasks.



6060

• We compare various aggregation layers to
extend BERT, with the MatchLSTM layer
achieving the best performance. It gets more
powerful representations than the original
output from BERT for sequence matching.

• Experiments on WikiQA and SNLI datasets
show that our model achieves better perfor-
mance than state-of-the-art methods.

2 Related Work

Sequence Matching task has attracted lots of at-
tention in the past decades. There are many
related works on Question Answering(QA) (Yin
et al., 2015; Tay et al., 2018; Wang et al., 2017;
Tymoshenko and Moschitti, 2018; Min et al.,
2017), Natural Language Inference(NLI) (Peters
et al., 2018; Kim et al., 2018; Radford et al.,
2018) and so on. (Yin et al., 2015) use atten-
tion mechanism with convolutional layer to model
sentence pairs. In (Tymoshenko and Moschitti,
2018), they combine the similarity features of
members within the same pair and traditional sen-
tence pair similarity and achieve state-of-the-art
results on several answer selection datasets includ-
ing WikiQA (Yang et al., 2015). (Peters et al.,
2018) and (Radford et al., 2018) incorporate pre-
trained language models to text sequence match-
ing task and achieve performance improvement on
SNLI (Bowman et al., 2015).

Recently, pre-trained language models have
been successfully used in NLP tasks. ELMo (Pe-
ters et al., 2017) is trained as a bidirectional
language model. OpenAI GPT (Alec Radford,
2018) uses a basic left-to-right transformer to
learn a language model. BERT, used in this pa-
per, is based on the architecture of a bidirectional
Transformer and per-trained on Masked Language
Model task and Next Sentence Prediction. Using
the pre-trained parameters, BERT (Devlin et al.,
2018) achieves state-of-the-art performance on
the GLUE benchmark (Wang et al., 2018) and
SQuAD 1.1 (Rajpurkar et al., 2016) by fine-tuning
in corresponding supervised data.

In this work, we design a sequence matching
model based on BERT.

3 Our Model

In this section, we will introduce our sequence
matching model. Our model is combined BERT
encoder with the MatchLSTM layer.

3.1 BERT Encoder

Given two natural language sentences A, B, the
tokens of both sentences are packed into a token
sequence S as “[CLS] A [SEP] B [SEP]”, where
[CLS], [SEP] are special tokens. The representa-
tion of each token is the sum of three types of em-
bedding: 1). WordPiece embedding(Wu et al.,
2016) is used in BERT to represent the input se-
quence, which has 30000 token vocabulary. The
tokens in S are split into several word pieces with
”##”. 2). Position embedding is used to cover
position information in transformer. 3). Segment
embedding is used to indicate whether the current
token is from A or B.

Then the representation of S is fed into the
BERT encoder which is a multi-layer bidirectional
Transformer(Vaswani et al., 2017). The imple-
ment of its architecture can refer to (Vaswani et al.,
2017; Devlin et al., 2018). BERT encodes the in-
put token sequence S into a context aware repre-
sentation h = {h0, h1, ..., h|S|−1}. Then h is used
to a match layer for downstream supervised tasks.

For downstream tasks of text sequence match-
ing, BERT extract the final hidden state of the
first token, h0, corresponding to the special to-
ken [CLS]. The label probabilities are computed
with h0 by a standard softmax layer. Finally, all
of the parameters of the model are fine-tuned to
maximize the log-probability of the correct label
in specific tasks.

3.2 MatchLSTM Layer

Although BERT has proved its effectiveness for
text sequence matching tasks, there still exist two
limitations. First, the two sentences can only in-
teract through a general attention mechanism in
the transformer, which is not enough for complex
matching tasks. Secondly, BERT only uses the
first token hidden state in sequence matching tasks
such as QA, MNLI in (Devlin et al., 2018), ignor-
ing the final hidden state hi, i > 0 in other po-
sitions. To overcome the two limitations, we add
a bidirectional MatchLSTM network (Wang and
Jiang, 2015), which computes the word-by-word
matching results at each position for input sen-
tence pairs.

To get the representation of each input sentence,
we first split the final hidden state h into hA and
hB as the representations of the sentence pair,
A and B, according to its position information.
Then we apply a bidirectional MatchLSTM net-



6061

work with the input of hA and hB . At each posi-
tion i of the tokens in sentence B, the MatchLSTM
layer first uses a standard word-by-word atten-
tion mechanism to obtain attention weight ai =
{ai0, ai1, ..., ai|A|−1} over sentence A and compute
a weighted sum of sentence A as ci, which can be
formulated as,

eij = u
T tanh(WAhAj +W

BhBi +W
ssi−1 + be)

aij =
eij∑|p|
k=1 e

i
k

; ci =

|A|∑
j=0

aijhj

(1)

where u, WA,WB ,W s and be are trainable param-
eters, si is the hidden state at the ith position of B.
The i,j in Equation 1 represents ith word in hA

and jth word in hB , and eij is the attention score
between the two words. We concatenate ci with
the current token of sentence B as:

ri = [h
B
i : ci]

si = LSTM(ri, si−1)
(2)

Then, we concatenate the last hidden state of
MatchLSTM layer s|B| with the first hidden state
h0 of BERT encoder and feed it into the output
softmax layer,

f = [s|B| : h0]

P = softmax(W ff + bf )
(3)

where W f and bf are trainable parameters, P is
the label probability distribution.

3.3 Loss Function
Finally we compute the loss function to maxi-
mize the log-probability of corresponding label
y ∈ {0, 1, ..,m − 1}, m is the number of target
label types. It can be formulated as,

L = −log(Pi)[i = y] (4)

3.4 Other Aggregation Layers
In this section, we introduce two other general
methods to utilize all the final hidden states of
BERT encoder.

3.4.1 Average Layer
To incorporate all the hidden states of BERT, a ba-
sic idea is to sum all final hidden states h directly
as the feature vector favg. It will be used to calcu-
late the probability of final label by Eq.3.

3.4.2 Convolutional Layer

We also evaluate the performance using the con-
volutional layer with max pooling. We set sev-
eral fixed window sizes to extract feature vectors.
For each windows size l, we use a kernel matrix
W l ∈ Rl×d and a non-linear function to operate
on the hidden vector h from BERT encoder. The
output of one operation is a local feature which
can be computed as follows:

zi = g(W
l · v[i : i+ l] + bl) (5)

where W l and bl are trainable parameters,
g is a non-linear function. We conduct
this operation on different hidden vectors
v1:l, v2:l+1, ..., vn−l+1:n and get a set of local
features z = {z1, z2, ..., zn−l+1}. We then extract
a maximum feature from the local features z
generated by one kernel through the max pooling
layer and get feature vector fcnn. We concatenate
fcnn with the first hidden state h0 as above to
get the final feature vector and calculate the
probability distribution of labels.

4 Experiment

We conduct experiments on the WikiQA and SNLI
corpuses. We use Mean Average Precision (MAP)
and Mean Reciprocal Rank (MRR) as the evalua-
tion metrics on WikiQA. On SNLI, we use accu-
racy(ACC) as our evaluation metric.

4.1 Datasets

WikiQA dataset (Yang et al., 2015) is a sentence-
level QA dataset, containing 2.0k/0.3k/0.6k
train/dev/test examples. Each sample contains a
query in Bing’s log and a snippet of a Wikipedia
retrieved by Bing. Each query contains 18.6 candi-
date sentences on average. The task is to select an
answer from candidate sentences for each query.

The Stanford Natural Language Inference
(SNLI) dataset (Bowman et al., 2015) is a large-
scale entailment classification task, containing
570K human annotated sentence pairs. Each sam-
ple consists of a sentence pair and a label from
relationship class (entailment, neutral, contradic-
tion, and -). We remove samples in ”-” class same
as previous works, since they do not have a con-
sensus label among the annotators. For training,
development and test sets, we use the same split
with (Bowman et al., 2015) to report our results.



6062

4.2 Settings
We use the pre-trained model of BERTBASE (Our
goal is to prove the effectiveness of our aggrega-
tion method, so we do not use the BERTLARGE
which is time and resource consuming), the num-
ber of Transformer blocks is 12, and dimension of
all hidden state is 768 in our model. The batch
size of the model is 32 and the initial learning rate
is 2e-5. We set the dropout rate to 0.5.

Except for using state-of-the-art methods as our
baseline, we also list variants of aggregated layers
with BERT in the experiments. “BERT+AVG” is
the model described in 3.4.1. “BERT+CNN” is the
model proposed in 3.4.2. “BERT” represents the
BERT with pre-trained parameters as initializa-
tion. “BERTno init” denotes BERT using random
parameters as initialization. The “-FHS” means
the model does not use the hidden state of first to-
ken in BERT encoder.

4.3 Experiments on WikiQA

Methods MAP MRR
ABCNN (Yin et al., 2015) 69.21 71.08
MULT (Wang and Jiang, 2016) 74.33 75.45
HyperQA (Tay et al., 2018) 72.70 71.20
BiMPM (Wang et al., 2017) 71.80 73.10
PTK (Tymoshenko and Moschitti, 2018) 73.34 74.68
SQuAD* (Min et al., 2017) 83.20 84.58
BERTno init 58.91 60.09
BERT (Devlin et al., 2018) 80.15 82.31
BERT+AVG 81.95 83.26
BERT+CNN 81.23 83.16
BERT+MatchLSTM 83.53 85.13

-FHS 70.46 72.44

Table 1: Performance for answer sentence selection on
WikiQA dataset. Results of PTK are ensemble results
and SQuAD* is trained with extra supervised corpus,
SQuAD.

We first study the effectiveness of our method
for answering on the WikiQA dataset. Our
model achieves new state-of-the-art performance
compared with previous methods. First, com-
paring “BERT+MatchLSTM” with “BERT”, we
find that “BERT+MatchLSTM” achieves signif-
icant improvement, which illustrates the effec-
tiveness of our method using a MatchLSTM
layer to extend “BERT”. Moreover, from the
results of “BERT+MatchLSTM”, “BERT+AVG”
and “BERT+CNN”, we find that the MatchLSTM
layer utilizes the output representations from the
BERT encoder more efficiently than “AVG” and
“CNN”. We also see that “BERTno init” does not
perform well on this task, which shows that the ar-

chitecture of BERT is not superior to existing ar-
chitectures, thus using other architectures to boost
the pre-trained BERT is meaningful and impor-
tant. From the result of “-FHS” we find that
the performance drops a lot, which illustrates the
method we used to concatenate the hidden state of
first token in BERT to the output of MatchLSTM
is very important.

4.4 Experiments on SNLI

SNLI train test
SLRC (Zhang et al., 2018) 89.1 89.1
LMPT (Radford et al., 2018) 96.6 89.9
ESIM+ELMo (Peters et al., 2018) 91.6 88.7

+Ensemble 92.1 89.3
DRCN (Kim et al., 2018) 93.1 88.9

+Ensemble 95.0 90.1
BERTno init 90.7 82.5
BERT (Devlin et al., 2018) 95.9 89.6
BERT+AVG 96.2 90.1
BERT+CNN 94.7 90.2
BERT+MatchLSTM 96.8 90.9

-FHS 91.4 88.3

Table 2: Accuracy on SNLI dataset. All compared re-
sults are from the SNLI Leaderboard.

Table 2 shows the comparisons of our method
with the state-of-the-art methods on the SNLI
dataset. The trends observed in this experiment
are consistent with the former one. Compared
with baseline methods, our “BERT+MatchLSTM”
achieves the best performance on both training and
test sets. Furthermore, compared with “BERT”,
“BERT+AVG” and “BERT+CNN”, our model
shows obvious superiority. Comparing the results
of “BERT” and “BERTno init”, we also observe
that the pre-training process is significantly help-
ful in this task.

5 Conclusion

In this work, we design an aggregation method
to combine BERT with a MatchLSTM layer for
sequence matching. We show that our model is
more effective in computing the interaction fea-
tures with the BERT encoder than the original
BERT model. Experiments show that our method
achieves new state-of-the-art performance on Wik-
iQA and SNLI datasets.

Acknowledgements

This work is supported by the National Natu-
ral Science Foundation of China under Grants
No.U1711263.



6063

References
Tim Salimans andIlya Sutskever Alec Radford,

Karthik Narasimhan. 2018. Improving language un-
derstanding with unsupervised learning. Technical
report, OpenAI.

Samuel R Bowman, Gabor Angeli, Christopher Potts,
and Christopher D Manning. 2015. A large anno-
tated corpus for learning natural language inference.
arXiv preprint arXiv:1508.05326.

Jacob Devlin, Ming-Wei Chang, Kenton Lee, and
Kristina Toutanova. 2018. Bert: Pre-training of deep
bidirectional transformers for language understand-
ing. arXiv preprint arXiv:1810.04805.

Bill Dolan, Chris Quirk, and Chris Brockett. 2004.
Unsupervised construction of large paraphrase cor-
pora: Exploiting massively parallel news sources.
In Proceedings of the 20th international conference
on Computational Linguistics, page 350. Associa-
tion for Computational Linguistics.

Seonhoon Kim, Jin-Hyuk Hong, Inho Kang, and No-
jun Kwak. 2018. Semantic sentence matching with
densely-connected recurrent and co-attentive infor-
mation. arXiv preprint arXiv:1805.11360.

Bryan McCann, James Bradbury, Caiming Xiong, and
Richard Socher. 2017. Learned in translation: Con-
textualized word vectors. In Advances in Neural In-
formation Processing Systems, pages 6294–6305.

Sewon Min, Minjoon Seo, and Hannaneh Hajishirzi.
2017. Question answering through transfer learn-
ing from large fine-grained supervision data. arXiv
preprint arXiv:1702.02171.

Matthew E Peters, Waleed Ammar, Chandra Bhagavat-
ula, and Russell Power. 2017. Semi-supervised se-
quence tagging with bidirectional language models.
arXiv preprint arXiv:1705.00108.

Matthew E Peters, Mark Neumann, Mohit Iyyer, Matt
Gardner, Christopher Clark, Kenton Lee, and Luke
Zettlemoyer. 2018. Deep contextualized word rep-
resentations. arXiv preprint arXiv:1802.05365.

Alec Radford, Karthik Narasimhan, Tim Sali-
mans, and Ilya Sutskever. 2018. Improv-
ing language understanding by generative pre-
training. URL https://s3-us-west-2. amazon-
aws. com/openai-assets/research-covers/language-
unsupervised/language understanding paper. pdf.

Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and
Percy Liang. 2016. Squad: 100,000+ questions
for machine comprehension of text. arXiv preprint
arXiv:1606.05250.

Yi Tay, Luu Anh Tuan, and Siu Cheung Hui. 2018.
Hyperbolic representation learning for fast and ef-
ficient neural question answering. In Proceedings of
the Eleventh ACM International Conference on Web
Search and Data Mining, pages 583–591. ACM.

Kateryna Tymoshenko and Alessandro Moschitti.
2018. Cross-pair text representations for answer
sentence selection. In Proceedings of the 2018 Con-
ference on Empirical Methods in Natural Language
Processing, pages 2162–2173.

Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob
Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz
Kaiser, and Illia Polosukhin. 2017. Attention is all
you need. In NIPS.

Alex Wang, Amapreet Singh, Julian Michael, Felix
Hill, Omer Levy, and Samuel R Bowman. 2018.
Glue: A multi-task benchmark and analysis platform
for natural language understanding. arXiv preprint
arXiv:1804.07461.

Shuohang Wang and Jing Jiang. 2015. Learning nat-
ural language inference with lstm. arXiv preprint
arXiv:1512.08849.

Shuohang Wang and Jing Jiang. 2016. A compare-
aggregate model for matching text sequences. arXiv
preprint arXiv:1611.01747.

Zhiguo Wang, Wael Hamza, and Radu Florian. 2017.
Bilateral multi-perspective matching for natural lan-
guage sentences. arXiv preprint arXiv:1702.03814.

Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V
Le, Mohammad Norouzi, Wolfgang Macherey,
Maxim Krikun, Yuan Cao, Qin Gao, Klaus
Macherey, et al. 2016. Google’s neural ma-
chine translation system: Bridging the gap between
human and machine translation. arXiv preprint
arXiv:1609.08144.

Yi Yang, Wen-tau Yih, and Christopher Meek. 2015.
Wikiqa: A challenge dataset for open-domain ques-
tion answering. In Proceedings of the 2015 Con-
ference on Empirical Methods in Natural Language
Processing, pages 2013–2018.

Wenpeng Yin, Hinrich Schütze, Bing Xiang, and
Bowen Zhou. 2015. Abcnn: Attention-based convo-
lutional neural network for modeling sentence pairs.
arXiv preprint arXiv:1512.05193.

Zhuosheng Zhang, Yuwei Wu, Zuchao Li, Shexia He,
Hai Zhao, Xi Zhou, and Xiang Zhou. 2018. I know
what you want: Semantic learning for text compre-
hension. arXiv preprint arXiv:1809.02794.


