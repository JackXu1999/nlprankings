



















































Artificial Error Generation with Fluency Filtering


Proceedings of the Fourteenth Workshop on Innovative Use of NLP for Building Educational Applications, pages 87–91
Florence, Italy, August 2, 2019. c©2019 Association for Computational Linguistics

87

Artificial Error Generation with Fluency Filtering

Mengyang Qiu†‡ Jungyeul Park†
† Department of Linguistics

‡ Department of Communicative Disorders and Sciences
State University of New York at Buffalo

{mengyang,jungyeul}@buffalo.edu

Abstract

The quantity and quality of training data plays
a crucial role in grammatical error correction
(GEC). However, due to the fact that obtain-
ing human-annotated GEC data is both time-
consuming and expensive, several studies have
focused on generating artificial error sentences
to boost training data for grammatical error
correction, and shown significantly better per-
formance. The present study explores how flu-
ency filtering can affect the quality of artifi-
cial errors. By comparing artificial data fil-
tered by different levels of fluency, we find that
artificial error sentences with low fluency can
greatly facilitate error correction, while high
fluency errors introduce more noise.

1 Introduction

Grammatical Error Correction (GEC), a NLP task
of automatically detecting and correcting gram-
matical errors in text, has received much attention
in the past few years, because of an ever-growing
demand for reliable and quick feedback to facil-
itate the progress of English learners. In a typi-
cal GEC task, an error sentence such as I follows
his advice needs to be corrected to a grammati-
cal sentence I follow his advice, while a grammat-
ical sentence She follows his advice should out-
put the same sentence without any modification.
Currently, neural machine translation (NMT) sys-
tems using sequence-to-sequence (seq2seq) learn-
ing (Sutskever et al., 2014) that “translate” incor-
rect sentences into correct ones, have shown to
be promising in grammatical error correction, and
several recent NMT approaches have obtained the
state-of-the-art results in GEC (e.g., Chollampatt
and Ng, 2018; Ge et al., 2018; Zhao et al., 2019).

While designing a GEC-oriented seq2seq archi-
tecture is one important aspect to achieve high
performance in grammatical error correction, the
quantity and quality of data also plays a crucial

role in the NMT approach to GEC, as NMT pa-
rameters cannot learn and generalize well with
limited training data. Due to the fact that ob-
taining human-annotated GEC data is both time-
consuming and expensive, several studies have fo-
cused on generating artificial error sentences to
boost training data for grammatical error correc-
tion. One main approach is to extract errors and
their surrounding context (the context window ap-
proach) from available annotated data, and then
apply the errors to error-free sentences naively
or probabilistically (Yuan and Felice, 2013; Fe-
lice, 2016). The other approach uses machine
back-translation, which switches the source-target
sentence pairs in GEC and learns to ”translate”
correct sentences into their incorrect counterparts
(Kasewa et al., 2018). While the first approach
may not generalize well to unseen errors, and the
second one may have no control over what kind
of error is produced, artificial error sentences gen-
erated from both approaches contribute to better
performance in grammatical error correction.

In this paper, we do not focus on which ap-
proach is superior in artificial error generation.
Rather, given that both approaches can generate
multiple error candidates for each correct sen-
tence, we investigate how to select the best ones
that can boost GEC performance the most. Al-
though previous studies have shown that artificial
errors that match the real error distributions tend
to generate better results (Felice, 2016; Xie et al.,
2018), we propose an alternative framework that
incorporates fluency filtering based on language
models. We evaluate four strategies of artificial er-
ror selection using different fluency ranges (from
lowest to highest) on the recent W&I+LOCNESS
test set. Our results show that three of the four
strategies lead to evident improvement over the
original baseline, which is in line with previous
findings that in general GEC benefits from artifi-



88

cial error data. The model trained with artificial
error sentences with the lowest fluency obtains the
highest recall among the four settings, while the
one trained with error sentences with the median
fluency achieves the highest performance in terms
of F0.5, with an absolute increase of 5.06% over
the baseline model.

2 Related Work

Our work mainly builds on the context window
approach to artificial error generation. In this ap-
proach, all the possible error fragments (errors and
their surrounding context) and their corresponding
correct fragments are first extracted from GEC an-
notated corpora. For example, I follows his and
I follow his are the fragments extracted from the
example sentences in the first paragraph. With
these correct-incorrect fragments, for each error-
free sentence, if we find the same correct fragment
in the sentence, we can inject errors by replacing
that fragment with the incorrect one. Felice (2016)
has shown that a context window size of one, that
is, one token before and after the error words or
phrases, is able to generate a decent amount of er-
ror sentences while maintaining the plausibility of
the errors. Thus, the current study also adopts this
context window size in extracting fragments.

The current work is also inspired by the fluency
boost learning proposed in Ge et al. (2018). In
their study, sentence fluency is defined as the in-
verse of the sentence’s cross entropy. During flu-
ency boost training, the fluency of candidate sen-
tences generated by their GEC seq2seq model is
monitored. Candidate sentences with less than
perfect fluency compared to the correct ones are
appended as additional error-contained data for
subsequent training. Fluency is also used during
multi-round GEC inference, in that inference con-
tinues as long as the fluency of the output sen-
tences keeps improving. The present study uses
fluency measure in an opposite way. We examine
how the decrease of fluency in artificial error sen-
tences influences the performance of grammatical
error correction.

3 Proposed Methods

To filter candidate error sentences based on flu-
ency, our first step is to generate all the candi-
date sentences. With correct-incorrect fragment
pairs extracted from GEC annotated corpora, we
replace all correct fragments found in each error-

free sentence with their incorrect counterparts ex-
haustively. Unlike a method described in Felice
(2016) that multiple errors can apply to one sen-
tence at the same time, we only allow one error
at a time. Table 1 shows an example of an error-
free sentence and the candidate sentences after ap-
plying all the possible error replacements. There
is only one error in each candidate sentence, and
the same position in the correct sentence can have
multiple different replacements (e.g., effects →
impacts|effect|dealing). We then calculate the flu-
ency score of each candidate sentence and select
the ones with the highest fluency, lowest fluency
and median fluency. Fluency score is measured
by sentence perplexity, the inverse probability of
the sentence based on a language model, normal-
ized by the number of words in that sentence. A
sentence’s fluency score is negatively related to its
perplexity. Our prediction is that low sentence flu-
ency (high perplexity) can facilitate error detec-
tion and correction by maximizing and highlight-
ing the difference between correct and incorrect
sentences. Conversely, artificial error sentences of
high fluency can be confusing to the model as the
difference between correct and incorrect sentences
may be subtle.

4 Experiments and Results

4.1 Dataset and evaluation

We used the four datasets — FCE, NUCLE,
W&I+LOCNESS and Lang-8 — provided in the
BEA 2019 Shared Task on GEC1 as the training
data for our baseline model (in total about 1.1M
sentence pairs). Table 2 shows the summary of the
four datasets. There are slightly over half a mil-
lion error-contained sentences in these datasets,
where we extracted 1.3M correct-incorrect frag-
ments. We applied our artificial error injection
procedure to the remaining 0.6M error-free sen-
tences, and over 0.4M of them received replace-
ments. We trained a 3-gram language model on all
the correct-side sentences using KenLM (Heafield,
2011). The language model was used to calcu-
late perplexity of artificial error sentences. From
the 0.4M sentences with error injections, we cre-
ated four different artificial datasets: one with the
highest fluency error sentences among the candi-
dates of each correct sentence, one with the low-
est, one with the median, and the last one was

1https://www.cl.cam.ac.uk/research/nl/
bea2019st/

https://www.cl.cam.ac.uk/research/nl/bea2019st/
https://www.cl.cam.ac.uk/research/nl/bea2019st/


89

Sentence Fluency
Correct the effects of the use of biometric identification are obvious .
Candidates: the effects of the used of biometric identification are obvious .

the effects of use of biometric identification are obvious . Median
the effects of the using of biometric identification are obvious .
the impacts of the use of biometric identification are obvious .
the effect of the use of biometric identification are obvious . Highest

...
the dealing of the use of biometric identification are obvious . Lowest

Table 1: An example of an error-free sentence and its error injected candidate sentences with three levels of fluency.

Corpus # Sent Pairs
FCE (Train) 28,346

NUCLE 57,113
W&I+LOCNESS

(Train)
34,304

LANG-8 1,037,561
Total 1,157,324

Correct 601,958
Error Injection

to Correct
444,521

Table 2: Summary of training data.

randomly selected. Each version was then com-
bined with the original error-contained sentences
and the remaining unchanged correct sentences so
that all these settings had the same number of sen-
tence pairs as in our baseline model (1.1M). The
goal of the experiment was to compare the GEC
performance trained with these four settings to the
baseline. The W&I+LOCNESS development set
of 4,382 sentences was used as validation, and
the W&I+LOCNESS test set of 4,477 sentences as
evaluation2. All these settings were run for three
times. Performance was evaluated in terms of pre-
cision, recall and F0.5 using ERRANT (Bryant
et al., 2017).

4.2 Experimental settings

We used the 7-layer convolutional seq2seq model3

proposed in Chollampatt and Ng (2018) for gram-
matical error correction with minimal modifica-
tion. The only difference to Chollampatt and Ng
(2018) is that the word embedding dimensions in
both encoders and decoders were set to 300 rather
than 500, and the word embeddings were trained

2https://competitions.codalab.org/
competitions/21922

3https://github.com/pytorch/fairseq

separately using the error and correct side training
data instead of external corpora. Other parame-
ters were set as recommended in Chollampatt and
Ng (2018), including the top 30K BPE tokens as
the vocabularies of input and output, 1,024 × 3
hidden layers in the encoders and decoders, Nes-
terov Accelerated Gradient as the optimizer with
a momentum of 0.99, dropout rate of 0.2, initial
learning rate of 0.25, and minimum learning rate
of 10−4. A beam size of 10 was used during model
inference. No spell checker was incorporated in
the present study, either as pre-processing or post-
processing.

4.3 Experimental results

Table 3 shows the results for our baseline and
models trained with different fluency-filtered ar-
tificial errors. The model trained on the baseline
data, which include 0.6M correct sentence pairs,
performs the worst in terms of recall (18.85%), be-
cause the large proportion of the same sentences
makes the model too conservative to make correc-
tions. Indeed, true positive for the baseline model
is only 749, which is about half of that in the low-
est fluency condition. All the four models with ar-
tificial errors obtain higher recall (over 26%), but
at the expense of precision. The model with error
sentences that have the highest fluency among can-
didate sentences, in particular, drops over 15% in
precision compared to the baseline, making it the
worst model in terms of F0.5 (42.86%). Error sen-
tences with the lowest fluency lead to the highest
recall (32.96%) and second highest F0.5 (48.68%)
among all the models, while the model in the me-
dian fluency condition achieves a good balance be-
tween precision drop and recall gain, resulting in
the highest F0.5 (49.03%).

https://competitions.codalab.org/competitions/21922
https://competitions.codalab.org/competitions/21922
https://github.com/pytorch/fairseq


90

Prec. Recall F0.5
Original

(Baseline)
65.93 18.85 43.97

Random 55.67 27.61 46.26
Highest 50.44 26.77 42.86
Median 57.69 30.64 49.03
Lowest 55.27 32.96 48.68

Table 3: Performance of multi-layer CNNs for GEC on
W&I+LOCNESS test set with different error data from
different fluency filtering.

5 Conclusions and Future Work

The goal of the current study was to explore how
the fluency of artificial error sentences can affect
the performance of grammatical error correction.
We greedily generated all possible error sentences
using the similar context window approach as in
Felice (2016), and then selected among candidate
sentences based on fluency score (sentence per-
plexity). As predicted, the model trained with
artificial error sentences of highest fluency per-
formed even worse than the baseline model with
a large proportion of correct sentence pairs. Mod-
els in both lowest and median fluency conditions
performed significantly better than the other three
models. The former one achieved the highest re-
call, while the latter one was more balanced with
the highest F0.5. These results indicate that flu-
ency filtering can be used a means to select high-
quality artificial error sentences for grammatical
error detection and correction.

Although the present study just focused on flu-
ency and ignored error probability, the two factors
are not mutually exclusive. Rather, combining the
two approaches may generate even better artifi-
cial errors. Additionally, fluency filtering is not
restricted to the context window approach to er-
ror generation, it can be part of the machine back-
translation approach to help select among the N
best translations.

One limitation of the current study is that we
only generated one error at a time for each sen-
tence. In the training data, the 0.5M error sen-
tences contain 1.3M errors, which means that on
average each sentence has 2.4 errors. Our next
step is to explore using fluency filtering to ensure
the quality of artificial multi-error sentences and
to see if this can boost GEC performance even fur-
ther.

References
Christopher Bryant, Mariano Felice, and Ted Briscoe.

2017. Automatic Annotation and Evaluation of Er-
ror Types for Grammatical Error Correction. In Pro-
ceedings of the 55th Annual Meeting of the Associa-
tion for Computational Linguistics (Volume 1: Long
Papers), pages 793–805, Vancouver, Canada. Asso-
ciation for Computational Linguistics.

Shamil Chollampatt and Hwee Tou Ng. 2018. A Mul-
tilayer Convolutional Encoder-Decoder Neural Net-
work for Grammatical Error Correction. In Pro-
ceedings of The Thirty-Second AAAI Conference
on Artificial Intelligence (AAAI-18), New Orleans,
Louisiana.

Mariano Felice. 2016. Artificial error generation
for translation-based grammatical error correction.
Technical Report UCAM-CL-TR-895, University of
Cambridge, Computer Laboratory.

Tao Ge, Furu Wei, and Ming Zhou. 2018. Fluency
Boost Learning and Inference for Neural Grammati-
cal Error Correction. In Proceedings of the 56th An-
nual Meeting of the Association for Computational
Linguistics (Volume 1: Long Papers), pages 1055–
1065, Melbourne, Australia. Association for Com-
putational Linguistics.

Kenneth Heafield. 2011. KenLM: Faster and Smaller
Language Model Queries. In Proceedings of the
Sixth Workshop on Statistical Machine Translation,
pages 187–197, Edinburgh, Scotland. Association
for Computational Linguistics.

Sudhanshu Kasewa, Pontus Stenetorp, and Sebastian
Riedel. 2018. Wronging a Right: Generating Bet-
ter Errors to Improve Grammatical Error Detec-
tion. In Proceedings of the 2018 Conference on
Empirical Methods in Natural Language Process-
ing, pages 4977–4983, Brussels, Belgium. Associ-
ation for Computational Linguistics.

Ilya Sutskever, Oriol Vinyals, and Quoc V. Le. 2014.
Sequence to Sequence Learning with Neural Net-
works. In Z. Ghahramani, M. Welling, C. Cortes,
N. D. Lawrence, and K. Q. Weinberger, editors, Ad-
vances in Neural Information Processing Systems
27, pages 3104–3112. Curran Associates, Inc.

Ziang Xie, Guillaume Genthial, Stanley Xie, Andrew
Ng, and Dan Jurafsky. 2018. Noising and Denois-
ing Natural Language: Diverse Backtranslation for
Grammar Correction. In Proceedings of the 2018
Conference of the North American Chapter of the
Association for Computational Linguistics: Human
Language Technologies, Volume 1 (Long Papers),
pages 619–628, New Orleans, Louisiana. Associa-
tion for Computational Linguistics.

Zheng Yuan and Mariano Felice. 2013. Constrained
grammatical error correction using statistical ma-
chine translation. In Proceedings of the Seven-
teenth Conference on Computational Natural Lan-
guage Learning: Shared Task, pages 52–61, Sofia,

http://aclweb.org/anthology/P17-1074
http://aclweb.org/anthology/P17-1074
http://arxiv.org/abs/1801.08831
http://arxiv.org/abs/1801.08831
http://arxiv.org/abs/1801.08831
http://www.cl.cam.ac.uk/techreports/UCAM-CL-TR-895.pdf
http://www.cl.cam.ac.uk/techreports/UCAM-CL-TR-895.pdf
https://www.aclweb.org/anthology/P18-1097
https://www.aclweb.org/anthology/P18-1097
https://www.aclweb.org/anthology/P18-1097
http://www.aclweb.org/anthology/W11-2123
http://www.aclweb.org/anthology/W11-2123
https://www.aclweb.org/anthology/D18-1541
https://www.aclweb.org/anthology/D18-1541
https://www.aclweb.org/anthology/D18-1541
http://papers.nips.cc/paper/5346-sequence-to-sequence-learning-with-neural-networks.pdf
http://papers.nips.cc/paper/5346-sequence-to-sequence-learning-with-neural-networks.pdf
https://doi.org/10.18653/v1/N18-1057
https://doi.org/10.18653/v1/N18-1057
https://doi.org/10.18653/v1/N18-1057
https://www.aclweb.org/anthology/W13-3607
https://www.aclweb.org/anthology/W13-3607
https://www.aclweb.org/anthology/W13-3607


91

Bulgaria. Association for Computational Linguis-
tics.

Wei Zhao, Liang Wang, Kewei Shen, Ruoyu Jia, and
Jingming Liu. 2019. Improving Grammatical Er-
ror Correction via Pre-Training a Copy-Augmented
Architecture with Unlabeled Data. In Proceedings
of the 2019 Conference of the North {A}merican
Chapter of the Association for Computational Lin-
guistics: Human Language Technologies, Volume
1 (Long and Short Papers), pages 156–165, Min-
neapolis, Minnesota. Association for Computational
Linguistics.

https://www.aclweb.org/anthology/N19-1014
https://www.aclweb.org/anthology/N19-1014
https://www.aclweb.org/anthology/N19-1014

