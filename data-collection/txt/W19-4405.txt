



















































Analysing Rhetorical Structure as a Key Feature of Summary Coherence


Proceedings of the Fourteenth Workshop on Innovative Use of NLP for Building Educational Applications, pages 46–51
Florence, Italy, August 2, 2019. c©2019 Association for Computational Linguistics

46

Analysing Rhetorical Structure as a
Key Feature of Summary Coherence

Jan Šnajder1 Tamara Sladoljev-Agejev2 Svjetlana Kolić-Vehovec3
1 University of Zagreb, Faculty of Electrical Engineering and Computing, TakeLab

2 University of Zagreb, Faculty of Economics and Business
3 University of Rijeka, Faculty of Humanities and Social Sciences, Department of Psychology

jan.snajder@fer.hr, tagejev@efzg.hr, skolic@ffri.hr

Abstract
We present a model for automatic scoring
of coherence based on comparing the rhetor-
ical structure (RS) of college student sum-
maries in L2 (English) against expert sum-
maries. Coherence is conceptualised as a con-
struct consisting of a rhetorical relation and its
arguments. Comparison with expert-assigned
scores shows that RS scores correlate with
both cohesion and coherence. Furthermore,
RS scores improve the accuracy of a regression
model for cohesion score prediction.

1 Introduction

Assessment of text quality may benefit from auto-
matic scoring as it is cognitively demanding and
often requires much expertise (Rahimi et al., 2017),
especially in college-level expository writing. One
of the key aspects of text quality is writing co-
herence (Crossley and McNamara, 2010) which
reflects students’ ability to connect ideas in their
mind and to convey the same message in essays or
summaries (Halliday and Hasan, 2014).

Existing approaches to text quality predomi-
nantly focus on surface measures for assessment
(e.g., number of cohesive devices), which some-
times have little relation either to human judgment,
e.g., text length (Mintz et al., 2014), or to text-
specific meaning (Rahimi et al., 2017). However,
automatic scoring of coherence should ideally pro-
vide clear and reliable feedback (Burstein et al.,
2013) based on features with cognitive validity,
e.g., (Loukina et al., 2015).

One way to meet such requirements is to define
coherence as the identification of relations between
the text’s ideas (Rapp et al., 2007). Such a defini-
tion may best be analysed in summaries in which
the key ideas of the source text are integrated into
a rhetorical structure (RS).

In cognitive terms, writing summaries is an exer-
cise in reading-for-understanding (RU) (Sabatini

et al., 2013) and gist reasoning (Chapman and Mu-
dar, 2013). The result of such processes is the
macrostructure of the source text constructed in
the reader’s mind (Louwerse and Graesser, 2005),
which consists of concepts and propositions, their
mutual relations (Sanders and Noordman, 2000),
and relations with prior knowledge. Coherent sum-
maries should express the intention of the source
text (Hobbs, 1993) using linguistic devices (cohe-
sion), which makes summarisation also a reading-
to-write (RW) task (Delaney, 2008). Moreover,
summaries have a distinctive feature for annotation:
a largely shared knowledge base, i.e., the source
text(s) known both to the writer and to the rater(s),
which assists raters in their judgment and helps
develop a reliable text-specific scoring tool.

In this paper we present a model for automatic
scoring of summaries based on analysing a rhetori-
cal structure of a student’s summary compared to
that of reference summaries. Our starting point is
coherence conceptualized as a construct consist-
ing of three elements: a rhetorical relation and its
two arguments. We posit that expository text has
a rhetorical structure (RS) consisting of a series
of text-specific rhetorical segments, the majority
of which will be conveyed in a coherent summary
if full text-level comprehension is achieved. The
model uses a discourse parser to extract rhetorical
structures of summaries, and then compares simi-
larity of these structures. We show that the scores
produced by the model correlate with the expert-
assigned cohesion and coherence scores as well as
with surface indices of cohesion. We also show that
the model-produced scores can be used to improve
cohesion score prediction.

2 Related Work

Automatic assessment of text quality can include
content, language accuracy, sophistication and style



47

as well as sometimes overlapping features such as
topic similarity, focus, coherence, cohesion, read-
ability, or text organisation and development, e.g.,
(Pitler et al., 2010; Yannakoudakis and Briscoe,
2012; Guo et al., 2013; Rahimi et al., 2015; Gao
et al., 2018). Coherence is a broad concept assessed
by different automatic tools, e.g., (Higgins et al.,
2004; Yannakoudakis and Briscoe, 2012; Burstein
et al., 2013). Scoring measures may include surface
features such as word or text length or the number
of pronouns and connectives, e.g., (Yannakoudakis
and Briscoe, 2012; MacArthur et al., 2018), which
may also be contextualised, e.g., (Pitler et al., 2010).
Source overlaps may also be used in scoring such as
overlapping n-grams in summaries (Madnani et al.,
2013), and semantic similarity (e.g,. LSA) may
provide information on relatedness between words,
e.g., lexical chaining (Somasundaran et al., 2014),
sentences (Foltz et al., 1998; Higgins et al., 2004;
Higgins and Burstein, 2007), or larger text sections
(Crossley and McNamara, 2010). Both types of fea-
tures (surface and LSA) are encompassed by Coh-
Metrix (Graesser et al., 2004; McNamara et al.,
2014), a comprehensive computational tool using a
range of measures to grasp cognitive aspects of text
analysis. Moreover, inter-sentential coherence can
be measured using syntax-based entity grids (Barzi-
lay and Lapata, 2008), for example, to distinguish
between high- and low-coherence essays (Burstein
et al., 2010), or analysing discourse relations (Pitler
and Nenkova, 2008; Skoufaki, 2009).

In order to improve the predictive value of auto-
matic assessment, scoring measures are often com-
bined. For example, Pitler and Nenkova (2008) use
entity grids, syntactic features, discourse relations
(Prasad et al., 2008), vocabulary, and length fea-
tures. Yannakoudakis and Briscoe (2012) examine
different measures and find that semantic similar-
ity is the best addition to lexical and grammati-
cal features. Somasundaran et al. (2014) combine
lexical chains, grammar, word usage, mechanics,
and RST discourse relations (Mann and Thomp-
son, 1988) in L1 and L2 texts, while Higgins et al.
(2004) use semantic similarity together with dis-
course structure to measure relatedness to the essay
question and between discourse segments. More
recently, Sladoljev-Agejev and Šnajder (2017) com-
bine reference-based and linguistic features (e.g.,
Coh-Metrix, BLEU, ROUGE) to predict coherence
and cohesion in college student summaries in L2.

The coherence assessment model presented here

relies on summaries as a RU/RW task which con-
sists of detecting and conveying the RS of the
source text. Similar to Higgins et al. (2004), we
use semantic similarity and rhetorical structure
to assess coherence of student summaries against
summaries written by experts. While Higgins et
al. measured the coherence of functional discourse
segments (e.g., thesis, conclusion) via semantic
similarity between their respective sentences, in
our study coherence is measured via similarity be-
tween rhethorical structures. Our intuition relies
on the establishment of source macrostructure as a
coherence-building exercise during reading. Such
an approach appears to be cognitively valid and
may ensure meaningful feedback both in terms of
comprehension and writing skills development or
assessment. Our model is constrained by the source
content, so we also compare its performance to
cohesion features provided by Coh-Metrix in (Sla-
doljev-Agejev and Šnajder, 2017) to assess generic
RW skills.

3 Summary Scoring Model

The summary scoring model works by comparing
the RS of a student summary against the rhetori-
cal structures of one or more reference summaries.
The model produces a score that indicates to what
extent the two structures overlap.

Discourse parsing. To extract the rhetorical rela-
tions and their arguments, we use the PDTB-style
parser of Lin et al. (2014), a state-of-the-art, end-to-
end parser which labels instances of both implicit
and explicit relations as well as their argument
spans. The PDTB relation labels are organized
in a three-level hierarchy of “sense tags” (Prasad
et al., 2008). The parser recognizes the first two
levels: relation Category (e.g., Comparison) and
Type (e.g., Contrast). The end-to-end performance
of the parser, measured as F1-score under partial
argument matching, is 48%. The output of this
step is, for each summary S, a set of rhetorical
relations {ri}i, where ri = (li, a1i , a2i ) is a rela-
tion of class/type label li, while a1i and a

2
i are text

segments corresponding to its arguments.

Comparing rhetorical structures. When com-
paring the similarity of summaries’ rhetorical struc-
tures, we want the model to assign high scores to
pairs of summaries that have many rhetorical re-
lations in common. Of course, we cannot expect
the arguments of rhetorical relations to be literally



48

the same, but, if two relations of the same label
are to be considered equivalent, their correspond-
ing arguments should be highly semantically sim-
ilar. We formalize this intuition by defining the
weight wij between a pair of rhetorical relations
ri = (li, a

1
i , a

2
i ) and rj = (lj , a

1
j , a

2
j ) as:

wij =

{
1
2

(
s(a1i , a

1
j ) + s(a

2
i , a

2
j )
)

if li = lj ,
0 otherwise.

where s(·, ·) is the semantic similarity between two
text segments. In line with much of recent work,
we rely on additive compositionality of word em-
beddings, and compute the semantic similarity as
the cosine similarity between averaged word em-
beddings of the two segments. We use the 300-
dimensional skip-gram word embeddings built on
the Google-News corpus (Mikolov et al., 2013).1

To compute the overlap score between a pair of
summaries S1 and S2, each consisting of a set of
rhetorical relations, we use the maximum bipartite
graph matching algorithm (Kuhn, 1955). The graph
edges represent pairs of relations (ri, rj), ri ∈ S1,
rj ∈ S2, weighted by wij . Let n1 = |S1| and
n2 = |S2| be the number of rhetorical relations
in S1 and S2, respectively, and m the maximum
matching score between S1 and S2. We define the
precision (P ) and recall (R) of the match as:

P =
m−max(0, n1 − n2)

n1

R =
m−max(0, n2 − n1)

n2

The intuition is that precision is maximized if all
relations from S1 are perfectly matched to some
relations from S2, and conversely for recall. The
F1-score is the harmonic mean of P and R. Finally,
we compute the F1-score of a student’s summary S
as the mean of pairwise F1-scores between S and
both reference summaries.

4 Evaluation

Dataset. For model evaluation, we adopt the
dataset of (Sladoljev-Agejev and Šnajder, 2017).
The dataset consists of a total of 225 text-present
summaries (c. 300 words) of two articles written by
114 first-year business undergraduates in English
as L2 (mostly upper intermediate and advanced).
Both articles (c. 900 words each) were taken from
The Economist, a business magazine. Two expert

1https://code.google.com/archive/p/word2vec/

raters used a 4-point analytic scale (grades 0–3) to
assess the summaries in terms of coherence (RU)
and cohesion (RW). The scales were quantified
by defining the number of coherence and cohe-
sion breaks. Descriptors for each grade included
expressions such as “meaningfully related ideas”
and “logical sequencing” (for coherence) and “lin-
guistically connected text segments” (for cohesion).
Inter-rater reliability (weighted kappas) was 0.69
for coherence and 0.83 for cohesion. The raters
discussed and agreed on all the grades although
reliability was adequate. As expected, we observe
a strong correlation between coherence and cohe-
sion scores (Spearman correlation coefficient of
0.64). All the summaries were checked for spelling
and basic grammar. For the two articles from The
Economist, two experts with considerable experi-
ence with business texts in English wrote 300-word
summaries following the same instruction as the
students.

Comparison with expert-assigned scores. To
assess the validity of the summary scoring model,
we measure the correlations of P, R, and F1 scores
produced by the model against expert-provided
coherence and cohesion scores, considering both
Class and Type levels of PDTB relations. Table 1
shows the results. We can make several observa-
tions. First, while all the scores correlate posi-
tively with both cohesion and coherence, correla-
tion for coherence is consistently lower, possibly
due to the role of the raters’ prior knowledge, which
is unavailable to the model (also note that inter-
annotator agreement is lower for coherence than
for cohesion). Second, correlation for Type level
is consistently lower than for Class level, which
can probably be traced to the PDTB parser being
less accurate on Type-level relations. Lastly, we
note that the highest correlation with both cohe-
sion and coherence is achieved with the F1-score
of the Class level model. These results suggest that
the proposed summary scoring model is at least
partially successful in modeling both cohesion and
coherence – and this in spite of the unavoidable
errors of the PDTB parser and errors in similarity
computations.

Comparison with Coh-Metrix indices. As
mentioned in the introduction, a number of studies
have used Coh-Metrix cohesion indices as predic-
tors of both cohesion and coherence. In partic-
ular, Sladoljev-Agejev and Šnajder (2017) found



49

Class Level Type Level

P@C R@C F1@C P@T R@T F1@T

Chs 0.218 0.320 0.444 0.207 0.295 0.426
Chr 0.105 0.297 0.381 0.071 0.257 0.337

Table 1: Spearman correlation coefficients between
expert-assigned cohesion (Chs) and coherence (Chr)
scores and model-produced scores (P, R, and F1) for
Class and Type levels of PDTB connectives. The high-
est correlations for cohesion and correlation are shown
in boldface. All correlations except those shown in ital-
ics are statistically significant (p<0.05).

Expert scores Model scores

Coh-Metrix index Chs Chr F1@C F1@T

CNCAdd 0.375 0.229 0.545 0.495
CNCLogic 0.453 0.330 0.492 0.409
CNCAll 0.408 0.289 0.477 0.421
CRFAOa 0.430 0.405 0.342 0.320
CRFCWOa 0.416 0.364 0.278 0.278

Table 2: Spearman correlation coefficients between
Coh-Metrix indices (connectives – CNC, referential co-
hesion – CRF) and expert-assigned cohesion (Chs) and
coherence (Chr) scores as well as model-produced F1
scores at Class level (F1@C) and Type level (F1@T)
of PDTB connectives. The highest correlations in each
column are shown in boldface. All correlations are sta-
tistically significant (p<0.05).

modest correlation between expert-assigned coher-
ence/cohesion and indices for connectives (addi-
tive connectives – CNCAdd, logical connectives –
CNCLogic, and all connectives – CNCAll) and ref-
erential cohesion indices (mean of noun/pronoun
overlaps between two sentences – CRFAOa, and
content word overlap – CRFCWOA). It is there-
fore interesting to investigate to what extent these
surface-level predictors correlate with the scores
of our model. Table 2 gives Spearman correla-
tion coefficients between the Coh-Metrix indices
and expert-provided scores as well as the Class-
and Type-level F1-scores of the model. The Coh-
Metrix indices correlate positively with both the
expert-assigned scores and the scores of our model.
However, while CNCLogic and CRFOAo indices
mostly correlate with the expert-assigned cohesion
and coherence scores, respectively, the scores of
our model mostly correlate with the CNCAdd in-
dex.

Supervised scoring. Following Sladoljev-Age-
jev and Šnajder (2017), we frame the automated

Model / Features Chs Chr

Baseline 0.369 0.361
Ridge / CM 0.489 0.409
Ridge / RS 0.476∗ 0.419
Ridge / CM+RS 0.511∗ 0.414

Table 3: Accuracy of cohesion (Chs) and coherence
(Chr) scores predictions for the baseline and ridge
regression models with Coh-Metrix (CM), rhetorical
structure (RS), and combined (CM+RS) feature sets.
The best results are shown in bold. The “*” indicates a
statistically significant difference to baseline (p<0.05,
Wilcoxon signed-rank test). The differences between
regression models with the CM feature set and models
with RS and CM+RS feature sets are not statistically
significant.

scoring as a multivariate regression task and use
two regression models, one for cohesion and the
other for coherence, each trained to predict the
expert-assigned score on a 0–3 scale. We use an
L2-regularized linear regression model (ridge re-
gression)2 and consider three sets of features: (1)
five Coh-Metrix CNC and CRF indices (“CM”),
(2) the F1-scores of the summary scoring model
computed at Class and Type levels (“RS”), and (3)
a combination of the two (“CM+RS”). We evaluate
the models using a nested 10×5 cross-validation:
the models’ performance is measured in terms of
accuracy averaged over the five outer folds, after
rounding the predictions to closest integers and lim-
iting the scores to the 0–3 range. All the features
are z-scored on the train set, and the same trans-
formation is applied on the test set. As baselines,
we use the rounded average of the expert-assigned
scores.

Table 3 shows the results. We can make three
main observations. Firstly, cohesion models out-
perform the corresponding coherence models. Sec-
ondly, the only two models for which the differ-
ences against the baseline are statistically signifi-
cant are the two cohesion models that use RS. This
suggests that our model does provide useful signals
for predicting expert-assigned cohesion scores. In
the absence of statistical significance, the results
for coherence are inconclusive, though we observe
a similar trend.

5 Conclusion

We have described a model for coherence scoring
based on a simple definition of coherence in line

2We use the implementation of Pedregosa et al. (2011).



50

with cognitive theories of text comprehension. The
model produces scores that correlate with expert-
assigned scores and improve the cohesion predic-
tion of a regression model: a model that uses rhetor-
ical structure scores as features yields a statistically
significant improvement over the baseline of aver-
aged expert-assigned scores. The proposed model
could provide a basis for meaningful feedback in
summaries and other similar tasks, and may also
be used for measuring gist reasoning in case of a
shared knowledge base between the rater and the
examinee.

Acknowledgments

We thank Višnja Kabalin-Borenić for her contribu-
tion in the assessment of summaries analysed in
this work.

References
Regina Barzilay and Mirella Lapata. 2008. Modeling

local coherence: An entity-based approach. Compu-
tational Linguistics, 34(1):1–34.

Jill Burstein, Joel Tetreault, and Slava Andreyev. 2010.
Using entity-based features to model coherence in
student essays. In Human language technologies:
The 2010 annual conference of the North American
chapter of the Association for Computational Lin-
guistics, pages 681–684.

Jill Burstein, Joel Tetreault, and Martin Chodorow.
2013. Holistic discourse coherence annotation
for noisy essay writing. Dialogue & Discourse,
4(2):34–52.

Sandra Bond Chapman and Raksha Anand Mudar.
2013. Discourse gist: A window into the brains
complex cognitive capacity. Discourse Studies,
15(5):519–533.

Scott Crossley and Danielle McNamara. 2010. Cohe-
sion, coherence, and expert evaluations of writing
proficiency. In Proceedings of the Annual Meeting
of the Cognitive Science Society, volume 32.

Yuly Asencion Delaney. 2008. Investigating the
reading-to-write construct. Journal of English for
Academic Purposes, 7(3):140–150.

Peter W Foltz, Walter Kintsch, and Thomas K Lan-
dauer. 1998. The measurement of textual coherence
with latent semantic analysis. Discourse processes,
25(2-3):285–307.

Yanjun Gao, Patricia M Davies, and Rebecca J Passon-
neau. 2018. Automated content analysis: A case
study of computer science student summaries. In
Proceedings of the Thirteenth Workshop on Innova-
tive Use of NLP for Building Educational Applica-
tions, pages 264–272.

Arthur C. Graesser, Danielle S. McNamara, Max M.
Louwerse, and Zhiqiang Cai. 2004. Coh-Metrix:
Analysis of text on cohesion and language. Behav-
ior Research Methods, 36(2):193–202.

Liang Guo, Scott A. Crossley, and Danielle S. McNa-
mara. 2013. Predicting human judgments of essay
quality in both integrated and independent second
language writing samples: A comparison study. As-
sessing Writing, 18(3):218–238.

Michael Alexander Kirkwood Halliday and Ruqaiya
Hasan. 2014. Cohesion in English. Routledge.

Derrick Higgins and Jill Burstein. 2007. Sentence simi-
larity measures for essay coherence. In Proceedings
of the 7th International Workshop on Computational
Semantics, pages 1–12.

Derrick Higgins, Jill Burstein, Daniel Marcu, and Clau-
dia Gentile. 2004. Evaluating multiple aspects of co-
herence in student essays. In Proceedings of the Hu-
man Language Technology Conference of the North
American Chapter of the Association for Computa-
tional Linguistics: HLT-NAACL 2004.

Jerry R Hobbs. 1993. Intention, information, and struc-
ture in discourse: A first draft. In Burning Issues
in Discourse, NATO Advanced Research Workshop,
pages 41–66. Citeseer.

Harold W Kuhn. 1955. The hungarian method for the
assignment problem. Naval research logistics quar-
terly, 2(1-2):83–97.

Ziheng Lin, Hwee Tou Ng, and Min-Yen Kan. 2014. A
PDTB-styled end-to-end discourse parser. Natural
Language Engineering, 20(2):151–184.

Anastassia Loukina, Klaus Zechner, Lei Chen, and
Michael Heilman. 2015. Feature selection for au-
tomated speech scoring. In Proceedings of the Tenth
Workshop on Innovative Use of NLP for Building Ed-
ucational Applications, pages 12–19.

M. M. Louwerse and A. C. Graesser. 2005.
Macrostructure. Encyclopedia of Language
and Linguistics.

Charles A MacArthur, Amanda Jennings, and Zoi A
Philippakos. 2018. Which linguistic features predict
quality of argumentative writing for college basic
writers, and how do those features change with in-
struction? Reading and Writing, pages 1–22.

Nitin Madnani, Jill Burstein, John Sabatini, and Tenaha
O’Reilly. 2013. Automated scoring of a summary-
writing task designed to measure reading compre-
hension. In Proceedings of the Eighth Workshop on
Innovative Use of NLP for Building Educational Ap-
plications, pages 163–168. Association for Compu-
tational Linguistics.

William C Mann and Sandra A Thompson. 1988.
Rhetorical structure theory: Toward a functional the-
ory of text organization. Text-Interdisciplinary Jour-
nal for the Study of Discourse, 8(3):243–281.



51

Danielle S. McNamara, Arthur C. Graesser, Philip M.
McCarthy, and Zhiqiang Cai. 2014. Automated eval-
uation of text and discourse with Coh-Metrix. Cam-
bridge University Press.

Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Cor-
rado, and Jeff Dean. 2013. Distributed representa-
tions of words and phrases and their compositional-
ity. In Advances in neural information processing
systems, pages 3111–3119.

Lisa Mintz, Dan Stefanescu, Shi Feng, Sidney D’Mello,
and Arthur Graesser. 2014. Automatic assessment
of student reading comprehension from short sum-
maries. In Educational Data Mining 2014.

Fabian Pedregosa, Gaël Varoquaux, Alexandre Gram-
fort, Vincent Michel, Bertrand Thirion, Olivier
Grisel, Mathieu Blondel, Peter Prettenhofer, Ron
Weiss, Vincent Dubourg, Jake Vanderplas, Alexan-
dre Passos, David Cournapeau, Matthieu Brucher,
Matthieu Perrot, and Edouard Duchesnay. 2011.
Scikit-learn: Machine learning in Python. Journal
of Machine Learning Research, 12(Oct):2825–2830.

Emily Pitler, Annie Louis, and Ani Nenkova. 2010.
Automatic evaluation of linguistic quality in multi-
document summarization. In Proceedings of the
48th annual meeting of the Association for Compu-
tational Linguistics, pages 544–554. Association for
Computational Linguistics.

Emily Pitler and Ani Nenkova. 2008. Revisiting read-
ability: A unified framework for predicting text qual-
ity. In Proceedings of the conference on empirical
methods in natural language processing, pages 186–
195. Association for Computational Linguistics.

Rashmi Prasad, Nikhil Dinesh, Alan Lee, Eleni Milt-
sakaki, Livio Robaldo, Aravind K Joshi, and Bon-
nie L Webber. 2008. The Penn discourse treebank
2.0. In LREC. Citeseer.

Zahra Rahimi, Diane Litman, Richard Correnti, Elaine
Wang, and Lindsay Clare Matsumura. 2017. As-
sessing students’ use of evidence and organization
in response-to-text writing: Using natural language
processing for rubric-based automated scoring. In-
ternational Journal of Artificial Intelligence in Edu-
cation, 27(4):694–728.

Zahra Rahimi, Diane J Litman, Elaine Wang, and
Richard Correnti. 2015. Incorporating coherence of
topics as a criterion in automatic response-to-text as-
sessment of the organization of writing. In Proceed-
ings of the Tenth Workshop on Innovative Use of
NLP for Building Educational Applications, pages
20–30. Association for Computational Linguistics.

David N Rapp, Paul van den Broek, Kristen L Mc-
Master, Panayiota Kendeou, and Christine A Espin.
2007. Higher-order comprehension processes in
struggling readers: A perspective for research and in-
tervention. Scientific studies of reading, 11(4):289–
312.

John Sabatini, Tenaha O’Reilly, and Paul Deane. 2013.
Preliminary reading literacy assessment framework:
Foundation and rationale for assessment and system
design. ETS Research Report Series, 2013(2).

Ted JM Sanders and Leo GM Noordman. 2000. The
role of coherence relations and their linguistic
markers in text processing. Discourse processes,
29(1):37–60.

Sophia Skoufaki. 2009. An exploratory application
of rhetorical structure theory to detect coherence er-
rors in L2 English writing: Possible implications
for automated writing evaluation software. Interna-
tional Journal of Computational Linguistics & Chi-
nese Language Processing, Volume 14, Number 2,
June 2009-Special Issue on Computer Assisted Lan-
guage Learning, 14(2).

Tamara Sladoljev-Agejev and Jan Šnajder. 2017. Us-
ing analytic scoring rubrics in the automatic assess-
ment of college-level summary writing tasks in l2.
In Proceedings of the Eighth International Joint
Conference on Natural Language Processing (Vol-
ume 2: Short Papers), pages 181–186.

Swapna Somasundaran, Jill Burstein, and Martin
Chodorow. 2014. Lexical chaining for measur-
ing discourse coherence quality in test-taker essays.
In Proceedings of COLING 2014, the 25th Inter-
national conference on computational linguistics:
Technical papers, pages 950–961.

Helen Yannakoudakis and Ted Briscoe. 2012. Model-
ing coherence in ESOL learner texts. In Proceedings
of the Seventh Workshop on Building Educational
Applications Using NLP, pages 33–43. Association
for Computational Linguistics.


