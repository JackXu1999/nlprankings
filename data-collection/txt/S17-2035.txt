



















































Jmp8 at SemEval-2017 Task 2: A simple and general distributional approach to estimate word similarity


Proceedings of the 11th International Workshop on Semantic Evaluations (SemEval-2017), pages 230–234,
Vancouver, Canada, August 3 - 4, 2017. c©2017 Association for Computational Linguistics

Jmp8 at SemEval-2017 Task 2: A simple and general distributional
approach to estimate word similarity

Josué Melka
LIASD - Université Paris 8

jmelka@ai.univ-paris8.fr

Gilles Bernard
LIASD - Université Paris 8
gb@ai.univ-paris8.fr

Abstract

We have built a simple corpus-based sy-
stem to estimate words similarity in mul-
tiple languages with a count-based appro-
ach. After training on Wikipedia corpora,
our system was evaluated on the multi-
lingual subtask of SemEval-2017 Task 2
and achieved a good level of performance,
despite its great simplicity. Our results
tend to demonstrate the power of the dis-
tributional approach in semantic similarity
tasks, even without knowledge of the un-
derlying language. We also show that di-
mensionality reduction has a considerable
impact on the results.

1 Introduction

Despite the crucial importance of semantic simila-
rity in NLP, the vast majority of experiments have
been conducted on the English language, which
raises the question whether the developed approa-
ches can be generalized.

SemEval-2017 Task 2 provides us with a fra-
mework for evaluating semantic representations in
multiple languages and compare them. We focus
here on the multilingual subtask, which consists
of five monolingual word similarity datasets.

Our submission is based on the well known sta-
tistical approach which uses bag-of-contexts re-
presentation of words in a vector space model.
We run two versions of our system, the first one
using a direct sparse representation and the se-
cond one with compressed dense representation
(detailed below). This second version was evalua-
ted after the official evaluation deadline, and pro-
duced superior results as will appear below.

We briefly describe the multilingual subtask in
section 2. Next, in section 3, we detail our sy-
stem and its parameters. The results are presented

and analyzed in section 4, and then we conclude
in section 5.

2 Task description

Camacho-Collados et al. (2017) describes the task
as follows:

Given a pair of words, the task is
to automatically measure their seman-
tic similarity. All pairs in our datasets
are scored according to a [0-4] simila-
rity scale, where 4 denotes that the two
words are synonymous and 0 indicates
that they are completely dissimilar.

Multilingual word similarity This
subtask provides five monolingual word
similarity datasets in English, German,
Italian, Spanish and Farsi. The sub-
task is intended to test not only mono-
lingual approaches but also multilingual
and language-independent techniques.

The individual score of the systems is defined
by the authors as the harmonic mean of Pearson
and Spearman correlations on the corresponding
dataset. However, as our analysis lead us to take
into account the separate behavior of both measu-
res, we did not focus here on the final score.

3 Our system

Our system is corpus-based only, and uses a few
well known ideas from the distributional approach
in word semantic similarity.

3.1 The training corpus
We have used the Wikipedia corpus taken from
https://sites.google.com/site/rmyeid/

projects/polyglot as recommended by the
authors of the task in order to compare fairly with
other corpus-based systems.

230



Some properties of these corpora are given in
Table 1. It should be noted that no preprocessing
was made on the corpora documents.

Table 1: Statistics of the Wikipedia corpora
size lines words uniques

en 8.7G 70.9M 1 392M 5.3M
de 3.5G 32.2M 482M 5.7M
it 1.8G 11.9M 265M 1.9M
es 2.1G 14.8M 338M 2.3M

3.2 Language model
Our model is count-based, and we have used the
same parameters for all languages.

First, we counted occurrences of alphabetic
words in each corpus (barring words with non alp-
habetic characters), and kept the 100,000 most fre-
quent for context words and the 300,000 most fre-
quent as vocabulary. These arbitrary limits are jus-
tified by physical constraints of memory and time.

Contexts
The context we use for a given word wi is defined
as wi−L, . . . , wi−1, wi+1, . . . , wi+L. In this work
we use a context length L = 4.

For each context word wi−k we apply a weight
of 1k to give a stronger influence to nearest words
in the context.

Then we built a word-context matrix by sum-
ming the weighted context occurrences for each
word in the vocabulary.

PPMI
Pointwise Mutual Information (PMI) introduced
by Ward Church and Hanks (1989) is one of the
popular ways to measure the semantic association
between words and their textual context as defined
above, and can be easily estimated from the word-
context matrix M , as:

PMI(wi, cj) = log
Mij

∑
k

∑
n Mkn∑

k Mik
∑

n Mnj

Bullinaria and Levy (2007) argue that the Posi-
tive PMI (PPMI) outperforms the other variants of
PMI for semantic similarity tasks.

PPMI(w, c) = max (0, PMI(w, c))

Vector compression
A common approach inspired by Latent Semantic
Analysis (Deerwester et al., 1990) is to use trunca-
ted singular value decomposition (SVD) to reduce

the vector dimensionality. The SVD factorization
of the PPMI matrix is MPPMI = U · Σ · V >, and
can be truncated to the first d components.

In our experiments, we have used the symme-
tric variant proposed by Levy et al. (2015) using
only the Ud matrix for representing word vectors,
and we chose d = 500. Randomized SVD (Halko
et al., 2009) from Scikit-learn was used to produce
the matrix decomposition.

3.3 Evaluating word pairs similarity
Basically, we have used the cosine similarity to
compare the word vectors.

Multi-word expressions
While some special features of the present task
(such as domain-specific terms and named enti-
ties) do not necessarily require a special adapta-
tion, multi-word expressions cannot be compared
directly with single-word vectors. For this rea-
son, we simply sum the vectors of every word in
a multi-word expression to give the corresponding
vector estimation.

See in section 4.3 a discussion about the results
of this method.

Out of vocabulary words
Some words of the test dataset do not appear in
our vocabulary, and we choose to give the me-
dian value .5 to the similarity of pairs including
one or more out of vocabulary (OOV) words. Ta-
ble 2 shows the numbers of such pairs for each
language.

Table 2: pairs with OOV words
pairs %

en 21 4.2
de 68 13.6
it 24 4.8
es 17 3.4

A closer look shows that some words (such as
“Brexit” or “DeepMind”) were missed because
they appeared too recently to be in our corpus, ot-
hers because they contain non-alphabetic charac-
ters (like apostrophes or dashes), and the main part
because they were not frequent enough to have
been retained in our vocabulary.

The fact that the German language presents a
higher OOV rate is not surprising, due to the mor-
phological richness of this language. This can
be improved by using a larger vocabulary and/or

231



using morphological approaches such as Boja-
nowski et al. (2016).

4 Results

We report the results obtained with our system
(Jmp8) on four different languages in Table 3 and
Table 4. Note that, due to a bug correction, the data
is not exactly the same as in the official evaluation,
though the magnitudes are similar. Moreover, the
results of our second version have not been sub-
mitted for the challenge due to lack of time.

Luminoso is the best performer on this subtask,
and HCCL is, to our knowledge, the best system
which is corpus based and uses the shared trai-
ning corpora. NASARI (Camacho-Collados et al.,
2016) is the baseline proposed by the authors of
the task.

Table 3: Pearson correlation
en de it es

Luminoso 0.783 0.7 0.728 0.732
HCCL 0.675 0.576 0.635 0.688
Jmp8-1 0.516 0.286 0.436 0.455
Jmp8-2 0.687 0.578 0.652 0.685
NASARI 0.683 0.513 0.597 0.602

Table 4: Spearman correlation
en de it es

Luminoso 0.795 0.7 0.754 0.754
HCCL 0.7 0.614 0.668 0.715
Jmp8-1 0.652 0.502 0.635 0.643
Jmp8-2 0.731 0.604 0.695 0.727
NASARI 0.681 0.514 0.594 0.597

4.1 Comparison of both Jmp8 versions

Jmp8-1 simply uses the PPMI matrix to compute
similarities with sparse vectors of 100,000 com-
ponents, while the second version, Jmp8-2, is ba-
sed on a truncated SVD matrix which represents
words as dense vectors of 500 components.

It turns out that Jmp8-1 produces a very im-
portant difference between Pearson and Spearman
correlations, while Jmp8-2 provides more consis-
tent results, and also better ones. In fact, Jmp8-2
outperforms NASARI in all cases, and achieves
similar performance to HCCL.

Interpretation

The important difference between both Jmp8 ver-
sions is explained by the fact that Jmp8-1 presents
a non-linear relationship with the gold standard, as
depicted in Figure 1.

Figure 1: Comparison of both versions (en)

0

1

2

3

4
gold
version 1

0 100 200 300 400 500
word pairs

0

1

2

3

4
sc

o
re

gold
version 2

4.2 Language independence

These results suggest that our method (especially
the second version) generalizes well for different
languages, even if there are differences.

Our interpretation is that the English language
is favored because its corpus is the biggest; Ita-
lian and Spanish results indicate that our appro-
ach remains interesting even with a much smal-
ler corpus. The results are significantly lower for
the German language despite the size of its corpus
(this is true for all methods mentioned here), pre-
sumably because there are many out of vocabulary
words.

This is supported by the fact that we found the
correlations to be much higher (comparable to Ita-
lian and Spanish values), if, instead of using .5 me-
dian value for OOV pairs, we simply deleted these
pairs from the dataset.

With SVD approach, these deletions improved
correlations by about 20% (p = 0.65 and s =
0.67) for German and by less than 3% for other
languages. Note that these numbers should be ta-

232



ken with caution because missing data can intro-
duce bias.

4.3 Multi-word influence

To show the effect of the number of words in ex-
pressions on global performances, we calculated
and plotted for each language the correlation with
the gold standard separately for each number of
words by expression (Figure 2). The size of the ci-
rcles indicates the amount of pairs in each group.

For multiple reasons, it is somewhat difficult
to analyze the influence of multi-word expressi-
ons on the overall performance. However, as one
can expect, our simplistic method appears to de-
grade performance when the number of words in
expressions increases. It is rather surprising that
our results are still quite good despite this nega-
tive influence, but this should be mitigated by the
number of pairs involved.

Figure 2: Multi-word influence in a pair (Jmp8-2)

1

2

3

en

Pearson Spearman

1

2

3

de

1

2

3

it

1 2 3

1

2

3

es

1 2 3

0.52

0.56

0.60

0.64

0.68

0.72

0.76

0.80

Another approach such as phrasing (Mikolov
et al., 2013) can be applied as well to address this
issue.

4.4 Comparison with WordSim-353 dataset

Leviant and Reichart (2015) has translated the
WordSim-353 dataset into several languages 1,
and we have tested our system with the similarity
subset (Agirre et al., 2009), which contains 201
pairs of words. It should be noted that WS353 uses
single words only, and we have very few OOV

1http://technion.ac.il/~ira.leviant/
Multilingual_SimLex_Wordsim.html

words (0 in English, 4 in German and 1 in Italian).
Table 5 shows our results.

Table 5: Correlations on WS353-sim dataset
en de it

Jmp8-1
P 0.608 0.461 0.447
S 0.667 0.547 0.592

Jmp8-2
P 0.722 0.654 0.600
S 0.737 0.676 0.602

The gap between Pearson and Spearman corre-
lations is still present for Jmp8-1, confirming that
sparse vectors do not perform well in semantic si-
milarity tasks.

Another interesting point is that the correlations
for the German language are significantly higher
than for the present task, which can be explained
by the lower OOV rate in this dataset, as discussed
above (section 4.2).

Surprisingly, contrary to the results of the pre-
sent task, Italian results are significantly lower
than for the other languages, though less so than
were German results in the present task. We have
not yet found a good explanation for this, as it is
clear that OOV words are out of the picture.

5 Conclusion

We have shown that it is possible to achieve a good
level of performance in multilingual word seman-
tic similarity task with a rather simple but genera-
list approach.

While one should take these results with cau-
tion, some important conclusions can be drawn
from our work. First, it is confirmed that the raw
sparse PPMI representation is less adapted to simi-
larity measure than the compressed dense SVD re-
presentation. Second, a specific approach needs to
be developed to address multi-word expressions,
although the vector addition seems to work mode-
rately well for 2-words. And last, we have seen
that OOV pairs can be problematic for a systema-
tic comparison between systems and/or languages.

The ability of our method to handle multiple
languages seems good, but needs further investi-
gation in those directions with more extensive test
sets in order to yield a refined analysis.

Finally, we are considering the combination
of this method with other approaches, both from
word embeddings methods and from supervised
techniques.

233



References
Eneko Agirre, Enrique Alfonseca, Keith Hall, Jana

Kravalova, Marius Pasca, and Aitor Soroa. 2009. A
study on similarity and relatedness using distributio-
nal and wordnet-based approaches. In Proceedings
of Human Language Technologies: The 2009 An-
nual Conference of the North American Chapter of
the Association for Computational Linguistics. As-
sociation for Computational Linguistics, pages 19–
27. http://aclweb.org/anthology/N09-1003.

Piotr Bojanowski, Edouard Grave, Armand Joulin, and
Tomas Mikolov. 2016. Enriching word vectors
with subword information. CoRR abs/1607.04606.
http://arxiv.org/abs/1607.04606.

John A Bullinaria and Joseph P Levy. 2007. Extracting
semantic representations from word co-occurrence
statistics: A computational study. Behavior rese-
arch methods 39(3):510–526.

Jose Camacho-Collados, Mohammad Taher Pilehvar,
Nigel Collier, and Roberto Navigli. 2017. Semeval-
2017 task 2: Multilingual and cross-lingual se-
mantic word similarity. In Proceedings of the
11th International Workshop on Semantic Evalua-
tion (SemEval-2017). Association for Computatio-
nal Linguistics, Vancouver, Canada, pages 15–26.
http://www.aclweb.org/anthology/S17-2002.

José Camacho-Collados, Mohammad Taher Pilehvar,
and Roberto Navigli. 2016. Nasari: Integrating ex-
plicit knowledge and corpus statistics for a multilin-
gual representation of concepts and entities. Artifi-
cial Intelligence 240:36–64.

Scott Deerwester, Susan T Dumais, George W Furnas,
Thomas K Landauer, and Richard Harshman. 1990.
Indexing by latent semantic analysis. Journal of the
American society for information science 41(6):391.

Nathan Halko, Per-Gunnar Martinsson, and Joel A.
Tropp. 2009. Finding structure with randomness:
Probabilistic algorithms for constructing approxi-
mate matrix decompositions. CoRR abs/0909.4061.
http://arxiv.org/abs/0909.4061.

Ira Leviant and Roi Reichart. 2015. Separated by
an Un-common Language: Towards Judgment Lan-
guage Informed Vector Space Modeling. CoRR
abs/1508.00106. http://arxiv.org/abs/1508.00106.

Omer Levy, Yoav Goldberg, and Ido Dagan. 2015. Im-
proving distributional similarity with lessons lear-
ned from word embeddings. Transactions of the As-
sociation of Computational Linguistics 3:211–225.
http://aclweb.org/anthology/Q15-1016.

Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Cor-
rado, and Jeff Dean. 2013. Distributed representa-
tions of words and phrases and their compositiona-
lity. In Advances in neural information processing
systems. pages 3111–3119.

F. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel,
B. Thirion, O. Grisel, M. Blondel, P. Prettenho-
fer, R. Weiss, V. Dubourg, J. Vanderplas, A. Pas-
sos, D. Cournapeau, M. Brucher, M. Perrot, and
E. Duchesnay. 2011. Scikit-learn: Machine learning
in Python. Journal of Machine Learning Research
12:2825–2830.

Peter D Turney and Patrick Pantel. 2010. From fre-
quency to meaning: Vector space models of se-
mantics. Journal of artificial intelligence research
37:141–188.

Kenneth Ward Church and Patrick Hanks. 1989.
Word association norms, mutual information, and
lexicography. In 27th Annual Meeting of
the Association for Computational Linguistics.
http://aclweb.org/anthology/P89-1010.

A Supplemental Material

We made our source code and outputs available at
https://github.com/yoch/jmp8

234


