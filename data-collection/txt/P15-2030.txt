



















































Non-Linear Text Regression with a Deep Convolutional Neural Network


Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics
and the 7th International Joint Conference on Natural Language Processing (Short Papers), pages 180–185,

Beijing, China, July 26-31, 2015. c©2015 Association for Computational Linguistics

Non-Linear Text Regression with a Deep Convolutional Neural Network

Zsolt Bitvai
University of Sheffield, UK
z.bitvai@shef.ac.uk

Trevor Cohn
University of Melbourne, Australia
t.cohn@unimelb.edu.au

Abstract

Text regression has traditionally been
tackled using linear models. Here we
present a non-linear method based on a
deep convolutional neural network. We
show that despite having millions of pa-
rameters, this model can be trained on
only a thousand documents, resulting in a
40% relative improvement over sparse lin-
ear models, the previous state of the art.
Further, this method is flexible allowing
for easy incorporation of side information
such as document meta-data. Finally we
present a novel technique for interpreting
the effect of different text inputs on this
complex non-linear model.

1 Introduction

Text regression involves predicting a real world
phenomenon from textual inputs, and has been
shown to be effective in domains including elec-
tion results (Lampos et al., 2013), financial risk
(Kogan et al., 2009) and public health (Lampos
and Cristianini, 2010). Almost universally, the
text regression problem has been framed as lin-
ear regression, with the modelling innovation fo-
cussed on effective regression, e.g., using Lasso
penalties to promote feature sparsity (Tibshirani,
1996).1 Despite their successes, linear models are
limiting: text regression problems will often in-
volve complex interactions between textual inputs,
thus requiring a non-linear approach to properly
capture such phenomena. For instance, in mod-
elling movie revenue conjunctions of features are
likely to be important, e.g., a movie described as
‘scary’ is likely to have different effects for chil-
dren’s versus adult movies. While these kinds of

1Some preliminary work has shown strong results for non-
linear text regression using Gaussian Process models (Lam-
pos et al., 2014), however this approach has not been shown
to scale to high dimensional inputs.

features can be captured using explicit feature en-
gineering, this process is tedious, limited in scope
(e.g., to conjunctions) and – as we show here –
can be dramatically improved by representational
learning as part of a non-linear model.

In this paper, we propose an artificial neu-
ral network (ANN) for modelling text regression.
In language processing, ANNs were first pro-
posed for probabilistic language modelling (Ben-
gio et al., 2003), followed by models of sentences
(Kalchbrenner et al., 2014) and parsing (Socher
et al., 2013) inter alia. These approaches have
shown strong results through automatic learning
dense low-dimensional distributed representations
for words and other linguistic units, which have
been shown to encode important aspects of lan-
guage syntax and semantics. In this paper we
develop a convolutional neural network, inspired
by their breakthrough results in image process-
ing (Krizhevsky et al., 2012) and recent applica-
tions to language processing (Kalchbrenner et al.,
2014; Kim, 2014). These works have mainly fo-
cused on ‘big data’ problems with plentiful train-
ing examples. Given their large numbers of pa-
rameters, often in the millions, one would expect
that such models can only be effectively learned
on very large datasets. However we show here
that a complex deep convolution network can be
trained on about a thousand training examples, al-
though careful model design and regularisation is
paramount.

We consider the problem of predicting the fu-
ture box-office takings of movies based on reviews
by movie critics and movie attributes. Our ap-
proach is based on the method and dataset of Joshi
et al. (2010), who presented a linear regression
model over uni-, bi-, and tri-gram term frequency
counts extracted from reviews, as well as movie
and reviewer metadata. This problem is especially
interesting, as comparatively few instances are
available for training (see Table 1) while each in-

180



train dev test total

# movies 1147 317 254 1718
# reviews per movie 4.2 4.0 4.1 4.1
# sentences per movie 95 84 82 91
# words per movie 2879 2640 2605 2794

Table 1: Movie review dataset (Joshi et al., 2010).

stance (movie) includes a rich array of data includ-
ing the text of several critic reviews from various
review sites, as well as structured data (genre, rat-
ing, actors, etc.) Joshi et al. found that regression
purely from movie meta-data gave strong predic-
tive accuracy, while text had a weaker but comple-
mentary signal. Their best results were achieved
by domain adaptation whereby text features were
conjoined with a review site identifier. Inspired by
Joshi et al. (2010) our model also operates over n-
grams, 1 ≤ n ≤ 3, and movie metadata, albeit
using an ANN in place of their linear model. We
use word embeddings to represent words in a low
dimensional space, a convolutional network with
max-pooling to represent documents in terms of
n-grams, and several fully connected hidden lay-
ers to allow for learning of complex non-linear in-
teractions. We show that including non-linearities
in the model is crucial for accurate modelling, pro-
viding a relative error reduction of 40% (MAE)
over their best linear model. Our final contribu-
tion is a novel means of model interpretation. Al-
though it is notoriously difficult to interpret the pa-
rameters of an ANN, we show a simple method of
quantifying the effect of text n-grams on the pre-
diction output. This allows for identification of the
most important textual inputs, and investigation of
non-linear interactions between these words and
phrases in different data instances.

2 Model

The outline of the convolutional network is shown
in Figure 1. We have n training examples of the
form {bi, ri, yi}ni=1, where bi is the meta data as-
sociated with movie i, yi is the target gross week-
end revenue, and ri is a collection of ui number of
reviews, ri = {xj , tj}uij=1 where each review has
review text xj and a site id tj . We concatenate all
the review texts di = (x1,x2, ...,xu) to form our
text input (see part I of Figure 1).

To acquire a distributed representation of the
text, we look up the input tokens in a pretrained
word embedding matrix E with size |V |×e, where

Embeddings, E

   •        •         •       •
This movie is the best 
one of the century, the 
actors are exceptional 
...

Boston Globe, r1= {x1,t1}

New York Times ru

m

u

|V|

This
movie

is
the

best
one

...

   •        •        •        •
...

e

co
nc

ate
na

te

   •        •        •       •

   •        •        •       •
   •        •        •       •

   •        •        •        •

(I)

lookup

Site ids, t
Document matrix, D

e

   0 1 1 0 0

This
movie

is
the

best
one

...

m

ReLu

g

ReLu ReLu

W(1)
W(2) W(3)

h h h

g
g

(II)

m m-1
m-2max 

pool

(III)

p(1) p(2) p(3)

Meta data

genre,
rating, 
famous 
actors 
...

b

ReLu

g
h

ReLu
g

h

ReLu

g
h

g
prediction $ Revenue

convolution

fully connected

error

J

f y

W1

W2

W3

w4

a1

o1=a2

o2=a3

o3=a4

(IV)

(V)

uni-grams

C(1)

bi-grams tri-grams

C(2)
C(3)

   •        •         •       •

   •        •        •        •
...

   •        •        •       •
   •        •        •       •

   •        •        •        •

   •        •        •       •

fully connected

fully connected

fully connected

Figure 1: Outline of the network architecture.

|V | is the size of our vocabulary and e is the em-
bedding dimensionality. This gives us a dense
document matrix Di,j = Edi,j with dimensions
m × e where m is the number of tokens in the
document.

Since the length of text documents vary, in part
II we apply convolutions with width one, two and
three over the document matrix to obtain a fixed
length representation of the text. The n-gram con-
volutions help identify local context and map that
to a new higher level feature space. For each fea-
ture map, the convolution takes adjacent word em-
beddings and performs a feed forward computa-
tion with shared weights over the convolution win-
dow. For a convolution with width 1 ≤ q ≤ m this
is

S(q)i,· = (Di,·,Di+1,·, ...,Di+q−1,·)

C(q)i,· = 〈S(q)i,· ,W(q)〉

where S(q)i,· is q adjacent word embeddings con-
catenated, and C(q) is the convolution output ma-
trix with (m−q+1) rows after a linear transforma-
tion with weights W(q). To allow for a non-linear

181



transformation, we make use of rectified linear ac-
tivation units, H(q) = max(C(q), 0), which are
universal function approximators. Finally, to com-
press the representation of text to a fixed dimen-
sional vector while ensuring that important infor-
mation is preserved and propagated throughout the
network, we apply max pooling over time, i.e. the
sequence of words, for each dimension, as shown
in part III,

p
(q)
j = maxH

(q)
·,j

where p(q)j is dimension j of the pooling layer for
convolution q, and p is the concatenation of all
pooling layers, p = (p(1),p(2), ...,p(q)).

Next, we perform a series of non-linear trans-
formations to the document vector in order to pro-
gressively acquire higher level representations of
the text and approximate a linear relationship in
the final output prediction layer. Applying multi-
ple hidden layers in succession can require expo-
nentially less data than mapping through a single
hidden layer (Bengio, 2009). Therefore, in part
IV, we apply densely connected neural net layers
of the form ok = h(g(ak,Wk)) where ak is the
input and ok = ak+1 is the output vector for layer
k, g is a linear transformation function 〈ak,Wk〉,
and h is the activation function, i.e. rectified linear
seen above. l = 3 hidden layers are applied be-
fore the final regression layer to produce the out-
put f = g(ol,wl+1) in part V.

The mean absolute error is measured between
the predictions f and the targets y, which is more
permissible to outliers than the squared error. The
cost J is defined as

J =
1
n

n∑
v=1

|fv − yv|.

The network is trained with stochastic gradient de-
scent and the Ada Delta (Zeiler, 2012) update rule
using random restarts. Stochastic gradient descent
is noisier than batch training due to a local esti-
mation of the gradient, but it can start converging
much faster. Ada Delta keeps an exponentially de-
caying history of gradients and updates in order to
adapt the learning rate for each parameter, which
partially smooths out the training noise. Regulari-
sation and hyperparmeter selection are performed
by early stopping on the development set. The
size of the vocabulary is 90K words. Note that
10% of our lexicon is not found in the embeddings

Model Description MAE($M)

Baseline mean 11.7
Linear Text 8.0
Linear Text+Domain+POS 7.4
Linear Meta 6.0
Linear Text+Meta 5.9
Linear Text+Meta+Domain+Deps 5.7

ANN Text 6.3
ANN Text+Domain 6.0
ANN Meta 3.9
ANN Text+Meta 3.4
ANN Text+Meta+Domain 3.4

Table 2: Experiment results on test set. Linear
models by (Joshi et al., 2010).

pretrained on Google News. Those terms are ini-
tialised with random small weights. The model
has around 4 million weights plus 27 million tun-
able word embedding parameters.

Structured data Besides text, injecting meta
data and domain information into the model likely
provides additional predictive power. Combin-
ing text with structured data early in the network
fosters joint non-linear interaction in subsequent
hidden layers. Hence, if meta data b is present,
we concatenate that with the max pooling layer
a1 = (p,b) in part III. Domain specific infor-
mation t is appended to each n-gram convolution
input (S(q)i,· , t) in part II, where tz = 1z indicates
whether domain z has reviewed the movie.2 This
helps the network bias the convolutions, and thus
change which features get propagated in the pool-
ing layer.

3 Results

The results in Table 2 show that the neural net-
work performs very well, with around 40% im-
provement over the previous best results (Joshi et
al., 2010). Our dataset splits are identical, and we
have accurately reproduced the results of their lin-
ear model. Non-linearities are clearly helpful as
evidenced by the ANN Text model beating the bag
of words Linear Text model with a mean absolute
test error of 6.0 vs 8.0. Moreover, simply using
structured data in the ANN Meta beats all the Lin-
ear models by a sizeable margin. Further improve-
ments are realised through the inclusion of text,
giving the lowest error of 3.4. Note that Joshi et al.
(2010) preprocessed the text by stemming, down-

2Alternatively, site information can be encoded with one-
hot categorical variables.

182



Model Description MAE($M)

fixed word2vec embeddings 3.4*
tuned word2vec embeddings 3.6
fixed random embeddings 3.6
tuned random embeddings 3.8

uni-grams 3.6
uni+bi-grams 3.5
uni+bi+tri-grams 3.4*
uni+bi+tri+four-grams 3.6

0 hidden layer 6.3
1 hidden layer 3.9
2 hidden layers 3.5
3 hidden layers 3.4*
4 hidden layers 3.6

Table 3: Various alternative configurations, based
on the ANN Text+Meta model. The asterisk (∗)
denotes the settings in the ANN Text+Meta model.

casing, and discarding feature instances that oc-
curred in fewer than five reviews. In contrast, we
did not perform any processing of the text or fea-
ture engineering, apart from tokenization, instead
learning this automatically.3

We find that both text and meta data con-
tain complementary signals with some informa-
tion overlap between them. This confirms the find-
ing of Bitvai and Cohn (2015) on another text re-
gression problem. The meta features alone almost
achieve the best results whereas text alone per-
forms worse but still well above the baseline. For
the combined model, the performance improves
slightly. In Table 3 we can see that contrary to ex-
pectations, fine tuning the word embeddings does
not help significantly compared to keeping them
fixed. Moreover, randomly initialising the embed-
dings and fixing them performs quite well. Fine
tuning may be a challenging optimisation due to
the high dimensional embedding space and the
loss of monolingual information. This is further
exacerbated due to the limited supervision signal.

One of the main sources of improvement ap-
pears to come from the non-linearity applied to the
neural network activations. To test this, we try us-
ing linear activation units in parts II and IV of the
network. Composition of linear functions yields a
linear function, and therefore we recover the lin-
ear model results. This is much worse than the
model with non-linear activations. Changing the
network depth, we find that the model performs
much better with a single hidden layer than with-

3Although we do make use of pretrained word embed-
dings in our text features.

out any, while three hidden layers are optimal. For
the weight dimensions we find square 1058 dimen-
sional weights to perform the best. The ideal num-
ber of convolutions are three with uni, bi and tri-
grams, but unigrams alone perform only slightly
worse, while taking a larger n-gram window n > 3
does not help. Average and sum pooling perform
comparatively well, while max pooling achieves
the best result. Note that sum pooling recovers a
non-linear bag-of-words model. With respect to
activation functions, both ReLU and sigmoid work
well.

Model extensions Multi task learning with task
identifiers, ANN Text+Domain, does improve the
ANN Text model. This suggests that the tendency
by certain sites to review specific movies is in it-
self indicative of the revenue. However this im-
provement is more difficult to discern with the
ANN Text+Meta+Domain model, possibly due to
redundancy with the meta data. An alternative ap-
proach for multi-task learning is to have a sepa-
rate convolutional weight matrix for each review
site, which can learn site specific characteristics
of the text. This can also be achieved with site
specific word embedding dimensions. However
neither of these methods resulted in performance
improvements. In addition, we experimented with
applying a hierarchical convolution over reviews
in two steps with k-max pooling (Kalchbrenner et
al., 2014), as well as parsing sentences recursively
(Socher et al., 2013), but did not observe any im-
provements.

For optimisation, both Ada Grad and Steepest
Gradient Descent had occasional problems with
local minima, which Ada Delta was able to es-
cape more often. In contrast to earlier work (Kim,
2014), applying dropout on the final layer did not
improve the validation error. The optimiser mostly
found good parameters after around 40 epochs
which took around 30 minutes on a NVidia Kepler
Tesla K40m GPU.

Model interpretation Next we perform anal-
ysis to determine which words and phrases in-
fluenced the output the most in the ANN Text
model. To do so, we set each phrase input to
zeros in turn and measure the prediction differ-
ence for each movie across the test set. We re-
port the min/max/average/count values in Table
4. We isolate the effect of each n-gram by mak-
ing sure the uni, bi and trigrams are independent,

183



transformers2

pinkpanther2

objective

mylifeinruins

inglouriousbasterds

informers
greatbuckhoward

grace

fastandfurious4

bobfunk

budget > $15M

budget < $15M
$0

$3.6K

$140K

$5.2M

$200M

R
e
v
e
n
u
e

Figure 2: Projection of the last hidden layer of
test movies using t-SNE. Red means high and blue
means low revenue. The cross vs dot symbols in-
dicate a production budget above or below $15M.

i.e. we process “Hong Kong” without zeroing
“Hong” or “Kong”. About 95% of phrases re-
sult in no output change, including common sen-
timent words, which shows that text regression is
a different problem to sentiment analysis. We see
that words related to series “# 2”, effects, awards
“praise”, positive sentiment “intense”, locations,
references “Batman”, body parts “chest”, and oth-
ers such as “plot twist”, “evil”, and “cameo” re-
sult in increased revenue by up to $5 million.
On the other hand, words related to independent
films “vérité”, documentaries “the period”, for-
eign film “English subtitles” and negative senti-
ment decrease revenue. Note that the model has
identified structured data in the unstructured text,
such as related to revenue of prequels “39 mil-
lion”, crew members, duration “15 minutes in”,
genre “[sci] fi”, ratings, sexuality, profanity, re-
lease periods “late 2008 release”, availability “In
selected theaters” and themes. Phrases can be
composed, such as “action unfolds” amplifies “ac-
tion”, and “cautioned” is amplified by “strongly
cautioned”. “functional” is neutral, but “func-
tional at best” is strongly negative. Some words
exhibit both positive and negative impacts depend-
ing on the context. This highlights the limitation
of a linear model which is unable to discover these
non-linear relationships. “13 - year [old]” is posi-
tive in New in Town, a romantic comedy and nega-
tive in Zombieland, a horror. The character strings
“k /” (mannerism of reviewer), “they’re” (unique
apostrophe), “&#39” (encoding error) are high im-
pact and unique to specific review sites, showing
that the model indirectly uncovers domain infor-
mation. This can explain the limited gain that can
be achieved via multi task learning. Last, we have

Top 5 positive phrases min max avg #

sequel 20 4400 2300 28
flick 0 3700 1600 22
k / 1500 3600 2200 3
product 10 3400 1800 27
predecessor 22 3400 1400 13

Top 5 negative phrases min max avg #

Mildly raunchy lang. -3100 -3100 -3100 1
( Under 17 -2500 1 -570 75
Lars von -2400 -900 -1500 3
talk the language -2200 -2200 -2200 1
. their English -2200 -2200 -2200 1

Selected phrases min max avg #

CGI 145 3000 1700 28
action -7 1500 700 105
summer 3 1200 560 42
they’re 3 1300 530 68
1950s 10 1600 500 17
hit 8 950 440 72
fi -15 340 160 26
Cage 7 95 45 28
Hong Kong -440 40 -85 11
requires acc. parent -780 1 -180 77
English -850 6 -180 41
Sundance Film Festival -790 3 -180 10
written and directed -750 -3 -220 19
independent -990 -2 -320 12
some strong language -1600 6 -520 13

Table 4: Selected phrase impacts on the predic-
tions in $ USD(K) in the test set, showing min,
max and avg change in prediction value and num-
ber of occurrences (denoted #). Periods denote ab-
breviations (language, accompanying).

plotted the last hidden layer of each test set movie
with t-SNE (Van der Maaten and Hinton, 2008).
This gives a high level representation of a movie.
In Figure 2 it is visible that the test set movies can
be discriminated into high and low revenue groups
and this also correlates closely with their produc-
tion budget.

4 Conclusions

In this paper, we have shown that convolutional
neural networks with deep architectures greatly
outperform linear models even with very little su-
pervision, and they can identify key textual and
numerical characteristics of data with respect to
predicting a real world phenomenon. In addition,
we have demonstrated a way to intuitively inter-
pret the model. In the future, we will investi-
gate ways for automatically optimising the hyper-
parameters of the network (Snoek et al., 2012) and
various extensions to recursive or hierarchical con-
volutions.

184



References
Yoshua Bengio, Réjean Ducharme, Pascal Vincent, and

Christian Janvin. 2003. A neural probabilistic lan-
guage model. The Journal of Machine Learning Re-
search, 3:1137–1155.

Yoshua Bengio. 2009. Learning deep architectures for
AI. Foundations and Trends in Machine Learning,
2(1):1–127.

Zsolt Bitvai and Trevor Cohn. 2015. Predicting peer-
to-peer loan rates using Bayesian non-linear regres-
sion. In Proceedings of the 29th AAAI conference
on Artificial Intelligence, pages 2203–2210.

Mahesh Joshi, Dipanjan Das, Kevin Gimpel, and
Noah A Smith. 2010. Movie reviews and revenues:
An experiment in text regression. In Human Lan-
guage Technologies: The 2010 Annual Conference
of the North American Chapter of the Association
for Computational Linguistics, pages 293–296.

Nal Kalchbrenner, Edward Grefenstette, and Phil Blun-
som. 2014. A convolutional neural network for
modelling sentences. Proceedings of the 52nd An-
nual Meeting of the Association for Computational
Linguistics.

Yoon Kim. 2014. Convolutional neural networks for
sentence classification. In Proceedings of the Con-
ference on Empirical Methods in Natural Language
Processing.

Shimon Kogan, Dimitry Levin, Bryan R Routledge,
Jacob S Sagi, and Noah A Smith. 2009. Pre-
dicting risk from financial reports with regression.
In Proceedings of Human Language Technologies:
The 2009 Annual Conference of the North American
Chapter of the Association for Computational Lin-
guistics, pages 272–280.

Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hin-
ton. 2012. ImageNet classification with deep con-
volutional neural networks. In Advances in Neural
Information Processing Systems, pages 1097–1105.

Vasileios Lampos and Nello Cristianini. 2010. Track-
ing the flu pandemic by monitoring the social web.
In Cognitive Information Processing, pages 411–
416.

Vasileios Lampos, Daniel Preotiuc-Pietro, and Trevor
Cohn. 2013. A user-centric model of voting inten-
tion from social media. In Proc 51st Annual Meet-
ing of the Association for Computational Linguis-
tics, pages 993–1003.

Vasileios Lampos, Nikolaos Aletras, D Preoţiuc-Pietro,
and Trevor Cohn. 2014. Predicting and characteris-
ing user impact on twitter. In Proceedings of the
14th Conference of the European Chapter of the As-
sociation for Computational Linguistics, pages 405–
–413.

Jasper Snoek, Hugo Larochelle, and Ryan P Adams.
2012. Practical Bayesian optimization of machine
learning algorithms. In Advances in Neural Infor-
mation Processing Systems, pages 2951–2959.

Richard Socher, Alex Perelygin, Jean Y Wu, Jason
Chuang, Christopher D Manning, Andrew Y Ng,
and Christopher Potts. 2013. Recursive deep mod-
els for semantic compositionality over a sentiment
treebank. In Proceedings of the Conference on Em-
pirical Methods in Natural Language Processing,
pages 1631–1642.

Robert Tibshirani. 1996. Regression shrinkage and se-
lection via the lasso. Journal of the Royal Statistical
Society. Series B (Methodological), pages 267–288.

Laurens Van der Maaten and Geoffrey Hinton. 2008.
Visualizing data using t-SNE. Journal of Machine
Learning Research, 9(2579-2605):85.

Matthew D Zeiler. 2012. Adadelta: an adaptive learn-
ing rate method. arXiv preprint arXiv:1212.5701.

185


