











































MedNorm: A Corpus and Embeddings for Cross-terminology Medical Concept Normalisation


Proceedings of the 4th Social Media Mining for Health Applications (#SMM4H) Workshop & Shared Task, pages 31–39
Florence, Italy, August 2, 2019. c©2019 Association for Computational Linguistics

31

MedNorm: A Corpus and Embeddings for Cross-terminology Medical
Concept Normalisation

Maksim Belousov
School of Computer Science

The University of Manchester
United Kingdom

William G. Dixon
Centre for Epidemiology

Versus Arthritis
The University of Manchester

United Kingdom
{maksim.belousov, will.dixon, g.nenadic}@manchester.ac.uk

Goran Nenadic
School of Computer Science

The University of Manchester
United Kingdom

Abstract

The medical concept normalisation task aims
to map textual descriptions to standard termi-
nologies such as SNOMED-CT or MedDRA.
Existing publicly available datasets annotated
using different terminologies cannot be simply
merged and utilised, and therefore become less
valuable when developing machine learning-
based concept normalisation systems. To ad-
dress that, we designed a data harmonisation
pipeline and engineered a corpus of 27,979
textual descriptions simultaneously mapped to
both MedDRA and SNOMED-CT, sourced
from five publicly available datasets across
biomedical and social media domains. The
pipeline can be used in the future to integrate
new datasets into the corpus and also could be
applied in relevant data curation tasks. We also
described a method to merge different termi-
nologies into a single concept graph preserv-
ing their relations and demonstrated that rep-
resentation learning approach based on ran-
dom walks on a graph can efficiently encode
both hierarchical and equivalent relations and
capture semantic similarities not only between
concepts inside a given terminology but also
between concepts from different terminolo-
gies. We believe that making a corpus and em-
beddings for cross-terminology medical con-
cept normalisation available to the research
community would contribute to a better under-
standing of the task.

1 Introduction

The medical concept normalisation task aims to
assign a corresponding identifier from a standard
terminology to text descriptions. Depending on
the domain, descriptions may vary from formal
medical jargon terms (e.g. “Dizziness”) to more
informal and colloquial expressions that rather
explain how the patient feels (e.g. “everything
that surrounds me is circling or rolling”, “kept

bumping into things”). There are multiple termi-
nologies of medical concepts that are commonly
used for mapping, such as SNOMED-CT (Sys-
tematized Nomenclature of Medicine - Clinical
Terms) (Stearns et al., 2001) and MedDRA (Med-
ical Dictionary for Regulatory Activities) (Brown
et al., 1999). The Unified Medical Language Sys-
tem (UMLS) (Schuyler et al., 1993) integrates
concepts from various biomedical vocabularies
and lexicons, including SNOMED-CT and Med-
DRA. Each concept is represented by its Con-
cept Unique Identifier (CUI). Clinicians choose
the most suitable terminology based on their par-
ticular case or application. Hence, when creating
corpora with annotated medical concepts, there
is no general agreement on which terminology
to use or which annotation guidelines to follow.
Also, variety of available concepts in terminolo-
gies (e.g. over 70,000 lowest level terms in Med-
DRA and over 350,000 concepts in SNOMED-
CT) makes it harder to achieve high agreement be-
tween annotators. For instance, annotators could
pick a different level of hierarchy (e.g. Fatigue
or more specific term Tiredness) or inconsistently
pick from similarly described concepts when a de-
scription is vague (e.g. Insomnia and Poor qual-
ity sleep). As a result, such variable annota-
tions cannot be simply merged and utilised, and
therefore, such data become less valuable when
developing machine learning-based concept nor-
malisation systems. To combine and harmonise
datasets, we need to tackle various problems asso-
ciated with providing cross-terminology mappings
between concepts and resolving inconsistent an-
notations from different datasets. Due to hetero-
geneous structures of medical terminologies, sim-
ple one-to-one mappings may be insufficient to
match and compare concepts. Therefore, it is also
necessary to harmonise and align terminologies
and find a way to represent medical concepts con-



32

sidering relations between them regardless of the
terminology. Representation learning techniques
have shown promising results in encoding struc-
tural information about nodes in graphs and het-
erogeneous networks (Perozzi et al., 2014; Grover
and Leskovec, 2016; Dong et al., 2017; Hamil-
ton et al., 2017), however this requires integrat-
ing various medical terminologies into a single
graph or network, which remains challenging. Re-
cently, it has been also demonstrated that termi-
nological embeddings can capture semantic simi-
larities and are especially well-suited for biomed-
ical ontology alignment (Kolyvakis et al., 2018).
In this paper, we present a MedNorm corpus con-
sisting of 27,979 textual descriptions (phrases)
simultaneously mapped to both MedDRA and
SNOMED-CT, that have been sourced from five
publicly available datasets across biomedical and
social media domains. To combine them, we
designed a data harmonisation pipeline that can
be re-used in the future to integrate new datasets
into the corpus or applied in relevant annotation
and data processing tasks. Also, we have de-
scribed a method to merge multiple medical ter-
minologies into a single network preserving both
terminology-specific and cross-terminology rela-
tions. We demonstrated that representation learn-
ing approach based on random walks on a graph
can efficiently encode equivalent and hierarchi-
cal relations and capture semantic similarities not
only between concepts inside a given terminol-
ogy, but also between concepts from different ter-
minologies. Finally, we have provided an analy-
sis of the corpus, investigated textual and concep-
tual similarities between utilised datasets and also
analysed cross-terminology medical concept em-
beddings. The corpus and concept embeddings1

as well as the harmonisation pipeline2 are publicly
available. Making such resources available to the
research community aimed to contribute to a better
understanding of the task.

2 Corpus material

2.1 Target medical ontologies
Relationships between medical concepts are en-
coded differently in medical ontologies. In this
section we describe the two ontologies that have
been used for mappings in the corpus.

SNOMED-CT (SCT) is a structured clinical
1https://dx.doi.org/10.17632/b9x7xxb9sz.1
2https://github.com/mbelousov/MedNorm-corpus

terminology that enables consistent documenta-
tion and annotation of clinical data. There are both
hierarchical and semantic (e.g. finding site, asso-
ciated morphology) relations between terms. Each
term can have multiple hierarchical paths with dif-
ferent lengths, so their specific level in the hierar-
chy is undefined.

MedDRA is a hierarchical terminology with
five levels (from very specific to very general) de-
signed for encoding adverse drug events for reg-
ulatory affairs. The most specific level is Lowest
Level Terms (LLT) and refers how a concept might
be reported in practice (e.g. “Feeling queasy”).
Each LLT is linked to exactly one Preferred Term
(PT), a distinct descriptor for a symptom, sign,
disease diagnosis, indication, procedure or medi-
cal history characteristic (e.g. “Nausea”). Related
PTs are grouped into High Level Terms (HLTs,
e.g. “Nausea and vomiting symptoms”), then into
High Level Group Terms (HLGTs, e.g. “Gas-
trointestinal signs and symptoms”), and finally
into “System Organ Classes” (SOC, e.g. “Gas-
trointestinal disorders”). Note that single HLT
can be linked to more than one HLGTs, and as
a result, PT will have more than one hierarchical
path to SOC.

2.2 Source corpora
The data for the MedNorm corpus was collected
across two different domains: biomedical docu-
ments (drug labels and PubMed abstracts) and so-
cial media (online health forums and drug-related
discussions in Twitter). The list of source datasets
and their descriptions are provided below. Table 1
represents the overview of utilised terminologies.

Dataset UMLS MedDRA SCT
CADEC ✗ ✓∗ ✓
TwADR-L ✓ ✗ ✗
TwiMed ✓ ✗ ✗
SMM4H-2017 ✗ ✓ ✗
TAC 2017 (ADR) ✗ ✓ ✗

∗ - partially mapped to MedDRA (only ADR mentions)

Table 1: Terminologies used in publicly available
datasets to annotate medical concepts.

CADEC: The CSIRO Adverse Drug Event Cor-
pus (CADEC) (Karimi et al., 2015) is an annotated
corpus of patient-reported adverse drug events
(ADEs) sourced from the medical forum called
AskAPatient3, which collects ratings and reviews
of medications from their consumers. It contains

3https://www.askapatient.com



33

1,250 forum posts annotated for mentions of Drug,
ADR, Disease, Symptom and Finding. Every men-
tion other than Drug has been mapped to the
corresponding SNOMED-CT concept identifier,
whereas ADR mentions have been also mapped to
the corresponding MedDRA term.

TwADR-L: The TwADR-L dataset has been
constructed by the University of Cambridge (Lim-
sopatham and Collier, 2016) from a collection of
three months of Twitter posts, which has been
sampled and annotated by undergrad-level lin-
guists who mapped each phrase to one of the con-
cepts in the UMLS Metathesaurus.

TwiMed: A corpus consists of 1,000 tweets
and 1,000 PubMed sentences selected using the
same strategy and annotated by two pharmacists
for a set of drugs, diseases and symptoms (Al-
varo et al., 2017). The TwiMed-Twitter set
contains 827 phrases and the TwiMed-PubMed
contains 1,142 phrases, both mapped to the UMLS
Metathesaurus.

SMM4H-2017: This is a dataset of concept
mentions and their corresponding human-assigned
MedDRA PTs has been provided as a part of
the 2nd Social Media Mining for Health Appli-
cations Shared Task at AMIA 2017 (Subtask 3)
(Sarker et al., 2018). It consist of two sets: the
SMM4H2017-train set (6,650 phrases) and the
SMM4H2017-test set (2,500 phrases).

TAC 2017 (ADR Track): The Text Analysis
Conference (TAC) 2017 Shared Task had a track
on Adverse Drug Reaction Extraction from Drug
Labels (Demner-Fushman et al., 2018), the final
task of which was focused on mapping extracted
ADRs in a Structured Product Labels (SPL) to
MedDRA PTs. The training set (TAC2017 ADR)
of 101 annotated drug labels has been released,
which contain 7,045 ADR mentions mapped to
MedDRA.

3 Corpus creation

The overview of the data harmonisation pipeline
used to create a corpus is illustrated in Figure 1.
Initially, we have combined all seven datasets from
five data sources mentioned above into a single
set of instances where each phrase is associated
with corresponding original identifiers in different
terminologies. We have represented the corpus
as a graph to preserve relations between datasets
and their annotations (Section 3.1). Then, we ex-
tracted hierarchical relations and linked all con-

cepts to their closely matched (equivalent) con-
cepts across terminologies (Section 3.2). We have
encoded both hierarchical and equivalent relations
between concepts in different terminologies in a
low-dimensional vector space that enables to mea-
sure the similarity between them (Section 3.3). In
addition, we attempted to identify and resolve po-
tential inconsistencies in human annotations (Sec-
tion 3.4). In order to achieve consistent hierarchy
levels across annotations, all instances have been
simultaneously mapped to either the Preferred
Term (PT) or higher level (e.g. when original
annotation was less specific) in MedDRA and its
equivalent level in SNOMED-CT. After such pro-
cess, each phrase could have more than one equiv-
alent mapping candidate (multi-label). Therefore,
to provide one-to-one mapping between phrases
and concepts, multiple candidates have been re-
duced to a single concept (single-label). As a re-
sult, we constructed our corpus of 27,979 textual
descriptions (phrases) simultaneously mapped to
both MedDRA (version 21.1) and SNOMED-CT
(version 2018-07-31).

Combine 
datasets 

Extract hierarchical
relations between

concepts

Cross-terminology
mapping

Map to consistent
hierarchical level

Resolve potential
annotation errors

community-based
mappings from BioPortal

manually-curated
mappings

...

MedDRA SCT

Build graph
representation

Annotated
dataset 2

Annotated
dataset N

Annotated
dataset 1

Corpus graphLearn concept
representations (VSM)

Concept
embeddings

multi-label

single-label

Terminologies

views

MedNorm
corpus

Figure 1: The data harmonisation pipeline

3.1 Building a corpus graph

In order to utilise the structure and relations of an-
notations in different datasets, the directed graph
or network has been created (Figure 2). In such
graph, each DATASET (e.g. CADEC) has a set



34

HAS_INSTANCE

CADEC

DESCRIBED_AS

ANNOTATED_AS ANNOTATED_AS

LIPITOR.430.ann_TT3

"unable to sleep"

IS_A
MAPPED_TO

LLT:10041017 IS_ASCT:248255005

CONTAINS

Sleeplessness Cannot sleep at all

unable sleepto

Insomnia

cannot at all

IS_A

SCT:193462001

sleeplessness insomnia

(a) Corpus graph schema (b) Example of a modelled instance

DESCRIBED_AS

ANNOTATED_AS

INSTANCE

NAMED_AS

CONCEPT

HAS_INSTANCE

DATASET

CONTAINS

PHRASE

TOKEN

NAME

IS_A

CONTAINS CONTAINS CONTAINS

NAMED_AS

PT:10022437

MAPPED_TO
NAMED_ASNAMED_AS

CONTAINS

Figure 2: Corpus graph schema (left) and an example of a modelled instance (right).

of instances, each INSTANCE can be originally
annotated with one or multiple CONCEPTs (e.g.
LLT:10041017, SCT:248255005) and described
with textual PHRASE (e.g. “unable to sleep”),
which in its turn contains a set of TOKENs (e.g.
{sleep, unable, to}). Each of the CONCEPT has
a corresponding NAME in the terminology (e.g.
Sleeplessness, Cannot sleep at all), which is en-
coded using NAMED AS link and also contains a
set of tokens (similar to phrase). To represent
hierarchical relations between concepts extracted
from medical terminologies, each CONCEPT can
be linked to its parent node (i.e. concept from
the higher level in the hierarchy) with IS A link
(e.g. Sleeplessness → Insomnia → Disturbances
in initiating and maintaining sleep → Sleep dis-
orders and disturbances → Psychiatric disor-
ders) and mapped to the equivalent concept node
using MAPPED TO relation (e.g. Sleeplessness
LLT:10041017 → Insomnia SCT:193462001). The represen-
tation of the corpus as a graph makes the further
processing and analysis easier. For example, test-
ing whether a particular phrase has been inconsis-
tently annotated in the same dataset (i.e. has more
than one associated concept) could be done by
counting the number of unique CONCEPT nodes
reachable from the target phrase. Moreover, all
links between concepts in different terminologies
(despite their various structures) are stored inside
the single graph.

3.2 Cross-terminology mapping

The automatic mapping between UMLS, Med-
DRA and SNOMED-CT has been done using

community-based mappings from BioPortal (Noy
et al., 2008) through the REST API 4. The two
concepts from different ontologies are considered
as equivalent or closely matched if they share the
same UMLS Concept Unique Identifier (CUI). Af-
ter a careful review of results, we observed that
some of the frequently mentioned concepts have
not been mapped automatically. Therefore, with
the help of medical experts, we defined an addi-
tional set of manually-curated mapping rules (pro-
vided in Appendix A, Table 6).

3.3 Learning cross-terminology
representations of concepts

Cross-terminology mappings allowed to link con-
cepts from multiple terminologies together, but
their heterogeneous hierarchical structures (i.e.
concepts are located deeper in the hierarchy or
have more relations) makes graph distance alone
insufficient to measure the similarity between con-
cepts in different terminologies. However, med-
ical concepts (or their corresponding nodes) can
be embedded into a low-dimensional vector space.
Initially, we have constructed a simplified hierar-
chical concept graph whose vertices are groups
of equivalent concepts (i.e. nodes linked with
MAPPED TO relation in the main corpus graph)
and edges are hierarchical IS A relations. Then,
we have used the DeepWalk (Perozzi et al., 2014),
a deep learning method based on generalisation
of language modelling applied on the streams of
short random walks treating them as the equiva-

4http://data.bioontology.org



35

lent of sentences. Performing 10 random walks
per node (with a length of 40 nodes) and train-
ing a Skip-gram model (Mikolov et al., 2013)
with the window size of 5, we have generated
64-dimensional concept vectors. The size of vec-
tors has been chosen empirically. Later, we have
split the groups under the assumption that all con-
cepts in a group (i.e. equivalent concepts) should
have the same vectors. Table 2 shows three se-
lected MedDRA concepts and their most similar
concepts (with cosine similarity) from all termi-
nologies. It demonstrates that both equivalent and
hierarchical relations between concepts has been
successfully encoded and the semantic similarity
can be captured by calculating the cosine similar-
ity between two corresponding concept vectors.

3.4 Corpus consistency

In order to make all annotations in our final corpus
consistent, we have performed the two operations
described below.

Resolving inconsistent annotations: After
performing a manual analysis of the combined
corpus we have noticed inconsistencies in the
original human annotations. For example, in
the CADEC, where phrases can be mapped si-
multaneously to both SNOMED-CT and Med-
DRA, 27 instances which were (correctly) an-
notated as Stomach cramps (SCT:51197009)
also were co-annotated as Learning disorder
(MEDDRA PT:10061265). To identify poten-
tial annotation errors in the original datasets, we
have utilised the concept graph to calculate the
distances between concept nodes (i.e. the short-
est path length) and the cosine similarity of corre-
sponding vectors in the latent vector space model
(VSM). Also, we made an effort to locate inconsis-
tent annotations across different datasets by iden-
tifying ambiguous tokens. In the usual case, a
specific token is used to describe groups (clusters)
of similar concepts (e.g. “walk” frequently de-
scribes concepts related to walking or mobility).
However, an ambiguous token describes clusters
of similar concepts frequently, but also sometimes
describes concepts that are different from those
clusters (i.e. the difference between the number of
occurrences in the groups is high). Note that com-
mon tokens (e.g. “unable”), that are not specific
for a particular group of concepts, will usually
have a high number of groups, but relatively small
difference between the numbers of occurrences.

We attempted to identify such outliers by calculat-
ing distances between concepts and their distance
deviations from the clusters. For example, token
“walk” was mentioned in 98 phrases and mapped
to 23 concepts in total. The most popular anno-
tation was Walking disability (e.g. “can barely
walk”), however it also has been annotated as My-
ocardial infarction (e.g. “walk a little funny”) that
could be a potential annotation error. After such
analysis and manual review, we have identified
and re-mapped 110 annotations (provided with the
source code).

Consistent hierarchical mapping: The Pre-
ferred Term (PT) level in MedDRA describes sin-
gle medical concept. Therefore it has been se-
lected as a standard to provide a consistent hi-
erarchical level among annotations in our cor-
pus. However, not all phrases are specific enough
to be mapped to the PT level or its equivalent.
In such cases, we kept annotations equivalent to
higher MedDRA levels (i.e. HLT, HLGT or SOC).
All lower level annotations (i.e. LLT-equivalent)
have been mapped to their PT-equivalent parents.
Using the corpus graph, we were able to au-
tomate this process. Initially, all instances re-
gardless of the terminology used in original an-
notations have been recursively mapped to their
corresponding equivalent PT candidates (i.e. in-
cluding mappings of mappings). Then, for each
MedDRA candidate, we selected equivalent can-
didates from SNOMED-CT. To filter concepts
that have emerged from such automatic mapping,
all concepts that have not been observed in the
original annotations were removed (except cases,
where it was the only possible candidate). Af-
ter such process, each phrase could have more
than one candidate for each terminology (multi-
label). Therefore, to provide one-to-one mapping
between phrases and terminologies, in each multi-
label group we have initially identified the most
similar MedDRA concept to the original annota-
tion (i.e. from the source dataset) but also the
most popular across the whole corpus (i.e. to min-
imise the number of outliers). Then, we selected
the SNOMED-CT concept (from the multi-label
group) that is the most similar to the selected Med-
DRA concept to achieve consistency in mapping
between terminologies. Hereby, each phrase has
been mapped to exactly one (single-label) Med-
DRA and its corresponding SNOMED-CT con-
cept simultaneously. As a result, the final corpus



36

Insomnia PT:10022437 Weight increased PT:10047899 Nausea PT:10028813
1.0000 Insomnia disorder LLT:10078083
1.0000 Insomnia SCT:193462001
1.0000 Insomnia NOS LLT:10022442
1.0000 Sleeplessness LLT:10041017
1.0000 Sleeplessness C0917801
0.9795 Sleep loss C0235161
0.9795 Sleep loss LLT:10041001
0.9795 Sleep decreased LLT:10040982
0.9779 Middle insomnia SCT:67233009
0.9779 Middle insomnia C0393761
0.9779 Sleep maintenance insomnia LLT:10068671
0.9779 Middle insomnia PT:10027590
0.9743 Trouble falling asleep LLT:10044698
0.9743 Initial insomnia C0393760
0.9743 Initial insomnia SCT:59050008
0.9743 Initial insomnia PT:10022035
0.9689 Early morning awakening LLT:10014046
0.9689 Terminal insomnia PT:10068932
0.9689 Terminal insomnia SCT:67062000
0.9689 Awakening early LLT:10003867

1.0000 Ponderal increased LLT:10063441
1.0000 Wt gain LLT:10048060
1.0000 Weight increasing SCT:161831008
1.0000 Weight increase LLT:10047898
1.0000 Weight gain finding SCT:8943002
1.0000 Weight gain C0043094
1.0000 Weight increased SCT:262286000
1.0000 Weight gain LLT:10047896
0.9532 Weight change finding SCT:365921005
0.9532 Weight change finding C1287464
0.9375 Weight loss finding SCT:89362005
0.9375 Weight decreased PT:10047895
0.9375 Weight decreased SCT:262285001
0.9375 Wt loss LLT:10048061
0.9375 Weight decreasing SCT:161832001
0.9375 Lost weight LLT:10024886
0.9375 Loss of weight LLT:10024883
0.9375 Weight decrease LLT:10047893
0.9375 Losing wt LLT:10024849
0.9375 Weight loss LLT:10047900

1.0000 Nauseous LLT:10028823
1.0000 Feeling queasy LLT:10016361
1.0000 Nauseated LLT:10028822
1.0000 Nausea SCT:422587007
1.0000 Nausea C0027497
1.0000 Queasy LLT:10037730
0.8677 Nausea and vomiting symptoms HLT:10028817
0.8677 Nausea and vomiting SCT:16932000
0.8677 Nausea and vomiting C0027498
0.7902 Gastrointestinal tract finding C1261141
0.7902 Gastrointestinal tract finding SCT:386618008
0.7832 Travel sickness NOS LLT:10044549
0.7832 Motion sickness C0026603
0.7832 Travel sickness LLT:10044548
0.7832 Motion sickness PT:10027990
0.7832 Motion sickness SCT:37031009
0.7721 Retching C0232602
0.7721 Dry heaves LLT:10052104
0.7721 Retching SCT:84480002
0.7721 Vomiturition LLT:10072124

Prefixes for concept identifiers: SCT - SNOMED-CT; C - UMLS; LLT, PT, HLT, HLGT, SOC - MedDRA (based on the level).
The equivalent concepts have similarity value of 1.0.

Table 2: MedDRA concepts and their most similar concepts across different terminologies.

Phrase Original annotations Mapped MedDRA Mapped SNOMED-CT
screwed my
endocrine system Endocrinedisorders SOC:10014698

Endocrine disorders SOC:10014698
Endocrine disorder PT:10014695

Disorder of endocrine
system SCT:362969004

Got 1.5 hours
of sleep Sleep disturbance C0037317 Sleep disturbances HLGT:10040998Sleep disorder PT:10040984

Disturbance in
sleep behavior SCT:53888004
Sleep disorder SCT:39898005

wrecking my sleep Poor quality sleep C1262141 Poor quality sleep PT:10062519
Dyssomnia PT:10061827
Sleep disorder PT:10040984

Dyssomnia SCT:44186003
Sleep disorder SCT:39898005

all I want to
do is sleep Somnolence PT:10041349 Somnolence PT:10041349Insomnia PT:10022437

Drowsy SCT:271782001
Insomnia SCT:193462001

weak Asthenia PT:10003549 Asthenia PT:10003549 Asthenia SCT:13791008
fatigue Fatigue C0015672 Fatigue PT:10016256

Asthenia PT:10003549
Fatigue SCT:84229001
Asthenia SCT:13791008
Lack of energy SCT:248274002

extremely tired
feeling Tiredness LLT:10043890Feeling tired SCT:314109004

Fatigue PT:10016256
Asthenia PT:10003549

Fatigue SCT:84229001
Asthenia SCT:13791008
Lack of energy SCT:248274002
Feeling tired SCT:248274002

Selected concepts (during multi-label reduction to single-label) are in bold-italic.

Table 3: Examples of originally annotated phrases and their multi-label and single-label mappings

has 27,957 PT-equivalent, two HLT-equivalent, 18
HLGT-equivalent and two SOC-equivalent anno-
tations. In Table 3 we have provided examples of
phrases, original annotations and our final Med-
DRA and SNOMED-CT annotations (mappings).

4 Corpus analysis

The descriptive statistics of datasets constituting
a corpus (grouped into biomedical and social do-
mains) are presented in Table 4. The length of
medical concept descriptions (phrases) are longer
in social domain. The longest phrase has been
found in the CADEC corpus: “when I went to sit
down instead of siting normally I would almost fall
down in the chair no control no strength, upon get-
ting up I had to hold on to something to get up”

(36 tokens) that describes Muscle weakness. We
have also investigated the degree of class imbal-
ance in the corpus and illustrated the most reported
MedDRA concepts in Figure 3. The most reported

Figure 3: Most popular concepts in the corpus



37

Dataset # inst # MedDRA∗ # SCT∗ # phrases # words phrase length
TAC2017 ADR 5,835 1,113 1,087 2,106 1,633 2.46± 1.49 [1− 13]
TwiMed-PubMed 1,067 254 255 436 478 1.93± 1.11 [1− 8]
All biomedical 6,902 1,191 1,169 2,397 1,804 2.42± 1.46 [1− 13]
CADEC 6,797 530 557 3,376 1,966 3.42± 2.26 [1− 36]
SMM4H2017-train 6,416 411 404 2,638 2,084 3.24± 2.22 [1− 25]
TwADR-L 4,626 1544 1566 2,581 2,492 2.46± 1.78 [1− 20]
SMM4H2017-test 2,447 227 224 1,148 1,165 3.31± 2.33 [1− 18]
TwiMed-Twitter 791 185 185 428 524 2.08± 1.49 [1− 12]
All social 21,077 1,740 1,778 8,890 4,975 3.26± 2.21 [1− 36]
ALL 27,979 2,062 2,089 10,572 5,584 3.18± 2.12 [1− 36]

∗ - single-label annotations

Table 4: Statistics of the datasets constituting the corpus.

concept is Insomnia (1,311 instances, 553 unique
phrases), followed by Pain (1,145 instances, 320
unique phrases) and Fatigue (800 instances, 125
unique phrases). However, about 40% of concepts
were under-reported and have only one instance,
corresponding to about 3% instances in the whole
corpus. The average number of unique phrases
per terminology concept is 5.13 for MedDRA and
5.06 for SNOMED-CT.

4.1 Asymmetric transferability between
datasets

To investigate how the knowledge acquired from
one dataset is potentially transferable to another
dataset, we introduced the asymmetric transfer-
ability index that takes into account both concep-
tual (i.e. concepts from various terminologies
used in the dataset) and textual (i.e. language used
to describe those concepts) similarities. Asymme-
try allows to see how much information can be un-
derstood from another dataset having all informa-
tion about the first dataset. It utilises two similarity
measures: cosine similarity CS(X,Y ) = X·Y󰀂X󰀂󰀂Y 󰀂
and the special case of Tversky Index (Tversky,
1977) with α = 1 and β = 0, that can be re-
written as TI(X,Y ) = |X∩Y ||X∩Y |+|X−Y | . We can
calculate the similarity between two sequences of
labels l1 and l2 with the cosine similarity between
the corresponding label count vectors c(l1) and
c(l2). However that measure will be symmetric,
and therefore we multiply it by asymmetric set-
based similarity:

s(l1, l2) = TI(l1, l2)× CS(c(l1), c(l2)) (1)

Having two datasets A and B, sets of phrases PA,
PB and sets of words WA, WB we obtain the
textual transferability index (from A to B) as the
arithmetic mean of phrasal and verbal asymmetric

similarities:

Itxt(A,B) =
TI(PA, PB) + TI(WA,WB)

2
(2)

For each terminology t, we extract sequences of
labels ℓ(A, t) in dataset A and ℓ(B, t) in dataset
B. The conceptual transferability index is the av-
erage asymmetric similarity between terminology-
specific label sets:

Icon(A,B) =
1

|T |
󰁛

t∈T
s(ℓ(A, t), ℓ(B, t)) (3)

Finally, we obtain the overall transferability index:

Iovr(A,B) =
Itxt(A,B) + Icon(A,B)

2
(4)

We have presented textual, conceptual and over-
all transferability matrices in Figure 4. The higher
transferability index shows the better chance to
understand information (i.e. match vocabulary
or concepts). The most transferable dataset was
TwADR-L, whereas the least transferable was
TwiMed-PubMed. It directly corresponds to the
number of unique concepts, phrases and words re-
ported previously in Table 4. Also, the datasets
collected from Twitter are highly transferable be-
tween each other. The CADEC dataset collected
from AskAPatient reports is still more similar to
Twitter (i.e. social domain).

4.2 Cross-terminology concept
representations

In order to analyse cross-terminology concept
representations, we used T-distributed Stochastic
Neighbour Embedding (t-SNE) (Maaten and Hin-
ton, 2008) to perform dimensionality reduction
from 64D to 2D (Figure 5). It can be observed that
semantically similar concepts have been clustered
together, providing additional evidence about the



38

Figure 4: Asymmetric dataset transferability matrices.

ability of concept representations to encode hier-
archical and equivalent relations and capture se-
mantic similarities.

Figure 5: t-SNE visualisation of cross-terminology
medical concept representations

In Table 5 we have presented the most simi-
lar MedDRA and SNOMED-CT annotations (i.e.
the final labels in the corpus) for the three most
frequently reported concepts: Insomnia, Pain and
Fatigue. Although such representations encoded
conceptual similarity well, they are insufficient to
identify opposite concepts correctly (e.g. Fatigue
and Energy increased). This is because we only
utilised hierarchical relations in terminologies (in-
formation about opposite concepts is not provided
in these terminologies explicitly).

5 Conclusion

We have presented a corpus for cross-terminology
medical concept normalisation that has been
sourced from five publicly available datasets
across the biomedical and social domains. The

Concept MedDRA SNOMED-CT

Insomnia

0.98 Middle insomnia
0.97 Initial insomnia
0.97 Terminal insomnia
0.97 Hyposomnia
0.91 Poor quality sleep

0.98 Middle insomnia
0.97 Initial insomnia
0.97 Early morning waking
0.97 Not getting enough sleep
0.91 Dyssomnia

Pain

0.82 Labour pain
0.78 Nyctalgia
0.76 Tenderness
0.60 Painful respiration
0.58 Odynophagia

0.82 Labor pain
0.78 Night pain
0.76 Tenderness
0.68 Burning epigastric pain
0.68 Postoperative pain

Fatigue

0.83 Asthenia
0.83 Lethargy
0.69 Malaise
0.69 Feeling abnormal
0.68 Energy increased

0.83 Asthenia
0.83 Lethargy
0.77 Sensation of heaviness in limbs
0.69 Generally unwell
0.69 Malaise

Table 5: Most similar MedDRA and SNOMED-CT
concepts (from annotations).

data harmonisation pipeline described in the paper
combines instances from various datasets and pro-
vides consistent simultaneous mappings to both
MedDRA and SNOMED-CT terminologies. Such
pipeline can be used in the future to integrate new
datasets into the corpus or could be also applied
in relevant data annotation and processing tasks.
Also, we have described a method to merge multi-
ple medical terminologies and demonstrated that
equivalent and hierarchical relations can be en-
coded into cross-terminology concept representa-
tions that are able to capture semantic similarities
not only between concepts inside a given termi-
nology but also between concepts from different
terminologies. The generated cross-terminology
medical concept representations can be used to im-
prove and analyse the performance of concept nor-
malisation systems. Making such resources avail-
able to the research community as well as provid-
ing an analysis of the final corpus aimed to con-
tribute to a better understanding of the task and
associated challenges.



39

References
Nestor Alvaro, Yusuke Miyao, and Nigel Collier. 2017.

Twimed: Twitter and pubmed comparable corpus
of drugs, diseases, symptoms, and their relations.
JMIR public health and surveillance, 3(2).

Elliot G Brown, Louise Wood, and Sue Wood. 1999.
The medical dictionary for regulatory activities
(meddra). Drug safety, 20(2):109–117.

Dina Demner-Fushman, Sonya E Shooshan, Laritza
Rodriguez, Alan R Aronson, Francois Lang, Willie
Rogers, Kirk Roberts, and Joseph Tonning. 2018.
A dataset of 200 structuerred product labels anno-
tated for adverse drug reactions. Scientific data,
5:180001.

Yuxiao Dong, Nitesh V Chawla, and Ananthram
Swami. 2017. metapath2vec: Scalable representa-
tion learning for heterogeneous networks. In Pro-
ceedings of the 23rd ACM SIGKDD international
conference on knowledge discovery and data min-
ing, pages 135–144. ACM.

Aditya Grover and Jure Leskovec. 2016. node2vec:
Scalable feature learning for networks. In Proceed-
ings of the 22nd ACM SIGKDD international con-
ference on Knowledge discovery and data mining,
pages 855–864. ACM.

William L Hamilton, Rex Ying, and Jure Leskovec.
2017. Representation learning on graphs: Methods
and applications. arXiv preprint arXiv:1709.05584.

Sarvnaz Karimi, Alejandro Metke-Jimenez, Madonna
Kemp, and Chen Wang. 2015. Cadec: A corpus of
adverse drug event annotations. Journal of biomed-
ical informatics, 55:73–81.

Prodromos Kolyvakis, Alexandros Kalousis, Barry
Smith, and Dimitris Kiritsis. 2018. Biomedical on-
tology alignment: an approach based on represen-
tation learning. Journal of biomedical semantics,
9(1):21.

Nut Limsopatham and Nigel Henry Collier. 2016. Nor-
malising medical concepts in social media texts by
learning semantic representation. Association for
Computational Linguistics.

Laurens van der Maaten and Geoffrey Hinton. 2008.
Visualizing data using t-sne. Journal of machine
learning research, 9(Nov):2579–2605.

Tomas Mikolov, Kai Chen, Greg Corrado, and Jef-
frey Dean. 2013. Efficient estimation of word
representations in vector space. arXiv preprint
arXiv:1301.3781.

Natalya F Noy, Nicholas Griffith, and Mark A Musen.
2008. Collecting community-based mappings in an
ontology repository. In International Semantic Web
Conference, pages 371–386. Springer.

Bryan Perozzi, Rami Al-Rfou, and Steven Skiena.
2014. Deepwalk: Online learning of social rep-
resentations. In Proceedings of the 20th ACM
SIGKDD international conference on Knowledge
discovery and data mining, pages 701–710. ACM.

Abeed Sarker, Maksim Belousov, Jasper Friedrichs,
Kai Hakala, Svetlana Kiritchenko, Farrokh
Mehryary, Sifei Han, Tung Tran, Anthony Rios,
Ramakanth Kavuluru, et al. 2018. Data and sys-
tems for medication-related text classification and
concept normalization from twitter: insights from
the social media mining for health (smm4h)-2017
shared task. Journal of the American Medical
Informatics Association, 25(10):1274–1283.

Peri L Schuyler, William T Hole, Mark S Tuttle,
and David D Sherertz. 1993. The umls metathe-
saurus: representing different views of biomedical
concepts. Bulletin of the Medical Library Associa-
tion, 81(2):217.

Michael Q Stearns, Colin Price, Kent A Spackman,
and Amy Y Wang. 2001. Snomed clinical terms:
overview of the development process and project
status. In Proceedings of the AMIA Symposium,
page 662. American Medical Informatics Associa-
tion.

Amos Tversky. 1977. Features of similarity. Psycho-
logical review, 84(4):327.

A Appendices

MedDRA Concept SNOMED-CT Concept
Withdrawal syndrome
(PT:10048010)

Drug withdrawal
(SCT:363101005)

Depression
(PT:10012378)

Depressive disorder
(SCT:35489007)

Drug ineffective
(PT:10013709)

Lack of drug action
(SCT:58848006)

Hangover
(PT:10019133)

Hangover
(SCT:32553006)

Infection
(PT:10021789)

Infectious disease
(SCT:40733004)

Feeling abnormal
(PT:10016322)

Malaise
(SCT:367391008)

Feeling jittery
(PT:10016338)

Feeling nervous
(SCT:424196004)

Poor quality sleep
(PT:10062519)

Dyssomnia
(SCT:44186003)

Thirst
(PT:10043458)

Thirst symptom
(SCT:249475006)

Lightheadedness
(LLT:10024492)

Lightheadedness
(SCT:386705008)

Table 6: An additional set of manually-curated map-
ping rules between MedDRA and SNOMED-CT.


