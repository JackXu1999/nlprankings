



















































Evaluating bilingual word embeddings on the long tail


Proceedings of NAACL-HLT 2018, pages 188–193
New Orleans, Louisiana, June 1 - 6, 2018. c©2018 Association for Computational Linguistics

Evaluating bilingual word embeddings on the long tail

Fabienne Braune1,2, Viktor Hangya1, Tobias Eder1, Alexander Fraser1
1Center for Information and Language Processing

LMU Munich, Germany
2Volkswagen Data Lab Munich, Germany
fabienne.braune@volkswagen.de

{hangyav, fraser}@cis.uni-muenchen.de
tobias.eder@germanistik.uni-muenchen.de

Abstract

Bilingual word embeddings are useful for
bilingual lexicon induction, the task of min-
ing translations of given words. Many studies
have shown that bilingual word embeddings
perform well for bilingual lexicon induction
but they focused on frequent words in gen-
eral domains. For many applications, bilin-
gual lexicon induction of rare and domain-
specific words is of critical importance. There-
fore, we design a new task to evaluate bilin-
gual word embeddings on rare words in dif-
ferent domains. We show that state-of-the-art
approaches fail on this task and present sim-
ple new techniques to improve bilingual word
embeddings for mining rare words. We release
new gold standard datasets and code to stimu-
late research on this task.

1 Introduction

Bilingual lexicon induction (BLI) is the task of
generating accurate translations for each word in
a list of source language words. Being able to per-
form BLI without parallel data is critical in many
low resource scenarios. Bilingual word embed-
dings (BWEs) represent words from two differ-
ent languages in the same vector space. BWEs
have been shown to be very effective for BLI
given a small seed lexicon (around 5000 word-
pairs) as the only bilingual signal. Until now,
BWEs have been evaluated on frequent words
from parliament proceedings or Wikipedia articles
and reached good accuracies on these datasets.
However, evaluations on rare and domain-specific
words have not yet been provided even though
such evaluation scenarios are critical for appli-
cations like machine translation (e.g., mining of
translations for OOV (out-of-vocabulary) items)
or bilingual terminology mining. In this paper,
we design a novel evaluation scenario for BWEs:
given (i) large amounts of monolingual data and

(ii) a seed lexicon of frequent word-pairs, the goal
is to create BWEs that enable accurate mining of
rare words. As gold standard data, we release
manually annotated pairs of rare words and their
translations from three domains: (i) web crawls
(ii) news commentaries (iii) medical texts. We
show that state-of-the-art BWEs perform poorly
on these data sets. We present simple techniques
to build and combine BWEs that yield strong per-
formance improvements. We study using fast-
text to build BWEs, using ensembles of BWEs,
and dealing with orthographic distance in BWEs,
all of which improve results for the new task of
rare word translation mining. A secondary con-
tribution is improvements over state-of-the-art ap-
proaches on frequent words (which have been al-
ready extensively studied in previous work). We
make our datasets and code publicly available1.

2 Bilingual Induction of Rare Words

We briefly present how BLI is performed using
BWEs and then introduce our new datasets.

Bilingual Lexicon Induction. The goal is to
generate translations t in target language Vt of pro-
vided words s from source language Vs. Given
a BWE representing Vs and Vt, an n-best list of
translations for each word s ∈ Vs can be induced
by taking the top n words ti ∈ Vt whose represen-
tation ~xti in the BWE is closest to the representa-
tion ~xs according to cosine distance.

Datasets. To create BWEs we use post-hoc
mapping which requires only monolingual texts
and a small seed lexicon (see §3). Our training
set consists of two large monolingual corpora:

• GENERAL: 4,400,309 English and German
sentences from parliament proceedings, news

1https://github.com/braunefe/BWEeval

188



commentaries and web crawls taken from the
WMT 2016 shared task (Bojar et al., 2016).

• MEDICAL: 3,108,183 English and German
sentences from titles of medical Wikipedia
articles, medical term-pairs, patents, doc-
uments from the European Medicines
Agency.2

Seed Lexicons. Throughout the paper, we work
with two lexicons. For each lexicon, we take the
most common words and translate these by tak-
ing the top-ranked translation from a probabilis-
tic dictionary.3 BWEs trained using this data are
evaluated on our gold standards containing pairs of
rare words (we will also report results on frequent
words, as in previous work, see below).

• GENLEX: 4955 most frequent words from
GENERAL

• MEDLEX: 6079 most frequent words from
MEDICAL

Gold Standards for Rare Words. We created
gold standard data for rare words by randomly
sampling words occurring between 3 and 5 times4

in GENERAL and MEDICAL. For GENERAL we
sample rare words from news commentaries and
web crawls separately, so we have two rare word
data sets here. For each (English) sampled word, a
German native speaker generated a German trans-
lation. We indicate the division into validation and
test sets:

• CRAWLRARE: 1000 rare words from web
crawls of GENERAL (250 validation, 750 test)

• NEWSRARE: 1144 rare words from news
commentaries of GENERAL (369 validation,
775 test)

• MEDRARE: 2109 rare words from MEDICAL
(1000 validation, 1109 test)

As English-German BLI of frequent words has
not been studied before, following previous work,
we annotated 2000 frequent English words taken
from each of the General and Medical corpora
with their German translations using the same
probabilistic dictionary as was used to generate the
Lexicon sets. These two silver standard datasets
will also be released with the paper:

2This is taken from the in-domain part of: https://
ufal.mff.cuni.cz/ufal_medical_corpus.

3This word-level dictionary is taken from a standard
phrase-based SMT system trained on WMT 2017 data.

4Words with frequencies 1 and 2 are very often tokeniza-
tion errors or borrowings from other languages, therefore we
start at frequency 3. We did not consider tokenization errors
as rare words and removed those from our data.

• GENFREQ: 2000 frequent words from GEN-
ERAL (1000 validation, 1000 test)

• MEDFREQ: 2000 frequent words from MED-
ICAL (1000 validation, 1000 test)

3 Bilingual Word Embedding Creation

To create bilingual word embeddings, we use
post-hoc mapping (PHM), a method that projects
monolingual words embeddings (MWEs) into a
shared space using a linear transformation trained
with a small seed lexicon (Mikolov et al., 2013b;
Faruqui and Dyer, 2014; Xing et al., 2015; Lazari-
dou et al., 2015; Vulić and Korhonen, 2016).
Among methods to generate BWEs, PHM uses a
very cheap bilingual signal.5

Given MWEs in two languages Vs and Vt,
the goal of post-hoc mapping is to find a ma-
trix W ∈ Rd1×d2 that maps each representa-
tion ~xs ∈ Rd1 of a source word s ∈ Vs to the
representation ~yt ∈ Rd2 of its translation t ∈
Vt. Typically, W is learned using a seed lexi-
conL = {( ~x1, ~y1), . . . , ( ~xn, ~yn)}, where each pair
(~xi, ~yi) ∈ Vs×Vt are mutual translations. A com-
mon objective for cross-lingual mapping is ridge
regression (Mikolov et al., 2013b) (RIDGE), where
W is estimated by:

W∗ = argmin
W∈Rd1×d2

|| XW− Y || +λ ||W || (1)

where X and Y are stacked vectors of ~xi and ~yi
respectively. Lazaridou et al. (2015) use a max-
margin ranking loss (MAX-MARG) to estimate W.
For each (~xi, ~yi) ∈ L, a candidate ~y∗i = W · ~xi is
computed. The ranking loss is:

k∑

j 6=i
max{0, γ + sim( ~y∗i , ~yi)− sim( ~y∗i , ~yj)} (2)

where ~yj is a randomly selected negative example,
i.e., it is not a translation of ~xi, k is the number of
negative examples and sim(~x, ~y) computes cosine
similarity between ~x and ~y. Hyperparameters γ
and k are tuned on held-out validation data.6

5Gouws and Søgaard (2015) and Duong et al. (2016) also
leverage seed lexicons. However, in order to generate high
quality BWEs, these approaches leverage much larger bilin-
gual dictionaries.

6Ideally, the sum in Equation 2 should be computed over
the complete target vocabulary (i.e., k =| Vt |). Since this
is not feasible in practice, Lazaridou et al. (2015) treat k as
another hyperparameter tuned together with γ.

189



Domain mapping W2V skip W2V cbow

genFreq ridge 27.1 (43.7) 24.0 (41.4)max-marg 32.1 (47.7) 22.8 (40.0)

medFreq ridge 14.9 (24.0) 18.1 (30.1)max-marg 16.0 (27.2) 16.8 (27.2)

Table 1: Bilingual lexicon induction of frequent word-
pairs on general and medical domain data. We report
top-1 and (top-5 in brackets) percentage accuracy. In
this paper, bolding indicates a best result so far for a
particular dataset.

3.1 Testing Previous Work

We reimplement Mikolov et al. (2013b) as well as
Lazaridou et al. (2015). To replicate their setup
on English-German texts we first evaluate these on
two standard tasks, mining frequent words from
GENERAL and MEDICAL. We follow the approach
of (Heyman et al., 2017) and use English as the
source language. First, we train 300 dimensional
MWEs on the monolingual data using W2V7 with
default parameters except that we lowered the
minimum word frequency threshold to 3 (Mikolov
et al., 2013a). To generate BWEs, we use MEDI-
CAL and MEDLEX for MEDRARE and MEDFREQ,
while we use GENERAL and GENLEX for the rest
of the test sets. We report results with the com-
bination of skip-gram (W2V SKIP) or cbow (W2V
CBOW) and RIDGE or MAX-MARG. As in previ-
ous work, we use top-1 (translation is the closest
neighbor) and top-5 (translation is one of 5 closest
neighbors) accuracies. The results in Table 1 show
that the best performing setups are W2V SKIP with
MAX-MARG for GENFREQ and W2V CBOW with
RIDGE for MEDFREQ. Accuracies are compara-
ble to previous work (which was on different lan-
guage pairs). The poor performance on MEDFREQ
is consistent with Heyman et al. (2017), who intro-
duced the task of mining frequent medical terms.

4 Applying BWEs for Mining Rare
Word-Pairs

We use the exact same BWEs training setup as
above (§3) and perform BLI on our new test sets
of rare words. The results in Table 2 show that
on low frequency word-pairs BWEs perform very
poorly. Compared to standard evaluation scenar-
ios (see Table 1) a massive performance decrease
is observed. Low accuracy is clearly caused by the
inability of context-based models (W2V) to build
accurate embedding vectors for words occurring

7https://github.com/dav/word2vec

Domain mapping W2V skip W2V cbow

crawlRare ridge 2.3 (3.2) 2.0 (2.4)max-marg 2.1 (3.3) 1.7 (2.3)

newsRare ridge 4.6 (9.4) 1.9 (4.9)max-marg 5.5 (11.0) 2.3 (4.8)

medRare ridge 0.1 (0.2) 0.1 (0.1)max-marg 0.1 (0.4) 0.1 (0.1)

Table 2: Bilingual lexicon induction of low frequency
word-pairs in different domains.

Domain mapping FTT skip FTT cbow

crawlRare ridge 10.1 (14.7) 4.7 (6.7)max-marg 11.5 (15.9) 7.3 (12.1)

newsRare ridge 23.2 (37.7) 6.8 (13.5)max-marg 25.3 (39.5) 15.1 (24.1)

medRare ridge 12.2 (19.0) 8.2 (14.2)max-marg 12.5 (20.0) 8.8 (15.3)

genFreq ridge 33.8 (51.4) 16.2 (32.1)max-marg 38.7 (56.5) 28.3 (45.3)

medFreq ridge 17.8 (33.6) 14.9 (26.7)max-marg 29.3 (42.7) 19.9 (33.2)

Table 3: Bilingual lexicon induction using MWEs
trained with FASTTEXT (FTT).

in very few contexts only. Through post-hoc map-
ping, these (poor) embeddings get projected ran-
domly into the bilingual space which results in
very poor performance on BLI especially for the
medical domain.

4.1 Using Subword Models
A first way to create BWEs that are better adapted
to rare words is to generate MWEs that provide
better vector representations for the words. One
simple idea is to try to add subword information.
We show empirically this helps BLI of rare words,
which has not been shown before, to our knowl-
edge. FASTTEXT (Bojanowski et al., 2017) ex-
tends W2V by adding subword information s(w, c)
to the context-based objective as follows:

s(w, c) =
∑

g∈Gw
z>g vc (3)

where Gw ⊂ {1, ..., G} is a set of character n-
gram indices corresponding to the n-grams that
appear in the word w, zg is the vector representa-
tion of the n-gram and vc is the vector of the con-
text words. Subword information helps for rare
words (by using n-gram information shared be-
tween words) and generates more accurate MWEs
especially for morphologically rich languages like
German. We create 300 dimensional MWEs us-
ing FASTTEXT skip-gram and cbow models with
default parameters and with the same exception
as before, i.e., we lowered the minimum word

190



Domain mapping ensemble ensemble + edit edit only % orth. close

crawlRare ridge 10.3 (14.6) 19.5 (22.1) 19.0 (21.7) 60.3max-marg 13.2 (17.6) 19.5 (22.3)

newsRare ridge 24.3 (39.9) 32.0 (42.8) 21.8 (29.5) 39.8max-marg 27.2 (40.0) 32.8 (43.5)

medRare ridge 15.1 (20.6) 25.5 (26.8) 26.0 (28.2) 76.7max-marg 12.5 (21.2) 26.3 (28.2)

genFreq ridge 42.9 (60.7) 44.8 (62.0) 16.0 (27.1) 26.5max-marg 45.4 (63.2) 47.2 (63.6)

medFreq ridge 31.6 (37.0) 35.5 (44.2) 20.7 (32.9) 55.7max-marg 37.7 (46.7) 38.6 (47.4)

Table 4: Bilingual lexicon induction of low-frequency and frequent word-pairs in different domains. Ensem-
ble denotes the results for ensembling all BWE models, ensemble + edit shows results by adding orthographic
similarity, edit only denotes the results obtained by using only orthographic distance (all other weights set to 0)
and % orth. close shows the percentage of orthographically similar gold standard word pairs (whose normalized
Levenshtein distance is at most 0.3).

No. source model1 model2 corpus glossary

W2V skip FTT skip
1. snowstorms skinhead schneestürme crawlRare skinhead
2. fire-extinguishers goldzertifikate feuerlöschern crawlRare gold certificates
3. tissue-specificity basismilieu gewebespezifizität medRare base environment
4. university universität harvard-universität crawlRare Harvard University
5. cabin kabine flugzeugkabine newsRare airplane cabin

FTT skip ensemble
6. rubbish mülltonnen müll newsRare trashcan
7. bathtub badezimmer badewanne newsRare bathroom
8. parenthood vaterschaft elternschaft newsRare fatherhood
9. cognitively neurokognitiven kognitiv medRare neuro cognitive
10. nanojoules nanojoule mikrotröpfchen medRare microdroplet

ensemble ensemble + edit
11. sleddogs pferdeschlittenfahrten schlittenhunde crawlRare sled rides
12. gnome-applets gtkhtml gnome-anwendungen crawlRare layout engine used by Gnome
13. glutenins getreideproteinen glutenine medRare grain proteins
14. esterify verestern esteröl medRare ester oil

Table 5: Examples comparing the predictions of the indicated models using ridge for the mapping where model1
and model2 shows the induced words for the given source. Bolding indicates the correct prediction and we give
glosses for the incorrect predictions.

frequency value to 3. We perform PHM using
RIDGE and MAX-MARGIN. The results in Table 3
show that this procedure yields impressive perfor-
mance improvements. After evaluation8 we man-
ually looked at the prediction of our models. We
present examples in table 5. Examples 1–3 shows
that the model improves non-trivial cases as well
where the meanings of the incorrect predictions
induced by W2V are not close to that of the in-
put. We also show counterexamples 4 and 5 where
subword elements cause errors by inducing hy-
ponymies of the correct words. Generating BWEs
with MAX-MARGIN on these improved MWEs is
particularly effective. By analyzing word similar-
ities we saw that in BWEs acquired with RIDGE
rare English words are often mapped near to noise.

8We added these examples to the camera-ready paper after
the results were finalized.

Because MAX-MARGIN uses negative noisy word
pairs as training examples this phenomenon is not
as strongly present there.

4.2 Model Ensembling

Although BWEs obtained with FASTTEXT and
MAX-MARGIN clearly outperform other methods
on rare words, a combination of BWEs obtained
with different models can further improve perfor-
mance by integrating several sources of informa-
tion. We ensemble BWEs obtained using differ-
ent MWEs as follows: we generate n-best lists
(n = 100) of translation candidates using each
model. For each pair (s, t) of candidate transla-
tions, we compute an ensemble weight given by a
weighted sum of similarity scores Simi(s, t) ob-

191



tained on each BWE:

M∑

i=1

γiSimi(s, t) (4)

Simi(s, t) is computed using cosine similarity.
When a candidate pair (s, t) is not in the n-best list
generated by a model i then Simi(s, t) is set to 0.
The weights γi for each test set are tuned on vali-
dation sets separately (presented in §2) using grid
search. The results (Table 4) show that ensem-
bling yields significant gains over subword models
alone for all data sets. We again looked for exam-
ples after evaluation (Table 5) where ensembling
helped compared with the previous best setup (ex-
amples 6–9) and saw that the method again im-
proves upon hard cases where the incorrect pre-
dictions are very close, in terms of meaning, to
the gold annotation. Row 10 shows a counter-
example. We note that this idea could be used in a
supervised neural network for BLI as well, where
information from multiple models could be inte-
grated by concatenating embeddings from them
for a given word.

4.3 Adding Orthographic Distance
While subword information captures orthographic
properties of words to a certain extent, it can-
not precisely represent the orthographic distance
of each word pairs in a predefined number of di-
mensions, especially not that of source and target
word pairs when performining post-hoc mapping
(MWEs are trained separately thus there is no such
cross-lingual information). Thus, it is beneficial to
strengthen BWEs by integrating a similarity mea-
sure between word strings directly. The BWEs
ensemble in Equation 4 can easily be augmented
with a weighted term γM+1OSim(s, t) that mea-
sures the orthographic similarity (which we de-
fine as one minus the normalized Levenshtein dis-
tance) between the surface-forms of words s and
t. We generate n-best lists of candidate transla-
tions using different BWE models as in §4.2. In
addition, we generate a list containing the n clos-
est target words according to OSim(s, t) and en-
semble all lists together. Results are shown in Ta-
ble 4. To measure the impact of orthographic in-
formation alone, we also report results obtained
when using this information only (all other ensem-
ble weights set to 0). For low frequency word-
pairs, orthographic information leads to massive
performance gains. We analyzed the gold stan-

dard word pairs in our datasets from the perspec-
tive of orthographic similarity. For CRAWLRARE
and MEDRARE the ratios of similar words are high
which explains the large improvements obtained
by adding this measure. Even though the ratio
is not high for NEWSRARE and the two frequent
datasets, orthographic information still improves
performance which shows the advantage of using
the technique in all cases. Table 5 shows non-
trivial examples (11–13) where orthographic dis-
tance improves performance. Example 11 shows
the advantage of combining the vector represen-
tation with orthographic distance, i.e., our model
could find translations of sleddogs that have sim-
ilar meaning, while in examples 12 and 13 ortho-
graphic distance helped to pick the correct transla-
tion which is the closest in terms of edit distance.
On the other hand, in example 14 orthographic dis-
tance caused an error because the incorrect predic-
tion is too close to the source word in orthographic
distance.

5 Conclusion

We evaluated BWEs on the novel task of rare term
mining in different domains. Our experiments
show that previous approaches to bilingual lexi-
con induction fail when mining rare words. We
have studied techniques for decreasing the impact
of these problems. By ensembling different BWEs
and combining those with orthographic cues, we
have reached state-of-the-art results. By making
our code and datasets publicly available, we hope
to encourage other researchers to further enhance
BWEs to perform well on this important task. In
the future, we would like to work on BLI of multi-
word translations and compound words.

Acknowledgments

We would like to thank Helmut Schmid and
the anonymous reviewers for their valuable in-
put. This project has received funding from
the European Unions Horizon 2020 research and
innovation programme under grant agreement
№ 644402 (HimL). This project has received fund-
ing from the European Research Council (ERC)
under the European Union’s Horizon 2020 re-
search and innovation programme (grant agree-
ment№ 640550).

192



References
Piotr Bojanowski, Edouard Grave, Armand Joulin, and

Tomas Mikolov. 2017. Enriching word vectors with
subword information. TACL .

Ondřej Bojar, Rajen Chatterjee, Christian Federmann,
Yvette Graham, Barry Haddow, Matthias Huck,
Antonio Jimeno Yepes, Philipp Koehn, Varvara
Logacheva, Christof Monz, Matteo Negri, Aure-
lie Neveol, Mariana Neves, Martin Popel, Matt
Post, Raphael Rubino, Carolina Scarton, Lucia Spe-
cia, Marco Turchi, Karin Verspoor, and Marcos
Zampieri. 2016. Findings of the 2016 conference
on machine translation. In Proceedings of the First
Conference on Machine Translation. Association for
Computational Linguistics, Berlin, Germany, pages
131–198.

Long Duong, Hiroshi Kanayama, Tengfei Ma, Steven
Bird, and Trevor Cohn. 2016. Learning crosslin-
gual word embeddings without bilingual corpora. In
Proc. EMNLP.

Manaal Faruqui and Chris Dyer. 2014. Improving vec-
tor space word representations using multilingual
correlation. In Proc. EACL.

Stephan Gouws and Anders Søgaard. 2015. Simple
task-specific bilingual word embeddings. In Proc.
NAACL.

Geert Heyman, Ivan Vulić, and Marie-Francine Moens.
2017. Bilingual lexicon induction by learning to
combine word-level and character-level representa-
tions. In Proc. EACL.

Angeliki Lazaridou, Georgiana Dinu, and Marco Ba-
roni. 2015. Hubness and pollution: Delving into
cross-space mapping for zero-shot learning. In
Proc. ACL.

Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey
Dean. 2013a. Efficient estimation of word represen-
tations in vector space. In ICLR.

Tomas Mikolov, Quoc V Le, and Ilya Sutskever. 2013b.
Exploiting similarities among languages for ma-
chine translation. CoRR abs/1309.4168.

Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Cor-
rado, and Jeff Dean. 2013c. Distributed represen-
tations of words and phrases and their composition-
ality. In C. J. C. Burges, L. Bottou, M. Welling,
Z. Ghahramani, and K. Q. Weinberger, editors, Ad-
vances in Neural Information Processing Systems
26. Curran Associates, Inc., pages 3111–3119.

Ivan Vulić and Anna Korhonen. 2016. On the role
of seed lexicons in learning bilingual word embed-
dings. In Proc. ACL.

Chao Xing, Dong Wang, Chao Liu, and Yiye Lin.
2015. Normalized word embedding and orthogonal
transform for bilingual word translation. In Proc.
NAACL.

193


