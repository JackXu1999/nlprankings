



















































Text Categorization as a Graph Classification Problem


Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics
and the 7th International Joint Conference on Natural Language Processing, pages 1702–1712,

Beijing, China, July 26-31, 2015. c©2015 Association for Computational Linguistics

Text Categorization as a Graph Classification Problem

François Rousseau Emmanouil Kiagias

LIX, École Polytechnique, France

Michalis Vazirgiannis

Abstract

In this paper, we consider the task of
text categorization as a graph classifica-
tion problem. By representing textual doc-
uments as graph-of-words instead of his-
torical n-gram bag-of-words, we extract
more discriminative features that corre-
spond to long-distance n-grams through
frequent subgraph mining. Moreover, by
capitalizing on the concept of k-core, we
reduce the graph representation to its dens-
est part – its main core – speeding up the
feature extraction step for little to no cost
in prediction performances. Experiments
on four standard text classification datasets
show statistically significant higher accu-
racy and macro-averaged F1-score com-
pared to baseline approaches.

1 Introduction

The task of text categorization finds applications
in a wide variety of domains, from news filter-
ing and document organization to opinion mining
and spam detection. With the ever-growing quan-
tity of information available online nowadays, it
is crucial to provide effective systems capable of
classifying text in a timely fashion. Compared
to other application domains of classification, its
specificity lies in its high number of features, its
sparse feature vectors and its skewed multiclass
scenario. For instance, when dealing with thou-
sands of news articles, it is not uncommon to have
millions of n-gram features, only a few hundreds
actually present in each document and tens of class
labels – some of them with thousands of articles
and some others will only a few hundreds. These
particularities have to be taken into account when
envisaging a different representation for a docu-
ment and in our case when considering the task as
a graph classification problem.

Graphs are powerful data structures that are
used to represent complex information about en-
tities and interaction between them and we think
text makes no exception. Historically, following
the traditional bag-of-words representation, uni-
grams have been considered as the natural features
and later extended to n-grams to capture some
word dependency and word order. However, n-
grams correspond to sequences of words and thus
fail to capture word inversion and subset match-
ing (e. g., “article about news” vs. “news article”).
We believe graphs can help solve these issues like
they did for instance with chemical compounds
where repeating substructure patterns are good in-
dicators of belonging to one particular class, e. g.,
predicting carcinogenicity in molecules (Helma et
al., 2001). Graph classification has received a
lot of attention this past decade and various tech-
niques have been developed to deal with the task
but rarely applied on textual data and at its scale.

In our work, we explored a graph representation
of text, namely graph-of-words, to challenge the
traditional bag-of-words representation and help
better classify textual documents into categories.
We first trained a classifier using frequent sub-
graphs as features for increased effectiveness. We
then reduced each graph-of-words to its main core
before mining the features for increased efficiency.
Finally, we also used this technique to reduce the
total number of n-gram features considered in the
baselines for little to no loss in prediction perfor-
mances.

The rest of the paper is organized as follows.
Section 2 provides a review of the related work.
Section 3 defines the preliminary concepts upon
which our work is built. Section 4 introduces the
proposed approaches. Section 5 describes the ex-
perimental settings and presents the results we ob-
tained on four standard datasets. Finally, Section
6 concludes our paper and mentions future work
directions.

1702



2 Related work

In this section, we present the related work in text
categorization, graph classification and the com-
bination of the two fields like in our case.

2.1 Text categorization

Text categorization, a.k.a. text classification, cor-
responds to the task of automatically predicting
the class label of a given textual document. We
refer to (Sebastiani, 2002) for an in-depth re-
view of the earliest works in the field and (Ag-
garwal and Zhai, 2012) for a survey of the more
recent works that capitalize on additional meta-
information. We note in particular the seminal
work of Joachims (1998) who was the first to pro-
pose the use of a linear SVM with TF×IDF term
features for the task. This approach is one of the
standard baselines because of its simplicity yet ef-
fectiveness (unsupervised n-gram feature mining
followed by standard supervised learning). An-
other popular approach is the use of Naive Bayes
and its multiple variants (McCallum and Nigam,
1998), in particular for the subtask of spam de-
tection (Androutsopoulos et al., 2000). Finally,
there are a couple of works such as (Hassan et
al., 2007) that used the graph-of-words representa-
tion to propose alternative weights for the n-gram
features but still without considering the task as a
graph classification problem.

2.2 Graph classification

Graph classification corresponds to the task of au-
tomatically predicting the class label of a given
graph. The learning part in itself does not differ
from other supervised learning problems and most
proposed methods deal with the feature extrac-
tion part. They fall into two main categories: ap-
proaches that consider subgraphs as features and
graph kernels.

2.2.1 Subgraphs as features
The main idea is to mine frequent subgraphs and
use them as features for classification, be it with
Adaboost (Kudo et al., 2004) or a linear SVM
(Deshpande et al., 2005). Indeed, most datasets
that were used in the associated experiments cor-
respond to chemical compounds where repeating
substructure patterns are good indicators of be-
longing to one particular class. Some popular
graph pattern mining algorithms are gSpan (Yan
and Han, 2002), FFSM (Huan et al., 2003) and

Gaston (Nijssen and Kok, 2004). The number of
frequent subgraphs can be enormous, especially
for large graph collections, and handling such a
feature set can be very expensive. To overcome
this issue, recent works have proposed to retain
or even only mine the discriminative subgraphs,
i. e. features that contribute to the classification
decision, in particular gBoost (Saigo et al., 2009),
CORK (Thoma et al., 2009) and GAIA (Jin et
al., 2010). However, when experimenting, gBoost
did not converge on our larger datasets while
GAIA and CORK consider subgraphs of node
size at least 2, which exclude unigrams, result-
ing in poorer performances. Moreover, all these
approaches have been developed for binary clas-
sification, which meant mining features as many
times as the number of classes instead of just once
(one-vs-all learning strategy). In this paper, we
tackle the scalability issue differently through an
unsupervised feature selection approach to reduce
the size of the graphs and a fortiori the number of
frequent subgraphs.

2.2.2 Graph kernels
Gärtner et al. (2003) proposed the first kernels be-
tween graphs (as opposed to previous kernels on
graphs, i. e. between nodes) based on either ran-
dom walks or cycles to tackle the problem of clas-
sification between graphs. In parallel, the idea of
marginalized kernels was extended to graphs by
Kashima et al. (2003) and by Mahé et al. (2004).
We refer to (Vishwanathan et al., 2010) for an in-
depth review of the topic and in particular its lim-
itations in terms of number of unique node labels,
which make them unsuitable for our problem as
tested in practice (limited to a few tens of unique
labels compared to hundreds of thousands for us).

2.3 Similar works

The work of Markov et al. (2007) is perhaps the
closest to ours since they also perform subgraph
feature mining on graph-of-words representations
but with non-standard datasets and baselines. The
works of Jiang et al. (2010) and Arora et al. (2010)
are also related but their representations are dif-
ferent and closer to parse and dependency trees
used as base features for text categorization by
Kudo and Matsumoto (2004) and Matsumoto et al.
(2005). Moreover, they do not discuss the choice
of the support value, which controls the total num-
ber of features and can potentially lead to millions
of subgraphs on standard datasets.

1703



3 Preliminary concepts

In this section, we introduce the preliminary con-
cepts upon which our work is built.

3.1 Graph-of-words

We model a textual document as a graph-of-words,
which corresponds to a graph whose vertices rep-
resent unique terms of the document and whose
edges represent co-occurrences between the terms
within a fixed-size sliding window. The under-
lying assumption is that all the words present in
a document have some undirected relationships
with the others, modulo a window size outside of
which the relationship is not considered. This rep-
resentation was first used in keyword extraction
and summarization (Ohsawa et al., 1998; Mihal-
cea and Tarau, 2004) and more recently in ad hoc
IR (Blanco and Lioma, 2012; Rousseau and Vazir-
giannis, 2013). We refer to (Blanco and Lioma,
2012) for an in-depth review of the graph repre-
sentations of text in NLP.

system softwar
implement

disciplin

scienc

span

topic

theoret

studi

rang

limit

algorithm

issu

practic

hardwar

comput

As a discipline, computer science spans a range of topics 
from theoretical studies of algorithms and the limits of 
computation to the practical issues of implementing 
computing systems in hardware and software.

hide meFigure 1: Graph-of-words representation of a tex-
tual document – in bold font, its main core.

Figure 1 illustrates the graph-of-words repre-
sentation of a textual document. The vertices cor-
respond to the remaining terms after standard pre-
processing steps have been applied (tokenization,
stop word removal and stemming). The undirected
edges were drawn between terms co-occurring
within a sliding window over the processed text of
size 4, value consistently reported as working well
in the references aforementioned and validated in
our experiments. Edge direction was used by Fil-
ippova (2010) so as to extract valid sentences but

not here in order to capture some word inversion.
Note that for small-enough window sizes

(which is typically the case in practice), we can
consider that two terms linked represent a long-
distance bigram (Bassiou and Kotropoulos, 2010),
if not a bigram. Furthermore, by extending the
denomination, we can consider that a subgraph
of size n is a long-distance n-gram, if not an n-
gram. Indeed, the nodes belonging to a subgraph
do not necessarily appear in a sequence in the doc-
ument like for a n-gram. Moreover, this enables us
to “merge” together n-grams that share the same
terms but maybe not in the same order. In the ex-
periments, by abusing the terminology, we will re-
fer to them as n-grams to adopt a common termi-
nology with the baseline approaches.

3.2 Node/edge labels and subgraph matching
In graph classification, it is common to introduce
a node labeling function µ to map a node id to its
label. For instance, consider the case of chemi-
cal compounds (e. g., the benzene C6H6). Then in
its graph representation (its “structural formula”),
it is crucial to differentiate between the multiple
nodes labeled the same (e. g., C or H). In the case
of graph-of-words, node labels are unique inside
a graph since they represent unique terms of the
document and we can therefore omit these func-
tions since they are injective in our case and we
can substitute node ids for node labels. In partic-
ular, the general problem of subgraph matching,
which defines an isomorphism between a graph
and a subgraph and is NP-complete (Garey and
Johnson, 1990), can be reduced to a polynomial
problem when node labels are unique. In our ex-
periments, we used the standard algorithm VF2
developed by Cordella et al. (2001).

3.3 K-core and main core
Seidman (1983) defined the k-core of a graph as
the maximal connected subgraph whose vertices
are at least of degree k within the subgraph. The
non-empty k-core of largest k is called the main
core and corresponds to the most cohesive set(s)
of vertices. The corresponding value of k may dif-
fer from one graph to another. Batagelj and Za-
veršnik (2003) proposed an algorithm to extract
the main core of an unweighted graph in time lin-
ear in the number of edges, complexity similar
in our case to the other NLP preprocessing steps.
Bold font on Figure 1 indicates that a vertex be-
longs to the main core of the graph.

1704



4 Graph-of-words classification

In this section, we present our work and the sev-
eral approaches we explored, from unsupervised
feature mining using gSpan to propose more dis-
criminative features than standard n-grams to un-
supervised feature selection using k-core to reduce
the total number of subgraph and n-gram features.

4.1 Unsupervised feature mining using gSpan

We considered the task of text categorization as a
graph classification problem by representing tex-
tual documents as graph-of-words and then ex-
tracting subgraph features to train a graph classi-
fier. Each document is a separate graph-of-words
and the collection of documents thus corresponds
to a set of graphs. Therefore, for larger datasets,
the total number of graphs increases but not the
average graph size (the average number of unique
terms in a text), assuming homogeneous datasets.

Because the total number of unique node la-
bels corresponds to the number of unique terms
in the collection in our case, graph kernels are
not suitable for us as verified in practice using the
MATLAB code made available by Shervashidze
(2009). We therefore only explored the meth-
ods that consider subgraphs as features. Repeat-
ing substructure patterns between graphs are intu-
itively good candidates for classification since, at
least for chemical compounds, shared subparts of
molecules are good indicators of belonging to one
particular class. We assumed it would the same for
text. Indeed, subgraphs of graph-of-words corre-
spond to sets of words co-occurring together, just
not necessarily always as the same sequence like
for n-grams – it can be seen as a relaxed definition
of a n-gram to capture additional variants.

We used gSpan (graph-based Substructure
pattern (Yan and Han, 2002)) as frequent sub-
graph miner like (Jiang et al., 2010; Arora et al.,
2010) mostly because of its fast available C++
implementation from gBoost (Saigo et al., 2009).
Briefly, the key idea behind gSpan is that in-
stead of enumerating all the subgraphs and test-
ing for isomorphism throughout the collection, it
first builds for each graph a lexicographic order
of all the edges using depth-first-search (DFS)
traversal and assigns to it a unique minimum DFS
code. Based on all these DFS codes, a hierarchical
search tree is constructed at the collection-level.
By pre-order traversal of this tree, gSpan discov-
ers all frequent subgraphs with required support.

Consider the set of all subgraphs in the collec-
tion of graphs, which corresponds to the set of all
potential features. Note that there may be overlap-
ping (subgraphs sharing nodes/edges) and redun-
dant (subgraphs included in others) features. Be-
cause its size is exponential in the number of edges
(just like the number of n-grams is exponential in
n), it is common to only retain/mine the most fre-
quent subgraphs (again just like for n-grams with a
minimum document frequency (Fürnkranz, 1998;
Joachims, 1998)). This is controlled via a param-
eter known as the support, which sets the mini-
mum number of graphs in which a given subgraph
has to appear to be considered as a feature, i. e.
the number of subgraph matches in the collection.
Here, since node labels are unique inside a graph,
we do not have to consider multiple occurrences
of the same subgraph in a given graph. The lower
the support, the more features selected/considered
but the more expensive the mining and the training
(not only in time spent for the learning but also for
the feature vector generation).

4.2 Unsupervised support selection

The optimal value for the support can be learned
through cross-validation so as to maximize the
prediction accuracy of the subsequent classifier,
making the whole feature mining process super-
vised. But if we consider that the classifier can
only improve its goodness of fit with more fea-
tures (the sets of features being nested as the sup-
port varies), it is likely that the lowest support will
lead to the best test accuracy; assuming subse-
quent regularization to prevent overfitting. How-
ever, this will come at the cost of an exponential
number of features as observed in practice. In-
deed, as the support decreases, the number of fea-
tures increases slightly up until a point where it
increases exponentially, which makes both the fea-
ture vector generation and the learning expensive,
especially with multiple classes. Moreover, we
observed that the prediction performances did not
benefit that much from using all the possible fea-
tures (support of 1) as opposed to a more manage-
able number of features corresponding to a higher
support. Therefore, we propose to select the sup-
port using the so-called elbow method. This is an
unsupervised empirical method initially developed
for selecting the number of clusters in k-means
(Thorndike, 1953). Figure 3 (upper plots) in Sec-
tion 5 illustrates this process.

1705



4.3 Considered classifiers
In text categorization, standard baseline classifiers
include k-nearest neighbors (kNN) (Larkey and
Croft, 1996), Naive Bayes (NB) (McCallum and
Nigam, 1998) and linear Support Vector Machines
(SVM) (Joachims, 1998) with the latter perform-
ing the best on n-gram features as verified in our
experiments. Since our subgraph features corre-
spond to “long-distance n-grams”, we used linear
SVMs as our classifiers in all our experiments –
the goal of our work being to explore and propose
better features rather than a different classifier.

4.4 Multiclass scenario
In standard binary graph classification (e. g., pre-
dicting chemical compounds’ carcinogenicity as
either positive or negative (Helma et al., 2001)),
feature mining is performed on the whole graph
collection as we expect the mined features to be
able to discriminate between the two classes (thus
producing a good classifier). However, for the
task of text categorization, there are usually more
than two classes (e. g., 118 categories of news ar-
ticles for the Reuters-21578 dataset) and with a
skewed class distribution (e. g., a lot more news
related to “acquisition” than to “grain”). There-
fore, a single support value might lead to some
classes generating a tremendous number of fea-
tures (e. g., hundreds of thousands of frequent sub-
graphs) and some others only a few (e. g., a few
hundreds subgraphs) resulting in a skewed and
non-discriminative feature set. To include dis-
criminative features for these minority classes, we
would need an extremely low support resulting
in an exponential number of features because of
the majority classes. For these reasons, we de-
cided to mine frequent subgraphs per class using
the same relative support (%) and then aggregat-
ing each feature set into a global one at the cost of
a supervised process (but which still avoids cross-
validated parameter tuning). This was not needed
for the tasks of spam detection and opinion mining
since the corresponding datasets consist of only
two balanced classes.

4.5 Main core mining using gSpan
Since the main drawback of mining frequent sub-
graphs for text categorization rather than chemical
compound classification is the very high number
of possible subgraphs because of the size of the
graphs and the total number of graphs (more than
10x in both cases), we thought of ways to reduce

the graphs’ sizes while retaining as much classifi-
cation information as possible.

The graph-of-words representation is designed
to capture dependency between words, i. e. de-
pendency between features in the context of ma-
chine learning but at the document-level. Ini-
tially, we wanted to capture recurring sets of words
(i. e. take into account word inversion and sub-
set matching) and not just sequences of words like
with n-grams. In terms of subgraphs, this means
words that co-occur with each other and form a
dense subgraph as opposed to a path like for a n-
gram. Therefore, when reducing the graphs, we
need to keep their densest part(s) and that is why
we considered extracting their main cores. Com-
pared to other density-based algorithms, retaining
the main core of a graph has the advantage of be-
ing linear in the number of edges, i. e. in the num-
ber of unique terms in a document in our case (the
number of edges is at most the number of nodes
times the fixed size of the sliding window, a small
constant in practice).

4.6 Unsupervised n-gram feature selection

Similarly to (Hassan et al., 2007) that used graph-
of-words to propose alternative weights for the n-
gram features, we can capitalize on main core re-
tention to still extract binary n-gram features for
classification but considering only the terms be-
longing to the main core of each document. Be-
cause some terms never belong to any main core
of any document, the dimension of the overall fea-
ture space decreases. Additionally, since a docu-
ment is only represented by a subset of its original
terms, the number of non-zero feature values per
document also decreases, which matters for SVM,
even for the linear kernel, when considering the
dual formulation or in the primal with more recent
optimization techniques (Joachims, 2006).

Compared to most existing feature selection
techniques in the field (Yang and Pedersen, 1997),
it is unsupervised and corpus-independent as it
does not rely on any labeled data like IG, MI
or χ2 nor any collection-wide statistics like IDF,
which can be of interest for large-scale text cate-
gorization in order to process documents in paral-
lel, independently of each other. In some sense,
it is similar to what Özgür et al. (2005) proposed
with corpus-based and class-based keyword selec-
tion for text classification except that we use here
document-based keyword selection following the
approach from Rousseau and Vazirgiannis (2015).

1706



5 Experiments

In this section we present the experiments we con-
ducted to validate our approaches.

5.1 Datasets
We used four standard text datasets: two for multi-
class document categorization (WebKB and R8),
one for spam detection (LingSpam) and one for
opinion mining (Amazon) so as to cover all the
main subtasks of text categorization:

• WebKB: 4 most frequent categories among
labeled webpages from various CS depart-
ments – split into 2,803 for training and 1,396
for test (Cardoso-Cachopo, 2007, p. 39–41).

• R8: 8 most frequent categories of Reuters-
21578, a set of labeled news articles from the
1987 Reuters newswire – split into 5,485 for
training and 2,189 for test (Debole and Se-
bastiani, 2005).

• LingSpam: 2,893 emails classified as spam
or legitimate messages – split into 10 sets for
10-fold cross validation (Androutsopoulos et
al., 2000).

• Amazon: 8,000 product reviews over four
different sub-collections (books, DVDs, elec-
tronics and kitchen appliances) classified as
positive or negative – split into 1,600 for
training and 400 for test each (Blitzer et al.,
2007).

5.2 Implementation
We developed our approaches mostly in Python
using the igraph library (Csardi and Nepusz,
2006) for the graph representation and main core
extraction. For unsupervised subgraph feature
mining, we used the C++ implementation of
gSpan from gBoost (Saigo et al., 2009). Finally
for classification and standard n-gram text catego-
rization we used scikit (Pedregosa et al., 2011),
a standard Python machine learning library.

5.3 Evaluation metrics
To evaluate the performance of our proposed ap-
proaches over standard baselines, we computed on
the test set both the micro- and macro-average F1-
score. Because we are dealing with single-label
classification, the micro-average F1-score corre-
sponds to the accuracy and is a measure of the
overall prediction effectiveness (Manning et al.,

Dataset # subgraphs before # subgraphs after reduction
WebKB 30,868 10,113 67 %

R8 39,428 11,373 71 %
LingSpam 54,779 15,514 72 %
Amazon 16,415 8,745 47 %

Dataset # n-grams before # n-grams after reduction
WebKB 1,849,848 735,447 60 %

R8 1,604,280 788,465 51 %
LingSpam 2,733,043 1,016,061 63 %
Amazon 583,457 376,664 35 %

Table 1: Total number of features (n-grams or sub-
graphs) vs. number of features present only in
main cores along with the reduction of the dimen-
sion of the feature space on all four datasets.

2008, p. 281). Conversely, the macro-average F1-
score takes into account the skewed class label dis-
tributions by weighting each class uniformly. The
statistical significance of improvement in accuracy
over the n-gram SVM baseline was assessed us-
ing the micro sign test (p < 0.05) (Yang and Liu,
1999). For the Amazon dataset, we report the av-
erage of each metric over the four sub-collections.

5.4 Results
Table 2 shows the results on the four considered
datasets. The first three rows correspond to the
baselines: unsupervised n-gram feature extrac-
tion and then supervised learning using kNN, NB
(Multinomial but Bernoulli yields similar results)
and linear SVM. The last three rows correspond to
our approaches.

In our first approach, denoted as “gSpan +
SVM”, we mine frequent subgraphs (gSpan) as
features and then train a linear SVM. These fea-
tures correspond to long-distance n-grams. This
leads to the best results in text categorization on
almost all datasets (all if we compare to baseline
methods), in particular on multiclass document
categorization (R8 and WebKB).

In our second approach, denoted as “MC +
gSpan + SVM”, we repeat the same procedure
except that we mine frequent subgraphs (gSpan)
from the main core (MC) of each graph-of-words
and then train an SVM on the resulting features.
Main cores can vary from 1-core to 12-core de-
pending on the graph structure, 5-core and 6-core
being the most frequent (more than 60%). This
yields results similar to the SVM baseline for a
faster mining and training compared to gSpan +
SVM. Table 1 (upper table) shows the reduction
in the dimension of the feature space and we see

1707



Table 2: Test accuracy and macro-average F1-score on four standard datasets. Bold font marks the best
performance in a column. * indicates statistical significance at p < 0.05 using micro sign test with regards
to the SVM baseline of the same column. MC corresponds to unsupervised feature selection using the
main core of each graph-of-words to extract n-gram and subgraph features. gSpan mining support values
are 1.6% (WebKB), 7% (R8), 4% (LingSpam) and 0.5% (Amazon).

Method
Dataset WebKB R8 LingSpam Amazon

Accuracy F1-score Accuracy F1-score Accuracy F1-score Accuracy F1-score
kNN (k=5) 0.679 0.617 0.894 0.705 0.910 0.774 0.512 0.644
NB (Multinomial) 0.866 0.861 0.934 0.839 0.990 0.971 0.768 0.767
linear SVM 0.889 0.871 0.947 0.858 0.991 0.973 0.792 0.790
gSpan + SVM 0.912* 0.882 0.955* 0.864 0.991 0.972 0.798* 0.795
MC + gSpan + SVM 0.901* 0.871 0.949* 0.858 0.990 0.973 0.800* 0.798
MC + SVM 0.872 0.863 0.937 0.849 0.990 0.972 0.786 0.774

# non-zero n-gram feature values before unsupervised feature selection
0

50

100

150

200

250

#
 d

o
cu

m
e
n
ts

0 1000 2000 3000 4000 5000
# non-zero n-gram feature values after unsupervised feature selection

0

50

100

150

200

250

#
 d

o
cu

m
e
n
ts

Figure 2: Distribution of non-zero n-gram feature
values before and after unsupervised feature selec-
tion (main core retention) on R8 dataset.

that on average less than 60% of the subgraphs are
kept for little to no cost in prediction effectiveness.

In our final approach, denoted as “MC + SVM”,
we performed unsupervised feature selection by
keeping the terms appearing in the main core (MC)
of each document’s graph-of-words representation
and then extracted standard n-gram features. Ta-
ble 1 (lower table) shows the reduction in the di-
mension of the feature space and we see that on av-
erage less than half the n-grams remain. Figure 2
shows the distribution of non-zero features before
and after the feature selection on the R8 dataset.
Similar changes in distribution can be observed on
the other datasets, from a right-tail Gaussian to a
power law distribution as expected from the main
core retention. Table 2 shows that the main core
retention has little to no cost in accuracy and F1-
score but can reduce drastically the feature space
and the number of non-zero values per document.

1 2 3 4

support (%)

0

50k

100k

150k

200k

250k

#
 f

e
a
tu

re
s

5 6 7 8 9 10 11 12 13

support (%)

1 2 3 4

support (%)

0.85

0.90

0.95

1.00

a
cc

u
ra

cy

5 6 7 8 9 10 11 12 13

support (%)

Figure 3: Number of subgraph features/accuracy
in test per support (%) on WebKB (left) and R8
(right) datasets: in black, the selected support
value chosen via the elbow method and in red, the
accuracy in test for the SVM baseline.

5.5 Unsupervised support selection

Figure 3 above illustrates the unsupervised heuris-
tic (elbow method) we used to select the support
value, which corresponds to the minimum number
of graphs in which a subgraph has to appear to be
considered frequent. We noticed that as the sup-
port decreases, the number of features increases
slightly up until a point where it increases expo-
nentially. This support value, highlighted in black
on the figure and chosen before taking into ac-
count the class label, is the value we used in our
experiments and for which we report the results in
Table 1 and 2. The lower plots provide evidence

1708



1-grams 2-grams 3-grams 4-grams 5-grams 6-grams
0

20

40

60

80

100
#

 f
e
a
tu

re
s 

(%
)

baseline

gSpan

MC + gSpan

Figure 4: Distribution of n-grams (standard and
long-distance ones) among all the features on We-
bKB dataset.

that the elbow method helps selecting in an unsu-
pervised manner a support that leads to the best or
close to the best accuracy.

5.6 Distribution of mined n-grams

In order to gain more insights on why the long-
distance n-grams mined with gSpan result in bet-
ter classification performances than the baseline n-
grams, we computed the distribution of the num-
ber of unigrams, bigrams, etc. up to 6-grams in the
traditional feature set and ours (Figure 4) as well
as in the top 5% features that contribute the most
to the classification decision of the trained SVM
(Figure 5). Again, a long-distance n-gram corre-
sponds to a subgraph of size n in a graph-of-words
and can be seen as a relaxed definition of the tra-
ditional n-gram, one that takes into account word
inversion for instance. To obtain comparable re-
sults, we considered for the baseline n-grams with
a minimum document frequency equal to the sup-
port. Otherwise, by definition, there are at least as
many bigrams as there are unigrams and so forth.

Figure 4 shows that our approaches mine way
more n-grams than unigrams compared to the
baseline. This happens because with graph-of-
words a subgraph of size n corresponds to a set
of n terms while with bag-of-words a n-gram cor-
responds to a sequence of n terms. Note that even
when restricting the subgraphs to the main cores,
there are still more higher order n-grams mined.

Figure 5 shows that the higher order n-grams
still contribute indeed to the classification deci-
sion and in higher proportion than with the base-
line, even when restricting to the main cores. For

1-grams 2-grams 3-grams 4-grams 5-grams 6-grams
0

20

40

60

80

100

#
 f

e
a
tu

re
s 

(%
)

baseline SVM

gSpan + SVM

MC + gSpan + SVM

Figure 5: Distribution of n-grams (standard and
long-distance ones) among the top 5% most dis-
criminative features for SVM on WebKB dataset.

instance, on the R8 dataset, {bank, base, rate}
was a discriminative (top 5% SVM features) long-
distance 3-gram for the category “interest” and
occurred in documents in the form of “barclays
bank cut its base lending rate”, “midland bank
matches its base rate” and “base rate of natwest
bank dropped”, pattern that would be hard to cap-
ture with traditional n-gram bag-of-words.

5.7 Timing

With an Intel Core i5-3317U clocking at 2.6GHz
and 8GB of RAM, mining the subgraph features
with gSpan takes on average 30s for the selected
support. It can take several hours with lower sup-
port and goes down to 5s using the main cores.

6 Conclusion

In this paper, we tackled the task of text cate-
gorization by representing documents as graph-
of-words and then considering the problem as a
graph classification one. We were able to extract
more discriminative features that correspond to
long-distance n-grams through frequent subgraph
mining. Experiments on four standard datasets
show statistically significant higher accuracy and
macro-averaged F1-score compared to baselines.

To the best of our knowledge, graph classifi-
cation has never been tested at that scale – thou-
sands of graphs and tens of thousands of unique
node labels – and also in the multiclass scenario.
For these reasons, we could not capitalize on all
standard methods. In particular, we believe new
kernels that support a very high number of unique
node labels could yield even better performances.

1709



References
Charu C. Aggarwal and ChengXiang Zhai. 2012. A

Survey of Text Classification Algorithms. In Mining
Text Data, pages 163–222.

Ion Androutsopoulos, John Koutsias, Konstantinos V.
Chandrinos, George Paliouras, and Constantine D.
Spyropoulos. 2000. An Evaluation of Naive
Bayesian Anti-Spam Filtering. In Proceedings of
the Workshop on Machine Learning in the New In-
formation Age, 11th European Conference on Ma-
chine Learning, pages 9–17.

Shilpa Arora, Elijah Mayfield, Carolyn Penstein-Rosé,
and Eric Nyberg. 2010. Sentiment Classification
Using Automatically Extracted Subgraph Features.
In Proceedings of the NAACL HLT 2010 Workshop
on Computational Approaches to Analysis and Gen-
eration of Emotion in Text, CAAGET ’10, pages
131–139.

Nikoletta Bassiou and Constantine Kotropoulos. 2010.
Word Clustering Using PLSA Enhanced with Long
Distance Bigrams. In Proceedings of the 20th Inter-
national Conference on Pattern Recognition, ICPR
’10, pages 4226–4229.

Vladimir Batagelj and Matjaž Zaversnik. 2003.
An O(m) Algorithm for Cores Decomposition of
Networks. The Computing Research Repository
(CoRR), cs.DS/0310049.

Roi Blanco and Christina Lioma. 2012. Graph-based
term weighting for information retrieval. Informa-
tion Retrieval, 15(1):54–92.

John Blitzer, Mark Dredze, and Fernando Pereira.
2007. Biographies, bollywood, boomboxes and
blenders: Domain adaptation for sentiment classi-
fication. In Proceedings of the 45th Annual Meet-
ing of the Association of Computational Linguistics,
ACL ’07, pages 440–447.

Ana Cardoso-Cachopo. 2007. Improving Methods for
Single-label Text Categorization. Ph.D. thesis, Insti-
tuto Superior Técnico, Universidade de Lisboa, Lis-
bon, Portugal.

Luigi Pietro Cordella, Pasquale Foggia, Carlo Sansone,
and Mario Vento. 2001. An improved algorithm for
matching large graphs. In Proceedings of the 3rd
IAPR-TC15 Workshop on Graph-based Representa-
tions in Pattern Recognition, pages 149–159.

Gabor Csardi and Tamas Nepusz. 2006. The igraph
software package for complex network research. In-
terJournal, Complex Systems, 1695(5):1–9.

Franca Debole and Fabrizio Sebastiani. 2005. An
Analysis of the Relative Hardness of Reuters-21578
Subsets: Research Articles. Journal of the Ameri-
can Society for Information Science and Technology,
56(6):584–596.

Mukund Deshpande, Michihiro Kuramochi, Nikil
Wale, and George Karypis. 2005. Fre-
quent Substructure-Based Approaches for Classi-
fying Chemical Compounds. IEEE Transactions
on Knowledge and Data Engineering, 17(8):1036–
1050.

Katja Filippova. 2010. Multi-sentence Compression:
Finding Shortest Paths in Word Graphs. In Proceed-
ings of the 23rd International Conference on Com-
putational Linguistics, COLING ’10, pages 322–
330.

Johannes Fürnkranz. 1998. A study using n-gram
features for text categorization. Technical Report
OEFAI-TR-98-30, Austrian Research Institute for
Artificial Intelligence.

Michael R. Garey and David S. Johnson. 1990. Com-
puters and Intractability; A Guide to the Theory of
NP-Completeness. W. H. Freeman & Co.

Thomas Gärtner, Peter Flach, and Stefan Wrobel.
2003. On graph kernels: Hardness results and
efficient alternatives. In Proceedings of the An-
nual Conference on Computational Learning The-
ory, COLT ’03, pages 129–143.

Samer Hassan, Rada Mihalcea, and Carmen Banea.
2007. Random-Walk Term Weighting for Improved
Text Classification. In Proceedings of the Interna-
tional Conference on Semantic Computing, ICSC
’07, pages 242–249.

Christoph Helma, Ross D. King, Stefan Kramer,
and Ashwin Srinivasan. 2001. The predictive
toxicology challenge 2000–2001. Bioinformatics,
17(1):107–108.

Jun Huan, Wei Wang, and Jan Prins. 2003. Efficient
Mining of Frequent Subgraphs in the Presence of
Isomorphism. In Proceedings of the 3rd IEEE In-
ternational Conference on Data Mining, ICDM ’03,
pages 549–552.

Chuntao Jiang, Frans Coenen, Robert Sanderson, and
Michele Zito. 2010. Text classification using graph
mining-based feature extraction. Knowledge-Based
Systems, 23(4):302–308.

Ning Jin, Calvin Young, and Wei Wang. 2010. GAIA:
graph classification using evolutionary computation.
In Proceedings of the 2010 ACM SIGMOD interna-
tional conference on Management of data, SIGMOD
’10, pages 879–890.

Thorsten Joachims. 1998. Text categorization with
Support Vector Machines: Learning with many rel-
evant features. In Proceedings of the 10th European
Conference on Machine Learning, ECML ’98, pages
137–142.

Thorsten Joachims. 2006. Training Linear SVMs
in Linear Time. In Proceedings of the 12th ACM
SIGKDD international conference on Knowledge
Discovery and Data mining, KDD ’06, pages 217–
226.

1710



Hisashi Kashima, Koji Tsuda, and Akihiro Inokuchi.
2003. Marginalized kernels between labeled graphs.
In Proceedings of the 20th International Conference
on Machine Learning, volume 3 of ICML ’03, pages
321–328.

Taku Kudo and Yuji Matsumoto. 2004. A Boosting Al-
gorithm for Classification of Semi-Structured Text.
In Proceedings of the 9th Conference on Empirical
Methods in Natural Language Processing, volume 4
of EMNLP ’04, pages 301–308.

Taku Kudo, Eisaku Maeda, and Yuji Matsumoto. 2004.
An application of boosting to graph classification.
In Advances in Neural Information Processing Sys-
tems 17, NIPS ’04, pages 729–736.

Leah S. Larkey and W. Bruce Croft. 1996. Combining
Classifiers in Text Categorization. In Proceedings
of the 19th annual international ACM SIGIR confer-
ence on Research and development in information
retrieval, SIGIR ’96, pages 289–297.

Pierre Mahé, Nobuhisa Ueda, Tatsuya Akutsu, Jean-
Luc Perret, and Jean-Philippe Vert. 2004. Exten-
sions of marginalized graph kernels. In Proceed-
ings of the 21st International Conference on Ma-
chine Learning, ICML ’04, pages 70–78.

Christopher D. Manning, Prabhakar Raghavan, and
Hinrich Schütze. 2008. Introduction to Information
Retrieval. Cambridge University Press, New York,
NY, USA.

Alex Markov, Mark Last, and Abraham Kandel. 2007.
Fast Categorization of Web Documents Represented
by Graphs. In Advances in Web Mining and Web
Usage Analysis, number 4811 in Lecture Notes in
Artificial Intelligence, pages 56–71.

Shotaro Matsumoto, Hiroya Takamura, and Manabu
Okumura. 2005. Sentiment Classification Using
Word Sub-sequences and Dependency Sub-trees. In
Proceedings of the 9th Pacific-Asia Conference on
Advances in Knowledge Discovery and Data Min-
ing, PAKDD ’05, pages 301–311.

Andrew McCallum and Kamal Nigam. 1998. A com-
parison of event models for Naive Bayes text classi-
fication. In Proceedings of the AAAI workshop on
learning for text categorization, AAAI ’98, pages
41–48.

Rada Mihalcea and Paul Tarau. 2004. TextRank:
Bringing Order into Texts. In Proceedings of the 9th
Conference on Empirical Methods in Natural Lan-
guage Processing, EMNLP ’04, pages 404–411.

Siegfried Nijssen and Joost N. Kok. 2004. A Quick-
start in Frequent Structure Mining Can Make a Dif-
ference. In Proceedings of the 10th ACM SIGKDD
international conference on Knowledge Discovery
and Data mining, KDD ’04, pages 647–652.

Yukio Ohsawa, Nels E. Benson, and Masahiko
Yachida. 1998. KeyGraph: Automatic Indexing by
Co-occurrence Graph Based on Building Construc-
tion Metaphor. In Proceedings of the Advances in
Digital Libraries Conference, ADL ’98, pages 12–
18.

Arzucan Özgür, Levent Özgür, and Tunga Güngör.
2005. Text Categorization with Class-based and
Corpus-based Keyword Selection. In Proceedings
of the 20th International Conference on Computer
and Information Sciences, ISCIS ’05, pages 606–
615.

Fabian Pedregosa, Gaël Varoquaux, Alexandre Gram-
fort, Vincent Michel, Bertrand Thirion, Olivier
Grisel, Mathieu Blondel, Peter Prettenhofer, Ron
Weiss, Vincent Dubourg, J. Vanderplas, A. Pas-
sos, D. Cournapeau, M. Brucher, M. Perrot, and
E. Duchesnay. 2011. Scikit-learn: Machine learn-
ing in Python. The Journal of Machine Learning
Research, 12:2825–2830.

François Rousseau and Michalis Vazirgiannis. 2013.
Graph-of-word and TW-IDF: New Approach to Ad
Hoc IR. In Proceedings of the 22nd ACM inter-
national conference on Information and knowledge
management, CIKM ’13, pages 59–68.

François Rousseau and Michalis Vazirgiannis. 2015.
Main Core Retention on Graph-of-words for Single-
Document Keyword Extraction. In Proceedings of
the 37th European Conference on Information Re-
trieval, ECIR ’15, pages 382–393.

Hiroto Saigo, Sebastian Nowozin, Tadashi Kadowaki,
Taku Kudo, and Koji Tsuda. 2009. gBoost: a math-
ematical programming approach to graph classifica-
tion and regression. Machine Learning, 75(1):69–
89.

Fabrizio Sebastiani. 2002. Machine Learning in Au-
tomated Text Categorization. ACM Computing Sur-
veys, 34(1):1–47.

Stephen B. Seidman. 1983. Network structure and
minimum degree. Social Networks, 5:269–287.

Nino Shervashidze. Visited on 30/05/2015. Graph ker-
nels. http://www.di.ens.fr/~shervashidze/code.html.

Marisa Thoma, Hong Cheng, Arthur Gretton, Ji-
awei Han, Hans-Peter Kriegel, Alexander J. Smola,
Le Song, Philip S. Yu, Xifeng Yan, and Karsten M.
Borgwardt. 2009. Near-optimal Supervised Feature
Selection among Frequent Subgraphs. In Proceed-
ings of the SIAM International Conference on Data
Mining, SDM ’09, pages 1076–1087.

Robert Thorndike. 1953. Who belongs in the family?
Psychometrika, 18(4):267–276.

S. V. N. Vishwanathan, Nicol N. Schraudolph, Risi
Kondor, and Karsten M. Borgwardt. 2010. Graph
kernels. Journal of Machine Learning Research,
11:1201–1242.

1711



Xifeng Yan and Jiawei Han. 2002. gspan: Graph-
based substructure pattern mining. In Proceedings
of the 2nd IEEE International Conference on Data
Mining, ICDM ’02, pages 721–724.

Yiming Yang and Xin Liu. 1999. A Re-examination of
Text Categorization Methods. In Proceedings of the
22nd annual international ACM SIGIR conference

on Research and development in information re-
trieval, SIGIR ’99, pages 42–49.

Yiming Yang and J. O. Pedersen. 1997. A Compar-
ative Study on Feature Selection in Text Catego-
rization. In Proceedings of the 14th International
Conference on Machine Learning, ICML ’97, pages
412–420.

1712


