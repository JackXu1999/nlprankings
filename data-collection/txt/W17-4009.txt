

















































Evaluation of Finite State Morphological Analyzers Based on Paradigm
Extraction from Wiktionary

Ling Liu and Mans Hulden
Department of Linguistics

University of Colorado
first.last@colorado.edu

Abstract

Wiktionary provides lexical information
for an increasing number of languages, in-
cluding morphological inflection tables. It
is a good resource for automatically learn-
ing rule-based analysis of the inflectional
morphology of a language. This paper per-
forms an extensive evaluation of a method
to extract generalized paradigms from
morphological inflection tables, which can
be converted to weighted and unweighted
finite transducers for morphological pars-
ing and generation. The inflection tables
of 55 languages from the English edition
of Wiktionary are converted to such gen-
eral paradigms, and the performance of
the probabilistic parsers based on these
paradigms are tested.

1 Introduction

Morphological inflection is used in many lan-
guages to convey syntactic and semantic infor-
mation. It is a systematic source of sparsity for
NLP tasks, especially for languages with rich mor-
phological systems where one lexeme can be in-
flected into as many as over a million distinct
word forms (Kibrik, 1998). In this case, mor-
phological parsers which can convert the inflected
word forms back to the lemma forms, or the
other way around can largely benefit downstream
tasks, like part-of-speech tagging, language mod-
eling, and machine translation (Tseng et al., 2005;
Hulden and Francom, 2012; Duh and Kirchhoff,
2004; Avramidis and Koehn, 2008). Various ap-
proaches have been adopted to tackle the mor-
phological inflection and lemmatization problem.
For example, Durrett and DeNero (2013) auto-
matically extracts transformation rules from la-
beled data and learns how to apply these rules
with a discriminative sequence model. Kann and
Schütze (2016) proposes to use a recurrent neural

network (RNN) encoder-decoder model to gener-
ate an inflected form of a lemma for a target mor-
phological tag combination. The SIGMORPHON
2016 shared task (Cotterell et al., 2016) of mor-
phological reinflection received 11 systems which
used various approaches such as conditional ran-
dom fields (CRF), RNNs, and other linguistics-
inspired heuristics. Among all the methods, one
standard technology is to use finite-state transduc-
ers, which are more interpretable and manually
modifiable, and thus more easily incorporated into
and made to assist linguists’ work. Hulden (2014)
presents a method to generalize inflection tables
into paradigms with finite state implementations
and Forsberg and Hulden (2016) subsequently in-
troduce how to transform morphological inflection
tables into both unweighted and weighted finite
transducers and apply the transducers to parsing
and generation, the result of which is very promis-
ing, especially for facilitating and assisting lin-
guists’ work in addition to applications to mor-
phological parsing and generation for downstream
NLP tasks. However, the system was evaluated
with only three languages (German, Spanish, and
Finnish), all with Latin script. This paper intends
to carry out a more extensive evaluation of this
method.

Wiktionary1 provides a source of morpholog-
ical paradigms for a wide and still increasing
range of languages, which is a useful resource of
crosslinguistic research. The data in this work also
originate with Wiktionary.

In this paper we evaluate the cross-linguistic
performance of the paradigm generalization
method on 55 languages, of which inflection tables
have been extracted from the Wiktionary data. All
the languages are consistently annotated with uni-
versal morphological tags (Sylak-Glassman et al.,
2015) and are in the native orthography. In par-
ticular, we evaluate the accuracy on the ability to
lemmatize previously unseen word forms and the

1http://www.wiktionary.org

69

Proceedings of the 13th International Conference on Finite State Methods and Natural Language Processing, pages 69–74,
Umeå, Sweden, 4–6 September 2017. c© 2017 Association for Computational Linguistics

https://doi.org/10.18653/v1/W17-4009

https://doi.org/10.18653/v1/W17-4009


GU Deep Learning & Linguistics  32

  schrei ben 
  schrei bend 
geschr ieben 
  schrei be 
  schrei b    st 
  schrei b     t 

unlabeled data (Goldsmith, 2001; Schone and Ju-
rafsky, 2001; Chan, 2006; Creutz and Lagus,
2007; Monson et al., 2008). Hammarström and
Borin (2011) provides a current overview of unsu-
pervised learning.

Previous work with similar semi-supervised
goals as the ones in this paper include Yarowsky
and Wicentowski (2000), Neuvel and Fulop
(2002), Clément et al. (2004). Recent machine
learning oriented work includes Dreyer and Eis-
ner (2011) and Durrett and DeNero (2013), which
documents a method to learn orthographic trans-
formation rules to capture patterns across inflec-
tion tables. Part of our evaluation uses the same
dataset as Durrett and DeNero (2013). Eskander
et al. (2013) shares many of the goals in this paper,
but is more supervised in that it focuses on learn-
ing inflectional classes from richer annotation.

A major departure from much previous work
is that we do not attempt to encode variation
as string-changing operations, say by string edits
(Dreyer and Eisner, 2011) or transformation rules
(Lindén, 2008; Durrett and DeNero, 2013) that
perform mappings between forms. Rather, our
goal is to encode all variation within paradigms
by presenting them in a sufficiently generic fash-
ion so as to allow affixation processes, phonolog-
ical alternations as well as orthographic changes
to naturally fall out of the paradigm specification
itself. Also, we perform no explicit alignment of
the various forms in an inflection table, as in e.g.
Tchoukalov et al. (2010). Rather, we base our al-
gorithm on extracting the longest common subse-
quence (LCS) shared by all forms in an inflection
table, from which alignment of segments falls out
naturally. Although our paradigm representation
is similar to and inspired by that of Forsberg et al.
(2006) and Détrez and Ranta (2012), our method
of generalizing from inflection tables to paradigms
is novel.

3 Paradigm learning

In what follows, we adopt the view that words
and their inflection patterns can be organized
into paradigms (Hockett, 1954; Robins, 1959;
Matthews, 1972; Stump, 2001). We essentially
treat a paradigm as an ordered set of functions
(f1, . . . , fn), where fi: x1, . . . , xn 7! ⌃⇤, that is,
where each entry in a paradigm is a function from
variables to strings, and each function in a partic-
ular paradigm shares the same variables.

3.1 Paradigm representation
We represent the functions in what we call ab-
stract paradigm. In our representation, an ab-
stract paradigm is an ordered collection of strings,
where each string may additionally contain in-
terspersed variables denoted x1, x2, . . . , xn. The
strings represent fixed, obligatory parts of a
paradigm, while the variables represent mutable
parts. These variables, when instantiated, must
contain at least one segment, but may otherwise
vary from word to word. A complete abstract
paradigm captures some generalization where the
mutable parts represented by variables are instan-
tiated the same way for all forms in one particu-
lar inflection table. For example, the fairly simple
paradigm

x1 x1+s x1+ed x1+ing

could represent a set of English verb forms, where
x1 in this case would coincide with the infinitive
form of the verb—walk, climb, look, etc.

For more complex patterns, several variable
parts may be invoked, some of them discontinu-
ous. For example, part of an inflection paradigm
for German verbs of the type schreiben (to write)
verbs may be described as:

x1+e+x2+x3+en INFINITIVE
x1+e+x2+x3+end PRESENT PARTICIPLE
ge+x1+x2+e+x3+en PAST PARTICIPLE
x1+e+x2+x3+e PRESENT 1P SG
x1+e+x2+x3+st PRESENT 2P SG
x1+e+x2+x3+t PRESENT 3P SG

If the variables are instantiated as x1=schr,
x2=i, and x3=b, the paradigm corresponds to
the forms (schreiben, schreibend, geschrieben,
schreibe, schreibst, schreibt). If, on the other
hand, x1=l, x2=i, and x3=h, the same paradigm re-
flects the conjugation of leihen (to lend/borrow)—
(leihen, leihend, geliehen, leihe, leihst, leiht).

It is worth noting that in this representation, no
particular form is privileged in the sense that all
other forms can only be generated from some spe-
cial form, say the infinitive. Rather, in the cur-
rent representation, all forms can be derived from
knowing the variable instantiations. Also, given
only a particular word form and a hypothetical
paradigm to fit it in, the variable instantiations can
often be logically deduced unambiguously. For
example, let us say we have a hypothetical form
steigend and need to fit it in the above paradigm,
without knowing which slot it should occupy. We

LCS = schrib 
x1 = schr 
x2 = i 
x3 = b

“paradigm”

x1 x2 x3

inflection table

A model of  Analogy
Formal claim: the common parts (stem) are calculated by extracting 
the Longest Common Subsequence from related forms (Hulden 
2014; Ahlberg, Forsberg, Hulden 2014/2015)

Figure 1: Illustration of the paradigm extraction mechanism.
An inflection table is given as input, and the longest com-
mon subsequence (LCS) is extracted and assigned to “vari-
able parts” of a more abstract paradigm, based on discontinu-
ities in the LCS. Several inflection tables may yield the same
“paradigm” in which case paradigms are collapsed, and infor-
mation about the shape of the variable strings xi is retained
for statistical modeling.

ability to assign correct morphosyntactic tags to a
word form.

2 Paradigm Extraction

The paradigm extraction method is based on the
idea of finding, among a list of related word forms,
the longest common subsequence (LCS) shared by
the forms. After the extraction, the LCS is marked
in each word form and assigned to “paradigm vari-
ables”. These variables are parts that are muta-
ble in a paradigm, i.e. may change when going
from one lemma to another, while the remaining,
non-variable parts represent inflectional informa-
tion. Figure 1 illustrates this process by showing
a few forms of the German verb schreiben, the
extraction of the LCS, and the assignment of the
LCS into variable parts.

After such a generalization process, many
paradigm representations which were generated
from inflection tables turn out to be identical—
indicating that the participating lemmas inflect ac-
cording to the same pattern. Identical paradigms
are collapsed, and the information about what
strings were witnessed in the variable slots is
stored for creating a probabilistic model of inflec-
tion. The reader is referred to (Hulden, 2014;
Ahlberg et al., 2014; Ahlberg et al., 2015) for de-
tails.

This model already provides a method for per-
forming morphological analysis when previously
unseen word forms are encountered. One can
create a transducer based on the paradigms that
maps entries in a paradigm back to their lemma
form in such a way that the variable parts xi may
correspond to any arbitrary string. For example,
the paradigm in figure 1 would yield a lemma-
tizing transducer that would map e.g. geliehen

7→ leihen since the l can be assumed to match
the variable x1, the i x2, and the h x3. Forsberg
and Hulden (2016) developl a model that creates
such lemmatizing transducers from inflection ta-
bles, which also return the inflectional information
of the source word form.

2.1 Analyzing word forms

This model has the disadvantage of often return-
ing a large number of plausible analyses due to
the fact that an unseen word form may fit many
different learned paradigms, and also fit them in
many different slots. One can, however, induce
a language model of each variable part xi in the
paradigms and create a probabilistic model which
favors production of such analyses where variable
parts resemble those that have been seen in the
training data. An n-gram model over the variables
seen in each paradigm can be formulated as fol-
lows. Many paradigms have been collapsed from
a large number of inflection tables which provide
us with statistics over the shape of the xi parts. We
can, when trying to fit an unseen word form with
a variable xi consisting of letters v1, . . . vn into a
paradigm and slot to produce its lemma, calculate
the joint probability using an n-gram approxima-
tion of the letters according to the expression:

P (v1, . . . , vn) =
n∏

i=1

P (vi|vi−(n−1), . . . , vi−1)

(1)
These quantities can be estimated by maximum

likelihood from the the training data as:

P (vi|vi−(n−1), . . . , vi−1)

=
#(vi−(n−1), . . . , vi−1, vi)

#(vi−(n−1), . . . , vi−1)
(2)

Such a model is induced for each variable
x1, . . . , xn in a paradigm, and when a proposed
analysis is evaluated, the quantity p(xi, . . . , xn) =
p(x1)× . . .× p(xn) is evaluated to give a score of
fit of a proposed variable assignment for a word to
be analyzed.

For example, to calculate the fit of geliehen into
the slot ge+x1+x2+e+x3+en (Figure 1), we would
evaluate p(x1) = l, p(x2) = i and p(x3) = h
based on the above, yielding a probability esti-
mate of the slot and paradigm matching the word

70



form geliehen. Likewise, every possible assign-
ment of variable parts in every paradigm will be
calculated. This process can be encoded into a
weighted finite state transducer (WFST) following
Forsberg and Hulden (2016).

Language Language group Script

Adyghe Northwest Caucasian Cyrillic
Albanian IE other Latin (Albanian alphabet)
Armenian IE other Armenian
Asturian IE/Italic/Romance/Western Latin
Bashkir Turkic Cyrillic, Latin, Arabic
Basque Language Isolate Latin (Basque alphabet)
Bengali IE/Indo-Iranian/Indo-Aryan Eastern Nagari script (Bengali alphabet)
Bulgarian IE/Balto-Slavic/Slavic Cyrillic (Bulgarian alphabet)
Catalan IE/Italic/Romance/Western Latin (Catalan alphabet)
Danish IE/Germanic/North G Latin (Dano-Norwegian alphabet)
Dutch IE/Germanic/West G Latin (Dutch alphabet)
Esperanto Created Latin (Esperanto alphabet)
Estonian Uralic/Finnic Latin (Estonian alphabet)
Faroese IE/Germanic/North G Latin (Faroese orthography)
Finnish Uralic/Finnic Latin (Finnish alphabet)
French IE/Italic/Romance/Western Latin (French alphabet)
Friulian IE/Italic/Romance/Western Latin
Galician IE/Italic/Romance/Western Latin (Galician alphabet)
Georgian Kartvelian Georgian
German IE/Germanic/West G Latin (German alphabet)
Greek IE/Hellenic Greek
Hebrew Afro-Asiatic/Semitic/Central S Hebrew
Hindi IE/Indo-Iranian/Indo-Aryan Devanagari
Hungarian Uralic/Finno-Ugric Latin (Hungarian alphabet)
Icelandic IE/Germanic/North G Latin (Icelandic alphabet)
Italian IE/Italic/Romance/Italo-Dalmatian Latin (Italian alphabet)
Ladin IE/Italic/Romance/Western Latin
Latin IE/Italic/Latino-Faliscan Latin
Latvian IE/Balto-Slavic/Baltic Latin (Latvian alphabet)
Lithuanian IE/Balto-Slavic/Baltic Latin (Lithuanian alphabet)
Lower Sorbian IE/Balto-Slavic/Slavic/West S Latin (Sorbian alphabet)
Luxembourgish IE/Germanic/West G Latin (Luxembourgish alphabet)
Macedonian IE/Balto-Slavic/Slavic/South S Cyrillic (Macedonian alphabet)
Navajo Da-Dene/Athabaskan Latin
Northern Sami Uralic/Sami Latin (Northern Sami alphabet)
Norwegian Bokmal IE/Germanic/North G Latin (Norwegian alphabet)
Norwegian Nynorsk IE/Germanic/North G Latin (Norwegian alphabet)
Occitan IE/Italic/Romance/Gallo-R Latin
Polish IE/Balto-Slavic/Slavic/West S Latin (Polish alphabet)
Portuguese IE/Italic/Romance/Western R Latin (Portuguese alphabet)
Quechua Quechua Latin (Quechua alphabet)
Romanian IE/Italic/Romance/Eastern R Latin (Romanian alphabet)
Russian IE/Balto-Slavic/Slavic/East S Cyrillic (Russian alphabet)
Sanskrit IE/Indo-Iranian/Indo-Aryan Brahmic
Scottish Gaelic IE/Celtic Latin (Scottish Gaelic orthography)
Slovak IE/Balto-Slavic/Slavic/West S Latin (Slovak alphabet)
Slovene IE/Balto-Slavic/Slavic/South S Latin (Slovene alphabet)
Spanish IE/Italic/Romance/Western Latin (Spanish alphabet)
Swahili Niger-Congo Latin (Roman Swahili alphabet)
Swedish IE/Germanic/North G Latin (Swedish alphabet)
Turkish Turkic Latin (Turkish alphabet)
Ukrainian IE/Balto-Slavic/Slavic/East Cyrillic (Ukrainian alphabet)
Urdu IE/Indo-Iranian/Indo-Aryan Extended Perso-Arabic (Urdu alphabet)
Venetian IE/Italic/Romance/Italo-Western Latin
Welsh IE/Celtic Latin (Welsh alphabet)

Table 1: Languages, Language Groups, and Scripts

2.2 Evaluation

The paradigm extraction and application method
presented in the previous part is evaluated with
55 languages from the Wiktionary Morphological
Database2 (Kirov et al., 2016) as part of the Uni-
Morph project3 which includes data for 350 lan-
guages at the time we downloaded it.4 For each

2https://github.com/ckirov/UniMorph/tree/master/data
3http://www.unimorph.org
4The 55 languages are: Adyghe, Albanian, Armenian, As-

turian, Bashkir, Basque, Bengali, Bulgarian, Catalan, Dan-
ish, Dutch, Esperanto, Estonian, Faroese, Finnish, French,
Friulian, Galician, Georgian, German, Greek, Hebrew, Hindi,
Hungarian, Icelandic, Italian, Ladin, Latin, Latvian, Lithua-
nian, Lower Sorbian, Luxembourgish, Macedonian, Navajo,
Northern Sami, Norwegian, Bokmal, Norwegian Nynorsk,

of the 55 languages, we learn paradigms from a
random selection of 90% of the available inflec-
tion tables and leave 10% of the tables as held-
out data. Tables for different parts-of-speech are
generalized identically and the system does not
keep these separate (although it is unlikely that a
noun would inflect like a verb, for example). This
means that the POS information is treated as a nor-
mal tag and we may receive analyses with different
parts of speech.

The evaluation task is to convert the inflected
word form back to its lemma form and assign it
morphological tags. At test time, we analyze each
form in the held-out tables separately and eval-
uate the accuracy on the highest scoring analy-
sis. We report accuracies on several combinations
of lemmatization correctness, morphosyntactic tag
correctness and POS tag correctness.

The 55 languages fall into 19 language groups:
Caucasian, Indo-European other, Italic (Romance,
with the exception of Latin), Turkic, Language
isolated, Indo-Aryan, Slavic, Baltic, Germanic,
Uralic, Celtic, Semitic, Hellenic, Kartvelian, Da-
Dene, Quechua, Turkic, Niger-Congo, and an ar-
tificial language—Esperanto. The data for each
language is in its native script, which consists of
10 different scripts: Latin, Cyrillic, Armenian,
Eastern Nagari, Georgian, Greek, Hebrew, De-
vanagari, Brahmic, and Pero-Arabic. The crite-
rion for selecting the 55 languages is that each
language has 8,000 or more entries in the origi-
nal data in UniMorph Wiktionary Morphological
Database. Languages with less entries are selected
to increase the representativeness of the data. A
summary of the languages, language groups and
scripts is presented in table 1. The data for each
language is used just at it is from the database.
Little work is done to improve the quality of the
data. Therefore, if there are misspellings or incor-
rect inflections in the data set, the paradigms are
extracted and tested with any errors uncorrected.

The number of inflection tables may not be the
same as the number of lemmas, because in cases
where there are alternative inflected forms for one
morphosyntactic description of a lemma in the
UniMorph database, each form is represented in
a separate table.

Occitan, Polish, Portuguese, Quechua, Romanian, Russian,
Sanskrit, Scottish Gaelic, Slovak, Slovene, Spanish, Swahili,
Swedish, Turkish, Ukrainian, Urdu, Venetian, Welsh.

71



Language LT LPOS LEMMA TNum PAllNum P1Ex P2Ex PMEx P0Var TopFreq WNum LNum

Adyghe 0.6012 0.6012 0.7887 1,593 16 5 1 10 0 550 18,874 1,593
Albanian 0.7016 0.7100 0.7179 616 68 36 6 21 5 121 37,411 589
Armenian 0.8994 0.9010 0.9631 14,905 305 104 32 158 11 1,730 703,902 7,040
Asturian 0.8840 0.8933 0.9140 1,304 113 58 14 39 2 235 41,835 938
Bashkir 0.8816 0.9245 0.9245 773 36 6 2 28 0 92 8,981 773
Basque 0.0 0.0 0.0021 45 44 38 0 0 6 1 13,627 45
Basque2 0.1276 0.3111 0.3588 620 504 9 0 2 493 52 27788 620
Bengali 0.7234 0.7255 0.7255 225 74 55 3 15 1 21 5,691 136
Bulgarian 0.7429 0.7445 0.7627 2,912 225 98 25 96 6 315 55,523 2,471
Catalan 0.8550 0.8894 0.8894 1,558 107 59 11 33 4 779 83,182 1,557
Danish 0.6826 0.6945 0.7075 3,180 248 180 11 57 0 658 28,584 3,180
Dutch 0.6993 0.7110 0.7323 4,985 274 112 34 128 0 959 60,437 4,979
Esperanto 0.7628 0.7638 0.7700 23,687 46 28 2 13 3 11,652 98,565 23,687
Estonian 0.5958 0.5970 0.5970 887 779 776 2 1 0 19 39,102 886
Faroese 0.5121 0.5297 0.5702 3,333 621 419 62 135 5 217 52,836 3,077
Finnish 0.6803 0.6823 0.7633 20,000 2,403 862 477 1,064 0 685 627,085 14,274
French 0.7412 0.7466 0.8556 19,937 1,248 604 201 428 15 2,241 387,669 7,555
Friulian 0.7844 0.7844 0.7929 155 37 22 2 8 5 73 6,593 145
Galician 0.7389 0.7485 0.7613 486 63 32 7 22 2 184 29,843 472
Georgian 0.6623 0.6630 0.7828 3,784 63 49 3 11 0 2,117 78,196 3,782
German 0.6221 0.6510 0.8216 17,749 851 512 100 239 0 1653 197,080 15,059
Greek 0.6555 0.6619 0.6938 9,861 2,072 1,426 212 377 57 468 149,187 8,780
Hebrew 0.8217 0.8263 0.8288 867 555 437 58 55 4 9 24,247 510
Hindi 0.9692 0.9696 0.9746 852 33 10 6 16 1 184 72,424 788
Hindi2 0.9522 0,9553 0.9597 788 31 11 4 15 1 181 58,856 788
Hungarian 0.8550 0.9197 0.9277 15,838 4,136 3,640 37 173 286 1,009 597,744 13,952
Icelandic 0.6917 0.7158 0.7233 5,082 384 155 46 170 14 377 86,864 4,769
Italian 0.8972 0.8973 0.8973 10,009 374 238 32 98 6 3919 519,571 10,009
Ladin 0.8400 0.8595 0.8875 813 111 72 11 20 8 97 17,830 512
Latin 0.8155 0.8167 0.8406 25,079 2,463 1,602 275 567 19 3,643 967,062 20,497
Latvian 0.7943 0.8269 0.8450 10,067 519 275 66 167 11 1,722 216,420 7,558
Lithuanian 0.7786 0.7820 0.7969 1,925 515 298 79 133 5 130 58,814 1,458
Lower Sorbian 0.6098 0.6265 0.6273 1,224 351 260 18 70 3 57 25,062 994
Luxembourgish 0.7422 0.7554 0.7591 1,277 222 172 12 34 4 318 48,175 1,276
Macedonian 0.7319 0.7563 0.7837 10,313 272 134 16 114 8 869 178,363 10,310
Navajo 0.2649 0.2746 0.2912 675 455 398 22 17 18 61 13,059 674
N Sami 0.4377 0.4414 0.4476 2,203 1,343 1,250 41 52 0 44 66,344 2,107
N Bokmal 0.5866 0.6170 0.6733 5,750 355 211 42 99 3 1,097 24,704 5,518
N Nynorsk 0.5959 0.6242 0.6720 5,041 380 237 44 94 5 1,333 23,093 4,677
Occitan 0.9474 0.9474 0.9555 173 28 19 3 6 0 101 5,162 172
Polish 0.6967 0.7059 0.7138 10,698 1,300 737 161 398 4 566 202,927 10,179
Portuguese 0.9053 0.9062 0.9369 5,088 283 162 11 107 3 2,400 309,084 4,001
Quechua 0.4513 0.4515 0.9128 1,244 17 8 0 9 0 372 181,248 1,006
Romanian 0.6258 0.6380 0.6989 3,504 671 447 76 144 4 205 68,174 3,479
Russian 0.7100 0.7615 0.7749 28,017 1,669 825 231 612 1 1,898 460,981 27,924
Sanskrit 0.6847 0.7029 0.7998 924 79 43 7 26 3 243 27,031 924
S Gaelic 0.9231 0.9692 0.9692 101 74 61 9 3 1 4 1,208 70
Slovak 0.5582 0.5899 0.5912 1,093 250 137 42 68 3 116 16,435 1,046
Slovene 0.4383 0.4646 0.4886 2,601 949 747 62 115 25 150 64,504 2,554
Spanish 0.9037 0.9038 0.9355 6,355 420 194 52 169 5 1,977 389,308 5,460
Swahili 0.8550 0.8559 0.8567 74 42 40 0 2 0 20 12,365 74
Swedish 0.6928 0.7156 0.7615 10,833 458 243 57 155 4 1,188 92,890 10,503
Turkish 0.8286 0.8414 0.8480 3,572 237 97 29 111 0 155 278,624 3,572
Ukrainian 0.5645 0.5900 0.5948 1,494 248 155 30 62 3 142 22,398 1,493
Urdu 0.8784 0.8827 0.8827 182 51 43 1 7 0 40 12,755 182
Venetian 0.8160 0.8342 0.9327 959 112 73 10 22 7 193 29,790 607
Welsh 0.2378 0.2478 0.2478 183 158 155 1 2 1 4 10824 183

Table 2: Result Summary. LT–Lemma+Tags, LPOS–Lemma+POS, LEMMA–Lemma, TNum–The number of inflection tables
in the data, PAllNum–The number of all paradigms for each language, P1Ex–The number of paradigms with only 1 example
for the variable(s), P2Ex–The number of paradigms with only 2 examples for the variable(s), PMEx–The number of paradigms
with more than 2 examples for the variable(s), P0Var–The number of paradigms with no variable in it, TopFreq – The number
of examples for the most popular paradigm, WNum–The number of words, LNum–The number of lemmas, N Sami–Northern
Sami, N Bokmal–Norwegian Bokmal, N Nynorsk–Norwegian Nynorsk, S Gaelic–Scottish Gaelic, Basque2–result of Basque
with more data, Hindi2–result of Hindi without alternative inflections

72



3 Result and Discussion

More abstract paradigms are extracted success-
fully from all the morphological tables in the train-
ing set for each of the 55 languages. Lemmati-
zation correctness ranges from a low end of 0%
(Basque) to a high end of 96.3% (Armenian),
97.5% (Hindi). The lemma-POS accuracy ranges
from 0% (Basque) to 97.0% (Hindi). The joint
lemma-tag accuracy ranges from 0% (Basque) to
96.9% (Hindi).

One advantage of the probabilistic model is that
it can rank the parsing and generation through the
language model over variables. For the evaluation,
we use only the most likely one. However, there
can still be alternatives for the top ranking result.
For example, the French word écris gets five anal-
yses: [écrir V; IND; PRS; 1; SG], [écrir V; IND;
PRS; 2; SG], [écrir V; IND; PST; 1; SG; PFV],
[écrir V; IND; PST; 2; SG; PFV] and [écrir V;
POS; IMP; 2; SG], with the same lemma form
and part-of-speech, but different tags. We eval-
uate the recall of the lemma and all tags, lemma
and only part-of-speech, and only lemma. Table
2 presents a summary of the evaluation result, as
well as the data size for each language in terms of
word counts, lemma counts and table counts, the
number of abstract paradigms extracted for each
language, the paradigm distribution as to their in-
stantiation case numbers, and the number of in-
stances for the most popular paradigm.

The results seem to correlate strongly with the
amount of available data. For languages where
only a few inflection tables are seen, or where all
inflection tables represent the same paradigm, ac-
curacies are low. For example, the initial evalua-
tion on Basque is only 45 lemmas (and the number
of inflection tables is the same), with 44 as train-
ing and 1 held-out for test. Each of the inflection
table represents a distinct paradigm as is reflected
by the fact that the number of abstract paradigms is
the same as the training data size, i.e. 44. There-
fore, the result of Basque is very low. A second
round of evaluation was conducted with more data
and the result is added to Table 2 as Basque2.
The increased Basque data is 620 lemmas, with
558 used for training and 62 for testing. The re-
sult is still low but better than the initial one, be-
cause the paradigms become more representative
with a larger coverage, which is testified by the
most representative paradigm getting 52 instantia-
tions from the training data. The result of Navajo

is close to that of Basque2. The data sizes of the
two languages are similar (Basque2 620, Navajo
675). For Navajo, the number of paradigms is 455,
and for Basque2, the number of paradigms is 504.
Basque2 has 493 paradigms without variables (i.e.
493 paradigms with only one instantiations), and
Navajo has 398 paradigms with only one instan-
tiation in the training data. The similarity of the
results coincides with the fact that both languages
are morphologically complex, and Basque mor-
phology has even more variation. Conversely, lan-
guages where many different types of inflection
tables are seen and the inflection tables are rep-
resentative of the language morphology (reflected
in a higher paradigm count and a lower ratio of
paradigm counts to table counts), produce ana-
lyzers that can perform quite robustly. For ex-
ample, the Hindi data produces 33 paradigms out
of the 767 inflection training tables, resulting in
a coverage where the recall for each of the three
tests is over 95%. As alternative inflections of a
lemma are represented with different tables, the
recall may be higher than the case where each
lemma gets only one inflection table, i.e. where
no related form of the associated lemma has been
witnessed. However, as alternative inflections are
limited, keeping them with different tables should
not influence the result by a large extent. Hindi2
is the result for Hindi using only one inflection ta-
ble for each lemma and ignoring alternative inflec-
tions.

4 Conclusion

Generalized paradigms are successfully extracted
for all the 55 languages from 19 language groups
in 10 different scripts. For languages with a
large size of representative data, the recalls of the
lemma, lemma plus part-of-speech, and lemma
plus all tags can be as high as over 95%. How-
ever, for languages for which the data is limited or
less representative, the recalls are very low. This
indicates that the method to extract generalized
paradigms from morphological inflection tables
works well despite linguistic diversity and script
variations. The probabilistic model can yield good
predictions and analyses when the available data
for a language is sufficient and representative.

Acknowledgements

This work has been partly sponsored by DARPA
I20 in the LORELEI program.

73



References
Malin Ahlberg, Markus Forsberg, and Mans Hulden.

2014. Semi-supervised learning of morphological
paradigms and lexicons. In Proceedings of the 14th
Conference of the European Chapter of the Asso-
ciation for Computational Linguistics, pages 569–
578, Gothenburg, Sweden. Association for Compu-
tational Linguistics.

Malin Ahlberg, Markus Forsberg, and Mans Hulden.
2015. Paradigm classification in supervised learn-
ing of morphology. In Proceedings of the 2015 Con-
ference of the North American Chapter of the Asso-
ciation for Computational Linguistics: Human Lan-
guage Technologies, pages 1024–1029, Denver, Col-
orado, May–June. Association for Computational
Linguistics.

Eleftherios Avramidis and Philipp Koehn. 2008. En-
riching morphologically poor languages for statisti-
cal machine translation. In ACL, pages 763–770.

Ryan Cotterell, Christo Kirov, John Sylak-Glassman,
David Yarowsky, Jason Eisner, and Mans Hulden.
2016. The SIGMORPHON 2016 shared task—
morphological reinflection. ACL 2016, page 10.

Kevin Duh and Katrin Kirchhoff. 2004. Automatic
learning of language model structure. In Proceed-
ings of the 20th international conference on Compu-
tational Linguistics, page 148. Association for Com-
putational Linguistics.

Greg Durrett and John DeNero. 2013. Supervised
learning of complete morphological paradigms. In
HLT-NAACL, pages 1185–1195.

Markus Forsberg and Mans Hulden. 2016. Learning
transducer models for morphological analysis from
example inflections. ACL 2016, page 42.

Mans Hulden and Jerid Francom. 2012. Boosting
statistical tagger accuracy with simple rule-based
grammars. In LREC, pages 2114–2117.

Mans Hulden. 2014. Generalizing inflection tables
into paradigms with finite state operations. In Pro-
ceedings of the 2014 Joint Meeting of SIGMOR-
PHON and SIGFSM, pages 29–36.

Katharina Kann and Hinrich Schütze. 2016. Single-
model encoder-decoder with explicit morphologi-
cal representation for reinflection. arXiv preprint
arXiv:1606.00589.

Aleksandr E. Kibrik. 1998. Archi. In Andrew Spencer
and Arnold M. Zwicky, editors, The Handbook of
Morphology, pages 455–476. Oxford: Blackwell
Publishers.

Christo Kirov, John Sylak-Glassman, Roger Que, and
David Yarowsky. 2016. Very-large scale pars-
ing and normalization of wiktionary morphological
paradigms. In Proceedings of the Tenth Interna-
tional Conference on Language Resources and Eval-
uation (LREC 2016), pages 3121–3126.

John Sylak-Glassman, Christo Kirov, Matt Post, Roger
Que, and David Yarowsky. 2015. A universal
feature schema for rich morphological annotation
and fine-grained cross-lingual part-of-speech tag-
ging. In Cerstin Mahlow and Michael Piotrowski,
editors, Proceedings of the 4th Workshop on Sys-
tems and Frameworks for Computational Morphol-
ogy (SFCM), pages 72–93. Springer, Berlin.

Huihsin Tseng, Daniel Jurafsky, and Christopher Man-
ning. 2005. Morphological features help POS tag-
ging of unknown words across language varieties.
In Proceedings of the fourth SIGHAN workshop on
Chinese language processing, pages 32–39.

74


	Introduction
	Paradigm Extraction
	Analyzing word forms
	Evaluation

	Result and Discussion
	Conclusion

