








































Joint Modeling of Topics, Citations, and Topical Authority in Academic
Corpora

Jooyeon Kim
KAIST

jooyeon.kim@kaist.ac.kr

Dongwoo Kim
Australian National University
dongwoo.kim@anu.edu.au

Alice Oh
KAIST

alice.oh@kaist.edu

Abstract

Much of scientific progress stems from pre-
viously published findings, but searching
through the vast sea of scientific publications
is difficult. We often rely on metrics of schol-
arly authority to find the prominent authors but
these authority indices do not differentiate au-
thority based on research topics. We present
Latent Topical-Authority Indexing (LTAI) for
jointly modeling the topics, citations, and top-
ical authority in a corpus of academic papers.
Compared to previous models, LTAI differs in
two main aspects. First, it explicitly models
the generative process of the citations, rather
than treating the citations as given. Second,
it models each author’s influence on citations
of a paper based on the topics of the cited pa-
pers, as well as the citing papers. We fit LTAI
into four academic corpora: CORA, Arxiv
Physics, PNAS, and Citeseer. We compare
the performance of LTAI against various base-
lines, starting with the latent Dirichlet alloca-
tion, to the more advanced models including
author-link topic model and dynamic author
citation topic model. The results show that
LTAI achieves improved accuracy over other
similar models when predicting words, cita-
tions and authors of publications.

1 Introduction

With a corpus of scientific literature, we can ob-
serve the complex and intricate process of scientific
progress. We can learn the major topics in journal
articles and conference proceedings, follow authors
who are prolific and influential, and find papers that
are highly cited. The huge number of publications

Topic Reinforcement Learning
Image 

Processing
Statistical 
Learning

Information 
Retrieval

Authority 2.76 0.14 10.29 0.10

highly cited

M. Jordan

Topic Reinforcement Learning
Image 

Processing
Statistical 
Learning

Information 
Retrieval

Authority 12.86 0.37 4.83 0.04

highly cited

R. Sutton

R. Sutton

M. Jordan

Citation Network

Inferred Authority

Figure 1: Overview of Latent Topical Authority In-
dexing (LTAI). Based on content, citation, and au-
thorship information (top), the LTAI discovers the
topical authority of authors; it increases when a pa-
per with certain topics gets cited (bottom). Topical
authority examples are the results of the LTAI with
CORA dataset and 100 topics.

and authors, however, makes it practically impossi-
ble to attain any deep or detailed understanding be-
yond the very broad trends. For example, if we want
to identify authors who are particularly influential in
a specific research field, it is difficult to do so with-
out the aid of automatic analysis.

Online publication archives, such as Google
Scholar, provide near real-time metrics of schol-

191

Transactions of the Association for Computational Linguistics, vol. 5, pp. 191–204, 2017. Action Editor: Noah Smith.
Submission batch: 11/2016; Revision batch: 2/2017; Published 7/2017.

c©2017 Association for Computational Linguistics. Distributed under a CC-BY 4.0 license.



arly impact, such as the h-index (Hirsch, 2005), the
journal impact factor (Garfield, 2006), and citation
count. Those indices, however, are still at a coarse
level of granularity. For example, both Michael Jor-
dan and Richard Sutton are researchers with very
high citation counts and h-index, but they are author-
itative in different topics, Jordan in the more gen-
eral machine learning topic of statistical learning,
and Sutton in the topic of reinforcement learning. It
would be much more helpful to know that via topical
authority scores, as shown in Figure 1.

Fortunately, various academic publication
archives contain the full contents, references, and
meta-data including titles, venues, and authors.
With such data, we can build and fit a model to
partition researchers’ scholarly domains into topics
at a much finer-grain and discover their academic
authority within each topic. To do that, we propose
a model named Latent Topical-Authority Indexing
(LTAI), based on the LDA, to jointly model the
topics, authors’ topical authority, and citations
among the publications.

We illustrate the modeling power of the LTAI with
four corpora encompassing a diverse set of academic
fields: CORA, Arxiv Physics, PNAS, and Citeseer.
To show the improvements over other related mod-
els, we carry out prediction tasks on words, cita-
tions and authorship using the LTAI and compare
the results with those of Latent Dirichlet Allocation
(Blei et al., 2003), relational topic model (Chang and
Blei, 2010), author-link topic model, and dynamic
author-cite topic model (Kataria et al., 2011), as well
as simple baselines of topical h-index. The results
show that the LTAI outperforms these other models
for all prediction tasks.

The rest of this paper is organized as follows.
In section 2, we describe related work, including
models that are most similar to the LTAI, and de-
scribe how the LTAI fits in and contributes to the
field. In section 3, we describe the LTAI model in
detail and present the generative process. In sec-
tion 4, we explain the algorithm for approximate in-
ference, and in section 5, we present a faster algo-
rithm for scalability. In section 6, we describe the
experimental setup and in section 7, we present the
results to show that the LTAI performs better than
other related models for word, citation and author-
ship prediction.

2 Related Work

In this section, we review related papers, first in the
field of NLP and ML-based analysis of scientific
corpora, then the approaches based on the Bayesian
topic models for academic corpora, and lastly joint
models of topics, authors, and citations. In ana-
lyzing scientific corpora, previous research presents
classifying scientific publications (Caragea et al.,
2015), recommending yet unlinked citations (Huang
et al., 2015; Neiswanger et al., 2014; Wang et al.,
2015; Jiang, 2015), summarizing and extracting key
phrases (Cohan and Goharian, 2015; Caragea et al.,
2014), triggering a better model fit (He et al., 2015),
incorporating authorship information to increase the
content and link predictability (Sim et al., 2015), es-
timating a paper’s potential influence on academic
community (Dong et al., 2015), and finding and
classifying different functionalities of citation prac-
tices (Moravcsik and Murugesan, 1975; Teufel et al.,
2006; Valenzuela et al., 2015).

Several variants of topic modeling consider the re-
lationship between topics and citations in academic
corpora. Topic models that use text and citation net-
works are divided into two types: (a) models that
generate text given citation networks (Dietz et al.,
2007; Foulds and Smyth, 2013) and (b) models that
generate citation networks given text (Nallapati et
al., 2008; Liu et al., 2009; Chang and Blei, 2010).
While our model falls into the latter category, we
also take into account the influence of the authors
on the citation structure.

Most closely related to the LTAI is the citation
author topic model (Tu et al., 2010), the author-
link topic model, and the dynamic author-cite topic
model (Kataria et al., 2011). Similar to the LTAI,
they are designed to capture the influence of the au-
thors. However, these models infer authority by ref-
erencing only the citing papers’ text, while our au-
thority is based on the predictive modeling of com-
paring both the citing and the cited papers. Further-
more, the LTAI defines a generative model of cita-
tions and publications by introducing a latent au-
thority index, whereas the previous models assume
the citation structure is given. The LTAI thus explic-
itly gives a topical authority index, which directly
answers the question of which author increases the
probability of a paper being cited.

192



3 Latent Topical-Authority Indexing

The LTAI models the complex relationships among
the topics of publications, the topical authority of the
authors, and the citations among these publications.
The generative process of the LTAI can be divided
into two parts: content generation and citation net-
work generation. We make several assumptions in
the LTAI to model citation structures of academic
corpora. First, we assume a citation is more likely
to occur between two papers that are similar in their
topic proportions. Second, we assume that an author
differs in their authority (i.e., potential to induce ci-
tation) for each topic, and an author’s topical author-
ity positively correlates with the probability of cita-
tions among publications. Also, in the LTAI, when
there are multiple authors in a single cited publica-
tion, their contribution of forming citations with re-
spect to different citing papers varies according to
their topical authority. Lastly, we assign different
concentration parameters for a pair of papers with
and without citations. In this paper, we use positive
and negative links to denote pairs of papers with and
without citations respectively.

Figure 2 illustrates the graphical model of the
LTAI, and we summarize the generative process of
the LTAI, where the variables of the model are ex-
plained in the remainder of this section, as follows:

1. For each topic k, draw topic βk ∼ Dir(αβ).

2. For each document i:

(a) Draw topic proportion θi ∼ Dir(αθ).
(b) For each word win:

i. Draw topic assignment zin|θi ∼
Mult(θi).

ii. Draw word win|zin, β1:K ∼
Mult(βzin).

3. For each author a and topic k:

(a) Draw authority index ηak ∼ N (0, α−1η ).

4. For each document pair from i to j:

(a) Draw influence proportion
parameter πi←j ∼ Dir(πi).

(b) Draw author ai|πi←j ∼ Mult(πi←j).
(c) Draw link xi←j ∼
N (∑k ηaikz̄ikz̄jk, c−1i←j).

✓i ✓j↵⌘⇡i

zin zjn

win wjn�k

⌘a

↵�

↵✓

Ni NjK

A

i jx

↵i j

Figure 2: Graphical representation of the LTAI.
The LTAI jointly models content-related variables
θ, z, w, β, and author and citation related variables
η and π.

3.1 Content Generation

To model the content of publications, we follow
a standard document generative process of Latent
Dirichlet Allocation (LDA) (Blei et al., 2003). We
inherit notations for variables from LDA; θ is the
per-document topic distribution, β is the per-topic
word distribution, z is the topic for each word in a
document where w is the corresponding word, and
αθ, αβ are the Dirichlet parameters of θ and β.

3.2 Citation Generation

Let xi←j be a binary valued variable which indicates
that publication j cites publication i. We formulate
a continuous variable ri←j which is a linear com-
bination of the authority variable and the topic pro-
portion variable to approximate xi←j by minimiz-
ing the sum of squared errors between the two vari-
ables. There is a body of research on using contin-
uous user and item-related variables to approximate
binary variables in the field of recommender systems
(Rennie and Srebro, 2005; Koren et al., 2009).

Approximating binary variables using a lin-
ear combination of continuous variables can be
probabilistically generalized (Salakhutdinov and
Mnih, 2007). Using probabilistic matrix factor-
ization, we approximate probability mass func-
tion p(xi←j) using probability density function
N (xi←j |ri←j , c−1i←j), where the precision parameter
ci←j can be set differently for each pair of papers as
will be discussed below.

193



Content Similarity Between Publications: In
the LTAI, we model relationships between a random
pair of documents i and j. The probability of pub-
lication j citing publication i is proportional to the
similarity of topic proportions of two publications,
i.e., ri←j positively correlates to

∑
k θikθjk. Fol-

lowing the relational topic model’s approach (Chang
and Blei, 2010), we use z̄i = 1Ni

∑
n zi,n ≈ θi in-

stead of a topic proportion parameter θi.
Topical Authority of Cited Paper: We introduce

a K-dimensional vector ηa for representing the top-
ical authority index of author a. ηak is a real number
drawn from the zero-mean normal distribution with
variance α−1η . Given the authority indices ηai for au-
thor a of cited publication i, the probability of a ci-
tation is further modeled as ri←j =

∑
k ηaikz̄ikz̄jk,

where the authority indices can promote or demote
the probability of a citation.

Different Degree of Contribution among Mul-
tiple Authors: Academic publications are often
written by more than one author. Thus, we need
to distinguish the influence of each author on a ci-
tation between two publications. Let Ai be a set
of authors of publication i. To measure the influ-
ence proportion of author a ∈ Ai on the citation
from i to j, we introduce an additional parameter
πi←j which is a one-hot vector drawn from a Dirich-
let distribution with |Ai|-dimensional parameter πi.
πi←ja ∈ {0, 1} is an element of πi←j which mea-
sures the influence of author a on the citation from
j to i and sums up to one (

∑
a∈Ai πi←ja = 1)

over all authors of publication i. We approximate
the probability of citation xi←j from publication
j to publication i by p(xi←j |z, πi←j , ai←j , ηa) ≈∑

a∈Ai πi←jaN (xi←j |
∑

k ηakz̄ikz̄jk, c
−1
i←j) which

is a mixture of normal distributions with precision
parameter ci←j . Therefore, if topic distributions of
paper i and j are similar and if η values of the cited
paper’s authors are high, the citation formation prob-
ability increases. On the other hand, dissimilar or
topically irrelevant pair of papers with less author-
itative authors on the cited paper will be assigned
with low probability of citation formation.

Different Treatment between Positive and Neg-
ative links: Citation is a binary problem where xi←j
is either one or zero. When xi←j is zero, this can be
interpreted in two ways: 1) the authors of citing pub-
lication j are unaware of the publication i, or 2) the

publication j is not relevant to publication i. Identi-
fying which case is true is impossible unless we are
the authors of the publication. Therefore the model
embraces this uncertainty in the absence of a link be-
tween publications. We control the ambiguity by the
Gaussian distribution with precision parameter ci←j
as follows:

ci←j =
{
c+ if xi←j = 1
c− if xi←j = 0

(1)

where c+ > c− to ensure that we have more confi-
dence on the observed citations. This is an implicit
feedback approach that permits using negative ex-
amples (xi←j = 0) of sparse observations by miti-
gating their importance (Hu et al., 2008; Wang and
Blei, 2011; Purushotham et al., 2012). Setting dif-
ferent values to the precision parameter ci←j accord-
ing to xi←j induces cyclic dependencies between the
two variables. Due to this cycle, the model no longer
becomes a Bayesian network, or a directed acyclic
graph. However, we note that this setting does lead
to better experimental results, and we show the prag-
matic benefit of the setting in the Evaluation section.

3.3 Joint Modeling of the LTAI

In the LTAI, the topics and the link structures are si-
multaneously learned, and thus the content-related
variables and the citation-related variables mutually
reshape one another during the posterior inference.
On the other hand, if content and citation data are
modeled separately, the topics would not reflect any
information about the document citation structures.
Thus, in the LTAI, documents with shared links are
more likely to have similar topic distributions which
leads to a better model fit. We develop and explain
this joint inference in section 4. In section 7, we il-
lustrate the differences in word-level predictive pow-
ers of the LTAI and LDA.

4 Posterior Inference

We develop a hybrid inference algorithm in which
the posterior of content-related parameters θ, z, and
β are approximated by variational inference, and
author-related parameters π and η are approximated
by an expectation-maximization (EM) algorithm. In
algorithm 1, we summarize the full inference proce-
dure of the LTAI.

194



4.1 Content Parameters: Variational Update

Since computing the posterior distribution of the
LTAI is intractable, we use variational inference to
optimize variational parameters each of which cor-
respond to original content-related variables. Fol-
lowing the standard mean-field variational approach,
we define fully factorized variational distributions
over the topic-related latent variables q(θ, β, z) =∏
i q(θi|Ψin)

∏
Ni
q(zin|γi)

∏
k q(βk|λk) where for

each factorized variational distribution, we place the
same family of distributions as the original distribu-
tion. Using the variational distributions, we bound
the log-likelihood of the model as follows:

L[q] = Eq[
∑

k

log p(βk|αβ) +
∑

i

log p(θi|αθ)

+
∑

i

∑

Ni

log p(zin|θd) + log p(win|βzin) (2)

+
∑

i,j

log p(xi←j |zi, zj , πi)]−H[q]

whereH[q] is the negative entropy of q.
Taking the derivatives of this lower bound with

respect to each variational parameter, we can ob-
tain the coordinate ascent updates. The update for
the variational Dirichlet parameters γi and the λk is
the same as the standard variational update for LDA
(Blei et al., 2003). The update for the variational
multinomial φin is:

φink ∝ exp
{∑

j ∂Eq[log p(xi←j |z̄i, z̄j , πi, η)]
∂φink

+

∑
j ∂Eq[log p(xj←i|z̄j , z̄i, πj , η)]

∂φink
(3)

+ Eq[log θik] + Eq[log βkwin ]}

where the gradient of expected log probabilities of
both incoming link xi←j and outgoing link xj←i
contribute to the variational parameter. The first ex-
pectation can be rewritten as

Eq[log p(xi←j |z̄i, z̄j , πi, η)] (4)
= Eq[log

∑

a∈Ai
p(ai←j = a|πi)p(xi←j |z̄i, z̄j , ηa)]

≥
∑

a∈Ai
p(ai←j = a|πi)Eq[log p(xi←j |z̄i, z̄j , ηa)]

Algorithm 1 Posterior inference algorithm for the
LTAI

Initialize γ, λ, π, and η randomly
Set learning-rate parameter ρt that satisfies
Robbins-Monro condition
Set subsample sizes SV , SE , SS and SA
repeat

Variational update: local publication parameters
SS ← SS randomly sampled publications
for i in SS do

for n = 1 to Ni do
S←, S→ ← Set of SV random samples
Update φink using Equation 4, 5, 9.

end for
γi ← αθ +

∑
Ni
φin

end for

EM update: local author parameters
SA ← SA randomly sampled authors
for a in SA do
SE ← SE random publication pairs
Update ηa using Equation 7, 10
for i in Da and j = 1 to D do

πi←ja ∝ πiaN (z̄i>ηaz̄j , c−1i←j)
end for

end for

Stochastic variational update
for k = 1 to K do

λ̂k ← αβ + DSS
∑SS
d=1

∑Nd
n=1 φ

k
dnwdn

end for
Set λ(t) ← (1− ρt)λ(t−1) + ρtλ̂

until satisfying converge criteria

where Ai is the set of authors of i. We take the
lower bound of the expectation using Jensen’s in-
equality. The last term is approximated by the first
order Taylor expansion Eq[log p(xi←j |z̄i, z̄j , ηa)] =
N (xi←j |φ̄>i diag(ηa)φ̄j , c−1i←j). Finally, the approxi-
mated gradient of φink with respect to the incoming
directions to document i is

∑
j ∂Eq[log p(xi←j |z̄i, z̄j , πi, η)]

∂φink
≈ (5)

∑

j

φ̄jkci←j
Ni

∑

a∈Ai
ηak(xi←j − φ̄>i diag(ηa)φ̄j)p(a|πi)

where diag is a diagonalization operator and φ̄i is∑Ni
n=1 φin/Ni. We can compute the gradient with

respect to the outgoing directions in the same way.

195



4.2 Author Parameters: EM Step

We use the EM algorithm to update author-related
parameters π, and η based on the lower bound com-
puted by variational inference. In the E step, we
compute the probability of author contribution to the
link between document i and j.

πi←ja =
πiaN (z̄i>ηaz̄j , c−1i←j)∑

a′∈Ai
πia′N (z̄i>ηa′ z̄j , c−1i←j)

(6)

In the M step, we optimize the authority param-
eter η for each author. Given the other estimated
parameters, taking the gradient of L with respect to
ηa and setting it to zero leads to the following update
equation:

ηa = (Ψ
>
a CaΨa + αηI)

−1Ψ>a CaXa (7)

Let Da be the set of documents written by author
a and Da(i) be the ith document written by a. Then
Ψa is a vertical stack of |Da|matrices ΨDa(i), whose
jth row is φ̄Da(i) ◦ φ̄j , the Hadamard product be-
tween φ̄Da(i) and φ̄j . Similarly, Ca is a vertical stack
of |Da|matrices CDa(i) whose j th diagonal element
is cDa(i)←j , andXa is a vertical stack of |Da| vectors
XDa(i) whose j th element is πDa(i)←ja×xDa(i)←j .
Finally, we update πDa(i)a =

∑
j πDa(i)←ja/D.

5 Faster Inference Using Stochastic
Optimization

To model topical authority, the LTAI considers the
linkage information. If two papers are linked by ci-
tation, the topical authority of the cited paper’s au-
thors will increase while the negative link buffers the
potential noise of irrelevant topics. This algorith-
mic design of the LTAI results in high model com-
plexity. To remedy this issue, we adopt the noisy
gradient method from the stochastic approximation
algorithm (Robbins and Monro, 1951) to subsam-
ple negative links for updating per-document topic
variational parameter φ and authority parameter η.
The prior work of using subsampled negative links
to reduce computational complexity is introduced in
(Raftery et al., 2012). We elucidate how stochastic
variational inference (Hoffman et al., 2013) is ap-
plied in our model to update global per-topic-word
variational parameter λ.

5.1 Updating φ and η
Updating φ̄i for document i in variational update re-
quires iterating over every other document and com-
puting the gradient of link probability. This leads to
the time complexity O(DK) for every φ̄i.

To apply the noisy gradient method, we divide the
gradient of the expected log probability of link into
two parts:

∑

j

∂Eq[log p(xi←j |z̄i, z̄j , πi)]
∂φink

= (8)

∑

j:xi←j=1

∂Eq[log p(xi←j)]
∂φink

+
∑

j:xi←j=0

∂Eq[log p(xi←j)]
∂φink

where the first and the second terms of the right-
hand side are the gradient sums of positive links
(xi←j = 1) and negative links (xi←j = 0) respec-
tively. Compared to positive links, the order of neg-
ative links is close to the total number of documents,
and thus computing the second term results in com-
putational inefficiency. However, in our model, we
reduced the importance of the negative links by as-
signing a larger variance c−1i←j compared to the pos-
itive links, and the empirical mean of φ̄j for neg-
ative links follows the Dirichlet expectation due to
the large number of negative links. Therefore, we
approximate the expectation of the gradient for the
negative links using the noisy gradient as follows:

∑

j:xi←j=0

∂Eq[log p(xi←j)]
∂φink

=
D−i
SV

∑

SV

∂Eq[log p(xi←s)]
∂φink

(9)

where D−i is the number of negative links (i.e.
xi←j = 0) of document i, and SV is the size of sub-
samples SV for the variational update. We randomly
sample SV documents, compute gradients on the
sampled documents, and then scale the average gra-
dient to the size of the negative link D−i . This noisy
gradient method reduces the updating time complex-
ity from O(DK) to O(SVK).

Now, we discuss how to approximate author’s
topical authority based on Equation 7. When K �
D × Da, the computational bottleneck is Ψ>a CaΨa
which has time complexity O(DDaK2). To allevi-
ate this complexity, we once again approximate the
large number of negative links using smaller num-
ber of subsamples. Specifically, while keeping the
positive link rows Ψa+ intact, we approximate nega-
tive link rows in Ψa using a smaller matrix Ψa− that

196



0 5000 10000 15000 20000 25000
Time (sec)

7.90

7.85

7.80

7.75

7.70

7.65

7.60

Lo
g
 P

re
d
ic

ti
v
e
 P

ro
b
a
b
ili

ty
Stochastic

Batch

Figure 3: Training time of the LTAI on CORA
dataset with stochastic and batch variational infer-
ence. Using stochastic variational inference, the per-
word predictive log likelihood converges faster than
using the batch variational inference.

Dataset # Tokens # Documents # Authors Avg C/D Avg C/A
CORA 17,059 13,147 12,111 3.46 12.17
Arxiv-Physics 49,807 27,770 10,950 12.70 67.93
PNAS 39,664 31,054 9,862 1.57 13.18
Citeseer 21,223 4,255 6,384 1.24 4.38

Table 1: Datasets. From left to right, each column
shows the number of word tokens, number of docu-
ments, number of authors, average citations per doc-
ument (Avg C/D), and average citations per author
(Avg C/A).

has SE rows, or the size of subsamples for the EM
step. Using this approximation, we can represent
Ψ>a CaΨa as

Ψ>a CaΨa = c+Ψ
>
a+Ψa+ +

c−D−a
SE

Ψ>a−Ψa− (10)

with the time complexity of O(SEK2), where D−a
is the number of rows with negative links in Ψa. Al-
though we do not incorporate rigorous analysis on
the performance of our model given the size of the
subsamples, we confirm that the negative link size
greater than 100 does not degrade the model perfor-
mance in any of our experiment.

5.2 Updating λ
In traditional coordinate ascent based variational in-
ference, the global variational parameter λ is up-
dated infrequently because all the other local param-
eters φ need to be updated beforehand. This problem
is more noticeable in the LTAI since updating φ us-
ing equation 3 is slower than updating φ in vanilla
LDA; moreover, per-author topical authority vari-
able η is another local variable that algorithm needs

to update a priori. However, using the stochastic
variational inference, the global parameters are up-
dated after a small portion of local parameters are
updated (Hoffman et al., 2013). Applying stochastic
variational inference for the LTAI is straightforward
after we calculate the intermediate topic-word varia-
tional parameter λ̂ by αβ + DSS

∑SS
d=1

∑Nd
n=1 φ

k
dnwdn

from the noisy estimate of the natural gradient with
respect to subsampled local parameters where Nd is
the number of words for document d, and SS is the
subsample size for the minibatch stochastic varia-
tional inference. The final global parameter for the
tth iteration λ(t) is updated by (1− ρt)λ(t−1) + ρtλ̂
where ρt is the learning-rate. Posterior inference
is guaranteed to converge at local optimum when
the learning rate satisfies the condition

∑∞
t=1 ρt =

∞,∑∞t=1 ρ2t < ∞ (Robbins and Monro, 1951). In
Figure 3, we confirm that stochastic variational in-
ference is applicable for the LTAI and reduces the
training time compared to using the batch counter-
part, while maintaining similar performance.

6 Experimental Settings

In this section, we introduce the four academic cor-
pora used to fit the LTAI, describe comparison mod-
els, and provide information about the evaluation
metric and parameter settings for the LTAI1.

6.1 Datasets

We experiment with four academic corpora: CORA
(McCallum et al., 2000), Arxiv-Physics (Gehrke et
al., 2003), the Proceedings of the National Academy
of Sciences (PNAS), and Citeseer (Lu and Getoor,
2003). CORA, Arxiv-Physics, and PNAS datasets
contain abstracts only, and the locations of the cita-
tions within each paper are not preserved, whereas
the Citeseer dataset contains the citation locations.
For CORA, Arxiv-Physics, and PNAS, we lemma-
tize words, remove stop words, and discard words
that occur fewer than four times in the corpus. Ta-
ble 1 describes the datasets in detail. Note that we
obtain citation data from the entire document, not
only from the abstract. Also, we consider within-
corpus citation only, which leads to less than 13 av-
erage citation counts per document for all corpora.

1Code and datasets are available at http://uilab.
kaist.ac.kr/research/TACL2017/

197



0 500 1000 1500 2000

Iteration

7.850

7.725

7.600

Lo
g
 P

re
d
ic

ti
v
e
 P

ro
b
a
b
ili

ty

(a) CORA

0 500 1000 1500 2000

Iteration

7.950

7.825

7.700

Lo
g
 P

re
d
ic

ti
v
e
 P

ro
b
a
b
ili

ty

(b) Arxiv-Physics

LTAI

LTAI-10% LTAI-20%

LDA

LTAI-30%

0 500 1000 1500 2000

Iteration

8.500

8.325

8.150

Lo
g
 P

re
d
ic

ti
v
e
 P

ro
b
a
b
ili

ty

(c) PNAS

0 500 1000 1500 2000

Iteration

8.3

8.0

7.7

Lo
g
 P

re
d
ic

ti
v
e
 P

ro
b
a
b
ili

ty

(d) Citeseer

Figure 4: Word-level prediction result. We mea-
sured per-word log predictive probability on four
datasets. As shown in the graphs, our model per-
forms better than LDA.

6.2 Comparison Models

We compare predictive performance of the LTAI
with five other models. Different comparison mod-
els have different degrees of expressive powers.
Each model conducts a certain type of prediction
task; while RTM, ALTM, and DACTM predicts ci-
tation structures, the topical h-index predicts author-
ship information. The baseline topic models are
implemented based on the inference methods sug-
gested in the corresponding papers; LDA, RTM and
the LTAI variants use variational inference, while
ALTM and DACTM use collapsed Gibbs sampling.
Finally, all the conditions for implementation such
as the choice of programming language and mod-
ules, except for parts that convey each model’s
unique assumption, are identically set; thus, the per-
formance differences between models are due to
their model assumption and different degrees of data
usage, rather than the implementation technicalities.

Latent Dirichlet Allocation: LDA (Blei et al.,
2003) discovers topics and represents each publica-
tion by a mixture of the topics. Compared to other
models, LDA only uses the content information.

LTAI-n%: In LTAI-n%, we remove n% of ac-
tual citations and displace them with arbitrarily se-
lected false connections. Note that the link struc-
tures are displaced rather than removed. If the ci-
tation links are just removed, the LTAI and LTAI-
n% cannot be fairly compared as the density of the
citation structures will be affected and each model
needs different concentration values. Performance
difference between the LTAI and this indicates that
under identical conditions, using the correct linkage
information is indeed beneficial for prediction.

LTAI-C: In LTAI-C the precision parameter ci←j
has constant value, rather than assigning different
values according to xi←j as discussed in section 3.

LTAI-SEP: LTAI-SEP has an identical structure
as the LTAI, but the topic and the authority variables
are separately learned. Once the topic variables are
learned using the vanilla LDA, authority and citation
variables are then inferred consecutively. Thus, the
performance edge of the LTAI over LTAI-SEP high-
lights the necessity of the LTAI’s joint modeling in
which both topic and authority related variables re-
shape one another in an iterative fashion.

Relational Topic Model: RTM (Chang and Blei,
2010) jointly models content and citation, and thus,
topic proportions of a pair of publications become
similar if the pair is connected by citations. Com-
pared to the LTAI, the author information is not con-
sidered, the link structure does not have directional-
ity and the model does not consider negative links.

Author-Link Topic Model: ALTM (Kataria et
al., 2011) is a variation of author topic model (ATM)
(Rosen-Zvi et al., 2004) that models both topical
interests and influence of authors in scientific cor-
pora. The model uses content information of citing
papers and names of the cited authors as word to-
kens. ALTM outputs per-topic author distribution
that functions as author influence indices.

Dynamic Author-Citation Topic Model:
DACTM (Kataria et al., 2011) is an extension of
ALTM that requires publication corpora which
preserves sentence structures. To model author
influence, DACTM selectively uses words that are
close to the point where the citation is presented.

198



100 200 300 400

Number of Topics

0.02

0.05

0.08

M
R

R

(a) CORA

100 200 300 400

Number of Topics

0.010

0.025

0.040

M
R

R

(b) Arxiv-Physics

LTAI

RTM

LTAI-C

DACTM

LTAI-SEP

ALTM

100 200 300 400

Number of Topics

0.010

0.025

0.040

M
R

R

(c) PNAS

100 200 300 400

Number of Topics

0.04

0.10

0.16

M
R

R

(d) Citeseer

Figure 5: Citation prediction results. The task is to
find out which paper is originally linked to a cited
paper. We measure Mean Reciprocal Rank (MRR)
to evaluate model performance. For all cases, the
LTAI performs better than the other methods.

In our corpora, only Citeseer dataset preserves the
sentence structure.

Topical h-index: To compute topical h-index, we
separate the papers into several clusters using LDA
and calculate the h-index within each cluster. Topi-
cal h-index is used for author prediction in the same
manner as we did for our model, except the topic
proportions are replaced to the LDA’s result and η is
replaced to the topical h-index values.

6.3 Evaluation Metric and Parameter Settings

We use MRR (Voorhees, 1999) to measure the pre-
dictive performance of the LTAI and the comparison
models. MRR is a widely used metric for evaluat-
ing link prediction tasks (Balog and de Rijke, 2007;
Diehl et al., 2007; Radlinski et al., 2008; Huang et
al., 2015). When the models output the correct an-

swers as ranks, MRR is the inverse of the harmonic
mean of such ranks.

We report the parameter values used for evalua-
tions. For all datasets, we set c− to 1. To predict a
citation, we set c+ to 10,000, 100, 1,000, 10, and
to predict authorship, we set c+ to 1,000, 1,000,
10,000, 1,000 for CORA, Arxiv-Physics, PNAS, and
Citeseer datasets. These values are obtained through
exhaustive parameter analysis. We set αθ to 1, and
αβ to 0.1. We fix the subsample sizes to 5002. For
fair comparison, all the parameters that the LTAI and
the baseline models share are set to have the same
values, and for other parameters that uniquely be-
long to the baseline models, the values are exhaus-
tively tuned as done in the LTAI. Finally, we note
that all parameters are tuned using the training set,
and test dataset is used only for the testing purpose.

7 Evaluation

We conduct the evaluation of the LTAI with three
different quantitative tasks, along with one qualita-
tive analysis. In the first task, we check whether
using citation and authorship information in the
LTAI helps increase the word-level predictive per-
formance. In the second and third tasks, we
measure the predictability of the LTAI regarding
missing publication-publication linkage and author-
publication linkage. With these two tasks, we com-
pare the predictive power of the LTAI with other
comparison models and use MRR as evaluation met-
ric. Finally, we observe famous researchers’ topi-
cal authority scores generated by the LTAI and in-
vestigate how these scores capture notable academic
characteristics of the researchers.

7.1 Word-level Prediction
In the LTAI, citation and authorship information af-
fect per-document topic proportions, as can be con-
firmed in equation 3. This joint modeling of content
and linkage structure, compared to vanilla LDA that
uses content data only, yields better performance in
terms of predicting missing words in documents. In
this task, we use log-predictive probability, a met-
ric that is widely used in other researches for mea-
suring model fitness (Teh et al., 2006; Asuncion et

2Although we do not present thorough sensitivity analysis in
this paper, we confirm that the performance of our model was
robust against adjusting the parameters within a factor of 2.

199



100 200 300 400

Number of Topics

0.050

0.085

0.120

M
R

R

(a) CORA

100 200 300 400

Number of Topics

0.02

0.04

0.06

M
R

R

(b) Arxiv-Physics

LTAI LTAI-C LTAI-SEP Topical h-index

100 200 300 400

Number of Topics

0.035

0.050

0.065

M
R

R

(c) PNAS

100 200 300 400

Number of Topics

0.100

0.115

0.130

M
R

R

(d) Citeseer

Figure 6: Author prediction results. The task is to
find out who the author of a cited paper is, given all
the citing papers. For all cases, the LTAI performs
better than the other methods.

al., 2009; Hoffman et al., 2013). For each corpus,
we separate one third of the documents as the test
set. For all documents in each test set, we use half
of the words for training per-document topic pro-
portion θ and predict the probability of word occur-
rence regarding the remaining half. Specifically, the
predictive probability for a word in a test set wnew
with respect to the given words wobs and the train-
ing documentDtrain is computed using the equation
p(wnew|Dtrain, wobs) =

∑K
k=1 Eq[θk]Eq[βk,wnew ].

Figure 4 illustrates the per-word log-predictive
probability in each corpus. We confirm that when
using the LTAI, the log predictive probability con-
verges at a higher value compared to the result us-
ing LDA. Also, when we corrupt the link structure
from 10% to 30% the predictive performances of the
LTAI gradually decrease. Thus, the LTAI’s superior
predictive performance is attributed to its usage of
correct citations rather than the algorithmic bias.

7.2 Citation Prediction

We evaluate model predictability regarding which
publication is originally citing a certain publica-
tion. Specifically, we randomly remove one ci-
tation from each of the documents in the test
set. To predict the citation link between pub-
lications, we first compute the probability that
publication j cites i from p(xi←j |z,Ai, πi) ∝∑

a∈Ai πi←jaN (xi←j |z̄>i diag(ηa)z̄j , c
−1
+ ). Given

the topic proportion of the cited publication θi and
the topical authorities of the authors ηa, we compute
which publication is more likely to cite the publi-
cation. Based on our model assumption in subsec-
tion 3.2, using topical authority increases the perfor-
mance of predicting linkage structure.

In Figure 5, the LTAI yields better citation predic-
tion performance than other models for all datasets
and with the most number of topics. Since the
LTAI incorporates topical authority for predicting ci-
tations, it performs better than RTM, which does not
discover topical authority. We can attribute the bet-
ter performance of the LTAI compared to ALTM and
DACTM to the LTAI’s multiple model assumptions
explained in section 3. We note that DACTM re-
quires additional information such as citation loca-
tion and sentence structure, and thus, is only appli-
cable for limited kinds of datasets.

7.3 Author Prediction

For author prediction, we randomly remove one
of the authors from documents in the test set
while preserving citation structures. Similar to
citation prediction, we predict which author is
more likely to write the cited publication based
on the topic proportions of cited publication i
and a set of citing publications J . We approx-
imate the probability of researcher a being an
author of publication i from p(a|z, ηa, xi←j) ∝∏
j∈J N (xi←j |z̄>i diag(ηa)z̄j , c−1+ ). Because the

mixture proportion of an unknown author πi←ja
cannot be obtained during posterior inference, we
assume the cited publication is written by a single
author to approximate the probability. For author
prediction, we choose the author that maximizes the
above probability. In Figure 6, the LTAI outperforms
the comparison models in most of the settings.

200



Author h-index # cite # paper Representative Topic T Authority

D Padua 12 291 21 parallel, efficient, computation, runtime 10.36

V Lesser 11 303 48 interaction, intelligent, multiagent, autonomous 11.92

M Lam 11 440 20 memory, processor, cache, synchronization 12.74

M Bellare 11 280 43 scheme, security, signature, attack 13.21

L Peterson 10 297 24 operating, mechanism, interface, thread 9.28

D Ferrari 10 377 18 traffic, delay, bandwidth, allocation 14.16

O Goldreich 9 229 49 proof, known, extended, notion 12.57

M Jordan 9 263 27 approximation, intelligence, artificial, correlation 10.15

D Culler 9 565 30 operating, mechanism, interface, thread 12.37

A Pentland 8 207 39 image, motion, visual, estimate 10.82

Table 2: Authors with the highest h-index scores and their statistics from the CORA dataset. We show the
authors with their h-index, number of citations (# cite), and number of papers (# paper), representative topic,
and their topical authority (T Authority) of the corresponding topic. We show that while the authors have
the highest h-indices with lots of papers written and lots of citations earned, the topics that the authors exert
authority varies.

7.4 Qualitative Analysis

To stress our model’s additional characteristics that
are not observed in the quantitative analysis, we look
at the assigned topical authority indices as well as
other statistics of some researchers in the dataset. In
the analyses, we set the number of topics to 100, and
use CORA dataset for demonstration.

We first demonstrate famous authors’ authorita-
tive topics that can be unveiled using our model. In
Table 2, we list top 10 authors with highest h-indices
along with their number of citations, number of pa-
pers, and their representative topics. Authors’ rep-
resentative topics are the topics with highest author-
ity scores. In the table, we observe that all authors
with top h-indices have written at least 18 papers and
earned at least 207 citations, which are the top 0.8%
and 0.2% values respectively. However, their au-
thoritative topics retrieved by the LTAI do not over-
lap for any of the authors. This table illustrates that
each of the top authors in the table exerts authority
on different academic topics that can be captured by
the LTAI, while the authors commonly have highest
h-index scores as well as other statistics.

We now stress attributes of topical authority index
that are different from other topic irrelevant statis-
tics. From Tables 3 to 5, we show four example
topics extracted by our model and list notable au-
thors within each topic with their topical authority
indices, h-indices, number of citations, and number
of papers. In the tables, we first find that all four

authors with highest topical authority values, Mon-
ica Lam, Alex Pentland, Michael Jordan, and Mihir
Bellare are also listed in the topic-irrelevant author-
ity rankings in Table 2. From this, we confirm that
authority score of the LTAI has a certain degree of
correlation to other statistics, while it splits the au-
thors by their authoritative topics.

At the same time, the topical authority score cor-
relates less with topic-irrelevant statistics than those
statistics correlate with themselves. In Table 5, Oded
Goldreich has lower topical authority score for the
computer security topic while having higher topic
irrelevant scores than the above four researchers,
because his main research filed is in the theory
of computation and randomness. We can spot au-
thors who exert high authority on multiple academic
fields, such as Tomaso Poggio in Table 3 and in Ta-
ble 4. Similarity, when comparing Federico Girosi
and Tomaso Poggio in Table 4, the two researchers
have similar authority indices for this topic while
Tomaso Poggio has higher values for the other three
topic-irrelevant indices. This is a reasonable out-
come when we investigate the two researchers’ pub-
lication history. Federico Girosi has relatively fo-
cused academic interest, with his publication his-
tory being skewed towards machine-learning-related
subjects, while Tomaso Poggio has broader topical
interests that include computer vision and statistical
learning, while also co-authoring most of the papers
that Federico Girosi wrote. Thus, Federico Girosi

201



Topic: image, motion, visual, estimate,

robust, shape, scene, geometric

Rank Author Topical Authority h-index # cite # paper

1 A Pentland 10.82 8 207 39

2 J Fessler 9.09 6 92 26

3 T Poggio 8.22 6 178 27

4 S Sclaroff 7.61 3 69 11

5 K Toyama 6.65 4 41 10

Table 3: Authors who have a high authority score in
a computer vision topic.

Topic: approximation, intelligence, artificial,

correlation, support, recognition, model, representation

Rank Author Topical Authority h-index # cite # paper

1 M Jordan 10.15 9 263 27

2 M Warmuth 9.57 8 160 17

13 T Poggio 3.48 6 178 27

17 F Girosi 3.22 3 101 9

34 M Jones 2.06 7 151 20

Table 4: Authors who have a high authority score in
an artificial intelligence topic.

has a similar authority index for this topic but has
lower authority indices for other topics than Tomaso
Poggio.

Also, our model is able to capture topic-specific
authoritative researchers that have relatively low
topic-irrelevant scores. For example, researchers
such as Stan Sclaroff and Kentaro Toyama are the
top 5 authoritative researchers in a computer vision
topic according to the LTAI, but it is difficult to de-
tect these researchers out of many other authoritative
authors using the topic-irrelevant scores.

Finally, the LTAI detect researchers’ topical au-
thority that is peripheral but not negligible. Mark
Jones in Table 4, who has high h-index, a number
of citations, and wrote many papers, is a researcher
whose academic interest lies in programming lan-
guage design and application. However, while most
of his papers’ main topics are about programming
language, he often uses inference techniques and al-
gorithms in machine learning in his papers. Our
model captures that tendency and assigns a positive
authority score for machine learning to him.

Topic: scheme, security, signature, attack,

threshold, authentication, cryptographic, encryption

Rank Author Topical Authority h-index # cite # paper

1 M Bellare 13.21 11 280 43

2 P Rogaway 11.98 7 117 13

3 H Krawczyk 7.29 6 75 15

4 R Canetti 7.13 4 40 10

9 O Goldreich 3.70 9 229 49

Table 5: Authors who have a high authority score in
a computer security topic.

8 Conclusion and Discussion

We proposed Latent Topical Authority Indexing
(LTAI) to model the topical-authority of academic
researchers. Based on the hypothesis that authors
play an important role in citations, we specifically
focus on their authority and develop a Bayesian
model to capture the authority. With model as-
sumptions that are necessary for extracting convinc-
ing and interpretable topical authority values for au-
thors, we have proposed speed-up methods that are
based on stochastic optimization.

While there is prior research in topic modeling
that provides topic-specific indices when modeling
the link structure, these do not extend to individual
indices, and most previous citation-based indices are
defined for each individual but without considering
topics. On the other hand, our model combines the
merits of both topic-specific and individual-specific
indices to provide topical authority information for
academic researchers.

With four academic datasets, we demonstrated
that the joint modeling of publication and author
related variables improve topic quality, when com-
pared to vanilla LDA. We quantitatively manifested
that including authority variables increases the pre-
dictive performance in terms of citation and author
predictions. Finally, we qualitatively demonstrated
the interpretability by topical-authority outcomes of
the LTAI from the CORA corpus.

Finally, there are issues that can be dealt with in
future work. We do not consider time information in
terms of when papers are published and when pairs
of papers are linked; we can use datasets that incor-
porate timestamps to enhance the model capability
to predict future citations and authorships.

202



Acknowledgments

We thank Jae Won Kim for collecting and refin-
ing the dataset and for contributing to the early
version of the manuscript. We also thank Ac-
tion Editor Noah Smith and the anonymous re-
viewers for their detailed and thoughtful comments;
and we thank Joon Hee Kim and the other UILab
members for providing helpful insights in the re-
search as well. Finally, we thank Editorial Assis-
tant Cindy Robinson for carefully proofreading the
manuscript. This work was supported by Institute
for Information & communications Technology Pro-
motion(IITP) grant funded by the Korea govern-
ment(MSIP) (No.B0101-15-0307, Basic Software
Research in Human-level Lifelong Machine Learn-
ing (Machine Learning Center)).

References

Arthur Asuncion, Max Welling, Padhraic Smyth, and
Yee Whye Teh. 2009. On smoothing and inference
for topic models. In UAI.

Krisztian Balog and Maarten de Rijke. 2007. Determin-
ing expert profiles (with an application to expert find-
ing). In IJCAI.

David Blei, Andrew Ng, and Michael Jordan. 2003. La-
tent Dirichlet Allocation. JMLR, 3:993–1022.

Cornelia Caragea, Florin Adrian Bulgarov, Andreea
Godea, and Sujatha Das Gollapalli. 2014. Citation-
enhanced keyphrase extraction from research papers:
A supervised approach. In EMNLP.

Cornelia Caragea, Florin Bulgarov, and Rada Mihalcea.
2015. Co-training for topic classification of scholarly
data. In EMNLP.

Jonathan Chang and David Blei. 2010. Hierarchical re-
lational models for document networks. The Annals of
Applied Statistics, 4(1):124–150.

Arman Cohan and Nazli Goharian. 2015. Scientific arti-
cle summarization using citation-context and article’s
discourse structure. In EMNLP.

Christopher Diehl, Galileo Namata, and Lise Getoor.
2007. Relationship identification for social network
discovery. In AAAI.

Laura Dietz, Steffen Bickel, and Tobias Scheffer. 2007.
Unsupervised prediction of citation influences. In
ICML.

Yuxiao Dong, Reid Johnson, and Nitesh Chawla. 2015.
Will this paper increase your h-index?: Scientific im-
pact prediction. In WSDM.

James Foulds and Padhraic Smyth. 2013. Modeling sci-
entific impact with topical influence regression. In
EMNLP.

Eugene Garfield. 2006. The history and meaning of the
journal impact factor. JAMA, 295(1):90–93.

Johannes Gehrke, Paul Ginsparg, and Jon Kleinberg.
2003. Overview of the 2003 KDD Cup. ACM
SIGKDD Explorations Newsletter, 5(2):149–151.

Yuan He, Cheng Wang, and Changjun Jiang. 2015. Dis-
covering canonical correlations between topical and
topological information in document networks. In
CIKM.

Jorge Hirsch. 2005. An index to quantify an individual’s
scientific research output. PNAS, 102(46):16569–
16572.

Matthew Hoffman, David Blei, Chong Wang, and John
Paisley. 2013. Stochastic variational inference.
JMLR, 14:1303–1347.

Yifan Hu, Yehuda Koren, and Chris Volinsky. 2008. Col-
laborative filtering for implicit feedback datasets. In
ICDM.

Wenyi Huang, Zhaohui Wu, Chen Liang, Prasenjit Mitra,
and Giles Lee. 2015. A neural probabilistic model for
context based citation recommendation. In AAAI.

Zhuoren Jiang. 2015. Chronological scientific infor-
mation recommendation via supervised dynamic topic
modeling. In WSDM.

Saurabh Kataria, Prasenjit Mitra, Cornelia Caragea, and
Giles Lee. 2011. Context sensitive topic models for
author influence in document networks. In IJCAI.

Yehuda Koren, Robert Bell, and Chris Volinsky. 2009.
Matrix factorization techniques for recommender sys-
tems. Computer, 42(8):30–37.

Yan Liu, Alexandru Niculescu-Mizil, and Wojciech
Gryc. 2009. Topic-link LDA: joint models of topic
and author community. In ICML.

Qing Lu and Lise Getoor. 2003. Link-based classifica-
tion. In ICML.

Andrew Kachites McCallum, Kamal Nigam, Jason Ren-
nie, and Kristie Seymore. 2000. Automating the con-
struction of internet portals with machine learning. In-
formation Retrieval, 3(2):127–163.

Michael Moravcsik and Poovanalingam Murugesan.
1975. Some results on the function and quality of ci-
tations. Social studies of science, 5(1):86–92.

Ramesh Nallapati, Amr Ahmed, Eric Xing, and William
Cohen. 2008. Joint latent topic models for text and
citations. In SIGKDD.

Willie Neiswanger, Chong Wang, Qirong Ho, and Eric P.
Xing. 2014. Modeling citation networks using latent
random offsets. In UAI.

Sanjay Purushotham, Yan Liu, and C.-C. Jay Kuo. 2012.
Collaborative topic regression with social matrix fac-
torization for recommendation systems. In ICML.

203



Filip Radlinski, Madhu Kurup, and Thorsten Joachims.
2008. How does clickthrough data reflect retrieval
quality? In CIKM.

Adrian Raftery, Xiaoyue Niu, Peter Hoff, and Ka Yee Ye-
ung. 2012. Fast inference for the latent space net-
work model using a case-control approximate likeli-
hood. Journal of Computational and Graphical Statis-
tics, 21(4):901–919.

Jasson Rennie and Nathan Srebro. 2005. Fast maximum
margin matrix factorization for collaborative predic-
tion. In ICML.

Herbert Robbins and Sutton Monro. 1951. A stochastic
approximation method. The annals of mathematical
statistics, 22(3):400–407.

Michal Rosen-Zvi, Thomas Griffiths, Mark Steyvers, and
Padhraic Smyth. 2004. The author-topic model for
authors and documents. In UAI.

Ruslan Salakhutdinov and Andriy Mnih. 2007. Proba-
bilistic matrix factorization. In NIPS.

Yanchuan Sim, Bryan Routledge, and Noah Smith. 2015.
A utility model of authors in the scientific community.
In EMNLP.

Yee Whye Teh, Michael Jordan, Matthew Beal, and

David Blei. 2006. Hierarchical Dirichlet pro-
cesses. Journal of the American Statistical Associa-
tion, 101(476):1566–1581.

Simone Teufel, Advaith Siddharthan, and Dan Tidhar.
2006. Automatic classification of citation function. In
EMNLP.

Yuancheng Tu, Nikhil Johri, Dan Roth, and Julia Hock-
enmaier. 2010. Citation author topic model in expert
search. In COLING.

Marco Valenzuela, Vu Ha, and Oren Etzioni. 2015.
Identifying meaningful citations. In Workshops at the
Twenty-Ninth AAAI Conference on Artificial Intelli-
gence.

Ellen Voorhees. 1999. The TREC-8 question answering
track report. In TREC.

Chong Wang and David Blei. 2011. Collaborative topic
modeling for recommending scientific articles. In
SIGKDD.

Jingang Wang, Dandan Song, Zhiwei Zhang, Lejian Liao,
Luo Si, and Chin-Yew Lin. 2015. LDTM: A latent
document type model for cumulative citation recom-
mendation. In EMNLP.

204


