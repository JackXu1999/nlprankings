


































Linear Compositional Distributional Semantics and Structural Kernels

Lorenzo Ferrone
University of Rome “Tor Vergata”

Via del Politecnico 1
00133 Roma, Italy

lorenzo.ferrone@gmail.com

Fabio Massimo Zanzotto
University of Rome “Tor Vergata”

Via del Politecnico 1
00133 Roma, Italy

fabio.massimo.zanzotto@uniroma2.it

Abstract

In this paper, we want to start the analy-
sis of the models for compositional dis-
tributional semantics (CDS) with respect
to the distributional similarity. We believe
that this simple analysis of the properties
of the similarity can help to better inves-
tigate new CDS models. We show that,
looking at CDS models from this point of
view, these models are strictly related with
convolution kernels (Haussler, 1999), e.g.:
tree kernels (Collins and Duffy, 2002).
We will then examine how the distributed
tree kernels (Zanzotto and Dell’Arciprete,
2012) are an interesting result to draw a
stronger link between CDS models and
convolution kernels.

1 Introduction

Distributional semantics (see (Turney and Pantel,
2010; Baroni and Lenci, 2010)) is an interest-
ing way of “learning from corpora” meaning for
words (Firth, 1957) and of comparing word mean-
ings (Harris, 1964). A flourishing research area
is compositional distributional semantics (CDS),
which aims to leverage distributional semantics
for accounting the meaning of word sequences and
sentences (Mitchell and Lapata, 2008; Baroni and
Zamparelli, 2010; Zanzotto et al., 2010; Guevara,
2010; Grefenstette and Sadrzadeh, 2011; Clark et
al., 2008; Socher et al., 2011). The area proposes
compositional operations to derive the meaning of
word sequences using the distributional meanings
of the words in the sequences.

The first and more important feature of distri-
butional semantics is to compare the meaning of
different words, a way to compute their similar-

ity. But, when focusing on compositional distribu-
tional semantics, methods are presented with re-
spect to the compositional operation of the vec-
tors. A scarce attention is given to how these oper-
ations affect the principal objective of the process
of compositional distributional semantics: assess-
ing the similarity between two word sequences.
This analysis is important as the similarity is gen-
erally used even by machine learning models such
as the kernel machines (Cristianini and Shawe-
Taylor, 2000).

In this paper, we want to start the analysis of the
models for compositional distributional semantics
with respect to the similarity measure. We focus
on linear CDS models. We believe that this sim-
ple analysis of the properties of the similarity can
help to better investigate new CDS models. We
show that, looking CDS models from this point
of view, these models are strictly related with the
convolution kernels (Haussler, 1999), e.g., the tree
kernels (Collins and Duffy, 2002). We will then
examine how the distributed tree kernels (Zanzotto
and Dell’Arciprete, 2012) are an interesting result
to draw a strongest link between CDS models and
convolution kernels.

The rest of the paper is organized as follows.
Section 2 focuses on the description of two basic
binary operations for compositional distributional
semantics, their recursive application to word se-
quences (or sentences) with a particular attention
to their effect on the similarity measure. Section 3
describes the tree kernels (Collins and Duffy,
2002), the distributed tree kernels (Zanzotto and
Dell’Arciprete, 2012), and the smoothed tree ker-
nels (Mehdad et al., 2010; Croce et al., 2011) to
introduce links with similarity measures applied
over compositionally obtained distributional vec-
tors. Section 4 draws sketches the future work.

85



2 Compositional distributional semantics
over sentences

Generally, the proposal of a model for composi-
tional distributional semantics stems from some
basic vector combination operations and, then,
these operations are recursively applied on the
parse tree on the sequence of words of the sen-
tences. In the rest of the section, we describe some
simple basic operations along with their effects
on the similarity between pairs of words and we
describe some simple recursive models based on
these operations. We finally describe how these
simple operations and their recursive applications
affect the similarity between sentences.

2.1 Two Basic Composition Operations
As we want to keep this analysis simple, we fo-
cus on two basic operations: the simple additive
model, (presented in (Mitchell and Lapata, 2008)
and cited as a comparative method in many re-
search papers), and the full additive model (esti-
mated in (Zanzotto et al., 2010; Guevara, 2010)).

We analyze these basic operations when result-
ing composed vectors are used to compute the sim-
ilarity between two pairs of words. For simplicity,
we use the dot product as the similarity measure.
Let a = a1a2 and b = b1b2 be the two sequences
of words and!a1,!a2,!b1, and!b2 be the related distri-
butional vectors. Let sim(a1a2, b1b2) be the sim-
ilarity computed applying the dot product on the
vectors !a and !b compositionally representing the
distributional semantics of a and b.

The Basic Additive model (ADD) (introduced
in (Mitchell and Lapata, 2008)) computes the dis-
tibutional semantics of a pair of words a = a1a2
as:

ADD(a1, a2) = (1− α) !a1 + α!a2

where 0 < α < 1 weigths the first and the second
word of the pair. Then, the similarity between two
pairs of words is:

sim(a, b) = ADD(a1, a2) ·ADD(b1, b2) =
(1− α)2!a1 ·!b1 + (1− α)α!a1 ·!b2 +
(1− α)α!a2 ·!b1 + α2!a2 ·!b2

that is, basically, the linear combination of the
similarities !ai · !bj between the words compos-
ing the sequences. For example, the similarity
between sim(animal extracts, beef extracts) takes
into consideration the similarity between animal

Figure 1: Two sentences and their simple depen-
dency graphs

(of the first pair) and extracts (of the second pair)
that can be totally irrelevant.

The Full Additive model (FADD) (used in
(Guevara, 2010) for adjective-noun pairs and
(Zanzotto et al., 2010) for three different syntactic
relations) computes the compositional vector !a of
a pair using two linear tranformations AR and BR
respectively applied to the vectors of the first and
the second word. These matrices generally only
depends on the syntactic relation R that links those
two words. The operation follows:

FADD(a1, a2, R) = AR !a1 +BR !a2

Then, the similariy between two pairs of words
linked by the same relation R is:

sim(a, b) = FADD(a1, a2) · FADD(b1, b2) =
= AR!a1 ·AR!b1 +AR!a1 ·BR!b2 +
BR!a2 ·AR!b1 +BR!a2 ·BR!b2

which is a linear combination of similarities be-
tween elements such as AR!ai, mixing a syn-
tactic factor, the matrix AR, and a single word
of the sequence, ai. In the above exam-
ple, sim(animal extracts, beef extracts), (where
we consider noun-noun (NN ) as the syntactic re-
lation) we also consider a factor (the similarity
BNN !extracts · ANN !beef ) that may not be rel-
evant in the similarity computation.

2.2 Recursive application on sentences and
its effects on the similarity

The two linear models seems so simple that it is
easy to think to recursively extend their applica-
tion to whole sentences. To explain what is going
on and what we are expecting, we will use two
sentences as a driving example: cows eat animal
extracts and chickens eat beef extracts. These are
similar because both have animals eating animal
extracts. This is what we expect the comparison
between these two sentences should evaluate. Let

86



RFADD(Ta) (2AV N !eat+BV N !cows+BV NANN !extracts+BV NBNN !animal)
sim(Ta, Tb) = · = ·

RFADD(Tb) (2AV N !eat+BV N !chickens+BV NANN !extracts+BV NBNN !beef)

Figure 2: Similarity using the recursive full additive model

x = x1 . . . xn and y = y1 . . . ym be two sentences
(or word sequences).

2.2.1 Recursive basic additive model
The recursive basic additive model (RADD) is
the first model we analyze. We can easily define
the model as follows:

RADD(xixi+1 . . . xn) =

= (1− α)"xi + αRADD(xi+1 . . . xn)

where RADD(xn) = "xn. Then, RADD(x) is a
weighted linear combination of the words in the
sentence x, that is:

RADD(x) =
n∑

i=1

λi "xi

where λi = αi−1(1− α) if i < n and λn = αn−1
depends on α and the position of xi in the se-
quence.

The similairity between two sentences x and y
is then:

sim(x, y) = RADD(x) ·RADD(y) =

=
n∑

i=1

m∑

j=1

λiλj "xi · "yj

This is the weighted linear combination of the sim-
ilariy among all the pairs of words taken from
the sentences x and y. Given these two sam-
ple sentences, this similarity measure hardly cap-
tures the similarity in terms of the generalized sen-
tence animals eating animal extracts. The mea-
sure also takes into consideration factors such as

"chicken · "beef that have a high similarity score
but that are not relevant for the similarity of the
whole sentence.

2.2.2 Recursive Full Additive Model
For the recursive Full Additive Model, we need to
introduce a structrured syntactic representation of
the sentences. The full additive models (presented
in Sec. 2.1) are defined on the syntactic depen-
dency R between the words of the pair. We then
use the dependency trees as syntactic representa-
tion. A dependency tree can be defined as a tree

whose nodes are words and the typed links are the
relations between two words. The root of the tree
represents the word that governs the meaning of
the sentence. A dependency tree T is then a word
if it is a final node or it has a root rT and links
(rT , Rel, Ci) where Ci is the i-th subtree of the
node rT and Rel is the relation that links the node
rT with Ci. The dependency trees of two example
sentences are reported in Figure 1.

Stemming from the full additive models
(FADD), the recursive FADD (RFADD) can be
straightforwardly and recursively defined as fol-
lows:

RFADD(T ) =
∑

i

(ARel "rT+BRelRFADD(Ci))

where (rT , Rel, Ci) are the links originated in the
root node rT .

By recursively applying the model to the first
sentence of the example (see Fig. 1), the resulting
vector is:

RFADD(cows eat animal extracts) =
= AV N "eat+BV N "cows+AV N "eat+

+BV NRFADD(animal extracts) =
= AV N "eat+BV N "cows+AV N "eat+

+BV NANN "extracts+BV NBNN "animal

A first observation is that each term of the sum has
a part that represents the structure and a part that
represents the meaning, for example:

structure︷ ︸︸ ︷
BV NBNN "beef︸︷︷︸

meaning

It is possible to formally show that the function
RFADD(T ) is a linear combination of elements
Ms "ws where Ms is a product of matrices that rep-
resents the structure and "ws is the distributional
meaning of one word in this structure, that is:

RFADD(T ) =
∑

s∈S(T )

Ms "ws

where S(T ) are the relevant substructures of T . In
this case, S(T ) contains the link chains.

87



Then, the similarity between two sentences in
this case is:

sim(T1, T2) = RFADD(T1) ·RFADD(T2) =
=

∑

s1∈S(T1),s2∈S(T2)

Ms1 !ws1 ·Ms2 !ws2

The similarity between the two sentences
Ta =cows eat animal extracts and Tb =chickens
eat beef extracts in Figure 1 is represented
in Figure 2. For the above dot product,
BV NANN !extracts · BV NANN !extracts = 1 as
these addend represents the same piece of struc-
ture, BV NBNN !beef)·BV NBNN !animal < 1 and
should be strictly related to the value of !beef ·

!animal as these two parts are representing the
branch of the tree describing the objects of the
verb in the two sentences. The same should hap-
pen for BV N !cows ·BV N !chickens < 1. But, what
do we expect for BV N !cows ·BV NBNN !beef? We
would like to have a similarity close to !beef · !cows
or a similarity near 0, as these words appear in a
different part of the structure? Going back to the
overall goal of evaluating the similarity between
the two sentences is clear that the second option
should be preferred.

3 Tree Kernels
We here come to the last point we want to describe,
the tree kernels (Collins and Duffy, 2002) and
some strictly related recent results, the distributed
tree kernels (Zanzotto and Dell’Arciprete, 2012)
and the smoothed tree kernels (Mehdad et al.,
2010; Croce et al., 2011). We want to show that
what is computed by the RADD and RFADD
is extremely similar to what is computed in tree
kernels.

Tree kernels are defined (Collins and Duffy,
2002) as convolution kernels (Haussler, 1999),
thus, they are generally defined recursively. But,
given two trees T1 and T2, these kernels are de-
fined as to compute the dot product between vec-
tors Φ(T1),Φ(T2), representing the trees in the
feature space Rn. Each dimensions (or features)
of this huge space Rn is a relevant subtree t and
we can consider that each relevant subtree t has
an associated vector !t. The vector !t is a vector of
the orthonormal basis of the space Rn. Then, the
function Φ, that maps trees into the space Rn, can
be defined as follows:

Φ(T ) =
∑

t∈S(T )

ωt!t

where ωt is a weight assigned to t in the tree T .
The tree kernel functions TK(T1, T2) then basi-
cally computes:

TK(T1, T2) =
∑

t1∈S(T1),t2∈S(T2)

ωt1ωt2 !t1 · !t2

where !t1 · !t2 is the Kronecker’s delta between t1
and t2, that is: !t1 · !t2 = δ(t1, t2).

The equation above is incredibly similar to
equation 1 that computes the similarity with
the recursive full additive model. There are
however two limitations in using tree kernels
to encode compositional distributional semantics
model: first, standard tree kernels only encode the
structure; second, standard tree kernels work in Rn
where n is huge making it infeasible to use such a
huge vectors. For the first issue, an interesting line
of research are the smoothed tree kernels (Mehdad
et al., 2010; Croce et al., 2011) that exploits distri-
butional vectors to compute the similarity among
nodes of the trees that contain words. For the sec-
ond issue an interesting recent result is the dis-
tributed tree kernel (Zanzotto and Dell’Arciprete,
2012) that approximates tree kernels by encoding
the huge space Rn in a smaller space Rd, with
d << n. This allows to encode structural infor-
mation into small vectors.

4 Conclusions

This paper presents some simple observations on
one of the current approaches to compositional
distributional semantics, drawing the link with the
deeply studied tree kernels and convolution ker-
nels. With this analysis, we aim to show that
these approaches are not radically different. In-
stead, (linear) compositional distributional models
can be rephrased as a compact version of some ex-
isting convolution kernels.

This paper is not conclusive as it leave open two
avenues: first, we need to prove that distributed
tree kernel (Zanzotto and Dell’Arciprete, 2012)
can also encode distributional informations as de-
scribed in the smoothed tree kernels (Mehdad et
al., 2010; Croce et al., 2011); second, it still leaves
unexplored how the similarity between sentences
is affected by the other compositional distribu-
tional models (Mitchell and Lapata, 2008; Ba-
roni and Zamparelli, 2010; Zanzotto et al., 2010;
Guevara, 2010; Grefenstette and Sadrzadeh, 2011;
Clark et al., 2008; Socher et al., 2011).

88



References
Marco Baroni and Alessandro Lenci. 2010. Distri-

butional memory: A general framework for corpus-
based semantics. Comput. Linguist., 36(4):673–721,
December.

Marco Baroni and Roberto Zamparelli. 2010. Nouns
are vectors, adjectives are matrices: Representing
adjective-noun constructions in semantic space. In
Proceedings of the 2010 Conference on Empirical
Methods in Natural Language Processing, pages
1183–1193, Cambridge, MA, October. Association
for Computational Linguistics.

Stephen Clark, Bob Coecke, and Mehrnoosh
Sadrzadeh. 2008. A compositional distributional
model of meaning. Proceedings of the Second
Symposium on Quantum Interaction (QI-2008),
pages 133–140.

Michael Collins and Nigel Duffy. 2002. New rank-
ing algorithms for parsing and tagging: Kernels over
discrete structures, and the voted perceptron. In Pro-
ceedings of ACL02.

Nello Cristianini and John Shawe-Taylor. 2000. An
Introduction to Support Vector Machines and Other
Kernel-based Learning Methods. Cambridge Uni-
versity Press, March.

Danilo Croce, Alessandro Moschitti, and Roberto
Basili. 2011. Structured lexical similarity via con-
volution kernels on dependency trees. In Proceed-
ings of the Conference on Empirical Methods in
Natural Language Processing, EMNLP ’11, pages
1034–1046, Stroudsburg, PA, USA. Association for
Computational Linguistics.

John R. Firth. 1957. Papers in Linguistics. Oxford
University Press., London.

Edward Grefenstette and Mehrnoosh Sadrzadeh. 2011.
Experimental support for a categorical composi-
tional distributional model of meaning. In Proceed-
ings of the Conference on Empirical Methods in
Natural Language Processing, EMNLP ’11, pages
1394–1404, Stroudsburg, PA, USA. Association for
Computational Linguistics.

Emiliano Guevara. 2010. A regression model of
adjective-noun compositionality in distributional se-
mantics. In Proceedings of the 2010 Workshop on
GEometrical Models of Natural Language Seman-
tics, pages 33–37, Uppsala, Sweden, July. Associa-
tion for Computational Linguistics.

Zellig Harris. 1964. Distributional structure. In Jer-
rold J. Katz and Jerry A. Fodor, editors, The Philos-
ophy of Linguistics. Oxford University Press, New
York.

David Haussler. 1999. Convolution kernels on discrete
structures. Technical report, University of Califor-
nia at Santa Cruz.

Yashar Mehdad, Alessandro Moschitti, and Fabio Mas-
simo Zanzotto. 2010. Syntactic/semantic struc-
tures for textual entailment recognition. In Human
Language Technologies: The 2010 Annual Confer-
ence of the North American Chapter of the Associa-
tion for Computational Linguistics, HLT ’10, pages
1020–1028, Stroudsburg, PA, USA. Association for
Computational Linguistics.

Jeff Mitchell and Mirella Lapata. 2008. Vector-based
models of semantic composition. In Proceedings
of ACL-08: HLT, pages 236–244, Columbus, Ohio,
June. Association for Computational Linguistics.

Richard Socher, Eric H. Huang, Jeffrey Pennington,
Andrew Y. Ng, and Christopher D. Manning. 2011.
Dynamic pooling and unfolding recursive autoen-
coders for paraphrase detection. In Advances in
Neural Information Processing Systems 24.

Peter D. Turney and Patrick Pantel. 2010. From fre-
quency to meaning: Vector space models of seman-
tics. J. Artif. Intell. Res. (JAIR), 37:141–188.

F.M. Zanzotto and L. Dell’Arciprete. 2012. Dis-
tributed tree kernels. In Proceedings of Interna-
tional Conference on Machine Learning, pages 193–
200.

Fabio Massimo Zanzotto, Ioannis Korkontzelos,
Francesca Fallucchi, and Suresh Manandhar. 2010.
Estimating linear models for compositional distribu-
tional semantics. In Proceedings of the 23rd Inter-
national Conference on Computational Linguistics
(COLING), August,.

89


