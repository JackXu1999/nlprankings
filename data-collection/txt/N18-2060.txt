



















































Syntactic Patterns Improve Information Extraction for Medical Search


Proceedings of NAACL-HLT 2018, pages 371–377
New Orleans, Louisiana, June 1 - 6, 2018. c©2018 Association for Computational Linguistics

Syntactic Patterns Improve Information Extraction for Medical Search
Roma Patel12, Yinfei Yang∗, Iain Marshall3, Ani Nenkova1 and Byron C. Wallace4

1Department of Computer and Information Science, University of Pennsylvania
2Department of Computer Science, Rutgers University-Camden

3Department of Primary Care and Public Health Sciences, Kings College London
4College of Computer and Information Science, Northeastern University

Abstract
Medical professionals search the published lit-
erature by specifying the type of patients, the
medical intervention(s) and the outcome mea-
sure(s) of interest. In this paper we demon-
strate how features encoding syntactic patterns
improve the performance of state-of-the-art se-
quence tagging models (both linear and neu-
ral) for information extraction of these medi-
cally relevant categories. We present an anal-
ysis of the type of patterns exploited, and
the semantic space induced for these, i.e., the
distributed representations learned for iden-
tified multi-token patterns. We show that
these learned representations differ substan-
tially from those of the constituent unigrams,
suggesting that the patterns capture contextual
information that is otherwise lost.

1 Introduction
The efficacy of medical treatments depends on pa-
tient characteristics, treatment administration de-
tails (e.g., dosage) and the measures or outcomes
used to quantify treatment success. These crite-
ria should be precisely defined when searching
the medical literature (Richardson et al., 1995;
Heneghan and Badenoch, 2013; Miller and For-
rest, 2001). Unfortunately, these aspects are not
usually described in a structured way. Abstracts
with explicit category headings (Nakayama et al.,
2005) partially address this, but these are not stan-
dardized nor uniform. Automated solutions are
thus emerging to better support medical search,
including methods for: identifying sentences con-
taining key pieces of clinical information (Wallace
et al., 2016); summarization (Sarker et al., 2016);
identifying contradictory claims in medical arti-
cles (Alamri and Stevenson, 2016); and informa-
tion retrieval system prototypes that harness this
type of information (Boudin et al., 2010a,b).

∗* now at Google Inc.

Several studies have assessed the use of the
PICO framework (Huang et al., 2006; Demner-
Fushman and Lin, 2007). Our task is also to iden-
tify spans of text describing PICO elements i.e.,
the participants (P), interventions (I)/comparators
(C), and outcomes (O) in the abstracts of ar-
ticles reporting findings from randomized con-
trolled trails (RCTs). We exploit the availabil-
ity of structured abstracts in the medical domain:
from these coarse (multi-)sentence labels we de-
rive patterns typically used in bootstrap meth-
ods for entity recognition and relation extraction
(Carlson et al., 2010). We incorporate these pat-
terns into supervised sequence labeling models to
improve the identification of P, I and O spans in
new texts. Below we show examples of each ex-
traction type: patterns are bolded and target PICO
description spans italicized. The extracted pat-
terns disambiguate fairly well the type of infor-
mation expressed in the segment when individual
words (e.g., “children”), do not. (P) The trial in-
cluded 230 children with Stage-IV lymphoblastic
leukemia ...

(I) In Group I, the children were treated with
prednisone ...

(O) .. reported that Group 2 children underwent
fewer isolated bone marrow relapses ..

We explore three strategies for exploiting ex-
tracted patterns in a state-of-the-art LSTM-CRF
sequence tagging model (Lample et al., 2016;
Ma and Hovy, 2016): as additional features at the
CRF layer; as one-hot indicators concatenated to
distributed representations of words; and as indi-
vidual units embedded in a semantic space shared
with words. The second representation improves
recall for two extraction tasks, and the third im-
proves precision for all three tasks. We analyze the
induced semantic space to show that patterns cap-
ture contextual information that is otherwise lost.

371



2 Data

For training sequence tagging models we use a
corpus of 4,741 medical article abstracts with
manual crowd-sourced annotations for P, I, and
O sequences. For testing we use a set of 191 ab-
stracts annotated for P, I, and O by medical profes-
sionals. There are 18,849 (831), 44,329 (1,808),
41,454 (1,711) variable length sequences for P, I,
and O in the training (testing) data.1

For minimally supervised extraction of n-gram
patterns, we use structured abstracts in which
the authors describe different aspects of their
work under targeted headings. We retrieved the
headings and associated sections automatically
from abstracts in XML format (downloaded from
PubMed2). In general abstracts are structured id-
iosyncratically (often as Introduction, Methods,
Results, Discussion). We capitalized on the minor-
ity of abstracts that used the explicit Participants,
Intervention and Outcome headings. We obtained
50,000 segments for each of these three categories.

3 Patterns extraction and analysis
We extract syntactic patterns associated with each
of the extraction types using AutoSlog-TS (Riloff,
1996), which consumes two sets of text: one rel-
evant to an extraction domain and one irrelevant.
In our case the relevant sets are the 50K P, I, and
O sections, respectively, from the structured ab-
stracts described above. The irrelevant set is a mix
of 25K of the other two categories.

AutoSlog-TS generates n-gram patterns from
input texts that capture the context of all noun
phrases appearing as subject, direct and indirect
object, or in a prepositional phrase. Each of these
patterns is scored with the estimated probability
that it occurred in an instance from the relevant
set (out of all occurrences of the pattern), scaled
by the number of times the pattern occurs (Riloff
and Phillips, 2004). Common patterns that tend to
occur in relevant sentences thus receive relatively
high scores. We filter out patterns that contain
digits, and those that occur fewer than 10 times
in the structured abstract texts. Of the remaining
patterns, we preserve those with probability 0.8 or

1The complete details of the corpus, along with inter-
annotator analysis and links for download of the full cor-
pus will be described in a forthcoming paper and eventually
made available here: http://www.byronwallace.
com/EBM_abstracts_data.

2https://www.nlm.nih.gov/databases/
download/pubmed_medline.html

higher of occurring with the relevant class. This
yields 3,499, 3,898 and 2,386 patterns associated
with P, I and O, respectively.

The vast majority of patterns are bigrams: 90%
for P, 81% for I and 86% O. Fewer than 0.5%
of the n-grams for each type are trigrams, and the
remaining are unigrams. Examples of extracted
patterns include: women who, years of and di-
agnosed with for P; patients received and per-
formed after for I; and scale of, patients reported
and rate of for O.

The majority (82.86%) of the extracted n-gram
patterns comprise a combination of a content word
and a function/stopword token.3 For example,
the patterns patients with, patients who or pa-
tients from are associated with the condition that
a patient had, while patients were, patients in
or patients received describe the treatment they
received. Function words provide disambiguat-
ing context for otherwise ambiguous words; this
aids text classification and information retrieval
(Riloff, 1995), and here we use them to improve
sequence tagging models.

4 Patterns + linear CRF
For supervised IE models, we first consider in-
cluding n-gram patterns as features in a linear-
chain CRF (Lafferty et al., 2001). The standard
set of token-level features used in the model in-
clude word identity, POS tag (from CoreNLP), and
a list of binary features indicating whether the to-
ken is a digit, title (i.e., the first token only is up-
percase), uppercase word, hyphenated word, or if
the token is a punctuation mark (colon, fullstop or
another symbol). In addition, features for the cur-
rent token include the identity of the previous and
next words, and the immediately preceeding and
following bi- and trigrams.

For the pattern-augmented CRF (CRF-
Pattern), we add nine binary features that
indicate if the current token and the immediately
preceeding/following bigrams are one of the
AutoSlog-TS patterns associated with a given
extraction type.4 There are three indicators, for
P, I and O respectively. For the context bigrams,
a feature is 1 if the bigram is one of the bigram
patterns associated with this extraction type,
0 otherwise. The remaining three indicators
have value 1 if the current token is one of the

3We use stopwords from CoreNLP (Manning et al., 2014).
4We ignore trigram patterns as they constitute <0.5% of

identified patterns.

372



Precision Recall F1
Model P I O P I O P I O
CRF 70.29 47.01 64.78 38.75 43.89 10.47 49.95 45.39 18.02

CRF-Pattern 73.3 52.1 66.37 40.62 45.41 44.07 52.27 48.52 52.96
LSTM-CRF 62.27 52.37 47.91 49.48 40.49 36.16 55.14 45.67 41.21

LSTM-CRF-Pattern (best) 76.10 58.25 44.66 64.75 43.39 35.20 69.97 49.74 39.69
Before CRF 61.87 38.65 46.27 41.45 23.8 37.27 49.64 29.55 41.28

Before BiLSTM 76.10 58.25 44.66 64.75 43.39 35.20 69.97 49.74 39.69
Embedding 55.18 51.07 44.30 54.24 47.41 41.60 54.71 49.17 42.91

Table 1: Models for extracting Participants, Intervention and Outcomes with and without pattern features, evalu-
ated via token-level precision, recall and F1 scores. The first and second groups of rows report results for CRF and
LSTM-CRF models without and with pattern features. The bottom group reports results achieved using different
means of incorporating pattern features in neural models.

unigram patterns associated with a given type.
For example, the nine features for the token
“chronic” in the sequence patients with chronic
sinus issues will be [1,0,0—0,0,0—0,0,0] because
patients with is one of the bigrams associated with
the P type, the word ”chronic” does not match any
of the unigram patterns and “sinus issues” does
not match any of the bigram patterns. Table 1
(top) reports the performance on the test data of
the original CRF model, and the one augmented
with pattern features. Including patterns yields
consistent and considerable improvements in both
precision and recall.

5 Patterns + LSTM-CRF
LSTM-CRF models (Lample et al., 2016; Ma and
Hovy, 2016) for sequence tagging are general in
that they do not require feature engineering. In-
stead, the features representing each token in the
CRF are generated by a bi-directional LSTM. To
generate this representation the LSTM consumes
distributed word representations as input and out-
puts vector representations describing words in
context (the bi-LSTM runs one LSTM in each
direction, concatenating outputs). This vector is
passed to a CRF layer for prediction. Character-
level information for each word is incorporated
by running a bi-LSTM over the characters of each
word (Lample et al., 2016). We used the IO tag-
ging scheme. We set the hidden state dimensions
to 200 and dropout to 0.5. We did not perform
gradient clipping. We used the Adam optimizer
(Kingma and Ba, 2014) with learning rate = 0.001.

We consider three alternatives for extending this
model with patterns. The first two use the indica-
tor features describing the presence of patterns in
the context, similar to those we described above
for the linear CRF model. The difference is where
these features are introduced: immediately before
the CRF layer, concatenated with the output of the

LSTMs (Before CRF), or as part of the input to
the LSTM, concatenated to the distributed word
and character representations (Before LSTM). We
use Moen and Ananiadou (2013)’s release of 200
dimensional word vectors trained over 5.5 billion
words from medical articles as pre-trained word
embeddings as input to the LSTM. We use the
same set of hyperparamaters for the LSTM as used
in Lample et al. (2016), and do not optimize these
for the present extraction tasks. The third alter-
native (Embedding) treats the patterns as collo-
cations; we derive embedded representations for
them as a unit, the way collocations are treated in
Mikolov et al. (2013b). In training and during pre-
diction each occurrence of a pattern in the input is
treated as a single token with a corresponding dis-
tributed representation. Character-level represen-
tations are concatenated to word representations
and the output of the LSTM cells is passed to the
CRF to make predictions (as above).

For these embeddings, we collected 6 million
PubMed abstracts (∼1.4 billion words) filtering
for only Human RCTs and used this to train word
vectors using the Word2Vec tool (Mikolov et al.,
2013a), inducing 200-dimensional vectors using
the Skip-Gram model, where our vocabulary now
consists of the learned n-gram patterns as single
units, along with other unigrams. We then test
these embedding representations by using them as
input to our neural model for the structured pre-
diction task.

6 Discussion of results
Table 1 reports the performance of the LSTM-
CRF model achieved using each of the three strate-
gies for incorporating pattern features discussed
above. Inserting the pattern indicator features be-
fore the CRF layer yields the worst performance.
Compared to the generic LSTM-CRF model, its
F -measure is lower or the same for all three ex-

373



n-gram similar to n-gram similar to unigram
have children 1: marry 2: conceive 3: breast-feed 1: adults 2: adolescents 3: toddlers

4: be pregnant 5: have surgery 4: youngsters 5: school-age
condition at 1: status at 2: features at 3: outcome at 1: circumstance 2: conditions 3: malady

4: qol at 5: outcomes 4: ailment 5: situation
filled with 1: covered with 2: mixed with 3: sealed with 1: sealed 2: obturated 3: enclosed

4: suspended 5: immersed in 4: enclosing 5: fill
side effects 1: toxicities 2: side-effect 3: complications 1: effect 2: Effects 3: action

4: AEs 5: nausea 4: impact 5: influence

Table 2: Example illustrating the shift in semantic space realized using pattern embeddings. For each of the listed
n-grams, we report the top 5 most similar words to: (1) the n-gram pattern embedding, and, (2) the most relevant
constituent n-gram i.e., the word in bold font.

traction categories, P, I, O.
Including the pattern features as input to the

LSTM or as part of the embedding leads to sub-
stantial improvements over the baseline model,
and this despite the smaller dataset over which pat-
tern embeddings were learned: compared to the
LSTM-CRF without pattern features, the former
markedly improves precision for P and I, while the
latter improves the recall for all three types. In
terms of F -measure, best results for P and I are
achieved by inserting the pattern features as in-
put to the LSTM, with about 15% and 4% abso-
lute improvement. For O, the best F -measure is
achieved by incorporating patterns as part of the
embeddings, yielding 1% absolute improvement.

The linear CRF and its variant enriched with
pattern feature has the best precision, outperform-
ing the LSTM-CRF models, but worse recall. It
may still be useful for scenarios in which high pre-
cision extraction is needed.

7 Semantics of pattern embeddings
We established that syntactic patterns can
markedly improve the extraction of patient,
intervention and outcome descriptions in medical
abstracts. We now turn to an analysis of how
the patterns fit into the semantic space of word
embeddings. Our goal is to quantify the extent to
which including pattern representations changes
which words will be considered similar to the
pattern, but not to the words that compose it.

To this end, we find the ten words most similar
(under cosine similarity) to each pattern, and those
most similar to the individual words these com-
prise, in the embedding space. We analyze the size
of the intersection of these two sets for all patterns
(∼10,000). To simplify the comparison we con-
sider only the constituent word that has the largest
intersection of similar words with the pattern of
interest. The size of the intersection theoretically
ranges from 0 to 10, but on average there is only

Figure 1: Scatter of PCA-reduced embeddings clus-
tered using K-means. <> brackets show the syntactic
pattern n-grams given by Autoslog-TS that are embed-
ding in the same space as unigrams.

one word overlap between the words most similar
with the pattern and those most similar with the
constituent word. For the majority (61%) of the
pattern–constituent word pairs, there is no over-
lap between the top 10 most similar words. To
make this discussion more concrete, Table 2 pro-
vides examples of the top 5 most similar words
to select bigram patterns and the constituent uni-
gram with greatest overlap, shown in italics. The
patterns encode disambiguating context that was
previously lost in unigram representations.

Finally, we present a scatter of learned embed-
dings, reduced via IncrementalPCA5 in Figure 1.
Embedded patterns cluster more intuitively than
their content words alone. For example, the pat-
terns injection of and administration of cluster
together, along with other topically similar uni-
grams such as infusion and intravenous that may
all correspond to Intervention terms. Similarly,
side effect is very different from its constituent
words side or effect, and moreover, clusters with
actual side effects like headache and fatigue that
patients may suffer from in the course of a trial.

5We use the implementation in scikit-learn (Pedregosa
et al., 2011).

374



Precision Recall F1
Model P I O P I O P I O

LSTM-CRF 62.27 52.37 47.91 49.48 40.49 36.16 55.14 45.67 41.21
LSTM-CRF (Bigrams) 64.41 53.37 43.20 50.33 41.24 37.32 59.91 46.52 40.04
LSTM-CRF (Autoslog) 76.10 58.25 44.66 64.75 43.39 35.20 69.97 49.74 39.69

Table 3: Results to illustrate syntactic nature of Autoslog bigrams. Row 1 shows results of baseline model with no
added features. Row 2 shows results of the model that uses all bigrams as features and Row 3 shows results of the
model that uses only Autoslog extracted bigrams as features. Features are added before the LSTM, as incorporated
in the best working model from Table 1.

8 Syntactic patterns vs bigrams

Our experiments show that using these bigram fea-
tures extracted by AutoSlog improves model pre-
dictions. AutoSlog takes a fundamentally syntax-
driven approach to identifying patterns, which
suggests the discovered patterns (and associated
performance boost) is due to exploiting syntax.
However, the performance gains could also be due
to additional contextual information that bigrams
and larger n-grams provide over unigrams alone,
rather than their syntactic properties.

We therefore performed an experiment to assess
the influence of the syntactic AutoSlog bigrams,
as compared to general bigram features. We con-
sider the same data used as input to AutoSlog,
i.e., 50,000 segments for the three categories P,
I, and O. In the same setup, we decompose sen-
tences within each category into bigrams, and col-
lect bigram counts in the respective categories. We
calculate precision for each category by collaps-
ing the other two categories, similar to the Au-
toSlog procedure. We use the same threshold val-
ues as AutoSlog for filtering, i.e., we remove bi-
grams that occur fewer than 10 times or that have
a score <0.8 of occuring with the target class out
of all occurrences. This procedure for identifying
predictive bigrams yields a notably larger number
of bigrams (30k) than AutoSlog (∼10K). Table
3 shows that while using generic bigrams as fea-
tures sometimes leads to small improvements, the
AutoSlog induced pattern bigrams result in sub-
stantially better performance. This suggests that
the exploitation of syntactic structure in identify-
ing patterns is indeed important. We also com-
pare the performance of word2vec embeddings for
unigrams and bigrams, and extended with collo-
cations and syntactic patterns, trained on exactly
the same data. In the experiments reported in Ta-
ble 1, the unigram embeddings are trained on a
larger dataset of generic medical text while the
patterns are trained on a smaller set of medical ab-

stracts describing RCTs. In addition here we com-
pare the AutoSlog patterns with collocations dis-
covered by word2vec. Representing collocations
leads to markedly lower F-score (Table 4). Rep-
resenting bigrams leads to prediction performance
better than that with collocations, but worse than
unigrams.

Standard unigram representations that we
trained work better than the off-the-shelf medi-
cal representations, possibly because they were
trained specifically on abstracts of papers report-
ing the conduct and results of RCTs and thus bet-
ter fit the abstracts we are analyzing. Most impor-
tantly, the LSTM-CRF with syntactic pattern em-
beddings results in the best observed performance.

Embedding Vocabulary P I O
Unigram 947,670 54.31 46.19 42.68
Bigram 9,326,144 52.01 43.71 38.77

collocation 1,254,863 50.31 40.21 40.21
Pattern 949,112 54.71 47.68 42.27

Table 4: LSTM-CRF predictions on word embeddings
trained on the same 6 million documents. Column 1
shows the type of embedding, column 2 shows the size
of the vocabulary and columns 3-5 show F1 score.

9 Conclusions
We presented a method for exploiting abun-
dant unlabeled biomedical texts to generate min-
imally supervised extraction patterns that improve
generic supervised models for sequence tagging
in this domain. We explored alternative ways to
incorporating the patterns in both linear and neu-
ral tagging models. In the latter, we analyzed the
changes in semantic space that likely explain the
observed gains in predictive performance.

10 Acknowledgements
This work was supported in part by the National
Cancer Institute (NCI) of the National Institutes of
Health (NIH), award number UH2CA203711 and
the National Science Foundation (NSF), award
number CCF-1433220.

375



References
Abdulaziz Alamri and Mark Stevenson. 2016. A

corpus of potentially contradictory research claims
from cardiovascular research abstracts. Journal of
biomedical semantics 7(1):36.

Florian Boudin, Jian-Yun Nie, and Martin Dawes.
2010a. Clinical information retrieval using doc-
ument and PICO structure. In Human Language
Technologies: Conference of the North American
Chapter of the Association of Computational Lin-
guistics, Proceedings, June 2-4, 2010, Los Angeles,
California, USA. pages 822–830.

Florian Boudin, Lixin Shi, and Jian-Yun Nie. 2010b.
Improving medical information retrieval with PICO
element detection. In Advances in Information Re-
trieval, 32nd European Conference on IR Research,
ECIR 2010, Milton Keynes, UK, March 28-31, 2010.
Proceedings. pages 50–61.

Andrew Carlson, Justin Betteridge, Bryan Kisiel, Burr
Settles, Estevam R. Hruschka Jr., and Tom M.
Mitchell. 2010. Toward an architecture for never-
ending language learning. In Proceedings of the
Twenty-Fourth AAAI Conference on Artificial Intel-
ligence, AAAI 2010, Atlanta, Georgia, USA, July 11-
15, 2010.

Dina Demner-Fushman and Jimmy Lin. 2007. An-
swering clinical questions with knowledge-based
and statistical techniques. Computational Linguis-
tics 33(1):63–103.

Carl Heneghan and Douglas Badenoch. 2013.
Evidence-based medicine toolkit. John Wiley &
Sons.

Xiaoli Huang, Jimmy Lin, and Dina Demner-Fushman.
2006. Evaluation of pico as a knowledge represen-
tation for clinical questions. In AMIA annual sym-
posium proceedings. American Medical Informatics
Association, volume 2006, page 359.

Diederik P Kingma and Jimmy Ba. 2014. Adam: A
method for stochastic optimization. arXiv preprint
arXiv:1412.6980 .

John Lafferty, Andrew McCallum, and Fernando CN
Pereira. 2001. Conditional random fields: Prob-
abilistic models for segmenting and labeling se-
quence data .

Guillaume Lample, Miguel Ballesteros, Sandeep Sub-
ramanian, Kazuya Kawakami, and Chris Dyer. 2016.
Neural architectures for named entity recognition.
In Proceedings of NAACL-HLT . pages 260–270.

Xuezhe Ma and Eduard Hovy. 2016. End-to-
end sequence labeling via bi-directional lstm-cnns-
crf. In Proceedings of the 54th Annual Meet-
ing of the Association for Computational Lin-
guistics (Volume 1: Long Papers). Association
for Computational Linguistics, Berlin, Germany,
pages 1064–1074. http://www.aclweb.org/
anthology/P16-1101.

Christopher D. Manning, Mihai Surdeanu, John Bauer,
Jenny Rose Finkel, Steven Bethard, and David Mc-
Closky. 2014. The stanford corenlp natural language
processing toolkit. In Proceedings of the 52nd An-
nual Meeting of the Association for Computational
Linguistics, ACL 2014, June 22-27, 2014, Baltimore,
MD, USA, System Demonstrations. pages 55–60.

Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey
Dean. 2013a. Efficient estimation of word represen-
tations in vector space. corr abs/1301.3781 (2013).

Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Cor-
rado, and Jeff Dean. 2013b. Distributed representa-
tions of words and phrases and their compositional-
ity. In Advances in neural information processing
systems. pages 3111–3119.

Syrene A Miller and Jane L Forrest. 2001. Enhanc-
ing your practice through evidence-based decision
making: PICO, learning how to ask good ques-
tions. Journal of Evidence Based Dental Practice
1(2):136–141.

SPFGH Moen and Tapio Salakoski2 Sophia Anani-
adou. 2013. Distributional semantics resources for
biomedical text processing.

Takeo Nakayama, Nobuko Hirai, Shigeaki Yamazaki,
and Mariko Naito. 2005. Adoption of structured ab-
stracts by general medical journals and format for a
structured abstract. Journal of the Medical Library
Association 93(2):237.

F. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel,
B. Thirion, O. Grisel, M. Blondel, P. Pretten-
hofer, R. Weiss, V. Dubourg, J. Vanderplas, A. Pas-
sos, D. Cournapeau, M. Brucher, M. Perrot, and
E. Duchesnay. 2011. Scikit-learn: Machine learning
in Python. Journal of Machine Learning Research
12:2825–2830.

W Scott Richardson, Mark C Wilson, Jim Nishikawa,
and Robert SA Hayward. 1995. The well-built clin-
ical question: a key to evidence-based decisions.
ACP journal club 123(3):A12–A12.

Ellen Riloff. 1995. Little words can make a big dif-
ference for text classification. In Proceedings of
the 18th annual international ACM SIGIR confer-
ence on Research and development in information
retrieval. ACM, pages 130–136.

Ellen Riloff. 1996. Automatically generating extrac-
tion patterns from untagged text. In Proceedings
of the Thirteenth National Conference on Artificial
Intelligence - Volume 2. AAAI Press, AAAI’96,
pages 1044–1049. http://dl.acm.org/
citation.cfm?id=1864519.1864542.

Ellen Riloff and William Phillips. 2004. An introduc-
tion to the sundance and autoslog systems. Techni-
cal report.

376



Abeed Sarker, Diego Mollá, and Cecile Paris.
2016. Query-oriented evidence extraction to sup-
port evidence-based medicine practice. Journal of
biomedical informatics 59:169–184.

Byron C. Wallace, Joël Kuiper, Aakash Sharma,
Mingxi (Brian) Zhu, and Iain James Marshall. 2016.
Extracting PICO sentences from clinical trial reports
using supervised distant supervision. Journal of
Machine Learning Research 17:132:1–132:25.

377


