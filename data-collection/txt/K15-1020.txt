



















































Making the Most of Crowdsourced Document Annotations: Confused Supervised LDA


Proceedings of the 19th Conference on Computational Language Learning, pages 194–203,
Beijing, China, July 30-31, 2015. c©2015 Association for Computational Linguistics

Making the Most of Crowdsourced Document Annotations:
Confused Supervised LDA

Paul Felt
Dept. of Computer Science
Brigham Young University

paul.lewis.felt@gmail.com

Eric K. Ringger
Dept. of Computer Science
Brigham Young University
ringger@cs.byu.edu

Jordan Boyd-Graber
Dept. of Computer Science

University of Colorado Boulder
Jordan.Boyd.Graber@colorado.edu

Kevin Seppi
Dept. of Computer Science
Brigham Young University
kseppi@byu.edu

Abstract

Corpus labeling projects frequently use
low-cost workers from microtask market-
places; however, these workers are often
inexperienced or have misaligned incen-
tives. Crowdsourcing models must be ro-
bust to the resulting systematic and non-
systematic inaccuracies. We introduce a
novel crowdsourcing model that adapts the
discrete supervised topic model sLDA to
handle multiple corrupt, usually conflict-
ing (hence “confused”) supervision sig-
nals. Our model achieves significant gains
over previous work in the accuracy of de-
duced ground truth.

1 Modeling Annotators and Abilities

Supervised machine learning requires labeled
training corpora, historically produced by labo-
rious and costly annotation projects. Microtask
markets such as Amazon’s Mechanical Turk and
Crowdflower have turned crowd labor into a com-
modity that can be purchased with relatively lit-
tle overhead. However, crowdsourced judgments
can suffer from high error rates. A common solu-
tion to this problem is to obtain multiple redundant
human judgments, or annotations,1 relying on the
observation that, in aggregate, non-experts often
rival or exceed experts by averaging over individ-
ual error patterns (Surowiecki, 2005; Snow et al.,
2008; Jurgens, 2013).

A crowdsourcing model harnesses the wisdom
of the crowd and infers labels based on the ev-
idence of the available annotations, imperfect

1In this paper, we call human judgments annotations to
distinguish them from gold standard class labels.

though they be. A common baseline crowd-
sourcing method aggregates annotations by ma-
jority vote, but this approach ignores important
information. For example, some annotators are
more reliable than others and their judgments
ought to be upweighted accordingly. State-of-
the-art crowdsourcing methods account for anno-
tator expertise, often through a probabilistic for-
malism. Compounding the challenge, assessing
unobserved annotator expertise is tangled with es-
timating ground truth from imperfect annotations;
thus, joint inference of these interrelated quantities
is necessary. State-of-the-art models also take the
data into account, because data features can help
ratify or veto human annotators.

We introduce a model that improves on state
of the art crowdsourcing algorithms by modeling
not only the annotations but also the features of
the data (e.g., words in a document). Section 2
identifies modeling deficiencies affecting previous
work and proposes a solution based on topic mod-
eling; Section 2.4 presents inference for the new
model. Experiments that contrast the proposed
model with select previous work on several text
classification datasets are presented in Section 3.
In Section 4 we highlight additional related work.

2 Latent Representations that Reflect
Labels and Confusion

Most crowdsourcing models extend the item-
response model of Dawid and Skene (1979). The
Bayesian version of this model, referred to here as
ITEMRESP, is depicted in Figure 1. In the gen-
erative story for this model, a confusion matrix
γj is drawn for each human annotator j. Each
row γjc of the confusion matrix γj is drawn from

194



θ

γjc

b(θ)
b(γ)

yd

adj

d = 1 . . . D

c = 1 . . . C

j = 1 . . . J

j = 1 . . . J

Figure 1: ITEMRESP as a plate diagram. Round
nodes are random variables. Rectangular nodes
are free parameters. Shaded nodes are observed.
D,J,C are the number of documents, annotators,
and classes, respectively.

a symmetric Dirichlet distribution Dir(b(γ)jc ) and
encodes a categorical probability distribution over
label classes that annotator j is apt to choose when
presented with a document whose true label is c.
Then for each document d an unobserved docu-
ment label yd is drawn. Annotations are generated
as annotator j corrupts the true label yd according
to the categorical distribution Cat(γjyd).

2.1 Leveraging Data
Some extensions to ITEMRESP model the features
of the data (e.g., words in a document). Many
data-aware crowdsourcing models condition the
labels on the data (Jin and Ghahramani, 2002;
Raykar et al., 2010; Liu et al., 2012; Yan et al.,
2014), possibly because discriminative classifiers
dominate supervised machine learning. Others
model the data generatively (Bragg et al., 2013;
Lam and Stork, 2005; Felt et al., 2014; Simp-
son and Roberts, 2015). Felt et al. (2015) argue
that generative models are better suited than condi-
tional models to crowdsourcing scenarios because
generative models often learn faster than their
conditional counterparts (Ng and Jordan, 2001)—
especially early in the learning curve. This advan-
tage is amplified by the annotation noise typical of
crowdsourcing scenarios.

Extensions to ITEMRESP that model document
features generatively tend to share a common
high-level architecture. After the document class
label yd is drawn for each document d, features are
drawn from class-conditional distributions. Felt et
al. (2015) identify the MOMRESP model, repro-
duced in Figure 2, as a strong representative of
generative crowdsourcing models. In MOMRESP,

θ

γjcφc

b(θ)

b(φ) b
(γ)

yd

xd adj

d = 1 . . . D

c = 1 . . . C

j = 1 . . . J

j = 1 . . . J

c = 1 . . . C

Figure 2: MOMRESP as a plate diagram. |xd| =
V , the size of the vocabulary. Documents with
similar feature vectors x tend to share a common
label y. Reduces to mixture-of-multinomials clus-
tering when no annotations a are observed.

the feature vector xd for document d is drawn from
the multinomial distribution with parameter vector
φyd . This class-conditional multinomial model of
the data inherits many of the strengths and weak-
nesses of the naı̈ve Bayes model that it resem-
bles. Strengths include easy inference and a strong
inductive bias which helps the model be robust
to annotation noise and scarcity. Weaknesses in-
clude overly strict conditional independence as-
sumptions among features, leading to overconfi-
dence in the document model and thereby caus-
ing the model to overweight feature evidence and
underweight annotation evidence. This imbalance
can result in degraded performance in the presence
of high quality or many annotations.

2.2 Confused Supervised LDA (CSLDA)

We solve the problem of imbalanced feature and
annotation evidence observed in MOMRESP by re-
placing the class-conditional structure of previous
generative crowdsourcing models with a richer
generative story where documents are drawn first
and class labels yd are obtained afterwards via a
log-linear mapping. This move towards condi-
tioning classes on documents content is sensible
because in many situations document content is
authored first, whereas label structure is not im-
posed until afterwards. It is plausible to assume
that there will exist some mapping from a latent
document structure to the desired document label
distinctions. Moreover, by jointly modeling top-
ics and the mapping to labels, we can learn the
latent document representations that best explain
how best to predict and correct annotator errors.

195



Term Definition
Nd Size of document d
Ndt

∑
n 1(zdn = t)

Nt
∑

d,n 1(zdn = t)
Njcc′

∑
d adjc′1(yd = c)

Njc 〈Njc1 · · ·NjcC〉
Nvt

∑
dn 1(wdn = v ∧ zdn = t)

Nt
∑

dn 1(zdn = t)
N̂ Count excludes variable being sampled
z̄d Vector where z̄dt = 1Nd

∑
n 1(zdn = t)

ˆ̄zd Excludes the zdn being sampled

Table 1: Definition of counts and select notation.
1(·) is the indicator function.

We call our model confused supervised LDA
(CSLDA, Figure 3), based on supervised topic
modeling. Latent Dirichlet Allocation (Blei et
al., 2003, LDA) models text documents as ad-
mixtures of word distributions, or topics. Al-
though pre-calculated LDA topics as features can
inform a crowdsourcing model (Levenberg et al.,
2014), supervised LDA (sLDA) provides a prin-
cipled way of incorporating document class la-
bels and topics into a single model, allowing topic
variables and response variables to co-inform one
another in joint inference. For example, when
sLDA is given movie reviews labeled with sen-
timent, inferred topics cluster around sentiment-
heavy words (Mcauliffe and Blei, 2007), which
may be quite different from the topics inferred by
unsupervised LDA. One way to view CSLDA is as
a discrete sLDA in settings with noisy supervision
from multiple, imprecise annotators.

The generative story for CSLDA is:

1. Draw per-topic word distributions φt from
Dir(b(θ)).

2. Draw per-class regression parameters ηc from
Gauss(µ,Σ).

3. Draw per-annotator confusion matrices γj
with row γjc drawn from Dir(b

(γ)
jc ).

4. For each document d,
(a) Draw topic vector θd from Dir(b(θ)).
(b) For each token position n, draw topic

zdn from Cat(θd) and word wdn from
Cat(φzdn).

(c) Draw class label yd with probability pro-
portional to exp[ηᵀyd z̄d].

(d) For each annotator j draw annotation
vector adj from γjyd .

θd

γjc

φt

ηc
b(θ)

b(φ)

b(γ)

µ

Σ

yd

zdn

wdn adj

d = 1 . . . D

n =
1 . . . Nd

c = 1 . . . C

j = 1 . . . J

c =
1 . . . C

j = 1 . . . J

t = 1 . . . T

Figure 3: CSLDA as a plate diagram. D,J,C, T
are the number of documents, annotators, classes,
and topics, respectively. Nd is the size of docu-
ment d. |φt| = V , the size of the vocabulary. ηc
is a vector of regression parameters. Reduces to
LDA when no annotations a are observed.

2.3 Stochastic EM

We use stochastic expectation maximization (EM)
for posterior inference in CSLDA, alternating be-
tween sampling values for topics z and document
class labels y (the E-step) and optimizing values
of regression parameters η (the M-step). To sam-
ple z and y efficiently, we derive the full condi-
tional distributions of z and y in a collapsed model
where θ, φ, and γ have been analytically integrated
out. Omitting multiplicative constants, the col-
lapsed model joint probability is

p(z, w, y, a|η) = p(z)p(w|z)p(y|z, η)p(a|y) (1)

∝M(a)·
(∏

d

B(Nd+b(θ))

)
·
(∏

t

B(Nt+b
(φ)
t )

)

·
(∏

d

exp(ηᵀyd z̄d)∑
c exp(η

ᵀ
c z̄d)

)
·
∏

j

∏
c

B(Njc+b
(γ)
jc )


where B(·) is the Beta function (multivariate as
necessary), counts N and related symbols are de-
fined in Table 1, and M(a) =

∏
d,jM(adj) where

M(adj) is the multinomial coefficient.
Simplifying Equation 1 yields full conditionals

for each word zdn,

p(zdn = t|ẑ, w, y, a, η) ∝
(
N̂dt + b

(θ)
t

)
(2)

· N̂wdnt + b
(φ)
wdn

N̂t + |b(φ)|1
· exp(

ηydt
Nd

)∑
c exp(

ηct
Nd

+ ηᵀc ˆ̄zd)
,

196



and similarly for document label yd:

p(yd = c|z, w, y, a, η) ∝ exp(η
ᵀ
c z̄d)∑

c′ exp(η
ᵀ
c′ z̄d)

(3)

·
∏
j

∏
c′

(
N̂jcc′ + b(γ)

)adjc′
(∑

c′ N̂jcc′ + b
(γ)
jcc′

)∑
c′ adjc′

,

where xk , x(x+ 1) · · · (x+ k − 1) is the rising
factorial. In Equation 2 the first and third terms
are independent of word n and can be cached at
the document level for efficiency.

For the M-step, we train the regression param-
eters η (containing one vector per class) by opti-
mizing the same objective function as for training
a logistic regression classifier, assuming that class
y is given:

p(y = c|z, η) =
∏
d

exp(ηᵀc z̄d)∑
c′ exp(η

ᵀ
c′ z̄d)

. (4)

We optimize the objective (Equation 4) using L-
BFGS and a regularizing Gaussian prior with µ =
0, σ2 = 1.

While EM is sensitive to initialization, CSLDA
is straightforward to initialize. Majority vote is
used to set initial y values ỹ. Corresponding initial
values for z and η are obtained by clamping y to ỹ
and running stochastic EM on z and η.

2.4 Hyperparameter Optimization

Ideally, we would test CSLDA performance under
all of the many algorithms available for inference
in such a model. Although that is not feasible,
Asuncion et al. (2009) demonstrate that hyperapa-
rameter optimization in LDA topic models helps
to bring the performance of alternative inference
algorithms into approximate agreement. Accord-
ingly, in Section 2.4 we implement hyperparame-
ter optimization for CSLDA to make our results as
general as possible.

Before moving on, however, we take a moment
to validate that the observation of Asuncion et al.
generalizes from LDA to the ITEMRESP model,
which, together with LDA, comprises CSLDA.
Figure 4 demonstrates that three ITEMRESP infer-
ence algorithms, Gibbs sampling (Gibbs), mean-
field variational inference (Var), and the iter-
ated conditional modes algorithm (ICM) (Besag,
1986), are brought into better agreement after opti-
mizing their hyperparameters via grid search. That

NEWSGROUPS

0.2

0.4

0.6

0.8

 0  5 10 15

Number of annotated instances x 1,000

A
c
c
u
ra

c
y

algorithm

Gibbs

Var

ICM

(a) Hyperparameters fixed
NEWSGROUPS

0.2

0.4

0.6

0.8

 0  5 10 15

Number of annotated instances x 1,000

A
c
c
u
ra

c
y

algorithm

Gibbs

Var

ICM

(b) Hyperparameters optimized via grid search on validation
data

Figure 4: Differences among the inferred label ac-
curacy learning curves of three ITEMRESP infer-
ence algorithms are reduced when hyperparame-
ters are optimized.

is, the algorithms in Figure 4b are in better agree-
ment, particularly near the extremes, than the algo-
rithms in Figure 4a. This difference is subtle, but it
holds to an equal and greater extent in other simu-
lation conditions we tested (experiment details are
similar to those reported in Section 3).

Fixed-point Hyperparameter Updates
Although a grid search is effective, it is not prac-
tical for a model with many hyperparameters such
as CSLDA. For efficiency, therefore, we use the
fixed-point updates of Minka (2000). Our up-
dates differ slightly from Minka’s since we tie hy-
perparameters, allowing them to be learned more
quickly from less data. In our implementation the
matrices of hyperparameters b(φ) and b(θ) over the
Dirichlet-multinomial distributions are completely
tied such that b(φ)tv = b

(φ)∀t, v and b(θ)t = b(θ)∀t.
This leads to

b(φ)←b(φ)·
∑

t,v[Ψ(Ntv+b
(φ))]−TVΨ(b(φ))

V [Ψ(Nt+V b(φ))−Ψ(V b(φ))]
(5)

and

b(θ)←b(θ)·
∑

d,t[Ψ(Ndt+b
(θ))]−NTΨ(b(θ))

T [Ψ(Nd+Tb(θ))−Ψ(Tb(θ))]
. (6)

The updates for b(γ) are slightly more involved
since we choose to tie the diagonal entries b(γ)d and
separately the off-diagonal entries b(γ)o , updating
each separately:

b
(γ)
d ←b(γ)d ·

∑
j,c[Ψ(Njcc+b

(γ)
d )]−JCΨ(b(γ)d )

Z(b(γ))
(7)

197



and b(γ)o ←

b(γ)o ·

∑
j,c,c′ 6=c

[Ψ(Njcc′+b(γ)o )]−JC(C−1)Ψ(b(γ)o )]

(C−1)Z(b(γ))
(8)

where

Z(b(γ)) =
∑
j,c

[Ψ(Njc + b
(γ)
d + (C − 1)b(γ)o )]

− JCΨ(b(γ)d + (C − 1)b(γ)o ).
As in the work of Asuncion et al. (2009), we add
an algorithmic gamma prior (b(·) ∼ G(α, β)) for
smoothing by adding α−1

b(·) to the numerator and β
to the denominator of Equations 5-8. Note that
these algorithmic gamma “priors” should not be
understood as first-class members of the CSLDA
model (Figure 3). Rather, they are regularization
terms that keep our hyperparameter search algo-
rithm from straying towards problematic values
such as 0 or∞.
3 Experiments

For all experiments we set CSLDA’s number of
topics T to 1.5 times the number of classes in each
dataset. We found that model performance was
reasonably robust to this parameter. Only when
T drops below the number of label classes does
performance suffer. As per Section 2.3, z and η
values are initialized with 500 rounds of stochas-
tic EM, after which the full model is updated with
1000 additional rounds. Predictions are generated
by aggregating samples from the last 100 rounds
(the mode of the approximate marginal posterior).

We compare CSLDA with (1) a majority vote
baseline, (2) the ITEMRESP model, and rep-
resentatives of the two main classes of data-
aware crowdsourcing models, namely (3) data-
generative and (4) data-conditional. MOMRESP
represents a typical data-generative model (Bragg
et al., 2013; Felt et al., 2014; Lam and Stork, 2005;
Simpson and Roberts, 2015). Data-conditional ap-
proaches typically model data features condition-
ally using a log-linear model (Jin and Ghahramani,
2002; Raykar et al., 2010; Liu et al., 2012; Yan et
al., 2014). For the purposes of this paper, we re-
fer to this model as LOGRESP. For ITEMRESP,
MOMRESP, and LOGRESP we use the variational
inference methods presented by Felt et al. (2015).
Unlike that paper, in this work we have augmented
inference with the in-line hyperparameter updates
described in Section 2.4.

WEATHER

0.6

0.7

0.8

0.9

1.0

 0  5 10 15

Number of annotations x 1,000

A
c
c
u
ra

c
y

algorithm

csLDA

MomResp

LogResp

ItemResp

Majority

Figure 5: Inferred label accuracy of models on
sentiment-annotated weather tweets.

3.1 Human-generated Annotations

To gauge the effectiveness of data-aware crowd-
sourcing models, we use the sentiment-annotated
tweet dataset distributed by CrowdFlower as a
part of its “data for everyone” initiative.2 In the
“Weather Sentiment” task, 20 annotators judged
the sentiment of 1000 tweets as either positive,
negative, neutral, or unrelated to the weather.
In the secondary “Weather Sentiment Evaluated”
task, 10 additional annotators judged the correct-
ness of each consensus label. We construct a
gold standard from the consensus labels that were
judged to be correct by 9 of the 10 annotators in
the secondary task.

Figure 5 plots learning curves of the accuracy
of model-inferred labels as annotations are added
(ordered by timestamp). All methods, including
majority vote, converge to roughly the same accu-
racy when all 20,000 annotations are added. When
fewer annotations are available, statistical mod-
els beat majority vote, and CSLDA is consider-
ably more accurate than other approaches. Learn-
ing curves are bumpy because annotation order is
not random and because inferred label accuracy is
calculated only over documents with at least one
annotation. Learning curves collectively increase
when average annotation depth (the number of an-
notations per item) increases and decrease when
new documents are annotated and average anno-
tation depth decreases. CSLDA stands out by be-
ing more robust to these changes than other algo-
rithms, and also by maintaining a higher level of
accuracy across the board. This is important be-
cause high accuracy using fewer annotations trans-
lates to decreased annotations costs.

2
http://www.crowdflower.com/data-for-everyone

198



D C V 1N
∑

dNd

20 News 16,995 20 22,851 111
WebKB 3,543 4 5,781 131
Reuters8 6,523 8 6,776 53
Reuters52 7,735 52 5,579 58
CADE12 34,801 12 41,628 110
Enron 3,854 32 14,069 431

Table 2: Dataset statistics. D is number of doc-
uments, C is number of classes, V is number of
features, and 1N

∑
dNd is average document size.

Values are calculated after setting aside 15% as
validation data and doing feature selection.

3.2 Synthetic Annotations

Datasets including both annotations and gold stan-
dard labels are in short supply. Although plenty
of text categorization datasets have been anno-
tated, common practice reflects that initial noisy
annotations be discarded and only consensus la-
bels be published. Consequently, we follow pre-
vious work in achieving broad validation by con-
structing synthetic annotators that corrupt known
gold standard labels. We base our experimen-
tal setup on the annotations gathered by Felt et
al. (2015),3 who paid CrowdFlower annotators to
relabel 1000 documents from the well-known 20
Newsgroups classification dataset. In that exper-
iment, 136 annotators contributed, each instance
was labeled an average of 6.9 times, and anno-
tator accuracies were distributed approximately
according to a Beta(3.6, 5.1) distribution. Ac-
cordingly we construct 100 synthetic annotators,
each parametrized by an accuracy drawn from
Beta(3.6, 5.1) and with errors drawn from a sym-
metric Dirichlet Dir(1). Datasets are annotated
by selecting an instance (at random without re-
placement) and then selecting K annotators (at
random without replacement) to annotate it before
moving on. We choose K = 7 to mirror the em-
pirical average in the CrowdFlower annotation set.

We evaluate on six text classification datasets,
summarized in Table 2. The 20 Newsgroups, We-
bKB, Cade12, Reuters8, and Reuters52 datasets
are described in more detail by Cardoso-Cachopo
(2007). The LDC-labeled Enron emails dataset is
described by Berry et al. (2001). Each dataset is

3The dataset is available via git at git://nlp.cs.
byu.edu/plf1/crowdflower-newsgroups.git

NEWSGROUPS

0.80

0.85

0.90

0.95

1.00

  0  25  50  75 100 125

Number of annotations x 1,000

A
c
c
u
ra

c
y

algorithm

csLDA

MomResp

LogResp

ItemResp

Majority

WEBKB

0.6

0.7

0.8

0.9

 0 10 20

Number of annotations x 1,000

A
c
c
u
ra

c
y

algorithm

csLDA

MomResp

LogResp

ItemResp

Majority

CADE12

0.6

0.7

0.8

0.9

  0  50 100 150 200

Number of annotations x 1,000

A
c
c
u
ra

c
y

algorithm

csLDA

MomResp

LogResp

ItemResp

Majority

Figure 6: Inferred label accuracy of models on
synthetic annotations. The first instance is anno-
tated 7 times, then the second, and so on.

preprocessed via Porter stemming and by removal
of the stopwords from MALLET’s stopword list
(McCallum, 2002). Features occurring fewer than
5 times in the corpus are discarded. In the case
of MOMRESP, features are fractionally scaled so
that each document is the same length, in keep-
ing with previous work in multinomial document
models (Nigam et al., 2006).

Figure 6 plots learning curves on three repre-
sentative datasets (Enron resembles Cade12, and
the Reuters datasets resemble WebKB). CSLDA
consistently outperforms LOGRESP, ITEMRESP,
and majority vote. The generative models
(CSLDA and MOMRESP) tend to excel in low-
annotation portions of the learning curve, par-
tially because generative models tend to converge
quickly and partially because generative models
naturally learn from unlabeled documents (i.e.,
semi-supervision). However, MOMRESP tends to
quickly reach a performance plateau after which
additional annotations do little good. The perfor-
mance of MOMRESP is also highly dataset de-

199



95% Accuracy CSLDA MOMRESP LOGRESP ITEMRESP Majority
20 News 85 (5.0x) 150 (8.8x) 152 (8.9x) 168 (9.9x) 233 (13.7x)
WebKB 31 (8.8x) - 46 (13.0x) 46 (13.0x) -
Reuters8 25 (3.8x) - 73 (11.2x) 62 (9.5x) -
Reuters52 33 (4.3x) 73 (9.4x) 67.5 (8.7x) 60 (7.8x) 87 (11.2x)
CADE12 250 (7.2x) - 295 (8.5x) 290 (8.3x) 570 (16.4x)
Enron 31 (8.0x) - 40 (10.4x) 38 (9.9x) 47 (12.2x)

Table 3: The number of annotations ×1000 at which the algorithm reaches 95% inferred label accuracy
on the indicated dataset (average annotations per instance are in parenthesis). All instances are annotated
once, then twice, and so on. Empty entries (’-’) do not reach 95% even with 20 annotations per instance.

pendent: it is good on 20 Newsgroups, mediocre
on WebKB, and poor on CADE12. By contrast,
CSLDA is relatively stable across datasets.

To understand the different behavior of the two
generative models, recall that MOMRESP is iden-
tical to ITEMRESP save for its multinomial data
model. Indeed, the equations governing infer-
ence of label y in MOMRESP simply sum together
terms from an ITEMRESP model and terms from
a mixture of multinomials clustering model (and
for reasons explained in Section 2.1, the multino-
mial data model terms tend to dominate). There-
fore when MOMRESP diverges from ITEMRESP
it is because MOMRESP is attracted toward a y as-
signment that satisfies the multinomial data model,
grouping similar documents together. This can
both help and hurt. When data clusters and la-
bel classes are misaligned, MOMRESP falters (as
in the case of the Cade12 dataset). In contrast,
CSLDA’s flexible mapping from topics to labels
is less sensitive: topics can diverge from label
classes so long as there exists some linear trans-
formation from the topics to the labels.

Many corpus annotation projects are not com-
plete until the corpus achieves some target level of
quality. We repeat the experiment reported in Fig-
ure 6, but rather than simulating seven annotations
for each instance before moving on, we simulate
one annotation for each instance, then two, and so
on until each instance in the dataset is annotated
20 times. Table 3 reports the minimal number of
annotations before an algorithm’s inferred labels
reach an accuracy of 95%, a lofty goal that can re-
quire significant amounts of annotation when us-
ing poor quality annotations. CSLDA achieves
95% accuracy with fewer annotations, correspond-
ing to reduced annotation cost.

NEWSGROUPS

0.70

0.75

0.80

0.85

0.90

20 30 40 50 60

Number of annotations x 1,000

A
c
c
u
ra

c
y algorithm

csLDA

csLDA−P

Figure 7: Joint inference for CSLDA vs pipeline
inference (CSLDA-P).

3.3 Joint vs Pipeline Inference

To isolate the effectiveness of joint inference in
CSLDA, we compare against the pipeline alterna-
tive where topics are inferred first and then held
constant during inference (Levenberg et al., 2014).
Joint inference yields modest but consistent bene-
fits over a pipeline approach. Figure 7 highlights
a portion of the learning curve on the Newsgroups
dataset (based on the experiments summarized in
Table 3). This trend holds across all of the datasets
that we examined.

3.4 Error Analysis

Class-conditional models like MOMRESP include
a feature that data-conditional models like CSLDA
lack: an explicit prior over class prevalence. Fig-
ure 8a shows that CSLDA performs poorly on the
CrowdFlower-annotated Newsgroups documents
described at the beginning of Section 3 (not the
synthetic annotations). Error analysis uncovers
that CSLDA lumps related classes together in this
dataset. This is because annotators could specify
up to 3 simultaneous labels for each annotation,
so that similar labels (e.g., “talk.politics.misc”
and “talk.politics.mideast”) are usually chosen in
blocks. Suppose each member of a set of doc-
uments with similar topical content is annotated

200



CFGROUPS1000

0.4

0.5

0.6

0.7

 0  2  4  6

Number of annotated instances x 1,000

A
c
c
u
ra

c
y

algorithm

csLDA

MomResp

LogResp

ItemResp

Majority

(a) Original data
CFSIMPLEGROUPS

0.75

0.80

0.85

0.90

0.95

 0  2  4  6

Number of annotated instances x 1,000

A
c
c
u
ra

c
y

algorithm

csLDA

MomResp

LogResp

ItemResp

Majority

(b) After combining frequently co-annotated label classes

Figure 8: An illustrative failure case. CSLDA,
lacking a class label prior, prefers to combine label
classes that are highly co-annotated.

with both label A and B. In this scenario it is ap-
parent that CSLDA will achieve its best fit by in-
ferring all documents to have the same label either
A or B. By contrast, MOMRESP’s uniform prior
distribution over θ leads it to prefer solutions with
a balance of A and B.

The hypothesis that class combination explains
CSLDA’s performance is supported by Figure 8b,
which shows that CSLDA recovers after com-
bining the classes that were most frequently co-
annotated. We greedily combine label class pairs
to maximize Krippendorf’s α until only 10 la-
bels were left: “alt.atheism,” religion, and poli-
tics classes were combined; also, “sci.electronics”
and the computing classes. The remaining eight
classes were unaltered. However, one could also
argue that the original behavior of CSLDA is in
some ways desirable. That is, if two classes of
documents are mostly the same both topically and
in terms of annotator decisions, perhaps those
classes ought to be collapsed. We are not overly
concerned that MOMRESP beats CSLDA in Fig-
ure 8, since this result is consistent with early rel-
ative performance in simulation.

4 Additional Related Work

This section reviews related work not already dis-
cussed. A growing body of work extends the item-
response model to account for variables such as
item difficulty (Whitehill et al., 2009; Passonneau

and Carpenter, 2013; Zhou et al., 2012), anno-
tator trustworthiness (Hovy et al., 2013), corre-
lation among various combinations of these vari-
ables (Zhou et al., 2014), and change in annotator
behavior over time (Simpson and Roberts, 2015).

Welinder et al. (2010) carefully model the pro-
cess of annotating objects in images, including
variables for item difficulty, item class, and class-
conditional perception noise. In follow-up work,
Liu et al. (2012) demonstrate that similar levels
of performance can be achieved with the sim-
ple item-response model by using variational in-
ference rather than EM. Alternative inference al-
gorithms have been proposed for crowdsourcing
models (Dalvi et al., 2013; Ghosh et al., 2011;
Karger et al., 2013; Zhang et al., 2014). Some
crowdsourcing work regards labeled data not as an
end in itself, but rather as a means to train clas-
sifiers (Lin et al., 2014). The fact-finding litera-
ture assigns trust scores to assertions made by un-
trusted sources (Pasternack and Roth, 2010).

5 Conclusion and Future Work

We describe CSLDA, a generative, data-aware
crowdsourcing model that addresses important
modeling deficiencies identified in previous work.
In particular, CSLDA handles data in which the
natural document clusters are at odds with the
intended document labels. It also transitions
smoothly from situations in which few annotations
are available to those in which many annotations
are available. Because of the flexible mapping in
CSLDA to class labels, many structural variants
are possible in future work. For example, this
mapping could depend not just on inferred topi-
cal content but also directly on data features (c.f.
Nguyen et al. (2013)) or learned embedded feature
representations.

The large number of parameters in the learned
confusion matrices of crowdsourcing models
present difficulty at scale. This could be addressed
by modeling structure both inside of the annotators
and classes. Redundant annotations give unique
insights into both inter-annotator and inter-class
relationships and could be used to induce anno-
tator or label class hierarchies with parsimonious
representations. Simpson et al. (2013) identify an-
notator clusters using community detection algo-
rithms but do not address annotator hierarchy or
scalable confusion representations.

201



Acknowledgments This work was supported by
the collaborative NSF Grant IIS-1409739 (BYU)
and IIS-1409287 (UMD). Boyd-Graber is also
supported by NSF grants IIS-1320538 and NCSE-
1422492. Any opinions, findings, conclusions, or
recommendations expressed here are those of the
authors and do not necessarily reflect the view of
the sponsor.

References
Arthur Asuncion, Max Welling, Padhraic Smyth, and

Yee Whye Teh. 2009. On smoothing and inference
for topic models. In Proceedings of Uncertainty in
Artificial Intelligence.

Michael W. Berry, Murray Browne, and Ben Signer.
2001. Topic annotated Enron email data set. Lin-
guistic Data Consortium.

Julian Besag. 1986. On the statistical analysis of dirty
pictures. Journal of the Royal Statistical Society,
48(3):259–302.

David M. Blei, Andrew Y. Ng, and Michael I. Jordan.
2003. Latent Dirichlet allocation. Journal of Ma-
chine Learning Research, 3:993–1022.

Jonathan Bragg, Mausam, and Daniel S. Weld. 2013.
Crowdsourcing multi-label classification for taxon-
omy creation. In First AAAI Conference on Human
Computation and Crowdsourcing.

Ana Margarida de Jesus Cardoso-Cachopo. 2007. Im-
proving Methods for Single-label Text Categoriza-
tion. Ph.D. thesis, Universidade Tecnica de Lisboa.

Nilesh Dalvi, Anirban Dasgupta, Ravi Kumar, and Vib-
hor Rastogi. 2013. Aggregating crowdsourced bi-
nary ratings. In Proceedings of World Wide Web
Conference.

Alexander P. Dawid and Allan M. Skene. 1979. Max-
imum likelihood estimation of observer error-rates
using the EM algorithm. Applied Statistics, pages
20–28.

Paul Felt, Robbie Haertel, Eric Ringger, and Kevin
Seppi. 2014. MomResp: A Bayesian model for
multi-annotator document labeling. In International
Language Resources and Evaluation.

Paul Felt, Eric Ringger, Kevin Seppi, and Robbie Haer-
tel. 2015. Early gains matter: A case for preferring
generative over discriminative crowdsourcing mod-
els. In Conference of the North American Chapter
of the Association for Computational Linguistics.

Arpita Ghosh, Satyen Kale, and Preston McAfee.
2011. Who moderates the moderators?: crowd-
sourcing abuse detection in user-generated content.
In Proceedings of the 12th ACM conference on Elec-
tronic commerce.

Dirk Hovy, Taylor Berg-Kirkpatrick, Ashish Vaswani,
and Eduard H. Hovy. 2013. Learning whom to trust
with MACE. In Conference of the North American
Chapter of the Association for Computational Lin-
guistics.

Rong Jin and Zoubin Ghahramani. 2002. Learning
with multiple labels. In Proceedings of Advances in
Neural Information Processing Systems, pages 897–
904.

David Jurgens. 2013. Embracing ambiguity: A
comparison of annotation methodologies for crowd-
sourcing word sense labels. In Proceedings of
NAACL-HLT, pages 556–562.

David R. Karger, Sewoong Oh, and Devavrat Shah.
2013. Efficient crowdsourcing for multi-class label-
ing. In ACM SIGMETRICS Performance Evaluation
Review, volume 41, pages 81–92. ACM.

Chuck P. Lam and David G. Stork. 2005. Toward
optimal labeling strategy under multiple unreliable
labelers. In AAAI Spring Symposium: Knowledge
Collection from Volunteer Contributors.

Abby Levenberg, Stephen Pulman, Karo Moilanen,
Edwin Simpson, and Stephen Roberts. 2014.
Predicting economic indicators from web text us-
ing sentiment composition. International Jour-
nal of Computer and Communication Engineering,
3(2):109–115.

Christopher H. Lin, Mausam, and Daniel S. Weld.
2014. To re (label), or not to re (label). In Sec-
ond AAAI Conference on Human Computation and
Crowdsourcing.

Qiang Liu, Jian Peng, and Alex T. Ihler. 2012. Varia-
tional inference for crowdsourcing. In Proceedings
of Advances in Neural Information Processing Sys-
tems.

Jon D. Mcauliffe and David Blei. 2007. Supervised
topic models. In Proceedings of Advances in Neural
Information Processing Systems.

Andrew McCallum. 2002. Mallet: A machine learning
for language toolkit. http://mallet.cs.umass.edu.

Thomas Minka. 2000. Estimating a Dirichlet distribu-
tion.

Andrew Y. Ng and Michael I. Jordan. 2001. On dis-
criminative vs. generative classifiers: A comparison
of logistic regression and Naive Bayes. Proceedings
of Advances in Neural Information Processing Sys-
tems.

Viet-An Nguyen, Jordan L. Boyd-Graber, and Philip
Resnik. 2013. Lexical and hierarchical topic regres-
sion. In Proceedings of Advances in Neural Infor-
mation Processing Systems.

Kamal Nigam, Andrew McCallum, and Tom Mitchell.
2006. Semi-supervised text classification using EM.
Semi-Supervised Learning, pages 33–56.

202



Rebecca J. Passonneau and Bob Carpenter. 2013. The
benefits of a model of annotation. In Proceedings of
the 7th Linguistic Annotation Workshop and Inter-
operability with Discourse, pages 187–195.

Jeff Pasternack and Dan Roth. 2010. Knowing what
to believe (when you already know something). In
Proceedings of International Conference on Compu-
tational Linguistics.

Vikas C. Raykar, Shipeng Yu, Linda H. Zhao, Ger-
ardo Hermosillo Valadez, Charles Florin, Luca Bo-
goni, and Linda Moy. 2010. Learning from crowds.
Journal of Machine Learning Research, 11:1297–
1322.

Edwin Simpson and Stephen Roberts. 2015. Bayesian
methods for intelligent task assignment in crowd-
sourcing systems. In Decision Making: Uncer-
tainty, Imperfection, Deliberation and Scalability,
pages 1–32. Springer.

E. Simpson, S. Roberts, I. Psorakis, and A. Smith.
2013. Dynamic bayesian combination of multiple
imperfect classifiers. In Decision Making and Im-
perfection, pages 1–35. Springer.

Rion Snow, Brendan O’Connor, Daniel Jurafsky, and
Andrew Y. Ng. 2008. Cheap and fast—but is it
good?: Evaluating non-expert annotations for natu-
ral language tasks. In Proceedings of EMNLP. ACL.

James Surowiecki. 2005. The Wisdom of Crowds.
Random House LLC.

Peter Welinder, Steve Branson, Pietro Perona, and
Serge J. Belongie. 2010. The multidimensional wis-
dom of crowds. In NIPS, pages 2424–2432.

Jacob Whitehill, Ting-fan Wu, Jacob Bergsma, Javier R
Movellan, and Paul L. Ruvolo. 2009. Whose
vote should count more: Optimal integration of la-
bels from labelers of unknown expertise. NIPS,
22:2035–2043.

Yan Yan, Rómer Rosales, Glenn Fung, Ramanathan
Subramanian, and Jennifer Dy. 2014. Learn-
ing from multiple annotators with varying expertise.
Machine Learning, 95(3):291–327.

Yuchen Zhang, Xi Chen, Dengyong Zhou, and
Michael I. Jordan. 2014. Spectral methods meet
em: A provably optimal algorithm for crowd-
sourcing. In Advances in Neural Information Pro-
cessing Systems 27, pages 1260–1268. Curran As-
sociates, Inc.

Dengyong Zhou, Sumit Basu, Yi Mao, and John C.
Platt. 2012. Learning from the wisdom of crowds
by minimax entropy. In NIPS, volume 25, pages
2204–2212.

Dengyong Zhou, Qiang Liu, John Platt, and Christo-
pher Meek. 2014. Aggregating ordinal labels from
crowds by minimax conditional entropy. In Pro-
ceedings of the International Conference of Machine
Learning.

203


