



















































GTI: An Unsupervised Approach for Sentiment Analysis in Twitter


Proceedings of the 9th International Workshop on Semantic Evaluation (SemEval 2015), pages 533–538,
Denver, Colorado, June 4-5, 2015. c©2015 Association for Computational Linguistics

GTI: An Unsupervised Approach for Sentiment Analysis in Twitter

Milagros Fernández-Gavilanes, Tamara Álvarez-López, Jonathan Juncal-Martı́nez,
Enrique Costa-Montenegro, Francisco Javier González-Castaño

GTI Research Group
AtlantTIC Centre, School of Telecommunication Engineering, University of Vigo

36310 Vigo, Spain
{milagros.fernandez, talvarez, jonijm, kike}@gti.uvigo.es,

javier@det.uvigo.es

Abstract

This paper presents the approach of the GTI
Research Group to SemEval-2015 task 10 on
Sentiment Analysis in Twitter, or more specif-
ically, subtasks A (Contextual Polarity Disam-
biguation) and B (Message Polarity Classifi-
cation). We followed an unsupervised depen-
dency parsing-based approach using a senti-
ment lexicon, created by means of an auto-
matic polarity expansion algorithm and Nat-
ural Language Processing techniques. These
techniques involve the use of linguistic pe-
culiarities, such as the detection of polar-
ity conflicts or adversative/concessive subor-
dinate clauses. The results obtained confirm
the competitive and robust performance of the
system.

1 Introduction

The domain of sentiment analysis has received in-
creasing attention in recent years (Liu, 2012), partic-
ularly due to the growth of the Internet and content
generated by users of social networks and other plat-
forms. Some of these, such as Twitter, allow people
to express their opinions using colloquial, compact
language. The result is a new form of expression
that may in the long term become a source of ex-
tremely valuable information. An increasing num-
ber of companies are now focusing their market-
ing campaigns on online comments, sentiments, and
opinions of brands from clients or potential clients,
and some are even trying to predict the acceptance
and rejection of certain products using this informa-
tion (Jansen et al., 2009).

Even though the approaches used for this pur-
pose are numerous and varied, they can be
broadly divided into two categories: supervised
machine-learning and unsupervised semantic-based
approaches. The former are often classifiers built
from features of a “bag of words” representation (Hu
and Liu, 2004; Pak et al., 2010). In other words,
they consist of automatically analyzing n-grams in
search of recurrent combinations of opinion words.
The latter aim at capturing and modeling linguistic
knowledge through the use of dictionaries (Taboada
et al., 2011) containing words that are tagged with
their semantic orientation. These methods detect the
words present in a text using different strategies in-
volving lexics, syntax or semantics (Quinn et al.,
2010) and then aggregate their values. Such meth-
ods usually combine two or more levels of analysis.

In recent years, work on sentiment classification
using different types of texts has shown that special-
ized methods are required. For example, emotions
are not conveyed in the same manner in newspaper
articles as in blogs, reviews, forums or other types
of user-generated content (Balahur, 2013). Dealing
with sentiment in Twitter, thus, requires an analy-
sis of the characteristics of tweets and the design of
adapted methods.

This paper presents a method for sentiment analy-
sis in English that uses dependency parsing to deter-
mine the polarity of tweets, using a previously cre-
ated sentiment lexicon and considering the special
structure and linguistic content of these postings.

The remainder of this article is structured as fol-
lows: Section 2 provides a brief description of the
task and some of its subtasks. Section 3 presents in

533



detail the system proposed for the performance of
these tasks, and Section 4 shows the results obtained
and discusses them. Finally, Section 5 summarizes
the main findings and conclusions.

2 Task Description

This paper describes our contribution to the
SemEval-2015 Task 10: Sentiment Analysis in Twit-
ter. Of the five subtasks established, we participated
in two:

• Contextual Polarity Disambiguation (A), on
determining the polarity of a marked instance
of a word or phrase in the context of a given
message.

• Message Polarity Classification (B), on clas-
sifying the content of a whole message.

This year there were two datasets for testing can-
didate systems for substasks A and B: The Official
2015 Test and a Progress Test. The first test con-
sisted of a set of Twitter messages (Rosenthal et al.,
2015) whilst the second test was a rerun of SemEval-
2014 Task 9 (Rosenthal et al., 2014), which includes
Twitter messages and other kinds of texts from dif-
ferent domains. Datasets formed by the datasets
given in SemEval-2013 Task 2 (Nakov et al., 2013)
were also provided for training and development. In
our case, the approach does not involve any training,
and all the datasets were used to test the behavior of
our system.

3 System Overview

The main objective of the tasks was to detect
whether a marked instance of word/phrase in a given
context (A) or message (B) expresses positive, nega-
tive or neutral sentiment. Most learning- or lexicon-
based systems do not usually take into account re-
lations between words, although they try to simu-
late comprehension of some linguistic constructions,
such as negation, but this does not always work cor-
rectly due to the complexity of human language. For
this reason, in this paper, we propose an alternative
system to exploit the information present in depen-
dencies obtained from a parsing analysis, without
the need for any kind of training. The research we

describe in this section has several linguistic pecu-
liarities that were used to improve sentiment detec-
tion performance. Our method, which was fully un-
supervised, consisted of four stages, which are each
explained in detail below.

3.1 Preprocessing

Working with tweets presents several challenges for
natural language processing. The language used on
social media sites is quite different from that used
in other forums because it often contains words that
are not found in a dictionary. One reason is that
tweets have particular orthographic and typograph-
ical characteristics, such as letter or word duplica-
tion. Hence, before applying our approach, it was
necessary to start with a data preprocessing stage to
normalize the language used, remove noisy elements
and generalize the vocabulary used to express senti-
ment. The aim of the preprocessing module is to
bring tweets as close as possible to natural language
by eliminating expressions that are not considered
part of current usage, in order to minimize the noise
in later stages. There are four main steps involved:

• URL links (such as “http://url”), hashtags links
(such as “#hashtag”) and username links (such
as “@username”) are replaced with “URL”,
“HASHTAG” and “USERNAME” placehold-
ers respectively.

• Replicated characters are removed to return the
word to its normal form, such as sweeeeet →
sweet.

• Emoticons1 are replaced by one of nine
labels: e laugh, e happy, e surprise,
e positive state, e neutral state,
e inexpressive, e negative state,
e sad and e sick. For instance, :-( is replaced
with e sad.

• Abbreviations2 are replaced with their respec-
tive full written forms, such as h8→ hate.

1taken from the list available at
http://www.datagenetics.com/blog/october52012/index.html

2taken from the lists available at
http://chatslang.com/terms/abbreviations.

534



3.2 Lexical and syntactic analysis

In order to derive the syntactic context, each pre-
processed social media message must first be bro-
ken into tokens and then into sentences. To then en-
sure that all inflected forms of a word are covered,
lemmatization and part-of-speech (POS) tagging are
performed using the Freeling Tagger (Atserias et
al., 2006; Padró et al., 2012), or more specifically,
its tagger implementation based on HMM (Brants,
2000). Freeling is a library that provides multiple
language analysis services, including probabilistic
prediction of categories of unknown words. POS
tagging allows the identification of lexical items that
can contribute to the correct recognition of senti-
ment in message. These items are namely adjectives,
adverbs, verbs and nouns.

The resulting lemmatized and POS-annotated
messages are fed to a parser that transforms the out-
put of the tagger into a full parse tree. Finally, the
tree is converted to dependencies, and the functions
are annotated. The entire process is performed with
Freeling Parser (Padró et al., 2012).

3.3 Sentiment lexicons

Sentiment lexicons, such as SOCAL (Taboada et al.,
2011), AFINN (Nielsen et al., 2011) and NRC Emo-
tion and Hashtag Sentiment Lexicon (Mohammad et
al., 2013; Mohammad et al., 2013b), have been used
in many systems for determining the semantic orien-
tation of a phrase within a tweet or sentence. These
lexicons contain English word lists sorted by lexical
category, i.e. adjectives, verbs, nouns and adverbs.
Each word is assigned a score of between -5 and 5.

However, these lexical resources are intrinsically
non-contextual, so it is necessary to improve their
coverage. To do this, we need to acquire new
polarities of subjective words that are not present
in generic dictionaries and adapt the scores of the
other words using the data available. Consequently,
we apply an automatic polarity expansion algorithm
based on graphs (Cruz et al., 2011). The graph is
generated from the syntactic dependencies provided
by the Freeling Parser, considering only those in-
volving verbs, nouns and adjectives. The starting
point of the algorithm is a subset of negative and
positive words, that are fed into the system as seed
words. In this regard, we chose the most negative

and positive words in the SOCAL and AFINN lex-
icons, as they resulted to work quite well for the
datasets provided, after carrying out different experi-
ments through the training datasets. Then, we apply
the iterative polarity expansion through the created
graph, and the result is merged with the unique word
list of SOCAL/AFINN lexicons, incorporating 5982
of new words. The next step is to include emoticon
labels, together with their polarities, in the resulting
sentiment lexicon.

3.4 Sentiment Detection

Once the lexical and syntactic analyses are com-
plete, it is possible to estimate the polarity result-
ing from a message. In other words, its sentiment
can be expressed by a real number, which can be
later interpreted as positive, negative or neutral. This
value is computed by using the lexical polarities of
the words included in the text (provided by the senti-
ment lexicon we have generated), and subjecting the
special parsing structure and its content to linguis-
tic processing which is described below. Once these
have been applied, the resulting sentiment is a prop-
agation of the values of linguistic elements within
the dependencies, from the leaves to the upper lev-
els until the root is achieved (Caro, 2013). Then, it
is classified according to defined interval.

3.4.1 Intensification treatment
Intensifiers and diminishers, such as “very” or “a

little”, are usually adverbs that emphasize or attenu-
ate the semantic orientation of the words or expres-
sions they precede. Intensification is achieved by
associating a positive or negative percentage, which
implies a graduation depending on its type (Zhang
et al., 2012). For instance, in “very good”, “very”
enhances the positivity of “good”. Our system de-
tects these structures and uses the parsing to identify
the exact scope of the intensification whose seman-
tic orientation will be altered. Superlative adjectives
are also taken into account by asuming that they be-
have like a word accompanied by an intensifier. An
example is “greatest”, where the superlative implies
an intensification of the word “great”.

3.4.2 Negation treatment
Negation can be used to deny or reject statements.

It is expressed grammatically through a variety of

535



negator words, such as “no”, “not”, “never” or “nei-
ther” (Zhang et al., 2012). In our case, it is first
necessary to identify the dependencies in which any
of the above negator forms are present to estimate
the negation scope. Later, the semantic polarities of
the words involved in the affected dependencies are
modified using a negative factor.

3.4.3 Polarity conflict treatment
The mere application of polar lexicons, intensi-

fiers, diminishers and negators on a syntactic struc-
ture is insufficient. That is, words cannot be con-
sidered individually (Moilanen et al., 2007). The
meaning and polarity of “unpleasant dream” differs
for example from those of “wonderful dream”. The
first statement has a negative connotation while the
second has a positive one. In both cases, the word
“dream” is involved, and we could expect that, re-
gardless of its accompanying terms, it should behave
in a specific way, with certain polarity effects or ex-
pectations. However, the meaning changes signifi-
cantly with the addition of “unpleasant” or “wonder-
ful”. In these cases, our system is able to detect po-
larity conflicts, i.e., it recognizes when a positive ad-
jective modifies a negative noun, or vice-versa, and
subsequently reduces the polarity of the elements
that cause the conflict.

3.4.4 Adversative/concessive clause treatment
There is a point in common between adversative

and concessive subordinate clauses. While the for-
mer express an objection in compliance with what is
said in the main clause, the latter express a difficulty
in fulfilling the main clause, although it is not im-
possible. In both cases, one part of the sentence is in
contrast with the other part. For this reason, in a con-
text of sentiment analysis, we can assume that both
constructions will restrict, exclude, amplify or di-
minish the sentiment reflected in the clauses. In this
regard, it is necessary to clearly distinguish them. In
an adversative structure, the argument introduced by
items such as “but” or “however” is usually more im-
portant (Winterstein, 2012; Poria et al., 2014), while
in a concessive structure, that introduced by items
such as “despite” or “in spite of” is the least impor-
tant (Rudolph, 1996).

Our approach is able to coherently estimate the
sentiment of sentences that involve not only adver-

sative clauses, such as “Bill Maher may be a little
out there, but he does make some points” (where the
speaker is backing the view of Bill in general), but
also concessive clauses such as “Despite going off
on Saturday, it looks like Ian Bennett could be fine
for Wembley” (where what appears to be really im-
portant is that Ian could go to Wembley).

4 Experimental Results

In this section we describe the experiments we
conducted for both subtasks. These experiments
were carried out using the datasets provided by the
SemEval-2015 task organizers. These datasets are
composed of texts extracted from Twitter (including
sarcastic tweets), LiveJournal and phone text mes-
sages. The performance of each system is measured
by means of the F-score, calculated as shown in
Equation 1,

F-score = (FP + FN )/2 (1)

where FP stands for the F-score estimated only for
positive results. In this case, this value is computed
as shown in Equation 2, where PP represents the
precision and RP the recall, both for positive results.
The same is calculated for negative results, denomi-
nated FN .

FP = (2 ∗ PP ∗RP )/(PP + RP ) (2)
Table 1 presents the overall score for subtasks A

and B, in Twitter2015 Test, as well as precision, re-
call and F-measure values for positive (P), negative
(N) and neutral (NEU) results.

Twitter 2015
Precision Recall F-score

Task A

P 87.33% 71.26% 78.48%
N 80.02% 72.47% 76.06%

NEU 10.25% 34.21% 15.78%
Overall score: 77.27%

Task B

P 72.13% 66.09% 68.98%
N 41.57% 59.45% 48.93%

NEU 61.72% 57.35% 59.45%
Overall score: 58.95%

Table 1: Results of our approach for subtasks A and B.

The approach previously described was applied
on both datasets (A and B) in the same way using the

536



TASK A TASK B
LJ’14 SMS’13 T’13 T’14 TS’14 LJ’14 SMS’13 T’13 T’14 TS’14

P 84.65% 82.16% 91.41% 93.23% 88.75% 73.53% 56.57% 68.54% 76.27% 52.17%
Precision N 85.04% 92.64% 87.53% 78.86% 95.83% 74.52% 58.47% 54.56% 51.94% 87.50%

NEU 31.62% 16.55% 7.76% 9.79% 10.00% 62.00% 81.64% 67.69% 60.03% 37.50%
P 76.06% 83.85% 81.38% 76.26% 86.59% 70.26% 71.75% 73.73% 70.06% 72.73%

Recall N 81.21% 79.80% 79.23% 71.63% 62.16% 64.47% 70.05% 59.73% 63.34% 35.00%
NEU 51.39% 30.19% 29.38% 52.27% 40.00% 71.05% 67.44% 60.43% 62.18% 69.23%

P 80.13% 82.99% 86.11% 83.90% 87.65% 71.86% 63.26% 71.04% 73.04% 60.76%
F-score N 83.08% 85.74% 83.17% 75.07% 75.41% 69.14% 63.74% 57.03% 58.26% 50.00%

NEU 39.15% 21.38% 12.27% 16.49% 16.00% 66.21% 73.87% 63.85% 61.09% 48.65%
Overall 81.61% 84.37% 84.64% 79.48% 81.53% 70.50% 63.50% 64.03% 65.65% 55.38%

Table 2: Performance of our approach on the progress test A and B.

generated sentiment lexicon and applying the propa-
gation of the sentiment values within the dependen-
cies. After performing several tests on the training
datasets provided by organizers, we set the neutral
sentiment intervals to [−0.05, 0.05] for subtask A
and [−1.0, 1.0] for subtask B.

As can be seen, all our results are adjusted, so we
can state that our system has no bias for one particu-
lar result, but performs quite well for all three types
of answers. However, as can be seen in subtask A,
the performance measures for neutral tweets are no-
tably lower than those obtained for positive and neg-
ative tweets. This can be explained by the content
of the dataset provided, which contained 1006 neg-
ative and 1896 positive tweets, but just 190 neutral
tweets, which is an insufficient sample for producing
reliable estimates on precision. The same problem
happened for progress test A, where the proportions
of tweets are similarly unbalanced.

Detailed scores for progress tests of subtasks A
and B are shown in Table 2. In general, we can say
that our system is quite stable, as it generates similar
results for the different kinds of texts under evalua-
tion. Also of note are the high percentages obtained
for sarcastic tweets, which ranked in the first posi-
tion in subtask A and in the tenth (test dataset) and
sixth positions (progress dataset) in subtask B (as
shown in Table 3).

5 Conclusions

This paper describes the participation of the GTI Re-
search Group, AtlantTIC Centre, University of Vigo,
in SemEval-2015 task 10: Sentiment Analysis in
Twitter. We achieved our results using a fully un-
supervised approach for message-level and phrase-

Test sets Task A Task B
Twitter2015 9/11 22/40

Twitter2015Sarcasm - 10/40
LiveJournal2014 8/11 18/40

SMS2013 6/11 16/40
Twitter2013 5/11 25/40
Twitter2014 9/11 22/40

Twitter2014Sarcasm 1/11 6/40

Table 3: Position of our approach for each test and task,
according to results provided on January 1, 2015.

level sentiment analysis of tweets. Table 3 shows
our position in the ranking published for both sub-
tasks A and B for all the different datasets evaluated.

Our approach comprises different processing
stages, including the generation of sentiment lexi-
cons, test preprocessing and the application of dif-
ferent methods for determining contextual polarity
based on syntactical structure. This makes our ap-
proach robust in diverse contexts without the need
for previous manual tagging of datasets. To the best
of our knowledge, it is the only system presented in
this competition whose sentiment analysis method
does not require any supervision.

Acknowledgments

This work was supported by the Spanish Govern-
ment, co-financed by the European Regional Devel-
opment Fund (ERDF) under project TACTICA, and
REdTEIC (R2014/037).

References

Atserias Jordi, Casas Bernardino, Comelles Elisabeth,
González Meritxell, Padró Luis and Padró Muntsa.

537



2006. FreeLing 1.3: Syntactic and semantic services
in an open-source NLP library. In Proc. of the 5th
International Conference on Language Resources and
Evaluation, p. 48–55. Genoa, Italy.

Balahur Alexandra. 2013. Sentiment Analysis in Social
Media Texts. In Proc. of the 4th Workshop on Compu-
tational Approaches to Subjectivity, Sentiment and So-
cial Media Analysis, p. 120–128, ACL. Atlanta, Geor-
gia.

Brants Thorsten. 2000. TnT: A Statistical Part-of-speech
Tagger. In Proc. of the 6th Conference on Applied Nat-
ural Language Processing, p. 224–231. Seattle, Wash-
ington.

Caro Luigi and Grella Matteo. 2013. Sentiment analysis
via dependency parsing. In Computer Standards &
Interfaces Journal, p. 442–453, volume 35 (5), New
York.

Cruz Fermı́n L., Troyano José A., Ortega F. Javier and
Enrı́quez Fernando. 2011. Automatic Expansion of
Feature-level Opinion Lexicons. In Proc. of the 2nd
Workshop on Computational Approaches to Subjectiv-
ity and Sentiment Analysis, p. 125–131. ACL. Port-
land.

Jansen Bernard J., Zhang Mimi, Sobel Kate and Chow-
dury Abdur. 2009. Twitter power: Tweets as elec-
tronic word of mouth. Journal of the American Soci-
ety for Information Science and Technology, p.2169–
2188, volume 60 (1), New York.

Hu Minqing and Liu Bing. 2004. Mining and sum-
marizing customer reviews. In Proc. of the 10th
ACM SIGKDD International Conference on Knowl-
edge Discovery and Data Mining, p. 168–177. ACM
Press.

Liu Bing. 2012. Sentiment Analysis and Opinion Min-
ing. In Synthesis Digital Library of Engineering and
Computer Science, Morgan & Claypool Publisher.

Mohammad Saif M. and Turney Peter D. 2013. Crowd-
sourcing a Word-Emotion Association Lexicon. Jour-
nal of Computational Intelligence, p. 436–465, volume
29 (3).

Mohammad Saif M., Kiritchenko Svetlana and Zhu Xi-
aodan 2013. NRC-Canada: Building the State-of-the-
Art in Sentiment Analysis of Tweets. In Proc. of the 7th
International workshop on Semantic Evaluation Exer-
cises (SemEval-2013), p. 282–290. Vienna, Austria.

Moilanen Karo and Pulman Stephen. 2007. Sentiment
Composition. In Proc. of Recent Advances in Natural
Language Processing (RANLP), p.378–382, Borovets,
Bulgaria.

Nakov Preslav, Rosenthal Sara, Kozareva Zornitsa, Stoy-
anov Veselin, Ritter Alan and Wilson Theresa. 2013.
SemEval-2013 Task 2: Sentiment Analysis in Twitter.

In Proc. of the 7th International Workshop on Seman-
tic Evaluation, p. 312–320. ACL. Atlanta, Georgia,
USA.

Nielsen Finn Årup. 2011. A new ANEW: Evaluation of
a word list for sentiment analysis in microblogs. In
Proc. of the ESWC2011 Workshop on Making Sense of
Microposts, p. 93–98.

Padró Lluı́s and Stanilovsky Evgeny. 2012. FreeLing
3.0: Towards Wider Multilinguality. In Proc. of the
8th International Conference on Language Resources
and Evaluation, p.23–25, Istanbul, Turkey.

Pak Alexander and Paroubek Patrick. 2010. Twitter as
a Corpus for Sentiment Analysis and Opinion Mining.
In Proc. of the 7th International Conference on Lan-
guage Resources and Evaluation (LREC’10), p.19–21,
Valleta, Malta.

Poria Soujanya, Cambria Erik, Winterstein Grégoire
and Huang Guang-Bin. 2014. Sentic patterns:
Dependency-based rules for concept-level sentiment
analysis. Journal of Knowledge-Based Systems, p.45–
63, volume 69 (1).

Quinn Kevin M., Monroe Burt L., Colaresi Michael,
Crespin Michael H. and Radev Dragomir R. 2010.
How to Analyze Political Attention with Minimal As-
sumptions and Costs. American Journal of Political
Science, p.209–228, volume 54 (1).

Rosenthal Sara, Nakov Preslav, Kiritchenko Svetlana,
Mohammad Saif M., Ritter Alan and Stoyanov
Veselin. 2015. SemEval-2015 Task 10: Sentiment
Analysis in Twitter. In Proc. of the 9th International
Workshop on Semantic Evaluation, ACL. Denver, Col-
orado.

Rosenthal Sara, Ritter Alan, Nakov Preslav and Stoyanov
Veselin. 2014. SemEval-2014 Task 9: Sentiment Anal-
ysis in Twitter. In Proc. of the 8th International Work-
shop on Semantic Evaluation, p.73–80. ACL. Dublin,
Ireland.

Rudolph Elisabeth. 1996. Contrast: Adversative and
Concessive Relations and Their Expressions in En-
glish, German, Spanish, Portuguese on Sentence and
Text Level. Research in text theory, Walter de Gruyter.
Berlin.

Taboada Maite, Brooke Julian, Tofiloski Milan, Voll
Kimberly and Stede Manfred. 2011. Lexicon-based
Methods for Sentiment Analysis. Journal of the Com-
putational Linguistics, p.267–307, volume 35 (2), MIT
Press. Cambridge, MA, USA.

Winterstein Grégoire. 2012. What but-sentences argue
for: a modern argumentative analysis of but. Lingua
122 (15), p. 1864–1885.

Zhang Lei, Ferrari Silvia and Enjalbert Patrice. 2012.
Opinion Analysis: the Effect of Negation on Polarity
and Intensity. In Proc. of KONVENS 2012 (PATHOS
2012 Workshop), p. 282–290. Vienna, Austria.

538


