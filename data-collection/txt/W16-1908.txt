



















































Learning Phone Embeddings for Word Segmentation of Child-Directed Speech


Proceedings of the 7th Workshop on Cognitive Aspects of Computational Language Learning, pages 53–63,
Berlin, Germany, August 11, 2016. c©2016 Association for Computational Linguistics

Learning Phone Embeddings for Word Segmentation
of Child-Directed Speech

Jianqiang Maa,b Çağrı Çöltekinb Erhard Hinrichsa,b
a SFB 833, University of Tübingen, Germany

b Department of Linguistics, University of Tübingen, Germany
{jma,ccoltekin,eh}@sfs.uni-tuebingen.de

Abstract

This paper presents a novel model that
learns and exploits embeddings of phone
ngrams for word segmentation in child
language acquisition. Embedding-based
models are evaluated on a phonemi-
cally transcribed corpus of child-directed
speech, in comparison with their symbolic
counterparts using the common learning
framework and features. Results show
that learning embeddings significantly im-
proves performance. We make use of ex-
tensive visualization to understand what
the model has learned. We show that the
learned embeddings are informative for
both word segmentation and phonology in
general.

1 Introduction

Segmentation is a prevalent problem in language
processing. Both humans and computers pro-
cess language as a combination of linguistic units,
such as words. However, spoken language does
not include reliable cues to word boundaries that
are found in many writing systems. The hearers
need to extract words from a continuous stream
of sounds using their linguistic knowledge and the
cues in the input signal. Although the problem is
still non-trivial, competent language users utilize
their knowledge of the input language, e.g., the
(mental) lexicon, to a large extent to aid extraction
of lexical units from the input stream.

Word segmentation in early language acquisi-
tion is especially interesting and challenging, as
early language learners barely have a lexicon or
any other linguistic knowledge to start with. Con-
sequently, it has been studied extensively through
psycholinguistic experiments (Cutler and Butter-
field, 1992; Jusczyk et al., 1999; Jusczyk et al.,

1993; Saffran et al., 1996; Jusczyk et al., 1999;
Suomi et al., 1997; van Kampen et al., 2008)
and computational modeling (Cairns et al., 1994;
Christiansen et al., 1998; Brent and Cartwright,
1996; Brent, 1999; Venkataraman, 2001; Xanthos,
2004; Goldwater et al., 2009; Johnson and Gold-
water, 2009).

The majority of the state-of-the-art computa-
tional models use symbolic representations for in-
put units. Due to Zipf’s law, most linguistic units,
however, are rare and thus the input provides lit-
tle evidence for their properties that are useful
for solving the task at hand. In machine learning
terms, the learner has to deal with the data sparse-
ness problem due to the rare units whose param-
eters cannot be estimated reliably. A model us-
ing distributed representations can counteract the
data sparseness problem by exploiting the similar-
ities between the units for parameter estimation.
This has motivated the introduction of embeddings
(Bengio et al., 2003; Collobert et al., 2011), a fam-
ily of low-dimensional, real-valued vector repre-
sentation of features that are learned from data.
Unlike purely symbolic representations, such dis-
tributed representations allow input units that ap-
pear in similar contexts to share similar vectors
(embeddings). The model can, then, exploit the
similarities between the embeddings during seg-
mentation and learning.

This paper studies the learning and use of em-
beddings of phone1 uni- and bi-grams for com-
putational models of word segmentation in child
language acquisition. Our work is inspired by
recent success of embeddings in NLP (Devlin et
al., 2014; Socher et al., 2013), especially in Chi-
nese word segmentation (Zheng et al., 2013; Pei
et al., 2014; Ma and Hinrichs, 2015). However,
this work differs from Chinese word segmenta-

1We use the therm phone as a theory-neutral term for the
distinct (phonetic) segments in the input.

53



tion models in two aspects. (1) The model (Sec-
tion 2) learns from a phonemically transcribed cor-
pus of child-directed speech (Section 3.1) instead
of large written text input. (2) The learning (Sec-
tion 2.2) only relies on utterance boundaries in in-
put as opposed to explicitly marked word bound-
aries. Although the number of phone types is
small, higher level ngrams of phones inevitably
increase the severity of data sparseness. Thus we
expect embeddings to be particularly useful when
larger phoneme ngrams are used as input units.
The contributions of this paper are three-fold:

• A novel model that constructs and uses em-
beddings of phone ngrams for word segmen-
tation in child language acquisition;

• Empirical evaluations of symbolic and em-
bedding representations for this task on the
benchmark data, which suggest that learning
embeddings boosts the performance;

• A deeper analysis of the learned embed-
dings through visualizations and clustering,
showing that the learned embeddings cap-
ture information relevant to segmentation and
phonology in general.

In the next section we define the distributed
representations we use in this study, phone-
embeddings, and a method for learning the em-
beddings and the segmentation parameters simul-
taneously from a corpus without word boundaries.
Then we present a set of experiments for compar-
ing embedding and symbolic representations (Sec-
tion 3). We show our visualization and clustering
analyses of the learned embeddings (Section 4) be-
fore discussing our results further in the context of
previous work (Section 5) and concluding the pa-
per.

2 Learning Segmentation with Phone
Embeddings

2.1 The architecture of the model

Figure 1 shows the architecture of the proposed
embedding-based model. Our model takes the em-
beddings of phone uni- and bi-grams in the lo-
cal window for each position in an utterance, and
predicts whether that position is a word boundary.
The embeddings for the phone ngrams are learned
jointly with the segmentation model. The model
has the following three components:

Figure 1: Architecture of our model.

Look-up table maps phone ngrams to their
corresponding embeddings. In this study, for
each position j, we consider the 4 unigrams
(cj−1, cj , cj+1, cj+2) and 2 bigrams (cj−1cj and
cj+1cj+2) that are in a window of 4 phones of po-
sitions j. The phone cj represents the phone on
the left of the current position j and so on.

Concatenation. To predict the segmentation
for position j, the embeddings of the phone uni-
and bi-gram features are concatenated into a sin-
gle vector, input embedding, ij ∈ RNK , where
K = 6 is the number of uni- and bi-gram used
and N = 50 is the dimension of the embedding of
each ngram.

Sigmoid function. The model then computes
the sigmoid function (1) of the dot product of the
input embedding ij and the weight vector w. The
output is a score ∈ [0, 1] that denotes the probabil-
ity that the current position being a word bound-
ary, which we call boundary probability.

f(j) =
1

1 + exp (−w · ij) (1)

2.2 Learning with utterance edge and
random sampling

Our model learns from utterances that have word
boundaries removed. It, however, utilizes the ut-
terance boundaries as positive instances of word
boundaries. Specifically, the position before the
first phone of an utterance is the left boundary
of the first word, and the position after the last
phone of an utterance is the right boundary of the
last word. For these positions, dummy symbols

54



are used as the two leftmost (rightmost) phones.
Moreover, one position within the utterance is ran-
domly sampled as negative instance. Although
such randomly sampled instances are not guaran-
teed to be actual negative ones, sampling balances
the positive instances, which makes learning pos-
sible.

The training follows an on-line learning strat-
egy, processing one utterance at a time and updat-
ing the parameters after processing each utterance.
The trainable parameters are the weight vector and
the embeddings of the uni- and bi-grams. For each
position j, the boundary probability is computed
with the current parameters. Then the parameters
are updated by minimizing the cross-entropy loss
function as in (2).

Jj = − [yj log f(j) + (1− yj) log (1− f(j))] (2)
In formula (2), f(j) is the boundary probabil-
ity estimated in (1) and yj is its presumed value,
which is 1 and 0 for utterance boundaries and sam-
pled intra-utterance positions, respectively. To off-
set over-fitting, we add an L2 regularization term
(||ij ||2 + ||w||2) to the loss function, as follows:

Jj ← Jj + λ2
(
||ij ||2 + ||w||2

)
(3)

The λ is a factor that adjusts the contribution
of the regularization term. To minimize the regu-
larized loss function, which is is still convex, we
perform stochastic gradient descent to iteratively
update the embeddings and the weight vector in
turn, each time considering the other as constant.
The gradients and update rules are similar to that
of logistic regression model as in Tsuruoka et al.
(2009), except that the input embeddings i are also
updated besides the standard weight vector.

In particular, the gradient of input embeddings
ij for each particular position j is computed ac-
cording to (4), where w is the weight vector and
yj is the assumed label. The input embeddings are
then updated by (5), where α is the learning rate.

∂Jj
∂ij

= (f (j)− yj) ·w + λij (4)

ij ← ij − α∂Jj
∂ij

(5)

2.3 Segmentation via greedy search
The word segmentation of utterances is a greedy
search procedure using the learned model. It irre-
versibly predicts segmentation for each position j

(1 ≤ j ≤ N = utterance length), one at a time, in
a left-to-right manner. If the boundary probability
given by the model greater than 0.5, the current
position is predicted as word boundary, otherwise
non-boundary. The segmented word sequence is
built from the predicted word boundaries in the ut-
terance.

3 Experiments and Results

The learning framework described in Section 2
can also be adopted for symbolic representations
where the ngram features for each position are rep-
resented by a sparse binary vector. In the sym-
bolic representation, each distinct uni- or bi-gram
is represented by a distinct dimension in the in-
put vector. In that case, the learning framework is
equivalent to a logistic regression model, the train-
ing of which only updates the weight vector but
not the feature representations. In this section, we
run experiments to compare the performances of
embedding- and symbolic-based models using the
same learning framework with the same features.
Before presenting the experiments and the results,
we describe the data and evaluation metrics.

3.1 Data

In the experiments reported in this paper, we use
the de facto standard corpus for evaluating seg-
mentation models. The corpus was collected by
Bernstein Ratner (1987) and converted to a phone-
mic transcription by Brent and Cartwright (1996).
The original corpus is part of the CHILDES
database (MacWhinney and Snow, 1985). Follow-
ing the convention in the literature, the corpus will
be called the BR corpus. Since our model does
not know the locations of true boundaries, we do
not make training and test set distinction, follow-
ing previous literature.

3.2 Evaluation metrics

As a measure of success, we report F-score, the
harmonic mean of precision and recall. F-score is
a well-known evaluation metric originated in in-
formation retrieval (van Rijsbergen, 1979). The
calculation of these measures depend on true pos-
itive (TP), false positive (FP) and false negative
(FN) values for each decision. Following ear-
lier studies, we report three varieties of F-scores.
The boundary F-score (BF) considers individual
boundary decisions. The word F-score (WF)
quantifies the accuracy of recognizing word to-

55



kens. And the lexicon F-scores (LF) are calcu-
lated based on the gold-standard lexicon and lex-
icon learned by the model. For details of the
metrics, see Goldwater et al. (2009). Follow-
ing the literature, the utterance boundaries are not
included in boundary F-score calculations, while
lexicon/word metrics include first and the last
words in utterance.

Besides these standard scores we also present
over-segmentation (EO) and under-segmentation
(EU) error rate (lower is better) defined as:

EO =
FP

FP + TN
EU =

FN
FN + TP

where TN is true negatives of boundaries. Besides
providing a different look at the models’ behav-
ior, it is straightforward to calculate the statistical
uncertainty around them since they resemble N
Bernoulli trials with a particular error rate, where
N is number of boundary and word-internal posi-
tions for EU and EO respectively.

The results of our model in this paper are di-
rectly comparable with the results of previous
work on the BR corpus using the above met-
rics. The utterance boundary information that our
method uses is also available to any “pure” un-
supervised method in literature, such as the EM-
based algorithm of Brent (1999) and the Bayesian
approach of Goldwater et al. (2009). In these
methods, word hypotheses that cross utterance
boundaries are not considered, which implicitly
utilizes utterance boundary “supervision.”

3.3 Experiments
To show the differences between the symbolic and
embedding representations, we train both models
on the BR corpus, and present the performance and
error scores on the complete corpus. The training
of all models use the linear decay scheme of learn-
ing rate with the initial value of 0.05 and the regu-
larization factor is set to 0.001 throughout the ex-
periments. Table 1 presents the results, including
standard errors for EO and EU, for emb(edding)-
and sym(bolic)-based models using unigram fea-
tures (uni) and unigram+bigram features (all), re-
spectively.

Table 1 shows the average of the results ob-
tained from 10 independent runs. For each run, we
take the scores from the 10th iteration of the whole
data set, where the scores are stabilized. All mod-
els learn quickly and have good performance after

Model EO EU BF WF LF
emb/all 6.4±0.1 17.3±0.2 82.9 68.7 42.6
sym/all 8.1±0.1 25.8±0.2 75.9 60.2 31.6
emb/uni 15.8±0.1 10.6±0.3 77.4 59.1 40.7
sym/uni 13.2±0.1 21.7±0.2 73.4 54.4 29.4

Table 1: Performance of embedding and symbolic
models. Numbers in percentage.

the first iteration already. And the differences be-
tween the scores of subsequent iterations are rather
small.

4 Visualization and Interpretation

The experiment results in the previous section
show that learning embeddings jointly with a seg-
mentation model, instead using symbolic repre-
sentations, leads to a boost of segmentation perfor-
mance. Nevertheless, it is not straightforward to
interpret embeddings, as the “semantics” of each
dimension is not pre-defined as in symbolic rep-
resentations. In this section, we use visualization
and clustering techniques to interpret the informa-
tion captured by the embeddings.

Phone symbols in the BR corpus. We use the
BR corpus for visualization as in the experiments.
The transcription in the BR corpus use symbols
that, unfortunately, can not be converted to Inter-
national Phonetic Alphabet (IPA) in a context-free,
deterministic way. Thus we keep them as they are
and suggest readers who are unfamiliar with such
symbols to refer to Appendix A.

4.1 Embeddings encode segmentation roles

Segmentation roles of phone ngrams. We first
investigate the correspondence of the embeddings
to the metrics that are indicative for segmentation
decisions. For distinguishing word-boundary po-
sitions from word-internal positions as in segmen-
tation models, it is helpful to know whether a par-
ticular phone unigram/bigram is more likely to oc-
cur at the beginning of a word (word-initial), at the
end of a word (word-final), in the middle of a word
(word-medial), or has a balanced distribution of
above positions. For a phone bigram, it can also
be corss word-boundary. We call such tendencies
of phone ngrams as segmentation roles.

We hypothesize that the embeddings that are
learned by our model can capture segmentation
roles: the embeddings of phone ngrams of the
same segmentation role are similar to each other
and are dissimilar to the phone ngrams of different

56



Figure 2: PCA Projections of the phone uni-gram (left) and bi-gram (right) embeddings learned in our
model.

segmentation roles. To test this, we use principal
component analysis (PCA) to project the embed-
dings of phone uni- and bi-grams that are learned
in our model into two-dimension space, where the
resulting vectors preserve 85% and 98% of the
variance in the original 50-dimension uni- and bi-
gram embeddings, respectively. We then plot such
PCA-projected 2-D vectors of the phone ngrams in
Figure 2, where the geometric distances between
data points reflect the (dis-)similarities between
the original embeddings of phone ngrams. These
data points are color coded to demonstrate the
dominant segmentation role of each phone ngram.

A phone ngram is categorized as word-initial,
word-medial, word-final or corss word-boundary
(only applicable for bigrams), if the ngram co-
occur more than 50% of the time with the corre-
sponding segmentation roles according to the gold
standard segmentation. If none of the roles reaches
the majority, the ngram is categorized as balanced
distribution. Note that segmentation roles are
assigned using the true word boundaries, while
the embeddings are learned only from utterance
boundaries.

Figure 2 (left) shows that phone unigrams of the
same category tend to cluster in the same neigh-
borhood, while unigrams of distinct categories
tend to locate apart from each other. This is con-
sistent with our hypothesis on embeddings being
capable of capturing segmentation roles. Figure
2 (right) shows that the distribution of phone bi-
grams is noisier, as many bigrams of different cat-

egories congest in the center. This suggests that
bigram embeddings are less well estimated than
unigrams ones, probably due to the larger number
and lower relative frequencies of bigrams. Nev-
ertheless, the word-initial v.s. word-final contrast
in bigrams is still sharp, as a result of our training
procedure that makes heavy use of the initial and
final positions of utterances, which are also word
boundaries. In summary, the information that are
encoded in our phone ngram embeddings is highly
indicative of correct segmentations.

4.2 Embeddings capture phonology
Hierarchical clustering of phones. Different
from the previous subsection that correlates the
learned embeddings with segmentation-specific
roles, we can alternatively explore the embeddings
more freely to see what structures emerge from
data. To this end, we apply hierarchical agglom-
erative clustering (Johnson, 1967) to the embed-
dings of phone unigrams to build up clusters in
a bottom-up manner. Initially, each unigram em-
bedding itself consists of a cluster. Then at each
step, the two most similar clusters are merged.
The procedure iterates until every embedding is
in the same cluster. The similarity between clus-
ters are computed by the single linkage method,
which outputs the highest score of all the pair-
wise cosine similarities between the embeddings
in the two clusters. Since the clustering proce-
dure is based on pair-wise cosine similarities be-
tween embeddings, we first compute such similar-
ity scores, composing the similarity matrix.

57



Figure 3: Hierarchical clustering and similarity matrix of phone embeddings learned by our model.

The dendrogram (Jones et al., 2001 ) that rep-
resents the clustering results is shown in Figure
3, together with the heatmap that represents the
similarity matrix. The dendrogram draws a U-
shaped link to indicate how a pair of child clusters
form their parent cluster, where the dissimilarity
between the two child clusters are shown by the
height of the top of the U-link. The intensity of the
color of each grid in the heatmap denotes the sim-
ilarity between the two corresponding phone em-
beddings. Moreover, each lowest node, i.e. leaf, of
the dendrogram is vertically aligned with the col-
umn of the heatmap that corresponds to the same
phone, which is labeled using the BR-corpus sym-
bols. Thus the dark blocks along the antidiagonal
also indicate the salient clusters in which phone
embeddings are similar to one another.

Phonological structure. The heatmap reveals
several salient blocks, such as the one on the
top-right corner and the one near the bottom-left
corner. The former is part of a group of clus-
ters spreading the whole right 2/3 of the den-
drogram/heatmap, which mostly consists English

consonants. In contrast, the latter contains short,
unrounded vowels in English, E, &, I and A, as
in bet, that, bit and but, respectively. It also
contains the long-short vowel pair a and O as in
hot and law. Immediately to the right of them
are the cluster of compound vowels, o, 3, e, Q.
In general, most clusters are either consonant- or
vowel-dominant, while groups of the similar vow-
els form sub-clusters under the big vowel cluster.
Although far from perfect, the results suggest that
the learned phone embeddings capture phonolog-
ical features of English. On one hand, the emer-
gence of such phonological structure is not sur-
prising, as phonology is part of what defines a
word, although our word segmentation model does
not explicitly target it. On the other hand, such re-
sults are relevant as they suggest that the phono-
logical regularities are salient and learnable from
transcriptions even if lexical knowledge is absent.

4.3 Comparison with word2vec embeddings

We see that our phone embeddings can capture
segmentation-informative and phonology-related

58



Figure 4: Heatmap of phone embeddings in word2vec (top) and our model (bottom).

patterns. A question remains: is this the conse-
quence of joint learning of the embeddings with
the segmentation model, or something also achiev-
able by general-purpose embeddings? We test this
by comparing our phone embeddings with the em-
beddings that are trained by a standard embed-
ding construction tool, word2vec (Mikolov et al.,
2013). We first preprocess the raw BR corpus to
construct the phone uni- and bi-gram corpora, re-
spectively. Then we run word2vec with skip-gram
method for 20 iterations on the two corpora to
train the embeddings for phone uni- and bi-grams,
respectively. The training relies on using each
ngram to predict other ngrams in the same local
window. We use a window size of 4 phones in the
training to be comparable with our models.

We first plot the heatmap of the unigram em-
beddings of the word2vec model and that of our
model in Fig 4, where the embeddings of distinct
phone categories in our model exhibit distinct pat-
terns, whereas such distinctions are unclear in the
word2vec embeddings. Then we conduct the same
PCA and hierarchical clustering analyses for the
word2vec embeddings, as we did for our learned
embeddings. The results are shown in Figure 5
and 6, respectively. We see that word2vec embed-
dings capture neither segmentation-specific fea-
tures nor phonological structures as our learned

embeddings do, which suggests that the joint
learning of the embeddings and the segmentation
model is essential for the success.

5 Discussion and Related Work

Performance. The focus of this paper is investi-
gating the usefulness of embeddings, rather than
achieving best segmentation performance. Since
multiple cues are useful for both segmentation
by children (Mattys et al., 2005; Shukla et al.,
2007) and computational models (Christiansen et
al., 1998; Christiansen et al., 2005; Çöltekin and
Nerbonne, 2014), our single-cue model is not ex-
pected to outperform multiple-cue ones. The up-
per part of Table 2 shows the results of two state-
of-the-art systems, both of which adopt multiple
cues. Goldwater et al. (2009) relies on Bayesian
models, especially hierarchical Dirichlet process,
which models phone unigrams, word unigrams
and bigrams using similar distributions. Unlike
our model, which has no explicit notion of words,
Goldwater et al. (2009) keeps track of phones,
words, as well as word bigrams. In comparison
with our on-line learning approach, their Gibbs
sampling-based learning method repeatedly pro-
cesses the data in a batch way. By contrast,
Çöltekin and Nerbonne (2014) does conduct on-
line learning. But their best performing model,

59



Figure 5: PCA Projections of the embeddings of
phone unigrams (top) and bigrams (bottom) in
word2vec models.

Figure 6: Hierarchical clustering and similarity
matrix of phone embeddings in word2vec.

PUW, does not only rely on utterance boundaries
(U) as in our model, but also combines the pre-
dictability information (P) and the lexicon (L) of
previously discovered words.

An interesting observation is that our our model
achieves reasonably good boundary and word-

token F-scores, even comparing with these state-
of-the-art models. Unfortunately, the lexicon F-
score of our model is significantly lower. The rea-
son is probably that our method models segmenta-
tion decisions per position without explicitly keep-
ing a lexicon, whereas both state-of-the-art models
are “lexicon-aware”, which gives status to recog-
nized words. The use of word context can help
to identify low frequency words, some of which,
especially longer ones, are difficult for our phone
window-based model.

Model BF WF LF
Goldwater et al. (2009) 85.2 72.3 59.1
Çöltekin and Nerbonne (2014): PUW 87.3 76.4 53.3
Daland and Pierrehumbert (2011) 62.7 42.5 10.1
Fleck (2008) 82.9 70.7 36.6
Çöltekin and Nerbonne (2014): U 83.8 71.1 44.9
Our model: embedding, uni- & bi-gram 82.9 68.7 42.6

Table 2: Comparisoin of the best performance of
our model (bottom) with the state-of-the-art sys-
tems on the task (upper) and the models using ut-
terance boundaries as the main cue (middle). U:
using utterance boundary only; PUW: using pre-
dictability, utterance boundary and the learned lex-
icon. Numbers in percentage.

It is probably more instructive to compare the
performance of our model with other models eval-
uated in similar settings and use utterance bound-
aries as the main cue. The results of such models
are shown in the middle part of Table 2. Among
them, Daland and Pierrehumbert (2011) uses only
unigrams, whereas Fleck (2008) and the utterance
boundary-based model (U) in Çöltekin and Ner-
bonne (2014) are more elaborate, combining one
to three-grams of phones. The performance would
probably be lower if only uni- or bigrams are used
as in our model.

The scores at the bottom of the Table 2 sug-
gest that our model fares well in comparison to the
models that exploit similar learning strategies and
information sources. The results also show that
embeddings of phone unigrams and bigrams are
effective for segmentation. In addition, we also
tried trigrams, which did not improve the results
for symbolic or embedding models. This may be
due to that the trigrams are too sparse, especially
when our training samples only one inter-utterance
position per utterance.

Model properties and design choice. As de-
scribed at the beginning of Section 3, the pro-

60



posed model can be seen as an extension to lo-
gistic regression model, where the resulting model
also learns the distributed representations of fea-
tures from the data. The training relies on isolated
positions, namely utterance boundaries and sam-
pled intra-utterance positions, making the model a
classifier that ignores the sequential dependencies.
For these reasons, our model is structurally simple
and computationally efficient. We also avoid batch
processing-based and computationally expensive
techniques such as Gibbs sampling, as adopted in
many Bayesian models. For cognitive modeling,
efficient, on-line learning is favorable, as human
brain appears to work that way.

To investigate the impact of learning and us-
ing distributed representations, we could alterna-
tively use other neural network architectures, such
as multi-layer feed-forward neural networks or re-
current neural networks. The computational com-
plexity would be much higher in that case. Nev-
ertheless, it is still interesting, as a future work,
to develop phone-level recurrent neural network
(RNN) models for the task. In particular, it may be
promising to experiment with a modern variation
of RNN, long short-term memory (Schmidhuber
and Hochreiter, 1997), as it recently achieved con-
siderable success on various NLP tasks. A chal-
lenge here is how to train effective RNN models
in the language acquisition setting, where explicit
supervision is mostly absent.

Embeddings boost segmentation. Table 1
demonstrates that learning embeddings instead of
using symbolic representations boosts segmen-
tation performance. This is true in both set-
tings where the model adopts unigrams and uni-
gram+bigrams as features, respectively. With em-
beddings, models apply the information obtained
from frequent input units to the decisions involv-
ing infrequent units with similar representations.
Hence, although embeddings are beneficial in both
settings, it is not surprising that the improvement
is higher for the unigrams+bigrams setting, where
the data sparseness is more severe.

Figure 7 shows the difference in the learn-
ing curves of the embedding-based and symbolic-
based models, both using unigram+bigram fea-
tures. The embedding model starts with a higher
error rate in comparison to the symbolic one, since
the vectors for each unit is randomly initialized.
However, as the embeddings are updated with
more input, the embedding model quickly catches

0 20 40 60 80 100

0.
0

0.
1

0.
2

0.
3

0.
4

0.
5

Input utterances (1000x)

M
ea

n 
of

 E
O

 a
nd

 E
U

emb
sym

Figure 7: The mean of the error rates during the 1st
iteration for the embedding and symbolic models.

up with the symbolic model and finally outper-
forms it, as the results in Table 1 show.

Other distributed representations. The ut-
terance boundary cue has been used in earlier
work (Aslin et al., 1996; Stoianov and Ner-
bonne, 2000; Xanthos, 2004; Monaghan and
Christiansen, 2010; Fleck, 2008), but not with em-
beddings. Distributed representations other than
learned embeddings, however, have been common
in the early connectionist models (Cairns et al.,
1994; Aslin et al., 1996; Christiansen et al., 1998).
Besides better performance, our model differs in
that it learns the embeddings from the input, while
earlier models used hand-crafted distributed rep-
resentations. This allows our model to optimize
representations for the task at hand.

6 Conclusion

In this paper, we have presented a model that
jointly learns word segmentation and the embed-
dings of phone ngrams. The learning in our model
is guided by the utterance boundaries. Hence,
our learning method, although not unsupervised
in machine learning terms, does not use any in-
formation that is unavailable to the children ac-
quiring language. To the best of our knowledge,
this is the first work of learning phone embed-
dings for computational models of word segmen-
tation in child language acquisition. Compared
with symbolic-based models using the same learn-
ing framework, embedding-based models signifi-
cantly improve results. Visualization and analyses
show that the learned embeddings are indicative
of not only correct segmentations, but also certain
phonological structures.

61



Acknowledgments

The authors would like to thank the anonymous
reviewers for their helpful comments and sugges-
tions. The financial support for the research re-
ported in this paper was partly provided by the
German Research Foundation (DFG) via the Col-
laborative Research Center “The Construction of
Meaning” (SFB 833), project A3.

References
Richard N. Aslin, Julide Z. Woodward, Nicholas P.

LaMendola, and Thomas G. Bever. 1996. Mod-
els of word segmentation in fluent maternal speech
to infants. In James L. Morgan and Katherine De-
muth, editors, Signal to Syntax: Bootstrapping From
Speech to Grammar in Early Acquisition, chapter 8,
pages 117–134. Lawrence Erlbaum Associates.

Yoshua Bengio, Réjean Ducharme, Pascal Vincent, and
Christian Janvin. 2003. A neural probabilistic lan-
guage model. The Journal of Machine Learning Re-
search, 3:1137–1155.

Nan Bernstein Ratner. 1987. The phonology of parent-
child speech. In K. Nelson and A. van Kleeck, ed-
itors, Children’s language, volume 6, pages 159–
174. Erlbaum, Hillsdale, NJ.

Michael R. Brent and Timothy A. Cartwright. 1996.
Distributional regularity and phonotactic constraints
are useful for segmentation. Cognition, 61:93–125.

Michael R. Brent. 1999. An efficient, probabilistically
sound algorithm for segmentation and word discov-
ery. Machine Learning, 34(1-3):71–105.

Paul Cairns, Richard Shillcock, Nick Chater, and Joe
Levy. 1994. Modelling the acquisition of lexical
segmentation. In Proceedings of the 26th Child Lan-
guage Research Forum. University of Chicago Press.

Çağrı Çöltekin and John Nerbonne. 2014. An explicit
statistical model of learning lexical segmentation us-
ing multiple cues. In Proceedings of EACL 2014
Workshop on Cognitive Aspects of Computational
Language Learning.

Çağrı Çöltekin. 2011. Catching Words in a Stream
of Speech: Computational simulations of segment-
ing transcribed child-directed speech. Ph.D. thesis,
University of Groningen.

Morten H. Christiansen, Joseph Allen, and Mark S.
Seidenberg. 1998. Learning to segment speech
using multiple cues: A connectionist model. Lan-
guage and Cognitive Processes, 13(2):221–268.

Morten H. Christiansen, Christopher M. Conway, and
Suzanne Curtin. 2005. Multiple-cue integration
in language acquisition: A connectionist model of
speech segmentation and rule-like behavior. In J.W.

Minett and W.S.-Y. Wang, editors, Language acqui-
sition, change and emergence: Essays in evolution-
ary linguistics, chapter 5, pages 205–249. City Uni-
versity of Hong Kong Press, Hong Kong.

Ronan Collobert, Jason Weston, Léon Bottou, Michael
Karlen, Koray Kavukcuoglu, and Pavel Kuksa.
2011. Natural language processing (almost) from
scratch. The Journal of Machine Learning Re-
search, 12:2493–2537.

Anne Cutler and Sally Butterfield. 1992. Rhythmic
cues to speech segmentation: Evidence from junc-
ture misperception. Journal of Memory and Lan-
guage, 31(2):218–236.

Robert Daland and Janet B Pierrehumbert. 2011.
Learning diphone-based segmentation. Cognitive
Science, 35(1):119–155.

Jacob Devlin, Rabih Zbib, Zhongqiang Huang, Thomas
Lamar, Richard Schwartz, and John Makhoul. 2014.
Fast and robust neural network joint models for sta-
tistical machine translation. In Proceedings of ACL,
pages 1370–1380.

Margaret M. Fleck. 2008. Lexicalized phonotactic
word segmentation. In Proceedings of the Annual
Meeting of the Association of Computational Lin-
guistics (ACL-08), pages 130–138.

Sharon Goldwater, Thomas L. Griffiths, and Mark
Johnson. 2009. A Bayesian framework for word
segmentation: Exploring the effects of context.
Cognition, 112:21–54.

Mark Johnson and Sharon Goldwater. 2009. Im-
proving nonparameteric Bayesian inference: exper-
iments on unsupervised word segmentation with
adaptor grammars. In Proceedings of Human Lan-
guage Technologies: The 2009 Annual Conference
of the North American Chapter of the Association
for Computational Linguistics, pages 317–325.

Stephen C Johnson. 1967. Hierarchical clustering
schemes. Psychometrika, 32(3):241–254.

Eric Jones, Travis Oliphant, Pearu Peterson, et al.
2001–. SciPy: Open source scientific tools for
Python. [Online; accessed 2016-04-29].

Peter W. Jusczyk, Anne Cutler, and Nancy J. Redanz.
1993. Infants’ preference for the predominant stress
patterns of English words. Child Development,
64(3):675–687.

Peter W. Jusczyk, Derek M. Houston, and Mary New-
some. 1999. The beginnings of word segmentation
in English-learning infants. Cognitive Psychology,
39:159–207.

Jianqiang Ma and Erhard Hinrichs. 2015. Accurate
linear-time Chinese word segmentation via embed-
ding matching. In Proceedings of ACL-IJCNLP
(Volume 1: Long Papers), pages 1733–1743, Bei-
jing, China, July. Association for Computational
Linguistics.

62



Brian MacWhinney and Catherine Snow. 1985. The
child language data exchange system. Journal of
Child Language, 12(2):271–269.

Sven L. Mattys, Laurence White, and James F. Mel-
horn. 2005. Integration of multiple speech seg-
mentation cues: A hierarchical framework. Journal
of Experimental Psychology: General, 134(4):477–
500.

Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Cor-
rado, and Jeff Dean. 2013. Distributed representa-
tions of words and phrases and their compositional-
ity. In Advances in neural information processing
systems, pages 3111–3119.

Padraic Monaghan and Morten H. Christiansen. 2010.
Words in puddles of sound: modelling psycholin-
guistic effects in speech segmentation. Journal of
Child Language, 37(Special Issue 03):545–564.

Wenzhe Pei, Tao Ge, and Chang Baobao. 2014. Max-
margin tensor neural network for chinese word seg-
mentation. In Proceedings of ACL, pages 239–303.

Jenny R. Saffran, Richard N. Aslin, and Elissa L. New-
port. 1996. Statistical learning by 8-month old in-
fants. Science, 274(5294):1926–1928.

Jürgen Schmidhuber and Sepp Hochreiter. 1997.
Long short-term memory. Neural computation,
7(8):1735–1780.

Mohinish Shukla, Marina Nespor, and Jacques Mehler.
2007. An interaction between prosody and statistics
in the segmentation of fluent speech. Cognitive Psy-
chology, 54(1):1–32.

Richard Socher, John Bauer, Christopher D Manning,
and Andrew Y Ng. 2013. Parsing with compo-
sitional vector grammars. In Proceedings of ACL,
pages 455–465.

Ivelin Stoianov and John Nerbonne. 2000. Explor-
ing phonotactics with simple recurrent networks.
In Frank van Eynde, Ineke Schuurman, and Ness
Schelkens, editors, Proceedings of Computational
Linguistics in the Netherlands 1999, pages 51–67.

Kari Suomi, James M. McQueen, and Anne Cut-
ler. 1997. Vowel harmony and speech segmenta-
tion in finnish. Journal of Memory and Language,
36(3):422–444.

Yoshimasa Tsuruoka, Jun’ichi Tsujii, and Sophia Ana-
niadou. 2009. Stochastic gradient descent training
for L1-regularized log-linear models with cumula-
tive penalty. In Proceedings of ACL-IJCNLP, pages
477–485.

Anja van Kampen, Güliz Parmaksız, Ruben van de Vi-
jver, and Barbara Höhle. 2008. Metrical and statis-
tical cues for word segmentation: The use of vowel
harmony and word stress as cues to word boundaries
by 6- and 9month-old Turkish learners. In Anna

Gavarro and M. Joao Freitas, editors, Language Ac-
quisition and Development: Proceedings of GALA
2007, pages 313–324.

C. J. van Rijsbergen. 1979. Information Retrieval.
Butterworth-Heinemann, 2nd edition.

Anand Venkataraman. 2001. A statistical model for
word discovery in transcribed speech. Computa-
tional Linguistics, 27(3):351–372.

Aris Xanthos. 2004. An incremental implementation
of the utterance-boundary approach to speech seg-
mentation. In Proceedings of Computational Lin-
guistics in the Netherlands (CLIN) 2003, pages 171–
180.

Xiaoqing Zheng, Hanyang Chen, and Tianyu Xu.
2013. Deep learning for Chinese word segmentation
and pos tagging. In Proceedings of EMNLP, pages
647–657.

A Symbols used in BR corpus

Consonants
Symbol Example
D the
G jump
L bottle
M rhythm
N sing
S ship
T thin
W when
Z azure
b boy
c chip
d dog
f fox
g go
h hat
k cut
l lamp
m man
n net
p pipe
r run
s sit
t toy
v view
w we
y you
z zip
~ button

Vowels
Symbol Example
& that
6 about
7 bOy
9 fly
A but
E bet
I bit
O law
Q bout
U put
a hot
e bay
i bee
o boat
u boot

Rhotic Vowels
Symbol Example
# are
% for
( here
) lure
* hair
3 bird
R butter

Adapted from Çöltekin (2011).

63


