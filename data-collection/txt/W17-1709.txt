



















































Multiword expressions and lexicalism: the view from LFG


Proceedings of the 13th Workshop on Multiword Expressions (MWE 2017), pages 73–79,
Valencia, Spain, April 4. c©2017 Association for Computational Linguistics

Multiword expressions and lexicalism: the view from LFG

Jamie Y. Findlay
University of Oxford / Oxford, UK

jamie.findlay@ling-phil.ox.ac.uk

Abstract

Multiword expressions (MWEs) pose a
problem for lexicalist theories like Lex-
ical Functional Grammar (LFG), since
they are prima facie counterexamples to a
strong form of the lexical integrity princi-
ple, which entails that a lexical item can
only be realised as a single, syntactically
atomic word. In this paper, I demonstrate
some of the problems facing any strongly
lexicalist account of MWEs, and argue
that the lexical integrity principle must be
weakened. I conclude by sketching a for-
malism which integrates a Tree Adjoining
Grammar into the LFG architecture, tak-
ing advantage of this relaxation.

1 Multiword expressions

Baldwin & Kim (2010, 269) define multiword ex-
pressions (MWEs), as

lexical items that: (a) can be decomposed into
multiple lexemes; and (b) display lexical, syn-
tactic, semantic, pragmatic and/or statistical id-
iomaticity.

This is a very broad definition, covering every-
thing from full-fledged idioms like cut the mustard
to mere hackneyed expressions like never tell me
the odds. In this paper, my focus is on semantic
idiomaticity, this being the prototypical feature of
MWEs, but what I say has implications for, and is
not incompatible with, the other kinds as well.

2 LFG

Lexical Functional Grammar (LFG: Kaplan and
Bresnan, 1982; Dalrymple, 2001; Bresnan et al.,
2015) is a constraint-based, lexicalist approach to
the architecture of the grammar. Its primary focus
has always been syntax, but with a special interest

IP

DP

D′

D

The

NP

N′

N

cat

I′

I

is

VP

V′

V

yawning

















PRED ‘YAWN’

SUBJ

[

PRED ‘CAT’
DEF +

]

TENSE PRESENT

ASPECT PROGRESSIVE

















φ

φ

Figure 1: C-structure and f-structure for The cat is
yawning.

in the interfaces between this and other compo-
nents of the grammar, including argument struc-
ture (e.g. Kibort, 2007), morphology (e.g. Butt
et al., 1996), semantics (e.g. Dalrymple, 1999),
information structure (e.g. Dalrymple and Niko-
laeva, 2011), and prosody (e.g. Mycock and Lowe,
2013).

A syntactic analysis in LFG involves two for-
mally distinct kinds of object: c(onstituent)-
structure, which is a phrase-structure tree that rep-
resents linear order as well as hierarchical relation-
ships like constituency; and f(unctional)-structure,
which is an attribute-value matrix that represents
more abstract, functional relations like ‘subject-
of’. The two are connected via the function φ. An
example is given in Figure 1.

The correspondence between c-structure and
f-structure is controlled via annotations on the
tree, provided either by phrase structure rules or
the lexical entries themselves. The convention
in writing such annotations is to use ↑ and ↓
as metavariables, representing φ(∗̂) and φ(∗) re-
spectively, where ∗ is a variable representing the
c-structure node where an annotation appears, and
∗̂ represents the mother of that node. Thus, for
example, the canonical English subject and object

73



rules can be written as follows:

(1) IP → DP
(↑ SUBJ) = ↓

I′

↑ = ↓
(2) V′ → V

↑ = ↓
DP

(↑ OBJ) = ↓
These say, essentially, that an IP can be made up
of a DP which is the subject of the clause, and an
I′, while a V′ can be made up of a V, and a DP
which is the object of the clause.

I omit the annotations on the tree in Figure 1
for reasons of space, but in principle all nodes are
annotated. Finding the f-structure is then a matter
of finding the minimal f-structure which satisfies
all of the equations. In this way, the f-structure
constrains the over-generation of the context-free
c-structure, expanding the grammar’s expressive
power.

LFG subscribes to a strong version of the lexical
integrity principle (LIP), namely that

[m]orphologically complete words are leaves of
the c-structure tree and each leaf corresponds to
one and only one c-structure node.
(Bresnan, 2001, 93)

This means that c-structure leaves are words, and
that words are c-structure leaves. The original mo-
tivation for LIP was to ensure that syntactic rules
should be ‘blind’ to morphology. But, in its strong
version, it works in the other direction too. This
facet of LIP is what Ackerman et al. (2011) call
the principle of unary expression (PUE):

In syntax, a lexeme is uniformly expressed as a
single morphophonologically integrated and syn-
tactically atomic word form.

If we think of MWEs as lexemes, then they are
clearly a challenge to this principle. But even if
we instead claim they are some kind of ‘listeme’
(Di Sciullo and Williams, 1987), there remains the
question of how a single object, be it in ‘the list’ or
the lexicon, can be realised as multiple potentially
disjoint word forms in the syntax. MWEs thus re-
main at least a prima facie challenge to a strongly
lexicalist theory.

3 Lexical ambiguity approaches

For any strongly lexicalist theory which adheres
to (at least the spirit of) PUE, the most obvious
way to deal with MWEs is via what we might call
the lexical ambiguity approach (LA). In such an
approach, MWEs are treated as made up of spe-
cial words which combine to give the appropriate

meaning for the whole expression. Words like pull
and strings become ambiguous, meaning either
pull′ and strings′ or exploit′ and connections′,
and so the semantic idiomaticity is encoded di-
rectly in the lexical entries. This sidesteps the PUE
issue, since MWEs are not single lexical items, but
rather collections of separate lexical items which
conspire to create the overall meaning. For this
reason, versions of LA have been popular in var-
ious lexicalist theories: see, for instance, Sailer
(2000) for HPSG, Arnold (2015) for LFG, Kay et
al. (2015) for SBCG, and Lichte and Kallmeyer
(2016) for LTAG. However, LA has a large num-
ber of shortcomings which mean that it is unten-
able as a general position.

Although LA seems to naturally explain so-
called decomposable idioms, where the mean-
ing of the whole can be distributed across the
parts (since this is exactly what the approach
does), it is not so clear how it should handle non-
decomposable idioms, like kick the bucket, blow
off steam, shoot the breeze, etc., where there is
no obvious way of breaking down the meaning
of the idiom such that its parts correspond to the
words that make up the expression. Solutions have
been proposed: for instance, Lichte and Kallmeyer
(2016) argue for what they call ‘idiomatic mirror-
ing’, whereby each of the parts of the idiom con-
tributes the meaning of the whole expression, so
that kick means die′, bucket means die′, and, pre-
sumably, the means die′ as well. A similar ap-
proach is pursued in work by Sascha Bargmann
and Manfred Sailer (Bargmann and Sailer, 2005,
in prep.). Both proposals, however, assume a se-
mantics which allows for redundancy, a decision
which is crucial for idiomatic mirroring to work.
In a strictly resource-sensitive conception of the
syntax-semantics interface like LFG+Glue (Dal-
rymple, 1999; Asudeh, 2012), each contribution
to the semantics must contribute something to the
meaning, with the result that multiple items can-
not contribute the same semantics without a con-
comitant change in meaning (big, big man means
something different from big man, for example).

Without idiomatic mirroring, we are forced to
assume that only one of the words in the expres-
sion bears the meaning, and that the rest are se-
mantically inert. For example, perhaps there is
a kickid which means die′, and selects for spe-
cial semantically inert forms theid and bucketid.
Notice, however, that the choice of where to lo-

74



cate the meaning is ultimately arbitrary. We may
as well have bucketid meaning die′, or even theid,
provided they select for the other inert forms and
then pass their meaning up to the whole VP. Such
arbitrariness seems undesirable.

It also leads to another formal issue: we now
face an explosive proliferation of semantically in-
ert forms throughout the lexicon. What is more,
each of these must be restricted so that it does not
appear outside of the appropriate expression. But
this means that the the in kick the bucket can’t be
the same the as in shoot the breeze. We need as
many thes as there are expressions which include
it. Instead of having to expand the lexicon by as
many entries as there are MWEs, we have to ex-
pand it by as many entries as there are words in
MWEs, which is much less appealing, and smacks
of redundancy.

One empirical issue facing LA relates to the
psycholinguistic findings on processing. Swin-
ney and Cutler (1979) showed that idioms are pro-
cessed in the same way as regular compositional
expressions; i.e. there is no special ‘idiom mode’
of comprehension. At the same time, others have
found that idiomatic meanings are processed faster
and in preference to literal ones (Estill and Kem-
per, 1982; Gibbs, 1986; Cronk, 1992). If both
these things are true, then LA is in trouble: in
this approach, there is no reason to think idioms
or other MWEs should be processed any faster; if
anything, we might expect them to be slower, since
they involve ambiguity by definition.

Rather, the psycholinguistic findings plead for
what seems intuitively appealing anyway: that
MWEs are inserted en bloc, being stored in the
lexicon as units. But this requires there to be ob-
jects in the lexicon which are larger than single
words, defined as terminal nodes in a tree, which
necessitates abandoning PUE.

4 TAG-LFG

Really, we want to be able to extend the domain
of the lexical entry so that it can naturally include
MWEs. This can be readily achieved in Lexi-
calised Tree Adjoining Grammar (LTAG: Joshi et
al., 1975), which has successfully been used to
analyse MWEs in the past (e.g. Abeillé, 1995).

One of the key strengths of any TAG-based ap-
proach is its extended domain of locality. Since the
operation of adjunction allows trees to grow ‘from
the inside out’, as it were, relationships can be

encoded locally even when the elements involved
may end up arbitrarily far apart. This is precisely
the situation which obtains with idioms and other
MWEs which allow for syntactic flexibility.

What is more, a TAG-based approach where
MWEs are multiply-anchored trees (that is, trees
with more than one terminal node specified in the
lexicon, so that they contain more than one word)
aligns with the psycholinguistic findings. A parse
involving a MWE will involve fewer elementary
trees: for example, in a parse of John kicked the
bucket, instead of the four trees for John, kicked,
the, and bucket, it will just involve the two for John
and kicked the bucket, explaining the increased
processing speed (Abeillé, 1995).

However, I am not advocating that LFG practi-
tioners should abandon LFG in favour of LTAG.
Space precludes a full defence of the virtues of
LFG here, but I believe it possesses a number of
advantageous features we should like to retain.
Firstly, there is the separation of abstract grammat-
ical information from the constituency-based syn-
tactic tree. A detailed and dedicated level of rep-
resentation for functional information is motivated
by the fact that it is important in grammatical de-
scription and not necessarily determined by phrase
structure. For example, functional information is
relevant in terms of describing binding domains
(Dalrymple, 1993), or for phenomena related to
the functional/accessibility hierarchy (Keenan and
Comrie, 1977), or in describing argument alterna-
tion (Bresnan, 1982).

Secondly, LFG has grown beyond just c- and
f-structure, and now has a well-developed gram-
matical architecture encompassing many differ-
ent levels of representation, from phonological, to
morphological, to semantic and information struc-
ture, and the relations and constraints that exist be-
tween them. This correspondence architecture (on
which see Asudeh, 2012, 49–54) is a powerful tool
for describing the multi-modal phenomenon that is
natural language, and something we would like to
preserve.

With this in mind, then, what we should like to
do is to incorporate the advantages of the TAG-
style extended domain of locality into the pre-
existing LFG architecture. The most obvious way
to do this is to replace the context-free grammar of
LFG’s c-structure with a TAG instead.1 Let us call

1This has been proposed before, though never developed:
e.g. Kameyama (1986), Burheim (1996), Rambow (1996)
(sadly these manuscripts have proved impossible to track

75



this variant TAG-LFG. In the rest of this section I
will sketch its key features.

The first thing to note is that such a move does
not increase the expressive power of LFG. Of
course, a TAG is mildly context sensitive, which
is more powerful than the context-free grammar
of LFG’s c-structure. However, LFG is not just
c-structure, and the presence of f-structure already
pushes LFGs beyond the mildly context-sensitive
space (Berwick, 1982). Thus, although we are em-
powering a part of the formalism, we are not in-
creasing the power of the formalism as a whole.

Since c-structure nodes in LFG can be ‘deco-
rated’ with functional information, another con-
cern is how to handle these during substitution and
adjunction. Substitution is straightforward: since
no elementary tree will be annotated on its root
node, we simply retain the annotation on the sub-
stitution target. For adjunction, feature-based TAG
standardly makes use of top and bottom features
(Vijay-Shanker, 1987). Since in TAG-LFG we are
unifying features from the whole tree in one place,
the f-structure, rather than locally on each node,
we do not need to separate annotations in the same
way. Instead, at the top of the adjunction structure,
annotations are retained from the target, while at
the bottom, they are retained from the foot of the
auxiliary tree. This is equivalent to seeing adjunc-
tion as two instances of substitution following a
dividing up of the tree; in each case the target re-
tains its annotations.

Let us now turn to the question of represen-
tation. In standard LFG, a lexical entry is a
triple (W,C,F ), where W is a word form, i.e.
the terminal node in the phrase-structure tree,
C is a c-structure category, i.e. the pre-terminal
node, and F is a functional description, i.e. a set
of expressions spelling out additional linguistic
information via the correspondence architecture.
In TAG-LFG, a lexical entry is instead a triple
(〈W 〉, T, F ), consisting of a list of word forms,
a tree, provided by some metagrammar (Crabbé et
al., 2013), and a functional description. A simple
example for a non-decomposable idiom is given in

down). See also Clément and Kinyon (2003) for a proposal
to generate both LFG and TAG grammars from the same set
of linguistic descriptions (encoded in a metagrammar).

A reviewer points out potential similarities with LFG-DOP
(Bod and Kaplan, 1998; see also references in Arnold and
Linardaki, 2007), which combines LFG with Data-Oriented
Parsing (Bod, 1992). This also makes use of tree fragments,
but it still relies on a lexicon stated in terms of context-free
rules to generate these fragments, and thus is still reliant on a
version of LA to encode MWEs in the lexicon.

〈W 〉 = 〈kicked, the, bucket〉
T = S

(↑ SUBJ)=↓
NP0⇓

VP

V♦0 (↑ OBJ)=↓
NP1

D♦1 N♦2

F = (Sφ TENSE) = PAST
...

λx.die(x) : (Sφ SUBJ)σ ⊸ Sφσ
Figure 2: TAG-LFG lexical entry for kicked the
bucket

Figure 2.2 The word forms occur as a list because
the trees for MWEs will be multiply anchored. For
regular lexical entries, this list will be a singleton.
The lexical anchors, marked with ♦s, are num-
bered according to the list index of the word form
that is to be inserted there. The functional descrip-
tion remains the same, although it now allows ref-
erence to more remote nodes, and so instead of ↑
or ↓ I use node labels as a shorthand for the nodes
in question.3 ,4

In Figure 2, I have given the semantics in the
form of a meaning constructor. This is an object
used in Glue Semantics, the theory of the syntax-
semantics interface most often coupled with LFG
(Dalrymple, 1999; Asudeh, 2012). It consists, on
the left-hand side, of a formula in some ‘mean-
ing language’, in this case a lambda expression,
and, on the right-hand side, of an expression in lin-
ear logic (Girard, 1987) over s(emantic)-structures
(projected from f-structures via the σ function),
which controls composition. In this case, it says
that the meaning of the whole sentence is obtained
by applying λx.die(x) to the meaning of the sen-
tence’s subject.

By associating the meaning constructor not with
any particular node in the tree, but with the tree as
a whole, via the lexical entry, we avoid the arbi-
trariness of having to choose one word to host the

2To avoid confusion with the LFG metavariable ↓, I use
the symbol ⇓ to represent a TAG substitution site.

3In reality, the node labels are not the nodes: they are the
output of a node labelling function λ applied to each node
(Kaplan, 1995).

4Sφ is shorthand for φ(S), and Sφσ for σ(φ(S)), i.e. the
f-structure and s(emantic)-structure corresponding to S, re-
spectively.

76



〈W 〉 = 〈pulled, strings〉
T = S

(↑ SUBJ)=↓
NP0⇓

VP

V♦0 (↑ OBJ)=↓
NP1

N♦1

F = (Sφ TENSE) = PAST
...

λx.connections(x) :
(NP1φσ VAR) ⊸ (NP1φσ RESTR)

λxλy.exploit(x, y) :
(Sφ SUBJ)σ ⊸ (Sφ OBJ)σ ⊸ Sφσ

Figure 3: TAG-LFG lexical entry for pulled
strings

meaning. It remains possible to represent decom-
posable idioms, too, since we can simply include
multiple meaning constructors in the f-description,
separating out the meaning and referring to the rel-
evant parts as required. Figure 3 gives an example
for pull strings. In this case, two meaning con-
structors are present, one for each of the decom-
posable ‘parts’ of the idiomatic meaning.5 This
allows for internal modification of strings, for ex-
ample (e.g. pull family strings).

The varying syntactic flexibility of MWEs can
be accounted for by the standard TAG approach of
associating each lexeme with different families of
elementary trees. For example, assuming a more
abstract level of lexemic entry, which is used to
generate the set of lexical entries associated with
each lexeme (or listeme) (Dalrymple, 2015), we
can simply say that the lexemic entry for kick the
bucket is associated with only the active voice tree,
while that for pull strings is associated with many
others, including trees for wh-questions, passive,
and relative clauses. This results in different sets
of potential lexical entries for each expression, and
thus different potential syntactic configurations.

One other notable property of idioms is that the
words they contain are morphologically related to
independently existing words: for example, kick
in kick the bucket inflects like a regular English
verb (such as literal kick), while come in come a
cropper inflects irregularly in just the same way
as literal come (e.g. he came a cropper). Space

5For simplicity, I ignore questions about the semantics of
plurality on the object.

precludes a full treatment of this here, but it is
straightforward enough to implement, for exam-
ple by having the idiomatic lexemic entry select
its word forms from the ‘form’ paradigms of ex-
isting lexemes (Stump, 2001, 2002). Note that
such a relationship, whereby parts of a lexical en-
try draw from the morphological paradigm of in-
dependent words, is not unique to MWEs: for ex-
ample, the lexeme UNDERSTAND is, in terms of
inflection, made up of UNDER+STAND, where the
second part is identical in inflectional terms to the
independent verb STAND, e.g. it shares the irregu-
lar past tense form, as in understood. Thus, such a
mechanism is needed independently of the present
proposal, and its extension to TAG-LFG should
not pose any undue problems.

5 Conclusion

Strongly lexicalist theories which subscribe to the
principle of unary expression cannot deal with
MWEs. They are forced to adopt some version of
the lexical ambiguity approach, which ultimately
fails both formally and empirically. Once we
abandon PUE, the question then open to us is how
to represent MWEs at the interface between the
lexicon and syntax. A formalism like (L)TAG of-
fers an elegant and well-tested means of doing just
this. And with minimal modifications, and no in-
crease in generative power, it can be integrated
into the LFG architecture.

Acknowledgements

I would like to thank many people for their helpful
comments and discussions on this topic, includ-
ing Ash Asudeh, Alex Biswas, Mary Dalrymple,
Timm Lichte, John Lowe, Stephen Pulman, and
Andrew Spencer. Special thanks also to the anony-
mous reviewers, whose advice greatly improved
this paper. Of course, any mistakes which remain
are mine alone. This work was completed while I
was the recipient of a UK Arts and Humanities Re-
search Council studentship (grant reference AH/
L503885/1), which I gratefully acknowledge.

References
Anne Abeillé. 1995. The flexibility of French idioms:

A representation with Lexicalized Tree Adjoining
Grammar. In Martin Everaert, Erik-Jan van der
Linden, André Schenk, and Rob Schreuder, editors,
Idioms: Structural and psychological perspectives.
Lawrence Erlbaum, Hove, UK.

77



Farrell Ackerman, Gregory T. Stump, and Gert Webel-
huth. 2011. Lexicalism, periphrasis, and implica-
tive morphology. In Robert D. Borsley and Kersti
Börjars, editors, Non-transformational syntax: For-
mal and explicit models of grammar, pages 325–
358. Wiley-Blackwell, Oxford, UK.

Doug Arnold and Evita Linardaki. 2007. Linguis-
tic constraints in LFG-DOP. In Miriam Butt and
Tracy Holloway King, editors, Proceedings of the
LFG07 Conference, pages 66–86. CSLI Publica-
tions, Stanford, CA.

Doug Arnold. 2015. A Glue Semantics for structurally
regular MWEs. Poster presented at the PARSEME
5th general meeting, 23–24th September 2015, Iaşi,
Romania.

Ash Asudeh. 2012. The logic of pronominal resump-
tion. Oxford University Press, Oxford, UK.

Timothy Baldwin and Su Nam Kim. 2010. Multi-
word Expressions. In Nitin Indurkhya and Fred J.
Damerau, editors, Handbook of Natural Language
Processing (2nd edn.), pages 267–292. CRC Press,
Boca Raton, FL.

Sascha Bargmann and Manfred Sailer. 2015. The
syntactic flexibility of non-decomposable idioms.
Poster presented at the PARSEME 4th general meet-
ing, 19–20 March 2015, Valletta, Malta.

Sascha Bargmann and Manfred Sailer. In prep.
The syntactic flexibility of semantically non-
decomposable idioms. In Manfred Sailer and Stella
Markantonatou, editors, Multiword expressions: In-
sights from a multi-lingual perspective. Language
Science Press, Berlin, DE.

Robert C. Berwick. 1982. Computational complexity
and Lexical-Functional Grammar. American Jour-
nal of Computational Linguistics, 8:97–109.

Rens Bod and Ronald Kaplan. 1998. A probabilis-
tic corpus-driven model for Lexical-Functional anal-
ysis. In Proceedings of the 36th Annual Meet-
ing of the Association for Computational Linguis-
tics and 17th International Conference on Com-
putational Linguistics, Volume 1, pages 145–151,
Montreal, Quebec, Canada, August. Association for
Computational Linguistics.

Rens Bod. 1992. A computational model of language
performance: Data Oriented Parsing. In Proceed-
ings of the 15th International Conference on Com-
putational Linguistics (Volume 3: Project notes with
demonstrations), pages 855–859, Nantes, France,
August. International Committee on Computational
Linguistics, Association for Computational Linguis-
tics.

Joan Bresnan, Ash Asudeh, Ida Toivonen, and Stephen
Wechsler. 2015. Lexical-functional syntax (2nd
edn.). Wiley-Blackwell, Oxford, UK.

Joan Bresnan. 1982. The passive in lexical theory.
In Joan Bresnan, editor, The mental representation
of grammatical relations, pages 3–86. MIT Press,
Cambridge, MA.

Joan Bresnan. 2001. Lexical-functional syntax. Black-
well, Oxford, UK.

Tore Burheim. 1996. Aspects of merging
Lexical-Functional Grammar with Lexicalized Tree-
Adjoining Grammar. Unpublished ms., University
of Bergen.

Miriam Butt, Marı́a-Eugenia Niño, and Frédérique
Segond. 1996. Multilingual processing of auxil-
iaries within LFG. In Dafydd Gibbon, editor, Natu-
ral language processing and speech technology: Re-
sults of the 3rd KONVENS Conference, Universität
Bielefeld, October 1996, pages 111–122. Mouton de
Gruyter, Berlin.

Lionel Clément and Alexandra Kinyon. 2003. Gen-
erating parallel multilingual LFG-TAG grammars
from a MetaGrammar. In Proceedings of the 41st
Annual Meeting of the Association for Computa-
tional Linguistics, pages 184–191, Sapporo, Japan,
July. Association for Computational Linguistics.

Benoı̂t Crabbé, Denys Duchier, Claire Gardent,
Joseph Le Roux, and Yannick Parmentier. 2013.
XMG: eXtensible MetaGrammar. Computational
Linguistics, 39(3):591–629.

Brian C. Cronk. 1992. The comprehension of idioms:
The effects of familiarity, literalness, and usage. Ap-
plied Psycholinguistics, 13:131–146.

Mary Dalrymple and Irina Nikolaeva. 2011. Objects
and information structure. Cambridge University
Press, Cambridge, UK.

Mary Dalrymple. 1993. The syntax of anaphoric bind-
ing. Number 36 in CSLI Lecture Notes. CSLI Pub-
lications, Stanford, CA.

Mary Dalrymple, editor. 1999. Semantics and syntax
in Lexical Functional Grammar: The resource logic
approach. MIT Press, Cambridge, MA.

Mary Dalrymple. 2001. Lexical Functional Gram-
mar. Number 34 in Syntax and Semantics. Aca-
demic Press, Stanford, CA.

Mary Dalrymple. 2015. Morphology in the LFG ar-
chitecture. In Miriam Butt and Tracy Holloway
King, editors, Proceedings of the LFG15 Confer-
ence, pages 64–83. CSLI Publications, Stanford,
CA.

Anna Maria Di Sciullo and Edwin Williams. 1987. On
the definition of word. Number 14 in Linguistic In-
quiry monographs. MIT Press, Cambridge, MA.

Robert B. Estill and Susan Kemper. 1982. Interpret-
ing idioms. Journal of Psycholinguistic Research,
11(6):559–568.

78



Raymond W. Gibbs, Jr. 1986. Skating on thin ice: Lit-
eral meaning and understanding idioms in context.
Discourse Processes, 9:17–30.

Jean-Yves Girard. 1987. Linear logic. Theoretical
Computer Science, 50(1):1–102.

Aravind K. Joshi, Leon S. Levy, and Masako Taka-
hashi. 1975. Tree adjunct grammars. Journal of
Computer and System Sciences, 10(1):136–163.

Megumi Kameyama. 1986. Characterising Lexical
Functional Grammar (LFG) in terms of Tree Adjoin-
ing Grammar (TAG). Unpublished ms., Department
of Computer and Information Science, University of
Pennsylvania.

Ronald M. Kaplan and Joan Bresnan. 1982. Lexical-
Functional Grammar: A formal system for gram-
matical representation. In Joan Bresnan, editor,
The mental representation of grammatical relations,
pages 173–281. MIT Press, Cambridge, MA.

Ronald M. Kaplan. 1995. The formal architecture
of Lexical-Functional Grammar. In Mary Dalrym-
ple, Ronald M. Kaplan, John T. Maxwell, III,, and
Annie Zaenen, editors, Formal issues in Lexical-
Functional Grammar, pages 7–28. CSLI Publica-
tions, Stanford, CA.

Paul Kay, Ivan A. Sag, and Daniel P. Flickinger. 2015.
A lexical theory of phrasal idioms. Unpublished
ms., CSLI, Stanford.

Edward L. Keenan and Bernard Comrie. 1977. Noun
phrase accessibility and Universal Grammar. Lin-
guistic Inquiry, 8:63–99.

Anna Kibort. 2007. Extending the applicability
of Lexical Mapping Theory. In Miriam Butt and
Tracy Holloway King, editors, Proceedings of the
LFG07 Conference, pages 250–270. CSLI Publica-
tions, Stanford, CA.

Timm Lichte and Laura Kallmeyer. 2016. Same
syntax, different semantics: A compositional ap-
proach to idiomaticity in multi-word expressions.
In Christopher Piñón, editor, Empirical Issues in
Syntax and Semantics 11. Colloque de Syntaxe et
Sémantique à Paris (CSSP), Paris.

Louise Mycock and John Lowe. 2013. The prosodic
marking of discourse functions. In Miriam Butt and
Tracy Holloway King, editors, Proceedings of the
LFG13 Conference, pages 440–460. CSLI Publica-
tions, Stanford, CA.

Owen Rambow. 1996. Word order, clause union, and
the formal machinery of syntax. In Miriam Butt
and Tracy Holloway King, editors, Proceedings of
the LFG96 Conference. CSLI Publications, Stan-
ford, CA.

Manfred Sailer. 2000. Combinatorial semantics and
idiomatic expressions in Head-Driven Phrase Struc-
ture Grammar. Doctoral dissertation, Eberhard-
Karls-Universität Tübingen.

Gregory T. Stump. 2001. Inflectional morphology: A
theory of paradigm structure. Cambridge University
Press, Cambridge, UK.

Gregory T. Stump. 2002. Morphological and syntac-
tic paradigms: Arguments for a theory of paradigm
linkage. In Geert Booij and Jaap van Marle, edi-
tors, Yearbook of morphology 2001, pages 147–180.
Kluwer, Dordrecht, NL.

David A. Swinney and Anne Cutler. 1979. The access
and processing of idiomatic expressions. Journal of
Verbal Learning and Verbal Behavior, 18:523–534.

K. Vijay-Shanker. 1987. A study of Tree Adjoin-
ing Grammars. Doctoral dissertation, University of
Pennsylvania.

79


