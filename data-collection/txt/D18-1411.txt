



















































Learning Latent Semantic Annotations for Grounding Natural Language to Structured Data


Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 3761–3771
Brussels, Belgium, October 31 - November 4, 2018. c©2018 Association for Computational Linguistics

3761

Learning Latent Semantic Annotations
for Grounding Natural Language to Structured Data

Guanghui Qin1∗ Jin-Ge Yao2 Xuening Wang3 Jinpeng Wang2 Chin-Yew Lin2
1Peking University, 2Microsoft Research Asia, 3University of California Los Angeles

ghq@pku.edu.cn, sherry9788@g.ucla.edu
{jinge.yao, jinpwa, cyl}@microsoft.com

Abstract

Previous work on grounded language learn-
ing did not fully capture the semantics under-
lying the correspondences between structured
world state representations and texts, espe-
cially those between numerical values and lex-
ical terms. In this paper, we attempt at learn-
ing explicit latent semantic annotations from
paired structured tables and texts, establishing
correspondences between various types of val-
ues and texts. We model the joint probabil-
ity of data fields, texts, phrasal spans, and la-
tent annotations with an adapted semi-hidden
Markov model, and impose a soft statistical
constraint to further improve the performance.
As a by-product, we leverage the induced an-
notations to extract templates for language
generation. Experimental results suggest the
feasibility of the setting in this study, as well as
the effectiveness of our proposed framework. 1

1 Introduction

The meaning of natural language should always
be accompanied by a context. Grounded lan-
guage acquisition aims at learning the meaning
of language in the context of an observed world
state. A solution framework typically addresses
the following subproblems: segmenting the text
into meaningful phrasal units, determining which
world state information is being referred to, and
finding proper alignments from these units to the
events of values in the world state.

The task has attracted much attention from the
NLP community with a special focus on align-
ing text descriptions onto processed, structured
event records (Snyder and Barzilay, 2007; Liang
et al., 2009; Hajishirzi et al., 2011). Various sta-
tistical models have been proposed, attempting at

∗ Contribution during internship at Microsoft Research
Asia.

1Our implementation is available at https:
//github.com/hiaoxui/D2T-Grounding.

TEAM WIN LOSS PTS . . . PT-QT4
Raptors 33 15 120 . . . 31
Wizards 31 17 116 . . . 15

The Toronto Raptors ( 33 - 15 ) barely edged out 
the Washington Wizards ( 31 - 17 ) 120 - 116 in 
overtime Saturday at the Verizon Center. The 
Raptors took a great lead before the fourth quar-
ter, but the Wizards fought back by outscoring 
Toronto 31  -  15 ....

Figure 1: An example of the summary and the corre-
sponding table.

characterizing the interaction between text spans
and categorical values (e.g., direction=‘East’) or
strings (e.g., person names). The previously ad-
dressed term semantic correspondences narrowly
describes the process of aligning natural language
spans to different data fields.

However, there still exists a gap between align-
ment results and the underlying semantics. People
tend to use various phrases to describe information
that are inferred from different amounts of numer-
ical values in a data field, or values derived from
additional operations over fields. Consider the ex-
ample description for a basketball game shown in
Figure 1. The phrase edged out in the first sen-
tence implies the fact that the Toronto Raptors had
beaten their opponent by a relatively narrow mar-
gin. This could only be derived from an operation
of subtraction between two scores that correspond
to the field PTS for both teams in the event table,
which leads to a relatively small difference of only
four points. Previous efforts on learning seman-
tic correspondences relying on categorical distri-
butions (Liang et al., 2009) or string pattern fea-
tures (Hajishirzi et al., 2012; Koncel-Kedziorski
et al., 2014) do not have the capability to accu-
rately capture numerical information, especially
for the part that does not appear explicitly in the
table and needs to be inferred.

https://github.com/hiaoxui/D2T-Grounding
https://github.com/hiaoxui/D2T-Grounding


3762

Such kind of language grounding is important
both for natural language understanding and for
natural language generation. For language un-
derstanding, establishing explicit connections be-
tween symbols and values beyond ungrounded
symbolic meaning representations will be useful
for acquisition and inference of numerical com-
monsense (Narisawa et al., 2013). For language
generation, properly aligned information is the key
to acquiring patterns of various lexical choices un-
der different world states (Roy and Reiter, 2005).

In this work, we make a step towards more
explicit semantic correspondences between struc-
tured data and texts. Rather than only produc-
ing coarse alignments between data fields and text
spans, we try to detect the latent semantics under-
lying these alignments by prompting explicit se-
mantic annotations. We make the first attempt at
utilizing publicly available datasets originally pre-
pared for data-to-text language generation to pro-
duce such annotations for words and phrases in
natural language without additional supervision.

Specifically, we conduct our study on a recently
released dataset of descriptions for NBA basket-
ball games with structured tables of game records.
Different from a few popular datasets that have
been well conjectured to be produced from rules
(Reiter, 2017), the summaries are all written by
humans. The text contains some numbers and
proper nouns, which are easier to establish corre-
spondence with data. However, the majority of
texts contain many informative words, some of
which need to be inferred indirectly from various
types of values in the data cells. We want to estab-
lish explicit correspondences for them.

We derive a set of semantic labels from orig-
inal data fields (Sec 4.1). These labels could be
executed to establish direct correspondences to
one or more values in the structured table. No
annotation on the original dataset means unsu-
pervised learning from weak distant supervision
should be conducted. We design a semi-hidden
Markov model to address this problem (Sec 4.2),
which could align a semantic label to a word span.
In the model, we leverage continuous probabil-
ity distributions to model the correspondences be-
tween numerical values and lexical terms, which
has not been well captured in previous work. To
address the emerged issue of “garbage collection”
that commonly appears in statistical alignment
models (Sec 4.5), we add a soft statistical con-

straint via posterior regularization (Ganchev et al.,
2010). As a by-product, we also show how the
derived semantic annotations could be used to in-
duce descriptive templates for data-to-text genera-
tion (Sec 5). Experimental results (Sec 6) suggest
the feasibility of the setting in this study, and show
the effectiveness of our proposed framework.

2 Related work

Grounded language acquisition has aroused wide
interest in various disciplines (Siskind, 1996; Yu
and Ballard, 2004; Gorniak and Roy, 2007; Yu
and Siskind, 2013; Chrupała et al., 2015). Later
work in the community of natural language pro-
cessing also moved in this direction by relax-
ing the amount of supervision to enable a model
to learn from ambiguous alignments (Kate and
Mooney, 2007; Chen and Mooney, 2008). Some
research aimed at establishing coarse alignments
between simulated robot soccer game records and
commentary sentences (Chen and Mooney, 2008;
Chen et al., 2010; Bordes et al., 2010; Hajishirzi
et al., 2011). For weather forecast domain, Liang
et al. (2009) used a hierarchical hidden Markov
model in order to map utterances to world states,
which coped with segmentation and alignment to-
gether. More recently, Koncel-Kedziorski et al.
(2014) tried to obtain the correspondences be-
tween real commentaries and structured football
(soccer) events in multiple resolutions. We are
distinct from this line of work in the fact that
we aim at producing explicit semantic annotations
that could capture information from structured ta-
bles. To achieve this goal, we need additional scal-
ing or operations to enable data fields and values to
be faithfully mapped onto texts. This will address
the issue of the lack of consideration for the rela-
tionship between lexical terms and numerical val-
ues. Our approach makes a significant difference
in that our framework could generalize to numeri-
cal values or value combinations that are unseen in
training, and will not be simply reciting cooccur-
rence patterns of exact values in the training data.

Our work relates to learning executable se-
mantic parsers under weak supervision. Early
semantic parsing started from fully supervised
training with annotated meaning representations
available (Zettlemoyer and Collins, 2005; Ge and
Mooney, 2006; Snyder and Barzilay, 2007), but
more recent work focused on reducing the amount
of supervision required (Artzi and Zettlemoyer,



3763

2013). The intuition behind weakly supervised
executable semantic parsing is that once the la-
tent semantic representation has been executed,
one could test whether the execution results could
match the information with available weak super-
vision signals such as answers to natural language
queries (Clarke et al., 2010; Liang et al., 2011),
or task completion from instructional navigations
(Misra et al., 2017). Such formulations have been
adapted for question answering over structured
tables (Pasupat and Liang, 2015; Krishnamurthy
et al., 2017). However, the current research fo-
cus is to convert a natural language question into
executable table queries and to directly retrieve re-
sults. They do not have the need of inference in-
volving numerical commonsense implied by var-
ious lexical patterns. A few unsupervised ap-
proaches exist (Poon and Domingos, 2009; Poon,
2013) but only specific to translating language into
queries in the highly structured database and can-
not be applied to our domain.

Our approach is implemented as assigning tag
annotations over text spans, which is conceptu-
ally related to fine-grained named entity tagging
(Ling and Weld, 2012). Our setting only requires
a rather weak and distant form of supervision
from paired tables and texts without annotations
for fine-grained alignments between phrases and
data cells. Similar modeling and learning strate-
gies could potentially be useful for considerably
large tag space derived from structured knowledge
bases in the future (Choi et al., 2018).

The feasibility of this work is partly due to the
availability of data, mostly comes from the field of
data-to-text language generation. Related work in
data-to-text generation mainly focused on directly
generating summary descriptions for structured
data (Mei et al., 2016; Kiddon et al., 2016; Mu-
rakami et al., 2017; Wiseman et al., 2017), with-
out establishing underlying semantic correspon-
dences. Texts generated thereby can be fluent but
not conforming to the input data, unlike template-
based approaches where lexical choices could be
directly controlled. In our work, we find the
derived semantic correspondences between data
and texts to be useful for template induction, ei-
ther with simple heuristics to automatically ex-
tract description patterns (how to say) and corre-
sponding triggers (what & when to say), or with
more crafted discriminative learning approaches
(cf. Angeli et al. (2010)).

3 Technical overview

Task Let S be the set of all world states, W be
the set of all texts, O be the set of all executable
operators, and V be the output space of O. A
world state s ∈ S is a table storing some infor-
mation, or more specifically in this work, a tabular
recording for a sports game. An operator o ∈ O
can be executed on a world state to retrieve val-
ues, i.e., each o could be treated as a mapping of
S → V . The result of an operation can be a string,
a continuous values or a discrete value. Mean-
while, each world state s is accompanied with a
piece of description w ∈ W . Here w consists
of a sequence of word tokens {wi ∈ w}. We fur-
ther define a segmentation variable π, which could
convert w into a sequence of word spans c, con-
taining each span of tokens ct ∈ c that could be
interpreted as a phrase. Note that we use super-
script t to denote indices of phrases, and subscript
i to denote indices of individual words. We further
define l as a sequence of latent labels, and value of
each lt is an operator oi ∈ O. 2 For each world
state s and corresponding description w, we want
to jointly find a proper segmentation π to obtain c,
and assign labels to every word span ct.

We conduct this study on the ROTOWIRE sub-
set of the openly available dataset released by
Wiseman et al. (2017), containing text descrip-
tions for NBA basketball games with structured
tables of game statistics. Take Figure 1 as an ex-
ample. The proper nouns (e.g. Toronto Raptors)
appeared in the sentence can be assigned with a
tag Team Name in our tag set, which could then
be aligned to the Team Name field in the table.
Some numbers appearing in the text, such as 15
in the example, can be assigned with the label
Team Losses, with the executable annotation to
extract the number of previously lost matches of
the mentioned team. What we are more interested
in is where the phrase edge out comes from. We
are aiming at a model which is capable to capture
the semantics behind the phrase edge out, which
is used to describe an event that one team has
beaten the other with close scores. In our an-
notation scheme, this phrase should be assigned
with the tag Team Points Delta, and execut-
ing that will return the score difference between
two teams.

The task is challenging in that there does not

2We will interchangeably use labels, operators, tags to re-
fer to the latent executable semantic annotations in this paper.



3764

Output Type Example
String Team City
Categorical Player Start Position
Numerical Team Points Delta

Table 1: Examples from the derived tag set, where each
tag could correspond to one of three types of values.

exist any other additional supervision signal. The
learning process will mostly rely on statistical
cooccurrences of information between the struc-
tured data and its text descriptions. Note that we
assume a consistent structure (schema) throughout
the whole dataset upon which the learning process
will be performed.
Model To jointly learn word segmentations c
and latent semantic annotations l between world
state s and text w in a unified framework, we pro-
pose a generative model to characterize the joint
distribution Ps(l, π,w; θ), parameterized by θ.
Learning The data contain paired texts and ta-
bles only, thus our model must learn segmenta-
tions and latent semantic annotations in an unsu-
pervised fashion. The target is to maximize the
complete data likelihood

L(θ) =
∏

(s,w)∈D

∑
l,π

Ps(l, π,w; θ),

where D represents the whole training data. To
reduce the search space in inference and to cap-
ture some patterns of content planning in the
text descriptions, we adopt a Markov assump-
tion over phrase segments, which leads to a hid-
den semi-Markov model (semi-HMM) (Murphy,
2002; Sarawagi and Cohen, 2005). The key part is
to characterize different types of correspondences
(Sec 4.2). We derive an expectation-maximization
(EM) algorithm to perform maximum likelihood
estimation, and introduce a soft statistical regular-
ization to guide the model towards a better solu-
tion (Sec 4.5).
Inference Once the model has been trained, we
use a Viterbi-like dynamic programming process
to perform MAP inference to segment the texts
and to assign the most likely tags for each span.

4 Framework

4.1 The set of annotations
We describe the process of how we derived our
set of semantic annotations here. There are two
kinds of specific tables for each NBA game in the
dataset: Box-Scores and Line-Scores, respectively

showing the performance statistics for individual
players and the whole teams. The types of pos-
sible values for data fields are strings, categorical
values and numerical values. Box-Score consists
of 24 fields, of which four are string-values, one
is categorical, the other 19 being numerical. Line-
Score consists of 16 fields, containing two string-
valued, one categorical, and 13 numerical fields.
A semantic tag works on a specified kind of field
type, from either team statistics or player statis-
tics. For a tag that could align to one single field
in the table, we let it return the exact value in the
cell that could maximize the likelihood. For ex-
ample, the tag Team City is used to extract the
name of the team in the table that corresponds to
the current word span. For fields taking numeri-
cal values, we additionally allow tags to be able
to perform mathematical operations, such as sub-
traction, between two values in the same field. 3

For example, the tag Team Points Delta can
be executed to return the score difference between
two teams. We list the different types of tags with
examples shown in Table 1 and leave the entire tag
set to Appendix A. Along with all these tags de-
rived from the original data fields, we also include
a special NULL tag which are supposed to be as-
signed to non-informative words or words contain-
ing information not contained in the given table.

Note that we impose little prior knowledge in
this step. We simply over-generate all possible la-
bels, and let the model figure out which part of
them should be eventually used. Although the
only compositional operation we used in this work
is numeric subtraction, common operations that
could produce string, categorical values or num-
bers could be easily introduced for other domains.

4.2 Semi-HMMs with continuous values
As previously mentioned, we will be modeling the
joint distribution of word segmentation c and the
latent semantic annotations l between paired world
state s and text w, which could be factorized as:

Ps(l, π,w) = Ps(w, π|l) · Ps(l),

and we write Ps(w, π|l) as Ps(c|l). In this sec-
tion, we focus on the probability of the alignments
between word spans and labels, namely, Ps(c|l).

Following Liang et al. (2009), we consider two
aspects. One is salience that captures the intuition

3We limit the number of arguments within two in this
work and leave more complex operators for future study.



3765

that some fields should be more frequently men-
tioned than others (henceforth some latent tags
should be more frequently triggered). The other
is (local) coherence, which refers to the order in
which the writer mentioning certain information
tends to follow some patterns. To capture these
two phenomena, we define a Markov model:

Ps(c, l) =
∏
t

P (lt|lt−1) · Ps(ct|lt),

where lt is the annotated label at time stamp t, and
we assume that the transition probabilities are in-
dependent of world state s. It resembles a standard
form of HMMs, despite the subscript s in Ps(c|l).
For different types of correspondences between lt

and ct, we define different probability distributions
to model Ps(ct|lt):

(1) Numerics-to-numerics: The numbers
in texts could sometimes be inaccurate
due to some rounding customs, thus
we use a Gaussian model for this type:
SoftIndicator(x, y|σ) = N (x − y|0, σ),
where N is the Gaussian density. When
the output type of tag lt is numeric and
the word span ct is a number, we set
Ps(c|l) = SoftIndicator(c, vl|σl), where
σl is different for different tags, and vl is the
corresponded value in the table for tag l. Note
that when σ → 0, SoftIndicator reduces to
an indicator function that only allows exact
matching.

(2) String-to-string: Similarly for strings, since
simple matching could fail if the text con-
tains Bob to refer to Bob Smith. We simply
use string matching to model the probability:
Ps(c|l) ∝ Match(vl, c), where the Match
function returns the number of shared words
between cell value vl and word c.

(3) Category-to-string: For labels correspond to
discrete categorical values, such as Sunday,
PG (point guard, a basketball position), we
adopt the same method used by Liang et al.
(2009): using a multinomial distribution over
all word spans for each possible category:

Ps(c|l) = νc,vl ,
∑
c

νc,v = 1, (1)

where vl is again the output value of tag l.

     the        Clippers         took           down           the

Team_Name -
 

Points_Delta -
 

null -
 

null -
 

Figure 2: A Semi-HMM that can yield an entire phrase
from one tag.

(4) Numerics-to-string: When the tag corre-
spond to a numeric value vl while the word
span c is not a number, the problem resembles
speech modeling (Huang et al., 1990). Apply-
ing the Bayes rule, we get: 4

Ps(c|l) = P (c|l, vl) ∝ P (vl|c, l) · P (c|l),

where we collapse the relevant part from
world state s into vl. The intuition be-
hind P (vl|c, l) is that when an informa-
tive word (e.g. routed) appears in the
text, the corresponding values should have
different chances to happen in the world,
e.g. P (30|routed,Points Delta) >
P (3|routed,Points Delta).
Due to the lack of prior knowledge on the dis-
tribution, we also model this term as Gaussian.
5 The result resembles a Gaussian mixture:

Ps(c|l) ∝ N (vl;µc,l, σc,l) · ηc,l,∑
c

ηc,l = 1,

where P (c|l) = ηc,l is also multinomial.

4.3 Modeling phrasal spans

Our model can enable phrase segmentation. Pre-
viously, Liang et al. (2009) treated the words in-
side a phrase individually and independently. This
could be problematic in our scenario. For exam-
ple, take down is used to describe a team defeating
another, while separately both take and down are
frequent words in general, making them difficult
to be jointly assigned with the correct label as a
whole. Instead, in our model we treat the phrasal

4We noticed that in parallel with our work, another study
(Zhang et al., 2018) on verb selection for data-to-text gen-
eration also use the same strategy of Bayes rule to estimate
parameters, and provide a noisy-channel interpretation.

5The double tails in Gaussian pdf have different interpre-
tations: unlikely to occur, or unlikely to occur conditionally.



3766

Team_Name - 

 ...     the        Clippers         took          down           the          Rockets   ...

Points_Delta - 
null - 

null - 

Team_Name - 

Figure 3: A slightly different Semi-HMM whose tran-
sition score is calculated by skipping NULL fields.

word span as a unit. The probability is assigned
to the whole span of phrase instead of individual
words, which will break the token-wise Markov
property (Fig. 2, henceforth Semi-HMM). For effi-
cient parameter estimation, We use a variant of the
standard forward-backward algorithm by adding
a parameter k, which is the maximum length of
word spans, onto the Markov chain. We leave de-
tailed descriptions to Appendix B.

4.4 Skipping null labels

Preliminary experiments suggest that the initial
model have too many words assigned to the NULL
tag. Informative alignments may not be adja-
cent, which breaks the simplest Markov assump-
tions. In our model, the transition score of two
non-NULL labels can be calculated by skipping
all the NULLs in between, as shown in Figure 3.
This is implemented without breaking the over-
all Markov property with the following trick used
in earlier work on statistical alignments (Brown
et al., 1993): Suppose we have m labels (i.e., m
latent states), we can design m different NULL la-
bels that share the same emission score, while pre-
serving their original outward-transition probabil-
ities. The types of NULL labels are inherited from
the previous label. This might seem to be waste-
ful at first sight as we use two-fold latent states,
but the Markov property is successfully preserved,
therefore simplifying our implementation.

4.5 Posterior regularization

The structured tables and text descriptions of the
dataset were originally crawled from different
sources. As a consequence, a non-negligible pro-
portion of texts is in fact describing information
outside the given table, such as historical records
(e.g., “win streak”). Ideally, words in these parts
of the text should remain unaligned, or in our set-
ting, be annotated with the NULL tag. However,
due to the notorious effect of garbage collection

from statistical alignment (Brown et al., 1993),
these words tend to be aligned to some irrelevant
fields in the table which are rarely mentioned.

We address this issue by adding a soft statisti-
cal constraint in the form of posterior regulariza-
tion (Ganchev et al., 2010; Graça et al., 2010).
With posterior regularization, we could add cer-
tain types of statistical constraints to the E-step
in the EM procedure, while keeping the inference
tractable. The constraints should be in the form of:

E[f(w, l)] =
∑
i

E[f(w, li)] ≤ bw, (2)

where the features f should be defined on local
cliques for tractable inference.

We use projected gradient descent to solve the
E-step sub-problem in this work. The statistical
constraint we add to the posterior is rather simple:
For each sentence, we “encourage” at least a pro-
portion of words to be aligned to NULL labels:

E[−f(w, l)] ≤ −r0 · n, (3)

f(w, l) =

n∑
i=1

1(li = NULL), (4)

where r0 is a adjustable ratio, n is the length of
w. We also tried other constraints but found this
simple soft regularization performing well.

5 By-product: template induction

Intuitively, the assigned semantic correspondences
could be useful to derive templates and trigger
rules for language generation. In this work, we
use the most straightforward heuristics to perform
template induction, utilizing the established cor-
respondences and inferred parameters. Specifi-
cally, we first blank out the correspondences of
numerics-to-numerics and string-to-string to be
empty slots and replace with the tag names. In
the example of Figure 1, we could replace Raptors
with Team Name, and 120 with Team Points,
if they have been correctly aligned.

We also need to know when to use each tem-
plate. We define a template trigger to be a quadru-
ple (c, l, µc,l, σc,l), where c is a phrase, l is a tag,
µc,l and σc,l are estimated Gaussian parameters.
6 We assign each template with a score to be the
minimum probability for all triggers inside:

score(s, t) = min
i
N (ti.l(s); ti.µ, ti.σ), (5)

6To use a unified notation, for categorical-value triggers
we set µc,l = argmaxvl νc,vl (defined in (1)) and σc,l = �.



3767

where t = {ti} denotes all possible triggers in
the template, and the tag l can be executed over
the world state s to retrieve a value l(s). We only
consider sentences satisfying both of the following
conditions in order to extract templates with high
quality: (1) sentences aligned to ≤ two teams or
one player, and (2) sentences with triggers derived
from continuous distributions.

Now that the templates and triggers are ready
for use, we will experiment with the following
straightforward rules to perform data-to-text gen-
eration: For every game, we first generate a sen-
tence describing the scoreline result, followed by
three sentences describing other information about
team performance. While keeping that no tem-
plate is repeatedly used, we will then choose the
template with the highest score for top ten players
sorted by their game points.

6 Experiments

6.1 Experimental setup

We conducted experiments on the ROTOWIRE sub-
set of the Wiseman et al. (2017) dataset. In our
experiment, we restricted the maximum length of
word span to two as a trade-off of speed and per-
formance. Empirically, most of the phrases in the
dataset are at a length of at most two. We empiri-
cally set the expected NULL ratio to be r0 = 0.5.

We did the following pre-processing steps for
all systems in comparison: we lemmatized all to-
kens in the sentences, and filtered out sentences
containing less than five words since they are
meaningless short sentences. To utilize the game
dates, we converted them from calendar date to the
day of week, e.g. 11/28/2016 is converted to Mon-
day as a categorical value. Due to the huge noise
in the ROTOWIRE dataset, containing many sen-
tences irrelevant with their corresponding tables,
we filtered out the sentences that contain no team
or player names, or those that mention more than
2 players, as most of them are irrelevant texts.

Following Liang et al. (2009), we also used the
parameters of a simpler model without Markov de-
pendency (which was uniformly initialized) to ini-
tialize our complete model with obtained param-
eters, and then trained it until convergence. We
adapted Liang et al. (2009)’s framework to the ta-
ble schema in the ROTOWIRE dataset, and ran ex-
periments accordingly as our baseline model.

6.2 Evaluation
6.2.1 Intrinsic evaluation
It is difficult to evaluate the accuracy of tag as-
signments for the entire dataset, since the tags are
not annotated in the original data. We recruited
three human annotators with familiarity in the do-
main of basketball games to label 300 sentences
(around 8,000 tokens in total, and 30% of them
are annotated with tags) from the test set. There
exists a fraction (18%) where agreements were
not made, we included all the proposed tags from
the annotators to be correct. Also, because of the
ambiguity of annotations, we use the base names
of derived tags (e.g. Rebounds Delta) for
numerics-to-string relationship evaluation. (e.g.
Rebounds Delta is reduced to Rebounds)
Finally, we calculated the precision and recall for
non-NULL tag assignments at word-level.

Precision Recall F1
Liang09 0.319 0.643 0.426
Liang09+PR 0.397 0.640 0.490
Semi-HMMs 0.254 0.765 0.381
Semi-HMMs+PR 0.504 0.786 0.614

Table 2: Word-level tag assignment results.

Correspondence type Proportion Accuracy
numerics-to-numerics 0.369 0.950
numerics-to-string 0.283 0.402
string-to-string 0.252 0.892
category-to-string 0.095 0.936

Table 3: The performance of Semi-HMMs+PR for dif-
ferent types of correspondences.

The results are shown in Table 2. The Liang
et al. (2009) framework could still achieve around
65% recall, because there exist a large proportion
of correspondences that could be easily captured
by exact matches and simple categorical distribu-
tions. Our model without PR achieves lower pre-
cision than the baseline, because the baseline did
not model numerics-to-string relationships and en-
countered less severe issues of garbage collection.
We can observe that our initial model indeed out-
performs the baseline system in recall, while PR
helps a lot to avoid distraction from irrelevant in-
formation that should be tagged as NULL.

We also include more fine-grained results for
different types of correspondences, shown in Ta-
ble 3. As expected, numerics-to-string correspon-



3768

 
The Boston Celtics   (   7   -   5   )   blew out the Brooklyn Nets   (   2   -   11   )   120   -   95   on Friday  .

 
null -

 Team
_Nam

e -

Points_Delta -
null -

null -

Team
_Points -

null -
null -

Team
_W

ins -

Team
_Nam

e -
null -

null -
null -

Team
_City -

Team
_Losses -

null -

Team
_City -

null -

Team
_W

ins -

Team
_Losses -

Team
_Points -

Date -

(a)

Hessan Whiteside led the charge of Miami with 23 points on super efficient 10   -   of   -   11  shooting   , ...  
Player_Nam

e -  
Player_Points -

null -

 * Block -
null -

Team
_City -

null -

Player_Points -
null -

FG_Pecent -

FG_Made -
null -

null -
null -

FG_Attem
pt -

null -

FG_Pecent -

(b)

 
Burke added  18  points , including  a perfect   8   -   of   -   8   from the charity stripe ,  in  22  minutes ...

 
null -

 Player_Nam
e -

Minute -

Player_Points -

Player_Points -

FT_Percent -

* Assist -

FT_Attem
pt -

FT_Made -

Minute -
null -

null -

FT_Made -
null -

null -
null -

null -
null -

null -
null -

null -

(c)

 
Burke added  18  points , including  a perfect   8   -   of   -   8   from the charity stripe ,  in  22  minutes ...

 * FG_Made -
 * Rebound -

Player_Nam
e -

* FG3_Attem
pt -

Minute -

Player_Points -

Player_Points -

* Assist -

* FG3_Attem
pt -

FT_Percent -

* FT_Attem
pt -

* FG_Attem
pt -

* Assist -

* FG_Attem
pt -

FT_Attem
pt -

* Rebound -

FT_Made -

* FG3_Attem
pt -

* FG3_Attem
pt -

Minute -

(d)

Figure 4: Example of latent tag assignment. Words in blue dashed boxes are phrases recognized by our model.
The labels red with asterisks (*) are false assignments. (a), (b), (c) are example of Semi-HMMs-PR, while (d) is
produced from the initial Semi-HMMs.

dences are the most difficult part in this study.
Another notable thing is that although we found
that around 40% of numerics-to-numerics corre-
spondences were ambiguous due to the appear-
ance of identical values from different table cells,
our model could still achieve a high accuracy of
95.0%.

6.2.2 Extrinsic evaluation
We also tested how the derived templates could
perform in language generation, when compared
with the baseline using the same heuristics de-
scribed in Sec 5. We report automatic metrics in-
cluding BLEU scores and those based on relation
extraction as proposed by Wiseman et al. (2017):
precision & number of unique relations in genera-
tion (RG), precision & recall for content selection
(CS), and content ordering (CO) score. These au-
tomatic metrics were designed for various aspects
in NLG and may not all suit our main focus well,
so we also conducted human evaluation on infor-
mation correctness (1-5 scale ratings, the higher
the better). We asked four human raters who are
fluent in English and with familiarity in basketball
terms to rate over outputs for 30 random games.
Results are shown in Table 4. We can observe that
templates derived from our model indeed outper-

form those from the baseline.
We put some inducted templates and generated

text examples in the Appendix.

6.3 Analysis

Figure 4 shows some examples produced from our
methods. Some of the alignments are meaning-
ful, for example, the model assigned the word per-
fect with the annotation FT Percent, which rep-
resents the percentage of free throws. Without
PR, our model performed poorly by aligning many
common words to those rarely mentioned cells.
In this example, the FT Made and FT Attempt
fields in the input table both have the same value
8, making it difficult for a model without proper
local coherence modeling to distinguish between
them. Because our initial model without PR can-
not annotate NULL correctly, the Markov transi-
tion between these two numbers was intercepted
by three meaningless tokens. However, after in-
jecting the PR constraint, most of the unmentioned
words were successfully identified. The model
captured the pattern that FT Attempt almost al-
ways follows FT Made, making it correctly as-
signed these two labels.

We conducted ablation experiments for some



3769

Model RG(P%) RG(#) CS(P%) CS(R%) CO BLEU Correctness
Liang09+PR 85.83 33.29 14.33 31.09 6.25 8.34 2.60

Semi-HMMs+PR 90.47 41.79 21.63 50.17 9.63 9.45 3.58
Gold-standard 91.77 12.84 100 100 100 100 4.88

Table 4: Results for data-to-text generation (Kendall’s W=0.83 from correctness raters)

of the components (Table 5). When setting the
maximum phrase length to be k = 1, the model
degenerates to a normal HMM. The performance
measured by F1-score drops for only a little. One
possible reason is that Semi-HMMs tend to out-
put some meaningless combinations of words as
phrases, such as the phrase points on in Fig-
ure 4 (b), which could lead to many redundant
annotations that hampers precision albeit its help
to recall. We also tried to disable the transition
probabilities during both training and inference,
which led to lower precision and lower recall natu-
rally as there was no modeling for local coherence.
Finally, by canceling the NULL-skipping mecha-
nism, we found that the numerics-to-numerics an-
notation accuracy dropped from 95.0% to 88.8%.
Many of the spurious numerics-to-numerics anno-
tations, such as the 8 - of - 8 in Figure 4 (d), could
be corrected using transition probabilities under
the skipping-NULL mechanism (Figure 4 (c)).

Precision Recall F1
Semi-HMMs+PR 0.504 0.786 0.614

HMM+PR 0.545 0.694 0.610
No transition 0.468 0.633 0.538

No skip 0.454 0.737 0.562

Table 5: Ablation results.

One additional advantage of our model is that
we can easily verify what the model has captured.
For the latent annotation Team Points Delta,
we sort its corresponding phrases by weights P (c|l)P (c)
and we list the top 12 weighted words in Fig-
ure 5. We can observe that most of the displayed
phrases have strong semantic relationship with
score differences. More interestingly, we found
the mean and variance values estimated by the
Gaussian distributions rather informative. When
l = Teams Points Delta, we observed that
µl,narrowly ≈ 2 while µl,blow out ≈ 26. We could
infer the conditions under which some phrases
should be used, providing useful insights for lexi-
cal choices in language generation.

Figure 5: Mean±std for the top 12 weighted phrases
assigned with Team Points Delta. (circle sizes
are proportional to weights)

7 Conclusion

In this paper, we attempt to learn executable la-
tent semantic annotations from paired structured
tables and texts. We model the joint probability of
data fields, texts, phrasal spans, and latent annota-
tions with an adapted semi-hidden Markov model
and impose a soft statistical constraint via poste-
rior regularization. Experimental results suggest
the feasibility of the setting in study and the effec-
tiveness of our framework.

This is a preliminary study for using weak su-
pervision from structured data and texts to address
the challenging problem of language grounding.
For future study, one could collect large-scale data
and texts in other domains where more complex
grounding on phrases such as “increasing trends”
should be done. To enhance modeling power, un-
supervised discriminative models that utilize rich
features (Berg-kirkpatrick et al., 2010) could also
be explored. We are also interested in collecting
more high-quality parallel data to induce grounded
compositional logic representations.

Acknowledgements

We would like to thank Hongyuan Mei and all
the anonymous reviewers for giving helpful com-
ments on an earlier draft of this paper.



3770

References
G. Angeli, P. Liang, and D. Klein. 2010. A Sim-

ple Domain-Independent Probabilistic Approach to
Generation. In Empirical Methods in Natural Lan-
guage Processing (EMNLP).

Y. Artzi and L. S. Zettlemoyer. 2013. Weakly Super-
vised Learning of Semantic Parsers for Mapping In-
structions to Actions. Transactions of the Associa-
tion for Computational Linguistics (TACL).

T. Berg-kirkpatrick, A. Bouchard-Côté, J. DeNero, and
D. Klein. 2010. Painless unsupervised learning with
features. In North American Association for Com-
putational Linguistics (NAACL).

A. Bordes, N. Usunier, and J. Weston. 2010. Label
Ranking under Ambiguous Supervision for Learn-
ing Semantic Correspondences. In International
Conference on Machine Learning (ICML).

P. F. Brown, S. A. D. Pietra, V. J. D. Pietra, and R. L.
Mercer. 1993. The mathematics of statistical ma-
chine translation: Parameter estimation. Computa-
tional Linguistics.

D. L. Chen, J. Kim, and R. J. Mooney. 2010. Training a
multilingual sportscaster: Using perceptual context
to learn language. Journal of Artificial Intelligence
Research (JAIR).

D. L. Chen and R. J. Mooney. 2008. Learning to
sportscast: a test of grounded language acquisition.
In International Conference on Machine Learning
(ICML).

E. Choi, O. Levy, Y. Choi, and L. Zettlemoyer. 2018.
Ultra-fine entity typing. In Association for Compu-
tational Linguistics (ACL).

G. Chrupała, Á. Kádár, and A. Alishahi. 2015. Learn-
ing language through pictures. In Association for
Computational Linguistics (ACL).

J. Clarke, D. Goldwasser, M. Chang, and D. Roth.
2010. Driving Semantic Parsing from the World’s
Response. In Computational Natural Language
Learning (CoNLL).

K. Ganchev, J. V. Graça, J. Gillenwater, and B. Taskar.
2010. Posterior Regularization for Structured Latent
Variable Models. Journal of Machine Learning Re-
search (JMLR).

R. Ge and R. J. Mooney. 2006. Discriminative rerank-
ing for semantic parsing. In Association for Compu-
tational Linguistics (ACL).

P. Gorniak and D. Roy. 2007. Situated language under-
standing as filtering perceived affordances. Cogni-
tive Science.

J. V. Graça, K. Ganchev, and B. Taskar. 2010. Learn-
ing Tractable Word Alignment Models with Com-
plex Constraints. Computational Linguistics.

H. Hajishirzi, J. Hockenmaier, Erik T. Mueller, and
E. Amir. 2011. Reasoning about RoboCup Soccer
Narratives. In Uncertainty in Artificial Intelligence
(UAI).

H. Hajishirzi, M. Rastegari, A. Farhadi, and J. Hod-
gins. 2012. Semantic Understanding of Proefes-
sional Soccer Commentaries. In Uncertainty in Ar-
tificial Intelligence (UAI).

X. D. Huang, Y. Ariki, and M. A. Jack. 1990. Hidden
Markov Models for Speech Recognition. Edinburgh
University Press.

R. J. Kate and R. J. Mooney. 2007. Learning language
semantics from ambiguous supervision. In Associ-
ation for the Advancement of Artificial Intelligence
(AAAI).

C. Kiddon, L. Zettlemoyer, and Y. Choi. 2016. Glob-
ally Coherent Text Generation with Neural Checklist
Models. In Empirical Methods in Natural Language
Processing (EMNLP).

R. Koncel-Kedziorski, H. Hajishirzi, and A. Farhadi.
2014. Multi-Resolution Language Grounding with
Weak Supervision. In Empirical Methods in Natural
Language Processing (EMNLP).

J. Krishnamurthy, P. Dasigi, and M. Gardner. 2017.
Neural Semantic Parsing with Type Constraints for
Semi-Structured Tables. In Empirical Methods in
Natural Language Processing (EMNLP).

P. Liang, M. I. Jordan, and D. Klein. 2009. Learn-
ing Semantic Correspondences with Less Supervi-
sion. In Association for Computational Linguistics
and International Joint Conference on Natural Lan-
guage Processing (ACL-IJCNLP).

P. Liang, M. I. Jordan, and D. Klein. 2011. Learning
Dependency-Based Compositional Semantics. In
Association for Computational Linguistics (ACL).

X. Ling and D. S. Weld. 2012. Fine-Grained Entity
Recognition. In AAAI Conference on Artificial In-
telligence (AAAI).

H. Mei, M. Bansal, and M. R. Walter. 2016. What
to talk about and how? Selective Generation us-
ing LSTMs with Coarse-to-Fine Alignment. In
North American Association for Computational Lin-
guistics and Human Language Technology (NAACL-
HLT).

D. Misra, J. Langford, and Y. Artzi. 2017. Mapping
Instructions and Visual Observations to Actions with
Reinforcement Learning. In Empirical Methods in
Natural Language Processing (EMNLP).

S. Murakami, A. Watanabe, A. Miyazawa,
K. Goshima, T. Yanase, H. Takamura, and
Y. Miyao. 2017. Learning to Generate Market
Comments from Stock Prices. In Association for
Computational Linguistics (ACL).



3771

K. P. Murphy. 2002. Hidden Semi-Markov Models
(HSMMs).

K. Narisawa, Y. Watanabe, J. Mizuno, N. Okazaki, and
K. Inui. 2013. Is a 204 cm Man Tall or Small?
Acquisition of Numerical Common Sense from the
Web. In Association for Computational Linguistics
(ACL).

P. Pasupat and P. Liang. 2015. Compositional Semantic
Parsing on Semi-Structured Tables. In Association
for Computational Linguistics (ACL).

H. Poon. 2013. Grounded Unsupervised Semantic
Parsing. In Association for Computational Linguis-
tics (ACL).

H. Poon and P. Domingos. 2009. Unsupervised seman-
tic parsing. In Empirical Methods in Natural Lan-
guage Processing (EMNLP).

E. Reiter. 2017. You Need to Understand your Cor-
pora! The Weathergov Example.

D. Roy and E. Reiter. 2005. Connecting language to
the world. Artificial Intelligence.

S. Sarawagi and W. W. Cohen. 2005. Semi-Markov
Conditional Random Fields for Information Extrac-
tion. In Advances in Neural Information Processing
Systems (NIPS).

J. M. Siskind. 1996. A computational study of cross-
situational techniques for learning word-to-meaning
mappings. Cognition.

B. Snyder and R. Barzilay. 2007. Database-text align-
ment via structured multilabel classification. In
International Joint Conference on Artificial Intelli-
gence (IJCAI).

S. Wiseman, S. M. Shieber, and A. M. Rush. 2017.
Challenges in Data-to-Document Generation. In
Empirical Methods in Natural Language Processing
(EMNLP).

C. Yu and D. H. Ballard. 2004. On the Integration of
Grounding Language and Learning Objects. In As-
sociation for the Advancement of Artificial Intelli-
gence (AAAI).

H. Yu and J. M. Siskind. 2013. Grounded Language
Learning from Video Described with Sentences. In
Association for Computational Linguistics (ACL).

L. S. Zettlemoyer and M. Collins. 2005. Learning to
Map Sentences to Logical Form: Structured Classi-
fication with Probabilistic Categorial Grammars. In
Uncertainty in Artificial Intelligence (UAI).

D. Zhang, J. Yuan, X. Wang, and A. Foster. 2018.
Probabilistic Verb Selection for Data-to-Text Gen-
eration. Transactions of the Association for Compu-
tational Linguistics (TACL).


