Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 599–607,

Beijing, August 2010

599

Dependency-driven Anaphoricity Determination for Coreference 

Resolution

Fang Kong    Guodong Zhou    Longhua Qian    Qiaoming Zhu*

JiangSu Provincial Key Lab for Computer Information Processing Technology 
{kongfang, gdzhou, qianlonghua, qmzhu}@suda.edu.cn 

School of Computer Science and Technology Soochow University 

Abstract

This  paper  proposes  a  dependency-driven 
scheme  to  dynamically  determine  the  syn-
tactic  parse  tree  structure  for  tree  ker-
nel-based  anaphoricity  determination 
in 
coreference resolution. Given a full syntactic 
parse  tree,  it  keeps  the nodes  and  the  paths 
related  with  current  mention  based  on  con-
stituent  dependencies  from  both  syntactic 
and  semantic  perspectives,  while  removing 
the noisy information, eventually leading to 
a  dependency-driven  dynamic  syntactic 
parse tree (D-DSPT). Evaluation on the ACE 
2003  corpus  shows  that  the  D-DSPT  out-
performs all previous parse tree structures on 
anaphoricity  determination,  and  that  apply-
ing  our  anaphoricity  determination  module 
in coreference resolution achieves the so far 
best performance. 

Introduction 

1
Coreference  resolution  aims  to  identify  which 
noun  phrases  (NPs,  or  mentions)  refer  to  the 
same  real-world  entity  in a  text.  According  to 
Webber  (1979),  coreference  resolution  can  be 
decomposed into two complementary sub-tasks: 
(1)  anaphoricity  determination,  determining 
whether a given NP is anaphoric or not; and (2) 
anaphor  resolution,  linking  together  multiple 
mentions  of  a  given  entity  in  the  world.  Al-
though machine learning approaches have per-
formed  reasonably  well  in  coreference  resolu-
tion  without  explicit  anaphoricity  determina-
tion  (e.g.  Soon  et  al.  2001;  Ng  and  Cardie 
2002b;  Yang  et  al.  2003,  2008;  Kong  et  al. 
2009),  knowledge  of  NP  anaphoricity  is  ex-
pected to much improve the performance of a 
coreference 
a 

resolution 

system, 

since 

                                                          
* Corresponding author 

non-anaphoric NP does not have an antecedent 
and therefore does not need to be resolved. 

Recently,  anaphoricity  determination  has 
been  drawing  more  and  more  attention.  One 
common approach involves the design of some 
heuristic  rules  to  identify  specific  types  of 
non-anaphoric  NPs,  such  as  pleonastic  it  (e.g. 
Paice and Husk 1987; Lappin and Leass 1994, 
Kennedy  and  Boguraev  1996;  Denber  1998) 
and definite descriptions (e.g. Vieira and Poe-
sio 2000). Alternatively, some studies focus on 
using  statistics  to  tackle  this  problem  (e.g., 
Bean  and  Riloff  1999;  Bergsma  et  al.  2008) 
and others apply machine learning approaches 
(e.g.  Evans  2001;Ng  and  Cardie  2002a, 
2004,2009;  Yang  et  al.  2005;  Denis  and  Bal-
bridge  2007;  Luo  2007;  Finkel  and  Manning 
2008; Zhou and Kong 2009).   

As  a  representative,  Zhou  and  Kong  (2009) 
directly employ a tree kernel-based method to 
automatically mine the non-anaphoric informa-
tion embedded in the syntactic parse tree. One 
main advantage of the kernel-based methods is 
that  they  are  very  effective  at  reducing  the 
burden  of  feature  engineering  for  structured 
objects. Indeed, the kernel-based methods have 
been  successfully  applied  to  mine  structured 
information  in  various  NLP  applications  like 
syntactic  parsing  (Collins  and  Duffy,  2001; 
Moschitti,  2004),  semantic  relation  extraction 
(Zelenko  et  al.,  2003;  Zhao  and  Grishman, 
2005; Zhou et al. 2007; Qian et al., 2008), se-
mantic role labeling (Moschitti, 2004); corefer-
ence resolution (Yang et al., 2006; Zhou et al., 
2008).  One  of  the  key  problems  for  the  ker-
nel-based methods is how to effectively capture 
the structured information according to the na-
ture of the structured object in the specific task. 
This paper advances the state-of-the-art per-
formance  in  anaphoricity  determination  by  ef-

600

fectively  capturing  the  structured  syntactic  in-
formation  via  a  tree  kernel-based  method.  In 
particular,  a  dependency-driven  scheme 
is 
proposed to dynamically determine the syntac-
tic  parse  tree  structure  for  tree  kernel-based 
anaphoricity  determination  by  exploiting  con-
stituent  dependencies  from  both  the  syntactic 
and  semantic  perspectives  to  keep  the  neces-
sary  information  in  the  parse  tree  as  well  as 
remove the noisy information. Our motivation 
is to employ critical dependency information in 
constructing  a  concise  and  effective  syntactic 
parse  tree  structure,  specifically  targeted  for 
tree kernel-based anaphoricity determination.   

The rest of this paper is organized as follows. 
Section 2 briefly describes the related work on 
both  anaphoricity  determination  and  exploring 
syntactic  parse  tree  structures  in  related  tasks. 
Section  3  presents  our  dependency-driven 
scheme  to  determine  the  syntactic  parse  tree 
structure.  Section  4  reports  the  experimental 
results. Finally, we conclude our work in Sec-
tion 5. 
2 Related Work 
This section briefly overviews the related work 
on  both  anaphoricity  determination  and  ex-
ploring syntactic parse tree structures. 
2.1 Anaphoricity Determination 
Previous  work  on  anaphoricity  determination 
can  be  broadly  divided  into  three  categories: 
heuristic  rule-based  (e.g.  Paice  and  Husk 
1987;Lappin  and  Leass  1994;  Kennedy  and 
Boguraev 1996; Denber 1998; Vieira and Poe-
sio  2000;  Cherry  and  Bergsma  2005),  statis-
tics-based  (e.g.  Bean  and  Riloff  1999;  Cherry 
and  Bergsma  2005;  Bergsma  et  al.  2008)  and 
learning-based  methods  (e.g.  Evans  2001;  Ng 
and Cardie 2002a; Ng 2004; Yang et al. 2005; 
Denis  and  Balbridge  2007;  Luo  2007;  Finkel 
and Manning 2008; Zhou and Kong 2009; Ng 
2009).   

The  heuristic  rule-based  methods  focus  on 
designing some heuristic rules to identify spe-
cific  types  of  non-anaphoric  NPs.  Representa-
tive  work  includes:  Paice  and  Husk  (1987), 
Lappin  and  Leass  (1994)  and  Kennedy  and 
Boguraev  (1996).  For  example,  Kennedy  and 
Boguraev  (1996)  looked  for  modal  adjectives 
(e.g. “necessary”) or cognitive verbs (e.g. “It is 

thought that…” in a set of patterned construc-
tions) in identifying pleonastic it.

Among  the  statistics-based  methods,  Bean 
and  Riloff  (1999)  automatically  identified  ex-
istential definite NPs which are non-anaphoric.   
The intuition behind is that many definite NPs 
are not anaphoric since their meanings can be 
understood from general world knowledge, e.g. 
“the FBI”. They found that existential NPs ac-
count for 63% of all definite NPs and 76% of 
them could be identified by syntactic or lexical 
means.  Cherry  and  Bergsma  (2005)  extended 
the  work  of  Lappin  and  Leass  (1994)  for 
large-scale anaphoricity determination by addi-
tionally  detecting  pleonastic it.  Bergsma  et  al. 
(2008) proposed a distributional method in de-
tecting non-anaphoric pronouns. They first ex-
tracted the surrounding context of the pronoun 
and gathered the distribution of words that oc-
curred within the context from a large corpus, 
and  then  identified  the  pronoun  either  ana-
phoric or non-anaphoric based on the word dis-
tribution.

various 

(2002a) 

identified 

employed 

automatically 

Among  the  learning-based  methods,  Evans 
(2001) 
the 
non-anaphoricity  of  pronoun  it  using  various 
kinds of lexical and syntactic features. Ng and 
Cardie 
do-
main-independent  features  in  identifying  ana-
phoric NPs. They trained an anaphoricity clas-
sifier to determine whether a NP was anaphoric 
or not, and employed an independently-trained 
coreference  resolution  system  to  only  resolve 
those  mentions  which  were  classified  as  ana-
phoric. Experiments showed that their method 
improved 
the  performance  of  coreference 
resolution  by  2.0  and  2.6  to  65.8  and  64.2  in 
F1-measure  on  the  MUC-6  and  MUC-7  cor-
pora,  respectively.  Ng  (2004)  examined  the 
representation and optimization issues in com-
puting  and  using  anaphoricity  information  to 
improve learning-based coreference resolution. 
On  the  basis,  he  presented  a  corpus-based  ap-
proach  (Ng,  2009)  for  achieving  global  opti-
mization by representing anaphoricity as a fea-
ture in coreference resolution. Experiments on 
the ACE 2003 corpus showed that their method 
improved  the  overall  performance  by  2.8,  2.2 
and  4.5  to  54.5,  64.0  and  60.8  in  F1-measure 
on  the  NWIRE,  NPAPER  and  BNEWS  do-
mains, respectively. However, he did not look 
into  the  contribution  of  anaphoricity  determi-

601

improved 

nation  on  coreference  resolution  of  different 
NP  types.  Yang  et  al.  (2005)  made  use  of 
non-anaphors to create a special class of train-
ing  instances  in  the  twin-candidate  model 
(Yang et al. 2003) and improved the perform-
ance  by  2.9  and  1.6  to  67.3  and  67.2  in 
F1-measure  on  the  MUC-6  and  MUC-7  cor-
pora, respectively. However, their experiments 
show  that  eliminating  non-anaphors  using  an 
anaphoricity determination  module  in  advance 
harms  the  performance.  Denis  and  Balbridge 
(2007)  employed  an  integer  linear  program-
ming (ILP) formulation for coreference resolu-
tion  which  modeled  anaphoricity  and  corefer-
ence as a joint task, such that each local model 
informed  the  other  for  the  final  assignments. 
Experiments on the ACE 2003 corpus showed 
that  this  joint  anaphoricity-coreference  ILP 
formulation 
the  F1-measure  by 
3.7-5.3  on  various  domains.  However,  their 
experiments assume true ACE mentions (i.e. all 
the ACE mentions are already known from the 
annotated  corpus).  Therefore,  the  actual  effect 
of  this  joint  anaphoricity-coreference  ILP  for-
mulation  on  fully  automatic  coreference  reso-
lution  is  still  unclear.  Luo  (2007)  proposed  a 
twin-model  for  coreference  resolution:  a  link 
component,  which  models  the  coreferential 
relationship  between  an  anaphor  and  a  candi-
date  antecedent,  and  a  creation  component, 
which models the possibility that a NP was not 
coreferential  with  any  candidate  antecedent. 
This  method  combined  the  probabilities  re-
turned  by  the  creation  component  (an  ana-
phoricity  model)  with  the  link  component  (a 
coreference model) to score a coreference par-
tition,  such  that  a  partition  was  penalized 
whenever  an  anaphoric  mention  was  resolved. 
Finkel and Manning (2008) showed that transi-
tivity constraints could be incorporated into an 
ILP-based  coreference  resolution  system  and 
much  improved  the  performance.  Zhou  and 
Kong  (2009)  employed  a  global 
learning 
method in determining the anaphoricity of NPs 
via  a  label  propagation  algorithm  to  improve 
learning-based  coreference  resolution.  Experi-
ments  on  the  ACE  2003  corpus  demonstrated 
that  this  method  was  very  effective.  It  could 
improve the F1-measure by 2.4, 3.1 and 4.1 on 
the  NWIRE,  NPAPER  and  BNEWS  domains, 
respectively.  Ng  (2009)  presented  a  novel  ap-
proach  to  the  task  of  anaphoricity  determina-

tion based on graph minimum cuts and demon-
strated  the  effectiveness  in  improving  a  learn-
ing-based coreference resolution system. 

Syntactic Parse Tree Structures 

In summary, although anaphoricity determi-
nation  plays  an  important  role  in  coreference 
resolution  and  achieves  certain  success  in  im-
proving the overall performance of coreference 
resolution, its contribution is still far from ex-
pectation.
2.2
For a tree kernel-based method, one key prob-
lem is how to represent and capture the struc-
tured  syntactic 
information.  During  recent 
years, various tree kernels, such as the convo-
lution  tree  kernel  (Collins  and  Duffy,  2001), 
the  shallow  parse  tree  kernel  (Zelenko  et  al 
2003)  and  the  dependency  tree  kernel  (Culota 
and Sorensen, 2004), have been proposed in the 
literature.  Among  these  tree  kernels,  the  con-
volution  tree  kernel  represents  the  state-of-the 
art  and  has  been  successfully  applied  by 
Collins and Duffy (2002) on syntactic parsing, 
Zhang et al. (2006) on semantic relation extrac-
tion and Yang et al. (2006) on pronoun resolu-
tion.

and 

that 

found 

Given a tree kernel, the key issue is how to 
generate a syntactic parse tree structure for ef-
fectively  capturing  the  structured  syntactic  in-
formation.  In  the  literature,  various  parse  tree 
structures have been proposed and successfully 
applied in some NLP applications. As a repre-
sentative, Zhang et al. (2006) investigated five 
parse  tree  structures  for  semantic  relation  ex-
traction 
the  Shortest 
Path-enclosed  Tree  (SPT)  achieves  the  best 
performance on the 7 relation types of the ACE 
RDC  2004  corpus.  Yang  et  al.  (2006)  con-
structed  a  document-level  syntactic  parse  tree 
for an entire text by attaching the parse trees of 
all its sentences to a new-added upper node and 
examined  three  possible  parse  tree  structures 
(Min-Expansion, 
and 
Full-Expansion)  that  contain  different  sub-
structures of the parse tree for pronoun resolu-
tion.  Experiments  showed  that  their  method 
achieved  certain  success  on  the  ACE  2003 
corpus  and  the  simple-expansion  scheme  per-
forms best. However, among the three explored 
schemes, there exists no obvious overwhelming 
one, which can well cover structured syntactic 
information. One problem of Zhang et al. (2006) 

Simple-Expansion 

602

and  Yang  et  al.  (2006)  is  that  their  parse  tree 
structures are context-free and do not consider 
the  information  outside  the  sub-trees.  Hence, 
their  ability  of  exploring  structured  syntactic 
information  is  much  limited.  Motivated  by 
Zhang  et  al.  (2006)  and  Yang  et  al.  (2006), 
Zhou et al. (2007) extended the SPT to become 
context-sensitive  (CS-SPT)  by  dynamically 
including  necessary  predicate-linked  path  in-
formation. Zhou et al. (2008) further proposed 
a  dynamic-expansion  scheme  to  automatically 
determine  a  proper  parse  tree  structure  for 
pronoun  resolution  by  taking  predicate-  and 
antecedent  competitor-related  information  in 
consideration.  Evaluation  on  the  ACE  2003 
corpus  showed  that  the  dynamic-expansion 
scheme  can  well  cover  necessary  structured 
information in the parse tree for pronoun reso-
lution. One problem with the above parse tree 
structures is that they may still contain unnec-
essary  information  and  also  miss  some  useful 
context-sensitive information. Qian et al. (2008) 
dynamically determined the parse tree structure 
for  semantic  relation  extraction  by  exploiting 
constituent dependencies to keep the necessary 
information in the parse tree as well as remove 
the noisy information. Evaluation on the ACE 
RDC  2004  corpus  showed  that  their  dynamic 
syntactic  parse  tree  structure  outperforms  all 
previous  parse  tree  structures.  However,  their 
solution  has  the  limitation  in  that  the  depend-
encies  were  found  according  to  some  manu-
ally-written  ad-hoc  rules  and  thus  may  not  be 
easily applicable to new domains and applica-
tions.

This  paper  proposes  a  new  scheme  to  dy-
namically  determine  the  syntactic  parse  tree 
structure  for  anaphoricity  determination  and 
systematically studies the application of an ex-
plicit  anaphoricity  determination  module  in 
improving coreference resolution. 
3 Dependency-driven  Dynamic  Syn-

tactic Parse Tree 

Given  a  full  syntactic  parse  tree  and  a  NP  in 
consideration, one key issue is how to choose a 
proper  syntactic  parse  tree  structure  to  well 
cover  structured  syntactic  information  in  the 
tree kernel computation. Generally, the more a 
syntactic parse tree structure includes, the more 
structured  syntactic 
information  would  be 

available, at the expense of more noisy (or un-
necessary) information.   

It  is  well  known  that  dependency  informa-
tion  plays  a  key  role  in  many  NLP  problems, 
such  as  syntactic  parsing,  semantic  role  label-
ing as well as semantic relation extraction. Mo-
tivated  by  Qian  et  al.  (2008)  and  Zhou  et  al. 
(2008),  we  propose  a  new  scheme  to  dynami-
cally  determine  the  syntactic  parse  tree  struc-
ture for anaphoricity determination by exploit-
ing  constituent  dependencies  from  both  the 
syntactic  and  semantic  perspectives  to  distin-
guish  the  necessary  evidence  from  the  unnec-
essary  information  in  the  syntactic  parse  tree. 
That  is,  constituent  dependencies  are  explored 
from  two  aspects:  syntactic  dependencies  and 
semantic dependencies.   
1)  Syntactic  Dependencies:  The  Stanford  de-
pendency  parser1  is  employed  as  our  syn-
tactic  dependency  parser  to  automatically 
extract  various  syntactic  (i.e.  grammatical) 
dependencies  between  individual  words.  In 
this  paper,  only  immediate  syntactic  de-
pendencies  with  current  mention  are  con-
sidered. The intuition behind is that the im-
mediate  syntactic  dependencies  carry  the 
major  contextual  information  of  current 
mention.

2)  Semantic  Dependencies:  A  state-of-the-art 
semantic  role  labeling  (SRL)  toolkit  (Li  et 
al. 2009) is employed for extracting various 
semantic  dependencies  related  with  current 
mention. In this paper, semantic dependen-
cies  include  all  the  predicates  heading  any 
node in the root path from current mention 
to the root node and their compatible argu-
ments  (except  those  overlapping  with  cur-
rent mention). 

We name our parse tree structure as a depend-
ency-driven  dynamic  syntactic  parse 
tree 
(D-DSPT). The intuition behind is that the de-
pendency 
information  related  with  current 
mention  in  the  same  sentence  plays  a  critical 
role  in  anaphoricity  determination.  Given  the 
sentence  enclosing  the  mention  under  consid-
eration,  we  can  get  the  D-DSPT  as  follows: 
(Figure 1 illustrates an example of the D-DSPT 
generation  given  the  sentence  “Mary  said  the 
woman in the room bit her” with “woman” as 
current mention.) 
                                                          
1 http://nlp.stanford.edu/software/lex-parser.shtml

603

Figure 1:    An example of generating the dependency-driven dynamic syntactic parse tree   

                     

1) Generating the full syntactic parse tree of 
the given sentence using a full syntactic parser. 
In  this  paper,  the  Charniak  parser  (Charniak 
2001) is employed and Figure 1 (a) shows the 
resulting full parse tree. 
2) Keeping  only  the  root  path  from  current 
mention to the root node of the full parse tree. 
Figure 1(b) shows the root path corresponding 
to  the  current  mention  “woman”.  In  the  fol-
lowing steps, we attach the above two types of 
dependency information to the root path.   
3) Extracting  all  the  syntactic  dependencies 
in  the  sentence  using  a  syntactic  dependency 
parser, and attaching all the nodes, which have 
immediate  dependency  relationship  with  cur-
rent mention, and their corresponding paths to 
the root path. Figure 1(c) illustrates the syntac-
tic  dependences  extracted  from  the  sentence, 
where  the  ones  in  italic  mean  immediate  de-
pendencies  with  current  mention.  Figure  1(d) 
shows the parse tree structure after considering 
syntactic dependencies. 
4) Attaching  all  the  predicates  heading  any 
node  in  the  root  path  from  current  mention  to 
the root node and their corresponding paths to 
the root path. For the example sentence, there 
are two predicates “said” and “bit”, which head 
the  “VP”  and  “S”  nodes  in  the  root  path  re-

spectively. Therefore, these two predicates and 
their corresponding paths should be attached to 
the root path as shown in Figure 1(e). Note that 
the  predicate  “bit”  and  its  corresponding  path 
has already been attached in Stop (3). As a re-
sult,  the  predicate-related  information  can  be 
attached. According to Zhou and Kong (2009), 
such  information  is  important  to  definite  NP 
resolution.
5) Extracting  the  semantic  dependencies  re-
lated  with  those  attached  predicates  using  a 
(shallow) semantic parser, and attaching all the 
compatible  arguments  (except  those  overlap-
ping  with  current  mention)  and  their  corre-
sponding  paths  to  the  root  path.  For  example, 
as  shown  in  Figure  1(e),  since  the  arguments 
“Mary” and “her” are compatible with current 
mention  “woman”,  these  two  nodes  and  their 
corresponding paths are attached while the ar-
gument “room” is not since its gender does not 
agree with current mention. 

In  this  paper,  the  similarity  between  two 
parse trees is measured using a convolution tree 
kernel,  which  counts  the  number  of  common 
sub-tree  as  the  syntactic  structure  similarity 
between  two  parse  trees.  For  details,  please 
refer to Collins and Duffy (2001). 

604

Experimental Setting 

4 Experimentation and Discussion 
This  section  evaluates  the  performance  of  de-
pendency-driven  anaphoricity  determination 
and its application in coreference resolution on 
the ACE 2003 corpus. 
4.1
The ACE 2003 corpus contains three domains: 
newswire  (NWIRE),  newspaper  (NPAPER), 
and  broadcast  news  (BNEWS).  For  each  do-
main,  there  exist  two  data  sets,  training  and 
devtest, which are used for training and testing.   
For  preparation,  all  the  documents  in  the 
corpus  are  preprocessed  automatically  using  a 
pipeline  of  NLP  components,  including  to-
kenization  and  sentence  segmentation,  named 
entity  recognition,  part-of-speech  tagging  and 
noun  phrase  chunking.  Among  them,  named 
entity  recognition,  part-of-speech  tagging  and 
noun  phrase  chunking  apply 
same 
state-of-the-art  HMM-based  engine  with  er-
ror-driven  learning  capability  (Zhou  and  Su, 
2000 & 2002). Our statistics finds that 62.0%, 
58.5%  and  61.4%  of  entity  mentions  are  pre-
served  after  preprocessing  on  the  NWIRE, 
NPAPER  and  BNEWS  domains  of  the  ACE 
2003  training  data  respectively  while  only 
89.5%, 89.2% and 94% of entity mentions are 
preserved after preprocessing on    the NWIRE, 
NPAPER  and  BNEWS  domains  of  the  ACE 
2003 devtest data. This indicates the difficulty 
of coreference resolution. In addition, the cor-
pus  is  parsed  using  the  Charniak  parser  for 
syntactic parsing and the Stanford dependency 
parser  for  syntactic  dependencies  while  corre-
sponding  semantic  dependencies  are  extracted 
using  a  state-of-the-art  semantic  role  labeling 
toolkit  (Li  et  al.  2009).  Finally,  we  use  the 
SVM-light2  toolkit  with  the  tree  kernel  func-
tion as the classifier. For comparison purpose, 
the  training  parameters  C  (SVM)  and  (cid:172)(tree
kernel)  are  set  to  2.4  and  0.4  respectively,  as 
done in Zhou and Kong (2009).   

the 

For  anaphoricity  determination,  we  report 
the  performance  in  Acc+  and  Acc-,  which 
measure the accuracies of identifying anaphoric 
NPs and non-anaphoric NPs, respectively. Ob-
viously,  higher  Acc+  means  that  more  ana-
phoric NPs would be identified correctly, while 
                                                          
2  http://svmlight.joachims.org/ 

Experimental Results 

higher  Acc-  means  that  more  non-anaphoric 
NPs  would  be  filtered  out.  For  coreference 
resolution, we report the performance in terms 
of  recall,  precision,  and  F1-measure  using  the 
commonly-used model theoretic MUC scoring 
program (Vilain et al. 1995). To see whether an 
improvement  is  significant,  we  also  conduct 
significance  testing  using  paired  t-test.  In  this 
paper, ‘***’, ‘**’ and ‘*’ denote p-values of an 
improvement  smaller  than  0.01,  in-between 
(0.01, 0,05] and bigger than 0.05, which mean 
significantly  better,  moderately  better  and 
slightly better, respectively. 
4.2
Performance of anaphoricity determination 
Table 1 presents the performance of anaphoric-
ity  determination  using  the  convolution  tree 
kernel  on  D-DSPT.  It  shows  that  our  method 
achieves 
the  accuracies  of  83.27/77.13, 
86.77/80.25  and  90.02/64.24  on  identifying 
anaphoric/non-anaphoric  NPs  in  the  NWIRE, 
NPAPER  and  BNEWS  domains,  respectively.   
This suggests that our approach can effectively 
filter out about 75% of non-anaphoric NPs and 
keep  about  85%  of  anaphoric  NPs.  In  com-
parison,  in  the  three  domains  Zhou  and  Kong 
(2009)  achieve  the  accuracies  of  76.5/82.3, 
78.9/81.6 and 74.3/83.2, respectively, using the 
tree  kernel  on  a  dynamically-extended  tree 
(DET). This suggests that their method can fil-
ter  out  about  82%  of  non-anaphoric  NPs  and 
only  keep  about  76%  of  anaphoric  NPs.  In 
comparison,  their  method  outperforms  our 
method  on  filtering  out  more  non-anaphoric 
NPs  while  our  method  outperforms 
their 
method  on  keeping  more  anaphoric  NPs  in 
coreference  resolution.  While  a  coreference 
resolution 
some 
non-anaphoric  NPs  (when  failing  to  find  the 
antecedent  candidate),  filtering  out  anaphoric 
NPs in anaphoricity determination would defi-
nitely cause errors and it is almost impossible 
to recover. Therefore, it is normally more im-
portant  to  keeping  more  anaphoric  NPs  than 
filtering out more non-anaphoric NPs. Table 1 
further presents the performance of anaphoric-
ity  determination  on  different  NP  types.  It 
shows that our method performs best at keep-
ing  pronominal  NPs  and  filtering  out  proper 
NPs.

system 

detect 

can 

605

NWIRE 

NPAPER 

BNEWS 
NP Type 
Acc+ Acc- Acc+ Acc- Acc+ Acc-
54.03 
95.07
Pronoun 
71.77 
84.61
Proper NP 
53.65 
Definite NP 
87.17
Indefinite NP  86.01
47.32 
64.24 
83.27

50.36
83.17
46.74
47.52
77.13

96.40
83.78
82.24
80.63
86.77

56.44
79.62
49.18
48.45
80.25

98.26
87.61
86.87
89.71
90.02

Over all 

Table 1: Performance of anaphoricity determination using the D-DSPT   

NWIRE 

Performance Change 

BNEWS 
Acc+ Acc- Acc+ Acc- Acc+ Acc-
64.24 
83.27
60.20 
78.67
-Syntactic Dependencies
-Semantic Dependencies
81.67
60.67 
Table 2: Contribution of including syntactic and semantic dependencies   

77.13
72.56
76.74

86.77
80.14
83.47

80.25
73.74
77.93

90.02
87.05
89.58

NPAPER 

D-DSPT 

in D-DSPT on anaphoricity determination   
NPAPER 

NWIRE 

Pronoun 
Proper NP 
Definite NP 
Indefinite NP 

Over all 
Pronoun 
Proper NP 
Definite NP 
Indefinite NP 

R% 
70.0 
76.3 
47.9 
23.6 
53.2 
67.7 
76.3 
42.5 
20.3 
50.1 
69.1 
78.6 
45.2 
40.9 
51.9 
Table 3: Performance of anaphoricity determination on coreference resolution 

P%
57.9 
80.1 
43.4 
26.3 
63.8 
70.2 
81.0 
  63.1 
55.3 
79.6 
71.5 
89.3 
85.9 
67.6 
81.7 

F 
69.4 
82.7 
45.6 
29.0 
63.5 
75.5 
83.1 
47.3 
41.2 
67.4 
77.7 
86.2 
58.8 
50.5 
69.6 

R%
70.8 
80.3 
35.9 
40.3 
55.0 
65.9 
80.3 
32.3 
36.4 
52.4 
68.6 
81.7 
41.8 
40.3 
54.6 

P% 
63.5 
83.6 
48.5 
22.9 
65.0 
78.7 
85.1 
61.7 
50.7 
80.3 
80.4 
90.1 
85.2 
65.1 
82.1 

R% 
76.5 
81.8 
43.1 
39.7 
62.1 
72.6 
81.2 
38.4 
34.7 
58.1 
75.2 
82.6 
44.9 
41.2 
60.4 

Over all 
Pronoun 
Proper NP 
Definite NP 
Indefinite NP 

F 
63.7
80.2
39.2
31.8
59.1
68.0
80.6
42.7
43.9
63.2
70.1
85.3
56.2
50.5
65.5

Over all 

BNEWS 
P% 
60.3 
76.8 
51.9 
10.7 
60.5 
75.8 
84.4 
66.4 
45.4 
79.8 
77.8 
88.7 
87.9 
50.1 
82.1 

F 
64.8
76.6
49.8
14.7
56.6
71.5
80.1
51.8
28.1
61.6
73.5
83.3
59.7
45.1
63.6

System 

Without ana-
phoricity de-
termination 
(Baseline)

With D-DSPT 
-based ana-
phoricity de-
termination 

With golden 
anaphoricity
determination 

System 

Without anaphoricity determina-
Zhou and 
Kong (2009)  With Dynamically Extended 
Tree-based anaphoricity determi-

tion (Baseline)

nation

Without anaphoricity determina-
Ng (2009)  With Graph Minimum Cut-based 

tion (Baseline)

anaphoricity determination

NWIRE 

NPAPER 

BNEWS 

R% P% F
53.1 67.4 59.4 57.7
51.6 77.2 61.8 55.2

R% P%
67.0
78.6

R% P% 

F 
F 
62.1  48.0  65.9  55.5
65.2  47.5  80.3  59.6

59.1

58.

58.6 60.8

54.1 69.0 60.6 57.9

62.6

71.2

61.7  57.7  52.6  55.0

63.9  53.1  67.5  59.4

Table 4: Performance comparison with other systems 

Table  2  further  presents  the  contribution  of 
including syntactic and semantic dependencies 
in  the  D-DSPT  on  anaphoricity  determination 
by excluding one or both of them. It shows that 
both  syntactic  dependencies  and  semantic  de-
pendencies contribute significantly (***). 
Performance of coreference resolution 
the  effect  of  our 
We  have  evaluated 
D-DSPT-based 
anaphoricity  determination 
module on coreference resolution by including 

it as a preprocessing step to a baseline corefer-
ence  resolution  system  without  explicit  ana-
phoricity  determination,  by  filtering  our  those 
non-anaphoric NPs according to the anaphoric-
ity  determination  module.  Here,  the  baseline 
system  employs  the  same  set  of  features,  as 
adopted in the single-candidate model of Yang 
et  al.  (2003)  and  uses  a  SVM-based  classifier 
with  the  feature-based  RBF  kernel.  Table  3 
presents 
the 
coreference  resolution  system  without  ana-

the  detailed  performance  of 

606

phoricity  determination,  with  D-DSPT-based 
anaphoricity  determination  and.  with  golden 
anaphoricity determination. Table 3 shows that: 
1) There is a performance gap of 6.4, 6.1 and 
7.0  in  F1-measure  on  the  NWIRE,  NPAPER 
and BNEWS domain, respectively, between the 
coreference  resolution  system  with  golden 
anaphoricity  determination  and  the  baseline 
system  without  anaphoricity  determination. 
This  suggests  the  usefulness  of  proper  ana-
phoricity  determination  in  coreference  resolu-
tion.  This  also  agrees  with  Stoyanov  et  al. 
(2009)  which  measured  the  impact  of  golden 
anaphoricity  determination  on  coreference 
resolution using only the annotated anaphors in 
both training and testing.   
2) Compared  to  the  baseline  system  without 
anaphoricity determination, the D-DSPT-based 
anaphoricity  determination  module  improves 
the  performance  by  4.1(***),  3.9(***)  and 
5.0(***) to 63.2, 67.4 and 61.6 in F1-measure 
on  the  NWIRE,  NPAPER  and  BNEWS  do-
mains, respectively, due to a large gain in pre-
cision  and  a  much  smaller  drop  in  recall.  In 
addition, D-DSPT-based anaphoricity determi-
nation  can  not  only  much  improve  the  per-
formance  of  coreference  resolution  on  pro-
nominal  NPs  (***)  but  also  on  definite 
NPs(***)  and  indefinite  NPs(***)  while  the 
improvement  on  proper  NPs  can  be  ignored 
due to the fact that proper NPs can be well ad-
dressed  by  the  simple  abbreviation  feature  in 
the baseline system. 
3) D-DSPT-based anaphoricity determination 
still  lags  (2.3,  2.2  and  2.0  on  the  NWIRE, 
NPAPER  and  BNEWS  domains,  respectively) 
behind  golden  anaphoricity  determination  in 
improving  the  overall  performance  of  corefer-
ence resolution. This suggests that there exists 
some  room  in  the  performance  improvement 
for anaphoricity determination. 
Performance comparison with other systems 
Table 4 compares the performance of our sys-
tem with other systems. Here, Zhou and Kong 
(2009) use the same set of features with ours in 
the 
dynami-
cally-extended  tree  structure  in  anaphoricity 
determination.  Ng  (2009)  uses  33  features  as 
described in Ng (2007) and a graph minimum 
cut algorithm in anaphoricity determination. It 
shows  that  the  overall  performance  of  our 

baseline 

system 

and 

a 

baseline  system  is  almost  as  good  as  that  of 
Zhou  and  Kong  (2009)  and  a  bit  better  than 
Ng’s (2009).   

For  overall  performance,  our  coreference 
resolution  system  with  D-DSPT-based  ana-
phoricity  determination  much  outperforms 
Zhou  and  Kong  (2009)  in  F1-measure  by  1.4, 
2.2  and  2.0  on  the  NWIRE,  NPAPER  and 
BNEWS domains, respectively, due to the bet-
ter  inclusion  of  dependency  information.  De-
tailed evaluation shows that such improvement 
comes  from  coreference  resolution  on  both 
pronominal  and  definite  NPs  (Please  refer  to 
Table  6  in  Zhou  and  Kong,  2009).  Compared 
with Zhou and Kong (2009) and Ng (2009), our 
approach  achieves  the  best  F1-measure  so  far 
for each dataset. 

5 Conclusion and Further Work 
This  paper  systematically  studies  a  depend-
ency-driven  dynamic  syntactic  parse 
tree 
(DDST) for anaphoricity determination and the 
application  of  an  explicit  anaphoricity  deter-
mination  module  in  improving  learning-based 
coreference resolution. Evaluation on the ACE 
2003  corpus 
that  D-DSPT-based 
anaphoricity determination much improves the 
performance of coreference resolution. 

indicates 

To our best knowledge, this paper is the first 
research  which  directly  explores  constituent 
dependencies  in  tree  kernel-based  anaphoricty 
determination from both syntactic and semantic 
perspectives. 

For  further  work,  we  will  explore  more 
structured syntactic information in coreference 
resolution. In addition, we will study the inter-
action between anaphoricity determination and 
coreference  resolution  and  better 
integrate 
anaphoricity  determination  with  coreference 
resolution.

Acknowledgments 
This  research  was  supported  by  Projects 
60873150, 60970056, and 90920004 under the 
National Natural Science Foundation of China, 
Project  200802850006  and  20093201110006 
under  the  Specialized  Research  Fund  for  the 
Doctoral  Program  of  Higher  Education  of 
China.

607

of  noun  phrase.  Computational  Linguistics, 
27(4):521-544. 

V.  Stoyanov,  N.  Gilbert,  C.  Cardie  and  E.  Riloff. 
2009.  Conundrums  in  Noun  Phrase  Coreference 
Resolution: Making Sense of the State-of-the Art. 
ACL’2009 

B.  L.  Webber.  1979.  A  Formal  Approach  to  Dis-

course Anaphora. Garland Publishing, Inc. 

X.F. Yang, G.D. Zhou, J. Su and C.L. Chew. 2003. 
Coreference  Resolution  Using  Competition 
Learning Approach. ACL’2003 

X.F.  Yang,  J.  Su  and  C.L.  Chew.  2005.  A  Twin 
Candidate Model of Coreference Resolution with 
Non-Anaphor 
Capability. 
IJCNLP’2005 

Identification 

X.F.  Yang,  J.  Su  and  C.L.  Chew.  2006.  Ker-
nel-based  pronoun  resolution  with  structured 
syntactic knowledge. COLING-ACL’2006 

X.F.  Yang,  J.  Su  and  C.L.  Tan  2008.  A 
Twin-Candidate  Model 
for  Learning-Based 
Anaphora Resolution. Computational Linguistics 
34(3):327-356 

M. Zhang, J. Zhang, J. Su and G.D. Zhou. 2006. A 
composite kernel to extract relations between en-
tities  with  both  flat  and  structured  features. 
COLING/ACL’2006 

S. Zhao and R. Grishman. 2005. Extracting relations 
with integered information using kernel methods. 
ACL’2005 

D.  Zelenko,  A.  Chinatsu  and  R.  Anthony.  2003. 
Kernel methods for relation extraction. Machine 
Learning Researching 3(2003):1083-1106 

G.D.  Zhou,  F.  Kong  and  Q.M.  Zhu.  2008.  Con-
text-sensitive convolution tree kernel for pronoun 
resolution. IJCNLP’2008 

G.D. Zhou and F. Kong. 2009. Global Learning of 
Noun Phrase Anaphoricity in Coreference Reso-
lution via Label Propagetion. EMNLP’2009 

G.D. Zhou and J. Su. 2002. Named Entity recogni-
tagger. 

tion  using  a  HMM-based  chunk 
ACL’2002 

G.D. Zhou, M. Zhang, D.H. Ji and Q.M. Zhu. 2007. 
Tree  kernel-based  relation  extraction  with  con-
text-sensitive  structured  parse  tree  information. 
EMNLP/CoNLL’2007

References 
D. Bean and E. Riloff 1999. Corpus-based Identifi-
cation  of  Non-Anaphoric  Noun  Phrases.  ACL’ 
1999 

S. Bergsma, D. Lin and R. Goebel 2008. Distribu-
tional Identification of Non-referential Pronouns. 
ACL’2008 

C.  Cherry  and  S.  Bergsma.  2005.  An  expectation 
maximization  approach  to  pronoun  resolution. 
CoNLL’2005 

M. Collins and N. Duffy. 2001. Covolution kernels 

for natural language. NIPS(cid:835)2001   

M. Denber 1998. Automatic Resolution of Anapho-
ria  in  English.  Technical  Report,  Eastman  Ko-
dakCo. 

P.  Denis  and  J.  Baldridge.  2007.  Global,  joint  de-
termination  of  anaphoricity  and  coreference 
resolution 
programming. 
using 
NAACL/HLT’2007

integer 

R. Evans 2001. Applying machine learning toward 
an  automatic  classification  of  it.  Literary  and 
Linguistic Computing, 16(1):45-57 

F. Kong, G.D. Zhou and Q.M. Zhu. 2009 Employ-
ing the Centering Theory in Pronoun Resolution 
from the Semantic Perspective. EMNLP’2009 

F. Kong, Y.C. Li, G.D. Zhou and Q.M. Zhu. 2009. 
Exploring Syntactic  Features  for  Pronoun Reso-
lution Using Context-Sensitive Convolution Tree 
Kernel. IALP’2009 

S. Lappin and J. L. Herbert. 1994. An algorithm for 
pronominal  anaphora  resolution.  Computational 
Linguistics, 20(4) 

J.H.  Li.  G.D.  Zhou,  H.  Zhao,  Q.M.  Zhu  and  P.D. 
Qian.  Improving  nominal  SRL  in  Chinese  lan-
guage  with  verbal  SRL  information  and  auto-
matic predicate recognition. EMNLP '2009 

X. Luo. 2007. Coreference or not: A twin model for 

coreference resolution.    NAACL-HLT’2007 

V. Ng and C. Cardie 2002. Identify Anaphoric and 
Improve 

Non-Anaphoric  Noun  Phrases 
to 
Coreference Resolution. COLING’2002 

V.  Ng  and  C.  Cardie  2002.  Improving  machine 
learning  approaches  to  coreference  resolution. 
ACL’2002 

V. Ng 2004. Learning Noun Phrase Anaphoricity to 
Improve Coreference Resolution: Issues in Rep-
resentation and Optimization. ACL’ 2004 

V. Ng 2009. Graph-cut based anaphoricity determi-
nation for coreference resolution. NAACL’2009 
L.H. Qian, G.D. Zhou, F. Kong, Q.M. Zhu and P.D. 
Qian. 2008. Exploiting constituent dependencies 
for    tree  kernel-based  semantic  relation  extrac-
tion. COLING’2008 

W. M. Soon, H. T. Ng and D. Lim    2001. A ma-
chine learning approach to coreference resolution 

