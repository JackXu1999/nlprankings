



















































Repeat before Forgetting: Spaced Repetition for Efficient and Effective Training of Neural Networks


Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 2401–2410
Copenhagen, Denmark, September 7–11, 2017. c©2017 Association for Computational Linguistics

Repeat before Forgetting: Spaced Repetition for Efficient and Effective
Training of Neural Networks

Hadi Amiri, Timothy A. Miller, Guergana Savova
Boston Children’s Hospital Informatics Program, Harvard Medical School

{firstname.lastname}@childrens.harvard.edu

Abstract

We present a novel approach for training
artificial neural networks. Our approach is
inspired by broad evidence in psychology
that shows human learners can learn effi-
ciently and effectively by increasing inter-
vals of time between subsequent reviews of
previously learned materials (spaced repeti-
tion). We investigate the analogy between
training neural models and findings in psy-
chology about human memory model and
develop an efficient and effective algorithm
to train neural models. The core part of our
algorithm is a cognitively-motivated sched-
uler according to which training instances
and their “reviews” are spaced over time.
Our algorithm uses only 34-50% of data
per epoch, is 2.9-4.8 times faster than stan-
dard training, and outperforms competing
state-of-the-art baselines.1

1 Introduction

Deep neural models are known to be computa-
tionally expensive to train even with fast hard-
ware (Sutskever et al., 2014; Wu et al., 2016). For
example, it takes three weeks to train a deep neu-
ral machine translation system on 100 Graphics
Processing Units (GPUs) (Wu et al., 2016). Fur-
thermore, a large amount of data is usually required
to train effective neural models (Goodfellow et al.,
2016; Hirschberg and Manning, 2015).

Bengio et al. (2009) and Kumar et al. (2010) de-
veloped training paradigms which are inspired by
the learning principle that humans can learn more
effectively when training starts with easier con-
cepts and gradually proceeds with more difficult
concepts. Since these approaches are motivated by

1Our code is available at scholar.harvard.edu/
hadi/RbF/

a “starting small” strategy they are called curricu-
lum or self-paced learning.

In this paper, we present a novel training
paradigm which is inspired by the broad evidence
in psychology that shows human ability to retain
information improves with repeated exposure and
exponentially decays with delay since last expo-
sure (Cepeda et al., 2006; Averell and Heathcote,
2011). Spaced repetition was presented in psychol-
ogy (Dempster, 1989) and forms the building block
of many educational devices, including flashcards,
in which small pieces of information are repeatedly
presented to a learner on a schedule determined
by a spaced repetition algorithm. Such algorithms
show that human learners can learn efficiently and
effectively by increasing intervals of time between
subsequent reviews of previously learned materi-
als (Dempster, 1989; Novikoff et al., 2012).

We investigate the analogy between training neu-
ral models and findings in psychology about human
memory model and develop a spaced repetition al-
gorithm (named Repeat before Forgetting, RbF)
to efficiently and effectively train neural models.
The core part of our algorithm is a scheduler that
ensures a given neural network spends more time
working on difficult training instances and less time
on easier ones. Our scheduler is inspired by fac-
tors that affect human memory retention, namely,
difficulty of learning materials, delay since their
last review, and strength of memory. The scheduler
uses these factors to lengthen or shorten review
intervals with respect to individual learners and
training instances. We evaluate schedulers based
on their scheduling accuracy, i.e., accuracy in es-
timating network memory retention with respect
to previously-seen instances, as well as their effect
on the efficiency and effectiveness of downstream
neural networks.2

2 In this paper, we use the terms memory retention, recall,
and learning interchangeably.

2401



The contributions of this paper are: (1) we show
that memory retention in neural networks is af-
fected by the same (known) factors that affect mem-
ory retention in humans, (2) we present a novel
training paradigm for neural networks based on
spaced repetition, and (3) our approach can be ap-
plied without modification to any neural network.

Our best RbF algorithm uses 34-50% of train-
ing data per epoch while producing similar results
to state-of-the-art systems on three tasks, namely
sentiment classification, image categorization, and
arithmetic addition.3 It also runs 2.9-4.8 times
faster than standard training, and outperforms com-
peting state-of-the-art baselines.

2 Neural and Brain Memory Models

Research in psychology describes the following
memory model for human learning: the probability
that a human recalls a previously-seen item (e.g.,
the Korean translation of a given English word) de-
pends on the difficulty of the item, delay since last
review of the item, and the strength of the human
memory. The relation between these indicators
and memory retention has the following functional
form (Reddy et al., 2016; Ebbinghaus, 1913):

Pr(recall) = exp(−difficulty × delay
strength

). (1)

An accurate memory model enables estimating
the time by which an item might be forgotten by
a learner so that a review can be scheduled for the
learner before that time.

We investigate the analogy between the above
memory model and memory model of artificial
neural networks. Our intuition is that if the proba-
bility that a network recalls an item (e.g., correctly
predicts its category) depends on the same factors
(difficulty of the item, delay since last review of
the item, or strength of the network), then we can
develop spaced repetition algorithms to efficiently
and effectively train neural networks.

2.1 Recall Indicators
We design a set of preliminarily experiments to
directly evaluate the effect of the aforementioned
factors (recall indicators) on memory retention in
neural networks. For this purpose, we use a set
of training instances that are partially made avail-
able to the network during training. This scheme

3We obtained similar results on QA tasks (Weston et al.,
2016) but they are excluded due to space limit.

Epochs

delay
Sliding review 

window

First review 
point (fRev)

Last review 
point (lRev)

Recall 
point (Rec) 

epoch = 0 

{A ∪ B }
{A ∪ C }  {A ∪ C }

Figure 1: Effect of recall indicators on network
retention. Training data is uniformly at random
divided into three disjoint sets A, B, and C that
respectively contain 80%, 10%, and 10% of the
data. Network retention is computed against set B
instances at recall point.

will allow us to intrinsically examine the effect of
recall indicators on memory retention in isolation
from external effects such as size of training data,
number of training epochs, etc.

We first define the following concepts to ease
understanding the experiments (see Figure 1):

• First and Last review points (fRev and
lRev) of a training instance are the first and
last epochs in which the instance is used to
train the network respectively,

• Recall point (Rec) is the epoch in which net-
work retention is computed against some train-
ing instances; network retention is the prob-
ability that a neural network recalls (i.e. cor-
rectly classifies) a previously-seen training in-
stance, and

• Delay since last review of a training instance
is the difference between the recall point and
the last review point of the training instance.

Given training data and a neural network, we uni-
formly at random divide the data into three disjoint
sets: a base set A, a review set B, and a replace-
ment set C that respectively contain 80%, 10%, and
10% of the data. As depicted in Figure 1, instances
of A are used for training at every epoch, while
those in B and C are partially used for training.
The network initially starts to train with {A ∪ C}
instances. Then, starting from the first review point,
we inject the review set B and remove C, training
with {A ∪ B} instances at every epoch until the
last review point. The network will then continue
training with {A ∪ C} instances until the recall
point. At this point, network retention is computed
against set B instances, with delay defined as the
number of epochs since last review point. The intu-
ition behind using review and replacement sets, B
and C respectively, is to avoid external effects (e.g.

2402



1 2 3 4 5 6 7 8 9 10
delay since last review (Rec - lRev)

0.935

0.940

0.945

0.950

0.955

0.960

0.965

a
v
e
ra

g
e
 n

e
tw

o
rk

 r
e
te

n
ti

o
n

(a) Delay full

1 2 3 4 5 6 7 8 9 10
delay since last review (Rec - lRev)

0.740

0.745

0.750

0.755

0.760

0.765

0.770

a
v
e
ra

g
e
 n

e
tw

o
rk

 r
e
te

n
ti

o
n

(b) Delay partial

0.00 0.01 0.02 0.03 0.04 0.05 0.06
item difficulty at last review point (normalized loss)

0.2

0.3

0.4

0.5

0.6

0.7

0.8

0.9

1.0

a
v
e
ra

g
e
 n

e
tw

o
rk

 r
e
te

n
ti

o
n

(c) Item difficulty

R1 R2 R3 R4 R5 R6 R7 R8 R9 R10
recall points

0.0

0.1

0.2

0.3

0.4

0.5

0.6

0.7

0.8

0.9

a
v
e
ra

g
e
 a

cc
u
ra

cy

network retention

network strength

(d) Network strength

Figure 2: (a) Delay since last review vs. average network retention (accuracy) on set B instances at recall
point. Recall point is fixed and set to the epoch in which networks obtain their best performance based on
rote training. (b) The same as (a) except that recall point is set to the epoch in which networks obtain half
of their best performance based on rote training. (c) Item Difficulty (normalized loss at last review point)
vs. average network retention at recall point on set B instances. (d) Network strength (network accuracy
on validation data at recall point) vs. average network retention at recall point on set B instances. Length
of sliding window is fixed throughout experiments and set to 5 epochs.

size of data or network generalization and learning
capability) for our intrinsic evaluation purpose.

To conduct these experiments, we identify dif-
ferent neural models designed for different tasks.4

For each network, we fix the recall point to either
the epoch in which the network is fully trained (i.e.,
obtains its best performance based on standard or
“rote” training in which all instances are used for
training at every iteration), or partially trained (i.e.,
obtains half of its best performance based on rote
training). We report average results across these
networks for each experiment.

2.1.1 Delay since Last Review
As aforementioned, delay since last review of a
training instance is the difference between the re-
call point (Rec) and the last review point (lRev)
of the training instance. We evaluate the effect of
delay on network retention (against set B instances)
by keeping the recall point fixed while moving the
sliding window in Figure 1. Figures 2(a) and 2(b)
show average network retention across networks
for the fully and partially trained recall points re-
spectively. The results show an inverse relationship
between network retention and delay since last re-
view in neural networks.

2.1.2 Item Difficulty
We define difficulty of training instances by the
loss values generated by a network for the instances.
Figure 2(c) shows the difficulty of set B instances at
the last review point against average network reten-
tion on these instances at recall point. We normal-
ize loss values to unit vectors (to make them com-

4See section 4, we use Addition and CIFAR10 datasets and
their corresponding neural networks for these experiments.

parable across networks) and then average them
across networks for both fully and partially trained
recall points. As the results show, network reten-
tion decreases as item difficulty increases.

2.1.3 Network Strength

We define strength of a network by its performance
on validation data. To understand the effect of
network strength on its retention, we use the same
experimental setup as before except that we keep
the delay (difference between recall point and last
review point) fixed while gradually increasing the
recall point; this will make the networks stronger
by training them for more epochs. Then, at every
recall point, we record network retention on set B
instances and network accuracy on validation data.
Average results across networks for two sets of 10
consecutive recall points (before fully and partially
trained recall points) are shown in Figure 2(d). As
the results show, network retention increases as
memory strength increases.

The above experiments show that memory re-
tention in neural networks is affected by the same
factors that affect memory retention in humans: (a)
neural networks forget training examples after a
certain period of intervening training data (b): the
period of recall is shorter for more difficult exam-
ples, and (c): recall improves as networks achieve
better overall performance. We conclude that de-
lay since last review, item difficulty (loss values of
training instances), and memory strength (network
performance on validation data) are key indicators
that affect network retention and propose to design
spaced repetition algorithms that take such indica-
tors into account in training neural networks.

2403



Algorithm 1. Leitner System
Input: H : training data, V : validation data, k : num-
ber of iterations, n : number of queues
Output: trained model

0 Q = [q0, q1, . . . , qn−1]
1 q0 = [H], qi = [] for i in [1, n− 1]
2 For epoch = 1 to k:
3 current batch = []
4 For i = 0 to n− 1:
5 If epoch%2i == 0:
6 current batch = current batch+ qi
7 End For
8 pmos, dmos,model = train(current batch,V)
9 update queue(Q, pmos, dmos)
10 End for
11 return model
q0 epochs = {1, 2, 3, 4, 5, . . . }
q1 epochs = {2, 4, 6, 8, 10, . . . }
q2 epochs = {4, 8, 12, 16, 20, . . . }
. . .

Figure 3: Leitner System. The train(.) function
trains the network for one epoch using instances
in the current batch, and the update queue(.)
function promotes the recalled (correctly classified)
instances, pmos, to the next queue and demotes the
forgotten ones, dmos, to q0.

3 Spaced Repetition

We present two spaced repetition-based algorithms:
a modified version of the Leitner system developed
in (Reddy et al., 2016) and our Repeat before For-
getting (RbF) model respectively.

3.1 Leitner System
Suppose we have n queues {q0, q1, . . . , qn−1}. The
Leitner system initially places all training instances
in the first queue, q0. As Algorithm 1 shows,
at each training iteration, the Leitner scheduler
chooses some queues to train a downstream neu-
ral network. Only instances in the selected queues
will be used for training the network. During train-
ing, if an instance from qi is recalled (e.g. correctly
classified) by the network, the instance will be “pro-
moted” to qi+1, otherwise it will be “demoted” to
the first queue, q0.5

The Leitner scheduler reviews instances of qi
at every 2i iterations. Therefore, instance in
lower queues (difficult/forgotten instances) are re-
viewed more frequently than those in higher queues
(easy/recalled ones). Figure 3 (bottom) provides
examples of queues and their processing epochs.
Note that the overhead imposed on training by

5 Note that in (Reddy et al., 2016) demoted instances are
moved to qi−1. We observed significant improvement in Leit-
ner system by moving such instances to q0 instead of qi−1.

the Leitner system is O(|current batch|) at every
epoch for moving instances between queues.

3.2 RbF Model
3.2.1 RbF Memory Models
The challenge in developing memory models is
to estimate the time by which a training instance
should be reviewed before it is forgotten by the
network. Accurate estimation of the review time
leads to efficient and effective training. However, a
heuristic scheduler such as Leitner system is sub-
optimal as its hard review schedules (i.e. only 2i-
iteration delays) may lead to early or late reviews.

We develop flexible schedulers that take recall in-
dicators into account in the scheduling process. Our
schedulers lengthen or shorten inter-repetition in-
tervals with respect to individual training instances.
In particular, we propose using density kernel func-
tions to estimate the latest epoch in which a given
training instance can be recalled. We aim to investi-
gate how much improvement (in terms of efficiency
and effectiveness) can be achieved using more flex-
ible schedulers that utilize the recall indicators.

We propose considering density kernels as sched-
ulers that favor (i.e., more confidently delay) less
difficult training instances in stronger networks. As
a kernel we can use any non-increasing function of
the following quantity:

xi =
di × ti
se

, (2)

where di indicates the loss of network for a training
instance hi ∈ H, ti indicates the number of epochs
to next review of hi, and se indicates the perfor-
mance of network— on validation data— at epoch
e. We investigate the Gaussian, Laplace, Linear,
Cosine, Quadratic, and Secant kernels as described
below respectively:

fgau(x, τ) = exp(−τx2), (3)
flap(x, τ) = exp(−τx), (4)

flin(x, τ) =

{
1− τx x < 1τ
0 otherwise

, (5)

fcos(x, τ) =

{
1
2 cos(τπx) + 1 x <

1
τ

0 otherwise
,

(6)

fqua(x, τ) =

{
1− τx2 x2 < 1τ
0 otherwise

, (7)

fsec(x, τ) =
2

exp(−τx2) + exp(τx2) , (8)

2404



0 1 2 3 4 5
0.0

0.2

0.4

0.6

0.8

1.0

Cos

Gau

Lap

Lin

Qua

Sec

Figure 4: RbF kernel functions with τ = 1.

where τ is a learning parameter. Figure 4 depicts
these kernels with τ = 1. As we will discuss in
the next section, we use these kernels to optimize
delay with respect to item difficulty and network
strength for each training instance.

3.2.2 RbF Algorithm
Our Repeat before Forgetting (RbF) model is a
spaced repetition algorithm that takes into account
the previously validated recall indicators to train
neural networks, see Algorithm 2. RbF divides
training instances into current and delayed batches
based on their delay values at each iteration. In-
stances in the current batch are those that RbF is
less confident about their recall and therefore are
reviewed (used to re-train the network) at current
iteration. On the other hand, instances in the de-
layed batch are those that are likely to be recalled
by the network in the future and therefore are not re-
viewed at current epoch. At each iteration, the RbF
scheduler estimates the optimum delay (number of
epochs to next review) for each training instance
in the current batch. RbF makes such item-specific
estimations as follows:

Given the difficulty of a training instance di, the
memory strength of the neural network at epoch e,
se, and an RbF memory model f (see section 3.2.1),
RbF scheduler estimates the maximum delay t̂i for
the instance such that it can be recalled with a con-
fidence greater than the given threshold η ∈ (0, 1)
at time e+ t̂i. As described before, di and se can
be represented by the current loss of the network
for the instance and the current performance of the
network on validation data respectively. Therefore,
the maximum delay between the current (epoch e)
and next reviews of the instance can be estimated
as follows:

t̂i = arg min
ti

(
f(xi, τ̂)− η

)2
, (9)

s.t 1 ≤ ti ≤ k − e

Algorithm 2. RbF Training Model
Input: H : training data, V : validation data, k : num-
ber of iterations, f : RbF kernel, η: recall confidence
Output: trained model

0 ti = 1 for hi ∈ H
1 For epoch = 1 to k:
2 current batch = {hi : ti <= 1}
3 delayed batch = {hi : ti > 1}
4 sepoch,model = train(current batch,V)
5 τ̂ = arg minτ

(
f(xi, τ)− ai

)2 ∀hi ∈ V, ai ≥ η
6 t̂i = arg minti

(
f(xi, τ̂)−η

)2∀hi ∈ current batch
7 ti = ti − 1 ∀hi ∈ delayed bach
8 End for
9 return model

Figure 5: RbF training model. The train(.) func-
tion at line 5 trains the network for one epoch using
instances in the current batch. Note that at each
iteration epoch, xi is computed using Equation (2)
and strength of the current model, sepoch.

where τ̂ is the optimum value for the learning pa-
rameter obtained from validation data, see Equa-
tion (10). In principle, reviewing instances could
be delayed for any number of epochs; in practice
however, delay is bounded both below and above
(e.g., by queues in the Leitner system). Thus, we
assume that, at each epoch e, instances could be
delayed for at least one iteration and at most k − e
iterations where k is the total number of training
epochs. We also note that ti is a lower bound of the
maximum delay as se is expected to increase and
di is expected to decrease as the network trains in
next iterations.

Algorithm 2 shows the outline of the proposed
RbF model. We estimate the optimum value of τ
(line 5 of Algorithm 2) for RbF memory models us-
ing validation data. In particular, RbF uses the loss
values of validation instances and strength of the
network obtained at the previous epoch to estimate
network retention for validation instances at the
current epoch (therefore ti = 1 for every validation
instance). The parameter τ for each memory model
is computed as follows:

τ̂ = arg min
τ

(
f(xj , τ)− aj

)2
,∀hj ∈ V, aj ≥ η,

(10)
where aj ∈ (0, 1) is the current accuracy of the
model for the validation instance hj . RbF then
predicts the delay for current batch instances and
reduces the delay for those in the delayed batch
by one epoch. The overhead of RbF is O(|H|) to
compute delays and O(|V|) to compute τ̂ . Note
that (9) and (10) have closed form solutions.

2405



Dataset train, dev, test Network Task

IMDb 20K, 5K, 25K

MLP/fastext
(Joulin et al.,
2017), best
epoch=8

sentiment
analysis

CIFAR10 45K, 5K, 10K
CNN (Chan
et al., 2015)
best epoch=64

image clas-
sification

Addition 40K, 5K, 10K
LSTM (Sutskever
et al., 2014)
best epoch=32

arithmetic
addition

Table 1: Datasets, models, and tasks.

4 Experiments

Table 1 describes the tasks, datasets, and models
that we consider in our experiments. It also reports
the training epochs for which the models produce
their best performance on validation data (based on
rote training). We note that the Addition dataset is
randomly generated and contains numbers with at
most 4 digits.6

We consider three schedulers as baselines: a
slightly modified version of the Leitner scheduler
(Lit) developed in Reddy et al. (2016) for human
learners (see Footnote 5), curriculum learning (CL)
in which training instances are scheduled with re-
spect to their easiness (Jiang et al., 2015), and
the uniform scheduler of rote training (Rote) in
which all instances are used for training at every
epoch. For Lit, we experimented with different
queue lengths, n = {3, 5, 7}, and set n = 5 in
the experiments as this value led to the best perfor-
mance of this scheduler across all datasets.

Curriculum learning starts training with easy
instances and gradually introduces more com-
plex instances for training. Since easiness infor-
mation is not readily available in most datasets,
previous approaches have used heuristic tech-
niques (Spitkovsky et al., 2010; Basu and Chris-
tensen, 2013) or optimization algorithms (Jiang
et al., 2015, 2014) to quantify easiness of training
instances. These approaches consider an instance
as easy if its loss is smaller than a threshold (λ).
We adopt this technique as follows: at each itera-
tion e, we divide the entire training data into easy
and hard sets using iteration-specific λe and the
loss values of instances, obtained from the current
partially-trained network. All easy instances in con-
junction with αe ∈ [0, 1] fraction of easiest hard
instances (those with smallest loss values greater
than λe) are used for training at iteration e. We set

6https://github.com/fchollet/keras/
blob/master/examples/addition_rnn.py

0.30 0.35 0.40 0.45 0.50 0.55 0.60 0.65 0.70
(delayed instances per epoch)%

0.66

0.68

0.70

0.72

0.74

0.76

0.78

0.80

sc
h
e
d
u
le

r 
a
cc

u
ra

cy

Cos

Gau

Lap

Lin

Qua

Sec

Lit

Figure 6: Accuracy of schedulers in predicting
network retention. For these experiments recall
confidence is set to its default value, η = 0.5.

each λe to the average loss of training instances
that are correctly classified by the current partially-
trained network. Furthermore, at each iteration e,
we set αe = e/k to gradually introduce complex
instances at every new iteration.7 Note that we treat
all instances as easy at e = 0.

Performance values reported in experiments are
averaged over 10 runs of systems and the confi-
dence parameter η is always set to 0.5 unless other-
wise stated.

4.1 Evaluation of Memory Models
In these experiments, we evaluate memory sched-
ulers with respect to their accuracy in predicting
network retention for delayed instances. Since cur-
riculum learning does not estimate delay for train-
ing instances, we only consider Leitner and RbF
schedulers in these experiments.

For this evaluation, if a scheduler predicts a delay
t for a training instance h at epoch e, we evaluate
network retention with respect to h at epoch e+ t.
If the network recalls (correctly classifies) the in-
stance at epoch e+ t, the scheduler has correctly
predicted network retention for h, and otherwise, it
has made a wrong prediction. We use this binary
outcome to evaluate the accuracy of each sched-
uler. Note that the performance of schedulers on
instances that have not been delayed is not a ma-
jor concern. Although failing to delay an item
inversely affects efficiency, it makes the network
stronger by providing more instances to train from.
Therefore, we consider a good scheduler as the one
that accurately delays more items.

Figure 6 depicts the average accuracy of sched-
ulers in predicting networks’ retention versus the
average fraction of training instances that they de-
layed per epoch. As the results show, all schedulers

7k is the total number of iterations.

2406



0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9
recall confidence (η)

0.55

0.60

0.65

0.70

0.75

0.80

sc
h
e
d
u
le

r 
a
cc

u
ra

cy

Cos

Gau

Lap

Lin

Qua

Sec

Lit

Figure 7: Effect of recall confidence η on the accu-
racy of different schedulers in predicting network
retention (best seen in color.)

delay substantial amount of instances per epoch. In
particular, Cos and Qua outperform Lit in both pre-
dicting network retention and delaying items, de-
laying around 50% of training instances per epoch.
This is while Gau and Sec show comparable ac-
curacy to Lit but delay more instances. On the
other hand, Lap, which has been found effective in
Psychology, and Lin are less accurate in predicting
network retention. This is because of the trade-
off between delaying more instances and creating
stronger networks. Since these schedulers are more
flexible in delaying greater amount of instances,
they might not provide networks with enough data
to fully train.

Figure 7 shows the performance of RbF sched-
ulers with respect to the recall confidence parame-
ter η, see Equation (9). As the results show, sched-
ulers have poor performance with smaller values of
η. This is because smaller values of η make sched-
ulers very flexible in delaying instances. However,
the performance of schedulers are not dramatically
low even with very small ηs. Our further analyses
on the delay patterns show that although a smaller
η leads to more delayed instances, the delays are
significantly shorter. Therefore, most delayed in-
stances will be “reviewed” shortly in next epochs.
These bulk reviews make the network stronger and
help it to recall most delayed instance in future
iterations.

On the other hand, greater ηs lead to more ac-
curate schedulers at the cost of using more train-
ing data. In fact, we found that larger ηs do not
delay most training instances in the first few itera-
tions. However, once the network obtains a reason-
ably high performance, schedulers start delaying
instances for longer durations. We will further
study this effect in the next section.

Model Accuracy TIPE X Faster Gain
CL 0.868 0.71 0.93 1.40
Lit 0.859 0.67 2.87 1.85
Gau 0.874 0.48 3.02 2.16
Lap 0.857 0.34 4.66 3.15
Lin 0.864 0.36 4.78 3.07
Cos 0.868 0.49 2.90 2.10
Qua 0.871 0.50 2.95 2.08
Sec 0.866 0.44 3.09 2.33
Cos η = 0.9 0.880 0.76 2.36 1.42
Rote 0.887 1.00 1.00 1.00

Table 2: Comparison of schedulers in terms of aver-
age network accuracy, average fraction of instances
used for training per epoch (TIPE), and the extent
to which a model runs faster than Rote training (X
Times Faster). Gain column indicates the AccuracyTIPE
improvement over rote training.

4.2 Efficiency and Effectiveness

We compare RbF against Leitner and curriculum
learning in terms of efficiency of training and effec-
tiveness of trained models. We define effectiveness
as the accuracy of a trained network on balanced
test data, and efficiency as (a): fraction of instances
used for training per epoch, and (b): required time
for training the networks. For RbF schedulers, we
set η to 0.5 and consider the best performing kernel
Cosine with η = 0.9 based on results in Figure 7.

The results in Table 2 show that all training
paradigms have comparable effectiveness (Accu-
racy) to that of rote training (Rote). Our RbF sched-
ulers use less data per epoch (34-50% of data) and
run considerably faster than Rote (2.90-4.78 times
faster for η = 0.5). The results also show that Lit is
slightly less accurate but runs 2.87 time faster than
Rote; note that, as a scheduler, Lit is less accurate
than RbF models, see Figures 6 and 7.

In addition, CL leads to comparable performance
to RbF but is considerably slower than other sched-
ulers. This is because this scheduler has to identify
easier instances and sort the harder ones to sample
training data at each iteration. Overall, the perfor-
mance of Lit, CL, Cos η = .5 and Cos η = .9
are only 2.76, 1.90, 1.88, and 0.67 absolute values
lower than that of Rote respectively. Considering
the achieved efficiency, these differences are negli-
gible (see the overall gain in Table 2).

Figure 8 reports detailed efficiency and effective-
ness results across datasets and networks. For clear
illustration, we report accuracy at iterations 2i ∀i in
which Lit is trained on the entire data, and consider
Cos η = .5 as RbF scheduler. In terms of efficiency
(first row of Figure 8), CL starts with (small set of)

2407



2 4 6 8
epoch

0

5

10

15

20

25

#
tr

a
in

in
g
 i
n
st

a
n
ce

s 
(×

1
0

3
)

Lit CL RbF Rote

(a) Efficiency IMDb

4 8 12 16 20 24 28 32
epoch

0

5

10

15

20

25

30

35

40

#
tr

a
in

in
g
 i
n
st

a
n
ce

s 
(×

1
0

3
)

Lit CL RbF Rote

(b) Efficiency Addition

8 16 24 32 40 48 56 64
epoch

0

10

20

30

40

50

#
tr

a
in

in
g
 i
n
st

a
n
ce

s 
(×

1
0

3
)

Lit CL RbF Rote

(c) Efficiency CIFAR10

1 2 4 8
epoch

0.80

0.82

0.84

0.86

0.88

0.90

n
e
tw

o
rk

 a
cc

u
ra

cy

Lit CL RbF Rote

(d) Effectiveness IMDb

1 2 4 8 16 32
epoch

0.2

0.3

0.4

0.5

0.6

0.7

0.8

0.9

1.0

n
e
tw

o
rk

 a
cc

u
ra

cy

Lit CL RbF Rote

(e) Effectiveness Addition

1 2 4 8 16 32 64
epoch

0.50

0.55

0.60

0.65

0.70

0.75

0.80

n
e
tw

o
rk

 a
cc

u
ra

cy

Lit CL RbF Rote

(f) Effectiveness CIFAR10

Figure 8: Efficiency and Effectiveness of schedulers across three datasets and networks. RbF uses Cos
η = .5 as kernel. CL starts with (small set of) easier instances and gradually incorporate slightly harder
instances at each iteration. Lit and RbF start big and gradually delay reviewing easy instances.

easier instances and gradually increases the amount
of training data by adding slightly harder instances
into its training set. On the other hand, Lit and
RbF start big and gradually delay reviewing (easy)
instances that the networks have learned. The dif-
ference between these two training paradigms is
apparent in Figures 8(a)-8(c).

The results also show that the efficiency of a
training paradigm depends on the initial effective-
ness of the downstream neural network. For CL
to be efficient, the neural network need to initially
have low performance (accuracy) so that the sched-
uler works on smaller set of easy instances. For
example, in case of Addition, Figures 8(b) and 8(e),
the initial network accuracy is only 35%, therefore
most instances are expected to be initially treated
as hard instances and don’t be used for training.
On the other hand, CL shows a considerably lower
efficiency for networks with slightly high initial
accuracy, e.g. in case of IMDb or CIFAR10 where
the initial network accuracy is above 56%, see Fig-
ures 8(a) and 8(d), and 8(c) and 8(f) respectively.

In contrast to CL, Lit and RbF are more efficient
when the network has a relatively higher initial per-
formance. A higher initial performance helps the

schedulers to more confidently delay “reviewing”
most instances and therefore train with a much
smaller set of instances. For example, since the
initial network accuracy in IMDb or CIFAR10 is
above 56%, Lit and RbF are considerably more
efficient from the beginning of the training pro-
cess. However, in case of low initial performance,
Lit and RbF tend to avoid delaying instances at
lower iterations which leads to poor efficiency at
the beginning. This is the case for the Addition
dataset in which instances are gradually delayed
by these two schedulers even at epoch 8 when the
performance of the network reaches above 65%,
see Figures 8(e) and 8(b). However, Lit gains its
true efficiency after iteration 12, see Figure 8(b),
while RbF still gradually improves the efficiency.
This might be because of the lower bound delays
that RbF estimates, see Equation (9).

Furthermore, the effectiveness results in Figure 8
(bottom) show that all schedulers produce compa-
rable accuracy to the Rote scheduler throughout the
training process, not just at specific iterations. This
indicates that these training paradigms can much
faster achieve the same generalizability as standard
training, see Figures 8(b) and 8(e).

2408



1 2 4 8 16 32
epoch

0.80

0.82

0.84

0.86

0.88

0.90

n
e
tw

o
rk

 a
cc

u
ra

cy

Lit CL RbF Rote

Figure 9: Robustness against overtraining.

4.3 Robustness against Overtraining

We investigate the effect of spaced repetition on
overtraining. The optimal number of training
epochs required to train fastText on the IMDb
dataset is 8 epochs (see Table 1). In this experiment,
we run fastText on IMDb for greater number
of iterations to investigate the robustness of differ-
ent schedulers against overtraining. The results in
Figure 9 show that Lit and RbF (Cos η = 0.5) are
more robust against overtraining. In fact, the perfor-
mance of Lit and RbF further improve at epoch 16
while CL and Rote overfit at epoch 16 (note that CL
and Rote also require considerably more amount
of time to reach to higher iterations). We attribute
the robustness of Lit and RbF to the scheduling
mechanism which helps the networks to avoid re-
training with easy instances. On the other hand,
overtraining affects Lit and RbF at higher training
iterations, compare performance of each scheduler
at epochs 8 and 32. This might be because these
training paradigms overfit the network by paying
too much training attention to very hard instances
which might introduce noise to the model.

5 Related Work

Ebbinghaus (1913, 2013), and recently Murre and
Dros (2015), studied the hypothesis of the expo-
nential nature of forgetting, i.e. how information
is lost over time when there is no attempt to re-
tain it. Previous research identified three critical
indicators that affect the probability of recall: re-
peated exposure to learning materials, elapsed time
since their last review (Ebbinghaus, 1913; Wixted,
1990; Dempster, 1989), and more recently item
difficulty (Reddy et al., 2016). We based our inves-
tigation on these findings and validated that these
indicators indeed affect memory retention in neural
networks. We then developed training paradigms
that utilize the above indicators to train networks.

Bengio et al. (2009) and Kumar et al. (2010)
also developed cognitively-motivated training
paradigms which are inspired by the principle that
learning can be more effective when training starts
with easier concepts and gradually proceeds with
more difficult ones. Our idea is motivated by the
spaced repetition principle which indicates learn-
ing improves with repeated exposure and decays
with delay since last exposure (Ebbinghaus, 1913;
Dempster, 1989). Based on this principle, we devel-
oped schedulers that space the reviews of training
instances over time for efficient and effective train-
ing of neural networks.

6 Conclusion and Future Work

We developed a cognitively-motivated training
paradigm (scheduler) that space instances over time
for efficient and effective training of neural net-
works. Our scheduler only uses a small fraction
of training data per epoch but still effectively train
neural networks. It achieves this by estimating the
time (number of epochs) by which training could
be delayed for each instance. Our work was in-
spired by three recall indicators that affect memory
retention in humans, namely difficulty of learning
materials, delay since their last review, and mem-
ory strength of the learner, which we validated in
the context of neural networks.

There are several avenues for future work in-
cluding the extent to which our RbF model and its
kernels could be combined with curriculum learn-
ing or Leitner system to either predict easiness of
novel training instances to inform curriculum learn-
ing or incorporate Leitner’s queueing mechanism
to the RbF model. Other directions include extend-
ing RbF to dynamically learn the recall confidence
parameter with respect to network behavior, or de-
veloping more flexible delay functions with theo-
retical analysis on their lower and upper bounds.

Acknowledgments

We thank Mitra Mohtarami for her constructive
feedback during the development of this paper
and anonymous reviewers for their thoughtful com-
ments. This work was supported by National Insti-
tutes of Health (NIH) grant R01GM114355 from
the National Institute of General Medical Sciences
(NIGMS). The content is solely the responsibil-
ity of the authors and does not necessarily repre-
sent the official views of the National Institutes of
Health.

2409



References
Lee Averell and Andrew Heathcote. 2011. The form of

the forgetting curve and the fate of memories. Jour-
nal of Mathematical Psychology, 55(1):25–35.

Sumit Basu and Janara Christensen. 2013. Teaching
classification boundaries to humans. In Proceedings
of the Twenty-Seventh AAAI Conference on Artificial
Intelligence, pages 109–115. AAAI Press.

Yoshua Bengio, Jérôme Louradour, Ronan Collobert,
and Jason Weston. 2009. Curriculum learning. In
Proceedings of the 26th annual international confer-
ence on machine learning, pages 41–48. ACM.

Nicholas J Cepeda, Harold Pashler, Edward Vul,
John T Wixted, and Doug Rohrer. 2006. Distributed
practice in verbal recall tasks: A review and quanti-
tative synthesis. Psychological bulletin, 132(3):354.

Tsung-Han Chan, Kui Jia, Shenghua Gao, Jiwen Lu, Zi-
nan Zeng, and Yi Ma. 2015. Pcanet: A simple deep
learning baseline for image classification? IEEE
Transactions on Image Processing, 24(12):5017–
5032.

Frank N Dempster. 1989. Spacing effects and their im-
plications for theory and practice. Educational Psy-
chology Review, 1(4):309–330.

Hermann Ebbinghaus. 1913. Memory: A contribution
to experimental psychology. 3. University Micro-
films.

Hermann Ebbinghaus. 2013. Memory: A contribu-
tion to experimental psychology. Annals of neuro-
sciences, 20(4):155.

Ian Goodfellow, Yoshua Bengio, and Aaron Courville.
2016. Deep Learning. MIT Press. http://www.
deeplearningbook.org.

Julia Hirschberg and Christopher D Manning. 2015.
Advances in natural language processing. Science,
349(6245):261–266.

Lu Jiang, Deyu Meng, Shoou-I Yu, Zhenzhong Lan,
Shiguang Shan, and Alexander Hauptmann. 2014.
Self-paced learning with diversity. In Z. Ghahra-
mani, M. Welling, C. Cortes, N. D. Lawrence, and
K. Q. Weinberger, editors, Advances in Neural In-
formation Processing Systems 27, pages 2078–2086.
Curran Associates, Inc.

Lu Jiang, Deyu Meng, Qian Zhao, Shiguang Shan, and
Alexander G. Hauptmann. 2015. Self-paced cur-
riculum learning. In Proceedings of the Twenty-
Ninth AAAI Conference on Artificial Intelligence,
AAAI’15, pages 2694–2700.

Armand Joulin, Edouard Grave, and Piotr Bo-
janowski Tomas Mikolov. 2017. Bag of tricks for
efficient text classification. European Chapter of the
Association for Computational Linguistics, EACL
2017, page 427.

M Pawan Kumar, Benjamin Packer, and Daphne Koller.
2010. Self-paced learning for latent variable models.
In Advances in Neural Information Processing Sys-
tems, pages 1189–1197.

Jaap MJ Murre and Joeri Dros. 2015. Replication and
analysis of ebbinghaus’ forgetting curve. PloS one,
10(7):e0120644.

Timothy P Novikoff, Jon M Kleinberg, and Steven H
Strogatz. 2012. Education of a model student.
Proceedings of the National Academy of Sciences,
109(6):1868–1873.

Siddharth Reddy, Igor Labutov, Siddhartha Banerjee,
and Thorsten Joachims. 2016. Unbounded human
learning: Optimal scheduling for spaced repetition.
In Proceedings of the 22nd ACM SIGKDD Inter-
national Conference on Knowledge Discovery and
Data Mining, pages 1815–1824. ACM.

Valentin I Spitkovsky, Hiyan Alshawi, and Daniel Ju-
rafsky. 2010. From baby steps to leapfrog: How less
is more in unsupervised dependency parsing. In Hu-
man Language Technologies: The 2010 Annual Con-
ference of the North American Chapter of the Associ-
ation for Computational Linguistics, pages 751–759.
Association for Computational Linguistics.

Ilya Sutskever, Oriol Vinyals, and Quoc V Le. 2014.
Sequence to sequence learning with neural networks.
In Advances in neural information processing sys-
tems, pages 3104–3112.

Jason Weston, Antoine Bordes, Sumit Chopra, Alexan-
der M Rush, Bart van Merriënboer, Armand Joulin,
and Tomas Mikolov. 2016. Towards ai-complete
question answering: A set of prerequisite toy tasks.
5th International Conference on Learning Represen-
tations.

John T Wixted. 1990. Analyzing the empirical course
of forgetting. Journal of Experimental Psychology:
Learning, Memory, and Cognition, 16(5):927.

Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V
Le, Mohammad Norouzi, Wolfgang Macherey,
Maxim Krikun, Yuan Cao, Qin Gao, Klaus
Macherey, et al. 2016. Google’s neural machine
translation system: Bridging the gap between hu-
man and machine translation. arXiv preprint
arXiv:1609.08144.

2410


