



















































Detection of Topic and its Extrinsic Evaluation Through Multi-Document Summarization


Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 241–246,
Baltimore, Maryland, USA, June 23-25 2014. c©2014 Association for Computational Linguistics

Detection of Topic and its Extrinsic Evaluation Through Multi-Document
Summarization

Yoshimi Suzuki
Interdisciplinary Graduate School of

Medicine and Engineering
University of Yamanashi
Kofu, 400-8511, JAPAN

ysuzuki@yamanashi.ac.jp

Fumiyo Fukumoto
Interdisciplinary Graduate School of

Medicine and Engineering
University of Yamanashi
Kofu, 400-8511, JAPAN

fukumoto@yamanashi.ac.jp

Abstract

This paper presents a method for detect-
ing words related to a topic (we call them
topic words) over time in the stream of
documents. Topic words are widely dis-
tributed in the stream of documents, and
sometimes they frequently appear in the
documents, and sometimes not. We pro-
pose a method to reinforce topic words
with low frequencies by collecting docu-
ments from the corpus, and applied Latent
Dirichlet Allocation (Blei et al., 2003) to
these documents. For the results of LDA,
we identified topic words by using Mov-
ing Average Convergence Divergence. In
order to evaluate the method, we applied
the results of topic detection to extractive
multi-document summarization. The re-
sults showed that the method was effective
for sentence selection in summarization.

1 Introduction

As the volume of online documents has drastically
increased, the analysis of topic bursts, topic drift
or detection of topic is a practical problem attract-
ing more and more attention (Allan et al., 1998;
Swan and Allan, 2000; Allan, 2003; Klinken-
berg, 2004; Lazarescu et al., 2004; Folino et al.,
2007). The earliest known approach is the work
of Klinkenberg and Joachims (Klinkenberg and
Joachims, 2000). They have attempted to han-
dle concept changes by focusing a window with
documents sufficiently close to the target concept.
Mane et. al. proposed a method to generate
maps that support the identification of major re-
search topics and trends (Mane and Borner, 2004).
The method used Kleinberg’s burst detection al-
gorithm, co-occurrences of words, and graph lay-
out technique. Scholz et. al. have attempted to
use different ensembles obtained by training sev-
eral data streams to detect concept drift (Scholz,

2007). However the ensemble method itself re-
mains a problem that how to manage several clas-
sifiers effectively. He and Parket attempted to find
bursts, periods of elevated occurrence of events as
a dynamic phenomenon instead of focusing on ar-
rival rates (He and Parker, 2010). However, the
fact that topics are widely distributed in the stream
of documents, and sometimes they frequently ap-
pear in the documents, and sometimes not often
hamper such attempts.

This paper proposes a method for detecting
topic over time in series of documents. We rein-
forced words related to a topic with low frequen-
cies by collecting documents from the corpus, and
applied Latent Dirichlet Allocation (LDA) (Blei
et al., 2003) to these documents in order to ex-
tract topic candidates. For the results of LDA, we
applied Moving Average Convergence Divergence
(MACD) to find topic words while He et. al., ap-
plied it to find bursts. The MACD is a technique
to analyze stock market trends (Murphy, 1999). It
shows the relationship between two moving av-
erages of prices modeling bursts as intervals of
topic dynamics, i.e., positive acceleration. Fuku-
moto et. al also applied MACD to find topics.
However, they applied it only to the words with
high frequencies in the documents (Fukumoto et
al., 2013). In contrast, we applied it to the topic
candidates obtained by LDA.

We examined our method by extrinsic evalua-
tion, i.e., we applied the results of topic detection
to extractive multi-document summarization. We
assume that a salient sentence includes words re-
lated to the target topic, and an event of each doc-
uments. Here, an event is something that occurs
at a specific place and time associated with some
specific actions(Allan et al., 1998). We identified
event words by using the traditional tf∗idf method
applied to the results of named entities. Each sen-
tence in documents is represented using a vector
of frequency weighted words that can be event

241



or topic words. We used Markov Random Walk
(MRW) to compute the rank scores for the sen-
tences (Page et al., 1998). Finally, we selected a
certain number of sentences according to the rank
score into a summary.

2 Topic Detection

2.1 Extraction of Topic Candidates

LDA presented by (Blei et al., 2003) models
each document as a mixture of topics (we call
it lda topic to discriminate our topic candidates),
and generates a discrete probability distribution
over words for each lda topic. The generative pro-
cess for LDA can be described as follows:

1. For each topic k = 1, · · · , K, generate φk,
multinomial distribution of words specific to
the topic k from a Dirichlet distribution with
parameter β;

2. For each document d = 1, · · · , D, generate θd,
multinomial distribution of topics specific to
the document d from a Dirichlet distribution
with parameter α;

3. For each word n = 1, · · · , Nd in document d;
(a) Generate a topic zdn of the nth word

in the document d from the multinomial
distribution θd

(b) Generate a word wdn, the word associ-
ated with the nth word in document d
from multinomial φzdn

Like much previous work on LDA, we used Gibbs
sampling to estimate φ and θ. The sampling prob-
ability for topic zi in document d is given by:

P (zi | z\i, W ) =
(nv\i,j + β)(n

d
\i,j + α)

(n·\i,j + Wβ)(n
d
\i,· + Tα)

. (1)

z\i refers to a topic set Z, not including the cur-
rent assignment zi. nv\i,j is the count of word v
in topic j that does not include the current assign-
ment zi, and n·\i,j indicates a summation over that
dimension. W refers to a set of documents, and T
denotes the total number of unique topics. After
a sufficient number of sampling iterations, the ap-
proximated posterior can be used to estimate φ and
θ by examining the counts of word assignments to
topics and topic occurrences in documents. The

.........

.........

(i) Lda_topic clusters

(ii) Task clusters

topic id0 topic id1
cluster

task1 task3 task2

task2task1

task1doc ....

doc

....

....

task1 task2

topic id2

topic id1

topic id1

topic id0

topic id0

topic id2... ...

.... cluster

Figure 1: Lda topic cluster and task cluster

approximated probability of topic k in the docu-

ment d, θ̂kd , and the assignments word w to topic
k, φ̂wk are given by:

θ̂kd =
Ndk + α

Nd + αK
. (2)

φ̂wk =
Nkw + β

Nk + βV
. (3)

We used documents prepared by summarization
tasks, NTCIR and DUC data as each task consists
of series of documents with the same topic. We
applied LDA to the set consisting of all documents
in the summarization tasks and documents from
the corpus. We need to estimate the appropriate
number of lda topic.

Let k′ be the number of lda topics and d′ be
the number of topmost d′ documents assigned to
each lda topic. We note that the result obtained
by LDA can be regarded as the two types of clus-
tering result shown in Figure 1: (i) each cluster
corresponds to each lda topic (topic id0, topic id1
· · · in Figure 1), and each element of the clusters
is the document in the summarization tasks (task1,
task2, · · · in Figure 1) or from the corpus (doc in
Figure 1), and (ii) each cluster corresponds to the
summarization task and each element of the clus-
ters is the document in the summarization tasks
or the document from the corpus assigned topic
id. For example, DUC2005 consists of 50 tasks.
Therefore the number of different clusters is 50.
We call the former lda topic cluster and the latter
task cluster. We estimated k′ and d′ by using En-
tropy measure given by:

E = − 1
log l

∑

j

Nj
N

∑

i

P (Ai, Cj) log P (Ai, Cj).(4)

242



l refers to the number of clusters. P (Ai, Cj) is a
probability that the elements of the cluster Cj as-
signed to the correct class Ai. N denotes the total
number of elements and Nj shows the total num-
ber of elements assigned to the cluster Cj . The
value of E ranges from 0 to 1, and the smaller
value of E indicates better result. Let Etopic and
Etask are entropy value of lda topic cluster and
task cluster, respectively. We chose the parame-
ters k′ and d′ whose value of the summation of
Etopic and Etask is smallest. For each lda topic,
we extracted words whose probabilities are larger
than zero, and regarded these as topic candidates.

2.2 Topic Detection by MACD

The proposed method does not simply use MACD
to find bursts, but instead determines topic words
in series of documents. Unlike Dynamic Topic
Models (Blei and Lafferty, 2006), it does not as-
sume Gaussian distribution so that it is a natural
way to analyze bursts which depend on the data.
We applied it to extract topic words in series of
documents. MACD histogram defined by Eq. (6)
shows a difference between the MACD and its
moving average. MACD of a variable xt is defined
by the difference of n1-day and n2-day moving
averages, MACD(n1,n2) = EMA(n1) - EMA(n2).
Here, EMA(ni) refers to ni-day Exponential Mov-
ing Average (EMA). For a variable x = x(t) which
has a corresponding discrete time series x = {xt | t
= 0,1,· · · }, the n-day EMA is defined by Eq. (5).

EMA(n)[x]t = αxt + (1− α)EMA(n− 1)[x]t−1

=
n
∑

k=0

α(1− α)kxt−k. (5)

α refers to a smoothing factor and it is often taken
to be 2(n+1) . MACD histogram shows a difference

between the MACD and its moving average1.

hist(n1, n2, n3) = MACD(n1, n2)−
EMA(n3)[MACD(n1, n2)]. (6)

The procedure for topic detection with MACD
is illustrated in Figure 2. Let A be a series of doc-
uments and w be one of the topic candidates ob-
tained by LDA. Each document in A is sorted in
chronological order. We set A to the documents
from the summarization task. Whether or not a
word w is a topic word is judged as follows:

1In the experiment, we set n1, n2, and n3 to 4, 8 and 5,
respectively (He and Parker, 2010).

T T

T

Correct histogram Bursts histogram

Histogram similarity

bursts bursts

bursts

Figure 2: Topic detection with MACD

1. Create document-based MACD histogram
where X-axis refers to T , i.e., a period of time
(numbered from day 1 to 365). Y-axis is the
document count in A per day. Hereafter, re-
ferred to as correct histogram.

2. Create term-based MACD histogram where
X-axis refers to T , and Y-axis denotes bursts
of word w in A. Hereafter, referred to as
bursts histogram.

3. We assume that if a term w is informative
for summarizing a particular documents in
a collection, its burstiness approximates the
burstiness of documents in the collection.
Because w is a representative word of each
document in the task. Based on this assump-
tion, we computed similarity between correct
and word histograms by using KL-distance2.
Let P and Q be a normalized distance of
correct histogram, and bursts histogram, re-
spectively. KL-distance is defined by D(P ||
Q) =

∑
i=1 P (xi) log

P (xi)
Q(xi)

where xi refers
bursts in time i. If the value of D(P || Q)
is smaller than a certain threshold value, w is
regarded as a topic word.

3 Extrinsic Evaluation to Summarization

3.1 Event detection

An event word is something that occurs at a spe-
cific place and time associated with some spe-
cific actions (Allan, 2003; Allan et al., 1998). It
refers to notions of who(person), where(place),

2We tested KL-distance, histogram intersection and Bhat-
tacharyya distance to obtain similarities. We reported only
the result obtained by KL-distance as it was the best results
among them.

243



when(time) including what, why and how in a doc-
ument. Therefore, we can assume that named en-
tities(NE) are linguistic features for event detec-
tion. An event word refers to the theme of the
document itself, and frequently appears in the doc-
ument but not frequently appear in other docu-
ments. Therefore, we first applied NE recogni-
tion to the target documents to be summarized, and
then calculated tf∗idf to the results of NE recogni-
tion. We extracted words whose tf∗idf values are
larger than a certain threshold value, and regarded
these as event words.

3.2 Sentence extraction

We recall that our hypothesis about key sentences
in multiple documents is that they include topic
and event words. Each sentence in the docu-
ments is represented using a vector of frequency
weighted words that can be event or topic words.

Like much previous work on extractive sum-
marization (Erkan and Radev, 2004; Mihalcea
and Tarau, 2005; Wan and Yang, 2008), we used
Markov Random Walk (MRW) model to compute
the rank scores for the sentences. Given a set
of documents to be summarized, G = (S, E) is
a graph reflecting the relationships between two
sentences. S is a set of vertices, and each vertex
si in S is a sentence. E is a set of edges, and each
edge eij in E is associated with an affinity weight
f(i → j) between sentences si and sj (i 6= j). The
affinity weight is computed using cosine measure
between the two sentences, si and sj . Two ver-
tices are connected if their affinity weight is larger
than 0 and we let f(i → i)= 0 to avoid self tran-
sition. The transition probability from si to sj is
then defined as follows:

p(i → j) =



















f(i→j)
|S|
∑

k=1

f(i→k)
, if Σf 6= 0

0 , otherwise.

(7)

We used the row-normalized matrix Uij =
(Uij)|S|×|S| to describe G with each entry corre-
sponding to the transition probability, where Uij =
p(i → j). To make U a stochastic matrix, the rows
with all zero elements are replaced by a smoothing
vector with all elements set to 1|S| . The final transi-
tion matrix is given by formula (8), and each score
of the sentence is obtained by the principal eigen-
vector of the matrix M .

M = µUT +
(1− µ)
| S | ~e~e

T . (8)

We selected a certain number of sentences accord-
ing to rank score into the summary.

4 Experiments

4.1 Experimental settings

We applied the results of topic detection to ex-
tractive multi-document summarization task, and
examined how the results of topic detection af-
fect the overall performance of the salient sen-
tence selection. We used two tasks, Japanese and
English summarization tasks, NTCIR-33 SUMM
Japanese and DUC4 English data. The baselines
are (i) MRW model (MRW): The method ap-
plies the MRW model only to the sentences con-
sisted of noun words, (ii) Event detection (Event):
The method applies the MRW model to the result
of event detection, (iii) Topic Detection by LDA
(LDA): MRW is applied to the result of topic can-
didates detection by LDA and (iv) Topic Detec-
tion by LDA and MACD (LDA & MACD): MRW
is applied to the result of topic detection by LDA
and MACD only, i.e., the method does not include
event detection.

4.2 NTCIR data

The data used in the NTCIR-3 multi-document
summarization task is selected from 1998 to 1999
of Mainichi Japanese Newspaper documents. The
gold standard data provided to human judges con-
sists of FBFREE DryRun and FormalRun. Each
data consists of 30 tasks. There are two types of
correct summary according to the character length,
“long” and “short”, All series of documents were
tagged by CaboCha (Kudo and Matsumoto, 2003).
We used person name, organization, place and
proper name extracted from NE recognition (Kudo
and Matsumoto, 2003) for event detection, and
noun words including named entities for topic de-
tection. FBFREE DryRun data is used to tuning
parameters, i.e., the number of extracted words ac-
cording to the tf∗idf value, and the threshold value
of KL-distance. The size that optimized the aver-
age Rouge-1(R-1) score across 30 tasks was cho-
sen. As a result, we set tf∗idf and KL-distance to
100 and 0.104, respectively.

We used FormalRun as a test data, and another
set consisted of 218,724 documents from 1998 to
1999 of Mainichi newspaper as a corpus used in

3http://research.nii.ac.jp/ntcir/
4http://duc.nist.gov/pubs.html

244



Number of documents

Entropy

Figure 3: Entropy against the # of topics and doc-
uments

Method Short Long
R-1 R-1

MRW .369 .454
Event .625 .724
LDA .525 .712
LDA & MACD .630 .742
Event & Topic .678 .744

Table 1: Sentence Extraction (NTCIR-3 test data)

LDA and MACD. We estimated the number of k′

and d′ in LDA, i.e., we searched k′ and d′ in steps
of 100 from 200 to 900. Figure 3 illustrates en-
tropy value against the number of topics k′ and
documents d′ using 30 tasks of FormalRun data.
Each plot shows that at least one of the docu-
ments for each summarization task is included in
the cluster. We can see from Figure 3 that the
value of entropy depends on the number of doc-
uments rather than the number of topics. From
the result shown in Figure 3, the minimum entropy
value was 0.025 and the number of topics and doc-
uments were 400 and 300, respectively. We used
them in the experiment. The summarization re-
sults are shown in Table 1.

Table 1 shows that our approach, “Event &
Topic” outperforms other baselines, regardless of
the summary type (long/short). Topic candidates
include surplus words that are not related to the
topic because the results obtained by “LDA” were
worse than those obtained by “LDA & MACD”,
and even worse than “Event” in both short and
long summary. This shows that integration of
LDA and MACD is effective for topic detection.

4.3 DUC data

We used DUC2005 consisted of 50 tasks for train-
ing, and 50 tasks of DUC2006 data for testing in
order to estimate parameters. We set tf∗idf and

Method R-1 Method R-1
MRW .381 Event .407
LDA .402 LDA & MACD .428
Event & Topic .438
PYTHY .426 HybHSum .456
hPAM .412 TTM .447

Table 2: Comparative results (DUC2007 test data)

KL-distance to 80 and 0.9. The minimum en-
tropy value was 0.050 and the number of topics
and documents were 500 and 600, respectively.
45 tasks from DUC2007 were used to evaluate
the performance of the method. All documents
were tagged by Tree Tagger (Schmid, 1995) and
Stanford Named Entity Tagger 5 (Finkel et al.,
2005). We used person name, organization and lo-
cation for event detection, and noun words includ-
ing named entities for topic detection. AQUAINT
corpus6 which consists of 1,033,461 documents
are used as a corpus in LDA and MACD. Table
2 shows Rouge-1 against unigrams.

We can see from Table 2 that Rouge-1 obtained
by our approach was also the best compared to the
baselines. Table 2 also shows the performance of
other research sites reported by (Celikylmaz and
Hakkani-Tur, 2010). The top site was “HybH-
Sum” by (Celikylmaz and Hakkani-Tur, 2010).
However, the method is a semi-supervised tech-
nique that needs a tagged training data. Our ap-
proach achieves performance approaching the top-
performing unsupervised method, “TTM” (Ce-
likylmaz and Hakkani-Tur, 2011), and is compet-
itive to “PYTHY” (Toutanoval et al., 2007) and
“hPAM” (Li and McCallum, 2006). Prior work
including “TTM” has demonstrated the usefulness
of semantic concepts for extracting salient sen-
tences. For future work, we should be able to
obtain further advantages in efficacy in our topic
detection and summarization approach by disam-
biguating topic senses.

5 Conclusion

The research described in this paper explores a
method for detecting topic words over time in se-
ries of documents. The results of extrinsic evalu-
ation showed that integration of LDA and MACD
is effective for topic detection.

5http://nlp.stanford.edu/software/CRF-NER.shtml
6http://catalog.ldc.upenn.edu/LDC2002T31

245



References

J. Allan, J. Carbonell, G. Doddington, J. Yamron, and
Y. Yang. 1998. Topic Detection and Tracking Pilot
Study Final Report. In Proc. of the DARPA Broad-
cast News Transcription and Understanding Work-
shop.

J. Allan, editor. 2003. Topic Detection and Tracking.
Kluwer Academic Publishers.

D. M. Blei and J. D. Lafferty. 2006. Dynamic Topic
Models. In Proc. of the 23rd International Confer-
ence on Machine Learning, pages 113–120.

D. M. Blei, A. Y. Ng, and M. I. Jordan. 2003. La-
tent Dirichlet Allocation. In The Journal of Machine
Learning Research, volume 3, pages 993–1022.

A. Celikylmaz and D. Hakkani-Tur. 2010. A Hy-
bird Hierarchical Model for Multi-Document Sum-
marization. In Proc. of the 48th Annual Meeting
of the Association for Computational Linguistics,
pages 815–824.

A. Celikylmaz and D. Hakkani-Tur. 2011. Discovery
of Topically Coherent Sentences for Extractive Sum-
marization. In Proc. of the 49th Annual Meeting of
the Association for Computational Linguistics: Hu-
man Language Technologies, pages 491–499.

G. Erkan and D. Radev. 2004. LexPageRank: Prestige
in Multi-Document Text Summarization. In Proc. of
the 2004 Conference on Empirical Methods in Nat-
ural Language Processing, pages 365–371.

J. R. Finkel, T. Grenager, and C. Manning. 2005. In-
corporating Non-local Information into Information
Extraction Systems by Gibbs Sampling. In Proc.
of the 43rd Annual Meeting of the Association for
Computational Linguistics, pages 363–370.

G. Folino, C. Pizzuti, and G. Spezzano. 2007. An
Adaptive Distributed Ensemble Approach to Mine
Concept-Drifting Data Streams. In Proc. of the 19th
IEEE International Conference on Tools with Artifi-
cial Intelligence, pages 183–188.

F. Fukumoto, Y. Suzuki, A. Takasu, and S. Matsuyoshi.
2013. Multi-document summarization based on
event and topic detection. In Proc. of the 6th Lan-
guage and Technology Conference: Human Lan-
guage Technologies as a Challenge for Computer
Science and Linguistics, pages 117–121.

D. He and D. S. Parker. 2010. Topic Dynamics: An
Alternative Model of Bursts in Streams of Topics.
In Proc. of the 16th ACM Special Interest Group on
Knowledge Discovery and Data Mining, pages 443–
452.

R. Klinkenberg and T. Joachims. 2000. Detecting
Concept Drift with Support Vector Machines. In
Proc. of the 17th International Conference on Ma-
chine Learning, pages 487–494.

R. Klinkenberg. 2004. Learning Drifting Concepts:
Example Selection vs. Example Weighting. Intel-
leginet Data Analysis, 8(3):281–300.

T. Kudo and Y. Matsumoto. 2003. Fast methods for
kernel-based text analysis. In Proc. of 41st Annual
Meeting of the Association for Computational Lin-
guistics, pages 24–31.

M. M. Lazarescu, S. Venkatesh, and H. H. Bui. 2004.
Using Multiple Windows to Track Concept Drift.
Intelligent Data Analysis, 8(1):29–59.

W. Li and A. McCallum. 2006. Pachinko Alloca-
tion: Dag-Structure Mixture Model of Topic Cor-
relations. In Proc. of the 23rd International Confer-
ence on Machine Learning, pages 577–584.

K. Mane and K. Borner. 2004. Mapping Topics
and Topic Bursts in PNAS. Proc. of the National
Academy of Sciences of the United States of Amer-
ica, 101:5287–5290.

R. Mihalcea and P. Tarau. 2005. Language Indepen-
dent Extractive Summarization. In In Proc. of the
43rd Annual Meeting of the Association for Compu-
tational Linguistics, pages 49–52.

J. Murphy. 1999. Technical Analysis of the Financial
Markets. Prentice Hall.

L. Page, S. Brin, R. Motwani, and T. Winograd. 1998.
The Pagerank Citation Ranking: Bringing Order to
the Web. In Technical report, Stanford Digital Li-
braries.

H. Schmid. 1995. Improvements in Part-of-Speech
Tagging with an Application to German. In Proc. of
the European chapter of the Association for Compu-
tational Linguistics SIGDAT Workshop.

M. Scholz. 2007. Boosting Classifiers for Drifting
Concepts. Intelligent Data Analysis, 11(1):3–28.

R. Swan and J. Allan. 2000. Automatic Generation
of Overview Timelines. In Proc. of the 23rd An-
nual International ACM SIGIR Conference on Re-
search and Development in Information Retrieval,
pages 38–45.

K. Toutanoval, C. Brockett, M. Gammon, J. Jagarla-
mudi, H. Suzuki, and L. Vanderwende. 2007. The
Phthy Summarization System: Microsoft Research
at DUC. In Proc. of Document Understanding Con-
ference 2007.

X. Wan and J. Yang. 2008. Multi-Document Summa-
rization using Cluster-based Link Analysis. In Proc.
of the 31st Annual International ACM SIGIR Con-
ference on Research and Development in Informa-
tion Retrieval, pages 299–306.

246


