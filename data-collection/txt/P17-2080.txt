



















































Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics


Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 504–509
Vancouver, Canada, July 30 - August 4, 2017. c©2017 Association for Computational Linguistics

https://doi.org/10.18653/v1/P17-2080

Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 504–509
Vancouver, Canada, July 30 - August 4, 2017. c©2017 Association for Computational Linguistics

https://doi.org/10.18653/v1/P17-2080

A Conditional Variational Framework for Dialog Generation

Xiaoyu Shen1∗, Hui Su2∗, Yanran Li3, Wenjie Li3, Shuzi Niu2, Yang Zhao4
Akiko Aizawa4 and Guoping Long2

1Saarland University, Saarbrücken, Germany
2Institute of Software, Chinese Academy of Science, China

3The Hong Kong Polytechnic University, Hong Kong
4National Institute of Informatics, Tokyo, Japan

Abstract

Deep latent variable models have been
shown to facilitate the response generation
for open-domain dialog systems. How-
ever, these latent variables are highly ran-
domized, leading to uncontrollable gener-
ated responses. In this paper, we propose a
framework allowing conditional response
generation based on specific attributes.
These attributes can be either manually as-
signed or automatically detected. More-
over, the dialog states for both speakers are
modeled separately in order to reflect per-
sonal features. We validate this framework
on two different scenarios, where the at-
tribute refers to genericness and sentiment
states respectively. The experiment result
testified the potential of our model, where
meaningful responses can be generated in
accordance with the specified attributes.

1 Introduction

Seq2seq neural networks, ever since the success-
ful application in machine translation (Sutskever
et al., 2014), have demonstrated impressive re-
sults on dialog generation and spawned a great
deal of variants (Vinyals and Le, 2015; Yao
et al., 2015; Sordoni et al., 2015; Shang et al.,
2015). The vanilla seq2seq models suffer from
the problem of generating too many generic re-
sponses (generic denotes safe, commonplace re-
sponses like ”I don’t know”). One major rea-
son is that the element-wise prediction models
stochastical variations only at the token level, se-
ducing the system to gain immediate short re-
wards and neglect the long-term structure. To

∗Authors contributed equally. Correspondence should
be sent to H. Su (suhui15@iscas.ac.cn) and X. Shen
(xshen@lsv.uni-saarland.de)

cope with this problem, (Serban et al., 2017) pro-
posed a variational hierarchical encoder-decoder
model (VHRED) that brought the idea of varia-
tional auto-encoders (VAE) (Kingma and Welling,
2013; Rezende et al., 2014) into dialog genera-
tion. For each utterance, VHRED samples a la-
tent variable as a holistic representation so that the
generative process will learn to maintain a coher-
ent global sentence structure. However, the latent
variable is learned purely in an unsupervised way
and can only be explained vaguely as higher level
decisions like topic or sentiment. Though effec-
tive in generating utterances with more informa-
tion content, it lacks the ability of explicitly con-
trolling the generating process.

This paper presents a conditional variational
framework for generating specific responses, in-
spired by the semi-supervised deep generative
model (Kingma et al., 2014). The principle idea is
to generate the next response based on the dialog
context, a stochastical latent variable and an exter-
nal label. Furthermore, the dialog context for both
speakers is modeled separately because they have
different talking styles, personality and sentiment.
The whole network structure functions like a con-
ditional VAE (Sohn et al., 2015; Yan et al., 2016).
We test our framework on two scenarios. For the
first scenario, the label serves as a signal to indi-
cate whether the response is generic or not. By as-
signing different values to the label, either generic
or non-generic responses can be generated. For
the second scenario, the label represents an imi-
tated sentiment tag. Before generating the next re-
sponse, the appropriate sentiment tag is predicted
to direct the generating process.

Our framework is expressive and extendable.
The generated responses agree with the predefined
labels while maintaining meaningful. By chang-
ing the definition of the label, our framework can

504

https://doi.org/10.18653/v1/P17-2080
https://doi.org/10.18653/v1/P17-2080


Figure 1: Computational graph for SPHRED
structure. The status vector for Speaker A and
Speaker B is modeled by separate RNNs then con-
catenated to represent the dialog context.

be easily applied to other specific areas.

2 Models

To provide a better dialog context, we build a hier-
archical recurrent encoder-decoder with separated
context models (SPHRED). This section first in-
troduces the concept of SPHRED, then explains
the conditional variational framework and two ap-
plication scenarios.

2.1 SPHRED

We decomposes a dialog into two levels: se-
quences of utterances and sub-sequences of words,
as in (Serban et al., 2016). Let w1, . . . ,wN
be a dialog with N utterances, where wn =
(wn,1, . . . , wn,Mn) is the n-th utterance. The prob-
ability distribution of the utterance sequence fac-
torizes as:

N∏

n=1

Mn∏

m=1

Pθ(wm,n|wm,<n,w<n) (1)

where θ represents the model parameters and w<n
encodes the dialog context until step n.

If we model the dialog context through a single
recurrent neural network (RNN), it can only rep-
resent a general dialog state in common but fail to
capture the respective status for different speakers.
This is inapplicable when we want to infer implicit
personal attributes from it and use them to influ-
ence the sampling process of the latent variable,
as we will see in Section 2.4. Therefore, we model
the dialog status for both speakers separately. As
displayed in Figure 1, SPHRED contains an en-
coder RNN of tokens and two status RNNs of ut-
terances, each for one speaker. When modeling
turn k in a dialog, each status RNN takes as in-
put the last encoder RNN state of turn k − 2. The

Figure 2: Graphical model for the conditional
variational framework. Solid lines denote gen-
erative model Pθ(zn|yn,wn−11 ) and Pθ(wn |
yn, zn,w

n−1
1 ). When y

t+1 is known, there exists
an additional link from yt+1 to z (dashed line). Ct

encodes context information up to time t. Dotted
lines are posterior approximation Qφ(zn|yn,wn1 ).

higher-level context vector is the concatenation of
both status vectors.

We will show later that SPHRED not only well
keeps individual features, but also provides a bet-
ter holistic representation for the response decoder
than normal HRED.

2.2 Conditional Variational Framework
VAEs have been used for text generation in (Bow-
man et al., 2015; Semeniuta et al., 2017), where
texts are synthesized from latent variables. Start-
ing from this idea, we assume every utterance
wn comes with a corresponding label yn and la-
tent variable zn. The generation of zn and wn
are conditioned on the dialog context provided by
SPHRED, and this additional class label yn. This
includes 2 situations, where the label of the next
sequence is known (like for Scenario 1 in Section
2.3) or not (Section 2.4). For each utterance, the
latent variable zn is first sampled from a prior dis-
tribution. The whole dialog can be explained by
the generative process:

Pθ(zn|yn,wn−11 ) = N (µprior,Σprior) (2)

Pθ(wn | yn, zn,wn−11 ) =
Mn∏

m=1

Pθ(wn,m | yn, zn,wn−11 , wn,m−1n,1 )
(3)

When the label yn is unknown, a suitable classi-
fier is implemented to first predict it from the con-
text vector. This classifier can be designed as, but
not restricted to, multilayer perceptrons (MLP) or
support vector machines (SVM).

Similarly, the posterior distribution of zn is ap-
proximated as in Equation 4, where the context

505



and label of the next utterance is provided. The
graphical model is depicted in Figure 2.

Qφ(zn|yn,wn1 ) = N (µposterior,Σposterior) (4)

The training objective is derived as in For-
mula 5, which is a lower bound of the logarithm
of the sequence probability. When the label is
to be predicted (ȳn), an additional classification
loss (first term) is added such that the distribution
qφ(yn|wn−11 ) can be learned together with other
parameters.

logPθ(w1, . . . ,wN ) ≥ Ep(wn,yn)
[
qφ(yn|wn−11 )

]

−
N∑

n=1

KL
[
Qψ(zn | wn1 ,yn)||Pθ(zn | wn−11 , ȳn)

]

+ EQψ(zn|wn1 ,yn)[logPθ(wn | zn,w
n−1
1 ,yn)]

(5)

2.3 Scenario 1
A major focus in the current research is to avoid
generating generic responses, so in the first sce-
nario, we let the label y indicate whether the cor-
responding sequence is a generic response, where
y = 1 if the sequence is generic and y = 0 oth-
erwise. To acquire these labels, we manually con-
structed a list of generic phrases like “I have no
idea”, “I don’t know”, etc. Sequences containing
any one of such phrases are defined as generic,
which in total constitute around 2 percent of the
whole corpus. At test time, if the label is fixed as
0, we expect the generated response should mostly
belong to the non-generic class.

No prediction is needed, thus the training cost
does not contain the first item in Formula 5. This
scenario is designed to demonstrate our frame-
work can explicitly control which class of re-
sponses to generate by assigning corresponding
values to the label.

2.4 Scenario 2
In the second scenario, we experiment with as-
signing imitated sentiment tags to generated re-
sponses. The personal sentiment is simulated by
appending :), :( or :P at the end of each utter-
ance, representing positive, negative or neutral
sentiment respectively. For example, if we ap-
pend ”:)” to the original ”OK”, the resulting ”OK
:)” becomes positive. The initial utterance of ev-
ery speaker is randomly tagged. We consider two
rules for the tags of next utterances. Rule 1 con-
fines the sentiment tag to stay constant for both

speakers. Rule 2 assigns the sentiment tag of next
utterance as the average of the preceding two ones.
Namely, if one is positive and the other is negative,
the next response would be neutral.

The label y represents the sentiment tag, which
is unknown at test time and needs to be predicted
from the context. The probability qφ(yn|wn−11 )
is modeled by feedforward neural networks. This
scenario is designed to demonstrate our frame-
work can successfully learn the manually defined
rules to predict the proper label and decode re-
sponses conforming to this label.

3 Experiments

We conducted our experiments on the Ubuntu di-
alog Corpus (Lowe et al., 2015), which contains
about 500,000 multi-turn dialogs. The vocabulary
was set as the most frequent 20,000 words. All the
letters are transferred to lowercase and the Out-
of-Vocabulary (OOV) words were preprocessed as
<unk> tokens.

3.1 Training Procedures

Model hyperparameters were set the same as in
VHRED model except that we reduced by half
the context RNN dimension. The encoder, con-
text and decoder RNNs all make use of the Gated
Recurrent Unit (GRU) structure (Cho et al., 2014).
Labels were mapped to embeddings with size 100
and word vectors were initialized with the pu-
bic Word2Vec embeddings trained on the Google
News Corpus1. Following (Bowman et al., 2015),
25% of the words in the decoder were randomly
dropped. We multiplied the KL divergence and
classification error by a scalar which starts from
zero and gradually increases so that the training
would initially focus on the stochastic latent vari-
ables. At test time, we outputted responses us-
ing beam search with beam size set to 5 (Graves,
2012) and <unk> tokens were prevented from
being generated. We implemented all the mod-
els with the open-sourced Python library Tensor-
flow (Abadi et al., 2016) and optimized using the
Adam optimizer (Kingma and Ba, 2014). Dialogs
are cut into set of slices with each slice containing
80 words then fed into the GPU memory. All mod-
els were trained with batch size 128. We use the
learning rate 0.0001 for our framework and 0.0002
for other models. Every model is tested on the val-

1https://code.google.com/archive/p/
word2vec/

506



idation dataset once every epoch and stops until it
gains nothing more within 5 more epochs.

3.2 Evaluation

Accurate automatic evaluation of dialog gener-
ation is difficult (Galley et al., 2015; Pietquin
and Hastie, 2013). In our experiment, we con-
ducted three embedding-based evaluations (aver-
age, greedy and extrema) (Liu et al., 2016) on
all our models, which map responses into vector
space and compute the cosine similarity. Though
not necessarily accurate, the embedding-based
metrics can to a large extent measure the semantic
similarity and test the ability of successfully gen-
erating a response sharing a similar topic with the
golden answer. The results of a GRU language
model (LM), HRED and VHRED were also pro-
vided for comparison. For the two scenarios of our
framework, we further measured the percentage
of generated responses matching the correct labels
(accuracy). In (Liu et al., 2016), current popular
metrics are shown to be not well correlated with
human judgements. Therefore, we also carried out
a human evaluation. 100 examples were randomly
sampled from the test dataset. The generated re-
sponses from the models were shuffled and ran-
domly distributed to 5 volunteers2. People were
requested to give a binary score to the response
from 3 aspects, grammaticality, coherence with
history context and diversity. Every response was
evaluated 3 times and the result agreed by most
people was adopted.

3.3 Results of Metric-based Evaluation

As can be seen from Table 1, SPHRED outper-
forms both HRED and LM over all the three
embedding-based metrics. This implies separating
the single-line context RNN into two independent
parts can actually lead to a better context represen-
tation. It is worth mentioning the size of context
RNN hidden states in SPHRED is only half of that
in HRED, but it still behaves better with fewer pa-
rameters. Hence it is reasonable to apply this con-
text information to our framework.

The last 4 rows in Table 1 display the results
of our framework applied in two scenarios men-
tioned in Section 2.3 and 2.4. SCENE1-A and
SCENE1-B correspond to Scenario 1 with the la-
bel fixed as 1 and 0. 90.9% of generated responses

2All volunteers are well-educated students who have re-
ceived a Bachelor’s degree on computer science or above.

in SCENE1-A are generic and 86.9% in SCENE1-
B are non-generic according to the manually-built
rule, which verified the proper effect of the label.
SCENE2-A and SCENE2-B correspond to rule 1
and 2 in Scenario 2. Both successfully predict
the sentiment with very minor mismatches (0.2%
and 0.8%). The high accuracy further demon-
strated SPHRED’s capability of maintaining indi-
vidual context information. We also experimented
by substituting the encoder with a normal HRED,
the resulting model cannot predict the correct sen-
timent at all because the context information is
highly mingled for both speakers. The embedding
based scores of our framework are still compara-
ble with SPHRED and even better than VHRED.
Imposing an external label didn’t bring any signif-
icant quality decline.

Model Average Greedy Extrema Accuracy
LM 0.360 0.348 0.310 -
HRED 0.429 0.466 0.383 -
SPHRED 0.468 0.478 0.434 -
VHRED 0.403 0.432 0.374 -
SCENE1-A - - - 90.9%
SCENE1-B 0.426 0.432 0.396 86.9%
SCENE2-A 0.465 0.440 0.428 99.8%
SCENE2-B 0.463 0.437 0.420 99.2%

Table 1: Metric-based Evaluation. SCENE1-A is
set to generate generic responses, so it makes no
sense to measure it with embedding-based metrics

3.4 Results of Human Evaluation

We conducted human evaluations on VHRED and
our framework (Table 3). All models share similar
scores, except SCENE1-A receiving lower scores
with respect to coherence. This can be explained
by the fact that SCENE1-A is trained to generate
only generic responses, which limits its power of
taking coherence into account. VHRED and Sce-
nario 2 perform close to each other. Scenario 1,
due to the effect of the label, receives extreme
scores for diversity.

In general, the statistical results of human eval-
uations on sentence quality are very similar be-
tween the VHRED model and our framework.
This agrees with the metric-based results and sup-
ports the conclusion drawn in Section 3.3. Though
the sample size is relatively small and human
judgements can be inevitably disturbed by subjec-
tive factors, we believe these results can shed some
light on the understanding of our framework.

A snippet of the generated responses can be

507



Context Response
anybody in the house ????? eou how to change the default
ubuntu wall paper ? eou how to change the default ubuntu
wallpaper ? eou eot Is there an echo in your head ? Is
there an echo in your head ? eou eot what do you mean
? eou eot Repeating = Bad . eou eot no body is
answering me eou eot

.
LM: What do you want to do with it ?
HRED: I don’t know .
SPHRED: If you want to change the
default wallpaper , you can change the
default theme

How can I install seamonkey ? eou To save me the pastebin
eou I am looking to install seamonkey , anyone ? eou
eot http://www.seamonkey-project.org/ eou eot It i

snot in the ubuntu repository any more eou eot

.
SCENE1-A: sorry i have no idea .
SCENE1-B: you need to find the
package that you can use .

hey guys , how can I add an extra xsession to ubuntu 10.04 ? :)
eou that is , I dont want GNOME :) eou eot try this

: https://wiki.ubuntu. com/CustomXSession :( eou eot

.
SCENE2-A: ok thanks :)

hey guys , how can I add an extra xsession to ubuntu 10.04 ? :(
eou that is , I dont want GNOME :( eou eot try this

: https://wiki.ubuntu. com/CustomXSession :) eou eot

.
SCENE2-B: thank you for the help !
:P

Table 2: Examples of context-response pairs for the neural network models. eou denotes end-of-
utterance and eot denotes end-of-turn token

Model G CD C¬D ¬CD ¬C¬D
VHRED 97% 41% 12 % 24% 23%
SCENE1-A 96% 3% 37% 1% 59%
SCENE1-B 96% 47% 9% 40% 4%
SCENE2-A 97% 40% 14 % 23% 23%
SCENE2-B 95% 38% 20% 31% 11%

Table 3: Human Judgements, G refers to Gram-
maticality and the last four columns is the confu-
sion matrix with respect to coherence and diversity

seen in Table 2. Generally speaking, SPHRED
better captures the intentions of both speakers,
while HRED updates the common context state
and the main topic might gradually vanish for the
different talking styles of speakers. SCENE1-A
and SCENE1-B are designed to reply to a given
context in two different ways. We can see both re-
sponses are reasonable and fit into the right class.
The third and fourth rows are the same context
with different appended sentiment tags and rules,
both generate a suitable response and append the
correct tag at the end.

4 Discussion and future work

In this work, we propose a conditional varia-
tional framework for dialog generation and ver-
ify it on two scenarios. To model the dialog state
for both speakers separately, we first devised the
SPHRED structure to provide the context vec-
tor for our framework. Our evaluation results

show that SPHRED can itself provide a better con-
text representation than HRED and help generate
higher-quality responses. In both scenarios, our
framework can successfully learn to generate re-
sponses in accordance with the predefined labels.
Though with the restriction of an external label,
the score of generated responses didn’t signifi-
cantly decreased, meaning that we can constrain
the generation within a specific class while still
maintaining the quality.

The manually-defined rules, though primitive,
represent two most common sentiment shift con-
ditions in reality. The results demonstrated the
potential of our model. To apply to real-world
scenarios, we only need to adapt the classifier
to detect more complex sentiments, which we
leave for future research. External models can be
used for detecting generic responses or classify-
ing sentiment categories instead of rule or symbol-
based approximations. We focused on the con-
trolling ability of our framework, future research
can also experiment with bringing external knowl-
edge to improve the overall quality of generated
responses.

5 Acknowledgement

This work was supported by the National Natu-
ral Science of China under Grant No. 61602451,
61672445 and JSPS KAKENHI Grant Numbers
15H02754, 16K12546.

508



References
Martı́n Abadi, Ashish Agarwal, Paul Barham, Eugene

Brevdo, Zhifeng Chen, Craig Citro, Greg S Corrado,
Andy Davis, Jeffrey Dean, Matthieu Devin, et al.
2016. Tensorflow: Large-scale machine learning on
heterogeneous distributed systems. arXiv preprint
arXiv:1603.04467 .

Samuel R Bowman, Luke Vilnis, Oriol Vinyals, An-
drew M Dai, Rafal Jozefowicz, and Samy Ben-
gio. 2015. Generating sentences from a continuous
space. arXiv preprint arXiv:1511.06349 .

Kyunghyun Cho, Bart Van Merriënboer, Caglar Gul-
cehre, Dzmitry Bahdanau, Fethi Bougares, Holger
Schwenk, and Yoshua Bengio. 2014. Learning
phrase representations using rnn encoder-decoder
for statistical machine translation. arXiv preprint
arXiv:1406.1078 .

Michel Galley, Chris Brockett, Alessandro Sordoni,
Yangfeng Ji, Michael Auli, Chris Quirk, Mar-
garet Mitchell, Jianfeng Gao, and Bill Dolan. 2015.
deltableu: A discriminative metric for genera-
tion tasks with intrinsically diverse targets. arXiv
preprint arXiv:1506.06863 .

Alex Graves. 2012. Sequence transduction with
recurrent neural networks. arXiv preprint
arXiv:1211.3711 .

Diederik Kingma and Jimmy Ba. 2014. Adam: A
method for stochastic optimization. arXiv preprint
arXiv:1412.6980 .

Diederik P Kingma, Shakir Mohamed, Danilo Jimenez
Rezende, and Max Welling. 2014. Semi-supervised
learning with deep generative models. In Advances
in Neural Information Processing Systems. pages
3581–3589.

Diederik P Kingma and Max Welling. 2013. Auto-
encoding variational bayes. arXiv preprint
arXiv:1312.6114 .

Chia-Wei Liu, Ryan Lowe, Iulian V Serban, Michael
Noseworthy, Laurent Charlin, and Joelle Pineau.
2016. How not to evaluate your dialogue system:
An empirical study of unsupervised evaluation met-
rics for dialogue response generation. arXiv preprint
arXiv:1603.08023 .

Ryan Lowe, Nissan Pow, Iulian Serban, and Joelle
Pineau. 2015. The ubuntu dialogue corpus: A large
dataset for research in unstructured multi-turn dia-
logue systems. arXiv preprint arXiv:1506.08909 .

Olivier Pietquin and Helen Hastie. 2013. A survey on
metrics for the evaluation of user simulations. The
knowledge engineering review 28(01):59–73.

Danilo Jimenez Rezende, Shakir Mohamed, and Daan
Wierstra. 2014. Stochastic backpropagation and
approximate inference in deep generative models.
arXiv preprint arXiv:1401.4082 .

Stanislau Semeniuta, Aliaksei Severyn, and Erhardt
Barth. 2017. A hybrid convolutional variational
autoencoder for text generation. arXiv preprint
arXiv:1702.02390 .

Iulian V Serban, Alessandro Sordoni, Yoshua Bengio,
Aaron Courville, and Joelle Pineau. 2016. Building
end-to-end dialogue systems using generative hier-
archical neural network models. AAAI .

Iulian Vlad Serban, Alessandro Sordoni, Ryan Lowe,
Laurent Charlin, Joelle Pineau, Aaron Courville,
and Yoshua Bengio. 2017. A hierarchical latent
variable encoder-decoder model for generating di-
alogues. AAAI .

Lifeng Shang, Zhengdong Lu, and Hang Li. 2015.
Neural responding machine for short-text conversa-
tion. arXiv preprint arXiv:1503.02364 .

Kihyuk Sohn, Honglak Lee, and Xinchen Yan. 2015.
Learning structured output representation using
deep conditional generative models. In Advances
in Neural Information Processing Systems. pages
3483–3491.

Alessandro Sordoni, Michel Galley, Michael Auli,
Chris Brockett, Yangfeng Ji, Margaret Mitchell,
Jian-Yun Nie, Jianfeng Gao, and Bill Dolan. 2015.
A neural network approach to context-sensitive gen-
eration of conversational responses. arXiv preprint
arXiv:1506.06714 .

Ilya Sutskever, Oriol Vinyals, and Quoc V Le. 2014.
Sequence to sequence learning with neural net-
works. In Advances in neural information process-
ing systems. pages 3104–3112.

Oriol Vinyals and Quoc Le. 2015. A neural conversa-
tional model. arXiv preprint arXiv:1506.05869 .

Xinchen Yan, Jimei Yang, Kihyuk Sohn, and Honglak
Lee. 2016. Attribute2image: Conditional image
generation from visual attributes. In European Con-
ference on Computer Vision. Springer, pages 776–
791.

Kaisheng Yao, Geoffrey Zweig, and Baolin Peng.
2015. Attention with intention for a neu-
ral network conversation model. arXiv preprint
arXiv:1510.08565 .

509


	A Conditional Variational Framework for Dialog Generation

