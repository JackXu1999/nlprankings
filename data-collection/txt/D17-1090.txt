



















































Question Generation for Question Answering


Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 866–874
Copenhagen, Denmark, September 7–11, 2017. c©2017 Association for Computational Linguistics

Question Generation for Question Answering

Nan Duan†, Duyu Tang†, Peng Chen§, Ming Zhou†
Microsoft Research Asia†

Microsoft Research and AI Group§

{nanduan,dutang,peche,mingzhou}@microsoft.com

Abstract

This paper presents how to generate ques-
tions from given passages using neural
networks, where large scale QA pairs
are automatically crawled and processed
from Community-QA website, and used as
training data. The contribution of the pa-
per is 2-fold: First, two types of question
generation approaches are proposed, one
is a retrieval-based method using convo-
lution neural network (CNN), the other is
a generation-based method using recurrent
neural network (RNN); Second, we show
how to leverage the generated questions to
improve existing question answering sys-
tems. We evaluate our question generation
method for the answer sentence selection
task on three benchmark datasets, includ-
ing SQuAD, MS MARCO, and WikiQA.
Experimental results show that, by using
generated questions as an extra signal, sig-
nificant QA improvement can be achieved.

1 Introduction

Question Answering (or QA) is one of the core
problems for AI, and consists of several typical
tasks, i.e. community-based QA (Qiu and Huang,
2015), knowledge-based QA (Berant et al., 2013),
text-based QA (Yu et al., 2014), and reading com-
prehension (Rajpurkar et al., 2016). Most of cur-
rent QA systems, e.g. (Berant and Liang, 2014),
(Qiu and Huang, 2015), (Xiong et al., 2017), (Yin
and Schtze, 2017), need labeled QA pairs as train-
ing data. Although labeling efforts have been
made, such as WebQuestions dataset (Berant et al.,
2013) and SimpleQuestions dataset (Bordes et al.,
2015) for knowledge-based QA, WikiQA dataset
(Yang et al., 2015) for text-based QA, SQuAD
dataset (Rajpurkar et al., 2016) and MS MARCO

dataset (Nguyen et al., 2016) for reading compre-
hension, these datasets are still with limited sizes,
as labeling is very expensive.

Motivated by this, we explore how to generate
questions from given passages using neural net-
works, with three expected goals: (1) the training
data should need few or no human efforts and re-
flect commonly-asked question intentions; (2) the
questions are generated based on natural language
passages, and should have good quality; (3) the
generated questions should be helpful to QA tasks.

To achieve the 1st goal, we propose to ac-
quire large scale high-quality training data from
Community-QA (CQA) website. The motivation
of using CQA website for training data collection
is that, such websites (e.g., YahooAnswers, Quo-
ra, etc.) contain large scale QA pairs generated
by real users, and these questions reflect the most
common user intentions, and therefore are useful
to search, QA, and chatbot scenarios.

To achieve the 2nd goal, we explore two ways
to generate questions for a given passage, one is
a retrieval-based method using convolution neural
network (CNN), the other is a generation-based
method using recurrent neural network (RNN).
We evaluate the generation quality by BLEU score
(Papineni et al., 2002) and human annotations, and
discuss their pros and cons in Section 9.

To achieve the 3rd goal, we integrate our ques-
tion generation approach into an end-to-end QA
task, i.e., answer sentence selection, and evaluate
its impact on three popular benchmark datasets,
SQuAD, MS MARCO, and WikiQA. Experimen-
tal results show that, the generated questions can
improve the QA quality on all these three datasets.

2 Question Generation

Formally, given a passage S , question generation
(QG) engine generates a set of questions {Qi},

866



where each generated Qi can be answered by S.
There are four components in our QG engine:

1. Question Pattern Mining, which extracts the
frequently-asked question patterns from large
scale CQA questions, without any human an-
notation effort;

2. Question Pattern Prediction, which predicts
top-N question patterns Q1p, ...,QNp given S,
by a retrieval-based method or a generation-
based method. Therefore, “Prediction” has
two different meanings here: in retrieval-
based method, it means to rank existing ques-
tion patterns and select the highest ranked
ones, while in generation-based method, it
means to generate question patterns based on
S in a sequence-to-sequence manner, each of
which could be a totally new question pattern
beyond the existing question pattern set;

3. Question Topic Selection, which selects a
phraseQt from S as the question topic, based
on a predicted question pattern Qp. Qt will
be filled intoQp to form a complete question;

4. Question Ranking, which ranks all generated
questions by a set of features. Here, multiple
questions with different intentions could be
generated, as S could contain multiple facts.

3 Question Pattern Mining

A question pattern (or QP) is defined as a word se-
quence Qp = {w1, w2, ..., wL}. Each QP should
contain one and only one special word “#” as the
placeholder, and all the other L− 1 words are ter-
minal words. For example, “who founded # ?” is a
question pattern, and “#” denotes the placeholder,
which could be an organization name. We gener-
ate questions only using frequently-asked question
patterns, where “frequently-asked” denotes that
each question pattern should be extracted from a
large scale question set more than T times, where
T denotes a pre-defined threshold.

In this paper, a question cluster (or QC) based
approach is proposed to mine frequently-asked
question patterns from large scale CQA questions.

First, a set of question clusters is collected from
CQA webpages, and each question cluster consists
of questions that are grouped as related question-
s1 by the CQA website. For example, when the

1Usually, each question page of a CQA website contains
a field that shows a list of related questions.

query “what is the population of nyc” is issued to
YahooAnswers2, the returned page contains a list
of related questions including “population of nyc”,
“nyc population”, “nyc census”, and etc.

Second, for each question cluster QC =
{Q1, ...,QK}, we enumerate all valid continuous
n-grams, each of which should contain at least one
content word and its order should be equal or less
then 7, as question topic candidates {Q1t , ...,QMt }.
We then assign an importance score Impts(·) to
each question topic candidate Qmt :

Impts(Qmt ) =
∑
Qk∈QC

δ(Qmt ,Qk) · |Qmt |

Qmt denotes the mth question topic candidate,
δ(Qmt ,Qk) equals to 1 when Qmt occurs in Qk,
and 0 otherwise, |Qmt | denotes Qmt ’s word count,
which boosts longer question topic candidates.

For each QC, we select Qt with the highest im-
portance score as the question topic, and remove
it from each question to form a question pattern.
We call each removed question topic as a histor-
ical question topic of its corresponding question
pattern. IfQt doesn’t exist in a question, ignore it.

Mining question patterns based on question
clusters is motivated by the observation that, all
questions within a QC tend to ask questions about
an identical “question topic” (e.g., nyc in the above
example) from different aspects, or the same as-
pect but using different expressions. Thus, we can
leverage the consensus information among ques-
tions to detect the boundary of the question topic:
the more times an n-gram occurs in different ques-
tions within a question cluster, the more likely it is
the question topic of the current question cluster.

Although the question pattern mining approach
described above is simple, it works surprisingly
well. Table 1 shows statistics of question patterns
mined from YahooAnswers, and Figure 1 gives ex-
amples of frequently-asked question patterns with
their corresponding historical question topics. We
have two interesting observations:

1. Most frequently-asked question patterns (fre-
quency>=10,000) are with high quality, and
reflect the most common user intentions;

2. Most historical question topics extracted are
entities. This is achieved without using any
prior semantic knowledge base or dictionary.

2https://answers.yahoo.com/

867



Figure 1: Examples of frequently-asked question patterns with corresponding historical question topics.

# of QCs 67,330,506
# of QPs 20,818,569

# of QPs (freq. >= 10,000) 105,623

Table 1: Statistics of question patterns mined from
YahooAnswers, where QC denotes Question Clus-
ter and QP denotes Question Pattern.

4 Question Pattern Prediction

Given a passage S, question pattern prediction
predicts S’s most related question patterns, and
then use them to generate questions. For exam-
ple, given S as “Tesla Motors is an American au-
tomaker and energy storage company co-founded
by Elon Musk, Martin Eberhard, Marc Tarpen-
ning, JB Straubel and Ian Wright, and is based in
Palo Alto.”, two question patterns can be derived
from S, including: (1) who founded # ?, which
can be inferred by the context around “co-founded
by”, and (2) where is # located ?, which can be in-
ferred by the context around “is based in”. Based
on these two question patterns, we can generate t-
wo questions, “who founded Tesla Motors ?” and
“where is Tesla Motors located ?” respectively.

4.1 Training Data Construction

We collect QA pairs from YahooAnswers. For
each QA pair < Q, A >, if (1) Q can be matched
by a frequently-asked question patternQp, and (2)
the matched question topic Qt of Q based on Qp
exists in A, then we create a training instance as
< A,Qp,Qt >. Qt ∈ Amakes sure that the ques-
tion topic occurs in both Q and A. If the matched
question topic only exists in Q, we just discard it.
By doing so, we collect a total of 1,984,401 train-
ing instances as training data for QP prediction.

Two neural network-based question pattern pre-

diction approaches are explored in this paper:

• Retrieval-based QP Prediction, which con-
siders QP prediction as a ranking task;

• Generation-based QP Prediction, which con-
siders QP prediction as a generation task.

4.2 Retrieval-based QP Prediction
The retrieval-based QP prediction is done based
on an attention-based convolution neural network.
It takes a passage and a question pattern as input,
and outputs their corresponding vector representa-
tions. We denote each input pair as 〈S,Qp〉, where
S is a passage, and Qp is a question pattern.

In the input layer, given an input pair 〈S,Qp〉,
an attention matrix Att ∈ <|S|×|Qp| is generat-
ed by pre-trained word embeddings of S and Qp,
where each element Atti,j ∈ Att is computed as:

Atti,j = cosine(vSi , vQpj )

where vSi (or v
Qp
j ) denotes the embedding vector

of the ith (or jth) word in S (or Qp).
Then, column-wise and row-wise max-pooling

are applied to Att to generate two attention vec-
tors V S ∈ <|S| and V Qp ∈ <|Qp|, where the kth
elements of V S and V Qp are computed as:

V Sk = max
1<l<|Qp|

{Attk,l} and V Qpk = max
1<l<|S|

{Attl,k}

V Sk (or V
Qp
k ) can be interpreted as the attention

score of the kth word in S (or Qp) with regard to
all words in Qp (or S).

Next, two attention distributions DS ∈ <|S|
and DQp ∈ <|Qp| are generated for S and Qp
based on V S and V Qp respectively, where the kth
elements of DS and DQp are computed as:

DSk =
eV

S
k∑|S|

l=1
eV

S
l

and D
SY
k =

eV
Qp
k∑|Qp|

l=1
eV

Qp
l

868



DSXk (or D
SY
k ) can be interpreted as the normal-

ized attention score of the kth word in S (or Qp)
with regard to all words in Qp (or S).

Last, we update each pre-trained word embed-
ding vSk (or v

Qp
k ) to v̂

S
k (or v̂

Qp
k ), by multiplying

every value in vSk (or v
Qp
k ) withD

S
k (orD

Qp
k ). The

underlying intuition of updating pre-trained word
embeddings is to re-weight the importance of each
word in S (or Qp) based on Qp (or S), instead of
treating them in an equal manner.

In the convolution layer, we first derive an in-
put matrix ZS = {l1, ..., l|S|}, where lt is the con-
catenation of a sequence of m = 2d − 13 updat-
ed word embeddings [v̂St−d, ..., v̂

S
t , ..., v̂

S
t+d], cen-

tralized in the tth word in S. Then, the convo-
lution layer performs sliding window-based fea-
ture extraction to project each vector representa-
tion lt ∈ ZS to a contextual feature vector hSt :

hSt = tanh(Wc · lt)

where Wc is the convolution matrix, tanh(x) =
1−e−2x
1+e−2x is the activation function. The same oper-
ation is performed to Qp as well.

In the pooling layer, we aggregate local fea-
tures extracted by the convolution layer from S,
and form a sentence-level global feature vector
with a fixed size independent of the length of the
input sentence. Here, max-pooling is used to force
the network to retain the most useful local features
by lSp = [vS1 , ..., vSK ], where:

vSi = max
t=1,...,|S|

{hSt (i)}

hSt (i) denotes the ith value in the vector hSt . The
same operation are performed to Qp as well.

In the output layer, one more non-linear trans-
formation is applied to lSp :

y(S) = tanh(Ws · lSp )

Ws is the semantic projection matrix, y(S) is the
final sentence embedding of S. The same opera-
tion is performed to Qp to obtain y(Qp).

We train model parameters Wc and Ws by min-
imizing the following ranking loss function:

L = max{0,M − cosine(y(S), y(Qp))
+cosine(y(S), y(Q−p ))}

where M is a constant, Q−p is a negative instance.
3In this paper, m is set to 3.

4.3 Generation-based QP Prediction
The generation-based QP prediction is done based
on an sequence-to-sequence BiGRU (Bahdanau
et al., 2015) that is commonly used in the neural
machine translation field.

The encoder reads a word sequence of an input
passage S = (x1, ..., x|S|), and the decoder pre-
dicts a word sequence of an output question pat-
tern Qp = (y1, ..., y|Qt|). The probability of gen-
erating a question pattern Qp is computed as:

p(Qp) =
|Qp|∏
i=1

p(yi|y1, ..., yi−1, ci)

where each conditional probability is defined as:

p(yi|y1, ..., yi−1, ci) = g(yi−1, si, ci)
g(·) denotes a nonlinear function that outputs the
probability of generating yi. si denotes the hidden
state of time t in decoder, which is computed as:

si = (1− zi) ◦ si−1 + zi ◦ s̃i
where

s̃i = tanh(WEyi−1 + U [ri ◦ si−1] + Cci)

zi = δ(WzEyi−1 + Uzsi−1 + Czci)

ri = δ(WrEyi−1 + Ursi−1 + Crci)

δ(·) is the sigmoid function, ◦ represents element-
wise multiplication, Ew ∈ <m×1 denotes the
word embedding of a word w, W , Wz , Wr ∈
<n×m, U , Uz , Ur ∈ <n×n, and C, Cz , Cr ∈
<n×2n are weights. ci denotes the context vector,
which is computed as:

ci =
|S|∑
j=1

αijhj

where

αij =
exp(eij)∑|S|

k=1 exp(eik)

eij = vTa tanh(Wasi−1 + Uahj)

va ∈ <n
′
, Wa ∈ <n

′×n, and Ua ∈ <n
′×2n are

weights. hj denotes the jth hidden state from en-
coder, which is the concatenation of the forward
hidden state

−→
h j and the back forward state

←−
h j .

869



For training, stochastic gradient descent (SGD)
algorithm is used, and Adadelta (Zeiler, 2012) is
used to adapt the learning rate of each parameter.
Given a batch of D = {< S,Qp >}Mi=1 pairs with
size M instances, the objective function is to min-
imize the negative log-likelihood:

L = − 1
M

∑
<S,Qp>

T∑
t=1

log(p(yt|y<t,S))

For prediction, beam search is used to output
the top-N question pattern predictions.

Retrieval-based approach can only find existing
question patterns for each passage, but it makes
sure that each question pattern comes from re-
al questions and is in a good grammatical form;
Generation-based approach, on the other hand, can
generate totally new question patterns beyond ex-
isting question pattern set. We will compare both
of them in the experimental part (Section 8).

5 Question Topic Selection

Given a passage S and a predicted question pattern
Qp, question topic selection selects an n-gram (or
a phrase) Qt from S, which can be then filled into
Qp to form a complete question. Since we have
two question pattern prediction methods, we have
two ways to select the question topic Qt as well.

For Qp from retrieval-based method, two
types of prior knowledge are used to extract ques-
tion topic candidates from S, including:
• Entities as question topic candidates, which

are detected based on Freebase4 entities;

• Noun phrases as question topic candidates,
which are detected based on the Stanford
parser (Klein and Manning, 2003).

Once a question topic candidate Qt is extracted
from S , we then measure how Qt can fit Qp by:

s(Qt,Qp) = 1
N
·
∑
k

#(Qtkp ) · dist(vQt , vQtkp )

s(Qt,Qp) denotes the confidence that Qt can be
filled into Qp to generate a reasonable question.
Qtkp denotes the kth historical question topic of
Qp. #(Qtkp ) denotes the number of times that
Qtkp is extracted from different question clusters
to generate Qp. vp denotes the question topic em-
bedding of p, which is computed as the average of

4https://developers.google.com/freebase/

word embeddings in p. dist(·) denotes the cosine
distance between two question topic embeddings.
N =

∑
k #(Qtkp ) denotes the total number of his-

torical question topics of Qp. The basic principle
of the above equation is that, the historical ques-
tion topics of a given question pattern can help
on measuring how possible a question topic can-
didate can be filled into this question pattern to
form a reasonable question. For example, as most
historical question topics of “who founded # ?” are
organization names, then it is very unlikely a date
or a film name is suitable for this question pattern
to generate a reasonable question.

For Qp from generation-based method, sup-
pose the placeholder # is the ith word in Qp, then
we select the jth word wj ∈ S as the question
topic, which satisfies the following constraint:

wj = arg max
w

j
′∈S

αij′ = arg maxw
j
′∈S

exp(eij′ )∑|S|
k=1 exp(eik)

This question topic selection strategy leverages the
attention scores between S and Qp, and can be
considered as a COPY mechanism.

6 Question Ranking

Given a predicted question pattern Qp and a se-
lected question topic Qt of an input passage S, a
complete question Q can be simply generated by
replacing # in Qp with Qt. We use a set of fea-
tures to rank generated question candidates:

• question pattern prediction score, which is
the prediction score by either retrieval-based
approach or generation-based approach;

• question topic selection score, for retrieval-
based approach, this score is computed as
s(Qt,Qp), while for generation-based ap-
proach, this score is the attention score;

• QA matching score, which measures rele-
vance between generated question Q and S.
• word overlap betweenQ and S, which counts

number of words that co-occur in Q and S;
• question pattern frequency, which equals to

the extraction frequency ofQp, ifQ is gener-
ated from or matched byQp, and 0 otherwise.

All features are combined by a linear model as:

p(Q|S) =
∑

i

λi · hi(Q,S,Qp,Qt)

870



where hi(Q,S,Qp,Qt) is one of the features de-
scribed above, and λi is the corresponding weight.

7 Question Generation for QA

This section describes how question generation
can improve existing QA systems. There are sev-
eral types of QA systems, i.e. knowledge-based
QA, community-based QA, text-based QA, etc,
and in this paper, we focus on text-based QA task
(a.k.a. answer sentence selection), which aims to
select one or multiple answer sentences from a tex-
t given an input question. We select this task as it
can be considered as a dual task of QG.

A typical answer sentence selection method,
such as (Yin et al., 2016; Santos et al., 2016; Miller
et al., 2016; Tymoshenko et al., 2016), computes
the relevance score between input question Q and
each answer candidateA, and selects the one with
the highest relevance score as the final answer:

Â = arg max
A

P (A|Q)
Motivated by Dual Learning (He et al., 2016),

we integrate question generation into answer rank-
ing procedure, by changing the above formula to:

Â = arg max
A
{P (A|Q) + λ ·QQ(Q,Qgenmax)}

λ is hyper-parameter, and in order to com-
puteQQ(Q,Qgenmax), we generate top-10 questions
{Qgen1 , ...,Qgen10 } for current answer candidate A,
and then compute the question-to-generated ques-
tion matching scoreQQ(Q,Qgenmax), by computing
the similarity between input question Q and gen-
erated questions {Qgen1 , ...,Qgen10 } as:

QQ(Q,Qgenmax) = arg max
i=1,...,10

sim(Q,Qgeni ) · p(Qgeni )

sim(Q,Qgeni ) is the similarity between the input
question Q and the ith generated question Qgeni ,
and computed as the cosine distance between av-
eraged word embedding of Q and averaged word
embedding ofQgeni , p(Qgeni ) denotes the posterior
probability that is computed based on the genera-
tion score of each generated question:

p(Qgeni ) =
p(Qgeni |A)∑10

i′=1 p(Q
gen

i′ |A)
p(Qgeni |A) is output by the question generation
model described in Section 6. The underlying
motivation is that, the questions generated from
correct answers are more likely to be similar to
labeled questions than questions generated from
wrong answers.

8 Related Work

Yao et al. (2012) proposed a semantic-based ques-
tion generation approach, which first parses the in-
put sentence into its corresponding Minimal Re-
cursion Semantics (MRS) representation, and then
generates a question guided by the English Re-
source Grammar that includes a large scale hand-
crafted lexicon and grammar rules.

Labutov et al. (2015) proposed an ’ontology-
crowd-relevance’ method for question genera-
tion. First, Freebase types and Wikipedia session
names are used as semantic tags to understand
texts. Question are then generated based on ques-
tion templates that are aligned with types/session
names and labeled by crowdsourcing. All gener-
ated questions are ranked by a relevance model.

Chali and Hasan (2015) proposed a topic-to-
question method, which uses about 350 general-
purpose rules to transform the semantic-role la-
beled sentences into corresponding questions.

Serban et al. (2016) used the encoder-decoder
framework to generate 30M QA pairs, but their in-
puts are knowledge triples, instead of passages.

Song and Zhao (2016) proposed a question gen-
eration method using question template seeds and
used search engine to do question expansion.

Du et al. (2017) proposed a neural question
generation method using a vanilla sequence-to-
sequence RNN model, which is most-related to
our work. But this method is still based on labeled
dataset, and tried RNN only.

Comparing to all these related work mentioned
above, our question generation approach has two
uniqueness: (1) all question patterns, that are used
as training data for question generation, are auto-
matically extracted from a large scale CQA ques-
tion set without any crowdsourcing effort. Such
question patterns reflect the most common user in-
tentions, and therefore are useful to search, QA,
and chatbot engines; (2) it is also the first time
question generation is integrated and evaluated in
an end-to-end QA task directly, and shows signifi-
cant improvements.

9 Experiment

9.1 Dataset

As described in Section 4.1, we collect 1, 984, 401
< A,Qp,Qt > pairs from YahooAnswers, and
use them as the training set of the question pat-
tern prediction model. We re-use the dev sets and

871



test sets of SQuAD, MS MARCO, and WikiQA,
to evaluate the quality of generated questions. The
dataset statistics are in Table 2.

# of QA Pairs
training set 1,984,401

dev set (SQuAD) 26,442
test set (SQuAD) 26,604

dev set (MS MARCO) 39,510
test set (MS MARCO) 42,850

dev set (WikiQA) 2,733
test set (WikiQA) 6,165

Table 2: Dataset Statistics.

Besides, an answer sentence selection model
(Yin et al., 2016) is trained based on the 1,984,401
QA pairs from the training set as well, and used to
compute the QA matching score for question rank-
ing, as we described in Section 6. Feature weights
for question ranking are optimized on dev set.

9.2 Evaluation on Question Generation

We first perform a vanilla sequence-to-sequence
method (Du et al., 2017) using the original training
sets of these three datasets, and show QG results in
Table 3, where BLEU 4 score is used as the metric.

BLEU 4 Seq2Seq-QG
SQuAD 12.28

MS MARCO 10.46
WikiQA 2.04

Table 3: QG results using original training sets.

We then evaluate the quality of the generated
questions based on auto-extracted training set. For
each passage in the test set, we generate two top-
1 questions based on retrieval-based method and
generation-based method respectively, and then
compare them with labeled questions using BLEU
4 as the metric. Results are listed in Table 4.

BLEU 4 R-QG G-QG
SQuAD (crawled) 9.87 12.39

MS MARCO (crawled) 9.86 11.46
WikiQA (crawled) 11.38 13.57

Table 4: QG results using auto-extracted training
set, where R-QG denotes results from Retrieval-
based QG method, G-QG denotes results from
Generation-based QG method.

From Table 3 and 4 we can see two findings:
(1) Comparing to QG results based on original la-
beled training sets, G-QG achieves comparable or
better results. We think this is due to two fact-
s: first, the size of the automatically constructed
training set is much larger than the labeled train-
ing sets, and second, as the QA pairs from CQA
websites are generated by real users, the quality
is good. (2) Generation-based QG performs bet-
ter than Retrieval-based QG. By analyzing outputs
we find that, for question pattern prediction, both
retrieval-based and generation-based methods per-
form similarly. However, Generation-based QG
performs better than Retrieval-based QG on ques-
tion topic selection. This could be caused by the
fact that, in Generation-based QG, question top-
ic selection is based on the attention mechanis-
m, which is optimized together with question pat-
tern prediction in an end-to-end way; while in
Retrieval-based QG, question topic selection is a
separate task, and based on the similarity between
each question topic candidate and historical ques-
tion topics of a given question pattern. The embed-
ding of each question topic is pre-trained, which
is not directly related to the question generation
task. So such method cannot handle unseen ques-
tion topics very well. Another disadvantage of
Retrieval-based QG is that, each time, we have to
compute the similarity between the input passage
and each question pattern. When question pattern
size is large, the computation is very expensive.

In order to better understand the question gener-
ation quality, we manually check a set of sampled
outputs, and list the main errors in Figure 2:

• Multi-Fact Error (40%). Most input pas-
sages include more than one fact. For such a
question, it is reasonable to generate different
questions from different aspects, all of which
can be answered by the input passage. For
each passage in QAGen, we only label one
question as ground truth. In the future, we
will extend QAGen to be a more comprehen-
sive dataset, by labeling multiple questions to
each passage for more reasonable evaluation;

• Paraphrase Error (30%). The same question
can be expressed by different ways. Labeling
more paraphrased questions for a passage can
alleviate this issue as well;

• Question Topic Selection Error (15%). This
error is caused by selecting either a total-

872



Figure 2: List of error analysis examples. [P] denotes a passage, [Ref] denotes the labeled question of
the passage, and [Gen] denotes the generated question of the passage.

ly wrong question topic, or a partially right
question topic. In the future, we plan to de-
velop an independent question topic selection
model for the question generation task.

9.3 Evaluation on QA
As described in Section 7, we combine question
generation into QA system for answer sentence s-
election task, and do evaluation on SQuAD, M-
S MARCO, and WikiQA. Evaluation results are
shown in Table 5, 6, and 7, where QA denotes
the result of our in-house implementation of a
retrieval-based answer selection approach (Doc-
Chat) proposed by (Yan et al., 2016), QA+QG de-
notes result by combining question-to-generated
question matching score with DocChat score.

SQuAD MAP MRR ACC@1
QA 0.8843 0.8915 0.8160

QA+QG 0.8887 0.8963 0.8232

Table 5: Impact of QG on SQuAD.

MS MARCO MAP MRR ACC@1
QA 0.5131 0.5195 0.3029

QA+QG 0.5230 0.5291 0.3153

Table 6: Impact of QG on MS MARCO.

The improvement on MS MARCO dataset is
most significant. We think it due to the fact that,
the questions from MS MARCO dataset are from
Bing search log, which are generated naturally by
real users. This is similar to the questions coming

WikiQA MAP MRR ACC@1
QA 0.7703 0.7851 0.6540

QA+QG 0.7742 0.7893 0.6624

Table 7: Impact of QG on WikiQA.

for CQA websites; while questions from the other
datasets are labeled by crowd-sourcing.

In order to explain these improvements, two
datasets, WikiQG+ and WikiQG-, are built from
WikiQA test set: given each document and its la-
beled question, we pair the question with its COR-
RECT answer sentence as a QA pair and add it to
WikiQG+; we also pair the same question with a
randomly selected WRONG answer sentence as a
QA pair and add it to WikiQG-. Then, we generate
questions for passages in WikiQG+ and WikiQG-
respectively, and compare them with labeled ques-
tions. The BLEU 4 score is 0.2031 on Wik-
iQG+, and 0.1301 on WikiQG-, which indicates
that the questions generated from correct answers
are more likely to be similar to labeled questions
than questions generated from wrong answers.

10 Conslusion

This paper presents a neural question generation
method that is based on training data collected
from CQA questions. We integrate the QA pair
generation task into an end-to-end QA task, and
show significant improvements, which indicates
that, QA task and QG are dual tasks that can boost
each other. In the future, we will explore more
ways to leverage QG for QA task.

873



References
Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Ben-

gio. 2015. Neural machine translation by jointly
learning to align and translate. In ICLR.

Jonathan Berant, Andrew Chou, Roy Frostig, and Per-
cy Liang. 2013. Semantic parsing on freebase from
question-answer pairs. In EMNLP.

Jonathan Berant and Percy Liang. 2014. Semantic
parsing via paraphrasing. In ACL.

Antoine Bordes, Nicolas Usunier, Sumit Chopra, and
Jason Weston. 2015. Large-scale simple question
answering with memory networks. In arXiv.

Yllias Chali and Sadid Hasan. 2015. Towards topic-to-
question generation. In CL.

Xinya Du, Junru Shao, and Claire Cardie. 2017. Learn-
ing to ask: Neural question generation for reading
comprehension. In Association for Computational
Linguistics (ACL).

Di He, Yingce Xia, Tao Qin, Liwei Wang, Nenghai Yu,
Tie-Yan Liu, and Wei-Ying Ma. 2016. Dual learning
for machine translation. In NIPS.

Dan Klein and Christopher D. Manning. 2003. Accu-
rate unlexicalized parsing. In ACL.

Igor Labutov, Sumit Basu, and Lucy Vanderwende.
2015. Deep questions without deep understanding.
In ACL.

Alexander Miller, Adam Fisch, Jesse Dodge, Amir-
Hossein Karimi, Antoine Bordes, and Jason West-
on. 2016. Key-value memory networks for directly
reading documents. In EMNLP.

Tri Nguyen, Mir Rosenberg, Xia Song, Jiangfeng Gao,
Saurabh Tiwary, Rangan Majumder, and Li Deng.
2016. Ms marco: A human generated machine read-
ing comprehension dataset. In NIPS.

Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a method for automatic eval-
uation of machine translation. In ACL.

Xipeng Qiu and Xuanjing Huang. 2015. Convolution-
al neural tensor network architecture for community
based question answering. In IJCAI.

Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and
Percy Liang. 2016. Squad: 100,000+ questions for
machine comprehension of text. In EMNLP.

Cicero dos Santos, Ming Tan, Bing Xiang, and Bowen
Zhou. 2016. Attentive pooling networks. In arXiv.

Iulian Vlad Serban, Alberto Garca-Durn, Caglar
Gulcehre, Sungjin Ahn, Sarath Chandar, Aaron
Courville, and Yoshua Bengio. 2016. Generat-
ing factoid questionswith recurrent neural networks:
The 30m factoid question-answer corpus. In ACL.

Linfeng Song and Lin Zhao. 2016. Domain-specific
question generation from a knowledge base. In arX-
iv.

Kateryna Tymoshenko, Daniele Bonadiman, and A-
lessandro Moschitt. 2016. Convolutional neural net-
works vs. convolution kernels: Feature engineering
for answer sentence reranking. In NAACL.

Caiming Xiong, Victor Zhong, and Richard Socher.
2017. Dynamic coattention networks for question
answering. In ICLR.

Zhao Yan, Nan Duan, Junwei Bao, Peng Chen, Ming
Zhou, Zhoujun Li, and Jianshe Zhou. 2016. Doc-
chat: An information retrieval approach for chatbot
engines using unstructured documents. In ACL.

Yi Yang, Wen-tau Yih, and Christopher Meek. 2015.
Wikiqa: A challenge dataset for open-domain ques-
tion answering. In EMNLP.

Xuchen Yao, Gosse Bouma, and Yi Zhang. 2012.
Semantics-based question generation and imple-
mentation. In Dialogue and Discourse.

Wenpeng Yin and Hinrich Schtze. 2017. Task-specific
attentive pooling of phrase alignments contributes to
sentence matching. In EACL.

Wenpeng Yin, Hinrich Schtze, Bing Xiang, and Bowen
Zhou. 2016. Abcnn: Attention-based convolution-
al neural network for modeling sentence pairs. In
TACL.

Lei Yu, Karl Morizt Hermann, Phil Blunsom, and
Stephen Pulman. 2014. Deep learning for answer
sentence selection. In NIPS Workshop.

Matthew Zeiler. 2012. Adadelta: an adaptive learning
rate method. In CoRR abs.

874


