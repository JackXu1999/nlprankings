



















































Using mention accessibility to improve coreference resolution


Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, pages 432–437,
Berlin, Germany, August 7-12, 2016. c©2016 Association for Computational Linguistics

Using Mention Accessibility to Improve Coreference Resolution

Kellie Webster and Joel Nothman
School of Information Technologies

University of Sydney
NSW 2006, Australia

{kellie.webster, jnothman}@sydney.edu.au

Abstract

Modern coreference resolution systems re-
quire linguistic and general knowledge
typically sourced from costly, manually
curated resources. Despite their intuitive
appeal, results have been mixed. In this
work, we instead implement fine-grained
surface-level features motivated by cogni-
tive theory. Our novel fine-grained feature
specialisation approach significantly im-
proves the performance of a strong base-
line, achieving state-of-the-art results of
65.29 and 61.13% on CoNLL-2012 using
gold and automatic preprocessing, with
system extracted mentions.

1 Introduction

Coreference resolution (Pradhan et al., 2011,
2012) is the task of clustering mentions in a docu-
ment according to their referent. For instance, we
need to resolve Ehud Barak, his, and he as corefer-
ential to understand the meaning of the excerpt:

Israeli Prime Minister Ehud Barak called his cabinet into
special session late Wednesday , to discuss what he called a
grave escalation of the level of violence ...

While knowledge-poor approaches establish a
reasonable baseline, they perform poorly when po-
sitional and surface form heuristics break down.
To address this, research has extracted world
knowledge from manually curated resources in-
cluding Wikipedia, Yago, Freebase, and FrameNet
(e.g. Uryupina et al., 2011; Rahman and Ng, 2011;
Ratinov and Roth, 2012; Hajishirzi et al., 2013;
Durrett and Klein, 2014). Despite their intuitive
appeal, results have been mixed. We instead focus
on linguistic knowledge which can be extracted
completely automatically, guided by insights from

Accessibility theory (Ariel, 2001). This result is
consistent with Wiseman et al. (2015) which sim-
ilarly finds performance gains above state-of-the-
art from extending simple, surface-level features.

We implement a mention classification scheme
based on the Accessibility hierarchy and use this
for feature specialisation, yielding state-of-the-art
results of 65.29 and 61.13% on CoNLL-2012 on
gold and automatic preprocessing, with system ex-
tracted mentions. Our approach is simple and ef-
fective, contributing to arguments for incorporat-
ing cognitive insights in computational modelling.

2 Accessibility Hierarchy

Accessibility theory (Ariel, 2001) builds on a body
of cognitively motivated theories of discourse pro-
cessing, notably Centering Theory (Grosz et al.,
1995). Where Centering describes pronoun in-
terpretation in terms of relative discourse en-
tity salience, Accessibility theory expands from
this, describing discourse entities as having corre-
sponding human memory nodes which fluctuate in
their degree of activation as the entity features in
a discourse. The surface form of a reference indi-
cates to the hearer how activated its corresponding
node is expected to be. That is, surface form is an
instruction for how to retrieve suitable referents,
guiding the resolution of coreference. Relative de-
gree of activation is captured in the theory’s hier-
archy of reference expression types, reproduced in
Figure 1. Section 4 proposes a mapping of this
hierarchy (derived for spoken Hebrew) to written
English.

The hierarchy encodes and expands the widely-
used rule of thumb that full names introduce an en-
tity (their referent has low accessibility; it has not
yet been discussed) and pronouns are anaphoric
(their referent is a highly accessible, active dis-

432



Full name + modifier < Full name < Long definite description < Short definite description < Last name < First name <

Distal demonstrative + modifier < Proximate demonstrative + modifier < Distal demonstrative + NP < Proximate

demonstrative + NP < Distal demonstrative < Proximate demonstrative < Stressed pronoun + gesture < Stressed pronoun <

Unstressed pronoun < Cliticised pronoun < Verbal inflections < Zero

Figure 1: Accessibility hierarchy from Ariel (2001)

course entity); the accessibility of definite descrip-
tions is intermediate. In this work, we show that
the fine-grained categorisation in the Accessibil-
ity hierarchy can be leveraged to improve the dis-
criminative power of a strong system, compared
to coarser-grained typologies from previous work.
That is, this work contributes valuable empirical
support for the psycholinguistic theory.

3 Related Work

A particularly successful way to leverage men-
tion classification has been to specialise mod-
elling by mention type. Denis and Baldridge
(2008) learn five different models, one each for
proper name, definite nominal, indefinite nomi-
nal, third person pronoun, and non-third person
pronoun. Bengtson and Roth (2008) and Dur-
rett and Klein (2013) implement specialisation
at the level of features within a model, rather
than explicitly learning separate models. Bengt-
son and Roth (2008) prefix each base feature
generated with the type of the current mention,
one of proper name, nominal, or pronoun, for
instance nominal-head match:true. Dur-
rett and Klein (2013) extend from this by
learning a model over three versions of each
base feature: unprefixed, conjoined with the
type of the current mention, and conjoined
with concatenation of the types of the cur-
rent mention and candidate antecedent mention:
nominal+nominal-head match=true.

The success of Durrett and Klein is possi-
ble due to the large training dataset provided by
OntoNotes (Pradhan et al., 2007). In this work,
we successfully extend data-driven specialisation
still further: Section 4 shows how we can dis-
cover fine-grained patterns in reference expression
usage, and Section 5 how these patterns can be
used to significantly improve the performance of
a strong coreference system.

4 Accessibility Transitions in OntoNotes

In this section, we propose an implementation
of the Accessibility hierarchy for written En-

AR Description %
1 Multi-word name + modifier 7.7
2 Multi-word name 8.7
3 Long indefinite description 18.9
4 Short indefinite description 16.3
5 Long definite description 10.2
6 Short definite description 5.0
7 Single-word name 8.8
8 Distal demonstrative + modifier 0.2
9 Proximate demonstrative + modifier 0.0

10 Distal demonstrative + NP 0.7
11 Proximate demonstrative + NP 1.2
12 Distal demonstrative 0.8
13 Proximate demonstrative 0.5
14 Pronoun 21.0

- Zero -

Table 1: Accessibility rank values used in our ex-
periments, with their base distribution over ex-
tracted NPs.

glish and how this can be used to encode fine-
grained discourse transitions. We discover trends
in OntoNotes, over mentions automatically ex-
tracted from the DEV portion of English CoNLL-
2012 (Pradhan et al., 2011).

4.1 Mention classification

Our experiments start by classifying a mention’s
Accessibility rank value, AR. Table 1 gives the
schema we propose for written English, along with
the base distribution over extracted mentions. This
mapping is a simple ordinal numbering of Figure 1
with the following refinements.

We have generalised last name and first name
to single-word name (AR = 7) and full name to
multi-word name (AR = 2) to handle non-person
entities. Name modifiers are tokens without the
head NER label, excluding determiners, possessive
markers, and punctuation. We have introduced
indefinite descriptions above definite descriptions
since they are more likely to introduce discourse
entities than definite descriptions are. We label
any nominal started by the or a possessive pro-
noun as a definite; otherwise it is indefinite. Long
descriptions comprise more than one token when
possessive markers, punctuation, and articles are
excluded. Distals start with those or that while

433



Table 2: Accessibility transitions (>0.05) CoNLL-2012 DEV.

proximates start with these or this.

4.2 Discourse Transitions

Discourse transitions are then AR tuples whose
values come from mentions aligned to the same
gold cluster. We chose 2-tuples, whose val-
ues come from mention-antecedent pairs, since
mention-pair models have dominated the research
space. However, we generate up to three pairs per
mention since antecedents are latent at the entity
level. That is, for he in the following, we generate
pairs (1, 14) and (14, 14).

Israeli Prime Minister Ehud BarakAR=1 called hisAR=14
cabinet into special session late Wednesday , to discuss what

heAR=14 called a grave escalation of the level of violence ...

The aggregated counts for each pair type are
represented in Table 2, with AR(antecedent) on
the vertical and AR(anaphor) on the horizontal.
The first column gives the proportion of cluster-
initial mentions of each AR type (e.g. 21% of gold
clusters have a long indefinite description as their
first mention). Each row is normalised to sum to
1 so each row indicates the probability distribu-
tion for the expected next mention of a cluster. For
clarity, only values 0.05 and higher are shown.

We can see that commonly used rules of thumb
are borne out in this data, though with some ex-
tra granularity. Modified and multi-word names
reduce to single-word names, and both reduce to
pronouns. Single word names retain their men-
tion form and reduce to pronouns with roughly
equal probability. All mention types reduce to be
pronouns and, once reference has reduced to be
pronominal, there is a high likelihood (82%) that
this form will be retained.

Encouragingly, we can also see transitions in

Table 3: Proportion of singletons by AR.

Table 2 can not be expressed with the coarser-
grained typologies of prior work. Firstly, men-
tion article is important. Long indefinite descrip-
tions are more likely to start coreference clusters
than long definite descriptions (21% vs. 14%),
which are in turn much more likely to start clus-
ters than demonstratives. Mention length is also
important: short indefinite descriptions are more
likely to reduce to pronouns than long definite
descriptions and short definite descriptions have
a higher chance of being retained throughout the
discourse than long definite descriptions. Explor-
ing further, of coreferential pairs where both men-
tions are short definite descriptions, 86% are head
matched, compared to 60% of long definite de-
scriptions; 60% of short definite descriptions are
string matched, compared to 27% of long.

4.3 Anaphoricity

Table 3 gives the proportion of extracted mentions
which can not be aligned to gold mentions, by
AR value. Modelling these discourse singletons is
important for models jointly learning coreference
and anaphoricity (Webster and Curran, 2014).

434



Gold Auto
MUC B3 CEAFE CoNLL MUC B3 CEAFE CoNLL

Fernandes et al. (2012) 72.18 59.17 55.72 62.36 70.51 57.58 53.86 60.65
Björkelund and Kuhn (2014) 73.80 62.00 59.06 64.95 70.72 58.58 55.61 61.63
LIMERIC Baseline 74.07 60.91 58.57 64.52 70.36 56.60 54.42 60.46
+ Fine-Grained Specialisation 74.73 61.72 59.43 65.29 70.72 57.40 55.26 61.13

Table 4: Performance on CoNLL-2012 TEST evaluated with gold and automatic annotations and system
extracted mentions.

After pronouns, demonstratives and proper
names have low proportion of singletons. Sin-
gle word names are less likely to be singletons
than modified and multi-word names. We high-
light two contributing factors. The first is that cer-
tain names, particularly the children of an apposi-
tion, are not markable in OntoNotes. The second
is that the burden of supplying disambiguation will
be more worthwhile for important entities.

Consistent with Recasens et al. (2013), indef-
inites are more likely to be singletons than defi-
nites, and long definites are more likely than short
definites. Since length and article are the key fac-
tors for AR typing, this is good evidence in favour
of using the hierarchy’s fine-grained classification.

5 Experiments

In this section, we show how fine-grained feature
specialisation can significantly improve the per-
formance of LIMERIC, a competitive coreference
resolution system. This strength demonstrates that
simple surface-form features have yet to be fully
utilised in current modelling, and that cognitive
theory can guide their development.

5.1 LIMERIC

The system we base our work on is LIMERIC
(Webster and Curran, 2014). We choose this sys-
tem due to its cognitive motivation and strong
performance. Importantly, this system already
uses the coarse-grained featurisation of Durrett
and Klein (2013), allowing us to directly measure
the impact of our proposed fine-grained featurisa-
tion.

We, however, improve it in a number of ways.
The biggest performance boosts came from using
MIRA (Margin Infused Relaxation Algorithm) up-
dates in place of standard perceptron updates and
implementing the full range of common features
from the literature. We also fix a number of bug
fixes and improve mention extraction. This im-

proved system forms our LIMERIC baseline in Ta-
ble 4.

5.2 Fine-Grained Feature Specialisation

We build on work in discourse transition prefix-
ing (particularly Durrett and Klein, 2013), which
expands the feature space of a learner by includ-
ing multiple versions of each generated feature.
LIMERIC previously used three versions of each
feature: one unprefixed, one prefixed with the cur-
rent mention’s type (one of name, nominal, or pro-
noun), and one prefixed with the concatenation of
the types of the current and candidate antecedents.
In this work, we introduce a fourth prefix, formed
by concatenating the AR of the current mention
with that of the closest mention in the candidate
antecedent cluster.

The power of such transition features is that
they allow us to learn, for instance, that pronoun to
name transitions are preferred when the anaphor is
distant from its antecedent and the name mention
is one token, or that head match is a particularly
strong indicator of coreferentiality between short
definite nominals: 6+6-head match=true.

5.3 Results

Table 4 tabulates system performance on CoNLL-
2012 TEST using system extracted mentions and
v8.01 of the official scorer (Pradhan et al., 2014).

Comparing feature specialisation against the
LIMERIC baseline, we can see that it yields sub-
stantial performance gains on all metrics and both
evaluation settings. Performance gains indicated
in bold are statistically significant for the conser-
vative p = 0.01 using bootstrap re-sampling1. Per-
formance gains indicated in italics are significant
at the standard threshold of p = 0.05.

We benchmark against the state-of-the-art by

1Since Specialisation is a development of LIMERIC, the
two models are not independent which means we would ex-
pect to see relatively high confidence values for relatively
small gains in score (see Berg-Kirkpatrick et al., 2012).

435



comparing performance to the winner of the
shared task (Fernandes et al., 2012), as well as
the best documented system at the time of this
work (Björkelund and Kuhn, 2014). Fine-grained
feature specialisation improves LIMERIC’s perfor-
mance to push past that of Björkelund and Kuhn
(2014) when using gold preprocessing. Further-
more, on the difficult automatic setting, we out-
perform Fernandes et al. (2012) and are not signif-
icantly worse than Björkelund and Kuhn (2014).

On the link-based MUC and B3 metrics, our
recall gains are larger than our precision gains.
That is, specialisation enables coreference indica-
tors to accrue sufficient weight so as to promote
new coreference links, a known problem case for
modern systems. We found particularly enhanced
weight on features for relaxed string matching.

6 Conclusion

In this paper, we have found fine-grained patterns
in reference expression usage based on the Ac-
cessibility hierarchy and shown how these can be
used to significantly improve the performance of
a strong system, LIMERIC. Despite being simple
to implement, we achieve comparable or improved
performance than the best reported results, further-
ing arguments for incorporating cognitive insights
in computational modelling.

7 Acknowledgements

The authors thank their anonymous reviewers and
members of the Schwa Lab at the University of
Sydney for their insightful and helpful feedback.
The first author was supported by an Australian
Postgraduate Award scholarship.

References
Mira Ariel. 2001. Accessibility theory: An overview. Text

representation: Linguistic and psycholinguistic aspects,
pages 29–87.

Eric Bengtson and Dan Roth. 2008. Understanding the value
of features for coreference resolution. In Proceedings of
the Conference on Empirical Methods in Natural Lan-
guage Processing, pages 294–303. Association for Com-
putational Linguistics.

Taylor Berg-Kirkpatrick, David Burkett, and Dan Klein.
2012. An empirical investigation of statistical significance
in nlp. In Proceedings of the 2012 Joint Conference on
Empirical Methods in Natural Language Processing and
Computational Natural Language Learning, pages 995–
1005. Association for Computational Linguistics, Jeju Is-
land, Korea.

Anders Björkelund and Jonas Kuhn. 2014. Learning struc-
tured perceptrons for coreference resolution with latent

antecedents and non-local features. ACL, Baltimore, MD,
USA, June.

Pascal Denis and Jason Baldridge. 2008. Specialized mod-
els and ranking for coreference resolution. In Proceedings
of the Conference on Empirical Methods in Natural Lan-
guage Processing, pages 660–669. Association for Com-
putational Linguistics.

Greg Durrett and Dan Klein. 2013. Easy victories and uphill
battles in coreference resolution. In Proceedings of the
Conference on Empirical Methods in Natural Language
Processing.

Greg Durrett and Dan Klein. 2014. A joint model for entity
analysis: Coreference, typing, and linking. In Proceedings
of the Transactions of the Association for Computational
Linguistics.

Eraldo Fernandes, Cı́cero dos Santos, and Ruy Milidiú. 2012.
Latent structure perceptron with feature induction for un-
restricted coreference resolution. In Joint Conference on
EMNLP and CoNLL - Shared Task, pages 41–48. Associ-
ation for Computational Linguistics, Jeju Island, Korea.

Barbara J Grosz, Scott Weinstein, and Aravind K Joshi. 1995.
Centering: A framework for modeling the local coherence
of discourse. Computational Linguistics, 21(2):203–225.

Hannaneh Hajishirzi, Leila Zilles, Daniel S Weld, and Luke
Zettlemoyer. 2013. Joint coreference resolution and
named-entity linking with multi-pass sieves. pages 289–
299.

Sameer Pradhan, Xiaoqiang Luo, Marta Recasens, Eduard
Hovy, Vincent Ng, and Michael Strube. 2014. Scoring
coreference partitions of predicted mentions: A reference
implementation.

Sameer Pradhan, Alessandro Moschitti, Nianwen Xue, Olga
Uryupina, and Yuchen Zhang. 2012. Conll-2012 shared
task: Modeling multilingual unrestricted coreference in
ontonotes. In Joint Conference on EMNLP and CoNLL
- Shared Task, pages 1–40. Association for Computational
Linguistics, Jeju Island, Korea.

Sameer Pradhan, Lance Ramshaw, Mitchell Marcus, Martha
Palmer, Ralph Weischedel, and Nianwen Xue. 2011.
Conll-2011 shared task: Modeling unrestricted corefer-
ence in ontonotes. In Proceedings of the Fifteenth Con-
ference on Computational Natural Language Learning:
Shared Task, pages 1–27. Association for Computational
Linguistics, Portland, Oregon, USA.

Sameer S. Pradhan, Eduard H. Hovy, Mitchell P. Mar-
cus, Martha Palmer, Lance A. Ramshaw, and Ralph M.
Weischedel. 2007. Ontonotes: a unified relational seman-
tic representation. Int. J. Semantic Computing, 1(4):405–
419.

Altaf Rahman and Vincent Ng. 2011. Coreference resolu-
tion with world knowledge. In Proceedings of the 49th
Annual Meeting of the Association for Computational Lin-
guistics: Human Language Technologies, volume 1, pages
814–824.

Lev Ratinov and Dan Roth. 2012. Learning-based multi-sieve
co-reference resolution with knowledge. In EMNLP.

Marta Recasens, Marie-Catherine de Marneffe, and Christo-
pher Potts. 2013. The life and death of discourse enti-
ties: Identifying singleton mentions. In Proceedings of
the 2013 Conference of the North American Chapter of
the Association for Computational Linguistics: Human
Language Technologies, pages 627–633. Association for
Computational Linguistics, Atlanta, Georgia.

436



Olga Uryupina, Massimo Poesio, Claudio Giuliano, and
Kateryna Tymoshenko. 2011. Disambiguation and filter-
ing methods in using web knowledge for coreference res-
olution. In Proceedings of the 24th International Florida
Artificial Intelligence Research Society Conference, pages
317–322.

Kellie Webster and James R Curran. 2014. Limited mem-
ory incremental coreference resolution. In Proceed-
ings of COLING 2014, the 25th International Conference
on Computational Linguistics: Technical Papers, pages
2129–2139. Dublin,Ireland.

Sam Wiseman, Alexander M Rush, Stuart M Shieber, Jason
Weston, Heather Pon-Barry, Stuart M Shieber, Nicholas
Longenbaugh, Sam Wiseman, Stuart M Shieber, Elif Ya-
mangil, et al. 2015. Learning anaphoricity and antecedent
ranking features for coreference resolution. In Proceed-
ings of the 53rd Annual Meeting of the Association for
Computational Linguistics, volume 1, pages 92–100. As-
sociation for Computational Linguistics.

437


