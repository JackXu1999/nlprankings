








































Exploiting Social Network Structure for Person-to-Person
Sentiment Analysis

Robert West
Stanford University

west@cs.stanford.edu

Hristo S. Paskov
Stanford University

hpaskov@stanford.edu

Jure Leskovec
Stanford University

jure@cs.stanford.edu

Christopher Potts
Stanford University

cgpotts@stanford.edu

Abstract
Person-to-person evaluations are prevalent in
all kinds of discourse and important for es-
tablishing reputations, building social bonds,
and shaping public opinion. Such evaluations
can be analyzed separately using signed so-
cial networks and textual sentiment analysis,
but this misses the rich interactions between
language and social context. To capture such
interactions, we develop a model that pre-
dicts individual A’s opinion of individual B
by synthesizing information from the signed
social network in which A and B are embed-
ded with sentiment analysis of the evaluative
texts relating A to B. We prove that this prob-
lem is NP-hard but can be relaxed to an ef-
ficiently solvable hinge-loss Markov random
field, and we show that this implementation
outperforms text-only and network-only ver-
sions in two very different datasets involving
community-level decision-making: the Wiki-
pedia Requests for Adminship corpus and the
Convote U.S. Congressional speech corpus.

1 Introduction

People’s evaluations of one another are prevalent
in all kinds of discourse, public and private, across
ages, genders, cultures, and social classes (Dunbar,
2004). Such opinions matter for establishing rep-
utations and reinforcing social bonds, and they are
especially consequential in political contexts, where
they take the form of endorsements, accusations, and
assessments intended to sway public opinion.

The significance of such person-to-person evalu-
ations means that there is a pressing need for com-
putational models and technologies that can analyze

them. Research on signed social networks suggests
one path forward: how one person will evaluate an-
other can often be predicted from the network they
are embedded in. Linguistic sentiment analysis sug-
gests another path forward: one could leverage tex-
tual features to predict the valence of evaluative texts
describing people. Such independent efforts have
been successful, but they generally neglect the ways
in which social and linguistic features complement
each other. In some settings, textual data is sparse
but the network structure is largely observed; in oth-
ers, text is abundant but the network is partly or un-
reliably recorded. In addition, we often see rich in-
teractions between the two kinds of information—
political allies might tease each other with negative
language to enhance social bonds, and opponents of-
ten use sarcastically positive language in their criti-
cisms. Separate sentiment or signed-network mod-
els will miss or misread these signals.

We develop (Sec. 3) a graphical model that syn-
thesizes network and linguistic information to make
more and better predictions about both. The objec-
tive of the model is to predict A’s opinion of B using
a synthesis of the structural context around A and B
inside the social network and sentiment analysis of
the evaluative texts relating A to B. We prove that
the problem is NP-hard but that it can be relaxed
to an efficiently solvable hinge-loss Markov random
field (Broecheler et al., 2010), and we show that this
implementation outperforms text-only and network-
only versions in two very different datasets involv-
ing community-level decision-making: the Wikipe-
dia Requests for Adminship corpus, in which Wi-
kipedia editors discuss and vote on who should be

297

Transactions of the Association for Computational Linguistics, 2 (2014) 297–310. Action Editor: Hal Daume III.
Submitted 5/2014; Revised 6/2014; Published 10/2014. c©2014 Association for Computational Linguistics.



promoted within the Wikipedia hierarchy (Sec. 4),
and the Convote U.S. Congressional speech corpus
(Thomas et al., 2006), in which elected officials dis-
cuss political topics (Sec. 5). These corpora differ
dramatically in size, in the style and quality of their
textual data, and in the structure and observability of
their networks. Together, they provide a clear pic-
ture of how joint models of text and network struc-
ture can excel where their component parts cannot.

2 Background and related work

2.1 Sentiment analysis

In NLP, the label sentiment analysis covers diverse
phenomena concerning how information about emo-
tions, attitudes, perspectives, and social identities is
conveyed in language (Pang and Lee, 2008). Most
work assumes a dimensional model in which emo-
tions are defined primarily by valence/polarity and
arousal/intensity (Russell, 1980; Feldman Barrett
and Russell, 1998; Rubin and Talerico, 2009), and
the dominant application is predicting the valence of
product, company, and service reviews.

We adopt the conceptual assumptions of this work
for our basic sentiment model, but our focus is on
person-to-person evaluations and their social conse-
quences. This involves elements of work on mod-
eling political affiliation (Agrawal et al., 2003; Mal-
ouf and Mullen, 2008; Yu et al., 2008), bias (Yano et
al., 2010; Recasens et al., 2013), and stance on de-
bate topics (Thomas et al., 2006; Somasundaran and
Wiebe, 2010; Lin et al., 2006; Anand et al., 2011),
but these aspects of belief and social identity are not
our primary concern. Rather, we expect them to be
predictive of the sentiment classifications we aim to
make—e.g., if two people share political views, they
will tend to evaluate each other positively.

Recent work in sentiment analysis has brought in
topical, contextual, and social information to make
more nuanced predictions about language (Jurafsky
et al., 2014; Wilson et al., 2005; Blitzer et al., 2007).
We build on these insights with our model, which
seeks to modulate sentiment predictions based on
network information (and vice versa).

2.2 Signed-network analysis

Many social networks encode person-to-person sen-
timent information via signed edges between users

summarizing their opinions of each other. In this set-
ting, one can leverage sociological theories of pair-
wise relationships and group-level organization to
identify and understand patterns in these relation-
ships (Heider, 1946; Cartwright and Harary, 1956).

Balance theory is based on simple intuitions like
‘a friend of my friend is my friend’, ‘an enemy of
my enemy is my friend’, and ‘an enemy of my friend
is my enemy’. In graph theory, these are statements
about the edge signs of triangles of connected nodes:
given the signs of two edges, balance theory predicts
the third, as summarized in Fig. 1(a), where the two
given edges (gray) determine the third (black).

For directed relationships, Leskovec et al. (2010b)
formulate an alternative called status theory, which
posits that networks organize according to social sta-
tus: a node has positive edges to others with higher
status and negative edges to those with lower sta-
tus. Fig. 1(a) illustrates the structure of various di-
rected signed triangles, where the sign of the third
edge (black) can be inferred based on the signs and
directions of the other two (gray).

Leskovec et al. (2010b) show that signed edges in
networks emerge in a manner that is broadly con-
sistent with both of these theories and that social-
network structure alone can support accurate edge-
sign predictions (Leskovec et al., 2010a). Kunegis et
al. (2013) predict hidden positive and negative edges
in a scenario where all observed edges are positive.
Bach et al. (2013) and Huang et al. (2013) frame sign
prediction as a hinge-loss Markov random field, a
type of probabilistic graphical model introduced by
Broecheler et al. (2010). Our model combines these
ideas with a sentiment model to achieve even more
robust predictions.

2.3 Synthesis of sentiment & network analysis
Models of sentiment and signed networks have been
successful at a variety of tasks. However, missing
from the current scientific picture is a deep under-
standing of the ways in which sentiment expres-
sion and social networks interact. To some extent,
these interactions are captured by adding contextual
and demographic features to a text-based sentiment
model, but those features only approximate the rich
relational structure encoded in a signed network.

Thomas et al. (2006) and Tan et al. (2011) cap-
italize on this insight using an elaboration of the

298



+

+ +– +– –

–+

– –

–

+ +

+

SOCIAL BALANCE THEORY SOCIAL STATUS THEORY

+ –

?
(a) Theories of social balance and status

+

+

–

–
?

?
+ ?

“You’re one
crazy mofo!” “Love u! :)”

“To whom it
may concern”

(b) Desiderata

Figure 1: (a) Predictions of social balance and status theories for the bold black edge, given the thin gray edges.
Balance theory reasons about undirected, status theory about directed, triangles. In the status diagrams, node size
signifies social status. A positive edge may be replaced by a negative edge in the opposite direction, and vice versa,
without changing the prediction. Status theory makes no prediction in the rightmost case. (b) Situations of the sort we
aim to capture. At left, the network resolves textual ambiguity. At right, the text compensates for edge-label sparsity.

graph-cuts approach of Pang and Lee (2004). They
are guided by an assumption of homophily, i.e., that
certain social relationships correlate with agreement
on certain topics: Thomas et al. (2006) use party af-
filiation and mentions in speeches to predict voting
patterns, and Tan et al. (2011) use Twitter follows
and mentions to predict attitudes about political and
social events. Related ideas are pursued by Ma et al.
(2011) and Hu et al. (2013), who add terms to their
models enforcing homophily between friends with
regard to their preferences.

We adopt some of the assumptions of the above
authors, but our task is fundamentally different in
two respects. First, whereas they model person-to-
item evaluations, we model person-to-person evalu-
ations; these are also the focus of Tang et al. (2013),
who, though, use an unsigned network, whereas our
work is geared toward distinguishing positive and
negative edge labels. Second, the above models
make overarching homophily assumptions, whereas
we allow our model to explore the full set of triangle
configurations suggested by Fig. 1(a).

3 Model

Here, we argue that combining textual and structural
features can help predict edge signs. We formu-
late a model, show that it is computationally hard,
and provide a relaxed version that is computationally
tractable, building on work by Bach et al. (2013).

3.1 Desiderata

In many real-world scenarios, rich features are asso-
ciated with edges between two people, such as com-
ments they made about each other, messages they

exchanged, or other behavioral features. Such fea-
tures may contain a strong sentiment signal useful
for predicting edge signs and may be used to fit a
conventional sentiment model (Sec. 2.1).

However, the sign of an edge also depends on the
signs of surrounding edges in the network (Sec. 2.2).
A purely edge-feature–based sentiment model can-
not account for the network structure, since it rea-
sons about edges as independent of each other.

We argue that considering sentiment and network
structure jointly can result in better predictions than
either one on its own. Fig. 1(b) provides two illus-
trative examples. Here, the gray edge signs are ob-
served, while the polarities of the black edges are
to be predicted. In the left network, the text of the
black edge (‘You’re one crazy mofo!’) might sug-
gest a negative polarity. However, a negative black
edge would make both triangles inconsistent with
balance theory (Fig. 1(a)), whereas a positive black
edge makes them consistent with the theory. So, in
this case, the network context effectively helps de-
tect the teasing, non-literal tone of the statement.

In the right network of Fig. 1(b), only one of
three edge signs is observed. Predicting two pos-
itive edges would be consistent with balance the-
ory, but the same would be true for predicting two
negative edges. The text on the lower black edge
(‘To whom it may concern’) does not carry any clear
sentiment signal, but the ‘Love u! :)’ on the other
edge strongly suggests a positive polarity. This lets
us conclude that the bottom edge should probably
be positive, too, since otherwise the triangle would
contradict balance theory. This shows that combin-
ing sentiment and network features can help when
jointly reasoning about several unknown edge signs.

299



3.2 Problem formulation
We now formulate a model capable of synthesizing
textual and network features.

Notation. We represent the given social network as
a signed graph G = (V,E,x), where the vertices V
represent people; the edges E, relationships between
people in V ; and the sign vector x ∈ {0,1}|E| repre-
sents edge polarities, i.e., xe = 1 (xe = 0) indicates a
positive (negative) polarity for edge e ∈ E.

Some types of relationships imply directed edges
(e.g., following a user on Twitter, or voting on a can-
didate in an election), whereas others imply undi-
rected edges (e.g., friendship on Facebook, or agree-
ment in a network of politicians). We formulate our
problem for undirected graphs here, but the exten-
sion to directed graphs is straightforward. We de-
fine a triangle t = {e1,e2,e3} ⊆ E to be a set of
three edges that form a cycle, and use T to indi-
cate the set of all triangles in G. Finally, we use
xt = (xe1 ,xe2 ,xe3) ∈ {0,1}3 to refer to t’s edge signs.
Optimization problem. We assume that the struc-
ture of the network (i.e., V and E) is fully observed,
whereas the edge signs x are only partially observed.
Further, we assume that we have a sentiment model
that outputs, for each edge e independently, an es-
timate pe of the probability that e is of positive po-
larity, based on textual features associated with e.
The task, then, is to infer the unobserved edge signs
based on the observed information.

The high-level idea is that we want to infer edge
signs that (1) agree with the predictions of the sen-
timent model, and (2) form triangles that agree with
social theories of balance and status. It is not always
possible to meet both objectives simultaneously for
all edges and triangles, so we need to find a trade-
off. This gives rise to a combinatorial optimization
problem, which we term TRIANGLE BALANCE, that
seeks to find edge signs x∗ that minimize an objec-
tive consisting of both edge and triangle costs:1

x∗ = argmin
x∈{0,1}|E|

∑

e∈E
c(xe, pe)+

∑

t∈T
d(xt). (1)

The first term is the total edge cost, in which each
edge e contributes a cost capturing how much its in-
ferred sign xe deviates from the prediction pe of the

1Of course, the entries of x∗ corresponding to observed
edges are not variable but fixed to their observed values in Eq. 1.

sentiment model. The second term, the total triangle
cost, penalizes each triangle t according to how un-
desirable its configuration is under its inferred signs
xt (e.g., if it contradicts status or balance theory).

We use the following edge cost function:

c(xe, pe) = λ1(1− pe)xe +λ0 pe(1− xe). (2)

Here, λ1,λ0 ∈ R+ are tunable parameters that allow
for asymmetric costs for positive and negative edges,
respectively, and pe is the probability of edge e be-
ing positive according to the sentiment model alone.
Intuitively, the more the inferred edge sign xe devi-
ates from the prediction pe of the sentiment model,
the higher the edge cost. (Note that at most one of
the two sum factors of Eq. 2 is non-zero.)

The triangle cost for triangle t is signified by
d(xt), which can only take on 8 distinct values be-
cause xt ∈ {0,1}3 (in practice, there are symmetries
that decrease this number to 4). The parameters
d(xt) may be tuned so that triangle configurations
that agree with social theory have low costs, while
those that disagree with it (e.g., ‘the enemy of my
friend is my friend’) have high costs.

3.3 Computational complexity

The problem defined in Eq. 1 is intuitive, but, as
with many combinatorial optimization problems, it
is hard to find a good solution. In particular, we
sketch a proof of this theorem in Appendix A:

Theorem 1. TRIANGLE BALANCE is NP-hard.

3.4 Relaxation as a Markov random field

The objective function of Eq. 1 may be seen as defin-
ing a Markov random field (MRF) over the underly-
ing social network G, with edge potentials (defined
by c) and triangle potentials (defined by d). Infer-
ence in MRFs (i.e., computing x∗) is a well-stud-
ied task for which a variety of methods have been
proposed (Koller and Friedman, 2009). However,
since our problem is NP-hard, no method can be ex-
pected to find x∗ efficiently. One way of dealing with
the computational hardness would be to find an ap-
proximate binary solution, using techniques such as
Gibbs sampling or belief propagation. Another op-
tion is to consider a continuous relaxation of the bi-
nary problem and find an exact non-binary solution
whose edge signs are continuous, i.e., xe ∈ [0,1].

300



We take this latter approach and cast our problem
as a hinge-loss Markov random field (HL-MRF).
This is inspired by Bach et al. (2013), who also use
an HL-MRF to predict edge signs based on triangle
structure, but do not use any edge features. An HL-
MRF is an MRF with continuous variables and with
potentials that can be expressed as sums of hinge-
loss terms of linear functions of the variables (cf.
Broecheler et al. (2010) for details). HL-MRFs have
the advantage that their objective function is convex
so that, unlike binary MRFs (as defined by Eq. 1),
exact inference is efficient (Bach et al., 2013).

We achieve a relaxation by using sums of hinge-
loss terms to interpolate c over the continuous do-
main [0,1] and d, over [0,1]3 (even though they are
defined only for binary domains). As a result, the
HL-MRF formulation is equivalent to Eq. 1 when
all xe are binary, but it also handles continuous val-
ues gracefully. We now interpret a real-valued ‘sign’
xe ∈ [0,1] as the degree to which e is positive.

We start by showing how to transform c: even
though it could be used in its current form (Eq. 2),
we create a tighter relaxation by using

c̃(xe, pe) = λ1‖xe− pe‖++λ0‖pe− xe‖+, (3)

where ‖y‖+ = max{0,y} is the hinge loss. At most
one term can be active for any xe ∈ [0,1] due to the
hinge loss, and c(xe, pe) = c̃(xe, pe) for binary xe.

To rewrite d, notice that, for any xt ∈ {0,1}3, we
can write d as

d(xt) =
∑

z∈{0,1}3
d(z) δ(xt ,z), (4)

where δ(xt ,z) = 1 if xt = z and 0 otherwise. While δ
is not convex, we can use

f (xt ,z) = ‖1−‖xt − z‖1‖+ (5)

as a convex surrogate. When xt is binary, either
xt = z so ‖xt − z‖1 = 0 or xt 6= z so ‖xt − z‖1 ≥ 1,
and hence f (xt ,z) = δ(xt ,z). To prove convexity,
note that, for any fixed binary z∈ {0,1}3, ‖x−z‖1 =∑3

i=1 |xi− zi| is linear in x ∈ [0,1]3, since |xi− zi|
equals either xi (if zi = 0) or 1− xi (if zi = 1). It fol-
lows that f is a hinge-loss function of a linear trans-
formation of xt and therefore convex in xt .

Training
Testing

Evidence   To infer

RANDOM SAMPLING BFS SAMPLING

u v

Figure 2: Options for training and testing our model.

Requiring the triangle cost d(z) to be nonnegative
for all triangle types z ∈ {0,1}3, we can use

d̃(xt) =
∑

z∈{0,1}3
d(z) f (xt ,z) (6)

as a convex surrogate for d. Our overall optimization
problem is then the following relaxation of Eq. 1:

x∗ = argmin
x∈[0,1]|E|

∑

e∈E
c̃(xe, pe)+

∑

t∈T
d̃(xt). (7)

This objective has the exact form of an HL-MRF,
since it is a weighted sum of hinge losses of lin-
ear functions of x. We use the Probabilistic Soft
Logic package2 to perform the optimization, which
is in turn based on the alternating-direction method
of multipliers (ADMM) (Boyd et al., 2011).

Learning. Clearly, a solution is only useful if the
cost parameters (λ1, λ0, and d(z) for all z ∈ {0,1}3)
are set appropriately. One option would be to set the
values heuristically, based on the predictions made
by the social balance and status theories (Sec. 2.2).
However, it is more principled to learn these param-
eters from data. For this purpose, we leverage the
learning procedures included in the HL-MRF imple-
mentation we employ, which uses the voted-percep-
tron algorithm to perform maximum-likelihood esti-
mation (Bach et al., 2013).

Since our data points (edges) interact with each
other via the network, some words on how we per-
form training and testing are in order. Fig. 2 shows
two options for obtaining training and testing sets
(we use both options in our experiments). In the
‘random sampling’ paradigm, we randomly choose
a set of edges for training (blue), and a disjoint set
of edges for testing (yellow). In ‘BFS sampling’, we

2http://psl.umiacs.umd.edu

301



run a breadth-first search from seed node u to obtain
a coherent training set (blue), and likewise from a
seed node v to obtain a coherent testing set (yellow),
taking care that no edges from the training set are
also included in the testing set.

During both training and testing, an arbitrary por-
tion of the edge signs may be fixed to observed val-
ues and need not be inferred. These are the solid
edges in Fig. 2; we refer to them as evidence. Fur-
ther, we define the evidence ratio as the number of
evidence edges, divided by the number of all edges
considered (solid and dashed).

The learning algorithm may use the structure (V
and E) of the training graph induced by all blue
edges (solid and dashed), the predictions pe of the
sentiment model for all blue edges, and the signs of
the solid blue edges to predict the dashed blue edges.

During testing, the network structure of all yel-
low edges, the sentiment predictions for all yellow
edges, and the signs of the solid yellow edges may
be used to predict the dashed yellow edge signs. In
principle, all training edges could be used as extra
evidence for testing (i.e., all blue edges may be made
solid yellow). However, in our experiments, we keep
the training and testing sets fully disjoint.

Technical details. For clarity, we give further de-
tails. First, the distribution of positive and negative
signs may be skewed; e.g., we observe a prior prob-
ability of 76% positive signs in our Wikipedia cor-
pus (Sec. 4). Therefore, as also done by Bach et al.
(2013), we add a cost term to our objective (Eq. 7)
that penalizes deviations from this prior probabil-
ity (as estimated on the training set). This ensures
that the model can default to a reasonable prediction
for edges that are not embedded in any triangles and
about which the sentiment model is uncertain.

Second, intuitively, we should not penalize devi-
ating from the sentiment model when it is itself un-
certain about its prediction (i.e., when pe is far from
both 0 and 1). Rather, we want to rely more heavily
on signals from the network structure in such cases.
To achieve this, we introduce 10 pairs of cost param-
eters (λ(1)1 ,λ

(1)
0 ), . . . ,(λ

(10)
1 ,λ

(10)
0 ). Then, we divide

the interval [0,1] into 10 bins, and when pe falls into
the i-th bin, we use λ(i)1 and λ

(i)
0 in Eq. 3. This way,

larger costs can be learned for the extreme bins close
to 0 and 1 than for the intermediate bins around 0.5.

Finally, hinge-loss terms may optionally be
squared in HL-MRFs. We use the squared hinge loss
in Eq. 5, since initial experimentation showed this to
perform slightly better than the linear hinge loss.

4 Wikipedia experiments

Our first set of experiments is conducted on the Wi-
kipedia Requests for Adminship corpus, which al-
lows us to evaluate our model’s ability to predict
person-to-person evaluations in Web texts that are
informal but pertain to important social outcomes.

4.1 Dataset description
For a Wikipedia editor to become an administrator,
a request for adminship (RfA)3 must be submitted,
either by the candidate or by another community
member. Subsequently, any Wikipedia member may
cast a supporting, neutral, or opposing vote. This
induces a directed, signed network in which nodes
represent Wikipedia members and edges represent
votes (we discard neutral votes).

We crawled and parsed all votes since the adop-
tion of the RfA process in 2003 through May 2013.4

This signed network was previously analyzed by
Leskovec et al. (2010b; 2010a). However, there is
also a rich textual component that has so far re-
mained untapped for edge-sign prediction: each vote
is typically accompanied by a short comment (me-
dian/mean: 19/34 tokens). A typical positive com-
ment reads, ‘I’ve no concerns, will make an excel-
lent addition to the admin corps’, while an example
of a negative comment is, ‘Little evidence of collab-
oration with other editors and limited content cre-
ation.’ The presence of a voting network alongside
textual edge features makes our method of Sec. 3
well-suited for this dataset.

The RfA network contains 11K nodes, 160K
edges (76% positive), and close to 1M triangles.

4.2 Experimental setup

Train/test sets. We follow the train–test paradigm
termed ‘BFS sampling’ in Sec. 3.4 and Fig. 2, choos-
ing 10 random seed nodes, from each of which we
perform a breadth-first search (following both in-
and out-links) until we have visited 350 nodes. We

3http://en.wikipedia.org/wiki/Wikipedia:RfA
4Data available online (West, 2014).

302



thus obtain 10 subgraphs with 350 nodes each. We
train a model for each subgraph i and test it on sub-
graph i+ 1 (mod 10), ensuring that edges from the
training graph are removed from the testing graph.

The BFS sampling paradigm was used because
the alternative (‘random sampling’ in Fig. 2) pro-
duces subgraphs with mostly isolated edges and only
a few triangles—an unrealistic scenario.

Evaluated models. We evaluate three models:

1. A standard, text-based sentiment model that
treats edges as independent data points;

2. our full model as specified in Sec. 3.4, which
combines edge costs based on the predictions
of the text-based sentiment model with triangle
costs capturing network context;

3. a version of our model that considers only trian-
gle costs, while ignoring the predictions of the
text-based sentiment model (akin to the model
proposed by Bach et al. (2013)).

We refer to these models as ‘sentiment’, ‘combined’,
and ‘network’, respectively.

Sentiment model. Our text-based sentiment model
is an L2-regularized logistic-regression classifier
whose features are term frequencies of the 10,000
overall most frequent words. The L2-penalty is cho-
sen via cross-validation on the training set. Since
comments often explicitly contain the label (‘sup-
port’ or ‘oppose’), we remove all words with pre-
fixes ‘support’ or ‘oppos’. We train the model only
once, on a random sample of 1,000 comments drawn
from the set of all 160K comments (the vast majority
of which will not appear in our 10 subgraphs).

Evidence ratio. Regarding the other two models,
recall from Sec. 3.4 our definition of the evidence
ratio, the fraction of edge signs that are fixed as evi-
dence and need not be inferred. In our experiments,
we explore the impact of the evidence ratio during
training and testing, since we expect performance to
increase as more evidence is available. (We use the
same evidence ratio for training and testing, but this
need not necessarily be so.)

Metrics. As our principal evaluation metrics, we
use the areas under the curve (AUC) of the receiver
operating characteristic (ROC) curve as well as the
precision–recall (PR) curves. There are two PR

● ● ● ●

Evidence ratio

A
re

a 
un

de
r 

th
e 

cu
rv

e

12
.5

%

25
%

50
%

75
%

0.5

0.6

0.7

0.8

0.9

1.0

●

●

●

●

● ●
● ●

(a) AUC/ROC

● ● ● ●

Evidence ratio1
2.

5% 25
%

50
%

75
%

0.2

0.4

0.6

0.8

1.0

●

●

●

●

●
●

●
●

Combined
Sentiment
Network
Random

(b) AUC/negPR

Figure 3: AUC as function of evidence ratio (Wikipedia),
with standard errors.

curves, one for the positive class, the other for the
negative one. Of these two, the positive class is less
interesting: due to the class imbalance of 76% posi-
tive edges, even a random guesser would achieve an
AUC of 0.76. The PR curve of the negative class is
more informative: here, it is much harder to achieve
high AUC, since random guessing yields only 0.24.
Moreover, the negative edges are arguably more im-
portant, not only because they are rarer, but also be-
cause they indicate tensions in the network, which
we might be interested in detecting and resolving.
For these reasons, we report only the AUC under the
negative PR curve (AUC/negPR) here.

Additionally, we report the area under the ROC
curve (AUC/ROC), a standard metric for quantify-
ing classification performance on unbalanced data.
It captures the probability of a random positive test
example receiving a higher score than a random neg-
ative one (so guessing gives an AUC/ROC of 0.5).

4.3 Results

Performance as a function of evidence ratio. The
AUCs as functions of the evidence ratio are shown
in Fig. 3(a). (We emphasize that these plots are not
themselves ROC and PR curves; rather, they are de-
rived from those curves by measuring AUC for a
range of models, parametrized by evidence ratio.)

Since we use the same sentiment model in all
cases (Sec. 4.2), its performance (yellow) does not
depend on the evidence ratio. It is remarkably high,
at an AUC/ROC of 0.88, as a consequence of the
highly indicative, sometimes even formulaic, lan-
guage used in the comments (examples in Sec. 4.2).

The network-only model (blue) works poorly on
very little evidence (AUC/ROC 0.56 for 12.5% ev-

303



● ●

●
●

●

●

●

Number of dropped features

A
re

a 
un

de
r 

th
e 

cu
rv

e

0 10 50 10
0

50
0

10
00

20
00

0.5

0.6

0.7

0.8

0.9

1.0

● ● ● ● ● ● ●

● ●
● ●

● ● ●

(a) AUC/ROC

●
●

●
●

●

●
●

Number of dropped features

0 10 50 10
0

50
0

10
00

20
00

0.2

0.4

0.6

0.8

1.0

● ● ● ● ● ● ●

● ●
● ●

● ● ●

Combined
Sentiment
Network
Random

(b) AUC/negPR

Figure 4: AUC as function of number of dropped features
(Wikipedia), with standard errors. Evidence ratio 75%.

idence) but improves steadily as more evidence is
used (AUC/ROC 0.82 for 75% evidence): this is in-
tuitive, since more evidence means stronger context
for each edge sign to be predicted.

Although the network-only model works poorly
on little evidence, our full model (black), which syn-
thesizes the sentiment and network models, is not af-
fected by this and effectively defaults to the behavior
of the sentiment-only model. Furthermore, although
the network-only model never attains the perfor-
mance of the sentiment-only model, combining the
two in our full model (black) nonetheless yields
a small performance boost in terms of AUC/ROC
to 0.89 for 75% evidence. The gains are signifi-
cantly larger when we consider AUC/negPR instead
of AUC/ROC: while the sentiment model achieves
0.60, the combined model improves on this by 13%,
to 0.68, at 75% evidence ratio.

Performance as a function of sentiment-model
quality. It seems hard to improve by much on a sen-
timent model that achieves an AUC/ROC of 0.88 on
its own; the Wikipedia corpus offers an exception-
ally explicit linguistic signal. Hence, in our next ex-
periment, we explore systematically how our model
behaves under a less powerful sentiment model.

First, we measure, for each feature (i.e., word),
how informative it is on its own for predicting the
signs of edges (quantified by its mutual information
with the edge sign), which induces a ranking of fea-
tures in terms of informativeness. Now, to make the
sentiment model less powerful in a controlled way,
we drop the top m features and repeat the experiment
described above for a range of m values (where we
keep the evidence ratio fixed at 75%).

●

●

●

●

●

●
●

●

●

●

1e
−

04
1e

−
02

1e
+

00

Sentiment−model prediction pe

N
or

m
. c

os
t f

or
 d

ev
ia

tin
g 

fr
om

 p
e

0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0

Figure 5: Normalized cost λ(i) (defined in Sec. 4.3; log-
arithmic scale) for deviating from sentiment-model pre-
dictions pe, for bins i = 1, . . . ,10 (Wikipedia). Upper bin
boundaries on the x-axis. Values shown are averages over
10 folds. Evidence ratio 75%.

Fig. 4 shows that the performance of the senti-
ment model (yellow) declines drastically as more
features are removed. The combined model (black),
on the contrary, is much less affected: when the per-
formance of the sentiment model drops to that of the
network-only (blue; ROC/AUC 0.81), the combined
model is still stronger than both (0.86). Even as the
sentiment model approaches random performance,
the combined model still never drops below the net-
work-only model—it simply learns to disregard the
predictions of the sentiment model altogether.

Learned edge costs. Recall from the final part of
Sec. 3.4 that each output pe of the sentiment model
falls into one of 10 bins [0,0.1], . . . , [0.9,1], with
separate edge-cost parameters λ(i)1 and λ

(i)
0 learned

for each bucket i. The rationale was to give the
model the freedom to trade off edge and triangle
costs differently for each edge e, depending on how
informative the sentiment model’s prediction pe is.

The goal of this section is to understand whether
our model indeed exposes such behavior. Recall
from Eq. 3 that λ1 is the constant with which the ab-
solute difference between pe and the inferred edge
sign xe is multiplied when xe > pe, while λ0 is the
constant when xe < pe. If pe falls into bin i, the sum
λ
(i)
1 +λ

(i)
0 expresses the cost of deviating from pe in a

single number; further, dividing this sum by the sum
of all costs (i.e., λ(i)1 and λ

(i)
0 for all bins i, plus the

costs d(z) of all triangle types z) yields a normalized
edge cost for each bin i, which we call λ(i).

Fig. 5 plots λ(i) for all bins i = 1, . . . ,10. We ob-
serve that deviating from the sentiment model costs
more when it makes a strong prediction (i.e., pe

304



S
en

t.

LO
O

LO
O

 +
 S

en
t.

0.
0

0.
2

0.
4

0.
6

0.
8

1.
0 Random

(a) AUC/ROC

S
en

t.

LO
O

LO
O

 +
 S

en
t.

0.
0

0.
2

0.
4

0.
6

0.
8

1.
0 Random

(b) AUC/negPR

Figure 6: Performance in a leave-one-out (LOO) scenario
(Wikipedia), with standard errors. For comparison: per-
formance of the sentiment model alone.

close to 0 or 1) than when it makes a non-informa-
tive one (e.g., pe ≈ 0.5). When pe ≈ 1, nearly 100%
of the total cost is spent to ensure xe ≈ pe, whereas
that fraction is only around 0.1% when pe ≈ 0.6.
Leave-one-out setting. Our model predicts many
missing edge signs simultaneously, using joint infer-
ence. Another scenario was proposed by Leskovec
et al. (2010a), who predict signs one at a time, as-
suming all other edge signs are known. We call this a
leave-one-out (‘LOO’ for short) setting. Assume we
want to predict the sign of the edge (u,v) in the LOO
setting, and that u, v, and w form a triangle. The
type of this triangle can be described by the direc-
tions and known polarities of the two edges linking
u to w, and v to w, respectively. The edge (u,v) may
be embedded in several triangles, and the histogram
over their types then serves as the feature vector of
(u,v) in a logistic regression; as additional features,
counts of u’s positive/negative out-links and v’s pos-
itive/negative in-links are used.

Since predictions in the LOO setup can draw on
the full triangle neighborhood of the edge in ques-
tion, we expect it to perform better than the net-
work-only model in which edge signs in the triangle
neighborhood are often missing. This expectation
is confirmed by Fig. 6, which shows that the LOO
model (gray) achieves an AUC/ROC (AUC/negPR)
of 0.88 (0.63), with the network-only model (Fig. 3)
at just 0.82 (0.54) at 75% evidence ratio.

However, LOO is outperformed by our combined
model incorporating sentiment information (Fig. 3),
which attains an AUC/ROC (AUC/negPR) of 0.89
(0.68). Finally, when we add the sentiment predic-
tion as another feature to the LOO model (‘LOO +

Sent.’ in Fig. 6), we do best, at 0.93 (0.75).
To summarize, we make two points: (1) By com-

bining sentiment and network features, our model
achieves better performance than a network-only
model (LOO) that has access to significantly more
network information. (2) Incorporating sentiment
information helps not only in our setup as described
in Sec. 4.2, but also in the previously proposed
leave-one-out setup (Leskovec et al., 2010a).

5 U.S. Congress experiments

We now evaluate our model in a setting in which
the linguistic person-to-person evaluations are less
direct and reliable than in the RfA corpus but the
signed network is considerably denser.

5.1 Dataset description
The ‘Convote’ corpus of Congressional speeches
(Thomas et al., 2006) consists of 3,857 speech seg-
ments drawn from 53 debates from the U.S. House
of Representatives in 2005. There is a mean of 72.8
speech segments per debate and 32.1 speakers per
debate. Segments are annotated with the speaker,
their party affiliation, the bill discussed, and how the
speaker voted on that bill (positive or negative).

Thomas et al. (2006) and others represent this cor-
pus as a bipartite person–item graph with signed
edges from Congresspeople to the bills (items) they
spoke about, and they add additional person–person
edges encoding who mentioned whom in the speech
segments. We take a different perspective, extract-
ing from it a dense, undirected person–person graph
by linking two Congresspeople if they ever voted on
the same bill, labeling the edge as positive if they
cast the same vote at least half of the time. We di-
rectly use the sentiment model trained by Thomas et
al. The resulting graph has 276 nodes, 14,690 edges
(54% positive), and 506,327 triangles.

5.2 Experimental setup
We split the network G = (V,E) into 5 folds us-
ing the ‘random sampling’ technique described in
Sec. 3.4 and Fig. 2: the set of nodes V is fixed across
all folds, and the set of edges E is partitioned ran-
domly so that each fold has 20% of all edges. In
the full graph, there is one clique per debate, so each
fold contains the overlay of several subgraphs, one
per debate and each 20% complete on average.

305



Here, random sampling was used because the al-
ternative (‘BFS sampling’ in Fig. 2) would produce
nearly complete subgraphs, on which we found the
prediction task to be overly easy (since the problem
becomes more constrained; Sec. 5.3).

We compare the three models also used on the
Wikipedia dataset (Sec. 4.2). Our sentiment model
comes right out of the box with the Convote cor-
pus: Thomas et al. (2006) distribute the text-level
scores from their SVM classifier with the corpus, so
we simply work with those, after transforming them
into probabilities via logistic regression (a standard
technique called Platt scaling (Platt, 1999)). Thus,
let qu and qv be the probabilistic sentiment predic-
tions for u and v on a given bill. The probability that
u and v agree on the bill is quqv +(1− qu)(1− qv),
and we define the probability pe of a positive sign on
the edge e = {u,v} as the average agreement proba-
bility over all bills that u and v co-voted on.

For instance, the speech containing the sentence,
‘Mr. Speaker, I do rise today in strong support of
H.R. 810,’ receives a probability of 98% of express-
ing a positive opinion on H.R. (i.e., House of Rep-
resentatives) bill 810, whereas the prediction for the
speech containing the words, ‘Therefore, I urge my
colleagues to vote against both H.R. 810 and H.R.
2520,’ is only 1%. Hence, the edge between the two
respective speakers has a probability of 98%×1%+
2%×99% = 3% of being positive.

5.3 Results
Fig. 7 summarizes our results. As in the Wikipedia
experiments, we report AUCs as a function of the
evidence ratio. The sentiment model alone (yellow)
achieves an AUC/ROC (AUC/negPR) of 0.65 (0.62),
well above the random baselines at 0.5 (0.46). The
network-only model (blue) performs much worse at
the start, but it surpasses the sentiment model even
with just 12.5% of the edges as evidence, a reflection
of the dense, high-quality network structure with
many triangles. When we combine the sentiment
and network models (black), we consistently see the
best results, with the largest gains in the realistic sce-
nario where there is little evidence.

Eventually, the network-only model catches up
to the combined model, simply because it reaches
an upper bound on performance given available evi-
dence. This owes mainly to the fact that, because we

● ● ● ● ● ●

Evidence ratio

A
re

a 
un

de
r 

th
e 

cu
rv

e

5% 10
%

12
.5

%

15
%

20
%

25
%

0.5

0.6

0.7

0.8

0.9

1.0

●

●

●

●

●

●

●

●

●

●

●

●

(a) AUC/ROC

● ● ● ● ● ●

Evidence ratio

5% 10
%

12
.5

%

15
%

20
%

25
%

0.4

0.5

0.6

0.7

0.8

0.9

1.0

●

●

●

●

●

●

●

●

●

●

●

●

Combined
Sentiment
Network
Random

(b) AUC/negPR

Figure 7: AUC as function of evidence ratio (Convote),
with standard errors.

derived the person–person signs from person–item
signs, only triangles with an even number of nega-
tive edges arise with noticeable frequency. To see
why, suppose the corpus contained speeches about
just one bill. In a triangle consisting of nodes u, v,
and w, if u agreed with v and with w, then v and w
must agree as well. (The fact that we have multi-
ple bills in the corpus opens up the possibility for
additional triangle types, but they rarely arise in the
data.) This constrains the solution space and makes
the problem easier than in the case of Wikipedia,
where all triangle types are possible.

Our plots so far have summarized precision–recall
curves by measuring AUC. Here, it is also informa-
tive to inspect a concrete PR curve, as in Fig. 8,
which shows all the values at 15% evidence ratio.
The network-only model (blue) achieves very high
precision up to a recall of about 0.20, where there is
a sudden drop. The reason is that, according to the
above argument about possible triangle types, the
model can be very certain about some edges (e.g.,
because it is the only non-evidence edge in a tri-
angle, making only a single triangle type possible),
which causes the plateau for low recall. The com-
bined model matches the precision on the plateau,
but also maintains significantly higher precision as
the network-only model starts to do more poorly:
even if an edge e is not fully determined by sur-
rounding evidence, the sentiment model might still
give strong signals for e itself and its neighbors, such
that the above reasoning effectively still applies.

6 Discussion

We developed a model that synthesizes textual and
social-network information to jointly predict the po-

306



(a) Positive signs (b) Negative signs

Figure 8: Precision/recall (Convote, evidence ratio 15%).

larity of person-to-person evaluations, and we as-
sessed this model in two datasets. Both involve com-
munal decision making, where people’s attitudes
and opinions of each other have profound social con-
sequences, but they are very different. In the Wi-
kipedia corpus, the sentiment signal is strong be-
cause of established community norms for how to
convey one’s opinions, but the network is sparse. In
the Convote corpus, the network is determined by
fully observed voting patterns, making it strong, but
the speech texts themselves only indirectly and nois-
ily convey person-to-person opinions. In both cases,
our method excels because it is adaptive: it learns
from the data how best to combine the two signals.

Our model’s adaptivity is important for real-world
applications, where one is unlikely to know ahead of
time which signals are most trustworthy. We envi-
sion the following use-case. One extracts a coherent
subgraph of the network of interest, perhaps using
one of our sampling methods (Fig. 2) and annotates
its edges for their evaluativity. Then, in conjunc-
tion with a sentiment model (out-of-the-box or spe-
cially trained), one trains our combined model and
uses it to predict new edge labels in the network.
In this setting, the sentiment model might be unreli-
able, and one might have the time and resources to
label only a small fraction of the edges. Individually,
the network and sentiment models would likely per-
form poorly; in bringing the two together, our single
model of joint inference could still excel.

Acknowledgements. This research has been sup-
ported in part by NSF IIS-1016909, CNS-1010921, IIS-
1149837, IIS-1159679; ARO MURI; DARPA SMISC,
GRAPHS; ONR N00014-13-1-0287; PayPal; Docomo;
and Volkswagen. Robert West has been supported by a
Facebook and a Stanford Graduate Fellowship.

Edge costs:
–1  0  +1

TWO-LEVEL
SPIN GLASS

TRIANGLE
BALANCE

v*

Figure 9: Reduction from TWO-LEVEL SPIN GLASS to
TRIANGLE BALANCE.

A Proof sketch of Theorem 1

Due to space constraints, we only give a proof sketch
here; the full proof is available online (West, 2014).

Proof sketch. By reduction from TWO-LEVEL SPIN
GLASS (TLSG), a problem known to be NP-hard
(Barahona, 1982). An instance of TLSG consists of
vertices V arranged in two 2D grids, one stacked
above the other, with edges E between nearest
neighbors, and with an edge cost cuv ∈ {−1,0,+1}
associated with each edge {u,v} (see Fig. 9 for
a small instance). Given such an instance, TLSG
asks for vertex signs x ∈ {−1,+1}|V | that mini-
mize the total energy H(x) = −∑{u,v}∈E cuv xu xv.
The crucial observation is that TLSG defines ver-
tex costs (implicitly all-zero) and edge costs, and
asks for vertex signs, whereas TRIANGLE BALANCE
defines edge costs and triangle costs, and asks for
edge signs. That is, vertices (edges) in TLSG corre-
spond to edges (triangles) in TRIANGLE BALANCE,
and our proposed reduction transforms an original
TLSG instance into a TRIANGLE BALANCE instance
in which each edge corresponds to exactly one orig-
inal vertex, and each triangle to exactly one original
edge. As shown in Fig. 9, which depicts the reduc-
tion schematically, this is achieved by introducing a
new vertex v∗ that is connected to each original ver-
tex and thus creates a triangle for each original edge.
The full proof (West, 2014) shows how the edge and
triangle costs can be constructed such that each op-
timal solution to the TLSG instance corresponds to
an optimal solution to the TRIANGLE BALANCE in-
stance, and vice versa.

307



References

Rakesh Agrawal, Sridhar Rajagopalan, Ramakrishnan
Srikant, and Yirong Xu. 2003. Mining newsgroups
using networks arising from social behavior. In
Proceedings of the 12th International Conference on
World Wide Web.

Pranav Anand, Marilyn Walker, Rob Abbott, Jean E.
Fox Tree, Robeson Bowmani, and Michael Minor.
2011. Cats rule and dogs drool! Classifying stance
in online debate. In Proceedings of the 2nd Workshop
on Computational Approaches to Subjectivity and Sen-
timent Analysis.

Stephen Bach, Bert Huang, Ben London, and Lise
Getoor. 2013. Hinge-loss Markov random fields:
Convex inference for structured prediction. In Pro-
ceedings of the 29th Conference on Uncertainty in Ar-
tificial Intelligence.

Francisco Barahona. 1982. On the computational com-
plexity of Ising spin glass models. Journal of Physics
A: Mathematical and General, 15(10):3241–3253.

John Blitzer, Mark Dredze, and Fernando Pereira. 2007.
Biographies, bollywood, boom-boxes and blenders:
Domain adaptation for sentiment classification. In
Proceedings of the 45th Annual Meeting of the Associ-
ation of Computational Linguistics.

Stephen Boyd, Neal Parikh, Eric Chu, Borja Peleato,
and Jonathan Eckstein. 2011. Distributed optimiza-
tion and statistical learning via the alternating direc-
tion method of multipliers. Foundations and Trends in
Machine Learning, 3(1):1–122.

Matthias Broecheler, Lilyana Mihalkova, and Lise
Getoor. 2010. Probabilistic similarity logic. In Pro-
ceedings of the 26th Conference on Uncertainty in Ar-
tificial Intelligence.

Dorwin Cartwright and Frank Harary. 1956. Structure
balance: A generalization of Heider’s theory. Psycho-
logical Review, 63(5):277–293.

Robin I. Dunbar. 2004. Gossip in evolutionary perspec-
tive. Review of General Psychology, 8(2):100–110.

Lisa Feldman Barrett and James A. Russell. 1998.
Independence and bipolarity in the structure of af-
fect. Journal of Personality and Social Psychology,
74(4):967–984.

Fritz Heider. 1946. Attitudes and cognitive organization.
The Journal of Psychology, 21(1):107–112.

Xia Hu, Lei Tang, Jiliang Tang, and Huan Liu. 2013. Ex-
ploiting social relations for sentiment analysis in mi-
croblogging. In Proceedings of the 6th ACM Interna-
tional Conference on Web Search and Data Mining.

Bert Huang, Angelika Kimmig, Lise Getoor, and Jennifer
Golbeck. 2013. A flexible framework for probabilis-
tic models of social trust. In Proceedings of the 2013

International Social Computing, Behavioral-Cultural
Modeling and Prediction Conference.

Dan Jurafsky, Victor Chahuneau, Bryan R. Routledge,
and Noah A. Smith. 2014. Narrative framing of con-
sumer sentiment in online restaurant reviews. First
Monday, 19(4–7).

Daphne Koller and Nir Friedman. 2009. Probabilistic
Graphical Models: Principles and Techniques. MIT
Press.

Jérôme Kunegis, Julia Preusse, and Felix Schwagereit.
2013. What is the added value of negative links in
online social networks? In Proceedings of the 22nd
International Conference on World Wide Web.

Jure Leskovec, Daniel Huttenlocher, and Jon Kleinberg.
2010a. Predicting positive and negative links in online
social networks. In Proceedings of the 19th Interna-
tional Conference on World Wide Web.

Jure Leskovec, Daniel Huttenlocher, and Jon Kleinberg.
2010b. Signed networks in social media. In Proceed-
ings of the SIGCHI Conference on Human Factors in
Computing Systems.

Wei-Hao Lin, Theresa Wilson, Janyce Wiebe, and
Alexander Hauptmann. 2006. Which side are you on?
Identifying perspectives at the document and sentence
levels. In Proceedings of the 10th Conference on Com-
putational Natural Language Learning.

Hao Ma, Dengyong Zhou, Chao Liu, Michael R Lyu, and
Irwin King. 2011. Recommender systems with social
regularization. In Proceedings of the 4th ACM Inter-
national Conference on Web Search and Data Mining.

Robert Malouf and Tony Mullen. 2008. Taking sides:
User classification for informal online political dis-
course. Internet Research, 18(2):177–190.

Bo Pang and Lillian Lee. 2004. A sentimental education:
Sentiment analysis using subjectivity summarization
based on minimum cuts. In Proceedings of the 42nd
Annual Meeting of the Association for Computational
Linguistics.

Bo Pang and Lillian Lee. 2008. Opinion mining and
sentiment analysis. Foundations and Trends in Infor-
mation Retrieval, 2(1):1–135.

John Platt. 1999. Probabilistic outputs for support vec-
tor machines and comparisons to regularized likeli-
hood methods. Advances in Large Margin Classifiers,
10(3):61–74.

Marta Recasens, Cristian Danescu-Niculescu-Mizil, and
Dan Jurafsky. 2013. Linguistic models for analyzing
and detecting biased language. In Proceedings of the
51st Annual Meeting of the Association for Computa-
tional Linguistics.

David C. Rubin and Jennifer M. Talerico. 2009. A com-
parison of dimensional models of emotion. Memory,
17(8):802–808.

308



James A. Russell. 1980. A circumplex model of af-
fect. Journal of Personality and Social Psychology,
39(6):1161–1178.

Swapna Somasundaran and Janyce Wiebe. 2010. Recog-
nizing stances in ideological on-line debates. In Pro-
ceedings of the NAACL HLT 2010 Workshop on Com-
putational Approaches to Analysis and Generation of
Emotion in Text.

Chenhao Tan, Lillian Lee, Jie Tang, Long Jiang, Ming
Zhou, and Ping Li. 2011. User-level sentiment anal-
ysis incorporating social networks. In Proceedings of
the 17th ACM SIGKDD International Conference on
Knowledge Discovery and Data Mining.

Jiliang Tang, Huiji Gao, Xia Hu, and Huan Liu. 2013.
Exploiting homophily effect for trust prediction. In
Proceedings of the 6th ACM International Conference
on Web Search and Data Mining.

Matt Thomas, Bo Pang, and Lillian Lee. 2006. Get
out the vote: Determining support or opposition from
Congressional floor-debate transcripts. In Proceed-
ings of the 2006 Conference on Empirical Methods in
Natural Language Processing.

Robert West. 2014. Supplementary material. Online.
http://infolab.stanford.edu/∼west1/TACL2014/.

Theresa Wilson, Janyce Wiebe, and Paul Hoffmann.
2005. Recognizing contextual polarity in phrase-level
sentiment analysis. In Proceedings of Human Lan-
guage Technology Conference and Conference on Em-
pirical Methods in Natural Language Processing.

Tae Yano, Philip Resnik, and Noah A. Smith. 2010.
Shedding (a thousand points of) light on biased lan-
guage. In Proceedings of the NAACL-HLT Workshop
on Creating Speech and Language Data With Ama-
zon’s Mechanical Turk.

Bei Yu, Stefan Kaufmann, and Daniel Diermeier.
2008. Classifying party affiliation from political
speech. Journal of Information Technology and Pol-
itics, 5(1):33–48.

309



310


