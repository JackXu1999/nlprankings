



















































MT Tuning on RED: A Dependency-Based Evaluation Metric


Proceedings of the Tenth Workshop on Statistical Machine Translation, pages 428–433,
Lisboa, Portugal, 17-18 September 2015. c©2015 Association for Computational Linguistics.

MT Tuning on RED: A Dependency-Based Evaluation Metric
Liangyou Li∗ Hui Yu† Qun Liu∗†
∗ ADAPT Centre, School of Computing

Dublin City University, Ireland
† Key Laboratory of Intelligent Information Processing

Institute of Computing Technology
Chinese Academy of Sciences, China

{liangyouli,qliu}@computing.dcu.ie
yuhui@ict.ac.cn

Abstract

In this paper, we describe our submis-
sion to WMT 2015 Tuning Task. We
integrate a dependency-based MT evalu-
ation metric, RED, to Moses and com-
pare it with BLEU and METEOR in con-
junction with two tuning methods: MERT
and MIRA. Experiments are conducted us-
ing hierarchical phrase-based models on
Czech–English and English–Czech tasks.
Our results show that MIRA performs bet-
ter than MERT in most cases. Using RED
performs similarly to METEOR when tun-
ing is performed using MIRA. We submit
our system tuned by MIRA towards RED
to WMT 2015. In human evaluations, we
achieve the 1st rank in all 7 systems on the
English–Czech task and 6/9 on the Czech–
English task.

1 Introduction

Statistical Machine Translation (SMT) is modeled
as a weighted combination of several features.
Tuning in SMT refers to learning a set of opti-
mized weights, which minimize a defined trans-
lation error on a tuning set. Typically, the er-
ror is measured by an automatic evaluation met-
ric. Thanks to its simplicity and language indepen-
dence, BLEU (Papineni et al., 2002) has served
as the optimization objective since the 2000s.
Although various lexical metrics, such as TER
(Snover et al., 2006) and METEOR (Lavie and
Denkowski, 2009) etc., have been proposed, none
of them can truly replace BLEU in a phrase-based
system (Cer et al., 2010).

However, BLEU has no proficiency to deal with
synonyms, paraphrases, and syntactic equivalent
etc. (Callison-Burch et al., 2006). In addition,
as a lexical and n-gram-based metric, BLEU may
be not suitable for optimization in a syntax-based
model.

In this paper, we integrate a reference
dependency-based MT evaluation metric, RED1

(Yu et al., 2014), into the hierarchical phrase-
based model (Chiang, 2005) in Moses (Koehn
et al., 2007). In doing so, we explore whether
a syntax-based translation system will perform
better when it is optimized towards a syntax-
based evaluation criteria. We compare RED with
two other evaluation metrics, BLEU and ME-
TEOR (Section 2). Two tuning algorithms are
used (Section 3). They are MERT (Och, 2003),
MIRA (Cherry and Foster, 2012). Experiments are
conducted on Czech–English and English–Czech
translation (Section 4).

2 Evaluation Metrics

An evaluation metric, which has a higher correla-
tion with human judgments, may be used to train
a better system. In this paper, we compare three
metrics: BLEU, METEOR, and RED.

2.1 BLEU
BLEU is the most widely used metric in SMT. It
is lexical-based and language-independent. BLEU
scores a hypothesis by combining n-gram pre-
cisions over reference translations with a length
penalty.

A n-gram precision pn is calculated separately
for different n-gram lengths. BLEU combines
these precisions using a geometric mean. The re-
sulting score is subsequently scaled by a length
penalty, which penalizes a hypothesis if it is
shorter than references. Equation (1) shows a for-
mula for calculating BLEU scores:

BLEU = BP ·
( N∏
n=1

pwnn

)
, (1)

where,

BP = min{1.0, exp(1− |r|/|h|)},
1REference Dependency

428



r and h are a reference and a hypothesis, respec-
tively. In this paper, we use N = 4 and uniform
weights wn = 1N .

Even though widely used in SMT, BLEU has
some pitfalls. Because of strictly relying on lexical
sequences, BLEU cannot correctly score meaning
equivalents, such as synonyms and paraphrases. It
does not distinguish between content words and
functional words as well. In addition, the penalty
is not sufficient to be an equivalent replacement of
n-gram recall.

2.2 METEOR
METEOR relies on unigrams but considers both
precision and recall. It evaluates a hypothesis by
aligning it to a reference. METEOR identifies all
possible matches between a hypothesis-reference
pair with the following matchers:

• Exact: match words that have the same word
form.

• Stem: match words whose stems are identi-
cal.

• Synonym: match words when they are de-
fined as synonyms in the WordNet database2.

• Paraphrase: match a phrase pair when they
are listed as paraphrases in a paraphrase ta-
ble.

Typically, there is more than one possible align-
ment. In METEOR, a final alignment is obtained
by beam search in the entire alignment space.
Given the final alignment, METEOR calculates a
unigram precision P and a unigram recall R by
assigning different weights to function words and
content words to distinguish them, as in Equation
(2) and Equation (3).

P =
∑

iwi · (δ ·mi(hc) + (1− δ) ·mi(hf ))
δ · |hc|+ (1− δ) · |hf |

(2)

R =
∑

iwi · (δ ·mi(rc) + (1− δ) ·mi(rf ))
δ · |rc|+ (1− δ) · |rf |

(3)
where mi is the ith matcher, hc and rc are con-
tent words in a hypothesis and a reference, hf and
rf are functions words in a hypothesis and a ref-
erence, respectively. Then the precision and recall
are combined as in Equation (4).

2https://wordnet.princeton.edu/

Fmean =
P ·R

α · P + (1− α) ·R (4)

To consider differences in word order, a penalty
is calculated on the basis of the total number (m)
of matched words and the number (ch) of chunks.
A chunk is defined as a sequence of matches,
which are contiguous and have identical word or-
der. The penalty is formulated as in Equation (5):

Pen = γ ·
(
ch

m

)β
. (5)

The final METEOR score is calculated as fol-
lows:

Score = (1− Pen) · Fmean. (6)
α, β, γ, δ and wi are constants, which can be op-
timized to maximize the correlation with human
judgments.

By considering synonym, paraphrases, ME-
TEOR has shown to be highly correlated with
human judgments. However, these resources
are language-dependent. Besides, METEOR is
unigram-based and thus has a lack of incorporat-
ing syntactic structures.

2.3 RED
Instead of collecting n-grams from word se-
quences as in BLEU, RED extracts n-grams ac-
cording to a dependency structure of a refer-
ence, called dep-ngrams, which have two types:
headword chain (Liu and Gildea, 2005) and
fixed/floating structures (Shen et al., 2010). A
headword chain is a sequence of words which cor-
responds to a path in a dependency tree, while a
fixed/floating structure covers a sequence of con-
tiguous words. Figure 1 shows an example of dif-
ferent types of dep-ngrams.

A Fmean score is separately calculated for
each different dep-ngram lengths. Then, they are
linearly combined as follows:

RED =
N∑
n=1

wn · Fmeann (7)

Inspired by other metrics, such as TERp
(Snover et al., 2009) and METEOR, RED inte-
grates some resources as follows:

• Stem and synonym: used to align words.
This increases the possibility of matching a
dep-ngram. Different matchers are assigned

429



ant

an

with

magnifier

a

saw

I ant

an

saw

with

magnifier

I

magnifier

saw

ant

an

with

a

(a) (b)

(c) (d)

Figure 1: An illustration of dep-ngrams. (a) is a
dependency tree, (b) is a headword chain, (c) is a
fixed structure and (d) is a floating structure.

different weights, this results in a scale factor
for a dep-ngram as in Equation (8).

sm =
∑n

i=1wmi
n

(8)

• Paraphrase: used for extracting paraphrase-
ngrams. In this case, RED ignores the depen-
dency structure of a reference. A paraphrase-
ngram has a weight wpar.

• Function Word: used to distinguish content
words from function words. The function
word score of a dep-ngram or a paraphrase-
ngram can be calculated as follows:

sf =
cntf · wf + cntc · (1− wf )

cntf + cntc
, (9)

where cntf and cntc are the number of func-
tion words and the number of content words.

Ideally, both a precision score P and a recall
score R are based on the total number of dep-
ngrams in a hypothesis and a reference, respec-
tively. However, in RED only dependency struc-
tures on the reference are available. Therefore, it
uses the length of the hypothesis to approximate
the number of the dep-ngrams in the hypothesis to
calculate P . Formulas for P and R are as follows:

P =
scorepar + scoredep

|c| , (10)

R =
scorepar + scoredep

Countn(r) + Countn(par)
, (11)

where

scorepar =
∑

par∈Pn
wpar · sf , (12)

scoredep =
∑
d∈Dn

p(d, c) · sm · sf , (13)

r and c are the reference and the hypothesis, Pn
is the set of paraphrase-ngrams, Dn is the set of
dep-ngrams. p(d, c) is a match score which is 0 if
no match is found; otherwise, it is a value between
0 and 13.

3 Tuning Algorithms

Tuning algorithms in SMT are designed to op-
timize decoding weights so that a defined trans-
lation error, typically measured by an automatic
metric, is minimal on a development set. In this
paper, we compare two algorithms: MERT and
MIRA.

First, we introduce some notations. Let 〈x, y〉 ∈
D be a tuning set, where x and y are a source and
a target, respectively. Let δy(dx) be an error made
by a derivation d on the source x given y as a ref-
erence. Let `m(D,w) be the total error measured
by a metricm on the tuning setD with parameters
w.

3.1 MERT

MERT learns weights to rank candidate transla-
tions of each source sentence so that the final
document-level score measured by a specific met-
ric on the one-best translations is the highest. For-
mally, it tries to minimize the document-level error
on the translations produced by the highest scoring
translation derivation for each source sentence, as
in Equation 14.

`MERT (D,w) = ⊕〈x,y〉∈Dδy(d∗x), (14)

where
d∗x = argmax

dx

w · Φ(dx), (15)

Φ are feature functions of the decoding model,
w · Φ(dx) is a score assigned to a deviation dx

3If a headword chain ngram d in a reference r has a match

in a hypothesis c, p(d, c) = exp{−
∑n−1

i=1 distri−distci
n−1 },

where distri and distci are relative distances between ith
word and (i + 1)th word in the reference and hypothesis, re-
spectively. If a fixed/floating structure is matched, p(d, c) =
1.

430



by the decoding model, ⊕ represents the accu-
mulation of potentially non-decomposable senten-
tial errors, which then produces a document-level
evaluation score.

3.2 MIRA

MIRA is an online large margin learning algo-
rithm (Crammer and Singer, 2003). Its appli-
cation to MT decoding model tuning was firstly
explored by Watanabe et al. (2007) and then re-
fined by Chiang et al. (2008) and Cherry and Fos-
ter (2012). The MIRA we use tries to separate
a “fear” derivation d−(x, y) from a “hope” one
d+(x, y) by a margin propositional to their metric
difference (Chiang et al., 2008). The two deriva-
tions are defined as follows:

d+(x, y) = argmax
d

w · Φ(d)− δy(d) (16)

d−(x, y) = argmax
d

w · Φ(d) + δy(d) (17)

Their model-score difference and metric-score dif-
ference are defined in Equation (18) and Equation
(19), respectively.

∆s(x, y) = δy(d+(x, y))− δy(d−(x, y)) (18)

∆m(x, y) = w · (Φ(d+(x, y))− Φ(d−(x, y))}
(19)

Cherry and Foster (2012) adapt a batch strategy
in MIRA. The error, that batch MIRA tries to min-
imize is defined as below:

`MIRA(D,w) =
1

2C
||w− w0||+

∑
〈x,y〉∈D

L(x, y)

(20)
where C is a constant and L(x, y) is a loss over
a source x and a reference y, which is defined in
Equation (21).

L(x, y) = max{0,∆s(x, y)−∆m(x, y)} (21)

4 Experiments

We conduct experiments on Czech–English and
English–Czech hierarchical phrase-based transla-
tion systems built using Moses with default con-
figurations and default feature functions.

We use WMT newstest2014 as our development
data, while our test data consists of the concate-
nation of newstest2012 and newstest2013, which

BLEU METEOR RED

BLEU 18.90 28.38 19.91

METEOR 18.68 28.64 20.02

RED 18.07 28.17 19.97

BLEU 19.12 28.54 20.02

METEOR 19.10 28.56 20.05

RED 17.74 28.82 20.02

Train \ Eval.

MERT

MIRA

Table 1: Czech–English evaluation performance.
In each column, the intensity of shades indicates
the rank of values.

includes 6,003 sentence pairs in total4. English
sentences are parsed into dependency structures
by Stanford parser (Marneffe et al., 2006). Czech
sentences are parsed by a Perl implementation5 of
the MST parser (McDonald et al., 2005).

4.1 Metrics Setting
As described in Section 2.1, we use the standard
BLEU parameters6. We use METEOR 1.47 in
our experiments with default optimized parame-
ters. Specifically, for Czech to English translation,
we adopt all four lexical matching strategies with
parameter values: α = 0.85, β = 0.2, γ = 0.6,
δ = 0.75 and wi = 1.0, 0.6, 0.8, 0.6. For English
to Czech translation, we use two lexical matching
strategies, including exact and paraphrase, with
parameter values: α = 0.95, β = 0.2, γ = 0.6,
δ = 0.8 and wi = 1.0, 0.4.

In RED, we use all four matchers in the Czech–
English task while we do not use stem and syn-
onym in the English–Czech task. The same pa-
rameter values are used in both tasks. We set
N = 3, the corresponding wi = 0.6, 0.5, 0.1. We
set wmi = 0.9, 0.6, 0.6 for three matchers includ-
ing exact, stem and synonym and wpar = 0.6 for
the paraphrase matcher. We setwf = 0.2 for func-
tion words and α = 0.9 for combining P and R in
Fmean.

4.2 Results
Table 1 and Table 2 show our experimental results
on two tasks, respectively. We have several find-
ings as below:

• In both tasks best scores are achieved when
4http://statmt.org/wmt14/

translation-task.html
5http://search.cpan.org/˜rur/

Treex-Parser-MSTperl
6i.e., up to 4-gram matching with uniform weighting of

n-gram precisions.
7http://www.cs.cmu.edu/˜alavie/METEOR/

431



BLEU METEOR RED

BLEU 11.25 17.36 14.95

METEOR 10.44 17.00 14.86

RED 9.51 16.81 14.58

BLEU 11.52 17.54 15.14

METEOR 11.43 17.56 15.26

RED 11.29 17.67 15.25

Train \ Eval.

MERT

MIRA

Table 2: English–Czech evaluation performance.
In each column, the intensity of shades indicates
the rank of values.

MIRA is used rather than MERT. In most
cases, MIRA is better than MERT.

• When RED is used in MERT, we obtain a
worse performance than that of BLEU and
METEOR in almost all cases, especially in
the English–Czech task.

• When BLEU is used as the evaluation metric,
the best score is obtained by using BLEU as
the optimization objective in tuning as well.
This follows the findings in Cer et al. (2010).

• The best METEOR score is achieved when
RED is used to tune our system while the best
RED score is obtained when METEOR is
used to tune. Taking that the same resources
are used in the two metrics into considera-
tion, this may indicate that the two metrics
are correlated.

5 Submission

We submit our system tuned by MIRA towards
RED. In human evaluations, we get 6th out of 9
systems on the Czech–English task and the 1st
rank in all 7 systems on the English–Czech task.

Such human judgments suggest that RED per-
forms better on Czech than English. We guess this
is because dependency n-grams have better capa-
bility of handling free word order in Czech sen-
tences. This hypothesis can be an avenue for fu-
ture work.

6 Conclusion

In this paper, we describe our submissions to
WMT 2015 tuning task on Czech–English and
English–Czech tasks. They are hierarchical
phrase-based models both tuned by MIRA to-
wards a dependency-based metric, RED. In hu-
man evaluations, our system gets the 1st rank in
the English–Czech task.

Acknowledgements

This research has received funding from the Peo-
ple Programme (Marie Curie Actions) of the Eu-
ropean Union’s Seventh Framework Programme
FP7/2007-2013/ under REA grant agreement no.
317471. The ADAPT Centre for Digital Con-
tent Technology is funded under the SFI Research
Centres Programme (Grant 13/RC/2106) and is
co-funded under the European Regional Develop-
ment Fund.

We thank Xiaofeng Wu for his discussion and
anonymous reviewers for their insightful com-
ments. In particular, we thank reviewer #2 for pro-
viding detailed suggestions.

References
Chris Callison-Burch, Miles Osborne, and Philipp

Koehn. 2006. Re-evaluating the Role of BLEU
in Machine Translation Research. In Proceedings
of the 11th Conference of the European Chapter
of the Association for Computational Linguistics,
pages 249–256.

Daniel Cer, Christopher D. Manning, and Daniel Ju-
rafsky. 2010. The Best Lexical Metric for Phrase-
based Statistical MT System Optimization. In Hu-
man Language Technologies: The 2010 Annual
Conference of the North American Chapter of the
Association for Computational Linguistics, pages
555–563, Los Angeles, California.

Colin Cherry and George Foster. 2012. Batch Tun-
ing Strategies for Statistical Machine Translation. In
Proceedings of the 2012 Conference of the North
American Chapter of the Association for Compu-
tational Linguistics: Human Language Technolo-
gies, NAACL HLT ’12, pages 427–436, Montreal,
Canada.

David Chiang, Yuval Marton, and Philip Resnik.
2008. Online Large-margin Training of Syntactic
and Structural Translation Features. In Proceedings
of the Conference on Empirical Methods in Natu-
ral Language Processing, pages 224–233, Honolulu,
Hawaii.

David Chiang. 2005. A Hierarchical Phrase-based
Model for Statistical Machine Translation. In Pro-
ceedings of the 43rd Annual Meeting on Association
for Computational Linguistics, pages 263–270, Ann
Arbor, Michigan.

Koby Crammer and Yoram Singer. 2003. Ultracon-
servative online algorithms for multiclass problems.
Journal of Machine Learning Research, 3:951–991,
March.

Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,

432



Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, Chris Dyer, Ondej Bojar, Alexandra
Constantin, and Evan Herbst. 2007. Moses: Open
Source Toolkit for Statistical Machine Translation.
In Proceedings of the 45th Annual Meeting of the
ACL on Interactive Poster and Demonstration Ses-
sions, ACL ’07, pages 177–180, Prague, Czech Re-
public.

Alon Lavie and Michael J. Denkowski. 2009. The Me-
teor Metric for Automatic Evaluation of Machine
Translation. Machine Translation, 23(2-3):105–
115, September.

Ding Liu and Daniel Gildea. 2005. Syntactic Features
for Evaluation of Machine Translation. In Proceed-
ings of the ACL Workshop on Intrinsic and Extrin-
sic Evaluation Measures for Machine Translation
and/or Summarization, pages 25–32.

Marie-Catherine de Marneffe, Bill MacCartney, and
Christopher D. Manning. 2006. Generating Typed
Dependency Parses from Phrase Structure Parses. In
Language Resources and Evaluation.

Ryan McDonald, Koby Crammer, and Fernando
Pereira. 2005. Online large-margin training of de-
pendency parsers. In Proceedings of the 43rd An-
nual Meeting on Association for Computational Lin-
guistics, pages 91–98, Ann Arbor, Michigan.

Franz Josef Och. 2003. Minimum Error Rate Train-
ing in Statistical Machine Translation. In Proceed-
ings of the 41st Annual Meeting on Association for
Computational Linguistics - Volume 1, pages 160–
167, Stroudsburg, PA, USA. Association for Com-
putational Linguistics.

Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: A Method for Automatic
Evaluation of Machine Translation. In Proceedings
of the 40th Annual Meeting on Association for Com-
putational Linguistics, ACL ’02, pages 311–318,
Philadelphia, Pennsylvania.

Libin Shen, Jinxi Xu, and Ralph Weischedel. 2010.
String-to-dependency Statistical Machine Transla-
tion. Computational Linguistics, 36(4):649–671,
December.

M. Snover, B. Dorr, R. Schwartz, L. Micciulla, and
J. Makhoul. 2006. A Study of Translation Edit Rate
with Targeted Human Annotation. In Proceedings
of Association for Machine Translation in the Amer-
icas, pages 223–231, Cambridge, Massachusetts,
USA, August.

Matthew Snover, Nitin Madnani, Bonnie J. Dorr, and
Richard Schwartz. 2009. Fluency, Adequacy,
or HTER?: Exploring Different Human Judgments
with a Tunable MT Metric. In Proceedings of the
Fourth Workshop on Statistical Machine Transla-
tion, pages 259–268, Athens, Greece.

Taro Watanabe, Jun Suzuki, Hajime Tsukada, and
Hideki Isozaki. 2007. Online Large-Margin Train-
ing for Statistical Machine Translation. In Pro-
ceedings of the 2007 Joint Conference on Empirical
Methods in Natural Language Processing and Com-
putational Natural Language Learning, pages 764–
773, Prague, June.

Hui Yu, Xiaofeng Wu, Jun Xie, Wenbin Jiang, Qun Liu,
and Shouxun Lin. 2014. RED: A Reference Depen-
dency Based MT Evaluation Metric. In Proceedings
of COLING 2014, the 25th International Conference
on Computational Linguistics: Technical Papers,
pages 2042–2051, Dublin, Ireland, August. Dublin
City University and Association for Computational
Linguistics.

433


