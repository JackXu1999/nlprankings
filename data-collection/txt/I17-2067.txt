



















































Extracting and Understanding Contrastive Opinion through Topic Relevant Sentences


Proceedings of the The 8th International Joint Conference on Natural Language Processing, pages 395–400,
Taipei, Taiwan, November 27 – December 1, 2017 c©2017 AFNLP

Extracting and Understanding Contrastive Opinion through Topic
Relevant Sentences

Ebuka Ibeke and Chenghua Lin and Adam Wyner and Mohamad Hardyman Barawi
Department of Computing Science

University of Aberdeen, UK
{e.e.ibeke,chenghua.lin,azwyner,r01mhbb}@abdn.ac.uk

Abstract

Contrastive opinion mining is essential
in identifying, extracting and organising
opinions from user generated texts. Most
existing studies separate input data into re-
spective collections. In addition, the rela-
tionships between the topics extracted and
the sentences in the corpus which express
the topics are opaque, hindering our un-
derstanding of the opinions expressed in
the corpus. We propose a novel unified
latent variable model (contraLDA) which
addresses the above matters. Experimen-
tal results show the effectiveness of our
model in mining contrasted opinions, out-
performing our baselines.

1 Introduction

Recent text mining applications have uncovered
public opinions and social trends. This is par-
tially driven by large corpora of opinionated doc-
uments in the web. Contrastive opinion mining
is the discovery of opposing opinions and senti-
ments held by individuals or groups about a given
topic. The usefulness of contrastive opinion min-
ing spans across many applications such as discov-
ering the public’s stand on major socio-political
events (Fang et al., 2012), observing heated de-
bates over controversial issues (Lippi and Tor-
roni, 2016), and product review sites (Lerman and
McDonald, 2009). Considering the volume of re-
views, it is highly desirable to acquire an overview
of the major viewpoints from large amounts of text
data automatically, allowing one to convert data
into actionable knowledge for timely decision-
making.

Recently, there have been some studies on min-
ing contrastive viewpoints or opinions from text
(Paul and Girju, 2009; Fang et al., 2012; Elahi and

Monachesi, 2012; Gutiérrez et al., 2016). How-
ever, these studies assume that input data are sep-
arated into different collections beforehand, e.g.,
news articles from CNN vs. those from Fox News
about the same set of events. While this assump-
tion might hold for some practical scenarios, one
quite often needs to analyse contrastive opin-
ions contained in a single collection such as an
open-ended discussion about government policy
or commercial products in order to understand the
viewpoints and their connections across the col-
lection.

In addition, it is natural that debates on some
topics are more prominent, indicating the impor-
tance of the topic. Therefore, being able to un-
derstand the prominence of a topic and the levels
of contrastive sentiment will help one to prioritise
actions. Finally, existing models generally inter-
pret contrastive opinions solely in terms of the ex-
tracted topic words, which are not adequate to help
us accurately understand the opinions presented in
the corpus since the topic words only express shal-
low semantics. Understanding the dependency be-
tween the sentences in the corpus and the topic of
discussion would be illuminating. The representa-
tive sentences also help to clarify the coherence of
the extracted topics.

In this paper, we address the aforementioned
issues by proposing a novel unified latent vari-
able model (contraLDA) for mining contrastive
opinion from text collections. The proposed
model contributes the following: (1) automati-
cally discovers contrastive opinion from both sin-
gle and multiple text collections; (2) quantifies
the strength of opinion contrastiveness towards the
topic of interest, which could allow one to swiftly
flag issues that require immediate attention; and
(3) adopts the sentence extraction approach in
(Barawi et al., 2017) to extract relevant sentences
related to topics, making sentiment-bearing top-

395



ics clearer to users. Experimental results show
that our model outperforms several baseline mod-
els in terms of extracting coherent and distinc-
tive sentiment-bearing topics which express con-
trastive opinions. The topic relevant sentences ex-
tracted by our approach further help us effectively
understand and interpret sentiment-bearing topics.

2 Methodology

T×L

TT Nd Dc

𝛽 𝜑

𝜆

w𝑧

𝑙

𝜃

𝜋

𝛼𝑐

𝜀 𝑐

𝛾𝑐

Figure 1: The contraLDA Model.

We propose a model called contraLDA which
offers a unified framework for mining contrastive
opinions from text, where the source of text could
be either a single collection or multiple collec-
tion of text. The graphical model of contraLDA
is shown in Figure 1. Given a collection of docu-
ments D, assume that D can be divided in to C
classes: D = {Dc}Cc=1 with Dc documents per
class, each document d in class c is a sequence of
Nd words, each word in the document is an item
from a vocabulary with V distinct terms, and c is
the class index. Also assuming that L and T are
the total number of sentiment labels and topics,
respectively, the complete procedure for generat-
ing a word wn in contraLDA is as follows: first,
one draws a topic z from the class-constrained
topic distribution θcd. Following that, one draws
a sentiment label l from the topic specific, class-
constrained sentiment distribution πcd,z . Finally,
one draws a word from the per-corpus word distri-
bution ϕz,l conditioned on both topic z and senti-
ment label l. Note that documents of all collections
share the same ϕ, and we can fully keep track of
which collection a document belongs to based on
its class index c. It is also important to note that
the number of classes C plays a key role in con-
trolling the operation mode of contraLDA. That is
when C = 1, contraLDA is essentially modelling

a single collection of text without any class mem-
bership information. In the scenario where C > 1,
contraLDA will be switching to model multiple
collections of text, e.g., documents annotated with
class labels, or articles from New York Times and
Xinhua News about the same set of events. We
summarise the generative process of contraLDA
as follows:

• For each topic z ∈ {1, · · · , T}
– For each sentiment label l ∈ {1, · · · , S}
∗ Draw ϕz,l ∼ Dir(βz,l).

• For each document d ∈D
– choose a distribution θcd ∼ Dir(�cz ·α).
– For each sentiment label l under document d,
∗ Choose a distribution πcd,z ∼ Dir(�cl · γ).

– For each word n ∈ {1, · · · , Ncd} in document d
∗ Choose a topic zn ∼ Mult(θcd),
∗ Choose a sentiment label ln ∼

Mult(πcd,zn),
∗ Choose a word wn ∼ Mult(ϕzn,ln).

2.1 Incorporating Supervised Information.
The contraLDA model can be trained flexibly de-
pending on the type of supervision information
available. Specifically, if there are only labelled
features available (e.g., sentiment lexicon, or topic
seed words), our model will incorporate the la-
belled features to constrain the Dirichlet prior of
topic-word distributions, which essentially plays a
role in governing the model inference. If there is
fully labelled data available, e.g., labelled docu-
ments, our model will account for the full super-
vision from document labels during the generative
process, where each document can associate with
a single class label or multiple class labels. How-
ever, if the dataset contains both labelled and un-
labelled data, our model will account for the avail-
able labels during the generative process as well as
incorporate the labelled features as above to con-
strain the Dirichlet prior.

When labelled data is available, contraLDA in-
corporates supervised information by constraining
that a training document can only be generated
from the topic set with class labels correspond-
ing to the document’s observed label set. This is
achieved by introducing a dependency link from
the document label matrix � to the Dirichlet pri-
ors α and γ. Suppose a corpus has three topi-
cal labels denoted by Z = {z1, z2, z3} and for
each label zk there are two sentiment labels de-
noted by l = {l1, l2}. Given observed label matrix
�c = {�cz, �cl } = {(1, 0, 1), (1, 0)}which indicates

396



that d is associated with topic labels z1, z3 as well
as sentiment label l1, we can encode the label in-
formation into contraLDA as

αcd = �
c
z ·α (1)

γcd = �
c
l · γ (2)

This ensures that d can only be generated from
topics associated with observed class labels from
�. If there are no labelled documents available,
contraLDA will incorporate labelled features from
λ (e.g., sentiment lexicons) for constraining the
Dirichlet priors β using the same strategy de-
scribed in (Lin and He, 2009; Lin et al., 2012a).

2.2 Inference.
From the contraLDA graphical model depicted in
Figure 1, we can write the joint distribution of all
observed and hidden variables which can be fac-
tored into three terms:

P (w, z, l|α,β,γ, c) =
P (w|z, l,β)P (l|z,γ, c)P (z|α, c) (3)

The main objective of inference in contraLDA
is then to find a set of model parameters that
can best explain the observed data, namely, the
class-constrained topic proportion θc, the class-
constrained topic label specific sentiment propor-
tion πc, and the per-corpus word distribution ϕ.
To compute these target distributions, we need to
calculate the posterior distribution of the model.
As the posterior is intractable, we use a collapsed
Gibbs sampler to approximate the posterior based
on the full conditional distribution for each word
token in position t. By evaluating the model joint
distribution in Eq. 3, we can yield the full condi-
tional distribution as follows

P (zt = k, lt = j|w, z−t, l−t,α,β,γ, c) ∝
N−tk,j,wt + βk,j,i
N−tk,j +

∑
i βk,j,i

· N
−t
d,k + α

c
d,k

N−td +
∑

k α
c
d,k

· N
−t
d,k,j + γ

c
d,k,j

N−td,k +
∑

j γ
c
d,k,j

. (4)

where the superscript −t denotes a quantity that
excludes data from tth position,Nk,j,w is the num-
ber of times word w appeared in topic k with sen-
timent label j, Nk,j is the number of times words
are assigned to topic k and sentiment label j, Nd,k

is the number of times topic k is assigned to some
word tokens in document d,Nd is the total number
of words in document d, Nd,k,j is the number of
times a word from document d is associated with
topic k and sentiment label j.

Using Eq. 4, we can obtain sampling as-
signments for contraLDA model, based on
which model parameters can be estimated as
ϕk,j,i =

Nk,j,i+βk,j,i
Nk,j+

∑
i βk,j,i

, θcd,k,j =
Nd,k+α

c
k,j

Nd+
∑

k α
c
d,k

and πcd,k =
Nd,k,j+γ

c
d,k,j

Nd,k+
∑

j γ
c
d,k,j

.

2.3 Modelling the associations between
sentiment-bearing topics and sentences.

Our model adopts a computational mechanism
(Barawi et al., 2017) that can uncover the as-
sociation between an opinionated (or sentiment-
bearing) topic and the underlying sentences of a
corpus. First, we preserve the sentential structure
of each document during the corpus preprocessing
step (see §3 for more details). Second, modelling
topic-sentence relevance is essentially equivalent
to calculating the probability of a sentence given
a sentiment-bearing topic p(sent|z, l). The poste-
rior inference of our model, based on Gibbs sam-
pling, can recover the hidden sentiment label and
topic label assignments for each word in the cor-
pus. Such label-word assignment information pro-
vides a means for re-assembling the relevance be-
tween a word and a sentiment-bearing topic. By
leveraging the sentential structure information and
gathering the label assignment statistics for each
word of a sentence, we can derive the probability
of a sentence given a sentiment-bearing topic as

p(sent|z, l) = p(z, l|sent) · p(sent)
p(z, l)

∝ p(z, l|sent) · p(sent) (5)

where

p(z, l|sent) =
∑

w,z′,l′ ϕz′,l′,w∑
w∈sentϕz′,l′,w

, (6)

p(sent) =
∑
z

∑
l

∏
w∈sent

ϕz,l,w. (7)

Also p(l, z) is discounted as it is a constant
when comparing sentential labels for the same
sentiment-bearing topic. The extracted sentences
for each sentiment-bearing topic are ranked based
on their probability scores.

397



3 Experimental Setup

Dataset. We evaluate the performance of our
model1 for contrastive opinion mining on the El
Capitan dataset2 (Ibeke et al., 2016) which con-
sists of reviews manually annotated (with 18 topic
labels and 3 sentiment labels in total) for vari-
ous opinion mining tasks. The dataset consists of
2,232 customer reviews, with topic and sentiment
annotations at both the review and sentence lev-
els. For the sentiment labels, we only concentrate
on positive and negative sentiment labels with the
2.3% of neutral reviews being ignored, since the
aim of this study is to mine contrastive opinion
from text. The dataset has 10,348 sentences with
an average length of 17.3 words.
Preprocessing. We preprocessed the experimen-
tal dataset by first performing automatic sentence
segmentation3 in order to preserve the sentential
structure information of each document. We then
remove punctuation, numbers, non-alphabet char-
acters, stop words, lowercase all words, and per-
form stemming.

4 Experimental Results

Topic coherence. We first quantitatively mea-
sure the coherence of the extracted topics by our
model and compare the results with a number
of baselines, namely, LDA (Blei et al., 2003),
ccLDA (Paul and Girju, 2009), TAM (Paul and
Girju, 2010), and VODUM (Thonet et al., 2016).
We employ normalised pointwise mutual infor-
mation (NPMI) (Bouma, 2009) which outper-
forms other metrics in measuring topic coher-
ence (Newman et al., 2010; Aletras and Stevenson,
2013). We run our model and the baseline mod-
els with two sentiment labels (i.e., positive and
negative), and vary the topic number setting T ∈
{5, 10, 20, 30, 40, 50}, setting β = 0.01 (Steyvers
and Griffiths, 2007) and α = 0.1. Our model learns
α directly from data using maximum-likelihood
estimation (Lin et al., 2012b).

As can be seen from Figure 2a, there is a gen-
eral pattern for all tested models, where the co-
herence score of the extracted topics decreases as
a larger number of topics K being modelled. This
is inline with the observations of (Mimno et al.,

1While our model can be applied to both single and mul-
tiple data collections, due to page limits, we only show the
experimental results on a single dataset.

2https://github.com/eibeke/El-Capitan-Dataset
3http://www.nltk.org/

2011; Gutiérrez et al., 2016), who discovered that
as the number of topics increases, lower-likelihood
topics tend to be more incoherent, resulting in
lower coherence score for topics. In terms of indi-
vidual models, our model consistently achieves a
higher coherent score than all baseline models. For
instance, when compared with the best baseline
VODUM, our model gives over 8% averaged im-
provement. This demonstrates the capability of the
proposed contraLDA in extracting coherent and
meaningful topics.
Analysis of opinion contrastiveness. We fur-
ther study the problem of quantifying the strength
of opinion contrastiveness towards the topic of in-
terest, which allows one to swiftly flag topics or
issues that require immediate attention. We ap-
proach this by computing the prominence score
for each sentiment-bearing topic extracted by con-
traLDA given a corpus c using

P (z, l|c) = 1|D|
D∑
d=1

P (l|z, d)P (z|d)

=
1
|D|

D∑
d=1

θd,z · πd,z,l, (8)

where D is the total number of documents in the
corpus. Thus the prominence for topic z in a cor-
pus can be derived as

P (z) =
∑
l

P (z, l). (9)

Figure 2b shows some contrastive opinion topic
pairs ordered by their prominence in the corpus.
Modelling topic prominence and sentiment con-
trastiveness provides a quick overview of the no-
table topics and the sentiments towards them. We
can easily identify that the most heated topics are
update and performance. In terms of opinion
contrastiveness, we see that Speed received quite
balanced positive and negative sentiment magni-
tude. Performance and Update are skewed
towards the negative sentiment, indicating that a
majority of customers experienced a performance
drop after upgrading to El Capitan.
Contrastive opinion topic analysis. In this ex-
periment, we qualitatively evaluate our model in
the task of discovering contrastive opinions.

The top panel of Table 1 shows contrastive
opinion topic pairs extracted by our model. Note
that Performance, Office and Yosemite
are label information from the El Capitan

398



0.3

0.35

0.4

0.45

0.5

0.55

0.6

0.65

5 10 20 30 40 50

C
O

H
ER

EN
C

E 
SC

O
R

E

NO. OF TOPICS

contraLDA VODUM ccLDA TAM LDA

(a)

0 0.05 0.1 0.15 0.2

Office

Yosemite

Speed

Performance

Update

Opinion Prominence Score

Topic Prominence

Negative

Positive

(b)

Figure 2: Topic coherence analysis using NPMI (a); Analysis of topic prominence and sentiment con-
trastiveness (b). NB: blue bar indicates the overall prominence of contrastive topic pair; green bar indi-
cates the strength of a positive sentiment topic, and red bar for negative sentiment topic.

Performance Office Yosemite
+ - + - + -

work crash offic offic yosemit yosemit
run work microsoft use work upgrad

perform time compat work time destroy
faster app quick microsoft downgrade slow
app use fine ms restor work

smooth slow work crash issu mac
new mac updat issu instal bad
pro open upgrad word machin problem

macbook freez new excel macbook maverick
better just didn appl revert appl

Performance + So much better than before, and apps run faster too.
Performance - Computer slows down dramatically, programs freeze.

Office + Office 2016 opens quickly with no issues.
Office - Update:Office apps tend to crash after the update!

Yosemite + So I downgraded back to Yosemite and - hey presto!
Yosemite - My 2010 iMac was destroyed by Yosemite.

Table 1: Contrastive opinion topic examples and
the top rated sentence for each topic.

dataset. A topic pair, e.g., (Performance+,
Performance-), expresses contrastive opin-
ions towards the same topic Performance,
with ‘+’ and ‘-’ indicating the topic sentiment
orientation. For instance, the two topics under
Performance+ suggests that some people feel
the system performs better and app runs faster,
whereas Performance- seems to show highly
contrastive opinion that people have bad experi-
ence after upgrade, e.g., app crashes or freezes,
mac becomes slow. However, it is still impossible
to accurately interpret the extracted topics solely
based on its multinomial distribution, especially
when one is unfamiliar with the topic domain. We
bridge this gap by extracting the most relevant sen-
tences for a given topic, which can greatly facili-
tate sentiment-bearing topic interpretation (as de-
scribed in § 2.3).

The bottom panel of Table 1 shows the extracted
top sentences (ranked based on Eq. 5) for each

topic. For instance, the extracted top sentences
for the Office topic show that some customers
recorded an improvement with their office app
(e.g., “Office 2016 opens quickly with no issues”),
while others are unhappy with the office app (e.g.,
“Update: Office apps tend to crash after the up-
date”). We see that the top sentences can effec-
tively bridge the gap between the topic word distri-
butions and the opinion encoded within the topic,
and hence can greatly help facilitate sentiment-
bearing topic understanding and interpretation.

5 Conclusion

We presented the contraLDA model which de-
tects contrastive opinions both in single and mul-
tiple data collections, and determines the senti-
ments of the extracted opinions. Our model effec-
tively mines coherent topics and contrastive opin-
ions from text. Experimental results show that our
model outperforms baselines in extracting coher-
ent topics. In addition, we presented a mechanism
for extracting sentences from corpus that are rele-
vant to sentiment-bearing topics, which helps un-
derstanding and interpretation of the topics dis-
covered. We plan to further investigate our ap-
proach on datasets from more domains.

Acknowledgments

This work is supported by the awards made by the
UK Engineering and Physical Sciences Research
Council (Grant number: EP/P005810/1).

399



References
Nikolaos Aletras and Mark Stevenson. 2013. Evaluat-

ing topic coherence using distributional semantics.
In Proceedings of the 10th International Conference
on Computational Semantics (IWCS). pages 13–22.

Mohamad H. Barawi, Chenghua Lin, and Advaith Sid-
dharthan. 2017. Automatically labelling sentiment-
bearing topics with descriptive sentence labels. In
In Proceedings of the 22nd International Confer-
ence on Natural Language and Information Systems
(NLDB). pages 299–312.

David M. Blei, Andrew Y. Ng, and Michael I. Jordan.
2003. Latent dirichlet allocation. The Journal of
Machine Learning Research 3:993–1022.

Gerlof Bouma. 2009. Normalized (pointwise) mutual
information in collocation extraction. In The Ger-
man Society for Computational Linguistics and Lan-
guage Technology (GSCL) pages 31–40.

Mohammad F. Elahi and Paola Monachesi. 2012. An
examination of cross-cultural similarities and differ-
ences from social media data with respect to lan-
guage use. In The Emotion and Sentiment Analy-
sis Workshop in the 10th Language Resources and
Evaluation Conference (LREC). pages 4080–4086.

Yi Fang, Luo Si, Naveen Somasundaram, and Zheng-
tao Yu. 2012. Mining contrastive opinions on po-
litical texts using cross-perspective topic model. In
Proceedings of the International Conference on Web
Search and Data Mining (WSDM). pages 63–72.

ED Gutiérrez, Ekaterina Shutova, Patricia Lichtenstein,
Gerard de Melo, and Luca Gilardi. 2016. Detecting
cross-cultural differences using a multilingual topic
model. Transactions of the Association for Compu-
tational Linguistics (TACL) 4:47–60.

Ebuka Ibeke, Chenghua Lin, Chris Coe, Adam Wyner,
Dong Liu, Mohamad H. Barawi, and Noor F.A. Yu-
sof. 2016. A curated corpus for sentiment-topic
analysis. In The Emotion and Sentiment Analysis
Workshop in the 10th Language Resources and Eval-
uation Conference (LREC), Slovenia.

Kevin Lerman and Ryan McDonald. 2009. Contrastive
summarization: an experiment with consumer re-
views. In Proceedings of human language technolo-
gies: The Annual Conference of the North American
Chapter of the Association for Computational Lin-
guistics (ACL). pages 113–116.

Chenghua Lin and Yulan He. 2009. Joint senti-
ment/topic model for sentiment analysis. In Pro-
ceedings of the 18th ACM conference on Informa-
tion and knowledge management. ACM, pages 375–
384.

Chenghua Lin, Yulan He, Richard Everson, and Stefan
Ruger. 2012a. Weakly supervised joint sentiment-
topic detection from text. IEEE Transactions
on Knowledge and Data Engineering 24(6):1134–
1145.

Chenghua Lin, Yulan He, Carlos Pedrinaci, and John
Domingue. 2012b. Feature lda: a supervised topic
model for automatic detection of web api documen-
tations from the web. In International Semantic Web
Conference (ISWC). Springer, pages 328–343.

Marco Lippi and Paolo Torroni. 2016. Argument min-
ing from speech: Detecting claims in political de-
bates. In In Association for the Advancement of Ar-
tificial Intelligence (AAAI). pages 2979–2985.

David Mimno, Hanna M. Wallach, Edmund Talley,
Miriam Leenders, and Andrew McCallum. 2011.
Optimizing semantic coherence in topic models. In
Proceedings of the Conference on Empirical Meth-
ods in Natural Language Processing (EMNLP).
pages 262–272.

David Newman, Jey H. Lau, Karl Grieser, and Timo-
thy Baldwin. 2010. Automatic evaluation of topic
coherence. In Human Language Technologies: The
Annual Conference of the North American Chap-
ter of the Association for Computational Linguistics
(ACL). pages 100–108.

Michael Paul and Roxana Girju. 2009. Cross-cultural
analysis of blogs and forums with mixed-collection
topic models. In Proceedings of the Conference on
Empirical Methods on Natural Language Process-
ing (EMNLP). pages 1408–1417.

Michael Paul and Roxana Girju. 2010. A two-
dimensional topic-aspect model for discovering
multi-faceted topics. In Association for the Ad-
vancement of Artificial Intelligence (AAAI) .

Mark Steyvers and Tom Griffiths. 2007. Probabilistic
topic models pages 424–440.

Thibaut Thonet, Guillaume Cabanac, Mohand
Boughanem, and Karen Pinel-Sauvagnat. 2016.
Vodum: a topic model unifying viewpoint, topic
and opinion discovery. In European Conference on
Information Retrieval. Springer, pages 533–545.

400


