



















































Effective Attention Modeling for Neural Relation Extraction


Proceedings of the 23rd Conference on Computational Natural Language Learning, pages 603–612
Hong Kong, China, November 3-4, 2019. c©2019 Association for Computational Linguistics

603

Effective Attention Modeling for Neural Relation Extraction

Tapas Nayak and Hwee Tou Ng
Department of Computer Science
National University of Singapore

nayakt@u.nus.edu, nght@comp.nus.edu.sg

Abstract

Relation extraction is the task of determin-
ing the relation between two entities in a sen-
tence. Distantly-supervised models are popu-
lar for this task. However, sentences can be
long and two entities can be located far from
each other in a sentence. The pieces of ev-
idence supporting the presence of a relation
between two entities may not be very direct,
since the entities may be connected via some
indirect links such as a third entity or via co-
reference. Relation extraction in such scenar-
ios becomes more challenging as we need to
capture the long-distance interactions among
the entities and other words in the sentence.
Also, the words in a sentence do not contribute
equally in identifying the relation between the
two entities. To address this issue, we propose
a novel and effective attention model which
incorporates syntactic information of the sen-
tence and a multi-factor attention mechanism.
Experiments on the New York Times corpus
show that our proposed model outperforms
prior state-of-the-art models.

1 Introduction

Relation extraction from unstructured text is an
important task to build knowledge bases (KB) au-
tomatically. Banko et al. (2007) used open in-
formation extraction (Open IE) to extract relation
triples from sentences where verbs were consid-
ered as the relation, whereas supervised informa-
tion extraction systems extract a set of pre-defined
relations from text. Mintz et al. (2009), Riedel
et al. (2010), and Hoffmann et al. (2011) proposed
distant supervision to generate the training data
for sentence-level relation extraction, where rela-
tion tuples (two entities and the relation between
them) from a knowledge base such as Freebase
(Bollacker et al., 2008) were mapped to free text
(Wikipedia articles or New York Times articles).

The idea is that if a sentence contains both enti-
ties of a tuple, it is chosen as a training sentence
of that tuple. Although this process can generate
some noisy training instances, it can give a signif-
icant amount of training data which can be used to
build supervised models for this task.

Mintz et al. (2009), Riedel et al. (2010), and
Hoffmann et al. (2011) proposed feature-based
learning models and used entity tokens and their
nearby tokens, their part-of-speech tags, and other
linguistic features to train their models. Recently,
many neural network-based models have been
proposed to avoid feature engineering. Zeng et al.
(2014, 2015) used convolutional neural networks
(CNN) with max-pooling to find the relation be-
tween two given entities. Though these models
have been shown to perform reasonably well on
distantly supervised data, they sometimes fail to
find the relation when sentences are long and enti-
ties are located far from each other. CNN mod-
els with max-pooling have limitations in under-
standing the semantic similarity of words with the
given entities and they also fail to capture the long-
distance dependencies among the words and enti-
ties such as co-reference. In addition, all the words
in a sentence may not be equally important in find-
ing the relation and this issue is more prominent
in long sentences. Prior CNN-based models have
limitations in identifying the multiple important
factors to focus on in sentence-level relation ex-
traction.

To address this issue, we propose a novel multi-
factor attention model1 focusing on the syntac-
tic structure of a sentence for relation extraction.
We use a dependency parser to obtain the syn-
tactic structure of a sentence. We use a linear
form of attention to measure the semantic similar-
ity of words with the given entities and combine

1The code and data of this paper can be found at
https://github.com/nusnlp/MFA4RE



604

it with the dependency distance of words from the
given entities to measure their influence in iden-
tifying the relation. Also, single attention may
not be able to capture all pieces of evidence for
identifying the relation due to normalization of at-
tention scores. Thus we use multi-factor atten-
tion in the proposed model. Experiments on the
New York Times (NYT) corpus show that the pro-
posed model outperforms prior work in terms of
F1 scores on sentence-level relation extraction.

2 Task Description

Sentence-level relation extraction is defined as
follows: Given a sentence S and two entities
{E1, E2} marked in the sentence, find the relation
r(E1, E2) between these two entities in S from a
pre-defined set of relations R ∪ {None}. None
indicates that none of the relations in R holds be-
tween the two marked entities in the sentence. The
relation between the entities is argument order-
specific, i.e., r(E1, E2) and r(E2, E1) are not the
same. Input to the system is a sentence S and
two entities E1 and E2, and output is the relation
r(E1, E2) ∈ R ∪ {None}.

3 Model Description

We use four types of embedding vectors in our
model: (1) word embedding vector w ∈ Rdw (2)
entity token indicator embedding vector z ∈ Rdz ,
which indicates if a word belongs to entity 1, en-
tity 2, or does not belong to any entity (3) a posi-
tional embedding vector u1 ∈ Rdu which repre-
sents the linear distance of a word from the start
token of entity 1 (4) another positional embedding
vector u2 ∈ Rdu which represents the linear dis-
tance of a word from the start token of entity 2.

We use a bi-directional long short-term memory
(Bi-LSTM) (Hochreiter and Schmidhuber, 1997)
layer to capture the interaction among words in
a sentence S = {w1, w2, ....., wn}, where n is
the sentence length. The input to this layer is the
concatenated vector x ∈ Rdw+dz of word embed-
ding vector w and entity token indicator embed-
ding vector z.

xt = wt || zt
−→
ht =

−−−−→
LSTM(xt,ht−1)

←−
ht =

←−−−−
LSTM(xt,ht+1)

ht =
−→
ht||
←−
ht

−→
ht ∈ Rdw+dz and

←−
ht ∈ Rdw+dz are the output at

the tth step of the forward LSTM and backward
LSTM respectively. We concatenate them to ob-
tain the tth Bi-LSTM output ht ∈ R2(dw+dz).

3.1 Global Feature Extraction

We use a convolutional neural network (CNN) to
extract the sentence-level global features for rela-
tion extraction. We concatenate the positional em-
beddings u1 and u2 of words with the hidden rep-
resentation of the Bi-LSTM layer and use the con-
volution operation with max-pooling on concate-
nated vectors to extract the global feature vector.

qt = ht‖u1t ‖u2t
ci = f

T (qi‖qi+1‖....‖qi+k−1)
cmax = max(c1, c2, ...., cn)

vg = [c
1
max, c

2
max, ...., c

fg
max]

qt ∈ R2(dw+dz+du) is the concatenated vector for
the tth word and f is a convolutional filter vector
of dimension 2k(dw + dz + du) where k is the
filter width. The index i moves from 1 to n and
produces a set of scalar values {c1, c2, ....., cn}.
The max-pooling operation chooses the maximum
cmax from these values as a feature. With fg
number of filters, we get a global feature vector
vg ∈ Rfg .

3.2 Attention Modeling

Figure 1 shows the architecture of our attention
model. We use a linear form of attention to find the
semantically meaningful words in a sentence with
respect to the entities which provide the pieces of
evidence for the relation between them. Our at-
tention mechanism uses the entities as attention
queries and their vector representation is very im-
portant for our model. Named entities mostly con-
sist of multiple tokens and many of them may not
be present in the training data or their frequency
may be low. The nearby words of an entity can
give significant information about the entity. Thus
we use the tokens of an entity and its nearby to-
kens to obtain its vector representation. We use
the convolution operation with max-pooling in the
context of an entity to get its vector representation.

ci = f
T (xi‖xi+1‖....‖xi+k−1)

cmax = max(cb, cb+1, ...., ce)

ve = [c
1
max, c

2
max, ...., c

fe
max]



605

Figure 1: Architecture of our attention model with m = 1. We have not shown the CNN-based global feature
extraction here. FFN=feed-forward network.

f is a convolutional filter vector of size k(dw+dz)
where k is the filter width and x is the concate-
nated vector of word embedding vector (w) and
entity token indicator embedding vector (z). b
and e are the start and end index of the sequence
of words comprising an entity and its neighbor-
ing context in the sentence, where 1 ≤ b ≤
e ≤ n. The index i moves from b to e and
produces a set of scalar values {cb, cb+1, ....., ce}.
The max-pooling operation chooses the maximum
cmax from these values as a feature. With fe num-
ber of filters, we get the entity vector ve ∈ Rfe .
We do this for both entities and get v1e ∈ Rfe and
v2e ∈ Rfe as their vector representation. We adopt
a simple linear function as follows to measure the
semantic similarity of words with the given enti-
ties:

f1score(hi,v
1
e) = h

T
i W

1
av

1
e

f2score(hi,v
2
e) = h

T
i W

2
av

2
e

hi is the Bi-LSTM hidden representation of the
ith word. W1a,W

2
a ∈ R2(dw+dz)×fe are trainable

weight matrices. f1score(hi,v
1
e) and f

2
score(hi,v

2
e)

represent the semantic similarity score of the ith
word and the two given entities.

Not all words in a sentence are equally impor-
tant in finding the relation between the two enti-
ties. The words which are closer to the entities

are generally more important. To address this is-
sue, we propose to incorporate the syntactic struc-
ture of a sentence in our attention mechanism.
The syntactic structure is obtained from the depen-
dency parse tree of the sentence. Words which are
closer to the entities in the dependency parse tree
are more relevant to finding the relation. In our
model, we define the dependency distance to every
word from the head token (last token) of an entity
as the number of edges along the dependency path
(See Figure 2 for an example). We use a distance
window sizews and words whose dependency dis-
tance is within this window receive attention and
the other words are ignored. The details of our at-
tention mechanism follow.

d1i =

{
1

2l
1
i
−1 exp(f

1
score(hi,v

1
e)) if l

1
i ∈ [1, ws]

1
2ws exp(f

1
score(hi,v

1
e)) otherwise

d2i =

{
1

2l
2
i
−1 exp(f

2
score(hi,v

2
e)) if l

2
i ∈ [1, ws]

1
2ws exp(f

2
score(hi,v

2
e)) otherwise

p1i =
d1i∑
j d

1
j

, p2i =
d2i∑
j d

2
j

d1i and d
2
i are un-normalized attention scores and

p1i and p
2
i are the normalized attention scores for

the ith word with respect to entity 1 and entity 2 re-
spectively. l1i and l

2
i are the dependency distances



606

Figure 2: An example dependency tree. The two num-
bers indicate the distance of the word from the head
token of the two entities respectively along the depen-
dency tree path.

of the ith word from the two entities. We mask
those words whose average dependency distance
from the two entities is larger than ws. We use
the semantic meaning of the words and their de-
pendency distance from the two entities together
in our attention mechanism. The attention feature
vectors v1a and v

1
a with respect to the two entities

are determined as follows:

v1a =

n∑
i=1

p1ihi, v
2
a =

n∑
i=1

p2ihi

3.3 Multi-Factor Attention
Two entities in a sentence, when located far from
each other, can be linked via more than one co-
reference chain or more than one important word.
Due to the normalization of the attention scores
as described above, single attention cannot cap-
ture all relevant information needed to find the re-
lation between two entities. Thus we use a multi-
factor attention mechanism, where the number of
factors is a hyper-parameter, to gather all relevant
information for identifying the relation. We re-
place the attention matrix Wa with an attention
tensor W1:ma ∈ Rm×2(dw+dz)×2fe where m is
the factor count. This gives us m attention vec-
tors with respect to each entity. We concatenate
all the feature vectors obtained using these atten-
tion vectors to get the multi-attentive feature vec-
tor vma ∈ R4m(dw+dz).

3.4 Relation Extraction
We concatenate vg, vma, v1e , and v

2
e , and this con-

catenated feature vector is given to a feed-forward
layer with softmax activation to predict the nor-
malized probabilities for the relation labels.

r = softmax(Wr(vg || vma || v1e || v2e) + br)

Wr ∈ R(fg+2fe+4m(dw+dz))×(|R|+1) is the weight
matrix, br ∈ R|R|+1 is the bias vector of the feed-
forward layer for relation extraction, and r is the

vector of normalized probabilities of relation la-
bels.

3.5 Loss Function

We calculate the loss over each mini-batch of size
B. We use the following negative log-likelihood
as our objective function for relation extraction:

L = − 1
B

B∑
i=1

log(p(ri|si, e1i , e2i , θ))

where p(ri|si, e1i , e2i , θ) is the conditional proba-
bility of the true relation ri when the sentence si,
two entities e1i and e

2
i , and the model parameters θ

are given.

4 Experiments

4.1 Datasets

We use the New York Times (NYT) corpus (Riedel
et al., 2010) in our experiments. There are two
versions of this corpus: (1) The original NYT cor-
pus created by Riedel et al. (2010) which has 52
valid relations and a None relation. We name this
dataset NYT10. The training dataset has 455, 412
instances and 330, 776 of the instances belong to
the None relation and the remaining 124, 636 in-
stances have valid relations. The test dataset has
172, 415 instances and 165, 974 of the instances
belong to the None relation and the remaining
6, 441 instances have valid relations. Both the
training and test datasets have been created by
aligning Freebase (Bollacker et al., 2008) tuples
to New York Times articles. (2) Another version
created by Hoffmann et al. (2011) which has 24
valid relations and a None relation. We name this
dataset NYT11. The corresponding statistics for
NYT11 are given in Table 1. The training dataset
is created by aligning Freebase tuples to NYT ar-
ticles, but the test dataset is manually annotated.

4.2 Evaluation Metrics

We use precision, recall, and F1 scores to evaluate
the performance of models on relation extraction
after removing the None labels. We use a confi-
dence threshold to decide if the relation of a test
instance belongs to the set of relations R or None.
If the network predicts None for a test instance,
then it is considered as None only. But if the net-
work predicts a relation from the setR and the cor-
responding softmax score is below the confidence
threshold, then the final class is changed to None.



607

NYT10 NYT11
# relations 53 25

Train

# instances 455,412 335,843
# valid relation tuples 124,636 100,671
# None relation tuples 330,776 235,172
avg. sentence length 41.1 37.2
avg. distance between
entity pairs 12.8 12.2

Test

# instances 172,415 1,450
# valid relation tuples 6,441 520
# None relation tuples 165,974 930
avg. sentence length 41.7 39.7
avg. distance between
entity pairs 13.1 11.0

Table 1: Statistics of the NYT10 and NYT11 dataset.

This confidence threshold is the one that achieves
the highest F1 score on the validation dataset. We
also include the precision-recall curves for all the
models.

4.3 Parameter Settings
We run word2vec (Mikolov et al., 2013) on the
NYT corpus to obtain the initial word embeddings
with dimension dw = 50 and update the embed-
dings during training. We set the dimension of
entity token indicator embedding vector dz = 10
and positional embedding vector du = 5. The
hidden layer dimension of the forward and back-
ward LSTM is 60, which is the same as the dimen-
sion of input word representation vector x. The
dimension of Bi-LSTM output is 120. We use
fg = fe = 230 filters of width k = 3 for fea-
ture extraction whenever we apply the convolution
operation. We use dropout in our network with a
dropout rate of 0.5, and in convolutional layers,
we use the tanh activation function. We use the
sequence of tokens starting from 5 words before
the entity to 5 words after the entity as its con-
text. We train our models with mini-batch size of
50 and optimize the network parameters using the
Adagrad optimizer (Duchi et al., 2011). We use
the dependency parser from spaCy2 to obtain the
dependency distance of the words from the enti-
ties and use ws = 5 as the window size for depen-
dency distance-based attention.

4.4 Comparison to Prior Work
We compare our proposed model with the follow-
ing state-of-the-art models.

(1) CNN (Zeng et al., 2014): Words are rep-
resented using word embeddings and two posi-
tional embeddings. A convolutional neural net-

2https://spacy.io/

work (CNN) with max-pooling is applied to ex-
tract the sentence-level feature vector. This fea-
ture vector is passed to a feed-forward layer with
softmax to classify the relation.

(2) PCNN (Zeng et al., 2015): Words are repre-
sented using word embeddings and two positional
embeddings. A convolutional neural network
(CNN) is applied to the word representations.
Rather than applying a global max-pooling oper-
ation on the entire sentence, three max-pooling
operations are applied on three segments of the
sentence based on the location of the two entities
(hence the name Piecewise Convolutional Neural
Network (PCNN)). The first max-pooling opera-
tion is applied from the beginning of the sentence
to the end of the entity appearing first in the sen-
tence. The second max-pooling operation is ap-
plied from the beginning of the entity appearing
first in the sentence to the end of the entity appear-
ing second in the sentence. The third max-pooling
operation is applied from the beginning of the en-
tity appearing second in the sentence to the end
of the sentence. Max-pooled features are concate-
nated and passed to a feed-forward layer with soft-
max to determine the relation.

(3) Entity Attention (EA) (Shen and Huang,
2016): This is the combination of a CNN model
and an attention model. Words are represented
using word embeddings and two positional em-
beddings. A CNN with max-pooling is used to
extract global features. Attention is applied with
respect to the two entities separately. The vec-
tor representation of every word is concatenated
with the word embedding of the last token of the
entity. This concatenated representation is passed
to a feed-forward layer with tanh activation and
then another feed-forward layer to get a scalar at-
tention score for every word. The original word
representations are averaged based on the attention
scores to get the attentive feature vectors. A CNN-
extracted feature vector and two attentive feature
vectors with respect to the two entities are con-
catenated and passed to a feed-forward layer with
softmax to determine the relation.

(4) BiGRU Word Attention (BGWA) (Jat et al.,
2017): Words are represented using word embed-
dings and two positional embeddings. They are
passed to a bidirectional gated recurrent unit (Bi-
GRU) (Cho et al., 2014) layer. Hidden vectors
of the BiGRU layer are passed to a bilinear op-
erator (a combination of two feed-forward layers)



608

NYT10 NYT11
Model Prec. Rec. F1 Prec. Rec. F1
CNN (Zeng et al., 2014) 0.413 0.591 0.486 0.444 0.625 0.519
PCNN (Zeng et al., 2015) 0.380 0.642 0.477 0.446 0.679 0.538†
EA (Shen and Huang, 2016) 0.443 0.638 0.523† 0.419 0.677 0.517
BGWA (Jat et al., 2017) 0.364 0.632 0.462 0.417 0.692 0.521
BiLSTM-CNN 0.490 0.507 0.498 0.473 0.606 0.531
Our model 0.541 0.595 0.566* 0.507 0.652 0.571*

Table 2: Performance comparison of different models on the two datasets. * denotes a statistically significant
improvement over the previous best state-of-the-art model with p < 0.01 under the bootstrap paired t-test. †

denotes the previous best state-of-the-art model.

Figure 3: Precision-Recall curve for the NYT10
dataset.

to compute a scalar attention score for each word.
Hidden vectors of the BiGRU layer are multiplied
by their corresponding attention scores. A piece-
wise CNN is applied on the weighted hidden vec-
tors to obtain the feature vector. This feature vec-
tor is passed to a feed-forward layer with softmax
to determine the relation.

(5) BiLSTM-CNN: This is our own baseline.
Words are represented using word embeddings
and entity indicator embeddings. They are passed
to a bidirectional LSTM. Hidden representations
of the LSTMs are concatenated with two posi-
tional embeddings. We use CNN and max-pooling
on the concatenated representations to extract the
feature vector. Also, we use CNN and max-
pooling on the word embeddings and entity indica-
tor embeddings of the context words of entities to
obtain entity-specific features. These features are
concatenated and passed to a feed-forward layer to
determine the relation.

Figure 4: Precision-Recall curve for the NYT11
dataset.

4.5 Experimental Results

We present the results of our final model on the
relation extraction task on the two datasets in Ta-
ble 2. Our model outperforms the previous state-
of-the-art models on both datasets in terms of F1
score. On the NYT10 dataset, it achieves 4.3%
higher F1 score compared to the previous best
state-of-the-art model EA. Similarly, it achieves
3.3% higher F1 score compared to the previ-
ous best state-of-the-model PCNN on the NYT11
dataset. Our model improves the precision scores
on both datasets with good recall scores. This will
help to build a cleaner knowledge base with fewer
false positives. We also show the precision-recall
curves for the NYT10 and NYT11 datasets in Fig-
ures 3 and 4 respectively. The goal of any rela-
tion extraction system is to extract as many rela-
tions as possible with minimal false positives. If
the recall score becomes very low, the coverage of



609

Figure 5: Performance comparison across different
sentence lengths on the NYT10 dataset.

the KB will be poor. From Figure 3, we observe
that when the recall score is above 0.4, our model
achieves higher precision than all the competing
models on the NYT10 dataset. On the NYT11
dataset (Figure 4), when recall score is above 0.6,
our model achieves higher precision than the com-
peting models. Achieving higher precision with
high recall score helps to build a cleaner KB with
good coverage.

5 Analysis and Discussion

5.1 Varying the number of factors (m)
We investigate the effects of the multi-factor count
(m) in our final model on the test datasets in Ta-
ble 3. We observe that for the NYT10 dataset,
m = {1, 2, 3} gives good performance with m =
1 achieving the highest F1 score. On the NYT11
dataset, m = 4 gives the best performance. These
experiments show that the number of factors giv-
ing the best performance may vary depending on
the underlying data distribution.

5.2 Effectiveness of Model Components
We include the ablation results on the NYT11
dataset in Table 4. When we add multi-factor at-
tention to the baseline BiLSTM-CNN model with-
out the dependency distance-based weight factor
in the attention mechanism, we get 0.8% F1 score
improvement (A2−A1). Adding the dependency
weight factor with a window size of 5 improves

Figure 6: Performance comparison across different
sentence lengths on the NYT11 dataset.

NYT10 NYT11
m Prec. Rec. F1 Prec. Rec. F1
1 0.541 0.595 0.566 0.495 0.621 0.551
2 0.521 0.597 0.556 0.482 0.656 0.555
3 0.490 0.617 0.547 0.509 0.633 0.564
4 0.449 0.623 0.522 0.507 0.652 0.571
5 0.467 0.609 0.529 0.488 0.677 0.567

Table 3: Performance comparison of our model with
different values of m on the two datasets.

the F1 score by 3.2% (A3−A2). Increasing the
window size to 10 reduces the F1 score marginally
(A3−A4). Replacing the attention normalizing
function with softmax operation also reduces the
F1 score marginally (A3−A5). In our model, we
concatenate the features extracted by each atten-
tion layer. Rather than concatenating them, we
can apply max-pooling operation across the mul-
tiple attention scores to compute the final atten-
tion scores. These max-pooled attention scores
are used to obtain the weighted average vector of
Bi-LSTM hidden vectors. This affects the model
performance negatively and F1 score of the model
decreases by 3.0% (A3−A6).

5.3 Performance with Varying Sentence
Length and Varying Entity Pair Distance

We analyze the effects of our attention model with
different sentence lengths in the two datasets in
Figures 5 and 6. We also analyze the effects of
our attention model with different distances be-



610

Figure 7: Performance comparison across different dis-
tances between entities on the NYT10 dataset.

Prec. Rec. F1
(A1) BiLSTM-CNN 0.473 0.606 0.531
(A2) Standard attention 0.466 0.638 0.539
(A3) Window size (ws) = 5 0.507 0.652 0.571
(A4) Window size (ws) = 10 0.510 0.640 0.568
(A5) Softmax 0.490 0.658 0.562
(A6) Max-pool 0.492 0.600 0.541

Table 4: Effectiveness of model components (m = 4)
on the NYT11 dataset.

tween the two entities in the two datasets in Fig-
ures 7 and 8. We observe that with increasing sen-
tence length and increasing distance between the
two entities, the performance of all models drops.
This shows that finding the relation between en-
tities located far from each other is a more diffi-
cult task. Our multi-factor attention model with
dependency-distance weight factor increases the
F1 score in all configurations when compared to
previous state-of-the-art models on both datasets.

6 Related Work

Relation extraction from a distantly supervised
dataset is an important task and many researchers
(Mintz et al., 2009; Riedel et al., 2010; Hoff-
mann et al., 2011) tried to solve this task us-
ing feature-based classification models. Recently,
Zeng et al. (2014, 2015) used CNN models for
this task which can extract features automatically.
Shen and Huang (2016) and Jat et al. (2017) used
attention mechanism in their model to improve
performance. Surdeanu et al. (2012), Lin et al.

Figure 8: Performance comparison across different dis-
tances between entities on the NYT11 dataset.

(2016), Vashishth et al. (2018), Wu et al. (2019),
and Ye and Ling (2019) used multiple sentences in
a multi-instance relation extraction setting to cap-
ture the features located in multiple sentences for
a pair of entities. In their evaluation setting, they
evaluated model performance by considering mul-
tiple sentences having the same pair of entities as a
single test instance. On the other hand, our model
and the previous models that we compare to in this
paper (Zeng et al., 2014, 2015; Shen and Huang,
2016; Jat et al., 2017) work on each sentence inde-
pendently and are evaluated at the sentence level.
Since there may not be multiple sentences that
contain a pair of entities, it is important to improve
the task performance at the sentence level. Future
work can explore the integration of our sentence-
level attention model in a multi-instance relation
extraction framework.

Not much previous research has exploited the
dependency structure of a sentence in different
ways for relation extraction. Xu et al. (2015) and
Miwa and Bansal (2016) used an LSTM network
and the shortest dependency path between two en-
tities to find the relation between them. Huang
et al. (2017) used the dependency structure of a
sentence for the slot-filling task which is close
to the relation extraction task. Liu et al. (2015)
exploited the shortest dependency path between
two entities and the sub-trees attached to that path
(augmented dependency path) for relation extrac-
tion. Zhang et al. (2018) and Guo et al. (2019)



611

used graph convolution networks with pruned de-
pendency tree structures for this task. In this
work, we have incorporated the dependency dis-
tance of the words in a sentence from the two en-
tities in a multi-factor attention mechanism to im-
prove sentence-level relation extraction.

Attention-based neural networks are quite suc-
cessful for many other NLP tasks. Bahdanau
et al. (2015) and Luong et al. (2015) used atten-
tion models for neural machine translation, Seo
et al. (2017) used attention mechanism for answer
span extraction. Vaswani et al. (2017) and Kundu
and Ng (2018) used multi-head or multi-factor at-
tention models for machine translation and an-
swer span extraction respectively. He et al. (2018)
used dependency distance-focused word attention
model for aspect-based sentiment analysis.

7 Conclusion

In this paper, we have proposed a multi-factor at-
tention model utilizing syntactic structure for rela-
tion extraction. The syntactic structure component
of our model helps to identify important words in
a sentence and the multi-factor component helps
to gather different pieces of evidence present in
a sentence. Together, these two components im-
prove the performance of our model on this task,
and our model outperforms previous state-of-the-
art models when evaluated on the New York Times
(NYT) corpus, achieving significantly higher F1
scores.

Acknowledgments

We would like to thank the anonymous reviewers
for their valuable and constructive comments on
this work.

References
Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Ben-

gio. 2015. Neural machine translation by jointly
learning to align and translate. In Proceedings of
the International Conference on Learning Represen-
tations.

Michele Banko, Michael J Cafarella, Stephen Soder-
land, Matthew Broadhead, and Oren Etzioni. 2007.
Open information extraction from the web. In Pro-
ceedings of the International Joint Conference on
Artificial Intelligence.

Kurt Bollacker, Colin Evans, Praveen Paritosh, Tim
Sturge, and Jamie Taylor. 2008. Freebase: A col-
laboratively created graph database for structuring

human knowledge. In Proceedings of the ACM SIG-
MOD International Conference on Management of
Data.

Kyunghyun Cho, Bart Van Merriënboer, Dzmitry Bah-
danau, and Yoshua Bengio. 2014. On the proper-
ties of neural machine translation: Encoder-decoder
approaches. In Proceedings of Eighth Workshop on
Syntax, Semantics and Structure in Statistical Trans-
lation.

John Duchi, Elad Hazan, and Yoram Singer. 2011.
Adaptive subgradient methods for online learning
and stochastic optimization. Journal of Machine
Learning Research.

Zhijiang Guo, Yan Zhang, and Wei Lu. 2019. Attention
guided graph convolutional networks for relation ex-
traction. In Proceedings of the 57nd Annual Meeting
of the Association for Computational Linguistics.

Ruidan He, Wee Sun Lee, Hwee Tou Ng, and Daniel
Dahlmeier. 2018. Effective attention modeling for
aspect-level sentiment classification. In Proceed-
ings of the 27th International Conference on Com-
putational Linguistics.

Sepp Hochreiter and Jürgen Schmidhuber. 1997. Long
short-term memory. Neural Computation.

Raphael Hoffmann, Congle Zhang, Xiao Ling, Luke
Zettlemoyer, and Daniel S Weld. 2011. Knowledge-
based weak supervision for information extraction
of overlapping relations. In Proceedings of the 49th
Annual Meeting of the Association for Computa-
tional Linguistics.

Lifu Huang, Avirup Sil, Heng Ji, and Radu Florian.
2017. Improving slot filling performance with atten-
tive neural networks on dependency structures. In
Proceedings of the 2017 Conference on Empirical
Methods in Natural Language Processing.

Sharmistha Jat, Siddhesh Khandelwal, and Partha
Talukdar. 2017. Improving distantly supervised re-
lation extraction using word and entity based atten-
tion. In Proceedings of the 6th Workshop on Auto-
mated Knowledge Base Construction.

Souvik Kundu and Hwee Tou Ng. 2018. A question-
focused multi-factor attention network for question
answering. In Proceedings of the Association for the
Advancement of Artificial Intelligence.

Yankai Lin, Shiqi Shen, Zhiyuan Liu, Huanbo Luan,
and Maosong Sun. 2016. Neural relation extraction
with selective attention over instances. In Proceed-
ings of the 54th Annual Meeting of the Association
for Computational Linguistics.

Yang Liu, Furu Wei, Sujian Li, Heng Ji, Ming Zhou,
and Houfeng Wang. 2015. A dependency-based
neural network for relation classification. In Pro-
ceedings of the 53rd Annual Meeting of the Associ-
ation for Computational Linguistics and the 7th In-
ternational Joint Conference on Natural Language
Processing.



612

Minh-Thang Luong, Hieu Pham, and Christopher D
Manning. 2015. Effective approaches to attention-
based neural machine translation. In Proceedings of
the 2015 Conference on Empirical Methods in Nat-
ural Language Processing.

Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey
Dean. 2013. Efficient estimation of word represen-
tations in vector space. In Proceedings of the Inter-
national Conference on Learning Representations.

Mike Mintz, Steven Bills, Rion Snow, and Dan Juraf-
sky. 2009. Distant supervision for relation extrac-
tion without labeled data. In Proceedings of the
47th Annual Meeting of the Association for Compu-
tational Linguistics and the 4th International Joint
Conference on Natural Language Processing.

Makoto Miwa and Mohit Bansal. 2016. End-to-end
relation extraction using LSTMs on sequences and
tree structures. In Proceedings of the 54th Annual
Meeting of the Association for Computational Lin-
guistics.

Sebastian Riedel, Limin Yao, and Andrew McCallum.
2010. Modeling relations and their mentions with-
out labeled text. In Proceedings of the European
Conference on Machine Learning and Knowledge
Discovery in Databases.

Minjoon Seo, Aniruddha Kembhavi, Ali Farhadi, and
Hannaneh Hajishirzi. 2017. Bidirectional attention
flow for machine comprehension. In Proceedings of
the International Conference on Learning Represen-
tations.

Yatian Shen and Xuanjing Huang. 2016. Attention-
based convolutional neural network for semantic re-
lation extraction. In Proceedings of the 26th Inter-
national Conference on Computational Linguistics.

Mihai Surdeanu, Julie Tibshirani, Ramesh Nallapati,
and Christopher D. Manning. 2012. Multi-instance
multi-label learning for relation extraction. In Pro-
ceedings of the 2012 Joint Conference on Empirical
Methods in Natural Language Processing and Com-
putational Natural Language Learning.

Shikhar Vashishth, Rishabh Joshi, Sai Suman Prayaga,
Chiranjib Bhattacharyya, and Partha Talukdar. 2018.
Reside: Improving distantly-supervised neural rela-
tion extraction using side information. In Proceed-
ings of the 2018 Conference on Empirical Methods
in Natural Language Processing.

Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob
Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz
Kaiser, and Illia Polosukhin. 2017. Attention is all
you need. In Proceedings of Advances in Neural In-
formation Processing Systems.

Shanchan Wu, Kai Fan, and Qiong Zhang. 2019. Im-
proving distantly supervised relation extraction with
neural noise converter and conditional optimal se-
lector. In Proceedings of the Association for the Ad-
vancement of Artificial Intelligence.

Yuning Xu, Lili Mou, Ge Li, Yunchuan Chen, Hao
Peng, and Zhi Jin. 2015. Classifying relations via
long short term memory networks along shortest de-
pendency paths. In Proceedings of the 2015 Con-
ference on Empirical Methods in Natural Language
Processing.

Zhi-Xiu Ye and Zhen-Hua Ling. 2019. Distant supervi-
sion relation extraction with intra-bag and inter-bag
attentions. In Proceedings of the 2019 Conference
of the North American Chapter of the Association
for Computational Linguistics.

Daojian Zeng, Kang Liu, Yubo Chen, and Jun Zhao.
2015. Distant supervision for relation extraction via
piecewise convolutional neural networks. In Pro-
ceedings of the 2015 Conference on Empirical Meth-
ods in Natural Language Processing.

Daojian Zeng, Kang Liu, Siwei Lai, Guangyou Zhou,
and Jun Zhao. 2014. Relation classification via con-
volutional deep neural network. In Proceedings of
the 25th International Conference on Computational
Linguistics.

Yuhao Zhang, Peng Qi, and Christopher D. Manning.
2018. Graph convolution over pruned dependency
trees improves relation extraction. In Proceedings of
the 2018 Conference on Empirical Methods in Nat-
ural Language Processing.


