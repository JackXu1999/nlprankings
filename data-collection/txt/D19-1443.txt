















































Transformer Dissection: An Unified Understanding for Transformer's Attention via the Lens of Kernel


Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing
and the 9th International Joint Conference on Natural Language Processing, pages 4344–4353,
Hong Kong, China, November 3–7, 2019. c©2019 Association for Computational Linguistics

4344

Transformer Dissection: An Unified Understanding for Transformer’s
Attention via the Lens of Kernel

Yao-Hung Hubert Tsai1 Shaojie Bai1 Makoto Yamada34
Louis-Philippe Morency2 Ruslan Salakhutdinov1

{1Machine Learning Department,2Language Technology Institute}, Carnegie Mellon University
3Kyoto University 4RIKEN AIP

{yaohungt, shaojieb, morency, rsalakhu}@cs.cmu.edu, myamada@i.kyoto-u.ac.jp

Abstract
Transformer is a powerful architecture that
achieves superior performance on various se-
quence learning tasks, including neural ma-
chine translation, language understanding, and
sequence prediction. At the core of the Trans-
former is the attention mechanism, which con-
currently processes all inputs in the streams.
In this paper, we present a new formulation
of attention via the lens of the kernel. To be
more precise, we realize that the attention can
be seen as applying kernel smoother over the
inputs with the kernel scores being the simi-
larities between inputs. This new formulation
gives us a better way to understand individ-
ual components of the Transformer’s attention,
such as the better way to integrate the posi-
tional embedding. Another important advan-
tage of our kernel-based formulation is that it
paves the way to a larger space of compos-
ing Transformer’s attention. As an example,
we propose a new variant of Transformer’s at-
tention which models the input as a product
of symmetric kernels. This approach achieves
competitive performance to the current state of
the art model with less computation. In our
experiments, we empirically study different
kernel construction strategies on two widely
used tasks: neural machine translation and se-
quence prediction.

1 Introduction

Transformer (Vaswani et al., 2017) is a relative
new architecture which outperforms traditional
deep learning models such as Recurrent Neural
Networks (RNNs) (Sutskever et al., 2014) and
Temporal Convolutional Networks (TCNs) (Bai
et al., 2018) for sequence modeling tasks across
neural machine translations (Vaswani et al., 2017),
language understanding (Devlin et al., 2018), se-
quence prediction (Dai et al., 2019), image gener-
ation (Child et al., 2019), video activity classifica-
tion (Wang et al., 2018), music generation (Huang

et al., 2018a), and multimodal sentiment analy-
sis (Tsai et al., 2019a). Instead of performing re-
currence (e.g., RNN) or convolution (e.g., TCN)
over the sequences, Transformer is a feed-forward
model that concurrently processes the entire se-
quence. At the core of the Transformer is its at-
tention mechanism, which is proposed to integrate
the dependencies between the inputs. There are
up to three types of attention within the full Trans-
former model as exemplified with neural machine
translation application (Vaswani et al., 2017): 1)
Encoder self-attention considers the source sen-
tence as input, generating a sequence of encoded
representations, where each encoded token has a
global dependency with other tokens in the in-
put sequence. 2) Decoder self-attention consid-
ers the target sentence (e.g., predicted target se-
quence for translation) as input, generating a se-
quence of decoded representations1, where each
decoded token depends on previous decoded to-
kens. 3) Decoder-encoder attention considers both
encoded and decoded sequences, generating a se-
quence with the same length as the decoded se-
quence. It should be noted that some applica-
tions has only the decoder self-attention such as
sequence prediction (Dai et al., 2019). In all cases,
the Transformer’s attentions follow the same gen-
eral mechanism.

At the high level, the attention can be seen as
a weighted combination of the input sequence,
where the weights are determined by the similari-
ties between elements of the input sequence. We
note that this operation is order-agnostic to the per-
mutation in the input sequence (order is encoded
with extra positional embedding (Vaswani et al.,
2017; Shaw et al., 2018; Dai et al., 2019)). The

1The generated sequence can be regarded as a translated
sequence (i.e., translating from the encoded sequence), where
each generated token depends on all tokens in the encoded
sequence.



4345

above observation inspires us to connect Trans-
former’s attention to kernel learning (Scholkopf
and Smola, 2001): they both concurrently and
order-agnostically process all inputs by calculat-
ing the similarity between the inputs. Therefore,
in the paper, we present a new formulation for
Transformer’s attention via the lens of kernel. To
be more precise, the new formulation can be in-
terpreted as a kernel smoother (Wasserman, 2006)
over the inputs in a sequence, where the kernel
measures how similar two different inputs are. The
main advantage of connecting attention to kernel
is that it opens up a new family of attention mech-
anisms that can relate to the well-established lit-
erature in kernel learning (Scholkopf and Smola,
2001). As a result, we develop a new variant of at-
tention which simply considers a product of sym-
metric kernels when modeling non-positional and
positional embedding.

Furthermore, our proposed formulation high-
lights naturally the main components of Trans-
former’s attention, enabling a better understand-
ing of this mechanism: recent variants of Trans-
formers (Shaw et al., 2018; Huang et al., 2018b;
Dai et al., 2019; Child et al., 2019; Lee et al.,
2018; Wang et al., 2018; Tsai et al., 2019a)
can be expressed through these individual com-
ponents. Among all the components, we argue
that the most important one is the construction of
the kernel function. We empirically study mul-
tiple kernel forms and the ways to integrate po-
sitional embedding in neural machine translation
(NMT) using IWSLT’14 German-English (De-En)
dataset (Edunov et al., 2017) and sequence pre-
diction (SP) using WikiText-103 dataset (Merity
et al., 2016).

2 Attention

This section aims at providing an understand-
ing of attention in Transformer via the lens of
kernel. The inspiration for connecting the ker-
nel (Scholkopf and Smola, 2001) and attention in-
stantiates from the observation: both operations
concurrently processes all inputs and calculate the
similarity between the inputs. We first introduce
the background (i.e., the original formulation) of
attention and then provide a new reformulation
within the class of kernel smoothers (Wasserman,
2006). Next, we show that this new formulation
allows us to explore new family of attention while
at the same time offering a framework to cate-

gorize previous attention variants (Vaswani et al.,
2017; Shaw et al., 2018; Huang et al., 2018b; Dai
et al., 2019; Child et al., 2019; Lee et al., 2018;
Wang et al., 2018; Tsai et al., 2019a). Last, we
present a new form of attention, which requires
fewer parameters and empirically reaches compet-
itive performance as the state-of-the-art models.

For notation, we use lowercase representing a
vector (e.g., x), bold lowercase representing a ma-
trix (e.g., x), calligraphy letter denoting a space
(e.g., X ), and S denoting a set. To relate the no-
tations in sequence to sequence learning (Vaswani
et al., 2017), x represents a specific element of a
sequence, x = [x1, x2,⋯, xT ] denotes a sequence
of features, Sx = {xexp, x2,⋯, xT } represents the
set with its elements being the features in sequence
x, and we refer the space of set Sx as S .

2.1 Technical Background

Unlike recurrent computation (Sutskever et al.,
2014) (i.e., RNNs) and temporal convolutional
computation (Bai et al., 2018) (i.e., TCNs), Trans-
former’s attention is an order-agnostic operation
given the order in the inputs (Vaswani et al., 2017).
Hence, in the presentation of the paper, we con-
sider the inputs as a set instead of a sequence.
When viewing sequence as a set, we lose the tem-
poral (positional) information in inputs which is
often crucial for sequence modeling (Sutskever
et al., 2014). As a result, Transformer (Vaswani
et al., 2017) introduced positional embedding to
indicate the positional relation for the inputs. For-
mally, a sequence x = [x1, x2,⋯, xT ] defines each
element as xi = (fi, ti) with fi ∈ F being the non-
temporal feature at time i and ti ∈ T as an tem-
poral feature (or we called it positional embed-
ding). Note that fi can be the word representa-
tion (in neural machine translation (Vaswani et al.,
2017)), a pixel in a frame (in video activity recog-
nition (Wang et al., 2018)), or a music unit (in mu-
sic generation (Huang et al., 2018b)). ti can be
a mixture of sine and cosine functions (Vaswani
et al., 2017) or parameters that can be learned dur-
ing back-propagation (Dai et al., 2019; Ott et al.,
2019). The feature vector are defined over a joint
space X ∶= (F × T ). The resulting permutation-
invariant set is: Sx = {x1, x2,⋯, xT } =
{(f1, t1), (f2, t2),⋯, (fT , tT )}.

Followed the definition by Vaswani et al.
(2017), we use queries(q)/keys(k)/values(v) to
represent the inputs for the attention. To be



4346

more precise, x{q/k/v} is used for denoting a
query/key/value data in the query/key/value se-
quence x{q/k/v} (x{q/k/v} ∈ Sx{q/k/v}) with
Sx{q/k/v} being its set representation. We note that
the input sequences are the same (xq = xk) for
self-attention and are different (xq from decoder
and xk from encoder) for encoder-decoder atten-
tion.

Given the introduced notation, the attention
mechanism in original Transformer (Vaswani
et al., 2017) can be presented as:

Attention(xq ; Sxk)

= softmax(xqWq(xkWk)
⊺

√
dk

)xkWv
(1)

with xq = fq + tq, xk = fk + tk, Wq/k/v being
the weight, and dk being the feature dimension of
xkWk. Decoder self-attention further introduces a
mask to block the visibility of elements in Sxk to
xq. Particularly, decoder self-attention considers
the decoded sequence as inputs (xk = xq), where
the decoded token at time t is not allowed to access
the future decoded tokens (i.e., tokens decoded at
time greater than t). On the contrary, encoder self-
attention and decoder-encoder attention consider
no additional mask to Eq. (1).

Recent work (Shaw et al., 2018; Dai et al., 2019;
Huang et al., 2018b; Child et al., 2019; Lee et al.,
2018; Parmar et al., 2018; Tsai et al., 2019a) pro-
posed modifications to the Transformer for the
purpose of better modeling inputs positional re-
lation (Shaw et al., 2018; Huang et al., 2018b;
Dai et al., 2019), appending additional keys in
Sxk (Dai et al., 2019), modifying the mask ap-
plied to Eq. (1) (Child et al., 2019), or applying
to distinct feature types (Lee et al., 2018; Parmar
et al., 2018; Tsai et al., 2019a). These works adopt
different designs of attention as comparing to the
original form (Eq. (1)). In our paper, we aim at
providing an unified view via the lens of kernel.

2.2 Reformulation via the Lens of Kernel
We now provide the intuition to reformulate Eq.
(1) via the lens of kernel. First, the softmax func-
tion can be realized as a probability function for
xq observing the keys {xk}s in Sxk (Sxk is the set
representation of sequence xk). The probability
is determined by the dot product between xq and
xk with additional mappings Wq/Wk and scaling
by dk, which we note the dot-product operation is
an instance of kernel function. We also introduce

a set filtering function M(xq, Sxk) ∶ X × S → S
which returns a set with its elements that operate
with (or are connected/visible to) xq. The filter-
ing function M(⋅, ⋅) plays as the role of the mask
in decoder self-attention (Vaswani et al., 2017).
Putting these altogether, we re-represent Eq. (1)
into the following definition.

Definition 1. Given a non-negative kernel func-
tion k(⋅, ⋅) ∶ X × X → R+, a set filtering func-
tion M(⋅, ⋅) ∶ X × S → S, and a value function
v(⋅) ∶ X → Y , the Attention function taking the
input of a query feature xq ∈ X is defined as

Attention(xq ; M(xq, Sxk))

= ∑
xk∈M(xq ,Sxk)

k(xq, xk)
∑xk′∈M(xq ,Sxk) k(xq, xk

′)v(xk).

(2)

The Definition 1 is a class of linear
smoothers (Wasserman, 2006) with kernel
smoothing:

∑
xk∈M(xq ,Sxk)

k(xq, xk)
∑xk′∈M(xq ,Sxk) k(xq, xk

′)v(xk)

= Ep(xk ∣xq)[v(xk)],

where v(xk) outputs the “values” and p(xk∣xq) =
k(xq ,xk)

∑xk′∈M(xq,Sxk ) k(xq ,xk
′) is a probability function

depends on k and N when k(⋅, ⋅) is always
positive. In the prior work (Vaswani et al.,
2017), k(xq, xk) = exp (⟨xqWq, xkWk⟩/

√
dk)

and v(xk) = xkWv. Note that the kernel form
k(xq, xk) in the original Transformer (Vaswani
et al., 2017) is a asymmetric exponential ker-
nel with additional mapping Wq and Wk (Wilson
et al., 2016; Li et al., 2017)2.

The new formulation defines a larger space for
composing attention by manipulating its individ-
ual components, and at the same time it is able to
categorize different variants of attention in prior
work (Shaw et al., 2018; Huang et al., 2018b; Dai
et al., 2019; Child et al., 2019; Lee et al., 2018;
Wang et al., 2018; Tsai et al., 2019a). In the fol-
lowing, we study these components by dissect-
ing Eq. (2) into: 1) kernel feature space X , 2)

2We note that rigorous definition of kernel func-
tion (Scholkopf and Smola, 2001) requires the kernel to be
semi-positive definite and symmetric. While in the paper, the
discussion on kernel allows it to be non-semi-positive definite
and asymmetric. In Section 3, we will examine the kernels
which are semi-positive and symmetric.



4347

kernel construction k(⋅, ⋅), 3) value function v(⋅),
and 4) set filtering function M(⋅, ⋅).

2.2.1 Kernel Feature Space X
In Eq. (2), to construct a kernel on X , the
first thing is to identify the kernel feature space
X . In addition to modeling sequences like
word sentences (Vaswani et al., 2017) or music
signals (Huang et al., 2018b), the Transformer
can also be applied to images (Parmar et al.,
2018), sets (Lee et al., 2018), and multimodal se-
quences (Tsai et al., 2019a). Due to distinct data
types, these applications admit various kernel fea-
ture space:
(i) Sequence Transformer (Vaswani et al., 2017;
Dai et al., 2019):

X ∶= (F × T )

with F being non-positional feature space and T
being the positional embedding space of the posi-
tion in the sequence.
(ii) Image Transformer (Parmar et al., 2018):

X ∶= (F ×H ×W)

with F being non-positional feature space, H be-
ing the positional space of the height in an image,
and W being the positional space of the width in
an image.
(iii) Set Transformer (Lee et al., 2018) and Non-
Local Neural Networks (Wang et al., 2018):

X ∶= (F)

with no any positional information present.
(iv) Multimodal Transformer (Tsai et al., 2019a):

X ∶= (F ` ×Fv ×Fa × T )

with F ` representing the language feature space,
Fv representing the vision feature space, Fa rep-
resenting the audio feature space, and T represent-
ing the temporal indicator space.

For the rest of the paper, we will focus on the
setting for sequence Transformer X = (F × T )
and discuss the kernel construction on it.

2.2.2 Kernel Construction and the Role of
Positional Embedding k(⋅, ⋅)

The kernel construction on X = (F × T ) has dis-
tinct design in variants of Transformers (Vaswani

et al., 2017; Dai et al., 2019; Huang et al., 2018b;
Shaw et al., 2018; Child et al., 2019). Since now
the kernel feature space considers a joint space,
we will first discuss the kernel construction on F
(the non-positional feature space) and then discuss
how different variants integrate the positional em-
bedding (with the positional feature space T ) into
the kernel.

Kernel construction on F . All the work con-
sidered the scaled asymmetric exponential kernel
with the mappingWq andWk (Wilson et al., 2016;
Li et al., 2017) for non-positional features fq and
fk:

kexp(fq, fk) = exp(
⟨fqWq, fkWk⟩√

dk
) . (3)

Note that the usage of asymmetric kernel is
also commonly used in various machine learning
tasks (Yilmaz, 2007; Tsuda, 1999; Kulis et al.,
2011), where they observed the kernel form can
be flexible and even non-valid (i.e., a kernel that is
not symmetric and positive semi-definite). In Sec-
tion 3, we show that symmetric design of the ker-
nel has similar performance for various sequence
learning tasks, and we also examine different ker-
nel choices (i.e., linear, polynomial, and rbf ker-
nel).

Kernel construction on X = (F × T ). The de-
signs for integrating the positional embedding tq
and tk are listed in the following.
(i) Absolute Positional Embedding (Vaswani et al.,
2017; Dai et al., 2019; Ott et al., 2019): For
the original Transformer (Vaswani et al., 2017),
each ti is represented by a vector with each di-
mension being sine or cosine functions. For
learned positional embedding (Dai et al., 2019; Ott
et al., 2019), each ti is a learned parameter and is
fixed for the same position for different sequences.
These works defines the feature space as the di-
rect sum of its temporal and non-temporal space:
X = F⊕T . Via the lens of kernel, the kernel
similarity is defined as

k(xq, xk) ∶= kexp(fq + tq, fk + tk). (4)

(ii) Relative Positional Embedding in
Transformer-XL (Dai et al., 2019): t repre-
sents the indicator of the position in the sequence,
and the kernel is chosen to be asymmetric of
mixing sine and cosine functions:

k(xq, xk) ∶= kexp(fq, fk) ⋅ kfq(tq, tk) (5)



4348

with kfq(tq, tk) being an asymmetric kernel with

coefficients inferred by fq: log kfq(tq, tk) =

∑⌊dk/2⌋−1p=0 c2p sin(
tq−tk

10000
2p
512

)+c2p+1 cos( tq−tk
10000

2p
512

)
with [c0,⋯, cdk−1] = fqWqWR where WR is an
learned weight matrix. We refer readers to Dai
et al. (2019) for more details.
(iii) Relative Positional Embedding of Shaw et al.
(2018) and Music Transformer (Huang et al.,
2018b): t⋅ represents the indicator of the position
in the sequence, and the kernel is modified to be
indexed by a look-up table:

k(xq, xk) ∶= Ltq−tk,fq ⋅ kexp(fq, fk), (6)

where Ltq−tk,fq = exp(fqWqatq−tk) with a⋅ being
a learnable matrix having matrix width to be the
length of the sequence. We refer readers to Shaw
et al. (2018) for more details.

Dai et al. (2019) showed that the way to inte-
grate positional embedding is better through Eq.
(5) than through Eq. (6) and is better through Eq.
(6) than through Eq. (4). We argue the reason
is that if viewing fi and ti as two distinct spaces
(X ∶= (F × T )), the direct sum xi = fi + ti may
not be optimal when considering the kernel score
between xq and xk. In contrast, Eq. (5) represents
the kernel as a product of two kernels (one for fi
and another for ti), which is able to capture the
similarities for both temporal and non-temporal
components.

2.2.3 Value Function v(⋅)
The current Transformers consider two different
value function construction:
(i) Original Transformer (Vaswani et al., 2017)
and Sparse Transformer (Child et al., 2019):

v(xk) = v((fk, tk)) ∶= (fk + tk)Wv. (7)

(ii) Transformer-XL (Dai et al., 2019), Music
Transformer (Huang et al., 2018b), Self-Attention
with Relative Positional Embedding (Shaw et al.,
2018):

v(xk) = v((fk, tk)) ∶= fkWv. (8)
Compared Eq. (7) to Eq. (8), Eq. (7) takes

the positional embedding into account for con-
structing the value function. In Section 3, we em-
pirically observe that constructing value function
with Eq. (8) constantly outperforms the construc-
tion with Eq. (7), which suggests that we do not
need positional embedding for value function.

2.2.4 Set Filtering Function M(⋅, ⋅)
In Eq. (2), the returned set by the set filtering
function M(xq, Sxk) defines how many keys and
which keys are operating with xq. In the follow-
ing, we itemize the corresponding designs for the
variants in Transformers:
(i) Encoder Self-Attention in original Trans-
former (Vaswani et al., 2017): For each query xq
in the encoded sequence, M(xq, Sxk) = Sxk con-
tains the keys being all the tokens in the encoded
sequence. Note that encoder self-attention consid-
ers xq = xk with xq being the encoded sequence.
(ii) Encoder-Decoder Attention in original Trans-
former (Vaswani et al., 2017): For each query xq
in decoded sequence, M(xq, Sxk) = Sxk contains
the keys being all the tokens in the encoded se-
quence. Note that encode-decoder attention con-
siders xq ≠ xk with xq being the decoded se-
quence and xk being the encoded sequence.

(iii) Decoder Self-Attention in original Trans-
former (Vaswani et al., 2017): For each query
xq in the decoded sequence, M(xq, Sxk) returns
a subset of Sxk (M(xq, Sxk) ⊂ Sxk ). Note that
decoder self-attention considers xq = xk with xq
being the decoded sequence. Since the decoded
sequence is the output for previous timestep, the
query at position i can only observe the keys being
the tokens that are decoded with position < i. For
convenience, let us define S1 as the set returned by
original Transformer (Vaswani et al., 2017) from
M(xq, Sxk), which we will use it later.
(iv) Decoder Self-Attention in Transformer-
XL (Dai et al., 2019): For each query xq in the de-
coded sequence,M(xq, Sxk) returns a set contain-
ing S1 and additional memories (M(xq, Sxk) =
S1 + Smem,M(xq, Sxk) ⊃ S1). Smem refers to
additional memories.

(v) Decoder Self-Attention in Sparse Trans-
former (Child et al., 2019): For each query xq in
the decoded sentence, M(xq, Sxk) returns a sub-
set of S1 (M(xq, Sxk) ⊂ S1).

To compare the differences for various designs,
we see the computation time is inversely propor-
tional to the number of elements in M(xq, Sxk).
For performance-wise comparisons, Transformer-
XL (Dai et al., 2019) showed that, the addi-
tional memories inM(xq, Sxk) are able to capture
longer-term dependency than the original Trans-
former (Vaswani et al., 2017) and hence results
in better performance. Sparse Transformer (Child



4349

et al., 2019) showed that although having much
fewer elements in M(xq, Sxk), if the elements are
carefully chosen, the attention can still reach the
same performance as Transformer-XL (Dai et al.,
2019).

2.3 Exploring the Design of Attention

So far, we see how Eq. (2) connects to the vari-
ants of Transformers. By changing the kernel con-
struction in Section 2.2.2, we can define a larger
space for composing attention. In this paper, we
present a new form of attention with a kernel that
is 1) valid (i.e., a kernel that is symmetric and pos-
itive semi-definite) and 2) delicate in the sense of
constructing a kernel on a joint space (i.e., X =
(F × T )):

k(xq, xk) ∶= kF (fq, fk) ⋅ kT (tq, tk)

with kF (fq, fk) = exp(
⟨fqWF , fkWF ⟩√

dk
)

and kT (tq, tk) = exp(
⟨tqWT , tkWT ⟩√

dk
),

(9)

where WF and WT are weight matrices. The new
form considers product of kernels with the first
kernel measuring similarity between non-temporal
features and the second kernel measuring similar-
ity between temporal features. Both kernels are
symmetric exponential kernel. Note that ti here
is chosen as the mixture of sine and cosine func-
tions as in the prior work (Vaswani et al., 2017; Ott
et al., 2019). In our experiment, we find it reaching
competitive performance as comparing to the cur-
rent state-of-the-art designs (Eq. (5) by Dai et al.
(2019)). We fix the size of the weight matrices W⋅
in Eq. (9) and Eq. (5) which means we save 33%
of the parameters in attention from Eq. (9) to Eq.
(5) (Eq. (5) has weightsWQ/WK/WR and Eq. (9)
has weights WF /WT ).

3 Experiments

By viewing the attention mechanism with Eq. (2),
we aims at answering the following questions re-
garding the Transformer’s designs:

Q1. What is the suggested way for incorporating
positional embedding in the kernel function?

Q2. What forms of kernel are recommended to
choose in the attention mechanism? Can we re-
place the asymmetric kernel with the symmetric
version?

Q3. Is there any exception that the attention mech-
anism is not order-agnostic with respect to inputs?
If so, can we downplay the role of positional em-
bedding?

Q4. Is positional embedding required in value
function?

We conduct experiments on neural machine
translation (NMT) and sequence prediction (SP)
tasks since these two tasks are commonly cho-
sen for studying Transformers (Vaswani et al.,
2017; Dai et al., 2019). Note that NMT has
three different types of attentions (e.g., encoder
self-attention, decoder-encoder attention, decoder
self-attention) and SP has only one type of atten-
tion (e.g., decoder self-attention). For the choice
of datasets, we pick IWSLT’14 German-English
(De-En) dataset (Edunov et al., 2017) for NMT
and WikiText-103 dataset (Merity et al., 2016) for
SP as suggested by Edunov et al. (Edunov et al.,
2017) and Dai et al. (Dai et al., 2019). For fair-
ness of comparisons, we train five random initial-
izations and report test accuracy with the highest
validation score. We fix the position-wise opera-
tions in Transformer3 and only change the atten-
tion mechanism. Similar to prior work (Vaswani
et al., 2017; Dai et al., 2019), we report BLEU
score for NMT and perplexity for SP.

3.1 Incorporating Positional Embedding

In order to find the best way to integrate positional
embedding (PE), we study different PE incorpora-
tion in the kernel function k(⋅, ⋅) in Eq. (2). Refer-
ring to Sections 2.2.2 and 2.3, we consider four
cases: 1) PE as direct sum in the feature space
(see Eq. (4)), 2) PE as a look-up table (see Eq.
(6)), 3) PE in product kernel with asymmetric ker-
nel (see Eq. (5)), and 4) PE in product kernel with
symmetric kernel (see Eq. (9)). We present the
results in Table 1.

First, we see that by having PE as a look-up
table, it outperforms the case with having PE as
direct-sum in feature space, especially for SP task.
Note that the look-up table is indexed by the rela-
tive position (i.e., tq − tk) instead of absolute posi-
tion. Second, we see that PE in the product kernel
proposed by Dai et al. (Dai et al., 2019) may not

3The computation of Transformer can be categorized into
position-wise and inter-positions (i.e., the attention mecha-
nism) operations. Position-wise operations include layer nor-
malization, residual connection, and feed-forward mapping.
We refer the readers to Vaswani et al. (Vaswani et al., 2017)
for more details.



4350

Table 1: Incorporating Positional Embedding (PE). NMT stands for neural machine translation on IWSLT’14 De-
En dataset (Edunov et al., 2017) and SP stands for sequence prediction on WikiText-103 dataset (Merity et al.,
2016). ↑ means the upper the better and ↓ means the lower the better.

Approach PE Incorporation Kernel Form NMT (BLEU↑) SP (Perplexity↓)

Vaswani et al. (2017) (Eq. (4)) Direct-Sum kexp(fq + tq, fk + tk) 33.98 30.97

Shaw et al. (2018) (Eq. (6)) Look-up Table Ltq−tk,fq ⋅ kexp(fq, fk) 34.12 27.56

Dai et al. (2019) (Eq. (5)) Product Kernel kexp(fq, fk) ⋅ kfq(tq, tk) 33.62 24.10

Ours (Eq. (9)) Product Kernel kF (fq, fk) ⋅ kT (tq, tk) 34.71 24.28

Table 2: Kernel Types. Other than manipulating the kernel choice of the non-positional features, we fix the
configuration by Vaswani et al. (2017) for NMT and the configuration by Dai et al. (2019) for SP.

Type Kernel Form
NMT (BLEU↑) SP (Perplexity↓)

Asym. (Wq ≠Wk) Sym. (Wq =Wk) Asym. (Wq ≠Wk) Sym. (Wq =Wk)

Linear ⟨faWq, fbWk⟩ not converge not converge not converge not converge

Polynomial (⟨faWq, fbWk⟩)
2

32.72 32.43 25.91 26.25

Exponential exp( ⟨faWq ,fbWk⟩√
dk

) 33.98 33.78 24.10 24.01

RBF exp( − ∥faWq−fbWk∥
2

√
dk

) 34.26 34.14 24.13 24.21

constantly outperform the other integration types
(it has lower BLEU score for NMT). Our proposed
product kernel reaches the best result in NMT and
is competitive to the best result in SP.

3.2 Kernel Types
To find the best kernel form in the attention mecha-
nism, in addition to the exponential kernel (see Eq.
(3)), we compare different kernel forms (i.e., lin-
ear, polynomial, and rbf kernel) for the non-
positional features. We also provide the results
for changing asymmetric to the symmetric kernel,
when forcing Wq = Wk, so that the resulting ker-
nel is a valid kernel (Scholkopf and Smola, 2001).
The numbers are shown in Table 2. Note that, for
fairness, other than manipulating the kernel choice
of the non-positional features, we fix the config-
uration by Vaswani et al. (Vaswani et al., 2017)
for NMT and the configuration by Dai et al. (Dai
et al., 2019) for SP.

We first observe that the linear kernel does not
converge for both NMT and SP. We argue the rea-
son is that the linear kernel may have negative
value and thus it violates the assumption in ker-
nel smoother that the kernel score must be pos-
itive (Wasserman, 2006). Next, we observe the
kernel with infinite feature space (i.e., exponen-
tial and rbf kernel) outperforms the kernel with fi-

nite feature space (i.e., polynomial kernel). And
we see rbf kernel performs the best for NMT and
exponential kernel performs the best for SP. We
conclude that the choice of kernel matters for the
design of attention in Transformer. Also, we see
no much performance difference when comparing
asymmetric to symmetric kernel. In the experi-
ment, we fix the size of W⋅ in the kernel, and thus
adopting the symmetric kernel benefits us from
saving parameters.

3.3 Order-Invariance in Attention
The need of the positional embedding (PE) in the
attention mechanism is based on the argument that
the attention mechanism is an order-agnostic (or,
permutation equivariant) operation (Vaswani et al.,
2017; Shaw et al., 2018; Huang et al., 2018b;
Dai et al., 2019; Child et al., 2019). However,
we show that, for decoder self-attention, the op-
eration is not order-agnostic. For clarification,
we are not attacking the claim made by the prior
work (Vaswani et al., 2017; Shaw et al., 2018;
Huang et al., 2018b; Dai et al., 2019; Child et al.,
2019), but we aim at providing a new look at the
order-invariance problem when considering the at-
tention mechanism with masks (masks refer to the
set filtering function in our kernel formulation). In
other words, previous work did not consider the



4351

Table 3: Order-Invariance in Attention. To save the space, we denote Encoder Self-Attention / Encoder-Decoder
Attention / Decoder Self-Attention as A/B/C. Note that SP only has decoder self-attention.

Approach Positional Embedding NMT (BLEU↑)

Ours (Eq. (9)) In A/B/C 34.71

Ours (Eq. (9)) In A/B 34.49

No Positional Embedding none 14.47

Approach Positional Embedding SP (Perplexity↓)

Vaswani et al. (2017) (Eq.
(4))

In C 30.97

Ours (Eq. (9) In C 24.28

No Positional Embedding none 30.92

Table 4: Positional Embedding in Value Function.

I: Value Function Considering Positional Embedding (Eq. (7)) / II: Value Function Considering no Positional Embedding (Eq. (8))

Approach
NMT (BLEU↑) SP (Perplexity↓)

I (v(xk) ∶= (fk + tk)WV ) II (v(xk) ∶= fkWV ) I (v(xk) ∶= (fk + tk)WV ) II (v(xk) ∶= fkWV )

Vaswani et al. (2017) (Eq. (4)) 33.98 34.02 30.97 30.50

Shaw et al. (2018) (Eq. (6)) 34.04 34.12 27.56 27.45

Dai et al. (2019) (Eq. (5)) 33.32 33.62 24.18 24.10

Ours (Eq. (9)) 34.60 34.71 24.42 24.28

mask between queries and keys when discussing
the order-invariance problem (Pérez et al., 2019).

To put it formally, we first present the definition
by Lee et al. (2018) for a permutation equivariance
function:

Definition 2. Denote Π as the set of all permu-
tations over [n] = {1,⋯, n}. A function func ∶
X n → Yn is permutation equivariant iff for any
permutation π ∈ Π, func(πx) = πfunc(x).

Lee et al. (2018) showed that the standard atten-
tion (encoder self-attention (Vaswani et al., 2017;
Dai et al., 2019) ) is permutation equivariant.
Here, we present the non-permutation-equivariant
problem on the decoder self-attention:

Proposition 1. Decoder self-attention (Vaswani
et al., 2017; Dai et al., 2019) is not permutation
equivariant.

To proceed the proof, we need the following
definition and propositions.

Definition 3. Denote Π as the set of all permuta-
tions over [n] = {1,⋯, n} and Sπxk as performing
permutation π over Sxk . Attention(xq;Sxk) is
said to be permutation equivariant w.r.t. Sxk if
and only if for any π ∈ Π, Attention(xq;Sπxk) =
Attention(xq;Sxk).
Proposition 2. Attention with the set filtering
function M(xq, Sxk) = Sxk is permutation equiv-
ariant w.r.t. Sxk .

Proof. It is easy to show that if M(xq, Sxk) =
Sxk , Eq. (2) remains unchanged for any permu-
tation π performed on Sxk . ∎

Proposition 3. Attention with the set difference
Sxk ∖M(xq, Sxk) ≠ φ is not permutation equiv-
ariant w.r.t. Sxk .

Proof. First, suppose that x̂ ∈ Sxk ∖M(xq, Sxk).
Then, we construct a permutation π such that
x̂ ∈ M(xq, Sπxk). It is obvious that Eq.
(2) changes after this permutation and thus
Attention(xq ; M(xq, Sxk)) is not permutation
equivariant w.r.t. Sxk . ∎

Proof. [Proof for Proposition 1] First, we have
xq ∼ Sxk . Hence, showing Attention(xq;Sxk)
not permutation equivariant w.r.t. Sxk equals to
showing Attention not permutation equivariant.
Then, since the decoder self-attention considers
masking (i.e., M(xq, Sxk) returns a subset of
Sxk ), by Proposition 3, the decoder self-attention
is not permutation equivariant. ∎

In fact, not only being a permutation inequivari-
ant process, the decoding process in the decoder
self-attention already implies the order informa-
tion from the data. To show this, take the decoded
sequence y = [init, y1, y2, y3, y4] as an example.
init stands for the initial token. When determin-
ing the output y1 from init, the set filtering func-
tion is M(init, Sy) = {init}. Similarly, we will
have M(y1, Sy),M(y2, Sy),M(y3, Sy) to be
{init, y1},{init, y1, y2},{init, y1, y2, y3}. Then,
it raises a concern: do we require PE in decoder
self-attention? By removing PE in decoder self-
attention, we present the results in Table 3. From
the table, we can see that, for NMT, removing PE



4352

only in decoder self-attention results in slight per-
formance drop (from 34.71 to 34.49). However,
removing PE in the entire model greatly degrades
the performance (from 34.71 to 14.47). On the
other hand, for SP, removing PE from our pro-
posed attention variant dramatically degrades the
performance (from 24.28 to 30.92). Nonetheless,
the performance is slightly better than considering
PE from the original Transformer (Vaswani et al.,
2017).

3.4 Positional Embedding in Value Function

To determine the need of positional embedding
(PE) in value function, we conduct the experi-
ments by adopting Eq. (7) or Eq. (8) in the at-
tention mechanism. The results are presented in
Table 4. From the table, we find that considering
PE in value function (Eq. (7)) does not gain per-
formance as compared to not considering PE in
value function (Eq. (8)).

3.5 Take-Home Messages

Based on the results and discussions, we can now
answer the questions given at the beginning of
this section. The answers are summarized into the
take-home messages in the following.

A1. We show that integrating the positional em-
bedding in the form of product kernel (Eq. (5)
or Eq. (9)) gives us best performance.

A2. The kernel form does matter. Adopting ker-
nel form with infinite feature dimension (i.e., ex-
ponential kernel or rbf kernel) gives us best results.
The symmetric design of the kernel may benefit
us from saving parameters and barely sacrifice the
performance as compared to the non-symmetric
one.

A3. The decoder self-attention is not an order-
agnostic operation with respect to the order of in-
puts. However, incorporating positional embed-
ding into the attention mechanism may still im-
prove performance.

A4. We find that there is no much performance
difference by considering or not considering the
positional embedding in value function.

4 Related Work

Other than relating Transformer’s attention mech-
anism with kernel methods, the prior work (Wang
et al., 2018; Shaw et al., 2018; Tsai et al.,
2019b) related the attention mechanism with

graph-structured learning. For example, Non-
Local Neural Networks (Wang et al., 2018) made
a connection between the attention and the non-
local operation in image processing (Buades et al.,
2005). Others (Shaw et al., 2018; Tsai et al.,
2019b) linked the attention to the message passing
in graphical models. In addition to the fundamen-
tal difference between graph-structured learning
and kernel learning, the prior work (Wang et al.,
2018; Shaw et al., 2018; Tsai et al., 2019b) fo-
cused on presenting Transformer for its particular
application (e.g., video classification (Wang et al.,
2018) and neural machine translation (Shaw et al.,
2018)). Alternatively, our work focuses on pre-
senting a new formulation of Transformer’s atten-
tion mechanism that gains us the possibility for
understanding the attention mechanism better.

5 Conclusions

In this paper, we presented a kernel formulation
for the attention mechanism in Transformer, which
allows us to define a larger space for designing at-
tention. As an example, we proposed a new vari-
ant of attention which reaches competitive perfor-
mance when compared to previous state-of-the-art
models. Via the lens of the kernel, we were able
to better understand the role of individual com-
ponents in Transformer’s attention and categorize
previous attention variants in a unified formula-
tion. Among these components, we found the con-
struction of the kernel function acts the most im-
portant role, and we studied different kernel forms
and the ways to integrate positional embedding on
neural machine translation and sequence predic-
tion. We hope our empirical study may potentially
allow others to design better attention mechanisms
given their particular applications.

Acknowledgments

We thank Zhilin Yang for helpful discussion on the
positional encoding in Transformer’s Attention.
This work was supported in part by the DARPA
grant FA875018C0150, Office of Naval Research
grant N000141812861, AFRL CogDeCON, NSF
Awards #1734868 #1722822, National Institutes
of Health, JST PRESTO program JPMJPR165A,
and Apple. We would also like to acknowledge
NVIDIA’s GPU support.



4353

References
Shaojie Bai, J Zico Kolter, and Vladlen Koltun.

2018. An empirical evaluation of generic convolu-
tional and recurrent networks for sequence model-
ing. arXiv preprint arXiv:1803.01271.

Antoni Buades, Bartomeu Coll, and J-M Morel. 2005.
A non-local algorithm for image denoising. In
2005 IEEE Computer Society Conference on Com-
puter Vision and Pattern Recognition (CVPR’05),
volume 2, pages 60–65. IEEE.

Rewon Child, Scott Gray, Alec Radford, and
Ilya Sutskever. 2019. Generating long se-
quences with sparse transformers. arXiv preprint
arXiv:1904.10509.

Zihang Dai, Zhilin Yang, Yiming Yang, William W
Cohen, Jaime Carbonell, Quoc V Le, and Ruslan
Salakhutdinov. 2019. Transformer-xl: Attentive lan-
guage models beyond a fixed-length context. arXiv
preprint arXiv:1901.02860.

Jacob Devlin, Ming-Wei Chang, Kenton Lee, and
Kristina Toutanova. 2018. Bert: Pre-training of deep
bidirectional transformers for language understand-
ing. arXiv preprint arXiv:1810.04805.

Sergey Edunov, Myle Ott, Michael Auli, David Grang-
ier, and Marc’Aurelio Ranzato. 2017. Classical
structured prediction losses for sequence to se-
quence learning. arXiv preprint arXiv:1711.04956.

Cheng-Zhi Anna Huang, Ashish Vaswani, Jakob
Uszkoreit, Noam Shazeer, Curtis Hawthorne, An-
drew M Dai, Matthew D Hoffman, and Douglas Eck.
2018a. An improved relative self-attention mecha-
nism for transformer with application to music gen-
eration. arXiv preprint arXiv:1809.04281.

Cheng-Zhi Anna Huang, Ashish Vaswani, Jakob
Uszkoreit, Ian Simon, Curtis Hawthorne, Noam
Shazeer, Andrew M Dai, Matthew D Hoffman,
Monica Dinculescu, and Douglas Eck. 2018b. Mu-
sic transformer: Generating music with long-term
structure.

Brian Kulis, Kate Saenko, and Trevor Darrell. 2011.
What you saw is not what you get: Domain adapta-
tion using asymmetric kernel transforms. In CVPR
2011, pages 1785–1792. IEEE.

Juho Lee, Yoonho Lee, Jungtaek Kim, Adam R Ko-
siorek, Seungjin Choi, and Yee Whye Teh. 2018. Set
transformer. arXiv preprint arXiv:1810.00825.

Chun-Liang Li, Wei-Cheng Chang, Yu Cheng, Yim-
ing Yang, and Barnabás Póczos. 2017. Mmd gan:
Towards deeper understanding of moment matching
network. In Advances in Neural Information Pro-
cessing Systems, pages 2203–2213.

Stephen Merity, Caiming Xiong, James Bradbury, and
Richard Socher. 2016. Pointer sentinel mixture
models. arXiv preprint arXiv:1609.07843.

Myle Ott, Sergey Edunov, Alexei Baevski, Angela
Fan, Sam Gross, Nathan Ng, David Grangier, and
Michael Auli. 2019. fairseq: A fast, extensible
toolkit for sequence modeling. In Proceedings of
NAACL-HLT 2019: Demonstrations.

Niki Parmar, Ashish Vaswani, Jakob Uszkoreit, Łukasz
Kaiser, Noam Shazeer, Alexander Ku, and Dustin
Tran. 2018. Image transformer. arXiv preprint
arXiv:1802.05751.

Jorge Pérez, Javier Marinković, and Pablo Barceló.
2019. On the turing completeness of mod-
ern neural network architectures. arXiv preprint
arXiv:1901.03429.

Bernhard Scholkopf and Alexander J Smola. 2001.
Learning with kernels: support vector machines,
regularization, optimization, and beyond. MIT
press.

Peter Shaw, Jakob Uszkoreit, and Ashish Vaswani.
2018. Self-attention with relative position represen-
tations. arXiv preprint arXiv:1803.02155.

Ilya Sutskever, Oriol Vinyals, and Quoc V Le. 2014.
Sequence to sequence learning with neural net-
works. In Advances in neural information process-
ing systems, pages 3104–3112.

Yao-Hung Hubert Tsai, Shaojie Bai, Paul Pu Liang,
Louis-Philippe Morency, and Ruslan Salakhutdinov.
2019a. Multimodal transformer for unaligned mul-
timodal language sequences. ACL.

Yao-Hung Hubert Tsai, Santosh Divvala, Louis-
Philippe Morency, Ruslan Salakhutdinov, and Ali
Farhadi. 2019b. Video relationship reasoning using
gated spatio-temporal energy graph. CVPR.

Koji Tsuda. 1999. Support vector classifier with asym-
metric kernel functions. In in European Symposium
on Artificial Neural Networks (ESANN. Citeseer.

Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob
Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz
Kaiser, and Illia Polosukhin. 2017. Attention is all
you need. In Advances in Neural Information Pro-
cessing Systems, pages 5998–6008.

Xiaolong Wang, Ross Girshick, Abhinav Gupta, and
Kaiming He. 2018. Non-local neural networks. In
Proceedings of the IEEE Conference on Computer
Vision and Pattern Recognition, pages 7794–7803.

Larry Wasserman. 2006. All of nonparametric statis-
tics. Springer Science & Business Media.

Andrew Gordon Wilson, Zhiting Hu, Ruslan Salakhut-
dinov, and Eric P Xing. 2016. Deep kernel learning.
In Artificial Intelligence and Statistics, pages 370–
378.

Alper Yilmaz. 2007. Object tracking by asymmetric
kernel mean shift with automatic scale and orienta-
tion selection. In 2007 IEEE Conference on Com-
puter Vision and Pattern Recognition, pages 1–6.
IEEE.


