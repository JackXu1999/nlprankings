



















































PD3: Better Low-Resource Cross-Lingual Transfer By Combining Direct Transfer and Annotation Projection


Proceedings of the 5th Workshop on Argument Mining, pages 131–143
Brussels, Belgium, November 1, 2018. c©2018 Association for Computational Linguistics

131

PD3: Better Low-Resource Cross-Lingual Transfer
By Combining Direct Transfer and Annotation Projection

Steffen Eger, Andreas Rücklé, Iryna Gurevych
Ubiquitous Knowledge Processing Lab (UKP-TUDA)

Department of Computer Science
Technische Universität Darmstadt
www.ukp.tu-darmstadt.de

Abstract

We consider unsupervised cross-lingual trans-
fer on two tasks, viz., sentence-level argumen-
tation mining and standard POS tagging. We
combine direct transfer using bilingual em-
beddings with annotation projection, which
projects labels across unlabeled parallel data.
We do so by either merging respective source
and target language datasets or alternatively by
using multi-task learning. Our combination
strategy considerably improves upon both di-
rect transfer and projection with few available
parallel sentences, the most realistic scenario
for many low-resource target languages.

1 Introduction

In recent years, interest in multi- and cross-lingual
natural language processing (NLP) has steadily in-
creased. This has not only to do with the recogni-
tion that performances of newly introduced systems
should be robust across several tasks (in several lan-
guages), but more fundamentally with the idea of
truly ‘universal’ NLP methods which should not
only suit English, an arguably particularly simple
exemplar of the world’s roughly 7,000 languages.

A further motivation for cross-lingual ap-
proaches is the fact that many labeled datasets are
to this date only available in English and labeled
data is generally costly to obtain—be it via expert
annotators or through crowd-sourcing. Therefore,
methods which are capable of training on labeled
data in a resource-rich language such as English
and which can then be applied to typically resource-
poor other languages are highly desirable.

Two standard cross-lingual approaches are pro-
jection (Yarowsky et al., 2001; Das and Petrov,
2011; Täckström et al., 2013; Agic et al., 2016)
and direct transfer (McDonald et al., 2011). Di-
rect transfer trains, in the source language L1, on
language-independent or shared features and then

directly applies the trained system to the target lan-
guage of interest L2. In contrast, projection trains
and evaluates on L2 itself. To do so, it uses par-
allel data, applies a system trained on L1 to its
source side and then projects the inferred labels
to the parallel L2 side. This projection step may
involve word alignment information. After projec-
tion, an annotated L2 dataset is available on which
L2 systems can be trained.

Projection and direct transfer each ignore impor-
tant information, however. For example, standard
projection ignores the available data in L1 once
the L2 dataset has been created and standard direct
transfer does not use any L2 information.

In this work, we investigate whether the inclu-
sion of both L1 and L2 data outperforms transfer
approaches that exploit only one type of such in-
formation, and if so, under what conditions. More
precisely, we first train a system on shared fea-
tures as in standard direct transfer on labeled L1
data. Then, we make use of two further datasets.
One is based on the source side of parallel unla-
beled data; it is derived similarly as in self-training
(Yarowsky, 1995) by applying the trained system
to unlabeled data, from which a pseudo-labeled
dataset is derived. The other is based on its target
side—using annotation projections—as in standard
projection. Thus, we explore the effects of com-
bining Projection and Direct transfer using three
datasets (PD3). Our approach is detailed in §2.

We report results for two L2 languages (French,
German) on one sentence-level problem (argumen-
tation mining) and one token-level problem (POS
tagging). We find that our suggested approach PD3
substantially outperforms both direct transfer and
projection when little parallel data is available, the
most realistic scenario for many L2 languages.

While our approach is general, our focus is
particularly on argumentation mining (ArgMin),
a rapidly growing research field in NLP. Cross-



132

lingual transfer is majorly important for ArgMin
because it is inherently costly to get high-quality
annotations for ArgMin due to: (i) subjectivity of
argumentation as well as divergent and competing
ArgMin theories (Daxenberger et al., 2017; Schulz
et al., 2018), leading to disagreement among crowd-
workers as well as expert annotators (Habernal and
Gurevych, 2017), (ii) dependence of argument an-
notations on background knowledge and parsing of
complex pragmatic relations (Moens, 2017). Thus,
in order not to reproduce the same annotation costs
for new languages, cross-lingual ArgMin meth-
ods are required. These techniques should both
perform well with little available parallel data, to
address many languages, and with general (non-
argumentative) parallel data, because this is much
more likely to be available. Our experiments ad-
dress both of these requirements.1

2 PD3

LetLS = {(xS , yS)} denote a set of L1 data points
in which each xS is an instance and yS its corre-
sponding label. We assume that xS is either a sen-
tence or a sequence of tokens, and yS is either a
single label or contains one label for each token in
the sequence. We assume access to a set US,T =
{(xS , xT )} of unlabeled L1 and L2 data points in
which target and source instances xT and xS are
translations of each other. We let US = US,TS stand
for the L1 part of US,T , i.e., US consists of the data
points xS only: US = {xS | (xS , xT ) ∈ US,T }.
We let UT = US,TT be analogously defined. Finally,
we assume that our instances xS and xT have a
shared representation, e.g., that their words have
a bilingual vector space representation in which
mono- and cross-lingually similar words are close
to each other. Table 1 (a),(b) illustrates our resource
assumptions.

PD3 is described in Algorithm 1. We first train a
classifier C (e.g., a neural network) on our labeled
L1 data LS . Then we apply the trained model
on the unlabeled xS instances from US , yielding
pseudo-labeled dataset D̂S . Next, we create an-
other pseudo-labeled L2 data set ˆ̂DT by projecting
the label ŷS of xS in a pair (xS , xT ) ∈ US,T to the
instance xT . We note that projection is trivial and
‘loss-less’ for sentence classification tasks because
there is exactly one label for the whole sentence.

1Data and code to reproduce our experiments are
available from https://github.com/UKPLab/
emnlp2018-argmin-workshop-pd3.

Algorithm 1: PD3
Input: LS ,US,T , C: labeled L1 data and unlabeled

L1-L2 translations, and a classifier C

Output: M
S~Ŝ~

ˆ̂
T

: a model trained (using C) on
LS as well as pseudo-labeled data derived
from US,T

1 MS ← trainC(LS);
2 ŶS ← predictMS (US) ; // D̂S = {(xS , ŷS)}
3 ˆ̂DT ← {(xT , ˆ̂yS) | (xS , xT ) ∈
US,T , (xS , ŷS) ∈ D̂S};

4 M
S~Ŝ~

ˆ̂
T
← trainC(LS ~ D̂S ~ ˆ̂DT );

In contrast, for sequence tagging problems, projec-
tion typically requires word alignment information,
which is an error prone process. This is the rea-
son why we use a ‘double hat’ for ˆ̂DT to indicate
that there may be two sources of noise: one from
prediction and one from projection.

Finally, we combine our original dataset LS with
the two pseudo-labeled dataset D̂S and ˆ̂DT and
train our classifier C on it; after training, our goal
in cross-lingual transfer is to apply the trained clas-
sifiers to L2 data.

We denote this combination operation by ~. A
simple approach is to let ~ be the “merging” (or,
concatenation) of both datasets (PD3-merge). In
this variant of PD3, LS , D̂S and ˆ̂DT are merged
into one big dataset on which training takes place.

A more sophisticated approach is to let ~ repre-
sent a multi-task learning (MTL) scenario (Caru-
ana, 1993; Søgaard and Goldberg, 2016) in which
L1 and L2 instances represent one task each (PD3-
MTL). Here, rather than merging LS , D̂S and ˆ̂DT ,
we treat source language datasets (LS and D̂S) as
one task and target language datasets ( ˆ̂DT ) as an-
other task, each having a dedicated output layer.
This leads to a different network architecture than
in PD3-merge, in which we now have two sep-
arate output layers (i.e., one for each language);
this distinction is also illustrated in Figure 1 below.
Thus, for each input instance, we predict two out-
puts (e.g., two ArgMin labels), one in the source
language and one in the target language.2

The general idea behind MTL is to learn several

2During training, we update parameters for the ‘correct’
task as well as for all shared weights. At test time, we only
pick the output corresponding to the target language task, if
we focus on cross-lingual transfer, or corresponding to the
source language, if we focus on in-language evaluation.

https://github.com/UKPLab/emnlp2018-argmin-workshop-pd3
https://github.com/UKPLab/emnlp2018-argmin-workshop-pd3


133

Not cooking [...] 1
To sum up [...] 0
For example [...] 2
I will [...] 3
...

...

(a) Labeled L1 data LS

He said [...] Er sagte [...]
A blue [...] Ein blauer [...]
Very good! Sehr gut!
How [...] Wie [...]
...

...

(b) Unlabeled parallel data US,T

LS D̂S ˆ̂DT
Direct Transfer 3
Projection 3
PD3 3 3 3
· · ·

(c) Resources used by approaches

Table 1: Illustration of resources used for PD3: (a) labeled source language data; (b) unlabeled parallel
data; (c) comparison with Direct Transfer and annotation projection. Arrows indicate the information
flow: we use LS to label the source side of parallel data and then project to its target side. Note that both
variants of PD3 (PD3-merge and PD3-MTL) use the same resources but utilize/combine them differently,
as described in the text.

tasks jointly, in one architecture with shared pa-
rameters, so that generalized representations can
be learned (in the hidden layers of a neural net-
work) that benefit multiple tasks. In our case, the
two tasks solve the same problem (e.g., ArgMin),
but in different languages. A general advantage of
MTL over merging arises when tasks have different
output spaces, in which case merging may confuse
a learner due to heterogeneous labels across the
two tasks. We do not face this situation. However,
in our context, an advantage of MTL over merge
may still be that the MTL paradigm has more ca-
pacity because it has connecting weights between
the task-specific output layers and the network’s
last (common) hidden layer. Further, MTL can ac-
commodate task-specific losses, which can be used
to, e.g., down-weight one of the two tasks, besides
further conceptual differences (Caruana, 1993). In
our situation, splitting original and pseudo-labeled
datasets by languages, in MTL, may also better
account for syntactic and semantic idiosyncrasies
of individual languages than merge, where such
distinctions are blurred.

Table 1 (c) compares the different resource as-
sumptions of direct transfer, annotation projection,
and PD3. Note that other selections of resources
might be possible (e.g., ‘PD2’, using only LS and
ˆ̂DT , or even differently annotated L2 data). We dis-
cuss some of these in the supplementary material.

3 Data

Table 2 gives dataset statistics for our two tasks,
which we describe in the following.

ArgMin Our focus task is ArgMin on the
sentence-level: the task is to determine whether
a sentence contains one of the argumentative con-

structs major claim, claim, premise, or else is non-
argumentative (Peldszus and Stede, 2013; Stab and
Gurevych, 2014). We use the latest version of an
English student essay corpus (Stab and Gurevych,
2017), which has recently also been translated to
German by student crowd-workers (Eger et al.,
2018). We give four examples from the English
ArgMin dataset in Table 3. The majority of all in-
stances is labeled as premise (47%). We use 3,000
sentences of the original training split as our par-
allel corpus and only train on the remaining 2,086
sentences (this is the set LS). We additionally eval-
uate our approaches with parallel data from TED
(Hermann and Blunsom, 2014), where we train on
the full 5,086 sentences from the ArgMin training
split. TED contains a collection of talks on science,
education, and related fields, transcribed into writ-
ten English and translated by crowd-workers into
different languages. We take two sources of par-
allel data here because the domain of the parallel
data intuitively has an influence on results in tasks
such as argumentation mining. That is, while stan-
dard NLP tasks such as POS tagging are relatively
stable across different domains, arguments may be
very differently realized across different datasets
(Daxenberger et al., 2017). Frequency aspects also
play a role, since argumentation may be prominent
in domains such as student essays or debate portal,
but much less ubiquitous in, e.g., news articles.

POS Tagging We also include a standard NLP
task, namely, POS tagging. We use subsets of
the Universal Dependency Treebanks (Nivre et al.,
2016) with English as L1 and German and French
as L2s. For English, we select 800 random sen-
tences from the corresponding English treebank
as training data and 200 sentences as development



134

Task Task type |Y| Train-EN Dev-EN Test-DE Test-FR
Sent. Tokens Sent. Tokens Sent. Tokens Sent. Tokens

POS Token-Level 18 800 13,292 200 3,174 799 12,512 1,478 35,766
AM Sentence-Level 4 5,086 105,990 607 12,658 1,448 29,234 - -

Table 2: Statistics for datasets used in this work. |Y| denotes the size of the label space.

data.3 We evaluate the system that transfers from
English to German or French on the original de-
velopment data provided in the corresponding tree-
bank splits. As our unlabeled parallel data, we
use subsets of various sizes from the TED parallel
corpus for English-French and English-German.

4 Experimental Setup

Sentence level network architecture: In our
sentence-level ArgMin experiments, we use a con-
volutional neural network (CNN) with 1-max pool-
ing to learn a representation of the input sentence
and feed this representation into a softmax regres-
sion classifier.4 We use 800 CNN filters with a
window size of 3. For optimization, we use Adam
with a learning rate of 0.001. Training sentences
are processed in minibatches of size 16. We do not
apply dropout or `2 regularization.

We report average macro F1 scores over 20 runs
with different random initializations. For PD3-
merge, we shuffle the merged data before training—
i.e., mini-batches can containLS , D̂S , and ˆ̂DT data.
For PD3-MTL, we shuffle L1 and L2 data individu-
ally and during training we sample each mini-batch
from either task according to its size. In the MTL
setup, we share the CNN layer across tasks and use
task-specific softmax regression layers.

Sequence tagging network architecture: For
token-level POS tagging, we implement a bidi-
rectional LSTM as in Ma and Hovy (2016) and
Lample et al. (2016) with a CRF output layer. This
is a state-of-the-art system for sequence tagging
tasks such as POS and NER. Our model uses pre-
trained word embeddings and optionally concate-
nates these with a learned character-level repre-
sentation. For all experiments, we use the same
network topology: we use two hidden layers with
100 hidden units each, applying dropout on the hid-
den units and on the word embeddings. We use

3We choose only 800 sentences in order to keep overall
computational costs of our experiments smaller. Note that 800
sentences yield an in-language performance of roughly 90%.

4An alternative would have been to directly work on
sentence-level representations using cross-lingual sentence
embeddings (Rücklé et al., 2018).

Adam as optimizer. Our network uses a CRF out-
put layer rather than a softmax classifier to account
for dependencies between successive labels.

In the MTL setup, we use the same architecture,
but connect the last hidden layer to individual out-
put layers, one for each task. Our MTL architecture
extends the architecture of Søgaard and Goldberg
(2016) by replacing the softmax output layer with a
CRF output layer, and by including character-level
word representations. The difference between MTL
and single-task learning (STL) is illustrated in Fig-
ure 1. STL is a network with only one task, as in
PD3-merge, direct transfer and standard projection.

We report average accuracy over five (or 10,
in case of very little data) random weight matrix
initializations. In the MTL setup, we choose a
mini-batch randomly in each iteration (contain-
ing instances from only one of the tasks as in our
sentence-level ArgMin experiments).

Cross-lingual Embeddings: For token-level ex-
periments, we initially train 100-d BIVCD em-
beddings (Vulić and Moens, 2015) from Europarl
(Koehn, 2005) (for EN-DE) and the UN corpus
(Ziemski et al., 2016) (for EN-FR), respectively.
For sentence-level experiments, we use 300-d
BIVCD embeddings. This means that we initially
assume that high-quality bilingual word embed-
dings are readily available for the two languages
involved. At first sight, this appears a realistic as-
sumption since high-quality bilingual embeddings
can already be obtained with very little available
bilingual data (Zhang et al., 2016; Artetxe et al.,
2017). In low-resource settings, however, even lit-
tle monolingual data is typically available for L2
and we address this setup subsequently.

Upper bound: For both ArgMin and POS, we
report the in-language upper bound, i.e., when the
model is trained and evaluated on L2. For this, we
choose random L2 train sets of size |LS |.

Projection strategy for sequence tagging: We
first word-align parallel data using fast-align (Dyer
et al., 2013). When an L2 word is uniquely aligned
to an L1 word, we assign it the L1 word’s unique



135

Not cooking fresh food will lead to lack of nutrition Claim
To sum up, [...] the merits of animal experiments still outweigh the demerits Major claim
For example, tourism makes up one third of Czech’s economy Premise
I will mention some basic reasoning as follows O

Table 3: Simplified examples (EN) from our AM corpus, one for each of the four classes.

h1

s1

w1

h2

s2

w2

h1

s1 t1

w1

h2

s2 t2

w2

Figure 1: Sequence tagging STL vs. MTL with two tasks. For readability, character-level representations
and CRF connections in the output layers are omitted. Bidirectional connections in the hidden layers are
also missing. Here, w are the input words and s and t denote different tasks; h are the hidden layers.

label. When an L2 word is aligned to several L1
words, we randomly draw one of the aligned source
labels. When an L2 word is not aligned to any L1
word, we draw a label randomly from its unique la-
bels in the remainder of the corpus. Our projection
strategy is standard, cf. Agic et al. (2016).

5 Experiments

5.1 Results

Detailed results for PD3-merge, PD3-MTL, stan-
dard projection, and direct transfer as a function
of the available parallel data are given in Table 4
in the appendix. Condensed and averaged results
(over DE and FR) are shown in Figure 2.

ArgMin Results are shown in Figure 2 (right),
ranging over {50, 100, 500, 1000, 2000, 3000} par-
allel sentences. PD3 is consistently more effective
than projection and outperforms direct transfer with
at least 100 parallel sentences. In particular, PD3-
merge outperforms direct transfer already with 50
parallel sentences (∼44% for PD3-merge vs.∼39%
for direct transfer) and quickly closes the gap
towards the in-language upper-bound (∼54% vs.
∼59% with 500 parallel sentences). PD3-MTL on
the other hand only slightly (but consistently) im-
proves upon projection. With an increased number
of parallel sentences, we observe that all methods
reach performances very close to the in-language
upper bound.

POS Tagging Figure 2 (left) shows POS results,
averaged across DE and FR, when transferring
from English. Tagging accuracies are given as a
function of the size of the available parallel data,
ranging over {50, 100, 500, 1000, 5000} parallel
sentences. As for ArgMin, PD3 is consistently
better than projection and improves upon direct
transfer with more than 50 parallel sentences. As
the number of parallel sentences increases, PD3-
MTL, PD3-merge and standard projection become
indistinguishable, indicating that it does not pay
out anymore to use the more resource-intensive
approach PD3. However, most importantly, with
little parallel data, gains of PD3 over standard pro-
jection are substantial: for 50 parallel sentences
performance values are roughly doubled (∼30%
accuracy for projection vs. >55% for PD3). For lit-
tle available parallel data, PD3-MTL can also con-
siderably improve upon PD3-merge. For example,
with 100 parallel sentences, PD3-MTL achieves an
accuracy of ∼65%, whereas PD3-merge achieves
∼60% and direct transfer achieves ∼61%.

5.2 Analysis

We now analyze several aspects of our approach,
such as the errors it commits and the differences
between PD3-MTL and PD3-merge, as well as
whether we observe the same trends for high- and
low-quality bilingual embeddings.

PD3-MTL vs. PD3-merge For POS, the better
performance of PD3-MTL in some cases compared



136

to PD3-merge may be due PD3-MTL having more
parameters due to independent connection weights
between the CRF classifier and the last hidden layer.
Moreover, some authors have also argued that MTL
is “fundamentally different” from simply adding
auxiliary data (Bollmann and Søgaard, 2016). In
contrast, for ArgMin, we observed that PD3-merge
substantially outperforms PD3-MTL in many cases.
We hypothesize that the reason is the model selec-
tion for PD3-MTL, which chooses the model with
best performance on the dev portion of ˆ̂DT . Since
the model trained on the small LS train set tends
to overpredict the majority class here, the label dis-
tribution on the parallel data differs substantially
from that of the test data. The effects in PD3-merge
are not as pronounced since it also contains parts
of data with the true label distribution.

Direct Transfer vs. PD3 Direct transfer some-
times outperforms PD3 for very few available par-
allel sentences because PD3 uses noisy data in the
form of projected labels, which are particularly
unreliable when parallel data is scarce (see our er-
ror analysis below). This is not true for ArgMin,
however, because projection is loss-less here, as
remarked above. Accordingly, direct transfer never
outperforms PD3 for ArgMin.

Domain shift of parallel data Using TED as
parallel corpus in ArgMin rather than a held-out
portion of the ArgMin dataset itself, we observe
the following, see Figure 3 (top) and Table 4: (i)
PD3-merge still outperforms all other methods; (ii)
PD3-MTL more strongly outperforms projection;
(iii) the in-language upper-bound is harder to reach.
Overall, however, our curves follow a very similar
trend as they do when parallel data comes from
ArgMin itself, even though argumentation in TED
is certainly much less pronounced than it is in stu-
dent essays. This means that our approach appears
robust to changes in domain of the parallel data
even for domain-specific problems such as ArgMin,
and can still outperform direct transfer in these
cases. This is important since parallel data is gen-
erally sparse and most likely there is a substantial
domain gap to the original L1 train data. The TED
results are also interesting insofar as PD3-merge
using 1K parallel sentences performs similarly as
standard projection does using 100K.

Error Analysis For POS, the projection system
that uses only 50 parallel sentences suffers not only
from a tiny L2 training corpus (50 sentences, 783

tokens). Because the parallel corpus is tiny, getting
high-quality alignments from fast-align on it is also
more difficult because the aligner lacks statistical
evidence. We checked alignment quality on 11 ran-
domly chosen short translation pairs (both pairs
shorter than 10 tokens) and on 3 long pairs (both
longer than 20 tokens) for EN-DE. On the short
pairs, 26% of the alignment decisions of fast-align
were wrong. On the long pairs, 46% were wrong.
In contrast, with 5000 parallel sentences error rates
were considerably lower: 11% and 16%, respec-
tively. Hence, projection uses a tiny corpus with
considerable noise in the case of very small amount
of parallel data, causing it to commit all kinds of
errors (e.g., tagging verbs as numbers, etc.). In con-
trast, PD3 uses a larger and much cleaner amount
of L1 data besides the tiny and noisy L2 corpus,
which causes it to perform substantially better.

Direct transfer systems suffer mostly from two
sources of noise: “syntactic shift” due to the L2
language having a different word order than the L1
counterpart on which they have been trained; “se-
mantic shift” due to the test words being all OOV
(this is analogous to monolingually replacing words
by OOV synonyms). The latter effect may be un-
derstood as a “blurring” of the input. Accordingly,
direct transfer easily confuses similar classes: for
example, the EN→DE direct transfer system has a
low F1-score on AUX (confusing auxiliary verbs
with actual verbs) of 35% and on NOUN (confus-
ing nouns with proper nouns) of 37%. Adding
L2 data to the train set, as in PD3, quickly alle-
viates this: the F1-score on AUX for 100 parallel
sentences is 37% and it is 62% for NOUN for PD3-
merge. For 5000 parallel sentences, corresponding
numbers are 56% and 76% respectively.

For ArgMin and tiny amounts of parallel data,
projection predicts all classes but has a very strong
tendency to predict the majority class ‘premise’.
The reason is not that projected labels are noisy—
in contrast, they are very good, because projection
is error-free, as stated above. The problem is rather
that the amount of training data for standard pro-
jection is tiny in this case (size of ˆ̂DT ). PD3 in
contrast trains on much more data and mimics the
true distribution much better. Common errors for
PD3 and direct transfer are confusing claims with
major claims; these often have very similar surface
realizations.

Low-resource shared representations In our
main experiments, we assumed access to high qual-



137

30

40

50

60

70

80

90

50 100 500 1K 5K

in
%

Projection
PD3-merge
PD3-MTL

Direct transfer
In-language

15

20

25

30

35

40

45

50

55

60

50 100 500 1K 2K 3K

in
%

Projection
PD3-merge
PD3-MTL

Direct transfer
In-language

Figure 2: Left: POS accuracies in % as a function of available parallel sentences. Right: Sentence-level
ArgMin F1 scores in % as a function of available parallel sentences.

20

25

30

35

40

45

50

55

60

65

1K 2K 5K 10K 20K 50K 100K

in
%

Projection
PD3-merge
PD3-MTL

Direct transfer
In-language

20

30

40

50

60

50 100 500 1K 2K 3K

in
%

Projection
PD3-merge
PD3-MTL

Direct transfer
In-language

Figure 3: Top: Sentence-level ArgMin F1 scores
in % as a function of available parallel sentences
(sampled from the parallel TED corpus). Bottom:
Sentence-level ArgMin F1 scores in % as a func-
tion of available parallel sentences (low-quality
bilingual word embeddings).

ity bilingual word embeddings. This may not be
justified when the L2 language is low-resource.
Hence, we investigated performances when little
monolingual data in L2 is available. That is, we
limited monolingual data to only 30K sentences in
L2. Given that we assumed 50-3000 parallel sen-
tences for projections and given that monolingual
data is typically much more plentiful than parallel
data, we deemed 30K plausible. We report trends
for the ArgMin sentence classification problem.

Hence, we trained a monolingual word2vec
model on 30K DE sentences (randomly sampled
from the German Wikipedia). For English, we
trained a similar model on the whole of the En-
glish Wikipedia (since L1 is not low-resource). To
induce a bilingual vector space, we then mapped
English and German in a common space via the
method of Artetxe et al. (2017). This approach it-
eratively expands a small seed lexicon of matched
word pairs, thereby successively improving vec-
tor space alignment across two languages. It has
been reported to induce good bilingual represen-
tations even when only common digits in the two
languages or a few dictionary entries are available
as initial seed lexicon. We induced a seed dic-
tionary from our parallel sentences (ranging over
{50, 100, 500, 1000, 2000, 3000} pairs) using fast-
align and then applied the technique of Artetxe
et al. (2017). We subsequently re-ran PD3 and all
other models with the resulting low-quality bilin-
gual word embeddings. The results, for ArgMin,
are shown in Figure 3 (bottom). As can be seen,
direct transfer becomes considerably worse in this
case, which is expected, since the embedding space
is of much lower quality now. The performance



138

drop is from 37% macro-F1 with high-quality em-
beddings to 23%. However, all trends stay the
same, e.g., PD3-merge remains the top performer
for the sentence-level experiments, followed by
PD3-MTL and standard projection. A difference
is that PD3-merge now becomes indistinguishable
from standard projection for 1K parallel sentences
already, rather than 2K as before.

In the extreme case when the bilingual vector
space separates into two independent spaces, one
for each language, then standard projection is at
least as good as PD3, for all sizes of parallel data.
This is because the L1 data cannot improve the L2
model since both operate on independent represen-
tations. However, it is likely that the added noise
may then even confuse a PD3 system if it is not
well-regularized.

We experimented with further reductions to 10K
monolingual sentences in L2 and still saw a sim-
ilar trend as in Figure 3 (bottom). Below 10K
sentences, we found that, somewhat surprisingly,
word2vec could not induce meaningful monolin-
gual embedding spaces, though it is conceivable
that other representation learning techniques, such
as those based on co-occurrence matrices, would
have performed better.

Comparison For POS, we note that our numbers
are generally incomparable to other works because
we use 800 monolingual sentences to train an En-
glish tagger from and (more importantly) treat the
number of parallel sentences as a variable whose
influence we investigate. Still, to give a reference:
Täckström et al. (2013) report cross-lingual tagging
accuracies of up to 90% for German and French as
L2 using a constraint feature-based CRF. They use
up to 5M parallel sentences and 500K size training
data in L2, massively more than we use.

For ArgMin, we also have no direct compar-
isons, because we are the first, to our knowledge,
to explore the student essay corpus of Stab and
Gurevych (2017) on sentence- rather than token-
level. Sentence-level annotation may be preferable
because it is sometimes both conventional as well
as difficult to decide which exact tokens should be
part of an argument component (Persing and Ng,
2016). In terms of cross-language drop, Eger et al.
(2018) report a similar drop of roughly 20pp when
training an argumentation mining system on En-
glish and applying it to similarly annotated German
data, for direct transfer. They close this gap using
machine translation, while we close it under much

milder assumptions using small amounts of parallel
data and a more sophisticated transfer approach.

6 Related Work

Our work connects to different strands of research.

Multi-Task Learning MTL was shown to be
particularly beneficial when tasks stand in a natural
hierarchy and when they are syntactic in nature
(Søgaard and Goldberg, 2016). Moreover, it has
been claimed that further main benefits for MTL
are observed when data for the main task is sparse,
in which case the auxiliary tasks may act as regu-
larizers that prevent overfitting (Ruder et al., 2017).
The latter is the case for PD3-MTL with little avail-
able parallel data.

MTL has also been made use of for supervised
cross-lingual transfer techniques (Cotterell and
Heigold, 2017; Yang et al., 2017; Kim et al., 2017;
Dinh et al., 2018). These assume small training
sets in L2, and a system trained on them is regular-
ized by a larger amount of training data in L1. In
contrast to these, we assume no gold labels in L2
(unsupervised transfer), which necessitates a pro-
jection step. Our approach could also be combined
with these supervised ones, by adding this small
gold data to the three different datasets that we use
in PD3.

Argumentation Mining ArgMin is a fast-
growing field in NLP with applications in decision
making and the legal domain (Palau and Moens,
2009) and can be solved on sentence-level (Daxen-
berger et al., 2017; Niculae et al., 2017; Stab et al.,
2018) or token-level (Eger et al., 2017; Schulz
et al., 2018). Cross-lingual ArgMin has recently at-
tracted interest (Aker and Zhang, 2017; Eger et al.,
2018). The proposed approaches mostly used ma-
chine translation, which is unavailable for the vast
majority of the world’s languages.

Low-resource transfer Low-resource language
transfer has recently become very popular, e.g.,
when relying on only very few translation pairs
for bilingual embedding space induction (Artetxe
et al., 2017; Zhang et al., 2016) or in unsupervised
machine translation using no parallel sources at all
(Artetxe et al., 2018; Lample et al., 2018). Low-
resource transfer (on a level of domains rather than
languages) has also been considered in ArgMin
(Schulz et al., 2018), assuming little annotated data
in a new target domain due to annotation costs of
ArgMin as a subjective high-level task.



139

7 Concluding Remarks

We combined direct transfer with annotation pro-
jection, addressing short-comings of both methods
and combining their strengths. We saw consistent
gains over either of the two methods in isolation,
particularly in the small dataset scenario with 50-
500 parallel sentences. This is arguably the most
realistic scenario for a good portion of the world’s
languages, for which several dozens of parallel sen-
tences are readily available e.g. from Bible trans-
lations (Christodoulopoulos and Steedman, 2015).
We also note that while translating 50 sentences
by hand may be as easy as labeling 50 sentences
in L2, provided the problem requires no expert
knowledge, parallel data serves many NLP prob-
lems, while the cost of labeling multiplies by the
number of problems.

We also analyzed our approach under changes to
external factors such as the bilingual embeddings
and the domain of the parallel data, and found it to
perform stable under such shifts, consistently out-
performing the two baselines it is built upon in the
setting of little available parallel sentences. This
is particularly important for tasks such as ArgMin,
for which it is inherently difficult to get domain
specific parallel data, let alone for many languages.

Future work should consider further extensions:
E.g., for cross-lingual approaches, it is also pos-
sible to select predictions on the source side of
parallel data into the train sets only if the classi-
fier’s confidence exceeds a certain threshold, or to
apply this process iteratively (Täckström, 2012).
This can be immediately applied and extended to
the PD3 approach. Another extension is to perform
self-training on L2 data, which we briefly discuss in
the supplementary material. Moreover, PD3 should
also be applied in scenarios where L2 is a more
distant language to English than considered here,
or to setups where L1 is another language than En-
glish, although it is unlikely that the general trends
we detected here would not persist under L1 and
L2 variations. Further, while we did not observe
consistent gains of PD3-MTL (sometimes consid-
erable losses) over PD3-merge, we note that there
are refinements of the MTL paradigm (e.g., Liu
et al. (2017)) which might yield better results in
our situation.

Acknowledgments

This work has been supported by the German Fed-
eral Ministry of Education and Research (BMBF)

under the promotional reference 01UG1816B
(CEDIFOR) and 03VP02540 (ArgumenText) and
by the German Research Foundation as part of the
QA-EduInf project (grant GU 798/18-1 and grant
RI 803/12-1).

References
Zeljko Agic, Anders Johannsen, Barbara Plank,

Héctor Martı́nez Alonso, Natalie Schluter, and An-
ders Søgaard. 2016. Multilingual projection for
parsing truly low-resource languages. Transac-
tions of the Association of Computational Linguis-
tics (TACL), 4:301–312.

Ahmet Aker and Huangpan Zhang. 2017. Projection
of argumentative corpora from source to target lan-
guages. In Proceedings of the 4th Workshop on Ar-
gument Mining, ArgMining@EMNLP 2017, Copen-
hagen, Denmark, September 8, 2017, pages 67–72.

Mikel Artetxe, Gorka Labaka, and Eneko Agirre. 2017.
Learning bilingual word embeddings with (almost)
no bilingual data. In Proceedings of the 55th An-
nual Meeting of the Association for Computational
Linguistics (ACL 2017), pages 451–462.

Mikel Artetxe, Gorka Labaka, Eneko Agirre, and
Kyunghyun Cho. 2018. Unsupervised neural ma-
chine translation. ICLR.

Marcel Bollmann and Anders Søgaard. 2016. Im-
proving historical spelling normalization with bi-
directional lstms and multi-task learning. In Pro-
ceedings of the 26th International Conference on
Computational Linguistics (COLING 2016), pages
131–139.

Rich Caruana. 1993. Multitask learning: A knowledge-
based source of inductive bias. In Proceedings of the
10th International Conference on Machine Learning
(ICML 1993), pages 41–48.

Christos Christodoulopoulos and Mark Steedman.
2015. A massively parallel corpus: the bible in
100 languages. Language Resources and Evalua-
tion, 49(2):375–395.

Ryan Cotterell and Georg Heigold. 2017. Cross-
lingual character-level neural morphological tag-
ging. In Proceedings of the 2017 Conference on
Empirical Methods in Natural Language Processing
(EMNLP 2017), pages 759–770.

Dipanjan Das and Slav Petrov. 2011. Unsupervised
part-of-speech tagging with bilingual graph-based
projections. In Proceedings of the 2011 Conference
of the North American Chapter of the Association
for Computational Linguistics: Human Language
Technologies (NAACL-HLT 2011), pages 600–609.

Johannes Daxenberger, Steffen Eger, Ivan Habernal,
Christian Stab, and Iryna Gurevych. 2017. What is

http://www.aclweb.org/anthology/Q16-1022
http://www.aclweb.org/anthology/Q16-1022
https://aclanthology.info/papers/W17-5108/w17-5108
https://aclanthology.info/papers/W17-5108/w17-5108
https://aclanthology.info/papers/W17-5108/w17-5108
https://doi.org/10.18653/v1/P17-1042
https://doi.org/10.18653/v1/P17-1042
http://www.aclweb.org/anthology/C16-1013
http://www.aclweb.org/anthology/C16-1013
http://www.aclweb.org/anthology/C16-1013
https://doi.org/10.1007/s10579-014-9287-y
https://doi.org/10.1007/s10579-014-9287-y
http://aclweb.org/anthology/D17-1078
http://aclweb.org/anthology/D17-1078
http://aclweb.org/anthology/D17-1078
http://www.aclweb.org/anthology/P11-1061
http://www.aclweb.org/anthology/P11-1061
http://www.aclweb.org/anthology/P11-1061


140

the Essence of a Claim? Cross-Domain Claim Iden-
tification. In Proceedings of the 2017 Conference on
Empirical Methods in Natural Language Processing,
pages 2045–2056.

Erik-Lân Do Dinh, Steffen Eger, and Iryna Gurevych.
2018. Killing four birds with two stones: Multi-
task learning for non-literal language detection. In
Proceedings of the 27th International Conference on
Computational Linguistics (COLING 2018), pages
1558–1569.

Chris Dyer, Victor Chahuneau, and Noah A. Smith.
2013. A simple, fast, and effective reparameteriza-
tion of ibm model 2. In Proceedings of the 2013
Conference of the North American Chapter of the
Association for Computational Linguistics: Human
Language Technologies (NAACL-HLT 2013), pages
644–648.

Steffen Eger, Johannes Daxenberger, and Iryna
Gurevych. 2017. Neural end-to-end learning for
computational argumentation mining. In Proceed-
ings of the 55th Annual Meeting of the Association
for Computational Linguistics (Volume 1: Long Pa-
pers), pages 11–22, Vancouver, Canada. Association
for Computational Linguistics.

Steffen Eger, Johannes Daxenberger, Christian Stab,
and Iryna Gurevych. 2018. Cross-lingual argumen-
tation mining: Machine translation (and a bit of pro-
jection) is all you need! In Proceedings of the 27th
International Conference on Computational Linguis-
tics (COLING 2018).

Ivan Habernal and Iryna Gurevych. 2017. Argumenta-
tion mining in user-generated web discourse. Com-
putational Linguistics, 43(1):125–179.

Karl Moritz Hermann and Phil Blunsom. 2014. Mul-
tilingual Models for Compositional Distributed Se-
mantics. In Proceedings of the 52nd Annual Meet-
ing of the Association for Computational Linguistics
(ACL 2014), pages 58–68.

Joo-Kyung Kim, Young-Bum Kim, Ruhi Sarikaya, and
Eric Fosler-Lussier. 2017. Cross-lingual transfer
learning for pos tagging without cross-lingual re-
sources. In Proceedings of the 2017 Conference on
Empirical Methods in Natural Language Processing
(EMNLP 2017), pages 2822–2828.

Philipp Koehn. 2005. Europarl: A Parallel Corpus for
Statistical Machine Translation. In In Proceedings
of the tenth Machine Translation Summit, pages 79–
86. AAMT.

Guillaume Lample, Miguel Ballesteros, Sandeep Sub-
ramanian, Kazuya Kawakami, and Chris Dyer. 2016.
Neural architectures for named entity recognition.
In Proceedings of the 2016 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics: Human Language Technologies
(NAACL-HLT 2016), pages 260–270.

Guillaume Lample, Ludovic Denoyer, and
Marc’Aurelio Ranzato. 2018. Unsupervised
machine translation using monolingual corpora only.
ICLR.

Pengfei Liu, Xipeng Qiu, and Xuanjing Huang. 2017.
Adversarial multi-task learning for text classifica-
tion. In Proceedings of the 55th Annual Meeting of
the Association for Computational Linguistics (ACL
2017), pages 1–10.

Xuezhe Ma and Eduard H. Hovy. 2016. End-to-end se-
quence labeling via bi-directional lstm-cnns-crf. In
Proceedings of the 54th Annual Meeting of the Asso-
ciation for Computational Linguistics (ACL 2016),
pages 1064–1074.

Ryan McDonald, Slav Petrov, and Keith Hall. 2011.
Multi-source transfer of delexicalized dependency
parsers. In Proceedings of the 2011 Conference on
Empirical Methods in Natural Language Processing
(EMNLP 2011), pages 62–72.

Marie-Francine Moens. 2017. Argumentation min-
ing: How can a machine acquire common sense and
world knowledge? Argument & Computation. Ac-
cepted.

Vlad Niculae, Joonsuk Park, and Claire Cardie. 2017.
Argument mining with structured svms and rnns. In
Proceedings of the 55th Annual Meeting of the As-
sociation for Computational Linguistics (Volume 1:
Long Papers), pages 985–995. Association for Com-
putational Linguistics.

Joakim Nivre, Marie-Catherine de Marneffe, Filip Gin-
ter, Yoav Goldberg, Jan Hajic, Christopher D. Man-
ning, Ryan McDonald, Slav Petrov, Sampo Pyysalo,
Natalia Silveira, Reut Tsarfaty, and Daniel Zeman.
2016. Universal dependencies v1: A multilingual
treebank collection. In Proceedings of the Tenth In-
ternational Conference on Language Resources and
Evaluation (LREC 2016).

Raquel Mochales Palau and Marie-Francine Moens.
2009. Argumentation mining: The detection, clas-
sification and structure of arguments in text. In Pro-
ceedings of the 12th International Conference on Ar-
tificial Intelligence and Law, ICAIL ’09, pages 98–
107, New York, NY, USA. ACM.

Andreas Peldszus and Manfred Stede. 2013. From
Argument Diagrams to Argumentation Mining in
Texts: A Survey. International Journal of Cognitive
Informatics and Natural Intelligence, 7(1):1–31.

Isaac Persing and Vincent Ng. 2016. End-to-end ar-
gumentation mining in student essays. In Proceed-
ings of the 2016 Conference of the North Ameri-
can Chapter of the Association for Computational
Linguistics: Human Language Technologies, pages
1384–1394, San Diego, California. Association for
Computational Linguistics.

http://tubiblio.ulb.tu-darmstadt.de/105435/
http://tubiblio.ulb.tu-darmstadt.de/105435/
http://www.aclweb.org/anthology/N13-1073
http://www.aclweb.org/anthology/N13-1073
http://aclweb.org/anthology/P17-1002
http://aclweb.org/anthology/P17-1002
http://tubiblio.ulb.tu-darmstadt.de/105431/
http://tubiblio.ulb.tu-darmstadt.de/105431/
http://tubiblio.ulb.tu-darmstadt.de/105431/
https://doi.org/10.3115/v1/P14-1006
https://doi.org/10.3115/v1/P14-1006
https://doi.org/10.3115/v1/P14-1006
http://aclweb.org/anthology/D17-1302
http://aclweb.org/anthology/D17-1302
http://aclweb.org/anthology/D17-1302
https://doi.org/10.18653/v1/N16-1030
https://doi.org/10.18653/v1/P17-1001
https://doi.org/10.18653/v1/P17-1001
https://doi.org/10.18653/v1/P16-1101
https://doi.org/10.18653/v1/P16-1101
http://www.aclweb.org/anthology/D11-1006
http://www.aclweb.org/anthology/D11-1006
https://lirias.kuleuven.be/handle/123456789/592159
https://lirias.kuleuven.be/handle/123456789/592159
https://lirias.kuleuven.be/handle/123456789/592159
https://doi.org/10.18653/v1/P17-1091
https://doi.org/10.1145/1568234.1568246
https://doi.org/10.1145/1568234.1568246
http://www.aclweb.org/anthology/N16-1164
http://www.aclweb.org/anthology/N16-1164


141

Andreas Rücklé, Steffen Eger, Maxime Peyrard, and
Iryna Gurevych. 2018. Concatenated power mean
word embeddings as universal cross-lingual sen-
tence representations. CoRR, abs/1803.01400.

Sebastian Ruder, Joachim Bingel, Isabelle Augenstein,
and Anders Søgaard. 2017. Sluice networks: Learn-
ing what to share between loosely related tasks. In
arXiv preprint.

Claudia Schulz, Steffen Eger, Johannes Daxenberger,
Tobias Kahse, and Iryna Gurevych. 2018. Multi-task
learning for argumentation mining in low-resource
settings. In Proceedings of the 16th Annual Con-
ference of the North American Chapter of the Asso-
ciation for Computational Linguistics: Human Lan-
guage Technologies, pages 35–41. Association for
Computational Linguistics.

Anders Søgaard and Yoav Goldberg. 2016. Deep multi-
task learning with low level tasks supervised at lower
layers. In Proceedings of the 54th Annual Meet-
ing of the Association for Computational Linguistics
(ACL 2016), pages 231–235.

Christian Stab, Johannes Daxenberger, Chris Stahlhut,
Tristan Miller, Benjamin Schiller, Christopher
Tauchmann, Steffen Eger, and Iryna Gurevych. 2018.
Argumentext: Searching for arguments in heteroge-
neous sources. In Proceedings of the 2018 Confer-
ence of the North American Chapter of the Associa-
tion for Computational Linguistics: System Demon-
strations, pages 21–25.

Christian Stab and Iryna Gurevych. 2014. Annotat-
ing argument components and relations in persua-
sive essays. In Proceedings of the 25th International
Conference on Computational Linguistics (COLING
2014), pages 1501–1510.

Christian Stab and Iryna Gurevych. 2017. Parsing ar-
gumentation structures in persuasive essays. Com-
putational Linguistics, 43(3):619–659.

Oscar Täckström. 2012. Nudging the envelope of di-
rect transfer methods for multilingual named entity
recognition. In Proceedings of the NAACL-HLT
Workshop on the Induction of Linguistic Structure,
pages 55–63.

Oscar Täckström, Dipanjan Das, Slav Petrov, Ryan T.
McDonald, and Joakim Nivre. 2013. Token and type
constraints for cross-lingual part-of-speech tagging.
Transactions of the Association of Computational
Linguistics (TACL), 1:1–12.

Oscar Täckström, Ryan McDonald, and Joakim Nivre.
2013. Target language adaptation of discrimina-
tive transfer parsers. In Proceedings of the 2013
Conference of the North American Chapter of the
Association for Computational Linguistics: Human
Language Technologies (NAACL-HLT 2013), pages
1061–1071.

Ivan Vulić and Marie-Francine Moens. 2015. Bilin-
gual word embeddings from non-parallel document-
aligned data applied to bilingual lexicon induction.
In Proceedings of the 53rd Annual Meeting of the
Association for Computational Linguistics and the
7th International Joint Conference on Natural Lan-
guage Processing (ACL-IJCNLP 2015), pages 719–
725.

Zhilin Yang, Ruslan Salakhutdinov, and William W.
Cohen. 2017. Transfer learning for sequence tag-
ging with hierarchical recurrent networks. 5th Inter-
national Conference on Learning Representations
(ICLR 2017).

David Yarowsky. 1995. Unsupervised word sense dis-
ambiguation rivaling supervised methods. In Pro-
ceedings of the 33rd Annual Meeting on Association
for Computational Linguistics (ACL 1995), pages
189–196.

David Yarowsky, Grace Ngai, and Richard Wicen-
towski. 2001. Inducing multilingual text analysis
tools via robust projection across aligned corpora. In
Proceedings of the First International Conference on
Human Language Technology Research (HLT 2001),
pages 1–8.

Yuan Zhang, David Gaddy, Regina Barzilay, and
Tommi Jaakkola. 2016. Ten pairs to tag – multilin-
gual pos tagging via coarse mapping between em-
beddings. In Proceedings of the 2016 Conference of
the North American Chapter of the Association for
Computational Linguistics: Human Language Tech-
nologies (NAACL-HLT 2016)), pages 1307–1317.

Micha Ziemski, Marcin Junczys-Dowmunt, and Bruno
Pouliquen. 2016. The united nations parallel cor-
pus v1.0. In Proceedings of the Tenth International
Conference on Language Resources and Evaluation
(LREC 2016). European Language Resources Asso-
ciation (ELRA).

http://arxiv.org/abs/1803.01400
http://arxiv.org/abs/1803.01400
http://arxiv.org/abs/1803.01400
http://arxiv.org/abs/1705.08142
http://arxiv.org/abs/1705.08142
https://doi.org/10.18653/v1/P16-2038
https://doi.org/10.18653/v1/P16-2038
https://doi.org/10.18653/v1/P16-2038
http://tubiblio.ulb.tu-darmstadt.de/105466/
http://tubiblio.ulb.tu-darmstadt.de/105466/
http://www.aclweb.org/anthology/C14-1142
http://www.aclweb.org/anthology/C14-1142
http://www.aclweb.org/anthology/C14-1142
http://www.aclweb.org/anthology/W12-1908
http://www.aclweb.org/anthology/W12-1908
http://www.aclweb.org/anthology/W12-1908
http://www.aclweb.org/anthology/Q13-1001
http://www.aclweb.org/anthology/Q13-1001
http://www.aclweb.org/anthology/N13-1126
http://www.aclweb.org/anthology/N13-1126
https://doi.org/10.3115/v1/P15-2118
https://doi.org/10.3115/v1/P15-2118
https://doi.org/10.3115/v1/P15-2118
http://www.aclweb.org/anthology/P95-1026
http://www.aclweb.org/anthology/P95-1026
https://doi.org/10.3115/1072133.1072187
https://doi.org/10.3115/1072133.1072187
https://doi.org/10.18653/v1/N16-1156
https://doi.org/10.18653/v1/N16-1156
https://doi.org/10.18653/v1/N16-1156


142

A Supplemental Material

In Table 4, we show detailed results across lan-
guages and tasks, as well as different transfer strate-
gies. Below, we discuss another transfer strategy
named L2-ST.

Further approaches When systems are trained
on shared features as in direct transfer, then an-
other approach for unsupervised cross-lingual trans-
fer is self-training on L2 data (Täckström, 2012;
Täckström et al., 2013). The idea is to train a sys-
tem on labeled source language data LS , and then
directly apply this trained system to the parallel
target language data (this is possible because of
the shared feature representation), rather than its
source side and merge this newly obtained “self-
labeled” dataset with LS .

We found this strategy, named L2-ST in Table
4 in the appendix, to perform substantially below
our considered transfer strategies when there is a
sufficient amount of L2 data available. Only with
very little target L2 data (50 parallel sentences) did
we observe some gains over PD3 in POS tagging.
The reason is that for very little parallel data, align-
ment links are very noisy, as discussed above, so
that the projected labels are of low quality. In this
case, however, the best strategy is then to combine
PD3 with self-training in L2, and thus to combine
four datasets: two of them in L1 and two of them
in L2. This strategy, which we dub PD4 in Table 4,
outperforms L2-ST, but is worse than PD3 for high-
and medium-sized parallel corpora. The reason is
that the system trained on LS is typically much
better when applied to L1 data than when applied
to L2—see our discussion on direct transfer—and
thus the L2 predictions resulting from labeling the
source side of parallel data and then projecting to
L2 are better than those from directly predicting
on L2, provided the projection step is sufficiently
good.

This is also the reason why PD4-merge always
underperforms PD3-merge for ArgMin—since pro-
jection is error-free for sentence level classification.



143

Task Projection PD3-merge PD3-MTL L2-ST PD4-merge Direct Transfer In-Language
Parallel Sentences (upper bound)

Token-level POS tagging with TED as parallel corpus (EN→DE)

50 37.86 53.63 55.21 53.56 56.09 55.63 86.29
100 45.37 60.84 61.27 55.81 60.14
500 67.07 70.16 70.18 57.60 64.68
1,000 70.74 72.30 72.24 57.27 66.18
5,000 76.04 77.22 76.55 56.28 66.67

Token-level POS tagging with TED as parallel corpus (EN→FR)

50 25.16 58.36 58.78 67.21 67.55 67.87 92.67
100 46.97 60.64 68.96 70.02 71.42
500 66.49 72.00 72.79 70.34 73.81
1,000 69.86 73.51 74.14 70.07 73.23
5,000 77.41 78.29 77.81 68.92 74.37

Sentence-level AM with 3K sentences of AM as parallel corpus (EN→DE)

50 18.80 43.89 20.45 39.95 41.13 37.94 59.25
100 21.46 49.20 26.95 36.55 45.89
500 45.18 53.93 46.75 39.18 49.53
1,000 50.62 55.45 52.20 38.24 49.87
2,000 55.55 57.32 56.39 38.47 50.29
3,000 57.47 57.42 57.52 38.35 51.41

Sentence-level AM with TED as parallel corpus (EN→DE)

1,000 21.51 45.61 24.93 42.32 46.59 43.93 62.42
2,000 31.21 50.48 35.93 41.63 45.87
5,000 32.71 50.03 37.40 43.57 46.85
10,000 37.22 49.35 43.57 44.42 47.24
20,000 41.23 49.13 45.78 45.08 48.02
50,000 47.16 51.57 48.66 43.18 50.20
100,000 48.58 51.32 48.72 45.05 50.53

Table 4: Individual results for all tasks, languages, and number of parallel sentences. We report the accuracy
for our token-level POS tagging experiments and F1 scores for our sentence-level AM experiments. L2-ST
denotes cross-lingual transfer with self-training using L2 data as in (Täckström, 2012; Täckström et al.,
2013). PD4-merge combines PD3 with self-training in L2.


