










































Learning a Product of Experts with Elitist Lasso


International Joint Conference on Natural Language Processing, pages 525–533,
Nagoya, Japan, 14-18 October 2013.

Learning a Product of Experts with Elitist Lasso

Mengqiu Wang
Computer Science Department

Stanford University
Stanford, CA 94305, USA

mengqiu@cs.stanford.edu

Christopher D. Manning
Computer Science Department

Stanford University
Stanford, CA 94305, USA

manning@cs.stanford.edu

Abstract

Discriminative models such as logistic re-
gression profit from the ability to incorpo-
rate arbitrary rich features; however, com-
plex dependencies among overlapping fea-
tures can often result in weight undertrain-
ing. One popular method that attempts to
mitigate this problem is logarithmic opin-
ion pools (LOP), which is a specialized
form of product of experts model that au-
tomatically adjusts the weighting among
experts. A major problem with LOP is
that it requires significant amounts of do-
main expertise in designing effective ex-
perts. We propose a novel method that
learns to induce experts — not just the
weighting between them — through the
use of a mixed `2`1 norm as previously
seen in elitist lasso. Unlike its more
popular sibling `1`2 norm (used in group
lasso), which seeks feature sparsity at the
group-level, `2`1 norm encourages spar-
sity within feature groups. We demon-
strate how this property can be leveraged
as a competition mechanism to induce
groups of diverse experts, and introduce
a new formulation of elitist lasso Max-
Ent in the FOBOS optimization frame-
work (Duchi and Singer, 2009). Results
on Named Entity Recognition task suggest
that this method gives consistent improve-
ments over a standard logistic regression
model, and is more effective than conven-
tional induction schemes for experts.

1 Introduction

Conditionally trained discriminative models like
logistic regression (a.k.a., MaxEnt models) gain

power from their ability to incorporate a large
number of arbitrarily overlapping features. But
such models also exhibit complex feature depen-
dencies and therefore are susceptible to the prob-
lem of weight undertraining — the contributions
of certain features are overlooked because of fea-
tures they co-occur with (Sutton et al., 2007; Hin-
ton et al., 2012).

One popular method that attempts to mitigate
this problem is logarithmic opinion pools (LOP)
(Heskes, 1998; Smith et al., 2005; Sutton et al.,
2007). LOP works in a similar fashion to a prod-
uct of experts model, in which a number of experts
are individually assembled first, and then their pre-
dictions are combined multiplicatively to create an
ensemble model. Experts are generated by first
partitioning the feature space into subsets, then an
independent model (expert) is trained on each sub-
set of features (Sutton et al., 2007). Under the
assumption that strongly correlated features are
partitioned into separate subsets, this method ef-
fectively forces the models to learn how to make
predictions with each subset of the features inde-
pendently. It also helps in scenarios where some
strong features stop other features from getting
weight, a problem known as feature co-adaptation.
The theoretical justification for favoring a product
ensemble over an additive ensemble is that infer-
ence with a product of log-linear models is sig-
nificantly easier than inference with a sum. Sut-
ton et al. (2007) also suggests that product ensem-
bles give better results than additive mixtures on
sequence labeling tasks.

Smith et al. (2005) and Sutton et al. (2007)
showed that the quality of experts and diversity
among them have a direct impact on the final per-
formance of the ensemble. Designing effective
and diverse experts was left as an art, and requires

525



a great deal of domain expertise.
In this paper, we directly learn to induce di-

verse experts by using a mixed `2`1 norm known
as elitist lasso in the context of generalized lin-
ear models (Kowalski and Torrésani, 2009). We
design a novel competition mechanism to encour-
age experts to use non-overlapping feature sets, ef-
fectively learning a partition of the feature space.
Efficient optimization of a maximum conditional
likelihood objective with respect to `2`1 norm
is non-trivial and has not been previously stud-
ied. We propose a novel formulation to incorpo-
rate `2`1 norm in the FOBOS optimization frame-
work (Duchi and Singer, 2009). Experiments on
Named Entity Recognition task suggest that our
proposed method gives consistent improvements
over a baseline MaxEnt model and conventional
LOP experts. In particular, our method gives the
most improvements in recognizing entity types for
out-of-vocabulary words.

2 MaxEnt Model

Given an input sequence x (e.g., words in a sen-
tence), a MaxEnt model defines the conditional
probability of an output variable y (e.g., named-
entity tag of a word) as follows:

P (y = vi|x) =
exp

(
K∑
k=1

θk(vi)fk(x)

)
V∑
j=1

exp

(
K∑
k=1

θk(vj)fk(x)

)
where fk(x) is the kth input feature, K is the total
number of input features, and θk(y) is the weight
parameter associated with feature fk for output
class vi. We denote the output classes of y to be
{v1, v2, · · · , vV }, where V is the total number of
output classes.

It is helpful to consider the connection between
a MaxEnt model and a Linear Network model
commonly seen in the neural network literature,
by re-expressing this objective using a softmax ac-
tivation function. The softmax function is defined
as:

softmax (q(y)) =
exp (q(y))∑
y′ exp (q(y

′))

in our case q(y) =
K∑
k=1

θk(y)fk(x), and we have:

P (y|x) = softmax

(
K∑
k=1

θk(y)fk(x)

)

f1(x)

f2(x)

y = v1

y = v2

θ1(v1)

θ2(v2)
softmax

Input Output Softmax

MaxEnt

f1(x)

f2(x)

y = v1

y = v2

θe11 (v1)

θe11 (v2)

θe22 (v1)

θe22 (v2)

softmax

α1

α1

α2

α2

e1

e2

Input Expert Output Softmax

LOP

f1(x)

f2(x)

y = v1

y = v2

θe11 (v1)

θe21 (v1)

θe22 (v1)

θe22 (v2)

softmax

α1

α1

α2

α2

e1

e2

Input Expert Output Softmax

elitist
LOP

Figure 1: The top part shows a regular MaxEnt
model with 2 features and 2 output classes. The
middle part shows a LOP with two experts. α1 and
α2 are the expert mixing coefficients to be learned
in the LOP model. The bottom part shows an eli-
tist LOP expert induction model with two experts.
For each feature weight parameter θ, the super-
script denotes which feature group it belongs to
(e.g., θe1 belongs to expert e1); the subscript de-
notes which input feature it belongs to (e.g., θ1
means it applies to input feature f1(x)); and the
value in parentheses denotes which output class
this feature is associated with (e.g., θ(v1) applies
to output class y = v1). Features that belong to
the same group are shown in the same color.

526



We visualize this using a neural network style di-
agram in the top part of Figure 1. In this dia-
gram, the value at each node is computed as the
sum of all incoming edge weights (parameters in
the model) multiplied by the values of nodes on
the originating end of edges. In this example,
K = 2 and we have two input features (f1(x) and
f2(x)); there are two possible value assignments
for y (v1 and v2). Correspondingly, there are four
weight parameters (θi(vj), for i = 1, 2, j = 1, 2)
as shown by the directed edge going from “Input”
layer nodes to “Output” layer nodes. We apply a
softmax function to the “Output” layer to produce
the probability assignment of each output class.

3 Weight Undertraining

The problem of weight undertraining occurs in
discriminatively trained classifiers when some
strongly indicative features occur during training
but not at test time. Because of the presence of
such strong features during training, weaker fea-
tures that occur at both training and test time do
not receive enough weight, and thus impede the
classifier from reaching its full potential at test
time. This phenomenon is also known as fea-
ture co-adaptation (Hinton et al., 2012). Sutton
et al. (2007) gave an excellent demonstration of
this problem using a synthetic logistic regression
model. In their example, the output function is the
softmax of the linear sum of a set of weights x1
to xn, where each of the xi acts as a “weak” pre-
dictor. A “strong” feature which takes the form of
x′ =

∑n
i=1 xi+N (whereN is an added Gaussian

noise) is then added to the classifier during train-
ing but removed at test time, which simulates a
feature co-adaptation scenario. Simulation results
show that classifier accuracy can drop by as much
as 40% as a result of weight undertraining.

This problem is difficult to combat because we
do not know a priori what set of features will be
present at test time. However, a number of meth-
ods including feature bagging (Sutton et al., 2007),
random feature dropout (Hinton et al., 2012), and
logarithmic opinion pools (Heskes, 1998; Smith et
al., 2005) have been introduced as ways to allevi-
ate weight undertraining.

4 Logarithmic Opinion Pools

The idea behind logarithmic opinion pools (LOP)
is that instead of training one classifier, we can
train a set of classifiers with non-overlapping or

competing feature sets. At test time, we can com-
bine these classifiers and average their results. By
creating a diverse set of classifiers and training
each of them separately, we can potentially re-
duce the effect of weight undertraining. For ex-
ample, when two strongly correlated features are
partitioned into different classifiers, they do not
overshadow each other. Unlike voting or feature
bagging where an additive ensemble of experts is
used, LOP derives a joint model by taking a prod-
uct of experts.1

The conditional probability of a LOP ensemble
can be expressed as:

PLOP (y|x) =
∏n
j=1 [Pj(y|x)]

αj∑
y′
∏n
j=1 [Pj(y|x)]

αj

Pj(y|x) = softmax

(
K∑
k=1

θ
ej
k (y)fk(x)

)
where n is the total number of experts, Pj(y|x) is
the probability assignment of expert j, and αj is
the mixing coefficient for this expert.

The experts are typically selected so that they
capture different signals from the training data
(e.g., via feature subsetting). Each expert is
trained separately, and their model parameters
θ
ej
k (y) are fixed during LOP combination.

Early work on LOPs either fixed the expert
mixing coefficients to some uniformly assigned
value (Hinton, 1999), or used some arbitrary hand-
picked values (Sutton et al., 2007). In both of these
two cases, no new parameter values need to be
learned, therefore no extra training is required.

Smith et al. (2005) proposed to learn the
weights by maximizing log-likelihood of the en-
semble product model, and showed that the
learned weights work better than uniformly as-
signed values.

We can visualize the LOP again using a network
diagram, as shown in the middle part of Figure 1.
Two experts are shown in this diagram, each repre-
sented by a plate in the “Expert” layer. This exam-
ple illustrates an expert generation scheme by par-
titioning the features into non-overlapping subsets
– expert e1 has feature function f1(x) while ex-
pert e2 has feature function f2(x). The weights of
the two experts are pre-trained and remain fixed.
Training the LOP model involves only learning the
α mixing parameters.

1A product in the raw probability space is equivalent to a
sum in the logarithmic space, and hence the name “logarith-
mic” opinion pools.

527



5 Automatic Induction of Experts

A major concern in adopting existing LOP meth-
ods is that there are no straightforward guidelines
on how to create experts. Different feature par-
tition schemes can result in dramatically differ-
ent performance (Smith et al., 2005). A success-
fully designed LOP expert ensemble typically in-
volves non-trivial engineering effort and a signif-
icant amount of domain expertise. Furthermore,
the improvements shown in one application do-
main by a well-designed feature partition scheme
do not necessarily carry over to other domains.

Therefore, it is our goal to investigate and
search for effective methods to automatically in-
duce LOP experts. We would like to find a
set of experts that are diverse (i.e., having non-
overlapping feature sets) and accurate. The main
idea here is that we can train multiple copies of
MaxEnt models together to optimize training like-
lihood, but at the same time we encourage the
MaxEnt models to differ as much as possible in
their choice of features to employ. To meet this
end, we use a `2`1 mixed norm to regularize a
LOP, and denote the new model as elitist LOP.

5.1 Elitist LOP
Before we introduce the `2`1 norm, which has not
been frequently used in NLP research, let us revisit
a closely related but much better known sibling –
the `1`2 norm used in group lasso.

For a set of model parameters θ, assume we
have some feature grouping that partitions the fea-
tures into G sub-groups. For simplicity, let us fur-
ther assume that each feature group has the same
number of features (M ). We denote the index of
each feature by g (for “group”) and m (for “mem-
ber”), and use θĝ to denote the feature group of
index g.

The `1`2 mixed norm used in group lasso takes
the following form:

‖θ‖1,2 =

 G∑
g=1

(
M∑
m=1

|θg,m|2
)1/2

= ‖(‖θ1̂‖2, · · · , ‖θĜ‖2)‖1

This norm applies `2 regularization to each
group of features, but enforces `1 regularization at
the group level. Since `1 norm encourages sparse
solutions, therefore the effect of the mixed norm is
to sparsify features at the group level by eliminat-
ing a whole group of features at a time.

It is easy to see how it relates to the `2`1 mixed
norm, which is given as:

‖θ‖2,1 =

 G∑
g=1

(
M∑
m=1

|θg,m|

)21/2
= ‖(‖θ1̂‖1, · · · , ‖θĜ‖1)‖2

Like `1`2, this is also a group-sensitive sparsity-
inducing norm, but instead of encouraging sparsity
across feature groups, it promotes sparsity within
each feature group.

We illustrate how we apply `2`1 norm to in-
duce diverse experts in the bottom diagram in Fig-
ure 1. Similar to the case of LOP, we create two
“Expert” layer plates, one for each expert (e1 and
e2). But unlike traditional LOP, where each ex-
pert only gets a subset of the input features (e.g.,
f1(x) 7→ e1 and f2(x) 7→ e2), each input fea-
ture is fully connected to each expert plate (i.e.,
there are four edges with the same expert super-
script from the “Input” layer going into each plate
in the “Expert” layer, shown in different colors).

We group every pair of feature weights that
originate from the same input feature and end
at the same output class into a separate feature
group (e.g., θe11 (v1) and θ

e2
1 (v1) belongs to one

group, while θe12 (v1) and θ
e2
2 (v1) belongs to an-

other group, etc.). In total we have four feature
groups, each shown in a different color.

When we apply `2`1 norm to this feature group-
ing while learning the parameter weights in the
LOP model, the regularization will push most of
the features that belong to the same group towards
0. But since we are optimizing a likelihood objec-
tive, if a particular input feature is indeed predic-
tive, the model will also try to assign some weights
to the weight parameters associated with this fea-
ture. These two opposite forces create a novel
competition mechanism among features that be-
long to the same group.

This competition mechanism encourages the
experts to use different sets of non-overlapping
features, thus achieving the diversity effect we
want. But because we are also optimizing the like-
lihood of the ensemble, each expert is encouraged
to use parameters that are as predictive as possi-
ble. Learning all the experts together can also be
seen as a form of feature sharing, as suggested by
Ando and Zhang (2005) for multi-task learning.

528



5.2 FOBOS with `2`1 norm
In our automatic expert induction model with `2`1
norm, the overall objective is given as:

L = argmin
θ

(
− log

( ∏n
j=1 [Pj(y|x)]

αj∑
y′
∏n
j=1 [Pj(y|x)]

αj

)
+λ‖(‖θ1̂‖1, · · · , ‖θĜ‖1)‖2

)
where λ is a parameter that controls the regular-
ization strength.

The feature groups are given as:

θ1̂ = {θ
e1
1 (v1), θ

e2
1 (v1), · · · , θ

en

1 (v1)}
θ2̂ = {θ

e1
2 (v1), θ

e2
2 (v1), · · · , θ

en

2 (v1)}
· · ·
θK̂ = {θ

e1
K (v1), θ

e2
K (v1), · · · , θ

en

K (v1)}
θ ˆK+1 = {θ

e1
1 (v2), θ

e2
1 (v2), · · · , θ

en

1 (v2)}
θ ˆK+2 = {θ

e1
2 (v2), θ

e2
2 (v2), · · · , θ

en

2 (v2)}
· · ·
θĜ = {θ

e1
K (vV ), θ

e2
K (vV ), · · · , θ

en

K (vV )}

There is a total of G = K×V feature groups, and
each feature group has M = n features.

The key difference here from LOP is that the
θ parameters are not pre-trained and fixed, but
jointly learned with respect to `2`1 regularization.
In the LOP case, learning the mixing coefficients
αi (i = {1, . . . , n}) can be done by taking the
gradients of αi with respect to the training ob-
jective and directly plugging it into a gradient-
based optimization framework, such as the lim-
ited memory variable metric (LMVM) used by
Smith et al. (2005) or Stochastic Gradient Descent.
However, learning the θ parameters in our case
is not as straightforward, since the objective is
non-differentiable at many points due to the `2`1
norm. We cannot simply throw in an off-the-shelf
gradient-based optimizer.

To alleviate the problem of non-differentiability,
the recently proposed FOrward-Backward Split-
ting (FOBOS) optimization framework (Duchi
and Singer, 2009) comes to mind. FOBOS is
a (sub-)gradient-based framework that solves the
optimization problem iteratively in two steps: in
each iteration, we first take a sub-gradient step,
and then take an analytical minimization step that
incorporates the regularization term, which can of-
ten be solved with a closed form solution. For-
mally, each iteration at timestamp t in FOBOS

consists of the following two steps:

θt+ 1
2

= θt − ηtgt (1)

θt+1 = argmin
θ

{
1

2
‖θ − θt+ 1

2
‖2 + ηtϕ(θ)

}
(2)

gt is the subgradient of −log(PLOP (y|x)) at θt.
Step (1) is just a regular gradient-based step,
where ηt is the step size and gt is the sub-gradient
vector of parameter vector θ. Step (2) finds a new
vector that stays close to the interim vector after
step (1), but also has a low penalty score accord-
ing to the regularization term ϕ(θ) (in our case,
ϕ(θ) = ‖θ‖2,1). In the case of `1 and `2 regular-
ization, each feature θi is independent in step (2),
and thus can be solved separately.2

Duchi and Singer (2009) gave closed form so-
lutions for solving the minimization problem in
step (2) for `1, `2, and `1`2 norms, but no past
research has demonstrated how to solve it for `2`1
norm. We leverage the results given by Kowal-
ski and Torrésani (2009) for elitist lasso, and show
that parameter θt+1,g,m (the mth feature of group
g at timestamp t+ 1) can be solved analytically as
follows:

θt+1,g,m = sign(θt+ 1
2
,g,m)

(
|θt+ 1

2
,g,m| − τg

)+
τg =

λ′

1 + λ′Mĝ(λ′)

∥∥∥xt+ 1
2
,g,1:Mĝ(λ′)

∥∥∥
1

sign(x) is the mathematical sign function. For x ∈
R, we have x+ = x if x ≥ 0 and x+ = 0 if x ≤ 0.
λ′ = ηtλ is the regularization weight adjusted by
the current step size.

Let θ̄ĝ denote a new vector that holds the posi-
tive absolute value of θĝ, sorted in descending or-
der, i.e., θ̄g,1 ≥ θ̄g,2 ≥ · · · θ̄g,M . Mĝ(λ′) is a posi-
tive integer given by the following definition:

θ̄g,Mĝ(λ′)+1 ≤
Mĝ(λ

′)+1∑
m=1

(
θ̄g,m − θ̄g,Mĝ(λ′)+1

)
and

θ̄g,Mĝ(λ′) > λ
′
Mĝ(λ

′)∑
m=1

(
θ̄g,m − θ̄g,Mĝ(λ′)

)
2This property turns out to be of great importance in real-

world large scale scenarios, since it allows the optimization
of high-dimensional feature vectors to be parallelized.

529



xt+ 1
2
,g,1:Mĝ(λ′)

is then the subset of features from
1 toMĝ(λ′) – in descending order of their absolute
value – in group g at timestamp t+ 12 .

One problem with finding the exact solution of
elitist lasso as illustrated above is that computing
the value Mĝ(λ′) involves sorting the parameter
vector θ, which is a O(n log n) operation. When
the number of features within each group is large,
this could be prohibitively expensive. An approx-
imate solution simply replaces Mĝ(λ′) with the
size of the group M . Kowalski and Torrésani
(2009) found that approximated elitist lasso works
nearly as well as the exact version, but is faster and
easier to implement. We adopt this approximation
in our experiments.

With this, we have described a general-purpose
method for solving any convex optimization prob-
lem with `2`1 norm.

6 Experiments

We evaluate the performance of our proposed
method on the task of Named Entity Recognition
(NER). The first dataset we evaluate on is the stan-
dard CoNLL-2003 English shared task benchmark
dataset (Sang and Meulder, 2003), which is a col-
lection of documents from Reuters newswire ar-
ticles, annotated with four entity types: Person,
Location, Organization, and Miscellaneous. We
adopt the BIO2 annotation standard. Beginning
and intermediate positions of an entity are marked
with B- and I- tags, and non-entities with O tag.
The training set contains 204K words (14K sen-
tences), the development set contains 51K words
(3.3K sentences), and the test set contains 46K
words (3.5K sentences). The second dataset is
the MUC6/7 set, which contains 255K words for
training and 59K words for testing. The MUC
data is annotated with 7 entity types. It is missing
the Miscellaneous entity type, but includes 4 ad-
ditional entity types that do not occur in CoNLL-
2003: Date, Time, Money, and Percent.

We used a comprehensive set of features from
Finkel et al. (2005) for training the MaxEnt model.
A total number of 437905 features were generated
on the CoNLL-2003 training dataset. The most
important features are listed in Table 1.

6.1 Experimental Setup

We tuned the regularization parameters in the `1,
`2 and `1`2 norms on the development dataset via
grid search. We used a tuning procedure similar to

– The word, word shape (e.g., whether capitalized, numeric, etc.), and
letter n-grams (up to 6gram) at current position

– The word and word shape of the previous and next position
– Previous word shape in conjunction with current word shape
– Disjunctive word set of the previous and next 4 positions
– Capitalization pattern in a 3 word window
– The current word matched against a list of name titles (e.g., Mr., Mrs.)
– Previous two words in conjunction with the word shape of the previous

word

Table 1: MaxEnt features.

the one used in Turian et al. (2010) where we eval-
uate results on the development set after each op-
timization iteration, and terminate the procedure
after not observing a performance increase on the
development set in 25 continuous iterations. We
found 150 to be a good λ value for `2, 0.1 for `1
and 0.1 for `2`1. Similarly, we tuned the num-
ber of experts for both LOP baselines and the eli-
tist LOP induction scheme. All model parameters
were initialized to a random value in [−0.1, 0.1].

Results are measured in precision (P), recall (R)
and F1 score. Statistical significance is measured
using the paired bootstrap resampling method
(Efron and Tibshirani, 1993). We compare our re-
sults against a MaxEnt baseline model with both
`1 and `2 regularization, as well as an automatic
expert induction model with `1 norm.

Two commonly used LOP expert induction
schemes were also compared. In the first scheme
(LOP random), experts are constructed by ran-
domly partitioning the features into n subsets,
where n is the number of experts. Each expert
therefore has 1n th of the full feature set. The sec-
ond scheme (LOP sequential) works in a similar
way, but instead of random partition, we create
feature subsets sequentially and preserve the rel-
ative order of the features. We also list results re-
ported in Smith et al. (2005) on the same dataset
for comparison. They experimented with three
manually crafted expert induction schemes (Smith
et al. 05 {simple;positional;label}) for LOP with
a Conditional Random Field (CRF), which is a
more powerful sequence model than our MaxEnt
baseline.

6.2 Main Results

Results on the CoNLL and MUC dataset are
shown in Table 2. The proposed automatic ex-
pert induction scheme (row elitist LOP) gives con-
sistent and statistically significant improvements
over the MaxEnt baselines on both CoNLL and
MUC test sets.

In particular, on the CoNLL test set, we ob-

530



Dev Set Test Set
Models # of experts Precision Recall F1 Precision Recall F1

CoNLL

MaxEnt-`2 1 88.46 87.73 88.09 81.42 81.64 81.53
MaxEnt-`1 1 88.46 89.63 89.04 82.20 84.24 83.21†

LOP random 2 88.75 90.79 89.76 82.73 85.84 84.25†‡

LOP sequential 2 88.68 90.79 89.72 83.03 86.03 84.50†‡

LOP-`1 5 88.76 90.41 89.58 82.90 85.94 84.40†‡

elitist LOP 3 89.65 91.08 90.36 83.96 86.67 85.29†‡

Smith et al. 05 simple 2 - - 90.26 - - 84.22
Smith et al. 05 positional 3 - - 90.35 - - 84.71
Smith et al. 05 label 5 - - 89.30 - - 83.27

MUC

MaxEnt-`2 1 91.47 86.20 88.76 89.06 80.44 84.53
MaxEnt-`1 1 92.56 85.50 88.89 89.61 79.09 84.02
LOP random 2 93.77 84.95 89.14 91.05 78.89 84.54
LOP sequential 2 94.34 84.18 88.97 91.71 77.42 83.96
LOP-`1 5 93.25 86.09 89.53 90.70 79.57 84.78‡

elitist LOP 3 93.47 86.48 89.84 91.22 80.15 85.33†‡

Table 2: Results of NER on CoNLL and MUC dataset. † and ‡ indicate statistically significantly better
F1 scores than the MaxEnt-`2 and MaxEnt-`1 baselines, respectively.

IV OOV
Models Precision Recall F1 Precision Recall F1

CoNLL MaxEnt-`2 89.21 86.81 88.00 85.69 80.61 83.08
elitist LOP 90.25 89.85 90.05† 86.30 86.67 86.49†

MUC MaxEnt-`2 90.21 80.88 85.29 84.90 78.80 81.74
elitist LOP 92.27 80.02 85.71 87.49 80.62 83.91†

Table 3: OOV and IV results breakdown. † indicates statistically significantly better F1 scores than the
MaxEnt-`2 baseline.

serve an improvement of 3.7% absolute F1 over
Maxent-`2. The gains are particularly large in re-
call (by 5% absolute score), although there is also
a 2.5% improvement in precision. The two con-
ventional LOP induction schemes also show sig-
nificant improvements over the MaxEnt baselines
on this dataset, with the sequential feature parti-
tion scheme working slightly better than the ran-
dom feature partition. However, elitist LOP out-
performs the two LOP schemes by as much as 1%
in absolute F1, and also gives better results than
manually crafted experts for CRFs in Smith et al.
(2005).

On the MUC test set, while elitist LOP still out-
performs the MaxEnt baselines, the LOP schemes
do not help or in some cases hurt performance.
This suggests the lack of robustness of LOP which
was discussed in Section 5. The automatically
learned LOP experts are more robust on this data
set, and gives a 0.8% improvement measured in
absolute F1 score over the MaxEnt-`2 baseline.

The elitist LOP model is more expressive than
MaxEnt since it has more parameters to be com-
bined. To understand whether performance gain is
obtained by the expressiveness or by the regular-
ization of `2`1 norm, we compare against an elitist

LOP model with just `1 regularization (LOP-`1).
Results show that the `2`1 norm is a better regular-
izer for avoiding overfitting with these models. We
also see a significant gain in LOP-`1 over MaxEnt-
`1, suggesting that the expressiveness of the model
also helps.

6.3 Out-of-vocabulary vs. In-vocabulary

To further understand how our method improves
over the MaxEnt baseline, we break down the test
results into two subsets: words that were seen in
the training dataset (in-vocabulary, or IV), versus
words that were not (out-of-vocabulary, or OOV).
We removed the entity boundary tags (e.g. “I-
ORG” becomes “ORG”) so that each word token
is evaluated separately. Dividing IV and OOV sub-
sets can be done by checking each word against a
lexicon compiled from training dataset.

If our automatic expert induction LOP method
does successfully mitigate the weight undertrain-
ing problem, we would expect to see an improve-
ment in OOV recognition. The result of this anal-
ysis is shown in Table 3.

On the CoNLL dataset, our method gives sig-
nificant improvements over MaxEnt in both OOV
and IV category. In particular, improvement from

531



OOV subset (3.4% in F1) is larger than the im-
provement from IV subset (2% in F1). This
matches that our hypothesis that our method mit-
igates the weight undertraining problem and thus
gives a stronger boost in OOV recognition.

This effect is even more pronounced on the
MUC dataset. The improvement in IV subset in
this case is actually quite modest (0.5%), but we
get a significant 2.2% improvement from the OOV
subset.

7 Related Work

Beyond the several aforementioned works that ad-
dress the issue of weight undertraining, another
recent work that looks at this problem is Wang
and Manning (2013). They examined the objective
function of dropout training prescribed by Hinton
et al. (2012), and proposed an approximation of
dropout by directly sampling from a Gaussian ap-
proximation. The resulting algorithm is orders of
magnitude faster than the iterative algorithm given
by Hinton et al. (2012). It will be interesting to
compare our proposed method with this method in
the future.

The use of group-sparsity norms in NLP re-
search is relatively rare. Martins et al. (2011) pro-
posed the use of structure-inducing norms, in par-
ticular `1`2 norm (group lasso), for learning the
structure of classifiers. A key observation is that
during testing, most of the runtime is consumed
in the feature instantiation stage. Since `1`2 norm
discards whole groups of features at a time, there
are fewer feature templates that need to be instan-
tiated, therefore it could give a significant runtime
speedup.

Our use of `2`1 norm takes a different flavor
in that we explore the internal structure of the
model. There is, however, one recent paper that
also makes use of `2`1 norm for a similar reason.
Das and Smith (2012) employed `2`1 norm for
learning sparse structure in a network formed by
lexical predicates and semantic frames. Their re-
sults show that `2`1 norm yields much more com-
pact models than `1 or `2 norms, with superior per-
formance in learning to expand lexicons.

However, the optimization problem in Das and
Smith (2012) was much simpler since all parame-
ters can only take on positive values, and therefore
they could directly use a gradient-descent method
with specialized edge condition checks. In our
work, we have shown a general-purpose method

in the FOBOS framework for optimizing any con-
vex function with `2`1 norm.

A related `1-`2 mixed norm is called elastic net
(Zou and Hastie, 2005). It was proposed to over-
come a problem of lasso (i.e., `1 regularization),
which occurs when there is a group of correlated
predictors, and lasso tends to pick one and ignore
all the others. Elastic net takes the form of a sum
of a `1 and a `2 norm. It was used in Lavergne
et al. (2010) for learning large-scale Conditional
Random Fields.

Another popular approach that also explores
diversity in model predictions is system com-
bination, which is related to bagging predictors
(Breiman, 1996). For example, Sang et al. (2000)
reported that by combing systems using differ-
ent tagging representation and diverse classifier
models (e.g., support vector machine, logistic re-
gression, naive Bayes), they were able to achieve
significantly higher accuracy than any individual
classifier alone. This approach has also been ap-
plied extensively in Machine Translation (DeNero
et al., 2010) and Speech Recognition (Mikolov et
al., 2011).

8 Conclusion

Our results show that previous automatic expert
definition methods used in logarithmic opinion
pools lack robustness across different tasks and
data sets. Instead of manually defining good
(i.e., diverse) experts, we demonstrated an effec-
tive way to induce experts automatically, by us-
ing a sparsity-inducing mixed `1`2 norm inspired
by elitist lasso. We proposed a novel formulation
of the optimization problem with `2`1 norm in the
FOBOS framework. Our method gives consistent
and significant improvements over a MaxEnt base-
line with fine-tuned `2 regularization on two NER
datasets. The gains are most evident in recogniz-
ing entity types for out-of-vocabulary words.

Acknowledgments

The authors would like to thank Rob Voigt and the
three anonymous reviewers for their valuable com-
ments and suggestions. We gratefully acknowl-
edge the support of the DARPA Broad Operational
Language Translation (BOLT) program through
IBM. Any opinions, findings, and conclusion or
recommendations expressed in this material are
those of the authors and do not necessarily reflect
the view of the DARPA or the US government.

532



References

Rie K. Ando and Tong Zhang. 2005. A framework
for learning predictive structures from multiple tasks
and unlabeled data. Journal of Machine Learning
Research, 6:1817–1853.

Leo Breiman. 1996. Bagging predictors. Machine
Learning, 24:123–140.

Dipanjan Das and Noah A. Smith. 2012. Graph-based
lexicon expansion with sparsity-inducing penalties.
In Proceedings of the 2012 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics: Human Language Technologies
(NAACL-HLT).

John DeNero, Shankar Kumar, Ciprian Chelba, and
Franz Och. 2010. Model combination for machine
translation. In Proceedings of the 11th Annual Con-
ference of the North American Chapter of the Asso-
ciation for Computational Linguistics (NAACL).

John Duchi and Yoram Singer. 2009. Efficient online
and batch learning using forward backward splitting.
Journal of Machine Learning Research, 10:2899–
2934.

Bradley Efron and Robert J. Tibshirani. 1993. An In-
troduction to the Bootstrap. Chapman & Hall, New
York.

Jenny R. Finkel, Trond Grenager, and Christopher D.
Manning. 2005. Incorporating non-local informa-
tion into information extraction systems by Gibbs
sampling. In Proceedings of the 43nd Annual Meet-
ing of the Association for Computational Linguistics
(ACL).

Tom Heskes. 1998. Selecting weighting factors in log-
arithmic opinion pools. In Proceedings of Advances
in Neural Information Processing Systems (NIPS)
10.

Geoffery Hinton, Nitish Srivastava, Alex Krizhevsky,
Ilya Sutskever, and Ruslan Salakhutdinov.
2012. Improving neural networks by prevent-
ing co-adaptation of feature detectors. CoRR,
abs/1207.0580.

Geoffery Hinton. 1999. Products of experts. In Pro-
ceedings of the Ninth International Conference on
Artificial Neural Networks (ICANN).

Matthieu Kowalski and Bruno Torrésani. 2009. Spar-
sity and persistence: mixed norms provide simple
signal models with dependent coefficients. Signal,
Image and Video Processing, 3(3):251–264.

Thomas Lavergne, Olivier Cappé, and François Yvon.
2010. Practical very large scale CRFs. In Proceed-
ings of the 48th Annual Meeting of the Association
for Computational Linguistics (ACL).

André F. T. Martins, Noah A. Smith, Pedro M. Q.
Aguiar, and Mário A. T. Figueiredo. 2011. Struc-
tured sparsity in structured prediction. In Proceed-
ings of the Conference on Empirical Methods on
Natural Language Processing (EMNLP).

Tomas Mikolov, Anoop Deoras, Stefan Kombrink,
Lukas Burget, and Jan Cernocky. 2011. Empirical
evaluation and combination of advanced language
modeling techniques. In Proceedings 12th Annual
Conference of the International Speech Communi-
cation Association (INTERSPEECH).

Erik F. Tjong Kim Sang and Fien De Meulder.
2003. Introduction to the CoNLL-2003 shared task:
language-independent named entity recognition. In
Proceedings of the 7th Conference on Natural lan-
guage learning (CoNLL).

Erik F. Tjong Kim Sang, Walter Daelemans, Hervé
Déjean, Rob Koeling, Yuval Krymolowski, Vasin
Punyakanok, and Dan Roth. 2000. Applying sys-
tem combination to base noun phrase identication.
In Proceedings of the 18th International Conference
on Computational Linguistics (COLING).

Andrew Smith, Trevor Cohn, and Miles Osborne.
2005. Logarithmic opinion pools for conditional
random fields. In Proceedings of the 43th Annual
Meeting of the Association for Computational Lin-
guistics (ACL).

Charles Sutton, Michael Sindelar, and Andrew McCal-
lum. 2007. Reducing weight undertraining in struc-
tured discriminative learning. In Proceedings of Hu-
man Language Technologies: The Annual Confer-
ence of the North American Chapter of the Associa-
tion for Computational Linguistics (NAACL-HLT).

Joseph Turian, Lev Ratinov, and Yoshua Bengio. 2010.
Word representations: A simple and general method
for semi-supervised learning. In Proceedings of the
48th Annual Meeting of the Association for Compu-
tational Linguistics (ACL).

Sida I. Wang and Christopher D. Manning. 2013. Fast
dropout training. In Proceedings of the 30th Inter-
national Conference on Machine Learning (ICML).

Hui Zou and Trevor Hastie. 2005. Regularization and
variable selection via the elastic net. Journal of the
Royal Statistical Society, Series B, 67:301–320.

533


