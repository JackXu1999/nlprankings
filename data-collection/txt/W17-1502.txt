



















































Enriching Basque Coreference Resolution System using Semantic Knowledge sources


Proceedings of the 2nd Workshop on Coreference Resolution Beyond OntoNotes (CORBON 2017), co-located with EACL 2017, pages 8–16,
Valencia, Spain, April 4, 2017. c©2017 Association for Computational Linguistics

Enriching Basque Coreference Resolution System using Semantic
Knowledge sources

Ander Soraluze and Olatz Arregi and Xabier Arregi and Arantza Dı́az de Ilarraza
University of the Basque Country
Donostia - San Sebastián, Spain

{ander.soraluze,olatz.arregi, xabier.arregi,a.diazdeilarraza}@ehu.eus

Abstract

In this paper we present a Basque coref-
erence resolution system enriched with se-
mantic knowledge. An error analysis car-
ried out revealed the deficiencies that the
system had in resolving coreference cases
in which semantic or world knowledge is
needed. We attempt to improve the de-
ficiencies using two semantic knowledge
sources, specifically Wikipedia and Word-
Net.

1 Introduction

Coreference resolution consists of identifying tex-
tual expressions (mentions) that refer to real-world
objects (entities) and determining which of these
mentions refer to the same entity. While differ-
ent string-matching techniques are useful to deter-
mine which of these mentions refer to the same
entity, there are cases in which more knowledge is
needed, that is the case of the Example in 1.

(1) [Osasunak] lehenengo mailara igotzeko
lehian azken astean bizi duen giroa oso
polita da. [Taldea] lasaitzeko asmoz
Oronozera eraman zituen Lotinak atzo
guztiak. Oronozko kontzentrazioa behar-
rezkoa dute [gorritxoek].
“[Osasuna] is going through a beautiful
moment in the last week in the race to as-
cend to the Premier League. In order to
reassure [the team] Lotina has decided to
give all of them to Oronoz. [The reds]
need to concentrate in Oronoz.”

Having the world knowledge that Osasuna is a
football team and its nickname is the reds would
be helpful for establishing the coreference rela-
tions between the mentions [Osasuna], [Taldea]
and [gorritxoek] in the example presented above.

Evaluation scores used in coreference resolu-
tion tasks can show how effective a system is;
however, they neither identify deficiencies of the
system, nor give any indication of how those er-
rors might be corrected. Error analyses are a good
option that can help to clear the deficiencies of
a coreference resolver. Bearing this in mind, we
have carried out an error analysis of the extended
version of the coreference resolution system pre-
sented in Soraluze et al. (2015). In this paper we
present an improvement of this Basque corefer-
ence resolution system by using semantic knowl-
edge sources in order to correctly resolve cases
like in Example 1.

This paper is structured as follows. After pre-
senting an error analysis of the coreference resolu-
tion system in Section 2, we analyse similar works
to ours in which semantic knowledge sources have
been used to improve coreference resolution in
Section 3. Section 4 presents how we integrated
the semantic knowledge in our system. The main
experimental results are outlined in Section 5 and
discussed in Section 6. Finally, we review the
main conclusions and preview future work.

2 Error Analysis

A deep error-analysis can reveal the weak points
of the coreference resolution system and help to
decide future directions in the improvement of the
system. The system we have evaluated is an adap-
tation of the Stanford Coreference resolution sys-
tem (Lee et al., 2013) to the Basque language. The
Stanford coreference resolution module is a deter-
ministic rule-based system which is based on ten
independent coreference models or sieves that are
precision-oriented, i.e., they are applied sequen-
tially from highest to lowest precision. All the
sieves of the system have been modified taking
into account the characteristics of the Basque lan-

8



guage and, one new sieve has been added, obtain-
ing an end-to-end coreference resolution system.

The corpus used to carry out the error analysis is
a part of EPEC (the Reference Corpus for the Pro-
cessing of Basque) (Aduriz et al., 2006). EPEC
is a 300,000 word sample collection of news pub-
lished in Euskaldunon Egunkaria, a Basque lan-
guage newspaper. The part of the corpus we have
used has about 45,000 words and it has been man-
ually tagged at coreference level by two linguists
(Ceberio et al., 2016). First of all, automatically
tagged mentions obtained by a mention detector
(Soraluze et al., 2016) have been corrected; then,
coreferent mentions have been linked in clusters.

More detailed information about the EPEC cor-
pus can be found in Table 1.

Words Mentions Clusters Singletons
Devel 30434 8432 1313 4383
Test 15949 4360 621 2445

Table 1: EPEC corpus division information

2.1 Error types
The errors have been classified following the cat-
egorization presented in Kummerfeld and Klein
(2013). The tool1 presented in the paper has been
used to help in identifying and quantifying the er-
rors produced by the coreference resolution sys-
tem:

• Span Error (SE): A mention span has been
identified incorrectly.

• Conflated Entities (CE): Two entities have
been unified creating a new incorrect one.

• Extra Mention (EM): An entity includes an
incorrectly identified mention.

• Extra Entity (EE): An entity which consists
of incorrectly identified mentions is outputted
by the system.

• Divided Entity (DE): An entity has been di-
vided in two entities.

• Missing Mention (MM): A not identified
mention is missing in an entity.

• Missing Entity (ME): The system misses an
entity which is present in the gold standard.

The error types are summarised in Table 2.
1code.google.com/p/berkeley-coreference-analyser/

Error Type System Gold
Span Error s1 s1 s2

Conflated Entities {m1, m2}e1 {m1}e1
- {m2}e2

Extra Mention {m1, m2} {m1}
Extra Entity {m1, m2} -

Divided Entity {m1}e1 {m1, m2}e1
{m2}e2 -

Missing Mention {m1} {m1, m2}
Missing Entity - {m1, m2}

Table 2: Error types. s=string, m=mention,
e=entity

2.2 Error causes
Apart from classifying the errors committed by the
coreference resolution system, it is important to
observe the causes of these error types. These are
the causes of errors we found:

• Preprocessing (PP): Errors in the prepro-
cessing step (lemmatization, PoS tagging,
etc.) provoke incorrect or missing links in
coreference resolution.

• Mention Detection (MD): These errors are
provoked due to incorrectly identified (not a
mention, incorrect boundaries..) or missed
mentions during mention detection step.
Missed mentions directly affect the recall of
the system, and incorrectly identified men-
tions affect precision.

• Pronominal Resolution (PR): The system
often generates incorrect links between the
pronoun and its antecedent.

• Ellipsis Resolution (ER): Elliptical men-
tions do not provide much information as
they omit the noun, as a consequence it is dif-
ficult to correctly link these types of mentions
with their correct antecedent.

For example, it is complicated to link the
elliptical mention [Yosi Beilin Israelgo Jus-
tizia ministroak Jeruralemi buruz esandako-
Ø2-ak] “what Yosi Beilin Israel Justice Min-
ister said” with its antecedent [Beilin Jus-
tizia ministroaren hitzak] “Beilin Justice min-
ister’s words”.

• Semantic Knowledge (SK): Errors related to
a semantic relation (synonymy, hyperonymy,
metonymy) between the heads of two men-
tions.

2In this case Ø refers to “what”.

9



For example, in mentions [Libanoko Par-
lamentuak] “Lebanon parliament” and
[Libanoko Legebiltzarrak] “Lebanon par-
liament”, parlamentua is a synomyn of
legebiltzarra.

• World Knowledge (WK): In some cases the
system is not able to link mentions as a con-
sequence of the lack of world knowledge re-
quired to resolve them correctly.

For example, to link the mention [Reala] “Re-
ala” with the mention [talde txuri-urdinak]
“white-blue team”, it is necessary to know
that Reala is a team and the nickname of the
football team is txuri-urdinak “white-blue”.

• Miscellaneous (MISC): In this category we
classify the errors that are not contained in
the above categories.

An example of a miscellaneous error could
be the following. The mention [Kelme, Eu-
skaltel eta Lampre] should be linked with the
mention [Hiru taldeak] “The three teams”. In
this specific example it is necessary to know
that Kelme, Euskaltel and Lampre are teams
and the enumerated mention has three ele-
ments.

After defining the error types and the error
causes, we analysed how the error causes affect
the error types in EPEC corpus. The distribution
of errors is shown in Figure 1.

As we observe in Figure 1, the most com-
mon errors types of the system fail in Span Error
(29.36%), Conflated Entities (11.92%), Divided
Entities (42.88%) and Missing Mention (11.92%)
categories.

Observing the error causes, we can conclude
that mention detection is crucial for coreference
resolution, 52.52% of errors. Improving men-
tion detection would likely improve the scores ob-
tained in coreference resolution. Nevertheless, in
order to identify deficiencies of a coreference res-
olution system, Pronominal Resolution (9.17%),
Ellipsis Resolution (3.21%), Semantics (6.42%)
and World Knowledge (9.86%) categories can re-
veal how the errors might be corrected. Due to the
variety of errors classified in miscellaneous cat-
egory, little improvement would be achieved de-
spite making a big effort to solve them.

Among all the error causes, in this paper we are
going to focus on errors provoked by the lack of

semantic and world knowledge.

3 Related Work

Lexical and encyclopedic information sources,
such as WordNet, Wikipedia, Yago or DBPedia
have been widely used to improve coreference res-
olution.

WordNet (Fellbaum, 1998) is the one of old-
est resources for lexical knowledge. It consists
of synsets, which link synonymous word senses
together. Using WordNet’s structure, it is possi-
ble to find synonyms and hyperonymic relations.
Wikipedia is a collaborative open source encyclo-
pedia edited by volunteers and provides a very
large domain-independent encyclopedic reposi-
tory. Yago (Suchanek et al., 2007) is a knowl-
edge base, linking Wikipedia entries to the Word-
Net ontology. And finally, DBPedia (Mendes et
al., 2012) contains useful ontological information
extracted from the data in Wikipedia.

Regarding works in which lexical and ency-
clopedic information sources have been exploited,
Ponzetto and Strube (2006) were the earliest to use
WordNet and Wikipedia.

Uryupina et al. (2011) extracted semantic com-
patibility and aliasing information from Wikipedia
and Yago and incorporated it in coreference res-
olution system. They showed that using such
knowledge with no disambiguation and filtering
does not bring any improvement over the baseline,
whereas a few very simple disambiguation and fil-
tering techniques lead to better results. In the end,
they improve their system’s performance by 2-3
percentage points.

Rahman and Ng (2011) used Yago to inject
knowledge attributes in mentions, but noticed that
knowledge injection could be noisy.

Durrett and Klein (2013) observed that the se-
mantic information contained even in a corefer-
ence corpus of thousands of documents is insuf-
ficient to generalize to unseen data, so system de-
signers have turned to external resources. Using
specialised features, as well as WordNet-based hy-
pernymy and synonymy and other resources, they
obtained a gain from 60.06 in CoNLL score to
61.58 using automatic mentions, and from 75.08
to 76.68 with gold mentions.

Ratinov and Roth (2012) extract attributes from
Wikipedia pages which they used to improve the
recall in their system, based on a hybrid (Lee et
al., 2013).

10



Figure 1: Distribution of error causes into error types.

In Hajishirzi et al. (2013) NECo, a new model
for named entity linking and coreference resolu-
tion, which solves both problems jointly, reduc-
ing the errors of each is introduced. NECo ex-
tends the Stanford deterministic coreference reso-
lution system by automatically linking mentions to
Wikipedia and introducing new sieves which profit
from information obtained by named entity link-
ing.

As pointed out in Recasens et al. (2013), opaque
mentions (mentions with very different words like
Google and the search giant) account for 65%
of the errors made by state-of-the-art systems, so
to improve coreference scores beyond 60-70% it
is necessary to make better use of semantic and
world knowledge to deal with non-identical-string
coreference. They use a corpus of comparable
documents to extract aliases and they report that
their method not only finds synonymy and in-
stance relations, but also metonymic cases. They
obtain a gain of 0.7% F1 score for the CoNLL met-
ric using gold mentions.

Lee et al. (2013) mention that the biggest chal-
lenge in coreference resolution, accounting for
42% of errors in the state-of-the art Stanford sys-
tem, is the inability to reason effectively about
background semantic knowledge.

The intuition behind the work presented in Dur-

rett and Klein (2014) is that named entity recogni-
tion on ambiguous instances can obtain benefit us-
ing coreference resolution, and similarly can ben-
efit from Wikipedia knowledge. At the same time,
coreference can profit from better named entity in-
formation.

4 Improving Coreference Resolution
with Semantic Knowledge sources

This section explains the improvement process of
the coreference resolution system with semantic
knowledge sources. In order to treat cases where
knowledge is needed, two new specialised sieves
have been added to the coreference resolution sys-
tem: One to extract knowledge from Wikipedia
and the other to obtain semantic information from
WordNet.

4.1 Enriching mentions with Named Entity
Linking

Named Entity Linking is the task of matching
mentions to corresponding entities in a knowledge
base, such as Wikipedia.

As pointed out in Versley et al. (2016), named
entity linking, or disambiguation of entity men-
tions, is beneficial to make full use of the infor-
mation in Wikipedia.

The Basque version of Wikipedia, contained

11



about 258,000 articles in September 2016, which
is much smaller in size when compared with En-
glish Wikipedia, which contained about 5,250,837
pages on the same date.

In order to disambiguate and link mentions to
Basque Wikipedia pages, the following formula
has been applied to all the named entity mentions
in a document:

P (s, c, e) = P (e | s)P (e | c)
P (e | s) is the probability of being entity e

given s string, i.e., the normalised probability of
being entity e linked with string s in Wikipedia.
P (e | c) is the probability of being entity e given
the context c. The context c is a window of
size [−50, +50] of the string s. To calculate
P (e | c) probability, UKB3 software has been
utilised. UKB software uses Personalized Page
Rank algorithm presented in (Agirre and Soroa,
2009) and (Agirre et al., 2014) to estimate the
probabilities.

If a named-entity mention is linked with any
page from Wikipedia, the page that UKB says it
is the most probable is used to enrich the mention.
From the Wikipedia page the following informa-
tion is obtained:

• The title of the page. The title some-
times gives useful information. For exam-
ple, for the named-entity mention AEK, the
title of its Wikipedia page is Alfabetatze
Euskalduntze Koordinakundea “Literacy and
Euskaldunization Coordinator”, where the
extent of the acronym is obtained. Further-
more it gives the information that AEK is a
coordinator, koordinakundea.

• The first sentence. The first paragraph of each
Wikipedia article provides a very brief sum-
mary of the entity. Usually the most use-
ful information is in the first sentence, this is
where the entity is defined.

• If the Wikipedia page has an Infobox, we ex-
tract information from it. Infoboxes contain
structured information in which the attributes
of many entities are listed in a standardized
way.

After the information is obtained from the
Wikipedia page, this information is processed and
the NPs are extracted.

3http://ixa2.si.ehu.es/ukb/

These NPs and their sub-phrases are used to en-
rich the mentions with world knowledge. To fur-
ther reduce the noise, the NPs that are location
named-entities in a Wikipedia page about a loca-
tion are discarded.

Taking Example 1, the mention Osasuna is en-
riched as follows: The most probable Wikipedia
page proposed by UKB for the mention Os-
asuna is Osasuna futbol kluba “Osasuna foot-
ball club”. Therefore, we obtain from this page
the title, the first sentence and Infobox infor-
mation. The NPs obtained after the informa-
tion is processed are gorritxoak “the reds”, Os-
asuna futbol kluba “Osasuna football club” and
Nafarroako futbol taldea “football team from
Navarre”. So the mention Osasuna is enriched
with the set of lemmas of the NPs and the
lemmas of their sub-phrases: {gorritxo, Os-
asuna futbol klub, futbol klub, klub, Nafarroa fut-
bol talde, futbol talde, talde} “{the reds, Os-
asuna football club, football club, club, foot-
ball team from Navarre, football team, team}”.

4.2 Wiki-alias sieve

The new Wiki-alias sieve uses the mentions en-
riched by information obtained from Wikipedia
pages.

Using this information, the Wiki-alias sieve as-
sumes that two mentions are coreferent if one of
the two following conditions is fulfilled:

i) the set of enriched word lemmas in the po-
tential antecedent has all the mention candidate’s
span lemmas. To better understand this constraint,
suppose that the mention Realak is enriched with
{talde, futbol talde, txuri-urdin} “{team, foot-
ball team, white and blue}”, as the potential an-
tecedent Realak has all the lemmas in the mention
candidate’s span, i.e., talde “{team}” and txuri-
urdin “{white and blue}”, the mention talde txuri-
urdinak “{white and blue team}” is considered
coreferent of Realak.

ii) the head word lemma of the mention can-
didate is equal to the head word lemma of the
potential antecedent or equal to any lemma in
the set of enriched lemmas of the potential an-
tecedent, and all the enriched lemmas of the po-
tential antecedent appear in the cluster lemmas
of the mention candidate. For example, this
constraint considers coreferent the potential an-
tecedent Jacques Chiracek and the mention can-
didate Jacques Chirac Frantziako errepublikako

12



presidentea. After Jacques Chiracek mention has
been enriched with lemmas {presidente, Frantzia
presidente} “{president, France president}”, the
head word lemma of the mention candidate pres-
idente is equal to a lemma in the set of enriched
lemmas of the potential antecedent presidente and
all the enriched lemmas of the potential antecedent
appear in the cluster lemmas of the mention can-
didate, so the second constraint is fulfilled. This
constraint aims to link coreferent mentions where
a mention with novel information appears later in
text than the less informative one. As pointed
out in Fox (1993), it is not common to introduce
novel information in later mentions but it some-
times happens.

4.3 Synonymy sieve

To create this new sieve, we have extracted from
Basque WordNet (Pociello et al., 2011) all the
words that are considered synonyms in this on-
tology. The Basque WordNet contains 32,456
synsets and 26,565 lemmas, and is complemented
by a hand-tagged corpus comprising 59,968 anno-
tations (Pociello et al., 2011).

From all synsets, a static list of 16,771 sets
of synonyms has been created and integrated in
the coreference resolution system. Using the syn-
onyms’ static list, the Synonymy sieve considers
two mentions as coreferent if the following con-
straints are fulfilled: i) the head word of the po-
tential antecedent and the head word of the men-
tion candidate are synonyms and ii) all the lemmas
in the mention candidate’s span are in the poten-
tial antecedent cluster word lemmas or vice versa.
For example, the mention candidate Libanoko leg-
ebiltzarra “Lebanon parliament” and the Libanoko
parlametua “Lebanon parliament” are considered
coreferent as the head words legebiltzarra and par-
lamentua are synonyms and the lemma Libano
“Lebanon” of the word Libanoko is present in the
cluster word lemmas of the potential antecedent.

5 System evaluation

In order to quantify the impact of using seman-
tic knowledge sources in coreference resolution,
we have tested the enriched coreference resolu-
tion system using the EPEC corpus and compared
the results with the baseline system. The exper-
imentation has been carried out using automatic
mentions and gold mentions. In both cases named
entity disambiguation and entity linking has been

performed automatically.

5.1 Metrics

The metrics used to evaluate the systems’ perfor-
mances are MUC (Vilain et al., 1995), B3 (Bagga
and Baldwin, 1998), CEAFe (Luo, 2005), CEAFm
(Luo, 2005), BLANC (Recasens and Hovy, 2011)
and LEA (Moosavi and Strube, 2016). The
CoNLL metrics is the arithmetic mean of MUC,
B3 and CEAFe metrics. The scores have been cal-
culated using the reference implementation of the
CoNLL scorer (Pradhan et al., 2014).

5.2 Experimental results

As pointed out in Rahman and Ng (2011), while
different knowledge sources have been shown to
be useful when applied in isolation to a corefer-
ence system, it is also interesting to observe if
they offer complementary benefits and can there-
fore further improve a resolver when applied in
combination. In order to quantify the individual
improvement of each new sieve, we compared the
baseline system (1) with the system in which the
wiki-alias sieve has been added (2), with the one
where the synonymy sieve has been added (3), and
with the final system combining both sieves (4).

Table 3 shows the results obtained by the base-
line system compared with those obtained by
the coreference resolution system, which uses
semantic knowledge sources. These scores are
obtained with automatically detected mentions
(F1 =77.57).

The scores obtained by systems using the gold
mentions (F1 =100), i.e., when providing all the
correct mentions to the coreference resolution sys-
tems, are shown in Table 4.

6 Discussion

Observing the results presented in Table 3, we can
see that the baseline system’s F1 scores are out-
performed in all the metrics by the semantically
enriched system. In CoNLL metric, the improved
system has a score of 55.81, which is slightly
higher than the baseline system, to be precise, 0.24
higher.

As shown in Table 4, the baseline F1 scores are
also outperformed in all the metrics, except in B3

when gold mentions are used. The official CoNLL
metric is improved by 0.39 points.

Regarding recall and precision scores when au-
tomatic and gold mentions are used, all the metrics

13



Automatic Mentions
MUC B3 CEAFm CEAFe BLANC LEA CoNLL

R P F1 R P F1 R P F1 R P F1 R P F1 R P F1 F1
1 34.1 55.76 42.32 57.98 68.83 62.94 60.78 62.31 61.54 66.02 58.41 61.98 38.41 53.57 43.18 46.71 51.78 49.12 55.74
2 34.41 55.70 42.54 58.09 68.64 62.93 60.73 62.26 61.49 65.94 58.49 61.99 38.65 53.27 43.35 46.82 51.64 49.11 55.82
3 34.57 56.03 42.76 58.08 68.80 62.98 60.85 62.38 61.61 65.99 58.51 62.03 38.53 53.65 43.31 46.83 51.97 49.27 55.92
4 34.88 55.90 42.95* 58.19 68.60 62.97 60.80 62.33 61.56 65.92 58.60 62.04 38.77 53.33 43.48* 46.94 51.83 49.26 55.98*

Table 3: Results obtained when automatic mentions are used. 1=Baseline, 2=1+Wiki sieve,
3=2+Synonymy sieve, 4=1+Wiki sieve+Synonymy sieve.

Gold Mentions
MUC B3 CEAFm CEAFe BLANC LEA CoNLL

R P F1 R P F1 R P F1 R P F1 R P F1 R P F1 F1
1 48.76 71.94 58.12 81.35 93.47 86.99 80.57 80.57 80.57 89.00 78.24 83.27 67.09 84.65 72.77 66.36 71.11 68.66 76.12
2 49.84 70.81 58.50 81.71 92.83 86.92 80.57 80.57 80.57 88.69 78.77 83.44 67.51 83.27 72.84 66.60 71.01 68.73 76.28
3 50.00 71.50 58.85 81.69 93.19 87.06 80.80 80.80 80.80 88.90 78.82 83.56 67.39 84.23 72.95 66.68 71.52 69.02 76.49
4 50.46 70.99 58.99* 81.86 92.81 86.99 80.71 80.71 80.71 88.71 79.00 83.57* 67.68 83.34 73.00 66.79 71.29 68.97 76.51*

Table 4: Results obtained when gold mentions are used. 1=Baseline, 2=1+Wiki sieve, 3=2+Synonymy
sieve, 4=1+Wiki sieve+Synonymy sieve.

except CEAFe show an improvement in recall and
decrease in precision when two new sieves are ap-
plied. The reason why the CEAFe metric is behav-
ing differently could be that, as mentioned by De-
nis and Baldridge (2009), CEAF ignores all cor-
rect decisions of unaligned response entities. Con-
sequently, the CEAF metric may lead to unreliable
results.

It is interesting to compare the improvements
obtained by the system which uses semantic
knowledge sources in CoNLL scores. The im-
provement when automatic mentions are used is
lower than when gold mentions are provided, 0.24
and 0.39 respectively. In both cases, even the im-
provements obtained are modest, they are statisti-
cally significant using Paired Student’s t-test with
p-value < 0.05.

As pointed out in Versley et al. (2016), in real-
istic settings, where the loss in precision would be
amplified by the additional non-gold mentions, it
is substantially harder to achieve gains by incorpo-
rate lexical and encyclopedic knowledge, but pos-
sible and necessary. A similar idea is concluded by
Durrett and Klein (2013). They mention that de-
spite the fact that absolute performance numbers
are much higher on gold mentions and there is less
room for improvement, the semantic features help
much more than they do in system mentions.

To conclude the analysis of the results, it is also
interesting to observe the difference between the
results obtained by both systems when automatic
mentions and when gold mentions are used. It
is clear that having accurate preprocessing tools
and a good mention detector are crucial to obtain
good results in coreference resolution. In both sys-

tems the difference in CoNLL score is about 20.00
points higher when gold mentions are used.

The results obtained have enabled us to carry
out a new error analysis in the development set.
After applying the new two sieves, the error anal-
ysis has revealed four major issues that directly
affect not obtaining bigger improvement when
knowledge resources are used:

1. Some mentions do not have Wikipedia en-
try, as the coverage of Basque Wikipedia
(257,546 pages) has less coverage than other
languages, for example English (5,250,837
pages), i.e., Basque version is 21 times
smaller.

2. Due to incorrect mention disambiguation,
some mentions are linked to incorrect
Wikipedia pages. The precision obtained in
disambiguation is 87,84%.

3. Precision errors, provoked by cases where
many proper noun mentions were potential
antecedent for a common noun. For exam-
ple, Oslo is linked by hiriburu “capital”, nev-
ertheless the correct antecedent for hiriburu
is another capital that appears in text, in this
specific case, Jerusalem.

4. Some indefinite mentions which do not have
antecedent are linked incorrectly. For exam-
ple, estaturik “state” is linked with Frantziak
“France”.

5. In the synonyms’ static list, some synonyms
that appear in texts are missing. In addi-
tion, many synonyms are so generic, i.e., they

14



are synonyms depending on the context in
which they appear. As a consequence of
missing synonyms, some mentions with syn-
onymy relations between them are not linked.
The presence of very generic synonyms pro-
vokes to incorrectly link mentions that are not
coreferent, so that precision decreases. Iden-
tifying the particular sense that a word has in
context would likely help to improve the pre-
cision.

Regarding the issues that affect improvement
of the systems when knowledge bases are used,
Uryupina et al. (2011) suggest that in their partic-
ular case the errors introduced are not caused by
any deficiencies in web knowledge bases, but re-
flect the complex nature of the coreference resolu-
tion task.

7 Conclusions and future work

We have enriched the Basque coreference resolu-
tion adding new two sieves, Wiki-alias and Syn-
onymy sieve, respectively. The first sieve uses the
enriched information of named-entity mentions af-
ter they have been linked to their correspondent
Wikipedia page, using Entity Linking techniques.
The second sieve uses a static list of synonyms ex-
tracted from Basque WordNet to consider whether
two mentions are coreferent.

Applying the two new sieves, the system ob-
tains an improvement of 0.24 points in CoNLL F1
when automatic mentions are used and the CoNLL
score is outperformed by 0.39 points when the
gold mentions are provided. The error analysis of
the enriched system has revealed that the knowl-
edge bases used, Basque Wikipedia and Basque
WordNet, have deficiencies in their coverage com-
pared with knowledge bases in major languages,
for example, English. We suggest that there is
margin of improvement, as Basque Wikipedia and
Basque WordNet coverage increase, bearing in
mind that coreference resolution is a complex task.

As future work, we intend to improve the Pro-
noun resolution and Ellipsis Resolution, as we ob-
served in the error analysis presented in Section 2
they are the cause of considerable coreference res-
olution errors, around % 12 of total errors.

Acknowledgments

This work has been supported by first author’s
PhD grant from Euskara Errektoreordetza, the

University of the Basque Country (UPV/EHU)
and by the EXTRECM project, Spanish Govern-
ment (TIN2013-46616-C2-1-R).

References
Itziar Aduriz, Marı́a Jesús Aranzabe, Jose Mari Ar-

riola, Maite Atutxa, Arantza Dı́az de Ilarraza, Nerea
Ezeiza, Koldo Gojenola, Maite Oronoz, Aitor Soroa,
and Ruben Urizar. 2006. Methodology and Steps
towards the Construction of EPEC, a Corpus of
Written Basque Tagged at Morphological and Syn-
tactic Levels for the Automatic Processing. In
Language and Computers, volume 56, pages 1–15.
Rodopi, Amsterdam, Netherlands.

Eneko Agirre and Aitor Soroa. 2009. Personalizing
PageRank for Word Sense Disambiguation. In Pro-
ceedings of the 12th Conference of the European
Chapter of the Association for Computational Lin-
guistics, EACL ’09, pages 33–41, Athens, Greece.
Association for Computational Linguistics.

Eneko Agirre, Oier López de Lacalle, and Aitor
Soroa. 2014. Random Walks for Knowledge-based
Word Sense Disambiguation. Comput. Linguist.,
40(1):57–84.

Amit Bagga and Breck Baldwin. 1998. Algorithms for
Scoring Coreference Chains. In The First Interna-
tional Conference on Language Resources and Eval-
uation Workshop on Linguistics Coreference, pages
563–566, Granada, Spain.

Klara Ceberio, Itziar Aduriz, Arantza Dı́az de Ilarraza,
and Ines Garcia-Azkoaga. 2016. Coreferential re-
lations in Basque: the annotation process. In The-
oretical Developments in Hispanic Linguistics. The
Ohio State University.

Pascal Denis and Jason Baldridge. 2009. Global
Joint Models for Coreference Resolution and Named
Entity Classification. Procesamiento del Lenguaje
Natural, 43:87–96.

Greg Durrett and Dan Klein. 2013. Easy Victories and
Uphill Battles in Coreference Resolution. In Pro-
ceedings of the 2013 Conference on Empirical Meth-
ods in Natural Language Processing, pages 1971–
1982, Seattle, Washington, USA. Association for
Computational Linguistics.

Greg Durrett and Dan Klein. 2014. A Joint Model for
Entity Analysis: Coreference, Typing, and Linking.
TACL, 2:477–490.

Christiane Fellbaum, editor. 1998. WordNet: an elec-
tronic lexical database. MIT Press.

Barbara A. Fox. 1993. Discourse structure and
anaphora: written and conversational English.
Cambridge Univeristy Press.

15



Hannaneh Hajishirzi, Leila Zilles, Daniel S. Weld, and
Luke Zettlemoyer. 2013. Joint Coreference Res-
olution and Named-Entity Linking with Multi-Pass
Sieves. In Proceedings of the 2013 Conference on
Empirical Methods in Natural Language Process-
ing, pages 289–299, Seattle, Washington, USA. As-
sociation for Computational Linguistics.

Jonathan K. Kummerfeld and Dan Klein. 2013. Error-
Driven Analysis of Challenges in Coreference Res-
olution. In Proceedings of the 2013 Conference on
Empirical Methods in Natural Language Process-
ing, pages 265–277, Seattle, Washington, USA.

Heeyoung Lee, Angel Chang, Yves Peirsman,
Nathanael Chambers, Mihai Surdeanu, and Dan Ju-
rafsky. 2013. Deterministic Coreference Resolution
Based on Entity-centric, Precision-ranked Rules.
Computational Linguistics, 39(4):885–916.

Xiaoqiang Luo. 2005. On Coreference Resolution Per-
formance Metrics. In Proceedings of the Confer-
ence on Human Language Technology and Empir-
ical Methods in Natural Language Processing, HLT
’05, pages 25–32, Vancouver, British Columbia,
Canada. Association for Computational Linguistics.

Pablo Mendes, Max Jakob, and Christian Bizer. 2012.
DBpedia: A Multilingual Cross-domain Knowledge
Base. In Proceedings of the Eight International
Conference on Language Resources and Evaluation
(LREC’12), Istanbul, Turkey. European Language
Resources Association (ELRA).

Nafise Sadat Moosavi and Michael Strube. 2016.
Which Coreference Evaluation Metric Do You
Trust? A Proposal for a Link-based Entity Aware
Metric. In Proceedings of the 54th Annual Meet-
ing of the Association for Computational Linguis-
tics, pages 632–642, Berlin, Germany. Association
for Computational Linguistics.

Elisabete Pociello, Eneko Agirre, and Izaskun Aldez-
abal. 2011. Methodology and construction of the
Basque WordNet. Language Resources and Evalu-
ation, 45(2):121–142.

Simone Paolo Ponzetto and Michael Strube. 2006.
Exploiting Semantic Role Labeling, WordNet and
Wikipedia for Coreference Resolution. In Pro-
ceedings of the Main Conference on Human Lan-
guage Technology Conference of the North Amer-
ican Chapter of the Association of Computational
Linguistics, HLT-NAACL ’06, pages 192–199. As-
sociation for Computational Linguistics.

Sameer Pradhan, Xiaoqiang Luo, Marta Recasens, Ed-
uard Hovy, Vincent Ng, and Michael Strube. 2014.
Scoring Coreference Partitions of Predicted Men-
tions: A Reference Implementation. In Proceedings
of the 52nd Annual Meeting of the Association for
Computational Linguistics, pages 30–35, Baltimore,
Maryland. Association for Computational Linguis-
tics.

Altaf Rahman and Vincent Ng. 2011. Coreference
Resolution with World Knowledge. In Proceedings
of the 49th Annual Meeting of the Association for
Computational Linguistics: Human Language Tech-
nologies, HLT ’11, pages 814–824, Portland, Ore-
gon. Association for Computational Linguistics.

Lev Ratinov and Dan Roth. 2012. Learning-based
Multi-sieve Co-reference Resolution with Knowl-
edge. In Proceedings of the 2012 Joint Confer-
ence on Empirical Methods in Natural Language
Processing and Computational Natural Language
Learning, EMNLP-CoNLL ’12, pages 1234–1244,
Jeju Island, Korea. Association for Computational
Linguistics.

Marta Recasens and Eduard Hovy. 2011. BLANC:
Implementing the Rand index for coreference eval-
uation. Natural Language Engineering, 17(4):485–
510.

Marta Recasens, Matthew Can, and Daniel Jurafsky.
2013. Same Referent, Different Words: Unsuper-
vised Mining of Opaque Coreferent Mentions. In
Proceedings of the 2013 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics: Human Language Technologies,
pages 897–906, Atlanta, Georgia. Association for
Computational Linguistics.

Ander Soraluze, Olatz Arregi, Xabier Arregi, and
Arantza Dı́az de Ilarraza. 2015. Coreference Res-
olution for Morphologically Rich Languages. Adap-
tation of the Stanford System to Basque. Proce-
samiento del Lenguaje Natural, 55:23–30.

Ander Soraluze, Olatz Arregi, Xabier Arregi, and
Arantza Dı́az De Ilarraza. 2016. Improving mention
detection for Basque based on a deep error analysis.
Natural Language Engineering, pages 1–34, 007.

Fabian M. Suchanek, Gjergji Kasneci, and Gerhard
Weikum. 2007. YAGO: a core of semantic knowl-
edge unifying WordNet and Wikipedia. In Proceed-
ings of the 16th International World Wide Web Con-
ference (WWW 2007), pages 697–706.

Olga Uryupina, Massimo Poesio, Claudio Giuliano,
and Kateryna Tymoshenko. 2011. Disambigua-
tion and Filtering Methods in Using Web Knowl-
edge for Coreference Resolution. In FLAIRS Con-
ference, pages 317–322. AAAI Press.

Yannick Versley, Massimo Poesio, and Simone
Ponzetto. 2016. Using Lexical and Encyclo-
pedic Knowledge. In Massimo Poesio, Roland
Stuckardt, and Yannick Versley, editors, Anaphora
Resolution: Algorithms, Resources, and Applica-
tions, pages 393–429. Springer Berlin Heidelberg,
Berlin, Heidelberg.

Marc Vilain, John Burger, John Aberdeen, Dennis Con-
nolly, and Lynette Hirschman. 1995. A Model-
theoretic Coreference Scoring Scheme. In Proceed-
ings of the 6th Conference on Message Understand-
ing, MUC6 ’95, pages 45–52. Association for Com-
putational Linguistics.

16


