



















































Model Selection for Type-Supervised Learning with Application to POS Tagging


Proceedings of the 19th Conference on Computational Language Learning, pages 332–337,
Beijing, China, July 30-31, 2015. c©2015 Association for Computational Linguistics

Model Selection for Type-Supervised Learning
with Application to POS Tagging

Kristina Toutanova
Microsoft Research

Redmond, WA, USA

Waleed Ammar∗
School of Computer Science
Carnegie Mellon University

Pallavi Chourdhury Hoifung Poon
Microsoft Research

Redmond, WA, USA

Abstract

Model selection (picking, for example, the
feature set and the regularization strength)
is crucial for building high-accuracy NLP
models. In supervised learning, we can es-
timate the accuracy of a model on a subset
of the labeled data and choose the model
with the highest accuracy. In contrast,
here we focus on type-supervised learn-
ing, which uses constraints over the pos-
sible labels for word types for supervi-
sion, and labeled data is either not avail-
able or very small. For the setting where
no labeled data is available, we perform a
comparative study of previously proposed
and one novel model selection criterion on
type-supervised POS-tagging in nine lan-
guages. For the setting where a small la-
beled set is available, we show that the set
should be used for semi-supervised learn-
ing rather than for model selection only –
using it for model selection reduces the er-
ror by less than 5%, whereas using it for
semi-supervised learning reduces the error
by 44%.

1 Introduction

Fully supervised training of NLP models (e.g.,
part-of-speech taggers, named entity recognizers,
relation extractors) works well when plenty of la-
beled examples are available. However, manu-
ally labeled corpora are expensive to construct
in many languages and domains, whereas an al-
ternative, if weaker, supervision is often read-
ily available. For example, corpora labeled with
POS tags at the token level are only available for
around 35 languages, while tag dictionaries of the

∗This research was conducted during the author’s intern-
ship at Microsoft Research.

form displayed in Fig. 1 are available for many
more languages, either in commercial dictionar-
ies or community created resources such as Wik-
tionary. Tag dictionaries provide type-level super-
vision for word types in the lexicon. Similarly,
while sentences labeled with named entities are
scarce, gazetteers and databases are more readily
available (Bollacker et al., 2008).

There has been substantial research on how best
to build models using such type-level supervision,
for POS tagging, super sense tagging, NER, and
relation extraction (Craven et al., 1999; Smith and
Eisner, 2005; Carlson et al., 2009; Mintz et al.,
2009; Johannsen et al., 2014), inter alia, focussing
on parametric forms and loss functions for model
training. However, there has been little research on
the practically important aspect of model selection
for type-supervised learning. While some previ-
ous work used criteria based on the type-level su-
pervision only (Smith and Eisner, 2005; Goldwa-
ter and Griffiths, 2007), much prior work used a la-
beled set for model selection (Vaswani et al., 2010;
Soderland and Weld, 2014). We are not aware of
any prior work aiming to compare or improve ex-
isting type-supervised model selection criteria.

For POS tagging, there is also work on us-
ing both type-level supervision from lexicons, and
projection from another language (Täckström et
al., 2013). Methods for training with a small
labeled set have also been developed (Søgaard,
2011; Garrette and Baldridge, 2013; Duong et al.,
2014), but there have not been studies on the util-
ity of a small labeled set for model selection ver-
sus model training. Our contributions are: 1) a
simple and generally applicable model selection
criterion for type-supervised learning, 2) the first
multi-lingual systematic evaluation of model se-
lection criteria for type-supervised models, 3) em-
pirical evidence that, if a small labeled set is avail-
able, the set should be used for semi-supervised

332



αυτούς ' 'det., pron.'
σταθερά ' 'adj., adv., noun'
μετέδωσα 'verb'
ανάπαυση 'noun'
παλιά  ' 'adj., adv.'

Greek tag dictionary 

ενός    ' 'det., num.'
γαλακτικέ 'adj.'

train!
lexicon!

dev !
lexicon!

Figure 1: A tag dictionary (lexicon) for Greek. The splits
into lextrain and lexdev are discussed in §2.

learning and not only for model selection.

2 Model selection and training for
type-supervised learning

Notation. In type-supervised learning, we have
unlabeled text T = {x} of token sequences, and
a lexicon lex which lists possible labels for word
types. Model training finds the model param-
eters θ which minimize a training loss function
L(θ; lex, T ,h). We use h to represent the config-
urations and modeling decisions (also known as
hyperparameters). Examples include the depen-
dency structure between variables, feature tem-
plates, and regularization strengths. Given a set
of fully-specified hyperparameter configurations
{h1, . . . ,hM}, model selection aims to find the
configuration hm̂ that maximizes the expected
performance of the corresponding model θm̂ ac-
cording to a suitable accuracy measure. x is a to-
ken sequence, y is a label sequence, and lex[x] is
the set of label sequences compatible with token
sequence x according to lex.

Task. For the application in this paper, the task
is type-supervised POS tagging, and the paramet-
ric model family we consider is that of featurized
first order HMMs (Berg-Kirkpatrick et al., 2010).
The hyperparameters specify the feature set used
and the strength of an L2 regularizer on the pa-
rameters.

Evaluation function. The evaluation function
used in model selection is the main focus of this
work. We use a function eval(m, Tdev) to esti-
mate the performance of the model trained with
hyperparameters hm on a development set Tdev. In
the following subsections, we discuss definitions
of eval when the development set Tdev is labeled
and when it is unlabeled, respectively.

2.1 Tdev is labeled
When the development set Tdev is labeled, a nat-
ural choice of eval is token-level prediction accu-

racy:

evalsup(m, Tdev) =
|Tdev |∑
i=1

1(ym[i] = ygold[i])
|Tdev|

Here, we use i to index all tokens in Tdev; ygold[i]
denotes the correct POS tag, and ym[i] denotes the
predicted POS tag of the i-th token obtained with
hyperparameters hm.

2.2 Tdev is unlabeled
When token-supervision is not available, we can-
not compute evalsup. Instead, previous work on
POS tagging with type supervision (Smith and
Eisner, 2005) used:

evalcond(m, Tdev) =
∑

x∈Tdev
log

∑
y∈lex[x]

pθm(y | x),

evaljoint(m, Tdev) =
∑

x∈Tdev
log

∑
y∈lex[x]

pθm(x,y)

evalcond estimates the conditional log-likelihood
of “lex-compatible” labels given token sequences,
while evaljoint estimates the joint log-likelihood
of lex-compatible labels and token sequences.

The held-out lexicon criterion. We propose a
new model selection criterion which estimates pre-
diction accuracy more directly and is applicable to
any model type, without requiring that the model
define conditional or joint probabilities of label se-
quences. The idea behind this proposed criterion
is simple: we hold out a portion of the lexicon en-
tries and use it to estimate model performance as
follows:

evaldevlex(m, T ) =
|T |∑

i=1:xi∈lexdev

1(ym[i] ∈ lex[xi])
|lex[xi]| × |Tx∈lexdev |

where lexdev is the held-out portion of the lexicon
entries, and xi is the i-th token in T .

The remainder of this section details the theory
behind this criterion. The expected token-level ac-
curacy of a model trained with hyperparameters
hm is defined as E(x,ygold,ym)∼D [1(ym = ygold)];
where D is a joint distribution over tokens x (in
context), their gold labels ygold, and the predicted
labels ym (for the configuration hm). Since when
no labeled data is available we do not have access
to samples fromD, we derive an approximation to
this distribution using lex and T .

We first split the full lexicon into a training
lexicon lextrain and a held-out (or development)

333



lexicon lexdev (see Fig. 1), by sampling words ac-
cording to their token frequency c(x) in T , and
placing them in the development or training por-
tions of the lexicon such that lexdev covers 25%
of the tokens in T . The goal of the sampling pro-
cess is to make the distribution of word tags for
words in the development lexicon representative
of the tag distribution for all words.

Given hm, we train a tagging model using
lextrain and use it to predict labels ym for all to-
kens in T . We then use the word tokens covered
by the development lexicon and their predicted
tags ym to approximate D by letting P (ygold | x)
be a uniform distribution over gold labels consis-
tent with the lexicon for x, resulting in the follow-
ing approximation PD(x, ygold, ym) ∝

c(x, ym)× 1(x ∈ lexdev, ygold ∈ lexdev[x])
|lexdev[x]|

We then compute the expected accuracy as
evaldevlex = ED [1(ygold = ym)], and select
the hyperparameter configuration m̂ which max-
imizes this criterion, then re-train the model with
the full lexicon lex.1

3 How to best use a small labeled set TL?
Several prior works used a labeled set for super-
vised hyper-parameter selection even when only
type-level supervision is assumed to be available
for training (Vaswani et al., 2010; Soderland and
Weld, 2014). In this section, we want to an-
swer the question: if a small labeled set is avail-
able, what are the potential gains from using it
for model selection only, versus using it for both
model training and model selection?

A simple way to use a small labeled set for
parameter training together with a larger unla-
beled set in our type-supervised learning setting,
is to do semi-supervised model training as follows
(Nigam et al., 2000): Starting with our training
loss function defined using a lexicon lex and unla-
beled set TU L(θ; lex, TU ,hm), we define a com-
bined loss function using both the unlabeled set
TU and the labeled set TL: L(θ; lex, TU ,hm) +
λL(θ; lex, TL,hm). We then select parameters
θm to minimize the new loss function, where λ
is now an additional hyperparameter that usually

1Note that this criterion underestimates the performance
of all models in consideration by virtue of evaluating model
versions trained using a subset of the full lexicon, but it can
still be useful for ranking the models.

gives more weight to the labeled set. An advan-
tage of this method is that it can be applied to any
type-supervised model using less than 100 lines
of code.2 We implement this method for semi-
supervised training, and we use the labeled set
both for semi-supervised model training and for
hyper-parameter selection using a standard five-
fold cross-validation approach.3

4 Experiments

We evaluate the introduced methods for model se-
lection and training with type supervision in two
type-supervised settings: when no labeled exam-
ples are available, and when a small number of la-
beled examples are available.

We use a feature-rich first-order HMM
model (Berg-Kirkpatrick et al., 2010) with an
L2 prior on feature weights.4 Instead of using a
multinomial distribution for the local emissions
and transitions, this model uses a log-linear distri-
bution (i.e., p(xi | yi) ∝ expλ>f(xi, yi)) with a
feature vector f and a weight vector λ. We use the
feature set described in (Li et al., 2012): transition
features, word-tag features (〈yi, xi〉) (lowercased
words with frequency greater than a threshold),
whether the word contains a hyphen and/or
starts with a capital letter, character suffixes, and
whether the word contains a digit. We initialize
the transition and emission distributions of the
HMM using unambiguous words as proposed
by (Zhang and DeNero, 2014). Data. We use
the Danish, Dutch, German, Greek, English,
Italian, Portuguese, Spanish and Swedish datasets
from CoNLL-X and CoNLL-2007 shared tasks
(Buchholz and Marsi, 2006; Nivre et al., 2007).
We map the POS labels in the CoNLL datasets
to the universal POS tagset (Petrov et al., 2012).
We use the tag dictionaries provided by Li et
al. (2012). Model configurations. For each

2The labeled data loss is the negative joint log-
likelihood of the observed token sequences and labels:
−∑x,y∈TL log pθ(x,y).

3We split the labeled set into five folds, and for each set-
ting of the hyper-parameters train five different models on
4
5

-ths of the data, estimating accuracy on the remaining 1
5

-th.
We average the accuracy estimates from different folds and
use this as a combined estimate of the accuracy of a model
trained using the full labeled and unlabeled set, given these
hyperparameters. After selecting a configuration of hyperpa-
rameters using cross-validation, we then use this configura-
tion and retrain the model on all available data.

4We used a first-order HMM for simplicity, but it is pos-
sible to obtain better results using a second-order HMM (Li
et al., 2012).

334



83.70

85.05

84.75
85.20

76

78

80

82

84

86

88

Greek Danish Dutch Italian Spanish Portugese German English Swedish Average

Conditional Likelihood Joint Likelihood Held-out Lexicon English Labeled

Figure 2: Token-level accuracy when doing training and model selection with no labeled data. Model selection with different
criteria (left to right): conditional log-likelihood, joint log-likelihood, held-out lexicon, and English-labeled.

85.05

85.75

91.73

81

83

85

87

89

91

93

95

97

Greek Danish Dutch Italian Spanish Portugese German English Swedish Average

Joint Likelihood (0 labeled)

Supervised model selection (300 labeled)

Semi-supervised training and supervised model selection (300 labeled)

Figure 3: Token-level accuracy with (left to right): no labeled data, model selection on labeled data (300 sentences), semi-
supervised training + model selection on labeled data (300 sentences).

language, we consider M = 15 configurations
of the hyperparameters which vary in the L2
regularization strength (5 values), and the min-
imum word frequency for word-tag features (3
values). When a small labeled set is available we
additionally choose one of 3 values for the weight
of the labeled set (see Section 3). We report final
performance of models selected using different
criteria using token-level accuracy on an unseen
test set.

No labeled examples. When no labeled exam-
ples are available, we do model training and se-
lection using only unlabeled text T and a tagging
lexicon lex. We compare three type-supervised

model selection criteria: conditional likelihood,
joint likelihood, and the held-out lexicon criterion
evaldevlex. Additionally, we include the perfor-
mance of a method which selects the hyperpa-
rameters using labeled data in English and uses
these (same) hyperparameters for English and all
other languages (we call this method “English La-
beled”). Fig. 2 shows the accuracy of the models
chosen by each of the four criteria on nine lan-
guages, as well as the average accuracy across lan-
guages. The lower (upper) bounds on average per-
formance obtained by always choosing the worst
(best) hyperparameters is 82.77 (85.83). evaljoint
outperformed evalcond on eight out of the nine

335



languages and achieved a significantly higher av-
erage accuracy (85.05 vs 83.70). evaldevlex out-
performed evaljoint on six out of nine languages,
but did significantly worse on one language (Ger-
man), which resulted in a slightly lower average
accuracy. Choosing the hyper-parameters using
English labeled data and using the same hyper-
parameters for all languages performed compara-
bly to evaljoint, with slightly higher average ac-
curacy even when limited to the non-English lan-
guages (85.0 vs 84.9). Overall the results showed
that the conditional log-likelihood criterion was
dominated by the other three, which were com-
parable in average accuracy. Looking at the eight
languages excluding English (since one criterion
uses labeled data for English), the newly proposed
held-out lexicon criterion was the winning method
on five out of eight languages, evalcond was best
on one language, evaljoint was best (or tied for
best) on two, and English-labeled was tied for best
on one language.

Few labeled examples. We consider two ways
of leveraging the labeled examples: (i) type-
supervised model training + supervised model se-
lection: only use unlabeled examples to optimize
model parameters, then use the labeled exam-
ples for supervised model selection with evalsup,
and (ii) semi-supervised model training + super-
vised model selection (see Section 3 for details).
Fig. 3 shows how much we can improve on the
method with highest average accuracy from Fig-
ure 2 (evaljoint), when a small number of ex-
amples is available. Using the 300 labeled sen-
tences for semi-supervised training and model se-
lection reduced the error by 44.6% (comparing to
the model with best average accuracy using only
type-level supervision with average performance
of 85.05, the semi-supervised average is 91.8). In
contrast, using the 300 sentences to select hyper-
parameters only reduced the error by less than 5%
(the average accuracy was 85.75). Even when only
50 labeled sentences are used for semi-supervised
training and supervised model selection, we still
see a boost to average accuracy of 89% (results
not shown in the Figure).

5 Conclusion

We presented the first comparative evaluation of
model selection criteria for type-supervised POS-
tagging on many languages. We introduced a
novel, generally applicable model selection cri-

terion which outperformed previously proposed
ones for a majority of languages. We evaluated the
utility of a small labeled set for model selection
versus model training, and showed that when such
labeled set is available, it should not be used solely
for supervised model selection, because using it
additionally for model parameter training provides
strikingly larger accuracy gains.

Acknowledgments
We thank Nathan Schneider and the anonymous
reviewers for helpful suggestions.

References
Taylor Berg-Kirkpatrick, Alexandre Bouchard-Côté,

John DeNero, and Dan Klein. 2010. Painless unsu-
pervised learning with features. In Proc. of NAACL.

Kurt Bollacker, Colin Evans, Praveen Paritosh, Tim
Sturge, and Jamie Taylor. 2008. Freebase: A col-
laboratively created graph database for structuring
human knowledge. In Proceedings of the 2008 ACM
SIGMOD International Conference on Management
of Data, SIGMOD ’08, pages 1247–1250.

Sabine Buchholz and Erwin Marsi. 2006. CoNLL-X
shared task on multilingual dependency parsing. In
CoNLL-X.

Andrew Carlson, Scott Gaffney, and Flavian Vasile.
2009. Learning a named entity tagger from
gazetteers with the partial perceptron. In AAAI
Spring Symposium: Learning by Reading and
Learning to Read, pages 7–13.

Mark Craven, Johan Kumlien, et al. 1999. Construct-
ing biological knowledge bases by extracting infor-
mation from text sources. In ISMB, volume 1999,
pages 77–86.

Long Duong, Trevor Cohn, Karin Verspoor, Steven
Bird, and Paul Cook. 2014. What can we get
from 1000 tokens? a case study of multilingual pos
tagging for resource-poor languages. In Proc. of
EMNLP.

Dan Garrette and Jason Baldridge. 2013. Learning a
part-of-speech tagger from two hours of annotation.
In Proc. of NAACL-HLT, pages 138–147.

Sharon Goldwater and Tom Griffiths. 2007. A fully
bayesian approach to unsupervised part-of-speech
tagging. In Annual meeting-association for compu-
tational linguistics.

Anders Johannsen, Dirk Hovy, Héctor Martınez
Alonso, Barbara Plank, and Anders Søgaard. 2014.
More or less supervised supersense tagging of twit-
ter. Proc. of* SEM, pages 1–11.

336



Shen Li, João Graça, and Ben Taskar. 2012. Wiki-
ly supervised part-of-speech tagging. In Proc. of
EMNLP.

Mike Mintz, Steven Bills, Rion Snow, and Dan Ju-
rafsky. 2009. Distant supervision for relation ex-
traction without labeled data. In Proceedings of the
Joint Conference of the 47th Annual Meeting of the
ACL and the 4th International Joint Conference on
Natural Language Processing of the AFNLP: Vol-
ume 2-Volume 2, pages 1003–1011. Association for
Computational Linguistics.

Kamal Nigam, Andrew Kachites McCallum, Sebastian
Thrun, and Tom Mitchell. 2000. Text classifica-
tion from labeled and unlabeled documents using
em. Machine learning, 39.

Joakim Nivre, Johan Hall, Sandra Kubler, Ryan Mc-
Donald, Jens Nilsson, Sebastian Riedel, and Deniz
Yuret. 2007. The CoNLL 2007 shared task on de-
pendency parsing. In Proc. of CoNLL.

Slav Petrov, Dipanjan Das, and Ryan McDonald. 2012.
A universal part-of-speech tagset. In Proc. of LREC,
May.

Noah A. Smith and Jason Eisner. 2005. Contrastive
estimation: Training log-linear models on unlabeled
data. In Proc. of ACL.

Mitchell Koch John Gilmer Stephen Soderland and
Daniel S Weld. 2014. Type-aware distantly super-

vised relation extraction with linked arguments. In
Proc. of EMNLP.

Anders Søgaard. 2011. Semisupervised condensed
nearest neighbor for part-of-speech tagging. In Pro-
ceedings of the 49th Annual Meeting of the Associ-
ation for Computational Linguistics: Human Lan-
guage Technologies: Short Papers - Volume 2, HLT
’11, pages 48–52, Stroudsburg, PA, USA. Associa-
tion for Computational Linguistics.

Oscar Täckström, Dipanjan Das, Slav Petrov, Ryan
McDonald, and Joakim Nivre. 2013. Token and
type constraints for cross-lingual part-of-speech tag-
ging. Transactions of the Association for Computa-
tional Linguistics, 1:1–12.

Ashish Vaswani, Adam Pauls, and David Chiang.
2010. Efficient optimization of an mdl-inspired ob-
jective function for unsupervised part-of-speech tag-
ging. In Proceedings of the ACL 2010 Conference
Short Papers, pages 209–214. Association for Com-
putational Linguistics.

Hui Zhang and John DeNero. 2014. Observational ini-
tialization of type-supervised taggers. In Proceed-
ings of the Association for Computational Linguis-

tics (Short Paper Track).

337


