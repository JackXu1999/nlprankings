



















































Matching Article Pairs with Graphical Decomposition and Convolutions


Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 6284–6294
Florence, Italy, July 28 - August 2, 2019. c©2019 Association for Computational Linguistics

6284

Matching Article Pairs with Graphical Decomposition and Convolutions

Bang Liu†, Di Niu†, Haojie Wei‡, Jinghong Lin‡, Yancheng He‡, Kunfeng Lai‡, Yu Xu‡
†University of Alberta, Edmonton, AB, Canada

{bang3, dniu}@ualberta.ca
‡Platform and Content Group, Tencent, Shenzhen, China

{fayewei, daphnelin, collinhe, calvinlai, henrysxu}@tencent.com

Abstract

Identifying the relationship between two arti-
cles, e.g., whether two articles published from
different sources describe the same break-
ing news, is critical to many document un-
derstanding tasks. Existing approaches for
modeling and matching sentence pairs do not
perform well in matching longer documents,
which embody more complex interactions be-
tween the enclosed entities than a sentence
does. To model article pairs, we propose the
Concept Interaction Graph to represent an ar-
ticle as a graph of concepts. We then match
a pair of articles by comparing the sentences
that enclose the same concept vertex through
a series of encoding techniques, and aggregate
the matching signals through a graph convo-
lutional network. To facilitate the evaluation
of long article matching, we have created two
datasets, each consisting of about 30K pairs
of breaking news articles covering diverse top-
ics in the open domain. Extensive evaluations
of the proposed methods on the two datasets
demonstrate significant improvements over a
wide range of state-of-the-art methods for nat-
ural language matching.

1 Introduction

Identifying the relationship between a pair of arti-
cles is an essential natural language understand-
ing task, which is critical to news systems and
search engines. For example, a news system needs
to cluster various articles on the Internet report-
ing the same breaking news (probably in different
ways of wording and narratives), remove redun-
dancy and form storylines (Shahaf et al., 2013; Liu
et al., 2017; Zhou et al., 2015; Vossen et al., 2015;
Bruggermann et al., 2016). The rich semantic and
logic structures in longer documents have made it
a different and more challenging task to match a
pair of articles than to match a pair of sentences or
a query-document pair in information retrieval.

Traditional term-based matching approaches es-
timate the semantic distance between a pair of
text objects via unsupervised metrics, e.g., via TF-
IDF vectors, BM25 (Robertson et al., 2009), LDA
(Blei et al., 2003) and so forth. These methods
have achieved success in query-document match-
ing, information retrieval and search. In recent
years, a wide variety of deep neural network mod-
els have also been proposed for text matching (Hu
et al., 2014; Qiu and Huang, 2015; Wan et al.,
2016; Pang et al., 2016), which can capture the
semantic dependencies (especially sequential de-
pendencies) in natural language through layers of
recurrent or convolutional neural networks. How-
ever, existing deep models are mainly designed for
matching sentence pairs, e.g., for paraphrase iden-
tification, answer selection in question-answering,
omitting the complex interactions among key-
words, entities or sentences that are present in a
longer article. Therefore, article pair matching re-
mains under-explored in spite of its importance.

In this paper, we apply the divide-and-conquer
philosophy to matching a pair of articles and bring
deep text understanding from the currently domi-
nating sequential modeling of language elements
to a new level of graphical document representa-
tion, which is more suitable for longer articles.
Specifically, we have made the following contri-
butions:

First, we propose the so-called Concept Inter-
action Graph (CIG) to represent a document as
a weighted graph of concepts, where each con-
cept vertex is either a keyword or a set of tightly
connected keywords. The sentences in the arti-
cle associated with each concept serve as the fea-
tures for local comparison to the same concept ap-
pearing in another article. Furthermore, two con-
cept vertices in an article are also connected by
a weighted edge which indicates their interaction
strength. The CIG does not only capture the essen-



6285

tial semantic units in a document but also offers a
way to perform anchored comparison between two
articles along the common concepts found.

Second, we propose a divide-and-conquer
framework to match a pair of articles based on
the constructed CIGs and graph convolutional net-
works (GCNs). The idea is that for each concept
vertex that appears in both articles, we first ob-
tain the local matching vectors through a range of
text pair encoding schemes, including both neu-
ral encoding and term-based encoding. We then
aggregate the local matching vectors into the fi-
nal matching result through graph convolutional
layers (Kipf and Welling, 2016; Defferrard et al.,
2016). In contrast to RNN-based sequential mod-
eling, our model factorizes the matching process
into local matching sub-problems on a graph, each
focusing on a different concept, and by using
GCN layers, generates matching results based on
a holistic view of the entire graph.

Although there exist many datasets for sentence
matching, the semantic matching between longer
articles is a largely unexplored area. To the best
of our knowledge, to date, there does not exist a
labeled public dataset for long document match-
ing. To facilitate evaluation and further research
on document and especially news article match-
ing, we have created two labeled datasets1, one
annotating whether two news articles found on In-
ternet (from different media sources) report the
same breaking news event, while the other anno-
tating whether they belong to the same news story
(yet not necessarily reporting the same breaking
news event). These articles were collected from
major Internet news providers in China, including
Tencent, Sina, WeChat, Sohu, etc., covering di-
verse topics, and were labeled by professional ed-
itors. Note that similar to most other natural lan-
guage matching models, all the approaches pro-
posed in this paper can easily work on other lan-
guages as well.

Through extensive experiments, we show that
our proposed algorithms have achieved signifi-
cant improvements on matching news article pairs,
as compared to a wide range of state-of-the-art
methods, including both term-based and deep text
matching algorithms. With the same encoding or
term-based feature representation of a pair of arti-
cles, our approach based on graphical decomposi-

1Our code and datasets are available at:
https://github.com/BangLiu/ArticlePairMatching

Text: Concept Interaction Graph:

[1] Rick asks Morty to travel with him
      in the universe.
[2] Morty doesn't want to go as Rick always
      brings him dangerous experiences.
[3] However, the destination of this journey
      is the Candy Planet, which is an fascinating
      place that attracts Morty.
[4] The planet is full of delicious candies.
[5] Summer wishes to travel with Rick.
[6] However, Rick doesn't like to travel with Summer.

Rick
Morty

Rick
Summer

Morty
Candy 
Planet

[1, 2] [5, 6]

[3, 4]

Figure 1: An example to show a piece of text and its
Concept Interaction Graph representation.

tion and convolutions can improve the classifica-
tion accuracy by 17.31% and 23.09% on the two
datasets, respectively.

2 Concept Interaction Graph

In this section, we present our Concept Interac-
tion Graph (CIG) to represent a document as an
undirected weighted graph, which decomposes a
document into subsets of sentences, each subset
focusing on a different concept. Given a docu-
ment D, a CIG is a graph GD, where each vertex
in GD is called a concept, which is a keyword or a
set of highly correlated keywords in document D.
Each sentence in D will be attached to the single
concept vertex that it is the most related to, which
most frequently is the concept the sentence men-
tions. Hence, vertices will have their own sentence
sets, which are disjoint. The weight of the edge
between a pair of concepts denotes how much the
two concepts are related to each other and can be
determined in various ways.

As an example, Fig. 1 illustrates how we con-
vert a document into a Concept Interaction Graph.
We can extract keywords Rick, Morty, Summer,
and Candy Planet from the document using stan-
dard keyword extraction algorithms, e.g., Tex-
tRank (Mihalcea and Tarau, 2004). These key-
words are further clustered into three concepts,
where each concept is a subset of highly corre-
lated keywords. After grouping keywords into
concepts, we attach each sentence in the document
to its most related concept vertex. For example, in
Fig. 1, sentences 1 and 2 are mainly talking about
the relationship between Rick and Morty, and are
thus attached to the concept (Rick, Morty). Other
sentences are attached to vertices in a similar way.
The attachment of sentences to concepts naturally
dissects the original document into multiple dis-
joint sentence subsets. As a result, we have repre-
sented the original document with a graph of key
concepts, each with a sentence subset, as well as



6286

Construct KeyGraph
by Word Co-occurrence

w

w w

w

w

w

w

w

w

w

w

w

w

w

w

w

w w

w

w

w

w

w

w

w

w

w

w

w

w

Document Pair KeyGraph

Concepts

Doc A

Doc B

Detect Concepts
 by Community Detection

Assign Sentences
 by Similarities

S1 S2

Concept 1

S1 S2

Concept 2

S1 S2

Concept 3

S1 S2

Concept 4

S1 S2

Concept 5

Concepts
with

sentences

Get Edge Weights
by Vertex Similarities

3
5

4

2
1

Concept
Interaction

Graph

Siamese
Encoder

Context Layer Contex Layer

Matching Layer

Sentences 1 Sentences 2

Vertex Feature

Term-based
Feature
Extractor

Sentence 1, Sentence 2

Feature Extractor

Vertex Feature

CIG with vertex features

vertex
features

Result

(a) Representation (b) Encoding (c) Transformation (d) Aggregation

Input

GCN Layers

Aggregation Layer

transformed
features

Siamese
matching

Term-based
matching

Siamese
matching

…

Term-based
matching

Global
matching

Concatenate

Classify

Figure 2: An overview of our approach for constructing the Concept Interaction Graph (CIG) from a pair of
documents and classifying it by Graph Convolutional Networks.

the interaction topology among them.
Fig 2 (a) illustrates the construction of CIGs for

a pair of documents aligned by the discovered con-
cepts. Here we first describe the detailed steps to
construct a CIG for a single document:

KeyGraph Construction. Given a document
D, we first extract the named entities and key-
words by TextRank (Mihalcea and Tarau, 2004).
After that, we construct a keyword co-occurrence
graph, called KeyGraph, based on the set of found
keywords. Each keyword is a vertex in the Key-
Graph. We connect two keywords by an edge if
they co-occur in a same sentence.

We can further improve our model by perform-
ing co-reference resolution and synonym analysis
to merge keywords with the same meaning. How-
ever, we do not apply these operations due to the
time complexity.

Concept Detection (Optional). The structure
of KeyGraph reveals the connections between key-
words. If a subset of keywords are highly cor-
related, they will form a densely connected sub-
graph in the KeyGraph, which we call a concept.
Concepts can be extracted by applying commu-
nity detection algorithms on the constructed Key-
Graph. Community detection is able to split a
KeyGraph Gkey into a set of communities C =
{C1, C2, ..., C|C|}, where each community Ci con-
tains the keywords for a certain concept. By using
overlapping community detection, each keyword
may appear in multiple concepts. As the num-
ber of concepts in different documents varies a lot,
we utilize the betweenness centrality score based
algorithm (Sayyadi and Raschid, 2013) to detect
keyword communities in KeyGraph.

Note that this step is optional, i.e., we can also

use each keyword directly as a concept. The ben-
efit brought by concept detection is that it reduces
the number of vertices in a graph and speeds up
matching, as will be shown in Sec. 4.

Sentence Attachment. After the concepts are
discovered, the next step is to group sentences by
concepts. We calculate the cosine similarity be-
tween each sentence and each concept, where sen-
tences and concepts are represented by TF-IDF
vectors. We assign each sentence to the concept
which is the most similar to the sentence. Sen-
tences that do not match any concepts in the docu-
ment will be attached to a dummy vertex that does
not contain any keywords.

Edge Construction. To construct edges that re-
veal the correlations between different concepts,
for each vertex, we represent its sentence set as a
concatenation of the sentences attached to it, and
calculate the edge weight between any two ver-
tices as the TF-IDF similarity between their sen-
tence sets. Although edge weights may be de-
cided in other ways, our experience shows that
constructing edges by TF-IDF similarity generates
a CIG that is more densely connected.

When performing article pair matching, the
above steps will be applied to a pair of documents
DA andDB , as is shown in Fig. 2 (a). The only ad-
ditional step is that we align the CIGs of the two
articles by the concept vertices, and for each com-
mon concept vertex, merge the sentence sets from
DA and DB for local comparison.

3 Article Pair Matching through Graph
Convolutions

Given the merged CIGGAB of two documentsDA
andDB described in Sec. 2, we match a pair of ar-



6287

ticles in a “divide-and-conquer” manner by match-
ing the sentence sets from DA and DB associated
with each concept and aggregating local matching
results into a final result through multiple graph
convolutional layers. Our approach overcomes the
limitation of previous text matching algorithms,
by extending text representation from a sequential
(or grid) point of view to a graphical view, and can
therefore better capture the rich semantic interac-
tions in longer text.

Fig. 2 illustrates the overall architecture of our
proposed method, which consists of four steps:
a) representing a pair of documents by a sin-
gle merged CIG, b) learning multi-viewed match-
ing features for each concept vertex, c) struc-
turally transforming local matching features by
graph convolutional layers, and d) aggregating lo-
cal matching features to get the final result. Steps
(b)-(d) can be trained end-to-end.

Encoding Local Matching Vectors. Given the
merged CIG GAB , our first step is to learn an
appropriate matching vector of a fixed length for
each individual concept v ∈ GAB to express the
semantic similarity between SA(v) and SB(v), the
sentence sets of concept v from documents DA
and DB , respectively. This way, the matching of
two documents is converted to match the pair of
sentence sets on each vertex of GAB . Specifically,
we generate local matching vectors based on both
neural networks and term-based techniques.

Siamese Encoder: we apply a Siamese neural
network encoder (Neculoiu et al., 2016) onto each
vertex v ∈ GAB to convert the word embeddings
(Mikolov et al., 2013) of {SA(v),SB(v)} into a
fixed-sized hidden feature vector mAB(v), which
we call the match vector.

We use a Siamese structure to take SA(v) and
SB(v)} (which are two sequences of word embed-
dings) as inputs, and encode them into two con-
text vectors through the context layers that share
the same weights, as shown in Fig. 2 (b). The
context layer usually contains one or multiple bi-
directional LSTM (BiLSTM) or CNN layers with
max pooling layers, aiming to capture the contex-
tual information in SA(v) and SB(v)}.

Let cA(v) and cB(v) denote the context vectors
obtained for SA(v) and SB(v), respectively. Then,
the matching vector mAB(v) for vertex v is given
by the subsequent aggregation layer, which con-
catenates the element-wise absolute difference and
the element-wise multiplication of the two context

vectors, i.e.,

mAB(v) = (|cA(v)− cB(v)|, cA(v) ◦ cB(v)),
(1)

where ◦ denotes Hadamard product.
Term-based Similarities: we also generate an-

other matching vector for each v by directly cal-
culating term-based similarities between SA(v)
and SB(v), based on 5 metrics: the TF-IDF co-
sine similarity, TF cosine similarity, BM25 co-
sine similarity, Jaccard similarity of 1-gram, and
Ochiai similarity measure. These similarity scores
are concatenated into another matching vector
m′AB(v) for v, as shown in Fig. 2 (b).

Matching Aggregation via GCN The local
matching vectors must be aggregated into a final
matching score for the pair of articles. We propose
to utilize the ability of the Graph Convolutional
Network (GCN) filters (Kipf and Welling, 2016)
to capture the patterns exhibited in the CIG GAB
at multiple scales. In general, the input to the GCN
is a graphG = (V, E) withN vertices vi ∈ V , and
edges eij = (vi, vj) ∈ E with weights wij . The
input also contains a vertex feature matrix denoted
by X = {xi}Ni=1, where xi is the feature vector
of vertex vi. For a pair of documents DA and DB ,
we input their CIG GAB (with N vertices) with
a (concatenated) matching vector on each vertex
into the GCN, such that the feature vector of ver-
tex vi in GCN is given by

xi = (mAB(vi),m
′
AB(vi)).

Now let us briefly describe the GCN lay-
ers (Kipf and Welling, 2016) used in Fig. 2 (c).
Denote the weighted adjacency matrix of the
graph as A ∈ RN×N where Aij = wij (in CIG,
it is the TF-IDF similarity between vertex i and
j). Let D be a diagonal matrix such that Dii =∑

j Aij . The input layer to the GCN is H
(0) = X ,

which contains the original vertex features. Let
H(l) ∈ RN×Ml denote the matrix of hidden rep-
resentations of the vertices in the lth layer. Then
each GCN layer applies the following graph con-
volutional filter onto the previous hidden represen-
tations:

H(l+1) = σ(D̃−
1
2 ÃD̃−

1
2H(l)W (l)), (2)

where Ã = A+ IN , IN is the identity matrix, and
D̃ is a diagonal matrix such that D̃ii =

∑
j Ãij .

They are the adjacency matrix and the degree ma-
trix of graph G, respectively.



6288

2016-10-28

!"#$%&'()%(

*+),-$.//%0

,12&'(,3)(,/1

2016-10-29

!"#$&45-),1

6/%$%&'()%(,13

*+),-$.//%0

,12&'(,3)(,/1

2016-10-30

7,-)%8$9:&'(,/1'

!"#;'$+/(,2)(,/1

6/%$%&'()%(,13

,12&'(,3)(,/1

2016-11-06

!"#$.,%&<(/%=

>/$<?)%3&'

)6(&%$1&@

%&2,&@$/6

7,-)%8$&+),-'

Hilary’s “mail door’’

2016-09-11

7,-)%8$)((&1.'

(?&$ABB

C11,2&%')%8

)1.$-&)2&$&)%-8

2016-09-12

D/<(/%$')8

7,-)%8$?)'

51&:+/1,)

2016-09-14

7,-)%8$')8

'?&$@)'

?&)-(?8

2016-09-16

7,-)%8$,'

%&</2&%&.

Hilary’s health condition

2016-10-07

E)'?,13(/1$F/'(

%&2&)-'$G%:+5;'

'5&&<?$)H/:(

</1(&+5(

6/%$@/+&1

2016-10-08

G%:+5$5:H-,<-8

)5/-/3,I&'$6/%$?,'

</1(%/2&%',)-$'5&&<?

)H/:($@/+&1

2016-11-02

7,-)%8$</1.&+1'

G%:+5$6/%

H:--8,13$@/+&1

Trump's speech about contempt for woman

2016-09-26

!,%'($&-&<(,/1

(&-&2,',/1

.&H)(&

2016-10-10

J&</1.$&-&<(,/1

(&-&2,',/1

.&H)(&

2016-10-19

G?,%.$&-&<(,/1

(&-&2,',/1

.&H)(&

Election television debates

2016-07-19

Trump become

presidential

candidate

2016-07-26

Hilary become

presidential

candidate

Presidential candidates

2016-09-28

7,-)%8$)<<:'&'

G%:+5$/6

%&6:',13$(/

.,'<-/'&$()4

,16/%+)(,/1

2016-10-02

>&@$K/%L$G,+&'

&45/':%&'$G%:+5

()4$)2/,.)1<&$$

Trump avoid tax

2016-11-09

D/1)-.$G%:+5

,'$&-&<(&.

5%&',.&1(

2016-11-08

C+&%,<)$2/(&'

(/$&-&<($1&@

5%&',.&1(

Voting for new president

2016 U.S. presidential election

Figure 3: The events contained in the story “2016 U.S.
presidential election”.

W (l) is the trainable weight matrix in the lth

layer. σ(·) denotes an activation function such as
sigmoid or ReLU function. Such a graph convolu-
tional rule is motivated by the first-order approxi-
mation of localized spectral filters on graphs (Kipf
and Welling, 2016) and when applied recursively,
can extract interaction patterns among vertices.

Finally, the hidden representations in the final
GCN layer is merged into a single vector (called
a graphically merged matching vector) of a fixed
length, denoted by mAB , by taking the mean of
the hidden vectors of all vertices in the last layer.
The final matching score will be computed based
on mAB , through a classification network, e.g., a
multi-layered perceptron (MLP).

In addition to the graphically merged matching
vector mAB described above, we may also append
other global matching features to mAB to expand
the feature set. These additional global features
can be calculated, e.g., by encoding two docu-
ments directly with state-of-the-art language mod-
els like BERT (Devlin et al., 2018) or by directly
computing their term-based similarities. However,
we show in Sec. 4 that such global features can
hardly bring any more benefit to our scheme, as
the graphically merged matching vectors are al-
ready sufficiently expressive in our problem.

4 Evaluation

Tasks. We evaluate the proposed approach on the
task of identifying whether a pair of news arti-
cles report the same breaking news (or event) and
whether they belong to the same series of news
story, which is motivated by a real-world news

app. In fact, the proposed article pair matching
schemes have been deployed in the anonymous
news app for news clustering, with more than 110
millions of daily active users.

Note that traditional methods to document clus-
tering include unsupervised text clustering and
text classification into predefined topics. How-
ever, a number of breaking news articles emerge
on the Internet everyday with their topics/themes
unknown, so it is not possible to predefine their
topics. Thus, supervised text classification cannot
be used here. It is even impossible to determine
how many news clusters there exist. Therefore,
the task of classifying whether two news articles
are reporting the same breaking news event or be-
long to the same story is critical to news apps and
search engines for clustering, redundancy removal
and topic summarization.

In our task, an “event” refers to a piece of break-
ing news on which multiple media sources may
publish articles with different narratives and word-
ing. Furthermore, a “story” consists of a series of
logically related breaking news events. It is worth
noting that our objective is fundamentally differ-
ent from the traditional event coreference litera-
ture, e.g., (Bejan and Harabagiu, 2010; Lee et al.,
2013, 2012) or SemEval-2018 Task 5 (Counting
Events) (Postma et al., 2018), where the task is
to detect all the events (or in fact, “actions” like
shooting, car crashes) a document mentions.

In contrast, although a news article may men-
tion multiple entities and even previous physical
events, the “event” in our dataset always refers to
the breaking news that the article intends to report
or the incident that triggers the media’s coverage.
And our task is to identify whether two articles
intend to report the same breaking news. For ex-
ample, two articles “University of California sys-
tem libraries break off negotiations with Elsevier,
will no longer order their journals” and “Univer-
sity of California Boycotts Publishing Giant El-
sevier” from two different sources are apparently
intended to report the same breaking news event
of UC dropping subscription to Elsevier, although
other actions may be peripherally mentioned in
these articles, e.g., “eight months of unsuccess-
ful negotiations.” In addition, we do not attempt
to perform reading comprehension question an-
swering tasks either, e.g., finding out how many
killing incidents or car crashes there are in a year
(SemEval-2018 Task 5 (Postma et al., 2018)).



6289

Dataset Pos Samples Neg Samples Train Dev Test

CNSE 12865 16198 17438 5813 5812
CNSS 16887 16616 20102 6701 6700

Table 1: Description of evaluation datasets.

As a typical example, Fig. 3 shows the events
contained in the story 2016 U.S. presidential elec-
tion, where each tag shows a breaking news event
possibly reported by multiple articles with dif-
ferent narratives (articles not shown here). We
group highly coherent events together. For exam-
ple, there are multiple events about Election tele-
vision debates. One of our objectives is to identify
whether two news articles report the same event,
e.g., a yes when they are both reporting Trump and
Hilary’s first television debate, though with differ-
ent wording, or a no, when one article is report-
ing Trump and Hilary’s second television debate
while the other is talking about Donald Trump is
elected president.

Datasets. To the best of our knowledge, there
is no publicly available dataset for long document
matching tasks. We created two datasets: the Chi-
nese News Same Event dataset (CNSE) and Chi-
nese News Same Story dataset (CNSS), which are
labeled by professional editors. They contain long
Chinese news articles collected from major Inter-
net news providers in China, covering diverse top-
ics in the open domain. The CNSE dataset con-
tains 29, 063 pairs of news articles with labels rep-
resenting whether a pair of news articles are re-
porting about the same breaking news event. Sim-
ilarly, the CNSS dataset contains 33, 503 pairs of
articles with labels representing whether two doc-
uments fall into the same news story. The average
number of words for all documents in the datasets
is 734 and the maximum value is 21791.

In our datasets, we only labeled the major event
(or story) that a news article is reporting, since
in the real world, each breaking news article on
the Internet must be intended to report some spe-
cific breaking news that has just happened to at-
tract clicks and views. Our objective is to deter-
mine whether two news articles intend to report
the same breaking news.

Note that the negative samples in the two
datasets are not randomly generated: we select
document pairs that contain similar keywords, and
exclude samples with TF-IDF similarity below a
certain threshold. The datasets have been made
publicly available for research purpose.

Table 1 shows a detailed breakdown of the two
datasets. For both datasets, we use 60% of all
the samples as the training set, 20% as the devel-
opment (validation) set, and the remaining 20%
as the test set. We carefully ensure that different
splits do not contain any overlaps to avoid data
leakage. The metrics used for performance eval-
uation are the accuracy and F1 scores of binary
classification results. For each evaluated method,
we perform training for 10 epochs and then choose
the epoch with the best validation performance to
be evaluated on the test set.

Baselines. We test the following baselines:

• Matching by representation-focused or
interaction-focused deep neural network
models: DSSM (Huang et al., 2013), C-
DSSM (Shen et al., 2014), DUET (Mitra
et al., 2017), MatchPyramid (Pang et al.,
2016), ARC-I (Hu et al., 2014), ARC-II (Hu
et al., 2014). We use the implementations
from MatchZoo (Fan et al., 2017) for the
evaluation of these models.

• Matching by term-based similarities: BM25
(Robertson et al., 2009), LDA (Blei et al.,
2003) and SimNet (which is extracting the
five text-pair similarities mentioned in Sec. 3
and classifying by a multi-layer feedforward
neural network).

• Matching by a large-scale pre-training lan-
guage model: BERT (Devlin et al., 2018).

Note that we focus on the capability of long text
matching. Therefore, we do not use any short text
information, such as titles, in our approach or in
any baselines. In fact, the “relationship” between
two documents is not limited to ”whether the same
event or not”. Our algorithm is able to identify
a general relationship between documents, e.g.,
whether two episodes are from the same season
of a TV series. The definition of the relationship
(e.g., same event/story, same chapter of a book) is
solely defined and supervised by the labeled train-
ing data. For these tasks, the availability of other
information such as titles can not be assumed.

As shown in Table 2, we evaluate different vari-
ants of our own model to show the effect of differ-
ent sub-modules. In model names, “CIG” means
that in CIG, we directly use keywords as concepts
without community detection, whereas “CIGcd”



6290

Baselines CNSE CNSS Our models CNSE CNSSAcc F1 Acc F1 Acc F1 Acc F1

I. ARC-I 53.84 48.68 50.10 66.58 XI. CIG-Siam 74.47 73.03 75.32 78.58
II. ARC-II 54.37 36.77 52.00 53.83 XII. CIG-Siam-GCN 74.58 73.69 78.91 80.72
III. DUET 55.63 51.94 52.33 60.67 XIII. CIGcd-Siam-GCN 73.25 73.10 76.23 76.94
IV. DSSM 58.08 64.68 61.09 70.58 XIV. CIG-Sim 72.58 71.91 75.16 77.27
V. C-DSSM 60.17 48.57 52.96 56.75 XV. CIG-Sim-GCN 83.35 80.96 87.12 87.57
VI. MatchPyramid 66.36 54.01 62.52 64.56 XVI. CIGcd-Sim-GCN 81.33 78.88 86.67 87.00
VII. BM25 69.63 66.60 67.77 70.40 XVII. CIG-Sim&Siam-GCN 84.64 82.75 89.77 90.07
VIII. LDA 63.81 62.44 62.98 69.11 XVIII. CIG-Sim&Siam-GCN-Simg 84.21 82.46 90.03 90.29
IX. SimNet 71.05 69.26 70.78 74.50 XIX. CIG-Sim&Siam-GCN-BERTg 84.68 82.60 89.56 89.97
X. BERT fine-tuning 81.30 79.20 86.64 87.08 XX. CIG-Sim&Siam-GCN-Simg&BERTg 84.61 82.59 89.47 89.71

Table 2: Accuracy and F1-score results of different algorithms on CNSE and CNSS datasets.

means that each concept vertex in the CIG con-
tains a set of keywords grouped via community de-
tection. To generate the matching vector on each
vertex, “Siam” indicates the use of Siamese en-
coder, while “Sim” indicates the use of term-based
similarity encoder, as shown in Fig. 2. “GCN”
means that we convolve the local matching vec-
tors on vertices through GCN layers. Finally,
“BERTg” or “Simg” indicates the use of additional
global features given by BERT or the five term-
based similarity metrics mentioned in Sec. 3, ap-
pended to the graphically merged matching vector
mAB , for final classification.

Implementation Details. We use Stanford
CoreNLP (Manning et al., 2014) for word segmen-
tation (on Chinese text) and named entity recogni-
tion. For Concept Interaction Graph construction
with community detection, we set the minimum
community size (number of keywords contained
in a concept vertex) to be 2, and the maximum size
to be 6.

Our neural network model consists of word
embedding layer, Siamese encoding layer, Graph
transformation layers, and classification layer. For
embedding, we load the pre-trained word vectors
and fix it during training. The embeddings of out
of vocabulary words are set to be zero vectors. For
the Siamese encoding network, we use 1-D con-
volution with number of filters 32, followed by
an ReLU layer and Max Pooling layer. For graph
transformation, we utilize 2 layers of GCN (Kipf
and Welling, 2016) for experiments on the CNSS
dataset, and 3 layers of GCN for experiments on
the CNSE dataset. When the vertex encoder is the
five-dimensional features, we set the output size
of GCN layers to be 16. When the vertex encoder
is the Siamese network encoder, we set the output
size of GCN layers to be 128 except the last layer.
For the last GCN layer, the output size is always
set to be 16. For the classification module, it con-

sists of a linear layer with output size 16, an ReLU
layer, a second linear layer, and finally a Sigmoid
layer. Note that this classification module is also
used for the baseline method SimNet.

As we mentioned in Sec. 1, our code and
datasets have been open sourced. We implement
our model using PyTorch 1.0 (Paszke et al., 2017).
The experiments without BERT are carried out on
an MacBook Pro with a 2 GHz Intel Core i7 pro-
cessor and 8 GB memory. We use L2 weight de-
cay on all the trainable variables, with parame-
ter λ = 3 × 10−7. The dropout rate between
every two layers is 0.1. We apply gradient clip-
ping with maximum gradient norm 5.0. We use
the ADAM optimizer (Kingma and Ba, 2014) with
β1 = 0.8, β2 = 0.999, � = 10

8. We use a learn-
ing rate warm-up scheme with an inverse exponen-
tial increase from 0.0 to 0.001 in the first 1000
steps, and then maintain a constant learning rate
for the remainder of training. For all the exper-
iments, we set the maximum number of training
epochs to be 10.

4.1 Results and Analysis

Table 2 summarizes the performance of all
the compared methods on both datasets. Our
model achieves the best performance on both two
datasets and significantly outperforms all other
methods. This can be attributed to two reasons.
First, as the input of article pairs are re-organized
into Concept Interaction Graphs, the two doc-
uments are aligned along the corresponding se-
mantic units for easier concept-wise comparison.
Second, our model encodes local comparisons
around different semantic units into local match-
ing vectors, and aggregate them via graph convo-
lutions, taking semantic topologies into considera-
tion. Therefore, it solves the problem of matching
documents via divide-and-conquer, which is suit-
able for handling long text.



6291

Impact of Graphical Decomposition. Com-
paring method XI with methods I-VI in Table 2,
they all use the same word vectors and use neu-
ral networks for text encoding. The key differ-
ence is that our method XI compares a pair of ar-
ticles over a CIG in per-vertex decomposed fash-
ion. We can see that the performance of method
XI is significantly better than methods I-VI. Sim-
ilarly, comparing our method XIV with methods
VII-IX, they all use the same term-based similar-
ities. However, our method achieves significantly
better performance by using graphical decompo-
sition. Therefore, we conclude that graphical de-
composition can greatly improve long text match-
ing performance.

Note that the deep text matching models I-
VI lead to bad performance, because they were
invented mainly for sequence matching and can
hardly capture meaningful semantic interactions
in article pairs. When the text is long, it is hard
to get an appropriate context vector representa-
tion for matching. For interaction-focused neural
network models, most of the interactions between
words in two long articles will be meaningless.

Impact of Graph Convolutions. Compare
methods XII and XI, and compare methods XV
and XIV. We can see that incorporating GCN lay-
ers has significantly improved the performance on
both datasets. Each GCN layer updates the hid-
den vector of each vertex by integrating the vec-
tors from its neighboring vertices. Thus, the GCN
layers learn to graphically aggregate local match-
ing features into a final result.

Impact of Community Detection. By compar-
ing methods XIII and XII, and comparing methods
XVI and XV, we observe that using community
detection, such that each concept is a set of corre-
lated keywords instead of a single keyword, leads
to slightly worse performance. This is reasonable,
as using each keyword directly as a concept vertex
provides more anchor points for article compari-
son . However, community detection can group
highly coherent keywords together and reduces the
average size of CIGs from 30 to 13 vertices. This
helps to reduce the total training and testing time
of our models by as much as 55%. Therefore, one
may choose whether to apply community detec-
tion to trade accuracy off for speedups.

Impact of Multi-viewed Matching. Compar-
ing methods XVII and XV, we can see that the
concatenation of different graphical matching vec-

tors (both term-based and Siamese encoded fea-
tures) can further improve performance. This
demonstrates the advantage of combining multi-
viewed matching vectors.

Impact of Added Global Features. Compar-
ing methods XVIII, XIX, XX with method XVII,
we can see that adding more global features, such
as global similarities (Simg) and/or global BERT
encodings (BERTg) of the article pair, can hardly
improve performance any further. This shows that
graphical decomposition and convolutions are the
main factors that contribute to the performance
improvement. Since they already learn to aggre-
gate local comparisons into a global semantic re-
lationship, additionally engineered global features
cannot help.

Model Size and Parameter Sensitivity: Our
biggest model without BERT is XVIII, which
contains only ∼34K parameters. In comparison,
BERT contains 110M-340M parameters. How-
ever, our model significantly outperforms BERT.

We tested the sensitivity of different parame-
ters in our model. We found that 2 to 3 layers
of GCN layers gives the best performance. Fur-
ther introducing more GCN layers does not im-
prove the performance, while the performance is
much worse with zero or only one GCN layer. Fur-
thermore, in GCN hidden representations of a size
between 16 and 128 yield good performance. Fur-
ther increasing this size does not show obvious im-
provement.

For the optional community detection step in
CIG construction, we need to choose the minimum
size and the maximum size of communities. We
found that the final performance remains similar
if we vary the minimum size from 2∼3 and the
maximum size from 6∼10. This indicates that our
model is robust and insensitive to these parame-
ters.

Time complexity. For keywords of news arti-
cles, in real-world industry applications, they are
usually extracted in advance by highly efficient
off-the-shelf tools and pre-defined vocabulary. For
CIG construction, let Ns be the number of sen-
tences in two documents, Nw be the number of
unique words in documents, and Nk represents
the number of unique keywords in a document.
Building keyword graph requiresO(NsNk +N2w)
complexity (Sayyadi and Raschid, 2013), and
betweenness-based community detection requires
O(N3k ). The complexity of sentence assignment



6292

and weight calculation is O(NsNk + N2k ). For
graph classification, our model size is not big and
can process document pairs efficiently.

5 Related Work

Graphical Document Representation. A major-
ity of existing works can be generalized into four
categories: word graph, text graph, concept graph,
and hybrid graph. Word graphs use words in a
document as vertices, and construct edges based
on syntactic analysis (Leskovec et al., 2004), co-
occurrences (Zhang et al., 2018; Rousseau and
Vazirgiannis, 2013; Nikolentzos et al., 2017) or
preceding relation (Schenker et al., 2003). Text
graphs use sentences, paragraphs or documents
as vertices, and establish edges by word co-
occurrence, location (Mihalcea and Tarau, 2004),
text similarities (Putra and Tokunaga, 2017), or
hyperlinks between documents (Page et al., 1999).
Concept graphs link terms in a document to real
world concepts based on knowledge bases such as
DBpedia (Auer et al., 2007), and construct edges
based on syntactic/semantic rules. Hybrid graphs
(Rink et al., 2010; Baker and Ellsworth, 2017)
consist of different types of vertices and edges.

Text Matching. Traditional methods repre-
sent a text document as vectors of bag of words
(BOW), term frequency inverse document fre-
quency (TF-IDF), LDA (Blei et al., 2003) and
so forth, and calculate the distance between vec-
tors. However, they cannot capture the semantic
distance and usually cannot achieve good perfor-
mance.

In recent years, different neural network archi-
tectures have been proposed for text pair match-
ing tasks. For representation-focused models, they
usually transform text pairs into context represen-
tation vectors through a Siamese neural network,
followed by a fully connected network or score
function which gives the matching result based on
the context vectors (Qiu and Huang, 2015; Wan
et al., 2016; Liu et al., 2018; Mueller and Thya-
garajan, 2016; Severyn and Moschitti, 2015). For
interaction-focused models, they extract the fea-
tures of all pair-wise interactions between words
in text pairs, and aggregate the interaction fea-
tures by deep networks to give a matching result
(Hu et al., 2014; Pang et al., 2016). However, the
intrinsic structural properties of long text docu-
ments are not fully utilized by these neural models.
Therefore, they cannot achieve good performance

for long text pair matching.
There are also research works which utilize

knowledge (Wu et al., 2018), hierarchical property
(Jiang et al., 2019) or graph structure (Nikolentzos
et al., 2017; Paul et al., 2016) for long text match-
ing. In contrast, our method represents documents
by a novel graph representation and combines the
representation with GCN.

Finally, pre-training models such as BERT (De-
vlin et al., 2018) can also be utilized for text
matching. However, the model is of high complex-
ity and is hard to satisfy the speed requirement in
real-world applications.

Graph Convolutional Networks. We also con-
tributed to the use of GCNs to identify the rela-
tionship between a pair of graphs, whereas pre-
viously, different GCN architectures have mainly
been used for completing missing attributes/links
(Kipf and Welling, 2016; Defferrard et al., 2016)
or for node clustering or classification (Hamilton
et al., 2017), but all within the context of a sin-
gle graph, e.g., a knowledge graph, citation net-
work or social network. In this work, the pro-
posed Concept Interaction Graph takes a simple
approach to represent a document by a weighted
undirected graph, which essentially helps to de-
compose a document into subsets of sentences,
each subset focusing on a different sub-topic or
concept.

6 Conclusion

We propose the Concept Interaction Graph to or-
ganize documents into a graph of concepts, and in-
troduce a divide-and-conquer approach to match-
ing a pair of articles based on graphical decom-
position and convolutional aggregation. We cre-
ated two new datasets for long document matching
with the help of professional editors, consisting
of about 60K pairs of news articles, on which we
have performed extensive evaluations. In the ex-
periments, our proposed approaches significantly
outperformed an extensive range of state-of-the-
art schemes, including both term-based and deep-
model-based text matching algorithms. Results
suggest that the proposed graphical decomposi-
tion and the structural transformation by GCN lay-
ers are critical to the performance improvement in
matching article pairs.



6293

References
Sören Auer, Christian Bizer, Georgi Kobilarov, Jens

Lehmann, Richard Cyganiak, and Zachary Ives.
2007. Dbpedia: A nucleus for a web of open data.
The semantic web, pages 722–735.

Collin Baker and Michael Ellsworth. 2017. Graph
methods for multilingual framenets. In Proceedings
of TextGraphs-11: the Workshop on Graph-based
Methods for Natural Language Processing, pages
45–50.

Cosmin Adrian Bejan and Sanda Harabagiu. 2010. Un-
supervised event coreference resolution with rich
linguistic features. In Proceedings of the 48th An-
nual Meeting of the Association for Computational
Linguistics, pages 1412–1422. Association for Com-
putational Linguistics.

David M Blei, Andrew Y Ng, and Michael I Jordan.
2003. Latent dirichlet allocation. Journal of ma-
chine Learning research, 3(Jan):993–1022.

Daniel Bruggermann, Yannik Hermey, Carsten Orth,
Darius Schneider, Stefan Selzer, and Gerasimos
Spanakis. 2016. Storyline detection and tracking us-
ing dynamic latent dirichlet allocation. In Proceed-
ings of the 2nd Workshop on Computing News Sto-
rylines (CNS 2016), pages 9–19.

Michaël Defferrard, Xavier Bresson, and Pierre Van-
dergheynst. 2016. Convolutional neural networks on
graphs with fast localized spectral filtering. In Ad-
vances in Neural Information Processing Systems,
pages 3844–3852.

Jacob Devlin, Ming-Wei Chang, Kenton Lee, and
Kristina Toutanova. 2018. Bert: Pre-training of deep
bidirectional transformers for language understand-
ing. arXiv preprint arXiv:1810.04805.

Yixing Fan, Liang Pang, JianPeng Hou, Jiafeng Guo,
Yanyan Lan, and Xueqi Cheng. 2017. Matchzoo:
A toolkit for deep text matching. arXiv preprint
arXiv:1707.07270.

William L Hamilton, Rex Ying, and Jure Leskovec.
2017. Representation learning on graphs: Methods
and applications. arXiv preprint arXiv:1709.05584.

Baotian Hu, Zhengdong Lu, Hang Li, and Qingcai
Chen. 2014. Convolutional neural network architec-
tures for matching natural language sentences. In
Advances in neural information processing systems,
pages 2042–2050.

Po-Sen Huang, Xiaodong He, Jianfeng Gao, Li Deng,
Alex Acero, and Larry Heck. 2013. Learning deep
structured semantic models for web search using
clickthrough data. In Proceedings of the 22nd ACM
international conference on Conference on informa-
tion & knowledge management, pages 2333–2338.
ACM.

Jyun-Yu Jiang, Mingyang Zhang, Cheng Li, Mike Ben-
dersky, Nadav Golbandi, and Marc Najork. 2019.
Semantic text matching for long-form documents.

Diederik P Kingma and Jimmy Ba. 2014. Adam: A
method for stochastic optimization. arXiv preprint
arXiv:1412.6980.

Thomas N Kipf and Max Welling. 2016. Semi-
supervised classification with graph convolutional
networks. arXiv preprint arXiv:1609.02907.

Heeyoung Lee, Angel Chang, Yves Peirsman,
Nathanael Chambers, Mihai Surdeanu, and Dan
Jurafsky. 2013. Deterministic coreference resolu-
tion based on entity-centric, precision-ranked rules.
Computational Linguistics, 39(4):885–916.

Heeyoung Lee, Marta Recasens, Angel Chang, Mihai
Surdeanu, and Dan Jurafsky. 2012. Joint entity and
event coreference resolution across documents. In
Proceedings of the 2012 Joint Conference on Empir-
ical Methods in Natural Language Processing and
Computational Natural Language Learning, pages
489–500. Association for Computational Linguis-
tics.

Jure Leskovec, Marko Grobelnik, and Natasa Milic-
Frayling. 2004. Learning sub-structures of docu-
ment semantic graphs for document summarization.

Bang Liu, Di Niu, Kunfeng Lai, Linglong Kong, and
Yu Xu. 2017. Growing story forest online from
massive breaking news. In Proceedings of the 2017
ACM on Conference on Information and Knowledge
Management, pages 777–785. ACM.

Bang Liu, Ting Zhang, Fred X Han, Di Niu, Kunfeng
Lai, and Yu Xu. 2018. Matching natural language
sentences with hierarchical sentence factorization.
In Proceedings of the 2018 World Wide Web Con-
ference on World Wide Web, pages 1237–1246. In-
ternational World Wide Web Conferences Steering
Committee.

Christopher Manning, Mihai Surdeanu, John Bauer,
Jenny Finkel, Steven Bethard, and David McClosky.
2014. The stanford corenlp natural language pro-
cessing toolkit. In Proceedings of 52nd annual
meeting of the association for computational lin-
guistics: system demonstrations, pages 55–60.

Rada Mihalcea and Paul Tarau. 2004. Textrank: Bring-
ing order into text. In Proceedings of the 2004 con-
ference on empirical methods in natural language
processing.

Tomas Mikolov, Kai Chen, Greg Corrado, and Jef-
frey Dean. 2013. Efficient estimation of word
representations in vector space. arXiv preprint
arXiv:1301.3781.

Bhaskar Mitra, Fernando Diaz, and Nick Craswell.
2017. Learning to match using local and distributed
representations of text for web search. In Proceed-
ings of the 26th International Conference on World



6294

Wide Web, pages 1291–1299. International World
Wide Web Conferences Steering Committee.

Jonas Mueller and Aditya Thyagarajan. 2016. Siamese
recurrent architectures for learning sentence similar-
ity. In Thirtieth AAAI Conference on Artificial Intel-
ligence.

Paul Neculoiu, Maarten Versteegh, Mihai Rotaru, and
Textkernel BV Amsterdam. 2016. Learning text
similarity with siamese recurrent networks. ACL
2016, page 148.

Giannis Nikolentzos, Polykarpos Meladianos, François
Rousseau, Yannis Stavrakas, and Michalis Vazir-
giannis. 2017. Shortest-path graph kernels for doc-
ument similarity. In Proceedings of the 2017 Con-
ference on Empirical Methods in Natural Language
Processing, pages 1890–1900.

Lawrence Page, Sergey Brin, Rajeev Motwani, and
Terry Winograd. 1999. The pagerank citation rank-
ing: Bringing order to the web. Technical report,
Stanford InfoLab.

Liang Pang, Yanyan Lan, Jiafeng Guo, Jun Xu,
Shengxian Wan, and Xueqi Cheng. 2016. Text
matching as image recognition. In AAAI, pages
2793–2799.

Adam Paszke, Sam Gross, Soumith Chintala, and Gre-
gory Chanan. 2017. Pytorch: Tensors and dynamic
neural networks in python with strong gpu acceler-
ation. PyTorch: Tensors and dynamic neural net-
works in Python with strong GPU acceleration.

Christian Paul, Achim Rettinger, Aditya Mogadala,
Craig A Knoblock, and Pedro Szekely. 2016. Effi-
cient graph-based document similarity. In European
Semantic Web Conference, pages 334–349. Springer.

Marten Postma, Filip Ilievski, and Piek Vossen. 2018.
Semeval-2018 task 5: Counting events and par-
ticipants in the long tail. In Proceedings of The
12th International Workshop on Semantic Evalua-
tion, pages 70–80.

Jan Wira Gotama Putra and Takenobu Tokunaga. 2017.
Evaluating text coherence based on semantic sim-
ilarity graph. In Proceedings of TextGraphs-11:
the Workshop on Graph-based Methods for Natural
Language Processing, pages 76–85.

Xipeng Qiu and Xuanjing Huang. 2015. Convolutional
neural tensor network architecture for community-
based question answering. In IJCAI, pages 1305–
1311.

Bryan Rink, Cosmin Adrian Bejan, and Sanda M
Harabagiu. 2010. Learning textual graph patterns
to detect causal event relations. In FLAIRS Confer-
ence.

Stephen Robertson, Hugo Zaragoza, et al. 2009. The
probabilistic relevance framework: Bm25 and be-
yond. Foundations and Trends R© in Information Re-
trieval, 3(4):333–389.

François Rousseau and Michalis Vazirgiannis. 2013.
Graph-of-word and tw-idf: new approach to ad hoc
ir. In Proceedings of the 22nd ACM international
conference on Information & Knowledge Manage-
ment, pages 59–68. ACM.

Hassan Sayyadi and Louiqa Raschid. 2013. A graph
analytical approach for topic detection. ACM Trans-
actions on Internet Technology (TOIT), 13(2):4.

Adam Schenker, Mark Last, Horst Bunke, and Abra-
ham Kandel. 2003. Clustering of web docu-
ments using a graph model. SERIES IN MA-
CHINE PERCEPTION AND ARTIFICIAL INTEL-
LIGENCE, 55:3–18.

Aliaksei Severyn and Alessandro Moschitti. 2015.
Learning to rank short text pairs with convolutional
deep neural networks. In Proceedings of the 38th in-
ternational ACM SIGIR conference on research and
development in information retrieval, pages 373–
382. ACM.

Dafna Shahaf, Jaewon Yang, Caroline Suen, Jeff Ja-
cobs, Heidi Wang, and Jure Leskovec. 2013. Infor-
mation cartography: creating zoomable, large-scale
maps of information. In Proceedings of the 19th
ACM SIGKDD international conference on Knowl-
edge discovery and data mining, pages 1097–1105.
ACM.

Yelong Shen, Xiaodong He, Jianfeng Gao, Li Deng,
and Grégoire Mesnil. 2014. Learning semantic rep-
resentations using convolutional neural networks for
web search. In Proceedings of the 23rd Interna-
tional Conference on World Wide Web, pages 373–
374. ACM.

Piek Vossen, Tommaso Caselli, and Yiota Kont-
zopoulou. 2015. Storylines for structuring massive
streams of news. In Proceedings of the First Work-
shop on Computing News Storylines, pages 40–49.

Shengxian Wan, Yanyan Lan, Jiafeng Guo, Jun Xu,
Liang Pang, and Xueqi Cheng. 2016. A deep ar-
chitecture for semantic matching with multiple po-
sitional sentence representations. In AAAI, vol-
ume 16, pages 2835–2841.

Yu Wu, Wei Wu, Can Xu, and Zhoujun Li. 2018.
Knowledge enhanced hybrid neural network for text
matching. In Thirty-Second AAAI Conference on
Artificial Intelligence.

Ting Zhang, Bang Liu, Di Niu, Kunfeng Lai, and
Yu Xu. 2018. Multiresolution graph attention net-
works for relevance matching. In Proceedings of
the 27th ACM International Conference on Informa-
tion and Knowledge Management, pages 933–942.
ACM.

Deyu Zhou, Haiyang Xu, and Yulan He. 2015. An un-
supervised bayesian modelling approach for story-
line detection on news articles. In Proceedings of
the 2015 Conference on Empirical Methods in Nat-
ural Language Processing, pages 1943–1948.


