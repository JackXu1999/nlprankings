



















































Model Adaptation for Personalized Opinion Analysis


Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics
and the 7th International Joint Conference on Natural Language Processing (Short Papers), pages 769–774,

Beijing, China, July 26-31, 2015. c©2015 Association for Computational Linguistics

Model Adaptation for Personalized Opinion Analysis

Mohammad Al Boni1, Keira Qi Zhou1, Hongning Wang2, and Matthew S. Gerber1

1Department of Systems and Information Engineering
2Department of Computer Science

1,2University of Virginia, USA
1,2{ma2sm,qz4aq,hw5x,msg8u}@virginia.edu

Abstract

Humans are idiosyncratic and variable: to-
wards the same topic, they might hold dif-
ferent opinions or express the same opin-
ion in various ways. It is hence impor-
tant to model opinions at the level of in-
dividual users; however it is impractical
to estimate independent sentiment classi-
fication models for each user with limited
data. In this paper, we adopt a model-
based transfer learning solution – using
linear transformations over the parame-
ters of a generic model – for personalized
opinion analysis. Extensive experimental
results on a large collection of Amazon
reviews confirm our method significantly
outperformed a user-independent generic
opinion model as well as several state-of-
the-art transfer learning algorithms.

1 Introduction

The proliferation of user-generated opinionated
text data has fueled great interest in opinion analy-
sis (Pang and Lee, 2008; Liu, 2012). Understand-
ing opinions expressed by a population of users
has value in a wide spectrum of areas, including
social network analysis (Bodendorf and Kaiser,
2009), business intelligence (Gamon et al., 2005),
marketing analysis (Jansen et al., 2009), person-
alized recommendation (Yang et al., 2013) and
many more.

Most of the existing opinion analysis research
focuses on population-level analyses, i.e., predict-
ing opinions based on models estimated from a
collection of users. The underlying assumption is
that users are homogeneous in the way they ex-
press opinions. Nevertheless, different users may
use the same words to express distinct opinions.
For example, the word “expensive” tends to be
associated with negative sentiment in general, al-
though some users may use it to describe their sat-
isfaction with a product’s quality. Failure to rec-

ognize this difference across users will inevitably
lead to inaccurate understanding of opinions.

However, due to the limited availability of user-
specific opinionated data, it is impractical to es-
timate independent models for each user. In this
work, we propose a transfer learning based solu-
tion, named LinAdapt, to address this challenge.
Instead of estimating independent classifiers for
each user, we start from a generic model and adapt
it toward individual users based on their own opin-
ionated text data. In particular, our key assump-
tion is that the adaptation can be achieved via a set
of linear transformations over the generic model’s
parameters. When we have sufficient observations
for a particular user, the transformations will push
the adapted model towards the user’s personalized
model; otherwise, it will back off to the generic
model. Empirical evaluations on a large collection
of Amazon reviews verify the effectiveness of the
proposed solution: it significantly outperformed a
user-independent generic model as well as several
state-of-the-art transfer learning algorithms.

Our contribution is two-fold: 1) we enable ef-
ficient personalization of opinion analysis via a
transfer learning approach, and 2) the proposed so-
lution is general and applicable to any linear model
for user opinion analysis.

2 Related Work

Sentiment Analysis refers to the process of iden-
tifying subjective information in source materials
(Pang and Lee, 2008; Liu, 2012). Typical tasks in-
clude: 1) classifying textual documents into posi-
tive and negative polarity categories, (Dave et al.,
2003; Kim and Hovy, 2004); 2) identifying textual
topics and their associated opinions (Wang et al.,
2010; Jo and Oh, 2011); and 3) opinion summa-
rization (Hu and Liu, 2004; Ku et al., 2006). Ap-
proaches for these tasks focus on population-level
opinion analyses, in which one model is shared
across all users. Little effort has been devoted
to personalized opinion analyses, where each user
has a particular model, due to the absence of user-

769



specific opinion data for model estimation.
Transfer Learning aims to help improve pre-

dictive models by using knowledge from different
but related problems (Pan and Yang, 2010). In
the opinion mining community, transfer learning
is used primarily for domain adaptation. Blitzer
et al. (2006) proposed structural correspondence
learning to identify the correspondences among
features between different domains via the concept
of pivot features. Pan et al. (2010) propose a spec-
tral feature alignment algorithm to align domain-
specific sentiment words from different domains
for sentiment categorization. By assuming that
users tend to express consistent opinions towards
the same topic over time, Guerra et al. (2011) ap-
plied instance-based transfer learning for real time
sentiment analysis.

Our method is inspired by a personalized rank-
ing model adaptation method developed by Wang
et al. (2013). To the best of our knowledge, our
work is the first to estimate user-level classifiers
for opinion analysis. By adapting a generic opin-
ion classification model for each user, heterogene-
ity among their expressions of opinions can be
captured and it help us understand users’ opinions
at a finer granularity.

3 Linear Transformation Based Model
Adaptation

Given a generic sentiment classification model y=
fs(x), we aim at finding an optimal adapted model
y = fu(x) for user u, such that fu(x) best cap-
tures u’s opinion in his/her generated textual doc-
uments Du ={xd, yd}|D|d=1, where xd is the feature
vector for document d, yd is the sentiment class
label (e.g., positive v.s., negative). To achieve so,
we assume that such adaptation can be performed
via a series of linear transformations on f s(x)’s
model parameter ws. This assumption is general
and can be applied to a wide variety of sentiment
classifiers, e.g., logistic regression and linear sup-
port vector machines, as long as they have a linear
core function. Therefore, we name our proposed
method as LinAdapt. In this paper, we focus on
logistic regression (Pang et al., 2002); but the pro-
posed procedures can be easily adopted for many
other classifiers (Wang et al., 2013).

Our global model y=fs(x) can be written as,

P s(yd = 1|xd) = 1
1 + e−wsTxd

(1)

where ws are the linear coefficients for the corre-
sponding document features.

Standard linear transformations, i.e., scaling,
shifting and rotation, can be encoded via a V ×

(V + 1) matrix Au for each user u as:



aug(1) c
u
g(1),12 c

u
g(1),13 0 0 b

u
g(1)

cug(2),21 a
u
g(2) c

u
g(2),23 . . . 0 b

u
g(2)

cug(3),31 c
u
g(3),32 a

u
g(3)

. . .
... bug(3)

0 . . . . . . . . .
. . .

...
0 0 . . . . . . aug(V ) b

u
g(V )


where V is the total number of features.

However, the above transformation introduces
O(V 2) free parameters, which are even more than
the number of free parameters required to estimate
a new logistic regression model. Following the so-
lution proposed by Wang et al. (2013), we further
assume the transformations can be performed in a
group-wise manner to reduce the size of param-
eters in adaptation. The intuition behind this as-
sumption is that features that share similar contri-
butions to the classification model are more likely
to be adapted in the same way. Another advantage
of feature grouping is that the feedback informa-
tion will be propagated through the features in the
same group while adaptation; hence the features
that are not observed in the adaptation data can
also be updated properly.

We denote g(·) as the feature grouping function,
which maps V original features to K groups, and
auk , b

u
k and c

u
k as the scaling, shifting and rotation

operations over ws in group k for user u. In addi-
tion, rotation is only performed for the features in
the same group, and it is assumed to be symmetric,
i.e., cuk,ij = c

u
k,ji, where g(i) = k and g(j) = k.

As a result, the personalized classification model
fu(x) after adaptation can be written as,

Pu(yd = 1|xd) = 1
1 + e−(Auw̃s)Txd

(2)

where w̃s = (ws, 1) to accommodate the shifting
operation.

The optimal transformation matrix Au for user
u can be estimated by maximum likelihood esti-
mation based on user u’s own opinionated docu-
ment collection Du. To avoid overfitting, we pe-
nalize the transformation which increases the dis-
crepancy between the adapted model and global
model by the following regularization term,

R(Au) = −η
2

K∑
k=1

(auk − 1)2 − σ
2

K∑
k=1

buk
2

− �
2

K∑
k=1

∑
i,g(i)=k

∑
j 6=i,g(j)=k

cuk,ij
2, (3)

where η, σ and � are trade-off parameters control-
ling the balance among shifting, scaling and rota-
tion operations in adaptation.

770



Combining the newly introduced regularization
term for Au and log-likelihood function for logis-
tic regression, we get the following optimization
problem to estimate the adaptation parameters,

max
Au

L(Au) = LLR(D
u;Pu) +R(Au) (4)

whereLLR(Du;P u) is the log-likelihood of logis-
tic regression on collection Du, and P u is defined
in Eq (2).

Gradient-based method is used to optimize
Eq (4), in which the gradient for auk , b

u
k and c

u
k

can be calculated as,

∂L(Au)

∂ak
=

Du∑
d=1

{yd[1− p(yd|xd)]
∑

i,g(i)=k

wsi xdi}−η(ak − 1)

∂L(Au)

∂bk
=

Du∑
d=1

{yd[1− p(yd|xd)]
∑

i,g(i)=k

xdi}−σbk

∂L(Au)

∂ck,ij
=

Du∑
d=1

{yd[1− p(yd|xd)]wsjxdi}−�ck,ij

4 Experiments and Discussion

We performed empirical evaluations of the pro-
posed LinAdapt algorithm on a large collection of
product review documents. We compared our ap-
proach with several state-of-the-art transfer learn-
ing algorithms. In the following, we will first in-
troduce the evaluation corpus and baselines, and
then discuss our experimental findings.

4.1 Data Collection and Baselines
We used a corpus of Amazon reviews provided
on Stanford SNAP website by McAuley and
Leskovec. (2013). We performed simple data pre-
processing: 1) annotated the reviews with ratings
greater than 3 stars (out of total 5 stars) as positive,
and others as negative; 2) removed duplicate re-
views; 3) removed reviewers who have more than
1,000 reviews or more than 90% positive or neg-
ative reviews; 4) chronologically ordered the re-
views in each user. We extracted unigrams and bi-
grams to construct bag-of-words feature represen-
tations for the review documents. Standard stop-
word removal (Lewis et al., 2004) and Porter stem-
ming (Willett, 2006) were applied. Chi-square
and information gain (Yang and Pedersen, 1997)
were used for feature selection and the union of
the resulting selected features are used in the fi-
nal controlled vocabulary. The resulting evalua-
tion data set contains 32,930 users, 281,813 posi-
tive reviews, and 81,522 negative reviews, where
each review is represented with 5,000 text features
with TF-IDF as the feature value.

Our first baseline is an instance-based adapta-
tion method (Brighton and Mellish, 2002). The k-
nearest neighbors of each testing review document
are found from the shared training set for person-
alized model training. As a result, for each test-
ing case, we are estimating an independent clas-
sification model. We denote this method as “Re-
Train.” The second baseline builds on the model-
based adaptation method developed by Geng et
al. (2012). For each user, it enforces the adapted
model to be close to the global model via an ad-
ditional L2 regularization when training the per-
sonalized model. But the full set of parameters in
logistic regression need to estimated during adap-
tation. We denote this method as “Reg-LR.”

In our experiments, all model adaptation is per-
formed in an online fashion: we first applied the
up-to-date classification model on the given test-
ing document; evaluated the model’s performance
with ground-truth; and used the feedback to up-
date the model. Because the class distribution of
our evaluation data set is highly skewed (77.5%
positive), it is important to evaluate the adapted
models’ performance on both classes. In the fol-
lowing comparisons, we report the average F-1
measure of both positive and negative classes.

4.2 Comparison of Adaptation Performance

First we need to estimate a global model for adap-
tation. A typical approach is to collect a portion
of historical reviews from each user to construct a
shared training corpus (Wang et al., 2013). How-
ever, this setting is problematic: it already exploits
information from every user and does not reflect
the reality that some (new) users might not exist
when training the global model. In our experi-
ment, we isolated a group of random users for
global model training. In addition, since there are
multiple categories in this review collection, such
as book, movies, electronics, etc, and each user
might discuss various categories, it is infeasible
to balance the coverage of different categories in
global model training by only selecting the users.
As a result, we vary the number of reviews in each
domain from the selected training users to estimate
the global model. We started with 1000 reviews
from the top 5 categories (Movies & TV, Books,
Music, Home & Kitchen, and Video Games), then
evaluated the global model on 10,000 testing users
which consist of three groups: light users with 2 to
10 reviews, medium users with 11 to 50 reviews,
and heavy users with 51 to 200 reviews. After each
evaluation run, we added an extra 1000 reviews
and repeated the training and evaluation.

771



Table 1: Global model training with varying size
of training corpus.

Model Metric 1000 2000 3000 4000 5000

Global Pos F1 0.741 0.737 0.738 0.734 0.729Neg F1 0.106 0.126 0.125 0.132 0.159

LinAdapt Pos F1 0.694 0.693 0.692 0.694 0.696Neg F1 0.299 0.299 0.296 0.299 0.304

Table 2: Effect of feature grouping in LinAdapt.

Method Metric 100 200 400 800 1000

Rand Pos F1 0.691 0.692 0.696 0.686 0.681Neg F1 0.295 0.298 0.300 0.322 0.322

SVD Pos F1 0.691 0.698 0.704 0.697 0.696Neg F1 0.298 0.302 0.300 0.322 0.334

Cross Pos F1 0.701 0.702 0.705 0.700 0.696Neg F1 0.298 0.299 0.303 0.328 0.331

To understand the effect of global model train-
ing in model adaptation, we also included the per-
formance of LinAdapt, which only used shifting
and scaling operations and Cross feature group-
ing method with k = 400 (detailed feature group-
ing method will be discussed in the next exper-
iment). Table 1 shows the performance of the
global model and LinAdapt with respect to differ-
ent training corpus size. We found that the global
model converged very quickly with around 5,000
reviews, and this gives the best compromise for
both positive and negative classes in both global
and adaptaed model. Therefore, we will use this
global model for later adaptation experiments.

We then investigated the effect of feature group-
ing in LinAdapt. We employed the feature group-
ing methods of SV D and Cross developed by
Wang et al. (2013). A random feature grouping
method is included to validate the necessity of
proper feature grouping. We varied the number
of feature groups from 100 to 1000, and evaluated
the adapted models using the same 10,000 testing
users from the previous experiment. As shown in
Table 2, Cross provided the best adaptation per-
formance and random is the worse; a moderate
group size balances performance between positive
and negative classes. For the remaining experi-
ments, we use the Cross grouping with k = 400
in LinAdapt. In this group setting, we found that
the average number of features per group is 12.47
while the median is 12, which means that features
are normally distributed across different groups.

Next, we investigated the effect of differ-
ent linear operations in LinAdapt, and com-
pared LinAdapt against the baselines. We started
LinAdapt with only the shifting operation, and
then included scaling and rotation. To validate
the necessity of personalizing sentiment classifica-

tion models, we also included the global model’s
performance in Figure 1. In particular, to under-
stand the longitudinal effect of personalized model
adaptation, we only used the heavy users (4,021
users) in this experiment. The results indicate
that the adapted models outperformed the global
model in identifying the negative class; while the
global model performs the best in recognizing pos-
itive reviews. This is due to the heavily biased
class distribution in our collection: global model
puts great emphasis on the positive reviews; while
the adaptation methods give equal weights to both
positive and negative reviews. In particular, in
LinAdapt, scaling and shifting operations lead to
satisfactory adaptation performance for the nega-
tive class with only 15 reviews; while rotation is
essential for recognizing the positive class.

To better understand the improvement of model
adaptation against the global model in different
types of users, we decomposed the performance
gain of different adaptation methods. For this ex-
periment, we used all the 10,000 testing users:
we used the first 50% of the reviews from each
user for adaptation and the rest for testing. Ta-
ble 3 shows the performance gain of different al-
gorithms under light, medium and heavy users.
For the heavy and medium users, which only
consist 0.1% and 35% of the total population in
our data set, our adaptation model achieved the
best improvement against the global model com-
pared with Reg-LR and ReTrain. For the light
users, who cover 64.9% of the total population,
LinAdapt was able to improve the performance
against the global model for the negative class, but
Reg-LR and ReTrain had attained higher perfor-
mance. For the positive class, none of those adap-
tation methods can improve over the global model
although they provide a very close performance (in
LinAdapt, the differences are not significant). The
significant improvement in negative class predic-
tion from model adaptation is encouraging con-
sidering the biased distribution of classes, which
results in poor performance in the global model.

The above improved classification performance
indicates the adapted model captures the hetero-
geneity in expressing opinions across users. To
verify this, we investigated textual features whose
sentiment polarities are most/least frequently up-
dated across users. We computed the variance of
the absolute difference between the learned feature
weights in LinAdapt and global model. High vari-
ance indicates the word’s sentiment polarity fre-
quently changes across different users. But there
are two reasons for a low variance: first, a rare

772



10 20 30 40 50 60 70 80 90 100
0

0.1

0.2

0.3

0.4

0.5

0.6

0.7

0.8

0.9

F
−M

ea
su

re

# adaption documents

 

 

shifting
shifting+scaling
shifting+scaling+rotation
Global
ReTrain
Reg−LR2 4 6 8 10

0

0.1

0.2

0.3

0.4

0.5

0.6

0.7

0.8

Zoomed Part

10 20 30 40 50 60 70 80 90 100
0

0.05

0.1

0.15

0.2

0.25

0.3

0.35

0.4

F
−M

ea
su

re

# adaption documents

 

 

shifting
shifting+scaling
shifting+scaling+rotation
Global
ReTrain
Reg−LR

(a) Positive F-1 measure (b) Negative F-1 measure

Figure 1: Online adaptation performance comparisons.

Table 3: User-level performance gain over global
model from ReTrain, Reg-LR and LinAdapt.

Method User Class Pos F1 Neg F1

ReTrain
Heavy -0.092 0.155∗

Medium -0.095 0.235∗
Light -0.157∗ 0.255∗

Reg-LR
Heavy -0.010 0.109∗

Medium -0.005 0.206∗
Light -0.060 0.232∗

LinAdapt
Heavy -0.046 0.248∗

Medium -0.049 0.235∗
Light -0.091 0.117∗

∗ p-value< 0.05 with paired t-test.
Table 4: Top 10 words with the highest and lowest
variance of learned polarity in LinAdapt.

Variance Features

Highest
waste good attempt
money return save
poor worst annoy

Lowest
lover correct pure
care the product odd
sex evil less than

word that is not used by many users; second, a
word is being used frequently, yet, with the same
polarity. We are only interested in the second case.
Therefore, for each word, we compute its user fre-
quency (UF), i.e., how many unique users used
this word in their reviews. Then, we selected 1000
most popular features by UF, and ranked them ac-
cording to the variance of learned sentiment polar-
ities. Table 4 shows the top ten features with the
highest and lowest polarity variance.

We inspected the learned weights in the adapted
models in each user from LinAdapt, and found
the words like waste, poor, and good share the
same sentiment polarity as in the global model
but different magnitudes; while words like money,
instead, and return are almost neutral in global
model, but vary across the personalized models.
On the other hand, words such as care, sex, evil,
pure, and correct constantly carry the same sen-

Table 5: Learned sentiment polarity range of three
typical words in LinAdapt.

Feature Range Global Used as Used as
Weight Positive Negative

Experience [-0.231,0.232] 0.002 3348 1503
Good [-0.170,0.816] 0.032 8438 1088
Money [-0.439,0.074] -0.013 646 6238

timent across users. Table 5 shows the detailed
range of learned polarity for three typical opin-
ion words in 10,000 users. This result indicates
LinAdapt well captures the fact that users express
opinions differently even with the same words.

5 Conclusion and Future Work

In this paper, we developed a transfer learning
based solution for personalized opinion mining.
Linear transformations of scaling, shifting and ro-
tation are exploited to adapt a global sentiment
classification model for each user. Empirical
evaluations based on a large collection of opin-
ionated review documents confirm that the pro-
posed method effectively models personal opin-
ions. By analyzing the variance of the learned
feature weights, we are able to discover words
that hold different polarities across users, which
indicates our model captures the fact that users
express opinions differently even with the same
words. In the future, we plan to further explore
this linear transformation based adaptation from
different perspectives, e.g., sharing adaptation op-
erations across users or review categories.

6 Acknowledgements

This research was funded in part by grant
W911NF-10-2-0051 from the United States Army
Research Laboratory. Also, Hongning Wang is
partially supported by the Yahoo Academic Career
Enhancement Award.

773



References
John Blitzer, Ryan McDonald, and Fernando Pereira.

2006. Domain adaptation with structural correspon-
dence learning. In Proceedings of the 2006 confer-
ence on empirical methods in natural language pro-
cessing, pages 120–128. Association for Computa-
tional Linguistics.

Freimut Bodendorf and Carolin Kaiser. 2009. Detect-
ing opinion leaders and trends in online social net-
works. In Proceedings of the 2nd ACM workshop on
Social web search and mining, pages 65–68. ACM.

Henry Brighton and Chris Mellish. 2002. Advances
in instance selection for instance-based learning al-
gorithms. Data mining and knowledge discovery,
6(2):153–172.

Pedro Henrique Calais Guerra, Adriano Veloso, Wag-
ner Meira Jr, and Virgı́lio Almeida. 2011. From
bias to opinion: a transfer-learning approach to real-
time sentiment analysis. In Proceedings of the 17th
ACM SIGKDD international conference on Knowl-
edge discovery and data mining, pages 150–158.
ACM.

Kushal Dave, Steve Lawrence, and David M Pennock.
2003. Mining the peanut gallery: Opinion extraction
and semantic classification of product reviews. In
Proceedings of the 12th international conference on
World Wide Web, pages 519–528. ACM.

Michael Gamon, Anthony Aue, Simon Corston-Oliver,
and Eric Ringger. 2005. Pulse: Mining customer
opinions from free text. In Advances in Intelligent
Data Analysis VI, pages 121–132. Springer.

Bo Geng, Yichen Yang, Chao Xu, and Xian-Sheng
Hua. 2012. Ranking model adaptation for domain-
specific search. Knowledge and Data Engineering,
IEEE Transactions on, 24(4):745–758.

Minqing Hu and Bing Liu. 2004. Mining and summa-
rizing customer reviews. In Proceedings of the tenth
ACM SIGKDD international conference on Knowl-
edge discovery and data mining, pages 168–177.
ACM.

Bernard J Jansen, Mimi Zhang, Kate Sobel, and Ab-
dur Chowdury. 2009. Twitter power: Tweets as
electronic word of mouth. Journal of the Ameri-
can society for information science and technology,
60(11):2169–2188.

Yohan Jo and Alice H Oh. 2011. Aspect and senti-
ment unification model for online review analysis.
In Proceedings of the fourth ACM international con-
ference on Web search and data mining, pages 815–
824. ACM.

Soo-Min Kim and Eduard Hovy. 2004. Determin-
ing the sentiment of opinions. In Proceedings of
the 20th international conference on Computational
Linguistics, page 1367. Association for Computa-
tional Linguistics.

Lun-Wei Ku, Yu-Ting Liang, and Hsin-Hsi Chen.
2006. Opinion extraction, summarization and track-
ing in news and blog corpora. In AAAI spring
symposium: Computational approaches to analyz-
ing weblogs, volume 100107.

David D Lewis, Yiming Yang, Tony G Rose, and Fan
Li. 2004. Smart stopword list.

Bing Liu. 2012. Sentiment analysis and opinion min-
ing. Synthesis Lectures on Human Language Tech-
nologies, 5(1):1–167.

Julian McAuley and Jure Leskovec. 2013. Hidden fac-
tors and hidden topics: understanding rating dimen-
sions with review text. In Proceedings of the 7th
ACM conference on Recommender systems, pages
165–172. ACM.

Sinno Jialin Pan and Qiang Yang. 2010. A survey on
transfer learning. Knowledge and Data Engineer-
ing, IEEE Transactions on, 22(10):1345–1359.

Sinno Jialin Pan, Xiaochuan Ni, Jian-Tao Sun, Qiang
Yang, and Zheng Chen. 2010. Cross-domain sen-
timent classification via spectral feature alignment.
In Proceedings of the 19th international conference
on World wide web, pages 751–760. ACM.

Bo Pang and Lillian Lee. 2008. Opinion mining and
sentiment analysis. Foundations and trends in infor-
mation retrieval, 2(1-2):1–135.

Bo Pang, Lillian Lee, and Shivakumar Vaithyanathan.
2002. Thumbs up?: sentiment classification using
machine learning techniques. In Proceedings of the
ACL-02 conference on Empirical methods in natural
language processing-Volume 10, pages 79–86. As-
sociation for Computational Linguistics.

Hongning Wang, Yue Lu, and Chengxiang Zhai. 2010.
Latent aspect rating analysis on review text data:
a rating regression approach. In Proceedings of
the 16th ACM SIGKDD international conference on
Knowledge discovery and data mining, pages 783–
792. ACM.

Hongning Wang, Xiaodong He, Ming-Wei Chang,
Yang Song, Ryen W White, and Wei Chu. 2013.
Personalized ranking model adaptation for web
search. In Proceedings of the 36th international
ACM SIGIR conference on Research and develop-
ment in information retrieval, pages 323–332. ACM.

Peter Willett. 2006. The porter stemming algorithm:
then and now. Program, 40(3):219–223.

Yiming Yang and Jan O Pedersen. 1997. A compara-
tive study on feature selection in text categorization.
In ICML, volume 97, pages 412–420.

Dingqi Yang, Daqing Zhang, Zhiyong Yu, and Zhu
Wang. 2013. A sentiment-enhanced personalized
location recommendation system. In Proceedings of
the 24th ACM Conference on Hypertext and Social
Media, pages 119–128. ACM.

774


