



















































Reviews Meet Graphs: Enhancing User and Item Representations for Recommendation with Hierarchical Attentive Graph Neural Network


Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing
and the 9th International Joint Conference on Natural Language Processing, pages 4884–4893,
Hong Kong, China, November 3–7, 2019. c©2019 Association for Computational Linguistics

4884

Reviews Meet Graphs: Enhancing User and Item Representations for
Recommendation with Hierarchical Attentive Graph Neural Network

Chuhan Wu1, Fangzhao Wu2, Tao Qi1, Suyu Ge1, Yongfeng Huang1,and Xing Xie2
1Department of Electronic Engineering, Tsinghua University, Beijing 100084, China

2Microsoft Research Asia, Beijing 100080, China
{wu-ch19, qit16, gsy17, yfhuang}@mails.tsinghua.edu.cn,

{fangzwu, xing.xie}@microsoft.com

Abstract

User and item representation learning is crit-
ical for recommendation. Many of exist-
ing recommendation methods learn represen-
tations of users and items based on their rat-
ings and reviews. However, the user-user and
item-item relatedness are usually not consid-
ered in these methods, which may be insuffi-
cient. In this paper, we propose a neural rec-
ommendation approach which can utilize use-
ful information from both review content and
user-item graphs. Since reviews and graphs
have different characteristics, we propose to
use a multi-view learning framework to in-
corporate them as different views. In the re-
view content-view, we propose to use a hier-
archical model to first learn sentence repre-
sentations from words, then learn review rep-
resentations from sentences, and finally learn
user/item representations from reviews. In
addition, we propose to incorporate a three-
level attention network into this view to se-
lect important words, sentences and reviews
for learning informative user and item repre-
sentations. In the graph-view, we propose a
hierarchical graph neural network to jointly
model the user-item, user-user and item-item
relatedness by capturing the first- and second-
order interactions between users and items in
the user-item graph. In addition, we apply at-
tention mechanism to model the importance of
these interactions to learn informative user and
item representations. Extensive experiments
on four benchmark datasets validate the effec-
tiveness of our approach.

1 Introduction

Precisely learning user and item representations
is critical for recommendation (Tay et al., 2018).
Many existing recommendation methods learn
user and item representations from their rating ma-
trix (Koren et al., 2009; Mnih and Salakhutdinov,
2008; Berg et al., 2017). For example, Koren et

I feel this is a must read for any Star Wars fan of the 
prequels and original trilogy. The story is very well 
written and makes it hard to put this book down. 

I loved this book.

I believe this is the 17th Star Wars book I've 
read. I've read from the from the earliest 
days, before the old republic up until Rogue 
One and cover to cover, this is hands down 
the best book I've read thus far. 

User-1

User-2

Item-1

Item-2

Figure 1: Two example users and items.

al. (2009) proposed to use singular value decom-
position (SVD) to learn latent user and item rep-
resentations based on the review ratings that users
gave to items. However, there are massive users
and items in online platforms, making the rating
matrix between users and items very sparse. Thus,
it is difficult for these methods to learn accurate
user and item representations (Zheng et al., 2017).

In recent years, several deep learning based rec-
ommendation methods are proposed to learn user
and item representations from review texts (Zheng
et al., 2017; Catherine and Cohen, 2017). For ex-
ample, Zheng et al. (2017) proposed a DeepCoNN
approach to learn the representations of users and
items from their reviews via convolutional neu-
ral networks (CNN). However, these methods only
consider the interactions of user-item pairs, while
the relatedness between users or items are ignored,
which may be insufficient for learning accurate
user and item representations.

Our work is motivated by several observations.
First, users who write reviews on the same prod-
ucts may have some relatedness. For example, in
Fig. 1 both User-1 and User-2 give 5 stars to Item-
1, and we can infer that both users are interested in
Star Wars. Second, items that commented by the



4885

same user may also have relatedness. For exam-
ple, in Fig. 1 both items are commented by User-
1, and they share the same topics on Star Wars.
Third, modeling the user-user and item-item in-
teractions is useful for recommendation. For ex-
ample, in Fig. 1 both users may have similar in-
terests on Star Wars, and both books have related
topics. Thus, it may be effective to recommend
Item-2 to User-2. Fourth, the interactions between
users and items, e.g., reviews, may have different
importance for representing users and items. For
example, in Fig. 1 the interaction between User-
1 and Item-1 is more important than that between
User-2 and Item-1 in representing this item, since
the review of User-1 contain more details about
item properties. In addition, different words and
sentences in the same review may also have dif-
ferent importance. For instance, the sentence “this
is the best book I’ve read thus far” is more impor-
tant than “I’ve read from the earliest days”, and the
word “best” is more important than “read” in the
former sentence.

In this paper, we propose a reviews meet graphs
approach (RMG) to combine reviews and the in-
formation of user-item graphs via a multi-view
learning framework. In the review content-view,
we use a hierarchical model to learn user and item
representations. It first learns sentence represen-
tations from words, then learn review representa-
tions from sentences, and finally learn user/item
representations from reviews. In addition, we pro-
pose to apply a three-level attention network to
select important words, sentences and reviews to
learn informative user and item representations.
In the graph-view, we propose to use a graph
neural network to capture the user-item, user-
user, and item-item relatedness in the user-item
bipartite graphs by modeling the first-order and
second-order interactions between different users
and items. In addition, we propose to incorpo-
rate attention mechanism into the graph neural net-
work to model the importance of these interactions
for informative user and item representation learn-
ing. Extensive experiments on four benchmark
datasets validate that our approach can effectively
improve the performance of recommendation and
outperform many baseline methods.

2 Related Work

Many previous works have studied the prob-
lem of learning user and item representations

from reviews for recommendation (Zhang et al.,
2014; Diao et al., 2014; He et al., 2015; Tan
et al., 2016; Ren et al., 2017). Many of exist-
ing methods rely on topic models to extract top-
ics from reviews for user and item representa-
tion (McAuley and Leskovec, 2013; Ling et al.,
2014; Bao et al., 2014). For example, McAuley
and Leskovec (2013) proposed a Hidden Fac-
tors as Topics (HFT) method to learn latent user
and item factors from review content via a la-
tent Dirichlet allocation (LDA) model. Ling
et al. (2014) proposed a Ratings Meet Reviews
(RMR) approach to represent users and items by
first extracting topics from reviews and then align-
ing the dimensions of these topics with the latent
user factors learned from review ratings via matrix
factorization. Bao et al. (2014) proposed a Top-
icMF method to represent users and items by in-
corporate the topics extracted from review texts to
enhance the learning of latent user and item fac-
tors from the rating matrix via non-negative ma-
trix factorization (NMF). However, these methods
only extract topics from reviews, and cannot ef-
fectively utilize the contexts and word orders in
reviews, both of which are important for learning
accurate user and item representations.

In recent years, several deep learning based
methods proposed to learn user and item repre-
sentations from original review texts (Zhang et al.,
2016; Zheng et al., 2017; Catherine and Cohen,
2017; Seo et al., 2017b,a; Chen et al., 2018; Tay
et al., 2018; Wu et al., 2019). For example, Zheng
et al. (2017) proposed a DeepCoNN approach to
learn user and item representations from reviews
via CNN networks. Catherine and Cohen (2017)
proposed a TransNets approach to use CNNs to
learn user and item representations. In their ap-
proach, these representations are regularized to be
close to the representations of reviews from the
target user-item pairs. Seo et al. (2017b) pro-
posed to use CNN networks to learn user and
item representations, and applied a word-level at-
tention network to select important words. Chen
et al. (2018) proposed to learn review representa-
tions using CNNs and they modeled the usefulness
of reviews via a review-level attention network
to learn informative user and item representation
learning. However, these methods can only model
the user-item interactions, while the user-user and
item-item relatedness is not considered. In addi-
tion, these methods usually aggregate reviews or



4886

their texts together, and cannot fully model their
informativeness. Different from these methods, in
our approach we propose a review meet graph ap-
proach which can combine reviews with user-item
graphs via a multi-view learning framework to en-
hance user and item representation learning. In
the review text-view, we use a hierarchical frame-
work to learn sentence representations from words
first, then learn review representations form sen-
tences, and finally learns user/item representations
from their reviews. In addition, we apply attention
network at each level to select important words,
sentences and reviews. In the graph-view, our ap-
proach uses a hierarchical graph neural network to
mine the user-item, user-user and item-item relat-
edness from user-item graphs, and attentively se-
lects the interactions between users and items. Ex-
periments on four benchmark datasets validate the
effectiveness of our approach in recommendation.

3 Our Approach

In this section, we will introduce our reviews meet
graphs approach for recommendation (denoted as
RMG). The architecture of our basic RMG ap-
proach is shown in Fig. 2. Our approach consists
of two different views, i.e., a review content-view
and a graph-view.

3.1 Review Content View

The review content-view module is used to learn
representations of users and items from their re-
view texts. It contains three modules, i.e, sentence
encoder, review encoder and user/item encoder.

There are three layers in the sentence encoder.
The first one is word embedding. It is used to con-
vert a sentence s into a low-dimensional semantic
vector sequence. Denote the word sequence of s
as [w1, w2, ..., wM ], where M is its length. It is
converted into a vector sequence [e1, e2, ..., eM ]
via a pre-trained embedding matrix.

The second one is a convolutional neural net-
work (CNN). Local contexts are important for re-
view representation learning. For example, in the
sentence “I am a star wars fan”, the local contexts
of “wars” such as “star” and “fan” are useful for
inferring this sentence is about a famous movie
series. Thus, we employ a CNN over word em-
beddings to learn contextual word representations
by capturing local contexts. It takes the embed-
ding sequence [e1, e2, ..., eM ] as input, and out-
puts a sequence of word contextual representation

vectors [cw1 , c
w
2 , ..., c

w
M ].

The third layer is a word-level attention net-
work. Different words in the same sentence
may have different informativeness in represent-
ing users and items. For example, in the sentence
“This story book is very interesting” , the word
“interesting” is more important than the word
“story” in representing this book. Thus, we use
an attention network over word representations to
learn informative sentence representations by se-
lecting important words. The attention weight of
the i-th word wi is computed as follows:

awi = tanh(ww × cwi + bw), (1)

αwi =
exp(awi )∑M
j=1 exp(a

w
j )
, (2)

where ww and bw are parameters. The final repre-
sentation of the sentence s is the summation of the
contextual word representations weighted by their
attention weights, i.e., s =

∑M
i=1 α

w
i c

w
i .

The review encoder module is used to learn rep-
resentations of reviews from their sentence rep-
resentations. There are two layers in the review
encoder module. The first layer is a sentence-
level CNN network. Neighboring sentences usu-
ally have some relatedness in their content. For
example, in the book review “The book is very
well written. I will recommend it to others”, the
two neighboring sentences have close relatedness
and they both express positive opinion towards the
book. Thus, we use a sentence-level CNN net-
work to learn contextual sentence representations
by capturing sentence-level local contexts. We de-
note the representation sequence of the sentences
in a review r as [s1, s2, ..., sN ]. The CNN layer
takes the representation sequence as input, and
outputs a sequence of contextual sentence repre-
sentations, denoted as [cs1, c

s
2, ..., c

s
N ].

The second layer is a sentence-level attention
network. Different sentences in the same review
may have different informativeness for modeling
users and items. For example, the sentence “The
book tells a good story” is more informative than
the sentence “I read this book today” in represent-
ing this book. Thus, we use a sentence-level atten-
tion network to select important sentences to learn
informative representations of reviews for recom-
mendation. The attention weight αsi of the i-th
sentence is formulated as follows:

asi = tanh(ws × csi + bs), (3)



4887

CNN CNN

CNN

𝒖𝒖𝒓𝒓

�𝒚𝒚

𝒕𝒕𝒓𝒓

Dot

𝒆𝒆1 𝒆𝒆2 𝒆𝒆𝑀𝑀 𝒆𝒆1 𝒆𝒆2 𝒆𝒆𝑀𝑀

𝒄𝒄1𝑤𝑤 𝒄𝒄2
𝑤𝑤 𝒄𝒄𝑀𝑀𝑤𝑤

𝒔𝒔1 𝒔𝒔𝑁𝑁𝒔𝒔2

𝒄𝒄1𝑠𝑠 𝒄𝒄𝑁𝑁𝑠𝑠𝒄𝒄2𝑠𝑠

𝒓𝒓1 𝒓𝒓𝑲𝑲𝒓𝒓2

𝒄𝒄1𝑤𝑤 𝒄𝒄2
𝑤𝑤 𝒄𝒄𝑀𝑀𝑤𝑤

𝑤𝑤1 𝑤𝑤2 𝑤𝑤𝑀𝑀 𝑤𝑤1 𝑤𝑤2 𝑤𝑤𝑀𝑀

Word Embedding

Review
Encoder

Sentence
Encoder

𝑠𝑠1 𝑠𝑠𝑁𝑁

𝑟𝑟1 𝑟𝑟𝑃𝑃

𝒖𝒖

𝒓𝒓1 𝒓𝒓𝑷𝑷𝒓𝒓2

Review
Encoder

Review
Encoder

Review
Encoder

Review
Encoder

Review
Encoder

Review
Encoder

𝑟𝑟1 𝑟𝑟𝑲𝑲

𝒓𝒓

Word Embedding

𝑟𝑟𝟐𝟐 𝑟𝑟𝟐𝟐

First
Order

Second
Order

User
Reviews

Item
Reviews

𝒕𝒕1𝒉𝒉 𝒕𝒕𝑃𝑃𝒉𝒉𝒕𝒕2𝒉𝒉

𝒖𝒖11𝒅𝒅 𝒖𝒖12𝒅𝒅 𝒖𝒖𝟏𝟏𝑲𝑲𝒅𝒅

𝒖𝒖𝒉𝒉

𝒖𝒖𝒅𝒅

𝒖𝒖1𝒉𝒉 𝒖𝒖𝐾𝐾𝒉𝒉𝒖𝒖2𝒉𝒉

𝒕𝒕𝒉𝒉

𝒕𝒕𝒅𝒅

𝛂𝛂1𝑤𝑤 𝛂𝛂2𝑤𝑤 𝛂𝛂𝑀𝑀𝑤𝑤𝛂𝛂1𝑤𝑤 𝛂𝛂2𝑤𝑤 𝛂𝛂𝑀𝑀𝑤𝑤

𝛂𝛂1𝒔𝒔 𝛂𝛂𝑁𝑁
𝒔𝒔𝛂𝛂2𝒔𝒔

𝛂𝛂1𝒓𝒓 𝛂𝛂𝑷𝑷𝒓𝒓𝛂𝛂2
𝒓𝒓

Item User Review

𝒖𝒖𝑃𝑃1𝒅𝒅 𝒖𝒖𝑃𝑃2𝒅𝒅 𝒖𝒖𝑷𝑷𝑲𝑲𝒅𝒅 𝒕𝒕11
𝒅𝒅 𝒕𝒕12𝒅𝒅 𝒕𝒕𝟏𝟏𝑷𝑷𝒅𝒅 𝒕𝒕𝐾𝐾1

𝒅𝒅 𝒕𝒕𝐾𝐾2𝒅𝒅 𝒕𝒕𝑲𝑲𝑷𝑷𝒅𝒅

User/Item Embedding User/Item Embedding

𝒕𝒕1𝒅𝒅 𝒕𝒕𝑷𝑷𝒅𝒅𝒕𝒕2𝒅𝒅 𝒖𝒖1
𝒅𝒅 𝒖𝒖𝑲𝑲𝒅𝒅𝒖𝒖2𝒅𝒅

𝛂𝛂1𝒓𝒓 𝛂𝛂𝑲𝑲𝒓𝒓𝛂𝛂2
𝒓𝒓

𝒕𝒕

Zero
Order

𝛂𝛂1𝒅𝒅 𝛂𝛂𝑃𝑃
𝒅𝒅𝛂𝛂2𝒅𝒅 𝛂𝛂1

𝒅𝒅 𝛂𝛂𝐾𝐾𝒅𝒅𝛂𝛂2𝒅𝒅

𝛂𝛂11𝒅𝒅 𝛂𝛂12𝒅𝒅 𝛂𝛂1𝐾𝐾𝒅𝒅 𝛂𝛂𝑃𝑃1𝒅𝒅 𝛂𝛂𝑃𝑃2𝒅𝒅 𝛂𝛂𝑃𝑃𝐾𝐾
𝒅𝒅 𝛂𝛂11𝒅𝒅 𝛂𝛂12𝒅𝒅 𝛂𝛂1𝑃𝑃𝒅𝒅 𝛂𝛂𝐾𝐾1𝒅𝒅 𝛂𝛂𝐾𝐾2𝒅𝒅 𝛂𝛂𝐾𝐾𝑃𝑃

𝒅𝒅

Review Content-View Graph-View

User/Item
Encoder

𝑢𝑢 𝑡𝑡

𝑡𝑡1 𝑡𝑡2 𝑡𝑡P
𝑢𝑢11𝑢𝑢12 𝑢𝑢1𝐾𝐾𝑢𝑢21𝑢𝑢22 𝑢𝑢2𝐾𝐾𝑢𝑢𝑃𝑃1𝑢𝑢𝑃𝑃2 𝑢𝑢𝑃𝑃𝐾𝐾

𝑢𝑢1 𝑢𝑢2 𝑢𝑢𝐾𝐾
𝑡𝑡11 𝑡𝑡12 𝑡𝑡1𝑃𝑃 𝑡𝑡21 𝑡𝑡22 𝑡𝑡2𝑃𝑃𝑡𝑡𝐾𝐾1𝑡𝑡𝐾𝐾2 𝑡𝑡𝐾𝐾𝑃𝑃

Figure 2: The framework of our RMG approach.

αsi =
exp(asi )∑N
j=1 exp(a

s
j)
, (4)

where ws and bs are parameters in the attention
network. The final contextual representation of
the review r the summation of the contextual sen-
tence representations weighted by their attention
weights, i.e., r =

∑N
i=1 α

s
ic

s
i . We apply the re-

view encoder to encode the reviews of users and
items into their representation vectors.

The user/item encoder is used to learn represen-
tations of users and items from the their review
representations. Different reviews usually have
different informativeness in characterizing users
and items. For example, in Fig. 1, the first review
of Item-1 is more informative than its second re-
view. Thus, we use a review-level attention net-
work to recognize informative reviews and attend
to them. Denote the reviews written by a user as
[r1, r2, ..., rP ], where P is the number of reviews.
The attention weight of the review ri is formulated
as follows:

ari = tanh(wr × ri + br), (5)

αri =
exp(ari )∑P
j=1 exp(a

r
j)
, (6)

where wr and br are parameters. The user rep-
resentation learned from reviews is the summa-
tion of the review representations weighted by
their attention weights, i.e., ur =

∑P
i=1 α

r
i ri.

Denote the K reviews commented on an item as

[r1, r2, ..., rK ]. The item representation tr learned
from these reviews is computed similarly, i.e.,
tr =

∑K
i=1 α

r
i ri, where α

r
i is the attention weight

of the review ri.

3.2 User-Item Graph View

The graph-view module is used to learn repre-
sentations of users and items by modeling their
interactions in the user-item bipartite graph, as
shown in Fig. 2. In this bipartite graph, users
and items are represented as nodes, and the inter-
actions of user-item pairs are regarded as edges.
However, different from typical graph neural net-
works (Veličković et al., 2017), the input of typ-
ical review-based recommender systems is only a
user-item pair (two nodes with an edge) rather than
the entire user-item graph. Therefore, it is diffi-
cult to directly capture high-order interactions be-
tween users and items via a stacked graph neural
networks. To solve this problem, in our approach
we propose a hierarchical attentive graph neural
network for recommendation which can model
high-order user-item interactions. It contains three
core components, i.e., second-order encoder, first-
order encoder, and zero-order encoder. We will
introduce each one as follows.

The first one is a second-order encoder. Since
a user-item graph is bipartite, there are no edges
among users (or items). Thus, it is difficult to
directly model their relatedness. Luckily, users
bought the same items may have some relatedness,
as well as the items bought by the same users.
Thus, we propose to indirectly mine the user-



4888

user and item-item relatedness by modeling the
second-order interactions in the user-item graph,
as shown in Fig. 2. For each user-item pair (u, t)
in the training set, we denote the items that this
user gave ratings to as [t1, t2, ..., tP ]. Among these
items, we denote the users who commented the i-
th item as [ui1, ui2, ..., uiK ]. Motivated by typical
graph neural networks (Veličković et al., 2017),
we incorporate the embedding of user and item
IDs as the node representation in the user-item
graph, and then learn the representations of each
item from the representations of the users who has
commented this item. Usually different users who
bought the same item may have different informa-
tiveness in representing this item. For example, in
Fig. 1, the first user is more important than second
user in representing the first item. Thus, we pro-
pose to use an attentive graph neural network to
model the importance of the users that connected
to the item node. Denote the representations of
these users as [ui1,ui2, ...,uiK ]. The attention
weight adij of the j-th user who comments the item
ti is computed as:

adij = tanh(w2 × uij + b2), (7)

αdij =
exp(adij)∑K
k=1 exp(a

d
ik)
, (8)

where w2 and b2 are parameters in this network.
The user-based representation thi of the item ti is
the summation of the embeddings of its related
user weighted by their attention weights, which is
formulated as:

thi =
K∑
k=1

αdikuik, (9)

The second one is a first-order encoder. Since
some latent properties of this item may not be re-
vealed by the interactions with users, we incorpo-
rate the embedding of item IDs to enhance rep-
resentation learning. The final representation of
node ti is the concatenation of the item node rep-
resentation thi learned from user node represen-
tations and the item ID embedding tdi , i.e., ti =
[thi , t

d
i ]. The representations of the users (denoted

as [u1,u2, ...,uK ]) who interact with the item t
are computed similarly. Similar with the second-
order encoder, we first learn the item-based repre-
sentation uh of the user u from the representations
of its neighbor item nodes in an attentive manner.

Denote the attention weight of the i-th item that
the user u commented as adi , which is computed
as:

adi = tanh(w1 × ti + b1), (10)

αdi =
exp(adi )∑P
p=1 exp(a

d
p)
, (11)

where w1 and b1 are parameters. The item-based
representation uh of the user u is the summation of
the item representations associated with this user
weighted by their attention weights, which is for-
mulated as:

uh =
P∑

p=1

αdptp, (12)

The third one is zero-order encoder. It learns the
graph-based representation of user u by using the
concatenation of the user node representation uh

and the user ID embedding ud, i.e., ug = [uh,ud].
The representation of the item t is computed sim-
ilarly. Denote the representation of the item t
learned from user-item interactions as th, and the
embedding of its ID as td. Then its graph-based
representation tg is calculated as tg = [th, td].

3.3 Rating Scoring
In our approach, the final representations of users
and items are the concatenation of the represen-
tations learned from the review content-view and
graph-view, i.e., u = [ug,ur] and t = [tg, tr].
The rating score of a user-item pair is predicted by
the inner product of user and item representations,
i.e., ŷ = uT t.

3.4 Model Training
For model training, mean squared error is used as
the loss function:

L = 1
T

T∑
i=1

(ŷi − yi)2, (13)

where T denotes the number of user-item pairs
for training, ŷi and yi respectively denote the pre-
dicted and gold rating score of the i-th user-item
pair. By optimizing the loss function, all parame-
ters can be tuned via backward propagation.

4 Experiments

4.1 Datasets and Experimental Settings
We conducted experiments on four widely used
benchmark datasets in different domains and



4889

Dataset #users #items #reviews
Toys 19,412 11,924 167,597

Kindle 68,223 61,935 982,619
Movies 123,960 50,052 1,679,533

Yelp 199,445 119,441 3,072,129

Table 1: Statistics of the benchmark datasets.

scales to validate the effectiveness of our ap-
proach. Following (Chen et al., 2018), we used
three datasets from the Amazon collection1(He
and McAuley, 2016), i.e., Toys and Games, Kin-
dle Store, and Movies and TV (respectively de-
noted as Toys, Kindle and Movies). Another
dataset is from Yelp Challenge 20172 (denoted as
Yelp), which is a large collection of restaurant re-
views. Following (Chen et al., 2018), we only kept
the users and items which have at least 5 reviews.
The detailed statistics of the four datasets are sum-
marized in Table 1. The ratings in these datasets
are ranged in [1, 5].

In our experiments, the word embeddings were
300-dimensional. We used the pre-trained Google
embedding (Mikolov et al., 2013) for initializa-
tion. The CNNs had 150 filters and their window
sizes were set to 3. The user and item embedding
vector was 100-dimensional. We applied 25%
dropout (Srivastava et al., 2014) to each layer in
our model to mitigate overfitting. Adam (Kingma
and Ba, 2014) was used as the optimization algo-
rithm. The batch size was set to 20. We randomly
selected 80% of the user-item pairs in each dataset
for training, 10% for validation and 10% for test.
All hyperparameters were tuned according to the
validation set. We independently repeated each
experiment 5 times and reported the average Root
Mean Square Error (RMSE).

4.2 Performance Evaluation
We evaluate the performance of our approach by
comparing it with several baseline methods3. The
methods to be compared include:

• PMF: Probabilistic Matrix Factorization,
which models users and items based on
ratings via matrix factorization (Mnih and
Salakhutdinov, 2008).

1http://jmcauley.ucsd.edu/data/amazon
2https://www.yelp.com/dataset challenge
3Since in our framework the input is a single user-item

pair, other popular graph neural architectures such as GCN
may not be applicable. We will also explore incorporating
them in our future work.

• NMF: Non-negative Matrix Factorization for
recommendation based on rating scores (Lee
and Seung, 2001).

• SVD++: The recommendation method based
on rating matrix via SVD and similarities be-
tween items (Koren, 2008).

• HFT: Hidden Factor as Topic (HFT), a
method to combine reviews with ratings via
LDA (McAuley and Leskovec, 2013).

• DeepCoNN: Deep Cooperative Neural Net-
works, a neural method to jointly model users
and items from their reviews via CNN (Zheng
et al., 2017).

• Attn+CNN: Attention-based CNN, which
uses both CNN and attention over word em-
beddings to learn user and item representa-
tion from reviews (Seo et al., 2017b).

• NARRE: Neural Attentional Rating Regres-
sion with Review-level Explanations, which
uses attention mechanism to model the in-
formativeness of reviews for recommenda-
tion (Chen et al., 2018).

• RMG-review: A variant of our approach with
the review content-view only.

• RMG: Our reviews meet graphs approach.

In Table 2, we present a simple comparison of
different methods by summarizing the informa-
tion they incorporated. Traditional matrix factor-
ization methods such as PMF, NMF and SVD++
learn user and item representations based on re-
view ratings only, while other methods can exploit
both rating scores and reviews texts to learn rep-
resentations for recommendation. Among these
methods, HFT is based on topic models and can-
not effectively utilize the contexts and word or-
ders of words. DeepCoNN and Attn+CNN sim-
ply concatenate all reviews into a long document,
and cannot model the informativeness of different
reviews. Although NARRE can model review in-
formativeness using a review-level attention net-
work, it simply aggregates all sentences and can-
not model the informativeness of words and sen-
tences. RMG-review can simultaneously recog-
nize important word, sentences and reviews, but
does not consider the information in user-item
graphs. Different from these methods, our RMG



4890

Information PMF NMF SVD++ HFT DeepCoNN Attn+CNN NARRE RMG-review RMG
Rating score X X X X X X X X X
Review text X X X X X X

Context & word order X X X X X
Review attention X X X
Word attention X X X

Sentence attention X X
User-item graph X

Table 2: Comparisons of the information used in different methods.

Methods Toys Kindle Movies Yelp
PMF 1.3076 0.9914 1.2920 1.3340
NMF 1.0399 0.9023 1.1125 1.2916

SVD++ 0.8860 0.7928 1.0447 1.1735
HFT 0.8925 0.7917 1.0291 1.1699

DeepCoNN 0.8890 0.7876 1.0128 1.1642
Attn+CNN 0.8805 0.7796 0.9984 1.1588

NARRE 0.8769 0.7783 0.9965 1.1559
RMG-review 0.8685 0.7511 0.9679 1.1310

RMG 0.8438 0.7366 0.9514 1.1113

Table 3: The performance of different methods on the
benchmark datasets. The metric is RMSE.

Figure 3: The influence of the depth of the hierarchical
attentive graph neural network.

approach can learn user and item representations
from both reviews and graphs, which has the po-
tential to learn more informative user and item rep-
resentations.

The performance of different methods is sum-
marized in Table 3. The results lead to several ob-
servations. First, the methods which combine re-
views and ratings (e.g., HFT, DeepCoNN, NARRE
and RMG) usually outperform the methods based
on ratings only (PMF, NMF and SVD++). This
is probably because reviews usually contain rich
clues on user preferences and item properties,
which is useful for user and item representation
learning for recommendation.

Second, among the methods considering the
content information of reviews, neural network
based methods (e.g., DeepCoNN, Attn+CNN,
NARRE and RMG) usually outperform HFT,
which is based on topic models. This is prob-
ably because HFT rely on bag-of-words features
to extract topics from reviews, and cannot utilize
the contexts and orders of words. Different from
topic model based methods, neural methods can
better model user preferences and item properties
by utilizing the rich semantic information in re-
views, which is beneficial for recommendation.

Third, among the methods based on deep learn-
ing, RMG-review and RMG can consistently out-
perform other baseline methods. This is because
different words, sentences and reviews usually
have different informativeness for modeling users
and items. Different from Attn+CNN, NARRE and
DeepCoNN, both RMG-review and RMG use a hi-
erarchical model to learn user and item representa-
tions. In addition, our approach can select impor-
tant words, sentences and reviews via a three-level
attention network to help learn more accurate user
and item representations for recommendation.

Fourth, our RMG approach which incorpo-
rates information of user-item graph outperforms
other methods based on reviews only (e.g., RMG-
review), and further hypothesis test results show
that the advantage over RMG-review is signifi-
cant. This is because the information of user-item
graphs is useful for recommendation, and model-
ing the interactions between users amd items in
graphs can help learn more accurate user and item
representations. This result validates the effective-
ness of our approach.

4.3 Influence of the Depth of Graph Neural
Network

In this section, we conducted several experiments
to explore the influence of the depth of the graph
neural network on our approach. We compare
the performance of our approach with its variants



4891

(a) Attentions in the review content-view.

(b) Attentions in the graph-view.

Figure 4: The effectiveness of different attentions.

with the zero-order encoder only or with both the
zero- and first-order encoder. The results are il-
lustrated in Fig. 3. According to the results, we
find the performance of our approach can be con-
sistently improved as the depth increases. This is
because the first-order information of graph con-
tains the interactions between users and items, and
the second-order information can reveal the user-
user and item-item relatedness. Thus, more infor-
mation can be incorporated as the depth increases,
which can benefit user and item representation
learning. Although the performance may be fur-
ther improved when the depth gets deeper, in our
approach we only set the depth of the graph neural
network to 2. This is because the user-item, user-
user and item-item interactions are usually suffi-
cient for modeling users and items, and their high-
order relatedness is usually weak. In addition, the
computational cost is huge when higher-order in-
formation is considered. Thus, a moderate depth
of the graph neural network (i.e., 2) is the most
appropriate for our approach.

4.4 Effectiveness of Attention Mechanism

In this section, we conducted experiments to ex-
plore the effectiveness of the attention networks
in our approach. First, we want to validate the
effectiveness of the attention network in the re-
view content-view. We compare the performance
of several variants of our approach with one kind
of attention network removed to evaluate the con-
tribution of different attentions. The results are
shown in Fig. 4(a). From Fig. 4(a), the word-level
attention can effectively improve the performance
of our approach. This is because different words in
the same review have different importance in rep-
resenting this review. Therefore, selecting impor-
tant words via a word-level attention network can
learn more informative sentence representations.
In addition, the sentence-level attention is also
useful. This is because different sentences also
have different informativeness, and selecting im-
portant sentences using a sentence-level attention
network can benefit review representation learn-
ing. Besides, the review-level attention is also use-
ful in our approach. This is because different re-
views have different informativeness for learning
user and item representations, and distinguishing
informative reviews from the uninformative ones
can help learn more accurate user and item repre-
sentations. Moreover, combining all three kinds
of attentions can further improve our approach,
which validates the effectiveness of our hierarchi-
cal attention framework.

Then, we want to validate the effectiveness of
the attention network in the graph-view. We com-
pare our approach with its variants by removing
the attention networks in the first-order or second-
order encoder to validate the effectiveness of them.
The results are shown in Fig. 4(b). According to
Fig. 4(b), we find the attention networks in both
first- and second-order encoders are important in
our approach. This may be because the interac-
tions between users and items usually have differ-
ent importance in representing them, and distin-
guish informative interactions from uninformative
ones is useful for learning better user and item rep-
resentations. These results validate the effective-
ness of the attention networks in the graph-view.

4.5 Case Study

In this section, we conducted several case studies
to visually explore our RMG approach can learn
better user and item representations. For instance,



4892

(a) RMG-review (b) RMG

Figure 5: Visualization of the item representations learned by RMG-review and RMG via t-SNE. Each point repre-
sents an item. The red points represent two similar items.

we use t-SNE to visualize the representations of
several randomly selected items in the Toys dataset
learned by RMG-review and RMG. Among these
items there are two very similar items, both of
which are Lego toys of the planes in Star Wars.
The results are respectively shown in Fig. 5(a)
and 5(b)4. From the results, we find the distance
between the representations of these two items
learned by RMG-review is large, which usually in-
dicates their similarity is low. This may be because
the reviews of both items are insufficient (only 3
reviews appeared in the training set), making it
difficult for RMG-review to learn accurate repre-
sentations of them. Different from RMG-review,
our RMG approach can correctly recognize that
both items are very similar. This is probably be-
cause our approach can mine the user-user and
item-item relatedness by modeling the first-order
and second-order interactions between users and
items. Thus, our RMG approach can learn better
user and item representations.

5 Conclusion and Future Work

In this paper, we propose a reviews meet graphs
approach for recommendation which can exploit
information of user-item graphs. In our approach,
we use a multi-view framework to incorporate re-
view texts and user-item graphs as different views.
In the review content-view, we use a hierarchical
model to first learn sentence representations from
words, then learn review representations from sen-
tences, and finally learn user/item representations

4We aims to show the relative similarities between the
representations of the two items, and their absolute distances
are not comparable in the two figures.

from reviews. In addition, we apply a three-level
attention network to select important words, sen-
tences and reviews for informative representation
learning. In the graph-view, we propose to use
a graph neural network to jointly mine user-item,
user-user and item-item relatedness by modeling
the first- and second-order interactions between
users and items in graphs. In addition, we pro-
pose to apply attention mechanism to model the
importance of these interactions to learn more ac-
curate user and item representations. Experiments
on four benchmark datasets validate the effective-
ness of our approach.

In our future work, we will explore several po-
tential directions. First, since we only use the local
neighbors of nodes in the user-item graph, we will
explore the use of other kinds of graph neural net-
works to further enhance the learning of users and
items. Second, since there are also other kinds of
user and item information, we will explore how
to incorporate heterogeneous user and item infor-
mation to benefit recommendation. Third, we will
work on how to reduce the computational cost of
our approach, which can improve the applicability
of our approach.

Acknowledgments

This work was supported by the National Key Re-
search and Development Program of China un-
der Grant number 2018YFC1604002, the Na-
tional Natural Science Foundation of China under
Grant numbers U1836204, U1705261, U1636113,
U1536201, and U1536207.



4893

References
Yang Bao, Hui Fang, and Jie Zhang. 2014. Topicmf:

Simultaneously exploiting ratings and reviews for
recommendation. In AAAI, volume 14, pages 2–8.

Rianne van den Berg, Thomas N Kipf, and Max
Welling. 2017. Graph convolutional matrix comple-
tion. arXiv preprint arXiv:1706.02263.

Rose Catherine and William Cohen. 2017. Transnets:
Learning to transform for recommendation. In Rec-
Sys, pages 288–296. ACM.

Chong Chen, Min Zhang, Yiqun Liu, and Shaoping
Ma. 2018. Neural attentional rating regression with
review-level explanations. In WWW, pages 1583–
1592.

Qiming Diao, Minghui Qiu, Chao-Yuan Wu, Alexan-
der J Smola, Jing Jiang, and Chong Wang. 2014.
Jointly modeling aspects, ratings and sentiments for
movie recommendation (jmars). In KDD, pages
193–202.

Ruining He and Julian McAuley. 2016. Ups and
downs: Modeling the visual evolution of fashion
trends with one-class collaborative filtering. In
WWW, pages 507–517.

Xiangnan He, Tao Chen, Min-Yen Kan, and Xiao
Chen. 2015. Trirank: Review-aware explainable
recommendation by modeling aspects. In CIKM,
pages 1661–1670.

Diederik P Kingma and Jimmy Ba. 2014. Adam: A
method for stochastic optimization. arXiv preprint
arXiv:1412.6980.

Yehuda Koren. 2008. Factorization meets the neigh-
borhood: a multifaceted collaborative filtering
model. In KDD, pages 426–434.

Yehuda Koren, Robert Bell, and Chris Volinsky. 2009.
Matrix factorization techniques for recommender
systems. Computer, (8):30–37.

Daniel D Lee and H Sebastian Seung. 2001. Al-
gorithms for non-negative matrix factorization. In
NIPS, pages 556–562.

Guang Ling, Michael R Lyu, and Irwin King. 2014.
Ratings meet reviews, a combined approach to rec-
ommend. In RecSys, pages 105–112.

Julian McAuley and Jure Leskovec. 2013. Hidden fac-
tors and hidden topics: understanding rating dimen-
sions with review text. In RecSys, pages 165–172.

Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Cor-
rado, and Jeff Dean. 2013. Distributed representa-
tions of words and phrases and their compositional-
ity. In NIPS, pages 3111–3119.

Andriy Mnih and Ruslan R Salakhutdinov. 2008. Prob-
abilistic matrix factorization. In NIPS, pages 1257–
1264.

Zhaochun Ren, Shangsong Liang, Piji Li, Shuaiqiang
Wang, and Maarten de Rijke. 2017. Social collabo-
rative viewpoint regression with explainable recom-
mendations. In WSDM, pages 485–494.

Sungyong Seo, Jing Huang, Hao Yang, and Yan Liu.
2017a. Interpretable convolutional neural networks
with dual local and global attention for review rating
prediction. In RecSys, pages 297–305. ACM.

Sungyong Seo, Jing Huang, Hao Yang, and Yan Liu.
2017b. Representation learning of users and items
for review rating prediction using attention-based
convolutional neural network. In MLRec.

Nitish Srivastava, Geoffrey E Hinton, Alex Krizhevsky,
Ilya Sutskever, and Ruslan Salakhutdinov. 2014.
Dropout: a simple way to prevent neural networks
from overfitting. Journal of machine learning re-
search, 15(1):1929–1958.

Yunzhi Tan, Min Zhang, Yiqun Liu, and Shaoping Ma.
2016. Rating-boosted latent topics: Understanding
users and items with ratings and reviews. In IJCAI,
pages 2640–2646.

Yi Tay, Anh Tuan Luu, and Siu Cheung Hui. 2018.
Multi-pointer co-attention networks for recommen-
dation. In KDD, pages 2309–2318.

Petar Veličković, Guillem Cucurull, Arantxa Casanova,
Adriana Romero, Pietro Lio, and Yoshua Bengio.
2017. Graph attention networks. arXiv preprint
arXiv:1710.10903.

Chuhan Wu, Fangzhao Wu, Junxin Liu, and Yongfeng
Huang. 2019. Hierarchical user and item represen-
tation with three-tier attention for recommendation.
In NAACL, pages 1818–1826.

Wei Zhang, Quan Yuan, Jiawei Han, and Jianyong
Wang. 2016. Collaborative multi-level embedding
learning from reviews for rating prediction. In IJ-
CAI, pages 2986–2992.

Yongfeng Zhang, Guokun Lai, Min Zhang, Yi Zhang,
Yiqun Liu, and Shaoping Ma. 2014. Explicit fac-
tor models for explainable recommendation based
on phrase-level sentiment analysis. In SIGIR, pages
83–92.

Lei Zheng, Vahid Noroozi, and Philip S Yu. 2017. Joint
deep modeling of users and items using reviews for
recommendation. In WSDM, pages 425–434. ACM.


