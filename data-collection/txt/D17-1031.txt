



















































Word Embeddings based on Fixed-Size Ordinally Forgetting Encoding


Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 310–315
Copenhagen, Denmark, September 7–11, 2017. c©2017 Association for Computational Linguistics

Word Embeddings based on Fixed-Size Ordinally Forgetting Encoding

Joseph Sanu†, Mingbin Xu†, Hui Jiang† and Quan Liu‡
†Department of Electrical Engineering and Computer Science,

York University, 4700 Keele Street, Toronto, Ontario, M3J 1P3, Canada
‡Department of EEIS, University of Science and Technology of China, Hefei, China

cse83186@cse.yorku.ca, xmb@cse.yorku.ca, hj@cse.yorku.ca, quanliu@mail.ustc.edu.cn

Abstract

In this paper, we propose to learn word
embeddings based on the recent fixed-
size ordinally forgetting encoding (FOFE)
method, which can almost uniquely en-
code any variable-length sequence into a
fixed-size representation. We use FOFE
to fully encode the left and right con-
text of each word in a corpus to con-
struct a novel word-context matrix, which
is further weighted and factorized using
truncated SVD to generate low-dimension
word embedding vectors. We have eval-
uated this alternative method in encoding
word-context statistics and show the new
FOFE method has a notable effect on the
resulting word embeddings. Experimen-
tal results on several popular word similar-
ity tasks have demonstrated that the pro-
posed method outperforms many recently
popular neural prediction methods as well
as the conventional SVD models that use
canonical count based techniques to gen-
erate word context matrices.

1 Introduction

Low dimensional vectors as word representations
are very popular in NLP tasks such as inferring
semantic similarity and relatedness. Most of these
representations are based on either matrix factor-
ization or context sampling described by (Baroni
et al., 2014) as count or predict models. The ba-
sis for both models is the distributional hypoth-
esis (Harris, 1954), which states that words that
appear in similar contexts have similar meaning.
Traditional context representations have been ob-
tained by capturing co-occurrences of words from
a fixed-size window relative to the focus word.
This representation however does not encompass

the entirety of the context surrounding the focus
word. Therefore, the distributional hypothesis is
not being taken advantage of to the fullest extent.
In this work, we seek to capture these contexts
through the fixed-size ordinally forgetting encod-
ing (FOFE) method, recently proposed in (Zhang
et al., 2015b). In addition to just capturing word
co-occurrences, we attempt to use the FOFE to
encode the full contexts of each focus word, in-
cluding the order information of the context se-
quences. We believe the full encoding of con-
texts can enhance the resulting word embedding
vectors, derived by factoring the corresponding
word-context matrix. As argued in (Zhang et al.,
2015b), the FOFE method can almost uniquely en-
code discrete sequences of varying lengths into
a fixed-size code, and this encoding method was
used to address the challenges of a limited size
window when using deep neural networks for lan-
guage modeling. The resulting algorithm fulfills
the needs of keeping long term dependency while
being fast. The word order in a sequence is mod-
eled by FOFE using an ordinally-forgetting mech-
anism which encodes each position of every word
in the sequence.

In this paper, we elaborate how to use the FOFE
to fully encode context information of each focus
word in text corpora, and present a new method
to construct the word-context matrix for word em-
bedding, which may be weighted and factorized
as in traditional vector space models (Turney and
Pantel, 2010). Next, we report our experimental
results on several popular word similarity tasks,
which demonstrate that the proposed FOFE-based
approach leads to significantly better performance
in these tasks, comparing with the conventional
vector space models as well as the popular neu-
ral prediction methods, such as word2vec, GloVe
and more recent Swivel. Finally, this paper will
conclude with the analysis and prospects of com-

310



bining this approach with other methods.

2 Related Work

There has been some debate as to what the opti-
mal length of a text should be for measuring word
similarity. Word occurrences from a fixed con-
text window of words can be used to represent
a context (Lund and Burgess, 1996). The word
co-occurrence frequencies are based on fixed win-
dows spanning in both directions from the focus
word. This is then used to create a word-context
matrix from which row vectors can be used to
measure word similarity. A weighting step is usu-
ally applied to highlight words with close associa-
tion in the co-occurrence matrix, and the truncated
SVD is used to factorize the weighted matrix to
generate low-dimension word vectors. Recently,
(Mikolov et al., 2013a) has introduced an alterna-
tive way to generate word embeddings using the
skipgram model trained with stochastic gradient
descent and negative sampling, named as SGNS.
SGNS tries to maximize the dot product between
w · c where both a word w and a context c are
obtained from observed word-context pairs, and
meanwhile it also tries to minimize the dot product
between w · c′ where c′ is a negative sample repre-
senting some contexts that are not observed in the
corpus. More recently, (Levy and Goldberg, 2014)
has showed that the objective function of SGNS
is essentially seeking to minimize the difference
between the models estimate and the log of co-
occurrence count. Their finding has shown that
the optimal solution is a weighted factorization of
a pointwise mutual information matrix shifted by
the log of the number of negative samples.

SGNS and GloVe (Pennington et al., 2014) se-
lect a fixed window of usually 5 words or less
around a focus word to encode its context and the
word order information within the window is com-
pletely ignored. Other attempts to fully capture the
contexts have been successful with the use of re-
current neural networks (RNNs) but these methods
are much more expensive to run over large corpora
when comparing with the proposed FOFE method
in this paper. Some previous approaches to en-
code order information, such as such as BEAGLE
(Jones and Mewhort, 2007) and Random Permu-
tations (Sahlgren et al., 2008), typically require
the use of expensive operations such as convolu-
tion and permutation to process all n-grams within
a context window to memorize order information

for a given word. On the contrary, the FOFE meth-
ods only use a simple recursion to process a sen-
tence once to memorize both context and order in-
formation for all words in the sentence.

3 FOFE based Embedding

To capture the full essence of the distributional hy-
pothesis, we need to fully encode the left and right
context of each focus word in the text, and fur-
ther take into accounts that words closer to the fo-
cus word should play a bigger role in representing
the context relevant to the focus word than other
words locating much farther away. Traditional co-
occurrence word-context matrixes fail to address
these concerns of context representation.

In this work, we propose to make use of the
fixed-size ordinally-forgetting encoding (FOFE)
method, proposed in (Zhang et al., 2015b) as a
unique encoding method for any variable-length
sequence of discrete words.

Given a vocabulary of size K, FOFE uses 1-of-
K one-hot representation to represent each word.
To encode any variable-length sequence of words,
FOFE generates the code using a simple recursive
formula from the first word (w1) to the last one
(wT ) of the sequence: (assume z0 = 0)

zt = α · zt−1 + et (1 ≤ t ≤ T ) (1)
where zt denotes the FOFE code for the partial
sequence up to word wt, α is a constant forget-
ting factor, and et denotes the one-hot vector rep-
resentation of word wt. In this case, the code
zT may be viewed as a fixed-size representation
of any sequence of {w1, w2, · · · , wT }. For ex-
ample, assume we have three symbols in vocabu-
lary, e.g., A, B, C, whose 1-of-K codes are [1, 0, 0],
[0, 1, 0] and [0, 0, 1] respectively. When calculat-
ing from left to right, the FOFE code for the se-
quence {ABC} is [α2, α, 1], and that of {ABCBC}
is [α4, α+ α3, 1 + α2].

The uniqueness of the FOFE code is made evi-
dent if the original sequence can be unequivocally
recovered from the given FOFE code. According
to (Zhang et al., 2015b), FOFE codes have some
nice theoretical properties to ensure the unique-
ness, as exemplified by the following two theo-
rems 1:

Theorem 1 If the forgetting factor α satisfies 0 <
α ≤ 0.5, FOFE is unique for any K and T .

1See (Zhang et al., 2015a) for the proof of these two the-
orems.

311



Theorem 2 For 0.5 < α < 1, given any finite
values of K and T , FOFE is almost unique every-
where for α ∈ (0.5, 1.0), except only a finite set of
countable choices of α.

Finally, for alpha values less than or equal to
0.5 and greater than 0, the FOFE is unique for
any sequence. For alpha values greater than 0.5,
the chance of collision is extremely low and the
FOFE is unique in almost all cases. Too find more
about the theoretical correctness of FOFE, please
refer to (Zhang et al., 2015b). In other words, the
FOFE codes can almost uniquely encode any se-
quences, serving as a fixed-size but theoretically
lossless representation for any variable-length se-
quences.

In this work, we propose to use FOFE to encode
the full context where each focus word appears in
text. As shown in Figure 1, the left context of a fo-
cus word, i.e., bank, may be viewed as a sequence
and encoded as a FOFE code L from the left to
right while its right context is encoded as another
FOFE code R from right to left. When a proper
forgetter factor α is chosen, the two FOFE codes
can almost fully represent the context of the focus
word. If the focus word appears multiple times
in text, a pair of FOFE codes [L,R] is generated
for each occurrence. Next, a mean vector is calcu-
lated for each word from all of its occurrences in
text. Finally, as shown in Figure 1, we may line up
these mean vectors (one word per row) to form a
new word-context matrix, called the FOFE matrix
here.

4 PMI-based Weighting and SVD-based
Matrix Factorization

We further weight the above FOFE matrix using
the standard positive pointwise mutual informa-
tion (PMI) (Church and Hanks, 1990) which has
been shown to be of benefit for regular word-
context matrices (Pantel and Lin, 2002). PMI is
used as a measure of association between a word
and a context. PMI tries to compute the asso-
ciation probabilities based on co-occurrence fre-
quencies. Positive pointwise mutual information
is a commonly adopted approach where all neg-
ative values in the PMI matrix are replaced with
zero. The PMI-based weighting function is critical
here since it helps to highlight the more surprising
events in original word-context matrix.

There are significant benefits in working with
low-dimensional dense vectors, as noted by (Deer-

wester et al., 1990) with the use of truncated sin-
gular value decomposition (SVD). Here, we also
use truncated SVD to factorize the above weighted
FOFE matrix as the product of three dense matri-
ces U,Σ, V T , where U and V T have orthonormal
columns and Σ is a diagonal matrix consisting of
singular values. If we select Σ to be of rank d, its
diagonal values represent the top d singular val-
ues, and Ud can be used to represent all word em-
beddings with d dimensions where each row rep-
resents a word vector.

5 Experiments

We conducted experiments on several popular
word similarity data sets and compare our FOFE
method with other existing word embedding mod-
els in these tasks. In this work, we opt to use
five data sets: WordSim353 (Finkelstein et al.,
2001), MEN (Bruni et al., 2012), Mechanical Turk
(Radinsky et al., 2011), Rare Words (Luong et al.,
2013) and SimLex-999 (Hill et al., 2015). The
word similarity performance is evaluated based on
the Spearman rank correlation coefficient obtained
by comparing cosine distance between word vec-
tors and human assigned similarity scores.

For our training data, we use the standard en-
wiki9 corpus which contains 130 million words.
The pre-processing stage includes discarding ex-
tremely long sentences, tokenizing, lowercasing
and splitting each sentence as a context. Our vo-
cabulary size is chosen to be 80,000 for the most
frequent words in the corpus. All words not in the
vocabulary are replaced with the token <unk>. In
this work, we use a python-based library, called
scipy 2, to perform truncated SVD to factorize all
word-context matrices.

5.1 Experimental Setup

Our first baseline is the conventional vector space
model (VSM) (Turney and Pantel, 2010), relying
on the PMI-weighted co-occurrence matrix with
dimensionality reduction performed using trun-
cated SVD. The dimension of word vectors is cho-
sen to be 300 and this number is kept the same
for all models examined in this paper. Our main
goal is to outperform VSM as the model proposed
in this paper also uses SVD based matrix factor-
ization. This allows for appropriate comparisons
between the different word encoding methods.

2 See http://docs.scipy.org/doc/scipy/
reference/.

312



left FOFE code Lw1 right FOFE code Rw1

left FOFE code Lw2 right FOFE code Rw2

left FOFE code LwK right FOFE code RwK

w1
w2

wK
K x 2K

Back in the day, we had an entire bank of computers devoted to this problem. 

left FOFE code L right FOFE code R

i) encoding left and right context for one occurrence of the focus word, i.e. bank

ii) forming the FOFE word-context matrix for all words

Figure 1: i) encoding left and right contexts of each focus word with FOFE and ii) forming the FOFE
word-context matrix.

For the purpose of completeness, the other non-
SVD based embedding models, mainly the more
recent neural prediction methods, are also com-
pared in our experiments. As a result, we build the
second baseline using the skip-gram model pro-
vided by the word2vec software package (Mikolov
et al., 2013a), denoted as SGNS. The word em-
beddings are generated using the recommended
hyper-parameters from (Levy et al., 2015). Their
findings show a larger number of negative sam-
ples is preferable and increments on the window
size have minimal improvements on word similar-
ity tasks. In our experiments the number of nega-
tive samples is set to 5 and the window size is set
to 5. In addition, we set the subsampling rate to
10−4 and run 3 iterations for training. In adition to
SGNS, we also obtained results for CBOW, GloVe
(Pennington et al., 2014) and Swivel (Shazeer
et al., 2016) models using similar recommended
settings. While the window size has a fixed limit
in the baseline models, our model does not have
a window size parameter as the entire sentence

is fully captured as well as distinctions between
left and right contexts when generating the FOFE
codes. The impact of closer context words is fur-
ther highlighted by the use of the forgetting factor
which is unique to the FOFE based word embed-
ding.

Finally, we use the FOFE codes to construct the
word-context matrix and generate word embed-
ding as described in sections 3 and 4. Throughout
our experiments, we have chosen to use a constant
forgetting factor α = 0.7. There is no significant
difference in word similarity scores after experi-
menting with different α values between [0.6, 0.9]
when generating FOFE codes.

We have applied the same hyperparameters to
both VSM and FOFE methods and fine-tune them
based on the recommended settings provided in
(Levy et al., 2015). Although it has been previ-
ously reported that context distribution smoothing
(Mikolov et al., 2013b) can provide a net posi-
tive effect, it did not yield significant gains in our
experiments. On the other hand, the eigenvalue

313



Table 1: The best achieved performance of various word embedding models on all five examined word
similarity tasks.

Method WordSim353 MEN Mech Turk Rare Words SimLex-999
VSM+SVD 0.7109 0.7130 0.6258 0.4813 0.3866

CBOW 0.6763 0.6768 0.6621 0.4280 0.3549
GloVe 0.5873 0.6350 0.5831 0.3934 0.2883
SGNS 0.7028 0.6689 0.6187 0.4360 0.3709
Swivel 0.7303 0.7246 0.7024 0.4430 0.3323

FOFE+SVD 0.7580 0.7637 0.6525 0.5002 0.3866

weighting parameter tuning (Caron, 2001) proved
to be incredibly effective for some datasets but in-
effectual in others. The net benefit however is pal-
pable and we include it for both VSM and FOFE
methods.

5.2 Results and Discussion

The best results of all word embedding models are
summarized in Table 1 for all five examined data
sets, which include the the traditional count based
VSM with SVD alongside SGNS using word2vec
and our proposed FOFE word embeddings. The
most discernible piece of information from the ta-
ble is that the FOFE method significantly outper-
forms the traditional count based VSM method on
most of these word similarity tasks. The results
in Table 1 show that substantial gains are obtained
by FOFE in WordSim353, MEN and Rare Words
data sets. The MEN dataset shows a 7% relative
improvement over the conventional VSM.

Among all of these five data sets, the proposed
FOFE word embedding significantly outperforms
VSM in four tasks while yielding similar perfor-
mance as VSM in the last data set, i.e. SimLex-
999. FOFE also outperforms all the other models
except Swivel in the Mech Turk dataset. It is im-
portant to note that this paper does not state that
SVD is obligatory to obtain the best model. The
FOFE method can be complemented with other
models such as Swivel in place of count based en-
coding methods. It is also theoretically guaranteed
that the original sentence is perfectly recoverable
from this FOFE code. This theoretical guarantee
is clearly missing in previous methods to encode
word order information, such as both BEAGLE
and Random Permutations. It is evident that over-
all the FOFE encoding method does achieve sig-
nificant gains in performance in these word sim-
ilarity tests over the traditional VSM method that
applies the same factorization method. This is sub-

stantial as (Levy et al., 2015) demonstrates that
larger window sizes when using SVD does not
payoff and the optimal context window is 2. We
establish that we can indeed encode more infor-
mation into our embedding with the FOFE codes.

In summary, our experimental results show
great promise in using the FOFE encoding to rep-
resent word contexts for traditional matrix factor-
ization methods. As for future work, the FOFE en-
coding method may be combined with other pop-
ular algorithms, such as Swivel, to replace the co-
occurrence statistics based on a fixed window size.

6 Conclusion

The ability to capture the full context without
restriction can play a crucial factor in generat-
ing superior word embeddings that excel in NLP
tasks. The fixed-size ordinally forgetting encod-
ing (FOFE) has the ability to seize large contexts
while discriminating contexts that are farther away
as being less significant. Conventional embed-
dings are derived from ambiguous co-occurrence
statistics that fail to adequately discriminate con-
texts words even within the fixed-size window.
The FOFE encoding technique trumps other ap-
proaches in its ability to procure the state of the
art results in several word similarity tasks when
combined with prominent factorization practices.

Acknowledgments

This work is partially supported by a Discovery
Grant from Natural Sciences and Engineering Re-
search Council (NSERC) of Canada, and a re-
search donation from iFLYTEK Co., Hefei, China.

References
Marco Baroni, Georgiana Dinu, and Germán

Kruszewski. 2014. Don’t count, predict! a
systematic comparison of context-counting vs.

314



context-predicting semantic vectors. In ACL (1),
pages 238–247.

Elia Bruni, Gemma Boleda, Marco Baroni, and Nam-
Khanh Tran. 2012. Distributional semantics in tech-
nicolor. In Proceedings of the 50th Annual Meet-
ing of the Association for Computational Linguis-
tics, pages 136–145. Association for Computational
Linguistics.

John Caron. 2001. Experiments with lsa scoring:
Optimal rank and basis. In Proceedings of the
SIAM Computational Information Retrieval Work-
shop, pages 157–169.

Kenneth W. Church and Patrick Hanks. 1990. Word
association norms, mutual information, and lexicog-
raphy. Computational linguistics, 16(1):22–29.

Scott Deerwester, Susan T Dumais, George W Fur-
nas, Thomas K Landauer, and Richard Harshman.
1990. Indexing by latent semantic analysis. Jour-
nal of the American society for information science,
41(6):391.

Lev Finkelstein, Evgeniy Gabrilovich, Yossi Matias,
Ehud Rivlin, Zach Solan, Gadi Wolfman, and Ey-
tan Ruppin. 2001. Placing search in context: The
concept revisited. In Proceedings of the 10th inter-
national conference on World Wide Web, pages 406–
414. ACM.

Zellig S Harris. 1954. Distributional structure. Word,
10(2-3):146–162.

Felix Hill, Roi Reichart, and Anna Korhonen. 2015.
Simlex-999: Evaluating semantic models with (gen-
uine) similarity estimation. Computational Linguis-
tics.

M.N Jones and D.J.K Mewhort. 2007. Represent-
ing word meaning and order information in a com-
posite holographic lexicon. Psychological Review,
114(1):1–37.

Omer Levy and Yoav Goldberg. 2014. Neural word
embedding as implicit matrix factorization. In Ad-
vances in Neural Information Processing Systems,
pages 2177–2185.

Omer Levy, Yoav Goldberg, and Ido Dagan. 2015. Im-
proving distributional similarity with lessons learned
from word embeddings. Transactions of the Associ-
ation for Computational Linguistics, 3:211–225.

Kevin Lund and Curt Burgess. 1996. Producing
high-dimensional semantic spaces from lexical co-
occurrence. Behavior Research Methods, Instru-
ments, and Computers, 28(2):203–208.

Thang Luong, Richard Socher, and Christopher D
Manning. 2013. Better word representations with
recursive neural networks for morphology. In
CoNLL, pages 104–113. Citeseer.

Tomas Mikolov, Kai Chen, Greg Corrado, and Jef-
frey Dean. 2013a. Efficient estimation of word
representations in vector space. arXiv preprint
arXiv:1301.3781.

Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Cor-
rado, and Jeff Dean. 2013b. Distributed representa-
tions of words and phrases and their compositional-
ity. In Advances in neural information processing
systems, pages 3111–3119.

Patrick Pantel and Dekang Lin. 2002. Discovering
word senses from text. In Proceedings of the eighth
ACM SIGKDD international conference on Knowl-
edge discovery and data mining, pages 613–619.

Jeffrey Pennington, Richard Socher, and Christopher
Manning. 2014. Glove: Global vectors for word
representation. In Proceedings of the 2014 Con-
ference on Empirical Methods in Natural Language
Processing (EMNLP), pages 1532–1543, Doha,
Qatar. Association for Computational Linguistics.

Kira Radinsky, Eugene Agichtein, Evgeniy
Gabrilovich, and Shaul Markovitch. 2011. A
word at a time: computing word relatedness using
temporal semantic analysis. In Proceedings of the
20th international conference on World wide web,
pages 337–346. ACM.

Magnus Sahlgren, Anders Holst, and Kanerva Pentti.
2008. Permutations as a means to encode order in
word space. In In Proceedings of the 30th Annual
Conference of the Cognitive Science Society, pages
1300–1305.

Noam Shazeer, Ryan Doherty, Colin Evans, and
Chris Waterson. 2016. Swivel: Improving embed-
dings by noticing whats missing. arXiv preprint
arXiv:1602.02215.

Peter D. Turney and Patrick Pantel. 2010. From fre-
quency to meaning: Vector space models of se-
mantics. Journal of Artificial Intelligence Research,
37:141–188.

Shiliang Zhang, Hui Jiang, Mingbin Xu, Junfeng Hou,
and Lirong Dai. 2015a. A fixed-size encoding
method for variable-length sequences with its ap-
plication to neural network language models. arXiv
preprint arXiv:1505.01504.

Shiliang Zhang, Hui Jiang, Mingbin Xu, Junfeng Hou,
and Lirong Dai. 2015b. The fixed-size ordinally-
forgetting encoding method for neural network lan-
guage models. In Proceedings of the 53rd Annual
Meeting of the Association for Computational Lin-
guistics and the 7th International Joint Conference
on Natural Language Processing, pages 495–500,
Beijing, China. Association for Computational Lin-
guistics.

315


