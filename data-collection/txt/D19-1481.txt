



















































Trouble on the Horizon: Forecasting the Derailment of Online Conversations as they Develop


Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing
and the 9th International Joint Conference on Natural Language Processing, pages 4743–4754,
Hong Kong, China, November 3–7, 2019. c©2019 Association for Computational Linguistics

4743

Trouble on the Horizon:
Forecasting the Derailment of Online Conversations as they Develop

Jonathan P. Chang
Cornell University

jpc362@cornell.edu

Cristian Danescu-Niculescu-Mizil
Cornell University

cristian@cs.cornell.edu

Abstract

Online discussions often derail into toxic ex-
changes between participants. Recent efforts
mostly focused on detecting antisocial behav-
ior after the fact, by analyzing single com-
ments in isolation. To provide more timely
notice to human moderators, a system needs
to preemptively detect that a conversation is
heading towards derailment before it actually
turns toxic. This means modeling derailment
as an emerging property of a conversation
rather than as an isolated utterance-level event.

Forecasting emerging conversational proper-
ties, however, poses several inherent modeling
challenges. First, since conversations are dy-
namic, a forecasting model needs to capture
the flow of the discussion, rather than proper-
ties of individual comments. Second, real con-
versations have an unknown horizon: they can
end or derail at any time; thus a practical fore-
casting model needs to assess the risk in an on-
line fashion, as the conversation develops. In
this work we introduce a conversational fore-
casting model that learns an unsupervised rep-
resentation of conversational dynamics and ex-
ploits it to predict future derailment as the con-
versation develops. By applying this model to
two new diverse datasets of online conversa-
tions with labels for antisocial events, we show
that it outperforms state-of-the-art systems at
forecasting derailment.

1 Introduction
“Ché saetta previsa vien più lenta.”1

– Dante Alighieri, Divina Commedia, Paradiso

Antisocial behavior is a persistent problem
plaguing online conversation platforms; it is both
widespread (Duggan, 2014) and potentially dam-
aging to mental and emotional health (Raskauskas
and Stoltz, 2007; Akbulut et al., 2010). The strain
this phenomenon puts on community maintainers

1“The arrow one foresees arrives more gently.”

(1) [User A] What does [quote omitted] refer to? I as-
sume it should be written from June 2010 to December
2011 and we should precise [sic] the months.
(2) [User B] No. It refers to 2007-2011 Belgian political
crisis
(3) [User A] 2007-2011 Belgian political crisis is a lit-
tle bit [of original research]. It merges 2 crisis in 1.
Sources 2 and 4 in the article talk about a 18 months cri-
sis in 2010-2011, ie what I refer to. What are the reliable
sources that make this crisis go back to 2007?
(4) [User B] Yes it’s not ridiculous at all to claim [it’s
original research] because it doesn’t fit your argument.
A crisis can be composed out of several smaller crisis.
It’s not original research if some of your sources only
talk about parts [...]
(5) [User A] Where is the source that claim the crisis is
4 year long? Sources state claim it is 18 month long and
refer to the period from June 2010 to December 2011.
(6) [User B] There were 4 governments and 2 years of
no government in 4 years time. You can not sanely claim
that this Must be viewed as two seperate crisis. What
exactly splits them up? [...]

Figure 1: Example start of a conversation that will
eventually derail into a personal attack.

has sparked recent interest in computational ap-
proaches for assisting human moderators.

Prior work in this direction has largely focused
on post-hoc identification of various kinds of an-
tisocial behavior, including hate speech (Warner
and Hirschberg, 2012; Davidson et al., 2017), ha-
rassment (Yin et al., 2009), personal attacks (Wul-
czyn et al., 2017), and general toxicity (Pavlopou-
los et al., 2017). The fact that these approaches
only identify antisocial content after the fact limits
their practicality as tools for assisting pre-emptive
moderation in conversational domains.

Addressing this limitation requires forecasting
the future derailment of a conversation based on
early warning signs, giving the moderators time
to potentially intervene before any harm is done
(Liu et al. 2018, Zhang et al. 2018a, see Jurgens
et al. 2019 for a discussion). Such a goal rec-
ognizes derailment as emerging from the devel-



4744

opment of the conversation, and belongs to the
broader area of conversational forecasting, which
includes future-prediction tasks such as predicting
the eventual length of a conversation (Backstrom
et al., 2013), whether a persuasion attempt will
eventually succeed (Tan et al., 2016; Wachsmuth
et al., 2018; Yang et al., 2019), whether team dis-
cussions will eventually lead to an increase in per-
formance (Niculae and Danescu-Niculescu-Mizil,
2016), or whether ongoing counseling conversa-
tions will eventually be perceived as helpful (Al-
thoff et al., 2016).2

Approaching such conversational forecasting
problems, however, requires overcoming several
inherent modeling challenges. First, conversa-
tions are dynamic and their outcome might depend
on how subsequent comments interact with each
other. Consider the example in Figure 1: while
no individual comment is outright offensive, a hu-
man reader can sense a tension emerging from
their succession (e.g., dismissive answers to re-
peated questioning). Thus a forecasting model
needs to capture not only the content of each in-
dividual comment, but also the relations between
comments. Previous work has largely relied on
hand-crafted features to capture such relations—
e.g., similarity between comments (Althoff et al.,
2016; Tan et al., 2016) or conversation structure
(Zhang et al., 2018b; Hessel and Lee, 2019)—,
though neural attention architectures have also re-
cently shown promise (Jo et al., 2018).

The second modeling challenge stems from the
fact that conversations have an unknown horizon:
they can be of varying lengths, and the to-be-
forecasted event can occur at any time. So when
is it a good time to make a forecast? Prior work
has largely proposed two solutions, both resulting
in important practical limitations. One solution is
to assume (unrealistic) prior knowledge of when
the to-be-forecasted event takes place and extract
features up to that point (Niculae et al., 2015; Liu
et al., 2018). Another compromising solution is to
extract features from a fixed-length window, of-
ten at the start of the conversation (Curhan and
Pentland, 2007; Niculae and Danescu-Niculescu-
Mizil, 2016; Althoff et al., 2016; Zhang et al.,

2We can distinguish two types of forecasting tasks, de-
pending on whether the to-be-forecasted target is an event
that might take place within the conversation (e.g., derail-
ment) or an outcome measured after the conversation will
eventually conclude (e.g., helpfulness). The following dis-
cussion of modeling challenges holds for both.

2018a, inter alia). Choosing a catch-all window-
size is however impractical: short windows will
miss information in comments they do not encom-
pass (e.g., a window of only two comments would
miss the chain of repeated questioning in com-
ments 3 through 6 of Figure 1), while longer win-
dows risk missing the to-be-forecasted event alto-
gether if it occurs before the end of the window,
which would prevent early detection.

In this work we introduce a model for fore-
casting conversational events that overcomes both
these inherent challenges by processing com-
ments, and their relations, as they happen (i.e., in
an online fashion). Our main insight is that models
with these properties already exist, albeit geared
toward generation rather than prediction: recent
work in context-aware dialog generation (or “chat-
bots”) has proposed sequential neural models that
make effective use of the intra-conversational dy-
namics (Sordoni et al., 2015b; Serban et al., 2016,
2017), while concomitantly being able to process
the conversation as it develops (see Gao et al.
(2018) for a survey).

In order for these systems to perform well in the
generative domain they need to be trained on mas-
sive amounts of (unlabeled) conversational data.
The main difficulty in directly adapting these mod-
els to the supervised domain of conversational
forecasting is the relative scarcity of labeled data:
for most forecasting tasks, at most a few thousands
labeled examples are available, insufficient for the
notoriously data-hungry sequential neural models.

To overcome this difficulty, we propose to de-
couple the objective of learning a neural repre-
sentation of conversational dynamics from the ob-
jective of predicting future events. The former
can be pre-trained on large amounts of unsuper-
vised data, similarly to how chatbots are trained.
The latter can piggy-back on the resulting repre-
sentation after fine-tuning it for classification us-
ing relatively small labeled data. While similar
pre-train-then-fine-tune approaches have recently
achieved state-of-the-art performance in a number
of NLP tasks—including natural language infer-
ence, question answering, and commonsense rea-
soning (discussed in Section 2)—to the best of our
knowledge this is the first attempt at applying this
paradigm to conversational forecasting.

To test the effectiveness of this new architecture
in forecasting derailment of online conversations,
we develop and distribute two new datasets. The



4745

first triples in size the highly curated ‘Conversa-
tions Gone Awry’ dataset (Zhang et al., 2018a),
where civil-starting Wikipedia Talk Page conver-
sations are crowd-labeled according to whether
they eventually lead to personal attacks; the sec-
ond relies on in-the-wild moderation of the pop-
ular subreddit ChangeMyView, where the aim is
to forecast whether a discussion will later be sub-
ject to moderator action due to “rude or hostile”
behavior. In both datasets, our model outperforms
existing fixed-window approaches, as well as sim-
pler sequential baselines that cannot account for
inter-comment relations. Furthermore, by virtue
of its online processing of the conversation, our
system can provide substantial prior notice of up-
coming derailment, triggering on average 3 com-
ments (or 3 hours) before an overtly toxic com-
ment is posted.

To summarize, in this work we:

• introduce the first model for forecasting con-
versational events that can capture the dy-
namics of a conversation as it develops;

• build two diverse datasets (one entirely new,
one extending prior work) for the task of fore-
casting derailment of online conversations;

• compare the performance of our model
against the current state-of-the-art, and evalu-
ate its ability to provide early warning signs.

Our work is motivated by the goal of assist-
ing human moderators of online communities by
preemptively signaling at-risk conversations that
might deserve their attention. However, we cau-
tion that any automated systems might encode or
even amplify the biases existing in the training
data (Park et al., 2018; Sap et al., 2019; Wiegand
et al., 2019), so a public-facing implementation
would need to be exhaustively scrutinized for such
biases (Feldman et al., 2015).

2 Further Related Work

Antisocial behavior. Antisocial behavior online
comes in many forms, including harassment (Vi-
tak et al., 2017), cyberbullying (Singh et al., 2017),
and general aggression (Kayany, 1998). Prior
work has sought to understand different aspects of
such behavior, including its effect on the commu-
nities where it happens (Collier and Bear, 2012;
Arazy et al., 2013), the actors involved (Cheng

et al., 2017; Volkova and Bell, 2017; Kumar et al.,
2018; Ribeiro et al., 2018) and connections to the
outside world (Olteanu et al., 2018).

Post-hoc classification of conversations. There
is a rich body of prior work on classifying the out-
come of a conversation after it has concluded, or
classifying conversational events after they hap-
pened. Many examples exist, but some more
closely related to our present work include iden-
tifying the winner of a debate (Zhang et al., 2016;
Potash and Rumshisky, 2017; Wang et al., 2017),
identifying successful negotiations (Curhan and
Pentland, 2007; Cadilhac et al., 2013), as well as
detecting whether deception (Girlea et al., 2016;
Pérez-Rosas et al., 2016; Levitan et al., 2018) or
disagreement (Galley et al., 2004; Abbott et al.,
2011; Allen et al., 2014; Wang and Cardie, 2014;
Rosenthal and McKeown, 2015) has occurred.

Our goal is different because we wish to fore-
cast conversational events before they happen and
while the conversation is still ongoing (potentially
allowing for interventions). Note that some post-
hoc tasks can also be re-framed as forecasting
tasks (assuming the existence of necessary labels);
for instance, predicting whether an ongoing con-
versation will eventually spark disagreement (Hes-
sel and Lee, 2019), rather than detecting already-
existing disagreement.

Conversational forecasting. As described in Sec-
tion 1, prior work on forecasting conversational
outcomes and events has largely relied on hand-
crafted features to capture aspects of conversa-
tional dynamics. Example feature sets include sta-
tistical measures based on similarity between ut-
terances (Althoff et al., 2016), sentiment imbal-
ance (Niculae et al., 2015), flow of ideas (Nicu-
lae et al., 2015), increase in hostility (Liu et al.,
2018), reply rate (Backstrom et al., 2013) and
graph representations of conversations (Garimella
et al., 2017; Zhang et al., 2018b). By contrast, we
aim to automatically learn neural representations
of conversational dynamics through pre-training.

Such hand-crafted features are typically ex-
tracted from fixed-length windows of the conver-
sation, leaving unaddressed the problem of un-
known horizon. While some work has trained
multiple models for different window-lengths (Liu
et al., 2018; Hessel and Lee, 2019), they consider
these models to be independent and, as such, do
not address the issue of aggregating them into a
single forecast (i.e., deciding at what point to make



4746

a prediction). We implement a simple sliding win-
dows solution as a baseline (Section 5).
Pre-training for NLP. The use of pre-training for
natural language tasks has been growing in pop-
ularity after recent breakthroughs demonstrating
improved performance on a wide array of bench-
mark tasks (Peters et al., 2018; Radford et al.,
2018). Existing work has generally used a lan-
guage modeling objective as the pre-training ob-
jective; examples include next-word prediction
(Howard and Ruder, 2018), sentence autoencod-
ing, (Dai and Le, 2015), and machine transla-
tion (McCann et al., 2017). BERT (Devlin et al.,
2019) introduces a variation on this in which the
goal is to predict the next sentence in a document
given the current sentence. Our pre-training ob-
jective is similar in spirit, but operates at a con-
versation level, rather than a document level. We
hence view our objective as conversational model-
ing rather than (only) language modeling. Further-
more, while BERT’s sentence prediction objective
is framed as a multiple-choice task, our objective
is framed as a generative task.

3 Derailment Datasets

We consider two datasets, representing related
but slightly different forecasting tasks. The first
dataset is an expanded version of the annotated
Wikipedia conversations dataset from Zhang et al.
(2018a). This dataset uses carefully-controlled
crowdsourced labels, strictly filtered to ensure the
conversations are civil up to the moment of a per-
sonal attack. This is a useful property for the pur-
poses of model analysis, and hence we focus on
this as our primary dataset. However, we are con-
scious of the possibility that these strict labels may
not fully capture the kind of behavior that modera-
tors care about in practice. We therefore introduce
a secondary dataset, constructed from the subred-
dit ChangeMyView (CMV) that does not use post-
hoc annotations. Instead, the prediction task is to
forecast whether the conversation will be subject
to moderator action in the future.
Wikipedia data. Zhang et al.’s ‘Conversations
Gone Awry’ dataset consists of 1,270 conversa-
tions that took place between Wikipedia editors on
publicly accessible talk pages. The conversations
are sourced from the WikiConv dataset (Hua et al.,
2018) and labeled by crowdworkers as either con-
taining a personal attack from within (i.e., hostile

behavior by one user in the conversation directed
towards another) or remaining civil throughout.

A series of controls are implemented to prevent
models from picking up on trivial correlations. To
prevent models from capturing topic-specific in-
formation (e.g., political conversations are more
likely to derail), each attack-containing conversa-
tion is paired with a clean conversation from the
same talk page, where the talk page serves as a
proxy for topic.3 To force models to actually cap-
ture conversational dynamics rather than detecting
already-existing toxicity, human annotations are
used to ensure that all comments preceding a per-
sonal attack are civil.

To the ends of more effective model training,
we elected to expand the ‘Conversations Gone
Awry’ dataset, using the original annotation pro-
cedure. Since we found that the original data
skewed towards shorter conversations, we focused
this crowdsourcing run on longer conversations:
ones with 4 or more comments preceding the at-
tack.4 Through this additional crowdsourcing, we
expand the dataset to 4,188 conversations, which
we are publicly releasing as part of the Cornell
Conversational Analysis Toolkit (ConvoKit).5

We perform an 80-20-20 train/dev/test split, en-
suring that paired conversations end up in the same
split in order to preserve the topic control. Finally,
we randomly sample another 1 million conversa-
tions from WikiConv to use for the unsupervised
pre-training of the generative component.
Reddit CMV data. The CMV dataset is con-
structed from conversations collected via the
Reddit API. In contrast to the Wikipedia-based
dataset, we explicitly avoid the use of post-hoc an-
notation. Instead, we use as our label whether a
conversation eventually had a comment removed
by a moderator for violation of Rule 2: “Don’t be
rude or hostile to other users”.6

Though the lack of post-hoc annotation limits
the degree to which we can impose controls on the
data (e.g., some conversations may contain toxic
comments not flagged by the moderators) we do
reproduce as many of the Wikipedia data’s con-
trols as we can. Namely, we replicate the topic

3Paired conversations were also enforced to be similar in
length, so that length distribution is the same between classes.

4We cap the length at 10 to avoid overwhelming the
crowdworkers.

5convokit.cornell.edu
6The existence of this specific rule, the standardized mod-

eration messages and the civil character of the Change-
MyView subreddit was our initial motivation for choosing it.

http://convokit.cornell.edu


4747

Let’s fix it I agree

Utt. Encoder

I don’t

Utt. EncoderUtt. Encoder

Context Encoder

Predictor MLP

Please explain

Decoder

pevent

Generative (pre-training) 
objective Prediction 

objective

Comment 2 Comment 3Comment 1

Figure 2: Sketch of the CRAFT architecture.

control pairing by choosing pairs of positive and
negative examples that belong to the same top-
level post, following Tan et al. (2016);7 and en-
force that the removed comment was made by a
user who was previously involved in the conversa-
tion.8 This process results in 6,842 conversations,
to which we again apply a pair-preserving 80-20-
20 split. Finally, we gather over 600,000 conver-
sations that do not include any removed comment,
for unsupervised pre-training.

4 Conversational Forecasting Model

We now describe our general model for forecast-
ing future conversational events. Our model in-
tegrates two components: (a) a generative dialog
model that learns to represent conversational dy-
namics in an unsupervised fashion; and (b) a su-
pervised component that fine-tunes this represen-
tation to forecast future events. Figure 2 provides
an overview of the proposed architecture, hence-
forth CRAFT (Conversational Recurrent Architec-
ture for ForecasTing).
Terminology. For modeling purposes, we treat a
conversation as a sequence of N comments C =
{c1, . . . , cN}. Each comment, in turn, is a se-
quence of tokens, where the number of tokens
may vary from comment to comment. For the
n-th comment (1 ≤ n ≤ N), we let Mn de-
note the number of tokens. Then, a comment cn
can be represented as a sequence of Mn tokens:
cn = {w1, . . . , wMn}.

7The top-level post is not part of the conversations.
8We also impose the same length restriction on the num-

ber of comments preceding the removed comment, for com-
parability and for computational considerations.

Generative component. For the generative com-
ponent of our model, we use a hierarchical recur-
rent encoder-decoder (HRED) architecture (Sor-
doni et al., 2015a), a modified version of the pop-
ular sequence-to-sequence (seq2seq) architecture
(Sutskever et al., 2014) designed to account for
dependencies between consecutive inputs. Ser-
ban et al. (2016) showed that HRED can suc-
cessfully model conversational context by encod-
ing the temporal structure of previously seen com-
ments, making it an ideal fit for our use case. Here,
we provide a high-level summary of the HRED
architecture, deferring deeper technical discussion
to Sordoni et al. (2015a) and Serban et al. (2016).

An HRED dialog model consists of three com-
ponents: an utterance encoder, a context encoder,
and a decoder. The utterance encoder is respon-
sible for generating semantic vector representa-
tions of comments. It consists of a recurrent neu-
ral network (RNN) that reads a comment token-
by-token, and on each token wm updates a hidden
state henc based on the current token and the pre-
vious hidden state:

hencm = f
RNN(hencm−1, wm) (1)

where fRNN is a nonlinear gating function (our im-
plementation uses GRU (Cho et al., 2014)). The
final hidden state hencM can be viewed as a vector
encoding of the entire comment.

Running the encoder on each comment cn re-
sults in a sequence of N vector encodings. A sec-
ond encoder, the context encoder, is then run over
this sequence:

hconn = f
RNN(hconn−1, h

enc
Mn) (2)

Each hidden state hconn can then be viewed as an
encoding of the full conversational context up to
and including the n-th comment. To generate a re-
sponse to comment n, the context encoding hconn is
used to initialize the hidden state hdec0 of a decoder
RNN. The decoder produces a response token by
token using the following recurrence:

hdect = f
RNN(hdect−1, wt−1)

wt = f
out(hdect )

(3)

where fout is some function that outputs a proba-
bility distribution over words; we implement this
using a simple feedforward layer. In our imple-
mentation, we further augment the decoder with
attention (Bahdanau et al., 2014; Luong et al.,



4748

2015) over context encoder states to help capture
long-term inter-comment dependencies. This gen-
erative component can be pre-trained using unla-
beled conversational data.
Prediction component. Given a pre-trained
HRED dialog model, we aim to extend the model
to predict from the conversational context whether
the to-be-forecasted event will occur. Our predic-
tor consists of a multilayer perceptron (MLP) with
3 fully-connected layers, leaky ReLU activations
between layers, and sigmoid activation for output.
For each comment cn, the predictor takes as input
the context encoding hconn and forwards it through
the MLP layers, resulting in an output score that
is interpreted as a probability pevent(cn+1) that the
to-be-forecasted event will happen (e.g., that the
conversation will derail).

Training the predictive component starts by ini-
tializing the weights of the encoders to the val-
ues learned in pre-training. The main train-
ing loop then works as follows: for each pos-
itive sample—i.e., a conversation containing an
instance of the to-be-forecasted event (e.g., de-
railment) at comment ce—we feed the context
c1, . . . , ce−1 through the encoder and classifier,
and compute cross-entropy loss between the clas-
sifier output and expected output of 1. Simi-
larly, for each negative sample—i.e., a conversa-
tion where none of the comments exhibit the to-be-
forecasted event and that ends with cN—we feed
the context c1, . . . , cN−1 through the model and
compute loss against an expected output of 0.

Note that the parameters of the generative com-
ponent are not held fixed during this process; in-
stead, backpropagation is allowed to go all the way
through the encoder layers. This process, known
as fine-tuning, reshapes the representation learned
during pre-training to be more directly useful to
prediction (Howard and Ruder, 2018).

We implement the model and training code us-
ing PyTorch, and we are publicly releasing our im-
plementation and the trained models together with
the data as part of ConvoKit.

5 Forecasting Derailment

We evaluate the performance of CRAFT in the
task of forecasting conversational derailment in
both the Wikipedia and CMV scenarios. To this
end, for each of these datasets we pre-train the
generative component on the unlabeled portion of

the data and fine-tune it on the labeled training
split (data size detailed in Section 3).

In order to evaluate our sequential system
against conversational-level ground truth, we need
to aggregate comment level predictions. If any
comment in the conversation triggers a positive
prediction—i.e., pevent(cn+1) is greater than a
threshold learned on the development split—then
the respective conversation is predicted to derail.
If this forecast is triggered in a conversation that
actually derails, but before the derailment actually
happens, then the conversation is counted as a true
positive; otherwise it is a false positive. If no pos-
itive predictions are triggered for a conversation,
but it actually derails then it counts as a false neg-
ative; if it does not derail then it is a true negative.
Fixed-length window baselines. We first seek to
compare CRAFT to existing, fixed-length window
approaches to forecasting. To this end, we im-
plement two such baselines: Awry, which is the
state-of-the-art method proposed in Zhang et al.
(2018a) based on pragmatic features in the first
comment-reply pair,9 and BoW, a simple bag-
of-words baseline that makes a prediction using
TF-IDF weighted bag-of-words features extracted
from the first comment-reply pair.
Online forecasting baselines. Next, we con-
sider simpler approaches for making forecasts as
the conversations happen (i.e., in an online fash-
ion). First, we propose Cumulative BoW, a model
that recomputes bag-of-words features on all com-
ments seen thus far every time a new comment ar-
rives. While this approach does exhibit the de-
sired behavior of producing updated predictions
for each new comment, it fails to account for re-
lationships between comments.

This simple cumulative approach cannot be
directly extended to models whose features are
strictly based on a fixed number of comments, like
Awry. An alternative is to use a sliding window:
for a feature set based on a window of W com-
ments, upon each new comment we can extract
features from a window containing that comment
and the W − 1 comments preceding it. We apply
this to the Awry method and call this model Slid-
ing Awry. For both these baselines, we aggregate
comment-level predictions in the same way as in
our main model.
CRAFT ablations. Finally, we consider two
modified versions of the CRAFT model in order

9We use the ConvoKit implementation.



4749

Capabilities Wikipedia Talk Pages Reddit CMV
Model D O L A P R FPR F1 A P R FPR F1

BoW 56.5 55.6 65.5 52.4 60.1 52.1 51.8 61.3 57.0 56.1
Awry X 58.9 59.2 57.6 39.8 58.4 54.4 55.0 48.3 39.5 51.4

Cumul. BoW X 60.6 57.7 79.3 58.1 66.8 59.9 58.8 65.9 46.2 62.1
Sliding Awry X X 60.6 60.2 62.4 41.2 61.3 56.8 56.6 58.2 44.6 57.4

CRAFT − CE X X 64.9 64.4 66.7 36.9 65.5 57.7 56.1 71.2 55.7 62.8
CRAFT X X X 66.5 63.7 77.1 44.1 69.8 63.4 60.4 77.5 50.7 67.9

Table 1: Comparison of the capabilities of each baseline and our CRAFT models (full and without the Context En-
coder) with regards to capturing inter-comment (D)ynamics, processing conversations in an (O)nline fashion, and
automatically (L)earning feature representations, as well as their performance in terms of (A)ccuracy, (P)recision,
(R)ecall, False Positive Rate (FPR), and F1 score. Awry is the model previously proposed by Zhang et al. (2018a)
for this task.

to evaluate the impact of two of its key compo-
nents: (1) the pre-training step, and (2) its ability
to capture inter-comment dependencies through its
hierarchical memory.

To evaluate the impact of pre-training, we train
the prediction component of CRAFT on only the
labeled training data, without first pre-training
the encoder layers with the unlabeled data. We
find that given the relatively small size of labeled
data, this baseline fails to successfully learn, and
ends up performing at the level of random guess-
ing.10 This result underscores the need for the pre-
training step that can make use of unlabeled data.

To evaluate the impact of the hierarchical mem-
ory, we implement a simplified version of CRAFT
where the memory size of the context encoder is
zero (CRAFT − CE), thus effectively acting as
if the pre-training component is a vanilla seq2seq
model. In other words, this model cannot capture
inter-comment dependencies, and instead at each
step makes a prediction based only on the utter-
ance encoding of the latest comment.
Results. Table 1 compares CRAFT to the base-
lines on the test splits (random baseline is 50%)
and illustrates several key findings. First, we find
that unsurprisingly, accounting for full conversa-
tional context is indeed helpful, with even the
simple online baselines outperforming the fixed-
window baselines. On both datasets, CRAFT out-
performs all baselines (including the other online
models) in terms of accuracy and F1. Further-
more, although it loses on precision (to CRAFT
− CE) and recall (to Cumulative BoW) individu-
ally on the Wikipedia data, CRAFT has the supe-

10We thus exclude this baseline from the results summary.

Figure 3: Precision-recall curves and the area under
each curve. To reduce clutter, we show only the curves
for Wikipedia data (CMV curves are similar) and ex-
clude the fixed-length window baselines (which per-
form worse).

rior balance between the two, having both a vis-
ibly higher precision-recall curve and larger area
under the curve (AUPR) than the baselines (Fig-
ure 3). This latter property is particularly useful
in a practical setting, as it allows moderators to
tune model performance to some desired precision
without having to sacrifice as much in the way of
recall (or vice versa) compared to the baselines
and pre-existing solutions.

6 Analysis

We now examine the behavior of CRAFT in
greater detail, to better understand its benefits and
limitations. We specifically address the following
questions: (1) How much early warning does the
the model provide? (2) Does the model actually



4750

Figure 4: Distribution of number of comments elapsed
between the model’s first warning and the attack.

learn an order-sensitive representation of conver-
sational context?11

Early warning, but how early? The recent in-
terest in forecasting antisocial behavior has been
driven by a desire to provide pre-emptive, action-
able warning to moderators. But does our model
trigger early enough for any such practical goals?

For each personal attack correctly forecasted by
our model, we count the number of comments
elapsed between the time the model is first trig-
gered and the attack. Figure 4 shows the dis-
tribution of these counts: on average, the model
warns of an attack 3 comments before it actu-
ally happens (4 comments for CMV). To further
evaluate how much time this early warning would
give to the moderator, we also consider the differ-
ence in timestamps between the comment where
the model first triggers and the comment contain-
ing the actual attack. Over 50% of conversations
get at least 3 hours of advance warning (2 hours
for CMV). Moreover, 39% of conversations get at
least 12 hours of early warning before they derail.
Does order matter? One motivation behind
the design of our model was the intuition that
comments in a conversation are not independent
events; rather, the order in which they appear mat-
ters (e.g., a blunt comment followed by a polite
one feels intuitively different from a polite com-
ment followed by a blunt one). By design, CRAFT
has the capacity to learn an order-sensitive repre-
sentation of conversational context, but how can
we know that this capacity is actually used? It is
conceivable that the model is simply computing
an order-insensitive “bag-of-features”. Neural net-
work models are notorious for their lack of trans-

11We choose to focus on the Wikipedia scenario since the
conversational prefixes are hand-verified to be civil. For com-
pleteness we also report results for Reddit CMV throughout,
but they should be taken with an additional grain of salt.

2
3
1
4

Shuffled
1
2
3
4

Original

trigger

...

shuffle

18% change prediction

...
Figure 5: The prefix-shuffling procedure (t = 4).

parency, precluding an analysis of how exactly
CRAFT models conversational context. Neverthe-
less, through two simple exploratory experiments,
we seek to show that it does not completely ignore
comment order.

The first experiment for testing whether the
model accounts for comment order is a prefix-
shuffling experiment, visualized in Figure 5. For
each conversation that the model predicts will de-
rail, let t denote the index of the triggering com-
ment, i.e., the index where the model first made a
derailment forecast. We then construct synthetic
conversations by taking the first t − 1 comments
(henceforth referred to as the prefix) and random-
izing their order.12 Finally, we count how often
the model no longer predicts derailment at index t
in the synthetic conversations. If the model were
ignoring comment order, its prediction should re-
main unchanged (as it remains for the Cumula-
tive BoW baseline), since the actual content of
the first t comments has not changed (and CRAFT
inference is deterministic). We instead find that
in roughly one fifth of cases (12% for CMV)
the model changes its prediction on the synthetic
conversations. This suggests that CRAFT learns
an order-sensitive representation of context, not a
mere “bag-of-features”.

To more concretely quantify how much this
order-sensitive context modeling helps with pre-
diction, we can actively prevent the model from
learning and exploiting any order-related dynam-
ics. We achieve this through another type of shuf-
fling experiment, where we go back even further
and shuffle the comment order in the conversa-
tions used for pre-training, fine-tuning and test-
ing. This procedure preserves the model’s abil-
ity to capture signals present within the individual
comments processed so far, as the utterance en-
coder is unaffected, but inhibits it from capturing
any meaningful order-sensitive dynamics. We find
that this hurts the model’s performance (65% ac-

12We restrict the experiment to cases where t ≥ 3, as pre-
fixes consisting of only one comment cannot be reordered.



4751

curacy for Wikipedia, 59.5% for CMV), lowering
it to a level similar to that of the version where we
completely disable the context encoder.

Taken together, these experiments provide ev-
idence that CRAFT uses its capacity to model
conversational context in an order-sensitive fash-
ion, and that it makes effective use of the dynam-
ics within. An important avenue for future work
would be developing more transparent models that
can shed light on exactly what kinds of order-
related features are being extracted and how they
are used in prediction.

7 Conclusions and Future Work

In this work, we introduced a model for fore-
casting conversational events that processes com-
ments as they happen and takes the full conver-
sational context into account to make an updated
prediction at each step. This model fills a void
in the existing literature on conversational fore-
casting, simultaneously addressing the dual chal-
lenges of capturing inter-comment dynamics and
dealing with an unknown horizon. We find that
our model achieves state-of-the-art performance
on the task of forecasting derailment in two differ-
ent datasets that we release publicly. We further
show that the resulting system can provide sub-
stantial prior notice of derailment, opening up the
potential for preemptive interventions by human
moderators (Seering et al., 2017).

While we have focused specifically on the task
of forecasting derailment, we view this work as a
step towards a more general model for real-time
forecasting of other types of emergent properties
of conversations. Follow-up work could adapt
the CRAFT architecture to address other forecast-
ing tasks mentioned in Section 2—including those
for which the outcome is extraneous to the con-
versation. We expect different tasks to be in-
formed by different types of inter-comment dy-
namics, and further architecture extensions could
add additional supervised fine-tuning in order to
direct it to focus on specific dynamics that might
be relevant to the task (e.g., exchange of ideas be-
tween interlocutors or stonewalling).

With respect to forecasting derailment, there re-
main open questions regarding what human mod-
erators actually desire from an early-warning sys-
tem, which would affect the design of a practi-
cal system based on this work. For instance, how
early does a warning need to be in order for moder-

ators to find it useful? What is the optimal balance
between precision, recall, and false positive rate at
which such a system is truly improving moderator
productivity rather than wasting their time through
false positives? What are the ethical implications
of such a system? Follow-up work could run a
user study of a prototype system with actual mod-
erators to address these questions.

A practical limitation of the current analysis is
that it relies on balanced datasets, while derail-
ment is a relatively rare event for which a more
restrictive trigger threshold would be appropri-
ate. While our analysis of the precision-recall
curve suggests the system is robust across multi-
ple thresholds (AUPR = 0.7), additional work
is needed to establish whether the recall tradeoff
would be acceptable in practice.

Finally, one major limitation of the present
work is that it assigns a single label to each con-
versation: does it derail or not? In reality, de-
railment need not spell the end of a conversa-
tion; it is possible that a conversation could get
back on track, suffer a repeat occurrence of anti-
social behavior, or any number of other trajecto-
ries. It would be exciting to consider finer-grained
forecasting of conversational trajectories, account-
ing for the natural—and sometimes chaotic—ebb-
and-flow of human interactions.

Acknowledgements. We thank Caleb Chiam,
Liye Fu, Lillian Lee, Alexandru Niculescu-Mizil,
Andrew Wang and Justine Zhang for insightful
conversations (with unknown horizon), Aditya Jha
for his great help with implementing and running
the crowd-sourcing tasks, Thomas Davidson and
Claire Liang for exploratory data annotation, as
well as the anonymous reviewers for their help-
ful comments. This work is supported in part by
the NSF CAREER award IIS-1750615 and by the
NSF Grant SES-1741441. This material is based
upon work supported by Google Cloud.

References
Rob Abbott, Marilyn Walker, Pranav Anand, Jean E.

Fox Tree, Robeson Bowmani, and Joseph King.
2011. How Can You Say Such Things?!?: Rec-
ognizing Disagreement in Informal Political Argu-
ment. In Proceedings of the Workshop on Lan-
guages in Social Media.

Yavuz Akbulut, Yusuf Levent Sahin, and Bahadir
Eristi. 2010. Cyberbullying Victimization among



4752

Turkish Online Social Utility Members. Educa-
tional Technology & Society, 13(4).

Kelsey Allen, Giuseppe Carenini, and Raymond T. Ng.
2014. Detecting Disagreement in Conversations us-
ing Pseudo-Monologic Rhetorical Structure. In Pro-
ceedings of EMNLP.

Tim Althoff, Kevin Clark, and Jure Leskovec. 2016.
Large-scale Analysis of Counseling Conversations:
An Application of Natural Language Processing to
Mental Health. Transactions of the Association for
Computational Linguistics, 4.

Ofer Arazy, Lisa Yeo, and Oded Nov. 2013. Stay on the
Wikipedia Task: When Task-related Disagreements
Slip Into Personal and Procedural Conflicts. J. Am.
Soc. Inf. Sci. Technol., 64(8).

Lars Backstrom, Jon Kleinberg, Lillian Lee, and Cris-
tian Danescu-Niculescu-Mizil. 2013. Characteriz-
ing and Curating Conversation Threads: Expansion,
Focus, Volume, Re-entry. In Proceedings of WSDM.

Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Ben-
gio. 2014. Neural Machine Translation by Jointly
Learning to Align and Translate. In Proceedings of
ICLR.

Anais Cadilhac, Nicholas Asher, Farah Benamara, and
Alex Lascarides. 2013. Grounding Strategic Con-
versation: Using Negotiation Dialogues to Predict
Trades in a Win-Lose Game. In Proceedings of
EMNLP.

Justin Cheng, Michael Bernstein, Cristian Danescu-
Niculescu-Mizil, and Jure Leskovec. 2017. Anyone
Can Become a Troll: Causes of Trolling Behavior in
Online Discussions. In Proceedings of CSCW.

Kyunghyun Cho, Bart van Merrienboer, Caglar
Gulcehre, Dzmitry Bahdanau, Fethi Bougares,
Holger Schwenk, and Yoshua Bengio. 2014.
Learning Phrase Representations using RNN En-
coder–Decoder for Statistical Machine Translation.
In Proceedings of EMNLP.

Benjamin Collier and Julia Bear. 2012. Conflict, Crit-
icism, or Confidence: An Empirical Examination of
the Gender Gap in Wikipedia Contributions. In Pro-
ceedings of CSCW.

Jared R. Curhan and Alex Pentland. 2007. Thin Slices
of Negotiation: Predicting Outcomes From Conver-
sational Dynamics Within the First 5 Minutes. Jour-
nal of Applied Psychology, 92.

Andrew M. Dai and Quoc V. Le. 2015. Semi-
supervised Sequence Learning. In Proceedings of
NeurIPS.

Thomas Davidson, Dana Warmsley, Michael Macy,
and Ingmar Weber. 2017. Automated Hate Speech
Detection and the Problem of Offensive Language.
In Proceedings of ICWSM.

Jacob Devlin, Ming-Wei Chang, Kenton Lee, and
Kristina Toutanova. 2019. BERT: Pre-training of
Deep Bidirectional Transformers for Language Un-
derstanding. In Proceedings of NAACL.

Maeve Duggan. 2014. Online Harassment.
http://www.pewinternet.org/2014/10/22/online-
harassment/.

Michael Feldman, Sorelle A. Friedler, John Moeller,
Carlos Scheidegger, and Suresh Venkatasubrama-
nian. 2015. Certifying and Removing Disparate Im-
pact. In Proceedings of KDD.

Michel Galley, Kathleen McKeown, Julia Hirschberg,
and Elizabeth Shriberg. 2004. Identifying Agree-
ment and Disagreement in Conversational Speech:
Use of Bayesian Networks to Model Pragmatic De-
pendencies. In Proceedings of ACL.

Jianfeng Gao, Michel Galley, and Lihong Li. 2018.
Neural Approaches to Conversational AI. In Pro-
ceedings of SIGIR.

Kiran Garimella, Gianmarco De Francisci Morales,
Aristides Gionis, and Michael Mathioudakis. 2017.
Quantifying Controversy in Social Media. ACM
Transactions on Social Computing, 1(1).

Codruta Girlea, Roxana Girju, and Eyal Amir. 2016.
Psycholinguistic Features for Deceptive Role Detec-
tion in Werewolf. In Proceedings of NAACL.

Jack Hessel and Lillian Lee. 2019. Something’s
Brewing! Early Prediction of Controversy-causing
Posts from Discussion Features. In Proceedings of
NAACL.

Jeremy Howard and Sebastian Ruder. 2018. Universal
Language Model Fine-tuning for Text Classification.
In Proceedings of ACL.

Yiqing Hua, Cristian Danescu-Niculescu-Mizil, Dario
Taraborelli, Nithum Thain, Jeffery Sorensen, and
Lucas Dixon. 2018. WikiConv: A Corpus of the
Complete Conversational History of a Large On-
line Collaborative Community. In Proceedings of
EMNLP.

Yohan Jo, Shivani Poddar, Byungsoo Jeon, Qinlan
Shen, Carolyn P. Rosé, and Graham Neubig. 2018.
Attentive Interaction Model: Modeling Changes in
View in Argumentation. In Proceedings of NAACL.

David Jurgens, Libby Hemphill, and Eshwar Chan-
drasekharan. 2019. A Just and Comprehensive
Strategy for Using NLP to Address Online Abuse.
In Proceedings of ACL.

Joseph M. Kayany. 1998. Contexts of uninhibited
online behavior: Flaming in social newsgroups on
usenet. Journal of the American Society for Infor-
mation Science, 49(12).

Srijan Kumar, William L. Hamilton, Jure Leskovec,
and Dan Jurafsky. 2018. Community Interaction and
Conflict on the Web. In Proceedings of WWW.



4753

Sarah Ita Levitan, Angel Maredia, and Julia
Hirschberg. 2018. Linguistic Cues to Deception
and Perceived Deception in Interview Dialogues. In
Proceedings of NAACL.

Ping Liu, Joshua Guberman, Libby Hemphill, and
Aron Culotta. 2018. Forecasting the Presence and
Intensity of Hostility on Instagram Using Linguistic
and Social Features. In Proceedings of ICWSM.

Minh-Thang Luong, Hieu Pham, and Christopher D.
Manning. 2015. Effective Approaches to Attention-
based Neural Machine Translation. In Proceedings
of EMNLP.

Bryan McCann, James Bradbury, Caiming Xiong, and
Richard Socher. 2017. Learned in Translation:
Contextualized Word Vectors. In Proceedings of
NeurIPS.

Vlad Niculae and Cristian Danescu-Niculescu-Mizil.
2016. Conversational Markers of Constructive Dis-
cussions. In Proceedings of NAACL.

Vlad Niculae, Srijan Kumar, Jordan Boyd-Graber, and
Cristian Danescu-Niculescu-Mizil. 2015. Linguistic
Harbingers of Betrayal: A Case Study on an Online
Strategy Game. In Proceedings of ACL.

Alexandra Olteanu, Carlos Castillo, Jeremy Boy, and
Kush Varshney. 2018. The Effect of Extremist Vio-
lence on Hateful Speech Online. In Proceedings of
ICWSM.

Ji Ho Park, Jamin Shin, and Pascale Fung. 2018. Re-
ducing Gender Bias in Abusive Language Detection.
In Proceedings of EMNLP.

John Pavlopoulos, Prodromos Malakasiotis, and Ion
Androutsopoulos. 2017. Deeper Attention to Abu-
sive User Content Moderation. In Proceedings of
EMNLP.

Verónica Pérez-Rosas, Mohamed Abouelenien, Rada
Mihalcea, Yao Xiao, C. J. Linton, and Mihai Burzo.
2016. Verbal and Nonverbal Clues for Real-life De-
ception Detection. In Proceedings of EMNLP.

Matthew E. Peters, Mark Neumann, Mohit Iyyer, Matt
Gardner, Christopher Clark, Kenton Lee, and Luke
Zettlemoyer. 2018. Deep Contextualized Word Rep-
resentations. In Proceedings of NAACL.

Peter Potash and Anna Rumshisky. 2017. Towards De-
bate Automation: A Recurrent Model for Predicting
Debate Winners. In Proceedings of EMNLP.

Alec Radford, Karthik Narasimhan, Tim Salimans, and
Ilya Sutskever. 2018. Improving Language Under-
standing by Generative Pre-training. Technical re-
port, OpenAI.

Juliana Raskauskas and Ann D. Stoltz. 2007. Involve-
ment in Traditional and Electronic Bullying Among
Adolescents. Developmental Psychology, 43(3).

Manoel Horta Ribeiro, Pedro H. Calais, Yuri A. San-
tos, Virgı́lio A. F. Almeida, and Wagner Meira Jr.
2018. Characterizing and Detecting Hateful Users
on Twitter. In Proceedings of ICWSM.

Sara Rosenthal and Kathleen McKeown. 2015. I
Couldn’t Agree More: The Role of Conversational
Structure in Agreement and Disagreement Detection
in Online Discussions. In Proceedings of SIGDIAL.

Maarten Sap, Dallas Card, Saadia Gabriel, Yejin Choi,
and Noah A. Smith. 2019. The Risk of Racial Bias
in Hate Speech Detection. In Proceedings of ACL.

Joseph Seering, Robert Kraut, and Laura Dabbish.
2017. Shaping Pro and Anti-Social Behavior on
Twitch Through Moderation and Example-Setting.
In Proceedings of CSCW.

Iulian V. Serban, Alessandro Sordoni, Yoshua Bengio,
Aaron Courville, and Joelle Pineau. 2016. Build-
ing End-To-End Dialogue Systems Using Genera-
tive Hierarchical Neural Network Models. In Pro-
ceedings of AAAI.

Iulian Vlad Serban, Alessandro Sordoni, Ryan Lowe,
Laurent Charlin, Joelle Pineau, Aaron Courville, and
Yoshua Bengio. 2017. A Hierarchical Latent Vari-
able Encoder-Decoder Model for Generating Dia-
logues. In Proceedings of AAAI.

Vivek K. Singh, Marie L. Radford, Qianjia Huang, and
Susan Furrer. 2017. ”They basically like destroyed
the school one day”: On Newer App Features and
Cyberbullying in Schools. In Proceedings of CSCW.

Alessandro Sordoni, Yoshua Bengio, Hossein Vahabi,
Christina Lioma, Jakob Grue Simonsen, and Jian-
Yun Nie. 2015a. A Hierarchical Recurrent Encoder-
Decoder for Generative Context-Aware Query Sug-
gestion. In Proceedings of CIKM.

Alessandro Sordoni, Michel Galley, Michael Auli,
Chris Brockett, Yangfeng Ji, Margaret Mitchell,
Jian-Yun Nie, Jianfeng Gao, and Bill Dolan. 2015b.
A Neural Network Approach to Context-Sensitive
Generation of Conversational Responses. In Pro-
ceedings of NAACL.

Ilya Sutskever, Oriol Vinyals, and Quoc V. Le. 2014.
Sequence to Sequence Learning with Neural Net-
works. In Proceedings of NeurIPS.

Chenhao Tan, Vlad Niculae, Cristian Danescu-
Niculescu, and Lillian Lee. 2016. Winning Argu-
ments: Interaction Dynamics and Persuasion Strate-
gies in Good-faith Online Discussions. In Proceed-
ings of WWW.

Jessica Vitak, Kalyani Chadha, Linda Steiner, and
Zahra Ashktorab. 2017. Identifying Women’s Ex-
periences With and Strategies for Mitigating Nega-
tive Effects of Online Harassment. In Proceedings
of CSCW.



4754

Svitlana Volkova and Eric Bell. 2017. Identifying Ef-
fective Signals to Predict Deleted and Suspended
Accounts on Twitter across Languages. In Proceed-
ings of ICWSM.

Henning Wachsmuth, Shahbaz Syed, and Benno Stein.
2018. Retrieval of the Best Counterargument with-
out Prior Topic Knowledge. In Proceedings of ACL.

Lu Wang, Nick Beauchamp, Sarah Shugars, and
Kechen Qin. 2017. Winning on the Merits: The
Joint Effects of Content and Style on Debate Out-
comes. Transactions of the Association for Compu-
tational Linguistics, 5.

Lu Wang and Claire Cardie. 2014. A Piece of My
Mind: A Sentiment Analysis Approach for Online
Dispute Detection. In Proceedings of ACL.

William Warner and Julia Hirschberg. 2012. Detecting
Hate Speech on the World Wide Web. In Proceed-
ings of the Second Workshop on Language in Social
Media.

Michael Wiegand, Josef Ruppenhofer, and Thomas
Kleinbauer. 2019. Detection of Abusive Language:
The Problem of Biased Datasets. In Proceedings of
NAACL.

Ellery Wulczyn, Nithum Thain, and Lucas Dixon.
2017. Ex Machina: Personal Attacks Seen at Scale.
In Proceedings of WWW.

Diyi Yang, Jiaao Chen, Zichao Yang, Dan Jurafsky,
and Eduard Hovy. 2019. Let’s Make Your Request
More Persuasive: Modeling Persuasive Strategies
via Semi-Supervised Neural Nets on Crowdfunding
Platforms. In Proceedings of NAACL.

Dawei Yin, Zhenzhen Xue, and Liangjie Hong. 2009.
Detection of Harassment on Web 2.0. In Proceed-
ings of CAW2.0.

Justine Zhang, Jonathan P. Chang, Cristian Danescu-
Niculescu-Mizil, Lucas Dixon, Nithum Thain,
Yiqing Hua, and Dario Taraborelli. 2018a. Conver-
sations Gone Awry: Detecting Early Signs of Con-
versational Failure. In Proceedings of ACL.

Justine Zhang, Cristian Danescu-Niculescu-Mizil,
Christina Sauper, and Sean J. Taylor. 2018b. Char-
acterizing Online Public Discussions Through Pat-
terns of Participant Interactions. In Proceedings of
CSCW.

Justine Zhang, Ravi Kumar, Sujith Ravi, and Cris-
tian Danescu-Niculescu-Mizil. 2016. Conversa-
tional Flow in Oxford-style Debates. In Proceedings
of NAACL.


