



















































The KnowRef Coreference Corpus: Removing Gender and Number Cues for Difficult Pronominal Anaphora Resolution


Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 3952–3961
Florence, Italy, July 28 - August 2, 2019. c©2019 Association for Computational Linguistics

3952

The KNOWREF Coreference Corpus: Removing Gender and Number
Cues for Difficult Pronominal Anaphora Resolution

Ali Emami*1, Paul Trichelair*1, Adam Trischler2, Kaheer Suleman2,
Hannes Schulz2, and Jackie Chi Kit Cheung1

1School of Computer Science, Mila/McGill University
2Microsoft Research Montreal

{ali.emami, paul.trichelair}@mail.mcgill.ca
{adam.trischler, kasulema, hannes.schulz}@microsoft.com

jcheung@cs.mcgill.ca

Abstract

We introduce a new benchmark for corefer-
ence resolution and NLI, KNOWREF, that tar-
gets common-sense understanding and world
knowledge. Previous coreference resolution
tasks can largely be solved by exploiting the
number and gender of the antecedents, or have
been handcrafted and do not reflect the diver-
sity of naturally occurring text. We present
a corpus of over 8,000 annotated text pas-
sages with ambiguous pronominal anaphora.
These instances are both challenging and re-
alistic. We show that various coreference
systems, whether rule-based, feature-rich, or
neural, perform significantly worse on the
task than humans, who display high inter-
annotator agreement. To explain this perfor-
mance gap, we show empirically that state-of-
the art models often fail to capture context, in-
stead relying on the gender or number of can-
didate antecedents to make a decision. We
then use problem-specific insights to propose
a data-augmentation trick called antecedent
switching to alleviate this tendency in mod-
els. Finally, we show that antecedent switch-
ing yields promising results on other tasks as
well: we use it to achieve state-of-the-art re-
sults on the GAP coreference task.

1 Introduction

Coreference resolution is one of the best known
tasks in Natural Language Processing (NLP). De-
spite a large body of work in the area over the
last few decades (Morton, 2000; Bean and Riloff,
2004; McCallum and Wellner, 2005; Rahman and
Ng, 2009), the task remains challenging. Many
resolution decisions require extensive world knowl-
edge and understanding common points of refer-
ence (Pradhan et al., 2011). In the case of pronomi-
nal anaphora resolution, these forms of “common
sense” become much more important when cues

*equal contribution

like gender and number do not by themselves indi-
cate the correct resolution (Trichelair et al., 2018).

To date, most existing methods for coreference
resolution (Raghunathan et al., 2010; Lee et al.,
2011; Durrett et al., 2013; Lee et al., 2017, 2018)
have been evaluated on a few popular datasets, in-
cluding the CoNLL 2011 and 2012 shared corefer-
ence resolution tasks (Pradhan et al., 2011, 2012).
These datasets were proposed as the first compre-
hensively tagged and large-scale corpora for coref-
erence resolution, to spur progress in state-of-the-
art techniques. According to Durrett and Klein
(2013), this progress would contribute in the “up-
hill battle” of modelling not just syntax and dis-
course, but also semantic compatibility based on
world knowledge and context.

Despite improvements in benchmark dataset per-
formance, the question of what exactly current sys-
tems learn or exploit remains open, particularly
with recent neural coreference resolution models.
Lee et al. (2017) note that their model does “little
in the uphill battle of making coreference decisions
that require world knowledge,” and highlight a few
examples in the CoNLL 2012 task that rely on more
complex understanding or inference. Because these
cases are infrequent in the data, systems can per-
form very well on the CoNLL tasks according to
standard metrics by exploiting surface cues. High-
performing models have also been observed to rely
on social stereotypes present in the data, which
could unfairly impact their decisions for some de-
mographics (Zhao et al., 2018).

There is a recent trend, therefore, to develop
more challenging and diverse coreference tasks.
Perhaps the most popular of these is the Winograd
Schema Challenge (WSC), which has emerged as
an alternative to the Turing test (Levesque et al.,
2011). The WSC task is carefully controlled such
that heuristics involving syntactic salience, the
number and gender of the antecedents, or other



3953

obvious syntactic/semantic cues are ineffective.
Previous approaches to common sense reasoning,
based on logical formalisms (Bailey et al., 2015) or
deep neural models (Liu et al., 2016), have solved
only restricted subsets of the WSC with high preci-
sion. These shortcomings can in part be attributed
to the limited size of the corpus (273 instances),
which is a side effect of its hand-crafted nature.
Webster et al. (2018) recently presented a corpus
called GAP that consists of about 4,000 unique bi-
nary coreference instances from English Wikipedia.
This corpus is intended to address gender bias and
the mentioned size limitations of the WSC. We be-
lieve that gender bias in coreference resolution is
part and parcel of a more general problem: current
models are unable to abstract away from the enti-
ties in the sentence to take advantage of the wider
context to make a coreference decision.

To tackle this issue, we present a coreference res-
olution corpus called KNOWREF that specifically
targets the ability of systems to reason about a situ-
ation described in the context.1 We designed this
task to be challenging, large-scale, and based on
natural text. The main contributions of this paper
are as follows:

1. We develop mechanisms by which we con-
struct a human-labeled corpus of 8,724
Winograd-like text samples whose resolution
requires significant common sense and back-
ground knowledge. As an example:

Marcus is undoubtedly faster than Jarrett
right now but in [his] prime the gap wasn’t
all that big. (answer: Jarrett)

2. We propose a task-specific metric called con-
sistency that measures the extent to which a
model uses the full context (as opposed to a
surface cue) to make a coreference decision.
We use this metric to analyze the behavior of
state-of-the-art methods and demonstrate that
they generally under-utilize context informa-
tion.

3. We find that a fine-tuned version of the recent
large-scale language model, BERT (Devlin
et al., 2018), performs significantly better than
other methods on KNOWREF, although with

1The corpus, the code to scrape the sentences
from the source texts, as well as the code to repro-
duce all of our experimental results are available at
https://github.com/aemami1/KnowRef.

substantial room for improvement to match
human performance.

4. We demonstrate the benefits of a data-
augmentation technique called antecedent
switching in expanding our corpus, further de-
terring models from exploiting surface cues,
as well as in transferring to models trained on
other co-reference tasks like GAP, leading to
state-of-the-art results.

2 Related Work

2.1 General coreference resolution
Automated techniques for standard coreference res-
olution — that is, the task of correctly partitioning
the entities and events that occur in a document into
resolution classes — date back to decision trees and
hand-written rules (Hobbs, 1977; McCarthy, 1995).
The earliest evaluation corpora were the Message
Understanding Conferences (MUC) (Grishman and
Sundheim, 1996) and the ACE (Doddington et al.,
2004). These focused on noun phrases tagged with
coreference information, but were limited in either
size or annotation coverage.

The datasets of Pradhan et al. (2011, 2012) from
the CoNLL-2011 and CoNLL-2012 Shared Tasks
were proposed as large-scale corpora with high
inter-annotator agreement. They were constructed
by restricting the data to coreference phenomena
with highly consistent annotations, and were pack-
aged with a standard evaluation framework to facil-
itate performance comparisons.

The quality of these tasks led to their widespread
use and the emergence of many resolution systems,
ranging from hand-engineered methods to deep-
learning approaches. The multi-pass sieve system
of Raghunathan et al. (2010) is fully deterministic
and makes use of mention attributes like gender
and number; it maintained the best results on the
CoNLL 2011 task for a number of years (Lee et al.,
2011). Later, lexical learning approaches emerged
as the new state of the art (Durrett and Klein, 2013),
followed more recently by neural models (Wise-
man et al., 2016; Clark and Manning, 2016). The
current state-of-the-art result on the CoNLL 2012
task is by an end-to-end neural model from Lee
et al. (2018) that does not rely on a syntactic parser
or a hand-engineered mention detector.

2.2 Gender bias in general coreference
resolution

Zhao et al. (2018) observed that state-of-the-art



3954

methods for coreference resolution become gender-
biased, exploiting various stereotypes that leak
from society into data. They devise a dataset of
3,160 manually written sentences called WinoBias
that serves both as a gender-bias test for corefer-
ence resolution models and as a training set to
counter stereotypes in existing corpora (i.e., the
two CoNLL tasks). The following example is rep-
resentative:

(1) The physician hired the secretary because
he was overwhelmed with clients.

(2) The physician hired the secretary because
she was overwhelmed with clients.

Experiments conducted on various models demon-
strated that an end-to-end neural model (Lee et al.,
2017) maintains its performance without the gen-
der bias when trained partially on both the previous
datasets and on WinoBias.

A concurrent work by Rudinger et al. (2018)
also proposed an empirical study of the biases in
coreference resolution systems. In contrast to Zhao
et al. (2018), who attribute the bias in part to the
datasets, they conjecture that the gender bias comes
primarily from the models themselves. Based on
statistics from the Bureau of Labor, they show that
various systems all exhibit significant gender bias.

This work on gender stereotypes provides some
insight into the behavior of current models. In the
example above, if she is predicted incorrectly to
refer to the secretary, it is likely because the model
learned a representation for the secretary profession
that encodes gender information. Current models
do not capture the context nor the relation between
was overwhelmed and hired that lead to the correct
resolution. The subject of our work is to investigate
the potential for models to capture contextual rela-
tionships instead of cues from, e.g., gender stereo-
types. Unlike WinoBias, our task is composed of
passages that occur naturally in text and it is several
times larger.

2.3 Difficult cases in coreference resolution
As the creators of the CoNLL tasks note, most
coreference techniques rely primarily on surface-
level features, like the proximity between mentions,
or shallow semantic features like number, gender,
named entities, semantic class, etc., rather than
knowledge and context.

To address this, Levesque et al. (2011) manually
constructed a dataset of challenging pronoun disam-
biguation problems called the Winograd Schema

Challenge. The goal was that any successful system
would necessarily use common-sense knowledge.
Although the WSC is an important step in evalu-
ating systems en route to human-like language un-
derstanding, its size and other characteristics are a
bottleneck for progress in pronoun disambiguation
(Trichelair et al., 2018). A Winograd-like expanded
corpus was proposed by Rahman and Ng (2012)
to address the WSC’s size limitations; however,
systems that perform well on the expanded dataset
do not transfer successfully to the original WSC
(Rahman and Ng, 2012; Peng et al., 2015), likely
due to loosened constraints in the former.

The task that we propose distinguishes itself
from the WSC by building on sentences that occur
in natural text. This yields highly diverse problem
instances. It is particularly important that, as well
as being challenging, tasks are representative of
natural text, so that improvements are more likely
to transfer to the full coreference setting.

Recently, Webster et al. (2018) presented a cor-
pus called GAP that consists of 4,4542 unique bi-
nary coreference instances from English Wikipedia.
It is meant to address gender bias and the described
size limitation of the WSC. For instance, it exposes
the unbalanced performance of current state-of-
the-art resolvers, which more accurately resolve
masculine pronouns than feminine pronouns. As
for the difficulty of the task, the models tested on
GAP were not trained directly on the corpus, which
does not give a clear picture of the task’s difficulty.
A simple heuristic called Parallelism+URL, which
is based on using the syntactic distance between
antecedents and the target pronoun, is so far the
strongest GAP baseline, at above 70% accuracy.
This suggests that GAP is vulnerable to exploits
that circumvent a need for knowledge, albeit not
the gender and number cues that coreference re-
solvers have exploited before. Finally, our corpus
construction process differs from that of GAP’s
by more strictly requiring that the sentences are in
WSC-format, that is, there are exactly two named
entities that occur strictly before the pronoun and
only one of which may co-refer with the pronoun
(in GAP, the pronoun may occur between and be-
fore the named entities and may in fact co-refer
with both named entities). In addition, our cor-
pus construction process exploits the fact that the
named entities can be replaced with any name in or-

2In GAP, one unique coreference instance corresponds to
two pronoun-name pairs, for which they report 8,908 pairs.



3955

Initial filtering:
Clean up raw

text and split it
into sentences.

Connective
Filtering:

Ensure the
occurrence of a

single connective
in the sentence.

Antecedent
Filtering:
Use POS

information
to ensure the
occurrence of
exactly two
NPs before

the connective.

Training set
(Reddit Comments):

Heuristically predict the
labels to a filtered set of
sentences for which there
is a gender “giveaway”.

Test Set
(Wikipedia +

OpenSubtitles):
Human annotators predict
the labels to the collected

sentences.

Quality Control:
Human annotators
examine collected

sentences

>100 million
sentences >1 million

sentences
>100 thousands
sentences

8,724 sentences

Label
Generation

Figure 1: The corpus construction process for KNOWREF

der to increase the task difficulty by automatically
removing gender giveaways as well as to signifi-
cantly increase the size of the corpus by switching
the named entities to create a new task instance.

As such, our paper seeks to explore a wider prob-
lem of which gender bias may be one facet: current
models do not effectively abstract away from the
entities (and instead rely on exploits using gender
or plurality) to make the coreference resolution. By
developing a benchmark task consisting strictly of
sentences for which such cues are ineffective, we
seek to challenge and potentially improve current
coreference resolution models. In addition, based
on our new benchmark, KNOWREF, we introduce
a data-augmentation mechanism, called antecedent
switching, to encourage models to perform this ab-
straction.

3 The KNOWREF Coreference Task

We develop a coreference task called KNOWREF
that features 8,724 difficult pronoun disambigua-
tion problems. Each instance is a short passage
containing a target pronoun that must be correctly
resolved to one of two possible antecedents.

Formally, each problem instance can be de-
scribed as a tuple P = {S,C1, C2, T,K}, where
S is the sentence, C1 and C2 are the candidate an-
tecedents, T is the target pronoun to be resolved
to one of C1 and C2, and K indicates the correct
antecedent. Note that C1, C2, T and K appear
in S. KNOWREF provides {S,C1, C2, T} as in-
put for models, which must predict K (e.g., as the
output of a binary classification over C1, C2). A
representative sentence S is the following.

(3) {Paul} helped {Lionel} hide when [he]
was pursued by the authorities.

Here, C1 = Paul, C2 = Lionel, T = he, and
K = C2 = Lionel.

We control the text so as not to give away the
pronoun’s correct antecedent in surface-level cues
involving syntactic salience or the number and gen-
der of the antecedent. Successful systems must
instead make use of the context, which may require
world knowledge and common-sense inferences;
i.e., that someone who is being helped to hide may
be one who is being pursued by the authorities.

In the following section, we describe the method-
ology used to construct our corpus, provide a
glimpse of a few of its instances and their resolu-
tion rationales, outline the task’s evaluation criteria,
and describe its characteristics.

3.1 Corpus construction

To construct KNOWREF, we scrape text samples
from a large collection of documents: the combi-
nation of 2018 English Wikipedia, OpenSubtitles,
and Reddit comments dating from 2006–2018. We
filter this text through a multi-stage process to en-
sure quality and diversity as depicted in Figure 1,
and described in more detail below.

3.1.1 Initial Filtering

After removing markup, non-ASCII characters, par-
enthetical expressions, headings and lists, we split
the text into sentences. We keep sentences of token
length between 9 and 33 words after naı̈ve tok-
enization, which start with an upper case letter, and
which contain no math.

3.1.2 Connective Filtering

Our first substantial filtering step uses regular ex-
pressions to ensure that each passed sentence con-



3956

KNOWREF Example 1: {Radu} appeared to be killed by {Brother Paulo}, but [he] reappears a short while later injured, but
alive. (K = Radu)

Original sentence: Radu appeared to be killed by Sister Paula, but he reappears a short while later injured, but alive.

KNOWREF Example 2: {Wanda} tries to apologize to {Rose}, but [she] refuses to accept. (K = Rose)
Original sentence: Warren tries to apologize to Rose, but she refuses to accept.

KNOWREF Example 3: {Tom} arrives to where {Alex} was tied, but [he] has come free of his lead. (K = Alex)
Original sentence: Tom arrives to where Vanessa was tied, but she has come free of her lead.

Table 1: Examples of KNOWREF instances.

tains connectives.3 We use a regular expression
to ensure that there is only one connective cluster
(e.g. “, and though”), and that there are at least two
non-stopwords before this connective and a pro-
noun after it. As a final check, we ensure that no
pronoun occurs before the connective, which tends
to remove sentences which are not self-contained.

3.1.3 Antecedent Filtering
On the remaining set of sentences, we use Stan-
ford’s Maxent tagger (Toutanova et al., 2003) to
infer a flat part-of-speech (POS) labelling. Using
the inferred POS tags, we ensure that there are ex-
actly two noun phrases (NPs) before the connective
that do not re-occur after it (a re-occurrence after
the connective means that the pronoun likely refers
to the non-repeated noun phrase).

The mentioned checks resulted in roughly
100,000 sentences across all three corpora. At least
some of these remaining sentences have similar
properties to Winograd schema sentences; that is,
the two noun phrases (NPs) and the pronoun share
the same type. From here, we keep only sentences
where the type indicates that both NPs correspond
to persons, which further filters the remaining sen-
tences. We do this because NPs that denote people
are often named entities or can easily be replaced
by named entities without loss of information. We
targeted these instances also because we investigate
how resolution systems use gender cues and most
gendered pronouns occur with person-type NPs.

3.1.4 Label Generation
We generate our training and test sets from distinct
sources of text using two different methods.

Training set: We automatically collect 70,000
sentences from Reddit that have passed the fil-
ters described above, and filter these down to
roughly 7,500 sentences for which the antecedents
are named entities of different genders. We use

3comma, semicolon, or, since, but, because, although, etc.

a Python library4 to infer the genders, based on a
list of 40,000 names categorized as female or male
compiled by Jörg Michael. Given the pronoun and
the distinct predicted genders for the antecedents,
we can infer the label for the pronoun’s correct res-
olution with high accuracy and without the need for
expensive human annotation. After assigning this
label, we remove the gender giveaway by replac-
ing one of the named entities so that both entities
and the pronoun all match in gender (e.g., in a
sentence with “James”, “Jessica”, and “she” as the
NPs and pronoun, we replace “James” with “Jane”).
These sentences form our training set. To assess
its quality, we gave an annotator a random sample
of 100 training instances with their heuristically
determined labels. The annotator then evaluated
each sentence as “correctly labelled”, “incorrectly
labelled”, or “unresolvable” if neither of the two
candidates were more suitable than the other to
corefer with the pronoun.5 In total, 86% of the in-
stances were deemed to be labelled correctly, 11%
incorrectly labelled, and 3% were not resolvable,
implying that our automatic selection heuristic is
strong but imperfect.

Test set: Human annotators examined all col-
lected sentences for quality control. We also use
a source for the test sentences that is distinct from
that of the training set, directing our pipeline to col-
lect sentences from Wikipedia and OpenSubtitles
rather than Reddit. This is to ensure that stylistic
cues common in the training source cannot be ex-
ploited by models at test time. In total, roughly
10,000 candidate sentences were extracted initially.
As before, we automatically remove gender give-
aways by replacing the named entities with names
of the same gender, rendering the pronoun ambigu-
ous. Then, six human annotators predicted which
antecedent was the correct coreferent of the pro-
noun for a sample of 2,000 candidate sentences, or

4https://pypi.org/project/SexMachine/
5The details and result of this quality-testing study will

also be made public along with the code and dataset.

https://pypi.org/project/SexMachine/


3957

Sentence Characteristic % of Data

Masculine target pronouns 52.7
Feminine target pronouns 47.3

First Antecedent Correct 50.7
Second Antecedent Correct 49.2

Table 2: Characteristics of the dataset, in terms of pro-
noun distribution and correct label.

they labeled the sentence with “neither” (in the case
where neither antecedent feasibly corefers with the
pronoun) or “unclear” (if the sentence was not in-
telligible). Sentences that have a strong agreement
from 5 or more annotators on a single antecedent
(and which are not labeled as “neither” or “un-
clear”) are kept for testing. This yielded 1,269
test sentences. We measured high inter-annotator
agreement on the test set with a Fleiss’ Kappa of
κ = 0.78.

Our pipeline thus yields a total of 8,724 sen-
tences (7,455 training and 1,269 test) whose pro-
noun disambiguation should not be clear from shal-
low features like gender, number, and semantic
type – they should instead require varying degrees
of external knowledge. These sentences consti-
tute the KNOWREF corpus. Examples of some
instances are given in Table 1. As these examples
reveal, each instance may require a unique bit of
common sense knowledge to resolve.

In the first example, the common understanding
that death (by way of killing) causes a disappear-
ance helps us to conclude that Radu, the victim of
murder, is the one to who reappears.

In the next example, human readers recognize
that to accept is something one does with an apol-
ogy. Therefore, she refers to the one that accepts
the apology, i.e., Rose.

For the third example, an understanding that be-
ing tied is related to being deprived of freedom
leads us to conclude that Alex has come free.

3.2 Task Characteristics

In Table 2, we report several statistical characteris-
tics of the data. These suggest a near-equal distri-
bution of feminine and masculine target pronouns
(he/him/his vs. she/her) as well as an equal distri-
bution of the two labels, which keeps chance-based
performance at 50% expected accuracy.

3.3 Evaluation

Our task requires a model to choose between two
candidates, but classical coreference models build
clusters of expressions that refer to the same entity.
With respect to our setting, several errors can be
made by these existing models: predicting that the
two entities and the pronoun share a similar clus-
ter (Both Antecedents Predicted), that none of the
two candidates shares a cluster with the pronoun
(No Decision), or creating a cluster that contains
the pronoun with the wrong candidate (Incorrect
Decision). To obtain a score specific to our task,
we compute a Task-Specific Accuracy which dis-
cards all of the cases in which the model makes no
decision relevant to the target pronoun or chooses
both entities as co-referring to the target pronoun.

4 Experiments and Results

In this section, we compare the performance of
five representative coreference systems on our
task: Stanford’s rule-based system (Raghunathan
et al., 2010) (Rule), Stanford’s statistical sys-
tem (Clark and Manning, 2015) (Stat), Clark
and Manning (2016)’s deep reinforcement learn-
ing system (Deep-RL), Martschat and Strube
(2015)’s latent tree model (Latent), and Lee et al.
(2018)’s end-to-end neural system (E2E). We
also report the accuracy of the state-of-the-art
model, E2E, after retraining on KNOWREF and
on KNOWREF+CoNLL.

Additionally, we develop a task-specific model
for KNOWREF: a discriminatively trained fine-
tuned instance of Bidirectional Encoder Represen-
tations from Transformers (BERT) (Devlin et al.,
2018). We train our task-specific BERT accord-
ing to recent work on language models (LMs) for
the WSC (Trinh and Le, 2018). We first construct
a modified version of the data wherein we dupli-
cate each sentence, replacing the pronoun with one
of the two antecedents in each copy. The task,
akin to NLI, is then to predict which of the two
modified sentences is most probable. To compute
probabilities, we add a softmax layer with task-
specific parameter vector v ∈ RH . Denote by
hS1 ∈ RH (respectively hS2) the final hidden state
for the sentence copy with the pronoun replaced by
the first antecedent (respectively the second). Then
the probability assigned to the first antecedent is

P1 =
ev

>hS1

ev>hS2 + ev>hS2
. (1)



3958

Model Both
Antecedents

Predicted

No Decision Incorrect
Decision

Correct
Decision

Task-
Specific

Accuracy

Random – – – – 0.50
Human5 – – – – 0.92

Rule 0.001 0.12 0.43 0.45 0.52
Stat 0.006 0.09 0.45 0.45 0.50
Deep-RL 0.001 0.09 0.46 0.45 0.49
Latent 0.000 0.12 0.41 0.47 0.54
E2E (CoNLL only) 0.01 0.42 0.23 0.35 0.60

E2E (KNOWREF) 0.000 0.26 0.31 0.43 0.58
E2E
(KNOWREF+CoNLL)

0.000 0.19 0.28 0.52 0.65

BERT (KNOWREF) 0.000 0.000 0.39 0.61 0.61

Table 3: Coverage and performance of various representative systems on the KNOWREF Test set.

The probability assigned to the second antecedent
is P2 = 1− P1. We use H = 768 hidden units in
our BERT implementation and learn v by minimiz-
ing the binary cross entropy with the ground-truth
antecedent labels (in one-hot format).

Human Performance: We determined human
performance on KNOWREF by collecting the pre-
dictions of six native English speakers on a ran-
domly generated sub-sample of 100 problem in-
stances; we consider correct those predictions that
agreed with the majority decision and matched the
ground-truth label derived from the original sen-
tence. We report the performance of the five coref-
erence systems and the human baseline in Table 3.

The human performance of 0.92 attests to the
task’s viability. The performance of the auto-
matic systems pretrained on CoNLL, at random or
slightly above random, demonstrates that state-of-
the-art coreference resolution systems are unable
to solve the task. This suggests the existence in the
wild of difficult but realistic coreference problems
that may be under-represented in CoNLL.

After training on KNOWREF, E2E improves by
more than 5% in task-specific accuracy. We can
infer from this result that the model can make some
use of context to make predictions if trained ap-
propriately, but that the CoNLL shared tasks may
not contain enough of such instances for models
to generalize from them. Finally our task-specific
model reaches an accuracy of at best 65%, far be-
low human performance despite having access to

the two candidates.

4.1 Analysis by Switching Entities

Inspired by Trichelair et al. (2018), we propose to
use a task-specific metric, consistency, to measure
the ability of a model to use context in its corefer-
ence prediction, as opposed to relying on gender
and number cues related to the entities. Account-
ing for this is critical, as we desire models that can
capture social, situational, or physical awareness.

To measure consistency in the KNOWREF cor-
pus, we duplicate the data set but switch the can-
didate antecedents each time they appear in a sen-
tence. This changes the correct resolution. If a
coreference model relies on knowledge and con-
textual understanding, its prediction should change
as well, thus it could be called consistent in its
decision process. If, however, its decision is influ-
enced solely by the antecedent, its output would
stay the same despite the change in context induced
by switching. We define the consistency score as
the percentage of predictions that change from the
original instances to the switched instances. An
example of a switching is:

(4) Original: {Alex} tells {Paulo}, but [he]
does not believe him.
Switched: {Paulo} tells {Alex}, but [he]
does not believe him.

The correct answer switches from K = Paulo to
K = Alex.

5This is an estimate based on a subsample of the data.



3959

Model Consistency

Rule 0%
Stat 76%
Deep-RL 66%
Latent 78%
E2E 62%

E2E (KNOWREF) 66%
E2E (KNOWREF+CoNLL) 67%
BERT (KNOWREF) 69%

Table 4: The sensitivity of various systems to the
instance antecedents, according to the number of
changed decisions when the antecedents are switched.
Higher is better.

Table 4 shows the consistency scores of the vari-
ous baseline models evaluated on the original and
switched duplicates of KNOWREF. The rule-based
system (Raghunathan et al., 2010) always resolves
to the same entity, suggesting that context is ig-
nored. Indeed, the mechanisms underlying this
model mostly rely on a gender and number dic-
tionary (Bergsma and Lin, 2006). This dictionary
informs a count-based approach that assigns a mas-
culine, feminine, neutral, and plural score to each
word. If the pronoun is his, the candidate with the
higher masculine score is likely to be linked to the
pronoun.

The other models, Stat, Deep-RL, E2E, Latent
and BERT are much more robust to the switching
procedure, demonstrating that the resolution par-
tially relies on context cues. Regarding E2E, we
can observe that training the model on KNOWREF
forces the model to rely more on the context, lead-
ing to an improvement of 5%. It further demon-
strates the usefulness of the corpus to obtain a better
representation of the context.

4.2 Data Augmentation by Switching

Inspired by the switching experiment, we propose
to extend the KNOWREF training set by switching
every entity pair (thereby doubling the number of
instances). We hypothesize that this data augmen-
tation trick could force the model to abstract away
from the entities to the context in order to boost per-
formance, since it encounters the same contextual
scenario in the doubled sentences.

Training on the augmented data, we observe an
improvement of 10% for fine-tuned BERT (Ta-
ble 5), yielding a task-specific accuracy of 71%

Model Accuracy ∆ Consistency

BERT
(KNOWREF)

71% +10% 89%

E2E
(KNOWREF)

61% +3% 71%

E2E
(KNOWREF+CoNLL)

66% +1% 75%

Table 5: Accuracy on the KNOWREF test set for each
model after augmenting the training set, as well as the
difference from the result without data augmentation.

on the KNOWREF test set. The improvement in ac-
curacy is marginal for E2E, but we observe a large
gain in consistency. We suspected that the data aug-
mentation trick might also be useful in mitigating
a model’s gender bias, by encouraging the model
to rely more on the context than on gendered entity
names. To test this hypothesis, we train the same
model with and without the data augmentation trick
on the recently released GAP corpus (Webster et al.,
2018).

Model F
F
1

FM1
F1

Parallelism6 0.93 66.9
Parallelism+URL6 0.95 70.6
BERT (GAP) 1.02 69.2
BERT (GAP) + Data Aug. 1.00 71.1

Table 6: Performance on the GAP test set

BERT fine-tuned on GAP achieves a state of the
art F1 of 71.1 after data augmentation (Table 6).
Not only does the augmentation improve the over-
all performance (+1.9) but it further balances the
predictions’ female:male ratio to 1:1.

4.3 Error Analysis
We show examples of BERT’s performance
(trained on KNOWREF) on our test set in Table
7. This includes instances on which it succeeds
and fails for both original and switched sentences.
In general, it is not clear why certain instances
are more difficult for BERT to resolve, although
training BERT on the augmented, switched corpus
significantly reduces the frequency of inconsistent
resolutions (from 31% to 11%).

These examples illustrate how challenging cer-
tain real-world situations can be for models to un-

6Scores reported in the original paper (Webster et al., 2018)



3960

Sentence Type Sentence Answer

Original
Switched

Kara is in love with Tanya but she is too shy to tell [her].
Tanya is in love with Kara but she is too shy to tell [her].

Tanya X
Kara X
(consistently correct)

Original
Switched

Peter had not realised how old Henry was until [he] sees his daughter.
Henry had not realised how old Peter was until [he] sees his daughter.

Henry 7
Peter 7
(consistently incorrect)

Original
Switched

Poulidor was no match for Merckx, although [he] offered much resistance .
Merckx was no match for Poulidor, although [he] offered much resistance .

Poulidor X
Poulidor 7
(inconsistent)

Table 7: Examples of various success/failure cases of BERT on the KNOWREF test set

derstand, compared to humans who can reason
about them with ease.

5 Conclusion

We present a new corpus and task, KNOWREF,
for coreference resolution. Our corpus contains
difficult problem instances that require a signifi-
cant degree of common sense and world knowl-
edge for accurate coreference link prediction, and
is larger than previous similar datasets. Using a
task-specific metric, consistency, we demonstrate
that training coreference models on KNOWREF im-
proves their ability to build better representations
of the context. We also show that progress in this
capability is linked to reducing gender bias, with
our proposed model setting the state of the art on
GAP.

In the future, we wish to study the use of
KNOWREF to improve performance on general
coreference resolution tasks (e.g., the CoNLL 2012
Shared Tasks). We also plan to develop new mod-
els on KNOWREF and transfer them to difficult
common sense reasoning tasks.

Acknowledgements

This work was supported by the Natural Sciences
and Engineering Research Council of Canada and
by Microsoft Research. Jackie Chi Kit Cheung is
supported by the Canada CIFAR AI Chair program.

References
Dan Bailey, Amelia Harrison, Yuliya Lierler, Vladimir

Lifschitz, and Julian Michael. 2015. The winograd
schema challenge and reasoning about correlation.
In Working Notes of the Symposium on Logical For-
malizations of Commonsense Reasoning.

David Bean and Ellen Riloff. 2004. Unsupervised
learning of contextual role knowledge for corefer-
ence resolution. In Proceedings of the Human Lan-

guage Technology Conference of the North Ameri-
can Chapter of the Association for Computational
Linguistics: HLT-NAACL 2004.

Shane Bergsma and Dekang Lin. 2006. Bootstrapping
path-based pronoun resolution. In Proceedings of
the 21st International Conference on Computational
Linguistics and the 44th annual meeting of the Asso-
ciation for Computational Linguistics.

Kevin Clark and Christopher D Manning. 2015. Entity-
centric coreference resolution with model stacking.
In Proceedings of the 53rd Annual Meeting of the
Association for Computational Linguistics and the
7th International Joint Conference on Natural Lan-
guage Processing.

Kevin Clark and Christopher D Manning. 2016. Deep
reinforcement learning for mention-ranking corefer-
ence models. In Proceedings of the 2016 Confer-
ence on Empirical Methods in Natural Language
Processing.

Jacob Devlin, Ming-Wei Chang, Kenton Lee, and
Kristina Toutanova. 2018. Bert: Pre-training of deep
bidirectional transformers for language understand-
ing. arXiv preprint arXiv:1810.04805.

George R Doddington, Alexis Mitchell, Mark A Przy-
bocki, Lance A Ramshaw, Stephanie Strassel, and
Ralph M Weischedel. 2004. The automatic content
extraction (ace) program-tasks, data, and evaluation.
In LREC.

Greg Durrett, David Hall, and Dan Klein. 2013. Decen-
tralized entity-level modeling for coreference resolu-
tion. In Proceedings of the 51st Annual Meeting of
the Association for Computational Linguistics.

Greg Durrett and Dan Klein. 2013. Easy victories and
uphill battles in coreference resolution. In Proceed-
ings of the 2013 Conference on Empirical Methods
in Natural Language Processing.

Ralph Grishman and Beth Sundheim. 1996. Mes-
sage understanding conference-6: A brief history.
In COLING 1996 Volume 1: The 16th Interna-
tional Conference on Computational Linguistics,
volume 1.



3961

Jerry R Hobbs. 1977. Pronoun resolution. ACM
SIGART Bulletin.

Heeyoung Lee, Yves Peirsman, Angel Chang,
Nathanael Chambers, Mihai Surdeanu, and Dan
Jurafsky. 2011. Stanford’s multi-pass sieve coref-
erence resolution system at the conll-2011 shared
task. In Proceedings of the fifteenth conference on
computational natural language learning: Shared
task.

Kenton Lee, Luheng He, Mike Lewis, and Luke Zettle-
moyer. 2017. End-to-end neural coreference resolu-
tion. In Proceedings of the 2017 Conference on Em-
pirical Methods in Natural Language Processing.

Kenton Lee, Luheng He, and Luke Zettlemoyer. 2018.
Higher-order coreference resolution with coarse-to-
fine inference. In Proceedings of the 2018 Confer-
ence of the North American Chapter of the Associ-
ation for Computational Linguistics: Human Lan-
guage Technologies.

Hector J Levesque, Ernest Davis, and Leora Morgen-
stern. 2011. The winograd schema challenge. In
AAAI Spring Symposium: Logical Formalizations of
Commonsense Reasoning.

Quan Liu, Hui Jiang, Andrew Evdokimov, Zhen-Hua
Ling, Xiaodan Zhu, Si Wei, and Yu Hu. 2016. Prob-
abilistic reasoning via deep learning: Neural associ-
ation models. arXiv preprint arXiv:1603.07704.

Sebastian Martschat and Michael Strube. 2015. Latent
structures for coreference resolution. Transactions
of the Association of Computational Linguistics.

Andrew McCallum and Ben Wellner. 2005. Condi-
tional models of identity uncertainty with applica-
tion to noun coreference. In Advances in neural in-
formation processing systems.

JF McCarthy. 1995. Using decision trees for corefer-
ence resolution. In Proc. 14th International Joint
Conf. on Artificial Intelligence.

Thomas S Morton. 2000. Coreference for nlp applica-
tions. In Proceedings of the 38th Annual Meeting on
Association for Computational Linguistics. Associa-
tion for Computational Linguistics.

Haoruo Peng, Daniel Khashabi, and Dan Roth. 2015.
Solving hard coreference problems. Urbana.

Sameer Pradhan, Alessandro Moschitti, Nianwen Xue,
Olga Uryupina, and Yuchen Zhang. 2012. Conll-
2012 shared task: Modeling multilingual unre-
stricted coreference in ontonotes. In Joint Confer-
ence on EMNLP and CoNLL-Shared Task.

Sameer Pradhan, Lance Ramshaw, Mitchell Marcus,
Martha Palmer, Ralph Weischedel, and Nianwen
Xue. 2011. Conll-2011 shared task: Modeling un-
restricted coreference in ontonotes. In Proceedings
of the Fifteenth Conference on Computational Natu-
ral Language Learning: Shared Task.

Karthik Raghunathan, Heeyoung Lee, Sudarshan Ran-
garajan, Nathanael Chambers, Mihai Surdeanu, Dan
Jurafsky, and Christopher Manning. 2010. A multi-
pass sieve for coreference resolution. In Proceed-
ings of the 2010 Conference on Empirical Methods
in Natural Language Processing. Association for
Computational Linguistics.

Altaf Rahman and Vincent Ng. 2009. Supervised mod-
els for coreference resolution. In Proceedings of
the 2009 Conference on Empirical Methods in Natu-
ral Language Processing. Association for Computa-
tional Linguistics.

Altaf Rahman and Vincent Ng. 2012. Resolving
complex cases of definite pronouns: the winograd
schema challenge. In Proceedings of the 2012 Joint
Conference on Empirical Methods in Natural Lan-
guage Processing and Computational Natural Lan-
guage Learning.

Rachel Rudinger, Jason Naradowsky, Brian Leonard,
and Benjamin Van Durme. 2018. Gender bias in
coreference resolution. In Proceedings of the 2018
Conference of the North American Chapter of the
Association for Computational Linguistics: Human
Language Technologies.

Kristina Toutanova, Dan Klein, Christopher D Man-
ning, and Yoram Singer. 2003. Feature-rich part-of-
speech tagging with a cyclic dependency network.
In Proceedings of the 2003 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics on Human Language Technology.

Paul Trichelair, Ali Emami, Jackie Chi Kit Cheung,
Adam Trischler, Kaheer Suleman, and Fernando
Diaz. 2018. On the evaluation of common-sense
reasoning in natural language understanding. The
NeurIPS Workshop on Critiquing and Correcting
Trends in Machine Learning.

Trieu H Trinh and Quoc V Le. 2018. A simple
method for commonsense reasoning. arXiv preprint
arXiv:1806.02847.

Kellie Webster, Marta Recasens, Vera Axelrod, and Ja-
son Baldridge. 2018. Mind the gap: A balanced
corpus of gendered ambiguous pronouns. Transac-
tions of the Association for Computational Linguis-
tics, 6:605–617.

Sam Wiseman, Alexander M Rush, and Stuart M
Shieber. 2016. Learning global features for coref-
erence resolution. In Proceedings of NAACL-HLT.

Jieyu Zhao, Tianlu Wang, Mark Yatskar, Vicente Or-
donez, and Kai-Wei Chang. 2018. Gender bias in
coreference resolution: Evaluation and debiasing
methods. In NAACL-HLT.


