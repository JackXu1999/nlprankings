



















































Neural Machine Translation: Hindi-Nepali


Proceedings of the Fourth Conference on Machine Translation (WMT), Volume 3: Shared Task Papers (Day 2) pages 202–207
Florence, Italy, August 1-2, 2019. c©2019 Association for Computational Linguistics

202

1

000

001

002

003

004

005

006

007

008

009

010

011

012

013

014

015

016

017

018

019

020

021

022

023

024

025

026

027

028

029

030

031

032

033

034

035

036

037

038

039

040

041

042

043

044

045

046

047

048

049

050

051

052

053

054

055

056

057

058

059

060

061

062

063

064

065

066

067

068

069

070

071

072

073

074

075

076

077

078

079

080

081

082

083

084

085

086

087

088

089

090

091

092

093

094

095

096

097

098

099

ACL 2019 Submission ***. Confidential Review Copy. DO NOT DISTRIBUTE.

Neural Machine Translation: Hindi ⇔ Nepali
Sahinur Rahman Laskar, Partha Pakray and Sivaji Bandyopadhyay

Department of Computer Science and Engineering
National Institute of Technology Silchar

Assam, India
{sahinurlaskar.nits, parthapakray, sivaji.cse.ju}@gmail.com

Abstract
With the extensive use of Machine Transla-
tion (MT) technology, there is progressively
interest in directly translating between pairs
of similar languages. Because the main chal-
lenge is to overcome the limitation of available
parallel data to produce a precise MT output.
Current work relies on the Neural Machine
Translation (NMT) with attention mechanism
for the similar language translation of WMT19
shared task in the context of Hindi-Nepali pair.
The NMT systems trained the Hindi-Nepali
parallel corpus and tested, analyzed in Hindi
⇔ Nepali translation. The official result de-
clared at WMT19 shared task, which shows
that our NMT system obtained Bilingual Eval-
uation Understudy (BLEU) score 24.6 for pri-
mary configuration in Nepali to Hindi transla-
tion. Also, we have achieved BLEU score 53.7
(Hindi to Nepali) and 49.1 (Nepali to Hindi) in
contrastive system type.

1 Introduction

MT acts as an interface, which handles language
perplexity issues using automatic translation in be-
tween pair of diverse languages in Natural Lan-
guage Processing (NLP). Although, corpus-based
based MT system overcome limitations of rule-
based MT system such as dependency on lin-
guistic expertise, the complexity of various tasks
of NLP and language diversity for Interlingua-
based MT system (Dave et al., 2001). But it
needs sufficient parallel corpus to get optimize
MT output. The NMT falls under the category
of corpus-based MT system, which provides bet-
ter accuracy than Statistical Machine Translation
(SMT), corpus-based MT system. The NMT
system used to overcome the demerits of SMT,
such as the issue of accuracy and requirement of
large datasets. Recurrent Neural Network (RNN)
encoder-decoder NMT system, which assists en-
coding of a variable-length source sentence into a

fixed-length vector and same is decoded to gen-
erate the target sentence (Cho et al., 2014). The
simple RNN adopted Long Short Term Memory
(LSTM), which is a gated RNN used to improve
the translation quality of longer sentences. The
importance of LSTM component is to learn long
term features for encoding and decoding. Be-
sides, LSTM, other aspects that improve the per-
formance of the NMT system like the require-
ment of test-time decoding using beam search,
input feeding using attention mechanism (Luong
et al., 2015). The reason behind the massive un-
folding of the NMT system over SMT is the abil-
ity of context analysis and fluent translation (Ma-
hata et al., 2018; Pathak and Pakray, 2018; Pathak
et al., 2018).

Motivated by the merits of the NMT over other
MT systems and the importance of direct trans-
lation in between pairs of similar languages, cur-
rent work has investigated similar language pair
namely, Hindi-Nepali, for translation from Hindi
to Nepali and vice-versa using the NMT sys-
tem. Due to lack of background work of similar
language pair translation, the specific translation
work for Hindi ⇔ Nepali is still in its infancy. To
examine the efficiency of our NMT systems, the
predicted translations exposed to automatic eval-
uation using the BLEU score (Papineni et al.,
2002).

The rest of the paper is structured as follows:
Section 2, details of the system description is pre-
sented. Section 3, result and analysis are discussed
and lastly, Section 4, concludes the paper with fu-
ture scope.

2 System Description

The key steps of system architecture are data pre-
processing, system training and system testing and
same have been described in the subsequent sub-
sections. We have used OpenNMT (Klein et al.,



203

2

100

101

102

103

104

105

106

107

108

109

110

111

112

113

114

115

116

117

118

119

120

121

122

123

124

125

126

127

128

129

130

131

132

133

134

135

136

137

138

139

140

141

142

143

144

145

146

147

148

149

150

151

152

153

154

155

156

157

158

159

160

161

162

163

164

165

166

167

168

169

170

171

172

173

174

175

176

177

178

179

180

181

182

183

184

185

186

187

188

189

190

191

192

193

194

195

196

197

198

199

ACL 2019 Submission ***. Confidential Review Copy. DO NOT DISTRIBUTE.

2017) and Marian NMT (Junczys-Dowmunt et al.,
2018) toolkit to train and test the NMT system.
The OpenNMT, an open source toolkit for NMT,
which prioritizes efficiency, modularity and sup-
port significant research extensibility. Likewise,
Marian, a research-friendly tookit based on dy-
namic computation graphs written in purely C++,
which achieved high training and translation speed
for NMT.

2.1 Data Preprocessing

During the preprocessing step, source and target
sentences of raw data are tokenized using Amun
toolkit and makes a vocabulary size of dimension
66000, 50000 for Nepali-Hindi parallel sentence
pairs, which indexes the words present in the train-
ing process. All unique words are listed out in
dictionary files. The details of the data set are dis-
cussed next.
Data The NMT system has been trained us-
ing parallel source-target sentence pairs for Hindi
and Nepali, where Hindi and Nepali are the
source and target language and vice-versa. The
training corpus has been compiled manually by
back-translation using Google translator1 from the
Wikipedia source of Hindi language,2 Nepali lan-
guage,3 and source of Bible4 and as well as
dataset provided by the WMT19 organizer (Bar-
rault et al., 2019). The test data provided by the or-
ganizer for Hindi to Nepali translation consists of
1,567 number of instances and for Nepali to Hindi
translation consists of 2,000 number of instances,
have been used to check the translational effect of
the trained system. Also, validate using a subset
of training corpus containing 500 instances. The
details of the corpus statistics are shown in Table
1. The NMT system has been trained and tested in
three different configurations such as Run-1, Run-
2, and Run-3 using primary and contrastive system
type, which are summarized in Table 2 and 3.

2.2 System Training

After preprocessing the data, the source and tar-
get sentences were trained using our NMT sys-
tems for translation prediction in case of both
Hindi to Nepali and Nepali to Hindi. Our NMT
systems adopted OpenNMT and Marian NMT to
train parallel training corpora using sequence-to-

1https://translate.google.com/
2https://en.wikipedia.org/wiki/Hindi
3https://en.wikipedia.org/wiki/Nepali language
4https://www.bible.com

Figure 1: NMT System Architecture.

sequence RNN having attention mechanism. In
NMT system architecture, encoder and decoder
are the main components of the system. The en-
coder consists of a two-layer network of LSTM
units, having 500 nodes in each layer, which trans-
forms the variable length input sentence of the
source language into a fixed size summary vec-
tor. After that, a two-layer LSTM decoder hav-
ing 500 hidden units, process the summary vec-
tor (output of encoder) to generate target sentence
as output. Multiple Graphics Processing Units
(GPU) were used to increase the performance of
training. The minimum batch size is set to 2000
for memory requirements, a drop out of 0.1 and
enable layer normalization, which guarantees that
memory will not grow during training that result
in a stable training run.
NMT System with Attention Mechanism The
main disadvantage of the basic encoder-decoder
model is that it transforms the source sentence into
a fixed length vector. Therefore, there is a loss
of information in case of a long sentence. The
encoder is unable to encode all valuable informa-
tion into the summary vector. Hence, an attention
mechanism is introduced to handle such an issue.
The encoder design is the main difference between
basic encoder-decoder model and attention model.
In the attention model, a context vector is taken
as input by the decoder, unlike a summary vec-
tor in the basic encoder-decoder model. The con-
text vector is computed using convex coefficients,
are called attention weights, which measure how
much important is the source word in the genera-
tion of the current target word.

Figure 1 presents the NMT system architec-
ture, where attention mechanism and input feed-
ing are used to translate Hindi source sentence

“ ” into the Nepali target sentence
“ ” (Luong et al., 2015). Here, <
eos > marks the end of a sentence.



204

3

200

201

202

203

204

205

206

207

208

209

210

211

212

213

214

215

216

217

218

219

220

221

222

223

224

225

226

227

228

229

230

231

232

233

234

235

236

237

238

239

240

241

242

243

244

245

246

247

248

249

250

251

252

253

254

255

256

257

258

259

260

261

262

263

264

265

266

267

268

269

270

271

272

273

274

275

276

277

278

279

280

281

282

283

284

285

286

287

288

289

290

291

292

293

294

295

296

297

298

299

ACL 2019 Submission ***. Confidential Review Copy. DO NOT DISTRIBUTE.

Nature of corpus Name of Corpus/Source Number of instances
Training WMT19 Organizer 65,505

Bible + Wikipedia 1,81,368
(using Back-translation)
Total 2,46,873

Test Hindi to Nepali 1,567
Nepali to Hindi 2,000

Validation WMT19 Organizer 500

Table 1: Corpus Statistics.

Configuration Tools Training Data
(No. of instances)

Primary Marian NMT 65,505
(NMT-1): Run-1 (WMT19 Organizer)
Contrastive OpenNMT 1,33,526
(NMT-2): Run-2 (65,505: WMT19 Organizer + Bible + Wikipedia)
Contrastive Marian NMT 2,46,873
(NMT-3): Run-3 (65,505: WMT19 Organizer + Bible + Wikipedia)

Table 2: Different configuration, tools and training data used for Hindi-Nepali Translation.

Configuration Tools Training Data
(No. of instances)

Primary Marian NMT 65,505
(NMT-1): Run-1 (WMT19 Organizer)
Contrastive Marian NMT 1,33,526
(NMT-2): Run-2 (65,505: WMT19 Organizer + Bible + Wikipedia)
Contrastive OpenNMT 2,46,873
(NMT-3): Run-3 (65,505: WMT19 Organizer + Bible + Wikipedia )

Table 3: Different configuration, tools and training data used for Nepali-Hindi Translation.



205

4

300

301

302

303

304

305

306

307

308

309

310

311

312

313

314

315

316

317

318

319

320

321

322

323

324

325

326

327

328

329

330

331

332

333

334

335

336

337

338

339

340

341

342

343

344

345

346

347

348

349

350

351

352

353

354

355

356

357

358

359

360

361

362

363

364

365

366

367

368

369

370

371

372

373

374

375

376

377

378

379

380

381

382

383

384

385

386

387

388

389

390

391

392

393

394

395

396

397

398

399

ACL 2019 Submission ***. Confidential Review Copy. DO NOT DISTRIBUTE.

2.3 System Testing

During system testing phase, the trained system
is carried out on test sentences as mentioned in
Section 2.1 provided by the WMT19 organizer for
predicting translations.

3 Result and Analysis

The official results of the competition are reported
by WMT19 organizer (Barrault et al., 2019) and
the same are presented in Table 4, 5, 6 and 7 re-
spectively.

A total of six, five teams participated in Hindi
to Nepali and Nepali to Hindi translation using
primary and contrastive system type. In the pri-
mary system type of Hindi to Nepali translation,
our NMT system attained a lower BLEU score and
a higher BLEU score in Nepali to Hindi trans-
lation than other participated teams. However,
in both directions of Hindi-Nepali translation un-
der contrastive configuration our system (Mar-
ian) obtained excellent BLEU score 53.7 (Hindi
to Nepali), 49.1 (Nepali to Hindi). Moreover, it
has been observed that our system’s BLEU score
of Marian outperforms OpenNMT in both direc-
tions of Hindi-Nepali translation under contrastive
as well as primary configuration.
Analysis To analyze the best and worst perfor-
mance of our NMT system, considered the sam-
ple sentences from test data provided by the orga-
nizer and predicted target sentences on the same
test data by our NMT system and Google transla-
tor. In the case of a short, medium, long sentences
of best performance are given in Table 8, our NMT
system provides a perfect prediction like Google
translation for the given test sentences. In Table 9,
the worst case prediction sentences are presented.
In Segment Id = 136, our NMT system’s predic-
tion is wrong. The predicted target sentence is
in a different language in Segment Id = 25 and
also, in case of a long sentence as given in Seg-
ment Id = 153, the prediction is not precise. How-
ever, Google translation yields accurate prediction
in the same sentences.

Table 8: Best Performance examples in Nepali to
Hindi translation.

Table 9: Worst Performance examples in Nepali to
Hindi translation.

Moreover, the BLEU scores of the test set trans-
lated by the Google translator with the test set pro-
vided by the organizer show close to each other for
both target language Hindi and Nepali, as shown
in Table 10.

Target BLEU
Language Score
Hindi 0.405171
Nepali 0.332679

Table 10: BLEU scores of Hindi and Nepali target lan-
guage for test data and test set translation by Google
translator.



206

5

400

401

402

403

404

405

406

407

408

409

410

411

412

413

414

415

416

417

418

419

420

421

422

423

424

425

426

427

428

429

430

431

432

433

434

435

436

437

438

439

440

441

442

443

444

445

446

447

448

449

450

451

452

453

454

455

456

457

458

459

460

461

462

463

464

465

466

467

468

469

470

471

472

473

474

475

476

477

478

479

480

481

482

483

484

485

486

487

488

489

490

491

492

493

494

495

496

497

498

499

ACL 2019 Submission ***. Confidential Review Copy. DO NOT DISTRIBUTE.

Team BLEU Score Type System
Panlingua-KMI 11.5 Primary PBSMT
CMUMEA N 11.1 Primary AUGTRAN
TeamZeroGang 8.2 Primary -
NITS-CNLP 3.7 Primary NMT-1 (Marian)

Table 4: BLEU scores result of participated teams at WMT19 shared task in Hindi to Nepali translation.

Team BLEU Score Type System
NITS-CNLP 24.6 Primary NMT-1 (Marian)
CMUMEA N 12.1 Primary AUGTRAN
Panlingua-KMI 9.8 Primary PBSMT
TeamZeroGang 9.1 Primary -
CFILT IITB 2.7 Primary WITH MONOLINGUAL

Table 5: BLEU scores result of participated teams at WMT19 shared task in Nepali to Hindi translation.

Team BLEU Score Type System
NITS-CNLP 53.7 Contrastive NMT-3 (Marian)
TeamZeroGang 8.2 Contrastive -
NITS-CNLP 3.6 Contrastive NMT-2 (OpenNMT)
CFILT IITB N 3.5 Contrastive Basic

Table 6: BLEU scores result of participated teams at WMT19 shared task in Hindi to Nepali translation.

Team BLEU Score Type System
NITS-CNLP 49.1 Contrastive NMT-3 (Marian)
TeamZeroGang 9.1 Contrastive -
Panlingua-KMI 4.2 Contrastive NMT
Panlingua-KMI 3.6 Contrastive NMT-Transformer
NITS-CNLP 1.4 Contrastive NMT-2 (OpenNMT)

Table 7: BLEU scores result of participated teams at WMT19 shared task in Nepali to Hindi translation.



207

6

500

501

502

503

504

505

506

507

508

509

510

511

512

513

514

515

516

517

518

519

520

521

522

523

524

525

526

527

528

529

530

531

532

533

534

535

536

537

538

539

540

541

542

543

544

545

546

547

548

549

550

551

552

553

554

555

556

557

558

559

560

561

562

563

564

565

566

567

568

569

570

571

572

573

574

575

576

577

578

579

580

581

582

583

584

585

586

587

588

589

590

591

592

593

594

595

596

597

598

599

ACL 2019 Submission ***. Confidential Review Copy. DO NOT DISTRIBUTE.

4 Conclusion and Future Scope

In this work, our NMT systems adopted attention
mechanism to predict translation of similar lan-
guage pair namely, Hindi to Nepali and vice-versa.
In the current competition, in primary configura-
tion, our NMT system obtained BLEU score 24.6
in Nepali to Hindi translation and BLEU score
3.7 in Hindi to Nepali translation. On the other
hand, in contrastive configuration, our NMT sys-
tem acquired BLEU score 53.7 (Hindi to Nepali),
49.1 (Nepali to Hindi). However, close analysis
of generated target sentences on given test sen-
tences remarks that our NMT systems need to im-
prove in case of wrong translation, translation in a
different language. Moreover, BLEU scores pre-
sented in Table 10, pointed out that is case of both
target language Hindi and Nepali, the scores are
in relatively stable in both directions of Hindi-
Nepali translation like our systems (both Marian
and OpenNMT) in contrastive configuration (as
mentioned in Table 6 and 7) but unlike in primary
configuration (Marian) (as mentioned in Table 4
and 5). Hence, more experiments and compar-
ative analysis will be needed in future work to
reason about Marian outperforms OpenNMT in
both directions i.e. Hindi to Nepali and Nepali to
Hindi translation. In the future work, more num-
ber of instances in Hindi-Nepali pair, different In-
dian similar language pair like Bengali-Assamese,
Telugu-Kannada, Hindi-Punjabi, shall be consid-
ered for machine translation, which may be possi-
ble to overcome the limitation of available parallel
data to produce precise MT output.

Acknowledgement

Authors would like to thank WMT19 Shared task
organizers for organizing this competition and
also, thank Centre for Natural Language Process-
ing (CNLP) and Department of Computer Science
and Engineering at National Institute of Technol-
ogy, Silchar for providing the requisite support
and infrastructure to execute this work.

References
Loı̈c Barrault, Ondřej Bojar, Marta R. Costa-jussà,

Christian Federmann, Mark Fishel, Yvette Gra-
ham, Barry Haddow, Matthias Huck, Philipp Koehn,
Shervin Malmasi, Christof Monz, Mathias Müller,
Santanu Pal, Matt Post, and Marcos Zampieri. 2019.
Findings of the 2019 conference on machine trans-
lation (wmt19). In Proceedings of the Fourth Con-

ference on Machine Translation, Volume 2: Shared
Task Papers, Florence, Italy. Association for Com-
putational Linguistics.

Kyunghyun Cho, Bart van Merrienboer, Caglar Gul-
cehre, Dzmitry Bahdanau, Fethi Bougares, Holger
Schwenk, and Yoshua Bengio. 2014. Learning
phrase representations using rnn encoder–decoder
for statistical machine translation. In Proceedings of
the 2014 Conference on Empirical Methods in Nat-
ural Language Processing (EMNLP), pages 1724–
1734, Doha, Qatar. Association for Computational
Linguistics.

Shachi Dave, Jignashu Parikh, and Pushpak Bhat-
tacharyya. 2001. Interlingua-based english–hindi
machine translation and language divergence. Ma-
chine Translation, 16(4):251–304.

Marcin Junczys-Dowmunt, Roman Grundkiewicz,
Tomasz Dwojak, Hieu Hoang, Kenneth Heafield,
Tom Neckermann, Frank Seide, Ulrich Germann,
Alham Fikri Aji, Nikolay Bogoychev, André F. T.
Martins, and Alexandra Birch. 2018. Marian: Fast
neural machine translation in C++. In Proceedings
of ACL 2018, System Demonstrations, pages 116–
121, Melbourne, Australia. Association for Compu-
tational Linguistics.

Guillaume Klein, Yoon Kim, Yuntian Deng, Jean
Senellart, and Alexander Rush. 2017. Opennmt:
Open-source toolkit for neural machine translation.
In Proceedings of ACL 2017, System Demonstra-
tions, pages 67–72, Vancouver, Canada. Association
for Computational Linguistics.

Thang Luong, Hieu Pham, and Christopher D. Man-
ning. 2015. Effective approaches to attention-based
neural machine translation. In Proceedings of the
2015 Conference on Empirical Methods in Natu-
ral Language Processing, pages 1412–1421, Lis-
bon, Portugal. Association for Computational Lin-
guistics.

Sainik Kumar Mahata, Dipankar Das, and Sivaji
Bandyopadhyay. 2018. Mtil2017: Machine trans-
lation using recurrent neural network on statistical
machine translation. Journal of Intelligent Systems,
pages 1–7.

Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: A method for automatic eval-
uation of machine translation. In Proceedings of
the 40th Annual Meeting on Association for Com-
putational Linguistics, ACL ’02, pages 311–318,
Stroudsburg, PA, USA. Association for Computa-
tional Linguistics.

Amarnath Pathak and Partha Pakray. 2018. Neural ma-
chine translation for indian languages. Journal of
Intelligent Systems, pages 1–13.

Amarnath Pathak, Partha Pakray, and Jereemi Ben-
tham. 2018. English–mizo machine translation us-
ing neural and statistical approaches. Neural Com-
puting and Applications, 30:1–17.

https://doi.org/10.3115/v1/D14-1179
https://doi.org/10.3115/v1/D14-1179
https://doi.org/10.3115/v1/D14-1179
https://doi.org/10.1023/A:1021902704523
https://doi.org/10.1023/A:1021902704523
http://www.aclweb.org/anthology/P18-4020
http://www.aclweb.org/anthology/P18-4020
https://www.aclweb.org/anthology/P17-4012
https://www.aclweb.org/anthology/P17-4012
https://doi.org/10.18653/v1/D15-1166
https://doi.org/10.18653/v1/D15-1166
https://doi.org/10.1515/jisys-2018-0016
https://doi.org/10.1515/jisys-2018-0016
https://doi.org/10.1515/jisys-2018-0016
https://doi.org/10.3115/1073083.1073135
https://doi.org/10.3115/1073083.1073135
https://doi.org/10.1515/jisys-2018-0065
https://doi.org/10.1515/jisys-2018-0065
https://doi.org/10.1007/s00521-018-3601-3
https://doi.org/10.1007/s00521-018-3601-3

