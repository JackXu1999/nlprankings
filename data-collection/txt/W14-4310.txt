Philadelphia, U.S.A., 18-20 June 2014. c(cid:13)2014 Association for Computational Linguistics

Proceedings of the SIGDIAL 2014 Conference, pages 74–78,

74

User Modeling by Using Bag-of-Behaviors for Building a Dialog System

Sensitive to the Interlocutor’s Internal State

Yuya Chiba, Takashi Nose, Akinori Ito

Graduate School of Engineering,

Tohoku University, Japan

Masashi Ito

Faculty of Engineering

Tohoku Institute of Technology, Japan

Abstract

When using spoken dialog systems in ac-
tual environments, users sometimes aban-
don the dialog without making any in-
put utterance. To help these users before
they give up, the system should know why
they could not make an utterance. Thus,
we have examined a method to estimate
the state of a dialog user by capturing the
user’s non-verbal behavior even when the
user’s utterance is not observed. The pro-
posed method is based on vector quan-
tization of multi-modal features such as
non-verbal speech, feature points of the
face, and gaze. The histogram of the VQ
code is used as a feature for determining
the state. We call this feature “the Bag-
of-Behaviors.” According to the experi-
mental results, we prove that the proposed
method surpassed the results of conven-
tional approaches and discriminated the
target user’s states with an accuracy of
more than 70%.
1 Introduction
Spoken dialog systems have an advantage of be-
ing a natural interface since speech commands are
less subject to the physical constraints imposed by
devices. On the other hand, if the system accepts
only a limited expression, the user need to learn
how to use the system. If the user is not familiar
with the system, he/she cannot even make an in-
put utterance. Not all users are motivated to con-
verse with the system in actual environments, and
sometimes a user will abandon the dialog with-
out making any input utterance. When the user
has difﬁculty to make the utterance, conventional
systems just repeat the prompt at ﬁxed interval
(Yankelovich, 1996) or taking the initiative in the
dialog to complete the task (Chung, 2004; Bo-
hus and Rudnicky, 2009). However, we think that
the system has to cope with the user’s implicit re-
quests to help the user more adequately. To solve
this problem, Chiba and Ito (2012) proposed a

method to estimate two “user’s states” by captur-
ing their non-verbal cues. Here, the state A is
when the user does not know what to input, and
the state B is when the user is considering how to
answer the system’s prompt. These states have not
been distinguished by the conventional dialog sys-
tems so far, but should be handled differently.

The researchers of spoken dialog systems have
focused on the various internal states of users
such as emotion (Forbes-Riley and Litman, 2011a;
Metallinou et al., 2012), preference (Pargellis et
al., 2004) and familiarity with the system (Jokinen
and Kanto, 2004; Rosis et al., 2006) to build natu-
ral dialog system. In particular, the user’s “uncer-
tainty” is assumed to be the nearest user’s states
that we wish to study. Forbes-Riley and Litman
(2011b) and Pon-Barry et al. (2005) introduced a
framework for estimating the user’s uncertainty to
a tutor system.

The above-mentioned researches have a cer-
tain result by employing linguistic information
for the estimation, but it remains difﬁcult to as-
sist a user who does not make any input utter-
ance. By contrast, the method by Chiba and Ito
(2012) estimated the target user’s state by only
using the user’s non-verbal information. In their
work, the user’s multi-modal behaviors were de-
ﬁned empirically, and the labels of the behaviors
were annotated manually. Based on this result, the
present paper proposes the method that does not
use manually-deﬁned labels nor manual annota-
tion. The multi-modal behaviors are determined
automatically using the vector quantization, and
the frequency distribution of the VQ code is used
for estimation of the user’s state. Because this ap-
proach expects to construct clusters of the speech
events or behaviors of the user, we called it as Bag-
of-Behaviors approach.

2 Data collection

The experimental data (video clips) were the same
as those used in the experiment by Chiba et al.
(Chiba and Ito, 2012; Chiba et al., 2012). The
video clips contained the frontal image of the user

75

and their speech, which were recorded with a web
camera and a lapel microphone, respectively. The
task of the dialog was a question-and-answer task
to ask users to answer common knowledge or
a number they remembered in advance, such as
“Please input your ID.” 16 users (14 males and 2
females) participated in the dialog collection.

Recorded clips were divided into sessions,
where one session included one interchange of the
system’s prompt and the user’s response. The total
number of sessions was 792. Then we employed
evaluators to label each video clip as either state A,
B or C, where state A and B were that described in
the previous section, and state C is the state where
the user had no problem answering the system. We
took the majority vote of the evaluators’ decisions
to determine the ﬁnal label of a clip. Fleiss’ κ
among the evaluators was 0.22 (fair agreement).
Finally, we obtained 59, 195 and 538 sessions of
state A, B and C, respectively.

3 Discrimination method by using

Bag-of-Behaviors

In the work of Chiba et al. (2013),
the user’s
state was determined using the labels of the multi-
modal events such as ﬁllers or face orientation,
which were estimated from the low-level acoustic
and visual features.

Here, inventory of multi-modal events was de-
termined empirically. There were, however, two
problems with this method. The ﬁrst one was that
the optimality of the inventory was not guaran-
teed. The second one is that it was difﬁcult to esti-
mate the events from the low-level features, which
made the ﬁnal decision more difﬁcult. Therefore,
we propose a new method for discriminating the
user’s state using automatically-determined events
obtained by the vector quantization.

First, a codebook of the low-level features
(which will be described in detail in the next
section) is created using k-means++ algorithm
(Arthur and Vassilvitskii, 2007). Let a low-level
feature vector at time t of session s of the training
data be x(s)
. Then we perform the clustering of
the low-level feature vectors for all of t and s, and
create a codebook C = {c1, . . . , cK}, where ck
denotes the k-th centroid of the codebook.

t

Then the input feature vectors are quantized
frame-by-frame using the codebook. When a ses-
sion for evaluation sE is given, we quantize the in-
put low-level feature vectors x(sE )
into
q1, . . . , qT , where

, . . . , x(sE )

T

1

qt = arg min

q

||x(sE )

t − cq||.

(1)

Then we calculate the histogram Q0(sE) =
(Q1, . . . , QK) where

Qk =

δ(k, qt)

(2)

T∑
{

t=1

1 x = y
0 x ̸= y

δ(x, y) =

(3)
Then Q(sE) = Q0(sE)/||Q0(sE)|| is used as
the feature of the discrimination. The similar fea-
tures based on the vector quantization were used
for image detection and scene analysis (Csurka
et al., 2004; Jiang et al., 2007; Natarajan et al.,
2012) and called “Bag-of-Features” or “Bag-of-
Keypoints.” In our research, each cluster of the
low-level features is expected to represent some
kind of user’s behavior. Therefore, we call the pro-
posed method the “Bag-of-Behaviors” approach.
After calculating the Bag-of-Behaviors, we em-
ploy an appropriate classiﬁer to determine the
user’s state in the given session. In this research,
the support vector machine (SVM) is used as a
classiﬁer.
4 The low-level features
In this section, we describe the acoustic and visual
features employed as the low-level features.

The target user’s states are assumed to have sim-
ilar aspects to emotion. Collignon et al. (2008)
suggested that emotion has a multi-modality na-
ture. For example, W¨ollmer et al. (2013) showed
that the acoustic and visual features contributed to
discriminate arousal and expectation, respectively.
Several other researches also have reported that
recognition accuracy of emotion was improved by
combining multi-modal information (Lin et al.,
2012; Wang and Venetsanopoulos, 2012; Paul-
mann and Pell, 2011; Metallinou et al., 2012).
Therefore, we employed similar features as those
used in these previous works, such as the spectral
features and intonation of the speech, and facial
feature points, etc.
4.1 Audio features
To represent spectral characteristics of the speech,
MFCC was employed as an acoustic feature. We
used a 39-dimension MFCC including the veloc-
ity and acceleration of the lower 12th-order coef-
ﬁcients and log power. In addition, a differential
component of log F 0 was used to represent the
prosodic feature of the speech, and zero cross (ZC)
was used to distinguish voiced and unvoiced seg-
ments. Therefore, total number of audio features
was 3. The basic conditions for extracting each
feature are shown in Table 1. Here, ﬁve frames

76

(the current frame, the two previous frames and
two following frames) were used to calculate the
∆ and ∆∆ components of MFCC and ∆ compo-
nent of log F 0.

4.2 Face feature
Face feature (Chiba et al., 2013) was extracted by
the Constraint Local Model (CLM) (Saragih et al.,
2011) frame by frame. The coordinates of the
points relative to the center of the face were used
as the face features. The scale of the feature points
was normalized by the size of the facial region.
The number of feature points was 66 and the di-
mension of the feature was 132.

4.3 Gaze feature
The evaluators of the dialogs declared that move-
ment of the user’s eyes seems to express their in-
ternal state. The present paper used the Haar-
like feature which has a fast calculation algo-
rithm using the integral image to represent the
brightness of the user’s eye regions. This feature
was extracted by applying ﬁlters comprehensively
changed the size and location to the image (eye
regions in our case). The eye regions were de-
tected by the facial feature points. Because this
feature had large dimensions, the principal com-
ponent analysis (PCA) was conducted to reduce
the dimensionality. Finally, gaze feature had 34 di-
mensions and the cumulative contribution rate was
about 95%.

4.4 Feature synchronization
The audio features were calculated every 10 ms
(see Table 1) while the visual features were ex-
tracted every 33 ms. Therefore, the features were
synchronized by copying the visual features of the
previous frame in every 10 ms.

5 Discrimination examination

5.1 Conditions of the Bag-of-Behaviors

construction

We built the Bag-of-Behaviors under two condi-
tions described below.

Let x(s)

at , x(s)

f t and x(s)

represent the audio fea-
ture, face feature and gaze feature of the session s
at time t, respectively.

et

Table 1: Conditions of audio feature extraction

Frame width
Frame shift

MFCC
25.0 ms
10.0 ms

log F 0
17.0 ms
10.0 ms

ZC

10.0 ms
10.0 ms

Table 2: Experimental conditions
# of sessions

Codebook size K
Ka
Kf
Ke

State A(59), State B(195)
4, 8, 16, 32, 64
4, 8, 16, 32, 64
4, 8, 16, 32, 64
4, 8, 16, 32, 64

In Condition (1), the three features are com-

bined to single feature vector x(s)

t

:

x(s)
t = (x(s)

at , x(s)

f t , x(s)
et )

(4)

t

t

Then, the low-level feature vectors x(s)
are clus-
tered to construct one codebook C with size K.
When an input session sE is given, we calculate
the combined feature vector x(sE )
, and generate
the Bag-of-Behaviors Q(sE). This method is a
kind of the feature-level fusion method.
In Condition (2), the three features are used sep-
arately. First, we generate three codebooks Ca,Cf
and Ce using the audio, face and gaze features, re-
spectively. Size of those codebooks were Ka, Kf
and Ke. When an input session sE is given,
we generate three Bag-of-Behaviors feature vec-
tors Qa(sE), Qf (sE) and Qe(sE) using the three
codebooks. Finally, we combine those features as

Q(sE) = (Qa(sE), Qf (sE), Qe(sE)).

(5)

5.2 Experimental condition
We employed the SVM with RBF-kernel as a clas-
siﬁer. The experimental conditions are summa-
rized in Table 2. The hyperparameters of the clas-
siﬁer were decided by grid-searching. Since the
session of state C and the other states (state A and
state B) were clearly distinguished by the duration
of the session, we used only the session of state
A and state B for the experiments. Hence, each
experiment was a two-class discrimination task.

As explained, the experimental data were un-
balanced. Since it is desirable that the system can
discriminate the user’s state without deviation, the
harmonic mean H of the accuracy of the two states
was used for measuring the performance. This is
calculated by

H =

2CACB
CA + CB

,

(6)

where CA and CB represent the discrimination ac-
curacy of state A and state B, respectively. The ex-
periments were conducted based on a 5-fold cross
validation.

77

used as input for a neural network for the classi-
ﬁcation. The gaze feature was not used in “Base-
line + NN.” We added the result when including
the gaze feature, shown as “Baseline + Gaze +
NN.” As shown in Table 3, the performance of the
method proposed in this paper surpassed the base-
line methods. Therefore, the proposed method
could not only automatically determine the inven-
tory of the audio-visual events, but also achieved
better discrimination accuracy. One of the reasons
of the improvement is VQ can construct the clus-
ters in proper quantities.

Comparing the two conditions of feature combi-
nation, H of condition (2) (denoted as “Condition
(2) + RBF-SVM”) was slightly higher than that of
condition (1) (denoted as “Condition (1) + RBF-
SVM”). This result was similar to Split-VQ (Pari-
wal and Atal, 1991) where a single feature vec-
tor split into subvectors and the input vector was
quantized subvector by subvector.

We conducted additional experiments for con-
dition (2) by using SVM with combined kernel
trained by Multiple Kernel Learning (MKL) (Son-
nenburg et al., 2006). The combined kernel is rep-
resented as a linear combination of several sub-
kernels. The distinct kernel was employed for
the speech, face feature and gaze feature, respec-
tively. This paper used the RBF-kernel having the
same width as the sub-kernels．The best result was
shown as “Condition (2) + MKL-SVM” in Table 3.
As shown in the table, the MKL-SVM showed the
highest performance of 72.0 %. The weights of the
audio, face and gaze feature were 0.246, 0.005 and
0.749, respectively. This result suggested that the
contribution of the face feature was weaker than
the other features.
6 Conclusion
In this paper, we proposed a method to estimate
the state of the user of the dialog system by us-
ing non-verbal features. We proposed the Bag-
of-Behaviors approach, in which the user’s mult-
modal behavior was ﬁrst classiﬁed by vector quan-
tization, and then the histogram of the VQ code
was used as a feature of the discrimination. We
veriﬁed that the method could discriminate the tar-
get user’s state with an accuracy of 70% or more.
One of the disadvantages of the current frame-
work is that it requires to observe the session until
just before the user’s input utterance. This prob-
lem makes it difﬁcult to apply this method to an
actual system, because the system has to be able
to evaluate the user’s state successively in order to
help the user at an appropriate timing. Therefore,
we will examine a sequential estimation method
by using the Bag-of-Behaviors in a future work.

Figure 1: Discrimination results of condition (1)

Figure 2: Discrimination results of condition (2)
arranged in descending order

5.3 Experimental results
The results of condition (1) are shown in Figure
1. The ﬁgure shows the best H of each num-
ber of clusters.
In condition (1), the best result
(H = 70.0%) was obtained when the number of
clusters K was 64. Figure 2 shows the results of
condition (2). In this ﬁgure, the results are shown
in descending order of the harmonic mean for all
combination of codebook size of the three code-
books (there were 53 = 125 conditions). The best
H = 70.7% was obtained when Ka = 8, Kf = 8
and Ke = 64.

The best results of the tested methods are sum-
marized in Table 3. Here, “Baseline + NN” in
the table denotes the result in Chiba et al. (2013),
where the visual events and acoustic events were
annotated manually, and the manual labels were

Table 3: Comparison of estimation methods
State A State B Harm.
58.2
61.9
70.0
70.7
72.0

52.5
Baseline + NN
Baseline + Gaze + NN
64.5
Condition (1) + RBF-SVM 67.9
Condition (2) + RBF-SVM 67.7
Condition (2) + MKL-SVM 68.0

65.1
59.5
72.3
73.8
76.4

]

%

[
 
t
l
u
s
e
r
 
n
o
i
t
a
n
m

i

i
r
c
s
i
D

90

80

70

60

50

40

30

State A(CA )
State B(CB )
Harm.(H)

4

8
32
Number of clusters K

16

64

]

%

[
 
)

H

(
 
t
l
u
s
e
r
 
n
o
i
t
a
n
m

i

i
r
c
s
i
D

75

70

65

60

55

50
0

Harm.(H)

20

40

60

Order of H

80

100

120

78

Pradeep Natarajan, Shuang Wu, Shiv Vitaladevuni, Xiao-
dan Zhuang, Stavros Tsakalidis, and Unsang Park, Ro-
hit Prasad, and Premkumar Natarajan. 2012. Multimodal
feature fusion for robust event detection in web videos.
In Proc. Computer Vision and Pattern Recognition, pages
1298–1305.

Andrew Pargellis, Hong-Kwang Jeff Kuo, and Chin-Hui Lee.
2004. An automatic dialogue generation platform for per-
sonalized dialogue applications. Speech Communication,
42:329–351.

Kuldip Paliwal and Bishnu Atal. 1993. Efﬁcient vector quan-
tization of lpc parameters at 24 bits/frame. In IEEE Trans.
Speech and Audio Processing, 1(1):3–14.

Silke Paulmann and Marc Pell. 2011. Is there an advantage
for recognizing multi-modal emotional stimuli? Motiva-
tion and Emotion, 35(2):192–201.

Heather Pon-Barry, Karl Schultz, Elizabeth Owen Bratt,
Brady Clark, and Stanley Peters. 2005. Responding to
student uncertainty in spoken tutorial dialogue systems.
Int. J. Artif. Intell. Edu., 16:171–194.

Fiorella Rosis, Nicole Novielli, Valeria Caroﬁglio, Addo-
lorata Cavalluzzi, and Berardina Carolis. 2006. User
modeling and adaptation in health promotion dialogs
with an animated character. J. Biomedical Informatics,
39:514–531.

Jason Saragih, Simon Lucey, and Jeffrey Cohn. 2011. De-
formable model ﬁtting by regularized landmark mean-
shift. Int. J. Computer Vision, 91(2):200–215.

Yongjin Wang and Anastasios Venetsanopoulos. 2012. Ker-
nel cross-modal factor analysis for information fusion
with application to bimodal emotion recognition.
IEEE
Trans. Multimedia, 14(3):597–607.

Martin W¨ollmer, Moritz Kaiser,

Eyben,
Bj¨orn Schuller, and Gerhard Rigoll.
Lstm-
modeling of continuous emotions in an audiovisual affect
recognition framework.
Image and Vision Computing,
31(2):153–163.

Florian
2013.

Nicole Yankelovich. 1996. How do users know what to say?

Interactions, 3(6):32–43.

References
David Arthur and Sergei Vassilvitskii. 2007. k-means++:
The advantages of careful seeding.
In Proc. the 18th
annual ACM-SIAM symposium on Discrete algorithms,
pages 1027–1035.

Dan Bohus and Alexander I. Rudnicky. 2009. The raven-
claw dialog management framework: Architecture and
systems. Computer Speech & Language, 23(3):332–361.

Yuya Chiba and Akinori

Estimating a
2012.
input utterance.
user’s internal state before the ﬁrst
Advances in Human-Computer Interaction, 2012:11,
DOI:10.1155/2012/865362, 2012.

Ito.

Yuya Chiba, Masashi Ito, and Akinori Ito. 2012. Effect of
linguistic contents on human estimation of internal state
of dialog system users.
In Proc. Feedback Behaviors in
Dialog, pages 11–14.

Yuya Chiba, Masashi Ito, and Akinori Ito. 2013. Estima-
tion of user’s state during a dialog turn with sequential
multi-modal features. In HCI International 2013-Posters’
Extended Abstracts, pages 572–576.

Grace Chung. 2004. Developing a ﬂexible spoken dialog
system using simulation. In Proc. the 42nd Annual Meet-
ing on Association for Computational Linguistics, pages
63–70.

Olivier Collignon, Simon Girard, Frederic Gosselin, Syl-
vain Roy, Dave Saint-Amour, Maryse Lassonde, and Lep-
ore Franco. 2008. Audio-visual integration of emotion
expression. Brain research, 1242:126–135.

Gabriella Csurka,

Christopher Dance,

Fan,
Jutta Willamowski, and C´edric Bray.
2004. Visual
categorization with bags of keypoints. In Proc. workshop
on statistical learning in computer vision, ECCV, pages
1–2.

Lixin

Kate Forbes-Riley and Diane Litman. 2011a. Beneﬁts and
challenges of real-time uncertainty detection and adapta-
tion in a spoken dialogue computer tutor. Speech Commu-
nication, 53:1115–1136.

Kate Forbes-Riley and Diane Litman. 2011b. Designing
and evaluating a wizarded uncertainty-adaptive spoken di-
alogue tutoring system. Computer Speech & Language,
25(1):105–126.

Yu-Gang Jiang, Chong-Wah Ngo, and Jun Yang. 2007. To-
wards optimal bag-of-features for object categorization
and semantic video retrieval.
In Proc. of the 6th ACM
international conference on Image and video retrieval,
pages 494–501.

Kristiina Jokinen and Kari Kanto. 2004. User expertise mod-
elling and adaptivity in a speech-based e-mail system. In
Proc. the 42nd Annual Meeting on Association for Com-
putational Linguistics, pages 88–95.

Jen-Chun Lin, Chung-Hsien Wu, and Wen-Li Wei. 2012.
Error weighted semi-coupled hidden markov model for
audio-visual emotion recognition. IEEE Trans. Multime-
dia, 14(1):142–156.

Florian Eyben,

Angeliki Metallinou, Martin W¨ollmer, Athanasios Kat-
and
samanis,
Shrikanth Narayanan. 2012. Context-sensitive learning
for enhanced audiovisual emotion classiﬁcation.
IEEE
Trans. Affective Computing, 3(2):184–198.

Schuller,

Bj¨orn

