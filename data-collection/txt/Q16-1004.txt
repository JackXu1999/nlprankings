








































Detecting Cross-Cultural Differences Using a Multilingual Topic Model

E.D. Gutiérrez1 Ekaterina Shutova2 Patricia Lichtenstein3
Gerard de Melo4 Luca Gilardi5
1 University of California, San Diego

2 Computer Laboratory, University of Cambridge
3 University of California, Merced

4 IIIS, Tsinghua University, 5 ICSI, Berkeley
edg@icsi.berkeley.edu es407@cam.ac.uk tricia1@uchicago.edu

gdm@demelo.org lucag@icsi.berkeley.edu

Abstract

Understanding cross-cultural differences has
important implications for world affairs and
many aspects of the life of society. Yet, the
majority of text-mining methods to date focus
on the analysis of monolingual texts. In con-
trast, we present a statistical model that simul-
taneously learns a set of common topics from
multilingual, non-parallel data and automati-
cally discovers the differences in perspectives
on these topics across linguistic communities.
We perform a behavioural evaluation of a sub-
set of the differences identified by our model
in English and Spanish to investigate their psy-
chological validity.

1 Introduction

Recent years have seen a growing interest in text-
mining applications aimed at uncovering public
opinions and social trends (Fader et al., 2007; Mon-
roe et al., 2008; Gerrish and Blei, 2011; Pennac-
chiotti and Popescu, 2011). They rest on the as-
sumption that the language we use is indicative of
our underlying worldviews. Research in cognitive
and sociolinguistics suggests that linguistic varia-
tion across communities systematically reflects dif-
ferences in their cultural and moral models and
goes beyond lexicon and grammar (Kövecses, 2004;
Lakoff and Wehling, 2012). Cross-cultural differ-
ences manifest themselves in text in a multitude of
ways, most prominently through the use of explicit
opinion vocabulary with respect to a certain topic
(e.g. “policies that benefit the poor”), idiomatic and
metaphorical language (e.g. “the company is spin-
ning its wheels”) and other types of figurative lan-
guage, such as irony or sarcasm.

The connection between language, culture and
reasoning remains one of the central research ques-
tions in psychology. Thibodeau and Borodit-
sky (2011) investigated how metaphors affect our
decision-making. They presented two groups of hu-
man subjects with two different texts about crime.
In the first text, crime was metaphorically portrayed
as a virus and in the second as a beast. The two
groups were then asked a set of questions on how
to tackle crime in the city. As a result, while the
first group tended to opt for preventive measures
(e.g. stronger social policies), the second group
converged on punishment- or restraint-oriented mea-
sures. According to Thibodeau and Boroditsky, their
results demonstrate that metaphors have profound
influence on how we conceptualize and act with re-
spect to societal issues. This suggests that in order to
gain a full understanding of social trends across pop-
ulations, one needs to identify subtle but systematic
linguistic differences that stem from the groups’ cul-
tural backgrounds, expressed both literally and fig-
uratively. Performing such an analysis by hand is
labor-intensive and often impractical, particularly in
a multilingual setting where expertise in all of the
languages of interest may be rare.

With the rise of blogging and social media, NLP
techniques have been successfully used for a number
of tasks in political science, including automatically
estimating the influence of particular politicians in
the US senate (Fader et al., 2007), identifying lex-
ical features that differentiate political rhetoric of
opposing parties (Monroe et al., 2008), predicting
voting patterns of politicians based on their use of
language (Gerrish and Blei, 2011), and predicting
political affiliation of Twitter users (Pennacchiotti
and Popescu, 2011). Fang et al. (2012) addressed

47

Transactions of the Association for Computational Linguistics, vol. 4, pp. 47–60, 2016. Action Editor: David Chiang.
Submission batch: 11/2015; Published 2/2016.

c©2016 Association for Computational Linguistics. Distributed under a CC-BY 4.0 license.



the problem of automatically detecting and visual-
ising the contrasting perspectives on a set of top-
ics attested in multiple distinct corpora. While suc-
cessful in their tasks, all of these approaches fo-
cused on monolingual data and did not reach be-
yond literal language. In contrast, we present a
method that detects fine-grained cross-cultural dif-
ferences from multilingual data, where such differ-
ences abound, expressed both literally and figura-
tively. Our method brings together opinion mining
and cross-lingual topic modelling techniques for this
purpose. Previous approaches to cross-lingual topic
modelling (Boyd-Graber and Blei, 2009; Jagarla-
mudi and Daumé III, 2010) addressed the problem
of mining common topics from multilingual cor-
pora. We present a model that learns such com-
mon topics, while simultaneously identifying lexi-
cal features that are indicative of the underlying dif-
ferences in perspectives on these topics by speakers
of English, Spanish and Russian. These differences
are mined from multilingual, non-parallel datasets of
Twitter and news data. In contrast to previous work,
our model does not merely output a list of mono-
lingual lexical features for manual comparison, but
also automatically infers multilingual contrasts.

Our system (1) uses word-document co-occur-
rence data as input, where the words are labeled
as topic words or perspective words; (2) finds the
highest-likelihood dictionary between topic words
in the two languages given the co-occurrence data;
(3) finds cross-lingual topics specified by distribu-
tions over topic-words and perspective-words; and
(4) automatically detects differences in perspective-
word distributions in the two languages. We perform
a behavioural evaluation of a subset of the differ-
ences identified by the model and demonstrate their
psychological validity. Our data and dictionaries are
available from the first author upon request.

2 Related work
View detection. Identifying different viewpoints
is related to the well-studied area of subjectivity
detection, which aims at exposing opinion, evalu-
ation, and speculation in text (Wiebe et al., 2004)
and attributing it to specific people (Awadallah et al.,
2011; Abu-Jbara et al., 2012). In our work, we are
less interested in explicit local forms of subjectivity,
instead aiming at detecting more general contrasts

across linguistic communities.
Another line of research has focused on inferring

author attributes such as gender, age (Garera and
Yarowsky, 2009), location (Jones et al., 2007), or po-
litical affiliation (Pennacchiotti and Popescu, 2011).
Such studies make use of syntactic style, discourse
characteristics, as well as lexical choice. The models
used for this are typically binary classifiers trained
in a fully supervised fashion. In contrast, in our
task, we automatically infer the topic distributions
and find topic-specific contrasts.

Probabilistic topic models. Probabilistic topic
models have proven useful for a variety of seman-
tic tasks, such as selectional-preference induction
(Ó Séaghdha, 2010; Ritter et al., 2010), sentiment
analysis (Boyd-Graber and Resnik, 2010) and study-
ing the evolution of concepts and ideas (Hall et al.,
2008). The goal of a topic model is to character-
ize observed data in terms of a much smaller set
of unobserved, semantically coherent topics. A par-
ticularly popular probabilistic topic model is Latent
Dirichlet Allocation (LDA) (Blei et al., 2003). Un-
der its assumptions, each document has a unique mix
of topics, and each topic is a distribution over terms
in the vocabulary. A topic is chosen for every word
token according to the topic mix of the document
to which it belongs, and then the word’s identity is
drawn from the corresponding topic’s distribution.

Handling multilingual corpora. LDA is de-
signed for monolingual text and thus it lacks the
structure necessary to model cross-lingually valid
topics. While topic models can be trained indi-
vidually on two languages and then the acquired
topics can be matched, the correspondences be-
tween the topics for the two terms will be highly
unstable. To address this, Boyd-Graber and Blei
(2009) (MUTO) and Jagarlamudi and Daumé III
(2010) (JOINTLDA) introduced the notion of cross-
lingually valid concepts associated with different
terms in different languages, using bilingual dictio-
naries to model topics across languages. Based on
a model by Haghighi et al. (2008), MUTO is capa-
ble of learning translations–i.e., matching between
terms in the different languages being compared.
The Polylingual Topic Model of Mimno et al. (2009)
is another approach to finding topics in multilingual
corpora, but it requires tuples composed of compa-

48



rable documents in each language of the corpus.

Topic models for view detection. LDA also as-
sumes that the distribution of each topic is fixed
across all documents in a corpus. Therefore, a topic
associated with, e.g., war will have the same dis-
tribution over the lexicon regardless of whether the
document was taken from a pro-war editorial or an
anti-war speech. However, in reality we may expect
a single topic to exhibit systematic and predictable
variations in its distribution based on authorship.

The cross-collection LDA model by Paul and
Girju (2009) addresses this by specifically aiming to
expose viewpoint differences across different doc-
ument collections. Ahmed and Xing (2010) pro-
posed a similar model for detecting ideological dif-
ferences. Fang et al. (2012)’s Cross-Perspective
Topic (CPT) model breaks up the terms in the vo-
cabulary into topic terms and perspective terms with
different generative processes, and differentiates be-
tween different collections of documents within the
corpus. The topic terms are assumed to be generated
as in LDA. However, the distribution of perspective
terms in a document is taken to be dependent on both
the topic mixture of the document as well as the col-
lection from which the document is drawn.

Recent works proposed models for specific types
of data. Qiu and Jiang (2013) use user identities
and interactions in threaded discussions, while Got-
tipati et al. (2013) developed a topic model for De-
batepedia, a semi-structured resource in which ar-
guments are explicitly enumerated. However, all of
these models perform their analyses on monolingual
datasets. Thus, they are useful for comparing differ-
ent ideologies expressed in the same language, but
not for cross-linguistic comparisons.

3 Method

The goal of our model is to analyse large, non-
parallel, multilingual corpora and present cross-
lingually valid topics and the associated perspec-
tives, automatically inferring the differences in con-
ceptualization of these topics across cultures. Fol-
lowing Boyd-Graber and Blei (2009) and Jagarla-
mudi and Daumé III (2010), our distributions of la-
tent topics range over latent, cross-lingual topic con-
cepts that manifest themselves as language-specific
topic words. We use bilingual dictionaries, contain-

Figure 1: Basic generative model.

ing words in one language and their translations in
another language, to represent the topic concepts.
These are represented as a bipartite graph, with each
translation entry being an edge and each topic word
in the two languages being a vertex. While the topic
words are tied together by the translation dictionary,
the perspective words can vary freely across lan-
guages. Following Fang et al. (2012), we treat nouns
as topic words and verbs and adjectives as perspec-
tive words1. The model assumes that adjective and
verb tokens in each document are assigned to topics
in proportion to the topic assignments of the topic
word tokens. Then, the perspective term for this
topic is drawn depending on the topic assignment
and the language of the speaker.

3.1 Basic Generative Model

Given the languages ` ∈ {a, b}, our model infers the
distributions of multi-lingual topics and language-
specific perspective-words (Fig. 2), as follows:

1. Draw a set C of concepts (u, v) matching topic
word u from language a to topic word v from lan-
guage b, where the probability of concept (u, v) is
proportional to a prior πu,v (e.g. based on informa-
tion from a translation dictionary).

2. Draw multinomial distributions:

1This approximation was adopted for convenience, compu-
tational efficiency and ease of interpretation. However, in prin-
ciple our method does not depend on it, since it can be applied
with all content words as topic or perspective words.

49



• For topic indices k ∈ {1, ...,K}, draw
language-independent topic-concept distribu-
tions φwk ∼ Dir(βw) over pairs (wa, wb) ∈ C.

• For topic indices k ∈ {1, ...,K} and lan-
guages ` ∈ {a, b}, draw language-specific
perspective-term distributions φ`,ok ∼ Dir(βo)
over perspective-terms in language `.

3. For each document d ∈ {1, ..., D} with lang. `d:
• Draw topic weights θd ∼ Dir(α)
• For each topic-word index i ∈ {1, ..., Nwd } of

document d:

– Draw topic zi ∼ θd
– Draw topic concept ci = (wa, wb) ∼ φwzi ,

and select w`d as the member of that pair
corresponding to language `d.

• For each perspective-word index
j ∈ {1, ..., Nod} of document d:

– Draw topic xj ∼ Uniform(zw1 , ..., zwNo
d
)

– Draw perspective-word oj ∼ φ`,oxj
3.2 Model Variants
We have experimented with several variants of our
model, in order to account for the translation of pol-
ysemous words, adapt the translation model to the
corpus used, and to handle words for which no trans-
lation is found.

a) SINGLE variants of the model match each topic
term in a language with at most one topic term in
the other language.

MULTIPLE variants allow each term to match to
multiple other words in the other language.

b) INFER variants allow higher-likelihood match-
ings to be inferred from the data.

STATIC variants treat the matchings as fixed,
which is equivalent to assigning a probability of
0 or 1 to every edge in our bipartite graph C.

c) RELEGATE variants relegate all unmatched
words in each language to a single separate back-
ground topic distinct from the topics that are
learned for the matched topic words. This is
akin to forcing the probability for currently un-
matched words to 0 in all topics except for

one, and forcing the probability of all currently
matched words to 0 in this topic.

INCLUDE variants do not restrict the assignment
unmatched words; they are assigned to the same
set of topics as the matched words.

We test the following six variants: SINGLESTATI-
CRELEGATE, SINGLESTATICINCLUDE, SIN-
GLEINFERRELEGATE, SINGLEINFERINCLUDE,
MULTIPLESTATICRELEGATE, and MULTI-
PLESTATICINCLUDE. We do not test MULTI-
PLEINFER variants because of the complexity of
inferring a multiple matching in a bipartite graph.

3.3 Learning & Inference
For all variants, a collapsed Gibbs sampler can be
used to infer topics φ`,o and φw, per-document topic
distributions θ, as well as topic assignments z and
x. This corresponds to the S-step below. For INFER
variants, we follow Boyd-Graber and Blei in using
an M-step involving a bipartite graph matching al-
gorithm to infer the matching m that maximizes the
posterior likelihood of the matching.
S-Step: Sample topics for words in the corpus using
a collapsed Gibbs sampler. For topic-word wi =
u belonging to document d, if the word occurs in
concept ci = (u, v), then sample the topic and entry
according to:

p(zi = k, ci = (u, v) | wi = u, z−i, C)

∝ Ndk + αk∑
j

(Ndj + αj)
×

Nk(u,v) + β
w
k∑

v′

(
Nk(u,v′) + β

w
k

)

where the sum in the denominator of the first term
is over all topics, and in the second term is over all
words matched to u. Ndk is the count of topic-words
of topic k in document d, Nk(u,v) is the count of
topic-words either of type u or of type v assigned
to topic k in all the corpora.2 For perspective-word
oi = n, sample the topic according to:

p(zi = k|oi = n, z−i, C) ∝
Ndk∑
j Ndj

× N
`d
kv + β

o
k∑

m

(
N `dkm + β

o
k

)

2In RELEGATE variants, for u unmatched zi is sampled as:
p(zi = k|wi = u, z−i, C) ∝ Ndk + αk∑

k

(Ndk + αk)
,

which can be seen as βwu· → ∞ for unmatched terms.

50



where the sum in the second term of the denominator
is over the perspective-word vocabulary of language
`d; Ndk is the count of topic words in document d
with topic k; and N `dkm is the count of perspective-
word m being assigned topic k in language `d. Note
that in all the counts above, the current word token i
is omitted from the count.

Given our sampling assignments, we can then es-
timate θd, φ`,o, and φw as follows:

θ̂kd =
Ndk + αk∑

k

(Ndk + αk)
,

φ̂wk(u,v) =
Nk(u,v) + β

w
(u,v)

∑
v′

(
Nk(u,v′) + β

w
(u,v′)

) ,

φ̂`,onk =
Nkn + β

o
n∑

m

(
N `km + β

o
n

) .

M-Step: (for INFER variants only): Run the Jonker-
Volgenant (Jonker and Volgenant, 1987) bipartite
matching algorithm to find the optimal matching C
given some weights. For topic-term u from language
a and topic-term v from language b, our weights
correspond to the log of the posterior odds that the
occurrences of u and v come from a matched topic
distribution, as opposed to coming from unmatched
distributions:

µu,v =
∑

k\{a∗,b∗}

(
Nk(u,v) log φ̂

w
k(u,v)

)

−Nu log φ̂wk(u,·) −Nv log φ̂wk(·,v) + πu,v,

where Nu is the count of topic-term u in the cor-
pus. This expression can also be interpreted as a
kind of pointwise mutual information (Haghighi et
al., 2008). The Jonker-Volgenant algorithm has time
complexity of at mostO(V 3), where V is the size of
the lexicon (Jonker and Volgenant, 1987).

3.4 Inference of Perspective-Word Contrasts
Having learned our model and inferred how likely
perspective-terms are for a topic in a given language,
we seek to know whether these perspectives differ
significantly in the two languages. More precisely,
can we infer whether word m in language a and the
equivalent word n in language b have significantly
different distributions under a topic k? To do this,
we make the assumption that the perspective-words

in languages a and b are in one-to-one correspon-
dence to each other. Recall that, for a given topic
k and language `, N `km is the count for term m and
φ`,ok,m is the probability for word m in language `.
Just as we collect the probabilities into word-topic
distribution vectors φ`,ok , we collect the counts into
word-topic count vectors [N `k1, N

`
k2, ..]. Then, since

our model assumes a prior over the parameter vec-
tors φ`,ok , we can infer the likelihood for that ob-
served word-topic counts Nakm and N

b
kn were drawn

from a single word-topic-distribution prior denoted
by φ̆ := φa,okm = φ

b,o
kn. Below all our probabilities

are conditioned implicitly on this event as well as on
Nak and N

b
k being fixed.

Denote the total count of word tokens in topic k
from language ` by N `k =

∑
mN

`
km. Now, we de-

rive the probability that we observe a ratio greater
than δ between the proportion of words in topic k
that belong to word typem in language a and to cor-
responding word type n in language b:

p

(
Nakm
Nak

N bk
N bkn

≥ δ
)

+ p

(
N bkn
N bk

Nak
Nakm

≥ δ
)

(1)

By symmetry, it suffices to derive an expression for
the first term. We note that the inequality in the prob-
ability is equivalent to a sum over a range of values
of Nakm and N

b
kn. By rearranging terms, applying

the law of conditional probability to condition on
the term φ̆, and exploiting the conditional indepen-
dence of Nakm and N

b
km given φ̆, N

a
k , and N

b
k , we

can rewrite this first term as

Nbk∑

x=0

Nak∑

y=xδNa/b

∫
p(N bkn = x|φ̆)p(Nakm = y|φ̆)p(φ̆)dφ̆,

where Na/b = N
a
k

Nbk
. Recall that φ`,ok ∼ Dir(βo) un-

der our model. Assume a symmetric Dirichlet dis-
tribution for simplicity. It can then be shown that
the marginal distribution of φ̆ is φ̆ ∼ Beta(βo, (V −
1)βo), where V is the total size of the perspective-
word vocabulary. Similarly, it can be shown that the
marginal distribution of N `km given φ

`,o
k is N

`
km ∼

Binom(N `k, φ
`,o
i ) for ` ∈ {a, b}. Therefore, the inte-

grand above is proportional to the beta-binomial dis-
tribution with number of trials Nak + N

b
k , successes

x + y, and parameters βo and (V − 1)βo, but with
partition function

(
Nak
y

)(
Nbk
x

)
. Denote the PMF of this

51



distribution by f(Nak +N
b
k, x+y, β

o). Then expres-
sion (1) above becomes:

Nbk∑

x=0

Nak∑

y=xδNa/b

f(Nak +N
b
k, x+ y, β

o)

+

Nak∑

x=0

Nbk∑

y=xδNb/a

f(Nak +N
b
k, x+ y, β

o). (2)

We cannot observe Nakb, N
b
kn, N

a
k and N

b
k explic-

itly, but we can estimate them by obtaining poste-
rior samples from our Gibbs sampler. We substitute
these estimates into expression (2).

4 Experiments

4.1 Data
Twitter Data. We gathered Twitter data in En-
glish, Spanish and Russian during the first two
weeks of December 2013 using the Twitter API.
Following previous work (Puniyani et al., 2010),
we treated each Twitter user account as a docu-
ment. We then tagged each document for part-of-
speech, and divided the word tokens in it into topic-
words and perspective-words. We constructed a lex-
icon of 2,000 topic terms and 1,500 perspective-
terms for each language by filtering out any terms
that occurred in more than 10% of the docu-
ments in that language, and then selecting the re-
maining terms with the highest frequency. Fi-
nally, we kept only documents that contained 4
or more topic words from our lexicon. This left
us with 847,560 documents in English (4,742,868
topic-word and 1,907,685 perspective-word tokens);
756,036 documents in Spanish (4,409,888 topic-
word and 1,668,803 perspective-word tokens); and
260,981 documents in Russian (1,621,571 topic-
word and 981,561 perspective-word tokens).
News Data. We gathered all the articles published
online during the year 2013 by the state-run media
agencies of the United States (Voice of America or
“VOA”–English), Russia (RIA Novosti or “RIA”–
Russian), and Venezuela (Agencia Venezolana de
Noticias or “AVN”–Spanish). These three news
agencies were chosen because they not only pro-
vide media in three distinct languages, but they are
guided by the political world-views of three dis-
tinct governments. We treated each news article as

a document, and removed duplicates. Once again,
we constructed a lexicon of 2,000 topic terms and
1,500 perspective-terms using the same criteria as
for Twitter, and kept only documents that contained
4 or more topic words from our lexicon. This left us
with 23,159 articles (10,410,949 tokens) from VOA,
41,116 articles (11,726,637 tokens) from RIA, and
8,541 articles (2,606,796 tokens) from AVN.

Dictionaries. To create the translation dictionar-
ies, we extracted translations from the English,
Spanish, and Russian editions of Wiktionary, both
from the translation sections and the gloss sections
if the latter contained single words as glosses. Multi-
word expressions were universally removed. We
added inverse translations for every original trans-
lation. From the resulting collection of translations,
we then created separate translation dictionaries for
each language and part-of-speech tag combination.

In order to give preference to more important
translations, we assigned each translation an initial
weight of 1 + 1r , where r was the rank of the trans-
lation within the page. Since a translation (or its in-
verse) can occur on multiple pages, we aggregated
these initial weights and then assigned final weights
of 1 + 1r′ , where r

′ was the rank after aggregation
and sorting in descending order of weights.

4.2 Experimental Conditions

To evaluate the different variants of our model, we
held out 30,000 documents (test set) during training.
We plugged in the estimates of φw and C acquired
during training using the rest of the corpus to pro-
duce a likelihood estimate for these held-out docu-
ments. All models were initialized with the prior
matching determined by the dictionary data. For
each number of topics K, we set α to 50/Kand the
β variables to 0.02, as in Fang et al. (2012). For
the MULTIPLE variants, we set πi,j = 1 if i and j
share an entry and 0 otherwise. For INFER variants,
only three M -steps were performed to avoid overfit-
ting, at 250, 500, and 750 iterations of Gibbs sam-
pling, following the procedure in Boyd-Graber and
Blei (2009).

4.3 Comparison of model variants

In order to compare the variants of our model,
we computed the perplexity and coherence for

52



each variant on TWITTER and NEWS, for English–
Spanish and English–Russian language pairs.
Perplexity is a measure of how well a model trained
on a training set predicts the co-occurrence of words
on an unseen test set H. Lower perplexity indicates
better model fit. We evaluate the held-out perplex-
ity for topic words wi and perspective-words oi sep-
arately. For topic words, the perplexity is defined
as exp(−∑wi∈H logp(wi)/Nw). As for standard
LDA, exact inference of p(wi) is intractable under
this model. Therefore we adapted the estimator de-
veloped by Murray and Salakhutdinov (2009) to our
models.
Coherence is a measure inspired by pointwise mu-
tual information (Newman et al., 2010). LetD(v) be
the the number of documents with at least one token
of type v and let D(v, w) be the number of docu-
ments containing at least one token of type v and at
least one token of type w. Then Mimno et al. (2011)
define the coherence of topic k as

1(
M
2

)
M∑

m=2

m−1∑

`=1

log
D(v

(k)
m , v

(k)
` ) + �

D(v
(k)
` )

,

where V (k) = (v(k)1 , ..., v
(k)
M ) is a list of the M most

probable words in topic k and � is a small smoothing
constant used to avoid taking the logarithm of zero.
Mimno et al. (2011) find that coherence correlates
better with human judgments than do likelihood-
based measures. Coherence is topic-specific mea-
sure, so for each model variant we trained, we com-
puted the median topic coherence across all the top-
ics learned by the model. We set � = 0.1.
Model performance and analysis. Fig. 2 shows
perplexity for the variants as a function of the num-
ber of iterations of Gibbs sampling on the English-
Spanish NEWS corpus. The figure confirms that
1000 iterations of Gibbs sampling on the NEWS
corpus was sufficient for convergence across model
variants. We omit figures for English-Russian and
for the TWITTER corpus, since the patterns were
nearly identical. Figure 3 shows how perplexity
varies as a function of the number of topics. We
used this information to choose optimal models for
the different corpora. The optimal number of top-
ics was K = 175 for the English-Spanish NEWS
corpus, K = 200 for the English-Russian NEWS,

K = 325 for the English-Spanish TWITTER, and
K = 300 for the English-Russian TWITTER. Al-
though the optimal number of topics varied across
corpora, the relative performance of the different
models was the same. In all of our corpora, the
MULTIPLE variants provided better fits than their
corresponding SINGLE variants. There are several
explanations for this. For one, the MULTIPLE vari-
ants are able to exploit the information from multi-
ple translations, unlike the SINGLE variants, which
discarded all but one translation per word. For an-
other, the matchings produced by the SINGLEINFER
variants can be purely coincidental and the result of
overfitting (see some examples below). INCLUDE
variants performed markedly better than RELEGATE
variants. INFER variants improved model fit com-
pared to STATIC variants, but required more topics
to produce optimal fit.

Recall that we performed an M-step in the IN-
FER variants 3 times, at 250, 500, and 750 itera-
tions. As noted in §3.3, the M-step in the INFER
variants maximizes the posterior likelihood of the
matching. However, Fig. 2 shows that this maxi-
mization causes held-out perplexity to increase sub-
stantially just after the first matching M-step, around
250 iterations, before decreasing again after about
50 more iterations of Gibbs sampling. We believe
that this happens because the M-step is maximizing
over expectations that are approximate, since they
are estimated using Gibbs sampling. If the sampler
has not yet converged, then the M-step’s maximiza-
tion will be unstable. We found support for this ex-
planation when we re-ran the INFER variants using
1000 iterations between M-steps, giving the Markov
chain enough time to converge. After this change,
perplexity went down immediately after the M-step
and kept decreasing monotonically, rather than in-
creasing after the M-step before decreasing. How-
ever, this did not result in a significantly lower final
perplexity or coherence and thus did not change the
relative performance of the models. In addition, Fig.
2 suggests that the second and third M-steps (at 500
and 750 iterations, respectively) had little effect on
perplexity. In light of the high computational ex-
pense of each inference step, this suggests in prac-
tice a single inference step may be sufficient.

Fig. 4 shows that the MULTIPLESTATICINCLUDE
variant was also the superior model as measured by

53



Figure 2: Perplexity of different model variants for dif-
ferent numbers of iterations at K=175.

median topic coherence. Once again, this general
pattern held true for the English-Russian pair and
TWITTER corpora. Overall, the results show that
MULTIPLESTATICINCLUDE provides superior per-
formance across measures, corpora, topic numbers,
and languages. We therefore used this variant in
further data analysis and evaluation. Incidentally,
the observed decrease in topic coherence as K in-
creases is expected, because as K increases, lower-
likelihood topics tend to be more incoherent (Mimno
et al., 2011). Experiments by Stevens et al. (2012)
show that this effect is observed for LDA-, NMF-,
and SVD-based topic models.

Cross-linguistic matchings. The matchings in-
ferred by the SINGLEINFERINCLUDE variant were
of mixed quality. Some of the matchings corrected
low-quality translations in the original dictionary.
For instance, our prior dictionary matched passage
in English to pasaje in Spanish. Though technically
correct, the dominant meaning of pasaje is [travel]
ticket. The TWITTER model correctly matched pas-
sage to ruta instead. Many of the matchings learned
by the model did not provide technically correct
translations, yet were still revelatory and interesting.
For instance, the dictionary translated the Spanish
word pito as cigarette in English. However, in infor-
mal usage this word refers specifically to cannabis
cigarettes, not tobacco cigarettes. The TWITTER

Figure 3: Perplexity of different model variants.

model matches pito to the English slang word weed
instead. The Spanish word Siria (Syria) was un-
matched in the prior dictionary; the NEWS model
matched it to the word chemical, which makes sense
in the context of extensive reporting of the usage of
chemical weapons in the ongoing Syrian conflict.

4.4 Data analysis and discussion

We have conducted a qualitative analysis of the
topics, perspectives and contrasts produced by our
models for English–Spanish and English–Russian,
TWITTER and NEWS datasets. While the topics
were coherent and consistent across languages, sets
of perspective words manifested systematic differ-
ences revealing interesting cross-cultural contrasts.
Fig. 5 and 7 show the top perspective words discov-
ered by the model for the topic of finance and econ-
omy in English and Spanish NEWS and TWITTER
corpora, respectively. While some of the perspec-
tive words are neutral, mostly literal and occur in
both English and Spanish (e.g. balance or autho-
rize), many others represent metaphorical vocabu-
lary (e.g. saddle, gut, evaporate in English, or in-
cendiar, sangrar, abatir in Spanish) pointing at dis-
tinct models of conceptualization of the topic. When
we applied the contrast detection method (described
in §3.4) to these perspective words, it highlighted
the differences in metaphorical perspectives, rather
than the literal ones, as shown in Fig. 6 and 8. En-

54



Figure 4: Coherence of different model variants.

glish speakers tend to discuss economic and finan-
cial processes using motion terms, such as “slow,
drive, boost or sluggish”, or a related metaphor of
horse-riding, e.g. “rein in debt”, “saddle with debt”,
or even “breed money”. In contrast, Spanish speak-
ers tend to talk about the economy in terms of size
rather than motion, using verbs such as ampliar or
disminuir, and other metaphors, such as sangrar
(to bleed) and incendiar (to light up). These ex-
amples demonstrate coherent conceptualization pat-
terns that differ in the two languages. Interestingly,
this difference manifested itself in both NEWS and
TWITTER corpora and echoes the findings of a pre-
vious corpus-linguistic study of Charteris-Black and
Ennis (2001), who manually analysed metaphors
used in English and Spanish financial discourse and
reported that motion and navigation metaphors that
abound in English were rarely observed in Spanish.

For the majority of the topics we analysed
the model revealed interesting cross-cultural differ-
ences. For instance, the Spanish corpora exhib-
ited metaphors of battle when talking about poverty
(with poverty seen as an enemy), while in the En-
glish corpus poverty was discussed more neutrally
as a social problem that needs a practical solu-
tion. English-Russian NEWS experiments revealed
a surprising difference with respect to the topic of
protests. They suggested that while US media tend
to use stronger metaphorical vocabulary, such as

Topic EN budget debt deficit reduction spend balance
cut increase limit downtown tax stress addition planet
Topic ES presupuesto deficit deuda reduccion equilib-
rio disminucion gasto aumentacion tasa sacerdote

Perspective EN balance default triple rein accumulate
accrue trim incur saddle slash prioritize avert gut bur-
den evaporate borrow pile cap cut tackle
Perspective ES renegociar mejora etiquetado desplo-
mar recortar endeudar incendiar destinar asignar au-
torizar aprobado ascender sangrar augurar abatir

Figure 5: Top perspectives in system output for the topic
of finance in the NEWS corpus (metaphors in red italics).

Contrasts EN: rein [in debt], saddle [with debt], cap
[debt], breed [money], gut [budget], [debt] hit, tackle
[debt], boost, slow, drive, sluggish [economy], spur

Contrasts ES: sangrar [dinero], ampliar, disminuir [la
economı́a], superar [la tasa], emitir [deuda]

Figure 6: Contrasts identified by the model in NEWS.

clash, erupt or fire, in Russian protests are discussed
more neutrally. Generally, the NEWS corpora con-
tained more abstract topics and richer information
about conceptual structure and sentiment in all lan-
guages. Many of the topics discovered in TWIT-
TER related to everyday concepts, such as pets or
concerts, with fewer topics covering societal issues.
Yet, a few TWITTER-specific contrasts could be ob-
served: e.g., the sports topic tends to be discussed
using war and battle vocabulary in Russian to a
greater extent than in English.

Our models tend to identify two general kinds
of differences: (1) cross-corpus differences repre-
senting world views of particular populations whom
the corpora characterize (such differences exist both
across and within languages, e.g. the metaphors
used in the progressive New York Times would be
different from the ones in the more conservative Wall
Street Journal); and (2) deeply entrenched cross-
linguistic differences, such as the motion versus
expansion metaphors for the economy in English
and Spanish. Such systematic cross-linguistic con-
trasts can be associated with contrastive behavioural
patterns across the different linguistic communities
(Casasanto and Boroditsky, 2008; Fuhrman et al.,
2011). In both NEWS and TWITTER data, our
model effectively identifies and summarises such
contrasts simplifying the manual analysis of the data

55



Topic EN economy growth rate percent bank
economist interest reserve market policy
Topic ES economı́a crecimiento tasa banco poltica
mercado interés inflacin empleo economista

Perspective EN economic financial grow global ex-
pect remain cut boost low slow drive
Perspective ES económico mundial agregar fi-
nanciero informal pequeño significar interno bajar

Figure 7: Top perspectives in system output for the econ-
omy topic in TWITTER (metaphors in red).

Contrasts EN: slow [the economy], push [the econ-
omy], strong [economy], weak [economy], stable
[economy], boost [the economy]

Contrasts ES: caer [la economı́a], disminuir, superar
[la economı́a], ampliar [el crecimiento]

Figure 8: Contrasts identified by the model in TWITTER.

by highlighting linguistic trends that are indicative
of the underlying conceptual differences. However,
the conceptual differences are not straightforward
to evaluate based on the surface vocabulary alone.
In order to investigate this further, we conducted a
behavioural experiment testing a subset of the con-
trasts discovered by our model.

5 Behavioural evaluation

We assessed the relevance of the contrasts through
an experimental study with native English-speaking
and native Spanish-speaking human subjects. We
focused on a linguistic difference in the metaphors
used by English speakers versus Spanish speak-
ers when discussing changes in a nation’s econ-
omy. While English speakers tend to use metaphors
involving both locative motion verbs (e.g. slow)
as well as expansive/contractive motion verbs (e.g.
shrink), Spanish speakers preferentially employ ex-
pansive/contractive motion verbs (e.g. disminuir) to
describe changes in the economy. These differences
could reflect linguistic artefacts (such as collocation
frequencies) or could reflect entrenched conceptual
differences. Our experiment addresses the question
of whether such patterns of behaviour arise cross-
linguistically in response to non-linguistic stimuli.
If the linguistic differences are indicative of en-
trenched conceptual differences, then we expect to
see responses to the non-linguistic stimuli that corre-
spond to the usage differences in the two languages.

5.1 Experimental setup

We recruited 60 participants from one English-
speaking country (the US) and 60 participants from
three Spanish-speaking countries (Chile, Mexico,
and Spain) using the CrowdFlower crowdsourcing
platform. Participants first read a brief description
of the experimental task, which introduced them to
a fictional country in which economists are devis-
ing a simple but effective graphic for “representing
change in [the] economy”. They then completed
a demographic questionnaire including information
about their native language. Results from 9 US and
3 non-US participants were discarded for failure to
meet the language requirement.

Participants navigated to a new page to complete
the experimental task. Stimuli were presented in a
1200 × 700-pixel frame. The center of the frame
contained a sphere with a 64-pixel diameter. For
each trial, participants clicked on a button to activate
an animation of the sphere which involved (1) a pos-
itive displacement (in rightward pixels) of 10% or
20%, or a negative displacement (in leftward pixels)
of 10% or 20%;3 and, (2) an expansion (in increased
pixel diameter) of 10% or 20%, or a contraction (in
decreased pixel diameter) of 10% or 20%.4

Participants saw each of the resulting conditions
3 times. The displacement and size conditions were
drawn from a random permutation of 16 condi-
tions using a Fisher-Yates shuffle (Fisher and Yates,
1963). Crucially, half of the stimuli contained con-
flicts of information with respect to the size and dis-
placement metaphors for economic change (e.g. the
sphere could both grow and move to the left). Over-
all we expected the Spanish speakers’ responses to
be more closely associated with changes in diam-
eter due to the presence and salience of the size
metaphor, and the English speakers’ responses to
be influenced by both conditions. We expected
these differences to be most prominent in the con-

3The use of leftward/rightward horizontal displacement to
represent decreases/increases in magnitude is supported by re-
search in numerical cognition showing that people associate
smaller magnitudes with the left side of space and larger mag-
nitudes with the right side (Dehaene, 1992; Fias et al., 1995).

4A demonstration of the English experimental interface can
be accessed at http://goo.gl/W3YVfC. The Spanish in-
terface is identical, but for a direct translation of the guidelines
provided by a native Spanish/fluent English speaker.

56



Figure 9: ”Economy Improved” response rate in conflict-
ing stimulus conditions.

flicting trials, which force English speakers (unlike
Spanish speakers) to choose between two available
metaphors. We focus on these conflicting trials in
our analysis and discussion of the results.

5.2 Results

In trials in which stimuli moving rightward were
simultaneously contracting, English speakers re-
sponded that the economy improved 66% of the
time, whereas Spanish speakers judged the econ-
omy to have improved 43% of the time. In trials in
which stimuli moving leftward were simultaneously
expanding, English speakers judged the economy to
have improved 34% of the time, and Spanish speak-
ers responded that the economy improved 55% of
the time. The results are illustrated in Figure 9.

These results indicate three effects: (1) En-
glish speakers exhibit a pronounced bias for us-
ing horizontal displacement rather than expan-
sion/contraction during the decision-making pro-
cess; (2) Spanish speakers are more biased to-
ward expansion/contraction in formulating a deci-
sion; and, (3) across the two languages the responses
show contrasting patterns. The results support our
expectation on the relevance of different metaphors
when reasoning about the economy by the English
and Spanish speakers.

To examine the significance of these effects, we
fit a binary logit mixed effects model5 to the data.
The full analysis modeled judgment with native lan-
guage, displacement, and size as fully crossed fixed

5See Fox and Weisberg (2011) for a discussion of such mod-
els including application of the Type II Wald test.

effects and participant as a random effect. This anal-
ysis confirmed that native language was associated
with judgments about economic change. In particu-
lar, it indicated that changes in size affected English
speakers’ judgments and Spanish speakers’ judg-
ments differently (p < 0.001), with an increase in
size increasing the odds (eβ = 2.5) of a judgment of
IMPROVED by Spanish speakers and decreasing the
odds (eβ = 0.44) of a judgment of IMPROVED by
English speakers. A Type II Wald test revealed the
interaction between language and size to be highly
statistically significant (χ2(1) < 0.001).

In summary, the patterns we see in the be-
havioural data are consistent with the patterns un-
covered in the output of our model. While much ter-
ritory remains to be investigated to delimit the nature
of this relationship, our results represent a first step
toward establishing an association between informa-
tion mined from large textual data collections and
information observed through behavioural responses
on a human scale.

6 Conclusion

We presented the first model that detects common
topics from multilingual, non-parallel data and au-
tomatically uncovers differences in perspectives on
these topics across linguistic communities. Our
data analysis and behavioural evaluation offer evi-
dence of a symbiotic relationship between ecolog-
ically sound corpus experiments and scientifically
controlled human subject experiments, paving the
way for the use of large-scale text mining to inform
cognitive linguistics and psychology research.

We believe that our model represents a good foun-
dation for future projects in this area. A promising
area for further work is in developing better methods
for identifying contrasts in perspective terms. This
could perhaps involve modifying the generative pro-
cess for perspective terms or incorporating syntactic
dependency information. It would also be interest-
ing to investigate the effect of dictionary quality and
corpus size on the relative performance of STATIC
and INFER variants. Finally, we note that the model
can be applied to identify contrastive perspectives in
monolingual as well as multilingual data, providing
a general tool for the analysis of subtle, yet impor-
tant, cross-population differences.

57



Acknowledgments

We would like to thank the anonymous review-
ers as well as the TACL editors, Sharon Gold-
water and David Chiang, for helpful comments
on an earlier draft of this paper. This work
used the Extreme Science and Engineering Discov-
ery Environment (XSEDE), which is supported by
National Science Foundation grant number ACI-
1053575. Ekaterina Shutova’s research is sup-
ported by the Leverhulme Trust Early Career Fel-
lowship. Gerard de Melo’s research is supported
by China 973 Program Grants 2011CBA00300,
2011CBA00301, and NSFC Grants 61033001,
61361136003, 61550110504.

References
Amjad Abu-Jbara, Mona Diab, Pradeep Dasigi, and

Dragomir Radev. 2012. Subgroup detection in ideo-
logical discussions. In Proceedings of the 50th Annual
Meeting of the Association for Computational Linguis-
tics: Long Papers - Volume 1, ACL ’12, pages 399–
409, Stroudsburg, PA, USA. Association for Compu-
tational Linguistics.

Amr Ahmed and Eric P. Xing. 2010. Staying in-
formed: Supervised and semi-supervised multi-view
topical analysis of ideological perspective. In Pro-
ceedings of the 2010 Conference on Empirical Meth-
ods in Natural Language Processing, EMNLP ’10,
pages 1140–1150, Stroudsburg, PA, USA. Association
for Computational Linguistics.

Rawia Awadallah, Maya Ramanath, and Gerhard
Weikum. 2011. OpinioNetIt: Understanding the
Opinions-People network for politically controversial
topics. In Proceedings of the 20th ACM International
Conference on Information and Knowledge Manage-
ment, CIKM ’11, pages 2481–2484, New York, NY,
USA. ACM.

David M. Blei, Andrew Y. Ng, and Michael I. Jordan.
2003. Latent Dirichlet allocation. Journal of Machine
Learning Research, 3:993–1022.

Jordan Boyd-Graber and David M. Blei. 2009. Multilin-
gual topic models for unaligned text. In Proceedings
of the Twenty-Fifth Conference on Uncertainty in Ar-
tificial Intelligence (UAI ’09), pages 75–82. Arlington,
VA, USA: AUAI Press.

Jordan Boyd-Graber and Philip Resnik. 2010. Holistic
sentiment analysis across languages: multilingual su-
pervised latent Dirichlet allocation. In Proceedings of
the 2010 Conference on Empirical Methods in Natural
Language Processing, pages 45–55.

Daniel Casasanto and Lera Boroditsky. 2008. Time in
the mind: Using space to think about time. Cognition,
106(2):579–593.

Jonathan Charteris-Black and Timothy Ennis. 2001. A
comparative study of metaphor in Spanish and En-
glish financial reporting. English for Specific Pur-
poses, 20:249–266.

Stanislas Dehaene. 1992. Varieties of numerical abili-
ties. Cognition, 44:1–42.

Anthony Fader, Dragomir Radev, Burt L. Monroe, and
Kevin M. Quinn. 2007. MavenRank: Identifying in-
fluential members of the US senate using lexical cen-
trality. In In Proceedings of the 2007 Joint Conference
on Empirical Methods in Natural Language Process-
ing and Computational Natural Language Learning,
pages 658–666.

Yi Fang, Luo Si, Naveen Somasundaram, and Zheng-
tao Yu. 2012. Mining contrastive opinions on polit-
ical texts using cross-perspective topic model. In Pro-
ceedings of the Fifth ACM International Conference
on Web Search and Data Mining (WSDM ’12), pages
63–72, New York. New York: ACM.

Wim Fias, Marc Brysbaert, Frank Geypens, and Géry
d’Ydewalle. 1995. The importance of magnitude
information in numerical processing: evidence from
the SNARC effect. Mathematical Cognition, 2(1):95–
110.

Ronald A. Fisher and Frank Yates. 1963. Statistical
Tables for Biological, Agricultural and Medical Re-
search. Oliver and Boyd, Edinburgh.

John Fox and Sanford Weisberg. 2011. An R Companion
to Applied Regression. SAGE Publications, CA: Los
Angeles.

Orly Fuhrman, Kelly McCormick, Eva Chen, Heidi
Jiang, Dingfang Shu, Shuaimei Mao, and Lera
Boroditsky. 2011. How linguistic and cultural forces
shape conceptions of time: English and Mandarin time
in 3D. Cognitive Science, 35:1305–1328.

Nikesh Garera and David Yarowsky. 2009. Model-
ing latent biographic attributes in conversational gen-
res. In Proceedings of the Joint Conference of the
47th Annual Meeting of the ACL and the 4th Interna-
tional Joint Conference on Natural Language Process-
ing of the AFNLP: Volume 2 - Volume 2, ACL ’09,
pages 710–718, Stroudsburg, PA, USA. Association
for Computational Linguistics.

Sean M. Gerrish and David M. Blei. 2011. Predict-
ing legislative roll calls from text. In Proceedings of
ICML.

Swapna Gottipati, Minghui Qiu, Yanchuan Sim, Jing
Jiang, and Noah A. Smith. 2013. Learning topics
and positions from Debatepedia. In Proceedings of
the 2013 Conference on Empirical Methods in Natu-
ral Language Processing, pages 1858–1868, Seattle,

58



Washington, USA, October. Association for Computa-
tional Linguistics.

Aria Haghighi, Percy Liang, Taylor Berg-Kirkpatrick,
and Dan Klein. 2008. Learning bilingual lexicons
from monolingual corpora. In Proceedings of the 46th
Annual Meeting of the Association for Computational
Linguistics, ACL-’08:HLT, pages 771–779, Colum-
bus, Ohio, USA.

David Hall, Daniel Jurafsky, and Christopher D. Man-
ning. 2008. Studying the history of ideas using topic
models. In Proceedings of the 2008 Conference on
Empirical Methods in Natural Language processing,
pages 363–371. Association for Computational Lin-
guistics.

Jagadeesh Jagarlamudi and Hal Daumé III. 2010. Ex-
tracting multilingual topics from unaligned compara-
ble corpora. In Cathal Gurrin, Yulan He, Gabriella
Kazai, Udo Kruschwitz, and Suzanne Little, editors,
Proceedings of the 32nd European Conference on Ad-
vances in Information Retrieval (ECIR’2010), pages
444–456. Springer-Verlag, Berlin.

Rosie Jones, Ravi Kumar, Bo Pang, and Andrew
Tomkins. 2007. “I know what you did last summer”:
Query logs and user privacy. In Proceedings of the Six-
teenth ACM Conference on Conference on Information
and Knowledge Management, CIKM ’07, pages 909–
914, New York, NY, USA. ACM.

Roy Jonker and Anton Volgenant. 1987. A shortest aug-
menting path algorithm for dense and sparse linear as-
signment problems. Computing, 38(4):325–340.

Zoltán Kövecses. 2004. Introduction: Cultural varia-
tion in metaphor. European Journal of English Stud-
ies, 8:263–274.

George Lakoff and Elisabeth Wehling. 2012. The Lit-
tle Blue Book: The Essential Guide to Thinking and
Talking Democratic. Free Press, New York.

David Mimno, Hanna M. Wallach, Jason Naradowsky,
David A. Smith, and Andrew McCallum. 2009.
Polylingual topic models. In Proceedings of the 2009
Conference on Empirical Methods in Natural Lan-
guage Processing: Volume 2, pages 880–889. Asso-
ciation for Computational Linguistics.

David Mimno, Hanna M. Wallach, Edmund Talley,
Miriam Leenders, and Andrew McCallum. 2011. Op-
timizing semantic coherence in topic models. In Pro-
ceedings of the 2011 Conference on Empirical Meth-
ods in Natural Language Processing. Association for
Computational Linguistics.

Burt L. Monroe, Michael P. Colaresi, and Kevin M.
Quinn. 2008. Fightin’ words: Lexical feature selec-
tion and evaluation for identifying the content of polit-
ical conflict. Political Analysis, 16(4):372–403.

Iain Murray and Ruslan R. Salakhutdinov. 2009. Evalu-
ating probabilities under high-dimensional latent vari-
able models. In Advances in Neural Information Pro-
cessing Systems, pages 1137–1144.

David Newman, Jey Han Lau, Karl Grieser, and Timothy
Baldwin. 2010. Automatic evaluation of topic coher-
ence. In Proceedings of the 2010 Conference of the
North American Chapter of the Association for Com-
putational Linguistics: Human Language Technolo-
gies. Association for Computational Linguistics.

Diarmuid Ó Séaghdha. 2010. Latent variable models
of selectional preference. In Proceedings of the 48th
Annual Meeting of the Association for Computational
Linguistics, pages 435–444, Uppsala, Sweden. Asso-
ciation for Computational Linguistics.

Michael Paul and Roxana Girju. 2009. Cross-cultural
analysis of blogs and forums with mixed-collection
topic models. In Proceedings of the 2009 Conference
on Empirical Methods in Natural Language Process-
ing: Volume 3 - Volume 3, EMNLP ’09, pages 1408–
1417, Stroudsburg, PA, USA. Association for Compu-
tational Linguistics.

Marco Pennacchiotti and Ana-Maria Popescu. 2011.
Democrats, Republicans and Starbucks afficionados:
user classification in Twitter. In Proceedings of
the 17th ACM SIGKDD International Conference on
Knowledge Discovery and Data Mining, KDD ’11,
pages 430–438.

Kriti Puniyani, Jacob Eisenstein, Shay Cohen, and Eric P.
Xing. 2010. Social links from latent topics in mi-
croblogs. In Proceedings of the NAACL/HLT 2010
Workshop on Computational Linguistics in a World of
Social Media, pages 19–20. Association for Computa-
tional Linguistics.

Minghui Qiu and Jing Jiang. 2013. A latent variable
model for viewpoint discovery from threaded forum
posts. In Proceedings of the 2013 Conference of the
North American Chapter of the Association for Com-
putational Linguistics: Human Language Technolo-
gies, pages 1031–1040, Atlanta, Georgia, June. Asso-
ciation for Computational Linguistics.

Alan Ritter, Mausam Etzioni, and Oren Etzioni. 2010. A
latent Dirichlet allocation method for selectional pref-
erences. In Proceedings of the 48th Annual Meeting of
the Association for Computational Linguistics, pages
424–434. Association for Computational Linguistics.

Keith Stevens, Philip Kegelmeyer, David Andrzejewski,
and David Buttler. 2012. Exploring topic coherence
over many models and many topics. In Proceedings
of the 2012 Joint Conference on Empirical Methods
in Natural Language Processing and Computational
Natural Language Learning, pages 952–961, Jeju Is-
land, Korea.

59



Paul H. Thibodeau and Lera Boroditsky. 2011.
Metaphors we think with: The role of metaphor in rea-
soning. PLoS ONE, 6(2):e16782.

Janyce Wiebe, Theresa Wilson, Rebecca Bruce, Matthew
Bell, and Melanie Martin. 2004. Learning subjective
language. Comput. Linguist., 30(3):277–308, Septem-
ber.

60


