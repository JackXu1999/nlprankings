



















































Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics


Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics, pages 1701–1711
Vancouver, Canada, July 30 - August 4, 2017. c©2017 Association for Computational Linguistics

https://doi.org/10.18653/v1/P17-1156

Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics, pages 1701–1711
Vancouver, Canada, July 30 - August 4, 2017. c©2017 Association for Computational Linguistics

https://doi.org/10.18653/v1/P17-1156

Active Sentiment Domain Adaptation

Fangzhao Wu†, Yongfeng Huang†∗, and Jun Yan‡
†Department of Electronic Engineering, Tsinghua University

‡Microsoft Research Asia, Beijing, China
wufangzhao@gmail.com, yfhuang@tsinghua.edu.cn, junyan@microsoft.com

Abstract

Domain adaptation is an important techno-
logy to handle domain dependence pro-
blem in sentiment analysis field. Existing
methods usually rely on sentiment classi-
fiers trained in source domains. Howe-
ver, their performance may heavily decline
if the distributions of sentiment features
in source and target domains have signifi-
cant difference. In this paper, we propose
an active sentiment domain adaptation ap-
proach to handle this problem. Instead
of the source domain sentiment classifiers,
our approach adapts the general-purpose
sentiment lexicons to target domain with
the help of a small number of labeled
samples which are selected and annota-
ted in an active learning mode, as well as
the domain-specific sentiment similarities
among words mined from unlabeled sam-
ples of target domain. A unified model is
proposed to fuse different types of senti-
ment information and train sentiment clas-
sifier for target domain. Extensive expe-
riments on benchmark datasets show that
our approach can train accurate sentiment
classifier with less labeled samples.

1 Introduction

Sentiment classification is widely known as a
domain-dependent problem (Liu, 2012; Pang and
Lee, 2008; Blitzer et al., 2007; Pan et al., 2010).
This is because different domains usually have
many different sentiment expressions. For exam-
ple, “lengthy” and “boring” are popularly used in
Book domain to express negative sentiment. Ho-
wever, they are rare in Kitchen appliance domain.
Moreover, the same word or phrase may convey

∗Corresponding author.

different sentiments in different domains. For in-
stance, “unpredictable” is frequently used to ex-
press positive sentiment in Movie domain (e.g.,
“The plot of this movie is fun and unpredictable”).
However, it tends to be used as a negative word
in Kitchen appliance domain (e.g., “Even holding
heat is unpredictable. It is just terrible!”). Thus,
every domain has many domain-specific sentiment
expressions, which cannot be captured by other
domains. The performance of directly applying a
general sentiment classifier or a sentiment classi-
fier trained in other domains to target domain is
usually suboptimal.

Since there are a large number of domains in
user-generated content, it is impractical to ma-
nually annotate enough samples for each dom-
ain to train an accurate domain-specific sentiment
classifier. Thus, sentiment domain adaptation,
which transfers the sentiment classifier trained in
a source domain with sufficient labeled data to a
target domain with no or scarce labeled data, has
been widely studied (Blitzer et al., 2007; Pan et al.,
2010; He et al., 2011; Glorot et al., 2011). Existing
sentiment domain adaptation methods are mainly
based on transfer learning techniques. Many of
them try to learn a new feature representation to
augment or replace the original feature space in
order to reduce the gap of sentiment feature distri-
butions between source and target domains (Pan
et al., 2010; Glorot et al., 2011). For example,
Blitzer et al. (2007) proposed to learn a latent re-
presentation for domain-specific words from both
source and target domains by using pivot features
as bridge. The advantage of these methods is that
no labeled data in target domain is needed. Howe-
ver, when the distributions of sentiment features in
source and target domains have significant diffe-
rence, the performance of domain adaptation will
heavily decline (Li et al., 2013). In some cases,
the performance of adaptation is even lower than

1701

https://doi.org/10.18653/v1/P17-1156
https://doi.org/10.18653/v1/P17-1156


that without adaptation, which is usually known as
negative transfer (Pan and Yang, 2010).

In this paper, we propose an active sentiment
domain adaptation approach to handle this pro-
blem by incorporating both general sentiment in-
formation and a small number of actively selected
labeled samples from target domain. More specifi-
cally, in our approach the general sentiment infor-
mation extracted from sentiment lexicons is adap-
ted to target domain using domain-specific senti-
ment similarities among words. The general sen-
timent information is regarded as a “background”
domain to transfer. The word similarities are ex-
tracted from unlabeled samples of target domain
using both syntactic rules and co-occurrence pat-
terns. Then we actively select and annotate a small
number of informative samples from target dom-
ain in an active learning manner. These labeled
samples are incorporated into our approach to im-
prove the performance of sentiment domain adap-
tation. A unified model is proposed to incorporate
different types of sentiment information to train
sentiment classifier for target domain. Extensive
experiments were conducted on benchmark data-
sets. The experimental results show that our ap-
proach can train accurate sentiment classifiers and
reduce the manual annotation effort.

2 Related Work

2.1 Sentiment Domain Adaptation

Sentiment classification is well known as a highly
domain-dependent task, and domain adaptation
is widely studied in sentiment analysis field to
handle this problem (Blitzer et al., 2007; Pan et al.,
2010; He et al., 2011; Glorot et al., 2011). Existing
sentiment domain adaptation methods are mainly
based on transfer learning technique (Pan and
Yang, 2010), where sentiment classifiers are trai-
ned in one or multiple source domains with suf-
ficient labeled samples, and then applied to target
domain where there is no or only scarce labeled
samples. In order to reduce the gap of sentiment
feature distributions between source and target
domains, many sentiment domain adaptation met-
hods try to learn a new feature representation to
augment or replace the original feature space. For
example, Pan et al. (2010) proposed a sentiment
domain adaptation method based on spectral fea-
ture alignment (SFA) algorithm. They first manu-
ally selected several domain-independent features
and computed the associations between domain-

specific features and domain-independent featu-
res. After that they built a bipartite graph where
domain-independent and domain-specific featu-
res were regarded as two types of nodes. Then
domain-specific features were grouped into se-
veral clusters using spectral clustering algorithm.
These clusters were used to augment the original
feature representations. Glorot et al. (2011) propo-
sed a sentiment domain adaptation method based
on a deep learning technique, i.e., Stacked Denoi-
sing Autoencoders. They learned the parameters
of neural networks using unlabeled samples from
both source and target domains, and used the hid-
den nodes of the neural networks as the latent fe-
ature representations of both domains. Then they
trained sentiment classifiers using source domain
labeled data in this new feature space and app-
lied it to target domain. The advantage of these
sentiment domain adaptation methods is that they
do not rely on the labeled data in target dom-
ain. However, they have a common shortcoming,
i.e., when the distributions of sentiment features in
source and target domains have significant diffe-
rence, the performance of domain adaptation will
heavily decline (Li et al., 2013). In some cases,
negative transfer may happen (Blitzer et al., 2007;
Li et al., 2013), which means the performance
of adaptation is worse than that without adapta-
tion (Pan and Yang, 2010). Different from many
existing sentiment domain adaptation methods, in
our approach we adapt the general sentiment in-
formation in sentiment lexicons to target domain
with the help of a small number of labeled samples
which are selected and annotated in an active lear-
ning mode. Since the sentiment words in general-
purpose sentiment lexicons usually convey con-
sistent sentiment polarities in different domains,
and the actively selected labeled samples contain
rich domain-specific sentiment information of tar-
get domain, our approach can effectively reduce
the risk of negative transfer.

The usefulness of labeled samples from target
domain in sentiment domain adaptation has been
observed by previous research works (Choi and
Cardie, 2009; Chen et al., 2011; Li et al., 2013;
Wu et al., 2016). For example, Choi and Cardie
(2009) proposed to adapt a sentiment lexicon to
a specific domain by exploiting both the relations
among words which co-occur in the same senti-
ment expressions and the relations between words
and labeled sentiment expressions. However, the

1702



labeled samples used in these methods are rand-
omly selected, while in our approach we actively
select informative samples from target domain to
annotate. Thus, our approach has the potential to
reduce the manual annotation effort.

2.2 Active Learning

Active learning is a useful technique in scena-
rios where unlabeled data is abundant but their
labels are difficult or expensive to obtain (Tong
and Koller, 2002; Settles, 2010). By actively se-
lecting informative samples to label, active lear-
ning can effectively reduce the annotation effort,
and improve the classification performance with
limited budget (Li et al., 2012). An important
problem in active learning is how to evaluate the
informativeness of unlabeled samples (Fu et al.,
2013). Different methods have been applied to se-
lect informative samples, such as uncertainty sam-
pling (Zhu et al., 2010; Yang et al., 2015), query-
by-committee (Freund et al., 1997; Li et al., 2013)
and so on. In our approach, uncertainty combined
with density is used to measure the informative-
ness of samples. A major difference between our
approach and existing active learning methods is
that in existing methods the parameters of the ini-
tial classifier are either initialized as zero (Cesa-
Bianchi et al., 2006) or learned from a set of rand-
omly selected samples (Settles, 2010). In contrast,
the initial sentiment classifier in our approach is
constructed by adapting the general sentiment in-
formation to target domain via the domain-specific
sentiment similarities among words.

There are a few works that apply active learning
methods to sentiment domain adaptation task (Rai
et al., 2010; Li et al., 2013). For example, Rai
et al. (2010) proposed an online active learning al-
gorithm for sentiment domain adaptation. They
started with a sentiment classifier trained on the
labeled samples of a source domain. Then they
sequentially selected informative samples in target
domain to annotate with a probability positively
related to classification uncertainty. The newly an-
notated samples were used to update the sentiment
classifier in an online learning manner. Li et al.
(2013) proposed another active learning method
for cross-domain sentiment classification. In their
method they trained two sentiment classifiers, one
on the labeled samples of source domain, and the
other one on the labeled samples of target domain.
Then query-by-committee strategy was used to se-

lect the informative instances from target domain.
Different from these methods, our approach does
not rely on the labeled data of source domains.
Instead, in our approach the general sentiment in-
formation in sentiment lexicons is actively adapted
to target domain, which usually has better genera-
lization ability in various domains than the senti-
ment classifier trained in a source domain. In ad-
dition, our approach can incorporate the domain-
specific sentiment similarities among words mined
from unlabeled samples of target domain, which
are not considered in these methods.

3 Active Sentiment Domain Adaptation

3.1 Notations

First we introduce several notations that will be
used in remaining part of this paper. Denote the
general sentiment information extracted from a
general-purpose sentiment lexicon as p ∈ RD×1,
where D is the vocabulary size. If the ith word is
labeled as positive (or negative) in the sentiment
lexicon, then pi = +1 (or pi = −1). Otherwise,
pi = 0. Following many previous works in senti-
ment classification field (Blitzer et al., 2007; Pan
et al., 2010), here we select linear classifier as sen-
timent classifier, and denote the linear classifica-
tion model as w ∈ RD×1. We use f(xi, yi,w)
to represent the loss of classifying the ith labe-
led sample in target domain under the classifica-
tion model w, where f is the classification loss
function, xi ∈ RD×1 is the feature vector of this
sample and yi is its sentiment label. In this paper
we focus on binary sentiment classification and
yi ∈ {+1,−1}. In addition, we select log loss for
f . Thus, f(xi, yi,w) = log(1+ exp(−yiwTxi)).
Besides, we use S ∈ RD×D to represent the senti-
ment similarities among words extracted from un-
labeled samples of target domain.

3.2 Domain-Specific Sentiment Similarities

Next we introduce the extraction of domain-
specific sentiment similarities among words from
unlabeled samples of target domain. Two types of
similarities are extracted in this paper. The first
one is based on syntactic rules, which is inspired
by (Hatzivassiloglou and McKeown, 1997; Huang
et al., 2014; Wu and Huang, 2016). If two words
have the same POS-tag such as adjective, verb, and
adverb, and they are connected by coordinating
conjunction “and” in the same sentence, then we
regard they convey the same sentiment polarity. In

1703



addition, if two words are connected by adversa-
tive conjunction “but” and have the same POS-tag,
then they are assumed to have opposite sentiment
polarities. Denote Sr ∈ RD×D as the sentiment
similarities extracted from unlabeled samples ac-
cording to syntactic rules, and the similarity score
between words i and j is defined as:

Sri,j =
Nsi,j −Noi,j

Nsi,j +N
o
i,j + α1

, (1)

where N si,j and N
o
i,j are the frequencies of words

i and j having the same or opposite sentiments re-
spectively according to the syntactic rules, and α1
is a positive smoothing factor. If two words have
much higher frequency of sharing the same senti-
ment than opposite sentiments, then they will have
a larger positive sentiment similarity score. Note
that Sri,j can be negative according to Eq. (1). Here
we focus on sentiment similarity rather than dissi-
milarity, and set all the negative values in Sr to
zero. The range of Sri,j is [0, 1].

The second type of sentiment similarities are
extracted according to the co-occurrence patterns
among words. It is inspired by the observation
that words frequently co-occurring with each ot-
her not only have a high probability to have simi-
lar semantics, but also tend to share similar senti-
ments (Turney, 2002; Velikovich et al., 2010; Yo-
gatama and Smith, 2014; Tang et al., 2015; Ha-
milton et al., 2016). In this paper, we compute the
co-occurrence between words in the context of do-
cument. DenoteD as the set of all documents, and
N id as the frequency of word i appearing in docu-
ment d. Then, the sentiment similarity score bet-
ween words i and j based on their co-occurrence
patterns is defined as:

Sci,j =

∑
d∈Dmin{N id, N

j
d}∑

d∈Dmax{N id, N
j
d}+ α2

, (2)

where α2 is a positive smoothing parameter. If
two words frequently co-occur with each other in
many documents, then they will have a high sen-
timent similarity score according to Eq. (2). The
range of Sci,j is also [0, 1]. Denote S

c ∈ RD×D
as the set of all sentiment similarities extracted ac-
cording to co-occurrence patterns.

The sentiment similarities extracted according
to syntactic rules are usually of high accuracy.
However, their coverage is limited, because the
word pairs detected by these syntactic rules are
sparse. In contrast, the coverage of sentiment si-
milarities extracted from co-occurrence patterns is

quite wide because document is a long context,
while their accuracies are not as high as the simi-
larities based on syntactic rules. Thus, we pro-
pose to combine these two types of sentiment si-
milarities to obtain a balance between accuracy
and coverage. Denote S ∈ RD×D as the final
sentiment similarities among words, and Si,j =
θSri,j + (1− θ)Sci,j , where θ ∈ [0, 1] is the combi-
nation coefficient. In this paper we set θ to 0.5,
which means that we regard these two types of
sentiment similarities as equally important.

3.3 Initial Sentiment Classifier Construction

In this section, we introduce the construction of
the initial sentiment classifier to start the active le-
arning process. Existing active learning methods
usually randomly select a set of unlabeled sam-
ples to annotate and then train the initial classifier
on them (Settles, 2010). However, these randomly
selected samples may be redundant and not infor-
mative enough. In this paper, we propose to build
the initial sentiment classifier by adapting the ge-
neral sentiment information to target domain via
domain-specific sentiment similarities as follows:

w0 = argmin
w
−

D∑

i=1

piwi + α

D∑

i=1

∑

j 6=i
Si,j(wi − wj)2,

(3)

where w0 ∈ RD×1 is the initial sentiment clas-
sifier, α is a positive regularization coefficient, pi
is the prior sentiment polarity of word i in sen-
timent lexicons, and Si,j is the sentiment simila-
rity score between words i and j. Eq. (3) is mo-
tivated by (Bengio et al., 2006), and the quadratic
cost criterion is equivalent to label propagation. In
Eq. (3), −∑Di=1 piwi means that if a word i is la-
beled as a positive (or negative) word in a general-
purpose sentiment lexicon, i.e., pi > 0 (or pi < 0),
then we constrain that its sentiment weight in the
sentiment classifier is also positive (or negative).
Otherwise, a penalty will be added to the objective
function. In addition,

∑D
i=1

∑
j 6=i Si,j(wi − wj)2

represents that if two words share high sentiment
similarity, then we constrain they have similar sen-
timent weights in sentiment classifier. For exam-
ple, if we find that “great” and “easy” have high
sentiment similarities in Kitchen appliances dom-
ain (e.g., “This is a great pan and easy to wash”),
and “great” is a positive sentiment word in many
sentiment lexicons, then we can infer that “easy”
may also be a positive sentiment word in this dom-
ain by propagating the sentiment information from

1704



“great” to “easy”. In this way, the general senti-
ment information can be adapted to many domain-
specific sentiment expressions in target domain.

3.4 Query Strategy
Active learning methods iteratively select the most
informative instances to label and add them to the
training set (Settles, 2010). Thus, an important is-
sue in these methods is how to measure the infor-
mativeness of unlabeled samples. In this paper, we
select classification uncertainty as the informative-
ness measure, which has been proven effective in
many active learning methods (Zhu et al., 2010;
Yang et al., 2015). Since we focus on binary
sentiment classification and the classification loss
function is log loss, the classification uncertainty
of an unlabeled instance x is defined as:

U(x) = 1−
∣∣∣∣1−

2

1 + exp(−wTx)

∣∣∣∣ , (4)

where w is the linear sentiment classification mo-
del. The range of U(x) is [0, 1]. If |wTx| is large,
which means that current sentiment classifier is
confident in classifying this instance, then the un-
certainty of x (i.e., U(x)) will be low. If |wTx| is
close to 0, then the sentiment classifier is very un-
certain about this instance, probably because the
sentiment expressions in this instance are not co-
vered by current sentiment classifier, and the un-
certainty of the instance x will be high. In this
case, annotating this instance and adding it to the
training set are beneficial, because it can provide
the information of unknown sentiment expressions
and has the potential to quickly improve the qua-
lity of target domain sentiment classifier.

However, many researchers have found that un-
labeled instances with high uncertainties can be
outliers, whose labels are useless and even misle-
ading (Settles, 2010; Zhu et al., 2010). Thus, here
we combine uncertainty with representativeness to
avoid outliers. Density is proven to be an effective
measure of representativeness in active learning
methods (Zhu et al., 2010; Hajmohammadi et al.,
2015). Here we use the k-nearest neighbour based
density proposed by Zhu et al. (2010) as the re-
presentativeness measure, which is formulated as:

D(x) =
1

k

∑

xi∈N (x)

xTxi
‖x‖2 · ‖xi‖2

, (5)

where N (x) is the set of k most similar instances
of x. The final informativeness score of an unlabe-
led sample is a linear combination of uncertainty

and density which is formulated as follows:

I(x) = η(t)U(x) + (1− η(t))D(x), (6)

where η(t) ∈ [0, 1] is the combination coeffi-
cient at the tth iteration. In this paper, we select
a monotonically increasing function for η(t), i.e.,
η(t) = 1

1+exp(2− 4t
T
)
, where T is the total number

of iterations. It means that at initial iterations we
put more emphasis on instances with high repre-
sentativeness, because the initial sentiment classi-
fier built by adapting the general sentiment infor-
mation via the domain-specific sentiment simila-
rities is relatively weak, and we prefer to select
instances with more popular sentiment expressi-
ons to annotate. As more and more labeled sam-
ples are added to the training set and the sentiment
classifier becomes stronger, we gradually focus on
more difficult instances, i.e., those having higher
classification uncertainty scores.

3.5 Active Domain Adaptation

Based on previous discussions, in this section we
introduce the complete procedure of our active
sentiment domain adaptation (ASDA) approach.
Different from existing sentiment domain adapta-
tion methods which rely on the sentiment classi-
fier trained in source domains to transfer, in our
approach we regard the general sentiment infor-
mation in sentiment lexicons as the “background”
domain and adapt it to target domain with the help
of a small number of labeled samples which are
selected and annotated in an active learning mode.
First, we build an initial sentiment classifier accor-
ding to Eq. (3) by adapting the general sentiment
information to target domain using the domain-
specific sentiment similarities among words mined
from unlabeled samples of target domain. Second,
we compute the density of each unlabeled sample
in U according to Eq. (5). Then we repeat fol-
lowing steps until the annotation budget has run
out. First, we compute the uncertainty of each un-
labeled sample in U according to Eq. (4), and furt-
her we compute their informativeness by combi-
ning uncertainty with density according to Eq. (6).
Next, we select the unlabeled sample with the hig-
hest informativeness from U and manually anno-
tate its sentiment polarity. Then we add it to the set
of labeled samples L and remove it from U . Af-
ter that we retrain the sentiment classifier for tar-
get domain based on the general sentiment infor-
mation p, the labeled samples L, and the domain-

1705



specific sentiment similarities S as follows:

argmin
w
−

D∑

i=1

piwi + α

D∑

i=1

∑

j 6=i
Si,j(wi − wj)2

+ β
∑

xi∈L
log(1 + exp(−yiwTxi)) + λ‖w‖22,

(7)

where α, β, and λ are nonnegative coefficients.
By the term −∑Di=1 piwi we constrain that the
target domain sentiment classifier learned by our
approach is consistent with the general sentiment
information. Through this way, the general sen-
timent information extracted from sentiment lex-
icons can be adapted to target domain. The term∑D

i=1

∑
j 6=i Si,j(wi − wj)2 is motivated by label

propagation (Bengio et al., 2006). If two words
tend to have high sentiment similarity with each
other according to many unlabeled samples of tar-
get domain, then we constrain that their senti-
ment weights in the target domain sentiment clas-
sifier are also similar. The term

∑
xi∈L log(1 +

exp(−yiwTxi)) means that we hope to minimize
the empirical classification loss on labeled sam-
ples of target domain. By this term the sentiment
information in the labeled samples is incorporated
into the learning of target domain sentiment clas-
sifier. The L2-norm regularization term is introdu-
ced to control model complexity. The sentiment
classifier trained in Eq. (7) is further used at the
next iteration of active sentiment domain adapta-
tion until all the budget of manual annotation has
been used. Then we obtain the final sentiment
classifier of target domain. The complete algo-
rithm of our active sentiment domain adaptation
(ASDA) approach is summarized in Algorithm 1.

Algorithm 1 Active sentiment domain adaptation.
1: Input: The set of unlabeled samples U , the general sen-

timent information p, the domain-specific sentiment si-
milarities S, and the total annotation budget N .

2: Output: Target domain sentiment classifier w.
3: Train the initial sentiment classifier w0 (Eq. (3)).
4: Compute the density of each sample xi in U (Eq. (5)).
5: Initialize the set of labeled samples L = ∅, the iteration

number t = 0, and the sentiment classifier w = w0.
6: while t < N do
7: t = t+ 1.
8: Compute the uncertainty score of each sample xi

in U (Eq. (4)).
9: Compute the informativeness score of each sample

xi in U (Eq. (6)).
10: Select x∗ from U which has the highest informati-

veness score.
11: Annotate x∗ and obtain its sentiment label y.
12: L = L+ {x∗, y}, U = U − x∗.
13: Update sentiment classifier w according to Eq. (7).
14: end while

4 Experiments

4.1 Datasets

The dataset used in our experiments is the Amazon
product review dataset1 collected by Blitzer et al.
(2007), which is widely used in sentiment analysis
and domain adaptation research (Pan et al., 2010;
Bollegala et al., 2011). This dataset contains pro-
duct reviews in four domains, i.e., Book, DVD,
Electronics, and Kitchen appliances. In each dom-
ain, 1,000 positive and 1,000 negative reviews as
well as a large number of unlabeled samples are
included. The detailed statistics of this dataset are
summarized in Table 1.

Book DVD Electronics Kitchen
positive 1,000 1,000 1,000 1,000
negative 1,000 1,000 1,000 1,000

unlabeled 973,194 122,438 21,009 17,856

Table 1: The statistics of the Amazon dataset.

Following many previous works (Blitzer et al.,
2007; Bollegala et al., 2011), unigrams and bi-
grams were used to build feature vectors in our
experiments. We randomly split the labeled sam-
ples in each domain into two parts with equal size.
The first part was used as test data, and the second
part was used as the pool of “unlabeled” samples
to perform active learning. The general sentiment
information was extracted from Bing Liu’s senti-
ment lexicon2 (Hu and Liu, 2004), which is one of
the state-of-the-art general-purpose sentiment lex-
icons. The domain-specific sentiment similarities
among words were extracted from the large-scale
unlabeled samples. The total number of samples
actively selected by our approach to annotate was
set to 100. The values of α, β, and λ were set to
0.1, 1, and 1 respectively. We repeated each ex-
periment 10 times independently and the average
results were reported.

4.2 Algorithm Effectiveness

First we conducted several experiments to explore
the effectiveness of our active sentiment domain
adaptation (ASDA) approach. We hope to answer
two questions via these experiments: 1) whether
the domain-specific sentiment similarities among
words mined from unlabeled samples of target

1https://www.cs.jhu.edu/˜mdredze/
datasets/sentiment/

2https://www.cs.uic.edu/liub/FBS/
sentiment-analysis.html

1706



Book DVD Electronics Kitchen
0.5

0.55

0.6

0.65

0.7

0.75

0.8

0.85

A
cc

ur
ac

y

 

 
Lexicon
Lexicon+SentiSim
Lexicon+SentiSim+Label

Figure 1: The performance of our approach
with different combinations of sentiment informa-
tion. Lexicon, SentiSim, and Label represent the
general-purpose sentiment lexicon, the domain-
specific sentiment similarities among words, and
a small number of actively selected and annotated
samples in target domain respectively.

domain are useful for adapting the general senti-
ment information to target domain; 2) whether a
small number of samples which are actively se-
lected and annotated in target domain can help
improve the domain adaptation performance. In
our experiments, we implemented different versi-
ons of our ASDA approach using different combi-
nations of sentiment information. The first one is
Lexicon, which means only using the general sen-
timent information and no domain adaptation is
conducted. It serves as a baseline. The second one
is Lexicon+SentiSim, which means adapting gene-
ral sentiment information to target domain using
domain-specific sentiment similarities, but labe-
led samples of target domain are not incorporated.
The third one is Lexicon+SentiSim+Label, which
is the complete ASDA approach. The experimental
results are summarized in Fig. 1.

According to Fig. 1, the performance of Lexicon
is suboptimal. This is because the general senti-
ment lexicons cannot capture the domain-specific
sentiment expressions in target domain (Choi and
Cardie, 2009). Lexicon+SentiSim performs signi-
ficantly better than Lexicon, which validates that
the sentiment similarities among words extracted
from unlabeled samples of target domain contain
rich domain-specific sentiment information, and
can help propagate the general sentiment informa-
tion to many domain-specific sentiment expressi-
ons. Besides, after incorporating a small number
of labeled samples which are actively selected and
annotated by our approach in an active learning
mode, the performance of our sentiment dom-

Book DVD Electronics Kitchen
0.7

0.75

0.8

0.85

A
cc

ur
ac

y

 

 
ASDA_Random
ASDA_Constant
ASDA_Dynamic

Figure 2: The performance of our approach with
labeled samples selected by different strategies.

ain adaptation approach is significantly improved.
This is because although these labeled samples are
in limited size and cannot cover all the sentiment
expressions in target domain, they can provide
sentiment information of popular domain-specific
sentiment expressions, which can be propagated to
other sentiment expressions in target domain du-
ring the domain adaptation process. Thus, above
experimental results validate the effectiveness of
our approach.

We also conducted several experiments to ve-
rify the advantage of the actively selected samples
over randomly selected samples and validate the
effectiveness of our active learning algorithm. We
also compared the dynamic weighting scheme for
combining uncertainty and density with the con-
stant weighting scheme. The experimental results
are summarized in Fig. 2. According to Fig. 2, our
approach with actively selected samples performs
better than that with randomly selected samples. It
indicates that these actively selected samples are
more informative than randomly selected samples
for sentiment domain adaptation. In addition, our
approach with dynamic weighting scheme in com-
bining uncertainty and density outperforms that
with constant weighting scheme, which implies
that it is beneficial to emphasize representative
samples at initial iterations and gradually focus on
difficult samples at later iterations. Thus, the ex-
perimental results validate the effectiveness of our
active learning algorithm.

4.3 Performance Evaluation

In this section we conducted experiments to eva-
luate the performance of our approach by compa-
ring it with several baseline methods. The met-
hods to be compared include: 1) MPQA and Bing-
Liu, using two state-of-the-art sentiment lexicons,

1707



i.e., MPQA (Wilson et al., 2005) and Bing Liu’s
lexicon (Hu and Liu, 2004) for sentiment clas-
sification following the suggestions in (Hu and
Liu, 2004); 2) SVM, LS, and LR, three popular
supervised sentiment classification methods, i.e.,
support vector machine (Pang et al., 2002), le-
ast squares (Hu et al., 2013) and logistic regres-
sion (Wu et al., 2015); 3) ZIAL, the zero initiali-
zed active learning method (Cesa-Bianchi et al.,
2006); 4) LIAL, the active learning method ini-
tialized by randomly selected labeled data (Sett-
les, 2010); 5) SCL and SFA, two famous sentiment
domain adaptation methods proposed in (Blitzer
et al., 2007) and (Pan et al., 2010) respectively; 6)
ILP, adapting sentiment lexicons to target domain
via integer linear programming (Choi and Cardie,
2009); 7) AODA, the active online domain adapta-
tion method (Rai et al., 2010); 8) ALCD, the active
learning method for cross-domain sentiment clas-
sification (Li et al., 2013); 9) ASDA, our active
sentiment domain adaptation approach. For above
methods, if labeled target domain samples are nee-
ded in training, the number of labeled samples was
set to 100, and if source domain labeled samples
are needed in training, the number of labeled sam-
ples was set to 1,000. The parameters in baseline
methods were tuned via cross-validation. The ex-
perimental results are summarized in Table 2.

Book DVD Electronics Kitchen
Acc Fscore Acc Fscore Acc Fscore Acc Fscore

MPQA 0.5953 0.5673 0.6149 0.5936 0.6150 0.6070 0.6392 0.6258
BingLiu 0.6015 0.6048 0.6539 0.6604 0.6248 0.6320 0.6765 0.6930

SVM 0.6580 0.6511 0.6688 0.6652 0.7138 0.7129 0.7386 0.7412
LS 0.6543 0.6542 0.6692 0.6687 0.7194 0.7185 0.7479 0.7465
LR 0.6606 0.6582 0.6774 0.6742 0.7257 0.7226 0.7492 0.7480

RIAL 0.6693 0.6663 0.6850 0.6821 0.7310 0.7299 0.7574 0.7568
LIAL 0.6756 0.6731 0.6866 0.6838 0.7374 0.7360 0.7599 0.7595
SCL 0.7233 0.7201 0.7469 0.7438 0.7768 0.7730 0.8099 0.8095
SFA 0.7307 0.7285 0.7513 0.7485 0.7846 0.7812 0.8174 0.8153
ILP 0.6942 0.6931 0.7153 0.7124 0.7463 0.7445 0.7793 0.7768

AODA 0.6928 0.6912 0.7172 0.7165 0.7518 0.7512 0.7698 0.7690
ALCD 0.7237 0.7221 0.7369 0.7364 0.7768 0.7788 0.7979 0.7970
ASDA 0.7508 0.7501 0.7764 0.7759 0.8014 0.8011 0.8329 0.8328

Table 2: Sentiment classification performance of
different methods in different domains. Acc and
Fscore represent accuracy and macro-averaged
Fscore respectively.

According to Table 2, the performance of di-
rectly applying sentiment lexicons to target dom-
ain is suboptimal. This is because there are many
domain-specific sentiment expressions that are not
covered by these general-purpose sentiment lex-
icons (Choi and Cardie, 2009). In addition, the
performance of supervised sentiment classifica-
tion methods such as SVM, LS, and LR is also

200 400 600 800 1000
0.65

0.7

0.75

0.8

0.85

Number of labeled samples

A
cc

ur
ac

y

 

 
ASDA
SVM

Figure 3: The performance of ASDA and SVM
with different numbers of labeled samples.

limited, because the labeled samples for training
are extremely scarce. The active learning met-
hods such as ZIAL (Cesa-Bianchi et al., 2006) and
LIAL (Settles, 2010) perform relatively better, be-
cause they can actively select informative samples
to annotate and learn. Our approach can outper-
form both of them. This is because besides the
labeled samples, our approach also adapts the ge-
neral sentiment information in sentiment lexicons
to target domain and incorporates it into the le-
arning of target domain sentiment classifier. Our
approach also performs better than state-of-the-art
domain adaptation methods such as SCL (Blitzer
et al., 2007) and SFA (Pan et al., 2010). It im-
plies that a small number of actively selected la-
beled samples from target domain are beneficial
for sentiment domain adaptation. ILP (Choi and
Cardie, 2009) tries to adapt a sentiment lexicon
to target domain, which is similar with our ap-
proach. ILP relies on labeled samples to extract
the relations among words and relations between
words and sentiment expressions. However, labe-
led samples in target domain are usually limited
and the sentiment information in many unlabeled
samples is not exploited in ILP. Thus, our appro-
ach can outperform it. Similar with our approach,
AODA (Rai et al., 2010) and ALCD (Li et al., 2013)
also apply active learning to domain adaptation.
The major difference is that in our approach the
general sentiment information extracted from sen-
timent lexicons is adapted to target domain, while
in AODA and ALCD the sentiment classifier trai-
ned in source domains is transferred. The superior
performance of our approach implies that the ge-
neral sentiment information has better generaliza-
tion ability than the sentiment classifier trained in
a specific source domain, and is more suitable for
sentiment domain adaptation.

1708



−4 −3 −2 −1 0
0.72

0.74

0.76

0.78

0.8

0.82

0.84

log
10

(α)

A
cc

ur
ac

y

 

 
Kitchen
Electronics
DVD
Book

(a) Parameter α.

−2 −1.5 −1 −0.5 0 0.5 1
0.7

0.75

0.8

0.85

log
10

(β)

A
cc

ur
ac

y

 

 
Kitchen
Electronics
DVD
Book

(b) Parameter β.

Figure 4: The influence of the parameter settings
of α and β on the performance of our approach.

We further conducted several experiments to
validate the advantage of our approach in trai-
ning accurate sentiment classifier for target dom-
ain with only a few labeled samples. We varied the
annotation budget, i.e., the number of labeled sam-
ples, from 100 to 1,000. The learning curve of our
ASDA approach in Book domain is shown in Fig. 3.
We also included a purely supervised sentiment
classification method, i.e., SVM, in Fig. 3 as a ba-
seline for comparison. Fig. 3 shows that our ASDA
approach can consistently outperform SVM when
the same number of labeled samples are used. The
performance advantage of our approach is more
significant when labeled samples are scarce. For
example, the performance of our approach with
only 200 labeled samples is similar to SVM with
more than 800 labeled samples. Thus, the expe-
rimental results validate that by adapting the ge-
neral sentiment information to target domain and
selecting the most informative samples to annotate
and learn, our approach can effectively reduce the
manual annotation effort, and can train accurate
sentiment classifier for target domain with much
less labeled samples.

4.4 Parameter Analysis

In this section, we conducted several experiments
to explore the influence of parameter settings on
the performance of our approach. α and β are the
two most important parameters in our approach,
which control the relative importance of domain-
specific sentiment similarities and the actively se-
lected samples in training sentiment classifier for
target domain. The experimental results of para-
meters α and β are summarized in Fig. 4.

According to Fig. 4, when α and β are too
small, the performance of our approach is not op-
timal. This is because the useful sentiment infor-
mation in domain-specific sentiment similarities
mined from unlabeled samples and the actively

selected labeled samples of target domain is not
fully exploited. Thus, the performance of our ap-
proach improves when these parameters increase
from a small value. However, when these para-
meters become too large, the performance of our
approach starts to decline. This is because when
β is too large the sentiment classifier learned by
our approach is mainly decided by the limited la-
beled samples, and the general sentiment informa-
tion extracted from sentiment lexicons is not fully
exploited. When α is too large, the information
in domain-specific sentiment similarities is over-
emphasized, and many different words will have
nearly the same sentiment weights. Thus, the per-
formance of our approach in these scenarios is also
not optimal. A moderate value of α and β is most
suitable for our approach.

5 Conclusion

In this paper we present an active sentiment dom-
ain adaptation approach to train accurate senti-
ment classifier for target domain with less labe-
led samples. In our approach, the general senti-
ment information in sentiment lexicons is adapted
to target domain with the help of a small number
of labeled samples which are selected and anno-
tated in an active learning mode. Both classifica-
tion uncertainty and density are considered when
selecting informative samples to label. In addi-
tion, we extract domain-specific sentiment simila-
rities among words from unlabeled samples of tar-
get domain based on both syntactic rules and co-
occurrence patterns, and incorporate them into the
domain adaptation process to propagate the gene-
ral sentiment information to many domain-specific
sentiment words in target domain. We also pro-
pose a unified model to incorporate different types
of sentiment information to train sentiment clas-
sifier for target domain. Experimental results on
benchmark datasets show that our approach can
train accurate sentiment classifier and at same time
reduce the manual annotation effort.

Acknowledgements

This research is supported by the Key Research
Project of the Ministry of Science and Technology
of China (Grant no. 2016YFB0800402) and the
Key Program of National Natural Science Founda-
tion of China (Grant nos. U1536201, U1536207,
and U1405254).

1709



References
Yoshua Bengio, Olivier Delalleau, and Nicolas

Le Roux. 2006. Label propagation and quadratic
criterion. Semi-supervised learning 10.

John Blitzer, Mark Dredze, Fernando Pereira, et al.
2007. Biographies, bollywood, boom-boxes and
blenders: Domain adaptation for sentiment clas-
sification. In ACL. volume 7, pages 440–447.
http://aclweb.org/anthology-new/P/P07/P07-1056.

Danushka Bollegala, David Weir, and John Carroll.
2011. Using multiple sources to construct a sen-
timent sensitive thesaurus for cross-domain senti-
ment classification. In ACL:HLT . pages 132–141.
http://aclweb.org/anthology/P11-1014.

Nicolo Cesa-Bianchi, Claudio Gentile, and Luca Zani-
boni. 2006. Worst-case analysis of selective sam-
pling for linear classification. Journal of Machine
Learning Research 7(Jul):1205–1230.

Minmin Chen, Kilian Q Weinberger, and John Blitzer.
2011. Co-training for domain adaptation. In NIPS.
pages 2456–2464.

Yejin Choi and Claire Cardie. 2009. Adap-
ting a polarity lexicon using integer linear
programming for domain-specific sentiment
classification. In EMNLP. pages 590–598.
http://aclweb.org/anthology/D09-1062.

Yoav Freund, H. Sebastian Seung, Eli Shamir,
and Naftali Tishby. 1997. Selective sam-
pling using the query by committee algo-
rithm. Machine Learning 28(2-3):133–168.
http://dx.doi.org/10.1023/A:1007330508534.

Yifan Fu, Xingquan Zhu, and Bin Li. 2013. A sur-
vey on instance selection for active learning. Kno-
wledge and Information Systems 35(2):249–283.
https://doi.org/10.1007/s10115-012-0507-8.

Xavier Glorot, Antoine Bordes, and Yoshua Bengio.
2011. Domain adaptation for large-scale sentiment
classification: A deep learning approach. In ICML.
pages 513–520.

Mohammad Sadegh Hajmohammadi, Roliana Ibrahim,
Ali Selamat, and Hamido Fujita. 2015. Combination
of active learning and self-training for cross-lingual
sentiment classification with density analysis of un-
labelled samples. Information sciences 317:67–77.
http://dx.doi.org/10.1016/j.ins.2015.04.003.

William L. Hamilton, Kevin Clark, Jure Le-
skovec, and Dan Jurafsky. 2016. Inducing
domain-specific sentiment lexicons from un-
labeled corpora. In EMNLP. pages 595–605.
http://aclweb.org/anthology/D/D16/D16-1057.

Vasileios Hatzivassiloglou and Kathleen R McKe-
own. 1997. Predicting the semantic orienta-
tion of adjectives. In ACL. pages 174–181.
http://aclweb.org/anthology/P/P97/P97-1023.

Yulan He, Chenghua Lin, and Harith Alani. 2011.
Automatically extracting polarity-bearing topics for
cross-domain sentiment classification. In ACL:HLT .
pages 123–131. http://aclweb.org/anthology/P11-
1013.

Minqing Hu and Bing Liu. 2004. Mining and summa-
rizing customer reviews. In KDD. pages 168–177.
http://doi.acm.org/10.1145/1014052.1014073.

Xia Hu, Lei Tang, Jiliang Tang, and Huan Liu. 2013.
Exploiting social relations for sentiment analysis
in microblogging. In WSDM. pages 537–546.
http://doi.acm.org/10.1145/2433396.2433465.

Sheng Huang, Zhendong Niu, and Chongyang Shi.
2014. Automatic construction of domain-specific
sentiment lexicon based on constrained label pro-
pagation. Knowledge-Based Systems 56:191–200.
http://dx.doi.org/10.1016/j.knosys.2013.11.009.

Lianghao Li, Xiaoming Jin, Sinno Jialin Pan, and
Jian-Tao Sun. 2012. Multi-domain active learning
for text classification. In KDD. pages 1086–1094.
http://doi.acm.org/10.1145/2339530.2339701.

Shoushan Li, Yunxia Xue, Zhongqing Wang, and Guo-
dong Zhou. 2013. Active learning for cross-domain
sentiment classification. In IJCAI. pages 2127–
2133.

Bing Liu. 2012. Sentiment analysis and opinion mi-
ning. Synthesis Lectures on Human Language
Technologies 5(1):1–167.

Sinno Jialin Pan, Xiaochuan Ni, Jian-Tao Sun, Qi-
ang Yang, and Zheng Chen. 2010. Cross-
domain sentiment classification via spectral fea-
ture alignment. In WWW. ACM, pages 751–760.
http://doi.acm.org/10.1145/1772690.1772767.

Sinno Jialin Pan and Qiang Yang. 2010. A survey
on transfer learning. TKDE 22(10):1345–1359.
http://dx.doi.org/10.1109/TKDE.2009.191.

Bo Pang and Lillian Lee. 2008. Opinion mi-
ning and sentiment analysis. Foundations and
trends in information retrieval 2(1-2):1–135.
http://dx.doi.org/10.1561/1500000011.

Bo Pang, Lillian Lee, and Shivakumar Vaithyanathan.
2002. Thumbs up?: sentiment classification using
machine learning techniques. In EMNLP. pages 79–
86. https://doi.org/10.3115/1118693.1118704.

Piyush Rai, Avishek Saha, Hal Daumé III, and Su-
resh Venkatasubramanian. 2010. Domain adap-
tation meets active learning. In Proceedings of
the NAACL HLT 2010 Workshop on Active Lear-
ning for Natural Language Processing. pages 27–
32. http://aclweb.org/anthology/W10-0104.

Burr Settles. 2010. Active learning literature survey.
University of Wisconsin, Madison 52(55-66):11.

1710



Jian Tang, Meng Qu, and Qiaozhu Mei.
2015. Pte: Predictive text embedding
through large-scale heterogeneous text net-
works. In KDD. ACM, pages 1165–1174.
http://doi.acm.org/10.1145/2783258.2783307.

Simon Tong and Daphne Koller. 2002. Sup-
port vector machine active learning with ap-
plications to text classification. The Jour-
nal of Machine Learning Research 2:45–66.
http://dx.doi.org/10.1162/153244302760185243.

Peter D Turney. 2002. Thumbs up or thumbs down?:
semantic orientation applied to unsupervised clas-
sification of reviews. In ACL. pages 417–424.
http://dx.doi.org/10.3115/1073083.1073153.

Leonid Velikovich, Sasha Blair-Goldensohn, Kerry
Hannan, and Ryan McDonald. 2010. The viability
of web-derived polarity lexicons. In NAACL. pages
777–785. http://www.aclweb.org/anthology/N10-
1119.

Theresa Wilson, Janyce Wiebe, and Paul Hoffmann.
2005. Recognizing contextual polarity in phrase-
level sentiment analysis. In EMNLP. pages 347–
354. http://dx.doi.org/10.3115/1220575.1220619.

Fangzhao Wu and Yongfeng Huang. 2016. Sentiment
domain adaptation with multiple sources. In ACL.
pages 301–310. http://aclweb.org/anthology/P16-
1029.

Fangzhao Wu, Yangqiu Song, and Yongfeng Huang.
2015. Microblog sentiment classification with con-
textual knowledge regularization. In AAAI. pages
2332–2338.

Fangzhao Wu, Sixing Wu, Yongfeng Huang, Son-
gfang Huang, and Yong Qin. 2016. Sentiment
domain adaptation with multi-level contextual sen-
timent knowledge. In CIKM. ACM, pages 949–958.
https://doi.org/10.1145/2983323.2983851.

Yi Yang, Zhigang Ma, Feiping Nie, Xiaojun
Chang, and Alexander G. Hauptmann. 2015.
Multi-class active learning by uncertainty sam-
pling with diversity maximization. Internatio-
nal Journal of Computer Vision 113(2):113–127.
http://dx.doi.org/10.1007/s11263-014-0781-x.

Dani Yogatama and Noah A. Smith. 2014. Making
the most of bag of words: Sentence regularization
with alternating direction method of multipliers. In
ICML. pages 656–664.

Jingbo Zhu, Huizhen Wang, Benjamin K Tsou,
and Matthew Ma. 2010. Active learning with
sampling by uncertainty and density for data an-
notations. IEEE Transactions on Audio, Speech,
and Language Processing 18(6):1323–1331.
http://dx.doi.org/10.1109/TASL.2009.2033421.

1711


	Active Sentiment Domain Adaptation

