



















































Named Entity Recognition for Novel Types by Transfer Learning


Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 899–905,
Austin, Texas, November 1-5, 2016. c©2016 Association for Computational Linguistics

Named Entity Recognition for Novel Types by Transfer Learning

Lizhen Qu1,2, Gabriela Ferraro1,2, Liyuan Zhou1,
Weiwei Hou1, Timothy Baldwin1,3

1 DATA61, Australia
2 The Australian National University

3 The University of Melbourne
{lizhen.qu,gabriela.ferraro,joe.zhou}@data61.csiro.au

houvivid2013@gmail.com, tb@ldwin.net

Abstract

In named entity recognition, we often don’t
have a large in-domain training corpus or a
knowledge base with adequate coverage to
train a model directly. In this paper, we pro-
pose a method where, given training data in a
related domain with similar (but not identical)
named entity (NE) types and a small amount of
in-domain training data, we use transfer learn-
ing to learn a domain-specific NE model. That
is, the novelty in the task setup is that we as-
sume not just domain mismatch, but also label
mismatch.

1 Introduction

There are two main approaches to named entity recog-
nition (NER): (i) build sequence labelling models
such as conditional random fields (CRFs) (Lafferty et
al., 2001) on a large manually-labelled training cor-
pus (Finkel et al., 2005); and (ii) exploit knowledge
bases to recognise mentions of entities in text (Rizzo
and Troncy, 2012; Mendes et al., 2011). For many
social media-based or security-related applications,
however, we cannot assume that we will have ac-
cess to either of these. An alternative is to have a
small amount of in-domain training data and access
to large-scale annotated data in a second domain, and
perform transfer learning over both the features and
label set. This is the problem setting in this paper.

NER of novel named entity (NE) types poses two
key challenges. First is the issue of sourcing labelled
training data. Handcrafted features play a key role
in supervised NER models (Turian et al., 2010), but
if we have only limited training amounts of training

data, we will be hampered in our ability to reliably
learn feature weights. Second, the absence of target
NE types in the source domain makes transfer diffi-
cult, as we cannot directly apply a model trained over
the source domain to the target domain. Alvarado
et al. (2015) show that even if the NE label set is
identical across domains, large discrepancies in the
label distribution can lead to poor performance.

Despite these difficulties, it is possible to transfer
knowledge between domains, as related NE types
often share lexical and context features. For example,
the expressions give lectures and attend tutorials of-
ten occur near mentions of NE types PROFESSOR
and STUDENT. If only PROFESSOR is observed
in the source domain but we can infer that the two
classes are similar, we can leverage the training data
to learn an NER model for STUDENT. In practice,
differences between NE classes are often more sub-
tle than this, but if we can infer, for example, that
the novel NE type STUDENT aligns with NE types
PERSON and UNIVERSITY, we can compose the
context features of PERSON and UNIVERSITY to
induce a model for STUDENT.

In this paper, we propose a transfer learning-based
approach to NER in novel domains with label mis-
match over a source domain. We first train an NER
model on a large source domain training corpus, and
then learn the correlation between the source and tar-
get NE types. In the last step, we reuse the model
parameters of the second step to initialise a linear-
chain CRF and fine tune it to learn domain-specific
patterns. We show that our methods achieve up to
160% improvement in F-score over a strong baseline,
based on only 125 target-domain training sentences.

899



2 Related work

The main scenario where transfer learning has been
applied to NER is domain adaptation (Arnold et al.,
2008; Maynard et al., 2001; Chiticariu et al., 2010),
where it is assumed that the label set Y is the same
for both the source and target corpora, and only the
domain varies. In our case, however, both the domain
and the label set differ across datasets.

Similar to our work, Kim et al. (2015) use transfer
learning to deal with NER data sets with different
label distributions. They use canonical correlation
analysis (CCA) to induce label representations, and
reduce the problem to one of domain adaptation. This
supports two different label mappings: (i) to a coarse
label set by clustering vector representations of the
NE types, which are combined with mention-level
predictions over the target domain to train a target
domain model; and (ii) between labels based on the
k nearest neighbours of each label type, and from
this transferring a pre-trained model from the source
to the target domain. They showed their automatic
label mapping strategies attain better results than
a manual mapping, with the pre-training approach
achieving the best results. Similar conclusions were
reached by Yosinski et al. (2014), who investigated
the transferability of features from a deep neural net-
work trained over the ImageNet data set. Sutton and
McCallum (2005) investigated how the target task
affects the source task, and demonstrated that decod-
ing for transfer is better than no transfer, and joint
decoding is better than cascading decoding.

Another way of dealing with a lack of annotated
NER data is to use distant supervision by exploiting
knowledge bases to recognise mentions of entities
(Ling and Weld, 2012; Dong et al., 2015; Yosef et
al., 2013; Althobaiti et al., 2015; Yaghoobzadeh and
Schütze, 2015). Having a fine-grained entity typol-
ogy has been shown to improve other tasks such as re-
lation extraction (Ling and Weld, 2012) and question
answering (Lee et al., 2007). Nevertheless, for many
social media-based or security-related applications,
we don’t have access to a high-coverage knowledge
base, meaning distant supervision is not appropriate.

3 Transfer Learning for NER

Our proposed approach TransInit consists of three
steps: (1) we train a linear-chain CRF on a large

source-domain corpus; (2) we learn the correlation
between source NE types and target NE types using
a two-layer neural network; and (3) we leverage the
neural network to train a CRF for target NE types.

Given a word sequence x of length L, an NER
system assigns each word xi a label yi ∈ Y , where
the label space Y includes all observed NE types and
a special category O for words without any NE type.
Let (x,y) be a sequence of words and their labels. A
linear-chain CRF takes the form:

1

Z

L∏

l=1

exp

(
Wff(yl,x) +W

gg(yl−1, yl)
)
, (1)

where f(yl,x) is a feature function depending only
on x, and the feature function g(yl−1, yl) captures
co-occurrence between adjunct labels. The feature
functions are weighted by model parameters W, and
Z serves as the partition function for normalisation.

The source domain model is a linear-chain CRF
trained on a labelled source corpus. The co-
occurrence of target domain labels is easy to learn
due to the small number of parameters (|Y |2). Mostly
such information is domain specific so that it is un-
likely that the co-occurrence of two source types
can be matched to the co-occurrence of the two tar-
get types. However the feature functions f(yl,x)
capture valuable information about the textual pat-
terns associated with each source NE type. Without
g(yl−1, yl), the linear-chain CRF is reduced to a lo-
gistic regression (LR) model:

σ(y∗,xi;Wf ) =
exp(Wf.y∗f(y

∗
i ,xi))∑

y∈Y exp(W
f
.yf(y,xi))

. (2)

In order to learn the correlation between source
and target types, we formulate it as a predictive task
by using the unnormalised probability of source types
to predict the target types. Due to the simplifica-
tion discussed above, we are able to extract a linear
layer from the source domain, which takes the form
ai = W

sxi, where Ws denotes the parameters of
f(yl,x) in the source domain model, and each ai
is the unnormalised probability for each source NE
type. Taking ai as input, we employ a multi-class LR
classifier to predict target types, which is essentially
p(y′|a) = σ(y′,ai;Wt), where y′ is the observed
type. From another point of view, the whole architec-
ture is a neural network with two linear layers.

900



We do not add any non-linear layers between
these two linear layers because we otherwise end
up with saturated activation functions. An activa-
tion function is saturated if its input values are its
max/min values (Glorot and Bengio, 2010). Taking
tanh(x) as an example, ∂tanh(z)∂z = 1 − tanh2(z).
If z is, for example, larger than 2, the correspond-
ing derivative is smaller than 0.08. Assume that
we have a three-layer neural network where zi de-
notes the input of layer i, tanh(z) is the middle
layer, and L(zi−2) is the loss function. We then
have ∂L(z

i−2)
∂zi−2 =

∂L
∂zi+1

∂ tanh(zi−1)
∂zi−1

∂zi−1
∂zi−2 . If the tanh

layer is saturated, the gradient propagated to the lay-
ers below will be small, and no learning based on
back propagation will occur.

If no parameter update is required for the bottom
linear layer, we will also not run into the issue of
saturated activation functions. However, in our ex-
periments, we find that parameter update is neces-
sary for the bottom linear layer because of covariate
shift (Sugiyama et al., 2007), which is caused by dis-
crepancy in the distribution between the source and
target domains. If the feature distribution differs be-
tween domains, updating parameters is a straightfor-
ward approach to adapt the model for new domains.

Although the two-layer neural network is capable
of recognising target NE types, it has still two draw-
backs. First, unlike a CRF, it doesn’t include a label
transition matrix. Second, the two-layer neural net-
work has limited capacity if the domain discrepancy
is large. If we rewrite the two-layer architecture in a
compact way, we obtain:

p(y′|x) = σ(y′,xi;WtWs). (3)

As the equation suggests, if we minimize the negative
log likelihood, the loss function is not convex. Thus,
we could land in a non-optimal local minimum using
online learning. The pre-trained parameter matrix
Ws imposes a special constraint that the computed
scores for each target type are a weighted combina-
tion of updated source type scores. If a target type
shares nothing in common with source types, the
pre-trained Ws does more harm than good.

In the last step, we initialise the model parameters
of a linear-chain CRF for f(yl,x) using the model
parameters from the previous step. Based on the
architecture of the NN model, we can collapse the

two linear transformations into one by:

Wf = WtWs, (4)

while initialising the other parameters of the CRF
to zero. After this transformation, each initialised
parameter vector Wf.y is a weighted linear combina-
tion of the updated parameter vectors of the source
types. Compared to the second step, the loss func-
tion we have now is convex because it is exactly a
linear-chain CRF. Our previous steps have provided
guided initialization of the parameters by incorpo-
rating source domain knowledge. The model also
has significantly more freedom to adapt itself to the
target types. In other words, collapsing the two ma-
trices simplifies the learning task and removes the
constraints imposed by the pre-trained Ws.

Because the tokens of the class O are generally
several orders of magnitude more frequent than the
tokens of the NE types, and also because of covariate
shift, we found that the predictions of the NN mod-
els are biased towards the class O (i.e. a non-NE).
As a result, the parameters of each NE type will al-
ways include or be dominated by the parameters of
O after initialisation. To ameliorate this effect, we
renormalise Wt before applying the transformation,
as in Equation (4). We do not include the parameters
of the source class O when we initialise parameters
of the NE types, while copying the parameters of the
source class O to the target class O. In particular, let
o be the index of source domain class O. For each
parameter vector Wti∗ of NE type, we set W

t
io = 0.

For the parameter vector for the target class O, we
set only the element corresponding to the weight be-
tween source type O and target class O to 1, and
other elements to 0.

Finally, we fine-tune the model over the target do-
main by maximising log likelihood. The training
objective is convex, and thus the local optimum is
also the global optimum. If we fully train the model,
we will achieve the same model as if we trained from
scratch over only the target domain. As the knowl-
edge of the source domain is hidden in the initial
weights, we want to keep the initial weights as long
as they contribute to the predictive task. Therefore,
we apply AdaGrad (Rizzo and Troncy, 2012) with
early stopping based on development data, so that
the knowledge of the source domain is preserved as
much as possible.

901



18 54 125 268 553 1123 4543 18222
Training size

0.0

0.2

0.4

0.6

0.8

1.0

F1
-M

e
a
su

re

BOW
Embed
LabelEmbed
CCA
TransInit

(a) Target: I2B2, Source: BBN

18 54 125 268 553 1123 4543 18222
Training size

0.0

0.2

0.4

0.6

0.8

1.0

F1
-M

e
a
su

re

BOW
Embed
LabelEmbed
CCA
TransInit

(b) Target: I2B2, Source: CoNLL

18 54 125 268 553 1123 4543 18222
Training size

0.0

0.2

0.4

0.6

0.8

1.0

F1
-M

e
a
su

re

BOW
Embed
LabelEmbed
CCA
TransInit

(c) Target: CADEC, Source: CoNLL

Figure 1: Macro-averaged F1 results across all novel classes on different source/target domain combinations

4 Experimental Setup

4.1 Datasets

We use CADEC (Karimi et al., 2015) and I2B2
(Ben Abacha and Zweigenbaum, 2011) as target cor-
pora with the standard training and test splits. From
each training set, we hold out 10% as the devel-
opment set. As source corpora, we adopt CoNLL
(Tjong Kim Sang and De Meulder, 2003) and BBN
(Weischedel and Brunstein, 2005).

In order to test the impact of the target domain
training data size on results, we split the training set
of CADEC and I2B2 into 10 partitions based on a log
scale, and created 10 successively larger training sets
by merging these partitions from smallest to largest
(with the final merge resulting in the full training set).
For all methods, we report the macro-averaged F1
over only the NE classes that are novel to the target
domain.

4.2 Baselines

We compare our methods with the following two
in-domain baselines, one cross-domain data-based
method, and three cross-domain transfer-based
benchmark methods.

BOW: an in-domain linear-chain CRF with hand-
crafted features, from Qu et al. (2015).

Embed: an in-domain linear-chain CRF with hand-
crafted features and pre-trained word embeddings,
from Qu et al. (2015).

LabelEmbed: take the labels in the source and tar-
get domains, and determine the alignment based on
the similarity between the pre-trained embeddings
for each label.

CCA: the method of Kim et al. (2015), where a
one-to-one mapping is generated between source and
target NE classes using CCA and k-NN (see Sec-
tion 2).

TransDeepCRF: A three-layer deep CRF. The bot-
tom layer is a linear layer initialised with Ws from
the source domain-trained CRF. The middle layer is
a hard tanh function (Collobert et al., 2011). The
top layer is a linear-chain CRF with all parameters
initialised to zero.

TwoLayerCRF: A two-layer CRF. The bottom
layer is a linear layer initialised with Ws from
the source domain-trained CRF. The top layer is a
linear-chain CRF with all parameters initialised to
zero.

We compare our method with one variation, which
is to freeze the parameters of the bottom linear layer
and update only the parameters of the LR classifier
while learning the correlation between the source and
target types.

4.3 Experimental Results

Figure 1 shows the macro-averaged F1 of novel
types between our method TransInit and the three
baselines on all target corpora. The evaluation re-
sults on CADEC with BBN as the source corpus are
not reported here because BBN contains all types of
CADEC. From the figure we can see that TransInit
outperforms all other methods with a wide margin
on I2B2. When CoNLL is taken as the source cor-
pus, despite not sharing any NE types with I2B2,
several target types are subclasses of source types:
DOCTOR and PATIENT w.r.t. PERSON, and HOS-

902



PITAL w.r.t. ORGANIZATION.
In order to verify if TransInit is able to capture

semantic relatedness between source and target NE
types, we inspected the parameter matrix Wt of the
LR classifier in the step of learning type correlations.
The corresponding elements in Wt indeed receive
much higher values than the semantically-unrelated
NE type pairs. When less than 300 target training
sentences are used, these automatically discovered
positive correlations directly lead to 10 times higher
F1 scores for these types than the baseline Embed,
which does not have a transfer learning step. Since
TransInit is able to transfer the knowledge of multi-
ple source types to related target types, this advantage
leads to more than 10% improvement in terms of F1
score on these types compared with LabelEmbed,
given merely 268 training sentences in I2B2. We
also observe that, in case of few target training exam-
ples, LabelEmbed is more robust than CCA if the
correlation of types can be inferred from their names.

We study the effects of transferring a large num-
ber of source types to target types by using BBN,
which has 64 types. Here, the novel types of I2B2
w.r.t. BBN are DOCTOR, PATIENT, HOSPITAL,
PHONE, and ID. For these types, TransInit success-
fully recognises PERSON as the most related type
to DOCTOR, as well as CARDINAL as the most
related type to ID. In contrast, CCA often fails to
identify meaningful type alignments, especially for
small training data sizes.
CADEC is definitely the most challenging task

when trained on CoNLL, because there is no se-
mantic connection between two of the target NE
types (DRUG and DISEASE) and any of the source
NE types. In this case, the baseline LabelEmbed
achieves competitive results with TransInit. This
suggests that the class names reflect semantic corre-
lations between source and target types, and there are
not many shared textual patterns between any pair of
source and target NE types in the respective datasets.

Even with a complex model such as a neural net-
work, the transfer of knowledge from the source types
to the target types is not an easy task. Figure 2 shows
that with a three-layer neural network, the whole
model performs poorly. This is due to the fact that
the hard tanh layer suffers from saturated function
values. We inspected the values of the output hidden

18 54 125 268 553 1123 4543 18222
Training size

0.0

0.2

0.4

0.6

0.8

1.0

F1
-M

e
a
su

re

DeepCRF
TwoLayerCRF
TransInit_NonUpdate

TransInit

Figure 2: Difficulty of Transfer. The source model is
trained on BBN.

units computed by Wsx on a random sample of tar-
get training examples before training on the target
corpora. Most values are either highly positive or
negative, which is challenging for online learning
algorithms. This is due to the fact that these hid-
den units are unnormalised probabilities produced
by the source domain classifier. Therefore, remov-
ing the hidden non-linear-layer layer leads to a dra-
matic performance improvement. Moreover, Figure 2
also shows that further performance improvement is
achieved by reducing the two-layer architecture into
a linear chain CRF. And updating the hidden layers
leads to up to 27% higher F1 scores than not updating
them in the second step of TransInit, which indicates
that the neural networks need to update lower-level
features to overcome the covariate shift problem.

5 Conclusion

We have proposed TransInit, a transfer learning-
based method that supports the training of NER mod-
els across datasets where there are mismatches in
domain and also possibly the label set. Our method
was shown to achieve up to 160% improvement in
F1 over competitive baselines, based on a handful of
in-domain training instances.

Acknowledgments

This research was supported by NICTA, funded by
the Australian Government through the Department
of Communications and the Australian Research
Council through the ICT Centre of Excellence Pro-
gram.

903



References

Maha Althobaiti, Udo Kruschwitz, and Massimo Poesio.
2015. Combining minimally-supervised methods for
arabic named entity recognition. Transactions of the
Association for Computational Linguistics, 3:243–255.

Julio Cesar Salinas Alvarado, Karin Verspoor, and Timo-
thy Baldwin. 2015. Domain adaption of named entity
recognition to support credit risk assessment. In Aus-
tralasian Language Technology Association Workshop
2015.

Andrew Arnold, Ramesh Nallapati, and W. William Co-
hen. 2008. Exploiting feature hierarchy for transfer
learning in named entity recognition. In Proceedings
of ACL-08: HLT, pages 245–253.

Asma Ben Abacha and Pierre Zweigenbaum. 2011. Med-
ical entity recognition: A comparison of semantic and
statistical methods. In Proceedings of BioNLP 2011
Workshop, pages 56–64.

Laura Chiticariu, Rajasekar Krishnamurthy, Yunyao Li,
Frederick Reiss, and Shivakumar Vaithyanathan. 2010.
Domain adaptation of rule-based annotators for named-
entity recognition tasks. In Proceedings of the 2010
Conference on Empirical Methods in Natural Language
Processing, pages 1002–1012.

Ronan Collobert, Jason Weston, Léon Bottou, Michael
Karlen, Koray Kavukcuoglu, and Pavel Kuksa. 2011.
Natural language processing (almost) from scratch.
Journal of Machine Learning Research, 12:2493–2537.

Li Dong, Furu Wei, Hong Tan, Sun, Ming Zhou, and
Ke Xu. 2015. A hybrid neural model for type classi-
fication of entity mentions. In Twenty-Fourth Interna-
tional Joint Conference on Artificial Intelligence (IJ-
CAI), pages 1243–1249.

Jenny Rose Finkel, Trond Grenager, and Christopher Man-
ning. 2005. Incorporating non-local information into
information extraction systems by Gibbs sampling. In
Proceedings of the 43rd Annual Meeting on Association
for Computational Linguistics, pages 363–370.

Xavier Glorot and Yoshua Bengio. 2010. Understanding
the difficulty of training deep feedforward neural net-
works. In Proceedings of the Thirteenth International
Conference on Artificial Intelligence and Statistics (AIS-
TATS 2010), pages 249–256.

Sarvnaz Karimi, Alejandro Metke-Jimenez, Madonna
Kemp, and Chen Wang. 2015. Cadec: A corpus of
adverse drug event annotations. Journal of Biomedical
Informatics, 55:73–81.

Young-Bum Kim, Karl Stratos, Ruhi Sarikaya, and Min-
woo Jeong. 2015. New transfer learning techniques
for disparate label sets. In Proceedings of the 53rd
Annual Meeting of the Association for Computational
Linguistics and the 7th International Joint Conference

on Natural Language Processing of the Asian Feder-
ation of Natural Language Processing, (ACL 2015),
pages 473–482.

John Lafferty, Andrew McCallum, and Fernando Pereira.
2001. Conditional random fields: Probabilistic models
for segmenting and labeling sequence data. In Proceed-
ings of the 18th International Conference on Machine
Learning, pages 282–289.

Changki Lee, Yi-Gyu Hwang, and Myung-Gil Jang. 2007.
Fine-grained named entity recognition and relation ex-
traction for question answering. In Proceedings of the
30th Annual International ACM SIGIR Conference on
Research and Development in Information Retrieval,
pages 799–800.

Xiao Ling and Daniel S. Weld. 2012. Fine-grained entity
recognition. In Proceedings of the 26th AAAI Confer-
ence on Artificial Intelligence.

Diana Maynard, Valentin Tablan, Cristian Ursu, Hamish
Cunningham, and Yorick Wilks. 2001. Named en-
tity recognition from diverse text types. In Recent
Advances in Natural Language Processing 2001 Con-
ference.

Pablo N Mendes, Max Jakob, Andrés Garcı́a-Silva, and
Christian Bizer. 2011. DBpedia spotlight: shedding
light on the web of documents. In Proceedings of
the 7th International Conference on Semantic Systems,
pages 1–8.

Lizhen Qu, Gabriela Ferraro, Liyuan Zhou, Weiwei Hou,
Nathan Schneider, and Timothy Baldwin. 2015. Big
data small data, in domain out-of domain, known word
unknown word: The impact of word representations
on sequence labelling tasks. In Proceedings of the
19th Conference on Computational Natural Language
Learning (CoNLL 2015), pages 83–93.

Giuseppe Rizzo and Raphaël Troncy. 2012. NERD: a
framework for unifying named entity recognition and
disambiguation extraction tools. In Proceedings of the
Demonstrations at the 13th Conference of the Euro-
pean Chapter of the Association for Computational
Linguistics, pages 73–76.

Masashi Sugiyama, Matthias Krauledat, and Klaus-Robert
Müller. 2007. Covariate shift adaptation by importance
weighted cross validation. Journal of Machine Learn-
ing Research, 8:985–1005.

Charles Sutton and Andrew McCallum. 2005. Composi-
tion of conditional random fields for transfer learning.
In Proceedings of the Conference on Human Language
Technology and Empirical Methods in Natural Lan-
guage Processing, HLT ’05, pages 748–754.

Erik F. Tjong Kim Sang and Fien De Meulder. 2003. In-
troduction to the CoNLL-2003 shared task: Language-
independent named entity recognition. In Proceedings
of CoNLL-2003, pages 142–147.

904



Joseph Turian, Lev Ratinov, and Yoshua Bengio. 2010.
Word representations: a simple and general method for
semi-supervised learning. In Proceedings of the 48th
annual meeting of the association for computational
linguistics, pages 384–394.

Ralph Weischedel and Ada Brunstein. 2005. BBN pro-
noun coreference and entity type corpus. Linguistic
Data Consortium.

Yadollah Yaghoobzadeh and Hinrich Schütze. 2015.
Corpus-level fine-grained entity typing using contextual
information. In Proceedings of the 2015 Conference
on Empirical Methods in Natural Language Processing
(EMNLP 2015), pages 715–725.

Mohamed Amir Yosef, Sandro Bauer, Johannes Hoffart,
Marc Spaniol, and Gerhard Weikum. 2013. HYENA-
live: Fine-grained online entity type classification from
natural-language text. In Proceedings of the 51st An-
nual Meeting of the Association for Computational Lin-
guistics: System Demonstrations, pages 133–138.

Jason Yosinski, Jeff Clune, Yoshua Bengio, and Hod Lip-
son. 2014. How transferable are features in deep neural
networks? In Advances in Neural Information Process-
ing Systems 27, pages 3320–3328.

905


