



















































Linguistically Informed Tweet Categorization for Online Reputation Management


Proceedings of the 5th Workshop on Computational Approaches to Subjectivity, Sentiment and Social Media Analysis, pages 73–78,
Baltimore, Maryland, USA. June 27, 2014. c©2014 Association for Computational Linguistics

Linguistically Informed Tweet Categorization for Online Reputation
Management

Gerard Lynch and Pádraig Cunningham
Centre for Applied Data Analytics Research

(CeADAR)
University College Dublin

Belfield Office Park
Dublin 4, Ireland

firstname.lastname@ucd.ie

Abstract

Determining relevant content automati-
cally is a challenging task for any ag-
gregation system. In the business intel-
ligence domain, particularly in the appli-
cation area of Online Reputation Manage-
ment, it may be desirable to label tweets
as either customer comments which de-
serve rapid attention or tweets from in-
dustry experts or sources regarding the
higher-level operations of a particular en-
tity. We present an approach using a com-
bination of linguistic and Twitter-specific
features to represent tweets and examine
the efficacy of these in distinguishing be-
tween tweets which have been labelled
using Amazon’s Mechanical Turk crowd-
sourcing platform. Features such as part-
of-speech tags and function words prove
highly effective at discriminating between
the two categories of tweet related to sev-
eral distinct entity types, with Twitter-
related metrics such as the presence of
hashtags, retweets and user mentions also
adding to classification accuracy. Accu-
racy of 86% is reported using an SVM
classifier and a mixed set of the aforemen-
tioned features on a corpus of tweets re-
lated to seven business entities.

1 Motivation

Online Reputation Management (ORM) is a grow-
ing field of interest in the domain of business in-
telligence. Companies and individuals alike are
highly interested in monitoring the opinions of
others across social and traditional media and this
information can have considerable business value
for corporate entities in particular.

1.1 Challenges

There are a number of challenges in creating an
end-to-end software solution for such purposes,
and several shared tasks have already been estab-
lished to tackle these issues1. The most recent
RepLab evaluation was concerned with four tasks
related to ORM, filtering, polarity for reputation,
topic detection and priority assignment. Based
on these evaluations, it is clear that although the
state of the art of topic-based filtering of tweets is
relatively accomplished (Perez-Tellez et al., 2011;
Yerva et al., 2011; Spina et al., 2013), other as-
pects of the task such as sentiment analysis and
prioritisation of tweets based on content are less
trivial and require further analysis.

Whether Twitter mentions of entities are ac-
tual customer comments or in fact represent the
views of traditional media or industry experts and
sources is an important distinction for ORM sys-
tems. With this study we investigate the degree to
which this task can be automated using supervised
learning methods.

2 Related Work

2.1 Studies on Twitter data

While the majority of research in the computa-
tional sciences on Twitter data has focused on is-
sues such as topic detection (Cataldi et al., 2010),
event detection, (Weng and Lee, 2011; Sakaki
et al., 2010), sentiment analysis, (Kouloumpis et
al., 2011), and other tasks based primarily on the
topical and/or semantic content of tweets, there
is a growing body of work which investigates
more subtle forms of information represented in
tweets, such as reputation and trustworthiness,
(O’Donovan et al., 2012), authorship attribution
(Layton et al., 2010; Bhargava et al., 2013) and
Twitter spam detection, (Benevenuto et al., 2010).

1See (Amigó et al., 2012) and (Amigó et al., 2013) for
details of the RepLab series

73



These studies combine Twitter-specific and textual
features such as retweet counts, tweet lengths and
hashtag frequency, together with sentence-length,
character n-grams and punctuation counts.

2.2 Studies on non-Twitter data
The textual features used in our work such
as n-grams of words and parts-of-speech have
been used for gender-based language classifica-
tion (Koppel et al., 2002), social profiling and per-
sonality type detection (Mairesse et al., 2007), na-
tive language detection from L2 text, (Brooke and
Hirst, 2012) translation source language detection,
(van Halteren, 2008; Lynch and Vogel, 2012) and
translation quality detection, (Vogel et al., 2013).

3 Experimental setup and corpus

Tweets were gathered between June 2013 and Jan-
uary 2014 using the twitter4j Java library. A lan-
guage detector was used to filter only English-
language tweets.2 The criteria for inclusion were
that the entity name was present in the tweet. The
entities focused on in this study had relatively un-
ambigious business names, so no complex filtering
was necessary.

3.1 Pilot study
A smaller pilot study was carried out before the
main study in order to examine response quality
and accuracy of instruction. Two hundred sam-
ple tweets concerning two airlines3 were anno-
tated using Amazon’s Mechanical Turk system by
fourteen Master annotators. After annotation, we
selected the subset (72%) of tweets for which both
annotators agreed on the category to train the clas-
sifier. During the pilot study, the tweets were
pre-processed4 to remove @ and # symbols and
punctuation to treat account names and hashtags
as words. Hyperlinks representations were main-
tained within the tweets. The Twitter-specific met-
rics were not employed in the pilot study.

3.2 Full study
In the full study, 2454 tweets concerning seven
business entities5 were tagged by forty annota-
tors as to whether they corresponded to one of the

2A small amount of non-English tweets were found in the
dataset, these were assigned to the Other category.

3Aer Lingus and Ryanair
4This was not done in the full study, these symbols were

counted and used as features.
5Aer Lingus, Ryanair, Bank of Ireland, C & C Group,

Permanent TSB, Glanbia, Greencore

three categories described in Section 1.1. For 57%
of the tweets, annotators agreed on the categories
with disagreement in the remaining 43%. The dis-
puted tweets were annotated again by two anno-
tators. From this batch, a similar proportion were
agreed on. For the non-agreed tweets in the sec-
ond round, a majority category vote was reached
by combining the four annotations over the first
and second rounds. After this process, roughly
two hundred tweets remained as ambiguous (each
having two annotations for one of two particular
categories) and these were removed from the cor-
pus used in the experiments.

3.3 Category breakdown

Table 5 displays the number of tweets for which
no majority category agreement was reached. The
majority disagreement class across all entities are
texts which have been labelled as both business
operations and other. For the airline entities, a
large proportion of tweets were annotated as both
customer comment and other, this appeared to be
a categorical issue which may have required clar-
ification in the instructions. The smallest cate-
gory for tied agreement is customer comment and
business operations, it appears that the distinc-
tion between these categories was clearer based
on the data provided to annotators. 2078 tweets
were used in the final experiments. The classes
were somewhat imbalanced for the final corpus,
the business operations category was the largest,
with 1184 examples, customer comments con-
tained 585 examples and the other category con-
tained 309 examples.

3.4 Feature types

The features used for classification purposes can
be divided into the following two categories:

1. Twitter-specific:

• Tweet is a retweet or not
• Tweet contains a mention
• Tweet contains a hashtag or a link
• Weight measure (See Fig 3)
• Retweet account for a tweet.

2. Linguistic: The linguistic features are based
on the textual content of the tweet repre-
sented as word unigrams, word bigrams and
part-of-speech bigrams.

74



We used TagHelperTools, (Rosé et al., 2008) for
textual feature creation which utilises the Stanford
NLP toolkit for NLP annotation and returns for-
matted representations of textual features which
can be employed in the Weka toolkit which imple-
ments various machine learning algorithms. All
linguistic feature frequencies were binarised in our
representations6.

4 Results

4.1 Pilot study

Using the Naive Bayes classifier in the Weka
toolkit and a feature set consisting of 130 word
tokens, 80% classification accuracy was obtained
using ten-fold cross validation on the full set of
tweets . Table 1 shows the top word features when
ranked using 10-fold cross validation and the in-
formation gain metric for classification power over
the three classes. Using the top 50 ranked POS-
bigram features alone, 74% classification accuracy
was obtained using the Naive Bayes classifier. Ta-
ble 2 shows the top twenty features, again ranked
by information gain.

Combining the fifty POS-bigrams and the 130
word features, we obtained 84% classification ac-
curacy using the Naive Bayes classifier. Accuracy
was improved by removing all noun features from
the dataset and using the top seventy five features
from the remaining set ranked with information
gain, resulting in 86.6% accuracy using the SVM
classifier with a linear kernel. Table 3 displays the
top twenty combined features.

Rank Feature Rank Feature
1 http 11 investors
2 flight 12 would
3 talks 13 by
4 for 14 says
5 strike 15 profit
6 an 16 cabin
7 you 17 crew
8 I 18 via
9 that 19 at
10 action 20 since

Table 1: Top 20 ranked word features for pilot
study

61 if feature is present in a tweet, otherwise 0.

Rank Feature Rank Feature
1 NNP EOL 11 VB PRP
2 VBD JJ 12 NN NNS
3 NNP VBD 13 IN PRP$
4 NNP NN 14 BOL CD
5 BOL PRP 15 BOL JJS
6 VBD NNP 16 IN VBN
7 NNP CC 17 PRP$ JJ
8 TO NNP 18 PRP MD
9 NN RB 19 PRP$ VBG

10 RB JJ 20 CC VBP

Table 2: Top 20 ranked POS bigram features for
pilot study

Rank Feature Rank Feature
1 http 11 TO NNP
2 NNP EOL 12 RB JJ
3 NNP VBD 13 that
4 VBD JJ 14 tells
5 NNP NN 15 way
6 BOL PRP 16 I
7 VBD NNP 17 would
8 NNP CC 18 you
9 for 19 NN RB

10 an 20 BOL JJS

Table 3: Top 20 ranked combined features for pilot
study

4.2 Full study
4.2.1 Results
Using the SMO classifier, Weka’s support vec-
tor machine implementation using a linear kernel,
a hybrid feature set containing linguistic, custom
and Twitter-specific features obtained 72% clas-
sification accuracy for the three categories. F-
measures were highest for the business operations
class, and lowest for the other class, which con-
tained the most diversity. Examining Figure 2, it
is clear that f-measures for the other class are al-
most zero. This indicates that tweets given this
category may not be homogeneous enough to cat-
egorise using the features defined in Table 7.

4.3 Two classes
After the removal of the other class from the
experiment, the same feature set obtained 86%
classification accuracy between the two remain-
ing classes. The distinguishing features consisted
predominantly of pronouns (I, me, my), part-of-

75



Entity BO CC Other
Aer Lingus 174 138 44
Ryanair 58 212 52
AIB 69 29 43
BOI 208 85 40
C&C 45 14 15
Glanbia 276 39 46
Greencore 37 4 13
Kerry Group 158 10 36
Permanent TSB 160 54 20

Table 4: Tweets per entity by category: Majority
agreement

Entity CC+BO O-CC O-BO
Aer Lingus 4 24 15
Ryanair 7 30 8
AIB 4 5 11
BOI 9 5 16
C&C 0 1 3
Glanbia 7 4 19
Greencore 0 0 2
Kerry Group 5 2 12
Permanent TSB 3 6 10

Table 5: Tweets per entity by category: Tied
agreement

speech bigrams including pairs of plural nouns,
lines beginning with prepositions and function
words (so, just, new, it). Business operations
tweets were more likely to mention a user account
or be a retweet, personal pronouns were more
commonplace in customer comments and as ob-
served in the pilot study, customer comments were
more likely to begin with a preposition and busi-
ness operations tweets were more likely to contain
noun-noun compounds and pairs of coordinating
conjunctions and nouns.

4.4 Features

Hashtags were slightly more common in business
operations tweets, however the number of hash-
tags was not counted, simply whether at least one
was present. Hashtags as a proportion of words
might be a useful feature for further studies. Func-
tion words and POS tags were highly discrimina-
tory, indicating that this classifier may be applica-
ble to different topic areas. Weight (See Figure 3)
was a distinguishing feature, with business opera-
tions tweets having higher weight scores, reflect-

Figure 1: F-scores by category for pilot study

Figure 2: F-scores by category for full study

ing the tendency for these tweets to originate from
Twitter accounts linked to news sources or influ-
ential industry experts.

5 Results per sub-category

To investigate whether the entity domain had a
bearing on the results, we separated the data into
three subsets, airlines, banks and food industry
concerns. We performed the same feature selec-
tion as in previous experiments, calculating each
feature type separately, removing proper nouns,
hashtags and account names from the word n-
grams, then combining and ranking the features
using ten-fold cross validation and information
gain. The SVM classifier reported similar results
to the main study on the three class problem for
each sub-domain, and for the two class problem
results ranged between 86-87% accuracy, similar

Number of followers
Number following

(retweets)

Figure 3: Twitter weight metric

76



to the results on the mixed set7. Thus, we be-
lieve that the individual subdomains do not war-
rant different classifiers for the problem, indeed
examining the top 20-ranked features for each sub-
domain, there is a large degree of overlap, as seen
in bold and italics in Table 6.

Banks Airlines Food
@ @ @
my NNP NNP PRP VBP

i i i
me BOL IN BOL IN

PRP VBP PRP VBP VB PRP
account DT NN BOL PRP

NNP VBZ IN PRP HASHASH
VB PRP the you
IN PRP new me

you PRP VBD know
BOL RB NNP VBZ my

RB JJ IN DT i know
NNP NNP you PRP CC
PRP VBD BOL PRP used
my bank ISRT BOL CC
DT NN it NNP CD

NN PRP me NN NNP
VBD PRP my CC PRP
BOL IN RB RB ISRT

i’m so CC NNP

Table 6: Top twenty ranked features by Informa-
tion Gain for three domains

6 Conclusions and future directions

6.1 Classification results
We found that accurate categorization of our pre-
defined tweet types was possible using shallow
linguistic features. This was aided by Twitter spe-
cific metrics but these did not add significantly to
the classification accuracy8. The lower score (72-
73%) in the three class categorization problem is
due to the linguistic diversity of the other tweet
category.

6.2 Annotation and Mechanical Turk
We found the definition of categorization criteria
to be an important and challenging step when us-
ing Mechanical Turk for annotation. The high de-
gree of annotator disagreement reflected this, how-
ever it is important to note that in many cases,
tweets fit equally into two or more of our defined
categories. The use of extra annotations9 allowed
for agreement to be reached in the majority of

7The food subset was highly imbalanced however, con-
taining only 43 customer comments and 313 business opera-
tions tweets, the other two subsets were relatively balanced.

8ca. 2% decrease in accuracy on removal.
9over the initial two annotators

cases, however employing more evaluations could
have also resulted in deadlock. Examples of am-
biguous tweets included: Cheap marketing tactics.
Well, if it ain’t broke, why fix it! RT @Ryanair’s
summer ’14 schedule is now on sale! where a
Twitter user has retweeted an official announce-
ment and added their own comment.

Another possible pitfall is that as Mechanical
Turk is a US-based service and requires workers to
have a US bank account in order to perform work,
Turkers tend to be US-based, and therefore an an-
notation task concerning non-US business entities
is perhaps more difficult without sufficient back-
ground awareness of the entities in question.

Future experiments will apply the methodology
developed here to a larger dataset of tweets, one
candidate would be the dataset used in the RepLab
2013 evaluation series which contains 2,200 an-
notated tweets for 61 business entities in four do-
mains.

Acknowledgments

The authors are grateful to Enterprise Ireland and
the IDA for funding this research and CeADAR
through their Technology Centre Programme.

Rank Feature Rank Feature
1 @ 26 NNP PRP
2 i 27 NN PRP
3 PRP VBP 28 VBP PRP
4 my 29 when
5 BOL IN 30 if
6 me 31 don’t
7 you 32 PRP MD
8 NNP NNP 33 they
9 IN PRP 34 like
10 VB PRP 35 PRP VB
11 PRP VBD 36 got
12 WEIGHT 37 CC NNP
13 so 38 but
14 NNP VBZ 39 RB IN
15 BOL PRP 40 RT
16 RB JJ 41 with
17 DT NN 42 PRP IN
18 BOL RB 43 a
19 it 44 NNS RB
20 PRP RB 45 CC PRP
21 RB RB 46 VBD PRP
22 IN DT 47 VBD DT
23 i’m 48 no
24 just 49 the
25 get 50 PRP$ NN

Table 7: Top 50 ranked mixed features for main
study

77



References
Enrique Amigó, Adolfo Corujo, Julio Gonzalo, Edgar

Meij, and Maarten de Rijke. 2012. Overview
of replab 2012: Evaluating online reputation man-
agement systems. In CLEF (Online Working
Notes/Labs/Workshop).

Enrique Amigó, Jorge Carrillo de Albornoz, Irina
Chugur, Adolfo Corujo, Julio Gonzalo, Tamara
Martı́n, Edgar Meij, Maarten de Rijke, and Dami-
ano Spina. 2013. Overview of replab 2013:
Evaluating online reputation monitoring systems.
In Information Access Evaluation. Multilinguality,
Multimodality, and Visualization, pages 333–352.
Springer.

Fabrıcio Benevenuto, Gabriel Magno, Tiago Ro-
drigues, and Virgılio Almeida. 2010. Detect-
ing spammers on twitter. In Collaboration, elec-
tronic messaging, anti-abuse and spam conference
(CEAS), volume 6.

Mudit Bhargava, Pulkit Mehndiratta, and Krishna
Asawa. 2013. Stylometric analysis for authorship
attribution on twitter. In Big Data Analytics, pages
37–47. Springer International Publishing.

Julian Brooke and Graeme Hirst. 2012. Measuring
interlanguage: Native language identification with
l1-influence metrics. In LREC, pages 779–784.

Mario Cataldi, Luigi Di Caro, and Claudio Schifanella.
2010. Emerging topic detection on twitter based on
temporal and social terms evaluation. In Proceed-
ings of the Tenth International Workshop on Multi-
media Data Mining, page 4. ACM.

Moshe Koppel, Shlomo Argamon, and Anat Rachel
Shimoni. 2002. Automatically categorizing writ-
ten texts by author gender. Literary and Linguistic
Computing, 17(4):401–412.

Efthymios Kouloumpis, Theresa Wilson, and Johanna
Moore. 2011. Twitter sentiment analysis: The good
the bad and the omg! In ICWSM.

Robert Layton, Paul Watters, and Richard Dazeley.
2010. Authorship attribution for twitter in 140 char-
acters or less. In Cybercrime and Trustworthy Com-
puting Workshop (CTC), 2010 Second, pages 1–8.
IEEE.

Gerard Lynch and Carl Vogel. 2012. Towards the au-
tomatic detection of the source language of a literary
translation. In COLING (Posters), pages 775–784.

François Mairesse, Marilyn A Walker, Matthias R
Mehl, and Roger K Moore. 2007. Using linguis-
tic cues for the automatic recognition of personality
in conversation and text. J. Artif. Intell. Res.(JAIR),
30:457–500.

John O’Donovan, Byungkyu Kang, Greg Meyer, To-
bias Hollerer, and Sibel Adalii. 2012. Credibility in

context: An analysis of feature distributions in twit-
ter. In Privacy, Security, Risk and Trust (PASSAT),
2012 International Conference on and 2012 Inter-
national Confernece on Social Computing (Social-
Com), pages 293–301. IEEE.

Fernando Perez-Tellez, David Pinto, John Cardiff, and
Paolo Rosso. 2011. On the difficulty of cluster-
ing microblog texts for online reputation manage-
ment. In Proceedings of the 2nd Workshop on Com-
putational Approaches to Subjectivity and Sentiment
Analysis, pages 146–152. Association for Computa-
tional Linguistics.

Carolyn Rosé, Yi-Chia Wang, Yue Cui, Jaime Ar-
guello, Karsten Stegmann, Armin Weinberger, and
Frank Fischer. 2008. Analyzing collaborative
learning processes automatically: Exploiting the ad-
vances of computational linguistics in computer-
supported collaborative learning. International
journal of computer-supported collaborative learn-
ing, 3(3):237–271.

Takeshi Sakaki, Makoto Okazaki, and Yutaka Matsuo.
2010. Earthquake shakes twitter users: real-time
event detection by social sensors. In Proceedings
of the 19th international conference on World wide
web, pages 851–860. ACM.

Damiano Spina, Julio Gonzalo, and Enrique Amigó.
2013. Discovering filter keywords for company
name disambiguation in twitter. Expert Systems with
Applications.

Hans van Halteren. 2008. Source language mark-
ers in europarl translations. In Proceedings of the
22nd International Conference on Computational
Linguistics-Volume 1, pages 937–944. Association
for Computational Linguistics.

Carl Vogel, Ger Lynch, Erwan Moreau, Liliana Ma-
mani Sanchez, and Phil Ritchie. 2013. Found in
translation: Computational discovery of translation
effects. Translation Spaces, 2(1):81–104.

Jianshu Weng and Bu-Sung Lee. 2011. Event detec-
tion in twitter. In ICWSM.

Surender Reddy Yerva, Zoltán Miklós, and Karl
Aberer. 2011. What have fruits to do with technol-
ogy?: the case of orange, blackberry and apple. In
Proceedings of the International Conference on Web
Intelligence, Mining and Semantics, page 48. ACM.

78


