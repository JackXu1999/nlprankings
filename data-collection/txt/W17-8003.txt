Proceedings of the Biomedical NLP Workshop associated with RANLP 2017, pages 15–23,

Varna, Bulgaria, 8 September 2017.

https://doi.org/10.26615/978-954-452-044-1_003

15

Discourse-Wide Extraction of Assay Frames from the Biological

Literature

Dayne Freitag
SRI International

9988 Hibert Street, Suite 203
San Diego, CA 92131, USA
freitag@ai.sri.com

Paul Kalmar

SRI International

9988 Hibert Street, Suite 203
San Diego, CA 92131, USA
paul.kalmar@sri.com

Eric Yeh

SRI International

333 Ravenswood Avenue

Menlo Park, CA 94025, USA

yeh@ai.sri.com

Abstract

We consider the problem of populating
multi-part knowledge frames from textual
information distributed over multiple sen-
tences in a document. We present a corpus
constructed by aligning papers from the
cellular signaling literature to a collection
of approximately 50,000 reference frames
curated by hand as part of a decade-long
project. We present and evaluate two ap-
proaches to the challenging problem of re-
constructing these frames, which formal-
ize biological assays described in the lit-
erature. One approach is based on clas-
sifying candidate records nominated by
sentence-local entity co-occurrence. In the
second approach, we introduce a novel vir-
tual register machine that traverses an arti-
cle and generates frames, trained on our
reference data. Our evaluations provide
evidence that best performance in the task
ultimately hinges on an integration of in-
formation distributed over multiple sen-
tences.

1

Introduction

Biological event and relation extraction have been
the focus of considerable study in recent years,
resulting in the availability of annotated cor-
pora (Kim et al., 2003; Pyysalo et al., 2007; Kim
et al., 2008; Thompson et al., 2009). In the interest
of replicability and progress on critical challenges,
such resources typically decompose the hard prob-
lem of factual understanding into several simpler
problems, such as entity recognition, binary rela-
tion detection, and co-reference resolution.

This methodology is subject to several criti-
cisms. The reliance on thorough annotation im-
poses overheads that prevent rapid progress. The

targeting of a ﬁxed set of simpliﬁed, typically
binary relations does justice neither to the com-
plexity of information expressed in a typical sen-
tence, nor to the biological processes under discus-
sion. And the methodology places an emphasis on
pieces of information amenable to expression in
individual sentences, leaving untouched informa-
tion that can be assembled only through traversal
of multiple sentences.

In this paper we address the problem of con-
structing multi-slot knowledge frames from the
technical literature on cellular signaling networks.
The frames in our study are a faithful representa-
tion of assays reported in this literature, called da-
tums, with only approximate localization to spe-
ciﬁc textual regions. We have no one-to-one map-
ping between frames and sentences, no guaran-
tee that the slots of a frame co-occur in a single
sentence, and no universal presentational conven-
tion governing the sequence of slot-relevant ex-
pressions. Nevertheless, we seek to learn proce-
dures for populating frames in new documents.

Success in this endeavor would have signiﬁcant
practical impact.
If we can automate the sep-
aration of experimental evidence from common
knowledge and speculation, we have the means
to construct a high-quality biomedical resource of
use to both experimental and computational biolo-
gists. Our efforts, for example, ultimately seek to
automate the maintenance and extension of high-
ﬁdelity machine models of signaling pathways as-
sociated with Ras-driven human cancer.

We offer three contributions. First, we describe
a problem of clear biomedical signiﬁcance that in-
volves synthesis of information distributed across
a document, one that poses pertinent challenges to
the current practice of machine reading. Second,
we describe and evaluate an approach (the frame
classiﬁcation approach) that formalizes this prob-
lem as a binary classiﬁcation of frames nominated

16

by protein pairs co-occurring in sentences. We
provide evidence that good performance on this
problem requires attention to how entities are ref-
erenced across a document, even in multiple doc-
uments, not just in the nominating sentence. Fi-
nally, we describe and evaluate an approach (the
register machine approach) that attempts to correct
deﬁciencies of the frame classiﬁcation approach,
speciﬁcally its limiting reliance on sentence-local
juxtaposition of frame slot elements. This ap-
proach formalizes the frame extraction problem
as learning the best sequence of instructions for
frame generation through document traversal.

2 Related Work

Progress in biomedical
information extraction
(BioIE) is measured against shared annotated cor-
pora that decompose the problem into entity ex-
traction and sentence-level relation and event de-
tection (Kim et al., 2003; Pyysalo et al., 2007;
Kim et al., 2008; Thompson et al., 2009). The
structure of these tasks has remained remarkably
stable over the years, differing in some important
ways from the task addressed in this paper. Most
notably, the canonical BioIE task is highly local-
ized and mostly agnostic to discourse context. The
objective is to determine whether a single sen-
tence expresses some event of interest–gene ex-
pression, phosphorylation, regulation, etc.–and, if
so, what roles the entities appearing in the sen-
tence play in the putative event. The “events” de-
tected in this fashion are divorced from their dis-
course context (modulo coreference resolution),
although some attention has been paid to epis-
temic qualiﬁcations, such as negations and spec-
ulations (Kim et al., 2011). We have posed our-
selves a more focused task—extract the experi-
ments described in a paper—and are forced to do
without reliable sentence-level annotation.

There is no doubt that our system must re-
spond to some of the same expressions that are
addressed in some of these shared tasks. In par-
ticular,
the Genia Event Extraction Task (Kim
et al., 2011) targets phosphorylation and regu-
lation events involving phosphorylation, among
other things. Many of these event mentions are en-
countered in sections detailing experiments. Thus,
our task can be addressed in part through disam-
biguation and assimilation of these events—which
were actually observed in experiments? In this pa-
per, we describe an approach to datum extraction

that elaborates this idea.

Our focus on multi-slot, multi-sentence factual
frames is reminiscent of early formulations of the
information extraction problem used in the Mes-
sage Understanding Conference (MUC) (Grish-
man and Sundheim, 1996; Chinchor et al., 1993).
Over successive iterations of MUC, target frames
became quite elaborate, similar in complexity to
the datums we ultimately seek to populate. Many
of the tasks that the information extraction com-
munity views as canonical, including named entity
recognition, co-reference resolution, word sense
disambiguation, and relation and event extrac-
tion, were introduced as simpliﬁcations of the core
frame-ﬁlling task in MUC6. The ﬁeld has since
largely neglected the discourse-wide frame-ﬁlling
challenge.

Of course, to take it up again and address it
with the latest machine learning techniques, we
require heuristics to align slot-level information
found in reference frames to expressions in train-
ing sentences. Using such heuristics, in combi-
nation with structured ground-truth data, such as
our collection of datums, is commonly referred
to as distant supervision, an approach pioneered
on a biomedical extraction problem (Craven and
Kumlien, 1999). Relatively little work has ap-
plied distant supervision to discourse-level extrac-
tion problems. A counter-example is Reschke et
al. (2014), which addresses event extraction at
the document level, attempting to populate event-
related Wikipedia “info boxes” with article source
text. The Reschke et al.
approach employs
SEARN (Daume III et al., 2009), a technique that
reduces complex structured classiﬁcation prob-
lems into simpler sequence learning problems,
ﬁnding that it yields performance superior to sev-
eral strong baselines.

Recently,

the problem of understanding ac-
counts of experiments in the biological litera-
ture has been the focus of a small amount of
study (Dasigi et al., 2017; Burns et al., 2016). This
work, which springs from the same motivation and
shares some of the same data as our own, is largely
concerned with modeling the discourse structure
of experimental narratives. It is therefore largely
complementary to our work, which targets factual
experimental details. Success in discourse model-
ing promises to solve key problems that we face,
such as the segmentation of the text into distinct
experiments.

17

cause ﬁgure captions and body sentences contain-
ing ﬁgure references are on average relatively rich
in information needed to populate the simpliﬁed
datums attached to corresponding ﬁgures.

1. Phosphorylation and activation of JAK1 and Stat6 are
essential for induction of Stat6 DNA binding activity. 2.
To ascertain whether the decrease in Stat6 DNA binding
activity in the SOCS-1 stable transfectants was due to in-
hibition of JAK1 kinase activity, we immunoprecipitated
lysates from cells untreated or treated with IL-4 with Abs
to JAK1 or Stat6 and probed with Ab to phospho-tyrosine.
3.
Induction of JAK1 and Stat6 phosphorylation in the
SOCS-1 stable clones was reduced when compared with
control (Fig. 3A), while induction in the SOCS-2 stable
clones (Fig. 3B) and in the SOCS-3 stable clones (Fig.
3C) was similar to that of controls. 4. To further conﬁrm
that SOCS-1 suppresses JAK1 activation, we measured
the IL-4-induced kinase activity of JAK1 in the SOCS-1
stable clones by in vitro kinase assay.

Table 1: Example sentences potentially express-
ing the key elements of a “phos” datum.

Table 1, which excerpts four contiguous sen-
tences (we have numbered them for convenience)
from a relevant article, renders this concrete, but
also illustrates some of the subtleties involved.
Our data notes two distinct phosphorylation assays
in this passage, both linked to Sentence 3 (the only
sentence with ﬁgure references), corresponding to
the subjects JAK1 and Stat6, respectively, each of
which are phosphorylated (i.e., the change of the
“phos” assay is “increased”) in response to IL-4
(the treatment).

This passage is abundant in evidence about the
relevant experiments, but the information is dis-
tributed. Sentences 1 and 3 both contain inﬂec-
tions of “phosphorylate,” providing evidence that
a “phos” assay was conducted, but both lack the
treatment IL-4, which is referenced in Sentences 2
and 4. Note that the “phosphorylate” sentences are
rich in entities, potentially posing a combinatoric
discrimination problem. Ultimately, if we wish to
extract the two target datums at Sentence 3, in-
formation about the experimental treatment must
be pulled in from one of the adjacent sentences,
and we must determine that exactly two datums
are warranted.1

1Actually, a number of experimental variants are under
discussion in this passage. These are captured the database
in supplementary records called “extras.” Extras are not the

Figure 1: An annotated Pathway Logic datum.

3 Problem
The Pathway Logic (PL) project pursues high-
ﬁdelity signaling pathway models centering on the
Ras family of proteins (Eker et al., 2004). Part
of the effort involves a manual curation of ex-
perimental results, which has resulted in approx-
imately 50K records, each containing a detailed
formal representation of a reported experiment and
its outcomes. Such records, called datums, retain
pointers to the papers and ﬁgures from which they
were derived.

Figure 1 displays a typical datum in its compact
formal syntax, highlighting the four key compo-
nents: the assay, encoding the type of assay con-
ducted (here, an in-vitro kinase activity assay); the
subject, the entity whose response was measured
(“Jnk1”); the treatment, the substance applied to
the cellular environment (here, some member of
the IL-1 family, either IL-1 alpha or IL-1 beta);
and the change or experimental outcome. It should
be apparent from the ﬁgure that the typical da-
tum records many additional experimental details.
We refer to the combination of these four ﬁelds,
stripped of such qualiﬁers, as a simpliﬁed datum,
and seek to reconstruct these 4-tuples in our exper-
iments.

Notable among the ﬁelds in Figure 1, is an en-
coding of the source of the datum, most frequently
as a PubMed ID and ﬁgure reference. The datum
curator, not a computational linguist, found it most
natural to localize datums to the ﬁgures display-
ing assay outcomes. As a consequence, we do not
have access to a simple procedure for identifying
speciﬁc textual expressions for the various datum
elements. In fact, the data comes with no guaran-
tee that such expressions are present at all.

However, after a manual review of a large num-
ber of datums, we know that while some datums
are not adequately described in the text of anno-
tated articles, most are. Furthermore, the align-
ment of datums to ﬁgures enables weak localiza-
tion of datum elements to individual sentences, be-

18

Whatever the textual evidence for datums in a
paper, our problem is essentially extraction at the
level of documents. Formally, we are given a set
of examples {hdi, yii}, in which di is a document
and yi is a set of tuples {hsij, tij, aij, ciji}, the el-
ements of each tuple representing subjects, treat-
ments, assay types, and observed changes, respec-
tively. Assay and change values are drawn from
closed classes, assays from the set of types rep-
resented in the Pathway Logic knowledge base,
and changes from the set {increased, decreased,
unchanged}. Subjects and treatments are drawn
from the effectively open class of chemicals used
for experiments in the literature. In practice, they
are usually proteins, and in our experiments these
two slots take Uniprot IDs. Our extraction task in-
volves inferring the correct set yi, given some di.
4 Approaches

We investigate two distinct approaches to this
problem, the frame classiﬁcation approach and the
register machine approach. The ﬁrst is applica-
ble only to subject-treatment pairs that co-occur in
individual sentences, while the second approach
can in principle associate subjects and treatments
found in different sentences.

4.1 Frame Classiﬁcation
We observe that datum subjects and treatments
tend to be mentioned together in individual sen-
tences. This motivates a simple framing of the
datum extraction problem as binary classiﬁcation.
Speciﬁcally, if we ﬁx the assay type (e.g., “phos”)
and change (e.g., “increased”), we can view each
document as a set of co-mentioned proteins—
all pairs of proteins mentioned together in some
sentence—and attempt to distinguish pairs in the
subject-treatment relation from other pairs. Of
course, we must perform this procedure for all
assay-change pairs of interest.

We follow an approach to featurization pro-
posed in Xu et al (2016). Consider the set of
sentences containing protein entities P1 and P2.
Given a target assay-change conﬁguration we train
two binary classiﬁcation models, one to distin-
guish cases in which P1 and P2 are subject and
treatment, respectively, and one for the opposite
assignment. Our feature vectors have four parts,
each part containing features that require the fre-
quency of lexical unigrams and bigrams found in

focus of this paper’s work, but are ultimately important.

various sentence contexts. Thus, the word “pro-
tein” corresponds to three distinct features: one
feature recording its frequency of occurrence be-
fore P1 in the set of sentences, between P1 and
P2 (encountered in that order), between P2 and P1
(encountered in that order), and after P2, respec-
tively.

We also included and recorded a small perfor-
mance beneﬁt from two non-lexical features. First,
observing that datum protein pairs tend to be more
frequent than others, we deﬁned a feature that re-
ﬂects the number of sentences in which a pair co-
occurs. Second, we deﬁned indicator features that
reﬂect whether speciﬁc proteins ﬁll a subject or
treatment role anywhere in the training data.

Admittedly, this approach suffers from certain
limitations, most obviously limited recall, as it
can only distinguish datums whose subject and
treatment co-occur in a sentence—e.g., discard-
ing some 40% of phosphorylation datums. And as
noted, because the classiﬁcation problem is con-
ditioned on assay and change, we must learn a
separate classiﬁer for each observed assay-change
combination. This is tractable in practice, because
the number of frequently observed assay-change
combinations occurring is manageable.

4.2 Register Machine
To accommodate the distribution of relevant in-
formation across the sentences in a discourse, we
imagine a model capable of traversing sentences,
accumulating information, and synthesizing da-
tums. We suppose that datums are produced by
a virtual machine with four registers (one for each
of the slots in a simpliﬁed datum) and two cursors
(to traverse the sentences in a caption and article
body, respectively). At each time step, the ma-
chine can execute an instruction to advance either
cursor, populate or delete the contents of registers,
or produce one or more datums. Speciﬁcally, we
deﬁne the following instructions:

• advanceSectionCursor, where Section can
be either Caption or Body. One of the cur-
sors is advanced to the next entity within the
current sentence, if present, or to the begin-
ning of the next sentence in body or among
ﬁgure captions.

• setClosedValue, where Closed can be either
Assay or Change, and Value is one of the le-
gal values for the indicated closed-class reg-
ister. The register becomes populated with

19

the speciﬁed value, replacing any previous
contents.

• setOpenfromSection, where Open is either
Subject or Treatment. The indicated register
is populated with the entity under the cursor
for Section. This instruction is illegal if there
is no such entity.

• addOpenfromSection. This instruction is
like the previous one, except the entity is ac-
cumulated into the indicated register. As this
implies, open-class registers can hold multi-
ple entities.

• deleteRegister empties the indicated register.
• deleteAll empties all registers.
• produceDatums causes datums to be gener-
ated from register contents. A different da-
tum is generated for each distinct combina-
tion of entities in the subject and treatment
registers.

X

i

Let us suppose we are given a sequence of instruc-
tions I = i1 ··· im applying the machine to some
example hdi, yii. It is easy to see than any such I
yields a set of datums y∗
i , which we can formal-
ize as some function, F (d, I) = y.2 Further, we
can speak of a policy π(d) = I that nominates
instructions sequences, given a document. Ulti-
mately, our objective is to ﬁnd the best policy:

argmin

π

L(F (di, π(di)), yi)

(1)

Here, L(y∗, y) is the loss experienced by some
machine-generated set of datums y∗ with respect
to the ground-truth y. In practice, we seek to op-
timize the F1 of extracted datums versus ground
truth under a strict equality standard, i.e., only
those datums that agree in all slots with some
ground-truth datum are counted as successes.

Of course, Equation 1 is difﬁcult to satisfy di-
rectly. Instead, we seek to learn a local ranking
model for individual instructions. Let S(d, I1,k)
represent the state of the machine after executing
k instructions I1,k = i1 ··· ik against document d,
including the positions of the cursors, the state of
the registers, and any generated datums. We seek
to learn a local policy ˆπ(S(d, I1,k)) = ik+1 that
chooses the best next instruction.

2We posit that illegal instructions (e.g., advancing a cursor

at the end of the document) have no effect.

Learning ˆπ is essentially a ranking problem:
given all legal instructions in the current state,
which is best to execute? We therefore adopt a
learning-to-rank approach, training an empirical
model to map machine states to real values, such
that the highest-scoring instruction is the best to
execute in the current state. To this end, we im-
plemented an oracular policy (henceforth the “or-
acle”) that nominates instructions based on full
knowledge of ground truth. Given our uncer-
tainty about which sentences express datum ele-
ments (the subject of one or more datums might be
mentioned dozens of time in an article), this policy
heuristically orients datum production around ﬁg-
ure captions and sentences containing ﬁgure refer-
ences: datums are aligned to such sentences, using
their source ﬁeld, and the machine is instructed to
load its registers and produce datums as close as
possible to the sentences identiﬁed in this way. For
example, if a datum having subject a and treatment
b is linked to sentence si, and b is mentioned in si,
but the nearest mention of a is in si−1, the oracle
instructs the machine to load its subject register at
the a mention in si−1, and its treatment, assay, and
change registers at the b mention in si, followed
by a produceDatums instruction (and typically
some combination of delete instructions).

In our current implementation, we train a multi-
class perceptron model to perform ranking, up-
dating it whenever it ranks an inappropriate in-
struction highest. The mistake-driven nature of
this training regime enables us to accommodate
a subtlety of the problem: there are often several
good instructions in any given state, and we cannot
know that the instruction preferred by the oracle is
truly optimal. To respond to this reality, the oracle
provides a second service—assessment of instruc-
tions preferred by the model. If such an instruction
is deemed adequate—if it does not ultimately pre-
vent the register machine from producing upcom-
ing datums—the model’s preferred instruction is
deemed correct, and no update is performed.

Any feature of the machine’s state, including
the contents of its registers, datums produced so
far, recently executed instructions, and, most im-
portantly, the language at and around cursors, may
be encoded to train the model. Table 2 lists the fea-
tures implemented to date, which should be self-
explanatory, except for the “Pattern” features. To
implement these, we separately induce a set of pat-
terns over dependency parses to detect expressions

20

Type
Cursor

Feature

atPosition(curs, pos)

Register

populated(reg)

cregContains(reg, val)

oregContains(curs, reg)

Pattern

activeAtSent(pat, curs)

activeAtEnt(pat, curs)

Other

producedDatums

bias

Description
True if the cursor curs (body or caption) is at the
indicated pos in its section (beginning, internal,
end)
True if the indicated reg (subject, treatment, assay,
or change) is populated.
True if a closed-class register reg (assay or
change) contains a particular value val legal for
that type (e.g. the assay register contains “phos”).
True if the open-class register reg contains the en-
tity under the cursor curs.
True if all four registers are populated.
True if the sentence under curs contains word.
over [−2, +2], from curs.
True if the detection pattern pat matches the sen-
tence under curs.
True if the pattern pat matches the entity under
curs.
True immediately after a produceDatums instruc-
tion has been executed.
Always true.

Lexical

allPopulated
sentContains(curs, word)
wordAtOffset(curs, offs, word) True if word is observed at offset offs, ranging

Table 2: Features used in experiments with the register machine.

that tend to signal the presence of an assay sub-
ject or treatment (Freitag and Niekrasz, 2016). For
each such pattern, we deﬁne two features, which
are true if the corresponding pattern matches any-
where in a cursor sentence or at a cursor entity, re-
spectively. In addition to the features listed in the
table, we automatically generate a large number of
conjunctive features from feature pairs, returning
true when both the constituent features are true.

5 Evaluation

We constructed our experimental data from the set
of datums in the Pathway Logic database, along
with the 2,394 papers to which they refer. Be-
cause most of these papers are available only as
PDF,3 we converted them to plain text and heuris-
tically identiﬁed paper sections, converting each
to a sequence of sentences. This data was then
annotated by machine to identify mentions of pro-
tein entities (heuristically mapped to Uniprot iden-
tiﬁes) and ﬁgure references. The latter were used
to align datums heuristically to sentences.

As noted previously, the Pathway Logic data

3Much of the curated data predates the establishment of

the NXML format.

Database
Experimental corpus
Visible
Fully visible

All
17,444
6,554
5,981
2,336

Phos
4,864
3,152
2,989
1,418

Table 3: Visibility of datums (of any type vs.
those representing phosphorylation assays).

comes with no guarantee that the datums are actu-
ally described in the text of an article.4 Moreover,
failures in entity recognition or resolution further
reduce what our models have the potential to “see”
in the text. We therefore limit our attention to “vis-
ible” datums, those datums for which we recog-
nize either the subject or treatment entity some-
where in the paper to which a datum is aligned. We
call datums for which both entities are recognized
“fully visible.” Our experimental corpus consists
of the 518 papers aligned to at least one visible
datum.

4Nor is there a strong guarantee that all experiments de-
scribed in a paper have been converted into datums. Our cura-
tor has it in her charter to do so, but we have encountered ex-
periments for which no datum was created. We do not know
how common this is.

21

Method
Oracle
Frame Gold
Frame
Machine

Precision Recall
0.5708
0.4937
0.296
0.1877

0.6935
0.9302
0.2426
0.2056

F1
0.5996
0.6017
0.2322
0.165

Table 4: Macro-averaged precision, recall, and F1
in extracting simpliﬁed “phos” datums.

Table 3 provides an overview of the data we
work with. For convenience in comparing our
two approaches, we focus on “phos” datums ex-
clusively, and therefore present separate totals for
“phos” datums in the table. (The register machine
targets all visible assay types, but we evaluate its
performance only against “phos” datums.) The
row labeled Database lists counts calculated from
our snapshot of the datum database, while Exper-
imental corpus considers only the subset aligned
to papers in our collection of 518 articles. The
rows Visible and Fully visible document the num-
ber of datums actually available for experiments.
The performance numbers that follow correspond
to those datums contained in the cell labeled Visi-
ble, Phos.

In our experiments, we randomly sampled 75%
of our 518 articles (and the corresponding da-
tums) for training, and evaluated against the re-
maining 25%.
In training the register machine,
we reserve some of the training data for valida-
tion, using F1 against this hold-out data as a stop-
ping criterion to prevent overﬁtting. To be deemed
correct, an extracted simpliﬁed datum must agree
with a ground-truth datum on all four slots. When
a ground-truth datum is partially visible, an extrac-
tor must populate the empty slot with a null in or-
der to be awarded credit. Note that this necessarily
limits the recall of frame classiﬁcation, which has
no way to produce a null slot.

Table 4 presents the results of our experiments.
The ﬁrst two rows in the table establish approxi-
mate upper bounds on performance. Oracle mea-
sures the performance of the policy used to gen-
erate training data for the register machine, while
Frame Gold lists the performance of a perfect
classiﬁer of candidate protein pairs nominated us-
ing the sentence co-occurrence heuristic. Interest-
ingly, the difference in recall between these two
approaches is fairly small, indicating that although
the register machine can in principle integrate ev-
idence distributed over multiple sentences, it is

difﬁcult to do so, even for a heuristically imple-
mented oracle.

The remaining two rows compare the two learn-
ing approaches to datum extraction described in
the paper, frame classiﬁcation (Frame) and the
register machine (Machine). Note that the exam-
ple generation procedure used in Frame leads to
considerable class skew, with the set of negative
example dwarﬁng the positive.
In these experi-
ments, we randomly sampled the negative exam-
ples to achieve a ten-to-one negative-to-positive
ratio.

The results appear to suggest that the relative
simplicity of frame classiﬁcation more than com-
pensates for the fact that it cannot account for a
signiﬁcant fraction of datums, those whose sub-
jects and treatments are not found together in an
individual sentence. We see clear evidence that
accumulation of evidence spread across sentences
enhances performance. In a separate experiment,
in which we classiﬁed individual sentences (simi-
lar to canonical relation extraction), we saw a drop
in F1 of about 2 points.

The register machine, which in principle can
accommodate the “distributed” datums that the
frame classiﬁer ignores, has difﬁculty learning in-
struction sequences well enough to achieve com-
parable performance. Its difﬁculty appears to cen-
ter primarily on the extracted components of da-
tums, the subjects and treatments.
If we evalu-
ate the register machine’s performance on indi-
vidual slots (e.g., by scoring the set of phos sub-
jects extracted against the set found in phos da-
tums aligned to a paper), we observe F1s of 0.92
and 0.67 on assay and change, respectively, but
only 0.42 and 0.38 on subject and treatment. We
believe that the feature set currently employed by
the machine is too impoverished to perform these
extractions accurately. Note that while frame clas-
siﬁcation accumulates evidence relevant to a pro-
tein pair from across an article, the register ma-
chine relies on mostly local information. This is
an unnecessary limitation, which we are attempt-
ing to rectify.

6 Discussion

Our work with the frame classiﬁer is leading the
way in this regard. In preliminary work conducted
after the experiments presented here, we have con-
tinued to mitigate keys drawbacks of the approach.
For example, by training individual protein classi-

22

ﬁers for “subjectness” and “treatmentness,” using
information distributed across an article, we ob-
serve a frame classiﬁcation F1 of 0.30 in prelimi-
nary experiments. We are also working to increase
the number of assay-change combinations targeted
by frame classiﬁcation to practical levels.

All this makes clear that the strict evaluation
metric used in this paper–simultaneous agreement
on four key slots with target datums–poses a stiff
challenge for computer readers. These perfor-
mance levels are understandable. Robust solutions
for many types of binary relation and event ex-
traction have yet to be reported. For example, a
characteristic approach to ACE-style relation ex-
traction reports peak F1 of about 0.55 (GuoDong
et al., 2005), and recent work in comparable
biomedical extraction problems yields qualita-
tively comparable performance–e.g., F1 of 0.53 in
a pathway curation task involving primarily binary
interactions having high domain overlap with the
current paper (N´edellec et al., 2013). As a rule,
adding slots to a target template leads to consider-
ably lowered extraction performance under a strict
matching regime. Moreover, the heuristic align-
ment of slot values to speciﬁc textual expressions
adds further noise to the training and evaluation
processes.

However, there is reason to believe that even
these modest performance numbers are useful
for certain applications.
In separate work under
the DARPA Big Mechanism program, we imple-
mented a manual datum extractor, as part of a sys-
tem that sought to conﬁrm events and relations
extracted by general-purpose bio-NLP readers by
looking for corroborative experiments in the same
paper. We were able to show, using hand-scored
results from the program evaluation, that 80% of
machine extractions corroborated in this way were
correct (about 17% of all such extractions), ver-
sus a baseline accuracy of 50%. This despite the
fact that we estimated the F1 of our the hand-
authored system, which over-generates wildly, at
less than 0.02. Thus, even a very noisy experiment
extractor has value as a source of corroboration
for assertions extracted without attention to prag-
matic context. Possibly key to this outcome was
the strict standard applied in the program evalua-
tion, which deprecated speculation or statements
of background knowledge.

Our focus on a very speciﬁc problem and data
set may leave the impression that these results are

of little further use. We argue that the opposite
is true, that this admittedly domain-speciﬁc chal-
lenge is an instance of a type of problem that will
become increasingly salient as machine reading
matures. Eventually, the ﬁeld must move beyond
sentence-local, contextless, low-arity extraction to
the full population of knowledge frames summa-
rizing information relevant to important use cases.
A key resource to this end will be “found” struc-
tured resources loosely attached to textual source
material, such as the auxiliary data associated with
biological publications with increasing frequency,
or Wikipedia info-boxes summarizing events in
newswire (Reschke et al., 2014). The ﬁeld re-
quires methods that exploit such resources for the
interpretation of key facts in text.

7 Conclusion
The problem introduced in this paper—that of ex-
tracting faithful representations of experiments de-
scribed in the biological literature—has two fea-
tures that distinguish it from much of the work
on biomedical NLP: (1) It is closely aligned to
the needs of computational biology, stemming
from research independent from and uninformed
by NLP. And (2) it cannot be adequately addressed
by models that target the information found in in-
dividual sentences in isolation. These two features
make for a problem of considerable depth and im-
portance, both for biology and NLP. While it is
clear that we have not solved this problem with the
approaches documented here, we have sketched
two potential solutions and illuminated some of
the speciﬁc challenges that remain.

Acknowledgments
This project was supported by the U.S. Army Re-
search Ofﬁce. The content of this paper does not
necessarily reﬂect the position or policy of the
U.S. Government. No ofﬁcial endorsement should
be inferred.

References
Gully APC Burns, Pradeep Dasigi, Anita de Waard,
and Eduard H Hovy. 2016. Automated detection of
discourse segment and experimental types from the
text of cancer pathway results sections. Database
2016.

Nancy Chinchor, David D Lewis,

and Lynette
Hirschman. 1993. Evaluating message understand-
ing systems: an analysis of the third message under-

23

Sampo Pyysalo, Filip Ginter, Juho Heimonen, Jari
Bjrne, Jorma Boberg, Jouni Jrvinen, and Tapio
Salakoski. 2007. BioInfer: a corpus for information
extraction in the biomedical domain. BMC bioinfor-
matics 8(1):50.

Kevin Reschke, Martin Jankowiak, Mihai Surdeanu,
Christopher D. Manning, and Daniel Jurafsky. 2014.
Event Extraction Using Distant Supervision.
In
LREC. pages 4527–4531.

Paul Thompson, Syed A. Iqbal, John McNaught, and
Sophia Ananiadou. 2009. Construction of an anno-
tated corpus to support biomedical information ex-
traction. BMC bioinformatics 10(1):1.

Jun Xu, Yonghui Wu, Yaoyun Zhang, Jingqi Wang,
Hee-Jin Lee, and Hua Xu. 2016. Cd-rest: a sys-
tem for extracting chemical-induced disease relation
in literature. Database 2016.

standing conference (muc-3). Computational lin-
guistics 19(3):409–449.

Mark Craven and Johan Kumlien. 1999. Constructing
biological knowledge bases by extracting informa-
tion from text sources. In ISMB. volume 1999, pages
77–86.

P. Dasigi, G. A. P. C. Burns, E. Hovy, and A. de Waard.
2017. Experiment Segmentation in Scientiﬁc Dis-
course as Clause-level Structured Prediction using
Recurrent Neural Networks. ArXiv e-prints .

Hal Daume III, John Langford, and Daniel Marcu.
2009. Search-based structured prediction. Machine
learning 75(3):297–325.

Steven Eker, Merrill Knapp, Keith Laderoute, Patrick
Lincoln, and Carolyn Talcott. 2004.
Pathway
logic: Executable models of biological networks.
Electronic Notes in Theoretical Computer Science
71:144–161.

Dayne Freitag and John Niekrasz. 2016.

Feature
derivation for exploitation of distant annotation via
pattern induction against dependency parses.
In
Proceedings of
the 15th Workshop on Biomedi-
cal Natural Language Processing. Association for
Computational Linguistics, Berlin, Germany, pages
36–45.

Ralph Grishman and Beth Sundheim. 1996. Message
understanding conference-6: A brief history. In Pro-
ceedings of the 16th conference on Computational
linguistics-Volume 1. Association for Computational
Linguistics, pages 466–471.

Zhou GuoDong, Su Jian, Zhang Jie, and Zhang Min.
2005. Exploring various knowledge in relation ex-
traction. In Proceedings of the 43rd annual meeting
on association for computational linguistics. Asso-
ciation for Computational Linguistics, pages 427–
434.

J.-D. Kim, Tomoko Ohta, Yuka Tateisi, and Junichi
Tsujii. 2003. GENIA corpusa semantically an-
notated corpus for bio-textmining. Bioinformatics
19(suppl 1):i180–i182.

Jin-Dong Kim, Tomoko Ohta, and Jun’ichi Tsujii.
2008. Corpus annotation for mining biomedical
events from literature. BMC bioinformatics 9(1):1.

Jin-Dong Kim, Yue Wang, Toshihisa Takagi, and Aki-
nori Yonezawa. 2011. Overview of genia event
task in bionlp shared task 2011.
In Proceedings
of the BioNLP Shared Task 2011 Workshop. Asso-
ciation for Computational Linguistics, Stroudsburg,
PA, USA, BioNLP Shared Task ’11, pages 7–15.

Claire N´edellec, Robert Bossy, Jin-Dong Kim, Jung-
Jae Kim, Tomoko Ohta, Sampo Pyysalo, and Pierre
Zweigenbaum. 2013. Overview of bionlp shared
task 2013.
In Proceedings of the BioNLP Shared
Task 2013 Workshop. Association for Computational
Linguistics Soﬁa, Bulgaria, pages 1–7.

