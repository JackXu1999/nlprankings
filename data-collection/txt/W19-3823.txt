



















































Measuring Bias in Contextualized Word Representations


Proceedings of the 1st Workshop on Gender Bias in Natural Language Processing, pages 166–172
Florence, Italy, August 2, 2019. c©2019 Association for Computational Linguistics

166

Measuring Bias in Contextualized Word Representations

Keita Kurita Nidhi Vyas Ayush Pareek Alan W Black Yulia Tsvetkov

Carnegie Mellon University
{kkurita,nkvyas,apareek,awb,ytsvetko}@andrew.cmu.edu

Abstract

Contextual word embeddings such as BERT
have achieved state of the art performance in
numerous NLP tasks. Since they are optimized
to capture the statistical properties of training
data, they tend to pick up on and amplify so-
cial stereotypes present in the data as well. In
this study, we (1) propose a template-based
method to quantify bias in BERT; (2) show that
this method obtains more consistent results in
capturing social biases than the traditional co-
sine based method; and (3) conduct a case
study, evaluating gender bias in a downstream
task of Gender Pronoun Resolution. Although
our case study focuses on gender bias, the pro-
posed technique is generalizable to unveiling
other biases, including in multiclass settings,
such as racial and religious biases.

1 Introduction

Type-level word embedding models, including
word2vec and GloVe (Mikolov et al., 2013; Pen-
nington et al., 2014), have been shown to exhibit
social biases present in human-generated train-
ing data (Bolukbasi et al., 2016; Caliskan et al.,
2017; Garg et al., 2018; Manzini et al., 2019).
These embeddings are then used in a plethora of
downstream applications, which perpetuate and
further amplify stereotypes (Zhao et al., 2017;
Leino et al., 2019). To reveal and quantify corpus-
level biases is word embeddings, Bolukbasi et al.
(2016) used the word analogy task (Mikolov et al.,
2013). For example, they showed that gendered
male word embeddings like he, man are associated
with higher-status jobs like computer programmer
and doctor, whereas gendered words like she or
woman are associated with homemaker and nurse.

Contextual word embedding models, such as
ELMo and BERT (Peters et al., 2018; Devlin et al.,
2019) have become increasingly common, replac-
ing traditional type-level embeddings and attain-
ing new state of the art results in the majority of

NLP tasks. In these models, every word has a dif-
ferent embedding, depending on the context and
the language model state; in these settings, the
analogy task used to reveal biases in uncontextual-
ized embeddings is not applicable. Recently, May
et al. (2019) showed that traditional cosine-based
methods for exposing bias in sentence embeddings
fail to produce consistent results for embeddings
generated using contextual methods. We find sim-
ilar inconsistent results with cosine-based methods
of exposing bias; this is a motivation to the devel-
opment of a novel bias test that we propose.

In this work, we propose a new method to quan-
tify bias in BERT embeddings (§2). Since BERT
embeddings use a masked language modelling ob-
jective, we directly query the model to measure the
bias for a particular token. More specifically, we
create simple template sentences containing the at-
tribute word for which we want to measure bias
(e.g. programmer) and the target for bias (e.g. she
for gender). We then mask the attribute and target
tokens sequentially, to get a relative measure of
bias across target classes (e.g. male and female).
Contextualized word embeddings for a given to-
ken change based on its context, so such an ap-
proach allows us measure the bias for similar cate-
gories divergent by the the target attribute (§2). We
compare our approach with the cosine similarity-
based approach (§3) and show that our measure of
bias is more consistent with human biases and is
sensitive to a wide range of biases in the model
using various stimuli presented in Caliskan et al.
(2017). Next, we investigate the effect of a specific
type of bias in a specific downstream task: gender
bias in BERT and its effect on the task of Gen-
dered Pronoun Resolution (GPR) (Webster et al.,
2018). We show that the bias in GPR is highly cor-
related with our measure of bias (§4). Finally, we
highlight the potential negative impacts of using
BERT in downstream real world applications (§5).
The code and data used in this work are publicly



167

available.1

2 Quantifying Bias in BERT

BERT is trained using a masked language mod-
elling objective i.e. to predict masked tokens, de-
noted as [MASK], in a sentence given the entire
context. We use the predictions for these [MASK]
tokens to measure the bias encoded in the actual
representations.

We directly query the underlying masked lan-
guage model in BERT2 to compute the association
between certain targets (e.g., gendered words)
and attributes (e.g. career-related words). For
example, to compute the association between the
target male gender and the attribute programmer,
we feed in the masked sentence “[MASK] is a
programmer” to BERT, and compute the proba-
bility assigned to the sentence ‘he is a program-
mer” (ptgt). To measure the association, however,
we need to measure how much more BERT prefers
the male gender association with the attribute pro-
grammer, compared to the female gender. We thus
re-weight this likelihood ptgt using the prior bias
of the model towards predicting the male gender.
To do this, we mask out the attribute programmer
and query BERT with the sentence “[MASK] is a
[MASK]”, then compute the probability BERT as-
signs to the sentence ‘he is a [MASK]” (pprior).
Intuitively, pprior represents how likely the word
he is in BERT, given the sentence structure and no
other evidence. Finally, the difference between the
normalized predictions for the words he and she
can be used to measure the gender bias in BERT
for the programmer attribute.

Generalizing, we use the following procedure
to compute the association between a target and
an attribute:

1. Prepare a template sentence
e.g.“[TARGET] is a [ATTRIBUTE]”

2. Replace [TARGET] with [MASK] and com-
pute ptgt=P([MASK]=[TARGET]| sentence)

3. Replace both [TARGET] and [ATTRIBUTE]
with [MASK], and compute prior probability
pprior=P([MASK]=[TARGET]| sentence)

4. Compute the association as log ptgtpprior

1https://bit.ly/2EkJwh1
2For all experiments we use the uncased version of

BERTBASE https://storage.googleapis.com/
bert_models/2018_10_18/uncased_L-12_
H-768_A-12.zip.

We refer to this normalized measure of associa-
tion as the increased log probability score and the
difference between the increased log probability
scores for two targets (e.g. he/she) as log proba-
bility bias score which we use as measure of bias.
Although this approach requires one to construct
a template sentence, these templates are merely
simple sentences containing attribute words of in-
terest, and can be shared across multiple targets
and attributes. Further, the flexibility to use such
templates can potentially help measure more fine-
grained notions of bias in the model.

In the next section, we show that our proposed
log probability bias score method is more effec-
tive at exposing bias than traditional cosine-based
measures.

3 Correlation with Human Biases

We investigate the correlation between our mea-
sure of bias and human biases. To do this, we
apply the log probability bias score to the same
set of attributes that were shown to exhibit human
bias in experiments that were performed using the
Implicit Association Test (Greenwald et al., 1998).
Specifically, we use the stimuli used in the Word
Embedding Association Test (WEAT) (Caliskan
et al., 2017).
Word Embedding Association Test (WEAT):
The WEAT method compares set of target con-
cepts (e.g. male and female words) denoted as X
and Y (each of equal size N ), with a set of at-
tributes to measure bias over social attributes and
roles (e.g. career/family words) denoted as A and
B. The degree of bias for each target concept t is
calculated as follows:

s(t, A,B) = [meana∈Asim(t, a)− meanb∈Bsim(t, b)],

where sim is the cosine similarity between the em-
beddings. The test statistics is

S(X,Y,A,B) = [meanx∈Xs(x,A,B)−
meany∈Y s(y,A,B)],

where the test is a permutation test over X and Y .
The p-value is computed as

p = Pr[S(Xi, Yi, A,B) > S(X,Y,A,B)]

The effect size is measured as

d =
S(X,Y,A,B)

stdt∈X∪Y s(t, A,B)

https://bit.ly/2EkJwh1
https://storage.googleapis.com/bert_models/2018_10_18/uncased_L-12_H-768_A-12.zip
https://storage.googleapis.com/bert_models/2018_10_18/uncased_L-12_H-768_A-12.zip
https://storage.googleapis.com/bert_models/2018_10_18/uncased_L-12_H-768_A-12.zip


168

Category Templates
Pleasant/Unpleasant (Insects/Flowers) T are A, T is A
Pleasant/Unpleasant (EA/AA) T are A, T is A
Career/Family (Male/Female) T likes A, T like A, T is interested in A
Math/Arts (Male/Female) T likes A, T like A, T is interested in A
Science/Arts (Male/Female) T likes A, T like A, T is interested in A

Table 1: Template sentences used for the WEAT tests (T: target, A: attribute)

Category Targets Templates
Pleasant/Unpleasant (Insects/Flowers) flowers,insects,flower,insect T are A, the T is A
Pleasant/Unpleasant (EA/AA) black, white T people are A, the T person is A
Career/Family (Male/Female) he,she,boys,girls,men,women T likes A, T like A, T is interested in A
Math/Arts (Male/Female) he,she,boys,girls,men,women T likes A, T like A, T is interested in A
Science/Arts (Male/Female) he,she,boys,girls,men,women T likes A, T like A, T is interested in A

Table 2: Template sentences used and target words for the grammatically correct sentences (T: target, A: attribute)

It is important to note that the statistical test is a
permutation test, and hence a large effect size does
not guarantee a higher degree of statistical signifi-
cance.

3.1 Baseline: WEAT for BERT

To apply the WEAT method on BERT, we first
compute the embeddings for target and attribute
words present in the stimuli using multiple tem-
plates, such as “TARGET is ATTRIBUTE” (Re-
fer Table 1 for an exhaustive list of templates used
for each category). We mask the TARGET to
compute the embedding3 for the ATTRIBUTE and
vice versa. Words that are absent in the BERT vo-
cabulary are removed from the targets. We ensure
that the number of words for both targets are equal,
by removing random words from the smaller tar-
get set. To confirm whether the reduction in vo-
cabulary results in a change of p-value, we also
conduct the WEAT on GloVe with the reduced vo-
cabulary.4

3.2 Proposed: Log Probability Bias Score

To compare our method measuring bias, and to
test for human-like biases in BERT, we also com-
pute the log probability bias score for the same
set of attributes and targets in the stimuli. We
compute the mean log probability bias score for
each attribute, and permute the attributes to mea-
sure statistical significance with the permutation
test. Since many TARGETs in the stimuli cause
the template sentence to become grammatically

3We use the outputs from the final layer of BERT as em-
beddings

4WEAT was originally used to study the GloVe embed-
dings

incorrect, resulting in low predicted probabili-
ties, we fixed the TARGET to common pro-
nouns/indicators of category such as flower, he,
she (Table 2 contains a full list of target words and
templates). This avoids large variance in predicted
probabilities, leading to more reliable results. The
effect size is computed in the same way as the
WEAT except the standard deviation is computed
over the mean log probability bias scores.

We experiment over the following categories
of stimuli in the WEAT experiments: Category 1
(flower/insect targets and pleasant/unpleasant at-
tributes), Category 3 (European American/African
American names and pleasant/unpleasant at-
tributes), Category 6 (male/female names and ca-
reer/family attributes), Category 7 (male/female
targets and math/arts attributes) and Category 8
(male/female targets and science/arts attributes).

3.3 Comparison Results

The WEAT on GloVe returns similar findings
to those of Caliskan et al. (2017) except for
the European/African American names and pleas-
ant/unpleasant association not exhibiting signifi-
cant bias. This is due to only 5 of the African
American names being present in the BERT vo-
cabulary. The WEAT for BERT fails to find any
statistically significant biases at p < 0.01. This
implies that WEAT is not an effective measure
for bias in BERT embeddings, or that methods for
constructing embeddings require additional inves-
tigation. In contrast, our method of querying the
underlying language model exposes statistically
significant association across all categories, show-
ing that BERT does indeed encode biases and that
our method is more sensitive to them.



169

Category WEAT on GloVe WEAT on BERT Ours on BERT
Log Probability Bias Score

Pleasant/Unpleasant (Insects/Flowers) 1.543* 0.6688 0.8744*
Pleasant/Unpleasant (EA/AA) 1.012 1.003 0.8864*
Career/Family (Male/Female) 1.814* 0.5047 1.126*
Math/Arts (Male/Female) 1.061 0.6755 0.8495*
Science/Arts (Male/Female) 1.246* 0.8815 0.9572*

Table 3: Effect sizes of bias measurements on WEAT Stimuli. (* indicates significant at p < 0.01)

Gender Prior Prob. Avg. Predicted Prob.

Male 10.3% 11.5%
Female 9.8% 13.9%

Table 4: Probability of pronoun referring to neither
entity in a sentence of GPR

4 Case Study: Effects of Gender Bias on
Gendered Pronoun Resolution

Dataset We examined the downstream effects of
bias in BERT using the Gendered Pronoun Res-
olution (GPR) task (Webster et al., 2018). GPR
is a sub-task in co-reference resolution, where a
pronoun-containing expression is to be paired with
the referring expression. Since pronoun resolving
systems generally favor the male entities (Webster
et al., 2018), this task is a valid test-bed for our
study. We use the GAP dataset5 by Webster et al.
(2018), containing 8,908 human-labeled ambigu-
ous pronoun-name pairs, created from Wikipedia.
The task is to classify whether an ambiguous pro-
noun P in a text refers to entity A, entity B or nei-
ther. There are 1,000 male and female pronouns
in the training set each, with 103 and 98 of them
not referring to any entity in the sentence, respec-
tively.

Model We use the model suggested on Kaggle,6
inspired by Tenney et al. (2019). The model uses
BERT embeddings for P , A and B, given the con-
text of the input sentence. Next, it uses a multi-
layer perceptron (MLP) layer to perform a naive
classification to decide if the pronoun belongs to
A, B or neither. The MLP layer uses a single hid-
den layer with 31 dimensions, a dropout of 0.6 and
L2 regularization with weight 0.1.

Results Although the number of male pronouns
associated with no entities in the training data is

5https://github.com/
google-research-datasets/gap-coreference

6https://www.kaggle.com/mateiionita/
taming-the-bert-a-baseline

slightly larger, the model predicted the female pro-
noun referring to no entities with a significantly
higher probability (p = 0.007 on a permutation
test); see Table 4. As the training set is balanced,
we attribute this bias to the underlying BERT rep-
resentations.

We also investigate the relation between the
topic of the sentence and model’s ability to as-
sociate the female pronoun with no entity. We
first extracted 20 major topics from the dataset us-
ing non-negative matrix factorization (Lee and Se-
ung, 2001) (refer to Appendix for the list of top-
ics). We then compute the bias score for each
topic as the sum of the log probability bias score
for the top 15 most prevalent words of each topic
weighted by their weights within the topic. For
this, we use a generic template “[TARGET] are in-
terested in [ATTRIBUTE]” where TARGET is ei-
ther men or women. Next we compute a bias score
for each sample in the training data as the sum
of individual bias scores of topics present in the
sample, weighted by the topic weights. Finally,
we measured the Spearman correlation coefficient
to be 0.207 (which is statistically significant with
p = 4e − 11) between the bias scores for male
gender across all samples and the model’s proba-
bility to associate a female pronoun with no entity.
We conclude that models using BERT find it chal-
lenging to perform coreference resolution when
the gender pronoun is female and if the topic is
biased towards the male gender.

5 Real World Implications

In previous sections, we discussed that BERT has
human-like biases, which are propagated to down-
stream tasks. In this section, we discuss an-
other potential negative impact of using BERT in
a downstream model. Given that three quarters of
US employers now use social media for recruiting
job candidates (Segal, 2014), many applications
are filtered using job recommendation systems and
other AI-powered services. Zhao et al. (2018)

https://github.com/google-research-datasets/gap-coreference
https://github.com/google-research-datasets/gap-coreference
https://www.kaggle.com/mateiionita/taming-the-bert-a-baseline
https://www.kaggle.com/mateiionita/taming-the-bert-a-baseline


170

discussed that resume filtering systems are biased
when the model has strong association between
gender and certain professions. Similarly, certain
gender-stereotyped attributes have been strongly
associated with occupational salary and prestige
(Glick, 1991). Using our proposed method, we
investigate the gender bias in BERT embeddingss
for certain occupation and skill attributes.
Datasets: We use three datasets for our study of
gender bias in employment attributes:

• Employee Salary Dataset7 for Montgomery
County of Maryland- Contains 6882 in-
stances of “Job Title” and “Salary” records
along with other attributes. We sort this
dataset in decreasing order of salary and take
the first 1000 instances as a proxy for high-
paying and prestigious jobs.

• Positive and Negative Traits Dataset8- Con-
tains a collection of 234 and 292 adjectives
considered “positive” and “negative” traits,
respectively.

• O*NET 23.2 technology skills9 Contains
17649 unique skills for 27660 jobs, which are
posted online

Discussion We used the following two templates
to measure gender bias:

• “TARGET is ATTRIBUTE”, where TAR-
GET are male and female pronouns viz. he
and she. The ATTRIBUTE are job titles from
the Employee Salary dataset, or the adjec-
tives from the Positive and Negative traits
dataset.

• “TARGET can do ATTRIBUTE”, where
the TARGETs are the same, but the AT-
TRIBUTE are skills from the O*NET
dataset.

Table 5 shows the percentage of attributes that
were more strongly associated with the male than
the female gender. The results prove that BERT
expresses strong preferences for male pronouns,
raising concerns with using BERT in downstream
tasks like resume filtering.

7https://catalog.data.gov/dataset/
employee-salaries-2017

8http://ideonomy.mit.edu/essays/
traits.html

9https://www.onetcenter.org/database.
html#individual-files

Dataset Percentage

Salary 88.5%
Pos-Traits 80.0%
Neg-Traits 78.9%
Skills 84.0%

Table 5: Percentage of attributes associated more
strongly with the male gender

6 Related Work

NLP applications ranging from core tasks such
as coreference resolution (Rudinger et al., 2018)
and language identification (Jurgens et al., 2017),
to downstream systems such as automated essay
scoring (Amorim et al., 2018), exhibit inherent so-
cial biases which are attributed to the datasets used
to train the embeddings (Barocas and Selbst, 2016;
Zhao et al., 2017; Yao and Huang, 2017). There
have been several efforts to investigate the amount
of intrinsic bias within uncontextualized word em-
beddings in binary (Bolukbasi et al., 2016; Garg
et al., 2018; Swinger et al., 2019) and multiclass
(Manzini et al., 2019) settings.

Contextualized embeddings such as BERT (De-
vlin et al., 2019) and ELMo (Peters et al., 2018)
have been replacing the traditional type-level em-
beddings. It is thus important to understand the ef-
fects of biases learned by these embedding models
on downstream tasks. However, it is not straight-
forward to use the existing bias-exposure methods
for contextualized embeddings. For instance, May
et al. (2019) used WEAT on sentence embeddings
of ELMo and BERT, but there was no clear indica-
tion of bias. Rather, they observed counterintuitive
behavior like vastly different p-values for results
concerning gender.

Along similar lines, Basta et al. (2019) noted
that contextual word-embeddings are less biased
than traditional word-embeddings. Yet, biases
like gender are propagated heavily in downstream
tasks. For instance, Zhao et al. (2019) showed
that ELMo exhibits gender bias for certain pro-
fessions. As a result, female entities are pre-
dicted less accurately than male entities for certain
occupation words, in the coreference resolution
task. Field and Tsvetkov (2019) revealed biases
in ELMo embeddings that limit their applicability
across data domains. Motivated by these recent
findings, our work proposes a new method to ex-
pose and measure bias in contextualized word em-
beddings, specifically BERT. As opposed to previ-

https://catalog.data.gov/dataset/employee-salaries-2017
https://catalog.data.gov/dataset/employee-salaries-2017
http://ideonomy.mit.edu/essays/traits.html
http://ideonomy.mit.edu/essays/traits.html
https://www.onetcenter.org/database.html#individual-files
https://www.onetcenter.org/database.html#individual-files


171

ous work, our measure of bias is more consistent
with human biases. We also study the effect of this
intrinsic bias on downstream tasks, and highlight
the negative impacts of gender-bias in real world
applications.

7 Conclusion

In this paper, we showed that querying the under-
lying language model can effectively measure bias
in BERT and expose multiple stereotypes embed-
ded in the model. We also showed that our mea-
sure of bias is more consistent with human-biases,
and outperforms the traditional WEAT method on
BERT. Finally we showed that these biases can
have negative downstream effects. In the future,
we would like to explore the effects on other
downstream tasks such as text classification, and
device an effective method of debiasing contextu-
alized word embeddings.

Acknowledgments

This material is based upon work supported by
the National Science Foundation under Grant
No. IIS1812327.

References
Evelin Amorim, Marcia Cançado, and Adriano Veloso.

2018. Automated essay scoring in the presence of
biased ratings. In Proc. of NAACL, pages 229–237.

Solon Barocas and Andrew D Selbst. 2016. Big data’s
disparate impact. Calif. L. Rev., 104:671.

Christine Basta, Marta R Costa-jussà, and Noe Casas.
2019. Evaluating the underlying gender bias in
contextualized word embeddings. arXiv preprint
arXiv:1904.08783.

Tolga Bolukbasi, Kai-Wei Chang, James Y Zou,
Venkatesh Saligrama, and Adam T Kalai. 2016.
Man is to computer programmer as woman is to
homemaker? debiasing word embeddings. In Proc.
of NIPS, pages 4349–4357.

Aylin Caliskan, Joanna J Bryson, and Arvind
Narayanan. 2017. Semantics derived automatically
from language corpora contain human-like biases.
Science, 356(6334):183–186.

Jacob Devlin, Ming-Wei Chang, Kenton Lee, and
Kristina Toutanova. 2019. Bert: Pre-training of deep
bidirectional transformers for language understand-
ing. In Proc. of NAACL.

Anjalie Field and Yulia Tsvetkov. 2019. Entity-centric
contextual affective analysis. In Proc. of ACL.

Nikhil Garg, Londa Schiebinger, Dan Jurafsky, and
James Zou. 2018. Word embeddings quantify
100 years of gender and ethnic stereotypes. Pro-
ceedings of the National Academy of Sciences,
115(16):E3635–E3644.

Peter Glick. 1991. Trait-based and sex-based discrimi-
nation in occupational prestige, occupational salary,
and hiring. Sex Roles, 25(5-6):351–378.

Anthony Greenwald, Debbie E. McGhee, and Jordan
L. K. Schwartz. 1998. Measuring individual differ-
ences in implicit cognition: The implicit association
test. Journal of personality and social psychology,
74:1464–80.

David Jurgens, Yulia Tsvetkov, and Dan Jurafsky.
2017. Incorporating dialectal variability for socially
equitable language identification. In Proc. of ACL,
pages 51–57.

Daniel Lee and Hyunjune Seung. 2001. Algorithms for
non-negative matrix factorization. In Proc. of NIPS.

Klas Leino, Matt Fredrikson, Emily Black, Shayak
Sen, and Anupam Datta. 2019. Feature-wise bias
amplification. In Prof. of ICLR.

Thomas Manzini, Yao Chong, Yulia Tsvetkov, and
Alan W Black. 2019. Black is to criminal as cau-
casian is to police: Detecting and removing multi-
class bias in word embeddings. In Proc. of NAACL.

Chandler May, Alex Wang, Shikha Bordia, Samuel R.
Bowman, and Rachel Rudinger. 2019. On measur-
ing social biases in sentence encoders. In Proc. of
NAACL.

Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Cor-
rado, and Jeff Dean. 2013. Distributed representa-
tions of words and phrases and their compositional-
ity. In Proc.of NIPS, pages 3111–3119.

Jeffrey Pennington, Richard Socher, and Christopher
Manning. 2014. GloVe: Global vectors for word
representation. In Proce. of EMNLP, pages 1532–
1543.

Matthew E Peters, Mark Neumann, Mohit Iyyer, Matt
Gardner, Christopher Clark, Kenton Lee, and Luke
Zettlemoyer. 2018. Deep contextualized word rep-
resentations. In Proc. of NAACL.

Rachel Rudinger, Jason Naradowsky, Brian Leonard,
and Benjamin Van Durme. 2018. Gender bias in
coreference resolution. In Proc. of NAACL.

J Segal. 2014. Social media use in hiring: Assessing
the risks. HR Magazine, 59(9).

Nathaniel Swinger, Maria De-Arteaga, Neil Heffer-
nan IV, Mark Leiserson, and Adam Kalai. 2019.
What are the biases in my word embedding? In
Proc. of the AAAI/ACM Conference on Artificial In-
telligence, Ethics, and Society (AIES).



172

Ian Tenney, Patrick Xia, Berlin Chen, Alex Wang,
Adam Poliak, R. Thomas McCoy, Najoung Kim,
Benjamin Van Durme, Samuel R. Bowman, Dipan-
jan Das, and Ellie Pavlick. 2019. What do you learn
from context? probing for sentence structure in con-
textualized word representations. In Proc. of ICLR.

Kellie Webster, Marta Recasens, Vera Axelrod, and Ja-
son Baldridge. 2018. Mind the gap: A balanced cor-
pus of gendered ambiguous pronouns. Transactions
of the Association for Computational Linguistics.

Sirui Yao and Bert Huang. 2017. Beyond parity: Fair-
ness objectives for collaborative filtering. In Ad-
vances in Neural Information Processing Systems,
pages 2921–2930.

Jieyu Zhao, Tianlu Wang, Mark Yatskar, Ryan Cot-
terell, Vicente Ordonez, and Kai-Wei Chang. 2019.
Gender bias in contextualized word embeddings. In
NAACL (short).

Jieyu Zhao, Tianlu Wang, Mark Yatskar, Vicente Or-
donez, and Kai-Wei Chang. 2017. Men also like
shopping: Reducing gender bias amplification using
corpus-level constraints. In Proc. of EMNLP.

Jieyu Zhao, Yichao Zhou, Zeyu Li, Wei Wang, and Kai-
Wei Chang. 2018. Learning gender-neutral word
embeddings.

Appendix

Topic Id Top 5 Words
1 match,round,second,team,season
2 times,city,jersey,york,new
3 married,son,died,wife,daughter
4 best,award,actress,films,film
5 friend,like,work,mother,life
6 university,music,attended,high,school
7 president,general,governor,party,state
8 songs,solo,song,band,album
9 medal,gold,final,won,world
10 best,role,character,television,series
11 kruse,moved,amy,esme,time
12 usa,trunchbull,pageant,2011,miss
13 american,august,brother,actress,born
14 sir,died,church,song,john
15 natasha,days,hospital,helene,later
16 played,debut,sang,role,opera
17 january,december,october,july,married
18 academy,member,american,university,family
19 award,best,played,mary,year
20 jersey,death,james,king,paul

Table 6: Extracted topics for the GPR dataset


