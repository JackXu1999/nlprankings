



















































The red one!: On learning to refer to things based on discriminative properties


Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, pages 213–218,
Berlin, Germany, August 7-12, 2016. c©2016 Association for Computational Linguistics

“The red one!”:
On learning to refer to things

based on discriminative properties

Angeliki Lazaridou and Nghia The Pham and Marco Baroni
University of Trento

{angeliki.lazaridou|thepham.nghia|marco.baroni}@unitn.it

Abstract

As a first step towards agents learning to
communicate about their visual environ-
ment, we propose a system that, given vi-
sual representations of a referent (CAT)
and a context (SOFA), identifies their dis-
criminative attributes, i.e., properties that
distinguish them (has_tail). More-
over, although supervision is only pro-
vided in terms of discriminativeness of
attributes for pairs, the model learns to
assign plausible attributes to specific ob-
jects (SOFA-has_cushion). Finally,
we present a preliminary experiment con-
firming the referential success of the pre-
dicted discriminative attributes.

1 Introduction

There has recently been renewed interest in devel-
oping systems capable of genuine language under-
standing (Hermann et al., 2015; Hill et al., 2015).
In this perspective, it is important to think of an
appropriate general framework for teaching lan-
guage to machines. Since we use language pri-
marily for communication, a reasonable approach
is to develop systems within a genuine commu-
nicative setup (Steels, 2003; Mikolov et al., 2015).
Out long-term goal is thus to develop communi-
ties of computational agents that learn how to use
language efficiently in order to achieve commu-
nicative success (Vogel et al., 2013; Foerster et al.,
2016).

Within this general picture, one fundamental as-
pect of meaning where communication is indeed
crucial is the act of reference (Searle, 1969; Ab-
bott, 2010), the ability to successfully talk to oth-
ers about things in the external world. A specific
instantiation of reference studied in this paper is
that of referring expression generation (Dale and

is_roundis_metal is_greenmade_of_wood

Figure 1: Discriminative attributes predicted by
our model. Can you identify the intended refer-
ent? See Section 6 for more information

Reiter, 1995; Mitchell et al., 2010; Kazemzadeh
et al., 2014). A necessary condition for achieving
successful reference is that referring expressions
(REs) accurately distinguish the intended referent
from any other object in the context (Dale and
Haddock, 1991). Along these lines, we present
here a model that, given an intended referent and a
context object, predicts the attributes that discrim-
inate between the two. Some examples of the be-
haviour of the model are presented in Figure 1.

Importantly, and distinguishing our work from
earlier literature on generating REs (Krahmer and
Van Deemter, 2012): (i) the input objects are
represented by natural images, so that the agent
must learn to extract relevant attributes from real-
istic data; and (ii) no direct supervision on the at-
tributes of a single object is provided: the training
signal concerns their discriminativeness for object
pairs (that is, during learning, the agent might be
told that has_tail is discriminative for 〈CAT,
SOFA〉, but not that it is an attribute of cats). We
use this “pragmatic” signal since it could later be
replaced by a measure of success in actual com-
munication between two agents (e.g., whether a
second agent was able to pick the correct referent
given a RE).

2 Discriminative Attribute Dataset

We generated the Discriminative Attribute
Dataset, consisting of pairs of (intended) referents
and contexts, with respect to which the referents
should be identified by their distinctive attributes.

213



〈referent, visual instances discriminative
context〉 attributes

〈CAT, SOFA 〉 has tail, has cushion,...

〈CAT, APPLE〉 has legs, is green, ...

Table 1: Example training data

Our starting point is the Visual Attributes for
Concepts Dataset (ViSA) (Silberer et al., 2013),
which contains per-concept (as opposed to per-
image) attributes for 500 concrete concepts (CAT,
SOFA, MILK) spanning across different categories
(MAMMALS, FURNITURE), annotated with 636
general attributes. We disregarded ambiguous
concepts (e.g., bat), thus reducing our working set
of concepts C to 462 and the number of attributes
V to 573, as we eliminated any attribute that did
not occur with concepts in C. We extracted on
average 100 images annotated with each of these
concepts from ImageNet (Deng et al., 2009). Fi-
nally, each image i of concept c was associated
to a visual instance vector, by feeding the image
to the VGG-19 ConvNet (Simonyan and Zisser-
man, 2014), as implemented in the MatConvNet
toolkit (Vedaldi and Lenc, 2015), and extracting
the second-to-last fully-connected (fc) layer as its
4096-dimensional visual representation vci .

We split the target concepts into training, val-
idation and test sets, containing 80%, 10% and
10% of the concepts in each category, respec-
tively. This ensures that (i) the intersection be-
tween train and test concepts is empty, thus al-
lowing us to test the generalization of the model
across different objects, but (ii) there are instances
of all categories in each set, so that it is possible
to generalize across training and testing objects.
Finally we build all possible combinations of con-
cepts in the training split to form pairs of refer-
ents and contexts 〈cr, cc〉 and obtain their (binary)
attribute vectors pcr and pcc from ViSA, result-
ing in 70K training pairs. From the latter, we de-
rive, for each pair, a concept-level “discriminative-
ness” vector by computing the symmetric differ-
ence dcr,cc = (pcr − pcc) ∪ (pcc − pcr). The
latter will contain 1s for discriminative attributes,
0s elsewhere. On average, each pair is associ-
ated with 20 discriminative attributes. The final
training data are triples of the form 〈cr, cc,dcr,cc〉
(the model never observes the attribute vectors of
specific concepts), to be associated with visual in-

stances of the two concepts. Table 1 presents some
examples.

Note that ViSA contain concept-level attributes,
but images contain specific instances of concepts
for which a general attribute might not hold. This
introduces a small amount of noise. For example,
is_green would in general be a discriminative
attribute for apples and cats, but it is not for the
second sample in Table 1. Using datasets with per-
image attribute annotations would solve this issue.
However, those currently available only cover spe-
cific classes of concepts (e.g., only clothes, or ani-
mals, or scenes, etc.). Thus, taken separately, they
are not general enough for our purposes, and we
cannot merge them, since their concepts live in dif-
ferent attribute spaces.

3 Discriminative Attribute Network

The proposed Discriminative Attribute Network
(DAN) learns to predict the discriminative at-
tributes of referent object cr and context cc without
direct supervision at the attribute level, but relying
only on discriminativeness information (e.g., for
the objects in the first row of Table 1, the gold vec-
tor would contain 1 for has_tail, but 0 for both
is_green and has_legs). Still, the model is
implicitly encouraged to embed objects into a con-
sistent attribute space, to generalize across the dis-
criminativeness vectors of different training pairs,
so it also effectively learns to annotate objects with
visual attributes.

Figure 2 presents a schematic view of DAN,
focusing on a single attribute. The model is pre-
sented with two concepts 〈CAT, SOFA〉, and ran-
domly samples a visual instance of each. The in-
stance visual vectors v (i.e., ConvNet second-to-
last fc layers) are mapped into attribute vectors of
dimensionality |V | (cardinality of all available at-
tributes), using weights Ma ∈ R4096×|V | shared
between the two concepts. Intuitively, this layer
should learn whether an attribute is active for a
specific object, as this is crucial for determining
whether the attribute is discriminative for an ob-
ject pair. In Section 5, we present experimental
evidence corroborating this hypothesis.

In order to capture the pairwise interactions be-
tween attribute vectors, the model proceeds by
concatenating the two units associated with the
same visual attribute v across the two objects (e.g.,
the units encoding information about has_tail)
and pass them as input to the discriminative layer.

214



Md
MD

has_tail

attribute layer
visual instance

vectors

visual instances

discriminative
layer

referent context

Ma Ma
MD

repeated |V| times 
with shared

and Md

Figure 2: Schematic representation of DAN. For
simplicity, the prediction process is only illus-
trated for has_tail

The discriminative layer processes the two units
by applying a linear transformation with weights
Md ∈ R2×h, followed by a sigmoid activation
function, finally deriving a single value by another
linear transformation with weights MD ∈ Rh×1.
The output d̂v encodes the predicted degree of
discriminativeness of attribute v for the specific
reference-context pair. The same process is ap-
plied to all attributes v ∈ V , to derive the esti-
mated discriminativeness vector d̂, using the same
shared weights Md and MD for each attribute.

To learn the parameters θ of the model (i.e. Ma,
Md and MD), given training data 〈cr, cc,dcr,cc〉,
we minimize MSE between the gold vector dcr,cc
and model-estimated d̂cr,cc . We trained the model
with rmsprop and with a batch size of 32. All hy-
perparameters (including the hidden size h which
was set to 60) were tuned to maximize perfor-
mance on the validation set.

4 Predicting Discriminativeness

We evaluate the ability of the model to predict
attributes that discriminate the intended referent
from the context. Precisely, we ask the model to
return all discriminative attributes for a pair, in-
dependently of whether they are positive for the
referent or for the context (given images of a cat
and a building, both +is_furry and −made_
of_bricks are discriminative of the cat).

Test stimuli We derive our test stimuli from the
VisA test split (see Section 2), containing 2000
pairs. Unlike in training, where the model was
presented with specific visual instances (i.e., sin-
gle images), for evaluation we use visual concepts
(CAT, BED), which we derive by averaging the
vectors of all images associated to an object (i.e.,
deriving CAT from all images of cats), due to lack

Model Precision Recall F1
DAN 0.66 0.49 0.56

attribute+sym. difference 0.64 0.48 0.55
no attribute layer 0.63 0.33 0.43
Random baseline 0.16 0.16 0.16

Table 2: Predicting discriminative features

of gold information on per-image attributes.

Results We compare DAN against a random
baseline based on per-attribute discriminativeness
probabilities estimated from the training data and
an ablation model without attribute layer. We test
moreover a model that is trained with supervi-
sion to predict attributes and then deterministically
computes the discriminative attributes. Specifi-
cally, we implemented a neural network with one
hidden layer, which takes as input a visual in-
stance, and it is trained to predict its gold attribute
vector, casting the problem as logistic regression,
thus relying on supervision at the attribute level.
Then, given two paired images, we let the model
generate their predicted attribute vectors and com-
pute the discriminative attributes by taking the
symmetric difference of the predicted attribute
vectors as we do for DAN. For the DAN and its ab-
lation, we use a 0.5 threshold to deem an attribute
discriminative, without tuning.

The results in Table 2 confirm that, with ap-
propriate supervision, DAN performs discrimina-
tiveness prediction reasonably well – indeed, as
well as the model with similar parameter capac-
ity requiring direct supervision on an attribute-by-
attribute basis, followed by the symmetric differ-
ence calculation. Interestingly, allowing the model
to embed visual representations into an interme-
diate attribute space has a strong positive effect
on performance. Intuitively, since DAN is eval-
uated on novel concepts, the mediating attribute
layer provides more high-level semantic informa-
tion helping generalization, at the expense of extra
parameters compared to the ablation without at-
tribute layer.

5 Predicting Attributes

Attribute learning is typically studied in su-
pervised setups (Ferrari and Zisserman, 2007;
Farhadi et al., 2009; Russakovsky and Fei-Fei,
2010). Our model learns to embed visual ob-
jects in an attribute space through indirect supervi-
sion about attribute discriminativeness for specific
<referent, context> pairs. Attributes are never

215



explicitly associated to a specific concept during
training. The question arises of whether discrim-
inativeness pushes the model to learn plausible
concept attributes. Note that the idea that the se-
mantics of attributes arises from their distinctive
function within a communication system is fully in
line with the classic structuralist view of linguistic
meaning (Geeraerts, 2009).

To test our hypothesis, we feed DAN the same
test stimuli (visual concept vectors) as in the pre-
vious experiment, but now look at activations in
the attribute layer. Since these activations are real
numbers whereas gold values (i.e., the visual at-
tributes in the ViSA dataset) are binary, we use
the validation set to learn the threshold to deem
an attribute active, and set it to 0.5 without tun-
ing. Note that no further training and no extra su-
pervision other than the discriminativeness signal
are needed to perform attribute prediction. The re-
sulting binary attribute vector p̂c for concept c is
compared against the corresponding gold attribute
vector pc.

Results We compare DAN to the random base-
line and to an explicit attribute classifier similar
to the one used in the previous experiment, i.e., a
one-hidden-layer neural network trained with lo-
gistic regression to predict the attributes. We re-
port moreover the best F1 score of Silberer et
al. (2013), who learn a SVM for each visual at-
tribute based on HOG visual features. Unlike in
our setup, in theirs, images for the same con-
cept are used both for training and to derive vi-
sual attributes (our setup is “zero-shot” at the con-
cept level, i.e., we predict attributes of concepts
not seen in training). Thus, despite the fact that
they used presumably less accurate pre-CNN vi-
sual features, the setup is much easier for them,
and we take their performance to be an upper
bound on ours.

DAN reaches, and indeed surpasses, the perfor-
mance of the model with direct supervision at the
attribute level, confirming the power of discrimi-
nativeness as a driving force in building semantic
representations. The comparison with Silberer’s
model suggests that there is room for improve-
ment, although the noise inherent in concept-level
annotation imposes a relatively low bound on re-
alistic performance.

Model Precision Recall F1
DAN 0.58 0.64 0.61

direct supervision 0.56 0.60 0.58
Silberer et. al. 0.70 0.70 0.70

Random baseline 0.13 0.12 0.12

Table 3: Predicting concept attributes

6 Evaluating Referential Success

We finally ran a pilot study testing whether DAN’s
ability to predict discriminative attributes at the
concept level translates into producing features
that would be useful in constructing successful ref-
erential expressions for specific object instances.

Test stimuli Our starting point is the ReferIt
dataset (Kazemzadeh et al., 2014), consisting
of REs denoting objects (delimited by bounding
boxes) in natural images. We filter out any 〈RE,
bounding box〉 pair whose RE does not overlap
with our attribute set V and annotate the remaining
ones with the overlapping attribute, deriving data
of the form 〈RE, bounding box, attribute〉.
For each intended referent of this type, we sample
as context another 〈RE, bounding box〉 pair such
that (i) the context RE does not contain the ref-
erent attribute, so that the latter is a likely
discriminative feature; (ii) referent and context
come from different images, so that their bound-
ing boxes do not accidentally overlap; (iii) there
is maximum word overlap between referent and
contexts REs, creating a realistic referential ambi-
guity setup (e.g., two cars, two objects in similar
environments). Finally we sample maximally 20
〈referent, context〉 pairs per attribute, result-
ing in 790 test items. For each referent and context
we extract CNN visual vectors from their bound-
ing boxes, and feed them to DAN to obtain their
discriminative attributes. Note that we used the
ViSA-trained DAN for this experiment as well.

Results For 12% of the test 〈referent, context〉
pairs, the discriminative attribute is con-
tained in the set of discriminative attributes pre-
dicted by DAN. A random baseline estimated from
the distribution of attributes in the ViSA dataset
would score 15% recall. This baseline however
on average predicts 20 discriminative attributes,
whereas DAN activates, only 4. Thus, the base-
line has a trivial recall advantage.

In order to evaluate whether in general the dis-
criminative attributes activated by DAN would
lead to referential success, we further sampled a

216



subset of 100 〈referent, context〉 test pairs. We
presented them separately to two subjects (one
a co-author of this study) together with the at-
tribute that the model activated with the largest
score (see Figure 1 for examples). Subjects were
asked to identify the intended referent based on
the attribute. If both agreed on the same referent,
we achieved referential success, since the model-
predicted attribute sufficed to coherently discrim-
inate between the two images. Encouragingly,
the subjects agreed on 78% of the pairs (p<0.001
when comparing against chance guessing, accord-
ing to a 2-tailed binomial test). In cases of dis-
agreement, the predicted attribute was either too
generic or very salient in both objects, a behaviour
observed especially in same-category pairs (e.g.,
is_round in Figure 1).

7 Concusion

We presented DAN, a model that, given a ref-
erent and a context, learns to predict their dis-
criminative features, while also inferring visual at-
tributes of concepts as a by-product of its train-
ing regime. While the predicted discriminative
attributes can result in referential success, DAN
is currently lacking all other properties of refer-
ence (Grice, 1975) (salience, linguistic and prag-
matic felicity, etc). We are currently working to-
wards adding communication (thus simulating a
speaker-listener scenario (Golland et al., 2010))
and natural language to the picture.

Acknowledgments

This work was supported by ERC 2011 Starting
Independent Research Grant n. 283554 (COM-
POSES). We gratefully acknowledge the support
of NVIDIA Corporation with the donation of the
GPUs used for this research.

References

Barbara Abbott. 2010. Reference. Oxford University
Press, Oxford, UK.

Robert Dale and Nicholas Haddock. 1991. Content
determination in the generation of referring expres-
sions. Computational Intelligence, 7(4):252–265.

Robert Dale and Ehud Reiter. 1995. Computational
interpretations of the gricean maxims in the gener-
ation of referring expressions. Cognitive science,
19(2):233–263.

Jia Deng, Wei Dong, Richard Socher, Lia-Ji Li, and
Li Fei-Fei. 2009. Imagenet: A large-scale hierarchi-
cal image database. In Proceedings of CVPR, pages
248–255, Miami Beach, FL.

Ali Farhadi, Ian Endres, Derek Hoiem, and David
Forsyth. 2009. Describing objects by their at-
tributes. In Proceedings of CVPR, pages 1778–
1785, Miami Beach, FL.

Vittorio Ferrari and Andrew Zisserman. 2007. Learn-
ing visual attributes. In Proceedings of NIPS, pages
433–440, Vancouver, Canada.

Jakob N. Foerster, Yannis M. Assael, Nando de Fre-
itas, and Shimon Whiteson. 2016. Learning
to communicate to solve riddles with deep dis-
tributed recurrent q-networks. Technical Report
arXiv:1602.02672.

Dirk Geeraerts. 2009. Theories of lexical semantics.
Oxford University Press, Oxford, UK.

Dave Golland, Percy Liang, and Dan Klein. 2010.
A game-theoretic approach to generating spatial de-
scriptions. In Proceedings of the 2010 conference
on empirical methods in natural language process-
ing, pages 410–419. Association for Computational
Linguistics.

Herbert P Grice. 1975. Logic and conversation. Syn-
tax and Semantics.

Karl Moritz Hermann, Tomáš Kočiský, Edward
Grefenstette, Lasse Espeholt, Will Kay, Mustafa Su-
leyman, and Phil Blunsom. 2015. Teaching ma-
chines to read and comprehend. In Advances in Neu-
ral Information Processing Systems (NIPS).

Felix Hill, Antoine Bordes, Sumit Chopra, and Jason
Weston. 2015. The Goldilocks principle: Read-
ing children’s books with explicit memory repre-
sentations. http://arxiv.org/abs/1511.
02301.

Sahar Kazemzadeh, Vicente Ordonez, Mark Matten,
and Tamara L Berg. 2014. Referitgame: Refer-
ring to objects in photographs of natural scenes. In
EMNLP, pages 787–798.

Emiel Krahmer and Kees Van Deemter. 2012. Compu-
tational generation of referring expressions: A sur-
vey. Computational Linguistics, 38(1):173–218.

Tomas Mikolov, Armand Joulin, and Marco Baroni.
2015. A roadmap towards machine intelligence.
arXiv preprint arXiv:1511.08130.

Margaret Mitchell, Kees van Deemter, and Ehud Re-
iter. 2010. Natural reference to objects in a visual
domain. In Proceedings of the 6th international nat-
ural language generation conference, pages 95–104.
Association for Computational Linguistics.

Olga Russakovsky and Li Fei-Fei. 2010. Attribute
learning in large-scale datasets. In Proceedings of
ECCV, pages 1–14.

217



John R. Searle. 1969. Speech Acts: An Essay in
the Philosophy of Language. Cambridge University
Press.

Carina Silberer, Vittorio Ferrari, and Mirella Lapata.
2013. Models of semantic representation with visual
attributes. In Proceedings of ACL, pages 572–582,
Sofia, Bulgaria.

Karen Simonyan and Andrew Zisserman. 2014. Very
deep convolutional networks for large-scale image
recognition. arXiv preprint arXiv:1409.1556.

Luc Steels. 2003. Social language learning. In Mario
Tokoro and Luc Steels, editors, The Future of Learn-
ing, pages 133–162. IOS, Amsterdam.

Andrea Vedaldi and Karel Lenc. 2015. MatConvNet –
Convolutional Neural Networks for MATLAB. Pro-
ceeding of the ACM Int. Conf. on Multimedia.

Adam Vogel, Max Bodoia, Christopher Potts, and
Daniel Jurafsky. 2013. Emergence of gricean max-
ims from multi-agent decision theory. In HLT-
NAACL, pages 1072–1081.

218


