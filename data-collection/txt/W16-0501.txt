



















































The Effect of Multiple Grammatical Errors on Processing Non-Native Writing


Proceedings of the 11th Workshop on Innovative Use of NLP for Building Educational Applications, pages 1–11,
San Diego, California, June 16, 2016. c©2016 Association for Computational Linguistics

The Effect of Multiple Grammatical Errors on Processing
Non-Native Writing

Courtney Napoles
Johns Hopkins University
courtneyn@jhu.edu

Aoife Cahill Nitin Madnani
Educational Testing Service

{acahill,nmadnani}@ets.org

Abstract

In this work, we estimate the deterioration
of NLP processing given an estimate of the
amount and nature of grammatical errors in
a text. From a corpus of essays written
by English-language learners, we extract un-
grammatical sentences, controlling the num-
ber and types of errors in each sentence. We
focus on six categories of errors that are com-
monly made by English-language learners,
and consider sentences containing one or more
of these errors. To evaluate the effect of gram-
matical errors, we measure the deterioration of
ungrammatical dependency parses using the
labeled F-score, an adaptation of the labeled
attachment score. We find notable differences
between the influence of individual error types
on the dependency parse, as well as interac-
tions between multiple errors.

1 Introduction

With the large number of English-language learn-
ers and the prevalence of informal web text, noisy
text containing grammatical errors is widespread.
However, the majority of NLP tools are developed
and trained over clean, grammatical text and the
performance of these tools may be negatively af-
fected when processing errorful text. One possi-
ble workaround is to adapt tools for noisy text, e.g.
(Foster et al., 2008; Cahill et al., 2014). However, it
is often preferable to use tools trained on clean text,
mainly because of the resources necessary for train-
ing and the limited availability of large-scale anno-
tated corpora, but also because tools should work
correctly in the presence of well-formed text.

Our goal is to measure the performance degrada-
tion of an automatic NLP task based on an estimate
of grammatical errors in a text. For example, if we
are processing student responses within an NLP ap-
plication, and the responses contain a mix of native
and non-native texts, it would be useful to be able
to estimate the difference in performance (if any) of
the NLP application on both types of texts.

We choose dependency parsing as our prototypic
task because it is often one of the first complex
downstream tasks in NLP pipelines. We will con-
sider six common grammatical errors made by non-
native speakers of English and systematically con-
trol the number and types of errors present in a sen-
tence. As errors are introduced to a sentence, the
degradation of the dependency parse is measured by
the decrease in the F-score over dependency rela-
tions.

In this work, we will show that

• increasing the number of errors in a sentence
decreases the accuracy of the dependency parse
(Section 4.1);
• the distance between errors does not affect the

accuracy (Section 4.2);
• some types of grammatical errors have a greater

impact, alone or in combination with other er-
rors (Section 4.3).

While these findings may seem self-evident, they
have not previously been quantified on a large cor-
pus of naturally occurring errors. Our analysis will
serve as the first step to understanding what happens
to a NLP pipeline when confronted with grammati-
cal errors.

1



2 Data

Previous research concerning grammatical errors
has artificially generated errors over clean text, such
as Foster et al. (2008) and Felice and Yuan (2014),
among others. While this is one approach for build-
ing a large-scale corpus of grammatical and ungram-
matical sentence pairs, we use text with naturally oc-
curring errors so that our analysis covers the types of
errors typically seen in non-native writing.

As the source of our data, we use the training sec-
tion of the NUS Corpus of Learner English (NU-
CLE),1 which is a large corpus of essays written
by non-native English speakers (Dahlmeier et al.,
2013). The NUCLE corpus has been annotated with
corrections to the grammatical errors, and each error
has been labeled with one of 28 error categories.

We will only consider the following common er-
rors types, which constitute more than 50% of the 44
thousand corrections in NUCLE:

• Article or determiner [Det]
• Mechanical (punctuation, capitalization, and

spelling) [Mec]
• Noun number [Noun]
• Preposition [Prep]
• Word form [Wform]
• Verb tense and verb form [Verb]
While other error coding schemes specify the na-

ture of the error (whether the text is unnecessary,
missing, or needs to be replaced) in addition to the
word class (Nicholls, 2004), the NUCLE error cat-
egories do not make that distinction. Therefore we
automatically labeled each error with an additional
tag for the operation of the correction, depending on
whether it was missing a token, had an unnecessary
token, or needed to replace a token. We labeled all
noun, verb, and word form errors as replacements,
and automatically detected the label of article, me-
chanical, and preposition errors by comparing the
tokens in the original and corrected spans of text. If
the correction had fewer unique tokens than the orig-
inal text, it was labeled unnecessary. If the correc-
tion had more unique tokens, it was labeled missing.
Otherwise the operation was labeled a replacement.
To verify the validity of this algorithm, we reviewed
the 100 most frequent error–correction pairs labeled

1Version 3.2

Det Mec Noun Prep Verb Wform
0

1000

2000

3000

4000

5000

6000

7000

8000
Unnecessary
Missing
Replace

Figure 1: The number of corrections by error type
and operation that we used in this study.

with each operation, which encompasses 69% of the
errors in the corpus.2

To compile our corpus of sentences, we selected
all of the corrections from NUCLE addressing one
of the six error types above. We skipped corrections
that spanned multiple sentences or the entire length
of a sentence, as well as corrections that addressed
punctuation spacing, since those errors would likely
be addressed during tokenization.3

We identified 14,531 NUCLE sentences contain-
ing errors subject to these criteria. We applied the
corrections of all other types of errors and, in the
rest of our analysis, we will use the term errors to
refer only to errors of the types outlined above. On
average, each of these sentence has 26.4 tokens and
1.5 errors, with each error spanning 1.2 tokens and
the correction 1.5 tokens. In total, there are 22,123
errors, and Figure 1 shows the total number of cor-
rections by error type and operation. Because of the
small number of naturally occurring sentences with
exactly 1, 2, 3, or 4 errors (Table 1), we chose to gen-
erate new sentences with varying numbers of errors
from the original ungrammatical sentences.

For each of the NUCLE sentences, we generated
ungrammatical sentences with n errors by system-
atically selecting n corrections to ignore, applying
all of the other corrections. We generated sentences

2Many error–correction pairs are very frequent: for exam-
ple, inserting or deleting the accounts for 3,851 of the errors
and inserting or deleting a plural s 2,804.

3NLTK was used for sentence and token segmentation
(http://nltk.org).

2



NUCLE Generated Exactly
# errors sentences sentences n errors

1 14,531 22,123 9,474
2 5,030 11,561 3,341
3 572 5,085 0
4 570 3,577 362

Table 1: The number of NUCLE sentences contain-
ing at least n errors, the number of sentences with n
errors that were generated from them, and the num-
ber of NUCLE sentences with exactly n errors.

with n = 1 to 4 errors, when there were at least
n corrections to the original sentence. For exam-
ple, a NUCLE sentence with 6 annotated corrections
would yield the following number of ungrammatical
sentences: 6 sentences with one error,

(
6
2

)
= 15

sentences with two errors,
(

6
3

)
= 20 sentences with

three errors, and so on. The number of original NU-
CLE sentences and generated sentences with each
number of errors is shown in Table 1. We also gen-
erated a grammatical sentence with all of the correc-
tions applied for comparison.

We parsed each sentence with the ZPar con-
stituent parser (Zhang and Clark, 2011) and gen-
erated dependency parses from the ZPar output us-
ing the Stanford Dependency Parser4 and the univer-
sal dependencies representation (De Marneffe et al.,
2014). We make the over-confident assumption that
the automatic analyses in our pipeline (tokenization,
parsing, and error-type labeling) are all correct.

Our analysis also depends on the quality of the
NUCLE annotations. When correcting ungrammat-
ical text, annotators are faced with the decisions
of whether a text needs to be corrected and, if so,
how to edit it. Previous work has found low inter-
annotator agreement for the basic task of judging
whether a sentence is grammatical (0.16 ≤ κ ≤
0.40) (Rozovskaya and Roth, 2010). The NUCLE
corpus is no different, with the three NUCLE anno-
tators having moderate agreement on how to correct
a span of text (κ = 0.48) and only fair agreement
for identifying what span of text needs to be cor-
rected (κ = 0.39) (Dahlmeier et al., 2013). Low
inter-annotator agreement is not necessarily an indi-
cation of the quality of the annotations, since it could

4Using the EnglishGrammaticalStructure class with the
flags -nonCollapsed -keepPunct.

also be attributed to the diversity of appropriate cor-
rections that have been made. We assume that the
annotations are correct and complete, meaning that
the spans and labels of annotations are correct and
that all of the grammatical errors are annotated. We
further assume that the annotations only fix gram-
matical errors, instead of providing a stylistic alter-
natives to grammatical text.

3 Metric: Labeled F-score

To measure the effect of grammatical errors on the
performance of the dependency parser, we compare
the dependencies identified in the corrected sentence
to those from the ungrammatical sentence.

The labeled attachment score (LAS) is a com-
monly used method for evaluating dependency
parsers (Nivre et al., 2004). The LAS calculates the
accuracy of the dependency triples from the candi-
date dependency graph with respect to those of the
gold standard, where each triple represents one re-
lation, consisting of the head, dependent, and type
of relation. The LAS assumes that the surface forms
of the sentences are identical but only the relations
have changed. In this work, we require a method
that accommodates unaligned tokens, which occur
when an error involves deleting or inserting tokens
and unequal surface forms (replacement errors).

There are some metrics that compare the parses
of unequal sentences, including SParseval (Roark et
al., 2006) and TEDeval (Tsarfaty et al., 2011), how-
ever neither of these metrics operate over dependen-
cies. We chose to evaluate dependencies because
dependency-based evaluation has been shown to be
more closely related to the linguistic intuition of
good parses compared to two other tree-based eval-
uations (Rehbein and van Genabith, 2007).

Since we cannot calculate the LAS over sentences
of unequal lengths, we instead measure the F1-score
of the dependency relations. So that substitutions
(such as morphological changes) are not severely pe-
nalized, we represent tokens with their index instead
of the surface form. First, we align the tokens in the
grammatical and ungrammatical sentences and as-
sign an index to each token such that the aligned to-
kens in each sentence share the same index. Because
reordering is uncommon in the NUCLE corrections,
we use dynamic programming to find the lowest-

3



cost alignment between a sentence pair, where the
cost for insertions and deletions is 1, and substitu-
tions receive a cost proportionate to the Levenshtein
edit distance between the tokens (to award “partial
credit” for inflections).

We calculate the Labeled F-score (LF) over de-
pendency relations of the form <head index, de-
pendent index, relation>. This evaluation metric
can be used for comparing the dependency parses of
aligned sentences with unequal lengths or tokens.5

A variant of the LAS, the Unlabeled Attachment
Score, is calculated over pairs of heads and depen-
dents without the relation. We considered the corre-
sponding unlabeled F-score and, since there was no
meaningful difference between that and the labeled
F-score, we chose to use labeled relations for greater
specificity.

In the subsequent analysis, we will focus on the
difference in LF before and after an error is intro-
duced to a sentence. We will refer to the LF of a
sentence with n errors as LFn. The LF of a sen-
tence identical to the correct sentence is 100, there-
fore LF0 is always 100. The decrease in LF of an
ungrammatical sentence with n errors from the cor-
rect parse is LF0−LFn = 100−LFn, where a higher
value indicates a larger divergence from the correct
dependency parse.

4 Analysis

Our analysis will be broken down by different char-
acteristics of ungrammatical sentences and quanti-
fying their effect on the LF. Specifically, we will ex-
amine increasing numbers of errors in a sentence,
the distance between errors, individual error types,
and adding more errors to an already ungrammatical
sentence.

4.1 Number of errors

The first step of our analysis is to verify our hy-
pothesis that the absolute LF decrease (LF0 − LFn)
increases as the number of errors in a sentence in-
creases from n = 1 to n = 4. Pearson’s correlation
reveals a weak correlation between the LF decrease
and number of errors (Figure 2). Since this analysis
will be considering sentences generated with only a

5Available for download at https://github.com/
cnap/ungrammatical-dependencies.

1 2 3 4
Number of errors

0

10

20

30

40

50

LF
 d

ec
re

as
e

r=0.31

All generated sentences
with n errors

1 2 3 4
Number of errors

r=0.22

Sentences with exactly
n errors originally

Figure 2: Mean absolute decrease in LF by the num-
ber of errors in a sentence (100− LFn).

30.4%11.8%

16.4%

10.9%
20.9%

9.6%

Det
Mec
Noun
Prep
Verb
Wform

Figure 3: The distribution of error types in sen-
tences with one error. The distribution is virtually
identical (±2 percentage points) in sentences with
2–4 errors.

subset of errors from the original sentence, we will
verify the validity of this data by comparing the LF
decrease of the generated sentences to the LF de-
crease of sentences that originally had exactly n er-
rors. Since the LF decreases of the generated and
original sentences are very similar, we presume that
the generated sentences exhibit similar properties as
the original sentences with the same number of er-
rors. We further compared the distribution of sen-
tences with each error type as the number of errors
per sentence changes, and find that the distribution
is fairly constant. The distribution of sentences with
one error is shown in Figure 3. We will next inves-
tigate whether the LF decrease is due to interaction
between errors or if there is an additive effect.

4



0 10 20 30 40 50
Distance between errors (tokens)

0

10

20

30

40

50

60

70

LF
 d

ec
re

as
e

r = -0.07

Figure 4: Distance between two errors and the de-
crease in LF.

4.2 Distance between errors

To determine whether the distance between errors is
a factor in dependency performance, we took sen-
tences with only two errors and counted the num-
ber of tokens between the errors (Figure 4). Sur-
prisingly, there is no relationship between the dis-
tance separating errors and the dependency parse ac-
curacy. We hypothesized that errors near each other
would either interact and cause the parser to misin-
terpret more of the sentence, or conversely that they
would disrupt the interpretation of only one clause
and not greatly effect the LF. However, neither of
these were evident based on the very weak negative
correlation. For sentences with more than two er-
rors, we calculated the mean, minimum, and maxi-
mum distances between all errors in each sentence,
and found a weak to very weak negative correla-
tion between those measures and the LF decrease
(−0.15 ≤ r ≤ −0.04).

4.3 Error type and operation

Next, we considered specific error types and their
operation—whether they were missing, unneces-
sary, or needed replacement. To isolate the impact
of individual error types on the LF, we calculated
the mean LF decrease (100−LF1) by error and oper-
ation over sentences with only one error (Figure 5).

The mean values by error type are shown in Figure 6,
column 1.

Two trends are immediately visible: there is a
clear difference between error types and, except for
determiner errors, missing and unnecessary errors
have a greater impact on the dependency parse than
replacements. Nouns and prepositions needing re-
placement have the lowest impact on the LF, with
100− LF1 < 4. This could be because the part
of speech tag for these substitutions does not of-
ten change (or only change NN to NNS in the case
of nouns), which would therefore not greatly af-
fect a dependency parser’s interpretation of the sen-
tence, but this hypothesis needs to be verified in fu-
ture work. A prepositional phrase and noun phrase
would likely still be found headed by that word.
Verb replacements exhibit more than twice the de-
crease in LF than nouns and prepositions. Unlike
noun and preposition replacements, replacing a verb
tends to elicit greater structural changes, since some
verbs can be interpreted as nouns or past partici-
ples and gerunds could be interpreted as modifying
nouns, etc. (Lee and Seneff, 2008).

Determiner errors also have a low impact on LF
and there is practically no difference by the oper-
ation of the correction. This can be explained be-
cause determiners occur at the beginning of noun
phrases, and so deleting, inserting, or replacing a
determiner would typically affect one child of the
noun phrase and not the overall structure. How-
ever, mechanical errors and missing or unnecessary
prepositions have a great impact on the LF, with LF1
at least 10% lower than LF0. Inserting or deleting
these types of words could greatly alter the struc-
ture of a sentence. For example, inserting a miss-
ing preposition would introduce a new prepositional
phrase and the subsequent noun phrase would attach
to that phrase. Regarding Mec errors, inserting com-
mas can drastically change the structure by breaking
apart constituents, and removing commas can cause
constituents to become siblings.

4.4 Adding errors to ungrammatical sentences

We have seen the mean LF decrease in sentences
with one error, over different error types. Next,
we examine what happens to the dependency parse
when an error is added to a sentence that is al-
ready ungrammatical. We calculated the LF of sen-

5



Det Mec Noun Prep Verb Wform
0

2

4

6

8

10

12

14

Missing Unnecessary Replace

Figure 5: The mean decrease in LF (100−LF1) for
sentences with one error, by error type.

tences with one error (LF1), introduced a second er-
ror into that sentence, and calculated the decrease in
LF (LF1−LF2). We controlled for the types of errors
both present in the original sentence and introduced
to the sentence, not differentiating the operation of
the error for ease of interpretation. The mean differ-
ences by error types are in Figure 6.

Each column indicates what type of error was
present in the original sentence (or the first er-
ror), with None indicating the original sentence was
grammatically correct and had no errors. Each row
represents the type of error that was added to the sen-
tence (the second error). Note that this does not indi-
cate the left–right order of the errors. This analysis
considers all combinations of errors: for example,
given a sentence with two determiner errors A and
B, we calculate the LF decrease after inserting error
A into the sentence that already had error B and vice
versa.

Generally, with respect to the error type, the rela-
tive magnitude of change caused by adding the sec-
ond error (column 2) is similar to adding that type
of error to a sentence with no errors (column 1).
However, introducing the second error always has
a lower mean LF decrease than introducing the first
error into a sentence, suggesting that each added er-
ror is less disruptive to the dependency parse as the
number of errors increase.

To verify this, we added an error to sentences
with 0 to 3 errors and calculated the LF change

None Det Mec Noun Prep Verb Wform
Error in original sentence

Wform

Verb

Prep

Noun

Mec

Det

In
se

rt
ed

 e
rr

or

6.8 4.8 5.3 5.1 4.5 4.7 6.6

8.0 6.4 5.9 6.6 5.2 5.1 7.3

5.8 4.2 3.8 4.3 3.9 3.0 3.4

3.6 2.8 2.3 2.7 1.9 2.7 2.4

8.9 6.7 4.6 6.8 5.6 6.2 6.0

5.3 4.2 3.5 4.1 4.2 3.7 4.2

3.0 4.5 6.0 7.5

Figure 6: Mean decrease in LF (LF1−LF2) for sen-
tences when introducing an error (row) into a sen-
tence that already has an error of the type in the col-
umn. The None column contains the mean decrease
when introducing a new error to a grammatical sen-
tence (100− LF1).

(LFn − LFn+1) each time a new error was intro-
duced. Figure 7 shows the mean LF decrease after
adding an error of a given type to a sentence that
already had 0, 1, 2, or 3 errors.

Based on Figure 7, it appears that the LF decrease
may converge for some error types, specifically de-
terminer, preposition, verb, and noun errors. How-
ever, the LF decreases at a fairly constant rate for
mechanical and word form errors, suggesting that
ungrammatical sentences become increasingly unin-
terpretable as these types of errors are introduced.
Further research is needed to make definitive claims
about what happens as a sentence gets increasingly
errorful.

5 Qualifying LF decrease

In the previous analysis, the range of LF decreases
are from 1 to around 10, suggesting that approx-
imately 1% to 10% of the dependency parse was
changed due to errors. However, this begs the ques-
tion of what a LF decrease of 1, 5, or 10 actually
means for a pair of sentences. Is the ungrammati-
cal sentence garbled after the LF decrease reaches a
certain level? How different are the dependencies
found in a sentence with a LF decrease of 1 ver-
sus 10? To illustrate these differences, we selected
an example sentence and calculated the LF decrease

6



0 1 2 3
Number of errors in original sentence

1

2

3

4

5

6

7

8

9
LF

 d
ec

re
as

e 
af

te
r a

dd
in

g 
an

 e
rr

or

Det
Mec

Noun
Prep

Verb
Wform

Figure 7: Mean decrease in LF (LFn−LFn+1) when
an error of a given type is added to a sentence that
already has n errors.

and dependency graph as more errors were added
(Table 2, Figure 8, and Figure 9).

Notice that the largest decrease in LF occurs after
the first and second errors are introduced (10 and
13 points, respectively). The introductions of these
errors result in structural changes of the graph, as
does the fourth error, which results in a lesser LF
decrease of 5. In contrast, the third error, a missing
determiner, causes a lesser decrease of about 2, since
the graph structure is not affected by this insertion.

Considering the LF decrease as the percent of a
sentence that is changed, for a sentence with 26 to-
kens (the mean length of sentences in our dataset),
a LF decrease of 5 corresponds to a change in 1.3
of the tokens, while a decrease of 10 corresponds to
a change in 2.6 tokens. Lower LF decreases (< 5
or so) generally indicate the insertion or deletion of
a token that does not affect the graph structure, or
changing the label of a dependency relation. On the
other hand, greater decreases likely reflect a struc-
tural change in the dependency graph of the ungram-
matical sentence, which affects more relations than
those containing the ungrammatical tokens.

6 Related work

There is a modest body of work focused on improv-
ing parser performance of ungrammatical sentences.

Unlike our experiments, most previous work has
used small (around 1,000 sentences) or artificially
generated corpora of ungrammatical/grammatical
sentence pairs.

The most closely related works compared the
structure of constituent parses of ungrammatical to
corrected sentences: with naturally occurring errors,
Foster (2004) and Kaljahi et al. (2015) and evalu-
ate parses of ungrammatical text based on the con-
stituent parse and Geertzen et al. (2013) evaluate
performance over dependencies. Cahill (2015) ex-
amines the parser performance using artificially gen-
erated errors, and Foster (2007) analyzes the parses
of both natural and artificial errors. In Wagner and
Foster (2009), the authors compared the parse prob-
abilities of naturally occurring and artificially gener-
ated ungrammatical sentences to the probabilities of
the corrected sentences. They found that the natural
ungrammatical sentences had a lower reduction in
parse probability than artificial sentences, suggest-
ing that artificial errors are not interchangeable with
spontaneous errors. This analysis suggests the im-
portance of using naturally occurring errors, which
is why we chose to generate sentences from the
spontaneous NUCLE errors.

Several studies have attempted to improve the ac-
curacy of parsing ungrammatical text. Some ap-
proaches include self-training (Foster et al., 2011;
Cahill et al., 2014), retraining (Foster et al., 2008),
and transforming the input and training text to be
more similar (Foster, 2010). Other work with un-
grammatical learner text includes Caines and But-
tery (2014), which identifies the need to improve
parsing of spoken learner English, and Tetreault et
al. (2010), which analyzes the accuracy of preposi-
tional phrase attachment in the presence of preposi-
tion errors.

7 Conclusion and future work

The performance of NLP tools over ungrammatical
text is little understood. Given the expense of anno-
tating a grammatical-error corpus, previous studies
have used either small annotated corpora or gener-
ated artificial grammatical errors in clean text.

This study represents the first large-scale analysis
of the effect of grammatical errors on a NLP task.
We have used a large, annotated corpus of grammat-

7



ical errors to generate more than 44,000 sentences
with up to four errors in each sentence. The ungram-
matical sentences contain an increasing number of
naturally occurring errors, facilitating the compari-
son of parser performance as more errors are intro-
duced to a sentence. This is the first step toward a
larger goal of providing a confidence score of parser
accuracy based on an estimate of how ungrammati-
cal a text may be. While many of our findings may
seem obvious, they have previously not been quan-
tified on a large corpus of naturally occurring gram-
matical errors. In the future, these results should be
verified over a selection of manually corrected de-
pendency parses.

Future work includes predicting the LF decrease
based on an estimate of the number and types of er-
rors in a sentence. As yet, we have only measured
change by the LF decrease over all dependency rela-
tions. The decrease can also be measured over indi-
vidual dependency relations to get a clearer idea of
which relations are affected by specific error types.
We will also investigate the effect of grammatical
errors on other NLP tasks.

We chose the NUCLE corpus because it is the
largest annotated corpus of learner English (1.2 mil-
lion tokens). However, this analysis is relies on
the idiosyncrasies of this particular corpus, such as
the typical sentence length and complexity. The es-
says were written by students at the National Uni-
versity of Singapore, who do not have a wide vari-
ety of native languages. The types and frequency
of errors differ depending on the native language
of the student (Rozovskaya and Roth, 2010), which
may bias the analysis herein. The available corpora
that contain a broader representation of native lan-
guages are much smaller than the NUCLE corpus:
the Cambridge Learner Corpus–First Certificate in
English has 420 thousand tokens (Yannakoudakis et
al., 2011), and the corpus annotated by (Rozovskaya
and Roth, 2010) contains only 63 thousand words.

One limitation to our method for generating un-
grammatical sentences is that relatively few sen-
tences are the source of ungrammatical sentences
with four errors. Even though we drew sentences
from a large corpus, only 570 sentences had at least
four errors (of the types we were considering), com-
pared to 14,500 sentences with at least one error.
Future work examining the effect of multiple errors

would need to consider a more diverse set of sen-
tences with more instances of at least four errors,
since there could be peculiarities or noise in the orig-
inal annotations, which would be amplified in gen-
erated sentences.

Acknowledgments

We would like to thank Martin Chodorow and Jen-
nifer Foster for their valuable insight while devel-
oping this research, and Beata Beigman Klebanov,
Brian Riordan, Su-Youn Yoon, and the BEA review-
ers for their helpful feedback. This material is based
upon work partially supported by the National Sci-
ence Foundation Graduate Research Fellowship un-
der Grant No. 1232825.

References
Aoife Cahill, Binod Gyawali, and James Bruno. 2014.

Self-training for parsing learner text. In Proceed-
ings of the First Joint Workshop on Statistical Pars-
ing of Morphologically Rich Languages and Syntactic
Analysis of Non-Canonical Languages, pages 66–73,
Dublin, Ireland, August. Dublin City University.

Aoife Cahill. 2015. Parsing learner text: To shoehorn
or not to shoehorn. In Proceedings of The 9th Lin-
guistic Annotation Workshop, pages 144–147, Denver,
Colorado, USA, June. Association for Computational
Linguistics.

Andrew Caines and Paula Buttery. 2014. The effect
of disfluencies and learner errors on the parsing of
spoken learner language. In Proceedings of the First
Joint Workshop on Statistical Parsing of Morphologi-
cally Rich Languages and Syntactic Analysis of Non-
Canonical Languages, pages 74–81, Dublin, Ireland,
August. Dublin City University.

Daniel Dahlmeier, Hwee Tou Ng, and Siew Mei Wu.
2013. Building a large annotated corpus of learner en-
glish: The NUS Corpus of Learner English. In Pro-
ceedings of the Eighth Workshop on Innovative Use
of NLP for Building Educational Applications, pages
22–31, Atlanta, Georgia, June. Association for Com-
putational Linguistics.

Marie-Catherine De Marneffe, Timothy Dozat, Natalia
Silveira, Katri Haverinen, Filip Ginter, Joakim Nivre,
and Christopher D Manning. 2014. Universal Stan-
ford dependencies: A cross-linguistic typology. In
Proceedings of Language Resources and Evaluation
Conference (LREC), volume 14, pages 4585–4592.

Mariano Felice and Zheng Yuan. 2014. Generating arti-
ficial errors for grammatical error correction. In Pro-
ceedings of the Student Research Workshop at the 14th

8



Num. Inserted LF
errors error type decrease Sentence

0 n/a n/a One of the factors that determines and shapes technological inno-
vation the most is the country ’s economic status .

1 Verb 10.0 One of the factors that determined and shapes technological in-
novation the most is the country ’s economic status .

2 Mec 13.1 One of the factors that determined and shapes technological in-
novation the most is the country economic status .

3 Det 1.9 One of the factors that determined and shapes the technological
innovation the most is the country economic status .

4 Verb 5.0 One of the factors that determined and shaped the technological
innovation the most is the country economic status .

Table 2: An example of a sentence with 4 errors added and the LF decrease (LFn−1 − LFn) after adding
each subsequent error to the previous sentence. Changed text is shown in bold italics.

One

determines

acl:relcl

factors

nmod

that

nsubj

and

cc

shapes

conj

most

advmod

of

case

the

det

innovation

dobj

the

det

technological

amod

is

the

country

det

's

case

economic

ROOT
status

nsubj cop nmod:poss amod

.

punct

Figure 8: Dependency graph of the correct sentence in Table 2.

9



1 error 2 errors
ROOT

status

One

nsubj

is

cop

country

nmod:poss

economic

amod

.

punct

factors

nmod

of

case

the

det

determined

acl:relcl

that

nsubj

and

cc

shapes

conj

most

advmod

innovation

dobj

the

det

technological

amod

the

det

's

case

ROOT

status

One

nsubj

is

cop

the

det

country

amod

economic

amod

.

punct

factors

nmod

of

case

the

det

determined

acl:relcl

that

nsubj

and

cc

shapes

conj

most

advmod

innovation

dobj

the

det

technological

amod

3 errors 4 errors
ROOT

status

One

nsubj

is

cop

the

det

country

amod

economic

amod

.

punct

factors

nmod

of

case

the

det

determined

acl:relcl

that

nsubj

and

cc

shapes

conj

most

advmod

innovation

dobj

the

det

the

det

technological

amod

ROOT

status

One

nsubj

is

cop

the

det

country

amod

economic

amod

.

punct

factors

nmod

of

case

the

det

determined

acl:relcl

that

nsubj

and

cc

shaped

conj

most

advmod

innovation

dobj

the

det

the

det

technological

amod

Figure 9: The dependency graphs of the sentence in Table 2 and Figure 8 after each error is introduced.

10



Conference of the European Chapter of the Associ-
ation for Computational Linguistics, pages 116–126,
Gothenburg, Sweden, April. Association for Compu-
tational Linguistics.

Jennifer Foster, Joachim Wagner, and Josef Van Gen-
abith. 2008. Adapting a WSJ-trained parser to gram-
matically noisy text. In Proceedings of the 46th An-
nual Meeting of the Association for Computational
Linguistics on Human Language Technologies: Short
Papers, pages 221–224. Association for Computa-
tional Linguistics.

Jennifer Foster, Özlem Çetinolu, Joachim Wagner, and
Josef van Genabith. 2011. Comparing the use of
edited and unedited text in parser self-training. In Pro-
ceedings of the 12th International Conference on Pars-
ing Technologies, pages 215–219, Dublin, Ireland, Oc-
tober. Association for Computational Linguistics.

Jennifer Foster. 2004. Parsing ungrammatical input: an
evaluation procedure. In Proceedings of Language Re-
sources and Evaluation Conference (LREC).

Jennifer Foster. 2007. Treebanks gone bad. Interna-
tional Journal of Document Analysis and Recognition
(IJDAR), 10(3-4):129–145.

Jennifer Foster. 2010. “cba to check the spelling”: In-
vestigating parser performance on discussion forum
posts. In Human Language Technologies: The 2010
Annual Conference of the North American Chapter of
the Association for Computational Linguistics, pages
381–384, Los Angeles, California, June. Association
for Computational Linguistics.

Jeroen Geertzen, Theodora Alexopoulou, and Anna Ko-
rhonen. 2013. Automatic linguistic annotation of
large scale L2 databases: The EF-Cambridge Open
Language Database (EFCAMDAT). In Proceed-
ings of the 31st Second Language Research Forum.
Somerville, MA: Cascadilla Proceedings Project.

Rasoul Kaljahi, Jennifer Foster, Johann Roturier,
Corentin Ribeyre, Teresa Lynn, and Joseph Le Roux.
2015. Foreebank: Syntactic analysis of customer sup-
port forums. In Conference on Empirical Methods in
Natural Language Processing (EMNLP).

John Lee and Stephanie Seneff. 2008. Correcting mis-
use of verb forms. In Proceedings of ACL-08: HLT,
pages 174–182, Columbus, Ohio, June. Association
for Computational Linguistics.

Diane Nicholls. 2004. The Cambridge Learner Corpus:
Error coding and analysis for lexicography and ELT.
In Proceedings of the Corpus Linguistics 2003 Con-
ference, pages 572–581.

Joakim Nivre, Johan Hall, and Jens Nilsson. 2004.
Memory-based dependency parsing. In HLT-NAACL
2004 Workshop: Eighth Conference on Computational
Natural Language Learning (CoNLL-2004), pages

49–56, Boston, Massachusetts, USA, May 6–May 7.
Association for Computational Linguistics.

Ines Rehbein and Josef van Genabith. 2007. Eval-
uating evaluation measures. In Proceedings of the
16th Nordic Conference of Computational Linguistics
(NODALIDA), pages 372–379.

Brian Roark, Mary Harper, Eugene Charniak, Bonnie
Dorr, Mark Johnson, Jeremy G Kahn, Yang Liu, Mari
Ostendorf, John Hale, Anna Krasnyanskaya, et al.
2006. SParseval: Evaluation metrics for parsing
speech. In Proceedings of Language Resources and
Evaluation Conference (LREC).

Alla Rozovskaya and Dan Roth. 2010. Annotating ESL
errors: Challenges and rewards. In Proceedings of the
NAACL HLT 2010 Fifth Workshop on Innovative Use
of NLP for Building Educational Applications, pages
28–36. Association for Computational Linguistics.

Joel Tetreault, Jennifer Foster, and Martin Chodorow.
2010. Using parse features for preposition selection
and error detection. In Proceedings of the 48th Annual
Meeting of the Association of Computational Linguis-
tics on Human Language Technologies: Short Papers,
pages 353–358, Uppsala, Sweden, July. Association
for Computational Linguistics.

Reut Tsarfaty, Joakim Nivre, and Evelina Andersson.
2011. Evaluating dependency parsing: Robust and
heuristics-free cross-annotation evaluation. In Pro-
ceedings of the 2011 Conference on Empirical Meth-
ods in Natural Language Processing, pages 385–396,
Edinburgh, Scotland, UK., July. Association for Com-
putational Linguistics.

Joachim Wagner and Jennifer Foster. 2009. The effect of
correcting grammatical errors on parse probabilities.
In Proceedings of the 11th International Conference
on Parsing Technologies, pages 176–179. Association
for Computational Linguistics.

Helen Yannakoudakis, Ted Briscoe, and Ben Medlock.
2011. A new dataset and method for automatically
grading esol texts. In Proceedings of the 49th Annual
Meeting of the Association for Computational Linguis-
tics: Human Language Technologies-Volume 1, pages
180–189. Association for Computational Linguistics.

Yue Zhang and Stephen Clark. 2011. Syntactic process-
ing using the generalized perceptron and beam search.
Computational Linguistics, 37(1):105–151.

11


