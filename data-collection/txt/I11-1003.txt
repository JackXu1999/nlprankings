















































Learning Logical Structures of Paragraphs in Legal Articles


Proceedings of the 5th International Joint Conference on Natural Language Processing, pages 20–28,
Chiang Mai, Thailand, November 8 – 13, 2011. c©2011 AFNLP

Learning Logical Structures of Paragraphs in Legal Articles

Ngo Xuan Bach, Nguyen Le Minh, Tran Thi Oanh, Akira Shimazu
School of Information Science,

Japan Advanced Institute of Science and Technology,
1-1 Asahidai, Nomi, Ishikawa, 923-1292, Japan

{bachnx,nguyenml,oanhtt,shimazu}@jaist.ac.jp

Abstract

This paper presents a new task, learning
logical structures of paragraphs in legal ar-
ticles, which is studied in research on Le-
gal Engineering (Katayama, 2007). The
goals of this task are recognizing logi-
cal parts of law sentences in a paragraph,
and then grouping related logical parts
into some logical structures of formulas,
which describe logical relations between
logical parts. We present a two-phase
framework to learn logical structures of
paragraphs in legal articles. In the first
phase, we model the problem of recog-
nizing logical parts in law sentences as
a multi-layer sequence learning problem,
and present a CRF-based model to recog-
nize them. In the second phase, we pro-
pose a graph-based method to group logi-
cal parts into logical structures. We con-
sider the problem of finding a subset of
complete sub-graphs in a weighted-edge
complete graph, where each node corre-
sponds to a logical part, and a complete
sub-graph corresponds to a logical struc-
ture. We also present an integer linear pro-
gramming formulation for this optimiza-
tion problem. Our models achieve 74.37%
in recognizing logical parts, 79.59% in
recognizing logical structures, and 55.73%
in the whole task on the Japanese National
Pension Law corpus.

1 Introduction

Legal Engineering (Katayama, 2007) is a new re-
search field which aims to achieve a trustworthy
electronic society. Legal Engineering regards that
laws are a kind of software for our society. Specif-
ically, laws such as pension law are specifications
for information systems such as pension systems.

To achieve a trustworthy society, laws need to be
verified about their consistency and contradiction.

Legal texts have some specific characteristics
that make them different from other kinds of doc-
uments. One of the most important characteristics
is that legal texts usually have some specific struc-
tures at both sentence and paragraph levels. At the
sentence level, a law sentence can roughly be di-
vided into two logical parts: requisite part and ef-
fectuation part (Bach, 2011a; Bach et al., 2011b;
Tanaka eta al., 1993). At the paragraph level, a
paragraph usually contains a main sentence1 and
one or more subordinate sentences (Takano et al.,
2010).

Analyzing logical structures of legal texts is an
important task in Legal Engineering. The outputs
of this task will be beneficial to people in under-
standing legal texts. They can easily understand
1) what does a law sentence say? 2) what cases
in which the law sentence can be applied? and
3) what subjects are related to the provision de-
scribed in the law sentence? This task is the pre-
liminary step, which supports other tasks in legal
text processing (translating legal articles into log-
ical and formal representations, legal text summa-
rization, legal text translation, question answering
in legal domains, etc) and serves legal text verifi-
cation, an important goal of Legal Engineering.

There have been some studies analyzing logi-
cal structures of legal texts. (Bach et al., 2011b)
presents the RRE task2, which recognizes the log-
ical structure of law sentences. (Bach et al.,
2010) describes an investigation on contributions
of words to the RRE task. (Kimura et al., 2009)
focuses on dealing with legal sentences includ-
ing itemized and referential expressions. These
works, however, only analyze logical structures of
legal texts at the sentence level. At the paragraph

1Usually, the first sentence is the main sentence.
2The task of Recognition of Requisite part and Effectua-

tion part in law sentences.

20



level, (Takano et al., 2010) classifies a legal para-
graph into one of six predefined categories: A, B,
C, D, E, and F . Among six types, Type A, B, and
C correspond to cases in which the main sentence
is the first sentence, and subordinate sentences are
other sentences. In paragraphs of Type D, E, and
F , the main sentence is the first or the second sen-
tence, and a subordinate sentence is an embedded
sentence in parentheses within the main sentence.

In this paper, we present a task of learning log-
ical structures of legal articles at the paragraph
level. We propose a two-phase framework to com-
plete the task. We also describe experimental re-
sults on real legal data.

Our main contributions can be summarized in
the following points:

• Introducing a new task to legal text pro-
cessing, learning logical structures of para-
graphs in legal articles.

• Presenting an annotated corpus for the task,
the Japanese National Pension Law corpus.

• Proposing a two-phase framework and pro-
viding solutions to solve the task.

• Evaluating our framework on the real anno-
tated corpus.

The rest of this paper is organized as follows.
Section 2 describes our task and its two sub-tasks:
recognition of logical parts and recognition of log-
ical structures. In Section 3, we present our frame-
work and proposed solutions. Experimental re-
sults on real legal articles are described in Section
4. Finally, Section 5 gives some conclusions.

2 Formulation

Learning logical structures of paragraphs in legal
articles is the task of recognition of logical struc-
tures between logical parts in law sentences. A
logical structure is usually formed from a pair of
a requisite part and an effectuation part. These
two parts are built from other kinds of logical
parts such as topic parts, antecedent parts, con-
sequent parts, and so on (Bach, 2011a; Bach et
al., 2011b)3. Usually, consequent parts describes a
law provision, antecedent parts describes cases in
which the law provision can be applied, and topic

3We only recognize logical structures (a set of related log-
ical parts). The task of translating legal articles into logical
and formal representations is not covered in this paper.

Figure 1: Two cases of inputs and outputs of the
task.

Figure 2: An example in natural language (E
means Effectuation part, R means Requisite part,
and LS means Logical Structure).

parts describe subjects which are related to the law
provision. In this paper, a logical structure can be
defined as a set of some related logical parts.

Figure 1 shows two cases of the inputs and out-
puts of the task. In the first case, the input is a
paragraph of two sentences, and the outputs are
four logical parts, which are grouped into two log-
ical structures. In the second case, the input is
a paragraph consisting of four sentences, and the
outputs are four logical parts, which are grouped
into three logical structures. An example in natu-
ral language4 is presented in Figure 2.

2.1 Sub-Task 1: Recognition of Logical Parts

Let s be a law sentence in the law sentence space
S, then s can be represented by a sequence of
words s = [w1w2 . . . wn]. A legal paragraph x in
the legal paragraph space X is a sequence of law
sentences x = [s1s2 . . . sl], where si ∈ S, ∀i =
1, 2, . . . , l. For each paragraph x, we denote a log-

4Because law sentences are very long and complicated,
we use toy sentences to illustrate the task.

21



ical part p by a quad-tuple p = (b, e, k, c) where
b, e, and k are three integers which indicate po-
sition of the beginning word, position of the end
word, and sentence position of p, and c is a logical
part category in the set of predefined categories C.
Formally, the set P of all possible logical parts de-
fined in a paragraph x can be described as follows:
P = {(b, e, k, c)|1 ≤ k ≤ l, 1 ≤ b ≤ e ≤

len(k), c ∈ C}.
In the above definition, l is the number of sen-

tences in the paragraph x, and len(k) is the length
of the kth sentence.

In this sub-task, we want to recognize some
non-overlapping (but possibly embedded) logical
parts in an input paragraph. A solution for this
task is a subset y ⊆ P which does not violate the
overlapping relationship. We say that two logical
parts p1 and p2 are overlapping if and only if they
are in the same sentence (k1 = k2) and b1 < b2 ≤
e1 < e2 or b2 < b1 ≤ e2 < e1. We denote the
overlapping relationship by ∼. We also say that
p1 is embedded in p2 if and only if they are in the
same sentence (k1 = k2) and b2 ≤ b1 ≤ e1 ≤ e2,
and denote the embedded relationship by ≺. For-
mally, the solution space can be described as fol-
lows: Y = {y ⊆ P |∀u, v ∈ y, u 6∼ v}. The learn-
ing problem in this sub-task is to learn a function
R : X → Y from a set of m training samples
{(xi, yi)|xi ∈ X, yi ∈ Y, ∀i = 1, 2, . . . ,m}.

In our task, we consider the following types of
logical parts:

1. An antecedent part is denoted by A
2. A consequent part is denoted by C
3. A topic part which depends on the antecedent

part is denoted by T1
4. A topic part which depends on the conse-

quent part is denoted by T2
5. A topic part which depends on both the an-

tecedent part and the consequent part is de-
noted by T3

6. The left part of an equivalent statement is de-
noted by EL

7. The right part of an equivalent statement is
denoted by ER

8. An object part, whose meaning is defined dif-
ferently in different cases, is denoted by Ob

9. An original replacement part, which will be
replaced by other replacement parts (denoted
by RepR) in specific cases, is denoted by
RepO.

Compared with previous works (Bach et al.,

2011b), we introduce three new kinds of logical
parts: Ob, RepO, and RepR.

2.2 Sub-Task 2: Recognition of Logical
Structures

In the second sub-task, the goal is to recognize a
set of logical structures given a set of logical parts.

Let G =< V,E > be a complete undirected
graph with the vertex set V and the edge set E. A
real value function f is defined on E as follows:
f : E → R, e ∈ E 7→ f(e) ∈ R.
In this sub-task, each vertex of the graph corre-

sponds to a logical part, and a complete sub-graph
corresponds to a logical structure. The value on an
edge connecting two vertices expresses the degree
that the two vertices belong to one logical struc-
ture. The positive (negative) value means that two
vertices are likely (not likely) to belong to one log-
ical structure.

Let Gs be a complete sub-graph of G, then
v(Gs) and e(Gs) are the set of vertices and the set
of edges of Gs, respectively. We define the total
value of a sub-graph as follows:
f(Gs) = f(e(Gs)) =

∑
e∈e(Gs) f(e).

Let Ω be the set of all complete sub-graphs of G.
The problem becomes determining a subset Ψ ⊆
Ω that satisfies the following constraints:

1. ∀g ∈ Ψ, |v(g)| ≥ 2,

2. ∪g∈Ψv(g) = V ,

3. ∀g1, g2 ∈ Ψ|v(g1) ⊆ v(g2) ⇒ v(g1) =
v(g2),

4. ∀g ∈ Ψ,∪h∈Ψ,h6=gv(h) 6= V , and

5.
∑

g∈Ψ f(g)→ maximize.

Constraint 1), minimal constraint, says that
each logical structure must contain at least two
logical parts. There is the case that a logical struc-
ture contains only a consequent part. Due to the
characteristics of Japanese law sentences, how-
ever, our corpus does not contain such cases. A
logical structure which contains a consequent part
will also contain a topic part or an antecedent part
or both of them. So a logical structure contains
at least two logical parts. Constraint 2), complete
constraint, says that each logical part must belong
to at least one logical structure. Constraint 3),
maximal constraint, says that we cannot have two
different logical structures such that the set of log-
ical parts in one logical structure contains the set

22



of logical parts in the other logical structure. Con-
straint 4), significant constraint, says that if we re-
move any logical structure from the solution, Con-
straint 2) will be violated. Although Constraint 3)
is guaranteed by Constraint 4), we introduce it be-
cause of its importance.

3 Proposed Solutions

3.1 Multi-layer Sequence Learning for
Logical Part Recognition

This sub-section presents our model for recogniz-
ing logical parts. We consider the recognition
problem as a multi-layer sequence learning prob-
lem. First, we give some related notions.

Let s be a law sentence, and P be the set of log-
ical parts of s, P = {p1, p2, . . . , pm}. Layer1(s)
(outer most layer) is defined as a set of logical
parts in P , which are not embedded in any other
part. Layeri(s) is defined as a set of logical parts
in P\ ∪i−1k=1 Layerk(s), which are not embedded
in any other part in P\∪i−1k=1Layerk(s). Formally,
we have:
Layer1(s) = {p|p ∈ P, p 6≺ q,∀q ∈ P, q 6=

p}.
Layeri(s) = {p|p ∈ Qi, p 6≺ q,∀q ∈ Qi, q 6=

p}, where
Qi = P\ ∪i−1k=1 Layerk(s)
Figure 3 illustrates a law sentence with four log-

ical parts in three layers: Part 1 and Part 2 in
Layer1, Part 3 in Layer2, and Part 4 in Layer3.

Figure 3: A law sentence with logical parts in three
layers.

Figure 4: An example of labeling in the multi-
layer model.

Let K be the number of layers in a law sentence
s, our model will recognize logical parts in K
steps. In the kth step we recognize logical parts in
Layerk. In each layer, we model the recognition

problem as a sequence labeling task in which each
word is an element. Logical parts in Layeri−1

will be used as input sequence in the ith step (in
the first step, we use original sentence as input).

Figure 4 gives an example of labeling for an in-
put sentence. The sentence consists of three logi-
cal parts in two layers. In our model, we use IOE
tag setting: the last element of a part is tagged with
E, the other elements of a part are tagged with I ,
and an element not included in any part is tagged
with O.

Let K∗ be the maximum number of layers in
all law sentences in training data. We learn K∗

models, in which the kth model is learned from
logical parts in the Layerk of training data, using
Conditional random fields (Lafferty et al., 2001;
Kudo, CRF toolkit). In the testing phase, we first
apply the first model to the input law sentence, and
then apply the ith model to the predicted logical
parts in Layeri−1.

3.2 ILP for Recognizing Logical Structures
Suppose that G′ is a sub-graph of G such that G′

contains all the vertices of G and the degree of
each vertex in G′ is greater than zero, then the
set of all the maximal complete sub-graphs (or
cliques) of G′ will satisfy all the minimal, com-
plete, maximal, and significant constraints. We
also note that, a set of cliques that satisfies all these
four constraints will form a sub-graph that has two
properties like properties of G′.

Let Λ be the set of all such sub-graphs G′ of G,
the sub-task now consists of two steps:

1. Finding G′ = argmaxG′∈Λf(G′), and

2. Finding all cliques of G′.

Each clique found in the second step will corre-
spond to a logical structure.

Recently, some researches have shown that in-
teger linear programming (ILP) formulations is an
effective way to solve many NLP problems such as
semantic role labeling (Punyakanok, 2004), coref-
erence resolution (Denis and Baldridge, 2007),
summarization (Clarke and Lapata, 2008), de-
pendency parsing (Martins et al., 2009), and so
on. The advantage of ILP formulations is that we
can incorporate non-local features or global con-
straints easily, which are difficult in traditional al-
gorithms. Although solving an ILP is NP-hard in
general, some fast algorithms and available tools5

5We used lp-solve from http://lpsolve.sourceforge.net/

23



make it a practical solution for many NLP prob-
lems (Martins et al., 2009).

In this work, we exploit ILP to solve the first
step. Let N be the number of vertices of G, we
introduce a set of integer variables {xij}1≤i<j≤N .
The values of {xij} are set as follows. If (i, j) ∈
e(G′) then xij = 1, otherwise xij = 0. ILP for-
mulations for the first step can be described as fol-
lows:

//- - - - - - - - Objective function - - - - - - - -//

Maximize :
∑

1≤i<j≤N
f(i, j) ∗ xij (1)

//- - - - - - - - - - Constraints - - - - - - - - - -//

Integer : {xij}1≤i<j≤N . (2)
0 ≤ xij ≤ 1, (1 ≤ i < j ≤ N). (3)

j−1∑

i=1

xij +
N∑

k=j+1

xjk ≥ 1, (1 ≤ j ≤ N). (4)

The last constraint guarantees that there is at
least one edge connecting to each vertex in G′.

The second step, finding all cliques of an undi-
rected graph, is a famous problem in graph the-
ory. Many algorithms have been proposed to solve
this problem efficiently. In this work, we exploit
the Bron-Kerbosch algorithm, a backtracking al-
gorithm. The main idea of the Bron-Kerbosch al-
gorithm is using a branch-and-bound technique to
stop searching on branches that cannot lead to a
clique (Bron and Kerbosch, 1973).

The remaining problem is how to define the
value function f . Our solution is that, first we
learn a binary classifier C using maximum entropy
model. This classifier takes a pair of logical parts
as the input, and outputs +1 if two logical parts
belong to one logical structure, otherwise it will
output −1. Then, we define the value function f
for two logical parts as follows:
f(p1, p2) = Prob(C(p1, p2) = +1)− 0.5.
Function f will receive a value from −0.5 to

+0.5, and it equals to zero in the case that the clas-
sifier assigns the same probability to +1 and −1.

4 Experiments

4.1 Corpus
We have built a corpus, Japanese National Pension
Law (JNPL) corpus, which consists of 83 legal ar-
ticles6 of Japanese national pension law. The ar-
chitecture of JNPL is shown in Figure 5. The law

6Because building corpus is an expensive and time-
consuming task, we only annotate a part of JNPL.

Figure 5: The architecture of JNPL.

consists of articles, articles consist of paragraphs,
and paragraphs contain sentences. A sentence may
belong to items, sub-items, or sub-sub-items of a
paragraph.

Figure 6 illustrates the relationship between a
law sentence and logical parts. A law sentence
may contain some logical parts, and a logical part
may be embedded in another one.

Figure 6: Relationship between a sentence and
logical parts.

In our corpus, a logical part is annotated with
information about its type (kind of part) and
formula-id (logical parts with the same id will be-

24



Figure 7: An annotated sentence in the JNPL cor-
pus. The sentence contains two logical structures
with four logical parts.

long to one logical structure). An example of an-
notated sentence in the JNPL corpus is shown in
Figure 7.

We employed two people in a data-making com-
pany, who analyzed and annotated our corpus. The
corpus consists of 83 legal articles, which contain
119 paragraphs with 426 sentences. On average,
each paragraph consists of 3.6 sentences. The to-
tal number of logical parts is 807, and the num-
ber of logical structures is 351. On average, each
paragraph consists of 6.8 logical parts and 3 logi-
cal structures.

Table 1 shows some statistics on the number of
logical parts of each type. Main types of parts are
A(35.4%), C(30.7%), T2(14.1%), ER(7.1%), and
EL(6.8%). Five main types of parts make up more
than 94% of all types.

4.2 Evaluation Methods

We divided the JNLP corpus into 10 sets, and con-
ducted 10-fold cross-validation tests. For the first
sub-task, we evaluated the performance of our sys-
tem by precision, recall, and F1 scores as follows:

precision = |correct parts||predicted parts| , recall =
|correct parts|
|actual parts| ,

F1 =
2∗precision∗recall
precision+recall

.

For the second sub-task, we used MUC preci-
sion, recall, and F1 scores as described in (Vilain
et al., 1995). We summarize them here for clarity.

Let P1, P2, . . . , Pn be n predicted logical struc-
tures, and G1, G2, . . . , Gm be the correct an-
swers or gold logical structures. To calculate
recall, for each gold logical structure Gi(i =
1, 2, . . . ,m), let k(Gi) be the smallest number
such that there exist k(Gi) predicted structures
P i1, P

i
2, . . . , P

i
k(Gi)

which satisfy Gi ⊆ ∪k(Gi)j=1 P ij :

recall =
∑m

i=1 (|Gi|−k(Gi))∑m
i=1 (|Gi|−1)

.
To calculate precision, we switch the roles of

predicted structures and gold structures. Finally,
F1 score is computed in a similar manner as in the
first sub-task.

4.3 Experiments on Sub-Task 1

4.3.1 Baseline: Filter-Ranking Perceptron
Algorithm

We chose the Filter-Ranking (FR) Perceptron al-
gorithm proposed by (Carreras and Marquez,
2005; Carreras et al., 2002) as our baseline model
because of its effectiveness on phrase recognition
problems, especially on problems that accept the
embedded relationship7. We use FR-perceptron
algorithm to recognize logical parts in law sen-
tences one by one in an input paragraph.

For beginning/end predictors, we got features of
words, POS tags, and Bunsetsu8 tags in a window
size 2. Moreover, with beginning predictor, we
used a feature for checking whether this position
is the beginning of the sentence or not. Similarly,
with end predictor, we use a feature for checking
whether this position is the end of the sentence or
not.

With each logical part candidate, we extract fol-
lowing kinds of features:

1. Length of the part

2. Internal structure: this feature is the concate-
nation of the top logical parts, punctuation
marks, parenthesis, and quotes inside the can-
didate. An example about internal structure
may be (A+,+C + .) (plus is used to con-
catenate items)

3. Word (POS) uni-gram, word (POS) bi-gram,
and word (POS) tri-gram.

4.3.2 Experimental Results
In our experiments, we focus on paragraphs in
Type A, B, and C defined in (Takano et al., 2010).
In these types, the first sentence is the main sen-
tence, which usually contains more logical parts
than other sentences. The other sentences often
have a few logical parts, and in most cases these
logical parts only appear in one layer. The first

7We re-implement the FR-perceptron algorithm by our-
self.

8In Japanese, a Bunsetsu is an unit of sentence which is
similar to a chunk in English.

25



Table 1: Statistics on logical parts of the JNPL corpus
Logical Part C A T1 T2 T3 EL ER Ob RepO RepR

Number 248 286 0 114 12 55 57 9 12 14

Table 2: Experimental results for Sub-task 1 on the
JNLP corpus(W:Word; P: POS tag; B: Bunsetsu
tag)

Model Prec(%) Recall(%) F1(%)
Baseline 79.70 52.54 63.33

W 79.18 69.27 73.89
W+P 77.62 68.77 72.93
W+B 79.63 69.76 74.37

W+P+B 77.89 69.39 73.39

sentences usually contain logical parts in two lay-
ers.

We divided sentences into two groups. The first
group consists of the first sentences in paragraphs,
and the second group consists of other sentences.
We set the number of layers k to 2 for sentences
in the first group, and to 1 for sentences in the
second group. To learn sequence labeling mod-
els, we used CRFs (Lafferty et al., 2001; Kudo,
CRF toolkit).

Experimental results on the JNPL corpus are
described in Table 2. We conducted experiments
with four feature sets: words; words and POS tags;
words and Bunsetsu tags; and words, POS tags,
and Bunsetsu tags. To extract features from source
sentences, we used the CaboCha tool (Kudo,
Cabocha), a Japanese morphological and syntac-
tic analyzer. The best model (word and Bunsetsu
tag features) achieved 74.37% in F1 score. It im-
proves 11.04% in F1 score (30.11% in error rate)
compared with the baseline model.

Table 3 shows experimental results of our best
model in more detail. Our model got good results
on most main parts: C(78.98%), A(80.42%), and
T2(82.14%). The model got low results on the
other types of parts. It is understandable because
three types of logical parts C, A, and T2 make up
more than 80%, while six other types only make
up 20% of all types.

4.4 Experiments on Sub-Task 2

4.4.1 Baseline: a Heuristic Algorithm
Our baseline is a heuristic algorithm to solve this
sub-task on graphs. This is an approximate algo-
rithm which satisfies minimal, complete, maximal,

Table 3: Experimental results in more details
Logical Part Prec(%) Recall(%) F1(%)

C 83.41 75.00 78.98
EL 76.74 60.00 67.35
ER 41.94 22.81 29.55
Ob 0.00 0.00 0.00
A 80.42 80.42 80.42

RepO 100 16.67 28.57
RepR 100 28.57 44.44
T2 83.64 80.70 82.14
T3 60.00 25.00 35.29

Overall 79.63 69.76 74.37

and significant constraints. The main idea of our
algorithm is picking up as many positive edges as
possible, and as few negative edges as possible.
We consider two cases: 1) There is no positive
value edge on the input graph; and 2) There are
some positive value edges on the input graph.

In the first case, because all the edges have neg-
ative values, we build logical structures with as
few logical parts as possible. In this case, each
logical structure contains exactly two logical parts.
So we gradually choose two nodes in the graph
with the maximum value on the edge connecting
them. An example of the first case is illustrated in
Figure 8. The maximum value on an edge is−0.1,
so the first logical structure will contain node 1
and node 3. The second logical structure contains
node 2 and node 49.

Figure 8: An example of the first case.

In the second case, we first consider the sub-
graph which only contains non-negative value
edges. In this sub-graph, we repeatedly build log-
ical structures with as many logical parts as possi-

9If the number of nodes is odd, the final logical structure
will consist of the final node and another node, so that the
edge connecting them has the maximal value.

26



Figure 9: An example of the second case.

ble. After building successfully a logical structure,
we remove all the nodes and the edges according
to it on the graph. When have no positive edge,
we will build logical structures with exactly two
logical parts.

An example of the second case is illus-
trated in Figure 9. First, we consider the sub-
graph with positive edges. This sub-graph con-
sists of five nodes {1, 2, 3, 4, 5} and four edges
{(1, 2), (1, 3), (2, 3), (2, 4)}. First, we have a log-
ical structure with three nodes {1, 2, 3}. We re-
move these nodes and the positive edges connect-
ing to these nodes. We have two nodes {4, 5} with
no positive edges. Now we build logical struc-
tures with exactly two nodes. We consider node
4. Among edges connecting to node 4, the edge
(2, 4) has maximal value. So we have the sec-
ond logical structure with two nodes {2, 4}. Next,
we consider node 5, and we have the third logical
structure with two nodes {1, 5}.

4.4.2 Experimental Results
In our experiments, to learn a maximum entropy
binary classification we used the implementation
of Tsuruoka (Tsuruoka, MEM). With a pair of
logical parts, we extracted the following features
(and combinations of them):

• Categories of two parts.

• Layers of two parts.

• The positions of the sentences that contain
two parts (the first sentence or not).

• Categories of other parts in the input para-
graph.

We conducted experiments on this sub-task in
two settings. In the first setting, we used annotated

Table 4: Experiments on Sub-task 2
Gold Input Setting

Model Prec(%) Recall(%) F1(%)
Heuristic 81.24 71.19 75.89

ILP 76.56 82.87 79.59
End-to-End Setting

Model Prec(%) Recall(%) F1(%)
Heuristic 54.88 47.84 51.12

ILP 57.51 54.06 55.73

logical parts (gold inputs) as the inputs to the sys-
tem. The purpose of this experiment is to evalu-
ate the performance of the graph-based method on
Sub-task 2. In the second setting, predicted logi-
cal parts (end-to-end) outputted by the Sub-task 1
were used as the inputs to the system. The purpose
of this experiment is to evaluate the performance
of our framework on the whole task.

In the second setting, end-to-end setting, be-
cause input logical parts may differ from the cor-
rect logical parts, we need to modify the MUC
scores. Let P1, P2, . . . , Pn be n predicted log-
ical structures, and G1, G2, . . . , Gm be the gold
logical structures. For each gold logical struc-
ture Gi(i = 1, 2, . . . ,m), let Di be the set of
logical parts in Gi which are not included in
the set of input logical parts. Di = {p ∈
Gi|p /∈ ∪nj=1Pj}. Let k(Gi) be the small-
est number such that there exist k(Gi) predicted
structures P i1, P

i
2, . . . , P

i
k(Gi)

which satisfy Gi ⊆
(∪k(Gi)j=1 P ij ) ∪Di.

recall =
∑m

i=1 (|Gi|−|Di|−k(Gi))∑m
i=1 (|Gi|−1)

.
To calculate the precision, we switch the roles

of predicted structures and gold structures.
Table 4 shows experimental results on the sec-

ond sub-task. The ILP model outperformed the
baseline model in both settings. It improved
3.70% in the F1 score (15.35% in error rate) in
the gold-input setting, and 4.61% in the F1 score
(9.43% in error rate) in the end-to-end setting
compared with the baseline model (heuristic algo-
rithm).

5 Conclusion

We have introduced the task of learning logical
structures of paragraphs in legal articles, a new
task which has been studied in research on Le-
gal Engineering. We presented the Japanese Na-
tional Pension Law corpus, an annotated corpus of

27



real legal articles for the task. We also described
a two-phase framework with multi-layer sequence
learning model and ILP formulation to complete
the task. Our results provide a baseline for further
researches on this interesting task.

In the future, we will continue to improve this
task. On the other hand, we also investigate the
task of translating legal articles into logical and
formal representations.

Acknowledgments

This work was partly supported by the 21st Cen-
tury COE program ‘Verifiable and Evolvable e-
Society’, Grant-in-Aid for Scientific Research,
Education and Research Center for Trustworthy e-
Society, and JAIST Overseas Training Program for
3D Program Students.

We would like to give special thanks to Kenji
Takano and Yoshiko Oyama, who analyzed law
sentences and built the corpus, and the reviewers,
who gave us valuable comments.

References
N.X. Bach. 2011a. A Study on Recognition of

Requisite Part and Effectuation Part in Law Sen-
tences. Master Thesis, School of Information Sci-
ence, Japan Advanced Institute of Science and Tech-
nology.

N.X. Bach, N.L. Minh, A. Shimazu. 2011b. RRE
Task: The Task of Recognition of Requisite Part
and Effectuation Part in Law Sentences. In Inter-
national Journal of Computer Processing Of Lan-
guages (IJCPOL), Volume 23, Number 2.

N.X. Bach, N.L. Minh, A. Shimazu. 2010. Exploring
Contributions of Words to Recognition of Requisite
Part and Effectuation Part in Law Sentences. In Pro-
ceedings of JURISIN, pp. 121-132.

C. Bron and J. Kerbosch. 1973. Algorithm 457: Find-
ing All Cliques of an Undirected Graph. In Commu-
nications of the ACM, Volume 16, Issue 9, pp. 575-
577.

X. Carreras and L. Marquez. 2005. Filtering-Ranking
Perceptron Learning for Partial Parsing. In Machine
Learning, Volume 60, Issue 1-3, pp. 41-71.

X. Carreras, L. Marquez, V. Punyakanok, D. Roth.
2002. Learning and Inference for Clause Identifi-
cation. In Proceedings of ECML, pp. 35-47.

J. Clarke and M. Lapata. 2008. Global Inference for
Sentence Compression: An Integer Linear Program-
ming Approach. In Journal of Artificial Intelligence
Research (JAIR), Volume 31, pp. 399-429.

P. Denis and J. Baldridge. 2007. Joint Determination
of Anaphoricity and Coreference Resolution Us-
ing Integer Programming. In Proceedings of HLT-
NAACL, pp. 236-243.

T. Katayama. 2007. Legal Engineering - An Engineer-
ing Approach to Laws in e-Society Age. In Proceed-
ings of JURISIN.

Y. Kimura, M. Nakamura, A. Shimazu. Treatment
of Legal Sentences Including Itemized and Referen-
tial Expressions - Towards Translation into Logical
Forms. New Frontiers in Artificial Intelligence, vol-
ume 5447 of LNAI, pp.242-253.

T. Kudo. Yet Another Japanese Depen-
dency Structure Analyzer. http://chasen.org/
taku/software/cabocha/.

T. Kudo. CRF++: Yet Another CRF toolkit.
http://crfpp.sourceforge.net/.

J. Lafferty, A. McCallum, F. Pereira. 2001. Condi-
tional Random Fields: Probabilistic Models for Seg-
menting and Labeling Sequence Data. In Proceed-
ings of ICML, pp.282-289.

A.F.T. Martins, N.A. Smith, E.P. Xing. 2009. Concise
Integer Linear Programming Formulations for De-
pendency Parsing. In Proceedings of ACL, pp.342-
350.

M. Nakamura, S. Nobuoka, A. Shimazu. 2007. To-
wards Translation of Legal Sentences into Logical
Forms. In Proceedings of JURISIN.

V. Punyakanok, D. Roth, W. Yih, D. Zimak. 2004. Se-
mantic Role Labeling Via Integer Linear Program-
ming Inference. In Proceedings of COLING, pp.
1346-1352.

K. Takano, M. Nakamura, Y. Oyama, A. Shimazu.
2010. Semantic Analysis of Paragraphs Consisting
of Multiple Sentences - Towards Development of a
Logical Formulation System. In Proceedings of JU-
RIX, pp. 117-126.

K. Tanaka, I. Kawazoe, H. Narita. 1993 Stan-
dard Structure of Legal Provisions - for the Legal
Knowledge Processing by Natural Language - (in
Japanese). In IPSJ Research Report on Natural Lan-
guage Processing, pp. 79-86.

Y. Tsuruoka. A simple C++ library for maxi-
mum entropy classification. http://www-tsujii.is.s.u-
tokyo.ac.jp/ tsuruoka/maxent/.

M. Vilain, J. Burger, J. Aberdeen, D. Connolly, L.
Hirschman. 1995. A Model-Theoretic Coreference
Scoring Scheme. In Proceedings of MUC-6, pp. 45-
52.

28


