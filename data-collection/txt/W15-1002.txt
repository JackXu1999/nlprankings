



















































Non-projective Dependency-based Pre-Reordering with Recurrent Neural Network for Machine Translation


Proceedings of SSST-9, Ninth Workshop on Syntax, Semantics and Structure in Statistical Translation, pages 10–20,
Denver, Colorado, June 4, 2015. c©2015 Association for Computational Linguistics

Non-projective Dependency-based Pre-Reordering with Recurrent
Neural Network for Machine Translation

Antonio Valerio Miceli-Barone
Università di Pisa

Largo B. Pontecorvo, 3
56127 Pisa, Italy

miceli@di.unipi.it

Giuseppe Attardi
Università di Pisa

Largo B. Pontecorvo, 3
56127 Pisa, Italy

attardi@di.unipi.it

Abstract

The quality of statistical machine trans-
lation performed with phrase based ap-
proaches can be increased by permuting
the words in the source sentences in an
order which resembles that of the target
language. We propose a class of recur-
rent neural models which exploit source-
side dependency syntax features to re-
order the words into a target-like order.
We evaluate these models on the German-
to-English language pair, showing signif-
icant improvements over a phrase-based
Moses baseline, obtaining a quality simi-
lar or superior to that of hand-coded syn-
tactical reordering rules.

1 Introduction

Statistical machine translation is typically per-
formed using phrase-based systems (Koehn et
al., 2007). These systems can usually produce
accurate local reordering but they have difficul-
ties dealing with the long-distance reordering
that tends to occur between certain language
pairs (Birch et al., 2008).

The quality of phrase-based machine transla-
tion can be improved by reordering the words
in each sentence of source-side of the paral-
lel training corpus in a ”target-like” order and
then applying the same transformation as a
pre-processing step to input strings during ex-
ecution.
When the source-side sentences can be ac-
curately parsed, pre-reordering can be per-
formed using hand-coded rules. This approach

has been successfully applied to German-to-
English (Collins et al., 2005) and other lan-
guages. The main issue with it is that these
rules must be designed for each specific lan-
guage pair, which requires considerable lin-
guistic expertise.

Fully statistical approaches, on the other
hand, learn the reordering relation from word
alignments. Some of them learn reordering
rules on the constituency (Dyer and Resnik,
2010) (Khalilov and Fonollosa, 2011) or projec-
tive dependency (Genzel, 2010), (Lerner and
Petrov, 2013) parse trees of source sentences.
The permutations that these methods can learn
can be generally non-local (i.e. high distance)
on the sentences but local (parent-child or
sibling-sibling swaps) on the parse trees. More-
over, constituency or projective dependency
trees may not be the ideal way of represent-
ing the syntax of non-analytic languages, which
could be better described using non-projective
dependency trees (Bosco and Lombardo, 2004).
Other methods, based on recasting reorder-
ing as a combinatorial optimization problem
(Tromble and Eisner, 2009), (Visweswariah et
al., 2011), can learn to generate in principle ar-
bitrary permutations, but they can only make
use of minimal syntactic information (part-of-
speech tags) and therefore can’t exploit the po-
tentially valuable structural syntactic informa-
tion provided by a parser.

In this work we propose a class of reorder-
ing models which attempt to close this gap by
exploiting rich dependency syntax features and

10



at the same time being able to process non-
projective dependency parse trees and generate
permutations which may be non-local both on
the sentences and on the parse trees.
We represent these problems as sequence pre-
diction machine learning tasks, which we ad-
dress using recurrent neural networks.

We applied our model to reorder German
sentences into an English-like word order as
a pre-processing step for phrase-based ma-
chine translation, obtaining significant im-
provements over the unreordered baseline sys-
tem and quality comparable to the hand-coded
rules introduced by Collins et al. (2005).

2 Reordering as a walk on a
dependency tree

In order to describe the non-local reorder-
ing phenomena that can occur between lan-
guage pairs such as German-to-English, we
introduce a reordering framework similar to
(Miceli Barone and Attardi, 2013), based on a
graph walk of the dependency parse tree of the
source sentence. This framework doesn’t re-
strict the parse tree to be projective, and allows
the generation of arbitrary permutations.

Let f ≡ ( f1, f2, . . . , fL f ) be a source sentence,
annotated by a rooted dependency parse tree:
∀j ∈ 1, . . . , L f , hj ≡ PARENT(j)

We define a walker process that walks from
word to word across the edges of the parse tree,
and at each steps optionally emits the current
word, with the constraint that each word must
be eventually emitted exactly once.
Therefore, the final string of emitted words f ′
is a permutation of the original sentence f , and
any permutation can be generated by a suitable
walk on the parse tree.

2.1 Reordering automaton

We formalize the walker process as a non-
deterministic finite-state automaton.
The state v of the automaton is the tuple v ≡
(j, E, a) where j ∈ 1, . . . , L f is the current vertex
(word index), E is the set of emitted vertices, a
is the last action taken by the automaton.
The initial state is: v(0) ≡ (root f , {}, null)
where root f is the root vertex of the parse tree.

At each step t, the automaton chooses one of
the following actions:

• EMIT: emit the word f j at the current ver-
tex j. This action is enabled only if the cur-
rent vertex has not been already emitted:

j /∈ E
(j, E, a) EMIT→ (j, E ∪ {j}, EMIT)

(1)

• UP: move to the parent of the current ver-
tex. Enabled if there is a parent and we did
not just come down from it:

hj 6= null, a 6= DOWNj
(j, E, a) UP→ (hj, E, UPj)

(2)

• DOWNj′ : move to the child j′ of the cur-
rent vertex. Enabled if the subtree s(j′)
rooted at j′ contains vertices that have not
been already emitted and if we did not just
come up from it:

hj′ = j, a 6= UPj′ , ∃k ∈ s(j′) : k /∈ E
(j, E, a)

DOWNj′→ (j′, E, DOWNj′)
(3)

The execution continues until all the vertices
have been emitted.

We define the sequence of states of the walker
automaton during one run as an execution v̄ ∈
GEN( f ). An execution also uniquely specifies
the sequence of actions performed by the au-
tomation.

The preconditions make sure that all execu-
tion of the automaton always end generating
a permutation of the source sentence. Further-
more, no cycles are possible: progress is made
at every step, and it is not possible to enter in
an execution that later turns out to be invalid.
Every permutation of the source sentence can
be generated by some execution. In fact, each
permutation f ′ can be generated by exactly one
execution, which we denote as v̄( f ′).

We can split the execution v̄( f ′) into a se-
quence of L f emission fragments v̄j( f ′), each end-
ing with an EMIT action.
The first fragment has zero or more DOWN∗ ac-
tions followed by one EMIT action, while each

11



other fragment has a non-empty sequence of
UP and DOWN∗ actions (always zero or more
UPs followed by zero or more DOWNs) fol-
lowed by one EMIT action.

Finally, we define an action in an execution
as forced if it was the only action enabled at the
step where it occurred.

2.2 Application

Suppose we perform reordering using a typical
syntax-based system which processes source-
side projective dependency parse trees and is
limited to swaps between pair of vertices which
are either in a parent-child relation or in a sib-
ling relation. In such execution the UP actions
are always forced, since the ”walker” process
never leaves a subtree before all its vertices
have been emitted.

Suppose instead that we could perform re-
ordering according to an ”oracle”. The execu-
tions of our automaton corresponding to these
permutations will in general contain unforced
UP actions. We define these actions, and the
execution fragments that exhibit them, as non-
tree-local.

In practice we don’t have access to a re-
ordering ”oracle”, but for sentences pairs in
a parallel corpus we can compute heuristic
”pseudo-oracle” reference permutations of the
source sentences from word-alignments.

Following (Al-Onaizan and Papineni, 2006),
(Tromble and Eisner, 2009), (Visweswariah et
al., 2011), (Navratil et al., 2012), we gener-
ate word alignments in both the source-to-
target and the target-to-source directions us-
ing IBM model 4 as implemented in GIZA++
(Och et al., 1999) and then we combine them
into a symmetrical word alignment using the
”grow-diag-final-and” heuristic implemented
in Moses (Koehn et al., 2007).

Given the symmetric word-aligned corpus,
we assign to each source-side word an integer
index corresponding to the position of the left-
most target-side word it is aligned to (attach-
ing unaligned words to the following aligned
word) and finally we perform a stable sort of
source-side words according to this index.

2.3 Reordering example

Consider the segment of a German sentence
shown in fig. 1. The English-reordered segment
”die Währungsreserven anfangs lediglich di-
enen sollten zur Verteidigung” corresponds to
the English: ”the reserve assets were origi-
nally intended to provide protection”.

In order to compose this segment from the
original German, the reordering automaton de-
scribed in our framework must perform a com-
plex sequence of moves on the parse tree:

• Starting from ”sollten”, de-
scend to ”dienen”, descent to
”Währungsreserven” and finally
to ”die”. Emit it, then go up to
”Währungsreserven”, emit it and go up to
”dienen” and up again to ”sollten”. Note
that the last UP is unforced since ”dienen”
has not been emitted at that point and has
also unemitted children. This unforced
action indicates non-tree-local reordering.

• Go down to ”anfangs”. Note that the
in the parse tree edge crosses another
edge, indicating non-projectivity. Emit
”anfangs” and go up (forced) back to
”sollten”.

• Go down to ”dienen”, down to ”zur”,
down to ”lediglich” and emit it. Go
up (forced) to ”zur”, up (unforced) to
”dienen”, emit it, go up (unforced) to
”sollten”, emit it. Go down to ”dienen”,
down to ”zur” emit it, go down to
”Verteidigung” and emit it.

Correct reordering of this segment would be
difficult both for a phrase-based system (since
the words are further apart than both the typ-
ical maximum distortion distance and maxi-
mum phrase length) and for a syntax-based
system (due to the presence of non-projectivity
and non-tree-locality).

3 Recurrent Neural Network reordering
models

Given the reordering framework described
above, we could try to directly predict the ex-

12



Figure 1: Section of the dependency parse tree of a German sentence.

ecutions as Miceli Barone and Attardi (2013)
attempted with their version of the frame-
work. However, the executions of a given sen-
tence can have widely different lengths, which
could make incremental inexact decoding such
as beam search difficult due to the need to
prune over partial hypotheses that have differ-
ent numbers of emitted words.

Therefore, we decided to investigate a dif-
ferent class of models which have the property
that state transition happen only in correspon-
dence with word emission. This enables us to
leverage the technology of incremental language
models.

Using language models for reordering is not
something new (Feng et al., 2010), (Durrani
et al., 2011), (Bisazza and Federico, 2013), but
instead of using a more or less standard n-
gram language model, we are going to base our
model on recurrent neural network language mod-
els (Mikolov et al., 2010).

Neural networks allow easy incorporation of
multiple types of features and can be trained
more specifically on the types of sequences that
will occur during decoding, hence they can
avoid wasting model space to represent the
probabilities of non-permutations.

3.1 Base RNN-RM

Let f ≡ ( f1, f2, . . . , fL f ) be a source sentence.
We model the reordering system as a determin-
istic single hidden layer recurrent neural net-
work:

v(t) = τ(Θ(1) · x(t) + ΘREC · v(t− 1)) (4)
where x(t) ∈ Rn is a feature vector associated
to the t-th word in a permutation f ′, v(0) ≡

vinit, Θ(1) and ΘREC are parameters1 and τ(·) is
the hyperbolic tangent function.

If we know the first t − 1 words of the per-
mutation f ′ in order to compute the probability
distribution of the t-th word we do the follow-
ing:

• Iteratively compute the state v(t− 1) from
the feature vectors x(1), . . . , x(t− 1).
• For the all the indices of the words that

haven’t occurred in the permutation so far
j ∈ J(t) ≡ ([1, L f ]− īt−1:), compute a score
hj,t ≡ ho(v(t− 1), xo(j)), where xo(·) is the
feature vector of the candidate target word.

• Normalize the scores using the logistic
softmax function: P( Īt = j| f , īt−1:, t) =

exp(hj,t)
∑j′∈J(t) exp(hj′ ,t)

.

The scoring function ho(v(t − 1), xo(j)) ap-
plies a feed-forward hidden layer to the fea-
ture inputs xo(j), and then takes a weighed in-
ner product between the activation of this layer
and the state v(t − 1). The result is then lin-
early combined to an additional feature equal
to the logarithm of the remaining words in the
permutation (L f − t),2 and to a bias feature:

hj,t ≡< τ(Θ(o) · xo(j)), θ(2) � v(t− 1) >
+ θ(α) · log(L f − t) + θ(bias)

(5)

where hj,t ≡ ho(v(t− 1), xo(j)).
1we don’t use a bias feature since it is redundant when

the layer has input features encoded with the ”one-hot”
encoding

2since we are then passing this score to a softmax of
variable size (L f − t), this feature helps the model to keep
the score already approximately scaled.

13



We can compute the probability of an entire
permutation f ′ just by multiplying the proba-
bilities for each word: P( f ′| f ) = P( Ī = ī| f ) =
∏

L f
t=1 P( Īt = īt| f , t)

3.1.1 Training

Given a training set of pairs of sentences
and reference permutations, the training prob-
lem is defined as finding the set of parame-
ters θ ≡ (vinit, Θ(1), θ(2), ΘREC, Θ(o), θ(α), θ(bias))
which minimize the per-word empirical cross-
entropy of the model w.r.t. the reference per-
mutations in the training set. Gradients can
be efficiently computed using backpropagation
through time (BPTT).

In practice we used the following training ar-
chitecture:
Stochastic gradient descent, with each train-
ing pair ( f , f ′) considered as a single mini-
batch for updating purposes. Gradients com-
puted using the automatic differentiation facil-
ities of Theano (Bergstra et al., 2010) (which im-
plements a generalized BPTT). No truncation
is used. L2-regularization 3. Learning rates
dynamically adjusted per scalar parameter us-
ing the AdaDelta heuristic (Zeiler, 2012). Gradi-
ent clipping heuristic to prevent the ”exploding
gradient” problem (Graves, 2013). Early stop-
ping w.r.t. a validation set to prevent overfit-
ting. Uniform random initialization for param-
eters other than the recurrent parameter ma-
trix ΘREC. Random initialization with echo state
property for ΘREC, with contraction coefficient
σ = 0.99 (Jaeger, 2001), (Gallicchio and Micheli,
2011).

Training time complexity is O(L2f ) per sen-
tence, which could be reduced to O(L f ) using
truncated BTTP at the expense of update accu-
racy and hence convergence speed. Space com-
plexity is O(L f ) per sentence.

3.1.2 Decoding

In order to use the RNN-RM model for pre-
reordering we need to compute the most likely

3λ = 10−4 on the recurrent matrix, λ = 10−6 on the
final layer, per minibatch.

permutation
∗
f ′ of the source sentence f :

∗
f ′ ≡ argmax

f ′∈GEN( f )
P( f ′| f ) (6)

Solving this problem to the global optimum is
computationally hard4, hence we solve it to a
local optimum using a beam search strategy.

We generate the permutation incrementally
from left to right. Starting from an initial state
consisting of an empty string and the initial
state vector vinit, at each step we generate all
possible successor states and retain the B-most
probable of them (histogram pruning), accord-
ing to the probability of the entire prefix of per-
mutation they represent.

Since RNN state vectors do not decompose in
a meaningful way, we don’t use any hypothesis
recombination.
At step t there are L f − t possible successor
states, and the process always takes exactly L f
steps5, therefore time complexity is O(B · L2f )
and space complexity is O(B).

3.1.3 Features

We use two different feature configurations:
unlexicalized and lexicalized.

In the unlexicalized configuration, the state
transition input feature function x(j) is com-
posed by the following features, all encoded us-
ing the ”one-hot” encoding scheme:

• Unigram: POS(j), DEPREL(j), POS(j) ∗
DEPREL(j). Left, right and parent un-
igram: POS(k), DEPREL(k), POS(k) ∗
DEPREL(k), where k is the index of re-
spectively the word at the left (in the orig-
inal sentence), at the right and the depen-
dency parent of word j. Unique tags are
used for padding.

• Pair features: POS(j) ∗ POS(k), POS(j) ∗
DEPREL(k), DEPREL(j) ∗ POS(k),
DEPREL(j) ∗ DEPREL(k), for k defined
as above.

4NP-hard for at least certain choices of features and pa-
rameters

5actually, L f − 1, since the last choice is forced

14



• Triple features POS(j) ∗ POS(le f tj) ∗
POS(rightj), POS(j) ∗ POS(le f tj) ∗
POS(parentj), POS(j) ∗ POS(rightj) ∗
POS(parentj).

• Bigram: POS(j) ∗ POS(k), POS(j) ∗
DEPREL(k), DEPREL(j) ∗ POS(k) where
k is the previous emitted word in the
permutation.

• Topological features: three binary features
which indicate whether word j and the
previously emitted word are in a parent-
child, child-parent or sibling-sibling rela-
tion, respectively.

The target word feature function xo(j) is the
same as x(j) except that each feature is also con-
joined with a quantized signed distance6 be-
tween word j and the previous emitted word.
Feature value combinations that appear less
than 100 times in the training set are replaced
by a distinguished ”rare” tag.

The lexicalized configuration is equivalent to
the unlexicalized one except that x(j) and xo(j)
also have the surface form of word j (not con-
joined with the signed distance).

3.2 Fragment RNN-RM

The Base RNN-RM described in the previous
section includes dependency information, but
not the full information of reordering frag-
ments as defined by our automaton model (sec.
2). In order to determine whether this rich
information is relevant to machine translation
pre-reordering, we propose an extension, de-
noted as Fragment RNN-RM, which includes re-
ordering fragment features, at expense of a sig-
nificant increase of time complexity.
We consider a hierarchical recurrent neural net-
work. At top level, this is defined as the previ-
ous RNN. However, the x(j) and xo(j) vectors,
in addition to the feature vectors described as
above now contain also the final states of an-
other recurrent neural network.
This internal RNN has a separate clock and a

6values greater than 5 and smaller than 10 are quan-
tized as 5, values greater or equal to 10 are quantized as
10. Negative values are treated similarly.

separate state vector. For each step t of the
top-level RNN which transitions between word
f ′(t− 1) and f ′(t), the internal RNN is reinitial-
ized to its own initial state and performs mul-
tiple internal steps, one for each action in the
fragment of the execution that the walker au-
tomaton must perform to walk between words
f ′(t − 1) and f ′(t) in the dependency parse
(with a special shortcut of length one if they are
adjacent in f with monotonic relative order).

The state transition of the inner RNN is de-
fined as:

vr(t) = τ(Θ(r1) · xr(tr)+ ΘrREC · vr(tr− 1))(7)
where xr(tr) is the feature function for the word
traversed at inner time tr in the execution frag-
ment. vr(0) = vinitr , Θ(r1) and ΘrREC are parame-
ters.
Evaluation and decoding are performed es-
sentially in the same was as in Base RNN-
RM, except that the time complexity is now
O(L3f ) since the length of execution fragments
is O(L f ).
Training is also essentially performed in the
same way, though gradient computation is
much more involved since gradients propagate
from the top-level RNN to the inner RNN. In
our implementation we just used the automatic
differentiation facilities of Theano.

3.2.1 Features

The unlexicalized features for the inner RNN
input vector xr(tr) depend on the current word
in the execution fragment (at index tr), the
previous one and the action label: UP, DOWN
or RIGHT (shortcut). EMIT actions are not
included as they always implicitly occur at the
end of each fragment.
Specifically the features, encoded with the
”one-hot” encoding are: A ∗ POS(tr) ∗
POS(tr − 1), A ∗ POS(tr) ∗ DEPREL(tr − 1),
A ∗ DEPREL(tr) ∗ POS(tr − 1), A ∗
DEPREL(tr) ∗ DEPREL(tr − 1).
These features are also conjoined with the
quantized signed distance (in the original
sentence) between each pair of words.
The lexicalized features just include the surface
form of each visited word at tr.

15



3.3 Base GRU-RM

We also propose a variant of the Base RNN-RM
where the standard recurrent hidden layer is re-
placed by a Gated Recurrent Unit layer, recently
proposed by Cho et al. (2014) for machine trans-
lation applications.
The Base GRU-RM is defined as the Base RNN-
RM of sec. 3.1, except that the recurrence rela-
tion 4 is replaced by fig. 2

Features are the same of unlexicalized Base
RNN-RM (we experienced difficulties training
the Base GRU-RM with lexicalized features).
Training is also performed in the same way ex-
cept that we found more beneficial to conver-
gence speed to optimize using Adam (Kingma
and Ba, 2014) 7 rather than AdaDelta.
In principle we could also extend the Fragment
RNN-RM into a Fragment GRU-RM, but we
did not investigate that model in this work.

4 Experiments

We performed German-to-English pre-
reordering experiments with Base RNN-RM
(both unlexicalized and lexicalized), Fragment
RNN-RM and Base GRU-RM .

4.1 Setup

The baseline phrase-based system was trained
on the German-to-English corpus included in
Europarl v7 (Koehn, 2005). We randomly split
it in a 1,881,531 sentence pairs training set, a
2,000 sentence pairs development set (used for
tuning) and a 2,000 sentence pairs test set. The
English language model was trained on the
English side of the parallel corpus augmented
with a corpus of sentences from AP News, for
a total of 22,891,001 sentences.
The baseline system is phrase-based Moses
in a default configuration with maximum
distortion distance equal to 6 and lexicalized
reordering enabled. Maximum phrase size is
equal to 7.
The language model is a 5-gram
IRSTLM/KenLM.
The pseudo-oracle system was trained on

7with learning rate 2 · 10−5 and all the other hyperpa-
rameters equal to the default values in the article.

the training and tuning corpus obtained by
permuting the German source side using
the heuristic described in section 2.2 and is
otherwise equal to the baseline system.
In addition to the test set extracted from Eu-
roparl, we also used a 2,525 sentence pairs test
set (”news2009”) a 3,000 sentence pairs ”chal-
lenge” set used for the WMT 2013 translation
task (”news2013”).

We also trained a Moses system with pre-
reordering performed by Collins et al. (2005)
rules, implemented by Howlett and Dras
(2011).
Constituency parsing for Collins et al. (2005)
rules was performed with the Berkeley parser
(Petrov et al., 2006), while non-projective de-
pendency parsing for our models was per-
formed with the DeSR transition-based parser
(Attardi, 2006).

For our experiments, we extract approxi-
mately 300,000 sentence pairs from the Moses
training set based on a heuristic confidence
measure of word-alignment quality (Huang,
2009), (Navratil et al., 2012). We randomly re-
moved 2,000 sentences from this filtered dataset
to form a validation set for early stopping, the
rest were used for training the pre-reordering
models.

4.2 Results

The hidden state size s of the RNNs was set
to 100 while it was set to 30 for the GRU
model, validation was performed every 2,000
training examples. After 50 consecutive vali-
dation rounds without improvement, training
was stopped and the set of training parame-
ters that resulted in the lowest validation cross-
entropy were saved.
Training took approximately 1.5 days for the
unlexicalized Base RNN-RM, 2.5 days for the
lexicalized Base RNN-RM and for the unlexi-
calized Base GRU-RM and 5 days for the unlex-
icalized Fragment RNN-RM on a 24-core ma-
chine without GPU (CPU load never rose to
more than 400%).

Decoding was performed with a beam size
of 4. Decoding the whole corpus took about
1.0-1.2 days for all the models except Fragment

16



vrst(t) = π(Θ
(1)
rst · x(t) + ΘRECrst · v(t− 1))

vupd(t) = π(Θ
(1)
upd · x(t) + ΘRECupd · v(t− 1))

vraw(t) = τ(Θ(1) · x(t) + ΘREC · v(t− 1)� vupd(t))
v(t) = vrst(t)� v(t− 1) + (1− vrst(t))� vraw(t)

(8)

Figure 2: GRU recurrence equations. vrst(t) and vupd(t) are the activation vectors of the ”reset”
and ”update” gates, respectively, and π(·) is the logistic sigmoid function.

.

Reordering BLEU improvement
none 62.10
unlex. Base RNN-RM 64.03 +1.93
lex. Base RNN-RM 63.99 +1.89
unlex. Fragment RNN-RM 64.43 +2.33
unlex. Base GRU-RM 64.78 +2.68

Figure 3: ”Monolingual” reordering scores (upstream system output vs. ”oracle”-permuted Ger-
man) on the Europarl test set. All improvements are significant at 1% level.

RNN-RM for which it took about 3 days.
Effects on monolingual reordering score are

shown in fig. 3, effects on translation quality
are shown in fig. 4.

4.3 Discussion and analysis

All our models significantly improve over the
phrase-based baseline, performing as well as or
almost as well as (Collins et al., 2005), which is
an interesting result since our models doesn’t
require any specific linguistic expertise.

Surprisingly, the lexicalized version of Base
RNN-RM performed worse than the unlexical-
ized one. This goes contrary to expectation as
neural language models are usually lexicalized
and in fact often use nothing but lexical fea-
tures.

The unlexicalized Fragment RNN-RM was
quite accurate but very expensive both during
training and decoding, thus it may not be prac-
tical.

The unlexicalized Base GRU-RM performed
very well, especially on the Europarl dataset
(where all the scores are much higher than the
other datasets) and it never performed signif-
icantly worse than the unlexicalized Fragment

RNN-RM which is much slower.
We also performed exploratory experiments

with different feature sets (such as lexical-only
features) but we couldn’t obtain a good train-
ing error. Larger network sizes should increase
model capacity and may possibly enable train-
ing on simpler feature sets.

5 Conclusions

We presented a class of statistical syntax-based
pre-reordering systems for machine transla-
tion.
Our systems processes source sentences parsed
with non-projective dependency parsers and
permutes them into a target-like word order,
suitable for translation by an appropriately
trained downstream phrase-based system.

The models we proposed are completely
trained with machine learning approaches and
is, in principle, capable of generating arbitrary
permutations, without the hard constraints
that are commonly present in other statistical
syntax-based pre-reordering methods.
Practical constraints depend on the choice of
features and are therefore quite flexible, allow-
ing a trade-off between accuracy and speed.

In our experiments with the RNN-RM and

17



Test set system BLEU improvement
Europarl baseline 33.00
Europarl ”oracle” 41.80 +8.80
Europarl Collins 33.52 +0.52
Europarl unlex. Base RNN-RM 33.41 +0.41
Europarl lex. Base RNN-RM 33.38 +0.38
Europarl unlex. Fragment RNN-RM 33.54 +0.54
Europarl unlex. Base GRU-RM 34.15 +1.15
news2013 baseline 18.80
news2013 Collins NA NA
news2013 unlex. Base RNN-RM 19.19 +0.39
news2013 lex. Base RNN-RM 19.01 +0.21
news2013 unlex. Fragment RNN-RM 19.27 +0.47
news2013 unlex. Base GRU-RM 19.28 +0.48
news2009 baseline 18.09
news2009 Collins 18.74 +0.65
news2009 unlex. Base RNN-RM 18.50 +0.41
news2009 lex. Base RNN-RM 18.44 +0.35
news2009 unlex. Fragment RNN-RM 18.60 +0.51
news2009 unlex. Base GRU-RM 18.58 +0.49

Figure 4: RNN-RM translation scores. All improvements are significant at 1% level.

GRU-RM models we managed to achieve trans-
lation quality improvements comparable to
those of the best hand-coded pre-reordering
rules.

References

Yaser Al-Onaizan and Kishore Papineni. 2006. Dis-
tortion models for statistical machine translation.
In Proceedings of the 21st International Conference
on Computational Linguistics and the 44th Annual
Meeting of the Association for Computational Lin-
guistics, ACL-44, pages 529–536, Stroudsburg, PA,
USA. Association for Computational Linguistics.

Giuseppe Attardi. 2006. Experiments with a multi-
language non-projective dependency parser. In
Proceedings of the Tenth Conference on Computa-
tional Natural Language Learning, CoNLL-X ’06,
pages 166–170, Stroudsburg, PA, USA. Associa-
tion for Computational Linguistics.

James Bergstra, Olivier Breuleux, Frédéric Bastien,
Pascal Lamblin, Razvan Pascanu, Guillaume Des-
jardins, Joseph Turian, David Warde-Farley, and
Yoshua Bengio. 2010. Theano: a CPU and GPU
math expression compiler. In Proceedings of the

Python for Scientific Computing Conference (SciPy),
June. Oral Presentation.

Alexandra Birch, Miles Osborne, and Philipp
Koehn. 2008. Predicting success in machine
translation. In Proceedings of the Conference on
Empirical Methods in Natural Language Processing,
EMNLP ’08, pages 745–754, Stroudsburg, PA,
USA. Association for Computational Linguistics.

Arianna Bisazza and Marcello Federico. 2013. Ef-
ficient solutions for word reordering in German-
English phrase-based statistical machine transla-
tion. In Proceedings of the Eighth Workshop on Sta-
tistical Machine Translation, pages 440–451, Sofia,
Bulgaria, August. Association for Computational
Linguistics.

Cristina Bosco and Vincenzo Lombardo. 2004. De-
pendency and relational structure in treebank an-
notation. In COLING 2004 Recent Advances in De-
pendency Grammar, pages 1–8.

Kyunghyun Cho, Bart van Merriënboer, Dzmitry
Bahdanau, and Yoshua Bengio. 2014. On
the properties of neural machine translation:
Encoder-decoder approaches. arXiv preprint
arXiv:1409.1259.

Michael Collins, Philipp Koehn, and Ivona
Kučerová. 2005. Clause restructuring for
statistical machine translation. In Proceedings of

18



the 43rd annual meeting on association for computa-
tional linguistics, pages 531–540. Association for
Computational Linguistics.

Nadir Durrani, Helmut Schmid, and Alexander
Fraser. 2011. A joint sequence translation model
with integrated reordering. In Proceedings of the
49th Annual Meeting of the Association for Compu-
tational Linguistics: Human Language Technologies-
Volume 1, pages 1045–1054. Association for Com-
putational Linguistics.

Chris Dyer and Philip Resnik. 2010. Context-
free reordering, finite-state translation. In Human
Language Technologies: The 2010 Annual Conference
of the North American Chapter of the Association for
Computational Linguistics, HLT ’10, pages 858–866,
Stroudsburg, PA, USA. Association for Computa-
tional Linguistics.

Minwei Feng, Arne Mauser, and Hermann Ney.
2010. A source-side decoding sequence model for
statistical machine translation. In Conference of the
Association for Machine Translation in the Americas
(AMTA).

C. Gallicchio and A. Micheli. 2011. Architectural
and markovian factors of echo state networks.
Neural Networks, 24(5):440 – 456.

Dmitriy Genzel. 2010. Automatically learning
source-side reordering rules for large scale ma-
chine translation. In Proceedings of the 23rd In-
ternational Conference on Computational Linguistics,
COLING ’10, pages 376–384, Stroudsburg, PA,
USA. Association for Computational Linguistics.

Alex Graves. 2013. Generating sequences
with recurrent neural networks. arXiv preprint
arXiv:1308.0850.

Susan Howlett and Mark Dras. 2011. Clause re-
structuring for SMT not absolutely helpful. In
Proceedings of the 49th Annual Meeting of the As-
socation for Computational Linguistics: Human Lan-
guage Technologies, pages 384–388.

Fei Huang. 2009. Confidence measure for word
alignment. In Proceedings of the Joint Conference
of the 47th Annual Meeting of the ACL and the 4th
International Joint Conference on Natural Language
Processing of the AFNLP: Volume 2-Volume 2, pages
932–940. Association for Computational Linguis-
tics.

Herbert Jaeger. 2001. The echo state ap-
proach to analysing and training recurrent neu-
ral networks-with an erratum note. Bonn, Ger-
many: German National Research Center for Infor-
mation Technology GMD Technical Report, 148:34.

Maxim Khalilov and José AR Fonollosa. 2011.
Syntax-based reordering for statistical ma-

chine translation. Computer speech & language,
25(4):761–788.

Diederik Kingma and Jimmy Ba. 2014. Adam:
A method for stochastic optimization. arXiv
preprint arXiv:1412.6980.

Philipp Koehn, Hieu Hoang, Alexandra Birch,
Chris Callison-Burch, Marcello Federico, Nicola
Bertoldi, Brooke Cowan, Wade Shen, Christine
Moran, Richard Zens, Chris Dyer, Ondřej Bojar,
Alexandra Constantin, and Evan Herbst. 2007.
Moses: open source toolkit for statistical machine
translation. In Proceedings of the 45th Annual Meet-
ing of the ACL on Interactive Poster and Demonstra-
tion Sessions, ACL ’07, pages 177–180, Strouds-
burg, PA, USA. Association for Computational
Linguistics.

Philipp Koehn. 2005. Europarl: A parallel corpus
for statistical machine translation. In Conference
Proceedings: the tenth Machine Translation Summit,
pages 79–86, Phuket, Thailand. AAMT, AAMT.

Uri Lerner and Slav Petrov. 2013. Source-side clas-
sifier preordering for machine translation. In Pro-
ceedings of the 2014 Conference on Empirical Methods
in Natural Language Processing (EMNLP ’13).

Antonio Valerio Miceli Barone and Giuseppe At-
tardi. 2013. Pre-reordering for machine transla-
tion using transition-based walks on dependency
parse trees. In Proceedings of the Eighth Workshop
on Statistical Machine Translation, pages 164–169,
Sofia, Bulgaria, August. Association for Compu-
tational Linguistics.

Tomas Mikolov, Martin Karafiát, Lukas Burget, Jan
Cernockỳ, and Sanjeev Khudanpur. 2010. Recur-
rent neural network based language model. In
INTERSPEECH, pages 1045–1048.

Jiri Navratil, Karthik Visweswariah, and Anan-
thakrishnan Ramanathan. 2012. A comparison of
syntactic reordering methods for english-german
machine translation. In COLING, pages 2043–
2058.

Franz Josef Och, Christoph Tillmann, Hermann
Ney, et al. 1999. Improved alignment models for
statistical machine translation. In Proc. of the Joint
SIGDAT Conf. on Empirical Methods in Natural Lan-
guage Processing and Very Large Corpora, pages 20–
28.

Slav Petrov, Leon Barrett, Romain Thibaux, and Dan
Klein. 2006. Learning accurate, compact, and in-
terpretable tree annotation. In Proceedings of the
21st International Conference on Computational Lin-
guistics and the 44th annual meeting of the Associa-
tion for Computational Linguistics, pages 433–440.
Association for Computational Linguistics.

19



Roy Tromble and Jason Eisner. 2009. Learning
linear ordering problems for better translation.
In Proceedings of the 2009 Conference on Empirical
Methods in Natural Language Processing: Volume 2 -
Volume 2, EMNLP ’09, pages 1007–1016, Strouds-
burg, PA, USA. Association for Computational
Linguistics.

Karthik Visweswariah, Rajakrishnan Rajkumar,
Ankur Gandhe, Ananthakrishnan Ramanathan,
and Jiri Navratil. 2011. A word reordering model
for improved machine translation. In Proceedings
of the Conference on Empirical Methods in Natural
Language Processing, EMNLP ’11, pages 486–496,
Stroudsburg, PA, USA. Association for Computa-
tional Linguistics.

Matthew D Zeiler. 2012. Adadelta: An adap-
tive learning rate method. arXiv preprint
arXiv:1212.5701.

20


