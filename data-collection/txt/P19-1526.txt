




















































Enhancing Unsupervised Generative Dependency Parser with Contextual Information


Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 5315–5325
Florence, Italy, July 28 - August 2, 2019. c©2019 Association for Computational Linguistics

5315

Enhancing Unsupervised Generative Dependency Parser with Contextual
Information

Wenjuan Han, Yong Jiang and Kewei Tu∗

{hanwj,jiangyong,tukw}@shanghaitech.edu.cn
School of Information Science and Technology

ShanghaiTech University, Shanghai, China

Abstract

Most of the unsupervised dependency parsers

are based on probabilistic generative models

that learn the joint distribution of the given

sentence and its parse. Probabilistic gener-

ative models usually explicit decompose the

desired dependency tree into factorized gram-

mar rules, which lack the global features of

the entire sentence. In this paper, we pro-

pose a novel probabilistic model called dis-

criminative neural dependency model with va-

lence (D-NDMV) that generates a sentence

and its parse from a continuous latent repre-

sentation, which encodes global contextual in-

formation of the generated sentence. We pro-

pose two approaches to model the latent rep-

resentation: the first deterministically sum-

marizes the representation from the sentence

and the second probabilistically models the

representation conditioned on the sentence.

Our approach can be regarded as a new type

of autoencoder model to unsupervised de-

pendency parsing that combines the benefits

of both generative and discriminative tech-

niques. In particular, our approach breaks the

context-free independence assumption in pre-

vious generative approaches and therefore be-

comes more expressive. Our extensive experi-

mental results on seventeen datasets from var-

ious sources show that our approach achieves

competitive accuracy compared with both gen-

erative and discriminative state-of-the-art un-

supervised dependency parsers.

1 Introduction

Dependency parsing is a very important task in

natural language processing. The dependency re-

lations identified by dependency parsing convey

syntactic information useful in subsequent appli-

cations such as semantic parsing, information ex-

traction, and question answering. In this paper, we

∗Corresponding author

focus on unsupervised dependency parsing, which

aims to induce a dependency parser from training

sentences without gold parse annotation.

Most previous approaches to unsupervised de-

pendency parsing are based on probabilistic gener-

ative models, for example, the Dependency Model

with Valence (DMV) (Klein and Manning, 2004)

and its extensions (Cohen and Smith, 2009; Head-

den III et al., 2009; Cohen and Smith, 2010; Berg-

Kirkpatrick et al., 2010; Gillenwater et al., 2010;

Jiang et al., 2016). A disadvantage of such ap-

proaches comes from the context-freeness of de-

pendency grammars, a strong independence as-

sumption that limits the information available in

determining how likely a dependency is between

two words in a sentence. In DMV, the probability

of a dependency is computed from only the head

and child tokens, the dependency direction, and

the number of dependencies already connected

from the head token. Additional information used

for computing dependency probabilities in later

work is also limited to local morpho-syntactic fea-

tures such as word forms, lemmas and categories

(Berg-Kirkpatrick et al., 2010), which does not

break the context-free assumption.

More recently, researchers have started to uti-

lize discriminative methods in unsupervised de-

pendency parsing based on the idea of discrimi-

native clustering (Grave and Elhadad, 2015), the

CRFAE framework (Cai et al., 2017) or the neu-

ral variational transition-based parser (Li et al.,

2019). By conditioning dependency prediction

on the whole input sentence, discriminative meth-

ods are capable of utilizing not only local infor-

mation, but also global and contextual informa-

tion of a dependency in determining its strength.

Specifically, both Grave and Elhadad (2015) and

Cai et al. (2017) include in the feature set of a de-

pendency the information of the tokens around the

head or child token of the dependency. In this way,



5316

they break the context-free independence assump-

tion because the same dependency would have dif-

ferent strength in different contexts. Besides, Li

et al. (2019) propose a variational autoencoder ap-

proach based on Recurrent Neural Network Gram-

mars.

In this paper, we propose a novel approach

to unsupervised dependency parsing in the mid-

dle between generative and discriminative ap-

proaches. Our approach is based on neural DMV

(Jiang et al., 2016), an extension of DMV that

employs a neural network to predict dependency

probabilities. Unlike neural DMV, however, when

computing the probability of a dependency, we

rely on not only local information as in DMV,

but also global and contextual information from

a compressed representation of the input sentence

produced by neural networks. In other words, in-

stead of modeling the joint probability of the input

sentence and its dependency parse as in a genera-

tive model, we model the conditional probability

of the sentence and parse given global information

of the sentence. Therefore, our approach breaks

the context-free assumption in a similar way to

discriminative approaches, while it is still able to

utilize many previous techniques (e.g., initializa-

tion and regularization techniques) of generative

approaches.

Our approach can be seen as an autoencoder.

The decoder is a conditional generative neural

DMV that generates the sentence as well as its

parse from a continuous representation that cap-

tures the global features of the sentence. To

model such global information, we propose two

types of encoders, one deterministically summa-

rizes the sentence with a continuous vector while

the other probabilistically models the continuous

vector conditioned on the sentence. Since the

neural DMV can act as a fully-fledged unsuper-

vised dependency parser, the encoder can be seen

as a supplementary module that injects contex-

tual information into the neural DMV for context-

specific prediction of dependency probabilities.

This is very different from the previous unsuper-

vised parsing approach based on the autoencoder

framework (Cai et al., 2017; Li et al., 2019), in

which the encoder is a discriminative parser and

the decoder is a generative model, both of which

are required for performing unsupervised parsing.

Our experiments verify that our approach

achieves a comparable result with recent state-of-

the-art approaches on extensive datasets from var-

ious sources.

2 Related Work

2.1 Dependency Model with Valence

The Dependency Model with Valence (DMV)

(Klein and Manning, 2004) is an extension of an

earlier dependency model (Carroll and Charniak,

1992) for grammar induction. Different from the

earlier model, there are three types of probabilis-

tic grammar rules in DMV, namely ROOT, CHILD

and CHILD rules. To generate a token sequence

and its corresponding dependency parse tree, the

DMV model first generates a token c from the

ROOT distribution p(c|root). Then the generation
continues in a recursive procedure. At each gener-

ation step, it makes a decision as to whether a new

token needs to be generated from the current head

token h in the dir direction by sampling a STOP or

CONTINUE symbol dec from the CHILD distri-

bution p(dec|h, dir, val) where val is an indicator
representing whether token h has already gener-

ated a token before. If dec is CONTINUE, a new

token is generated from the CHILD distribution

p(c|h, dir, val). If dec is STOP, then the gener-
ation process switches to a new direction or a new

head token. DMV can be trained from an unanno-

tated corpus using the expectation-maximization

algorithm.

2.2 Neural DMV

The DMV model is very effective in inducing syn-

tactic dependency relations between tokens in a

sentence. One limitation of DMV is that correla-

tion between similar tokens (such as different verb

POS tags) is not taken into account during learning

and hence rules involving similar tokens have to

be learned independently. Berg-Kirkpatrick et al.

(2010) proposed a feature-based DMV model in

which the grammar rule probabilities are com-

puted by a log-linear model with manually de-

signed features that reflect token similarity. Jiang

et al. (2016) proposed the neural DMV model

which learns token embeddings to better capture

correlations between tokens and utilizes a neu-

ral network to calculate grammar rule probabili-

ties from the embeddings. Both approaches sig-

nificantly outperform the original DMV. However,

because of the strong independence assumption in

such generative models, they can only utilize local

information of a grammar rule (e.g., the head and



5317

child tokens, direction, and valence) when com-

puting its probability.

3 Discriminative Neural DMV

We extend the neural DMV such that when pre-

dicting the probability of a grammar rule in pars-

ing a sentence, the model incorporates not only lo-

cal information of the rule but also global informa-

tion of the sentence. Specifically, we model each

grammar rule probability conditioned on a con-

tinuous vector. We therefore call our model the

discriminative neural DMV (D-NDMV). In this

way, the probability of a dependency rule becomes

sensitive to the input sentence, which breaks the

context-free assumption in the neural DMV. Here,

we provide two approaches to model this global

continuous vector.

3.1 Deterministic Variant for D-NDMV

Model

Suppose we have a sentence (i.e., a word se-

quence) w, the corresponding POS tag sequence

x, and the dependency parse z which is hidden

in unsupervised parsing. DMV and its variants

model the joint probability of the POS tag se-

quence and the parse P (x, z) and, because of the
context-free assumption, factorize the probability

based on the grammar rules used in the parse. In

contrast, to the global features of the sentence,

we model the conditional probability of the POS

tag sequence and the parse given the sequence

w: P (x, z|w). We assume conditional context-
freeness and factorize the conditional probability

based on the grammar rules.

PΘ(x, z|w) =
∏

r∈(x,z)

p(r|w)
(1)

where r ranges over all the grammar rules used

in the parse z of tag sequence x, Θ is the set of
parameters to compute parameters of the distribu-

tion. Since one can reliably predict the POS tags

x from the words w without considering the parse

z (as most POS taggers do), to avoid degeneration

of the model, we compute p(r|w) based on global
information of w produced by a long short-term

memory network (LSTM).

Figure 1 shows the neural network structure for

parametering p(chd|head, dir, val,w), the prob-
abilities of CHILD rules given the input sentence

w. The structure is similar to the one used in neu-

ral DMV except for using LSTM sentence encoder

Inputs:

Softmax Layer:

Valence Head Tag

…
 Outputs

Wdir

Wchd
Hidden Layer:

g = ReLU(Wdir[vval; vh; vw])

word 1 word 2 word 3

LSTM LSTM LSTM

Embeddings:

Sentence

Representation:

Concatenation:
[vval; vh; vw]

Softmax(Wchdg)

vval vh
Embeddings:

Sequence:w

vw

Figure 1: The neural network structure for computing

the probabilities of CHILD rules.

to get the representation s from the sentence w.

The embeddings of the head POS tag and valence

are represented by vh and vval. The concatena-

tion [vval;vh; s] is fed into a fully-connected layer
with a direction-specific weight matrix Wdir and

the ReLU activation function to produce the hid-
den layer g. All possible child POS tags are repre-

sented by the matrix Wchd. The i-th row of Wchd
represents the output embedding of the i-th POS

tag. We take the product of the hidden layer g and

the child matrix Wchd and apply a softmax func-

tion to obtain the CHILD rule probabilities. ROOT

and CHILD rule probabilities are computed in a

similar way.

Since the mapping from w to s is deterministic,

we call it the deterministic variant of D-NDMV.

To make the notations consistent with subsequent

sections, we add an auxiliary random variable s

to represent the global information of sentence w.

The probabilistic distribution of s is defined as,

PΦ(s|w) = δ(s− vw) (2)

where Φ is the set of parameters of the LSTM neu-
ral network.

Figure 2 (left) shows the directed graphical rep-

resentation of this model. If we diminish the ca-

pacity of s (e.g., by shrinking its dimension), then

our model gradually reduces to neural DMV.

Parsing

Given a deterministic variant with fixed parame-

ters Φ,Θ. we can parse a sentence represented
by POS tag sequence x and word sequence w



5318

w

sΦ Θ

x, z

sΦ Θ

x, z

N N
x

Figure 2: Left: the illustration of the deterministic

variant of D-NDMV as a directed graph. The deter-

ministic variant models an autoencoder with PΦ(s|w))
as the encoder and PΘ(x, z|s) as the decoder. Right:
the illustration of the variational variant of D-NDMV

as a directed graph. We use dashed lines to denote

the variational approximation qΦ(s|x) to the intractable
posterior PΦ(s|x), and the solid lines to denote the gen-
erative model P (s)PΘ(x, z|s).

by searching for a dependency tree z∗ which has

the highest probability p(x, z|w) among the set of
valid parse trees Z(x).

z
∗ = argmax

z∈Z(x)
PΘ,Φ(x, z|w) (3)

Note that once we compute all the grammar rule

probabilities based on w, our model becomes a

standard DMV and therefore dynamic program-

ming can be used to parse each sentence efficiently

(Klein and Manning, 2004).

Unsupervised Learning

Objective Function: In a typical unsupervised

dependency parsing setting, we are given a set of

training sentences with POS tagging but without

parse annotations. The objective function of learn-

ing deterministic variant is as follows.

J(Θ,Φ) =
1

N

N∑

i=1

logPΘ,Φ(x
(i)|w(i)) (4)

The log conditional likelihood is defined as:

logPΘ,Φ(x|w) = log
∑

z∈Z(x)

PΘ,Φ(x, z|w) (5)

We may replace summation with maximization so

that it becomes the conditional Viterbi likelihood.

Learning Algorithm: We optimize our objec-

tive function using the expectation-maximization

(EM) algorithm. Specifically, the EM algorithm

alternates between E-steps and M-steps to maxi-

mize a lower-bound of the objective function. For

each training sentence, the lower bound is defined

as:

Q(q,Θ,Φ) = logPΘ,Φ(x|w)

−KL(q(z)‖PΘ,Φ(z|x,w))
(6)

where q(z) is an auxiliary distribution over the la-
tent parse z.

In the E-step, we fix Θ,Φ and maximize
Q(q,Θ,Φ) with respect to q. The maximum is
reached when the Kullback-Leibler divergence is

zero, i.e.,

q(z) = PΘ,Φ(z|x,w) (7)

Based on the optimal q, we compute the ex-

pected counts Eq(z)c(r,x, z) using the inside-
outside algorithm, where c(r,x, z) is the number
of times rule r is used in producing parse z of tag

sequence x.

In the M-step, we fix q and maximize

Q(q,Θ,Φ) with respect to Θ,Φ. The lower bound
now takes the following form:

Q(Θ,Φ) =
∑

r

log p(r|w)Eq(z)c(r,x, z)

− Constant

(8)

where r ranges over all the grammar rules and

Constant is a constant value. The probabilities
p(r|w,Θ,Φ) are computed by the neural networks
and we can back-propagate the objective Q(Θ,Φ)
into the parameters of the neural networks.

We initialize the model either heuristically

(Klein and Manning, 2004) or using a pre-trained

unsupervised parser (Jiang et al., 2016); then we

alternate between E-steps and M-steps until con-

vergence.

Note that if we require q(z) to be a delta func-
tion, then the algorithm becomes hard-EM, which

computes the best parse of each training sentence

in the E-step and set the expected count to 1 if the

rule is used in the parse and 0 otherwise. It has

been found that hard-EM outperforms EM in un-

supervised dependency parsing (Spitkovsky et al.,

2010; Tu and Honavar, 2012), so we use hard-EM

in our experiments.

3.2 Variational Variant for D-NDMV

Motivated by (Bowman et al., 2016), we pro-

pose to model the global representation s as draw-

ing from a prior distribution, generally a standard



5319

Gaussian distribution. We also propose a vari-

ational posterior distribution qΦ(s|x) to approxi-
mate this prior distribution. In this way, we for-

malize it into a variational inference framework.

We call this model variational variant and illus-

trate its graphical model in Figure 2 (right). It can

be seen from Figure 2 (right) that the variational

variant shares the same formulation of the encoder

part with the variational autoencoder (VAE). Dif-

ferent from the vanilla VAE model with a simple

multilayered feedforward neural network as the

decoder, our decoder is a generative latent variable

model with the structured hidden variable z.

For the learning of the variational variant, we

use the log likelihood as the objective function and

optimize its lower bound. We show the derivation

as followings:

logPΦ,Θ(x)

≥−KL(qΦ(s|x)||p(s)) + EqΦ(s|x) logPΘ(x|s)

(9)

By performing the Monte Carlo method to esti-

mate the expectation w.r.t. qΦ(s|x) and set the
number of samples L to 1, we rewrite the second

term as:

EqΦ(s|x) log pΘ(x|s)

≃
1

L

L∑

l=1

log
∑

z∈Z(x)

pΘ(x, z|s
(l))

= log
∑

z∈Z(x)

pΘ(x, z|s
(1))

(10)

where s(l) is estimated by the reparameteriza-

tion trick (Kingma and Welling, 2014), which en-

ables low gradient variances and stabilizes train-

ing.

Because this formula is similar to Eq. 5, we

can follow the subsequent derivation of determin-

istic variant and learn the variational variant us-

ing EM. It is worth noting that different from de-

terministic variant, in M-step an additional KL di-

vergence term in Eq. 9 should be optimized by

back-propagation.

4 Experiments

We tested our methods on seventeen treebanks

from various sources. For each dataset, we com-

pared with current state-of-the-art approaches on

the specific dataset.

4.1 Dataset and Setup

English Penn Treebank We conducted exper-

iments on the Wall Street Journal corpus (WSJ)

with section 2-21 for training, section 22 for val-

idation and section 23 for testing. We trained

our model with training sentences of length ≤ 10,
tuned the hyer-parameters on validation sentences

of length ≤ 10 the and evaluated on testing sen-
tences of length ≤ 10 (WSJ10) and all sentences
(WSJ). We reported the directed dependency ac-

curacy (DDA) of the learned grammars on the test

sentences.

Universal Dependency Treebank Following

the setup of Jiang et al. (2017); Li et al. (2019),

we conducted experiments on selected eight lan-

guages from the Universal Dependency Treebank

1.4 (Nivre et al., 2016). We trained our model on

training sentences of length ≤ 15 and report the
DDA on testing sentences of length ≤ 15 and ≤
40.

Datasets from PASCAL Challenge on Gram-

mar Induction We conducted experiments on

corpora of eight languages from the PASCAL

Challenge on Grammar Induction (Gelling et al.,

2012). We trained our model with training sen-

tences of length ≤ 10 and evaluated on testing sen-
tences of length ≤ 10 and all sentences.

Note that on the UD Treebanks and PASCAL

datasets, we used the same hyper-parameters as in

the WSJ experiments without further tuning.

Setup Following previous work, we conducted

experiments under the unlexicalized setting where

a sentence is represented as a sequence of gold

part-of-speech tags with punctuations removed.

The embedding length was set to 10 for the head

and child tokens and the valence. The sentence

embedding length was also set to 10. We trained

the neural networks using stochastic gradient de-

scent with batch size 10 and learning rate 0.01. We

used the change of the loss on the validation set

as the stop criteria. For our methods in the WSJ

experiments, we followed Han et al. (2017) and

initialized our model using the pre-trained model

of Naseem et al. (2010), which significantly in-

creased the accuracy and decreased the variance.

For the other experiments, we used a pre-trained

NDMV model to initialize our method. We ran our

model for 5 times and report the average DDA.



5320

METHODS WSJ10 WSJ

Systems in Basic Setup

DMV (Klein and Manning, 2004) 58.3 39.4
LN (Cohen et al., 2008) 59.4 40.5
Convex-MST (Grave and Elhadad, 2015) 60.8 48.6
Shared LN (Cohen and Smith, 2009) 61.3 41.4
Feature DMV (Berg-Kirkpatrick et al., 2010) 63.0 -
PR-S (Gillenwater et al., 2010) 64.3 53.3
E-DMV (Headden III et al., 2009) 65.0 -
TSG-DMV (Blunsom and Cohn, 2010) 65.9 53.1
UR-A E-DMV (Tu and Honavar, 2012) 71.4 57.0
CRFAE (Cai et al., 2017) 71.7 55.7
Neural E-DMV(Jiang et al., 2016) 72.5 57.6
HDP-DEP (Naseem et al., 2010) 73.8 -
NVTP (Li et al., 2019) 54.7 37.8
L-EVG* (Headden III et al., 2009) 68.8 -
LexTSG-DMV* (Blunsom and Cohn, 2010) 67.7 55.7
L-NDMV* (Han et al., 2017) 75.1 59.5

variational variant D-NDMV 75.5 60.4
deterministic variant D-NDMV 75.6 61.4

Systems with Additional Training Data (for reference)

CS (Spitkovsky et al., 2013) 72.0 64.4
MaxEnc* (Le and Zuidema, 2015) 73.2 65.8

Table 1: Comparison on WSJ. ∗: approaches with lex-
icalized information.

4.2 Results on English Penn Treebank

In Table 1, we compared our method with a large

number of previous approaches to unsupervised

dependency parsing. Both variational variant

and deterministic variant outperform recent ap-

proaches in the basic setup, which demonstrates

the benefit of utilizing contextual information in

dependency strength prediction. Deterministic

variant has a slightly better parsing accuracy than

variational variant but variational variant is more

stable. The standard derivations of determinis-

tic variant and variational variant are 0.530 and
0.402 respectively for 5 runs.

4.3 Results on Universal Dependency

Treebank

We compare our model with several state-of-the-

art models on the UD Treebanks and report the re-

sults in Table 2.

We first compare our model with two generative

models: NDMV and left corner DMV (LC-DMV)

(Noji et al., 2016). The LC-DMV is the recent

state-of-the-art generative approach on Universal

Dependency Treebank. Our variational variant D-

NDMV outperforms the LC-DMV and the NDMV

on average.

Furthermore, we compare our model with cur-

rent state-of-the-art discriminative models, the

neural variational transition-based parser (NVTP)

(Li et al., 2019) and Convex-MST (Grave and El-

NO UP + UP

NDMV LD DV VV NVTP CM

Length ≤ 15

Basque 48.3 47.9 40.6 42.7 52.9 52.5

Dutch 44.1 35.5 42.1 43.0 39.6 43.4

French 59.5 52.1 59.0 61.7 59.9 61.6

German 56.2 51.9 56.4 58.5 57.5 54.4

Italian 72.7 73.1 59.6 63.5 59.7 73.2

Polish 72.7 66.2 70.5 75.8 57.1 66.7

Portuguese 34.4 70.5 68.8 69.1 52.7 60.7

Spanish 38.1 65.5 63.8 66.6 55.6 61.6

Average 53.3 57.8 57.6 60.1 54.4 59.3

Length ≤ 40

Basque 47.8 45.4 39.9 42.4 48.9 50.0

Dutch 35.6 34.1 42.4 43.7 42.5 45.3

French 38.1 48.6 57.2 58.5 55.4 62.0

German 50.4 50.5 54.5 52.9 54.2 51.4

Italian 63.6 71.1 60.2 61.3 55.7 69.1

Polish 62.8 63.7 66.7 73.0 51.7 63.4

Portuguese 49.0 67.2 64.7 65.7 45.3 57.1

Spanish 58.0 61.9 64.3 64.4 52.4 61.9

Average 50.7 55.3 56.2 57.7 50.8 57.5

Table 2: Comparison on Universal Dependency Tree-

bank. No UP: Systems without universal linguistic

prior. +UP: Systems with universal linguistic prior.

LD: LC-DMV (Noji et al., 2016). DV: determinis-

tic variant of D-NDMV. VV: variational variant of

D-NDMV. NVTP: neural variational transition-based

parser (Li et al., 2019). CM: Convex-MST.

hadad, 2015). Note that current discriminative ap-

proaches usually rely on strong universal linguistic

prior 1 to get better performance. So the compar-

isons may not be fair for our model. Despite this,

we find that our model can achieve competitive ac-

curacies compared with these approaches.

4.4 Results on Datasets from PASCAL

Challenge

We also perform experiments on the datasets from

PASCAL Challenge (Gelling et al., 2012), which

contains eight languages: Arabic, Basque, Czech,

Danish, Dutch, Portuguese, Slovene and Swedish.

We compare our approaches with NDMV (Jiang

et al., 2016), Convex-MST (Grave and Elhadad,

2015) and CRFAE (Cai et al., 2017). NDMV and

CRFAE are two state-of-the-art approaches on the

PASCAL Challenge datasets. We show the di-

rected dependency accuracy on the testing sen-

tences no longer than 10 (Table 3) and on all the

testing sentences (Table 4). It can be seen that on

average our models outperform other state-of-the-

1Universal linguistic prior (UP) is a set of syntactic depen-
dencies that are common in many languages (Naseem et al.,
2010).



5321

Arabic Basque Czech Danish Dutch Portuguese Slovene Swedish Average

Approaches Without Using Universal Linguistic Prior

E-DMV 38.4 41.5 45.5 52.4 37.0 40.9 35.2 52.6 42.9

Neural DMV 60.0 44.1 46.2 63.3 33.2 36.9 31.6 48.3 45.4

Convex-MST 55.2 29.4 36.5 49.3 35.5 43.2 27.5 30.2 38.3

CRF-AE 42.4 45.8 24.4 23.9 28.8 33.0 33.4 45.6 34.6

deterministic variant 54.4 44.8 55.2 58.9 37.2 40.1 35.2 50.3 47.0

variational variant 60.0 45.4 59.1 63.6 34.6 42.7 28.3 45.9 47.5

Approaches Using Universal Linguistic Prior

Convex-MST 39.0 27.8 43.8 48.1 35.9 55.6 62.6 49.6 45.3

CRF-AE 39.2 33.9 45.1 44.5 42.2 61.9 41.9 66.0 46.8

Table 3: DDA on testing sentences no longer than 10 on eight additional languages from PASCAL Challenge.

Arabic Basque Czech Danish Dutch Portuguese Slovene Swedish Average

Approaches Without Using Universal Linguistic Prior

E-DMV 27.4 33.8 37.4 44.9 24.7 34.8 23.2 40.2 33.3

Neural DMV 30.9 37.7 38.1 53.3 22.9 30.7 19.9 33.9 33.4

Convex-MST 47.7 30.5 33.4 44.2 28.3 35.9 18.1 29.2 33.4

CRF-AE 29.9 39.1 20.3 18.6 17.8 32.6 28.0 37.0 27.9

deterministic variant 38.2 38.8 47.3 47.3 24.7 34.1 23.2 40.1 36.7

variational variant 33.9 41.2 48.4 54.7 25.3 35.8 28.1 40.5 38.5

Approaches Using Universal Linguistic Prior

Convex-MST 34.2 24.9 39.0 36.3 35.2 46.0 51.7 39.6 38.3

CRF-AE 37.2 30.3 36.4 33.2 38.3 52.4 29.2 47.1 38.2

Table 4: DDA on all the testing sentences on eight additional languages from PASCAL Challenge.

art approaches including those utilizing the univer-

sal linguistic prior.

5 Analysis

In this section, we studies what information is cap-

tured in the sentence embeddings and the some

configurations that are sensitive to our model.

Here we use deterministic variant of D-NDMV to

conduct the following analysis. deterministic vari-

ant of D-NDMV performs similar to deterministic

variant of D-NDMV.

5.1 Rule Probabilities in Different Sentences

The motivation behind D-NDMV is to break the

independence assumption and utilize global infor-

mation in predicting grammar rule probabilities.

Here we conduct a few case studies of what infor-

mation is captured in the sentence embedding and

how it influences grammar rule probabilities.

We train a D-NDMV on WSJ and extract all the

embeddings of the training sentences. We then

focus on the following two sentences: “What ’s

next” and “He has n’t been able to replace the

M’Bow cabal”. We now examine the dependency

rule probability of VBZ generating JJ to the right

with valence 0 in these two sentences (illustrated
in Figure 3). In the first sentence, this rule is used

in the gold parse (“’s” is the head of “next”); but

VBZ

…         has           n’t been           able         …

What            ’s               next

JJ

RBVBZ JJVBN

WP

0.0798

0.0699

Figure 3: Rule probabilities predicted by D-NDMV

given the two example sentences

in the second sentence, this rule is not used in the

gold parse (the head of “able” is “been” instead

of “has”). We observe that the rule probability re-

dicted by D-NDMV given the first sentence is in-

deed significantly larger than that given the second

sentence, which demonstrates the positive impact

of conditioning rule probability prediction on the

sentence embedding.

To obtain a more holistic view of how rule prob-

abilities change in different sentences, we collect

the probabilities of a particular rule (“IN” gener-

ating “CD” to the right with valence 1) predicted
by our model for all the sentences of WSJ. Figure

4 shows two distributions over the rule probability

when the rule is used in the gold parse vs. when

the rule is applicable to parsing the sentence but is

not used in the gold parse. It can be seen that when



5322

F
re

q
u

e
n

c
y

0

0.1

0.2

0.3

0.4

Rule Probability

0.1 - 0.15 

0.15 - 0.2 

0.2 - 0.25 

0.25 - 0.3 

0.3 - 0.35 

0.35 - 0.4 

0.4 - 0.45 

0.45 - 0.5 

0.5 - 0.55 

0.55 - 0.6 

0.6 - 0.65 

0.65 - 0.7 

0.7 - 0.75 

0.75 - 0.8 

Not In Gold Parse In Gold Parse

Figure 4: Comparison of the distributions over the rule

probability when the rule appears vs. does not appear

in the gold parse.

AVERAGE PROBABILITY D-NDMV E-DMV

All 0.107 0.094
In gold parse 0.253 0.219
Not in gold parse 0.097 0.085

Table 5: Comparison of the average probabilities in D-

NDMV and E-DMV when the rule is used and not used

in the gold parse.

the rule appears in the gold parse, its probability is

clearly boosted in our model.

Finally, for every sentence of WSJ, we collect

the probabilities predicted by our model for all the

rules that are applicable to parsing the sentence.

We then calculate the average probability 1) when

the rule is used in the gold parse, 2) when the

rule is not used in the gold parse, and 3) regard-

less of whether the rule is used in the gold parse

or not. We use the E-DMV model as the base-

line in which rule probabilities do not change with

sentences. The results are shown in Table 5. We

observe that compared with the E-DMV baseline,

the rule probabilities predicted by our model are

increased by 14.0% on average, probably because

our model assigns higher probabilities to rules ap-

plicable to parsing the input sentence than to rules

not applicable (e.g., if the head or child of the rule

does not appear in the sentence). The increase of

the average probability when the rule is used in the

gold parse (15.7%) is higher than when the rule is

not used in the gold parse (13.7%), which again

demonstrates the advantage of our model.

5.2 Choice of Sentence Encoder

Besides LSTM, there are a few other methods of

producing the sentence representation. Table 6

compares the experimental results of these meth-

ods. The bag-of-tags method simply computes the

average of all the POS tag embeddings and has the

lowest accuracy, showing that the word order is in-

SENTENCE ENCODER DDA

Bag-of-Tags Method 74.1
Anchored Words Method 75.1
LSTM 75.9
Attention-Based LSTM 75.5
Bi-LSTM 74.2

Table 6: Comparison of different sentence encoders in

D-NDMV.

formative for sentence encoding in D-NDMV. The

anchored words method replaces the POS tag em-

bddings used in the neural network of the neural

DMV with the corresponding hidden vectors pro-

duced by a LSTM on top of the input sentence,

which leads to better accuracy than bag-of-tags but

is still worse than LSTM. Replacing LSTM with

Bi-LSTM or attention-based LSTM also does not

lead to better performance, probably because these

models are more powerful and hence more likely

to result in degeneration and overfitting.

5.3 Impact of Genres

All the sentences in WSJ come from newswire,

which conform to very similar syntactic styles.

Here we study whether our method can capture

different syntactic styles by learning our method

from Chinese Treebank 9.0 (2005) which contains

sentences of two different genres: the informal

genre and the formal genre. The experimental

setup is the same as that in section 4. We pick

the rule of “CD” (number) generating “AD” (ad-

verb) to the left with valence 0 and collect the rule
probability in sentences from the two genres.In in-

formal sentences our model assigns smaller prob-

abilities to the rule than in formal sentences. This

may reflect the fact that formal texts are more pre-

cise than informal text when presenting numbers,

which is captured by our model2.

5.4 Impact of Sentence Embedding

Dimension

The dimension of sentence embeddings in our

model is an important hyper-parameter. If the di-

mension is too large, the sentence embedding may

capture too much information of the sentence and

hence the model is very likely to degenerate or

overfit as discussed in section 3.1. If the dimen-

sion is too small, the model loses the benefit of

sentence information and becomes similar to neu-

ral DMV. As Figure 5 illustrates, dimension 10
leads to the best parsing accuracy, while dimen-

2More details can be found in the supplementary materi-
als.



5323

-23

-21

-19

-17

-15

74

74.5

75

75.5

76

Sentence Embedding Dimension

5 10 20

DDA

Training Loss

Validation Loss

Figure 5: Impact of the sentence embedding dimension

on both the testing set parsing accuracy and the average

conditional log Viterbi likelihood (w.r.t. loss) of the

training set and the validation set.

sion 20 produces lower parsing accuracy proba-
bly because of a combination of degeneration and

overfitting. The conditional log Viterbi likelihood

curves on the training set and the validation set in

Figure 5 confirm that overfitting indeed occur with

dimension 20.

6 Conclusion

We propose D-NDMV, a novel unsupervised

parser with characteristics from both generative

and discriminative approaches to unsupervised

parsing. D-NDMV extends neural DMV by pars-

ing a sentence using grammar rule probabilities

that are computed based on global information of

the sentence. In this way, D-NDMV breaks the

context-free independence assumption in genera-

tive dependency grammars and is therefore more

expressive. Our extensive experimental results

show that our approach achieves competitive ac-

curacy compared with state-of-the-art parsers.

Acknowledgments

This work was supported by the Major Program

of Science and Technology Commission Shanghai

Municipal (17JC1404102).

References

Taylor Berg-Kirkpatrick, Alexandre Bouchard-Côté,
John DeNero, and Dan Klein. 2010. Painless un-
supervised learning with features. In Human Lan-
guage Technologies: The 2010 Annual Conference
of the North American Chapter of the Association
for Computational Linguistics, pages 582–590. As-
sociation for Computational Linguistics.

Phil Blunsom and Trevor Cohn. 2010. Unsupervised
induction of tree substitution grammars for depen-

dency parsing. In Proceedings of the 2010 Con-
ference on Empirical Methods in Natural Language
Processing, pages 1204–1213. Association for Com-
putational Linguistics.

Samuel R Bowman, Luke Vilnis, Oriol Vinyals, An-
drew M Dai, Rafal Jozefowicz, and Samy Ben-
gio. 2016. Generating sentences from a continuous
space. CoNLL 2016, page 10.

Jiong Cai, Yong Jiang, and Kewei Tu. 2017. Crf
autoencoder for unsupervised dependency parsing.
In Proceedings of the 2017 Conference on Empiri-
cal Methods in Natural Language Processing, pages
1638–1643.

Glenn Carroll and Eugene Charniak. 1992. Two exper-
iments on learning probabilistic dependency gram-
mars from corpora. Department of Computer Sci-
ence, Univ.

Shay B Cohen, Kevin Gimpel, and Noah A Smith.
2008. Logistic normal priors for unsupervised prob-
abilistic grammar induction. In Advances in Neural
Information Processing Systems, pages 321–328.

Shay B Cohen and Noah A Smith. 2009. Shared lo-
gistic normal distributions for soft parameter tying
in unsupervised grammar induction. In Proceed-
ings of Human Language Technologies: The 2009
Annual Conference of the North American Chap-
ter of the Association for Computational Linguistics,
pages 74–82. Association for Computational Lin-
guistics.

Shay B Cohen and Noah A Smith. 2010. Covari-
ance in unsupervised learning of probabilistic gram-
mars. The Journal of Machine Learning Research,
11:3017–3051.

Douwe Gelling, Trevor Cohn, Phil Blunsom, and Joao
Graça. 2012. The pascal challenge on grammar in-
duction. In Proceedings of the NAACL-HLT Work-
shop on the Induction of Linguistic Structure, pages
64–80. Association for Computational Linguistics.

Jennifer Gillenwater, Kuzman Ganchev, Joao Graça,
Fernando Pereira, and Ben Taskar. 2010. Sparsity in
dependency grammar induction. In Proceedings of
the ACL 2010 Conference Short Papers, pages 194–
199. Association for Computational Linguistics.

Edouard Grave and Noémie Elhadad. 2015. A con-
vex and feature-rich discriminative approach to de-
pendency grammar induction. In Proceedings of the
53rd Annual Meeting of the Association for Compu-
tational Linguistics and the 7th International Joint
Conference on Natural Language Processing (Vol-
ume 1: Long Papers), volume 1, pages 1375–1384.

Wenjuan Han, Yong Jiang, and Kewei Tu. 2017. De-
pendency grammar induction with neural lexicaliza-
tion and big training data. In Proceedings of the
2017 Conference on Empirical Methods in Natural
Language Processing, pages 1683–1688.



5324

William P Headden III, Mark Johnson, and David
McClosky. 2009. Improving unsupervised depen-
dency parsing with richer contexts and smoothing.
In Proceedings of Human Language Technologies:
The 2009 Annual Conference of the North American
Chapter of the Association for Computational Lin-
guistics, pages 101–109. Association for Computa-
tional Linguistics.

Yong Jiang, Wenjuan Han, and Kewei Tu. 2016. Un-
supervised neural dependency parsing. In Proceed-
ings of the 2016 Conference on Empirical Methods
in Natural Language Processing, pages 763–771,
Austin, Texas. Association for Computational Lin-
guistics.

Yong Jiang, Wenjuan Han, and Kewei Tu. 2017. Com-
bining generative and discriminative approaches to
unsupervised dependency parsing via dual decom-
position. In Proceedings of the 2017 Conference on
Empirical Methods in Natural Language Process-
ing, pages 1689–1694, Copenhagen, Denmark. As-
sociation for Computational Linguistics.

Diederik P Kingma and Max Welling. 2014. Auto-
encoding variational bayes. In ICLR.

Dan Klein and Christopher D. Manning. 2004. Corpus-
based induction of syntactic structure: Models of de-
pendency and constituency. In Proceedings of the
42Nd Annual Meeting on Association for Computa-
tional Linguistics, ACL ’04, Stroudsburg, PA, USA.
Association for Computational Linguistics.

Phong Le and Willem Zuidema. 2015. Unsupervised
dependency parsing: Let’s use supervised parsers.
In Proceedings of the 2015 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics: Human Language Technologies,
pages 651–661.

Bowen Li, Jianpeng Cheng, Yang Liu, and Frank
Keller. 2019. Dependency grammar induction with
a neural variational transition-based parser. In AAAI.

Laurens Van der Maaten and Geoffrey Hinton. 2008.
Visualizing data using t-sne. Journal of Machine
Learning Research, 9(2579-2605):85.

Tahira Naseem, Harr Chen, Regina Barzilay, and Mark
Johnson. 2010. Using universal linguistic knowl-
edge to guide grammar induction. In Proceedings of
the 2010 Conference on Empirical Methods in Nat-
ural Language Processing, pages 1234–1244. Asso-
ciation for Computational Linguistics.

Joakim Nivre, Marie-Catherine De Marneffe, Filip
Ginter, Yoav Goldberg, Jan Hajic, Christopher D
Manning, Ryan T McDonald, Slav Petrov, Sampo
Pyysalo, Natalia Silveira, et al. 2016. Universal de-
pendencies v1: A multilingual treebank collection.
In LREC.

Hiroshi Noji, Yusuke Miyao, and Mark Johnson. 2016.
Using left-corner parsing to encode universal struc-
tural constraints in grammar induction. In Proceed-
ings of the 2016 Conference on Empirical Methods
in Natural Language Processing, pages 33–43.

Valentin I Spitkovsky, Hiyan Alshawi, and Daniel Ju-
rafsky. 2013. Breaking out of local optima with
count transforms and model recombination: A study
in grammar induction. In EMNLP, pages 1983–
1995.

Valentin I Spitkovsky, Hiyan Alshawi, Daniel Jurafsky,
and Christopher D Manning. 2010. Viterbi training
improves unsupervised dependency parsing. In Pro-
ceedings of the Fourteenth Conference on Computa-
tional Natural Language Learning, pages 9–17. As-
sociation for Computational Linguistics.

Kewei Tu and Vasant Honavar. 2012. Unambiguity
regularization for unsupervised learning of proba-
bilistic grammars. In Proceedings of the 2012 Joint
Conference on Empirical Methods in Natural Lan-
guage Processing and Computational Natural Lan-
guage Learning, pages 1324–1334. Association for
Computational Linguistics.

Naiwen Xue, Fei Xia, Fu-Dong Chiou, and Marta
Palmer. 2005. The penn chinese treebank: Phrase
structure annotation of a large corpus. Natural lan-
guage engineering, 11(2):207–238.

A Impact of Genres

All the sentences in WSJ come from newswire,

which conform to very similar syntactic styles.

Here we study whether our method can capture

different syntactic styles by learning our method

from Chinese Treebank 9.0 (2005) which con-

tains sentences of two different genres: the infor-

mal genre (chat messages and transcribed conver-

sational telephone speech) and the formal genre

(newswire, broadcast and so on). The experimen-

tal setup is the same as that in section 4.

We extract the embeddings of the training sen-

tences from the learned model and map them onto

a 3D space via the t-SNE algorithm (Van der

Maaten and Hinton, 2008) (Figure 6). It can be

seen that although the two types of sentences are

mixed together overall, many regions are clearly

dominated by one type or the other. This verifies

that sentence embeddings learned by our approach

can capture some genre information.

We pick the rule of “CD” (number) generating

“AD” (adverb) to the left with valence 0 and illus-
trate the distributions of the rule probability in sen-

tences from the two genres in Figure 7. It can be

seen that in informal sentences our model assigns



5325

What ’s next He has n’t been able to replace the M’Bow cabal
WP VBZ JJ PRP VBZ RB VBN JJ TO VB DT NNP NN

The government is nervous. I was shaking the whole time.
DT NN VBZ JJ. PRP VBD VBG DT JJ NN.

Both were right. But says Mr. Bock It was a close call.
DT VBD JJ. CC VBZ NNP NNP PRP VBD DT JJ NN.

That is n’t easy. Then there ’ll be another swing.
DT VBZ RB JJ. RB EX MD VB DT NN.

The IRA portion of the Packwood-Roth plan is irresponsible. He ’s totally geared to a punitive position.
DT NNP NN IN DT NNP NN VBZ JJ. PRP VBZ RB VBN TO DT JJ NN.

These figures are n’t seasonally adjusted. Her sister Cynthia wishes Toni had a different job.
DT NNS VBP RB RB JJ. PRP$ NN NNP VBZ NNP VBD DT JJ NN.

Table 7: Sentences closest to the two example sentences in terms of the L2 distance between their learned embed-

dings. Both the word sequence and the POS tag sequence are shown for each sentence.

Figure 6: 3D visualization of the learned sentence em-

beddings from CTB. Orange dots denote informal sen-

tences and blue dots denote formal sentences.

smaller probabilities to the rule than in formal sen-

tences. This may reflect the fact that formal texts

are more precise than informal text when present-

ing numbers, which is captured by our model.

B Nearby Sentences in Embedding Space

We train a our method on WSJ and extract all the

embeddings of the training sentences. We then

focus on the following two sentences: “What ’s

next” and “He has n’t been able to replace the

M’Bow cabal”.

Table 7 shows the two sentences as well as a

few other sentences closest to them measured by

the L2 distance between their embeddings. It can

be seen that most sentences close to the first sen-

tence contain a copula followed by a predicative

adjective, while most sentences close to the sec-

ond sentence end with a noun phrase where the

noun has a preceding modifier. These two exam-

ples show that the sentence embeddings learned

F
re

q
u

e
n

c
y

0

0.175

0.35

0.525

0.7

Rule Probability

0.0 - 0.1 

0.1 - 0.2 

0.2 - 0.3 

0.3 - 0.4 

0.4 - 0.5 

0.5 - 0.6 

0.6 - 0.7 

0.7 - 0.8 

0.8 - 0.9 

0.9 - 1.0 

Informal Genre Formal Genre

Figure 7: Comparison of the distributions over the rule

probability in sentences from the two genres.

by our approach encode syntactic information that

can be useful in parsing.


