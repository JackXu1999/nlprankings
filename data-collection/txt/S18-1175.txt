



















































ECNU at SemEval-2018 Task 11: Using Deep Learning Method to Address Machine Comprehension Task


Proceedings of the 12th International Workshop on Semantic Evaluation (SemEval-2018), pages 1048–1052
New Orleans, Louisiana, June 5–6, 2018. ©2018 Association for Computational Linguistics

ECNU at SemEval-2018 Task 11: Using Deep Learning Method to
Address Machine Comprehension Task

Yixuan Sheng1, Man Lan1,2∗, Yuanbin Wu1,2
1Department of Computer Science and Technology,
East China Normal University, Shanghai, P.R.China

2Shanghai Key Laboratory of Multidimensional Information Processing
51164500026@stu.ecnu.edu.cn, {mlan, ybwu}@cs.ecnu.edu.cn

Abstract
This paper describes the system we submitted
to the Task 11 in SemEval 2018, i.e., Machine
Comprehension using Commonsense Knowl-
edge. Given a passage and some questions that
each have two candidate answers, this task re-
quires the participate system to select out one
answer meet the meaning of original text or
commonsense knowledge from the candidate
answers. For this task, we use a deep learning
method to obtain final predict answer by calcu-
lating relevance of choices representations and
question-aware document representation.

1 Introduction

In recent years, the presentation of challenge and
large-scale reading comprehension corpora has
driven the development of technology for machine
reading comprehension, and most of these ma-
chine comprehension datasets do not need com-
monsense knowledge to answer questions. The
purpose of Machine Comprehension using Com-
monsense Knowledge task in Semeval 2018 is
to provide a platform for finding a way for the
machine to better understand the text and enable
the machine answer questions based on the text,
and encourage participants to make use any ex-
ternal resources (e.g., DeScript, narrative chains,
Wikipedia, etc) to improve the system perfor-
mance (Ostermann et al., 2018b). The task 11 is
a multiple-choice machine comprehension, which
requires a system read a narrative text about every-
day activities (Ostermann et al., 2018a) and then
answer multiple-choice questions based on this
text. Some questions need to be answered accord-
ing to the original text, and others can be answered
by commonsense knowledge. Each question is as-
sociated with a set of two answers. Table 1 gives
an example of the dataset.

To address this machine comprehension task,
we utilized rule-based methods and a deep learn-

document: Early this morning I woke up to the
sound of my newspaper landing on my driveway.
I sat up and wrapped my pink robe around me. I
slipped my feet into my slippers and looked at the
clock. It was only 7:00 but it was time for me to get
my newspaper and drink some coffee. I looked out
the window and noticed it was raining quite a bit.
I saw the newspaper at the end of my driveway, as
far away as it could be. I grabbed my umbrella out
of my coat closet and opened my front door enough
to stick the umbrella through and open it outside. I
stepped out the door and quickly covered my head
with the umbrella. Then I ran to the end of my
driveway, scooped up the newspaper in its plastic
wrapping, and ran back to my front door. I closed
my umbrella, took off my slippers, and dried off.
Then, I unwrapped my newspaper and sat down to
read it.
question: Do they read the paper daily?
candidate answers:
0. No they usually watch TV in the mornings.
1. Yes
answer: 1

Table 1: An example from the SemEval2018 task11
dataset.

ing method. Our final submission use Gated-
Attention Reader (Dhingra et al., 2016) to fuse
question information into document and acquire
a question-awared document representation, the
degree of interaction between choices and docu-
ment are regard as the probabilities of choices be-
ing returned as an answer. The above two meth-
ods do not use additional commonsense knowl-
edge, which may lead to the poor preformance
of our system. In future work, we may explore
more methods to integrate common knowledge
into models.

The rest of this paper is organized as follows.
Section 2 describes our systems. Section 3 de-
scribes datasets, experimental setting and analyse
results on datasets. Finally, Section 4 concludes
this work.

1048



2 System Description

2.1 Task Description
Formally, this multiple-choice machine compre-
hension task can be expressed as a quadruple:<D,
Q, A, a>. Where D represents a narrative text
about everyday activities, Q represents a question
for the content of the narrative text, A is the can-
didate answer choice set to the question(this task
contains two candidate answers choice a0 and a1)
and a represents the correct answer. The system is
expected to select an answer from A that best an-
swers Q according to the evidences in document
D or commonsense knowledge.

2.2 Two Rule-based Baselines
First of all, we implemented a rule-based system
proposed in (Richardson et al., 2013), which used
the sliding-window (SW) and word distance-based
(WD) algorithms to calculate the answer scores
according to the rules and return the highest-score
answer. We also tried the improved SW and
WD algorithms proposed in (Smith et al., 2015),
and the system performance has improvement.
Sliding-window and Word Distance-based algo-
rithms are are described as follows:

Sliding-Window: Given a data sample <D, Q,
a0(or a1), a>, firstly, we calculate the inverse
word counts of each word in the document D.
Then we set a window that slides word by word
from the beginning of the document to the end.
When the window slides to a position, the sum of
inverse word counts of all the words that appears
in the question Q or the candidate choice a0 (or a1)
is the score of the window at this moment. Until
the window slides to the end of the passage, we
choose the highest window score as the final score
of the candidate choice a0 (or a1). Window size is
size of union of the question Q and the choice a0
(or a1), and the window slides over full passage
only once. In the improved SW algorithm, the
window size is 2-30, and window passes full pas-
sage window several times, and increasing the size
of the window by one after each sliding over the
full passage. The summing up values of all passes
is served as improved sliding window score.

Word Distance-based: Given a data sample
<D, Q, a0(or a1), a>, firstly, we define two col-
lections, setdq and setdc, setdq represents the in-
tersection of the question words and the document
words, and setdc represents the intersection of the
words in the choice and the words of the docu-

ment. If neither setdq nor setdc is empty set, we
calculate the shortest distance between words of
setdp and words of setdc in the document, denote
the shortest distance as dmin, and the word dis-
tance score of the choice is dmin+1|D|−1 , otherwise, the
word distance score of the choice is zero.

The sliding-window score minus the word
distance-based score is the final score of the
choice. We separately calculated the scores of the
two choices for the question and then selected the
choice with higher score as the answer to the ques-
tion.

2.3 Deep Learning model

Both of the above unsupervised methods score the
overlap that between each answer and the doc-
ument by making a sliding-window passes over
the document. Therefore, we roughly count the
proportion of words in correct answers appear in
the document1, and we find that the proportion
of correct answers whose words appear entirely
in the article is not high in all correct answers.
The proportion show that there is a limit to us-
ing the above method to improve system perfor-
mance. Hence we used a deep learning approach
to passage representations modeling. Inspired by
(Lai et al., 2017), we use the state-of-art Gated-
Attention Reader which performs well on several
datasets. When a sample data <D, Q, A, a> is
given, the steps of the model processing this data
sample are described below, Figure 1 shows the
system.

2.3.1 Passage, Question and Choice Encoder
First, each word in D, Q, and choices (two
choices in candidate answer set A) is mapped to
d-dimendional vector. The 300-dim GloVe em-
bedding (Pennington et al., 2014) is used. For
the input word vectors of D, we also include a
5-dim binary feature to indicates the overlap be-
tween the ducument and the question(or choices)
which inspired by (Chen et al., 2017). Each di-
mension of the 5-dim binary match feature repre-
sent whether the word present in the query, in the
choice a0, in the choice a1, in both question and

1We use the following equations to estimate how many
answers appear entirely in the document: if |answer word∩
document word|/|answer word|= 1, it means the answer
appears entirely in the document, where |A| means size of set
A. Then we calculate |ansce|/|ansc|, where ansce means
correct answers which entirely appeared in document, and
ansc means correct answers. The percentage of the correct
answers entirely appeared in document is about 24%.

1049



choice a0 , in both question and choice a1, respec-
tively. Take passage as an example, we have doc-
ument D: xD1 , xD2 , ..., xDm ∈ R|D|∗dim, and next we
use bi-directional GRU to encode each document
word embedding xDi ,
−→
hDi = biGRU(

−−→
hDi−1, x

D
1 ), i = 1, 2, ...,m

←−
hDi = biGRU(

←−−
hDi+1, x

D
1 ), i = m,m− 1, ..., 1

we define hDi ∈ R2d is concatenation of the
−→
hDi

and
←−
hDi , where d is hidden size. At this time, we

get the encoded document representation De = {
hD1 , h

D
2 , ... , h

D
m }. Meanwhile, we use separate bi-

directional GRU to form representation for ques-
tion, we denote these representations as Qe = { hQ1 ,
hQ2 , ... , h

Q
n }. As for choices, we concat

−→
hCn and←−

hC1 to make up a vector represent a choice, so we
get C0 ∈ R2d and C1 ∈ R2d.
2.3.2 Summarize Question-aware Passage

Representation
The interaction layer of Gated-Attention Reader
is a l-layers multi-hop architecture with gated-
attention units. Each multi-hop layer contain a bi-
GRU and a gated-attention unit. As shown in Fig-
ure 1, we sent Qe ∈ R|Q|∗2d and De ∈ R|D|∗2d
into a gated-attention unit. Gated-attention unit
fuses information from question to each docu-
ment tokens and generates a set of vectors DGAl =
{ d(l)1 ,d

(l)
2 , ...,d

(l)
m }, where superscript (l) denote

l-th multi-hop layer. To generates DGAl , firstly,
the question soft attention to each document word
to obtain attention weight αi, and then we use
αi to calculate a weighted question representation
qi for i-th word in D, finally, the weighted ques-
tion qi representation is element-wise multiplied
by hi makes di. The specific calculation steps of a
gated-attention unit are as follows.

αi = softmax(Qehi) (1)

qi = Q
eTαi (2)

di = hi · qi (3)
After obtaining the current layer question-aware
document representation, we put this representa-
tion into next hop layer, until after l layers multi-
hops, we generate the a set of question-aware vec-
tors DGAl for document. Finally, we sent D

GA
l into a

layer biGRU and concat the last outputs of each di-

rection (
−−→
hGAl+1 and

←−−
hGAl+1) to get a ultimate question-

aware document representation vector D̃ ∈ R2d

Figure 1: Architecture of our system.

2.3.3 Answer Selection

Now, we have a question-aware representation D̃,
two choice representations C0 and C1. We es-
timate the probability that the choice selected as
the correct answer by equation (4), and the choice
with a higher-probability is returned as the predict
answer.

[p0, p1] = softmax([C0,C1]W D̃) (4)

3 Experiments

3.1 Datasets and Evaluation Metric
Table 2 shows the statistics of articles and ques-
tions in training, development, test data sets of this
task. Here “#text” and “#commonsense” repre-
sent the question types, which are unknown during
test and officially provided by organizors after test.
Therefore, we do not use the class information of
questions for system construction. Clearly, around
70% questions are from text and 30% are from
commonsense. Without the aid of additional com-
monsense knowledge base, these questions from
commonsense makes this task a huge challenge.

1050



Dataset Articles Questions
#text #commonsense #total

train 1,470 7,032 2,699 9,731
dev 219 1,006 405 1,411
test 429 2,074 723 2,797

Table 2: The statistics of data sets in training, develop-
ment and test data.

To evaluate the system performance, the official
evaluation criterion is accuracy.

3.2 Preprocessing and Experimental Setting
For rule-based baselines, we first converted words
into their lowercase and then performed tokeniza-
tion and stemming using Stanford CoreNLP2. For
deep learning system, we use 300-D pretrained
word vectors provided by GloVe3 as initial word
embedding, which are fine-tuned during training.
The encoding layer use one layer biGRU with 128-
dims hidden size to encoder texts. Learning rate
is 0.3, droprate is 0.5, epoch is 100, and num
of multi-hops is 2. We use cross entropy and
vanilla stochastic gradient descent (SGD) to train
our models.

3.3 Experiments on Training Data
Table 3 shows the results of Task 11 with different
methods on dev dataset, where “GA(biGRU)” de-
notes the final system we submit, “GA(biLSTM)”
represents the experiment that we replace all bi-
GRU units in the system with biLSTM units, “GA
−fmatch” represents the system without 5-dim
match feature, “#text” and “#commonsens” rep-
resent the accuracy under different question types,
respectively.

Methods Accuracy
#text #commonsense #total

SW + WD 62.62% 45.92% 57.83%
improved SW+WD65.01% 47.65% 60.02%
GA(biGRU) 77.33% 78.51% 77.63%
GA(biLSTM) 76.41% 77.53% 76.76%
GA −fmatch 76.34% 78.02% 76.82%

Table 3: The results on dev.

Based on above experimental results, we find
that the performance of GA system is much bet-
ter than rule-based approaches, this is because

2https://stanfordnlp.github.io/CoreNLP/
3http://nlp.stanford.edu/data/wordvecs/glove.6B.zip

the multi-hop structure merges the information of
the question and the document repeatedly which
is helpful to select final answer, unlike the rule-
based approach that considers only word match-
ing within a window-size distance. Furthermore,
we find that the improved SW + WD algorithm
is better than SW + WD algorithm, because the
improved SW + WD algorithm considers the de-
gree of word matching at different distances. From
the GA system results, we find the performance of
using biGRU units is better than that of biLSTM
units and matching features also improves the sys-
tem performance. Compare the accuracy of dif-
ferent types of questions under different methods,
we find that the rule-based approaches considers
only the word-matched features lead to lower ac-
curacy on the commonsense type questions. GA
systems perform better than rule-based systems
on both types of questions, because the GA sys-
tem takes into account the semantic similarity of
the question-aware document and choices. Fur-
ther, there are some commonsense types questions
which the document content does not clearly indi-
cate the correct answer but clearly does not meet
the meaning of wrong answer. This may be the
reason why we did not use external resources but
the accuracy of the commonsense type question
predicted by GA system is improved.

3.4 Results on Test Data

Table 4 shows the our result and official results of
top-ranked teams on SemEval 2018 Task 11 test
set.

Teamname Rank Accuracy(total)

ECNU 10 0.7311
iFLYTEK & HIT (HFL) 1 0.8413
Yuanfudao 2 0.8395
MITRE 3 0.8227

Table 4: Our result and the top three results on test sets.

The final result we submitted is generated by
GA system used biGRU units, the specific con-
figuration of which is mentioned in Section 3.2.
Compared with the top ranked systems, there is
much room for improvement in our work. In ad-
dition, the use of external knowledge resources by
the system also have an impact on system perfor-
mance because there are about 26% commonsense
type questions in the dataset. This is where our

1051



system lacks.

4 Conclusion

In this paper, we implement rule-based and deep
learning approaches to address Machine Compre-
hension Using Commonsense Knowledge task in
SemEval 2018. We explored two rule-based al-
gorithm i.e., sliding window and word distance-
based algorithm. We also utilized a deep learn-
ing method which use a multi-hop architecture
(Gated-attention Reader). The above two methods
do not use additional commonsense knowledge,
this is a point that we need to improve.

Acknowledgements

This work is is supported by the Science and
Technology Commission of Shanghai Municipal-
ity Grant (No.15ZR1410700) and the open project
of Shanghai Key Laboratory of Trustworthy Com-
puting (No.07dz22304201604).

References

Danqi Chen, Adam Fisch, Jason Weston, and An-
toine Bordes. 2017. Reading wikipedia to an-
swer open-domain questions. arXiv preprint
arXiv:1704.00051.

Bhuwan Dhingra, Hanxiao Liu, Zhilin Yang,
William W Cohen, and Ruslan Salakhutdinov.
2016. Gated-attention readers for text comprehen-
sion.

Guokun Lai, Qizhe Xie, Hanxiao Liu, Yiming Yang,
and Eduard Hovy. 2017. Race: Large-scale read-
ing comprehension dataset from examinations. In
EMNLP.

Simon Ostermann, Ashutosh Modi, Michael Roth, Ste-
fan Thater, and Manfred Pinkal. 2018a. MCScript:
A Novel Dataset for Assessing Machine Compre-
hension Using Script Knowledge. In Proceedings
of the 11th International Conference on Language
Resources and Evaluation (LREC 2018), Miyazaki,
Japan.

Simon Ostermann, Michael Roth, Ashutosh Modi, Ste-
fan Thater, and Manfred Pinkal. 2018b. SemEval-
2018 Task 11: Machine Comprehension using Com-
monsense Knowledge. In Proceedings of Interna-
tional Workshop on Semantic Evaluation (SemEval-
2018), New Orleans, LA, USA.

Jeffrey Pennington, Richard Socher, and Christopher
Manning. 2014. Glove: Global vectors for word
representation. In Proceedings of the 2014 confer-
ence on empirical methods in natural language pro-
cessing (EMNLP), pages 1532–1543.

Matthew Richardson, Christopher J. C. Burges, and
Erin Renshaw. 2013. Mctest: A challenge dataset
for the open-domain machine comprehension of
text. In EMNLP, pages 193–203. ACL.

Ellery Smith, Nicola Greco, Matko Bosnjak, and An-
dreas Vlachos. 2015. A strong lexical matching
method for the machine comprehension test. In Pro-
ceedings of the 2015 Conference on Empirical Meth-
ods in Natural Language Processing, pages 1693–
1698, Lisbon, Portugal. Association for Computa-
tional Linguistics.

1052


