




































An Encoding Strategy Based Word-Character LSTM for Chinese NER


Proceedings of NAACL-HLT 2019, pages 2379–2389
Minneapolis, Minnesota, June 2 - June 7, 2019. c©2019 Association for Computational Linguistics

2379

An Encoding Strategy Based Word-Character LSTM for Chinese NER

Wei Liu1,2, Tongge Xu3,1, Qinghua Xu1,2, Jiayu Song2, Yueran Zu2
1Hefei Innovation Research Institute, Beihang University

2School of Computer Science and Engineering, Beihang University
3School of Cyber Science and Technology, Beihang University

{liuwei1206, xutg, xuqh buaa, heavensyc, yueranzu}@buaa.edu.cn

Abstract

A recently proposed lattice model has demon-
strated that words in character sequence can
provide rich word boundary information for
character-based Chinese NER model. In this
model, word information is integrated into a
shortcut path between the start and the end
characters of the word. However, the exis-
tence of shortcut path may cause the model
to degenerate into a partial word-based model,
which will suffer from word segmentation er-
rors. Furthermore, the lattice model can not
be trained in batches due to its DAG struc-
ture. In this paper, we propose a novel word-
character LSTM(WC-LSTM) model to add
word information into the start or the end char-
acter of the word, alleviating the influence of
word segmentation errors while obtaining the
word boundary information. Four different
strategies are explored in our model to en-
code word information into a fixed-sized rep-
resentation for efficient batch training. Ex-
periments on benchmark datasets show that
our proposed model outperforms other state-
of-the-arts models.

1 Introduction

Name Entity Recognition(NER) is a basic task
of many NLP systems including Information
Retrieval (Virga and Khudanpur, 2003), Rela-
tionship Extraction (Miwa and Bansal, 2016),
Question Answering (Mollá et al., 2006). The
main task of NER is to identify named enti-
ties such as person, location, organization, etc.
in given text. Various methods have been pro-
posed to tackle this problem, including Hid-
den Markov Models(HMMs) (Saito and Nagata,
2003), Maximum Entropy Models(ME) (Chieu
and Ng, 2003), Support Vector Machines(SVM)
(Ekbal and Bandyopadhyay, 2010) and Condi-
tional Random Fields(CRF) (Feng et al., 2006).
With the development of deep learning, neural net-

c c c c c c

	

Up

§
Rise


of

3
Water

^
River

~
Long

	§
Rise

^3
River Water

~^
Yangtze River

w w w

O O O B-LOC M-LOC E-LOC

O O O B-LOC E-LOC O

(a) Original lattice model

c c c c c c

	

Up

§
Rise


of

3
Water

^
River

~
Long

	§
Rise

^3
River Water

~^
Yangtze River

w w w

O O O B-LOC M-LOC E-LOC

O O O B-LOC E-LOC O

(b) Degraded lattice model in extreme cases

Figure 1: An example of the lattice model degen-
erates into a partial word-based model. Due to the
shortcut path ”江(River)” → ”江水(River Water)”
→ ”水(Water)”, the model incorrectly predicts that
”江(River)” and ”水(Water)” belong to the same entity.
Red labels(without underline) denote predicted labels,
and blue labels(with underline) denote gold labels.

works (Huang et al., 2015; Lample et al., 2016;
Habibi et al., 2017) have been introduced to NER
task. To avoid the segmentation errors, most of
neural Chinese NER models are character-based.

Although character-based method has achieved
good performance, it does not exploit word in-
formation in character sequence. Entity bound-
aries usually coincide with some word bound-
aries, which suggests that words in character se-
quence can provide rich boundary information for
character-based model. To integrate words in-
formation into character-based model, Zhang and
Yang (2018) propose a lattice-structured LSTM



2380

model to encode a sequence of input characters
as well as all potential words that match a lex-
icon. Their model is an extension of character-
based LSTM-CRF model and uses extra ”shortcut
paths” to link the memory cell between the start
and the end characters of a word for utilizing word
information. And the gated recurrent unit is used
to control the contribution of shortcut paths and
path between adjacent characters. However, as
the study of (Yang et al., 2018) shown, the gate
mechanism fails to choose the right path some-
times. As shown in Figure 1, wrong choices may
cause lattice model to degenerate into a partial
word-based model, which suffers from word seg-
mentation errors. In addition, due to the variable
length of words, the length of the whole path is
not fixed. Besides, each character is bounded with
a variable-sized candidate word sets, which means
the amount of incoming and outcoming paths is
not fixed either. In this case, lattice LSTM model
is deprived of the power of batch training, and
hence it is highly inefficient.

To address the above problems, we propose
a novel word-character LSTM(WC-LSTM) to
integrate word information into character-based
model. To prevent our model from degenerating
into a partial word-based model, we assign word
information to a single character and ensure that
there are no shortcut paths between characters.
Specifically, word information is assigned to its
end character and start character in forward WC-
LSTM and backward WC-LSTM respectively. We
introduce four strategies to extract fixed-sized use-
ful information from different words, which en-
sures that our proposed model can perform batch
training without losing word information.

We demonstrate the effectiveness of our archi-
tecture on four widely used datasets. Experimental
results show that our proposed model outperforms
other state-of-the-art models on the four datasets.

Our contributions of this paper can be con-
cluded as follows:

• We propose a novel word-character
LSTM(WC-LSTM) to incorporate word
information into character-based model.

• We explore four different strategies to en-
code word information into a fixed-sized vec-
tor, which enables our proposed model to be
trained in batches and adapted to various ap-
plication scenarios.

• Our proposed model outperforms other mod-
els and achieves new state-of-the-art over
four Chinese NER datasets. We release the
source code for further research1.

2 Related Work

Neural Networks have been shown to achieve im-
pressive results on Name Entity Recognition task
(Gregoric et al., 2018; Lin and Lu, 2018). Based
on the level of granularity, most of the models can
be divided into three categories: word-based mod-
els, character-based models, and hybrid models.

Word-Based Models. Collobert and Weston
(2008) propose one of the first word-based mod-
els for NER, with feature constructed from ortho-
graphic features, dictionaries and lexicons (Yadav
and Bethard, 2018). Collobert et al. (2011) replace
the hand-crafted features with word embeddings.
Huang et al. (2015) propose a BiLSTM-CRF
model for NER and achieves good performance.
Ma and Hovy (2016) and Chiu and Nichols (2016)
use CNN to capture spelling characteristics and
Lample et al. (2016) use LSTM instead. When
applied to Chinese NER, the above models all suf-
fer from segmentation errors, since Chinese word
segmentation is compulsory for those models.

Character-Based Models. Peng and Dredze
(2015) propose to add segmentation features for
better recognition of entity boundary. Dong
et al. (2016) integrate radical-level features into
character-based model. To eliminate the ambigu-
ity of character, Sun and He (2017) take the posi-
tion of character into account. Although the above
models have achieved good results, they all ignore
word information in character sequence.

Hybrid Models. Some efforts have been
made to integrate word boundary information into
character-based models. Motivated by the suc-
cess of multi-task learning for Natural Language
Processing (Liu et al., 2016, 2017; Zhang et al.,
2018), Peng and Dredze (2016) first proposed
to jointly train Chinese NER with Chinese word
segmentation(CWS) task. Cao et al. (2018) ap-
ply adversarial transfer learning framework to in-
tegrate the task-shared word boundary informa-
tion into Chinese NER task. Another way to ob-
tain word boundary information is proposed by
(Zhang and Yang, 2018), using a lattice LSTM
to integrate word information into character-based
model, which is similar to what is proposed in

1https://github.com/liuwei1206/CCW-NER



2381

	
Up

§
Rise


Of

~
Long

^
River

3
Water

D5
Ö

D6
Ö

D7
Ö

D:
Ö

D8
Ö

D9
Ö

T5
Ö

T6
Ö

T7
Ö

T:
Ö

T8
Ö

T9
Ö

T5
êæ

T6
êæ

T7
êæ

T:
êæ

T8
êæ

T9
êæ

~^
Yangtze River

	§
Rise

<PAD> <PAD> <PAD>
^3

River Water

Tã
ê

Stgy

T65
ê

Stgy

Tã
ê

Stgy

Tã
ê

Stgy

T95
ê

Stgy

T:5
ê

Stgy

15
Ö

16
Ö

17
Ö

1:
Ö

18
Ö

19
Ö

O O O B-LOC E-LOC O

S S S S S S

Word emb

Char emb

Strategy

WC-LSTM

CRF

Figure 2: The architecture of our unidirectional model. The blue part can be seen as a standard character-based
model but with a word-character LSTM(WC-LSTM), and the red part indicates the process of encoding word
information into a fixed-size representation. Word information is integrated into the end character of the word.
Where ”<PAD>” denotes padding value; ”Stgy” denotes a certain encoding strategy and⊕ denotes concatenation
operation.

this paper. The main differences are as follows.
Firstly, they exploit word information by a DAG-
structured LSTM, while we use a chain-structured
LSTM. Secondly, instead of integrating to the hid-
den state of LSTM, our model add word informa-
tion into the input vector. Finally, our model can
be trained in batches and is more efficient.

3 Method

The architecture of our proposed model is shown
in Figure 2. Same as the widely used neural
Chinese NER model, we use LSTM-CRF as our
main network structure. The differences between
our model and a standard LSTM-CRF model are
mainly on the embedding layer and LSTM and can
be summarized as follows. First, we represent a
Chinese sentence as a sequence of character-words
pairs to integrate word information into each char-
acter. Second, to enable our model to train in
batches and to meet different application require-
ments, we introduce four encoding strategies to
extract fixed-sized but different information from
words. Finally, a chain-structured word-character
LSTM is used to extract features from both char-
acter and word for better predicting.

Next, we will explain the main ideas for each
component, including word-character embedding
layer, word encoding strategy, and word-character
LSTM.

Formally, we denote a Chinese sentence as s =

{c1, c2, ..., cn}, where ci denotes the ith charac-
ter. We use cb,e to denote a character subsequence
in s, which begins with bth character and ends
with eth character. Take the sentence in Figure 2
for example, c1,2 is ”上涨(Rise)”. We use −→wsi
to denote words assigned to ith character in for-
ward WC-LSTM, which are a set of character sub-
sequences cb,i, where b < i and cb,i matches a
word in lexicon D. The lexicon D is the same as
the one used in (Zhang and Yang, 2018), which
is built by using automatically segmented large
raw text. Similarly, we use ←−wsi to denote the
words for ith character in backward WC-LSTM,
which are a set of character subsequences ci,e,
where e > i and ci,e matches a word in lexi-
con D. Finally, the sentence s is represented as
−→rs = {(c1,−−→ws1), (c2,−−→ws2), ..., (cn,−−→wsn)} in our
model, and its reverse representation is ←−rs =
{(cn,←−−wsn), (cn−1,←−−−−wsn−1), ..., (c1,←−−ws1)}.

3.1 Word-Character Embedding Layer

In our model, Each position i in −→rs consists of
two parts: ith character ci and the assigned words−→wsi. The origin number of words in −→wsi is sti, and
words are sorted by their length. We ensure each
−→wsi has the same number spi 2 in the whole batch
by padding. We embed each character ci in dis-

2The number depends on the maximum sti in the whole
batch, and it can not be less than 1.



2382

tributional space as xci :

xci = e
c(ci) (1)

where ec denotes a pre-trained character embed-
ding lookup table. Similarly, for each −→wsi =
{−→wi1, ...,−−→wispi }, the lth word

−→wil in −→wsi is repre-
sented using

x
−→w
il = e

w(−→wil) (2)

where ew denotes a pre-trained word embedding
lookup table. As a result, the distributional repre-
sentation of words −→wsi is {x

−→w
i1 , ...,x

−→w
ispi
}.

3.2 Words Encoding Strategy
Although the number of assigned words spi for
each character ci is same in one batch, the num-
ber varies from batch to batch. As a result, the
size of input to the model is not fixed, which is
not conducive to batch training. To acquire fixed-
sized input, we introduce four different encoding
strategies in this section. And we use x

−→ws
i to de-

note the final representation of word information
for position i in following sections.

Shortest Word First: For each word set−→wsi =
{−→wi1, ...,−−→wispi }, we simply select word whose
length is the shortest, i.e. −→wi1. Then

x
−→ws
i = x

−→w
i1 (3)

Longest Word First: Contrary to the short-
est word first, we select word whose length is the
longest, i.e. −−→wisti . Note that s

t
i may be 0, in this

case, we set it to 1. Then

x
−→ws
i = x

−→w
isti

(4)

Average: While the first two strategies can only
use the information of partial words, we intro-
duce an average strategy to utilize all word infor-
mation. As its name indicates, the average strat-
egy computes the centroid of the embeddings of
all elements except paddings in word set , i.e.
{−→wi1, ...,−−→wisti}. If s

t
i = 0, we simply average all

the padding value in the word set. Then

x
−→ws
i =


1
sti

∑sti
l=1 x

−→w
il , if s

t
i > 0

1
spi

∑spi
l=1 x

−→w
il , if s

t
i = 0

(5)

Self-Attention: Inspired by self-attention
mechanism applied to sentence embedding (Lin
et al., 2017), we exploit self-attention to better

capture useful information from assigned words.
For simplicity, we denote all the x

−→w
il as Wi, which

has the size spi -by-d
w, where dw denotes the di-

mensionality of word embedding ew.

Wi = (x
−→w
i1 , ...,x

−→w
ispi

) (6)

We use self-attention mechanism to obtain a linear
combination of spi word embeddings in Wi. The
attention mechanism takes Wi as input, and gen-
erates a weight vector ai.

ai = softmax(w2tanh(W1W
T
i )) (7)

W1 is a weight matrix with the size of da-by-dw

and w2 is a da dimensional vector, where da is
a hyperparameter. Both of them are trainable pa-
rameters.

If sti > 0, we use the mask to exclude the
padding values; otherwise we reserve them. Fi-
nally, we use ai to get the weighted sum of all
words.

x
−→ws
i =

{∑sti
l=1 ailx

−→w
il , if s

t
i > 0∑spi

l=1 ailx
−→w
il , if s

t
i = 0

(8)

where ail denotes the lth value in ai.

3.3 Word-Character LSTM(WC-LSTM)
Inspired by the way character bigram is integrated
into sequence labeling model (Chen et al., 2015;
Yang et al., 2017), we concatenate each xci with
x
−→ws
i to utilize word information. And this is quite

different from the way used in (Zhang and Yang,
2018), since they use extra shortcut paths to in-
tegrate word information into the hidden layer of
LSTM. By concatenating, there is no shortcut path
in our model and information can only flow be-
tween adjacent characters, which ensures that our
model will not degenerate into a partial word-
based model. Then the WC-LSTM functions are:


c̃i
oi
ii
fi

 =

tanh
σ
σ
σ

(Wp [ xihi−1
]
+ bp

)
(9)

xi = x
c
i ⊕ x

−→ws
i

ci = c̃i � ii + ci−1 � fi
hi = oi � tanh(ci)

(10)

where oi, ii and fi denote output gate, input gate
and forget gate respectively. Wp and bp are pa-
rameters of affine transformation; σ denotes the



2383

logistic sigmoid function; ⊕ denotes concatena-
tion operation and � denotes elementwise multi-
plication.

The bidirectional WC-LSTM is applied in our
model to leverage both information from the
past and the future. To get the future infor-
mation, we use a second WC-LSTM that reads
the reverse representation of −→rs, i.e., ←−rs =
{(cn,←−−wsn), (cn−1,←−−−−wsn−1), ..., (c1,←−−ws1)}. And
the following operations to get each backward
WC-LSTM hidden vector

←−
hi is the same as the one

in the forward WC-LSTM. Finally, the update of
each bidirectional WC-LSTM unit can be written
as follows:

−→xi = xci ⊕ x
−→ws
i

←−xi = xci ⊕ x
←−ws
i

−→
hi =

−−−−−−−−−→
WC− LSTM(

−−→
hi−1,

−→
xi)

←−
hi =

←−−−−−−−−−
WC− LSTM(

←−−
hi+1,

←−
xi)

hi =
−→
hi ⊕

←−
hi

(11)

where
−→
hi and

←−
hi are hidden states at position i

of forward and backward WC-LSTM respectively,
and ⊕ denotes concatenation operation.

3.4 Decoding and Training
Considering the dependencies between successive
labels, we use a CRF layer to make sequence tag-
ging. We define matrix O to be scores calculated
based on the output H = {h1,h2, ...,hn}:

O = WoH+ bo (12)

For a label sequence y = {y1, y2, ..., yn}, we de-
fine its probability to be:

p(y|s) =
exp

(∑
i

(
Oi,yi +Tyi−1,yi

))∑
ỹ exp

(∑
i

(
Oi,ỹi +Tỹi−1,ỹi

))
(13)

Where Wo and bo are paramters to calculate O;
T is a transition score matrix and ỹ denotes all
possible tag sequences.

While decoding, we use the Viterbi algorithm to
find the label sequences that obtained the highest
score:

y∗ = argmax
y∈ỹ

∑
i

(
Oi,yi +Tyi−1,yi

)
(14)

Given N manually labeled data {(sj ,yj)}|Nj=1,
we minimize the sentence-level negative log-
likelihood loss to train the model:

L = −
∑
j

log(p(yj |sj)) (15)

Dataset Train sent Dev sent Test sent
OntoNotes 15724 4301 4346

MSRA 46364 - 4365
Weibo NER 1350 270 270

Chinese resume 3821 463 477

Table 1: Statistics of the datasets

4 Experiments

4.1 Experimental Settings

Dataset. We evaluate our model on four datasets,
including OntoNotes4 (Weischedel et al., 2011),
MSRA (Levow, 2006), Weibo NER (Peng and
Dredze, 2015) and a Chinese resume dataset
(Zhang and Yang, 2018). Both OntoNotes4 and
MSRA datasets are news in simplified Chinese.
Weibo NER dataset is social media data, which
is drawn from the Sina Weibo. Chinese resume
dataset consists of resumes of senior executives,
which is annotated by (Zhang and Yang, 2018).
For OntoNotes, we use the same training, devel-
opment and test splits as (Che et al., 2013). For
other datasets which have already been split, and
we don’t change them. We summarize the datasets
in Table 1.

Implementation Details. We utilize the char-
acter and word embeddings used in (Zhang and
Yang, 2018), both of which are pre-trained on Chi-
nese Giga-Word using word2vec model. Follow-
ing (Zhang and Yang, 2018), we use the word em-
bedding dictionary as Lexicon D in our model.
For characters and words that do not appear in the
pretrained embeddings, we initialize them with a
uniform distribution3. When training the model,
character embeddings and word embeddings are
updated along with other parameters.

For hyper-parameter configurations, we mostly
refer to the settings in (Zhang and Yang, 2018).
We set both character embedding size and word
embedding size to 50. The dimensionality of each
unidirectional multi-input LSTM hidden states is
100 for Weibo NER and Chinese Resume, and
200 for OntoNote 4 and MSRA. For self-attention
strategy, we set the da to 50. To avoid overfitting,
we apply dropout to both embeddings and LSTM
with a rate of 0.5. We use SGD to optimize all
the trainable parameters. Learning rate is set to
0.015 initially and decays during training at a rate

3The range is
[
−
√

3
dim

,+
√

3
dim

]
, where dim demotes

the size of embedding.



2384

Input Models P R F1

Gold seg
Wang et al. (2013) 76.43 72.32 74.32
Che et al. (2013) 77.71 72.51 75.02
Yang et al. (2016) 65.59 71.84 68.57

No seg

Lattice (Zhang and Yang, 2018) 76.35 71.56 73.88
Character baseline 70.08 60.53 64.95
WC-LSTM + shortest 76.39 72.39 74.34
WC-LSTM + longest 75.62 72.76 74.16
WC-LSTM + average 76.04 72.03 73.98
WC-LSTM + self-attention 76.09 72.85 74.43

Table 2: Results on OntoNotes

of 0.05.
For evaluation, we use the Precision(P), Re-

call(R) and F1 score as metrics in our experiments.

4.2 Experimental Results

OntoNotes. Table 2 shows the experimental re-
sults on OntoNote 4 dataset. The ”Input” column
shows the representation of input sentence, where
”Gold seg” means a sequence of words with gold-
standard segmentation, and ”No seg” means a se-
quence of character without any segmentation.

The first block in Table 2 are the results of word-
based models (Wang et al., 2013; Che et al., 2013;
Yang et al., 2016). By using gold-standard seg-
mentation and external labeled data, all of them
achieve good performance. But the only resource
used in our model are pretrained character and
word embeddings.

The first two rows in the second block show the
performance of the lattice model and character-
based model. The character baseline denotes
the original character-based BiLSTM-CRF model.
Zhang and Yang (2018) propose a lattice LSTM
to exploit word information in character sequence,
giving the F1 score of 73.88%. Compared with
the character baseline, lattice model gains 8.92%
improvement in F1 score, which shows the impor-
tance of word information in character sequence.

In the last four rows, we list the results of our
proposed model. The results show that all of our
models outperform other character-based models,
and the one with self-attention strategy achieves
the best result. Without gold-standard segmen-
tation and external labeled data, our model gives
competitive results to the word-based models on
this dataset. Compared with the character base-
line, our model with self-attention obtains 9.48%
improvement in F1 score, which proves the effec-
tiveness of our way to integrating word informa-
tion. Compared with lattice model, all of our mod-
els achieve better results, which shows that our

Models P R F1
Zhang et al. (2006) 92.20 90.18 91.18
Zhou et al. (2013) 91.86 88.75 90.28
Dong et al. (2016) 91.28 90.62 90.95
Cao et al. (2018) 91.73 89.58 90.64
Lattice (Zhang and Yang, 2018) 93.57 92.79 93.18
Character baseline 89.61 86.98 88.37
WC-LSTM + shortest 93.97 92.59 93.28
WC-LSTM + longest 94.33 93.11 93.71
WC-LSTM + average 94.58 92.91 93.74
WC-LSTM + self-attention 94.36 92.38 93.36

Table 3: Results on MSRA

Models NE NM Overall
Peng and Dredze (2015) 51.96 61.05 56.05
Peng and Dredze (2016) 55.28 62.97 58.99
Sun and He (2017) 54.50 62.17 58.23
He and Sun (2017) 50.60 59.32 54.82
Cao et al. (2018) 54.34 57.35 58.70
Lattice (Zhang and Yang, 2018) 53.04 62.25 58.79
Character baseline 47.98 57.94 52.88
WC-LSTM + shortest 52.99 65.75 59.20
WC-LSTM + longest 52.55 67.41 59.84
WC-LSTM + average 53.19 64.17 58.67
WC-LSTM + self-attention 49.86 65.31 57.51

Table 4: Results on Weibo NER

approach to integrating word information is more
reasonable than lattice model.

MSRA. Table 3 shows the results on MSRA
dataset. Zhang et al. (2006) and Zhou et al. (2013)
use the statistical model with rich hand-crafted
features. Dong et al. (2016) exploit radical fea-
tures in Chinese character. Cao et al. (2018) joint
train Chinese NER task with Chinese word seg-
mentation, in which adversarial learning and self-
attention mechanism are applied for better perfor-
mance. We can observe that our proposed mod-
els outperformance the above models and the one
with average strategy achieves new state-of-the-art
performance.

Weibo. Table 4 shows the results4 on Weibo
dataset. The ”NE”, ”NM” and ”Overall” columns
denote F1-score for named entities, nominal en-
tities(excluding named entities) and both respec-
tively. We can see that WC-LSTM model with
longest word first strategy achieves new state-of-
the-art performance. Multi-task learning (Peng
and Dredze, 2015, 2016; Cao et al., 2018) and
semi-supervised learning (Sun and He, 2017; He
and Sun, 2017) are the most common methods

4The results of (Peng and Dredze, 2015, 2016) are taken
from (Peng and Dredze, 2017)



2385

Models P R F1
Lattice (Zhang and Yang, 2018) 94.81 94.11 94.46
Character baseline 93.26 93.44 93.35
WC-LSTM + shortest 94.97 94.91 94.94
WC-LSTM + longest 95.27 95.15 95.21
WC-LSTM + average 95.09 94.97 95.03
WC-LSTM + self-attention 95.14 94.79 94.96

Table 5: Results on Chinese Resume

Time(s)/epoch
Character baseline (batch size=1) 880
Character baseline (batch size=8) 253
Lattice 2245
WC-LSTM (batch size=1) 980
WC-LSTM (batch size=8) 350

Table 6: Time per epoch of models

for Weibo NER task due to the small amount of
training data. All of the above models require
additional cross-domain or semi-supervised data.
Compared with those models, our model does not
need additional labeled data; we only exploit pre-
trained character and word embeddings.

Resume. Table 5 shows the results on Chinese
Resume dataset. Consistent with the previous re-
sults, our models outperform lattice model (Zhang
and Yang, 2018). The above experimental results
strongly verify that our method to utilize word in-
formation is more effective than the lattice model.

Our proposed model has achieved state-of-the-
art results on various domains such as news, so-
cial media, and Chinese resume.

4.3 Efficiency

To further explore the efficiency of our model, we
conduct some comparative experiments on train-
ing time and convergence speed. The lattice
model proposed in (Zhang and Yang, 2018) is
our principal comparison object, since it also uti-
lizes the word information in character sequence.
Our model is an extension of the character-based
model, so we also report the results on character-
based model as character baseline. We only con-
duct our experiments on OnteNotes dataset due to
space limitation. And we choose the model with
the self-attention strategy for the comparative ex-
periments, as it outperforms other strategies on
OntoNotes dataset.

The training time of each epoch for all models
is shown in Table 6. The lattice model needs the
most training time for each epoch, since it can only

Figure 3: Convergence curve of models. Our model
can converge within the same epochs as lattice model
does. ”1” and ”8” denotes batch size. Lattice model
can only be trained with batch size=1 due to its DAG
structure.

be trained with batch size=1 due to its complex
DAG structure. Compared with it, our model with
batch size=1 only need half of the training time.
Which shows that our model is more efficient.
With batch size=8, our model is nearly 6 times
faster than the lattice model, which further demon-
strates the efficiency of our model. Compared with
the character baseline, our model only adds a small
amount of training time but greatly improves the
performance. All the experiments are conducted
on a single GPU with NVIDIA Tesla K40m.

Figure 3 shows the learning curve of the mod-
els in Table 6. As we can see from the figure,
whether with batch size=1 or 8, our model can
converge within the same epochs as lattice model
does. But compared with the lattice model, our
model with batch size=8 only takes about 1/7 of
their training time per epoch. Besides, we can ob-
serve from Figure 3, both our model and lattice
model significantly outperform the character base-
line, which shows the importance of the word in-
formation again.

4.4 Detailed Analysis

Case Study. Word information is very useful
for Chinese NER task, since it can provide rich
word boundary information. To verify that our
model can better utilize the boundary information,
we analyze an example from OntoNotes dataset.
As shown in Table 7, the character-based model
cannot detect the existence of the entity ”东北
亚(Northeast Asia)” without word information.
The lattice model incorrectly recognizes ”东北亚



2386

Sentence 新的 东北亚 大陆桥
(truncated) New Northeast Asian Continental Bridge

Latent words
东北,东北亚,北亚,亚大,亚大陆,大陆,大陆桥,陆桥

Northest, Northeast Asia, North Asia, Second largest, Subcontinent,
Continent, Continental bridge, Land bridge

Gold labels
新 的 东 北 亚 大 陆 桥

O O B-LOC M-LOC E-LOC O O O

Character
新 的 东 北 亚 大 陆 桥

O O O O O O O O

Lattice
新 的 东 北 亚 大 陆 桥

O O B-LOC M-LOC M-LOC M-LOC M-LOC E-LOC

Shortest
新 的 东 北 亚 大 陆 桥

O O B-LOC M-LOC E-LOC O O O

Longest
新 的 东 北 亚 大 陆 桥

O O B-LOC M-LOC E-LOC O O O

Average
新 的 东 北 亚 大 陆 桥

O O B-LOC M-LOC E-LOC O O O

Self-attention
新 的 东 北 亚 大 陆 桥

O O B-LOC M-LOC E-LOC O O O

Table 7: An example of that our models can mitigate
the influence of wrong boundary information while uti-
lizing word information. ”Latent words” denotes all
words in character sequences; ”Character” denotes the
character-based model; ”Lattice” denotes lattice model
and the last four rows are our models with different en-
coding strategies.

大陆桥(Northeast Asian Continental Bridge)” as
an entity, which is caused by the wrong selection
of paths. Different from the lattice model, our
models are not disturbed by the wrong boundary
information and make the correct predictions.
Strategies Analysis. In this part, we analyze
the difference between strategies. The applica-
tion scenarios of shortest word first and longest
word first can be explained by Nested Name Entity
Recognition (Ju et al., 2018; Sohrab and Miwa,
2018). Short word first is good at identifying in-
ner nested entities due to the short word infor-
mation, while longest word first tends to identify
flat entities with the help of long word informa-
tion. Taking ”长江三角洲(Yangtze River Delta)”
as an example, shortest word first recognizes ”长
江(Yangtze)” and ”三角洲(Delta)” as entities, but
longest word first tend to think that they are part
of the entity ”长江三角洲(Yangtze River Delta)”.
Both results are reasonable, but the right result de-
pends on specific needs.

The average and self-attention strategies are the
combination of all words information and can use
more information. Intuitively, they should outper-
form the shortest word first and the longest word
first. But results on Weibo NER(Table 4) and Re-
sume(Table 5) show the opposite effect. We con-
jecture that this is caused by the small amount
of training data since more word information but
small dataset will lead to overfitting. The average
strategy is a special case of the self-attention strat-

Sentence 房维中经叔平
(truncated) Fang Weizhong and Jing Shuping

Latent words
房维,中经,经叔平,经叔,叔平

Fang Wei, Zhongjing, Jing Shuping, Jingshu, Shuping

Gold labels
房 维 中 经 叔 平

B-PER M-PER E-PER B-PER M-PER E-PER

Average
房 维 中 经 叔 平

B-PER M-PER E-PER B-PER M-PER E-PER

Self-attention
房 维 中 经 叔 平

B-PER M-PER M-PER E-PER B-PER E-PER

Table 8: An example of our model applied to infor-
mal text using the average strategy and self-attention
strategy. ”Latent words” denotes all words in character
sequences.

egy where all weights are the same, so we would
like to see the latter outperforms the former when
training data is sufficient. Surprisingly, the aver-
age strategy achieves higher F1 score than the self-
attention strategy in MSRA dataset(Table 3). We
carefully analyze the experimental results and find
that there are a large number of informal texts in
the MSRA test set. Specifically, the MSRA test
set contains some very long sentences, in which
there are a series of Chinese person name without
delimiter. As shown in Table 8, when applied to
such informal text, the self-attention strategy fails
to determine the entity boundary sometimes while
the average strategy correctly recognizes the enti-
ties. And we conjecture that, with more trainable
parameters, the self-attention strategy can better fit
the formal text in the training set but cannot adapt
well to the informal data in the test set, so it per-
forms worse than the average strategy.

Finally, the application scenarios of different
strategies can be summarized as followings. If the
training data is sufficient, we recommend using
self-attention for formal texts and average strategy
for informal texts. If there is only a very small
amount of annotated data, we recommend using
the shortest words first for inner nested entities and
longest word first strategy for flat entities.
Lexicon and Embeddings. To further analyze
the contribution from word lexicon and pretrained
word embeddings, we conduct some compara-
tive experiments by using the same word lexi-
con with and without pretrained embeddings. We
choose the strategy that achieving the best per-
formance for each dataset. We estimate the con-
tribution of the lexicon by replacing pretrained
word embeddings with randomly initialized em-
beddings5. As shown in Table 9, both lexicon

5Same initialization strategy as in ”Implement Details”.



2387

OntoNotes Resume MSRA Weibo
Character baseline 64.95 93.35 88.37 52.88
WC-LSTM + init 67.81(+2.86) 94.51(+1.16) 90.68(+2.31) 54.94(+2.06)
WC-LSTM + pretrain 74.43(+9.48) 95.21(+1.86) 93.74(+5.37) 59.84(+6.96)

Table 9: Comparison F1 scores between our proposed
model with and without pretrained word embeddings.
Where ”init” and ”pretrain” denote without and with
pretrained embeddings respectively. ”+” denotes the
boost value to baseline.

and pretrained word embeddings are useful to our
model. However, different from the result in lat-
tice model(Yang et al., 2018), pretrained word
embeddings contribute more than lexicon to our
model. Taking the result on Ontonote for example,
the contribution of pretrained embeddings can be
estimated as (9.48%− 2.86%) = 6.62%, which is
higher than the contribution of lexicon 2.86%. The
results show that our model relies more on pre-
trained embeddings instead of the lexicon, which
explains the excellent performance of our model
in different domains.

5 Conclusion and Future Work

In this paper, we propose a novel method to utilize
word information in character sequence for Chi-
nese NER. Four encoding strategies are introduced
to extract fixed-sized but different information for
batch training. By using WC-LSTM to extract
features from the character vector and word vec-
tor, our model can effectively exploit word bound-
ary information and mitigate the influence of word
segmentation errors. Experiments on datasets in
different domains show that our model is more ef-
ficient and faster than the lattice model and also
outperforms other state-of-the-art models.

In the future, we plan to further improve and
perfect the proposed method, such as exploring
some strategies to handle OOV words. Also, the
proposed methods can be further extended to other
Chinese NLP tasks, such as CWS, Text Classifica-
tion, and Sentiment Analysis.

Acknowledgments

The research work is supported by the No.
BHKX-17-07 project of Hefei Innovation Re-
search Institute, Beihang University. We would
like to thank Jie Yang for his open-source toolkit
”NCRF++” (Yang and Zhang, 2018). Moreover,
we sincerely thank all anonymous reviewers for
their valuable comments.

References
Pengfei Cao, Yubo Chen, Kang Liu, Jun Zhao, and

Shengping Liu. 2018. Adversarial transfer learn-
ing for chinese named entity recognition with self-
attention mechanism. In Proceedings of the 2018
Conference on Empirical Methods in Natural Lan-
guage Processing, Brussels, Belgium, October 31 -
November 4, 2018, pages 182–192.

Wanxiang Che, Mengqiu Wang, Christopher D. Man-
ning, and Ting Liu. 2013. Named entity recog-
nition with bilingual constraints. In Human Lan-
guage Technologies: Conference of the North Amer-
ican Chapter of the Association of Computational
Linguistics, Proceedings, June 9-14, 2013, Westin
Peachtree Plaza Hotel, Atlanta, Georgia, USA,
pages 52–62.

Xinchi Chen, Xipeng Qiu, Chenxi Zhu, Pengfei Liu,
and Xuanjing Huang. 2015. Long short-term mem-
ory neural networks for chinese word segmentation.
In Proceedings of the 2015 Conference on Empirical
Methods in Natural Language Processing, EMNLP
2015, Lisbon, Portugal, September 17-21, 2015,
pages 1197–1206.

Hai Leong Chieu and Hwee Tou Ng. 2003. Named en-
tity recognition with a maximum entropy approach.
In Proceedings of the Seventh Conference on Natu-
ral Language Learning, CoNLL 2003, Held in coop-
eration with HLT-NAACL 2003, Edmonton, Canada,
May 31 - June 1, 2003, pages 160–163.

Jason P. C. Chiu and Eric Nichols. 2016. Named en-
tity recognition with bidirectional lstm-cnns. TACL,
4:357–370.

Ronan Collobert and Jason Weston. 2008. A unified
architecture for natural language processing: deep
neural networks with multitask learning. In Ma-
chine Learning, Proceedings of the Twenty-Fifth In-
ternational Conference (ICML 2008), pages 160–
167.

Ronan Collobert, Jason Weston, Léon Bottou, Michael
Karlen, Koray Kavukcuoglu, and Pavel P. Kuksa.
2011. Natural language processing (almost) from
scratch. Journal of Machine Learning Research, 12.

Chuanhai Dong, Jiajun Zhang, Chengqing Zong,
Masanori Hattori, and Hui Di. 2016. Character-
based LSTM-CRF with radical-level features for
chinese named entity recognition. In In Natural
Language Understanding and Intelligent Applica-
tions, pages 239–250.

Asif Ekbal and Sivaji Bandyopadhyay. 2010. Named
entity recognition using support vector machine:
A language independent approach. International
Journal of Electrical and Electronics Engineering,
4(2):155–170.

Yuanyong Feng, Le Sun, and Yuanhua Lv. 2006. Chi-
nese word segmentation and named entity recogni-
tion based on conditional random fields models. In



2388

Proceedings of the Fifth SIGHAN Workshop on Chi-
nese Language Processing, pages 181–184.

Andrej Zukov Gregoric, Yoram Bachrach, and Sam
Coope. 2018. Named entity recognition with par-
allel recurrent neural networks. In Proceedings of
the 56th Annual Meeting of the Association for Com-
putational Linguistics, ACL 2018, Melbourne, Aus-
tralia, July 15-20, 2018, Volume 2: Short Papers,
pages 69–74.

Maryam Habibi, Leon Weber, Mariana L. Neves,
David Luis Wiegandt, and Ulf Leser. 2017. Deep
learning with word embeddings improves biomed-
ical named entity recognition. Bioinformatics,
33(14):i37–i38.

Hangfeng He and Xu Sun. 2017. A unified model
for cross-domain and semi-supervised named entity
recognition in chinese social media. In Proceedings
of the Thirty-First AAAI Conference on Artificial In-
telligence, February 4-9, 2017, San Francisco, Cal-
ifornia, USA., pages 3216–3222.

Zhiheng Huang, Wei Xu, and Kai Yu. 2015. Bidirec-
tional lstm-crf models for sequence tagging. Com-
puting Research Repository, arXiv:1508.01991.

Meizhi Ju, Makoto Miwa, and Sophia Ananiadou.
2018. A neural layered model for nested named en-
tity recognition. In Proceedings of the 2018 Confer-
ence of the North American Chapter of the Associ-
ation for Computational Linguistics: Human Lan-
guage Technologies, NAACL-HLT 2018, New Or-
leans, Louisiana, USA, June 1-6, 2018, Volume 1
(Long Papers), pages 1446–1459.

Guillaume Lample, Miguel Ballesteros, Sandeep Sub-
ramanian, Kazuya Kawakami, and Chris Dyer. 2016.
Neural architectures for named entity recognition.
In The 2016 Conference of the North American
Chapter of the Association for Computational Lin-
guistics: Human Language Technologies, pages
260–27.

Gina-Anne Levow. 2006. The third international chi-
nese language processing bakeoff: Word segmen-
tation and named entity recognition. In Proceed-
ings of the Fifth Workshop on Chinese Language
Processing, SIGHAN@COLING/ACL 2006, Sydney,
Australia, July 22-23, 2006, pages 108–117.

Bill Yuchen Lin and Wei Lu. 2018. Neural adapta-
tion layers for cross-domain named entity recogni-
tion. In Proceedings of the 2018 Conference on
Empirical Methods in Natural Language Process-
ing, Brussels, Belgium, October 31 - November 4,
2018, pages 2012–2022.

Zhouhan Lin, Minwei Feng, Cı́cero Nogueira dos San-
tos, Mo Yu, Bing Xiang, Bowen Zhou, and Yoshua
Bengio. 2017. A structured self-attentive sentence
embedding. In International Conference on Learn-
ing Representations 2017.

Pengfei Liu, Xipeng Qiu, and Xuanjing Huang. 2016.
Deep multi-task learning with shared memory for
text classification. In Proceedings of the 2016
Conference on Empirical Methods in Natural Lan-
guage Processing, EMNLP 2016, Austin, Texas,
USA, November 1-4, 2016, pages 118–127.

Pengfei Liu, Xipeng Qiu, and Xuanjing Huang. 2017.
Adversarial multi-task learning for text classifica-
tion. In Proceedings of the 55th Annual Meeting
of the Association for Computational Linguistics,
pages 1–10.

Xuezhe Ma and Eduard H. Hovy. 2016. End-to-end se-
quence labeling via bi-directional lstm-cnns-crf. In
Proceedings of the 54th Annual Meeting of the As-
sociation for Computational Linguistics, ACL 2016,
August 7-12, 2016, Berlin, Germany, Volume 1:
Long Papers.

Makoto Miwa and Mohit Bansal. 2016. End-to-end re-
lation extraction using lstms on sequences and tree
structures. In Proceedings of the 54th Annual Meet-
ing of the Association for Computational Linguis-
tics, pages 1105–1116.

Diego Mollá, Menno van Zaanen, and Daniel
Smith. 2006. Named entity recognition for
question answering. In Proceedings of the
2006 Australasian Language Technology Workshop
(ALTW2006), pages 51–58.

Nanyun Peng and Mark Dredze. 2015. Named en-
tity recognition for chinese social media with jointly
trained embeddings. In Proceedings of the 2015
Conference on Empirical Methods in Natural Lan-
guage Processing, EMNLP 2015, Lisbon, Portugal,
September 17-21, 2015, pages 548–554.

Nanyun Peng and Mark Dredze. 2016. Improving
named entity recognition for chinese social media
with word segmentation representation learning. In
Proceedings of the 54th Annual Meeting of the As-
sociation for Computational Linguistics, ACL 2016.

Nanyun Peng and Mark Dredze. 2017. Supplementary
results for named entity recognition on chinese so-
cial media with an updated dataset. Technical re-
port.

Kuniko Saito and Masaaki Nagata. 2003. Multi-
language named-entity recognition system based on
hmm. In Proceedings of the ACL 2003 Workshop
on Multilingual and Mixed-language Named Entity
Recognition.

Mohammad Golam Sohrab and Makoto Miwa. 2018.
Deep exhaustive model for nested named entity
recognition. In Proceedings of the 2018 Conference
on Empirical Methods in Natural Language Pro-
cessing, Brussels, Belgium, October 31 - November
4, 2018, pages 2843–2849.

Xu Sun and Hangfeng He. 2017. F-score driven max
margin neural network for named entity recognition
in chinese social media. In Proceedings of the 15th



2389

Conference of the European Chapter of the Asso-
ciation for Computational Linguistics, EACL 2017,
pages 713–718.

Paola Virga and Sanjeev Khudanpur. 2003. Translit-
eration of proper names in cross-lingual information
retrieval. In Proceedings of the 54th Annual Meeting
of the Association for Computational Linguistics.

Mengqiu Wang, Wanxiang Che, and Christopher D.
Manning. 2013. Effective bilingual constraints for
semi-supervised learning of named entity recogniz-
ers. In Proceedings of the Twenty-Seventh AAAI
Conference on Artificial Intelligence, July 14-18,
2013, Bellevue, Washington, USA.

Ralph Weischedel, Martha Palmer, Mitchell Marcus,
Eduard Hovy, Sameer Pradhan, Lance Ramshaw,
Ann Taylor Nianwen Xue, Jeff Kaufman, Michelle
Franchini, Mohammed El-Bachouti, Robert Belvin,
and Ann Houston. 2011. Ontonotes release 4.0
ldc2011t03. dvd. In Linguistic Data Consortium.

Vikas Yadav and Steven Bethard. 2018. A survey on
recent advances in named entity recognition from
deep learning models. In Proceedings of the 27th In-
ternational Conference on Computational Linguis-
tics, pages 2145–2158. Association for Computa-
tional Linguistics.

Jie Yang, Zhiyang Teng, Meishan Zhang, and Yue
Zhang. 2016. Combining discrete and neural fea-
tures for sequence labeling. In Computational Lin-
guistics and Intelligent Text Processing - 17th Inter-
national Conference, CICLing 2016, Konya, Turkey,
April 3-9, 2016, Revised Selected Papers, Part I,
pages 140–154.

Jie Yang and Yue Zhang. 2018. Ncrf++: An open-
source neural sequence labeling toolkit. In Proceed-
ings of ACL 2018, System Demonstrations, pages
74–79. Association for Computational Linguistics.

Jie Yang, Yue Zhang, and Fei Dong. 2017. Neural
word segmentation with rich pretraining. In Pro-
ceedings of the 55th Annual Meeting of the Associa-
tion for Computational Linguistics, ACL 2017, Van-
couver, Canada, July 30 - August 4, Volume 1: Long
Papers, pages 839–849.

Jie Yang, Yue Zhang, and Shuailong Liang. 2018. Sub-
word encoding in lattice LSTM for chinese word
segmentation. Computing Research Repository,
abs/1810.12594.

Qi Zhang, Xiaoyu Liu, and Jinlan Fu. 2018. Neu-
ral networks incorporating dictionaries for chinese
word segmentation. In Proceedings of the Thirty-
Second AAAI Conference on Artificial Intelligence,
(AAAI-18), the 30th innovative Applications of Arti-
ficial Intelligence (IAAI-18), and the 8th AAAI Sym-
posium on Educational Advances in Artificial Intelli-
gence (EAAI-18), pages 5682–5689. Springer Berlin
Heidelberg.

Suxiang Zhang, Ying Qin, Juan Wen, and Xiaojie
Wang. 2006. Word segmentation and named en-
tity recognition for SIGHAN bakeoff3. In Proceed-
ings of the Fifth Workshop on Chinese Language
Processing, SIGHAN@COLING/ACL 2006, Sydney,
Australia, July 22-23, 2006, pages 158–161.

Yue Zhang and Jie Yang. 2018. Chinese ner using lat-
tice lstm. In Proceedings of the 56th Annual Meet-
ing of the Association for Computational Linguis-
tics, ACL 2018, pages 1554–1564. Springer Berlin
Heidelberg.

Junsheng Zhou, Weiguang Qu, and Fen Zhang. 2013.
Chinese named entity recognition via joint identifi-
cation and categorization. Chinese journal of elec-
tronics, 22(2):225–230.


