



















































Learning How to Actively Learn: A Deep Imitation Learning Approach


Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers), pages 1874–1883
Melbourne, Australia, July 15 - 20, 2018. c©2018 Association for Computational Linguistics

1874

Learning How to Actively Learn: A Deep Imitation Learning Approach

Ming Liu Wray Buntine
Faculty of Information Technology, Monash University

{ming.m.liu, wray.buntine, gholamreza.haffari} @ monash.edu

Gholamreza Haffari

Abstract

Heuristic-based active learning (AL)
methods are limited when the data
distribution of the underlying learning
problems vary. We introduce a method
that learns an AL policy using imitation
learning (IL). Our IL-based approach
makes use of an efficient and effective
algorithmic expert, which provides the
policy learner with good actions in the
encountered AL situations. The AL strat-
egy is then learned with a feedforward
network, mapping situations to most
informative query datapoints. We evaluate
our method on two different tasks: text
classification and named entity recogni-
tion. Experimental results show that our
IL-based AL strategy is more effective
than strong previous methods using
heuristics and reinforcement learning.

1 Introduction

For many real-world NLP tasks, labeled data is
rare while unlabelled data is abundant. Active
learning (AL) seeks to learn an accurate model
with minimum amount of annotation cost. It is
inspired by the observation that a model can get
better performance if it is allowed to choose the
data points on which it is trained. For example, the
learner can identify the areas of the space where it
does not have enough knowledge, and query those
data points which bridge its knowledge gap.

Traditionally, AL is performed using engi-
neered heuristics in order to estimate the useful-
ness of unlabeled data points as queries to an an-
notator. Recent work (Fang et al., 2017; Bachman
et al., 2017; Woodward and Finn, 2017) have fo-
cused on learning the AL querying strategy, as en-
gineered heuristics are not flexible to exploit char-

acteristics inherent to a given problem. The basic
idea is to cast AL as a decision process, where the
most informative unlabeled data point needs to be
selected based on the history of previous queries.
However, previous works train for the AL pol-
icy by a reinforcement learning (RL) formulation,
where the rewards are provided at the end of se-
quences of queries. This makes learning the AL
policy difficult, as the policy learner needs to deal
with the credit assignment problem. Intuitively,
the learner needs to observe many pairs of query
sequences and the resulting end-rewards to be able
to associate single queries with their utility scores.

In this work, we formulate learning AL strate-
gies as an imitation learning problem. In par-
ticular, we consider the popular pool-based AL
scenario, where an AL agent is presented with a
pool of unlabelled data. Inspired by the Dataset
Aggregation (DAGGER) algorithm (Ross et al.,
2011), we develop an effective AL policy learn-
ing method by designing an efficient and effective
algorithmic expert, which provides the AL agent
with good decisions in the encountered states. We
then use a deep feedforward network to learn the
AL policy to associate states to actions. Unlike
the RL approach, our method can get observa-
tions and actions directly from the expert’s trajec-
tory. Therefore, our trained policy can make better
rankings of unlabelled datapoints in the pool, lead-
ing to more effective AL strategies.

We evaluate our method on text classification
and named entity recognition. The results show
our method performs better than strong AL meth-
ods using heuristics and reinforcement learning,
in that it boosts the performance of the under-
lying model with fewer labelling queries. An
open source implementation of our model is avail-
able at: https://github.com/Grayming/
ALIL.

https://github.com/Grayming/ALIL
https://github.com/Grayming/ALIL


1875

2 Pool-based AL as a Decision Process

We consider the popular pool-based AL setting
where we are given a small set of initial labeled
data and a large pool of unlabelled data, and a bud-
get for getting the annotation of some unlabelled
data by querying an oracle, e.g. a human anno-
tator. The goal is to intelligently pick those unla-
belled data for which if the annotations were avail-
able, the performance of the underlying re-trained
model would be improved the most.

The main challenge in AL is how to identify and
select the most beneficial unlabelled data points.
Various heuristics have been proposed to guide the
unlabelled data selection (Settles, 2010). How-
ever, there is no one AL heuristic which performs
best for all problems. The goal of this paper is to
provide an approach to learn an AL strategy which
is best suited for the problem at hand, instead of
resorting to ad-hoc heuristics.

The AL strategy can be learned by attempting
to actively learn on tasks sampled from a distribu-
tion over the tasks (Bachman et al., 2017). The
idea is to simulate the AL scenario on instances of
the problem created using available labeled data,
where the label of some part of the data is kept
hidden. This allows to have an automatic oracle
to reveal the labels of the queried data, resulting
in an efficient way to quickly evaluate a hypothe-
sised AL strategy. Once the AL strategy is learned
on simulations, it is then applied to real AL sce-
narios. The more related are the tasks in the real
scenario to those used to train the AL strategy, the
more effective the AL strategy would be.

We are interested to train a model mφφφ which
maps an inputxxx ∈ X to its label yyy ∈ Yxxx, where Yxxx
is the set of labels for the input xxx and φφφ is the pa-
rameter vector of the underling model. For exam-
ple, in the named entity recognition (NER) task,
the input is a sentence and the output is its label se-
quence, e.g. in the IBO format. Let D = {(xxx,yyy)}
be a support set of labeled data, which is randomly
partitioned into labeledDlab, unlabelledDunl, and
evaluation Devl datasets. Repeated random parti-
tioning creates multiple instances of the AL prob-
lem. At each time step t of an AL problem, the
algorithm interacts with the oracle and queries the
label of a datapoint xxxt ∈ Dunlt . As the result of
this action, the followings happen:

• The automatic oracle reveals the label yyyt;

• The labeled and unlabelled datasets are up-

dated to include and exclude the recently
queried data point, respectively;

• The underlying model is re-trained based on
the enlarged labeled data to update φφφ; and

• The AL algorithm receives a reward
−loss(mφφφ, Devl), which is the negative
loss of the current trained model on the
evaluation set, defined as

loss(mφφφ, D
evl) :=

∑
(xxx,yyy)∈Devl

loss(mφφφ(xxx), yyy)

where loss(yyy′, yyy) is the loss incurred due to
predicting yyy′ instead of the ground truth yyy.

More formally, a pool-based AL problem is
a Markov decision process (MDP), denoted by
(S,A, Pr(ssst+1|ssst, at), R) where S is the state
space, A is the set of actions, Pr(ssst+1|ssst, at) is
the transition function, and R is the reward func-
tion. The state ssst ∈ S at time t consists of the
labeled Dlabt and unlabelled D

unl
t datasets paired

with the parameters of the currently trained model
φt. An action at ∈ A corresponds to the selec-
tion of a query datapoint, and the reward function
R(ssst, at, ssst+1) := −loss(mφφφt , Devl).

We aim to find the optimal AL policy prescrib-
ing which datapoint needs to be queried in a given
state to get the most benefit. The optimal policy is
found by maximising the following objective over
the parameterised policies:

E(Dlab,Dunl,Devl)∼D

[
Eπθθθ

[ B∑
t=1

R(ssst, at, ssst+1)
]]

(1)

where πθθθ is the policy network parameterised by
θθθ,D is a distribution over possible AL problem in-
stances, and B is the maximum number of queries
made in an AL run, a.k.a. an episode. Following
(Bachman et al., 2017), we maximise the sum of
the rewards after each time step to encourage the
anytime behaviour, i.e. the model should perform
well after each label query.

3 Deep Imitation Learning to Train the
AL Policy

The question remains as how can we train the
policy network to maximise the training objec-
tive in eqn 1. Typical learning approaches resort
to deep reinforcement learning (RL) and provide
training signal at the end of each episode to learn
the optimal policy (Fang et al., 2017; Bachman



1876

et al., 2017) e.g., using policy gradient methods.
These approaches, however, need a large number
of training episodes to learn a reasonable policy as
they need to deal with the credit assignment prob-
lem, i.e. discovery of the utility of individual ac-
tions in the sequence based on the achieved reward
at the end of the episode. This exacerbates the dif-
ficulty of finding a good AL policy.

We formulate learning for the AL policy as an
imitation learning problem. At each state, we pro-
vide the AL agent with a correct action which is
computed by an algorithmic expert. The AL agent
uses the sequence of states observed in an episode
paired with the expert’s sequence of actions to up-
date its policy. This directly addresses the credit
assignment problem, and reduces the complexity
of the problem compared to the RL approaches.
In what follows, we describe the ingredients of
our deep imitation learning (IL) approach, which
is summarised in Algorithm 1.

Algorithmic Expert. At a given AL state ssst, our
algorithmic expert computes an action by evaluat-
ing the current pool of unlabeled data. More con-
cretely, for each xxx′ ∈ Dpoolrnd and its correct label
yyy′, the underlying model mφφφt is re-trained to get
mxxx
′

φφφt
, where Dpoolrnd ⊂ D

unl
t is a small subset of the

current large pool of unlabeled data. The expert
action is then computed as:

arg min
xxx′∈Dpoolrnd

loss(mxxx
′

φφφt
(xxx), Devl). (2)

In other words, our algorithmic expert tries a sub-
set of actions to roll-out one step from the current
state, in order to efficiently compute a reasonable
action. Searching for the optimal action would be
O(|Dunl|B), which is computationally challeng-
ing due to (i) the large action set, and (ii) the ex-
ponential dependence on the length of the roll out.
We will see in the experiments that our method ef-
ficiently learns effective AL policies.

Policy Network. Our policy network is a feed-
forward network with two fully-connected hidden
layers. It receives the current AL state, and pro-
vides a preference score for a given unlabeled data
point, allowing to select the most beneficial one
corresponding to the highest score. The input
to our policy network consists of three parts: (i)
a fixed dimensional representation of the content
and the predicted label of the unlabeled data point
under consideration, (ii) a fixed-dimensional rep-

resentation of the content and the labels of the la-
beled dataset, and (iii) a fixed-dimensional repre-
sentation of the content of the unlabeled dataset.

score

Dpoolrand

Dlab

xxx

yyy

Figure 1: The policy network and its inputs.

Imitation Learning Algorithm. A typical ap-
proach to imitation learning (IL) is to train the
policy network so that it mimics the expert’s be-
haviour given training data of the encountered
states (input) and actions (output) performed by
the expert. The policy network’s prediction af-
fects future inputs during the execution of the pol-
icy. This violates the crucial independent and
identically distributed (iid) assumption, inherent
to most statistical supervised learning approaches
for learning a mapping from states to actions.

We make use of Dataset Aggregation
(DAGGER) (Ross et al., 2011), an iterative
algorithm for IL which addresses the non-iid
nature of the encountered states during the AL
process (see Algorithm 1). In round τ of DAG-
GER, the learned policy network π̂τ is applied to
the AL problem to collect a sequence of states
which are paired with the expert actions. The
collected pair of states and actions are aggregated
to the dataset of such pairs M , collected from the
previous iterations of the algorithm. The policy
network is then re-trained on the aggregated set,
resulting in π̂τ+1 for the next iteration of the algo-
rithm. The intuition is to build up the set of states
that the algorithm is likely to encounter during its
execution, in order to increase the generalization
of the policy network. To better leverage the
training signal from the algorithmic expert, we
allow the algorithm to collect state-action pairs
according to a modified policy which is a mixture
of π̂τ and the expert policy π̃∗τ , i.e.

πτ = βτ π̃
∗ + (1− βτ )π̂τ

where βτ ∈ [0, 1] is a mixing coefficient. This
amounts to tossing a coin with parameter βτ in



1877

each iteration of the algorithm to decide one of
these two policies for data collection.

Re-training the Policy Network. To train our
policy network, we turn the preference scores to
probabilities, and optimise the parameters such
that the probability of the action prescribed by
the expert is maximized. More specifically, let
M := {(sssi, aaai)}Ii=1 be the collected states paired
with their expert’s prescribed actions. LetDpooli be
the set of unlabelled datapoints in the pool within
the state, and aaai denote the datapoint selected by
the expert in the set. Our training objective is∑I

i=1 logPr(aaai|D
pool
i ) where

Pr(aaai|Dpooli ) :=
exp π̂(aaai;sssi)∑

xxx∈Dpooli
exp π̂(xxx;sssi)

.

The above can be interpreted as the probability of
aaai being the best action among all possible actions
in the state. Following (Mnih et al., 2015), we ran-
domly sample multiple1 mini-batches from the re-
play memoryM, in addition to the current round’s
stat-action pair, in order to retrain the policy net-
work. For each mini-batch, we make one SGD
step to update the policy, where the gradients of
the network parameters are calculated using the
backpropagation algorithm.

Transferring the Policy. We now apply the pol-
icy learned on the source task to AL in the tar-
get task. We expect the learned policy to be effec-
tive for target tasks which are related to the source
task in terms of the data distribution and charac-
teristics. Algorithm 2 illustrates the policy trans-
fer. The pool-based AL scenario in Algorithm 2 is
cold-start; however, extending to incorporate ini-
tially available labeled data is straightforward.

4 Experiments

We conduct experiments on text classification and
named entity recognition (NER). The AL sce-
narios include cross-domain sentiment classifica-
tion, cross-lingual authorship profiling, and cross-
lingual named entity recognition (NER), whereby
an AL policy trained on a source domain/language
is transferred to the target domain/language.

We compare our proposed AL method using im-
itation learning (ALIL) with the followings:

• Random sampling: The query datapoint is cho-
sen randomly.

1In our experiments, we use 10 mini-bathes, each of
which of size 100.

Algorithm 1 Learn active learning policy via imi-
tation learning
Input: large labeled data D, max episodes T , budget B,

sample size K, the coin parameter β
Output: The learned policy
1: M ← ∅ . the aggregated dataset
2: initialise π̂1 with a random policy
3: for τ=1, . . . , T do
4: Dlab, Dunl, Devl ← dataPartition(D)
5: φφφ1 ← trainModel(Dlab)
6: c← coinToss(β)
7: for t ∈ 1, . . . ,B do
8: Dpoolrnd ← sampleUniform(D

unl,K)

9: ssst ← (Dlab, Dpoolrnd ,φφφt)
10: aaat ← argminxxx′∈Dpool

rnd
loss(mxxx

′
φφφt
, Devl)

11: if c is head then . the expert
12: xxxt ← aaat
13: else . the policy
14: xxxt ← argmaxxxx′∈Dpool

rnd
π̂τ (xxx

′;ssst)

15: end if
16: Dlab ← Dlab + {(xxxt, yyyt)}
17: Dunl ← Dunl − {xxxt}
18: M ←M + {(ssst, aaat)}
19: φφφt+1 ← retrainModel(φφφt, Dlab)
20: end for
21: π̂τ+1 ← retrainPolicy(π̂τ ,M)
22: end for
23: return π̂T+1

Algorithm 2 Active learning by policy transfer
Input: unlabeled pool Dunl, budget B, policy π̂
Output: labeled dataset and trained model
1: Dlab ← ∅
2: initialise φφφ randomly
3: for t ∈ 1, . . . ,B do
4: ssst ← (Dlab, Dunl,φφφ)
5: xxxt ← argmaxxxx′∈Dunl π̂(xxx′;ssst)
6: yyyt ← askAnnotation(xxxt)
7: Dlab ← Dlab + {(xxxt, yyyt)}
8: Dunl ← Dunl − {xxxt}
9: φ← retrainModel(φφφ,Dlab)

10: end for
11: return Dlab and φφφ

• Diversity sampling: The query datapoint is
argminxxx

∑
xxx′∈Dlab Jaccard(xxx,xxx

′), where the Jac-
card coefficient between the unigram features
of the two given texts is used as the similarity
measure.

• Uncertainty-based sampling: For text
classification, we use the datapoint
with the highest predictive entropy,
argmaxxxx−

∑
y p(y|xxx,D

lab) log p(y|xxx,Dlab) where
p(yyy|xxx,Dlab) comes from the underlying
model. We further use a state-of-the-art exten-
sion of this method, called uncertainty with
rationals (Sharma et al., 2015), which not only
considers uncertainty but also looks whether



1878

doc. (src/tgt)
src tgt number avg. len. (tokens)

elec. music dev. 27k/1k 35/20
book movie 24k/2k 140/150

en sp 3.6k/4.2k 1.15k/1.35k
en pt 3.6k/1.2k 1.15k/1.03k

Table 1: The data sets used in sentiment classifica-
tion (top part) and gender profiling (bottom part).

the unlabelled document contains sentiment
words or phrases that were returned as ratio-
nales for any of the existing labeled documents.
For NER, we use the Total Token Entropy
(TTE) as the uncertainty sampling method,
argmaxxxx−

∑|xxx|
i=1

∑
yi
p(yi|xxx,Dlab) log p(yi|xxx,Dlab)

which has been shown to be the best heuristic
for this task among 17 different heuristics
(Settles and Craven, 2008).

• PAL: A reinforcement learning based approach
(Fang et al., 2017), which makes use a deep
Q-network to make the selection decision for
stream-based active learning.

4.1 Text Classification

Datasets and Setup. The first task is senti-
ment classification, in which product reviews ex-
press either positive or negative sentiment. The
data comes from the Amazon product reviews
(McAuley and Yang, 2016); see Table 1 for data
statistics.

The second task is Authorship Profiling, in
which we aim to predict the gender of the text
author. The data comes from the gender profil-
ing task in PAN 2017 (Rangel et al., 2017), which
consists of a large Twitter corpus in multiple lan-
guages: English (en), Spanish (es) and Portuguese
(pt). For each language, all tweets collected from a
user constitute one document; Table 1 shows data
statistics. The multilingual embeddings for this
task come from off-the-shelf CCA-trained embed-
dings (Ammar et al., 2016) for twelve languages,
including English, Spanish and Portuguese. We
fix these word embeddings during training of both
the policy and the underlying classification model.

For training, 10% of the source data is used as
the evaluation set for computing the best action in
imitation learning. We run T = 100 episodes with
the budget B = 100 documents in each episode,
set the sample size K = 5, and fix the mixing
coefficient βτ = 0.5. For testing, we take 90%
of the target data as the unlabeled pool, and the

remaining 10% as the test set. We show the test
accuracy w.r.t. the number of labelled documents
selected in the AL process.

As the underlying model mφφφ, we use a fast and
efficient text classifier based on convolutional neu-
ral networks. More specifically, we apply 50 con-
volutional filters with ReLU activation on the em-
bedding of all words in a document xxx, where the
width of the filters is 3. The filter outputs are aver-
aged to produce a 50-dimensional document rep-
resentation hhh(xxx), which is then fed into a softmax
to predict the class.

Representing state-action. The input to the
policy network, i.e. the feature vector represent-
ing a state-action pair, includes: the candidate doc-
ument represented by the convolutional net hhh(xxx),
the distribution over the document’s class labels
mφφφ(xxx), the sum of all document vector represen-
tations in the labeled set

∑
xxx′∈Dlab hhh(xxx

′), the sum
of all document vectors in the random pool of un-
labelled data

∑
xxx′∈Dpoolrnd

hhh(xxx′), and the empirical
distribution of class labels in the labeled dataset.

Results. Fig 2 shows the results on product
sentiment prediction and authorship profiling, in
cross-domain and cross-lingual AL scenarios2.
Our ALIL method consistently outperforms both
heuristic-based and RL-based (PAL) (Fang et al.,
2017) approaches across all tasks. ALIL tends to
convergence faster than other methods, which in-
dicates its policy can quickly select the most infor-
mative datapoints. Interestingly, the uncertainty
and diversity sampling heuristics perform worse
than random sampling on sentiment classification.
We speculate this may be due to these two heuris-
tics not being able to capture the polarity informa-
tion during the data selection process. PAL per-
forms on-par with uncertainty with rationals on
musical device, both of which outperform the tra-
ditional diversity and uncertainty sampling heuris-
tics. Interestingly, PAL is outperformed by ran-
dom sampling on movie reviews, and by the tra-
ditional uncertainty sampling heuristic on author-
ship profiling tasks. We attribute this to ineffec-
tiveness of the RL-based approach for learning a
reasonable AL query strategy.

We further investigate combining the transfer of
the policy network with the transfer of the under-
lying classifier. That is, we first train a classi-

2Uncertainty with rationale cannot be done for authorship
profiling as the rationales come from a sentiment dictionary.



1879

Figure 2: The performance of different active learning methods for cross domain sentiment classification
(left two plots) and cross lingual authorship profiling (right two plots).

fier on all of the annotated data from the source
domain/language. Then, this classifier is ported
to the target domain/language; for cross-language
transfer, we make use of multilingual word em-
beddings. We start the AL process starting from
the transferred classifier, referred to as the warm-
start AL. We compare the performance of the di-
rectly transferred classifier with those obtained af-
ter the AL process in the warm-start and cold-start
scenarios. The results are shown in Table 2. We
have run the cold-start and warm-start AL for 25
times, and reported the average accuracy in Ta-
ble 2. As seen from the results, both the cold and
warm start AL settings outperform the direct trans-
fer significantly, and the warm start consistently
gets higher accuracy than the cold start. The dif-
ference between the results are statistically signif-
icant, with a p-value of .001, according to McNe-
mar test3 (Dietterich, 1998).

musical movie es pt
direct transfer 0.715 0.640 0.675 0.740
cold-start AL 0.800 0.760 0.728 0.773
warm-start AL 0.825 0.765 0.730 0.780

Table 2: Classifiers performance under three dif-
ferent transfer settings.

4.2 Named Entity Recognition
Data and setup We use NER corpora from the
CONLL2002/2003 shared tasks, which include
annotated text in English (en), German (de), Span-
ish (es), and Dutch (nl). The original annotation
is based on IOB1, which we convert to the IO

3As the contingency table needed for the McNemar test,
we have used the average counts across the 25 runs.

labelling scheme. Following Fang et al. (2017),
we consider two experimental conditions: (i) the
bilingual scenario where English is the source
(used for policy training) and other languages are
the target, and (ii) the multilingual scenario where
one of the languages (except English) is the target
and the remaining ones are the source used in joint
training of the AL policy. The underlying model
mφφφ is a conditional random field (CRF) treating
NER as a sequence labelling task. The prediction
is made using the Viterbi algorithm.

In the existing corpus partitions from CoNLL,
each language has three subsets: train, testa and
testb. During policy training with the source lan-
guage(s), we combine these three subsets, shuf-
fle, and re-split them into simulated training, unla-
belled pool, and evaluation sets in every episode.
We run N = 100 episodes with the budget B =
200, and set the sample size k = 5. When we
transfer the policy to the target language, we do
one episode and select B datapoints from train
(treated as the pool of unlabeled data) and report
F1 scores on testa.

Representing state-action. The input to the
policy network includes the representation of the
candidate sentence using the sum of its words’
embeddings hhh(xxx), the representation of the la-
belling marginals using the label-level convolu-
tional network cnnlab(Emφφφ(yyy|xxx)[yyy]) (Fang et al.,
2017), the representation of sentences in the la-
beled data

∑
(xxx′,yyy′)∈Dlab hhh(xxx

′), the representa-
tion of sentences in the random pool of un-
labelled data

∑
xxx′∈Dpoolrnd

hhh(xxx′), the representa-
tion of ground-truth labels in the labeled data∑

(xxx′,yyy′)∈Dlab cnnlab(yyy
′) using the empirical distri-

butions, and the confidence of the sequential pre-



1880

Figure 3: The performance of active learning methods on the bilingual
and multilingual settings for three target languages: German (de), Span-
ish (es) and Dutch (nl).

Figure 4: The learning curves
of agents with different K on
Spanish (es) NER.

diction |xxx|
√

maxyyymφφφ(yyy|xxx), where |xxx| denotes the
length of the sentence xxx. For the word embed-
dings, we use off-the-shelf CCA trained multilin-
gual embeddings (Ammar et al., 2016) with 40 di-
mensions; we fix these during policy training.

Results. Fig. 3 shows the results for three tar-
get languages. In addition to the strong heuristic-
based methods, we compare our imitation learn-
ing approach (ALIL) with the reinforcement learn-
ing approach (PAL) (Fang et al., 2017), in both
bilingual (bi) and multilingual (mul) transfer set-
tings. Across all three languages, ALIL.bi and
ALIL.mul outperform the heuristic methods, in-
cluding Uncertainty Sampling based on TTE. This
is expected as the uncertainty sampling largely re-
lies on a high quality underlying model, and di-
versity sampling ignores the labelling information.
In the bilingual case, ALIL.bi outperforms PAL.bi
on Spanish (es) and Dutch (nl), and performs sim-
ilarly on German (de). In the multilingual case,
ALIL.mul achieves the best performance on Span-
ish, and performs competitively with PAL.mul on
German and Dutch.

4.3 Analysis

Insight on the selected data. We compare the
data selected by ALIL to other methods. This will
confirm that ALIL learns policies which are suit-
able for the problem at hand, without resorting to
a fixed engineered heuristics. For this analysis, we
report the mean reciprocal rank (MRR) of the data
points selected by the ALIL policy under rank-
ings of the unlabelled pool generated by the un-
certainty and diversity sampling. Furthermore, we
measure the fraction of times the decisions made
by the ALIL policy agrees with those which would

movie sentiment gender pt NER es
acc Unc. 0.06 0.58 0.51

MRR Unc. 0.083 0.674 0.551
acc Div. 0.05 0.52 0.45

MRR Div. 0.057 0.593 0.530
acc PAL 0.15 0.56 0.52

Table 3: The first four rows show MRR and accu-
racy of instances returned by ALIL under the rank-
ings of uncertainty and diversity sampling, the last
row give average accuracy of instances under PAL.

have been made by the heuristic methods, which
is measured by the accuracy (acc). Table 3 re-
port these measures. As we can see, for sentiment
classification since uncertainty and diversity sam-
pling perform badly, ALIL has a big disagreement
with them on the selected data points. While for
gender classification on Portuguese and NER on
Spanish, ALIL shows much more agreement with
other three heuristics.

Lastly, we compare chosen queries by ALIL
to those by PAL, to investigate the extent of the
agreement between these two methods. This is
simply measure by the fraction of identical query
data points among the total number of queries (i.e.
accuracy). Since PAL is stream-based and sen-
sitive to the order in which it receives the data
points, we report the average accuracy taken over
multiple runs with random input streams. The ex-
pected accuracy numbers are reported in Table 3.
As seen, ALIL has higher overlap with PAL than
the heuristic-based methods, in terms of the se-
lected queries.

Sensitivity toK. As seen in Algorithm 1, we re-
sort to an approximate algorithmic expert, which
selects the best action in a random subset of the



1881

Figure 5: The learning curves of agents with dif-
ferent schedules for β before the trajectory on
Spanish (es) NER.

pool of unlabelled data with size K, in order to
make the policy training efficient. Note that, in
policy training, settingK to one and the size of the
unlabelled data pool correspond to stream-based
and pool-based AL scenarios, respectively. By
changingK to values between these two extremes,
we can analyse the effect of the quality of the al-
gorithmic expert on the trained policy; Figure 4
shows the results. A larger candidate set may cor-
respond to a better learned policy, needed to be
traded off with the training time growing linearly
with K. Interestingly, even small candidate sets
lead to strong AL policies as increasing K beyond
10 does not change the performance significantly.

Dynamically changing β. In our algorithm, β
plays an important role as it trades off exploration
versus exploitation. In the above experiments, we
fix it to 0.5; however, we can change its value
throughout trajectory collection as a function of
τ (see Algorithm 1). We investigate schedules
which tend to put more emphasis on exploration
and exploitation towards the beginning and end of
data collection, respectively. We investigate the
following schedules: (i) linear βτ = max(0.5, 1−
0.01τ), (ii) exponential βτ = 0.9τ , and (iii) and
inverse sigmoid βτ = 55+exp(τ/5) , as a function of
iterations. Fig. 5 shows the comparisons of these
schedules. The learned policy seems to perform
competitively with either a fixed or an exponential
schedule. We have also investigated tossing the
coin in each step within the trajectory roll out, but
found that it is more effective to have it before the
full trajectory roll out (as currently done in Algo-
rithm 1).

5 Related Work

Traditional active learning algorithms rely on
various heuristics (Settles, 2010), such as un-
certainty sampling (Settles and Craven, 2008;
Houlsby et al., 2011), query-by-committee (Gilad-
Bachrach et al., 2006), and diversity sampling
(Brinker, 2003; Joshi et al., 2009; Yang et al.,
2015). Apart from these, different heuristics
can be combined, thus creating integrated strat-
egy which consider one or more heuristics at the
same time. Combined with transfer learning,
pre-existing labeled data from related tasks can
help improve the performance of an active learner
(Xiao and Guo, 2013; Kale and Liu, 2013; Huang
and Chen, 2016; Konyushkova et al., 2017). More
recently, deep reinforcement learning is used as
the framework for learning active learning algo-
rithms, where the active learning cycle is consid-
ered as a decision process. (Woodward and Finn,
2017) extended one shot learning to active learn-
ing and combined reinforcement learning with a
deep recurrent model to make labeling decisions.
(Bachman et al., 2017) introduced a policy gradi-
ent based method which jointly learns data repre-
sentation, selection heuristic as well as the model
prediction function. (Fang et al., 2017) designed
an active learning algorithm based on a deep Q-
network, in which the action corresponds to bi-
nary annotation decisions applied to a stream of
data. The learned policy can then be transferred
between languages or domains.

Imitation learning (IL) refers to an agent’s ac-
quisition of skills or behaviours by observing an
expert’s trajectory in a given task. It helps re-
duce sequential prediction tasks into supervised
learning by employing a (near) optimal oracle at
training time. Several IL algorithms has been
proposed in sequential prediction tasks, including
SEARA (Daumé et al., 2009), AggreVaTe (Ross
and Bagnell, 2014), DaD (Venkatraman et al.,
2015), LOLS(Chang et al., 2015), DeeplyAggre-
VaTe (Sun et al., 2017). Our work is closely re-
lated to Dagger (Ross et al., 2011), which can
guarantee to find a good policy by addressing the
dependency nature of encountered states in a tra-
jectory.

6 Conclusion

In this paper, we have proposed a new method for
learning active learning algorithms using deep im-
itation learning. We formalize pool-based active



1882

learning as a Markov decision process, in which
active learning corresponds to the selection de-
cision of the most informative data points from
the pool. Our efficient algorithmic expert pro-
vides state-action pairs from which effective active
learning policies can be learned. We show that the
algorithmic expert allows direct policy learning,
while at the same time, the learned policies trans-
fer successfully between domains and languages,
demonstrating improvement over previous heuris-
tic and reinforcement learning approaches.

Acknowledgments

We would like to thank the feedback from anony-
mous reviewers. G. H. is grateful to Trevor Cohn
for interesting discussions. This work was sup-
ported by computational resources from the Multi-
modal Australian ScienceS Imaging and Visuali-
sation Environment (MASSIVE) at Monash Uni-
versity.

References
Waleed Ammar, George Mulcaire, Yulia Tsvetkov,

Guillaume Lample, Chris Dyer, and Noah A Smith.
2016. Massively multilingual word embeddings.
arXiv preprint arXiv:1602.01925.

Philip Bachman, Alessandro Sordoni, and Adam
Trischler. 2017. Learning algorithms for active
learning. In Proceedings of the 34th International
Conference on Machine Learning, volume 70 of
Proceedings of Machine Learning Research, pages
301–310, International Convention Centre, Sydney,
Australia. PMLR.

Klaus Brinker. 2003. Incorporating diversity in active
learning with support vector machines. In Proceed-
ings of the 20th International Conference on Ma-
chine Learning (ICML-03), pages 59–66.

Kai-Wei Chang, Akshay Krishnamurthy, Alekh Agar-
wal, Hal Daume, and John Langford. 2015. Learn-
ing to search better than your teacher. In Proceed-
ings of the 32nd International Conference on Ma-
chine Learning (ICML-15), pages 2058–2066.

Ido Dagan and Sean P Engelson. 1995. Selective sam-
pling in natural language learning. In Proceedings
of IJCAI-95 Workshop on New Approaches to Learn-
ing for Natural Language Processing.

Hal Daumé, John Langford, and Daniel Marcu. 2009.
Search-based structured prediction. Machine learn-
ing, 75(3):297–325.

Thomas G. Dietterich. 1998. Approximate statistical
tests for comparing supervised classification learn-
ing algorithms. Neural Comput., 10(7):1895–1923.

Meng Fang, Yuan Li, and Trevor Cohn. 2017. Learning
how to active learn: A deep reinforcement learning
approach. arXiv preprint arXiv:1708.02383.

Ran Gilad-Bachrach, Amir Navot, and Naftali Tishby.
2006. Query by committee made real. In Ad-
vances in neural information processing systems,
pages 443–450.

Neil Houlsby, Ferenc Huszár, Zoubin Ghahramani,
and Máté Lengyel. 2011. Bayesian active learn-
ing for classification and preference learning. arXiv
preprint arXiv:1112.5745.

Sheng-Jun Huang and Songcan Chen. 2016. Transfer
learning with active queries from source domain. In
IJCAI, pages 1592–1598.

Ajay J Joshi, Fatih Porikli, and Nikolaos Pa-
panikolopoulos. 2009. Multi-class active learning
for image classification. In Computer Vision and
Pattern Recognition, 2009. CVPR 2009. IEEE Con-
ference on, pages 2372–2379. IEEE.

David Kale and Yan Liu. 2013. Accelerating active
learning with transfer learning. In Data Mining
(ICDM), 2013 IEEE 13th International Conference
on, pages 1085–1090. IEEE.

Ksenia Konyushkova, Raphael Sznitman, and Pascal
Fua. 2017. Learning active learning from data. In
I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach,
R. Fergus, S. Vishwanathan, and R. Garnett, editors,
Advances in Neural Information Processing Systems
30, pages 4225–4235. Curran Associates, Inc.

Julian McAuley and Alex Yang. 2016. Addressing
complex and subjective product-related queries with
customer reviews. In Proceedings of the 25th In-
ternational Conference on World Wide Web, pages
625–635. International World Wide Web Confer-
ences Steering Committee.

Volodymyr Mnih, Koray Kavukcuoglu, David Silver,
Andrei A. Rusu, Joel Veness, Marc G. Bellemare,
Alex Graves, Martin Riedmiller, Andreas K. Fidje-
land, Georg Ostrovski, Stig Petersen, Charles Beat-
tie, Amir Sadik, Ioannis Antonoglou, Helen King,
Dharshan Kumaran, Daan Wierstra, Shane Legg,
and Demis Hassabis. 2015. Human-level con-
trol through deep reinforcement learning. Nature,
518(7540):529–533.

Kamal Nigam and Andrew McCallum. 1998. Pool-
based active learning for text classification. In
Conference on Automated Learning and Discovery
(CONALD).

Francisco Rangel, Paolo Rosso, Martin Potthast, and
Benno Stein. 2017. Overview of the 5th author pro-
filing task at PAN 2017: Gender and language vari-
ety identification in twitter. Working Notes Papers
of the CLEF.

Stephane Ross and J Andrew Bagnell. 2014. Rein-
forcement and imitation learning via interactive no-
regret learning. arXiv preprint arXiv:1406.5979.

https://doi.org/10.1162/089976698300017197
https://doi.org/10.1162/089976698300017197
https://doi.org/10.1162/089976698300017197
http://papers.nips.cc/paper/7010-learning-active-learning-from-data.pdf


1883

Stéphane Ross, Geoffrey J Gordon, and Drew Bagnell.
2011. A reduction of imitation learning and struc-
tured prediction to no-regret online learning. In In-
ternational Conference on Artificial Intelligence and
Statistics, pages 627–635.

Burr Settles. 2010. Active learning literature survey.
University of Wisconsin, Madison, 52(55-66):11.

Burr Settles and Mark Craven. 2008. An analysis of ac-
tive learning strategies for sequence labeling tasks.
In Proceedings of the conference on empirical meth-
ods in natural language processing, pages 1070–
1079. Association for Computational Linguistics.

Claude E Shannon. 1948. A note on the concept of
entropy. Bell System Tech. J, 27:379–423.

Manali Sharma, Di Zhuang, and Mustafa Bilgic. 2015.
Active learning with rationales for text classifica-
tion. In Proceedings of the 2015 Conference of
the North American Chapter of the Association for
Computational Linguistics: Human Language Tech-
nologies, pages 441–451.

Wen Sun, Arun Venkatraman, Geoffrey J Gor-
don, Byron Boots, and J Andrew Bagnell. 2017.
Deeply aggrevated: Differentiable imitation learn-
ing for sequential prediction. arXiv preprint
arXiv:1703.01030.

Arun Venkatraman, Martial Hebert, and J Andrew
Bagnell. 2015. Improving multi-step prediction of
learned time series models. In AAAI, pages 3024–
3030.

Mark Woodward and Chelsea Finn. 2017. Active one-
shot learning. arXiv preprint arXiv:1702.06559.

Min Xiao and Yuhong Guo. 2013. Online active learn-
ing for cost sensitive domain adaptation. In Pro-
ceedings of the Seventeenth Conference on Compu-
tational Natural Language Learning, pages 1–9.

Yi Yang, Zhigang Ma, Feiping Nie, Xiaojun Chang,
and Alexander G Hauptmann. 2015. Multi-class ac-
tive learning by uncertainty sampling with diversity
maximization. International Journal of Computer
Vision, 113(2):113–127.


