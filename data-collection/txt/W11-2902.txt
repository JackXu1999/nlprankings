










































A Generalized View on Parsing and Translation


Proceedings of the 12th International Conference on Parsing Technologies, pages 2–13,
October 5-7, 2011, Dublin City University. c© 2011 Association for Computational Linguistics

A Generalized View on Parsing and Translation

Alexander Koller
Dept. of Linguistics

University of Potsdam, Germany
koller@ling.uni-potsdam.de

Marco Kuhlmann
Dept. of Linguistics and Philology

Uppsala University, Sweden
marco.kuhlmann@lingfil.uu.se

Abstract

We present a formal framework that gen-
eralizes a variety of monolingual and syn-
chronous grammar formalisms for parsing
and translation. Our framework is based on
regular tree grammars that describe deriva-
tion trees, which are interpreted in arbitrary
algebras. We obtain generic parsing algo-
rithms by exploiting closure properties of
regular tree languages.

1 Introduction

Over the past years, grammar formalisms that relate
pairs of grammatical structures have received much
attention. These formalisms include synchronous
grammars (Lewis and Stearns, 1968; Shieber and
Schabes, 1990; Shieber, 1994; Rambow and Satta,
1996; Eisner, 2003) and tree transducers (Comon
et al., 2007; Graehl et al., 2008). Weighted vari-
ants of both of families of formalisms have been
used for machine translation (Graehl et al., 2008;
Chiang, 2007), where one tree represents a parse
of a sentence in one language and the other a parse
in the other language. Synchronous grammars and
tree transducers are also useful as models of the
syntax-semantics interface; here one tree represents
the syntactic analysis of a sentence and the other
the semantic analysis (Shieber and Schabes, 1990;
Nesson and Shieber, 2006).

When such a variety of formalisms are avail-
able, it is useful to take a step back and look for
a generalized model that explains the precise for-
mal relationship between them. There is a long
tradition of such research on monolingual grammar
formalisms, where e.g. linear context-free rewriting
systems (LCFRS, Vijay-Shanker et al. (1987)) gen-
eralize various mildly context-sensitive formalisms.
However, few such results exist for synchronous
formalisms. A notable exception is the work by
Shieber (2004), who unified synchronous tree-
adjoining grammars with tree transducers.

In this paper, we make two contributions. First,
we provide a formal framework – interpreted reg-
ular tree grammars – which generalizes both syn-
chronous grammars, tree transducers, and LCFRS-
style monolingual grammars. A grammar of this
formalism consists of a regular tree grammar (RTG,
Comon et al. (2007)) defining a language of deriva-
tion trees and an arbitrary number of interpreta-
tions which map these trees into objects of arbi-
trary algebras. This allows us to capture a wide
variety of (synchronous and monolingual) gram-
mar formalisms. We can also model heterogeneous
synchronous languages, which relate e.g. trees with
strings; this is necessary for applications in ma-
chine translation (Graehl et al., 2008) and in pars-
ing strings with synchronous tree grammars.

Second, we also provide parsing and decoding al-
gorithms for our framework. The key concept that
we introduce is that of a regularly decomposable
algebra, where the set of all terms that evaluate to
a given object form a regular tree language. Once
an algorithm that computes a compact representa-
tion of this language is known, parsing algorithms
follow from a generic construction. All important
algebras in natural language processing that we are
aware of – in particular the standard algebras of
strings and trees – are regularly decomposable.

In summary, we obtain a formalism that pulls
together much existing research under a common
formal framework, and makes it possible to ob-
tain parsers for existing and new formalisms in a
modular, universal fashion.

Plan of the paper. The paper is structured as
follows. We start by laying the formal foundations
in Section 2. We then introduce the framework of
interpreted RTGs and illustrate it with some simple
examples in Section 3. The generic parsing and
decoding algorithms are described in Section 4.
Section 5 discusses the role of binarization in our
framework. Section 6 shows how interpreted RTGs
can be applied to existing grammar formalisms.

2



2 Formal Foundations

For n ≥ 0, we define [n] = { i | 1 ≤ i ≤ n }.
A signature is a finite set Σ of function sym-

bols f , each of which has been assigned a non-
negative integer called its rank. Given a signature
Σ, we can define a (finite constructor) tree over Σ
as a finite tree whose nodes are labeled with sym-
bols from Σ such that a node with a label of rank
n has exactly n children. We write TΣ for the set
of all trees over Σ. Trees can be written as terms;
f(t1, . . . , tn) stands for the tree with root label f
and subtrees t1, . . . , tn. The nodes of a tree can be
identified by paths π ∈ N∗ from the root: The root
has address �, and the i-th child of the node at path
π has the address πi. We write t(π) for the symbol
at path π in the tree t.

A Σ-algebra A consists of a non-empty set A
called the domain and, for each symbol f ∈ Σ
with rank n, a total function fA : An → A, the
operation associated with f . We can evaluate a
term t ∈ TΣ to an object JtKA ∈ A by executing
the operations:

Jf(t1, . . . , tn)KA = fA(Jt1KA, . . . , JtnKA) .
Sets of trees can be specified by regular tree

grammars (RTGs) (Gécseg and Steinby, 1997;
Comon et al., 2007). Formally, such a grammar is
a structure G = (N,Σ,P, S), where N is a signa-
ture of nonterminal symbols, all of which are taken
to have rank 0, Σ is a signature of terminal sym-
bols, S ∈ N is a distinguished start symbol, and P
is a finite set of productions of the form B → t,
where B is a nonterminal symbol, and t ∈ TN∪Σ .
The productions of a regular tree grammar are used
as rewriting rules on terms. More specifically, the
derivation relation of G is defined as follows. Let
t1, t2 ∈ TN∪Σ be terms. Then G derives t2 from t1
in one step, denoted by t1 ⇒G t2, if there exists a
production of the form B → t and t2 can be ob-
tained by replacing an occurrence of B in t1 by t.
The (regular) language L(G) generated by G is the
set of all terms t ∈ TΣ that can be derived, in zero
or more steps, from the term S.

A (tree) homomorphism is a total function
h: TΣ → T∆ which expands symbols of Σ into
trees over ∆ while following the structure of the in-
put tree. Formally, h is specified by pairs (f, h(f)),
where f ∈ Σ is a symbol with some rank n, and
h(f) ∈ T∆∪{x1,...,xn} is a term with variables.
Given t ∈ TΣ , the value of t under h is defined as
h(f(t1, . . . , tn)) = h(f){h(ti)/xi | i ∈ [n] } ,

where { ti/xi | i ∈ [n] } represents the substitu-
tion that replaces all occurrences of xi with the
respective ti. A homomorphism is called linear
if every term h(f) contains each variable at most
once; and a delabeling if every term h(f) is of the
form g(xπ(1), . . . , xπ(n)) where n is the rank of f
and π a permutation of {1, . . . , n}.

3 Interpreted Regular Tree Grammars

We will now present a generalized framework for
synchronous and monolingual grammars in terms
of regular tree grammars, tree homomorphisms,
and algebras. We will illustrate the framework with
two simple examples here, but many other grammar
formalisms can be seen as special cases too, as we
will show in Section 6.

3.1 An Introductory Example

The derivation process of context-free grammar is
usually seen as a string-rewriting process in which
nonterminals are successively replaced by the right-
hand sides of production rules. The actual parse
tree is explained as a post-hoc description of the
rules that were applied in the derivation.

However, we can alternatively view this as a two-
step process which first computes a derivation tree
and then interprets it as a string. Say we have the
CNF grammar G in Fig. 2, and we want to derive
the string w = “Sue watches the man with the
telescope”. In the first step, we use G to generate a
derivation tree like the one in Fig. 2a. The nodes of
this tree are labeled with names of the production
rules in G; nodes with labels r7 and r3 are licensed
by G to be the two children of r1 because r1 has
the two nonterminals NP and VP in its right-hand
side, and the left-hand sides of r7 and r3 are NP
and VP, respectively. In a second step, we can then
interpret the derivation tree into w by interpreting
each leaf labeled with a terminal production (say,
r7) as the string on its right-hand side (“Sue”), and
each internal node as a string concatenation opera-
tion which arranges the string yields of its subtrees
in the order given by the right-hand side of the
production rule.

This view differs from the traditional perspec-
tive on context-free grammars in that it makes the
derivation tree the primary participant in the deriva-
tion process. The string is only one particular in-
terpretation of the derivation tree, and instead of
a string we could also have interpreted it as some
other kind of object. For instance, if we had inter-

3



Figure 1: Our unified perspective on grammar formalisms: (a) ordinary grammar formalisms; (b) synchronous for-
malisms; (c) multiple “inputs” and “outputs”.

result of this can be (non-standardly) recorded as a
derivation tree, whose nodes are labeled by names
of production rules, and in which a rule application
a1 is the child of another a2 if a1 introduced a non-
terminal occurrence that was expanded by a2. In a
second step, we can transform this derivation tree
into a string by interpreting each rule application as
a string-concatenation operation.

While this picture seems complicated for context-
free grammars by themselves, the separation into
two different generative processes (first a derivation
tree, then the string from the derivation tree) is appli-
cable much more widely and, we argue, widely use-
ful. The general picture looks as follows. Consider
a regular tree grammar G over a signature Σ, an al-
gebra A with signature ∆, and a homomorphism h :
TΣ → T∆. If we apply h to any tree t ∈ L(G), we
obtain a term over A, which we can interpret as an
element of A. By collecting all such terms, we ob-
tain a language LA(G, h) = {�h(t)�A | t ∈ L(G)}
of elements of A. This perspective is illustrated in
Fig. 1a.

We can define an obvious membership problem:
Given some element a ∈ A, is a ∈ LA(G, h)? We
can also define a parsing problem: For every ele-
ment a ∈ LA(G, h), compute (some compact repre-
sentation of)

parsesA,G,h(a) = {t ∈ L(G) | �h(t)�A = a}.
We call the trees over Σ derivation trees, and the
trees in parses(a) the derivation trees of a.

In the case of context-free grammars, it is known
that the language of derivation trees is a regular tree
language (Comon et al., 2007). It is defined by an
RTG G over the signature of production rule names
of the context-free grammar G. For every produc-
tion rule r of the form A → ω1A1 . . . Anωn+1
(where A and all Ai are nonterminals, and the ωi are

S → NP VP
VP → V NP
NP → John
NP → Mary

V → loves

r1

r3 r2

r5 r4

Figure 2: A context-free grammar and one of its deriva-
tion trees.

possibly empty strings of terminals), G contains a
rule A → r(A1, . . . , An). We interpret such deriva-
tion trees into strings in the string algebra As over
some terminal alphabet T ; the elements of this alge-
bra are the strings in T ∗, and we have constants for
the elements of T and a binary string concatenation
operation ·. As a last step, we use a homorphism h to
map each rule into a term over As: for the above rule
r, we have h(r) = ω1 · x1 · . . . · xn · ωn+1. It can be
shown that under this construction, LAs(G, h) is ex-
actly L(G), the string language of the original gram-
mar.

For illustration, consider the context-free gram-
mar in Fig. 2a, and let’s say we want to parse the
sentence “John loves Mary”. The RTG for the gram-
mar contains rules such as S → r1(NP, V P );
it generates the derivation tree shown in Fig. 2b.
This tree can now be interpreted using a homomor-
phism h with h(r1) = x1 · x2, h(r3) = John,
etc. h maps the derivation tree in Fig. 2b to the
term (John · loves) · Mary over As, which eval-
uates to the string “John loves Mary”. This means
that it is a derivation tree of that string. In fact,
parses(“John loves Mary”) is the set that contains
only this derivation tree.

string transducer); these differences simply amount
to the appropriate selection of algebras and homo-
morphisms.

Plan of the paper. The paper is structured as fol-
lows. We will start by laying the formal foundations
in Section 2. We will then show how to combine bi-
morphisms with algebraic interpretations and illus-
trate the approach on several grammar formalisms in
Section 3. We will define generic algorithms in Sec-
tion 4. Section 7 discusses related work, and Sec-
tion 8 concludes.

2 Formal foundations

A signature is a finite set Σ of function symbols f ,
each of which has been assigned a non-negative in-
teger called its rank. Given a signature Σ, we can
define a (finite constructor) tree over Σ as a finite
tree whose nodes are labeled with symbols from Σ
such that a node with a label of rank n has exactly n
children. We write TΣ for the set of all trees over Σ.
Trees can be written as terms; f(t1, . . . , tn) stands
for the tree with root label f and subtrees t1, . . . , tn.
The nodes of a tree can be identified by the paths
π ∈ N∗ from the root to the node: The root has ad-
dress �, and the i-th child of the node below path π
has the address πi. We write t(π) for the symbol at
path π in the tree t.

A Σ-algebra A consists of a non-empty set A
called the domain and, for each symbol f ∈ Σ with
rank m, a total function fA : Am → A, called the
operation associated with f . We can evaluate a term
t ∈ TΣ to an object �t�A ∈ A by executing the op-
erations:

�σ(t1, . . . , tm)�A = fAσ (�t1�A, . . . , �tm�A) .

Sets of trees can be specified by regular tree
grammars (Gécseg and Steinby, 1997; Comon et
al., 2007). Formally, such a grammar is a structure
G = (N, Σ, P, S), where N is a signature of nonter-
minal symbols, all of which are taken to have rank 0,
Σ is a signature of terminal symbols, S ∈ N is a
distinguished start symbol, and P is a finite set of
productions of the form B → t, where B is a non-
terminal symbol, and t ∈ TN∪Σ. The productions
of a regular tree grammar are used as rewriting rules
on terms. More specifically, the derivation relation
of G is defined as follows. Let t1, t2 ∈ TN∪Σ be

terms. Then G derives t2 from t1 in one step, de-
noted by t1 ⇒G t2, if there exists a production of
the form B → t and t2 can be obtained by replacing
an occurrence of B in t1 by t. The language L(G)
generated by G is the set of all terms t ∈ TΣ that can
be derived, in zero or more steps, from the term S.

A (tree) homomorphism is a function h : TΣ →
T∆ which expands symbols of Σ into (possibly mul-
tiple) symbols of ∆ while following the structure
of the input tree. Formally, h is defined by a term
h(f) ∈ T∆∪{x1,...,xn} for each f ∈ Σ, where n is
the rank of f and the xi are variable symbols of rank
0. Given a term t ∈ TΣ, h(t) is defined recursively
by

h(f(t1, . . . , tn)) = h(f){h(t1)/x1, . . . , h(tn)/xn},
where {t�1/x1, . . . , t�n/xn} represents a substitution
that replaces all occurrences of xi with the respec-
tive t�i. A homomorphism is called linear if every
term h(f) contains each variable at most once.

Finally, a tree transducer is a device M for de-
scribing binary relations between trees; the first tree
in each pair is usually seen as the input and the sec-
ond as the output. They generalize string transduc-
ers to the tree case and are defined in more detail in
(Comon et al., 2007). A useful way of thinking of
a tree transducer is in terms of bimorphisms. A bi-
morphism is a triple B = (h1,G, h2) of an RTG G
and two homomorphisms h1, h2; it represents the bi-
nary relation {(h1(t), h2(t)) | t ∈ L(G)}. vielleicht
brauchen wir das hier gar nicht

3 Grammar formalisms based on tree
automata

We will now present a unified framework of
synchronous and non-synchronous grammar for-
malisms in terms of regular tree languages, tree ho-
momorphisms, and algebras. We will illustrate the
framework using ordinary context-free grammars
and synchronous tree-substitution grammars, but the
framework is much more general than this, and we
will hint at this at the end of the section.

3.1 Ordinary grammars
The process of generating a string from a context-
free grammar G can be seen as a two-step process.
In a first step, we generate a derivation of G by ex-
panding nonterminals using production rules. The

h

Figure 1: Our unified perspective on grammar formalisms: (a) ordinary grammar formalisms; (b) synchronous for-
malisms; (c) multiple “inputs” and “outputs”.

result of this can be (non-standardly) recorded as a
derivation tree, whose nodes are labeled by names
of production rules, and in which a rule application
a1 is the child of another a2 if a1 introduced a non-
terminal occurrence that was expanded by a2. In a
second step, we can transform this derivation tree
into a string by interpreting each rule application as
a string-concatenation operation.

While this picture seems complicated for context-
free grammars by themselves, the separation into
two different generative processes (first a derivation
tree, then the string from the derivation tree) is appli-
cable much more widely and, we argue, widely use-
ful. The general picture looks as follows. Consider
a regular tree grammar G over a signature Σ, an al-
gebra A with signature ∆, and a homomorphism h :
TΣ → T∆. If we apply h to any tree t ∈ L(G), we
obtain a term over A, which we can interpret as an
element of A. By collecting all such terms, we ob-
tain a language LA(G, h) = {�h(t)�A | t ∈ L(G)}
of elements of A. This perspective is illustrated in
Fig. 1a.

We can define an obvious membership problem:
Given some element a ∈ A, is a ∈ LA(G, h)? We
can also define a parsing problem: For every ele-
ment a ∈ LA(G, h), compute (some compact repre-
sentation of)

parsesA,G,h(a) = {t ∈ L(G) | �h(t)�A = a}.
We call the trees over Σ derivation trees, and the
trees in parses(a) the derivation trees of a.

In the case of context-free grammars, it is known
that the language of derivation trees is a regular tree
language (Comon et al., 2007). It is defined by an
RTG G over the signature of production rule names
of the context-free grammar G. For every produc-
tion rule r of the form A → ω1A1 . . . Anωn+1
(where A and all Ai are nonterminals, and the ωi are

S → NP VP
VP → V NP
NP → John
NP → Mary

V → loves

r1

r3 r2

r5 r4

Figure 2: A context-free grammar and one of its deriva-
tion trees.

possibly empty strings of terminals), G contains a
rule A → r(A1, . . . , An). We interpret such deriva-
tion trees into strings in the string algebra As over
some terminal alphabet T ; the elements of this alge-
bra are the strings in T ∗, and we have constants for
the elements of T and a binary string concatenation
operation ·. As a last step, we use a homorphism h to
map each rule into a term over As: for the above rule
r, we have h(r) = ω1 · x1 · . . . · xn · ωn+1. It can be
shown that under this construction, LAs(G, h) is ex-
actly L(G), the string language of the original gram-
mar.

For illustration, consider the context-free gram-
mar in Fig. 2a, and let’s say we want to parse the
sentence “John loves Mary”. The RTG for the gram-
mar contains rules such as S → r1(NP, V P );
it generates the derivation tree shown in Fig. 2b.
This tree can now be interpreted using a homomor-
phism h with h(r1) = x1 · x2, h(r3) = John,
etc. h maps the derivation tree in Fig. 2b to the
term (John · loves) · Mary over As, which eval-
uates to the string “John loves Mary”. This means
that it is a derivation tree of that string. In fact,
parses(“John loves Mary”) is the set that contains
only this derivation tree.

string transducer); these differences simply amount
to the appropriate selection of algebras and homo-
morphisms.

Plan of the paper. The paper is structured as fol-
lows. We will start by laying the formal foundations
in Section 2. We will then show how to combine bi-
morphisms with algebraic interpretations and illus-
trate the approach on several grammar formalisms in
Section 3. We will define generic algorithms in Sec-
tion 4. Section 7 discusses related work, and Sec-
tion 8 concludes.

2 Formal foundations

A signature is a finite set Σ of function symbols f ,
each of which has been assigned a non-negative in-
teger called its rank. Given a signature Σ, we can
define a (finite constructor) tree over Σ as a finite
tree whose nodes are labeled with symbols from Σ
such that a node with a label of rank n has exactly n
children. We write TΣ for the set of all trees over Σ.
Trees can be written as terms; f(t1, . . . , tn) stands
for the tree with root label f and subtrees t1, . . . , tn.
The nodes of a tree can be identified by the paths
π ∈ N∗ from the root to the node: The root has ad-
dress �, and the i-th child of the node below path π
has the address πi. We write t(π) for the symbol at
path π in the tree t.

A Σ-algebra A consists of a non-empty set A
called the domain and, for each symbol f ∈ Σ with
rank m, a total function fA : Am → A, called the
operation associated with f . We can evaluate a term
t ∈ TΣ to an object �t�A ∈ A by executing the op-
erations:

�σ(t1, . . . , tm)�A = fAσ (�t1�A, . . . , �tm�A) .

Sets of trees can be specified by regular tree
grammars (Gécseg and Steinby, 1997; Comon et
al., 2007). Formally, such a grammar is a structure
G = (N, Σ, P, S), where N is a signature of nonter-
minal symbols, all of which are taken to have rank 0,
Σ is a signature of terminal symbols, S ∈ N is a
distinguished start symbol, and P is a finite set of
productions of the form B → t, where B is a non-
terminal symbol, and t ∈ TN∪Σ. The productions
of a regular tree grammar are used as rewriting rules
on terms. More specifically, the derivation relation
of G is defined as follows. Let t1, t2 ∈ TN∪Σ be

terms. Then G derives t2 from t1 in one step, de-
noted by t1 ⇒G t2, if there exists a production of
the form B → t and t2 can be obtained by replacing
an occurrence of B in t1 by t. The language L(G)
generated by G is the set of all terms t ∈ TΣ that can
be derived, in zero or more steps, from the term S.

A (tree) homomorphism is a function h : TΣ →
T∆ which expands symbols of Σ into (possibly mul-
tiple) symbols of ∆ while following the structure
of the input tree. Formally, h is defined by a term
h(f) ∈ T∆∪{x1,...,xn} for each f ∈ Σ, where n is
the rank of f and the xi are variable symbols of rank
0. Given a term t ∈ TΣ, h(t) is defined recursively
by

h(f(t1, . . . , tn)) = h(f){h(t1)/x1, . . . , h(tn)/xn},
where {t�1/x1, . . . , t�n/xn} represents a substitution
that replaces all occurrences of xi with the respec-
tive t�i. A homomorphism is called linear if every
term h(f) contains each variable at most once.

Finally, a tree transducer is a device M for de-
scribing binary relations between trees; the first tree
in each pair is usually seen as the input and the sec-
ond as the output. They generalize string transduc-
ers to the tree case and are defined in more detail in
(Comon et al., 2007). A useful way of thinking of
a tree transducer is in terms of bimorphisms. A bi-
morphism is a triple B = (h1,G, h2) of an RTG G
and two homomorphisms h1, h2; it represents the bi-
nary relation {(h1(t), h2(t)) | t ∈ L(G)}. vielleicht
brauchen wir das hier gar nicht

3 Grammar formalisms based on tree
automata

We will now present a unified framework of
synchronous and non-synchronous grammar for-
malisms in terms of regular tree languages, tree ho-
momorphisms, and algebras. We will illustrate the
framework using ordinary context-free grammars
and synchronous tree-substitution grammars, but the
framework is much more general than this, and we
will hint at this at the end of the section.

3.1 Ordinary grammars
The process of generating a string from a context-
free grammar G can be seen as a two-step process.
In a first step, we generate a derivation of G by ex-
panding nonterminals using production rules. The

h

string transducer); these differences simply amount
to the appropriate selection of algebras and homo-
morphisms.

Plan of the paper. The paper is structured as fol-
lows. We will start by laying the formal foundations
in Section 2. We will then show how to combine bi-
morphisms with algebraic interpretations and illus-
trate the approach on several grammar formalisms in
Section 3. We will define generic algorithms in Sec-
tion 4. Section 7 discusses related work, and Sec-
tion 8 concludes.

2 Formal foundations

A signature is a finite set Σ of function symbols f ,
each of which has been assigned a non-negative in-
teger called its rank. Given a signature Σ, we can
define a (finite constructor) tree over Σ as a finite
tree whose nodes are labeled with symbols from Σ
such that a node with a label of rank n has exactly n
children. We write TΣ for the set of all trees over Σ.
Trees can be written as terms; f(t1, . . . , tn) stands
for the tree with root label f and subtrees t1, . . . , tn.
The nodes of a tree can be identified by the paths
π ∈ N∗ from the root to the node: The root has ad-
dress �, and the i-th child of the node below path π
has the address πi. We write t(π) for the symbol at
path π in the tree t.

A Σ-algebra A consists of a non-empty set A
called the domain and, for each symbol f ∈ Σ with
rank m, a total function fA : Am → A, called the
operation associated with f . We can evaluate a term
t ∈ TΣ to an object �t�A ∈ A by executing the op-
erations:

�σ(t1, . . . , tm)�A = fAσ (�t1�A, . . . , �tm�A) .

Sets of trees can be specified by regular tree
grammars (Gécseg and Steinby, 1997; Comon et
al., 2007). Formally, such a grammar is a structure
G = (N, Σ, P, S), where N is a signature of nonter-
minal symbols, all of which are taken to have rank 0,
Σ is a signature of terminal symbols, S ∈ N is a
distinguished start symbol, and P is a finite set of
productions of the form B → t, where B is a non-
terminal symbol, and t ∈ TN∪Σ. The productions
of a regular tree grammar are used as rewriting rules
on terms. More specifically, the derivation relation
of G is defined as follows. Let t1, t2 ∈ TN∪Σ be

terms. Then G derives t2 from t1 in one step, de-
noted by t1 ⇒G t2, if there exists a production of
the form B → t and t2 can be obtained by replacing
an occurrence of B in t1 by t. The language L(G)
generated by G is the set of all terms t ∈ TΣ that can
be derived, in zero or more steps, from the term S.

A (tree) homomorphism is a function h : TΣ →
T∆ which expands symbols of Σ into (possibly mul-
tiple) symbols of ∆ while following the structure
of the input tree. Formally, h is defined by a term
h(f) ∈ T∆∪{x1,...,xn} for each f ∈ Σ, where n is
the rank of f and the xi are variable symbols of rank
0. Given a term t ∈ TΣ, h(t) is defined recursively
by

h(f(t1, . . . , tn)) = h(f){h(t1)/x1, . . . , h(tn)/xn},
where {t�1/x1, . . . , t�n/xn} represents a substitution
that replaces all occurrences of xi with the respec-
tive t�i. A homomorphism is called linear if every
term h(f) contains each variable at most once.

Finally, a tree transducer is a device M for de-
scribing binary relations between trees; the first tree
in each pair is usually seen as the input and the sec-
ond as the output. They generalize string transduc-
ers to the tree case and are defined in more detail in
(Comon et al., 2007). A useful way of thinking of
a tree transducer is in terms of bimorphisms. A bi-
morphism is a triple B = (h1,G, h2) of an RTG G
and two homomorphisms h1, h2; it represents the bi-
nary relation {(h1(t), h2(t)) | t ∈ L(G)}. vielleicht
brauchen wir das hier gar nicht

3 Grammar formalisms based on tree
automata

We will now present a unified framework of
synchronous and non-synchronous grammar for-
malisms in terms of regular tree languages, tree ho-
momorphisms, and algebras. We will illustrate the
framework using ordinary context-free grammars
and synchronous tree-substitution grammars, but the
framework is much more general than this, and we
will hint at this at the end of the section.

3.1 Ordinary grammars
The process of generating a string from a context-
free grammar G can be seen as a two-step process.
In a first step, we generate a derivation of G by ex-
panding nonterminals using production rules. The

'h'

Figure 1: Our unified perspective on grammar formalisms: (a) ordinary grammar formalisms; (b) synchronous for-
malisms; (c) multiple “inputs” and “outputs”.

result of this can be (non-standardly) recorded as a
derivation tree, whose nodes are labeled by names
of production rules, and in which a rule application
a1 is the child of another a2 if a1 introduced a non-
terminal occurrence that was expanded by a2. In a
second step, we can transform this derivation tree
into a string by interpreting each rule application as
a string-concatenation operation.

While this picture seems complicated for context-
free grammars by themselves, the separation into
two different generative processes (first a derivation
tree, then the string from the derivation tree) is appli-
cable much more widely and, we argue, widely use-
ful. The general picture looks as follows. Consider
a regular tree grammar G over a signature Σ, an al-
gebra A with signature ∆, and a homomorphism h :
TΣ → T∆. If we apply h to any tree t ∈ L(G), we
obtain a term over A, which we can interpret as an
element of A. By collecting all such terms, we ob-
tain a language LA(G, h) = {�h(t)�A | t ∈ L(G)}
of elements of A. This perspective is illustrated in
Fig. 1a.

We can define an obvious membership problem:
Given some element a ∈ A, is a ∈ LA(G, h)? We
can also define a parsing problem: For every ele-
ment a ∈ LA(G, h), compute (some compact repre-
sentation of)

parsesA,G,h(a) = {t ∈ L(G) | �h(t)�A = a}.
We call the trees over Σ derivation trees, and the
trees in parses(a) the derivation trees of a.

In the case of context-free grammars, it is known
that the language of derivation trees is a regular tree
language (Comon et al., 2007). It is defined by an
RTG G over the signature of production rule names
of the context-free grammar G. For every produc-
tion rule r of the form A → ω1A1 . . . Anωn+1
(where A and all Ai are nonterminals, and the ωi are

S → NP VP
VP → V NP
NP → John
NP → Mary

V → loves

r1

r3 r2

r5 r4

Figure 2: A context-free grammar and one of its deriva-
tion trees.

possibly empty strings of terminals), G contains a
rule A → r(A1, . . . , An). We interpret such deriva-
tion trees into strings in the string algebra As over
some terminal alphabet T ; the elements of this alge-
bra are the strings in T ∗, and we have constants for
the elements of T and a binary string concatenation
operation ·. As a last step, we use a homorphism h to
map each rule into a term over As: for the above rule
r, we have h(r) = ω1 · x1 · . . . · xn · ωn+1. It can be
shown that under this construction, LAs(G, h) is ex-
actly L(G), the string language of the original gram-
mar.

For illustration, consider the context-free gram-
mar in Fig. 2a, and let’s say we want to parse the
sentence “John loves Mary”. The RTG for the gram-
mar contains rules such as S → r1(NP, V P );
it generates the derivation tree shown in Fig. 2b.
This tree can now be interpreted using a homomor-
phism h with h(r1) = x1 · x2, h(r3) = John,
etc. h maps the derivation tree in Fig. 2b to the
term (John · loves) · Mary over As, which eval-
uates to the string “John loves Mary”. This means
that it is a derivation tree of that string. In fact,
parses(“John loves Mary”) is the set that contains
only this derivation tree.

string transducer); these differences simply amount
to the appropriate selection of algebras and homo-
morphisms.

Plan of the paper. The paper is structured as fol-
lows. We will start by laying the formal foundations
in Section 2. We will then show how to combine bi-
morphisms with algebraic interpretations and illus-
trate the approach on several grammar formalisms in
Section 3. We will define generic algorithms in Sec-
tion 4. Section 7 discusses related work, and Sec-
tion 8 concludes.

2 Formal foundations

A signature is a finite set Σ of function symbols f ,
each of which has been assigned a non-negative in-
teger called its rank. Given a signature Σ, we can
define a (finite constructor) tree over Σ as a finite
tree whose nodes are labeled with symbols from Σ
such that a node with a label of rank n has exactly n
children. We write TΣ for the set of all trees over Σ.
Trees can be written as terms; f(t1, . . . , tn) stands
for the tree with root label f and subtrees t1, . . . , tn.
The nodes of a tree can be identified by the paths
π ∈ N∗ from the root to the node: The root has ad-
dress �, and the i-th child of the node below path π
has the address πi. We write t(π) for the symbol at
path π in the tree t.

A Σ-algebra A consists of a non-empty set A
called the domain and, for each symbol f ∈ Σ with
rank m, a total function fA : Am → A, called the
operation associated with f . We can evaluate a term
t ∈ TΣ to an object �t�A ∈ A by executing the op-
erations:

�σ(t1, . . . , tm)�A = fAσ (�t1�A, . . . , �tm�A) .

Sets of trees can be specified by regular tree
grammars (Gécseg and Steinby, 1997; Comon et
al., 2007). Formally, such a grammar is a structure
G = (N, Σ, P, S), where N is a signature of nonter-
minal symbols, all of which are taken to have rank 0,
Σ is a signature of terminal symbols, S ∈ N is a
distinguished start symbol, and P is a finite set of
productions of the form B → t, where B is a non-
terminal symbol, and t ∈ TN∪Σ. The productions
of a regular tree grammar are used as rewriting rules
on terms. More specifically, the derivation relation
of G is defined as follows. Let t1, t2 ∈ TN∪Σ be

terms. Then G derives t2 from t1 in one step, de-
noted by t1 ⇒G t2, if there exists a production of
the form B → t and t2 can be obtained by replacing
an occurrence of B in t1 by t. The language L(G)
generated by G is the set of all terms t ∈ TΣ that can
be derived, in zero or more steps, from the term S.

A (tree) homomorphism is a function h : TΣ →
T∆ which expands symbols of Σ into (possibly mul-
tiple) symbols of ∆ while following the structure
of the input tree. Formally, h is defined by a term
h(f) ∈ T∆∪{x1,...,xn} for each f ∈ Σ, where n is
the rank of f and the xi are variable symbols of rank
0. Given a term t ∈ TΣ, h(t) is defined recursively
by

h(f(t1, . . . , tn)) = h(f){h(t1)/x1, . . . , h(tn)/xn},
where {t�1/x1, . . . , t�n/xn} represents a substitution
that replaces all occurrences of xi with the respec-
tive t�i. A homomorphism is called linear if every
term h(f) contains each variable at most once.

Finally, a tree transducer is a device M for de-
scribing binary relations between trees; the first tree
in each pair is usually seen as the input and the sec-
ond as the output. They generalize string transduc-
ers to the tree case and are defined in more detail in
(Comon et al., 2007). A useful way of thinking of
a tree transducer is in terms of bimorphisms. A bi-
morphism is a triple B = (h1,G, h2) of an RTG G
and two homomorphisms h1, h2; it represents the bi-
nary relation {(h1(t), h2(t)) | t ∈ L(G)}. vielleicht
brauchen wir das hier gar nicht

3 Grammar formalisms based on tree
automata

We will now present a unified framework of
synchronous and non-synchronous grammar for-
malisms in terms of regular tree languages, tree ho-
momorphisms, and algebras. We will illustrate the
framework using ordinary context-free grammars
and synchronous tree-substitution grammars, but the
framework is much more general than this, and we
will hint at this at the end of the section.

3.1 Ordinary grammars
The process of generating a string from a context-
free grammar G can be seen as a two-step process.
In a first step, we generate a derivation of G by ex-
panding nonterminals using production rules. The

h1

string transducer); these differences simply amount
to the appropriate selection of algebras and homo-
morphisms.

Plan of the paper. The paper is structured as fol-
lows. We will start by laying the formal foundations
in Section 2. We will then show how to combine bi-
morphisms with algebraic interpretations and illus-
trate the approach on several grammar formalisms in
Section 3. We will define generic algorithms in Sec-
tion 4. Section 7 discusses related work, and Sec-
tion 8 concludes.

2 Formal foundations

A signature is a finite set Σ of function symbols f ,
each of which has been assigned a non-negative in-
teger called its rank. Given a signature Σ, we can
define a (finite constructor) tree over Σ as a finite
tree whose nodes are labeled with symbols from Σ
such that a node with a label of rank n has exactly n
children. We write TΣ for the set of all trees over Σ.
Trees can be written as terms; f(t1, . . . , tn) stands
for the tree with root label f and subtrees t1, . . . , tn.
The nodes of a tree can be identified by the paths
π ∈ N∗ from the root to the node: The root has ad-
dress �, and the i-th child of the node below path π
has the address πi. We write t(π) for the symbol at
path π in the tree t.

A Σ-algebra A consists of a non-empty set A
called the domain and, for each symbol f ∈ Σ with
rank m, a total function fA : Am → A, called the
operation associated with f . We can evaluate a term
t ∈ TΣ to an object �t�A ∈ A by executing the op-
erations:

�σ(t1, . . . , tm)�A = fAσ (�t1�A, . . . , �tm�A) .

Sets of trees can be specified by regular tree
grammars (Gécseg and Steinby, 1997; Comon et
al., 2007). Formally, such a grammar is a structure
G = (N, Σ, P, S), where N is a signature of nonter-
minal symbols, all of which are taken to have rank 0,
Σ is a signature of terminal symbols, S ∈ N is a
distinguished start symbol, and P is a finite set of
productions of the form B → t, where B is a non-
terminal symbol, and t ∈ TN∪Σ. The productions
of a regular tree grammar are used as rewriting rules
on terms. More specifically, the derivation relation
of G is defined as follows. Let t1, t2 ∈ TN∪Σ be

terms. Then G derives t2 from t1 in one step, de-
noted by t1 ⇒G t2, if there exists a production of
the form B → t and t2 can be obtained by replacing
an occurrence of B in t1 by t. The language L(G)
generated by G is the set of all terms t ∈ TΣ that can
be derived, in zero or more steps, from the term S.

A (tree) homomorphism is a function h : TΣ →
T∆ which expands symbols of Σ into (possibly mul-
tiple) symbols of ∆ while following the structure
of the input tree. Formally, h is defined by a term
h(f) ∈ T∆∪{x1,...,xn} for each f ∈ Σ, where n is
the rank of f and the xi are variable symbols of rank
0. Given a term t ∈ TΣ, h(t) is defined recursively
by

h(f(t1, . . . , tn)) = h(f){h(t1)/x1, . . . , h(tn)/xn},
where {t�1/x1, . . . , t�n/xn} represents a substitution
that replaces all occurrences of xi with the respec-
tive t�i. A homomorphism is called linear if every
term h(f) contains each variable at most once.

Finally, a tree transducer is a device M for de-
scribing binary relations between trees; the first tree
in each pair is usually seen as the input and the sec-
ond as the output. They generalize string transduc-
ers to the tree case and are defined in more detail in
(Comon et al., 2007). A useful way of thinking of
a tree transducer is in terms of bimorphisms. A bi-
morphism is a triple B = (h1,G, h2) of an RTG G
and two homomorphisms h1, h2; it represents the bi-
nary relation {(h1(t), h2(t)) | t ∈ L(G)}. vielleicht
brauchen wir das hier gar nicht

3 Grammar formalisms based on tree
automata

We will now present a unified framework of
synchronous and non-synchronous grammar for-
malisms in terms of regular tree languages, tree ho-
momorphisms, and algebras. We will illustrate the
framework using ordinary context-free grammars
and synchronous tree-substitution grammars, but the
framework is much more general than this, and we
will hint at this at the end of the section.

3.1 Ordinary grammars
The process of generating a string from a context-
free grammar G can be seen as a two-step process.
In a first step, we generate a derivation of G by ex-
panding nonterminals using production rules. The

'

string transducer); these differences simply amount
to the appropriate selection of algebras and homo-
morphisms.

Plan of the paper. The paper is structured as fol-
lows. We will start by laying the formal foundations
in Section 2. We will then show how to combine bi-
morphisms with algebraic interpretations and illus-
trate the approach on several grammar formalisms in
Section 3. We will define generic algorithms in Sec-
tion 4. Section 7 discusses related work, and Sec-
tion 8 concludes.

2 Formal foundations

A signature is a finite set Σ of function symbols f ,
each of which has been assigned a non-negative in-
teger called its rank. Given a signature Σ, we can
define a (finite constructor) tree over Σ as a finite
tree whose nodes are labeled with symbols from Σ
such that a node with a label of rank n has exactly n
children. We write TΣ for the set of all trees over Σ.
Trees can be written as terms; f(t1, . . . , tn) stands
for the tree with root label f and subtrees t1, . . . , tn.
The nodes of a tree can be identified by the paths
π ∈ N∗ from the root to the node: The root has ad-
dress �, and the i-th child of the node below path π
has the address πi. We write t(π) for the symbol at
path π in the tree t.

A Σ-algebra A consists of a non-empty set A
called the domain and, for each symbol f ∈ Σ with
rank m, a total function fA : Am → A, called the
operation associated with f . We can evaluate a term
t ∈ TΣ to an object �t�A ∈ A by executing the op-
erations:

�σ(t1, . . . , tm)�A = fAσ (�t1�A, . . . , �tm�A) .

Sets of trees can be specified by regular tree
grammars (Gécseg and Steinby, 1997; Comon et
al., 2007). Formally, such a grammar is a structure
G = (N, Σ, P, S), where N is a signature of nonter-
minal symbols, all of which are taken to have rank 0,
Σ is a signature of terminal symbols, S ∈ N is a
distinguished start symbol, and P is a finite set of
productions of the form B → t, where B is a non-
terminal symbol, and t ∈ TN∪Σ. The productions
of a regular tree grammar are used as rewriting rules
on terms. More specifically, the derivation relation
of G is defined as follows. Let t1, t2 ∈ TN∪Σ be

terms. Then G derives t2 from t1 in one step, de-
noted by t1 ⇒G t2, if there exists a production of
the form B → t and t2 can be obtained by replacing
an occurrence of B in t1 by t. The language L(G)
generated by G is the set of all terms t ∈ TΣ that can
be derived, in zero or more steps, from the term S.

A (tree) homomorphism is a function h : TΣ →
T∆ which expands symbols of Σ into (possibly mul-
tiple) symbols of ∆ while following the structure
of the input tree. Formally, h is defined by a term
h(f) ∈ T∆∪{x1,...,xn} for each f ∈ Σ, where n is
the rank of f and the xi are variable symbols of rank
0. Given a term t ∈ TΣ, h(t) is defined recursively
by

h(f(t1, . . . , tn)) = h(f){h(t1)/x1, . . . , h(tn)/xn},
where {t�1/x1, . . . , t�n/xn} represents a substitution
that replaces all occurrences of xi with the respec-
tive t�i. A homomorphism is called linear if every
term h(f) contains each variable at most once.

Finally, a tree transducer is a device M for de-
scribing binary relations between trees; the first tree
in each pair is usually seen as the input and the sec-
ond as the output. They generalize string transduc-
ers to the tree case and are defined in more detail in
(Comon et al., 2007). A useful way of thinking of
a tree transducer is in terms of bimorphisms. A bi-
morphism is a triple B = (h1,G, h2) of an RTG G
and two homomorphisms h1, h2; it represents the bi-
nary relation {(h1(t), h2(t)) | t ∈ L(G)}. vielleicht
brauchen wir das hier gar nicht

3 Grammar formalisms based on tree
automata

We will now present a unified framework of
synchronous and non-synchronous grammar for-
malisms in terms of regular tree languages, tree ho-
momorphisms, and algebras. We will illustrate the
framework using ordinary context-free grammars
and synchronous tree-substitution grammars, but the
framework is much more general than this, and we
will hint at this at the end of the section.

3.1 Ordinary grammars
The process of generating a string from a context-
free grammar G can be seen as a two-step process.
In a first step, we generate a derivation of G by ex-
panding nonterminals using production rules. The

1

n hn

...

1

string transducer); these differences simply amount
to the appropriate selection of algebras and homo-
morphisms.

Plan of the paper. The paper is structured as fol-
lows. We will start by laying the formal foundations
in Section 2. We will then show how to combine bi-
morphisms with algebraic interpretations and illus-
trate the approach on several grammar formalisms in
Section 3. We will define generic algorithms in Sec-
tion 4. Section 7 discusses related work, and Sec-
tion 8 concludes.

2 Formal foundations

A signature is a finite set Σ of function symbols f ,
each of which has been assigned a non-negative in-
teger called its rank. Given a signature Σ, we can
define a (finite constructor) tree over Σ as a finite
tree whose nodes are labeled with symbols from Σ
such that a node with a label of rank n has exactly n
children. We write TΣ for the set of all trees over Σ.
Trees can be written as terms; f(t1, . . . , tn) stands
for the tree with root label f and subtrees t1, . . . , tn.
The nodes of a tree can be identified by the paths
π ∈ N∗ from the root to the node: The root has ad-
dress �, and the i-th child of the node below path π
has the address πi. We write t(π) for the symbol at
path π in the tree t.

A Σ-algebra A consists of a non-empty set A
called the domain and, for each symbol f ∈ Σ with
rank m, a total function fA : Am → A, called the
operation associated with f . We can evaluate a term
t ∈ TΣ to an object �t�A ∈ A by executing the op-
erations:

�σ(t1, . . . , tm)�A = fAσ (�t1�A, . . . , �tm�A) .

Sets of trees can be specified by regular tree
grammars (Gécseg and Steinby, 1997; Comon et
al., 2007). Formally, such a grammar is a structure
G = (N, Σ, P, S), where N is a signature of nonter-
minal symbols, all of which are taken to have rank 0,
Σ is a signature of terminal symbols, S ∈ N is a
distinguished start symbol, and P is a finite set of
productions of the form B → t, where B is a non-
terminal symbol, and t ∈ TN∪Σ. The productions
of a regular tree grammar are used as rewriting rules
on terms. More specifically, the derivation relation
of G is defined as follows. Let t1, t2 ∈ TN∪Σ be

terms. Then G derives t2 from t1 in one step, de-
noted by t1 ⇒G t2, if there exists a production of
the form B → t and t2 can be obtained by replacing
an occurrence of B in t1 by t. The language L(G)
generated by G is the set of all terms t ∈ TΣ that can
be derived, in zero or more steps, from the term S.

A (tree) homomorphism is a function h : TΣ →
T∆ which expands symbols of Σ into (possibly mul-
tiple) symbols of ∆ while following the structure
of the input tree. Formally, h is defined by a term
h(f) ∈ T∆∪{x1,...,xn} for each f ∈ Σ, where n is
the rank of f and the xi are variable symbols of rank
0. Given a term t ∈ TΣ, h(t) is defined recursively
by

h(f(t1, . . . , tn)) = h(f){h(t1)/x1, . . . , h(tn)/xn},
where {t�1/x1, . . . , t�n/xn} represents a substitution
that replaces all occurrences of xi with the respec-
tive t�i. A homomorphism is called linear if every
term h(f) contains each variable at most once.

Finally, a tree transducer is a device M for de-
scribing binary relations between trees; the first tree
in each pair is usually seen as the input and the sec-
ond as the output. They generalize string transduc-
ers to the tree case and are defined in more detail in
(Comon et al., 2007). A useful way of thinking of
a tree transducer is in terms of bimorphisms. A bi-
morphism is a triple B = (h1,G, h2) of an RTG G
and two homomorphisms h1, h2; it represents the bi-
nary relation {(h1(t), h2(t)) | t ∈ L(G)}. vielleicht
brauchen wir das hier gar nicht

3 Grammar formalisms based on tree
automata

We will now present a unified framework of
synchronous and non-synchronous grammar for-
malisms in terms of regular tree languages, tree ho-
momorphisms, and algebras. We will illustrate the
framework using ordinary context-free grammars
and synchronous tree-substitution grammars, but the
framework is much more general than this, and we
will hint at this at the end of the section.

3.1 Ordinary grammars
The process of generating a string from a context-
free grammar G can be seen as a two-step process.
In a first step, we generate a derivation of G by ex-
panding nonterminals using production rules. The

m
'

h'1

h'm
(c)(b)(a)

...

Figure 1: Interpreted tree grammars: (a) monolingual; (b) synchronous; (c) multiple “inputs” and “outputs”.

preted r7 as the tree N(Sue) and a rule like r1 as
a tree operation which takes the tree yields of its
two subtrees and inserts them as the children of a
root with label S, etc., the interpretation of a deriva-
tion tree would now be a tree; namely, an ordinary
parse tree of G. We could even interpret the same
derivation tree simultaneously as a string and as a
tree using two different interpretation functions.

3.2 Interpreted Regular Tree Grammars
While this view is unnecessarily complex for ex-
plaining context-free grammars alone, the separa-
tion into two different generative processes (first
derivation, then interpretation) is widely applicable.
In particular, the derivation part can be independent
of whetherw is a string or some other algebra of ob-
jects. We formalize our view as follows. First, we
need an interpretative component that maps deriva-
tion trees to terms of the relevant object algebra:

Definition 1 Let Σ be a signature. A Σ-interpre-
tation is a pair I = (h,A), whereA is a∆-algebra,
and h: TΣ → T∆ is a homomorphism. 2

We then capture the derivation process with a
single regular tree grammar, and connect it with
(potentially multiple) interpretations as follows:

Definition 2 An interpreted regular tree grammar
(IRTG) is a structure G = (G, I1, . . . , In), n ≥ 1,
where G is a regular tree grammar with terminal
alphabet Σ, and the Ii are Σ-interpretations. 2

Let Ii = (hi,Ai) be the ith interpretation. If we
apply the homomorphism hi to any tree t ∈ L(G),
we obtain a term hi(t), which we can evaluate to an
object ofAi. Based on this, we define the language
generated by G as follows. We write JtKIi as a
shorthand for Jhi(t)KAi .

L(G) = { 〈JtKI1 , . . . , JtKIn〉 | t ∈ L(G) }
Given this notion, we can define an obvious

membership problem: For a given tuple of objects
~a = 〈a1, . . . , an〉, is ~a ∈ L(G)? We can also de-
fine a parsing task: For every element ~a ∈ L(G),
compute (some compact representation of) the set

parsesG(~a) = { t ∈ L(G) | ∀i. JtKIi = ai }
We call the trees in this set the derivation trees of ~a.

3.3 Monolingual Grammars
Let us use these definitions to make our example
with context-free grammars as string-generating
devices precise. This is a case with a single inter-
pretation (n = 1), as illustrated in Fig. 1a.

We can adapt a standard construction (Goguen
et al., 1977). Let G be a context-free grammar with
nonterminals N , terminals T , and productions P .
We start by defining a regular tree grammar G. For
a string α ∈ (N ∪ T )∗, let nt(α) denote the string
of nonterminals in α, in the same order. We in-
clude into G all (and only) productions of the form
A → p(A1, . . . , Am), where p = A → α is a
production of G, and A1 · · ·Am = nt(α). Note
that by doing so, we view p as a symbol of rank
|nt(α)|. The nonterminals and the start symbol
of G are as for G. We now interpret the trees gen-
erated by G over the string algebra over T , which
we denote by T ∗. The domain of this algebra is
the set of all strings over T , and we have constants
for the symbols in T and the empty string, as well
as a single binary concatenation operation •. As
a last step, we use a homorphism rb to map each
rule of G into a term over the signature of T ∗:
For each production p of the form above, rb(p)
is the right-branching tree obtained from decom-
posing α into a series of concatenation operations,
where the nonterminal Ai is replaced with the vari-
able xi. Thus we have constructed an IRTG gram-
mar G = (G, (rb, T ∗)). It can be shown that under
this construction L(G) is exactly L(G), the string
language of the original grammar.

Consider the context-free grammar in Fig. 2.
The RTG G contains production rules such as
S→ r1(NP,VP); it generates an infinite language
of trees, including the derivation trees shown in
Fig. 2a and 2b. These trees can now be interpreted
using rb with rb(r1) = x1 •x2,1 rb(r7) = Sue, etc.
This maps the tree in Fig. 2a to the term
Sue•(watches•(the•(man•(with•(the•telescope)))))
over the signature of T ∗, which evaluates in the
algebra T ∗ to the string w mentioned earlier. Simi-
larly, rb maps the tree in Fig. 2b to the term

1Here and below, we write • in infix notation.

4



S

NP1 ↓ NP2 ↓loves

t

e1 ↓

e2 ↓

@

@

loves

NP

John

e

j*

NP

Mary

e

m*

S

NP NPloves

John Mary

t

e

e

@

@

loves j*
m*

α1 α2 α3
α1

α2 α3

(a) (b) (c)

Figure 3: Synchronous TSG: (a) a lexicon consisting of three tree pairs; (b) a derived tree; (c) a derivation tree.

r1 : S → NP VP
r2 : NP → Det N
r3 : VP → V NP
r4 : N → N PP
r5 : VP → VP PP
r6 : PP → P NP
r7 : NP → Sue
r8 : Det → the
r9 : N → man
r10 : N → telescope
r11 : V → watches
r12 : P → with

r1

r7 r3

r11 r2

r8 r4

r6
r2

r10

r9
r12

r8r1

r7 r5

r6

r12 r2

r10r8

r3
r2

r9

r11

r8

(a)

(b)

Figure 2: A CFG and two of its derivation trees.

Sue•((watches•(the•man))•(with•(the•telescope)))
This means that L(G) is a set of strings which in-

cludes w. The tree language L(G) contains further
trees, which map to strings other than w. Therefore
L(G) includes other strings, but the trees in Fig. 2a
and b are the only two derivation trees of w.

3.4 Synchronous Grammars

We can quite naturally represent grammars that
describe binary relations between objects, i.e. syn-
chronous grammars, as IRTGs with two interpre-
tations (n = 2). We write these interpretations
as IL = (hL,AL) (“left”) and IR = (hR,AR)
(“right”); see Fig. 1b.

Parsing with a synchronous grammar means to
compute (a compact representation of) the set of
all derivation trees for a given pair (aL, aR) from
the set AL × AR. This is precisely the parsing
task that we defined in Section 3.2. A related
task is decoding, in which we take the grammar
G = (G, IL, IR) as a translation device for map-
ping input objects aL ∈ AL to output objects
aR ∈ AR. We define the decoding task as the
task of computing, for a given aL ∈ AL, (a com-
pact representation of) the following set, where
GL = (G, IL):

decodesG(aL) = { JtKIR | t ∈ parsesGL(aL) }
To illustrate this with an example, consider the

case of synchronous tree-substitution grammars
(STSGs, Eisner (2003)). An STSG combines lexi-

con entries, as shown in Fig. 3a, into larger derived
trees by replacing corresponding substitution nodes
with trees from other lexicon entries. In the figure,
we have marked the correspondence with numeric
subscripts. The trees in Fig. 3a can be combined
into the derived tree in Fig. 3b in two steps; this
process is recorded in the derivation tree in Fig. 3c.

We capture an STSG GS as an IRTG G by inter-
preting the (regular) language of derivation trees
in appropriate tree algebras. The tree algebra T∆
over some signature ∆ consists of all trees over ∆;
every symbol f ∈ ∆ of rank m is interpreted as
an m-place operation that returns the tree with root
symbol f and its arguments as subtrees. To model
STSG, we use the two tree algebras over all the
symbols occurring in the left and right components
of the lexicon entries, respectively. We can obtain
an RTG G for the derivation trees using a standard
construction (Schmitz and Le Roux, 2008; Shieber,
2004); its nonterminals are pairs 〈AL, AR〉 of non-
terminals occurring in the left and right trees ofGS .
To encode a lexicon entry α with root nonterminals
AL and AR, left substitution nodes A1L, . . . , A

n
L,

and right substitution nodes A1R, . . . , A
n
R, we add

an RTG rule of the form

〈AL, AR〉 → α(〈A1L, A1R〉, . . . , 〈AnL, AnR〉) .
We also let hL(α) and hR(α) be the left and right
tree of α, with substitution nodes replaced by vari-
ables; hL and hR interpret derivation trees into
derived trees in tree algebras TΣ and T∆ of appro-
priate (and possibly different) signatures. In the
example, we obtain

〈S, t〉 → α1(〈NP, e〉, 〈NP, e〉) ,
hL(α1) = S(x1, loves, x2) , and

hR(α1) = t(@(@(loves, x2), x1)) .

The variables reflect the corresponding substitution
nodes. So if we let G = (G, (hL, TΣ), (hR, T∆)),
L(G) will be a language of pairs of derived trees,
including the pair in Fig. 3b.

Parsing as defined above amounts to computing
a common derivation tree for a given pair of derived
trees; given only a left derived tree, the decoding

5



problem is to compute the corresponding right de-
rived trees. However, in an application of STSG to
machine translation or semantic construction, we
are typically given a string as the left input and
want to decode it to a right output tree. We can
support this by left-interpreting the derivation trees
directly as strings: We use the appropriate string
algebra T ∗ (consisting of the symbols of Σ with
arity zero) for AL, and map every lexicon entry
to a term that concatenates the string yields of the
elementary trees. In the example grammar, we can
let h′L(α1) = ((x1 • loves) • x2), h′L(α2) = John,
and h′L(α3) = Mary. With this local change, we
obtain a new IRTG G′ = (G, (h′L, T ∗), (hR, T∆)),
whose language contains pairs of a (left) string
and a (right) tree. One such pair has the string
“John loves Mary” as the left and the right-hand
tree in Fig. 3b as the right component. There-
fore decodes(“John loves Mary”), i.e. the set of
right derived trees that are consistent with the input
string, contains the right-hand tree in Fig. 3b.

We conclude this section by remarking that de-
coding can be easily generalized to n input objects
and m output objects, all of which can be taken
from different algebras (see Fig. 1c).

4 Algorithms

In the previous section, we have taken a view on
parsing and translation in which languages and
translations are obtained as the interpretation of
regular tree grammars. One advantage of this way
of looking at things is that it is possible to define
completely generic parsing algorithms by exploit-
ing closure properties of regular tree languages.

4.1 Parsing
The fundamental problem that we must solve is to
compute, for a given IRTG G = (G, (h,A)) and
object a ∈ A, a regular tree grammar Ga such that
L(Ga) = parsesG(a). A parser for IRTGs with
multiple interpretations follows from this immedi-
ately. Assume that G = (G, I1, . . . , In); then

parsesG(a1, . . . , an) =
n⋂
i=1

parses(G,Ii)(ai) .

Because regular tree languages are closed under in-
tersection, we can parse the different ai separately
and then intersect all the Gai .

The general idea of our parsing algorithm is as
follows. Suppose we were able to compute the
set termsA(a) of all possible terms t over A that

evaluate to a. Then parsesG(a) can be written as
h−1(termsA(a)) ∩ L(G). Of course, termsA(a)
may be a large or infinite set, so computing it in
general algebras is infeasible. But now assume
an algebra A in which termsA(a) is a regular tree
language for every a ∈ A, and in which we can
compute, for each a, a regular tree grammar D(a)
with L(D(a)) = termsA(a). Since regular tree
languages are effectively closed under both inverse
homomorphisms and intersections (Comon et al.,
2007), we obtain a parsing algorithm which first
computes D(a), and then Ga as the grammar for
h−1(L(D(a))) ∩ L(G).

Formally, this can be done for the following class
of algebras.

Definition 3 A Σ-algebraA is called regularly de-
composable if there is a computable function D(·)
which maps every object a ∈ A to a regular tree
grammar D(a) such that L(D(a)) = termsA(a).2

Consider the example of context-free grammars.
We have shown in Section 3.3 how these can be
seen as an IRTG with an interpretation into T ∗. The
string algebra T ∗ is regularly decomposable be-
cause the possible term representations of a string
simply correspond to its bracketings: For a string
w = w1 · · ·wn, the grammar D(w) consists of a
rule Ai−1,i → wi for each 1 ≤ i ≤ n, and a rule
Ai,k → Ai,j • Aj,k for all 0 ≤ i < j < k ≤ n. In
our example “Sue watches the man with the tele-
scope” from Section 3.2, these are rules such as
A2,3 → the, A3,4 → man, A2,4 → A2,3 • A3,4,
and so on. The grammar generates a tree language
consisting of the 132 binary bracketings of the sen-
tence, including the two mentioned in Section 3.3.

Tree algebras are an even simpler example of a
regularly decomposable algebra. For a given tree
t ∈ TΣ , the grammar D(t) consists of the rules
Aπ → f(Aπ1, . . . , Aπn) for all nodes π in t with
label f . D(t) generates a language that contains a
single tree, namely t itself. Thus we can use the
parsing algorithm to parse tree inputs (say, in the
context of an STSG) just as easily as string inputs.

4.2 Computing Inverse Homomorphisms

The performance bottleneck of the parsing algo-
rithm is the computation of the inverse homo-
morphisms. The input of this problem is h and
D(a); the task is to compute an RTG H′ that
uses terminal symbols from the signature Σ of
G and the same nonterminals as D(a), such that
h(L(H′)) = L(D(a)). This problem is nontrivial

6



h(f)(π) = xi A ∈ ND(a)
[f, π,A, {A/xi}]

(var)

A→ g(A1, . . . , An) inH h(f)(π) = g
[f, π1, A1, σ1] · · · [f, πn,An, σn]

σ = merge(σ1, . . . , σn) 6= fail
[f, π,A, σ]

(up)

Figure 4: Algorithm for computing h−1(H).

because h may not be a delabeling, so a term h(f)
may have to be parsed by multiple rule applications
in D(a) (see e.g. h′L(α1) in Section 3.4), and we
cannot simply take the homomorphic pre-images
of the production rules of D(a). One approach
(Comon et al., 2007) is to generate all possible
production rules A → f(B1, . . . , Bm) out of ter-
minals f ∈ Σ and D(a)-nonterminals and check
whether A ⇒∗D(a) h(f(B1, . . . , Bm)). Unfortu-
nately, this algorithm blindly combines arbitrary tu-
ples of nonterminals. For parsing with context-free
grammars in Chomsky normal form, this approach
leads to an O(n4) parsing algorithm.

The problem can be solved more efficiently by
the algorithm in Fig. 4. This algorithm computes
an RTGH′ for h−1(L(H)), whereH is an RTG in
a normal form in which every rule contains a sin-
gle terminal symbol; bringing a grammar into this
form only leads to a linear size increase (Gécseg
and Steinby, 1997). The algorithm derives items of
the form [f, π,A, σ], stating that H can generate
the subtree of h(f)σ at node π if it uses A as the
start symbol; the substitution σ is responsible for
replacing the variables in h(f) by nonterminal sym-
bols. It starts by guessing all possible instantiations
of each variable in h(f) (rule var). It then com-
putes items bottom-up, deriving an item [f, π,A, σ]
if there is a rule in H that can combine the non-
terminals derived for the children π1, . . . , πn of π
into A (rule up). The substitution σ is obtained by
merging all mappings in the σ1, . . . , σn; if some
σi, σj assign different nonterminals to the same
variable, the rule fails.

Whenever the algorithm derives an item of
the form [f, �, A, σ], it has processed a com-
plete tree h(f), and we add a production A →
f(σ(x1), . . . , σ(xn)) to H′; for variables xi on
which σ is undefined, we let σ(xi) = $ for the
special nonterminal $. We also add rules to H′
which generate any tree from TΣ out of $.

The complexity of this algorithm is bounded by
the number of instances of the up rule (McAllester,
2002). For parsing with context-free grammars, up
is applied to rules of the form Al,r → Al,k • Ak,r
of D(w); the premises are [f, π1, Al,k, σ1] and
[f, π2, Ak,r, σ2] and the conclusion [f, π,Al,r, σ].
The substitution σ defines a segmentation of the
substring between positions l and r into m − 1
smaller substrings, where m is the number of vari-
ables in the domain of σ. So the instances of up are
uniquely determined by at most m+ 1 string posi-
tions, where m is the total number of variables in
the tree h(f); the parsing complexity is O(nm+1).
By our encoding of context-free grammars into
IRTGs, m corresponds to the maximal number of
nonterminals in the right-hand side of a production
of the original grammar. In particular, the generic
algorithm parses Chomsky normal form grammars
(where m = 2) in cubic time, as expected.

4.3 Parse Charts

We will now illustrate the operation of the parsing
algorithm with our example context-free grammar
from Fig. 2 and our example sentence w = “Sue
watches the man with the telescope”. We first com-
pute D(w), which generates all bracketings of the
sentence. Next, we use the algorithm in Fig. 4 to
compute a grammarH′ for h−1(L(D(w))) for the
language of all derivation trees that are mapped by
h to a term evaluating to w. H′ contains rules such
as A2,4 → r2(A2,3, A3,4), A3,5 → r2(A3,4, A4,5),
and A3,4 → man. That is, H′ uses terminal sym-
bols from Σ, but the nonterminals from D(w). Fi-
nally, we intersectH′ with G to retain only deriva-
tion trees that are grammatical according to G.
We obtain a grammar Gw for parses(w), which is
shown in Fig. 5 (we have left out unreachable and
unproductive rules). The nonterminals of Gw are
pairs of the form (N,Ai,k), i.e. nonterminals of G
andH′; we abbreviate these pairs asNi,k. Note that
L(Gw) consists of exactly two trees, the derivation
trees shown in Fig. 2.

There is a clear parallel between the RTG in
Fig. 5 and a parse chart of the CKY parser for the
same input. The RTG describes how to build larger
parse items from smaller ones, and provides exactly
the same kind of structure sharing for ambiguous
sentences that the CKY chart would. For all intents
and purposes, the RTG Gw is a parse chart. When
we parse grammars of other formalisms, such as
STSG, the nonterminals of Ga generally record non-

7



terminals of G and positions in the input objects, as
encoded in the nonterminals of D(a1), . . . , D(an);
the spans [i, k] occurring in CKY parse items sim-
ply happen to be the nonterminals of the D(a) for
the string algebra.

In fact, we maintain that the fundamental pur-
pose of a chart is to act as a device for generating
the set of derivation trees for an input. This tree-
generating nature of parse charts is made explicit by
modeling them directly as RTGs; the well-known
view of parse charts as context-free grammars (Bil-
lot and Lang, 1989) captures the same intuition, but
abuses context-free grammars (which are primarily
string-generating devices) as tree description for-
malisms. One difference between the two views
is that regular tree languages are closed under in-
tersection, which means that parse charts that are
modeled as RTGs can be easily restricted by ex-
ternal constraints (see Koller and Thater (2010)
for a related approach), whereas this is hard in the
context-free view.

4.4 Decoding

We conclude this section by explaining how to
solve the decoding problem. Suppose that in the
scenario of Fig. 1c, we have obtained a parse chart
G~a for a tuple ~a = 〈a1, . . . , an〉 of inputs, if neces-
sary by intersecting the individual parse charts Gai .
Decoding means that we want to compute RTGs
for the languages h′j(L(G~a)) where j ∈ [m]. The
actual output objects can be obtained from these
languages of terms by evaluating the terms.

In the case where the homomorphisms h′j are
linear, we can once again exploit closure proper-
ties: Regular tree languages are closed under the
application of linear homomorphisms (Comon et
al., 2007), and therefore we can apply a standard
algorithm to compute the output RTGs from the
parse chart. In the case of non-linear homomor-
phisms, the output languages are not necessarily
regular, so decoding exceeds the expressive capac-
ity of our framework. However, linear output ho-
momorphisms are frequent in practice; see e.g. the
analysis of synchronous grammar formalisms in
(Shieber, 2004; Shieber, 2006). Some of the work-
load of a non-linear homomorphism may also be
carried by the output algebra, whose operations
may copy or delete material freely (as long as the
algebra remains regularly decomposable). Notice
that non-linear input homomorphisms are covered
by the algorithm in Fig. 4.

S0,7 → r1(NP0,1, VP1,7) NP5,7 → r2(Det5,6, N6,7)
VP1,7 → r3(V1,2, NP2,7) NP0,1 → r7
VP1,7 → r5(VP1,4, PP4,7) V1,2 → r11
NP2,7 → r2(Det2,3, N3,7) Det2,3 → r8

N3,7 → r4(N3,4, PP4,7) N3,4 → r9
VP1,4 → r3(V1,2, NP2,4) P4,5 → r12
NP2,4 → r2(Det2,3, N3,4) Det5,6 → r8
PP4,7 → r6(P4,5, NP5,7) N6,7 → r10

Figure 5: A “parse chart” RTG for the sentence “Sue
watches the man with the telescope”.

5 Membership and Binarization

A binarization transforms an m-ary grammar into
an equivalent binary one. Binarization is essen-
tial for achieving efficient recognition algorithms,
in particular the usual O(n3) time algorithms for
context-free grammars, and O(n6) time recogni-
tion of synchronous context-free grammars. In this
section, we discuss binarization in terms of IRTGs.

5.1 Context-Free Grammars

We start with a discussion of parsing context-free
grammars. Let G = (G, (rb, T ∗)) be a CFG as
we defined it in Section 3.3. We have shown in
Section 4.2 that our generic parsing algorithm pro-
cesses a sentencew = w1 . . . wn in timeO(nm+1),
where m is the maximal number of nonterminal
symbols in the right-hand side of the grammar. To
achieve the familiar cubic time complexity, an al-
gorithm needs to convert the grammar into a binary
form, either explicitly (by converting it to Chomsky
normal form) or implicitly (as in the case of the
Earley algorithm, which binarizes on the fly).

Strictly speaking, no algorithm that works on the
binarized grammar is a parsing algorithm in the
sense of ‘parsing’ as we defined it above. Under
our view of things, such an algorithm does not
compute the set parsesG(w) of derivation trees ofw
according to the grammar G, but according to a
second, binarized grammar G′ = (G′, (rb, T ∗)).
The binarization then takes the form of a function
bin that transforms terms over the signature of the
RTG G into terms over the binary signature of the
RTG G′. For a binarized grammar, we have m =
2, and so the parsing complexity is O(n3) plus
whatever time it takes to compute G′ from G andw.
Standard binarization techniques of context-free
grammars are linear in the size of the grammar.

Although binarization does not simplify pars-
ing in the sense of this paper, it does simplify
the membership problem of G: Given a string

8



G

bin

G2

A1 A2

h1

h21

h2

h22

Figure 6: Binarization.

w ∈ T ∗, is there some derivation tree t ∈ G
such that Jh(t)K = w? Because L(G) = L(G′),
this question can be decided by testing the empti-
ness of parsesG′(w), without the need to compute
parsesG(w). Furthermore, the set parsesG′(w) is
useful not only for deciding membership in L(G),
but also for computing other quantities, such as
inside probabilities of derivation trees of G.

5.2 Synchronous Context-Free Grammars

Synchronous context-free grammars can be repre-
sented as IRTGs along the same lines as STSG
grammars in Section 3.4. The resulting gram-
mar G = (G, (h1, T ∗1 ), (h2, T ∗2 )) consists of two
‘context-free’ interpretations of the RTG G into
string algebras T ∗1 and T

∗
2 ; as above, the synchro-

nization is ensured by requiring that related strings
in T ∗1 and T

∗
2 are interpretations of the same deriva-

tion tree t ∈ L(G). As above, we can parse
synchronously by parsing separately for the two
interpretations and intersecting the results. This
yields a parsing complexity for SCFG parsing of
O(nm+11 · nm+12 ), where n1 and n2 are the lengths
of the input strings and m is the rank of the RTG G.
Unlike in the monolingual case, this is now consis-
tent with the result that the membership problem of
SCFGs is NP-complete (Satta and Peserico, 2005).

The reason for the intractability of SCFG pars-
ing is that SCFGs, in general, cannot be binarized.
However, Huang et al. (2009) define the class of
binarizable SCFGs, which can be brought into a
weakly equivalent normal form in which all produc-
tion rules are binary and the membership problem
can be solved in time O(n31 ·n32). The key property
of binarizable SCFGs, in our terms, is that if r is
any production rule pair of the SCFG, h1(r) and
h2(r) can be chosen in such a way that they can be
transformed into each other by locally swapping
the subterms of a node. For instance, an SCFG rule
pair 〈A → A1 A2 A3 A4, B → B3 B4 B2 B1〉
can be represented by h1(r) = (x1 •x2)• (x3 •x4)
and h2(r) = (x4•x3)•(x1•x2), and h2(r) can be
obtained from h1(r) by swapping the children of

the nodes � and 2. In such a situation, we can bina-
rize the rule 〈A,B〉 → r(〈A1, B1〉, . . . , 〈A4, B4〉)
in a way that follows the structure of h1(r), e.g.

〈A,B〉 → r�1(〈Ar1, Br1〉, 〈Ar2, Br2〉)
〈Ar1, Br1〉 → r11(〈A1, B1〉, 〈A2, B2〉)
〈Ar2, Br2〉 → r21(〈A3, B3〉, 〈A4, B4〉)

We can then encode the local rotations in two
new left and right homomorphisms h21 and h

2
2, i.e.

h21(r
�
1) = h

2
1(r

1
1) = h

2
1(r

2
1) = h

2
2(r

2
1) = x1 •

x2, h22(r
�
1) = h

2
2(r

1
1) = x2 • x1. To determine

membership of some (a1, a2) inL(G), we compute
the pre-images of D(a1) and D(a2) under h21 and
h21 and intersect them with the binarized version,
G2, of G. This can be done in time O(n31 · n32).

5.3 A Generalized View on Binarization
The common theme of both examples we have just
discussed is that binarization, when it is available,
allows us to solve the membership problem in less
time than the parsing problem. A lower bound for
the membership problem of a tuple 〈a1, . . . , an〉 of
inputs is O(|D(a1)| · · · |D(an)|), because the pre-
images of the D(ai) grammars are at least as big
as the grammars themselves, and the intersection
algorithm computes the product of these. This
means that a membership algorithm is optimal if it
achieves this runtime.

As we have illustrated above, the parsing algo-
rithm from Section 4 is not optimal for monolin-
gual context-free membership, because the RTG G
has a higher rank than D(a), and therefore permits
too many combinations of input spans into rules.
The binarization constructions above indicate one
way towards a generic optimal membership algo-
rithm. Assume that we have algebras A1, . . . ,An,
all of which over signatures with maximum rank
k, and an IRTG G = (G, (h1,A1), . . . , (hn,An)),
where G is an RTG over some signature Σ. As-
sume further that we have some other signature
∆, of maximum rank k, and a homomorphism
bin : TΣ → T∆. We can obtain a RTG G2
with L(G2) = bin(L(G)) as in the SCFG exam-
ple above. Now assume that there are delabelings
h2i : T∆ → TAi such that h2i (L(G2)) = hi(L(G))
for all i ∈ [n] (see Fig. 6). Then we can decide
membership of a tuple 〈a1, . . . , an〉 by intersecting
G2 with all the (h2i )−1(L(D(ai))). Because the
h2i are delabelings, computing the pre-images can
be done in linear time; therefore this membership
algorithm is optimal. Notice that if the result of

9



the intersection is the RTGH, then we can obtain
parses(a1, . . . , an) = bin−1(L(H)); this is where
the exponential blowup can happen.

The constructions in Sections 5.1 and 5.2 are
both special cases of this generalized approach,
which however also maintains a clear connection to
the strong generative capacity. It is not obvious to
us that it is necessary that the homomorphisms h2i
must be delabelings for the membership algorithm
to be optimal. Exploring this landscape, which ties
in with the very active current research on binariza-
tion, is an interesting direction for future research.

6 Discussion and Related Work

We conclude this paper by discussing how a num-
ber of different grammar formalisms from the lit-
erature relate to IRTGs, and use this discussion to
highlight a number of features of our framework.

6.1 Tree-Adjoining Grammars

We have sketched in Section 3.4 how we can cap-
ture tree-substitution grammars by assuming an
RTG for the language of derivation trees and a ho-
momorphism into the tree algebra which spells out
the derived trees; or alternatively, a homomorphism
into the string algebra which computes the string
yield. This construction can be generalized to tree-
adjoining grammars (Joshi and Schabes, 1997).

Assume first that we are only interested in the
string language of the TAG grammar. Unlike in
TSG, the string yield of a derivation tree in TAG
may be discontinuous. We can model this with
an algebra whose elements are strings and pairs
of strings, along with a number of different con-
catenation operators that represent possible ways in
which these elements can be combined. (These are
a subset of the operations considered by Gómez-
Rodríguez et al. (2010).) We can then specify a
homomorphism, essentially the binarization proce-
dure that Earley-like TAG parsers do on the fly, that
maps derivation trees into terms over this algebra.
The TAG string algebra is regularly decomposable,
and D(a) can be computed in time O(n6).

Now consider the case of mapping derivation
trees into derived trees. This cannot easily be done
by a homomorphic interpretation in an ordinary tree
algebra. One way to deal with this, which is taken
by Shieber (2006), is to replace homomorphisms
by a more complex class of tree translation func-
tions called embedded pushdown tree transducers.
A second approach is to interpret homomorphically

into a more powerful algebra. This approach is
taken by Maletti (2010), who uses an ordinary tree
homomorphism to map a derivation tree t into a
tree t′ of ‘building instructions’ for a derived tree,
and then applies a function ·E to execute these
building instructions and build the TAG derived
tree. Maletti’s approach fits nicely into our frame-
work if we assume an algebra in which the building
instruction symbols are interpreted according to ·E .

Synchronous tree-adjoining grammars (Shieber
and Schabes, 1990) can be modeled simply as an
RTG with two separate TAG interpretations. We
can separately choose to interpret each side as trees
or strings, as described in Section 3.4.

6.2 Weighted Tree Transducers

One influential approach to statistical syntax-based
machine translation is to use weighted transducers
to map parse trees for an input language to parse
trees or strings of the output language (Graehl et al.,
2008). Bottom-up tree transducers can be modeled
in terms of bimorphisms, i.e. triples (hL,G, hR) of
an RTG G and two tree homomorphisms hL and hR
that map a derivation t ∈ L(G) into the input tree
hL(t) and the output tree hR(t) of the transducer
(Arnold and Dauchet, 1982). Thus bottom-up trans-
ducers fit into the view of Fig. 1b. Although Graehl
et al. use extended top-down transducers and not
bottom-up transducers, a first inspection of their
transducers leads us to believe that nothing hinges
on this specific choice for their application. The
exact situation bears further investigation.

Graehl et al.’s transducers further differ from
the setup we have presented above in that they are
weighted, i.e. each derivation step is associated
with a numeric weight (e.g., a probability), and
we can ask for the optimum derivation for a given
input. Our framework can be straightforwardly
extended to cover this case by assuming that the
RTG G is a weighted RTG (wRTG, Knight and
Graehl (2005)). The parsing algorithm from Sec-
tion 4.1 generalizes to an algorithm for computing
a weighted chart RTG, from which the best deriva-
tion can be extracted efficiently (Knight and Graehl,
2005). Similarly, the decoding algorithm from Sec-
tion 4.4 can be used to compute a weighted RTG for
the output terms, and an algorithm for EM training
can be defined directly on the weighted charts. In
general, every grammar formalism that can be cap-
tured as an IRTG has a canonical weighted variant
in this way. As probabilistic grammar formalisms,

10



these assume that all RTG rule applications are
statistically independent. That is, the canonical
probabilistic version of context-free grammars is
PCFG, and the canonical probabilistic version of
tree-adjoining grammar is PTAG (Resnik, 1992).

A final point is that Graehl et al. invest consider-
able effort into defining different versions of their
transducer training algorithms for the tree-to-tree
and tree-to-string translation cases. The core of
their paper, in our terms, is to define synchronous
parsing algorithms to compute an RTG of deriva-
tion trees for (tree, tree) and (tree, string) input
pairs. In their setup, these two cases are formally
completely different objects, and they define two
separate algorithms for these problems. Our ap-
proach is more modular: The training and parsing
algorithms can be fully generic, and all that needs
to be changed to switch between tree-to-tree and
tree-to-string is to replace the algebra and homo-
morphism on one side, as in Section 3.4. In fact, we
are not limited to interpreting derivation trees into
strings or trees; by interpreting into the appropriate
algebras, we can also describe languages of graphs
(Eaton et al., 2007), pictures (Drewes, 2006), 3D
models (Bokeloh et al., 2010), and other objects
with a suitable algebraic structure.

6.3 Generalized Context-Free Grammars

Finally, the view we advocate here embraces a
tradition of grammar formalisms going back to
generalized context-free grammar (GCFG, Pollard
(1984)), which follows itself research in theoret-
ical computer science (Mezei and Wright, 1967;
Goguen et al., 1977). A GCFG grammar can be
seen as an RTG over a signature Σ whose trees are
evaluated as terms of some Σ-algebra A. This is
a special case of an IRTG, in which the homomor-
phism is simply the identical function on TΣ , and
the algebra is A. In fact, we could have equiva-
lently defined an IRTG as an RTG whose trees are
interpreted in multiple Σ-algebras; the mediating
homomorphisms do not add expressive power. We
go beyond GCFG in three ways. First, the fact
that we map the trees described by the RTG into
terms of other algebras using different homomor-
phisms means that we can choose the signatures of
these algebras and the RTG freely; in particular, we
can reuse common algebras such as T ∗ for many
different RTGs and homomorphisms. This is espe-
cially important in relation to the second advance,
which is that we offer a generic parsing algorithm

for arbitrary regularly decomposable algebras; be-
cause the algebras and RTGs are modular, we can
reuse algorithms for computing D(a) even when
we change the homomorphism. Finally, we offer a
more transparent view on synchronous grammars,
which separates the different dimensions clearly.

An important special case of GCFG is that of
linear context-free rewrite systems (LCFRS, Vijay-
Shanker et al. (1987)). LCFRSs are essentially
GCFGs with a “yield” homomorphism that maps
objects of A to strings or tuples of strings. There-
fore every grammar formalism that can be seen as
an LCFRS, including certain dependency grammar
formalisms (Kuhlmann, 2010), can be phrased as
string-generating IRTGs. One particular advantage
that our framework has over LCFRS is that we
do not need to impose a bound on the length of
the string tuples. This makes it possible to model
formalisms such as combinatory categorial gram-
mar (Steedman, 2001), which may be arbitrarily
discontinuous (Koller and Kuhlmann, 2009).

7 Conclusion

In this paper, we have defined interpreted RTGs, a
grammar formalism that generalizes over a wide
range of existing formalisms, including various syn-
chronous grammars, tree transducers, and LCFRS.
We presented a generic parser for IRTGs; to apply
it to a new type of IRTG, we merely need to define
how to compute decomposition grammars D(a)
for input objects a. This makes it easy to define
synchronous grammars that are heterogeneous both
in the grammar formalism and in the objects that
each of its dimensions describes.

The purpose of our paper was to pull together
a variety of existing research and explain it in a
new, unified light: We have not shown how to do
something that was not possible before, only how
to do it in a uniform way. Nonetheless, we expect
that future work will benefit from the clarified for-
mal setup we have proposed here. In particular,
we believe that the view of parse charts as RTGs
may lead to future algorithms which exploit their
closure under intersection, e.g. to reduce syntactic
ambiguity (Schuler, 2001).

Acknowledgments

We thank the reviewers for their helpful comments,
as well as all the colleagues with whom we have
discussed this work—especially Martin Kay for a
discussion about the relationship to chart parsing.

11



References

A. Arnold and M. Dauchet. 1982. Morphismes et
bimorphismes d’arbres. Theoretical Computer
Science, 20(1):33–93.

Sylvie Billot and Bernard Lang. 1989. The struc-
ture of shared forests in ambiguous parsing. In
Proceedings of the 27th ACL.

M. Bokeloh, M. Wand, and H.-P. Seidel. 2010.
A connection between partial symmetry and in-
verse procedural modeling. In Proceedings of
SIGGRAPH.

David Chiang. 2007. Hierarchical phrase-
based translation. Computational Linguistics,
33(2):201–228.

H. Comon, M. Dauchet, R. Gilleron, C. Löding,
F. Jacquemard, D. Lugiez, S. Tison, and M. Tom-
masi. 2007. Tree automata techniques and ap-
plications. Available on: http://www.grappa.
univ-lille3.fr/tata.

Frank Drewes. 2006. Grammatical picture genera-
tion: a tree-based approach. EATCS. Springer.

Nancy Eaton, Zoltán Füredi, Alexandr V. Kos-
tochka, and Jozef Skokan. 2007. Tree repre-
sentations of graphs. European Journal of Com-
binatorics, 28(4):1087–1098.

Jason Eisner. 2003. Learning non-isomorphic tree
mappings for machine translation. In Proceed-
ings of the 41st ACL.

Ferenc Gécseg and Magnus Steinby. 1997. Tree
languages. In Grzegorz Rozenberg and Arto Sa-
lomaa, editors, Handbook of Formal Languages,
volume 3, pages 1–68. Springer.

Joseph A. Goguen, James W. Thatcher, Eric G.
Wagner, and Jesse B. Wright. 1977. Initial alge-
bra semantics and continuous algebras. Journal
of the Association for Computing Machinery,
24(1):68–95.

Carlos Gómez-Rodríguez, Marco Kuhlmann, and
Giorgio Satta. 2010. Efficient parsing of well-
nested linear context-free rewriting systems. In
Proceedings of NAACL-HLT.

Jonathan Graehl, Kevin Knight, and Jonathan May.
2008. Training tree transducers. Computational
Linguistics, 34(4):391–427.

Liang Huang, Hao Zhang, Daniel Gildea, and
Kevin Knight. 2009. Binarization of syn-
chronous context-free grammars. Computa-
tional Linguistics, 35(4):559–595.

Aravind K. Joshi and Yves Schabes. 1997. Tree-
Adjoining Grammars. In Grzegorz Rozenberg
and Arto Salomaa, editors, Handbook of Formal
Languages, volume 3, pages 69–123. Springer.

Kevin Knight and Jonathan Graehl. 2005. An
overview of probabilistic tree transducers for
natural language processing. In Computational
linguistics and intelligent text processing, pages
1–24. Springer.

Alexander Koller and Marco Kuhlmann. 2009. De-
pendency trees and the strong generative capac-
ity of CCG. In Proceedings of the 12th EACL.

Alexander Koller and Stefan Thater. 2010. Com-
puting weakest readings. In Proceedings of the
48th ACL.

Marco Kuhlmann. 2010. Dependency structures
and lexicalized grammars: An algebraic ap-
proach, volume 6270 of Lecture Notes in Com-
puter Science. Springer.

P. M. Lewis and R. E. Stearns. 1968. Syntax-
directed transduction. Journal of the ACM,
15(3):465–488.

Andreas Maletti. 2010. A tree transducer model
for synchronous tree-adjoining grammars. In
Proceedings of the 48th ACL.

David McAllester. 2002. On the complexity analy-
sis of static analyses. Journal of the Association
for Computing Machinery, 49(4):512–537.

Jorge E. Mezei and Jesse B. Wright. 1967. Alge-
braic automata and context-free sets. Informa-
tion and Control, 11(1–2):3–29.

Rebecca Nesson and Stuart M. Shieber. 2006. Sim-
pler TAG semantics through synchronization. In
Proceedings of the 11th Conference on Formal
Grammar.

Carl J. Pollard. 1984. Generalized Phrase Struc-
ture Grammars, Head Grammars, and Natural
Language. Ph.D. thesis, Stanford University.

Owen Rambow and Giorgio Satta. 1996. Syn-
chronous models of language. In Proceedings of
the 34th ACL.

12



Philip Resnik. 1992. Probabilistic tree-adjoining
grammar as a framework for statistical natural
language processing. In Proceedings of COL-
ING.

Giorgio Satta and Enoch Peserico. 2005.
Some computational complexity results for syn-
chronous context-free grammars. In Proceed-
ings of Human Language Technology Confer-
ence and Conference on Empirical Methods in
Natural Language Processing (HLT/EMNLP).

Sylvain Schmitz and Joseph Le Roux. 2008. Fea-
ture unification in tag derivation trees. In Pro-
ceedings of the 9th TAG+ Workshop.

William Schuler. 2001. Computational proper-
ties of environment-based disambiguation. In
Proceedings of the 39th ACL.

Stuart Shieber and Yves Schabes. 1990. Syn-
chronous tree-adjoining grammars. In Proceed-
ings of the 13th COLING.

Stuart Shieber. 1994. Restricting the
weak generative capacity of synchronous tree-
adjoining grammars. Computational Intelli-
gence, 10(4):371–386.

Stuart M. Shieber. 2004. Synchronous grammars
as tree transducers. In Proceedings of the Sev-
enth International Workshop on Tree Adjoining
Grammar and Related Formalisms (TAG+ 7).

Stuart M. Shieber. 2006. Unifying synchronous
tree-adjoining grammars and tree transducers via
bimorphisms. In Proceedings of the 11th EACL.

Mark Steedman. 2001. The Syntactic Process.
MIT Press.

K. Vijay-Shanker, David J. Weir, and Aravind K.
Joshi. 1987. Characterizing structural de-
scriptions produced by various grammatical for-
malisms. In Proceedings of the 25th ACL.

13


