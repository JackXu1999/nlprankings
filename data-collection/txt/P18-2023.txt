



















































Analogical Reasoning on Chinese Morphological and Semantic Relations


Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 138–143
Melbourne, Australia, July 15 - 20, 2018. c©2018 Association for Computational Linguistics

138

Analogical Reasoning on Chinese Morphological and Semantic Relations

Shen Li1,2,♠ Zhe Zhao3,♣ Renfen Hu1,2,♠,† Wensi Li1,2,� Tao Liu3,♣ Xiaoyong Du3,♣

♠{shen, irishere}@mail.bnu.edu.cn
♣{helloworld, tliu, duyong}@ruc.edu.cn

�zjklws@163.com
1 Institute of Chinese Information Processing, Beijing Normal University

2 UltraPower-BNU Joint Laboratory for Artificial Intelligence, Beijing Normal University
3 School of Information, Renmin University of China

Abstract

Analogical reasoning is effective in cap-
turing linguistic regularities. This paper
proposes an analogical reasoning task on
Chinese. After delving into Chinese lexi-
cal knowledge, we sketch 68 implicit mor-
phological relations and 28 explicit se-
mantic relations. A big and balanced
dataset CA8 is then built for this task,
including 17813 questions. Furthermore,
we systematically explore the influences
of vector representations, context features,
and corpora on analogical reasoning. With
the experiments, CA8 is proved to be a re-
liable benchmark for evaluating Chinese
word embeddings.

1 Introduction

Recently, the boom of word embedding draws our
attention to analogical reasoning on linguistic reg-
ularities. Given the word representations, anal-
ogy questions can be automatically solved via vec-
tor computation, e.g. “apples - apple + car ≈
cars” for morphological regularities and “king -
man + woman ≈ queen” for semantic regularities
(Mikolov et al., 2013). Analogical reasoning has
become a reliable evaluation method for word em-
beddings. In addition, It can be used in inducing
morphological transformations (Soricut and Och,
2015), detecting semantic relations (Herdagdelen
and Baroni, 2009), and translating unknown words
(Langlais and Patry, 2007).

It is well known that linguistic regularities vary
a lot among different languages. For example,
Chinese is a typical analytic language which lacks
inflection. Figure 1 shows that function words and
reduplication are used to denote grammatical and
semantic information. In addition, many semantic

† Corresponding author.

rén	

人 
rén	rén	

人人 

person every	person 

+ān	

天 
+ān	+ān	

天天 

day every	day 

(a) (b) 

easier 

gèng	

更 
jiǎn	dān	

简单 

easy 

jiǎn	dān	

简单 
xiē	

些 

easiest 
zuì	

最 
jiǎn	dān	

简单 

jiǎn	dān	

简单 

Figure 1: Examples of Chinese lexical knowledge:
(a) function words (in orange boxes) are used to
indicate the comparative and superlative degrees;
(b) reduplication yields the meaning of “every”.

relations are closely related with social and cul-
tural factors, e.g. in Chinese “shı̄-xiān” (god of
poetry) refers to the poet Li-bai and “shı̄-shèng”
(saint of poetry) refers to the poet Du-fu.

However, few attempts have been made in
Chinese analogical reasoning. The only Chi-
nese analogy dataset is translated from part of
an English dataset (Chen et al., 2015) (denote as
CA_translated). Although it has been widely used
in evaluation of word embeddings (Yang and Sun,
2015; Yin et al., 2016; Su and Lee, 2017), it could
not serve as a reliable benchmark since it includes
only 134 unique Chinese words in three semantic
relations (capital, state, and family), and morpho-
logical knowledge is not even considered.

Therefore, we would like to investigate linguis-
tic regularities beneath Chinese. By modeling
them as an analogical reasoning task, we could
further examine the effects of vector offset meth-
ods in detecting Chinese morphological and se-
mantic relations. As far as we know, this is the first
study focusing on Chinese analogical reasoning.
Moreover, we release a standard benchmark for
evaluation of Chinese word embedding, together
with 36 open-source pre-trained embeddings at



139

GitHub1, which could serve as a solid basis for
Chinese NLP tasks.

2 Morphological Relations

Morphology concerns the internal structure of
words. There is a common belief that Chinese is
a morphologically impoverished language since a
morpheme mostly corresponds to an orthographic
character, and it lacks apparent distinctions be-
tween roots and affixes. However, Packard (2000)
suggests that Chinese has a different morpholog-
ical system because it selects different “settings”
on parameters shared by all languages. We will
clarify this special system by mapping its morpho-
logical analogies into two processes: reduplication
and semi-affixation.

2.1 Reduplication

Reduplication means a morpheme is repeated to
form a new word, which is semantically and/or
syntactically distinct from the original morpheme,
e.g. the word “tiān-tiān”(day day) in Figure 1(b)
means “everyday”. By analyzing all the word cat-
egories in Chinese, we find that nouns, verbs, ad-
jectives, adverbs, and measure words have redupli-
cation abilities. Given distinct morphemes A and
B, we summarize 6 repetition patterns in Figure 2.

A-BAA-yi-A Pattern 2

A-lái-A-qù

Pattern 3

A-A
Pattern 1

A-lǐ-A-B

A-B-A-B

A-A-B-B
Pattern 4 

Pattern 5

Pattern 6

Figure 2: Reduplication patterns of A and A-B.

Each pattern may have one or more morpho-
logical functions. Taking Pattern 1 (A→AA) as
an example, noun morphemes could form kinship
terms or yield every/each meaning. For verbs, it
signals doing something a little bit or things hap-
pen briefly. AA reduplication could also intensify
an adjective or transform it to an adverb.

• bà(dad)→ bà-bà(dad)
• tiān(day)→ tiān-tiān(everyday)
• shuō(say)→ shuō-shuo(say a little)
• kàn(look)→ kàn-kàn(have a brief look)
• dà(big)→ dà-dà(very big; greatly)
• shēn(deep)→ shēn-shēn(deeply)
1https://github.com/Embedding/Chinese-Word-Vectors

2.2 Semi-affixation
Affixation is a morphological process whereby a
bound morpheme (an affix) is attached to roots or
stems to form new language units. Chinese is a
typical isolating language that has few affixes. Liu
et al. (2001) points out that although affixes are
rare in Chinese, there are some components be-
having like affixes and can also be used as inde-
pendent lexemes. They are called semi-affixes.

To model the semi-affixation process, we un-
cover 21 semi-prefixes and 41 semi-suffixes.
These semi-suffixes can be used to denote changes
of meaning or part of speech. For example, the
semi-prefix “dì-” could be added to numerals to
form ordinal numbers, and the semi-suffix “-zi” is
able to nominalize an adjective:

• yı̄(one)→ dì-yı̄(first)
èr(two)→ dì-èr(second)
• pàng(fat)→ pàng-zi(a fat man)

shòu(thin)→ shòu-zi(a thin man)

3 Semantic Relations

To investigate semantic knowledge reasoning, we
present 28 semantic relations in four aspects: ge-
ography, history, nature, and people. Among them
we inherit a few relations from English datasets,
e.g. country-capital and family members, while
the rest of them are proposed originally on the ba-
sis of our observation of Chinese lexical knowl-
edge. For example, a Chinese province may have
its own abbreviation, capital city, and representa-
tive drama, which could form rich semantic analo-
gies:

• ān-huı̄ vs zhè-jiāng (province)
• wǎn vs zhè (abbreviation)
• hé-féi vs háng-zhōu (capital)
• huáng-méi-xì vs yuè-jù (drama)

We also address novel relations that could be
used for other languages, e.g. scientists and their
findings, companies and their founders.

4 Task of Chinese Analogical Reasoning

Analogical reasoning task is to retrieve the answer
of the question “a is to b as c is to ?”. Based
on the relations discussed above, we firstly collect
word pairs for each relation. Since there are no
explicit word boundaries in Chinese, we take dic-
tionaries and word segmentation specifications as
references to confirm the inclusion of each word

https://github.com/Embedding/Chinese-Word-Vectors


140

Benchmark Category Type #questions #words Relation

CA_translated Semantic
Capital 506 46 capital-country
State 175 54 city-province

Family 272 34 family members

CA8

Morphological

Reduplication A 2554 344 A-A, A-yi-A, A-lái-A-qù
Reduplication AB 2535 423 A-A-B-B, A-lı̌-A-B, A-B-A-B

Semi-prefix 2553 656 21 semi-prefixes: 大,小,老,第,亚, etc.
Semi-suffix 2535 727 41 semi-suffixes: 者,式,主义,性, etc.

Semantic

Geography 3192 305
country-capital, country-currency,

province-abbreviation, province-capital,
province-dramma, etc.

History 1465 177 dynasty-emperor, dynasty-capital,title-emperor, celebrity-country

Nature 1370 452 number, time, animal, plant, body,physics, weather, reverse, color, etc.

People 1609 259 finding-scientist, work-writer,family members, etc.

Table 1: Comparisons of CA_translated and CA8 benchmarks. More details about the relations in CA8
can be seen in GitHub.

Window
(dynamic) Iteration Dimension

Sub-
sampling

Low-frequency
threshold

Context distribution
smoothing

Negative
(SGNS/PPMI)

Vector
offset

5 5 300 1e-5 50 0.75 5/1 3COSMUL

Table 2: Hyper-parameter details. Levy and Goldberg (2014b) unifies SGNS and PPMI in a framework,
which share the same hyper-parameter settings. We exploit 3COSMUL to solve the analogical questions
suggested by Levy and Goldberg (2014a).

pair. To avoid the imbalance problem addressed in
English benchmarks (Gladkova et al., 2016), we
set a limit of 50 word pairs at most for each rela-
tion. In this step, 1852 unique Chinese word pairs
are retrieved. We then build CA8, a big, balanced
dataset for Chinese analogical reasoning including
17813 questions. Compared with CA_translated
(Chen et al., 2015), CA8 incorporates both mor-
phological and semantic questions, and it brings
in much more words, relation types and questions.
Table 1 shows details of the two datasets. They are
both used for evaluation in Experiments section.

5 Experiments

In Chinese analogical reasoning task, we aim at in-
vestigating to what extent word vectors capture the
linguistic relations, and how it is affected by three
important factors: vector representations (sparse
and dense), context features (character, word, and
ngram), and training corpora (size and domain).
Table 2 shows the hyper-parameters used in this
work. All the text data used in our experiments (as
shown in Table 3) are preprocessed via the follow-
ing steps:

• Remove the html and xml tags from the texts
and set the encoding as utf-8. Digits and
punctuations are remained.

• Convert traditional Chinese characters into
simplified characters with Open Chinese
Convert (OpenCC)2.

• Conduct Chinese word segmentation with
HanLP(v_1.5.3)3.

5.1 Vector Representations

Existing vector representations fall into two types,
dense vectors and sparse vectors. SGNS (skip-
gram model with negative sampling) (Mikolov
et al., 2013) and PPMI (Positive Pointwise Mutual
Information) (Levy and Goldberg, 2014a) are re-
spectively typical methods for learning dense and
sparse word vectors. Table 4 lists the performance
of them on CA_translated and CA8 datasets under
different configurations.

We can observe that on CA8 dataset, SGNS
representations perform better in analogical rea-
soning of morphological relations and PPMI rep-
resentations show great advantages in semantic
relations. This result is consistent with per-
formance of English dense and sparse vectors
on MSR (morphology-only), SemEval (semantic-
only), and Google (mixed) analogy datasets (Levy
and Goldberg, 2014b; Levy et al., 2015). It is

2https://github.com/BYVoid/OpenCC
3https://github.com/hankcs/HanLP

https://github.com/Embedding/Chinese-Word-Vectors
https://github.com/BYVoid/OpenCC
https://github.com/hankcs/HanLP


141

Corpus Size #tokens |V | Description

Wikipedia 1.3G 223M 2129K Wikipedia data obtained fromhttps://dumps.wikimedia.org/

Baidubaike 4.1G 745M 5422K Chinese wikipedia data fromhttps://baike.baidu.com/

People’s Daily News 3.9G 668M 1664K News data from People’s Daily (1946-2017)http://data.people.com.cn/

Sogou news 3.7G 649M 1226K News data provided by Sogou Labshttp://www.sogou.com/labs/

Zhihu QA 2.1G 384M 1117K Chinese QA data from https://www.zhihu.com/,including 32137 questions and 3239114 answers
Combination 14.8G 2668M 8175K We build this corpus by combining the above corpora

Table 3: Detailed information of the corpora. #tokens denotes the number of tokens in corpus. |V |
denotes the vocabulary size.

CA_translated CA8
Cap. Sta. Fam. A AB Pre. Suf. Mor. Geo. His. Nat. Peo. Sem.

SGNS
word .706 .966 .603 .117 .162 .181 .389 .222 .414 .345 .236 .223 .327

word+ngram .715 .977 .640 .143 .184 .197 .429 .250 .449 .308 .276 .310 .368
word+char .676 .966 .548 .358 .540 .326 .612 .455 .468 .226 .296 .305 .368

PPMI
word .925 .920 .548 .103 .139 .138 .464 .226 .627 .501 .300 .515 .522

word+ngram .943 .960 .658 .102 .129 .168 .456 .230 .680 .535 .371 .626 .586
word+char .913 .886 .614 .106 .190 .173 .505 .260 .638 .502 .288 .515 .524

Table 4: Performance of word representations learned under different configurations. Baidubaike is used
as the training corpus. The top 1 results are in bold.

probably because the reasoning on morphological
relations relies more on common words in con-
text, and the training procedure of SGNS favors
frequent word pairs. Meanwhile, PPMI model
is more sensitive to infrequent and specific word
pairs, which are beneficial to semantic relations.

The above observation shows that CA8 is a re-
liable benchmark for studying the effects of dense
and sparse vectors. Compared with CA_translated
and existing English analogy datasets, it offers
both morphological and semantic questions which
are also balanced across different types 4.

5.2 Context Features

To investigate the influence of context features on
analogical reasoning, we consider not only word
features, but also ngram features inspired by sta-
tistical language models, and character (Hanzi)
features based on the close relationship between
Chinese words and their composing characters 5.
Specifically, we use word bigrams for ngram fea-
tures, character unigrams and bigrams for charac-
ter features.

4CA_translated and SemEval datasets contain only se-
mantic questions, MSR dataset contains only morphological
questions, and in Google dataset the capital:country relation
constitutes 56.72% of all semantic questions.

5The SGNS with word and character features are im-
plemented by fasttext toolkit, the rest are implemented by
ngram2vec toolkit.

Ngrams and Chinese characters are effective
features in training word representations (Zhao
et al., 2017; Chen et al., 2015; Bojanowski et al.,
2016). However, Table 4 shows that there is
only a slight increase on CA_translated dataset
with ngram features, and the accuracies in most
cases decrease after integrating character features.
In contrast, on CA8 dataset, the introduction of
ngram and character features brings significant
and consistent improvements on almost all the cat-
egories. Furthermore, character features are espe-
cially advantageous for reasoning of morphologi-
cal relations. SGNS model integrating with char-
acter features even doubles the accuracy in mor-
phological questions.

Besides, the representations achieve surpris-
ingly high accuracies in some categories of
CA_translated, which means that there is little
room for further improvement. However it is
much harder for representation methods to achieve
high accuracies on CA8. The best configuration
only achieves 68.0%.

5.3 Corpora
We compare word representations learned upon
corpora of different sizes and domains. As shown
in Table 3, six corpora are used in the experi-
ments: Chinese Wikipedia, Baidubaike, People’s
Daily News, Sogou News, Zhihu QA, and “Com-

https://dumps.wikimedia.org/
https://baike.baidu.com/
http://data.people.com.cn/
http://www.sogou.com/labs/
https://www.zhihu.com/
https://github.com/facebookresearch/fastText
https://github.com/zhezhaoa/ngram2vec


142

CA_translated CA8
Cap. Sta. Fam. A AB Pre. Suf. Mor. Geo. His. Nat. Peo. Sem.

Wikipedia 1.2G .597 .771 .360 .029 .018 .152 .266 .180 .339 .125 .147 .079 .236
Baidubaike 4.3G .706 .966 .603 .117 .162 .181 .389 .222 .414 .345 .236 .223 .327

People’s Daily 4.2G .925 .989 .547 .140 .158 .213 .355 .226 .694 .019 .206 .157 .455
Sogou News 4.0G .619 .966 .496 .057 .075 .131 .176 .115 .432 .067 .150 .145 .302
Zhihu QA 2.2G .277 .491 .625 .175 .199 .134 .251 .189 .146 .147 .250 .189 .181

Combination 15.9G .872 .994 .710 .223 .300 .234 .518 .321 .662 .293 .310 .307 .467

Table 5: Performance of word representations learned upon different training corpora by SGNS with
context feature of word. The top 2 results are in bold.

bination” which is built by combining the first five
corpora together.

Table 5 shows that accuracies increase with the
growth in corpus size, e.g. Baidubaike (an online
Chinese encyclopedia) has a clear advantage over
Wikipedia. Also, the domain of a corpus plays
an important role in the experiments. We can ob-
serve that vectors trained on news data are benefi-
cial to geography relations, especially on People’s
Daily which has a focus on political news. An-
other example is Zhihu QA, an online question-
answering corpus which contains more informal
data than others. It is helpful to reduplication rela-
tions since many reduplication words appear fre-
quently in spoken language. With the largest size
and varied domains, “Combination” corpus per-
forms much better than others in both morpholog-
ical and semantic relations.

Based on the above experiments, we find that
vector representations, context features, and cor-
pora all have important influences on Chinese ana-
logical reasoning. Also, CA8 is proved to be a re-
liable benchmark for evaluation of Chinese word
embeddings.

6 Conclusion

In this paper, we investigate the linguistic regular-
ities beneath Chinese, and propose a Chinese ana-
logical reasoning task based on 68 morphological
relations and 28 semantic relations. In the experi-
ments, we apply vector offset method to this task,
and examine the effects of vector representations,
context features, and corpora. This study offers an
interesting perspective combining linguistic anal-
ysis and representation models. The benchmark
and embedding sets we release could also serve as
a solid basis for Chinese NLP tasks.

Acknowledgments

This work is supported by the Fundamental Re-
search Funds for the Central Universities, National

Natural Science Foundation of China with Grant
(No.61472428) and Chinese Testing International
Project (No.CTI2017B12).

References
Piotr Bojanowski, Edouard Grave, Armand Joulin,

and Tomas Mikolov. 2016. Enriching word vec-
tors with subword information. arXiv preprint
arXiv:1607.04606 .

Xinxiong Chen, Lei Xu, Zhiyuan Liu, Maosong Sun,
and Huan-Bo Luan. 2015. Joint learning of charac-
ter and word embeddings. In IJCAI. pages 1236–
1242.

Etienne Denoual. 2007. Analogical translation of un-
known words in a statistical machine translation
framework. Proceedings of Machine Translation
Summit XI, Copenhagen .

Anna Gladkova, Aleksandr Drozd, and Satoshi Mat-
suoka. 2016. Analogy-based detection of morpho-
logical and semantic relations with word embed-
dings: what works and what doesn’t. In Proceedings
of the NAACL Student Research Workshop. pages 8–
15.

Amac Herdagdelen and Marco Baroni. 2009. Bag-
pack: A general framework to represent semantic re-
lations. In Proceedings of the Workshop on Geomet-
rical Models of Natural Language Semantics. Asso-
ciation for Computational Linguistics, pages 33–40.

Philippe Langlais and Alexandre Patry. 2007. Trans-
lating unknown words by analogical learning. In
EMNLP-CoNLL. pages 877–886.

Omer Levy and Yoav Goldberg. 2014a. Linguistic reg-
ularities in sparse and explicit word representations.
In Proceedings of the eighteenth conference on com-
putational natural language learning. pages 171–
180.

Omer Levy and Yoav Goldberg. 2014b. Neural word
embedding as implicit matrix factorization. In Ad-
vances in neural information processing systems.
pages 2177–2185.

Omer Levy, Yoav Goldberg, and Ido Dagan. 2015. Im-
proving distributional similarity with lessons learned



143

from word embeddings. Transactions of the Associ-
ation for Computational Linguistics 3:211–225.

Yuehua Liu, Wenyu Pan, and Wei Gu. 2001. Practi-
cal grammar of modern Chinese. The Commercial
Press.

Tomas Mikolov, Wen-tau Yih, and Geoffrey Zweig.
2013. Linguistic regularities in continuous space
word representations. In hlt-Naacl. volume 13,
pages 746–751.

Jerome L Packard. 2000. The morphology of Chinese:
A linguistic and cognitive approach. Cambridge
University Press.

Radu Soricut and Franz Josef Och. 2015. Unsu-
pervised morphology induction using word embed-
dings. In HLT-NAACL. pages 1627–1637.

Tzu-ray Su and Hung-yi Lee. 2017. Learning chi-
nese word representations from glyphs of characters.
In Proceedings of the 2017 Conference on Empiri-
cal Methods in Natural Language Processing. pages
264–273.

Peter D Turney. 2008. A uniform approach to analo-
gies, synonyms, antonyms, and associations. In
Proceedings of the 22nd International Conference
on Computational Linguistics-Volume 1. Associa-
tion for Computational Linguistics, pages 905–912.

Liner Yang and Maosong Sun. 2015. Improved learn-
ing of chinese word embeddings with semantic
knowledge. In Chinese Computational Linguistics
and Natural Language Processing Based on Natu-
rally Annotated Big Data, Springer, pages 15–25.

Rongchao Yin, Quan Wang, Peng Li, Rui Li, and Bin
Wang. 2016. Multi-granularity chinese word em-
bedding. In EMNLP. pages 981–986.

Zhe Zhao, Tao Liu, Shen Li, Bofang Li, and Xiaoyong
Du. 2017. Ngram2vec: Learning improved word
representations from ngram co-occurrence statistics.
In Proceedings of the 2017 Conference on Empiri-
cal Methods in Natural Language Processing. pages
244–253.


