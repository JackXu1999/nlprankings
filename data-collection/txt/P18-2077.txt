



















































Simpler but More Accurate Semantic Dependency Parsing


Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 484–490
Melbourne, Australia, July 15 - 20, 2018. c©2018 Association for Computational Linguistics

484

Simpler but More Accurate Semantic Dependency Parsing

Timothy Dozat
Stanford University

tdozat@stanford.edu

Christopher D. Manning
Stanford University

manning@stanford.edu

Abstract
While syntactic dependency annotations
concentrate on the surface or functional
structure of a sentence, semantic depen-
dency annotations aim to capture between-
word relationships that are more closely
related to the meaning of a sentence, using
graph-structured representations. We ex-
tend the LSTM-based syntactic parser of
Dozat and Manning (2017) to train on and
generate these graph structures. The re-
sulting system on its own achieves state-
of-the-art performance, beating the pre-
vious, substantially more complex state-
of-the-art system by 0.6% labeled F1.
Adding linguistically richer input repre-
sentations pushes the margin even higher,
allowing us to beat it by 1.9% labeled F1.

1 Introduction

Syntactic dependency parsing is arguably the most
popular method for automatically extracting the
low-level relationships between words in a sen-
tence for use in natural language understanding
tasks. However, typical syntactic dependency
frameworks are limited in the number and types of
relationships that can be captured. For example, in
the sentence Mary wants to buy a book, the word
Mary is the subject of both want and buy—either
or both relationships could be useful in a down-
stream task, but a tree-structured representation of
this sentence (as in Figure 1a) can only represent
one of them.1

The 2014 SemEval shared task on Broad-
Coverage Semantic Dependency Parsing (Oepen
et al., 2014) introduced three new dependency rep-
resentations that do away with the assumption of

1Though efforts have been made to address this limitation;
seeDe Marneffe et al. (2006); Nivre et al. (2016); Schuster
and Manning (2016); Candito et al. (2017) for examples.

strict tree structure in favor of a richer graph-
structured representation, allowing them to cap-
ture more linguistic information about a sentence.
This opens up the possibility of providing more
useful information to downstream tasks (Reddy
et al., 2017; Schuster et al., 2017), but increases
the difficulty of automatically extracting that in-
formation, since most previous work on parsing
has focused on generating trees.

Dozat and Manning (2017) developed a suc-
cessful syntactic dependency parsing system with
few task-specific sources of complexity. In this
paper, we extend that system so that it can train
on and produce the graph-structured data of se-
mantic dependency schemes. We also consider
straightforward extensions of the system that are
likely to increase performance over the straightfor-
ward baseline, including giving the system access
to lemma embeddings and building in a character-
level word embedding model. Finally, we briefly
examine some of the design choices of that archi-
tecture, in order to assess which components are
necessary for achieving the highest accuracy and
which have little impact on final performance.

2 Background

2.1 Semantic dependencies

The 2014 SemEval (Oepen et al., 2014, 2015)
shared task introduced three new semantic de-
pendency formalisms, applied to the Penn Tree-
bank (shown in Figure 1, compared to Universal
Dependencies (Nivre et al., 2016)): DELPH-IN
MRS, or DM (Flickinger et al., 2012; Oepen and
Lønning, 2006); Predicate-Argument Structures,
or PAS (Miyao and Tsujii, 2004); and Prague Se-
mantic Dependencies, or PSD (Hajic et al., 2012).
Whereas syntactic dependencies generally anno-
tate functional relationships between words—such
as subject and object—semantic dependencies aim



485

Mary wants to buy a book

NSUBJ

ROOT

DET

XCOMP

MARK

OBJ

NSUBJ

(a) UD (with enhanced depen-
dencies dashed)

Mary wants to buy a book

TOP

ARG1

ARG2ARG1

ARG2

BV

(b) DM

Mary wants to buy a book

TOP

VERB ARG1

VERB ARG1

VERB ARG2

DET ARG1VERB ARG2

COMP ARG1

(c) PAS

Mary wants to buy a book

TOP

ACT-ARG

PAT-ARGACT-ARG PAT-ARG

(d) PSD

Figure 1: Comparison between syntactic and semantic dependency schemes

to reflect semantic relationships—such as agent
and patient (cf. semantic role labeling (Gildea and
Jurafsky, 2002)). The SemEval semantic depen-
dency schemes are also directed acyclic graphs
(DAGs) rather than trees, allowing them to anno-
tate function words as being heads without length-
ening paths between content words (as in 1b).

2.2 Related work

Our approach to semantic dependency parsing is
primarily inspired by the success of Dozat and
Manning (2017) and Dozat et al. (2017) at syn-
tactic dependency parsing and Peng et al. (2017)
at semantic dependency parsing. In Dozat and
Manning (2017) and Peng et al. (2017), parsing in-
volves first using a multilayer bidirectional LSTM
over word and part-of-speech tag embeddings.
Parsing is then done using directly-optimized self-
attention over recurrent states to attend to each
word’s head (or heads), and labeling is done with
an analgous multi-class classifier.

Peng et al.’s (2017) system uses a max-margin
classifer on top of a BiLSTM, with the score for
each graph coming from several sources. First, it
scores each word as either taking dependents or
not. For each ordered pair of words, it scores the
arc from the first word to the second. Lastly, it
scores each possible labeled arc between the two
words. The graph that maximizes these scores
may not be consistent, with an edge coming from
a non-predicate, for example, so they enforce hard
constraints in order to prune away invalid semantic
graphs. Decisions are not independent, so in order
to find the highest-scoring graph that follows these
constraints, they use the AD3 decoding algorithm
(Martins et al., 2011).

Dozat and Manning’s (2017) approach to syn-
tactic dependency parsing is similar, but avoids
the possibility of generating invalid trees by fully
factorizing the system. Rather than summing the
scores from multiple modules and then finding the
valid structure that maximizes that sum, the sys-

tem makes parsing and labeling decisions sequen-
tially, choosing the labels for each edge only af-
ter the edges in the tree have been finalized by an
MST algorithm.

Wang et al. (2018) take a different approach in
their recent work, using a transition-based parser
built on stack-LSTMs (Dyer et al., 2015). They
extend Choi and McCallum’s (2013) transition
system for producing non-projective trees so that
it can produce arbitrary DAGs and they modify the
stack-LSTM architecture slightly to make the net-
work more powerful.

3 Approach

3.1 Basic approach

We can formulate the semantic dependency pars-
ing task as labeling each edge in a directed graph,
with null being the label given to pairs with no
edge between them. Using only one module that
labels each edge in this way would be an unfactor-
ized approach. We can, however, factorize it into
two modules: one that predicts whether or not a
directed edge (wj , wi) exists between two words,
and another that predicts the best label for each
potential edge.

Our approach closely follows that of Dozat and
Manning (2017). As with many successful recent
parsers, we concatenate word and POS tag2 em-
beddings, and feed them into a multilayer bidirec-
tional LSTM to get contextualized word represen-
tations.3

xi = e
(word)
i ⊕ e

(tag)
i (1)

R = BiLSTM(X) (2)

2We use the POS tags (and later, lemmas) provided with
each dataset.

3We follow the convention of representing scalars in low-
ercase italics a, vectors in lowercase bold a, matrices in up-
percase italics A, and tensors in uppercase bold A. We main-
tain this convention when indexing and stacking, so ai is
row i of matrix A and A contains the sequence of vectors
(a1, . . . ,an).



486

For each of the two modules, we use single-layer
feedforward networks (FNN) to split the top re-
current states into two parts—a head representa-
tion, as in Eq. (5, 6) and a dependent representa-
tion, as in Eq. (7, 8). This allows us to reduce the
recurrent size to avoid overfitting in the classifer
without weakening the LSTM’s capacity. We can
then use bilinear or biaffine classifiers in Eq. (3,
4)—which are generalizations of linear classifiers
to include multiplicative interactions between two
vectors—to predict edges and labels.4

Bilin(x1,x2) = x>1 Ux2 (3)

Biaff(x1,x2) = x>1 Ux2 +W (x1 ⊕ x2) + b (4)

h
(edge-head)
i = FNN

(edge-head)(ri) (5)

h
(label-head)
i = FNN

(label-head)(ri) (6)

h
(edge-dep)
i = FNN

(edge-dep)(ri) (7)

h
(label-dep)
i = FNN

(label-dep)(ri) (8)

s
(edge)
i,j = Biaff

(edge)
(
h
(edge-dep)
i ,h

(edge-head)
j

)
(9)

s
(label)
i,j = Biaff

(label)
(
h
(label-dep)
i ,h

(label-head)
j

)
(10)

y
′(edge)
i,j = {si,j ≥ 0} (11)

y
′(label)
i,j = argmax si,j (12)

The tensor U can optionally be diagonal (such that
ui,k,j = 0 wherever i 6= j) to conserve param-
eters. The unlabeled parser scores every edge be-
tween pairs of words in the sentence—these scores
can be decoded into a graph by keeping only edges
that received a positive score. The labeler scores
every label for each pair of words, so we simply
assign each predicted edge its highest-scoring la-
bel and discard the rest. We can train the system by
summing the losses from the two modules, back-
propagating error to the labeler only through edges
with a non-null gold label. This system is shown
graphically in Figure 2. We find that sometimes
the loss for one module overwhelms the loss for
the other, causing the system to underfit. Thus we
add a tunable interpolation constant λ ∈ (0, 1) to
even out the two losses.

` = λ`(label) + (1− λ)`(edge) (13)

Worth noting is that the removal of the max-
imum spanning tree algorithm and change from
softmax cross-entropy to sigmoid cross-entropy in

4For the labeled parser, U will be (d×c×d)-dimensional,
where c is the number of labels. For the unlabeled parser, U
will be (d× 1× d)-dimensional, so that si,j will be a single
score.

. . .BiLSTM

Embed

FC

Edges Labels

Figure 2: The basic architecture of our factorized
system. Labels are only assigned to word pairs
with an edge between them.

the unlabeled parser represent the only changes
needed to allow the original syntactic parser to
generate fully graph-structured semantic depen-
dency output. Note also that this system is gen-
eral enough that it could be used for any graph-
structured dependency scheme, including the en-
hanced dependencies of the Universal Dependen-
cies formalism (which allows cyclic graphs).

3.2 Augmentations

Ballesteros et al. (2016), Dozat et al. (2017), and
Ma et al. (2018) find that character-level word em-
bedding models improve performance for syntac-
tic dependency parsing, so we also want to explore
the impact it has on semantic dependency pars-
ing. Dozat et al. (2017) confirm that their syntactic
parser performs better with POS tags, which leads
us to examine whether word lemmas—another
form of low-level lexical information—might also
improve dependency parsing performance.

4 Results

4.1 Hyperparameters

We tuned the hyperparameters for our basic sys-
tem (with no character embeddings or lemmas)
fairly extensively on the DM development data.
The hyperparameter configuration for our final
system is given in Table 2. All input embeddings
(word, pretrained, POS, etc.) were concatenated.
We used 100-dimensional pretrained GloVe em-
beddings (Pennington et al., 2014), but linearly
transformed them to be 125-dimensional. Only
words or lemmas that occurred 7 times or more
were included in the word and lemma embedding
matrix—including less frequent words appeared to
facilitate overfitting. Character-level word embed-
dings were generated using a one-layer unidirec-
tional LSTM that convolved over three character
embeddings at a time, whose end state was linearly
transformed to be 100-dimensional. The core BiL-



487

DM PAS PSD Avg
ID OOD ID OOD ID OOD ID OOD

(Du et al., 2015) 89.1 81.8 91.3 87.2 75.7 73.3 85.3 80.8
(Almeida and Martins, 2015) 88.2 81.8 90.9 86.9 76.4 74.8 85.2 81.2
WCGL18 90.3 84.9 91.7 87.6 78.6 75.9 86.9 82.8
PTS17: Basic 89.4 84.5 92.2 88.3 77.6 75.3 87.4 83.6
PTS17: Freda3 90.4 85.3 92.7 89.0 78.5 76.4 88.0 84.4
Ours: Basic 91.4 86.9 93.9 90.8 79.1 77.5 88.1 85.0
Ours: +Char 92.7 87.8 94.0 90.6 80.5 78.6 89.1 85.7
Ours: +Lemma 93.3 88.8 93.9 90.5 80.3 78.7 89.1 86.0
Ours: +Char +Lemma 93.7 88.9 93.9 90.6 81.0 79.4 89.5 86.3

Table 1: Comparison between our system and the previous state of the art on in-domain (WSJ) and
out-of-domain (Brown corpus) data, according to labeled F1 (LF1).

Hidden Sizes
Word/Glove/POS/

100
Lemma/Char
GloVe linear 125
Char LSTM 1 @ 400
Char linear 100
BiLSTM 3 @ 600
Arc/Label 600

Dropout Rates (drop prob)
Word/GloVe/

20%
POS/Lemma
Char LSTM (FF/recur) 33%
Char linear 33%
BiLSTM (FF/recur) 45%/25%
Arc/Label 25%/33%

Loss & Optimizer
Interpolation (λ) .025
L2 regularization 3e−9

Learning rate 1e−3

Adam β1 0
Adam β2 .95

Table 2: Final hyperparameter configuration.

STM was three layers deep. The different types of
word embeddings—word, GloVe, and character-
level—were dropped simultaneously, but indepen-
dently from POS and lemma embeddings (which
were dropped independently from each other).
Dropped embeddings were replaced with learned
<DROP> tokens. LSTMs used same-mask recur-
rent dropout (Gal and Ghahramani, 2016). The
systems were trained with batch sizes of 3000 to-
kens for up to 75,000 training steps, terminating
early after 10,000 steps pass with no improve-

ment in validation accuracy. The L2 regularization
penalty was so small that it likely had little impact.

4.2 Performance

We use biaffine classifiers, with no nonlinearities,
and a diagonal tensor in the label classifier but not
the edge classifier. The system trains at a speed
of about 300 sequences/second on an nVidia Ti-
tan X and parses about 1,000 sequences/second.
Du et al. (2015) and Almeida and Martins (2015)
are the systems that won the 2015 shared task
(closed track). PTS17: Basic represents the
single-task versions of Peng et al. (2017), which
they make multitask across the three datasets in
Freda3 by adding frustratingly easy domain adap-
tation (Daumé III, 2007; Kim et al., 2016) and
a third-order decoding mechanism. WCGL18 is
Wang et al.’s (2018) transition-based system. Ta-
ble 1 compares our performance with these sys-
tems. Our fully factorized basic system already
substantially outperforms Peng et al.’s single-task
baseline and also beats out their much more com-
plex multi-task approach. Simply adding in either
a character-level word embedding model (similar
to Dozat et al.’s (2017)) or a lemma embedding
matrix likewise improves performance quite a bit,
and including both together generally pushes per-
formance even higher. Many infrequent words
were excluded from the frequent token embedding
matrix, so it makes sense that the system should
improve when provided more lexical information
that’s harder to overfit on.

Surprisingly, the PAS dataset seems not to ben-
efit substantially from lemma or character embed-
dings. It has been noted that PAS is the easiest of
the three datasets to achieve good performance for;



488

Basic Unfac. No
Hidden

Bilin Diag ReLU No
Hidden

Bilin Non­
Diag

ReLU
90.8

91.0

91.2

91.4

91.6
La

be
le

d 
F1

Architecture Variations

Both
Parser
Labeler

Figure 3: Performance of architecture variations:
our basic system; unfactorized (labeler-only); om-
mitting the hidden layers (Eqs. 7–6); with bilinear
classifiers (Eq. 3); with nondiagonal tensors in the
labeler or diagonal tensors in the parser; with the
ReLU nonlinearity.

so one possible explanation is that 94% LF1 may
simply be near the ceiling of what can be achieved
for the dataset. Alternatively, the main difference
bewteen PAS as DM/PSD is that PAS includes se-
mantically vacuous function words in its represen-
tation. Because function words are extremely fre-
quent, it’s possible that they are being dispropor-
tionately represented in the loss or LF1 score. Us-
ing a hinge loss (like Peng et al. (2017)) instead of
a cross-entropy loss might help, since the system
would stop focusing on potentially “easy” func-
tional predicates once it learned to predict their ar-
gument structures confidently, allowing it to put
more resources into modeling more challenging
phenomena.

4.3 Variations
We also consider the impact that slight variations
on basic architecture have on final performance
in Figure 3. We train twenty models on the DM
treebank for each variation we consider, reduc-
ing the number of training steps but keeping all
other hyperparameters constant. Rank-sum tests
(Lehmann et al., 1975) reveal that the basic sys-
tem outperforms variants with no hidden layers in
the edge classifier (W=339; p<.001) or the label
classifier (W=307; p<.01). Using a diagonal ten-
sor U in the unlabeled parser also significantly
hurts performance (W=388; p<.001), likely be-
ing too underpowered. While the other variations
(especially the unfactorized and ReLU systems)
appeared to make a difference during hyperparam-
eter tuning, they were not significant here.

The improved performance of deeper systems

(replicating Dozat and Manning (2017)) likely jus-
tifies the added complexity. On the other hand,
the choice between biaffine and bilinear classi-
fiers comes down largely to aesthetics. This is
perhaps unsurprising since the change from bi-
affine to bilinear represents only a small decrease
in overall power. Unusually, using no nonlin-
earity in the hidden layers in Eqs. (7–6) works
as well as ReLU—in fact, using ReLU in the
unlabeled parser marginally reduced performance
(W=269; p=.063). Overall, the parser displayed
considerable invariance to architecture changes.
Since our system is significantly larger and more
heavily regularized than the systems we com-
pare against, this suggests that unglamorous, low-
level hyperparameters—such as hidden sizes and
dropout rates—are more critical to system perfor-
mance than high-level architecture enhancements.

5 Discussion

We minimally extended a simple syntactic de-
pendency parser to produce graph-structured de-
pendencies. Without any further augmentations,
our carefully-tuned system achieves state-of-the-
art performance, highlighting the importance of
finding the best hyperparameter configuration (and
by extention, building fast systems that can be
trained quickly). Additionally, we can see that a
multitask system relying on a complex decoding
algorithm to prune away invalid graph structures
isn’t necessary for achieving the level of parsing
performance a simple system can achieve (though
it could push performance even higher). We also
find easier or independently motivated ways to
improve accuracy—taking advantage of provided
lemma or subtoken information provides a boost
comparable to one found by drastically increasing
system complexity.

Further, we observe a high-performing graph-
based parser can be adapted to different types
of dependency graphs (projective tree, non-
projective tree, directed graph) with only small
changes without obviously hurting accuracy. By
contrast, transition-based parsers—which were
originally designed for parsing projective con-
stituency trees (Nivre, 2003; Aho and Ullman,
1972)—require whole new transition sets or even
data structures to generate arbitrary graphs. We
feel that this points to graph-based parsers be-
ing the most natural way to produce dependency
graphs with different structural restrictions.



489

References
Alfred V Aho and Jeffrey D Ullman. 1972. The theory

of parsing, translation, and compiling, volume 1.
Prentice Hall.

Mariana SC Almeida and André FT Martins. 2015.
Lisbon: Evaluating turbosemanticparser on multiple
languages and out-of-domain data. In Proceedings
of the 9th International Workshop on Semantic Eval-
uation (SemEval 2015). pages 970–973.

Miguel Ballesteros, Yoav Goldberg, Chris Dyer, and
Noah A Smith. 2016. Training with exploration im-
proves a greedy stack-LSTM parser. Proceedings of
the conference on empirical methods in natural lan-
guage processing .

Marie Candito, Bruno Guillaume, Guy Perrier, and
Djamé Seddah. 2017. Enhanced ud dependencies
with neutralized diathesis alternation. In Depling
2017-Fourth International Conference on Depen-
dency Linguistics.

Jinho D Choi and Andrew McCallum. 2013.
Transition-based dependency parsing with se-
lectional branching. In Proceedings of the 51st
Annual Meeting of the Association for Compu-
tational Linguistics (Volume 1: Long Papers).
volume 1, pages 1052–1062.

Hal Daumé III. 2007. Frustratingly easy domain adap-
tation. ACL 2007 page 256.

Marie-Catherine De Marneffe, Bill MacCartney,
Christopher D Manning, et al. 2006. Generat-
ing typed dependency parses from phrase structure
parses. In Proceedings of LREC. volume 6, pages
449–454.

Timothy Dozat and Christopher D. Manning. 2017.
Deep biaffine attention for neural dependency pars-
ing. ICLR 2017 .

Timothy Dozat, Peng Qi, and Christopher D Manning.
2017. Stanford’s graph-based neural dependency
parser at the conll 2017 shared task. CoNLL pages
20–30.

Yantao Du, Fan Zhang, Xun Zhang, Weiwei Sun, and
Xiaojun Wan. 2015. Peking: Building semantic de-
pendency graphs with a hybrid parser. In Proceed-
ings of the 9th International Workshop on Semantic
Evaluation (SemEval 2015). pages 927–931.

Chris Dyer, Miguel Ballesteros, Wang Ling, Austin
Matthews, and Noah A Smith. 2015. Transition-
based dependency parsing with stack long short-
term memory. Proceedings of the conference on em-
pirical methods in natural language processing .

Dan Flickinger, Yi Zhang, and Valia Kordoni. 2012.
Deepbank. a dynamically annotated treebank of the
wall street journal. In Proceedings of the 11th In-
ternational Workshop on Treebanks and Linguistic
Theories. pages 85–96.

Yarin Gal and Zoubin Ghahramani. 2016. Dropout as
a bayesian approximation: Representing model un-
certainty in deep learning. International Conference
on Machine Learning .

Daniel Gildea and Daniel Jurafsky. 2002. Automatic
labeling of semantic roles. Computational linguis-
tics 28(3):245–288.

Jan Hajic, Eva Hajicová, Jarmila Panevová, Petr Sgall,
Ondrej Bojar, Silvie Cinková, Eva Fucı́ková, Marie
Mikulová, Petr Pajas, Jan Popelka, et al. 2012.
Announcing prague czech-english dependency tree-
bank 2.0. In LREC. pages 3153–3160.

Young-Bum Kim, Karl Stratos, and Ruhi Sarikaya.
2016. Frustratingly easy neural domain adaptation.
In Proceedings of COLING 2016, the 26th Inter-
national Conference on Computational Linguistics:
Technical Papers. pages 387–396.

Erich Leo Lehmann, HJM D’Abrera, et al. 1975. Non-
parametrics. Holden-Day.

Xuezhe Ma, Zecong Hu, Jingzhou Liu, Nanyun Peng,
Graham Neubig, and Eduard Hovy. 2018. Stack-
pointer networks for dependency parsing. ACL .

André FT Martins, Noah A Smith, Pedro MQ Aguiar,
and Mário AT Figueiredo. 2011. Dual decomposi-
tion with many overlapping components. In Pro-
ceedings of the Conference on Empirical Methods
in Natural Language Processing. Association for
Computational Linguistics, pages 238–249.

Yusuke Miyao and Jun’ichi Tsujii. 2004. Deep lin-
guistic analysis for the accurate identification of
predicate-argument relations. In Proceedings of
the 20th international conference on Computational
Linguistics. Association for Computational Linguis-
tics, page 1392.

Joakim Nivre. 2003. An efficient algorithm for pro-
jective dependency parsing. In Proceedings of the
8th International Workshop on Parsing Technologies
(IWPT . Citeseer.

Joakim Nivre, Marie-Catherine de Marneffe, Filip Gin-
ter, Yoav Goldberg, Jan Hajic, Christopher D Man-
ning, Ryan McDonald, Slav Petrov, Sampo Pyysalo,
Natalia Silveira, et al. 2016. Universal dependencies
v1: A multilingual treebank collection. In Proceed-
ings of the 10th International Conference on Lan-
guage Resources and Evaluation (LREC 2016).

Stephan Oepen, Marco Kuhlmann, Yusuke Miyao,
Daniel Zeman, Silvie Cinková, Dan Flickinger, Jan
Hajic, and Zdenka Uresova. 2015. Semeval 2015
task 18: Broad-coverage semantic dependency pars-
ing. In Proceedings of the 9th International Work-
shop on Semantic Evaluation. pages 915–926.

Stephan Oepen, Marco Kuhlmann, Yusuke Miyao,
Daniel Zeman, Dan Flickinger, Jan Hajic, Angelina
Ivanova, and Yi Zhang. 2014. Semeval 2014 task
8: Broad-coverage semantic dependency parsing. In



490

Proceedings of the 8th International Workshop on
Semantic Evaluation. pages 63–72.

Stephan Oepen and Jan Tore Lønning. 2006.
Discriminant-based mrs banking. In Proceedings
of the 5th International Conference on Language
Resources and Evaluation. pages 1250–1255.

Hao Peng, Sam Thomson, and Noah A Smith. 2017.
Deep multitask learning for semantic dependency
parsing. In ACL. volume 1, pages 2037–2048.

Jeffrey Pennington, Richard Socher, and Christopher D
Manning. 2014. Glove: Global vectors for word
representation. Proceedings of the Empiricial Meth-
ods in Natural Language Processing (EMNLP 2014)
12.

Siva Reddy, Oscar Täckström, Slav Petrov, Mark
Steedman, and Mirella Lapata. 2017. Universal se-
mantic parsing. arXiv preprint arXiv:1702.03196 .

Sebastian Schuster and Christopher D. Manning. 2016.
Enhanced English Universal Dependencies: An im-
proved representation for natural language under-
standing tasks. In Proceedings of the Tenth Interna-
tional Conference on Language Resources and Eval-
uation (LREC 2016).

Sebastian Schuster, Éric Villemonte de la Clergerie,
Marie Candito, Benoı̂t Sagot, Christopher D. Man-
ning, and Djamé Seddah. 2017. Paris and Stan-
ford at EPE 2017: Downstream evaluation of graph-
based dependency representations. In Proceedings
of the 2017 Shared Task on Extrinsic Parser Evalu-
ation (EPE 2017).

Yuxuan Wang, Wanxiang Che, Jiang Guo, and Ting
Liu. 2018. A neural transition-based approach for
semantic dependency graph parsing. AAAI .


