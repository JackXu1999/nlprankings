



















































Searching for the X-Factor: Exploring Corpus Subjectivity for Word Embeddings


Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers), pages 1212–1221
Melbourne, Australia, July 15 - 20, 2018. c©2018 Association for Computational Linguistics

1212

Searching for the X-Factor:
Exploring Corpus Subjectivity for Word Embeddings

Maksim Tkachenko and Chong Cher Chia and Hady W. Lauw
School of Information Systems

Singapore Management University
maksim.tkatchenko@gmail.com

{ccchia.2014,hadywlauw}@smu.edu.sg

Abstract

We explore the notion of subjectivity, and
hypothesize that word embeddings learnt
from input corpora of varying levels of
subjectivity behave differently on natural
language processing tasks such as classi-
fying a sentence by sentiment, subjectiv-
ity, or topic. Through systematic com-
parative analyses, we establish this to be
the case indeed. Moreover, based on the
discovery of the outsized role that senti-
ment words play on subjectivity-sensitive
tasks such as sentiment classification, we
develop a novel word embedding SentiVec
which is infused with sentiment informa-
tion from a lexical resource, and is shown
to outperform baselines on such tasks.

1 Introduction

Distributional analysis methods such as Word2Vec
(Mikolov et al., 2013) and GloVe (Pennington
et al., 2014) have been critical for the success
of many large-scale natural language processing
(NLP) applications (Collobert et al., 2011; Socher
et al., 2013; Goldberg, 2016). These methods em-
ploy distributional hypothesis (i.e., words used in
the same contexts tend to have similar meaning) to
derive distributional meaning via context predic-
tion tasks and produce dense word embeddings.

While there have been active and ongoing re-
search on improving word embedding methods
(see Section 5), there is a relative dearth of study
on the impact that an input corpus may have on
the quality of the word embeddings. The previous
preoccupation centers around corpus size, i.e., a
larger corpus is perceived to be richer in statistical
information. For instance, popular corpora include
Wikipedia, Common Crawl, and Google News.

We postulate that there may be variations across
corpora owing to factors that affect language use.
Intuitively, the many things we write (a work
email, a product review, an academic publication,
etc.) may each involve certain stylistic, syntactic,
and lexical choices, resulting in meaningfully dif-
ferent distributions of word cooccurrences. Con-
sequently, such factors may be encoded in the
word embeddings, and input corpora may be dif-
ferentially informative towards various NLP tasks.

In this work, we are interested in the notion
of subjectivity. Some NLP tasks, such as senti-
ment classification, revolve around subjective ex-
pressions of likes or dislikes. Others, such as topic
classification, revolve around more objective ele-
ments of whether a document belongs to a topic
(e.g., science, politics). Our central hypothesis is
that word embeddings learnt from input corpora
of contrasting levels of subjectivity perform dif-
ferently when classifying sentences by sentiment,
subjectivity, or topic. As the first contribution, we
outline an experimental scheme to explore this hy-
pothesis in Section 2, and conduct a series of con-
trolled experiments in Section 3 establishing that
there exists a meaningful difference between word
embeddings derived from objective vs. subjective
corpora. We further systematically investigate fac-
tors that could potentially explain the differences.

Upon discovering from the investigation that
sentiment words play a particularly important role
in subjectivity-sensitive NLP tasks, such as sen-
timent classification, as the second contribution,
in Section 4 we develop SentiVec, a novel word
embedding method infused with information from
lexical resources such as a sentiment lexicon. We
further identify two alternative lexical objectives:
Logistic SentiVec based on discriminative logistic
regression, and Spherical SentiVec based on soft
clustering effect of von Mises-Fisher distributions.
In Section 6, the proposed word embeddings show



1213

evident improvements on sentiment classification,
as compared to the base model Word2Vec and
other baselines using the same lexical resource.

2 Data and Methodology

We lay out the methodology for generating word
embeddings of contrasting subjectivity, whose ef-
fects are tested on several text classification tasks.

2.1 Generating Word Embeddings

As it is difficult to precisely quantify the degree
of subjectivity of a corpus, we resort to generat-
ing word embeddings from two corpora that con-
trast sharply in subjectivity, referring to them as
the Objective Corpus and the Subjective Corpus.

Objective Corpus As virtually all contents
are written by humans, an absolutely objective
corpus (in the philosophical sense) may prove elu-
sive. There are however exemplars where, by
construction, a corpus aspires to be as objective
as possible, and probably achieves that in prac-
tical terms. We postulate that one such corpus
is Wikipedia. Its list of policies and guidelines1,
assiduously enforced by an editorial team, spec-
ify that an article must be written from a neutral
point of view, which among other things means
“representing fairly, proportionately, and, as far
as possible, without editorial bias, all of the sig-
nificant views that have been published by reliable
sources on a topic.”. Moreover, it is a common
resource for training distributional word embed-
dings and adopted widely by the research commu-
nity to solve various NLP problems. Hence, in this
study, we use Wikipedia as the Objective Corpus.

Subjective Corpus By extension, one may
then deem a corpus subjective if its content does
not at least meet Wikipedia’s neutral point of view
requirement. In other words, if the content is re-
plete with personal feelings and opinions. We
posit that product reviews would be one such cor-
pus. For instance, Amazon’s Community Guide-
line2 states that “Amazon values diverse opin-
ions”, and that “Content you submit should be rel-
evant and based on your own honest opinions and
experience.”. Reviews consist of expressive con-
tent written by customers, and may not strive for
the neutrality of an encyclopedia. We rely on a

1https://en.wikipedia.org/wiki/
Wikipedia:List_of_policies_and_
guidelines

2https://www.amazon.com/gp/help/
customer/display.html?nodeId=201929730

large corpus of Amazon reviews from various cat-
egories (e.g., electronics, jewelry, books, and etc.)
(McAuley et al., 2015) as the Subjective Corpus.

Word Embeddings For the comparative anal-
ysis in Section 3, we employ Word2Vec (reviewed
below) to generate word embeddings from each
corpus. Later on in Section 4, we will propose a
new word embedding method called SentiVec.

For Word2Vec, we use the Skip-gram model to
train distributional word embeddings on the Ob-
jective Corpus and the Subjective Corpus respec-
tively. Skip-gram aims to find word embeddings
that are useful for predicting nearby words. The
objective is to maximize the context probability:

logL(W ;C) =
∑
w∈W

∑
w′∈C(w)

log P(w′|w), (1)

where W is an input corpus and C(w) is the con-
text of token w. The probability of context word
w′, given observed word w is defined via softmax:

P(w′|w) = exp (vw
′ · vw)∑

ŵ∈V exp (vŵ · vw)
, (2)

where vw and vw′ are corresponding embeddings
and V is the corpus vocabulary. Though theoret-
ically sound, the formulation is computationally
impractical and requires tractable approximation.

Mikolov et al. (2013) propose two efficient pro-
cedures to optimize (1): Hierarchical Softmax and
Negative Sampling (NS). In this work we focus
on the widely adopted NS. The intuition is that a
“good” model should be able to differentiate ob-
served data from noise. The differentiation task
is defined using logistic regression; the goal is to
tell apart real context-word pair (w′, w) from ran-
domly generated noise pair (ŵ, w). Formally,

logL[w‘,w] = log σ (vw′ · vw) +
k∑
i=1

log σ (−vŵi · vw),

(3)

where σ( · ) is a sigmoid function, and {ŵi}ki=1
are negative samples. Summing up all the context-
word pairs, we derive the NS Skip-gram objective:

logLword2vec(W ;C) =
∑
w∈W

∑
w′∈C(w)

logL[w‘,w]. (4)

Training word embeddings with Skip-gram, we
keep the same hyperparameters across all the runs:
300 dimensions for embeddings, k = 5 negative
samples, and window of 5 tokens. The Objective

https://en.wikipedia.org/wiki/Wikipedia:List_of_policies_and_guidelines
https://en.wikipedia.org/wiki/Wikipedia:List_of_policies_and_guidelines
https://en.wikipedia.org/wiki/Wikipedia:List_of_policies_and_guidelines
https://www.amazon.com/gp/help/customer/display.html?nodeId=201929730
https://www.amazon.com/gp/help/customer/display.html?nodeId=201929730


1214

and Subjective corpora undergo the same prepro-
cessing, i.e., discarding short sentences (< 5 to-
kens) and rare words (< 10 occurrences), remov-
ing punctuation, normalizing Unicode symbols.

2.2 Evaluation Tasks

To compare word embeddings, we need a com-
mon yardstick. It is difficult to define an inherent
quality to word embeddings. Instead, we put them
through several evaluation tasks that can leverage
word embeddings and standardize their formula-
tions as binary classification tasks. To boil the
comparisons down to the essences of word em-
beddings (which is our central focus), we rely on
standardized techniques so as to attribute as much
of the differences as possible to the word embed-
dings. We use logistic regression for classification,
and represent a text snippet (e.g., a sentence) in
the feature space as the average of the word em-
beddings of tokens in the snippet (ignoring out-of-
vocabulary tokens). The evaluation metric is the
average accuracy from 10-fold cross validation.

There are three evaluation tasks of varying de-
grees of hypothetical subjectivity, as outlined be-
low. Each may involve multiple datasets.

Sentiment Classification Task This task clas-
sifies a sentence into either positive or negative.
We use two groups of datasets as follows.

The first group consists of 24 datasets from
UCSD Amazon product data3 corresponding to
various product categories. Each review has a rat-
ing from 1 to 5, which is transformed into pos-
itive (ratings 4 or 5) or negative (ratings 1 or 2)
class. For each dataset respectively, we sample
5000 sentences each from the positive and nega-
tive reviews. Note that these sentences used for
this evaluation task have not participated in the
generation of word embeddings. Due to space
constraint, in most cases we present the average
accuracy across the datasets, but where appropri-
ate we enumerate the results for each dataset.

The second is Cornell’s sentence polarity
dataset v1.04 (Pang and Lee, 2005), made up of
5331 each of positive and negative sentences from
Rotten Tomatoes movie reviews. The inclusion of
this out-of-domain evaluation dataset is useful for
examining whether the performance of word em-
beddings from the Subjective Corpus on the first

3http://jmcauley.ucsd.edu/data/amazon/
4http://www.cs.cornell.edu/people/

pabo/movie-review-data/rt-polaritydata.
README.1.0.txt

group above may inadvertently be affected by in-
domain advantage arising from its Amazon origin.

Subjectivity Classification Task This task
classifies a sentence into subjective or objective.
The dataset is Cornell’s subjectivity dataset v1.05,
consisting of 5000 subjective sentences derived
from Rotten Tomatoes (RT) reviews and 5000 ob-
jective sentences derived from IMDB plot sum-
maries (Pang and Lee, 2004). This task is prob-
ably less sensitive to the subjectivity within word
embeddings than sentiment classification, as de-
termining whether a sentence is subjective or ob-
jective should ideally be an objective undertaking.

Topic Classification Task We use the 20
Newsgroups dataset6 (“bydate” version), whereby
the newsgroups are organized into six subject mat-
ter groupings. We extract the message body and
split them into sentences. Each group’s sentences
then form the in-topic class, and we randomly
sample an equivalent number of sentences from
the remaining newsgroups to form the out-of-topic
class. This results in six datasets, each correspond-
ing to a binary classification task. In most cases,
we present the average results, and where appro-
priate we enumerate the results for each dataset.
Hypothetically, this task is the least affected by the
subjectivity within word embeddings.

3 Comparative Analyses of Subjective vs.
Objective Corpora

We conduct a series of comparative analyses under
various setups. For each, we compare the perfor-
mance in the evaluation tasks when using the Ob-
jective Corpus and the Subjective Corpus. Table 1
shows the results for this series of analyses.

Initial Condition Setup I seeks to answer
whether there is any difference between word em-
beddings derived from the Objective Corpus and
the Subjective Corpus. The word embeddings
were trained on the whole data respectively. Ta-
ble 1 shows the corpus statistics and classification
accuracies. Evidently, the Subjective word embed-
dings outperform the Objective word embeddings
on all the evaluation tasks. The margins are largest
for sentiment classification (86.5% vs. 81.5% or
+5% Amazon, and 78.2% vs. 75.4% or +2.8% on
Rotten Tomatoes or RT). For subjectivity and topic
classifications, the differences are smaller.

5http://www.cs.cornell.edu/people/
pabo/movie-review-data/subjdata.README.
1.0.txt

6http://qwone.com/˜jason/20Newsgroups/

http://jmcauley.ucsd.edu/data/amazon/
http://www.cs.cornell.edu/people/pabo/movie-review-data/rt-polaritydata.README.1.0.txt
http://www.cs.cornell.edu/people/pabo/movie-review-data/rt-polaritydata.README.1.0.txt
http://www.cs.cornell.edu/people/pabo/movie-review-data/rt-polaritydata.README.1.0.txt
http://www.cs.cornell.edu/people/pabo/movie-review-data/subjdata.README.1.0.txt
http://www.cs.cornell.edu/people/pabo/movie-review-data/subjdata.README.1.0.txt
http://www.cs.cornell.edu/people/pabo/movie-review-data/subjdata.README.1.0.txt
http://qwone.com/~jason/20Newsgroups/


1215

Setup Corpus
Corpus Statistics Classification (Accuracy)

# types # tokens # sentences
Sentiment

Subjectivity Topic
Amazon RT

I
Objective 1.34M 1.81B 89M 81.5 75.4 90.5 83.2
Subjective 1.47M 5.49B 313M 86.5 78.2 91.1 83.4

II
Objective 1.34M 1.81B

89M
81.5 75.4 90.5 83.2

Subjective 0.59M 1.56B 85.5 77.9 90.7 82.8

III
Objective

0.29M
1.75B

89M
81.6 75.6 90.6 83.4

Subjective 1.54B 85.4 77.9 90.6 82.8

Table 1: Controlled comparison of Objective and Subjective corpora

As earlier hypothesized, the sentiment classifi-
cation task is more sensitive to subjectivity within
word embeddings than the other tasks. Therefore,
training word embeddings on a subjective corpus
may confer an advantage for such tasks. On the
other hand, the corpus statistics show a substan-
tial difference in corpus size, which could be an
alternative explanation for the outperformance by
the Subjective Corpus if the larger corpus contains
more informative distributional statistics.

Controlling for Corpus Size In Setup II, we
keep the number of sentences in both corpora the
same, by randomly downsampling sentences in the
Subjective Corpus. This procedure consequently
reduces the number of types and tokens (see Ta-
ble 1, Setup II, Corpus Statistics). Note that the
number of tokens in the Subjective corpus is now
fewer than in the Objective, the latter suffers no
change. Yet, even after a dramatic reduction in
size, the Subjective embeddings still outperform
the Objective significantly on both datasets of the
sentiment classification task (+4% on Amazon and
+2.5% on RT), while showing similar performance
on subjectivity and topic classifications.

This bolsters the earlier observation that senti-
ment classification is more sensitive to subjectiv-
ity. While there is a small effect due to corpus size
difference, the gap in performance between Sub-
jective and Objective embeddings on sentiment
classification is still significant and cannot be ex-
plained away by the corpus size alone.

Controlling for Vocabulary While the Sub-
jective Corpus has a much smaller vocabulary
(i.e., # types), we turn a critical eye on whether
its apparent advantage lies in having access to spe-
cial word types that do not exist in the Objective
Corpus. In Setup III, we keep the training vocabu-
lary the same for both, removing the types that are

Objective Corpus Subjective Corpus
waste, money, return, love,
great, and, loves, refund,
Great, This, product,
recommend, this, even,
Very, returned, easy, not,
send, sent, customer, item,
broke, defective, her

money, waste, return, and,
Great, love, refund,
recommend, great, this,
loves, even, product, This,
Very, easy, item, junk,
anyone, Don’t, horrible,
gift, poor, Do, returned

Table 2: Top words of misclassified sentences

present in one corpus but not in the other, so that
out-of-vocabulary words are ignored in the train-
ing phase. Table 1, Setup III, shows significant
reduction in types for both corpora. Yet, the out-
performance by the Subjective embeddings on the
sentiment classification task still stands (+3.8% on
Amazon and +2.3% on RT). Moreover, it is so for
both Amazon and Rotten Tomatoes datasets, im-
plying that it is not due to close in-domain sim-
ilarity between the corpora used for training the
word embeddings and the classification tasks.

Significant Words To get more insights on
the difference between the Subjective and Objec-
tive corpora, we analyze the mistakes word em-
beddings make on the development folds. At this
point we focus on the sentiment classification task
and specifically on the Amazon data, which in-
dicates the largest performance differences in the
controlled experiments (see Table 1, Setup III).

As words are still the main unit of informa-
tion in distributional word embeddings, we extract
words strongly associated with misclassified sen-
tences. We employed log-odds ratio with informa-
tive Dirichlet prior method (Monroe et al., 2008)
to quantify this association. It is used to contrast
the words in misclassified vs. correctly classified
sentences, and accounts for the variance of words
and their prior counts taken from a large corpus.



1216

Table 2 shows the top 25 words most associated
with the misclassified sentences, sorted by their
association scores. On average 50% of the mis-
takes overlap for both word embeddings, there-
fore, some of the words are included in both lists.
40 − 44% of these words carry positive or neg-
ative sentiment connotations in general (see the
underlined words in Table 2), while other words
like return or send may carry sentiment connota-
tion in e-commerce context. We check if a word
carries sentiment connotation using sentiment lex-
icon compiled by Hu and Liu (2004), including
6789 words along with positive or negative labels.

We also observe linguistic negations (i.e., not,
Don’t). For instance, the word most associ-
ated with the Objective-specific mistakes (exclud-
ing the Subjective misclassified sentences) is not,
which suggests that perhaps Subjective word em-
bedding accommodates better understanding of
linguistic negations, which may partially explain
the difference. However, our methodology as out-
lined in Section 2.2 permits exchangeable word or-
der and is not intended to analyze structural inter-
action between words. We focus on further anal-
ysis of sentiment words, leaving linguistic nega-
tions in word embeddings for future investigation.

Controlling for Sentiment Words To con-
trol for the “amount” of sentiment in the Subjec-
tive and Objective corpora, we use sentiment lex-
icon compiled by Hu and Liu (2004). For each
corpus, we create two subcorpora: With Sentiment
contains only the sentences with at least one word
from the sentiment lexicon, while Without Senti-
ment is the complement. We match the corpora on
the number of sentences, downsampling the larger
corpus, train word embeddings on each subcorpus,
and proceed with the classification experiments.
Table 3 shows the results, including that of random
word embeddings for reference. Sentiment lexi-
con has a significant impact on the performance
of sentiment and subjectivity classifications, and
a smaller impact on topic classification. Without
sentiment, the Subjective embeddings prove more
robust, still outperforming the Objective on senti-
ment classification, while the Objective performs
close to random word embeddings on Amazon .

In summary, evidences from the series of con-
trolled experiments support the existence of some
X-factor to the Subjective embeddings, which con-
fers superior performance in subjectivity-sensitive
tasks such as sentiment classification.

Corpus Subcorpus Sentiment Subject- TopicSentiment? Amazon RT ivity

Objective With 81.8 75.2 90.7 83.1Without 76.1 67.2 87.8 82.6

Subjective With 85.5 78.0 90.3 82.5Without 79.8 71.0 89.1 82.2

Random Embeddings 76.1 62.2 80.1 71.5

Table 3: With and without sentiment

4 Sentiment-Infused Word Embeddings

To leverage the consequential sentiment informa-
tion, we propose a family of methods, called
SentiVec, for training distributional word embed-
dings that are infused with information on the sen-
timent polarity of words. The methods are built
upon Word2Vec optimization algorithm and make
use of available lexical sentiment resources such
as SentiWordNet (Baccianella et al., 2010), senti-
ment lexicon by Hu and Liu (2004), and etc.

SentiVec seeks to satisfy two objectives, namely
context prediction and lexical category prediction:

logL = logLword2vec(W ;C) + λ logLlex(W,L), (5)

where Lword2vec(W ;C) is the Skip-gram objec-
tive as in (4); Llex(W,L) is a lexical objective for
corpus W and lexical resource L; and λ is a trade-
off parameter. Lexical resource L = {Xi}ni=1
comprises of n word sets, each Xi contains words
of the same category. For sentiment classification,
we consider positive and negative word categories.

4.1 Logistic SentiVec
Logistic SentiVec admits lexical resource in the
form of two disjoint word sets, L = {X1, X2},
X1 ∩X2 = ∅. The objective is to tell apart which
word set of L word w belongs to:

logLlex(W,L) (6)

=
∑
w∈X1

log P(w ∈ X1) +
∑
w∈X2

log P(w ∈ X2).

We further tie these probabilities together, and cast
the objective as a logistic regression problem:

P(w ∈ X1) = 1− P(w ∈ X2) = σ(vw · τ), (7)

where vw is a word embedding and τ is a direc-
tion vector. Since word embeddings are gener-
ally invariant to scaling and rotation when used
as downstream feature representations, τ can be
chosen randomly and fixed during training. We



1217

experiment with randomly sampled unit length di-
rections. For simplicity, we also scale embedding
vw to its unit length when computing vw ·τ , which
now equals to cosine similarity between vw and τ .

When vw is completely aligned with τ , the co-
sine similarity between them is 1, which maxi-
mizes P(w ∈ X1) and favors words in X1. When
vw is opposite to τ , the cosine similarity equals to
−1, which maximizes P(w ∈ X2) and predicts
vectors from X2. Orthogonal vectors have cosine
similarity of 0, which makes both w ∈ X1 and
w ∈ X2 equally probable. Optimizing (6) makes
the corresponding word embeddings ofX1 andX2
gravitate to the opposite semispaces and simulates
clustering effect for the words of the same cate-
gory, while the Word2Vec objective prevents words
from collapsing to the same directions.

Optimization The objective in (6) permits
simple stochastic gradient ascent optimization and
can be combined with negative sampling proce-
dure for Skip-gram in (5). The gradient for un-
normalized embedding vw is solved as follows:(
logL[w∈X1](D,L)

)′
vwi

= (log P (x ∈ X1))′vwi

=
1

‖vw‖2
σ

(
−vw · τ
‖vw‖

)(
τi ‖vw‖ − vwi

vw · τ
‖vw‖

)
(8)

The optimization equation for vw, when w ∈ X2,
can be derived analogously.

4.2 Spherical SentiVec
Spherical SentiVec extends Logistic SentiVec by
dealing with any number of lexical categories,
L = {Xi}ni=1. As such, the lexical objective takes
on generic form:

logLlex(W,L) =
n∑
i=1

∑
w∈Xi

log P (w ∈ Xi), (9)

Each P (w ∈ Xi) defines embedding generating
process. We assume each length-normalized vw
for w of L is generated w.r.t. a mixture model
of von Mises-Fisher (vMF) distributions. vMF is
a probability distribution on a multidimensional
sphere, characterized by parameters µ (mean di-
rection) and κ (concentration parameter). Sam-
pled points are concentrated around µ; the greater
the κ, the closer the sampled points are to µ.
We consider only unimodal vMF distributions, re-
stricting concentration parameters to be strictly
positive. Hereby, each Xi ∈ L is assigned to vMF

distribution parameters (µi, κi) and the member-
ship probabilities are defined as follows:

P(w ∈ Xi) = P (vw;µi, κi) =
1

Zκi
eκiµi·vw ,

(10)

where Zκ is the normalization factor.
The Spherical SentiVec lexical objective forces

words of every Xi ∈ L to gravitate towards and
concentrate around their direction mean µi. As in
Logistic SentiVec, it simulates clustering effect for
the words of the same set. In comparison to the
direction vector of Logistic SentiVec, mean direc-
tions of Spherical SentiVec when fixed can sub-
stantially influence word embeddings training and
must be carefully selected. We optimize the mean
directions along with the word embeddings using
alternating procedure resembling K-means clus-
tering algorithm. For simplicity, we keep concen-
tration parameters tied, κ1 = κ2 = ... = κn = κ,
and treat κ as a hyperparameter of this algorithm.

Optimization We derive optimization pro-
cedure for updating word embeddings assuming
fixed direction means. Like Logistic SentiVec,
Spherical SentiVec can be combined with the neg-
ative sampling procedure of Skip-gram. The gra-
dient for unnormalized word embedding vw is
solved by the following equation:

(
logL[w∈Xi] (W,L)

)′
vwj

= κi

(
µij ‖vw‖ − vwj vw·µi‖vw‖

)
‖vw‖2

(11)

Once word embedding vw (w ∈ Xi) is updated,
we revise direction mean µi w.r.t. maximum like-
lihood estimator:

µi =

∑
w∈Xi vw∥∥∥∑w∈Xi vw∥∥∥ . (12)

Updating the direction means in such a way en-
sures that the lexical objective is non-decreasing.
Assuming the stochastic optimization procedure
for Lword2vec complies with the same non-
decreasing property, the proposed alternating pro-
cedure converges.

5 Related Work

There have been considerable research on im-
proving the quality of distributional word em-
beddings. Bolukbasi et al. (2016) seek to de-
bias word embeddings from gender stereotypes.
Rothe and Schütze (2017) incorporate WordNet



1218

lexeme and synset information. Mrkšic et al.
(2016) encode antonym-synonym relations. Liu
et al. (2015) encode ordinal relations such as hy-
pernym and hyponym. Kiela et al. (2015) augment
Skip-gram to enforce lexical similarity or related-
ness constraints, Bollegala et al. (2016) modify
GloVe optimization procedure for the same pur-
pose. Faruqui et al. (2015) employ semantic re-
lations of PPDB, WordNet, FrameNet to retrofit
word embeddings for various prediction tasks. We
use this Retrofitting method7 as a baseline.

Socher et al. (2011) derive multi-word embed-
dings for sentiment distribution prediction, while
we focus on lexical distributional analysis. Maas
et al. (2011) and Tang et al. (2016) use document-
level sentiment annotations to fit word embed-
dings, but document annotation might not always
be available for distributional analysis on neutral
corpora such as Wikipedia. SentiVec relies on
simple sentiment lexicon instead. Refining (Yu
et al., 2018) aligns the sentiment scores taken from
lexical resource and the cosine similarity scores
of corresponding word embeddings. The method
generally requires fine-grained sentiment scores
for the words, which may not be available in some
settings. We use Refining as a baseline and adopt
coarse-grained sentiment lexicon for this method.

Villegas et al. (2016) compare various distri-
butional word embeddings arising from the same
corpus for sentiment classification, whereas we fo-
cus on the differentiation in input corpora and pro-
pose novel sentiment-infused word embeddings.

6 Experiments

The objective of experiments is to study the ef-
ficacy of Logistic SentiVec and Spherical SentiVec
word embeddings on the aforementioned text clas-
sification tasks. One natural baseline is Word2Vec,
as SentiVec subsumes its context prediction objec-
tive, while further incorporating lexical category
prediction. We include two other baselines that
can leverage the same lexical resource but in man-
ners different from SentiVec, namely: Retrofitting
(Faruqui et al., 2015) and Refining (Yu et al.,
2018). For these methods, we generate their word
embeddings based on Setup III (see Section 3).
All the methods were run multiple times with var-
ious hyperparameters, optimized via grid-search;
for each we present the best performing setting.

7Original code is available at: https://github.
com/mfaruqui/retrofitting

First, we discuss the sentiment classification
task. Table 4 shows the unfolded results for the 24
classification datasets of Amazon, as well as for
Rotten Tomatoes. For each classification dataset
(row), and for the Objective and Subjective em-
bedding corpora respectively, the best word em-
bedding methods are shown in bold. An aster-
isk indicates statistically significant8 results at 5%
in comparison to Word2Vec. Both SentiVec vari-
ants outperform Word2Vec in the vast majority of
the cases. The degree of outperformance is higher
for the Objective than the Subjective word embed-
dings. This is a reasonable trend given our previ-
ous findings in Section 3. As the Objective Corpus
encodes less information than the Subjective Cor-
pus for sentiment classification, the former is more
likely to benefit from the infusion of sentiment in-
formation from additional lexical resources. Note
that the sentiment infusion into the word embed-
dings comes from separate lexical resources, and
does not involve any sentiment classification label.

SentiVec also outperforms the two baselines
that benefit from the same lexical resources.
Retrofitting does not improve upon Word2Vec,
with the two embeddings essentially indistinguish-
able (the difference is only noticeable at the sec-
ond decimal point). Refining makes the word em-
beddings perform worse on the sentiment classifi-
cation task. One possible explanation is that Refin-
ing normally requires fine-grained labeled lexicon,
where the words are scored w.r.t. the sentiment
scale, whereas we use sentiment lexicon of two la-
bels (i.e., positive or negative). SentiVec accepts
coarse-grained sentiment lexicons, and potentially
could be extended to deal with fine-grained labels.

As previously alluded to, topic and subjectivity
classifications are less sensitive to the subjectiv-
ity within word embeddings than sentiment clas-
sification. One therefore would not expect much,
if any, performance gain from infusion of senti-
ment information. However, such infusion should
not subtract or harm the quality of word embed-
dings either. Table 5 shows that the unfolded re-
sults for topic classification on the six datasets, and
the result for subjectivity classification are similar
across methods. Neither the SentiVec variants, nor
Retrofitting and Refining, change the subjectivity
and topic classification capabilities much, which
means that the used sentiment lexicon is targeted
only at the sentiment subspace of embeddings.

8We use paired t-test to compute p-value.

https://github.com/mfaruqui/retrofitting
https://github.com/mfaruqui/retrofitting


1219

Corpus/Category
Objective Embeddings Subjective Embeddings

Word2Vec Retrofitting Refining SentiVec Word2Vec Retrofitting Refining SentiVecSpherical Logistic Spherical Logistic
Amazon

Instant Video 84.1 84.1 81.9 84.9∗ 84.9∗ 87.8 87.8 86.9 88.1 88.2
Android Apps 83.0 83.0 80.9 84.0∗ 84.0∗ 86.3 86.3 85.0 86.6 86.5
Automotive 80.7 80.7 78.8 81.0 81.3 85.1 85.1 83.8 84.9 85.0
Baby 80.9 80.9 78.6 82.1 82.2∗ 84.2 84.2 82.8 84.4 84.6
Beauty 81.8 81.8 79.8 82.4 82.7∗ 85.2 85.2 83.5 85.2 85.4
Books 80.9 80.9 78.9 81.0 81.3 85.3 85.3 83.6 85.3 85.5
CD & Vinyl 79.4 79.4 77.6 79.4 79.9 83.5 83.5 81.9 83.7 83.6
Cell Phones 82.2 82.2 80.0 82.9 83.0∗ 86.8 86.8 85.3 86.8 87.0
Clothing 82.6 82.6 80.7 83.8 84.0∗ 86.3 86.3 84.7 86.4 86.8
Digital Music 82.3 82.3 80.5 82.8 83.0∗ 86.3 86.3 84.6 86.1 86.3
Electronics 81.0 81.0 78.8 80.9 81.3 85.2 85.2 83.6 85.3 85.3
Grocery & Food 81.7 81.7 79.4 83.1∗ 83.1∗ 85.0 85.0 83.7 85.1 85.6∗
Health 79.7 79.7 77.9 80.4∗ 80.4 84.0 84.0 82.3 84.0 84.3
Home & Kitchen 81.6 81.6 79.5 82.1 82.1 85.4 85.4 83.9 85.3 85.4
Kindle Store 84.7 84.7 83.2 85.2 85.4∗ 88.3 88.3 87.2 88.3 88.6
Movies & TV 81.4 81.4 78.5 81.9 81.9 85.2 85.2 83.5 85.4 85.5
Musical Instruments 81.7 81.6 79.7 82.4 82.4 85.8 85.8 84.1 85.9 85.7
Office 82.0 82.0 80.0 83.0∗ 82.9 86.1 86.1 84.5 86.4 86.5∗
Garden 80.4 80.4 77.9 81.0 81.5 84.1 84.1 82.5 84.3 84.6∗
Pet Supplies 79.7 79.7 77.5 80.4 80.2 83.2 83.2 81.5 83.4 83.8
Sports & Outdoors 80.8 80.8 79.1 81.3∗ 81.2 84.6 84.6 83.1 84.3 84.7
Tools 81.0 81.0 79.3 81.0 81.3 84.7 84.7 83.2 84.8 84.9
Toys & Games 83.8 83.8 82.0 84.7 84.9∗ 87.2 87.2 85.7 87.1 87.5
Video Games 80.3 80.3 77.4 81.5 81.7∗ 84.9 84.9 83.2 85.0 84.9

Average 81.6 81.6 79.5 82.2 82.4 85.4 85.4 83.9 85.5 85.7

Rotten Tomatoes 75.6 75.6 73.4 75.8∗ 75.4 77.9 77.9 76.7 77.7 77.9

Table 4: Comparison of Sentiment-Infused Word Embeddings on Sentiment Classification Task

Corpus/Category
Objective Embeddings Subjective Embeddings

Word2Vec Retrofitting Refining SentiVec Word2Vec Retrofitting Refining SentiVecSpherical Logistic Spherical Logistic
Topic

Computers 79.8 79.8 79.6 79.6 79.8 79.8 79.8 79.8 79.7 79.7
Misc 89.8 89.8 89.7 89.8 90.0 90.4 90.4 90.6 90.4 90.3
Politics 84.6 84.6 84.4 84.5 84.6 83.8 83.8 83.5 83.6 83.5
Recreation 83.4 83.4 83.1 83.1 83.2 82.6 82.6 82.5 82.7 82.8
Religion 84.6 84.6 84.5 84.5 84.6 84.2 84.2 84.2 84.1 84.2
Science 78.2 78.2 78.2 78.1 78.3 76.4 76.4 76.1 76.7 76.6

Average 83.4 83.4 83.2 83.3 83.4 82.8 82.8 82.8 82.9 82.8

Subjectivity 90.6 90.6 90.0 90.6 90.6 90.6 90.6 90.3 90.7 90.8

Table 5: Comparison of Word Embeddings on Subjectivity and Topic Classification Tasks

Illustrative Changes in Embeddings To give
more insights on the difference between SentiVec
and Word2Vec, we show “flower” diagrams in Fig-
ure 1 for Logistic SentiVec and Figure 2 for Spher-
ical SentiVec. Each is associated with a reference
word (e.g., good for Figure 1a), and indicates rel-
ative changes in cosine distances between the ref-
erence word and the testing words surrounding the
“flower”. Every testing word is associated with a
“petal” or black axis extending from the center of
the circle. The “petal” length is proportional to the
relative distance change in two word embeddings:
κ =

dSentiV ec(wref ,wtesting)
dword2vec(wref ,wtesting)

, where dSentiV ec and
dword2vec are cosine distances between reference
wref and testing wtesting words in SentiVec and
Word2Vec embeddings correspondingly. If the dis-
tance remains unchanged (κ = 1), then the “petal”
points at the circumference; if the reference and
testing words are closer in the SentiVec embedding

than they are in Word2Vec (κ < 1), the “petal”
lies inside the circle; when the distance increases
(κ > 1), the “petal” goes beyond the circle.

The diagrams are presented for Objective Em-
beddings9. We use three reference words: good
(positive), bad (negative), time (neutral); as well
as three groups of testing words: green for words
randomly sampled from positive lexicon (Sec-
tor I-II), red for words randomly sampled from
negative lexicon (Sector II-III), and gray for fre-
quent neutral common nouns (Sector III-I).

Figure 1 shows changes produced by Logistic
SentiVec. For the positive reference word (Fig-
ure 1a), the average distance to the green words is
shortened, whereas the distance to the red words
increases. The reverse is observed for the nega-
tive reference word (Figure 1b). This observation

9The diagrams for Subjective Embeddings show the same
trend, with the moderate changes.



1220

I

IIIII

(a) Reference word: good (positive)

I

IIIII

(b) Reference word: bad (negative)

I

IIIII

(c) Reference word: time (neutral)

Figure 1: Relative changes in cosine distances in Logistic SentiVec contrasted with Word2Vec

I

IIIII

(a) Reference word: good (positive)

I

IIIII

(b) Reference word: bad (negative)

I

IIIII

(c) Reference word: time (neutral)

Figure 2: Relative changes in cosine distances in Spherical SentiVec contrasted with Word2Vec

complies with the lexical objective (7) of Logistic
SentiVec, which aims to separate the words of two
different classes. Note that the gray words suffer
only moderate change with respect to positive and
negative reference words. For the neutral refer-
ence word (Figure 1c), the distances are only mod-
erately affected across all testing groups.

Figure 2 shows that Spherical SentiVec tends
to make embeddings more compact than Logistic
SentiVec. As the former’s lexical objective (9) is
designed for clustering, but not for separation, we
look at the comparative strength of the clustering
effect on the testing words. For the positive refer-
ence word (Figure 2a), the largest clustering effect
is achieved for the green words. For the negative
reference word (Figure 2b), as expected, the red
words are affected the most. The gray words suf-
fer the least change for all the reference words.

In summary, SentiVec effectively provides an
advantage for subjectivity-sensitive task such as
sentiment classification, while not harming the
performance of other text classification tasks.

7 Conclusion

We explore the differences between objective and
subjective corpora for generating word embed-
dings, and find that there is indeed a difference in
the embeddings’ classification task performances.
Identifying the presence of sentiment words as one
key factor for the difference, we propose a novel
method SentiVec to train word embeddings that
are infused with the sentiment polarity of words
derived from a separate sentiment lexicon. We
further identify two lexical objectives: Logistic
SentiVec and Spherical SentiVec. The proposed
word embeddings show improvements in senti-
ment classification, while maintaining their per-
formance on subjectivity and topic classifications.

Acknowledgments

This research is supported by the National Re-
search Foundation, Prime Minister’s Office, Sin-
gapore under its NRF Fellowship Programme
(Award No. NRF-NRFF2016-07).



1221

References
Stefano Baccianella, Andrea Esuli, and Fabrizio Sebas-

tiani. 2010. Sentiwordnet 3.0: an enhanced lexical
resource for sentiment analysis and opinion mining.
In LREC. volume 10.

Danushka Bollegala, Mohammed Alsuhaibani,
Takanori Maehara, and Ken-ichi Kawarabayashi.
2016. Joint word representation learning using a
corpus and a semantic lexicon. In Proceedings of
AAAI.

Tolga Bolukbasi, Kai-Wei Chang, James Y Zou,
Venkatesh Saligrama, and Adam T Kalai. 2016.
Man is to computer programmer as woman is to
homemaker? debiasing word embeddings. In Pro-
ceedings of NIPS.

Ronan Collobert, Jason Weston, Léon Bottou, Michael
Karlen, Koray Kavukcuoglu, and Pavel Kuksa.
2011. Natural language processing (almost) from
scratch. JMLR 12(Aug).

Manaal Faruqui, Jesse Dodge, Sujay Kumar Jauhar,
Chris Dyer, Eduard Hovy, and Noah A Smith. 2015.
Retrofitting word vectors to semantic lexicons. In
Proceedings of NAACL-HLT .

Yoav Goldberg. 2016. A primer on neural network
models for natural language processing. JAIR 57.

Minqing Hu and Bing Liu. 2004. Mining and summa-
rizing customer reviews. In Proceedings of the tenth
ACM SIGKDD international conference on Knowl-
edge discovery and data mining. ACM.

Douwe Kiela, Felix Hill, and Stephen Clark. 2015.
Specializing word embeddings for similarity or re-
latedness. In Proceedings of EMNLP.

Quan Liu, Hui Jiang, Si Wei, Zhen-Hua Ling, and
Yu Hu. 2015. Learning semantic word embeddings
based on ordinal knowledge constraints. In Pro-
ceedings of ACL-IJCNLP. volume 1.

Andrew L Maas, Raymond E Daly, Peter T Pham, Dan
Huang, Andrew Y Ng, and Christopher Potts. 2011.
Learning word vectors for sentiment analysis. In
Proceedings of ACL-HLT .

Julian McAuley, Christopher Targett, Qinfeng Shi, and
Anton Van Den Hengel. 2015. Image-based recom-
mendations on styles and substitutes. In Proceed-
ings of the 38th International ACM SIGIR Confer-
ence on Research and Development in Information
Retrieval. ACM.

Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Cor-
rado, and Jeff Dean. 2013. Distributed representa-
tions of words and phrases and their composition-
ality. In C. J. C. Burges, L. Bottou, M. Welling,
Z. Ghahramani, and K. Q. Weinberger, editors, Ad-
vances in Neural Information Processing Systems
26.

Burt L Monroe, Michael P Colaresi, and Kevin M
Quinn. 2008. Fightin’words: Lexical feature selec-
tion and evaluation for identifying the content of po-
litical conflict. Political Analysis 16(4).

Nikola Mrkšic, Diarmuid OSéaghdha, Blaise Thom-
son, Milica Gašic, Lina Rojas-Barahona, Pei-Hao
Su, David Vandyke, Tsung-Hsien Wen, and Steve
Young. 2016. Counter-fitting word vectors to lin-
guistic constraints. In Proceedings of NAACL-HLT .

Bo Pang and Lillian Lee. 2004. A sentimental educa-
tion: Sentiment analysis using subjectivity summa-
rization based on minimum cuts. In Proceedings of
ACL.

Bo Pang and Lillian Lee. 2005. Seeing stars: Exploit-
ing class relationships for sentiment categorization
with respect to rating scales. In Proceedings of ACL.

Jeffrey Pennington, Richard Socher, and Christopher
Manning. 2014. Glove: Global vectors for word
representation. In Proceedings of EMNLP.

Sascha Rothe and Hinrich Schütze. 2017. Autoex-
tend: Combining word embeddings with semantic
resources. Computational Linguistics 43(3).

Richard Socher, Jeffrey Pennington, Eric H Huang,
Andrew Y Ng, and Christopher D Manning. 2011.
Semi-supervised recursive autoencoders for predict-
ing sentiment distributions. In Proceedings of
EMNLP.

Richard Socher, Alex Perelygin, Jean Wu, Jason
Chuang, Christopher D Manning, Andrew Ng, and
Christopher Potts. 2013. Recursive deep models
for semantic compositionality over a sentiment tree-
bank. In Proceedings of EMNLP.

Duyu Tang, Furu Wei, Bing Qin, Nan Yang, Ting Liu,
and Ming Zhou. 2016. Sentiment embeddings with
applications to sentiment analysis. IEEE TKDE
28(2).

Marı́a Paula Villegas, Marı́a José Garciarena Uce-
lay, Juan Pablo Fernández, Miguel A Álvarez Car-
mona, Marcelo Luis Errecalde, and Leticia Cagn-
ina. 2016. Vector-based word representations for
sentiment analysis: a comparative study. In XXII
Congreso Argentino de Ciencias de la Computación
(CACIC 2016)..

L. C. Yu, J. Wang, K. R. Lai, and X. Zhang. 2018. Re-
fining word embeddings using intensity scores for
sentiment analysis. IEEE/ACM Transactions on Au-
dio, Speech, and Language Processing 26(3).


