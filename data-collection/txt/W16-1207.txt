



















































Learning Translations for Tagged Words: Extending the Translation Lexicon of an ITG for Low Resource Languages


Proceedings of the Workshop on Multilingual and Cross-lingual Methods in NLP, pages 55–64,
San Diego, California, June 17, 2016. c©2016 Association for Computational Linguistics

Learning Translations for Tagged Words:
Extending the Translation Lexicon of an ITG for Low Resource Languages

Markus Saers and Dekai Wu
Human Language Technology Center

Department of Computer Science and Engineering
The Hong Kong University of Science and Technology

HKUST, Clear Water Bay, Kowloon, Hong Kong
{masaers|dekai}@cs.ust.hk

Abstract

We tackle the challenge of learning part-of-
speech classified translations as part of an
inversion transduction grammar, by learning
translations for English words with known
part-of-speech tags, both from existing trans-
lation lexica and from parallel corpora. When
translating from a low resource language into
English, we can expect to have rich resources
for English, such as treebanks, and small
amounts of bilingual resources, such as trans-
lation lexica and parallel corpora. We solve
the problem of integrating these heterogeneous
resources into a single model using stochastic
Inversion Transduction Grammars, which we
augment with wildcards to handle unknown
translations.

1 Introduction

We introduce an augmentation to Inversion Trans-
duction Grammars, or ITGs (Wu, 1997), that allow
us to under specify the translation of lexical rules,
and defer to observed usage to decide, within the
syntactic context of the lexical rule, how the wild-
card should be instantiated. Having specific wild-
card rules instead of just instantiating all possible
translations for the sentence pair at hand (a) allows
us to use it as a back-off: we can limit the use of
these spurious rules to the circumstances when we
have no other choice, and (b) allows us to explic-
itly reserve some probability mass for the unknown
translations of a lexical unit, which also gives us a
hint about how certain we are about the known trans-
lations. This allows us to say things like “we know
that twelve is a cardinal number but it’s not in our

translation lexicon, let’s see what it is translated to in
the parallel corpus”. Whith small amounts of data,
it is imperative to get the structural generalizations
right in order to get adequate statistics; there simply
aren’t enough examples of longer chunks to get reli-
able counts. This approach allows us to make good
use of the limited parallel resources that are available
in a way that traditional statistical machine transla-
tion systems are unable to; and to make good use
of the human translation examples in a way that tra-
ditional rule-based machine translation systems are
unable to. Even statistical tree-based models typi-
cally require word alignments as input, which again
requires large amounts of parallel data to learn well.
In contrast, Inversion Transduction Grammars learn
the translation model and the word alignments si-
multaneously, effectively integrating over all pos-
sible alignments during training. This is possible
because of the strong modeling bias, that limits the
search space of possible compositions to make the
seemingly intractable problem of bilingual compo-
sition tractable, without resorting to heuristics. The
modeling bias has been empirically shown to still
allows the model to express most structural differ-
ences that have been observed between natural lan-
guages. Taken together, this lets us mostly rely on
what we already know, and incorporate new knowl-
edge as we encounter the need to. This is ideal in
the low resource language setting, where we want
to stick to presumably hard earned prior knowledge
when possible, but still have the option of adding to
this knowledge base when needed.

The low-resource language translation setting pre-
sumes that we have small amounts of resources in

55



the input language, and large amounts of resources
in the output language (English). The job of a trans-
lation system is to produce fluent output that ade-
quately represents the meaning of the input, so a
translation system should be biased towards the out-
put language. We do this by basing our ITG model
on an English treebank, which allows us to (a) ex-
tract a binarized context-free grammar, CFG, and (b)
estimate initial probabilities for the structural rules.
The stochastic CFG can then be mirrored to form a
grammatical channel model (Wu and Wong, 1998).

Conventional statistical machine translation, or
SMT, systems such as phrase-based SMT rely on
large amounts of parallel data to collect statistics
over how large chunks translate between two lan-
guages. These models are highly specific, and may
have two different rules for example, for a long and
complicated noun phrase with the determiner and
for the very same noun phrase without it. Need-
less to say this kind of modeling is too wasteful to
be of much use when there is very small amounts of
parallel data available. Tree-based models, models
that allow for chunks containing general categories
as opposed to fully lexicalized chunks, are better
able to generalize, but make poor use of the data
by disregarding translations that fall outside of the
single monolingually motivated parse tree that has
been committed to. A forest-based system would al-
leviate this problem, but two problems still remain:
The conventional systems all take a preexisting word
alignment as input, disregarding anything that does
not conform to it. And none of them make use of
forests in the output language.

In contrast to conventional systems, our proposed
model jointly models the parse forest of the out-
put language, the lexical alignment to the input lan-
guage, and thus also the projection of the output lan-
guage parse forest onto the input language sentence,
as well as the structural differences needed to make
this projection happen. All of this is possible because
of ITGs: Their inductive bias allows us to disre-
gard huge swathes of the search space when explain-
ing the structural differences. Their similarity with
CFGs allows us to use an English context-free gram-
mar as a starting point for induction. And their com-
putational complexity allows us to efficiently collect
rule expectations.

The presented model is capable of exploring the

entire space of structural differences between, in our
case, English and Chinese, that conform to (a) the
English CFG, and (b) the ITG-constraints. It is also
capable of expanding the translation dictionary be-
yond what we initialize it with to cover the small par-
allel corpus that we assume is available. Although
Chinese is by no means a low resource language, it
is vastly different from English, meaning that there
are plenty of structural differences to learn. We also
allow very limited Chinese resources in building our
model, simulating a low resource language setting.

Inversion transductions are formally a family of
transductions, where a transduction is the bilingual
version of a formal language, such that it relates two
formal languages to each other. Inversion transduc-
tions are generated by Inversion Transduction Gram-
mars, or ITGs (Wu, 1997), which share several traits
with CFGs in that the transduction rules have single
non-terminals on the left-hand side, and in that there
is always a 2-normal form equivalence for every
ITG. The latter is quite rare for transduction gram-
mars, and limits the structural differences that can
be generated between the languages. These limits in
structural differences have been empirically shown
to include most of the differences found between nat-
ural languages (Søgaard and Wu, 2009), and make
efficient processing possible.

Formally, an ITG is a tuple ⟨N ,W0,W1,R, S⟩,
where N is a finite nonempty set of nonterminals,
W0 is a finite set of terminals in the output language
L0, W1 is a finite set of terminals in the input lan-
guage L1, R is a finite nonempty set of inversion
transduction rules and S ∈ N is a designated start
symbol. An inversion transduction rule is restricted
to take one of the following forms:

S → [A] , A→ [φ+] , A→ ⟨φ+⟩

where S ∈ N is the start symbol, A ∈ N is a non-
terminal, and φ+ is a nonempty sequence of non-
terminals and biterminals. A biterminal is a pair:
W∗0 ×W∗1 , where at least one of the strings have to
be nonempty. The square and angled brackets signal
straight and inverted order respectively. The brack-
ets are frequently left out when there is only one el-
ement on the right-hand side.

The ITG 2-normal form is analogous to the Chom-
sky normal form for CFGs, where the rules are fur-

56



ther restricted to only the following forms:

S → A, A→ [BC] , A→ ⟨BC⟩,
A→ e/f, A→ e/ϵ, A→ ϵ/f

where S ∈ N is the start symbol, A,B,C ∈ N are
nonterminals, e ∈ W0 is an L0 token, f ∈ W1 is an
L1 token, and ϵ is the empty token.

A bracketing ITG, or BITG, has only one nonter-
minal symbol (other than the dedicated start sym-
bol), which means that the nonterminals carry no in-
formation at all other than the fact that their yields
are discrete unit.

2 Related work

The use of structure as input to the training of a sta-
tistical model in machine translation was pioneered
by Yamada and Knight (2001), where they extend
the IBM model 1 (Brown et al., 1993) to incorpo-
rate syntactic features derived from a parse tree on
the output language (the input to the noisy chan-
nel, but the output of the decoder). The genera-
tive story of the model is that an English parse tree
has the children of its nodes reordered, gets the op-
tion to insert a foreign token to the left or right of
any node, and finally have all the English leaf nodes
translated. Reading the leaf nodes of the tree in or-
der yields the generated foreign sentence. Being a
generative model, it is straight forward to train us-
ing EM, which they do. Manual evaluation shows
that the word alignments corresponding to the one-
best derivation are better than those of IBM model
5, and that they managed to get 10 out of the eval-
uated 50 sentence pairs perfectly aligned, whereas
IBM model 5 got none. There are two key differ-
ences between Yamada and Knight (2001) and out
model: (a) Their model describes how a foreign sen-
tence is generated from an English parse tree, our
model describes how sentence pairs are jointly gen-
erated. And (b) their model requires committing to
a single English parse tree, our model jointly parses
and aligns the sentences, effectively integrating out
all parse trees that our grammar allows for the En-
glish sentence.

Perhaps the most prolific translation model that
involves trees is Galley et al. (2004), which
learns very complex rules such as (ne VB pas) →
(VP (AUX does)  (RB not)  x2) where x2 is a vari-
able binding to the second element in the left-hand

side. The method takes a parallel sentence pair
where one of the sentences has been parsed, and a
word alignment, and produces, for each observed
word aligned sentence pair, the minimal set of rules
to explain it. This method allows complicated rules
to be extracted, and different feature scores to be
calculated for the extracted rules. This does, how-
ever, come at the cost of not being able to opti-
mize the model in any meaningful way. Instead, one
has to resort to tuning the feature weights used by
the decoder. Any mistakes made by the parser or
the automatic word aligner are incorporated into the
model without any recourse. In contrast, our model
jointly parses the output language and aligns the sen-
tences, within a generative model that can be opti-
mized globally across the entire training data.

It is possible to alleviate the mismatch between
given alignments and parse trees. Riesa and Marcu
(2010) formulate a discriminative k-best alignment
model with hundreds of features that can be used to
choose the best alignment that matches a given tree.
The method as presented require some analysis on
both input and output language for the feature model,
as well as a set of hand aligned sentences for train-
ing; neither of which are readily available for low
resource languages. Similarly, DeNero and Klein
(2007) describe a model to tailor word alignments
to existing syntactic trees; in our model, we instead
consider all possible trees allowed by a CFG. Burkett
et al. (2010) and Burkett and Klein (2012) take the
opposite approach and alter the trees to fit the align-
ments, as presented, their approach requires parallel
treebanks to train on, which cannot be expected to be
available when translating to or from a low resource
language.

It is also possible to learn translation rules focus-
ing on the syntax of the input language rather than
output language. Huang et al. (2006) turn the tables,
and learn translation rules with the input language
parsed. Their approach forces the decoder to com-
mit to a single input tree and then build possible out-
put strings rather than building possible output trees.
The model has also been generalized so that rules
can be extracted from input forests rather than sin-
gle trees (Mi and Huang, 2008), and the decoder has
been extended to accept forests as input (Mi et al.,
2008). The rule extraction process still requires a
word alignment to be provided. Since our model is

57



based on a grammar rather than parse trees, it essen-
tially integrates out all trees as well as the alignments
needed to perform the rule extraction. There is a dif-
ference in that we have an output language grammar,
but we still implicitly build input language forests.

Our model is related to the grammatical channel
model of Wu and Wong (1998) in that the way we set
up our initial grammar is similar, but where they then
proceed to translate directly with it, induction has
just started with our model. Key differences is that
they merely mirror structural rules, allowing them to
be completely straight or completely inverted, and
that they make no effort to extend the translation dic-
tionary. In contrast, we allow long rules to be broken
down, which allows for more differences in word
order, and induce additional lexical translations that
are helpful in explaining the training data.

3 Induction Algorithm

The main focus of this paper is to learn translations
of known English lexical units. Our method relies
on being able to realize when it lacks the translation
needed to use an English lexical unit in a specific
syntactic context, and to find viable candidate trans-
lations from the observed sentence pair. The model
knows which English tokens go where (from the
treebank), and can be expected to have some trans-
lation vocabulary, mainly of content words (from
the translation lexicon). It can easily realize when
it lacks an applicable translation, and if there is an
applicable wildcard-rule we allow it to hypothesize
a realization of that wildcard from the current situ-
ation. This is a good mechanism for extending the
translation lexicon for content words, but it requires
the biparser to determine the syntactic context that
this hypothesizing takes place in. This in turn re-
quires the English structural rules to be adapted so
that they can handle Chinese as well. To do this, we
hypothesize that there are two main differences be-
tween English and Chinese: word order and func-
tion words. It makes sense for most content words
to have some kind of correspondence in the other
language, but they rarely occur in the same order.
The function words, on the other hand are sometimes
realized in both languages, but frequently one lan-
guage will use a function word, whereas the other
language will ignore that distinction, or realize it

NP

DT JJ NNS PP

NP’ NP’ NP’

NP’ NP’

Figure 1: All possible binarization of a structural rule with
four constituents. Any valid hyperpath from the four leaves to
the root constitute a valid binarization.

through word order or in some other syntactic way.
An example of the former is the Chinese question
particle (吗), which is realized in English through the
syntactic question construction. An example of the
latter is determinate case in English, which is sim-
ply not realized in Chinese, meaning that the word
the needs to be intelligently inserted when translat-
ing from Chinese to English.

To account for the word order differences, we
binarize the structural rules, allowing each binary
rule to be either straight or inverted. This allows
the model to account for any word order difference
within the ITG-constraints. Since the model is al-
ready heavily biased towards English, allowing En-
glish words to not translate into anything takes care
of the case where there is no Chinese equivalence
of an English function word. For the Chinese func-
tion words, there either is an English equivalence,
in which case the mechanism for content words will
find it, or there isn’t, in which case we simply allow
the biparser to algorithmically skip these words.

3.1 Step 1

Like Wu and Wong (1998) we start with an English
grammar and a translation dictionary. But, where
they allow structural rules to be either completely
straight or completely inverted, we allow any bina-
rization of the structural rules to be either straight or
inverted. And, where they allow any English word

58



Table 1: The result of converting the two English CFG rules JJ→ nice and NP→ DT JJ NNS PP to ITG rules using our proposed
method and that of Wu and Wong (1998). Clubs are wild, and NP′ is the auxiliary symbol for NP.

Our method Wu and Wong (1998)
JJ → nice/和蔼 NP → [DT NP′] NP → ⟨DT NP′⟩
JJ → nice/可亲 NP → [NP′ NP′] NP → ⟨NP′ NP′⟩

JJ → nice/♣ NP → [NP′ PP] NP → ⟨NP′ PP⟩
NP′ → [DT NP′] NP′ → ⟨DT NP′⟩
NP′ → [JJ NP′] NP′ → ⟨JJ NP′⟩
NP′ → [NP′ PP] NP′ → ⟨NP′ PP⟩

NP′ → [NP′ NNS] NP′ → ⟨NP′ NNS⟩
NP′ → [DT JJ] NP′ → ⟨DT JJ⟩

NP′ → [JJ NNS] NP′ → ⟨JJ NNS⟩
NP′ → [NNS PP] NP′ → ⟨NNS PP⟩

JJ → nice/和蔼
JJ → nice/可亲

NP → [DT JJ NNS PP]
NP → ⟨DT JJ NNS PP⟩

to translate into whatever the translation dictionary
mandates, we additionally assume that any English
word can translate into a wildcard token that repre-
sents the translations not in the dictionary. When bi-
narizing rules, there are many approaches one could
take. It is possible to “box” nonterminals to pro-
duce an exactly equivalent binary grammar. Boxing
means replacing any right-hand side occurrence of
a sequence, eg. DT NN with a single boxed nonter-
minal DT NN, which deterministically expand to the
original sequence with the rule DT NN → DT NN.
This tends to expand the set of nonterminals need-
lessly, and differentiating, for example, between the
case when a noun phrase contains three versus four
nouns is counter-productively specific. Instead, we
opt to have each left-hand side symbol be associated
with one auxiliary symbol that handles the binariza-
tion of that category, and represents a fragment of
it. We essentially generate the entire parse forest
rooted in the left-hand side, with the right-hand side
symbols as the leaves, and every internal node la-
beled with the auxiliary symbol (see Figure 1). From
this parse forest of the rule, we can extract all bi-
nary rules, and add a straight and an inverted ver-
sion of them to our ITG. This allows us to account
for any permutation of the right-hand sides that fall
within the ITG-constraints. Table 1 shows the result
of applying our method as opposed to applying the
method of Wu and Wong (1998) to the two English
CFG-rules NP → DT JJ NNS PP and JJ → nice.

3.2 Step 2
At this point our model has too many structural rules
and too few lexical rules, so the next step is to root
out the superfluous structural rules and build out the
coverage of the lexical rules. We do this by rees-
timating the ITG using a small parallel corpus and
variational Bayes; to weed out unnecessary struc-
tural rules we use a sparse prior, and to build out the
lexical coverage, we hypothesis that the wild card
will be binding predominantly to valid translations
of the associated English word, and instantiate the
observed bindings into the ITG.

Variational Bayes is mechanically similar to ex-
pectation maximization (Dempster et al., 1977) with
inside-outside (Lari and Young, 1990), but optimizes
the maximum a posteriori probability of the model
given the data and a prior, rather than the maximum
likelihood of the model given the data. Intuitively it
can be understood as collecting the fractional counts,
or expectations, from the data and discounting them
before maximizing. Given that we have rule expec-
tations from inside-outside E (A→ φ), the reesti-
mated probability of a rule is:

p (A→ φ) = e
ψ(E(A→φ)+αA→φ)∑
ϕ e

ψ(E(A→ϕ)+αA→ϕ)

where ψ is the digamma function, and αA→φ is the
prior of the rule A → φ (Kurihara and Sato, 2006).
It has previously been used for ITG learning (Zhang
et al., 2008; Saers and Wu, 2013), but only with uni-
form priors. In this paper, we distinguish three dif-
ferent classes of rules, and assign them different pri-

59



ors: preexisting lexical rules, lexical rules with wild-
cards, and other rules. We want the preexisting lex-
ical rules to have a relatively high prior, since we
trust our translation dictionary. We may or may not
want to keep the wildcard rules, and having a sepa-
rate prior for them allows us to easily choose their
fate. All other rules need to earn their keep, and are
thus subjected to a sparse prior.

The lexical coverage is extended by hypothesiz-
ing that what the wildcard binds to in the expectation
collection phase constitutes a valid translation. This
is an iterative step, where we first biparse the par-
allel corpus with the previous ITG. The wildcard is
allowed to match a preset number of Chinese tokens
when no known translation applies, and we also al-
low skipping both English and Chinese tokens as a
backoff. Once a sentence pair has been processed,
we inspect the parse forest and extract all bindings
for all wildcards and instantiate the corresponding
rules with half of the fractional counts it would have
had if it existed. The other half of the fractional
counts is retained with the wildcard rule.

The biparser we have is based on Saers et al.
(2009), with the addition of wildcard matching and
skipping. The algorithm is very suitable for our
needs, as it approximates the search to bring down
the time complexity of collecting the fractional
counts from O

(
n6

)
to O

(
bn3

)
, where n is propor-

tional to the length of the sentences in the pair, and b
is the width of the search beam (the wider the beam,
the more accurate the approximation). We imple-
ment skipping as implicit, low probability rules on
the following forms:

A→ [A ϵ/f ] , A→ [ϵ/f  A] ,
A→ [A e/ϵ] , A→ [e/ϵ A]

This allows the parser to maintain the category, but
consume a foreign or English token adjacent to a
known constituent. The low probability makes the
parser avoid skipping if possible.

4 Experimental Setup

To test the induction algorithm, we empirically com-
pare the results of our proposed system to a brack-
eting ITG induced from the same parallel corpus,
but without any prior knowledge of English. To ex-
tract a CFG over English, we use the Penn treebank

(Marcus et al., 1993), with relative frequencies of the
productions as the rule probabilities. As translation
dictionary, we use the Chinese–English Translation
Lexicon(Huang and Graff, 2002). When transform-
ing the CFG into an ITG (Section 3.1) we divide the
probability mass of the CFG-rules uniformly among
the ITG rules they spawn.

As the small parallel data set we use the IWSLT07
Chinese–English data set (Fordyce, 2007), which
contains 46,867 sentence pairs. Chinese sentence
are typically written without spaces, so we use a tool
(Wu, 1999) to segment it into more “word like” units.
We allow the wildcard to match zero or one such Chi-
nese tokens. To make use of the parallel data, we per-
form 10 iterations of reestimation (Section 3.2), with
a beam width b = 100, and the following priors: pre-
existing lexical rules: 10−2, lexical rules with wild-
cards: 10−5, other rules: 10−10. We qualitatively
evaluate the resulting ITG by looking at how it ex-
plains some of the parallel sentences, as opposed to
how the baseline bracketing ITG explains them.

5 Results

We evaluate the results qualitatively, by comparing
the output of our induced system with that of the
baseline BITG. Figures 2 and 4 contain the output of
our system, and Figures 3 and 5 contain the output of
the baseline BITG. All Figures have the same struc-
ture: the English sentence on the top; the Chinese
sentence on the left; a compositional alignment ma-
trix between them where black boxes represent cor-
rect terminals, gray boxes represent incorrect termi-
nals and outlined boxes represent the compositions;
and the corresponding ITG parse tree over the En-
glish sentence.

Figure 2 shows a single noun phrase utterance
(room fifty-six twelve. translating into 五六一二房。
‘five six one two room.’) taken from our training
data, as biparsed by our induced ITG. The tokens
align well, which would not have been possible if
the Chinese segmenter hadn’t grouped一 ‘one’ and
二 ‘two’ together as a single token for twelve to align
with. It also aligns fifty to五 ‘five’, which is techni-
cally wrong, but may be useful when different lan-
guages prefer to group numbers differently. Notice
that the noun phrase needs to separate the number
series from the noun (room) in order to flip the or-

60



ROOT 

CD 

: 

NP’ 

CD 

NP’ 

CD 

NP’ 

NN 

NP 

. 

room fifty - six twelve .

Figure 2: The tree structure imposed on one of the training
sentences by our induced ITG.

der in Chinese. It has also chosen to make the NP′ a
right-heavy tree, when it could very well have made
it left-heavy or balanced. This is desirable, since it
appears to have generalized arbitrarily long chains of
numbers, so what it has learned is that a noun-phrase
fragment can consist of a noun-phrase fragment and
a number. Inspecting the grammar, we find the fol-
lowing relevant rules:

NP′ 6.4%→ [CD NP′] NP′ 4.8%→ ⟨CD NP′⟩
NP′ 1.4%→ [NP′ CD] NP′ 1.2%→ ⟨NP′ CD⟩

There is a clear bias in favor of right-heavy trees
(the first two rules account for about 11.2% of noun-
phrase fragments, as opposed to 2.6% which are left-
heavy), as well as a bias for maintaining the order of
number sequences in noun-phrase fragments (7.8%
as opposed to 6%).

The bracketing ITG does not know what to do in
these circumstances. As Figure 3 shows, the number
sequence is arbitrarily nested. It is possible to force
an ITG to always prefer a canonical left- or right-
heavy sequence of straight (or inverted) rule appli-
cations, but that is an external constraint put on it
manually. We managed to induce that preference au-
tomatically.

The second sentence pair is the question how long
does it take to reach Japan? with its Chinese transla-
tion到日本要花多长时间？ ‘reach Japan need spend
how long time?’. Notice that this is a very hard trans-
lation to recreate for an automatic system. English

room fifty - six twelve .

Figure 3: The tree structure imposed on one of the training
sentences by the baseline bracketing ITG.

mandates a subject in well-formed clauses, Chinese
does not; English requires an auxiliary verb in ques-
tions, Chinese does not; English requires an infini-
tive marker, Chinese does not; English allows time
to be implicit from how long, Chinese requires it to
be explicit.

Figure 4 shows our system, and Figure 5 shows
the BITG. Our system has learned that English has
a mandatory subject, which Chinese lacks, as is ev-
ident from it begin correctly classified as a pronoun,
and translated into nothing, whereas the BITG has
it incorrectly translated as 花 ‘spend’. Our system
has incorrectly aligned does to 多 ‘how’, whereas
the BITG has it incorrectly aligned it to时间 ‘time’.
Our system has correctly classified to as an infini-
tive marker as opposed to a preposition, but aligned
it with到 ‘reach’, whereas the BITG has is correctly
aligned to nothing. It is, however worth noting that
our system has managed to compose to reach Japan
as a unit, whereas the BITG has no concept of an
infinitive verb at all. The treebank we extract our
grammar from has no specific category for infinitive
verb phrases either, but our system has repurposed
the S-7 category (that occurs in the treebank, but not
it’s annotation guidelines) to represent infinitive verb
phrases. Both systems have incorrectly aligned时间
‘time’ to something.

Looking at the nestings, it becomes clear that the
BITG has done a good job of nailing the token-to-
token correspondences, but that the structure is all
wrong. It is a left-heavy tree where one would ex-

61



ROOT 

WRB 

how long does it take to reach Japan ?

RB 

VBZ 

PRP 

WAP 

SQ SBARQ 

VB 

NNP VP 

TO S-7 

VB 

S’ S 

. 

Figure 4: The tree structure imposed on one of the training
sentences by our induced ITG.

pect a mostly right-heavy tree. Our system does a
better job with the nesting, although one would have
wished to see take to reach Japan be dominated by how
long does it, rather than being on the same level. The
spans are also reasonably well labeled.

6 Conclusions

We have presented a novel approach to low-resource
language translation that relies on extending the lex-
ical coverage of a linguistically informed inversion
transduction grammar. The linguistic information is
derived from an output language treebank, and ex-
tending the translation lexicon requires the correct
translation for a word with a specific part-of-speech
tag to be identified. We have shown a way of doing
precisely that through setting aside part of the prob-
ability mass for unknown translations, represented
as wildcards in the ITG. Staying within the formal-
ism of ITGs is desirable, as they can be trained di-
rectly on parallel data, without the need to first in-
duce, and commit to, a single word alignment. The
inductive bias of the ITG formalism allows us to in-
tegrate over all relevant word alignments in poly-
nomial time, while still being able to capture most
of the structural variation observed between human
languages. It also allows us to combine multiple

how long does it take to reach Japan ?

Figure 5: The tree structure imposed on one of the training
sentences by the baseline bracketing ITG.

heterogenous sources of knowledge, in this paper
we used (a) a context-free grammar derived from an
English treebank, (b) a Chinese–English translation
dictionary, and (c) a small Chinese–English parallel
corpus, which is imperative in a low-resource lan-
guage setting, where all available resources need to
be utilized.

Acknowledgements

This material based upon work supported in part by
the Defense Advanced Research Projects Agency
(DARPA) under LORELEI contract HR0011-15-
C-0114, BOLT contracts HR0011-12-C-0014 and
HR0011-12-C-0016, and GALE contracts HR0011-
06-C-0022 and HR0011-06-C-0023; by the Euro-
pean Union under the Horizon 2020 grant agree-
ment 645452 (QT21) and FP7 grant agreement
287658; and by the Hong Kong Research Grants
Council (RGC) research grants GRF16210714,
GRF16214315, GRF620811 and GRF621008. Any
opinions, findings and conclusions or recommen-
dations expressed in this material are those of the
authors and do not necessarily reflect the views of
DARPA, the EU, or RGC.

62



References

Peter F. Brown, Stephen A. Della Pietra, Vincent J.
Della Pietra, and Robert L. Mercer. The mathe-
matics of machine translation: Parameter estima-
tion. Computational Linguistics, 19(2):263–311,
1993.

David Burkett and Dan Klein. Transforming trees to
improve syntactic convergence. In Proceedings
of the 2012 Joint Conference on Empirical Meth-
ods in Natural Language Processing and Compu-
tational Natural Language Learning, pages 863–
872, Jeju Island, Korea, July 2012. Association for
Computational Linguistics.

David Burkett, John Blitzer, and Dan Klein. Joint
parsing and alignment with weakly synchronized
grammars. In Human Language Technologies:
The 2010 Annual Conference of the North Ameri-
can Chapter of the Association for Computational
Linguistics (NAACL HLT 2010), pages 127–135,
Los Angeles, California, June 2010.

Arthur Pentland Dempster, Nan M. Laird, and Don-
ald Bruce Rubin. Maximum likelihood from in-
complete data via the EM algorithm. Journal of
the Royal Statistical Society. Series B (Method-
ological), 39(1):1–38, 1977.

John DeNero and Dan Klein. Tailoring word align-
ments to syntactic machine translation. In Pro-
ceedings of the 45th AnnualMeeting of the Associ-
ation of Computational Linguistics, pages 17–24,
Prague, Czech Republic, June 2007. Association
for Computational Linguistics.

C. S. Fordyce. Overview of the IWSLT 2007 evalua-
tion campaign. In International Workshop on Spo-
ken Language Translation (IWSLT 2007), pages
1–12, 2007.

Michel Galley, Mark Hopkins, Kevin Knight, and
Daniel Marcu. What’s in a translation rule?
In HLT-NAACL 2004: Main Proceedings, pages
273–280, Boston, Massachusetts, May 2004.

Shudong Huang and David Graff. Chinese–English
translation lexicon version 3.0 LDC2002L27,
2002.

Liang Huang, Kevin Knight, and Aravind Joshi. Sta-
tistical syntax-directed translation with extended
domain of locality. In 7th Biennial Conference

Association for Machine Translation in the Amer-
icas (AMTA 2006), pages 66–73, Boston, Mas-
sachusetts, 2006.

Kenichi Kurihara and Taisuke Sato. Variational
bayesian grammar induction for natural language.
In Proceedings of the 8th International Confer-
ence on Grammatical Inference: Algorithms and
Applications, ICGI’06, pages 84–96, Berlin, Hei-
delberg, 2006. Springer-Verlag.

Karim Lari and Steve J. Young. The estimation of
stochastic context-free grammars using the inside-
outside algorithm. Computer Speech&Language,
4(1):35–56, 1990.

Mitchell P. Marcus, Mary Ann Marcinkiewicz, and
Beatrice Santorini. Building a large annotated cor-
pus of english: The penn treebank. Computational
Linguistics, 19(2):313–330, June 1993.

Haitao Mi and Liang Huang. Forest-based trans-
lation rule extraction. In 2008 Conference on
Empirical Methods in Natural Language Process-
ing (EMNLP 2008), pages 206–214, Honolulu,
Hawaii, October 2008.

Haitao Mi, Liang Huang, and Qun Liu. Forest-based
translation. In 46th Annual Meeting of the Associ-
ation for Computational Linguistics: Human Lan-
guage Technologies (ACL-08: HLT), pages 192–
199, Columbus, Ohio, June 2008.

Jason Riesa and Daniel Marcu. Hierarchical search
for word alignment. In 48th Annual Meeting of the
Association for Computational Linguistics (ACL
2010), pages 157–166, Uppsala, Sweden, July
2010.

Markus Saers and Dekai Wu. Bayesian induction
of bracketing inversion transduction grammars.
In Sixth International Joint Conference on Nat-
ural Language Processing (IJCNLP2013), pages
1158–1166, Nagoya, Japan, October 2013. Asian
Federation of Natural Language Processing.

Markus Saers, Joakim Nivre, and Dekai Wu. Learn-
ing stochastic bracketing inversion transduction
grammars with a cubic time biparsing algorithm.
In 11th International Conference on Parsing Tech-
nologies (IWPT’09), pages 29–32, Paris, France,
October 2009.

Anders Søgaard and Dekai Wu. Empirical lower
bounds on translation unit error rate for the full

63



class of inversion transduction grammars. In Pro-
ceedings of the 11th International Conference on
Parsing Technologies (IWPT’09), pages 33–36,
Paris, France, October 2009.

Dekai Wu and Hongsing Wong. Machine translation
with a stochastic grammatical channel. In 36th
Annual Meeting of the Association for Compu-
tational Linguistics and 17th International Con-
ference on Computational Linguistics (COLING-
ACL ’98), volume 2, pages 1408–1415, Montreal,
Quebec, August 1998.

Dekai Wu. Stochastic inversion transduction gram-
mars and bilingual parsing of parallel corpora.
Computational Linguistics, 23(3):377–403, 1997.

Zhibiao Wu. LDC Chinese segmenter, 1999.
Kenji Yamada and Kevin Knight. A syntax-based

statistical translation model. In 39th Annual Meet-
ing of the Association for Computational Linguis-
tics and 10th Conference of the European Chapter
of the Association for Computational Linguistics,
pages 523–530, Toulouse, France, July 2001.

Hao Zhang, Chris Quirk, Robert C. Moore, and
Daniel Gildea. Bayesian learning of non-
compositional phrases with synchronous pars-
ing. In 46th Annual Meeting of the Associa-
tion for Computational Linguistics: Human Lan-
guage Technologies (ACL-08: HLT), pages 97–
105, Columbus, Ohio, June 2008.

64


