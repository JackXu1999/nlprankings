




































Cross-lingual Multi-Level Adversarial Transfer to Enhance Low-Resource Name Tagging


Proceedings of NAACL-HLT 2019, pages 3823–3833
Minneapolis, Minnesota, June 2 - June 7, 2019. c©2019 Association for Computational Linguistics

3823

Cross-lingual Multi-Level Adversarial Transfer to Enhance
Low-Resource Name Tagging

Lifu Huang1, Heng Ji1, Jonathan May2
1 Computer Science Department, Rensselaer Polytechnic Institute

{huangl7,jih}@rpi.edu
2 Information Sciences Institute, University of Southern California

jonmay@isi.edu

Abstract

We focus on improving name tagging for low-
resource languages using annotations from re-
lated languages. Previous studies either di-
rectly project annotations from a source lan-
guage to a target language using cross-lingual
representations or use a shared encoder in
a multitask network to transfer knowledge.
These approaches inevitably introduce noise
to the target language annotation due to mis-
matched source-target sentence structures. To
effectively transfer the resources, we develop
a new neural architecture that leverages multi-
level adversarial transfer: (1) word-level ad-
versarial training, which projects source lan-
guage words into the same semantic space as
those of the target language without using any
parallel corpora or bilingual gazetteers, and
(2) sentence-level adversarial training, which
yields language-agnostic sequential features.
Our neural architecture outperforms previous
approaches on CoNLL data sets. Moreover,
on 10 low-resource languages, our approach
achieves up to 16% absolute F-score gain over
all high-performing baselines on cross-lingual
transfer without using any target-language re-
sources.1

1 Introduction

Low-resource language name tagging is an impor-
tant but challenging task. An effective solution is
to perform cross-lingual transfer, by leveraging the
annotations from high-resource languages. Most
of these efforts achieve cross-lingual annotation
projection based on bilingual parallel corpora com-
bining with automatic word alignment (Yarowsky
et al., 2001; Wang et al., 2013; Fang and Cohn,
2016; Ehrmann et al., 2011; Ni et al., 2017),
bilingual gazetteers (Feng et al., 2017; Zirikly

1Our programs will be released at https://github.
com/wilburOne/AdversarialNameTagger

and Hagiwara, 2015), cross-lingual word embed-
ding (Fang and Cohn, 2017; Wang et al., 2017;
Huang et al., 2018), or cross-lingual Wikifica-
tion (Kim et al., 2012; Nothman et al., 2013; Tsai
et al., 2016; Pan et al., 2017), but these resources
are still only available for dozens of languages.
Recent efforts on multi-task learning model each
language as one single task while all the tasks
share the same encoding layer (Yang et al., 2016,
2017; Lin et al., 2018). These methods can transfer
knowledge via the shared encoder without using
bilingual resources. However, different languages
usually have different underlying sequence struc-
tures, as shown in Figure 1. Without an explicit
constraint, the encoder is not guaranteed to extract
language-independent sequential features. More-
over, when the size of annotated resources is not
balanced, the encoder is likely to be biased toward
the resource-dominant language.

NED:

ENG:

ESP:

The European Union5' s competition policy3 has been of
central importance4 since European integration2 began1.

La política de competencia3 de la Unión Europea5 ha sido de
central importancia4 desde que se inició1 la integración europea2.

Sedert het begin1 van de Europese integratie2 is het
mededingingsbeleid3 van groot belang4 voor de Europese Unie5. 

Figure 1: Example of parallel sentences between En-
glish (ENG), Spanish (ESP) andDutch (NED) fromEu-
roparl Parallel Corpus (Koehn, 2005). The information
units with the same color and superscript are aligned.

Considering these challenges, we develop a new
neural architecture which can effectively transfer
resources from source languages to improve target
language name tagging. Our neural architecture
is built upon a state-of-the-art sequence tagger:
bi-directional long short-term memory as input to
conditional random fields (Bi-LSTM-CRF) (Lam-
ple et al., 2016; Huang et al., 2015; Ma and



3824

...

...

Target Language

Source Language

...

Linear Projection

Word Discriminator Sequence Feature
Encoder

Context
Encoder 

CRF Name
Tagger 

Sequence
Discriminator

C
onvolutional

N
eural N

etw
orks

B­PER 

I­PER 

O 

... 

O 

B­GPE 

Figure 2: Architecture overview.

Hovy, 2016), integrated with multi-level adver-
sarial transfer: (1) word level adversarial trans-
fer, similar to Conneau et al. (2017), applying a
projection function on the source language and a
discriminator to distinguish each word of the tar-
get language from that of the source language, re-
sulting in a bilingual shared semantic space; (2)
sentence-level adversarial transfer, where a dis-
criminator is trained to distinguish each sentence
of the target language from that of the source lan-
guage,2 and a sequence encoder is applied to each
sentence of both languages to prevent the dis-
criminator from correctly predicting the source of
each sentence, yielding language-agnostic sequen-
tial features. These features can better facilitate the
resource transfer from the source language to the
target language.
Our contributions are twofold: (1) with-

out requiring any parallel corpora or bilingual
gazetteers, the multi-level adversarial approach
can efficiently transfer annotated resources from
the source language to the target language and im-
prove target language name tagging; (2) In ad-
dition to outperforming previous high-performing
baselines on CoNLL data sets, we also evaluate
cross-lingual name tagging on 10 low-resource
languages and achieve up to 16% absolute F-score
gain over all baselines when there is no annotated
resource for the target language.

2 Approach

2.1 Approach Overview
Figure 2 shows the overview of our neural archi-
tecture. It consists of three components:

2For the name tagging task, ‘sequence’ always means
‘sentence.’

Cross-lingual word embedding learning with
adversarial training: Given pre-trained mono-
lingual word embeddings for a target language t
and a source language s, we first apply a map-
ping function to each word representation from s,
then feed both the projected source word repre-
sentations and the target word representations to a
word discriminator to predict the language of each
word. If the discriminator cannot distinguish the
language of t from the projection of s, then we
consider t and the projection of s to be in a shared
space.

Language-agnostic sequential feature extrac-
tion: For each sentence of t and s, we ap-
ply a sequence encoder to extract sequential
features, and a Convolutional Neural Network
(CNN) (Krizhevsky et al., 2012) based sequence
discriminator to predict the language source of
each sentence. The sequence encoder is trained to
prevent the sequence discriminator from correctly
predicting the language of each sentence, such that
it finally extracts language-agnostic sequential fea-
tures.

Language-independent name tagger The
language-agnostic sequential features from both
t and s are further fed into a context encoder to
better capture and refine contextual information
and a conditional random field (CRF) (Lafferty
et al., 2001) based name tagger.
Next we show the details of each component in

our architecture.

2.2 Word-level Adversarial Transfer

To better leverage the resources from the source
language, our first step is to construct a shared se-



3825

mantic space where the words from the source and
target languages are semantically aligned. With-
out requiring any bilingual gazetteers, recent ef-
forts (Zhang et al., 2017b; Conneau et al., 2017;
Chen and Cardie, 2018) explore unsupervised ap-
proaches to learn cross-lingual word embeddings
and achieve comparable performance to super-
vised methods. Following these studies, we per-
form word-level adversarial training to automati-
cally align word representations from s and t.
Formally, assume we are given pre-

trained monolingual word embeddings
Vt = {vt1, vt2, ..., vtN} ∈ RN×dt for t, and
Vs = {vs1, vs2, ..., vsM} ∈ RM×ds for s, where
vti and vsj are the vector representations of words
wti and wsi from t and s, N and M denote the
vocabulary sizes, dt and ds denote the embedding
dimensionality of t and s respectively. We then
apply a mapping function f to project s into the
same semantic space as t:

Ṽs = f(Vs) = VsU (1)

where U ∈ Rds×dt is the transformation matrix.
Ṽs ∈ RM×dt are the projected word embeddings
for s, andΘf = {θf} denotes the set of parameters
to be optimized for f .
Similar to Xing et al. (2015), Conneau et al.

(2017), and Chen and Cardie (2018), we constrain
the transformation matrix U to be orthogonal with
singular value decomposition (SVD) to reduce the
parameter search space:

U = AB⊤ ,with AΣB⊤ = SVD(ṼsV⊤s ) (2)

To automatically optimize the mapping function
f without using extra bilingual signals, we intro-
duce amulti-layer perceptronD as a word discrim-
inator, which takes word embeddings of t and pro-
jected word embeddings of s as input features and
outputs a single scalar. D(w∗i ) represents the prob-
ability of w∗i coming from t. The word discrim-
inator is trained by minimizing the binary cross-
entropy loss:

Lwdis =−
1

It;s
·
It;s∑
i=0

(
yi · log(D(w∗i ))

+ (1− yi) · log(1−D(w∗i ))
)
,

yi =δi(1− 2ϵ) + ϵ ,

where δi = 1 when w∗i is from t and δi = 0 oth-
erwise. It;s represents the number of words sam-
pled from the vocabulary of t and s together. ϵ is a
smoothed value added to the positive and negative
labels. Θdis = {θD} is the parameter set.
The mapping function f and word discrimina-

tor D are two adversarial players, thus we flip the
word labels and optimize f by minimizing the fol-
lowing loss:

Lwf =−
1

It;s
·
It;s∑
i=0

(
(1− yi) · log(D(w∗i ))

+ yi · log(1−D(w∗i ))
)
,

yi =δi(1− 2ϵ) + ϵ

Following the standard training procedures of
deep adversarial networks (Goodfellow et al.,
2014), we train the word discriminator and the
mapping function successivelywith stochastic gra-
dient descent (SGD) (Bottou, 2010) to minimize
Lwdis and L

w
f . Similar to Conneau et al. (2017), af-

ter word-level adversarial training, we also adopt a
refinement step to construct a bilingual dictionary
for the top-kmost frequent words in the source lan-
guage3 based on Ṽs and Vt, and further optimize
U with Equation 2 in a supervised way.

2.3 Sentence-level Adversarial Transfer
Once s is projected into the same semantic space
as t, we can regard both sentences as coming from
one unified language and directly project annota-
tions from s to t. However, name tagging not only
relies on word level features, but also on sequen-
tial contextual features for entity type classifica-
tion. Without constraints, the sequence encoder
can only extract sequential features for both t and
s based on their final training signals while these
features are not necessarily beneficial to the target
language. Thus, we further design sentence level
adversarial transfer to encourage the encoder to ex-
tract language-agnostic sequential features.
Given a sentence xt = {wt1, wt2, ...} from t and

a sentence xs = {ws1, ws2, ...} from s, we first
use Vt and Ṽs to initialize a vector representation
for each wti and wsi . We also apply a character-
based CNN (denoted as CharCNN) (Kim et al.,
2016) for each language to compose a word rep-
resentation from its characters. For each word, we

3We set k=15,000 in our experiment.



3826

concatenate its word representation and character
based representation. Then we feed the sequence
of vector representations into a weight sharing Bi-
LSTM encoder E to obtain sequential features
Ht = {ht1,ht2, ...} and Hs = {hs1,hs2, ...} for
xt and xs respectively. The parameter set of op-
timizing both language-dependent CharCNN and
the sequence encoder can be denoted as Θe =
{θCharCNNt , θCharCNNs , θE}.
Based on these sequential features, we use a

sequence discriminator to predict the language
source of each sentence. Given a sentence x∗ and
its sequential features H = {h∗1,h∗2, ...} from E,
we first apply a language-independent CNN with
max-pooling to get an overall vector representa-
tion for x∗, then feed it into another multi-layer
perceptron, D̃, to predict the probability that x∗
comes from language t. The sequence discrimina-
tor is trained by minimizing the following binary
cross-entropy loss:

Lxdis =−
1

Ĩt;s
·
Ĩt;s∑
i=0

(
ỹi · log(D̃(x∗i ))

+ (1− ỹi) · log(1− D̃(x∗i ))
)
,

ỹi =δ̃i(1− 2η) + η ,

where δ̃i = 1 if the sentence x∗i is from t and
δ̃i = 0 otherwise. Ĩt;s represents the number of
sentences sampled from the whole data set of t and
s. η is another smoothed value for sequence labels.
Θ

d̃is
= {θCNN, θD̃} denotes the parameter set for

optimizing the sequence discriminator.
The sequence encoder E and the sequence dis-

criminator D̃ are two adversarial players and E is
optimized by trying to fool D̃ to correctly predict
the language source of each sentence. Thus we flip
the sequence labels and optimizeE by minimizing
the following loss:

Lxe =−
1

Ĩt;s
·
Ĩt;s∑
i=0

(
(1− ỹi) · log(D̃(x∗i ))

+ ỹi · log(1− D̃(x∗i ))
)
,

ỹi =δ̃i(1− 2η) + η

2.4 Name Tagger Training
With the language-agnostic sequential features
from E, we can directly combine all annotated

Algorithm 1Multi-level Adversarial Training for
Improving Target Language Name Tagging
Input: Monolingual pre-trained word embeddingsVt for tar-
get language t, and Vs for source language s. Annotated sen-
tence set∆t for t and∆s for related language s.

1. for iter = 1 to word_epoch do
2. for a = 1 to word_dis_steps do
3. sample a batch of words bt ∼ Vt, bs ∼ Vs
4. loss = Lwdis([bt, f(bs)])
5. update Θdis to minimize loss
6. sample a batch of words b

′
t ∼ Vt, b

′
s ∼ Vs

7. loss
′
= Lwf ([b

′
t, f(b

′
s)])

8. update Θf to minimize loss
′

9. build a parallel dictionary with Vt and f(Vs) and refine
projected word embeddings Ṽs = f(Vs)

10. for iter = 1 to seq_epoch do
11. sample a batch of sentences b̃t ∼ ∆t, b̃s ∼ ∆s
12. extract sequential features from b̃t, b̃s with E
13. loss = Lxdis([E(b̃t), E(b̃s)])
14. update Θe, Θd̃is to minimize loss
15. for g = 1 to seq_tagger_steps do
16. sample a batch of sequences b̃

′
t ∼ ∆t, b̃

′
s ∼ ∆s

17. loss
′
= Lxe ([E(b̃

′
t), E(b̃

′
s)]) + Lcrf ([b̃

′
t, b̃

′
s])

18. update Θe, Θc to minimize loss
′

training data from both t and s to train the name
tagger for t. To do so, we feed the sequential
features from E to another Bi-LSTM encoder Ec
to refine the context information for each token,
and use a CRF output layer to render predictions
for each token, which can effectively capture de-
pendencies among name tags (e.g., an “inside-
organization” token cannot follow a “beginning-
person” token).
Specifically, given an input sentence x =

{w1, w2, ...wn}, we extract language-agnostic se-
quential features with E, and further obtain
a new sequence of contextual features H̃ =
{h̃1, h̃2, ..., h̃n} with Ec. Then we a apply a linear
layer ℓ to further convert each h̃i to a score vec-
tor yi, in which each dimension denotes the pre-
dicted score for a tag (the starting, inside or out-
side of a name mention with a pre-defined entity
type). Then we feed the sequence of score vec-
tors Y = {y1, y2, ..., yn} into the CRF layer. The
score of a sequence of tags Z = {z1, z2, ..., zn} is
defined as:

Score(x,Y,Z) =
n∑

i=1

(Rzi−1,zi + Yi,zi)

whereR is a transition matrix andRp,q denotes the
binary score of transitioning from tag p to tag q.



3827

Yi,z represents the unary score of assigning tag z
to the i-th word.
Given the annotated sequence of tags Z, the

CRF loss is:

Lcrf = log
∑
Z′∈Z̃

eScore(x,Y,Z
′
) − Score(x,Y,Z)

where Z̃ is the set of all possible tagging paths. The
parameter set for optimizing the name tagger can
be denoted as Θc = {θEc , θℓ, θCRF}.
We jointly optimize the sequence encoderE, the

context encoder Ec and the CRF together by mini-
mizing the loss L′ = Lxe +Lcrf , and successively
minimize Lxdis and L

′ with SGD. The end-to-end
training for our neural architecture is described in
Algorithm 1.

3 Experiment

3.1 Data and Experimental Setup
We evaluate our methods from multiple settings.
We first evaluate our architecture on 10 low-
resource languages from the DARPA LORELEI
project. The annotations are released by the Lin-
guistic Data Consortium (LDC).4 Each dataset has
four predefined name types: person (PER), orga-
nization (ORG), location (LOC) and geo-political
entity (GPE). For each target low-resource lan-
guage, we choose a source language if they are
from the same language family or use the same
script. To show the impact of resource transfer be-
tween distinct languages, we also use English as a
source language for each target low-resource lan-
guage. We create the English annotated resource
by combining the TAC-KBP 2015 English Entity
Discovery and Linking (Ji et al., 2015) data set and
the Automatic Content Extraction (ACE2005) data
set.5 To avoid the impact of parameter initializa-
tion, we perform 5-fold cross validation. For each
experiment, we run twice and get the averaged F-
score. Table 1 shows the statistics of each data set.
We also evaluate our approach on high-resource

languages. We use Dutch (nl) and Spanish (es)
data sets from the CoNLL 2002 (Tjong Kim Sang,
2002) shared task as target languages, and use
English (en) data from the CoNLL 2003 (Tjong
Kim Sang and De Meulder, 2003) shared task as

4The annotations are from: am (LDC2016E87), ti
(LDC2017E39), ar (LDC2016E89), fa (LDC2016E93), om
(LDC2017E27), so (LDC2016E91), sw (LDC2017E64), yo
(LDC2016E105), ug (LDC2016E70), uz (LDC2016E29)

5The data sets are LDC2015E103 and LDC2006T06

Language # of Sents # of Tokens # of Names
Amharic (am) 4,770 71,399 3,891
Tigrinya (ti) 5,023 95,364 6,201
Arabic (ar) 4,781 80,715 4,937
Farsi (fa) 3,855 72,629 3,966
Oromo (om) 2,987 52,876 4,985
Somali (so) 3,453 78,400 5,571
Swahili (sw) 4,155 96,902 6,044
Yoruba (yo) 1,599 46,084 2,016
Uyghur (ug) 3,961 60,999 2,575
Uzbek (uz) 11,135 177,816 10,937

English (en) 17,936 388,120 23,938

Table 1: Data set statistics for each low-resource lan-
guage.

the source language. All the data sets have four
pre-defined name types: PER, ORG, LOC and
miscellaneous (MISC). Table 2 shows the statistics
of these data sets.
For fair comparison, we use the same pre-

trained word embeddings of English, Dutch and
Spanish as Lin et al. (2018), while for each low-
resource language we train their word embed-
dings using the documents from their LDC pack-
ages with FastText.6 Table 3 lists the key hyper-
parameters we used in our experiments.

3.2 Baselines
We compare our methods with three categories of
baseline methods:7

• Monolingual Name Tagging Using monolin-
gual annotations only, the current state-of-the-
art name tagging model is the Bi-LSTM-CRF
network (Huang et al., 2015; Lample et al., 2016;
Ma and Hovy, 2016).8

• Multi-task Learning Lin et al. (2018) apply
multi-task learning to boost name tagging per-
formance by introducing additional annotations
from source languages using a weight sharing
context encoder across multiple languages.

• Language Universal Representations We ap-
ply word adversarial transfer only to project the
source language into the same semantic space as
the target language, then train the name tagger on
the annotations of source and target languages.
Word-Adv1 refers to the approach which is di-
rectly trained on the combination of the anno-
6https://fasttext.cc/
7All the baselines are trained for 100 epochs
8For eachword, we also combine its word embeddingwith

a CharCNN based representation.



3828

Language Resource Train Dev Test
English (en) source language 204,567 (23,499) 51,578 (5,942) 46,666 (5,648)
Dutch (nl) target language 202,931 (13,344) 37,761 (2,616) 68,994 (3,941)
Spanish (es) target language 264,715 (18,797) 52,923 (4,351) 51,533 (3,558)

Table 2: CoNLL data set statistics: # of tokens and # of names (between parentheses).

Parameter Name Value
Monolingual Embedding Size 100
CharCNN Filter Size 25
CharCNN Filter Widths [2, 3]
LSTM Hidden Size 100
Droupout Rate 0.5
Smoothing Value ϵ for Word Discriminator 0.1
Word Adversarial Training Epochs 5
Smoothing Value η for Sequence Discriminator 0.3
Sequence Adversarial & Name Tagging Train-
ing Epochs

60

# of Steps for Sequence Tagging Training 5
Batch Size 20
Initial Learning Rate 0.01
Optimizer SGD

Table 3: Hyper-parameters.

tations, while Word-Adv2 refers to the baseline
that is first trained on the target language anno-
tations and then further tuned on the related lan-
guage annotations.

3.3 Cross-lingual Transfer with Zero Target
Language Annotated Resource

We first evaluate our approach on a cross-lingual
transfer setting without using any annotated train-
ing data from the target language. We con-
duct experiments on 8 low-resource languages.
Among those, some pairs, such as Amharic (am)
and Tigrinya (ti), Oromo (om) and Somali (so),
or Yoruba (yo) and Swahili (sw), are from the
same language family and are closely related,
while some are not, such as Arabic (ar) and Farsi
(fa). Since our approach requires some unlabeled
sentences from the target language to train the
sentence-level discriminator, we entirely remove
the annotations from the annotated data set of the
target language. Table 4 presents the results.
Our approach significantly outperforms the pre-

vious methods on all languages. Specifically,
compared with the Word-Adv1 baseline, which
only performs word-level adversarial transfer, our
approach achieves 10% absolute F-score gain on
average, which demonstrates the effectiveness of
the sentence-level adversarial transfer. In addition,
compared with Lin et al. (2018), who only apply a
shared context-encoder to transfer the knowledge,
our approach not only includes a language-sharing

target Cross-lingual Multitask Our
(source) Word-Adv1 Learning Approach
am (ti) 15.19 19.72 26.86
ti (am) 16.20 9.06 29.36
ar (fa) 1.53 3.52 13.83
fa (ar) 2.59 0.91 11.14
om (so) 4.66 3.40 14.14
so (om) 4.12 2.98 20.02
sw (yo) 7.20 5.60 18.25
yo (sw) 13.07 6.14 23.73

Table 4: Cross-lingual transfer when the target lan-
guage has no resources (F-score %).

encoder, but also performs multi-level adversar-
ial training to encourage the semantic alignment
of words from both languages and a sequence en-
coder to extract language-agnostic sequential fea-
tures.
Here we use some Arabic (Farsi) examples to

further show the effectiveness of each level of
adversarial training in our architecture. Without
using any annotated training data from Arabic,
both our approach and the Word-Adv1 baseline
successfully identify الفرنسية (French) as a GPE
from the Arabic (ar) sentence in Figure 3, since
with word-level adversarial training, the seman-
tics of الفرنسية is well aligned with theGPE names
in Farsi annotated data, such as فرانسه (France),
روسیه (Russia) and آلمان (Germany). However,
both the Word-Adv1 and Lin et al. (2018) base-
lines fail to identify الجزائرية (Algerian) as a GPE
since its top ranked similar words in Farsi in-
clude مذاکرات (negotiations), دوحه (Doha) and
توافقنامه (agreement). With sentence-level adver-
sarial training, our approach successfully captures
language-agnostic sequential features, such as او“
(or) usually connects two names with the same
type”, thus our approach successfully identifies
الجزائرية (Algerian) as a GPE name.

3.4 Cross-lingual Transfer for Low-Resource
Languages

We also investigate the impact of cross-lingual
transfer when the target languages have some an-
notated resources. For each target low-resource
language, we explore the use of a related low-
resource language vs. using the high-resource En-



3829

target Monolingual Cross-lingual Embedding Multitask Our Approach
(related) Bi-LSTM-CRF Word-Adv1 Word-Adv2 Learning Multi-Adversarial
am (ti) 72.23 72.15 72.01 72.35 73.98
ti (am) 74.68 74.43 74.83 74.71 74.93
ar (fa) 48.92 48.37 47.90 47.53 49.76
fa (ar) 64.35 63.93 64.43 63.21 65.09
om (so) 76.37 76.43 76.19 76.18 77.19
so (om) 77.63 77.31 77.13 77.99 78.15
sw (yo) 77.01 77.31 77.85 77.86 76.28
yo (sw) 68.97 68.89 69.62 70.12 70.59
ug (uz) 68.73 68.53 68.29 68.39 69.46
uz (ug) 74.59 74.21 74.74 74.56 75.37

am (en) 72.23 72.43 71.63 72.22 73.35
ti (en) 74.68 74.61 74.69 74.68 74.80
ar (en) 48.92 48.50 47.91 47.40 50.08
fa (en) 64.35 64.04 64.25 63.44 63.92
om (en) 76..27 76.68 76.53 76.2 77.29
so (en) 77.63 76.67 77.88 77.88 78.21
sw (en) 77.01 77.52 76.84 77.89 77.01
yo (en) 68.97 69.21 69.46 70.43 70.88
ug (en) 68.73 68.14 68.79 68.69 69.06
uz (en) 74.59 73.95 74.46 74.48 74.75

Table 5: Cross-lingual transfer when the target language has resources (F-score %).

AR: ویكون نائب المدعي العام قد اعتبر ان االدلة ضد الموقوفین الذین
. یحملون الجنسیة الفرنسیة او الجزائریة في غالبیتھم ، كافیة

EN: The deputy prosecutor has ruled that the
evidence against those with French or Algerian
nationality is mostly sufficient.

AR: ویكون نائب المدعي العام قد اعتبر ان االدلة ضد الموقوفین الذین

. یحملون الجنسیة 3الفرنسیة 2او 1الجزائریة في غالبیتھم ، كافیة
EN: The deputy prosecutor has ruled that the

evidence against those with French3 or2 Algerian1

nationality is mostly sufficient.

Figure 3: Example of an Arabic (ar) name tagging out-
put with Farsi (fa) annotated training data only.

glish as our source language. Table 5 shows the
performance on 10 low-resource languages.
Comparing cross-lingual embedding based

baselines to the monolingual baseline, we observe
that for most low-resource languages, directly
adding the annotations from the source language
to the target language slightly hurts the model.
This suggests that when the training data for the
target language is not enough, the model will be
very sensitive to noise. The multitask learning
based baseline (Lin et al., 2018) performs better
than the monolingual baseline only when the
target and source languages are very close, such
as Amharic (am) and Tigrinya (ti), or Swahili (sw)
and Yoruba (yo).
By introducing annotated training data from En-

glish, the performance of all the baselines becomes
worse than the monolingual baseline. Since the
script and sequence structure of English is very
different from these low-resource languages, the
addition of English to the limited target language
training data yields a considerably noisy corpus.

However, by forcing the sequence encoder to ex-
tract language-agnostic features, our approach still
achieves better performance than the monolingual
baseline for most languages. All of these exper-
iments demonstrate that our approach is more ef-
fective in leveraging annotations from other lan-
guages to improve target language name tagging.

3.5 Cross-lingual Transfer for High Resource
Languages

Language Model F-score

Dutch

Lample et al. (2016) 81.74
Yang et al. (2017) 85.19
Lin et al. (2018) 85.71
Gillick et al. (2016) 82.84
Word-Adv1 85.87
Word-Adv2 86.43
Our Model (Bi-LSTM) 86.87

Spanish

Lample et al. (2016) 85.75
Yang et al. (2017) 85.77
Lin et al. (2018) 85.02
Gillick et al. (2016) 82.95
Word-Adv1 85.92
Word-Adv2 85.84
Our Model (Bi-LSTM) 86.41

Table 6: Comparison on cross-lingual transfer for
Dutch and Spanish with various baselines: monolin-
gual baseline (Lample et al. (2016)), multitask base-
lines (Yang et al. (2017) and Lin et al. (2018)), language
universal representation baselines (Gillick et al. (2016),
Word-Adv1, Word-Adv2).

We finally investigate the results when both the
source and target languages are all high-resource



3830

languages. Table 6 presents the performance on
Dutch and Spanish while using English as the
source language. Our approach significantly out-
performs all the other approaches even when the
size of the annotated training data for the target
language is huge. We notice that our approach
achieves larger improvement on Dutch than Span-
ish. The reason may be that, compared with
Spanish, Dutch is much closer to English (Cutler
and Pasveer, 2006). Both English and Dutch are
from the sameWest Germanic branch of the Indo-
European language family while Spanish is from
the Italic branch.

3.6 Impact of Annotation Size from Source
and Target Languages

We use Amharic as the target language and
Tigrinya as the source language to show the im-
pact of the size of their annotations. Specifically,
to explore the impact of the size of target language
annotations, we use 0, 10%, 50%, or 100% an-
notated training data from Amharic. Similarly, to
show the effect of the size of source language an-
notations, for each experiment, we also gradually
add 0, 20%, 50%, or 100% annotated training data
from Tigrinya. For all experiments, we use the
same dev and test set of Amharic. As Figure 4
shows, as we gradually add annotations from the
source or target language, the performance can al-
ways be improved. When the size of target lan-
guage annotations is small, such as 400 sentences,
we can achieve 5%-30% F-score gain by adding
about 4,000 sentences from the source language.
When the size of target language annotations is
over 2,000 sentences, the improvement is about
2% if we add in about 4,000 sentences from source
language annotations.

0 0.2 0.4 0.6 0.8 1
Sample Rate of Source Training Data

0

10

20

30

40

50

60

70

80

F-
sc

or
e

0 target training data
10% target training data
50% target training data
100% target training data

Figure 4: The impact of the size of annotations from
source and target languages on Amharic name tagging.

4 Related Work

Name taggingmethods based on sequence labeling
have been widely studied in recent years. Huang
et al. (2015) and Lample et al. (2016) propose
an effective Bi-LSTM-CRF architecture; the Bi-
LSTM encodes previous and following contexts,
and the CRF is used for tag prediction. Other
studies incorporate a character-level CNN (Ma
and Hovy, 2016), global contexts (Zhang et al.,
2018), or language models (Liu et al., 2018;
Peters et al., 2017, 2018; Devlin et al., 2018)
to improve name tagging. In addition, sev-
eral approaches (Zhang et al., 2016a, 2017a; Al-
Badrashiny et al., 2017) attempt to incorporate
hand-crafted linguistic features into a Bi-LSTM-
CRF to improve low-resource name tagging per-
formance.
Recent attempts on cross-lingual transfer for

name tagging can be divided into two cate-
gories: the first projects annotations from a source
language to a target language via parallel cor-
pora (Yarowsky et al., 2001; Wang and Manning,
2013; Wang et al., 2013; Zhang et al., 2016b;
Fang and Cohn, 2016; Ehrmann et al., 2011; En-
ghoff et al., 2018; Ni et al., 2017), a bilingual
gazetteer (Feng et al., 2017; Zirikly and Hagiwara,
2015), Wikipedia anchor links (Kim et al., 2012;
Nothman et al., 2013; Tsai et al., 2016; Pan et al.,
2017), and language universal representations, in-
cluding Unicode bytes (Gillick et al., 2016) and
cross-lingual word embeddings (Fang and Cohn,
2017; Wang et al., 2017; Huang et al., 2018; Xie
et al., 2018). The second is based on multitask
learning via a weight sharing encoder (Yang et al.,
2016, 2017; Lin et al., 2018). Compared to these
studies, our approach not only automatically learns
cross-lingual word embeddings without requir-
ing any parallel resources, but also carefully ex-
tracts language-agnostic sequential features, yield-
ing better performance.
Adversarial training has also been extensively

studied and applied for cross-lingual and cross-
domain transfer. Several studies (Barone, 2016;
Zhang et al., 2017c,b; Conneau et al., 2017; Chen
and Cardie, 2018) explore adversarial training
to automatically induce bilingual and multilin-
gual word representations without using any par-
allel corpora or bilingual gazetteers. Adversar-
ial training is also applied to extract language-
agnostic (Chen et al., 2016; Zou et al., 2018; Wang
and Pan, 2018; Kim et al., 2017a; Muis et al.,



3831

2018; Cao et al., 2018) and domain-agnostic fea-
tures (Kim et al., 2017b; Ganin et al., 2016; Tzeng
et al., 2017; Chen et al., 2017; Li et al., 2017;
Fu et al., 2017; Bousmalis et al., 2016; Shi et al.,
2018) for cross-lingual and cross-domain adapta-
tion. Compared with these methods, our approach
combines both word-level and sentence-level ad-
versarial training.

5 Conclusions and Future Work

We design a new neural architecture which inte-
grates multi-level adversarial transfer into a Bi-
LSTM-CRF to improve low-resource name tag-
ging. With word-level adversarial training, it can
automatically project the source language into a
shared semantic space with the target language
without requiring any comparable data or bilin-
gual gazetteers. Moreover, considering the differ-
ent underlying sequential structures among vari-
ous languages, we further design a sentence-level
adversarial transfer to encourage the sequence en-
coder to extract language-agnostic features. The
experiments show that our approach achieves the
state-of-the-art on both CoNLL data sets and 10
low-resource languages. In the future, we will fur-
ther explore selecting the feature-consistent anno-
tations from the source language and add to the tar-
get language, and explore unsupervised pretrained
cross-lingual language models (Peters et al., 2018;
Radford et al., 2018; Devlin et al., 2018; Lample
and Conneau, 2019) for cross-lingual low resource
name tagging.

Acknowledgments

This research is based upon work supported in part
by U.S. DARPA LORELEI Program # HR0011-
15-C-0115, and the Office of the Director of
National Intelligence (ODNI), Intelligence Ad-
vanced Research Projects Activity (IARPA), via
contract # FA8650-17-C-9116, and ARL NS-CTA
No. W911NF-09-2-0053. The views and conclu-
sions contained herein are those of the authors and
should not be interpreted as necessarily represent-
ing the official policies, either expressed or im-
plied, of DARPA, ODNI, IARPA, or the U.S. Gov-
ernment. The U.S. Government is authorized to
reproduce and distribute reprints for governmen-
tal purposes notwithstanding any copyright anno-
tation therein.

References
Mohamed Al-Badrashiny, Jason Bolton, Arun Tejasvi
Chaganty, Kevin Clark, Craig Harman, Lifu Huang,
Matthew Lamm, Jinhao Lei, Di Lu, Xiaoman Pan,
et al. 2017. Tinkerbell: Cross-lingual cold-start
knowledge base construction. In Proceedings of
TAC 2017.

Antonio Valerio Miceli Barone. 2016. Towards cross-
lingual distributed representations without parallel
text trained with adversarial autoencoders. Proceed-
ings of ACL 2016.

Léon Bottou. 2010. Large-scale machine learning
with stochastic gradient descent. In Proceedings of
COMPSTAT 2010.

Konstantinos Bousmalis, George Trigeorgis, Nathan
Silberman, Dilip Krishnan, and Dumitru Erhan.
2016. Domain separation networks. In Proceedings
of NIPS 2016.

Pengfei Cao, Yubo Chen, Kang Liu, Jun Zhao, and
Shengping Liu. 2018. Adversarial transfer learn-
ing for chinese named entity recognition with self-
attention mechanism. In Proceedings of EMNLP
2018, pages 182–192.

Tseng-Hung Chen, Yuan-Hong Liao, Ching-Yao
Chuang, Wan Ting Hsu, Jianlong Fu, and Min Sun.
2017. Show, adapt and tell: Adversarial training of
cross-domain image captioner. In Proceedings of
ICCV 2017.

Xilun Chen and Claire Cardie. 2018. Unsupervised
multilingual word embeddings. In Proceedings of
EMNLP 2018.

Xilun Chen, Yu Sun, Ben Athiwaratkun, Claire Cardie,
and Kilian Weinberger. 2016. Adversarial deep av-
eraging networks for cross-lingual sentiment classi-
fication. arXiv preprint arXiv:1606.01614.

Alexis Conneau, Guillaume Lample, Marc’Aurelio
Ranzato, Ludovic Denoyer, and Hervé Jégou. 2017.
Word translation without parallel data. arXiv
preprint arXiv:1710.04087.

Anne Cutler and Dennis Pasveer. 2006. Explaining
cross-linguistic differences in effects of lexical stress
on spoken-word recognition. In 3rd International
Conference on Speech Prosody. TUD press.

Jacob Devlin, Ming-Wei Chang, Kenton Lee, and
Kristina Toutanova. 2018. Bert: Pre-training of deep
bidirectional transformers for language understand-
ing. arXiv preprint arXiv:1810.04805.

Maud Ehrmann, Marco Turchi, and Ralf Steinberger.
2011. Building a multilingual named entity-
annotated corpus using annotation projection. In
Proceedings of RANLP 2011, pages 118–124.

Jan Vium Enghoff, Sren Harrison, and Zeljko Agi.
2018. Low-resource named entity recognition via
multi-source projection: Not quite there yet? In The
4th Workshop on Noisy User-generated Text.



3832

Meng Fang and Trevor Cohn. 2016. Learning when
to trust distant supervision: An application to low-
resource pos tagging using cross-lingual projection.
Proceedings of CoNLL 2016.

Meng Fang and Trevor Cohn. 2017. Model transfer
for tagging low-resource languages using a bilingual
dictionary. In Proceedings of ACL 2017.

Xiaocheng Feng, Lifu Huang, Bing Qin, Ying Lin,
Heng Ji, and Ting Liu. 2017. Multi-level cross-
lingual attentive neural architecture for low resource
name tagging. Tsinghua Science and Technology,
pages 633–645.

Lisheng Fu, Thien Huu Nguyen, BonanMin, and Ralph
Grishman. 2017. Domain adaptation for relation ex-
traction with domain adversarial neural network. In
Proceedings of IJCNLP 2017.

Yaroslav Ganin, Evgeniya Ustinova, Hana Ajakan,
Pascal Germain, Hugo Larochelle, François Lavio-
lette, Mario Marchand, and Victor Lempitsky. 2016.
Domain-adversarial training of neural networks. The
Journal of Machine Learning Research.

Dan Gillick, Cliff Brunk, Oriol Vinyals, and Amar-
nag Subramanya. 2016. Multilingual language pro-
cessing from bytes. In Proceedings of NAACL-HLT
2016.

Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza,
Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron
Courville, and Yoshua Bengio. 2014. Generative ad-
versarial nets. In Proceedings of NIPS 2014.

Lifu Huang, Kyunghyun Cho, Boliang Zhang, Heng
Ji, and Kevin Knight. 2018. Multi-lingual common
semantic space construction via cluster-consistent
word embedding. Proceedings of EMNLP 2018.

Zhiheng Huang, Wei Xu, and Kai Yu. 2015. Bidirec-
tional lstm-crf models for sequence tagging. arXiv
preprint arXiv:1508.01991.

Heng Ji, Joel Nothman, Ben Hachey, and Radu Flo-
rian. 2015. Overview of tac-kbp2015 tri-lingual en-
tity discovery and linking. In Proceedings of TAC
2015.

Joo-Kyung Kim, Young-Bum Kim, Ruhi Sarikaya, and
Eric Fosler-Lussier. 2017a. Cross-lingual transfer
learning for pos tagging without cross-lingual re-
sources. In Proceedings of EMNLP 2017.

Sungchul Kim, Kristina Toutanova, and Hwanjo Yu.
2012. Multilingual named entity recognition using
parallel data and metadata from wikipedia. In Pro-
ceedings of ACL 2012, pages 694–702.

Taeksoo Kim, Moonsu Cha, Hyunsoo Kim, Jung Kwon
Lee, and Jiwon Kim. 2017b. Learning to discover
cross-domain relations with generative adversarial
networks. In Proceedings of ICML 2017.

Yoon Kim, Yacine Jernite, David Sontag, and Alexan-
der M Rush. 2016. Character-aware neural language
models. In Proceedings of AAAI 2016, pages 2741–
2749.

Philipp Koehn. 2005. Europarl: A parallel corpus for
statistical machine translation. In MT summit, vol-
ume 5, pages 79–86.

Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hin-
ton. 2012. Imagenet classification with deep con-
volutional neural networks. In Proceedings of NIPS
2012, pages 1097–1105.

John D. Lafferty, Andrew McCallum, and Fernando
C. N. Pereira. 2001. Conditional random fields:
Probabilistic models for segmenting and labeling se-
quence data. In Proceedings of ICML 2001.

Guillaume Lample, Miguel Ballesteros, Sandeep Sub-
ramanian, Kazuya Kawakami, and Chris Dyer. 2016.
Neural architectures for named entity recognition. In
Proceedings of NAACL-HLT 2016.

Guillaume Lample and Alexis Conneau. 2019. Cross-
lingual language model pretraining. arXiv preprint
arXiv:1901.07291.

Zheng Li, Yu Zhang, YingWei, YuxiangWu, andQiang
Yang. 2017. End-to-end adversarial memory net-
work for cross-domain sentiment classification. In
Proceedings of IJCAI 2017.

Ying Lin, Shengqi Yang, Veselin Stoyanov, and Heng
Ji. 2018. A multi-lingual multi-task architecture for
low-resource sequence labeling. In Proceedings of
ACL 2018, volume 1, pages 799–809.

Liyuan Liu, Jingbo Shang, Xiang Ren,
Frank Fangzheng Xu, Huan Gui, Jian Peng,
and Jiawei Han. 2018. Empower sequence label-
ing with task-aware neural language model. In
Proceedings of AAAI 2018.

Xuezhe Ma and Eduard Hovy. 2016. End-to-end se-
quence labeling via bi-directional lstm-cnns-crf. In
Proceedings of ACL 2016.

AldrianObajaMuis, NaokiOtani, Nidhi Vyas, Ruochen
Xu, Yiming Yang, Teruko Mitamura, and Eduard
Hovy. 2018. Low-resource cross-lingual event type
detection via distant supervision with minimal effort.
In Proceedings COLING 2018.

Jian Ni, Georgiana Dinu, and Radu Florian. 2017.
Weakly supervised cross-lingual named entity recog-
nition via effective annotation and representation
projection. In Proceedings of ACL 2017.

Joel Nothman, Nicky Ringland, Will Radford, Tara
Murphy, and James R Curran. 2013. Learning mul-
tilingual named entity recognition from wikipedia.
Artificial Intelligence, pages 151–175.



3833

Xiaoman Pan, Boliang Zhang, Jonathan May, Joel
Nothman, Kevin Knight, and Heng Ji. 2017. Cross-
lingual name tagging and linking for 282 languages.
In Proceedings of ACL 2017, pages 1946–1958.

Matthew Peters, Waleed Ammar, Chandra Bhagavat-
ula, and Russell Power. 2017. Semi-supervised se-
quence tagging with bidirectional language models.
In Proceedings of ACL 2017.

Matthew Peters, Mark Neumann, Mohit Iyyer, Matt
Gardner, Christopher Clark, Kenton Lee, and Luke
Zettlemoyer. 2018. Deep contextualized word rep-
resentations. In Proceedings of NAACL 2018.

Alec Radford, Karthik Narasimhan, Tim Salimans, and
Ilya Sutskever. 2018. Improving language under-
standing by generative pre-training.

Ge Shi, Chong Feng, Lifu Huang, Boliang Zhang, Heng
Ji, Lejian Liao, and Heyan Huang. 2018. Genre sep-
aration network with adversarial training for cross-
genre relation extraction. In Proceedings of EMNLP
2018.

Erik F. Tjong Kim Sang. 2002. Introduction to
the conll-2002 shared task: Language-independent
named entity recognition. In Proceedings of CoNLL
2002, pages 1–4.

Erik F Tjong Kim Sang and Fien DeMeulder. 2003. In-
troduction to the conll-2003 shared task: Language-
independent named entity recognition. In Proceed-
ings of HLT-NAACL 2003.

Chen-Tse Tsai, Stephen Mayhew, and Dan Roth. 2016.
Cross-lingual named entity recognition via wikifica-
tion. In Proceedings of CoNLL 2016.

Eric Tzeng, Judy Hoffman, Kate Saenko, and Trevor
Darrell. 2017. Adversarial discriminative domain
adaptation. In Proceedings of CVPR 2017.

Dingquan Wang, Nanyun Peng, and Kevin Duh. 2017.
A multi-task learning approach to adapting bilin-
gual word embeddings for cross-lingual named en-
tity recognition. In Proceedings of IJCNLP 2017.

Mengqiu Wang, Wanxiang Che, and Christopher D
Manning. 2013. Joint word alignment and bilingual
named entity recognition using dual decomposition.
In Proceedings of ACL 2013, pages 1073–1082.

Mengqiu Wang and Christopher D Manning. 2013.
Cross-lingual pseudo-projected expectation regular-
ization for weakly supervised learning. arXiv
preprint arXiv:1310.1597.

Wenya Wang and Sinno Jialin Pan. 2018. Transition-
based adversarial network for cross-lingual aspect
extraction. In Proceedings of IJCAI 2018, pages
4475–4481.

Jiateng Xie, Zhilin Yang, Graham Neubig, Noah A
Smith, and Jaime Carbonell. 2018. Neural cross-
lingual named entity recognition with minimal re-
sources. In Proceedings of EMNLP 2018.

Chao Xing, DongWang, Chao Liu, and Yiye Lin. 2015.
Normalized word embedding and orthogonal trans-
form for bilingual word translation. In Proceedings
of NAACL 2015.

Zhilin Yang, Ruslan Salakhutdinov, and William
Cohen. 2016. Multi-task cross-lingual se-
quence tagging from scratch. arXiv preprint
arXiv:1603.06270.

Zhilin Yang, Ruslan Salakhutdinov, andWilliamWCo-
hen. 2017. Transfer learning for sequence tagging
with hierarchical recurrent networks. arXiv preprint
arXiv:1703.06345.

David Yarowsky, Grace Ngai, and Richard Wicen-
towski. 2001. Inducing multilingual text analysis
tools via robust projection across aligned corpora. In
Proceedings of HLT 2001.

Boliang Zhang, Di Lu, Xiaoman Pan, Ying Lin, Hal-
idanmu Abudukelimu, Heng Ji, and Kevin Knight.
2017a. Embracing non-traditional linguistic re-
sources for low-resource language name tagging. In
Proceedings of IJCNLP 2017), pages 362–372.

Boliang Zhang, Xiaoman Pan, Tianlu Wang, Ashish
Vaswani, Heng Ji, Kevin Knight, and Daniel Marcu.
2016a. Name tagging for low-resource incident lan-
guages based on expectation-driven learning. In
Proceedings of NAACL 2016, pages 249–259.

Boliang Zhang, Spencer Whitehead, Lifu Huang, and
Heng Ji. 2018. Global attention for name tagging.
In Proceedings of CoNLL 2018, pages 86–96.

Dongxu Zhang, Boliang Zhang, Xiaoman Pan, Xi-
aocheng Feng, Heng Ji, and XU Weiran. 2016b. Bi-
text name tagging for cross-lingual entity annotation
projection. In Proceedings of COLING 2016.

Meng Zhang, Yang Liu, Huanbo Luan, and Maosong
Sun. 2017b. Adversarial training for unsupervised
bilingual lexicon induction. In Proceedings of ACL
2017.

Meng Zhang, Yang Liu, Huanbo Luan, and Maosong
Sun. 2017c. Earth mover’s distance minimization
for unsupervised bilingual lexicon induction. InPro-
ceedings of EMNLP 2017.

Ayah Zirikly and Masato Hagiwara. 2015. Cross-
lingual transfer of named entity recognizers without
parallel corpora. In Proceedings of ACL 2015.

Bowei Zou, Zengzhuang Xu, Yu Hong, and Guodong
Zhou. 2018. Adversarial feature adaptation for
cross-lingual relation classification. In Proceedings

of COLING 2018.


