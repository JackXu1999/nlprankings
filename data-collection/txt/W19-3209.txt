



















































Identifying Adverse Drug Events Mentions in Tweets Using Attentive, Collocated, and Aggregated Medical Representation


Proceedings of the 4th Social Media Mining for Health Applications (#SMM4H) Workshop & Shared Task, pages 62–70
Florence, Italy, August 2, 2019. c©2019 Association for Computational Linguistics

62

Identifying adverse drug events mentions in tweets using attentive,
collocated, and aggregated medical representation

Xinyan Zhao,1∗ Deahan Yu,1∗ V.G.Vinod Vydiswaran2,1
1 School of Information, 2 Department of Learning Health Sciences, University of Michigan

{zhaoxy,deahanyu,vgvinodv}@umich.edu
* denotes equal contribution

Abstract

Identifying mentions of medical concepts in
social media is challenging because of high
variability in free text. In this paper, we pro-
pose a novel neural network architecture, the
Collocated LSTM with Attentive Pooling and
Aggregated representation (CLAPA), that in-
tegrates a bidirectional LSTM model with at-
tention and pooling strategy and utilizes the
collocation information from training data to
improve the representation of medical con-
cepts. The collocation and aggregation lay-
ers improve the model performance on the
task of identifying mentions of adverse drug
events (ADE) in tweets. Using the dataset
made available as part of the workshop shared
task, we show that careful selection of neigh-
borhood contexts can help uncover useful local
information and improve the overall medical
concept representation.

1 Introduction

Multiple studies have analyzed health forums and
other social media for drug uses, pharmacovigi-
lance, and effectiveness of medications (Nikfar-
jam et al., 2015; Daniulaityte et al., 2016). How-
ever, research related to drugs and adverse drug
effects (ADE) in social media continues to grow
rapidly. Automatically detecting ADE mentions
in social media posts has been challenging due to
the large variability of free text. One of the main
challenges in studying natural language process-
ing (NLP) approaches for medical information ex-
traction is the lack of access to health-related in-
formation on social media (Weissenbacher et al.,
2019).

Having a robust representation of words is im-
portant to train high-performance information ex-
traction approaches. In domain-specific tasks,
being able to properly represent domain words
or concepts could significantly improve the mod-

els. While many studies have undertaken classi-
fications of ADE mentions in posts with various
state-of-the-art techniques (Nikfarjam et al., 2015;
Weissenbacher et al., 2018), there is still room to
improve for the task. For example, in many trained
word embedding models (Pennington et al., 2014;
Godin et al., 2015; Joulin et al., 2017), the embed-
ding of each word is treated as a vector summariz-
ing multiple semantic meanings for each word as
independent dimensions. Indeed, pre-trained em-
beddings that are trained on a large data corpus
usually provide robust representation for common
words, compared to traditional feature-based tech-
niques such as bag of words. Yet, for domain-
specific tasks, a drawback of pre-trained embed-
dings is that representations of domain words may
not be sufficiently tuned to be able to represent the
expected meaning.

Attempts have been made previously to cap-
ture the word embedding for medical concepts
from a variety of medical data sources (Huang
et al., 2016). Similarly, domain-specific knowl-
edge graphs have been shown effective as external
resources for feature expansion to represent medi-
cal concepts (Choi et al., 2017; Wang et al., 2017).
However, even domain-based knowledge graphs
sometime contain redundant information stem-
ming from how they are constructed (Yu et al.,
2014; Paulheim, 2017; Zaveri et al., 2016). Fol-
lowing prior work by (Turenne, 2003) that show
that co-occurring pattern of terms could be bene-
ficial to classification tasks, in this work, we con-
sider an alternate graph-based representation that
utilizes local information derived from the training
data set. We build a collocation graph – a word-
based graph built from the training data set where
nodes correspond to vocabulary words and edges
between two nodes indicate the co-occurrence of
the corresponding words. We investigate if a
model built over the collocation graph could use



63

pre-trained word embeddings and other informa-
tion to recognize medical concepts from data. We
hypothesize that the representation of a medical
word can be further enriched by its neighbors in
the collocation graph.

In this paper, we propose Collocated LSTM
with Attentive Pooling and Aggregated repre-
sentation (CLAPA), a novel approach that inte-
grates bidirectional LSTM model with attention
and pooling strategy and utilizes the collocation
information in the training data set to help enhance
the pre-trained word embedding of medical con-
cepts. We show that our model leads to a signif-
icant improvement on an ADE detection task. To
the best of our knowledge, this is the first attempt
that utilizes local collocation information to im-
prove the representation of domain concepts in so-
cial media.

To summarize, we make the following contribu-
tions in this paper:

• We propose a novel architecture that encodes
locally stored domain information into sen-
tence representation.

• Our work explores the possibility that limited
training data could be better exploited by in-
cluding attentive collocation information.

• We provide implication for other domain-
related works where better representation of
domain terms is important, especially when
the data set is highly imbalanced.

2 Related work

Researchers have tackled the problem of iden-
tifying posts mentioning ADEs in social me-
dia in different ways. Various methods have
been used in the 2018 Social Media Mining for
Health Applications (SMM4H) shared task, rang-
ing from statistical models such as support vec-
tor machines (SVM) to deep neural network mod-
els such as convolutional neural network (CNN),
long short-term memory (LSTM), and bidirec-
tional LSTM models. Fourteen teams participated
in the 2018 SMM4H shared tasks (Weissenbacher
et al., 2018), and used deep neural network mod-
els and various text processing steps such as cor-
recting misspellings, accounting for class imbal-
ance in data, and incorporating external resources.
For the ADE mention classification task, the best
system achieved an F1 score of 0.522, while the

next best system achieved an F1 score of 0.478.
The best system (Wu et al., 2018) was based on a
bidirectional LSTM model with hierarchical tweet
representation and multi-head self-attention.

In recent years, models such as CNN (Kim,
2014) and bidirectional LSTM (Graves and
Schmidhuber, 2005) were used for text classifica-
tion. In addition, models with attention mecha-
nism, which incorporates information of other in-
put tokens to improve representation of each to-
ken, was introduced by (Vaswani et al., 2017).
Several max-pooling techniques, which help to de-
tect important ngrams, were explored by (Jacovi
et al., 2018) and (Zhou et al., 2016). Such mech-
anisms and technique have been powerful tools to
build better text classification systems. To train
distributed representations of words, (Mikolov
et al., 2013) introduced Word2Vec in which each
word is represented in a low-dimensional vector
space. Other popular, pre-trained word embed-
dings include GloVe (Pennington et al., 2014),
Word2vec over Twitter (Godin et al., 2015), and
FastText (Joulin et al., 2017). Similarly, graph
embedding techniques over large-scale networks
were studied by numerous prior works, includ-
ing LINE (Tang et al., 2015), DeepWalk (Per-
ozzi et al., 2014), and Node2Vec (Grover and
Leskovec, 2016). Although graph embedding is
similar to word embedding, it is trained on not
only nodes adjacent to each node but on the entire
local network around the node. So, graph embed-
ding could capture the relations between nodes,
and has been used for multi-label classification
and community detection (Grover and Leskovec,
2016; Qiu et al., 2018). Since most text-based
graphs are typically reducible to a linear chain, and
the ADE detection task is a binary classification
problem, we focus on only the word embedding-
based approaches in this paper.

3 Collocation and aggregated
representation models

In this section, we describe the architecture of our
model in detail. The model contains the follow-
ing three key components — medical collocation
embedding, sentence encoder, and max pooling.
The overall architecture of our model is shown in
Figure 1. For each word, the embedding is com-
posed of two parts, namely, a pre-trained word
embedding and an attentive neighborhood embed-
ding. Attentive neighborhood embedding is de-



64

Figure 1: Overall architecture of the proposed model for identifying adverse drug events

rived from the Concept-Neighbor (C-N ) tensor.
In a C-N cube, each Ni represents the neighbor-
hood for the i-th concept. Based on an attention
vector (MedAttni), a concept embedding matrix C
is formed in which ci is the embedding for the
concept. The collocation embedding for a word
wt will be ci if wt is the i-th concept, otherwise,
the collocation embedding will be initialized to the
zero vector. The concatenated embedding is then
fed into an LSTM layer, and multi-head attention
and maxpooling are applied to extract informative
neurons, which are then concatenated with (1) the
final state of the LSTM (sentence encoding) and
(2) the sum of the concept embedding matrix. The
final output is then computed via a fully connected
neural network with a softmax function. Table 1
summarizes the notations used in this paper.

3.1 Medical collocation embedding

In order to better utilize the medical information
embedded in text, we propose two word embed-
ding methods – a pre-trained word embedding,
and a second embedding method that enhances the
pre-trained representation of medical terms by ex-
tracting information around those terms from the
collocation graph.

Our medical collocation embedding can there-

Notation Definition
W = [w1, . . . , wT ] a sequence of words
S = [s1, . . . , s|S|] medical concept set
C = [c1, . . . , c|S|] concept matrix of size R|S|×d

Ni = [ni1, . . . , niK ] neighborhood matrix of size
RK×d for the i-th concept.

C-N tensor neighborhood tensor with the
size R|C|×K×d composed by the
neighborhood of each concept

wt t-th word in a text sequence.
si i-th medical concept word in S
ci medical collocation embedding

of the si
nik word embedding of the k-th

neighbor for the i-th concept in
the concept set.

mt medical collocation embedding
for the word wt.

|S| total number of concepts
T total number of words in a se-

quence
K maximum neighborhood size
L total number of attention heads
d dimension of word embedding

dh dimension of hidden states in
LSTM

Table 1: Notation definitions

fore be defined as following (Eq. 1):

MedAttnij =
exp(f(nij ,W1))∑
k exp(f(nik,W1))

ci = MedAttnij ×Ni
mt = ∆(wt, si)× ci

(1)

where f(·) represents a linear transformation
and the WK×11 is a trainable parameter matrix.
MedAttnij calculates the attention that should be



65

paid to the j-th neighbor for the concept si. There-
fore, the embedding ci is represented by the em-
bedding of its neighborhood weighted by attention
scores. Lastly, mt represents the medical colloca-
tion embedding for the t-th word in text, wt. If the
word is matched to the i-th medical concept, then
mt = ci. (∆(x, y) = 1 if x = y; 0 otherwise).

3.2 Aggregated Medical Representation

In addition to the word-based medical concept em-
bedding described in Sec. 3.1, we propose an-
other aggregated medical representation strategy
using the collocation information that aggregates
the medical concept information in a sentence into
a fixed feature space.

First, we use an attentive embedding, ci, de-
scribed in Eq. 1, to construct a medical concept
representation using the neighborhood informa-
tion. Then, the aggregated representation is con-
structed, as follows:

c∗i = ci ⊕ e(si)

Aggre =
∑
i

δ(i)× c∗i (2)

where e(·) is the function that retrieves the orig-
inal representation of the medical concept word
from pre-trained embedding. δ(·) = 1, when the
sentence contains the concept word, and 0 oth-
erwise. This aggregated medical representation
serves as the residual medical information that is
to be added to the output layer.

3.3 Sentence encoding

To encode a sentence for the classification task,
we used an attention-based LSTM to encode the
entire sentence into a fixed vector space. L at-
tention heads are applied to re-represent hidden
states. The new hidden states from the l-th atten-
tion head can be described as follows (Eq. 3):

H, s = LSTM([e(w1)⊕m1, . . . , e(wT )⊕mT ])

SentAttnlt =
exp(f(ht,W

l
2))∑

k exp(f(hk,W
l
2))

ĥlt = SentAttn
l
t · ht

(3)
where H = [h1, . . . , hT ] ∈ Rdh×T is a hidden
state matrix representing the information status at
each time step, and dh is a hidden dimension. e(·)
and f(·) are the same as defined in Eq. 1. SentAttnlt
is a scalar representing the attention that should be

paid to ht. Therefore, ĥlt is the attentive hidden
state scaled by attention values in the l-th attention
head.

3.4 Max pooling layer
Motivated by previous studies (Jacovi et al., 2018;
Zhou et al., 2016), the application of max pooling
behavior can highlight the important signals from
features and hence improve classification tasks.
Following these previous approaches, we apply
a max pooling layer to extract important signals
from the attentive hidden state in each attention
head (Eq. 4).

signall = pooling(Ĥl) (4)

where Ĥl = [ĥl1, . . . , ĥ
l
T ] ∈ Rdh×T , and the

pooling is applied on the dimension of dh so that
signall ∈ Rdh contains important signals from
each hidden dimension.

3.5 Classification layer
In the final output layer, the classification decision
is made on whether or not a sentence contains an
ADE mention. A fully connected network module
is implemented as:

r = s⊕ signal1 ⊕ . . .⊕ signalL ⊕ Aggre

r
′

= ReLU(U1r + b1)

ŷ = softmax(U2r
′
+ b2)

(5)

where r is the combination of the final state of
LSTM, multiple pooled states using max pool-
ing, and aggregated medical concept representa-
tion. Each pooled state vector signall comes from
one attention layer (L attention layers in total) that
is applied in sentence encoding (Eq. 3). U1, U2, b1,
and b2 are parameters to be trained. Cross-entropy
is used as the loss function for training:

loss = −
∑
i

∑
k

yk log(ŷk) (6)

4 Experiments

4.1 Data
For our experiments, we used the data set pro-
vided as part of Task 1 of the SMM4H 2019 shared
tasks (Gonzalez-Hernandez et al., 2019). As sum-
marized in Table 2, the total number of anno-
tated tweets is 25,678. The data set was randomly
split into a training set (80%) and a validation set



66

Training set Validation set
N = 25,678 (80% of data) (20% of data)
ADE tweets 1,892 485

Non-ADE tweets 18,650 4,651

Table 2: Number of ADE and non-ADE tweets in
training and validation data sets.

(20%), while maintaining the target class propor-
tions according to the original distribution. As a
result, our training set contains 1,892 tweets that
have an ADE mention (positive cases), and 18,650
tweets that do not have any mention of ADEs (neg-
ative cases). The validation set contains 485 pos-
itive and 4,651 negative tweets. We cleaned the
tweets by separating punctuation marks, removing
special characters, and replacing mentions, URLs,
and number representations with normalized to-
kens. Finally, we used fastText (Joulin et al., 2017)
as the pre-trained word embedding model.

4.2 Collocation graph
To build our collocation graph, we treat each
unique word in the training set as a node, and add
undirected edges from a word to adjacent words in
a tweet. The collocation graph consists of 27,440
nodes and 188,329 edges. To reduce the graph
size, we removed all words that appeared fewer
than three times in the corpus. The resultant graph
has 12,438 nodes and 159,759 edges. The mean of
degree centrality is 25.39 (sd =114.59). 50% of
the nodes have degrees less than 8, and 75% of the
nodes have degrees less than 17.

Tysabri Walgreens

Figure 2: Examples of a collocation graph: Tysabri is
considered as a medical concept while Walgreens is not
considered as a medical concept.

Figure 2 shows the examples of a collocation
graph. The graph has two colors: red and grey.
The red nodes are words that are identified as med-
ical concepts while the grey nodes are words that
are not identified as medical concepts. The col-
location graph on the left is for a medical word,
Tysabri. The neighborhood of the word is com-
prised of both medical and non-medical words.

Tysabri contains other medical words as neigh-
bors such as infusion, treatment, and gilenya. The
collocation graph on the right is for a word, wal-
greens. It contains few medical words such as
cipro and miralax.

4.3 Medical concepts extraction

MetaMap, a widely used system for identifying
medical concepts in the unified language medical
system (UMLS), is used to extract potential con-
cepts from our tweet data set (Aronson, 2006).
Given a sentence as input, MetaMap identifies
phrases that could be medical concepts, and maps
concepts to a preferred name using UMLS. How-
ever, since MetaMap is designed to parse clinical
documents rather than free text on social media,
we consider only those marked phrases that are the
same as the preferred name as valid medical con-
cepts. After processing, 1, 340 concepts were ex-
tracted by MetaMap from ADE tweets and 3, 921
concepts were extracted from non-ADE tweets.
Concepts are later split into single words.

4.4 Training setup

All hyperparameters are jointly trained with a
learning rate of 0.001 for ten epochs. In the exper-
iments, we used FastText pretrained embedding,
and the hidden size for LSTM is set to be 300.
Number of multi-head attention layer is set to be
3. For each experiment, the score is taken from the
average of five runs.

4.5 Results

To evaluate our model, we set two baselines:
an attention-based LSTM model (Eq. 3), and an
attention-based LSTM model with max pooling
(Eq. 4). The results are presented in Table 3 as
rows (1) and (4), respectively.

Model Precision Recall F1
(1). LSTM+Attn (LA) 0.6626 0.4495 0.5356
(2). (1)+colloc (CLA) 0.6392 0.4639 0.5142
(3). (2)+Aggr (CLAA) 0.5181 0.5918 0.5525
(4). (1)+Pool (LAP) 0.6475 0.4887 0.5570
(5). (4)+colloc (CLAP) 0.6359 0.5546 0.5925
(6). (5)+Aggr (CLAPA) 0.6017 0.5979 0.5998
CLAPA on Test set 0.5944 0.5431 0.5676
Avg. system score 0.5351 0.5054 0.5019

Table 3: Comparison of models on Precision, Recall,
and F1 measures for the ADE detection task on the val-
idation set. The scores in the last two rows are over the
test set of the 2019 SMM4H 2019 shared task 1.



67

As presented in Table 3, the model perfor-
mance is significantly improved with the addition
of collocation medical embedding and aggregated
embedding, over the attention-based bi-direction
LSTM models. Further, adding aggregated medi-
cal information helps improve recall, but reduces
the model precision and only slightly increases
the F1 score, compared to the collocation based
model. Hence, while highlighting medical in-
formation can reduce false negative decisions, it
also causes more instances to be labeled as ADE
tweets, thereby increasing a false positive rate as
well. The CLAPA model, that integrates both col-
location and aggregated representation along with
attentive pooling strategy performs the best.

When run against the test set for the shared
task, the CLAPA model achieves a F1 score of
0.5676 (see Table 3). As a comparison, the aver-
age F1 score of systems participating in this task is
0.5019. This shows our CLAPA model performs
significantly better than average on this task.

4.6 Model learning stability
To show that our model consistently works better
even with smaller training data, we independently
and randomly sampled 10%, 30%, 50%, 70%, and
90% data from training set and retrained the mod-
els. Figure 3 shows that our model consistently
performed well on the validation set, even with
reduced training size, compared to the baseline
model of bidirectional LSTM model with atten-
tive pooling (the “LAP” model). The results are
similar to those on the full validation data set in
Table 3, in that even when only a fraction of train-
ing data is available, the model achieves higher F1
score because of significantly better recall and at a
relatively small reduction in precision.

4.7 Effect of concept vocabulary
Next, we analyzed the effect of medical concepts
observed in the ADE tweets to understand if there
is any difference in terms of the use of medical
concepts in ADE tweets vs. non-ADE tweets.
We calculated a propensity ratio of each medi-
cal term, based on number of times it appears in
ADE tweets compared to non-ADE tweets. We
found that causing, gain, drowsiness, and sweats
are likely to appear in ADE tweets about 15 times
more often than in non-ADE tweets. Similarly,
crippled is likely to appear in an ADE tweet about
26 times more often than in a non-ADE tweet.
Considering the highly skewed appearance ratio

Figure 3: Effects of training size on model performance
stability

for certain concepts, we analyzed the effect on us-
ing concepts from the ADE tweets alone. We com-
pared two models – one trained over medical con-
cepts identified from the ADE tweets and another
trained over concepts from the entire training set,
i.e. both ADE and non-ADE tweets.

Concepts from Precision Recall F1
All tweets 0.6142 0.5546 0.5829
Only ADE tweets 0.6017 0.5979 0.5998

Table 4: Effects of concept vocabulary on model per-
formance

As summarized in Table 4, the model trained
with concepts from just the ADE tweets achieved
a higher F1 score. While the precision is slightly
lower, the model trained over concepts from ADE
tweets has a significantly higher recall. On fur-
ther analysis, we find that out of the 1, 183 concept
words extracted from the ADE tweets, 866 con-
cepts (73.2%) occurred more frequently in ADE
tweets than in non-ADE tweets. However, when
using the concepts words extracted from both
ADE and non-ADE tweets, the number of con-
cepts are higher (n = 4, 643), but only 1, 094 con-
cepts (23.6%) of those appear more frequently in
the ADE tweets. This indicates that propensity ra-
tio could be used for selecting medical concepts
used in the ADE tweets as features.

4.8 Effects of neighborhood selection
We analyzed two additional questions related to
parameter tuning:

(1) What method should be used to pick a
neighbor? To answer this question, we fixed the



68

neighborhood size as 15 words, and selected one
of the following three methods to choose neigh-
bors:

(a) Random: Given a node n, we randomly se-
lect k of its neighbors n1, n2, . . . , nk ∈ N , where
N is a set of all neighbors for node n.

(b) Popularity: For each medical concept, we
first selected a neighbor that has the highest de-
gree. When node ni has more neighbors than node
nj , we say that node ni is more popular than node
nj . Then, given a node n, we select k popular
neighbors n1, n2, . . . , nk that have the highest de-
gree. In case of ties in popularity, neighbors are
selected at random from this set.

(c) Medical neighbor: Given node n, we add k
medically-related neighbors.

For all three neighborhood selection methods,
if the total number of first-degree neighbors is less
than k, then an additional random selection is used
among second-degree neighbors to fill the gap.

Table 5 shows the results using different selec-
tion methods under the two scenarios described in
Section 4.7. The left column depicts the model
trained on concepts from all tweets, and the right
column represents the model trained with concepts
from ADE tweets alone.

F1 scores
Selection method ADE+non-ADE ADE
Random 0.5796 0.5683
Popularity 0.5819 0.5998
Medical neighbor 0.5829 0.5887

Table 5: Effects of neighborhood selection methods
on F1 scores on both ADE+non-ADE tweets and only
ADE tweets

Table 5 shows that targeting at neighbors us-
ing either popularity or medical attributes always
leads to better performance regardless of differ-
ent scenarios. However, when using medical con-
cepts of both ADE and non-ADE tweets, picking
a medical neighborhood could be a better choice,
whereas popular neighborhood is preferred when
concepts are identified from ADE tweets. Med-
ical neighborhood has a higher probability of in-
cluding informative words related to ADE; and
when only ADE tweets are considered, the fre-
quency of co-occurrence of a neighbor and the
concepts become more important. This explana-

tion also aligns with how language models are usu-
ally trained.

Figure 4: Effects of neighborhood size (k) on model
performance

(2) How should we decide neighborhood size?
We experimented with different neighborhood
size. As shown in Fig. 4, as the neighborhood
size k increases, the performance is not affected
much when k is small (from 5 to 20). How-
ever, the performance drops significantly when k
is larger (k > 20). We explain this by aligning
back to our neighborhood selection method where
we found that choosing good neighbors (popular
or medically related) favors the model. We want to
choose informative neighbors instead of all neigh-
bors. Therefore, when k is small, the selected
neighbors (high degree) can be easily differenti-
ated from the ones not selected. However, when k
is large, the selected neighbors become less infor-
mative because many unimportant, noisy, neigh-
bor words (low degree/non-popular) may be in-
cluded that harm the model.

5 Limitation and future work

After the above examination of our model, we
argue that our model suffers from three main
limitations. First, although MetaMap has been
found useful at parsing medical notes, due to the
different linguistic use on social media, running
MetaMap on tweets may not identify relevant con-
cepts. Second, the use of collocation graph and ag-
gregated medical concept representation reduced
precision of models, although the overall recall
and F1 improved. Additional studies are need to
further improve the precision. Third, the colloca-
tion graph is built solely on the training data set.



69

This may not favor the model when the data set
is not representative enough to provide neighbor-
hood of high quality. To address the first two is-
sues, we believe a pre-trained state-of-the-art med-
ication detection system could be helpful to iden-
tify high-quality medical concepts from tweets.
For the third issue, we plan to use domain based
knowledge base such as UMLS to expand the cov-
erage of the limited data.

We used fastText as the pre-trained word em-
bedding for our model. While fastText is trained
on sub-word representations, models trained over
medical or larger text corpora might provide addi-
tional contextual representation. Additional stud-
ies are needed to test our model on different pre-
trained word embeddings such as Word2vec over
Twitter (Godin et al., 2015). We also note that
there is a difference in the use of medical related
concepts in different classes by testing two scenar-
ios — a model using medical concepts identified
from both ADE and non-ADE cases and one us-
ing those from the ADE cases. In future, we plan
to test this approach by exploring the use of unique
nodes in different classes. Meanwhile, the appli-
cation of our approach on other domain-specific
tasks should be verified to examine the generaliza-
tion of the approach.

6 Conclusion

In this work, we argue that a collocation graph can
be utilized to enrich the representation of a med-
ical concept. We further propose a novel neural
network architecture that uses attentive informa-
tion from a collocation graph to re-embed medical
words. Our experiments show that, with a good
selection of neighborhood, more useful local in-
formation can be accessed, which in turn improves
the medical concept representation and the overall
model performance in detecting mentions of ad-
verse drug events in tweets.

Acknowledgment

We would like to thank Daniel Romero for his ad-
vice and feedback throughout the study.

References

Alan R Aronson. 2006. Metamap: Mapping text to
the umls metathesaurus. Bethesda, MD: NLM, NIH,
DHHS, pages 1–26.

Edward Choi, Mohammad Taha Bahadori, Le Song,
Walter F. Stewart, and Jimeng Sun. 2017. Gram:
Graph-based attention model for healthcare repre-
sentation learning. In KDD.

Raminta Daniulaityte, Lu Chen, Francois R Lamy,
Robert G Carlson, Krishnaprasad Thirunarayan, and
Amit Sheth. 2016. “when ‘bad’ is ‘good’ ”:
Identifying personal communication and sentiment
in drug-related tweets. JMIR Public Health and
Surveillance, 2(2):e162.

Fréderic Godin, Baptist Vandersmissen, Wesley
De Neve, and Rik Van de Walle. 2015. Multimedia
lab @ acl wnut ner shared task: Named entity recog-
nition for twitter microposts using distributed word
representations. In Proceedings of the Workshop on
Noisy User-generated Text, pages 146–153, Beijing,
China. Association for Computational Linguistics.

Graciela Gonzalez-Hernandez, Davy Weissenbacher,
Michael Paul, Abeed Sarker, Ari Z. Klein, Ar-
jun Magge, Ashlynn R. Daughton, and Karen
O’Connor. 2019. Social media mining for health ap-
plications (smm4h) workshop & shared task 2019.
Https://healthlanguageprocessing.org/smm4h/.

Alex Graves and Jurgen Schmidhuber. 2005. Frame-
wise phoneme classification with bidirectional lstm
networks. In Proceedings of the IEEE International
Joint Conference on Neural Networks, volume 4,
pages 2047–2052.

Aditya Grover and Jure Leskovec. 2016. node2vec:
Scalable feature learning for networks. In Proceed-
ings of the International Conference on Knowledge
Discovery & Data Mining, pages 855–864.

Jian Huang, Keyang Xu, and V.G. Vinod Vydiswaran.
2016. Analyzing multiple medical corpora using
word embedding. In IEEE International Conference
on Healthcare Informatics (ICHI), volume 1, pages
527–533.

Alon Jacovi, Oren Sar Shalom, and Yoav Gold-
berg. 2018. Understanding convolutional neural
networks for text classification. arXiv preprint
arXiv:1809.08037.

Armand Joulin, Edouard Grave, Piotr Bojanowski, and
Tomas Mikolov. 2017. Bag of tricks for efficient
text classification. In Proceedings of the 15th Con-
ference of the European Chapter of the Association
for Computational Linguistics: Volume 2, Short Pa-
pers, pages 427–431. Association for Computational
Linguistics.

Yoon Kim. 2014. Convolutional neural networks for
sentence classification. In Proceedings of the 2014
Conference on Empirical Methods in Natural Lan-
guage Processing, EMNLP 2014, October 25-29,
2014, Doha, Qatar, A meeting of SIGDAT, a Special
Interest Group of the ACL, pages 1746–1751.

https://doi.org/10.2196/publichealth.6327
https://doi.org/10.2196/publichealth.6327
https://doi.org/10.2196/publichealth.6327
https://doi.org/10.18653/v1/W15-4322
https://doi.org/10.18653/v1/W15-4322
https://doi.org/10.18653/v1/W15-4322
https://doi.org/10.18653/v1/W15-4322
https://doi.org/10.1109/IJCNN.2005.1556215
https://doi.org/10.1109/IJCNN.2005.1556215
https://doi.org/10.1109/IJCNN.2005.1556215
https://doi.org/10.1109/ICHI.2016.94
https://doi.org/10.1109/ICHI.2016.94
http://aclweb.org/anthology/D/D14/D14-1181.pdf
http://aclweb.org/anthology/D/D14/D14-1181.pdf


70

Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Cor-
rado, and Jeff Dean. 2013. Distributed representa-
tions of words and phrases and their compositional-
ity. In Advances in neural information processing
systems, pages 3111–3119.

Azadeh Nikfarjam, Abeed Sarker, Karen O’Connor,
Rachel Ginn, and Graciela Gonzalez. 2015. Phar-
macovigilance from social media: Mining adverse
drug reaction mentions using sequence labeling
with word embedding cluster features. Journal
of the American Medical Informatics Association :
JAMIA, 22(3):671–681.

Heiko Paulheim. 2017. Knowledge graph refinement:
A survey of approaches and evaluation methods. Se-
mantic web, 8(3):489–508.

Jeffrey Pennington, Richard Socher, and Christopher
Manning. 2014. Glove: Global vectors for word
representation. In Proceedings of the 2014 confer-
ence on empirical methods in natural language pro-
cessing (EMNLP), pages 1532–1543.

Bryan Perozzi, Rami Al-Rfou’, and Steven Skiena.
2014. Deepwalk: online learning of social repre-
sentations. In KDD.

Jiezhong Qiu, Yuxiao Dong, Hao Ma, Kuansan Wang,
and Jie Tang. 2018. Network embedding as ma-
trix factorization: Unifying deepwalk, line, pte, and
node2vec. In WSDM.

Jian Tang, Meng Qu, Mingzhe Wang, Ming Zhang, Jun
Yan, and Qiaozhu Mei. 2015. Line: Large-scale in-
formation network embedding. In WWW.

Nicolas Turenne. 2003. Learning semantic classes for
improving email classification. In Proceedings of
Text Mining and Link Analysis Workshop.

Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob
Uszkoreit, Llion Jones, Aidan N Gomez, Ł ukasz
Kaiser, and Illia Polosukhin. 2017. Attention is all
you need. In I. Guyon, U. V. Luxburg, S. Bengio,
H. Wallach, R. Fergus, S. Vishwanathan, and R. Gar-
nett, editors, Advances in Neural Information Pro-
cessing Systems 30, pages 5998–6008. Curran As-
sociates, Inc.

Meng Wang, Mengyue Liu, Jun Liu, Sen Wang,
Guodong Long, and Buyue Qian. 2017. Safe
medicine recommendation via medical knowledge
graph embedding. CoRR, abs/1710.05980.

Davy Weissenbacher, Abeed Sarker, Arjun Magge,
Ashlynn Daughton, Karen O’Connor, Michael Paul,
and Graciela Gonzalez-Hernandez. 2019. Overview
of the fourth Social Media Mining for Health
(SMM4H) shared task at ACL 2019. In Proceedings
of the 2019 ACL Workshop SMM4H: The 4th Social
Media Mining for Health Applications Workshop &
Shared Task.

Davy Weissenbacher, Abeed Sarker, Michael J Paul,
and Graciela Gonzalez-Hernandez. 2018. Overview
of the third social media mining for health (smm4h)
shared tasks at emnlp 2018. In Proceedings of the
2018 EMNLP Workshop SMM4H: The 3rd Social
Media Mining for Health Applications Workshop &
Shared Task, pages 13–16.

Chuhan Wu, Fangzhao Wu, Junxin Liu, Sixing Wu,
Yongfeng Huang, and Xing Xie. 2018. Detect-
ing tweets mentioning drug name and adverse drug
reaction with hierarchical tweet representation and
multi-head self-attention. In Association for Com-
putational Linguistics, The 3rd Social Media Mining
for Health Applications Workshop and Shared Task.

Dian Yu, Hongzhao Huang, Taylor Cassidy, Heng Ji,
Chi Wang, Shi Zhi, Jiawei Han, Clare Voss, and Ma-
lik Magdon-Ismail. 2014. The wisdom of minority:
Unsupervised slot filling validation based on multi-
dimensional truth-finding. In Proceedings of COL-
ING 2014, the 25th International Conference on
Computational Linguistics: Technical Papers, pages
1567–1578.

Amrapali Zaveri, Anisa Rula, Andrea Maurino, Ri-
cardo Pietrobon, Jens Lehmann, and Sören Auer.
2016. Quality assessment for linked data: A survey.
Semantic Web, 7(1):63–93.

Peng Zhou, Zhenyu Qi, Suncong Zheng, Jiaming Xu,
Hongyun Bao, and Bo Xu. 2016. Text classifi-
cation improved by integrating bidirectional lstm
with two-dimensional max pooling. arXiv preprint
arXiv:1611.06639.

https://doi.org/10.1093/jamia/ocu041
https://doi.org/10.1093/jamia/ocu041
https://doi.org/10.1093/jamia/ocu041
https://doi.org/10.1093/jamia/ocu041
http://papers.nips.cc/paper/7181-attention-is-all-you-need.pdf
http://papers.nips.cc/paper/7181-attention-is-all-you-need.pdf
http://arxiv.org/abs/1710.05980
http://arxiv.org/abs/1710.05980
http://arxiv.org/abs/1710.05980

