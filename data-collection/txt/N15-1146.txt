



















































MPQA 3.0: An Entity/Event-Level Sentiment Corpus


Human Language Technologies: The 2015 Annual Conference of the North American Chapter of the ACL, pages 1323–1328,
Denver, Colorado, May 31 – June 5, 2015. c©2015 Association for Computational Linguistics

MPQA 3.0: An Entity/Event-Level Sentiment Corpus

Lingjia Deng
Intelligent Systems Program

University of Pittsburgh
lid29@pitt.edu

Janyce Wiebe
Intelligent Systems Program

Department of Computer Science
University of Pittsburgh
wiebe@cs.pitt.edu

Abstract

This paper presents an annotation scheme for
adding entity and event target annotations to
the MPQA corpus, a rich span-annotated opin-
ion corpus. The new corpus promises to be
a valuable new resource for developing sys-
tems for entity/event-level sentiment analysis.
Such systems, in turn, would be valuable in
NLP applications such as Automatic Question
Answering. We introduce the idea of entity
and event targets (eTargets), describe the an-
notation scheme, and present the results of an
agreement study.

1 Introduction

Much work in sentiment analysis and opinion min-
ing is at the document level (Pang et al., 2002;
Turney, 2002). There is increasing interest in
more fine-grained levels - sentence-level (Yu and
Hatzivassiloglou, 2003; McDonald et al., 2007),
phrase-level (Choi and Cardie, 2008), aspect-level
(Hu and Liu, 2004; Titov and McDonald, 2008),
etc. We specifically address sentiments toward en-
tities and events (i.e., eTargets) expressed in data
such as blogs, newswire, and editorials. A system
that could recognize sentiments toward entities and
events would be valuable in an application such as
Automatic Question Answering, to support answer-
ing questions such as “Toward whom/what is X neg-
ative/positive?” “Who is negative/positive toward
X?” (Stoyanov et al., 2005). Or, to augment an
automatic wikification system (Ratinov et al., 2011)
– in addition to relationships such as spouse and
parents, the system could include information about

whom or what the subject supports or opposes. A re-
cent NIST evaluation – The Knowledge Base Popu-
lation (KBP) Sentiment track1 — aims at using cor-
pora to collect information regarding sentiments ex-
pressed toward or by named entities.

Annotated corpora of reviews (e.g., (Hu and Liu,
2004; Titov and McDonald, 2008)), widely used in
NLP, often include target annotations. Such targets
are often aspects or features of products or services,
and as such are somewhat limited.2

Recently, to create the Sentiment Treebank
(Socher et al., 2013), researchers crowdsourced an-
notations of movie review data and then overlaid
the annotations onto syntax trees. Thus, the tar-
gets are not limited to aspects of products/services.
However, annotators were asked to annotate small
and then increasingly larger segments of the sen-
tence. Thus, the annotations are mixed in the de-
gree to which context was considered when making
the judgements. Previously, we (Deng et al., 2013)
annotated a corpus of non-review data with senti-
ments toward entities, but only for those that partic-
ipate in certain types of events. In all of the above
corpora, the only sentiments considered are those of
the writer, excluding sentiments attributed to other
entities.

The MPQA opinion annotated corpus (Wiebe et
al., 2005; Wilson, 2007) is entirely span-based, and
contains no eTarget annotations. However, it pro-
vides an infrastructure for sentiment annotation that
is not provided by other sentiment NLP corpora, and

1http://www.nist.gov/tac/2014/KBP/Sentiment/index.html
2For example, as stated in SemEval-2014: “We annotate

only aspect terms naming particular aspects.”

1323



is much more varied in topic, genre, and publication
source. This paper addresses adding eTarget annota-
tions to the MPQA corpus; we believe that the result
will be a valuable new resource for the community.

2 From MPQA 2.0 to MPQA 3.0

To create MPQA 3.0, entity-target and event-target
(eTarget) annotations are added to the MPQA 2.0
annotations.3 The MPQA annotations consist of
private states, which are states of sources holding
attitudes toward targets. In the MPQA 2.0 anno-
tations, the top-level annotations are direct subjec-
tive (DS) and objective speech event annotations.
DS annotations are for private states, and objec-
tive speech event annotations are for objective state-
ments attributed to a source. An important property
of sources is that they are nested, reflecting the fact
that private states and speech events are often em-
bedded in one another.

As shown in Figure 1, one DS may contain links
to multiple attitude annotations, meaning that all of
the attitudes share the same nested source. The atti-
tudes differ from one another in their attitude types,
polarities, and/or targets. There are several types of
attitudes included in MPQA 2.0 (Wilson, 2007; So-
masundaran et al., 2007), including sentiment and
arguing. This work focuses on sentiments, which
are defined in (Wilson, 2007) as positive and nega-
tive evaluations, emotions, and judgements.

MPQA 2.0 also contains expressive subjective el-
ement (ESE) annotations, which pinpoint specific
expressions used to express subjectivity (Wiebe et
al., 2005). An ESE also has a nested-source an-
notation. Since we focus on sentiments, we only
consider ESEs whose polarity is positive or negative
(excluding those marked neutral).

The target-span annotations in MPQA 2.0 are
linked to from the attitudes. More than one target
may be linked to from an attitude, but most atti-
tudes have only one target. The MPQA 2.0 anno-
tators identified the main/most important target(s)
they perceive in the sentence. If there is no target,
the target-span annotation is “none”. However, there
are many other eTargets to be identified. First, while
ESE annotations have nested sources, they do not
have any target annotations. Second, there are many

3Available at http://mpqa.cs.pitt.edu

Figure 1: Structure in MPQA 3.0.

more targets that may be marked than the major ones
identified in MPQA 2.0. In Figure 1, the eTargets are
what we add in MPQA 3.0. We identify the blue (or-
ange) eTargets that are in the span of a blue (orange)
target in MPQA 2.0. We also identify the green eTar-
gets that are not in the scope of any target.

Since our priority was to add eTargets to senti-
ments, no eTargets have yet been added to objective
speech events, as shown in Figure 1.

To create MPQA 3.0, the corpus is first parsed,
and potential eTarget annotations are automatically
created from the heads of NPs and VPs. The annota-
tors then consider each sentiment attitude and each
polar ESE, and decide for each which eTargets to
add. By adding eTargets to the existing annotations,
the information in MPQA 2.0 is retained. Before
presenting the scheme, we first give some examples.

2.1 Examples

For each example, a subset of the annotations are
shown. The phrase in blue is an attitude span, the
phrase in red is a target span, the tokens in yellow are
the eTargets which are newly annotated in MPQA
3.0. The underlined phrases are ESE spans. Each
example is followed by the MPQA structure of the
annotations.

In Ex(1), a negative attitude is shown, issued the
fatwa against. The source is the Imam. The target
is the event Rushdie insulting the Prophet. How-
ever, the assertion that the Imam is negative toward
the insult event is within the scope of this article.
This is captured by an objective speech event anno-
tation (not shown) whose target span includes the in-
sult event, and whose source is the writer (w). Thus,
the complete interpretation of this negative attitude
is, according to the writer, the Imam is negative to-

1324



ward the insult event. And, the nested source is w,
imam.

Ex(1) When the Imam issued the fatwa
against Salman Rushdie for insulting
the Prophet ...

DS: issued the fatwa
nested-source: w, imam
attitude: issued the fatwa against

attitude-type: sentiment-negative
target: Salman...insult...Prophet

eTarget : Rushdie, insulting

We find two eTargets in the target-span:
“Rushdie” himself plus his act of “insulting.”

In the same sentence, there is another negative at-
titude, insulting, as shown in Ex(2). The source is
Salman Rushdie and the target is the Prophet. Note
that the span covering this event is the target span of
the attitude in Ex(1) — the private state of Ex(2) is
nested in the private state of Ex(1). Thus, the com-
plete interpretation of the negative attitude in Ex(2)
is: according to the writer, the Imam is negative
toward Rushdie insulting the Prophet. The nested
source is w, Imam, Rushdie.

Ex(2) When the Imam issued the fatwa
against Salman Rushdie for insulting the
Prophet ...

DS: insulting
nested-source: w, imam, rushdie
attitude: insulting

attitude-type: sentiment-negative
target: the Prophet

eTarget : Prophet

We add an eTarget for the Prophet, anchored to
the head “Prophet.” Interestingly, “Prophet” is an
eTarget for w,Iman,Rushdie (i.e., Rushdie is nega-
tive toward the Prophet), but not for w,Imam (i.e.,
the Imam is not negative toward the Prophet).

In the following example, the target span is short.

Ex(3) He is therefore planning to

trigger wars ...

DS: (entire sentence)
nested-source: w

attitude: planning to trigger wars
attitude-type: sentiment-negative
target: He

eTarget : He
eTarget : planning, trigger, wars

“He” is George W. Bush; this article appeared in
the early 2000s. The writer is negative toward Bush
because (the writer claims) he is planning to trig-
ger wars. As shown in the example, the MPQA 2.0
target span is only “He,” for which we do create
an eTarget. But there are three additional eTargets,
which are not included in the target span. The writer
is negative toward Bush planning to trigger wars; we
make sense of this by inferring that the writer is neg-
ative toward the idea of triggering wars and thus to-
ward war itself.

Ex(4) Three leading international organ-
isations warned jointly Thursday that
the international fight against terror-
ism should not be a pretext for the

violation of human rights.

DS: warned
nested-source: w, threeint
attitude: warned

attitude-type: sentiment-negative
target: the international ... rights

eTarget :be, pretext, violation
ESE: pretext

nested-source: w, threeint
polarity: negative
eTarget : pretext

The viewpoints in the article of Ex(4) are not
against fighting terrorism (another sentence begins
“While we recognize that the threat of terrorism re-
quires specific measures ...”) but against doing so
in certain ways. Here the three organizations are
against the fight being used as a pretext for civil
rights violations. Thus, “be”, “pretext”, and “vi-
olation” are eTargets, but “fight” and “terrorism”
are not. We mark “be” as an eTarget because the
source is negative toward the state of the fight be-
ing a pretext for the violation of human rights. This
makes sense with the source also being negative to-
ward “pretext” and “violation.” The fact that “pre-

1325



text” is identified as a negative ESE annotation in the
MPQA 2.0 supports this as well.

There is a difference between ESE and attitude
eTarget annotations. Since ESE annotations pin-
point specific wording used to express subjectivity,
ESE eTargets are annotated more narrowly than atti-
tude eTargets. For ESEs, the eTargets are the entities
and events that are directly evaluated by the expres-
sion, while, for attitudes, the eTargets include all en-
tities and events toward which the attitude holds (as
we saw in the examples above). For example:

Ex(5) ... because the hard-line wing in
the US administration comprising Vice
President Dick Cheney ...

DS: (entire sentence)
nested-source: w
attitude: hard-line

attitude-type: sentiment-negative
target: wing in the .. Dick Cheney

eTarget : wing, Cheney
ESE: hard-line wing

nested-source: w
polarity: negative
eTarget : wing

The ESE has only one eTarget, “wing,” while the
attitude has two: “wing” and “Cheney.”

2.2 MPQA 3.0 Annotation Scheme

An eTarget is an entity or event that is the target of
a sentiment (identified in MPQA 2.0 by a sentiment
attitude or polar ESE span). The eTarget annotation
is anchored to the head word of the NP or VP that
refers to the entity or event, and has three slots: id
(unique within the document), isNegated (yes or no),
and type (entity or event; note that event includes
both states and events). The isNegated = yes option
is for the case where the eTarget is the negation of
the event referred to by the head word, for example,
when the source is positive toward someone not do-
ing something.

An attitude has one or more target-span annota-
tions in MPQA 2.0. We provide two slots for the kth

target annotation. k-targetSpan shows the kth tar-
get span. k-eTarget-link is to be filled with a list of
ids of eTargets whose text anchors are within the kth

target span. An additional slot new-eTarget-link is to
be filled with a list of ids of other eTargets.

Each eTarget of an ESE has two slots, one for the
eTarget id, and one for an attribute, isReferedInSpan
(yes, or no). The value is yes if the eTarget is referred
to in the ESE span.

3 Agreement Study

We developed the manual via iterative annotation,
discussion, and revision. Once the manual was de-
veloped, we participated in an agreement study.

For the formal agreement study, one document
was randomly selected from each of the four topics
of the OPQA subset (Stoyanov et al., 2005) of the
MPQA corpus. They were not any of the documents
used to develop the manual. We then independently
annotated the four documents. There are 292 eTar-
gets in the four documents in total.

To evaluate the results, the same agreement mea-
sure is used for both attitude and ESE eTargets.
Given an attitude or ESE, let set A be the set of
eTargets annotated by annotator X , and set B be the
set of eTargets annotated by annotator Y . Following
(Wilson and Wiebe, 2003; Johansson and Moschitti,
2013), which treat each set A and B in turn as the
gold-standard, we calculate the average F-measure,
denoted agr(A, B). The agr(A, B) is 0.82 on aver-
age over the four documents, showing good agree-
ment: agr(A, B) = (|A∩B|/|B|+ |A∩B|/|A|)/2.

4 Disagreement Analysis

One issue is whether an attitude toward an entity or
event is indeed communicated in the sentence. Con-
sider this sentence: “President Mugabe’s reelection
has been praised by OAU.” The OAU is positive to-
ward “reelection,” which is an eTarget both annota-
tors mark. The question is whether it is also commu-
nicated in this sentence that the OAU is also positive
toward “Mugabe.” X did not mark Mugabe as an
eTarget, whereas Y did. During the subsequent dis-
cussion, X now agrees that it should be marked. In
general, X was using what we now consider to be
a too conservative policy. Overall, 29% of all dis-
agreements are of this type of borderline case.

8% of the disagreements arise when there are
multiple attitudes with overlapping spans, the same
source, the same polarity, but different targets and

1326



intensities4. When there are new eTargets which are
not in any target span, annotator X splits the new
eTargets into different attitudes based on the inten-
sity, while annotator Y adds the new eTargets to all
the attitudes regardless of intensity. Later the an-
notators discuss to decide which attitude each new
eTarget should be linked to in the final version.

31% of the disagreements are caused by negli-
gence, meaning an annotator realized, during later
discussion, that she should have included an eTarget
when she saw that the other annotator had included
it.

The remaining disagreements are due to annotator
mistakes such as filling in the wrong id.

5 Current Corpus

The current corpus consists of 70 documents, in-
cluding the subset of the documents in MPQA 2.0
that come from English-language sources (i.e., that
are not translations) and a subset of the OPQA sub-
set in MPQA 2.0. A subset contains consensus an-
notations of X and Y and the rest were annotated
by Y . The 70 documents have 1,029 ESEs, 1,287
attitudes, and 1,213 target spans of attitudes (exclud-
ing the target span that are marked as “none”) from
MPQA 2.0; they have 4,459 eTargets in total. We
added 1,366 eTargets to the ESEs and 1,608 eTar-
gets to the target spans. We added 1,485 eTargets
which are not in any target span.

6 An Example

In this section, we present an example from the
OPQA subset (Stoyanov et al., 2005) to demonstrate
how eTargets could help to automatically answer a
question. There are opinion and fact questions for
each document in the OPQA subset. The sentence
below is annotated in MPQA 2.0 to answer the ques-
tion, “Is the US Annual Human Rights Report re-
ceived with universal approval around the world?”
Here the writer is negative toward the report.

It is due to this hegemony, which the
United States wants to maintain, that its
State Department makes an assessment
of the human rights situation in different

4In MPQA 2.0, an attitude is marked with an intensity (low,
medium, or high) representing the intensity.

countries and prepares a report on their
violations all over the world.

The annotations in MPQA 2.0:
S1: 〈writer-US, positive, hegemony〉
S2: 〈writer, negative, the United States〉
ESE1: 〈writer, negative, N/A〉

First, it is possible for a state-of-the-art system
to be trained to recognize the sentiment S1, by the
maintaining phrase and syntax information. But it
would be difficult to find S2. There is no direct sen-
timent modifying the US, nor is there any sentiment
or ESE annotation toward maintain or hegemony in
MPQA 2.0. Now, in MPQA 3.0, we add the eTarget
of the ESE1, so that it becomes 〈writer, negative,
hegemony〉. This is a critical step, because the com-
plete ESE bridges the two sentiments together.

Second, even though we have the two sentiments
and the ESE, there is still a gap between the United
States in the sentence and report in the question.
One of the eTargets we add is “report.” It is more
feasible for a co-reference system to recognize re-
port in both the sentence and the question as the
same thing, than recognizing that the United States
and report refer to the same concept.

Third, in this sentence, according to the newly
added eTargets, the system knows the writer is nega-
tive toward both the United States and State Depart-
ment. When building a knowledge base about the
human rights report, this reveals that the two entities
have the same stance toward this topic, even without
any world knowledge.

7 Conclusion

This paper presents an annotation scheme for adding
entity and event target annotations to the MPQA cor-
pus. A subset of MPQA has already been annotated
according to the new scheme. We believe that the
corpus will be a valuable new resource for develop-
ing entity/event-level sentiment analysis systems to
facilitate NLP applications such as Automatic Ques-
tion Answering.

Acknowledgements. This work was supported
in part by DARPA-BAA-12-47 DEFT grant
#12475008. We thank the anonymous reviewers for
their helpful comments.

1327



References
Yejin Choi and Claire Cardie. 2008. Learning with com-

positional semantics as structural inference for subsen-
tential sentiment analysis. In Proceedings of the Con-
ference on Empirical Methods in Natural Language
Processing, pages 793–801. Association for Compu-
tational Linguistics.

Lingjia Deng, Yoonjung Choi, and Janyce Wiebe. 2013.
Benefactive/malefactive event and writer attitude an-
notation. In ACL (2), pages 120–125.

Minqing Hu and Bing Liu. 2004. Mining and summa-
rizing customer reviews. In Proceedings of the tenth
ACM SIGKDD international conference on Knowl-
edge discovery and data mining, pages 168–177.
ACM.

Richard Johansson and Alessandro Moschitti. 2013.
Relational features in fine-grained opinion analysis.
Computational Linguistics, 39(3):473–509.

Ryan McDonald, Kerry Hannan, Tyler Neylon, Mike
Wells, and Jeff Reynar. 2007. Structured mod-
els for fine-to-coarse sentiment analysis. In Annual
Meeting-Association For Computational Linguistics,
volume 45, page 432. Citeseer.

Bo Pang, Lillian Lee, and Shivakumar Vaithyanathan.
2002. Thumbs up?: sentiment classification using ma-
chine learning techniques. In Proceedings of the ACL-
02 conference on Empirical methods in natural lan-
guage processing-Volume 10, pages 79–86. Associa-
tion for Computational Linguistics.

Lev Ratinov, Dan Roth, Doug Downey, and Mike An-
derson. 2011. Local and global algorithms for dis-
ambiguation to wikipedia. In Proceedings of the 49th
Annual Meeting of the Association for Computational
Linguistics: Human Language Technologies-Volume
1, pages 1375–1384. Association for Computational
Linguistics.

Richard Socher, Alex Perelygin, Jean Y Wu, Jason
Chuang, Christopher D Manning, Andrew Y Ng, and
Christopher Potts. 2013. Recursive deep models for
semantic compositionality over a sentiment treebank.
In Proceedings of the Conference on Empirical Meth-
ods in Natural Language Processing (EMNLP), pages
1631–1642. Citeseer.

Swapna Somasundaran, Theresa Wilson, Janyce Wiebe,
and Veselin Stoyanov. 2007. Qa with attitude: Ex-
ploiting opinion type analysis for improving question
answering in on-line discussions and the news. In In-
ternational Conference on Weblogs and Social Media,
Boulder, CO.

Veselin Stoyanov, Claire Cardie, and Janyce Wiebe.
2005. Multi-Perspective Question Answering us-
ing the OpQA corpus. In Proceedings of the Hu-
man Language Technologies Conference/Conference

on Empirical Methods in Natural Language Process-
ing (HLT/EMNLP-2005), pages 923–930, Vancouver,
Canada.

Ivan Titov and Ryan T McDonald. 2008. A joint model
of text and aspect ratings for sentiment summarization.
In ACL, volume 8, pages 308–316. Citeseer.

Peter D Turney. 2002. Thumbs up or thumbs down?:
semantic orientation applied to unsupervised classifi-
cation of reviews. In Proceedings of the 40th annual
meeting on association for computational linguistics,
pages 417–424. Association for Computational Lin-
guistics.

Janyce Wiebe, Theresa Wilson, and Claire Cardie. 2005.
Annotating expressions of opinions and emotions in
language ann. Language Resources and Evaluation,
39(2/3):164–210.

Theresa Wilson and Janyce Wiebe. 2003. Annotating
opinions in the world press. In Proceedings of the 4th
ACL SIGdial Workshop on Discourse and Dialogue
(SIGdial-03), pages 13–22.

Theresa Wilson. 2007. Fine-grained Subjectivity and
Sentiment Analysis: Recognizing the Intensity, Polar-
ity, and Attitudes of private states. Ph.D. thesis, Intel-
ligent Systems Program, University of Pittsburgh.

Hong Yu and Vasileios Hatzivassiloglou. 2003. Towards
answering opinion questions: Separating facts from
opinions and identifying the polarity of opinion sen-
tences. In Proceedings of the 2003 conference on Em-
pirical methods in natural language processing, pages
129–136. Association for Computational Linguistics.

1328


