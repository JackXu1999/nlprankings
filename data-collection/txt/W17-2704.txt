Proceedings of the Events and Stories in the News Workshop, pages 21–30,

Vancouver, Canada, August 4, 2017. c(cid:13)2017 Association for Computational Linguistics

21

Improving Shared Argument Identiﬁcation in Japanese Event Relation

Knowledge Acquisition

Graduate School of Informatics

Graduate School of Informatics

Yin Jou Huang

Kyoto University

Sadao Kurohashi

Kyoto University

huang@nlp.ist.i.kyoto-u.ac.jp

kuro@i.kyoto-u.ac.jp

Abstract

Event relation knowledge represents the
knowledge of causal and temporal rela-
tions between events. Shared arguments
of event relation knowledge encode pat-
terns of role shifting in successive events.
A two-stage framework was proposed for
the task of Japanese event relation knowl-
edge acquisition, in which related event
pairs are ﬁrst extracted, and shared argu-
ments are then identiﬁed to form the com-
plete event relation knowledge. This paper
focuses on the second stage of this frame-
work, and proposes a method to improve
the shared argument identiﬁcation of re-
lated event pairs. We constructed a gold
dataset for shared argument learning. By
evaluating our system on this gold dataset,
we found that our proposed model out-
performed the baseline models by a large
margin.

1 Introduction
Natural language understanding requires not only
linguistic knowledge but also common knowledge
about the real world. Event relation knowledge
is a type of common knowledge of critical im-
portance, representing the knowledge of the rela-
tion between events as well as the typical patterns
of role shifting between events. Event relation
knowledge is useful for natural language under-
standing tasks as well as natural language genera-
tion tasks which require modeling of the possible
event sequences.

In this paper, we deﬁne an event to be a predi-
cate argument structure (PAS), which consists of a

1In this paper we adopt the Japanese case marker, ga, wo,
ni, and de, which roughly corresponds to nominative, ac-
cusative, dative, and instrumental/locative cases.

Figure 1: Event relation knowledge with shared
arguments.1
predicate and its relevant arguments. In addition,
we deﬁne one unit of event relation knowledge to
be a pair of successive events with one or more
shared arguments. Figure 1 represents an example
of event relation knowledge, which consists of two
events, pas1 and pas2.

The shared arguments correspond to the com-
mon participants of the two events, such as A1 and
A3 in the above example. These shared arguments
play an important role in the application of event
relation knowledge since they encode the corre-
spondence relations between case slots within a
piece of event relation knowledge.

In this paper, we aim to improve the shared ar-
gument identiﬁcation in Japanese event relation
knowledge. Event relation knowledge acquisition
in Japanese is a much more challenging task than
its counterpart of English, due to several linguistic
properties of Japanese. For example:
(1)

a. John attached a stamp to the letter, and

he dropped it into the mailbox.

b. John attached a stamp to the letter, and
(ϕhe) dropped (ϕletter) into the mailbox.
In the above example, (1-b) is the Japanese cor-
respondence of (1-a), directly translated into En-
glish. We can observe that Japanese has an abun-
dance of omitted arguments. In addition, Japanese
lacks linguistic clues regarding the accordance in
gender, number, etc., such as ‘he’ and ‘it’ in (1-a).
These linguistic properties hinder the perfor-
mance of Japanese coreference resolution sys-

22

Figure 2: Two-stage approach for Japanese event relation knowledge acquisition.

tems, and make it unsuitable to apply coreference-
based methods of English event relation knowl-
edge acquisition (Chambers and Jurafsky, 2008)
directly to Japanese.

On the other hand, event relation knowledge can
beneﬁt the task of the coreference resolution. The
shared arguments within an event relation knowl-
edge provide direct clues that the case slots shar-
ing an argument should hold co-referring argu-
ments. These clues are particularly critical in cases
in which selectional preference is not helpful,
such as coreference resolution problems presented
in Winograd Schema Challenge (Levesque et al.,
2012; Rahman and Ng, 2012). Consider the fol-
lowing example:
(2)

a. グーグルが モトローラ を買収した。

彼ら が破綻したからだ。
(Google-ga acquired Motorola-wo, be-
cause they-ga went bankrupt.)
b. A1-ga go bankrupt → A2-ga A1-wo ac-

quire

In the example of (2-a), both precedents of ‘they’,
‘Google’ and ‘Motorola’, are of the same cate-
gory. While selectional preference is not helpful
in this case, the event relation knowledge in (2-b)
can help us resolve (2-a) correctly.

In this work, we adopted the two-stage frame-
work for Japanese event relation knowledge acqui-
sition (Shibata and Kurohashi, 2011). In the ﬁrst
stage of related event pair extraction, we adopted
the method proposed by Shibata and Kurohashi
(2011); and in the second stage of shared ar-
gument identiﬁcation, we extended the model of
Kohama et al. (2015) to incorporate all types of

shared arguments in our gold dataset. We designed
a richer feature representation for shared argument
learning, which considers the interaction between
shared arguments and the mechanism of argument
omission in depth.

In addition, we manually constructed a gold
dataset for shared argument learning. With the
help of linguistic experts, we established an an-
notation scheme for shared argument. We classi-
ﬁed the shared arguments into three types: stan-
dard shared argument, quasi shared argument,
and multiple shared argument. We evaluated our
method of shared argument identiﬁcation on the
gold dataset. By comparing our proposed methods
with several baseline models, we observed a sig-
niﬁcant improvement for shared argument identi-
ﬁcation.

2 Related Work

As a resource-rich language, coreference resolu-
tion of English has achieved a satisfying perfor-
mance. Thus, several works which utilize corefer-
ence information were proposed for English event
relation knowledge acquisition.

Chambers and Jurafsky (2008) introduced the
concept of narrative event chains as a representa-
tion of structured event relation knowledge. Their
method utilizes the coreference chains within the
input text to collect events involving the same en-
tity, which they called the protagonist. Among the
set of events involving the same entity, event se-
quences that are observed a signiﬁcant number of
times are extracted as typical event sequences.

23

pas1

pas2

Support sentences

切手を貼る

(stamp-wo paste) →

ポストに入れる
(mailbox-ni put)

薬を飲む

(medicine-wo take) →

症状が軽くなる

(symptom-ga alleviate)

親宛に書いた葉書きに切手を貼ってポストに入れた。

(I pasted a stamp on the postcard to my parents, and put it into the mailbox.)

手紙を書いて、封をして、切手を貼って、ポストに入れる。
(I write a letter, seal it, paste a stamp, and put it into the mailbox.)

薬を飲み続けていると、アレルギーの症状は大分軽くなってきている。

(Taking the medicine alleviates the allergy symptom signiﬁcantly.)
抗ヒスタミン系の薬を処方され、飲めば症状は軽くなります。
(I was prescribed antihistamine, the symptom alleviated after taking it.)

Table 1: Related Event Pairs.

Pichotta and Mooney (2014) used a richer rep-
resentation of event than in the work of Cham-
bers et al. and achieved an improvement in pre-
dicting performance.
Instead of representing an
event as a (predicate, dependency) pair, they con-
sidered an event as a structure of a predicate and
arguments with subject, object, direct object rela-
tions with the predicate. With this multi-argument
event representation, their model performs better
in the cases of ambiguous verbs, and is more ca-
pable of capturing complex interactions between
multiple entities.

There are several works proposed for Japanese
event relation knowledge acquisition utilizing the
co-occurrences of events. Abe et al. (2008) pro-
posed a pattern-based method which utilized a pre-
deﬁned set of lexico-syntactic co-occurrence pat-
terns to perform bootstrapping for event relation
learning. Their work focused on the acquisition of
related event pairs, but not the relations between
the arguments of the related events.

Shibata and Kurohashi (2011) proposed a two-
stage approach for Japanese event relation knowl-
edge acquisition (Figure 2).
In the ﬁrst stage,
related event pairs are extracted from large-scale
corpora by association rule mining.
In the sec-
ond stage, shared arguments of the event pairs are
identiﬁed heuristically based on case slot similar-
ity scores.

Kohama et al. (2015) improved the work of
Shibata and Kurohashi (2011) by utilizing crowd-
sourced data for shared argument learning. They
proposed a joint model that simultaneously pre-
dicts the shared argument conﬁguration and dis-
ambiguates the meaning of the predicates. How-
ever, their work failed to identify the shared argu-
ments accurately for two reasons. First, the crowd-
sourced data they used is very noisy and lacks a
well-deﬁned standard of labeling. Second, the fea-
tures used in their model are not sufﬁcient for cap-
turing the characteristics of shared arguments.

3 Shared Argument Identiﬁcation

In this section, we introduce our method of shared
argument identiﬁcation.
In Section 3.1, we ﬁrst
introduce the acquisition of related event pairs,
which are the inputs to our shared argument identi-
ﬁcation model. We introduce the gold dataset used
for model learning in Section 3.2. In Section 3.3,
we describe the selection of case frames. These
case frames will be used to model different mean-
ings of predicates in our model. The remaining of
the section will be dedicated to the description of
our proposed methods of shared argument identi-
ﬁcation.

3.1 Related Event Pairs

Our work is based on the two-stage frame-
work of event relation knowledge proposed by
Shibata and Kurohashi (2011). We adopt the ﬁrst
stage of related event pair extraction proposed in
their work to obtain the related event pairs, which
will be the input to our shared argument identiﬁ-
cation model.

Here, we brieﬂy describe the ﬁrst stage of re-
lated event pair extraction. Starting from the web
corpus, we ﬁrst extract the PAS pairs with syntac-
tic dependency, and use the Apriori algorithm to
pick out the related event pairs efﬁciently (Figure
2). In order to improve the quality of the extracted
event pairs, we apply an additional ﬁltering step
based on the clause relations between event pairs
as suggested in Kohama et al. (2015).

Table 1 shows several examples of related event
pairs extracted in this process. Each event pair R
consists of two PASs, pas1 and pas2, and the sen-
tences containing both pas1 and pas2 are regarded
as the support sentences of R. These support sen-
tences contain many valuable clues for the task of
shared argument identiﬁcation. Thus, the event
pair R along with its support sentences will serve
as the input to our shared argument identiﬁcation
model.

24

Type

Event Pair

Shared Argument

Standard 切手を 手紙に 貼る → 手紙を ポストに入れる
(stamp-wo letter-ni paste) (letter-wo mailbox-ni put)

Quasi

Multiple

牛を 飼う→ 牛乳で チーズを作る

(cow-wo raise) (milk-de cheese-wo make)
観光客が 町を/に 訪れる→ 町が 賑わう

(tourist-ga town-wo/ni visit) (town-ga be crowded)
Table 2: Types of Shared Arguments.

n-w

w-d’

w/n-g

Type

Standard

Quasi
Multiple

Shared Argument

n-w
w-d’
n-n/w

Standard Shared Argument Set

{n-w}
{w-d, ϕ}
{n-n, n-w}

Table 3: Transforming different types of shared arguments to their standard shared argument sets.

3.2 Gold Dataset
We manually constructed a gold dataset for learn-
ing shared argument identiﬁcation model. In this
work, we train and evaluate our proposed model
on this gold dataset.

This dataset contains 809 related event pairs,
with each of the event pair annotated with its
shared argument conﬁguration. Three annotators
with linguistic background participated in the con-
struction of this dataset.
Type of Shared Arguments
The gold dataset contains the following types of
shared arguments (Table 2):

1. Standard Shared Argument:

The arguments shared between one case slot
of the ﬁrst event and another case slot of the
second event. This type of shared argument
represents the fact that arguments of the two
cases should correspond to an identical real
world entity.
In this work, we only consider the four main
cases of ga (が), wo (を), ni (に), and de
(で). From now on, we use the shorthand no-
tation of g, w, n, and d to represent these four
main cases. The ﬁrst example in Table 2 has
a standard shared argument between the ﬁrst
ni-case and the second wo-case, which both
correspond to the entity ‘letter’. we use the
notation n-w to represent it.
2. Quasi Shared Argument:

Quasi shared argument is a pair of arguments
which are closely related to each other in the
context of the given event relation knowl-
edge. As can be seen from the example in
Table 2, the arguments of the ﬁrst wo-case

and the second de-case are ‘cow’ and ‘milk’,
respectively. These two arguments are con-
sidered to be closely related since the milk in
the context corresponds to the speciﬁc milk
which is produced by the cow in the same
context.
We attached an apostrophe (’) to denote a
quasi shared argument.

3. Multiple Shared Argument:

Multiple shared argument occurs when more
than two case slots share the same argument.
As can be seen from the example in Table 2,
the argument ‘town’ is shared between three
cases: wo-case or ni-case of the ﬁrst event,
and the ga-case of the second event.
We use the symbol ‘/’ to separate different
case slots of the same predicate which share
arguments.

Preprocessing of Gold dataset
In this work, we only focus on the identiﬁcation of
standard shared arguments. For utilizing the gold
dataset with other shared argument types, we per-
form a pre-processing to the gold annotation be-
fore model training. We transform each shared ar-
gument conﬁguration into its corresponding stan-
dard conﬁguration set.

First, we deﬁne the corresponding standard
shared argument set for each shared argument in
the following manner (Table 3):

1. For each standard shared argument, we trans-
form it into the standard shared argument set
containing only itself.

2. For each quasi shared argument, we trans-
form it into the standard shared argument set
containing a null shared argument (ϕ) and its

25

Shared Argument Conﬁguration

[g-g]

[g-g w-d’]
[g-g n-n/w]

[g-g w-d’ n-n/w]

Standard Conﬁguration Set

{[g-g]}

{[g-g, w-d], [g-g]}

{[g-g, n-n], [g-g, n-w]}

{[g-g, n-n], [g-g, n-w], [g-g, n-n, w-d], [g-g, n-w, w-d]}

Table 4: Transforming shared argument conﬁguration to corresponding standard conﬁguration set.

standard counterpart in which all the apostro-
phe (’) mark is removed. See the second ex-
ample in Table 3.

3. For each multiple shared argument, we trans-
form it into the standard shared argument
set containing all the shared arguments that
could be entailed from it. See the third exam-
ple in Table 3.

For a given shared argument conﬁguration, we
ﬁrst transform each of its containing shared argu-
ment into its corresponding standard shared argu-
ment set in the above manner. By taking the prod-
uct of these standard shared argument sets, we ob-
tain the corresponding standard conﬁguration set
of the shared argument conﬁguration. See Table 4
for examples.

3.3 Case Frame Selection
Selectional preferences provide important clues
for the task of share argument identiﬁcation. Case
frames are good sources of selectional preference
information, and it handles the issue of predicate
ambiguity by clustering the usage of each predi-
cate by their meanings. In turn, the meaning of a
case frame is represented by the argument distri-
bution in each case slot of its corresponding case
frame.

In this work, we consider wide-coverage case
frames constructed automatically from a huge raw
corpus as the source of selectional preference in-
formation (Hayashibe et al., 2015). For each event
pair R(pas1→ pas2), we select 10 relevant case
frames for both pas1 and pas2 by utilizing the sup-
porting sentences S of R. Here, we describe the
method for selecting relevant case frames for each
event pair, which are used in our proposed models.
Given a case frame cf, we denote the bag-of-
words (BoW) representation of arguments within
each case slot of cf as follows:

V g, V w, V n, V d

We denote the BoW representation of arguments
appearing in the corresponding case slots of the

We deﬁne the relevance score of cf with respect

support sentences S as follows:

U g, U w, U n, U d

to R as follows:

rel(cf, R) =

∑

x={g,w,n,d}

cos(U x, V x)

(1)

which is the sum of cosine similarity scores be-
tween the BoW representation of case slots in the
four main cases.

Finally, we rank all the case frames in descend-
ing order with respect to relevance score and take
the top 10 of them as relevant case frames. Table
5 represents the ﬁrst ﬁve relevant case frames of
the predicate 訪れる (visit) of the following event
pair:

観光客が訪れる→賑わう

(tourist-ga visit → be crowded)

3.4 Joint Prediction of Shared Argument and

Case Frame

As mentioned in Section 3.3, case frames provide
important information of selectional preferences.
However, the gold data does not provide the ap-
propriate case frame of each predicate. To tackle
this problem, we propose a model of shared ar-
gument identiﬁcation that simultaneously predicts
the appropriate case frame for each predicate.
Model
We adopt a maximum entropy (MaxEnt) classiﬁer
model.
Given a related event pair R(pas1 → pas2) and
its supporting sentences S, the conditional prob-
ability of a shared argument conﬁguration A and
case frame pair cf1, cf2 is modeled as:
P (A, cf1, cf2|R, S; w) = exp{w·ϕ(A,cf1,cf2,R,S)}
(2)
In the above equation, ϕ(A, cf1, cf2, R, S) is the
feature representation of the shared argument con-
ﬁguration, w is the model parameter, and Z is the
normalization constant. In Table 6 we summarized
the features used, under the example of shared ar-
gument n-w.

Z

26

Rank Case Frame

Relevance Score

1

2

3

4

5

[観光客, 人] が [地, 日本] を [実際] に 訪れる
[tourist, person]-ga [place, Japan]-wo [practically]-ni visit
[数人, 人] が [事務所, 京都] を [激励, 視察] に 訪れる
[people, person]-ga [ofﬁce, Kyoto]-wo [encourage,inspection]-ni visit
[観光客, 大統領] が [中国, 台湾] を [視察, 見学] に 訪れる
[tourist, president]-ga [China, Taiwan]-wo [inspection, ﬁeld trip]-ni visit
[客, 観光客] が [店, ショップ] を [目当, 実際] に 訪れる
[guest, tourist]-ga [store, shop]-wo [goal, practically]-ni visit
[人, 観光客] が [博物館, 美術館] を [見学] に 訪れる
[person, tourist]-ga [museum, art museum]-wo [ﬁeld trip]-ni visit

0.966

0.807

0.760

0.748

0.742

Feature
Conﬁguration
Post-predicate
Core

Case slot similarity

Normalized case slot similarity

Conﬂict

Context

Table 5: Relevant case frames of 訪れる (visit).
Description
Binary feature indicating the existence of the shared argument n-w.
Binary feature indicating the existence of argument in w case of pas2.
Binary features indicating if n case of cf1 and w case of cf2 are core cases. If a case slot
takes argument in more than 10% of the time in the selected case frame, we deﬁne it as a
core case.
The cosine similarity between the vocabulary distribution of n case of cf1 and w case of
cf2.
Case slot similarity of n-w normalized over the similarities of all case slots of cf1. Same
for cf2.
The ratio of support sentences in S that holds different arguments in the ﬁrst n case and
the second w case.
We collect words that appear in S but not within the event pair as context words. We
calculate the relative probability of each context word to appear in the ﬁrst n case com-
pared to other main cases, and similar for the second w case. A tf-idf weighted sum of
this probability is added as feature.
Table 6: Features for shared argument n-w.

Prediction
During the prediction phase, the shared argument
conﬁguration ˆA and case frame pair ˆcf1, ˆcf2 that
gives the highest probability is chosen:

(ˆA, ˆcf1, ˆcf2) = argmax
A,cf1,cf2

P (A, cf1, cf2|R, S; w)
(3)
For each related event pair R, we choose 10 rele-
vant case frames for each predicate of concern as
candidate of cf1 and cf2, as described in Section
3.3.

Model Training
In the training phase, the most probable case frame
pair ( ˆcf1, ˆcf2) and the model parameter w are up-
dated alternatively. Also, the most probable gold
conﬁguration ˆg among the standard conﬁguration
set is also updated along with the case frame pair.

The training algorithm is summarized below:

1. Initialize model parameter w randomly.

2. Use the current parameter w to update the
most probable gold conﬁguration and the
most probable case frame pair (ˆg, ˆcf1, ˆcf2):

N∑

ˆg, ˆcf1, ˆcf2 = argmax
g,cf1,cf2

P (g, cf1, cf2|R, S; w)
(4)

3. Use (ˆg, ˆcf1, ˆcf2) to update model parameter
w. The following is the objective function,
in which the superscripts of g, cf1, and cf2
denote the id of the event pairs, and N is the
total number of training objects:

L =

logP (g(n), cf (n)

1

, cf (n)

2

|R, S; w)

n=1

− α∥w∥2

ˆw = argmax

L

w

(Hyper-parameter α is set to 1.0.)

(5)

(6)

4. Back to 2 until convergence. The conver-
gence condition is that the most probable
(ˆg, ˆcf1, ˆcf2) for all event pairs are the same
as the previous iteration. If the convergence
condition is not satisﬁed after 15 iterations,
we terminate the training process.

27

3.5 Shared Argument Learning with

Combined Case Frame

Here, we introduce another model for learning
shared arguments which uses the combined case
frames.

The joint reference model (Section 3.4) picks
exactly one case frame for each predicate. On
the other hand, the combined case frame model
combines the relevant case frames by taking the
weighed sum of them by the relevance scores with
respect to the event pair. This method does not de-
cide the most appropriate case frame of each pred-
icate. Instead, all of the relevant case frames are
considered, and case frames with higher relevance
scores have larger inﬂuence on the feature repre-
sentation.

Combined Case Frame
A combined case frame is obtained by combin-
ing the relevant case frames according to their rel-
evance scores. The calculation of the relevance
scores of each case frame is described in Section
3.3.

ﬁned the combined case framefcf as follows:

Given a set of relevant case frames CF , we de-

fcf : fV g, fV w,fV n,fV d

(7)

∑

fV x =

cf∈CF
in which V x
of cf.

rel(cf, R) × V x

cf , ∀x ∈ {g, w, n, d}

(8)

cf is the vocabulary distribution vector

Model
Similar to the joint prediction model presented in
section 3.4, we adopt a MaxEnt classiﬁer model.
Given an event pair R(pas1→ pas2) and its sup-
porting sentences S, we model the conditional
probability of shared argument conﬁguration A as:

P (A|R, S; w) = exp{w · ϕ(A,fcf1,fcf2, R, S)}

(9)
In the above equation, ϕ is the feature represen-
tation as summarized in Table 6, w is the model
parameter, and Z is the normalization constant.

The training algorithm is similar to the one de-
scribed in Section 3.4. In the training phase, the
most probable gold conﬁguration ˆg and the model
parameter w are updated alternatively until con-
vergence.

Z

4 Experiments
4.1 Settings
The case frames used in the experiments are built
from a web corpus of four billion sentences, with
the method proposed by Hayashibe et al. (2015).
We use Classias (Okazaki, 2009) as the imple-
mentation of maximum entropy classiﬁer and L-
BFGS (Nocedal, 1980) as the optimization algo-
rithm for learning. We train and evaluate our pro-
posed models by a 5-fold cross-validation test on
the gold shared argument dataset.

4.2 Evaluation and Result
We apply three evaluation metrics: precision, re-
call, and F-score (F1) for the evaluation of our
shared argument identiﬁcation models.

Baseline[Kohama+15]

Model

Baseline[g-g]

Precision Recall
0.717
0.733
0.786
0.748
Table 7: Evaluation result.

0.731
0.729
0.747
0.753

Joint

Combined

F1
0.724
0.731
0.766
0.750

We compared our proposed models with two
baseline models. The ﬁrst baseline model, denoted
as Baseline[g-g] in Table 7, is the majority classi-
ﬁer which gives the output of g-g regardless of the
event pair given. The second baseline model, de-
noted as Baseline[Kohama+15], is the model pro-
posed by Kohama et al. (2015).

The experiment results are summarized in Table
In addition, several event relation knowledge

7.
acquired are shown in Table 8.

proposed

Compared

the model

4.3 Discussion
Comparison with Baseline Models
As can be observed from Table 7, both of our pro-
posed models outperformed the baseline models
by a large margin.
to

by
Kohama et al. (2015), we use a richer feature
representation for shared argument conﬁguration.
In their work, a shared argument is represented
by the vocabulary distribution similarity between
two case slots, such as the similarity between case
frames, or the similarity between arguments in the
supporting sentences. However, by considering
only the distributional similarities between two
case slots, their method overlooked two impor-
tant intrinsic properties of the shared argument
identiﬁcation task:

28

Event Pair

Gold Annotation System Output Error Type

熟成させる→出荷される

(ripen) (ship)

ジュースが安くなる→買う

(juice-ga become cheaper) (buy)

肌に与える→若返らせる
(skin-ni give) (rejuvenate)

切手を貼る→ポストに入れる

(stamp-wo paste) (mailbox-ni put)

迫害される→殺される

(suffer persecution) (be killed)
明るくなる→太陽が顔を出す

(become brighter) (sun-ga face-wo appear)

w-g

g-w

w-g

g-w

w/g-g n-w

g-g n-w

g-g n-w

g-g n-n

ϕ

g-g

n-g

g-w

-

-

-

1

2

3

Table 8: Evaluation results of the proposed and baseline models for shared argument identiﬁcation.

(I, who, people, mom, self, mother, ..)

Case Arguments
ga 私, 誰, 人, ママ, 夫, 自分, 母, ..
wo 茶, 私, 子供, 花, 模様, .., 手紙, .., 封筒, ..
ni 中, 風呂, 部屋, 手, 家, ポスト, ..

(tea, I, child, ﬂower, pattern, .., letter, .., envelope, ..)

(interior, bathroom, room, hand, house, mailbox)
⟨ 数量 ⟩+人, ⟨ 時間 ⟩, 急須, 白, 湯, 鉛筆, ..
(⟨number⟩+people, ⟨time⟩, teapot, white, hot water, pencil)

de

Table 9: Example of bad case frame

1. Interaction of shared arguments:

Different pieces of shared arguments are not
independent, and shared arguments that share
a case slot have repulsive effects on each
other. For example, if a shared argument con-
ﬁguration already includes g-g, then it would
be unlikely that g-w also exists in the same
conﬁguration. We add the normalized case
slot similarity feature which considers not
only the case slot similarity of a pair of case
slots, but also the relative similarity of them,
to account for this property.

2. The mechanism of argument omission in

related event pairs:
High
similarity
vocabulary
indicates the existence of shared arguments,
but not vice versa. Consider the following
example:

distribution

ジュースが 安くなる→ ジュースを 買う
(juice-ga become cheaper → juice-wo buy)
Although there exists a shared argument of
g-w, the vocabulary distributions of the two
corresponding case slots are quite different.
To address this property, we add the context
feature which considers each context word
and the relative probability of them to appear
in each of the main case slots.

Comparison Between Proposed Models
The major difference between the two proposed
models lies in how case frames for feature con-
struction are decided.

As can be observed from Table 7, the joint pre-
diction model achieved a better F-score than the
combined case frame model. We conclude that
deciding one best case frame is a better way for
modeling the selectional preference of a predicate,
compared to combining case frames with respect
to the relevance scores. The result also veriﬁed
the effectiveness of the joint model of case frames
and shared arguments.

Error Analysis
In the following are several patterns of error ob-
served in the system output. Examples of each er-
ror type are presented in Table 8.

1. Error due to case frame granularity (Er-

ror Type 1):
Our proposed model jointly predicts the most
appropriate case frame along with the shared
argument conﬁguration. By selecting a single
case frame for each predicate, we are able to
model the selectional preference of the predi-
cates accurately. However, the automatically
constructed case frames do not always pro-
vide the granularity suitable for our task. If
a coarse-grained case frame is selected dur-

29

ing prediction phase, the prediction of shared
argument will also be affected.
For the example shown in Table 8, an ap-
propriate case frame of the second predicate
‘put’ should contain words that supports n-
w shared argument in the wo-case. Table 9
represents the most appropriate case frame of
the predicate ‘put’ among all the case frames
of this predicate. It can be observed that al-
though the wo-case contains words relevant
to the n-w shared argument, such as ‘let-
ter’ and ‘envelope’, there are other irrelevant
words dominating this case. These kind of
broad, somewhat noisy case frames hinder
the performance of our shared argument iden-
tiﬁcation model.

2. Error due to event participants with simi-

lar characteristics (Error Type 2):
Our method relies largely on selectional pref-
erence information for identifying shared ar-
guments. Thus, the prediction performance
of our system is not very good for event pairs
containing multiple participants with similar
characteristics.
For the example shown in Table 8, our model
wrongly identiﬁed the shared argument n-g.
Although both cases are expected to hold hu-
man participants, the entity in the ﬁrst ni-case
should correspond to the victim of both ac-
tions ‘persecute’ and ‘kill’, while the second
ga-case should hold the entity of the perpetra-
tor of the two actions. In the scenario of the
above event pair, there are two participants
of similar characteristics, which are both ex-
pected to be human. Since selectional prefer-
ence cannot effectively distinguish between
these similar participants, our model often
has difﬁculty dealing with event pairs with
multiple similar participants.

3. Error due to ﬁxed expression (Error Type

3)
In a ﬁxed expression, an argument often
takes on a different meaning than it usually
does. Fixed expressions within events some-
times cause problems in shared argument
identiﬁcation. For the example shown in
Table 8, the system output is as follows:

顔が 明るくなる→太陽が 顔を 出す

(face-ga become brighter → sun-ga face-wo appear)
Independently, both PASs shown above are
plausible. However, the ﬁrst PAS, ‘face-ga

become brighter’, means showing a cheerful
look; while the second PAS, ‘sun-ga face-wo
appear’, means sun rising. Although both
expression contains the argument ‘face’, the
shared argument of g-w does not exist.

5 Conclusion

This paper proposed a method for shared argument
identiﬁcation in event relation knowledge acquisi-
tion. By addressing several problems of the pre-
vious works, we improved the shared argument
identiﬁcation model signiﬁcantly. We proposed a
richer feature representation of shared argument
conﬁguration which is more suitable for model
learning.
In order to incorporate different types
of shared argument in the gold dataset, we up-
date the most appropriate gold conﬁguration along
with case frames during the training process. We
evaluated our model on a manually annotated gold
dataset, and our model outperformed the baseline
models by a large margin.

Our proposed model jointly predicts the shared
argument conﬁguration and the appropriate case
frames. By comparing the result of our proposed
model with the combined case frame model, we
veriﬁed the effectiveness of this joint model to pre-
dict the appropriate case frames.

References
Shuya Abe, Kentaro Inui, and Yuji Matsumoto. 2008.
Acquiring event relation knowledge by learning
cooccurrence patterns and fertilizing cooccurrence
samples. Proceedings of the 3rd International Joint
Conference on Natural Language Processing pages
479–504.

Nathanael Chambers and Dan Jurafsky. 2008. Unsu-
pervised learning of narrative event chains. In Pro-
ceedings of the 46th Annual Meeting of the Associ-
ation of Computational Linguistics. Association for
Computational Linguistics, Columbus, Ohio, pages
789–797.

Yuta Hayashibe, Daisuke Kawahara, and Sadao Kuro-
hashi. 2015. Japanese case frame construction con-
sidering varieties of case patterns. Technical report.

Shotaro Kohama, Tomohide Shibata, and Sadao Kuro-
hashi. 2015. Argument alignment learning in event
knowledge.
In Proceedings of the 21th Annual
Meeting of the Association for Natural Language
Processing (In Japanese). Association for Natural
Language Processing, Kyoto, Japan, pages 1065–
1067.

30

Hector Levesque, Ernest Davis, and Leora Morgen-
In
stern. 2012. The winograd schema challenge.
13th International Conference on the Principles of
Knowledge Representation and Reasoning. Associ-
ation for the Advancement of Artiﬁcial Intelligence,
pages 552–561.

Jorge Nocedal. 1980. Updating quasi-newton matrices
with limited storage. Mathematics of computation
35(151):773–782.

Naoaki Okazaki. 2009. Classias:

a collection of

machine-learning algorithms for classiﬁcation.

Karl Pichotta and Raymond Mooney. 2014. Statisti-
In
cal script learning with multi-argument events.
Proceedings of the 14th Conference of the European
Chapter of the Association for Computational Lin-
guistics. Association for Computational Linguistics,
Gothenburg, Sweden, pages 220–229.

Altaf Rahman and Vincent Ng. 2012.

Resolving
complex cases of deﬁnite pronouns: The wino-
grad schema challenge. In Proceedings of the 2012
Joint Conference on Empirical Methods in Natural
Language Processing and Computational Natural
Language Learning. Association for Computational
Linguistics, Jeju Island, Korea, pages 777–789.

Tomohide Shibata and Sadao Kurohashi. 2011. Ac-
quiring strongly-related events using predicate-
argument co-occurring statistics and case frames. In
Proceedings of 5th International Joint Conference
on Natural Language Processing. Asian Federation
of Natural Language Processing, Chiang Mai, Thai-
land, pages 1028–1036.

