



















































A Bambara Tonalization System for Word Sense Disambiguation Using Differential Coding, Segmentation and Edit Operation Filtering


Proceedings of the The 8th International Joint Conference on Natural Language Processing, pages 694–703,
Taipei, Taiwan, November 27 – December 1, 2017 c©2017 AFNLP

A Bambara Tonalization System for Word Sense Disambiguation
Using Differential Coding, Segmentation and Edit Operation Filtering

Luigi (Yu-Cheng) Liu
ER-TIM, INALCO, 2 rue de Lille, Paris, France

luigi.liu@laposte.net

Damien Nouvel
ER-TIM, INALCO, 2 rue de Lille, Paris, France

damien.nouvel@inalco.fr

Abstract

In many languages such as Bambara or
Arabic, tone markers (diacritics) may be
written but are actually often omitted.
NLP applications are confronted to am-
biguities and subsequent difficulties when
processing texts. To circumvent this prob-
lem, tonalization may be used, as a word
sense disambiguation task, relying on con-
text to add diacritics that partially disam-
biguate words as well as senses. In this pa-
per, we describe our implementation of a
Bambara tonalizer that adds tone markers
using machine learning (CRFs). To make
our tool efficient, we used differential cod-
ing, word segmentation and edit opera-
tion filtering. We describe our approach
that allows tractable machine learning and
improves accuracy: our model may be
learned within minutes on a 358K-word
corpus and reaches 92.3% accuracy.

1 Introduction

Bambara (Bamana, Bamanankan, ISO-369 Bam)
is the most widely spoken language of the Mand-
ing language group. It is spoken mainly in Mali
(and among the considerable Malian diaspora) by
12 to 15 million people. It is not an official lan-
guage; however it is the major language (besides
French) on Malian radio and TV broadcasts, there
are newspapers in Bambara, it is broadly used
in humanities and in primary schools, and it is
taught at several universities around the world. In
the Mande language family, Bambara is among
the best described: there are numerous works on
Bambara language, such as dictionaries (Bailleul,
2007; Dumestre, 2011), description of its grammar
(Dumestre, 2003; Vydrin, 1999b,a).

Bambara is a tonal language, with the impor-
tant drawback that its official orthography does
not represent tones. Because several tonalized
forms can correspond to some unaccented tokens,
it makes word sense more ambiguous in corpus
and imposes important challenges to NLP appli-
cations. Our goal is to remedy this issue by im-
plementing an automatic tonalizer for Bambara, to
improve subsequent NLP processings and facili-
tate linguistic analysis for Bambara.

2 Bambara Reference Corpus

Our work relies on an annotated corpus, the Bam-
bara Reference Corpus (BRC, in French Corpus
Bambara de Référence), a linguistically annotated
corpus suitable both for linguistic research and for
NLP tools development. Building BRC started in
2010 as a project conducted by a small group of
specialists in Manding linguistics and computer
sciences in Russia. Later on, colleagues from
other countries (France, Germany) joined the team
to provide more texts, clean the corpus, and au-
tomatically preprocess it. The corpus is available
online since April 2012, most relevant information
about the project may be found online1 or in pub-
lications (Vydrin, 2013, 2014).

Part
Type Words (dist.) Punct.

Non-disamb. 2,160,155 (58,277) 358,659
Disamb. 358,794 (23,875) 61,847

Table 1: Corpus statistics

Table 1 and Figure 1 present overall corpus
statistics and characteristics 2. The corpus consists

1http://cormand.huma-num.fr/index.html
2Corpus is continuously growing and contains, as of June

2017, 4,113,006 raw words and 903,585 in the disambiguated
part.

694



Written

49.8%

Internet
5.7%

Oral

34% Undet.
10.5%

Audiovisual
2.8%

Manuscript
10.4%

Popular

43%

Magazines

19.5%

Academic

17.1%
Undet.

7.2%

Figure 1: Corpus composition (medium, source)

of two parts: an automatically annotated subcor-
pus which contains ambiguous interpretations for
each token (non-disambiguated), and a manually
disambiguated subcorpus which serves as a gold-
standard annotated dataset.

2.1 BRC Tools and Resources

The available data within the BRC project includes
source text files and an electronic Bambara dic-
tionary. Dedicated tools have been implemented
to annotate the corpus. The collection of soft-
ware tools developed for the BRC (parser, dis-
ambiguation interface, various auxiliary scripts) is
called Daba (Maslinsky, 2014) and is available on-
line3. Among them, a rule-based parser was built
to bootstrap the annotation of the corpus.

Thanks to this data and these tools, a gold stan-
dard has been created by human annotators pro-
ficient in Bambara. This process is described in
Figure 2: The parser’s output is reviewed, and an-
notation is done by selecting or editing required
information for each wordform.

Daba
Text
Files

Dictionary
(Bamadaba)

HTML
Files

Annotators

NoSketch
Engine

Linguists

Figure 2: BRC data workflow

2.2 Main Goals for BRC

The project’s goal is to disambiguate the whole
corpus. Wordform annotation is done for each
wordform and for three main features:

• POS Tagging

• Tone Marker Restoration

• Gloss Assignment
3https://github.com/maslinych/daba

We are currently working on these tasks by us-
ing sequential modeling with various additional
techniques. For POS tagging, we use state of
the art, Conditional random fields (CRFs; Laf-
ferty et al. 2001) over 23 morpho-syntactic pos-
sible tags, our implementation reaches 94% accu-
racy, which is quite satisfying for such an under-
resourced language.

For the tone marker restoration task, we consid-
ered using similar methods, but the tag set is much
larger: 20,870 distinctive tonal forms are found,
which prevents an efficient learning. We have to
reduce training complexity by finding an adequate
representation that will allow to model tone addi-
tion without listing every possible form.

2.3 Tone Markers in Bambara

The word-level tonal information is provided by
tonal form or so-called diacritized form in the lit-
erature. The tone markers inserted in tonal form
are mainly acute accent ( ´ ) for low tone, grave
accent ( ` ) for high tone, háček ( ˇ ) for phonet-
ically rising tone and circumflex accent ( ˆ ) for
lexical rising tone. For instance, a non-tonalized
token tugu can be tonalized as a noun tùgu (arms)
or a verb túgu (to close) that we can find in its
quasi-homonym list.

Ideally, the only difference between a tonalized
token compared to its non-tonalized form is the
presence of tone accents. In reality, annotators
also often introduce other modifications, mainly
typographic and orthographic correction. As we
will see later, this fact makes it necessary to oper-
ate filtering on edit operations, in order to properly
focus on tonalization operations.

To give an idea of tonal form variety, we pro-
pose to compare annotated tokens to their original
forms. Related statistics are summarized in Ta-
ble 2, showing that the probability for modifica-
tions other than tonalization during annotation is
not negligible : 8.90% modification are related to
other modification such as typographic or ortho-
graphic correction. We will have to take this into
account for our system to focus on tonalization op-
erations.

3 Problem Formulation

Our objective is to design a word sense disam-
biguation system for Bambara helping to choose
the most correct interpretation from the quasi-
homonym list provided for each token in the sub-

695



Operation Ratio
Tonalization 38.73%

Other 8.90%
None 52.35%

Table 2: Distribution of edit operation to anno-
tated forms receiving: only tone markers, other
characters, or nothing (i.e., tonalized token is iden-
tical to its non-tonalized form)

corpus processed by Daba.
The drawback of modeling sequences of large-

scale label set is the expensive computational
cost needed to estimate CRF parameters. Indeed,
the quasi-Newton method that CRF training em-
ploys for solving parameter estimation of CRF
model has a computational cost proportional to
M2 where M is the size of label set (Sutton and
McCallum, 2010). This makes CRF learning com-
putationally expensive when labels are the set of
possible tonalized words.

4 Related Works

4.1 Automatic Diacritization and Sub-word
Level Modeling

Simard (1998) presented a method for accent
insertion in French using a two-layers Hidden
Markov Model (HMM) trained over a POS-tagged
corpus while Tufiş and Chiţu (1999) adapted a tri-
gram tagger to this task for Romanian. Focused on
word-level modeling, their method requires lexical
resources.

Elshafei et al. (2006) use a single-layer HHM
for diacritization of Arabic text. Scannell
(2011) extended diacritization to uni-codification
in African languages by solving it using a Naive
Bayes classifier. Their models, trained with tri-
gram features on word and character levels, do not
need other resources than an accented corpus, so it
is compatible with resource-scarce languages.

Nguyen et al. (2012) implemented a system for
accent restoration in Vietnamese based on two dif-
ferent models including the CRFs. The training
corpus is preprocessed and annotated to indicate,
both at syllable and character levels, the accents to
insert to recover diacritic forms. Learning how to
infer diacritic forms is indirectly reached by learn-
ing their differential representation.

Some hybrid approaches were also proposed for
Arabic Diacritization : Said et al. (2013) com-

bined a CRF and a morphological analyzer to
improve diacritization accuracy, while Metwally
et al. (2016) proposed a three-layer processing
composed of a CRF, a HMM and a morphologi-
cal analyzer.

4.2 Category Decomposition
In the domain of CRF-based tagging, the idea to
decompose label set in small pieces to train in or-
der to gain learning performance can be found in
(Tellier et al., 2010). They proposed to map to-
tal label set to a category tree and experiment to
train on labels in a cascade manner or to train
independently on each label components which
correspond to a sub-category and then recombine
them to obtain the total label. They concluded that
their method, coined “category decomposition”,
improves greatly the time-wise efficiency of learn-
ing process.

5 Methodology

5.1 Fundamental Definitions
In the following, we will introduce essential ele-
ments to our methodology .

LetX , Y , ∆ be three discrete random variables:
non-tonalized token, tonalized token and code. X
and Y take value from Ω (character set with or
without diacritics) and ∆ is either ∅ (no modi-
fication) or a concatenation of codewords, where
a codeword is a triplet containing operation type
(insertion or deletion), position for operation and
character (if insertion) from Σ:

Σ ≡ {+1,−1} × N>0 × Ω (1)

We define E and D, respectively encoder and
decoder functions, inverses of each other, such as:

∆ = E(Y ;X)
Y = D(∆;X)

(2)

5.1.1 Entropy Reduction Ratio
rE is called entropy reduction ratio of E as de-
fined:

rE(∆, Y ) ≡ H(Y )H(∆) or
H(Y )

H(E(Y ;X)) (3)

for any ∆ satisfyingH(∆) 6= 0.
We need to predict tonalization for every non-

tonalized token value X = x. Instead of predict-
ing on Y , we can predict first on ∆ and decode

696



∆ to obtain Y indirectly by using the second re-
lation in (2). This will allow to greatly reduce the
number of possible values of our model: we pre-
dict differential codes instead of tonalized words
themselves.

In the next subsections, we try to design an ef-
ficient encoder E which maximizes rE so that the
resulting code ∆ has a reduced set of possible val-
ues, it is more easy for a CRF-based modeler to
learn on ∆ than on Y as explained in the last para-
graph of section 3.

5.2 Tonalization as Edit Operations
To maximize rE (i.e. reduce ∆), we use an algo-
rithm to represent the difference between Y and
X by a minimal sequence of basic operations re-
quired for recovering Y from X . These basic op-
erations are known as edit operations that we will
present in the following.

By edit operation s, we mean a mapping from
string a to string b, where a, b are sets of
string drawn from the alphabet Ω such that if
a is expressed as concatenation of characters as
a1 a2 ... aN−1 aN , then the mapping can be writ-
ten as

b = s(a;σ) (4)

where σ = (m, p, c) ∈ Σ is a parameter tuple, m
is an operation type indicator, p is a position pa-
rameter, c is a character involved by the operation.

And the mapping output b will be

b=

{
a1a2...ap−1 c ap...aN , m=+1
a1a2...ap−1 ap+1...aN , m=−1

(5)

We can see in (5), that when m = +1, s rep-
resents an insertion operation which consists in
adding c just before the p-th character4in a, while
in the case of m = −1, s denotes a deletion oper-
ation which removes the p-th character from a.

5.2.1 Use of Wagner-Fischer Algorithm
For any pair of strings (x, y), the Wagner-
Fischer algorithm5 in (Wagner and Fischer, 1974)
allows to obtain a minimal sequence S =
(σ1, σ2, ..., σM−1) such that:

Ai+1 = s(Ai;σi + (0, li,∅))
li+1 = li +m(σi)

A0 = x,AM = y, l0 = 0
(6)

4Or just after the (p− 1)-th character, when p = N
5In this article, we apply Wagner-Fischer algorithm in its

special case where there are only 2 available edit operations
against 3 edit operations including the substitution as in its
general case.

Where 0 ≤M , 1 ≤ i ≤M−1, position param-
eters p of σi denoted by p(σi) form an increasing
series with respect to index i:

p(σi) ≤ p(σi+1) (7)
For the edit operations acting on the same po-

sition p, insertions must precede6 deletion as de-
scribed below:

m(σi) ≥ m(σi+1), if p(σi) = p(σi+1) (8)
Because it does not make sense to delete a char-

acter of a string twice, we assume that, for a given
character position, only one deletion may occur:

Thus, h(σi) below forms an increasing series

h(σi) ≡ 2 · p(σi) + 0.5× [1−m(σi)] (9)
5.2.2 Encoder
If we set encoder E(y;x) as an application of
Wagner-Fischer algorithm on (x, y), and δ as the
concatenation of elements of S like

δ = σ1σ2...σM−1 (10)

Then E(y;x) is specified so that the first relation
in (2) is satisfied.

5.2.3 Decoder
Moreover, if we choose D(δ;x) as an implemen-
tation of (6) applied on x with δ as parameter, this
specifies a way to use x and the code δ to recovery
y, and define a decoder.

To show that the entropy is effectively reduced,
we will present experiments in section 8 for eval-
uation of entropy reduction ratio of rE .

5.3 Segmentation
Segment decomposition of a string x is a map-
ping from a string x to strings called segments
x(1)x(2)x(3)...x(L) so that the concatenation of the
latter is equal to the original string x.

We can then divide the resulting code δ in L
code segments δ(1)δ(2) ... δ(L) described as:

δ = δ(1)δ(2) ... δ(L) (11)

δ(i) is the code segment (i.e. a sub-sequence of
code δ) associated with x(i):

δ(i) = σmin(Si)...σmax(Si) (12)

6Or we can also state that deletions precede insertions. In
this case, the second relation of (6) will be different for that
decoder to work properly. So this was our design choice.

697



where Si is the set which contains the indices of
all edit operations acting on segment x(i).

Furthermore, we define shifted version of δ(i) as
below

δ′(i) = σ′min(Si)...σ
′
max(Si)

(13)

where σ′min(Si) = σmin(Si) + (0,−ei,∅), ei =∑i−1
k=1 |xk| position offset. It can be shown that

y can be obtained by concatenation 7 of decoding
results of D on shifted code segment δ′(i) as below

y=D(δ′(1);x(1))D(δ′(2);x(2))...D(δ′(L);x(L)) (14)

Using the proposed segmentation allows to get
the shifted code segment δ′(i) associated with seg-
ment x(i). For obtaining y, it suffices to individ-
ually decode code segments δ′(i) using the same
decoder defined in (2). Hence, an encoder - de-
coder pair is indirectly specified as:

δ′(i) = E′(y, x(i))

y(i) = D(δ′(i), x(i))
(15)

The segmentation technique allows a CRF-
based system to learn δ′(i) individually instead of
a total code δ at once. The relation in (14) can then
be used to recover y from predicted code segments
δ′(i) produced by CRF taggers.

To show the effect of segmentation on entropy
reduction ratio rE′ , we will evaluate experiments
in section 8 with different segmentation settings
such as syllabification and fixed-width segmenta-
tion.

5.4 Edit Operation Filtering

As previously explained, segmentation on x leads
to splitting code δ in several code segments δ(i)

(or its shifted version δ′(i)) because it regroups edit
operations σj = (mj , pj , cj) inside δ by position
parameter pj .

Edit operation filtering allows to split each code
segment δ(i) in some subgroups by operation type
mi and character involved by operation ci. But
as opposed to segmentation, in the filtering pro-
cess, some edit operations are dropped because
of their irrelevance regarding our tonalization goal
for Bambara.

7Alternatively, it is also possible to obtain the integral
code δ from δ′(i) using relations (11), (12), (13) and to get
the underlying tonalized token y by direct decoding on the
integral code δ.

First, we recall that the order of edit operations
called σ′k inside the sequence δ

′(i) is partially de-
termined by their elements. It can be shown, more
generally that the order restriction in (7), (8) leads
directly to

p′k < p
′
l =⇒ k < l (16)

m′k > m
′
l ∧ p′k = p′l =⇒ k < l (17)

where m′k ≡ m(σ′k), p′k ≡ p(σ′k), k 6= l.
Thus, the order between any two edit operations

in δ′(i), called σ′k, σ
′
l is free only in the case of

multiple insertion acting on the same position of
a string, i.e. m′k = m

′
l = +1, p

′
k = p

′
l.

5.4.1 Tone Marker Filtering
The aim of tone marker filtering is to remove edit
operations irrelevant to tonalization. It consists
in removing all insertions of characters which are
not considered as tone markers, and keeping only
the first of all insertions (also only the first one of
all tone deletions) operating on the same position
(because neither multiple insertions nor multiple
deletions are supposed to be present in pure tonal-
ization).

In addition, the filter contributes to reduce the
code so that the order of edit operations in fil-
ter output becomes totally determined by edit op-
eration arguments (i.e. p and m) via relations
(16)(17). Therefore, the indexing function h de-
fined in 9 forms a strictly increasing series.

5.4.2 Edit Operation Decomposition
We define an edit operation dispatcher Fm as a
mapping from input code δin to its sub-sequence
described as:

Fm(δin; k) = σmin(Vk(δin))...σmax(Vk(δin)) (18)

where Vk(δ) is the set which contains the indices
of all edit operations in δ of type k, k ∈ {−1,+1}

For recall, the tone marker filter produces a
code result in which h(σi), i > 0 forms a
strictly increasing sequence. This property im-
plies that there exists an inverse mapping from
{Fm(δin; k)}k∈{−1,+1} to δin if δin is a filtered
result. We call this underlying mapping edit op-
eration assembler.

698



6 System Architecture

In this section, we present the architecture of our
system for automatic tone marker insertion for
Bambara text by detailing its functional blocks and
internal data flow.

6.1 Training Stage

At training stage, as shown in Figure 3, the tonal-
ization system uses an encoder to represent the dif-
ference of the tonalized token value y relative to its
original non-tonalized token value x by the code
δ. The code δ as well as the non-tonalized token x
are then fed into the code segmenter which breaks
down δ in small shifted code segments δ′(i). The
tone marker filter takes the shifted code segments,
selects tone markers and sends its output to a dis-
patcher which distinguishes insertion and deletion
operations.

In the following, two CRF models are trained
individually on relation (x, δ′(i)+ ) and on relation
(x, δ′(i)− ) in the reference corpus.

6.2 Tonalization Stage

In tone recovery stage, the two trained CRF mod-
els are used to predict the two parts of shifted code
segment δ′(i)+ , δ

′(i)
− from tonalized token x. Then

they are fed to, as shown in Figure 4, an edit oper-
ation assembler to get predicted shifted code seg-
ment δ′(i) ; at the same time, the non-tonalized
token x is re-segmented to get x(i). Then, the
non-tonalized segment x(i) as well as the shifted
code segment δ′(i) are sent to be decoded to obtain
tonalized segment y(i) in decoder output. In the
end, the decoded tonalized segments y(i) are re-
united in the code assembler to give the estimated
integral tonalized token value y.

7 Running examples

To illustrate more concretely how our system
works at training stage, we propose in the follow-
ing, two running examples: the first one shows
how our system models a pure tonalization; the
second one shows how the tone marker filter could
be helpful when some typographic modifications
are introduced in the tonalized form.

7.1 Example 1 (pure tonalization)

In this example, the system models the tonaliza-
tion from lakali to lákàlı́. Hence, x, y are set as

below:

x = lakali
y = lákàlı́

The tone encoder compares x and y, then outputs
the differential representation as tonal code δ be-
low:

δ = [(+1, 2, )́, (+1, 4, )̀, (+1, 6, )́]

In the above, the 3 codewords that δ contains rep-
resent the 3 tone markers inserted into x to model
for this tonalization.

δ(1) = [(+1, 2, )́]

δ(2) = [(+1, 4, )̀]

δ(3) = [(+1, 6, )́]

The syllabification is examplified below on non-
tonalized and tonalized form:

x(1) = la, y(1) = lá

x(2) = ka, y(2) = kà

x(3) = li, y(3) = ĺi

We observe that the 3 tone markers are all inserted
after the second character for each of 3 syllables.
Therefore, we got 3 shifted versions of code seg-
ments operating at position 2 as below.

δ
′(1)
+ = δ

′(1) = [(+1, 2, )́]

δ
′(2)
+ = δ

′(2) = [(+1, 2, )̀]

δ
′(3)
+ = δ

′(3) = [(+1, 2, )́]

As in this example, only the tone marker insertions
are to be modeled, the tone marker filter, edit oper-
ation decomposition unit do not affect system re-
sult at training stage.

7.2 Example 2 (noisy tonalization)

The system models the tonalization from
taanikasegin to táa-ká-ségin. Hence, x, y are set
as below:

x = taanikasegin
y = táa− ká− ségin

In the following, we show the syllabification re-
sult of y presented with its related to non-tonalized

699



Encoder
(E)

y

x
δ

Code
Segmenter

(S)
δ′(i)

Tone
Marker
Filter
(F)

Edit
Operation
Dispatcher

(D)

δ
′(i)
+

δ
′(i)
−

Figure 3: Block diagram for the proposed Bambara tonalization system at training stage

Edit
Operation
Assembler

δ
′(i)
+

δ
′(i)
−

Segmenterx

δ′(i)

x(i)

Decoder y(i) Assembler y

Figure 4: Block diagram for the proposed Bambara tonalization system at tonalization stage

form.

x(1) = taani, y(1) = táa−
x(2) = ka, y(2) = ká−
x(3) = se, y(3) = sé

x(4) = y(4) = gin

Subsequently, 7 codewords are obtained from dif-
ferential tone encoding, then departed in 4 code
segments corresponding to 4 syllables:

δ(1) =[(+1, 2, )́,
(+1, 3,−), (−1, 4, n), (−1, 5, i)]

δ(2) =[(+1, 7, )́, (+1, 7,−)]
δ(3) =[(+1, 9, )́]

δ(4) =∅

As a remark, the first syllable in the above is the
most concerned by noise : one dash symbol inser-
tion and 2 character removals are observed. The
tone marker filter removes all character deletions
and insertions in code segments. Therefore, tonal-
ization is modeled by only tone marker insertions
as below:

δ
′(1)
+ = δ

′(1) = [(+1, 2,́ )]

δ
′(2)
+ = δ

′(2) = [(+1, 2,́ )]

δ
′(3)
+ = δ

′(3) = [(+1, 2,́ )]

δ
′(4)
+ = δ

′(4) = ∅

Here, we found a tonalization representation sim-
ilar to the one from the preceding example: inser-
tion of a tone marker at position 2 for some sylla-
bles.

8 Experiment

8.1 Experiment Setup

The disambiguated corpus of BRC which contains
tonalized tokens is chosen as training data. The
train data then is split into training and evaluation
data sets with ratio p : (1− p). By default, p is set
to 50% for efficiency reason.

For CRF implementation, we use CRFSuite as
open-source library with l-BFGS algorithm speci-
fied as training method, Viterbi algorithm as infer-
ence method (Okazaki, 2007) .

The segmentation mode is specified by the in-
teger w described in the following : w = −1 in-
dicates a syllabification (as obtained by BRC mor-
phological parser), w = 0 for no segmentation and
w > 0 signals a w-width regular segmentation8.

We denote different configurations of our sys-
tem by their functional layout at training stage.
For specifying the functional layout, the follow-
ing symbols D, F , S, E are used to indicate re-
spectively four constituent blocks of the system :
D for edit operation decomposition block, F for
tone marker filter block, S for segmentation block,
E for encoder block, and ◦ denotes an inter-block
connection. For example, S ◦ E represents an en-
coder followed by a segmenter as system.

8A regular segmenter forms a segment of every w succes-
sive characters, from left to right (i.e. in direction of writing
of Bambara), in its input string. By exception, the last seg-
ment at output contains the rest of the string which has not
yet been segmented so that we allow it to be equal or shorter
than a segment of w characters.

700



8.2 Feature Engineering
The feature patterns we created are the same for all
CRF models. The features are generated for each
segment9 for tokens in the training data x(i).

For each segment, features indicate:

• At segment (sub-word) level
– Previous, current and next segment
– Substring containing all vowels for cur-

rent segment
– Position of current segment (integer)

relative to beginning or ending of word
– Typography of segment: all capital let-

ters, presence of numbers or punctuation
marks

• At word level
– Previous, current and next words
– Prefix and suffix for previous, current

and next words
– Position of current word (integer) rela-

tive to beginning or ending of sentence

8.3 Evaluation Measure
As shown by the example of the section 7.2, the
tonalized token y can itself be noisy. On the other
hand, we are particularly interested in errors intro-
duced by CRF-based in predicting δ′(i)+ ,δ

′(i)
− from

x. Consequently, our accuracy evaluation is based
on the comparison of two tokens described as be-
low:

• A gold standard according to tonalization fil-
tering (i.e. typographic modification are not
expected)

• The token tonalized by our system, result-
ing from applying δ′(i)+ ,δ

′(i)
− (predicted by the

CRFs) to the segments x(i)

8.4 Experiment Results
8.4.1 Coding and Segmentation
The entropy for the tonalized token Y that we cal-
culated on the disambiguated part of BRC is 8.73
bits of entropy for 20,870 distinctive tonal forms.
As shown in Table 3, the tonalization representa-
tion produced by tone encoder ∆ has only 3.38
bits of entropy (1,273 distinctive codes). In other

9If the edit operation decomposition is used, as we use two
CRF models, features are generated on segment components
(for insertion and for deletion) individually

word, the entropy is reduced by a factor of 2.58
by only using the differential tone encoder. If
segmentation is also used, shorter segments lead
to greater entropy reduction. The extreme case
is to segment by character (w = 1), leading to
an entropy reduction ratio of 4.82, and implies
character-level processing.

8.4.2 Segmentation and Operation Filtering
Table 4 presents an experiment on the impact
of system configuration (related to Figure 4) and
segmentation mode on tonalization accuracy and
training time of our system. For comparison, the
accuracy of a baseline based on majority voting
method is also provided in the last row.

We note first that segmentation improves accu-
racy. Syllabification as well as regular segmen-
tation with a fixed width close to averaged sylla-
ble length (3.24 characters) gives the best result in
accuracy (0.92). Regarding system configuration,
accuracy is generally preserved over the four dif-
ferent configurations except of when applying op-
eration decomposition without filtering. This can
be explained by the fact that decomposition is not
totally recoverable as mentioned in section 5.4.2
for data which is not processed by tone marker fil-
ter, it can introduce some errors within assembler
at tonalization phase and degrades its accuracy.

Training time is dramatically reduced by seg-
mentation and by edit operation decomposition.
This may sometimes be at the cost of accuracy, but
syllabification or fixed segmentation with width
w = 2 is a nice trade-off, preserving accuracy and
keeping training time acceptable.

Those experiments allow to state that segmenta-
tion is gainful both for accuracy and training time
(see last column). Decomposition and filtering do
not have a tremendous impact on accuracy, but
are unavoidable to be able to train models on the
whole dataset in reasonable time.

8.4.3 Effect of Train Size and Error Analysis
Figure 5 shows how the training set size influences
the tonalization accuracy and training time. We
can see the progress in accuracy with respect to
training set size is less considerable when p ≥
50% than when p < 50%. The training time in-
creases dramatically with respect to training set
size.

Table 5 shows distribution of errors which oc-
cur in automatic tone insertion (not in tone dele-
tion). The error occurs due to a bad prediction in

701



Entropy
Width w

-1 (Syllabe) 1 2 3 4 5 6 0

H(δ′(i)) 2.67 1.81 2.56 2.86 3.11 3.23 3.29 3.38
rE(δ′(i), y(i)) 3.27 4.82 3.41 3.05 2.80 2.70 2.65 2.58

Table 3: Encoder entropy H(∆) and reduction ratio rE(δ′(i), y(i))

Systems
Width w

-1 (Syll.) 1 2 3 4 5 6 0

D ◦ F ◦ S ◦ E 0.923 0.912 0.923 0.923 0.918 0.911 0.904 0.893
time 19.88 17.62 13.17 15.67 19.62 32.40 44.25 261.83

F ◦ S ◦ E 0.924 0.916 0.923 0.923 0.918 0.911 0.905 0.894
time 57.68 19.42 20.72 35.70 62.85 121.08 168.27 2455.80

D ◦ S ◦ E 0.921 0.911 0.921 0.922 0.916 0.910 0.903 0.892
time 28.28 21.27 19.72 29.17 39.90 57.35 70.15 793.85

S ◦ E 0.923 0.915 0.922 0.922 0.917 0.910 0.904 0.893
time 101.63 25.52 42.03 235.35 378.37 169.55 318.23 2683.72

Majority vote 0.843

Table 4: Accuracy for our system trained with four different system configurations and eight segmenta-
tion modes (p = 50%)

20 40 60 80
0

20

40

60

training size (%)

m
in

.

0.7

0.8

0.9

1

%

accuracy
time

Figure 5: Accuracy and time of training (config-
ured as D ◦ F ◦ S ◦ E using syllabification) with
respect to different training size 90%-10%

Error Type Ratio
Tone Only 58.52%

Position Only 1.17%
Tone and Position 0.023%

Silence 40.08%

Table 5: Error distribution by type for insertion
operations with p = 50%, system = D ◦ F ◦ S ◦E

character to insert but correct prediction in charac-
ter position is prevalent (58.52%) and the silence
(40.08%) which is a false negative case is also very
frequent. Prediction error only in character po-
sition is as weak (1.17%) as error rate (0.023%)
for both character position and symbol error. This
shows that our system correctly predicts positions
(mainly at beginning of words) and that most of
the errors are due to the difficulty to predict if a
tone has to be added, and which one.

Predicted
´ ` ˆ ˇ

A
ct

ua
l ´ 0.9541 0.0438 0.0021 0.0000

` 0.0841 0.9141 0.0015 0.0003
ˆ 0.0035 0.0322 0.9643 0.0000
ˇ 0.0000 0.0952 0.0000 0.9048

Table 6: Confusion matrix on prediction of tone
markers

Table 6 presents the confusion matrix of the er-
ror occurring due to bad prediction in character
mentioned previously in Table 5. It shows that
these errors are mainly due to the confusion be-
tween low tone ( ´ ) and high tone ( ` ), which are
the two most frequent markers in BRC.

702



9 Conclusion

In this paper, we presented our Bambara tonaliza-
tion system for automatic detection of tone mark-
ers in Bambara. Our experiments show that using
segmentation both increases tonalization accuracy
and greatly reduces training time. Our differential
encoder reduces entropy of labels to be predicted,
making CRF learning efficient and allowing to im-
plement a tone marker filter and edit operation de-
composition unit within the tonalization process.
The tone marker filter plays the role of normaliza-
tor of tonalized token to learn in training phase and
in normalizing the token, it leads to reduce train-
ing time. The edit operation decomposition unit
which follows the filter split the tokens in insertion
and deletion of tone markers, allows to accelerate
furthermore the training time reduction without al-
tering tonalization accuracy.

Acknowledgments

We thank National Institute for Oriental Lan-
guages and Civilizations which supports this work
as part of its research project called MANTAL. We
acknowledge, in this regard, Valentin Vydrin, Kir-
ill Maslinsky, Davy Auffret, Elvis Mboning and
Arthur Provenier for thier contributions, which
make possible our work.

References
Charles Bailleul. 2007. Dictionnaire Bambara-Franais

(3e dition corrige). Bamako : Donniya.

Grard Dumestre. 2003. Grammaire fondamentale du
bambara. Paris : Karthala.

Grard Dumestre. 2011. Dictionnaire bambara-franais
suivi dun index abrg franais-bambara. Paris :
Karthala.

Moustafa Elshafei, Husni Al-Muhtaseb, and Mansour
Alghamdi. 2006. Statistical methods for automatic
diacritization of arabic text. In The Saudi 18th Na-
tional Computer Conference. Riyadh, volume 18,
pages 301–306.

John Lafferty, Andrew McCallum, and Fernando CN
Pereira. 2001. Conditional random fields: Prob-
abilistic models for segmenting and labeling se-
quence data.

Kirill Maslinsky. 2014. Daba: a model and tools for
manding corpora. In TALN-RECITAL 2014 Work-
shop TALAf 2014 : Traitement Automatique des
Langues Africaines (TALAf 2014: African Language
Processing), pages 114–122. Association pour le
Traitement Automatique des Langues.

Aya S Metwally, Mohsen A Rashwan, and Amir F
Atiya. 2016. A multi-layered approach for ara-
bic text diacritization. In Cloud Computing and
Big Data Analysis (ICCCBDA), 2016 IEEE Interna-
tional Conference on, pages 389–393. IEEE.

Minh Trung Nguyen, Quoc Nhan Nguyen, and
Hong Phuong Nguyen. 2012. Vietnamese diacrit-
ics restoration as sequential tagging. In Comput-
ing and Communication Technologies, Research, In-
novation, and Vision for the Future (RIVF), 2012
IEEE RIVF International Conference on, pages 1–
6. IEEE.

Naoaki Okazaki. 2007. Crfsuite: a fast implementation
of conditional random fields (crfs).

Ahmed Said, Mohamed El-Sharqwi, Achraf Chalabi,
and Eslam Kamal. 2013. A hybrid approach for
arabic diacritization. In International Conference
on Application of Natural Language to Information
Systems, pages 53–64. Springer.

Kevin P Scannell. 2011. Statistical unicodification of
african languages. Language resources and evalua-
tion, 45(3):375–386.

Michel Simard. 1998. Automatic insertion of accents
in french text. In EMNLP, pages 27–35.

Charles Sutton and Andrew McCallum. 2010. An
introduction to conditional random fields. arXiv
preprint arXiv:1011.4088.

Isabelle Tellier, Iris Eshkol, Samer Taalab, and Jean-
Philippe Prost. 2010. Pos-tagging for oral texts with
crf and category decomposition. Research in Com-
puting Science, 46:79–90.

Dan Tufiş and Adrian Chiţu. 1999. Automatic diacrit-
ics insertion in romanian texts. In Proceedings of
COMPLEX99 International Conference on Compu-
tational Lexicography.

Valentin Vydrin. 1999a. Les parties du discours en
bambara : un essai de bilan. 35:73–93.

Valentin Vydrin. 1999b. Manding-english dictionary
(maninka, bamana). In St. Petersburg: Dmitry Bu-
lanin Publishing House, volume 1.

Valentin Vydrin. 2013. Bamana reference corpus (brc).
Procedia-Social and Behavioral Sciences, 95:75–
80.

Valentin Vydrin. 2014. Projet des corpus écrits des
langues manding: le bambara, le maninka. In
Traitement Automatique du Langage Naturel 2014.

Robert A Wagner and Michael J Fischer. 1974. The
string-to-string correction problem. Journal of the
ACM (JACM), 21(1):168–173.

703


