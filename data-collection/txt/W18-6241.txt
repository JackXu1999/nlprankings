



















































Identifying Opinion-Topics and Polarity of Parliamentary Debate Motions


Proceedings of the 9th Workshop on Computational Approaches to Subjectivity, Sentiment and Social Media Analysis, pages 280–285
Brussels, Belgium, October 31, 2018. c©2018 Association for Computational Linguistics

https://doi.org/10.18653/v1/P17

280

Identifying Opinion-Topics and Polarity of Parliamentary Debate Motions

Gavin Abercrombie and Riza Batista-Navarro
School of Computer Science

University of Manchester
Kilburn Building, Manchester M13 9PL

gavin.abercrombie@postgrad.manchester.ac.uk
riza.batista@manchester.ac.uk

Abstract

Analysis of the topics mentioned and opinions
expressed in parliamentary debate motions–
or proposals–is difficult for human readers,
but necessary for understanding and automatic
processing of the content of the subsequent
speeches. We present a dataset of debate mo-
tions with pre-existing ‘policy’ labels, and in-
vestigate the utility of these labels for simul-
taneous topic and opinion polarity analysis.
For topic detection, we apply one-versus-the-
rest supervised topic classification, finding that
good performance is achieved in predicting
the policy topics, and that textual features de-
rived from the debate titles associated with the
motions are particularly indicative of motion
topic. We then examine whether the output
could also be used to determine the positions
taken by proposers towards the different poli-
cies by investigating how well humans agree
in interpreting the opinion polarities of the mo-
tions. Finding very high levels of agreement,
we conclude that the policies used can be reli-
able labels for use in these tasks, and that suc-
cessful topic detection can therefore provide
opinion analysis of the motions ‘for free’.

1 Introduction

In the House of Commons of the UK Parlia-
ment, the topics contained in a debate’s motion–
a proposal one Member of Parliament (MP) puts
to the other Members of the House–are the fo-
cus of opinions expressed during all subsequent
speeches. These motions are therefore crucial for
understanding the content of MPs’ speeches and
the opinions they convey.

It is often difficult for people to process debate
motions due to the level of domain-specific knowl-
edge related to the language and workings of Par-
liament they contain. Indeed, these motions are so
hard for ordinary citizens to understand that par-
liamentary monitoring organisations like the Pub-

lic Whip1 and They Work for You2 produce man-
ually written summaries and annotated versions of
them, which are written by crowd-sourced volun-
teers with domain expertise or interest.

Figure 1: Example of a debate motion as presented on
the theyworkforyou.com website. Motions contain in-
formation about the topics of debates, as well as the
proposer’s opinions towards those topics, but can be
difficult for human readers to process.

In conducting sentiment analysis of debate
speeches, it has been observed that, when formu-
lating these motions, the speakers that propose
them themselves express sentiment towards the
topics of the motions, and that these motions can
act as polarity shifters for subsequent speeches in
a debate–that is, depending on the sentiment po-
larity of a motion, the sentiment polarity of lan-
guage used in subsequent speeches may be re-
versed (Abercrombie and Batista-Navarro, 2018).
Identification of both the topics and polarity of
motions is therefore crucial for any further inves-
tigation of debates, and is likely to be a key step
in tasks such as sentiment or stance analysis of de-
bate speeches.
Our contributions We create a dataset of UK par-
liamentary debate motions labelled with both topic

1http://www.publicwhip.org.uk
2https://www.theyworkforyou.com/

http://www.publicwhip.org.uk
https://www.theyworkforyou.com/


281

and opinion polarity. We then investigate the util-
ity of these labels for two tasks:
1) For motion topic detection, we treat the policy
labels as topic classes and assess the performance
of a multilabel classifier in predicting them.
2) We exploit the fact that the policies used as
topic labels inherently incorporate information re-
garding the proposers’ opinions towards those
policies–that is their policy positions: whether
they support or oppose them. We investigate
whether, by correctly identifying a motion’s pol-
icy category, we can also determine its position
towards the policy in question, in effect obtain-
ing opinion analysis of the motions ‘for free’. We
compare the output of this approach to human pro-
duced opinion polarity labels.

2 Background

2.1 Hansard debate transcripts

Debates in the House of Commons are of the fol-
lowing format: An MP proposes a motion, to
which other MPs may respond when invited by the
Speaker (the presiding officer of the chamber), ei-
ther in support of, or opposition to the motion.

A domain with unique characteristics, the
Hansard transcripts lie somewhere between for-
mal written language and transcripts of spoken
dialogue–they are near-verbatim transcriptions of
almost everything that is said in Parliament, al-
though disfluencies are removed and some contex-
tual information (such as the names of the speak-
ers) is added by the parliamentary reporters.

There exist a number of challenges associated
with this domain. Here, analysis is complicated
by the language employed by politicians, who
tend to use: (1) little extreme or overtly polarised
(especially negative) language, and (2) a tactical,
political use of terminology–for example, poli-
cies that may be percieved as negative (such as
cuts to services or tax increases) are generally
not framed using those terms (Abercrombie and
Batista-Navarro, 2018).

Additionally, the format of debates is complex,
with manifold topics discussed by multiple par-
ticipants. Motions may reference various entities,
some of which may be described only within other
debates or documents referred to in the motion (as
in Figure 1).

Finally, the language used is often arcane, with
much procedural terminology. In fact, many mo-
tions consist entirely of such language, giving lit-

tle or no clue as to the topic under discussion (for
further details see Section 3).

However, the existence of motions that have
been manually labelled with ‘policy votes’ indi-
cates that it may be feasible to train machine clas-
sifiers to conduct a form of motion topic detection.
The fact that these labels also encompass policy
positions suggests that they could also be used si-
multaneously for opinion analysis.

2.2 Opinion-topic labelling

Parliamentary monitoring website the Public
Whip maintains a list of debates organised under
‘policies.’3 These are sets ‘of votes that repre-
sent a view on a particular issue’ such as Euro-
pean Union – For and Stop climate change. Under
each of these, members of the public are invited
to submit debates–motions with vote outcomes–
which match these descriptions.

We make use of these categorisations as la-
bels for supervised topic classification. In many
cases, it is not straightforward to determine a mo-
tion’s policy label from the debate title–for exam-
ple, for the policy ‘More Powers for Local Coun-
cils’, debate titles include ‘High Streets’, ‘Hous-
ing’, ‘Fixed Odds Betting Terminals’, ‘Local Bus
Services’. Similarly, the text of a motion alone
does not necessarily reveal its topic, with many
motions consisting purely of procedural language,
such as ‘That the Bill be read a Second time’.
As a result, human readers often require access to
the title, motion, and sometimes other information
found elsewhere in a debate in order to determine
the motion’s polarity.

While the policies represent both a policy topic
and a polarised position towards it, this is a reflec-
tion of the vote outcome of the debate, not nec-
essarily the position expressed in the motion. For
example, if a motion proposed in support of a pol-
icy position is rejected, it will be labelled with a
policy that reflects opposition to that position (see
Figure 2).

In a further layer of complexity, the Pub-
lic Whip also provides motions with a ‘policy
vote’ label–the contributors’ assessment of how
somebody who supports each policy ‘would have
voted’–with the additional tags ‘majority’, ‘minor-
ity’, or ‘abstain’. All in all, this means that each
label has two potential polarity shifters (the vote

3https://www.publicwhip.org.uk/
policies.php

https://www.publicwhip.org.uk/policies.php
https://www.publicwhip.org.uk/policies.php


282

Figure 2: Motion from the policy category Asylum Sys-
tem – More strict. The motion, from a debate entitled
‘Humanitarian Crisis in the Mediterranean and Eu-
rope’, opposes the idea of making the asylum system
stricter, but the fact that it was rejected by the House,
explains why it has been given this label.

outcome and the policy vote), which need to be
taken into account if the Public Whip policies are
to be used as labels for opinion polarity analysis.

3 Data

We present a dataset4 of 592 UK parliamentary de-
bate motions proposed in the House of Commons
between 1997 and 2018.5 We match these with the
corresponding policies from the Public Whip for
use as labels for supervised opinion-topic classifi-
cation. We therefore include only those motions,
which have been classified by policy on the Public
Whip website. In order to provide sufficient ex-
amples to train a classifier we use only those de-
bates for which there exist at least 20 examples
per policy label. Because, for example, a debate
may have been categorised with both the specific
policy ‘Higher taxes on alcoholic drinks’, as well
as the more general label ‘Increase VAT’ (Value
Added Tax), motions may have been included in
more than one policy category. The final dataset
includes 13 different policy topic labels, with each
applied to a minimum of 24 and a maximum of
129 motions (µ = 46.6). 14 of the motions have
two labels, while the remaining 578 have just one.

In addition to the Public Whip’s crowdsourced
4Available at https://data.mendeley.com/

datasets/j83yzp7ynz/1
5The transcripts were obtained from https://www.

theyworkforyou.com/pwdata/scrapedxml/
debates/

labels, we provide a second set of manually an-
notated opinion polarity labels. For these, annota-
tion was conducted by the first author of this paper,
who read each example (motion, title, and supple-
mentary information), and applied either positive
or negative labels according to the opinion they
perceived to be expressed towards the policy in
question.

As potential machine classification features, we
include the textual content of the motions as well
as the following metadata information from the
transcripts:

• motion speaker name: Some MPs are more
or less likely to speak on various topics, de-
pending on their interests and position.

• motion party: Party affiliation of speakers is
likely to be an indicator of both interest in
topics and policy positions.

• debate title: Titles are often, but not always,
related to policy vote topics.

• additional information: Information such as
the names of relevant documents or explana-
tions of amendments is often included in the
transcripts, preceding the motion.

Motions in this dataset broadly follow one of
the following three formats:

1. ‘That this House {verb} {argument}; {verb}
{argument}; ... and {verb} {argument}’
–where the motion may contain several
clauses (see examples in Figures 1 and 2).

2. That the {legislation} be now read {a Sec-
ond/the Third} time.
–where {legislation} is a Bill, Paper etc.

3. ...amendment {number}, page {number},
line {number}, leave out ‘{‘phrase’}’ and
insert ‘{phrase}’.

Motions of type 2 and 3 contain very little topic
information, so it may be necessary to make use
of cues in the debate title or additional information
provided in the transcript in order to determine the
topic in such cases.

https://data.mendeley.com/datasets/j83yzp7ynz/1
https://data.mendeley.com/datasets/j83yzp7ynz/1
https://www.theyworkforyou.com/pwdata/scrapedxml/debates/
https://www.theyworkforyou.com/pwdata/scrapedxml/debates/
https://www.theyworkforyou.com/pwdata/scrapedxml/debates/


283

4 Method

Data pre-processing consisted of removal of stop-
words, lowercasing and stemming of textual data,
and binarization of metadata information.

In order to detect the topics of debate motions
we employ a supervised machine classification ap-
proach. For this, we investigate the use of combi-
nations of the following features:
— Textual features: uni-, bi-, and trigrams from
the debate titles, motions and supplementary in-
formation.
— Metadata features: speaker name and party af-
filiation.

As some motions have more than one topic la-
bel, we apply one-vs-the-rest classification on a
randomised 90-10% train-test split of the data. Af-
ter initial experimentation with a range of algo-
rithms, we apply a multilabel implementation of
Support Vector Machine classification.

5 Results

Because we have 13 different classes, and there-
fore highly imbalanced datasets for each round of
one-vs-the-rest classification, we use the F1 score
as a performance metric. Strongest performance
is achieved using n-gram features from both the
debate motions and titles (F1 = 77.0).

0.0 0.2 0.4 0.6 0.8

F1 score

motion
title

motion + title
motion + title + additional

motion + title + premotion + party
motion + title + additional + name

motion + title + additional + party + name
motion + title + name

motion + party
motion + name

motion + party + name
title + party
title + name

title + party + name

Figure 3: F1 score for different combinations of fea-
tures used for topic classification. Highest perfor-
mances (lighter, orange bars) are achieved when textual
features derived from the debate titles are included.

Overall, use of the debate titles, with or without
metadata features, produces the highest F1 scores,
while the addition of other textual features does
not generally lead to improvement, and in some
cases results in losses in performance.

The motions themselves do not appear to pro-
vide particularly useful features for topic detec-

tion. Many consist solely of procedural terms that
give no indication of the topics under discussion–
such as motion types 2 and 3 (described in Section
3). Indeed, only 121 (20.8%) motions are of the
more informative type 1.

Of the metadata features used, speaker name
is more indicative of topic than party affiliation.
This reflects the fact that each party is represented
in most policy categories, but that individual MPs
tend to be strongly associated with just a few, or in
most cases, one single topic related to their partic-
ular role–of 234 MPs in the dataset, 163 (69.7%)
propose motions on only one policy, and only one
is represented in more than four.

Figure 4: F1 scores for classification with different
thresholds (θ) for the minimum number of example
motions. As this threshold is lowered, the number of
topic classes (n) increases and performance decreases.

As the threshold for the minumum number of
examples per policy in the dataset is somewhat ar-
bitary, we also test the system with a range of dif-
ferent thresholds. As the threshold decreases and
the number of different topic classes increases, the
F1 score drops, indicating that it may be challeng-
ing to obtain good results with a larger corpus and
a greater number of topics (Figure 4).

6 Discussion

6.1 Topic detection

Considering the small number of training exam-
ples for each class, reasonable results are obtained
using these labels for topic classification. How-
ever, it should be noted that many of the policy
classes in this dataset feature debates with similar
or even identical titles, in which cases the classifier
is trained and tested on very similar data. While
this is a common scenario in Parliament–the same
pieces of legislation are debated mutiple times and
often revisited year after year–it remains to be seen



284

how well this system would perform on new, com-
pletely unseen examples from future debates.

6.2 Opinion polarity analysis

As Public Whip Policies are created with inbuilt
policy positioning, we examine their use as opin-
ion labels by comparing their polarity with the sec-
ond set of manual annotations. We ignore cases
labelled in the Public Whip with the policy vote
‘abstain’ (as these are assumed not to take a posi-
tion towards the policy in question). We then treat
the ‘majority’ motions as being labelled according
to the vote outcome–those which were ‘approved’
by the vote are positive, while those which were
‘rejected’ are negative–and the ‘minority’ labels
as polarity shifters–that is, the tag ‘minority’ re-
verses the label derived from the outcome, while a
‘majority’ tag preserves it (see Table 1).

Outcome Policy Vote Opinion

Policy
Approved

‘majority’ positive
‘minority’ negative

Rejected
‘majority’ negative
‘minority’ positive

Table 1: Interpretation of Policy labels for opinion
analysis. For each of its policy labels, a motion also
has two tags–outcome and policy vote–that can poten-
tially reverse its opinion polarity.

To examine the utility of the output labels, we
calculate inter-rater agreement between these and
our own annotations, finding Cohen’s kappa (κ)
to be 94.2. This represents ‘near-perfect’ agree-
ment,6 indicating that the Public Whip’s policies
appear to be reliable labels for opinion position
of motions towards the policies in question. Al-
though these results are promising, it should be
noted that the system used to interpret motion
opinion from policies relies on the use of addi-
tional, manually applied policy vote tags. For use
with future, unseen examples that do not have such
tags, it would be necessary to reorganise the way
that the Public Whip’s policies are created, split-
ting those labelled ‘majority’ and ‘minority’ into
different for and against Policy categories.

7 Related work

The legislative debates domain has attracted inter-
est from researchers with a variety of backrounds,
and there is a considerable body of work related

6Interpretation of κ: (Landis and Koch, 1977).

to the analysis of both topics and speaker opinion
contained in parliamentary and congressional de-
bates, although these tasks have been tackled sep-
arately and from differing research perspectives.

For opinion analysis of US congressional de-
bates, the dataset of Thomas et al. (2006) has been
widely used (e.g. Balahur et al., 2009; Burfoot
et al., 2011), and similar experiments have also
been conducted on other legislatures such as the
Dutch parliament (Grijzenhout et al., 2010), and
the UK House of Commons (Salah, 2014).

Others have utilised similar techniques to facil-
itate other tasks. For example, Duthie et al. (2016)
attempt to identify the ‘ethos’ of speakers in the
UK Parliament, while Li et al. (2017) detect polit-
ical ideology in those of the US Congress. Mean-
while, political scientists, such as Proksch and
Slapin (2010) and Lauderdale and Herzog (2016)
have analysed debates to position speakers on a
range of scales related to policy and ideology.

While most work on this domain focuses on
speeches, ignoring the role of motions in shaping
the content of debates, Abercrombie and Batista-
Navarro (2018) include analysis of the sentiment
expressed in debate motions. However, they do
not analyse the topics or identify the targets of sen-
timent in the motions.

Analysis of the topics contained within legisla-
tive debates has primarily focused on topic mod-
elling based on speech content. For example,
van der Zwaan et al. (2016) combine topic and
political position analysis on Dutch parliamentary
speech transcripts, while Zirn (2014) do similar
for the German Bundestag. As far as we are aware,
there exists no previous work on extracting topics
from debate motions.

8 Conclusion

Opinion-topic labels derived from the Public
Whip’s policies can be used to train a classifier to
achieve good performance in classifying the pol-
icy topics of parliamentary debate motions. These
categories, which incorporate inbuilt policy posi-
tion information, also appear to be reliable mark-
ers of opinion polarity, suggesting that we can use
these labels to simultaneously obtain opinion anal-
ysis ‘for free’. However, the ability of this ap-
proach to deal with new examples from future, un-
seen debates is uncertain, and it may be advisable
to explore unsupervised methods of determining
motion opinion-topics.



285

References
Gavin Abercrombie and Riza Batista-Navarro. 2018.

‘Aye’ or ‘no’? Speech-level sentiment analysis of
Hansard UK parliamentary debate transcripts. In
Proceedings of the Eleventh International Confer-
ence on Language Resources and Evaluation (LREC
2018), Miyazaki, Japan. European Language Re-
sources Association (ELRA).

Alexandra Balahur, Zornitsa Kozareva, and Andrés
Montoyo. 2009. Determining the polarity and
source of opinions expressed in political debates. In
Computational Linguistics and Intelligent Text Pro-
cessing. CICLing 2009. Lecture Notes in Computer
Science, pages 468–480. Springer.

Clinton Burfoot, Steven Bird, and Timothy Baldwin.
2011. Collective classification of congressional
floor-debate transcripts. In Proceedings of the 49th
Annual Meeting of the Association for Computa-
tional Linguistics: Human Language Technologies,
pages 1506–1515. Association for Computational
Linguistics.

Rory Duthie, Katarzyna Budzynska, and Chris Reed.
2016. Mining ethos in political debate. In Compu-
tational Models of Argument: Proceedings from the
Sixth International Conference on Computational
Models of Argument (COMMA), pages 299–310.
IOS Press.

Steven Grijzenhout, Valentin Jijkoun, Maarten Marx,
et al. 2010. Opinion mining in Dutch Hansards. In
Proceedings of the Workshop From Text to Political
Positions, Free University of Amsterdam.

J Richard Landis and Gary G Koch. 1977. The mea-
surement of observer agreement for categorical data.
Biometrics, 4(1):159–174.

Benjamin E Lauderdale and Alexander Herzog.
2016. Measuring political positions from legislative
speech. Political Analysis, 24(3):374–394.

Xilian Li, Wei Chen, Tengjiao Wang, and Weijing
Huang. 2017. Target-specific convolutional bi-
directional lstm neural network for political ideol-
ogy analysis. In Asia-Pacific Web (APWeb) and
Web-Age Information Management (WAIM) Joint
Conference on Web and Big Data, pages 64–72.
Springer.

Sven-Oliver Proksch and Jonathan B Slapin. 2010.
Position taking in European Parliament speeches.
British Journal of Political Science, 40(3):587–611.

Zaher Salah. 2014. Machine learning and sentiment
analysis approaches for the analysis of parliamen-
tary debates. Ph.D. thesis, University of Liverpool.

Matt Thomas, Bo Pang, and Lillian Lee. 2006. Get out
the vote: Determining support or opposition from
congressional floor-debate transcripts. In Proceed-
ings of the 2006 Conference on Empirical Methods
in Natural Language Processing, pages 327–335.
Association for Computational Linguistics.

Cäcilia Zirn. 2014. Analyzing positions and topics
in political discussions of the German Bundestag.
In Proceedings of the ACL 2014 Student Research
Workshop, pages 26–33. Association for Computa-
tional Linguistics.

Janneke M van der Zwaan, Maarten Marx, and Jaap
Kamps. 2016. Validating cross-perspective topic
modeling for extracting political parties’ positions
from parliamentary proceedings. In Proceedings of
ECAI: 22nd European Conference on Artificial In-
telligence, pages 28–36. IOS Press.


