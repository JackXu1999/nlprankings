



















































Exploring Pre-trained Language Models for Event Extraction and Generation


Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 5284–5294
Florence, Italy, July 28 - August 2, 2019. c©2019 Association for Computational Linguistics

5284

Exploring Pre-trained Language Models for Event Extraction and
Generation

Sen Yang†, Dawei Feng†, Linbo Qiao, Zhigang Kan, Dongsheng Li‡
National University of Defense Technology, Changsha, China

{sen yang,linbo.qiao,kanzhigang13}@nudt.edu.cn
davyfeng.c@gmail.com, lds1201@163.com

Abstract

Traditional approaches to the task of ACE
event extraction usually depend on manually
annotated data, which is often laborious to cre-
ate and limited in size. Therefore, in addi-
tion to the difficulty of event extraction itself,
insufficient training data hinders the learning
process as well. To promote event extraction,
we first propose an event extraction model to
overcome the roles overlap problem by sep-
arating the argument prediction in terms of
roles. Moreover, to address the problem of in-
sufficient training data, we propose a method
to automatically generate labeled data by edit-
ing prototypes and screen out generated sam-
ples by ranking the quality. Experiments on
the ACE2005 dataset demonstrate that our ex-
traction model can surpass most existing ex-
traction methods. Besides, incorporating our
generation method exhibits further significant
improvement. It obtains new state-of-the-art
results on the event extraction task, including
pushing the F1 score of trigger classification to
81.1%, and the F1 score of argument classifi-
cation to 58.9%.

1 Introduction

Event extraction is a key and challenging task for
many NLP applications. It targets to detect event
trigger and arguments. Figure 1 illustrates a sen-
tence containing an event of type Meet triggered
by ”meeting”, with two arguments: ”President
Bush” and ”several Arab leaders”, both of which
play the role ”Entity”.

There are two interesting issues in event ex-
traction that require more efforts. On the one
hand, roles in an event vary greatly in frequency
(Figure 2), and they can overlap on some words,

†These two authors contributed equally.
‡Corresponding Author.

[Trigger]
Event type: Meet
Sentence  : President Bush           is going to be meeting 

                   with several Arab leaders 

[Entity]

[Entity]

Figure 1: An event of type Meet is highlighted in the
sentence, including one trigger and two arguments.

even sharing the same argument (the roles over-
lap problem). For example, in sentence ”The
explosion killed the bomber and three shoppers”,
”killed” triggers an Attack event, while argument
”the bomber” plays the role ”Attacker” as well
as the role ”Victim” at the same time. There are
about 10% events in the ACE2005 dataset (Dod-
dington et al., 2004) having the roles overlap prob-
lem. However, despite the evidence of the roles
overlap problem, few attentions have been paid to
it. On the contrary, it is often simplified in evalu-
ation settings of many approaches. For example,
in most previous works, if an argument plays mul-
tiple roles in an event simultaneously, the model
classifies correctly as long as the prediction hits
any one of them, which is obviously far from ac-
curate to apply to the real world. Therefore, we
design an effective mechanism to solve this prob-
lem and adopt more rigorous evaluation criteria in
experiments.

On the other hand, so far most deep learn-
ing based methods for event extraction follow the
supervised-learning paradigm, which requires lots
of labeled data for training. However, annotating
accurately large amounts of data is a very labo-
rious task. To alleviate the suffering of existing
methods from the deficiency of predefined event
data, event generation approaches are often used
to produce additional events for training (Yang
et al., 2018; Zeng et al., 2018; Chen et al., 2017).
And distant supervision (Mintz et al., 2009) is a
commonly used technique to this end for label-
ing external corpus. But the quality and quantity



5285

 0
 0.1
 0.2
 0.3
 0.4
 0.5
 0.6
 0.7

Victim Place Agent Instrument Time

Fr
eq

ue
nc

y

Figure 2: Frequency of roles that appear in events of
type Injure in the ACE2005 dataset.

of events generated with distant supervision are
highly dependent on the source data. In fact, ex-
ternal corpus can also be exploited by pre-trained
language models to generate sentences. Therefore,
we turn to pre-trained language models, attempt-
ing to leverage their knowledge learned from the
large-scale corpus for event generation.

Specifically, this paper proposes a framework
based on pre-trained language models, which in-
cludes an event extraction model as our baseline
and a labeled event generation method. Our pro-
posed event extraction model is constituted of a
trigger extractor and an argument extractor which
refers result of the former for inference. In addi-
tion, we improve the performance of the argument
extractor by re-weighting the loss function based
on the importance of roles.

Pre-trained language models have also been ap-
plied to generating labeled data. Inspired by the
work of Guu et al. (2018), we take the existing
samples as prototypes for event generation, which
contains two key steps: argument replacement and
adjunct token rewriting. Through scoring the qual-
ity of generated samples, we can pick out those
of high quality. Incorporating them with existing
data can further improve the performance of our
event extractor.

2 Related work

Event Extraction In terms of analysis granularity,
there are document-level event extraction (Yang
et al., 2018) and sentence-level event extraction
(Zeng et al., 2018). We focus on the statistical
methods of the latter in this paper. These meth-
ods can be further divided into two detailed cat-
egories: the feature based ones (Liao and Grish-
man, 2010; Liu et al., 2010; Miwa et al., 2009; Liu
et al., 2016; Hong et al., 2011; Li et al., 2013b)
which track designed features for extraction, and
the neural based ones that take advantage of neu-
ral networks to learn features automatically (Chen

et al., 2015; Nguyen and Grishman, 2015; Feng
et al., 2016).

Event Generation External resources such as
Freebase, Frame-Net and WordNet are commonly
employed to generate event and enrich the train-
ing data. Several previous event generation ap-
proaches (Chen et al., 2017; Zeng et al., 2018)
base a strong assumption in distant supervision1

to label events in unsupervised corpus. But in fact,
co-occurring entities could have none expected re-
lationship. In addition, Huang et al. (2016) incor-
porates abstract meaning representation and distri-
bution semantics to extract events. While Liu et al.
(2016, 2017) manages to mine additional events
from the frames in FrameNet.

Pre-trained Language Model Pre-trained lan-
guage models are capable of capturing the mean-
ing of words dynamically in consideration of their
context. McCann et al. (2017) exploits language
model pre-trained on supervised translation corpus
in the target task. ELMO (Embeddings from Lan-
guage Models) (Peters et al., 2018) gets context
sensitive embeddings by encoding characters with
stacked bidirectional LSTM (Long Short Term
Memory) and residual structure (He et al., 2016).
Howard and Ruder (2018) obtains comparable re-
sult on text classification. GPT (Generative Pre-
Training) (Radford et al., 2018) improves the state
of the art in 9 of 12 tasks. BERT (Bidirectional
Encoder Representations from Transformers) (De-
vlin et al., 2018) breaks records of 11 NLP task
and received a lot of attention.

3 Extraction Model

This section describes our approach to extract
events that occur in plain text. We consider event
extraction as a two-stage task, which includes trig-
ger extraction and argument extraction, and pro-
pose a Pre-trained Language Model based Event
Extractor (PLMEE). Figure 3 illustrates the archi-
tecture of PLMEE. It consists of a trigger extractor
and an argument extractor, both of which rely on
the feature representation of BERT.

3.1 Trigger Extractor

Trigger extractor targets to predict whether a token
triggers an event. So we formulate trigger extrac-
tion as a token-level classification task with labels

1If two entities have a relationship in a knowledge base,
then all sentences that mention these two entities will express
that relationship.



5286

killed
explosion

the
bomber

and
three

shoppers

BERT

Em
bedding

Classifier

Conflict.Attack

Trigger 

The explosion killed the bomber and three shoppers 

BERT

Em
bedding

Attacker Victim Place
Cstart Cend Cstart Cend Cstart Cend Cstart Cend...

0 1 0 1 0 1 0 1 0 1 0 10 1 0 1

Argument

Loss

Classifier Set

Attacker
Victim
Place... Role Importance

killed
explosion

the
bomber

and
three

shoppers

For inference

The explosion killed the bomber and three shoppers 

WordPiece Segment Position WordPiece Segment Position

The The

Figure 3: Illustration of the PLMEE architecture, including a trigger extractor and an argument extractor. The
processing procedure of an event instance triggered by the word ”killed” is also shown.

being event types, and just add a multi-classifier
on BERT to build the trigger extractor.

The input of the trigger extractor follows the
BERT, i.e. the sum of three types of embed-
dings, including WordPiece embedding (Wu et al.,
2016), position embedding and segment embed-
ding. Since the input contains only one sentence,
all its segment ids are set to zero. In addition, to-
ken [CLS] and [SEP]2 are placed at the start and
end of the sentence.

In many cases, the trigger is a phrase. There-
fore, we treat consecutive tokens which share the
same predicted label as a whole trigger. As gen-
eral, we adopt cross entropy as the loss function
for fine-tuning.

3.2 Argument Extractor

Given the trigger, argument extractor aims to ex-
tract related arguments and all roles they play.
Compared with trigger extraction, argument ex-
traction is more complicated because of three is-
sues: the dependency of arguments on the trigger,
most arguments being long noun phrases, and the
roles overlap problem. We take exactly a series of
actions to deal with these obstacles.

In common with trigger extractor, argument ex-
tractor requires three kinds of embeddings as well.
However, it needs to know which tokens comprise
the trigger. Therefore, we feed argument extractor
with the segment ids of trigger tokens being one.

2[CLS], [SEP] and [MASK] are special tokens of BERT.

To overcome the latter two issues in argument
extraction, we add multiple sets of binary classi-
fiers on the BERT. Each set of classifiers sever for
a role to determine the spans (each span includes a
start and an end) of all arguments that play it. This
approach is similar to the question answering task
on the SQuAD (Rajpurkar et al., 2016) in which
there is only one answer, while multiple arguments
playing the same role can appear simultaneously
in an event. Since the prediction is separated with
roles, an argument can play multiple roles, and a
token can belong to different arguments. Thus, the
roles overlap problem can also be solved.

3.3 Argument Span Determination
In PLMEE, a token t is predicted as the start of an
argument that plays role r with probability:

P rs (t) = Softmax (W
r
s · B (t)) ,

while as the end with probability:

P re (t) = Softmax (W
r
e · B (t)) ,

in which we use subscript ”s” to represent ”start”
and subscript ”e” to represent ”end”. W rs is the
weight of binary classifier that aims to detect starts
of arguments playing role r, while W re is the
weight of another binary classifier that aims to de-
tect ends. B is the BERT embedding.

For each role r, we can get two lists Brs and B
r
e

of 0 and 1 according to P rs and P
r
e . They indicate

respectively whether a token in the sentence is the



5287

start or end of an argument that plays role r3. Al-
gorithm 1 is used to detect each token sequentially
to determine spans of all arguments that play the
role r.

Algorithm 1 Argument span determination

In: P rs and P re , Brs and Bre , sentence length l.
Out: Span list L of the arguments that play role r
Initiate: as ←-1, ae ←-1

1: for i← 0 to l do
2: if In State 1 & the ith token is a start then
3: as ← i and change to State 2
4: end if
5: if In State 2 then
6: if the ith token is a new start then
7: as ← i if P rs [i] > P rs [as]
8: end if
9: if the ith token is an end then

10: ae ← i and change to State 3
11: end if
12: end if
13: if In State 3 then
14: if the ith token is a new end then
15: ae ← i if P re [i] > P re [ae]
16: end if
17: if the ith token is a new start then
18: Append [as, ae] to L
19: ae ← -1, as ← i and change to State 2
20: end if
21: end if
22: end for

Algorithm 1 contains a finite state machine,
which changes from one state to another in re-
sponse to Brs and B

r
e . There are three states to-

tally: 1) Neither start nor end has been detected;
2) Only a start has been detected; 3) A start as well
as an end have been detected. Specially, the state
changes according to the following rules: State 1
changes to State 2 when the current token is a start;
State 2 changes to State 3 when the current token
is an end; State 3 changes to State 2 when the cur-
rent token is a new start. Notably, if there has been
a start and another start arises, we will choose the
one with higher probability, and the same for end.

3.4 Loss Re-weighting
We initially define Ls as the loss function of all
binary classifiers that are responsible for detect-
ing starts of arguments. It is the average of cross

3The ith token is a start if Brs [i]=1 or an end if Bre [i]=1.

entropy between the output probabilities and the
golden label y:

Ls =
1

|R| × |S|
∑
r∈R

CE (P rs ,y
r
s) ,

in which CE is cross entropy,R is the set of roles,
S is the input sentence, and |S| is the number of
tokens in S. Similarly, we define Le as the loss
function of all binary classifiers that detect ends:

Le =
1

|R| × |S|
∑
r∈R

CE (P re ,y
r
e) .

We finally average Ls and Le as the loss L of ar-
gument extractor.

As Figure 2 shows, there exists a big gap in fre-
quency between roles. This implies that roles have
different levels of ”importance” in an event. The
”importance” here means the ability of a role to
indicate events of a specific type. For example,
the role ”Victim” is more likely to indicate a Die
event than the role ”Time”. Inspired by this, we
re-weight Ls and Le according to the importance
of roles, and propose to measure the importance
with the following definitions:

Role Frequency (RF) We define RF as the fre-
quency of role r appearing in events of type v:

RF(r, v) =
N rv∑

k∈RN
k
v

,

where N rv is the count of the role r that appear in
the events of type v.

Inverse Event Frequency (IEF) As the mea-
sure of the universal importance of a role, we de-
fine IEF as the logarithmically scaled inverse frac-
tion of the event types that contain the role r:

IEF(r) = log
|V|

|{v ∈ V : r ∈ v}|
,

where V is tht set of event types.
Finally we take RF-IEF as the product of RF

and IEF: RF-IEF(r, v) = RF(r, v)× IEF(r). With
RF-IEF, we can measure the importance of a role
r in events of type v:

I(r, v) =
expRF-IEF(r,v)∑

r′∈R exp
RF-IEF(r′,v) .

We choose three event types and list the two
most important roles of each type in Table 1. It
shows that although there could be multiple roles



5288

Event Type Top 2 Roles Sum
Transport(15) Artifact, Origin 0.76

Attack(14) Attacker, Target 0.85
Die(12) Victim, Agent 0.90

Table 1: Top two roles and their sum importance for
each event type. The number in brackets behind event
type is the count of roles that have appeared in it.

in events of someone type, only a few of them is
indispensable.

Give the event type v of input, we re-weight Ls
and Le based on each role’s importance in v:

Ls =
∑
r∈R

I(r, v)

|S|
CE (P rs ,y

r
s)

Le =
∑
r∈R

I(r, v)

|S|
CE (P re ,y

r
e) .

The loss of argument extractor L is still the aver-
age of Ls and Le.

4 Training Data Generation

In addition to PLMEE, we also propose a pre-
trained language model based method for event
generation as illustrated in Figure 4. By edit-
ing prototypes, this method can generate a con-
trollable number of labeled samples as the extra
training corpus. It consists of three stages: pre-
processing, event generation and scoring.

To facilitate the generation method, we define
adjunct tokens as the tokens in sentences except
triggers and arguments, including not only words
and numbers, but also punctuation. Taking sen-
tence in Figure 1 as an example, ”is” and ”going”
are adjunct tokens. It is evident that adjunct tokens
can adjust the smooth and diversity of expression.
Therefore, we try to rewrite them to expand the di-
versity of the generation results, while keeping the
trigger and arguments unchanged.

4.1 Pre-processing
With the golden labels, we first collect arguments
in the ACE2005 dataset as well as the roles they
play. However, those arguments overlap with oth-
ers are excluded. Because such arguments are of-
ten long compound phrases that contain too much
unexpected information, and incorporating them
in argument replacement could bring more unnec-
essary errors.

We also adopt BERT as the target model to
rewrite adjunct tokens in the following stage, and

fine-tune it on the ACE2005 dataset with the
masked language model task (Devlin et al., 2018)
to bias its prediction towards the dataset distribu-
tion. In common with the pre-training procedure
of BERT, each time we sample a batch of sen-
tences and mask 15% of tokens. Its goal is still
to predict the correct token without supervision.

4.2 Event generation

To generate events, we conduct two steps on a pro-
totype. We first replace the arguments in the proto-
type with those similar that have played the same
role. Next, we rewrite adjunct tokens with the fine-
tuned BERT. Through these two steps, we can ob-
tain a new sentence with annotations.

Argument Replacement The first step is to re-
place arguments in the event. Both the argument
to be replaced and the new one should have played
ever the same role. While the roles are inherited
after replacement, so we can still use origin labels
for the generated samples.

In order not to change the meaning drastically,
we employ similarity as the criteria for selecting
new arguments. It is based on the following two
considerations: one is that two arguments that play
the same role may diverge significantly in seman-
tics; another is that the role an argument plays
is largely dependent on its context. Therefore,
we should choose arguments that are semantically
similar and coherent with the context.

We use cosine similarity between embeddings
to measure the similarity of two arguments. And
due to ELMO’s ability to handle the OOV prob-
lem, we employ it to embed arguments:

E(a) = 1
|a|
∑
t∈a
E(t),

where a is the argument, E is ELMO embedding.
We choose the top 10 percent most similar argu-
ments as candidates, and use softmax operation on
their similarity to allocate probability.

An argument is replaced with probability 80%
while keeping constant with probability 20% to
bias the representation towards the actual event
(Devlin et al., 2018). Note that the triggers remain
unchanged to avoid undesirable deviation of de-
pendency relation.

Adjunct Token Rewriting The results of argu-
ment replacement can already be considered as the
generated data, but the constant context may in-
crease the risk of overfitting. Therefore, to smooth



5289

Dataset

BERT

Argument
Collection

Fine-tuning BERT

Argument 
Replacement

Adjunct Token
Rewriting

Scorer

Out: Prime minister Blair is reported to the
meeting with the leaders

Stage  1: Pre-processing Stage  2: Event generation Stage  3: Scoring

Quality: 0.5

In: President Bush is going to be meeting 
with several Arab leaders

Entity
1. President
2. Prime minister Blair
3. the prime minister
4. the Arab leaders 
5. an Arab counterpart
6. the Palestinians
7. the leaders
8. ...

Prime minister Blair is going to be meeting 
with the leaders

Figure 4: Flow chart of the generation approach.

the generated data and expand their diversity, we
manage to rewrite adjunct tokens with the fine-
tuned BERT.

The rewriting is to replace some adjunct tokens
in the prototype with the new ones that are more
matchable with the current context. We take it as
a Cloze task (Taylor, 1953), where some adjunct
tokens are randomly masked and the BERT fine
tuned in the first stage is used to predict vocabulary
ids of suitable tokens based on the context. We use
a parameter m to denote the proportion of adjunct
tokens that need to be rewritten.

Adjunct token rewriting is a step-by-step pro-
cess. Each time we mask 15% of adjunct tokens
(with the token [MASK]). Then the sentence is fed
into BERT to produce new adjunct tokens. The ad-
junct tokens that have not yet been rewritten will
temporarily remain in the sentence.

To further illustrate the above two steps, we give
an instance in Figure 4. In this instance, we set
m to 1.0, which means all the adjunct tokens will
be rewritten. The final output is ”Prime minister
Blair is reported to the meeting with the leaders”,
which shares the labels with the original event in
the prototype. It is evident that some adjunct to-
kens are preserved despite m is 1.0.

4.3 Scoring

Theoretically, infinite number of events can be
generated with our generation method. However,
not all of them are valuable for the extractor and
some may even degrade its performance. There-
fore, we add an extra stage to quantify the quality
of each generated sample to pick out those valu-
able. Our key insight for evaluating the quality
lies that it is tightly related to two factors, which
are the perplexity and the distance to the original
dataset. The former reflects the rationality of gen-

eration, and the latter reflects the differences be-
tween the data.

Perplexity (PPL) Different with the masked
perplexity (Devlin et al., 2018) of logarithmic ver-
sion, we take the average probability of those ad-
junct tokens that have been rewritten as the per-
plexity of generated sentence S ′:

PPL(S ′) = 1
|A(S ′)|

∑
t∈A(S′)

P (t),

whereA is the set of adjunct tokens in S ′ that have
been rewritten.

Distance (DIS) We measure the distance be-
tween S ′ and the dataset D with cosine similarity:

DIS(S ′,D) = 1− 1
|D|

∑
S∈D

B(S ′) · B(S)
|B(S ′)| × |B(S)|

.

Different with embedding arguments by ELMO,
we utilize BERT to embed sentence and take the
embedding of the first token [CLS] as the sentence
embedding.

Both the PPL and the DIS are limited in [0,1].
We consider that generated samples of high qual-
ity should have both low PPL and DIS. Therefore,
we define the quality function as:

Q(S ′) = 1−
(
λPPL

(
S ′
)
+ (1− λ)DIS

(
S ′,D

))
, where λ ∈ [0, 1] is the balancing parameter. This
function is used to select generated samples of
high quality in experiments.

5 Experiments

In this section, we first evaluate our event extractor
PLMEE on the ACE2005 dataset. Then we give a
case study of generated samples and conduct au-
tomatic evaluations by adding them into the train-
ing set. Finally, we illustrate the limitations of the
generation method.



5290

Model

Phase Trigger Trigger Argument Argument
Identification(%) Calssfication(%) Identification(%) Calssfication(%)

P R F P R F P R F P R F
Cross Event N/A 68.7 68.9 68.8 50.9 49.7 50.3 45.1 44.1 44.6
Cross Entity N/A 72.9 64.3 68.3 53.4 52.9 53.1 51.6 45.5 48.3
Max Entropy 76.9 65.0 70.4 73.7 62.3 67.5 69.8 47.9 56.8 64.7 44.4 52.7
DMCNN 80.4 67.7 73.5 75.6 63.6 69.1 68.8 51.9 59.1 62.2 46.9 53.5
JRNN 68.5 75.7 71.9 66.0 73.0 69.3 61.4 64.2 62.8 54.2 56.7 55.4
DMCNN-DS 79.7 69.6 74.3 75.7 66.0 70.5 71.4 56.9 63.3 62.8 50.1 55.7
ANN-FN N/A 79.5 60.7 68.8 N/A N/A
ANN-AugATT N/A 78.0 66.3 71.7 N/A N/A
PLMEE(-)

84.8 83.7 84.2 81.0 80.4 80.7 71.5 59.2 64.7 61.7 53.9 57.5
PLMEE 71.4 60.1 65.3 62.3 54.2 58.0

Table 2: Performance of all methods. Bold denotes the best result.

As previous works (Li et al., 2013b; Chen et al.,
2015; Hong et al., 2011), we take the test set
with 40 newswire documents, while 30 other doc-
uments as the validation set, and the remaining
529 documents to be the training set. However,
different with previous works, we take the follow-
ing criteria to evaluate the correctness of each pre-
dicted event mention:

1. A trigger prediction is correct only if its span
and type match with the golden labels.

2. An argument prediction is correct only if its
span and all roles it plays match with the
golden labels.

It is worth noting that all the predicted roles for
an argument are required to match with the golden
labels, instead of just one of them. We adopt Pre-
cision (P), Recall (R) and F measure (F1) as the
evaluation metrics.

5.1 Results of Event Extraction

We take several previous classic works for com-
parison, and divide them into three categories:

Feature based methods Document-level infor-
mation is utilized in Cross event (Liao and Gr-
ishman, 2010) to assist event extraction. While
Cross entity (Hong et al., 2011) uses cross-entity
inference in extraction. Max Extropy (Li et al.,
2013a) extracts triggers as well as arguments to-
gether based on structured prediction.

Neural based methods DMCNN (Chen et al.,
2015) adopts firstly dynamic multi-pooling CNN
to extract sentence-level features automatically.
JRNN (Nguyen et al., 2016) proposes a joint

framework based on bidirectional RNN for event
extraction.

External resource based methods DMCNN-
DS (Chen et al., 2017) uses FreeBase to label
potential events in unsupervised corpus by dis-
tance supervision. ANN-FN (Liu et al., 2016)
improves extraction with additionally events au-
tomatically detected from FrameNet, while ANN-
AugATT (Liu et al., 2017) exploits argument infor-
mation via the supervised attention mechanisms to
improve the performance further.

In order to verify the effectiveness of loss re-
weighting, two groups of experiments are con-
ducted for comparison. Namely, the group where
the loss function is simply averaged on all clas-
sifiers’ output (indicated as PLMEE(-)) and the
group where the loss is re-weighted based on role
importance (indicated as PLMEE).

Table 2 compares the results of the aforemen-
tioned models with PLMEE on the test set. As is
shown, in both the trigger extraction task and the
argument extraction task, PLMEE(-) has achieved
the best results among all the compared meth-
ods. The improvement on the trigger extraction
is quite significant, seeing a sharp increase of near
10% on the F1 score. While the improvement in
argument extraction is not so obvious, achieving
about 2%. This is probably due to the more rigor-
ous evaluation metric we have taken and the diffi-
culty of argument extraction task as well. More-
over, compared with feature based methods, neu-
ral based methods can achieve better performance.
And the same observation appears when compar-
ing external resource based methods with neural
based methods. It demonstrates that external re-



5291

Prototype m Generated Event

President Bush is
going to be meeting

with several Arab
leaders

0.2 Russian President Putin is going to the meeting with the Arab leaders
0.4 The president is reported to be meeting with an Arab counterpart
0.6 Mr. Bush is summoned to a meeting with some Shiite Muslim groups
0.8 The president is attending to the meeting with the Palestinians
1.0 Prime minister Blair is reported to the meeting with the leaders

Table 3: Example samples generated with different proportion of rewritten adjunct tokens. Italic indicates argument
and bold indicates trigger.

sources are useful to improve event extraction. In
addition, the PLMEE model can achieve better re-
sults on the argument extraction task - with im-
provement of 0.6% on F1 score for identification
and 0.5% for classification - than the PLMEE(-)
model, which means that re-weighting the loss can
effectively improve the performance.

5.2 Case Study

Table 3 illustrates a prototype and its generation
with parameter m ranging from 0.2 to 1.0. We
can observe that the arguments after replacement
can match the context in prototype relatively well,
which indicates that they are resembling with the
original ones in semantic.

On the other hand, rewriting the adjunct tokens
can smooth the generated data and expand their di-
versity. However, since there is no explicit guide,
this step can also introduce unpredictable noise,
making the generation not fluent as expected.

5.3 Automatic Evaluation of Generation

So far, there are mainly three aspects of the gen-
eration method that could have significant impacts
on the performance of the extraction model, in-
cluding the amount of generated samples (repre-
sented by n, which indicates times the generation
size is the number of dataset size), the proportion
of rewritten adjunct tokens m, and the quality of
the generated samples. The former two factors
are controllable in the generation process. Spe-
cially, we can reuse a prototype and get a variety of
combinations of arguments via similarity based re-
placement, which will bring different contexts for
rewriting adjunct tokens. Moreover, the propor-
tion of rewritten adjunct tokens can be adjusted,
making a further variation. Although the quality of
generation cannot be controlled arbitrarily, it can
be quantified by the score function Q so that those
samples of higher quality can be picked out and
added into the training set. With λ in Q changing,

different selection strategies can be used to screen
out the generated samples.

We first tuned the former two parameters on the
development set through grid search. Specially,
we set m ranging from 0.2 to 1.0 with an interval
of 0.2, and set n to be 0.5, 1.0 and 2.0, while keep-
ing other parameters unchanged in the generation
process. We conduct experiments with these pa-
rameters. By analyzing the results, we find that the
best performance of PLMEE on both trigger ex-
traction and argument extraction can be achieved
with m = 0.4 and n = 1.0. It suggests that nei-
ther too few generated samples nor too much is a
better choice for extraction. Too few has limited
influence, while too much could bring more noise
that disturbs the distribution of the dataset. For the
better extraction performance, we use such param-
eter settings in the following experiments.

We also investigate the effectiveness of the
sample selection approach, a comparison is con-
ducted between three groups with different selec-
tion strategies. We obtain a total of four times the
size of the ACE2005 dataset using our generation
method with m = 0.4, and pick out one quarter of
them (n = 1.0) with λ being 0, 0.5 and 1.0 respec-
tively. When λ is 0 or 1.0, it is either perplexity
or distance that determines the quality exclusively.
We find that the selection method with λ = 0.5
in quality function is able to pick out samples that
are more advantageous to promote the extraction
performance.

Model Trigger(%) Argument(%)
PLMEE 80.7 58.0

PLMEE(+) 81.1 58.9

Table 4: F1 score of trigger classification and argument
classification on the test set.

Finally, we incorporate the above generated
data with the ACE2005 dataset and investigate the
effectiveness of our generation method on the test



5292

set. In Table 4, we use PLMEE(+) denotes the
PLMEE model trained with extra generated sam-
ples. The results illustrate that with our event gen-
eration method, the PLMEE model can achieve the
state of the art result of event extraction.

5.4 Limitation

By comparing the annotations in generated sam-
ples and manually labeled samples, we find that
one issue of our generation method is that the roles
may deviate, because the semantics could change
a lot with only a few adjunct tokens been rewritten.
Taking Figure 5 as an example. The roles played
by argument ”Pittsburgh” and ”Boston” should be
”Destination” and ”Origin”, rather not the oppo-
site as in the prototype. This is because the to-
ken ”from” has been replaced with the token ”for”,
while token ”drive to” been replaced with ”return
from”.

Trigger leave
Event type Movement.Transport
Arguments Niagara Falls Toronto
Roles Origin Destination

Trigger leave
Event type Movement.Transport
Arguments Pittsburgh Boston
Roles Origin Destination

Prototype: Leave from Niagara Falls and drive to Toronto, on 85 miles

Generation: Leave for Pittsburgh and return from Boston in 200 miles

x x

✓

Figure 5: One of the generated samples with wrong
annotations.

6 Conclusion and Discussion

In this paper, we present a framework to promote
event extraction by using a combination of an ex-
traction model and a generation method, both of
which are based on pre-trained language models.
To solve the roles overlap problem, our extraction
approach tries to separate the argument predictions
in terms of roles. Then it exploits the importance
of roles to re-weight the loss function. To perform
event generation, we present a novel method that
takes the existing events as prototypes. This event
generation method can produce controllably la-
beled samples through argument replacement and
adjunct tokens rewriting. It also benefits from the
scoring mechanism which is able to quantify the
quality of generated samples. Experimental re-
sults show that the quality of generated data is
competitive and incorporating them with existing
corpus can make our proposed event extractor to
be superior to several state of the art approaches.

On the other hand, there are still limitations in
our work. Events of the same type often share sim-
ilarity. And co-occurring roles tend to hold a tight
relation. Such features are ignored in our model,
but they deserve more investigation for improving
the extraction model. In addition, although our
generation method can control the number of gen-
erated samples and filter with quality, it still suf-
fers the deviation of roles alike with distant super-
vision. Therefore, for the future work, we will in-
corporate relation between events and relation be-
tween arguments into pre-trained language mod-
els, and take effective measures to overcome the
deviation problem of roles in the generation.

Acknowledgments

The work was sponsored by the National Key Re-
search and Development Program of China under
Grant No.2018YFB0204300, and National Nat-
ural Science Foundation of China under Grant
No.61872376 and No.61806216.

References
Yubo Chen, Shulin Liu, Xiang Zhang, Kang Liu, and

Jun Zhao. 2017. Automatically labeled data gener-
ation for large scale event extraction. In Proceed-
ings of the 55th Annual Meeting of the Association
for Computational Linguistics (Volume 1: Long Pa-
pers), volume 1, pages 409–419.

Yubo Chen, Liheng Xu, Kang Liu, Daojian Zeng,
and Jun Zhao. 2015. Event extraction via dy-
namic multi-pooling convolutional neural networks.
In Proceedings of the 53rd Annual Meeting of the
Association for Computational Linguistics and the
7th International Joint Conference on Natural Lan-
guage Processing (Volume 1: Long Papers), vol-
ume 1, pages 167–176.

Jacob Devlin, Ming-Wei Chang, Kenton Lee, and
Kristina Toutanova. 2018. Bert: Pre-training of deep
bidirectional transformers for language understand-
ing. arXiv preprint arXiv:1810.04805.

George R Doddington, Alexis Mitchell, Mark A Przy-
bocki, Lance A Ramshaw, Stephanie M Strassel, and
Ralph M Weischedel. 2004. The automatic content
extraction (ace) program-tasks, data, and evaluation.
In LREC, volume 2, page 1.

Xiaocheng Feng, Lifu Huang, Duyu Tang, Heng Ji,
Bing Qin, and Ting Liu. 2016. A language-
independent neural network for event detection. In
Proceedings of the 54th Annual Meeting of the As-
sociation for Computational Linguistics (Volume 2:
Short Papers), volume 2, pages 66–71.



5293

Kelvin Guu, Tatsunori B Hashimoto, Yonatan Oren,
and Percy Liang. 2018. Generating sentences by
editing prototypes. Transactions of the Association
of Computational Linguistics, 6:437–450.

Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian
Sun. 2016. Deep residual learning for image recog-
nition. In Proceedings of the IEEE conference on
computer vision and pattern recognition, pages 770–
778.

Yu Hong, Jianfeng Zhang, Bin Ma, Jianmin Yao,
Guodong Zhou, and Qiaoming Zhu. 2011. Us-
ing cross-entity inference to improve event extrac-
tion. In Proceedings of the 49th Annual Meeting of
the Association for Computational Linguistics: Hu-
man Language Technologies-Volume 1, pages 1127–
1136. Association for Computational Linguistics.

Jeremy Howard and Sebastian Ruder. 2018. Universal
language model fine-tuning for text classification. In
Proceedings of the 56th Annual Meeting of the As-
sociation for Computational Linguistics (Volume 1:
Long Papers), pages 328–339.

Lifu Huang, Taylor Cassidy, Xiaocheng Feng, Heng
Ji, Clare R Voss, Jiawei Han, and Avirup Sil. 2016.
Liberal event extraction and event schema induction.
In Proceedings of the 54th Annual Meeting of the As-
sociation for Computational Linguistics (Volume 1:
Long Papers), volume 1, pages 258–268.

Peifeng Li, Qiaoming Zhu, and Guodong Zhou.
2013a. Joint modeling of argument identification
and role determination in chinese event extraction
with discourse-level information. In Twenty-Third
International Joint Conference on Artificial Intelli-
gence.

Qi Li, Heng Ji, and Liang Huang. 2013b. Joint event
extraction via structured prediction with global fea-
tures. In Proceedings of the 51st Annual Meeting of
the Association for Computational Linguistics (Vol-
ume 1: Long Papers), volume 1, pages 73–82.

Shasha Liao and Ralph Grishman. 2010. Using doc-
ument level cross-event inference to improve event
extraction. In Proceedings of the 48th Annual Meet-
ing of the Association for Computational Linguis-
tics, pages 789–797. Association for Computational
Linguistics.

Bing Liu, Longhua Qian, Hongling Wang, and
Guodong Zhou. 2010. Dependency-driven feature-
based learning for extracting protein-protein interac-
tions from biomedical text. In Proceedings of the
23rd International Conference on Computational
Linguistics: Posters, pages 757–765. Association
for Computational Linguistics.

Shulin Liu, Yubo Chen, Shizhu He, Kang Liu, and
Jun Zhao. 2016. Leveraging framenet to improve
automatic event detection. In Proceedings of the
54th Annual Meeting of the Association for Compu-
tational Linguistics (Volume 1: Long Papers), vol-
ume 1, pages 2134–2143.

Shulin Liu, Yubo Chen, Kang Liu, and Jun Zhao. 2017.
Exploiting argument information to improve event
detection via supervised attention mechanisms. In
Proceedings of the 55th Annual Meeting of the As-
sociation for Computational Linguistics (Volume 1:
Long Papers), volume 1, pages 1789–1798.

Bryan McCann, James Bradbury, Caiming Xiong, and
Richard Socher. 2017. Learned in translation: Con-
textualized word vectors. In Advances in Neural In-
formation Processing Systems, pages 6294–6305.

Mike Mintz, Steven Bills, Rion Snow, and Dan Juraf-
sky. 2009. Distant supervision for relation extrac-
tion without labeled data. In Proceedings of the
Joint Conference of the 47th Annual Meeting of the
ACL and the 4th International Joint Conference on
Natural Language Processing of the AFNLP: Vol-
ume 2-Volume 2, pages 1003–1011. Association for
Computational Linguistics.

Makoto Miwa, Rune Sætre, Yusuke Miyao, and
Jun’ichi Tsujii. 2009. A rich feature vector for
protein-protein interaction extraction from multiple
corpora. In Proceedings of the 2009 Conference on
Empirical Methods in Natural Language Process-
ing: Volume 1-Volume 1, pages 121–130. Associa-
tion for Computational Linguistics.

Thien Huu Nguyen, Kyunghyun Cho, and Ralph Gr-
ishman. 2016. Joint event extraction via recurrent
neural networks. In Proceedings of the 2016 Con-
ference of the North American Chapter of the Asso-
ciation for Computational Linguistics: Human Lan-
guage Technologies, pages 300–309.

Thien Huu Nguyen and Ralph Grishman. 2015. Event
detection and domain adaptation with convolutional
neural networks. In Proceedings of the 53rd Annual
Meeting of the Association for Computational Lin-
guistics and the 7th International Joint Conference
on Natural Language Processing (Volume 2: Short
Papers), volume 2, pages 365–371.

Matthew Peters, Mark Neumann, Mohit Iyyer, Matt
Gardner, Christopher Clark, Kenton Lee, and Luke
Zettlemoyer. 2018. Deep contextualized word rep-
resentations. In Proceedings of the 2018 Confer-
ence of the North American Chapter of the Associ-
ation for Computational Linguistics: Human Lan-
guage Technologies, Volume 1 (Long Papers), pages
2227–2237.

Alec Radford, Karthik Narasimhan, Tim Salimans, and
Ilya Sutskever. 2018. Improving language under-
standing by generative pre-training. URL https://s3-
us-west-2. amazonaws. com/openai-assets/research-
covers/languageunsupervised/language under-
standing paper. pdf.

Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and
Percy Liang. 2016. Squad: 100,000+ questions for
machine comprehension of text. In Proceedings of
the 2016 Conference on Empirical Methods in Nat-
ural Language Processing, pages 2383–2392.



5294

Wilson L Taylor. 1953. “cloze procedure”: A new
tool for measuring readability. Journalism Bulletin,
30(4):415–433.

Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V
Le, Mohammad Norouzi, Wolfgang Macherey,
Maxim Krikun, Yuan Cao, Qin Gao, Klaus
Macherey, et al. 2016. Google’s neural ma-
chine translation system: Bridging the gap between
human and machine translation. arXiv preprint
arXiv:1609.08144.

Hang Yang, Yubo Chen, Kang Liu, Yang Xiao, and Jun
Zhao. 2018. Dcfee: A document-level chinese fi-
nancial event extraction system based on automat-
ically labeled training data. Proceedings of ACL
2018, System Demonstrations, pages 50–55.

Ying Zeng, Yansong Feng, Rong Ma, Zheng Wang, Rui
Yan, Chongde Shi, and Dongyan Zhao. 2018. Scale
up event extraction learning via automatic training
data generation. In Thirty-Second AAAI Conference
on Artificial Intelligence.


