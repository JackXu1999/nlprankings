



















































Contextual Inter-modal Attention for Multi-modal Sentiment Analysis


Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 3454–3466
Brussels, Belgium, October 31 - November 4, 2018. c©2018 Association for Computational Linguistics

3454

Contextual Inter-modal Attention for Multi-modal Sentiment Analysis

Deepanway Ghosal†, Md Shad Akhtar†, Dushyant Chauhan†, Soujanya Poria?,
Asif Ekbal† and Pushpak Bhattacharyya†

† Department of Computer Science & Engineering, Indian Institute of Technology Patna, India
{deepanway.me14,shad.pcs15,1821CS18,asif,pb}@iitp.ac.in

? School of Computer Science and Engineering, Nanyang Technological University, Singapore
sporia@ntu.edu.sg

Abstract

Multi-modal sentiment analysis offers various
challenges, one being the effective combina-
tion of different input modalities, namely text,
visual and acoustic. In this paper, we propose
a recurrent neural network based multi-modal
attention framework that leverages the con-
textual information for utterance-level senti-
ment prediction. The proposed approach ap-
plies attention on multi-modal multi-utterance
representations and tries to learn the contribut-
ing features amongst them. We evaluate our
proposed approach on two multi-modal senti-
ment analysis benchmark datasets, viz. CMU
Multi-modal Opinion-level Sentiment Inten-
sity (CMU-MOSI) corpus and the recently re-
leased CMU Multi-modal Opinion Sentiment
and Emotion Intensity (CMU-MOSEI) corpus.
Evaluation results show the effectiveness of
our proposed approach with the accuracies of
82.31% and 79.80% for the MOSI and MO-
SEI datasets, respectively. These are approxi-
mately 2 and 1 points performance improve-
ment over the state-of-the-art models for the
datasets.

1 Introduction

Traditionally, sentiment analysis (Pang and Lee,
2005, 2008) has been applied to a wide variety
of texts (Hu and Liu, 2004; Liu, 2012; Turney,
2002; Akhtar et al., 2016, 2017; Mohammad et al.,
2013). In contrast, multi-modal sentiment analysis
has recently gained attention due to the tremen-
dous growth of many social media platforms such
as YouTube, Instagram, Twitter, Facebook (Chen
et al., 2017; Poria et al., 2016, 2017d,b; Zadeh
et al., 2017, 2016) etc. It depends on the infor-
mation that can be obtained from more than one
modality (e.g. text, visual and acoustic) for the
analysis. The motivation is to leverage the vari-
eties of (often distinct) information from multiple
sources for building an efficient system. For ex-

ample, it is a non-trivial task to detect the senti-
ment of a sarcastic sentence “My neighbours are
home!! it is good to wake up at 3am in the morn-
ing.” as negative considering only the textual in-
formation. However, if the system has access to
some other sources of information, e.g. visual, it
can easily detect the unpleasant gestures of the
speaker and would classify it with the negative
sentiment polarity. Similarly, for some instances
acoustic features such as intensity, pitch, pause
etc. have important roles to play in the correctness
of the system. However, combining these informa-
tion in an effective manner is a non-trivial task that
researchers often have to face (Zadeh et al., 2017;
Chen et al., 2017).

A video provides a good source for extracting
multi-modal information. In addition to the visual
frames, it also provides information such as acous-
tic and textual representation of spoken language.
Additionally, a speaker can utter multiple utter-
ances in a single video and these utterances can
have different sentiments. The sentiment informa-
tion of an utterance often has inter-dependence on
other contextual utterances. Classifying such an
utterance in an independent manner poses many
challenges to the underlying algorithm.

In this paper, we propose a novel method that
employs a recurrent neural network based multi-
modal multi-utterance attention framework for
sentiment prediction.We hypothesize that apply-
ing attention to contributing neighboring utter-
ances and/or multi-modal representations may as-
sist the network to learn in a better way. The main
challenge in multi-modal sentiment analysis lies
in the proper utilization of the information ex-
tracted from multiple modalities. Although it is of-
ten argued that incorporation of all the available
modalities is always beneficial for enhanced per-
formance, it must be noted that not all the modal-
ities play equal role. Another concern in multi-



3455

modal framework is that the presence of noise in
one modality can affect the overall performance.
To better address these concerns we propose a
novel fusion method by focusing on inter-modality
relations computed between the target utterance
and its context. We argue that in multi-modal sen-
timent classification, not only the relation among
two modalities of the same utterance is important,
but also relatedness with the modalities across its
context are important.

Think of an utterance Ut that constitutes of
three modalities, say At (i.e. audio), Vt (i.e. vi-
sual) and Tt (i.e. text). Let us also assume Uk being
a member of the contextual utterances consisting
of the modalities - Ak, Vk and Tk. In this case,
our model computes the relatedness among the
modalities (for e.g., Vt and Tk) of Ut and Uk in or-
der to produce a richer multi-modal representation
for final classification. The attention mechanism
is then used to attend to the important contextual
utterances having higher relatedness or similarity
(computed using inter-modality correlations) with
the target utterance.

Unlike previous approaches that simply apply
attentions over the contextual utterance for classi-
fication, we attend over the contextual utterances
by computing correlations among the modali-
ties of the target utterance and the context ut-
terances. This explicitly helps us to distinguish
which modalities of the relevant contextual utter-
ances are more important for sentiment predic-
tion of the target utterance. The model facilitates
this modality selection by attending over the con-
textual utterances and thus generates better multi-
modal feature representation when these modali-
ties from the context are combined with the modal-
ities of the target utterance. We evaluate our pro-
posed approach on two recent benchmark datasets,
i.e. CMU-MOSI (Zadeh et al., 2016) and CMU-
MOSEI (Zadeh et al., 2018c), with one being the
largest (CMU-MOSEI) available dataset for multi-
modal sentiment analysis (c.f. Section 4.1). Evalu-
ation shows that the proposed attention framework
attains better performance than the state-of-the-art
systems for various combinations of input modal-
ities (i.e. text, visual & acoustic).

The main contributions of our proposed work
are three-fold: a) we propose a novel technique for
multi-modal sentiment analysis; b) we propose an
effective attention framework that leverages con-
tributing features across multiple modalities and

neighboring utterances for sentiment analysis; and
c) we present the state-of-the-art systems for senti-
ment analysis in two different benchmark datasets.

2 Related Work

A survey of the literature suggests that multi-
modal sentiment prediction is relatively a new area
as compared to textual based sentiment prediction
(Morency et al., 2011; Mihalcea, 2012; Poria et al.,
2016, 2017b; Zadeh et al., 2018a). A good re-
view covering the literature from uni-modal anal-
ysis to multi-modal analysis is presented in (Po-
ria et al., 2017a). An application of multi-kernel
learning based fusion technique was proposed in
(Poria et al., 2016), where they employed deep
convolutional neural networks for extracting the
textual features and fused it with other (visual &
acoustic) modalities for prediction.

Zadeh et al. (2016) introduced the multi-modal
dictionary to better understand the interaction be-
tween facial gestures and spoken words when ex-
pressing the sentiment. Authors introduced the
MOSI dataset, the first of its kind to enable the
studies of multi-modal sentiment intensity analy-
sis. Zadeh et al. (2017) proposed a Tensor Fusion
Network (TFN) model to learn the intra-modality
and inter-modality dynamics of the three modali-
ties (i.e. text, visual and acoustic). They reported
the improved accuracy using multi-modality on
the CMU-MOSI dataset. An application to lever-
age on the gated multi-modal embedded Long
Short Term Memory (LSTM) with temporal at-
tention (GME-LSTM(A)) for the word-level fu-
sion of multi-modality inputs is proposed in (Chen
et al., 2017). The Gated Multi-modal Embedding
(GME) alleviates the difficulties of fusion while
the LSTM with Temporal Attention (LSTM(A))
performs word-level fusion.

The works mentioned above did not take con-
textual information into account. Poria et al.
(2017b) proposed a LSTM based framework that
leverages the contextual information to capture
the inter-dependencies between the utterances. In
another work, Poria et al. (2017d) proposed an
user opinion based framework to combine the
three modality inputs (i.e. text, visual & acous-
tic) by applying a multi-kernel learning based
method. Zadeh et al. (2018a) proposed multi-
attention blocks (MAB) to capture information
across three modalities (text, visual & acoustic).
They reported improved accuracies in the range of



3456

2-3% over the state-of-the-art models for the dif-
ferent datasets.

The fundamental difference between our pro-
posed method and the existing works is that our
framework applies focus on the neighboring ut-
terances to leverage contextual information for
utterance-level sentiment prediction. To the best of
our knowledge, our current work is the very first
of its kind that attempts to employ multi-modal at-
tention block (exploiting neighboring utterances)
for sentiment prediction. We use multi-modal at-
tention framework that leverages contributing fea-
tures across multiple modalities and the neighbor-
ing utterances for sentiment analysis.

3 Proposed Methodology

In our proposed framework, we aim to leverage
the multi-modal and contextual information for
predicting the sentiment of an utterance. Utter-
ances of a particular speaker in a video represent
the time series information and it is logical that
the sentiment of a particular utterance would af-
fect the sentiments of the other neighboring utter-
ances. To model the relationship with the neigh-
boring utterances and multi-modality, we propose
a recurrent neural network based multi-modal at-
tention framework. The proposed framework takes
multi-modal information (i.e. text, visual & acous-
tic) for a sequence of utterances and feeds it into
three separate bi-directional Gated Recurrent Unit
(GRU) (Cho et al., 2014). This is followed by a
dense (fully-connected) operation which is shared
across the time-steps or utterances (one each for
text, visual & acoustic). We then apply multi-
modal attention on the outputs of the dense layers.
The objective is to learn the joint-association be-
tween the multiple modalities & utterances, and to
emphasize on the contributing features by putting
more attention to these. In particular, we employ
bi-modal attention framework, where an atten-
tion function is applied to the representations of
pairwise modalities i.e. visual-text, text-acoustic
and acoustic-visual. Finally, the outputs of pair-
wise attentions along with the representations are
concatenated and passed to the softmax layer for
classification. We call our proposed architecture
Multi-Modal Multi-Utterance - Bi-Modal Atten-
tion (MMMU-BA) framework. An overall archi-
tecture of the proposed MMMU-BA framework is
illustrated in Figure 1. Please refer to Figure 3 in
appendix for illustration of attention computation.

For comparison, we also experiment with
two other variants of the proposed MMMU-BA
framework i.e. a). Multi-Modal Uni-Utterance-
Self Attention (MMUU-SA) framework and b).
Multi-Utterance-Self Attention (MU-SA) frame-
work. The architecture of these variants differ
with respect to the attention computation module
and the naming conventions “MMMU”, “MMUU”
or “MU” signify the information that partici-
pates in the attention computation. For example,
in MMMU-BA, we compute attention over the
multi-modal and multi-utterance inputs, whereas
in MMUU-SA, the attention is computed over the
mutli-modal but uni-utterance inputs. In contrast,
we compute attention over only multi-utterance in-
puts in MU-SA. Rest of the components for all the
three variants remain same.

3.1 Multi-modal Multi-utterance - Bi-modal
Attention (MMMU-BA) Framework

Assuming a particular video has ‘u’ utterances, the
raw utterance level multi-modal features are rep-
resented as TR ∈ Ru×300 (raw text), VR ∈ Ru×35
(raw visual) and AR ∈ Ru×74 (raw acoustic).
Three separate Bi-GRU layers with forward &
backward state concatenation are first applied on
the raw data followed by the fully-connected dense
layers, resulting in T ∈ Ru×d (text), V ∈ Ru×d
(visual) and A ∈ Ru×d (acoustic), where ‘d’ is
the number of neurons in the dense layer. Finally,
pairwise-attentions are computed on various com-
binations of three modalities- (V, T), (T, A) & (A,
V). In particular the attention between V and T is
computed as follows:
•Bi-modal Attention: Modality representations of
V & T are obtained from the Bi-GRU network,
and hence contain the contextual information of
the utterances for each modality. At first, we com-
pute a pair of matching matrices M1,M2 ∈ Ru×u
over two representations that account for the cross-
modality information.

M1 = V.T
T & M2 = T.V T

• Multi-Utterance Attention: As mentioned ear-
lier, in the proposed model we aim to leverage the
contextual information of each utterance for the
prediction. We compute the probability distribu-
tion scores (N1 ∈ Ru×u & N2 ∈ Ru×u) over
each utterance of bi-modal attention matrices M1
& M2 using a softmax function. This essentially
computes the attention weights for the contextual



3457

B
i-G

RU

D
ense

B
i-G

RU

D
ense

B
i-G

RU

D
ense

AR (raw acoustic)
utt1
utt2
uttu

VR (raw video)
utt1
utt2
uttu

TR (raw text)
utt1
utt2
uttu

A
ttention

A
ttention

A
ttention

concat

softm
ax

Figure 1: Overall architecture of the proposed MMMU-BA framework.

utterances. Finally, soft attention is applied over
the multi-modal multi-utterance attention matrices
to compute the modality-wise attentive representa-
tions (i.e. O1 & O2).

N1(i, j) =
eM1(i,j)∑u
k=1 e

M1(i,k)
for i, j = 1, .., u

N2(i, j) =
eM2(i,j)∑u
k=1 e

M2(i,k)
for i, j = 1, .., u.

O1 = N1.T & O2 = N2.V

• Multiplicative Gating & Concatenation: Fi-
nally, a multiplicative gating function following
(Dhingra et al., 2016) is computed between the
multi-modal utterance specific representations of
each individual modality and the other modalities.
This element-wise matrix multiplication assists in
attending to the important components of multiple
modalities and utterances.

A1 = O1 � V & A2 = O2 � T

Attention matrices A1 & A2 are then concatenated
to obtain the MMMU-BAVT ∈ Ru×2d between V
and T.

MMMU-BAV T = concat[A1, A2]

MMMU-BAAV & MMMU-BATA computations:
Similar to MMMU-BAVT, we follow the same
procedure to compute MMMU-BAAV & MMMU-
BATA. For a data source comprising of raw vi-
sual (VR), acoustic (AR) & text (TR) modalities,
at first, we compute the bi-modal attention pairs
for each combination i.e. MMMU-BAVT, MMMU-
BAAV & MMMU-BATA. Finally, motivated by the
residual skip connection network (He et al., 2016),

we concatenate the bi-modal attention pairs with
individual modalities (i.e. V, A & T) to boost the
gradient flow to the lower layers. This concate-
nated feature is then used for final classification.

3.2 Multi-Modal Uni-Utterance - Self
Attention (MMUU-SA) Framework

MMUU-SA framework does not account for infor-
mation from the other utterances at the attention
level, rather it utilizes multi-modal information of
single utterance for predicting the sentiment. For a
video having ‘q’ utterances, ‘q’ separate attention
blocks are needed, where each block computes the
self-attention over multi-modal information of a
single utterance. Let Xup ∈ R3×d is the informa-
tion matrix of the pth utterance where the three ‘d’
dimensional rows are the outputs of the dense lay-
ers for the three modalities.

The attention matrix Aup ∈ R3×d is computed
separately for, p = 1st, 2nd, ... qth utterances. Fi-
nally, for each utterance p, Aup and Xup are con-
catenated and passed to the output layer for clas-
sification. Please refer to the appendix for more
details.

3.3 Multi-Utterance - Self Attention (MU-SA)
Framework

In MU-SA framework, we apply self attention on
the utterances of each modality separately, and use
these for classification. In contrast to MMUU-SA
framework, MU-SA utilizes the contextual infor-
mation of the utterances at the attention level. Let,
T ∈ Ru×d (text), V ∈ Ru×d (visual) and A ∈
Ru×d (acoustic) are the outputs of the dense lay-
ers. For the three modalities, three separate atten-
tion blocks are required, where each block takes



3458

multi-utterance information of a single modality
and computes the self attention matrix. Attention
matrices At, Av and Aa are computed for text, vi-
sual and acoustic, respectively. Finally Av, At, Aa,
V , T & A are concatenated and passed to the out-
put layer for classification.

4 Datasets, Experiments and Analysis

In this section we describe the datasets used for
our experiments and report the results along with
the necessary analysis.

4.1 Datasets

We evaluate our proposed approach on two
benchmark datasets, namely CMU Multi-modal
Opinion-level Sentiment Intensity (CMU-MOSI)
corpus (Zadeh et al., 2016) and the recently pub-
lished CMU Multi-modal Opinion Sentiment and
Emotion Intensity (CMU-MOSEI) dataset (Zadeh
et al., 2018c). CMU-MOSI dataset consists of 93
videos spanning over 2199 utterances. Each utter-
ance has a sentiment label associated with it. It has
52, 10 & 31 videos in training, validation & test set
accounting for 1151, 296 & 752 utterances.

CMU-MOSEI has 3229 videos with 22676 ut-
terances from more than 1000 online YouTube
speakers. The training, validation & test set com-
prise of 16216, 1835 & 4625 utterances, respec-
tively. More details about these datasets are pre-
sented in the appendix.

Each utterance in CMU-MOSI dataset has been
annotated as either positive or negative, whereas in
CMU-MOSEI dataset labels are in the continuous
range of -3 to +3. However, in this work we project
the instances of CMU-MOSEI in a two-class clas-
sification setup with values ≥ 0 signify positive
sentiments and values < 0 signify negative sen-
timents. We adopt such a strategy to be consistent
with the previous published works on CMU-MOSI
datasets (Poria et al., 2017b; Chen et al., 2017).

4.2 Feature extraction

We use the CMU-Multi-modal Data SDK1 (Zadeh
et al., 2018a) for feature extraction. For MOSEI
dataset, word-level features were provided where
text features were extracted by GloVe embeddings,
visual features by Facets2 & acoustic features by
CovaRep (Degottex et al., 2014). Thereafter, we

1https://github.com/A2Zadeh/
CMU-MultimodalDataSDK

2https://pair-code.github.io/facets/

compute the average of word-level features in an
utterance to obtain the utterance-level features.
For each word, the dimension of the feature vector
is set to 300 (text), 35 (visual) & 74 (acoustic).

In contrast, for MOSI dataset we use utterance-
level features3 provided in (Poria et al., 2017b).
These utterance-level features represent the out-
puts of a convolutional neural network (Karpathy
et al., 2014), 3D convolutional neural network (Ji
et al., 2013) & openSMILE (Eyben et al., 2010)
for text, visual & acoustic modalities, respectively.
Dimensions of utterance-level features are 100,
100 & 73 for text, visual & acoustic, respectively.

4.3 Experiments

We evaluate our proposed approach for CMU-
MOSI (test data) & CMU-MOSEI (dev data) 4.
Accuracy score is used as the evaluation metric.

We use Bi-directional GRUs having 300 neu-
rons, each followed by a dense layer consisting of
100 neurons. Utilizing the dense layer, we project
the input features of all the three modalities to the
same dimensions. We set dropout=0.5 (MOSI) &
0.3 (MOSEI) as a measure of regularization. In
addition, we also use dropout=0.4 (MOSI) & 0.3
(MOSEI) for the Bi-GRU layers. We employ ReLu
activation function in the dense layers, and soft-
max activation in the final classification layer. For
training the network we set the batch size=32, use
Adam optimizer with cross-entropy loss function
and train for 50 epochs. We report the average re-
sult of 5 runs for all our experiments.

We experiment with all the valid combinations
of uni-modal (where only one modality is taken
at a time), bi-modal (any two modalities are taken
at a time) and tri-modal (all three modalities are
taken at a time) inputs for text, visual and acoustic.
In multi-modal attention frameworks i.e. MMMU-
BA & MMUU-SA, the attention is computed over
at least two modalities, hence, these two frame-
works are not-applicable (NA) for uni-modal ex-
periments in Table 1).

For MOSEI dataset, we obtain better per-
formance with text. Subsequently, we take two
modalities at a time for constructing bi-modal in-
puts and feed it to the network. For text-acoustic
input pairs, we obtain the highest accuracies with
79.74%, 79.60% and 79.32% for MMMU-BA,

3https://github.com/SenticNet/
contextual-sentiment-analysis

4Gold annotation of CMU-MOSEI test data wasn’t re-
leased at the time of paper submission.

https://github.com/A2Zadeh/CMU-MultimodalDataSDK
https://github.com/A2Zadeh/CMU-MultimodalDataSDK
https://pair-code.github.io/facets/
https://github.com/SenticNet/contextual-sentiment-analysis
https://github.com/SenticNet/contextual-sentiment-analysis


3459

Modality T V A
CMU-MOSEI CMU-MOSI

MMMU-BA MMUU-SA MU-SA MMMU-BA MMUU-SA MU-SA

Uni-modal
X - - NA NA 78.23 NA NA 80.18
- X - NA NA 74.84 NA NA 63.70
- - X NA NA 75.88 NA NA 62.10

Bi-modal
X X - 79.40 79.02 79.26 81.51 80.85 80.45
X - X 79.74 79.60 79.32 80.58 80.31 79.78
- X X 76.66 76.46 76.43 65.16 64.22 63.22

Tri-modal X X X 79.80 79.76 79.63 82.31 79.52 80.58

Table 1: Experimental results on CMU-MOSEI and CMU-MOSI datasets. MMMU-BA & MMUU-SA
frameworks require atleast two modalities to compute the attentions, hence, these two frameworks are
not-applicable (NA) for uni-modal inputs. All results are average of 5 runs with different random seeds.
T: Text, V: Visual, A: Acoustic. Results are reported in accuracy.

MMUU-SA and MU-SA frameworks, respectively.
The results that we obtain from the bi-modal com-
binations suggest that the text-acoustic combina-
tion is a better choice than the others as it improves
the overall performance. Finally, we experiment
with tri-modal inputs and observe an improved
performance of 79.80%, 79.76% and 79.63% for
MMMU-BA, MMUU-SA and MU-SA frameworks,
respectively. This improvement entails that combi-
nation of all the three modalities is a better choice.
The performance improvement was also found
to be statistically significant (T-test) than the bi-
modality and uni-modality inputs. Further, we ob-
serve that the MMMU-BA framework reports the
best accuracy of 79.80% for the MOSEI dataset,
thus supporting our claim that multi-modal atten-
tion framework (i.e. MMMU-BA) captures more
information than the self-attention frameworks
(i.e. MMUU-SA & MU-SA).

4.4 Analysis of Attention Mechanism

We analyze the attention values to understand the
learning behavior of the proposed architecture.
To illustrate, we take an example video from the
CMU-MOSI test dataset. The transcript of the ut-
terances for this particular video are presented in
Table 2. The gold sentiments are positive for all
the utterances except u3 & u4. We found that the
proposed tri-modal MMMU-BA model predicts the
labels of all the nine instances correctly, whereas
other models make at least one misclassification.
For the proposed tri-modal MMMU-BA model, the
heatmaps of the pair-wise MMMU-BA softmax at-
tention weights N1 & N2 of visual-text, acoustic-
visual & text-acoustic are illustrated in Figure 2a,
Figure 2b & Figure 2c, respectively. N1 & N2 are
the softmax attention weights obtained from the

pairwise matching matrices M1 & M2. Elements
of the rows of N1 & N2 matrices signify differ-
ent weights across multiple utterances. From the
attention heatmaps, it is evident that by applying
different weights across contextual utterances and
modalities the model is able to predict labels of all
the utterances correctly. All the heatmaps justify
that the model learns to incorporate multi-modal &
multi-utterance information and thus is able to cor-
rectly predict the labels of all the utterances. For
example, heatmap of MMMU-BAVT (Figure 2a)
signifies that elements of N1 are weighted higher
than N2, and thus the model puts more attention on
the textual part and relatively lesser on the visual
part (as N1 is multiplied with T & N2 is multiplied
with V). Also it can be concluded that textual fea-
tures of the first few utterances are the most help-
ful compared to the rest of the textual features and
visual features.

The softmax attention weights of text (Nt), vi-
sual (Nv) & acoustic (Na) in tri-modal MU-SA
model are illustrated in Figure 2d, Figure 2e &
Figure 2f, respectively. The attention matrices are
9*9 dimensional. This model wrongly predicts
the label of the utterance u5. On the other hand,
softmax attention weights in tri-modal MMUU-SA
model are illustrated in Figure 2g. Nine separate
attention weights (Nu1 , Nu2 , .., Nu9) are com-
puted for the nine utterances. This model wrongly
predicts the labels of the utterances u4 & u5.

We further analyze our proposed architecture
(i.e. MMMU-BA) with and without attention. In
MOSI for tri-modal inputs, the MMMU-BA archi-
tecture reports a reduced accuracy of 80.89% with-
out attention framework as compared to 82.31%
with attention. We observe similar performance
in the MOSEI dataset, where we obtain 79.02%



3460

Transcript Gold Label Predicted
MMMU-BA MMUU-SA MU-SA

u1 well he plays that well so he‘s good villain Positive Positive Positive Positive
u2 he also has some really cool guns so that its like a

desert eagle but it has to barrels to it
Positive Positive Positive Positive

u3 it‘s pretty pretty scary looking Negative Negative Negative Negative
u4 i wouldn‘t want that pointed at me Negative Negative Positive Negative
u5 and i who would i mean Positive Positive Negative Negative
u6 um like i said i thought the movie was great Positive Positive Positive Positive
u7 the action they do have is really well done Positive Positive Positive Positive
u8 um they did a good job with car Positive Positive Positive Positive
u9 they did a good job with fight scenes Positive Positive Positive Positive

Table 2: Transcript, gold labels and predicted labels of a video in CMU-MOSI dataset having nine utter-
ances. 7 utterances are labeled positive whereas 2 utterances are labeled negative. Predicted labels are
for our different tri-modal models. Bolded labels are misclassified by at least one model.

u1 u2 u3 u4 u5 u6 u7 u8 u9 u1 u2 u3 u4 u5 u6 u7 u8 u9

u
9

u
8

u
7

u
6

u
5

u
4

u
3

u
2

u
1

0.008

0.016

0.024

0.032

0.040

0.048

(a) Softmax attention weights N1 & N2
for MMMU- BAVT.

u1 u2 u3 u4 u5 u6 u7 u8 u9 u1 u2 u3 u4 u5 u6 u7 u8 u9

u
9

u
8

u
7

u
6

u
5

u
4

u
3

u
2

u
1

0.012

0.016

0.020

0.024

0.028

(b) Softmax attention weights N1 & N2
for MMMU- BAAV.

u1 u2 u3 u4 u5 u6 u7 u8 u9 u1 u2 u3 u4 u5 u6 u7 u8 u9

u
9

u
8

u
7

u
6

u
5

u
4

u
3

u
2

u
1

0.02

0.04

0.06

0.08

0.10

(c) Softmax attention weights N1 & N2
for MMMU- BATA.

u1 u2 u3 u4 u5 u6 u7 u8 u9

u
9

u
8

u
7

u
6

u
5

u
4

u
3

u
2

u
1

0.0

0.1

0.2

0.3

0.4

0.5

(d) MU-SAtext matrix.
u1 u2 u3 u4 u5 u6 u7 u8 u9

u
9

u
8

u
7

u
6

u
5

u
4

u
3

u
2

u
1

0.0

0.2

0.4

0.6

0.8

(e) MU-SAvisual matrix.
u1 u2 u3 u4 u5 u6 u7 u8 u9

u
9

u
8

u
7

u
6

u
5

u
4

u
3

u
2

u
1

0.02

0.04

0.06

0.08

(f) MU-SAacoustic matrix.

u1(visual) u1(acoustic) u1(text)

u1(text)

u1(acoustic)

u1(visual)

0.15

0.30

0.45

0.60

0.75

u2(visual) u2(acoustic) u2(text)

u2(text)

u2(acoustic)

u2(visual)

0.15

0.30

0.45

0.60

0.75

u3(visual) u3(acoustic) u3(text)

u3(text)

u3(acoustic)

u3(visual)

0.15

0.30

0.45

0.60

0.75

u4(visual) u4(acoustic) u4(text)

u4(text)

u4(acoustic)

u4(visual)

0.2

0.4

0.6

0.8

u5(visual) u5(acoustic) u5(text)

u5(text)

u5(acoustic)

u5(visual)

0.2

0.4

0.6

0.8

u6(visual) u6(acoustic) u6(text)

u6(text)

u6(acoustic)

u6(visual)

0.15

0.30

0.45

0.60

0.75

u7(visual) u7(acoustic) u7(text)

u7(text)

u7(acoustic)

u7(visual)

0.15

0.30

0.45

0.60

0.75

0.90

u8(visual) u8(acoustic) u8(text)

u8(text)

u8(acoustic)

u8(visual)

0.15

0.30

0.45

0.60

0.75

u9(visual) u9(acoustic) u9(text)

u9(text)

u9(acoustic)

u9(visual)

0.15

0.30

0.45

0.60

0.75

(g) Softmax attention weights (Nu1 , Nu2 , .., Nu9 ) for MMUU-SA model.

Figure 2: (a), (b) & (c): Pair-wise softmax attention weights N1 & N2 of visual-text, acoustic-visual &
text-acoustic in Tri-modal MMMU-BA model. Solid line at the center represents boundary of N1 & N2.
The heatmaps represent attention weights of a particular utterance with respect to other utterances in N1
& N2. (d), (e) & (f) Softmax attention weights of text (Nt), visual (Nv) and acoustic (Na) in Tri-modal
MU-SA model. This model wrongly predicts the label of utterance u5. (g) Softmax attention weights of
the 9 utterances (NU1 , NU2 , .., NU9) in Tri-modal MMUU-SA model. This model wrongly predicts the
label of utterance u4 & u5. The Tri-modal MMMU-BA model predicts all 9 instances correctly, whereas,
the other two models makes at least one misclassification. Heatmap signifies that the model is able to
predict labels of all the utterances correctly by incorporating multi-modal & multi-utterance information.



3461

T V A
CMU-MOSEI CMU-MOSI

w/ attention w/o attention w/ attention w/o attention

X X - 79.40 78.27 81.51 80.71
X - X 79.74 78.12 80.58 80.18
- X X 76.66 76.32 65.16 63.69
X X X 79.80 79.02 82.31 80.89

Table 3: Analysis of attention mechanism in
MMMU-BA architecture. w/ attention → with
multi-modal multi utterance attention mechanism
and w/o attention→without attention mechanism.

accuracy without attention framework against
79.80% accuracy with attention framework. Sta-
tistical T-test shows these improvements to be sig-
nificant. We also observed the similar trends for
bi-modal inputs in both the datasets. All these ex-
periments (c.f. Table 3) suggest that the attention
framework is an important component in our pro-
posed architecture, and in absence of this the net-
work finds it more difficult for learning in all the
cases (i.e. bi-modal & tri-modal input setups).

We successfully show that attention computa-
tion on pairwise combination of modalities (i.e. bi-
modal attention framework) is more effective than
the combination of self-attention on single modal-
ity. Further for the completeness of the proposed
approach, we also experiment with tri-modal at-
tention framework (attention is computed on three
modalities at a time). Though the results that we
obtain are convincing, it does not improve the per-
formance over the bi-modal attention framework.
We obtain the accuracies of 79.58% & 81.25% on
MOSEI and MOSI, respectively, for the tri-modal
attention framework.

4.5 Comparative Analysis
For MOSI datasets we compare the performance
of our proposed approach with the the following
state-of-the-art systems: i). Poria et al. (2017b)-
LSTM-based sequence model to capture the con-
textual information of the utterances; ii). Poria
et al. (2017c)- Tensor level fusion technique for
combining all the three modalities; iii). Chen
et al. (2017)-A gated multi-modal embedded
LSTM with temporal attention (GME-LSTM(A))
for word-level fusion of multi-modality inputs.
and iv). Zadeh et al. (2018a)- Multiple attention
blocks for capturing the information across the
three modalities.

In Table 4 we present the comparative perfor-
mance between our proposed model and other
state-of-the-art systems. In MOSI dataset, Poria

et al. (2017b; 2017c) reported the accuracies of
80.3% & 81.3 %, respectively, utilizing tri-modal
inputs. Zadeh et al. (2018a) obtained an accuracy
of & 77.4%. Chen et al. (2017) reported accuracies
of 75.7% (LSTM(A)) & 76.5% (GME-LSTM(A))
for two variants of their model. In contrast to the
state-of-the-art systems, our proposed model at-
tains an improved accuracy of 82.31% when we
utilize all the three modalities, i.e. text, visual &
acoustic. Our proposed system also obtains better
performance as compared to the state-of-the-arts
for bi-modal inputs.

For MOSEI dataset, we evaluate against the fol-
lowing systems: i) Poria et al. (2017b), ii) Zadeh
et al. (2018a), and iii) Zadeh et al. (2018b), where
authors proposed a memory fusion network for
multi-view sequential learning. We evaluate the
system of Poria et al. (2017b) on MOSEI dataset
and obtain 77.64% accuracy with the tri-modal in-
puts. Authors in (Zadeh et al., 2018a) & (Zadeh
et al., 2018b) reported the accuracy 76.0% and
76.4%, respectively, with the tri-modal inputs. In
comparison, our proposed approach yields an ac-
curacy of 79.80%. As reported in Table 4 the pro-
posed approach also attains better performance for
all the bi-modal and uni-modal input combinations
when compared to Poria et al. (2017b).

As reported in Table 4, we observe that the per-
formance achieved in our proposed approach is
significantly better in comparison to the state-of-
the-art systems with p-value< 0.05 (obtained us-
ing T-test). For further analysis, we also report re-
sults for three-class classification (positive, neu-
tral & negative classes) problem setup for MOSEI
dataset in Table 7. Note that this setup is not feasi-
ble in MOSI as labels are only positive or negative.

4.6 Error Analysis

We perform error analysis on the predictions of
our proposed MMMU-BA model with all the three
input sources. Confusion matrices for both the
datasets are demonstrated in Table 5. For MO-
SEI dataset we observe that the precision and re-
call for positive class (84% precision & 88% re-
call; are quite encouraging. However, the same are
comparatively on the lower side for the negative
class (68% precision & 58% recall. In contrast, for
the MOSI dataset - which is relatively balanced -
we obtain quite similar performance for both the
classes i.e. positive (86% precision & 85% recall)
and negative (77% precision & 75% recall). Please



3462

Modality T V A
CMU-MOSEI CMU-MOSI

Poria
et al.

(2017b)

Zadeh
et al.

(2018a)

Zadeh
et al.

(2018b)

Proposed Poria
et al.

(2017b)

Poria
et al.

(2017c)

Chen et al. (2017) Zadeh
et al.

(2018a)

Proposed

LSTM(A)
GME-

LSTM(A)

Uni-modal
X - - 76.75 - - 78.23 78.1 - 71.3 - - 80.18
- X - 71.84 - - 74.84 60.3 - 52.3 - - 63.70
- - X 70.94 - - 75.88 55.8 - 55.4 - - 62.10

Bi-modal
X X - 77.03 - - 79.40 79.3 79.9 74.3 - - 81.51
X - X 76.89 - - 79.74 80.2 80.1 73.5 - - 80.58
- X X 72.74 - - 76.66 62.1 62.9 - - - 65.16

Tri-modal X X X 77.64 76.0 76.4 79.80 80.3 81.3 75.7 76.5 77.4 82.31
T-test (p-values) - - - 0.0025 - - - - - 0.0006

Table 4: Comparative analysis of the proposed approach with recent state-of-the-art systems. Significance
T-test p-values < 0.05

MOSEI
102 234

1230 269

Positive Negative

63 215

404 70

Positive Negative

MOSI

Table 5: Con-
fusion matrix
for tri-modal
MMMU-BA.

Text Actual Predicted Possible Reason

M
O

SI

At first I thought the movie would appeal more to younger audience. negative positive
Implicit sentiment.

Its really non-stop from beginning to end. negative positive
But its action isn’t particularly memorable. negative positive

Negation & strong word.
I mean I don’t regret seeing it. positive negative
Um I was really looking forward to it. negative positive Sarcastic sentence.

M
O

SE
I

And when I was going to school it was really difficult for me to find
avenues and resources to be able to reach higher education.

negative positive
Implicit sentiment.

We could have a decision from the court on the stay any day now. positive negative
Holidays never really happen in online courses I guess. negative positive

Negation & strong word.Young people dropping out of the labour market are actually not
counted anymore as unemployed as they are inactive.

positive negative

Thank you for your efforts and consideration. negative positive Sarcastic sentence.

Table 6: Error Analysis: Frequent error cases and their possible reasons of failure
for the tri-modal MMMU-BA framework.

Metric

CMU-MOSEI
Poria et al.

(2017b)
MMMU-BA
(Tri-modal)

Accuracy 61.89 63.30
F1 Score 61.60 63.07

Table 7: Three class (positive, negative, neutral)
classification results in MOSEI dataset.

refer to the appendix for PR curves of different in-
put combinations.

We further analyze our outputs qualitatively and
list a few frequently occurring error categories
with examples in Table 6.

5 Conclusion

In this paper, we have proposed a recurrent neu-
ral network based multi-modal attention frame-
work that leverages the contextual information for
utterance-level sentiment prediction. The network
learns on top of three modalities, viz. text, vi-
sual and acoustic, considering sequence of utter-

ances in a video. Through evaluation results on
two benchmark datasets (one being the popular &
commonly used (MOSI) and other being the most
recent & largest (MOSEI) dataset for multi-modal
sentiment analysis), we successfully showed that
the proposed attention based framework performs
better than various state-of-the-art systems.

In future, we would like to investigate new tech-
niques, and explore the ways to handle implicit
sentiment and sarcasm. Future direction of work
also include adding more dimensions, e.g. emo-
tion analysis & intensity prediction.

6 Acknowledgment

The research reported here is partially supported
by SkyMap Global India Private Limited. Asif
Ekbal acknowledges the Young Faculty Research
Fellowship (YFRF), supported by Visvesvaraya
PhD scheme for Electronics and IT, Ministry of
Electronics and Information Technology (MeitY),
Government of India, being implemented by Dig-
ital India Corporation (formerly Media Lab Asia).



3463

References
Md Shad Akhtar, Deepak Gupta, Asif Ekbal, and Push-

pak Bhattacharyya. 2017. Feature selection and en-
semble construction: A two-step method for aspect
based sentiment analysis. Knowledge-Based Sys-
tems, 125:116 – 135.

Md Shad Akhtar, Ayush Kumar, Asif Ekbal, and Push-
pak Bhattacharyya. 2016. A Hybrid Deep Learning
Architecture for Sentiment Analysis. In Proceed-
ings of the 26th International Conference on Com-
putational Linguistics (COLING 2016): Technical
Papers, December 11-16, 2016, pages 482–493, Os-
aka, Japan.

Minghai Chen, Sen Wang, Paul Pu Liang, Tadas Bal-
trušaitis, Amir Zadeh, and Louis-Philippe Morency.
2017. Multimodal sentiment analysis with word-
level fusion and reinforcement learning. In Proceed-
ings of the 19th ACM International Conference on
Multimodal Interaction, pages 163–171. ACM.

Kyunghyun Cho, Bart Van Merriënboer, Caglar Gul-
cehre, Dzmitry Bahdanau, Fethi Bougares, Holger
Schwenk, and Yoshua Bengio. 2014. Learning
phrase representations using rnn encoder-decoder
for statistical machine translation. arXiv preprint
arXiv:1406.1078.

G. Degottex, J. Kane, T. Drugman, T. Raitio, and
S. Scherer. 2014. Covarep - a collaborative voice
analysis repository for speech technologies. In
2014 IEEE International Conference on Acous-
tics, Speech and Signal Processing (ICASSP), pages
960–964.

Bhuwan Dhingra, Hanxiao Liu, Zhilin Yang,
William W Cohen, and Ruslan Salakhutdinov.
2016. Gated-attention readers for text comprehen-
sion. arXiv preprint arXiv:1606.01549.

Florian Eyben, Martin Wöllmer, and Björn Schuller.
2010. Opensmile: the munich versatile and fast
open-source audio feature extractor. In Proceedings
of the 18th ACM international conference on Multi-
media, pages 1459–1462. ACM.

Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian
Sun. 2016. Deep residual learning for image recog-
nition. In Proceedings of the IEEE conference on
computer vision and pattern recognition, pages 770–
778.

M. Hu and B. Liu. 2004. Mining and summarizing
customer reviews. In Proceedings of the 10th KDD,
pages 168–177, Seattle, WAs.

Shuiwang Ji, Wei Xu, Ming Yang, and Kai Yu. 2013.
3d convolutional neural networks for human action
recognition. IEEE transactions on pattern analysis
and machine intelligence, 35(1):221–231.

Andrej Karpathy, George Toderici, Sanketh Shetty,
Thomas Leung, Rahul Sukthankar, and Li Fei-Fei.

2014. Large-scale video classification with convolu-
tional neural networks. In Proceedings of the IEEE
conference on Computer Vision and Pattern Recog-
nition, pages 1725–1732.

Bing Liu. 2012. Sentiment Analysis and Opinion Min-
ing. Synthesis Lectures on Human Language Tech-
nologies. Morgan & Claypool Publishers.

Rada Mihalcea. 2012. Multimodal sentiment analy-
sis. In Proceedings of the 3rd Workshop in Com-
putational Approaches to Subjectivity and Sentiment
Analysis, WASSA@ACL 2012, July 12, 2012, Jeju Is-
land, Republic of Korea, page 1.

Saif Mohammad, Svetlana Kiritchenko, and Xiaodan
Zhu. 2013. NRC-Canada: Building the state-of-
the-art in sentiment analysis of tweets. In Second
Joint Conference on Lexical and Computational Se-
mantics (*SEM), Volume 2: Proceedings of the Sev-
enth International Workshop on Semantic Evalua-
tion (SemEval 2013), pages 321–327, Atlanta, Geor-
gia, USA. Association for Computational Linguis-
tics.

Louis-Philippe Morency, Rada Mihalcea, and Payal
Doshi. 2011. Towards multimodal sentiment anal-
ysis: harvesting opinions from the web. In Pro-
ceedings of the 13th International Conference on
Multimodal Interaction, ICMI 2011, Alicante, Spain,
November 14-18, 2011, pages 169–176.

Bo Pang and Lillian Lee. 2005. Seeing Stars: Ex-
ploiting Class Relationships for Sentiment Catego-
rization with Respect to Rating Scales. In Proceed-
ings of the 43rd Annual Meeting on Association for
Computational Linguistics, ACL ’05, pages 115–
124, Stroudsburg, PA, USA. Association for Com-
putational Linguistics.

Bo Pang and Lillian Lee. 2008. Opinion mining and
sentiment analysis. Found. Trends Inf. Retr., 2(1-
2):1–135.

Soujanya Poria, Erik Cambria, Rajiv Bajpai, and Amir
Hussain. 2017a. A review of affective computing:
From unimodal analysis to multimodal fusion. In-
formation Fusion, 37:98–125.

Soujanya Poria, Erik Cambria, Devamanyu Hazarika,
Navonil Majumder, Amir Zadeh, and Louis-Philippe
Morency. 2017b. Context-dependent sentiment
analysis in user-generated videos. In Proceedings of
the 55th Annual Meeting of the Association for Com-
putational Linguistics (Volume 1: Long Papers), vol-
ume 1, pages 873–883.

Soujanya Poria, Erik Cambria, Devamanyu Haz-
arika, Navonil Mazumder, Amir Zadeh, and Louis-
Philippe Morency. 2017c. Multi-level multiple at-
tentions for contextual multimodal sentiment analy-
sis. In Data Mining (ICDM), 2017 IEEE Interna-
tional Conference on, pages 1033–1038. IEEE.



3464

Soujanya Poria, Iti Chaturvedi, Erik Cambria, and
Amir Hussain. 2016. Convolutional mkl based mul-
timodal emotion recognition and sentiment analysis.
In Data Mining (ICDM), 2016 IEEE 16th Interna-
tional Conference on, pages 439–448. IEEE.

Soujanya Poria, Haiyun Peng, Amir Hussain, Newton
Howard, and Erik Cambria. 2017d. Ensemble appli-
cation of convolutional neural networks and multiple
kernel learning for multimodal sentiment analysis.
Neurocomputing, 261:217–230.

P. D. Turney. 2002. Thumbs up or thumbs down?: Se-
mantic orientation applied to unsupervised classifi-
cation of reviews. In Proceedings of the 40th Asso-
ciation for Computational Linguistics (ACL), pages
417–424, Philadelphia, USA.

A Zadeh, PP Liang, S Poria, P Vij, E Cambria, and
LP Morency. 2018a. Multi-attention recurrent net-
work for human communication comprehension. In
Thirty-Second AAAI Conference on Artificial Intel-
ligence (AAAI-2018), pages 5642 – 5649, New Or-
leans, USA.

A. Zadeh, R. Zellers, E. Pincus, and L. P. Morency.
2016. Multimodal Sentiment Intensity Analysis in
Videos: Facial Gestures and Verbal Messages. IEEE
Intelligent Systems, 31(6):82–88.

Amir Zadeh, Minghai Chen, Soujanya Poria, Erik
Cambria, and Louis-Philippe Morency. 2017. Ten-
sor fusion network for multimodal sentiment analy-
sis. In Proceedings of the 2017 Conference on Em-
pirical Methods in Natural Language Processing,
pages 1103–1114, Copenhagen, Denmark. Associ-
ation for Computational Linguistics.

Amir Zadeh, Paul Pu Liang, Navonil Mazumder,
Soujanya Poria, Erik Cambria, and Louis-Philippe
Morency. 2018b. Memory Fusion Network for
Multi-view Sequential Learning. arXiv preprint
arXiv:1802.00927.

Amir Zadeh, Paul Pu Liang, Soujanya Poria, Erik Cam-
bria, and Louis-Philippe Morency. 2018c. Mul-
timodal Language Analysis in the Wild: CMU-
MOSEI Dataset and Interpretable Dynamic Fusion
Graph. In Proceedings of the 56th Annual Meet-
ing of the Association for Computational Linguistics
(Volume 1: Long Papers), pages 2236–2246. Associ-
ation for Computational Linguistics.

7 Appendix

7.1 Multi-Modal Uni-Utterance - Self
Attention (MMUU-SA) Framework

Let Xp ∈ R3×d is the information matrix of the
pth utterance where the three ‘d’ dimensional rows
are the outputs of the time-distributed dense layer
for the three modalities. Computation in the pth

attention block proceeds as follows:

Mup = Xup .X
T
up

Nup(i, j) =
eMup (i,j)∑3
k=1 e

Mup (i,k)
for i, j = 1, 2, 3;

Oup = Nup .Xup

Aup = Oup �Xup

The attention matrix Aup ∈ R3×d is computed
separately for, p = 1st, 2nd, ... qth utterances. Fi-
nally, for each utterance p, Aup and Xup are con-
catenated and passed to the output layer for classi-
fication.

7.2 Multi-Utterance - Self Attention (MU-SA)
Framework

In MU-SA framework, we apply self attention on
the utterances of each modality separately, and use
these for classification. In contrast to MMUU-SA
framework, MU-SA utilizes the contextual infor-
mation of the utterances at the attention level. Let,
T ∈ Ru×d (text), V ∈ Ru×d (visual) and A ∈
Ru×d (acoustic) are the outputs of the dense lay-
ers. For the three modalities, three separate atten-
tion blocks are required, where each block takes
multi-utterance information of a single modality
and computes the self attention matrix. Specifi-
cally, the MU-SA attention (Av) on V (visual) will
be computed as follows,

Mv = V.V
T

Nv(i, j) =
eMv(i,j)∑u
k=1 e

Mv(i,k)
for i, j = 1, .., u

Ov = Nv.V

Av = Ov � V

The attention matrix Ap ∈ R3×d is computed for
p = 1st, 2nd, ...uth utterances. Finally, for each ut-
terance u, Ap and Xp are concatenated and passed
to the output layer with softmax activation for
classification.

7.3 Dataset Statistics

Dataset statistics are presented in Table 8.

7.4 Attention Computation

MMMU-BAVT attention computation is illustrated
in Figure 3.



3465

Statistics
CMU-MOSI CMU-MOSEI

Tr Dv Ts Tr Dv Ts

#Videos 52 10 31 2250 300 679
#Utterance 1151 296 752 16216 1835 4625
#Utterance/Video - Min 9 9 10 1 1 1
#Utterance/Video - Max 63 34 43 98 37 52
#Utterance/Video - Avg 24.692 22.9 22.129 7.207 6.116 6.821
#Positive 556 153 467 11498 1332 3281
#Negative 595 143 285 4718 503 1344
#Words/Utter. - Min 1 1 1 1 1 1
#Words/Utter. - Max 99 44 108 515 224 549
#Words/Utter. - Avg 11.533 10.786 13.176 18.227 18.498 18.658
#Utter-Len/Video - Min 0.219s 0.648s 0.229s 0.089s 0.22s 0.15s
#Utter-Len/Video - Max 38.233s 13.599s 31.957s 208.27s 90.42s 188.22s
#Utter-Len/Video - Avg 3.635s 3.538s 4.536s 6.896s 6.960s 7.158s
#Speakers 89 1000

(a) Data Statistics. Tr→Train set; Dv→Development set; Ts→Test set;

Table 8: Dataset statistics for MOSI (Zadeh et al., 2016) and MOSEI (Zadeh et al., 2018c).

X

X

X

XRow
softmax

Row
softmax

*

*

*
X

Figure 3: MMMU-BAVT attention computation.

7.5 Precision-Recall (PR) curve

We illustrate the precision, recall & f-measure for
different input combinations in Figure 4 & Figure
5.



3466

AV TA VT AVT

60

65

70

75

80

63.49

79.69 80.31
81.28

60.91

78.93
80.67 80.97

60.24

79.27
80.48 81.12

Precision Recall F1 Score

Figure 4: Precision, Recall & F-measure for differ-
ent input combinations in MMMU-BA architecture
of MOSI dataset.

AV TA VT AVT

60

65

70

75

70.73
72.03

73.32
74.5

59.71

70.56
69.42

68.5

64.75

71.28 71.31 71.37

Precision Recall F1 Score

Figure 5: Precision, Recall & F-measure for differ-
ent input combinations in MMMU-BA architecture
of MOSEI dataset.


