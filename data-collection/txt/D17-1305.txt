



















































Visual Denotations for Recognizing Textual Entailment


Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 2853–2859
Copenhagen, Denmark, September 7–11, 2017. c©2017 Association for Computational Linguistics

Visual Denotations for Recognizing Textual Entailment
Dan Han1

dan.han@aist.go.jp
Pascual Martı́nez-Gómez1

pascual.mg@aist.go.jp

Koji Mineshima2
mineshima.koji@ocha.ac.jp

1Artificial Intelligence Research Center, AIST
2Ochanomizu University

Tokyo, Japan

Abstract

In the logic approach to Recognizing
Textual Entailment, identifying phrase-to-
phrase semantic relations is still an un-
solved problem. Resources such as the
Paraphrase Database offer limited cover-
age despite their large size whereas unsu-
pervised distributional models of meaning
often fail to recognize phrasal entailments.
We propose to map phrases to their visual
denotations and compare their meaning in
terms of their images. We show that our
approach is effective in the task of Recog-
nizing Textual Entailment when combined
with specific linguistic and logic features.

1 Introduction and Related Work

Recognizing Textual Entailment (RTE) is a chal-
lenging task that was described as the best way
of testing an NLP system’s semantic capacity
(Cooper et al., 1994). In this task, given a text
T and a hypothesis H, the objective is to recognize
whether T implies H (yes), whether T contradicts
H (no) or otherwise (unk). For example, given:

(T) Some men walk in the tall and green grass.
(H) Some people walk in the field.

the system needs to recognize that T implies H
(yes). Although humans can easily solve these
problems, machines face great difficulties (Dagan
et al., 2013). RTE has been approached from dif-
ferent perspectives, ranging from purely statistical
systems (Lai and Hockenmaier, 2014; Zhao et al.,
2014) to purely logical (Bos et al., 2004; Abzian-
idze, 2015; Mineshima et al., 2015) and hybrid
systems (Beltagy et al., 2013).

We evaluate our idea on top of a logic system
since they generally offer a high precision and in-
terpretability, which is useful to our purposes. In

this approach, there are two main challenges. The
first challenge is to model the logical semantic
composition of sentences guided by the syntax and
logical words (e.g. most, not, some, every). The
second challenge is to introduce lexical knowledge
that describes the relationship between words or
phrases (e.g. men→ people, tall and green grass
→ field).

Whereas the relationship men → people can
be found in high precision ontological resources
such as WordNet (Miller, 1995), phrasal relations
such as tall and green grass→ field are not avail-
able in databases such as the Paraphrase Database
(PPDB) (Ganitkevitch et al., 2013) despite their
large size. Moreover, although unsupervised dis-
tributional similarity models have an infinite do-
main (given a compositional function on words),
they often fail to identify entailments (e.g. guitar
has a high similarity to piano but they do not en-
tail each other). To address these issues, Roller
et al. (2014) investigated supervised methods to
identify word-to-word hypernym relations given
word vectors whereas Beltagy et al. (2016) pro-
posed a mechanism to extract phrase pairs from
T and H and train a classifier to identify para-
phrases in unseen T-H problems. Our approach
is largely inspired by their work and our intention
is to increase the performance of these phrase and
sentence level entailment classifiers using multi-
modal features.

Our assumption is that the same concept ex-
pressed using different phrase forms is mapped to
similar visual representations since humans tend
to ground the meaning of phrases into the same
visual denotation. In a similar line, Kiela and Bot-
tou (2014) proposed a simple yet effective con-
catenation of pre-trained distributed word repre-
sentations and visual features, whereas Izadinia
et al. (2015) suggests a tighter parametric integra-
tion using a set of hand annotated phrasal entail-

2853



ment relations; however, their work was limited
to recognizing word or phrase relations, ignoring
the additional challenges that come in RTE which
we show is critical. Young et al. (2014) and Lai
and Hockenmaier (2014) did tackle sentence-level
RTE using visual denotations. However, their ap-
proach is only applicable to those RTE problems
whose words or phrases appear in the FLICKR30K
corpus, which is a considerable limitation. Lai
and Hockenmaier (2017) extended the approach
to also recognize unseen phrasal semantic rela-
tions using a neural network augmented with con-
ditional probabilities estimated from visual deno-
tations. Instead, our approach is much simpler and
similarly effective.

Our contribution is a method to judge phrase-
to-phrase semantic relations using an asymmetric
similarity scoring function between their sets of
visual denotations. We identify the conditions in
which this function contributes to sentence-level
RTE and show empirically its benefit. Our ap-
proach is simpler than previous methods and it
does not require annotated phrase relations. More-
over, this approach is not limited to specific cor-
pora or evaluation datasets and it is potentially lan-
guage independent.

2 Methodology

We formulate our framework in terms of a classi-
fier gθ : T × H → {yes, no, unk} that outputs
an entailment judgment for any text T ∈ T and
hypothesis H ∈ H. There are three key issues
in designing an effective classifier that uses visual
denotations: i) to discern when it is appropriate to
use visual denotations to recognize phrasal entail-
ments, ii) to extract candidate phrase pairs and iii)
to map those phrases into visual denotations1 and
measure their semantic similarity in terms of their
associated images.

Textual and Logic Features The first issue is
to understand the linguistic and logic limitations
of visual denotations in recognizing phrasal entail-
ments. From our observations, the linguistic phe-
nomena that make visual denotations ineffective
are word-to-word verb relations (e.g. laughing and
crying) since their associated images may depict
different actions with similar entities (e.g. pictures
of a baby crying are similar to those of a baby
laughing); antonym relations between any word

1We approximate the visual denotations of a phrase by
obtaining the images associated to that phrase.

in a phrase pair (e.g. similar images for big car
and small vehicle); and words that denote people
of different gender (e.g. boy versus lady, man ver-
sus woman) as they often display high visual sim-
ilarity compared to other entities. The logic phe-
nomena we identified signal sentences with small
differences in critical words, phrases or structures,
as in the presence of negations (e.g. images of no
cat still display cats), passive-active constructions
and subject-object case mismatches (e.g. images
of boy eats apple and apple eats boy are similar)
between T and H.

These logic phenomena can be easily detected
from logic formulas with the aid of the variable
unification during the theorem proving process.
For instance, using event semantics (Davidson,
1967; Parsons, 1990), an active sentence a boy
eats an apple and its corresponding passive sen-
tence an apple is eaten by a boy can be composi-
tionally mapped to the same logical formula, i.e.,
∃e∃x∃y(boy(x)∧apple(y)∧ eat(e)∧ (subj(e) =
x) ∧ (obj(e) = y)), while a boy eats an apple
and an apple eats a boy are mapped to different
formulas. When trying to prove the formula cor-
responding to H from the formula corresponding
to T, one needs to unify the variables contained in
these formulas, so that the non-logical predicates
such as boy, apple and eat in T and H are aligned
by taking into account logical signals.

Extract candidate phrase pairs between T and
H The second issue is to find candidate phrase
pairs between T and H for which we compare
their visual denotations. In our running example
(see Figure 1), a desirable candidate phrase pair
would be tall and green grass and field. We use
a tree mapping algorithm (Martı́nez-Gómez and
Miyao, 2016) that finds node correspondences be-
tween the syntactic trees of T and H. The search
is carried out bottom-up, guided by an ensemble
of cost functions. This ensemble rewards word or
phrase correspondences that are equal or if a lin-
guistic relationship (i.e. synonymy, hypernymy,
etc.) holds between them according to WordNet.
This tree mapping implicitly defines hierarchical
phrase pair correspondences between T and H.
We only select those phrase pairs for which both
phrases have less than 6 words. We believe that
discerning the entailment relation between longer
phrases should be left to the logic prover and the
compositional mechanism of meaning.

2854



Figure 1: Phrase-image mappings for the phrase pair tall and green grass and field in one RTE problem.

Visual Features At this stage it remains to mea-
sure the semantic relation between the candidate
phrase pairs (extracted with the tree mapping algo-
rithm described above) using their visual denota-
tions (see Figure 1 for a schematic diagram2). For
this purpose, we select the phrase pairs (t, h) with
highest and lowest similarity score. We define the
similarity score as the average cosine similarity
between the best image correspondences. That is:

score(t, h) =
1
|Ih|

∑
ihl ∈Ih

max
itk∈It

f(itk, i
h
l ) (1)

where It = {it1, . . . , itn} are the n images associ-
ated with phrase t from T and Ih = {ih1 , . . . , ihn}
are the n images for phrase h from H, for 1 ≤
l, k ≤ n. Note the asymmetry in Eq. 1 which cap-
tures semantic subsumptions (a picture of river is
among the pictures of body of water). The func-
tion f returns the cosine similarity between two
images:

f(itk, i
h
l ) = cos(vvv(i

t
k),vvv(i

h
l )) =

vvv(itk) ·vvv(ihl )
||vvv(itk)|| · ||vvv(ihl )||

(2)

where v(i) is the vector representation of an im-
age i. We obtain these vector representations con-
catenating the activations of the first 7 layers of
GoogLeNet (Szegedy et al., 2015) as it is common
practice (Kiela and Bottou, 2014).

Given the phrases with the highest and lowest
2 Due to copyright, images in this paper are a subset of

Google Image Search results for which we have a publishing
license. Nevertheless, they are faithful representatives.

similarity score,3 we extract four features from
each pair. The first feature is the similarity score
itself. The other three features capture statistics of
the relationship f(It × Ih) between the two sets
of visual denotations It and Ih. This relationship
f(It × Ih) is defined as the the matrix of image
cosine similarities:

f(It × Ih) =
f(it1, i

h
1) f(i

t
1, i

h
2) · · · f(it1, ihn)

f(it2, i
h
1) f(i

t
2, i

h
2) · · · f(it2, ihn)

...
...

. . .
...

f(itn, i
h
1) f(i

t
n, i

h
2) · · · f(itn, ihn)

 (3)
Specifically, these three features are:
• max f(It × Ih) returns the cosine similarity

between the two most similar images. This
feature is robust against polysemic phrases
(at least one image associated to pupil is simi-
lar to at least one image associated to student)
and hypernymy.
• averagef(It × Ih) returns the average simi-

larity across all image pairs and aims to mea-
sure the visual denotation overlap between
both phrases in the pair.
• min f(It×Ih) returns the similarity between

the two most different images and gives a no-
tion of how different the meanings of the two
phrases can be.

3 If there are no candidate phrase pairs, the T-H problem
is ignored. If there is only one phrase pair, such a pair is used
as the pair with highest and lowest score.

2855



All features above are concatenated into a fea-
ture vector which is paired with the T-H entailment
gold label to train the classifier.

3 Experiments

Our system is independent from the logic back-
end but we use ccg2lambda (Martı́nez-Gómez
et al., 2016)4 for its high precision and capabilities
to solve word-to-word divergences using WordNet
and VerbOcean (Chklovski and Pantel, 2004).

We evaluate our system on the SemEval-2014
version of the SICK dataset (Marelli et al., 2014)
with train/trial/test splits of 4, 500/500/4, 927 T-
H pairs and a yes/no/unk label distribution of
.29/.15/.56. We chose SICK for its relatively
limited vocabulary (2, 409 words) and short sen-
tences. The average T and H sentence length was
10.6 where 3.6 to 3.8 words appeared in T and
not in H or vice versa. We used scipy’s Random
Forests (Breiman, 2001) as our entailment classi-
fier with 500 trees and feature value standardiza-
tion, trained and evaluated on those T-H pairs for
which ccg2lambda outputs unknown (around
71% of the problems).

Using the tree mapping algorithm,5 we obtained
an average of 9.8 phrase pairs per T-H problem.
We obtained n = 30 images for every phrase using
Google Image Search API which we consider as
our visual denotations. The images and their vec-
tor representations were obtained between Sept.
2016 and Feb. 2017 using the image miner and
the feature extractor of Kiela (2016).6

Our main baseline is ccg2lambda when us-
ing only WordNet and VerbOcean to account for
word-to-word lexical divergences. ccg2lambda
is augmented with a classifier c that uses either
text and logic features t or image features from
10, 20, or 30 images (10i, 20i or 30i). On the
training data (Table 1), ccg2lambda obtains an
accuracy of 82.89%. Using our classifier with all
features, we carried out 10 runs of a 10-fold cross-
validation on the training data and we obtained
an accuracy (standard deviation) of 84.14 (0.06),
84.30 (0.14) and 84.28 (0.11) when using 10, 20
and 30 images, respectively. Thus, no significant
differences in accuracy were observed for differ-
ent numbers of images. When using only text
and logic features (c-t), the accuracy dropped

4 https://github.com/mynlp/ccg2lambda
5 https://github.com/pasmargo/t2t-qa
6 https://github.com/douwekiela/mmfeat

System Accuracy Std.
ccg2lambda 82.89 −
ccg2lambda, c-t-10i 84.14 0.06
ccg2lambda, c-t-20i 84.30 0.14
ccg2lambda, c-t-30i 84.28 0.11
ccg2lambda, c-t 76.60 0.03
ccg2lambda, c-20i 82.85 0.08

Table 1: Results (accuracy and standard devia-
tion) of the classifier c in a cross-validation on the
training split of SICK dataset using text and logic
features t for 10i, 20i and 30i images.

System Prec. Rec. Acc.
ccg2lambda + images 90.24 71.08 84.29
ccg2lambda, only text 96.95 62.65 83.13
L&H, text + images − − 82.70
L&H, only text − − 81.50
Illinois-LH, 2014 81.56 81.87 84.57
Yin & Schütze, 2017 − − 87.10
Baseline (majority) − − 56.69

Table 2: Results on the test split of SICK dataset
using precision, recall and accuracy. The system
“ccg2lambda + images” uses text and logics
features and 20 images per phrase: c-t-20i.

to 76.60 (0.03); when using only image features
(c-20i), the accuracy dropped to 82.85%. These
results show that using visual denotations to rec-
ognize phrasal entailments contributes to improve-
ments in accuracy and that the interaction with text
and logic features produces further gains.

On the test data, we obtained 1.1% higher accu-
racy (84.29 versus 83.13) over the ccg2lambda
baseline with a standard deviation of 0.07% over
10 runs (see Table 2) when using the setting
c-t-20i. As a comparison, Lai and Hocken-
maier (2017) obtain a similar accuracy increase
when using visual denotations (1.2%) with a sub-
stantially more complex approach that requires
training on the SNLI dataset (Bowman et al.,
2015), a much larger corpus.

The best SemEval-2014 system obtained an ac-
curacy of 84.57 (Lai and Hockenmaier, 2014)
and other heavily engineered, finely-tuned sys-
tems (Beltagy et al., 2016; Yin and Schütze, 2017)
reported up to 3% points of accuracy improvement
since then. Thus, our results are still below the
state of the art.

2856



Figure 2: True positive, ID: 4012; gold: yes.

Figure 3: False positive, ID: 1215; gold: unk.

Figure 4: False negative, ID: 1318; gold: yes.

4 Error analysis

We had an average of 126 true positives (gold la-
bel yes, system label yes) and 81 false positives
(gold label unk, system label yes) in our cross-
validation over the training data. Figure 2 shows
an example of a true positive where the tree map-
ping algorithm extracted the phrase pair kangaroo
that is little and baby kangaroo. The image sim-
ilarity features showed a high score causing the
classifier to correctly produce the judgment yes.
Figure 3 shows a false positive where the extracted
phrase pair was marsh and river and for which
the image similarity is unfortunately high. These
cases are common when comparing people (boy
and man) or scenery (such as beach and desert).

Figure 4 shows a false negative (gold label yes,
system label unk) where the candidate phrase pair
was plastic sword and toy weapon. In this case,
there was only one image with a plastic sword
within the images associated to toy weapon which
may have caused the cosine similarities to be low.

5 Discussion and Conclusion

In this paper we have evaluated our method on the
SICK dataset which was originally created from
image captions. For that reason, the proportion
of concepts with good visual denotations might be
higher than in typically occurring RTE problems.
Our future work is to assess the applicability of
our approach into other RTE problems such as the
RTE challenges, SNLI (Bowman et al., 2015) and
MultiNLI (Williams et al., 2017) datasets and fur-
ther investigate what syntactic or semantic units
can be best represented using visual denotations.

Another issue is the use of a commercial im-
age search API as a black box to retrieve images.
These search engines may include heuristics that
map similar phrases or keywords into the same
canonical form and that are difficult to control ex-
perimentally. However, we believe that our ap-
proach is still valid for a variety of image search
mechanisms and it is generally useful to resolve
lexical ambiguity at a high coverage.

We identified the conditions in which visual de-
notations are effective for sentence-level RTE and
devised a simple scoring function to assess phrasal
semantic subsumption, which may serve as the ba-
sis for more elaborated strategies. Our system is
independent on the semantic parser but the en-
tailment recognition mechanism requires a theo-
rem prover that displays remaining sub-goals. The
system and instructions are available at https:
//github.com/mynlp/ccg2lambda

Acknowledgments

This paper is based on results obtained from a
project commissioned by the New Energy and
Industrial Technology Development Organization
(NEDO). This project is also supported by JSPS
KAKENHI Grant Number 17K12747, partially
funded by Microsoft Research Asia and JST
CREST Grant Number JPMJCR1301, Japan. We
thank Ola Vikholt and the anonymous reviewers
for their valuable comments.

2857



References
Lasha Abzianidze. 2015. A tableau prover for natural

logic and language. In Proceedings of the 2015 Con-
ference on Empirical Methods in Natural Language
Processing, pages 2492–2502, Lisbon, Portugal. As-
sociation for Computational Linguistics.

Islam Beltagy, Cuong Chau, Gemma Boleda, Dan
Garrette, Katrin Erk, and Raymond Mooney. 2013.
Montague meets markov: Deep semantics with
probabilistic logical form. In Second Joint Con-
ference on Lexical and Computational Semantics
(*SEM), Volume 1: Proceedings of the Main Con-
ference and the Shared Task: Semantic Textual Sim-
ilarity, pages 11–21, Atlanta, Georgia, USA. Asso-
ciation for Computational Linguistics.

Islam Beltagy, Stephen Roller, Pengxiang Cheng, Ka-
trin Erk, and Raymond J. Mooney. 2016. Repre-
senting meaning with a combination of logical and
distributional models. Computational Linguistics,
42(4):763–808.

Johan Bos, Stephen Clark, Mark Steedman, James R
Curran, and Julia Hockenmaier. 2004. Wide-
coverage semantic representations from a CCG
parser. In Proceedings of the 20th international con-
ference on Computational Linguistics, pages 104–
111. Association for Computational Linguistics.

Samuel R. Bowman, Gabor Angeli, Christopher Potts,
and Christopher D. Manning. 2015. A large anno-
tated corpus for learning natural language inference.
In Proceedings of the 2015 Conference on Empiri-
cal Methods in Natural Language Processing, pages
632–642, Lisbon, Portugal. Association for Compu-
tational Linguistics.

Leo Breiman. 2001. Random forests. Machine learn-
ing, 45(1):5–32.

Timothy Chklovski and Patrick Pantel. 2004. VerbO-
cean: Mining the web for fine-grained semantic verb
relations. In Proceedings of EMNLP 2004, pages
33–40, Barcelona, Spain. Association for Computa-
tional Linguistics.

Robin Cooper, Richard Crouch, Jan van Eijck, Chris
Fox, Josef van Genabith, Jan Jaspers, Hans Kamp,
Manfred Pinkal, Massimo Poesio, Stephen Pulman,
et al. 1994. FraCaS–a framework for computational
semantics. Deliverable, D6.

Ido Dagan, Dan Roth, Mark Sammons, and Fabio Mas-
simo Zanzotto. 2013. Recognizing textual entail-
ment: Models and applications, volume 6. Morgan
& Claypool Publishers.

Donald Davidson. 1967. The logical form of action
sentences. In Nicholas Rescher, editor, The Logic of
Decision and Action. University of Pittsburgh Press.

Juri Ganitkevitch, Benjamin Van Durme, and Chris
Callison-Burch. 2013. PPDB: The paraphrase
database. In Proceedings of the 2013 Conference of

the North American Chapter of the Association for
Computational Linguistics: Human Language Tech-
nologies, pages 758–764, Atlanta, Georgia. Associ-
ation for Computational Linguistics.

Hamid Izadinia, Fereshteh Sadeghi, Santosh K. Div-
vala, Hannaneh Hajishirzi, Yejin Choi, and Ali
Farhadi. 2015. Segment-phrase table for semantic
segmentation, visual entailment and paraphrasing.
In The IEEE International Conference on Computer
Vision (ICCV).

Douwe Kiela. 2016. Mmfeat: A toolkit for extracting
multi-modal features. In Proceedings of ACL-2016
System Demonstrations, pages 55–60, Berlin, Ger-
many. Association for Computational Linguistics.

Douwe Kiela and Léon Bottou. 2014. Learning image
embeddings using convolutional neural networks for
improved multi-modal semantics. In Proceedings
of the 2014 Conference on Empirical Methods in
Natural Language Processing (EMNLP), pages 36–
45, Doha, Qatar. Association for Computational Lin-
guistics.

Alice Lai and Julia Hockenmaier. 2014. Illinois-LH: A
denotational and distributional approach to seman-
tics. In Proceedings of the 8th International Work-
shop on Semantic Evaluation (SemEval 2014), pages
329–334, Dublin, Ireland. Association for Computa-
tional Linguistics and Dublin City University.

Alice Lai and Julia Hockenmaier. 2017. Learning to
predict denotational probabilities for modeling en-
tailment. In Proceedings of the 15th Conference of
the European Chapter of the Association for Compu-
tational Linguistics: Volume 1, Long Papers, pages
721–730, Valencia, Spain. Association for Compu-
tational Linguistics.

Marco Marelli, Stefano Menini, Marco Baroni, Luisa
Bentivogli, Raffaella Bernardi, and Roberto Zam-
parelli. 2014. A SICK cure for the evaluation of
compositional distributional semantic models. In
Proceedings of LREC2014, pages 216–223.

Pascual Martı́nez-Gómez, Koji Mineshima, Yusuke
Miyao, and Daisuke Bekki. 2016. ccg2lambda: A
compositional semantics system. In Proceedings
of ACL-2016 System Demonstrations, pages 85–
90, Berlin, Germany. Association for Computational
Linguistics.

Pascual Martı́nez-Gómez and Yusuke Miyao. 2016.
Rule extraction for tree-to-tree transducers by cost
minimization. In Proceedings of the 2016 Confer-
ence on Empirical Methods in Natural Language
Processing, pages 12–22, Austin, Texas. Associa-
tion for Computational Linguistics.

George A. Miller. 1995. WordNet: A lexical
database for English. Communications of the ACM,
38(11):39–41.

2858



Koji Mineshima, Pascual Martı́nez-Gómez, Yusuke
Miyao, and Daisuke Bekki. 2015. Higher-order log-
ical inference with compositional semantics. In Pro-
ceedings of the 2015 Conference on Empirical Meth-
ods in Natural Language Processing, pages 2055–
2061, Lisbon, Portugal. Association for Computa-
tional Linguistics.

Terence Parsons. 1990. Events in the Semantics of En-
glish: A Study in Subatomic Semantics. The MIT
Press.

Stephen Roller, Katrin Erk, and Gemma Boleda. 2014.
Inclusive yet selective: Supervised distributional hy-
pernymy detection. In Proceedings of COLING
2014, the 25th International Conference on Compu-
tational Linguistics: Technical Papers, pages 1025–
1036, Dublin, Ireland. Dublin City University and
Association for Computational Linguistics.

Christian Szegedy, Wei Liu, Yangqing Jia, Pierre
Sermanet, Scott Reed, Dragomir Anguelov, Du-
mitru Erhan, Vincent Vanhoucke, and Andrew Ra-
binovich. 2015. Going deeper with convolutions. In
The IEEE Conference on Computer Vision and Pat-
tern Recognition (CVPR).

Adina Williams, Nikita Nangia, and Samuel R. Bow-
man. 2017. A broad-coverage challenge corpus for
sentence understanding through inference. CoRR,
abs/1704.05426.

Wenpeng Yin and Hinrich Schütze. 2017. Task-
specific attentive pooling of phrase alignments con-
tributes to sentence matching. In Proceedings of the
15th Conference of the European Chapter of the As-
sociation for Computational Linguistics: Volume 1,
Long Papers, pages 699–709, Valencia, Spain. As-
sociation for Computational Linguistics.

Peter Young, Alice Lai, Micah Hodosh, and Julia
Hockenmaier. 2014. From image descriptions to
visual denotations: New similarity metrics for se-
mantic inference over event descriptions. Transac-
tions of the Association for Computational Linguis-
tics, 2:67–78.

Jiang Zhao, Man Lan, and Tiantian Zhu. 2014. ECNU:
Expression- and message-level sentiment orienta-
tion classification in twitter using multiple effec-
tive features. In Proceedings of the 8th Interna-
tional Workshop on Semantic Evaluation (SemEval
2014), pages 259–264, Dublin, Ireland. Association
for Computational Linguistics and Dublin City Uni-
versity.

2859


