



















































Modeling Inter-Aspect Dependencies for Aspect-Based Sentiment Analysis


Proceedings of NAACL-HLT 2018, pages 266–270
New Orleans, Louisiana, June 1 - 6, 2018. c©2018 Association for Computational Linguistics

Modeling Inter-Aspect Dependencies for Aspect-Based Sentiment Analysis

Devamanyu Hazarika∗
School of Computing,

National University of Singapore
devamanyu@comp.nus.edu.sg

Soujanya Poria∗
Artificial Intelligence Initiative,

A*STAR, Singapore
sporia@ihpc.a-star.edu.sg

Prateek Vij
Department of Computer Science
and Engineering, IIT, Guwahati
v.prateek@iitg.ernet.in

Gangeshwar Krishnamurthy
Artificial Intelligence Initiative,

A*STAR, Singapore
gangeshwark@ihpc.a-star.edu.sg

Erik Cambria
School of Computer Science and

Engineering, NTU, Singapore
cambria@ntu.edu.sg

Roger Zimmermann
School of Computing,

National University of Singapore
rogerz@comp.nus.edu.sg

Abstract

Aspect-based Sentiment Analysis is a fine-
grained task of sentiment classification for
multiple aspects in a sentence. Present neural-
based models exploit aspect and its contextual
information in the sentence but largely ignore
the inter-aspect dependencies. In this paper,
we incorporate this pattern by simultaneous
classification of all aspects in a sentence along
with temporal dependency processing of their
corresponding sentence representations using
recurrent networks. Results on the benchmark
SemEval 2014 dataset suggest the effective-
ness of our proposed approach.

1 Introduction

Aspect-based Sentiment Analysis (ABSA) is a
fine-grained task of sentiment classification. Sen-
timentally involved sentences in reviews, debates,
etc., often comprise of multiple aspects that have
varied sentiment polarities. An important sub-
task of ABSA is aspect or aspect-term classifi-
cation which involves predicting sentiment of as-
pects embodied in a sentence (Young et al., 2017).
Present works in the literature approach this task
by analyzing associations between aspects and
their contexts provided in the sentence. In this
work, we argue that to classify an aspect into sen-
timent categories, knowledge of surrounding as-
pects, their sentiment orientation, and resulting
inter-dependencies, is beneficial.

Inter-aspect dependencies abound in sentences
with multiple aspects. Largely ignored in present

∗⋆ means authors contributed equally.

literature, these dependencies may reveal them-
selves in many forms, such as a) Incomplete in-
formation, where a certain aspect does not contain
enough contextual information to convey the sen-
timent. In such cases, the surrounding aspects and
their sentiment tone become crucial to fill the con-
textual gap. As an example, in the sentence The
menu is very limited - I think we counted 4 or 5
entries., the subsentence I think ... entries contain-
ing aspect entries does not provide the required
sentiment unless considered with the aspect menu.
Here, the negative sentiment of menu induces en-
tries to have the same sentiment. b) Sentiment in-
fluence in conjunctions, in which, the sentiment of
an aspect in a sentence influences the succeeding
aspects due to the presence of conjunctions. In
particular, for sentences containing conjunctions
like and, not only, also, but, however, though, etc.,
aspects tend to share/contrast their sentiments. In
the sentence Food is usually very good, though I
wonder about freshness of raw vegetables, the as-
pect raw vegetables does not have any sentiment
marker linked to it. However, the positive senti-
ment of food due to the word good and presence
of conjunction though determines the sentiment of
raw vegetables to be negative. Thus, aspects when
arranged as a sequence, reveal high correlation and
interplay of sentiments.

In this paper, we facilitate such phenomena by
proposing a neural network where the information
is shared among the aspects by means of a Long
Short-Term Memory (LSTM) network (Hochre-
iter and Schmidhuber, 1997). In other words,
we model the sequential relationship between the
aspects as per their occurrence in the sentence.

266



Specifically, our model first takes a sentence along
with all of its aspect-terms and then generates
the sentential representations relative to each as-
pect to get better aspect-oriented features (Tang
et al., 2016a). This is done using an attention-
based LSTM network, where the attention mech-
anism enables the model to focus on key parts
of the sentence that modulate the sentiment of
the aspects. To further guide the attention pro-
cess the model incorporates aspect information
at the word-level by concatenating aspect repre-
sentations with each word (Wang et al., 2016).
Finally, to capture the inter-aspect dependencies,
the aspect-based sentential representations are or-
dered as a sequence and temporally modeled using
another LSTM. Each timestep of this LSTM corre-
sponds to a particular aspect. The hidden state out-
put for each timestep is then projected to a dense
layer and fed to a softmax classifier to predict the
polarities of the corresponding aspect. To the best
of our knowledge, use of inter-aspect dependen-
cies in neural models is unprecedented and fills a
significant gap in the literature.

In the remaining paper, Section 2 first provides
a summary of existing works; Section 3 then de-
scribes the proposed approach in detail; Section 4
gives training and dataset details followed by re-
sults and a qualitative case study. Finally, Sec-
tion 5 concludes the paper.

2 Related Works

Traditional methods in this field leveraged sen-
timent lexicons to solve this task (Rao and
Ravichandran, 2009; Perez-Rosas et al., 2012)
whereas present methods have transitioned to
neural-based approaches. Tang et al. 2016a intro-
duced the idea of aspect-based sentential represen-
tations which generates a custom representation of
the sentence based on the aspect. This approach
has been heavily adapted by modern works. Wang
et al. 2016 built on this framework and introduced
attention mechanism for generating these senten-
tial features. They also incorporated aspect infor-
mation into the attention module by concatenat-
ing them with the words. More recently, Ma et al.
2017 proposed a model where both context and as-
pect representations interact with each other’s at-
tention mechanism to generate the overall repre-
sentation. Tay et al. 2017 proposed word-aspect
associations using circular correlation as an im-
provement over Wang et al.’s work. ABSA has

also been approached from a question-answering
perspective where memory networks have played
a major role (Tang et al., 2016b; Li et al., 2017).
Our work is different from all these works since
we train all aspects of a particular sentence to-
gether and capitalize on inter-aspect dependency
modeling which they ignore.

3 Proposed Approach

Let us take a sentence S = [w1, ...,wn] having
n words. Each word is represented as a low-
dimensional real-valued vector of size dem, called
word embedding. To get the embeddings, we use
the pre-trained Glove vectors (Pennington et al.,
2014) having dem = 300. We can thus represent S
as a matrix of dimensions Rdem×n.

The sentence S also contains m aspect-terms
(or aspects), where for each i ∈ [1,m], as-
pect Ai is a multi-word subsequence of S, i.e.,∃j ∈ [1, n], such that, Ai = [wj , ...,wj+∣Ai∣−1] ∈
Rdem×∣Ai∣. All the aspects A1, ...,Am are enumer-
ated as per their order of occurrence in the sen-
tence. The goal is to determine the sentiment label
for each of these m aspects belonging to S.

The proposed model comprises two distinct
phases (Figure 1). The first phase involves the
generation of aspect-based sentential representa-
tions s1, ..., sm, where, vector si is created by cou-
pling aspectAi with sentence S. The second phase
models the inter-aspect dependencies in a sentence
using an LSTM which is followed by the senti-
ment prediction for all the aspects.

3.1 Phase 1: Aspect-based sentential
representations

Below, we describe the methodology to generate
the ith aspect-based sentential representation si
for aspect Ai and sentence S.

Given sentence S and aspect-termAi, the model
first generates the aspect representation ti. This
is done by passing Ai through an LSTM, named
LSTMa, having internal dimension da. LSTMa’s
final hidden state vector h∣Ai∣a ∈ Rda is taken to be
this representation, i.e., ti = h∣Ai∣a .

Following this, an attention-based LSTM model
is used to create si using S and ti (Wang et al.,
2016). First, each word vector wj in S is con-
catenated with aspect ti to create a comprehensive
feature vector xji = (wj ; ti) ∈ R(dem+da), where
; is the concatenation operator. We then take this
new sequence representationXi = [x1i , ..., xni ] and

267



1   aspect-based  
sentential representation 

generator

aspect A1sentence S

t1 tn

aspect Ansentence S

Phase 1

Phase 2

s1 sn

classification

hadn
LSTMad

n   aspect-based  
sentential representation 

generator

thst

LSTMad LSTMad

i    aspect-based sentential representation generatorth

si

t i t i

w1 wn

hs
nh1s

LSTMs LSTMs LSTMs

LSTMa LSTMa

Figure 1: Overall architecture of the proposed method. The aspect-based sentential representation generator is
described in the right end of the figure.

apply an LSTM, named LSTMs with dimension
ds, to model the long-term temporal dependen-
cies within the sentence. The hidden state mem-
ory vectors across all n timesteps result in matrix
Hi = [h1s, ..., hns ] ∈ Rds×n.
Attention: Attention mechanism is applied on
Hi to get an attention vector α, which is in
turn used to generate a weighted representation
of Hi. We use this weighted representation to be
the ith aspect’s sentential representation si. Pre-
vious concatenations of words with the aspect-
representations infuse aspect information into the
attention process. This enables the attention mech-
anism to focus on relevant segments in the sen-
tence with respect to the aspect. The overall atten-
tion mechanism to generate si is summarized as:

M = tanh(Hi.Wh) (1)
α = softmax(MT .Wb) (2)

si =H.αT (3)
where, Wh ∈ Rn×1 and Wb ∈ Rds×n are projec-

tion parameters to be learnt during training and ds
is the dimension of the final sentence vector, i.e.,
si ∈ Rds .

The overall process described above is individ-
ually applied to all m aspects to get sentential rep-
resentations s1, ..., sm.

3.2 Phase 2: Inter-aspect relationship
To capture the implicit inter-aspect dependencies,
we model the sentential representations as a se-
quence [s1, ..., sm], following the order of occur-
rence of their corresponding aspect-terms in sen-

tence S. An LSTM, named LSTMad with dimen-
sion dad is then applied on this sequence and at
each of the ith timestep, its hidden state is pro-
jected to another vector having dimensions equal
to the number of classes to predict. Finally, soft-
max operation is applied on this vector to get the
prediction probabilities for the sentiment of this
ith aspect-term for sentence S. The transitions are
as follows:

[had1 , ..., hadm] = LSTMad([s1, ..., sm]) (4)
ŷi = softmax(Wad.hadi) (5)

Here, ŷi ∈ RC is the predicted probability distri-
bution for the ith aspect of sentence S where C is
the number of sentiment classes. Wad ∈ RC×dad is
a parameter and softmax(xi) = exi/∑j exj .
Loss Function: We use categorical cross-
entropy as the loss function which is averaged over
all aspects for a sentence. Thus, stochastic loss for
sentence S is calculated as:

Loss = −1
m

m∑
i=1

C∑
j=1yi,j log2(ŷi,j) + λ∣∣ θ ∣∣2 (6)

Here, m is the number of aspects for a sentence
and C is the number of sentiment categories. yi
is the one-hot vector ground truth of ith aspect of
sentence S and ŷi,j is its predicted probability of
belonging to sentiment class j. λ is the L2 - reg-
ularization term and θ is the parameter set, i.e.,
θ = {W[h,b,ad], LSTM[t,s,ad]}, where LSTM[]
represents the internal parameters of that LSTM.

4 Experimentation

Training details: To perform experiments and
subsequent hyperparameter tuning, we first split

268



the training set randomly in the ratio 9 ∶ 1 to get
a held-out validation set. For optimization, we use
the Adam optimizer (Kingma and Ba, 2014) hav-
ing learning rate 0.01. Embedding dimensions are
set as follows, da = 100, ds and dad = 300. To
facilitate batch processing, we attach dummy as-
pects in sentences with lesser aspects and also pro-
vide masking schemes. For termination, we use
the early-stopping procedure with a patience value
of 10 that is monitored on the validation loss.

Dataset: We conduct our experiments using the
dataset for SemEval 2014 Task 4 containing cus-
tomer reviews on restaurants and laptops. Each
review has one or more aspects with their corre-
sponding polarities. The polarity of an aspect can
be positive, negative, neutral or conflict; however,
we consider the first three labels for classification.
Table 1 contains the statistics for the dataset.

4.1 Results

Table 2 presents the results of our proposed model
along with state-of-the-art methods. Our model
significantly surpasses the performance of ATAE-
LSTM (Wang et al., 2016). Given that ATAE’s
architecture has a strong correlation to our aspect-
based sentential generator (see Figure 1), their
work can be categorized as a baseline to our
model. This reinforces our hypothesis that a model
capable of capturing inter-aspect dependencies in-
deed performs better. We also compare our model
to the recently proposed IAN (Ma et al., 2017). On
both datasets, our model performs competitively
with IAN and produces nominal improvement.
Given that IAN explores the inter-dependencies of
aspects with their contexts, while we try to model
inter-dependencies between aspects, an interesting
direction would be to explore the IAN modeled in
our proposed setting (Phase 2 of Figure 1). We set
this path as an option for future research.

Table 1 also presents variations of our proposed

Data
Aspect Labels No. of

reviewsPositive Negative Neutral

Rest.
Train 2148 790 628 1977

Test 725 195 196 600

Laptop
Train 974 839 450 1462

Test 340 125 169 411
∗ Rest. = Restaurant

Table 1: Labels and review statistics for the dataset Se-
mEval 2014.

Models
Attn. Fusion 3-way classification

Rest. Laptop

LSTM 7 - 74.3 66.5
AE-LSTM 3 Concat 76.6 68.9
ATAE-LSTM 3 Concat 77.2 68.7
IAN 3 - 78.6 72.1
Our Model 3 Hadamard 73.42 63.7
Our Model 7 Concat 74.5 69.6
Our Model 3 Concat 79.0 72.5
* Attn. = Attention, Rest. = Restaurant

Table 2: Accuracies for three-way classification on the
Restaurant and Laptop SemEval 2014 dataset.

model. Specifically, we try out variants (a) With-
out attention: in this setting, we omit the attention
mechanism while generating aspect-based senten-
tial representation si (Equation 1-3). Instead, we
define si to be hns , i.e., the last hidden state vec-
tor of LSTMs with input S and Ai. However, re-
moving attention brings degradation in the perfor-
mance of our model on the Restaurant and Laptop
dataset by 4% and 3%, respectively. This signi-
fies the importance of an attention mechanism to
derive the aspect-based sentential representations.
(b) With hadamard fusion: instead of concate-
nation of wj and ti, we use the hadamard prod-
uct which is the element wise multiplication of
the vectors. Although this variation reduces the
total parameter sizes of the network, it still does
not benefit the model and gives a poorer perfor-
mance to simple concatenation. Numerous other
fusion methods such as tensor fusion (Zadeh et al.,
2017), compact bilinear pooling (Gao et al., 2016),
attention-based fusion (Poria et al., 2017; Haz-
arika et al., 2018), etc. are applicable, whose anal-
yses, however, is not the focus of this paper.

4.2 Case Study

A qualitative study on the test set classifications
by our model reveals its capability to learn inter-
aspect dependencies (Section 1). For the sentence
I love the keyboard and the screen, the model cor-
rectly identifies the sentiment of screen as positive
which is hinted by positive aspect keyboard and
conjunction and. In another case, for the sentence
The best thing about this laptop is the price along
with some of the newer features, aspect features
is correctly classified as positive which is influ-
enced by aspect price and positive word best. This
shows that our model is performing well in clas-

269



sifying joint aspects having conjunctions. For the
slightly harder case of tackling incomplete infor-
mation, our model fares well in sentences having
this pattern. For example, one of the sentence Boot
up slowed significantly after all windows updates
were installed has aspect windows update which
does not have a clear sentiment orientation but is
implicitly dependent on the aspect boot up hav-
ing a negative sentiment. This was also correctly
classified by our model. Moreover, the above ex-
amples were incorrectly classified by ATAE. This
reaffirms our hypothesis that the ability to learn
inter-aspect dependencies is a crucial factor in the
task of ABSA.

5 Conclusion

In this paper, we present a way to incorporate
inter-aspect dependencies in the task of Aspect-
based Sentiment Analysis. Our results suggest
that capturing such information indeed improves
the task of prediction. Through this work, we
hope that future attempts by researchers include
this idea in their methods.

6 Acknowledgement

This research was supported in part by the Na-
tional Natural Science Foundation of China under
Grant no. 61472266 and by the National Univer-
sity of Singapore (Suzhou) Research Institute, 377
Lin Quan Street, Suzhou Industrial Park, Jiang Su,
People’s Republic of China, 215123. We would
also like to thank the anonymous reviewers for
their valuable feedback.

References

Yang Gao, Oscar Beijbom, Ning Zhang, and Trevor
Darrell. 2016. Compact bilinear pooling. In Pro-
ceedings of the IEEE Conference on Computer Vi-
sion and Pattern Recognition, pages 317–326.

Devamanyu Hazarika, Sruthi Gorantla, Soujanya Po-
ria, and Roger Zimmermann. 2018. Self-attentive
feature-level fusion for multimodal emotion detec-
tion.

Sepp Hochreiter and Jürgen Schmidhuber. 1997.
Long short-term memory. Neural computation,
9(8):1735–1780.

Diederik Kingma and Jimmy Ba. 2014. Adam: A
method for stochastic optimization. arXiv preprint
arXiv:1412.6980.

Cheng Li, Xiaoxiao Guo, and Qiaozhu Mei. 2017.
Deep memory networks for attitude identification.
In Proceedings of the Tenth ACM International Con-
ference on Web Search and Data Mining, pages 671–
680. ACM.

Dehong Ma, Sujian Li, Xiaodong Zhang, and Houfeng
Wang. 2017. Interactive attention networks for
aspect-level sentiment classification. arXiv preprint
arXiv:1709.00893.

Jeffrey Pennington, Richard Socher, and Christopher
Manning. 2014. Glove: Global vectors for word
representation. In Proceedings of the 2014 confer-
ence on empirical methods in natural language pro-
cessing (EMNLP), pages 1532–1543.

Veronica Perez-Rosas, Carmen Banea, and Rada Mi-
halcea. 2012. Learning sentiment lexicons in span-
ish. In LREC, volume 12, page 73.

Soujanya Poria, Erik Cambria, Devamanyu Haz-
arika, Navonil Mazumder, Amir Zadeh, and Louis-
Philippe Morency. 2017. Multi-level multiple at-
tentions for contextual multimodal sentiment analy-
sis. In 2017 IEEE International Conference on Data
Mining (ICDM), pages 1033–1038. IEEE.

Delip Rao and Deepak Ravichandran. 2009. Semi-
supervised polarity lexicon induction. In Proceed-
ings of the 12th Conference of the European Chap-
ter of the Association for Computational Linguistics,
pages 675–682. Association for Computational Lin-
guistics.

Duyu Tang, Bing Qin, Xiaocheng Feng, and Ting Liu.
2016a. Effective lstms for target-dependent sen-
timent classification. In Proceedings of COLING
2016, the 26th International Conference on Compu-
tational Linguistics: Technical Papers, pages 3298–
3307.

Duyu Tang, Bing Qin, and Ting Liu. 2016b. Aspect
level sentiment classification with deep memory net-
work. arXiv preprint arXiv:1605.08900.

Yi Tay, Anh Tuan Luu, and Siu Cheung Hui. 2017.
Learning to attend via word-aspect associative fu-
sion for aspect-based sentiment analysis. arXiv
preprint arXiv:1712.05403.

Yequan Wang, Minlie Huang, Xiaoyan Zhu, and
Li Zhao. 2016. Attention-based lstm for aspect-level
sentiment classification. In EMNLP, pages 606–
615.

Tom Young, Devamanyu Hazarika, Soujanya Poria,
and Erik Cambria. 2017. Recent trends in deep
learning based natural language processing. arXiv
preprint arXiv:1708.02709.

Amir Zadeh, Minghai Chen, Soujanya Poria, Erik
Cambria, and Louis-Philippe Morency. 2017. Ten-
sor fusion network for multimodal sentiment analy-
sis. In Proceedings of the 2017 Conference on Em-
pirical Methods in Natural Language Processing,
pages 1103–1114.

270


