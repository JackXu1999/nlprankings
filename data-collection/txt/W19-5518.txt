





























mhirano at the FinSBD Task: Pointwise Prediction Based on Multi-layer
Perceptron for Sentence Boundary Detection

Masanori HIRANO1∗ , Hiroki SAKAJI1 , Kiyoshi IZUMI1 , Hiroyasu MATSUSHIMA1
1School of Engineering, The University of Tokyo

hirano@g.ecc.u-tokyo.ac.jp, {sakaji, izumi, matsushima}@sys.t.u-tokyo.ac.jp

Abstract

This paper proposes a pointwise prediction for a
sentence boundary detection task. The proposed
pointwise prediction is combined with our original
word embedding method and three-layered percep-
tron. It predicts whether the targeted words have
the role of the beginning/end of a sentence or not
by using word features around the targeted words.
We tested our model by changing some parameters
in our model and then ensembled these models with
various parameters. Consequently, the ensembled
model achieved 0.88 and 0.84 averaged f1-score by
testing the data both in English and French, and it
also obtained 0.84 in English and 0.86 in French
as the final results of this shared task. In addition,
we developed a baseline model, that is, a rule-based
prediction model, for comparison. The result shows
that the proposed pointwise prediction model out-
performed the rule-based prediction model in any
index.

1 Introduction
This paper presents the application technique1 of the point-
wise prediction to a shared task of Sentence Boundary De-
tection in PDF Noisy Text in the Financial Domain for the
FinSBD 2019 shared task [Ait Azzi et al., 2019]2.

We address the sentence boundary detection problem in
PDF Noisy Text using a type of approach referred to as
“pointwise” prediction. Pointwise prediction is an approach
used to make every single independent decision at each point
by using only the features around a single point. In this task, a
pointwise prediction indicates predicting whether each word
has a role as the beginning/end of the sentence or not by us-
ing only the features around that word. This type of approach
was also used in Japanese morphological analysis, which is
a task to detect boundaries of the smallest meaningful units
because Japanese text has no spacing between each word.

∗Contact author; https://mhirano.jp/
1Our code is available on https://s.mhirano.jp/FinSBD2019
2https://sites.google.com/nlg.csie.ntu.edu.tw/finnlp/

shared-task-finsbd

The pointwise prediction for Japanese morphological anal-
ysis [Neubig and Mori, 2010; Neubig et al., 2011]3 achieved
high results. The advantages of this pointwise prediction are
its robustness and adaptiveness. Presently, many prediction
techniques based on machine learning exist. Machine learn-
ing, specifically recurrent neural network (RNN), remarkably
depends on features, and when one of the features is wrong,
the results through these machine learning can also be wrong.
However, the effect of the wrong feature or missing feature
information is supposed to be local when this type of point-
wise prediction is used. In addition, prediction using machine
learning techniques apart from the pointwise prediction, par-
ticularly RNN, requires many training data, but the training
data in this task are limited. Nevertheless, by using the point-
wise prediction, the training data become more abundant than
that of other machine learning techniques because the num-
ber of sentences is limited and much less than the number
of candidates for sentence boundaries. We supposed that the
pointwise prediction has some advantages on this prediction
task, that is, sentence boundary detection with noise.

For this shared task, we submitted two test predictions, one
of which is the result of our pointwise model and the other
one is that of a simple rule-based model only. Here, we fo-
cused on describing the first one, and the latter is treated as a
baseline model. In addition, we used the script that was dis-
tributed by the organizer of this shared task for evaluations.

2 Pointwise Prediction Model

First, we explain our pointwise prediction model. Figure 1
shows the model outline.

The proposed prediction model uses words around the tar-
get point, which is to be classified into the beginning of a sen-
tence, the end of a sentence, and others. That is, by using a
window size NW , our prediction model utilizes (NW ×2+1)
words, including a target point word and NW words before
and after the target point words. In Figure 1, a window size
of 4 is employed. These words to be input into a three-layered
perceptron also are embedded into vectors.

Next, we describe details about words embedding and the
multi-layer perceptron.

3This Japanese morphological analyzer is called “KyTea.”

102
Proceedings of the First Workshop on Financial Technology and Natural Language Processing 

(FinNLP@IJCAI 2019), pages 102-107, Macao, China, August 12, 2019.

https://mhirano.jp/
https://s.mhirano.jp/FinSBD2019
https://sites.google.com/nlg.csie.ntu.edu.tw/finnlp/shared-task-finsbd
https://sites.google.com/nlg.csie.ntu.edu.tw/finnlp/shared-task-finsbd


of   its   publication   .    Main   points   of   contract   for

E[-4] E[-3] E[-2] E[-1]  E[1]  E[2]  E[3]  E[4]

Three-Layered Perceptron

Three-Class Prediction
   - Beginning of a sentence

- End of a sentence
- Other

 E[0]

Target point

Embedding

Part of sentence

Embedded vectors

Machine learning

Classification

Figure 1: Pointwise prediction outline. This example employs a window size of 4. Words around the target point are gathered and embedded.
The embedded vectors are going through a three-layered perceptron and making a prediction for a target point. The prediction has three
classes: the beginning of the sentence, the end of the sentence, and other.

2.1 Words Embedding
For words embedding, we use three types of factors. Here are
the features for word embedding:

1. Word2vec (gensim in python3)
2. Part-of-speech(POS) tag (NLTK POS tag)
3. CAPITAL/non-capital patterns for each character in a

word
4. Whether including line feed code or not
Word2vec [Mikolov et al., 2013] is a method of translating

words into vectors. In this task, we used the training data
from this shared task organizers for the word2vec training
data, and the word2vec window size is set equal to the win-
dow size of our model NW . Moreover, the word embedding
dimension of word2vec ND is set as a hyperparameter. We
employed the vectors through this word2vec as the first ND
factors in the embedded vectors.

The POS tag is fetched from the Natural Language Toolkit
(NLTK) [Bird, 2006]. This NLTK’s POS tag data is obtained
via nltk in python3 and contains 45 types of POS pat-
terns. In case the POS tag classifications in nltk fail, we
added one additional class to the original 45 classes from
NLTK. Hence, this 46-class data are translated into one-hot
vectors and used as the subsequent 46 factors in the embed-
ded vectors.

The CAPITAL/non-capital patterns for each character in a
word are for recognizing words’ appearance patterns, such
as “This,” “this,” “THIS,” and others, including numbers and
symbols. We categorized each word into the following pat-
terns:

1. Numbers and/or characters (e.g., 2, #, -, .).
2. All characters are alphabet and non-capital (e.g., this,

have).
3. All characters are alphabet and CAPITAL (e.g., THIS,

HAVE).

4. All characters are alphabet and only the first character is
CAPITAL (e.g., This, Have).

5. Others that are not classified above.

These five classifications also are translated into one-hot vec-
tors and used as the next five factors of the embedded vectors.

The last one factor of the embedded vectors is whether to
include the line feed code or not. In the data, line feed code
is “\n.”

The details of the embedded vectors are described previ-
ously, and the embedded vectors for each word have a total
of ND + 46 + 5 + 1 = ND + 52 factors. In addition, the
embedded vectors of 2NW + 1 words are concatenated and
input into the subsequent three-layered perceptron; hence, the
total input vectors’ length become (ND + 52)× (2NW + 1).

2.2 Three-Layered Perceptron
We employed a three-layered perceptron as the classifica-
tion model. This three-layered perceptron has one input,
one output, and two hidden layers. The input layer has
(ND + 52)× (2NW + 1) nodes, both the hidden layers have
NH nodes each, and the output layer has three layers. The in-
put and first hidden layers, the first and second hidden layers,
and the second hidden and output layers are fully connected.
Additional details are presented in Appendix A.

3 Rule-based Prediction Model (Baseline
Model)

The rule-based prediction model that we developed is remark-
ably simple and is constructed as a baseline. This model has
two features definition: namely, features definition for begin-
nings and endings. The features definition of beginnings are
as follows:

1. The first character in a word is CAPITAL, whereas the
other characters are non-capital.

103



2. The word is “-” (hyphen).
3. The word is “(” (left bracket) and the subsequent of the

next word is “)” (right bracket).
4. The word is “` ” (backquote).

The features definition of endings are as follows:
1. The word ending with “.\n” (a period and a line feed

code).
2. The word is “.” (a period) and the previous and subse-

quent words are not digits.
3. The word ending with “;” (semi-colon).
4. The word ending with “;\n” (a semi-colon and a line

feed code).
5. The word ending with “:\n” (a colon and a line feed

code).
6. The word ending with “\n” (a line feed code)4.
Then, our rule-based model counts these features for each

word.
The next process is deciding where the sentences begin

and end based on the numbers of features of begins/ends.
Basically, the model supposed that the numbers of features
of begins/ends are not zero indicate the possibilities of be-
gins/ends. It finds the pairs of the beginning and ending of the
sentence that has a minimum length. If the words that have
possibilities of the same type of features comes continuously
(e.g., the words with possibilities of the beginning of the sen-
tence come again without the appearance of the words having
possibilities of the end of the sentence), then we adopt the
word that has the highest number of features as the beginning
or ending. The algorithm details are presented in Appendix
B.

4 Experiments
We were provided with the data by the organizers of this
shared task, containing the “training data” and “development
data” for two languages, i.e., English and French. We per-
formed the same experiments on each language. In addition,
we treated the “training data” as training data and the “devel-
opment data” as test data. (Actually, the organizers provided
also the “test data” to form the leaderboard of this shared task,
but we ignored these data in this paper other than final re-
sults.) These data contain sentences from the PDF Noisy Text
but were split well for each word, symbols, or something, the
list of the beginning of the sentences, and the list of the be-
ginning of the sentences 5.

Using these data, we tested our model. In our model, sev-
eral unfixed hyperparameters are the following:

1. NW : window size;
2. ND: the dimension for word2vec; and
3. NH : the number of nodes on every single hidden layer

in the three-layered perceptron.
4This feature sometimes overlaps with other features, but when

it is, we count it redundantly.
5Details are shown in https://sites.google.com/nlg.csie.ntu.edu.

tw/finnlp/shared-task-finsbd or [Ait Azzi et al., 2019].

We modified these hyperparameters as in Table 1 and tested
the model.

Hyperparameter Candidates

NW 5, 10, 200
ND 10, 50, 100
NH 300, 600, 1200

Table 1: Hyperparameter candidates for each hyperparameter.

Apart from these parameter sets, we ensembled the results
of all models. In the ensembling process, only the words
that two-thirds of all models agree with become the begin-
ning/ending of the sentences.

5 Results

NW ND NH BS ES Ave.

Ensemble 0.84 0.92 0.88
5 10 1200 0.83 0.92 0.88

10 100 600 0.82 0.91 0.87
5 50 600 0.82 0.91 0.87

10 50 1200 0.81 0.91 0.86
10 100 300 0.81 0.91 0.86
5 10 600 0.81 0.91 0.86

10 50 300 0.81 0.91 0.86
10 10 1200 0.82 0.90 0.86
5 100 1200 0.82 0.90 0.86
5 50 1200 0.82 0.90 0.86

20 50 1200 0.80 0.91 0.86
5 100 600 0.81 0.90 0.86

10 10 600 0.81 0.90 0.86
10 100 1200 0.81 0.90 0.86
20 100 600 0.80 0.91 0.86
5 10 300 0.81 0.90 0.86

20 100 1200 0.80 0.90 0.85
10 10 300 0.81 0.89 0.85
20 10 1200 0.79 0.91 0.85
20 10 600 0.79 0.91 0.85
5 50 300 0.80 0.89 0.85

20 50 600 0.79 0.90 0.85
5 100 300 0.80 0.89 0.85

20 50 300 0.78 0.90 0.84
20 100 300 0.78 0.90 0.84
20 10 300 0.77 0.89 0.83
10 50 600 0.74 0.80 0.77

Table 2: Results of the English language using the pointwise pre-
diction model and its hyperparameter sets. Here, BS/ES indicates
f1-score for the prediction of the beginning/ending of the sentences
and Ave. denotes the average of BS and ES.

Each result for each parameter sets using pointwise predic-
tion model are shown in Tables 2 and 4. Tables 2 and 3 are
results for English and Tables 4 and 5 are results for French.
An evaluation tool is provided by the organizers of this shared
task. The tool calculates the f1-scores for the prediction of

104

https://sites.google.com/nlg.csie.ntu.edu.tw/finnlp/shared-task-finsbd
https://sites.google.com/nlg.csie.ntu.edu.tw/finnlp/shared-task-finsbd


Ensemble model (En) Precision Recall F1-score

Others 1.00 0.99 0.99
Beginnings 0.81 0.87 0.84

Ends 0.87 0.98 0.92

Micro avg 0.99 0.99 0.99
Macro avg 0.89 0.95 0.92

Weighted avg 0.99 0.99 0.99

Table 3: Detailed results for English using the pointwise prediction
ensemble model.

NW ND NH BS ES Ave.

Ensemble 0.80 0.88 0.84
10 10 1200 0.79 0.86 0.83
10 50 1200 0.78 0.86 0.82
10 100 1200 0.77 0.87 0.82
10 100 600 0.77 0.86 0.82
20 50 1200 0.75 0.86 0.81
10 10 600 0.76 0.85 0.81
10 50 600 0.77 0.85 0.81
20 100 1200 0.76 0.86 0.81
5 10 1200 0.74 0.85 0.80

20 10 1200 0.73 0.86 0.80
10 50 300 0.73 0.84 0.79
20 10 600 0.73 0.84 0.79
20 50 600 0.74 0.84 0.79
20 100 600 0.74 0.84 0.79
10 10 300 0.72 0.84 0.78
5 10 600 0.72 0.83 0.78

10 100 300 0.72 0.83 0.78
20 100 300 0.71 0.83 0.77
5 100 600 0.70 0.83 0.77
5 100 1200 0.71 0.83 0.77
5 50 1200 0.72 0.82 0.77
5 50 600 0.72 0.82 0.77

20 50 300 0.69 0.82 0.76
5 10 300 0.70 0.82 0.76
5 50 300 0.69 0.80 0.75
5 100 300 0.67 0.80 0.74

20 10 300 0.66 0.81 0.74

Table 4: Results of the French language using the pointwise predic-
tion model and its each hyperparameter sets. Here, BS/ES indicates
f1-score for the prediction of the beginning/ending of the sentences
and Ave. denotes the average of BS and ES.

Ensemble model (Fr) Precision Recall F1-score

Others 0.99 0.99 0.99
Beginnings 0.74 0.86 0.80

Ends 0.82 0.94 0.88

Micro avg 0.98 0.98 0.98
Macro avg 0.85 0.93 0.89

Weighted avg 0.98 0.98 0.98

Table 5: Detailed results for French using the pointwise prediction
ensemble model.

BS ES Ave.

Pointwise Prediction (English) 0.84 0.92 0.88
Baseline: Rule-based (English) 0.73 0.81 0.76

Pointwise Prediction (French) 0.80 0.88 0.84
Baseline: Rule-based (French) 0.67 0.69 0.68

Table 6: Comparing the results of pointwise prediction with ensem-
bling and the results of the baseline model, i.e., rule-based model.

Test Final result

Pointwise Prediction (English) 0.88 0.84
Baseline: Rule-based (English) 0.76 0.63

Pointwise Prediction (French) 0.84 0.86
Baseline: Rule-based (French) 0.68 0.68

Table 7: Test and final results of all models and languages. The final
result is the result of this shared task and was on the leaderboard.

the beginning/ending of the sentences and others. F1-score
denotes the harmonic average of precision and recall.

Table 6 shows the comparison between the results of the
pointwise prediction model with ensembling and the results
of the baseline model, i.e., rule-based model. In both English
and French, our pointwise prediction model outperformed our
baseline model in any index.

Table 7 shows the test results from Table 6 and the final
results from this shared task’s leaderboard. Predictions for
the French language in the test and final results have a slight
difference, whereas those for the English language have sig-
nificant gaps. Specifically, the rule-based model in English
performs worse in the final results.

6 Discussion
First, based on Table 6, the proposed pointwise prediction
model outperformed the rule-based prediction model. Ev-
idently, the reason for the accurate predictions is that our
pointwise prediction model employs a type of feature learn-
ing. Rules for the rule-based model were implemented by us
and the rules are not sufficient. By using the feature learn-
ing, these processes, such as implementing rules, are not nec-
essary and render the model more adaptive for wide cases.
We employed only a simple three-layered perceptron, but this
model worked well, as observed in the final results’ leader-
board.

Second, the result of pointwise predictions using var-
ious parameters suggests interesting insights. Models
with (NW , ND, NH) = (10, 100, 600), (5, 10, 1200), and
(5, 50, 600) show the highest results in both English and
French. Table 8 shows the top results in various parame-
ters. As observed in this table, NW = 20 does not appear.
Thus, the window sizes are limited when the results are high.
This result shows that the beginning/ending of sentences can
be recognized only by watching approximately 5–10 words
around. In addition, the size of the hidden layer of the three-
layered perceptron NH of the top results tends to be high.

105



NW ND NH En Fr Ave.

10 100 600 0.87 0.83 0.85
5 10 1200 0.88 0.82 0.85
5 50 600 0.87 0.82 0.85

10 50 300 0.86 0.82 0.84
10 50 1200 0.86 0.81 0.84
10 100 1200 0.86 0.81 0.84
5 50 1200 0.86 0.81 0.84

Table 8: Top of the results in various parameters. Here, Ave. denotes
the average of the results of both English and French.

However, when NW = 10, ND = 100, the input length
for the three-layered perceptron must be 3192. Only when
NW = 5, ND = 10, it must be 682. These results suggest
that additional hidden layer sizes are possibly necessary

Third, the final result predictions in English were notice-
ably worse than those of the test results, whereas the French
data were not. We assume that the English test data for the
final results include some out-of-sample data. Both the point-
wise prediction and rule-based models perform poorly; the
characteristics of the data might be the cause of these lousy
results.

As future works, we have to test additional parameters for
NW , ND, NH . Moreover, we must change the fixed param-
eters in Appendix A. Not only the parameters but also the
feature learning models, apart from the three-layered percep-
tron, should be evaluated for their accuracy.

7 Conclusion
In this paper, we presented the application approach of point-
wise prediction to sentence boundary detection in the PDF
Noisy Text in the financial domain for the FinSBD 2019
shared task. Our point prediction model achieved 0.88 and
0.84 averaged f1-score for the beginning/ending of sentences
in English and French. In the final results, this model ob-
tained 0.84 in English and 0.86 in French. Evidently, the
proposed pointwise prediction model outperformed the rule-
based prediction model in any index. In our model, we em-
ployed some sets of parameters and ensembled models with
these parameter sets. The result shows that the ensemble
models outperformed any model without ensembling. How-
ever, other parameter sets that are also accurate for this task
are possible. Moreover, we fixed some parameters. As future
works, these parameters should also be modified.

Acknowledgments
Funding from Daiwa Securities Group is gratefully acknowl-
edged.

A Details of the Three-Layered Perceptron
Table 9 shows the parameters of the employed three-layered
perceptron. These parameters are totally fixed and not opti-
mized. Therefore, we will enhance these parameters in the
future. As observed in table 9, the maximum training epoch
is 10,000. However, training was forced to be stopped using
Algorithm 1.

Parameter

Activation function relu
Loss function Cross entropy loss
Optimize function SGD
Dropout True
Dropuout rate 0.2
Learning rate 0.01
Momentun 0.9
Weight decay 5× 10−4
Training data split 90% for training, 10% for evaluating.
Batch size 1,000
Max training epochs 10,000

Table 9: Parameters for three-layered perceptron

Algorithm 1 The algorithm for stop training
Input: Training data
Output: The best performed model

1: step← 0.
2: max acc← 0, max step← 0, acc list← [].
3: loop
4: Train(90% of training data).
5: acc← Evaluate(10% of training data).
6: acc list[step]← acc.
7: if acc ≥ max acc then
8: max acc← acc.
9: max step← step.

10: SaveModel().
11: end if
12: if step mod 10 = 0 and step ≥ 100 then
13: Calculate all 100 steps moving average of acc list.
14: M ←(Step with the best moving average).
15: if Step > max (M × 1.1,M + 200) then
16: Break.
17: end if
18: end if
19: step← step+ 1.
20: end loop
21: return max acc, max step

B Detailed Algorithm of the Rule-based
Prediction Model

The algorithm is shown in Algorithm 2.

106



Algorithm 2 The algorithm for rule-based prediction
Input: The list of the numbers of features of the begin-
ning/end of the sentences fob, foe
Output: The list of the beginning bos and the list of the end
of the sentences eos

1: i← 0.
2: bos← [], eos← [], status← False.
3: last start← 0, last end← 0.
4: loop
5: if fob[i] = 0 ∧ foe[i] = 0 then
6: Go to 34.
7: else if fob[i] = 0 ∧ foe[i] 6= 0 then
8: if status then
9: Append last start to bos.

10: status← False, last end← i.
11: else if foe[last end] < foe[i] then
12: last end← i.
13: end if
14: else if fob[i] 6= 0 ∧ foe[i] = 0 then
15: if status then
16: if fos[last start] < fos[i] then
17: last start← i.
18: end if
19: else
20: Append last end to eos.
21: status← True, last start← i.
22: end if
23: else
24: if status then
25: Append last start to bos.
26: Append i to eos.
27: last start← i.
28: else
29: Append i to bos.
30: Append last end to eos.
31: last end← i.
32: end if
33: end if
34: i← i+ 1.
35: end loop
36: return bos, eos

References
[Ait Azzi et al., 2019] Abderrahim Ait Azzi, Houda

Bouamor, and Sira Ferradans. The FinSBD-2019 Shared
Task: Sentence boundary detection in PDF Noisy text in
the Financial Domain. In The First Workshop on Financial
Technology and Natural Language Processing (FinNLP
2019), Macao, China, 2019.

[Bird, 2006] Steven Bird. NLTK: The Natural Language
Toolkit. In Proceedings of the COLING/ACL on Interac-
tive presentation sessions, pages 69–72, Morristown, NJ,
USA, 2006. Association for Computational Linguistics.

[Mikolov et al., 2013] Tomas Mikolov, Kai Chen, Greg Cor-
rado, and Jeffrey Dean. Distributed Representations of
Words and Phrases and their Compositionality. In Pro-
ceedings of Twenty-seventh Conference on Neural Infor-
mation Processing Systems (NeurIPS 2013), pages 3111–
3119, Stateline, Nevada, USA, 2013.

[Neubig and Mori, 2010] Graham Neubig and Shinsuke
Mori. Word-based Partial Annotation for Efficient Cor-
pus Construction. In Proceedings of the Seventh Interna-
tional Conference on Language Resources and Evaluation
(LREC 2010), pages 2723–2727, Valletta, Malta, 2010.
European Language Resources Association.

[Neubig et al., 2011] Graham Neubig, Yosuke Nakata, and
Shinsuke Mori. Pointwise Prediction for Robust , Adapt-
able Japanese Morphological Analysis. In Proceedings of
the 49th Annual Meeting of the Association for Computa-
tional Linguistics: Human Language Technologies (ACL
HLT 2011), pages 529–533, Portland, Oregon, USA, 2011.

107


