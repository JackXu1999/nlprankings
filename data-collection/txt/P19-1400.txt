



















































Distant Learning for Entity Linking with Automatic Noise Detection


Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 4081–4090
Florence, Italy, July 28 - August 2, 2019. c©2019 Association for Computational Linguistics

4081

Distant Learning for Entity Linking with Automatic Noise Detection

Phong Le1 and Ivan Titov1,2
1University of Edinburgh 2University of Amsterdam

lephong.xyz@gmail.com ititov@inf.ed.ac.uk

Abstract

Accurate entity linkers have been produced
for domains and languages where annotated
data (i.e., texts linked to a knowledge base)
is available. However, little progress has been
made for the settings where no or very limited
amounts of labeled data are present (e.g., le-
gal or most scientific domains). In this work,
we show how we can learn to link mentions
without having any labeled examples, only a
knowledge base and a collection of unanno-
tated texts from the corresponding domain.
In order to achieve this, we frame the task
as a multi-instance learning problem and rely
on surface matching to create initial noisy la-
bels. As the learning signal is weak and
our surrogate labels are noisy, we introduce
a noise detection component in our model: it
lets the model detect and disregard examples
which are likely to be noisy. Our method,
jointly learning to detect noise and link en-
tities, greatly outperforms the surface match-
ing baseline. For a subset of entity categories,
it even approaches the performance of super-
vised learning.

1 Introduction

Entity linking (EL) is the task of linking poten-
tially ambiguous textual mentions to the corre-
sponding entities in a knowledge base. Accurate
entity linking is crucial in many natural language
processing tasks, including information extraction
(Hoffart et al., 2011) and question answering (Yih
et al., 2015). Though there has been significant
progress in entity linking recently (Ratinov et al.,
2011; Hoffart et al., 2011; Chisholm and Hachey,
2015; Globerson et al., 2016; Yamada et al., 2017;
Ganea and Hofmann, 2017; Le and Titov, 2018),
previous work has focused on supervised learning.
Annotated data necessary for supervised learning
is available for certain knowledge bases and do-
mains. For example, one can directly use web-

pages linking to Wikipedia to learn a Wikipedia
linker. Similarly, there exist domain-specific sets
of manually annotated documents (e.g., AIDA-
CoNLL news dataset for YAGO (Hoffart et al.,
2011)). However, for many ontologies and do-
mains annotation is not available or limited (e.g.,
law). Our goal is to develop a method which does
not rely on any training data besides unlabeled
texts and a knowledge base.

In order to construct such a method, we use
an insight from simple surface matching heuris-
tics (e.g., Riedel et al. (2010)). Such heuristics
choose entities from a knowledge base by measur-
ing the overlap between the sets of content words
in the mention and in the entity name. For ex-
ample, in Figure 1, the entities BILL CLINTON
(PRESIDENT) and PRESIDENCY OF BILL CLIN-
TON both have two matching words with the men-
tion Bill Clinton. Whereas we will see in our ex-
periments that this method alone is not particularly
accurate at selecting the best entity, the candidate
lists it provides often include the correct entity.
This implies that we can both focus on learning
to select candidates from these lists and, less ob-
viously, that we can leverage the lists as weak or
distant supervision.

We frame this distance learning (DL) task as the
multi-instance learning (MIL) problem (Dietterich
et al., 1997). In MIL, each bag of examples is
marked with a class label: the label indicates that
the bag contains at least one example correspond-
ing to that class. Relying on such labeled bags,
MIL methods aim at learning classifiers for indi-
vidual examples.

Our DL problem can be regarded as a binary
version of MIL. For a list of entities (and im-
portantly given the corresponding mention and its
document context), we assume that we know if the
list contains a correct entity or not. The ‘posi-
tive lists’ are essentially top candidates from the



4082

Can Bill Clinton really emerge as a beloved father figure to a frazzled America ?
Bill_Clinton (TV episode)
Bill_Clinton (president)
Bill_Clinton's_victory
Presidency_of_Bill_Clinton
...

America (song)
Volunteers_of_America
United_States_of_America (nation)
United_States_of_America (music track)
...

name-matched candidates name-matched candidatesknowledge-base triples

Figure 1: We annotate raw sentences using entity names and knowledge base triples. In training, we keep only red
entities as positive candidates. In testing, we consider |E+| = 100 name-matched candidates.

matching heuristic. For example, the four candi-
date entities for the mention ‘Bill Clinton’ in Fig-
ure 1 could be marked as a positive set. The ‘neg-
ative lists’ are randomly sampled sets of entities
from the knowledge base. As with other MIL ap-
proaches, while relying on labeled lists, we learn
to classify individual entities, i.e. to predict if an
entity should be linked to the mention.

One important detail is that the classifier must
not have access to information which and how
many words match between the mention and the
entity name. If it would know this, it would eas-
ily figure out which entity set is a candidate list
and which one consists of randomly generated en-
tities based solely on this information. Instead, by
hiding it from the classifier, we force the classifier
to extract features of the mention and its context
predictive of the entity properties (e.g., an entity
type), and hence ensure generalization.

Unfortunately, our supervision is noisy. The
positive lists will often miss the correct entity for
the given mention. This confuses the MIL model.
In order to address this issue, we, jointly with
the MIL model, learn a classifier which detects
potentially problematic candidate lists. In other
words, the classifier predicts how likely a given
list is noisy (i.e., how much we should trust it).
The probability is then used to weight the cor-
responding term in the objective function of the
MIL model. By jointly training the MIL model
and the noise detection classifier, we effectively
let the MIL model choose which examples to use
for training. As we will see in our experimen-
tal analysis, this joint learning method leads to a
substantial improvement in performance. We also
confirm that the noise detection model is generally
able to identify and exclude wrong candidate lists
by comparing its predictions to the gold standard.

DL is the mainstream approach to learning re-
lation extractors (RE) (Mintz et al., 2009; Riedel

et al., 2010), a problem related to entity linking.
However, the two instantiations of the DL frame-
work are very different. For RE, a bag of sentences
is assigned to a categorical label (a relation). For
EL, we assign a bag of entities, conditioned on the
mention, to a positive class (correct) or a negative
class (incorrect).

We evaluate our approach on the news domain
for English as, having gold standard annotation
(AIDA CoNLL), we can both assess performance
and compute the upper bound, given by super-
vised learning. Nevertheless, we expect that our
methodology is applicable to a wider range of
knowledge bases, as long as unlabeled texts can
be obtained for the corresponding domain. We
plan to verify this claim in future work. In addi-
tion, we restrict ourselves to sentence-level mod-
eling and, unlike state-of-the-art supervised meth-
ods, (Yamada et al., 2017; Ganea and Hofmann,
2017; Le and Titov, 2018) ignore interaction be-
tween linking decisions in the document. Again,
it would be interesting to see if such global mod-
eling would be beneficial in the distance learning
setting.

Our contributions can be summarized as follows

• we show how the entity linking problem can
be framed as a distance learning problem,
namely as a binary MIL task;

• we construct a model for this task;

• we introduce a method for detecting noise in
the automatic annotation;

• we demonstrate the effectiveness of our ap-
proach on a standard benchmark.

2 Entity linking as MIL

For each entity mention m with context c, we
denote E+ and E− lists of positive candidates



4083

and negative candidates: E+ should have a high
chance of containing the correct entity e, while
E− should include only incorrect entities. As
standard in MIL, this will be the only supervision
the model receives at training time. When using
this supervision, the model will need to learn to
decide which entity e in E+ is most likely to cor-
respond to the mention-context pair (m, c). At test
time, the model with be provided with the list E+

and will need to select an entity from this list.
Performing entity linking in two stages, candi-

date selection (generating candidate lists) and en-
tity disambiguation (choosing an entity from the
list), is standard in EL, with the first stage usu-
ally handled with heuristics and the second one ap-
proached with statistical modeling (Ratinov et al.,
2011; Hoffart et al., 2011).

However, in our DL setting both stages change
substantially. The candidate selection stage relies
primarily on a surface matching heuristic, as de-
scribed in Section 4. Whereas supervised learning
for the disambiguation stage (e.g., Hoffart et al.
(2011)) is replaced with MIL learning as described
below in Section 3.1

To make the following sections clear, we intro-
duce the following terms.

Definition 1. A data point is a tuple
〈m, c,E+, E−〉 of mention m, context c, positive
set E+, and negative set E−. In testing, E− = ∅.
Definition 2. A data point 〈m, c,E+, E−〉 is
noisy if E+ does not contain the correct entity for
mention m. If a data point is not noisy, we will
refer to it as valid.

3 Models

We introduce two approaches. The first one di-
rectly applies MIL, disregarding the fact that many
data points are noisy. The second one addresses
this shortcoming by integrating a noise detection
component.

3.1 Model 1: MIL

Encoding context Context c is the entire l-word
sentence w1, ..., wl which also includes the men-
tion m = (wh, ..., wk), 1 ≤ h ≤ k ≤ l. We use
a BiLSTM to encode sentences. The input to the
BiLSTM is a concatenation w∗i = [wi,pi] where
pi ∈ Rdp is position embedding and w ∈ Rdw is

1Supervised learning is equivalent to assuming that E+

are singletons containing only the gold-standard entity.

from GloVe2 (Pennington et al., 2014). Forward
fi and backward bi states of BiLSTM are fed into
the classifier described below.

Entity embeddings In this work, we use a
simple and scalable approach which involves
computing entity embeddings on the fly using
associated types. For instance, the TV episode
BILL CLINTON is associated with several types
including BASE.TYPE ONTOLOGY.NON AGENT
and TV.TV SERIES EPISODE. Specifically, in
order to produce an entity embedding, each type t
is assigned a vector t ∈ Rdt . We then compute a
vector for entity e as

e = ReLU(We
1

|Te|
∑
t∈Te

t+ be),

where Te is the set of e’s types, and We ∈ Rde×dt ,
b ∈ Rde are a weight matrix and a bias vector.

More sophisticated approaches to producing en-
tity embeddings (e.g., using relational graph con-
volutional networks (Schlichtkrull et al., 2018))
are likely to yield further improvements.

Scoring a candidate We use a one-hidden layer
feed forward NN to compute score compatibility
between a context-mention pair (m, c) and an en-
tity e:

g(e,m, c) = FFNg([e, fh−1,bh−1, fk,bk])

If e∗ is the correct entity, we want g(e∗,m, c) >
g(e,m, c) for any entity e 6= e∗.

Training Recall that for each mention-context
pair (m, c), we have a positive set E+ and a neg-
ative set E−. We want to train the model to score
at least one candidate in E+ higher than any can-
didate in E−. We use the max-margin loss to
achieve this. Let

l(m, c) = [max
e∈E−

g(e,m, c) + δ − max
e∈E+

g(e,m, c)]+

L1 =
∑

(m,c)∈D

l(m, c)

where δ is a margin and [x]+ = x if x > 0 else
0; D is the training set. We want to minimize L1
with respect to the model parameters. We rely on
Adam optimizer and employ early stopping.

2 http://nlp.stanford.edu/data/glove.
840B.300d.zip

http://nlp.stanford.edu/data/glove.840B.300d.zip
http://nlp.stanford.edu/data/glove.840B.300d.zip


4084

3.2 Model 2: MIL with Noise Detection
(MIL-ND)

The model 1 ignores the fact that many data points
are noisy, i.e. E+ may not contain the correct en-
tity. We address this by integrating a binary noise
detection (ND) classifier which predicts if a data
point is noisy. Intuitively, data points classified as
noisy need to be discarded from training of the EL
model. In practice, we weight them with the confi-
dence of the ND classifier. As discussed below, we
train the ND classifier jointly with the EL model.

Representation forE+ The ND classifier needs
to decide if there is at least one entity in the listE+

corresponding to the mention-context pair (m, c).
The question is now how to represent E+ to make
classification as easy as possible. One option is
to use mean pooling, but this would result in un-
informative representations, especially for longer
candidate lists. Another option is max pooling,
but it would not take into account which mention-
context pair (m, c) is currently considered, so also
unlikely to yield informative features of E+. In-
stead we use attention, with the attention weight
computed as a function of (m, c):

eE+ =
∑
e∈E+

αee

where αe are attention weights

αe =
exp{g′(e,m, c)/T}∑

e′∈E+ exp{g′(e′,m, c)/T}
,

where g′ is a score function. Instead of learn-
ing a separate attention function for the ND clas-
sifier, we reuse the one from the EL model, i.e.
g = g′. This will reduce the number of param-
eters and make the method less prone to overfit-
ting. Maybe more importantly, we expect that the
better the entity disambiguation score function is,
the better the ND classifier is, so tying the two to-
gether may provide an appropriate inductive bias.
T is temperature, controlling how sharp αe should
be. We found that a small T = 1/3 stabilizes the
learning.

Noise detection We use a binary classifier to de-
tect noisy data points. The probability that a data
point is noisy is defined as

pN (1|m, c,E+) =

σ

(
FFNf ([eE+ , fh−1,bh−1, fk,bk])

T

)
,

σ is the logistic sigmoid function. For simplicity,
we use the same T as above.

Training Our goal is to down-weight potentially
noisy data points. Our new loss is

L2 =
∑

(m,c)∈D

pN (0|m, c,E+)l(m, c)+

η × KL(
∑

(m,c)∈D pN (·|m, c,E+)
|D|

|p∗N ),

where p∗N is a prior distribution indicating our be-
liefs about the proportion of noisy data points; η
is a hyper-parameter. We optimize the objective
with respect to the parameters of both ND and EL
models. The second term is necessary, as without
it the loss can be trivially minimized by the ND
classifier predicting that all data points are noisy
with the probability of 1. This would set the first
term to exactly zero.

Intuitively, when using the second term, the
model can disregard certain data points but disre-
garding too many of them incurs a penalty. Which
data points are likely to be disregarded? Presum-
ably the ones less consistent with the predictions
of the EL model. In other words, joint training of
EL and ND models encourages learning an entity-
linking scoring function consistent with a large
proportion of the data set but not necessarily with
the entire data set. As we will see in the exper-
imental section, the ND classifier indeed detects
noisy data points rather than chooses some ran-
dom subset of the data.3

We use the same optimization procedure as for
the model 1. The second term is estimated at the
mini-batch level.

Testing Differently from model 1, with model 2
we have two options on how to use it at test time:

• ignoring the ND classifier, thus doing entity
disambiguation the same way as for model 1,
or

• using the ND classifier as a mechanism to
decide if the test data point should be clas-
sified as ‘undecidable’ or not. Specifically, if
pN (1|m, c,E+) > τ , model 2 will not output
an entity for this data point. This should in-
crease precision, as at test time E+ also may
not contain the correct entity.

3The second term is similar to that used in posterior regu-
larization (Ganchev et al., 2010) and generalized expectation
criteria method (Mann and McCallum, 2010).



4085

Set # sentences # mentions
Train 170,000 389,989
Dev 2,275 4,603
Test 2,414 4,286

Table 1: The statistics of the proposed dataset.

We call the two versions MIL-ND and τMIL-ND,
respectively.

4 Dataset

We describe how we create our dataset. We use
Freebase4, though our approach should be appli-
cable to many other knowledge bases. Brief statis-
tics of the dataset are shown in Table 1.

4.1 Training set

We took raw texts from the New York Times cor-
pus, tagged them with the CoreNLP named entity
recognizer5 (Manning et al., 2014). We then se-
lected only sentences that contain at least two en-
tity mentions. We did this because on the one hand
in most applications of EL we care about relations
between entities (e.g., relation extraction), on the
other hand, it provides us with an opportunity to
prune the candidate list effectively, as discussed
below. Note that we do it only for training.

For each mention m we carried out candidate
selection as follows. First, we listed all entities
which names contain all words of m. For in-
stance, “America” (Figure 1) can be both the na-
tion UNITED STATES OF AMERICA and Simon
& Garfunkel’s song AMERICA. We ranked these
chosen entities by the entity ordering in the knowl-
edge base (i.e., the one that appears first in the
knowledge base would be ranked first); for Free-
base this order is correlated with prominence.

Second, for each mention (e.g., “Bill Clinton”),
we kept only entities which participate in a relation
with one of the candidate entities for another men-
tion in the sentence. For example, BILL CLIN-
TON (PRESIDENT) is kept because it is in the PER-
SON.PERSON.NATIONALITY relation with the en-
tity UNITED STATES OF AMERICA (NATION).

Last, to keep candidate lists manageable, we se-
lected only |E+| = 100 candidates from step 2 for

4https://developers.google.com/
freebase/. Freebase is chosen because it contains
the largest set of entities among available knowledge bases
(Färber and Rettinger, 2018).

5https://stanfordnlp.github.io/
CoreNLP/

15.16%

29.85%

45.10%
50.10%

63.61%
69.09%

76.25% 76.67%

# positive candidates

O
ra

cl
e 

Re
ca

ll

0.00%

20.00%

40.00%

60.00%

80.00%

1 10 100 1000 10000 100000

Figure 2: Oracle recall as a function of |E+| (the num-
ber of positive candidates) on the development set.

each mention as positive candidates. During train-
ing, we sampled |E−| = 10 candidates from the
rest of the knowledge base as negative candidates.

4.2 Development and test sets

We took manually annotated AIDA-A and AIDA-
B as development and test sets (Hoffart et al.,
2011). We turned the ground truth Wikipedia links
in these sets to Freebase entities, thanks to the
mapping available in Freebase.6

Candidate selection was done in the same way
as for training, except for not filtering out sen-
tences with only 1 entity (i.e. no step 2 from
Section 4.1). The oracle recall for surface name
matching (i.e. step 1 from Section 4.1) is 77%. It
goes down to 50% if we restrict |E+| = 100 (see
Figure 2). We believe that there are straightfor-
ward ways to improve the selection heuristic (e.g.,
modifying the string matching heuristic or using
word embeddings to match words in entity names
and words in the mention) but we leave this for
future work.

Note that because AIDA CoNLL dataset is
based on Reuters newswire articles, these develop-
ment and test sets do not overlap with the training
set.

5 Experiments

We evaluated the models above using the data
from Section 4. The source code and the
data are available at https://github.com/
lephong/dl4el

We ran each model five times and report mean
and 95% confidence interval of three metrics:

6We could not handle NIL cases here because the knowl-
edge base used to annotate AIDA-CoNLL is different from
the one we use.

https://developers.google.com/freebase/
https://developers.google.com/freebase/
https://stanfordnlp.github.io/CoreNLP/
https://stanfordnlp.github.io/CoreNLP/
https://github.com/lephong/dl4el
https://github.com/lephong/dl4el


4086

(micro) precision, (micro) recall, and (micro) F1
(Cornolti et al., 2013) under two settings:

• ‘All’: all mentions are taken into account,

• ‘In E+’: only mentions with E+ containing
the correct entity are considered.

The latter, though not realistic, is interesting as it
lets us concentrate on the contribution of the dis-
ambiguation model, and ignore cases which are
hopeless with the considered candidate selection
method.

Note that, for system outputting exactly one en-
tity for each mention (e.g., MIL model 1), preci-
sion and recall are equal.

5.1 Systems

We compared our models against ‘Name match-
ing’. It was proposed by Riedel et al. (2010) for
RE: a mention is linked to an entity if it matches
the entity’s name. For tie cases, we chose the
first matched entity appearing in Freebase. For in-
stance, “America” is linked to the song instead of
the nation. To our knowledge, name matching is
the only method tried in previous work for our set-
ting (i.e. with no annotated texts).

We also compared with a supervised version of
model 1. We used the same method in Section 4.2
to convert AIDA CoNLL training set, withE+ be-
ing singletons consisting of the correct entity pro-
vided by human annotators. This system can be
considered as an upper-bound of our two models
because: (i) it is trained in supervised rather than
MIL setting with gold standard labels rather than
weak supervision, and (ii) the training set is in the
same domain (i.e. Reuter) with the test set. Al-
though it uses only entity types but no other entity-
related information for entity disambiguation, in
Appendix B we show that this system performs on
par with Hoffart et al. (2011) when evaluated in
their setting.

Note that comparison with supervised linkers
proposed in previous work is not possible as they
require Wikipedia (see Section 6) for candidate se-
lection, as a source of supervision, and often for
learning entity embeddings.

We tuned hyper-parameters on the development
set. Details are in Appendix A. Note that, in model
2 (both MIL-ND and τMIL-ND), we set the prior
p∗N (1) to 0.9, i.e. requiring 90% of training data

points should be ignored.7 We experimented with
|E+| = 100 for both training and testing. For
training, we set |E−| = 10.

5.2 Results

Table 2 shows results on the test set. ‘Name
matching’ is far behind the two models. Many en-
tities in the knowledge base have similar or even
identical names, so relying only on the surface
form does not result in an effective method.8

MIL-ND achieves higher precision, recall, and
F1 than MIL, this suggests that the ND classifier
helped to eliminate bad data points during train-
ing. Using its confidence at test time (τMIL-ND,
‘All’ setting) was also beneficial in terms of pre-
cision and F1 (it cannot possibly increase recall).
Because all the test data points are valid for the ‘In
E+’ setting, using the ND classifier had a slight
negative effect on F1.

MIL-ND significantly outperforms MIL: the
95% confidence intervals for them do not over-
lap. However, this is not the case for MIL-ND
and τMIL-ND. We therefore conclude that the ND
classifier is clearly helpful for training and poten-
tially for testing.

5.3 Analysis

Error types In Table 3 we classified errors ac-
cording to named entity types thanks to the an-
notation from Tjong Kim Sang and De Meulder
(2003). PER is the easiest type for all systems.
Even name matching, without any learning, can
correctly predict in half of the cases.

For LOC, it turns out that candidate selection is
a bottleneck: when candidate selection was flaw-
less, the models made only about 12% errors,
down from about 57%. For MISC a similar con-
clusion can be drawn.

Can the ND classifier detect noise? From the
training set, we collected 100 data points and man-
ually checked if a data point is valid (i.e., E+ con-
tains the correct entity). We then checked how
the accuracy changes depending on the threshold
τ (Figure 3), the accuracy is defined as

# valid data points with pN < τ
# all data points with pN < τ

7 Section 5.3 shows that 90% is too high, but it helps the
model to rely only on those entity disambiguation decisions
that are very certain.

8For instance, there are 36 entities named BILL CLINTON,
and 248 entities having ‘Bill Clinton’ in their name.



4087

All In E+

System P R F1 P R F1
Name matching 15.03 15.03 15.03 29.13 29.13 29.13
MIL (model 1) 35.87 35.87 35.87 ±0.72 69.38 69.38 69.38 ±1.29
MIL-ND (model 2) 37.42 37.42 37.42 ±0.35 72.50 72.50 72.50 ±0.68
τMIL-ND (model 2) 38.91 36.73 37.78 ±0.26 73.19 71.15 72.16 ±0.48
Supervised learning 42.90 42.90 42.90 ±0.59 83.12 83.12 83.12 ±1.15

Table 2: Results on the test set under two settings. 95% confidence intervals of F1 scores are shown.

All In E+

System LOC ORG PER MISC LOC ORG PER MISC
Name matching 96.26 89.48 57.38 96.60 92.32 76.87 47.40 76.29

MIL 57.09 76.30 41.35 93.35 11.90 47.90 27.60 53.61
MIL-ND 57.15 77.15 35.95 92.47 12.02 49.77 20.94 47.42
τMIL-ND 55.15 76.56 34.03 92.15 11.14 51.18 20.59 40.00

Supervised learning 55.58 61.32 24.98 89.96 8.80 14.95 7.40 29.90

Table 3: % errors on the development set for different named entity types under two settings. (Smaller is better.)

τ

ac
cu

ra
cy

 (%
)

0.00%

25.00%

50.00%

75.00%

100.00%

0.7 0.8 0.9 1

Figure 3: Accuracy vs τ . There are large plateaus
between τ ∈ (0.7, 0.95) and τ < 0.6 because the
ND classifier hardly used these ranges. We hence set
τ = 0.75 in τMIL-ND.

As expected, the smaller τ is, the higher the
chance is that the chosen data point is valid (i.e.,
not noise). Hence, we can use the ND classifier to
select high quality data points by adjusting τ .

For a further examination, from the training set,
we collected all 47,213 data points (i.e. 27.8%)
with pN (1|m, c,E+) > τ = 0.75, and randomly
chose 100 data points. We found that 89% are in-
deed noisy. This further confirms that the ND clas-
sifier is sufficiently accurate. Some examples are
given in Table 4.

Number of positive candidates We also experi-
mented with different values of |E+| (10, 50, 100)
on the development set (Figure 4).

First, MIL-ND and τMIL-ND are always better

# positive candidates

F1

10.00%

20.00%

30.00%

40.00%

10 50 100

MIL MIL-ND τMIL-ND

# positive candidates

F1

70.00%

73.67%

77.33%

81.00%

10 50 100

MIL MIL-ND τMIL-ND

Figure 4: F1 (top:‘All’; bottom:‘In E+’) on the devel-
opment set with different numbers of positive candi-
dates.

than MIL. This is more apparent in the ‘In E+’
settings: with this evaluation regime, we zoom in
on cases where our models can predict correct en-
tities (of course, all models equally fail for exam-
ples outside E+).

Using the ND classifier at test time to decide to
predict any entity or skip (τMIL-ND) is helpful in



4088

Correctly detected as noise:
* Small-market teams , like Milwaukee and San Diego , even big-market clubs , like Boston ,
Atlanta and the two [Chicago] franchises , trumpet them .
Candidates: CHICAGO (music single), BOSTON TO CHICAGO (music track)
* The politically powerful [Green] movement in Germany has led public opposition to genetic
technology research and production .
Candidates: THE GREEN PRINCE (movie), THE GREEN ARCHER (movie), GREEN GOLD (movie)
Incorrectly detected as noise:
* Everything Forrest remains unaffected by , [Jenny] self-indulgently , self-destructively drowns in
: radical politics , drug abuse , promiscuity .
Candidates: JENNY CURRAN (book/film character)

Table 4: Examples of 100 randomly chosen sentences from the training set whose pN (1|m, c,E+) > τ = 0.75.
The first two examples are correctly detected as noise by our ND classifier. The last one is incorrectly detected.

the more realistic ‘All’ setting. The difference be-
tween τMIL-ND and MIL-ND is less pronounced
for larger E+. This is expected as the proportion
of valid data points is higher, and hence the ND
classifier is less necessary at test time. For ‘inE+’
setting, τMIL-ND performs worse than MIL-ND,
as we expected, because there are no noisy data
points at test time.

What is wrong with the candidate selector?
The above results show that candidate selection
is a bottleneck and that the used selector is far
from perfect. We found two cases where the se-
lector is problematic: (i) the mention or the en-
tity name is in an abbreviated form, such as ‘U.N.’
rather than ‘United Nations’, (ii) the mention and
the entity’s name only fuzzily match, such as ‘[En-
glish] county’ and ENGLAND (country). We can
overcome these problems via extending our sur-
face matching as in Charton et al. (2014); Usbeck
et al. (2014) or using word embeddings.

Even in some cases when the selector does
not have any problems with surface matching,
the number of candidates may be too large. For
instance, consider ‘[Simpson] killed his wife...’,
there are more than 1,500 entities in the knowl-
edge base containing the word ‘Simpson’. It is
unlikely that our entity disambiguation model can
deal with such large lists. We may need a stronger
mechanism for reducing the number of candidates.
For example, we could use document-level infor-
mation to discard highly unlikely entities.

6 Related work

High performance approaches to EL, such as Rati-
nov et al. (2011); Chisholm and Hachey (2015);
Globerson et al. (2016); Yamada et al. (2017);

Ganea and Hofmann (2017), are two-stage meth-
ods: candidate generation is followed by select-
ing an entity for the candidate lists. We follow
the same paradigm but with some important dif-
ferences discussed below.

Most approaches use alias-entity maps, i.e.
weighted sets of (mention, entity) pairs created
from anchors in Wikpedia. For example, one
can count how many times phrase “the president”
refers to BILL CLINTON to assign the weight to
the corresponding pair. However, the method re-
quires large annotated datasets, and it cannot deal
with less prominent entities. As we do not have
access to links, we use surface matching instead.

To choose an entity from a candidate list, two
main disambiguation frameworks (Ratinov et al.,
2011) are introduced: local which resolves men-
tions independently, and global which makes use
of coherence modeling at the document level.
Though we experimented with local models, the
local-global distinction is largely orthogonal as we
can directly integrate coherence modeling compo-
nents in our DL approach.

Different types of supervision have been con-
sidered in previous work: full supervision (Ya-
mada et al., 2017; Ganea and Hofmann, 2017; Le
and Titov, 2018), using combinations of labeled
and unlabeled data (Lazic et al., 2015), and even
distant supervision (Fan et al., 2015). The ap-
proach of Fan et al. (2015) is heavily Wikipedia-
based: they rely on a heuristic mapping from Free-
base entities to Wikipedia entities, and learn fea-
tures from Wikipedia articles. Unlike ours, their
approach cannot be generalized to set-ups where
no documents are available for entities.



4089

7 Conclusions

We introduced the first approach to entity linking
which neither uses annotated texts, nor assumes
that entities are associated with textual documents
(e.g., Wikipedia articles).

We learn the model using the MIL paradigm,
and introduce a novel component, a noise detect-
ing classifier, estimated jointly with the EL model.
The classifier lets us disregard noisy labels, re-
sulting in a more accurate entity linking model.
Experimental results showed that our models sub-
stantially outperform the heuristic baseline, and,
for certain categories, they approach the model es-
timated with supervised learning.

In future work we will aim to improve candidate
selection (including different strategies to select
candidate lists E+, E−). We will also use extra
document information and jointly predict entities
for different mentions in the document. Besides,
we will consider additional knowledge bases (e.g.
YAGO and Wikidata).

Acknowledgments

We would like to thank anonymous reviewers for
their suggestions and comments. The project
was supported by the European Research Council
(ERC StG BroadSem 678254), the Dutch National
Science Foundation (NWO VIDI 639.022.518),
and an Amazon Web Services (AWS) grant.

References
Eric Charton, Marie-Jean Meurs, Ludovic Jean-Louis,

and Michel Gagnon. 2014. Improving entity link-
ing using surface form refinement. In Proceedings
of the Ninth International Conference on Language
Resources and Evaluation (LREC’14).

Andrew Chisholm and Ben Hachey. 2015. Entity dis-
ambiguation with web links. Transactions of the As-
sociation for Computational Linguistics, 3:145–156.

Marco Cornolti, Paolo Ferragina, and Massimiliano
Ciaramita. 2013. A framework for benchmarking
entity-annotation systems. In Proceedings of the
22nd international conference on World Wide Web,
pages 249–260. ACM.

Thomas G. Dietterich, Richard H. Lathrop, and Tomás
Lozano-Pérez. 1997. Solving the multiple instance
problem with axis-parallel rectangles. Artif. Intell.,
89(1-2):31–71.

Miao Fan, Qiang Zhou, and Thomas Fang Zheng. 2015.
Distant supervision for entity linking. In Proceed-
ings of the 29th Pacific Asia Conference on Lan-
guage, Information and Computation, pages 79–86.

Michael Färber and Achim Rettinger. 2018. Which
knowledge graph is best for me? CoRR,
abs/1809.11099.

Kuzman Ganchev, Jennifer Gillenwater, Ben Taskar,
et al. 2010. Posterior regularization for structured
latent variable models. Journal of Machine Learn-
ing Research, 11(Jul):2001–2049.

Octavian-Eugen Ganea and Thomas Hofmann. 2017.
Deep joint entity disambiguation with local neural
attention. In Proceedings of the 2017 Conference on
Empirical Methods in Natural Language Process-
ing, pages 2619–2629, Copenhagen, Denmark. As-
sociation for Computational Linguistics.

Amir Globerson, Nevena Lazic, Soumen Chakrabarti,
Amarnag Subramanya, Michael Ringaard, and Fer-
nando Pereira. 2016. Collective entity resolution
with multi-focal attention. In Proceedings of the
54th Annual Meeting of the Association for Compu-
tational Linguistics (Volume 1: Long Papers), pages
621–631, Berlin, Germany. Association for Compu-
tational Linguistics.

Johannes Hoffart, Mohamed Amir Yosef, Ilaria Bor-
dino, Hagen Fürstenau, Manfred Pinkal, Marc Span-
iol, Bilyana Taneva, Stefan Thater, and Gerhard
Weikum. 2011. Robust disambiguation of named
entities in text. In Proceedings of the 2011 Con-
ference on Empirical Methods in Natural Language
Processing, pages 782–792, Edinburgh, Scotland,
UK. Association for Computational Linguistics.

Nevena Lazic, Amarnag Subramanya, Michael Ring-
gaard, and Fernando Pereira. 2015. Plato: A selec-
tive context model for entity resolution. Transac-
tions of the Association for Computational Linguis-
tics, 3:503–515.

Phong Le and Ivan Titov. 2018. Improving entity link-
ing by modeling latent relations between mentions.
In Proceedings of the 56th Annual Meeting of the
Association for Computational Linguistics (Volume
1: Long Papers), pages 1595–1604. Association for
Computational Linguistics.

Gideon S Mann and Andrew McCallum. 2010. Gener-
alized expectation criteria for semi-supervised learn-
ing with weakly labeled data. Journal of machine
learning research, 11(Feb):955–984.

Christopher Manning, Mihai Surdeanu, John Bauer,
Jenny Finkel, Steven Bethard, and David McClosky.
2014. The stanford corenlp natural language pro-
cessing toolkit. In Proceedings of 52nd Annual
Meeting of the Association for Computational Lin-
guistics: System Demonstrations, pages 55–60. As-
sociation for Computational Linguistics.

Mike Mintz, Steven Bills, Rion Snow, and Daniel Ju-
rafsky. 2009. Distant supervision for relation ex-
traction without labeled data. In Proceedings of
the Joint Conference of the 47th Annual Meeting of
the ACL and the 4th International Joint Conference

https://tacl2013.cs.columbia.edu/ojs/index.php/tacl/article/view/494
https://tacl2013.cs.columbia.edu/ojs/index.php/tacl/article/view/494
https://doi.org/10.1016/S0004-3702(96)00034-3
https://doi.org/10.1016/S0004-3702(96)00034-3
http://aclweb.org/anthology/Y15-1010
http://arxiv.org/abs/1809.11099
http://arxiv.org/abs/1809.11099
https://www.aclweb.org/anthology/D17-1277
https://www.aclweb.org/anthology/D17-1277
http://www.aclweb.org/anthology/P16-1059
http://www.aclweb.org/anthology/P16-1059
http://www.aclweb.org/anthology/D11-1072
http://www.aclweb.org/anthology/D11-1072
https://tacl2013.cs.columbia.edu/ojs/index.php/tacl/article/view/637
https://tacl2013.cs.columbia.edu/ojs/index.php/tacl/article/view/637
http://aclweb.org/anthology/P18-1148
http://aclweb.org/anthology/P18-1148
https://doi.org/10.3115/v1/P14-5010
https://doi.org/10.3115/v1/P14-5010
http://www.aclweb.org/anthology/P/P09/P09-1113
http://www.aclweb.org/anthology/P/P09/P09-1113


4090

on Natural Language Processing of the AFNLP,
pages 1003–1011, Suntec, Singapore. Association
for Computational Linguistics.

Jeffrey Pennington, Richard Socher, and Christopher
Manning. 2014. Glove: Global vectors for word
representation. In Proceedings of the 2014 Con-
ference on Empirical Methods in Natural Language
Processing (EMNLP), pages 1532–1543, Doha,
Qatar. Association for Computational Linguistics.

Lev Ratinov, Dan Roth, Doug Downey, and Mike An-
derson. 2011. Local and global algorithms for dis-
ambiguation to wikipedia. In Proceedings of the
49th Annual Meeting of the Association for Com-
putational Linguistics: Human Language Technolo-
gies, pages 1375–1384. Association for Computa-
tional Linguistics.

Sebastian Riedel, Limin Yao, and Andrew McCallum.
2010. Modeling relations and their mentions with-
out labeled text. In Joint European Conference
on Machine Learning and Knowledge Discovery in
Databases, pages 148–163. Springer.

Michael Schlichtkrull, Thomas N Kipf, Peter Bloem,
Rianne Van Den Berg, Ivan Titov, and Max Welling.
2018. Modeling relational data with graph convolu-
tional networks. In European Semantic Web Confer-
ence, pages 593–607. Springer.

Erik F. Tjong Kim Sang and Fien De Meulder.
2003. Introduction to the conll-2003 shared task:
Language-independent named entity recognition. In
Proceedings of the Seventh Conference on Natu-
ral Language Learning at HLT-NAACL 2003, pages
142–147.

Ricardo Usbeck, Axel-Cyrille Ngonga Ngomo,
Michael Röder, Daniel Gerber, Sandro Athaide
Coelho, Sören Auer, and Andreas Both. 2014.
Agdistis-graph-based disambiguation of named
entities using linked data. In International semantic
web conference, pages 457–471. Springer.

Ikuya Yamada, Hiroyuki Shindo, Hideaki Takeda, and
Yoshiyasu Takefuji. 2017. Learning distributed rep-
resentations of texts and entities from knowledge
base. Transactions of the Association for Compu-
tational Linguistics, 5:397–411.

Wen-tau Yih, Ming-Wei Chang, Xiaodong He, and
Jianfeng Gao. 2015. Semantic parsing via staged
query graph generation: Question answering with
knowledge base. In Proceedings of the 53rd Annual
Meeting of the Association for Computational Lin-
guistics and the 7th International Joint Conference
on Natural Language Processing (Volume 1: Long
Papers), pages 1321–1331, Beijing, China. Associ-
ation for Computational Linguistics.

A Hyper-parameters

The used values for the hyper-parameters are
shown in Table 5.

Hyper-parameters Model Value
learning rate (Adam) 1, 2 0.001
mini-batch size 1, 2 50
number of epochs 1, 2 20
dw (word emb. dim.) 1, 2 300
dp (position emb. dim.) 1, 2 5
dt (type emb. dim.) 1, 2 50
de (entity emb. dim.) 1, 2 100
BiLSTM hidden dim. 1, 2 100
FFNg hidden dim. 1, 2 300
FFNh hidden dim. 2 300
δ (margin) 1, 2 0.1
T (temperature) 1, 2 1/3
η (KL coefficient) 2 5
p∗I(1) (prior) 2 0.9
τ (threshold) 2 0.75

Table 5: Values of hyper-parameters.

System Micro accuracy
Ours 81.47 ±1.27
Hoffart et al. (2011) 81.91

Table 6: Micro accuracy on AIDA CoNLL testb of our
supervised system and (Hoffart et al., 2011).

B Supervised learning system

Our supervised learning system is a supervised
version of model 1. To examine how good this sys-
tem is, we tested it on the AIDA CoNLL dataset.
Because this system uses entity types for entity
disambiguation (and uses no other information re-
lated to entities), we made use of the map between
Freebase entities and Wikipedia entities. We com-
pared it with Hoffart et al. (2011). Note that we
did not tune the system: it used the same values of
hyper-parameters with model 1 (see Table5).

Because Wikipedia and YAGO are often used
for candidate selection and/or for additional su-
pervision, we here also used them for candidate
selection and for computing p(e|m) as a feature as
in most existing systems (such as in Hoffart et al.
(2011); Globerson et al. (2016); Ganea and Hof-
mann (2017)). For the candidate section, for each
mention we kept maximally 20 candidates.

We ran our system five times and report mean
and 95% confidence interval. Table 6 shows micro
accuracy (in knowledge-base). Our system per-
forms on par with Hoffart et al. (2011).

http://www.aclweb.org/anthology/D14-1162
http://www.aclweb.org/anthology/D14-1162
http://aclweb.org/anthology/P11-1138
http://aclweb.org/anthology/P11-1138
http://www.aclweb.org/anthology/W03-0419.pdf
http://www.aclweb.org/anthology/W03-0419.pdf
https://transacl.org/ojs/index.php/tacl/article/view/1065
https://transacl.org/ojs/index.php/tacl/article/view/1065
https://transacl.org/ojs/index.php/tacl/article/view/1065
http://www.aclweb.org/anthology/P15-1128
http://www.aclweb.org/anthology/P15-1128
http://www.aclweb.org/anthology/P15-1128

