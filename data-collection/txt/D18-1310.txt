



















































Evaluating the Utility of Hand-crafted Features in Sequence Labelling


Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 2850–2856
Brussels, Belgium, October 31 - November 4, 2018. c©2018 Association for Computational Linguistics

2850

Evaluating the Utility of Hand-crafted Features in Sequence Labelling∗

Minghao Wu♠♥† Fei Liu♠ Trevor Cohn♠
♠The University of Melbourne, Victoria, Australia

♥JD AI Research, Beijing, China
wuminghao@jd.com

fliu3@student.unimelb.edu.au
t.cohn@unimelb.edu.au

Abstract

Conventional wisdom is that hand-crafted fea-
tures are redundant for deep learning mod-
els, as they already learn adequate represen-
tations of text automatically from corpora. In
this work, we test this claim by proposing a
new method for exploiting handcrafted fea-
tures as part of a novel hybrid learning ap-
proach, incorporating a feature auto-encoder
loss component. We evaluate on the task of
named entity recognition (NER), where we
show that including manual features for part-
of-speech, word shapes and gazetteers can im-
prove the performance of a neural CRF model.
We obtain a F1 of 91.89 for the CoNLL-2003
English shared task, which significantly out-
performs a collection of highly competitive
baseline models. We also present an abla-
tion study showing the importance of auto-
encoding, over using features as either inputs
or outputs alone, and moreover, show includ-
ing the autoencoder components reduces train-
ing requirements to 60%, while retaining the
same predictive accuracy.

1 Introduction

Deep neural networks have been proven to be a
powerful framework for natural language process-
ing, and have demonstrated strong performance on
a number of challenging tasks, ranging from ma-
chine translation (Cho et al., 2014b,a), to text cat-
egorisation (Zhang et al., 2015; Joulin et al., 2017;
Liu et al., 2018b). Not only do such deep models
outperform traditional machine learning methods,
they also come with the benefit of not requiring
difficult feature engineering. For instance, both
Lample et al. (2016) and Ma and Hovy (2016)
propose end-to-end models for sequence labelling
task and achieve state-of-the-art results.

∗https://github.com/minghao-wu/CRF-AE
†Work carried out at The University of Melbourne

Orthogonal to the advances in deep learning is
the effort spent on feature engineering. A rep-
resentative example is the task of named entity
recognition (NER), one that requires both lexi-
cal and syntactic knowledge, where, until recently,
most models heavily rely on statistical sequential
labelling models taking in manually engineered
features (Florian et al., 2003; Chieu and Ng, 2002;
Ando and Zhang, 2005). Typical features include
POS and chunk tags, prefixes and suffixes, and ex-
ternal gazetteers, all of which represent years of
accumulated knowledge in the field of computa-
tional linguistics.

The work of Collobert et al. (2011) started
the trend of feature engineering-free modelling
by learning internal representations of compo-
sitional components of text (e.g., word embed-
dings). Subsequent work has shown impressive
progress through capturing syntactic and semantic
knowledge with dense real-valued vectors trained
on large unannotated corpora (Mikolov et al.,
2013a,b; Pennington et al., 2014). Enabled by
the powerful representational capacity of such em-
beddings and neural networks, feature engineering
has largely been replaced with taking off-the-shelf
pre-trained word embeddings as input, thereby
making models fully end-to-end and the research
focus has shifted to neural network architecture
engineering.

More recently, there has been increasing recog-
nition of the utility of linguistic features (Li et al.,
2017; Chen et al., 2017; Wu et al., 2017; Liu et al.,
2018a) where such features are integrated to im-
prove model performance. Inspired by this, tak-
ing NER as a case study, we investigate the util-
ity of hand-crafted features in deep learning mod-
els, challenging conventional wisdom in an at-
tempt to refute the utility of manually-engineered
features. Of particular interest to this paper is
the work by Ma and Hovy (2016) where they

https://github.com/minghao-wu/CRF-AE


2851

EU rejects German call to ...Input Sentence 

Character
Representations Word Embeddings Hand-crafted Features

Feature
Concatenation 

Bi-directional LSTMFeatureEncoder

NER Auto-EncoderOutput Layer 

Figure 1: Main architecture of our neural network.
Character representations are extracted by a character-
level CNN. The dash line indicates we use an auto-
encoder loss to reconstruct hand-crafted features.

introduce a strong end-to-end model combining
a bi-directional Long Short-Term Memory (Bi-
LSTM) network with Convolutional Neural Net-
work (CNN) character encoding in a Conditional
Random Field (CRF). Their model is highly capa-
ble of capturing not only word- but also character-
level features. We extend this model by integrating
an auto-encoder loss, allowing the model to take
hand-crafted features as input and re-construct
them as output, and show that, even with such a
highly competitive model, incorporating linguistic
features is still beneficial. Perhaps the closest to
this study is the works by Ammar et al. (2014) and
Zhang et al. (2017), who show how CRFs can be
framed as auto-encoders in unsupervised or semi-
supervised settings.

With our proposed model, we achieve strong
performance on the CoNLL 2003 English NER
shared task with an F1 of 91.89, significantly out-
performing an array of competitive baselines. We
conduct an ablation study to better understand the
impacts of each manually-crafted feature. Finally,
we further provide an in-depth analysis of model
performance when trained with varying amount of
data and show that the proposed model is highly
competent with only 60% of the training set.

2 Methodology

In this section, we first outline the model archi-
tecture, then the manually crafted features, and fi-
nally how they are incorporated into the model.

2.1 Model Architecture

We build on a highly competitive sequence la-
belling model, namely Bi-LSTM-CNN-CRF, first

introduced by Ma and Hovy (2016). Given an in-
put sequence of x = {x1, x2, . . . , xT } of length
T , the model is capable of tagging each input with
a predicted label ŷ, resulting in a sequence of ŷ =
{ŷ1, ŷ2, . . . , ŷT } closely matching the gold label
sequence y = {y1, y2, . . . , yT }. Here, we extend
the model by incorporating an auto-encoder loss
taking hand-crafted features as in/output, thereby
forcing the model to preserve crucial information
stored in such features and allowing us to eval-
uate the impacts of each feature on model per-
formance. Specifically, our model, referred to
as Neural-CRF+AE, consists of four major com-
ponents: (1) a character-level CNN (char-CNN);
(2) a word-level bi-directional LSTM (Bi-LSTM);
(3) a conditional random field (CRF); and (4) an
auto-encoder auxiliary loss. An illustration of the
model architecture is presented in Figure 1.

Char-CNN. Previous studies (Santos and
Zadrozny, 2014; Chiu and Nichols, 2016; Ma
and Hovy, 2016) have demonstrated that CNNs
are highly capable of capturing character-level
features. Here, our character-level CNN is similar
to that used in Ma and Hovy (2016) but differs in
that we use a ReLU activation (Nair and Hinton,
2010).1

Bi-LSTM. We use a Bi-LSTM to learn contex-
tual information of a sequence of words. As in-
puts to the Bi-LSTM, we first concatenate the
pre-trained embedding of each word wi with its
character-level representation cwi (the output of
the char-CNN) and a vector of manually crafted
features fi (described in Section 2.2):

−→
h i =

−−−−→
LSTM(

−→
h i−1, [wi; cwi ;fi]) (1)

←−
h i =

←−−−−
LSTM(

←−
h i+1, [wi; cwi ;fi]) , (2)

where [; ] denotes concatenation. The outputs of
the forward and backward pass of the Bi-LSTM
is then concatenated hi = [

−→
h i;
←−
h i] to form the

output of the Bi-LSTM, where dropout is also ap-
plied.

CRF. For sequence labelling tasks, it is intuitive
and beneficial to utilise information carried be-
tween neighbouring labels to predict the best se-
quence of labels for a given sentence. Therefore,

1While the hyperbolic tangent activation function results
in comparable performance, the choice of ReLU is mainly
due to faster convergence.



2852

x U.N. official Ekeus heads for Baghdad .

POS NNP NN NNP VBZ IN NNP .
Word shape X.X. xxxx Xxxxx xxxx xxx Xxxxx .
Dependency tags compound compound compound ROOT prep pobj punct
Gazetteer O O PER O O LOC O

y B-ORG O B-PER O O B-LOC O

Table 1: Example sentence (top), showing the different types of linguistic features used in this work as additional
inputs and auxiliary outputs (middle), and its labelling (bottom).

we employ a conditional random field layer (Laf-
ferty et al., 2001) taking as input the output of
the Bi-LSTM hi. Training is carried out by max-
imising the log probability of the gold sequence:
LCRF = log p(y|x) while decoding can be effi-
ciently performed with the Viterbi algorithm.

Auto-encoder loss. Alongside sequence la-
belling as the primary task, we also deploy, as aux-
iliary tasks, three auto-encoders for reconstruct-
ing the hand-engineered feature vectors. To this
end, we add multiple independent fully-connected
dense layers, all taking as input the Bi-LSTM out-
put hi with each responsible for reconstructing a
particular type of feature: f̂ ti = σ(W

thi) where
σ is the sigmoid activation function, t denotes the
type of feature, and W t is a trainable parameter
matrix. More formally, we define the auto-encoder
loss as:

LtAE =
T∑
i=0

XEntropy(f ti , f̂
t
i ) . (3)

Model training. Training is carried out by opti-
mising the joint loss:

L = LCRF +
∑
t

λtLtAE , (4)

where, in addition to LCRF , we also add the auto-
encoder loss, weighted by λt. In all our experi-
ments, we set λt to 1 for all ts.

2.2 Hand-crafted Features
We consider three categories of widely used fea-
tures: (1) POS tags; (2) word shape; and (3)
gazetteers and present an example in Table 1.
While POS tags carry syntactic information re-
garding sentence structure, the word shape feature
focuses on a more fine-grained level, encoding
character-level knowledge to complement the loss
of information caused by embedding lookup, such
as capitalisation. Both features are based on the
implementation of spaCy.2 For the gazetteer fea-

2https://spacy.io/

ture, we focus on PERSON and LOCATION and
compile a list for each. The PERSON gazetteer
is collected from U.S. census 2000, U.S. cen-
sus 2010 and DBpedia whereas GeoNames is the
main source for LOCATION, taking in both of-
ficial and alternative names. All the tokens on
both lists are then filtered to exclude frequently
occurring common words.3 Each category is con-
verted into a one-hot sparse feature vector f ti and
then concatenated to form a multi-hot vector fi =
[fPOSi ;f

shape
i ;f

gazetteer
i ] for the i-th word. In addi-

tion, we also experimented with including the la-
bel of the incoming dependency edge to each word
as a feature, but observed performance deteriora-
tion on the development set. While we still study
and analyse the impacts of this feature in Table 3
and Section 3.2, it is excluded from our model
configuration (not considered as part of fi unless
indicated otherwise).

3 Experiments

In this section, we present our experimental setup
and results for name entity recognition over the
CoNLL 2003 English NER shared task dataset
(Tjong Kim Sang and De Meulder, 2003).

3.1 Experimental Setup

Dataset. We use the CoNLL 2003 NER shared
task dataset, consisting of 14,041/3,250/3,453 sen-
tences in the training/development/test set respec-
tively, all extracted from Reuters news articles dur-
ing the period from 1996 to 1997. The dataset
is annotated with four categories of name en-
tities: PERSON, LOCATION, ORGANIZATION
and MISC. We use the IOBES tagging scheme, as
previous study have shown that this scheme pro-
vides a modest improvement to the model per-
formance (Ratinov and Roth, 2009; Chiu and
Nichols, 2016; Lample et al., 2016; Ma and Hovy,
2016).

3Gazetteer data is included in the code release.

https://spacy.io/


2853

Model configuration. Following the work of
Ma and Hovy (2016), we initialise word embed-
dings with GloVe (Pennington et al., 2014) (300-
dimensional, trained on a 6B-token corpus). Char-
acter embeddings are 30-dimensional and ran-
domly initialised with a uniform distribution in

the range [−
√

3
dim ,+

√
3

dim ]. Parameters are opti-
mised with stochastic gradient descent (SGD) with
an initial learning rate of η = 0.015 and momen-
tum of 0.9. Exponential learning rate decay is ap-
plied every 5 epochs with a factor of 0.8. To re-
duce the impact of exploding gradients, we em-
ploy gradient clipping at 5.0 (Pascanu et al., 2013).

We train our models on a single GeForce GTX
TITAN X GPU. With the above hyper-parameter
setting, training takes approximately 8 hours for a
full run of 40 epochs.

Evaluation. We measure model performance
with the official CoNLL evaluation script and re-
port span-level named entity F-score on the test set
using early stopping based on the performance on
the validation set. We report average F-scores and
standard deviation over 5 runs for our model.

Baseline. In addition to reporting a number of
prior results of competitive baseline models, as
listed in Table 2, we also re-implement the Bi-
LSTM-CNN-CRF model by Ma and Hovy (2016)
(referred to as Neural-CRF in Table 2) and report
its average performance.

3.2 Results
The experimental results are presented in Table 2.
Observe that Neural-CRF+AE, trained either on
the training set only or with the addition of the de-
velopment set, achieves substantial improvements
in F-score in both settings, superior to all but one
of the benchmark models, highlighting the utility
of hand-crafted features incorporated with the pro-
posed auto-encoder loss. Compared against the
Neural-CRF, a very strong model in itself, our
model significantly improves performance, show-
ing the positive impact of our technique for ex-
ploiting manually-engineered features. Although
Peters et al. (2018) report a higher F-score using
their ELMo embedding technique, our approach
here is orthogonal, and accordingly we would ex-
pect a performance increase if we were to incor-
porate their ELMo representations into our model.

Ablation Study To gain a better understanding
of the impacts of each feature, we perform an ab-

Model F1

Chieu and Ng (2002) 88.31
Florian et al. (2003) 88.76
Ando and Zhang (2005) 89.31
Collobert et al. (2011) 89.59
Huang et al. (2015) 90.10
Passos et al. (2014) 90.90
Lample et al. (2016) 90.94
Luo et al. (2015) 91.20
Ma and Hovy (2016) 91.21
Yang et al. (2017) 91.62
Peters et al. (2018) 90.15
Peters et al. (2018)+ELMo 92.22 (± 0.10)
Neural-CRF‡ 91.06 (± 0.18)
Neural-CRF+AE‡∗ 91.89 (± 0.23)

Ratinov and Roth (2009)† 90.80
Chiu and Nichols (2016)† 91.62
Neural-CRF+AE† ‡ 92.29 (± 0.20)

Table 2: NER Performance on the CoNLL 2003 En-
glish NER shared task test set. Bold highlights best
performance. †marks models trained on both the train-
ing and development sets. ‡ indicates average perfor-
mance over 5 runs. ∗ indicates statistical significance
on the test set against Neural-CRF by two-sample Stu-
dent’s t-test at level α = 0.05.

lation study and present the results in Table 3.
We observe performance degradation when elim-
inating POS, word shape and gazetteer features,
showing that each feature contributes to NER per-
formance beyond what is learned through deep
learning alone. Interestingly, the contribution of
gazetteers is much less than that of the other fea-
tures, which is likely due to the noise introduced in
the matching process, with many incorrectly iden-
tified false positives.

Including features based on dependency tags
into our model decreases the performance slightly.
This might be a result of our simple implemen-
tation (as illustrated in Table 1), which does not
include dependency direction, nor parent-child re-
lationships.

Next, we investigate the impact of different
means of incorporating manually-engineered fea-
tures into the model. To this end, we experi-
ment with three configurations with features as:
(1) input only; (2) output only (equivalent to
multi-task learning); and (3) both input and out-
put (Neural-CRF+AE) and present the results in
Table 4. Simply using features as either input or
output only improves model performance slightly,
but insignificantly so. It is only when features are
incorporated with the proposed auto-encoder loss
do we observe a significant performance boost.



2854

Model Dev F1 Test F1

Neural-CRF+AE 94.87 (± 0.21) 91.89 (± 0.23)
− POS tagging∗ 94.78 (± 0.17) 91.30 (± 0.28)
− word shape∗ 94.83 (± 0.31) 91.36 (± 0.30)
− gazetteer 94.85 (± 0.20) 91.80 (± 0.19)
+ dependencies 94.74 (± 0.16) 91.66 (± 0.18)

Table 3: Ablation study. Average performance over 5
runs with standard deviation. + and − denote adding
and removing a particular feature (to/from Neural-
CRF+AE trained on the training set only with POS tag-
ging, word shape and gazetteer features). ∗ indicates
statistical significance on the test set against Neural-
CRF+AE by two-sample Student’s t-test at level α =
0.05. Note that in this table, ∗ measures the drop in
performance.

Model Dev F1 Test F1

Neural-CRF 94.53 (± 0.21) 91.06 (± 0.18)
+ input 94.63 (± 0.23) 91.17 (± 0.25)
+ output 94.69 (± 0.22) 91.23 (± 0.19)
+ input & output∗ 94.87 (± 0.21) 91.89 (± 0.23)

Table 4: Average performance of Neural-CRF with dif-
ferent features configurations over 5 runs with stan-
dard deviation. Note that + input & output = Neural-
CRF+AE. ∗ indicates statistical significance on the test
set against Neural-CRF by two-sample Student’s t-test
at level α = 0.05.

Training Requirements Neural systems typi-
cally require a large amount of annotated data.
Here we measure the impact of training with vary-
ing amount of annotated data, as shown in Fig-
ure 2. Wtih the proposed model architecture, the
amount of labelled training data can be drastically
reduced: our model, achieves comparable perfor-
mance against the baseline Neural-CRF, with as
little as 60% of the training data. Moreover, as
we increase the amount of training text, the perfor-
mance of Neural-CRF+AE continues to improve.

Hyperparameters Three extra hyperparameters
are introduced into our model, controlling the
weight of the autoencoder loss relative to the CRF
loss, for each feature type. Figure 3 shows the ef-
fect of each hyperparameter on test performance.
Observe that setting λi = 1 gives strong perfor-
mance, and that the impact of the gazetteer is less
marked than the other two feature types. While in-
creasing λ is mostly beneficial, performance drops
if the λs are overly large, that is, the auto-encoder
loss overwhelms the main prediction task.

20 40 60 80 100
84

86

88

90

92

Fraction of training data (%)

F
1

sc
or

e

Neural-CRF+AE
Baseline

Figure 2: Comparing the Neural-CRF+AE (red solid
line) trained with varying amounts of data vs. a Neural-
CRF baseline (blue dashed line), trained on the full
training set. Performance averaged over 5 runs, and
error bars show ± 1 std.dev.

10−8 10−6 10−4 10−2 100

91.4

91.6

91.8

92

λi

F
1

sc
or

e

POS tagging
Word Shape

Gazetteer

Figure 3: Effect of hyperparameter values on model
performance. Each curve shows the effect of λi, for
feature type i, with all other λj = 1, j 6= i. Perfor-
mance averaged over 5 runs, and error bars show ± 1
variance.

4 Conclusion

In this paper, we set out to investigate the utility
of hand-crafted features. To this end, we have
presented a hybrid neural architecture to validate
this hypothesis extending a Bi-LSTM-CNN-CRF
by incorporating an auto-encoder loss to take man-
ual features as input and then reconstruct them. On
the task of named entity recognition, we show sig-
nificant improvements over a collection of com-
petitive baselines, verifying the value of such fea-
tures. Lastly, the method presented in this work
can also be easily applied to other tasks and mod-
els, where hand-engineered features provide key
insights about the data.

References
Waleed Ammar, Chris Dyer, and Noah A Smith. 2014.

Conditional random field autoencoders for unsuper-
vised structured prediction. In Proceedings of the
27th International Conference on Neural Informa-



2855

tion Processing Systems (NIPS 2014), pages 3311–
3319.

Rie Kubota Ando and Tong Zhang. 2005. A framework
for learning predictive structures from multiple tasks
and unlabeled data. Journal of Machine Learning
Research, 6(Nov):1817–1853.

Huadong Chen, Shujian Huang, David Chiang, and Ji-
ajun Chen. 2017. Improved neural machine trans-
lation with a syntax-aware encoder and decoder. In
Proceedings of the 55th Annual Meeting of the Asso-
ciation for Computational Linguistics (ACL 2017),
pages 1936–1945.

Hai Leong Chieu and Hwee Tou Ng. 2002. Named
entity recognition: A maximum entropy approach
using global information. In Proceedings of the
19th International Conference on Computational
Linguistics (COLING 2002), pages 1–7.

Jason PC Chiu and Eric Nichols. 2016. Named entity
recognition with bidirectional lstm-cnns. Transac-
tions of the Association for Computational Linguis-
tics, 4:357–370.

Kyunghyun Cho, Bart van Merrienboer, Dzmitry Bah-
danau, and Yoshua Bengio. 2014a. On the proper-
ties of neural machine translation: Encoder–decoder
approaches. In Proceedings of SSST-8, Eighth Work-
shop on Syntax, Semantics and Structure in Statisti-
cal Translation (SSST-8 2014), pages 103–111.

Kyunghyun Cho, Bart van Merrienboer, Caglar Gul-
cehre, Dzmitry Bahdanau, Fethi Bougares, Holger
Schwenk, and Yoshua Bengio. 2014b. Learning
phrase representations using RNN encoder–decoder
for statistical machine translation. In Proceedings of
the 2014 Conference on Empirical Methods in Nat-
ural Language Processing (EMNLP 2014), pages
1724–1734.

Ronan Collobert, Jason Weston, Léon Bottou, Michael
Karlen, Koray Kavukcuoglu, and Pavel Kuksa.
2011. Natural language processing (almost) from
scratch. Journal of Machine Learning Research,
12(Aug):2493–2537.

Radu Florian, Abe Ittycheriah, Hongyan Jing, and
Tong Zhang. 2003. Named entity recognition
through classifier combination. In Proceedings
of the Seventh Conference of the North American
Chapter of the Association for Computational Lin-
guistics: Human Language Technologies (NAACL-
HLT 2003), pages 168–171.

Zhiheng Huang, Wei Xu, and Kai Yu. 2015. Bidirec-
tional lstm-crf models for sequence tagging. arXiv
preprint arXiv:1508.01991.

Armand Joulin, Edouard Grave, Piotr Bojanowski, and
Tomas Mikolov. 2017. Bag of tricks for efficient
text classification. In Proceedings of the 15th Con-
ference of the European Chapter of the Association
for Computational Linguistics (EACL 2017), pages
427–431.

John Lafferty, Andrew McCallum, and Fernando CN
Pereira. 2001. Conditional random fields: Prob-
abilistic models for segmenting and labeling se-
quence data. In Proceedings of the 18th Inter-
national Conference on Machine Learning (ICML
2001), pages 282–289.

Guillaume Lample, Miguel Ballesteros, Sandeep Sub-
ramanian, Kazuya Kawakami, and Chris Dyer. 2016.
Neural architectures for named entity recognition.
In Proceedings of the 2016 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics: Human Language Technologies
(NAACL-HLT 2016), pages 260–270.

Junhui Li, Deyi Xiong, Zhaopeng Tu, Muhua Zhu, Min
Zhang, and Guodong Zhou. 2017. Modeling source
syntax for neural machine translation. In Proceed-
ings of the 55th Annual Meeting of the Association
for Computational Linguistics (ACL 2017), pages
688–697.

Fei Liu, Trevor Cohn, and Timothy Baldwin. 2018a.
Narrative modeling with memory chains and seman-
tic supervision. In Proceedings of the 56th Annual
Meeting of the Association for Computational Lin-
guistics (ACL 2018), pages 278–284.

Fei Liu, Trevor Cohn, and Timothy Baldwin. 2018b.
Recurrent entity networks with delayed memory up-
date for targeted aspect-based sentiment analysis. In
Proceedings of the 2018 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics: Human Language Technologies
(NAACL HLT 2018), pages 278–283.

Gang Luo, Xiaojiang Huang, Chin-Yew Lin, and Za-
iqing Nie. 2015. Joint entity recognition and disam-
biguation. In Proceedings of the 2015 Conference
on Empirical Methods in Natural Language Pro-
cessing (EMNLP 2015), pages 879–888.

Xuezhe Ma and Eduard Hovy. 2016. End-to-end se-
quence labeling via bi-directional lstm-cnns-crf. In
Proceedings of the 54th Annual Meeting of the Asso-
ciation for Computational Linguistics (ACL 2016),
pages 1064–1074.

Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey
Dean. 2013a. Efficient estimation of word represen-
tations in vector space. In Proceedings of the 1st
International Conference on Learning Representa-
tions (ICLR 2013).

Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg Cor-
rado, and Jeffrey Dean. 2013b. Distributed repre-
sentations of words and phrases and their composi-
tionality. In Proceedings of the 26th International
Conference on Neural Information Processing Sys-
tems (NIPS 2013), pages 3111–3119.

Vinod Nair and Geoffrey E Hinton. 2010. Rectified
linear units improve restricted boltzmann machines.
In Proceedings of the 27th international conference
on machine learning (ICML 2010), pages 807–814.



2856

Razvan Pascanu, Tomas Mikolov, and Yoshua Ben-
gio. 2013. On the difficulty of training recurrent
neural networks. In Proceedings of the 30th Inter-
national Conference on Machine Learning (ICML
2013), pages 1310–1318.

Alexandre Passos, Vineet Kumar, and Andrew McCal-
lum. 2014. Lexicon infused phrase embeddings for
named entity resolution. In Proceedings of the Eigh-
teenth Conference on Computational Natural Lan-
guage Learning (CoNLL 2014), pages 78–86.

Jeffrey Pennington, Richard Socher, and Christopher
Manning. 2014. GloVe: Global vectors for word
representation. In Proceedings of the 2014 Con-
ference on Empirical Methods in Natural Language
Processing (EMNLP 2014), pages 1532–1543.

Matthew Peters, Mark Neumann, Mohit Iyyer, Matt
Gardner, Christopher Clark, Kenton Lee, and Luke
Zettlemoyer. 2018. Deep contextualized word rep-
resentations. In Proceedings of the 2018 Confer-
ence of the North American Chapter of the As-
sociation for Computational Linguistics: Human
Language Technologies (NAACL HLT 2018), pages
2227–2237.

Lev Ratinov and Dan Roth. 2009. Design challenges
and misconceptions in named entity recognition.
In Proceedings of the Thirteenth Conference on
Computational Natural Language Learning (CoNLL
2009), pages 147–155.

Cicero D Santos and Bianca Zadrozny. 2014. Learning
character-level representations for part-of-speech
tagging. In Proceedings of the 31st International
Conference on Machine Learning (ICML 2014),
pages 1818–1826.

Erik F. Tjong Kim Sang and Fien De Meulder.
2003. Introduction to the conll-2003 shared task:
Language-independent named entity recognition. In
Proceedings of the Seventh Conference of the North
American Chapter of the Association for Computa-
tional Linguistics: Human Language Technologies
(NAACL-HLT 2003), pages 142–147.

Shuangzhi Wu, Dongdong Zhang, Nan Yang, Mu Li,
and Ming Zhou. 2017. Sequence-to-dependency
neural machine translation. In Proceedings of the
55th Annual Meeting of the Association for Compu-
tational Linguistics (ACL 2017), pages 698–707.

Jie Yang, Yue Zhang, and Fei Dong. 2017. Neural
reranking for named entity recognition. In Pro-
ceedings of the International Conference Recent Ad-
vances in Natural Language Processing (RANLP
2017), pages 784–792.

Xiang Zhang, Junbo Zhao, and Yann LeCun. 2015.
Character-level convolutional networks for text clas-
sification. In Proceedings of the 28th International
Conference on Neural Information Processing Sys-
tems (NIPS 2015), pages 649–657.

Xiao Zhang, Yong Jiang, Hao Peng, Kewei Tu, and
Dan Goldwasser. 2017. Semi-supervised structured
prediction with neural crf autoencoder. In Proceed-
ings of the 2017 Conference on Empirical Methods
in Natural Language Processing (EMNLP 2017),
pages 1701–1711.


