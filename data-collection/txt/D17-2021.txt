



















































Interactive Visualization and Manipulation of Attention-based Neural Machine Translation


Proceedings of the 2017 EMNLP System Demonstrations, pages 121–126
Copenhagen, Denmark, September 7–11, 2017. c©2017 Association for Computational Linguistics

Interactive Visualization and Manipulation of
Attention-based Neural Machine Translation

Jaesong Lee and Joong-Hwi Shin and Jun-Seok Kim
NAVER Corp.

{jaesong.lee,joonghwi.shin,jun.seok}@navercorp.com

Abstract

While neural machine translation (NMT)
provides high-quality translation, it is still
hard to interpret and analyze its behav-
ior. We present an interactive interface for
visualizing and intervening behavior of
NMT, specifically concentrating on the be-
havior of beam search mechanism and at-
tention component. The tool (1) visualizes
search tree and attention and (2) provides
interface to adjust search tree and atten-
tion weight (manually or automatically) at
real-time. We show the tool help users un-
derstand NMT in various ways.

1 Introduction

Recent advances in neural machine translation
(NMT) (Sutskever et al., 2014) have changed the
direction of machine translation community. Com-
pared to traditional phrase-based statistical ma-
chine translation (SMT) (Koehn, 2010), NMT pro-
vides more accurate and fluent translation results.
Companies also have started to adopt NMT for
their machine translation service.

However, it is still challenging to analyze trans-
lation behavior of NMT. While SMT provides in-
terpretable features (like phrase table), NMT di-
rectly learns complex features which are obscure
to human. This is especially problematic in the
case of wrong translation, since it is even hard to
understand why the system generated such sen-
tences.

To help the analysis, we propose a tool for vi-
sualizing and intervening NMT behavior, concen-
trated on beam search decoder and attention. The
features can be grouped by two categories:

• Visualizing decoder result, including how
decoder assigns probability to each token

Figure 1: Beam search tree interface. Beam search
result is shown as a tree (section 3.1). By hovering
mouse over node, its corresponding output candi-
dates can be seen. User may click the candidate
to expand node which are discarded during search
(section 3.3).

(word, sub-word, etc.), how beam search
maintains and discards intermediate hypothe-
ses, and how attention layer assigns attention
weight. This enables detailed observation of
decoder behavior.

• Intervening in decoder behavior, including
manually expanding hypothesis discarded
during search and adjusting attention weight.
This helps understanding how the compo-
nents affect translation quality.

We show the mechanism of visualization (Sec-
tion 3.1 and 3.2) and manipulation (Section 3.3
and 3.4) and its usefulness with examples.

2 Related Work

There have been various methods proposed for vi-
sualizing and intervening neural models for NLP.
(Li et al., 2015) provides a concise literature re-
view.

Visualization and manipulation of NMT could
be grouped into three parts: RNN (of encoder and
decoder), attention (of decoder), and beam search
(of decoder).

121



Figure 2: Beam search tree of de-en NMT.

Figure 3: Beam search tree of en-ko NMT. Input sentence is: “As a bass player, he is known for his highly
distinctive tone and phrasing.”

RNN plays a central role in recognizing source
sentences and generating target sentences. Al-
though we here treat RNN as a black-box, there
exists various methods to understand RNNs, e.g.
by observing intermediate values (Strobelt et al.,
2016; Karpathy et al., 2015; Li et al., 2015) or by
removing some parts of them (Goh, 2016; Li et al.,
2016).

Attention (Bahdanau et al., 2014; Luong et al.,
2015) is an important component for improving
NMT quality. Since the component behaves like
alignment in traditional SMT, it has been proposed
to utilize attention during training (Cheng et al.,
2015; Tu et al., 2016b) or during decoding (Wu
et al., 2016). In this work, we propose a way to
manipulate attention and to understand the behav-
ior.

Beam search is known to improve quality of
NMT translation output. However, it is also known
that larger beam size does not always helps but
rather hurts the quality (Tu et al., 2016a). There-
fore it is important to understand how beam search
affects quality. (Wu et al., 2016; Freitag and Al-
Onaizan, 2017) proposed several penalty functions
and pruning methods for beam search. We directly
visualize beam search result as a tree and manually
explore hypotheses discarded by decoder.

Figure 4: Diagram of NMT decoder step. Search
tree visualization (Figure 2) shows input and out-
put tokens as a tree.

3 Interactive Beam Search

We propose an interactive tool for visualizing and
manipulating NMT decoder behavior. The sys-
tem consists of two parts: back-end NMT server
and front-end web interface. NMT server is re-
sponsible for NMT computation. Web interface is
responsible for requesting computation to NMT
server and showing results at real time.

For back-end implementation, we use two NMT
models. For English-Korean (en-ko), we use a
model used in Naver Papago (Lee et al., 2016) ser-
vice1 ported to TensorFlow. For German-English
(de-en), we adopted Nematus2 and pretrained
models provided by (Sennrich et al., 2016). For
front-end we implemented JavaScript-based web

1https://papago.naver.com/
2https://github.com/rsennrich/nematus

122



Figure 5: Attention Table. Weight is represented as number and green color.

Figure 6: Attention Graph Dialog. Weight is represented as red color.

page with d3.js3.

3.1 Search Tree Visualization
To understand how beam search decoder selects
and discards intermediate hypothesis, we first plot
all hypotheses as a tree (Figure 2, 3). For each in-
put token (word or sub-word) and decoder (RNN)
state vector, the decoder computes output prob-
ability of all possible output token, then beam
search routine selects token based on its probabil-
ity value (Figure 4). We plot each input and out-
put token as tree node, and input-output relation
as edge. If a node is mouse-hovered, it shows its
next possible tokens with highest probability, in-
cluding pruned ones (Figure 1). We also visualize
output probability of node using edge thickness;
thicker edge means higher probability.

3.2 Attention Visualization
We show the attention weight of (partially) gen-
erated sentence as table (Figure 5) and as graph
(Figure 6). Table interface provides detailed infor-
mation, and graph interface provides more concise
view therefore better for long sentences.

3.3 Search Tree Manipulation
We implemented an interface to manually expand
nodes which are discarded during beam search.
In search tree visualization (Figure 7) or attention
manipulation dialog (Figure 9), a user can click
one of output candidate (green node) then the sys-
tem computes its next outputs and extends the tree.

3https://d3js.org/

Figure 7: Manual expansion of nodes not explored
at Figure 2. For the partial sentence “the Pal@@
li@@ ative Care is”, new subtree “. . . used when
there . . . ” are created.

This enables exploration of hypotheses not cov-
ered by decoder but worth to analyze.

3.4 Attention Manipulation

We are interested in understanding attention layer
of (Bahdanau et al., 2014; Luong et al., 2015), es-
pecially the role and effect of attention weights.
To achieve it, we modified NMT decoder to ac-
cept arbitrary attention weight instead of what the
decoder computes (Figure 8).

3.4.1 Manual Adjustment of Attention
Weight

For given memory cells (encoder outputs)
(m1, · · · , mn) and decoder internal state h, the
attention layer first computes relevance score of
memory cell si = f(mi, h) and attention weight
wi = softmax(s1, · · · , sn)i. Then memory cells

123



Figure 8: Diagram of attention manipulation
mechanism. The dashed components are origi-
nal component of NMT decoder. Here “attention
weight” is replaced by “custom attention weight”
which is given by user or computed to maximize
probability of output token.

Figure 9: Result of attention manipulation for two
output tokens “어조” and “음색”.

are summarized into one fixed vector (m̃) via
weighted sum: m̃ =

∑
i wimi. The summarized

vector is fed to next layer to compute output token
probabilities: p(yj) = g(m̃, h)j .

We modified the decoder to accept custom
weight w′ = (w′1, · · ·w′n) instead of original ones
w, when w′ is provided by user. We also im-
plemented front-end interface to adjust custom
weight (Figure 9). If user drags circle on the bar,
the weights are adjusted and the system computes
new output probabilities using the weight. It helps
to understand what is encoded in memory cell and
how decoder utilizes the attended memory m̃. For
example, user may increase or decrease weight of
specific memory cell and observe its effect.

Figure 9 shows an illustrative example that how
adjusting attention weight could change output
probability distribution. When weight of “highly”
and “tone” are high, NMT puts high probability
to “어조” (“tone of voice”). When weight of “dis-
tinctive” is high, NMT recognizes “tone” in cur-
rent context (musical instrument) and puts high
probability to “음색” (“timbre”).

3.4.2 Automatic Adjustment of Attention
Weight

We also implemented a method to find attention
weight maximizing output probability of a specific
token. For attention weight w and token y, we see
this problem as a constrained optimization: maxi-
mize log p(y|w, · · ·) s.t. wi ≥ 0,

∑
i wi = 1. Since

the toolkits we use (TensorFlow4 and Theano5)
provide unconstrained gradient descent optimizer,
we cast the original problem to unconstrained op-
timization: instead of weight w, we optimize un-
normalized score s before softmax, initialized as
si = log wi. The method can be used to opti-
mize weight for specific time step (Figure 9) or
for whole sentence (Figure 10).

For English-Korean, this technique is particu-
larly useful because the original attention weight
is sometimes hard to interpret. Due to ordering
differences between two languages, en-ko NMTs
tend to generate diverse sentences and they have
very different orderings among each other. In
Figure 3, input sentence is “As a bass player,
he is . . . ”. NMT puts high probability to output
sentences starting with either “베이스” (“bass”)
or “그” (“he”), since both are valid. Therefore,

4https://www.tensorflow.org/
5http://deeplearning.net/software/

theano/

124



Figure 10: Two attention graphs of en-ko NMT. The first one shows attention weights from NMT. The
second one shows attention weights adjusted to maximize target sentence. It reveals clearer and more
interpretable relation than original attention.

corresponding source words have high attention
weights (0.14 for “bass” and 0.07 for “he”). Since
output token is chosen after attention, the atten-
tion weights do not necessarily look like alignment
between source and output sentences, but rather
look like a mixture of alignments of possible out-
put sentences.

Once output token is chosen, we can find new
attention weight which increases probability of
output token, which would be more interpretable
than the original weight. An example of such ad-
justment is shown at Figure 10.

4 Conclusion

We propose a web-based interface for visualiz-
ing, investigating and understanding neural ma-
chine translation (NMT). The tool provides sev-
eral methods to understand beam search and at-
tention mechanism in an interactive way, by visu-
alizing search tree and attention, expanding search
tree manually, and changing attention weight ei-
ther manually or automatically. We show the vi-
sualization and manipulation helps understanding
NMT behavior.

References
Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Ben-

gio. 2014. Neural machine translation by jointly
learning to align and translate. In Proceedings of
the International Conference on Learning Represen-
tations.

Yong Cheng, Shiqi Shen, Zhongjun He, Wei He,
Hua Wu, Maosong Sun, and Yang Liu. 2015.
Agreement-based joint training for bidirectional
attention-based neural machine translation. CoRR,
abs/1512.04650.

Markus Freitag and Yaser Al-Onaizan. 2017. Beam
search strategies for neural machine translation.
CoRR, abs/1702.01806.

Gabriel Goh. 2016. Decoding the thought vector.

Andrej Karpathy, Justin Johnson, and Li Fei-Fei. 2015.
Visualizing and understanding recurrent networks.
arXiv preprint arXiv:1506.02078.

Philipp Koehn. 2010. Statistical Machine Translation,
1st edition. Cambridge University Press, New York,
NY, USA.

Hyoung-Gyu Lee, Jun-Seok Kim, Joong-Hwi Shin,
Jaesong Lee, Ying-Xiu Quan, and Young-Seob
Jeong. 2016. papago: A machine translation service
with word sense disambiguation and currency con-
version. In Proceedings of COLING 2016, the 26th
International Conference on Computational Lin-
guistics: System Demonstrations, pages 185–188,
Osaka, Japan. The COLING 2016 Organizing Com-
mittee.

Jiwei Li, Xinlei Chen, Eduard Hovy, and Dan Jurafsky.
2015. Visualizing and understanding neural models
in nlp. arXiv preprint arXiv:1506.01066.

Jiwei Li, Will Monroe, and Dan Jurafsky. 2016. Un-
derstanding neural networks through representation
erasure. CoRR, abs/1612.08220.

Minh-Thang Luong, Hieu Pham, and Christopher D.
Manning. 2015. Effective approaches to attention-
based neural machine translation. In Proceedings of
the 2015 Conference on Empirical Methods in Nat-
ural Language Processing, pages 1412–1421, Lis-
bon, Portugal. Association for Computational Lin-
guistics.

Rico Sennrich, Barry Haddow, and Alexandra Birch.
2016. Edinburgh neural machine translation sys-
tems for wmt 16. In Proceedings of the First
Conference on Machine Translation, pages 371–
376, Berlin, Germany. Association for Computa-
tional Linguistics.

125



Hendrik Strobelt, Sebastian Gehrmann, Bernd Huber,
Hanspeter Pfister, and Alexander M. Rush. 2016.
Visual analysis of hidden state dynamics in recurrent
neural networks. CoRR, abs/1606.07461.

Ilya Sutskever, Oriol Vinyals, and Quoc V Le. 2014.
Sequence to sequence learning with neural net-
works. In Advances in neural information process-
ing systems, pages 3104–3112.

Zhaopeng Tu, Yang Liu, Lifeng Shang, Xiaohua Liu,
and Hang Li. 2016a. Neural machine translation
with reconstruction. CoRR, abs/1611.01874.

Zhaopeng Tu, Zhengdong Lu, Yang Liu, Xiaohua Liu,
and Hang Li. 2016b. Coverage-based neural ma-
chine translation. CoRR, abs/1601.04811.

Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V.
Le, Mohammad Norouzi, Wolfgang Macherey,
Maxim Krikun, Yuan Cao, Qin Gao, Klaus
Macherey, Jeff Klingner, Apurva Shah, Melvin
Johnson, Xiaobing Liu, Łukasz Kaiser, Stephan
Gouws, Yoshikiyo Kato, Taku Kudo, Hideto
Kazawa, Keith Stevens, George Kurian, Nishant
Patil, Wei Wang, Cliff Young, Jason Smith, Jason
Riesa, Alex Rudnick, Oriol Vinyals, Greg Corrado,
Macduff Hughes, and Jeffrey Dean. 2016. Google’s
neural machine translation system: Bridging the gap
between human and machine translation. CoRR,
abs/1609.08144.

126


