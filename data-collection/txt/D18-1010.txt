



















































TwoWingOS: A Two-Wing Optimization Strategy for Evidential Claim Verification


Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 105–114
Brussels, Belgium, October 31 - November 4, 2018. c©2018 Association for Computational Linguistics

105

TWOWINGOS: A Two-Wing Optimization Strategy
for Evidential Claim Verification

Wenpeng Yin, Dan Roth
University of Pennsylvania

{wenpeng,danroth}@seas.upenn.edu

Abstract

Determining whether a given claim is sup-
ported by evidence is a fundamental NLP
problem that is best modeled as Textual En-
tailment. However, given a large collection of
text, finding evidence that could support or re-
fute a given claim is a challenge in itself, am-
plified by the fact that different evidence might
be needed to support or refute a claim. Nev-
ertheless, most prior work decouples evidence
identification from determining the truth value
of the claim given the evidence.

We propose to consider these two aspects
jointly. We develop TWOWINGOS (two-
wing optimization strategy), a system that,
while identifying appropriate evidence for a
claim, also determines whether or not the
claim is supported by the evidence. Given
the claim, TWOWINGOS attempts to iden-
tify a subset of the evidence candidates; given
the predicted evidence, it then attempts to
determine the truth value of the correspond-
ing claim. We treat this challenge as cou-
pled optimization problems, training a joint
model for it. TWOWINGOS offers two ad-
vantages: (i) Unlike pipeline systems, it facil-
itates flexible-size evidence set, and (ii) Joint
training improves both the claim verification
and the evidence identification. Experiments
on a benchmark dataset show state-of-the-art
performance.1

1 Introduction

A claim, e.g., “Marilyn Monroe worked with
Warner Brothers”, is an assertive sentence that
may be true or false. While the task of claim
verification will not tell us the absolute truth of
this claim, it is expected to determine whether the
claim is supported by evidence in a given text col-
lection. Specifically, given a claim and a text cor-
pus, evidential claim verification, demonstrated in

1cogcomp.org/page/publication_view/847

maybe ha
oyofajrfngn
ovajrnvhar

yaojnbarlvjh
nhjarnohg 

nvhyhnv  va j

maybe ha
oyofajrfngn
ovajrnvhar

yaojnbarlvjh
nhjarnohg 

nvhyhnv  va j

whatno jmof
jag 

as ajgonah
nbjunaeorg  
varguoergu

arg ag .
arghoguerng  

mao rhg aer
are hn kvar
enb bhebn

bnjb  ye nerb
hbjanrih

bjrbn  areb
ahofjrf 

Marilyn Monroe worked with
Warner Brothers

Canada is the second
largest country

no
enough

info

false

true

conflicting

...

text corpus claim entailment decision

Figure 1: Illustration of evidential claim verification
task. For a claim, we determine its truth value by evi-
dence identified from a text corpus.

Figure 1, aims at identifying text snippets in the
corpus that act as evidence that supports or refutes
the claim.

This problem has broad applications. For exam-
ple, knowledge bases (KB), such as Freebase (Bol-
lacker et al., 2008), YAGO (Suchanek et al., 2007),
can be augmented with a new relational statement
such as “(Afghanistan, is source of, Kushan Dy-
nasty)”. This needs to be first verified by a claim
verification process and supported by evidence
(Roth et al., 2009; Chaganty et al., 2017). More
broadly, claim verification is a key component in
any technical solution addressing recent concerns
about the trustworthiness of online content (Vy-
diswaran et al., 2011; Pasternack and Roth, 2013;
Hovy et al., 2013). In both scenarios, we care
about whether or not a claim holds, and seek re-
liable evidence in support of this decision.

Evidential claim verification requires that we
address three challenges. First, to locate text snip-
pets in the given corpus that can potentially be
used to determine the truth value of the given
claim. This differs from the conventional textual
entailment (TE) problem (Dagan et al., 2013) as
here we first look for the premises given a hypoth-
esis. Clearly, the evidence one seeks depends on
the claim, as well as on the eventual entailment

cogcomp.org/page/publication_view/847


106

s1

s2

si

sm

sm−1

x

y1

y2

yi

yn

yn−1

0

1

0

1

1

0

0

1

0

0

evidence
candidates

claim

entailment 
decisions

binary
vector

one-hot
vector

S Y

Figure 2: TWOWINGOS, a generic two-wing optimiza-
tion framework. A subset of the evidence candidates
Se = {s1, . . . , sm−1, sm} is chosen via a binary vector
(left), and an n-valued entailment decision yi ∈ Y is
chosen (right), with respect to the claim x.

decision – the same claim would require different
supporting than refuting evidence. This motivates
us to develop an approach that can transfer knowl-
edge from claim verification to evidence identifi-
cation. Second, the evidence for a claim might re-
quire aggregating information from multiple sen-
tences and even multiple documents (rf. #3 in Ta-
ble 4). Therefore, a set, rather than a collection of
independent text snippets, should be chosen to act
as evidence. And, finally, in difference from TE,
given a set of evidence sentences as a premise, the
truth value of the claim should depend on all of the
evidence, rather than on a single sentence there.

The discussion above suggests that claim verifi-
cation and evidence identification are tightly cou-
pled. Claim should influence the identification of
appropriate evidence, and “trusted evidence boosts
the claim’s veracity” (Vydiswaran et al., 2011).
Consequently, we propose TWOWINGOS, a two-
wing optimization strategy2, to support this pro-
cess. As shown in Figure 2, we consider a set
of sentences S as the candidate evidence space, a
claim x, and a decision space Y for the claim veri-
fication. In the optimal condition, a one-hot vector
over Y indicates which decision to make towards
the claim, and a binary vector over S indicates a
subset of sentences Se (in blue in Figure 2) to act
as evidence.

Prior work mostly approached this problem as
a pipeline procedure – first, given a claim x, de-
termine Se by some similarity matching; then,
conduct textual entailment over (Se, x) pairs.
Our framework, TWOWINGOS, optimizes the two

2By “two-wing optimization”, we mean that the same ob-
ject, i.e., the claim, is mapped into two target spaces in a joint
optimization scheme.

subtasks jointly, so that both claim verification and
evidence identification can enhance each other.
TWOWINGOS is a generic framework making
use of a shared representation of the claim to co-
train evidence identification and claim verifica-
tion.

TWOWINGOS is tested on the FEVER bench-
mark (Thorne et al., 2018), showing≈30% F1 im-
provement for evidence identification, and ≈23%
accuracy increase in claim verification. Our analy-
sis shows that (i) entity mentions in claims provide
a strong clue for retrieving relevant passages; (ii)
composition of evidence clues across sentences
helps claim verification; and that (iii) the joint
training scheme provides significant benefits of a
pipeline architecture.

2 Related Work

Most work focuses on the dataset construction
while lacking advanced models to handle the prob-
lem. Vlachos and Riedel (2014) propose and de-
fine the “fact checking” problem, without a con-
crete solution. Ferreira and Vlachos (2016) re-
lease the dataset “Emergent” for rumor debunking.
Each claim is accompanied by an article headline
as evidence. Then a three-way logistic regression
model is used over some rule-based features. No
need to search for evidence. Wang (2017) release a
larger dataset for fake news detection, and propose
a hybrid neural network to integrate the statement
and the speaker’s meta data to do classification.
However, the presentation of evidences is ignored.
Kobayashi et al. (2017) release a similar dataset to
(Thorne et al., 2018), but they do not consider the
evaluation of evidence reasoning.

Some work mainly pays attention to determin-
ing whether the claim is true or false, assuming ev-
idence facts are provided or neglecting presenting
evidence totally, e.g., (Angeli and Manning, 2014)
– given a database of true facts as premises, pre-
dicting whether an unseen fact is true and should
belong to the database by natural logic inference.
Open-domain question answering (QA) against a
text corpus (Yin et al., 2016; Chen et al., 2017;
Wang et al., 2018) can also be treated as claim ver-
ification problem, if we treat (question, correct an-
swer) as a claim. However, little work has studied
how well a QA system can identify all the answer
evidence.

Only a few works considered improving the evi-
dence presentation in claim verification problems.



107

Roth et al. (2009) introduce the task of Entailed
Relation Recognition – given a set of short para-
graphs and a relational fact in the triple form of
(argument1, relation, argument2), finding the para-
graphs that can entail this fact. They first use Ex-
panded Lexical Retrieval to rank and keep the top-
k paragraphs as candidates, then build a TE clas-
sifier over each (candidate, statement) pair. The
work directly related to us is by Thorne et al.
(2018). Given claims and a set of Wikipages,
Thorne et al. (2018) use a retrieval model based on
TF-IDF to locate top-5 sentences in top-5 pages as
evidence, then utilize a neural entailment model to
classify (evidence, claim) pairs.

In contrast, our work tries to optimize the claim
verification as well as the evidence identification
in a joint training scheme, which is more than just
supporting or refuting the claims.

3 The TWOWINGOS Model

Figure 2 illustrates the two-wing optimization
problem addressed in this work: given a collec-
tion of evidence candidates S={s1, s2, · · · , si, · · · ,
sm}, a claim x and a decision set Y = {y1 · · · , yn},
the model TWOWINGOS predicts a binary vector
p over S and a one-hot vector o over Y against the
ground truth, a binary vector q and a one-hot vec-
tor z, respectively. A binary vector over S means
a subset of sentences (Se) act as evidence, and the
one-hot vector indicates a single decision (yi) to
be made towards the claim x given the evidence
Se. Next, we use two separate subsections to elab-
orate the process of evidence identification (i.e.,
optimize p to q) and the claim verification (i.e.,
optimize o to z).

3.1 Evidence identification

A simple approach to identifying evidence is to de-
tect the top-k sentences that are lexically similar to
the claim, as some pipeline systems (Roth et al.,
2009; Thorne et al., 2018) do. However, a claim-
unaware fixed k is less optimal, adding noise or
missing key supporting factors, consequently lim-
iting the performance.

In this work, we approach the evidence by mod-
eling sentences S={s1, · · · , si, · · · , sm} with
the claim x as context in a supervised learning
scheme. For each si, the problem turns out to be
learning a probability: how likely si can entail the
claim conditioned on other candidates as context,
as shown by the blue items in Figure 2.

To start, a piece of text t (t ∈ S ∪ {x}) is repre-
sented as a sequence of l hidden states, forming a
feature map T ∈ Rd×l, where d is the dimension-
ality of hidden states. We first stack a vanilla CNN
(convolution & max-pooling) (LeCun et al., 1998)
over T to get a representation for t. As a result,
each evidence candidate si has a representation si,
and the claim x has a representation x. To get a
probability for each si, we need first to build its
claim-aware representation ri.

Coarse-grained representation. We directly
concatenate the representation of si and x, gen-
erated by the vanilla CNN, as:

ri = [si,x, si · xT ] (1)

This coarse-grained approach makes use of merely
the sentence-level representations while neglect-
ing more fine-grained interactions between the
sentences and the claim.

Fine-grained representation. Instead of di-
rectly employing the sentence-level representa-
tions, here we explore claim-aware representations
for each word in sentence si, then compose them
as the sentence representation ri, inspired by the
Attentive Convolution (Yin and Schütze, 2017).

For each word sji in si, we first calculate its
matching score towards each word xz in x, by dot
product over their hidden states. Then the repre-
sentation of the claim, as the context for the word
sji , is formed as:

cji =
∑
z

softmax(sji · (x
z)T ) · xz (2)

Now, word sji has left context s
j−1
i , right con-

text sj+1i in si, and the claim-aware context c
j
i

from x. A convolution encoder generates its
claim-aware representation iji :

iji = tanh(W · [s
j−1
i , s

j
i , s

j+1
i , c

j
i ] + b) (3)

where parameters W ∈ Rd×4d, b ∈ Rd.
To compose those claim-aware word represen-

tations as the representation for sentence si, we
use a max-pooling over {iji} along with j, gener-
ating ii.

We use term fint(si, x) to denote this whole
process, so that:

ii = fint(si, x) (4)

At this point, the fine-grained representation for
evidence candidate si is:

ri = [si,x, si · xT , ii] (5)



108

Loss function. With a claim-aware representa-
tion ri, the sentence si subsequently gets a prob-
ability, acting as the evidence, αi ∈ (0, 1) via a
non-linear sigmoid function:

αi = sigmoid(v · rTi ) (6)

where parameter vector v has the same dimension-
ality as ri.

In the end, all evidence candidates in S have
a ground-truth binary vector q and the predicted
probability vector α; then loss lev (“ev”: evidence)
is implemented as a binary cross-entropy:

lev =
m∑
i=1

−(qi log(αi)+(1−qi) log(1−αi)) (7)

As the output of this evidence identification
module, we binarize the probability vector α by
pi = [αi > 0.5] (“[x]” is 1 if x is true or 0 other-
wise). pi indicates si is evidence or not. All {si}
with pi = 1 act as evidence set Se.

3.2 Claim verification
As shown in Figure 2, to figure out an entailment
decision yi for the claim x, the evidence Se pos-
sibly consists of more than one sentence. Further-
more, those evidence sentences are not necessar-
ily in textual order nor from the same passage.
So, we need a mechanism that enables each evi-
dence or even each word inside to be aware of the
content from other evidence sentences. Similar to
the aforementioned approach to evidence identifi-
cation, we come up with three methods, with dif-
ferent representation granularity, to learn a repre-
sentation for (Se, x), i.e., the input for claim veri-
fication, shown in Figure 3.

Coarse-grained representation. In this case,
we treat Se as a whole, constructing its represen-
tation e by summing up the representations of all
sentences in Se in a weighted way:

e =
m∑
i=1

αi · pi · si (8)

where αi, from Equation 6, is the probability of si
being the evidence.

Then the (Se, x) pair gets a coarse-grained con-
catenated representation: [e,x]. It does not model
the interactions within the evidence nor the in-
teractions between the evidence and the claim.
Based on our experience in evidence identification

x

e

si

αi

x

Se

weighted
sum up

(a) Coarse-grained representations

ii

e

αi

xi

x

αi

si
x

Se

weighted
max-pooling

weighted
max-pooling

attentive 
convolution 

(b) Single-channel fine-grained representations

ii

e

αi

si

xi

x

αi

x

si

si

updated

weighted
max-pooling

weighted
max-pooling

attentive 
convolution 

Se

(c) Two-channel fine-grained representations

Figure 3: Three representation learning methods in
claim verification. Green arrows act as context in at-
tentive convolution.

module, the representation of a sentence is better
learned by composing context-aware word-level
representations. Next, we introduce how to learn
fine-grained representation for the (Se, x) pair.

Single-channel fine-grained representation.
By “single-channel,” we mean each sentence si is
aware of the claim x as its single context.

For a single pair (si, x), we utilize the func-
tion fint() in Equation 4 to build the fine-grained
representations for both si and x, obtaining ii =



109

fint(si, x) for si and xi = fint(x, si) for x.
For (Se, x), we compose all the {ii} and all the
{xi} along with i, via a weighted max-pooling:

e = maxpooli(αi · pi · ii) (9)
x = maxpooli(αi · pi · xi) (10)

This weighted max-pooling ensures that the
sentences with higher probabilities of being evi-
dence have a higher chance to present their fea-
tures. As a result, (Se, x) gets a concatenated rep-
resentation: [e, x]

Two-channel fine-grained representation. By
“two-channel,” we mean that each evidence si is
aware of two kinds of context, one from the claim
x, the other from the remaining evidences.

Our first step is to accumulate evidence clues
within Se. To start, we concatenate all sentences in
Se as a fake long sentence Ŝ consisting of hidden
states {ŝ}. Similar to Equation 2, for each word
sji in sentence si, we accumulate all of its related
clues (cji ) from Ŝ as follows:

cji =
∑
z

softmax(sji · (ŝ
z)T ) · ŝz (11)

Then we update sji , the representation of word
sji , by element-wise addition:

sji = s
j
i ⊕ c

j
i (12)

This step enables the word sji to “see” all related
clues from Se. The reason we add s

j
i and c

j
i is mo-

tivated by a simple experience: Assume the claim
“Lily lives in the biggest city in Canada”, and one
sentence contains a clue “· · · Lily lives in Toronto
· · · ” and another sentence contains a clue “· · ·
Toronto is Canada’s largest city· · · ”. The most
simple yet effective approach to aggregating the
two clues is to sum up their representation vectors
(Blacoe and Lapata, 2012) (we do not concatenate
them, as those clues have no consistent textual or-
der across different sji ).

After updating the representation of each word
in si, we perform the aforementioned “single-
channel fine-grained representation” between the
updated si and the claim x, generating [e, x].

Loss function. For the claim verification input
(Se, x), we forward its representation [e, x] to a

#SUPPORTED #REFUTED #NEI
train 80,035 29,775 35,639
dev 3,333 3,333 3,333
test 3,333 3,333 3,333

Table 1: Statistics of claims in FEVER dataset

logistic regression layer in order to infer a proba-
bility distribution o over the label space Y :

o = softmax(W · [e,x] + b) (13)

where W ∈ Rn×2d, b ∈ Rn
The loss lcv (“cv”: claim verification) is imple-

mented as negative log-likelihood:

lcv = − log(o · zT ) (14)

where z is the ground truth one-hot label vector
for the claim x on the space Y .

3.3 Joint optimization

Given the loss lev in evidence identification and
the loss lcv in claim verification, the overall train-
ing loss is represented by:

l = lev + lcv (15)

To ensure that we jointly train the two coupled
subtasks with intensive knowledge communica-
tion instead of simply putting two pipeline neural
networks together, our TWOWINGOS has follow-
ing configurations:
• Both subsystems share the same set of word

embeddings as parameters; the vanilla CNNs for
learning sentence and claim representations share
parameters as well.
• The output binary vector p by the evidence

identification module is forwarded to the module
of claim verification, as shown in Equations 8-10.
• Though the representation of a claim’s deci-

sion yi is not put explicitly into the module of ev-
idence identification, the claim’s representation x
will be fine-tuned by the yi, so that the evidence
candidates can get adjustment from the decision
yi, since the claims are shared by two modules.

4 Experiments

4.1 Setup

Dataset. In this work, we use FEVER (Thorne
et al., 2018). The claims in FEVER were gen-
erated from the introductory parts of about 50K



110

1 2 3 4 5 6 7 8 9 10 >10
#sentence and #page in evidence

0

5

10

15

...
%

4.07

2.85 2.90

1.76
0.98 0.68 0.49 0.40

1.85

71.88

12.13

1.85
0.74 0.46 0.29 0.21 0.15 0.10 0.06 0.13

83.67

12.35

#sent.
#page

Figure 4: Distribution of #sentence and #pages in
FEVER evidence

Wikipedia pages of a June 2017 dump. Anno-
tators construct claims about a single fact of the
title entity with arbitrarily complex expressions
and entity forms. To increase the claim com-
plexity so that claims would not be trivially ver-
ified, annotators adopt two routes: (i) Provid-
ing additional knowledge: Annotators can explore
a dictionary of terms that were (hyper-)linked,
along with their pages; (ii) Mutate claims in six
ways: negation, paraphrasing, substitution of a
relation/entity with a similar/dissimilar one, and
making the claims more general/specific. All re-
sulting claims have 9.4 tokens in average. Apart
from claims, FEVER also provides a Wikipedia
corpus in size of about 5.4 million.

Each claim is labeled as SUPPORTED, RE-
FUTED or NOTENOUGHINFO (NEI). In addition,
evidence sentences, from any wiki page, are re-
quired to be provided for SUPPORTED and RE-
FUTED. Table 1 lists the data statistics. Figure 4
shows the distributions of sentence sizes and page
sizes in FEVER’s evidence set. We can see that
roughly 28% of the evidence covers more than
one sentence, and approximately 16.3% of the ev-
idence covers more than one wiki page.

This task has three evaluations: (i)
NOSCOREEV – accuracy of claim verifica-
tion, neglecting the validity of evidence; (ii)
SCOREEV – accuracy of claim verification with
a requirement that the predicted evidence fully
covers the gold evidence for SUPPORTED and RE-
FUTED; (iii) F1 – between the predicted evidence
sentences and the ones chosen by annotators. We
use the officially released evaluation scorer 3.

3https://github.com/sheffieldnlp/fever-scorer

Wiki page retrieval4. For each claim, we search
in the given dictionary of wiki pages in the form of
{title: sentence list}, and keep the top-5 ranked
pages for fair comparison with Thorne et al.
(2018). Algorithm 1 briefly shows the steps of
wiki page retrieval. To speed up, we first build
an inverted index from words to titles, then for
each claim, we only search in the titles that cover
at least one claim word.

Input: A claim, wiki={title: page vocab}
Output: A ranked top-k wiki titles
Generate entity mentions from the claim;
while each title do

if claim.vocab∩title.vocab is empty then
discard this title

else
title score = the max recall value of title.vocab

in claim and in entity mentions of the claim;
if title score = 1.0 then

title.score = title score
else

page score = recall of claim in
page vocab;

title.score = title score + page score
end

end
end
Sort titles by title.score in descending order

Algorithm 1: Algorithm description of wiki
page retrieval for FEVER claims.

All sentences of the top-5 retrieved wiki pages
are kept as evidence candidates for claims in train,
dev and test. It is worth mentioning that this page
retrieval step is a reasonable preprocessing which
controls the complexity of evidence searching in
real-world, such as the big space – 5.4 million – in
this work.

Training setup. All words are initialized by
300D Word2Vec (Mikolov et al., 2013) embed-
dings, and are fine-tuned during training. The
whole system is trained by AdaGrad (Duchi et al.,
2011). Other hyperparameter values include:
learning rate 0.02, hidden size 300, mini-batch
size 50, filter width 3.

Baselines. In this work, we first consider the two
systems reported by Thorne et al. (2018): (i) MLP:
A multi-layer perceptron with one hidden layer,
based on TF-IDF cosine similarity between the
claim and the evidence (all evidence sentences are
concatenated as a longer text piece) (Riedel et al.,
2017); (ii) Decomp-Att (Parikh et al., 2016): A
decomposable attention model that develops atten-

4Our retrieval results are released as well.



111

k (Thorne et al., 2018) ours
rate acc ceiling rate acc ceiling

1 25.31 50.21 76.58 84.38
5 55.30 70.20 89.63 93.08
10 65.86 77.24 91.19 94.12
25 75.92 83.95 92.81 95.20
50 82.49 90.13 93.36 95.57
100 86.59 91.06 94.19 96.12

Table 2: Wikipage retrieval evaluation on dev. “rate”:
claim proportion, e.g., x%, if its gold passages are
fully retrieved (for “SUPPORT” and “REFUTE” only);
“acc ceiling”: x%·(#S+#R)+#N#S+#R+#N , the upper bound of
accuracy for three classes if the coverage x% satisfies.

tion mechanisms to decompose the problem into
subproblems to solve in parallel. Note that both
systems first employed an IR system to keep top-
5 relevant sentences from the retrieved top-5 wiki
pages as static evidence for claims.

We further consider the following variants of
our own system TWOWINGOS:
• Coarse-coarse: Both evidence identification

and claim verification adopt coarse-grained repre-
sentations.

To further study our system, we test this
“coarse-coarse” in three setups: (i) “pipeline” –
train the two modules independently. Forward the
predicted evidence to do entailment for claims; (ii)
“diff-CNN” – joint training with separate CNN pa-
rameters to learn sentence/claim representations;
(iii) “share-CNN” – joint training with shared
CNN parameters.

The following variants are in joint training.
• Fine&sentence-wise: Given the evidence

with multiple sentences, a natural baseline is to do
entailment reasoning for each (sentence, claim),
then compose. We do entailment reasoning be-
tween each predicted evidence sentence and the
claim, generating a probability distribution over
the label space Y . Then we sum up all the distribu-
tion vectors element-wise, as an ensemble system,
to predict the label;
• Four combinations of different grained rep-

resentation learning: “coarse&fine(single)”,
“coarse&fine(two)”, “fine&coarse” and
“fine&fine(two)”. “Single” and “two” refer
to the single/two-channel cases respectively.

4.2 Results

Performance of passage retrieval. Table 2
compares our wikipage retriever with the one in

(Thorne et al., 2018), which used a document re-
triever5 from DrQA (Chen et al., 2017).

Our document retrieval module surpasses the
competitor by a big margin in terms of the cover-
age of gold passages: 89.63% vs. 55.30% (k = 5
in all experiments). Its powerfulness should be
attributed to: (i) Entity mention detection in the
claims. (ii) As wiki titles are entities, we have a
bi-channel way to match the claim with the wiki
page: one with the title, the other with the page
body, as shown in Algorithm 1.

Performance on FEVER Table 3 lists the
performances of baselines and the TWOWIN-
GOS variants on FEVER (dev&test). From the
dev block, we observe that:
• TWOWINGOS (from “share-CNN”) sur-

passes prior systems in big margins. Overall,
fine-grained schemes in each subtask contribute
more than the coarse-grained counterparts;
• In the three setups – “pipeline”, “diff-CNN”

and “share-CNN” – of coarse-coarse, “pipeline”
gets better scores than (Thorne et al., 2018) in
terms of evidence identification. “Share-CNN”
has comparable F1 as “diff-CNN” while gaining
a lot on NOSCOREEV (72.32 vs. 39.22) and
SCOREEV (50.12 vs. 21.04). This clearly shows
that the claim verification gains much knowledge
transferred from the evidence identification mod-
ule. Both “diff-CNN” and “share-CNN” perform
better than “pipeline” (except for the slight inferi-
ority at SCOREEV: 21.04 vs. 22.26).
• Two-channel fine-grained representations

show more effective than the single-channel
counterpart in claim verification (NOSCOREEV:
78.77 vs. 75.65, SCOREEV: 53.64 vs. 52.65).
As we expected, evidence sentences should
collaborate in inferring the truth value of the
claims. Two-channel setup enables an evidence
candidate aware of other candidates as well as the
claim.
• In the last three rows of dev, there is no

clear difference among their evidence identifica-
tion scores. Recall that “sent-wise” is essentially
an ensemble system over each (sentence, claim)
entailment result. “Coarse-grained”, instead, first
sums up all sentence representation, then performs
(
∑

(sentence), claim) reasoning. We can also
treat this “sum up” as an ensemble. Their com-
parison shows that these two kinds of tricks do not

5It compares passages and claims as TF-IDF weighted
bag-of-bigrams.



112

claim verification evidence identification
system NOSCOREEV SCOREEV recall precision F1

de
v

MLP 41.86 19.04 44.22 10.44 16.89
Decomp-Att 52.09 32.57 44.22 10.44 16.89

T
W

O
W

IN
G

O
S

coarse&coarse
pipeline 35.72 22.26 53.75 29.42 33.80
diff-CNN 39.22 21.04 46.88 43.01 44.86
share-CNN 72.32 50.12 45.55 40.77 43.03

coarse&fine(single) 75.65 52.65 45.81 42.53 44.11
coarse&fine(two) 78.77 53.64 45.78 39.23 42.25
fine&sent-wise 71.02 53.43 52.70 48.31 50.40
fine&coarse 71.48 53.17 52.75 47.30 49.87
fine&fine(two) 78.90 56.16 53.81 47.73 50.59

te
st (Thorne et al., 2018) 50.91 31.87 45.89 10.79 17.47

TWOWINGOS 75.99 54.33 49.91 44.68 47.15

Table 3: Performance on dev and test of FEVER. TWOWINGOS outperforms prior systems if vanilla CNN
parameters are shared by evidence identification and claim verification subsystems. It gains more if fine-grained
representations are adopted in both subtasks.

1 2 3 4 >4
#sentence in evidence

0

10

20

30

40

50

60

70

80

%

ScoreEv
NoScoreEv
Precision
Recall
F1

Figure 5: Performance vs. #sentence in evidence. Our
system has robust precisions. The overall performance
NOSCOREEV is not influenced by the decreasing re-
call; this verifies the fact that the truth value of most
claims can be determined by a single identified evi-
dence sentence.

make much difference.
If we adopt “two-channel fine-grained repre-

sentation” in claim verification, big improvements
are observed in both NOSCOREEV (+7.42%) and
SCOREEV (+3%).

In the test block, our system (fine&fine(two))
beats the prior top system across all measurements
by big margins – F1: 47.15 vs. 17.47; SCOREEV:
54.33 vs. 31.87; NOSCOREEV: 75.99 vs. 50.91.

In both dev and test blocks, we can observe that
our evidence identification module consistently

obtains balanced recall and precision. In con-
trast, the pipeline system by Thorne et al. (2018)
has much higher recall than precision (45.89 vs.
10.79). It is worth mentioning that the SCOREEV
metric is highly influenced by the recall value,
since SCOREEV is computed on the claim in-
stances whose evidences are fully retrieved, re-
gardless of the precision. So, ideally, a system can
set all sentences as evidence, so that SCOREEV
can be promoted to be equal to NOSCOREEV. Our
system is more reliable in this perspective.

Performance vs. #sent. in evidence. Figure 5
shows the results of the five evaluation measures
against different sizes of gold evidence sentences
in test set. We observe that: (i) Our system has
robust precisions across #sentence; however, the
recall decreases. This is not that surprising, since
the more ground-truth sentences in evidence, the
harder it is to retrieve all of them; (ii) Due to the
decrease in recall, the SCOREEV also gets influ-
enced for bigger #sentence. Interestingly, high
precision and worse recall in evidence with more
sentences still make consistently strong overall
performance, i.e., NOSCOREEV. This should be
due to the fact that the majority (83.18% (Thorne
et al., 2018)) of claims can be correctly entailed by
a single ground truth sentence, even if any remain-
ing ground truth sentences are unavailable.

Error analysis. The case #1 in Table 4 shows
that our system identifies two pieces of evidence



113

# G/P claim gold evidence predicted evidence

1 0/1 Telemundo is an English-languagetelevision network.

(Telemundo, 0) (Telemundo, 0)
(Telemundo, 1) (Telemundo, 4)
(Telemundo, 4) (Fourth television network, 0)
(Telemundo, 5) (Fourth television network, 4)

(Hispanic and Latino Americans, 0)

2 1/2 Home for the Holidays stars a famousAmerican actor.

(Anne Bancroft, 0)

∅(Charles Durning, 0)(Holly Hunter, 0)
(Home for the Holidays (1995 film), 5)

3 0/2 Both hosts of Weekly Idol were born in1983.

(Weekly Idol, 0)
(Weekly Idol, 1)(Weekly Idol, 1)

(Defconn, 0)

Table 4: Error cases of TWOWINGOS in FEVER. “G/P”: gold/predicted label (“0”: refute; “1”: support; “2”: not
enough information). To save space, we use “(title, i)” to denote the ith sentence in the corresponding wiki page.

(i.e., (Telemundo, 0) and (Telemundo, 4)) cor-
rectly; however, it falsely predicts the claim la-
bel. (Telemundo, 0): Telemundo is an Amer-
ican Spanish-language terrestrial television · · · .
We can easily find that the keyword “Spanish-
language” should refute the claim. However,
both “Spanish-language” in this evidence and the
“English-language” in the claim are unknown to-
kens with randomly initialized embeddings. This
hints that a more careful data preprocessing may
be helpful. In addition, to refute the claim, an-
other clue comes from the combination of (Tele-
mundo, 4) and (Hispanic and Latino Americans,
0). (Telemundo, 4): “The channel · · · aimed
at Hispanic and Latino American audiences”;
(Hispanic and Latino Americans, 0): “Hispanic
Americans and Latino Americans · · · are descen-
dants of people from countries of Latin America
and Spain.”. Our system only retrieved (Telemu-
ndo, 4). And this clue is hard to grasp as it re-
quires some background knowledge – people from
Latin America and Spain usually are not treated as
English-speaking.

In the case #2, our system fails to iden-
tify any evidence. This is due to the failure
of our passage retrieval module: it detects
entity mentions “Home”, “Holidays” and
“American”, and the top-5 retrieved pas-
sages are “Home”, “Home for the Holidays”,
“American Home”, “American” and
“Home for the Holidays (song)”, which un-
fortunately cover none of the four ground truth
passages. Interestingly, (i) given the falsely re-
trieved passages, our system predicts “no sentence
is valid evidence” (denoted as ∅ in Table 4); (ii)
given the empty evidence, our system predicts
“NoEnoughInfo” for this claim. Both make sense.

In the case #3, a successful classification of the

claim requires information aggregation over the
three gold evidence sentences: (Weekly Idol, 0):
“Weekly Idol is a South Korean variety show · · · ”;
(Weekly Idol, 1): “The show is hosted by come-
dian Jeong Hyeong-don and rapper Defconn.”;
(Defconn, 0): “Defconn (born Yoo Dae-joon; Jan-
uary 6 , 1977 ) is a · · · ”. To successfully retrieve
the three sentences as a whole set of evidence is
challenging in evidence identification. Addition-
ally, this example relies on the recognition and
matching of digital numbers (1983 vs. 1977),
which is beyond the expressivity of word embed-
dings, and is expected to be handled by rules more
easily.

5 Summary

In this work, we build TWOWINGOS, a two-wing
optimization framework to address the claim veri-
fication problem by presenting precise evidence.
Differing from a pipeline system, TWOWIN-
GOS ensures the evidence identification mod-
ule and the claim verification module are trained
jointly, in an end-to-end scheme. Experiments
show the superiority of TWOWINGOS in the
FEVER benchmark.

Acknowledgments

We thank group colleagues (Nitish Gupta and Jen-
nifer Sheffield) and Dr. Mo Yu from IBM AI
Foundations Lab for providing insightful com-
ments and critiques. This work was supported by
Contract HR0011-15-2-0025 with the US Defense
Advanced Research Projects Agency (DARPA).
Approved for Public Release, Distribution Unlim-
ited. The views expressed are those of the authors
and do not reflect the official policy or position
of the Department of Defense or the U.S. Govern-
ment.



114

References
Gabor Angeli and Christopher D. Manning. 2014. Nat-

uralli: Natural logic inference for common sense
reasoning. In Proceedings of EMNLP, pages 534–
545.

William Blacoe and Mirella Lapata. 2012. A com-
parison of vector-based representations for seman-
tic composition. In Proceedings of EMNLP-CoNLL,
pages 546–556.

Kurt Bollacker, Colin Evans, Praveen Paritosh, Tim
Sturge, and Jamie Taylor. 2008. Freebase: a collab-
oratively created graph database for structuring hu-
man knowledge. In Proceedings of SIGMOD, pages
1247–1250.

Arun Tejasvi Chaganty, Ashwin Paranjape, Percy
Liang, and Christopher D. Manning. 2017. Impor-
tance sampling for unbiased on-demand evaluation
of knowledge base population. In Proceedings of
EMNLP, pages 1038–1048.

Danqi Chen, Adam Fisch, Jason Weston, and Antoine
Bordes. 2017. Reading wikipedia to answer open-
domain questions. In Proceedings of ACL, pages
1870–1879.

Ido Dagan, Dan Roth, Mark Sammons, and Fabio Mas-
simo Zanzoto. 2013. Recognizing textual entail-
ment: Models and applications.

John Duchi, Elad Hazan, and Yoram Singer. 2011.
Adaptive subgradient methods for online learning
and stochastic optimization. JMLR, 12:2121–2159.

William Ferreira and Andreas Vlachos. 2016. Emer-
gent: a novel data-set for stance classification. In
Proceedings of NAACL, pages 1163–1168.

Dirk Hovy, Taylor Berg-Kirkpatrick, Ashish Vaswani,
and Eduard H. Hovy. 2013. Learning whom to
trust with MACE. In Proceedings of NAACL, pages
1120–1130.

Mio Kobayashi, Ai Ishii, Chikara Hoshino, Hiroshi
Miyashita, and Takuya Matsuzaki. 2017. Auto-
mated historical fact-checking by passage retrieval,
word statistics, and virtual question-answering. In
Proceedings of IJCNLP, pages 967–975.

Yann LeCun, Léon Bottou, Yoshua Bengio, and Patrick
Haffner. 1998. Gradient-based learning applied to
document recognition. In Proceedings of the IEEE,
pages 2278–2324.

Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Cor-
rado, and Jeff Dean. 2013. Distributed representa-
tions of words and phrases and their compositional-
ity. In Proceedings of NIPS, pages 3111–3119.

Ankur P. Parikh, Oscar Täckström, Dipanjan Das, and
Jakob Uszkoreit. 2016. A decomposable attention
model for natural language inference. In Proceed-
ings of EMNLP, pages 2249–2255.

Jeff Pasternack and Dan Roth. 2013. Latent credibil-
ity analysis. In Proceedings of WWW, pages 1009–
1020.

Benjamin Riedel, Isabelle Augenstein, Georgios P. Sp-
ithourakis, and Sebastian Riedel. 2017. A simple but
tough-to-beat baseline for the fake news challenge
stance detection task. CoRR, abs/1707.03264.

Dan Roth, Mark Sammons, and V. G. Vinod Vydis-
waran. 2009. A framework for entailed relation
recognition. In Proceedings of ACL, pages 57–60.

Fabian M. Suchanek, Gjergji Kasneci, and Gerhard
Weikum. 2007. Yago: a core of semantic knowl-
edge. In Proceedings of WWW, pages 697–706.

James Thorne, Andreas Vlachos, Christos
Christodoulopoulos, and Arpit Mittal. 2018.
FEVER: a large-scale dataset for fact extraction and
verification. In Proceedings of NAACL.

Andreas Vlachos and Sebastian Riedel. 2014. Fact
checking: Task definition and dataset construction.
In Proceedings of the Workshop on Language Tech-
nologies and Computational Social Science@ACL,
pages 18–22.

V. G. Vinod Vydiswaran, ChengXiang Zhai, and Dan
Roth. 2011. Content-driven trust propagation frame-
work. In Proceedings of SIGKDD, pages 974–982.

Shuohang Wang, Mo Yu, Xiaoxiao Guo, Zhiguo Wang,
Tim Klinger, Wei Zhang, Shiyu Chang, Gerry
Tesauro, Bowen Zhou, and Jing Jiang. 2018. R3:
Reinforced ranker-reader for open-domain question
answering. In Proceedings of AAAI.

William Yang Wang. 2017. “Liar, Liar Pants on Fire”:
A new benchmark dataset for fake news detection.
In Proceedings of ACL, pages 422–426.

Wenpeng Yin, Sebastian Ebert, and Hinrich Schütze.
2016. Attention-based convolutional neural network
for machine comprehension. In Proceedings of the
NAACL Workshop on Human-Computer Question
Answering, pages 15–21.

Wenpeng Yin and Hinrich Schütze. 2017. Attentive
convolution. CoRR, abs/1710.00519.


