



















































SUMBT: Slot-Utterance Matching for Universal and Scalable Belief Tracking


Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 5478‚Äì5483
Florence, Italy, July 28 - August 2, 2019. c¬©2019 Association for Computational Linguistics

5478

SUMBT: Slot-Utterance Matching
for Universal and Scalable Belief Tracking

Hwaran Lee* Jinsik Lee*
SK T-Brain, AI Center, SK telecom

{hwaran.lee, jinsik16.lee, oceanos}@sktbrain.com

Tae-Yoon Kim

Abstract

In goal-oriented dialog systems, belief track-
ers estimate the probability distribution of slot-
values at every dialog turn. Previous neu-
ral approaches have modeled domain- and
slot-dependent belief trackers, and have diffi-
culty in adding new slot-values, resulting in
lack of flexibility of domain ontology con-
figurations. In this paper, we propose a
new approach to universal and scalable be-
lief tracker, called slot-utterance matching be-
lief tracker (SUMBT). The model learns the
relations between domain-slot-types and slot-
values appearing in utterances through atten-
tion mechanisms based on contextual seman-
tic vectors. Furthermore, the model pre-
dicts slot-value labels in a non-parametric way.
From our experiments on two dialog cor-
pora, WOZ 2.0 and MultiWOZ, the proposed
model showed performance improvement in
comparison with slot-dependent methods and
achieved the state-of-the-art joint accuracy.

1 Introduction

As the prevalent use of conversational agents,
goal-oriented systems have received increasing at-
tention from both academia and industry. The
goal-oriented systems help users to achieve goals
such as making restaurant reservations or booking
flights at the end of dialogs. As the dialog pro-
gresses, the system is required to update a distribu-
tion over dialog states which consist of users‚Äô in-
tent, informable slots, and requestable slots. This
is called belief tracking or dialog state tracking
(DST). For instance, for a given domain and slot-
types (e.g., ‚Äòrestaurant‚Äô domain and ‚Äòfood‚Äô slot-
type), it estimates the probability of corresponding
slot-value candidates (e.g., ‚ÄòKorean‚Äô and ‚ÄòModern

*Hwaran Lee and Jinsik Lee equally contributed to this
work.

European‚Äô) that are pre-defined in a domain ontol-
ogy. Since the system uses the predicted outputs
of DST to choose the next action based on a dialog
policy, the accuracy of DST is crucial to improve
the overall performance of the system. Moreover,
dialog systems should be able to deal with newly
added domains and slots1 in a flexible manner, and
thus developing scalable dialog state trackers is in-
evitable. Regarding to this, Chen et al. (2016) has
proposed a model to capture relations from intent-
utterance pairs for intent expansion.

Traditional statistical belief trackers (Hender-
son et al., 2014) are vulnerable to lexical and
morphological variations because they depend on
manually constructed semantic dictionaries. With
the rise of deep learning approaches, several neu-
ral belief trackers (NBT) have been proposed and
improved the performance by learning semantic
neural representations of words (MrksÃåicÃÅ et al.,
2017; MrksÃåicÃÅ and VulicÃÅ, 2018). However, the
scalability still remains as a challenge; the previ-
ously proposed methods either individually model
each domain and/or slot (Zhong et al., 2018; Ren
et al., 2018; Goel et al., 2018) or have difficulty
in adding new slot-values that are not defined in
the ontology (Ramadan et al., 2018; Nouri and
Hosseini-Asl, 2018).

In this paper, we focus on developing a ‚Äúscal-
able‚Äù and ‚Äúuniversal‚Äù belief tracker, whereby only
a single belief tracker serves to handle any domain
and slot-type. To tackle this problem, we pro-
pose a new approach, called slot-utterance match-
ing belief tracker (SUMBT), which is a domain-
and slot-independent belief tracker as shown in
Figure 1. Inspired by machine reading com-
prehension techniques (Chen et al., 2017; Seo
et al., 2017), SUMBT considers a domain-slot-

1For example, as reported by Kim et al. (2018), hundreds
of new skills are added per week in personal assistant ser-
vices.



5479

type (e.g., ‚Äòrestaurant-food‚Äô) as a question and
finds the corresponding slot-value in a pair of user
and system utterances, assuming the desirable an-
swer exists in the utterances. SUMBT encodes
system and user utterances using recently pro-
posed BERT (Devlin et al., 2018) which provides
the contextualized semantic representation of sen-
tences. Moreover, the domain-slot-types and slot-
values are also literally encoded by BERT. Then
SUMBT learns the way where to attend that is re-
lated to the domain-slot-type information among
the utterance words based on their contextual se-
mantic vectors. The model predicts the slot-value
label in a non-parametric way based on a certain
metric, which enables the model architecture not
to structurally depend on domains and slot-types.
Consequently, a single SUMBT can deal with any
pair of domain-slot-type and slot-value, and also
can utilize shared knowledge among multiple do-
mains and slots.

We will experimentally demonstrate the effi-
cacy of the proposing model on two goal-oriented
dialog corpora: WOZ 2.0 and MultiWOZ. We will
also qualitatively analyze how the model works.
Our implementation is open-published.2

2 SUMBT

The proposed model consists of four parts as il-
lustrated in Figure 1: BERT encoders for encod-
ing slots, values, and utterances (the grey and blue
boxes); a slot-utterance matching network (the red
box); a belief tracker (the orange box); and a non-
parametric discriminator (the dashed line on top).

2.1 Contextual Semantic Encoders

For sentence encoders, we employed a pre-trained
BERT model (Devlin et al., 2018) which is a
deep stack of bi-directional Transformer encoders.
Rather than a static word vector, it provides effec-
tive contextual semantic word vectors. Moreover,
it offers an aggregated representation of a word se-
quence such as a phrase and sentence, and there-
fore we can obtain an embedding vector of slot-
types or slot-values that consist of multiple words.

The proposed method literally encodes words
of domain-slot-types s and slot-values vt at turn
t as well as the system and user utterances. For
the pair of system and user utterances, xsyst =
(wsys1 , ..., w

sys
n ) and xusrt = (wusr1 , ..., wusrm ), the

pre-trained BERT encodes each word w into a
2https://github.com/SKTBrain/SUMBT

Multi-head
Attention

RNN

LayerNorm

ùëà"

h"$

d"$
d"&'$

y)"$ùëë(y)"$, y"$)

[CLS] restaurant ‚Äì food [SEP] 

Trm Trm Trm

Trm Trm Trm

EMB0 EMB1 EMBs+2

‚Ä¶

‚Ä¶

‚Ä¶

BERTsv

[CLS] ùë§$' [SEP]

q$

[CLS] what type of food would you like ? [SEP]
a moderately priced modern European food . [SEP]

Trm Trm Trm

Trm Trm Trm

EMB0 EMB1 EMBn+m+2

‚Ä¶

‚Ä¶

‚Ä¶

BERT

[CLS] ùë§"' [SEP]

u"6 u"' u"7898:

Trm Trm Trm

Trm Trm Trm

EMB0 EMB1 EMBs+2

‚Ä¶

‚Ä¶

‚Ä¶

BERTsv

[CLS] ùë§;' [SEP]

y"$

[CLS] modern European [SEP] 

Figure 1: The architecture of slot-utterance matching
belief tracker (SUMBT). An example of system and
user utterances, a domain-slot-type, and a slot-value is
denoted in red.

contextual semantic word vector u, and the en-
coded utterances are represented in the following
matrix representation:

Ut = BERT ([xsyst , x
usr
t ]) . (1)

Note that the sentence pairs are concatenated with
a separation token [SEP], and BERT will be fine-
tuned with the loss function (Eq. 7).

For the domain-slot-type s and slot-value vt,
another pre-trained BERT which is denoted as
BERTsv encodes their word sequences xs and xvt
into contextual semantic vectors qs and yvt , re-
spectively.

qs = BERTsv(xs),

yvt = BERTsv(x
v
t ).

(2)

We use the output vectors corresponding to the
classification embedding token [CLS] that sum-
marizes the whole input sequence.

Note that we consider xs as a phrase of do-
main and slot words (e.g., xs = ‚Äúrestaurant ‚Äì price
range‚Äù) so that qs represents both domain and
slot information. Moreover, fixing the weights
of BERTsv during training allows the model to
maintain the encoded contextual vector of any new
pairs of domain and slot-type. Hence, simply by
forwarding them into the slot-value encoder, the
proposed model can be scalable to the new do-
mains and slots.

https://github.com/SKTBrain/SUMBT


5480

2.2 Slot-Utterance Matching

In order to retrieve the relevant information cor-
responding to the domain-slot-type from the ut-
terances, the model uses an attention mechanism.
Considering the encoded vector of the domain-
slot-type qs as a query, the model matches it to the
contextual semantic vectors u at each word posi-
tion, and then the attention scores are calculated.

Here, we employed multi-head attention
(Vaswani et al., 2017) for the attention mech-
anism. The multi-head attention maps a query
matrix Q, a key matrix K, and a value matrix V
with different linear h projections, and then the
scaled dot-product attention is performed on those
matrices. The attended context vector hst between
the slot s and the utterances at t is

hst = MultiHead(Q,K, V ), (3)

where Q is Qs and K and V are Ut.

2.3 Belief Tracker

As the conversation progresses, the belief state at
each turn is determined by the previous dialog his-
tory and the current dialog turn. The flow of dia-
log can be modeled by RNNs such as LSTM and
GRU, or Transformer decoders (i.e., left-to-right
uni-directional Transformer).

In this work, the attended context vector ht is
fed into an RNN,

dst = RNN(d
s
t‚àí1,h

s
t ). (4)

It learns to output a vector that is close to the target
slot-value‚Äôs semantic vector.

Since the output of BERT is normalized by
layer normalization (Ba et al., 2016), the output
of RNN dt is also fed into a layer normalization
layer to help training convergence,

yÃÇst = LayerNorm(d
s
t ). (5)

2.4 Training Criteria

The proposed model is trained to minimize the
distance between outputs and target slot-value‚Äôs
semantic vectors under a certain distance metric.
The probability distribution of a slot-value vt is
calculated as

p
(
vt|xsys‚â§t ,x

usr
‚â§t , s

)
=

exp (‚àíd(yÃÇst ,yvt ))‚àë
v‚Ä≤‚ààCs exp

(
‚àíd(yÃÇst ,yv

‚Ä≤
t )
) ,

(6)

where d is a distance metric such as Euclidean dis-
tance or negative cosine distance, and Cs is a set of
the candidate slot-values of slot-type s which is
defined in the ontology. This discriminative clas-
sifier is similar to the metric learning method pro-
posed in Vinyals et al. (2016), but the distance
metric is measured in the fixed space that BERT
represents rather than in a trainable space.

Finally, the model is trained to minimize the log
likelihood for all dialog turns t and slot-types s ‚àà
D as following:

L(Œ∏) = ‚àí
‚àë
s‚ààD

T‚àë
t=1

log p(vt|xsys‚â§t ,x
usr
‚â§t , s). (7)

By training all domain-slot-types together, the
model can learn general relations between slot-
types and slot-values, which helps to improve per-
formance.

3 Experimental Setup

3.1 Datasets
To demonstrate the performance of our approach,
we conducted experiments over WOZ 2.0 (Wen
et al., 2017) and MultiWOZ (Budzianowski et al.,
2018) datasets. WOZ 2.0 dataset3 is a single
‚Äòrestaurant reservation‚Äô domain, in which belief
trackers estimate three slots (area, food, and price
range). MultiWOZ dataset4 is a multi-domain
conversational corpus, in which the model has to
estimate 35 slots of 7 domains.

3.2 Baselines
We designed three baseline models: BERT+RNN,
BERT+RNN+Ontology, and a slot-dependent
SUMBT. 1) The BERT+RNN consists of a con-
textual semantic encoder (BERT), an RNN-based
belief tracker (RNN), and a linear layer fol-
lowed by a softmax output layer for slot-value
classification. The contextual semantic en-
coder in this model outputs aggregated out-
put vectors like those of BERTsv. 2) The
BERT+RNN+Ontology consists of all compo-
nents in the BERT+RNN, an ontology encoder
(Ontology), and an ontology-utterance match-
ing network which performs element-wise mul-
tiplications between the encoded ontology and

3Downloaded from https://github.com/
nmrksic/neural-belief-tracker

4Downloaded from http://dialogue.mi.eng.
cam.ac.uk/index.php/corpus. Before conducting
experiments, we performed data cleansing such as correcting
misspelled words.

https://github.com/nmrksic/neural-belief-tracker
https://github.com/nmrksic/neural-belief-tracker
http://dialogue.mi.eng.cam.ac.uk/index.php/corpus
http://dialogue.mi.eng.cam.ac.uk/index.php/corpus


5481

utterances as in Ramadan et al. (2018). Note
that two aforementioned models BERT+RNN and
BERT+RNN+Ontology use the linear layer to
transform a hidden vector to an output vector,
which depends on a candidate slot-value list. In
other words, the models require re-training if the
ontology is changed, which implies that these
models have lack of scalability. 3) The slot-
dependent SUMBT has the same architecture with
the proposed model, but the only difference is that
the model is individually trained for each slot.

3.3 Configurations

We employed the pre-trained BERT model that
has 12 layers of 784 hidden units and 12 self-
attention heads.5 We experimentally found the
best configuration of hyper-parameters in which
search space is denoted in the following braces.
For slot and utterance matching, we used the
multi-head attention with {4, 8} heads and 784
hidden units. We employed a single-layer
{GRU,LSTM} with {100, 200, 300} hidden units
as the RNN belief tracker. For distance measure,
both Euclidean and negative cosine distances were
investigated. The model was trained with Adam
optimizer in which learning rate linearly increased
in the warm-up phase then linearly decreased. We
set the warm-up proportion to be {0.01, 0.05, 0.1}
of {300, 500} epochs and the learning rate to be
{1√ó 10‚àí5, 5√ó 10‚àí5}. The training stopped early
when the validation loss was not improved for 20
consecutive epochs. We report the mean and stan-
dard deviation of joint goal accuracies over 20 dif-
ferent random seeds. For reproducibility, we pub-
lish our PyTorch implementation code and the pre-
processed MultiWOZ dataset.

4 Experimental Results

4.1 Joint Accuracy Performance

The experimental results on WOZ 2.0 corpus
are presented in Table 1. The joint accuracy
of SUMBT is compared with those of the base-
line models that are described in Section 3.2
as well as previously proposed models. The
models incorporating the contextual semantic en-
coder BERT beat all previous models. Further-
more, the three baseline models, BERT+RNN,
BERT+RNN+Ontology, and the slot-dependent

5The pretrained model is published in
https://github.com/huggingface/
pytorch-pretrained-BERT

Model Joint Accuracy
NBT-DNN (MrksÃåicÃÅ et al., 2017) 0.844
BT-CNN (Ramadan et al., 2018) 0.855
GLAD (Zhong et al., 2018) 0.881
GCE (Nouri and Hosseini-Asl, 2018) 0.885
StateNetPSI (Ren et al., 2018) 0.889
BERT+RNN (baseline 1) 0.892 (¬±0.011)
BERT+RNN+Ontology (baseline 2) 0.893 (¬±0.013)
Slot-dependent SUMBT (baseline 3) 0.891 (¬±0.010)
Slot-independent SUMBT (proposed) 0.910 (¬±0.010)

Table 1: Joint goal accuracy on the evaluation dataset
of WOZ 2.0 corpus.

Model Joint Accuracy
Benchmark baseline 6 0.2583
GLAD (Zhong et al., 2018) 0.3557
GCE (Nouri and Hosseini-Asl, 2018) 0.3558
SUMBT 0.4240 (¬±0.0187)

Table 2: Joint goal accuracy on the evaluation dataset
of MultiWOZ corpus.

SUMBT, showed no significant performance dif-
ferences. On the other hand, the slot-independent
SUMBT which learned the shared information
from all across domains and slots significantly
outperformed those baselines, resulting in 91.0%
joint accuracy. This implies the importance of uti-
lizing common knowledge through a single model.

Table 2 shows the experimental results of the
slot-independent SUMBT model on MultiWOZ
corpus. Note that MultiWOZ has more domains
and slots to be learned than WOZ 2.0 corpus. The
SUMBT greatly surpassed the performances of
previous approaches by yielding 42.4% joint accu-
racy. The proposed model achieved state-of-the-
art performance in both WOZ 2.0 and MultiWOZ
datasets.

4.2 Attention Weights Analysis

Figure 2 shows an example of attention weights as
a dialog progresses. We can find that the model
attends to the part of utterances which are seman-
tically related to the given slots, even though the
slot-value labels are not expressed in the lexically
same way. For example, in case of ‚Äòprice range‚Äô
slot-type at the first turn, the slot-value label is
‚Äòmoderate‚Äô but the attention weights are relatively

6 The benchmark baseline is the model proposed
in Ramadan et al. (2018) and the performance is de-
scribed in http://dialogue.mi.eng.cam.ac.uk/
index.php/corpus/

https://github.com/huggingface/pytorch-pretrained-BERT
https://github.com/huggingface/pytorch-pretrained-BERT
http://dialogue.mi.eng.cam.ac.uk/index.php/corpus/
http://dialogue.mi.eng.cam.ac.uk/index.php/corpus/


5482

Turn 1 Turn 2 Turn 3

area price range
(none) (moderate)

are  price range
(none)     (moderate)

area price range
(don‚Äôt care) (moderate)

U: Hello, I‚Äôm looking for a restaurant, either 
Mediterranean or Indian, it must be reasonably 
priced though.

S: Sorry, we don‚Äôt have any matching restaurants.

U: How about Indian?

S: We have plenty of Indian restaurants. Is there 
a particular place you‚Äôd like to stay in?

U: I have no preference for the location, 
I just need an address and phone number.

Turn 1,

Turn 2,

Turn 3,

Dialog Example

1 2 3 4 1 2 3 4 1 2 3 4 1 2 3 4 1 2 3 4 1 2 3 4

Figure 2: Attention visualizations of the first three turns in a dialog (WOZ 2.0). At each turn, the first and second
columns are the attention weights when the given slots are ‚Äòarea‚Äô and ‚Äòprice range‚Äô, respectively. The slot-value
labels are denoted in the parentheses.

high on the phrase ‚Äòreasonably priced‚Äô. When ap-
propriate slot-values corresponding to the given
slot-type are absent (i.e., the label is ‚Äònone‚Äô), the
model attends to [CLS] or [SEP] tokens.

5 Conclusion

In this paper, we propose a new approach to uni-
versal and scalable belief tracker, called SUMBT
which attends to words in utterances that are rele-
vant to a given domain-slot-type. Besides, the con-
textual semantic encoders and the non-parametric
discriminator enable a single SUMBT to deal with
multiple domains and slot-types without increas-
ing model size. The proposed model achieved
the state-of-the-art joint accuracy performance in
WOZ 2.0 and MultiWOZ corpora. Furthermore,
we experimentally showed that sharing knowledge
by learning from multiple domain data helps to
improve performance. As future work, we plan
to explore whether SUMBT can continually learn
new knowledge when domain ontology is updated.

Acknowledgements

We would like to thank Jinyoung Yeo and anony-
mous reviewers for their constructive feedback
and helpful discussions. We are also grateful to
SK T-Brain Meta AI team for GPU cluster sup-
ports to conduct massive experiments.

References
Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E. Hin-

ton. 2016. Layer normalization. arXiv preprint,
arXiv:1607.06450.

Pawe≈Ç Budzianowski, Tsung Hsien Wen, Bo-Hsiang
Tseng, InÃÉigo Casanueva, Stefan Ultes, Osman Ra-
madan, and Milica GasÃåicÃÅ. 2018. MultiWOZ - a
large-scale multi-domain Wizard-of-Oz dataset for
task-oriented dialogue modelling. In Proceedings of
the 2018 Conference on Empirical Methods in Nat-
ural Language Processing, pages 5016‚Äì5026. Asso-
ciation for Computational Linguistics.

Danqi Chen, Adam Fisch, Jason Weston, and Antoine
Bordes. 2017. Reading Wikipedia to answer open-
domain questions. In Proceedings of the 55th An-
nual Meeting of the Association for Computational
Linguistics, pages 1870‚Äì1879. Association for Com-
putational Linguistics.

Yun-Nung Chen, Dilek Hakkani-TuÃàr, and Xiaodong
He. 2016. Zero-shot learning of intent embed-
dings for expansion by convolutional deep struc-
tured semantic models. In Proceedings of the
41st IEEE International Conference on Acoustics,
Speech and Signal Processing (ICASSP), pages
6045‚Äì6049. IEEE.

Jacob Devlin, Ming-Wei Chang, Kenton Lee, and
Kristina Toutanova. 2018. BERT: Pre-training of
deep bidirectional transformers for language under-
standing. arXiv preprint, arXiv:1810.04805.

Rahul Goel, Shachi Paul, Tagyoung Chung, Jeremie
Lecomte, Arindam Mandal, and Dilek Hakkani-Tur.
2018. Flexible and scalable state tracking frame-
work for goal-oriented dialogue systems. arXiv
preprint, arXiv:1811.12891.

https://arxiv.org/abs/1607.06450
http://aclweb.org/anthology/D18-1547
http://aclweb.org/anthology/D18-1547
http://aclweb.org/anthology/D18-1547
http://aclweb.org/anthology/P17-1171
http://aclweb.org/anthology/P17-1171
https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=7472838
https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=7472838
https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=7472838
https://arxiv.org/abs/1810.04805
https://arxiv.org/abs/1810.04805
https://arxiv.org/abs/1810.04805
https://arxiv.org/abs/1811.12891
https://arxiv.org/abs/1811.12891


5483

Matthew Henderson, Blaise Thomson, and Steve
Young. 2014. Word-based dialog state tracking with
recurrent neural networks. In Proceedings of the
SIGDIAL 2014 Conference, pages 292‚Äì299. Asso-
ciation for Computational Linguistics.

Young-Bum Kim, Dongchan Kim, Anjishnu Kumar,
and Ruhi Sarikaya. 2018. Efficient large-scale neu-
ral domain classification with personalized attention.
In Proceedings of the 56th Annual Meeting of the As-
sociation for Computational Linguistics (Long Pa-
pers), pages 2214‚Äì2224. Association for Computa-
tional Linguistics.

Nikola MrksÃåicÃÅ and Ivan VulicÃÅ. 2018. Fully statisti-
cal neural belief tracking. In Proceedings of the
56th Annual Meeting of the Association for Compu-
tational Linguistics (Short Papers), pages 108‚Äì113.
Association for Computational Linguistics.

Nikola MrksÃåicÃÅ, Diarmuid OÃÅ SeÃÅaghdha, Tsung Hsien
Wen, Blaise Thomson, and Steve Young. 2017.
Neural belief tracker: Data-driven dialogue state
tracking. In Proceedings of the 55th Annual Meet-
ing of the Association for Computational Linguis-
tics, pages 1777‚Äì1788. Association for Computa-
tional Linguistics.

Elnaz Nouri and Ehsan Hosseini-Asl. 2018. Toward
scalable neural dialogue state tracking model. arXiv
preprint, arXiv:1812.00899.

Osman Ramadan, Pawe≈Ç Budzianowski, and Milica
GasÃåicÃÅ. 2018. Large-scale multi-domain belief track-
ing with knowledge sharing. In Proceedings of the
56th Annual Meeting of the Association for Compu-
tational Linguistics (Short Papers), pages 432‚Äì437.
Association for Computational Linguistics.

Liliang Ren, Kaige Xie, Lu Chen, and Kai Yu. 2018.
Towards universal dialogue state tracking. In Pro-
ceedings of the 2018 Conference on Empirical Meth-
ods in Natural Language Processing, pages 2780‚Äì
2786. Association for Computational Linguistics.

Minjoon Seo, Aniruddha Kembhavi, Ali Farhadi, and
Hannaneh Hajishirzi. 2017. Bidirectional attention
flow for machine comprehension. In International
Conference on Learning Representations (ICLR).

Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob
Uszkoreit, Llion Jones, Aidan N Gomez, ≈Åukasz
Kaiser, and Illia Polosukhin. 2017. Attention is all
you need. In Advances in Neural Information Pro-
cessing Systems, pages 5998‚Äì6008.

Oriol Vinyals, Charles Blundell, Timothy Lillicrap,
Daan Wierstra, et al. 2016. Matching networks for
one shot learning. In Advances in neural informa-
tion processing systems, pages 3630‚Äì3638.

Tsung-Hsien Wen, David Vandyke, Nikola MrksÃåicÃÅ,
Milica Gasic, Lina M. Rojas Barahona, Pei-Hao Su,
Stefan Ultes, and Steve Young. 2017. A network-
based end-to-end trainable task-oriented dialogue

system. In Proceedings of the 15th Conference of
the European Chapter of the Association for Compu-
tational Linguistics: Volume 1, Long Papers, pages
438‚Äì449. Association for Computational Linguis-
tics.

Victor Zhong, Caiming Xiong, and Richard Socher.
2018. Global-locally self-attentive dialogue state
tracker. In Proceedings of the 56th Annual Meet-
ing of the Association for Computational Linguis-
tics (Long Papers), pages 1458‚Äì1467. Association
for Computational Linguistics.

https://www.aclweb.org/anthology/W/W14/W14-4340.pdf
https://www.aclweb.org/anthology/W/W14/W14-4340.pdf
http://www.aclweb.org/anthology/P18-1206
http://www.aclweb.org/anthology/P18-1206
http://www.aclweb.org/anthology/P18-2018
http://www.aclweb.org/anthology/P18-2018
http://aclweb.org/anthology/P17-1163
http://aclweb.org/anthology/P17-1163
https://arxiv.org/pdf/1812.00899.pdf
https://arxiv.org/pdf/1812.00899.pdf
http://www.aclweb.org/anthology/P18-2069
http://www.aclweb.org/anthology/P18-2069
http://aclweb.org/anthology/D18-1299
https://arxiv.org/abs/1611.01603
https://arxiv.org/abs/1611.01603
https://arxiv.org/abs/1706.03762
https://arxiv.org/abs/1706.03762
https://arxiv.org/abs/1606.04080
https://arxiv.org/abs/1606.04080
http://aclweb.org/anthology/E17-1042
http://aclweb.org/anthology/E17-1042
http://aclweb.org/anthology/E17-1042
http://aclweb.org/anthology/P18-1135
http://aclweb.org/anthology/P18-1135

