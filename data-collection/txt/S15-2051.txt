



















































SemEval-2015 Task 14: Analysis of Clinical Text


Proceedings of the 9th International Workshop on Semantic Evaluation (SemEval 2015), pages 303–310,
Denver, Colorado, June 4-5, 2015. c©2015 Association for Computational Linguistics

SemEval-2015 Task 14: Analysis of Clinical Text

Noémie Elhadad♣, Sameer Pradhan†, Sharon Lipsky Gorman♣,
Suresh Manandhar♦, Wendy Chapman♠, Guergana Savova†

♣ Columbia University, USA
† Boston Children’s Hospital, USA

♦ University of York, UK
♠ University of Utah, USA

noemie.elhadad@columbia.edu, guergana.savova@childrens.harvard.edu

Abstract

We describe two tasks—named entity recog-
nition (Task 1) and template slot filling (Task
2)—for clinical texts. The tasks leverage an-
notations from the ShARe corpus, which con-
sists of clinical notes with annotated men-
tions disorders, along with their normaliza-
tion to a medical terminology and eight addi-
tional attributes. The purpose of these tasks
was to identify advances in clinical named en-
tity recognition and establish the state of the
art in disorder template slot filling. Task 2
consisted of two subtasks: template slot fill-
ing given gold-standard disorder spans (Task
2a) and end-to-end disorder span identifica-
tion together with template slot filling (Task
2b). For Task 1 (disorder span detection and
normalization), 16 teams participated. The
best system yielded a strict F1-score of 75.7,
with a precision of 78.3 and recall of 73.2.
For Task 2a (template slot filling given gold-
standard disorder spans), six teams partici-
pated. The best system yielded a combined
overall weighted accuracy for slot filling of
88.6. For Task 2b (disorder recognition and
template slot filling), nine teams participated.
The best system yielded a combined relaxed F
(for span detection) and overall weighted ac-
curacy of 80.8.

1 Introduction

Patient records are abundant with reports, narratives,
discussions, and updates about patients. This un-
structured part of the record is dense with mentions
of clinical entities, such as conditions, anatomical
sites, medications, and procedures. Identifying the

different entities discussed in a patient record, their
status towards the patient, and how they relate to
each other is one of the core tasks of clinical natural
language processing. Indeed, with robust systems
to extract such mentions, along with their associated
attributes in the text (e.g., presence of negation for
a given entity mention), several high-level applica-
tions can be developed such as information extrac-
tion, question answering, and summarization.

In biomedicine, there are rich lexicons that can
be leveraged for the task of named entity recogni-
tion and entity linking or normalization. The Uni-
fied Medical Language System (UMLS) represents
over 130 lexicons/thesauri with terms from a va-
riety of languages. The UMLS Metathesaurus in-
tegrates standard resources such as SNOMED-CT,
ICD9, and RxNORM that are used worldwide in
clinical care, public health, and epidemiology. In
addition, the UMLS also provides a semantic net-
work in which every concept in the Metathesaurus is
represented by its Concept Unique Identifier (CUI)
and is semantically typed (Bodenreider and McCray,
2003).

The SemEval-2015 Task 14, Analysis of Clinical
Text is the newest iteration in a series of community
challenges organized around the tasks of named en-
tity recognition for clinical texts. In SemEval-2014
Task 7 (Pradhan et al., 2014) and previous challenge
2013 (Pradhan et al., 2013), we had focused on the
task of named entity recognition for disorder men-
tions in clinical texs, along with normalization to
UMLS CUIs. This year, we shift focus on the task
of identifying a series of attributes describing a dis-
order mention. Like for previous challenges, we use

303



the ShARe corpus1 and introduce a new set of anno-
tations for disorder attributes.

In the remainder of this paper, we describe the
dataset and the annotations provided to the task par-
ticipants, the subtasks comprising the overall task,
and the results of the teams that participated along
with notable approaches in their systems.

2 Dataset

Train Dev Test
Notes 298 133 100
Words 182K 153K 109K

Table 1: Notes, words, and disorder distributions in the
training, development, and testing sets.

The dataset used is the ShARe corpus (Pradhan et
al., 2015). As a whole, it consists of 531 deidentified
clinical notes (a mix of discharge summaries and ra-
diology reports) selected from the MIMIC II clinical
database version 2.5 (Saeed et al., 2002). Part of the
ShARe corpus was released as part of Semeval 2014
Task 7. In fact, to enable meaningful comparisons
of systems performance across years, the 2015 Se-
mEval training set combines the 2014 training and
development sets, while the 2015 SemEval devel-
opment set consists of the 2014 test set. The 2015
test set is a previously unseen set of clinical notes
from the ShARe corpus. Table 2 provides descrip-
tive statistics about the different sets. In addition
to the ShARe corpus annotations, task participants
were provided with a large set of unlabeled deiden-
tified clinical notes, also from MIMIC II (400,000+
notes).

The ShARe corpus contains gold-standard anno-
tations of disorder mentions and a set of attributes, as
described in Table 2. We refer to the nine attributes
as a disorder template. The annotation schema for
the template was derived from the established clini-
cal element model2. The complete guidelines for the
ShARe annotations are available on the ShARe web-
site3. Here, we provide a few examples to illustrate
what each attribute captures.

1share.healthnlp.org
2www.clinicalelement.com
3share.healthnlp.org

Train Dev
Disorder mentions 11,144 7,967
CUI=CUI-less 30% 24%
CUI 70% 76%
Unique CUIs 1,352 1,139
Negation = yes 19.6% 20.1%
Negation = no 80.4% 79.9%
Subject = patient 99.2% 98.4%
Subject = family member <1% 1.4%
Subject = other <1% <1%
Subject = donor other <1% 0%
Uncertainty = yes 8.9% 5.9%
Uncertainty = no 91.1% 94.1%
Course = changed <1% <1%
Course = resolved <1% <1%
Course = worsened < 1% <1%
Course = improved < 1% 1%
Course = decreased 1.6% <1%
Course = increased 2% 1.7%
Course = unmarked 94.1% 95.2%
Severity = slight 1.1% <1%
Severity = severe 3.5% 2.6%
Severity = moderate 5.9% 2.3%
Severity = unmarked 89.49% 94.2%
Conditional = true 4.9% 6.2%
Conditional = false 95.1% 93.8%
Generic = true <1% 1%
Generic = false 99.1 99%
Body Location = CUI 55.3% 44.7%
Body Location = null 44.4% 54.6%
Body Location = CUI-less <1% <1%
Unique BL CUIs 734 511

Table 3: Distribution of different attribute values in the
training and testing sets.

• In the statement “patient denies numbness,” the
disorder numbness has an associated negation at-
tribute set to “yes.”
• In the sentence “son has schizophrenia”, the dis-

order schizophrenia has a subject attribute set to
“family member.”
• The sentence “Evaluation of MI.” contains a dis-

order (MI) with the uncertainty attribute set to
“yes”.
• An example of disorder with a non-default course

attribute can be found in the sentence “The cough
got worse over the next two weeks.”, where its
value is “worsened.”
• The severity attribute is set to “slight” in “He has

slight bleeding.”

304



Slot Description Possible Values
CUI CUI; indicates normalized disorder CUI, CUI-less
NEG Negation; indicates whether disorder is negated no∗, yes
SUB Subject; indicates who experiences the disorder patient∗, null, other, family member,

donor family member, donor other
UNC Uncertainty; indicates presence of doubt about the disorder no∗, yes
COU Course; indicates progress or decline of the disorder unmarked∗, changed, increased, de-

creased, improved, worsened, resolved
SEV Severity; indicates how severe the disorder is unmarked∗, slight, moderate, severe
CND Conditional; indicates conditional existence of disorder un-

der specific circumstances
false∗, true

GEN Generic; indicates a generic mention of a disorder false∗, true
BL Body Location; represents normalized CUI of body loca-

tion(s) associated with disorder
null∗, CUI, CUI-less

Table 2: Disorder attributes and their possible values. Default values are indicated with an *.

• In the sentence “Pt should come back if any rash
occurs,” the disorder rash has a conditional at-
tribute with value “true.”
• In the sentence “Patient has a facial rash”, the

body location associated with the disorder “facial
rash” is “face” with CUI C0015450. Note that the
body location does not have to be a substring of
the disorder mention, even though in this example
it is.

The ShARe corpus was annotated following a rig-
orous process. Annotators were professional coders
who trained for the specific task of ShARe annota-
tions. The annotation process consisted of a double
annotation step followed by an adjudication phase.
For all annotations, in addition to all the values for
the attributes, their corresponding character spans in
the text were recorded and are available as part of
the ShARe annotations. Table 3 shows the distri-
bution of the different attributes in the training and
development sets.

3 Tasks

The Analysis of Clinical Text Task is split into two
tasks, one on named entity recognition, and one on
template slot filling for the named entities. Partici-
pants were able to submit to either or both tasks.

3.1 Task 1: Disorder Identification
For task 1, disorder identification, the goal is to rec-
ognize the span of a disorder mention in input clin-
ical text and to normalize the disorder to a unique
CUI in the UMLS/SNOMED-CT terminology. The

UMLS/SNOMED-CT terminology is defined as the
set of CUIs in the UMLS, but restricted to concepts
that are included in the SNOMED-CT terminology.

Participants were free to use any publicly avail-
able resources, such as UMLS, WordNet, and
Wikipedia, as well as the large corpus of un-
annotated clinical notes.

The following are examples of input/output for
Task 1.

1 In “The rhythm appears to be atrial fibrillation.”
the span “atrial fibrillation” is the gold-standard
disorder, and its normalization is CUI C0004238
(preferred term atrial fibrillation). This is a

2 In “The left atrium is moderately dilated.” the dis-
order span is discontiguous: “left atrium...dilated”
and its normalization is CUI C0344720 (preferred
term left atrial dilatation).

3 In “53 year old man s/p fall from ladder.” the
disorder is “fall from ladder” and is normalized
to C0337212 (preferred term accidental fall from
ladder).

Example 1 represents the easiest cases. Example
2 represents instances of disorders as listed in the
UMLS that are best mapped to discontiguous men-
tions. In Example 3, one has to infer that the
description is a synonym of the UMLS preferred
term. Finally, in some cases, a disorder mention
is present, but there is no good equivalent CUI in
UMLS/SNOMED-CT. The disorder is then normal-
ized to “CUI-less”.

305



3.2 Task 2: Disorder Slot Filling

This task focuses on identifying the normalized
value for the nine attributes described above: the
CUI of the disorder (very much like in Task 1), nega-
tion indicator, subject, uncertainty indicator, course,
severity, conditional, generic indicator, and body lo-
cation.

We describe Task 2 as a slot-filling task: given a
disorder mention (either provided by gold-standard
or identified automatically) in a clinical note, iden-
tify the normalized value of the nine slots. Note that
there are two aspects to slot filling: cues in the text
and normalized value. In this task, we focus on nor-
malized value and ignore cue detection.

To understand the state of the art for this new task,
we considered two subtasks. In both cases, given a
disorder span, participants are asked to identify the
nine attributes related to the disorder. In Task 2a,
the gold-standard disorder span(s) are provided as
input. In Task 2b, no gold-standard information is
provided; systems must recognize spans for disorder
mentions and fill in the value of the nine attributes.

4 Evaluation Metrics

4.1 Task 1 Evaluation Metrics

Evaluation for Task 1 is reported according to a F-
score, that captures both the disorder span recogni-
tion and the CUI normalization steps. We compute
two versions of the F-score:
• Strict F-score: a predicted mention is considered

a true positive if (i) the character span of the dis-
order is exactly the same as for the gold-standard
mention; and (ii) the predicted CUI is correct. The
predicted disorder is considered a false positive if
the span is incorrect or the CUI is incorrect.
• Relaxed F-score: a predicted mention is a true

positive if (i) there is any word overlap between
the predicted mention span and the gold-standard
span (both in the case of contiguous and discon-
tiguous spans); and (ii) the predicted CUI is cor-
rect. The predicted mention is a false positive if
the span shares no words with the gold-standard
span or the CUI is incorrect.
Thus, given, Dtp, the number of true positives

disorder mentions, Dfp, the number of false posi-
tive disorder mentions, and Dfn, the number of false

negative disorder mentions

Precision = P =
Dtp

Dtp + Dfp
(1)

Recall = R =
Dtp

Dtp + Dfn
(2)

F =
2× P ×R

P + R
(3)

4.2 Task 2 Evaluation Metrics
We introduce a variety of evaluation metrics, which
capture different aspects of the task of disorder tem-
plate slot filling. Overall, for Task 2a, we reported
average unweighted accuracy, weighted accuracy,
and per-slot weighted accuracy for each of the nine
slots. For Task 2b, we report the same metrics, and
in addition report relaxed F for span identification.

We now describe per-disorder evaluation met-
rics, and then describe the overall evaluation metrics
which provide aggregated system assessment. Given
the K slots (s1, ..., sK) to fill (in our task the nine
different slots), each slot sk has nk possible normal-
ized values (sik)i ∈ 1..nk. For a given disorder, its
gold-standard value for slot sk is denoted gsk, and
its predicted value is denoted psk.

4.2.1 Per-Disorder Evaluation Metrics
Per-disorder unweighted accuracy The un-
weighted accuracy represents the ability of a system
to identify all the slot values for a given disorder.
The per-disorder unweighted accuracy is simply
defined as: ∑K

k=1 I(gsk, psk)
K

where I is the identity function: I(x, y) = 1 if x = y
and 0 otherwise.

Per-disorder weighted accuracy The weighted
per-disorder accuracy takes into account the preva-
lence of different values for each of the slots. This
metric captures how good a system is at identifying
rare values of different slots. The weights are thus
defined as follows:
• The CUI slot’s weight is set to 1, for all CUI val-

ues.
• The body location slot’s weight is defined as

weight(NULL) = 1-prevalence(NULL), and the
weight for any non-NULL value (including CUI-
less) is set to weight(CUI) = 1-prevalence(body
location with a non-NULL value).

306



• For each other slot sk, we define nk weights
weight(sik) (one for each of its possible normal-
ized values) as follows:

∀i ∈ 1..nk, weight(sik) = 1− prevalence(sik)

where prevalence(sik) is the prevalence of value
sik in the overall corpus(training, development, and
testing sets). The weights are such that highly preva-
lent values have smaller weights and rare values
have bigger weight.

Thus, weighted per-disorder accuracy is defined
as ∑K

k=1 weight(gsk) ∗ I(gsk, psk)∑K
k=1 weight(gsk)

(4)

where, like above, gsk is the gold-standard value of
slot sk and psk is the predicted value of slot sk, and
I is the identity function: I(x, y) = 1 if x = y and 0
otherwise.

4.2.2 Overall Evaluation Metrics
Weighted and Unweighted Accuracy. Armed
with the per-disorder unweighted and weighted ac-
curacy scores, we can compute an average across all
true-positive disorders. For task 2a, the disorders are
provided, so they are all true positive, but for task 2b,
it is important to note that we only consider the true-
positive disorders to compute the overall accuracy.

Accuracy =
∑#tp

i=1 per disorder acc(tpi)
#tp

(5)

Per-Slot Accuracy. Per-slot accuracy are useful in
assessing the ability of a system to fill in a particu-
lar slot. For each slot, an average per-slot accuracy
is defined as the accuracy for each true-positive dis-
order to recognize the value for that particular slot
across the true-positive spans. Thus, for slot sk, the
per-slot accuracy is:

∑#tp
i=1 weight(gsi,k) ∗ I(gsi,k, psi,k)∑#tp

i=1 weight(gsi,k)
(6)

where for each true-positive span there is a gold-
standard value gsi,k and a predicted value psi,k for
slot sk.

Figure 1: Task 1 results.

Disorder Span Identification. This overall met-
ric is only meaningful for Task 2b, where the sys-
tem has to identify disorders prior to filling in their
templates. Like in Task 1, we report an F-score met-
ric to assess how good the system is at identifying
disorder span. Note that unlike in Task 1, this F
score does not consider CUI normalization, as this
is captured through the accuracy in the template fill-
ing task. Thus, a true disorder span is defined as
any overalp with a gold-stand disorder span. In the
case of several predicted spans that overlap with a
gold-standard span, then only one of them is chosen
to be true positive (the longest ones), and the other
predicted spans are considered false positives.

5 Results

5.1 Task 1

16 teams participated in Task 1. Strict and relaxed
precision, recall, and F metrics are reported in Fig-
ure 1. We relied on the strict F to rank different sub-
missions. The best system from team ezDI reported

307



75.7 strict F, also reporting the highest relaxed F
(78.7) (Pathak et al., 2015).

For disorder span recognition, most teams used
a CRF-based approach. Features explored included
traditional NER features: lexical (bag of words
and bigrams, orthographic features), syntactic fea-
tures derived from either part-of-speech and phrase
chunking information or dependency parsing, and
domain features (note type and section headers of
clinical note). Lookup to dictionary (either UMLS
or customized lexicon of disorders) was an essential
feature for performance. To leverage further these
lexicons, for instance, Xu and colleagues (Xu et al.,
2015) implemented a vector-space model similar-
ity computation to known disorders as an additional
feature in their appraoch.

The best-performing teams made use of the large
unannotated corpus of clinical notes provided in the
challenge (Pathak et al., 2015; Leal et al., 2015; Xu
et al., 2015). Teams explored the use of Brown clus-
ters (Brown et al., 1992) and word embeddings (Col-
lobert et al., 2011). Pathak and colleagues (Pathak et
al., 2015) note that word2vec (Mikolov et al., 2013)
did not yield satisfactory results. Instead, they report
better results clustering sentences in the unannotated
texts based on their sequence of part-of-speech tags,
and using the clusters as feature in the CRF.

Teams continued to explore approaches for rec-
ognizing discontiguous entities. Pathak and col-
leagues (Pathak et al., 2015), for instance, built a
specialized SVM-based classifier for that purpose.

For CUI normalization, the best performing teams
focused on augmenting existing dictionaries with
lists of unambiguous abbreviations (Leal et al.,
2015) or by pre-processing UMLS and breaking
down existing lexical variants to account for high
paraphrasing power of disorder terms (Pathak et al.,
2015).

5.2 Task 2
Six teams participated in Task 2a. Evaluation met-
rics are reported in Figure 2. We relied on the
Weighted Accuracy (WA) to rank the teams (high-
lighted in the Figure is F*WA, but since in Task
2a gold-standard disorders are provided, F is 1).
The best system (team UTH-CCB) yielded a WA of
88.6 (Xu et al., 2015).

For Task 2b, nine teams participated. Evaluation

metrics are reported in Figure 3. We relied on the
combination of F score for disorder span identifica-
tion and Weighted Accuracy for template filling to
rank the teams (F*WA in the figure). The best sys-
tem (team UTH-CCB) yielded a F*WA of 80.8.

Approaches to template filling focused on build-
ing classifiers for each attribute. Specialized lex-
icons of trigger terms for each attribute (e.g., list
of negation terms) along with distance to disorder
spans was a helpful feature. Overall, like in Task
1, a range of feature types from lexical to syntactic
proved useful in the template filling task.

The per-slot accuracies (columns BL, CUI, CND,
COU, GEN, NEG, SEV, SUB, and UNC in Figures 2
and 3) indicate that overall some attributes are eas-
ier to recognize than others. Body Location, per-
haps not surprisingly, was the most difficult after
CUI normalization, in part because it also requires
a normalization to an anatomical site.

6 Conclusion

In this task, we introduced a new version of the
ShARe corpus, with annotations of disorders and a
wide set of disorder attributes. The biggest improve-
ments in the task of disorder recognition (both span
identification and CUI normalization) come from
leveraging large amounts of unannotated texts and
using word embeddings as additional feature in the
task. The detection of discontiguous disorder seems
to still be an open challenge for the community,
however.

The new task of template filling (identifying nine
attributes for a given disorder) was met with enthu-
siasm by the participating teams. We introduced a
variety of evaluation metrics to capture the differ-
ent aspects of the task. Different approaches show
that while some attributes are harder to identify than
other, overall the best performing teams achieved
excellent results.

Acknowledgments

This work was supported by the Shared Annotated
Resources (ShARe) project NIH R01 GM090187.
We greatly appreciate the hard work of our program
committee members and the ShARe annotators.

308



Figure 2: Task 2a results.

References

Olivier Bodenreider and Alexa T McCray. 2003. Explor-
ing semantic groups through visual approaches. Jour-
nal of biomedical informatics, 36(6):414–432.

Peter F Brown, Peter V Desouza, Robert L Mercer, Vin-
cent J Della Pietra, and Jenifer C Lai. 1992. Class-
based n-gram models of natural language. Computa-
tional linguistics, 18(4):467–479.

Ronan Collobert, Jason Weston, Léon Bottou, Michael
Karlen, Koray Kavukcuoglu, and Pavel Kuksa. 2011.
Natural language processing (almost) from scratch.
The Journal of Machine Learning Research, 12:2493–
2537.

André Leal, Bruno Martins, and Francisco Couto. 2015.
ULisboa: Semeval 2015 - task 14 analysis of clinical
text: Recognition and normalization of medical con-
cepts. In Proceedings of SemEval-2015.

Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Cor-
rado, and Jeff Dean. 2013. Distributed representations
of words and phrases and their compositionality. In
Advances in Neural Information Processing Systems,
pages 3111–3119.

Parth Pathak, Pinal Patel, Vishal Panchal, Sagar Soni,
Kinjal Dani, Narayan Choudhary, and Amrish Patel.
2015. ezDI: A semi-supervised nlp system for clinical
narrative analysis. In Proceedings of SemEval-2015.

Sameer Pradhan, Noemie Elhadad, Brett R South, David
Martinez, Lee Christensen, Amy Vogel, Hanna Suomi-
nen, Wendy Chapman, and Guergana Savova. 2013.
Task 1: Share/clef ehealth evaluation lab 2013. In On-
line Working Notes of CLEF, page 230.

Sameer Pradhan, Noémie Elhadad, Wendy Chapman,
Suresh Manandhar, and Guergana Savova. 2014.

Semeval-2014 task 7: Analysis of clinical text. In Pro-
ceedings of the 8th International Workshop on Seman-
tic Evaluation (SemEval 2014), pages 54–62.

Sameer Pradhan, Noémie Elhadad, Brett R South, David
Martinez, Lee Christensen, Amy Vogel, Hanna Suomi-
nen, Wendy W Chapman, and Guergana Savova.
2015. Evaluating the state of the art in disorder
recognition and normalization of the clinical narrative.
Journal of the American Medical Informatics Associa-
tion, 22(1):143–154.

Mohammed Saeed, C Lieu, G Raber, and RG Mark.
2002. Mimic II: a massive temporal ICU patient
database to support research in intelligent patient mon-
itoring. In Computers in Cardiology, 2002, pages
641–644. IEEE.

Jun Xu, Yaoyun Zhang, Jingqi Wang, Yonghui Wu, Min
Jian, Ergin Soysal, and Hua Xu. 2015. UTH-CCB:
The participation of the SemEval 2015 challenge - task
14. In Proceedings of SemEval-2015.

309



Figure 3: Task 2b results.

310


