



















































Approximation Strategies for Multi-Structure Sentence Compression


Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 1241–1251,
Baltimore, Maryland, USA, June 23-25 2014. c©2014 Association for Computational Linguistics

Approximation Strategies for Multi-Structure Sentence Compression

Kapil Thadani
Department of Computer Science

Columbia University
New York, NY 10025, USA

kapil@cs.columbia.edu

Abstract

Sentence compression has been shown to
benefit from joint inference involving both
n-gram and dependency-factored objec-
tives but this typically requires expensive
integer programming. We explore instead
the use of Lagrangian relaxation to decou-
ple the two subproblems and solve them
separately. While dynamic programming
is viable for bigram-based sentence com-
pression, finding optimal compressed trees
within graphs is NP-hard. We recover ap-
proximate solutions to this problem us-
ing LP relaxation and maximum spanning
tree algorithms, yielding techniques that
can be combined with the efficient bigram-
based inference approach using Lagrange
multipliers. Experiments show that these
approximation strategies produce results
comparable to a state-of-the-art integer
linear programming formulation for the
same joint inference task along with a sig-
nificant improvement in runtime.

1 Introduction

Sentence compression is a text-to-text genera-
tion task in which an input sentence must be
transformed into a shorter output sentence which
accurately reflects the meaning in the input
and also remains grammatically well-formed.
The compression task has received increasing
attention in recent years, in part due to the
availability of datasets such as the Ziff-Davis cor-
pus (Knight and Marcu, 2000) and the Edinburgh
compression corpora (Clarke and Lapata, 2006),
from which the following example is drawn.

Original: In 1967 Chapman, who had cultivated a con-
ventional image with his ubiquitous tweed jacket and pipe,
by his own later admission stunned a party attended by his
friends and future Python colleagues by coming out as a
homosexual.

Compressed: In 1967 Chapman, who had cultivated a
conventional image, stunned a party by coming out as a
homosexual.

Following an assumption often used in compres-
sion systems, the compressed output in this corpus
is constructed by dropping tokens from the input
sentence without any paraphrasing or reordering.1

A number of diverse approaches have been
proposed for deletion-based sentence compres-
sion, including techniques that assemble the out-
put text under an n-gram factorization over the
input text (McDonald, 2006; Clarke and Lapata,
2008) or an arc factorization over input depen-
dency parses (Filippova and Strube, 2008; Galanis
and Androutsopoulos, 2010; Filippova and Altun,
2013). Joint methods have also been proposed that
invoke integer linear programming (ILP) formu-
lations to simultaneously consider multiple struc-
tural inference problems—both over n-grams and
input dependencies (Martins and Smith, 2009) or
n-grams and all possible dependencies (Thadani
and McKeown, 2013). However, it is well-
established that the utility of ILP for optimal infer-
ence in structured problems is often outweighed
by the worst-case performance of ILP solvers
on large problems without unique integral solu-
tions. Furthermore, approximate solutions can
often be adequate for real-world generation sys-
tems, particularly in the presence of linguistically-
motivated constraints such as those described by
Clarke and Lapata (2008), or domain-specific

1This is referred to as extractive compression by Cohn and
Lapata (2008) & Galanis and Androutsopoulos (2010) fol-
lowing the terminology used in document summarization.

1241



pruning strategies such as the use of sentence tem-
plates to constrain the output.

In this work, we develop approximate inference
strategies to the joint approach of Thadani and
McKeown (2013) which trade the optimality guar-
antees of exact ILP for faster inference by sep-
arately solving the n-gram and dependency sub-
problems and using Lagrange multipliers to en-
force consistency between their solutions. How-
ever, while the former problem can be solved
efficiently using the dynamic programming ap-
proach of McDonald (2006), there are no efficient
algorithms to recover maximum weighted non-
projective subtrees in a general directed graph.
Maximum spanning tree algorithms, commonly
used in non-projective dependency parsing (Mc-
Donald et al., 2005), are not easily adaptable to
this task since the maximum-weight subtree is not
necessarily a part of the maximum spanning tree.

We therefore consider methods to recover ap-
proximate solutions for the subproblem of finding
the maximum weighted subtree in a graph, com-
mon among which is the use of a linear program-
ming relaxation. This linear program (LP) ap-
pears empirically tight for compression problems
and our experiments indicate that simply using the
non-integral solutions of this LP in Lagrangian re-
laxation can empirically lead to reasonable com-
pressions. In addition, we can recover approxi-
mate solutions to this problem by using the Chu-
Liu Edmonds algorithm for recovering maximum
spanning trees (Chu and Liu, 1965; Edmonds,
1967) over the relatively sparse subgraph defined
by a solution to the relaxed LP. Our proposed ap-
proximation strategies are evaluated using auto-
mated metrics in order to address the question: un-
der what conditions should a real-world sentence
compression system implementation consider ex-
act inference with an ILP or approximate infer-
ence? The contributions of this work include:

• An empirically-useful technique for approx-
imating the maximum-weight subtree in a
weighted graph using LP-relaxed inference.
• Multiple approaches to generate good ap-

proximate solutions for joint multi-structure
compression, based on Lagrangian relaxation
to enforce equality between the sequential
and syntactic inference subproblems.
• An analysis of the tradeoffs incurred by joint

approaches with regard to runtime as well as
performance under automated measures.

2 Multi-Structure Sentence Compression

Even though compression is typically formulated
as a token deletion task, it is evident that drop-
ping tokens independently from an input sentence
will likely not result in fluent and meaningful com-
pressive text. Tokens in well-formed sentences
participate in a number of syntactic and seman-
tic relationships with other tokens, so one might
expect that accounting for heterogenous structural
relationships between tokens will improve the co-
herence of the output sentence. Furthermore,
much recent work has focused on the challenge
of joint sentence extraction and compression, also
known as compressive summarization (Martins
and Smith, 2009; Berg-Kirkpatrick et al., 2011;
Almeida and Martins, 2013; Li et al., 2013; Qian
and Liu, 2013), in which questions of efficiency
are paramount due to the larger problems in-
volved; however, these approaches largely restrict
compression to pruning parse trees, thereby im-
posing a dependency on parser performance. We
focus in this work on a sentence-level compression
system to approximate the ILP-based inference of
Thadani and McKeown (2013) which does not re-
strict compressions to follow input parses but per-
mits the generation of novel dependency relations
in output compressions.

The rest of this section is organized as fol-
lows: §2.1 provies an overview of the joint se-
quential and syntactic objective for compression
from Thadani and McKeown (2013) while §2.2
discusses the use of Lagrange multipliers to en-
force consistency between the different structures
considered. Following this, §2.3 discusses a dy-
namic program to find maximum weight bigram
subsequences from the input sentence, while §2.4
covers LP relaxation-based approaches for ap-
proximating solutions to the problem of finding a
maximum-weight subtree in a graph of potential
output dependencies. Finally, §2.5 discusses the
features and model training approach used in our
experimental results which are presented in §3.

2.1 Joint objective

We begin with some notation. For an input sen-
tence S comprised of n tokens including dupli-
cates, we denote the set of tokens in S by T ,
{ti : 1 ≤ i ≤ n}. Let C represent a compres-
sion of S and let xi ∈ {0, 1} denote an indicator
variable whose value corresponds to whether to-
ken ti ∈ T is present in the compressed sentence

1242



C. In addition, we define bigram indicator vari-
ables yij ∈ {0, 1} to represent whether a particular
order-preserving bigram2 〈ti, tj〉 from S is present
as a contiguous bigram inC as well as dependency
indicator variables zij ∈ {0, 1} corresponding to
whether the dependency arc ti → tj is present in
the dependency parse of C. The score for a given
compression C can now be defined to factor over
its tokens, n-grams and dependencies as follows.

score(C) =
∑
ti∈T

xi · θtok(ti)

+
∑

ti∈T∪{START},
tj∈T∪{END}

yij · θbgr(〈ti, tj〉)

+
∑

ti∈T∪{ROOT},
tj∈T

zij · θdep(ti → tj) (1)

where θtok, θbgr and θdep are feature-based scoring
functions for tokens, bigrams and dependencies
respectively. Specifically, each θv(·) ≡ w>v φv(·)
where φv(·) is a feature map for a given vari-
able type v ∈ {tok, bgr, dep} and wv is the cor-
responding vector of learned parameters.

The inference task involves recovering the high-
est scoring compression C∗ under a particular set
of model parameters w.

C∗ = arg max
C

score(C)

= arg max
x,y,z

x>θtok + y>θbgr + z>θdep (2)

where the incidence vector x , 〈xi〉ti∈T repre-
sents an entire token configuration over T , with y
and z defined analogously to represent configura-
tions of bigrams and dependencies. θv , 〈θv(·)〉
denotes a corresponding vector of scores for each
variable type v under the current model parame-
ters. In order to recover meaningful compressions
by optimizing (2), the inference step must ensure:

1. The configurations x, y and z are consistent
with each other, i.e., all configurations cover
the same tokens.

2. The structural configurations y and z are
non-degenerate, i.e, the bigram configuration
y represents an acyclic path while the depen-
dency configuration z forms a tree.

2Although Thadani and McKeown (2013) is not restricted
to bigrams or order-preserving n-grams, we limit our discus-
sion to this scenario as it also fits the assumptions of McDon-
ald (2006) and the datasets of Clarke and Lapata (2006).

These requirements naturally rule out simple ap-
proximate inference formulations such as search-
based approaches for the joint objective.3 An
ILP-based inference solution is demonstrated in
Thadani and McKeown (2013) that makes use of
linear constraints over the boolean variables xi, yij
and zij to guarantee consistency, as well as aux-
iliary real-valued variables and constraints repre-
senting the flow of commodities (Magnanti and
Wolsey, 1994) in order to establish structure in y
and z. In the following section, we propose an al-
ternative formulation that exploits the modularity
of this joint objective.

2.2 Lagrangian relaxation

Dual decomposition (Komodakis et al., 2007) and
Lagrangian relaxation in general are often used
for solving joint inference problems which are
decomposable into individual subproblems linked
by equality constraints (Koo et al., 2010; Rush
et al., 2010; Rush and Collins, 2011; DeNero
and Macherey, 2011; Martins et al., 2011; Das
et al., 2012; Almeida and Martins, 2013). This
approach permits sub-problems to be solved sepa-
rately using problem-specific efficient algorithms,
while consistency over the structures produced is
enforced through Lagrange multipliers via itera-
tive optimization. Exact solutions are guaranteed
when the algorithm converges on a consistent pri-
mal solution, although this convergence itself is
not guaranteed and depends on the tightness of
the underlying LP relaxation. The primary advan-
tage of this technique is the ability to leverage the
underlying structure of the problems in inference
rather than relying on a generic ILP formulation
while still often producing exact solutions.

The multi-structure inference problem de-
scribed in the previous section seems in many
ways to be a natural fit to such an approach since
output scores factor over different types of struc-
ture that comprise the output compression. Even if
ILP-based approaches perform reasonably at the
scale of single-sentence compression problems,
the exponential worst-case complexity of general-
purpose ILPs will inevitably pose challenges when
scaling up to (a) handle larger inputs, (b) use
higher-order structural fragments, or (c) incorpo-
rate additional models.

3This work follows Thadani and McKeown (2013) in re-
covering non-projective trees for inference. However, recov-
ering projective trees is tractable when a total ordering of out-
put tokens is assumed. This will be addressed in future work.

1243



Consider once more the optimization problem
characterized by (2) The two structural problems
that need to be solved in this formulation are
the extraction of a maximum-weight acyclic sub-
sequence of bigrams y from the lattice of all
order-preserving bigrams from S and the recov-
ery of a maximum-weight directed subtree z. Let
α(y) ∈ {0, 1}n denote the incidence vector of
tokens contained in the n-gram sequence y and
β(z) ∈ {0, 1}n denote the incidence vector of
words contained in the dependency tree z. We can
now rewrite the objective in (2) while enforcing
the constraint that the words contained in the se-
quence y are the same as the words contained in
the tree z, i.e., α(y) = β(z), by introducing a
vector of Lagrange multipliers λ ∈ Rn. In addi-
tion, the token configuration x can be rewritten in
the form of a weighted combination of α(y) and
β(z) to ensure its consistency with y and z. This
results in the following Lagrangian:

L(λ,y, z) = y>θbgr + z>θdep
+ θ>tok (ψ ·α(y) + (1− ψ) · β(z))
+ λ> (α(y)− β(z)) (3)

Finding the y and z that maximize this Lagrangian
above yields a dual objective, and the dual prob-
lem corresponding to the primal objective speci-
fied in (2) is therefore the minimization of this ob-
jective over the Lagrange multipliers λ.

min
λ

max
y,z

L(λ,y, z)

= min
λ

max
y

y>θbgr + (λ + ψ · θtok)>α(y)

+ max
z

z>θdep − (λ + (ψ − 1) · θtok)> β(z)
= min

λ
max

y
f(y,λ, ψ,θ)

+ max
z
g(z,λ, ψ,θ) (4)

This can now be solved with the iterative subgra-
dient algorithm illustrated in Algorithm 1. In each
iteration i, the algorithm solves for y(i) and z(i)

under λ(i), then generates λ(i+1) to penalize in-
consistencies between α(y(i)) and β(z(i)). When
α(y(i)) = β(z(i)), the resulting primal solution is
exact, i.e., y(i) and z(i) represent the optimal struc-
tures under (2). Otherwise, if the algorithm starts
oscillating between a few primal solutions, the un-
derlying LP must have a non-integral solution in
which case approximation heuristics can be em-

Algorithm 1 Subgradient-based joint inference
Input: scores θ, ratio ψ, repetition limit lmax,
iteration limit imax, learning rate schedule η
Output: token configuration x

1: λ(0) ← 〈0〉n
2: M ← ∅,Mrepeats ← ∅
3: for iteration i < imax do
4: ŷ← arg maxy f(y,λ, ψ,θ)
5: ẑ ← arg maxz g(z,λ, ψ,θ)
6: if α(ŷ) = β(ẑ) then return α(ŷ)
7: if α(ŷ) ∈M then
8: Mrepeats ←Mrepeats ∪ {α(ŷ)}
9: if β(ẑ) ∈M then

10: Mrepeats ←Mrepeats ∪ {β(ẑ)}
11: if |Mrepeats| ≥ lmax then break
12: M ←M ∪ {α(ŷ),β(ẑ)}
13: λ(i+1) ← λ(i) − ηi (α(ŷ)− β(ẑ))

return arg maxx∈Mrepeats score(x)

ployed.4 The application of this Lagrangian relax-
ation strategy is contingent upon the existence of
algorithms to solve the maximization subproblems
for f(y,λ, ψ,θ) and g(z,λ, ψ,θ). The following
sections discuss our approach to these problems.

2.3 Bigram subsequences
McDonald (2006) provides a Viterbi-like dynamic
programming algorithm to recover the highest-
scoring sequence of order-preserving bigrams
from a lattice, either in unconstrained form or with
a specific length constraint. The latter requires a
dynamic programming table Q[i][r] which repre-
sents the best score for a compression of length r
ending at token i. The table can be populated us-
ing the following recurrence:

Q[i][1] = score(S, START, i)
Q[i][r] = max

j<i
Q[j][r − 1] + score(S, i, j)

Q[i][R+ 1] = Q[i][R] + score(S, i, END)

where R is the required number of output tokens
and the scoring function is defined as

score(S, i, j) , θbgr(〈ti, tj〉) + λj + ψ · θtok(tj)
so as to solve f(y,λ, ψ,θ) from (4). This ap-
proach requires O(n2R) time in order to identify

4Heuristic approaches (Komodakis et al., 2007; Rush et
al., 2010), tightening (Rush and Collins, 2011) or branch and
bound (Das et al., 2012) can still be used to retrieve optimal
solutions, but we did not explore these strategies here.

1244



A

B

C D

-20

3
10 2

1

Figure 1: An example of the difficulty of recover-
ing the maximum-weight subtree (B→C, B→D)
from the maximum spanning tree (A→C, C→B,
B→D).
the highest scoring sequence y and corresponding
token configuration α(y).

2.4 Dependency subtrees
The maximum-weight non-projective subtree
problem over general graphs is not as easily
solved. Although the maximum spanning tree for
a given token configuration can be recovered ef-
ficiently, Figure 1 illustrates that the maximum-
scoring subtree is not necessarily found within
it. The problem of recovering a maximum-weight
subtree in a graph has been shown to be NP-hard
even with uniform edge weights (Lau et al., 2006).

In order to produce a solution to this subprob-
lem, we use an LP relaxation of the relevant
portion of the ILP from Thadani and McKeown
(2013) by omitting integer constraints over the to-
ken and dependency variables in x and z respec-
tively. For simplicity, however, we describe the
ILP version rather than the relaxed LP in order to
motivate the constraints with their intended pur-
pose rather than their effect in the relaxed prob-
lem. The objective for this LP is given by

max
x,z

x>θ′tok + z>θdep (5)

where the vector of token scores is redefined as

θ′tok , (1− ψ) · θtok − λ (6)
in order to solve g(z,λ, ψ,θ) from (4).

Linear constraints are introduced to produce de-
pendency structures that are close to the optimal
dependency trees. First, tokens in the solution
must only be active if they have a single active in-
coming dependency edge. In addition, to avoid
producing multiple disconnected subtrees, only
one dependency is permitted to attach to the ROOT
pseudo-token.

xj −
∑

i

zij = 0, ∀tj ∈ T (7)∑
j

zij = 1, if ti = ROOT (8)

ROOT

Production was closed down at Ford last night .

5
γ3,1 = 1 2 1

γ3,9 = 1

Figure 2: An illustration of commodity values for
a valid solution of the non-relaxed ILP.

In order to avoid cycles in the dependency tree,
we include additional variables to establish single-
commodity flow (Magnanti and Wolsey, 1994) be-
tween all pairs of tokens. These γij variables carry
non-negative real values which must be consumed
by active tokens that they are incident to.

γij ≥ 0, ∀ti, tj ∈ T (9)∑
i

γij −
∑

k

γjk = xj , ∀tj ∈ T (10)

These constraints ensure that cyclic structures are
not possible in the non-relaxed ILP. In addition,
they serve to establish connectivity for the de-
pendency structure z since commodity can only
originate in one location—at the pseudo-token
ROOT which has no incoming commodity vari-
ables. However, in order to enforce these prop-
erties on the output dependency structure, this
acyclic, connected commodity structure must con-
strain the activation of the z variables.

γij − Cmaxzij ≤ 0, ∀ti, tj ∈ T (11)

where Cmax is an arbitrary upper bound on the
value of γij variables. Figure 2 illustrates how
these commodity flow variables constrain the out-
put of the ILP to be a tree. However, the effect
of these constraints is diminished when solving an
LP relaxation of the above problem.

In the LP relaxation, xi and zij are redefined as
real-valued variables in [0, 1], potentially resulting
in fractional values for dependency and token indi-
cators. As a result, the commodity flow network is
able to establish connectivity but cannot enforce a
tree structure, for instance, directed acyclic struc-
tures are possible and token indicators xi may be
partially be assigned to the solution structure. This
poses a challenge in implementing β(z) which is
needed to recover a token configuration from the
solution of this subproblem.

We propose two alternative solutions to address
this issue in the context of the joint inference strat-
egy. The first is to simply use the relaxed token
configuration identified by the LP in Algorithm 1,

1245



i.e., to set β(z̃) = x̃ where x̃ and z̃ represent the
real-valued counterparts of the incidence vectors x
and z. The viability of this approximation strategy
is due to the following:

• The relaxed LP is empirically fairly tight,
yielding integral solutions 89% of the time on
the compression datasets described in §3.
• The bigram subproblem is guaranteed to re-

turn a well-formed integral solution which
obeys the imposed compression rate, so we
are assured of a source of valid—if non-
optimal—solutions in line 13 of Algorithm 1.

We also consider another strategy that attempts to
approximate a valid integral solution to the depen-
dency subproblem. In order to do this, we first
include an additional constraint in the relaxed LP
which restrict the number of tokens in the output
to a specific number of tokens R that is given by
an input compression rate.∑

i

xi = R (12)

The addition of this constraint to the relaxed LP
reduces the rate of integral solutions drastically—
from 89% to approximately 33%—but it serves to
ensure that the resulting token configuration x̃ has
at least as many non-zero elements asR, i.e., there
are at least as many tokens activated in the LP so-
lution as are required in a valid solution.

We then construct a subgraph G(z̃) consisting
of all dependency edges that were assigned non-
zero values in the solution, assigning to each edge
a score equal to the score of that edge in the LP as
well as the score of its dependent word, i.e., each
zij in G(z̃) is assigned a score of θdep(〈ti, tj〉) −
λj + (1− ψ) · θtok(tj). Since the commodity flow
constraints in (9)–(11) ensure a connected z̃, it is
therefore possible to recover a maximum-weight
spanning tree from G(z̃) using the Chu-Liu Ed-
monds algorithm (Chu and Liu, 1965; Edmonds,
1967).5 Although the runtime of this algorithm
is cubic in the size of the input graph, it is fairly
speedy when applied on relatively sparse graphs
such as the solutions to the LP described above.
The resulting spanning tree is a useful integral
approximation of z̃ but, as mentioned previously,
may contain more nodes than R due to fractional
values in x̃; we therefore repeatedly prune leaves

5A detailed description of the Chu-Liu Edmonds algo-
rithm for MSTs is available in McDonald et al. (2005).

with the lowest incoming edge weight in the cur-
rent tree until exactly R nodes remain. The result-
ing tree is assumed to be a reasonable approxima-
tion of the optimal integral solution to this LP.

The Chu-Liu Edmonds algorithm is also em-
ployed for another purpose: when the underly-
ing LP for the joint inference problem is not
tight—a frequent occurrence in our compression
experiments—Algorithm 1 will not converge on
a single primal solution and will instead oscillate
between solutions that are close to the dual opti-
mum. We identify this phenomenon by counting
repeated solutions and, if they exceed some thresh-
old lmax with at least one repeated solution from
either subproblem, we terminate the update proce-
dure for Lagrange multipliers and instead attempt
to identify a good solution from the repeating ones
by scoring them under (2). It is straightforward to
recover and score a bigram configuration y from a
token configuration β(z). However, scoring so-
lutions produced by the dynamic program from
§2.3 also requires the score over a corresponding
parse tree; this can be recovered by constructing
a dependency subgraph containing across only the
tokens that are active in α(y) and retrieving the
maximum spanning tree for that subgraph using
the Chu-Liu Edmonds algorithm.

2.5 Learning and Features

The features used in this work are largely based on
the features from Thadani and McKeown (2013).

• φtok contains features for part-of-speech
(POS) tag sequences of length up to 3 around
the token, features for the dependency label
of the token conjoined with its POS, lexical
features for verb stems and non-word sym-
bols and morphological features that identify
capitalized sequences, negations and words
in parentheses.
• φbgr contains features for POS patterns in a

bigram, the labels of dependency edges in-
cident to it, its likelihood under a Gigaword
language model (LM) and an indicator for
whether it is present in the input sentence.
• φdep contains features for the probability of

a dependency edge under a smoothed depen-
dency grammar constructed from the Penn
Treebank and various conjunctions of the fol-
lowing features: (a) whether the edge appears
as a dependency or ancestral relation in the
input parse (b) the directionality of the depen-

1246



dency (c) the label of the edge (d) the POS
tags of the tokens incident to the edge and
(e) the labels of their surrounding chunks and
whether the edge remains within the chunk.

For the experiments in the following section, we
trained models using a variant of the structured
perceptron (Collins, 2002) which incorporates
minibatches (Zhao and Huang, 2013) for easy par-
allelization and faster convergence.6 Overfitting
was avoided by averaging parameters and mon-
itoring performance against a held-out develop-
ment set during training. All models were trained
using variants of the ILP-based inference approach
of Thadani and McKeown (2013). We followed
Martins et al. (2009) in using LP-relaxed inference
during learning, assuming algorithmic separabil-
ity (Kulesza and Pereira, 2007) for these problems.

3 Experiments

We ran compression experiments over the
newswire (NW) and broadcast news transcription
(BN) corpora compiled by Clarke and Lapata
(2008) which contain gold compressions pro-
duced by human annotators using only word
deletion. The datasets were filtered to eliminate
instances with less than 2 and more than 110
tokens for parser compatibility and divided into
training/development/test sections following the
splits from Clarke and Lapata (2008), yielding
953/63/603 instances for the NW corpus and
880/78/404 for the BN corpus. Gold dependency
parses were approximated by running the Stanford
dependency parser7 over reference compressions.

Following evaluations in machine translation
as well as previous work in sentence compres-
sion (Unno et al., 2006; Clarke and Lapata, 2008;
Martins and Smith, 2009; Napoles et al., 2011b;
Thadani and McKeown, 2013), we evaluate sys-
tem performance using F1 metrics over n-grams
and dependency edges produced by parsing sys-
tem output with RASP (Briscoe et al., 2006) and
the Stanford parser. All ILPs and LPs were solved
using Gurobi,8 a high-performance commercial-
grade solver. Following a recent analysis of com-
pression evaluations (Napoles et al., 2011b) which
revealed a strong correlation between system com-
pression rate and human judgments of compres-
sion quality, we constrained all systems to produce

6We used a minibatch size of 4 in all experiments.
7http://nlp.stanford.edu/software/
8http://www.gurobi.com

compressed output at a specific rate—determined
by the the gold compressions available for each
instance—to ensure that the reported differences
between the systems under study are meaningful.

3.1 Systems

We report results over the following systems
grouped into three categories of models: tokens +
n-grams, tokens + dependencies, and joint models.

• 3-LM: A reimplementation of the unsuper-
vised ILP of Clarke and Lapata (2008) which
infers order-preserving trigram variables pa-
rameterized with log-likelihood under an LM
and a significance score for token variables
inspired by Hori and Furui (2004), as well as
various linguistically-motivated constraints
to encourage fluency in output compressions.
• DP: The bigram-based dynamic program of

McDonald (2006) described in §2.3.9
• LP→MST: An approximate inference ap-

proach based on an LP relaxation of ILP-
Dep. As discussed in §2.4, a maximum span-
ning tree is recovered from the output of the
LP and greedily pruned in order to generate
a valid integral solution while observing the
imposed compression rate.
• ILP-Dep: A version of the joint ILP of

Thadani and McKeown (2013) without n-
gram variables and corresponding features.
• DP+LP→MST: An approximate joint infer-

ence approach based on Lagrangian relax-
ation that uses DP for the maximum weight
subsequence problem and LP→MST for the
maximum weight subtree problem.
• DP+LP: Another Lagrangian relaxation ap-

proach that pairs DP with the non-integral
solutions from an LP relaxation of the maxi-
mum weight subtree problem (cf. §2.4).
• ILP-Joint: The full ILP from Thadani and

McKeown (2013), which provides an upper
bound on the performance of the proposed
approximation strategies.

The learning rate schedule for the Lagrangian re-
laxation approaches was set as ηi , τ/(τ + i),10
while the hyperparameter ψ was tuned using the

9For consistent comparisons with the other systems, our
reimplementation does not include the k-best inference strat-
egy presented in McDonald (2006) for learning with MIRA.

10τ was set to 100 for aggressive subgradient updates.

1247



Inference n-grams F1% Syntactic relations F1% Inference
objective technique n = 1 2 3 4 z Stanford RASP time (s)

n-grams
3-LM (CL08) 74.96 60.60 46.83 38.71 - 60.52 57.49 0.72
DP (McD06) 78.80 66.04 52.67 42.39 - 63.28 57.89 0.01

deps
LP→MST 79.61 64.32 50.36 40.97 66.57 66.82 59.70 0.07
ILP-Dep 80.02 65.99 52.42 43.07 72.43 67.63 60.78 0.16
DP + LP→MST 79.50 66.75 53.48 44.33 64.63 67.69 60.94 0.24

joint DP + LP 79.10 68.22 55.05 45.81 65.74 68.24 62.04 0.12
ILP-Joint (TM13) 80.13 68.34 55.56 46.60 72.57 68.89 62.61 0.31

Table 1: Experimental results for the BN corpus, averaged over 3 gold compressions per instance. All
systems were restricted to compress to the size of the median gold compression yielding an average
compression rate of 77.26%.

Inference n-grams F1% Syntactic relations F1% Inference
objective technique n = 1 2 3 4 z Stanford RASP time (s)

n-grams
3-LM (CL08) 66.66 51.59 39.33 30.55 - 50.76 49.57 1.22
DP (McD06) 73.18 58.31 45.07 34.77 - 56.23 51.14 0.01

deps
LP→MST 73.32 55.12 41.18 31.44 61.01 58.37 52.57 0.12
ILP-Dep 73.76 57.09 43.47 33.44 65.45 60.06 54.31 0.28
DP + LP→MST 73.13 57.03 43.79 34.01 57.91 58.46 53.20 0.33

joint DP + LP 72.06 59.83 47.39 37.72 58.13 58.97 53.78 0.21
ILP-Joint (TM13) 74.00 59.90 47.22 37.01 65.65 61.29 56.24 0.60

Table 2: Experimental results for the NW corpus with all systems compressing to the size of the gold
compression, yielding an average compression rate of 70.24%. In both tables, bold entries show signifi-
cant gains within a column under the paired t-test (p < 0.05) and Wilcoxon’s signed rank test (p < 0.01).

development split of each corpus.11

3.2 Results

Tables 1 and 2 summarize the results from our
compression experiments on the BN and NW cor-
pora respectively. Starting with the n-gram ap-
proaches, the performance of 3-LM leads us to
observe that the gains of supervised learning far
outweigh the utility of higher-order n-gram factor-
ization, which is also responsible for a significant
increase in wall-clock time. In contrast, DP is an
order of magnitude faster than all other approaches
studied here although it is not competitive under
parse-based measures such as RASP F1% which
is known to correlate with human judgments of
grammaticality (Clarke and Lapata, 2006).

We were surprised by the strong performance
of the dependency-based inference techniques,
which yielded results that approached the joint
model in both n-gram and parse-based measures.

11We were surprised to observe that performance improved
significantly when ψ was set closer to 1, thereby emphasiz-
ing token features in the dependency subproblem. The final
values chosen were ψBN = 0.9 and ψNW = 0.8.

The exact ILP-Dep approach halves the run-
time of ILP-Joint to produce compressions that
have similar (although statistically distinguish-
able) scores. Approximating dependency-based
inference with LP→MST yields similar perfor-
mance for a further halving of runtime; however,
the performance of this approach is notably worse.

Turning to the joint approaches, the strong
performance of ILP-Joint is expected; less so
is the relatively high but yet practically reason-
able runtime that it requires. We note, how-
ever, that these ILPs are solved using a highly-
optimized commercial-grade solver that can uti-
lize all CPU cores12 while our approximation
approaches are implemented as single-processed
Python code without significant effort toward op-
timization. Comparing the two approximation
strategies shows a clear performance advantage
for DP+LP over DP+LP→MST: the latter ap-
proach entails slower inference due to the over-
head of running the Chu-Liu Edmonds algorithm
at every dual update, and furthermore, the error in-
troduced by approximating an integral solution re-

1216 cores in our experimental environment.

1248



sults in a significant decrease in dependency recall.
In contrast, DP+LP directly optimizes the dual
problem by using the relaxed dependency solution
to update Lagrange multipliers and achieves the
best performance on parse-based F1 outside of the
slower ILP approaches. Convergence rates also
vary for these two techniques: DP+LP has a lower
rate of empirical convergence (15% on BN and 4%
on NW) when compared to DP+LP→MST (19%
on BN and 6% on NW).

Figure 3 shows the effect of input sentence
length on inference time and performance for ILP-
Joint and DP+LP over the NW test corpus.13 The
timing results reveal that the approximation strat-
egy is consistently faster than the ILP solver. The
variation in RASP F1% with input size indicates
the viability of a hybrid approach which could bal-
ance accuracy and speed by using ILP-Joint for
smaller problems and DP+LP for larger ones.

4 Related Work

Sentence compression is one of the better-studied
text-to-text generation problems and has been ob-
served to play a significant role in human summa-
rization (Jing, 2000; Jing and McKeown, 2000).
Most approaches to sentence compression are su-
pervised (Knight and Marcu, 2002; Riezler et
al., 2003; Turner and Charniak, 2005; McDon-
ald, 2006; Unno et al., 2006; Galley and McK-
eown, 2007; Nomoto, 2007; Cohn and Lapata,
2009; Galanis and Androutsopoulos, 2010; Gan-
itkevitch et al., 2011; Napoles et al., 2011a; Fil-
ippova and Altun, 2013) following the release of
datasets such as the Ziff-Davis corpus (Knight and
Marcu, 2000) and the Edinburgh compression cor-
pora (Clarke and Lapata, 2006; Clarke and Lap-
ata, 2008), although unsupervised approaches—
largely based on ILPs—have also received con-
sideration (Clarke and Lapata, 2007; Clarke and
Lapata, 2008; Filippova and Strube, 2008). Com-
pression has also been used as a tool for document
summarization (Daumé and Marcu, 2002; Zajic
et al., 2007; Clarke and Lapata, 2007; Martins
and Smith, 2009; Berg-Kirkpatrick et al., 2011;
Woodsend and Lapata, 2012; Almeida and Mar-
tins, 2013; Molina et al., 2013; Li et al., 2013;
Qian and Liu, 2013), with recent work formulating
the summarization task as joint sentence extrac-
tion and compression and often employing ILP or
Lagrangian relaxation. Monolingual compression

13Similar results were observed for the BN test corpus.

Figure 3: Effect of input size on (a) inference time,
and (b) the corresponding difference in RASP
F1% (ILP-Joint – DP+LP) on the NW corpus.

also faces many obstacles common to decoding in
machine translation, and a number of approaches
which have been proposed to combine phrasal and
syntactic models (Huang and Chiang, 2007; Rush
and Collins, 2011) inter alia offer directions for
future research into compression problems.

5 Conclusion
We have presented approximate inference strate-
gies to jointly compress sentences under bigram
and dependency-factored objectives by exploiting
the modularity of the task and considering the two
subproblems in isolation. Experiments show that
one of these approximation strategies produces re-
sults comparable to a state-of-the-art integer linear
program for the same joint inference task with a
60% reduction in average inference time.

Acknowledgments
The author is grateful to Alexander Rush for help-
ful discussions and to the anonymous reviewers
for their comments. This work was supported
by the Intelligence Advanced Research Projects
Activity (IARPA) via Department of Interior Na-
tional Business Center (DoI/NBC) contract num-
ber D11PC20153. The U.S. Government is autho-
rized to reproduce and distribute reprints for Gov-
ernmental purposes notwithstanding any copy-
right annotation thereon.14

14The views and conclusions contained herein are those of
the authors and should not be interpreted as necessarily repre-
senting the official policies or endorsements, either expressed
or implied, of IARPA, DoI/NBC, or the U.S. Government.

1249



References
Miguel Almeida and André F. T. Martins. 2013. Fast

and robust compressive summarization with dual de-
composition and multi-task learning. In Proceed-
ings of ACL, pages 196–206, August.

Taylor Berg-Kirkpatrick, Dan Gillick, and Dan Klein.
2011. Jointly learning to extract and compress. In
Proceedings of ACL-HLT, pages 481–490.

Ted Briscoe, John Carroll, and Rebecca Watson. 2006.
The second release of the RASP system. In Pro-
ceedings of the ACL-COLING Interactive Presenta-
tion Sessions.

Yoeng-jin Chu and Tseng-hong Liu. 1965. On the
shortest arborescence of a directed graph. Science
Sinica, 14:1396–1400.

James Clarke and Mirella Lapata. 2006. Models
for sentence compression: a comparison across do-
mains, training requirements and evaluation mea-
sures. In Proceedings of ACL-COLING, pages 377–
384.

James Clarke and Mirella Lapata. 2007. Modelling
compression with discourse constraints. In Proceed-
ings of EMNLP-CoNLL, pages 1–11.

James Clarke and Mirella Lapata. 2008. Global in-
ference for sentence compression: an integer linear
programming approach. Journal for Artificial Intel-
ligence Research, 31:399–429, March.

Trevor Cohn and Mirella Lapata. 2008. Sentence com-
pression beyond word deletion. In Proceedings of
COLING, pages 137–144.

Trevor Cohn and Mirella Lapata. 2009. Sentence com-
pression as tree transduction. Journal of Artificial
Intelligence Research, 34(1):637–674, April.

Michael Collins. 2002. Discriminative training meth-
ods for hidden Markov models. In Proceedings of
EMNLP, pages 1–8.

Dipanjan Das, André F. T. Martins, and Noah A.
Smith. 2012. An exact dual decomposition algo-
rithm for shallow semantic parsing with constraints.
In Proceedings of the First Joint Conference on Lexi-
cal and Computational Semantics (*SEM), SemEval
’12, pages 209–217.

Hal Daumé, III and Daniel Marcu. 2002. A noisy-
channel model for document compression. In Pro-
ceedings of ACL, pages 449–456.

John DeNero and Klaus Macherey. 2011. Model-
based aligner combination using dual decomposi-
tion. In Proceedings of ACL-HLT, pages 420–429.

Jack R. Edmonds. 1967. Optimum branchings. Jour-
nal of Research of the National Bureau of Standards,
71B:233–240.

Katja Filippova and Yasemin Altun. 2013. Overcom-
ing the lack of parallel data in sentence compression.
In Proceedings of EMNLP, pages 1481–1491.

Katja Filippova and Michael Strube. 2008. Depen-
dency tree based sentence compression. In Proceed-
ings of INLG, pages 25–32.

Dimitrios Galanis and Ion Androutsopoulos. 2010. An
extractive supervised two-stage method for sentence
compression. In Proceedings of HLT-NAACL, pages
885–893.

Michel Galley and Kathleen McKeown. 2007. Lex-
icalized Markov grammars for sentence compres-
sion. In Proceedings of HLT-NAACL, pages 180–
187, April.

Juri Ganitkevitch, Chris Callison-Burch, Courtney
Napoles, and Benjamin Van Durme. 2011. Learn-
ing sentential paraphrases from bilingual parallel
corpora for text-to-text generation. In Proceedings
of EMNLP, pages 1168–1179.

Chiori Hori and Sadaoki Furui. 2004. Speech summa-
rization: an approach through word extraction and a
method for evaluation. IEICE Transactions on In-
formation and Systems, E87-D(1):15–25.

Liang Huang and David Chiang. 2007. Forest rescor-
ing: Faster decoding with integrated language mod-
els. In Proceedings of ACL, pages 144–151, June.

Hongyan Jing and Kathleen R. McKeown. 2000. Cut
and paste based text summarization. In Proceedings
of NAACL, pages 178–185.

Hongyan Jing. 2000. Sentence reduction for auto-
matic text summarization. In Proceedings of the
Conference on Applied Natural Language Process-
ing, pages 310–315.

Kevin Knight and Daniel Marcu. 2000. Statistics-
based summarization - step one: Sentence compres-
sion. In Proceedings of AAAI, pages 703–710.

Kevin Knight and Daniel Marcu. 2002. Summariza-
tion beyond sentence extraction: a probabilistic ap-
proach to sentence compression. Artificial Intelli-
gence, 139(1):91–107, July.

Nikos Komodakis, Nikos Paragios, and Georgios Tzir-
itas. 2007. MRF optimization via dual decomposi-
tion: Message-passing revisited. In Proceedings of
ICCV, pages 1–8, Oct.

Terry Koo, Alexander M. Rush, Michael Collins,
Tommi Jaakkola, and David Sontag. 2010. Dual
decomposition for parsing with non-projective head
automata. In Proceedings of EMNLP, pages 1288–
1298.

Alex Kulesza and Fernando Pereira. 2007. Structured
learning with approximate inference. In John C.
Platt, Daphne Koller, Yoram Singer, and Sam T.
Roweis, editors, NIPS. Curran Associates, Inc.

1250



Hoong Chuin Lau, Trung Hieu Ngo, and Bao Nguyen
Nguyen. 2006. Finding a length-constrained
maximum-sum or maximum-density subtree and
its application to logistics. Discrete Optimization,
3(4):385 – 391.

Chen Li, Fei Liu, Fuliang Weng, and Yang Liu. 2013.
Document summarization via guided sentence com-
pression. In Proceedings of EMNLP, pages 490–
500, Seattle, Washington, USA, October.

Thomas L. Magnanti and Laurence A. Wolsey. 1994.
Optimal trees. In Technical Report 290-94,
Massechusetts Institute of Technology, Operations
Research Center.

André F. T. Martins and Noah A. Smith. 2009. Sum-
marization with a joint model for sentence extraction
and compression. In Proceedings of the Workshop
on Integer Linear Programming for Natural Lan-
gauge Processing, pages 1–9.

André F. T. Martins, Noah A. Smith, and Eric P. Xing.
2009. Concise integer linear programming formu-
lations for dependency parsing. In Proceedings of
ACL-IJCNLP, pages 342–350.

André F. T. Martins, Noah A. Smith, Pedro M. Q.
Aguiar, and Mário A. T. Figueiredo. 2011. Dual
decomposition with many overlapping components.
In Proceedings of EMNLP, pages 238–249.

Ryan McDonald, Fernando Pereira, Kiril Ribarov, and
Jan Hajič. 2005. Non-projective dependency pars-
ing using spanning tree algorithms. In Proceedings
of EMNLP-HLT, pages 523–530.

Ryan McDonald. 2006. Discriminative sentence com-
pression with soft syntactic evidence. In Proceed-
ings of EACL, pages 297–304.

Alejandro Molina, Juan-Manuel Torres-Moreno, Eric
SanJuan, Iria da Cunha, and Gerardo Eugenio
Sierra Martı́nez. 2013. Discursive sentence com-
pression. In Computational Linguistics and Intelli-
gent Text Processing, volume 7817, pages 394–407.
Springer.

Courtney Napoles, Chris Callison-Burch, Juri Ganitke-
vitch, and Benjamin Van Durme. 2011a. Paraphras-
tic sentence compression with a character-based
metric: tightening without deletion. In Proceedings
of the Workshop on Monolingual Text-To-Text Gen-
eration, pages 84–90.

Courtney Napoles, Benjamin Van Durme, and Chris
Callison-Burch. 2011b. Evaluating sentence com-
pression: pitfalls and suggested remedies. In Pro-
ceedings of the Workshop on Monolingual Text-To-
Text Generation, pages 91–97.

Tadashi Nomoto. 2007. Discriminative sentence com-
pression with conditional random fields. Infor-
mation Processing and Management, 43(6):1571–
1587, November.

Xian Qian and Yang Liu. 2013. Fast joint compression
and summarization via graph cuts. In Proceedings
of EMNLP, pages 1492–1502, Seattle, Washington,
USA, October.

Stefan Riezler, Tracy H. King, Richard Crouch, and
Annie Zaenen. 2003. Statistical sentence condensa-
tion using ambiguity packing and stochastic disam-
biguation methods for lexical-functional grammar.
In Proceedings of HLT-NAACL, pages 118–125.

Alexander M. Rush and Michael Collins. 2011. Ex-
act decoding of syntactic translation models through
Lagrangian relaxation. In Proceedings of ACL-HLT,
pages 72–82.

Alexander M. Rush, David Sontag, Michael Collins,
and Tommi Jaakkola. 2010. On dual decomposition
and linear programming relaxations for natural lan-
guage processing. In Proceedings of EMNLP, pages
1–11.

Kapil Thadani and Kathleen McKeown. 2013. Sen-
tence compression with joint structural inference. In
Proceedings of CoNLL.

Jenine Turner and Eugene Charniak. 2005. Super-
vised and unsupervised learning for sentence com-
pression. In Proceedings of ACL, pages 290–297.

Yuya Unno, Takashi Ninomiya, Yusuke Miyao, and
Jun’ichi Tsujii. 2006. Trimming CFG parse trees
for sentence compression using machine learning
approaches. In Proceedings of ACL-COLING, pages
850–857.

Kristian Woodsend and Mirella Lapata. 2012. Mul-
tiple aspect summarization using integer linear pro-
gramming. In Proceedings of EMNLP, pages 233–
243.

David Zajic, Bonnie J. Dorr, Jimmy Lin, and Richard
Schwartz. 2007. Multi-candidate reduction: Sen-
tence compression as a tool for document summa-
rization tasks. Information Processing and Manage-
ment, 43(6):1549–1570, Nov.

Kai Zhao and Liang Huang. 2013. Minibatch and par-
allelization for online large margin structured learn-
ing. In Proceedings of HLT-NAACL, pages 370–
379, Atlanta, Georgia, June.

1251


