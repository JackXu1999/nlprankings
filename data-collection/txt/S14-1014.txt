



















































Generating Simulations of Motion Events from Verbal Descriptions


Proceedings of the Third Joint Conference on Lexical and Computational Semantics (*SEM 2014), pages 99–109,
Dublin, Ireland, August 23-24 2014.

Generating Simulations of Motion Events from Verbal Descriptions

James Pustejovsky
Computer Science Dept.

Brandeis University
Waltham, MA USA
jamesp@cs.brandeis.edu

Nikhil Krishnaswamy
Computer Science Dept.

Brandeis University
Waltham, MA USA

nkrishna@brandeis.edu

Abstract
In this paper, we describe a computational
model for motion events in natural lan-
guage that maps from linguistic expres-
sions, through a dynamic event interpreta-
tion, into three-dimensional temporal sim-
ulations in a model. Starting with the
model from (Pustejovsky and Moszkow-
icz, 2011), we analyze motion events us-
ing temporally-traced Labelled Transition
Systems. We model the distinction be-
tween path- and manner-motion in an op-
erational semantics, and further distin-
guish different types of manner-of-motion
verbs in terms of the mereo-topological re-
lations that hold throughout the process of
movement. From these representations,
we generate minimal models, which are
realized as three-dimensional simulations
in software developed with the game en-
gine, Unity. The generated simulations
act as a conceptual “debugger” for the se-
mantics of different motion verbs: that
is, by testing for consistency and infor-
mativeness in the model, simulations ex-
pose the presuppositions associated with
linguistic expressions and their composi-
tions. Because the model generation com-
ponent is still incomplete, this paper fo-
cuses on an implementation which maps
directly from linguistic interpretations into
the Unity code snippets that create the sim-
ulations.

1 Introduction

Semantic interpretation requires access to both
knowledge about words and how they compose.
As the linguistic phenomena associated with lexi-
cal semantics have become better understood, sev-
eral assumptions have emerged across most mod-
els of word meaning. These include the following:

(1) a. Lexical meaning involves some sort of
“componential analysis”, either through
predicative primitives or a system of
types.

b. The selectional properties of predicators
can be explained in terms of these com-
ponents;

c. An understanding of event semantics and
the different role of event participants
seems crucial for modeling linguistic ut-
terances.

As a starting point in lexical semantic analysis,
a standard methodology in both theoretical and
computational linguistics is to identify features in
a corpus that differentiate the data in meaningful
ways; meaningful in terms of prior theoretical as-
sumptions or in terms of observably differentiated
behaviors. Combining these strategies we might,
for instance, take a theoretical constraint that we
hope to justify through behavioral distinctions in
the data. An example of this is the theoretical
claim that motion verbs can be meaningfully di-
vided into two classes: manner- and path-oriented
predicates (Talmy, 1985; Jackendoff, 1983; Talmy,
2000). These constructions can be viewed as en-
coding two aspects of meaning: how the move-
ment is happening and where it is happening. The
former strategy is illustrated in (2a) and the latter
in (2b) (where m indicates a manner verb, and p
indicates a path verb).

(2) a. The ball rolledm.
b. The ball crossedp the room.

With both of the verb types, adjunction can make
reference to the missing aspect of motion, by intro-
ducing a path (as in (3a)) or the manner of move-
ment (in (3b)).

(3) a. The ball rolledm across the room.
b. The ball crossedp the room rolling.

Differences in syntactic distribution and grammat-
ical behavior in large datasets, in fact, correlate

99



fairly closely with the theoretical claims made by
linguists using small introspective datasets.

The path-manner classification is a case where
there are data-derived distinctions that corre-
late nicely with theoretically inspired predictions.
More often than not, however, lexical semantic
distinctions are formal stipulations in a linguistic
model, that often have no observable correlations
to data. For example, an examination of the man-
ner of movement class from Levin (1993) illus-
trates this point. The verbs below are all Levin-
class manner of motion verbs:

(4) MANNER OF MOTION VERBS: drive, walk,
run, crawl, fly, swim, drag, slide, hop, roll

Assuming the two-way distinction between path
and manner predication of motion mentioned
above, these verbs do, in fact, tend to pattern ac-
cording to the latter class in the corpus. Given
that they are all manner of motion verbs, however,
any data-derived distinctions that emerge within
this class will have to be made in terms of addi-
tional syntactic or semantic dimensions. While it
is most likely possible to differentiate, for exam-
ple, the verbs slide from roll, or walk from hop in
the corpus, given enough data, it is important to
realize that conceptual and theoretical modeling is
often necessary to reveal the factors that semanti-
cally distinguish such linguistic expressions, in the
first place.

We argue that this problem can be approached
with the use of minimal model generation. As
Blackburn and Bos (2008) point out, theorem
proving (essentially type satisfaction of a verb in
one class as opposed to another) provides a “nega-
tive handle” on the problem of determining consis-
tency and informativeness for an utterance, while
model building provides a “positive handle” on
both. For our concerns, simulation construction
provides a positive handle on whether two man-
ner of motion processes are distinguished in the
model. Further, the simulation must specify how
they are distinguished, the analogue to informa-
tiveness.

In this paper, we argue that traditional lexical
modeling can benefit greatly from examining how
semantic interpretations are contextually and con-
ceptually grounded. We explore a dynamic in-
terpretation of the lexical semantic model devel-
oped in Generative Lexicon Theory (Pustejovsky,
1995; Pustejovsky et al., 2014). Specifically, we
are interested in using model building (Blackburn

and Bos, 2008; Konrad, 2004; Gardent and Kon-
rad, 2000) and simulation generation (Coyne and
Sproat, 2001; Siskind, 2011) to reveal the concep-
tual presuppositions inherent in natural language
expressions. In this paper, we focus our attention
on motion verbs, in order to distinguish between
manner and path motion verbs, as well as to model
mereotopological distinctions within the manner
class.

2 Situating Motion in Space and Time

The interpretation of motion in language has been
one of the most researched areas in linguistics
and Artificial Intelligence (Kuipers, 2000; Freksa,
1992; Galton, 2000; Levinson, 2003; Mani and
Pustejovsky, 2012). Because of their grammatical
and semantic import, linguistic interest in identi-
fying where events happen has focused largely on
motion verbs and the role played by paths. Jack-
endoff (1983), for example, elaborates a semantics
for motion verbs incorporating explicit reference
to the path traversed by the mover, from source to
destination (goal) locations. Talmy (1983) devel-
ops a similar conceptual template, where the path
followed by the figure is integral to the conceptu-
alization of the motion against a ground. Hence,
the path can be identified as the central element in
defining the location of the event (Talmy, 2000).
Related to this idea, both Zwarts (2005) and Puste-
jovsky and Moszkowicz (2011) develop mecha-
nisms for dynamically creating the path traversed
by a mover in a manner of motion predicate, such
as run or drive. Starting with this approach, the
localization of a motion event, therefore, is at
least minimally associated with the path created
by virtue of the activity.

In addition to capturing the spatial trace of the
object in motion, several researchers have pointed
out that identifying the shape of the path dur-
ing motion is also critical for fully interpreting
the semantics of movement. Eschenbach et al.
(1999) discusses the orientation associated with
the trajectory, something they refer to as oriented
curves. Motivated more by linguistic considera-
tions, Zwarts (2006) introduces the notion of an
event shape, which is the trajectory associated
with an event in space represented by a path. He
defines a shape function, which is a partial func-
tion assigning unique paths to those events involv-
ing motion or extension in physical space. This
work suggests that the localization of an event

100



makes reference to orientational as well as config-
urational factors, a view that is pursued in Puste-
jovsky (2013b). This forces us to look at the var-
ious spatio-temporal regions associated with the
event participants, and the interactions between
them.

These issues are relevant to our present con-
cerns, because in order to construct a simulation, a
motion event must be embedded within an appro-
priate minimal embedding space. This must suf-
ficiently enclose the event localization, while op-
tionally including room enough for a frame of ref-
erence visualization of the event (the viewer’s per-
spective). We return to this issue later in the paper
when constructing our simulation from the seman-
tic interpretation associated with motion events.

3 Modeling Motion in Language

3.1 Theoretical Assumptions

The advantage of adopting a dynamic interpre-
tation of motion is that we can directly distin-
guish path predication from manner of motion
predication in an operational semantics (Miller
and Charles, 1991; Miller and Johnson-Laird,
1976) that maps nicely to a simulation environ-
ment. Models of processes using updating typi-
cally make reference to the notion of a state tran-
sition (van Benthem, 1991; Harel, 1984). This is
done by distinguishing between formulae, φ, and
programs, π. A formula is interpreted as a clas-
sical propositional expression, with assignment of
a truth value in a specific model. We will inter-
pret specific models by reference to specific states.
A state is a set of propositions with assignments
to variables at a specific index. Atomic programs
are input/output relations ( [[π]] ⊆ S × S), and
compound programs are constructed from atomic
ones following rules of dynamic logic (Harel et al.,
2000).

For the present discussion, we represent the dy-
namics of actions in terms of Labeled Transition
Systems (LTSs) (van Benthem, 1991).1 An LTS
consists of a triple, 〈S,Act,→〉, where: S is the
set of states; Act is a set of actions; and→ is a to-
tal transition relation:→⊆ S×Act×S. An action,
α ∈ Act, provides the labeling on an arrow, mak-
ing it explicit what brings about a state-to-state

1This is consistent with the approach developed in (Fer-
nando, 2009; Fernando, 2013). This approach to a dynamic
interpretation of change in language semantics is also in-
spired by Steedman (2002).

transition. As a shorthand for (e1, α, e2) ∈→, we
will also use e1

α−→ e2. If reference to the state
content (rather than state name) is required for in-
terpretation purposes (van Benthem et al., 1994),
then as shorthand for ({φ}e1 , α, {¬φ}e2) ∈→, we
use, φ

e1

α−→ ¬φ
e2

. Finally, when referring
to temporally-indexed states in the model, where
ei@i indicates the state ei interpreted at time i, as
shorthand for ({φ}e1@i, α, {¬φ}e2@i+1) ∈→, we
will use, φ

i

e1

α−→ ¬φ i+1
e2

, as described in Puste-
jovsky (2013).

3.2 Distinguishing Path and Manner Motion

We will assume that change of location of an ob-
ject can be viewed as a special instance of a first-
order program, which we will refer to as ν (Puste-
jovsky and Moszkowicz, 2011).2

(5) x := y (ν-transition, where loc(z) is value
being updated)
“x assumes the value given to y in the next
state.”
〈M, (i, i+ 1), (u, u[x/u(y)])〉 |= x := y
iff 〈M, i, u〉 |= loc(z) = x ∧ 〈M, i +
1, u[x/u(y)]〉 |= loc(z) = y

Given a simple transition, a process can be viewed
as simply an iteration of ν (Fernando, 2009).
However, as (Pustejovsky, 2013a) points out, since
most manner motion verbs in language are ac-
tually directed processes, simple decompositions
into change-of-location are inadequate. That is,
they are guarded transitions where the test is not
just non-coreference, but makes reference to val-
ues on a scale, C, and ensures that it continues in
an order-preserving change through the iterations.
When this test references the values on a scale, C,
we call this a directed ν-transition (~ν), e.g., x 4 y,
x < y:

(6) ~ν =df
C?x
ei

ν−→ ei+1.

(7) loc(z) = x e0
~ν−→ loc(z) = y1 e1 ~ν−→ . . .

loc(z) = yn en

This now provides us with our dynamic interpre-
tation of directed manner of motion verbs, such
as slide, swim, roll, where we have an iteration of
assignments of locations, undistinguished except

2Cf. Groenendijk and Stokhof (1990) for dynamic updat-
ing, and Naumann (2001) for a related analysis.

101



that the values are order-preserving according to a
scalar constraint.

This is quite different from the dynamic inter-
pretation of path predicates. Following (Galton,
2004; Pustejovsky and Moszkowicz, 2011), path
predicates such as arrive and leave make refer-
ence to a “distinguished location”, not an arbi-
trary location. For example, the ball enters the
room is satisified when the distinguished location,
D, (the room) is successfully tested as the loca-
tion for the moving object. That is, the location
is tested against the current location for an object
((loc(x) 6= D)?), and retested until it is satisfied
((loc(x) = D)?).

(8)

(loc(x)6=D)?
x

loc(z) = x e0
~ν−→

(loc(x) 6=D)?
x

loc(z) = y1 e1
~ν−→ . . .

(loc(x)=D)?
x

loc(z) = yn en

While beyond the scope of the present discus-
sion, it is worth noting that the model of event
structure adopted here for motion verbs fits well
with most of the major semantic and syntactic phe-
nomena associated with event classes and Aktion-
sarten.3

3.3 Mereotopological Distinctions in Manner

Given the formal distinction between path and
manner predicates as described above, let us ex-
amine how to differentiate meaning within the
manner class. Levin (1993) differentiates this
class in terms of argument alternation patterns,
and identifies the following verb groupings: ROLL,
RUN, EPONYMOUS VEHICLE, WALTZ, ACCOM-
PANY, and CHASE verbs. While suggestive, these
distinctions are only partially useful towards actu-
ally teasing apart the semantic dimensions along
which we identify the contributing factors of man-
ner.

Mani and Pustejovsky (2012) suggest a differ-
ent strategy involving the identification of seman-
tic parameters that clearly differentiate verb senses
from each other within this class. One parameter
exploited quite extensively within the motion class
involves the mereotopological contraints that in-
here throughout the movement of the object (Ran-
dell et al., 1992; Asher and Vieu, 1995; Gal-
ton, 2000). Using this parameter, we are able to
distinguish several of Levin’s classes of manner

3Cf. (Pustejovsky, 2013a) and (Krifka, 1992).

as well as some novel ones, as described in (9),
where a class is defined by the constraints that hold
throughout the event (where EC is “externally con-
nected”, and DC is “disconnected”).

(9) For Figure (F) relative to Ground (G):
a. EC(F,G), throughout motion:
b. DC(F,G), throughout motion:
c. EC(F,G) followed by DC(F,G), through-
out motion:
d. Sub-part(F’,F), EC(F’,G) followed by
DC(F’,G), throughout motion:
e. Containment of F in a Vehicle (V).

For example, consider the semantic distinction be-
tween the verbs slide and hop or bounce. When the
latter are employed in induced (directed) motion
constructions (Levin, 1993; Jackendoff, 1996),
they take on the meaning of manner of motion
verbs. Distinguishing between a sliding and hop-
ping motion involves inspecting the next-state
content in the motion n-gram: namely, there is a
continuous satisfaction of EC(F,G) throughout the
motion for slide and a toggling effect (on-off) for
the predicates bounce and hop, as shown in (10).

(10)

¬DC(x,G)?
x

loc(z) = x e0
~ν−→

DC(x,G)?
x

loc(z) = y1 e1
~ν−→

¬DC(x,G)?
x

loc(z) = y2 e2

With the surface as the ground argument, these
verbs are defined in terms of two transitions.4

B

A

s1

AA

s3s2

l1 l2 l3

Figure 1: Slide Motion

B

A

s1

A

A

s3s2

l1 l2 l3

Figure 2: Hop Motion
4Many natural language predicates require reference to at

least three states. These include the semelfactives mentioned
above, as well as blink and iterative uses of knock and clap
(Vendler, 1967; Dowty, 1979; Rothstein, 2008).

102



Distinguishing between a sliding motion and a
rolling motion is also fairly straightforward. We
have the entailments that result from each kind of
motion, given a set of initial conditions, as in the
following short sentence describing the motion of
a ball relative to a floor (the domain for our event
simulations).

• The ball slid.: At the termination of the ac-
tion, object ball has moved relative to a sur-
face in a manner that is [+translate].
That is, the movement is a translocation
across absolute space, but other attributes
(such as the ball’s orientation) do not change.

• The ball rolled.: At the termination of the
action, object ball has moved relative to a
surface in a manner that is [+translate]
and [+rotate]. Here, the translocation
across space is preserved, with the addition
of an orientation change.

We can further decompose these features, cast-
ing the [+translate] in terms of the trans-
lation’s dimensionality. For both the ball slid
and the ball rolled, it is required that the ball re-
main in the contact with the relevant surface, thus
we can enforce a [-3-dimensional] con-
straint on the [+translate] feature. Thus,
we arrive at the following differentiating se-
mantic constraints for these verbs: (a) slide,
[+translate], [-3-dimensional]; (b)
roll, [+translate], [-3-dimensional],
[+rotate]. This is illustrated below over three
states of execution.

B

s1

A

a

b

c

B

s2

A

b

a c

B

s3

A

c

b

a

Figure 3: Roll Motion

In our approach to conceptual modeling, we hy-
pothesize that between the members of any pair of
motion verbs, there exists at least one distinctive
feature of physical motion that distinguishes the

two predicates. While this may be too strong, it
is helpful in our use of simulations for debugging
the lexical semantics of linguistic expressions.5 In
order to quantify the qualitative distinctions be-
tween motion predicates and identify the precise
primitive components of a motion verb, we build
a real-time simulation, within which the individ-
ual features of a single motion verb can be defined
and isolated in three-dimensional space.

The idea of constructing simulations from lin-
guistic utterances is, of course, not new. There
are two groups of researchers who have developed
related ideas quite extensively: simulation theo-
rists, working in the philosophy of mind, such as
Alvin Goldman and Robert Gordon; and cogni-
tive scientists and linguists, such as Jerry Feldman,
Ron Langacker, and Ben Bergen. According to
Goldman (1989), simulation provides a process-
driven theory of mind and mental attribution, dif-
fering from the theory-driven models proposed by
Churchland and others (Churchland, 1991). From
the cognitive linguistics tradition, simulation se-
mantics has come to denote the mental instanti-
ation of an interpretation of any linguistic utter-
ance (Feldman, 2006; Bergen et al., 2007; Bergen,
2012). While these communities do not seem to
reference each other, it is clear from our perspec-
tive, that they are both pursuing similar programs,
where distinct linguistic utterances correspond to
generated models that have differentiated struc-
tures and behaviors (Narayanan, 1999; Siskind,
2011; Goldman, 2006).

4 Simulations as Minimal Models
The approach to simulation construction intro-
duced in the previous section is inspired by work
in minimal model generation (Blackburn and Bos,
2008; Konrad, 2004). Type satisfaction in the
compositional process mirrors the theorem prov-
ing component, while construction of the specific
model helps us distinguish what is inherent in the
different manner of motion events. This latter as-
pect is the “positive handle”, (Blackburn and Bos,
2008) which demonstrates the informativeness of
a distinction in our simulation.

Simulation software must be able to map a pred-
icate to a known behavior, its arguments to objects
in the scene, and then prompt those objects to ex-
ecute the behavior. A simple input sentence needs

5Obviously, true synonyms in the lexicon would not be
distinguishable in a model.

103



to be tagged and parsed and transformed into pred-
icate/argument representation, and from there into
a dynamic event structure, as in (Pustejovsky and
Moszkowicz, 2011). The event structure is inter-
preted as the transformation executed over the ob-
ject or objects in each frame, and then rendered.

Ball1/NNP crossed/VBD Floor/NNP

SBJ OBJ

Ball1/NNP rolled/VBD

SBJ

Table 1: Dependency parses for Ball1 crossed
Floor (top) and Ball1 rolled (bottom).

We currently use only proper names to refer to
objects in the scene, to simplify model generation,
hence Ball1 and Floor. This facilitates easy object
identification in this prototype development stage.

Given a tagged and dependency parsed sen-
tence, we can the transform the parse into a pred-
icate formula, using the root of the parse as the
predicate, the subject as a singleton first argument,
and all objects as an optional stack of subsequent
arguments.

1. pred := cross 1. pred := roll
2. x := Ball1 2. x := Ball1
3. y.push(Floor)
cross(Ball1,[Floor]) roll(Ball1)

Table 2: Transformation to predicate formula for
Ball1 crossed Floor and Ball1 rolled.

The resulting predicates are represented in Ta-
ble 3 as expressions in Dynamic Interval Tempo-
ral Logic (DITL) (Pustejovsky and Moszkowicz,
2011), which are equivalent to the LTS expres-
sions used above.

cross(Ball1,Floor)
loc(Ball1) := y, target(Ball1) := z; b := y;
(y := w; y 6= w; d(b,y) < d(b,w),
d(b,z) > d(z,w), IN(y,Floor))+

roll(Ball1)
loc(Ball1) := y, rot(Ball1) := z; bloc := y,
brot := z; (y := w; y 6= w; d(bloc,y) < d(bloc,w),
IN(y,Floor))+, (z := v; z 6= v; z-brot < v-brot)+

Table 3: DITL expressions for Ball1 crossed Floor
and Ball1 rolled.

The DITL expression forms the basis of the
coded behavior. The first two initialization steps
are coded into the behavior’s start function while
the the third, Kleene iterated step, is encoded in
the behavior’s update function.

5 Generating Simulations

We use the freely-available game engine, Unity,
(Goldstone, 2009) to handle all underlying graph-
ics processing, and limited our object library to
simple primitive shapes of spheroids, rectangular
prisms, and planes. For every instance of an ob-
ject, the game engine maintains a data structure for
the object’s virtual representation. Table 4 shows
the data structure for Entity, the superclass of
all movable objects.

Entity:
position: 3-vector rotation: 3-vector
scale: 3-vector transform: Matrix

collider =

center: 3-vector
min: 3-vector
max: 3-vector
radius: float

geometry: Mesh

currentBehavior: Behavior

Table 4: Data structure of motion-capable entities.

The position and scale of the object are
represented as 3-vectors of floating point numbers.
The rotation is represented as the Euler angles
of the object’s current rotation, also a 3-vector.
This 3-vector is computed as a quaternion for ren-
dering purposes. The transform matrix com-
poses the position, scale, and quaternion rotation
into the complete transformation applied to the ob-
ject at any given frame. The geometry is a mesh.
The points, edges, faces, and texture attributes that
comprise the mesh are all immutable at the mo-
ment so the mesh type is considered atomic for
our purposes. The collider contains the coor-
dinates of the center of the object, minimum and
maximum extents of the object’s boundaries, and
radius of the boundaries (for spherical objects).

Behaviors can only be executed over Entity
instances, so we also provide each one with a
currentBehavior property, referencing the
code to be executed over the object every frame
that said behavior is being run. This code performs
a transformation over the object at every step, gen-
erating a new state in a dynamic model of the
event denoted by the a given predicate. Thus, the
event6 is decomposed into frame-by-frame trans-
formations representing the ν-transition from Sec-
tion 3.2.

We generate example simulations of behaviors
in a sample environment, shown in Figure 4, that

6These events are linguistic events, and not the same as
“events” as used in software development or with event han-
dlers.

104



consists of a sealed four-walled room that contains
a number of primitive objects.

Figure 4: Sample environment in top-down and
perspective views.

The behaviors currently coded into our software
map directly from DITL to the simulation. The
various parts of the DITL formula that describes a
given behavior are coded into the behavior’s start
or update functions in Unity. Below is one such
C# code snippet: the per-frame transformation for
roll.

(11) transform.rotation = new Vector3(
0.0,0.0,transform.rotation.z+
(rotSpeed*deltaTime));

transform.position = new Vector3(
transform.position.x-radius*
deltaTime,transform.position.y,
transform.position.z);

This “translates” the DITL expression (y := w; y
6= w; d(bloc,y) < d(bloc,w))+, (z := v; z 6= v; z-brot
< v-brot),IN(y,Floor)+ while explicitly calculat-
ing the value of the precise differences in location
and rotation between each frame or time step. The
variables moveSpeed, rotSpeed and radius
are given explicit value. deltaTime refers to the
time elapsed between frames.

Translating a DITL formula into executable
code makes evident the differences in minimal
verb pairs, such as the ball (or box) rolled and the
ball (or box) slid. When an object rolls, one area

on the object must remain in contact with the sup-
porting surface, and that area must be adjacent to
the area contacting the surface in the previous time
step. When an object slides, the same area on the
object must contact the supporting surface. Com-
pare the per-frame transformation for slide below
to the given transformation for roll.

(12) transform.position = new Vector3(
transform.position.x-radius*deltaTime,
transform.position.y,
transform.position.z);

This maps the DITL expression (y := w; y 6= w;
d(bloc,y) < d(bloc,w),IN(y,Floor))+. Here, the ob-
ject’s location changes along a path leading away
from the start location, but does not rotate as in
roll.

DITL expressions and their coded equivalents
can also be composed into new, more specific mo-
tions. The cross formula from Section 4 can be
composed with that for roll to describe a “roll
across” motion.

In a model, a path verb such as cross does
not necessarily need an explicit manner of mo-
tion specified. In a simulation, the manner needs
to be given a value, requiring the composition of
the path verb (e.g., cross) with one of a certain
subsets of manner verbs specifying how the ob-
ject moves relative to the supporting surface. Be-
low are DITL expressions and code implementa-
tions for two cross predicates, the first a cross mo-
tion while sliding, the second a cross motion while
rolling.

(13) loc(Ball1) := y, target(Ball1) := z; b := y;
(y := w; y 6= w; d(b,y) < d(b,w), d(b,z) >
d(z,w), IN(y,Floor))+
offset = transform.position-
destination;

offset = Vector3.Normalize(offset);
transform.position = new Vector3(
transform.position.x-offset.x*
radius*deltaTime,
transform.position.y,
transform.position.z-
offset.z*radius*deltaTime);

At each frame, the distance between the object’s
current position and its previously computed des-
tination is computed again, and the update moves
the object away from its current position (d(b,y) <
d(b,w)) toward the destination (d(b,z) > d(z,w)).
Since no other manner of motion is specified, the
object does not turn or rotate as it moves, but sim-
ply “slides.”

105



(14) loc(Ball1) := y, target(Ball1) := z; b := y; (y
:= w; y 6= w; d(bloc,y) < d(bloc,w), d(bloc,z)
> d(z,w), (u := v; u 6= v; u-brot < v-brot),
IN(y,Floor))+
offset = transform.position-
destination;

offset = Vector3.Normalize(offset);
transform.rotation = new Vector3(
0.0,arccos(offset.z)*(360/PI*2),
transform.rotation.z+
(rotSpeed*deltaTime));

transform.position = new Vector3(
transform.position.x-offset.x*
radius*deltaTime,
transform.position.y,
transform.position.z-offset.z*
radius*deltaTime);

Here the update is the same as above, but
with the introduction of the rolling motion. In
both code snippets, the non-changing value of
transform.position.y implicitly maps the
IN RCC condition in the DITL formulas, and
keeps the moving object attached to the floor.

If there exists a behavior corresponding to the
predicate (by name) on an entity bearing the name
of the predicate’s first (subject) argument, the
transformation encoded in that behavior is per-
formed over the entity until an end condition spe-
cific to the behavior is met. The resulting animated
motion depicts the manner of motion denoted by
the predicate. Given a predicate of arity greater
than 1, the simulator tries to prompt a behavior on
the first argument that can be run using parameters
of the subsequent arguments.

A cross behavior, for example, divides the
supporting surface into regions and attempts to
move the crossing object from one region to the
the opposite region. In figure 5, the bounds of
Floor completely surround the bounds of Ball2
(IN(Ball2,Floor) in RCC8). This configuration
makes it possible for the simulation to compute a
motion moving the Ball2 object from one side of
the Floor to the other.

The left side of figure 5 shows a ball rolling and
a box sliding, a depiction of two predicates: Box1
slid and Ball1 rolled. The right side depicts Ball2
crossed Floor (from the rear center to the front
center). The starting state of each scene is over-
laid semi-transparently while the in-progress state
is fully opaque.

6 Discussion and Conclusion
In this paper, we describe a model for mapping
natural language motion expressions into a 3D
simulation environment. Our strategy has been to
use minimal simulation generation as a conceptual

Figure 5: Roll and slide motions in progress (top),
and cross motion in progress (bottom).

debugging tool, in order to tease out the semantic
differences between linguistic expressions; specif-
ically those between verbs that are members of
conventionally homogeneous classes, according to
linguistic analysis.

It should be pointed out that our goal is different
from WordsEye (Coyne and Sproat, 2001). While
we are interested in using simulation generation
to differentiate semantic distinctions in both lex-
ical classes and compositional constructions, the
goal behind WordsEye is to provide an enhanced
interface to allow non-specialists create 3D scenes
without being familiar with special software mod-
els for everyday objects and relations. There are
obvious synergies between these two goals that
can be pursued.

The simulations we create provide an interpre-
tation of the given motion predicate over the given
entity, but not the only interpretation. Just as
Coyne et al. (2010) does for static objects in the
WordsEye system, we must apply some implicit
constraints to our motion predicates to allow them
to be visually simulated. For instance, in the roll
and slide examples given in Figure 5, both objects
are moving in the same direction–parallel to the
back wall of the room object. Had the objects been
moving perpendicular to the back wall or in any
other direction, as long as they remained in con-

106



tact with the floor at all times, the simulated mo-
tion would still be considered a “roll” (if rotating
around an axis parallel to the floor), or a “slide”
(if not), regardless of what the precise direction of
motion is. Minimal pairs in a model have to be
compared and contrasted in a discriminative way,
and thus in modeling a slide predicate versus a roll
predicate, knowing that the distinction is one of
rotation parallel to the surface is enough to distin-
guish the two predicates in a model.

In a simulation, the discriminative process re-
quires that the two contrasting behaviors look dif-
ferent, and as such, the simulation software must
be able to completely render a scene for each
frame from behavior start to behavior finish, and
so every variable for every object being rendered
must have an assigned value, including the posi-
tion of the object from frame to frame. If these
values are left unspecified, the software either fails
to compile or throws an exception. Thus, we are
forced to arbitrarily choose a direction of motion
(as well as direction of rotation, speed of rota-
tion, speed of motion, etc.). As long as all non-
changing variables are kept consistent between a
minimal pair of behaviors, we can evaluate the
quantitative and qualitative differences between
the values that do change. As simulations re-
quire values to be assigned to variables that can be
left unspecified in an ordinary modeling process,
simulations expose presuppositions about the se-
mantics of motion verbs and of compositions that
would not be necessary in a model alone.

In order to evaluate the appropriateness of a
given simulation, we are currently experimenting
with a strategy often used in classification and an-
notation tasks, namely pairwise similarity judg-
ments (Rumshisky et al., 2012; Pustejovsky and
Rumshisky, 2014). This involves presenting a user
with a simple discrimination task that has a re-
duced cognitive load, comparing the similarity of
the example to the target instances. In the present
context, a subject is shown a specific simulation
resulting from the translation from textual input,
through DITL, to the visualization. A set of ac-
tivity or event descriptions is given, and the sub-
ject is then asked to select which best describes
the simulation shown; e.g., “Is this a sliding?”, “Is
this a rolling?”. The results of this experiment are
presently being evaluated.

The system is currently in the prototype stage
and needs to be expanded in three main areas: ob-

ject library, parsing pipeline, and predicate han-
dling. Our object and behavior libraries are cur-
rently limited to geometric primitives and the mo-
tions that can be applied over them. While roll,
slide, and cross behaviors can be scripted for
spheres and cubes and shapes derived from them,
a predicate like walk cannot be supported on the
current infrastructure. Thus, we intend to expand
the object library to include more complex inan-
imate objects (tables, chairs, or other household
objects) as well as animate objects. Having an ob-
ject library containing forms capable of executing
greater numbers of predicates will allow us to im-
plement those predicates.

The parsing pipeline described in Section 4 is
only partially implemented, with the only com-
pleted parts being the latter stages, relating a for-
mulas to a scripted behavior and its arguments. We
intend to expand the parsing pipeline to include all
the steps described in this paper: taking input as
a simple natural language sentence, tagging and
parsing it to extract the constituent parts of a pred-
icate/argument representation, and using that out-
put to prompt a behavior in software as a dynamic
event structure. More robust parsing will afford
us the opportunity to expand the diversity of pred-
icates that the software can handle as well (Mc-
Donald and Pustejovsky, 2014). While currently
limited to unary and binary predicates, we need
to extend the capability to ternary predicates and
predicates of greater arity, including the use of ad-
junct phrases and indirect objects. We are in the
process of developing an implementation that uses
Boxer (Curran et al., 2007) so that we can create
first-order models from the dynamic expressions
used here.

Acknowledgements

We would like to thank David McDonald for com-
ments and discussion. We would also like to thank
the reviewers for several substantial suggestions
for improvements and clarifications to the paper.
All remaining errors are of course the responsi-
bilities of the authors. This work was supported
in part by the Department of the Navy, Office of
Naval Research under grant N00014-13-1-0228.
Any opinions, findings, and conclusions or recom-
mendations expressed in this material are those of
the authors and do not necessarily reflect the views
of the Office of Naval Research.

107



References
Nicholas Asher and Laure Vieu. 1995. Towards a ge-

ometry of common sense: a semantics and a com-
plete axiomatisation of merotopology. In Proceed-
ings of IJCAI95, Montreal, Canada.

Benjamin K. Bergen, Shane Lindsay, Teenie Matlock,
and Srini Narayanan. 2007. Spatial and linguistic
aspects of visual imagery in sentence comprehen-
sion. Cognitive Science, 31(5):733–764.

Benjamin K Bergen. 2012. Louder than words: The
new science of how the mind makes meaning. Basic
Books.

Patrick Blackburn and Johan Bos. 2008. Computa-
tional semantics. THEORIA. An International Jour-
nal for Theory, History and Foundations of Science,
18(1).

Paul M Churchland. 1991. Folk psychology and the
explanation of human behavior. The future of folk
psychology: Intentionality and cognitive science,
pages 51–69.

Bob Coyne and Richard Sproat. 2001. Wordseye: an
automatic text-to-scene conversion system. In Pro-
ceedings of the 28th annual conference on Computer
graphics and interactive techniques, pages 487–496.
ACM.

Bob Coyne, Owen Rambow, Julia Hirschberg, and
Richard Sproat. 2010. Frame semantics in text-to-
scene generation. In Knowledge-Based and Intel-
ligent Information and Engineering Systems, pages
375–384. Springer.

James Curran, Stephen Clark, and Johan Bos. 2007.
Linguistically motivated large-scale nlp with c&c
and boxer. In Proceedings of the 45th Annual Meet-
ing of the Association for Computational Linguis-
tics Companion Volume Proceedings of the Demo
and Poster Sessions, pages 33–36, Prague, Czech
Republic, June. Association for Computational Lin-
guistics.

David R Dowty. 1979. Word meaning and Mon-
tague grammar: The semantics of verbs and times in
generative semantics and in Montague’s PTQ, vol-
ume 7. Springer.

C. Eschenbach, C. Habel, L. Kulik, et al. 1999. Rep-
resenting simple trajectories as oriented curves. In
FLAIRS-99, Proceedings of the 12th International
Florida AI Research Society Conference, pages 431–
436.

Jerome Feldman. 2006. From molecule to metaphor:
A neural theory of language. MIT press.

Tim Fernando. 2009. Situations in ltl as strings. Infor-
mation and Computation, 207(10):980–999.

Tim Fernando. 2013. Segmenting temporal intervals
for tense and aspect. In The 13th Meeting on the
Mathematics of Language, page 30.

Christian Freksa. 1992. Using orientation information
for qualitative spatial reasoning. Springer.

Antony Galton. 2000. Qualitative Spatial Change.
Oxford University Press, Oxford.

Antony Galton. 2004. Fields and objects in space,
time, and space-time. Spatial Cognition and Com-
putation, 4(1).

Claire Gardent and Karsten Konrad. 2000. Interpreting
definites using model generation. Journal of Lan-
guage and Computation, 1(2):193–209.

Alvin I Goldman. 1989. Interpretation psycholo-
gized*. Mind & Language, 4(3):161–185.

Alvin I Goldman. 2006. Simulating minds: The phi-
losophy, psychology, and neuroscience of mindread-
ing. Oxford University Press.

Will Goldstone. 2009. Unity Game Development Es-
sentials. Packt Publishing Ltd.

Jeroen Groenendijk and Martin Stokhof. 1990. Dy-
namic predicate logic. Linguistics and Philosophy,
14:39–100.

David Harel, Dexter Kozen, and Jerzy Tiuyn. 2000.
Dynamic Logic. The MIT Press, 1st edition.

David Harel. 1984. Dynamic logic. In M. Gabbay
and F. Gunthner, editors, Handbook of Philosophi-
cal Logic, Volume II: Extensions of Classical Logic,
page 497?604. Reidel.

Ray Jackendoff. 1983. Semantics and Cognition. MIT
Press.

Ray Jackendoff. 1996. The proper treatment of mea-
suring out, telicity, and perhaps even quantification
in english. Natural Language & Linguistic Theory,
14(2):305–354.

Karsten Konrad. 2004. Model generation for natural
language interpretation and analysis, volume 2953.
Springer.

Manfred Krifka. 1992. Thematic relations as links be-
tween nominal reference and temporal constitution.
Lexical matters, 2953.

Benjamin Kuipers. 2000. The spatial semantic hierar-
chy. Artificial Intelligence, 119(1):191–233.

Beth Levin. 1993. English verb class and alternations:
a preliminary investigation. University of Chicago
Press.

S.C. Levinson. 2003. Space in Language and Cog-
nition: Explorations in Cognitive Diversity. Lan-
guage, culture, and cognition. Cambridge University
Press.

Inderjeet Mani and James Pustejovsky. 2012. Inter-
preting Motion: Grounded Representations for Spa-
tial Language. Oxford University Press.

108



David McDonald and James Pustejovsky. 2014. On
the representation of inferences and their lexicaliza-
tion. In Advances in Cognitive Systems, volume 3.

G. Miller and W. Charles. 1991. Contextual corre-
lates of semantic similarity. Language and Cogni-
tive Processes, 6(1):1–28.

George A Miller and Philip N Johnson-Laird. 1976.
Language and perception. Belknap Press.

Srinivas Narayanan. 1999. Reasoning about actions in
narrative understanding. IJCAI, 99:350–357.

Ralf Naumann. 2001. Aspects of changes: a dynamic
event semantics. Journal of semantics, 18:27–81.

James Pustejovsky and Jessica Moszkowicz. 2011.
The qualitative spatial dynamics of motion. The
Journal of Spatial Cognition and Computation.

James Pustejovsky and Anna Rumshisky. 2014. Deep
semantic annotation with shallow methods. LREC
Tutorial, May.

James Pustejovsky, Anna Rumshisky, Olga Batiukova,
and Jessica Moszkowicz. 2014. Annotation of com-
positional operations with glml. In Harry Bunt, ed-
itor, Computing Meaning, pages 217–234. Springer
Netherlands.

J. Pustejovsky. 1995. The Generative Lexicon. Brad-
ford Book. Mit Press.

James Pustejovsky. 2013a. Dynamic event structure
and habitat theory. In Proceedings of the 6th Inter-
national Conference on Generative Approaches to
the Lexicon (GL2013), pages 1–10. ACL.

James Pustejovsky. 2013b. Where things happen: On
the semantics of event localization. In Proceedings
of ISA-9: International Workshop on Semantic An-
notation.

David Randell, Zhan Cui, and Anthony Cohn. 1992.
A spatial logic based on regions and connections. In
Morgan Kaufmann, editor, Proceedings of the 3rd
Internation Conference on Knowledge Representa-
tion and REasoning, pages 165–176, San Mateo.

Susan Rothstein. 2008. Two puzzles for a theory of
lexical aspect: Semelfactives and degree achieve-
ments. Event structures in linguistic form and in-
terpretation, 5:175.

Anna Rumshisky, Nick Botchan, Sophie Kushkuley,
and James Pustejovsky. 2012. Word sense inven-
tories by non-experts. In LREC, pages 4055–4059.

Jeffrey Mark Siskind. 2011. Grounding the lexi-
cal semantics of verbs in visual perception using
force dynamics and event logic. arXiv preprint
arXiv:1106.0256.

Mark Steedman. 2002. Plans, affordances, and combi-
natory grammar. Linguistics and Philosophy, 25(5-
6):723–753.

Leonard Talmy. 1983. How language structures space.
In Herbert Pick and Linda Acredolo, editors, Spa-
tial Orientation: Theory, Research, and Application.
Plenum Press.

Leonard Talmy. 1985. Lexicalization patterns: seman-
tic structure in lexical forms. In T. Shopen, editor,
Language typology and semantic description Vol-
ume 3:, pages 36–149. Cambridge University Press.

Leonard Talmy. 2000. Towards a cognitive semantics.
MIT Press.

Johan van Benthem, Jan van Eijck, and Vera Ste-
bletsova. 1994. Modal logic, transition systems
and processes. Journal of Logic and Computation,
4(5):811–855.

Johannes Franciscus Abraham Karel van Benthem.
1991. Logic and the flow of information.

Z. Vendler. 1967. Linguistics in philosophy. Cornell
University Press Ithaca.

J. Zwarts. 2005. Prepositional aspect and the algebra
of paths. Linguistics and Philosophy, 28(6):739–
779.

J. Zwarts. 2006. Event shape: Paths in the semantics
of verbs.

109


