



















































Incorporating Graph Attention Mechanism into Knowledge Graph Reasoning Based on Deep Reinforcement Learning


Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing
and the 9th International Joint Conference on Natural Language Processing, pages 2623–2631,
Hong Kong, China, November 3–7, 2019. c©2019 Association for Computational Linguistics

2623

Incorporating Graph Attention Mechanism into Knowledge Graph
Reasoning Based on Deep Reinforcement Learning

Heng Wang? Shuangyin Li∗ Rong Pan† Mingzhi Mao†
?Tencent, China

†School of Data and Computer Science, Sun Yat-sen University, China
∗School of Computer Science, South China Normal University, China

jimmyhwang@tencent.com
{panr@mail,mcsmmz@mail}.sysu.edu.cn

shuangyinli@m.scnu.edu.cn

Abstract

Knowledge Graph (KG) reasoning aims at
finding reasoning paths for relations, in or-
der to solve the problem of incompleteness in
KG. Many previous path-based methods like
PRA and DeepPath suffer from lacking mem-
ory components, or stuck in training. There-
fore, their performances always rely on well-
pretraining. In this paper, we present a deep
reinforcement learning based model named
by AttnPath, which incorporates LSTM and
Graph Attention Mechanism as the memory
components. We define two metrics, Mean Se-
lection Rate (MSR) and Mean Replacement
Rate (MRR), to quantitatively measure how
difficult it is to learn the query relations, and
take advantages of them to fine-tune the model
under the framework of reinforcement learn-
ing. Meanwhile, a novel mechanism of re-
inforcement learning is proposed by forcing
an agent to walk forward every step to avoid
the agent stalling at the same entity node con-
stantly. Based on this operation, the proposed
model not only can get rid of the pretraining
process, but also achieves state-of-the-art per-
formance comparing with the other models.
We test our model on FB15K-237 and NELL-
995 datasets with different tasks. Extensive
experiments show that our model is effective
and competitive with many current state-of-
the-art methods, and also performs well in
practice.

1 Introduction

Knowledge Graphs (KGs), such as NELL (Carl-
son et al., 2010), Freebase (Bollacker et al.,
2008) and WordNet (Miller, 1995) play an in-
creasingly critical role in many downstream NLP
applications, e.g., question answering (Dubey
et al., 2018), information retrieval (Liu et al.,
2018), personalized recommendations (Wang
et al., 2019), etc. However, KGs are always
incomplete, which would affect many down-
stream tasks. Many links could miss among

the entities in a KG. Thus, it is an important
and challenge task on how to complete the KG
by predicting the missing links between the
entities through the method based on reasoning.
For instance, if athleteP laysForTeam(X,Y )
and teamPlaysInLeague(Y, Z) both ex-
ist in the KG, then we can infer that
athleteP laysInLeague(X,Z), i.e, filling
the missing edge athleteP laysInLeague
between X and Z.

There are mainly three ways to accomplish
this task, such as Rule-Based (Wang and Cohen,
2016; Yang et al., 2017), Embedding-Based (Bor-
des et al., 2013; Lin et al., 2015) and Path-Based
(Lao et al., 2011). Meanwhile, it provides a new
perspective to bring Deep Reinforcement Learn-
ing (DRL) into the task of predicting the missing
links, such as DeepPath (Xiong et al., 2017), a
type of path-based method. DeepPath is the first
work which incorporates DRL into KG reason-
ing. It achieves significant improvements com-
pared with PRA, but still has several drawbacks.
First, it lacks memory components, resulting in re-
quiring pretraining. The operation of pretraining
is demanded to provide many known (or existed)
paths to the model training. This brute-force oper-
ation may make the model prone to overfit on the
given paths from pretraining. Second, it’s inappro-
priate to set the same hyperparameter for different
relations in a KG when training, which ignores the
diversity of connections among the entities. Last,
when the agent selects an invalid path, it will stop
and reselect, which leads to select this invalid path
constantly and finally stuck in one node.

Thus, in this paper, we present a novel deep
reinforcement learning model and an algorithm,
which aims at tackling the drawbacks mentioned
above. The proposed model also belongs to the
path-based framework. Our contributions can be
summarized as follows:

• We propose AttnPath, a model which incor-



2624

porates LSTM and graph attention as mem-
ory components, and is no longer subject to
pretraining.

• Two metrics are defined (MSR and MRR), to
quantitatively measure the difficulty to learn
a relation’s replaceable paths, which are used
to fine-tune the model.

• A novel mechanism of reinforcement learn-
ing is proposed by forcing an agent to walk
forward every step to avoid the agent stalling
at the same entity node constantly.

We test AttnPath on FB15K-237 and NELL-
995 datasets with two downstream tasks: fact pre-
diction and link prediction. We also test on the
success rate in finding paths and show the effec-
tiveness of the Graph Attention Mechanism in the
experiments.

2 Related Works

To date, there are many works proposed to solve
the problem of KG incompleteness. Rule-Based
methods, like ProPPR (Wang and Cohen, 2016)
and Neural LP (Yang et al., 2017), generate rea-
soning rules manually or by mathematical logic
rules, and then apply them to fill missing links
based on existing triples. Although this type of
methods has a solid mathematical background,
they are hard to scale to large KGs, since they
directly operate on symbols, while the number
of possible reasoning paths is exponential to the
number of the entities. Embedding-Based meth-
ods, like TransE (Bordes et al., 2013) and TransR
(Lin et al., 2015), map entities and relations into
a low-dimensional and continuous vector space,
which captures the feature of distance between en-
tities and relations. Then, they judge whether the
query relation exists by comparing the distance be-
tween the two trained entities’ embeddings and the
query relation’s embedding. This type of methods
requires all triples in the KG to participate in train-
ing, and are only suitable for single-hop reasoning.

Path-Based, like PRA (Lao et al., 2011) and
DeepPath (Xiong et al., 2017), train an agent to
navigate on a KG, find replaceable paths for a cer-
tain relation, and then, use them as features to
the downstream tasks. Path Ranking Algorithm
(PRA) is the first path-based reasoning method.
Neelakantan et al. develop a compositional model
based on RNN, which composes the implications

of a path and reasons about conjunctions of multi-
hop relations non-atomically (Neelakantan et al.,
2015). Guu et al. propose a soft edge traversal
operator, which can be recursively applied to pre-
dict paths and reduce cascaded propagation errors
faced by single-hop KG completion methods like
TransE and TransR (Guu et al., 2015). Toutanova
et al. propose a dynamic programming algorithm
which incorporates all relations’ paths of bounded
length in a KG, and models both relations and in-
termediate nodes in the compositional path repre-
sentations (Toutanova et al., 2016). Such repre-
sentations can aid in generating more high-quality
reasoning paths.

Das et al. improve DeepPath (Xiong et al.,
2017) to MINERVA (Das et al., 2018), which
views KG from QA’s perspective. It gets rid of
pretraining, introduces LSTM to memorize paths
traversed before, and trains an agent to circulate
on a certain entity if it believes this entity is the
right answer. Lin et al. improve these two methods
by introducing reward shaping and action dropout
(Lin et al., 2018). Reward shaping replaces
fixed penalty for useless selection with a dynamic
penalty, which can either base on margin-based
pretrained embeddings like TransE, or probability-
based embeddings like ConvE (Dettmers et al.,
2018). While action dropout randomly masks a
certain proportion of valid actions, in order to re-
duce irrelevant paths to the query relation. DIVA
(Chen et al., 2018) regards paths as latent vari-
ables, and relations as observed variables, so as to
build a variational inference model to accomplish
the KG reasoning task. It also uses beam search to
broaden the search horizon. M-Walk (Shen et al.,
2018) utilizes another RL algorithm called Monte
Carlo Tree Search (MCTS), to tackle the problem
of sparse rewards. The attention mechanism is first
introduced into multi-hop KG reasoning by (Wang
et al., 2018). However, it only computes the at-
tention weights of the query’s embedding and all
found paths’ embeddings. They are used to help
to judge whether the answer found by the vanilla
model is true.

3 AttnPath: Incorporating Memory
Components

In this section, we will introduce the details of the
proposed AttnPath. We will also show the defi-
nition of mean selection rate (MSR) and mean re-
placement rate (MRR), and also how they are used
to fine-tune the model for different query relations.



2625

3.1 RL framework for KG reasoning
Since we use Reinforcement Learning (RL) as the
training algorithm of a sequential decision model,
we first introduce basic elements of the RL frame-
work in the KG reasoning, including environment,
state, action, and reward.

Environment: In this task, the environment
refers to the whole KG, excluding the query rela-
tion and its inverse. The environment retains con-
sistent in the whole training process.

State: The state of an agent is concatenated
with three parts: the embedding part, the LSTM
part, and the graph attention part. We will show
the computation of the LSTM part and the graph
attention part in the next section, and introduce the
embedding part first.

Following (Xiong et al., 2017), the embedding
part mt is concatenated with two subparts. The
first one is et, which is the embedding of the cur-
rent entity node. The other one is etarget − et,
denoting the distance between the tail entity node
and the current node. Different from DeepPath
which uses TransE (Bordes et al., 2013) as pre-
trained embeddings, we take advantage of TransD
(Ji et al., 2015), which is an improvement of
TransE and also a common-used benchmark. Un-
der TransD, for a query relation, we project all the
entities onto the vector space of this query rela-
tion. Then, an entity’s projected embedding e⊥
becomes

e⊥ = (rpe
>
p + I)e, (1)

where p indicates the projection vectors. So mt
should be [et⊥; etarget⊥ − et⊥].

Action: For the KG reasoning task, an action
refers to an agent choosing a relation path to step
forward. Based on the framework of DRL, it
chooses the relation according to the probabilities
obtained by the model. Actions are either valid or
invalid. Valid action means that there is an output
relation connected with the current entity, while
invalid action denotes that there is no relation.

Reward: Reward is a feedback to the agent ac-
cording to whether the action is valid, and whether
a series of actions can lead to ground truth tail en-
tities in a specified number of times. We adopt re-
ward shaping trick proposed by (Lin et al., 2018).
For the invalid actions, the reward is -1. For the ac-
tions which don’t lead to ground truth, we choose
the output of ConvE (Dettmers et al., 2018) as the
reward. Since ConvE outputs probabilities, which
are in the range of (0, 1), we resort a logarithm op-

erator to enlarge the range of this reward and im-
prove discrimination. For the actions which lead
to ground truth, i.e, a successful episode, the re-
ward is the weighted sum of the global accuracy,
the path efficiency, and the path diversity. The
global accuracy is set to 1 by convention, and the
path efficiency is the reciprocal of the path length,
because we encourage the agent to step as fewer
times as possible. The path diversity is defined as

rDIVERSITY = −
1

|F |

|F |∑

i=1

cos(p,pi), (2)

where |F | is the number of found paths, and p is
the path embedding, simply the sum of all rela-
tions’ embeddings in the path. The above defini-
tion guarantees that the reward of the valid actions
are always larger than the invalid actions, and the
reward of the successful episodes are always larger
than the unsuccessful ones.

3.2 LSTM and Graph Attention as Memory
Components

In our model, we utilize a three-layer LSTM, en-
abling the agent to memorize and learn from the
actions taken before. Denote the hidden state of
LSTM at step t by ht, and the initial hidden state
h0 by 0. Then we obtain

ht = LSTM(ht−1,mt), (3)

where mt is defined in Eq. (1). This is the LSTM
part of the state described above.

Typically, an entity has several different aspects,
for example, a football player may be connected
with professional relations like playsForTeam
or playsInLeague, and also family relations like
spouse or father. For different query relations,
it’s better for the agent to focus more on rela-
tions and neighbors which are highly related to the
query relations. So we introduce Graph Attention
mechanism (GAT) into our model, which is pro-
posed by (Velickovic et al., 2018).

GAT is indeed self-attention on the entity nodes.
We use a single-layer feedforward neural network
to calculate attention weights, with a linear trans-
formation matrix W and a weight vector ~a shared
across all entities. LeakyReLU with negative in-
put slope α = 0.2 is chosen as nonlinearity. So
the attention weight from entity i to entity j is cal-
culated as

aij = LeakyReLU(~a
>[Wei⊥;Wej⊥]). (4)



2626

For entity i, we only compute attention weights to
all its direct-connected neighbors, and normalize
them with SoftMax. So the normalized attention
weight is

αij =
exp(aij)∑

k∈Ni exp(aik)
. (5)

Now we can compute the graph attention part for
the state, which is simply the weighted sum of all
neighbors’ embedding on the attention space with

ai =
∑

k∈Ni

αikWek. (6)

Thus, the state vector si,t of entity i in time t is

si,t = [mi,t;ht;ai], (7)

which in turn inputs a three-layer feedforward
neural network, whose final output is a Softmax
probability with length equal to the number of all
relations in KG. The agent selects an action and
obtains a reward. After it successfully reaches the
tail entity or doesn’t reach in a specified number
of times, the rewards of the whole episode are
used to update all parameters. The optimization is
done using the REINFORCE (Williams, 1992) al-
gorithm and updates θ with the following stochas-
tic gradient:

∇θJ(θ) ≈ ∇θ
T∑

t=1

R(sT |es, r)logπθ(at|st), (8)

where es is head entity, r the query relation, and
πθ(at|st) the probability of all relations. Figure 1
shows our AttnPath model.

eiei

ej1ej1

ej2ej2
ej3ej3

ai1ai1
ai2ai2

ai3ai3
ai...ai...

mi,tmi,t

∑∑

htht aiai

SoftMax

…… Full Connection, ReLU

Figure 1: AttnPath: an RL framework composed with
LSTM and graph attention for KG reasoning.

Algorithm 1 Training Algorithm of the agent for
AttnPath

1: for episode← 1 to N do
2: Initialize LSTM’s hidden state h0 to 0
3: Initialize state vector s0
4: Initialize num steps to 0
5: while num steps < max steps do
6: Randomly sample action a ∼ πθ(at|st)
7: if Action invalid (leads to nothing) then
8: Add < st, a > toMneg
9: Normalize πθ(at|st) on valid actions

and sample af
10: Add < st, af > toMpos
11: else
12: Add < st, a > toMpos
13: end if
14: Increment num steps
15: if success or num steps = max steps

then
16: break
17: end if
18: end while
19: Update θ using g ∝∑

Mneg logπ(at|st; θ)(−1)
20: if success then
21: Rtotal ← λ1rGLOBAL +

λ2rEFFICIENCY + λ3rDIVERSITY

22: Update θ using g ∝∑
Mpos logπ(at|st; θ)Rtotal

23: else
24: Compute shaped reward Rshaping for

Mpos
25: Update θ using g ∝∑

Mpos logπ(at|st; θ)Rshaping
26: end if
27: end for

3.3 Mean Selection / Replacement Rate

For different query relations, it is required to train
different models for each of them. While, in prac-
tice, the difficulty values of each relation are quite
different. Some relation may have more replace-
ment relations, indicating that an agent can easily
choose a replacement path walking from head en-
tity to tail. So we invent two metrics, Mean Selec-
tion Rate (MSR) and Mean Replacement Rate
(MRR) here, to quantitatively measure the diffi-
culty value of each relation.

Denote all triples related to relation r by a set



2627

of Tr = {(h, ro, t)|ro = r}. The selection rate for
relation r with regard to triple (h, r, t) is defined
as

SR((h, r, t)) =
|{(ho, ro, to)|ho = h, ro = r}|
|{(ho, ro, to)|ho = h}|

,

(9)
i.e, the proportion of relation r occupying h’s out-
going paths.

Thus, MSR is the average of SR on Tr:

MSR(r) =
∑

(ho,r,to)∈Tr

SR(ho, r, to)/|Tr|. (10)

Lower MSR denotes that it’s more difficult to learn
r, because the entities connected with relation r
may have more aspects.

The replacement rate for relation r with regard
to triple (h, r, t) is defined as

RR((h, r, t)) = |{(ho,ro,to)|ho=h,ro 6=r,to=t}||{(ho,ro,to)|ho=h,to=t}| ,
(11)

i.e, the proportion of relations that directly con-
nects h and t, except relation r.

Similarly, MRR is the average of RR on Tr:

MRR(r) =
∑

(ho,r,to)∈Tr

RR(ho, r, to)/|Tr|. (12)

Higher MRR denotes a relation may have more re-
placement relations, so it’s easier to learn since an
agent can directly choose an alternative relation to
reach the destination.

In our model, we have three ways to prevent
overfitting: L2 regularization, dropout, and action
dropout. However, for the relations which are eas-
ier to learn (high MSR and MRR), we wish to im-
pose more regularization to encourage the agent
to find more diverse paths, without overfitting on
the immediate success. Otherwise, for the rela-
tions which are more difficult to learn (low MSR
and MRR), we’d better focus on the success rate
of finding a path, so we should impose less regu-
larization.

For simplicity, we use exponential to compute
the difficulty coefficient of relation r. It is defined
as exp(MSR(r)+MRR(r)) and multiplied with
the base rate of three regularization methods, re-
spectively. The base rate of regularization meth-
ods is KG-based, shared across all relations in the
same KG.

Table 1: Statistics of the datasets. #Rel. and #Tri.
doesn’t include the inverse triples.

Dataset #Ent. #Rel. #Tri. #Task
FB15K-237 14,505 237 310,116 20
NELL-995 75,492 200 154,213 12

3.4 Overall Training Algorithm
Based on the proposed model, we present a novel
training algorithm shown in Algorithm 1.

One of our contributions in our algorithm is,
when the agent selects an invalid path, our model
not only penalizes it, but also forces it to select a
valid relation to step forward. The probabilities
from the neural network are normalized across all
valid relations, which in turn act the probabilities
of the forced action.

After the initializations, Line 6 samples an ac-
tion according to the output of the network. When
the agent selects an invalid action, Line 7 ∼ 10 is
executed, and Line 9 ∼ 10 forces the agent to step
forward. When the agent selects a valid action,
Line 12 is executed. Lines 19, 22 and 25 update
parameters for the invalid actions, the valid actions
in a successful episode, and the valid actions in
an unsuccessful episode, respectively, with reward
−1, Rtotal and Rshaping.

4 Experiments

In this section, we will perform extensive experi-
ments to validate the effectiveness of our proposed
AttnPath. For each task, we will mainly focus on
three quantitative metrics: success rate (SR) in
finding paths, MAP of fact prediction (FP), and
MAP of link prediction (LP). We will also demon-
strate some reasoning paths and triples to show
that graph attention is effective in finding more
high-quality paths and mining which aspect of the
entity is important in a specific task.

4.1 Datasets and Settings
Two datasets, FB15K-237 (Toutanova et al., 2015)
and NELL-995 (Xiong et al., 2017), are used in
our experiments. Statistics of these two datasets
are displayed in Table 1. Following the previous
works, for each triple (h, r, t), we add its inverse
triple (t, r−1, h), to allow the agent to step back-
ward.

We summary the hyperparameters involved in
our experiments here. The pretrained embedding
dimension is set to 100. The LSTM hidden di-
mension is set to 200. The attention dimension



2628

is set to 100. Thus, s is a 500-dimension vector
by concatenating the above three vectors with two
times of the pretrained embedding dimension. λ1
is 0.1, λ2 is 0.8, λ3 is 0.1. For FB15K-237 dataset,
we set base L2 regularization, Dropout rate and
action dropout rate to 0.005, 0.15, and 0.15, re-
spectively. Also, for NELL-995, we set them to
0.005, 0.1 and 0.1, respectively. We choose Adam
(Kingma and Ba, 2015) as the optimizer with dif-
ferent learning rates of 0.001, β1 0.9 and β2 0.999.
For each task, we train 500 episodes, and for each
episode, max steps is set to 50.

We verify the learned paths for each triple in
FP and LP tasks when training, by the BFS-based
method (Xiong et al., 2017).

4.2 Success Rate in Finding Paths
Our model is ignorant of the environment and
triples at first, since it doesn’t rely on pretraining.
Thus, we record total SR, and SR in the recent 10
episodes to validate the agent’s ability to learn the
paths. For the tasks with less than 500 training
samples, the samples are iterated first, then ran-
domly sampled to reach 500 episodes. For the
tasks with more than 500 training samples, we ran-
domly select 500 out for training.

We select two relations,
athleteP laysInLeague and
organizationHeadquarteredInCity from
NELL-995, to investigate its total SR and SR in
recent 10 episodes. The former has relatively
lower MSR and MRR, and the latter has higher.
Figure 2 shows the results. It can be discovered
that DeepPath outperforms our method at first,
however, after 50 ∼ 150 epochs, our models
surpass it. From SR-10 of AttnPath Force, we find
that the initial SR is approximate to MRR, because
the model knows nothing at all at first, so it selects
a path randomly. As the training proceeds, the
performance gains steady improvement. Similar
results can also be found in other relations on
FB15K-237.

4.3 Fact Prediction
Fact prediction (FP) aims at predicting whether an
unknown fact is true or false. The proportion of
positive triples and negative triples is about 1 : 10.
For each relation, we use all found paths and recip-
rocal of length as weight, to accumulate the score
of each triple based on whether the path is valid
between h and t. Scores are ranked across all test
set and Mean Average Precision (MAP) is used as

Table 2: Fact prediction MAP (%). Results of TransE /
H / R / D are cited from (Xiong et al., 2017). DeepPath
is retrained with TransD for a fair comparison, which
performs slightly better than (Xiong et al., 2017) based
on TransE.

Method FB15K-237 NELL-995
TransE 27.7 38.3
TransH 30.9 38.9
TransR 30.2 40.6
TransD 30.3 41.3

DeepPath TransD 31.3 53.5
AttnPath 31.5 59.8

AttnPath MS / RR 34.6 65.4
AttnPath Force 37.9 69.3

the evaluation metric. The results are shown in
Table 2. It can be seen that AttnPath outperforms
TransE / R and DeepPath significantly. AttnPath
MS / RR, which uses MSR and MRR to fine-tune
the hyperparameters, also gains performance im-
provement. AttnPath Force is also effective. By
forcing the agent to walk forward every step, it
improves the SR of finding a path, which in turn
enriches the feature of the downstream tasks. This
is especially important for the relations, which
lack direct-connected replacement paths and re-
quire path with long-term dependency. In fact,
our methods achieve SOTA results on both the two
datasets.

4.4 Link Prediction

Link prediction (LP) aims at predicting the target
entities. For each (h, r) pair, there is a ground
truth t and about 10 generated false t. It is di-
vided into a training set and a test set. We use the
found paths as binary features, and train a classifi-
cation model on the training set, applying it on the
test set. LP also uses MAP as the evaluation met-
ric, and the detailed results are shown in Table 3
1. Alike FP, our model also attains better results in
LP, and the SOTA results on FB15K-237 dataset2.

However, we also notice that AttnPath doesn’t
attain the best result under a small part of query
relations, even lower than TransE / R. By analyz-
ing triples related to these relations, we found that:
1) they have more outgoing edges of the other re-
lations pointing to the entities which are not the

1We omit to show detailed result of TransH / D here, while
(Xiong et al., 2017) also doesn’t give detailed result of it,
and it is mentioned in this paper that vanilla DeepPath al-
ready outperforms main embedding-based methods, includ-
ing TransE / H / R / D.

2DIVA (Chen et al., 2018) claims that it attains 88.6%
MAP on NELL-995 dataset.



2629

Figure 2: Total SR and SR-10 for two relations of NELL-995. DeepPath / AttnPath TransD means using TransD
as the pretrained embeddings. AttnPath MS/RR is adding MSR and MRR to fine-tune hyperparameters. AttnPath
Force is forcing the agent to move forward every step. These abbreviations are used throughout this section.

Table 3: Link Prediction MAP. Top half is FB15K-237, bottom half is NELL-995. TE, TR, DP, AP, APM and
APF are abbreviations of TransE, TransR, DeepPath, AttnPath, AttnPath MS/RR, AttnPath Force, respectively.
The result of TransE / R is cited from (Xiong et al., 2017). All the numbers are percentage (%).

Tasks MSR MRR TE TR DP AP APM APF
filmDirector 7.5 11.9 38.6 39.9 43.4 40.9 43.3 43.7

filmLanguage 3.7 0.1 64.2 64.1 70.1 56.6 70.9 71.8
filmWrittenBy 2.6 4.3 56.3 60.5 45.4 55.6 56.3 58.9

capitalOf 8.8 16.1 55.4 49.3 79.3 86.0 86.7 87.2
musicianOrigin 3.0 1.5 36.1 37.9 51.4 47.9 51.0 52.6

organizationFounded 4.2 1.3 39.0 33.9 31.8 31.1 34.9 46.8
personNationality 4.6 0.1 64.1 72.0 83.6 74.6 84.6 84.6

birthPlace 3.7 1.6 40.3 41.7 55.0 49.9 54.4 54.4
teamSports 6.7 0 89.6 78.4 91.6 70.2 91.2 91.3
tvLanguage 5.3 0.3 80.4 90.6 96.7 95.9 97.1 96.8

...
Overall — — 53.2 54.0 63.5 58.5 65.3 66.1

athleteHomeStadium 32.4 0.0 71.8 72.2 89.3 89.3 88.7 89.4
athletePlaysForTeam 42.5 5.6 62.7 67.3 76.0 74.6 74.3 76.1
athletePlaysInLeague 34.9 0.3 77.3 91.2 96.2 95.6 95.1 96.5

athletePlaysSport 60.9 0 87.6 96.3 96.3 96.5 94.3 97.0
orgHeadquarterCity 42.6 25.0 62.0 65.7 92.8 92.9 93.2 94.1

orgHiredPerson 24.9 18.2 71.9 73.7 75.4 79.7 79.8 81.6
bornLocation 34.7 16.4 71.2 81.2 75.9 76.7 74.6 78.6

personLeadsOrg 32.6 36.9 75.1 77.2 79.6 80.9 81.0 82.8
teamPlaysSport 19.9 0 76.1 81.4 74.1 75.7 75.5 82.1

worksFor 45.6 27.7 67.7 69.2 71.2 71.3 71.8 77.5
...

Overall — — 73.7 78.9 82.9 83.4 83.0 85.8

true tails, so MSR of these query relations are
low. 2) Tail entities are only connected with edges
of query relations and their inverse, while these
edges are removed during training, so tail entities
become isolated, without any possible replaceable
paths. It will also lower MRR of these query rela-
tions. Take birthP lace and bornLocation as ex-
amples. If a person was born in a remote place,
this place is difficult to be connected with the
other entities, so it is prone to be isolated. How-
ever, such one-to-one relations are the strength of
TransX methods.

4.5 Qualitative Analysis

4.5.1 Paths Found By DeepPath and
AttnPath

We take capitalOf from FB15K-237, and
athleteP laysInLeague from NELL-995, as ex-

amples, to analyze these paths found by DeepPath
and AttnPath. Table 4 shows the top 5 frequent
paths and their frequencies for all the methods. It
shows that AttnPath is more capable of capturing
long-term dependencies between relations, which
is useful for relations lack of direct-connected re-
placeable paths. AttnPath can also find more im-
portant and concentrated paths, so the distribution
of paths doesn’t have a heavy long tail. In the
training process, we also find that AttnPath is bet-
ter at stepping backward when it enters a dead end.

4.5.2 Effectiveness of Graph Attention
Mechanism

We sample several pairs of entity and relation,
compute the entity’s attention weights to its neigh-
bors under this relation, and investigate neighbors
with top 5 attention weights. Table 5 displays the
examples. It shows that GAT is capable of pay-



2630

Table 4: Top 5 frequent paths found for capitalOf (FB15K-237) and athleteP laysInLeague (NELL-995) using
two methods. ” inv” indicates inverse relation.

Relation Reasoning Path

capitalOf
(DeepPath)

location−1 (83.2%); location−1 → location−1 (5.3%); administrativeArea−1 (4.4%); location−1
→ location (1.8%); marriage−1 → spouse−1 → awardWinner → awardNomination
→ currency → rent−1 (0.9%)

capitalOf
(AttnPath)

location−1 (89.1%); county (3.0%); location−1 → location−1 (2.4%); usCounty (1.8%);
administrativeArea−1 (1.2%)

athletePlaysInLeague
(DeepPath)

athleteplaysforteam → teamplaysinleague (28.5%); athleteplayssport → teamplayssport inv
→ teamplaysinleague (24.5%); athleteledsportsteam → teamplaysinleague (13.2%);
athleteflyouttosportsteamposition → athleteflyouttosportsteamposition inv → athleteplaysforteam
→ teamplaysinleague (2.4%); athletehomestadium → leaguestadiums inv (2.4%)

athletePlaysInLeague
(AttnPath)

athleteplayssport → teamplayssport inv → teamplaysinleague (70.0%); athleteplaysforteam
→ teamplaysinleague (19.5%); athleteledsportsteam → teamplaysinleague (4.3%);
athleteflyouttosportsteamposition → athleteflyouttosportsteamposition inv → athleteplayssport
→ teamplayssport inv → teamplaysinleague (3.4%); athleteplaysforteam
→ teamplaysagainstteam inv → teamplaysinleague (0.6%)

Table 5: Samples displaying which neighbor does the model pays the most attention to under certain relation. Bold
Text means this entity is related to the query relation. The first four pairs are from FB15K-237 and the latter four
are from NELL-995.

Entity Relation Neighbors
Ventura location The United States, California, Pacific Time Zone, Marriage, The Rock

Sikh peopleLanguagesSpoken Malay, Thai, Bhubaneswar, Hindi, Pashto

Anthony Minghella personNationality
United Kingdom, England, BAFTA Award for Best Direction,
BAFTA Award for Best Film, University of Hull

filmDirector BAFTA Award for Best Adapted Screenplay, The Talented Mr. Ripley,Academy Award for Best Picture, England, Film Producer

Elon Musk personLeadsOrganization Tesla Motors, Paypal, Tesla, SpaceX Museum, SpaceX company

New Jersey Devils teamPlaysSport Hockey, Stanley Cup, Games, Capitals, Buffalo Sabres

Brandon Marshall athleteHomeStadium Invesco Field, Broncos, Golf, Super Bowl, Super Bowl XLIIathletePlaysSport Golf, Broncos, Invesco Field, Super Bowl XLII, Super Bowl

ing more attention to the neighbors, which is re-
lated to the query relation, especially for Anthony
Minghella and Brandon Marshall, which pay dif-
ferent attention to neighbors with different query
relations.

5 Conclusion and Future Work

In this paper, we propose AttnPath, a DRL based
model for KG reasoning task which incorporates
LSTM and Graph Attention Mechanism as mem-
ory components, to alleviate the model from pre-
training. We also invent two metrics, MSR and
MRR, to measure the learning difficulty of rela-
tions, and use it to better fine-tune training hy-
perparameters. We improve the training process
to avoid the agent stalling in a meaningless state.
Qualitative experiments and quantitative analysis
show that our method outperforms DeepPath and
embedding-based method significantly, proving its
effectiveness.

In the future, we are interested in utilizing
multi-task learning, to enable the model to learn

reasoning paths for several query relations simul-
taneously. We would also like to research how to
utilize GAT, MSR and MRR into other KG related
tasks, such as KG representation, relation cluster-
ing and KB-QA.

Acknowledgments

This work is supported by the National Key
R&D Program of China (2018YFB1004404),
Key R&D Program of Guangdong Province
(2018B010107005), National Natural Science
Foundation of China (U1711262, U1401256,
U1501252, U1611264, U1711261).

Rong Pan is the corresponding author.

References
Kurt Bollacker, Colin Evans, Praveen Paritosh, Tim

Sturge, and Jamie Taylor. 2008. Freebase: a col-
laboratively created graph database for structuring
human knowledge. In SIGMOD, pages 1247–1250.

Antoine Bordes, Nicolas Usunier, Alberto Garcia-



2631

Duran, Jason Weston, and Oksana Yakhnenko.
2013. Translating embeddings for modeling multi-
relational data. In NeurIPS, pages 2787–2795.

Andrew Carlson, Justin Betteridge, Bryan Kisiel,
Burr Settles, Estevam R Hruschka Jr, and Tom M
Mitchell. 2010. Toward an architecture for never-
ending language learning. In AAAI, volume 5, pages
1306–1313.

Wenhu Chen, Wenhan Xiong, Xifeng Yan, and William
Wang. 2018. Variational knowledge graph reason-
ing. In NAACL-HLT, pages 1823–1832.

Rajarshi Das, Shehzaad Dhuliawala, Manzil Zaheer,
Luke Vilnis, Ishan Durugkar, Akshay Krishna-
murthy, Alex Smola, and Andrew McCallum. 2018.
Go for a walk and arrive at the answer: Reasoning
over paths in knowledge bases using reinforcement
learning. In ICLR.

Tim Dettmers, Pasquale Minervini, Pontus Stenetorp,
and Sebastian Riedel. 2018. Convolutional 2d
knowledge graph embeddings. In AAAI, pages
1811–1818.

Mohnish Dubey, Debayan Banerjee, Debanjan Chaud-
huri, and Jens Lehmann. 2018. Strong baselines for
simple question answering over knowledge graphs
with and without neural networks. In ISWC, pages
108–126.

Kelvin Guu, John Miller, and Percy Liang. 2015.
Traversing knowledge graphs in vector space. In
EMNLP, pages 318–327.

Guoliang Ji, Shizhu He, Liheng Xu, Kang Liu, and Jun
Zhao. 2015. Knowledge graph embedding via dy-
namic mapping matrix. In ACL, volume 1, pages
687–696.

Diederik P Kingma and Jimmy Ba. 2015. Adam: A
method for stochastic optimization. In ICLR.

Ni Lao, Tom Mitchell, and William W Cohen. 2011.
Random walk inference and learning in a large scale
knowledge base. In EMNLP, pages 529–539.

Xi Victoria Lin, Richard Socher, and Caiming Xiong.
2018. Multi-hop knowledge graph reasoning with
reward shaping. In EMNLP, pages 3243–3253.

Yankai Lin, Zhiyuan Liu, Maosong Sun, Yang Liu, and
Xuan Zhu. 2015. Learning entity and relation em-
beddings for knowledge graph completion. In AAAI,
volume 15, pages 2181–2187.

Zhenghao Liu, Chenyan Xiong, Maosong Sun, and
Zhiyuan Liu. 2018. Entity-duet neural ranking: Un-
derstanding the role of knowledge graph semantics
in neural information retrieval. In ACL, pages 2395–
2405.

George A Miller. 1995. Wordnet: a lexical database for
english. Communications of the ACM, 38(11):39–
41.

Arvind Neelakantan, Benjamin Roth, and Andrew Mc-
Callum. 2015. Compositional vector space models
for knowledge base completion. In ACL, pages 156–
166.

Yelong Shen, Jianshu Chen, Po-Sen Huang, Yuqing
Guo, and Jianfeng Gao. 2018. M-walk: Learning
to walk over graphs using monte carlo tree search.
In NeurIPS, pages 6787–6798.

Kristina Toutanova, Danqi Chen, Patrick Pantel, Hoi-
fung Poon, Pallavi Choudhury, and Michael Gamon.
2015. Representing text for joint embedding of text
and knowledge bases. In EMNLP, pages 1499–
1509.

Kristina Toutanova, Victoria Lin, Wen-tau Yih, Hoi-
fung Poon, and Chris Quirk. 2016. Compositional
learning of embeddings for relation paths in knowl-
edge base and text. In ACL, volume 1, pages 1434–
1444.

Petar Velickovic, Guillem Cucurull, Arantxa Casanova,
Adriana Romero, Pietro Lio, and Yoshua Bengio.
2018. Graph attention networks. In ICLR.

William Yang Wang and William W Cohen. 2016.
Learning first-order logic embeddings via matrix
factorization. In IJCAI, pages 2132–2138.

Xiang Wang, Dingxian Wang, Canran Xu, Xiangnan
He, Yixin Cao, and Tat-Seng Chua. 2019. Explain-
able reasoning over knowledge graphs for recom-
mendation. In AAAI.

Zikang Wang, Linjing Li, Daniel Dajun Zeng, and Yue
Chen. 2018. Attention-based multi-hop reasoning
for knowledge graph. In ISI, pages 211–213.

Ronald J Williams. 1992. Simple statistical gradient-
following algorithms for connectionist reinforce-
ment learning. Machine learning, 8(3-4):229–256.

Wenhan Xiong, Thien Hoang, and William Yang
Wang. 2017. Deeppath: A reinforcement learning
method for knowledge graph reasoning. In EMNLP,
pages 564–573.

Fan Yang, Zhilin Yang, and William W Cohen. 2017.
Differentiable learning of logical rules for knowl-
edge base reasoning. In NeurIPS, pages 2319–2328.


