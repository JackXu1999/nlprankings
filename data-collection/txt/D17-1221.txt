



















































Cascaded Attention based Unsupervised Information Distillation for Compressive Summarization


Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 2081–2090
Copenhagen, Denmark, September 7–11, 2017. c©2017 Association for Computational Linguistics

Cascaded Attention based Unsupervised Information Distillation for
Compressive Summarization∗

Piji Li† Wai Lam† Lidong Bing‡ Weiwei Guo§ Hang Li\
†Department of Systems Engineering and Engineering Management,

The Chinese University of Hong Kong
‡AI Lab, Tencent Inc., Shenzhen, China
§LinkedIn, Mountain View, CA, USA

\Noah’s Ark Lab, Huawei Technologies, Hong Kong
†{pjli, wlam}@se.cuhk.edu.hk, ‡lyndonbing@tencent.com

§wguo@linkedin.com, \hangli.hl@huawei.com

Abstract

When people recall and digest what they
have read for writing summaries, the im-
portant content is more likely to attract
their attention. Inspired by this obser-
vation, we propose a cascaded attention
based unsupervised model to estimate the
salience information from the text for
compressive multi-document summariza-
tion. The attention weights are learned
automatically by an unsupervised data re-
construction framework which can capture
the sentence salience. By adding sparsity
constraints on the number of output vec-
tors, we can generate condensed informa-
tion which can be treated as word salience.
Fine-grained and coarse-grained sentence
compression strategies are incorporated to
produce compressive summaries. Experi-
ments on some benchmark data sets show
that our framework achieves better results
than the state-of-the-art methods.

1 Introduction

The goal of Multi-Document Summarization
(MDS) is to automatically produce a succinct
summary, preserving the most important informa-
tion of a set of documents describing a topic1

(Luhn, 1958; Edmundson, 1969; Goldstein et al.,
2000; Erkan and Radev, 2004b; Wan et al., 2007;
Nenkova and McKeown, 2012). Considering the
procedure of summary writing by humans, when
people read, they will remember and forget part

∗The work described in this paper is supported by grants
from the Research and Development Grant of Huawei Tech-
nologies Co. Ltd (YB2015100076/TH1510257) and the
Grant Council of the Hong Kong Special Administrative Re-
gion, China (Project Code: 14203414).

1A topic represents a real event, e.g., “AlphaGo versus
Lee Sedol”.

of the content. Information which is more impor-
tant may make a deep impression easily. When
people recall and digest what they have read to
write summaries, the important information usu-
ally attracts more attention (the behavioral and
cognitive process of selectively concentrating on
a discrete aspect of information, whether deemed
subjective or objective, while ignoring other per-
ceivable information2) since it may repeatedly ap-
pears in some documents, or be positioned in the
beginning paragraphs.

In the context of multi-document summariza-
tion, to generate a summary sentence for a key as-
pect of the topic, we need to find its relevant parts
in the original documents, which may attract more
attention. The semantic parts with high atten-
tion weights plausibly represent and reconstruct
the topic’s main idea. To this end, we propose a
cascaded neural attention model to distill salient
information from the input documents in an un-
supervised data reconstruction manner, which in-
cludes two components: reader and recaller. The
reader is a gated recurrent neural network (LSTM
or GRU) based sentence sequence encoder which
can map all the sentences of the topic into a global
representation, with the mechanism of remember-
ing and forgetting. The recaller decodes the global
representation into significantly fewer diversified
vectors for distillation and concentration. A cas-
caded attention mechanism is designed by incor-
porating attentions on both the hidden layer (dense
distributed representation of a sentence) and the
output layer (sparse bag-of-words representation
of summary information). It is worth noting that
the output vectors of the recaller can be viewed
as word salience, and the attention matrix can be
used as sentence salience. Both of them are auto-
matically learned by data reconstruction in an un-

2https://en.wikipedia.org/wiki/Attention (Apr., 2017)

2081



supervised manner. Thereafter, the word salience
is fed into a coarse-grained sentence compression
component. Finally, the attention weights are in-
tegrated into a phrase-based optimization frame-
work for compressive summary generation.

In fact, the notion of “attention” has gained
popularity recently in neural network modeling,
which has improved the performance of many
tasks such as machine translation (Bahdanau et al.,
2015; Luong et al., 2015). However, very few
previous works employ attention mechanism to
tackle MDS. Rush et al. (2015) and Nallapati
et al. (2016) employed attention-based sequence-
to-sequence (seq2seq) framework only for sen-
tence summarization. Gu et al. (2016), Cheng
and Lapata (2016), and Nallapati et al. (2016) also
utilized seq2seq based framework with attention
modeling for short text or single document sum-
marization. Different from their works, our frame-
work aims at conducting multi-document summa-
rization in an unsupervised manner.

Our contributions are as follows: (1) We pro-
pose a cascaded attention model that captures
salient information in different semantic represen-
tations. (2) The attention weights are learned au-
tomatically by an unsupervised data reconstruc-
tion framework which can capture the sentence
salience. By adding sparsity constraints on the
number of output vectors of the recaller, we can
generate condensed vectors which can be treated
as word salience; (3) We thoroughly investigate
the performance of combining different attention
architectures and cascaded structures. Experimen-
tal results on some benchmark data sets show that
our framework achieves better performance than
the state-of-the-art models.

2 Framework Description

2.1 Overview

Our framework has two phases, namely, in-
formation distillation for finding salient
words/sentences, and compressive summary
generation. For the first phase, our cascaded neu-
ral attention model consists of two components:
reader and recaller as shown in Figure 1. The
reader component reads in all the sentences in the
document set corresponding to the topic/event.
The information distillation happens in the re-
caller component where only the most important
information is preserved. Precisely, the recaller
outputs fewer vectors s than that of the input

Enc

Dec

Figure 1: Our cascaded attention based unsuper-
vised information distillation framework. X is the
original input sentence sequence of a topic. H i is
the hidden vectors of sentences. “Enc” and “Dec”
represent the RNN-based encoding and decoding
layer respectively. cg is the global representation
for the whole topic. Ah and Ao are the distilled
attention matrices for the hidden layer and the out-
put layer respectively, representing the salience of
sentences. Ho is the output hidden layer. s1 and
s2 are the distilled condensed vectors representing
the salience of words. Note that they are neither
origin inputs nor golden summaries.

sentences x for the reader.
After the learning of the neural attention model

finishes, the obtained salience information will be
used in the second phase for compressive sum-
mary generation. This phase consists of two com-
ponents: (i) the coarse-grained sentence compres-
sion component which can filter the trivial infor-
mation based on the output vectors S from the
neural attention model; (ii) the unified phrase-
based optimization method for summary genera-
tion in which the attention matrix Ao is used to
conduct fine-grained compression and summary
construction.

2.2 Attention Modeling for Distillation

2.2.1 Reader
In the reader stage, for each topic, we extract all
the sentences X = {x1, x2, . . . , xm} from the set
of input documents corresponding to a topic and
generate a sentence sequence with length m. The
sentence order is the same as the original order of
the documents. Then the reader reads the whole
sequence sentence by sentence. We employ the
bag-of-words (BOW) representation as the initial
semantic representation for sentences. Assume

2082



that the dictionary size is k, then xi ∈ Rk.
Sparsity is one common problem for the BOW

representation, especially when each vector is gen-
erated from a single sentence. Moreover, down-
stream algorithms might suffer from the curse of
dimensionality. To solve these problems, we add
a hidden layer Hv (v for input layer) which is a
densely distributed representation above the input
layer as shown in Figure 1. Such distributed rep-
resentation can provide better generalization than
BOW representation in many different tasks (Le
and Mikolov, 2014; Mikolov et al., 2013). Specif-
ically, the input hidden layer will project the input
sentence vector xj to a new space Rh according
to Equation 1. Then we obtain a new sentence se-
quence Hv = [hv1, h

v
2, . . . , h

v
m].

hvj = tanh(W
v
xhxj + b

v
h) (1)

where W vxh and b
v
h are the weight and bias respec-

tively. The superscript v means that the variables
are from the input layer.

While reading the sentence sequence, the reader
should have the ability of remembering and for-
getting. Therefore, we employ the RNN models
with various gates (input gate, forget gate, etc.)
to imitate the remembering and forgetting mech-
anism. Then the RNN based neural encoder (the
third layer in Figure 1) will map the whole embed-
ding sequence to a single vector cg which can be
regarded as a global representation for the whole
topic. Let t be the index of the sequence state for
the sentence xt, the hidden unit het (e for encoder
RNN) of the RNN encoder can be computed as:

het = f(h
e
t−1, h

v
t ) (2)

where the RNN f(·) computes the current hidden
state given the previous hidden state het−1 and the
sentence embedding hvt . The encoder generates
hidden states {het} over all time steps. The last
state {hem} is extracted as the global representa-
tion cg for the whole topic. The structure for f(·)
can be either an LSTM (Hochreiter and Schmid-
huber, 1997) or GRU (Cho et al., 2014).

2.2.2 Recaller
The recaller stage is a reverse of the reader stage,
but it outputs less number of vectors in S as shown
in Figure 1. Given the global representation cg, the
past hidden state hdt−1 (d for decoder RNN) from
the decoder layer, an RNN based decoder gener-
ates several hidden states according to:

hdt = f(h
d
t−1, cg) (3)

We use cg to initialize the first decoder hidden
state. The decoder will generate several hidden
states {hdt } over pre-defined time steps. Then,
similar to the reader stage, we add an output hid-
den layer after the decoder layer:

hot = tanh(W
o
hhh

d
t + b

o
h) (4)

where W ohh and b
o
h are the weight and bias respec-

tively for the projection from hdt to h
o
t . Finally, the

output layer maps these hidden vectors to the con-
densed vectors S = [s1, s2, . . . , sn], Each output
vector st has the same dimension k as the input
BOW vectors and is obtained as follows:

st = σ(Whshot + bs) (5)

For the purpose of distillation and concentration,
we restrict n to be very small.

2.2.3 Cascaded Attention Modeling
Salience estimation for words and sentences is a
crucial component in MDS, especially in the un-
supervised summarization setting. We propose a
cascaded attention model for information distil-
lation to tackle the salience estimation task for
MDS. We add attention mechanism not only in
the hidden layer, but also in the output layer. By
this cascaded attention model, we can capture the
salience of sentences from two different and com-
plementary vector spaces. One is the embedding
space that provides better generalization, and the
other one is the BOW vector space that captures
more nuanced and subtle difference.

For each output hidden state hot , we align it with
each input hidden state hvi by an attention vector
aht,i ∈ Rm (recall that m is the number of input
sentences). aht,i is derived by comparing h

o
t with

each input sentence hidden state hvi :

aht,i =
exp(score(hot , h

v
i ))∑

i′ exp(score(h
o
t , h

v
i′))

(6)

where score(·) is a content-based function to cap-
ture the relation between two vectors. Several dif-
ferent formulations can be used as the function
score(·) which will be elaborated later.

Based on the alignment vectors {aht,i}, we can
create a context vector cht by linearly blending the
sentence hidden states {hvi′}:

cht =
∑

i′
aht,i′h

v
i′ (7)

Then the output hidden state can be updated based
on the context vector. Let h̃ot = h

o
t , then update the

2083



original state according to the following operation:

hot = tanh(W
a
chc

h
t +W

a
hhh̃

o
t ) (8)

The alignment vector aht,i captures which sentence
should be attended more in the hidden space when
generating the condensed representation for the
whole topic.

Besides the attention mechanism on the hidden
layer, we also directly add attention on the out-
put BOW layer which can capture more nuanced
and subtle difference information from the BOW
vector space. The hidden attention vector aht,i is
integrated with the output attention by a weight
λa ∈ [0, 1]:

āot,i =
exp(score(st, xi))∑
i′ exp(score(st, xi′))

(9)

aot,i = λaā
o
t,i + (1− λa)aht,i (10)

The output context vector is computed as:

cot =
∑

i′
aot,i′xi′ (11)

To update the output vector st in Equation 5, we
develop a different method from that of the hidden
attentions. Specifically we use a weighted combi-
nation of the context vectors and the original out-
puts with λc ∈ [0, 1]. Let s̃t = st, then the updated
st is:

st = λccot + (1− λc)s̃t (12)
The parameters λa and λc can also be learned dur-
ing training.

There are several different alternatives for the
function score(·):

score(ht, hs) =

 ht
T hs dot

ht
T Whs tensor

vT tanh(W [ht; hs]) concat
(13)

Considering their behaviors as studied in (Luong
et al., 2015), we adopt “concat” for the hidden
attention layer, and “dot” for the output attention
layer.

2.2.4 Unsupervised Learning
By minimizing the loss owing to using the con-
densed output vectors to reconstruct the original
input sentence vectors, we are able to learn the so-
lutions for all the parameters as follows.

min
Θ

1
2m

m∑
i=1

‖xi −
n∑

j=1

sja
o
j,i‖22 + λs‖S‖1 (14)

where Θ denotes all the parameters in our model.
In order to penalize the unimportant terms in the
output vectors, we put a sparsity constraint on the
rows of S using l1-regularization, with the weight
λs as a scaling constant for determining its relative
importance.

Let s̄ be the magnitude vector computed from
the columns in S (S ∈ Rn×k). Once the train-
ing is finished, each dimension of the vector s̄ can
be regarded as the word salience score. Accord-
ing to Equation 14, si ∈ S is used to reconstruct
the original sentence space X , and n � m (the
number of sentences in X is much more than the
number of vectors in S) Therefore a large value
in s̄ means that the corresponding word contains
important information about this topic and it can
serve as the word salience.

Moreover, the output layer attention matrix
Ao can be regarded as containing the sentence
salience information. Note that each output vec-
tor si is generated based on the cascaded atten-
tion mechanism. Assume that aoi = A

o
i,: ∈ Rm

is the attention weight vector for si. According to
Equation 9, a large value in aoi conveys a meaning
that the corresponding sentence should contribute
more when generating si. We also use the magni-
tude of the columns inAo to represent the salience
of sentences.

2.3 Compressive Summary Generation Phase

2.3.1 Coarse-grained Sentence Compression

Using the information distillation result from the
cascaded neural attention model, we conduct
coarse-grained compression for each individual
sentence. Such strategy has been adopted in some
multi-document summarization methods (Li et al.,
2013; Wang et al., 2013; Yao et al., 2015). Our
coarse-grained sentence compression jointly con-
siders word salience obtained from the neural at-
tention model and linguistically-motivated rules.
The linguistically-motivated rules are designed
based on the observed obvious evidence for uncrit-
ical information from the word level to the clause
level, which include news headers such as “BEI-
JING, Nov. 24 (Xinhua) –”, intra-sentential at-
tribution such as “, police said Thursday”, “, he
said”, etc. The information filtered by the rules
will be processed according to the word salience
score. Information with smaller salience score
(< �) will be removed.

2084



2.3.2 Phrase-based Optimization for
Summary Construction

After coarse-grained compression on each single
sentence as described above, we design a uni-
fied optimization method for summary generation.
We refine the phrase-based summary construction
model in (Bing et al., 2015) by adjusting the goal
as compressive summarization. We consider the
salience information obtained by our neural atten-
tion model and the compressed sentences in the
coarse-grained compression component.

Based on the parsed constituency tree for each
input sentence as described in Section 2.3.1, we
extract the noun-phrases (NPs) and verb-phrases
(VPs). The salience Si of a phrase Pi is defined
as:

Si = {
∑
t∈Pi

tf(t)/
∑

t∈Topic
tf(t)} × ai (15)

where ai is the salience of the sentence containing
Pi. tf(t) is the frequency of the concept t (uni-
gram/bigram) in the whole topic. Thus, Si inherits
the salience of its sentence, and also considers the
importance of its concepts.

The overall objective function of our optimiza-
tion formulation for selecting salient NPs and VPs
is formulated as an integer linear programming
(ILP) problem:

max{
∑

i
αiSi−

∑
i<j

αij(Si + Sj)Rij} (16)

where αi is the selection indicator for the phrase
Pi, Si is the salience scores of Pi, αij and Rij
is the co-occurrence indicator and the similarity
of a pair of phrases (Pi, Pj) respectively. The
similarity is calculated by the Jaccard Index based
method. Specifically, this objective maximizes the
salience score of the selected phrases as indicated
by the first term, and penalizes the selection of
similar phrase pairs.

In order to obtain coherent summaries with
good readability, we add some constraints into the
ILP framework such as sentence generation con-
straint: Let βk denote the selection indicator of
the sentence xk. If any phrase from xk is selected,
βk = 1. Otherwise, βk = 0. For generating a
compressed summary sentence, it is required that
if βk = 1, at least one NP and at lease one VP of
the sentence should be selected. It is expressed as:

∀Pi ∈ xk, αi ≤ βk ∧
∑

i
αi ≥ βk, (17)

Other constraints include sentence number, sum-
mary length, phrase co-occurrence, etc. For de-
tails, please refer to McDonald (2007), Woodsend
and Lapata (2012), and Bing et al. (2015).

The objective function and constraints are lin-
ear. Therefore the optimization can be solved
by existing ILP solvers such as the simplex algo-
rithm (Dantzig and Thapa, 2006). In the imple-
mentation, we use a package called lp solve3.

In the post-processing, the phrases and sen-
tences in a summary are ordered according to their
natural order if they come from the same docu-
ment. Otherwise, they are ordered according to
the timestamps of the corresponding documents.

3 Experimental Setup

3.1 Datasets

DUC: Both DUC 2006 and DUC 2007 are used
in our evaluation. DUC 2006 and DUC 2007 con-
tain 50 and 45 topics respectively. Each topic has
25 news documents and 4 model summaries. The
length of the model summary is limited to 250
words. TAC: We also use TAC 2010 and TAC
2011 in our experiments. TAC 2011 is the latest
standard summarization benchmark data set and
it contains 44 topics. Each topic falls into one
of 5 predefined event categories and contains 10
related news documents and 4 model summaries.
TAC 2010 is used as the parameter tuning data set
of our TAC evaluation.

3.2 Settings

For text processing, the input sentences are repre-
sented as BOW vectors with dimension k. The
dictionary is created using unigrams and named
entity terms. The word salience threshold � used
in sentence compression is 0.005. For the neu-
ral network framework, we set the hidden size as
500. All the neural matrix parametersW in hidden
layers and RNN layers are initialized from a uni-
form distribution between [−0.1, 0.1]. Adadelta
(Schmidhuber, 2015) is used for gradient based
optimization. Gradient clipping is adopted by
scaling gradients then the norm exceeded a thresh-
old of 10. The maximum epoch number in the op-
timization procedure is 200. We limit the num-
ber of distilled vectors n = 5. The attention cas-
caded parameter λa and λc can be learned by our
model. The sparsity penalty λs in Equation 14 is

3http://lpsolve.sourceforge.net/5.5/

2085



Table 1: Comparisons on TAC 2010

System R-1 R-2 R-SU4
CW 0.353 0.092 0.123
SC 0.346 0.083 0.116
AttenC-tensor-gru 0.339 0.078 0.115
AttenC-concat-gru 0.353 0.089 0.121
AttenC-dot-lstm 0.352 0.089 0.121
AttenH-dot-gru 0.348 0.086 0.119
AttenO-dot-gru 0.348 0.085 0.118
AttenC-dot-gru 0.359 0.092 0.124
(w\o coarse-comp) 0.351 0.089 0.122

0.001. Our neural network based framework is im-
plemented using Theano (Bastien et al., 2012) on
a single GPU of Tesla K80.

We use ROUGE score as our evaluation metric
(Lin, 2004) with standard options. F-measures of
ROUGE-1 (R-1), ROUGE-2 (R-2) and ROUGE-
SU4 (R-SU4) are reported.

4 Results and Discussions

4.1 Effect of Existing Salience Models and
Different Attention Architectures

We quantitatively evaluate the performance of dif-
ferent variants on the dataset of TAC 2010. The
experimental results are shown in Table 1. Note
that the summary generation phase for different
methods are the same, and only the salience es-
timation methods are different. Commonly used
existing methods for salience estimation include:
concept weight (CW) (Bing et al., 2015) and
sparse coding (SC) (Li et al., 2015). As men-
tioned in Section 2.2.3, there are several alterna-
tives for the attention scoring function score(·):
dot, tensor, and concat. Moreover, we also de-
sign experiments to show the benefit of our cas-
caded attention mechanism versus the single atten-
tion method. AttenC denotes the cascaded atten-
tion mechanism. AttenH and AttenO represent
the attention only on the hidden layer or the output
layer respectively without cascaded combination.

Among all the methods, the cascaded attention
model with dot structure achieves the best perfor-
mance. The effect of different RNN models, such
as LSTM and GRU, is similar. However, there
are less parameters in GRU resulting in improve-
ments for the efficiency of training. Therefore, we
choose AttenC-dot-gru as the attention structure
of our framework in the subsequent experiments.
Moreover, the results without coarse-grained sen-

Table 2: Results on DUC 2006.

System R-1 R-2 R-SU4
Random 0.280 0.046 0.088
Lead 0.308 0.048 0.087
LexRank 0.360 0.062 0.118
TextRank 0.373 0.066 0.125
MDS-Sparse 0.340 0.052 0.107
DSDR 0.377 0.073 0.117
RA-MDS 0.391 0.081 0.136
ABS-Phrase 0.392 0.082 0.137
C-Attention 0.393* 0.087* 0.141*

Table 3: Results on DUC 2007.

System R-1 R-2 R-SU4
Random 0.302 0.046 0.088
Lead 0.312 0.058 0.102
LexRank 0.378 0.075 0.130
TextRank 0.403 0.083 0.144
MDS-Sparse 0.353 0.055 0.112
DSDR 0.398 0.087 0.137
RA-MDS 0.408 0.097 0.150
ABS-Phrase 0.419 0.103 0.156
C-Attention 0.423* 0.107* 0.161*

tence compression (Section 2.3.1) show that the
compression can indeed improve the sumamriza-
tion performance.

4.2 Main Results of Compressive MDS

We compare our system C-Attention with sev-
eral unsupervised summarization baselines and
state-of-the-art models. Random baseline se-
lects sentences randomly for each topic. Lead
baseline (Wasson, 1998) ranks the news chrono-
logically and extracts the leading sentences one
by one. TextRank (Mihalcea and Tarau, 2004)
and LexRank (Erkan and Radev, 2004a) esti-
mate sentence salience by applying the PageRank
algorithm to the sentence graph. PKUTM (Li
et al., 2011) employs manifold-ranking for sen-
tence scoring and selection; ABS-Phrase (Bing
et al., 2015) generates abstractive summaries us-
ing phrase-based optimization framework. Three
other unsupervised methods based on sparse cod-
ing are also compared, namely, DSDR (He et al.,
2012), MDS-Sparse (Liu et al., 2015), and RA-
MDS (Li et al., 2015).

As shown in Table 2, Table 3, and Table 4, our
system achieves the best results on all the ROUGE

2086



Table 4: Results on TAC 2011.

System R-1 R-2 R-SU4
Random 0.303 0.045 0.090
Lead 0.315 0.071 0.103
LexRank 0.313 0.060 0.102
TextRank 0.332 0.064 0.107
PKUTM 0.396 0.113 0.148
ABS-Phrase 0.393 0.117 0.148
RA-MDS 0.400 0.117 0.151
C-Attention 0.400* 0.121* 0.153*

* Statistical significance tests show that our method is better
than the best baselines.

metrics. The reasons are as follows: (1) The
attention model can directly capture the salient
sentences, which are obtained by minimizing the
global data reconstruction error; (2) The cascaded
structure of attentions can jointly consider the
embedding vector space and bag-of-words vector
space when conducting the estimation of sentence
salience; (3) The coarse-grained sentence com-
pression based on distilled word salience, and the
fine-grained compression via phrase-based unified
optimization framework can generate more con-
cise and salient summaries. It is worth noting that
PKUTM used a Wikipedia corpus for providing
domain knowledge. The system SWING (Min
et al., 2012) is the best system for TAC 2011. Our
results are not as good as SWING. The reason
is that SWING employs category-specific features
and requires supervised training. These features
help them select better category-specific content
for the summary. In contrast, our model is basi-
cally unsupervised.

4.3 Linguistic Quality Evaluation

The linguistic quality of summaries generated by
ABS-Phrase, PKUTM, and our model from 20
topics of TAC 2011 is evaluated using the five lin-
guistic quality questions on grammaticality (Q1),
non-redundancy (Q2), referential clarity (Q3), fo-
cus (Q4), and coherence (Q5) in Document Un-
derstanding Conferences (DUC). A Likert scale
with five levels is employed with 5 being very
good with 1 being very poor. A summary was
blindly evaluated by three assessors on each ques-
tion. The results are given in Table 5. PKUTM
is an extractive method that picks the original sen-
tences, hence it achieves higher score in Q1 gram-
maticality. ABS-Phrase is an abstractive method
and can generate new sentences by merging differ-

Table 5: Evaluation of linguistic quality.

System Q1 Q2 Q3 Q4 Q5 AVG
ABS-Phrase 3.75 3.38 3.75 3.35 3.12 3.47
PKUTM 4.13 3.45 3.83 3.33 2.92 3.53
Ours 3.96 3.50 3.79 3.50 3.25 3.60

Table 6: Top-10 terms extracted from each topic
according to the word salience

Topic 1 Topic 2 Topic 3
school heart HIV

shooting disease Africa
Auvinen study circumcision
Finland risk study
police test infection
video blood trial

Wednesday red woman
gunman telomere drug

post level health

ent phrases, which decreases the grammaticality.
Grammaticality of our compression-based frame-
work is better than ABS-Phrase, but not as good
as PKUTM. However, our framework performs
the best on some other metrics such as Q2 (non-
redundancy) and Q4 (focus). The reason is that
our framework can compress and remove some
uncritical and redundancy content from the orig-
inal sentences, which leads to better performance
on Q2 and Q4.

4.4 Case Study: Distilled Word Salience

As mentioned above, the output vectors S in our
neural model contain the distilled word salience
information. In order to show the performance
of word salience estimation, we select 3 topics
(events) from different categories of TAC 2011:
“Finland Shooting”, “Heart Disease”, and “Hiv In-
fection Africa”. For each topic, we sort the dic-
tionary terms according to their salience scores,
and extract the top-10 terms as the salience esti-
mation results as shown in Table 6. We can see
that the top-10 terms reveal the most important in-
formation of each topic. For the topic “Finland
Shooting”, there is a sentence from the golden
summary “A teenager at a school in Finland went
on a shooting rampage Wednesday, November 11,
2007, killing 8 people, then himself.” It is obvious
that the top-10 terms from Table 6 can capture this
main point.

2087



4.5 Case Study: Attention-based Sentence
Salience

In our model, the distilled attention matrix Ao can
be treated as sentence salience estimation. Let â be
the magnitude of the columns in Ao and â ∈ Rm.
âi represents the salience of the sentence xi. We
collect all the attention vectors for 8 topics of TAC
2011, and display them as an image as shown in
Figure 2. The x-axis represents the sentence id (we
show at most 100 sentences), and the y-axis repre-
sents the topic id. The gray level of pixels in the
image indicates different salience scores, where
dark represents a high salience score and light
represents a small score. Note that different top-
ics seem to hold different ranges of salience scores
because they have different number of sentences,
i.e. m. According to Equation 9, topics contain-
ing more sentences will distribute the attention to
more units, therefore, each sentence will get a rela-
tively smaller attention weight. But this issue does
not affect the performance of MDS since different
topics are independently processed.

In Figure 2, there are some chunks in each topic
(see Topic 3 as an example) having higher atten-
tion weights, which indeed automatically captures
one characteristic of MDS: sentence position is
an important feature for news summarization. As
observed by several previous studies (Li et al.,
2015; Min et al., 2012), the sentences in the be-
ginning of a news document are usually more im-
portant and tend to be used for writing model sum-
maries. Manual checking verified that those high-
attention chunks correspond to the beginning sen-
tences. Our model is able to automatically capture
this information by assigning the latter sentences
in each topic lower attention weights.

4.6 Summary Case Analysis

Table 7 shows the summary of the topic “Hawkins
Robert Van Maur” in TAC 2011. The summary
contains four sentences, which are all compressed
with different compression ratio. Some uncrit-
ical information is excluded from the summary
sentences, such as “police said Thursday” in S2,
“But” in S3, and “he said” in S4. In addition, the
VP “killing eight people” in S2 is also excluded
since it is duplicate with the phrase “killed eight
people” in S3. Moreover, from the case we can
find that the compression operation did not harm
the linguistic quality.

Table 7: The summary of the topic “Hawkins
Robert Van Maur”.

S1: The young gunman who opened fire at a mall
busy with holiday shoppers appeared to choose his
victims at random, according to police[, but a note
he left behind hinted at a troubled life].
S2: The teenage gunman who went on a shooting
rampage in a department store, [killing eight peo-
ple,] may have smuggled an assault rifle into the
mall underneath clothing[, police said Thursday].
S3: [But] police said it was Hawkins who went
into an Omaha shopping mall on Wednesday and
began a shooting rampage that killed eight people.
S4: Mall security officers noticed Hawkins briefly
enter the Von Maur department store at Omaha’s
Westroads Mall earlier Wednesday[, he said].

5 Related Works

According to different machine learning
paradigms, summarization models can be
divided into supervised framework and unsuper-
vised framework. Some previous works have been
proposed based on unsupervised models. For
example, Mihalcea and Tarau (2004) and Erkan
and Radev (2004a) estimated sentence salience by
applying the PageRank algorithm to the sentence
graph. He et al. (2012), Liu et al. (2015), Li et al.
(2015) and Song et al. (2017) employed sparse
coding techniques for finding the salient sentences
as summaries. Li et al. (2017) conducted salience
estimation jointly considering reconstructions on
several different vector spaces generated by a
variational auto-ecoder framework.

Some recent works utilize attention modeling
based recurrent neural networks to tackle the task
of single-document summarization. Rush et al.
(2015) proposed a sentence summarization frame-
work based on a neural attention model using a
supervised sequence-to-sequence neural machine
translation model. Gu et al. (2016) combined a
copying mechanism with the seq2seq framework
to improve the quality of the generated summaries.
Nallapati et al. (2016) also employed the typi-
cal attention modeling based seq2seq framework,
but utilized a trick to control the vocabulary size
to improve the training efficiency. However, few
previous works employ attention mechanism to
tackle the unsupervised MDS problem. In con-
trast, our attention-based framework can gener-
ate summaries for multi-document summarization

2088



Figure 2: Visualization for sentence attention.

settings in an unsupervised manner.

6 Conclusions

We propose a cascaded neural attention based un-
supervised salience estimation method for com-
pressive multi-document summarization. The at-
tention weights for sentences and salience values
for words are both learned by data reconstruction
in an unsupervised manner. We thoroughly inves-
tigate the performance of combining different at-
tention architectures and cascaded structures. Ex-
perimental results on some benchmark data sets
show that our framework achieves good perfor-
mance compared with the state-of-the-art meth-
ods.

References
Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Ben-

gio. 2015. Neural machine translation by jointly
learning to align and translate. In ICLR.

Frédéric Bastien, Pascal Lamblin, Razvan Pascanu,
James Bergstra, Ian Goodfellow, Arnaud Bergeron,
Nicolas Bouchard, David Warde-Farley, and Yoshua
Bengio. 2012. Theano: new features and speed im-
provements. arXiv preprint arXiv:1211.5590.

Lidong Bing, Piji Li, Yi Liao, Wai Lam, Weiwei Guo,
and Rebecca Passonneau. 2015. Abstractive multi-
document summarization via phrase selection and
merging. In ACL, pages 1587–1597.

Jianpeng Cheng and Mirella Lapata. 2016. Neural
summarization by extracting sentences and words.
In ACL, pages 484–494.

Kyunghyun Cho, Bart van Merriënboer Caglar Gul-
cehre, Dzmitry Bahdanau, Fethi Bougares Holger
Schwenk, and Yoshua Bengio. 2014. Learning
phrase representations using rnn encoder–decoder
for statistical machine translation. EMNLP, pages
1724–1734.

George B Dantzig and Mukund N Thapa. 2006. Linear
programming 1: introduction. Springer Science &
Business Media.

Harold P Edmundson. 1969. New methods in au-
tomatic extracting. Journal of the ACM (JACM),
16(2):264–285.

Günes Erkan and Dragomir R Radev. 2004a. Lex-
pagerank: Prestige in multi-document text summa-
rization. In EMNLP, volume 4, pages 365–371.

Günes Erkan and Dragomir R Radev. 2004b. Lexrank:
Graph-based lexical centrality as salience in text
summarization. JAIR, pages 457–479.

Jade Goldstein, Vibhu Mittal, Jaime Carbonell, and
Mark Kantrowitz. 2000. Multi-document sum-
marization by sentence extraction. In NAACL-
ANLPWorkshop, pages 40–48.

Jiatao Gu, Zhengdong Lu, Hang Li, and Victor OK
Li. 2016. Incorporating copying mechanism in
sequence-to-sequence learning. In ACL, pages
1631–1640.

Zhanying He, Chun Chen, Jiajun Bu, Can Wang, Li-
jun Zhang, Deng Cai, and Xiaofei He. 2012. Doc-
ument summarization based on data reconstruction.
In AAAI, pages 620–626.

Sepp Hochreiter and Jürgen Schmidhuber. 1997.
Long short-term memory. Neural computation,
9(8):1735–1780.

Quoc Le and Tomas Mikolov. 2014. Distributed rep-
resentations of sentences and documents. In ICML,
pages 1188–1196.

Chen Li, Fei Liu, Fuliang Weng, and Yang Liu. 2013.
Document summarization via guided sentence com-
pression. In EMNLP, pages 490–500.

Huiying Li, Yue Hu, Zeyuan Li, Xiaojun Wan, and
Jianguo Xiao. 2011. PKUTM participation in
TAC2011. In TAC.

Piji Li, Lidong Bing, Wai Lam, Hang Li, and Yi Liao.
2015. Reader-aware multi-document summariza-
tion via sparse coding. In IJCAI, pages 1270–1276.

Piji Li, Zihao Wang, Wai Lam, Zhaochun Ren, and
Lidong Bing. 2017. Salience estimation via vari-
ational auto-encoders for multi-document summa-
rization. In AAAI, pages 3497–3503.

2089



Chin-Yew Lin. 2004. Rouge: A package for auto-
matic evaluation of summaries. In Text summariza-
tion branches out: Proceedings of the ACL-04 work-
shop, volume 8.

He Liu, Hongliang Yu, and Zhi-Hong Deng. 2015.
Multi-document summarization based on two-level
sparse representation model. In AAAI, pages 196–
202.

Hans Peter Luhn. 1958. The automatic creation of lit-
erature abstracts. IBM Journal of research and de-
velopment, 2(2):159–165.

Minh-Thang Luong, Hieu Pham, and Christopher D
Manning. 2015. Effective approaches to attention-
based neural machine translation. In EMNLP, pages
1412–1421.

Ryan McDonald. 2007. A study of global inference
algorithms in multi-document summarization. In
ECIR, pages 557–564. Springer.

Rada Mihalcea and Paul Tarau. 2004. Textrank: Bring-
ing order into texts. In EMNLP, pages 404–411.

Tomas Mikolov, Kai Chen, Greg Corrado, and Jef-
frey Dean. 2013. Efficient estimation of word
representations in vector space. arXiv preprint
arXiv:1301.3781.

Ziheng Lin Min, Yen Kan Chew, and Lim Tan. 2012.
Exploiting category-specific information for multi-
document summarization. COLING, pages 2093–
2108.

Ramesh Nallapati, Bowen Zhou, Caglar Gulcehre,
Bing Xiang, et al. 2016. Abstractive text summa-
rization using sequence-to-sequence rnns and be-
yond. arXiv preprint arXiv:1602.06023.

Ani Nenkova and Kathleen McKeown. 2012. A survey
of text summarization techniques. In Mining Text
Data, pages 43–76. Springer.

Alexander M Rush, Sumit Chopra, and Jason Weston.
2015. A neural attention model for abstractive sen-
tence summarization. EMNLP, pages 379–389.

Jürgen Schmidhuber. 2015. Deep learning in neural
networks: An overview. Neural Networks, 61:85–
117.

Hongya Song, Zhaochun Ren, Shangsong Liang, Piji
Li, Jun Ma, and Maarten de Rijke. 2017. Summa-
rizing answers in non-factoid community question-
answering. In WSDM, pages 405–414. ACM.

Xiaojun Wan, Jianwu Yang, and Jianguo Xiao.
2007. Manifold-ranking based topic-focused multi-
document summarization. In IJCAI, volume 7,
pages 2903–2908.

Lu Wang, Hema Raghavan, Vittorio Castelli, Radu Flo-
rian, and Claire Cardie. 2013. A sentence com-
pression based framework to query-focused multi-
document summarization. In ACL, pages 1384–
1394.

Mark Wasson. 1998. Using leading text for news sum-
maries: Evaluation results and implications for com-
mercial summarization applications. In ACL, pages
1364–1368.

Kristian Woodsend and Mirella Lapata. 2012. Multiple
aspect summarization using integer linear program-
ming. In EMNLP-CNLL, pages 233–243.

Jin-ge Yao, Xiaojun Wan, and Jianguo Xiao. 2015.
Compressive document summarization via sparse
optimization. In IJCAI, pages 1376–1382.

2090


