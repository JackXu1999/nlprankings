



















































Whitepaper of NEWS 2015 Shared Task on Machine Transliteration


Proceedings of the Fifth Named Entity Workshop, joint with 53rd ACL and the 7th IJCNLP, pages 1–9,
Beijing, China, July 26-31, 2015. c©2015 Association for Computational Linguistics

Whitepaper of NEWS 2015 Shared Task on Machine Transliteration∗

Min Zhang?, Haizhou Li†, Rafael E. Banchs†, A Kumaran‡
?Soochow University, China 215006
{minzhang}@suda.edu.cn

†Institute for Infocomm Research, A*STAR, Singapore 138632
{hli,rembanchs}@i2r.a-star.edu.sg

‡Multilingual Systems Research, Microsoft Research India
A.Kumaran@microsoft.com

Abstract
Transliteration is defined as phonetic
translation of names across languages.
Transliteration of Named Entities (NEs)
is necessary in many applications, such
as machine translation, corpus alignment,
cross-language IR, information extraction
and automatic lexicon acquisition. Al-
l such systems call for high-performance
transliteration, which is the focus of
shared task in the NEWS 2015 workshop.
The objective of the shared task is to pro-
mote machine transliteration research by
providing a common benchmarking plat-
form for the community to evaluate the
state-of-the-art technologies.

1 Task Description

The task is to develop machine transliteration sys-
tem in one or more of the specified language pairs
being considered for the task. Each language pair
consists of a source and a target language. The
training and development data sets released for
each language pair are to be used for developing
a transliteration system in whatever way that the
participants find appropriate. At the evaluation
time, a test set of source names only would be re-
leased, on which the participants are expected to
produce a ranked list of transliteration candidates
in another language (i.e. n-best transliterations),
and this will be evaluated using common metrics.
For every language pair the participants must sub-
mit at least one run that uses only the data provid-
ed by the NEWS workshop organisers in a given
language pair (designated as “standard” run, pri-
mary submission). Users may submit more “stan-
rard” runs. They may also submit several “non-
standard” runs for each language pair that use oth-
er data than those provided by the NEWS 2015

∗http://translit.i2r.a-star.edu.sg/news2015/

workshop; such runs would be evaluated and re-
ported separately.

2 Important Dates

Research paper submission deadline 14 May 2015

Shared task
Registration opens 25 Feb 2015
Registration closes 25 April 2015
Training/Development data release 20 Feb 2015
Test data release 28 April 2015
Results Submission Due 4 May 2015
Results Announcement 9 May 2015
Task (short) Papers Due 14 May 2015

For all submissions
Acceptance Notification 14 June 2015
Camera-Ready Copy Deadline 21 June 2015
Workshop Date 31 July 2015

3 Participation

1. Registration (15 Feb 2015)

(a) NEWS Shared Task opens for registra-
tion.

(b) Prospective participants are to register to
the NEWS Workshop homepage.

2. Training & Development Data (20 Feb 2015)

(a) Registered participants are to obtain
training and development data from the
Shared Task organiser and/or the desig-
nated copyright owners of databases.

(b) All registered participants are required
to participate in the evaluation of at least
one language pair, submit the results and
a short paper and attend the workshop at
ACL-IJCNLP 2015.

3. Test data (28 April 2015)

1



(a) The test data would be released on 28
April 2015, and the participants have a
maximum of 5 days to submit their re-
sults in the expected format.

(b) One “standard” run must be submitted
from every group on a given language
pair. Additional “standard” runs may
be submitted, up to 4 “standard” run-
s in total. However, the participants
must indicate one of the submitted “s-
tandard” runs as the “primary submis-
sion”. The primary submission will be
used for the performance summary. In
addition to the “standard” runs, more
“non-standard” runs may be submitted.
In total, maximum 8 runs (up to 4 “stan-
dard” runs plus up to 4 “non-standard”
runs) can be submitted from each group
on a registered language pair. The defi-
nition of “standard” and “non-standard”
runs is in Section 5.

(c) Any runs that are “non-standard” must
be tagged as such.

(d) The test set is a list of names in source
language only. Every group will pro-
duce and submit a ranked list of translit-
eration candidates in another language
for each given name in the test set.
Please note that this shared task is a
“transliteration generation” task, i.e.,
given a name in a source language one
is supposed to generate one or more
transliterations in a target language. It
is not the task of “transliteration discov-
ery”, i.e., given a name in the source lan-
guage and a set of names in the target
language evaluate how to find the ap-
propriate names from the target set that
are transliterations of the given source
name.

4. Results (4 May 2015)

(a) On 4 May 2015, the evaluation results
would be announced and will be made
available on the Workshop website.

(b) Note that only the scores (in respective
metrics) of the participating systems on
each language pairs would be published,
and no explicit ranking of the participat-
ing systems would be published.

(c) Note that this is a shared evaluation task

and not a competition; the results are
meant to be used to evaluate systems on
common data set with common metric-
s, and not to rank the participating sys-
tems. While the participants can cite the
performance of their systems (scores on
metrics) from the workshop report, they
should not use any ranking information
in their publications.

(d) Furthermore, all participants should a-
gree not to reveal identities of other par-
ticipants in any of their publications un-
less you get permission from the other
respective participants. By default, all
participants remain anonymous in pub-
lished results, unless they indicate oth-
erwise at the time of uploading their re-
sults. Note that the results of all systems
will be published, but the identities of
those participants that choose not to dis-
close their identity to other participants
will be masked. As a result, in this case,
your organisation name will still appear
in the web site as one of participants, but
it will not be linked explicitly to your re-
sults.

5. Short Papers on Task (14 May 2015)

(a) Each submitting site is required to sub-
mit a 4-page system paper (short paper)
for its submissions, including their ap-
proach, data used and the results on ei-
ther test set or development set or by n-
fold cross validation on training set.

(b) The review of the system papers will be
done to improve paper quality and read-
ability and make sure the authors’ ideas
and methods can be understood by the
workshop participants. We are aiming at
accepting all system papers, and select-
ed ones will be presented orally in the
NEWS 2015 workshop.

(c) All registered participants are required
to register and attend the workshop to
introduce your work.

(d) All paper submission and review will be
managed electronically through https://
www.softconf.com/acl2015/news2015/.

2



4 Language Pairs

The tasks are to transliterate personal names or
place names from a source to a target language as
summarised in Table 1. NEWS 2015 Shared Task
offers 14 evaluation subtasks, among them ChEn
and ThEn are the back-transliteration of EnCh and
EnTh tasks respectively. NEWS 2015 releases
training, development and testing data for each of
the language pairs. NEWS 2015 continues all lan-
guage pairs that were evaluated in NEWS 2011
and 2012. In such cases, the training, development
and test data in the release of NEWS 2015 are the
same as those in NEWS 2012.

Please note that in order to have an accurate s-
tudy of the research progress of machine transla-
tion technology, different from previous practice,
the test/reference sets of NEWS 2011 are not re-
leased to the research community. Instead, we use
the test sets of NEWS 2011 as progress test set-
s in NEWS 2015. NEWS 2015 participants are
requested to submit results on the NEWS 2015
progress test sets (i.e., NEWS 2011 test sets). By
doing so, we would like to do comparison studies
by comparing the NEWS 2015 and NEWS 2011
results on the progress test sets and the NEWS
2015 and NEWS 2012 results on the test sets. We
hope that we can have some insightful research
findings in the progress studies.

The names given in the training sets for Chi-
nese, Japanese, Korean, Thai and Persian lan-
guages are Western names and their respective
transliterations; the Japanese Name (in English)
→ Japanese Kanji data set consists only of native
Japanese names; the Arabic data set consists only
of native Arabic names. The Indic data set (Hin-
di, Tamil, Kannada, Bangla) consists of a mix of
Indian and Western names.

Examples of transliteration:

English→ Chinese
Timothy→«

English→ Japanese Katakana
Harrington→ÏêóÈó

English→ Korean Hangul
Bennett → 베넷

Japanese name in English→ Japanese Kanji
Akihiro→Ë

English→ Hindi
San Francisco → सैन फ्रान्सिस्को

English→ Tamil
London → லண்டன்

English→ Kannada
Tokyo → ಟೋಕ್ಯೋ

Arabic→ Arabic name in English
→ Khalidدلاخ

5 Standard Databases

Training Data (Parallel)
Paired names between source and target lan-
guages; size 7K – 37K.
Training Data is used for training a basic
transliteration system.

Development Data (Parallel)
Paired names between source and target lan-
guages; size 1K – 2.8K.
Development Data is in addition to the Train-
ing data, which is used for system fine-tuning
of parameters in case of need. Participants
are allowed to use it as part of training data.

Testing Data
Source names only; size 1K – 2K.
This is a held-out set, which would be used
for evaluating the quality of the translitera-
tions.

Progress Testing Data
Source names only; size 0.6K – 2.6K.
This is the NEWS 2011 test set, it is held-out
for progress study.

1. Participants will need to obtain licenses from
the respective copyright owners and/or agree
to the terms and conditions of use that are
given on the downloading website (Li et al.,
2004; MSRI, 2010; CJKI, 2010). NEWS
2015 will provide the contact details of each
individual database. The data would be pro-
vided in Unicode UTF-8 encoding, in XML
format; the results are expected to be sub-
mitted in UTF-8 encoding in XML format.
The XML formats details are available in Ap-
pendix A.

2. The data are provided in 3 sets as described
above.

3. Name pairs are distributed as-is, as provided
by the respective creators.

3



Name origin Source script Target script Data Owner Data Size Task IDTrain Dev Progress
Test

2012/2015
Test

Western English Chinese Institute for Infocomm Research 37K 2.8K 2K 1K EnCh
Western Chinese English Institute for Infocomm Research 28K 2.7K 2.2K 1K ChEn
Western English Korean Hangul CJK Institute 7K 1K 609 1K EnKo
Western English Japanese Katakana CJK Institute 26K 2K 1.8K 1K EnJa
Japanese English Japanese Kanji CJK Institute 10K 2K 571 1K JnJk
Arabic Arabic English CJK Institute 27K 2.5K 2.6K 1K ArEn
Mixed English Hindi Microsoft Research India 12K 1K 1K 1K EnHi
Mixed English Tamil Microsoft Research India 10K 1K 1K 1K EnTa
Mixed English Kannada Microsoft Research India 10K 1K 1K 1K EnKa
Mixed English Bangla Microsoft Research India 13K 1K 1K 1K EnBa
Western English Thai NECTEC 27K 2K 2K 1K EnTh
Western Thai English NECTEC 25K 2K 1.9K 1K ThEn
Western English Persian Sarvnaz Karimi / RMIT 10K 2K 2K 1K EnPe
Western English Hebrew Microsoft Research India 9.5K 1K 1K 1K EnHe

Table 1: Source and target languages for the shared task on transliteration.

(a) While the databases are mostly man-
ually checked, there may be still in-
consistency (that is, non-standard usage,
region-specific usage, errors, etc.) or in-
completeness (that is, not all right varia-
tions may be covered).

(b) The participants may use any method to
further clean up the data provided.

i. If they are cleaned up manually, we
appeal that such data be provided
back to the organisers for redistri-
bution to all the participating group-
s in that language pair; such shar-
ing benefits all participants, and fur-
ther ensures that the evaluation pro-
vides normalisation with respect to
data quality.

ii. If automatic cleanup were used,
such cleanup would be considered a
part of the system fielded, and hence
not required to be shared with al-
l participants.

4. Standard Runs We expect that the partici-
pants to use only the data (parallel names)
provided by the Shared Task for translitera-
tion task for a “standard” run to ensure a fair
evaluation. One such run (using only the data
provided by the shared task) is mandatory for
all participants for a given language pair that
they participate in.

5. Non-standard Runs If more data (either par-
allel names data or monolingual data) were
used, then all such runs using extra data must

be marked as “non-standard”. For such “non-
standard” runs, it is required to disclose the
size and characteristics of the data used in the
system paper.

6. A participant may submit a maximum of 8
runs for a given language pair (including the
mandatory 1 “standard” run marked as “pri-
mary submission”).

6 Paper Format

Paper submissions to NEWS 2015 should follow
the ACL 2015 paper submission policy, including
paper format, blind review policy and title and au-
thor format convention. Full papers (research pa-
per) are in two-column format without exceeding
eight (8) pages of content plus two (2) extra page
for references and short papers (task paper) are al-
so in two-column format without exceeding four
(4) pages content plus two (2) extra page for ref-
erences. Submission must conform to the official
ACL 2015 style guidelines. For details, please re-
fer to the ACL 2015 website2.

7 Evaluation Metrics

We plan to measure the quality of the translitera-
tion task using the following 4 metrics. We accept
up to 10 output candidates in a ranked list for each
input entry.

Since a given source name may have multiple
correct target transliterations, all these alternatives
are treated equally in the evaluation. That is, any
of these alternatives are considered as a correct

2http://www.ACL2015.org/

4



transliteration, and the first correct transliteration
in the ranked list is accepted as a correct hit.

The following notation is further assumed:
N : Total number of names (source word-

s) in the test set
ni : Number of reference transliterations

for i-th name in the test set (ni ≥ 1)
ri,j : j-th reference transliteration for i-th

name in the test set
ci,k : k-th candidate transliteration (system

output) for i-th name in the test set
(1 ≤ k ≤ 10)

Ki : Number of candidate transliterations
produced by a transliteration system

1. Word Accuracy in Top-1 (ACC) Also
known as Word Error Rate, it measures correct-
ness of the first transliteration candidate in the can-
didate list produced by a transliteration system.
ACC = 1 means that all top candidates are cor-
rect transliterations i.e. they match one of the ref-
erences, and ACC = 0 means that none of the top
candidates are correct.

ACC =
1
N

N∑
i=1

{
1 if ∃ ri,j : ri,j = ci,1;
0 otherwise

}
(1)

2. Fuzziness in Top-1 (Mean F-score) The
mean F-score measures how different, on average,
the top transliteration candidate is from its closest
reference. F-score for each source word is a func-
tion of Precision and Recall and equals 1 when the
top candidate matches one of the references, and
0 when there are no common characters between
the candidate and any of the references.

Precision and Recall are calculated based on the
length of the Longest Common Subsequence be-
tween a candidate and a reference:

LCS(c, r) =
1
2

(|c|+ |r| − ED(c, r)) (2)

where ED is the edit distance and |x| is the length
of x. For example, the longest common subse-
quence between “abcd” and “afcde” is “acd” and
its length is 3. The best matching reference, that
is, the reference for which the edit distance has
the minimum, is taken for calculation. If the best
matching reference is given by

ri,m = arg min
j

(ED(ci,1, ri,j)) (3)

then Recall, Precision and F-score for i-th word
are calculated as

Ri =
LCS(ci,1, ri,m)

|ri,m| (4)

Pi =
LCS(ci,1, ri,m)

|ci,1| (5)

Fi = 2
Ri × Pi
Ri + Pi

(6)

• The length is computed in distinct Unicode
characters.

• No distinction is made on different character
types of a language (e.g., vowel vs. conso-
nants vs. combining diereses� etc.)

3. Mean Reciprocal Rank (MRR) Measures
traditional MRR for any right answer produced by
the system, from among the candidates. 1/MRR
tells approximately the average rank of the correct
transliteration. MRR closer to 1 implies that the
correct answer is mostly produced close to the top
of the n-best lists.

RRi =
{

minj 1j if ∃ri,j , ci,k : ri,j = ci,k;
0 otherwise

}
(7)

MRR =
1
N

N∑
i=1

RRi (8)

4. MAPref Measures tightly the precision in the
n-best candidates for i-th source name, for which
reference transliterations are available. If all of
the references are produced, then the MAP is 1.
Let’s denote the number of correct candidates for
the i-th source word in k-best list as num(i, k).
MAPref is then given by

MAPref =
1
N

N∑
i

1
ni

(
ni∑

k=1

num(i, k)

)
(9)

8 Contact Us

If you have any questions about this share task and
the database, please email to

Dr. Rafael E. Banchs
Institute for Infocomm Research (I2R),
A*STAR
1 Fusionopolis Way
#08-05 South Tower, Connexis
Singapore 138632
rembanchs@i2r.a-star.edu.sg

5



Dr. Min Zhang
Soochow University
China 215006
zhangminmt@hotmail.com

References
[CJKI2010] CJKI. 2010. CJK Institute.

http://www.cjk.org/.

[Li et al.2004] Haizhou Li, Min Zhang, and Jian Su.
2004. A joint source-channel model for machine
transliteration. In Proc. 42nd ACL Annual Meeting,
pages 159–166, Barcelona, Spain.

[MSRI2010] MSRI. 2010. Microsoft Research India.
http://research.microsoft.com/india.

6



A Training/Development Data

• File Naming Conventions:
NEWS12 train XXYY nnnn.xml
NEWS12 dev XXYY nnnn.xml
NEWS12 test XXYY nnnn.xml
NEWS11 test XXYY nnnn.xml
(progress test sets)

– XX: Source Language
– YY: Target Language
– nnnn: size of parallel/monolingual

names (“25K”, “10000”, etc)

• File formats:
All data will be made available in XML for-
mats (Figure 1).

• Data Encoding Formats:
The data will be in Unicode UTF-8 encod-
ing files without byte-order mark, and in the
XML format specified.

B Submission of Results

• File Naming Conventions:
You can give your files any name you like.
During submission online you will need to
indicate whether this submission belongs to
a “standard” or “non-standard” run, and if it
is a “standard” run, whether it is the primary
submission.

• File formats:
All data will be made available in XML for-
mats (Figure 2).

• Data Encoding Formats:
The results are expected to be submitted in
UTF-8 encoded files without byte-order mark
only, and in the XML format specified.

7



<?xml version="1.0" encoding="UTF-8"?>

<TransliterationCorpus
CorpusID = "NEWS2012-Train-EnHi-25K"
SourceLang = "English"
TargetLang = "Hindi"
CorpusType = "Train|Dev"
CorpusSize = "25000"
CorpusFormat = "UTF8">

<Name ID=�1�>
<SourceName>eeeeee1</SourceName>
<TargetName ID="1">hhhhhh1_1</TargetName>

<TargetName ID="2">hhhhhh1_2</TargetName>
...
<TargetName ID="n">hhhhhh1_n</TargetName>

</Name>
<Name ID=�2�>

<SourceName>eeeeee2</SourceName>
<TargetName ID="1">hhhhhh2_1</TargetName>
<TargetName ID="2">hhhhhh2_2</TargetName>
...
<TargetName ID="m">hhhhhh2_m</TargetName>

</Name>
...
<!-- rest of the names to follow -->
...

</TransliterationCorpus>

Figure 1: File: NEWS2012 Train EnHi 25K.xml

8



<?xml version="1.0" encoding="UTF-8"?>

<TransliterationTaskResults
SourceLang = "English"
TargetLang = "Hindi"
GroupID = "Trans University"
RunID = "1"
RunType = "Standard"
Comments = "HMM Run with params: alpha=0.8 beta=1.25">

<Name ID="1">
<SourceName>eeeeee1</SourceName>
<TargetName ID="1">hhhhhh11</TargetName>
<TargetName ID="2">hhhhhh12</TargetName>
<TargetName ID="3">hhhhhh13</TargetName>
...
<TargetName ID="10">hhhhhh110</TargetName>

<!-- Participants to provide their
top 10 candidate transliterations -->

</Name>
<Name ID="2">

<SourceName>eeeeee2</SourceName>
<TargetName ID="1">hhhhhh21</TargetName>
<TargetName ID="2">hhhhhh22</TargetName>
<TargetName ID="3">hhhhhh23</TargetName>
...
<TargetName ID="10">hhhhhh110</TargetName>
<!-- Participants to provide their
top 10 candidate transliterations -->

</Name>
...
<!-- All names in test corpus to follow -->
...

</TransliterationTaskResults>

Figure 2: Example file: NEWS2012 EnHi TUniv 01 StdRunHMMBased.xml

9


