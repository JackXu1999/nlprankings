



















































Supervised Clustering of Questions into Intents for Dialog System Applications


Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 2310–2321
Brussels, Belgium, October 31 - November 4, 2018. c©2018 Association for Computational Linguistics

2310

Supervised Clustering of Questions into Intents
for Dialog System Applications

Iryna Haponchyk1, Antonio Uva1∗,
Seunghak Yu2, Olga Uryupina1 and Alessandro Moschitti3,1

1DISI, University of Trento, Povo (TN), Italy
2Samsung Research, Seoul, Korea

3Amazon, Manhattan Beach, CA, USA
iryna.haponchyk@unitn.it, antonio.uva@unitn.it

seunghak.yu@samsung.com, olga.uryupina@unitn.it, amosch@amazon.com

Abstract

Modern automated dialog systems require
complex dialog managers able to deal with
user intent triggered by high-level seman-
tic questions. In this paper, we propose a
model for automatically clustering questions
into user intents to help the design tasks. Since
questions are short texts, uncovering their se-
mantics to group them together can be very
challenging. We approach the problem by us-
ing powerful semantic classifiers from ques-
tion duplicate/matching research along with a
novel idea of supervised clustering methods
based on structured output. We test our ap-
proach on two intent clustering corpora, show-
ing an impressive improvement over previous
methods for two languages/domains.

1 Introduction

The recent years have seen a resurgent interest
for dialog systems, ranging from help desks and
more complex task-based dialog to general pur-
pose conversational agents, e.g., Alexa, Cortana or
Siri. All these different application scenarios show
that users expect to formulate complex informa-
tion needs in natural language, with no limitation
to their expressiveness. In other words, standard
shallow semantic parsing based on concept seg-
mentation and labeling is no longer sufficient for
dialog modeling in today’s applications. In partic-
ular, when designing dialog managers, we need to
consider user intents expressed by articulated nat-
ural language text.

Current solutions to the intent detection prob-
lem consist in manually analyzing user questions
and creating a taxonomy of intents to be attached
to the appropriate actions. For example, if sev-
eral semantically similar/identical questions re-
gard BookFlight, the designer will build a cat-

∗ The first two authors contributed equally to this
manuscript.

egory. This is a rather costly, difficult and time
consuming task, which can often prevent the fast
prototyping of dialog systems even for small do-
mains. Moreover, the effort is extremely task- and
system-specific, with very limited possibilities of
porting the intent model, at least partially, across
platforms, domains and languages. For example,
in the recent 2016/2017 Natural Language Under-
standing Benchmarks (Coucke et al., 2017), the or-
ganizers have evaluated built-in intents generated
by the major dialog managers (Apple’s SiriKit,
Amazon’s Alexa, Microsoft’s Luis, Google’s Di-
alogflow, and Snips.ai) against a rather small set
of relatively generic intents (e.g., GetWeather).
This involved a considerable effort on aligning dif-
ferent system outputs.

To our knowledge, no previous work has been
dedicated to automatizing this task, mainly be-
cause the underlying problem, semantic ques-
tion paraphrasing, is very challenging. How-
ever, recent initiatives for automatic question du-
plicate detection1, question relatedness (Nakov
et al., 2016, 2017) and semantic textual similarity
(Agirre et al., 2012; Cer et al., 2017) have shown
that current technology achieves good accuracy in
matching short text expressing similar semantics.

In this paper, we propose a model for automat-
ically clustering questions into user intent cate-
gories, which can help the design of dialog sys-
tems. Our approach aims at overcoming the dif-
ficulty of providing a unique definition of intent
from either a theoretical or practical perspective.
Collaborating with stakeholders, we observe that
it is very challenging to capture the intent prop-
erty that can optimize dialogue/chatbot engineer-
ing work in terms of design and effort. The impor-
tant aspect of our approach is that given a notion of
intent, explicitly provided by annotated data, our

1Quora: https://www.kaggle.com/c/quora-question-pairs



2311

model can create clusterings driven by such intrin-
sic definition. This is one of our major contribu-
tions: providing an effective supervised clustering
approach, which can learn definitions from exam-
ples.

Regarding the technical solution our approach is
bridging (i) the state-of-the-art methods for ques-
tion similarity/paraphrasing with (ii) powerful su-
pervised clustering algorithms. The former are
obtained by exploiting previous research, e.g., on
Quora, whereas the latter, are obtained by capi-
talizing on the structured output machine learn-
ing methods used for coreference resolution (CR),
e.g., (Yu and Joachims, 2009; Fernandes et al.,
2014). The main difference with CR consists in
substituting mentions and their vector representa-
tion with the one of entire questions. It should be
noted that structured output methods require a rep-
resentation of edges connecting questions, which
we obtain from question similarity research.

To train our model, we define a clustering cor-
pus by automatically deriving question clusters
from the Quora data, complementing the avail-
able question pair annotation with the transitive
closure of the semantic matching property. Ad-
ditionally, since Quora data is noisy, we manually
annotate a question sample, defining an intent hi-
erarchical taxonomy. To test the applicability of
our methodology across languages and domains,
we run evaluation experiments on another intent-
based corpus, a collection of FAQs for an Italian
online service.

We evaluate our approach comparing it to the
classical k-means using (i) tf-idf model, (ii) our
learned question similarity, and (iii) the similar-
ity with spectral clustering. The last two methods
can be considered another relatively novel contri-
bution. Our results show that our new structured
output method for question clustering: (i) highly
outperforms all its competitors with respect to the
standard clustering accuracy/purity as well as the
measures defined in the CR domain, (ii) provides
clustering accuracy of 80% with respect to the
original Quora annotation and a still valuable ac-
curacy of 65% with respect to the intent classes
designed by an expert in Dialog modeling. This
is a promising result as our designer can only pro-
vide one of the many interpretations of the intent
taxonomy, which can be effectively applied.

Finally, a particular strength of our approach
lies in identifying new intents (singleton clus-

ters). These unseen intents are clearly problem-
atic for dialogue management and cannot be cov-
ered by traditional approaches (e.g., our unsuper-
vised clustering baselines show much lower per-
formance on singleton clusters).

2 Related Work

Intent is a key concept for building dialog systems
and is therefore a central research topic in the area.
In particular, recent general-purpose dialog sys-
tems have to rely on extensive intent modeling to
be able to correctly analyze a wide variety of user
queries. This has led to a considerable amount of
research on data-driven intent modeling.

In particular, Xu et al. (2013) represent query’s
intent as trees and employ a procedure for map-
ping an NL query into a tree-structured intent.
The problem of this approach is that a new set
of intent trees is required for new domains. Kim
et al. (2016); Celikyilmaz et al. (2011) use semi-
supervised approaches with large amounts of un-
labeled data to improve the accuracy in map-
ping user queries into intents. However, they
still require a small amount of labeled data in or-
der to learn a given intent. Chen et al. (2016a)
train a Convolutional Deep Neural Network to
jointly learn the representations of human intents
and associated utterances. Chen et al. (2016b)
propose feature-enriched matrix factorization to
model open domain intents. This leverages knowl-
edge from Wikipedia and Freebase to acquire in-
formation from unexplored domains according to
new users’ requests. Unfortunately, it also requires
external knowledge bases to induce concepts ap-
pearing within the intents.

Approaching the same problem from the op-
posite direction, several studies investigate algo-
rithms for automatic question clustering. Wen
et al. (2001) propose to cluster together queries
that lead to the same group of web pages that are
frequently selected by users. Jeon et al. (2005)
use machine translation to estimate word trans-
lation probabilities and retrieve similar questions
from question archives. Li et al. (2008) try to
infer the intent of unlabeled queries according to
the proximity with respect to the labeled queries
in a click graph. Beitzel et al. (2007) propose
to automatically classify web queries from logs
into a set of topics by using a combination of
different techniques, either supervised or unsu-
pervised. The extracted topics are further used



2312

for efficient web search. Deepak (2016) presents
MiXKmeans, a variation of k-means algorithm,
suited for clustering threads present on forums and
Community Question Answering websites. How-
ever, most techniques use unsupervised clustering
to group similar questions/queries, without model-
ing intents. In contrast, our study relies on super-
vised clustering to learn intent-based similarity.

Finally, our work is related to a large body of
research on dialog acts (Stolcke et al., 2000; Kim
et al., 2010; Chen et al., 2018): our low-level
intent labels (Table 1) can be seen as very fine-
grained dialog acts (Core and Allen, 1997; Bunt
et al., 2010; Oraby et al., 2017).

However, our paper’s objective is different as
our goal is not to rigidly define intents and then ex-
ploit them to derive a semantic interpretation. We
focus on two contributions: first, we aim at provid-
ing a tool to help implementing dialog managers
such that the designer can more easily create cate-
gories from precomputed clusters. Note that hav-
ing in mind hundreds of questions to create intent
category from scratch is clearly an exigent task.

Second, our approach can dynamically cluster
questions with the same semantics, without any
concept annotation. Indeed, the important con-
cepts will be learned from the training data, which
constitutes a much simpler annotation task than
the creation of ad hoc dialogue acts. In particu-
lar, this can help with domain-specific intents: the
domain-level semantics will be learned from data
with no need for advanced manual engineering.

3 Question Clustering Algorithms

In this paper, we explore techniques for clustering
questions into user intent. To this end, an input
set of questions Q undergoes splitting into subsets
(clusters), ci = {qij}

Ni
j=1, where q

i
j is the j-th ques-

tion in the cluster i of sizeNi and
⊔

i ci = Q. Each
ci is assumed to contain questions with the same
intent, i.e., to represent a distinct intent. Perform-
ing a clustering of a new question set Q in an un-
supervised manner may generally be troublesome
due to the lack of any information about the struc-
ture of Q and the target number of distinct intents
in it. To overcome this issue, we learn a clustering
function from data annotated with gold question
clusters.

In this work, we pose the task as a supervised
clustering problem following the formulation by
Finley and Joachims (2005). Having the train-

ing examples of the form {(xi,yi)}ni=1, where
each input xi is a set of elements of some nature
and yi – the corresponding gold standard cluster-
ing of such a set, the goal is to learn a predictor
h : X → Y , from the space of sets X to the
space of clusterings Y . Supervised clustering has
been shown particularly effective for the NLP task
of coreference resolution (Yu and Joachims, 2009;
Fernandes et al., 2014). These models learn to in-
fer an optimal clustering y of an input set x in a
structured way, i.e., as one output object optimiz-
ing a global scoring function f : X × Y → R.
This is different from local models, which aggre-
gate multiple clustering decisions taken with re-
spect to pairs of elements.

At the test time, the global models draw predic-
tions by finding

ŷ = argmax
y∈Y

f(x,y). (1)

In the following, we give the necessary details of
the original approach of Yu and Joachims (2009)
and its adaptation for clustering questions.

3.1 Structured Output Clustering
To facilitate the inference of the optimal clustering
in Equation 1, Yu and Joachims represent clus-
tering variables y using graph structures. For an
input x, they construct a fully-connected undi-
rected graph G, whose nodes are elements xi of
the input x and edges are all the pairwise links
between them (xi, xj).2 Any spanning forest h
on G straightforwardly translates into a clustering
y. The nodes, aka elements represented by them,
in each connected component (spanning tree) of
h are considered to belong to the same cluster.
The authors incorporate the spanning forest struc-
tures h as latent variables and decompose the fea-
ture representation of the input-output pair (x,y),
which is extended with h, over the edges of h:

Φ(x,y,h) =
∑

(xi,xj)∈h

φ(xi, xj). (2)

They employ Kruskal’s spanning algo-
rithm (Kruskal, 1956) to infer the optimal h,
and, respectively, y.

A linear model w is trained using the Latent for-
mulation of the Structural SVM learner (LSSVM),

2Note that we distinguish between boldface letters which
we use to denote structural input/output variables, and nor-
mal font letters - for their rather elementary constituents, and
simple variables.



2313

to score the output clusterings according to the
function f(x,y,h) = w · Φ(x,y,h). In fact, w
learns to score the edges since the structural fea-
ture vector decomposes over the edges. However,
imposing a structure onto the output is supposed
to produce a better w, which we test in our exper-
iments described in Section 5.

Alternatively, we employ the same structural
model with another learning algorithm – latent
structured perceptron (LSP) by Sun et al. (2009);
Fernandes et al. (2014).

3.2 Pairwise question similarity classifier

Our intent clustering algorithm relies on the pair-
wise similarity between questions (edge score).
To provide an accurate estimation of question-
question similarity, we build upon an extensive
state-of-the-art research in semantic similarity for
short text, more specifically, on our previous
work (Filice et al., 2016, 2017; Barrón-Cedeño
et al., 2016; Da San Martino et al., 2016) solu-
tions/features shown effective in the shared tasks
by Nakov et al. (2016, 2017); Agirre et al. (2012,
2013).

In such work, the classifier is trained with
SVMs, which learn a classification function f :
Q × Q → {0, 1} on duplicate vs. non-duplicate
pairs of questions belonging to the question set Q.
The classifier score is used to decide if two ques-
tions in the dataset qi and qj are duplicate or not.
We represent question pairs as vectors of similar-
ity features derived between two questions.

Feature Vectors are built for questions pairs,
(qi, qj), using a set of text similarity features
that capture the relations between two ques-
tions. More specifically, we compute 20 sim-
ilarity features sim(qi, qj) using word n-grams
(n = [1, . . . , 4]), after stopword removal, greedy
string tiling (Wise, 1996), longest common sub-
sequences (Allison and Dix, 1986), Jaccard coef-
ficient (Jaccard, 1901), word containment (Lyon
et al., 2001), and cosine similarity.

4 Building Intent clusters

In this study, we rely on two datasets, providing
some important insights on question semantics.
None of them, however, fully annotates intents ex-
plicitly. Below we describe our approach for con-
verting these resources into intent corpora, relying
on an automatic procedure followed by a manual
post-annotation step. Our intent corpora as well

as the larger raw question clusters collections are
available to the research community.3

4.1 Quora corpus

The original Quora task requires detecting
whether two questions are semantically dupli-
cate or not. The associated dataset contains over
404, 348 pairs of questions, posted by users on
Quora website, labeled as a duplicate pair or not.
For example, How do you start a bakery? and
How can one start a bakery business? are dupli-
cate, while What are natural numbers? and What
is a least natural number? are not. Note that the
coders label pairs in isolation, only having access
to one pair to be labeled at a time on Quora website
(answer base). The pairs to be labeled are not se-
lected randomly. To make the task more challeng-
ing, as well as more useful for practical applica-
tions, the organizers only offer pairs of questions
that are somewhat semantically related:

(3) q1: How does an automobile works?
q2: How does automobile R&D work?

(4) q1: Will I lose weight if I fast ?
q2: Why am I losing weight so fast ?
q3: How can I lose my weight fast ?

In (3), the lexical material is very similar, yet the
questions are rather distinct, as reflected in the
Quora annotation. They also express very differ-
ent user intents: while q1 is a generic curiosity
question about automobiles, q2 is a practical re-
quest for information on R&D in the automotive
industry. Example (4) shows why Quora dupli-
cate detection task is very challenging and requires
a very good level of NL Understanding: while
these three questions are very similar on the sur-
face level, they all convey distinct semantics.

4.1.1 Question clusters from Quora
Differently from the original task, in this work,
we are interested in automatically acquiring in-
tents from large question repositories. Given this,
we need a corpus that contains clusters of ques-
tions annotated by their underlying intent. As a
first step in this direction, we approximate intent
clusters with the clusters of similar questions from
Quora. These can be obtained by exploiting the
pairwise annotation and relying on the transitivity
property: for each pair q1, q2 annotated as a du-
plicate, we assign q1 and q2 to the same cluster;

3https://ikernels-portal.disi.unitn.
it/repository/intent-qa

https://ikernels-portal.disi.unitn.it/repository/intent-qa
https://ikernels-portal.disi.unitn.it/repository/intent-qa


2314

negative pairs (non-duplicate question) do not im-
pact the clustering in any way.

This procedure has obvious limitations by de-
sign: (i) it will not give us any intent labels, only
the clusters and (ii) it will not provide any hier-
archy of intents, or any general/large intent cate-
gories. Still, it provides data for a large number
of user-generated intents that fully manual anno-
tation initiatives (e.g., Natural Language Under-
standing Benchmarks) cannot guarantee.

4.1.2 Manually annotating intent clusters
The use of Quora dataset to derive intent raises
several potential issues: (i) no consistency is en-
forced across labels, (ii) duplicate or very similar
Quora answers potentially pollute the annotation
for their corresponding questions, (iii) specific de-
cisions may depend on availability and granular-
ity of the underlying answers, and (iv) the anno-
tation of popular questions might be very spuri-
ous since the users have no access to all the other
related questions. The first issue has also been
noted by the Quora competition4 participants, who
found the same pairs of questions appearing sev-
eral times in the training set with different labels.
Moreover, we found numerous cases where the an-
notation does not respect the transitivity property:

(5) q1: What are, if any, the medical benefits of fasting?
q2: What are the benefits of water fasting?
q3: What are the health benefits of fasting?

Here, the three independent coders have produced
inconsistent labeling: although q2 and q3 are ex-
plicitly labeled as non-duplicate, they are both
considered duplicates of q1.

The second issue arises when the answer base
contains (near-)duplicate entries. For example, the
following two very similar questions are consid-
ered non-duplicates since they lead to two distinct
answers:

(6) q1: Which is better - DC or Marvel?
q2: DC VS Marvel: which do you like more? [non-
duplicate]

Note that this typically happens for rather popular
questions that are therefore important to be ana-
lyzed correctly, either manually or automatically.

The third issue is extremely important for areas
only partially covered by the answer DB. For ex-
ample, for the set of questions, Why is Saltwater

4https://www.kaggle.com/c/
quora-question-pairs/discussion/30435

Taffy candy imported in LOCATION?, most LO-
CATIONs are covered by a generic answer, and
all the corresponding questions are judged dupli-
cates. However, some specific LOCATIONs, e.g.,
Fiji, have a dedicated answer and thus the corre-
sponding questions form singleton clusters.

Finally, the annotation coherence problem
arises for very popular areas covering a lot of
closely related questions. Thus, more than 100
questions cover different aspects of Weight Loss.
Since the coders do not have any access to all the
questions on the same topic, the individual deci-
sions are not coordinated, which leads to rather ar-
bitrary partitioning of the area into clusters:

(7) Gold Quora Cluster 1:
How can I lose weight ?
What is the easiest way to lose weight faster ?
How can you lose weight quickly ?
How do I lose 7kgs in 2 weeks ?
What a great diet to lose weight fast and not make you
hungry or keep on measuring portions ?
.. many more
Gold Quora Cluster 2:
How can I lose 3 kg in one week?
Gold Quora Cluster 3:
What are the good diets for weight loss ?
What is the best diet plan for weight loss?

To overcome these issues, we manually re-
annotated a portion of the original Quora dataset
with intent-based clusters. Starting from automat-
ically induced clusters described above, we first
re-assess the partitioning, correcting eventual mis-
takes and then assign intent-based labels to our
clusters. Our labels are hierarchical, thus allow-
ing for a better flexibility when designing dialog
managers: the dialog flow/actions can be defined
in terms of more generic (e.g., Advice) or more
specific (e.g., Advice-WeightLoss-diet)
intents, depending on different implementation
considerations (query frequency for specific in-
tents, overall importance for the application, diffi-
culty of processing inter alia). To stay inline with
the recent intent-based dialogue research, we also
annotate slots, where applicable: entities external
to the intent, yet playing an essential role for the
correct semantic interpretation of the question. Ta-
ble 1 shows an example of the intent-based cluster
annotation.

4.2 FAQ: Hype Intent corpus

Our second corpus, Hype, allows for a more di-
rect evaluation of the intent clustering algorithm.
The data come from a set of questions asked by

https://www.kaggle.com/c/quora-question-pairs/discussion/30435
https://www.kaggle.com/c/quora-question-pairs/discussion/30435


2315

Intent Slots Questions
Recommend-TourismCuisine streetfood, Delhi What are the best street food in Delhi ?
Recommend-TourismCuisine streetfood, Delhi What is the best street food in Delhi ?
Recommend-TourismCuisine streetfood, Delhi What are the best street foods in delhi ?
Recommend-TourismRestaurant streetfood, Delhi What are the best street food places of delhi ?

Table 1: Manually annotated intent clusters for Quora

users to a conversational agent, collected and man-
ually processed for constructing a FAQ section for
Hype—an online service that offers a credit card,
a bank account number and an ibanking app to its
customers. Unlike Quora, the questions are explic-
itly assigned to clusters by human annotators, and
these clusters correspond to intents by construc-
tion. However, they do not have any informative
labels and there is therefore no associated hierar-
chy. While this corpus provides very valuable data
for our study, its main disadvantage is a very lim-
ited number of questions. Some questions are re-
ported below:

(8) q0: Cos’é HYPE? (What is HYPE?)
q1: Volevo dei chiarimente di cos’é la app hype (I’d like
to have more information about the hype app)
q2: mi puó spiegare cose’é la app hype (could you please
explain me what the hype app is?)
q3: informazione applicazione hype (information about
hype app)

At the current stage of our research, we use the
FAQ/Hype corpus directly, with no automatic or
manual adjustments as we did for Quora. We plan
to annotate the FAQ categories with explicit intent
labels in future work.

5 Experiments

We describe our comparative experiments on our
clustering models on two corpora in two different
languages.

5.1 Setup

We used two different corpora, described in Sec-
tion 4:

The Quora Intent corpus contains 270, 146
and 212 clusters in the training, development and
test sets. The clusters contain different numbers
of questions, ranging from singletons to groups of
100+ questions. The singletons are the dominant
group in the Quora dataset. This is probably due
to the inclusion of non-duplicate questions that ap-
pear in the original Quora dataset. Overall, there
are 1, 334 questions distributed in 628 clusters (an
average of 2.12 questions/cluster).

The FAQ/Hype corpus contains no small-size
(< 3) clusters by construction since smaller clus-
ters are typically not selected as FAQ entries. The
largest groups of clusters are those of size 8 and 9.
Overall, the FAQ Intent corpus contains 147 ques-
tions spread in 28 intent-based clusters, which cor-
respond to an average of 5.25 questions/cluster.

Models To perform supervised clustering, we
use: (i) the original implementation of the Latent
SVMstruct 5 – LSSVM, and (ii) our implementation
of the LSP algorithm based on the same clustering
inference on undirected graphs using Kruskal’s
spanning algorithm.

We compare these approaches to two unsuper-
vised clustering baselines: (i) spectral cluster-
ing (Ng et al., 2001), for which we employ the
implementation from the smile6 library, and (ii) re-
lational k-means (Szalkai, 2013). The former im-
plementation takes a matrix of pairwise similari-
ties between data points as input, whereas the lat-
ter approach is a generalization of k-means to an
arbitrary matrix of pairwise distances. Thus, they
can be run on the scores relative to the question
pairs (qi, qj).

We provide two variants of such scores: first,
we run both the methods on the scores obtained
from a binary pairwise similarity classifier, de-
scribed in Section 3.2. Second, we run the cluster-
ing baselines on the tf-idf scores computed for the
question pairs. Note that our use of the scores from
a trained pairwise classifier introduces some su-
pervision in standard unsupervised clustering ap-
proaches, originating new hybrid methods.

Parametrization LSSVM and LSP require the
tuning of a regularization parameter, C, and of a
specific loss parameter, r (penalty for adding an
incorrect edge), which we select on the dev. set.
We pick up C from {1.0, 10.0, 100.0, 1000.0},
and the r values from {0.1, 0.5, 1.0}. K-means
and spectral clustering algorithms require the in-
dication of the number of clusters k. In all of our

5 www.cs.cornell.edu/˜cnyu/latentssvm/
6http://haifengl.github.io/smile/

www.cs.cornell.edu/~cnyu/latentssvm/
http://haifengl.github.io/smile/


2316

Model Clustering Pairwise ClassificationPrecision Recall F1 CEAFe Precision Recall F1 Accuracy
LSSVM 80.16 77.81 78.96 63.68 43.74 32.00 36.96 88.41
LSP 66.06 91.64 76.78 51.50 20.36 76.85 32.19 65.62
SVM + spectral clustering 72.06 62.40 66.89 47.04 28.07 3.52 6.26 88.80
SVM + k-means 70.76 66.58 68.60 53.87 31.03 7.92 12.62 88.35
tfidf + spectral clustering 72.06 62.92 67.18 52.96 33.90 4.36 7.72 88.94
tfidf + k-means 69.19 65.01 67.04 50.94 29.95 5.33 9.04 88.62
SVM 26.25 72.23 38.50 75.50

Table 2: Supervised vs. unsupervised clustering models and pairwise classification baselines on the test set, where the gold
labels are from the original Quora annotation. Note that pairwise classification does not provide a good estimation of

clustering accuracy.

experiments, we use the gold standard k of each
example (clustering) as parameter to run the base-
line approaches. This corresponds to comparing
with an upperbound of the baselines.

Measures We compare the output clustering
ŷ =

⊔
j ĉj to the ground truth y

∗ =
⊔

i ci, where
ci, in our case, are either the clusters obtained with
transitive closure from Quora annotation or the
manually annotated categories (see Section 4.1).

For evaluation purposes, we assign each clus-
ter ĉj to the most frequent gold class (cluster), i.e.,
argmaxi|ci ∩ ĉj |, and compute the average preci-
sion over the clustering as:

Precision =
1

N

k̂∑
j=1

maxi|ci ∩ ĉj |, (9)

where N is the number of questions to be clus-
tered, and k̂ is the number of output clusters. This
number is exactly the standard clustering purity
by Zhao and Karypis (2002). Since the purity
is known to favor the clustering outputs with the
large number of clusters, we interchange the roles
of output and gold clusters, which gives us the
clustering

Recall =
1

N

k∑
j=1

maxi|ĉi ∩ cj |, (10)

where k is the number of gold standard clusters.
We then compute F1 from the above measures.

The defined majority-class based clustering
measure allows assigning more than one cluster to
the same gold cluster. The coreference resolution
metric CEAF (Luo, 2005; Cai and Strube, 2010)
solves this issue by finding one-to-one alignment
between the clusters in the output and in the
ground truth, based on which the final score is
computed. We use CEAFe, the variant with the
entity-based similarity, as an alternative evaluation

measure.7 Note that, although we split the data
into samples, all the clustering measures we use,
the majority-based, defined by equations 9 and 10,
and CEAF, are computed over the whole test sets
(not by averaging scores separately for each sam-
ple).

We evaluate the results as well in terms of the
classification scores relative to the correctness of
the models in detecting the pairs of questions
with the same/different intent. This enables the
comparison against the pairwise classification ap-
proaches and an evaluation of their impact. We
compute the Precision, Recall, and Accuracy of
question pairs with the same intent.

5.2 Experiments on Quora

Original question label-based evaluation:
first, we test the models on the clustering data
from the Quora corpus, derived as described in
Section 4.1. We train LSSVM, LSP, and the SVM
classifier on the training part. The results of all
the models on the test set are depicted in Table 2.
In terms of clustering accuracy (the left half of
the table), the LSSVM approach outperforms
all the clustering baselines, improving about 10
points the highest baseline model, i.e., SVM +
k-means both in terms of F1 and CEAF. LSP,
in this setting, shows a slightly worse F1 than
LSSVM, producing a model with high recall.

To study the impact of the pairwise classifier,
we consider all the pairs of questions predicted
by the clustering approaches as belonging to the
same cluster as positive and the rest – as nega-
tive. All the clustering models show high accu-
racy, superior to that shown by the SVM classi-
fier, but this is mainly due to the fact that there
are much more negative question pairs. Interest-
ingly, only LSSVM, though, among all the clus-

7 We used the version 8 of the official coreference scorer
conll.cemantix.org/2012/software.html

conll.cemantix.org/2012/software.html


2317

Model Clustering Pairwise ClassificationPrecision Recall F1 CEAFe Precision Recall F1 Accuracy
LSSVM 84.92 51.76 64.32 49.72 33.24 7.86 12.71 78.07
LSP 71.36 89.45 79.38 59.99 37.22 83.46 51.48 68.03
SVM + spectral clustering 62.31 43.22 51.04 33.04 35.11 2.95 5.44 79.17
SVM + k-means 68.84 47.24 56.03 48.62 42.71 6.48 11.25 79.23
tfidf + spectral clustering 65.83 45.23 53.62 35.46 38.18 3.62 6.62 79.23
tfidf + k-means 65.83 47.24 55.00 38.49 29.31 9.43 14.26 76.98
SVM 40.35 62.33 48.99 73.62

Table 3: Supervised vs. unsupervised clustering models and pairwise classification baselines on the test set, where the gold
labels are provided by the intent-based manual annotation on a portion of the test set.

tering model, approaches the classifier in terms of
classification F1. However, the cluster accuracy
depends on many factors in addition to the pair-
wise classification accuracy.

Intent-based evaluation In Table 3, we present
the results obtained on the portion of the test set
which we manually annotated with the intent clus-
ters. Here, we apply the same LSSVM, LSP, and
the SVM classifier models trained in the experi-
ments of the previous paragraph. However, we re-
compute all the four unsupervised clustering base-
lines supplying them with the new k – the number
of gold intent-based annotated clusters.

In spite of being trained on data with differ-
ent style of annotation (clusters automatically de-
rived form Quora annotation), which is potentially
rather noisy, LSSVM is able to recover the new in-
tent categories better than the baseline approaches
in terms of all the clustering metrics. The differ-
ence from the closest unsupervised clustering ap-
proach, which is the same as in the previous exper-
iment, is now reduced in terms of CEAF. However,
the information about the number of clusters in
the ground truth critically impacts on the accuracy.
The LSP model scored the best with respect to the
new annotation. The lower classification accuracy
of LSSVM with respect to the pairwise classifier
is expected as the cluster number changed notably
with the new annotation.

5.3 Evaluation on the FAQ dataset

Due to the limited size of the FAQ HYPE dataset,
we split it into two parts, each of which forms one
sample. We use the one containing 19 out of 28
clusters for training and the other with the remain-
ing 9 clusters – for testing. The training sample
comprises 97 questions, while the test sample –
50. The plots in Figure 1 illustrate the perfor-
mance of LSSVM and LSP in terms of the clus-
tering F1 in confront to the clustering baselines.

1 5 10 15 20 25 30 35 40 45 50

40

60

80

k
C

lu
st

er
in

g
F1

SVM + k-means SVM + spectral clustering LSSVM
tf-idf + k-means tf-idf + spectral clustering LSP

Figure 1: LSSVM and baseline clustering models; the latter
vary with the cluster number k, on the FAQ HYPE test set.

We run the k-means and the spectral clustering al-
gorithms with k in the range (1, 50), which covers
all the possible values for the given test set size.

LSSVM is better than the spectral clustering
models with any k. k-means curves surpass
LSSVM only in a narrow interval, showing high
unstability. This suggests that guessing the k value
in a realistic scenario in the absence of supervision
does not seem an easy task. It should be also taken
into consideration that we deal with very scarce
training data. This also explains slightly insuffi-
cient accuracy of LSP compared to the k-means
baseline.

5.4 Error Analysis and Discussion

As seen in tables 2 and 3, the structural output
model consistently outperforms approaches based
on spectral and k-means clustering on the Quora
dataset. The most prominent improvement comes
from singleton clusters: questions that are not du-
plicate with any other entries. Recall that the orig-
inal dataset is constructed in such a way that sin-
gleton clusters are somewhat similar or related to
existing material, but are still considered distinct
by Quora annotators. LSSVM correctly recovers



2318

71% of singleton clusters, whereas other methods
perform much worse (5-30%). In the question-
answering setting, singleton clusters correspond to
novel questions that require setting up of a new
entry in the answer base. Accurate recognition of
singletons would allow for a timely allocation of
resources to keep the answer base up-to-date and
in line with incoming user requests.

Larger clusters are problematic for all the com-
pared methods. Still, as evidenced by the CEAF
score8, the structural clustering is doing a better
job at recovering non-singleton clusters. This mir-
rors our observations that even human annotators
have difficulties correctly and consistently detect-
ing duplicates in complex over-populated seman-
tic areas (see Example 7) in the absence of the
global context (e.g., list of all the related ques-
tions).

Finally, the clusters created by the LSSVM ap-
proach are more semantically related. Thus, 97%
of all the suggested clusters contain questions with
the same intent but, possibly, incorrect slots.
For example, in the following question cluster:

(11) gold cluster
Advice-Weightloss: fast,deadline
q1: How do I loose 50 lbs by Dec 2016?
q2: How do I loose weight fast for operation ?
q3: How can I lose 20 lbs super fast to audition for a small
role in a movie ?
q4: I want to lose weight for an event coming up in 2
weeks and I really don’t care if I gain it back afterwords.
What should I do ?

the user wants advice on losing their weight very
fast by a specific deadline. LSSVM groups these
questions with some others, more generic queries
on fast weight loss (How do I loose weight fast?).
This means that LSSVM captures the intent hi-
erarchy well, providing meaningful clusters, al-
though occasionally missing some important de-
tails. Other methods, on the contrary, form more
poorly-related clusters (25-42% of clusters sug-
gested by unsupervised approaches contain unre-
lated intents). Thus, the questions from Example
(11) get grouped by other methods with such unre-
lated queries as How is it to be in true love? (spec-
tral clustering over tf-idf).

Note that neither LSSVM nor unsupervised ap-
proaches have any access to the cluster labels and
their hierarchy: in the training data, we only spec-
ify the clustering itself. Yet, by taking into account

8The reference scorer adopted by the coreference commu-
nity discards singleton clusters.

the global cluster structure, the LSSVM method
can uncover the underlying hierarchy.

In the FAQ setting, most clusters are mid-size
(5-9 questions). All the methods are doing a mod-
erate job at recuperating the intent structure in this
experiment. However, LSSVM shows better per-
formance (cf. Section 5.3). Moreover, structural
output is the only method that perfectly recuper-
ates at least some clusters, e.g.:

(12) q1: Non ricordo piú la password per accedere all’App
(I don’t remember the password for the App)
q2: mi sono dimenticato la password (I forgot the pass-
word)
q3: reimpostare la password (reset the password)
q4: cambio password (change the password)

Here, LSSVM predicts the correct cluster exactly.
K-means based approaches put q1 − q4 into the
same cluster, however, they also merge them with
bloccare gli acquisti online (block the online pur-
chases). Finally, spectral clustering does a poor
job on this particular example, tearing either q1
(tf-idf based spectral clustering) or q2 (SVM pair-
wise classifier-based spectral clustering) apart and
introducing a lot of spurious material.

6 Conclusion

We have proposed structured output methods fed
with semantic question paraphrasing models to
automatically extract user intents from question
repositories. Our approach provides clustering ac-
curacy of 80% with respect to the original Quora
annotation and still valuable accuracy of 65% with
respect one to of the many interpretations of ques-
tion intent of our dataset, carried out by our expert
in dialog modeling. This line of research looks
promising as it can potentially simplify and speed
up the work of Dialog Manager engineers. Al-
though a deeper study is required to assess the ben-
efits of our approach, the feedback of our designer
clearly suggests that automatic clusters, even if
were not perfect, simplify the annotation work.
Several future research directions are enabled by
our study, ranging from the use of neural clus-
tering models to the application of our models
to fast and semi-automatic prototyping of Dialog
Systems. For this purpose, we make our data and
software available to the research community.

Acknowledgements

This research has been supported by a grant from
the Samsung Global Research Outreach Program:
DiQuaVe Samsung GRO 2017.



2319

References
Eneko Agirre, Daniel Cer, Mona Diab, and Aitor

Gonzalez-Agirre. 2012. Semeval-2012 task 6: A pi-
lot on semantic textual similarity. In *SEM 2012:
The First Joint Conference on Lexical and Compu-
tational Semantics – Volume 1: Proceedings of the
main conference and the shared task, and Volume 2:
Proceedings of the Sixth International Workshop on
Semantic Evaluation (SemEval 2012), pages 385–
393. Association for Computational Linguistics.

Eneko Agirre, Daniel Cer, Mona Diab, Aitor Gonzalez-
Agirre, and Weiwei Guo. 2013. *sem 2013 shared
task: Semantic textual similarity. In Second Joint
Conference on Lexical and Computational Seman-
tics (*SEM), Volume 1: Proceedings of the Main
Conference and the Shared Task: Semantic Textual
Similarity, pages 32–43. Association for Computa-
tional Linguistics.

Lloyd Allison and Trevor Dix. 1986. A bit-string
longest-common-subsequence algorithm. Informa-
tion Processing Letters, 23(6):305–310.

Alberto Barrón-Cedeño, Giovanni Da San Martino,
Shafiq Joty, Alessandro Moschitti, Fahad A. Al
Obaidli, Salvatore Romeo, Kateryna Tymoshenko,
and Antonio Uva. 2016. ConvKN at SemEval-2016
Task 3: Answer and question selection for question
answering on Arabic and English fora. In Proceed-
ings of the 10th International Workshop on Semantic
Evaluation, SemEval ’16, San Diego, CA.

Steven M Beitzel, Eric C Jensen, David D Lewis, Ab-
dur Chowdhury, and Ophir Frieder. 2007. Auto-
matic classification of web queries using very large
unlabeled query logs. ACM Transactions on Infor-
mation Systems (TOIS), 25(2):9.

Harry Bunt, Jan Alexandersson, Jean Carletta, Jae-
Woong Choe, Alex Chengyu Fang, Koiti Hasida,
Kiyong Lee, Volha Petukhova, Andrei Popescu-
Belis, Laurent Romary, Claudia Soria, and David
Traum. 2010. Towards an iso standard for dialogue
act annotation. In Proceedings of the Seventh In-
ternational Conference on Language Resources and
Evaluation (LREC’10), Valletta, Malta. European
Language Resources Association (ELRA).

Jie Cai and Michael Strube. 2010. Evaluation met-
rics for end-to-end coreference resolution systems.
In Proceedings of the 11th Annual Meeting of the
Special Interest Group on Discourse and Dialogue,
SIGDIAL ’10, pages 28–36, Stroudsburg, PA, USA.
Association for Computational Linguistics.

Asli Celikyilmaz, Dilek Hakkani-Tür, and Gokhan Tur.
2011. Leveraging web query logs to learn user intent
via bayesian latent variable model.

Daniel Cer, Mona Diab, Eneko Agirre, Inigo Lopez-
Gazpio, and Lucia Specia. 2017. Semeval-2017
task 1: Semantic textual similarity multilingual and
crosslingual focused evaluation. In Proceedings of

the 11th International Workshop on Semantic Eval-
uation (SemEval-2017), pages 1–14. Association for
Computational Linguistics.

Yun-Nung Chen, Dilek Hakkani-Tür, and Xiaodong
He. 2016a. Zero-shot learning of intent embed-
dings for expansion by convolutional deep struc-
tured semantic models. In Acoustics, Speech and
Signal Processing (ICASSP), 2016 IEEE Interna-
tional Conference on, pages 6045–6049. IEEE.

Yun-Nung Chen, Ming Sun, Alexander I Rudnicky, and
Anatole Gershman. 2016b. Unsupervised user in-
tent modeling by feature-enriched matrix factoriza-
tion. In Acoustics, Speech and Signal Processing
(ICASSP), 2016 IEEE International Conference on,
pages 6150–6154. IEEE.

Zheqian Chen, Rongqin Yang, Zhou Zhao, Deng Cai,
and Xiaofei He. 2018. Dialogue act recognition
via crf-attentive structured network. In The 41st
International ACM SIGIR Conference on Research
&#38; Development in Information Retrieval, SI-
GIR ’18, pages 225–234, New York, NY, USA.
ACM.

Mark G. Core and James F. Allen. 1997. Coding di-
alogs with the damsl annotation scheme. In Working
Notes of the AAAI Fall Symposium on Communica-
tive Action in Humans and Machines, pages 28–35,
Cambridge, MA.

Alice Coucke, Adrien Ball, Clement Delpuech,
Clement Doumouro, Sylvain Raybaud, Thibault
Gisselbrecht, and Joseph Dureau. 2017. Bench-
marking natural language understanding systems:
Google, Facebook, Microsoft, Amazon, and Snips.

Giovanni Da San Martino, Alberto Barrón-Cedeño,
Salvatore Romeo, Antonio Uva, and Alessandro
Moschitti. 2016. Learning to re-rank questions in
community question answering using advanced fea-
tures. In Proceedings of the ACM Conference on In-
formation and Knowledge Management, CIKM ’16,
pages 1997–2000, Indianapolis, IN, USA.

P. Deepak. 2016. Mixkmeans: Clustering question-
answer archives. In Proceedings of the 2016 Con-
ference on Empirical Methods in Natural Language
Processing, pages 1576–1585. Association for Com-
putational Linguistics.

Eraldo Rezende Fernandes, Cı́cero Nogueira dos San-
tos, and Ruy Luiz Milidiú. 2014. Latent trees for
coreference resolution. Computational Linguistics,
40(4):801–835.

Simone Filice, Danilo Croce, Alessandro Moschitti,
and Roberto Basili. 2016. KeLP at SemEval-2016
Task 3: Learning semantic relations between ques-
tions and answers. In Proc. of the 10th Intl. Work-
shop on Semantic Evaluation, SemEval ’16, San
Diego, California, USA.



2320

Simone Filice, Giovanni Da San Martino, and Alessan-
dro Moschitti. 2017. KeLP at SemEval-2017 task
3: Learning pairwise patterns in community ques-
tion answering. In Proceedings of the 11th Interna-
tional Workshop on Semantic Evaluation, SemEval
’17, pages 327–334, Vancouver, Canada.

Thomas Finley and Thorsten Joachims. 2005. Super-
vised clustering with support vector machines. In
ICML ’05: Proceedings of the 22nd international
conference on Machine learning, pages 217–224,
New York, NY, USA. ACM.

Paul Jaccard. 1901. Étude comparative de la distribu-
tion florale dans une portion des Alpes et des Jura.
Bulletin del la Société Vaudoise des Sciences Na-
turelles.

Jiwoon Jeon, W. Bruce Croft, and Joon Ho Lee. 2005.
Finding similar questions in large question and an-
swer archives. In Proceedings of the 14th ACM In-
ternational Conference on Information and Knowl-
edge Management, CIKM ’05, pages 84–90, New
York, NY, USA. ACM.

Su Nam Kim, Lawrence Cavedon, and Timothy Bald-
win. 2010. Classifying dialogue acts in one-on-one
live chats. In Proceedings of the 2010 Conference
on Empirical Methods in Natural Language Pro-
cessing, EMNLP ’10, pages 862–871, Stroudsburg,
PA, USA. Association for Computational Linguis-
tics.

Young-Bum Kim, Karl Stratos, and Ruhi Sarikaya.
2016. Scalable semi-supervised query classifica-
tion using matrix sketching. In Proceedings of the
54th Annual Meeting of the Association for Compu-
tational Linguistics (Volume 2: Short Papers), vol-
ume 2, pages 8–13.

Joseph Bernard Kruskal. 1956. On the Shortest Span-
ning Subtree of a Graph and the Traveling Salesman
Problem. In Proceedings of the American Mathe-
matical Society, 7.

Xiao Li, Ye-Yi Wang, and Alex Acero. 2008. Learn-
ing query intent from regularized click graphs. In
Proceedings of the 31st annual international ACM
SIGIR conference on Research and development in
information retrieval, pages 339–346. ACM.

Xiaoqiang Luo. 2005. On coreference resolution per-
formance metrics. In Proceedings of the Confer-
ence on Human Language Technology and Empir-
ical Methods in Natural Language Processing, HLT
’05, pages 25–32, Stroudsburg, PA, USA. Associa-
tion for Computational Linguistics.

Caroline Lyon, James Malcolm, and Bob Dickerson.
2001. Detecting short passages of similar text in
large document collections. In Proceedings of the
2001 Conference on Empirical Methods in Natu-
ral Language Processing, EMNLP, pages 118–125,
Pittsburgh, PA, USA.

Preslav Nakov, Doris Hoogeveen, Lluı́s Màrquez,
Alessandro Moschitti, Hamdy Mubarak, Timothy
Baldwin, and Karin Verspoor. 2017. Semeval-2017
task 3: Community question answering. In Proceed-
ings of the 11th International Workshop on Semantic
Evaluation (SemEval-2017), pages 27–48. Associa-
tion for Computational Linguistics.

Preslav Nakov, Lluı́s Màrquez, Alessandro Moschitti,
Walid Magdy, Hamdy Mubarak, abed Alhakim Frei-
hat, Jim Glass, and Bilal Randeree. 2016. Semeval-
2016 task 3: Community question answering. In
Proceedings of the 10th International Workshop on
Semantic Evaluation (SemEval-2016), pages 525–
545. Association for Computational Linguistics.

Andrew Y. Ng, Michael I. Jordan, and Yair Weiss.
2001. On spectral clustering: Analysis and an al-
gorithm. In Proceedings of the 14th International
Conference on Neural Information Processing Sys-
tems: Natural and Synthetic, NIPS’01, pages 849–
856, Cambridge, MA, USA. MIT Press.

Shereen Oraby, Pritam Gundecha, Jalal Mahmud,
Mansurul Bhuiyan, and Rama Akkiraju. 2017. ”how
may i help you?”: Modeling twitter customer ser-
viceconversations using fine-grained dialogue acts.
In Proceedings of the 22Nd International Confer-
ence on Intelligent User Interfaces, IUI ’17, pages
343–355, New York, NY, USA. ACM.

Andreas Stolcke, Noah Coccaro, Rebecca Bates, Paul
Taylor, Carol Van Ess-Dykema, Klaus Ries, Eliza-
beth Shriberg, Daniel Jurafsky, Rachel Martin, and
Marie Meteer. 2000. Dialogue act modeling for au-
tomatic tagging and recognition of conversational
speech. Comput. Linguist., 26(3):339–373.

Xu Sun, Takuya Matsuzaki, Daisuke Okanohara, and
Jun’ichi Tsujii. 2009. Latent variable perceptron al-
gorithm for structured classification. In Proceedings
of the 21st International Jont Conference on Artifi-
cal Intelligence, IJCAI’09, pages 1236–1242, San
Francisco, CA, USA. Morgan Kaufmann Publishers
Inc.

Balázs Szalkai. 2013. An implementation of the rela-
tional k-means algorithm. CoRR, abs/1304.6899.

Ji-Rong Wen, Jian-Yun Nie, and Hong-Jiang Zhang.
2001. Clustering user queries of a search engine. In
Proceedings of the 10th International Conference on
World Wide Web, WWW ’01, pages 162–168, New
York, NY, USA. ACM.

Michael J Wise. 1996. Yap3: Improved detection of
similarities in computer program and other texts. In
ACM SIGCSE Bulletin, volume 28, pages 130–134.
ACM.

Juan Xu, Qi Zhang, and Xuanjing Huang. 2013. Un-
derstanding the semantic intent of natural language
query. In Proceedings of the Sixth International
Joint Conference on Natural Language Processing,
pages 552–560. Asian Federation of Natural Lan-
guage Processing.



2321

Chun-Nam John Yu and Thorsten Joachims. 2009.
Learning structural svms with latent variables. In
Proceedings of the 26th Annual International Con-
ference on Machine Learning, ICML ’09, pages
1169–1176, New York, NY, USA. ACM.

Ying Zhao and George Karypis. 2002. Criterion func-
tions for document clustering: Experiments and
analysis. Technical report.


