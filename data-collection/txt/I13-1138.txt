










































Using Shallow Semantic Parsing and Relation Extraction for Finding Contradiction in Text


International Joint Conference on Natural Language Processing, pages 1017–1021,
Nagoya, Japan, 14-18 October 2013.

Using Shallow Semantic Parsing and Relation Extraction for Finding
Contradiction in Text

Minh Quang Nhat Pham, Minh Le Nguyen and Akira Shimazu

Japan Advanced Institute of Science and Technology

1-1 Asahidai, Nomi, Ishikawa, 923-1292, JAPAN

{minhpqn,nguyenml,shimazu}@jaist.ac.jp

Abstract

Finding contradiction text is a fundamen-

tal problem in natural language under-

standing. Previous work on finding contra-

diction in text incorporate information de-

rived from predicate-argument structures

as features in supervised machine learning

frameworks. In contrast to previous work,

we combine shallow semantic representa-

tions derived from semantic role labeling

with binary relations extracted from sen-

tences in a rule-based framework. Eval-

uation experiments conducted on stan-

dard data sets indicated that our system

achieves better recall and F1 score for con-

tradiction detection than most of baseline

methods, and the same recall as a state of

the art supervised method for the task.

1 Introduction

Contradiction detection (CD) in text is a funda-

mental task in natural language understanding,

and necessary for many applications (De Marn-

effe et al., 2008; Voorhees, 2008). For instance,

contradictions need to be recognized by question

answering systems or multi-document summariza-

tion systems (Harabagiu et al., 2006). The task

is to detect whether the contradiction relationship

exists in a pair of a text T and a hypothesis H.

There are several approaches to the CD task.

Contradiction detection can be formalized as a

binary classification problem (Harabagiu et al.,

2006; De Marneffe et al., 2008). The main effort

of work which adopt this approach is to find out ef-

fective features for recognizing contradiction. The

other approach is using functional relations indi-

cated by verb or noun phrases for detecting con-

tradiction (Ritter et al., 2008).

Beyond string-based matching approaches, one

can approach to the CD task by applying logical

inference techniques. Although the logical infer-

ence approach may obtain good precision, it is

not widely used for the task due to the fact that

full predicate-logic analysis is currently not prac-

tical for wide-coverage semantic processing (Bur-

chardt et al., 2009). Given that fact, (Burchardt et

al., 2009) pointed out that using shallow seman-

tic representations based on predicate-argument

structures and frame knowledge is an intuitive

and straightforward approach to textual inference

tasks.

In contrast to previous work which integrate

predicate-argument structures as features in ma-

chine learning-based systems (Harabagiu et al.,

2006; De Marneffe et al., 2008), this paper com-

bines shallow semantic representations derived

from semantic role labeling with binary relations

extracted from sentences for the CD task. The

proposed system consists of two modules. The

first module relies on the alignment of semantic

role (SRL) frames extracted from the text and the

hypothesis in each pair while the second one per-

forms contradiction detection over binary relations

extracted from the pair. If the SRL-based mod-

ule fails to identify the contradiction relationship

in the pair, the second module will be applied.

We expect that the second module will improve

the coverage of the first one. Evaluation exper-

iments on standard data sets obtained from RTE

challenges (Giampiccolo et al., 2007; Giampic-

colo et al., 2008; Bentivogli et al., 2009) show

that the proposed system achieves better recall and

F1 score for contradiction detection than most of

baseline methods, and the same recall as a state of

the art supervised method for the task.

2 Linguistic Analysis

After parsing the text and the hypothesis of a pair

by using Stanford CoreNLP 1, we utilize SENNA

1Stanford CoreNLP is available online on:
http://nlp.stanford.edu/software/corenlp.shtml

1017



package2 (Collobert et al., 2011) for semantic role

labeling. Then, we extract SRL frames from the

output of SENNA. An SRL frame consists of a

verb predicate and a list of SRL elements.

In the system, we use REVERB (Fader et al.,

2011) – a tool which can automatically identify

and extract binary relations from English sen-

tences. The input of REVERB is a POS-tagged

and NP-chunked sentence and its output is a set

of extraction triples of the form (arg1, R, arg2),

in which R represents the relation phrase between

two arguments: arg1 and arg2.

REVERB cannot extract some useful relations

such as “isA” relations which specify the equiva-

lent relation of two objects. In addition, in some

cases, relation phrases of two extraction triples

cannot be compared without using inference rules

that specify the entailment relationship between

two triples. Therefore, we propose several sim-

ple heuristic methods to extract additional binary

relations from a text segment.

First, we extract “isA” relations from three in-

formation sources: i) co-reference resolution in-

formation; ii) noun phrases which the ending parts

are recognized as a named entity;; and iii) “ab-

brev” relations in dependency parses.

Second, entailment rules or inference rules

which specify directional entailment relations be-

tween two text fragments have been shown to be

useful for RTE and question answering (Berant et

al., 2011). In this study, we transform triples gen-

erated by REVERB by looking up the corpus of

30, 000 entailment rules between typed predicates
obtained from (Berant et al., 2011).

3 Contradiction Detection by Matching

Semantic Frames

Let us denote an SRL frame by a tuple S =
{V,E1, . . . , Ek}, where V is used to denote the
verb predicate; and Ei represents the i-th SRL el-

ement in the frame. Each SRL element has a type

and underlying words. Types of SRL elements fol-

low the annotation guideline in PropBank (Palmer

et al., 2005). SRL elements can be arguments

or modifiers (adjuncts). We denote two sets of

SRL frames of T and H by T = {S
(t)
i }

m
i=1 and

H = {S
(h)
j }

n
j=1, in which m and n are the num-

ber of SRL frames extracted from T and H, respec-

tively.

2SENNA is available online on: http://ml.nec-
labs.com/senna/

3.1 Contradiction Detection Model

The contradiction detection model consists of a

contradiction function FS(T,H) which calculates
the contradiction measurement for the pair (T, H)

on their SRL frames. Then, FS(T,H) is com-
pared with a threshold value t1. IfFS(T,H) ≥ t1,
we determine that T and H are contradictory.

In order to define the contradiction function

FS(T,H), we rely on the assumption that T and
H are contradictory if there exists an event indi-

cated by an SRL frame in H, which is incompat-

ible with an event indicated by T. Formally, the

function FS(T,H) is defined as following:

FS(T,H) = max
S
(t)
i ∈T,S

(h)
j ∈H

f(S
(t)
i , S

(h)
j ), (1)

where S
(t)
i and S

(h)
j are two SRL frames in T and

H, respectively; and f(S
(t)
i , S

(h)
j ) is a contradic-

tion function defined on the two SRL frames.

Next, we define the function f(S
(t)
1 , S

(h)
2 ) of

two SRL frames S
(t)
1 ∈ T and S

(t)
2 ∈ H . For con-

creteness, we denote S
(t)
1 = {V1, E

(1)
1 , . . . , E

(1)
k }

and S
(h)
2 = {V2, E

(2)
1 , . . . , E

(2)
ℓ }.

The function f(S
(t)
1 , S

(h)
2 ) relies on the align-

ment of SRL elements across two frames. Since

the number of SRL elements in an SRL frame is

not very large, we propose a greedy alignment al-

gorithm that considers all possible pairs of an SRL

element in S
(t)
1 and an SRL element in S

(h)
2 . The

core part of the greedy algorithm is the similarity

measure between two SRL elements. We apply the

local lexical level matching method (Dagan et al.,

2007) to calculate the similarity of two SRL ele-

ments. In addition, we utilize co-reference resolu-

tion information by substituting mentions found in

an SRL element with their equivalent mentions in

the corresponding co-reference chain.

After generating the alignment between ele-

ments of two SRL frames, we define the contra-

diction function f(S
(t)
1 , S

(h)
2 ) as follows.

From the rationale that two events are not con-

tradictory if they are not related, we filter out

“not contradictory” SRL frame pairs by calculat-

ing their relatedness. The relatedness of two SRL

frames is defined as product of the relatedness of

their verb predicates and SRL elements:

R(S
(t)
1 , S

(h)
2 ) = R(V1, V2)×maxi,jR(E

(1)
i , E

(2)
j ),
(2)

1018



where R represents the relatedness between two

items; E
(1)
i ∈ S

(t)
1 and E

(2)
j ∈ S

(h)
2 are SRL el-

ements; V1 and V2 are verbs of S
(t)
1 and S

(h)
2 , re-

spectively.

The relatedness of two verbs is assigned to 1.0
if their relation is found in WordNet (Fellbaum,

1998) or in VerbOcean database (Chklovski and

Pantel, 2004). In other cases, we employ Word-

Net::Similarity package (Pedersen et al., 2004) to

compute the similarity of two verbs. The related-

ness of two SRL elements E
(1)
i and E

(2)
j is defined

as the local lexical level matching score.

The relatedness of two SRL frames is compared

with a threshold. If it is below the threshold, then

S
(t)
1 and S

(h)
2 are not related.

If two SRL frames are related, we consider two

situations: 1) two verb predicates are matching

and 2) Two verb predicates are opposite. Note that

if two verb predicates are neither matching nor op-

posite, f(S
(t)
1 , S

(h)
2 ) is also assigned to 0.

In the system, that two verbs are matching are

determined by utilizing synonyms in WordNet and

WordNet-base semantic similarity. If two verb

are matching, the function f(S
(t)
1 , S

(h)
2 ) is defined

based on the alignment generated in the alignment

process. We use the incompatibility of aligned ar-

guments and modifiers such as temporal, location,

or negation modifiers to calculate f(S
(t)
1 , S

(h)
2 ).

In the second case, two verbs are opposite if

they are found as antonym verbs in WordNet or

opposite verbs in VerbOcean. In this case, the

contradiction function f(S
(t)
1 , S

(h)
2 ) is defined as

the similarity of their SRL elements. We define

the element-based similarity of two frames as the

product of similarity scores of the aligned ele-

ments having the same type.

4 Contradiction Detection by Relation

Matching

The main idea of this module is as follows. In

the first step, we extract triples from T and H by

using REVERB tool and our heuristics. Next, we

compare each triple in H with every triple in T, and

determine whether the contradiction relationship

exists in some pairs of triples.

Formally, we denote a extraction triple by

(x, r, y) where x and y respectively represent the
first and second argument, and r represents the re-

lation phrase of the triple.

We denote T = {(x
(t)
i , r

(t)
i , y

(t)
i )}

m
i=1 and H =

{(x
(h)
j , r

(h)
j , y

(h)
j )}

n
j=1. Here, m and n are respec-

tively the numbers of triples in T and H. The con-

tradiction detection task is reduced to searching

for incompatible triple pairs across T and H. We

define the contradiction function on triples of T

and H as follows.

FT (T,H) = max
Ti∈T ;Hj∈H

g(Ti,Hj), (3)

where Ti is the i-th triple of T; Hj is the j-th triple

of H; and g(Ti,Hj) is the contradiction function
of the two triples Ti and Hj .

The function g(Ti,Hj) is based on the mis-
match of two triples Ti and Hj . We consider

three cases as follows. If their relation phrases

and first arguments are matching, the mismatch of

second arguments will be calculated. If two rela-

tion phrases are matching and roles of arguments

in the two triples are exchanged, g(Ti,Hj) is as-
signed to 1.0. However, this rule is not applied
for “isA” (equivalent) relations. In contrast, if two

relation phrases are opposite, the similarity mea-

sures of first arguments and second arguments are

taken into account.

In the procedure for calculating g(Ti,Hj), we
need to determine whether two relation phrases

r
(t)
i and r

(h)
j are matching or not. If the surface

and base forms of two relation phrases are differ-

ent, we use WordNet to detect whether main verbs

of r
(t)
i and r

(h)
j are synonyms. In order to check if

two relation phrases r
(t)
i and r

(h)
j are opposite or

not, we utilize antonym relations in WordNet and

opposite relations in VerbOcean.

In the module, that two arguments are match-

ing is checked by using their similarity. The sim-

ilarity score of two arguments is computed by the

same method as that for computing the similarity

of two SRL elements. When we detect the contra-

diction of two arguments, we use the contradiction

rule as follows. Two arguments are contradictory

if they include two entities having the same type

but different values. Especially, we take into ac-

count four categories: NUMBER, DATE, TIME,

and LOCATION. In other cases, we use the simi-

larity of two arguments as the evidence for contra-

diction detection.

5 Evaluation Experiments

5.1 Data Sets

In experiments, we evaluate the proposed method

on the test sets of the three-way subtask at RTE-

1019



Table 1: Label distribution in three test sets
Data Set Contradiction Entailment Unknown Total

RTE-3 Test 72 410 318 800

RTE-4 Test 150 500 350 1000

RTE-5 Test 90 300 210 600

3, RTE-4, and RTE-5 competitions (Giampiccolo

et al., 2007; Giampiccolo et al., 2008; Bentivogli

et al., 2009). The development sets provided at

each competition are used to tuned threshold val-

ues in two CD modules of the system. The three-

way subtask requires participant systems to decide

whether the entailment, contradiction, or unknown

relationship exists in a pair. Since in this study,

we focus on contradiction relationship in a text

pair, entailment and unknown labels in data sets

are converted into non-contradiction labels. Ta-

ble 1 provides statistics on the test sets of three-

way subtask in RTE-3, RTE-4, and RTE-5.

The data sets used in experiments are unbal-

anced, so the average accuracy over all labels is

not an appropriate evaluation measures. There-

fore, we use Precision, Recall, and F1 scores of

the contradiction label as evaluation measures.

5.2 Baseline Methods

The first baseline method is the method presented

in (De Marneffe et al., 2008), which employed su-

pervised machine learning techniques for the CD

task. To the best of our knowledge, (De Marneffe

et al., 2008) is the only contradiction detection-

focused work that evaluates on data sets of RTE

challenges.

The second baseline is the BLUE system of

Boeing’s team (Clark and Harrison, 2009) at RTE-

4 and RTE-5 competitions. The BLUE system

adopted the logical inference approach to RTE,

which performs inference on logic-based repre-

sentations of the text and the hypothesis in a pair.

We use best scores among submitted runs of the

BLUE system at each competition.

In experiments, we also compare the results

achieved by our system with average results of

submitted systems for three-way subtask at RTE-

3, RTE-4 and RTE-5 challenges. The numbers

submitted systems in RTE-3, RTE-4 and RTE-5

for the three-way subtask are 12, 34, and 24 sub-

missions, respectively.

In order to assess the effectiveness of the two-

stage system scheme, we separately run each CD

module on the three data sets and compare the re-

sults with those of the combined system.

5.3 Experimental Results

Table 2 provides experimental results achieved on

test sets of RTE-3, RTE-4, and RTE-5 challenges

by our system and baseline methods. As shown

in results, the proposed system consistently ob-

tained better recall values and F1 scores than those

of baseline methods except the supervised ma-

chine learning-based method in (De Marneffe et

al., 2008). Compared with the method presented

(De Marneffe et al., 2008), our system achieves

the same recall but lower precision.

The results shown in Table 2 indicated that the

SRL-based module consistently achieved better

recall and F1 score than those of the triple-based

module. A possible explanation is that the infor-

mation contained in shallow semantic representa-

tions is richer than that of extraction triples, so

the SRL-based module covers more contradiction

phenomena than the triple-based module. As ex-

pected, the combined system consistently obtained

better recall and F1 score than each separate mod-

ule. Experimental results confirmed our observa-

tion that the second backup module increases the

coverage of contradiction phenomena for our sys-

tem.

6 Conclusion

In this paper, we have presented a new rule-based

method for finding contradiction in text, which

combines shallow semantic representations with

binary relations extracted from sentences. We de-

fine contradiction measurements on the predicate-

argument structures and binary relations extracted

from the text and the hypothesis in a pair. We

deal with the low-coverage problem of semantic

role resources by using a backup module which

exploits extraction triples. Experimental results

achieved on standard data sets showed that our

proposed system obtained better recall and F1

score for contradiction detection than most of

baseline methods.

1020



Table 2: Experimental results on three data sets

Method
RTE-3 Pilot RTE-4 Test RTE-5 Test

P R F1 P R F1 P R F1

De Marneffe (2008) 22.95 19.44 21.04 – – – – – –

BLUE system – – – 41.67 10.0 16.13 42.86 6.67 11.54

Average result 10.72 11.69 11.18 25.26 13.47 13.63 26.40 13.70 14.79

SRL-based 13.41 15.27 14.28 22.41 17.33 19.55 22.72 16.67 19.23

Triple-based 22.58 9.72 13.59 26.3 10.0 14.49 19.48 16.67 17.96

Two-stage (our system) 14.0 19.44 16.27 23.0 22.67 22.82 21.14 28.89 24.4

References

L. Bentivogli, I. Dagan, H. T. Dang, D. Giampiccolo,
and B. Magnini. 2009. The fifth pascal recognizing
textual entailment challenge. In In Proceedings of
TAC Workshop.

Jonathan Berant, Ido Dagan, and Jacob Goldberger.
2011. Global learning of typed entailment rules. In
Proceedings of the 49th Annual Meeting of the Asso-
ciation for Computational Linguistics: Human Lan-
guage Technologies, pages 610–619, Portland, Ore-
gon, USA, June. Association for Computational Lin-
guistics.

Aljoscha Burchardt, Marco Pennacchiotti, Stefan
Thater, and Manfred Pinkal. 2009. Assessing
the impact of frame semantics on textual entail-
ment. Natural Language Engineering, 15(Special
Issue 04):527–550.

Timothy Chklovski and Patrick Pantel. 2004. Ver-
bocean: Mining the web for fine-grained semantic
verb relations. In Dekang Lin and Dekai Wu, ed-
itors, Proceedings of EMNLP 2004, pages 33–40,
Barcelona, Spain, July. Association for Computa-
tional Linguistics.

Peter Clark and Phil Harrison. 2009. Recognizing tex-
tual entailment with logical inference. In In Pro-
ceedings of the First Text Analysis Conference (TAC
2008).

Ronan Collobert, Jason Weston, Léon Bottou, Michael
Karlen, Koray Kavukcuoglu, and Pavel Kuksa.
2011. Natural language processing (almost) from
scratch. J. Mach. Learn. Res., 999888:2493–2537,
November.

Ido Dagan, Dan Roth, and Fabio Massimo. 2007. A
tutorial on textual entailment.

Marie-catherine De Marneffe, Anna N Rafferty, and
Christopher D Manning. 2008. Finding contradic-
tions in text. In In Proceedings of ACL 2008.

Anthony Fader, Stephen Soderland, and Oren Etzioni.
2011. Identifying relations for open information ex-
traction. In Proceedings of the Conference on Em-
pirical Methods in Natural Language Processing,
EMNLP ’11, pages 1535–1545, Stroudsburg, PA,
USA. Association for Computational Linguistics.

Christiane Fellbaum. 1998. WordNet: An Electronic
Lexical Database. MIT Press.

Danilo Giampiccolo, Bernardo Magnini, Ido Dagan,
and Bill Dolan. 2007. The third pascal recogniz-
ing textual entailment challenge. In Proceedings of
the ACL-PASCAL Workshop on Textual Entailment
and Paraphrasing, pages 1–9.

Danilo Giampiccolo, Hoa Trang Dang, Bernardo
Magnini, Ido Dagan, Elena Cabrio, and Bill Dolan.
2008. The fourth pascal recognizing textual entail-
ment challenge. In In Proceedings of TAC 2008
Workshop.

Sanda Harabagiu, Andrew Hickl, and Finley Lacatusu.
2006. Negation, contrast, and contradiction in text
processing. In Proceedings of the Twenty-First Na-
tional Conference on Artificial Intelligence (AAAI-
06).

Martha Palmer, Daniel Gildea, and Paul Kingsbury.
2005. The proposition bank: An annotated corpus
of semantic roles. Comput. Linguist., 31(1):71–106,
March.

Ted Pedersen, Siddharth Patwardhan, and Jason Miche-
lizzi. 2004. Wordnet::similarity: measuring the re-
latedness of concepts. In Demonstration Papers at
HLT-NAACL 2004, HLT-NAACL–Demonstrations
’04, pages 38–41, Stroudsburg, PA, USA. Associ-
ation for Computational Linguistics.

Alan Ritter, Stephen Soderland, Doug Downey, and
Oren Etzioni. 2008. It’s a contradiction – no, it’s
not: A case study using functional relations. In Pro-
ceedings of the 2008 Conference on Empirical Meth-
ods in Natural Language Processing, pages 11–20,
Honolulu, Hawaii, October. Association for Compu-
tational Linguistics.

Ellen M. Voorhees. 2008. Contradictions and justifica-
tions: Extensions to the textual entailment task. In
Proceedings of ACL-08: HLT, pages 63–71, Colum-
bus, Ohio, June. Association for Computational Lin-
guistics.

1021


