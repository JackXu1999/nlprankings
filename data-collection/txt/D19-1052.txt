



















































Neural data-to-text generation: A comparison between pipeline and end-to-end architectures


Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing
and the 9th International Joint Conference on Natural Language Processing, pages 552–562,
Hong Kong, China, November 3–7, 2019. c©2019 Association for Computational Linguistics

552

Neural data-to-text generation:
A comparison between pipeline and end-to-end architectures

Thiago Castro Ferreira1,2 Chris van der Lee1 Emiel van Miltenburg1 Emiel Krahmer1
1Tilburg center for Cognition and Communication (TiCC), Tilburg University, The Netherlands

2Federal University of Minas Gerais (UFMG), Brazil
{tcastrof,c.vdrlee}@tilburguniversity.edu

{c.w.j.vanmiltenburg,e.j.krahmer}@tilburguniversity.edu

Abstract

Traditionally, most data-to-text applications
have been designed using a modular pipeline
architecture, in which non-linguistic input data
is converted into natural language through sev-
eral intermediate transformations. By contrast,
recent neural models for data-to-text gener-
ation have been proposed as end-to-end ap-
proaches, where the non-linguistic input is
rendered in natural language with much less
explicit intermediate representations in be-
tween. This study introduces a systematic
comparison between neural pipeline and end-
to-end data-to-text approaches for the gen-
eration of text from RDF triples. Both ar-
chitectures were implemented making use of
the encoder-decoder Gated-Recurrent Units
(GRU) and Transformer, two state-of-the art
deep learning methods. Automatic and human
evaluations together with a qualitative analy-
sis suggest that having explicit intermediate
steps in the generation process results in better
texts than the ones generated by end-to-end ap-
proaches. Moreover, the pipeline models gen-
eralize better to unseen inputs. Data and code
are publicly available.1

1 Introduction

Data-to-text Natural Language Generation (NLG)
is the computational process of generating mean-
ingful and coherent natural language text to de-
scribe non-linguistic input data (Gatt and Krah-
mer, 2018). Practical applications can be found
in domains such as weather forecasts (Mei et al.,
2016), health care (Portet et al., 2009), feedback
for car drivers (Braun et al., 2018), diet manage-
ment (Anselma and Mazzei, 2018), election re-
sults (Leppänen et al., 2017) and sportscasting
news (van der Lee et al., 2017).

1https://github.com/ThiagoCF05/
DeepNLG/

Traditionally, most of data-to-text applications
have been designed in a modular fashion, in which
the non-linguistic input data (be it, say, numerical
weather information or game statistics) are con-
verted into natural language (e.g., weather fore-
cast, game report) through several explicit inter-
mediate transformations. The most prominent ex-
ample is the ‘traditional’ pipeline architecture (Re-
iter and Dale, 2000) that performs tasks related
to document planning, sentence planning and lin-
guistic realization in sequence. Many of the tradi-
tional, rule-based NLG systems relied on modules
because (a) these modules could be more easily
reused across applications, and (b) because going
directly from input to output using rules was sim-
ply too complex in general (see Gatt and Krahmer
2018 for a discussion of different architectures).

The emergence of neural methods changed this:
provided there is enough training data, it does be-
come possible to learn a direct mapping from in-
put to output, as has also been shown in, for ex-
ample, neural machine translation. As a result,
in NLG more recently, neural end-to-end data-to-
text models have been proposed, which directly
learn input-output mappings and rely much less on
explicit intermediate representations (Wen et al.,
2015; Dušek and Jurcicek, 2016; Mei et al., 2016;
Lebret et al., 2016; Gehrmann et al., 2018).

However, the fact that neural end-to-end ap-
proaches are possible does not necessarily entail
that they are better than (neural) pipeline mod-
els. On the one hand, cascading of errors is a
known problem of pipeline models in general (an
error in an early module will impact all later mod-
ules in the pipeline), which (almost by definition)
does not apply to end-to-end models. On the other
hand, it is also conceivable that developing ded-
icated neural modules for specific tasks leads to
better performance on each of these successive
tasks, and combining them might lead to better,

https://github.com/ThiagoCF05/DeepNLG/
https://github.com/ThiagoCF05/DeepNLG/


553

and more reusable, output results. In fact, this has
never been systematically studied, and this is the
main goal of the current paper.

We present a systematic comparison between
neural pipeline and end-to-end data-to-text ap-
proaches for the generation of output text from
RDF input triples, relying on an augmented ver-
sion of the WebNLG corpus (Gardent et al.,
2017b). Using two state-of-the-art deep learning
techniques, GRU (Cho et al., 2014) and Trans-
former (Vaswani et al., 2017), we develop both
a neural pipeline and an end-to-end architecture.
The former, which also makes use of the Neu-
ralREG approach (Castro Ferreira et al., 2018a),
tackles standard NLG tasks (discourse ordering,
text structuring, lexicalization, referring expres-
sion generation and textual realization) in se-
quence, while the latter does not address these in-
dividual tasks, but directly tries to learn how to
map RDF triples into corresponding output text.

Using a range of evaluation techniques, includ-
ing both automatic and human measures, com-
bined with a qualitative analysis, we provide an-
swers to our two main research questions: (RQ1)
How well do deep learning methods perform as
individual modules in a data-to-text pipeline ar-
chitecture? And (RQ2) How well does a neural
pipeline architecture perform compared to a neu-
ral end-to-end one? Our results show that adding
supervision during the data-to-text generation pro-
cess, by distinguishing separate modules and com-
bining them in a pipeline, leads to better results
than full end-to-end approaches. Moreover, the
pipeline architecture offers somewhat better gen-
eralization to unseen domains and compares favor-
ably to the current state-of-the-art.

2 Data

The experiments presented in this work were con-
ducted on the WebNLG corpus (Gardent et al.,
2017a,b), which consists of sets of 〈 Subject, Pred-
icate, Object 〉 RDF triples and their target texts. In
comparison with other popular NLG benchmarks
(Belz et al., 2011; Novikova et al., 2017; Mille
et al., 2018), WebNLG is the most semantically
varied corpus, consisting of 25,298 texts describ-
ing 9,674 sets of up to 7 RDF triples in 15 do-
mains. Out of these domains, 5 are exclusively
present in the test set, being unseen during the
training and validation processes. Figure 1 depicts
an example of a set of 3 RDF triples and its related

A.C. Cesena manager Massimo Drago
Massimo Drago club S.S.D. Potenza Calcio
Massimo Drago club Calcio Catania

↓

Massimo Drago played for the club SSD Potenza Calcio and
his own club was Calcio Catania. He is currently managing
AC Cesena.

Figure 1: Example of a set of triples (top) and the cor-
responding text (bottom).

text.
To evaluate the intermediate stages between the

triples and the target text, we use the augmented
version of the WebNLG corpus (Castro Ferreira
et al., 2018b), which provides gold-standard rep-
resentations for traditional pipeline steps, such as
discourse ordering (i.e., the order in which the
source triples are verbalized in the target text), text
structuring (i.e., the organization of the triples into
paragraph and sentences), lexicalization (i.e., ver-
balization of the predicates) and referring expres-
sion generation (i.e., verbalization of the entities).

3 Pipeline Architecture

Based on Reiter and Dale (2000), we propose a
pipeline architecture which converts a set of RDF
triples into text in 5 sequential steps.

3.1 Discourse Ordering

Originally designed to be performed when the
document is planned, Discourse Ordering is the
process of determining the order in which the
communicative goals should be verbalized in the
target text. In our case, the communicative goals
are the RDF triples received as input by the model.

Given a set of linearized triples, this step deter-
mines the order in which they should be verbal-
ized. For example, given the triple set in Figure 1
in the linearized format:

<TRIPLE> A.C. Cesena manager Massimo Drago
</TRIPLE> <TRIPLE> Massimo Drago club
S.S.D. Potenza Calcio </TRIPLE> <TRIPLE> Mas-
simo Drago club Calcio Catania </TRIPLE>

Our discourse ordering model would ideally re-
turn the set club club manager, which later
is used to retrieve the input triples on the predicted
order. In case of triples with the same predicates,
as club, our implementation will randomly re-
trieve the triples.



554

3.2 Text Structuring

Text Structuring is the step which intends to orga-
nize the ordered triples into paragraphs and sen-
tences. Since the WebNLG corpus only contains
single-paragraph texts, this step will be only evalu-
ated on sentence planning, being closer to the Ag-
gregation task of the original architecture (Reiter
and Dale, 2000). However, it can be easily ex-
tended to predict paragraph structuring in multi-
paragraph datasets.

Given a linearized set of ordered triples, this
step works by generating the predicates seg-
mented by sentences based on the tokens <SNT>
and </SNT>. For example, given the ordered
triple set in Figure 1 in the same linearized
format as in Discourse Ordering, the module
would generate <SNT> club club </SNT>
<SNT> manager </SNT>, where predicates
are replaced by the proper triples for the next step.

3.3 Lexicalization

Lexicalization involves finding the proper phrases
and words to express the content to be included
in each sentence (Reiter and Dale, 2000). In this
study, given a linearized ordered set of triples seg-
mented by sentences, the Lexicalization step aims
to predict a template which verbalizes the predi-
cates of the triples. For our example based on Fig-
ure 1, given the ordered triple set segmented by
sentences in the following format:

<SNT> <TRIPLE> Massimo Drago club
S.S.D. Potenza Calcio </TRIPLE> <TRIPLE> Mas-
simo Drago club Calcio Catania </TRIPLE> </SNT>
<SNT> <TRIPLE> A.C. Cesena manager Massimo Drago
</TRIPLE> </SNT>

This step would ideally return a template like:

ENTITY-1 VP[aspect=simple, tense=past, voice=active,
person=null, number=null] play for DT[form=defined]
the club ENTITY-2 and ENTITY-1 own club
VP[aspect=simple, tense=past, voice=active, per-
son=null, number=singular] be ENTITY-3 .
ENTITY-1 VP[aspect=simple, tense=present,
voice=active, person=3rd, number=singular] be cur-
rently VP[aspect=progressive, tense=present, voice=active,
person=null, number=null] manage ENTITY-4 .

The template format not only selects the proper
phrases and words to verbalize the predicates, but
also does indications for the further steps. The
general tags ENTITY-[0-9] indicates where
references should be realized. The number in
an entity tag indicates the entity to be realized

based on its occurrence in the ordered triple
set. For instance, ENTITY-3 refers to the entity
Calcio Catania, the third mentioned entity in
the ordered triple set.

Information for the further textual realization
step is stored in the tags VP, which contains the as-
pect, mood, tense, voice and number of the subse-
quent lemmatized verb, and DT, which depicts the
form of the subsequent lemmatized determiner.2

3.4 Referring Expression Generation

Referring Expression Generation (REG) is the
pipeline task responsible for generating the refer-
ences to the entities of the discourse (Krahmer and
van Deemter, 2012). As previously explained, the
template created in the previous step depicts where
and to which entities such references should be
generated. Given our example based in Figure 1,
the result of the REG step for the template pre-
dicted in the previous step would be:

Massimo Drago VP[...] play for DT[...] the club SSD
Potenza Calcio and his own club VP[...] be Calcio Cata-
nia . He VP[...] be currently VP[...] manage AC
Cesena .

To perform the task, we used the NeuralREG
algorithm (Castro Ferreira et al., 2018a). Given a
reference to be realized, this algorithm works by
encoding the template before (pre-context) and af-
ter (post-contex) the reference using two different
Bidirectional LSTMs (Hochreiter and Schmidhu-
ber, 1997). Attention vectors are then computed
for both vectors and concatenated together with
the embedding of the entity. Finally, this repre-
sentation is decoded into the referring expression
to the proper entity in the given context.3

3.5 Textual Realization

Textual Realization aims to perform the last
steps of converting the non-linguistic data into
text. In our pipeline architecture this in-
cludes setting the verbs (e.g., VP[aspect=simple,
tense=past, voice=passive, person=3rd, num-
ber=singular] locate → was located) and de-
terminers (DT[form=undefined] a American na-
tional → an American national) to their right for-
mats. Both verbs and determiners are solved in a

2Both kind of tags with their respective information are
treated as a single token.

3NeuralREG works with the Wikipedia representation of
the entities (e.g., Massimo Drago) in the templates instead
of general tags (e.g., ENTITY-1).



555

rule-based strategy, where the implications are ex-
tracted from the training set. This step will not be
individually evaluated as the other ones.

4 End-to-End Architecture

Our end-to-end architecture is similar to novel
data-to-text models (Wen et al., 2015; Dušek and
Jurcicek, 2016; Mei et al., 2016; Lebret et al.,
2016; Gehrmann et al., 2018), which aim to
convert a non-linguistic input into natural lan-
guage without explicit intermediate representa-
tions, making use of Neural Machine Translation
techniques. In this study, our end-to-end architec-
ture intends to directly convert an unordered (lin-
earized) set of RDF triples into text.

5 Models Set-Up

Both pipeline steps and the end-to-end archi-
tecture were modelled using two deep learning
encoder-decoder approaches: Gated-Recurrent
Units (GRU; Cho et al. 2014) and Transformer
(Vaswani et al., 2017). These models differ in the
way they encode their input. GRUs encode the
input data by going over the tokens one-by-one,
while Transformers (which do not have a recur-
rent structure) may encode the entire source se-
quence as a whole, using position embeddings to
keep track of the order. We are particularly inter-
ested in the capacity of such approaches to learn
order and structure in the process of text genera-
tion. The model settings are explained in the sup-
plementary materials.

6 Experiment 1: Learning the pipeline

Most of the data-to-text pipeline applications have
their steps implemented using rule-based or sta-
tistical data-driven models. However, these tech-
niques have shown to be outperformed by deep
neural networks in other Computational Linguis-
tics subfields and in particular pipeline steps like
Referring Expression Generation. NeuralREG
(Castro Ferreira et al., 2018a), for instance, out-
performs other techniques in generating references
and co-references along a single-paragraph text.
Given this context, our first experiment intends to
analyze how well deep learning methods perform
particular steps of the pipeline architecture, like
Discourse Ordering, Text Structuring, Lexicaliza-
tion and Referring Expression Generation, in com-
parison with simpler data-driven baselines.

6.1 Data
We used version 1.5 of the augmented WebNLG
corpus (Castro Ferreira et al., 2018b)4 to evaluate
the steps of our pipeline approach. Based on its
intermediate representations, we extracted gold-
standards to train and evaluate the different steps.

Discourse Ordering We used pairs of RDF
triple sets and their ordered versions to evaluate
our Discourse Ordering approaches. For the cases
in the training set where a triple set was verbalized
in more than one order, we added one entry per
verbalization taking the proper order as the target.
To make sure the source set followed a pattern, we
ordered the input according to the alphabetic or-
der of its predicates, followed by the alphabetical
order of its subjects and objects in case of similar
predicates. In total, our Discourse Ordering data
consists of 13,757, 1,730 and 3,839 ordered triple
sets for 5,152, 644 and 1,408 training, develop-
ment and test input triple sets, respectively.

Text Structuring 14,010, 1,752 and 3,955
structured triple sets were extracted for 10,281,
1,278 and 2,774 training, development and test or-
dered triple sets, respectively.

Lexicalization 18,295, 2,288 and 5,012 lexical-
ization templates were used for 12,814, 1,601 and
3,463 training, development and test structured
triple sets, respectively.

Referring Expression Generation To evaluate
the performance of the REG models, we extracted
67,144, 8,294 and 19,210 reference instances from
training, development and test part of the corpus.
Each instance consists of the cased tokenized re-
ferring expression, the identifier of the target entity
and the uncased tokenized pre- and post-contexts.

6.2 Metrics
Discourse Ordering and Text Structuring ap-
proaches were evaluated based on their accuracy
to predict one of the gold-standards given the in-
put (many of the RDF triple sets in the corpus were
verbalized in more than one order and structure).
Referring Expression Generation approaches were
also evaluated based on their accuracy to predict
the uncased tokenized gold-standard referring ex-
pressions. Lexicalization was evaluated based on
the BLEU score of the predicted templates in their
uncased tokenized form.

4https://github.com/ThiagoCF05/webnlg

https://github.com/ThiagoCF05/webnlg


556

6.3 Baselines

We proposed random and majority baselines for
the steps of Discourse Ordering, Text Structuring
and Lexicalization. In comparison with Neural-
REG, we used the OnlyNames baseline, also in-
troduced in Castro Ferreira et al. (2018a).

Discourse Ordering The random baseline re-
turns the triple set in a random order, whereas the
majority one returns the most frequent order of the
input predicates in the training set. For unseen sets
of predicates, the majority model returns the triple
set in the same order as the input.

Text Structuring The random baseline for this
step chooses a random split of triples in sentences,
inserting the tags <SNT> and </SNT> in aleatory
positions among them. The majority baseline re-
turns the most frequent sentence intervals in the
training set based on the input predicates. In case
of an unseen set, the model looks for sentence in-
tervals in subsets of the input.

Lexicalization Algorithm 1 depicts our baseline
approach for Lexicalization. As in Text Structur-
ing, given a set of input triples structured in sen-
tences, the random and majority models return a
random and the most frequent template that de-
scribes the input predicates, respectively (line 6).
If the set of predicates is unseen, the model returns
a template that describes a subset of the input.

Algorithm 1 Lexicalization Pseudocode
Require: struct, model
1: start, end← 0, |struct|
2: template← ∅
3: while start < |struct| do
4: snts← struct[start,end)
5: if snts ∈ model then
6: template← template ∪ model[snts]
7: start← end
8: end← |struct|
9: else

10: end← end −1
11: if start = end then
12: start← start + 1
13: end← |struct|
14: end if
15: end if
16: end while
17: return template

Referring Expression Generation We used
OnlyNames, a baseline introduced in Castro Fer-
reira et al. (2018a), in contrast to NeuralREG.
Given an entity to be referred to, this model re-
turns the entity Wikipedia identifier with under-

All Seen Unseen

Discourse Ordering

Random 0.31 0.29 0.35
Majority 0.48 0.51 0.44
GRU 0.35 0.56 0.10
Transformer 0.34 0.56 0.09

Text Structuring

Random 0.29 0.29 0.30
Majority 0.27 0.45 0.06
GRU 0.39 0.63 0.13
Transformer 0.36 0.59 0.12

Lexicalization

Random 39.49 40.46 33.79
Majority 44.82 45.65 39.43
GRU 37.43 49.26 23.63
Transformer 38.12 48.14 24.15

Referring Expression Generation

OnlyNames 0.51 0.53 0.50
NeuralREG 0.39 0.70 0.07

Table 1: Accuracy of Discourse Ordering, Text Struc-
turing and Referring Expression models, as well as
BLEU score of Lexicalization approaches.

scores replaced by spaces (Massimo Drago →
Massimo Drago).

6.4 Results

Table 1 shows the results for our models for each
of the 4 evaluated pipeline steps. In general, the
results show a clear pattern in all of these steps:
both neural models (GRU and Transformer) intro-
duced higher results on domains seen during train-
ing, but their performance drops substantially on
unseen domains in comparison with the baselines
(Random and Majority). The only exception is
found in Text Structuring, where the neural mod-
els outperforms the Majority baseline on unseen
domains, but are still worse than the Random base-
line. Between both neural models, recurrent net-
works seem to have an advantage over the Trans-
former in Discourse Ordering and Text Structur-
ing, whereas the latter approach performs better
than the former one in Lexicalization.

7 Experiment 2: Pipeline vs. End-to-End

In this experiment, we contrast our pipeline with
our end-to-end implementation and state-of-the-
art models for RDF-to-text. The models were eval-
uated in automatic and human evaluations, fol-
lowed by a qualitative analysis.



557

7.1 Approaches
Pipeline We evaluated 4 implementations of our
pipeline architecture, where the output of the pre-
vious step is fed into the next one. We call
these implementations Random, Majority, GRU
and Transformer, where each one has its steps
solved by one of the proposed baselines or deep
learning implementations. In Random and Major-
ity, the referring expressions were generated by the
OnlyNames baseline, whereas for GRU and Trans-
former, NeuralREG was used for the seen entities,
OnlyNames for the unseen ones and special rules
to realize dates and numbers.

End-to-End We aimed to convert a set of RDF-
triples into text using a GRU and a Transformer
implementation without explicit intermediate rep-
resentations in-between.

7.2 Models for Comparison
To ground this study with related work, we com-
pared the performance of the proposed approaches
with 4 state-of-the-art RDF-to-text models.

Melbourne is the approach which obtained the
highest performance in the automatic evaluation
of the WebNLG Challenge. The approach consists
of a neural encoder-decoder approach, which en-
codes a linearized triple set, with predicates split
on camel case (e.g. floorArea → floor
area) and entities represented by general (e.g.,
ENTITY-1) and named entity recognition (e.g.,
PERSON) tags, into a template where references
are also represented with general tags. The refer-
ring expressions are later generated in the template
simply by replacing these general tags with an ap-
proach similar to OnlyNames.

UPF-FORGe obtained the highest ratings in the
human evaluation of the WebNLG challenge, hav-
ing a performance similar to texts produced by
humans. It also follows a pipeline architecture,
which maps predicate-argument structures onto
sentences by applying a series of rule-based graph-
transducers (Mille et al., 2019).

Marcheggiani and Perez (2018) proposed a
graph convolutional network that directly encodes
the input triple set in contrast with previous model
that first linearize the input to then decode it into
text.

Moryossef et al. (2019) proposed an approach
which converts an RDF triple set into text in two

steps: text planning, a non-neural method where
the input will be ordered and structured, followed
by a neural realization step, where the ordered and
structured input is converted into text.

7.3 Evaluation

Automatic Evaluation We evaluated the textual
outputs of each system using the BLEU (Papineni
et al., 2002) and METEOR (Lavie and Agarwal,
2007) metrics. The evaluation was done on the
entire test data, as well as only in their seen and
unseen domains.

Human Evaluation We conducted a human
evaluation, selecting the same 223 samples used
in the evaluation of the WebNLG challenge (Gar-
dent et al., 2017b). For each sample, we used the
original texts and the ones generated by our 6 ap-
proaches and by the Melbourne and UPF-FORGe
reference systems, totaling 2,007 trials. Each trial
displayed the triple set and the respective text. The
goal of the participants was to rate the trials based
on the fluency (i.e., does the text flow in a natural,
easy to read manner?) and semantics (i.e., does
the text clearly express the data?) of the text in a
1-7 Likert scale.

We recruited 35 raters from Mechanical Turk to
participate in the experiment. We first familiarized
them with the set-up of the experiment, depicting
a trial example in the introduction page accompa-
nied by an explanation. Then each participant had
to rate 60 trials, randomly chosen by the system,
making sure that each trial was rated at least once.5

Qualitative Analysis To have a better under-
standing of the positive and negative aspects of
each model, we also performed a qualitative anal-
ysis, where the second and third authors of this
study analyzed the original texts and the ones gen-
erated by the previous 8 models for 75 trials ex-
tracted from the human evaluation sample for each
combination between size and domain of the cor-
pus. The trials were displayed in a similar way
to the human evaluation, where the annotators did
not know which model produced the text. The
only difference was the additional display of the
predicted structure by the pipeline approaches (a

5The raters had an average age of 32.29 and 40% were
female. 17 participants indicated they were fluent in English,
while 18 were native. The experiment, which obtained ethical
clearance from the university, took around 20-30 minutes to
be completed and each rater received a pay in U.S. dollars for
participation.



558

All Seen Unseen All Seen Unseen

BLEU METEOR

Random 41.68 41.72 41.51 0.20 0.27 -
Majority 43.82 44.79 41.13 0.33 0.41 0.22
GRU 50.55 55.75 38.55 0.33 0.42 0.22
Transformer 51.68 56.35 38.92 0.32 0.41 0.21
E2E GRU 33.49 57.20 6.25 0.25 0.41 0.09
E2E Transformer 31.88 50.79 5.88 0.25 0.39 0.09
Melbourne 45.13 54.52 33.27 0.37 0.41 0.33
UPF-FORGe 38.65 40.88 35.70 0.39 0.40 0.37
(Marcheggiani and Perez, 2018) - 55.90 - - 0.39 -
(Moryossef et al., 2019) 47.40 - - 0.39 - -

Fluency Semantic

Random 4.55E 4.79D 4.07D 4.44D 4.73D 3.86C

Majority 5.00CD 5.25CD 4.49CD 5.02BC 5.41BC 4.25BC

GRU 5.31B 5.51AB 4.91BC 5.21BC 5.48AB 4.67B

Transformer 5.03BC 5.53AB 4.05D 4.87C 5.49AB 3.64C

E2E GRU 4.73DE 5.40BC 3.45E 4.47D 5.21CD 3.03D

E2E Transformer 5.02BC 5.38BC 4.32CD 4.70CD 5.15BCD 3.81C

Melbourne 5.04CD 5.23BC 4.65CD 4.94C 5.33BC 4.15C

UPF-FORGe 5.46B 5.43BC 5.51AB 5.31B 5.35BC 5.24A

Original 5.76A 5.82A 5.63A 5.74A 5.80A 5.63A

Table 2: (1) BLEU and METEOR scores of the models in the automatic evaluation, and (2) Fluency and Semantic
obtained in the human evaluation. In the first part, best results are boldfaced and second best ones are underlined.
In the second part, ranking was determined by pair-wise Mann-Whitney statistical tests with p < 0.05.

Semantic

Ord. Struct. Txt. Ovr. Keep.

Random 1.00 1.00 0.43 0.05 0.41
Majority 1.00 1.00 0.75 0.01 0.69
GRU 0.77 0.73 0.67 0.01 0.81
Transformer 0.75 0.69 0.68 0.08 0.80
E2E GRU - - 0.47 0.41 -
E2E Trans. - - 0.39 0.53 -
Melbourne - - 0.73 0.19 -
UPF-FORGe - - 0.91 0.00 -
Original - - 0.99 0.12 -

Grammaticality

Verb Det. Reference

Random 0.95 0.91 0.89
Majority 1.00 1.00 0.99
GRU 1.00 0.99 0.80
Transformer 0.95 1.00 0.93
E2E GRU 0.97 1.00 0.91
E2E Trans. 0.95 0.97 0.79
Melbourne 0.96 0.87 0.77
UPF-FORGe 1.00 1.00 1.00
Original 0.95 0.95 0.92

Table 3: Qualitative analysis. The first part shows the
percentage of trials that keeps the input predicates over
Discourse Ordering (Ord.), Text Structuring (Struct.)
and in the final text (Txt.). It also shows the ratio of text
trials with more predicates than in the input (Ovr.) and
the pipeline texts which keep the decisions of previous
steps (Keep.). The second part shows the number of
trials without verb, determiner and reference mistakes.

fake structure was displayed for the other models).
Both annotators analyzed grammaticality aspects,
like whether the texts had mistakes involving the
determiners, verbs and references, and semantic
ones, like whether the text followed the predicted
order and structure, and whether it verbalizes less
or more information than the input triples.6

7.4 Results

Table 2 depicts the results of automatic and human
evaluations, whereas Table 3 shows the results of
the qualitative analysis.

Automatic Evaluation In terms of BLEU, our
neural pipeline models (GRU and Transformer)
outperformed all the reference approaches in all
domains, whereas our end-to-end GRU and Ran-
dom pipeline obtained the best results on seen and
unseen domains, respectively.

Regarding METEOR, which includes syn-
onymy matching to score the inputs, reference
methods introduced the best scores in all domains.
In seen and unseen domains, our neural GRU
pipeline and reference approach UPF-FORGe ob-
tained the best results, respectively.

6Inter-annotator agreement for the evaluated aspects
ranged from 0.26 (Reference) to 0.93 (Input Triples), with
an average Krippendorff α of 0.67.



559

Human Evaluation In all domains, neural GRU
pipeline and UPF-FORGe were rated the high-
est in fluency by the participants of the evalua-
tion. In seen ones, both our neural pipeline ap-
proaches (GRU and Transformer) were rated the
best, whereas UPF-FORGe was considered the
most fluent approach in unseen domains.

UPF-FORGe was also rated the most seman-
tic approach in all domains, followed by neural
GRU and Majority pipeline approaches. For seen
domains, similar to the fluency ratings, both our
neural pipeline approaches were rated the highest,
whereas UPF-FORGe was considered the most se-
mantic approach in unseen domains.

Qualitative Analysis In general, UPF-FORGe
emerges as the system which follows the input the
best: 91% of the evaluated trials verbalized the in-
put triples. Moreover, the annotators did not find
any grammatical mistakes in the output of this ap-
proach.

When focusing on the neural pipeline ap-
proaches, we found that in all the steps up to Text
Structuring, the recurrent networks retained more
information than the Transformer. However, 68%
of the Transformer’s text trials contained all the
input triples, against 67% of the GRU’s trials. As
in Experiment 1, we see that recurrent networks
as GRUs are better in ordering and structuring the
discourse, but is outperformed by the Transformer
in the Lexicalization step. In terms of fluency, we
did not see a substantial difference between both
kinds of approaches.

Regarding our end-to-end trials, different from
the pipeline ones, less than a half verbalized all
the input triples. Moreover, the end-to-end outputs
also constantly contained more information than
there were in the non-linguistic input.

8 Discussion

This study introduced a systematic comparison be-
tween pipeline and end-to-end architectures for
data-to-text generation, exploring the role of deep
neural networks in the process. In this section we
answer the two introduced research questions and
additional topics based on our findings.

How well do deep learning methods perform as
individual modules in a data-to-text pipeline?
In comparison with Random and Majority base-
lines, we observed that our deep learning imple-
mentations registered a higher performance in the

pipeline steps on domains seen during training, but
their performance dropped considerably on unseen
domains, being lower than the baselines.

In the comparison between our GRU and Trans-
former, the former seems to be better at ordering
and structuring the non-linguistic input, whereas
the latter performs better in verbalizing an or-
dered and structured set of triples. The advantage
of GRUs over the Transformer in Discourse Or-
dering and Text Structuring may be its capacity
to implicitly take order information into account.
On the other hand, the Transformer could have
had difficulties caused by the task’s design, where
triples and sentences were segmented by tags (e.g.
<TRIPLE> and <SNT>), rather than positional
embeddings, which suits this model better. In sum,
more research needs to be done to set this point.

How well does a neural pipeline architecture
perform compared to a neural end-to-end one?
Our neural pipeline approaches were superior to
the end-to-end ones in most tested circumstances:
the former generates more fluent texts which bet-
ter describes data on all domains of the corpus.
The difference is most noticeable for unseen do-
mains, where the performance of end-to-end ap-
proaches drops considerably. This shows that end-
to-end approaches do not generalize as well as
the pipeline ones. In the qualitative analysis, we
also found that end-to-end generated texts have
the problem of describing non-linguistic represen-
tations which are not present in the input, also
known as Hallucination (Rohrbach et al., 2018).

The example in Figure 2 shows the advantage
of our pipeline approaches in comparison with the
end-to-end ones. It depicts the texts produced by
the proposed approaches for an unseen set dur-
ing training of 4 triples, where 2 out of the 4
predicates are present in the WebNLG training set
(e.g., birthPlace and occupation). In this con-
text, the pipeline approaches managed to gener-
ate a semantic text based on the two seen predi-
cates, whereas the end-to-end approaches halluci-
nated texts which have no semantic relation with
the non-linguistic input.

Related Work We compared the proposed ap-
proaches with 4 state-of-the-art RDF-to-text sys-
tems. Except for Marcheggiani and Perez (2018),
all the others are not end-to-end approaches, al-
ready directing the field to pipeline architectures.
UPF-FORGe is a proper pipeline system with sev-



560

Ace Wilder background “solo singer”
Ace Wilder birthPlace Sweden
Ace Wilder birthYear 1982
Ace Wilder occupation Songwriter

↓

GRU Ace Wilder, born in Sweden, performs as Songwriter.

Transformer Ace Wilder (born in Sweden) was Songwriter.

E2E GRU
The test pilot who was born in Willington, who was born in New York, was born
in New York and is competing in the competing in the U.S.A. The construction
of the city is produced in Mandesh.

E2E Trans. Test pilot Elliot See was born in Dallas and died in St. Louis.

Figure 2: Example of a set of triples from an unseen domain during training (top) and the corresponding texts
produced by our pipeline (e.g., GRU and Transformer) and end-to-end approaches (e.g., E2E GRU and E2E Trans.)
(bottom). In the top set of triples, predicates seen during training are highlighted in italic, whereas the unseen ones
are underlined.

eral sequential steps, Melbourne first generates a
delexicalized template to later realize the referring
expressions, and Moryossef et al. (2019) splits
the process up into Planning, where ordering and
structuring are merged, and Realization.

Besides the approach of Marcheggiani and
Perez (2018), the ADAPT system, introduced in
the WebNLG challenge (Gardent et al., 2017b),
is another full end-to-end approach to the task.
It obtained the highest results in the seen part of
the WebNLG corpus (BLEU = 60.59; METEOR
= 0.44). However, the results drastically dropped
on the unseen part of the dataset (BLEU = 10.53;
METEOR = 0.19). Such results correlate with
our findings showing the difficult of end-to-end
approaches to generalize to new domains.

By obtaining the best results in almost all the
evaluated metrics, UPF-FORGe emerges as the
best reference system, showing again the advan-
tage of generating text from non-linguistic data
in several explicit intermediate representations.
However, it is important to observe that the advan-
tage of UPF-FORGe over our pipeline approaches
is the fact that it was designed taking the seen and
unseen domains of the corpus into account. So in
practice, there was no “unseen” domains for UPF-
FORGe. In a fair comparison between this refer-
ence system with our neural pipeline approaches
in only seen domains, we may see that ours are
rated higher in almost all the evaluated metrics.

General Applicability Besides our findings, the
results for other benchmarks, such as Elder et al.

(2019), suggest that pipeline approaches can work
well in the context of neural data-to-text gener-
ation. Concerning our pipeline approach specif-
ically, although it was designed to convert RDF
triples to text, we assume it can be adapted to
other domains (and languages) where communica-
tive goals can be linearized and split in units, as in
the E2E dataset (Novikova et al., 2017). In future
work, we plan to study this in more detail.

Conclusion In a systematic comparison, we
show that adding supervision during the data-to-
text process leads to more fluent text that better
describes the non-linguistic input data than full
end-to-end approaches, confirming the trends in
related work in favor of pipeline architectures.

Acknowledgments

This work is part of the research program “Dis-
cussion Thread Summarization for Mobile De-
vices” (DISCOSUMO) which is financed by the
Netherlands Organization for Scientific Research
(NWO). We also acknowledge the three reviewers
for their insightful comments.

References
Luca Anselma and Alessandro Mazzei. 2018. Design-

ing and testing the messages produced by a virtual
dietitian. In Proceedings of the 11th International
Conference on Natural Language Generation, pages
244–253, Tilburg University, The Netherlands. As-
sociation for Computational Linguistics.

https://www.aclweb.org/anthology/W18-6531
https://www.aclweb.org/anthology/W18-6531
https://www.aclweb.org/anthology/W18-6531


561

Anja Belz, Mike White, Dominic Espinosa, Eric Kow,
Deirdre Hogan, and Amanda Stent. 2011. The first
surface realisation shared task: Overview and eval-
uation results. In Proceedings of the 13th European
Workshop on Natural Language Generation, pages
217–226, Nancy, France. Association for Computa-
tional Linguistics.

Daniel Braun, Ehud Reiter, and Advaith Siddharthan.
2018. Saferdrive: An NLG-based behaviour change
support system for drivers. Natural Language Engi-
neering, 24(4):551–588.

Thiago Castro Ferreira, Diego Moussallem, Ákos
Kádár, Sander Wubben, and Emiel Krahmer. 2018a.
NeuralREG: An end-to-end approach to referring
expression generation. In Proceedings of the 56th
Annual Meeting of the Association for Computa-
tional Linguistics (Volume 1: Long Papers), pages
1959–1969. Association for Computational Linguis-
tics.

Thiago Castro Ferreira, Diego Moussallem, Emiel
Krahmer, and Sander Wubben. 2018b. Enriching
the WebNLG corpus. In Proceedings of the 11th In-
ternational Conference on Natural Language Gen-
eration, pages 171–176. Association for Computa-
tional Linguistics.

Kyunghyun Cho, Bart van Merrienboer, Dzmitry Bah-
danau, and Yoshua Bengio. 2014. On the Prop-
erties of Neural Machine Translation: Encoder–
Decoder Approaches. In Proceedings of SSST-8,
Eighth Workshop on Syntax, Semantics and Struc-
ture in Statistical Translation, pages 103–111, Doha,
Qatar.

Ondřej Dušek and Filip Jurcicek. 2016. Sequence-to-
sequence generation for spoken dialogue via deep
syntax trees and strings. In Proceedings of the 54th
Annual Meeting of the Association for Computa-
tional Linguistics (Volume 2: Short Papers), pages
45–51, Berlin, Germany. Association for Computa-
tional Linguistics.

Henry Elder, Jennifer Foster, James Barry, and Alexan-
der O’Connor. 2019. Designing a symbolic inter-
mediate representation for neural surface realization.
In Proceedings of the Workshop on Methods for Op-
timizing and Evaluating Neural Language Genera-
tion, pages 65–73, Minneapolis, Minnesota. Associ-
ation for Computational Linguistics.

Claire Gardent, Anastasia Shimorina, Shashi Narayan,
and Laura Perez-Beltrachini. 2017a. Creating train-
ing corpora for NLG micro-planners. In Proceed-
ings of the 55th Annual Meeting of the Association
for Computational Linguistics (Volume 1: Long Pa-
pers), ACL’17, pages 179–188, Vancouver, Canada.
Association for Computational Linguistics.

Claire Gardent, Anastasia Shimorina, Shashi Narayan,
and Laura Perez-Beltrachini. 2017b. The WebNLG
challenge: Generating text from RDF data. In
Proceedings of the 10th International Conference

on Natural Language Generation, INLG’17, pages
124–133, Santiago de Compostela, Spain. Associa-
tion for Computational Linguistics.

Albert Gatt and Emiel Krahmer. 2018. Survey of the
state of the art in natural language generation: Core
tasks, applications and evaluation. Journal of Artifi-
cial Intelligence Research, 61:65–170.

Sebastian Gehrmann, Falcon Dai, Henry Elder, and
Alexander Rush. 2018. End-to-end content and plan
selection for data-to-text generation. In Proceed-
ings of the 11th International Conference on Natural
Language Generation, pages 46–56, Tilburg Uni-
versity, The Netherlands. Association for Computa-
tional Linguistics.

Sepp Hochreiter and Jürgen Schmidhuber. 1997.
Long short-term memory. Neural computation,
9(8):1735–1780.

Emiel Krahmer and Kees van Deemter. 2012. Compu-
tational generation of referring expressions: A sur-
vey. Computational Linguistics, 38(1):173–218.

Alon Lavie and Abhaya Agarwal. 2007. Meteor: An
automatic metric for MT evaluation with high levels
of correlation with human judgments. In Proceed-
ings of the Second Workshop on Statistical Machine
Translation, StatMT’07, pages 228–231, Prague,
Czech Republic.

Rémi Lebret, David Grangier, and Michael Auli. 2016.
Neural text generation from structured data with ap-
plication to the biography domain. In Proceedings
of the 2016 Conference on Empirical Methods in
Natural Language Processing, EMNLP’16, pages
1203–1213, Austin, Texas. Association for Compu-
tational Linguistics.

Chris van der Lee, Emiel Krahmer, and Sander
Wubben. 2017. PASS: A Dutch data-to-text system
for soccer, targeted towards specific audiences. In
Proceedings of the 10th International Conference on
Natural Language Generation, INLG’2017, pages
95–104, Santiago de Compostela, Spain. Associa-
tion for Computational Linguistics.

Leo Leppänen, Myriam Munezero, Mark Granroth-
Wilding, and Hannu Toivonen. 2017. Data-driven
news generation for automated journalism. In Pro-
ceedings of the 10th International Conference on
Natural Language Generation, pages 188–197. As-
sociation for Computational Linguistics.

Diego Marcheggiani and Laura Perez. 2018. Deep
graph convolutional encoders for structured data to
text generation. In Proceedings of the 11th Interna-
tional Conference on Natural Language Generation,
pages 1–9. Association for Computational Linguis-
tics.

Hongyuan Mei, Mohit Bansal, and Matthew R. Walter.
2016. What to talk about and how? selective gen-
eration using LSTMs with coarse-to-fine alignment.
In Proceedings of the 2016 Conference of the North

http://www.aclweb.org/anthology/W11-2832
http://www.aclweb.org/anthology/W11-2832
http://www.aclweb.org/anthology/W11-2832
http://aclweb.org/anthology/P18-1182
http://aclweb.org/anthology/P18-1182
http://aclweb.org/anthology/W18-6521
http://aclweb.org/anthology/W18-6521
http://www.aclweb.org/anthology/W14-4012
http://www.aclweb.org/anthology/W14-4012
http://www.aclweb.org/anthology/W14-4012
https://doi.org/10.18653/v1/P16-2008
https://doi.org/10.18653/v1/P16-2008
https://doi.org/10.18653/v1/P16-2008
https://doi.org/10.18653/v1/W19-2308
https://doi.org/10.18653/v1/W19-2308
https://doi.org/10.18653/v1/P17-1017
https://doi.org/10.18653/v1/P17-1017
http://aclweb.org/anthology/W17-3518
http://aclweb.org/anthology/W17-3518
https://www.aclweb.org/anthology/W18-6505
https://www.aclweb.org/anthology/W18-6505
http://dl.acm.org/citation.cfm?id=1626355.1626389
http://dl.acm.org/citation.cfm?id=1626355.1626389
http://dl.acm.org/citation.cfm?id=1626355.1626389
https://doi.org/10.18653/v1/D16-1128
https://doi.org/10.18653/v1/D16-1128
http://aclweb.org/anthology/W17-3513
http://aclweb.org/anthology/W17-3513
https://doi.org/10.18653/v1/W17-3528
https://doi.org/10.18653/v1/W17-3528
http://aclweb.org/anthology/W18-6501
http://aclweb.org/anthology/W18-6501
http://aclweb.org/anthology/W18-6501
https://doi.org/10.18653/v1/N16-1086
https://doi.org/10.18653/v1/N16-1086


562

American Chapter of the Association for Computa-
tional Linguistics: Human Language Technologies,
HLT-NAACL’16, pages 720–730, San Diego, Cali-
fornia. Association for Computational Linguistics.

Simon Mille, Anja Belz, Bernd Bohnet, Yvette Gra-
ham, Emily Pitler, and Leo Wanner. 2018. The first
multilingual surface realisation shared task (SR’18):
Overview and evaluation results. In Proceedings of
the First Workshop on Multilingual Surface Realisa-
tion, pages 1–12, Melbourne, Australia. Association
for Computational Linguistics.

Simon Mille, Stamatia Dasiopoulou, and Leo Wanner.
2019. A portable grammar-based NLG system for
verbalization of structured data. In Proceedings of
the 34th ACM/SIGAPP Symposium on Applied Com-
puting, pages 1054–1056. ACM.

Amit Moryossef, Yoav Goldberg, and Ido Dagan. 2019.
Step-by-step: Separating planning from realization
in neural data-to-text generation. In Proceedings of
the 2019 Conference of the North American Chap-
ter of the Association for Computational Linguistics:
Human Language Technologies, Volume 1 (Long
and Short Papers), pages 2267–2277, Minneapolis,
Minnesota. Association for Computational Linguis-
tics.

Jekaterina Novikova, Ondrej Dusek, and Verena Rieser.
2017. The E2E dataset: New challenges for end-to-
end generation. In Proceedings of the 18th Annual
SIGdial Meeting on Discourse and Dialogue, pages
201–206, Saarbrücken, Germany.

Kishore Papineni, Salim Roukos, Todd Ward, and
Wei-Jing Zhu. 2002. Bleu: a method for auto-
matic evaluation of machine translation. In Pro-
ceedings of 40th Annual Meeting of the Association
for Computational Linguistics, ACL’02, pages 311–
318, Philadelphia, Pennsylvania, USA. Association
for Computational Linguistics.

Franois Portet, Ehud Reiter, Albert Gatt, Jim Hunter,
Somayajulu Sripada, Yvonne Freer, and Cindy
Sykes. 2009. Automatic generation of textual sum-
maries from neonatal intensive care data. Artificial
Intelligence, 173(78):789 – 816.

Ehud Reiter and Robert Dale. 2000. Building natural
language generation systems. Cambridge Univer-
sity Press, New York, NY, USA.

Anna Rohrbach, Lisa Anne Hendricks, Kaylee Burns,
Trevor Darrell, and Kate Saenko. 2018. Object hal-
lucination in image captioning. In Proceedings of
the 2018 Conference on Empirical Methods in Nat-
ural Language Processing, pages 4035–4045, Brus-
sels, Belgium. Association for Computational Lin-
guistics.

Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob
Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz
Kaiser, and Illia Polosukhin. 2017. Attention is all
you need. In Advances in neural information pro-
cessing systems, pages 5998–6008.

Tsung-Hsien Wen, Milica Gasic, Nikola Mrkšić, Pei-
Hao Su, David Vandyke, and Steve Young. 2015.
Semantically conditioned LSTM-based natural lan-
guage generation for spoken dialogue systems.
In Proceedings of the 2015 Conference on Em-
pirical Methods in Natural Language Processing,
EMNLP’15, pages 1711–1721, Lisbon, Portugal.
Association for Computational Linguistics.

https://doi.org/10.18653/v1/W18-3601
https://doi.org/10.18653/v1/W18-3601
https://doi.org/10.18653/v1/W18-3601
https://doi.org/10.18653/v1/N19-1236
https://doi.org/10.18653/v1/N19-1236
https://aclanthology.info/papers/W17-5525/w17-5525
https://aclanthology.info/papers/W17-5525/w17-5525
https://doi.org/10.3115/1073083.1073135
https://doi.org/10.3115/1073083.1073135
https://www.aclweb.org/anthology/D18-1437
https://www.aclweb.org/anthology/D18-1437
http://aclweb.org/anthology/D15-1199
http://aclweb.org/anthology/D15-1199

