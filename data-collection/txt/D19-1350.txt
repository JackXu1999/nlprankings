



















































Neural Topic Model with Reinforcement Learning


Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing
and the 9th International Joint Conference on Natural Language Processing, pages 3478–3483,
Hong Kong, China, November 3–7, 2019. c©2019 Association for Computational Linguistics

3478

Neural Topic Model with Reinforcement Learning

Lin Gui1,§, Jia Leng2,§, Gabriele Pergola1, Yu Zhou1, Ruifeng Xu2,3,4, Yulan He1,†
1Department of Computer Science, University of Warwick, UK

2Harbin Institute of Technology (Shenzhen), China
3Peng Cheng Laboratory, Shenzhen, China

4Joint Lab of Harbin Institute of Technology and RICOH
Lin.gui@warwick.ac.uk, lengjia@stu.hit.edu.cn

gabriele.pergola@warwick.ac.uk, Yu.Zhou.1@warwick.ac.uk
xuruifeng@hit.edu.cn, yulan.he@warwick.ac.uk

Abstract

In recent years, advances in neural variational
inference have achieved many successes in
text processing. Examples include neural topic
models which are typically built upon varia-
tional autoencoder (VAE) with an objective of
minimising the error of reconstructing original
documents based on the learned latent topic
vectors. However, minimising reconstruction
errors does not necessarily lead to high quality
topics. In this paper, we borrow the idea of re-
inforcement learning and incorporate topic co-
herence measures as reward signals to guide
the learning of a VAE-based topic model. Fur-
thermore, our proposed model is able to au-
tomatically separating background words dy-
namically from topic words, thus eliminating
the pre-processing step of filtering infrequent
and/or top frequent words, typically required
for learning traditional topic models. Exper-
imental results on the 20 Newsgroups and the
NIPS datasets show superior performance both
on perplexity and topic coherence measure
compared to state-of-the-art neural topic mod-
els.

1 Introduction

Probabilistic topic models have been used widely
in nature language processing (Li et al., 2016;
Zeng et al., 2018). The fundamental principle is
that words are assumed to be generated from la-
tent topics which can be inferred from data based
on word co-occurrence patterns (Neal, 1993; An-
drieu et al., 2003). In recent years, Variational
Autoencoder (VAE) has been proved more effec-
tive and efficient to approximating deep, complex
and underestimated variance in integrals (Kingma
and Welling, 2013; He et al., 2017). However, the
VAE-based topic models focus on the construc-
tion of deep neural networks to approximate the
§The two authors contributed equally to this work.
†Corresponding author.

intractable distribution between observed words
and latent topics based on log-likelihood and the
learning objective is to minimise the error of re-
constructing the original documents based on the
learned latent topic vectors rather than improving
the quality of learned topics, for example, mea-
sured by coherence scores (Kingma and Welling,
2013; Sønderby et al., 2016; Miao et al., 2016;
Card et al., 2017; Srivastava and Sutton, 2017;
Bouchacourt et al., 2018). The lack of considera-
tion of topic coherence measures during the learn-
ing process of VAE-based topic models makes it
difficult to control the quality of the generated top-
ics. Intuitively, one solution is to jointly consider
coherence scores in the learning objective. How-
ever, this is not feasible since coherence score is an
unsupervised measure of topics based on a large-
scale knowledge source, there is no ground truth
“best topics”.

Another limitation of existing approaches is that
they typically require a pre-processing step to fil-
ter infrequent and/or top frequent words in order to
reduce the vocabulary size and achieve better topic
extraction results. Word filtering is often done
heuristically. Although there have been attempts
to automatically distinguishing background words
and topic words, existing approaches either re-
quire a switch variable defined at each word posi-
tion to indicate whether the word is a background
word, which makes the models cumbersome, or
model each latent topic as the deviation in log-
frequency from a constant background distribution
(Eisenstein et al., 2011; Smith et al., 2018).

In this paper, we propose a new framework to
use reinforcement learning (Pan et al., 2018; Qin
et al., 2018; Yin et al., 2018) to incorporate the
topic coherence measures into the learning of a
neural topic model and filter background words
dynamically. More concretely, given an input doc-
ument, its constituent words will first be sampled



3479

Figure 1: Neural topic model with reinforcement learning.

by a weight vector which assigns higher weights
to words with higher coherence scores and have
more concentrated topic distributions. The sam-
pled words will then be fed into a VAE-based neu-
ral topic model to reconstruct the original docu-
ment. A reward function is deployed to take into
account both topic coherence scores and the de-
gree of word overlapping between topics. The re-
ward signal derived is subsequently used to up-
date the sampling weight vector for each word. In
this way, we do not need to directly add the co-
herence scores into the loss function. Our experi-
mental results show that our proposed framework
outperforms the traditional topic model and exist-
ing neural topic modelling approaches on the 20
Newsgroups (Lang, 1995) and the NIPS data (Tan
et al., 2017) in topic coherence and perplexity.

The rest of the paper is organized as follows.
Section 2 presents our proposed reinforcement
learning framework for topic modelling. Section 3
reports the experimental setup and results. Section
4 concludes the paper and outlines future research
directions.

2 Proposed Method

In this section, we introduce our proposed re-
inforcement learning (RL) framework for topic
modelling. A standard RL framework contains
three components: action, state, and reward. Here,
the action aims to select words with high coher-
ence scores and filter background words. The state
is the distribution of latent topics among words,
which is obtained from a VAE-based topic model.
The reward is a function to measure the quality of
topics based on an external corpus and guide the
weight updating of the next word selecting action.
The overall architecture is illustrated in Figure 1.

We detail our framework in the following.

2.1 Action

For an input document d = {w1, w2, ..., wU},
where each word wi is represented by a one-hot
representation, the action is determined by a prob-
abilistic vector P = {p1, p2, ..., pU} which is
used to filter the less topical-coherent and back-
ground words at each iteration of model learn-
ing. Here, each pi present the sampling probabil-
ity for word wi, and U is the full vocabulary size.
We aim to select V words from the full vocabu-
lary based on P and mask out other words in d,
thus d̃ = {w′1, w′2, ..., w′V } where V ≤ U . The
goal of our method is to assign higher probabili-
ties to words which contribute more to the topic
coherence scores and lower probabilities to those
less topical-coherent words and background words
which occur equally likely across topics.

2.2 State

After word selection, the new document represen-
tation d̃ is fed into a neural topic model to obtain
the state, which is the topic distribution in the topic
model. Here, we deploy the VAE (Miao et al.,
2016; Card et al., 2017) to learn the latent topics,
which consists of two main components, the In-
ference Network and the Generation Network. For
the Inference Network, we use VAE to approxi-
mate the posterior distribution over topics for all
the training instances. In the Generation Network,
the words are generated via Gaussian softmax con-
struction from the topic distribution generated by
the Inference Network. The architecture of the
neural topic model is shown in Figure (1) and we
describe the model in more details below.
Inference Network. Following the idea of VAE



3480

which computes a variational approximation to an
intractable posterior using MLPs, we define two
MLPs, fµθ and fΣθ , which takes as input the word
counts in a document and outputs mean and vari-
ance of a Gaussian distribution, both being vec-
tors in RK , µθ = fµθ(wd), Σθ = diag(fΣθ(wd)).
Here, ‘diag’ converts a column vector to a diago-
nal matrix. For a document d, its variational dis-
tribution is q(θ) ' N (µθ,Σθ). With such a for-
mulation, we can generate samples from q(θ) by
first sampling � ∼ N (0, I2) and then computing
θ̂ = σ(µθ + Σθ

1/2�).
Generation Network. We feed the sampled θ̂
to two MLPs to generate zd. Here, zd is a K-
dimensional latent topic representation of docu-
ment d. The probability of d-th word in n-th doc-
ument wd,n can be parameterised by another net-
work,

p(wd,n|wd, zd) ∝ exp(md +W · zd) (1)

where md is the V -dimensional background log-
frequency word distribution, W ∈ RV×K is a
weight matrix. With the sampled θ̂ , for each docu-
ment d ∈ Nd, we can estimate the Evidence Lower
Bound (ELBO) with a Monte Carlo approximation
using L independent samples:

Lt(wd) ≈
1

L

L∑
l=1

Nd∑
n=1

log p(wd,n|θ̂(l))

−KL(q(zd|wd)||p(zd))

(2)

By minimising the ELBO in Eq. (2), the neu-
ral topic model reconstructs the input document
wd. At the reconstruction layer, the matrix W ∈
RV×K in a single-layer network, which is used to
capture the sampling weights between each word
and the latent topics, is the state which produces
specific topic coherence scores.

2.3 Reward
Intuitively, words with higher topic coherence and
lower degree of overlapping among different top-
ics should be assigned higher reward in the next
iteration of learning. Hence, the reward function
should be composed by two terms for each word:
the average coherence score and topic overlapping
value. The average coherence score is defined as:

COaverage = (W · Cv(W ))� Pw′i∈wd (3)

where matrix W is the distribution of latent top-
ics among words,Cv(W ) is aK-dimension vector

which contains the coherence score for each topic
based on the sampling weight matrix, Pw′i∈wd is
the sampling probability for each word in docu-
ment d (i.e., which action to take as described in
Section 2.1), and � is the element-wise product.
Hence, COaverage is a V -dimension weight vec-
tor to distribute coherence scores to the selected
words based on sampling probabilities in action
and topic distribution in topic modelling.

The Topic Overlapping (TO) is defined as:

TO = sumrow(abs(I −W ·W T )) (4)

where I is a V × V identity matrix. TO ∈ R|v| is
to measure the separation based on mean distribu-
tion. In TO, the high value indicates that the asso-
ciated word appears frequently across topics and
hence could be considered as background words.

Based on the average coherence score and the
topic overlapping value, the reward function is:

Rt = COaverage − α · TO (5)
Qt = β ·Qt−1 + (1− β) ·Rt−1 (6)

where α and β ∈ (0, 1) are trade-off coefficient.
Then, the reward at the current time step, Rt, and
the history rewards encoded by Qt will be used to
update the sampling weight in the action:

Pt = max(Pt−1 + λP · (Rt −Qt), 0 + �) (7)

where Pt is the sampling vector in action, � > 0
is a minimal value, and λP is the learning rate for
P . We choose a ramp function with � to ensure the
sampling probability is positive.

2.4 Training

For pre-processing, we performed stop word re-
moval∗ and use Adam to optimise the parameters
in the neural networks. The learning rate for VAE
and θp are both 0.0001, the mini-batch size is 32, α
is 0.1, β is 0.5, � is 0.01, and the coherence scores
are obtained from Wikipedia. The parameters in
VAE are updated in each mini-batch, and the prob-
abilistic vector P for action selection is updated
every 2,000 mini-batches.

3 Experiments

We evaluate our model on the 20 Newsgroups†

consisting of 18K documents, and NIPS (Tan
∗http://mallet.cs.umass.edu/import-stoplist.php
†http://qwone.com/˜jason/20Newsgroups/

http://qwone.com/~jason/20Newsgroups/


3481

20News NIPS
Methods PPL Cv PPL Cv

#Topics = 30, frequency-based vocab.
LDA 1,213.1 0.503 1,042.7 0.507
NVDM 980.8 0.497 931.6 0.492
NGTM 929.3 0.479 938.9 0.503
Scholar 1,345.9 0.537 1,350.9 0.512

#Topics = 30, RL-based vocab.
LDA 1,451.7 0.522 1,093.1 0.534
NVDM 845.8 0.510 768.7 0.509
NGTM 791.5 0.517 757.2 0.527
Scholar 1,158.4 0.560 1,273.6 0.548
VTMRL 803.7 0.577 730.6 0.568

#Topics = 50, frequency-based vocab.
LDA 1,015.9 0.501 995.5 0.503
NVDM 1,014.0 0.471 927.6 0.506
NGTM 903.5 0.491 908.8 0.498
Scholar 1,514.5 0.521 1,373.2 0.508

#Topics = 50, RL-based vocab.
LDA 1,251.6 0.518 921.4 0.527
NVDM 837.9 0.502 767.0 0.514
NGTM 772.2 0.514 749.7 0.511
Scholar 1,335.9 0.526 1,299.8 0.530
VTMRL 725.2 0.559 712.2 0.566

Table 1: Perplexity and topic coherence results of dif-
ference models. ‘frequency-based vocab.’ denotes that
the vocabulary is constructed by filtering out rare words
while ‘RL-based vocab.’ denotes that the vocabulary is
dynamically generated by our model using RL.

et al., 2017) consisting of 6.6k documents. We use
10% training data as the validation set to fine-tune
the parameters. We compare our results with those
obtained from the following baselines:
LDA: Latent Dirichlet Allocation Model (Blei
et al., 2003).
NVDM: Neural Variational Document model
(Miao et al., 2016).
NGTM: Neural Generative Topic model (Card
et al., 2017).
Scholar: Topic model with metadata (Card et al.,
2018).
VTMRL: Our proposed Variational Topic Model
with Reinforcement Learning.

For all the baseline models, we follow the com-
mon pre-processing step in existing approaches
by performing stop word removal, and selecting
the most frequent 2,000 words as the vocabulary.
Since our proposed framework dynamically select
words at each iteration of learning, we do not need

to pre-set the vocabulary prior to model learn-
ing. Instead, we only activate 2,000 words at each
mini-batch of training based on the word sampling
probabilities. As our model dynamically select
words during the training process, in order to en-
sure fair comparison with other models, we also
report the results of training baselines using the
vocabulary dynamically generated by our model.

In our experiments, the models are evaluated
based on the perplexity (PPL, lower is better) and
topic coherence measure (Cv) based on external
corpus (Röder et al., 2015) (higher is better). The
results with 30 and 50 topics are shown in Table 1.

LDA is a conventional topic model, while all
the other models are neural topic models. It
can be observed from Table 1 that NVDM and
NGTM achieve better perplexities compared to
LDA. However, in terms of topic coherence mea-
sure, NVDM and NGTM perform slightly worse
than LDA. A similar observation has been re-
ported in (Card et al., 2017). Scholar achieves
better coherence compared to other neural mod-
els. Nevertheless, after using reinforcement learn-
ing based on the topic coherence scores in our pro-
posed model, VTMRL outperforms all the other
models on the topic coherence measure by a large
margin. RL could activate words which are se-
mantically related to topics regardless of their oc-
currence frequency. The inclusion of some rare
words would impact the models’ predictive prob-
abilities. As such, we observe worse perplexity
results for models trained with RL-based vocabu-
lary compared to frequency-based vocabulary in
20 Newsgroups, though the converse is true for
NIPS. Nevertheless, the coherence scores improve
for all the models with RL-based vocabulary.

As incorporating RL could increase the compu-
tational complexity of VTMRL, we report in Ta-
ble 2 the total number of parameters and average
training time per epoch when the vocabulary size
is 2,000 and the number of topics is 50.

Strictly speaking, the number of parameters in
LDA is not directly comparable with neural mod-
els. Neural models have similar parameter size.
With the incorporation of RL, VTMRL only in-
creased the parameter size by 1.4%. Due to the ef-
ficiency of GPU, the running costs of neural mod-
els are better than that of LDA. Although our pro-
posed VTMRL used full vocabulary, the active
words in each epoch are limited. Hence, there is
no significant increase in terms of the running cost.



3482

Model #parameters Training time(s)
LDA 1.00× 105 660.89
NVDM 1.38× 106 62.16
NGTM 1.39× 106 72.12
Scholar 1.15× 106 3.41
VTMRL 1.41× 106 5.78

Table 2: Number of parameters for each model and the
average training time per epoch with vocabulary size
2,000 and topic number 50.

Topic Topic Words
Without RL

1 university <NUM> subject host idea
2 organization article writes surrender lines
3 people organization posting article lines

With RL
1 mouse x11r5 keyboard serial remote
2 chip design products build system
3 drives friend sports espn michigan

Table 3: Example topic words with/without RL by
VTMRL (#Topics = 50) in 20 Newsgroup.

We show in Table 3 example topics
with/without RL by VTMRL in 20 News-
groups. The RL method seems producing more
interpretable topics. Also, due to the reward-
based words sampling in RL, words with low
occurrence frequency would still have a chance to
be promoted in specific topics, such as ‘x11r5’,
which is a serial number of the Windows system.

We next compare the topic coherence changes
during model training. We observe that for
VTMRL the coherence value increases at the be-
ginning of the training and remains relatively sta-
ble in subsequent training iterations. As a con-
trast, the coherence value of our model without RL
is not stable, and decreases rapidly after 10 train-
ing epochs. This is not surprising since the model
without RL did not consider topic coherence in its
learning process.

We also evaluate the effectiveness of using the
learned topics as features to train text classifiers
on the 20 Newsgroups data. The results are ob-
tained by using logistic regression as the classifier
trained from the topics generated by various afore-
mentioned models. We also report the results by
training logistic regression from the combination
of word features (tf-idf) and topic features (#topic
= 30). In addition, we include the results using
neural models such as CNN and RNN in Table 4.

Using only topics extracted from topic models
as features to train logistic regression, our pro-

Model Acc Model Acc
LDA 0.412 tf-idf + LDA 0.822
NGTM 0.375 tf-idf + NGTM 0.825
NVDM 0.303 tf-idf + NVDM 0.824
VTMRL 0.478 tf-idf + VTMRL 0.830
CNN 0.515 RNN 0.509

Table 4: Text classification accuracy of different mod-
els on the 20 Newsgroups data.

Figure 2: Topic coherence changes with/without RL
(#Topics = 50).

posed model VTMRL beats other baselines. How-
ever, the topic features have only 30 dimensions
so the performance is limited in comparison with
CNN and RNN. When we combine the topic fea-
tures with tf-idf based word features, the perfor-
mance is boosted significantly compared to CNN
and RNN and the best result is obtained by us-
ing the logistic regression model trained from the
combined word features with topics generated by
our proposed VTMRL.

4 Conclusion

In this paper, we have proposed a new reinforce-
ment learning (RL) framework for neural topic
modelling, where words are activated dynamically
by RL according to topic coherence scores and
topic overlapping values. The experiments on the
20 Newsgroups and NIPS datasets show encourag-
ing results both on perplexity and topic coherence
measures in comparison with existing neural topic
models. In future work, we will explore extending
our model for temporal topic modelling.

Acknowledgment

This work was supported by the Innovate UK
(grant no. 103652), the EU-H2020 (grant no.
794196), the National Natural Science Foundation
of China U1636103, 61876053.



3483

References
Christophe Andrieu, Nando de Freitas, Arnaud Doucet,

and Michael I. Jordan. 2003. An introduction to
MCMC for machine learning. Machine Learning,
50(1-2):5–43.

David M Blei, Andrew Y Ng, and Michael I Jordan.
2003. Latent dirichlet allocation. Journal of ma-
chine Learning research, 3(Jan):993–1022.

Diane Bouchacourt, Ryota Tomioka, and Sebastian
Nowozin. 2018. Multi-level variational autoen-
coder: Learning disentangled representations from
grouped observations. In Proceedings of the Thirty-
Second AAAI Conference on Artificial Intelligence,
New Orleans, Louisiana, USA, February 2-7, 2018.

Dallas Card, Chenhao Tan, and Noah A. Smith. 2017.
A neural framework for generalized topic models.
CoRR, abs/1705.09296.

Dallas Card, Chenhao Tan, and Noah A. Smith. 2018.
Neural models for documents with metadata. In
Proceedings of the 56th Annual Meeting of the As-
sociation for Computational Linguistics, ACL 2018,
Melbourne, Australia, July 15-20, 2018, Volume 1:
Long Papers, pages 2031–2040.

Jacob Eisenstein, Amr Ahmed, and Eric P Xing. 2011.
Sparse additive generative models of text. In Pro-
ceedings of the 28th International conference on
Machine Learning (ICML).

Ruidan He, Wee Sun Lee, Hwee Tou Ng, and Daniel
Dahlmeier. 2017. An unsupervised neural attention
model for aspect extraction. In Proceedings of the
55th Annual Meeting of the Association for Compu-
tational Linguistics (Volume 1: Long Papers), pages
388–397. Association for Computational Linguis-
tics.

Diederik P. Kingma and Max Welling. 2013. Auto-
encoding variational bayes. CoRR, abs/1312.6114.

Ken Lang. 1995. Newsweeder: Learning to filter net-
news. In Proceedings of the Twelfth International
Conference on Machine Learning, pages 331–339.

Jing Li, Ming Liao, Wei Gao, Yulan He, and Kam-Fai
Wong. 2016. Topic extraction from microblog posts
using conversation structures. In Proceedings of the
54th Annual Meeting of the Association for Compu-
tational Linguistics, ACL 2016, August 7-12, 2016,
Berlin, Germany, Volume 1: Long Papers.

Yishu Miao, Lei Yu, and Phil Blunsom. 2016. Neu-
ral variational inference for text processing. In Pro-
ceedings of the 33nd International Conference on
Machine Learning, ICML 2016, New York City, NY,
USA, June 19-24, 2016, pages 1727–1736.

Radford M Neal. 1993. Probabilistic inference using
markov chain monte carlo methods.

Boyuan Pan, Yazheng Yang, Zhou Zhao, Yueting
Zhuang, Deng Cai, and Xiaofei He. 2018. Dis-
course marker augmented network with reinforce-
ment learning for natural language inference. In
Proceedings of the 56th Annual Meeting of the As-
sociation for Computational Linguistics (Volume 1:
Long Papers), pages 989–999. Association for Com-
putational Linguistics.

Pengda Qin, Weiran XU, and William Yang Wang.
2018. Robust distant supervision relation extrac-
tion via deep reinforcement learning. In Proceed-
ings of the 56th Annual Meeting of the Association
for Computational Linguistics (Volume 1: Long Pa-
pers), pages 2137–2147. Association for Computa-
tional Linguistics.

Michael Röder, Andreas Both, and Alexander Hinneb-
urg. 2015. Exploring the space of topic coherence
measures. In Proceedings of the Eighth ACM Inter-
national Conference on Web Search and Data Min-
ing, WSDM 2015, Shanghai, China, February 2-6,
2015, pages 399–408.

Noah A. Smith, Dallas Card, and Chenhao Tan. 2018.
Neural models for documents with metadata. In
ACL.

Casper Kaae Sønderby, Tapani Raiko, Lars Maaløe,
Søren Kaae Sønderby, and Ole Winther. 2016. Lad-
der variational autoencoders. In Advances in Neu-
ral Information Processing Systems 29: Annual
Conference on Neural Information Processing Sys-
tems 2016, December 5-10, 2016, Barcelona, Spain,
pages 3738–3746.

Akash Srivastava and Charles Sutton. 2017. Autoen-
coding variational inference for topic models. In
Proceedings of the 5th International Conference on
Learning Representations (ICLR).

Chenhao Tan, Dallas Card, and Noah A. Smith. 2017.
Friendships, rivalries, and trysts: Characterizing re-
lations between ideas in texts. In Proceedings of the
55th Annual Meeting of the Association for Compu-
tational Linguistics, ACL 2017, Vancouver, Canada,
July 30 - August 4, Volume 1: Long Papers, pages
773–783.

Qingyu Yin, Yu Zhang, Wei-Nan Zhang, Ting Liu,
and William Yang Wang. 2018. Deep reinforce-
ment learning for chinese zero pronoun resolution.
In Proceedings of the 56th Annual Meeting of the As-
sociation for Computational Linguistics (Volume 1:
Long Papers), pages 569–578. Association for Com-
putational Linguistics.

Xingshan Zeng, Jing Li, Lu Wang, Nicholas
Beauchamp, Sarah Shugars, and Kam-Fai Wong.
2018. Microblog conversation recommendation via
joint modeling of topics and discourse. In Proceed-
ings of the 2018 Conference of the North American
Chapter of the Association for Computational Lin-
guistics: Human Language Technologies, Volume
1 (Long Papers), pages 375–385. Association for
Computational Linguistics.

https://doi.org/10.1023/A:1020281327116
https://doi.org/10.1023/A:1020281327116
http://arxiv.org/abs/1705.09296
https://doi.org/10.18653/v1/P17-1036
https://doi.org/10.18653/v1/P17-1036
http://arxiv.org/abs/1312.6114
http://arxiv.org/abs/1312.6114
http://jmlr.org/proceedings/papers/v48/miao16.html
http://jmlr.org/proceedings/papers/v48/miao16.html
http://aclweb.org/anthology/P18-1199
http://aclweb.org/anthology/P18-1199
https://doi.org/10.1145/2684822.2685324
https://doi.org/10.1145/2684822.2685324
http://papers.nips.cc/paper/6275-ladder-variational-autoencoders
http://papers.nips.cc/paper/6275-ladder-variational-autoencoders
http://aclweb.org/anthology/P18-1053
http://aclweb.org/anthology/P18-1053
https://doi.org/10.18653/v1/N18-1035
https://doi.org/10.18653/v1/N18-1035

