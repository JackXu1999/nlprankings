



















































Zero-Shot Cross-Lingual Abstractive Sentence Summarization through Teaching Generation and Attention


Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 3162–3172
Florence, Italy, July 28 - August 2, 2019. c©2019 Association for Computational Linguistics

3162

Zero-Shot Cross-Lingual Abstractive Sentence Summarization through
Teaching Generation and Attention

Xiangyu Duan1,2∗, Mingming Yin1∗, Min Zhang1,2, Boxing Chen3, Weihua Luo3
1 Institute of Artificial Intelligence, Soochow University, Suzhou, China

2 School of Computer Science and Technology, Soochow University, Suzhou, China
3 Alibaba DAMO Academy, Hangzhou, China

xiangyuduan@suda.edu.cn; mmyin@stu.suda.edu.cn; minzhang@suda.edu.cn
{boxing.cbx,weihua.luowh}@alibaba-inc.com

Abstract

Abstractive Sentence Summarization (AS-
SUM) targets at grasping the core idea of the
source sentence and presenting it as the sum-
mary. It is extensively studied using statis-
tical models or neural models based on the
large-scale monolingual source-summary par-
allel corpus. But there is no cross-lingual
parallel corpus, whose source sentence lan-
guage is different to the summary language,
to directly train a cross-lingual ASSUM sys-
tem. We propose to solve this zero-shot prob-
lem by using resource-rich monolingual AS-
SUM system to teach zero-shot cross-lingual
ASSUM system on both summary word gen-
eration and attention. This teaching pro-
cess is along with a back-translation pro-
cess which simulates source-summary pairs.
Experiments on cross-lingual ASSUM task
show that our proposed method is significantly
better than pipeline baselines and previous
works, and greatly enhances the cross-lingual
performances closer to the monolingual per-
formances. We release the code and data
at https://github.com/KelleyYin/
Cross-lingual-Summarization.

1 Introduction

Abstractive Sentence Summarization (ASSUM) is
a task of condensing the source sentences into
the summaries based on the core meaning of the
source sentences. ASSUM provides quick ac-
cess to the important content of the source sen-
tences through the informative re-written sum-
maries. Major ASSUM explorations are monolin-
gual based. There is an urgent demand of cross-
lingual ASSUM which produces summaries for
people who do not speak the language the same
to the source language.

Unlike the monolingual ASSUM receiving ex-
tensive studies that are based on the large-scale

∗ Equal contribution.

monolingual ASSUM corpus, the cross-lingual
ASSUM is seldom explored due to the lack of
training corpus. This zero-shot challenge drives
the cross-lingual ASSUM to resort to two existing
independent techniques, i.e., the monolingual AS-
SUM and the bilingual translation. The both tech-
niques should be leveraged together to overcome
the difficulty of data scarcity in the cross-lingual
ASSUM.

Regarding the techniques of the monolingual
ASSUM, neural methods become dominant in this
area since the creation of the large-scale ASSUM
corpus (Rush et al., 2015; Nallapati et al., 2016;
Hu et al., 2015). The corpus consists of huge num-
ber of source-summary pairs, and neural methods
model these pairs as as a sequence-to-sequence
task by encoding the source sentence into vector-
ized information and decoding it into the abstrac-
tive summary.

Regarding the techniques of the bilingual trans-
lation, recent years witnessed the method tran-
sition from statistical machine translation (SMT)
(Koehn et al., 2003) to neural machine translation
(NMT). NMT employs the sequence-to-sequence
architecture with various implementations such as
RNN-based (Sutskever et al., 2014; Bahdanau
et al., 2015), CNN-based (Gehring et al., 2017),
and Transformer (Vaswani et al., 2017).

Early works on the cross-lingual ASSUM lever-
age the above two techniques through using bilin-
gual features to cooperate with the monolingual
ASSUM based on the data condition that large-
scale monolingual ASSUM corpus is not avail-
able while large-scale translation corpora are easy
to obtain. They utilize bilingual features such as
phrase pairs or predicate-argument parallel struc-
tures, which are obtained from SMT systems,
to achieve extractive or abstractive cross-lingual
summarization (Wan, 2011; Yao et al., 2015;
Zhang et al., 2016).

https://github.com/KelleyYin/Cross-lingual-Summarization
https://github.com/KelleyYin/Cross-lingual-Summarization


3163

Recently, Ayana et al. (2018) propose the
first large-scale corpus-based cross-lingual AS-
SUM system in which the ASSUM corpus is
monolingual. They generate summaries using
the monolingual ASSUM system, and train the
cross-lingual ASSUM based on these pseudo sum-
maries.

On the contrary, we propose in this paper to
use genuine summaries paired with the generated
pseudo sources to train the cross-lingual ASSUM
system. We use the teacher-student framework in
which the monolingual ASSUM system is taken
as the teacher and the cross-lingual ASSUM sys-
tem is the student. The teacher let the student to
simulate both the summary word distribution and
attention weights according to those of the teacher
networks. In comparison to the pseudo summaries
used in the work of Ayana et al. (2018), we gen-
erate pseudo sources instead and use true sum-
maries to constitute source-summary pairs. This
is motivated by the successful application of back-
translation which generates pseudo-source paired
with true-target for NMT (Sennrich et al., 2016a;
Lample et al., 2018).

The main contributions of this paper include:

• We propose teaching both summary word
generation distribution and attention weights
in the cross-lingual ASSUM networks by
using the monolingual ASSUM networks.
The distribution teacher is directly from the
monolingual ASSUM, while the attention
weights teacher is obtained by an attention
relay mechanism.

• We use a back-translation procedure that gen-
erates pseudo source sentences paired with
the true summaries to build a training cor-
pus for the cross-lingual ASSUM. This alle-
viates the data scarcity that no cross-lingual
ASSUM corpus is available.

• Extensive experimental results on two bench-
mark datasets show that our proposed method
is able to perform better than several base-
lines and related works, and significantly re-
duce the performance gap between the cross-
lingual ASSUM and the monolingual AS-
SUM.

2 Related Work

2.1 Monolingual ASSUM
There are various methods exploring the effective
way to model the monolingual ASSUM process
including statistical models (Banko et al., 2000;
Cohn and Lapata, 2008) or neural models (Rush
et al., 2015; Chopra et al., 2016; Nallapati et al.,
2016). Neural models become dominant in this
task since the creation of the large-scale ASSUM
corpus (Rush et al., 2015; Nallapati et al., 2016;
Hu et al., 2015). On the basis of the sequence-to-
sequence neural architecture, there are many fur-
ther explorations such as using rich linguistic fea-
tures and large vocabulary set (Nallapati et al.,
2016), global training procedures on the sentence
level (Ayana et al., 2016; Li et al., 2018; Edunov
et al., 2018; Wang et al., 2018), topic enhancement
in the summaries (Wang et al., 2018), additional
selective gate networks in the encoder (Zhou
et al., 2017), and facts fusion measures (Cao et al.,
2018).

2.2 Zero Resource Neural Machine
Translation

Current state-of-the-art NMT models are effective
in modeling the translation process, but they are
highly dependent on the large-scale parallel cor-
pus. When applied on zero resource language
pairs such as the two languages that do not have
direct parallel corpus, the NMT systems perform
well below the satisfactory level. To address such
problem, three NMT paradigms are explored. The
first is the triangular NMT systems that add one
additional resource rich language to the zero re-
source language pair to build a triangular trans-
lation scenario (Chen et al., 2017; Zheng et al.,
2017; Cheng et al., 2017), the second is the multi-
lingual translation system that concatenates paral-
lel corpora of different language pairs and builds
one NMT model for all (Johnson et al., 2017),
the third is the unsupervised NMT systems that do
not use any parallel data resources (Artetxe et al.,
2018; Lample et al., 2018).

Our work is closely related to the first paradigm
in which source language, pivot language, and
target language form a triangular translation sce-
nario. In our setting, the target language
{sentence, summary} pair and the source language
sentence form the triangle in which the target lan-
guage sentence functions as the pivot. We adopt
the teacher-student framework that is also applied



3164

in Chen et al. (2017), but we have significant dif-
ference to them in that we generate pseudo source
while they generate pseudo target, which results in
different teacher-student networks.

2.3 Cross-lingual Summarization

Early explorations on cross-lingual summariza-
tion mainly depend on the traditional monolingual
summarization methods, and integrate bilingual
parallel informations into the monolingual meth-
ods through sentence selection based on transla-
tion quality estimation (Wan et al., 2010), sen-
tence ranking based on cross-lingual sentence sim-
ilarity (Wan, 2011), or abstractive summariza-
tion based on phrase pair (Yao et al., 2015) and
predicate-argument structure fusing (Zhang et al.,
2016).

The first cross-lingual ASSUM system based
on the large-scale monolingual ASSUM corpus is
proposed by Ayana et al. (2018), which is most
related to our work. It is motivated by the tri-
angular NMT systems with pseudo target in the
teacher-student networks. In contrast, we use
pseudo source and apply different teacher-student
networks.

3 Our Approach

3.1 Overall Framework

To overcome the data scarcity in the cross-lingual
ASSUM, it is easy to come up with the pipeline
method at the first thought. The source language
sentence can be translated to the target language
sentence, followed by target language summariza-
tion step to get the final target language summary.
Alternatively, the source language sentence can
be summarized into source language summary at
first, then is translated into the target language
summary. Both pipeline methods face the error
propagation problem that errors in the early steps
will harm the final summary quality.

We propose a jointly optimizing framework that
avoids the independent two steps in the pipeline
methods. Figure 1 (a) illustrates our overall frame-
work. We introduce a bridge between the source
language sentence and the target language sum-
mary. The target language sentence functions as
the bridge convenient for the information flow
from the source sentence to the target summary.

The overall framework mainly consists of two
modules: the teacher networks and the student net-
works. The teacher is the monolingual ASSUM

teacher

src lang sentence tgt lang sentence tgt lang summary
NMT

student
(a)

teacher

src lang sentence tgt lang sentence tgt lang summary
NMT

student
(b)

Figure 1: Illustration of the comparison between (a)
our overall framework and (b) the framework of Ayana
et al. (2018). Solid line boxes denote genuine data,
while dashed line boxes denote automatically gener-
ated pseudo data. Solid line arrows denote the sum-
marization direction, while dashed line arrows denote
pseudo data generation direction. Note that the gen-
uine data is used in the teacher of our framework, while
pseudo data is used in the teacher of the framework of
Ayana et al. (2018).

neural networks trained on the large-scale mono-
lingual ASSUM corpus. Note that in our frame-
work, the teacher is strong since the utilized mono-
lingual ASSUM corpus is genuine and no pseudo
data is used in the teacher. The student is the cross-
lingual ASSUM networks trained to mimic the be-
havior of the teacher.

To manifest the difference between our frame-
work and the most related framework of Ayana et
al. (2018), we depict both in Figure 1. In the
framework of Ayana et al. (2018), the source
language sentence is automatically translated into
the target language sentence, which is automati-
cally summarized into the target language sum-
mary. The data on both sides of the teacher net-
works are pseudo. This is significantly different
to our framework in which the teacher networks
have the strong data basis that all data on both
sides of the teacher networks are genuine. When
comparing the student networks, we can find that
we adopt pseudo source sentence, while Ayana et
al. (2018) adopt pseudo target summary. Further-
more, we also teach the student with the teacher’s
attention weights via a new attention relay mecha-
nism.



3165

3.2 Back-Translation

Our framework contains a back-translation pro-
cedure which is inspired by that used in semi-
supervised or unsupervised machine translation
(MT) (Sennrich et al., 2016a; Lample et al.,
2018). In MT, the back-translation process trans-
lates unpaired target text into source text. The
resulted pseudo source-target pair serves as addi-
tional training data for source-to-target translation.
Our proposed back-translation procedure involves
triple kinds of data. It translates the target lan-
guage sentence back into the source language sen-
tence by a third-party NMT system. The gener-
ated pseudo source is paired with the true target
summary to build a training resource for the stu-
dent networks. The back-translation procedure is
denoted as the dashed arrow NMT in Figure 1(a).

3.3 The Teacher-Student Training Procedure

We use the monolingual ASSUM system as the
teacher networks, and use the cross-lingual AS-
SUM system as the student networks. Both the
teacher and the student apply Transformer archi-
tecture which is effective for modeling sequence-
to-sequence tasks such as machine translation
(Vaswani et al., 2017). Two functions of the
teacher are set as the learning objective for the
student. One is the probability distribution of the
summary word generation, the other is the atten-
tion weights in the attention mechanism.

Given the source language text X, the target lan-
guage text Y, and the target language summary
YS, the training procedure for the teacher-student
framework is presented in the following:

Teaching The Summary Word Generation

Let P (YSi|YSi−11 ,Y) denote the teacher distribu-
tion of the summary word given summary word
generation history and Y, P (YSi|YSi−11 , X̂) de-
note the student distribution of the summary word
given summary word generation history and X̂. X̂
denotes the pseudo source which is generated by
the back-summarization procedure. We use cross
entropy loss to encourage the similarity between
the two distributions:

Lgen = −P (YSi|YSi−11 ,Y)logP (YSi|YS
i−1
1 , X̂)

(1)
Through Equation (1), the cross-lingual AS-

SUM learns from the monolingual ASSUM about

Y1 Y2 Y3 Y4 Y5

YS1 YS2  

            

Figure 2: Illustration of the attention relay. The ar-
rows are the attentions with the direction of decoder
side word attending to encoder side words. Solid ar-
rows are top-k or the biggest attention weights, and the
dashed arrows are the left attention weights. k is 2 in
the figure.

how to generate summary word under appropriate
distribution.

Teaching The Attention Weights via Attention
Relay

Besides the summary word generation distribu-
tion, the attention of the monolingual ASSUM is
also a valuable learning resource. But such atten-
tion only connects the encoder and the decoder
of the monolingual ASSUM system, it has to be
relayed to reach the other language to teach the
cross-lingual ASSUM system.

The attention relay mechanism is illustrated in
Figure 2. The monolingual attention weights of YS
attending to Y is relayed to form the teacher atten-
tion weights of YS attending to X̂. In particular, Y2
and Y4 receive top-2 attention weights from YS1
in Figure 2, and Y2 receives biggest attention from
X̂1, Y4 receives biggest attention from X̂2. Then
the attention weights of YS1 attending to X̂1 and
X̂2 are set 1/2. Other attention weights distributed
over the rest words of X̂ are set zero. In general
case, if top-k attention weights are relayed from
YS to X̂, then the teacher attention weights over the
k words of X̂ are set 1/k each, and other attention
weights are set zero 1.

We use the Euclidean distance between the
teacher attention weights and the student attention

1We also use the attention matrix of YS attending to Y
multiplies the attention matrix of X̂ attending to Y to form
the teacher attention, but we found that the teacher attention
weights are evenly distributed, resulting in worse student per-
formance.



3166

weights as the loss to encourage their consistency:

Latt =

√∑
j

(Aj − Āj)2 (2)

where Aj denotes a teacher attention weight, Āj
denotes a student attention weight.

Note that in our work, the attention only refers
to the encoder-decoder attention, not the self-
attention in Transformer. Since our teacher net-
works and the student networks adopt Transformer
architecture which contains multi-head attention,
we use the average attention that averages atten-
tion weights of all heads in the same layer.

3.4 Training and Testing

The training objective is to minimize the joint loss:

L = λLgen + (1− λ)Latt (3)

where λ is the weight balancing Lgen and Latt.
During testing, only the student networks

are used to decode X into YS. In detail, only
P (YSi|YSi−11 ,X) participates in the beam search,
the summary word generation teacher and whole
Latt-related teacher-student networks are not in-
volved in the testing process.

4 Experiments

We conduct experiments on Chinese-to-English
ASSUM, which takes Chinese sentence as input,
and outputs English abstractive summary. We
build evaluation sets for this task by manually
translating English sentences of the existing En-
glish evaluation sets into Chinese inputs. To the
best of our knowledge, these are the first evalua-
tion sets on which the cross-lingual ASSUM sys-
tem and the monolingual ASSUM system can be
directly compared.

4.1 Datasets

In our experiments, the English ASSUM sys-
tem and the English-Chinese NMT system are in-
volved. The data for training both systems are pre-
sented below.

The data for training the English ASSUM sys-
tem is from the annotated Gigaword corpus, and
we preprocess it identically to Rush et al. (2015),
which results in around 3.8M training pairs, 190K
validation pairs and 1951 test pairs. In this data,
the sentence-summary pairs are built by pairing
the first sentence of each article with the article’s

headline. Additionally, DUC-2004 is adopted as
another English data set only for testing. It con-
tains 500 documents, and each document has four
human-generated reference summaries.

To build the evaluation sets, English sentences
of the validation and test sets of Gigaword and
DUC2004 are manually translated into Chinese
by graduate students of the linguistics department
and our institute, who are bilingual with Chinese
as the mother tongue. Specifically, in the Gi-
gaword validation set, we randomly select 2000
sentence-summary pairs and manually translate
their English sentences into Chinese. The English
summaries are not translated. The Chinese sen-
tences are segmented by the word segmentation
tool Jieba2.

Additionally, we also implement some base-
lines for comparison, some of which utilize a large
corpus of Chinese short text summarization (LC-
STS) (Hu et al., 2015), which is collected from the
Chinese microblogging website Sina Weibo with
2.4M sentence-summary pairs for training and 725
pairs for testing.

4.2 Experimental Configuration

Baseline Systems

• The pipeline of translating source sentence
into target sentence at first, then summariz-
ing the target sentence into the summary. We
denote this method Pipeline-TS.

• The pipeline of summarizing the source sen-
tence into the source summary, then translat-
ing the source summary into the target sum-
mary. We denote this method Pipeline-ST.

• The framework of Ayana et al. (2018), which
uses pseudo summary for training. We de-
note it Pseudo-Summary3.

• The pivot system that enforcing the source-
to-pivot system and the pivot-to-target sys-
tem sharing the same pivot language embed-
ding (Cheng et al., 2017). We denote it Pivot-
based.

2https://pypi.org/project/jieba/
3We also implement the framework that uses the NMT

model to teach the cross-lingual ASSUM (Ayana et al.,
2018). Since it highly depends on the LCSTS data, whose
style is different to our evaluation sets, it performs signifi-
cantly worse.



3167

NIST02 NIST03 NIST04 NIST05 NIST08 Avg
Our Transformer Cn2En 45.58 45.19 46.80 46.56 37.27 44.28
Robust Translation Cn2En (Cheng et al., 2018) 46.10 44.07 45.61 44.06 34.94 42.96
Our Transformer En2Cn 39.38 34.48 38.10 36.20 30.80 35.79

Table 1: BLEU of the NMT systems on NIST evaluation sets. Cn2En denotes Chinese-to-English translation, and
En2Cn denotes the reverse direction.

System Gigaword DUC2004ROUGE-1 ROUGE-2 ROUGE-L ROUGE-1 ROUGE-2 ROUGE-L
ABS+ (Rush et al., 2015) 29.8 11.9 27.0 28.2 8.5 23.8
Actor-Critic (Li et al., 2018) 36.1 17.4 33.5 29.4 9.8 25.9
StructuredLoss (Edunov et al., 2018) 36.7 17.9 34.3 - - -
FactAware (Cao et al., 2018) 37.3 17.7 34.2 - - -
Transformer 37.1 18.2 34.4 30.6 10.5 26.6
Transformerbpe 38.1 19.1 35.2 31.2 10.7 27.1

Table 2: Comparison on the monolingual ASSUM performances. “-” denotes that no score is reported in that work.

• Translating the English sentences into the
Chinese sentences, and pair these pseudo
Chinese sentences with English summaries
to build a training corpus for Chinese-to-
English ASSUM. We denote it Pseudo-
Chinese. We implement it by using Trans-
former machine translation model to trans-
late the English sentences, and use Trans-
former architecture to train a Chinese-to-
English ASSUM system. Note that this is just
the student network without being taught by
a teacher network.

Parameter Setup and Evaluation Metric

Transformer is employed as our basis architecture4

(Vaswani et al., 2017). Six layers are stacked in
both the encoder and decoder, and the dimensions
of the embedding vectors and all hidden vectors
are set 512. We set eight heads in the multi-head
attention. The source embedding, the target em-
bedding and the linear sublayer are shared in the
teacher networks, while are not shared in the stu-
dent networks. Byte-pair encoding is employed
with a vocabulary of about 32k tokens on En-
glish side and Chinese side respectively (Sennrich
et al., 2016b).

During evaluation, we employ ROUGE (Lin,
2004) as our evaluation metric. On Gigaword, the
full-length F-1 based ROUGE scores are reported.
On DUC2004, the recall based ROUGE scores are
reported to be consistent with previous works.

NMT Performance

The NMT system involved in all our experiments
4https://github.com/pytorch/fairseq

is Transformer, with the same parameter setup
to those of ASSUM systems. It is trained on
1.25M sentence pairs extracted from LDC cor-
pora5, and is evaluated on NIST sets using multi-
bleu.perl. Chinese-to-English results of case-
insensitive BLEU and English-to-Chinese results
of character-based BLEU are reported in Table 1.
Since there are four English references for one
Chinese sentence in NIST evaluation sets, we re-
port averaged BLEU of four English input sen-
tences in English-to-Chinese translation.

Compared to Cheng et al. (2018) on Chinese-
to-English translation, which targets at robust ma-
chine translation and uses the same data to ours,
our Transformer significantly outperforms their
work, indicating that we build a solid system for
machine translation.

4.3 Experimental Results

Monolingual ASSUM Performance

We build a strong monolingual ASSUM system
as shown in Table 2. The comparison is made
between our basis architecture Transformer and
previous works including state-of-the-art monolin-
gual ASSUM systems. The work of ABS+ (Rush
et al., 2015) is the pioneer work of using neu-
ral models for monolingual ASSUM. The works
of Actor-Critic (Li et al., 2018) and Structured-
Loss (Edunov et al., 2018) are training methods
avoiding exposure bias problems in sequence-to-
sequence learning. The work of FactAware (Cao
et al., 2018) encode factual informations such as

5The corpora include LDC2002E18, LDC2003E07,
LDC2003E14, Hansards portion of LDC2004T07,
LDC2004T08 and LDC2005T06.



3168

System Gigaword DUC2004ROUGE-1 ROUGE-2 ROUGE-L ROUGE-1 ROUGE-2 ROUGE-L
Transformerbpe 38.1 19.1 35.2 31.2 10.7 27.1
Pipeline-TS 25.8 9.7 23.6 23.7 6.8 20.9
Pipeline-ST 22.0 7.0 20.9 20.9 5.3 18.3
Pseudo-Summary (Ayana et al., 2018) 21.5 6.6 19.6 19.3 4.3 17.0
Pivot-based (Cheng et al., 2017) 26.7 10.2 24.3 24.0 7.0 21.3
Pseudo-Chinese 27.9 10.9 25.6 24.4 6.6 21.4
Teaching Generation 29.6 12.1 27.3 25.6 7.9 22.7
Teaching Attention 28.1 11.4 26.0 24.3 7.4 21.7
Teaching Generation+Attention 30.1 12.2 27.7 26.0 8.0 23.1

Table 3: Comparison on the cross-lingual ASSUM performances.

those extracted from openIE and dependency re-
lations into the neural network to get factual sum-
maries.

Transformer with BPE pre-processing (denoted
by Transformerbpe) performs consistently better
than the related monolingual ASSUM systems.
We build the cross-lingual ASSUM system basing
on Transformerbpe.

Cross-lingual ASSUM performance

Table 3 mainly presents the results of the cross-
lingual ASSUM systems. The first row lists
the performance of Transformerbpe, which is the
monolingual ASSUM system. It sets the ceiling of
the cross-lingual ASSUM performance since the
cross-lingual process introduces information loss
when using another language.
Comparisons between the Baselines The mid-
dle part of Table 3 is about baseline systems. It
shows that Pipeline-TS is significantly better than
Pipeline-ST. The optimal order of the two steps in
the pipeline methods should be translating source
sentence at first, then summarizing the trans-
lation. The Pseudo-Summary method (Ayana
et al., 2018) performs even below the Pipeline-
ST method. It indicates that using the pseudo tar-
get side is not effective for learning better cross-
lingual summarization model. Meanwhile, as Fig-
ure 1(b) illustrates, both source side and target side
of the teacher network in the framework of Ayana
et al. (2018) are pseudo, resulting in less solid data
basis for training the student. The pseudo source
side is generated by translating LCSTS Chinese
sentences.

The two baseline systems that surpass the
pipeline systems are Pivot-based system and
Pseudo-Chinese system. We re-implement the
Pivot-based system but using Transformer instead
of RNN, which is used in Cheng et al. (2017).
Pseudo-Chinese system is the best baseline sys-

tem indicating that pseudo source based parallel
data is effective for training cross-lingual ASSUM
system.
Our Systems VS. the Baselines The bottom part
of Table 3 lists the performances of our meth-
ods. It manifests that both teaching summary word
generation and teaching attention weights are able
to improve the performance over the baselines.
When the summary word generation and attention
weights are taught simultaneously (denoted by
Teaching Generation+Attention), the performance
is further improved, surpassing the best baseline
by more than two points on Gigaword evaluation
set and more than one point on DUC2004.
Our Systems VS. the Ceiling Teaching Gener-
ation+Attention greatly reduces the gap between
the cross-lingual ASSUM performance and the
performance ceiling, i.e., the monolingual AS-
SUM performance shown in the first row. The
gap is narrowed from 10.2 ROUGE-1 points to 8
ROUGE-1 points. In fact, our best method per-
forms even better than ABS+, which is the early
system for monolingual ASSUM (Rush et al.,
2015).

4.4 Experiment Analyses

Hyper-Parameters

λ ROUGE-1 ROUGE-2 ROUGE-L
0.1 44.8 22.0 41.7
0.3 45.1 22.3 42.0
0.5 45.0 22.2 41.9
0.7 44.9 22.2 41.8
0.9 44.8 21.8 41.7

top-k ROUGE-1 ROUGE-2 ROUGE-L
2 44.8 22.4 41.9
3 44.9 22.0 42.0
4 45.1 22.3 42.0
5 45.1 22.2 41.8

Table 4: Performances of varying hyper-parameters on
the validation set.



3169

5

10

15

20

25

30

0-10 11-20 21-30 31-40 41-50 >50

RO
U

GE
 F

-1
 S

co
re

s

Input Sentence Length

Pseudo-Chinese

Teaching Generation

Teaching Attention

Teaching Generation+Attention

Figure 3: ROUGE-1 scores on different length source sentences in the Gigaword test set.

There are two main hyper-parameters. One is λ
in Equation (3) that balances the weights between
teaching generation and teaching attention during
training. The other is top-k which controls how
many portion of the monolingual ASSUM atten-
tion can be relayed to the source side as illustrated
in Figure 2. Table 4 presents the performance vari-
ance when the two hyper-parameters vary.

It shows that the performance is best when λ is
0.3, indicating that training process is balanced to-
wards teaching attention via attention relay. Based
on the best λ of 0.3, we explore top-k ranging
from 2 to 5. We can find that top-4 monolingual
ASSUM attention weights achieve the best perfor-
mance on the validation set. We select the best
hyper-parameters according to Table 4 for testing.

Layers for Attention Relay

Transformer architecture used in our experiment is
with six layers on both encoder and decoder. At-
tention relay can take place on each layer. Since
each layer has eight heads for attention computa-
tion, we average the weights of all eight heads in
the same layer. We study the attention relay ef-
fects on all six layers. The results in Table 5 show
that relaying attention on the last layer achieves
the best performance.

Performances on Different Lengths

We study the performance of each system on sets
with different source sentence lengths. The source
sentences are divided into six groups according
to their lengths. Figure 3 presents the ROUGE-

Layer ROUGE-1 ROUGE-2 ROUGE-L
1 44.7 21.8 41.6
2 44.7 22.3 41.7
3 45.0 22.0 41.8
4 44.9 22.1 41.3
5 44.9 22.1 41.9
6 45.1 22.3 42.0

Table 5: Validation set performances of using different
layers for attention relay.

1 scores on the test set. The strongest baseline
Pseudo-Chinese is used in this study. It shows
that our methods perform better than Pseudo-
Chinese on most groups, while teaching attention
is slightly worse on the group with the longest
length. The sentences with length range 10-50
take up 94.2% of the whole test set. Our meth-
ods are consistently better than Pseudo-Chinese on
theses sentences.

Qualitative Analysis

Table 6 presents some examples of the cross-
lingual ASSUM. The differences between our
methods and the strongest baseline Pseudo-
Chinese are highlighted. It shows that more accu-
rate summary words are produced in our systems.
In contrast, Pseudo-Chinese may produce incor-
rect words that are even contrary to the meaning
of the original sentence.

5 Conclusion

In this paper, we propose a teacher-student frame-
work together with the back-translation proce-
dure to deal with the zero-shot challenge of cross-



3170

Cn-sentence 据周六报道，印度最高核专家对广岛日印度人的反核抗议不屑一顾，称激进分子应该在华
盛顿和莫斯科喊口号。

En-sentence a india ’s top nuclear expert shrugged off antinuclear protests by indians on hiroshima day , saying the activists
should instead shout slogans in washington and moscow , a newspaper reported saturday .

Ref-summary top nuclear scientist shrugs off indian antinuclear protests
Psueo-Chinese india ’s top nuclear expert calls for anti-nuke demo in hiroshima
Teaching-Generation india ’s top nuclear expert warns against nuclear protests
Teaching-Attention india ’s top nuclear scientist defies hiroshima protest
Teaching-Gener+Attn india ’s top nuclear expert defies anti-nuclear protests

Cn-sentence 黎巴嫩总理拉菲克 -哈里里星期二指责英国支持以色列袭击黎巴嫩真主党游击队，同时他宣布
计划访问伦敦。

En-sentence lebanese prime minister rafic hariri accused britain on tuesday of supporting the israeli assault on hezbollah
guerrillas in lebanon as he announced plans to visit london .

Ref-summary hariri to visit britain which he accuses of backing israel
Psueo-Chinese lebanese pm accuses britain of supporting hezbollah
Teaching-Generation lebanese pm accuses britain of backing hezbollah attacks
Teaching-Attention lebanese pm accuses britain of supporting hezbollah
Teaching-Gener+Attn lebanese pm accuses britain of backing israel

Cn-sentence 苏丹武装部队发言人今天说，政府军击退了叛军沿苏丹东部边境发动的攻击。
En-sentence government troops has repelled an attack by rebel forces along sudan ’s eastern borders , the spokesman of the

sudanese armed forces said today .
Ref-summary government forces repel rebel attack in eastern
Psueo-Chinese sudanese government forces attack rebels in eastern sudan
Teaching-Generation government troops repulse rebel attack in eastern sudan
Teaching-Attention sudanese army says it foiled rebel attack on eastern border
Teaching-Gener+Attn government troops repel rebel attack in eastern sudan

Table 6: Examples of the cross-lingual ASSUM.

lingual ASSUM, which has no parallel data for
training. We use monolingual ASSUM which has
large-scale training resources as the teacher, and
set the cross-lingual ASSUM as the student. Two
properties of the teacher are proposed to teach the
student. One is the summary word generation
probabilities, the other is the attention weights.
We also propose attention relay mechanism to
form the attention weights of the teacher. Experi-
ments show that our method performs significantly
better than several baselines, and is able to signif-
icantly reduce the performance gap between the
cross-lingual ASSUM and the monolingual AS-
SUM over the benchmark datasets.

Acknowledgments

The authors would like to thank the anonymous
reviewers for the helpful comments. This work
was supported by National Key R&D Program of
China (Grant No. 2016YFE0132100), National
Natural Science Foundation of China (Grant No.
61525205, 61673289), and was also partially sup-
ported by the joint research project of Alibaba and
Soochow University.

References

Mikel Artetxe, Gorka Labaka, and Eneko Agirre. 2018.
Unsupervised statistical machine translation. In
Proceedings of the 2018 Conference on Empirical
Methods in Natural Language Processing, pages
261–270.

Ayana, Shi-Qi Shen, Yun Chen, Yang Cheng, Zhiyuan
Liu, and Maosong Sun. 2018. Zero-shot cross-
lingual neural headline generation. IEEE/ACM
Transactions on Audio, Speech, and Language Pro-
cessing, 26(12):2319–2327.

Ayana, Shiqi Shen, Yu Zhao, Zhiyuan Liu, and
Maosong Sun. 2016. Neural headline generation
with sentence-wise optimization. arXiv preprint
arXiv:1604.01904.

Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Ben-
gio. 2015. Neural machine translation by jointly
learning to align and translate. In Proc of Interna-
tional Conference on Learning Representations.

Michele Banko, Vibhu O. Mittal, and Michael J. Wit-
brock. 2000. Headline generation based on statisti-
cal translation. In Proceedings of the 38th Annual
Meeting of the Association for Computational Lin-
guistics, pages 310–317.

Ziqiang Cao, Furu Wei, Wenjie Li, and Sujian Li. 2018.
Faithful to the original: Fact aware neural abstrac-
tive summarization. In Thirty-Second AAAI Confer-
ence on Artificial Intelligence.

Yun Chen, Yang Liu, Yong Cheng, and Victor O.K.
Li. 2017. A teacher-student framework for zero-
resource neural machine translation. In Proceed-
ings of the 55th Annual Meeting of the Association
for Computational Linguistics (Volume 1: Long Pa-
pers), pages 1925–1935.

Yong Cheng, Zhaopeng Tu, Fandong Meng, Junjie
Zhai, and Yang Liu. 2018. Towards robust neural
machine translation. In Proceedings of the 56th An-
nual Meeting of the Association for Computational
Linguistics (Volume 1: Long Papers), pages 1756–
1766.



3171

Yong Cheng, Qian Yang, Yang Liu, Maosong Sun, and
Wei Xu. 2017. Joint training for pivot-based neural
machine translation. In Proceedings of the Twenty-
Sixth International Joint Conference on Artificial In-
telligence, pages 3974–3980.

Sumit Chopra, Michael Auli, and Alexander M Rush.
2016. Abstractive sentence summarization with at-
tentive recurrent neural networks. In Proceedings of
the 2016 Conference of the North American Chap-
ter of the Association for Computational Linguistics:
Human Language Technologies, pages 93–98.

Trevor Cohn and Mirella Lapata. 2008. Sentence com-
pression beyond word deletion. In Proceedings
of the 22nd International Conference on Computa-
tional Linguistics, pages 137–144.

Sergey Edunov, Myle Ott, Michael Auli, David Grang-
ier, and Marc’Aurelio Ranzato. 2018. Classical
structured prediction losses for sequence to se-
quence learning. In Proceedings of the 2018 Con-
ference of the North American Chapter of the Asso-
ciation for Computational Linguistics: Human Lan-
guage Technologies, Volume 1 (Long Papers), pages
355–364.

Jonas Gehring, Michael Auli, David Grangier, De-
nis Yarats, and Yann N. Dauphin. 2017. Convo-
lutional sequence to sequence learning. In Pro-
ceedings of the 34th International Conference on
Machine Learning - Volume 70, pages 1243–1252.
JMLR.org.

Baotian Hu, Qingcai Chen, and Fangze Zhu. 2015. Lc-
sts: A large scale chinese short text summarization
dataset. In Proceedings of the 2015 Conference on
Empirical Methods in Natural Language Process-
ing, pages 1967–1972.

Melvin Johnson, Mike Schuster, Quoc V. Le, Maxim
Krikun, and Jeffrey Dean. 2017. Google’s multi-
lingual neural machine translation system: Enabling
zero-shot translation. Transactions of the Associa-
tion for Computational Linguistics, pages 339–351.

Philipp Koehn, Franz J Och, and Daniel Marcu. 2003.
Statistical phrase-based translation. In Proceedings
of the 2003 Human Language Technology Confer-
ence of the North American Chapter of the Associa-
tion for Computational Linguistics, pages 127–133.

Guillaume Lample, Myle Ott, Alexis Conneau, Lu-
dovic Denoyer, and Marc’Aurelio Ranzato. 2018.
Phrase-based and neural unsupervised machine
translation. In Proceedings of the 2018 Conference
on Empirical Methods in Natural Language Pro-
cessing, pages 5039–5049.

Piji Li, Lidong Bing, and Wai Lam. 2018. Actor-
critic based training framework for abstractive sum-
marization. arXiv:1803.11070.

Chin-Yew Lin. 2004. Rouge: A package for automatic
evaluation of summaries. In Text Summarization
Branches Out: Proceedings of the ACL-04 Work-
shop, pages 74–81.

Ramesh Nallapati, Bowen Zhou, Cicero Nogueira Dos
Santos, Caglar Gulcehre, and Xiang Bing. 2016.
Abstractive text summarization using sequence-to-
sequence rnns and beyond. In Proceedings of The
20th SIGNLL Conference on Computational Natu-
ral Language Learning, pages 280–290.

Alexander M Rush, Sumit Chopra, and Jason Weston.
2015. A neural attention model for abstractive sen-
tence summarization. In Proceedings of the 2015
Conference on Empirical Methods in Natural Lan-
guage Processing, pages 379–389.

Rico Sennrich, Barry Haddow, and Alexandra Birch.
2016a. Improving neural machine translation mod-
els with monolingual data. In Proceedings of the
54th Annual Meeting of the Association for Compu-
tational Linguistics (Volume 1: Long Papers), pages
86–96.

Rico Sennrich, Barry Haddow, and Alexandra Birch.
2016b. Neural machine translation of rare words
with subword units. In Proceedings of the 54th An-
nual Meeting of the Association for Computational
Linguistics (Volume 1: Long Papers), pages 1715–
1725.

Ilya Sutskever, Oriol Vinyals, and Quoc V Le. 2014.
Sequence to sequence learning with neural net-
works. In Advances in neural information process-
ing systems, pages 3104–3112.

Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob
Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz
Kaiser, and Illia Polosukhin. 2017. Attention is all
you need. In Advances in Neural Information Pro-
cessing Systems 30, pages 5998–6008.

Xiaojun Wan. 2011. Using bilingual information for
cross-language document summarization. In Pro-
ceedings of the 49th Annual Meeting of the Asso-
ciation for Computational Linguistics: Human Lan-
guage Technologies-Volume 1, pages 1546–1555.

Xiaojun Wan, Huiying Li, and Jianguo Xiao. 2010.
Cross-language document summarization based on
machine translation quality prediction. In Proceed-
ings of the 48th Annual Meeting of the Association
for Computational Linguistics, pages 917–926.

Li Wang, Junlin Yao, Yunzhe Tao, Li Zhong, Wei
Liu, and Qiang Du. 2018. A reinforced topic-aware
convolutional sequence-to-sequence model for ab-
stractive text summarization. In Proceedings of the
Twenty-Seventh International Joint Conference on
Artificial Intelligence, pages 4453–4460.

Jinge Yao, Xiaojun Wan, and Jianguo Xiao. 2015.
Phrase-based compressive cross-language summa-
rization. In Proceedings of the 2015 Conference on
Empirical Methods in Natural Language Process-
ing, pages 118–127.

Jiajun Zhang, Yu Zhou, and Chengqing Zong. 2016.
Abstractive cross-language summarization via trans-
lation model enhanced predicate argument structure



3172

fusing. IEEE/ACM Trans. Audio Speech, Lang. Pro-
cess, vol. 24, no. 10.

Hao Zheng, Yong Cheng, and Yang Liu. 2017.
Maximum expected likelihood estimation for zero-
resource neural machine translation. In Proceedings
of the Twenty-Sixth International Joint Conference
on Artificial Intelligence, pages 4251–4257.

Qingyu Zhou, Yang Nan, Furu Wei, and Zhou Ming.
2017. Selective encoding for abstractive sentence
summarization. In Proceedings of the 55th An-
nual Meeting of the Association for Computational
Linguistics (Volume 1: Long Papers), pages 1095–
1104.


