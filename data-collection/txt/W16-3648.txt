



















































Neural Utterance Ranking Model for Conversational Dialogue Systems


Proceedings of the SIGDIAL 2016 Conference, pages 393–403,
Los Angeles, USA, 13-15 September 2016. c©2016 Association for Computational Linguistics

Neural Utterance Ranking Model for Conversational Dialogue Systems

Michimasa Inaba
Hiroshima City University

3-4-1 Ozukahigashi, Asaminami-ku,
Hiroshima, Japan

inaba@hiroshima-cu.ac.jp

Kenichi Takahashi
Hiroshima City University

3-4-1 Ozukahigashi, Asaminami-ku,
Hiroshima, Japan

takahashi@hiroshima-cu.ac.jp

Abstract

In this study, we present our neural ut-
terance ranking (NUR) model, an utter-
ance selection model for conversational
dialogue agents. The NUR model ranks
candidate utterances with respect to their
suitability in relation to a given context
using neural networks; in addition, a di-
alogue system based on the model con-
verses with humans using highly ranked
utterances. Specifically, the model pro-
cesses word sequences in utterances and
utterance sequences in context via recur-
rent neural networks. Experimental re-
sults show that the proposed model ranks
utterances with higher precision relative
to deep learning and other existing meth-
ods. Furthermore, we construct a conver-
sational dialogue system based on the pro-
posed method and conduct experiments on
human subjects to evaluate performance.
The experimental result indicates that our
system can offer a response that does
not provoke a critical dialogue breakdown
with a probability of 92% and a very natu-
ral response with a probability of 58%.

1 Introduction

The study of conversational dialogue systems
(also known as non-task-oriented or chat-oriented
dialogue systems) has a long history. To con-
struct such systems, rule-based methods have long
been used (Weizenbaum, 1966; Colby, 1981; Wal-
lace, 2008); however, construction and mainte-
nance costs are very high because these rules are
manually created. Moreover, intuition tells us that
the performance of such systems depends on the
number of established rules, though reports indi-
cate that performance did not improve much even

if the number of rules was doubled (Higashinaka
et al., 2015b), indicating that performance of rule-
based systems is limited.

Recently, the study of statistical-based meth-
ods that use statistical processing with large vol-
umes of web data has become increasingly ac-
tive. The key benefit of this approach is that man-
ual response creation is not necessary; thus, con-
struction and maintenance costs are low; however,
since web data contains noise, this approach has
the potential to output grammatically or seman-
tically incorrect sentences. To tackle this prob-
lem, some studies extract correct sentences as ut-
terances for dialogue systems from web data (In-
aba et al., 2014; Higashinaka et al., 2014)．These
studies focus solely on extraction and do not in-
dicate how replies are generated using extracted
sentences.

In our study, we propose a neural utterance
ranking (NUR) model that ranks candidate utter-
ances by their suitability in a given context using
neural networks. Previously, we proposed an ut-
terance selection model (Koshinda et al., 2015) in
the framework same as that of the NUR model,
which ranks utterances in order of suitability to
given context. In section 4, we experimentally
show that the performance of the NUR model ex-
ceeds that of our previous model.

Our proposed method processes the word se-
quences in utterances and utterance sequences in
context via multiple recurrent neural networks
(RNNs). More specifically, the RNN encodes
both utterances in a given context and candidates
into fixed-length vectors. Such encoding enables
suitable feature extraction for ranking. Next, an-
other RNN receives these utterance-encoded vec-
tors in chronological order, and our proposed NUR
model ranks candidates using the output of this
RNN. Our model considers the order of utterances
in a given context; this architecture makes it pos-

393



sible to handle distant semantic relationships be-
tween context and candidates.

2 Related Work

Statistical-based response methods incorporate
two major approaches.

The first approach is the example-based method
(Murao et al., 2003), which searches a large
database of previously recorded dialogue for given
user input selecting an utterance identified as the
most similar. Well-known dialogue systems based
on this approach include Jabberwacky (De An-
geli and Carpenter, 2005) which won the Loebner
prize contesta (i.e., a conversational dialogue sys-
tem competition) in 2005 and 2006. In addition,
Banch and Li. proposed a model based on the vec-
tor space model (Banchs and Li, 2012) and Nio et
al. constructed a dialogue system that uses movie
scripts and Twitter data (Nio et al., 2014). A dis-
advantage of example-based methods is that it is
difficult to consider context. If the implemented
approach searches for user input with context in a
database, it can be difficult to find a suitable con-
text because of the diversity of contexts; in such
cases, system replies become unsuitable. In con-
trast, our NUR model can select responses while
also taking into account a flexible set of contexts.

The second statistical-based response approach
is the machine translation (MT) method. Ritter et
al. first introduced the MT technique into response
generation (Ritter et al., 2011). They used tweet-
reply pairs in Twitter data, regarding a tweet as the
source language sentence and the reply as a target
one in MT. In other words, the MT method trans-
lates user input into system responses. More re-
cently, response generation using neural networks
has been widely studied, most work grounded in
the MT method (Cho et al., 2014; Sordoni et al.,
2015; Shang et al., 2015; Vinyals and Le, 2015).
A problem with this method is that it might gener-
ate utterances containing syntax errors; further, it
tends to generate utterances with broad utility that
frequently appear in training data, e.g., “I don’t
know.” or “I’m OK.” (Li et al., 2016).

Our proposed method is not categorized into ei-
ther of the above two methods. Some hard-to-
classify statistical-based response methods simi-
lar to our model have been proposed, e.g., Shibata
et al. proposed a method that selects a suitable
sentence extracted from webpages as a response

ahttp://www.loebner.net/Prizef/loebner-prize.html

Figure 1: Neural utterance ranking model

to user input (Shibata et al., 2009). Sugiyama et
al. generated responses using templates and de-
pendency structures of sentences gathered from
Twitter (Sugiyama et al., 2013). There are only
few common points, although most of the hard-
to-classify methods use not only dialogue data but
also non-dialogue data such as webpages or nor-
mal tweets (not pairs of tweet reply) on Twitter.

3 Neural Utterance Ranking Model

For our ranking model, we first define sequences
of utterances from the beginning of a dialogue
to a certain point of time in context c =
(u1, u2, . . . , ul) Each ui(i = 1, 2, ..., l) denotes an
utterance in the context, and l denotes the number
of utterances. We assume here that a dialogue sys-
tem and user speak alternately and last utterance ul
is given by the system. We define candidate utter-
ance list ac = (ac1, a

c
2, . . . , a

c
m) generated depend-

ing on context c, and score tc = (tc1, t
c
2, . . . , t

c
m).

Herein, m denotes the number of candidate utter-
ances. We define utterance ranking to sort given
candidate utterance list ac in order of suitabil-
ity to context c. The correct order is defined by
score tc with sorting based on the model’s output
yac = (y1, y2, . . . , ym) corresponding to ac.

Our proposed utterance ranking model, i.e., the
NUR model illustrated in in Figure 1, receives
context c and candidate utterance list ac, then out-
puts yac . Details of our NUR model are described
below.

394



3.1 Utterance Encoding

To extract information from context and candi-
date utterances for suitable utterance selection, our
NUR model utilizes an RNN encoder.

Previous work utilized an RNN encoder for MT
(Kalchbrenner and Blunsom, 2013; Bahdanau et
al., 2015) and response generation in dialogue sys-
tems (Cho et al., 2014; Sordoni et al., 2015; Shang
et al., 2015; Vinyals and Le, 2015). In these stud-
ies, the encoder reads as input a variable-length
word sequence and outputs a fixed-length vec-
tor. Next, another RNN decodes a given fixed-
length vector, producing an objective variable-
length word sequence. Therefore, the encoder has
learned to embed necessary information to gener-
ate objective sentences and place them into vec-
tors. The RNN in our model does not generate
sentences using this RNN decoder approach. Re-
sults of encoding are used for features to rank can-
didate utterances. The RNN encoder in our NUR
model has a similar architecture, but the character-
istics of the output vector are profoundly different,
because our model learns to extract important fea-
tures for utterance ranking.

Our model first converts word sequence w =
(w1, w2, . . . , wn) in an utterance into a distributed
representation of word sequence, i.e., x =
(x1, x2, . . . , xn) which the RNN encoder then
reads. To convert into a distributed representation
here, a neural network for word embedding (as
shown in Figure 1) learns via the skip-gram model
(Mikolov et al., 2013). This network has two lay-
ers, i.e., an input layer that reads a one-hot-vector
representing each word and a certain denomina-
tional hidden layer.

The RNN encoder has two networks, i.e., a for-
ward and a backward network. The forward RNN
reads x at the beginning of a sentence and out-
puts

−→
h = (

−→
h1,
−→
h2, . . . ,

−→
hn) correspond to input

sequence. The backward RNN reads x in reverse,
then outputs

←−
h = (

←−
h1,
←−
h2, . . . ,

←−
hn). By joining

the outputs of these forward and backward RNNs,
we acquire objective encoded utterance vector v =
[
−→
hn;
←−
hn]; note that [x; y]the concatenation of vec-

tors x and y.
In the following experiments, we used two-

layer long short-term memory (LSTM) (Hochre-
iter and Schmidhuber, 1997) networks as our RNN
encoders. The effective features extracted from
utterances for candidate ranking are different be-
tween the user and the system. Therefore, our

Figure 2: RNN for ranking utterances

NUR model has two RNN encoders, one for user
utterances, the other for system utterances, as il-
lustrated in Figure 1

3.2 Ranking Candidate Utterances
Another RNN is used to rank candidate utterances,
as illustrated in 2. This RNN has two LSTM layers
and two linear layers; further, we use rectified lin-
ear unit (ReLU) as the activation function. Thus,
this RNN reads encoded utterance sequences and
outputs scores.

3.2.1 Context-Candidate Vector Sequence
To select suitable responses, we not only must
evaluate suitability of utterances based on the last
utterance in the given context, but also must con-
sider prior dialogue. The RNN for ranking utter-
ances in our model reads vector sequences con-
structed by context and candidate utterances in
chronological order, then outputs scores for the
candidate in relation to the context.

Thus, context-candidate vector sequence vcai is
constructed using context vector sequence vc =
(vu1 , vu2 , . . . , vul), with ith candidate utterance
vector vai defined as follows:

vcai =


([vu1 ; vu2 ], [vu3 ; vu4 ], . . . , [vul ; v

c
ai ]),

if l is odd
([0; vu1 ], [vu2 ; vu3 ], . . . , [vul ; v

c
ai ]),

if l is even

Here, 0 denotes the zero vector. Our model inputs
user and system utterances at one time so that it
can consider dialogue history in a given context
along with the relevance between candidate utter-
ances and the last response given by a user.

3.2.2 Loss Function in Learning
In cases where a neural network outputs a one-
dimensional value, like our model, the mean

395



squared error (MSE) between training data and
the model’s output is generally used as a loss
function; however, our objective is not to model
scores, but rather for ranking, thus we use the dis-
tance between rank data based on training data and
that based on the model’s outputs as a loss func-
tion. Several methods for modeling rank data have
been proposed, including the Bradley-Terry-Luce
model (Bradley and Terry, 1952; Luce, 1959), the
Mallows model (Mallows, 1957) and the Plackett-
Luce model (Plackett, 1975; Luce, 1959). In our
study, to calculate ranking distance, we selected
the Plackett-Luce model, which has been used in
various ranking models, such as ListNet (Cao et
al., 2007), BayesRank (Kuo et al., 2009), etc.

The Plackett-Luce model transforms a score list
for ranking into a probability distribution wherein
higher scores in the given list are allocated higher
probabilities. Probability of score tci in score list
tc = (tc1, t

c
2, . . . , t

c
m) ranked on the top is calcu-

lated by the Plackett-Luce model as follows:

p(tci ) =
exp(tci )∑m

k=1 exp(t
c
k)

Using the same equation, the output scores of our
NUR model are transformed into probability dis-
tributions. We use cross-entropy between proba-
bility distributions as our loss function.

4 Experiments

We conducted experiments to verify the perfor-
mance of ranking given candidate utterances and
given contexts. For comparison, we also tested a
few baseline methods.

4.1 Datasets
For our experiments, we used dialogue data be-
tween a conversational dialogue system and a user
for both training and test data. We released a
conversational dialogue system called KELDIC
on Twitter (screen name: @KELDIC)b. KELDIC
selects an appropriate response from candidates
extracted by the utterance acquisition method of
(Inaba et al., 2014) using ListNet(Cao et al.,
2007). The utterance acquisition method extracted
suitable sentences for system utterances related
to given keywords from Twitter data by filter-
ing inappropriate sentences. Details of the re-
sponse algorithm of KELDIC is further described
in (Koshinda et al., 2015).

bhttps://twitter.com/KELDIC

We collected training and test data by first col-
lecting pairs of context and candidate utterances
that the system used for reply on Twitter. Next, an-
notators evaluated the suitability of each candidate
utterance in relation to the given context. Here an-
notators must evaluate utterances that were actu-
ally used by the system on Twitter.

Evaluation criterion was based on the Dia-
logue Breakdown Detection Challenge (DBDC)
(Higashinaka et al., 2016). Each system’s utter-
ances were annotated using one of the following
three breakdown labels:

(NB) Not a breakdown It is easy to continue the
conversation.

(PB) Possible breakdown It is difficult to con-
tinue the conversation smoothly.

(B) Breakdown It is difficult to continue the con-
versation.

Annotators evaluated dialogue data on a tool we
prepared. They were first shown a context and 10
candidate utterances, including how KELDIC ac-
tually replied on Twitter, as well as labels for each
candidate. We instructed them to assign at least
one NB label to given candidate utterances. If
there were no suitable candidates for the NB label,
they could optionally add candidate utterances. If
they were still not able to find a suitable response,
we allowed them to skip the evaluation. We re-
cruited annotators on crowd-sourcing site Crowd-
Worksc.

In our evaluation, we regard candidates with
50% or more annotators decided as NB as correct
utterances and others as incorrect.

We used 1581 data points (i.e., 1581 contexts
and 17533 candidate utterances), each evaluated
by three or more annotators. We choose 300 data
points that contained at least one correct candidate
for the given test data; the remaining 1057 data
points were used for training data. Table 1 shows
statistics for our data.

In learning the model, we need scores for can-
didate utterances to define ranking. Score yci of
candidate utterance aci is calculated as follows:

yci = sNB
nNB
N

+ sPB
nPB
N

+ sB
nB
N

N = nNB + nPB + nB
chttps://crowdworks.jp

396



Table 1: Statistics of the datasets
Train Test All

Data 1281 300 1581
Utterances in context 1.67 2.04 1.74
Candidates per data 11.12 10.94 11.09
Words per candidate 11.17 10.70 11.08
Num of Annotators 3.97 3.88 3.95

Here, nNB, nPB and nB denote the numbers of
annotators assigned as NB, PB and B, respectively,
and sNB, sPB and sB denote scoring parameters of
NB, PB and B, respectively. In our experiments,
we set (sNB, sPB, sB) = (10.0,−5.0,−10.0).
4.2 Experimental Settings
In the word-embedding neural network of our
NUR model, we used 1000 embedding cells, a
skip-gram window size of five, and learned via
100GB of Twitter data (Other layers were learned
by 1281 data points).

In our encoding and ranking RNNs, we used
LSTM layers with 1000 hidden cells in each layer.
The dropout rate was set to 0.5, and the model was
trained via AdaGrad (Duchi et al., 2011).

To validate our NUR model, we conducted ex-
periments with the following two settings:.

Proposed using limited context 　
To verify the effectiveness of context se-
quence processing by the ranking RNN, this
setting causes our system to only use the last
user utterance as context, discarding the rest.

Proposed using MSE 　
To verify the effectiveness of the Plackett-
Luce model, this setting causes our system to
learn using the MSE of utterance scores in-
stead of the Plackett-Luce model.

We also compared performance to the following
three methods:

BoW + DNN 　
This method ranks candidate utterances using
deep neural networks (DNNs) and bag-of-
words (BoW) features. The DNN consisted
of six layers, excluding input and output lay-
ers optimized by MSE. The input vector is
made by concatenating three BoW vectors,
i.e., candidate utterance, last user utterance in
the given context, and the given context with-
out the last user utterance. In the BoW vector,

Figure 3: MAP over top n candidates

we used 6203 words that occur at least two
times in the training data, thus, the input layer
of the DNN has 18609 cells. Each hidden
layer has 5000 cells, with ReLU as the activa-
tion function, the dropout rate set to 0.5, and
the model trained by AdaGrad (Duchi et al.,
2011). The score for training was the same as
the model proposed in Section 4.1.

KELDIC 　
The second comparative approach used the
output of our KELDIC system. This dialogue
system ranks utterances using ListNet (Cao
et al., 2007) and selects the top-ranked utter-
ance to reply. The feature vector for ranking
is generated from context and candidate ut-
terance. It primarily utilizes n-gram pairs be-
tween utterances in context and candidates as
features.

Random 　
This approach randomly shuffles candidates
and uses them as a ranking list, thus serving
as a baseline for ranking performance.

4.3 Results
To evaluate ranking performance, we used mean
average precision (MAP) and mean reciprocal
rank (MRR) measures. Figure 3 shows MAP re-
sults over the top n ranked candidate utterances,
while Figure 4 shows MRR results. Using the
MAP measure, our proposed method showed the
highest performance as compared to the other
methods. The proposed using limited context and
MSE follow this, suggesting that utterance encod-
ing by RNN is effective to extract features for
ranking.

397



Figure 4: Mean Reciprocal Rank

BoW + DNN did not provide strong perfor-
mance results, because it could not handle the
order relation of utterances in context and syn-
tax due to the use of BoW features. KELDIC
showed higher performance than that of Random,
but lower than that of BoW + DNN, because it also
has the problem of context processing and its gen-
eralization capability is lower than that of DNNs.

Here, n = 1 of MAP indicates that the rate of
correct utterance ranked on the top (The maximum
value of n = 1 of MAP is 1.0 because each data
points in the test data contains at least one correct
candidate utterance). Since the top-ranked utter-
ance is selected as a response in dialogue systems,
it was found that our proposed method correctly
replied with a probability of approximately 60%.

Results of MRR (i.e., Figure 4) showed very
similar results, i.e., our proposed method ranked
suitable utterances higher.

Table 2 shows an example of context in the test
data and Table 3 shows candidate utterances to the
context shown in Table 2, plus ranking results for
the applied methods and NB rates of annotations
for candidates. These results indicate that our
proposed method ranked correct utterances higher
and incorrect utterances lower, as desired.

5 Dialogue Experiment

In the previous section, since test data must con-
tain correct candidate utterances, the ability of our
NUR model in terms of actual dialogue is uncer-
tain, thus we developed a conversational dialogue
system based on our proposed method and con-
ducted dialogue experiments with human subjects.

The dialogue format and rules were fully
compliant with the Dialogue Breakdown Detec-
tion Challenge (DBDC) (see (Higashinaka et al.,
2015a)). A dialogue is started by a system utter-
ance, then user and the system communicate with

one another. When a system speaks 11 times, the
dialogue is finished. Therefore, a dialogue con-
tains 11 system and 10 human utterances.

Our dialogue system and subjects chat on our
website; we collected 120 text chat dialogues. An-
notators then labeled 1200 system utterances (ex-
cluding initial greetings) using breakdown labels
NB, PB, and B. We again recruited subjects and
annotators via CrowdWorks.

For comparison, we used DBDC develop-
ment/test datad collected by chatting with a system
based on NTT Docomo’s chat APIe (see (Onishi
and Yoshimura, 2014) . Since the DBDC system
selects a suitable response from large-scale utter-
ance data, the architecture is similar to our model
and therefore suitable as a comparative system.

DBDC data has been annotated by 30 annota-
tors using the breakdown labels and we use them
without any change in this experiment. Therefore,
the annotation rule is same but the annotators are
different between our dialogue data and DBDC
data.

5.1 Dialogue System
A conversational dialogue system based on our
NUR model selects an utterance as a response
from candidates generated by the acquisition
method of (Inaba et al., 2014). The system ex-
tracts nouns from the last user and system ut-
terances, generating candidate utterances related
to nouns. We used approximately one billion
Japanese tweets collected from January through
February 2015 for utterance acquisition. Our NUR
model ranked candidates, and the system used top-
ranked utterances as responses. If there were less
than five acquired utterances, the system retroac-
tively extracted nouns in context one by one to ac-
quire further candidates.

The first utterance in the beginning of a dialogue
was randomly selected from 16 manually created
open question utterances, such as “What is your
favorite website?” or “What kind of part-time job
do you have?”. If the user’s response does not
contain any nouns or the number of acquired utter-
ances is less than five, the system randomly selects
the 16 utterances again.

5.2 Results
Table 4 shows statistics of the data, annotations,
and experimental results. Dialogue data used in

dhttps://sites.google.com/site/dialoguebreakdowndetection/
ehttps://www.nttdocomo.co.jp/service/

398



Table 2: Example context (translated by authors)
Speaker Utterance in context

User 大学行くモチベーションが見た目に如実に表れてる
My motivation to attend college is cosmetically significant.

System わかります。一夜が明け、仕事へのモチベーションはまずまずであるものの
テンションは非常に低いよね
I see. It’s dawn and although I’m downhearted now, my motivation for working is not bad.

User わかる
Sure.

System だよね！普段のテンションが高い分、落ち込んだらとことん落ち込むよ
Indeed! I have high motivation on an average day, but once I get depressed,
things become increasingly worse.

User そうなんだ。落ち込むな元気出せ！
Oh, don’t be so depressed and cheer up!

Table 3: Example candidate utterances with ranking results (translated by authors)
Proposed BoW + DNN KELDIC NB Candidate Utterance

1 6 3 1.00 相当なモチベーションが必要だよ
It requires a considerable degree of motivation.

2 8 6 0.33 独学はモチベーションを保つのが大変だよ
Self-education is difficult to keep me motivated.

3 10 2 0.00 どんなモチベーションでチャリこげばいいよね
What is my motivation to pedal a bicycle?

4 1 8 0.33 勉強へのモチベーションがすごい
My motivation to study is quite good.

5 9 4 0.33 モチベーションには繋がるよ
It’s to be a motivation.

6 4 9 0.00 ポケモンのモチベーションが皆無だよ
I have no motivation to play Pokemon.

7 7 10 0.00 実習のモチベーション保つのって大変だね
It’s hard to stay motivated in practical training.

8 3 5 0.00 東方のモチベーションがすごくなってるよ
0 My motivation to play Touhou games is quite high.

9 5 1 0.00 PCに対するモチベーション低いしやる気でない
My motivation to use a PC is low, and I don’t feel
like doing anything.

10 2 7 0.00 モチベーション低い幹事は良くない
An organizer who has low motivation is bad.

our system were annotated by 34 human annota-
tors. Fleiss’s K measure for our system’s data was
lower than that of the DBDC dataset, but both are
low. “PB + B” indicates that PB and B are treated
as a single label. The table also shows the ratio
of NB, PB, and B labels. These annotation results
indicate that output probabilities of PB and B ut-
terances by our system were significantly lower,
while NB was higher than that of the DBDC sys-
tem (p < 0.01).

The Breakdown ratio (B) and (PB + B) values
are calculated by the labels of majority vote in 34
(proposed) or 30 (DBDC) annotators in each sys-
tem’s utterance. Breakdown ratio (B) is the ra-
tio of the B majority label to all majority labels.
Breakdown ratio (PB + B) is the ratio of PB and B
majority labels (treated as a single label). This in-
dicates that our system can offer a response that
does not provoke a critical dialogue breakdown
with a probability of approximately 90% and a

399



Table 4: Statistics of the data and experimental re-
sults (U and S indicate statistics of user and system
utterances, respectively)

Proposed DBDC
Dialogues 120 100
Utterances (U) 1200 1000
Utterances (S) 1320 1100
Words per utterance (U) 9.32 9.43
Words per utterance (S) 8.63 7.17
Vocabularies (U) 1684f 1491
Vocabularies (S) 1386f 1218
Annotators 34 30
NB (Not a breakdown) 57.7% 37.1%
PB (Possible breakdown) 27.0% 32.2%
B (Breakdown) 15.2 % 30.6%
Fleiss’s κ (NB, PB, B) 0.26 0.20
Fleiss’s κ (NB, PB+B) 0.37 0.27
Breakdown ratio (B) 0.08 0.25
Breakdown ratio (PB+B) 0.42 0.71

very natural response with a probability of 60%.
Both breakdown ratios showed significant differ-
ences between our system and the DBDC system
(p < 0.001).

Table 4 also shows the number of words per ut-
terance and the number of vocabularies. These re-
sults are important for system evaluation, because
if a system always use innocuous responses, such
as “I don’t know” or “That’s true”, it is relatively
easy to avoid dialogue breakdown. By these val-
ues, we can find whether a system frequently uses
innocuous responses or not; however, to increase
user satisfaction with a dialogue system, it is im-
portant not only to avoid dialogue breakdown, but
also to offer flexible replies. From Table 4, we
also observe that the number of words per utter-
ance and the number of vocabularies in our system
were bigger than that of the DBDC system, indi-
cating that our system infrequently used innocu-
ous responses and had a good vocabulary for gen-
erating responses. Indeed, our system rarely used
such utterances, but the DBDC system sometimes
used them.

The number of words per utterance by user be-
tween both datasets was almost the same, but the
number of vocabularies by user of the DBDC sys-
tem was lower than that of our system. This
was attributable to the DBDC system’s utterances
that increased the incident of dialogue breakdown.

fcalculated using 100 dialogues

When the DBDC system uses such utterances, the
user responds with formulaic responses, such as
“What do you mean?”. Since the DBDC sys-
tem frequently caused dialogue breakdowns, users
used formulaic replies, and as a result, the number
of vocabularies decreased.

6 Conclusions

In this study, we proposed a new utterance selec-
tion method called the NUR model for conversa-
tional dialogue systems. Our model ranks candi-
date utterances by their suitability in given con-
texts using neural networks. Our proposed model
encodes utterances in context and candidates into
fixed-length vectors, then processes these encoded
vectors in chronological order to rank utterances.
Experimental results showed that our proposed
model ranked utterances more accurately than that
of deep learning and other existing methods. In
addition, we constructed a conversational dialogue
system based on our proposed method and con-
ducted experiments to evaluate its performance via
dialogue with human subjects. By comparing the
dialogue system of DBDC, we found our system
able to conduct conversations more naturally than
DBDC.

The dialogue system used in the experiment ac-
quired topic words from given context in a sim-
ple manner. Because of this, there are some cases
that the system selects inappropriate topics and
fails in changing topics. Thus, future work in-
cludes topic management. Moreover, the sys-
tem is unskilled at answering questions, and it of-
ten provokes dialogue breakdown. It requires a
question-answering method corresponding to con-
versational dialogue systems.

Acknowledgements

This study received a grant of JSPS Grants-in-aid
for Scientific Research 16H05880 and a research
grant from Kayamori Foundation of Informational
Science Advancement.

References
Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Ben-

gio. 2015. Neural machine translation by jointly
learning to align and translate. Proc. ICLR.

Rafael E Banchs and Haizhou Li. 2012. Iris: a chat-
oriented dialogue system based on the vector space
model. In Proceedings of the ACL 2012, pages 37–
42. Association for Computational Linguistics.

400



Ralph Allan Bradley and Milton E Terry. 1952. Rank
analysis of incomplete block designs: I. the method
of paired comparisons. Biometrika, 39(3/4):324–
345.

Z. Cao, T. Qin, T.Y. Liu, M.F. Tsai, and H. Li. 2007.
Learning to rank: from pairwise approach to listwise
approach. In Proceedings of the 24th international
conference on Machine learning, pages 129–136.

Kyunghyun Cho, Bart van Merrienboer, Caglar Gul-
cehre, Dzmitry Bahdanau, Fethi Bougares, Holger
Schwenk, and Yoshua Bengio. 2014. Learning
phrase representations using rnn encoder-decoder
for statistical machine translation. Proceedings of
the 2014 Conference on Empirical Methods in Nat-
ural Language Processing (EMNLP), pages 1724–
1734.

K.M. Colby. 1981. Modeling a paranoid mind. Behav-
ioral and Brain Sciences, 4(4):515–560.

Antonella De Angeli and Rollo Carpenter. 2005.
Stupid computer! abuse and social identities.
In Proceedings of the INTERACT 2005 workshop
Abuse: The darker side of Human-Computer Inter-
action.

John Duchi, Elad Hazan, and Yoram Singer. 2011.
Adaptive subgradient methods for online learning
and stochastic optimization. The Journal of Ma-
chine Learning Research, 12:2121–2159.

Ryuichiro Higashinaka, Nozomi Kobayashi, Toru Hi-
rano, Chiaki Miyazaki, Toyomi Meguro, Toshiro
Makino, and Yoshihiro Matsuo. 2014. Syntactic
filtering and content-based retrieval of twitter sen-
tences for the generation of system utterances in di-
alogue systems. Proc. IWSDS, pages 113–123.

Ryuichiro Higashinaka, Kotaro Funakoshi, Masahiro
Araki, Hiroshi Tsukahara, Yuka Kobayashi, and
Masahiro Mizukami. 2015a. Towards taxonomy of
errors in chat-oriented dialogue systems. In 16th An-
nual Meeting of the Special Interest Group on Dis-
course and Dialogue, pages 87–95.

Ryuichiro Higashinaka, Toyomi Meguro, Hiroaki
Sugiyama, Toshiro Makino, and Yoshihiro Matsuo.
2015b. On the difficulty of improving hand-crafted
rules in chat-oriented dialogue systems. In 2015
Asia-Pacific Signal and Information Processing As-
sociation Annual Summit and Conference (APSIPA),
pages 1014–1018.

Ryuichiro Higashinaka, Kotaro Funakoshi, Kobayashi
Yuka, and Michimasa Inaba. 2016. The dialogue
breakdown detection challenge: Task description,
datasets, and evaluation metrics. In 10th edition
of the Language Resources and Evaluation Confer-
ence.

Sepp Hochreiter and Jürgen Schmidhuber. 1997.
Long short-term memory. Neural computation,
9(8):1735–1780.

Michimasa Inaba, Sayaka Kamizono, and Kenichi
Takahashi. 2014. Candidate utterance acquisition
method for non-task-oriented dialogue systems from
twitter. Transactions of the Japanese Society for Ar-
tificial Intelligence, 29(1):21–31.

Nal Kalchbrenner and Phil Blunsom. 2013. Recur-
rent continuous translation models. Proc. EMNLP,
3(39):413.

Makoto Koshinda, Michimasa Inaba, and Kenichi
Takahashi. 2015. Machine-learned ranking based
non-task-oriented dialogue agent using twitter data.
In 2015 IEEE/WIC/ACM International Conference
on Web Intelligence and Intelligent Agent Technol-
ogy (WI-IAT), volume 3, pages 5–8.

Jen-Wei Kuo, Pu-Jen Cheng, and Hsin-Min Wang.
2009. Learning to rank from bayesian decision in-
ference. In Proceedings of the 18th ACM conference
on Information and knowledge management, pages
827–836.

Jiwei Li, Michel Galley, Chris Brockett, Jianfeng Gao,
and Bill Dolan. 2016. A diversity-promoting objec-
tive function for neural conversation models. Pro-
ceedings of the NAACL-HLT 2016.

R.D. Luce. 1959. Individual choice behavior: A theo-
retical analysis. Wiley, New York.

Colin L Mallows. 1957. Non-null ranking models. i.
Biometrika, 44(1/2):114–130.

Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Cor-
rado, and Jeff Dean. 2013. Distributed representa-
tions of words and phrases and their compositional-
ity. In Advances in neural information processing
systems, pages 3111–3119.

Hiroya Murao, Nobuo Kawaguchi, Shigeki Matsubara,
Yukiko Yamaguchi, and Yasuyoshi Inagaki. 2003.
Example-based spoken dialogue system using woz
system log. In 4th SIGdial Workshop on Discourse
and Dialogue, pages 140–148.

Lasguido Nio, Sakriani Sakti, Graham Neubig, Toda
Tomoki, and Satoshi Nakamura. 2014. Utilizing
human-to-human conversation examples for a multi
domain chat-oriented dialog system. IEICE TRANS-
ACTIONS on Information and Systems, 97(6):1497–
1505.

Kanako Onishi and Takeshi Yoshimura. 2014. Ca-
sual conversation technology achieving natural di-
alog with computers. NTT DOCOMO Technical
Jouranl, 15(4):16–21.

RL Plackett. 1975. The analysis of permutations. Ap-
plied Statistics, pages 193–202.

Alan Ritter, Colin Cherry, and William B Dolan. 2011.
Data-driven response generation in social media. In
Proceedings of the conference on empirical methods
in natural language processing, pages 583–593.

401



Lifeng Shang, Zhengdong Lu, and Hang Li. 2015.
Neural Responding Machine for Short Text Conver-
sation. Proceedings of the 53th Annual Meeting of
Association for Computational Linguistics and the
7th International Joint Conference on Natural Lan-
guage Processing, pages 1577–1586.

Masahiro Shibata, Tomomi Nishiguchi, and Yoichi
Tomiura. 2009. Dialog system for open-ended con-
versation using web documents. Informatica, 33(3).

Alessandro Sordoni, Michel Galley, Michael Auli,
Chris Brockett, Yangfeng Ji, Margaret Mitchell,
Jian-Yun Nie, Jianfeng Gao, and Bill Dolan. 2015.
A neural network approach to context-sensitive gen-
eration of conversational responses. In Proceedings
of the NAACL-HLT 2015, pages 196–205.

Hiroaki Sugiyama, Toyomi Meguro, Ryuichiro Hi-
gashinaka, and Yasuhiro Minami. 2013. Open-
domain utterance generation for conversational di-
alogue systems using web-scale dependency struc-
tures. In Proc. SIGDIAL, pages 334–338.

Oriol Vinyals and Quoc Le. 2015. A neural conver-
sational model. In Proceedings of the ICML Deep
Learning Workshop, pages 1–7.

R.S. Wallace. 2008. The anatomy of A.L.I.C.E. Pars-
ing the Turing Test, pages 181–210.

J. Weizenbaum. 1966. ELIZA-a computer program
for the study of natural language communication be-
tween man and machine. Communications of the
ACM, 9(1):36–45.

A Dialogue Example

Table 5 shows example dialogue between our
NUR-based dialogue system and a user. In the
table, NB, PB, and B indicate respective rates of
each label by 34 annotators.

402



Table 5: Examples of dialogues by our system (translated by authors)
Speaker NB PB B Utterance
System - - - こんにちは。最近はどんな感じ？

Hello. How have you been recently?
User - - - 鼻炎がひどくてたいへんだよ。

My sinus is really bothering me.
System 100.0% 0.0% 0.0% 鼻炎の人って辛すぎるよ

Sinus victims feel terrible.
User - - - そうなんだよ、くしゃみと鼻水が止まらなんだ。

That’s right. I can’t stop sneezing and have a runny nose.
System 100.0% 0.0% 0.0% くしゃみする時の顔ってやばすぎるよ

A sneezing face looks so terrible.
User - - - なので、マスクは必須アイテムです。

So a mask is an indispensable item
System 85.3% 14.7% 0.0% くしゃみの音が大きいのが悩みだよ

My loud sound of sneezing is more worrying.
User - - - たしかに、マスクじゃ音は止められないね。

I agree that the sound can’t be stopped by a mask.
System 94.1% 5.9% 0.0% くしゃみやばいのは結構辛い

Sneezing is bad for my health
User - - - 鼻炎の薬飲んでみたらどうですか？

Why don’t you try sinus medicine?
System 58.8% 35.3% 5.9% 鼻炎が酷すぎて辛い

My sinus is too terrible.
User - - - そうだよね、ティッシュペーパーがいくらあっても足りな

いよね。
Sure. You never have enough tissue paper, don’t you?

System 61.8% 32.4% 5.9% お腹に不安がありすぎて迂闊に咳やくしゃみも出来ねえ
Because of my concerns about my stomach, I can’t cough and
sneeze.

User - - - やっぱり、医者で鼻炎のおクスリを貰ったらどうですか？
Ideally, why don’t you get medicine from a hospital?

System 58.8% 23.5% 17.6% 風邪っぽいのでクスリ飲むよ
I seem to have a cold, so I’ll take medicine.

User - - - そりゃあ、早く飲んで栄養つけて休んだ方がいいよ。
You should take that, get nutrition and get some rest.

System 73.5% 26.5% 0.0% お腹も弱いんだよね
I also have a weak stomach.

User - - - じゃあ、おかゆでも食べる？
Then would you have rice gruel?

System 91.2% 8.8% 0.0% 味がついたおかゆはおいしい
Flavored rice gruel is delicious.

User - - - はい、たまごを混ぜてお塩で味付けした粥です。
Yes, this rice gruel is flavored with egg and salt.

System 20.6% 44.1% 35.3% 照り焼きチキンとたまごのサンドイッチはおいしい
Chicken teriyaki and egg sandwich is delicious.

403


