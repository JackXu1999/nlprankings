















































Using Text Reviews for Product Entity Completion


Proceedings of the 5th International Joint Conference on Natural Language Processing, pages 983–991,
Chiang Mai, Thailand, November 8 – 13, 2011. c©2011 AFNLP

Using Text Reviews for Product Entity Completion

Mrinmaya Sachan Tanveer Faruquie L. V. Subramaniam Mukesh K. Mohania
IBM Research India

New Delhi, India
{mrsachan, ftanveer, lvsubram, mkmukesh}@in.ibm.com

Abstract

In this paper we address the problem
of obtaining structured information about
products in the form of attribute-value
pairs by leveraging a combination of enter-
prise internal product descriptions and ex-
ternal data. Product descriptions are short
text strings used internally within enter-
prises to describe a product. These strings
usually comprise of the Brand name, name
of the product, and its attributes like size,
color, etc. Existing product data quality
solutions provide us the capability to stan-
dardize and segment these descriptions
into their composing attributes using do-
main specific rulesets. We provide tech-
niques that can leverage the supervision
provided by these existing rulesets for ex-
tracting missing values from other exter-
nal text data sources accurately. We use
a large real life data collection to demon-
strate the effectiveness of our approach.

1 Introduction

Enterprises usually store information of its prod-
ucts in the form of unstructured text strings. Such
product descriptions contain the name of the prod-
uct and its specific attributes. These product de-
scriptions are usually written by multiple people
and could contain overlapping information or even
the same information written differently. For ex-
ample, a superstore may source the same product
from many different vendors and each vendor may
give varying descriptions of the same product. In-
formation could be scattered through various de-
partments and held by certain employees or sys-
tems instead of being available centrally. This re-
sults into varying standards and vocabulary.

Consider the following product descriptions ob-
tained from an enterprise selling cameras. They

are provided by different vendors supplying the
cameras to the enterprise:
Nikon D90 4288×2848 703 g Digicam F/1.8
Nikon Digital 90 Cam 12.3MP 1.8 F-Len(1.55lb)
Nikon D-90 Camera with Nikkor 50mm 1.8D
Expert knowledge specific to the domain (that
D90, D-90 and Digital 90, 4288 × 2848 and 12.3
MP, F-number 1.8 and 1.8 D focal length, and 703
g and 1.55lbs are same) is required to conclude
that the entries above are the same product. Cou-
pled with data entry errors, the problem of identi-
fying a standard representation of the product be-
comes even harder.

The problem of obtaining a structured represen-
tation of such product descriptions is similar to the
‘Attribute-Value pair’ mining problem. Attribute
represents an aspect of the product. It could be
anything from a manufacturing detail like model
number to information like color, size and weight.

Due to its practical applications, the problem
has drawn interest from the research community
as well as the industry. There are many products
which provide solutions for standardizing, match-
ing, merging and validating such descriptions.
Popular ones include Oracle middleware, Silver-
creek, Ethoscontent product-copy, Trilliumsoft-
ware and IBM Data Stage-Quality Stage. Since
rules are easy to understand, manage and give
good accuracies in practice, they are widely used
by these solutions. Overall, the product data
cleansing solution is achieved by a collection of
rule sets, each tackling a given product vertical. A
ruleset scales to descriptions within its vertical.

Often, the product descriptions are very brief
and do not convey the entire information about
the product. Critical information about the prod-
uct could be missing. Because of this, an enter-
prise does not have a complete view of its products
and services. Suppose an enterprise wants to have
manufacturer wise information about its products
for making important demand-supply decisions. If

983



the manufacturer information is erroneously cap-
tured or missing, it wouldn’t be possible. This
lack of information makes it difficult for a poten-
tial customer to compare products and make an in-
formed decision about it. At the same time, low
data quality impedes business. An enterprise may
loose its competitive edge due to poor customer
service and advertising resulting from incorrect re-
porting and incomplete view of its products and
services. Having a standard and complete struc-
tured representation of a product can help in con-
solidation and is useful in various business intelli-
gence applications. Given the purchasing history
of its customers, a better view of enterprise prod-
ucts would result into a host of benefits like better
targeted advertising, better recommender systems
and reduced maintenance effort.

At the same time, as more and more people
are beginning to write reviews, blogs and opin-
ions about their experiences in using products, it
is possible to obtain a lot of information about the
products on the web. Popular merchant sites like
Ebay and Amazon contain a large number of prod-
uct reviews from its customers. These reviews not
only contain reviewer sentiments but also contain
key information which can be used to create an
enriched view of the products. Our goal is to ob-
tain a complete view of the products by extracting
attribute values from such sources.

Here we note that existing rulesets used by data
cleansing solutions can give us critically important
supervision to drive the attribute-value extraction.
This would not only help us in achieving greater
accuracy but also allow us to extract true product
values. Infact, a key differentiator that puts this
work apart from its peers is that the supervision
provided by the rules allows us to extract attribute
‘values’ in the real sense and not merely sentiment
words as in most previous works.

Overall, the task of creating a complete view
of the products comprises of standardization of
appropriate features present in the product de-
scriptions along with enrichment using web data.
We carry out product description segmentation by
writing handcrafted rules on top of a domain spe-
cific dictionary. Then, we show that the same rules
can be reused on web data to fill-in missing val-
ues for many of the product attributes. Our pa-
per is organized as follows. Section 2 gives us the
necessary background and describes related work.
Then, Section 3 describes our approach in detail.

In Section 4, we report experimental results on real
datasets. Finally, Section 5 concludes.

2 Related Work and Background

There has been significant work in information ex-
traction from text data for products. In particular
the extraction of product attributes and user sen-
timents has received wide attention. One of the
methods (Hu and Liu, 2004) is to use frequent item
sets of nouns along with the opinion words to mine
infrequent product attributes. This method is fur-
ther improved (Zhuang et al., 2006) by using do-
main knowledge along with noun phrases for at-
tribute extraction. Another refinement (Qiu et al.,
2009) uses extraction rules based on different rela-
tions existing between opinion words and attribute
words. These relations are syntactic and are prop-
agated in an iterative manner.

Some approaches detect product attributes
along with opinion extraction. (Liu et al., 2005)
first detects attributes by using a rule miner to find
noun phrases. Further, it finds polarity descrip-
tors for these noun phrases. (Popescu and Etzioni,
2005) computes the point wise mutual information
between noun phrases and product class specific
discriminators to determine whether a noun phrase
is a product attribute. It finds part-whole patterns
by querying the web and uses a part-whole pattern
for attribute mining. It further finds the sentiments
of these attributes. In contrast, our work finds ac-
tual values for the attributes and not merely senti-
ments expressed by users.

An approach to finding attributes and senti-
ments jointly is to mine patterns of aspects-
evaluation (Kobayashi et al., 2007) using statisti-
cal and contextual cues. Here aspects are attributes
for a particular product and evaluations are the
opinions expressed. The significance of discov-
ered patterns are computed based on their statisti-
cal strength. (Wang and Wang, 2008) uses itera-
tive boot strapping to find opinion words from at-
tributes and then finding attributes from opinion
words in an alternating fashion. It uses mutual
information to measure association between them
and linguistic rules to identify infrequent attributes
and opinion words. In one of the most interest-
ing works of its kind, (Zhai et al., 2010) groups
domain synonyms to form feature groups using a
naive Bayesian EM formulation iteratively on the
labeled and unlabeled data. It leads to each unla-
beled example being assigned a posterior proba-

984



Table 1: Sample Segmented Product Descriptions
Description BrandName ProductName Product Lens Resolution Price
Fujifilm Quick Snap Single Use Camera Fujifilm Quick Snap Camera NULL NULL NULL
$25 cannon EF f/2.8 USM Lens Canon EF Lens f/2.8 USM NULL $ 25
Sony DSCP 8 3.2 MP Camra Sony DSCP 8 Camera NULL 3.2 MP NULL

bility of belonging to a group giving it the abil-
ity to find multigram attribute terms. However,
these methods concentrate on detecting product at-
tributes and do not find associated values.

Among machine learning approaches CRF
(Peng and McCallum, 2006) (Stoyanov and
Cardie, 2008) has been effective if we have train-
ing data and want to extract template attributes and
values. However, we do not always have a pre-
defined list of explicit attributes and values which
have to be extracted and populated. The method
in (Ghani et al., 2006) finds attributes and values
together using a naive bayes algorithm combined
with EM. This method requires labeled training
data which is often difficult and expensive to pro-
duce. Also, since both the attribute and value
extraction is automatic, the accuracy is low and
hence, not useful for most business applications.
Our method differs from it as we propose an auto-
matic approach to detect potential attributes and a
combination of rules and semi-supervised learning
to extract values. Our method also has the flexibil-
ity to detect values of pre-defined attributes.

3 Our Approach

We pose the problem of Product Entity Comple-
tion as two sub-problems: Product description
segmentation and Enrichment. The segmentation
problem simulates the product data standardiza-
tion done in the industry. It constructs a stan-
dard attribute value representation using product
descriptions found in the enterprise database. A
set of attributes {A1 . . . An} is decided by apply-
ing some initial domain knowledge. We are given
a set of product descriptions comprising of values
for one or more attributes. Our task is to segment
these descriptions and put each segment into one
of the n attribute bins. Since the descriptions are
incomplete, large number of attribute values are
null after the segmentation step. Enrichment task
leverages the supervision provided by the rules to
extract unknown or missing values from external
web data.

3.1 Product Description Segmentation

Enterprise descriptions of products are short
strings containing information about one or more
attributes like “Brand Name”, “Product Name”,
“Model Number”, “Manufacturer” and few other
product specific attributes. To attain a standard
view of products and to conform to organization-
wide specifications, product data cleansing solu-
tions are used to segment the descriptions into
these product attributes. Non-standard represen-
tations are converted to standard forms and mis-
spellings, etc are corrected. Some typical exam-
ples of product descriptions and their correspond-
ing standardized forms are shown in Table 1.

To begin with, the enterprise or a domain expert
ascertains the attributes comprising the descrip-
tions. To perform the task of moving free-form de-
scriptions into these pre-determined fixed attribute
columns, a dictionary classification for generic to-
kens like involved brand names and product names
is maintained from various intellectual property
organizations like UNSPSC1 and WIPO2, and
common metric units and currency symbols from
common knowledge. This dictionary of standards
is often stored in the form of rich taxonomies and
is fixed. Each token is assigned a classification
symbol depending on its type. To account for all
misspellings (“Camera”, “Camcorder” and “Cam-
recorder”), difference in vocabularies(“Oz” and
“Ounces”), classifications, synonyms and abbrevi-
ations, and other non-standard representations, the
native forms(like Camrecorder) are mapped to a
standard form (Camcorder) using signature clus-
tering techniques as described in (Prasad et al.,
2011). This is conveniently achieved by popu-
lar string similarity measures and by looking at
context of these native forms in the input de-
scriptions. Such data driven context mining ap-
proaches to find various ways in which similar
words (which often have the same classification
entry) like “Camera”, “Camcorder” and “Cam-
recorder”, and “LCD”, “CFD” and “TFT” help re-
duce the manual effort in rule writing and dictio-

1http://www.unspsc.org/
2http://www.wipo.int/portal/index.html.en

985



nary building. Despite this, suitable additions to
these dictionaries are sometimes needed from do-
main experts. Example classification for our run-
ning example is given in Table 2.

Table 2: Classification Entries
NativeForm StandardForm Classification Symbol
Canon CANON Brand B
Cannon CANON Brand B
Sony SONY Brand B
Fujifilm FUJIFILM Brand B
Quick Snap QUICK SNAP Product Name N
Camera CAMERA Product P
Camra CAMERA Product P
USM USM Lens Property L
$ $ Currency C
MP MEGAPIXEL Metric M
F F Alphabet A

Also, we generate symbols for each number or
unknown word to help us make use of the context
to write rules. The classifications lead to a pattern
of symbols for every product description entry.

For the following product description,
“Canon Powershot SD1200IS 10 MPIXEL Digital
Cam f/3.5− 6.3 8′′ LCD Screen $123”
the corresponding pattern would be:
“B +@#M + PA/#−##M ++C#”
Here, B is a brand name and P is a product
name recognized from one of the catalogues lying
with the enterprise or some intellectual property
database. M is a metric unit and C is a currency
symbol lying in the dictionary of metric standards
and currencies, respectively. Other symbols in-
clude + for an unknown word, # for a number
and @ for an alphanumeric. Finally, a domain ex-
pert writes rules to move entries into appropriate
database columns and complete the standardiza-
tion. Rules are written to process important sub-
patterns from the left or the right and capture at-
tribute values in one pass. Table 3 lists some hand
crafted segmentation rules to capture Resolution,
Focal Length and Price. People invest a great
amount of time, effort and money in building and
maintaining these rules for extracting information
from product descriptions. The output of these
rules are used to populate Data Warehouses and
Product Information Management (PIM) systems.
However, these rules are applied only to short
product descriptions which do not lead to a com-
plete view of the product. In the subsequent step,
we provides techniques that employ these rules to
discover missing values of existing attributes. Do-
ing so reuses the time and effort spent in building

Table 3: Rules for Segmentation Process
Subpattern Rulea

# | M=“MP” COPY [1][2] “Resolution”;
A=“F” | / | # | - | # COPY [1][2][3][4][5] “FocalLength”;

C | # COPY [1] “Currency”;
COPY [2] “Price”;

a[N] represents the N th token in the subpattern
“Resolution”, “FocalLength” and “Price” are Attribute columns
| represents a token separator

the rules on external content which otherwise is
very expensive and time consuming to build.

3.2 Missing Value Filling

We observe that the product descriptions after
standardization have missing values for many pre-
defined attributes (Nambiar et al., 2011). Next, to
obtaining a complete view of the product we ex-
tract these values from the product reviews avail-
able on the web. Our approach is general and can
be extended to other data sources like Sales and
Marketing data, Product Catalogs, Website Prod-
uct Listings and so on. Unlike many previous at-
tempts, we extract meaningful ‘values’ of interest-
ing attributes such as the shape, size and manufac-
turer’s name and not merely use sentiments. We
make use of the supervision provided by already
existing rulesets frequently used for standardiza-
tion to get a handle of such values. However as
the reviews are verbose and noisy, these values too
contain a lot of noise. Hence some text processing
heuristics are used to choose most appropriate val-
ues.

However, we should note that the rules are
meant for short product descriptions and not the
user reviews. Due to high degree of verbosity and
possibility of competing product talk in the re-
views, direct rule application on the reviews yields
many candidate values for each attribute. Hence,
we devise a strategy to prune inappropriate can-
didates. Our approach to prune out unimportant
candidates assesses the affinity of all the candidate
values with their corresponding product descrip-
tions and calculates a confidence level for each of
them. Confidence level intuitively measures the
likelihood that the candidate is a true value of the
attribute and product in question. For each prod-
uct and each of its attributes, the enterprise can
then choose the value with the highest confidence
(if its confidence is above a certain threshold).

986



Table 4: Sample Standardized Product Descriptions
Brand Name Product Name Product Focal Length Lens Diameter Price Weight

Canon EF Lens 2.8D 70-200mm - -

Table 5: Sample Product Reviews
... I bought a Canon 2.8D lens...certainly worth each of the 1369 bucks.....2.9 pounds is a bit heavier...
... new Nikon AF f/3.5-5.6G ... fair price deal of $ 685 ...

We compute the affinity for a candidate by look-
ing at the appearances of known attribute val-
ues(values obtained by segmentation of the prod-
uct description strings) of the product in its con-
text. The ‘known attribute values’ are assigned
certain weights and the confidence score for the
candidate is computed using the weights carried
by the ‘known attribute values’ in its context. As-
suming all weights to be equal, the following ex-
ample explains the idea in detail:
Consider the product description “Canon EF 70-
200mm f/2.8D Lens” and its corresponding seg-
mented output in table 4. Here - represents the
missing values to be filled using the reviews. Con-
sider the two reviews given in table 5. Out of
the candidates “1369 bucks ” and “$ 685” for the
price attribute, “1369 bucks” is chosen as it con-
tains more known attribute values (Canon, 2.8D,
lens) in its context in review 1. On the other hand
as “$ 685” has many competing attribute values
(which do not match the known values) in its con-
text (Nikon, AF, f/3.5-5.6G), it is rejected. Sim-
ilarly, “2.9 pounds” being the only candidate for
the weight attribute is accepted due to the presence
of many matching known values in its context.

In the above example, we simply counted the
number of known attribute values in each candi-
date’s context to compute its confidence score. All
the known values carried equal weights.

However, certain values occur much more
rarely than others in free text. Hence, matching
a rarer value in a candidate’s context should give
us more confidence that the review author is
talking about the same product (as in the product
description string) than one which occurs more
frequently. For example, matching a value Focal
length ’18-55 mm’ (which occurs rarely) instills
more confidence than if the value color ’black’
(a value which should occurs very frequently)
is matched. We quantify this idea by assigning
an ‘importance’ measure to all known values.
The importance I(v) of a known value v can
be decided by the proportion of distinct product

entities in the database(output of the standardiza-
tion phase) that contain the value v. Since we
wish to weigh rarer values more as they signify
more importance, we use the inverse document
frequency(IDF) formulation popularly used in
previous text mining works to define:

I(v) =

{
log( Nn(v)) if n(v) >0

0 otherwise

where, N is the total number of distinct entities
for the attribute in the database, and n(v) is the
number of distinct product entities that contain the
value v.

We also note that matching the value for a cer-
tain attribute can signify greater confidence than
others. For instance, if the “Product:Camera” or
“Brand:Nikon” matches we still cannot be very
sure because the author can be comparing two dif-
ferent cameras or two Nikon products. However
if a value mentioning weight or lens of the camera
matches, we can be more certain as it is unlikely
to have two cameras with exactly the same weight
or the same lens.

With this intuition in mind, we also give dif-
ferent weights to each ‘attribute’ in the standard-
ization schema for matching. These weights in-
tuitively signify our confidence on a value of the
given attribute towards matching. In our running
example, intuitively we should give more weight
to ‘Weight’ or ‘Lens’ attributes than ‘Product’
and‘Brand’. Weight of the attributes can be ex-
pressed by the domain expert during the schema
decision process. Please note that this schema de-
cision and weight assignment needs to be done
only once per product vertical. For the matching
to be effective, all real numbers were morphed to a
common representation ‘#’ and misspellings were
corrected for terms known in the dictionary.

Finally, these weights and importance measures
are tied together to estimate the confidence for
each candidate value. We introduce a distance
metric to quantify the distance between two words
in a review. The effect of a value on the confi-

987



dence of a candidate dies off with its distance from
the candidate. A formal description of the idea is
given below.

Given: Attribute schema A={A1 . . . AN}, set of
products P and set of reviews Rp for each p ∈ P ,

R =
⋃

p∈P
Rp

Let Ai(p) be the value of attribute Ai for the
product p ∈ P from the segmentation step. Let
Ip(Ai, r) be the set of index positions at which
Ai(p) occurs in the review r ∈ Rp. Define:

Bp(Ai) =

{
1 if Ai(p) is known
0 otherwise

Let attribute value Aj be missing for some product
p after the segmentation step i.e. Bp(Aj) = 0
for some j ∈ {1 . . . n}. Given a set of candidates
Cp(Aj , r) (obtained by applying rules on a prod-
uct review r ∈ Rp) for attribute Aj and product p,
we define:

Qp(c, r) =

n∑

x=1


Bp (Ax)W (Ax)

∑

i∈Ip(Ax,r)

I(i)dr(i, c)




∀c ∈ Cp(Aj , r)

where dr is a distance metric defined over every
pair of words in a given review r.
In similar lines to the idea presented so
far, occurrence of a value that contests a
value already known from the standardization
stage(segmentation output of the product descrip-
tion) can be used as an indicator that the author is
talking of some other product or attribute. Hence,
we have a case to reject candidates in its vicin-
ity. Also, often people compare two products or
brands while writing a review. Use of comparative
adjective forms or coordinating conjunctions like
“but, whereas, while, although, etc.” that express
a contrast mark such cases. To incorporate both of
the above ideas, we can easily extend the objec-
tive function Qp to account for these contingen-
cies by adding terms to the summation that reduce
the score of a candidate if conflicting attribute val-
ues and active comparison is found in its vicinity.
Consider the following review:
I love the Point and Shoot mode in my new camera
X. I was bored of using the same auto mode in my
old cam Y. Though it lacks the auto-program fea-
ture, still it’s worth its price in gold. While auto
mode in Camera Y was a sham, night mode was

a cool addition. A f/1.8 lens , preferably brand Z
would just be the icing on the cake.
Here the author compares his old camera with the
one he just purchased and finally talks about buy-
ing a different product (a lens). A naive extraction
scheme will extract features for each of cameras
X and Y, and lens Z. The problem could be further
compounded if the author swings back and forth
comparing two products leading us to the deep wa-
ters of Pronoun Resolution and Attribute Corefer-
ence Resolution. However, we easily find a way
around them with the assumption that the switch
will not happen too frequently in most reviews.

Finally, as the descriptions are often sparse, we
do not have all representative values for an at-
tribute. A better ‘importance’ measure for a value
(I(v)) can be obtained from the reviews. Also, in
the above description, Qp values can be arbitrarily
large or small. Hence, we translate this idea in the
probabilistic sense to a scale between 0 and 1. In
an informal manner, the affinity of each candidate
with the product can be treated as a confidence
measure (represented by P (c) instead of discrete
Qp values) in a scale of 0 to 1.
We also give a linear time algorithm based on the
above idea. The algorithm iterates over all the can-
didates, modifying their affinity and dispersing the
change to all other candidates in the review.

Given:- A set of products P and a set of re-
views Rp for every product p ∈ P . Let Cp,r be the
set of candidates for product p in review r. Let A
be the set of attributes. Wsim(Ak) and Wdiff (Ak)
are the weights of the attribute Ak for a matching
value and a competing value, respectively. Weight
Wcomp penalizes a comparative sentence in the
affinity calculation. dr(c, c′) is some distance
metric defined on all candidate pairs in a review
r. We set the distance between two candidates as
the number of words between their occurrence in
the review in our simulations.

for each p ∈ P :
for each r ∈ Rp:

for c ∈ Cp,r:
Compute:I(c)

Initialize:P (c) = 1
2

Init value=P(c)

for k ∈ 1 . . . n:
if c = Ak(p):

P (c) := Wsim(Ak) ∗ P (c) + (1 −
Wsim(Ak))

988



end if

else if c = Ak(p′), p′ ∈ P, p′! = p:
P (c) := Wdiff (A(k)) ∗ P (c)

end elseif

end for

if c lies in a comparative sentence:

P (c) := Wcomp ∗ P (c)
end if

Change(c)=P (c)-Init value

for c′ ∈ Cp,r, c′! = c:
P (c)+ = Change(c′)× I(c′)× e−dr(c′,c)

end for

Normalize P

end for

end for

end for

Selection:- Finally for each product p and
attribute Aj that misses a value for p, we choose
the candidate with maximum affinity(P (c)) i.e.
choose c∗p,Aj such that,

c∗p,Aj = argmax
c′∈Cp(Aj ,r),r∈Rp

P (c′, r).

Finally, c∗p,Aj is filled in as a value for Aj if
P (c∗p,Aj , r))(r is the review containing cp,Aj ) is
above a certain threshold. The algorithm runs with
a complexity of O(N+C2) where N is the size of
the reviews(number of words) and C is the number
of candidates generated by applying rules on them.
Since C is generally small, this is effectively linear
with respect to size.

4 Evaluation

4.1 Dataset

The algorithms were tested on a real life dataset
crawled from ‘Amazon’. It contains reviews and
short descriptions of 996 products in the Camera
and Accessories space (Cameras, lenses, filters,
etc). The total number of reviews was 23,337 lead-
ing to reviews per product ratio of about 24. Av-
erage number of words per review was around 40.
Each product has a minimum of 20 reviews.

4.2 Experimental Setup

4.2.1 Product Description Segmentation

The experiments were carried out by first iden-
tifying the appropriate schema and 12 attributes
were identified for description segmentation. They
are given in Table 6. Rules (as described in Sec-
tion 4.1) were used to standardize the descrip-

tions into the composing attributes. The dictionary
augmentation and rule writing together took nine
man-hours. This is much lower than usual since
we used Intellectual Property datasets on Brands
and Products and clustering used to detect mis-
spellings and varying vocabulary.

Table 6: Attributes guessed by the Rule Writer
Attribute Value Attribute Value
Brand Nikon Product Type Digital
Zoom True Color Black
Lens F/2.8 D Retail Price $ 250
Model D 90 Product Camera
Filter UV Size Compact
Resolution 12MP Features Point&Shoot

4.2.2 Missing Value Filling

It was observed that around 60% of the attributes
after the segmentation stage were null. So we used
the reviews to fill in these missing values. Reusing
the rules on the reviews led to a whopping 8434
candidates for the 12 attribute places in 969 prod-
ucts, which on using the confidence score based
pruning scheme reduced to 5321. We set 0.5 as the
weights for Brand Name, Product Name, Product
Type and Color; and 1 for the remaining attributes.
Recall that this is in-line with our arguments that
there can be many products with the same Brand
name, Product name, type and color but it is less
likely for two products to have the same value for
other attributes (say Retail Price or Weight). This
choice of weights is done once and can be done
at the time of the initial schema selection. Finally
to select or reject candidates, we use a threshold.
We used 0.4 times the mean(of the candidate con-
fidence values) as the threshold for selection in our
experiments. The threshold can be chosen based
on the general confidence that the enterprise has
on the correctness of the reviews. As the thresh-
old is decreased, the number of values filled in
increases but the precision-recall (and thereby F-
Score) decreases.

4.3 Results

To evaluate our work, we also compute the en-
tire attribute values view manually. Using this as
ground truth, we calculate the Precision, Recall
and F-Score of the overlap it has with our pro-
posed techniques. Sometimes the values extracted
are only partially correct for example, if the mea-
surement unit is missing for the weight attribute.

989



Table 7: Evaluation Metrics for each Stage
Evaluation Stages 100 Cameras 100 Lenses
Partially Correct Precision Recall F-Score Precision Recall F-Score

1.Standardization 1.000 0.909 0.952 1.000 0.925 0.961

2.Missing Value Extraction 0.888 0.705 0.786 0.837 0.700 0.762
Baseline 1 0.728 0.682 0.704 0.706 0.676 0.691
Baseline 2 0.771 0.709 0.739 0.741 0.703 0.712

Completely Correct Precision Recall F-Score Precision Recall F-Score

1.Standardization 0.998 0.901 0.947 1.000 0.917 0.957

2.Missing Value Filling 0.773 0.613 0.683 0.721 0.632 0.674
Baseline 1 0.644 0.607 0.625 0.618 0.596 0.607
Baseline 2 0.613 0.534 0.571 0.578 0.527 0.551

Hence, we evaluate results for both cases (when
the values match perfectly or only partially).

4.3.1 Product Description Segmentation

As the Standardization stage is rule based, with
time, close to perfect precision and recall can be
achieved. In nine man-hours of effort, we achieved
very high precision and recall as shown in Table 7.

4.3.2 Missing Value Filling

In Table 7, we give the precision and recall for the
Enrichment phase. Here, we also draw two base-
line comparisons for our work.

Baseline 1 is drawn by using the values ex-
tracted by the standardization stage from a train-
ing set (remaining 886 cameras) as seeds to train
a CRF. The value extraction task is treated as
a multi-class classification problem where each
word is to be classified as a value to one of the
attributes (A1 . . . An) or as a “not a value” class.
To generate a training set, every word is repre-
sented as a feature vector of n+ 20 features. First
n features are boolean entries and represent if the
word matches an already known attribute value of
the product. If the word matches with a value for
Aj , then jth feature is set to true and the rest to
false. 10 words in the context of that word and
their POS tags are remaining 20 features. The
seeds are used to assign class labels to the train-
ing set. State of the art CRF is used for value
extraction in a 10-fold setting. Please note that
drawing the seeds from the standardization stage
(which was rule based) gives this baseline the su-
pervision provided by the rules. This is done to
make a comparison with our approach fair. Recall
that our technique for missing value Assignment
leverages the supervision of existing rules to come

up with meaningful values for the attributes.
(Ghani et al., 2006) laid down an inge-

nious technique to automatically extract candidate
attribute-value seeds. They considered all pair of
consecutive words (wi, wi+1) where wi is a can-
didate value for the attribute wi+1. Next, they
computed the mutual information between all such
candidate attribute-value pairs to prune down to
fewer but cleaner seeds. Baseline 2 generates
seeds by their technique and filters them for the at-
tributes {A1 . . . An}. Again, CRF is trained with
the same feature space.

It can further be observed that due to rule based
cues, our techniques do well even when com-
pletely correct values are expected. However,
Baseline 2 (projection of (Ghani et al., 2006) on
the attribute set) falls apart as most extracted val-
ues are incomplete and partial.

The entire experiment is also carried out on a
similar dataset of 100 lenses to prove the general-
ization ability of our technique within the product
domain(Photography). Results on the lens dataset
(shown in Table 7) are very close to the camera
dataset, and occasionally better.

5 Conclusion

In this work, we tackle the problem of creating the
complete view of an enterprise’s products and ser-
vices. We utilize the rulesets developed by exist-
ing product data cleansing solutions for value ex-
traction from unstructured text media. Hereby, we
escaped the laborious process of writing annota-
tors. Supervision provided by the rules helped us
uncover values which can give us a better view of
the product and not merely sentiments.

990



References

Rayid Ghani, Katharina Probst, Yan Liu, Marko
Krema, and Andrew Fano. 2006. Text mining for
product attribute extraction. SIGKDD Explorations
Newsletter, 8(1):41–48.

Minqing Hu and Bing Liu. 2004. Mining and summa-
rizing customer reviews. In Proceedings of the In-
ternational Conference on Knowledge discovery and
data mining, pages 168–177.

Nozomi Kobayashi, Kentaro Inui, and Yuji Matsumoto.
2007. Extracting aspect-evaluation and aspect-of re-
lations in opinion mining. In Proceedings of the
Joint Conference on Empirical Methods in Natural
Language Processing and Computational Natural
Language Learning.

Bing Liu, Minqing Hu, and Junsheng Cheng. 2005.
Opinion observer: analyzing and comparing opin-
ions on the web. In Proceedings of the International
Conference on World Wide Web, pages 342–351.

U. Nambiar, A. Faruquie, K. H. Prasad, L. V. Subra-
maniam, and M. K. Mohania. 2011. Data augmen-
tation as a service for single view creation. In Pro-
ceedings of IEEE International Conference on Ser-
vices Computing (SCC).

Fuchun Peng and Andrew McCallum. 2006. Infor-
mation extraction from research papers using condi-
tional random fields. Information Processing Man-
agement, 42(4):963–979.

Ana-Maria Popescu and Oren Etzioni. 2005. Extract-
ing product features and opinions from reviews. In
Proceedings of the conference on Human Language
Technology and Empirical Methods in Natural Lan-
guage Processing, pages 339–346.

K. H. Prasad, T.A. Faruquie, S. Joshi, S. Chaturvedi,
L. V. Subramaniam, and M. K. Mohania. 2011.
Data cleansing techniques for large enterprise
datasets. In Proceedings of SRII Global Conference
(SRII).

Guang Qiu, Bing Liu, Jiajun Bu, and Chun Chen.
2009. Expanding domain sentiment lexicon through
double propagation. In Proceedings of the Inter-
national joint Conference on Artifical intelligence,
pages 1199–1204.

Veselin Stoyanov and Claire Cardie. 2008. Topic iden-
tification for fine-grained opinion analysis. In Pro-
ceedings of International Conference on Computa-
tional Linguistics, pages 817–824, Morristown, NJ,
USA.

B. Wang and H. Wang. 2008. Bootstrapping Both
Product Features and Opinion Words from Chinese
Customer Reviews with Cross-Inducing. Proceed-
ings of International Joint Conference on Natural
Language Processing.

Zhongwu Zhai, Bing Liu, Hua Xu, and Peifa Jia. 2010.
Grouping product features using semi-supervised
learning with soft-constraints.

Li Zhuang, Feng Jing, and Xiao-Yan Zhu. 2006.
Movie review mining and summarization. In Pro-
ceedings of the International Conference on Infor-
mation and knowledge management, pages 43–50.

991


