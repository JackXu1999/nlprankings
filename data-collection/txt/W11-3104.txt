















































Integrating Knowledge Resources and Shallow Language Processing for Question Classification


Proceedings of the KRAQ11 Workshop, pages 22–28,
Chiang Mai, Thailand, November 12, 2011.

Integrating Knowledge Resources and Shallow Language Processing for
Question Classification

Maheen Bakhtyar
Department of CSIM,

Asian Institute of Technology, Thailand.
Department of CS&IT,

University of Balochistan, Pakistan.
Maheen.Bakhtyar@ait.asia

Asanee Kawtrakul
Department of Computer Engineering,

Kasetsart University,Thailand.
NECTEC, Pathumthani,

Thailand.
asanee naist@yahoo.com

Abstract

Typically, Question Classification (QC)
is the first phase in Question Answer-
ing (QA) systems. This phase is respon-
sible for finding out the type of the ex-
pected answer by having the answer space
reduced by pruning out the extra infor-
mation that is not relevant for the answer
extraction. This paper focuses on some
Location based questions and some En-
tity type questions. Almost all the previ-
ous QC algorithms evaluated their work by
using the classes defined by Li and Roth
(2002). The coarse grained classes Loca-
tion and Entity both have fine grained class
Other. In this paper we target and present
the mechanism to create new classes to re-
place the Other classes in Location and
Entity class. Additionally, we also present
an automatic hierarchy creation method to
add new class nodes using the knowledge
resources and shallow language process-
ing. We also show how language process-
ing and knowledge resources are impor-
tant in the question processing and its ad-
vantage on Answer Extraction phase.

1 Introduction

Usually people are interested in the exact answer
and do not desire to look for the answer them-
selves in long list of documents. Exact answer is
more interesting and useful than getting a list of
documents.

Query analysis, processing or classification
phase have been always emphasized. The follow-
ing examples 1 show the importance of this phase
with respect to the Answer Extraction.
Example 1: Who was the first American to walk

1Questions and answer sentence taken from TREC-10
Text-REtrieval-Conference-10 (2001)

in space?. The answer sentence obtained is “In
1965 astronaut Edward White became the first
American to “walk” in space during the flight of
Gemini 4”2. Suppose the question is classified as
Human:Individual by some classification mecha-
nism. We notice that the answer line contains the
matching string “first American to walk in space”
therefore, the answer to the question is to be se-
lected from the remaining part “1965”, “Edward
White” or “Gemini 4”. Correct classification now
leads us to the answer Edward White.

Example 2: What day and month did John
Lennon die?. If this question is classified as Num-
ber:Date, it means that only date type will be tar-
geted from the text. This implies that the question
when correctly classified will give a hint about the
answer which helps the system in judging and ex-
tracting the answer from the corpus.

The questions can be categorized mainly in two
ways i.e. considering the question word and sec-
ond the answer type. Ray et al. (2010) categorizes
the factoid questions first in the categories such
as “who”, “why”, “what”, “where”, “how” and
“when” and classify them based on the two level
hierarchy of classes defined by Li and Roth (2002)
and shown in Table 1.

2 Problem Statements

Question Classification is important and helpful
for extracting the answers. A correct and meaning-
ful classification will lead the system to more ef-
ficient and correct answer extraction mechanisms.
On the other hand, a wrong or meaningless classi-
fication will not improve the answer extraction and
might become a cause of inaccurate final results.

2This line is taken from the document number DOCNO:
AP890527-0145 and contains the answer to this question

22



Table 1: Coarse and Fine grained classes
Coarse Fine
ABBR abbreviation, expansion
DESC definition, description, manner, reason

ENTY

animal, body, color, creation,
currency, disease/medical, event,
food, instrument, language, letter,
other,plant, product, religion, sport,
substance, symbol, technique, term,
vehicle, word

HUM description, group, individual, title
LOC city, country, mountain, other, state

NUM

code, count, date, distance, money,
order, other, percent, period, speed,
temperature, size, weight

2.1 Insufficient classes in the taxonomy

Question classes defined and labeled in UIUC3

dataset by Li and Roth (2002) are most widely
used in the previous work (Quan et al. (2011),
Song et al. (2011), Yu et al. (2010), Buscaldi et
al. (2010), Huang et al. (2007) and Boldrini et
al. (2009)). Many of the researchers developed
their systems using these classes and the labeled
question dataset. In the labeled dataset, if a ques-
tion is not mapped to some class, it is placed into
the fine grained class Other. Assigning to a class
Other is not very helpful in the answer extrac-
tion. For example, in case of Location category,
Location:Other will only prune out city, country,
mountain and state as possible answer categories.
Therefore, a close analysis of questions belonging
to this class is needed and a new set of classes is
required to overcome this deficiency.

We currently focus on two of the coarse grained
classes; Location and Entity; and all their fine
grained classes. It is also observed that many of
the fine grained classes are missing in the exist-
ing class hierarchy which needs to be mapped to
the questions. For instance, the class river, lake or
any other water body is not present in the existing
class taxonomy whereas some questions require
such classes e.g. the question What body of water
are the Canary Islands in ? is currently placed in
class LOC:Other by Li and Roth (2002). This as-
signed class neither gives an exact hint nor helps to
filter the candidate answers. Whereas, mapping it
to a class such as waterbody makes it more mean-

3http://cogcomp.cs.illinois.edu/Data/QA/QC/

ingful and easier to find the answers. Similarly,
the question “what is Bill Gates of Microsoft e-
mail address” ? is labeled as LOC:Other by the
authors. If this question is searched using a search
engine, a lot of documents will be returned having
all the key concepts in the question. A chunk of
text containing the answer is as follows, “All the
Good Emails get sent to another Bill Gates Email
Address, which he checks twice a week. Because
He knows everyone will be looking for his email
address under @microsoft.com. The Employees
who checks his email under billg@microsoft.com
send it to the one he checks” 4. This chunk
from the document contains all the question key-
words. Without the classes defined, we do not
know which part of the chunk is more important.
Whereas, if we determine that the answer should
be an email address, then we only need to target
the email addresses in the text without taking care
of the rest of the document. Therefore, the detail
of classes and subclasses is needed to cover more
and more questions instead of assigning them to
the LOC:Other class.

Li and Roth (2002) show that among 500 ques-
tions in TREC 10, 62% of the location questions
belong to the class Other. The highest number
of questions lie under the location category Other
which is actually not very helpful or meaningful
in extracting the answer. It means that about 62%
of the location questions will be answered during
the answer extraction phase without making use
of the classes, despite the efforts put into classifi-
cation phase. Similarly, 13% of the entity ques-
tions belong to the class Other. Entity class has
22 fine grained classes and the large number of
questions are mapped to Other after animal and
substance. Later, Li and Roth (2006) again gave a
statistics of distribution of questions in each class
of TREC 10 and 11 Text-REtrieval-Conference
(1999 to 2007) questions, collectively. They ob-
served that out of 1000 questions, 195(19.5%)
are Location based. In Location based questions,
there are 22.6% questions mapped to class city,
10.8% questions about class country, 2.6% about
mountain, 58.5% are mapped to class other, and
5.6% questions are mapped to class state.

One of the main advantage of replacing the class
Other with fine grained classes is that it makes as-
signment of a single question to multiple classes/-

4http://email.about.com/b/2009/05/30/how-can-i-email-
bill-gates-what-is-bill-gatess-email-address.htm

23



24



25



26



27



fication scheme but initially only for the specific
pattern of questions as discussed earlier.

Answer extraction phase requires the question
to be classified in some manner. If a classifica-
tion mechanism is developed by using our set of
classes, then answer extraction technique be more
helpful to extract the answer.

5 Conclusion and Future Work

We propose a new hierarchy for the questions
that earlier belonging to the class Location:Other
or Entity:Other . We show that classifying the
questions into “Other” is not very useful for the
answer extraction phase. These two classes are
now represented as a hierarchy which is popu-
lated using some NLP techniques and knowledge
resources i.e. WordNet and DBPedia. We also an-
alyzed how the new hierarchy helped to prune out
the extra unnecessary details for efficient answer
extraction.

This is the initial work carried out with ex-
tremely limited questions. We only focused on
the question with a specific pattern for generat-
ing the new hierarchy using knowledge resources.
We plan to work on the remaining question types
and patterns in the future. Moreover, we also plan
to target the other coarse classes, “NUM” having
sub-type “Other”.
Additionally, we plan to label the questions and
publish with the hierarchy obtained for all the
questions set so a new set of classes is obtained
and is comparable for the other researchers.

References

E. Boldrini, S. Ferrández, R. Izquierdo, D. Tomás,
O. Ferrández, and J. L. Vicedo. 2009. A proposal
of expected answer type and named entity annota-
tion in a question answering context. In Proceed-
ings of the 2nd conference on Human System In-
teractions, HSI’09, pages 315–319, Piscataway, NJ,
USA. IEEE Press.

Davide Buscaldi, Paolo Rosso, José Manuel Gómez-
Soriano, and Emilio Sanchis. 2010. Answering
questions with an n-gram based passage retrieval en-
gine. J. Intell. Inf. Syst., 34:113–134, April.

Peng Huang, Jiajun Bu, Chun Chen, and Guang Qiu.
2007. An effective feature-weighting model for
question classification. In Proceedings of the 2007
International Conference on Computational Intelli-
gence and Security, CIS ’07, pages 32–36, Washing-
ton, DC, USA. IEEE Computer Society.

Xin Li and Dan Roth. 2002. Learning question clas-
sifiers. In Proceedings of the 19th international
conference on Computational linguistics, pages 1–
7, Morristown, NJ, USA. Association for Computa-
tional Linguistics.

Xin Li and Dan Roth. 2006. Learning question clas-
sifiers: the role of semantic information. Natural
Language Engineering, 12(03):229–249.

George A. Miller. 1995. Wordnet: A lexical database
for english. Communications of the ACM, 38:39–41.

Xiaojun Quan, Liu Wenyin, and Bite Qiu. 2011.
Term weighting schemes for question categoriza-
tion. IEEE Trans. Pattern Anal. Mach. Intell.,
33(5):1009–1021.

Santosh Kumar Ray, Shailendra Singh, and B.P. Joshi.
2010. A semantic approach for question classifica-
tion using wordnet and wikipedia. Pattern Recog-
nition Letters, 31(13):1935 – 1943. Meta-heuristic
Intelligence Based Image Processing.

Vincent Schickel-Zuber and Boi Faltings. 2007. Oss:
a semantic similarity function based on hierarchical
ontologies. In Proceedings of the 20th international
joint conference on Artifical intelligence, pages 551–
556, San Francisco, CA, USA. Morgan Kaufmann
Publishers Inc.

Wanpeng Song, Liu Wenyin, Naijie Gu, Xiaojun Quan,
and Tianyong Hao. 2011. Automatic categorization
of questions for user-interactive question answering.
Inf. Process. Manage., 47:147–156, March.

Text-REtrieval-Conference-10. 2001. Trec-10 ques-
tion answering data.

Text-REtrieval-Conference. 1999 to 2007. Trec qa
main page.

Zhengtao Yu, Lei Su, Lina Li, Quan Zhao, Cunli Mao,
and Jianyi Guo. 2010. Question classification based
on co-training style semi-supervised learning. Pat-
tern Recogn. Lett., 31:1975–1980, October.

28


