



















































ABDN at SemEval-2018 Task 10: Recognising Discriminative Attributes using Context Embeddings and WordNet


Proceedings of the 12th International Workshop on Semantic Evaluation (SemEval-2018), pages 1017–1021
New Orleans, Louisiana, June 5–6, 2018. ©2018 Association for Computational Linguistics

ABDN at SemEval-2018 Task 10: Recognising Discriminative Attributes
using Context Embeddings and WordNet

Rui Mao, Guanyi Chen, Ruizhe Li and Chenghua Lin
Computing Science Department

University of Aberdeen
Aberdeen, Scotland, UK

{r03rm16, g.chen, ruizhe.li, chenghua.lin}@abdn.ac.uk

Abstract

This paper describes the system that we sub-
mitted for SemEval-2018 task 10: capturing
discriminative attributes. Our system is built
upon a simple idea of measuring the attribute
word’s similarity with each of the two seman-
tically similar words, based on an extended
word embedding method and WordNet. In-
stead of computing the similarities between
the attribute and semantically similar words
by using standard word embeddings, we pro-
pose a novel method that combines word and
context embeddings which can better measure
similarities. Our model is simple and effective,
which achieves an average F1 score of 0.62 on
the test set.

1 Introduction

Capturing discriminative attributes is a novel task,
which is very different from classical semantic
tasks that model similarities in semantics. The
task aims to recognise semantic differences be-
tween words. Traditional semantic similarity eval-
uation tasks were designed for evaluating the qual-
ity of word representations based on the fact that
words with similar semantics will be close to each
other in vector space. Recent state-of-the-art dis-
tributed semantic models (Ling et al., 2015; Bo-
janowski et al., 2017) inspired by the success of
word2vec (Mikolov et al., 2013) gave good per-
formance in these similarity measure tasks. Nev-
ertheless, how to capture discriminative attributes
between semantically similar words is still a chal-
lenge for traditional word embedding methods, be-
cause these methods are designed to capture simi-
lar semantics.

We have two observations for the nature of
the task and the provided data: (1) only limited
data is available for model training; (2) the in-
puts of the model are merely isolated words them-
selves, which lack context information for apply-

ing complex models. Therefore, we propose a
novel framework that differentiates two semanti-
cally similar words with the attribute word by us-
ing their word and context embeddings. We ex-
perimented with both Continuous Bag of Words
(CBOW) and Skip-gram, demonstrating that using
the combination of word and context embeddings
outperforms using word embeddings alone.

The contribution of this work can be sum-
marised as follows. We examine word and con-
text embeddings in CBOW and Skip-gram, show-
ing that using both word and context embeddings
can better measure the co-occurrence of two words
in sentences than simply using word embeddings.
Hence our similarity measure can recognise the
discriminative attributes of two semantically simi-
lar words more accurately.

2 System Description

Our system is trained based on word and context
embedding features as well as WordNet features
(Fellbaum, 1998). Before introducing our frame-
work in detail, we first introduce the two key tech-
nical parts of our framework, i.e., context embed-
ding and WordNet.

2.1 Context embeddings
In contrast to simply using traditional word em-
beddings which model semantic similarities based
on contextual similarities, we consider using both
word and context embeddings. Word and context
embeddings are the vectors of target words and
context words in CBOW and Skip-gram. Using
them together can model the co-occurrence of at-
tribute words and distinguished words in a sen-
tence, which is useful in predicting whether the at-
tribute word can distinguish two semantically sim-
ilar words.

Take the Skip-gram model as an example. Skip-
gram uses a neural network with a single hidden

1017



layer of neurons. Given a target word, the objec-
tive function is to maximize the probability of pre-
dicting each context word (several words before
and after the target word in a training sentence)
(Rong, 2014):

argmax p(wc|wt) (1)

where wt is a target word in a training sentence,
wc is a context word of wt, appearing within the
same sentence.

During every training epoch on each context
word, the weight matrices before and after the hid-
den layer will be updated. A row vector in the ma-
trix before the hidden layer is a traditional word
embedding vt, as the vector is updated when the
corresponding word wt is in the target word po-
sition. A column vector in the matrix after the
hidden layer is defined as context embedding vc,
as the column vector is updated when the cor-
responding word wc is in the context word po-
sition. Each word has two vectors, vt and vc,
as each word can be a target word or a context
word of other target words. Some popular toolkits,
e.g., gensim word2vec (Řehůřek and Sojka, 2010),
abandon Skip-gram’s context embeddings vc after
training, as experimental research (Nalisnick et al.,
2016) proves that simply using vt or vc (IN-IN or
OUT-OUT in the original paper) for two function
or type similar words to measure their similarity
yields higher scores.

Actually, the conditional probability in the ob-
jective function in Eq. 1 can be expanded as:

p(wc|wt) =
exp (vt · vc)∑

i∈|V | exp (vt · vi)
(2)

where V is the vocabulary of the training set, the
dot product vt · vc in numerator computes the sim-
ilarity between the target word vector and the con-
text word vector. The denominator is to normalise
the similarity into a probability. Thus, given a tar-
get word, training the whole model involves up-
dating the matrices before and after the hidden
layer to maximize the probability of predicting the
context word. This is similar to maximizing the
similarity between the target word embeddings vt
and context word embeddings vc. It means that
if we can reuse this trained similarity measure, to
compute e.g., cosine similarity, then we will get a
much better result. In other words, using both the
word and context embeddings of two words that
frequently appeared within each other’s contexts

will result in a better similarity measure, which
may be incorporating the co-occurrence informa-
tion of the two words.

CBOW can be considered as the reverse of
Skip-gram. Given a context, the target is to maxi-
mize the probability of predicting the target word
appearing in the context. Later, we will examine
both CBOW and Skip-gram’s word and context
embeddings in our model.

2.2 Word definition in WordNet
We also introduce features based on word sense
definitions in WordNet (Fellbaum, 1998), consid-
ering the differences between the definitions of
two semantically similar words. The two words
may be similar in semantics, but different in def-
initions. An eligible discriminative attribute word
may have high possibility to appear within one
of the two word definitions, rather than both of
them. For example, ears can distinguish corn and
broccoli, as in WordNet, ears occurs in the defi-
nition of corn as “tall annual cereal grass bear-
ing kernels on large ears: widely cultivated in
America in many varieties; the principal cereal
in Mexico and Central and South America since
pre-Columbian times”, rather than broccoli’s defi-
nition that “plant with dense clusters of tight green
flower buds”. We will also capture such characters
to distinguish two words.

2.3 Hypothesis and framework
Our first hypothesis is that an attribute word wA
can distinguish two semantically similar words w1
and w2, if the attribute word co-occurs much more
frequently with one word than the other in the cor-
pora. In vector space, the attribute word can be
closer to a distinguished word than the other one.
Our second hypothesis is that if wA can distin-
guish w1 and w2, wA may appear within one of
the definitions of w1 and w2 in WordNet.

The framework of our model can be summa-
rized as: (1) we firstly train word embeddings vt
and context embeddings vc on a Wikipedia dump.
(2) Given a triple (w1, w2, wA), we then com-
pute wA’s cosine similarities with w1 and w2, and
the difference in their similarities, which are used
as three input features in the following classifi-
cations. For example, given the context embed-
dings of w1 and w2 and the word embedding of
wA, we compute Feature 1: cosine(vw1c , v

wA
t );

Feature 2: cosine(vw2c , v
wA
t ); and Feature 3:

|cosine(vw1c , vwAt ) − cosine(vw2c , vwAt )|, respec-

1018



1 cosine(w1, wA)
2 cosine(w2, wA)
3 |cosine(w1, wA)− cosine(w2, wA)|
4 binary variable, indicating if wA appears in the

WordNet definitions of w1
5 binary variable, indicating if wA appears in the

WordNet definitions of w2

Table 1: Feature descriptions.

tively. (3) Next, we introduce two binary features
to indicate whether wA appears in any sense def-
initions of w1 and w2 in WordNet (Feature 4 and
5), respectively. (4) We train a random forest clas-
sifier with the above five features (see Table 1) to
classify if the attribute word wA can distinguish
two semantically similar words w1 and w2.

3 Experimental Settings

Data. The data was provided by the organizers
of SemEval 2018 Task 10: Capturing Discrimi-
native Attributes1. There are 17,501, 2,722 and
2,340 triples (w1, w2, wA) in training, validation
and testing sets, respectively. All the words in the
triples are nouns. Note that the discriminative at-
tribute words wA in the given dataset are selected,
because they represent the visual attribute of one
of two semantically similar words. For example,
red can differentiate apple and banana, because
visually, apple is red, while banana is yellow. The
task does not consider other discriminative fea-
tures, such as sound and taste. So, using image
features may take advantages in this dataset, how-
ever, semantic features can also capture invisible
discriminative attributes.
Word and context embedding. We first it-
eratively train 300 dimensional word and context
embeddings based on CBOW and Skip-gram with
a Wikipedia dump2 for 3 epochs respectively, set-
ting a context window of 5 words before and after
the target word. Words with frequency less than 5
in the Wikipedia are ignored. The down sampling
rate is 10−4.

Based on CBOW and Skip-gram, we test all
possible combinations of word and context em-
beddings to compute cosine similarities. The first
combination is context embeddings of the two se-
mantically similar words w1 and w2, and word
embeddings of the attribute word wA. In Table 2

1https://competitions.codalab.org/
competitions/17326

2https://dumps.wikimedia.org/enwiki/
20170920/

and 4, this approach is represented as vw1c v
w2
c v

wA
t .

The second vw1t v
w2
t v

wA
c uses word embeddings of

w1 and w2, and context embeddings of wA. The
third vw1c v

w2
c v

wA
c is simply context embeddings of

w1, w2 and wA. The fourth vw1t v
w2
t v

wA
t is simply

word embeddings of w1, w2 and wA.

4 Results

We cast the challenge task as a supervised classi-
fication problem. We first examine which combi-
nation of word and context embeddings and which
training method (CBOW or Skip-gram) is optimal
in this task. In this step, we only use Feature 1-3
(see Table 1) to classify the triple (w1, w2, wA).

As can be seen in Table 2, both the CBOW
based methods that use word and context embed-
dings yield the highest average F1 of 0.55 in the
validation set. Skip-gram based models generally
perform worse than CBOW based models, but us-
ing Skip-gram word embeddings of w1 and w2
and context embeddings of wA also outperforms
the word embedding based model in the validation
set. The experiments running on the test set show
similar trends that word and context embedding
based models outperform word embedding based
models. Such results demonstrate that using word
and context embeddings together can better dis-
tinguish two semantically similar words with an
attribute word, than simply using standard word
embeddings. The results also support our first hy-
pothesis that if the attribute word frequently ap-
pears in one word’s context than the other one, it
can distinguish the two words.

We also examine WordNet definition features
individually. As shown in Table 3, simply using
Feature 4-5 cannot classify the triple accurately.
The F1 score of setting positive label as 1 is very
low on the validation set (F1=36%). This is for
the reason that an eligible discriminative attribute
word cannot always associate with the definitions
of one of two semantically similar words. So, sim-
ply using such features cannot identify discrimina-
tive words precisely.

Finally, we combine both similarity and Word-
Net features together to address this challenge.
There is no significant difference between CBOW
based vw1c v

w2
c v

wA
t and v

w1
t v

w2
c v

wA
c in the vali-

dation set in terms of average F1. We select
vw1c v

w2
c v

wA
t as the winner combination of word

and context embeddings, because this approach
has closer F1 scores, when setting different la-

1019



setup positive=1 positive=0 average F1P R F1 P R F1

validation

CBOW

vw1c v
w2
c v

wA
t 0.56 0.46 0.51 0.54 0.64 0.59 0.55

vw1t v
w2
t v

wA
c 0.57 0.45 0.50 0.55 0.66 0.60 0.55

vw1c v
w2
c v

wA
c 0.50 0.37 0.43 0.50 0.63 0.56 0.49

vw1t v
w2
t v

wA
t 0.50 0.30 0.37 0.50 0.70 0.58 0.48

Skip-gram

vw1c v
w2
c v

wA
t 0.54 0.38 0.45 0.52 0.68 0.59 0.52

vw1t v
w2
t v

wA
c 0.55 0.47 0.51 0.54 0.62 0.58 0.54

vw1c v
w2
c v

wA
c 0.52 0.33 0.40 0.51 0.70 0.59 0.49

vw1t v
w2
t v

wA
t 0.53 0.41 0.46 0.52 0.64 0.57 0.52

test

CBOW

vw1c v
w2
c v

wA
t 0.54 0.56 0.55 0.63 0.62 0.63 0.59

vw1t v
w2
t v

wA
c 0.54 0.53 0.53 0.62 0.63 0.63 0.58

vw1c v
w2
c v

wA
c 0.50 0.47 0.49 0.59 0.62 0.61 0.55

vw1t v
w2
t v

wA
t 0.49 0.39 0.44 0.58 0.67 0.62 0.53

Skip-gram

vw1c v
w2
c v

wA
t 0.52 0.50 0.51 0.61 0.63 0.62 0.56

vw1t v
w2
t v

wA
c 0.52 0.56 0.54 0.62 0.59 0.60 0.57

vw1c v
w2
c v

wA
c 0.50 0.43 0.46 0.59 0.65 0.62 0.54

vw1t v
w2
t v

wA
t 0.51 0.51 0.51 0.60 0.61 0.61 0.56

Table 2: Experimental results by using word and context embeddings (Feature 1-3).

setup positive=1 positive=0 average F1P R F1 P R F1

WordNet validation 0.66 0.24 0.36 0.53 0.87 0.66 0.51test 0.66 0.26 0.37 0.60 0.89 0.71 0.54

Table 3: Experimental results by using WordNet definition features (Feature 4-5).

setup positive=1 positive=0 average F1P R F1 P R F1

CBOW+WordNet validation v
w1
c v

w2
c v

wA
t +WN 0.57 0.53 0.55 0.56 0.59 0.57 0.56

test vw1c vw2c vwAt +WN 0.58 0.60 0.59 0.66 0.65 0.65 0.62

Table 4: Final results by using CBOW word and context embeddings, and WordNet features (Feature 1-5).

bels (1 or 0) as the positive label. Thus, in the
final submission, we use CBOW trained context
embeddings of w1 and w2, and word embeddings
of wA to compute similarity features. We identify
whether an attribute word wA can distinguish w1
and w2 by using the above similarity features and
WordNet definition features together. Although
word and context embedding based similarity fea-
tures are much more effective than WordNet fea-
tures, by introducing WordNet features, the model
further improves its performance, achieving 62%
F1 on the test set (Table 4). WordNet definitions
are also supportive features in this task.

Error analysis. We found that a significant por-
tion of failures appear in those examples that the
textual associations of the attribute words and the
semantically similar words are not always discrim-
inative. E.g., given a triple, (sons, father, young),
our model failed in identifying young as a discrim-
inative attribute, because young has been widely
used to describe sons and father in the text (e.g.,
young sons and a young father). In this case, our
word co-occurrence based method is suboptimal.

5 Conclusion

In this paper, we extended traditional word em-
bedding methods (CBOW and Skip-gram) to dis-
tinguish two semantically similar words using an
attribute word. In contrast with simply using tra-
ditional word representations, using both context
and word embeddings can better model the co-
occurrence between the two similar words and
their discriminative attribute word. If the at-
tribute word frequently co-occurs with one of the
similar words more than another one within the
same sentences, then the two semantically sim-
ilar words can be distinguished by the attribute
word. By using CBOW word and context embed-
ding based similarity features and simple WordNet
based word sense definition features, our model
performs an average F1 of 62% on the test set.

Acknowledgments

This work is supported by the award made by the
UK Engineering and Physical Sciences Research
Council (Grant number: EP/P005810/1).

1020



References
Piotr Bojanowski, Edouard Grave, Armand Joulin, and

Tomas Mikolov. 2017. Enriching word vectors with
subword information. Transactions of the Associa-
tion for Computational Linguistics, 5:135–146.

Christiane Fellbaum. 1998. WordNet: An Electronic
Lexical Database. Bradford Books.

Wang Ling, Chris Dyer, Alan Black, and Isabel
Trancoso. 2015. Two/too simple adaptations of
word2vec for syntax problems. In Proceedings of
the 2015 Conference of the North American Chap-
ter of the Association for Computational Linguis-
tics: Human Language Technologies. Association
for Computational Linguistics.

Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey
Dean. 2013. Efficient estimation of word represen-
tations in vector space. Proceedings of International
Conference on Learning Representations.

Eric Nalisnick, Bhaskar Mitra, Nick Craswell, and
Rich Caruana. 2016. Improving document ranking
with dual word embeddings. In Proceedings of the
25th International Conference Companion on World
Wide Web, pages 83–84. International World Wide
Web Conferences Steering Committee.

Radim Řehůřek and Petr Sojka. 2010. Software Frame-
work for Topic Modelling with Large Corpora. In
Proceedings of the LREC 2010 Workshop on New
Challenges for NLP Frameworks, pages 45–50, Val-
letta, Malta. ELRA. http://is.muni.cz/
publication/884893/en.

Xin Rong. 2014. word2vec parameter learning ex-
plained. arXiv preprint arXiv:1411.2738.

1021


