



















































A Web-scale system for scientific knowledge exploration


Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics-System Demonstrations, pages 87–92
Melbourne, Australia, July 15 - 20, 2018. c©2018 Association for Computational Linguistics

87

A Web-scale system for scientific knowledge exploration

Zhihong Shen
Microsoft Research

Redmond, WA, USA

Hao Ma
Microsoft Research

Redmond, WA, USA
{zhihosh,haoma,kuansanw}@microsoft.com

Kuansan Wang
Microsoft Research

Redmond, WA, USA

Abstract

To enable efficient exploration of Web-
scale scientific knowledge, it is neces-
sary to organize scientific publications
into a hierarchical concept structure. In
this work, we present a large-scale sys-
tem to (1) identify hundreds of thou-
sands of scientific concepts, (2) tag these
identified concepts to hundreds of mil-
lions of scientific publications by leverag-
ing both text and graph structure, and (3)
build a six-level concept hierarchy with
a subsumption-based model. The sys-
tem builds the most comprehensive cross-
domain scientific concept ontology pub-
lished to date, with more than 200 thou-
sand concepts and over one million rela-
tionships.

1 Introduction

Scientific literature has grown exponentially over
the past centuries, with a two-fold increase ev-
ery 12 years (Dong et al., 2017), and millions of
new publications are added every month. Effi-
ciently identifying relevant research has become
an ever increasing challenge due to the unprece-
dented growth of scientific knowledge. In order
to assist researchers to navigate the entirety of sci-
entific information, we present a deployed system
that organizes scientific knowledge in a hierarchi-
cal manner.

To enable a streamlined and satisfactory seman-
tic exploration experience of scientific knowledge,
three criteria must be met:

• a comprehensive coverage on the broad spec-
trum of academic disciplines and concepts
(we call them concepts or fields-of-study, ab-
breviated as FoS, in this paper);

Figure 1: Three modules of the system: concept
discovery, concept-document tagging, and con-
cept hierarchy generation.

• a well-organized hierarchical structure of sci-
entific concepts;

• an accurate mapping between these concepts
and all forms of academic publications, in-
cluding books, journal articles, conference
papers, pre-prints, etc.

To build such a system on Web-scale, the fol-
lowing challenges need to be tackled:

• Scalability: Traditionally, academic disci-
pline and concept taxonomies have been cu-
rated manually on a scale of hundreds or
thousands, which is insufficient in modeling
the richness of academic concepts across all
domains. Consequently, the low concept cov-
erage also limits the exploration experience
of hundreds of millions of scientific publica-
tions.

• Trustworthy representation: Traditional
concept hierarchy construction approaches
extract concepts from unstructured docu-
ments, select representative terms to denote
a concept, and build the hierarchy on top of
them (Sanderson and Croft, 1999; Liu et al.,
2012). The concepts extracted this way not
only lack authoritative definition, but also



88

Concept Concept Hierarchy
discovery tagging building

Main scalability / trustworthy scalability / stability /
challenges representation coverage accuracy
Problem knowledge base multi-label topic hierarchy

formulation type prediction text classification construction
Solution / Wikipedia / KB / word embedding / extended
model(s) graph link analysis text + graph structure subsumption

Data scale 105 – 106 109 – 1010 106 – 107

Data update
frequency monthly weekly monthly

Table 1: System key features at a glance.

contain erroneous topics with subpar quality
which is not suitable for a production system.

• Temporal dynamics: Academic publica-
tions are growing at an unprecedented pace
(about 70K more papers per day according to
our system) and new concepts are emerging
faster than ever. This requires frequent inclu-
sion on latest publications and re-evaluation
in tagging and hierarchy-building results.

In this work, we present a Web-scale sys-
tem with three modules—concept discovery,
concept-document tagging, and concept-hierarchy
generation—to facilitate scientific knowledge ex-
ploration (see Figure 1). This is one of the core
components in constructing the Microsoft Aca-
demic Graph (MAG), which enables a semantic
search experience in the academic domain1. MAG
is a scientific knowledge base and a heterogeneous
graph with six types of academic entities: publica-
tion, author, institution, journal, conference, and
field-of-study (i.e., concept or FoS). As of March
2018, it contains more than 170 million publica-
tions with over one billion paper citation relation-
ships, and is the largest publicly available aca-
demic dataset to date2.

To generate high-quality concepts with com-
prehensive coverage, we leverage Wikipedia ar-
ticles as the source of concept discovery. Each
Wikipedia article is an entity in a general knowl-
edge base (KB). A KB entity associated with a
Wikipedia article is referred to as a Wikipedia en-
tity. We formulate concept discovery as a knowl-
edge base type prediction problem (Neelakantan

1The details about where and how we obtain, aggregate,
and ingest academic publication information into the system
is out-of-scope for this paper and for more information please
refer to (Sinha et al., 2015).

2https://www.openacademic.ai/oag/

and Chang, 2015) and use graph link analysis to
guide the process. In total, 228K academic con-
cepts are identified from over five million English
Wikipedia entities.

During the tagging stage, both textual informa-
tion and graph structure are considered. The text
from Wikipedia articles and papers’ meta informa-
tion (e.g., titles, keywords, and abstracts) are used
as the concept’s and publication’s textual repre-
sentations respectively. Graph structural informa-
tion is leveraged by using text from a publication’s
neighboring nodes in MAG (its citations, refer-
ences, and publishing venue) as part of the pub-
lication’s representation with a discounting factor.
We limit the search space for each publication to a
constant range, reduce the complexity to O(N) for
scalability, where N is the number of publications.
Close to one billion concept-publication pairs are
established with associated confidence scores.

Together with the notion of subsump-
tion (Sanderson and Croft, 1999), this confidence
score is then used to construct a six-level directed
acyclic graph (DAG) hierarchy with over 200K
nodes and more than one million edges.

Our system is a deployed product with regular
data refreshment and algorithm improvement. Key
features of the system are summarized in Table 1.
The system is updated weekly or monthly to in-
clude fresh content on the Web. Various docu-
ment and language understanding techniques are
experimented with and incorporated to incremen-
tally improve the performance over time.

2 System Description

2.1 Concept Discovery

As top level disciplines are extremely important
and highly visible to system end users, we man-

https://www.openacademic.ai/oag/


89

ually define 19 top level (“L0”) disciplines (such
as physics, medicine) and 294 second level (“L1”)
sub-domains (examples are machine learning, al-
gebra) by referencing existing classification3 and
get their correspondent Wikipedia entities in a
general in-house knowledge base (KB).

It is well understood that entity types in a gen-
eral KB are limited and far from complete. Enti-
ties labeled with FoS type in KB are in the lower
thousands and noisy for both in-house KB and
latest Freebase dump4. The goal is to identify
more FoS type entities from over 5 million English
Wikipedia entities in an in-house KB. We formu-
late this task as a knowledge base type prediction
problem, and focus on predicting only one specific
type—FoS.

In addition to the above-mentioned “L0” and
“L1” FoS, we manually review and identify over
2000 high-quality ones as initial seed FoS. We it-
erate a few rounds between a graph link analysis
step for candidate exploration and an entity type
based filtering and enrichment step for candidate
fine-tuning based on KB types.

Graph link analysis: To drive the process of
exploring new FoS candidates, we apply the in-
tuition that if the majority of an entity’s nearest
neighbours are FoS, then it is highly likely an FoS
as well. To calculate nearest neighbours, a dis-
tance measure between two Wikipedia entities is
required. We use an effective and low-cost ap-
proach based on Wikipedia link analysis to com-
pute the semantic closeness (Milne and Witten,
2008). We label a Wikipedia entity as an FoS can-
didate if there are more than K neighbours in its
top N nearest ones are in a current FoS set. Em-
pirically, N is set to 100 and K is in [35, 45] range
for best results.

Entity type based filtering and enrichment:
The candidate set generated in the above step con-
tains various types of entities, such as person,
event, protein, book topic, etc.5 Entities with
obvious invalid types are eliminated (e.g. per-
son) and entities with good types are further in-
cluded (e.g. protein, such that all Wikipedia enti-
ties which have labeled type as protein are added).

3http://science-metrix.com/en/
classification

4https://developers.google.com/
freebase/

5Entity types are obtained from the in-house KB, which
has higher type coverage compared with Freebase, details on
how the in-house KB produces entity types is out-of-scope
and not discussed in this paper.

The results of this step are used as the input for
graph link analysis in the next iteration.

More than 228K FoS have been identified with
this iterative approach, based on over 2000 initial
seed FoS.

2.2 Tagging Concepts to Publications
We formulate the concept tagging as a multi-label
classification problem; i.e. each publication could
be tagged with multiple FoS as appropriate. In a
naive approach, the complexity could reach M ·N
to exhaust all possible pairs, where M is 200K+
for FoS and N is close to 200M for publications.
Such a naive solution is computationally expen-
sive and wasteful, since most scientific publica-
tions cover no more than 20 FoS based on empiri-
cal observation.

We apply heuristics to cut candidate pairs ag-
gressively to address the scalability challenge, to
a level of 300–400 FoS per publication6. Graph
structural information is incorporated in addition
to textual information to improve the accuracy and
coverage when limited or inadequate text of a con-
cept or publication is accessible.

We first define simple representing text (or SRT)
and extended representing text (or ERT). SRT is
the text used to describe the academic entity it-
self. ERT is the extension of SRT and leverages
the graph structural information to include textual
information from its neighbouring nodes in MAG.

A publishing venue’s full name (i.e. the jour-
nal name or the conference name) is its SRT. The
first paragraph of a concept’s Wikipedia article is
used as its SRT. Textual meta data, such as title,
keywords, and abstract is a publication’s SRT.

We sample a subset of publications from a given
venue and concatenate their SRT. This is used as
this venue’s ERT. For broad disciplines or do-
mains (e.g. L0 and L1 FoS), Wikipedia text be-
comes too vague and general to best represent its
academic meanings. We manually curate such
concept-venue pairs and aggregate ERT of venues
associated with a given concept to obtain the ERT
for the concept. For example, SRT of a subset of
papers from ACL are used to construct ERT for
ACL, and subsequently be part of the ERT for nat-
ural language processing concept. A publication’s
ERT includes SRT from its citations, references
and ERT of its linked publishing venue.

6We include all L0s and L1s and FoS entities spotted in
a publication’s extended representing text, which is defined
later in this section

http://science-metrix.com/en/classification
http://science-metrix.com/en/classification
https://developers.google.com/freebase/
https://developers.google.com/freebase/


90

We use hps and h
p
e to denote the representation

of a publication (p)’s SRT and ERT, hvs and h
v
e for

a venue (v)’s SRT and ERT. Weight w is used to
discount different neighbours’ impact as appropri-
ate. Equation 1 and 2 formally define publication
ERT and venue ERT calculation.

hpe = h
p
s +

∑
i∈Cit

wih
p
s(i)+

∑
j∈Ref

wjh
p
s(j)+wvh

v
e

(1)

hve =
∑
i∈V

hps(i) + h
v
s (2)

Four types of features are extracted from the
text: bag-of-words (BoW), bag-of-entities (BoE),
embedding-of-words (EoW), and embedding-of-
entities (EoE). These features are concatenated for
the vector representation h used in Equation 1 and
2. The confidence score of a concept-publication
pair is the cosine similarity between these vector
representations.

We pre-train the word embeddings by us-
ing the skip-gram (Mikolov et al., 2013) on
the academic corpus, with 13B words based on
130M titles and 80M abstracts from English sci-
entific publications. The resulting model con-
tains 250-dimensional vectors for 2 million words
and phrases. We compare our model with pre-
trained embeddings based on general text (such as
Google News7 and Wikipedia8) and observe that
the model trained from academic corpus performs
better with higher accuracy on the concept-tagging
task with more than 10% margin.

Conceptually, the calculation of publication and
venue’s ERT is to leverage neighbours’ informa-
tion to represent itself. The MAG contains hun-
dreds of millions of nodes with billions of edges,
hence it is computationally prohibitive by optimiz-
ing the node latent vector and weights simultane-
ously. Therefore, in Equation 1 and 2, we initial-
ize hps and h

v
s based on textual feature vectors de-

fined above and adopt empirical weight values to
directly compute hpe and h

v
e to make it scalable.

After calculating the similarity for about 50 bil-
lion pairs, close to 1 billion are finally picked
based on the threshold set by the confidence score.

7https://code.google.com/archive/p/
word2vec/

8https://fasttext.cc/docs/en/
pretrained-vectors.html

Figure 2: Extended subsumption for hierarchy
generation.

2.3 Concept Hierarchy Building
In this subsection, we describe how to build a con-
cept hierarchy based on concept-document tag-
ging results. We extend Sanderson and Croft’s
early work (1999) which uses the notion of
subsumption—a form of co-occurrence—to asso-
ciate related terms. We say term x subsumes y if
y occurs only in a subset of the documents that x
occurs in. In the hierarchy, x is the parent of y.
In reality, it is hard for y to be a strict subset of x.
Sanderson and Croft’s work relaxed the subsump-
tion to 80% (e.g. P (x|y) ≥ 0.8, P (y|x) < 1).

In our work, we extend the concept co-
occurrence calculation weighted with the concept-
document pair’s confidence score from previous
step. More formally, we define a weighted rela-
tive coverage score between two concepts i and j
as below and illustrate in Figure 2.

RC(i, j) =

∑
k∈(I∩J)wi,k∑

k∈I wi,k
−

∑
k∈(I∩J)wj,k∑

k∈J wj,k
(3)

Set I and J are documents tagged with concepts
i and j respectively. I ∩J is the overlapping set of
documents that are tagged with both i and j. wi,k
denotes the confidence score (or weights) between
concept i and document k, which is the final con-
fidence score in the previous concept-publication
tagging stage. When RC(i, j) is greater than a
given positive threshold9, i is the child of j. Since

9It is usually in [0.2, 0.5] based on empirical observation.

https://code.google.com/archive/p/word2vec/
https://code.google.com/archive/p/word2vec/
https://fasttext.cc/docs/en/pretrained-vectors.html
https://fasttext.cc/docs/en/pretrained-vectors.html


91

Figure 3: Deployed system homepage at March
2018, with all six types of entities statistics: over
228K fields-of-study.

this approach does not enforce single parent for
any FoS, it results in a directed acyclic graph
(DAG) hierarchy.

With the proposed model, we construct a six
level FoS hierarchy (from L0 and L5) on over
200K concepts with more than 1M parent-child
pairs. Due to the high visibility, high impact and
small size, the hierarchical relationships between
L0 and L1 are manually inspected and adjusted if
necessary. The remaining L2 to L5 hierarchical
structures are produced completely automatically
by the extended subsumption model.

One limitation of subsumption-based models is
the intransitiveness of parent-child relationships.
This model also lacks a type-consistency check
between parents and children. More discussions
on such limitations with examples will be in eval-
uation section 3.2.

3 Deployment and Evaluation

3.1 Deployment

The work described in this paper has been de-
ployed in the production system of Microsoft Aca-
demic Service10. Figure 3 shows the website
homepage with entity statistics. The contents of
MAG, including the full list of FoS, FoS hierarchy
structure, and FoS tagging to papers, are accessi-
ble via API, website, and full graph dump from
Open Academic Society11.

Figure 4 shows the example for word2vec con-
cept. Concept definition with linked Wikipedia
page, its immediate parents (machine learning,
artificial intelligence, natural language process-
ing) in the hierarchical structure and its related

10https://academic.microsoft.com/
11https://www.openacademic.ai/oag/

Step Accuracy
1. Concept discovery 94.75%
2. Concept tagging 81.20%
3. Build hierarchy 78.00%

Table 2: Accuracy results for each step.

concepts12 (word embedding, artificial neural net-
work, deep learning, etc.) are shown on the
right rail pane. Top tagged publications (without
word2vec explicitly stated in their text) are recog-
nized via graph structure information based on ci-
tation relationship.

3.2 Evaluation

For this deployed system, we evaluate the accu-
racy on three steps (concept discovery, concept
tagging, and hierarchy building) separately.

For each step, 500 data points are randomly
sampled and divided into five groups with 100 data
points each. On concept discovery, a data point
is an FoS; on concept tagging, a data point is a
concept-publication pair; and on hierarchy build-
ing, a data point is a parent-child pair between two
concepts. For the first two steps, each 100-data-
points group is assigned to one human judge. The
concept hierarchy results are by nature more con-
troversial and prone to individual subjective bias,
hence we assign each group of data to three judges
and use majority voting to decide final results.

The accuracy is calculated by counting positive
labels in each 100-data-points group and averag-
ing over 5 groups for each step. The overall accu-
racy is shown in Table 2 and some sampled hierar-
chical results are listed in Table 3.

Most hierarchy dissatisfaction is due to the in-
transitiveness and type-inconsistent limitations of
the subsumption model. For example, most pub-
lications that discuss the polycystic kidney disease
also mention kidney; however, for all publications
that mentioned kidney, only a small subset would
mention polycystic kidney disease. According to
the subsumption model, polycystic kidney disease
is the child of kidney. It is not legitimate for a dis-
ease as the child of an organ. Leveraging the en-
tity type information to fine-tune hierarchy results
is in our plan to improve the quality.

12Details about how to generate related entities are out-of-
scope and not included in this paper.

https://academic.microsoft.com/
https://www.openacademic.ai/oag/


92

Figure 4: Word2vec example, with its parent FoS, related FoS and top tagged publications.

L5 L4 L3 L2 L1 L0
Convolutional Deep Deep belief Deep Artificial Machine Computer

Belief Networks network learning neural network learning Science
(Methionine synthase) Methionine Amino Biochemistry / Chemistry /

reductase synthase Methionine acid Molecular biology Biology
(glycogen-synthase-D) Phosphorylase Glycogen

phosphatase kinase synthase Glycogen Biochemistry Chemistry
Fréchet Generalized extreme Extreme

distribution value distribution value theory Statistics Mathematics
Hermite’s Hermite Spline Mathematical
problem spline interpolation Interpolation analysis Mathematics

Table 3: Sample results for FoS hierarchy.

4 Conclusion

In this work, we demonstrated a Web-scale pro-
duction system that enables an easy exploration of
scientific knowledge. We designed a system with
three modules: concept discovery, concept tagging
to publications, and concept hierarchy construc-
tion. The system is able to cover latest scientific
knowledge from the Web and allows fast iterations
on new algorithms for document and language un-
derstanding.

The system shown in this paper builds the
largest cross-domain scientific concept ontology
published to date, and it is one of the core
components in the construction of the Microsoft
Academic Graph, which is a publicly available
academic knowledge graph—a data asset with
tremendous value that can be used for many tasks
in domains like data mining, natural language un-
derstanding, science of science, and network sci-
ence.

References
Yuxiao Dong, Hao Ma, Zhihong Shen, and Kuansan

Wang. 2017. A century of science: Globalization of

scientific collaborations, citations, and innovations.
In KDD, pages 1437–1446.

Xueqing Liu, Yangqiu Song, Shixia Liu, and Haixun
Wang. 2012. Automatic taxonomy construction
from keywords. In KDD, pages 1433–1441.

Tomas Mikolov, Ilya Sutskever, Kai Chen, Gregory S.
Corrado, and Jeffrey Dean. 2013. Distributed repre-
sentations of words and phrases and their composi-
tionality. In NIPS, pages 3111–3119.

David Milne and Ian H. Witten. 2008. An effective,
low-cost measure of semantic relatedness obtained
from wikipedia links. WIKIAI, pages 25–30.

Arvind Neelakantan and Ming-Wei Chang. 2015. In-
ferring missing entity type instances for knowledge
base completion: New dataset and methods. In
NAACL, pages 515–525.

Mark Sanderson and W. Bruce Croft. 1999. Deriving
concept hierarchies from text. In SIGIR, pages 206–
213.

Arnab Sinha, Zhihong Shen, Yang Song, Hao Ma, Dar-
rin Eide, Bo-June Paul Hsu, and Kuansan Wang.
2015. An overview of microsoft academic service
(mas) and applications. In WWW, pages 243–246.


