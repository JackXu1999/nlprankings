



















































Using Coreference Links to Improve Spanish-to-English Machine Translation


Proceedings of the 2nd Workshop on Coreference Resolution Beyond OntoNotes (CORBON 2017), co-located with EACL 2017, pages 30–40,
Valencia, Spain, April 4, 2017. c©2017 Association for Computational Linguistics

Using Coreference Links to Improve
Spanish-to-English Machine Translation

Lesly Miculicich Werlen and Andrei Popescu-Belis
Idiap Research Institute
Rue Marconi 19, CP 592

1920 Martigny, Switzerland
{lmiculicich,apbelis}@idiap.ch

Abstract

In this paper, we present a proof-of-
concept of a coreference-aware decoder
for document-level machine translation.
We consider that better translations should
have coreference links that are closer to
those in the source text, and implement
this criterion in two ways. First, we define
a similarity measure between source and
target coreference structures, by project-
ing the target ones onto the source ones,
and then reusing existing monolingual co-
reference metrics. Based on this similar-
ity measure, we re-rank the translation hy-
potheses of a baseline MT system for each
sentence. Alternatively, to address the lack
of diversity of mentions among the MT
hypotheses, we focus on mention pairs and
integrate their coreference scores with MT
ones, resulting in post-editing decisions.
Experiments with Spanish-to-English MT
on the AnCora-ES corpus show that our
second approach yields a substantial in-
crease in the accuracy of pronoun transla-
tion, while BLEU scores remain constant.

1 Introduction

Considering entire texts for machine translation,
rather than separate sentences, has the potential to
improve the consistency of the translations. In this
paper, we focus on coreference links, which con-
nect referring expressions that denote the same en-
tity within or across sentences. As perfect transla-
tions should provide the reader the same under-
standing of entities as the source texts, we pro-
pose to use the similarity of coreference links be-
tween a source text and its translation as a crite-
rion to improve translation hypotheses. This in-
formation should be beneficial to the translation of

pronouns, which often depends on the properties
of their antecedent, but should also ensure lexical
consistency in the translation of coreferent nouns.

We provide here the first proof-of-concept
showing that the coreference criterion can lead
to measurable improvements in the translation of
referring expressions, in the case of Spanish-to-
English machine translation (MT). To implement
this criterion, we need to compute first the core-
ference links in the source and target texts. Then,
we propose and compare two approaches: either
computing a global coreference score by compar-
ing the links and using it to rerank the hypothe-
ses of an MT system; or integrating mention-pair
scores from a coreference resolution system with
MT scores, and post-editing each mention to max-
imize the total score.

The paper is organized as follows. In Section 2,
we present an overview of related work on core-
ference and anaphora resolution and MT. In Sec-
tion 3, we explain how we compute source and
target-side coreference links, respectively by tak-
ing advantage of gold standard coreference links
on the Spanish AnCora-ES corpus, and using the
Stanford Coreference Resolution system on the
English MT output – for both coreference-aware
MT methods that we present. In Section 4, we
compare coreference links globally by projecting
the referring expressions (mentions) from target
to source texts, and measuring similarity with ex-
isting coreference resolution metrics (MUC, B3,
CEAF). As a sanity check, in Section 4.2, we
show that better translations, in the sense of higher
BLEU scores, exhibit higher coreference similar-
ity scores as well. Global coreference similar-
ity is then used in Section 4.3 as a constraint to
rerank hypotheses of the Moses MT decoder. Al-
ternatively, as the top MT hypotheses do not vary
enough in terms of mentions, we propose in Sec-
tion 5 a different method, which focuses only on

30



the translation variants of the mentions, and post-
edits them using information from coreference
chains in the source text. Finally, the results pre-
sented in Section 6 show that the second method
increases the accuracy of pronoun translation from
Spanish to English, while obtaining BLEU scores
similar to those of the MT baseline.

2 Related Work

2.1 Coreference Resolution and Evaluation

Coreference resolution is the task of grouping to-
gether the expressions that refer to the same entity
in a text. This task includes two stages: mention
identification, and coreference resolution. The
first stage is usually based on part-of-speech an-
notation and named-entity recognition. Candidate
mentions are usually noun phrases, pronouns, and
named entities (Lee et al., 2011). Coreference re-
solvers follow three main approaches: pairwise,
re-ranking, and clustering. Pairwise resolvers per-
form a binary classification, predicting if two men-
tions refer to the same entity or not. This as-
sumes strong independence of mentions and does
not utilize features of the entire entity (Bengtson
and Roth, 2008). The second approach lists a set
of candidate antecedents for each mention that are
simultaneously considered to find the best match.
Interpolation between the best and worse candi-
date is considered (Wiseman et al., 2015; Bengt-
son and Roth, 2008). Finally, the clustering ap-
proach considers the features of a complete cluster
of mentions to decide whether a mention belongs
or not to a cluster (Clark and Manning, 2015; Fer-
nandes et al., 2012).

Coreference resolution is typically evaluated
in comparison with a gold-standard annotation
(Popescu-Belis, 1999; Recasens and Hovy, 2011).
The main metrics used for evaluation are MUC
(Vilain et al., 1995), which counts the minimum
number of links between mentions to be inserted
or deleted in order to map the evaluated document
to the gold-standard. The B3 measure (Bagga and
Baldwin, 1998) computes precision and recall for
all mentions of a document, while CEAF (Luo,
2005) computes them at the entity level. BLANC
(Recasens and Hovy, 2011) makes use of the Rand
Index, an algorithm for the evaluation of cluster-
ing. These metrics are implemented in the scorer
for CoNLL 2012 (Pradhan et al., 2014) and the
SemEval 2013 one (Màrquez et al., 2013).

2.2 Coreference-Aware Machine Translation

Despite the numerous coreference and anaphora
resolution systems designed in the past decades
(Mitkov, 2002; Ng, 2010), the interest in using
them to improve pronoun translation has only re-
cently emerged (Le Nagard and Koehn, 2010;
Hardmeier and Federico, 2010; Guillou, 2012).
The still limited accuracy of coreference resolu-
tion may explain its restricted use in MT, although,
it has long been known that some pronouns require
knowledge of the antecedent for correct transla-
tion. For instance, Le Nagard and Koehn (2010)
trained an English-French translation model on an
annotated corpus in which each occurrence of the
English pronouns it and they was annotated with
the gender of its antecedent on the target side.
Their system correctly translated 40 pronouns out
of the 59 that they examined, but did not outper-
form the MT baseline. Recently, a model for MT
decoding proposed by Luong (2016; 2017) com-
bined several features of the antecedent candidates
(gender, number and humanness) with an MT de-
coder, in a probabilistic way, and demonstrated
improvement on pronouns.

Two shared tasks on pronoun-focused transla-
tion have been recently organized. The improve-
ment of pronoun translation was only marginal
with respect to a baseline SMT system in the 2015
shared task (Hardmeier et al., 2015), while the
2016 shared task was only aiming at pronoun pre-
diction given source texts and lemmatized refer-
ence translations (Guillou et al., 2016). Some of
the best systems developed for these tasks avoided,
in fact, the direct use of anaphora resolution (with
the exception of Luong et al. (2015)). For exam-
ple, Callin et al. (2015) designed a classifier based
on a feed-forward neural network, which consid-
ered as features the preceding nouns and determin-
ers along with their part-of-speech tags. The win-
ning systems of the 2016 task used deep neural
networks: Luotolahti et al. (2016) and Dabre et
al. (2016) summarized the preceding and follow-
ing contexts of the pronoun to predict and passed
them to a recurrent neural network. To the best of
our knowledge, we present here the first proof-of-
concept that coreference links across noun phrases
and pronouns can serve to improve statistical MT.

3 Coreference Resolution for MT

A principle of translation is that the information
conveyed in a document should be preserved in

31



Source Human Translation Machine Translation

La pelı́cula narra la historia de
[un joven parisiense]c1 que mar-
cha a Rumanı́a en busca de
[una cantante zı́ngara]c2 , ya que
[su]c1 fallecido padre escuchaba
siempre [sus]c2 canciones.

The film tells the story of [a
young Parisian]c1 who goes to
Romania in search of [a gypsy
singer]c2 , as [his]c1 deceased
father use to listen to [her]c2
songs.

The film tells the story of [a
young Parisian]c1 who goes to
Romania in search of [a gypsy
singer]c2 , as [his]c2 deceased
father always listened to [his]c2
songs.

Pudiera considerarse un viaje
fallido, porque [∅]c1 no encuen-
tra [su]c1 objetivo, pero el azar
[le]c1 conduce a una pequea co-
munidad...

It could be considered a failed
journey, because [he]c1 does
not find [his]c1 objective, but
the fate leads [him]c1 to a small
community...

It could be considered [a failed
trip]c3 , because [it]c3 does not
find [its]c3 objective, but the
chance leads ∅ to a small com-
munity...

Table 1: Comparison of coreference chains in the Spanish source vs. English human and machine trans-
lations. English chains were obtained with the Stanford coreference resolver (Manning et al., 2014). The
chains are numbed c1, c2, . . . and are also color-coded. The void symbol ∅ indicates a correct null subject
pronoun in Spanish, and an incorrect object pronoun dropped by the MT system. The third coreference
chain (c3) in the MT output is erroneous.

its translation. Here, we focus on the referen-
tial information, i.e. the coreference links between
mentions. If we apply coreference resolution to a
source text and to a faithful translation of it, then
the grouping of mentions should be identical. We
thus formulate the following criterion for MT: bet-
ter translations should have coreference links that
are more similar to the source.

Table 1 illustrates the above criterion on an ex-
ample of Spanish-to-English translation, extracted
from the AnCora-ES corpus (Recasens and Martı́,
2010),1 with source coreference chains coming
from the AnCora-ES annotations. The automatic
translation comes from a commercial online MT
system, while the human translation was done by
the authors of this paper. The Stanford Statistical
Coreference Resolution system (Clark and Man-
ning, 2015)2 was applied to both translations, and
the resulting coreference chains are indicated in
the table with numbers and colors. We observe
that the chains in the human translation match well
those in the source, but this is less the case for the
automatic translation, in particular due to wrong
pronoun translations. Although the MT output is
still understandable, this requires more time than
with the human translation, due to the wrong set
of coreference links inferred by the reader.

In what follows, we will implement a proof-
of-concept coreference-aware MT system for

1http://clic.ub.edu/corpus/
2http://stanfordnlp.github.io/CoreNLP/

coref.html

Spanish-to-English translation. This pair is partic-
ularly challenging because Spanish is a pro-drop
language, so that an MT system must not only se-
lect the correct translation of pronouns, but it must
also generate English pronouns from Spanish null
ones. In this study, in order to avoid introducing
errors made by the coreference resolution system,
we will always use on the source side the gold-
standard coreference annotation from AnCora-ES
(Recasens and Martı́, 2010), which was used in
the SemEval-2010 Task 1 on coreference resolu-
tion in multiple languages (Recasens et al., 2010).3

As our proposal does not require specific training
on coreference-annotated data, AnCora-ES will be
used for testing only.

On the target side, as coreference resolution
must be performed for each translation hypothe-
sis, we must use an automatic system. One ad-
vantage of the Spanish-to-English direction is that
English coreference resolution systems have been
studied and developed for a long time, more than
any other language, thus keeping coreference er-
rors to a minimum. We use again the Stanford Sta-
tistical Coreference Resolution system proposed
by Clark and Manning (2015). Moreover, to ob-
tain pairwise mention scores, needed in Section 5,
we use the code of the pairwise classifier available
with the source code of the Stanford CoreNLP
toolkit (Manning et al., 2014)4.

3http://stel.ub.edu/semeval2010-coref/
4Source class ‘edu.stanford.nlp.scoref.PairwiseModel’ at

http://stanfordnlp.github.io/CoreNLP/.

32



4 Using Coreference Similarity to
Rerank MT Hypotheses

4.1 Measuring Coreference Similarity
After applying coreference resolution to the
source and a candidate translation, we need to
compare the sets of coreference links, with the
source playing the role of the ground-truth or gold-
standard. Traditional metrics for evaluating core-
ference resolution could be used, but they have
been designed to compare texts in the same lan-
guage, and not across different languages, which
raises difficulties for matching the referring ex-
pressions (i.e. mentions, or markables).

We propose to project the mentions of the target
text back to the source text, so that each word in
the source is aligned with its corresponding trans-
lation (one or more words). This alignment can be
obtained directly from the Moses MT system (see
start of Section 4.3).

There is not always a one-to-one word cor-
respondence between the words in the source
and target sentences, and word order also differs.
Thus, we apply the following heuristic to improve
the cross-language mapping of the mentions. As
through word-alignment the words that comprise
the mentions may have changed order in the trans-
lation, we take the first and last words in the target
side, aligned to any word of the mention in the
source, and we assume that all words in between
are also part of the mention. The null pronouns are
transfer to the next immediate verb, and we refine
the alignment to be sure these verbs are aligned to
the generated pronoun in the target.

Once the target mentions are mapped to the
source, we apply the MUC, B3 and CEAF-m co-
reference similarity metrics from the CoNLL 2012
scorer (see Section 2.1) between the source docu-
ment ds and the projected target one dt. To mit-
igate individual variations, we use the average of
the three scores at the similarity criterion and note
itCsim(dt, ds). We did not include BLANC in this
pool based on initial experiments that showed that
its rate of variation was much higher than the other
three metrics.

4.2 Validating the Relationship between
Coreference and Translation Quality

To validate the insight that better translations cor-
relate with better coreference similarity scores, we
present in Table 2 the MUC, B3 and CEAF scores
of a human translation vs. two systems: the Moses

baseline phrase-based MT system used below and
an online commercial MT system using neural net-
works. The source is a set of documents with
ca. 3.5 thousand words with gold-standard core-
ference annotation from AnCora-ES. The English
translation was done by the authors of the paper.
On the target side, we applied the Stanford auto-
matic coreference resolution system (Manning et
al., 2014).

By definition, the best translation is made by
the human. Then, according BLEU score mea-
sured on the same set of documents, the second
best translation is made by the commercial MT
with 49.4, and the last one by the baseline MT
with 43.7. We observe that the coreference scores
also decrease in this order, and they decrease con-
sistently for the three evaluation metrics. These
results thus support the principle that translation
quality and coreference similarity are correlated.
We will now show how to use this principle to im-
prove translation quality.

Metric Translation Recall Prec. F1
MUC Human 31 46 37

Commercial MT 21 38 28
Baseline MT 18 33 23

B3 Human 24 49 32
Commercial MT 20 38 26
Baseline MT 17 40 24

CEAF Human 41 40 41
Commercial MT 34 39 36
Baseline MT 32 35 33

Table 2: Coreference similarity scores (%) be-
tween source and target texts for different trans-
lations. The scores increase with the quality of
translations.

4.3 Reranking MT Hypotheses

We propose to use the document-level coreference
similarity score Csim defined above to rerank for
each sentence the n-best hypotheses of an MT sys-
tem. The coreference similarity is not measured
individually for each sentence, but at the document
level. Our goal is to find a combination of transla-
tions that optimizes this global score.

For this purpose, we use the Moses toolkit to
build a phrase-based statistical MT system (Koehn
et al., 2007), with training data from the transla-
tion task of the WMT 2013 workshop (Bojar et
al., 2013). The English-Spanish training set con-
sists of 14 million sentences, with approximately
340 million tokens. The tuning set is the News
Test 2010-2011 one, with ca. 5,500 sentences and

33



almost 120k tokens. We built a 4-gram language
model from the same training data augmented by
ca. 5,500 sentences monolingual data from News
Test 2015. Our baseline system has a BLEU score
of 30.8 on the News Test 2013 with 3,000 sen-
tences.

We thus model the problem as follows. A trans-
lated document dt is represented as an array of
translations dt = (s1, s2, ..., sM ), where each sen-
tence can be selected from a list of n-best transla-
tion hypotheses si ∈ {si1, si2, ..., siN}. The objec-
tive is to select the best combination of hypotheses
based on their coreference similarityCsim with the
source, i.e.:

arg max
h1,h2,..,hM

Csim((s1h1 , s
2
h2 , ..., s

m
hM

), ds)

To limit the decrease of sentence-level translation
scores when optimizing the document-level objec-
tive, we keep track of the former and select the
sentences with the best translation scores if they
lead to the same Csim.

This combinatorial problem is expensive, so we
try to reduce the search space to allow reasonable
performance. First, we filter out candidate sen-
tences. In this approach, the important variations
in translation are the mentions, thus sentences are
modeled as sets of mentions and duplicate sets are
filtered out. Second, we apply beam search opti-
mization. Based on the fact that the first mentions
of entities usually contain more information than
the next ones, the beam search starts from the first
sentence and aggregates at each step the transla-
tion hypothesis with the highest similarity scores
with the preceding ones.

We foresee several limitations of this approach.
First, with a sentence containing several mentions,
there is no guarantee that the n-best hypotheses
include a combination of mention translations that
optimize all mentions as the same time. What is
worse, the correct translation of a given mention
may not be present at all among the n-best hy-
potheses, because the differences among the top
hypotheses are often very small, especially when
sentences are long. In order to solve these prob-
lems, we present a second approach.

5 Post-editing Mentions Based on
Mention Pair and MT Scores

This approach differs from the previous one in two
aspects. First, it uses hypotheses of translation of

individual coreferent mentions rather than of com-
plete sentences. This allows to optimize the trans-
lation of each mention independently, and to in-
crease the variety of hypotheses of each mention.
Second, coreference resolution is applied only in
the source side. So, instead of searching for sim-
ilar clustering in the target side, we try to induce
it. The selection of the best translation hypoth-
esis of a mention is based on a cluster-level co-
reference score. We choose the hypothesis that
correlates better with other mentions in the same
cluster. This method improves the performance
because it uses coreference resolution only once
instead of multiple times, and as shown in the ex-
perimental section, it is more effective at improv-
ing the translation of mentions.

5.1 Selecting Candidate Translations

In order to obtain the n-best translation hypothe-
ses of the mentions, it is important to include the
surrounding context in the translation, otherwise,
an independent translation could lead to the con-
struction of invalid or erroneous sentences.

We would like to have a MT system that brings
hypotheses corresponding only to mentions and
fix the translations of other word, in a way that
we can interchange the hypotheses of one mention
in the same text. Building such MT system would
require a significant modification of the baseline.

As an alternative solution, we will simply per-
form two passes of MT. The first pass is a sim-
ple translation of the text. Then, the mentions are
identified in the target text and they are replaced
by their source-language version. This results into
a mixed language text that will be passed a sec-
ond time to the MT system, so that the system will
identify and translate only the words in the source
language. Nevertheless, the language and reorder-
ing models are still going to evaluate on the com-
plete sentence. To avoid any translation of the con-
text words (i.e. not mentions) in the second pass,
we filter out from the translation table all words
not corresponding to mentions.

It is important to note that we consider only the
heads of mentions obtained from the parse tree
(this annotation is included in AnCora corpus), in
order to avoid long mentions such as the ones with
subordinate clauses, and focus on the most impor-
tant part of each mention.

34



5.2 Cluster-level Coreference Score
In this approach, we rely on the coreference re-
solver applied to the source side to define the clus-
ters of mentions. Each cluster is defined as a set
of mentions cx = {mi,mj , ..,mk}, where each
mention can be selected from a set of translation
hypotheses mi ∈ {mi1,mi2, ...,miN}.

By definition, the mentions in a cluster repre-
sent the same entity. Thus, they have to correlate
in features such as gender, number, animation, etc.
In order to achieve this objective in the target side,
we define a cluster-level coreference score Css. It
represents the likelihood that all mentions in that
cluster belong to the same entity. So, for each
given cluster, we select the combination of trans-
lation hypotheses of mentions with higher cluster-
level coreference score.

This combinatorial problem is expensive, there-
fore, it is simplified with a beam search approach.
Mentions are processed one at a time. The transla-
tion hypotheses of a new upcoming mention are
compared with each of the previously selected
ones. Then, the combinations with lower Css are
pruned. The algorithm continue in the same man-
ner until it processes the last mention.

In order to compare two mentions, we use the
mention pair scorer from (Clark and Manning,
2015). It uses a logistic classifier to assign a prob-
ability to a pair of hypotheses, which represents
the likelihood that they are coreferent. The pair
score is defined as follows:

ppair(mihi ,m
j
hj

) = (1 + e
θT f(mihi

,mjhj
)
)−1

where f(mihi ,m
j
hj

) is a vector of feature func-
tions of the mentions and θ is the vector of feature
weights. Finally, we define the cluster-level core-
ference score Css as the product of the individual
pairwise probabilities:

Css(cx) =
∏
mi∈cx

∏
mi6=j∈cx

ppair(mihi ,m
j
hj

)

We illustrate this idea with an example. Here,
we have a sentence in Spanish and its translation
to English. We show one coreference cluster c1
formed by three mentions:

Source (es): La alcaldesa de Málaga y cabeza del
[partido]c1 [que]c1 ganó en esta ciudad, pidió a los
militantes de [este partido polı́tico]c1 ...
Target (en): The mayor of Malaga and head of the
[m1]c1 [m2]c1 won in this city, asked the militants
of this [m3]c1 to...

In this example, the three marked mentions
have the following translation hypotheses: m1 ∈
{match, party}, m2 ∈ {who,which}, and
m3 ∈ {political party}. We calculate the pair-
wise score ppair of each combination and show the
results in the following table.

m1, m2 (match, who) = 0.03, (match, which) = 0.35,
(party, who) = 0.01, (party, which) = 0.26

m1, m3 (match, political party) = 0.08,
(party, political party) = 0.53

m2, m3 (political party, who) = 0.12,
(political party, which) = 0.27

Finally, we find that the set of translation hy-
potheses with the highest cluster-level coreference
Css score is {‘party’, ‘which’, ‘political party’},
with a score of 0.04. Intuitively, we can verify that
this final combination is the best solution for the
example.

5.3 Incorporating Entity and Translation
Information

The proposed score guides the system to select
translation hypotheses which are more likely to
refer to the same entity in a cluster. In order
to enhance the decision process, we include two
sources of additional information: the translation
frequency, that can help to decide between syn-
onyms by selecting the most frequently translated
one; and information of the entity in the source
side, which enriches the knowledge of the entity.

The information about frequency of transla-
tion can indicate how well a particular hypothe-
sis translates the mention. Therefore, we define a
translation score, Ts, at mention-level. The trans-
lation score of a hypothesis is calculated based on
its relative frequency of emission by the MT sys-
tem, as follows:

Ts(mihi) = count(m
i
hi)/

∑
j

count(mij)

The information about the entity in source side
can indicate how well a particular hypothesis rep-
resents it. Thus, we define a simple representation
of an entity by setting relevant features such as
gender, number, and animation. The features are
extracted and summarized from all mentions in the
cluster. This is a naive representation, and more
advanced work on entity-level representations has
been performed in relation to coreference resolu-
tion (Clark and Manning, 2016; Wiseman et al.,
2016), which could be applied here in the future.

35



Having an entity representation, we define a
simple scoring function which measures how well
a candidate represents an entity with respect to
other alternatives:

Es(mihi = f(m
i
hi
, θex)/

∑
j

f(mij , θex)

where f is a linear function and θex are the entity
features.

5.4 Combining Scores
Finally, the decision is made through the combina-
tion of the three previous scores: cluster-level co-
reference, translation, and entity matching. As one
additional step, we adjust the coreference score to
the same scale as others:

Cs = Css(mihi ,m
j
hj
, . . .)/

∑
x,y,...

Css(mix,m
j
y, . . .).

The final score is defined as follows:

Cscore(mihi ,m
j
hj
, . . .) =Cs(mihi ,m

j
hj
, . . .)λ1×

[Ts(mihi).Ts(m
j
hj

) . . .]λ2×
[Es(mihi).Es(m

j
hj

) . . .]λ3

where
∑

i λi = 1 are predefined hyper-parameters
of the function. The final set is given by:

(mi,mj , . . .) = arg max
hi,hj ,...

Cscore(mihi ,m
j
hj
, . . .).

These three hyper-parameters were optimized on
a different subset of AnCora-ES than the one used
for evaluation. The optimized values are λ1=0.5,
λ2=0.1, and λ3=0.4.

6 Experimental Results

The objective of our initial experiments is to mea-
sure how much coreference can improve the cor-
rect choices of translation of mentions, and impact
of these choices on global translation quality. We
translated 10 sample documents from the test set
to serve as reference translations for evaluation.

6.1 Evaluation with Automatic Metrics
The evaluation of global MT quality is made with
the well-known BLEU n-gram precision metric
(Papineni et al., 2002), while the evaluation of
mentions, being less standardized, is performed in
several ways. We reuse previous insights on pro-
noun translation and therefore score them with a

Metric
System BLEU APT ANT
Baseline
PBSMT 46.5±4.3 0.35±0.07 0.78±0.08
Baseline
NMT 46.9±3.7 0.37±0.07 0.78±0.07
PBSMT +
Re-rank 41.7±3.9

∗∗∗ 0.40±0.10∗ 0.74±0.01∗∗
PBSMT +
Post-edit 46.4±3.9 0.59±0.13

∗∗∗ 0.78±0.07
PBSMT +
Post-edit +
Automatic
coreference

46.1±4.3 0.41±0.07∗ 0.76±0.09

Table 3: Comparison of baseline MT and our pro-
posals for reranking or post-editing, for three met-
rics. In addition to the average scores and stan-
dard deviation over the ten test documents, we in-
dicate the statistical significance level of the differ-
ence between each of our systems and the baseline
(∗ for 95.0%, ∗∗ for 99.0% and ∗∗∗ for 99.9%).

metric that automatically computes the accuracy
of pronoun translation (APT) in terms of number
of pronouns that are identical vs. different from
a human reference translation (Miculicich Werlen
and Popescu-Belis, 2016)5.

More originally, in order to provide a complete
view of the performance, we compute the “accu-
racy of noun translation” (ANT), by reusing the
same idea as in APT to count the number of ex-
actly matched nouns between MT and the refer-
ence translation.

We test the two proposed methods re-ranking
and post-editing vs. the phrase-based statistical
MT (PBSMT) baseline described in Section 4.3.
We also include a neural machine translation
(NMT) baseline (Bahdanau et al., 2015) as a ref-
erence for comparison. We chose to build our sys-
tems over a PBSMT system for simplicity, because
the word-alignment can be obtained directly from
the system. Additionally, we also present the re-
sults obtained with an automatic coreference re-
solver in the source side, namely the CorZu system
(Tuggener, 2016; Rios, 2015), for the post-editing
approach.

Table 3 shows the results of the experiments.
We first calculate BLEU, APT, and ANT values
at document-level, and show the values of the av-
erage and standard deviation for the three evalu-
ated systems: baseline, and our two proposed ap-
proaches. Additionally, we show the significance
levels (t-test) of the results in comparison to the

5https://github.com/idiap/APT

36



Figure 1: Pronoun translation in comparison with
the reference: numbers of equal vs. different pro-
nouns for the three systems, including also miss-
ing pronouns in target, reference, and both sides
(counts based on source pronouns).

baseline. The post-editing approach improves the
pronoun translation quite significantly, without de-
creasing the overall quality of translation. This
improvement is demonstrated by the rise of APT
score, whereas BLUE score remains without sig-
nificant change. However, the quality of the trans-
lation of nouns does not change significantly, as
shown by the ANT.

The re-ranking approach shows a significant in-
crease in the quality of pronoun translation. Nev-
ertheless, the overall quality of translation de-
creases significantly, as well as the quality of noun
translation. These results can be explained by
the limitations of this approach. The optimization
was done by taking into account the correlation of
mentions, but the changes were made at sentence
level, and the overall quality of translation at sen-
tence level was not considered. To address this
problem, a combination of coreference similarity
and translation probability for each sentence could
be used in future.

Figure 1 shows the distribution of pronouns
translated by the three evaluated systems (i.e.
baseline, re-ranking, and post-editing) in compar-
ison with the reference. The number of pronouns
equal to the reference increases for both proposed
approaches, specially for the post-editing. The
pronouns that improve the most were the third-
person personal and possessive ones. Also, the
translation of some of the null pronouns in the
source was improved. The association with other
mentions of the same entity, and the representa-
tion of the entity coming from the source side was
important for this improvement.

System
Evaluation Baseline Re-rank Post-edit
No. ‘0’ (wrong) 53 55 21
No. ‘1’ (acceptable) 21 19 28
No. ‘2’ (eq. to ref.) 115 115 140
Sum of the scores 251 249 308

Table 4: Manual evaluation of fourth randomly se-
lected documents. The evaluation was done over
nouns and pronouns.

6.2 Human Evaluation

Finally, we perform manual evaluation by exam-
ining source mentions, as annotated over AnCora-
ES, and evaluating their individual translations by
the baseline MT along with the two approaches
presented above (in Sections 4 vs. 5). When pre-
sented to the evaluator, the three translations of
each source sentence are provided in a random or-
der, so that the evaluator does not know to which
system they belong. The evaluator assigned a
score of ‘2’ to a translation identical to the ref-
erence, ‘1’ for translation that is different but still
good or acceptable, and ‘0’ to a wrong or unac-
ceptable translation. To minimize the time spent
on manual evaluation at this stage, one evaluator
rated four test documents.

Table 4 shows the results of the manual evalu-
ation, scored as explained above, which includes
nouns and pronouns together. In general, it sup-
ports the results of the automatic evaluation. Here,
the post-editing approach has 32 less mentions
scored as “wrong” than the baseline, 7 of them
were score as “acceptable”, and the rest 25 as iden-
tical to the reference. The re-ranking approach,
despite the theoretical appeal of its definition, fails
to improve noun and pronoun translation.

Table 5 shows examples of translations obtained
with our approaches. The translations of nouns are
already good for the baseline, and the differences
are in many cases due to the use of synonyms and
acronyms. Still, there are source nouns that suf-
fer from sense ambiguity, which may be improved
by our method. However, this particular test set is
too small and does not contain enough instances
of this type to evaluate their translations with cer-
tainty.

7 Conclusion

We have presented two methods for improving
noun and pronoun translation based on corefe-
rence similarity of source and translated texts.

37



Correctly modified examples
S: [Barton]3 , por [su]3 parte , también dudó de la ca-
pacidad de [Megawati]2 en [su]3 [nueva tarea]4 .
R: [Barton]3 , for [his]3 part , also doubted [Megawati]2
’s ability in [her]2 [new task]4 .
B: [Barton]3 , for [its]3 part , also doubted the capacity
of Megawati in [his]2 [new task]4 .
P: [Barton]3 , for [his]3 part , also doubted the capacity
of [Megawati]2 in [her]2 [new task]4 .

S: ... que “ [parece estar]2 abrumada ... crı́ticos con-
sideran que [no será]2 capaz de hacerse con el papel de
lı́der .
R: ...that “ [she seems]2 overwhelmed ... critics consider
[she will not be]2 able to take the lead role .
B: ... that “ [appears to be]2 overwhelmed ... critics
believe that [it will not be]2 able to take a leading role .
P: ...that “ [she seems]2 to be overwhelmed ... critics
believe that [she will not be]2 able to take a leading role
.
Incorrectly modified example
S: - [Es]1 iconoclasta por valenciano ? - .
R:: - [Are you]1 iconoclastic by Valencian ? - .
B: - [Is]1 an iconoclast by Valencian ? - .
P: - [he is]1 an iconoclast by Valencian ? - .

Table 5: Examples of source, reference, baseline
and post-edited sentences.

While the re-ranking approach did not achieve its
goals, the post-editing approach brought a sig-
nificant improvement of Spanish-to-English pro-
noun translation. This should be confirmed, in
the future, by more detailed measurements on
larger data sets. Also, one simplifying assumption,
namely the use of ground-truth coreference anno-
tation on the target side (here, from AnCora-ES)
should be relaxed, in order to address the chal-
lenge of using automated coreference resolution
on both source and target sides – and thus produce
a fully-automated, unrestricted MT system.

This study contributes to a growing body of
research on modeling longer range dependencies
than those modeled in phrase-based or neural MT,
across different sentences of a document. The
Docent decoder (Hardmeier et al., 2012), which
uses document-level features to improve coher-
ence across translated sentences, could also be
used in combination with the coreference simi-
larity score, or, alternatively, neural MT could be
adapted to take advantage of neural network rep-
resentations of coreference information.

Acknowledgments

We are grateful for support to the Swiss Na-
tional Science Foundation (SNSF) under the Sin-
ergia MODERN project (grant n. 147653, see

www.idiap.ch/project/modern/) and to
the European Union under the Horizon 2020
SUMMA project (grant n. 688139, see www.
summa-project.eu). We thank the COR-
BON anonymous reviewers for their helpful sug-
gestions.

References
Amit Bagga and Breck Baldwin. 1998. Algorithms

for scoring coreference chains. In Proceedings of
the First International Conference on Language Re-
sources and Evaluation Workshop on Linguistics
Coreference, volume 1, pages 563–566, Granada,
Spain.

Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Ben-
gio. 2015. Neural machine translation by jointly
learning to align and translate. In Proceedings of
the International Conference on Learning Represen-
tations (ICLR), San Diego, USA.

Eric Bengtson and Dan Roth. 2008. Understand-
ing the value of features for coreference resolution.
In Proceedings of the 2008 Conference on Empiri-
cal Methods in Natural Language Processing, pages
294–303, Honolulu, Hawaii, October. Association
for Computational Linguistics.

Ondřej Bojar, Christian Buck, Chris Callison-Burch,
Christian Federmann, Barry Haddow, Philipp
Koehn, Christof Monz, Matt Post, Radu Soricut, and
Lucia Specia. 2013. Findings of the 2013 Workshop
on statistical machine translation. In Proceedings of
the Eighth Workshop on Statistical Machine Trans-
lation, pages 1–44, Sofia, Bulgaria, August. Associ-
ation for Computational Linguistics.

Jimmy Callin, Christian Hardmeier, and Jörg Tiede-
mann. 2015. Part-of-speech driven cross-lingual
pronoun prediction with feed-forward neural net-
works. In Proceedings of the Second Workshop
on Discourse in Machine Translation, pages 59–64,
Lisbon, Portugal, September. Association for Com-
putational Linguistics.

Kevin Clark and Christopher D. Manning. 2015.
Entity-centric coreference resolution with model
stacking. In Proceedings of the 53rd Annual Meet-
ing of the Association for Computational Linguistics
and the 7th International Joint Conference on Natu-
ral Language Processing (Volume 1: Long Papers),
pages 1405–1415, Beijing, China, July. Association
for Computational Linguistics.

Kevin Clark and Christopher D. Manning. 2016. Im-
proving coreference resolution by learning entity-
level distributed representations. In Proceedings of
the 54th Annual Meeting of the Association for Com-
putational Linguistics (Volume 1: Long Papers),
pages 643–653, Berlin, Germany, August. Associ-
ation for Computational Linguistics.

38



Raj Dabre, Yevgeniy Puzikov, Fabien Cromieres, and
Sadao Kurohashi. 2016. The Kyoto University
cross-lingual pronoun translation system. In Pro-
ceedings of the First Conference on Machine Trans-
lation, pages 571–575, Berlin, Germany, August.
Association for Computational Linguistics.

Eraldo Fernandes, Cı́cero dos Santos, and Ruy Milidiú.
2012. Latent structure perceptron with feature in-
duction for unrestricted coreference resolution. In
Joint Conference on EMNLP and CoNLL - Shared
Task, pages 41–48, Jeju Island, Korea, July. Associ-
ation for Computational Linguistics.

Liane Guillou, Christian Hardmeier, Preslav Nakov,
Sara Stymne, Jörg Tiedemann, Yannick Vers-
ley, Mauro Cettolo, Bonnie Webber, and Andrei
Popescu-Belis. 2016. Findings of the 2016 WMT
shared task on cross-lingual pronoun prediction. In
Proceedings of the First Conference on Machine
Translation, pages 525–542, Berlin, Germany, Au-
gust. Association for Computational Linguistics.

Liane Guillou. 2012. Improving pronoun translation
for statistical machine translation. In Proceedings of
the Student Research Workshop at the 13th Confer-
ence of the European Chapter of the Association for
Computational Linguistics, pages 1–10, Avignon,
France, April. Association for Computational Lin-
guistics.

Christian Hardmeier and Marcello Federico. 2010.
Modelling pronominal anaphora in statistical ma-
chine translation. In IWSLT (International Work-
shop on Spoken Language Translation), pages 283–
289, Paris, France, December.

Christian Hardmeier, Joakim Nivre, and Jörg Tiede-
mann. 2012. Document-wide decoding for phrase-
based statistical machine translation. In Proceed-
ings of the 2012 Joint Conference on Empiri-
cal Methods in Natural Language Processing and
Computational Natural Language Learning, pages
1179–1190, Jeju Island, Korea, July. Association for
Computational Linguistics.

Christian Hardmeier, Preslav Nakov, Sara Stymne, Jörg
Tiedemann, Yannick Versley, and Mauro Cettolo.
2015. Pronoun-focused MT and cross-lingual pro-
noun prediction: Findings of the 2015 DiscoMT
shared task on pronoun translation. In Proceedings
of the Second Workshop on Discourse in Machine
Translation, pages 1–16, Lisbon, Portugal, Septem-
ber. Association for Computational Linguistics.

Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, Chris Dyer, Ondrej Bojar, Alexan-
dra Constantin, and Evan Herbst. 2007. Moses:
Open source toolkit for statistical machine transla-
tion. In Proceedings of the 45th Annual Meeting of
the Association for Computational Linguistics Com-
panion Volume Proceedings of the Demo and Poster
Sessions, pages 177–180, Prague, Czech Republic,
June. Association for Computational Linguistics.

Ronan Le Nagard and Philipp Koehn. 2010. Aiding
pronoun translation with co-reference resolution. In
Proceedings of the Joint Fifth Workshop on Statisti-
cal Machine Translation and MetricsMATR, pages
252–261, Uppsala, Sweden, July. Association for
Computational Linguistics.

Heeyoung Lee, Yves Peirsman, Angel Chang,
Nathanael Chambers, Mihai Surdeanu, and Dan Ju-
rafsky. 2011. Stanford’s multi-pass sieve corefe-
rence resolution system at the CoNLL-2011 shared
task. In Proceedings of the Fifteenth Conference on
Computational Natural Language Learning: Shared
Task, pages 28–34, Portland, Oregon, USA, June.
Association for Computational Linguistics.

Xiaoqiang Luo. 2005. On coreference resolution per-
formance metrics. In Proceedings of the Confer-
ence on Human Language Technology and Empiri-
cal Methods in Natural Language Processing, pages
25–32, Vancouver, B.C., Canada.

Ngoc Quang Luong and Andrei Popescu-Belis. 2016.
Improving pronoun translation by modeling core-
ference uncertainty. In Proceedings of the First
Conference on Machine Translation, pages 12–20,
Berlin, Germany, August. Association for Computa-
tional Linguistics.

Ngoc Quang Luong and Andrei Popescu-Belis. 2017.
Machine translation of Spanish personal and pos-
sessive pronouns using anaphora probabilities. In
Proceedings of the 15th Conference of the European
Chapter of the Association for Computational Lin-
guistics (EACL), Valencia, Spain, April.

Ngoc Quang Luong, Lesly Miculicich Werlen, and An-
drei Popescu-Belis. 2015. Pronoun translation and
prediction with or without coreference links. In Pro-
ceedings of the Second Workshop on Discourse in
Machine Translation, pages 94–100, Lisbon, Portu-
gal, September. Association for Computational Lin-
guistics.

Juhani Luotolahti, Jenna Kanerva, and Filip Ginter.
2016. Cross-lingual pronoun prediction with deep
recurrent neural networks. In Proceedings of the
First Conference on Machine Translation, pages
596–601, Berlin, Germany, August. Association for
Computational Linguistics.

Christopher Manning, Mihai Surdeanu, John Bauer,
Jenny Finkel, Steven Bethard, and David McClosky.
2014. The Stanford CoreNLP natural language pro-
cessing toolkit. In Proceedings of 52nd Annual
Meeting of the Association for Computational Lin-
guistics: System Demonstrations, pages 55–60, Bal-
timore, Maryland, June. Association for Computa-
tional Linguistics.

Lluı́s Màrquez, Marta Recasens, and Emili Sapena.
2013. Coreference resolution: an empirical study
based on SemEval-2010 Shared Task 1. Language
Resources and Evaluation, 47(3):661–694.

39



Lesly Miculicich Werlen and Andrei Popescu-Belis.
2016. Validation of an automatic metric for the ac-
curacy of pronoun translation (APT). Technical Re-
port Idiap-RR-29-2016, Idiap Research Institute.

Ruslan Mitkov. 2002. Anaphora Resolution. Long-
man, London, UK.

Vincent Ng. 2010. Supervised noun phrase core-
ference research: The first fifteen years. In Pro-
ceedings of the 48th Annual Meeting of the Asso-
ciation for Computational Linguistics, pages 1396–
1411, Uppsala, Sweden, July. Association for Com-
putational Linguistics.

Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: a method for automatic
evaluation of machine translation. In Proceedings
of 40th Annual Meeting of the Association for Com-
putational Linguistics, pages 311–318, Philadelphia,
Pennsylvania, USA, July. Association for Computa-
tional Linguistics.

Andrei Popescu-Belis. 1999. Evaluation numérique
de la résolution de la référence: Critiques et propo-
sitions. TAL: Traitement automatique des langues,
40(2):117–146.

Sameer Pradhan, Xiaoqiang Luo, Marta Recasens, Ed-
uard Hovy, Vincent Ng, and Michael Strube. 2014.
Scoring coreference partitions of predicted men-
tions: A reference implementation. In Proceed-
ings of the 52nd Annual Meeting of the Association
for Computational Linguistics (Volume 2: Short Pa-
pers), pages 30–35, Baltimore, Maryland, June. As-
sociation for Computational Linguistics.

Marta Recasens and Eduard Hovy. 2011. BLANC:
Implementing the Rand index for coreference eval-
uation. Natural Language Engineering, 17(4):485–
510.

Marta Recasens and M. Antònia Martı́. 2010. Ancora-
CO: Coreferentially annotated corpora for Spanish
and Catalan. Language Resources and Evaluation,
44(4):315–345.

Marta Recasens, Lluı́s Màrquez, Emili Sapena,
M. Antònia Martı́, Mariona Taulé, Véronique Hoste,
Massimo Poesio, and Yannick Versley. 2010.
SemEval-2010 task 1: Coreference resolution in
multiple languages. In Proceedings of the 5th Inter-
national Workshop on Semantic Evaluation, pages
1–8, Uppsala, Sweden, July. Association for Com-
putational Linguistics.

Annette Rios. 2015. A Basic Language Technology
Toolkit for Quechua. Ph.D. thesis, University of
Zurich, January.

Don Tuggener. 2016. Incremental Coreference Res-
olution for German. Ph.D. thesis, University of
Zurich.

Marc Vilain, John Burger, John Aberdeen, Dennis Con-
nolly, and Lynette Hirschman. 1995. A model-
theoretic coreference scoring scheme. In Proceed-
ings of the 6th Conference on Message Understand-
ing, pages 45–52, Columbia, MD, USA.

Sam Wiseman, Alexander M. Rush, Stuart Shieber, and
Jason Weston. 2015. Learning anaphoricity and an-
tecedent ranking features for coreference resolution.
In Proceedings of the 53rd Annual Meeting of the
Association for Computational Linguistics and the
7th International Joint Conference on Natural Lan-
guage Processing (Volume 1: Long Papers), pages
1416–1426, Beijing, China, July. Association for
Computational Linguistics.

Sam Wiseman, Alexander M. Rush, and Stuart M.
Shieber. 2016. Learning global features for co-
reference resolution. In Proceedings of the 2016
Conference of the North American Chapter of the
Association for Computational Linguistics: Hu-
man Language Technologies, pages 994–1004, San
Diego, California, June. Association for Computa-
tional Linguistics.

40


