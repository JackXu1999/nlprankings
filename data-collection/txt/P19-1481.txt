



















































Cross-Lingual Training for Automatic Question Generation


Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 4863–4872
Florence, Italy, July 28 - August 2, 2019. c©2019 Association for Computational Linguistics

4863

Cross-Lingual Training for Automatic Question Generation

Vishwajeet Kumar1,2, Nitish Joshi2, Arijit Mukherjee2, Ganesh Ramakrishnan2, and Preethi Jyothi2

1IITB-Monash Research Academy, Mumbai, India
2IIT Bombay, Mumbai, India

{vishwajeet, nitishj, ganesh, pjyothi}@cse.iitb.ac.in
{arijitmukh007}@gmail.com

Abstract

Automatic question generation (QG) is a chal-
lenging problem in natural language under-
standing. QG systems are typically built as-
suming access to a large number of training in-
stances where each instance is a question and
its corresponding answer. For a new language,
such training instances are hard to obtain mak-
ing the QG problem even more challenging.
Using this as our motivation, we study the
reuse of an available large QG dataset in a sec-
ondary language (e.g. English) to learn a QG
model for a primary language (e.g. Hindi) of
interest. For the primary language, we assume
access to a large amount of monolingual text
but only a small QG dataset. We propose a
cross-lingual QG model which uses the fol-
lowing training regime: (i) Unsupervised pre-
training of language models in both primary
and secondary languages and (ii) joint super-
vised training for QG in both languages. We
demonstrate the efficacy of our proposed ap-
proach using two different primary languages,
Hindi and Chinese. We also create and release
a new question answering dataset for Hindi
consisting of 6555 sentences.

1 Introduction

Automatic question generation from text is an im-
portant yet challenging problem especially when
there is limited training data (i.e., pairs of sen-
tences and corresponding questions). Standard se-
quence to sequence models for automatic question
generation have been shown to perform reason-
ably well for languages like English, for which
hundreds of thousands of training instances are
available. However, training sets of this size are
not available for most languages. Manually cu-
rating a dataset of comparable size for a new lan-
guage will be tedious and expensive. Thus, it
would be desirable to leverage existing question
answering datasets to help build QG models for a

Sentence : िव�ा के य ेसभी �प हमारे रा�ट्रीय �ान के िविवध अंग ह�
(All these forms of education are diverse aspects of our 
national knowledge system.)
Question (ground truth) : िव�ा के सभी �प हमारे रा�ट्रीय �ान के �या ह� �
(What is the relationship between different forms of 
education and our national knowledge systems?)
Question (predicted) : िव�ा के सभी �प �या ह� �

(What are all the forms of education?)

Sentence : स�यता का अथ� है सपंि� की िनरतंर विृ� , �यव�था और र�ा अपनी सपंि� की 
र�ा औजारो ंके �ारा की जाती है
(Civilization means continuous growth of prosperity, the system 
and its security are facilitated by the defense mechanism of the 
civilization.) 
Question (ground truth) : स�यता का �या अथ� है �
(What is the meaning of civilization?)
Question (predicted) : स�यता का �या अथ� है �
(What is the meaning of civilization?)

1.

.

2.

Figure 1: Automatic QG from Hindi text.

new language. This is the overarching idea that
motivates this work. In this paper, we present a
cross-lingual model for leveraging a large ques-
tion answering dataset in a secondary language
(such as English) to train models for QG in a pri-
mary language (such as Hindi) with a significantly
smaller question answering dataset.

We chose Hindi to be one of our primary
languages. There is no established dataset
available for Hindi that can be used to build
question answering or question generation sys-
tems, making it an appropriate choice as a
primary language. We create a new ques-
tion answering dataset for Hindi (named Hi-
QuAD): https://www.cse.iitb.ac.in/
˜ganesh/HiQuAD/clqg/. Figure 1 shows
two examples of sentence-question pairs from Hi-
QuAD along with the questions predicted by our
best model. We also experimented with Chinese as
a primary language. This choice was informed by
our desire to use a language that was very different
from Hindi. We use the same secondary language
– English – with both choices of our primary lan-
guage.

Drawing inspiration from recent work on unsu-
pervised neural machine translation (Artetxe et al.,

https://www.cse.iitb.ac.in/~ganesh/HiQuAD/clqg/
https://www.cse.iitb.ac.in/~ganesh/HiQuAD/clqg/


4864

2018; Yang et al., 2018), we propose a cross-
lingual model to leverage resources available in
a secondary language while learning to automati-
cally generate questions from a primary language.
We first train models for alignment between the
primary and secondary languages in an unsuper-
vised manner using monolingual text in both lan-
guages. We then use the relatively larger QG
dataset in a secondary language to improve QG on
the primary language. Our main contributions can
be summarized as follows:
• We present a cross-lingual model that effec-

tively exploits resources in a secondary language
to improve QG for a primary language.
• We demonstrate the value of cross-lingual

training for QG using two primary languages,
Hindi and Chinese.
• We create a new question answering dataset

for Hindi, HiQuAD.

2 Related Work

Prior work in QG from text can be classified into
two broad categories.

Rule-based: Rule-based approaches (Heilman,
2011) mainly rely on manually curated rules for
transforming a declarative sentence into an in-
terrogative sentence. The quality of the ques-
tions generated using rule-based systems highly
depends on the quality of the handcrafted rules.
Manually curating a large number of rules for a
new language is a tedious and challenging task.
More recently, Zheng et al. (2018) propose a
template-based technique to construct questions
from Chinese text, where they rank generated
questions using a neural model and select the top-
ranked question as the final output.

Neural Network Based: Neural network based
approaches do not rely on hand-crafted rules, but
instead use an encoder-decoder architecture which
can be trained in an end-to-end fashion to automat-
ically generate questions from text. Several neural
network based approaches (Du et al., 2017; Ku-
mar et al., 2018a,b) have been proposed for au-
tomatic question generation from text. Du et al.
(2017) propose a sequence to sequence model
for automatic question generation from English
text. Kumar et al. (2018a) use a rich set of lin-
guistic features and encode pivotal answers pre-
dicted using a pointer network based model to au-
tomatically generate a question for the encoded

WEpri

WEshared WDshared

Denoising Autoencoding

Back Translation

Supervised Training

WEpri, WDpri WEsec, WDsec

WEpri, WDsec WEsec, WDpri

All Training Phases

WEshared, WDshared

WEsec

WDpri

WDsec
WEpri, WDpri WEsec, WDsec

Figure 2: Schematic diagram of our cross-lingual QG
system. WEpri and WEsec refer to parameters of the
encoder layers specific to the primary and secondary
languages; WDpri and WDsec are the weights of the cor-
responding decoder layers. WEshared and WDshared re-
fer to weights of the encoder and decoder layers shared
across both languages, respectively. Weights updated
in each training phase are explicitly listed.

answer. All existing models optimize a cross-
entropy based loss function, that suffers from ex-
posure bias (Ranzato et al., 2016). Further, exist-
ing methods do not directly address the problem of
handling important rare words and word repetition
in QG. Kumar et al. (2018b) propose a reinforce-
ment learning based framework which addresses
the problem of exposure bias, word repetition and
rare words. Tang et al. (2017) and Wang et al.
(2017) propose a joint model to address QG and
the question answering problem together.

All prior work on QG assumed access to a suf-
ficiently large number of training instances for a
language. We relax this assumption in our work
as we only have access to a small question an-
swering dataset in the primary language. We show
how we can improve QG performance on the pri-
mary language by leveraging a larger question an-
swering dataset in a secondary language. (Simi-
larly in spirit, cross-lingual transfer learning based
approaches have been recently proposed for other
NLP tasks such as machine translation (Schuster
et al., 2019; Lample and Conneau, 2019).)

3 Our Approach

We propose a shared encoder-decoder architecture
that is trained in two phases. The first, is an un-
supervised pretraining phase, consisting of de-
noising autoencoding and back-translation. This
pretraining phase only requires sentences in both
the primary and secondary languages. This is fol-
lowed by a supervised question generation train-
ing phase that uses sentence-question pairs in both
languages to fine-tune the pretrained weights.



4865

1 Unsupervised Pretraining
while not converged do

2 Train autoencoder to generate sentence xp from
noisy sentence x̃p in primary language and
similarly xs from x̃s in the secondary language.

3 Back Translation: Generate sentences x
′
p and xs′ in

primary and secondary4 languages from xs and xp
respectively, using the current translation model.

5 Train a new translation model using x
′
p and xs′

where xs and xp are used for supervision,
respectively.

end
6 Supervised Question Generation
7 Initialize with pretrained weights

while not converged do
8 Train sequence to sequence models for question

generation in both the primary and secondary
languages.

end
Algorithm 1: Cross-lingual Training Algorithm
for QG

In Algorithm 1, we outline our training pro-
cedure and Figure 2 illustrates the overall archi-
tecture of our QG system. Our cross-lingual QG
model consists of two encoders and two decoders
specific to each language. We also enforce shared
layers in both the encoder and the decoder whose
weights are updated using data in both languages.
(This weight sharing is discussed in more detail in
Section 3.3.) For the encoder and decoder layers,
we use the newly released Transformer (Vaswani
et al., 2017) model that has shown great success
compared to recurrent neural network-based mod-
els in neural machine translation. Encoders and
decoders consist of a stack of four identical lay-
ers, of which two layers are independently trained
and two are trained in a shared manner. Each layer
of the transformer consists of a multi-headed self-
attention model followed by a position-wise fully
connected feed-forward network.

3.1 Unsupervised Pretraining

We use monolingual corpora available in the pri-
mary (Hindi/Chinese) and secondary (English)
languages for unsupervised pretraining. Similar
to Artetxe et al. (2018), we use denoising autoen-
coders along with back-translation (described in
Section 3.1.1) for pretraining the language mod-
els in both the primary and secondary languages.
Specifically, we first train the model to reconstruct
their inputs, which will expose the model to the
grammar and vocabulary specific to each language
while enforcing a shared latent-space with the help

of the shared encoder and decoder layers. To pre-
vent the model from simply learning to copy ev-
ery word, we randomly permute the word order in
the input sentences so that the model learns mean-
ingful structure in the language. If xp denotes the
true input sentence to be generated from the sen-
tence with permuted word order x̃p for the pri-
mary language, then during each pass of the au-
toencoder training we update the weights WEpri ,
WEshared , WDshared and WDpri . For the secondary
language, we analogously update WEsec , WDsec
and the weights in the shared layers as shown in
Figure 2.

3.1.1 Back translation
In addition to denoising autoencoders, we utilize
back-translation (Sennrich et al., 2016a). This
further aids in enforcing the shared latent space
assumption by generating a pseudo-parallel cor-
pus (Imankulova et al., 2017).1 Back translation
has been demonstrated to be very important for un-
supervised NMT (Yang et al., 2018; Lample et al.,
2018). Given a sentence in the secondary language
xs, we generate a translated sentence in the pri-
mary language, x̃p. We then use the translated sen-
tence x̃p to generate the original xs back, while up-
dating the weights WEsec , WEshared , WDshared and
WDpri as shown in Figure 2. Note that we uti-
lize denoising autoencoding and back-translation
for both languages in each step of training.

3.2 Supervised Question Generation

We formulate the QG problem as a sequence
to sequence modeling task where the input is a
sentence and the output is a semantically con-
sistent, syntactically correct and relevant ques-
tion in the same language that corresponds to
the sentence. Each encoder receives a sentence
x (from the corresponding language) as input
and the decoder generates a question ȳ such
that ȳ = argmaxy P (y|x), and P (y|x) =
|y|∏
t=1

P (yt|x, y<t), where probability of each sub-

word yt is predicted conditioned on all the sub-
words generated previously y<t and the input sen-
tence x. We initialize the encoder and decoder
weights using unsupervised pretraining and fine-
tune these weights further during the supervised

1A pseudo-parallel corpus consists of pairs of translated
sentences using the current state of the model along with the
original sentences.



4866

QG model training. Specifically, in each step of
training, we update the weights WEsec , WEshared ,
WDshared and WDsec using QG data in the sec-
ondary language and WEpri , WEshared , WDshared
and WDpri using QG data in the primary language.

3.3 More Architectural Details

We make three important design choices:

1. Use of positional masks: Shen et al. (2018)
point out that transformers are not capable
of capturing within the attention, information
about order of the sequence. Following Shen
et al. (2018), we enable our encoders to use
directional self attention so that temporal in-
formation is preserved. We use positional en-
codings which are essentially sine and cosine
functions of different frequencies. More for-
mally, positional encoding (PE) is defined as:

PE(pos,2i) = sin

(
pos

m
2i

dmodel

)
(1)

PE(pos,2i+1) = cos

(
pos

m
2i

dmodel

)
(2)

where m is a hyper-parameter, pos is the
position, dmodel is the dimensionality of the
transformer and i is the dimension. Follow-
ing Vaswani et al. (2017), we set m to 10000
in all our experiments. Directional self at-
tention uses positional masks to inject tempo-
ral order information. Based on Shen et al.
(2018), we define a forward positional mask
(Mf ) and a backward positional mask (M b),

Mfij =

{
0, i < j.

−∞, otherwise.

M bij =

{
0, i > j.

−∞, otherwise.

that processes the sequence in the forward
and backward direction, respectively.

2. Weight sharing: Based on the assumption
that sentences and questions in two languages
are similar in some latent space, in order to
get a shared language independent represen-
tation, we share the last few layers of the
encoder and the first few layers of the de-
coder (Yang et al., 2018). Unlike Artetxe
et al. (2018); Lample et al. (2018), we do not

share the encoder completely across the two
languages, thus allowing the encoder layers
private to each language to capture language-
specific information. We found this to be use-
ful in our experiments.

3. Subword embeddings: We represent data
using BPE (Byte Pair Encoding) (Gage,
1994) embeddings. We use BPE embeddings
for both unsupervised pretraining as well as
the supervised QG training phase. This al-
lows for more fine-grained control over in-
put embeddings compared to word-level em-
beddings (Sennrich et al., 2016b). This also
has the advantage of maintaining a relatively
smaller vocabulary size.2

4 Experimental Setup

We first describe all the datasets we used in
our experiments, starting with a detailed de-
scription of our new Hindi question answering
dataset, “HiQuAD”. We will then describe var-
ious implementation-specific details relevant to
training our models. We conclude this section with
a description of our evaluation methods.

4.1 Datasets

4.1.1 HiQuAD
HiQuAD (Hindi Question Answering dataset) is
a new question answering dataset in Hindi that
we developed for this work. This dataset contains
6555 question-answer pairs from 1334 paragraphs
in a series of books called Dharampal Books. 3

Similar to SQuAD (Rajpurkar et al., 2016), an
English question answering dataset that we de-
scribe further in Section 4.1.2, HiQuAD also con-
sists of a paragraph, a list of questions answerable
from the paragraph and answers to those ques-
tions. To construct sentence-question pairs, for
a given question, we identified the first word of
the answer in the paragraph and extracted the cor-
responding sentence to be paired along with the
question. We curated a total of 6555 sentence-
question pairs.

We tokenize the sentence-question pairs to re-
move any extra white spaces. For our experiments,
we randomly split the HiQuAD dataset into train,

2Using word embeddings across pretraining and the main
QG task makes the vocabulary very large, thus leading to
large memory issues.

3HiQuAD can be downloaded from: https://www.
cse.iitb.ac.in/˜ganesh/HiQuAD/clqg/

https://www.cse.iitb.ac.in/~ganesh/HiQuAD/clqg/
https://www.cse.iitb.ac.in/~ganesh/HiQuAD/clqg/


4867

#pairs (Train set) 4000
#pairs (Dev set) 1300
#pairs (Test set) 1255

Text: avg tokens 28.64
Question: avg tokens 14.13

Table 1: HiQuAD dataset details

development and test sets as shown in Table 1. All
model hyperparameters are optimized using the
development set and all results are reported on the
test set.

4.1.2 Other Datasets
We briefly describe all the remaining datasets used
in our experiments. (The relevant primary or
secondary language is mentioned in parenthesis,
alongside the name of the datasets.)

IITB Hindi Monolingual Corpus (Primary
language: Hindi) We extracted 93,000 sentences
from the IITB Hindi monolingual corpus4 , where
each sentence has between 4 and 25 tokens. These
sentences were used for unsupervised pretraining.

IITB Parallel Corpus (Primary language:
Hindi) We selected 100,000 English-Hindi sen-
tence pairs from IITB parallel corpus (Kunchukut-
tan et al., 2018) where the number of tokens in the
sentence was greater than 10 for both languages.
We used this dataset to further fine-tune the
weights of the encoder and decoder layers after
unsupervised pretraining.

DuReader (He et al., 2018) Chinese Dataset:
(Primary language: Chinese) This dataset con-
sists of question-answer pairs along with the ques-
tion type. We preprocessed and used “DESCRIP-
TION” type questions for our experiments, result-
ing in a total of 8000 instances. From this sub-
set, we created a 6000/1000/1000 split to construct
train, development and test sets for our experi-
ments. We also preprocessed and randomly ex-
tracted 100,000 descriptions to be used as a Chi-
nese monolingual corpus for the unsupervised pre-
training stage.

News Commentary Dataset: (Primary lan-
guage: Chinese) This is a parallel corpus of

4http://www.cfilt.iitb.ac.in/
iitb_parallel/iitb_corpus_download/
monolingual.hi.tgz

news commentaries provided by WMT.5 It con-
tains roughly 91000 English sentences along with
their Chinese translations. We preprocessed this
dataset and used this parallel data for fine-tuning
the weights of the encoder and decoder layers af-
ter unsupervised pretraining.

SQuAD Dataset: (Secondary language: En-
glish) This is a very popular English question an-
swering dataset (Rajpurkar et al., 2016). We used
the train split of the pre-processed QG data re-
leased by Du et al. (2017) for supervised QG
training. This dataset consists of 70,484 sentence-
question pairs in English.

4.2 Implementation Details
We implemented our model in TensorFlow.6 We
used 300 hidden units for each layer of the trans-
former with the number of attention heads set to 6.
We set the size of BPE embeddings to 300. Our
best model uses two independent encoder and de-
coder layers for both languages, and two shared
encoder and decoder layers each. We used a resid-
ual dropout set to 0.2 to prevent overfitting. Dur-
ing both the unsupervised pretraining and super-
vised QG training stages, we used the Adam opti-
mizer (Kingma and Ba, 2015) with a learning rate
of 1e−5 and batch size of 64.

4.2.1 Unsupervised Pretraining
For Hindi as the primary language, we use 93000
Hindi sentences from the IITB Hindi Monolin-
gual Corpus and around 70000 English sentences
from the preprocessed SQuAD dataset for unsu-
pervised pretraining. We pretrain the denoising
autoencoders over 15 epochs. For Chinese, we
use 100000 Chinese sentences from the DuReader
dataset for this stage of training.

4.2.2 Supervised Question Generation
Training

We used 73000 sentence-question pairs from
SQuAD and 4000 sentence-question pairs from
HiQuAD (described in Section 4.1.1) to train
the supervised QG model in Hindi. We used
6000 Chinese sentence-question pairs from the
DuReader dataset to train the supervised QG
model in Chinese. We initialize all the weights,
including the BPE embeddings, from the pretrain-
ing phase and fine-tune them until convergence.

5http://opus.nlpl.eu/
News-Commentary-v11.php

6Code available at https://github.com/vishwajeet93/clqg

http://www.cfilt.iitb.ac.in/iitb_parallel/iitb_corpus_download/monolingual.hi.tgz
http://www.cfilt.iitb.ac.in/iitb_parallel/iitb_corpus_download/monolingual.hi.tgz
http://www.cfilt.iitb.ac.in/iitb_parallel/iitb_corpus_download/monolingual.hi.tgz
http://opus.nlpl.eu/News-Commentary-v11.php
http://opus.nlpl.eu/News-Commentary-v11.php


4868

Language Model BLEU-1 BLEU-2 BLEU-3 BLEU-4 METEOR ROUGE-L

Hindi
Transformer 28.414 18.493 12.356 8.644 23.803 29.893
Transformer+pretraining 41.059 29.294 21.403 16.047 28.159 39.395
CLQG 41.034 29.792 22.038 16.598 27.581 39.852
CLQG+parallel 42.281 32.074 25.182 20.242 29.143 40.643

Chinese
Transformer 25.52 9.22 5.14 3.25 7.64 27.40
Transformer+pretraining 30.38 14.01 8.37 5.18 10.46 32.71
CLQG 30.69 14.51 8.82 5.39 10.44 31.82
CLQG+parallel 30.30 13.93 8.43 5.51 10.26 31.58

Table 2: BLEU, METEOR and ROUGE-L scores on the test set for Hindi and Chinese question generation. Best
results for each metric (column) are highlighted in bold.

4.3 Evaluation Methods

We evaluate our systems and report results
on widely used BLEU (Papineni et al., 2002),
ROUGE-L and METEOR metrics. We also per-
formed a human evaluation study to evaluate the
quality of the questions generated. Following Ku-
mar et al. (2018a), we measure the quality of ques-
tions in terms of syntactic correctness, semantic
correctness and relevance. Syntactic correctness
measures the grammatical correctness of a gener-
ated question, semantic correctness measures nat-
uralness of the question, and relevance measures
how relevant the question is to the text and answer-
ability of the question from the sentence.

5 Results

We present our automatic evaluation results in Ta-
ble 2, where the primary language is Hindi or Chi-
nese and the secondary language in either setting
is English. We do not report on Chinese as a sec-
ondary language owing to the relatively poor qual-
ity of the Chinese dataset. Here are all the models
we compare and evaluate:
• Transformer: We train a transformer

model (Vaswani et al., 2017) using the QG dataset
in the primary language. This serves as a natural
baseline for comparison.7 This model consists of
a two-layer encoder and a two-layer decoder.
• Transformer+pretraining: The above-

mentioned Transformer model undergoes an
additional step of pretraining. The encoder and
decoder layers are pretrained using monolingual
data from the primary language. This model will
help further demonstrate the value of cross-lingual
training.

7We also trained a sequence-to-sequence model by aug-
menting HiQuAD with SQuAD sentences translated into
Hindi using Google Translate. This did not perform well giv-
ing a BLEU-4 score of 7.54.

• CLQG: This is our main cross-lingual ques-
tion generation model (described in Section 3)
where the encoder and decoder layers are initial-
ized in an unsupervised pretraining phase using
primary and secondary language monolingual cor-
pora, followed by a joint supervised QG training
using QG datasets in the primary and secondary
languages.

• CLQG+parallel: The CLQG model under-
goes further training using a parallel corpus (with
primary language as source and secondary lan-
guage as target). After unsupervised pretraining,
the encoder and decoder weights are fine-tuned us-
ing the parallel corpus. This fine-tuning further re-
fines the language models for both languages and
helps enforce the shared latent space across both
languages.

We observe in Table 2 that CLQG+parallel out-
performs all the other models for Hindi. For Chi-
nese, parallel fine-tuning does not give signifi-
cant improvements over CLQG; this could be at-
tributed to the parallel corpus being smaller in size
(when compared to Hindi) and domain-specific
(i.e. the news domain).

Model
Syntax Semantics Relevance

Score Kappa Score Kappa Score Kappa
Transformer 71 0.239 62.5 0.46 32 0.75
CLQG 72 0.62 68.5 0.82 54 0.42
+parallel

Table 3: Human evaluation results as well as inter-rater
agreement (column “Kappa”) for each model on the
Hindi test set. The scores are between 0-100, 0 be-
ing the worst and 100 being the best. Best results for
each metric (column) are in bold. The three evalua-
tion criteria are: (1) syntactic correctness (Syntax), (2)
semantic correctness (Semantics), and (3) relevance to
the paragraph (Relevance).



4869

Sentence : आज दशे म� जो हो रहा है वह तो एक बहुत िनचल े
�तर का यरूोप व अमरीका का अनकुरण हो रहा है
(What is happening in the country today is a 
very low level emulation of Europe and 
America.)
Question (human generated) : आज भारत दशे म� 
जो हो रहा है वह �या है � 
(How do you describe whatever is 
happening in India today?)
Question (predicted) : आज भारत दशे म� जो कुछ हो 
रहा है वह �या है �
(How do you describe whatever is 
happening in India today?)

(a)

Sentence : �लफेेयर ने कहा िक ग�ु�वाकष�ण िस�ांत एवं 
इ�टीगर्ल केलकुलस के गिणतीय िस�ातो ंके �ान के िबना भारतीय 
गिणत� इतना अचकू गिणत �योितषीय आकलन कर ही नही ंसकते थे
(Playfair said that without the knowledge of 
the mathematical principles of ....)

Question (human generated) : �लफेेयर ने �या 
कहा � 
(What did Playfair say?)
Question (predicted) : �लफेेयर ने �या कहा �
(What did Playfair say?)

(b)

Sentence : इस गाथा के अनसुार बर्� के तप व सकं�प स ेसिृ�ट 
का सज�न होता है , और िफर यह अनेकानेक आवत�नो ंस ेहोती हुई , 
वापस बर्� म� लीन हो जाती है
(According to this narrative, the universe is 
created by tenacity and resolution of...)

Question (human generated) : इस गाथा के अनसुार 
िकसस ेसिृ�ट का सज�न होता है � 
(According to this narrative, how is the universe 
created?)
Question (predicted) : िकस चीज़ के अनसुार सिृ�ट का सज�न 
होता है �
(According to what the universe is created?)

(c)

Figure 3: Three examples of correctly generated Hindi questions by our model, further analyzed in Section 6.2.

Sentence :  इसी ईसाईकरण का दसूरा नाम पि�चमीकरण है , िजस ेकरने के 
पर्य�न �वततंर् भारत की सरकार� भी करती चली आ रही ह�
(The second name of this Christianization is 
Westernization, which independent India's 
governments has been trying to do.)
Question (human generated) : ईसाईकरण का दसूरा नाम �या है � 
(What is the second name of Christianization?)
Question (predicted) : िव�ान का दसूरा नाम �या है �
(What is the second name of science?)

(a)
Sentence : हम जानते ह� िक अरब बहुत बड़ा िवदशे �यापार करते थे
(We know that the Arabs used to very big foreign trade.)
Question (human generated) : अरब �या करते थे � 
(What did Arab people used to do?)
Question (predicted) : अरब लोग िकस तरह के थे �
(What kind of people were the Arabs?)

(b)

Figure 4: Two examples of incorrectly generated Hindi
questions by our model, further analyzed in Sec-
tion 6.2.

6 Discussion and Analysis

We closely inspect our cross-lingual training
paradigm using (i) a human evaluation study in
Section 6.1 (ii) detailed error analysis in Sec-
tion 6.2 and (iii) ablation studies in Section 6.3.
All the models analyzed in this section used Hindi
as the primary language.8

6.1 Human evaluation
We conduct a human evaluation study compar-
ing the questions generated by the Transformer
and CLQG+parallel models. We randomly se-
lected a subset of 100 sentences from the Hindi
test set and generated questions using both mod-
els. We presented these sentence-question pairs
for each model to three language experts and asked
for a binary response on three quality parame-
ters namely syntactic correctness, semantic cor-
rectness and relevance. The responses from all
the experts for each parameter was averaged for

8Figure 5 shows two examples of correctly generated Chi-
nese questions.

Sentence : 打开 微信 ， 点击 “ 我 ” ， 选择 通⽤ ， 点击 功能 ， 选择 群发 助⼿ ， 点 开始 群发 ， 如果 被 对⽅ 删 了 发布 出去.
(Open WeChat, click "I", select General, click on function, 
select the group assistant, click to start the group, if it is 
deleted by the other party, release it.)
Question (human generated) : 怎么 知道 对⽅ 微信 是否 把 我 删 了 ?
(How do I know if I have been deleted by the other 
person's Wechat?)
Question (predicted) : 怎样 知道 微信 好友 是否 删除 ⾃己 ?
(How do I know if my WeChat friends deleted me? )

(a)
Sentence : 放置 在 冰箱 ⾥ ； 把 百⾹果 洗⼲净 切成 条 放在 太阳 底下 晒 成果 ⼲.
(Put them in the refrigerator; wash and cut them into strips 
and dry them in the sun.)
Question (human generated) : 百⾹果 怎么 保存 得 久 ⼀点 ?
(How can fruit be stored for longer ?))
Question (predicted) : 樱桃 怎么 保存 ?
(How to store cherries? )

(b)

Figure 5: Automatic QG from Chinese text.

each model to get the final numbers shown in Ta-
ble 3. Although we perform comparably to the
baseline model on syntactic correctness scores, we
obtain significantly higher agreement across anno-
tators using our cross-lingual model. Our cross-
lingual model performs significantly better than
the Transformer model on “Relevance” at the
cost of agreement. On semantic correctness, we
perform signficantly better both in terms of the
score and agreement statistics.

6.2 Error Analysis

Correct examples: We show several examples
where our model is able to generate semantically
and syntactically correct questions in Figure 3.
Figure 3b shows our model is able to generate
questions that are identical to human-generated
questions. Fig. 3c demonstrates that our model can
generate new questions which clearly differ from
the human-generated questions but are syntacti-
cally correct, semantically correct and relevant to
the text. Fig. 3a shows a third question which dif-
fers from the human-generated question in only a



4870

Model BLEU-1 BLEU-2 BLEU-3 BLEU-4 METEOR ROUGE-L
CLQG (no pretraining) 31.707 20.727 13.954 9.862 24.209 32.332
CLQG 41.034 29.792 22.038 16.598 27.581 39.852
CLQG+ parallel 42.281 32.074 25.182 20.242 29.143 40.643

Table 4: Ablation study showing the importance of both unsupervised and unsupervised pretraining for Hindi

Dataset BLEU-1 BLEU-2 BLEU-3 BLEU-4 METEOR ROUGE-L
Hindi QG only 41.66 31.576 24.572 19.538 28.665 40.765
Hindi QG + English QG 42.281 32.074 25.182 20.242 29.143 40.643

Table 5: Ablation study showing the importance of using English QG data for Hindi QG

single word but does not alter its quality.

Incorrect examples: We also present a couple
of examples where our model is unable to generate
good questions and analyze possible reasons for
the same. In Fig. 4a, the model captures the type of
question correctly but gets the main subject of the
sentence wrong. On the other hand, Fig. 4b shows
a question which is syntactically correct and rele-
vant to the main subject, but is not consistent with
the given sentence.

6.3 Ablation Studies

We performed two experiments to better under-
stand the role of each component in our model to-
wards automatic QG from Hindi text.

6.3.1 Importance of unsupervised
pretraining

We construct a model which does not employ
any unsupervised or supervised pretraining but
uses the same network architecture. This helps
in studying the importance of pretraining in our
model. We present our results in Table 4. We
observe that our shared architecture does not di-
rectly benefit from the English QG dataset with
simple weight sharing. Unsupervised pretraining
(with back-translation) helps the shared encoder
and decoder layers capture higher-level language-
independent information giving an improvement
of approximately 7 in BLEU-4 scores. Addition-
ally, the use of parallel data for fine-tuning unsu-
pervised pretraining aids this process further by
improving BLEU-4 scores by around 3 points.

6.3.2 Importance of secondary language
resources

To demonstrate the improvement in Hindi QG
from the relatively larger English SQuAD dataset,
we show results of using only HiQuAD during the

Figure 6: Trade-off between HiQuAD training dataset
size and BLEU scores.

main task in Table 5; unsupervised and supervised
pretraining are still employed. We obtain modest
performance improvements on the standard evalu-
ation metrics (except ROUGE-L) by using English
SQuAD data in the main task. These improve-
ments (albeit small) demonstrate that our proposed
cross-lingual framework is a step in the right di-
rection towards leveraging information from a sec-
ondary language.

6.4 How many sentence-question pairs are
needed in the primary language?

To gain more insight into how much data is re-
quired to be able to generate questions of high
quality, Fig. 6 presents a plot of BLEU scores
when the number of Hindi sentence-question pairs
is varied. Here, both unsupervised and supervised
pretraining are employed but the English SQuAD
dataset is not used. After significant jumps in
BLEU-4 performance using the first 2000 sen-
tences, we see a smaller but steady improvement in
performance with the next set of 2000 sentences.



4871

7 Conclusion

Neural models for automatic question generation
using the standard sequence to sequence paradigm
have been shown to perform reasonably well for
languages such as English, which have a large
number of training instances. However, large
training sets are not available for most languages.
To address this problem, we present a cross-
lingual model that leverages a large QG dataset
in a secondary language (along with monolingual
data and parallel data) to improve QG perfor-
mance on a primary language with a limited num-
ber of QG training pairs. In future work, we will
explore the use of cross-lingual embeddings to fur-
ther improve performance on this task.

Acknowledgments

The authors thank the anonymous reviewers for
their insightful comments that helped improve this
paper. The authors also gratefully acknowledge
support from IBM Research, India (specifically
the IBM AI Horizon Networks - IIT Bombay ini-
tiative).

References
Mikel Artetxe, Gorka Labaka, Eneko Agirre, and

Kyunghyun Cho. 2018. Unsupervised neural ma-
chine translation. In ICLR.

Xinya Du, Junru Shao, and Claire Cardie. 2017. Learn-
ing to ask: Neural question generation for reading
comprehension. In ACL.

Philip Gage. 1994. A new algorithm for data compres-
sion. The C Users Journal.

Wei He, Kai Liu, Jing Liu, Yajuan Lyu, Shiqi Zhao,
Xinyan Xiao, Yuan Liu, Yizhong Wang, Hua Wu,
Qiaoqiao She, Xuan Liu, Tian Wu, and Haifeng
Wang. 2018. DuReader: a Chinese machine read-
ing comprehension dataset from real-world applica-
tions. In Workshop on Machine Reading for Ques-
tion Answering.

Michael Heilman. 2011. Automatic factual question
generation from text. Language Technologies Insti-
tute School of Computer Science Carnegie Mellon
University.

Aizhan Imankulova, Takayuki Sato, and Mamoru
Komachi. 2017. Improving low-resource neural
machine translation with filtered pseudo-parallel
corpus. In 4th Workshop on Asian Translation
(WAT2017).

Diederik P. Kingma and Jimmy Ba. 2015. Adam: A
method for stochastic optimization. In ICLR.

Vishwajeet Kumar, Kireeti Boorla, Yogesh Meena,
Ganesh Ramakrishnan, and Yuan-Fang Li. 2018a.
Automating reading comprehension by generating
question and answer pairs. In PAKDD.

Vishwajeet Kumar, Ganesh Ramakrishnan, and Yuan-
Fang Li. 2018b. A framework for automatic ques-
tion generation from text using deep reinforcement
learning. arXiv preprint arXiv:1808.04961.

Anoop Kunchukuttan, Pratik Mehta, and Pushpak
Bhattacharyya. 2018. The iit bombay english-hindi
parallel corpus. In LREC.

Guillaume Lample and Alexis Conneau. 2019. Cross-
lingual language model pretraining. arXiv preprint
arXiv:1901.07291.

Guillaume Lample, Myle Ott, Alexis Conneau, Lu-
dovic Denoyer, and Marc’Aurelio Ranzato. 2018.
Phrase-based & neural unsupervised machine trans-
lation. In EMNLP.

Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a method for automatic eval-
uation of machine translation. In ACL.

Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and
Percy Liang. 2016. Squad: 100,000+ questions for
machine comprehension of text. In EMNLP.

Marc’Aurelio Ranzato, Sumit Chopra, Michael Auli,
and Wojciech Zaremba. 2016. Sequence level train-
ing with recurrent neural networks. In ICLR.

Sebastian Schuster, Sonal Gupta, Rushin Shah, and
Mike Lewis. 2019. Cross-lingual transfer learning
for multilingual task oriented dialog. In NAACL.

Rico Sennrich, Barry Haddow, and Alexandra Birch.
2016a. Improving neural machine translation mod-
els with monolingual data. In ACL.

Rico Sennrich, Barry Haddow, and Alexandra Birch.
2016b. Neural machine translation of rare words
with subword units. In ACL.

Tao Shen, Tianyi Zhou, Guodong Long, Jing Jiang,
Shirui Pan, and Chengqi Zhang. 2018. Disan: Di-
rectional self-attention network for rnn/cnn-free lan-
guage understanding. In AAAI.

Duyu Tang, Nan Duan, Tao Qin, Zhao Yan, and
Ming Zhou. 2017. Question answering and ques-
tion generation as dual tasks. arXiv preprint
arXiv:1706.02027.

Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob
Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz
Kaiser, and Illia Polosukhin. 2017. Attention is all
you need. In NeurIPS, pages 5998–6008.

Tong Wang, Xingdi Yuan, and Adam Trischler. 2017.
A joint model for question answering and question
generation. arXiv preprint arXiv:1706.01450.



4872

Zhen Yang, Wei Chen, Feng Wang, and Bo Xu.
2018. Unsupervised neural machine translation with
weight sharing. In ACL.

Hai-Tao Zheng, JX Han, JY Chen, and Arun Kumar
Sangaiah. 2018. A novel framework for automatic
chinese question generation based on multi-feature
neural network model. Comput. Sci. Inf. Syst.


