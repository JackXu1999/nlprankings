



















































Centroid-based Text Summarization through Compositionality of Word Embeddings


Proceedings of the MultiLing 2017 Workshop on Summarization and Summary Evaluation Across Source Types and Genres, pages 12–21,
Valencia, Spain, April 3, 2017. c©2017 Association for Computational Linguistics

Centroid-based Text Summarization through
Compositionality of Word Embeddings

Gaetano Rossiello Pierpaolo Basile Giovanni Semeraro
Department of Computer Science

University of Bari, 70125 Bari, Italy
{firstname.secondname}@uniba.it

Abstract

The textual similarity is a crucial aspect
for many extractive text summarization
methods. A bag-of-words representation
does not allow to grasp the semantic re-
lationships between concepts when com-
paring strongly related sentences with no
words in common. To overcome this is-
sue, in this paper we propose a centroid-
based method for text summarization that
exploits the compositional capabilities of
word embeddings. The evaluations on
multi-document and multilingual datasets
prove the effectiveness of the continu-
ous vector representation of words com-
pared to the bag-of-words model. De-
spite its simplicity, our method achieves
good performance even in comparison to
more complex deep learning models. Our
method is unsupervised and it can be
adopted in other summarization tasks.

1 Introduction

The goal of text summarization is to produce a
shorter version of a source text by preserving the
meaning and the key contents of the original text.
This is a very complex problem since it requires
to emulate the cognitive capacity of human beings
to generate summaries. Thus, text summarization
poses open challenges in both natural language un-
derstanding and generation. Due to the difficulty
of this task, research work in the literature focused
on the extractive aspect of summarization, where
the generated summary is a selection of relevant
sentences from a document (or a set of documents)
in a copy-paste fashion. A good extractive sum-
marization method must satisfy and optimize both
coverage and diversity properties, where the se-
lected sentences should cover a sufficient amount

of topics from the original source text, avoiding
the redundancy of information in the summary.
The diversity property is fundamental especially
for a multi-document summarization. For instance
in a news aggregator, a selection of too similar sen-
tences may compromise the quality of the gener-
ated summary.

An extractive method should define a sentence
representation model, a technique for assigning a
score to each sentence in the original source and
a ranking module to properly select the most rel-
evant sentences by relying on a similarity func-
tion. Following this vision, several summariza-
tion methods proposed in the literature use the
bag of words (BOW) as representation model for
the sentence scoring and selection modules (Radev
et al., 2004; Erkan and Radev, 2004; Lin and
Bilmes, 2011). Despite their proven effectiveness,
these methods rely heavily on the notion of sim-
ilarity between sentences, and a BOW represen-
tation is often not suitable to grasp the semantic
relationships between concepts when comparing
sentences. For example, taking into account the
following two sentences “Syd leaves Pink Floyd”
and “Barrett abandons the band”, in the BOW
model their vector (sparse) representations result
orthogonal since they have no words in common,
nonetheless the two sentences are strongly related.

In attempt to solve this issue, in this work we
propose a novel and simple extractive summariza-
tion method based on the geometric meaning of
the centroid vector of a (multi) document by tak-
ing advantage of compositional properties of the
word embeddings (Mikolov et al., 2013b). Empir-
ically, we prove the effectiveness of word embed-
dings with a fair comparison to the BOW represen-
tation by limiting, as much as possible, the param-
eters and the complexity of the method. Surpris-
ingly, the results achieved from our method on the
gold standard DUC-2004 dataset are comparable,

12



and in some cases better, to those obtained using
a more complex sentence representations coming
from the deep learning models.

In the following section we provide a brief de-
scription of word embeddings and text summariza-
tion methods. The centroid-based summarization
method that uses word embeddings is described in
Section 3, followed by experimental results in Sec-
tion 4. Final remarks and a discussion about our
future plans are reported in Section 5.

2 Related Work

2.1 Word Embeddings
Word embedding stands for a continuous vector
representation able to capture syntactic and se-
mantic information of a word. Several methods
have been proposed in order to create word em-
beddings that follow the Distributional Hypothe-
sis (Harris, 1954). In our work we use two mod-
els1, continuous bag-of-words and skip-gram, in-
troduced by (Mikolov et al., 2013a). These mod-
els learn a vector representation for each word us-
ing a neural network language model and can be
trained efficiently on billions of words. Word2vec
allows to learn complex semantic relationships us-
ing simple vectorial operators, such as vec(king)
− vec(man) + vec(woman) ≈ vec(queen) and
vec(Barrett) − vec(singer) + vec(guitarist) ≈
vec(Gilmour). However, our method is general
and other approaches for building word embed-
dings can be used (Goldberg, 2015).

2.2 Text Summarization
Since the first method proposed by (Luhn, 1958),
automatic text summarization has been widely ad-
dressed by the research community with the pro-
posal of different methodologies as well as toolk-
its (Saggion and Gaizauskas, 2004). Good sur-
veys are proposed by (Jones, 2007; Saggion and
Poibeau, 2013). Since our method exploits word
embeddings as alternative representation to BOW,
here we focus on the methods sharing this fea-
ture. Methods based on matrix factorization, such
as Latent Semantic Analysis (LSA) (Ozsoy et
al., 2011) and Non-Negative Matrix Factorization
(NMF) (Lee et al., 2009), have the aim to arise
the latent factors by producing dense and compact
representations of sentences. Recently, riding the
wave of prominent results of modern Deep Learn-
ing (DL) models in many natural language pro-

1Commonly called word2vec.

Figure 1: Sentence and centroid embeddings 2D
visualization of the Donkey Kong (video game)
Wikipedia article. Dimensionality reduction is
performed using t-SNE algorithm. For each sen-
tence the position in the document is shown.
The closest sentences to centroid embedding are
marked in green.

cessing tasks (LeCun et al., 2015; Goodfellow et
al., 2016), several groups have started to exploit
deep neural networks for both abstractive (Rush
et al., 2015; Nallapati et al., 2016) and extractive
(Kågebäck et al., 2014; Cao et al., 2015; Cheng
and Lapata, 2016) text summarization.

3 Centroid-based Method

The centroid-based method for extractive summa-
rization was introduced by (Radev et al., 2004).
The centroid represents a pseudo-document which
condenses the meaningful information of a docu-
ment2. The main idea is to project in the vector
space the vector representations of both the cen-
troid and each sentence of a document. Then, the
sentences closer to the centroid are selected. The
original method adopts the BOW model for the
vector representations using the tf ∗ idf weight
scheme (Salton and McGill, 1986), where the size
of vectors is equal to that of the document vocab-
ulary. We adapt the centroid-based method intro-
ducing a distributed representation of words where
each word in a document is represented by a vector
of real numbers of an established size. Formally,
given a corpus of documents [D1, D2, . . . ] and its
vocabulary V with size N = |V |, we define a ma-
trix E ∈ RN,k, so-called lookup table, where the
i-th row is a word embedding of size k, k << N ,

2In this section we refer to a single document, but the
method can be extended to a cluster of documents.

13



arcade donkey kong game nintendo coleco centroid embedding
arcades goat hong gameplay mario intellivision nes
pac-man pig macao multiplayer wii atari gamecube
console monkey fung videogame console nes konami
famicom horse taiwan rpg nes msx wii

sega cow wong gamespot gamecube 3do famicom

Table 1: Centroid words of the Donkey Kong (video game) article having the tf-idf values greater than a
topic threshold equal to 0.3. For each centroid word, the five closest words are shown using the skip-gram
model trained on the Wikipedia (en) content. In the last column the words most similar to the centroid
embedding computed using element-wise addition are shown.

of the i-th word in V . The values of the word
embeddings matrix E are learned using the neu-
ral network model introduced by (Mikolov et al.,
2013b). The model can be trained on the collec-
tion of documents to be summarized or on a larger
corpus. This is a peculiar advantage of Represen-
tation Learning (RL) (Bengio et al., 2013) that al-
lows to reuse an external knowledge and this is
especially useful for the summarization of docu-
ments in specific domains, where large amount of
data are not available. After learning the lookup
table, our summarization method consists of four
steps: 1) preprocess the input document; 2) build
the centroid embedding; 3) compute the sentence
scores; 4) select the relevant sentences.

3.1 Preprocessing

The first step follows the common pipeline for the
summarization task: split the document into sen-
tences, convert all words in lower case and remove
stopwords. Stemming is not performed because
we let the word embeddings to discover the lin-
guistic regularities of words with the same root
(Mikolov et al., 2013c). For instance, the most
similar embeddings to the words that compose the
centroid vector using the skip-gram model trained
on the Wikipedia content are reported in Table 1.
The closest word of arcade is its plural arcades,
while they are orthogonal in the vector space ac-
cording to the BOW representation.

3.2 Centroid Embedding

In order to build a centroid vector using word em-
beddings, we first select the meaningful words into
the document. For simplicity and a fair compar-
ison with the original method, we select those
words having the tf ∗ idf weight greater than
a topic threshold. Thus, we compute the cen-

troid embedding as the sum3 of the embeddings
of the top ranked words in the document using the
lookup table E.

C =
∑

w∈D,tfidf(w)>t
E[idx(w)] (1)

In the eq. (1) we denote with C the cen-
troid embedding related to the document D and
with idx(w) a function that returns the index of
the word w in the vocabulary. In the headers of
the Table 1 the centroid words extracted from a
Wikipedia article are reported. The last column
shows the words most similar to centroid embed-
ding computed using element-wise addition. It is
important to underline that all five closest words to
the centroid vector are semantically related to the
main topic of the document despite the size of the
Wikipedia vocabulary (about 1 million words).

3.3 Sentence Scoring
For each sentence in the document, we create an
embedding representation by summing the vectors
for each word in the sentence stored in the lookup
table E.

Sj =
∑
w∈Sj

E[idx(w)] (2)

In the eq. (2) we denote with Sj the j-th sen-
tence in the document D. Then, the sentence score
is computed as the cosine similarity between the
embedding of the sentence Sj and that of the cen-
troid C of the document D.

sim(C, Sj) =
CT • Sj
||C|| · ||Sj || (3)

Figure 1 shows a visualization of sentence and
centroid embeddings of a Wikipedia article. We

3Some works use the average rather than the addition to
compose word embeddings. However, the sum and the aver-
age do not change the similarity value when using the cosine
distance, since the angle between vectors remains the same.

14



Sent ID Sentence Score
136 The original arcade version of the game appears in the Nintendo 64 game

Donkey Kong 64.
0.9533

131 The game was ported to Nintendo’s Family Computer (Famicom) console in
1983 as one of the system’s three launch titles; the same version was a launch
title for the Famicom’s North American version, the Nintendo Entertainment
System (NES).

0.9375

186 In 2004, Nintendo released Mario vs. Donkey Kong, a sequel to the Game
Boy title.

0.9366

192 In 2007, Donkey Kong Barrel Blast was released for the Nintendo Wii. 0.9362
135 The NES version was re-released as an unlockable game in Animal Crossing for

the GameCube and as an item for purchase on the Wii’s Virtual Console.
0.9308

Table 2: The most relevant sentences of the Donkey Kong article selected with the centroid-based sum-
marization method using word embeddings. For each sentence are reported the related position ID in
the document and the similarity score computed between sentence and centroid embeddings. The words
that compose the centroid vector are marked in bold. The most similar words to the centroid ones are
reported in italic.

use t-SNE method (van der Maaten and Hinton,
2008) to reduce the dimensionality of vectors from
300 to 2. For each sentence the position ID in
the document is shown. The closest sentences
to the centroid embedding are marked in green.
The words that compose the centroid are the same
showed in Table 1. In Table 2 we report the sen-
tences near to the centroid with the related cosine
similarity values. As we expected, the most rele-
vant sentence (136) contains many words close to
the centroid vector. However, the relevant aspect
concerns the last sentence (135). Despite this sen-
tence does not contain any centroid word, it has
a high similarity value so it is close to the cen-
troid embedding in the vector space. The reason
is due to the presence of the words, such as NES,
GameCube and Wii, that are the closest words to
the centroid embedding (Table 1). This proves the
effectiveness of the compositionality of word em-
beddings to encode the semantic relations between
words through vector dense representations.

3.4 Sentence Selection

The sentences are sorted in descending order of
their similarity scores. The top ranked sentences
are iteratively selected and added to the summary
until the limit4 is reached. In order to satisfy
the redundancy property, during the iteration we
compute the cosine similarity between the next
sentence and each one already in the summary.
We discard the incoming sentence if the similar-
ity value is greater than a threshold. This proce-
dure is reported in Algorithm 1. However, sim-

4The limit can be the number of bytes/words in the sum-
mary or a compression rate.

ilar sentence selection approaches are described
in (Carbonell and Goldstein, 1998; Saggion and
Gaizauskas, 2004).

Algorithm 1 Sentence selection
Input: S, Scores, st, limit
Output: Summary

S ← SORTDESC(S,Scores)
k ← 1
for i← 1 to m do

length← LENGTH(Summary)
if length > limit then return Summary
SV ← SUMVECTORS(S[i])
include← True
for j ← 1 to k do

SV 2← SUMVECTORS(Summary[j])
sim← SIMILARITY(SV ,SV 2)
if sim > st then

include← False
if include then

Summary[k]← S[i]
k ← k + 1

4 Experiments

In this section we describe the benchmarks con-
ducted on two text summarization tasks. The main
goal is to compare the centroid-based method us-
ing two different representations (bag-of-words
and word embeddings). In Section 4.1 and in Sec-
tion 4.2 we report the experimental results carried
out on Multi-Document and Multilingual Single
Document summarization tasks, respectively.

15



4.1 Multi-Document Summarization

Dataset and Metrics During the document un-
derstanding conference (DUC)5 from 2001 to
2007, several gold standard datasets have been
developed to evaluate the summarization meth-
ods. In particular, we evaluate our method on
multi-document summarization using the DUC-
2004 Task 2 dataset composed by 50 clusters, each
of which consists of 10 documents coming from
Associated Press and New York Times newswires.
For each cluster, four summaries written by dif-
ferent humans were supplied. For the evaluation,
we adopt the ROUGE (Lin, 2004), a set of recall-
based metrics that compare the automatic and hu-
man summaries on the basis of the n-gram overlap.
In our experiment, we adopt both ROUGE-1 and
ROUGE-26.

Baselines For the comparison, we propose sev-
eral baselines. Firstly, we adapt the cen-
troid method proposed by (Radev et al., 2004)
(C BOW) for a fair comparison. In the original
work, the sentence scores are the linear combina-
tion of the centroid score, the positional value and
the first sentence overlap. The centroid score is
the sum of tf ∗ idf weights of the words occur-
ring both in the sentence and in the centroid. In
our experiment, we apply both our sentence score
and selection algorithms. LEAD simply chooses
the first 665 bytes from the most recent article in
each cluster. SumBasic is a simple probabilistic
method proposed by (Nenkova and Vanderwende,
2005) commonly used as baseline in the summa-
rization evaluation. Peer65 is the winning system
in DUC-2004 Task 2. To compare our method
with others which also use compact and dense
representations, we use the method proposed by
(Lee et al., 2009) that adopts the generic rele-
vance of sentences method using NMF. Another
method often used in summarization evaluations
is LexRank proposed by (Erkan and Radev, 2004)
which uses the TextRank algorithm (Mihalcea and
Tarau, 2004) to establish a ranking between sen-
tences. Finally, we compare our method with the
one proposed by (Cao et al., 2015) that uses Recur-
sive Neural Network (RNN) for learning sentence
embeddings by encoding syntactic features.

5http://duc.nist.gov
6ROUGE-1.5.5 with options -c 95 -b 665 -m -n 2 -x

System R1 R2 tt st size
LEAD 32.42 6.42
SumBasic 37.27 8.58
Peer65 38.22 9.18
NMF 31.60 6.31
LexRank 37.58 8.78
RNN 38.78 9.86
C BOW 37.76 8.08 0.1 0.6
C GNEWS 37.91 8.45 0.2 0.9 300
C CBOW 38.68 8.93 0.3 0.93 200
C SKIP 38.81 9.97 0.3 0.94 400

Table 3: ROUGE scores (%) on DUC-2004
dataset. tt and st are the topic and similarity
thresholds respectively. size is the dimension of
embeddings.

Implementation Our system7 is written in
Python by relying on nltk, scikit-learn and gen-
sim libraries for text preprocessing, building the
sentence-term matrix and import the word2vec
model. We train the word embeddings on DUC-
2004 corpus using the original word2vec8 imple-
mentation. We test both continuous bag-of-words
(C CBOW) and skip-gram (C SKIP) neural ar-
chitectures proposed in (Mikolov et al., 2013a) us-
ing the same parameters9 but varying the embed-
ding sizes. Moreover, we compare our method us-
ing the model trained on a part of Google News
dataset (C GNEWS) which consists of about 100
billion words. In the preprocessing step each clus-
ter of documents is divided in sentences and stop-
words are removed. We do not perform stemming
as reported in Section 3. To find the best parame-
ters configuration, we run a grid search using this
setting: embedding size in [100, 200, 300, 400,
500], topic and similarity thresholds respectively
in [0, 0.5] and [0.5, 1] with a step of 0.01.

Results and Discussion The results of the ex-
periment are shown in Table 3. We report the
best scores of our method using the three different
word2vec models along with their parameters. For
all word embeddings models, our method outper-
forms the original centroid one. In detail, with the
skip-gram model we obtain an increment of 1.05%
and 1.71% with respect to the BOW model using
ROUGE-1 and ROUGE-2 respectively. Moreover,
our simple method with skip-gram performs bet-

7https://github.com/gaetangate/text-summarizer
8https://code.google.com/archive/p/word2vec/
9-hs 1 -min-count 0 -window 10 -negative 10 -iter 10

16



ter than the more complex models based on RNN.
This proves the effectiveness of the compositional
capability of word embeddings in order to en-
code the information word sequences by applying
a simple sum of word vectors, as already proved in
(Wieting et al., 2015). Although our method with
the model pre-trained on Google News does not
achieve the best score, it is interesting to notice the
flexibility of the word embeddings in reusing ex-
ternal knowledge. Regarding the comparison be-
tween BOW and embedding representations, the
experiment shows different behaviors of the simi-
larity threshold. In particular, the use of word2vec
requires a higher threshold because the word em-
beddings are dense vectors unlike the sparse rep-
resentation of BOW. This proves that the embed-
dings of sentences are closer in the vector space,
thus the cosine similarity returns closer values.

System R1 R2 tt st size
C BOW 37.56 8.26 0 0.6
C GNEWS 36.91 7.35 0 0.9 300
C CBOW 37.69 7.64 0 0.83 300
C SKIP 37.61 8.10 0 0.91 300

Table 4: ROUGE scores without topic threshold.

Also the topic threshold shows different trends.
The word embeddings require a higher threshold
value to make our method effective. In order to
analyze this aspect we run another experiment set-
ting the topic threshold to 0. The results are re-
ported in Table 4. Results show that the BOW
representation is more stable and obtains the best
ROUGE-2 score, while the performance obtained
by word2vec decreases considerably. This means
that word embeddings are more sensitive to noise
and they require an accurate choice of the mean-
ingful words to compose the centroid vector.

Summaries Overlap Although the different
methods achieve similar ROUGE scores, they not
necessarily generate similar summaries. An ex-
ample is reported in Table 6. In this section we
conduct a further analysis by comparing the sum-
maries generated by the best four configurations of
the centroid method reported in Table 3. We adopt
the same criterion presented in (Hong et al., 2014),
where the different summaries are compared in
terms of sentences and words overlap using the
Jaccard coefficient. Due to space constraint, we
report in Table 5 only the sentence overlap. The
results prove that different word representations

GNEWS CBOW SKIP BOW
GNEWS 1 0.109 0.171 0.075

CBOW 1 0.460 0.072
SKIP 1 0.105
BOW 1

Table 5: Sentence overlap.

lead to different summaries. In particular, the sum-
maries using BOW differ considerably from those
generated using word2vec, but this is true even for
different embedding models. On the other hand,
only the models trained on the DUC-2004 corpus
(CBOW and SKIP) tend to generate more similar
summaries. This analysis suggests that a combina-
tion of various models trained on different corpora
could result in good performance.

4.2 Multilingual Document Summarization

Task Description We carried out an experiment
on Multilingual Single-document Summarization
(MSS). Our main goal is to prove empirically
the effectiveness of the use of word embeddings
in the document summarization task across dif-
ferent languages. For this purpose, we evaluate
our method on the MSS task proposed in Multi-
Ling 2015 (Giannakopoulos et al., 2015), a spe-
cial session at SIGDIAL 2015. Starting from 2011
the aim of MultiLing community is to promote
the cutting-edge research in automatic summa-
rization by providing datasets and by introducing
several pilot tasks to encourage further develop-
ments in single and multi-document summariza-
tion and in summarizing human dialogs in on-line
forums and customer call centers. The goal of
the MSS 2015 task is to generate a single doc-
ument summary from a selection of some of the
best written Wikipedia articles with at least one
out of 38 languages defined by organizers of the
task. The dataset10 is divided into a training and
a test sets, both consisting of 30 documents for
each of 38 languages. For both datasets, the body
of the articles and the related abstracts with the
character length limits are provided. Since the
Wikipedia abstracts are summaries written by hu-
mans, they are useful to perform automatic evalua-
tions. We evaluate our method using five different
languages: English, Italian, German, Spanish and
French.

10http://multiling.iit.demokritos.gr/pages/view/1532/task-
mss-single-document-summarization-data-and-information

17



BOW - Bag of Words baseline

The controversy centers on the payment of nearly dlrs 400,000 in scholarships to relatives of IOC members by the Salt Lake bid committee which won
the right to stage the 2002 games. Pound said the panel would investigate allegations that “there may or may not have been payments for the benefit of
members of the IOC or their families connected with the Salt Lake City bid.” Samaranch said he was surprised at the allegations of corruption in the
International Olympic Committee made by senior Swiss member Marc Hodler.

CBOW - Continuous Bag of Words trained on DUC-2004 dataset

Marc Hodler, a senior member of the International Olympic Committee executive board, alleged malpractices in the voting for the 1996 Atlanta Games,
2000 Sydney Olympics and 2002 Salt Lake Games. The IOC, meanwhile, said it was prepared to investigate allegations made by Hodler of bribery in the
selection of Olympic host cities. The issue of vote-buying came to the fore in Lausanne because of the recent disclosure of scholarship payments made
to six relatives of IOC members by Salt Lake City officials during their successful bid to play host to the 2002 Winter Games.

SKIP - Skip-gram trained on DUC-2004 dataset

Marc Hodler, a senior member of the International Olympic Committee executive board, alleged malpractices in the voting for the 1996 Atlanta Games,
2000 Sydney Olympics and 2002 Salt Lake Games. The IOC, meanwhile, said it was prepared to investigate allegations made by Hodler of bribery in the
selection of Olympic host cities. Saying “if we have to clean, we will clean,” Juan Antonio Samaranch responded on Sunday to allegations of corruption
in the Olympic bidding process by declaring that IOC members who were found to have accepted bribes from candidate cities could be expelled.

GNEWS - Skip-gram trained on Google News dataset

The International Olympic Committee has ordered a top-level investigation into the payment of nearly dlrs 400,000 in scholarships to relatives of IOC
members by the Salt Lake group which won the bid for the 2002 Winter Games. The mayor of the Japanese city of Nagano, site of the 1998 Winter
Olympics, denied allegations that city officials bribed members of the International Olympic Committee to win the right to host the games. Swiss IOC
executive board member Marc Hodler said Sunday he might be thrown out of the International Olympic Committee for making allegations of corruption
within the Olympic movement.

Table 6: Summaries of the cluster d30038 in DUC-2004 dataset using the centroid-based summarization
method with different sentence representations.

Model Configuration In order to learn word
embeddings for the different languages, we ex-
ploit five Wikipedia dumps11, one for each cho-
sen language. We extract the plain text from the
Wiki markup language using Wikiextractor12, a
Wikimedia parser written in Python. Each article
is converted from UTF-8 to ASCII encoding us-
ing the Unidecode Python package. Since in the
previous evaluation we observe a similar behav-
ior between the continuous bag of words and skip-
gram models, in this evaluation we adopt only the
skip-gram one using the same training parame-
ters13 for all five languages. The Table 7 reports
the Wikipedia statistics for the five languages re-
garding the number of words and the size of the
vocabularies.

Language # Words Vocabulary
English 1,890,356,976 973,839
Italian 371,218,773 378,286

German 657,234,125 1,042,683
Spanish 464,465,399 419,683
French 551,057,299 458,748

Table 7: Wikipedia statistics.

Experiment Protocol In order to reproduce the
same challenging scenario of the MultiLing 2015

11https://dumps.wikimedia.org/ lang wiki/20161220/
with lang in [en, it, de, es, fr]

12https://github.com/attardi/wikiextractor/wiki
13-hs 1 -min-count 10 -window 8 -negative 5 -iter 5

MSS task, we performed the tuning of parameters
using only the training set. To find the best topic
and similarity threshold parameters we run a grid
search as explained in Section 4.1. The grid search
is performed for each language separately using
both BOW and skip-gram representations. The pa-
rameter configurations are in line with those of the
previous experiment on DUC-2004. In detail, the
topic thresholds are in the range [0.1, 0.2] using
the BOW model and in the range [0.3, 0.5] using
word embeddings. While, the similarity thresh-
olds are slightly higher w.r.t. the multi-document
experiment: about 0.7 and 0.95 for BOW and
skip-gram, respectively. This is due to the fact
that too similar sentences are rare, especially with
well-written documents as Wikipedia articles. The
best parameters configuration for each language is
used to generate summaries for the documents in
the test set. Also for this task, each document is
preprocessed with the sentences segmentation and
stopwords removal, without stemming. We adopt
the same automatic evaluation metrics used by the
participating systems in MSS 2015 task: ROUGE-
1, -2, -SU414. ROUGE-SU4 computes the score
between the generated and human summaries con-
sidering the overlap of the skip-bigrams of 4 as
well as the unigrams. Finally, the generated sum-
mary for each document must comply with a spe-
cific length constraint (rather than using a unique
length limit for the whole collection). This differs

14ROUGE-1.5.7 with options -n 2 -2 4 -u -x -m

18



English Italian German Spanish French
R1 R2 R1 R2 R1 R2 R1 R2 R1 R2

LEAD 44.33 11.68 30.46 4.38 29.13 3.21 43.02 9.17 42.73 8.07
WORST 37.17 9.93 39.68 10.01 33.02 4.88 45.20 13.04 46.68 12.96

BEST 50.38 15.10 43.87 12.50 40.58 8.80 53.23 17.86 51.39 15.38
C BOW 49.06 13.43 33.44 4.82 35.28 4.93 48.38 12.88 46.13 10.45
C W2V 50.43‡ 13.34† 35.12 6.81 35.38† 5.39† 49.25† 12.99 47.82† 12.15

ORACLE 61.91 22.42 53.31 17.51 54.34 13.32 62.55 22.36 58.68 17.18

Table 8: ROUGE-1, -2 scores (%) on MultiLing MSS 2015 dataset for five different languages.

from the previous evaluation on DUC-2004.

Results and Discussion The results for each
languages are shown in Table 8. We report the
ROUGE-1, -2 scores for each chosen language.
LEAD and C BOW represent the same baselines
used in the multi-document experiment. The for-
mer uses the initial text of each article truncated
to the length of the Wikipedia abstract. The lat-
ter is the centroid-based method with the BOW
representation. Our method that uses word em-
beddings learned with skip-gram model is labeled
with C W2V. For each metric and language we
also report the WORST and the BEST scores15
obtained by the 23 participating systems at MSS
2015 task. Finally, ORACLE scores can be con-
sidered as an upper bound approximation for the
extractive summarization methods. It uses a cover-
ing algorithm (Davis et al., 2012) that selects sen-
tences from the original text covering the words in
the summary without disregarding the length limit.
We highlight in bold the scores of our method
when it outperforms the baseline C BOW. On the
other hand, the superscripts † and ‡ imply a bet-
ter performance of our method with respect to the
WORST and the BEST scores respectively.

Both centroid-based methods overcome the
simple baseline over all languages. Our method
always achieves better scores against the BOW
model except for ROUGE-2 metric for English.
This confirms the effectiveness of using word
embeddings as alternative sentence representa-
tions able to capture the semantic similarities be-
tween the centroid words and the sentences, when
summarizing single documents too. Moreover,
our method outperforms substantially the lowest
scores performing systems participating in MSS
2015 task for English and German languages. For

15http://multiling.iit.demokritos.gr/file/view/1629/mss-
multilingual-single-document-summarization-submission-
and-automatic-score

English our method obtains a ROUGE-1 score
even better than the one of the best system in
MSS 2015. Instead, our method fails in summa-
rizing Italian documents and it achieves the worst
ROUGE-2 for Spanish and French languages. The
reason may lie in the size of the Wikipedia dumps
used to learn the word embeddings for different
languages. As showing in Table 7, the sizes of
the various corpora as well as the ratios between
the number of words and dimension of the vocab-
ularies, differ consistently. The English version
of Wikipedia consists of nearly 2 billion words
against about 300 million words of Italian one.
Thus, according to the distributional hypothesis
reported in (Harris, 1954), we expect better per-
formance for our method in summarizing English
or German articles with respect to the other lan-
guages where the word embeddings are learned
using a smaller corpus. Our results and in partic-
ular the ROUGE-SU4 scores reported in Figure 2
support this hypothesis.

Figure 2: ROUGE-SU4 scores (%) comparison on
MultiLing MSS 2015 dataset.

19



5 Conclusion

In this paper, we propose a centroid-based method
for extractive summarization which exploits the
compositional capability of word embeddings.
One of the advantages of our method lies on its
simplicity. Indeed, it can be used as a base-
line in experimenting new articulate semantic rep-
resentations in summarization tasks. Moreover,
following the idea of representation learning, it
is feasible to infuse knowledge by training the
word embeddings from external sources. Finally,
the proposed method is fully unsupervised, thus
it can be adopted in other summarization tasks,
such as query-based document summarization. As
future work, we plan to evaluate the centroid-
based summarization method using a topic model,
such as Latent Dirichlet Allocation (LDA) (Blei
et al., 2003) or Non-negative Matrix Factoriza-
tion (NMF) (Berry et al., 2007), in order to ex-
tract the meaningful words to compute the cen-
troid embedding as well as to carry out a com-
prehensive comparison of different sentence rep-
resentations using more complex neural language
models (Le and Mikolov, 2014; Zhang and Le-
Cun, 2015; Józefowicz et al., 2016). Finally,
the combination of distributional and relational se-
mantics (Fried and Duh, 2014; Verga and McCal-
lum, 2016; Rossiello, 2016) applied to extractive
text summarization is a promising further direc-
tion that we want to investigate.

References

Y. Bengio, A. Courville, and P. Vincent. 2013. Repre-
sentation learning: A review and new perspectives.
IEEE Transactions on Pattern Analysis and Machine
Intelligence, 35(8):1798–1828, Aug.

Michael W. Berry, Murray Browne, Amy N. Langville,
V. Paul Pauca, and Robert J. Plemmons. 2007. Al-
gorithms and applications for approximate nonneg-
ative matrix factorization. Computational Statistics
& Data Analysis, 52(1):155–173, September.

David M. Blei, Andrew Y. Ng, and Michael I. Jordan.
2003. Latent dirichlet allocation. J. Mach. Learn.
Res., 3:993–1022, March.

Ziqiang Cao, Furu Wei, Li Dong, Sujian Li, and
Ming Zhou. 2015. Ranking with recursive neu-
ral networks and its application to multi-document
summarization. In Proceedings of the Twenty-
Ninth AAAI Conference on Artificial Intelligence,
AAAI’15, pages 2153–2159. AAAI Press.

Jaime Carbonell and Jade Goldstein. 1998. The use of
mmr, diversity-based reranking for reordering docu-
ments and producing summaries. In Proceedings of
the 21st Annual International ACM SIGIR Confer-
ence on Research and Development in Information
Retrieval, SIGIR ’98, pages 335–336, New York,
NY, USA. ACM.

Jianpeng Cheng and Mirella Lapata. 2016. Neural
summarization by extracting sentences and words.
pages 484–494, August.

S. T. Davis, J. M. Conroy, and J. D. Schlesinger. 2012.
Occams – an optimal combinatorial covering algo-
rithm for multi-document summarization. In 2012
IEEE 12th International Conference on Data Min-
ing Workshops, pages 454–463, Dec.

Gunes Erkan and Dragomir R. Radev. 2004. Lexrank:
Graph-based lexical centrality as salience in text
summarization. J. Artif. Int. Res., 22(1):457–479,
December.

Daniel Fried and Kevin Duh. 2014. Incorporating both
distributional and relational semantics in word rep-
resentations. CoRR, abs/1412.4369.

George Giannakopoulos, Jeff Kubina, John M. Conroy,
Josef Steinberger, Benoı̂t Favre, Mijail A. Kabadjov,
Udo Kruschwitz, and Massimo Poesio. 2015. Mul-
tiling 2015: Multilingual summarization of single
and multi-documents, on-line fora, and call-center
conversations. In Proceedings of the SIGDIAL 2015
Conference., pages 270–274.

Yoav Goldberg. 2015. A primer on neural network
models for natural language processing. CoRR,
abs/1510.00726.

Ian Goodfellow, Yoshua Bengio, and Aaron Courville.
2016. Deep Learning. MIT Press. http://www.
deeplearningbook.org.

Zellig Harris. 1954. Distributional structure. Word,
10(23):146–162.

Kai Hong, John Conroy, Benoit Favre, Alex Kulesza,
Hui Lin, and Ani Nenkova. 2014. A repository of
state of the art and competitive baseline summaries
for generic news summarization. In Nicoletta Cal-
zolari (Conference Chair), Khalid Choukri, Thierry
Declerck, Hrafn Loftsson, Bente Maegaard, Joseph
Mariani, Asuncion Moreno, Jan Odijk, and Stelios
Piperidis, editors, Proceedings of the Ninth Interna-
tional Conference on Language Resources and Eval-
uation (LREC’14), Reykjavik, Iceland, may. Euro-
pean Language Resources Association (ELRA).

Karen Spärck Jones. 2007. Automatic summarising:
The state of the art. Information Processing & Man-
agement, 43(6):1449–1481.

Rafal Józefowicz, Oriol Vinyals, Mike Schuster, Noam
Shazeer, and Yonghui Wu. 2016. Exploring the lim-
its of language modeling. CoRR, abs/1602.02410.

20



Mikael Kågebäck, Olof Mogren, Nina Tahmasebi, and
Devdatt Dubhashi. 2014. Extractive summariza-
tion using continuous vector space models. In Pro-
ceedings of the 2nd Workshop on Continuous Vector
Space Models and their Compositionality (CVSC)@
EACL, pages 31–39.

Quoc Le and Tomas Mikolov. 2014. Distributed rep-
resentations of sentences and documents. In Tony
Jebara and Eric P. Xing, editors, Proceedings of the
31st International Conference on Machine Learning
(ICML-14), pages 1188–1196. JMLR Workshop and
Conference Proceedings.

Yann LeCun, Yoshua Bengio, and Geoffrey Hinton.
2015. Deep learning. Nature, 521:436–444.

Ju-Hong Lee, Sun Park, Chan-Min Ahn, and Daeho
Kim. 2009. Automatic generic document summa-
rization based on non-negative matrix factorization.
Inf. Process. Manage., 45(1):20–34, January.

Hui Lin and Jeff Bilmes. 2011. A class of submodu-
lar functions for document summarization. In Pro-
ceedings of the 49th Annual Meeting of the Associ-
ation for Computational Linguistics: Human Lan-
guage Technologies, pages 510–520, Portland, Ore-
gon, USA, June. Association for Computational Lin-
guistics.

Chin-Yew Lin. 2004. Rouge: A package for auto-
matic evaluation of summaries. In Stan Szpakowicz
Marie-Francine Moens, editor, Text Summarization
Branches Out: Proceedings of the ACL-04 Work-
shop, pages 74–81, Barcelona, Spain, July. Associa-
tion for Computational Linguistics.

H. P. Luhn. 1958. The automatic creation of literature
abstracts. IBM J. Res. Dev., 2(2):159–165, April.

Rada Mihalcea and Paul Tarau. 2004. Textrank:
Bringing order into texts. In Dekang Lin and Dekai
Wu, editors, Proceedings of EMNLP 2004, pages
404–411, Barcelona, Spain, July. Association for
Computational Linguistics.

Tomas Mikolov, Kai Chen, Greg Corrado, and Jef-
frey Dean. 2013a. Efficient estimation of word
representations in vector space. arXiv preprint
arXiv:1301.3781.

Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Cor-
rado, and Jeff Dean. 2013b. Distributed represen-
tations of words and phrases and their composition-
ality. In Advances in neural information processing
systems, pages 3111–3119.

Tomas Mikolov, Wen-tau Yih, and Geoffrey Zweig.
2013c. Linguistic regularities in continuous space
word representations. In Proceedings of the 2013
Conference of the North American Chapter of the
Association for Computational Linguistics: Human
Language Technologies, pages 746–751, Atlanta,
Georgia, June. Association for Computational Lin-
guistics.

Ramesh Nallapati, Bowen Zhou, Cicero dos Santos,
Caglar Gulcehre, and Bing Xiang. 2016. Abstrac-
tive text summarization using sequence-to-sequence
rnns and beyond. pages 280–290, August.

Ani Nenkova and Lucy Vanderwende. 2005. The im-
pact of frequency on summarization. Microsoft Re-
search, Redmond, Washington, Tech. Rep. MSR-TR-
2005-101.

Makbule G. Ozsoy, Ferda N. Alpaslan, and Ilyas Ci-
cekli. 2011. Text summarization using latent se-
mantic analysis. Journal of Information Science,
37(4):405–417.

Dragomir R. Radev, Hongyan Jing, Malgorzata Stys,
and Daniel Tam. 2004. Centroid-based summariza-
tion of multiple documents. Inf. Process. Manage.,
40(6):919–938, November.

Gaetano Rossiello. 2016. Neural abstractive text sum-
marization. In Proceedings of the Doctoral Consor-
tium of AI*IA 2016 co-located with the 15th Inter-
national Conference of the Italian Association for
Artificial Intelligence (AI*IA 2016), Genova, Italy,
November 29, 2016., pages 70–75.

Alexander M. Rush, Sumit Chopra, and Jason Weston.
2015. A neural attention model for abstractive sen-
tence summarization. In Proceedings of the 2015
Conference on Empirical Methods in Natural Lan-
guage Processing, pages 379–389, Lisbon, Portugal,
September. Association for Computational Linguis-
tics.

Horacio Saggion and Robert Gaizauskas. 2004. Multi-
document summarization by cluster/profile rele-
vance and redundancy removal. In Proceedings of
the HLT/NAACL Document Understanding Work-
shop (DUC 2004), Boston, May.

Horacio Saggion and Thierry Poibeau. 2013. Auto-
matic text summarization: Past, present and future.
In Multi-source, Multilingual Information Extrac-
tion and Summarization, pages 3–21. Springer.

Gerard Salton and Michael J. McGill. 1986. Intro-
duction to Modern Information Retrieval. McGraw-
Hill, Inc., New York, NY, USA.

Laurens van der Maaten and Geoffrey E. Hinton.
2008. Visualizing high-dimensional data using
t-sne. Journal of Machine Learning Research,
9:2579–2605.

Patrick Verga and Andrew McCallum. 2016. Row-less
universal schema. CoRR, abs/1604.06361.

John Wieting, Mohit Bansal, Kevin Gimpel, and Karen
Livescu. 2015. Towards universal paraphrastic sen-
tence embeddings. CoRR, abs/1511.08198.

Xiang Zhang and Yann LeCun. 2015. Text understand-
ing from scratch. CoRR, abs/1502.01710.

21


