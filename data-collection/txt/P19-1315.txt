



















































Towards Understanding Linear Word Analogies


Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 3253–3262
Florence, Italy, July 28 - August 2, 2019. c©2019 Association for Computational Linguistics

3253

Towards Understanding Linear Word Analogies

Kawin Ethayarajh, David Duvenaud†, Graeme Hirst
University of Toronto

†Vector Institute
{kawin, duvenaud, gh}@cs.toronto.edu

Abstract

A surprising property of word vectors is that
word analogies can often be solved with vector
arithmetic. However, it is unclear why arith-
metic operators correspond to non-linear em-
bedding models such as skip-gram with neg-
ative sampling (SGNS). We provide a formal
explanation of this phenomenon without mak-
ing the strong assumptions that past theories
have made about the vector space and word
distribution. Our theory has several implica-
tions. Past work has conjectured that linear
substructures exist in vector spaces because re-
lations can be represented as ratios; we prove
that this holds for SGNS. We provide novel
justification for the addition of SGNS word
vectors by showing that it automatically down-
weights the more frequent word, as weighting
schemes do ad hoc. Lastly, we offer an in-
formation theoretic interpretation of Euclidean
distance in vector spaces, justifying its use in
capturing word dissimilarity.

1 Introduction

Distributed representations of words are a corner-
stone of current methods in natural language pro-
cessing. Word embeddings, also known as word
vectors, can be generated by a variety of models,
all of which share Firth’s philosophy (1957) that
the meaning of a word is defined by “the company
it keeps”. The simplest such models obtain word
vectors by constructing a low-rank approximation
of a matrix containing a co-occurrence statistic
(Landauer and Dumais, 1997; Rohde et al., 2006).
In contrast, neural network models (Bengio et al.,
2003; Mikolov et al., 2013b) learn word embed-
dings by trying to predict words using the contexts
they appear in, or vice-versa.

A surprising property of word vectors learned
via neural networks is that word analogies can of-
ten be solved with vector arithmetic. For example,

‘king is to ? as man is to woman’ can be solved by
finding the closest vector to ~king− ~man+ ~woman,
which should be ~queen. It is unclear why arith-
metic operators can effectively compose embed-
dings generated by non-linear models such as
skip-gram with negative sampling (SGNS). There
have been two attempts to rigorously explain this
phenomenon, but both have made strong assump-
tions about either the embedding space or the word
distribution. The paraphrase model (Gittens et al.,
2017) hinges on words having a uniform distribu-
tion rather than the typical Zipf distribution, which
the authors themselves acknowledge is unrealistic.
The latent variable model (Arora et al., 2016) as-
sumes that word vectors are known a priori and
generated by randomly scaling vectors sampled
from the unit sphere.

In this paper, we explain why – and under what
conditions – word analogies can be solved with
vector arithmetic, without making the strong as-
sumptions past work has. We focus on GloVe and
SGNS because they implicitly factorize a word-
context matrix containing a co-occurrence statis-
tic (Levy and Goldberg, 2014), which allows us to
interpret the inner product of a word and context
vector. We begin by formalizing word analogies
as functions that transform one word vector into
another. When this transformation is simply the
addition of a displacement vector – as is the case
when using vector arithmetic – we call the anal-
ogy a linear analogy. Central to our theory is the
expression PMI(x,y) + log p(x,y), which we call
the co-occurrence shifted pointwise mutual infor-
mation (csPMI) of (x,y).

We prove that in both SGNS and GloVe spaces
without reconstruction error (i.e., when the factor-
ized word-context matrix can be perfectly recon-
structed), a linear analogy holds over a set of or-
dered word pairs iff csPMI(x,y) is the same for
every word pair, csPMI(x1,x2) = csPMI(y1,y2)



3254

for any two word pairs, and the row vectors of
x1,x2,y1,y2 in the factorized matrix are coplanar.
By then framing vector addition as a kind of word
analogy, we offer several new insights:

1. Past work has often cited the Pennington
et al. (2014) conjecture as an intuitive ex-
planation of why vector arithmetic works for
analogy solving. The conjecture is that an
analogy of the form a is to b as x is to y
holds iff p(w|a)/p(w|b)≈ p(w|x)/p(w|y) for
every word w in the vocabulary. While this
is sensible, it is not based on any theoretical
derivation or empirical support. We provide
a formal proof that this is indeed true.

2. Consider two words x,y and their sum ~z =
~x+~y in an SGNS embedding space with no
reconstruction error. If z were in the vo-
cabulary, the similarity between z and x (as
measured by the csPMI) would be the log
probability of y shifted by a model-specific
constant. This implies that the addition of
two words automatically down-weights the
more frequent word. Since many weight-
ing schemes are based on the idea that more
frequent words should be down-weighted ad
hoc (Arora et al., 2017), the fact that this is
done automatically provides novel justifica-
tion for using addition to compose words.

3. Consider any two words x,y in an SGNS
or GloVe embedding space with no recon-
struction error. The squared Euclidean dis-
tance between ~x and ~y is a decreasing lin-
ear function of csPMI(x,y). In other words,
the more similar two words are (as measured
by csPMI) the smaller the distance between
their vectors. Although this is intuitive, it is
also the first rigorous explanation of why the
Euclidean distance in embedding space is a
good proxy for word dissimilarity.

Although our main theorem only concerns em-
bedding spaces with no reconstruction error, we
also explain why, in practice, linear word analo-
gies hold in embedding spaces with some noise.
We conduct experiments that support the few as-
sumptions we make and show that the transforma-
tions represented by various word analogies corre-
spond to different csPMI values. Without making
the strong assumptions of past theories, we thus
offer a formal explanation of why, and when, word
analogies can be solved with vector arithmetic.

2 Related Work

PMI Pointwise mutual information (PMI) cap-
tures how much more frequently x,y co-occur than
by chance (Church and Hanks, 1990):

PMI(x,y) = log
p(x,y)

p(x)p(y)
(1)

Word Embeddings Word embeddings are dis-
tributed representations in a low-dimensional con-
tinuous space. Also called word vectors, they cap-
ture semantic and syntactic properties of words,
even allowing relationships to be expressed arith-
metically (Mikolov et al., 2013b). Word vectors
are generally obtained in two ways: (a) from neu-
ral networks that learn representations by predict-
ing co-occurrence patterns in the training corpus
(Bengio et al., 2003; Mikolov et al., 2013b; Col-
lobert and Weston, 2008); (b) from low-rank ap-
proximations of word-context matrices containing
a co-occurrence statistic (Landauer and Dumais,
1997; Levy and Goldberg, 2014).

SGNS The objective of skip-gram with nega-
tive sampling (SGNS) is to maximize the proba-
bility of observed word-context pairs and to mini-
mize the probability of k randomly sampled nega-
tive examples. For an observed word-context pair
(w,c), the objective would be logσ(~w ·~c) + k ·
Ec′∼Pn [log(−~w ·~c′)], where c′ is the negative con-
text, randomly sampled from a scaled distribution
Pn. Though no co-occurrence statistics are explic-
itly calculated, Levy and Goldberg (2014) proved
that SGNS is in fact implicitly factorizing a word-
context PMI matrix shifted by − logk.

Latent Variable Model The latent variable
model (Arora et al., 2016) was the first attempt
at rigorously explaining why word analogies can
be solved arithmetically. It is a generative model
that assumes that word vectors are generated by
the random walk of a “discourse” vector on the
unit sphere. Gittens et al.’s criticism of this proof
is that it assumes that word vectors are known a
priori and generated by randomly scaling vectors
uniformly sampled from the unit sphere (or hav-
ing properties consistent with this sampling pro-
cedure). The theory also relies on word vectors
being uniformly distributed (isotropic) in embed-
ding space; however, experiments by Mimno and
Thompson (2017) have found that this generally
does not hold in practice, at least for SGNS.



3255

Paraphrase Model The paraphrase model (Git-
tens et al., 2017) was the only other attempt to for-
mally explain why word analogies can be solved
arithmetically. It proposes that any set of con-
text words C = {c1, ...,cm} is semantically equiva-
lent to a single word c if p(w|c1, ...,cm) = p(w|c).
One problem with this is that the number of pos-
sible context sets far exceeds the vocabulary size,
precluding a one-to-one mapping; the authors cir-
cumvent this problem by replacing exact equal-
ity with the minimization of KL divergence. As-
suming that the words have a uniform distribution,
the paraphrase of C can then be written as an un-
weighted sum of its context vectors. However, this
uniformity assumption is unrealistic – word fre-
quencies obey a Zipf distribution, which is Pareto
(Piantadosi, 2014). A later attempt at using para-
phrases (Allen and Hospedales, 2019) completely
ignores the effect of negative sampling in SGNS’
factorization. Neither work provides any empiri-
cal evidence in support of the paraphrase model.

3 The Structure of Word Analogies

3.1 Formalizing Analogies

A word analogy is a statement of the form “a is to
b as x is to y”, which we will write as (a,b)::(x,y).
It asserts that a and x can be transformed in the
same way to get b and y respectively, and that b
and y can be inversely transformed to get a and x.
A word analogy can hold over an arbitrary num-
ber of ordered pairs: e.g., “Berlin is to Germany
as Paris is to France as Ottawa is to Canada ...”.
The elements in each pair are not necessarily in
the same space – for example, the transformation
for (king,roi)::(queen,reine) is English-to-French
translation. For (king,queen)::(man,woman), the
canonical analogy in the literature, the transforma-
tion corresponds to changing the gender. There-
fore, to formalize the definition of an analogy, we
will refer to it as a transformation.

Definition 1 An analogy f is an invertible trans-
formation that holds over a set of ordered pairs S
iff ∀ (x,y) ∈ S, f (x) = y∧ f−1(y) = x.

The word embedding literature (Mikolov et al.,
2013b; Pennington et al., 2014) has focused on
a very specific type of transformation, the ad-
dition of a displacement vector. For example,
for (king,queen)::(man,woman), the transforma-
tion would be ~king + ( ~woman− ~man) = ~queen,
where the displacement vector is expressed as the

difference ( ~woman− ~man). To make a distinction
with our general class of analogies in Definition 1,
we will refer to these as linear analogies.

Definition 2 A linear analogy f is an invertible
transformation of the form~x 7→~x+~r. f holds over
a set of ordered pairs S iff ∀ (x,y) ∈ S,~x+~r =~y.

Definition 3 Let W be an SGNS or GloVe word
embedding space and C its corresponding context
space. Let k denote the number of negative sam-
ples, Xx,y the frequency, and bx,by the learned bi-
ases for GloVe. If there is no reconstruction error,
for any words x,y with~x,~y ∈W and ~xc,~yc ∈C:

SGNS : 〈~x,~yc〉= PMI(x,y)− logk
GloVe : 〈~x,~yc〉= logXx,y−bx−by

(2)

SGNS and GloVe generate two vectors for each
word in the vocabulary: a context vector, for when
it is a context word, and a word vector, for when it
is a target word. Context vectors are generally dis-
carded after training. The SGNS identity in (2) is
from Levy and Goldberg (2014), who proved that
SGNS is implicitly factorizing the shifted word-
context PMI matrix. The GloVe identity is sim-
ply the local objective for a word pair (Pennington
et al., 2014). Since the matrix being factorized in
both models is symmetric, 〈~x,~yc〉= 〈~xc,~y〉.

Definition 4 The co-occurrence shifted PMI of a
word pair (x,y) is PMI(x,y)+ log p(x,y).

Definition 5 Let M denote the word-context ma-
trix that is implicitly factorized by GloVe or SGNS.
If there is no reconstruction error, any four words
{a,b,x,y} are contextually coplanar iff

rank

 Ma,·−My,·Mb,·−My,·
Mx,·−My,·

≤ 2 (3)
For example, for SGNS, the first row of this ma-
trix would be (PMI(a, ·)− logk)− (PMI(y, ·)−
logk) = log[p(·|a)/p(·|y)]. This condition can be
trivially derived from the fact that any four vectors
~a,~b,~x,~y in a d-dimensional space (for d ≥ 3) are
coplanar iff rank(W ∗)≤ 2, where

W ∗ =

 ~aT −~yT~bT −~yT
~xT −~yT

 (4)
Given that the vocabulary size is much greater
than the dimensionality d, and assuming that the



3256

context matrix C is full rank, rank(W ∗CT ) =
rank(W ∗). The product W ∗CT is the matrix in (3);
each of its three rows is the difference between two
rows of M (e.g., Ma,·−My,·). Thus we can translate
coplanarity in the embedding space to the copla-
narity of M’s row vectors.

Co-occurrence Shifted PMI Theorem Let W
be an SGNS or GloVe word embedding space with
no reconstruction error and S be a set of or-
dered word pairs such that ∀ (x,y) ∈ S,~x,~y ∈W
and |S|> 1. A linear analogy f holds over S iff
∃ γ ∈ R, ∀(x,y) ∈ S,csPMI(x,y) = γ and for any
two word pairs (x1,y1),(x2,y2)∈ S, the four words
are contextually coplanar and csPMI(x1,x2) =
csPMI(y1,y2).

In sections 3.2 to 3.4 of this paper, we prove the
csPMI Theorem. In section 3.5, we explain why,
in practice, perfect reconstruction is not needed to
solve word analogies using vector arithmetic. In
section 4, we explore what the csPMI Theorem
implies about vector addition and Euclidean dis-
tance in embedding spaces.

3.2 Analogies as Parallelograms
Lemma 1 A linear analogy f holds over a set
of ordered word pairs S iff ∃ γ ′ ∈ R,∀ (x,y) ∈
S,2〈~x,~y〉−‖~x‖22−‖~y‖22= γ ′ and for any two pairs
(x1,y1),(x2,y2) ∈ S, words x1,x2,y1,y2 are copla-
nar and 2〈~x1,~x2〉 − ‖~x1‖22−‖~x2‖22= 2〈~y1,~y2〉 −
‖~y1‖22−‖~y2‖22 .

f holds over every subset {(x1,y1),(x2,y2)} ⊂
S iff it holds over S. We start by noting that by
Definition 2, f holds over {(x1,y1),(x2,y2)} iff:

~x1 +~r =~y1∧~x2 +~r =~y2 (5)

By rearranging (5), we know that~x2−~y2 =~x1−~y1
and~x2−~x1 =~y2−~y1. Put another way, x1,y1,x2,y2
form a quadrilateral in vector space whose oppo-
site sides are parallel and equal in length. By def-
inition, this quadrilateral is then a parallelogram.
In fact, this is often how word analogies are visu-
alized in the literature (see Figure 1).

To prove the first part of Lemma 1, we let γ ′ =
−‖~r‖22. A quadrilateral is a parallelogram iff each
pair of opposite sides is equal in length. For every
possible subset, ~r = (~y1 − ~x1) = (~y2 − ~x2). This
implies that ∀ (x,y) ∈ S,

γ ′ =−‖~y−~x‖22= 2〈~x,~y〉−‖~x‖22−‖~y‖22 (6)

However, this condition is only necessary and not
sufficient for the parallelogram to hold. The other

man

king queen

woman

royal royal

female

female

Figure 1: The parallelogram structure of the linear
analogy (king,queen)::(man,woman). A linear analogy
transforms the first element in an ordered word pair by
adding a displacement vector to it. Arrows indicate the
directions of the semantic relations.

pair of opposite sides, which do not correspond
to~r, are equal in length iff −‖~x1−~x2‖22= −‖~y1−
~y2‖22 ⇐⇒ 2〈~x1,~x2〉 − ‖~x1‖22−‖~x2‖22= 2〈~y1,~y2〉 −
‖~y1‖22−‖~y2‖22, as stated in Lemma 1. Note that the
sides that do not equal~r do not necessarily have a
fixed length across different subsets of S.

Although points defining a parallelogram are
necessarily coplanar, in higher dimensional em-
bedding spaces, it is possible for ‖~x1 − ~x2‖=
‖~y1 − ~y2‖ and ‖~y1 − ~x1‖= ‖~y2 − ~x2‖ to be satis-
fied without the points necessarily defining a par-
allelogram. Therefore, we must also require that
x1,y1,x2,y2 be coplanar. However, we do not need
the word embeddings themselves to verify copla-
narity; when there is no reconstruction error, we
can express it as a constraint over M, the ma-
trix that is implicitly factorized by the embedding
model (see Definition 5).

3.3 Analogies in the Context Space
Lemma 2 A linear analogy f :~x 7→~x+~r holds
over a set of ordered pairs S in an SGNS or GloVe
word embedding space W with no reconstruction
error iff ∃ λ ∈ R,g : ~xc 7→ ~xc +λ~r holds over S in
the corresponding context space C.

In other words, an analogy f that holds over S in
the word space has a corresponding analogy g that
holds over S in the context space. The displace-
ment vector of g is simply the displacement vector
of f scaled by some λ ∈ R. To prove this, we be-
gin with (5) and any word w in the vocabulary:

~x2−~y2 = ~x1−~y1
⇐⇒ 〈~wc,(~x2−~y2)− (~x1−~y1)〉= 0
⇐⇒ 〈~w,(~x2c−~y2c)− (~x1c−~y1c)〉= 0
⇐⇒ ~x2c−~y2c = ~x1c−~y1c

(7)

Note that we can rewrite the second equation as
the third because the matrices being factorized in



3257

(2) are symmetric and there is no reconstruction
error. We can simplify from the second-last step
because not all word vectors lie in the same hyper-
plane, implying that (~x2c−~y2c)− (~x1c−~y1c) =~0.

Thus a linear analogy with displacement vec-
tor (~y1 − ~x1) holds over S in the word embed-
ding space iff an analogy with displacement vector
(~y1c−~x1c) holds over S in the context space. This
is supported by empirical findings that word and
context spaces perform equally well on word anal-
ogy tasks (Pennington et al., 2014). Since there
is an analogous parallelogram structure formed by
x1,y1,x2,y2 in the context space, there is some
linear map from ~w 7→ ~wc for each word w ∈ S.
The real matrix A describing this linear map is
symmetric: 〈~x,~yc〉=~xT A~y = (AT~x)T~y = 〈~xc,~y〉 for
any (x,y) ∈ S. This implies that C = AW , since
〈~w,~xc〉= 〈~wc,~x〉 for any word w.

Since A is a real symmetric matrix, by the fi-
nite-dimensional spectral theorem, there is an or-
thonormal basis of W consisting of eigenvectors
of A. If A had distinct eigenvalues, then the rela-
tive geometry of the word embeddings would not
be preserved by the transformation, in which case
it would be possible for two words x,y to satisfy
〈~x,~yc〉 6= 〈~xc,~y〉. This would be a contradiction,
given that the factorized word-context matrix is
symmetric. Therefore, the relative geometry is
only preserved when A has non-distinct eigenval-
ues. Because A’s eigenvectors are a basis for W
and all have the same eigenvalue λ , all word vec-
tors lie in the same eigenspace: ∃ λ ∈ R,∀ ~w ∈
W, ~wc = A~w = λ~w. Experiments on embedding
isotropy in past work (Mimno and Thompson,
2017) provide some empirical support of this re-
sult.

3.4 Proof of the csPMI Theorem

From Lemma 1, we know that if a linear analogy
f holds over a set of ordered pairs S, then ∃ γ ′ ∈
R,∀ (x,y) ∈ S,2〈~x,~y〉−‖~x‖22−‖~y‖22= γ ′. Because
there is no reconstruction error, by Lemma 2, we
can rewrite the inner product of two word vectors
in terms of the inner product of a word and context
vector. Using the SGNS identity in (2), we can
then rewrite (6):

γ ′ = 2〈~x,~y〉−‖~x‖22−‖~y‖22
= (1/λ )〈~x−~y,~yc−~xc〉

λγ ′ = 2 PMI(x,y)−PMI(x,x)−PMI(y,y)
= csPMI(x,y)− log p(x|x)p(y|y)

(8)

We get the same equation using the GloVe iden-
tity in (2), since the learned bias terms bx,by can-
cel out. Note that p(x|x) 6= 1 because p(x|x) is the
probability that the word x will appear in the con-
text window when the target word is also x, which
is not guaranteed.

For log p(x|x)p(y|y) to not be undefined, ev-
ery word in S must appear in its own context at
least once in the training corpus. However, de-
pending on the size of the corpus and the con-
text window, this may not necessarily occur. For
this reason, we assume that p(w,w), the probabil-
ity that a word co-occurs with itself, follows the
Zipf distribution of p(w) scaled by some constant
ρ ∈ (0,1). We find this assumption to be justi-
fied, since the Pearson correlation between p(w)
and non-zero p(w,w) is 0.825 for uniformly ran-
domly sampled words in Wikipedia. We can there-
fore treat log p(x|x)p(y|y) ∀ (x,y)∈ S as a constant
α ∈ R−. Rewriting (8), we get

λγ ′+α = csPMI(x,y) (9)

The second identity in Lemma 1 can be expanded
analogously, implying that f holds over a set of
ordered pairs S iff (9) holds for every pair (x,y) ∈
S and csPMI(x1,x2) = csPMI(y1,y2) for any two
pairs (x1,y1),(x2,y2) ∈ S with contextually copla-
nar words. In section 5, we provide empirical sup-
port of this finding by showing that there is a mod-
erately strong correlation (Pearson’s r > 0.50) be-
tween csPMI(x,y) and γ ′, in both normalized and
unnormalized SGNS embedding spaces.

3.5 Robustness to Noise

In practice, linear word analogies hold in embed-
ding spaces even when there is non-zero recon-
struction error. There are three reasons for this:
the definition of vector equality is looser in prac-
tice, the number of word pairs in an analogy set
is small relative to vocabulary size, and analo-
gies mostly hold over frequent word pairs, which
are associated with less variance in reconstruc-
tion error. For one, in practice, an analogy task
(a,?)::(x,y) is solved by finding the most simi-
lar word vector to ~a+ (~y−~x), where dissimilar-
ity is defined in terms of Euclidean or cosine dis-
tance and ~a,~x,~y are excluded as possible answers
(Mikolov et al., 2013b). The correct solution to a
word analogy can be found even when that solu-
tion is not exact. This also means that the solution
does not need to lie exactly on the plane defined



3258

by ~a,~x,~y. Although the csPMI Theorem assumes
no reconstruction error for all word pairs, if we
ignore the coplanarity constraint in Definition 5,
only |S|2+2|S| word pairs need to have no recon-
struction error for f to hold exactly over S. This
number is far smaller than the size of the factor-
ized word-context matrix.

Lastly, in practice, linear word analogies mostly
hold over frequent word pairs, which are asso-
ciated with less variance in reconstruction error.
More specifically, for a word pair (x,y), the vari-
ance of the noise εx,y = Mx,y−〈~x,~yc〉 is a strictly
decreasing function of its frequency Xx,y. This
is because the cost of deviating from the opti-
mal value is higher for more frequent word pairs:
this is implicit in the SGNS objective (Levy and
Goldberg, 2014) and explicit in GloVe objective
(Pennington et al., 2014). We also show that this
holds empirically in section 5. Assuming εx,y ∼
N (0,h(Xx,y)), where δ is the Dirac delta distribu-
tion:

lim
Xx,y→∞

h(Xx,y) = 0 =⇒ lim
Xx,y→∞

N (0,h(Xx,y)) = δ

=⇒ lim
Xx,y→∞

εx,y = 0

(10)

As the frequency increases, the probability that
the noise is close to zero increases. Although word
pairs do not have an infinitely large frequency, as
long as the frequency of each word pair is suffi-
ciently large, the noise will likely be small enough
for a linear analogy to hold over them in practice.
Our experiments in section 5 bear this out: analo-
gies involving countries and their capitals, which
have a median word pair frequency of 3436.5 in
Wikipedia, can be solved with 95.4% accuracy;
analogies involving countries and their currency,
which have a median frequency of just 19, can
only be solved with 9.2% accuracy.

A possible benefit of h mapping lower frequen-
cies to larger variances is that it reduces the prob-
ability that a linear analogy f will hold over rare
word pairs. One way of interpreting this is that
h essentially filters out the word pairs for which
there is insufficient evidence, even if the condi-
tions in the csPMI Theorem are satisfied. This
would explain why reducing the dimensionality
of word vectors – up to a point – actually im-
proves performance on word analogy tasks (Yin
and Shen, 2018). Representations with the optimal
dimensionality have enough noise to preclude spu-
rious analogies that satisfy the csPMI Theorem,

but not so much noise that non-spurious analogies
(e.g., (king,queen)::(man,woman)) are also pre-
cluded.

4 Vector Addition as a Word Analogy

4.1 Formalizing Addition

Corollary 1 Let ~z =~x+~y be the sum of words
x,y in an SGNS word embedding space with no
reconstruction error. If z were a word in the vo-
cabulary, where δ is a model-specific constant,
csPMI(x,z) = log p(y)+δ .

To frame the addition of two words x,y as an
analogy, we need to define a set of ordered pairs
S such that a linear analogy holds over S iff ~x+
~y =~z. To this end, consider the set {(x,z),( /0,y)},
where z is a placeholder for the composition of x
and y and the null word /0 maps to ~0 for a given
embedding space. From Definition 2:

(~x+~r =~z)∧ (~/0+~r =~y)
⇐⇒~z−~x =~y−~/0
⇐⇒~x+~y =~z

(11)

Even though /0 is not in the vocabulary, we can
map it to ~0 because its presence does not affect
any other word vector. To understand why, con-
sider the shifted word-context PMI matrix M that
does not have /0, and the matrix M′ that does, of
which M is a submatrix. Where W and C are
the word and context matrices, WCT = M ⇐⇒
[W ~0][C~0]T = M′. Even if the null word does not
exist for a given corpus, the embeddings we would
get by training on a corpus that did have the null
word would otherwise be identical.

An inner product with the zero vector is always
0, so we can infer from the SGNS identity in (2)
that PMI( /0, ·)− logk = 0 for every word in the vo-
cabulary. The vectors~x,~y,~z,~/0 are all coplanar, and
we know from the csPMI Theorem that if a linear
analogy holds over {(x,z),( /0,y)}, then

PMI(x,z)+ log p(x,z)

=2 PMI( /0,y)+ log p(y)+ log p( /0)

= log p(y)+δ

where δ = logk2 + log p( /0)

(12)

Thus the csPMI of the sum and one word is equal
to the log probability of the other word shifted by
a model-specific constant. If we assume, as in sec-
tion 3.5, that the noise is normally distributed, then



3259

even without the assumption of zero reconstruc-
tion error, the csPMI of the sum and one word is
on average equal to the log probability of the other
word shifted by a constant. We cannot repeat this
derivation with GloVe because it is unclear what
the optimal values of the learned biases would be,
even with perfect reconstruction.

4.2 Automatically Weighting Words
Corollary 2 In an SGNS word embedding space,
on average, the sum of two words has more in com-
mon with the rarer word, where commonality is
measured by csPMI.

For two words x,y, assume without loss of gen-
erality that p(x)> p(y). By (12):

p(x)> p(y) ⇐⇒ log p(x)+δ > log p(y)+δ
⇐⇒ csPMI(z,y)> csPMI(z,x)

(13)

Therefore addition automatically down-weights
the more frequent word. For example, if the vec-
tors for x = ‘the’ and y = ‘apple’ were added
to create a vector for z = ‘the apple’, we would
expect csPMI(‘the apple’, ‘apple’) > csPMI(‘the
apple’, ‘the’); being a stopword, ‘the’ would on
average be heavily down-weighted. While the
rarer word is not always the more informative
one, weighting schemes like inverse document
frequency (IDF) (Robertson, 2004) and unsuper-
vised smoothed inverse frequency (uSIF) (Etha-
yarajh, 2018) are all based on the principle that
more frequent words should be down-weighted
because they are typically less informative. The
fact that addition automatically down-weights the
more frequent word thus provides novel justifica-
tion for using addition to compose words.

4.3 Interpreting Euclidean Distance
Corollary 3 ∃ λ ∈R+,α ∈R− such that for any
two words x and y in an SGNS or GloVe embed-
ding space with no reconstruction error, λ ‖~x−
~y‖22=−csPMI(x,y)+α .

From (9), we know that for some λ ,α,γ ′ ∈ R,
csPMI(x,y) = λγ ′ + α , where γ ′ = −‖~x−~y‖22.
Rearranging this identity, we get

‖~x−~y‖22 =−γ ′

= (−1/λ )(csPMI(x,y)−α)
λ‖~x−~y‖22 =−csPMI(x,y)+α

(14)

Thus the squared Euclidean distance between two
word vectors is simply a linear function of the

negative csPMI. Since csPMI(x,y) ∈ (−∞,0] and
‖~x−~y‖22 is non-negative, λ is positive. This iden-
tity is intuitive: the more similar two words are
(as measured by csPMI), the smaller the distance
between their word embeddings. In section 5, we
provide empirical evidence of this, showing that
there is a moderately strong positive correlation
(Pearson’s r > 0.50) between −csPMI(x,y) and
‖~x−~y‖22, in both normalized and unnormalized
SGNS embedding spaces.

4.4 Are Relations Ratios?
Pennington et al. (2014) conjectured that linear re-
lationships in the embedding space – which we
call displacements – correspond to ratios of the
form p(w|x)/p(w|y), where (x,y) is a pair of
words such that ~y−~x is the displacement and w
is some word in the vocabulary. This claim has
since been repeated in other work (Arora et al.,
2016). For example, according to this conjecture,
the analogy (king,queen)::(man,woman) holds iff
for every word w in the vocabulary

p(w|king)
p(w|queen)

≈ p(w|man)
p(w|woman)

(15)

However, as noted earlier, this idea was neither de-
rived from empirical results nor rigorous theory,
and there has been no work to suggest that it would
hold for models other than GloVe, which was de-
signed around it. We now prove this conjecture for
SGNS using the csPMI Theorem.

Pennington et al. Conjecture Let S be a set of
ordered word pairs (x,y) with vectors in an em-
bedding space. A linear word analogy holds over
S iff ∀ (x1,y1),(x2,y2) ∈ S, p(w|x1)/p(w|y1) ≈
p(w|x2)/p(w|y2) for every word w in the vocab-
ulary.

Assuming there is no reconstruction error, we
replace approximate equality with exact equality
and rewrite the identity for SGNS using (2):

p(w|x1)
p(w|y1)

=
p(w|x2)
p(w|y2)

⇐⇒ PMI(w,x1)−PMI(w,y1) =
PMI(w,x2)−PMI(w,y2)

⇐⇒ 〈~wc,~x1〉−〈~wc,~y1〉= 〈~wc,~x2〉−〈~wc,~y2〉
⇐⇒ 〈~wc,(~x1−~y1)− (~x2−~y2)〉= 0

(16)

The same equation appears in the derivation in (7).
This holds iff ~x1− ~y1 = ~x2− ~y2 (i.e., iff, by Defi-
nition 2, an analogy holds over {(x1,y1),(x2,y2)})



3260

Figure 2: The noise distribution for an SGNS embed-
ding model (i.e., 〈~x,~yc〉− [PMI(x,y)− logk]) at various
frequencies. The noise is normally distributed and the
variance decreases as the frequency increases.

or if ~wc is orthogonal to non-zero (~x1−~y1)−(~x2−
~y2). Even if the context vector of some word is
orthogonal to the difference between the relation
vectors, not all are – as noted in section 3.4, not all
word or context vectors lie in the same hyperplane
in embedding space. Therefore, a linear word
analogy holds over {(x1,y1),(x2,y2)} iff for every
word w, p(w|x1)/p(w|y1) = p(w|x2)/p(w|y2). If
this applies to every (x1,y1),(x2,y2) ∈ S, as stated
in the conjecture, then the same analogy holds
over S.

5 Experiments

Measuring Noise We uniformly sample word
pairs in Wikipedia and estimate the noise (i.e.,
〈~x,~yc〉 − [PMI(x,y)− logk]) using SGNS vectors
trained on the same corpus. As seen in Figure
2, the noise has an approximately zero-centered
Gaussian distribution and the variance of the noise
is lower at higher frequencies, supporting our as-
sumptions in section 3.5. As previously men-
tioned, this is partly why linear word analogies
are robust to noise: in practice, they typically hold
over very frequent word pairs, and at high frequen-
cies, the amount of noise is often negligible.

Estimating csPMI The csPMI Theorem implies
that if an analogy holds exactly over a set of word
pairs when there is no reconstruction error, then
each word pair has the same csPMI value. In Ta-
ble 1, we provide the mean csPMI values for var-
ious analogies in Mikolov et al. (2013a) over the
set of word pairs for which they should hold (e.g.,
{(Paris, France), (Berlin, Germany)} for capital-

Figure 3: The negative csPMI for a word pair against
the squared Euclidean distance between its SGNS word
vectors. There is a positive correlation (Pearson’s r =
0.502); the more similar two words are, the smaller the
Euclidean distance between their vectors. In the nor-
malized SGNS word space, the correlation is just as
strong (Pearson’s r = 0.514).

world). We also provide the accuracy of the vec-
tor arithmetic solutions for each analogy, found by
minimizing cosine distance over the 100K most
frequent words in the vocabulary.

As expected, when the variance in csPMI is
lower, solutions to word analogies are more ac-
curate: the Pearson correlation between accuracy
and csPMI variance is −0.70 and statistically sig-
nificant at the 1% level. This is because an anal-
ogy is more likely to hold over a set of word
pairs when the displacement vectors are identi-
cal, and thus when the csPMI values are identi-
cal. Similar analogies, such as capital-world and
capital-common-countries, also have similar mean
csPMI values – our theory implies this, since sim-
ilar analogies have similar displacement vectors.
As the csPMI changes, the type of analogy gradu-
ally changes from geography (capital-world, city-
in-state) to verb tense (gram5-present-participle,
gram7-past-tense) to adjectives (gram2-opposite,
gram4-superlative). We do not witness a similar
gradation with the mean PMI, implying that analo-
gies correspond uniquely to csPMI but not PMI.

Euclidean Distance Because the sum of two
word vectors is not in the vocabulary, we can-
not calculate co-occurrence statistics involving the
sum, precluding us from testing Corollaries 1 and
2. We test Corollary 3 by uniformly sampling
word pairs and plotting, in Figure 3, the neg-
ative csPMI against the squared Euclidean dis-
tance between the SGNS word vectors. As ex-



3261

Analogy Mean csPMI Mean PMI Median Word Pair Frequency csPMI Variance Accuracy

capital-world −9.294 6.103 980.0 0.496 0.932
capital-common-countries −9.818 4.339 3436.5 0.345 0.954
city-in-state −10.127 4.003 4483.0 2.979 0.744
gram6-nationality-adjective −10.691 3.733 3147.0 1.651 0.918
family −11.163 4.111 1855.0 2.897 0.836
gram8-plural −11.787 4.208 342.5 0.590 0.877
gram5-present-participle −14.530 2.416 334.0 2.969 0.663
gram9-plural-verbs −14.688 2.409 180.0 2.140 0.740
gram7-past-tense −14.840 1.006 444.0 1.022 0.651
gram3-comparative −15.111 1.894 194.5 1.160 0.872
gram2-opposite −15.630 2.897 49.0 3.003 0.554
gram4-superlative −15.632 2.015 100.5 2.693 0.757
currency −15.900 3.025 19.0 4.008 0.092
gram1-adjective-to-adverb −17.497 1.113 46.0 1.991 0.500

Table 1: The mean csPMI for analogies in Mikolov et al. (2013a) over the word pairs for which they should hold
(e.g., (Paris, France) for capital-world). Similar analogies have a similar mean csPMI and arithmetic solutions are
less accurate when the csPMI variance is higher (Pearson’s r = −0.70). The type of analogy gradually changes
with the csPMI, from geography (capital-world) to verb tense (gram7-past-tense) to adjectives (gram2-opposite).

pected, there is a moderately strong positive corre-
lation (Pearson’s r = 0.502): the more similar two
words are (as measured by csPMI), the smaller
the Euclidean distance between them in embed-
ding space. The correlation is just as strong in the
normalized SGNS word space, where Pearson’s r
= 0.514. As mentioned earlier, our assumption in
section 3.4 that p(w,w)∝ p(w) is justified because
there is a strong positive correlation between the
two (Pearson’s r = 0.825).

Unsolvability The csPMI Theorem reveals two
reasons why an analogy may be unsolvable in
a given embedding space: polysemy and corpus
bias. Consider senses {x1, ...,xM} of a polysemous
word x. Assuming perfect reconstruction, a linear
analogy f whose displacement has csPMI γ does
not hold over (x,y) if γ 6= PMI(x,y)+ log p(x,y) =
log [p(x1|y)+ ...+ p(xM|y)] p(y|x). The Theorem
applies over all the senses of x, even if only a par-
ticular sense is relevant to the analogy. For exam-
ple, while (open,closed)::(high,low) makes intu-
itive sense, it is unlikely to hold in practice, given
that all four words are highly polysemous.

Even if (a,b)::(x,y) is intuitive, there is also
no guarantee that csPMI(a,b) ≈ csPMI(x,y) and
csPMI(a,x) ≈ csPMI(b,y) for a given training
corpus. The less frequent a word pair, the more
sensitive its csPMI to even small changes in fre-
quency. Infrequent word pairs are also associated
with more reconstruction error (see section 3.5),
making it even more unlikely that the analogy will
hold in practice. This is why the accuracy for
the currency analogy is so low (see Table 1) – in

Wikipedia, currencies and their country co-occur
with a median frequency of only 19.

6 Conclusion

In this paper, we explained why word analogies
can be solved using vector arithmetic. We proved
that an analogy holds in an SGNS or GloVe em-
bedding space with no reconstruction error iff the
co-occurrence shifted PMI is the same for every
word pair and across any two word pairs, provided
the row vectors of those words in the factorized
word-context matrix are coplanar. This had three
implications. First, we provided a formal proof
of the Pennington et al. (2014) conjecture, the in-
tuitive explanation of this phenomenon. Second,
we provided novel justification for the addition of
SGNS word vectors by showing that it automat-
ically down-weights the more frequent word, as
weighting schemes do ad hoc. Third, we provided
the first rigorous explanation of why the Euclidean
distance between word vectors is a good proxy for
word dissimilarity. Most importantly, we provided
empirical support of our theory and avoided mak-
ing the strong assumptions in past work, making
our theory a much more tenable explanation.

Acknowledgments

We thank Omer Levy, Yoav Goldberg, and the
anonymous reviewers for their insightful com-
ments. We thank the Natural Sciences and Engi-
neering Research Council of Canada (NSERC) for
their financial support.



3262

References
Carl Allen and Timothy Hospedales. 2019. Analo-

gies explained: Towards understanding word em-
beddings. arXiv preprint arXiv:1901.09813.

Sanjeev Arora, Yuanzhi Li, Yingyu Liang, Tengyu Ma,
and Andrej Risteski. 2016. A latent variable model
approach to PMI-based word embeddings. Transac-
tions of the Association for Computational Linguis-
tics, 4:385–399.

Sanjeev Arora, Yingyu Liang, and Tengyu Ma. 2017.
A simple but tough-to-beat baseline for sentence em-
beddings. In International Conference on Learning
Representations.

Yoshua Bengio, Réjean Ducharme, Pascal Vincent, and
Christian Jauvin. 2003. A neural probabilistic lan-
guage model. Journal of Machine Learning Re-
search, 3(Feb):1137–1155.

Kenneth Ward Church and Patrick Hanks. 1990. Word
association norms, mutual information, and lexicog-
raphy. Computational Linguistics, 16(1):22–29.

Ronan Collobert and Jason Weston. 2008. A unified
architecture for natural language processing: Deep
neural networks with multitask learning. In Pro-
ceedings of the 25th International Conference on
Machine Learning, pages 160–167. ACM.

Kawin Ethayarajh. 2018. Unsupervised random walk
sentence embeddings: A strong but simple baseline.
In Proceedings of The Third Workshop on Represen-
tation Learning for NLP, pages 91–100.

John R Firth. 1957. A synopsis of linguistic theory,
1930-1955. Studies in linguistic analysis.

Alex Gittens, Dimitris Achlioptas, and Michael W Ma-
honey. 2017. Skip-gram – Zipf + uniform = vector
additivity. In Proceedings of the 55th Annual Meet-
ing of the Association for Computational Linguistics
(Volume 1: Long Papers), volume 1, pages 69–76.

Thomas K Landauer and Susan T Dumais. 1997. A
solution to Plato’s problem: The latent semantic
analysis theory of acquisition, induction, and rep-
resentation of knowledge. Psychological review,
104(2):211.

Omer Levy and Yoav Goldberg. 2014. Neural word
embedding as implicit matrix factorization. In Ad-
vances in Neural Information Processing Systems,
pages 2177–2185.

Tomas Mikolov, Kai Chen, Greg S Corrado, and
Jeff Dean. 2013a. Efficient estimation of word
representations in vector space. arXiv preprint
arXiv:1301.3781.

Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Cor-
rado, and Jeff Dean. 2013b. Distributed representa-
tions of words and phrases and their compositional-
ity. In Advances in Neural Information Processing
Systems, pages 3111–3119.

David Mimno and Laure Thompson. 2017. The strange
geometry of skip-gram with negative sampling. In
Proceedings of the 2017 Conference on Empirical
Methods in Natural Language Processing, pages
2873–2878.

Jeffrey Pennington, Richard Socher, and Christopher
Manning. 2014. GloVe: Global vectors for word
representation. In Proceedings of the 2014 Con-
ference on Empirical Methods in Natural Language
Processing (EMNLP), pages 1532–1543.

Steven T Piantadosi. 2014. Zipf’s word frequency
law in natural language: A critical review and fu-
ture directions. Psychonomic Bulletin & Review,
21(5):1112–1130.

Stephen Robertson. 2004. Understanding inverse doc-
ument frequency: on theoretical arguments for IDF.
Journal of Documentation, 60(5):503–520.

Douglas LT Rohde, Laura M Gonnerman, and David C
Plaut. 2006. An improved model of semantic simi-
larity based on lexical co-occurrence. Communica-
tions of the ACM, 8(627-633):116.

Zi Yin and Yuanyuan Shen. 2018. On the dimension-
ality of word embedding. In Advances in Neural In-
formation Processing Systems, pages 894–905.


