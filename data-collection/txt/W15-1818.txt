






















Sentence Compression For Automatic Subtitling

Juhani Luotolahti and Filip Ginter
Department of Information Technology

University of Turku, Finland
mjluot,figint@utu.fi

Abstract

This paper investigates sentence compres-
sion for automatic subtitle generation us-
ing supervised machine learning. We
present a method for sentence compres-
sion as well as discuss generation of train-
ing data from compressed Finnish sen-
tences, and different approaches to the
problem. The method we present outper-
forms state-of-the-art baseline in both au-
tomatic and human evaluation. On real
data, 44.9% of the sentences produced
by the compression algorithm have been
judged to be useable as-is or after minor
edits.

1 Introduction

Automated broadcast programme subtitling is an
important task, especially with the recent intro-
duction of legislation which mandates all pro-
grammes of the Finnish national broadcasting cor-
poration to be subtitled, even in cases where
the programme is in Finnish, and not in a for-
eign language. Providing such a subtitling is a
resource-intensive task, requiring human editors
to manually transcribe the programme and pro-
duce the subtitles. There is an obvious motiva-
tion to automate this process to increase the subti-
tling throughput and drive the costs down. While
ultimately aiming at a fully automated pipeline
from speech recognition to screen-ready subtitles,
in this paper we focus specifically on the task of
text compression for subtitling.

The need for this task arises from the fact that
the whole spoken content of the programme can-
not be displayed in the area of the screen devoted
to subtitles while respecting the standards setting
the maximum number of characters per line and
the minimum amount of time the subtitles must
be shown. The subtitling naturally also needs to

remain in time synchronisation with the spoken
programme. In practice, the subtitling editors thus
need to compress the text of the subtitles, remov-
ing or abridging parts which are less critical for
the understandability of the programme.

2 Sentence Compression

The goal of automatic sentence compression is to
create a shorter version of the input sentence, in
a way preserving its meaning. Sentence compres-
sion is most often extractive, formed by dropping
words from a sentence that are not needed for the
sentence to be grammatical and do not importantly
contribute to the meaning of the sentence. Many
sentence compression methods are based on su-
pervised learning using parallel corpora as train-
ing material (Knight and Marcu, 2002; Turner and
Charniak, 2005; McDonald, 2006; Cohn and La-
pata, 2009). Some methods don’t require parallel
corpora, but are either based on rules (Gagnon and
Da Sylva, 2005) or use language models or statis-
tics gathered from non-parallel sources (Chiori
and Furui, 2004; Filippova and Strube, 2008;
Clarke and Lapata, 2006). While some systems
prune the sentence based on the linear order of
the words, others prune the parse trees or modi-
fied parse trees. Language models are commonly
used to ensure grammatical output.

3 Data and its pre-processing

We draw our data from subtitles of the Finnish
national broadcasting corporation television pro-
grams provided to us by Lingsoft Inc. From Ling-
soft, we have obtained the texts both before and af-
ter the compression step of the subtitling process,
extracted from the internal processing pipeline. As
illustrated in Figure 1, each programme consists of
the subtitle texts and the associated time-stamps
which define the time period in which the subti-
tle is shown on the screen. The full, unabridged

Proceedings of the 20th Nordic Conference of Computational Linguistics (NODALIDA 2015) 135



13
10:01:31,12 --> 10:01:35,24 
Jaro has returned to Finland after a
victorious racing trip. 

14
10:01:36,01 --> 10:01:41,01
Champion Toni Gardemaister
awaits him with a surprise. 

15
10:01:42,00 --> 10:01:44,15 
Oh, hello. 
-Hi there 

16
10:01:44,17 --> 10:01:49,21
Let's go to the kart racing track
and let's see how good you are. 

11 
00:01:48,000 --> 00:01:51,600 
Jaro has returned to Finland after a
victorious racing trip to Estonia.
 
12 
00:01:52,400 --> 00:01:56,000 
Champion Toni Gardemaister awaits
him with a surprise. 

13 
00:01:57,700 --> 00:02:00,000 
Oh, hello hello. Hi there. What's up? Nothing
much. 

15 
00:02:00,600 --> 00:02:05,100
Let's, you know, go cruising a little to the kart
racing track. And let's see how good you are at
kart racing.

OriginalSubtitle

Figure 1: Example document excerpt from the data, translated to English.

Pertti hei , mä käyn näyttämäs näitä dioja yhelle asiantuntijalle .
Pertti hey , I will_go show these slides one to_expert .
* * * Mä käyn näyttämässä näitä dioja * asiantuntijalle .
- - - I will_go show these slides - to_expert .
D D D + - = - - D - -

* Mites tää puhelin , onks tää toiminu ?
- How_about this phone , has it worked ?
Onko * tää puhelin * * * toiminu ?
Has - this phone - - - worked ?
I D - + D D D - -

Figure 2: Example alignments

version of the programme is used for internal pur-
poses of the company and is the result of manual
correction of speech recognition output. The com-
pressed version of the programme consists of com-
pleted subtitles, exactly as delivered and subse-
quently aired. The subtitles often include spoken
language, with incomplete words and slang terms,
making them different in style from the strictly
grammatical text which would be ideal.

The first step in pre-processing the data is to ob-
tain a good alignment of the texts so that the indi-
vidual edits can be identified. The data was re-
ceived as raw subtitle files. A subtitle file consists
of text units to be shown on a screen at a partic-
ular moment and the time to show it. Because
the unabridged version was a result of speech
recognition, its timing didn’t correspond with the
abridged version’s timing. The subtitles and the
amount of sentences they include are also differ-

ent in size, because in many cases whole sentences
were removed or introduced in the abridging pro-
cess.

To identify the edits made to the subtitles, es-
pecially tokens being removed, it was necessary
to obtain token to token level alignments between
the two versions of the subtitles. String alignment
was created using a distance matrix generated by
calculating Levenshtein distance between the two
subtitles on a token level. The edits were ex-
tracted from the distance matrix the method gen-
erates. Tokens with only minimal modifications
(eg. ’ohjelma’, ’program’ and ’ohjelmamme’,
’our program’) were aligned instead of counting
as a substitution. Minimal modification of tokens
was defined as being sufficiently close to each
other, when calculated with a string matching al-
gorithm.

Because of the edits made in the abridging pro-

Proceedings of the 20th Nordic Conference of Computational Linguistics (NODALIDA 2015) 136



Well
No

yes
kyllä

me_too
minäkin

happily
mielelläni

learn_would
opettelisin

yes
kyllä

Saami
saamen

language
kieltä

,
,
but
mut

it
se

may
voi

be
olla

quite
aika

hard
hankalaa

this
näin

old
vanhempana

.

.

advmod> <advmod advmod> <poss <aux <advmod <advmod
<nsubj <cop nommod>dobj>

<intj punct> <nsubj-cop
cc>

conj>
punct>

Figure 3: An example of the extended SD scheme.

cess, sometimes sentences were combined and
sometimes cut in the abridged version. After we
had the alignments the original subtitles with the
abridged ones the subtitles were sentence split and
the sentences were aligned with sentences of the
abridged text.

Each sentence was parsed using the recently
published Finnish statistical dependency parsing
pipeline (Haverinen et al., 2014). The parser pro-
duces full dependency parse trees in a minor modi-
fication of the well-known Stanford Dependencies
(SD) scheme (de Marneffe and Manning, 2008).

4 Methods

Since we have the appropriate training data at our
disposal, we will approach the task as a super-
vised machine learning problem, whereby a classi-
fier will predict which words will be removed from
each sentence. We will test a few different ap-
proaches to the task, all based on supervised learn-
ing, but using different sentence reduction strate-
gies. The feature set consists mainly of features
based on the dependent token and features derived
from the dependency tree, which we will describe
later.

The first approach is to prune the dependency
trees of the sentences. The goal of this approach is
to produce more syntactically valid output than re-
ducing the sentence based on its linear order. This,
however, does not guarantee the syntactic validity
of the sentence. Consider, for instance the subtree
headed by hankalaa (hard) in Figure 3. Remov-
ing the word, together with its subtree will leave
the conjunction mut (but) orphaned, resulting in
an ungrammatical sentence. We will address the
most common such cases with a set of straightfor-
ward post-processing rules. For dependency tree
pruning we have the following strategies:

The first strategy is to let the classifier decide
which dependencies (edges) to prune from the de-
pendency tree and remove the complete subtree
along with the removed dependency. For this we
train an SVM classifier to recognize the dependen-

cies to remove and clean the training data from
dependencies which would be removed by a re-
moval of a dependency higher up in the tree. The
SVM implementation used is libSVM (Chang and
Lin, 2011). In the results table, we refer to this
approach as mp svm.

Another strategy is to remove a dependency
only if all of the dependencies under it in the de-
pendency tree have been removed as well. For this
an SVM classifier is trained which for each de-
pendency makes a prediction on whether to keep
or remove it. Unlike previously, this time the train-
ing data contains all dependencies. The motivation
behind this strategy is to conserve important sub-
trees of the tree. In the results table, we refer to
this approach as mk svm.

The second approach for compressing the sen-
tence is to let a classifier freely remove tokens
from the sentence without limitations of the de-
pendency tree structure. To gain advantage from
the linear order of the sentence we cast the prob-
lem as a sequence classification problem and
use Conditional Random Fields (CRF), as imple-
mented in CRFSuite (Okazaki, 2007) with the
same feature set as previously to predict which to-
kens to keep and which to drop. We refer to the
CRF model as crf.

We also train another CRF model (referred to
as crf pos) without the dependency tree features to
see how well it fares against the full feature set and
to judge the extent to which syntax contributes to
the classification.

We will also implement a baseline system based
on the work of Filippova and Strube (2008), which
is based on finding an optimal abridged tree us-
ing Integer Linear Programming and unsupervised
learning. This will be referred as base ilp in the
evaluation tables.

The last system we build is a modification of the
mentioned baseline system, described in detail in
a later chapter. In this system we replace the statis-
tical scores used by the baseline with those given
by the crf model, such that basically the probabil-

Proceedings of the 20th Nordic Conference of Computational Linguistics (NODALIDA 2015) 137



ity of token being removed is decided by the CRF
classifier and is used by the ILP process to make
the final decisions. The appeal of this strategy is
the use of ILP, and a direct comparison with the
baseline since it uses a qualitatively different re-
moval scheme than our systems. This is referred
as crf ilp in the evaluation.

All of these systems allow their rate of removal
to be fixed by altering the classifier threshold and
in the case of the Integer Linear Programming
based method by setting a fixed rate of removal.
The score used to adjust the rate of removal for
SVM is simply the classifier score and for CRF,
the marginal probabilities for token removal. It is
to be noted that for the CRF based methods alter-
ing the threshold is very important, since without
threshold manipulation the classifier produces too
low compression rates (∼5% of tokens removed in
the dev-set with full feature set).

As with any similar supervised machine learn-
ing method, feature engineering is an important
part of the development. The first class of features
we use is derived from the morphological analysis
of the target word. The second class of features
is based on the surrounding structure of the parse
tree. These features model the syntactic context
of the target word to capture its immediate syn-
tactic neighbourhood. We also add combination
features where appropriate, since the underlying
classifier is linear. The third class of features in-
cludes those that encode information about the tar-
get word’s position in the sentence and within the
tree. We also employ features from the Seman-
tic Role Labeling system of Kanerva et al. (2014).
We list the exact features used below, and in Sec-
tion 5 we will present a feature ablation study to
demonstrate their relative merits.

Features

The exhaustive list of features is described in the
following:

Features based on the token

• Morphological tags
• The morphological tags of the next and the

previous token in the sentence
• Word and lemma of the token
• Next and previous word and lemma of the to-

ken
• Next and previous pos-tags of the token

Tree structure based Features
• Dependency type
• The dependency types of the next and the pre-

vious token in the sentence
• The types of dependencies that have the de-

pendent token as a governor and also this fea-
ture combined with the dependency type of
the dependent token
• Dependency type combined with the depen-

dent token’s morphological tags
• The dependency types of the tokens which

are siblings of this token in the dependency
tree with information about whether the to-
kens are to the left or to the right of the de-
pendent token in the sentence. This is also
combined with the dependency type of the to-
ken.
• The morphological tags of the governing to-

ken
• The dependency type of the governing token

with and without the dependent token’s de-
pendency type combined
• How many tokens depend on the current to-

ken
• How large a subtree would be removed if this

dependency was pruned
• Whether this token is a leaf in the tree
• Whether this token has incoming semantic

edges

Location and sentence based Features
• Whether sentence length is over 5, 10, or 15

tokens with and without the dependency type
• How long a path in the dependency tree is to

the root node from this node
• Whether the number of dependencies above

this dependency in the path to the root node
is in the first, second, third or fourth quarter
of the longest path to the root node in the tree
• Whether the dependent token is located in the

first, second, third or fourth quarter of the
sentence
• The two above features combined into one

5 Evaluation

Problem setting
The data is divided into training, development and
test sets, with the division carried out by sampling
sentences randomly. The development and test
sets were both 3247 sentences long (roughly 18%

Proceedings of the 20th Nordic Conference of Computational Linguistics (NODALIDA 2015) 138



of all sentences). In the training data, only sen-
tences with removals and no other edits such as
substitutions are used, to ensure the classifier does
not learn to remove tokens that are in fact sub-
stituted for another token, possibly at some later
point in the sentence. The development and test
sets are, however, preserved exactly as-is, contain-
ing all sentences from the original data. The com-
pression experiments are done with compression
rate of 85% (i.e. 15% token removal) to be in line
with the test data and also 70% to see how the sys-
tems fare with a higher rate of removal.

Baseline

As the baseline, we have re-implemented the
method of Filippova and Strube (2008). Like our
method, the baseline is based on the removal of de-
pendency sub-trees, however, it is an unsupervised
method, requiring only a dependency treebank for
its training. This will allow us to study to what ex-
tent the availability of supervised data affects the
overall performance on the compression task, as
compared to an unsupervised baseline previously
shown to have a good performance and based on
the same principle of dependency subtree removal.

The baseline method has three steps: transform-
ing the source tree by adding additional edges,
compression, and tree linearisation. The method
assigns a score for each edge of a dependency tree
and tries to find optimal edges to remove, max-
imizing the score of the remaining edges and at
the same time maintaining a correct tree structure.
The edge scores are calculated from statistics de-
rived from a treebank. The method uses integer
linear programming to find a globally optimal so-
lution.

In our experiment, statistics of the dependen-
cies and tokens are calculated from an approxi-
mately 500 million token corpus obtained by ex-
tracting Finnish text from the CommonCrawl1 In-
ternet crawl data and automatically parsed using
the Finnish dependency parsing pipeline of Haver-
inen et al. (2014).

The trees are first pre-processed by creating a
dummy root node and adding an edge from the
dummy root to each finite verb in the sentence,
making the trees graphs. Then, auxiliary verbs,
negation markers and function words are merged
with their governors to ensure they are not re-
moved separately. And finally, coordination struc-

1http://www.commoncrawl.org

tures are decomposed by propagating the governor
of the first coordinated element to every other el-
ement in the coordination, preserving the depen-
dency type.

Tree compression is then cast as an optimization
problem and solved using integer linear program-
ming. Each dependency from a governor word g
to a dependent d with type l is assigned a binary
variable:

xlg,d =

{
1 if edge preserved;
0 if edge not preserved.

The method then optimizes the objective func-
tion

f(X) =
∑
x

xlg,d · P (l|g) · I(d) (1)

where P (l|h) is the conditional probability of
the dependency type l, given that g is the governor.
I(d) is an importance score defined as:

I(di) =
l

N
fi · log(

Fa
Fi

) (2)

where l is the number of clause nodes above the
dependency, N is the maximum level of embed-
ding, fi is the frequency of the word in current
document, Fa is the sum of frequencies of topic
words in the corpus and Fi is the frequency of the
word in corpus.

Constraints are added into the integer linear pro-
gramming problem in order to ensure that a correct
structure is produced, making sure that each word
has a governor. The maximal number of tokens in
the resulting tree is also encoded as a constraint to
the problem, allowing the control of the compres-
sion ratio. The pruned tree is then linearized in the
original order of words in the sentence.

Test Set Evaluation
First, we compare the performance of our pro-
posed methods and the baseline in terms of F-
score on the test set. Precision is defined as
the proportion of predicted removed tokens which
were also removed in gold standard, and recall is
conversely defined as the proportion of removed
tokens in the gold standard, whose removal is also
predicted by the system. The main results in Ta-
ble 1 show that with essentially equal compres-
sion ratios, the feature-rich CRF (referred to as
crf ) results in the highest F-score in both rates of

Proceedings of the 20th Nordic Conference of Computational Linguistics (NODALIDA 2015) 139



F CR(tkn) CR(chr)
gold 1.0 0.863 0.884
crf ilp85 0.2346 0.847 0.844
base ilp85 0.1983 0.845 0.819
crf85 0.3809 0.850 0.879
crf pos85 0.3511 0.850 0.884
mk svm85 0.3613 0.849 0.883
mp svm85 0.3258 0.849 0.872

crf ilp70 0.2611 0.715 0.712
base ilp70 0.2504 0.714 0.683
crf70 0.3758 0.700 0.761
crf pos70 0.3527 0.700 0.777
mk svm70 0.3640 0.700 0.759
mp svm70 0.3408 0.699 0.741

Table 1: F1-Scores for the test set. CR(tkn) is to-
ken level rate of compression and CR(chr) is char-
acter level compression rate of the system output.

removal, followed by the SVM classifier (which
makes independent predictions, unlike the CRF
sequence classifier). The baseline performs sub-
stantially worse. As a further insight into the
methods, we present in Table 3 the ten most of-
ten removed dependency types for the best scor-
ing crf model, the baseline, and in the test set.
As expected, with few exceptions we see depen-
dency types associated with modifiers and func-
tional words rather than core semantic arguments.
Most of these types will also tend to have a single,
leaf dependent. We can also observe a rather wide
overlap (7/10) of the commonly removed depen-
dency types between the two methods, showing
that the systems target similar points of compres-
sion.

Further, we perform a small-scale feature abla-
tion study with a CRF classifier. The most impor-
tant feature groups are those related to the token it-
self, such as its POS-tag and lemma. The features
gathered from the syntactic trees, and related loca-
tion group of features both contribute positively to
the classification, even though the contribution is
not major.

The F-score measure can be evaluated on as
many runs as necessary, for instance in param-
eter selection, but it does not necessarily reflect
the ability of the system to produce fluent and
meaning-preserving compressions. The underly-

Feature Set Dev-set@85% F-score
Token 34.10
Token+location 35.96
Token+tree structure 36.31
All 37.16

Table 2: CRF feature ablation table on develop-
ment set with 85% compression rate.

Gold Base crf
advmod 26.0% advmod 18.9% advmod 36.8%
punct 17.9% nommod 13.0% punct 10.8%
nommod 7.3% punct 9.0% det 8.4%
det 5.6% amod 6.7% intj 6.1%
nsubj 5.4% dobj 5.8% cc 5.4%
intj 4.4% det 5.5% nommod 4.6%
cc 3.9% poss 5.3% nsubj 3.7%
amod 3.1% cc 4.0% complm 3.3%
conj 2.7% conj 3.5% amod 2.8%
dobj 2.6% cop 3.4% conj 2.3%

Table 3: Dependency types pruned in the test set

ing problem is that any given sentence can have
a number of valid compressions, but only one of
them will be counted as a true positive, all others
will be counted as false positives. To address these
issues, we perform also a manual evaluation of the
result, discussed in the following section.

Manual evaluation

In this evaluation, we focus on the ability of the
systems to produce fluent output and preserve im-
portant, content-bearing parts of the sentence (i.e.
its “main message”). These two qualities are to
some degree independent (although clearly not en-
tirely so) and we thus evaluate them separately.

We selected 399 random sentences which had
been compressed by the systems from the test sec-
tion of the corpus, and for each sentence we pro-
duced four compressed versions: one using the
baseline method, one using the crf model which
got the highest F-score on the test set. In addition
we test crf pos and crf ilp. The crf pos set is se-
lected because of its relatively high F-score on the
test set, even though it does not employ the syn-
tax of the sentence. The crf ilp is selected to test
both the integer linear programming method and
to provide the baseline with a comparison using
the same approach to sentence reduction. For the
test, compression rates were aligned. In the end
all systems had a rate of token removal of 75% for
the sentences being tested.

The compressed versions of the sentences were
subsequently evaluated by a native speaker in

Proceedings of the 20th Nordic Conference of Computational Linguistics (NODALIDA 2015) 140



Fluency
3 Readable as is
2 Minor revisions needed
1 Major revisions needed
0 Incomprehensible

Meaning
3 Message preserved perfectly
2 Important message preserved
1 Minor revisions needed
0 Important message lost

Table 4: Manual evaluation scales.

Fluency Meaning
crf 799 (66.7%) 609 (50.8%)
crf pos 796 (66.5%) 579 (49.9%)
crf ilp 795 (66.4%) 487 (40.7%)
Baseline 720 (60.2%) 383 (32.0%)

Table 5: Sum of the scores over all test sentences
given by the evaluator. The percentages are given
in terms of maximum possible value, which for all
quantities is 399 sentences × maximum score of
3, i.e. 1197.

terms of fluency and in terms of content preser-
vation using the scales shown in Table 4. The or-
der in which the compressed versions were pre-
sented for evaluation was randomly changed for
every sentence separately, i.e. it was not possible
to distinguish the methods by the evaluator. Fur-
ther, the evaluator was not involved in the develop-
ment of the methods in any manner and was thus
not primed to recognize features typical of com-
pressions produced by any of these methods.

To gain an initial insight into the evaluation re-
sults, we show in Table 5 the sum of scores given
across all sentences. We can see that the crf gains
on top of the baseline in terms of both measures:
6.5pp (percent points) in terms of fluency and
18.8pp in terms of meaning. These correspond to
16.3% and 27.6% of relative error decrease over
the baseline. Both differences are statistically sig-
nificant with p < 0.02 (two-tailed paired t-test).

For practical deployment, the proportion of sen-
tences which need no, or only minor corrections is
a crucial measure. For the best performing CRF
method, 75.4% and 44.9% (fluency and meaning)
were assigned score of 2 or 3, i.e. usable as-is
or with only minor corrections. For the baseline
method, the corresponding proportions are 68.2%

and 15.3%, reflecting a notable gain of the pro-
posed method over the baseline.

When both fluency and meaning had to be as-
signed score of 2 or 3, 44.9% of the sentences pro-
duced by the crf method required only minor mod-
ifications for fluency or were readily usable. For
the baseline method only 15.0% of the sentences
were rated as readily usable or requiring only mi-
nor modifications for fluency. The 29.9pp gain of
the proposed method corresponds to a 35.1% rel-
ative decrease in error, and ultimately manual ef-
fort saved by the proposed method. 74.2% of the
crf produced sentences are usable as-is or require
minor fixing in terms of fluency and/or meaning,
when using a more relaxed criteria and requiring
fluency to be scored 2 or 3 and meaning to be 1
(Minor revisions needed for maintaining meaning)
or greater, while baseline produces such sentences
63.9% of the time. This difference of 10.3pp cor-
responds to a relative decrease in error rate of
28.5%. The difference of performance between
crf and crf pos when it comes to fluency or mean-
ing is not statistically significant, although crf is
rated higher on both measures. Human evalua-
tion would suggest the crf pos performs slightly
worse in maintaining the meaning of the sentence
(0.9pp) than crf, while the difference in fluency is
very small (0.2pp). This would suggest the syn-
tax of the sentence might help the system deciding
which tokens are important for the meaning of the
sentence.

The difference in fluency between crf and crf
ilp is not statistically significant, but difference
between meaning is statistically significant (p <
0.02 on two-tailed paired t-test). Because this sys-
tem is identical in all respects but the scores used
to prune the tree, to the baseline of which differ-
ence of fluency is statistically significant to the crf,
this shows the CRF based scores do help with the
fluency of the output. The comparison between crf
ilp and the baseline is interesting, because they are
essentially the same system except one is based
on supervised learning and the other is based on
statistics. The crf ilp outperforms the baseline on
both metrics and the results are statistically sig-
nificant. This speaks in favour of the supervised
approach.

Human agreement

Earlier in the development process, we also per-
formed another human evaluation to test both the

Proceedings of the 20th Nordic Conference of Computational Linguistics (NODALIDA 2015) 141



process of human evaluation and the performance
of the system. We were especially interested in the
subjectivity of the task and the human agreement.
While in this earlier experiment the test data, sys-
tem and rate of removal are different from the
above final test setting, it still offers insight into
the evaluation process and especially the level of
agreement of the human evaluators. For this hu-
man evaluation round, two evaluators were used,
and rate of token compression for both classifiers
was set to 85%. The participants of this evaluation
are mp svm against the ILP-based baseline. 200
sentences were selected and all judged indepen-
dently by both evaluators.

The Kappa score of the inter-annotator agree-
ment over all 800 annotation data points (2 tasks
× 2 methods× 200 sentences) is 0.32. When mea-
sured per method and task, Kappa varies from 0.25
(baseline method, meaning task) to 0.36 (proposed
method, meaning task). For the specific binary de-
cision of whether the score of an individual sen-
tence is ≥ 2 or not, the overall Kappa score is
0.39. The overall scores of 0.32 and 0.39 would
be interpreted as “fair” using the criteria of Viera
et al. (2005).

6 Discussion and conclusions

Comparison of the F-score evaluation between the
supervised method and the unsupervised baseline
shows that the supervised training gives a rather
substantial benefit over the unsupervised base-
line. Numerically, the F-scores remained very low,
which, however, can be largely attributed to the
rather arbitrary nature of the sentence compression
task where any sentence of a reasonable length
may have a number of alternative compressions.
Of these, one was selected when producing the
sub-titles and the alternatives count as errors in
the F-score based evaluation. This cannot be ad-
dressed without a major data curation effort which,
in our practical setting, is not possible.

The manual evaluation not only shows consider-
ably more promising results in the numeric sense,
but also shows correlation with the F-score based
evaluation. This suggests that it is possible to use
the F-score evaluation to develop and optimize the
method, while a manual evaluation is clearly nec-
essary to gain insight into the practical usability of
the output compressions.

Interestingly, we find a clearly better perfor-
mance of the CRF-based method also in terms of

fluency, even though the baseline method uses lin-
ear programming to find a globally optimal solu-
tion and the statistics it relies on were gathered on
a parsed corpus of a substantial size.

From a practical point of view, the manual eval-
uation shows that about one half of the com-
pressed sentences are acceptable as-is or nearly
as-is. If deployed in a setting where the neces-
sary minor edits are technically easily carried out,
it would seem feasible that the sentence compres-
sion would lead to a streamlining of the subtitling
process and subsequent cost reduction.

The lack of training data is an often cited prob-
lem for sentence compression. We have shown
that subtitling data is a good source for sentence
compression method development, even though
non-trivial pre-processing is necessary to align the
textual corpora and produce suitable training data.
With the increasing pressure on the availability of
subtitling and textual transcriptions, this task rep-
resents an important use case for text compression
and increases the chance that such data can be-
come available through industry collaboration.

There are many future work opportunities. The
method currently does not take into account con-
text beyond a single sentence. We thus lose the op-
portunity to model whether a particular sentence
element has been discussed in the preceding sen-
tence and may be pruned with a higher likelihood.
There is also room for improvement in ensuring
the grammaticality of the output. Others have used
for instance language models to improve the gram-
maticality and fluency of the output. Modelling
subcategorization frames could also be applied for
this purpose.

Studying the data, we have noticed that often
long words are replaced with their shorter syn-
onyms to compress the sentence without any loss
of information. Finding shorter synonyms and
learning to substitute words and phrases would be
very helpful for the sentence compression task,
possibly applying the recent advancements in vec-
tor space representations and modelling phrase
compositionality.

Acknowledgments

This work was supported by the Kone Foundation.
Computational resources were provided by CSC
– IT Center for Science. We thank Simo Vihja-
nen from Lingsoft Inc. for the data and the overall
problem setting.

Proceedings of the 20th Nordic Conference of Computational Linguistics (NODALIDA 2015) 142



References
Chih-Chung Chang and Chih-Jen Lin. 2011. LIB-

SVM: A library for support vector machines. ACM
Transactions on Intelligent Systems and Technol-
ogy, 2:27:1–27:27. Software available at http://
www.csie.ntu.edu.tw/˜cjlin/libsvm.

HORI Chiori and Sadaoki Furui. 2004. Speech sum-
marization: An approach through word extraction
and a method for evaluation. IEICE TRANSAC-
TIONS on Information and Systems, 87(1):15–25.

James Clarke and Mirella Lapata. 2006. Constraint-
based sentence compression: An integer program-
ming approach. In Proceedings of the COL-
ING/ACL 2006 Main Conference Poster Sessions,
pages 144–151.

Trevor Cohn and Mirella Lapata. 2009. Sentence com-
pression as tree transduction. Journal of Artificial
Intelligence Research, 34:637–674.

Marie-Catherine de Marneffe and Christopher D. Man-
ning. 2008. The Stanford typed dependencies rep-
resentation. In Coling 2008: Proceedings of the
workshop on Cross-Framework and Cross-Domain
Parser Evaluation, pages 1–8.

Katja Filippova and Michael Strube. 2008. Depen-
dency tree based sentence compression. In Proceed-
ings of the Fifth International Natural Language
Generation Conference, pages 25–32.

Michel Gagnon and Lyne Da Sylva. 2005. Text
summarization by sentence extraction and syntactic
pruning. In Proceedings of Computational Linguis-
tics in the North East (CliNE05).

Katri Haverinen, Jenna Nyblom, Timo Viljanen,
Veronika Laippala, Samuel Kohonen, Anna Missilä,
Stina Ojala, Tapio Salakoski, and Filip Ginter. 2014.
Building the essential resources for Finnish: The
Turku Dependency Treebank. Language Resources
and Evaluation, 48(3):493–531.

Jenna Kanerva, Juhani Luotolahti, and Filip Ginter.
2014. Turku: Broad-coverage semantic parsing with
rich features. In Proceedings of the 8th Interna-
tional Workshop on Semantic Evaluation (SemEval
2014), pages 678–682.

Kevin Knight and Daniel Marcu. 2002. Summariza-
tion beyond sentence extraction: A probabilistic ap-
proach to sentence compression. Artificial Intelli-
gence, 139(1):91–107, July.

Ryan T McDonald. 2006. Discriminative sentence
compression with soft syntactic evidence. In Pro-
ceedings of the 11th conference of EACL, pages
297–304.

Naoaki Okazaki. 2007. CRFsuite: A fast implementa-
tion of Conditional Random Fields (CRFs).

Jenine Turner and Eugene Charniak. 2005. Supervised
and unsupervised learning for sentence compres-
sion. In Proceedings of the 43rd Annual Meeting
on Association for Computational Linguistics(ACL),
pages 290–297.

Anthony Viera and Joanne Garrett. 2005. Understand-
ing interobserver agreement: The Kappa statistic.
Family Medicine, 37(5):360–363.

Proceedings of the 20th Nordic Conference of Computational Linguistics (NODALIDA 2015) 143


