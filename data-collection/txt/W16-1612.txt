



















































Mapping Unseen Words to Task-Trained Embedding Spaces


Proceedings of the 1st Workshop on Representation Learning for NLP, pages 100–110,
Berlin, Germany, August 11th, 2016. c©2016 Association for Computational Linguistics

Mapping Unseen Words to Task-Trained Embedding Spaces

Pranava Swaroop Madhyastha∗ Mohit Bansal† Kevin Gimpel† Karen Livescu†
∗Universitat Politècnica de Catalunya

pranava@cs.upc.edu
†Toyota Technological Institute at Chicago

{mbansal,kgimpel,klivescu}@ttic.edu

Abstract

We consider the supervised training set-
ting in which we learn task-specific word
embeddings. We assume that we start
with initial embeddings learned from unla-
belled data and update them to learn task-
specific embeddings for words in the su-
pervised training data. However, for new
words in the test set, we must use ei-
ther their initial embeddings or a single
unknown embedding, which often leads
to errors. We address this by learning a
neural network to map from initial em-
beddings to the task-specific embedding
space, via a multi-loss objective func-
tion. The technique is general, but here
we demonstrate its use for improved de-
pendency parsing (especially for sentences
with out-of-vocabulary words), as well as
for downstream improvements on senti-
ment analysis.

1 Introduction
Performance on NLP tasks drops significantly

when moving from training sets to held-out
data (Petrov et al., 2010). One cause of this drop
is words that do not appear in the training data but
appear in test data, whether in the same domain or
in a new domain. We refer to such out-of-training-
vocabulary (OOTV) words as unseen words. NLP
systems often make errors on unseen words and, in
structured tasks like dependency parsing, this can
trigger a cascade of errors in the sentence.

Word embeddings can counter the effects of
limited training data (Necsulescu et al., 2015;
Turian et al., 2010; Collobert et al., 2011). While
the effectiveness of pretrained embeddings can be
heavily task-dependent (Bansal et al., 2014), there

is a great deal of work on updating embeddings
during supervised training to make them more
task-specific (Kalchbrenner et al., 2014; Qu et al.,
2015; Chen and Manning, 2014). These task-
trained embeddings have shown encouraging re-
sults but raise some concerns: (1) the updated em-
beddings of infrequent words are prone to overfit-
ting, and (2) many words in the test data are not
contained in the training data at all. In the lat-
ter case, at test time, systems either use a single,
generic embedding for all unseen words or use
their initial embeddings (typically derived from
unlabelled data) (Søgaard and Johannsen, 2012;
Collobert et al., 2011). Neither choice is ideal: A
single unknown embedding conflates many words,
while the initial embeddings may be in a space that
is not comparable to the trained embedding space.

In this paper, we address both concerns by
learning to map from the initial embedding space
to the task-trained space. We train a neural net-
work mapping function that takes initial word em-
beddings and maps them to task-specific embed-
dings that are trained for the given task, via a
multi-loss objective function. We tune the map-
per’s hyperparameters to optimize performance
on each domain of interest, thereby achieving
some of the benefits of domain adaptation. We
demonstrate significant improvements in depen-
dency parsing across several domains and for the
downstream task of dependency-based sentiment
analysis using the model of Tai et al. (2015).

2 Mapping Unseen Representations
Let V = {w1, . . . , wV } be the vocabulary of

word types in a large, unannotated corpus. Let
eoi denote the initial (original) embedding of word
wi computed from this corpus. The initial em-
beddings are typically learned in an unsupervised

100



loss(eti, e
m
i )

Model
Parameters

W

Annotated
Training

Sentences

Initial
Embeddings

eoi

Non-Linear
Layer

Mapper Function

Task-Trained
Embeddings

eti

Parser
Training

Mapped
Embeddings

emi

(a) Mapper Training

Initial
Embeddings

eoi

Testing
Sentences

Model
Parameters
W

Non-Linear
Layer

Mapper Function

Unseen
Mapped

Embeddings
emi

Seen
Task-Trained
Embeddings

eti

Parser

(b) Test-time: Parsing with Mapped Embed-
dings

Figure 1: System Pipeline

way, but for our purposes they can be any ini-
tial embeddings. Let T ⊆ V be the subset of
words that appear in the annotated training data
for some supervised task-specific training. We de-
fine unseen words as those in the set V \T . While
our approach is general, for concreteness, we con-
sider the task of dependency parsing, so the anno-
tated data consists of sentences paired with depen-
dency trees. We assume a dependency parser that
learns task-specific word embeddings eti for word
wi ∈ T , starting from the original embedding eoi .
In this work, we use the Stanford neural depen-
dency parser (Chen and Manning, 2014).

The goal of the mapper is as follows.
We are given a training set of N pairs
of initial and task-trained embeddings D ={(

eo1, e
t
1

)
, . . . ,

(
eoN , e

t
N

)}
, and we want to learn a

function G that maps each initial embedding eoi to
be as close as possible to its corresponding output
embedding eti. We denote the mapped embedding
emi , i.e., e

m
i = G (e

o
i ).

Figure 1a describes the training procedure of
the mapper. We use a supervised parser which is
trained on an annotated dataset and initialized with
pre-trained word embeddings eoi . The parser uses
back-propagation to update these embeddings dur-
ing training, producing task-trained embeddings eti
for all wi ∈ T . After we train the parser, the map-
ping function G is trained to map an initial word
embedding eoi to its parser-trained embedding e

t
i.

At test (or development) time, we use the trained

mapper G to transform the original embeddings of
unseen test words to the parser-trained space (see
Figure 1b). When parsing held-out data, we use
the same parser model parameters (W ) as shown
in Figure 1b. The only difference is that now some
of the word embeddings (i.e., for unseen words)
have changed to mapped ones.

2.1 Mapper Architecture
Our proposed mapper is a multi-layer feedfor-

ward neural network that takes an initial word em-
bedding as input and outputs a mapped represen-
tation of the same dimensionality. In particular,
we use a single hidden layer with a hardtanh non-
linearity, so the function G is defined as:

G(eoi ) = W2(hard tanh(W1e
o
i + b1)) + b2 (1)

where W1 and W2 are parameter matrices and b1
and b2 are bias vectors.

The ‘hardtanh’ non-linearity is the standard
‘hard’ version of hyperbolic tangent:

hard tanh(z) =


−1 if z < −1
z if −1 ≤ z ≤ 1
1 if z > 1

In preliminary experiments we compared with
other non-linear functions (sigmoid, tanh, and
ReLU), as well as with zero and more than one
non-linear layers. We found that fewer or more
non-linear layers did not improve performance.

101



2.2 Loss Function
We use a weighted, multi-loss regression ap-

proach, optimizing a weighted sum of mean
squared error and mean absolute error:

loss(y, ŷ) =

α
n∑

j=1

|yj − ŷj |+ (1− α)
n∑

j=1

|yj − ŷj |2 (2)

where y = eti (the ground truth) and ŷ = e
m
i (the

prediction) are n-dimensional vectors. This multi-
loss approach seeks to make both the conditional
mean of the predicted representation close to the
task-trained representation (via the squared loss)
and the conditional median of the predicted rep-
resentation close to the task-trained one (via the
mean absolute loss). A weighted multi-criterion
objective allows us to avoid making strong as-
sumptions about the optimal transformation to
be learned. We tune the hyperparameter α on
domain-specific held-out data. We try to minimize
the assumptions in our formulation of the loss, and
let the tuning determine the particular mapper con-
figuration that works best for each domain. Strict
squared loss or an absolute loss are just special
forms of this loss function.

For optimization, we use batch limited memory
BFGS (L-BFGS) (Liu and Nocedal, 1989). In pre-
liminary experiments comparing with stochastic
optimization, we found L-BFGS to be more sta-
ble to train and easier to check for convergence
(as has recently been found in other settings as
well (Ngiam et al., 2011)).

2.3 Regularization
We use elastic net regularization (Liu and No-

cedal, 1989), which linearly combines ℓ1 and ℓ2
penalties on the parameters to control the capacity
of the mapper function. This equates to minimiz-
ing:

F (θ) = L(θ) + λ1‖θ‖1 + λ22 ‖θ‖
2
2

where θ is the full set of mapper parameters and
L(θ) is the loss function (Eq. 2 summed over map-
per training examples). We tune the hyperparam-
eters of the regularizer and the loss function sep-
arately for each task, using a task-specific devel-
opment set. This gives us additional flexibility to
map the embeddings for the domain of interest, es-
pecially when the parser training data comes from
a particular domain (e.g., newswire) and we want

to use the parser on a new domain (e.g., email).
We also tried dropout-based regularization (Sri-
vastava et al., 2014) for the non-linear layer but
did not see any significant improvement.

2.4 Mapper-Parser Thresholds
Certain words in the parser training data T are

very infrequent, which may lead to inferior task-
specific embeddings eti learned by the parser. We
want our mapper function to be learned on high-
quality task-trained embeddings. After learning a
strong mapping function, we can use it to remap
the inferior task-trained embeddings.

We thus consider several frequency thresholds
that control which word embeddings to use to train
the mapper and which to map at test time. Below
are the specific thresholds that we consider:

Mapper-training Threshold (τt) The mapper is
trained only on embedding pairs for words seen at
least τt times in the training data T .
Mapping Threshold (τm) For test-time infer-
ence, the mapper will map any word whose count
in T is less than τm. That is, we discard parser-
trained embeddings eti of these infrequent words
and use our mapper to map the initial embeddings
eoi instead.

Parser Threshold (τp) While training the
parser, for words that appear fewer than τp times
in T , the parser replaces them with the ‘unknown’
embedding. Thus, no parser-trained embeddings
will be learned for these words.

In our experiments, we explore a small set of
values from this large space of possible threshold
combinations (detailed below). We consider only
relatively small values for the mapper-training (τt)
and parser thresholds (τp) because as we increase
them, the number of training examples for the
mapper decreases, making it harder to learn an ac-
curate mapping function1.

3 Related Work
There are several categories of related ap-

proaches, including those that learn a single

1Note that the training of the mapper tends to be very
quick because training examples are word types rather than
word tokens. When we increase τt, the number of training
examples reduces further. Hence, since we do not have many
examples, we want the mapping procedure to have as much
flexibility as possible, so we use multiple losses and regular-
ization strategies, and then tune their relative strengths.

102



embedding for unseen words (Søgaard and Jo-
hannsen, 2012; Chen and Manning, 2014; Col-
lobert et al., 2011), those that use character-level
information (Luong et al., 2013; Botha and Blun-
som, 2014; Ling et al., 2015; Ballesteros et al.,
2015), those using morphological and n-gram in-
formation (Candito and Crabbé, 2009; Habash,
2009; Marton et al., 2010; Seddah et al., 2010;
Attia et al., 2010; Bansal and Klein, 2011; Keller
and Lapata, 2003), and hybrid approaches (Dyer
et al., 2015; Jean et al., 2015; Luong et al., 2015;
Chitnis and DeNero, 2015). The representation for
the unknown token is either learned specifically or
computed from a selection of rare words, for ex-
ample by averaging their embedding vectors.

Other work has also found improvements by
combining pre-trained, fixed embeddings with
task-trained embeddings (Kim, 2014; Paulus et
al., 2014). Also relevant are approaches devel-
oped specifically to handle large target vocabular-
ies (including many rare words) in neural machine
translation systems (Jean et al., 2015; Luong et al.,
2015; Chitnis and DeNero, 2015).

Closely related to our approach is that of
Tafforeau et al. (2015). They induce embeddings
for unseen words by combining the embeddings
of the k nearest neighbors. In Sec. 4, we show that
our approach outperforms theirs. Also related is
the approach taken by Kiros et al. (2015). They
learn a linear mapping of the initial embedding
space via unregularized linear regression. Our ap-
proach differs by considering nonlinear mapping
functions, comparing different losses and mapping
thresholds, and learning separately tuned mappers
for each domain of interest. Moreover, we focus
on empirically evaluating the effect of the map-
ping of unseen words, showing statistically signif-
icant improvements on both parsing and a down-
stream task (sentiment analysis).

4 Experimental Setup
4.1 Dependency Parser

We use the feed-forward neural network depen-
dency parser of Chen and Manning (2014). In all
our experiments (unless stated otherwise), we use
the default arc-standard parsing configuration and
hyperparameter settings. For evaluation, we com-
pute the percentage of words that get the correct
head, reporting both unlabelled attachment score
(UAS) and labelled attachment score (LAS). LAS
additionally requires the predicted dependency la-
bel to be correct. To measure statistical signifi-

cance, we use a bootstrap test (Efron and Tibshi-
rani, 1986) with 100K samples.

4.2 Pre-Trained Word Embeddings
We use the 100-dimensional GloVe word em-

beddings from Pennington et al. (2014). These
were trained on Wikipedia 2014 and the Gigaword
v5 corpus and have a vocabulary size of approxi-
mately 400,000.2

4.3 Datasets
We consider a number of datasets with varying

rates of OOTV words. We define the OOTV rate
(or, equivalently, the unseen rate) of a dataset as
the percentage of the vocabulary (types) of words
occurring in the set that were not seen in training.

Wall Street Journal (WSJ) and
OntoNotes-WSJ We conduct experiments
on the Wall Street Journal portion of the English
Penn Treebank dataset (Marcus et al., 1993).
We follow the standard splits: sections 2-21 for
training, section 22 for validation, and section 23
for testing. We convert the original phrase struc-
ture trees into dependency trees using Stanford
Basic Dependencies (De Marneffe and Manning,
2008) in the Stanford Dependency Parser. The
POS tags are obtained using the Stanford POS
tagger (Toutanova et al., 2003) in a 10-fold
jackknifing setup on the training data (achieving
an accuracy of 96.96%). The OOTV rate in the
development and test sets is approximately 2-3%.

We also conduct experiments on the OntoNotes
4.0 dataset (which we denote OntoNotes-WSJ).
This dataset contains the same sentences as the
WSJ corpus (and we use the same data splits),
but has significantly different annotations. The
OntoNotes-WSJ training data is used for the Web
Treebank test experiments. We perform the same
pre-processing steps as for the WSJ dataset.

Web Treebank We expect our mapper to be
most effective when parsing held-out data with
many unseen words. This often happens when the
held-out data is drawn from a different distribution
than the training data. For example, when train-
ing a parser on newswire and testing on web data,

2
http://www-nlp.stanford.edu/data/glove.6B.100d.txt.gz;

We have also experimented with the downloadable 50-
dimensional SENNA embeddings from Collobert et al.
(2011) and with word2vec (Mikolov et al., 2013) embed-
dings that we trained ourselves; in preliminary experiments
the GloVe embeddings performed best, so we use them for
all experiments below.

103



many errors occur due to differing patterns of syn-
tactic usage and unseen words (Foster et al., 2011;
Petrov and McDonald, 2012; Kong et al., 2014;
Wang et al., 2014).

We explore this setting by training our parser
on OntoNotes-WSJ and testing on the Web Tree-
bank (Petrov and McDonald, 2012), which in-
cludes five domains: answers, email, newsgroups,
reviews, and weblogs. Each domain contains ap-
proximately 2000-4000 manually annotated syn-
tactic parse trees in the OntoNotes 4.0 style. In
this case, we are adapting the parser which is
trained on OntoNotes corpora using the small de-
velopment set for each of the sub-domains (the
size of the Web Treebank dev corpora is only
around 1000-2000 trees so we use it for valida-
tion instead of including it in training). As be-
fore, we convert the phrase structure trees to de-
pendency trees using Stanford Basic Dependen-
cies. The parser and the mapper hyperparameters
were tuned separately on the development set for
each domain. The unseen rate is typically 6-10%
in the domains of the Web Treebank. We used
the Stanford tagger (Toutanova et al., 2003), which
was trained on the OntoNotes training corpus, for
part-of-speech tagging the Web Treebank corpora.
The tagger used bidirectional architecture and it
included word shape and distributional similarity
features. We train a separate mapper for each do-
main, tuning mapper hyperparameters separately
for each domain using the development sets. In
this way, we obtain some of the benefits of domain
adaptation for each target domain.

Downstream Task: Sentiment Analysis with
Dependency Tree LSTMs We also perform ex-
periments to analyze the effects of embedding
mapping on a downstream task, in this case senti-
ment analysis using the Stanford Sentiment Tree-
bank (Socher et al., 2013). We use the de-
pendency tree long short-term memory network
(Tree-LSTM) proposed by Tai et al. (2015), sim-
ply replacing their default dependency parser with
our version that maps unseen words. The de-
pendency parser is trained on the WSJ corpus
and mapped using the WSJ development set. We
use the same mapper that was optimized for the
WSJ development set, without further hyperpa-
rameter tuning for the mapper. For the Tree-
LSTM model, we use the same hyperparameter
tuning as described in Tai et al. (2015). We
use the standard train/development/test splits of

6820/872/1821 sentences for the binary classifica-
tion task and 8544/1101/2210 for the fine-grained
task.

4.4 Mapper Settings and Hyperparameters
The initial embeddings given to the mapper

are the same as the initial embeddings given to
the parser. These are the 100-dimensional GloVe
embeddings mentioned above. The output di-
mensionality of the mapper is also fixed to 100.
All model parameters of the mappers are initial-
ized to zero. We set the dimensionality of the
non-linear layer to 400 across all experiments.
The model parameters are optimized by maximiz-
ing the weighted multiple-loss objective using L-
BFGS with elastic-net regularization (Section 2).
The hyperparameters include the relative weight
of the two objective terms (α) and the regulariza-
tion constants (λ1, λ2). For α, we search over val-
ues in {0, 0.1, 0.2, . . . , 1}. For each of λ1 and λ2,
we consider values in {10−1, 10−2, . . . , 10−9, 0}.
The hyperparameters are tuned via grid search to
maximize the UAS on the development set.

5 Results and Analysis
5.1 Results on WSJ, OntoNotes, and

Switchboard
The upper half of Table 1 shows our main test

results on WSJ, OntoNotes, and Switchboard, the
low-OOTV rate datasets. Due to the small initial
OOTV rates (<3%), we only see modest gains of
0.3-0.4% in UAS, with statistical significance at
p < 0.05 for WSJ and OntoNotes and p < 0.07
for Switchboard. The initial OOTV rates are cut
in half by our mapper, with the remaining un-
known words largely being numerical strings and
misspellings.3 When only considering test sen-
tences containing OOTV words (the row labeled
“OOTV subset”), the gains are significantly larger
(0.5-0.8% UAS at p < 0.05).

5.2 Results on Web Treebank
The lower half of Table 1 shows our main test

results on the Web Treebank’s five domains, the
high-OOTV rate datasets. As expected, the map-
per has a much larger impact when parsing these
out-of-domain datasets with high OOTV word

3We could potentially train the initial embeddings on a
larger corpus or use heuristics to convert unknown numbers
and misspellings to forms contained in our initial embed-
dings.

104



Lower OOTV word rate Higher OOTV word rate
WSJ OntoNotes Avg. Answers Emails Newsgroups Reviews Weblogs Avg.

UAS 91.85→92.21 90.17→90.49 90.38→90.70 82.67→83.21 81.76→82.42 84.68→85.13 84.25→84.99 87.73→88.43 84.22→84.84
LAS 89.49→89.73 87.68→87.92 87.92→88.14 78.98→79.59 77.93→78.56 81.88→82.71 81.26→81.92 85.68→86.29 81.01→81.81
OOTV % 2.72→1.45 2.72→1.4 − 8.53→1.22 10.56→3.01 10.34→1.04 6.84→0.73 8.45→0.38 −
OOTV UAS 89.88→90.51 89.27→89.81 89.12→89.78 80.88→81.75 79.29→81.02 82.54→83.71 81.17→82.22 86.43→87.31 82.06→83.20
#Sents 337 329 − 671 644 579 632 541 −

Table 1: Results of dependency parsing on various treebanks. Entries of the form A→B give results for parsing without mapped
embeddings (A) and with mapped embeddings (B). “OOTV %” entries A→B indicate that A% of the test set vocabulary was
unseen in the parser training, and B% remain unknown after mapping the embeddings. “OOTV UAS” refers to UAS measured
on the subset of the test set sentences that contain at least one OOTV word, and “#Sents” gives the number of sentences in this
subset.

Wife and I attempted to adopt a dog and was nothing but frustrating

nsubj

nsubj

cc

conj

nsubj

conj

cc

xcomp

det
dobj

cc aux

xcomp

nn

conj

attr

Unseen Word

cc

(a) We obtain correct attachments and correct tree after the mapper maps the unseen word ‘attempted’.

Try google to find the title . . .
xcomp

aux

dobj

xcomp

aux

dobj

det

Unseen Word

(b) The mapper incorrectly maps ‘google’, resulting in wrong attachments and wrong tree.

Figure 2: Examples where the mapper helps and hurts: In the above examples the top arcs are before mapping and bottom ones
are after mapping; dotted lines refer to incorrect attachment.

rates.4

The OOTV rate reduction is much larger than
for the WSJ-style datasets, and the parsing im-
provements (UAS and LAS) are statistically sig-
nificant at p < 0.05. On subsets containing at
least one OOTV word (that also has an initial
embedding), we see an average gain of 1.14%
UAS (see row labeled “OOTV subset”). In this
case, all improvements are statistically significant
at p < 0.02. We observe that the relative reduction
in OOTV% for the Web Treebanks is larger than
for the WSJ, OntoNotes, or Switchboard datasets.
In particular, we are able to reduce the OOTV%
by 71-95% relative. We also see the intuitive trend

4As stated above, we train the parser on the OntoNotes
dataset, but tune mapper hyperparameters to maximize pars-
ing performance on each development section of the Web
Treebank’s five domains. We then map the OOTV word vec-
tors on each test set domain using the learned mapper for that
domain.

that larger relative reductions in OOTV rate corre-
late with larger accuracy improvements.

5.3 Downstream Results
We now report results using the Dependency

Tree-LSTM of Tai et al. (2015) for sentiment anal-
ysis on the Stanford Sentiment Treebank. We con-
sider both the binary (positive/negative) and fine-
grained classification tasks ({very negative, nega-
tive, neutral, positive, and very positive}). We use
the implementation provided by Tai et al. (2015),
changing only the dependency parses that are fed
to their model. The sentiment dataset contains ap-
proximately 25% OOTV words in the training set
vocabulary, 5% in the development set vocabulary,
and 9% in the test set vocabulary. We map un-
seen words using the mapper tuned on the WSJ
development set. We use the same Dependency
Tree-LSTM experimental settings as Tai et al. Re-

105



Fine-Grained Binary
48.4→49.5 85.7→ 86.1

Table 2: Improvements on Stanford Sentiment Treebank test
set using our parser with the Dependency Tree-LSTM.

Baseline t1 t3 t5 t∞
84.11 84.89 84.97 84.81 84.14

Table 3: Average Web Treebank development UAS at differ-
ent threshold settings.

sults are shown in Table 2. We improve upon the
original accuracies in both binary and fine-grained
classification. 5 We also reduce the OOTV rate
from 25% in the training set vocabulary to about
6%, and from 9% in the test set vocabulary down
to 4%.

5.4 Effect of Thresholds
We also experimented with different values for

the thresholds described in Section 2. For the map-
ping threshold τm, mapper-training threshold τt,
and parser threshold τp, we consider the following
four settings:

t1 : τm = τt = τp = 1
t3 : τm = τt = τp = 3
t5 : τm = τt = τp = 5

t∞ : τm = ∞, τp = τt = 5
Using τm = ∞ corresponds to mapping all words
at test time, even words that we have seen many
times in the training data and learned task-specific
embeddings for.

We report the average development set UAS
over all Web Treebank domains in Table 3. We
see that t3 performs best, though settings t1 and
t5 also improve over the baseline. At threshold t3
we have approximately 20,000 examples for train-
ing the mapper, while at threshold t5 we have only
about 10,000 examples. We see a performance
drop at t∞, so it appears better to directly use
the task-specific embeddings for words that appear
frequently in the training data. In other results re-
ported in this paper, we used t3 for the Web Tree-
bank test sets and t1 for the rest.

5.5 Effect of Weighted Multi-Loss Objective
We analyzed the results when varying α, which

balances between the two components of the map-
5Note that we report accuracies and improvements on the

dependency parse based system of Tai et al. (2015) because
the neural parser that we use is dependency-based.

per’s multi-loss objective function. We found that,
for all domains except Answers, the best results
are obtained with some α between 0 and 1. The
optimal values outperformed the cases with α = 0
and α = 1 by 0.1-0.3% UAS absolute. However,
on the Answers domain, the best performance was
achieved with α = 0; i.e., the mapper preferred
mean squared error. For other domains, the opti-
mal α tended to be within the range [0.3, 0.7].

5.6 Comparison with Related Work
We compare to the approach presented by

Tafforeau et al. (2015). They propose to refine em-
beddings for unseen words based on the relative
shifts of their k nearest neighbors in the original
embeddings space. Specifically, they define “arti-
ficial refinement” as:

φr(t) = φo(t) +
K∑

k=1

αk(φr(nk)− φo(nk)) (3)

where φr(.) is the vector in the refined embedding
space and φo(.) is the vector in the original em-
bedding space. They define αk to be proportional
to the cosine similarity between the target unseen
word (t) and neighbor (nk):

αk = s(t, nk) =
φo(t).φo(nk)
|φ(t)||φo(nk)|

Avg. UAS Avg. LAS
Baseline 84.11 81.02
k-NN 84.54 81.38
Our Mapped 84.97 81.79

Table 4: Comparison to k-nearest neighbor matching of
Tafforeau et al. (2015).

Table 4 shows the average performance of the
models over the development sets of the Web Tree-
bank. On average, our mapper outperforms the k-
NN approach (k = 3).

5.7 Dependency Parsing Examples
In Figure 2, we show two sentences: an instance

where the mapper helps and another where the
mapper hurts the parsing performance.6 In the first
sentence (Figure 2a), the parsing model has not
seen the word ‘attempted’ during training. Note
that the sentence contains 3 verbs: ‘attempted’,
‘adopt’, and ‘was’. Even with the POS tags, the

6Sentences in Figure 2 are taken from the development
portion of the Answers domain from the Web Treebank.

106



parser was unable to get the correct dependency
attachment. After mapping, the parser correctly
makes ‘attempted’ the root and gets the correct
arcs and the correct tree. The 3 nearest neighbors
of ‘attempted’ in the mapped embedding space are
‘attempting’, ‘tried’, and ‘attempt’. We also see
here that a single unseen word can lead to multi-
ple errors in the parse.

In the second example (Figure 2b), the default
model assigns the correct arcs using the POS in-
formation even though it has not seen the word
‘google’. However, using the mapped represen-
tation for ‘google’, the parser makes errors. The
3-nearest neighbors for ‘google’ in the mapped
space are ‘damned’, ‘look’, and ‘hash’. We hy-
pothesize that the mapper has mapped this noun
instance of ‘google’ to be closer to verbs instead
of nouns, which would explain the incorrect at-
tachment.

5.8 Analyzing Mapped Representations

To understand the mapped embedding space,
we use t-SNE (Van der Maaten and Hinton, 2008)
to visualize a small subset of embeddings. In Fig-
ure 3, we plot the initial embeddings, the parser-
trained embeddings, and finally the mapped em-
beddings. We include four unseen words (shown
in caps): ‘horrible’, ‘poor’, ‘marvelous’, and
‘magnificent’. In Figure 3a and Figure 3b, the em-
beddings for the unseen words are identical (even
though t-SNE places them in different places when
producing its projection). In Figure 3c, we ob-
serve that the mapper has placed the unseen words
within appropriate areas of the space with respect
to similarity with the seen words. We contrast this
with Figure 3b, in which the unseen words appear
to be within a different region of the space from
all seen words.

6 Conclusion

We have described a simple method to resolve
unseen words when training supervised models
that learn task-specific word embeddings: a feed-
forward neural network that maps initial embed-
dings to the task-specific embedding space. We
demonstrated significant improvements in depen-
dency parsing accuracy across several domains, as
well as improvements on a downstream task. Our
approach is simple, effective, and applicable to
many other settings, both inside and outside NLP.

(a) Initial Representational Space

(b) Learned Representational Space

(c) Mapped Representational Space

Figure 3: t-SNE plots on initial, parser trained, and mapped
representations.

Acknowledgments

We would like to thank the anonymous review-
ers for their useful comments. This research was
supported by a Google Faculty Research Award to
Mohit Bansal, Karen Livescu and Kevin Gimpel.

107



References
Mohammed Attia, Jennifer Foster, Deirdre Hogan,

Joseph Le Roux, Lamia Tounsi, and Josef Van Gen-
abith. 2010. Handling unknown words in statis-
tical latent-variable parsing models for arabic, en-
glish and french. In Proceedings of the NAACL
HLT 2010 First Workshop on Statistical Parsing of
Morphologically-Rich Languages, pages 67–75. As-
sociation for Computational Linguistics.

Miguel Ballesteros, Chris Dyer, and Noah A. Smith.
2015. Improved transition-based parsing by model-
ing characters instead of words with lstms. In Pro-
ceedings of the 2015 Conference on Empirical Meth-
ods in Natural Language Processing, pages 349–
359, Lisbon, Portugal, September. Association for
Computational Linguistics.

Mohit Bansal and Dan Klein. 2011. Web-scale fea-
tures for full-scale parsing. In Proceedings of ACL.

Mohit Bansal, Kevin Gimpel, and Karen Livescu.
2014. Tailoring continuous word representations for
dependency parsing. In Proceedings of ACL.

Jan A. Botha and Phil Blunsom. 2014. Compositional
morphology for word representations and language
modelling. In International Conference on Machine
Learning (ICML).

Marie Candito and Benoı̂t Crabbé. 2009. Improving
generative statistical parsing with semi-supervised
word clustering. In Proceedings of the 11th Inter-
national Conference on Parsing Technologies, pages
138–141. Association for Computational Linguis-
tics.

Danqi Chen and Christopher D Manning. 2014. A fast
and accurate dependency parser using neural net-
works. In Empirical Methods in Natural Language
Processing (EMNLP).

Rohan Chitnis and John DeNero. 2015. Variable-
length word encodings for neural translation models.
In Proceedings of the 2015 Conference on Empiri-
cal Methods in Natural Language Processing, pages
2088–2093, Lisbon, Portugal, September. Associa-
tion for Computational Linguistics.

R. Collobert, J. Weston, L. Bottou, M. Karlen,
K. Kavukcuoglu, and P. Kuksa. 2011. Natural lan-
guage processing (almost) from scratch. Journal of
Machine Learning Research, 12:24932537.

Marie-Catherine De Marneffe and Christopher D Man-
ning. 2008. Stanford typed dependencies manual.
Technical report, Stanford University.

Chris Dyer, Miguel Ballesteros, Wang Ling, Austin
Matthews, and Noah A. Smith. 2015. Transition-
based dependency parsing with stack long short-
term memory. In Proceedings of the 53rd Annual
Meeting of the Association for Computational Lin-
guistics and the 7th International Joint Conference
on Natural Language Processing (Volume 1: Long
Papers), pages 334–343, Beijing, China, July. Asso-
ciation for Computational Linguistics.

B. Efron and R. Tibshirani. 1986. Bootstrap meth-
ods for standard errors, confidence intervals, and
other measures of statistical accuracy. Statist. Sci.,
1(1):54–75, 02.

Jennifer Foster, Özlem Çetinoglu, Joachim Wagner,
Joseph Le Roux, Stephen Hogan, Joakim Nivre,
Deirdre Hogan, and Josef Van Genabith. 2011. #
hardtoparse: Pos tagging and parsing the twitter-
verse. In AAAI 2011 Workshop on Analyzing Mi-
crotext, pages 20–25.

Nizar Habash. 2009. Remoov: A tool for online han-
dling of out-of-vocabulary words in machine trans-
lation. In Proceedings of the 2nd International Con-
ference on Arabic Language Resources and Tools
(MEDAR), Cairo, Egypt.

Sébastien Jean, Kyunghyun Cho, Roland Memisevic,
and Yoshua Bengio. 2015. On using very large
target vocabulary for neural machine translation.
In Proceedings of the 53rd Annual Meeting of the
Association for Computational Linguistics and the
7th International Joint Conference on Natural Lan-
guage Processing (Volume 1: Long Papers), pages
1–10, Beijing, China, July. Association for Compu-
tational Linguistics.

Nal Kalchbrenner, Edward Grefenstette, and Phil Blun-
som. 2014. A convolutional neural network for
modelling sentences. In Proceedings of the 52nd
Annual Meeting of the Association for Computa-
tional Linguistics (Volume 1: Long Papers), pages
655–665, Baltimore, Maryland, June. Association
for Computational Linguistics.

Frank Keller and Mirella Lapata. 2003. Using the web
to obtain frequencies for unseen bigrams. Computa-
tional linguistics, 29(3):459–484.

Yoon Kim. 2014. Convolutional neural networks
for sentence classification. In Proceedings of the
2014 Conference on Empirical Methods in Natu-
ral Language Processing (EMNLP), pages 1746–
1751, Doha, Qatar, October. Association for Com-
putational Linguistics.

Ryan Kiros, Yukun Zhu, Ruslan Salakhutdinov,
Richard S Zemel, Antonio Torralba, Raquel Urta-
sun, and Sanja Fidler. 2015. Skip-thought vectors.
arXiv preprint arXiv:1506.06726.

Lingpeng Kong, Nathan Schneider, Swabha
Swayamdipta, Archna Bhatia, Chris Dyer, and
Noah A. Smith. 2014. A dependency parser for
tweets. In Proceedings of the 2014 Conference on
Empirical Methods in Natural Language Processing
(EMNLP), pages 1001–1012, Doha, Qatar, October.
Association for Computational Linguistics.

Wang Ling, Tiago Luı́s, Luı́s Marujo, Rámon Fernan-
dez Astudillo, Silvio Amir, Chris Dyer, Alan W
Black, and Isabel Trancoso. 2015. Finding func-
tion in form: Compositional character models for
open vocabulary word representation. In Proc. of
EMNLP.

108



D. C. Liu and J. Nocedal. 1989. On the limited
memory BFGS method for large scale optimization.
Math. Programming, 45(3, (Ser. B)):503–528.

Thang Luong, Richard Socher, and Christopher Man-
ning. 2013. Better word representations with recur-
sive neural networks for morphology. In Proceed-
ings of the Seventeenth Conference on Computa-
tional Natural Language Learning, pages 104–113,
Sofia, Bulgaria, August. Association for Computa-
tional Linguistics.

Thang Luong, Ilya Sutskever, Quoc Le, Oriol Vinyals,
and Wojciech Zaremba. 2015. Addressing the rare
word problem in neural machine translation. In Pro-
ceedings of the 53rd Annual Meeting of the Associ-
ation for Computational Linguistics and the 7th In-
ternational Joint Conference on Natural Language
Processing (Volume 1: Long Papers), pages 11–19,
Beijing, China, July. Association for Computational
Linguistics.

Mitchell P Marcus, Mary Ann Marcinkiewicz, and
Beatrice Santorini. 1993. Building a large anno-
tated corpus of english: The penn treebank. Compu-
tational linguistics, 19(2):313–330.

Yuval Marton, Nizar Habash, and Owen Rambow.
2010. Improving arabic dependency parsing with
lexical and inflectional morphological features. In
Proceedings of the NAACL HLT 2010 First Work-
shop on Statistical Parsing of Morphologically-Rich
Languages, pages 13–21, Los Angeles, CA, USA,
June. Association for Computational Linguistics.

Tomas Mikolov, Kai Chen, Greg Corrado, and Jef-
frey Dean. 2013. Efficient estimation of word
representations in vector space. arXiv preprint
arXiv:1301.3781.

Silvia Necsulescu, Sara Mendes, David Jurgens, Núria
Bel, and Roberto Navigli. 2015. Reading between
the lines: Overcoming data sparsity for accurate
classification of lexical relationships. In Proceed-
ings of the Fourth Joint Conference on Lexical and
Computational Semantics, pages 182–192, Denver,
Colorado, June. Association for Computational Lin-
guistics.

Jiquan Ngiam, Adam Coates, Ahbik Lahiri, Bobby
Prochnow, Quoc V. Le, and Andrew Y. Ng. 2011.
On optimization methods for deep learning. In Pro-
ceedings of the 28th International Conference on
Machine Learning (ICML-11), pages 265–272.

Romain Paulus, Richard Socher, and Christopher D
Manning. 2014. Global belief recursive neural net-
works. In Advances in Neural Information Process-
ing Systems, pages 2888–2896.

Jeffrey Pennington, Richard Socher, and Christopher
Manning. 2014. Glove: Global vectors for word
representation. In Proceedings of the 2014 Con-
ference on Empirical Methods in Natural Language
Processing (EMNLP), pages 1532–1543, Doha,

Qatar, October. Association for Computational Lin-
guistics.

Slav Petrov and Ryan McDonald. 2012. Overview of
the 2012 shared task on parsing the web. Notes of
the First Workshop on Syntactic Analysis of Non-
Canonical Language (SANCL).

Slav Petrov, Pi-Chuan Chang, Michael Ringgaard, and
Hiyan Alshawi. 2010. Uptraining for accurate de-
terministic question parsing. In Proceedings of the
2010 Conference on Empirical Methods in Natural
Language Processing, pages 705–713. Association
for Computational Linguistics.

Lizhen Qu, Gabriela Ferraro, Liyuan Zhou, Wei-
wei Hou, Nathan Schneider, and Timothy Baldwin.
2015. Big data small data, in domain out-of domain,
known word unknown word: The impact of word
representation on sequence labelling tasks. arXiv
preprint arXiv:1504.05319.

Djamé Seddah, Grzegorz Chrupała, Ozlem Cetinoglu,
Josef van Genabith, and Marie Candito. 2010.
Lemmatization and lexicalized statistical parsing of
morphologically-rich languages: the case of french.
In Proceedings of the NAACL HLT 2010 First Work-
shop on Statistical Parsing of Morphologically-Rich
Languages, pages 85–93, Los Angeles, CA, USA,
June. Association for Computational Linguistics.

Richard Socher, Alex Perelygin, Jean Wu, Jason
Chuang, Christopher D. Manning, Andrew Ng, and
Christopher Potts. 2013. Recursive deep models
for semantic compositionality over a sentiment tree-
bank. In Proceedings of the 2013 Conference on
Empirical Methods in Natural Language Process-
ing, pages 1631–1642, Seattle, Washington, USA,
October. Association for Computational Linguistics.

Anders Søgaard and Anders Johannsen. 2012. Ro-
bust learning in random subspaces: Equipping NLP
for OOV effects. In Proceedings of COLING 2012:
Posters, Mumbai, India, December.

Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky,
Ilya Sutskever, and Ruslan Salakhutdinov. 2014.
Dropout: A simple way to prevent neural networks
from overfitting. Journal of Machine Learning Re-
search, 15:1929–1958.

Jeremie Tafforeau, Thierry Artieres, Benoit Favre, and
Frederic Bechet. 2015. Adapting lexical represen-
tation and oov handling from written to spoken lan-
guage with word embedding. In Interspeech.

Kai Sheng Tai, Richard Socher, and Christopher D.
Manning. 2015. Improved semantic representa-
tions from tree-structured long short-term memory
networks. In Proceedings of the 53rd Annual Meet-
ing of the Association for Computational Linguistics
and the 7th International Joint Conference on Natu-
ral Language Processing (Volume 1: Long Papers),
pages 1556–1566, Beijing, China, July. Association
for Computational Linguistics.

109



Kristina Toutanova, Dan Klein, Christopher D Man-
ning, and Yoram Singer. 2003. Feature-rich part-of-
speech tagging with a cyclic dependency network.
In Proceedings of the 2003 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics on Human Language Technology-
Volume 1, pages 173–180. Association for Compu-
tational Linguistics.

Joseph Turian, Lev Ratinov, and Yoshua Bengio. 2010.
Word representations: a simple and general method
for semisupervised learning. In Proceedings of the
48th annual meeting of the association for compu-
tational linguistics, page 384394. Association for
Computational Linguistics.

Laurens Van der Maaten and Geoffrey Hinton. 2008.
Visualizing data using t-sne. Journal of Machine
Learning Research, 9(2579-2605):85.

William Yang Wang, Lingpeng Kong, Kathryn
Mazaitis, and William W Cohen. 2014. Depen-
dency parsing for weibo: An efficient probabilis-
tic logic programming approach. In Proceedings of
the 2014 Conference on Empirical Methods in Nat-
ural Language Processing (EMNLP), pages 1152–
1158, Doha, Qatar, October. Association for Com-
putational Linguistics.

110


