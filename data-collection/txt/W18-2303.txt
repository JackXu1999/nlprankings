



















































Identifying Risk Factors For Heart Disease in Electronic Medical Records: A Deep Learning Approach


Proceedings of the BioNLP 2018 workshop, pages 18–27
Melbourne, Australia, July 19, 2018. c©2018 Association for Computational Linguistics

18

Identifying Risk Factors For Heart Disease in Electronic Medical
Records: A Deep Learning Approach

Thanat Chokwijitkul1, Anthony Nguyen2, Hamed Hassanzadeh2, Siegfried Perez3
1School of Information Technology and Electrical Engineering, The University of Queensland

2The Australian e-Health Research Centre, CSIRO
3Emergency Department, Logan Hospital

t.chokwijitkul@uqconnect.edu.au
{Anthony.Nguyen, Hamed.Hassanzadeh}@csiro.au

SiegfriedRobert.Perez@health.qld.gov.au

Abstract

Automatic identification of heart disease
risk factors in clinical narratives can ex-
pedite disease progression modelling and
support clinical decisions. Existing prac-
tical solutions for cardiovascular risk de-
tection are mostly hybrid systems entailing
the integration of knowledge-driven and
data-driven methods, relying on dictionar-
ies, rules and machine learning methods
that require a substantial amount of hu-
man effort. This paper proposes a com-
parative analysis on the applicability of
deep learning, a re-emerged data-driven
technique, in the context of clinical text
classification. Various deep learning ar-
chitectures were devised and evaluated for
extracting heart disease risk factors from
clinical documents. The data provided for
the 2014 i2b2/UTHealth shared task fo-
cusing on identifying risk factors for heart
disease was used for system development
and evaluation. Results have shown that a
relatively simple deep learning model can
achieve a high micro-averaged F-measure
of 0.9081, which is comparable to the best
systems from the shared task. This is
highly encouraging given the simplicity
of the deep learning approach compared
to the heavily feature-engineered hybrid
approaches that were required to achieve
state-of-the-art performances.

1 Introduction

Heart disease is a leading cause of morbidity and
mortality worldwide (Benjamin et al., 2017). As
failure to recognise atypical representations of
such serious illness may lead to adverse outcomes,
accurate diagnosis is crucial to ensure that patients

are placed on the proper treatment pathway. Elec-
tronic medical records (EMR) can be used to im-
prove the diagnosis ability along with measuring
the quality of care. The rapid adoption of EMRs
along with the necessity to enhance the quality of
health care has incentivised the development of
natural language processing (NLP) in the medi-
cal domain. An abundant amount of clinical in-
formation used for medical investigation is organ-
ised in unstructured narrative form, which is suit-
able for expressing medical concepts or events but
challenging for analysis and decision support as
gaining a full aspect of a patients medical history
by reading through EMRs is significantly time-
consuming, especially when only a specific piece
of information is needed. The difficulty of this
process increases in the case of heart disease due
to its complex progression, which regularly in-
volves various factors including lifestyle and so-
cial factors as well as specific medical conditions
(Stubbs and Uzuner, 2015). Various methods have
been proposed in the field of clinical concept ex-
traction, ranging from simple pattern matching to
systems based on symbolic or statistical data and
machine learning (Meystre et al., 2008; Gonzalez-
Hernandez et al., 2017). Those previously pro-
posed approaches have shown promising results
but it is very difficult to reach that point due to
the assiduous process of defining rules and extract-
ing features. This is where deep learning comes in
as this intriguing re-emerged concept can allevi-
ate heavily human dependent efforts required for
knowledge-based approaches and the lack of the
ability of many conventional machine learning al-
gorithms to learn without the necessity of careful
feature engineering with considerable domain ex-
pertise (LeCun et al., 2015).

This paper presents a comparative analysis
of two widely used deep learning architectures,
namely convolutional neural network (CNN) and



19

recurrent neural network (RNN) as well as three
RNN variants, including long short-term mem-
ory (LSTM) (Hochreiter and Schmidhuber, 1997),
bidirectional long short-term memory (BLSTM),
and gated recurrent unit (GRU) (Cho et al., 2014),
for extracting cardiac risk factors from EMRs. Us-
ing the data set from the i2b2/UTHealth shared
task (Stubbs and Uzuner, 2015), the goal is to de-
termine the risk factor indicators contained within
each document along with the temporal attributes
with respect to the document creation time (DCT).

2 Related Work

2.1 Deep Learning for Clinical Information
Extraction

Many recent publications have focused on extract-
ing relevant clinical information from EMRs us-
ing deep learning. One of the most fundamental
tasks involves the extraction of medical concepts
from unstructured clinical notes. This concept ex-
traction problem can be treated as a sequence la-
belling problem where the goal is to assign a clini-
cally relevant tag to each word in an EMR (Jagan-
natha and Yu, 2016). Jagannatha and Yu (2016)
experimented with different deep learning archi-
tectures based on recurrent networks, including
GRUs, LSTMs and BLSTMs. It turned out that
all the RNN variants outperformed the conditional
random field (CRF) baselines, which had previ-
ously been considered the state-of-the-art method
for information extraction in general.

As patient EMRs evolve over time, the sequen-
tiality of clinical events can be used for disease
progression analysis and the prediction of impend-
ing disease conditions (Cheng et al., 2016). Its
temporality induces the necessity of assigning no-
tions of time to each extracted medical concept.
Fries (2016) devised a solution for such more com-
plex problems by using a standard RNN initialised
with word2vec (Mikolov et al., 2013a) vectors
along with utilising DeepDive (Shin et al., 2015)
for forming relationships and predictions. Li and
Huang (2016) and Chikka (2016) also employed
word embedding vectors within their frameworks
but used CNNs to extract the temporal attributes
instead. While still not state-of-the-art, these ap-
proaches produced competitive results in the field
of temporal event extraction but also required a
separate model for each subtask (extracting con-
cepts and temporal attributes) and slight manual
engineering (Shickel et al., 2017; Bethard et al.,

2016). One thing to remark is that none of the ex-
isting systems has ever tried using a single, univer-
sal model that naturally learns the temporal char-
acteristics of those concepts based on their con-
texts and incorporates them into the feature learn-
ing process, which can be used for extracting med-
ical concepts and temporal attributes simultane-
ously. This work intends to explore this idea and
prove that the aforementioned capability is well
within the reach of deep learning.

2.2 i2b2/UTHealth Shared Task

In 2014, the Informatics for Integrating Biology
and the Bedside (i2b2) issued an NLP shared task
focusing on identifying risk factors for heart dis-
ease in clinical narratives. According to Stubbs
et al. (2015), a total of 49 systems from 20 teams
were submitted. The systems varied broadly,
from rule-based systems to complex hybrid sys-
tems with a combination of machine learning
techniques. Nevertheless, some similarities were
found among the top systems including the use
of preprocessing tools to obtain syntactic infor-
mation and section headers for determining tem-
poral labels. The results revealed that the top 10
systems achieved micro-averaged F1 scores over
0.87 while the top 6 systems were able to reach
micro-averaged F1 scores over 0.90. The most
successful system managed to achieve an F1 score
of 0.928 (Roberts et al., 2015) while the averaged
F1 score among all the systems was 0.815. While
half of the top 10 teams used a combination of
knowledge-driven methods, such as lexicon and
rules, and machine learning algorithms, including
CRF, support vector machine (SVM), Naı̈ve Bayes
classifier and Maximum Entropy, none of the par-
ticipants attempted to integrate neural networks
or deep learning into their systems. Furthermore,
there has not existed any approaches that use deep
learning to extract risk factor indicators from the
shared task data since its inception in 2014, which
is a research gap that this work intends to fill.

3 Methodology

3.1 Dataset

The dataset used in this work is the corpus pro-
vided for the 2014 i2b2/UTHealth shared task.
The corpus consists of 1,304 medical records de-
scribing 296 diabetic patients for cardiovascular
risk factors and time attributes with respect to
the DCT. The dataset was split by the challenge



20

Risk Factor Indicator Training Instances Testing Instances Time Attribute

CAD mention, event, test, symptom 1186 784 X

Diabetes mention, high A1c, high glucose 1695 1180 X

Obesity mention, high BMI 433 262 X

Hyperlipidemia mention, high cholesterol, high LDL 1062 751 X

Hypertension mention, high blood pressure 1926 1293 X

Medication

ACE inhibitor, amylin, anti-diabetes,
ARB, aspirin, beta blocker, calcium
channel blocker, diuretic, DPP4 inhibitors,
ezetimibe, fibrate, GLP1 agonist, insulin,
Meglitinide, metformin, niacin, nitrate,
obesity medications, statin, sulfonylurea,
thiazolidinedione, thienopyridine

8638 5674 X

Smoking current, past, ever, never, unknown 771 512 n/a

Family history present, not present 790 514 n/a

Table 1: The indicators associated with each cardiac risk factor and the number of training and testing
instances at annotation level

Evidence Type Example

Phrase-based Significant PMH for CAD, HTN,
GERD, and past cerebral embolism

Logic-based Seen in Cardiac rehab locally last
week and BP 170/80

Discourse-based Findings suggestive of an obstructive,
coronary lesion in the left circumflex
distribution

Table 2: Three types of evidence

provider. The training set consists of 60% of the
entire dataset (790 records) and the test set con-
tains the remaining 40% (514 records). The anno-
tation guidelines describe a set of annotations to
indicate the presence of diseases (coronary artery
disease (CAD) and diabetes), relevant risk factors
(hyperlipidaemia, hypertension, obesity, smoking
status and family history) and associated medica-
tions. Each annotation for a risk factor also has an
indicator value from its own set (see Table 1) as
well as the time attribute (before, during or after
the DCT). Figure 1 shows an example of annota-
tions used for training and evaluation. The ulti-
mate goal is to classify risk factors and time in-
dicators at document level as per Gold Standard
annotation.

The evidence of risk factor indicators can be
categorised into three types according to the ter-
minologies described by Chen et al. (2015), which
include phrase-based, logic-based and discourse-
based indicators as presented in Table 2. Phrase-
based indicators are those that can be identified
directly by locating relevant phrases or particular

Risk Factor Phrase-based
Logic-
based

Discourse-
based

CAD mention n/a event, test,
symptom

Diabetes mention high A1c,
high glucose

n/a

Obesity mention BMI n/a
Hyperlipidemia mention high cholesterol,

high LDL
n/a

Hypertention mention high blood
pressure

n/a

Medication all types n/a n/a
Smoking n/a n/a all statuses
Family history n/a n/a all statuses

Percentage of
training instances 85.33% 8.10% 6.57%

Table 3: Relationships between the indicators and
evidence types and the percentage of training in-
stances belonging to each type

names. Logic-based indicators are indirect infor-
mation that needs a comparison or further analy-
sis after being identified. Finally, discourse-based
indicators are those that appear in the form of sen-
tences and may require a parsing process. The re-
lationships between indicators and evidence types
are listed in Table 3.

3.2 Problem Formation and Evaluation

The classification of risk factors and time indi-
cators was posed as a document-level classifica-
tion problem. This can be seen as a multilabel
classification task where multiple labels are iden-
tified given an EMR. However, unique to the an-
notation guideline (Stubbs and Uzuner, 2015) and



21

Complete version (for training):
<DIABETES start=“122” end=“130” text=“diabetes” time=“before DCT” indicator=“mention”/>
<DIABETES start=“512” end=“528” text=“diabetes type II” time=“before DCT” indicator=“mention”/>
<DIABETES start=“701” end=“718” text=“diabetes mellitus” time=“before DCT” indicator=“mention”/>

Gold standard version (for evaluation):
<DIABETES time=“before DCT” indicator=“mention”/>

Figure 1: Each complete annotation contains token-level information (risk factor tag, risk factor indicator,
offset, text information, and time attribute) while each gold standard annotation contains document-level
information (risk factor tag, risk factor indicator and time attribute) and cannot be duplicated.

the structure of the training data, which contains
phrase-level risk factor and time indicator annota-
tions (see Figure 1), it seems appropriate to for-
mulate the problem as an information extraction
task instead. This approach regards data as a se-
quence of tokens labelled using the Inside-Outside
(IO) scheme: I represents named entity tokens and
O indicates non-entity ones. As the main goal is
to determine the risk factor indicators contained
within the record along with the temporal cate-
gories of those indicators with respect to the DCT,
each entity is tagged with a label using the follow-
ing format:

I-risk factor.indicator.time

Figure 2 shows a sample EMR (represented by a
sequence of words) and associated labels. In this
example, the word “coronary” with the label “I-
cad.mention.before dct” can be interpreted that as
a mention of CAD which was present before the
document creation time.

Words: he, has, coronary, artery, disease,
and, diabetes

Labels: O, O, I-cad.mention.before dct,
I-cad.mention.before dct,
I-cad.mention.before dct, O,
I-diabetes.mention.before dct

Figure 2: A sample phrase in an EMR and associ-
ated labels

Given an EMR as input, the output is a se-
quence of labels, with each label belonging to
a given word. After removing duplicate labels,
the final output will be a set of unique labels
identified for that record (excluding the O la-
bel). For the example in Figure 2, the final out-
put will be generated as a set of two unique la-
bels, including “I-cad.mention.before dct” and “I-
diabetes.mention.before dct”. These labels will

be used to generate system annotations similar to
the one presented in Figure 1 which will subse-
quently be evaluated against the gold standard an-
notations provided by the challenge provider us-
ing the micro-averaged recall, precision and F-
measure as the primary evaluation metrics1.

3.3 Deep Neural Network Models
3.3.1 Convolutional Neural Network

He has coronary artery disease
(word of interest)

0 1 0

I-CAD

Input layer

Convolutional
layer

Max-pooling
layer

Fully-connected
layer and
softmax output

Figure 3: CNN architecture with multiple filter re-
gion sizes

The CNN model, as shown in Figure 3, is based
on the CNN architecture of Kim (2014) but uses
the window approach for NER, introduced by Col-
lobert et al. (2011), to classify each individual
word at a time instead of the entire sentence. This
approach assumes the label of a word is depen-
dent on its neighbouring words. Given a word to
tag, a fixed size window of n words around the
target word where n is odd is taken into account.
A window of n words is represented as a matrix
S ∈ Rd×n:

S =
[
w1 ... wn− (n−1)

2

... wn
]

(1)

1The official evaluation script is available at
https://github.com/kotfic/i2b2 evaluation scripts

https://github.com/kotfic/i2b2_evaluation_scripts


22

where wi ∈ Rd is the d-dimensional word vec-
tor representing the ith word in S and w

n− (n−1)
2

is the target word. Let wi:i+j be the concatena-
tion of words wi,wi+1, ...,wi+j . A convolution
operation involves applying a filter k ∈ Rd×h to a
window of h words, where h < n, to generate a
new feature. For instance, a feature xi is computed
by

xi = f(k ·wi:i+h−1 + b) (2)

where f is an activation function and b ∈ R is
a bias. Note that this CNN architecture can em-
ploy multiple filter region sizes for extracting mul-
tiple features. This operation is applied to ev-
ery possible window of words in the sequence
{w1:h,w2:h+1, ...,wn−h+1:n} to generate a fea-
ture map x = (x1, x2, ..., xn−h+1) where x ∈
Rn−h+1. The pooling layer then applies the max-
pooling operation to down-sample each feature
map by taking the maximum value x̂ = max(x)
which represents the most important feature. Fi-
nally, multiple down-sampled feature maps form
a fully-connected layer, which is used as inputs
to the softmax distribution over all classes. The
subsampled feature maps provide a sequence rep-
resentation for softmax to map to an appropriate
class.

3.3.2 Recurrent Neural Network

He has coronary artery disease

Input layer

Recurrent
layer

Output layer

O O I-CAD I-CAD I-CAD

Figure 4: Basic structure of an RNN

A recurrent neural network is a class of neural net-
works specialised for processing sequential data.
Unlike the CNN, the RNN uses a recurrent layer to
learn the representation of clinical text, as shown
in Figure 4. The input to an RNN is a word se-
quence of length l representing the entire docu-
ment, denoted by a matrix S ∈ Rd×l:

S =
[
w1 w2 ... wl

]
(3)

where wi ∈ Rd is the d-dimensional word vector
representing the ith word in S. In an Elman-type

network (Elman, 1990), a hidden state output hi
is a result of nonlinear transformation of an input
vector wi and the previous hidden state hi−1:

hi = f(hi−1,wi) (4)

where f is a recurrent unit, such as a standard
recurrent unit, LSTM and GRU. Finally, the
hidden state hi is then used as an input to softmax
for identifying a risk factor in the IO format.

Bidirectionality. A bidirectional recurrent neu-
ral network (Schuster and Paliwal, 1997) consists
of two separated recurrent layers for computing
the forward hidden states (

−→
h1,
−→
h2, ...,

−→
hl) and the

backward hidden states (
←−
h1,
←−
h2, ...,

←−
hl). In this

settings,
−→
hi and

←−
hi can be regarded as preserved

information from the past and the future respec-
tively. By using the hidden states from both di-
rections combined, the network has complete past
and future context for every point in the input se-
quence.

3.4 Pre-trained Word Embeddings
Due to the incapability of neural networks to pro-
cess text input, each word is fed to the network
as an index taken from a finite dictionary. As
this simple representation does not contain much
semantic information, the first layer of each net-
work maps each index into its vector represen-
tation using pre-trained word embeddings. The
pre-trained vectors were trained on the 2014 i2b2
dataset. The number of embedding dimensions
was determined empirically. Given a small vo-
cabulary (36,663 words) and a range of embed-
ding dimensions from 20 to 300, an embedding
dimension of 20 yielded best results. Each vec-
tor was trained via the word2vec’s continuous bag-
of-words (CBOW) model (Mikolov et al., 2013b)
similar to that used by Kim (2014).

3.5 Hyperparameters and Training
The CNN model used 5-gram of each EMR as
input since a window of 5 words has shown to
be effective for many NLP tasks (Collobert et al.,
2011). Based on the hyperparameters described
by Kim (2014) and Zhang and Wallace (2015), the
convolutional layer uses multiple filter region sizes
{2, 3, 4}, each of which has 32 filters, and a rec-
tifier (ReLU) as the activation function. For the
RNN approach, experiments were performed on
the standard RNN as well as its variants: LSTM,



23

BLSTM and GRU. All the recurrent networks use
the hyperbolic tangent as activation functions as it
was considered one of the most common choices
for RNN-type networks (Graves, 2012).

The hyperparameters apart from the above men-
tioned were tuned on the validation set (20% of
the training set) using the hyperparameter tun-
ing library within the framework of Bayesian
optimisation, namely Hyperopt (Bergstra et al.,
2013). Based on the hyperparameter optimi-
sation results, all the networks were trained
with mini-batch stochastic gradient descent us-
ing Nadam (Adam RMSprop with Nesterov mo-
mentum) (Dozat, 2016) with a batch size of 32.
Dropout regularisation was also applied to the
penultimate layer of each network for overfitting
prevention. The resulting optimal values of other
hyperparameters, including the number of hidden
units (hidden), learning rate (lr), dropout rate and
the number of epochs are listed in Table 4.

CNN RNN GRU LSTM BLSTM

hidden 256* 256 256 512 256
lr 0.001 0.002 0.002 0.002 0.004
dropout 0.2 0.3 0.1 0.3 0.5
epochs 15 40 50 45 40

* The number of units in the fully connected layer

Table 4: Hyperparameters estimated by Hyperopt

4 Results and Discussion

4.1 Overall Performance

Results for each deep learning model’s best run
against state-of-the-art models from the 2014
i2b2/UTHealth shared task are listed in Table 5.
Among the deep learning approaches, RNN-type
networks outperformed CNN in the context of
clinical text classification. Although the CNN
model achieved the highest recall, its precision is
far from being competitive, which results in a rel-
atively low F-measure. A comparison between the
RNN-type models shows that BLSTM achieved
the highest micro-averaged F-measure (0.9081) on
the test data, followed closely by GRU and LSTM.
A two-tailed unpaired t-test was also performed
to determine the significance of the difference in
F-measure between the two best-performing net-
works. Over 50 independent training and testing
sessions with different weight initialisation (drawn
from the uniform distribution), the test yielded a
statistically significant difference between the per-

formance of BLSTM (µ = 0.903, σ = 0.002) and
GRU (µ = 0.899, σ = 0.002) with p < 0.05, which
implies that the improvement in performance of
the BLSTM model is also statistically significant
compared with that of other remaining models.

In comparison with the top performing sys-
tems from the previous work, the results reveal
that the BLSTM model without employing any
knowledge-driven approaches ranked in the top
6 systems, and was substantially better than the
overall average (0.815) of all the participating sys-
tems in the shared task. As a universal classi-
fier, the performance of the BLSTM model is aus-
picious since it produced only 0.0195 loss in F-
measure when comparing against the first-ranked
system (Roberts et al., 2015) which involves the
use of a series of SVMs along with a rule-based
classifier and additional annotations. Besides
the best-performing model, the LSTM and GRU
models ranked in the top 7 systems while the
CNN and standard RNN models performed well
within the top 10 systems from the shared task.
This outcome concludes that simple deep learn-
ing models still can rank within the top 10 heavily
feature-engineered best-performing systems from
the shared task.

Model Recall Precision F-score

BLSTM 0.9180 0.8983 0.9081
GRU 0.9091 0.9002 0.9046
LSTM 0.9191 0.8836 0.9010
RNN 0.8956 0.8844 0.8900
CNN 0.9245 0.8383 0.8793

Roberts et al. (2015)* 0.9625 0.8951 0.9276
Chen et al. (2015)* 0.9436 0.9106 0.9268
Torii et al. (2014)* 0.9409 0.8972 0.9185
Cormack et al. (2015)† 0.9375 0.8975 0.9171
Yang and Garibaldi (2014)* 0.9488 0.8847 0.9156
Shivade et al. (2015)† 0.9261 0.8907 0.9081
Chang et al. (2015)* 0.9387 0.8594 0.8973
NCU‡ 0.9256 0.8586 0.8909
Karystianis et al. (2015)† 0.9007 0.8557 0.8776
Khalifa and Meystre (2015)† 0.8951 0.8552 0.8747

* A combination of knowledge- and data-driven approaches (hybrid)
† Knowledge-driven approaches only e.g. lexicon and rules
‡ Unknown (National Central University did not submit a paper)

Table 5: Experimental results and state-of-the-art
systems from 2014 i2b2/UTHealth shared task

4.2 Performance on Individual Risk Factors

Table 6 shows the performance of the deep learn-
ing models on individual risk factors. All five ar-
chitectures achieved micro-averaged F-measures



24

CNN RNN GRU LSTM BLSTM

CAD 0.6553 0.7966 0.7972 0.8010 0.8074
Diabetes 0.9133 0.9227 0.9177 0.9272 0.9171
Obesity 0.8717 0.8739 0.8819 0.8880 0.8880
Hyperlipidemia 0.9154 0.9209 0.9100 0.9243 0.9323
Hypertension 0.8839 0.9093 0.9102 0.9043 0.9187
Medication 0.9075 0.8901 0.9192 0.9090 0.9171
Smoking 0.8350 0.8077 0.8146 0.8152 0.8409
Family history 0.9397 0.9630 0.9572 0.9591 0.9630

Overall 0.8798 0.8900 0.9046 0.9010 0.9081

Table 6: Micro-averaged F-measure for individual risk factor categories (best runs); highest F-measures
for each category are bolded

over 0.87. These deep networks performed best on
the family history category, achieved F-measures
above 0.90 for the hyperlipidemia and diabetes
risk factors, and maintained F-measures over 0.87
for the hypertension and obesity risk factors along
with relevant medications. The worst classifica-
tion performance of all the models was obtained
for the CAD risk factor, followed by the smoking
status.

Among the deep learning models, highest
micro-averaged F-measures for most of the risk
factor categories were achieved by the BLSTM
network while the top performance for the dia-
betes and medication categories were obtained by
the LSTM and GRU networks respectively. Low-
est classification scores for most of the risk fac-
tor categories were achieved by the CNN model,
which implies its inferiority in comparison with
the RNN-type models for extracting cardiac risk
factor information from EMRs. The overall out-
come also reveals that even though the neural net-
work architectures with the integration of recur-
rent units can be potentially applied to this par-
ticular task with higher success rate, the capabil-
ity of the standard RNN is far from being highly
efficient and thus using the gating mechanism as
well and introducing bidirectionality can substan-
tially increase the chance of achieving better per-
formances.

4.3 Performance on Individual Risk Factor
Indicators

The results in Table 7 reveals that phrase-based in-
dicators have comparatively high F-measures in all
models. As the deep learning approach for clinical
concept extraction can be posed as a standard the
named entity recognition task, specific keywords
play a significant role in identifying named enti-
ties and an increase in the predictive performance

is simply due to a tremendous amount of sample
instances in the training data.

In contrast, the logic-based and discourse-based
indicators have substantially lower F-measure. As
both types of indicators infrequently appear in the
training data (see Table 3), the primary cause of
poor performance is likely due to the sparsity and
imbalance of training instances.

CNN RNN GRU LSTM BLSTM

Phrase-
based 0.7679 0.6818 0.7810 0.7342 0.7808

Logic-
based 0.3643 0.1857 0.2185 0.2114 0.2640

Discourse-
based 0.5341 0.4983 0.5425 0.5328 0.5721

Table 7: The average of F-measure performances
across all risk factor indicators for each evidence
type

4.4 Error Analysis

4.4.1 Complex Textual Evidence

Even though phrase-based evidence may vary
(e.g. CAD can appear as “heart disease” or
“CAD”), these phrases along with a sufficiently
large amount of samples are generally enough for
deep neural networks to achieve high classification
accuracy. However, the context of discourse-based
evidence may appear to be as complex as “prob-
able inferior and old anteroseptal myocardial in-
farction” or “Cath (5/88): 3v disease: RCA 90%,
LAD 30% mid, 80% distal, D1 70%, D2 40%
and 60%, LCx 30%, OM2 80%”. The difficulty
of learning the patterns and identifying these in-
dicators implies the need for a higher amount of
training instances and perhaps amended semantic
matching of medical terms to medical terminol-
ogy resources such as the UMLS Metathesaurus



25

(Bodenreider, 2004) or Systematised Nomencla-
ture of Medicine – Clinical Terms (SNOMED
CT) (Stearns et al., 2001), such that information
in EMRs can be more accurately extracted using
deep learning.

4.4.2 Conditional Textual Evidence
Although deep learning requires less human effort
and time than dictionary-based and rule-based ap-
proaches as it can automatically learn the patterns
in data which results in more flexible predictive
power, the experimental results demonstrate the
limitation of such data-driven approach as it is in-
feasible to accurately identify logic-based indica-
tors in the test set without having seen the numbers
and their contexts in the training set. For example,
it is unlikely for deep learning models to classify
the evidence “glucose 420” as the diabetes.glucose
indicator without learning that particular pattern
during training as it is unable to perform compar-
ison during classification whether 420 is greater
than 126 (the glucose level greater than 126 is con-
sidered a risk factor (Stubbs and Uzuner, 2015)).
A decrease in classification accuracy is primarily
due to a massive amount of unforeseen evidence in
the test data i.e. many numbers that imply the risk
of heart disease never appear in the training set.
In this case, utilising dictionaries and rules based
on the domain knowledge would be more optimal
than collecting more data in which every possible
pattern, which may include every number that is
considered a risk factor as well as its context, is
required.

4.4.3 Data Sparsity and Class Imbalance
Figure 5 illustrates the relationship between clas-
sification performance of the BLSTM network2

and the number of training instances in terms of
risk factor indicators. When the number of sam-
ples is low (less than approximately 200 sam-
ples), each network’s performance significantly
varies depending on risk factor indicator. How-
ever, the prediction capability raises and tends to
be more stable as the number of training instance
increases. As many of the machine learning al-
gorithms greatly suffer from insufficient and im-
balanced data where the classes are not equally
presented, it is not surprising if deep learning is

2The relationship between classification performance of
the BLSTM network and the number of training instances
is selected as it is the best-performing model from the ex-
periment and the patterns found among other deep learning
architectures are very similar.

severely impacted by the same problem. Inade-
quate training samples typically result in failure
of pattern recognition while imbalanced classes in
the training set tend to bias the trained models to-
wards more common classes. These non-trivial is-
sues likely explain the relatively poor classifica-
tion results for various risk factor indicators, es-
pecially those that belong to the logic-based and
discourse-based types, due to misclassification of
either indicators or time attributes or both. Re-
garding the report from the 2014 i2b2/UTHealth
risk factor challenge (Stubbs et al., 2015), all the
participating systems also produced similar sets of
results due to these problems.

0 200 400 600 800 1000 1200 1400 1600
Number of Training Instances

0.0

0.2

0.4

0.6

0.8

1.0

F-
m

ea
su

re

Number of Training Instances and F-measure

Risk factor indicator

Figure 5: Effect of training-sample size illustrated
by the relationship between classification perfor-
mance of the BLSTM network and the number of
training instances (risk factor indicator-level)

5 Conclusion

This work empirically evaluated the performance
of different deep learning architectures for identi-
fying risk factors for heart disease in clinical text.
The experimental results showed that the deep
learning approaches were not only comparable to
highly feature-engineered hybrid systems but most
importantly achieved relatively high performances
without the help of any knowledge-driven meth-
ods. The findings leads to an anticipation that
leveraging knowledge-based approaches with the
BLSTM model could potentially provide signifi-
cant performance improvements over best systems
for extracting key cardiac risk factors from EMRs.



26

References
Emelia J Benjamin, Michael J Blaha, Stephanie E Chi-

uve, Mary Cushman, Sandeep R Das, Rajat Deo,
Sarah D de Ferranti, James Floyd, Myriam Fornage,
Cathleen Gillespie, et al. 2017. Heart disease and
stroke statistics 2017 update: a report from the amer-
ican heart association. Circulation, 135(10):e146–
e603.

James Bergstra, Dan Yamins, and David D Cox. 2013.
Hyperopt: A python library for optimizing the hy-
perparameters of machine learning algorithms. In
Proceedings of the 12th Python in Science Confer-
ence, pages 13–20.

Steven Bethard, Guergana Savova, Wei-Te Chen, Leon
Derczynski, James Pustejovsky, and Marc Verha-
gen. 2016. SemEval-2016 task 12: Clinical Tem-
pEval. In Proceedings of the 10th International
Workshop on Semantic Evaluation (SemEval-2016),
pages 1052–1062.

Olivier Bodenreider. 2004. The unified medical lan-
guage system (UMLS): integrating biomedical ter-
minology. Nucleic Acids Research, 32(1):D267–
D270.

Nai-Wen Chang, Hong-Jie Dai, Jitendra Jonnagaddala,
Chih-Wei Chen, Richard Tzong-Han Tsai, and Wen-
Lian Hsu. 2015. A context-aware approach for pro-
gression tracking of medical concepts in electronic
medical records. Journal of Biomedical Informat-
ics, 58:S150–S157.

Qingcai Chen, Haodi Li, Buzhou Tang, Xiaolong
Wang, Xin Liu, Zengjian Liu, Shu Liu, Weida Wang,
Qiwen Deng, Suisong Zhu, et al. 2015. An auto-
matic system to identify heart disease risk factors in
clinical texts over time. Journal of Biomedical In-
formatics, 58:S158–S163.

Yu Cheng, Fei Wang, Ping Zhang, and Jianying
Hu. 2016. Risk prediction with electronic health
records: A deep learning approach. In Proceedings
of the 2016 SIAM International Conference on Data
Mining, pages 432–440. SIAM.

Veera Raghavendra Chikka. 2016. CDE-IIITH at
SemEval-2016 task 12: Extraction of temporal in-
formation from clinical documents using machine
learning techniques. In Proceedings of the 10th
International Workshop on Semantic Evaluation
(SemEval-2016), pages 1237–1240.

Kyunghyun Cho, Bart Van Merriënboer, Dzmitry Bah-
danau, and Yoshua Bengio. 2014. On the properties
of neural machine translation: Encoder-decoder ap-
proaches. arXiv preprint arXiv:1409.1259.

Ronan Collobert, Jason Weston, Léon Bottou, Michael
Karlen, Koray Kavukcuoglu, and Pavel Kuksa.
2011. Natural language processing (almost) from
scratch. Journal of Machine Learning Research,
12:2493–2537.

James Cormack, Chinmoy Nath, David Milward,
Kalpana Raja, and Siddhartha R Jonnalagadda.
2015. Agile text mining for the 2014 i2b2/UTHealth
cardiac risk factors challenge. Journal of Biomedi-
cal Informatics, 58:S120–S127.

Timothy Dozat. 2016. Incorporating nesterov momen-
tum into adam. 4th International Conference on
Learning Representations (ICLR 2016).

Jeffrey L Elman. 1990. Finding structure in time. Cog-
nitive science, 14(2):179–211.

Jason Alan Fries. 2016. Brundlefly at SemEval-2016
task 12: Recurrent neural networks vs. joint infer-
ence for clinical temporal information extraction.
arXiv preprint arXiv:1606.01433.

G Gonzalez-Hernandez, A Sarker, K O’Connor, and
G Savova. 2017. Capturing the patients perspective:
a review of advances in natural language processing
of health-related text. Yearbook of Medical Infor-
matics, 26(01):214–227.

Alex Graves. 2012. Supervised sequence labelling with
recurrent neural networks. Springer, Berlin, Heidel-
berg.

Sepp Hochreiter and Jürgen Schmidhuber. 1997.
Long short-term memory. Neural computation,
9(8):1735–1780.

Abhyuday N Jagannatha and Hong Yu. 2016. Struc-
tured prediction models for rnn based sequence la-
beling in clinical text. In Proceedings of the Con-
ference on Empirical Methods in Natural Language
Processing. Conference on Empirical Methods in
Natural Language Processing, volume 2016, page
856.

George Karystianis, Azad Dehghan, Aleksandar Ko-
vacevic, John A Keane, and Goran Nenadic. 2015.
Using local lexicalized rules to identify heart disease
risk factors in clinical notes. Journal of Biomedical
Informatics, 58:S183–S188.

Abdulrahman Khalifa and Stéphane Meystre. 2015.
Adapting existing natural language processing re-
sources for cardiovascular risk factors identification
in clinical notes. Journal of Biomedical Informatics,
58:S128–S132.

Yoon Kim. 2014. Convolutional neural networks for
sentence classification. Proceedings of the 2014
Conference on Empirical Methods in Natural Lan-
guage Processing (EMNLP), pages 1746–1751.

Yann LeCun, Yoshua Bengio, and Geoffrey Hinton.
2015. Deep learning. Nature, 521(7553):436–444.

Peng Li and Heng Huang. 2016. University of Texas
at Arlington (UTA) with deep learning based natu-
ral language processing (DLNLP) at SemEval-2016
task 12: deep learning based natural language pro-
cessing system for clinical information identifica-
tion from clinical notes and pathology reports. In



27

Proceedings of the 10th International Workshop on
Semantic Evaluation (SemEval-2016), pages 1268–
1273.

Stéphane M Meystre, Guergana K Savova, Karin C
Kipper-Schuler, and John F Hurdle. 2008. Ex-
tracting information from textual documents in the
electronic health record: a review of recent re-
search. IMIA Yearbook of Medical Informatics
2008, 35(128):128–144.

Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey
Dean. 2013a. Efficient estimation of word represen-
tations in vector space. Proceedings of the Inter-
national Conference on Learning Representations
(ICLR 2013), pages 1–12.

Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Cor-
rado, and Jeff Dean. 2013b. Distributed representa-
tions of words and phrases and their compositional-
ity. In Advances in Neural Information Processing
Systems, pages 3111–3119.

Kirk Roberts, Sonya E Shooshan, Laritza Rodriguez,
Swapna Abhyankar, Halil Kilicoglu, and Dina
Demner-Fushman. 2015. The role of fine-grained
annotations in supervised recognition of risk factors
for heart disease from EHRs. Journal of Biomedical
Informatics, 58:S111–S119.

Mike Schuster and Kuldip K Paliwal. 1997. Bidirec-
tional recurrent neural networks. IEEE Transactions
on Signal Processing, 45(11):2673–2681.

Benjamin Shickel, Patrick James Tighe, Azra Bihorac,
and Parisa Rashidi. 2017. Deep EHR: A survey of
recent advances in deep learning techniques for elec-
tronic health record (EHR) analysis. IEEE Journal
of Biomedical and Health Informatics.

Jaeho Shin, Sen Wu, Feiran Wang, Christopher De Sa,
Ce Zhang, and Christopher Ré. 2015. Incremen-
tal knowledge base construction using DeepDive.
Proceedings of the VLDB Endowment, 8(11):1310–
1321.

Chaitanya Shivade, Pranav Malewadkar, Eric Fosler-
Lussier, and Albert M Lai. 2015. Comparison of
UMLS terminologies to identify risk of heart disease
using clinical notes. Journal of Biomedical Infor-
matics, 58:S103–S110.

Michael Q Stearns, Colin Price, Kent A Spackman,
and Amy Y Wang. 2001. SNOMED clinical terms:
overview of the development process and project
status. In Proceedings of the AMIA Symposium,
page 662. American Medical Informatics Associa-
tion.

Amber Stubbs, Christopher Kotfila, Hua Xu, and
Özlem Uzuner. 2015. Identifying risk factors
for heart disease over time: Overview of 2014
i2b2/UTHealth shared task track 2. Journal of
Biomedical Informatics, 58:S67–S77.

Amber Stubbs and Özlem Uzuner. 2015. Annotating
risk factors for heart disease in clinical narratives for
diabetic patients. Journal of Biomedical Informat-
ics, 58:S78–S91.

Manabu Torii, Jung-wei Fan, Wei-li Yang, Theodore
Lee, Matthew T Wiley, Daniel Zisook, and Yang
Huang. 2014. De-identification and risk factor de-
tection in medical records. In Seventh i2b2 Shared
Task and Workshop: Challenges in Natural Lan-
guage Processing for Clinical Data.

Hui Yang and Jonathan Garibaldi. 2014. Automatic ex-
traction of risk factors for heart disease in clinical
texts. Proceeding of the i2b2/UTHealth NLP Chal-
lenge.

Ye Zhang and Byron Wallace. 2015. A sensitivity anal-
ysis of (and practitioners’ guide to) convolutional
neural networks for sentence classification. arXiv
preprint arXiv:1510.03820.


