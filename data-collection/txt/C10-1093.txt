Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 824–832,

Beijing, August 2010

824

Automatic Discovery of Feature Sets for Dependency Parsing

Peter Nilsson

Pierre Nugues

Department of Computer Science

Lund University

peter.nilsson.lund@telia.com

Pierre.Nugues@cs.lth.se

Abstract

This paper describes a search procedure
to discover optimal feature sets for depen-
dency parsers. The search applies to the
shift–reduce algorithm and the feature sets
are extracted from the parser conﬁgura-
tion. The initial feature is limited to the
ﬁrst word in the input queue. Then, the
procedure uses a set of rules founded on
the assumption that topological neighbors
of signiﬁcant features in the dependency
graph may also have a signiﬁcant contri-
bution. The search can be fully automated
and the level of greediness adjusted with
the number of features examined at each
iteration of the discovery procedure.

Using our automated feature discovery
on two corpora, the Swedish corpus in
CoNLL-X and the English corpus in
CoNLL 2008, and a single parser system,
we could reach results comparable or bet-
ter than the best scores reported in these
evaluations. The CoNLL 2008 test set
contains, in addition to a Wall Street Jour-
nal (WSJ) section, an out-of-domain sam-
ple from the Brown corpus. With sets of
15 features, we obtained a labeled attach-
ment score of 84.21 for Swedish, 88.11 on
the WSJ test set, and 81.33 on the Brown
test set.

1 Introduction

The selection of relevant feature sets is crucial
to the performance of dependency parsers and
this process is still in large part manual. More-

over, feature sets are speciﬁc to the languages be-
ing analyzed and a set optimal for, say, English
can yield poor results in Chinese. With depen-
dency parsers being applied today to dozens of
languages,
this makes the parametrization of a
parser both a tedious and time-consuming opera-
tion. Incidentally, the advent of machine-learning
methods seems to have shifted the tuning steps in
parsing from polishing up grammar rules to the
optimization of feature sets. And as with the writ-
ing of a grammar, the selection of features is a
challenging task that often requires a good deal of
effort and inspiration.

Most automatic procedures to build feature sets
resort to greedy algorithms. Forward selection
constructs a set by adding incrementally features
from a predetermined superset while backward
elimination removes them from the superset (At-
tardi et al., 2007). Both methods are sometimes
combined (Nivre et al., 2006b). The selection pro-
cedures evaluate the relevance of a candidate fea-
ture in a set by its impact on the overall parsing
score: Does this candidate improve or decrease
the performance of the set?

Greedy search, although it simpliﬁes the design
of feature sets, shows a major drawback as it starts
from a closed superset of what are believed to be
the relevant features. There is a broad consen-
sus on a common feature set including the words
close to the top of the stack or the beginning of the
queue, for the shift–reduce algorithm, but no clear
idea on the limits of this set.

In this paper, we describe an automatic discov-
ery procedure that is not bounded by any prior
knowledge of a set of potentially relevant features.
It applies to the shift–reduce algorithm and the ini-

825

tial feature consists solely of the ﬁrst word of the
queue. The search explores nodes along axes of
the parser’s data structures and the partially built
graph using proximity rules to uncover sequences
of relevant, efﬁcient features. Using this proce-
dure on the Swedish corpus in CoNLL-X and the
English corpus in CoNLL 2008, we built feature
sets that enabled us to reach a labeled attachment
score of 84.21 for Swedish, 88.11 on the Wall
Street Journal section of CoNLL 2008, and 81.33
on the Brown part of it with a set cardinality of 15.

2 Transition-based Parsing

(Covington,

Transition-based methods
2001;
Nivre, 2003; Yamada and Matsumoto, 2003;
Zhang and Clark, 2009) have become a popular
approach in multilingual dependency parsing be-
cause of their speed and performance. Transition-
based methods share common properties and
build a dependency graph from a sequence of ac-
tions, where each action is determined using a fea-
ture function. In a data-driven context, the func-
tion is typically implemented as a classiﬁer and
the features are extracted from the partially built
graph and the parser’s data structures, most often
a queue and a stack.

2.1 Parser Implementation
In this study, we built a parser using Nivre’s al-
gorithm (Nivre, 2003). The parser complexity is
linear and parsing completes in at most 2n + 1 op-
erations, where n is the length of the sentence. Ta-
ble 1 shows the transitions and actions to construct
a dependency graph.

Given a sentence to parse, we used a classiﬁer-
based guide to predict the transition sequence to
apply. At each step, the guide extracts features
from the parser conﬁguration and uses them as in-
put to a classiﬁer to predict the next transition. Be-
fore training the classiﬁcation models, we projec-
tivized the corpus sentences (Kunze, 1967; Nivre
and Nilsson, 2005). We did not attempt to recover
nonprojective sentences after parsing.

2.2 Training and Parsing Procedure
We extracted the features using a gold-standard
parsing of the training set. We organized the clas-
siﬁcation, and hence the feature extraction, as a

Action
Init.
End
LeftArc

RightArc

Reduce
Shift

Parser conﬁguration
(cid:1)nil,W, /0(cid:2)
(cid:1)S,nil,G(cid:2)
(cid:1)n|S,n(cid:3)|Q,G(cid:2) →
(cid:1)n|S,n(cid:3)|Q,G(cid:2) →
(cid:1)n|S,Q,G(cid:2) → (cid:1)S,Q,G(cid:2)
(cid:1)S,n|Q,G(cid:2) → (cid:1)n|S,Q,G(cid:2)

(cid:1)S,n(cid:3)|Q,G∪{(cid:1)n(cid:3),n(cid:2)}(cid:2)
(cid:1)n(cid:3)|n|S,Q,G∪{(cid:1)n,n(cid:3)(cid:2)}(cid:2)

Table 1: Parser transitions (Nivre, 2003). W is
the input, G, the graph, S, the stack, and Q, the
queue. The triple (cid:1)S,Q,G(cid:2) represents the parser
conﬁguration and n, n (cid:3), and n(cid:3)(cid:3) are lexical tokens.
(cid:1)n(cid:3),n(cid:2) represents an arc from n(cid:3) to n.

two-step process. The ﬁrst step determines the ac-
tion among LeftArc, RightArc, Reduce, and Shift;
the second one, the grammatical function, if the
action is either a left arc or a right arc.

Once the features are extracted, we train the
corresponding models that we apply to the test
corpus to predict the actions and the arc labels.

3 Feature Discovery

We designed an automatic procedure to discover
and select features that is guided by the structure
of the graph being constructed. The search al-
gorithm is based on the assumption that if a fea-
ture makes a signiﬁcant contribution to the parsing
performance, then one or more of its topological
neighbors in the dependency graph may also be
signiﬁcant. The initial state, from which we de-
rive the initial feature, consists of the ﬁrst word in
the queue. There is no other prior knowledge on
the features.

3.1 Node Attributes

In the discovery procedure, we considered the
nodes of four data structures: the queue, the stack,
the sentence, and the graph being constructed.
We extracted three attributes (or ﬁelds) from each
node:
two static ones, the lexical value of the
node and its part of speech, and a dynamic one
evaluated at parse time: the dependency label of
the arc linking the node to its head, if it exists.
We denoted the attributes of node w, respectively,

826

LEX(w), POS(w), and DEP(w). These attributes
are used as input by most dependency parsers,
whatever the language being parsed.

3.2 Search Axes
The feature search covers three different axes: the
parser’s data structures – the queue and the stack
–, the graph being constructed, and the sentence.
Given a feature set at step n of the discovery pro-
cedure, we deﬁned a successor function that gen-
erates the set of topological neighbors of all the
members in the feature set along these three axes.
For a particular feature:
The data structure axis consists of the nodes in
the stack and the queue. The immediate
neighbors of a node in the stack are the ad-
jacent nodes above and below. In the queue,
these are the adjacent nodes before and af-
ter it. The top node on the stack and the
next node in the queue have a special con-
nection, since they are the ones used by the
parser when creating an arc. Therefore, we
considered them as immediate neighbors to
each other. For a node that is neither in the
stack, nor in the queue, there is no connec-
tion along this axis.
The graph axes traverse

partially

the

con-
structed graph horizontally and vertically.
The horizontal axis corresponds
to the
sibling nodes connected by a common head
(Figure 1). The immediate neighbors of a
node are its nearest siblings to the left and
to the right. The vertical axis corresponds
to the head and child nodes. The immediate
neighbors are the head node as well as the
leftmost and rightmost child nodes. There is
no connection for nodes not yet part of the
graph.

The sentence axis traverses the nodes in the or-
der they occur in the original sentence. The
immediate neighbors of a node are the previ-
ous and next words in the sentence.

4 Representing Features and Their

Neighbors

We represented features with a parameter format
partly inspired by MaltParser (Nivre et al., 2006a).

Vertical axis

Head

Left sibling

Right sibling

CN

Horizontal axis

Leftmost child

Rightmost child

Figure 1: The vertical and horizontal axes, respec-
tively in light and dark gray, relative to CN.

Each parameter consists of two parts. The ﬁrst
one represents a node in a data structure (STACK
or QUEUE) and an attribute:
The nodes are identiﬁed using a zero-based in-
dex. Thus STACK1 designates the second
node on the stack.

The attribute of a node is one of part of speech
(POS), lexical value (LEX), or dependency
label (DEP), as for instance LEX(QUEUE0)
that corresponds to the lexical value of the
ﬁrst token in the queue.

The second part of the parameter is an optional
navigation path that allows to ﬁnd other destina-
tion nodes in the graph. It consists of a sequence
of instructions to move from the start node to the
destination node. The list of possible instructions
are:

• h: head of the current node;
• lc/rc: leftmost/rightmost child of the node;
• pw/ f w:
previous/following word of the

node in the original sentence.

An example of a feature obtained using the nav-
igation part is POS(STACK1 lc pw), which is in-
terpreted as: start from STACK1. Then, using the
instructions lc and pw, move to the left child of the
start node and to the previous word of this child in
the sentence. The requested feature is the part of
speech of the destination node.

827

5 Initial State and Successor Function

The feature discovery is an iterative procedure that
grows the feature set with one new feature at each
iteration. We called generation such an iteration,
where generation 1 consists of a single node. We
denoted FeatSeti = { f1, f2, ..., fi} the feature set
obtained at generation i.
Although the features of a classiﬁer can be
viewed as a set, we also considered them as a tu-
ple, where Feati = (cid:1) f1, f2, ..., fi(cid:2) is the i-tuple at
generation i and fk, the individual feature discov-
ered at generation k with 1 (cid:1) k (cid:1) i. This enables
us to keep the order in which the individual fea-
tures are obtained during the search.

Initial State

5.1
We start the feature search with the empty set, /0,
that, by convention, has one neighbor:
the ﬁrst
node in the queue QUEUE0. We chose this node
because this is the only one which is certain to
exist all along the parsing process.
Intuitively,
this is also obvious that QUEUE0 plays a signiﬁ-
cant role when deciding a parsing action. We de-
ﬁned the successor function of the empty set as:
SUCC(/0) = {POS(QUEUE0),LEX(QUEUE0)}.
5.2 Successors of a Node
The successors of a node consist of itself and all
its topological neighbors along the three axes with
their three possible attributes: part of speech, lex-
ical value, and dependency label. For a particular
feature in FeatSet, the generation of its successors
is carried out through the following steps:

1. Interpret the feature with its possible naviga-
tion path and identify the destination node n.
2. Find all existing immediate neighboring

nodes of n along the three search axes.

3. Assign the set of attributes – POS, LEX, and

DEP – to n and its neighboring nodes.

If at any step the requested node does not exist,
the feature evaluates to NOT HING.

5.3 Rules to Generate Neighbors
The generation of all the neighbors of the features
in FeatSet may create duplicates as a same node
can sometimes be reached from multiple paths.

For instance, if we move to the leftmost child of a
node and then to the head of this child, we return
to the original node.

To compute the successor function, we built a
set of rules shown in Table 2. It corresponds to
a subset of the rules described in the axis search
(Sect. 3.2) so that it omits the neighbors of a node
that would unavoidably create redundancies. The
third column in Table 2 shows the rules to gener-
ate the neighbors of POS(QUEUE0). They corre-
spond to the rows:

PL. This stands for the POS and LEX attributes
of the node. We only add LEX(QUEUE0)
as we already have POS(QUEUE0).

PLD lc and PLD rc. POS, LEX, and DEP of the

node’s leftmost and rightmost children.

PLD pw. POS, LEX, and DEP of the previous
word in the original string. The following
word is the same as the next node in the
queue, which is added in the next step. For
that reason, following word is not added.

PL QUEUE1. POS and LEX of QUEUE1.
PLD STACK0. POS, LEX, and DEP of STACK0.
This rule connects the queue to the top node
of the stack.

Table 3 summarizes the results of the rule appli-
cation and shows the complete list of successors
of POS(QUEUE0). In this way, the search for a
node’s neighbors along the axes is reduced to one
direction, either left or right, or up or down, that
will depend on the topological relation that intro-
duced the node in the feature set.

6 Feature Selection Algorithm

At each generation, we compute the Cartesian
product of the current feature tuple Feati and the
set deﬁned by its neighbors. We deﬁne the set of
candidate tuples CandFeati+1 at generation i + 1
as:

CandFeati+1 = {Feati}× SUCC(Feati),

have
where
Card(SUCC(Feati)).

we

Card(CandFeati+1) =

The members of CandFeati+1 are ranked ac-
cording to their parsing score on the development

828

Data structures

Navigation paths

STACK0
PLD
PLD h
PLD lc
PLD rc
PLD ls
PLD rs
PLD pw
PLD fw
PLD STACK1
PL QUEUE0

STACKn,n > 0
PLD
PLD h
PLD lc
PLD rc
PLD ls
PLD rs
PLD pw
PLD fw
PLD STACKn+1

QUEUE0
PL

PLD lc
PLD rc

PLD pw

PL QUEUE1
PLD STACK0

QUEUEn,n > 0
PL

PL QUEUEn+1

lc, rc

h
h

lc
rc
ls
ls
rs
rs
pw pw
fw fw

ls

rs

lc
rc

lc
rc
ls

pw f w
h
lc
rc
ls
rs
pw pw pw
fw fw

h
lc
rc
ls
rs

fw

rs

Table 2: Rules to compute the successors of a node. For each node category given in row 2, the
procedure adds the features in the column headed by the category. PLD stands for the POS, LEX,
and DEP attributes. In the right-hand side of the table, the category corresponds to the last instruction
of the navigation path, if it exists, for instance pw in the feature POS(STACK1 lc pw). We read the
six successors of this node in the ﬁfth column headed by pw: STACK1 lc pw h, STACK1 lc pw lc,
STACK1 lc pw rc, STACK1 lc pw ls, STACK1 lc pw rs, and STACK1 lc pw pw. We then apply all the
attributes to these destination nodes to generate the features.

Initial feature
Successors

POS QUEUE 0
LEX QUEUE 0
PLD QUEUE 0
PLD QUEUE 0
PLD QUEUE 0
PL
QUEUE 1
PLD STACK 0

lc
rc
pw

Table 3: Features generated by the successor func-
tion SUCC({POS(QUEUE0)}). PLD stands for
the three attributes POS, LEX, and DEP of the
node; PL for POS and LEX.

set and when applying a greedy best-ﬁrst search,
Feati+1 is assigned with the tuple yielding the
highest score:

Feati+1 ← eval best(CandFeati+1).

7 Experimental Setup

In a ﬁrst experiment, we used the Swedish cor-
pus of the CoNLL-X shared task (Buchholz and
Marsi, 2006). In a second experiment, we applied
the feature discovery procedure to the English cor-
pus from CoNLL 2008 (Surdeanu et al., 2008), a
dependency corpus converted from the Penn Tree-
bank and the Brown corpus. In both experiments,
we used the LIBSVM package (Chang and Lin,
2001) with a quadratic kernel, γ = 0.2, C = 0.4,
and ε = 0.1. These parameters are identical to
Nivre et al. (2006b) to enable a comparison of the
scores.

We evaluated the feature candidates on a de-
velopment set using the labeled and unlabeled at-
tachment scores (LAS and UAS) that we com-
puted with the eval.pl script from CoNLL-X.
As there was no development set for the Swedish
corpus, we created one by picking out every 10th
sentence from the training set. The training was
then carried out on the remaining part of the set.

The procedure is repeated with the immediate
neighbors of Feati+1 until the improvement of the
score is below a certain threshold.

8 Feature Discovery on a Swedish

Corpus

We extended this greedy version of the discov-
ery with a beam search that retains the N-best
successors from the candidate set. In our exper-
iments, we used beam widths of 4 and 8.

In a ﬁrst run, the search was optimized for the
UAS. In a second one, we optimized the LAS. We
also report the results we obtained subsequently
on the CoNLL-X test set as an indicator of how

829

well the training generalized.

8.1 The First and Second Generations
Table 4 shows the feature performance at the ﬁrst
generation sorted by UAS. The ﬁrst row shows the
two initial feature candidates, (cid:1)POS(QUEUE0)(cid:2)
and (cid:1)LEX(QUEUE0)(cid:2). The third row shows
the score produced by the initial features alone.
The next rows show the unlabeled and labeled
attachment scores with feature pairs combining
one of the initial features and the one listed in
the row. The combination of POS(QUEUE0)
and POS(STACK0) yielded the best UAS: 74.02.
The second feature improves the performance of
POS(QUEUE0) by more than 30 points from
43.49.

Dev set

For each generation, we applied a beam
search. We kept the eight best pairs as start-
ing states for the second generation and we
added their neighboring nodes. Table 5 shows
the eight best results out of 38 for the pair
(cid:1)POS(QUEUE0),POS(STACK0)(cid:2).
Parent state: (cid:1)POS(QUEUE0),POS(STACK0)(cid:2)
UAS
79.50
78.73
77.42
77.06
76.83
76.63
76.44
76.39

LAS
65.86
64.51 L STACK 0 fw
61.86 L QUEUE 1
62.90 L QUEUE 0 pw
63.77 L QUEUE 0
63.17
62.02 L STACK 0
61.16 L QUEUE 0 lc

UAS
79.07
76.04
74.63
75.28
73.61
74.75
74.09
73.99

LAS
65.34
66.98
63.08
64.54
66.01
63.62
64.24
63.12

Successors
P QUEUE 1

P STACK 0 fw

Test set

5:

the

Ranking

successors

Table
of
the
(cid:1)POS(QUEUE0),POS(STACK0)(cid:2)
Swedish corpus.
Out of the 38 successors,
we show the eight that yielded the best results. P
stands for POS, L for LEX, and D for DEP.

on

8.2 Optimizing the Unlabeled Attachement

Score

We iterated the process over a total of 16 gener-
ations. Table 6, left-hand side, shows the list of
the best scores for each generation. The scores on
the development set increased steadily until gen-

eration 13, then reached a plateau, and declined
around generation 15. The test set closely fol-
lowed the development set with values about 1%
lower. On this set, we reached a peak performance
at generation 12, after which the results decreased.
Table 6, right-hand side, shows the features pro-
ducing the ﬁnal score in their order of inclusion
in the feature set. As we applied a beam search,
a feature listed at generation i does not necessary
correspond to the highest score for this generation,
but belongs to the feature tuple producing the best
result at generation 16.

8.3 Optimizing the Labeled Attachement

Score

We also applied the feature discovery with a
search optimized for the labeled attachment score.
This time, we reduced the beam width used in the
search from 8 to 4 as we noticed that the candi-
dates between ranks 5 and 8 never contributed to
the best scoring feature set for any generation.

We observed a score curve similar to that of the
UAS-optimized search. The train set followed the
development set with increasing values for each
generation but 1-2% lower. The optimal value was
obtained at generation 15 with 84.21% for the test
set. Then, the score for the test set decreased.

9 Feature Discovery on a Corpus of

English

The training and development sets of the CoNLL
2008 corpus contain text from the Wall Street
Journal exclusively. The test set contains text
from the Brown corpus as well as from the Wall
Street Journal. Table 7 shows the results after 16
generations. We used a beam width of 4 and the
tests were optimized for the unlabeled attachment
score. As for Swedish, we reached the best scores
around generation 14-15. The results on the in-
domain test set peaked at 90.89 and exceeded the
results on the development set. As expected, the
results for the out-of-domain corpus were lower,
87.50, however the drop was limited to 3.4.

10 Discussion and Conclusion

The results we attained with feature set sizes as
small as 15 are competitive or better than ﬁgures

830

Parent state
UAS
43.49

LAS
26.45 None

(cid:1)POS(QUEUE0)(cid:2)
Successors

(cid:1)LEX(QUEUE0)(cid:2)
LAS
Successors
23.56 None

UAS
42.76

74.02
67.77
58.37
55.28
51.53
51.05
49.71
49.49
49.37
48.68
48.47
46.77
46.40
42.27
41.04

65.86
59.67 POS STACK 0
58.59
54.50 LEX STACK 0
51.98
41.83 POS QUEUE 0 pw
38.49 LEX QUEUE 0 pw 50.44
30.43 POS QUEUE 1
50.38
49.37
32.66 LEX QUEUE 0 lc
31.54 POS QUEUE 0 lc
48.91
48.66
29.18 LEX QUEUE 1
47.25
32.27 LEX QUEUE 0
47.09
29.34 DEP STACK 0
46.68
30.84 LEX QUEUE 0 rc
45.69
26.86 DEP QUEUE 0 lc
44.77
29.95 POS QUEUE 0 rc
25.21 DEP QUEUE 0 pw 44.43
41.87
26.56 DEP QUEUE 0 rc

52.18 POS STACK 0
45.51 LEX STACK 0
37.70 POS QUEUE 0 pw
29.71 POS QUEUE 1
35.24 LEX QUEUE 0 pw
32.27 POS QUEUE 0
27.77 LEX QUEUE 1
29.91 LEX QUEUE 0 lc
28.92 LEX QUEUE 0 rc
28.65 POS QUEUE 0 lc
27.08 DEP QUEUE 0 lc
27.83 POS QUEUE 0 rc
26.17 DEP STACK 0
26.47 DEP QUEUE 0 rc
23.04 DEP QUEUE 0 pw

Table 4: Results of the beam search on the Swedish corpus at the ﬁrst generation with the two initial
feature candidates, (cid:1)POS(QUEUE0)(cid:2) and (cid:1)LEX(QUEUE0)(cid:2), respectively on the left- and right-hand
side of the table. The third row shows the score produced by the initial features alone and the next rows,
the ﬁgures for the candidate pairs combining the initial feature and the successor listed in the row. The
eight best combinations shown in bold are selected for the next generation.

Generation

Dev set

Test set

Features

1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16

UAS
43.49
74.02
79.50
83.58
85.96
87.23
88.42
89.43
89.84
90.23
90.49
90.73
90.81
90.81
90.85
90.84

LAS UAS
45.93
26.45
71.60
59.67
65.34
79.07
82.75
71.76
84.82
76.03
77.32
86.34
87.67
80.00
88.09
81.56
88.69
83.20
83.89
89.17
89.58
84.31
89.66
84.47
89.52
84.60
84.70
89.32
89.13
84.67
84.68
88.65

POS QUEUE 0
POS STACK 0
POS QUEUE 1
LEX STACK 0 fw
LEX STACK 0
LEX QUEUE 0 lc
POS STACK 1
LEX QUEUE 1
LEX QUEUE 0

LAS
30.19
58.37
65.86
70.98
74.75
76.52
78.99
80.26
82.33
83.31 DEP STACK 0 lc
83.85
POS STACK 0 fw
LEX STACK 0 fw ls
83.83
LEX STACK 0 fw ls lc
83.75
83.73
POS STACK 1 h
LEX STACK 1 rs
83.21
82.75
POS STACK 0 fw ls rc

Table 6: Best results for each generation on the Swedish corpus, optimized for UAS. Figures in bold
designate the best scores. The right-hand side of the table shows the feature sequence producing the
best result at generation 16.

831

Generation

Dev set

1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16

UAS
45.25
64.42
78.62
81.83
84.43
85.95
86.95
88.03
88.61
89.09
89.54
89.95
90.26
90.54
90.61
90.65

Test set WSJ
LAS
34.49
56.44
70.30
77.82
80.88
82.93
84.09
84.74
86.20
86.60
87.40
87.77
87.80
87.88
88.11
88.09

LAS UAS
45.82
33.77
64.71
55.64
68.77
78.99
82.46
76.67
84.89
79.78
81.60
86.61
87.73
82.73
88.52
83.62
89.15
84.97
85.43
89.47
90.25
85.87
90.63
86.21
90.64
86.56
86.81
90.71
90.89
86.94
87.00
90.88

Test set Brown
LAS
UAS
40.70
52.12
62.41
71.29
78.67
65.17
72.95
80.57
76.99
84.03
84.55
77.80
78.48
85.26
78.73
85.66
79.86
86.29
86.43
80.02
80.75
87.00
80.46
86.87
80.86
87.35
87.50
81.30
81.33
87.47
87.42
81.28

Features

POS QUEUE 0
LEX STACK 0
POS QUEUE 1
LEX STACK 0 fw
POS STACK 0
DEP QUEUE 0 lc
LEX STACK 1
LEX QUEUE 1
LEX QUEUE 0
POS QUEUE 2
POS STACK 0 pw
POS QUEUE 3
POS STACK 1 pw
POS QUEUE 0 pw
LEX STACK 0 lc
POS STACK 0 pw ls

Table 7: Best results for each generation. English corpus. Selection optimized for UAS.

reported by state-of-the-art transition-based sys-
tems. We reached a UAS of 89.66 on the CoNLL-
X Swedish corpus. On the same corpus, the top
scores reported in the shared task were slightly
lower: 89.54 and 89.50. Our best LAS was 84.21,
and the two best scores in CoNLL-X were 84.58
and 82.55. Our results for the English corpus from
CoNLL 2008 were optimized for an unlabeled at-
tachment score and we obtained 90.89 for the in-
domain test set and 87.50 for the out-of-domain
one. Our best LAS were 88.11 and 81.33. Ofﬁcial
results in CoNLL 2008 only reported the labeled
attachment scores, respectively 90.13 and 82.811.
We believe these results remarkable. We used a
single-parser system as opposed to ensemble sys-
tems and the results on the Brown corpus show
an excellent resilience and robustness on out-of-
domain data. The automatic discovery produced
results matching or exceeding comparable sys-
tems, although no prior knowledge of the lan-
guage being analyzed was used and no feature set
was provided to the parser.

Although, a systematic search requires no in-
tuitive guessing, it still consumes a considerable

1Results are not exactly comparable as we used the
CoNLL-X evaluation script that gives slightly higher ﬁgures.

machine time. Due to the learning algorithm we
use, SVM, training a model takes between 1 and
130 hours depending on the size of the corpus.
The number of models to train at each generation
corresponds to the number of feature candidates
times the beam width. The ﬁrst generation con-
tains about 15 feature candidates per feature set
and since features are only added, the number of
candidates can grow to 100 at generation 10.

We believe there is a margin for improvement
both in the parsing scores and in the time needed
to determine the feature sets. Our scores in Swed-
ish were obtained with models trained on 90% of
the training set. They could probably be slightly
improved if they had been trained on a com-
plete set. In our experiments, we used three at-
tributes: the part of speech, lexical value, and de-
pendency label of the node. These attributes could
be extended to lemmas and grammatical features.
SVMs yield a high performance, but they are slow
to train. Logistic regression with, for instance,
the LIBLINEAR package (Fan et al., 2008) would
certainly reduce the exploration time.

832

Nivre, Joakim, Johan Hall, Jens Nilsson, G¨ulsen
Eryigit, and Svetoslav Marinov. 2006b. Labeled
pseudo-projective dependency parsing with support
vector machines. In Proceedings of the Tenth Con-
ference on Computational Natural Language Learn-
ing (CoNLL), pages 221–225, New York, June, 8-9.

Nivre, Joakim.

2003. An efﬁcient algorithm for
projective dependency parsing.
In Proceedings of
the 8th International Workshop on Parsing Tech-
nologies (IWPT 03), pages 149–160, Nancy, 23-25
April.

Surdeanu, Mihai, Richard Johansson, Adam Meyers,
2008. The
Llu´ıs M`arquez, and Joakim Nivre.
CoNLL 2008 shared task on joint parsing of syn-
tactic and semantic dependencies. In CoNLL 2008:
Proceedings of the 12th Conference on Computa-
tional Natural Language Learning, pages 159–177,
Manchester, August.

Yamada, Hiroyasu and Yuji Matsumoto. 2003. Sta-
tistical dependency analysis with support vector
machines.
the 8th Interna-
tional Workshop on Parsing Technologies (IWPT
03), pages 195–206, Nancy, 23-25 April.

In Proceedings of

Zhang, Yue and Stephen Clark. 2009. Transition-
based parsing of the Chinese treebank using a global
discriminative model.
In Proceedings of the 11th
International Conference on Parsing Technologies
(IWPT 09), pages 162–171, Paris, October.

Acknowledgments

The research leading to these results has received
funding from the European community’s seventh
framework program FP7/2007-2013, challenge 2,
cognitive systems,
robotics, under
grant agreement No 230902—ROSETTA.

interaction,

References
Attardi, Giuseppe, Felice Dell’Orletta, Maria Simi,
Atanas Chanev, and Massimiliano Ciaramita. 2007.
Multilingual dependency parsing and domain adap-
tation using DeSR.
In Proceedings of the CoNLL
Shared Task Session of EMNLP-CoNLL 2007, pages
1112–1118, Prague, Czech Republic, June. Associ-
ation for Computational Linguistics.

Buchholz, Sabine and Erwin Marsi. 2006. CoNLL-
X shared task on multilingual dependency parsing.
In Proceedings of the Tenth Conference on Com-
putational Natural Language Learning (CoNLL-X),
pages 149–164, New York City, June. Association
for Computational Linguistics.

Chang, Chih-Chung and Chih-Jen Lin. 2001. LIB-
SVM: a library for support vector machines.
Software available at http://www.csie.ntu.
edu.tw/˜cjlin/libsvm.

Covington, Michael A. 2001. A fundamental algo-
rithm for dependency parsing. In Proceedings of the
39th Annual ACM Southeast Conference, Athens,
Georgia.

Fan, Rong-En, Kai-Wei Chang, Cho-Jui Hsieh, Xiang-
Rui Wang, and Chih-Jen Lin. 2008. LIBLINEAR:
A library for large linear classiﬁcation. Journal of
Machine Learning Research, 9:1871–1874.

Kunze, J¨urgen.

1967.

Die Behandlung nicht-
projektiver Strukturen bei der syntaktischen Anal-
yse und Synthese des englischen und des deutschen.
In MASPEREVOD-67: Internationales Symposium
der Mitgliedsl¨ander des RGW, pages 2–15, Bu-
dapest, 10.–13. Oktober.

2005.
Pseudo-
Nivre, Joakim and Jens Nilsson.
In Proceedings
projective dependency parsing.
of
the Association
for Computational Linguistics (ACL’05), pages 99–
106, Ann Arbor, June.

the 43rd Annual Meeting of

Nivre, Joakim, Johan Hall, and Jens Nilsson. 2006a.
Maltparser: A data-driven parser-generator for de-
pendency parsing.
In Proceedings of the ﬁfth in-
ternational conference on Language Resources and
Evaluation (LREC2006), pages 2216–2219, Genoa,
May 24-26.

