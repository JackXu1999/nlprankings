















































Labeling Unlabeled Data using Cross-Language Guided Clustering


Proceedings of the 5th International Joint Conference on Natural Language Processing, pages 383–391,
Chiang Mai, Thailand, November 8 – 13, 2011. c©2011 AFNLP

Labeling Unlabeled Data using Cross-Language Guided Clustering

Sachindra Joshi
IBM Research

New Delhi, India
jsachind@in.ibm.com

Danish Contractor
IBM Research

New Delhi, India
dcontrac@in.ibm.com

Sumit Negi
IBM Research

New Delhi, India
sumitneg@in.ibm.com

Abstract

The effort required to build a classifier for
a task in a target language can be signifi-
cantly reduced by utilizing the knowledge
gained during an earlier effort of model
building in a source language for a sim-
ilar task. In this paper, we investigate
whether unlabeled data in the target lan-
guage can be labeled given the availabil-
ity of labeled data for a similar domain in
the source language. We view the problem
of labeling unlabeled documents in the tar-
get language as that of clustering them
such that the resulting partitioning has the
best alignment with the classes provided
in the source language. We develop a
cross language guided clustering (CLGC)
method to achieve this. We also pro-
pose a method to discover concept map-
ping between languages which is utilized
by CLGC to transfer supervision across
languages. Our experimental results show
significant gains in the accuracy of label-
ing documents over the baseline methods.

1 Introduction

The last few years have seen a rapid growth in the
development of machine learning applications for
non-English languages. This growth can be at-
tributed to several factors such as increased Inter-
net penetration (especially in non-English speak-
ing countries) and wide adoption of Unicode stan-
dards that allow people to generate content in their
own language.

A key guiding principal in the development of
such applications for a new language (referred to
as the target or resource-poor language) has been
to leverage the existing models and linguistic re-

sources available for a popular language such as
English (also called source or resource-rich lan-
guage). Existing literature examines two ways
of utilizing this knowledge. The first way is to
adapt an existing statistical model for a new tar-
get language. Examples of this is the problem
of cross-lingual sentiment classification (Xiaojun
Wan 2009), or in a more general setting for cross
language domain adaptation for classification (Pe-
ter Prettenhofer and Benno Stein 2010). The sec-
ond way is to develop linguistic resources for a
target or resource-poor language by leveraging the
resources available in a source or resource-rich
language. An example of this is the work done for
automatically transferring syntactic relations (in
WordNet) from a source language (English) into a
target language (Romanian) (Verginica Barbu Mi-
titelu and Radu Ion 2005).

In this paper, we investigate another way of uti-
lizing the knowledge gained in one language for
building machine learning applications in an an-
other language. Our work focuses on generating
training data (in contrast to adapting models and
language resources) in the target language, given
in-domain training data for the source language.
The labeled data in the source language could be
used to guide the grouping of unlabeled data in the
target language, where each group aligns to a class
label from the source language. We assume that
the domain for both the source and target language
data is similar and therefore the set of class labels
across the two languages will be shared (but may
not be exactly the same). As an example consider
a real world scenario from a call routing applica-
tion. A call routing application maps natural lan-
guage utterances (typically a caller’s response to
an open ended question such as “how may I help
you”) to one of a given set of classes also called
call types. Figure 1 shows examples of a few ut-

383



.

Figure 1: Utterances and class labels in source and target languages

terances (in English) along with associated class
labels from the banking domain. These labeled ut-
terances could be used as training data for build-
ing a call-routing classifier for the two class la-
bels namely “Balance-Enquiry” and “Credit-Card-
Enquiry”. Let us assume that we now have ut-
terances in a new language (in Hindi) which are
unlabeled. Given that these utterances belong to
the same domain, they can be labeled using the
same label set as the one used for the source lan-
guage. This is shown in the Figure 1 where ut-
terances h.1 and h.2 are grouped together and la-
beled as “Balance-Enquiry” and utterance h.3 and
h.4 is labeled as “Credit-Card-Enquiry”. The la-
beled data can then be used to train a classifier in
the target language.

To label the target language documents au-
tomatically we propose a method called cross-
language guided clustering (CLGC). This method
is built upon a recently proposed approach called
cross guided clustering (CGC). CGC guides clus-
tering of documents in a target domain given clus-
ters/classes in a source domain (Bhattacharya et al.
2009). This is achieved by discovering a partition-
ing in the target domain that is most “similar” or
“aligned” to a given partitioning in the source do-
main. In CLGC we view the problem of labeling
unlabeled documents in the target language as that
of clustering them such that the resulting partition-
ing has the best alignment with the classes pro-
vided in the source language. Since in our case the
source and target data are in different languages,
we extend the CGC framework to transfer supervi-

sion across different languages. We develop cross
language similarity measures that use word level
and concept level mappings to guide the clustering
across languages. We also develop methods to dis-
cover concept level mapping between languages.
Our experimental results show significant gains in
the accuracy of labeling documents over the base-
line methods.

One could argue that if the final goal is to clas-
sify documents in the target language, this could
be achieved by either of the following approaches -
(1) by adapting the source language classifier (Pe-
ter Prettenhofer and Benno Stein 2010) or (2) by
translating unlabeled documents from the target
language to the source language and then apply-
ing a source language classifier (Mckeown et al.
2003). We claim that our approach is more gen-
eral and has several advantages over both these
approaches. First, building a classifier given a
training dataset is a well studied and understood
problem. Several off-the-shelf machine learning
tools exist that can readily be used for tasks such
as feature construction, and building classifiers,
provided a training dataset is available (Hall et
al. 2009). Our approach can be used to gener-
ate a training dataset for the target language which
enables use of existing approaches not only for
building classifiers, but also for feature engineer-
ing tasks such as feature construction and feature
selection. This cannot be done using either of the
above mentioned approaches.

Second, a key assumption made in both these
approaches is that the class labels across languages

384



are completely shared. This may not be true in
several cases as there could be categories that are
specific to the target language dataset. As an ex-
ample, while most of the Hindi utterances in the
Figure 1 can be grouped and aligned with a class
label in the source language, there exist utterances
(h.5,h.6) which do not belong to any of the ex-
isting labels in the source language. Our method
allows such groupings to be discovered which can
then be used to build target language specific class
labels. Moreover, it is worth mentioning that
apart from these advantages our proposed method
is more efficient than machine translation based
methods as it does not require a complete machine
translation system.

The specific contributions made by us in this pa-
per are two fold. First, we introduce the problem
of labeling documents in one language using the
set of labeled documents in another language and
show that it is not only feasible but also better than
other competitor techniques. Second, we extend
the CGC framework to transfer supervision across
languages. For this we develop methods to dis-
cover concept level mapping between languages
that is utilized to guide the clustering across lan-
guages.

The rest of the paper is organized as follows.
In Section 2 we present related work. We for-
mulate the problem in Section 3. We describe
the cross-language guided clustering framework in
Section 4. In Section 5, we describe the cross lan-
guage similarity measure that is used in the CLGC
framework. We provide the experimental results
in Section 6 and conclude in Section 7.

2 Prior Work

The two research areas that are related to our work
are, (1) cross lingual classification and clustering,
and (2) semi-supervised clustering.

Cross Lingual Classification and Clustering :
Traditional approaches to cross language text clas-
sification use linguistic resources such as bilin-
gual dictionaries or parallel corpora to induce
correspondences between two languages (Olsson
2005). Some of these methods employ latent se-
mantic analysis (LSA) (Dumais et.al. 1997) or
kernel canonical correlation analysis, CCA (For-
tuna and Shawe-Taylor 2005). The major limita-
tions of these approaches are their computational
complexity and dependence on a parallel corpus.
Cross-lingual clustering aims to cluster a hetero-
geneous (a collection of documents from different

languages) document collection. Initial work done
in cross-lingual document clustering employed an
expensive machine translation (MT) system to fill
the gap between two languages (Mckeown et al.
2003). Later work (Wu 2007) done in this area
demonstrated that it was possible to achieve com-
parable performance to the direct MT method us-
ing simple linguistic resource such as bilingual
dictionaries.

Semi-supervised clustering: Semi-supervised
clustering aims to improve clustering performance
by limited supervision in the form of a small set
of labeled instances. Alternatively, a small set
of labeled instances can be used to learn a pa-
rameterized distance function (M. Bilenko and R.
J. Mooney 2003), (Klein et al. 2002). The co-
clustering approach (Dhillon et al. 2003), (N.
Slonim and N. Tishby 2000) clusters related di-
mensions simultaneously through explicitly pro-
vided relations between them, such as words and
documents, or people and reviews.

The problem that we address in this paper dif-
fers significantly from the above mentioned work.
Unlike others, our objective is to cluster target lan-
guage documents such that the resulting clusters
are most ‘similar’ or best ‘aligned’ to the given
source language classes. This problem is an in-
stance of semi-supervised clustering in a bilingual
setting, which to the best of our best knowledge
has received very little attention. Our work builds
upon Cross Guided Clustering (CGC) work (Bhat-
tacharya et al. 2009) where supervision is discov-
ered in the form of cluster level similarities ob-
tained from labeled instances from a different do-
main, having different but related labels. In our
work we extend the CGC framework to transfer
supervision across different languages.

3 Problem Formulation

Let TS = {< dS1 , lS1 >,< dS2 , lS2 >, . . . , <
dSn , l

S
n >} denote a training dataset in the source

language S for a classification task γ. Here dSi ∈
DS denotes a document that has an associated
class label lSi ∈ LS where, LS denotes the set of
class labels used in TS . Note, that LS induces a
partitioning of DS , where each class label lSi can
be seen as a cluster containing documents dSi that
have lSi as the class label. We are also given a set
of unlabeled documents DT = {dT1 , dT2 , . . . , dTm}
where all the documents are from a similar domain
as in TS but are from a different language T . Our
objective is to generate a training dataset usingDT

385



for the classification task γ. We pose this as a
clustering problem over document set DT , where
the resulting clusters are aligned with the given
classes in the source language dataset. The align-
ment is achieved by taking the supervision from
the partitioning of DS , which is induced by the la-
bel set LS , to guide the clustering of document set
DT . We refer to this clustering method as cross-
language guided clustering. In the next section,
we describe cross-language guided clustering in
detail.

4 Cross-Language Guided Clustering

In this section, we modify the cross guided clus-
tering framework as described in (Bhattacharya et
al. 2009) to transfer supervision across languages.
Let Dis(dTi , d

T
j ) provide a distance measure be-

tween documents dTi and d
T
j in the target lan-

guage T . A clustering method partitions the given
document set into k clusters denoted by centroids
CT = {CT1 , CT2 , . . . , CTk } such that the total di-
vergence Div(CT ) also referred to as target only
divergence is minimized. This is defined as fol-
lows.

Div
T
(C

T
) =

∑

CT
i

∑

dT
j

δ(C
T
i , d

T
j )Dis(C

T
i , d

T
j )

2 (1)

Here δ(CTi , d
T
j ) returns 1 if d

T
j is assigned to

the centroid CTi else returns 0. This is a standard
formulation used in the K-Means algorithm (Hall
et al. 2009).

In our problem setting, we are additionally pro-
vided with a labeled dataset in the source lan-
guage where the label set induces a partitioning
CS = {CS1 , CS2 , . . . , CSl } ofDS in the source lan-
guage. Our objective is to discover partitioning
of DT such that each resulting cluster is aligned
with at most one class label from the source lan-
guage and vice-versa. This enables discovery of
clusters in the target language that are aligned
with the classes in the source language while si-
multaneously allowing for discovery of any ad-
ditional concept in the target language. To do
this, we require a cross-language similarity func-
tion SimX(..) that given two documents from dif-
ferent languages, returns a similarity score. This is
non-trivial as documents in different languages are
represented in entirely separate attribute/feature
space. We develop a cross-language similarity
measure to achieve this in Section 5. For now, we
assume that we have access to such a measure.

To find a cross-language alignment between the

source partition and the target partition we con-
struct a bipartite cross language graph Gx that has
one set of vertices CS corresponding to source
centroids, and another set CT corresponding to
target centroids. An edge is added between ev-
ery pair of vertices (CSi , C

T
j ) where the weight of

the edge is given by SimX(CSi , C
T
j ). Now find-

ing the best cross language alignment is equivalent
to finding the maximum weighted bipartite match
in the graph Gx. Recall that a matching is a sub-
set of the edges such that any vertex is spanned by
at most one edge. The score of a matching is the
sum of the weights of all the edges in it. In our
implementation, we use the ‘Hungarian method’
to determine the matching (Kuhn 1955).

The matching provides an alignment between
the source classes and the target clusters. We
only consider those edges in the matching whose
weight is more than some predefined threshold.
To measure the goodness of cross-language align-
ment we define a cross-language divergence mea-
sure:

Div
X
(C

S
, C

T
) =

∑

CS
i

∑

CT
j

δ
X
(C

S
i , C

T
j )(1−Sim

X
(C

S
i , C

T
j ))

2|CTj |

(2)

Here, δX(CSi , C
T
j ) returns the weight of the

edge between nodeCSi and nodeC
T
j if these nodes

are matched, else it returns 0. Here |CTj | denotes
the size of the cluster for which CTj is the cen-
troid. The weighing by |CTj | is done to make
DivX(CS , CT ) comparable to Div(CT ). Now
the combined divergence between the source par-
tition and the target partition is computed by tak-
ing a weighted sum of target-only divergence and
cross-language divergence.

Div(C
S
, C

T
) = α ∗DivT (CT ) + (1− α) ∗DivX(CS , CT ) (3)

Here α captures the relative importance of the
two divergences.

We now provide an algorithm (see Figure 2) that
minimizes the objective function given in Equa-
tion 3. The algorithm starts by selecting k ran-
dom data points as centroids from the target lan-
guage and then executes the following two steps
in each iteration. It first assigns points to their
nearest centroids and then re-estimates the target
centroids to minimize cross-language divergence
as given in Equation 3. This is achieved by the

386



Procedure CrossLanguageGuidedClustering

Select k centroids randomly fromDT

% Initialize target clusters
Iterate n times or until convergence

Iteratem times
Assign each dTi ∈ DT to the nearest centroid
Recompute the centroids

% Start CLGC
Create cross language similarity graphGx using SimX

Compute maximum bipartite graph matching overGx
Iterate over k target centroids in CT

Update centroid using the cross language update rule
Assign each dTi ∈ DT to the nearest centroid

Return k centroids

Figure 2: Procedure for Cross Language Guided
Clustering

following update rule that is obtained by differen-
tiating the divergence function in Equation 3 with
respect to the current target centroids.

C
T
i =

α
∑

dT
i

∈CT
i
dTi + (1− α)

∑
j δ

x(CTi , C
S
j )φ(C

S
j )

α|CTi |+ (1− α)|CTi |
∑

j δ
x(CTi , C

S
j )φ(C

S
j )

(4)

Here the δX function captures the current
matching of target clusters with source classes. In-
tuitively, there are two factors contributing to the
update rule. The first factor tries to move the
current target centroid towards the center of the
cluster computed using the currently assigned data
points. This is similar to the standard K-means ap-
proach. The second factor that arises due to cross-
language alignment tries to move the centroid to-
wards the currently matched source class. Since
the feature space used to represent source classes
and target centroids are different, we use the func-
tion φ that projects source classes in the feature
space used by the target language. We provide
more details regarding the projection function and
cross-language similarity in the next section.

5 Cross Language Similarity

In order to perform cross language guided cluster-
ing we need a similarity function SimX that given
two documents dSi and d

T
j from source and tar-

get languages, computes a similarity score. Let
V S and V T be the vocabularies used to repre-
sent documents in source and target language re-
spectively. Given a word wSi ∈ V S , let the
function proj(wSi ) return a probability distribu-
tion P = {p1, p2, . . . , p|V T |} where pj represents
the probability of the word wSi being translated to
the word wTj in target dictionary. The function

proj(..) has access to a statistical dictionary DST
for doing this. The dictionary could be constructed
using some large general purpose parallel corpus.
We now present three different methods to com-
pute the similarity function SimX(dSi , d

T
j ).

Projection based Method: Let M represent a
matrix of dimension |V S | ∗ |V T | where each ith
row contains the probability distribution returned
by proj(wSi ) for 1 ≤ i ≤ |V S |. Given a source
document dSi , let d̄

S
i refer to its vector representa-

tion using the feature space V S . Then the projec-
tion function φ(d̄Si ) = (d̄

S
i )

′
M and the similarity

function SimX can be defined as follows, where
′

denotes transpose of a matrix:

SimX(dSi , d
T
j ) = φ(d̄

S
i )d̄

T
j = (d̄

S
i )

′
Md̄Tj (5)

Weighted Projection based Method: The
function proj(wSi ) returns a probability distribu-
tion that captures the likelihood thatwSi gets trans-
lated to a word wTj in the target dictionary. Since,
this function uses a general purpose bi-lingual sta-
tistical dictionary it does not capture domain spe-
cific translations. For example, the English word
“bank” may have equal probabilities for being
translated as “b{\k” or “EknArA” however, given a
corpus from the banking domain, it is more likely
that the word “bank” translates to “b{\k”. There-
fore, given a source term we weigh the probability
values of the target terms that it translates to, by
the frequency of the target terms computed over
the target corpus. We then normalize these values
again to obtain a probability distribution.

Semantic Mapping based Method: There are
multiple words that are synonymous to each other
and can be used to represent the same meaning.
For example, the word “games” and “sports” are
synonymous English words and can be used to
represent the same meaning as “K�l” or “g�m̂s”.
The matrix M used in the previous methods, cap-
tures the translation probabilities at the word level.
In this method we first discover the concepts in
each language and then find translation probabili-
ties at the concept level. We refer to this as seman-
tic mapping between the two languages.

To discover the concepts, words from the source
and target vocabulary are clustered into term clus-
ters based on the words that occur in its context.
For this a word-by-word co-occurrence matrix is
built for the given language. The entry (i, j) in
the matrix contains the number of times the word
wi and wj occur within a fixed window of L

387



words in the corpus. Thus, each word is repre-
sented by a vector called “context vector” that cap-
tures words occurring in the context of the given
word. We then use an off-the-shelf clustering al-
gorithm (Hall et al. 2009) to obtain term clus-
ters in a language. These term clusters are re-
ferred to as concepts. The Figure 3 shows exam-
ples of concepts identified in English and Hindi
languages. Let GS = {GS1 , GS2 , . . . , GSl } and
GT = {GT1 , GT2 , . . . , GTm} be the source and tar-
get concepts obtained by clustering. To find the se-
mantic relationship across concepts from different
languages, we construct a bipartite graph that has
one set of vertices GS corresponding to the source
concepts, and another set GT corresponding to the
target concepts. Now for each word wS ∈ GSi , we
determine the set of target words TwS that it trans-
lates to along with the corresponding translation
probabilities. For each word wT ∈ TwS , we find
the concept GTj that contains w

T and add a weight
p on the edge between the vertex GSi and G

T
j ,

where p is the probability ofwS being translated to
wT . After repeating this process for all the source
concepts, we normalize the edge weights such that
for each GSi , the sum of weights corresponding to
the edges connecting GSi and any concept in the
target language equals to 1. Thus for each source
concept the normalized bipartite graph contains a
distribution over the target concepts. We call this
normalized bipartite graph as the semantic map-
ping between the two languages. Note, that the
normalized bipartite graph can be seen as a matrix
Mmap where the rows and columns correspond to
source and target concepts respectively and the en-
try (i, j) denotes the probability that the ith source
concept corresponds to jth target concept.

Now using the matrix Mmap, the similarity
function SimX(dSi , d

T
j ) can be defined as follows:

SimX(dSi , d
T
j ) = (c̄

S
i )

′Mmapc̄Tj (6)

Here, c̄Si and c̄
T
i denote the concept vector rep-

resentation of dSi and d
T
j respectively. The concept

vector for a document is obtained by replacing the
occurrence of each word wi in the document by its
concept.

6 Experimental Evaluation

There are three key questions for which we seek
an answer through our experimental evaluation.
First, whether the availability of labeled data in
a source language is helpful for labeling unla-
beled documents in the target language. Second,

Figure 3: Vocabulary after Semantic Projection

whether discovery of concepts and concept map-
ping between languages improves the CLGC per-
formance. Third, given that the target language
contains exactly the same classes as the source lan-
guage (which is not an assumption for CLGC),
whether labeling documents using CLGC gives
comparable performance to computationally more
expensive method that uses a machine translation
system. We next describe the dataset, baselines
and evaluation metrics that we use to answer these
questions.

Dataset and Resources: To evaluate the perfor-
mance of our method, we constructed a dataset of
news articles by crawling an English and a Hindi
news site. The crawled news articles are from a
four month period and belong to the following five
categories, viz, (1) Economy and Finance - these
are news reports on macro-economic events (such
as cuts in interest rates, stock market and increase
in taxes), (2) Healthcare and BioTech - these are
business reports from the Healthcare and Biotech-
nology industry (mergers and acquisition, patents
lawsuits , expansion etc), (3) Energy - these are
news reports from the energy and utility sector, (4)
sports and (5) Auto. The number of documents
for each language and category are shown in Ta-
ble 1. As mentioned earlier, the CLGC method
does not assume that the same set of categories
are present in both the languages, to verify this
claim we have an additional category, viz, “Auto”
in our Hindi dataset which is absent in the English
dataset. Even though both English and Hindi news
articles are from the same time frame these articles
are not aligned.

388



Language Economy BioTech Energy Sports Auto
English 1012 510 500 268 0
Hindi 412 300 350 275 153

Table 1: News Dataset used for Experimentation

English Hindi
Number of Unique Words 18128 14521
General Dictionary coverage 11061 (61%) 9344 (64%)
Domain Dictionary coverage 14969 (82.5%) 11767 (81%)

Table 2: Dictionary Statistics

In our experiments, we use an English-Hindi
statistical dictionary which was built using the
Moses toolkit (Koehn 2007). The training data for
the dictionary was a collection of 150,000 English
and Hindi parallel sentences sourced from a gen-
eral corpus. The dictionary built using this cor-
pus is referred to as a “general dictionary” (GD).
We further collected 10,000 parallel sentences on
the topics present in our news dataset. These were
then used along with the earlier set of parallel sen-
tence to learn a dictionary that contains domain
specific words and their translations. We refer to
this dictionary as a “domain dictionary” (DD). The
statistics for these dictionaries in terms of word
coverage is shown in Table 2. The objective of cre-
ating these two dictionaries is to observe the per-
formance of CLGC when a general purpose dictio-
nary is used in contrast to a domain specific dictio-
nary.

Baselines: One of the objective of experimen-
tal evaluation is to see if the availability of source
classes helps in clustering documents in the tar-
get language. In order to measure gains achieved
by the availability of source class information, we
compare the performance of CLGC against the
standard k-means algorithm. We refer to this as
k-means baseline.

Another objective of the experimental evalua-
tion is to see whether labeling documents using
CLGC gives comparable performance to compu-
tationally more expensive method that uses a ma-
chine translation system. For this we train a clas-
sifier using the English news articles referred to as
source classifier. We then translate Hindi new ar-
ticles into English using Google’s machine trans-
lation system and then label them using the source
classifier. We refer to this as NB baseline.

Evaluation Metric The objective of the CLGC
approach is to label the unlabeled target dataset.
We use the following approach for evaluating this.
As the true class-labels for the target news articles
are known we assign to each cluster the class-label

Dictionary Method F1 Purity
K-Means 0.45 0.61

General dictionary
PB 0.49 0.63
WPB 0.56 0.66
SM 0.62 0.71

Domain Dictionary
PB 0.57 0.64
WPB 0.61 0.69
SM 0.64 0.73

Table 3: Comparison of k means with CLGC us-
ing different cross lingual similarity measures

which is the most frequent in the cluster. All arti-
cles in the cluster are now labeled with the cor-
responding cluster-label. Based on this labeling
strategy and the available ground truth we report
the accuracy/purity measure which is computed by
dividing the correctly labelled documents by the
total number of documents. We also evaluate clus-
tering quality by considering the correctness of
clustering decisions over all document pairs. We
report the standard F1 measure over the pairwise
clustering decisions. The F1 measure is the har-
monic mean of precision and recall over pairwise
decisions.

Experiment 1: In our first experiment, We
compare the performance of k-means with the pro-
jection based method, referred to as PB, weighted
projection based method referred to as WPB and
semantic mapping based method, referred to as
SM. For this experiment we use the English
dataset as the source dataset and Hindi dataset as
the target dataset with 4 and 5 categories respec-
tively. For the semantic mapping based method,
we discover concepts using the word clustering.
The word clustering algorithm uses k-means algo-
rithm. We set k to a large value (we set it to 1000)
and use only the first 100 best clusters where good-
ness of a cluster is measured in terms of its diver-
gence. For each word that is not covered by the
first best 100 clusters, we create singleton clusters
for the word. We use this procedure for both the
source and target dataset. We then use the method
described in Section 5 to discover concept map-
pings.

Since the results obtained for both the k means
and all the variations of CLGC depends on the
choice of initial centroids, in each experimental
run all the methods are seeded with the same set
of centroids. The reported results are averaged

389



over 10 runs with random initialization. We set
the value of k equal to the actual number of cate-
gories in each dataset for both k-means as well as
for CLGC. The value of α in Equation 4 is set to
0.5 and value of n and m in the procedure given
in the Figure 2 is kept 20 and 5 respectively.

The results are reported in Table 3. The re-
sults show that there is a significant gain that is
achieved by CLGC methods over K-means. This
shows that the presence of labeled data in the
source language helps in the clustering of docu-
ments in the target language. We further note that
the SM methods, both using “general dictionary”
(GD) and “domain dictionary” (DD) outperforms
all other methods in their class. This happens be-
cause words that do not get translated using the
statistical dictionary, are taken into account as they
become part of concept mappings that have corre-
spondence across languages. Thus, these terms get
accounted in the computation of the SM similarity
measure. These terms were not being considered
in the PB and WPB similarity computations. As
an example the statistical dictionary did not have
the translation for the word “bharti” , which is
the name of a company from the telecommunica-
tion and retail sector. However the word “bharti”
mapped to a concept from the source language
which contained words such as “communication”,
“retail” and “ipo”. This cluster mapped to a con-
cept in Hindi which had words such as “s cAr”,
“ErV�l” and “BArtF” where the first two words
are translations for the words ”communication”
and “retail” respectively. As a result of this cor-
respondence between the two concepts the words
“bharti” and “BArtF” get associated. Another key
point to note is that the performance of Semantic
Mapping using General Dictionary is only slightly
worse than Semantic Mapping using the Domain
Dictionary. This shows that the semantic mapping
based method is able to achieve good performance
even when it does not have access to a domain spe-
cific dictionary.

Experiment 2: In our second experiment, we
compare the performance of SM method which is
the best performing CLGC method with the NB
baseline. We use the rainbow package (McCal-
lum 1996) to train a naı̈ve Bayes classifier using
the English dataset. For translating Hindi docu-
ments to English, we use Google 1 translation en-
gine. The accuracy results for this experiment are
provided in Table 4.

1http://code.google.com/p/google-api-translate-java

Method Accuracy
NB 0.71
SM 0.73

Table 4: Comparison of naı̈ve Bayes with CLGC
(SM using General Dictionary)

We note that the performance of SM is slightly
higher than the naı̈ve Bayes approach. We in-
vestigated the reasons behind this and found that
there are a few important features that are specific
to the Hindi dataset. As the naı̈ve Bayes classi-
fier is trained using the English dataset only, it
does not have access to these features and there-
fore incorrectly classifies the documents that con-
tain such features. While classification techniques
such as those based on Support Vector Machines
can be expected to perform better than simple NB,
our aim here is only to demonstrate that in a re-
source poor language, where building such classi-
fiers may not be possible (due to the lack of a good
machine translation system etc), CLGC can prove
to be a useful method.

7 Concluding Remarks

In this paper, we presented cross language guided
clustering (CLGC) that utilizes the labeled data
from a source language to label unlabeled data
from a target language. CLGC tries to cluster
unlabeled target language documents such that
the resulting clusters are most ‘similar’ or best
‘aligned’ to the given source language classes.
To achieve this alignment we defined a cross-
language similarity measures that returns a sim-
ilarity score between two documents in differ-
ent languages. We presented and compared three
cross-language similarity measure namely Projec-
tion Based, Weighted Projection Based and Se-
mantic Mapping and demonstrate their effective-
ness on real-world data-sets. Our Semantic Map-
ping method, which discovers concepts and their
associated mapping across languages, shows the
maximum gain in the accuracy of labeling docu-
ments over the baseline methods.

References

Xiaojun Wan. 2009. Co-Training for Cross-
Lingual Sentiment Classification, Proceed-
ings of the 47th Annual Meeting of the Asso-
ciation for Computational Linguistics, pages
235–243.

390



Peter Prettenhofer and Benno Stein. 2010. Cross-
Language Text Classification using Struc-
tural Correspondence Learning. Proceedings
of the 48th Annual Meeting of the Associ-
ation for Computational Linguistics, pages
1118–1127.

Verginica Barbu Mititelu and Radu Ion. 2005. Au-
tomatic Import of Verbal Syntactic Relations
Using Parallel Corpora. Cross-Language
Knowledge Induction Workshop.

Indrajit Bhattacharya and Shantanu Godbole and
Sachindra Joshi and Ashish Verma. 2009.
Cross-Guided Clustering: Transfer of Rel-
evant Supervision across Domains for Im-
proved Clustering. Proceedings of the In-
ternationl Conference on Data Mining, pages
41–50.

Mark Hall and Eibe Frank and Geoffrey Holmes
and Bernhard Pfahringer and Peter Reute-
mann and Ian H. Witten. 2009. The WEKA
Data Mining Software: An Update. SIGKDD
Explorations, Volume 11, Issue 1.

Kathleen Mckeown and Regina Barzilay and John
Chen and David Elson and David Evans
and Judith Klavans and Ani Nenkova and
Barry Schiffman and Sergey Sigelman. 2003.
Columbias newsblaster: New features and
future directions. In Proceedings of NAACL-
HLT03.

M. Bilenko and R. J. Mooney. 2003 Adaptive du-
plicate detection using learnable string sim-
ilarity measures In ACM SIGKDD Interna-
tional Conference on Knowledge Discovery
and Data Mining, 2003.

D. Klein and S. D. Kamvar and C. Manning. 2002
From instance level constraints to space-level
constraints: Making the most of prior knowl-
edge in data clustering In International Con-
ference on Machine Learning, 2002.

I. Dhillon and S. Mallela and D. S. Modha.
2003 Information theoretic co-clustering
On ACM SIGKDD International Conference
on Knowledge Discovery and Data Mining,
2003.

N. Slonim and N. Tishby. 2000 Document clus-
tering using word clusters via the informa-
tion bottleneck method In The Annual Inter-
national ACM SIGIR Conference, 2000.

I. Bhattacharya and L. Getoor. 2007 Collective en-
tity resolution in relational data ACM Trans-
actions on Knowledge Discovery from Data,
vol. 1, no. 1, pp. 1–36, March 2007.

H. W. Kuhn. 1955. The hungarian method for the
assignment problem Naval Research Logis-
tics Quarterly, vol. 2, pp. 83–97, 1955.

J. Scott Olsson and Douglas W. Oard and Jan Ha-
jic. 2005. Cross language text classification
In Proceedings of SIGIR-05, pages 645–646.

Andrew Kachites McCallum 1996. Bow:
A toolkit for statistical language modeling,
text retrieval, classification and clustering.
http://www.cs.cmu.edu/ mccallum/bow.

Susan T. Dumais, Todd A. Letsche, Michael L.
Littman, and Thomas K. Landauer. 1997. Au-
tomatic cross-language retrieval using latent
semantic indexing In AAAI Symposium on
Cross-Language Text and Speech Retrieval.

Blaz Fortuna and John Shawe-Taylor. 2005. The
use of machine translation tools for cross-
lingual text mining. In Proceedings of the
ICML Workshop on Learning with Multiple
Views.

Ke Wu and Bao-Liang Lu. 2007. Cross-Lingual
Document Clustering . In Lecture Notes in
Computer Science, Volume 4426/2007, 956–
963,

Philipp Koehn, Hieu Hoang, Alexandra Birch
Mayne, Christopher Callison-Burch, Mar-
cello Federico, Nicola Bertoldi, Brooke
Cowan, Wade Shen, Christine Moran,
Richard Zens, Chris Dyer, Ondrej Bojar,
Alexandra Constantin, Evan Herbst 2007.
Open source toolkit for statistical machine
translation. Annual Meeting of the Asso-
ciation for Computation Linguistics (ACL),
Demonstration Session

391


