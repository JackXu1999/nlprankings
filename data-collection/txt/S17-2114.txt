



















































MI&T Lab at SemEval-2017 task 4: An Integrated Training Method of Word Vector for Sentiment Classification


Proceedings of the 11th International Workshop on Semantic Evaluations (SemEval-2017), pages 689–693,
Vancouver, Canada, August 3 - 4, 2017. c©2017 Association for Computational Linguistics

MI&T Lab at SemEval-2017 task 4: An Integrated Training Method of
Word Vector for Sentiment Classification

Jingjing Zhao, Yan Yang, Bing Xu
Laboratory of Machine Intelligence and Translation, Harbin Institute of Technology

School of Computer Science and Technology, Harbin Institute of Technology, Harbin, China
zhaojingjing@hit.edu.cn,yangyan@hit.edu.cn, hitxb@hit.edu.cn

Abstract

A CNN method for sentiment classifica-
tion task in Task 4A of SemEval 2017
is presented. To solve the problem of
word2vec training word vector slowly, a
method of training word vector by inte-
grating word2vec and Convolutional Neu-
ral Network (CNN) is proposed. This
training method not only improves the
training speed of word2vec, but also
makes the word vector more effective for
the target task. Furthermore, the word2vec
adopts a full connection between the input
layer and the projection layer of the Con-
tinuous Bag-of-Words (CBOW) for ac-
quiring the semantic information of the
original sentence.

1 Introduction

The polarity of a Twitter message is classified into
positive, negative and neutral in Twitter sentiment
analysis. However, the difficulty of sentiment
analysis greatly increases due to the ambiguity and
the rhetorical of natural language (Liu, 2012). In
recent years, the deep learning model has shown
great potential in the task of sentiment classifica-
tion (Socher et al., 2011; Poria et al., 2015; Socher
et al., 2013). For short text data such as Twit-
ter, Convolutional Neural Network (CNN) model
(Kim, 2014; Dos Santos and Gatti, 2014; Chen
et al., 2016) is the most widely and successfully
used, and in the SemEval 2016-task4A competi-
tion, the system ranked first also uses CNN model
(Deriu et al., 2016). So CNN model is used to
complete the task in our system. The task 4A
of SemEval 20171 is a polarity classification task
which requires participated systems to classify a

1http://alt.qcri.org/semeval2017/
task4/

given Twitter message into positive, negative or
neutral (Rosenthal et al., 2017).

The system integrates the word2vec and CNN
to train the labeled data, generating the word vec-
tor of each word in the data. This method can
improve the training speed of word vector. In or-
der to preserve the more semantic information of
the original sentence effectively, the word2vec is
fully connected between the input layer and the
projection layer of the Continuous Bag-of-Words
(CBOW).

2 System description

2.1 Word vector representation method

Word2vec can represent every word that ap-
pears in a large number of training texts as
a lower dimension vector (usually 50-100 di-
mensions). (Mikolov et al., 2013b; Rong, 2014;
Mikolov et al., 2013a) have a detail descrip-
tion of word2vec. Word2vec in our sys-
tem uses Continuous Bag-of-Words (CBOW)
model, and the structure is shown in Fig-
ure 1, in which wi is a word, and the se-
quence 〈w(i−c), ..., w(i−1), w(i+1), ..., w(i+c)〉 rep-
resents the context of the word wi, and c is the
window size. The word-vector length of the word
wi is d. The traditional word2vec adds 2c words’
word vector on the input layer to the projection
layer. (Mikolov et al., 2013b) has a detail descrip-
tion of this method, which has been exactly used in
Google Word2vec released in 2013. However, in
the sentiment analysis task, whether there is a neg-
ative word before the sentiment word will influ-
ence the identification of polarity (Liu, 2012). So
in order to preserve more emotional semantic in-
formation, the input layer and the projection layer
of CBOW are fully connected in this system.

The procedure of integrating word2vec and
CNN to train the words vector follows four

689



Figure 1: The structure of word2vec in the system.

steps: (i) initialization and pre-training: initial-
ize word2vec and CNN parameters, and pre-
train word2vec a certain number of iterations; (ii)
CNN training: input the latest words vector from
word2vec to CNN; (iii) word2vec training: input
the latest words vector from CNN to word2vec;
(iv) alternate the (ii) and (iii) steps until the train-
ing phase converges. CNN can help word2vec
extract more effective text features. Experiments
show that the model obtained by integrating CNN
and word2vec performs better when the data is
sufficient compared to adopting them separately.

2.2 Deep learning model

The system proposes CNN model to predict the
sentiment polarity of a Twitter text. The CNN
structure diagram is shown in Figure 2.

Word vector sequence: Each word is repre-
sented as a d-dimensional word vector, a sentence
or a Twitter text containing n words can be ex-
pressed as n d-dimensional vectors, which are
concatenated together into a matrix X ∈ Rd×n
form which represents a sentence or a Twitter text.
Each row of matrix X is treated as a new vec-
tor, thus d n-dimensional vectors are obtained and
concatenated as input to the CNN network.

Convolutional layer: The convolution layer
uses full convolution operation. Let F li ∈ RM1
represents ith feature map at lth layer2, andmlj,i ∈
RM2 represents the convolutional kernel of the jth
convolutional result C l+1j at l + 1th layer. So the

2Here, a layer refers to one convolutional and one pooling
layer.

jth convolutional result C l+1j at l + 1th layer is
the result of convolution operation between each
feature map at lth layer and convolutional kernel
mlj,i, i.e.,

C l+1j =
∑

j

mlj,i ∗ F li (1)

k-max pooling: After the convolution oper-
ation, the max-pooling operation is performed
which preserves the largest value in each convolu-
tion result. The system uses k-max pooling, which
preserves the largest k values instead. For exam-
ple:

(1, 6, 3, 8) 2−max−−−−→ (6, 8)
Where k is a parameter that needs to be set man-

ually.
full-connection Layer: The full connection

layer receives the output of the last layer of CNN
and fully connects itself to the output layer, i.e.
W ∗x+b operation, whereW and b can be trained
during the network training phase.

output Layer: The output layer uses the Soft-
max operation and outputs the probability that
the input sentence or Twitter text belongs to each
class, and the class with the maximum probability
is the predicted class judged by the system.

2.3 Combination prediction
Because of the insufficiency of training data and
the great quantity bias in different classes’ train-
ing data, the trained CNN can’t work so well. So
our system adds Support Vector Machine (SVM)
(Suykens, 2001) model to predict jointly with

690



Figure 2: The structure of CNN in the system.

CNN. The steps of combination prediction are
shown in Figure 3.

Figure 3: The steps of combination prediction.

3 Experimental datasets and model
parameters

3.1 Experimental datasets
The datasets of the experiment is provided by Se-
mEval 2017, and the specific datasets used are
shown in Table 1.

Dataset positive negative neutral Total
train 18123 14354 21748 54225
dev 1000 500 1500 3000
test 2375 3972 5937 12284

Table 1: Datasets of the experiment.

3.2 Model parameters

The parameters set of word2vec and CNN in our
system are shown in Table 2.

word2vec parameters
Window size of context c 3
Number of hidden layer 1
Dimensionality of hidden layer [50]
Convolutional Neural Network parameters

Number of Convolutional layer 1
Number of max-pooling layer 1
Convolutional kernel size 7
Number of feature map at Convolutional 100
k-max pooling parameter k 2

Table 2: Parameters used in the word2vec and
CNN.

4 Experimental results and analysis

4.1 Evaluation method

The measure metric of the Evaluation is average
macro recall (Rosenthal et al., 2017). The formula

691



is as follows:

ρPUN =
ρP + ρU + ρN

3
(2)

Here, ρP , ρU and ρN denote recall for the posi-
tive class, neutral class and negative class.

The other two measure metrics are the average
macro F1 and the average macro precision:

FPUN1 =
FP1 + F

U
1 + F

N
1

3
(3)

PPUN =
PP + PU + PN

3
(4)

Here, FP1 , F
U
1 and F

N
1 denote F1 for the pos-

itive class, neutral class and negative class; PP ,
PU and PN denote precision for the positive class,
neutral class and negative class.

4.2 Analysis of experimental results
Table 3 lists the average macro recall for each
model on the development dataset. From the ta-
ble 3, the effect of word2vec+CNN model is bet-
ter than SVM, and word2vec + CNN + SVM is the
best of the three models, so the best results on the
test set are submitted.

System SVM CNN SVM+CNN
ρPUN 0.589 0.601 0.653

Table 3: The results of different models on the
development dataset.

Table 4 shows the details of our system’s result
in comparison with the three top ranked systems’
results. It can be seen from the table that our re-
sult’s ρN is not good, but ρU is better than the top
three systems. The decrease of experimental re-
sults is from the quantity bias in training data of
different classes.

For deep learning models, a lot of training data
are required. Due to the lack of Twitter texts,

word2vec training is not sufficient and do not gen-
erate effective words vector representation. In the
future, semi-supervisory mechanisms will be con-
sidered to expand the number of training data.

In the future, we can improve the system’s per-
formance from following points: (i) to expand the
amount of training data; (ii) to improve the type
of combination: the results can be combined with
multiple CNN systems to predict; (iii) to add more
emotional semantic features.

5 Conclusion

This paper presentes a method of training word
vector by integrating word2vec with CNN and us-
ing the trained CNN to complete the Twitter senti-
ment analysis task. In the future work, we hope to
continue to improve system’s performance in mul-
tiple ways, such as trying to modify some param-
eters or improve the type of classifiers’ combina-
tion, adding some sentiment features.

Acknowledgments

This work is supported by the National Natural
Science Foundation of China (No. 61402134).

References
Peng Chen, Bing Xu, Muyun Yang, and Sheng Li.

2016. Clause sentiment identification based on con-
volutional neural network with context embedding.
In Natural Computation, Fuzzy Systems and Knowl-
edge Discovery (ICNC-FSKD), 2016 12th Interna-
tional Conference on. IEEE, pages 1532–1538.

Jan Deriu, Maurice Gonzenbach, Fatih Uzdilli, Au-
relien Lucchi, Valeria De Luca, and Martin Jaggi.
2016. Swisscheese at semeval-2016 task 4: Sen-
timent classification using an ensemble of convo-
lutional neural networks with distant supervision.
Proceedings of SemEval pages 1124–1128.

Cı́cero Nogueira Dos Santos and Maira Gatti. 2014.
Deep convolutional neural networks for sentiment
analysis of short texts. In COLING. pages 69–78.

System
Positive Negative Neutral Score

P ρ F1 P ρ F1 P ρ F1 ρPUN

DataStories(1) 0.6259 0.7023 0.6619 0.5929 0.8291 0.6914 0.7471 0.5115 0.6073 0.681
BB twtr(1) 0.6851 0.6522 0.6682 0.5848 0.8776 0.7019 0.7518 0.5144 0.6109 0.681

LIA (3) 0.6480 0.6518 0.6499 0.6082 0.8170 0.6973 0.7289 0.5599 0.6333 0.676
MI&T Lab 0.4780 0.5171 0.4968 0.5496 0.5451 0.5473 0.6066 0.5902 0.5983 0.551

Table 4: The details of our result and the three top ranked results.

692



Yoon Kim. 2014. Convolutional neural net-
works for sentence classification. arXiv preprint
arXiv:1408.5882 .

Bing Liu. 2012. Sentiment analysis and opinion min-
ing. Synthesis lectures on human language tech-
nologies 5(1):1–167.

Tomas Mikolov, Kai Chen, Greg Corrado, and Jef-
frey Dean. 2013a. Efficient estimation of word
representations in vector space. arXiv preprint
arXiv:1301.3781 .

Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Cor-
rado, and Jeff Dean. 2013b. Distributed representa-
tions of words and phrases and their compositional-
ity. In Advances in neural information processing
systems. pages 3111–3119.

Soujanya Poria, Erik Cambria, and Alexander F Gel-
bukh. 2015. Deep convolutional neural network
textual features and multiple kernel learning for
utterance-level multimodal sentiment analysis. In
EMNLP. pages 2539–2544.

Xin Rong. 2014. word2vec parameter learning ex-
plained. arXiv preprint arXiv:1411.2738 .

Sara Rosenthal, Noura Farra, and Preslav Nakov. 2017.
SemEval-2017 task 4: Sentiment analysis in Twit-
ter. In Proceedings of the 11th International Work-
shop on Semantic Evaluation. Association for Com-
putational Linguistics, Vancouver, Canada, SemEval
’17.

Richard Socher, Jeffrey Pennington, Eric H Huang,
Andrew Y Ng, and Christopher D Manning. 2011.
Semi-supervised recursive autoencoders for predict-
ing sentiment distributions. In Proceedings of the
conference on empirical methods in natural lan-
guage processing. Association for Computational
Linguistics, pages 151–161.

Richard Socher, Alex Perelygin, Jean Y Wu, Jason
Chuang, Christopher D Manning, Andrew Y Ng,
Christopher Potts, et al. 2013. Recursive deep
models for semantic compositionality over a senti-
ment treebank. In Proceedings of the conference on
empirical methods in natural language processing
(EMNLP). Citeseer, volume 1631, page 1642.

Johan AK Suykens. 2001. Nonlinear modelling and
support vector machines. In Instrumentation and
Measurement Technology Conference, 2001. IMTC
2001. Proceedings of the 18th IEEE. IEEE, vol-
ume 1, pages 287–294.

693


