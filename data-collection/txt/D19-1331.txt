



















































On NMT Search Errors and Model Errors: Cat Got Your Tongue?


Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing
and the 9th International Joint Conference on Natural Language Processing, pages 3356–3362,
Hong Kong, China, November 3–7, 2019. c©2019 Association for Computational Linguistics

3356

On NMT Search Errors and Model Errors: Cat Got Your Tongue?

Felix Stahlberg∗ and Bill Byrne
University of Cambridge

Department of Engineering
Trumpington St, Cambridge CB2 1PZ, UK
{fs439,wjb31}@cam.ac.uk

Abstract

We report on search errors and model errors in
neural machine translation (NMT). We present
an exact inference procedure for neural se-
quence models based on a combination of
beam search and depth-first search. We use
our exact search to find the global best model
scores under a Transformer base model for the
entire WMT15 English-German test set. Sur-
prisingly, beam search fails to find these global
best model scores in most cases, even with a
very large beam size of 100. For more than
50% of the sentences, the model in fact assigns
its global best score to the empty translation,
revealing a massive failure of neural models in
properly accounting for adequacy. We show
by constraining search with a minimum trans-
lation length that at the root of the problem
of empty translations lies an inherent bias to-
wards shorter translations. We conclude that
vanilla NMT in its current form requires just
the right amount of beam search errors, which,
from a modelling perspective, is a highly un-
satisfactory conclusion indeed, as the model
often prefers an empty translation.

1 Introduction

Neural machine translation (Kalchbrenner and
Blunsom, 2013; Sutskever et al., 2014; Bahdanau
et al., 2015, NMT) assigns the probability P (y|x)
of a translation y = yJ1 ∈ T J of length J over
the target language vocabulary T for a source sen-
tence x ∈ SI of length I over the source language
vocabulary S via a left-to-right factorization using
the chain rule:

logP (y|x) =
J∑

j=1

logP (yj |yj−11 ,x). (1)

The task of finding the most likely translation ŷ ∈
T ∗ for a given source sentence x is known as the

*Now at Google.

decoding or inference problem:

ŷ = argmax
y∈T ∗

P (y|x). (2)

The NMT search space is vast as it grows expo-
nentially with the sequence length. For example,
for a common vocabulary size of |T | = 32, 000,
there are already more possible translations with
20 words or less than atoms in the observable
universe (32, 00020 � 1082). Thus, complete
enumeration of the search space is impossible.
The size of the NMT search space is perhaps the
main reason why – besides some preliminary stud-
ies (Niehues et al., 2017; Stahlberg et al., 2018b;
Ott et al., 2018) – analyzing search errors in NMT
has received only limited attention. To the best of
our knowledge, none of the previous studies were
able to quantify the number of search errors in un-
constrained NMT due to the lack of an exact infer-
ence scheme that – although too slow for practi-
cal MT – guarantees to find the global best model
score for analysis purposes.

In this work we propose such an exact decod-
ing algorithm for NMT that exploits the mono-
tonicity of NMT scores: Since the conditional
log-probabilities in Eq. 1 are always negative,
partial hypotheses can be safely discarded once
their score drops below the log-probability of any
complete hypothesis. Using our exact inference
scheme we show that beam search does not find
the global best model score for more than half of
the sentences. However, these search errors, para-
doxically, often prevent the decoder from suffer-
ing from a frequent but very serious model error in
NMT, namely that the empty hypothesis often gets
the global best model score. Our findings suggest
a reassessment of the amount of model and search
errors in NMT, and we hope that they will spark
new efforts in improving NMT modeling capabil-
ities, especially in terms of adequacy.



3357

Algorithm 1 BeamSearch(x, n ∈ N+)
Input: x: Source sentence, n: Beam size

1: Hcur ← {(�, 0.0)} {Initialize with empty translation prefix and zero score}
2: repeat
3: Hnext ← ∅
4: for all (y, p) ∈ Hcur do
5: if y|y| = < /s > then
6: Hnext ← Hnext ∪ {(y, p)} {Hypotheses ending with < /s > are not expanded}
7: else
8: Hnext ← Hnext ∪

⋃
w∈T (y · w, p+ logP (w|x,y)) {Add all possible continuations}

9: end if
10: end for
11: Hcur ← {(y, p) ∈ Hnext : |{(y′, p′) ∈ Hnext : p′ > p}| < n} {Select n-best}
12: (ỹ, p̃)← argmax(y,p)∈Hcur p
13: until ỹ|ỹ| = < /s >
14: return ỹ

Algorithm 2 DFS(x,y, p ∈ R, γ ∈ R)
Input: x: Source sentence

y: Translation prefix (default: �)
p: logP (y|x) (default: 0.0)
γ: Lower bound

1: if y|y| = < /s > then
2: return (y, p) {Trigger γ update}
3: end if
4: ỹ←⊥ {Initialize ỹ with dummy value}
5: for all w ∈ T do
6: p′ ← p+ logP (w|x,y)
7: if p′ ≥ γ then
8: (y′, γ′)← DFS(x,y · w, p′, γ)
9: if γ′ > γ then

10: (ỹ, γ)← (y′, γ′)
11: end if
12: end if
13: end for
14: return (ỹ, γ)

2 Exact Inference for Neural Models

Decoding in NMT (Eq. 2) is usually tackled with
beam search, which is a time-synchronous approx-
imate search algorithm that builds up hypotheses
from left to right. A formal algorithm description
is given in Alg. 1. Beam search maintains a set
of active hypotheses Hcur. In each iteration, all
hypotheses in Hcur that do not end with the end-
of-sentence symbol< /s > are expanded and col-
lected inHnext. The best n items inHnext consti-
tute the set of active hypotheses Hcur in the next
iteration (line 11 in Alg. 1), where n is the beam

size. The algorithm terminates when the best hy-
pothesis in Hcur ends with the end-of-sentence
symbol < /s >. Hypotheses are called complete
if they end with < /s > and partial if they do not.

Beam search is the ubiquitous decoding algo-
rithm for NMT, but it is prone to search errors
as the number of active hypotheses is limited by
n. In particular, beam search never compares
partial hypotheses of different lengths with each
other. As we will see in later sections, this is
one of the main sources of search errors. How-
ever, in many cases, the model score found by
beam search is a reasonable approximation to the
global best model score. Let γ be the model
score found by beam search (p̃ in line 12, Alg. 1),
which is a lower bound on the global best model
score: γ ≤ logP (ŷ|x). Furthermore, since the
conditionals logP (yj |yj−11 ,x) in Eq. 1 are log-
probabilities and thus non-positive, expanding a
partial hypothesis is guaranteed to result in a lower
model score, i.e.:1

∀j ∈ [2, J ] : logP (yj−11 |x) > logP (y
j
1|x). (3)

Consequently, when we are interested in the global
best hypothesis ŷ, we only need to consider partial
hypotheses with scores greater than γ. In our ex-
act decoding scheme we traverse the NMT search
space in a depth-first order, but cut off branches
along which the accumulated model score falls be-
low γ. During depth-first search (DFS), we up-
date γ when we find a better complete hypothesis.

1Equality in Eq. 3 is impossible since probabilities are
modeled by the neural model via a softmax function which
never predicts a probability of exactly 1.



3358

Alg. 2 specifies the DFS algorithm formally. An
important detail is that elements in T are ordered
such that the loop in line 5 considers the < /s >
token first. This often updates γ early on and leads
to better pruning in subsequent recursive calls.2

Exact inference under length constraints Our
admissible pruning criterion based on γ relies on
the fact that the model score of a (partial) hy-
pothesis is always lower than the score of any
of its translation prefixes. While this monotonic-
ity condition is true for vanilla NMT (Eq. 3), it
does not hold for methods like length normaliza-
tion (Jean et al., 2015; Boulanger-Lewandowski
et al., 2013; Wu et al., 2016) or word rewards (He
et al., 2016): Length normalization gives an ad-
vantage to longer hypotheses by dividing the score
by the sentence length, while a word reward di-
rectly violates monotonicity as it rewards each
word with a positive value. In Sec. 4 we show
how our exact search can be extended to handle ar-
bitrary length models (Murray and Chiang, 2018;
Huang et al., 2017; Yang et al., 2018) by introduc-
ing length dependent lower bounds γk and report
initial findings on exact search under length nor-
malization. However, despite being of practical
use, methods like length normalization and word
penalties are rather heuristic as they do not have
any justification from a probabilistic perspective.
They also do not generalize well as (without re-
tuning) they often work only for a specific beam
size. It would be much more desirable to fix the
length bias in the NMT model itself.

3 Results without Length Constraints

We conduct all our experiments in this sec-
tion on the entire English-German WMT
news-test2015 test set (2,169 sentences) with
a Transformer base (Vaswani et al., 2017) model
trained with Tensor2Tensor (Vaswani et al., 2018)
on parallel WMT18 data excluding ParaCrawl.
Our pre-processing is as described by Stahlberg
et al. (2018a) and includes joint subword seg-
mentation using byte pair encoding (Sennrich
et al., 2016) with 32K merges. We report cased
BLEU scores.3 An open-source implementation
of our exact inference scheme is available in the

2Note that the order in which the for-loop in line 5 of
Alg. 2 iterates over T may be important for efficiency but
does not affect the correctness of the algorithm.

3Comparable with http://matrix.statmt.org/

Search BLEU Ratio #Search errors #Empty
Greedy 29.3 1.02 73.6% 0.0%
Beam-10 30.3 1.00 57.7% 0.0%
Exact 2.1 0.06 0.0% 51.8%

Table 1: NMT with exact inference. In the absence of
search errors, NMT often prefers the empty translation,
causing a dramatic drop in length ratio and BLEU.

28.8

29.0

29.2

29.4

29.6

29.8

30.0

30.2

30.4

54%57%60%63%66%69%72%

0.95

0.96

0.97

0.98

0.99

1.00

1.01

1.02

10030105321

B
L

E
U

L
e

n
g

th
 r

a
ti
o

#Search errors

Beam size

 BLEU
Length ratio

Figure 1: BLEU over the percentage of search er-
rors. Large beam sizes yield fewer search errors but
the BLEU score suffers from a length ratio below 1.

-12.2

-12.0

-11.8

-11.6

-11.4

-11.2

-11.0

-10.8

 10  20  30  40  50  60  70  80  90 100

 55%

 60%

 65%

 70%

L
o

g
-l
ik

e
lih

o
o

d

#
S

e
a

rc
h

 e
rr

o
rs

Beam size

Log-likelihood
#Search errors

Figure 2: Even large beam sizes produce a large num-
ber of search errors.

SGNMT decoder (Stahlberg et al., 2017, 2018b).4

Our main result is shown in Tab. 1. Greedy
and beam search both achieve reasonable BLEU
scores but rely on a high number of search er-
rors5 to not be affected by a serious NMT model
error: For 51.8% of the sentences, NMT assigns
the global best model score to the empty transla-
tion, i.e. a single < /s > token. Fig. 1 visualizes
the relationship between BLEU and the number of
search errors. Large beam sizes reduce the num-
ber of search errors, but the BLEU score drops be-
cause translations are too short. Even a large beam
size of 100 produces 53.62% search errors. Fig. 2
shows that beam search effectively reduces search

4http://ucam-smt.github.io/sgnmt/html/,
simpledfs decoding strategy.

5A sentence is classified as search error if the decoder
does not find the global best model score.

http://matrix.statmt.org/
http://ucam-smt.github.io/sgnmt/html/


3359

 0

 200

 400

 600

 800

 1000

 1200

[0.0,0.1]

(0.1,0.3]

(0.3,0.5]

(0.5,0.7]

(0.7,0.9]

(0.9,1.1]

(1.1,1.3]

(1.3,1.5]

(1.5,1.7]

(1.7,1.9]

>1.9

#
S

e
n

te
n

c
e

s

Target/source ratio

Reference
Exact

Beam-10

Figure 3: Histogram over target/source length ratios.

Model Beam-10 Exact
BLEU #Search err. #Empty

LSTM∗ 28.6 58.4% 47.7%
SliceNet∗ 28.8 46.0% 41.2%
Transformer-Base 30.3 57.7% 51.8%
Transformer-Big∗ 31.7 32.1% 25.8%

Table 2: ∗: The recurrent LSTM, the convolutional
SliceNet (Kaiser et al., 2017), and the Transformer-Big
systems are strong baselines from a WMT’18 shared
task submission (Stahlberg et al., 2018a).

errors with respect to greedy decoding to some de-
gree, but is ineffective in reducing search errors
even further. For example, Beam-10 yields 15.9%
fewer search errors (absolute) than greedy decod-
ing (57.68% vs. 73.58%), but Beam-100 improves
search only slightly (53.62% search errors) despite
being 10 times slower than beam-10.

The problem of empty translations is also vis-
ible in the histogram over length ratios (Fig. 3).
Beam search – although still slightly too short –
roughly follows the reference distribution, but ex-
act search has an isolated peak in [0.0, 0.1] from
the empty translations.

Tab. 2 demonstrates that the problems of search
errors and empty translations are not specific to the

  0%

 20%

 40%

 60%

 80%

100%

[0,10] [10,20) [20,30) [30,40) [40,50) [50,60] >60

P
e

rc
e

n
ta

g
e

Source sentence length

#Search errors
#Empty global best

#Search errors w/o empty trans.

Figure 4: Number of search errors under Beam-10 and
empty global bests over the source sentence length.

 0

 100

 200

 300

 400

 500

 600

 700

 800

[0.0,0.1]

(0.1,0.3]

(0.3,0.5]

(0.5,0.7]

(0.7,0.9]

(0.9,1.1]

(1.1,1.3]

(1.3,1.5]

(1.5,1.7]

(1.7,1.9]

>1.9

#
S

e
n

te
n

c
e

s

Target/source ratio

Reference
Exact

Beam-10

Figure 5: Histogram over length ratios with minimum
translation length constraint of 0.25 times the source
sentence length. Experiment conducted on 73.0% of
the test set.

Transformer base model and also occur with other
architectures. Even a highly optimized Trans-
former Big model from our WMT18 shared task
submission (Stahlberg et al., 2018a) has 25.8%
empty translations.

Fig. 4 shows that long source sentences are
more affected by both beam search errors and the
problem of empty translations. The global best
translation is empty for almost all sentences longer
than 40 tokens (green curve). Even without sen-
tences where the model prefers the empty transla-
tion, a large amount of search errors remain (blue
curve).

4 Results with Length Constraints

To find out more about the length deficiency
we constrained exact search to certain translation
lengths. Constraining search that way increases
the run time as the γ-bounds are lower. Therefore,
all results in this section are conducted on only a
subset of the test set to keep the runtime under con-
trol.6 We first constrained search to translations
longer than 0.25 times the source sentence length
and thus excluded the empty translation from the
search space. Although this mitigates the prob-
lem slightly (Fig. 5), it still results in a peak in the
(0.3, 0.5] cluster. This suggests that the problem
of empty translations is the consequence of an in-
herent model bias towards shorter hypotheses and
cannot be fixed with a length constraint.

We then constrained exact search to either the
6We stopped decoding if the decoder took longer than a

day for a single sentence on a single CPU. Exact search with-
out length constraints is much faster and does not need max-
imum execution time limits.



3360

Search BLEU Ratio
Beam-10 37.0 1.00
Exact for Beam-10 length 37.0 1.00
Exact for reference length 37.9 1.01

Table 3: Exact search under length constraints. Exper-
iment conducted on 48.3% of the test set.

Search W/o length norm. With length norm.
BLEU Ratio BLEU Ratio

Beam-10 37.0 1.00 36.3 1.03
Beam-30 36.7 0.98 36.3 1.04
Exact 27.2 0.74 36.4 1.03

Table 4: Length normalization fixes translation lengths,
but prevents exact search from matching the BLEU
score of Beam-10. Experiment conducted on 48.3%
of the test set.

length of the best Beam-10 hypothesis or the ref-
erence length. Tab. 3 shows that exact search con-
strained to the Beam-10 hypothesis length does
not improve over beam search, suggesting that
any search errors between beam search score and
global best score for that length are insignificant
enough so as not to affect the BLEU score. The
oracle experiment in which we constrained exact
search to the correct reference length (last row in
Tab. 3) improved the BLEU score by 0.9 points.

A popular method to counter the length bias in
NMT is length normalization (Jean et al., 2015;
Boulanger-Lewandowski et al., 2013) which sim-
ply divides the sentence score by the sentence
length. We can find the global best translations un-
der length normalization by generalizing our ex-
act inference scheme to length dependent lower
bounds γk. The generalized scheme7 finds the best
model scores for each translation length k in a cer-
tain range (e.g. zero to 1.2 times the source sen-
tence length). The initial lower bounds are derived
from the Beam-10 hypothesis ybeam as follows:8

γk = (k + 1)
logP (ybeam|x)
|ybeam|+ 1

. (4)

Exact search under length normalization does not
suffer from the length deficiency anymore (last
row in Tab. 4), but it is not able to match our best
BLEU score under Beam-10 search. This suggests
that while length normalization biases search to-
wards translations of roughly the correct length, it
does not fix the fundamental modelling problem.

7Available in our SGNMT decoder (Stahlberg et al., 2017,
2018b) as simplelendfs strategy.

8We add 1 to the lengths to avoid division by zero errors.

5 Related Work

Other researchers have also noted that large
beam sizes yield shorter translations (Koehn and
Knowles, 2017). Sountsov and Sarawagi (2016)
argue that this model error is due to the locally nor-
malized maximum likelihood training objective in
NMT that underestimates the margin between the
correct translation and shorter ones if trained with
regularization and finite data. A similar argu-
ment was made by Murray and Chiang (2018) who
pointed out the difficulty for a locally normalized
model to estimate the “budget” for all remaining
(longer) translations. Kumar and Sarawagi (2019)
demonstrated that NMT models are often poorly
calibrated, and that that can cause the length defi-
ciency. Ott et al. (2018) argued that uncertainty
caused by noisy training data may play a role.
Chen et al. (2018) showed that the consistent best
string problem for RNNs is decidable. We pro-
vide an alternative DFS algorithm that relies on
the monotonic nature of model scores rather than
consistency, and that often converges in practice.

To the best of our knowledge, this is the first
work that reports the exact number of search errors
in NMT as prior work often relied on approxima-
tions, e.g. via n-best lists (Niehues et al., 2017) or
constraints (Stahlberg et al., 2018b).

6 Conclusion

We have presented an exact inference scheme for
NMT. Exact search may not be practical, but it al-
lowed us to discover deficiencies in widely used
NMT models. We linked deteriorating BLEU
scores of large beams with the reduction of search
errors and showed that the model often prefers the
empty translation – an evidence of NMT’s failure
to properly model adequacy. Our investigations
into length constrained exact search suggested that
simple heuristics like length normalization are un-
likely to remedy the problem satisfactorily.

Acknowledgments

This work was supported by the U.K. Engineering
and Physical Sciences Research Council (EPSRC)
grant EP/L027623/1 and has been performed us-
ing resources provided by the Cambridge Tier-2
system operated by the University of Cambridge
Research Computing Service9 funded by EPSRC
Tier-2 capital grant EP/P020259/1.

9http://www.hpc.cam.ac.uk

http://www.hpc.cam.ac.uk


3361

References
Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Ben-

gio. 2015. Neural machine translation by jointly
learning to align and translate. In ICLR.

Nicolas Boulanger-Lewandowski, Yoshua Bengio, and
Pascal Vincent. 2013. Audio chord recognition with
recurrent neural networks. In ISMIR, pages 335–
340. Citeseer.

Yining Chen, Sorcha Gilroy, Andreas Maletti, Jonathan
May, and Kevin Knight. 2018. Recurrent neu-
ral networks as weighted language recognizers. In
Proceedings of the 2018 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics: Human Language Technologies,
Volume 1 (Long Papers), pages 2261–2271, New
Orleans, Louisiana. Association for Computational
Linguistics.

Wei He, Zhongjun He, Hua Wu, and Haifeng Wang.
2016. Improved neural machine translation with
SMT features. In Proceedings of the Thirtieth AAAI
Conference on Artificial Intelligence, pages 151–
157. AAAI Press.

Liang Huang, Kai Zhao, and Mingbo Ma. 2017. When
to finish? Optimal beam search for neural text
generation (modulo beam size). In Proceedings
of the 2017 Conference on Empirical Methods in
Natural Language Processing, pages 2134–2139,
Copenhagen, Denmark. Association for Computa-
tional Linguistics.

Sébastien Jean, Orhan Firat, Kyunghyun Cho, Roland
Memisevic, and Yoshua Bengio. 2015. Montreal
neural machine translation systems for WMT’15. In
Proceedings of the Tenth Workshop on Statistical
Machine Translation, pages 134–140, Lisbon, Por-
tugal. Association for Computational Linguistics.

Lukasz Kaiser, Aidan N Gomez, and Francois Chol-
let. 2017. Depthwise separable convolutions
for neural machine translation. arXiv preprint
arXiv:1706.03059.

Nal Kalchbrenner and Phil Blunsom. 2013. Recurrent
continuous translation models. In Proceedings of
the 2013 Conference on Empirical Methods in Nat-
ural Language Processing, pages 1700–1709. Asso-
ciation for Computational Linguistics.

Philipp Koehn and Rebecca Knowles. 2017. Six chal-
lenges for neural machine translation. In Pro-
ceedings of the First Workshop on Neural Machine
Translation, pages 28–39, Vancouver. Association
for Computational Linguistics.

Aviral Kumar and Sunita Sarawagi. 2019. Calibration
of encoder decoder models for neural machine trans-
lation. arXiv preprint arXiv:1903.00802.

Kenton Murray and David Chiang. 2018. Correct-
ing length bias in neural machine translation. In
Proceedings of the Third Conference on Machine

Translation: Research Papers, pages 212–223, Bel-
gium, Brussels. Association for Computational Lin-
guistics.

Jan Niehues, Eunah Cho, Thanh-Le Ha, and Alex
Waibel. 2017. Analyzing neural MT search and
model performance. In Proceedings of the First
Workshop on Neural Machine Translation, pages
11–17.

Myle Ott, Michael Auli, David Grangier, et al. 2018.
Analyzing uncertainty in neural machine translation.
In International Conference on Machine Learning,
pages 3953–3962.

Rico Sennrich, Barry Haddow, and Alexandra Birch.
2016. Neural machine translation of rare words
with subword units. In Proceedings of the 54th An-
nual Meeting of the Association for Computational
Linguistics (Volume 1: Long Papers), pages 1715–
1725, Berlin, Germany. Association for Computa-
tional Linguistics.

Pavel Sountsov and Sunita Sarawagi. 2016. Length
bias in encoder decoder models and a case for global
conditioning. In Proceedings of the 2016 Confer-
ence on Empirical Methods in Natural Language
Processing, pages 1516–1525, Austin, Texas. Asso-
ciation for Computational Linguistics.

Felix Stahlberg, Adrià de Gispert, and Bill Byrne.
2018a. The University of Cambridge’s machine
translation systems for WMT18. In Proceedings
of the Third Conference on Machine Translation:
Shared Task Papers, pages 504–512, Belgium, Brus-
sels. Association for Computational Linguistics.

Felix Stahlberg, Eva Hasler, Danielle Saunders, and
Bill Byrne. 2017. SGNMT – A flexible NMT de-
coding platform for quick prototyping of new mod-
els and search strategies. In Proceedings of the 2017
Conference on Empirical Methods in Natural Lan-
guage Processing: System Demonstrations, pages
25–30. Association for Computational Linguistics.

Felix Stahlberg, Danielle Saunders, Gonzalo Iglesias,
and Bill Byrne. 2018b. Why not be versatile? Ap-
plications of the SGNMT decoder for machine trans-
lation. In Proceedings of the 13th Conference of the
Association for Machine Translation in the Ameri-
cas (Volume 1: Research Papers), pages 208–216.
Association for Machine Translation in the Ameri-
cas.

Ilya Sutskever, Oriol Vinyals, and Quoc V Le. 2014.
Sequence to sequence learning with neural net-
works. In Advances in Neural Information Process-
ing Systems, pages 3104–3112.

Ashish Vaswani, Samy Bengio, Eugene Brevdo, Fran-
cois Chollet, Aidan N. Gomez, Stephan Gouws,
Llion Jones, Łukasz Kaiser, Nal Kalchbrenner, Niki
Parmar, Ryan Sepassi, Noam Shazeer, and Jakob
Uszkoreit. 2018. Tensor2tensor for neural machine
translation. CoRR, abs/1803.07416.

https://doi.org/10.18653/v1/N18-1205
https://doi.org/10.18653/v1/N18-1205
https://doi.org/10.18653/v1/D17-1227
https://doi.org/10.18653/v1/D17-1227
https://doi.org/10.18653/v1/D17-1227
https://doi.org/10.18653/v1/W15-3014
https://doi.org/10.18653/v1/W15-3014
http://aclweb.org/anthology/D13-1176
http://aclweb.org/anthology/D13-1176
https://doi.org/10.18653/v1/W17-3204
https://doi.org/10.18653/v1/W17-3204
https://www.aclweb.org/anthology/W18-6322
https://www.aclweb.org/anthology/W18-6322
https://doi.org/10.18653/v1/P16-1162
https://doi.org/10.18653/v1/P16-1162
https://doi.org/10.18653/v1/D16-1158
https://doi.org/10.18653/v1/D16-1158
https://doi.org/10.18653/v1/D16-1158
https://www.aclweb.org/anthology/W18-6427
https://www.aclweb.org/anthology/W18-6427
https://doi.org/10.18653/v1/D17-2005
https://doi.org/10.18653/v1/D17-2005
https://doi.org/10.18653/v1/D17-2005
http://aclweb.org/anthology/W18-1821
http://aclweb.org/anthology/W18-1821
http://aclweb.org/anthology/W18-1821
http://arxiv.org/abs/1803.07416
http://arxiv.org/abs/1803.07416


3362

Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob
Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz
Kaiser, and Illia Polosukhin. 2017. Attention is all
you need. In Advances in Neural Information Pro-
cessing Systems, pages 5998–6008.

Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V
Le, Mohammad Norouzi, Wolfgang Macherey,
Maxim Krikun, Yuan Cao, Qin Gao, Klaus
Macherey, et al. 2016. Google’s neural ma-
chine translation system: Bridging the gap between
human and machine translation. arXiv preprint
arXiv:1609.08144.

Yilin Yang, Liang Huang, and Mingbo Ma. 2018.
Breaking the beam search curse: A study of (re-)
scoring methods and stopping criteria for neural ma-
chine translation. In Proceedings of the 2018 Con-
ference on Empirical Methods in Natural Language
Processing, pages 3054–3059, Brussels, Belgium.
Association for Computational Linguistics.

https://www.aclweb.org/anthology/D18-1342
https://www.aclweb.org/anthology/D18-1342
https://www.aclweb.org/anthology/D18-1342

