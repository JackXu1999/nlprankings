



















































Gender Bias in Contextualized Word Embeddings


Proceedings of NAACL-HLT 2019, pages 629–634
Minneapolis, Minnesota, June 2 - June 7, 2019. c©2019 Association for Computational Linguistics

629

Gender Bias in Contextualized Word Embeddings

Jieyu Zhao§ Tianlu Wang† Mark Yatskar‡
Ryan Cotterellℵ Vicente Ordonez† Kai-Wei Chang§

§University of California, Los Angeles {jyzhao, kwchang}@cs.ucla.edu
†University of Virginia {tw8bc, vicente}@virginia.edu

‡Allen Institute for Artificial Intelligence marky@allenai.org
ℵUniversity of Cambridge rdc42@cam.ac.uk

Abstract

In this paper, we quantify, analyze and miti-
gate gender bias exhibited in ELMo’s contex-
tualized word vectors. First, we conduct sev-
eral intrinsic analyses and find that (1) train-
ing data for ELMo contains significantly more
male than female entities, (2) the trained ELMo
embeddings systematically encode gender in-
formation and (3) ELMo unequally encodes
gender information about male and female en-
tities. Then, we show that a state-of-the-art
coreference system that depends on ELMo in-
herits its bias and demonstrates significant bias
on the WinoBias probing corpus. Finally, we
explore two methods to mitigate such gender
bias and show that the bias demonstrated on
WinoBias can be eliminated.

1 Introduction

Distributed representations of words in the form
of word embeddings (Mikolov et al., 2013; Pen-
nington et al., 2014) and contextualized word em-
beddings (Peters et al., 2018; Devlin et al., 2018;
Radford et al., 2018; McCann et al., 2017; Radford
et al., 2019) have led to huge performance improve-
ment on many NLP tasks. However, several re-
cent studies show that training word embeddings in
large corpora could lead to encoding societal biases
present in these human-produced data (Bolukbasi
et al., 2016; Caliskan et al., 2017). In this work, we
extend these analyses to the ELMo contextualized
word embeddings.

Our work provides a new intrinsic analysis of
how ELMo represents gender in biased ways. First,
the corpus used for training ELMo has a significant
gender skew: male entities are nearly three times
more common than female entities, which leads to
gender bias in the downloadable pre-trained con-
textualized embeddings. Then, we apply princi-
pal component analysis (PCA) to show that after
training on such biased corpora, there exists a low-
dimensional subspace that captures much of the

gender information in the contextualized embed-
dings. Finally, we evaluate how faithfully ELMo
preserves gender information in sentences by mea-
suring how predictable gender is from ELMo repre-
sentations of occupation words that co-occur with
gender revealing pronouns. Our results show that
ELMo embeddings perform unequally on male and
female pronouns: male entities can be predicted
from occupation words 14% more accurately than
female entities.

In addition, we examine how gender bias in
ELMo propagates to the downstream applications.
Specifically, we evaluate a state-of-the-art coref-
erence resolution system (Lee et al., 2018) that
makes use of ELMo’s contextual embeddings on
WinoBias (Zhao et al., 2018a), a coreference di-
agnostic dataset that evaluates whether systems
behave differently on decisions involving male and
female entities of stereotyped or anti-stereotyped
occupations. We find that in the most challenging
setting, the ELMo-based system has a disparity in
accuracy between pro- and anti-stereotypical pre-
dictions, which is nearly 30% higher than a similar
system based on GloVe (Lee et al., 2017).

Finally, we investigate approaches for mitigating
the bias which propagates from the contextualized
word embeddings to a coreference resolution sys-
tem. We explore two different strategies: (1) a
training-time data augmentation technique (Zhao
et al., 2018a), where we augment the corpus for
training the coreference system with its gender-
swapped variant (female entities are swapped to
male entities and vice versa) and, afterwards, re-
train the coreference system; and (2) a test-time
embedding neutralization technique, where input
contextualized word representations are averaged
with word representations of a sentence with enti-
ties of the opposite gender. Results show that test-
time embedding neutralization is only partially ef-
fective, while data augmentation largely mitigates
bias demonstrated on WinoBias by the coreference



630

system.

2 Related Work

Gender bias has been shown to affect several real-
world applications relying on automatic language
analysis, including online news (Ross and Carter,
2011), advertisements (Sweeney, 2013), abusive
language detection (Park et al., 2018), machine
translation (Font and Costa-jussà, 2019; Vanmassen-
hove et al., 2018), and web search (Kay et al., 2015).
In many cases, a model not only replicates bias in
the training data but also amplifies it (Zhao et al.,
2017).

For word representations, Bolukbasi et al. (2016)
and Caliskan et al. (2017) show that word embed-
dings encode societal biases about gender roles and
occupations, e.g. engineers are stereotypically men,
and nurses are stereotypically women. As a con-
sequence, downstream applications that use these
pretrained word embeddings also reflect this bias.
For example, Zhao et al. (2018a) and Rudinger et al.
(2018) show that coreference resolution systems
relying on word embeddings encode such occupa-
tional stereotypes. In concurrent work, May et al.
(2019) measure gender bias in sentence embed-
dings, but their evaluation is on the aggregation
of word representations. In contrast, we analyze
bias in contextualized word representations and its
effect on a downstream task.

To mitigate bias from word embeddings, Boluk-
basi et al. (2016) propose a post-processing method
to project out the bias subspace from the pre-trained
embeddings. Their method is shown to reduce
the gender information from the embeddings of
gender-neutral words, and, remarkably, maintains
the same level of performance on different down-
stream NLP tasks. Zhao et al. (2018b) further pro-
pose a training mechanism to separate gender in-
formation from other factors. However, Gonen and
Goldberg (2019) argue that entirely removing bias
is difficult, if not impossible, and the gender bias
information can be often recovered. This paper
investigates a natural follow-up question: What are
effective bias mitigation techniques for contextual-
ized embeddings?

3 Gender Bias in ELMo

In this section we describe three intrinsic analyses
highlighting gender bias in trained ELMo contex-
tual word embeddings (Peters et al., 2018). We
show that (1) training data for ELMo contains sig-

#occurrence #M-biased occs. #F-biased occs.
M 5,300,000 170,000 81,000
F 1,600,000 33,000 36,000

Table 1: Training corpus for ELMo. We show to-
tal counts for male (M) and female (F) pronouns
in the corpus, and counts corresponding to their co-
occurrence with occupation words where the occupa-
tions are stereotypically male (M-biased) or female (F-
biased).

nificantly more male entities compared to female
entities leading to gender bias in the pre-trained
contextual word embeddings (2) the geometry of
trained ELMo embeddings systematically encodes
gender information and (3) ELMo propagates gen-
der information about male and female entities un-
equally.

3.1 Training Data Bias
Table 1 lists the data analysis on the One Billion
Word Benchmark (Chelba et al., 2013) corpus, the
training corpus for ELMo. We show counts for
the number of occurrences of male pronouns (he,
his and him) and female pronouns (she and her)
in the corpus as well as the co-occurrence of occu-
pation words with those pronouns. We use the set
of occupation words defined in the WinoBias cor-
pus and their assignments as prototypically male
or female (Zhao et al., 2018a). The analysis shows
that the Billion Word corpus contains a significant
skew with respect to gender: (1) male pronouns
occur three times more than female pronouns and
(2) male pronouns co-occur more frequently with
occupation words, irrespective of whether they are
prototypically male or female.

3.2 Geometry of Gender
Next, we analyze the gender subspace in ELMo.
We first sample 400 sentences with at least one gen-
dered word (e.g., he or she from the OntoNotes 5.0
dataset (Weischedel et al., 2012) and generate the
corresponding gender-swapped variants (changing
he to she and vice-versa). We then calculate the dif-
ference of ELMo embeddings between occupation
words in corresponding sentences and conduct prin-
cipal component analysis for all pairs of sentences.
Figure 1 shows there are two principal components
for gender in ELMo, in contrast to GloVe which
only has one (Bolukbasi et al., 2016). The two
principal components in ELMo seem to represent
the gender from the contextual information (Con-
textual Gender) as well as the gender embedded in
the word itself (Occupational Gender).



631

0 2 4 6 8
0.00

0.05

0.10

0.15

0.20

0.25

0.30

4 3 2 1 0 1 2 3
Contextual Gender

8

7

6

5

4

3

2

1

O
cc

up
at

io
na

l G
en

de
r

laywer
laywer

chief

chief

developer

developer

driver
driver

physician

physician

secretary

secretary

nurse
nurse

cashier
cashier

cleaner

cleaner

librarian
librarian

actress
waitress

princess
actor

waiter

prince

Figure 1: Left: Percentage of explained variance in PCA in the embedding differences. Right: Selected words
projecting to the first two principle components where the blue dots are the sentences with male context and the
orange dots are from the sentences with female context.

To visualize the gender subspace, we pick a few
sentence pairs from WinoBias (Zhao et al., 2018a).
Each sentence in the corpus contains one gendered
pronoun and two occupation words, such as “The
developer corrected the secretary because she made
a mistake” and also the same sentence with the op-
posite pronoun (he). In Figure 1 on the right, we
project the ELMo embeddings of occupation words
that are co-referent with the pronoun (e.g. secre-
tary in the above example) for when the pronoun
is male (blue dots) and female (orange dots) on
the two principal components from the PCA analy-
sis. Qualitatively, we can see the first component
separates male and female contexts while the sec-
ond component groups male related words such
as lawyer and developer and female related words
such as cashier and nurse.

3.3 Unequal Treatment of Gender
To test how ELMo embeds gender information in
contextualized word embeddings, we train a clas-
sifier to predict the gender of entities from occu-
pation words in the same sentence. We collect
sentences containing gendered words (e.g., he-she,
father-mother) and occupation words (e.g., doc-
tor)1 from the OntoNotes 5.0 corpus (Weischedel
et al., 2012), where we treat occupation words as a
mention to an entity, and the gender of that entity
is taken to the gender of a co-referring gendered
word, if one exists. For example, in the sentence
“the engineer went back to her home,” we take engi-
neer to be a female mention. Then we split all such
instances into training and test, with 539 and 62 in-
stances, respectively and augment these sentences
by swapping all the gendered words with words of
the opposite gender such that the numbers of male

1We use the list collected in (Zhao et al., 2018a)

and female entities are balanced.
We first test if ELMo embedding vectors carry

gender information. We train an SVM classifier
with an RBF kernel2 to predict the gender of a men-
tion (i.e., an occupation word) based on its ELMo
embedding. On development data, this classifier
achieves 95.1% and 80.6% accuracy on sentences
where the true gender was male and female respec-
tively. For both male and female contexts, the accu-
racy is much larger than 50%, demonstrating that
ELMo does propagate gender information to other
words. However, male information is more than
14% more accurately represented in ELMo than fe-
male information, showing that ELMo propagates
the information unequally for male and female en-
tities.

4 Bias in Coreference Resolution

In this section, we establish that coreference sys-
tems that depend on ELMo embeddings exhibit
significant gender bias. Then we evaluate two sim-
ple methods for removing the bias from the systems
and show that the bias can largely be reduced.

4.1 Setup
We evaluate bias with respect to the WinoBias
dataset (Zhao et al., 2018a), a benchmark of paired
male and female coreference resolution examples
following the Winograd format (Hirst, 1981; Rah-
man and Ng, 2012; Peng et al., 2015). It contains
two different subsets, pro-stereotype, where pro-
nouns are associated with occupations predomi-
nately associated with the gender of the pronoun, or
anti-stereotype, when the opposite relation is true.

2We use the ν-SVC formulation and tune the hyper-
parameter ν (Chang and Lin, 2011) in the range of [0.1, 1]
with a step 0.1.



632

Embeddings Data Augmentation Neutralization OntoNotes Semantics Only w/ Syntactic CuesGloVe ELMo Pro. Anti. Avg. | Diff | Pro. Anti. Avg. | Diff |
GloVe 67.7 76.0 49.4 62.7 26.6* 88.7 75.2 82.0 13.5*
GloVe 65.8 63.9 62.8 63.4 1.1 81.3 83.4 82.4 2.1

GloVe+ELMo 72.7 79.1 49.5 64.3 29.6* 93.0 85.9 89.5 7.1*
GloVe+ELMo 71.0 65.9 64.9 65.4 1.0 87.8 88.9 88.4 1.2
GloVe+ELMo 71.0 72.6 57.8 64.9 14.3* 90.2 88.6 89.4 1.6
GloVe+ELMo 71.1 71.7 60.6 66.2 11.1* 90.3 89.2 89.8 1.1

Table 2: F1 on OntoNotes and WinoBias development sets. WinoBias dataset is split Semantics Only and w/
Syntactic Cues subsets. ELMo improves the performance on the OntoNotes dataset by 5% but shows stronger bias
on the WinoBias dataset. Avg. stands for averaged F1 score on the pro- and anti-stereotype subsets while “Diff.”
is the absolute difference between these two subsets. * indicates the difference between pro/anti stereotypical
conditions is significant (p < .05) under an approximate randomized test (Graham et al., 2014). Mitigating bias
by data augmentation reduces all the bias from the coreference model to a neglect level. However, the neutralizing
ELMo approach only mitigates bias when there are other strong learning signals for the task.

Each subset consists of two types of sentences: one
that requires semantic understanding of the sen-
tence to make coreference resolution (Semantics
Only) and another that relies on syntactic cues (w/
Syntactic Cues). Gender bias is measured by taking
the difference of the performance in pro- and anti-
stereotypical subsets. Previous work (Zhao et al.,
2018a) evaluated the systems based on GloVe em-
beddings but here we evaluate a state-of-the-art
system that trained on the OntoNotes corpus with
ELMo embeddings (Lee et al., 2018).

4.2 Bias Mitigation Methods
Next, we describe two methods for mitigating bias
in ELMo for the purpose of coreference resolution:
(1) a train-time data augmentation approach and
(2) a test-time neutralization approach.

Data Augmentation Zhao et al. (2018a) propose
a method to reduce gender bias in coreference res-
olution by augmenting the training corpus for this
task. Data augmentation is performed by replacing
gender revealing entities in the OntoNotes dataset
with words indicating the opposite gender and then
training on the union of the original data and this
swapped data. In addition, they find it useful to
also mitigate bias in supporting resources and there-
fore replace standard GloVe embeddings with bias
mitigated word embeddings from Bolukbasi et al.
(2016). We evaluate the performance of both as-
pects of this approach.

Neutralization We also investigate an approach
to mitigate bias induced by ELMo embeddings
without retraining the coreference model. Instead
of augmenting training corpus by swapping gender
words, we generate a gender-swapped version of
the test instances. We then apply ELMo to obtain
contextualized word representations of the original

and the gender-swapped sentences and use their
average as the final representations.

4.3 Results
Table 2 summarizes our results on WinoBias.

ELMo Bias Transfers to Coreference Row 3
in Table 2 summarizes performance of the ELMo
based coreference system on WinoBias. While
ELMo helps to boost the coreference resolution F1
score (OntoNotes) it also propagates bias to the
task. It exhibits large differences between pro- and
anti-stereotyped sets (|Diff|) on both semantic and
syntactic examples in WinoBias.

Bias Mitigation Rows 4-6 in Table 2 summa-
rize the effectiveness of the two bias mitigation
approaches we consider. Data augmentation is
largely effective at mitigating bias in the corefer-
ence resolution system with ELMo (reducing |Diff
| to insignificant levels) but requires retraining the
system. Neutralization is less effective than aug-
mentation and cannot fully remove gender bias
on the Semantics Only portion of WinoBias, indi-
cating it is effective only for simpler cases. This
observation is consistent with Gonen and Goldberg
(2019), where they show that entirely removing
bias from an embedding is difficult and depends on
the manner, by which one measures the bias.

5 Conclusion and Future Work

Like word embedding models, contextualized word
embeddings inherit implicit gender bias. We ana-
lyzed gender bias in ELMo, showing that the cor-
pus it is trained on has significant gender skew and
that ELMo is sensitive to gender, but unequally so
for male and female entities. We also showed this
bias transfers to downstream tasks, such as corefer-
ence resolution, and explored two bias mitigation



633

strategies: 1) data augmentation and 2) neutralizing
embeddings, effectively eliminating the bias from
ELMo in a state-of-the-art system. With increasing
adoption of contextualized embeddings to get bet-
ter results on core NLP tasks, e.g. BERT (Devlin
et al., 2018), we must be careful how such unsu-
pervised methods perpetuate bias to downstream
applications and our work forms the basis of evalu-
ating and mitigating such bias.

Acknowledgement

This work was supported in part by National Sci-
ence Foundation Grant IIS-1760523. RC was sup-
ported by a Facebook Fellowship. We also ac-
knowledge partial support from the Institute of the
the Humanities and Global Cultures at the Univer-
sity of Virginia. We thank all reviewers for their
comments.

References
Tolga Bolukbasi, Kai-Wei Chang, James Zou,

Venkatesh Saligrama, and Adam Kalai. 2016.
Man is to computer programmer as woman is to
homemaker? debiasing word embeddings. In
NeurIPS.

Aylin Caliskan, Joanna J Bryson, and Arvind
Narayanan. 2017. Semantics derived automatically
from language corpora contain human-like biases.
Science, 356(6334):183–186.

Chih-Chung Chang and Chih-Jen Lin. 2011. Libsvm:
a library for support vector machines. ACM transac-
tions on intelligent systems and technology (TIST),
2(3):27.

Ciprian Chelba, Tomas Mikolov, Mike Schuster, Qi Ge,
Thorsten Brants, Phillipp Koehn, and Tony Robin-
son. 2013. One billion word benchmark for measur-
ing progress in statistical language modeling. arXiv
preprint arXiv:1312.3005.

Jacob Devlin, Ming-Wei Chang, Kenton Lee, and
Kristina Toutanova. 2018. BERT: Pre-training of
deep bidirectional transformers for language under-
standing. arXiv preprint arXiv:1810.04805.

Joel Escudé Font and Marta R. Costa-jussà. 2019.
Equalizing gender biases in neural machine trans-
lation with word embeddings techniques. arXiv
preprint arXiv:1901.03116.

Hila Gonen and Yoav Goldberg. 2019. Lipstick on a
pig: Debiasing methods cover up systematic gender
biases in word embeddings but do not remove them.
CoRR, abs/1903.03862.

Yvette Graham, Nitika Mathur, and Timothy Baldwin.
2014. Randomized significance tests in machine
translation. In WMT@ ACL.

Graeme Hirst. 1981. Anaphora in natural language un-
derstanding. Berlin Springer Verlag.

Matthew Kay, Cynthia Matuszek, and Sean A. Munson.
2015. Unequal representation and gender stereo-
types in image search results for occupations. In
Human Factors in Computing Systems. ACM.

Kenton Lee, Luheng He, Mike Lewis, and Luke Zettle-
moyer. 2017. End-to-end neural coreference resolu-
tion. In EMNLP.

Kenton Lee, Luheng He, and Luke S. Zettlemoyer.
2018. Higher-order coreference resolution with
coarse-to-fine inference. In NAACL.

Chandler May, Alex Wang, Shikha Bordia, Samuel R.
Bowman, and Rachel Rudinger. 2019. On mea-
suring social biases in sentence encoders. arXiv
preprint arXiv:1903.10561.

Bryan McCann, James Bradbury, Caiming Xiong, and
Richard Socher. 2017. Learned in translation: Con-
textualized word vectors. In NeurIPS.

Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Cor-
rado, and Jeff Dean. 2013. Distributed representa-
tions of words and phrases and their compositional-
ity. In NeurIPS.

Ji Ho Park, Jamin Shin, and Pascale Fung. 2018. Re-
ducing gender bias in abusive language detection. In
EMNLP.

Haoruo Peng, Daniel Khashabi, and Dan Roth. 2015.
Solving hard coreference problems. In NAACL.

Jeffrey Pennington, Richard Socher, and Christopher
Manning. 2014. Glove: Global vectors for word rep-
resentation. In EMNLP.

Matthew E. Peters, Mark Neumann, Mohit Iyyer, Matt
Gardner, Christopher Clark, Kenton Lee, and Luke
Zettlemoyer. 2018. Deep contextualized word repre-
sentations. In NAACL.

Alec Radford, Karthik Narasimhan, Tim Salimans, and
Ilya Sutskever. 2018. Improving language under-
standing by generative pre-training.

Alec Radford, Jeff Wu, Rewon Child, David Luan,
Dario Amodei, and Ilya Sutskever. 2019. Language
models are unsupervised multitask learners.

Altaf Rahman and Vincent Ng. 2012. Resolving
complex cases of definite pronouns: The winograd
schema challenge. In EMNLP.

Karen Ross and Cynthia Carter. 2011. Women and
news: A long and winding road. Media, Culture
& Society, 33(8).

Rachel Rudinger, Jason Naradowsky, Brian Leonard,
and Benjamin Van Durme. 2018. Gender bias in
coreference resolution. In NAACL.



634

Latanya Sweeney. 2013. Discrimination in online ad
delivery. Queue, 11(3):10.

Eva Vanmassenhove, Christian Hardmeier, and Andy
Way. 2018. Getting Gender Right in Neural Ma-
chine Translation. In EMNLP.

Ralph Weischedel, Sameer Pradhan, Lance Ramshaw,
Jeff Kaufman, Michelle Franchini, Mohammed El-
Bachouti, Nianwen Xue, Martha Palmer, Jena D
Hwang, Claire Bonial, et al. 2012. Ontonotes re-
lease 5.0.

Jieyu Zhao, Tianlu Wang, Mark Yatskar, Vicente Or-
donez, and Kai-Wei Chang. 2017. Men also like
shopping: Reducing gender bias amplification using
corpus-level constraints. In EMNLP.

Jieyu Zhao, Tianlu Wang, Mark Yatskar, Vicente Or-
donez, and Kai-Wei Chang. 2018a. Gender bias
in coreference resolution: Evaluation and debiasing
methods. In NAACL.

Jieyu Zhao, Yichao Zhou, Zeyu Li, Wei Wang, and Kai-
Wei Chang. 2018b. Learning gender-neutral word
embeddings. In EMNLP.


