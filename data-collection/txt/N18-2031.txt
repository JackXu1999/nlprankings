



















































Frustratingly Easy Meta-Embedding – Computing Meta-Embeddings by Averaging Source Word Embeddings


Proceedings of NAACL-HLT 2018, pages 194–198
New Orleans, Louisiana, June 1 - 6, 2018. c©2018 Association for Computational Linguistics

Frustratingly Easy Meta-Embedding – Computing Meta-Embeddings by
Averaging Source Word Embeddings

Joshua N Coates
Department of Computer Science

University of Liverpool
j.n.coates@liverpool.ac.uk

Danushka Bollegala
Department of Computer Science

University of Liverpool
danushka@liverpool.ac.uk

Abstract

Creating accurate meta-embeddings from pre-
trained source embeddings has received at-
tention lately. Methods based on global and
locally-linear transformation and concatena-
tion have shown to produce accurate meta-
embeddings. In this paper, we show that the
arithmetic mean of two distinct word embed-
ding sets yields a performant meta-embedding
that is comparable or better than more com-
plex meta-embedding learning methods. The
result seems counter-intuitive given that vector
spaces in different source embeddings are not
comparable and cannot be simply averaged.
We give insight into why averaging can still
produce accurate meta-embedding despite the
incomparability of the source vector spaces.

1 Introduction

Distributed vector representations of words,
henceforth referred to as word embeddings, have
been shown to exhibit strong performance on a
variety of NLP tasks (Turian et al., 2010; Zou
et al., 2013). Methods for producing word em-
bedding sets exploit the distributional hypothesis
to infer semantic similarity between words within
large bodies of text, in the process they have been
found to additionally capture more complex lin-
guistic regularities, such as analogical relation-
ships (Mikolov et al., 2013c). A variety of meth-
ods now exist for the production of word embed-
dings (Collobert and Weston, 2008; Mnih and Hin-
ton, 2009; Huang et al., 2012; Pennington et al.,
2014; Mikolov et al., 2013a). Comparative work
has illustrated a variation in performance between
methods across evaluative tasks (Chen et al., 2013;
Yin and Schütze, 2016).

Methods of “meta-embedding”, as first pro-
posed by Yin and Schütze (2016), aim to con-
duct a complementary combination of informa-
tion from an ensemble of distinct word embedding

sets, each trained using different methods, and re-
sources, to yield an embedding set with improved
overall quality.

Several such methods have been proposed.
1TON (Yin and Schütze, 2016), takes an ensem-
ble of K pre-trained word embedding sets, and
employs a linear neural network to learn a set of
meta-embeddings along with K global projection
matrices, such that through projection, for every
word in the meta-embedding set, we can recover
its corresponding vector within each source word
embedding set. 1TON+ (Yin and Schütze, 2016),
extends this method by predicting embeddings for
words not present within the intersection of the
source word embedding sets. An unsupervised lo-
cally linear meta-embedding approach has since
been taken (Bollegala et al., 2017), for each source
embedding set, for each word; a representation
as a linear combination of its nearest neighbours
is learnt. The local reconstructions within each
source embedding set are then projected to a com-
mon meta-embedding space.

The simplest approach considered to date, has
been to concatenate the word embeddings across
the source sets (Yin and Schütze, 2016). Despite
its simplicity, concatenation has been used to pro-
vide a good baseline of performance for meta-
embedding.

A method which has not yet been proposed is
to conduct a direct averaging of embeddings. The
validity of this approach may perhaps not seem ob-
vious, owing to the fact that no correspondence ex-
ists between the dimensions of separately trained
word embedding sets. In this paper we first pro-
vide some analysis and justification that, despite
this dimensional disparity, averaging can provide
an approximation of the performance of concate-
nation without increasing the dimension of the em-
beddings. We give empirical results demonstrat-
ing the quality of average meta-embeddings. We

194



make a point of comparison to concatenation since
it is the most comparable in terms of simplicity,
whilst also providing a good baseline of perfor-
mance on evaluative tasks. Our aim is to highlight
the validity of averaging across distinct word em-
bedding sets, such that it may be considered as a
tool in future meta-embedding endeavours.

2 Analysis

To evaluate semantic similarity between word em-
beddings we consider the Euclidean distance mea-
sure. For `2 normalised word embeddings, Eu-
clidean distance is a monotonically decreasing
function of the cosine similarity, which is a popu-
lar choice in NLP tasks that use word embeddings
such as semantic similarity prediction and analogy
detection (Levy et al., 2015; Levy and Goldberg,
2014). We defer the analysis of other types of
distance measures to future work. By evaluating
the relationship between the Euclidean distances
of pairs of words in the source embedding sets
and their corresponding Euclidean distances in the
meta-embedding space we can obtain a view as
to how the meta-embedding procedure is combin-
ing semantic information. We begin by examining
concatenation through this lens, before moving on
to averaging.

2.1 Concatenation
We can express concatenation by first zero-
padding our source embeddings, before combin-
ing them through addition.

Without loss of generality, we consider both
concatenation and averaging over only two source
word embedding sets for ease of exposition. Let
S1 and S2 be unique embedding sets of real-valued
continuous embeddings. We make no assumption
that S1 and S2 were trained using the same method
or resources. Consider two semantically similar
words u and v such that u,v ∈ S1 ∩ S2. Let uS1
and vS1 , and uS2 and vS2 denote the specific word
embeddings of u and v within the embeddings S1,
and S2 respectively.

Let the dimensions of embeddings S1, and S2
be denoted dS1 , and dS2 respectively. We zero-
pad embeddings from S1 by front-loading dS2 zero
entries to each word embedding vector. In con-
trast, we zero-pad embeddings from S2 by adding
dS1 zero entries to the end of each embedding vec-
tor. The resulting embeddings from S1 and S2 now
share a common dimension of dS1 + dS2 . Denote

the resulting embeddings of any word u ∈ S1∩S2,
as uzeroS1 and u

zero
S2 respectively. Now, combining

our source embeddings through addition we obtain
equivalency to concatenation.

uzeroS1 + u
zero
S2 =




uS2(1)
uS2(2)

...
uS2(dS2 )
uS1(1)
uS1(2)

...
uS1(dS1 )




=

[
uS2
uS1

]
(1)

Note that the zero-padded vectors are orthogonal.
Let the Euclidean distance between these words

in each embedding be denoted by ES1 and ES2 .
Note that for any vector u ∈ Rn the addition of
zero-valued dimensions does not affect the value
of its `2-norm. So we have

ES1 = ||uS1 − vS1 ||2 =
∣∣∣∣uzeroS1 − vzeroS1

∣∣∣∣
2

(2)

ES2 = ||uS2 − vS2 ||2 =
∣∣∣∣uzeroS2 − vzeroS2

∣∣∣∣
2

(3)

Consider the Euclidean distance between u and v
after concatenation.

ECONC

=

∣∣∣∣
∣∣∣∣
[
uS2
uS1

]
−
[
vS2
vS1

]∣∣∣∣
∣∣∣∣
2

=
∣∣∣∣(uzeroS1 + uzeroS2 )− (vzeroS1 + vzeroS2 )

∣∣∣∣
2

=
∣∣∣∣(uzeroS1 − vzeroS1 )− (vzeroS2 − uzeroS2 )

∣∣∣∣
2

=
√

(ES1)2 + (ES2)2 − 2ES1ES2cos(θ)

=
√

(ES1)2 + (ES2)2 − 2ES1ES2(0)

=
√

(ES1)2 + (ES2)2

For any two words belonging to the resultant em-
bedding obtained by concatenation, the distance
between these words in the resultant space is the
root of the sum of squares of Euclidean distances
between these words in S1 and S2.

2.2 Average word embeddings
Here we now make the assumption that S1 and S2
have common dimension d.1

1Without loss of generality, source embeddings with dif-
ferent dimensionality can be appropriately padded to have the
same dimensionality.

195



Despite there being no obvious correspondence
between dimensions of S1 and S2 we can show
that the average embedding set retains semantic
information through preservation of the relative
distances between words.

Consider the positioning of words u, and v af-
ter performing a word-wise average between the
source embedding sets. The Euclidean distance
between u and v in the resultant meta-embedding
is given by

EAV G

=

∣∣∣∣
∣∣∣∣
(uS1 + uS2)

2
− (vS1 + vS2)

2

∣∣∣∣
∣∣∣∣
2

=
1

2
||(uS1 − vS1)− (vS2 − uS2)||2

∝
√
(ES1)2 + (ES2)2 − 2ES1ES2 cos(θ)

Now in this case, unlike concatenation, we have
not designed our source embedding sets such that
they are orthogonal to each other, and so it seems
we are left with a term dependant on the angle be-
tween (uS1−vS1) and (vS2−uS2). However, Cai
et al. (2013) showed that, if X is a set of random
points ∈ Rn with cardinality |X |, then the limit-
ing distribution of angles, as |X | → ∞, between
pairs of elements from X , is Gaussian with mean
π/2. In addition, Cai et al. (2013) showed that the
variance of this distribution shrinks as the dimen-
sionality increases.

Word embedding sets typically contain in the
order of ten thousand or more points, and are typ-
ically of relatively high dimension. Moreover,
assuming the difference vector between any two
words in an embedding set is sufficiently random,
we may approximate the limiting Gaussian distri-
bution described by Cai et al. (2013). In such a
case the expectation would then be that the vec-
tors (uS1 − vS1) and (vS2 −uS2) are orthogonal,
leading to the following result.

E[EAV G] =
1

2

√
(ES1)2 + (ES2)2 ∝ ECONC

(4)

To summarise, if word embeddings can be shown
to be approximately orthogonal, then averaging
will approximate the same information as concate-
nation, without increasing the dimensionality of
the embeddings.

3 Experiments

We first empirically test our theory that word
embeddings are sufficiently random and high di-
mensional, such that they are approximately all
orthogonal to each other. We then present an
empirical evaluation of the performance of the
meta-embeddings produced through averaging,
and compare against concatenation.

3.1 Datasets
We use the following pre-trained embedding sets
that have been used in prior work on meta-
embedding learning (Yin and Schütze, 2016; Bol-
legala et al., 2017) for experimentation.

• GloVe (Pennington et al., 2014). 1,917,494
word embeddings of dimension 300.

• CBOW (Mikolov et al., 2013b). Phrase em-
beddings discarded, leaving 929,922 word
embeddings of dimension 300.

• HLBL (Turian et al., 2010). 246,122 hierar-
chical log-bilinear (Mnih and Hinton, 2009)
word embeddings of dimension 100.

Note that the purpose of this experiment is not
to compare against previously proposed meta-
embedding learning methods, but to empirically
verify averaging as a meta-embedding method and
validate the assumptions behind the theoretical
analysis. By using three pre-trained word em-
beddings with different dimensionalities and em-
pirical accuracies, we can evaluate the averaging-
based meta-embeddings in a robust manner.

We pad HLBL embeddings to the rear with 200
zero-entries to bring their dimension up to 300.
For GloVe, we `2 normalise each dimension of
the embedding across the vocabulary, as recom-
mended by the authors. Every individual word
embedding from each embedding set is then `2-
normalised. The proposed averaging operation, as
well as concatenation, operate only on the inter-
section of these embeddings. The intersectional
vocabularies GloVe ∩ CBOW, GloVe ∩ HLBL,
and CBOW∩HLBL contain 154,076; 90,254; and
140,479 word embeddings respectively.

3.2 Empirical distribution analysis
We conduct an empirical analysis of the distribu-
tion of the angle ^[(uS1 − vS1), (vS2 − uS2)]
for each pair of datasets. Table 1 shows the
mean and variance of these distributions, obtained

196



0.75 1.00 1.25 1.50 1.75 2.00 2.25

Angle in Radians.

0.0

0.5

1.0

1.5

2.0

2.5

3.0

3.5

Figure 1: Distribution of angles between embeddings
within GloVe ∩ CBOW.
from samples of 200,000 random pairs of words
from each intersectional vocabulary. We find that
the angles are approximately normally distributed
around π/2.

Embeddings µ σ2

GloVe & CBOW 1.5609 0.0121
GloVe & HLBL 1.5709 0.0129
CBOW & HLBL 1.5740 0.0126

Table 1: Observed distribution parameters.

Figure 1 shows a normalised histogram of the
results for GloVe ∩ CBOW, along with a nor-
mal distribution characterised by the sample mean
and variance. GloVe ∩ HLBL, and CBOW ∩
HLBL plots are not shown due to space limita-
tions, but are similarly normally distributed. This
result shows that the pre-trained word embeddings
approximately satisfy the predictions made by Cai
et al. (2013), thereby empirically justifying the as-
sumption made in the derivation of (4).

3.3 Evaluation Tasks
3.3.1 Semantic Similarity
We measure the similarity between words by cal-
culating the cosine similarity between their em-
beddings; we then calculate Spearman correlation
against human similarity scores. The following
datasets are used: RG (Rubenstein and Goode-
nough, 1965), MC (Miller and Charles, 1991),
WS (Finkelstein et al., 2001), RW (Luong et al.,
2013), and SL (Hill et al., 2015).

3.3.2 Word Analogy
Using the Google dataset GL (Mikolov et al.,
2013b) (19544 analogy questions), we solve ques-
tions of the form a is to b as c is to what?, using

Embeddings RG MC WS RW SL GL

sources
HLBL 100 35.3 49.3 35.7 19.1 22.1 15.0
CBOW 300 76.0 82.2 69.8 53.4 44.2 67.1
GloVe 300 82.9 87.0 75.4 48.7 45.3 68.7

AVG
CBOW+HLBL 300 69.2 81.0 60.1 48.7 37.3 49.4
GloVe+CBOW 300 82.2 87.0 74.5 52.9 46.5 73.8
GloVe+HLBL 300 73.7 74.1 64.2 44.6 38.8 49.5

CONC
CBOW+HLBL 400 68.7 80.2 62.9 49.1 39.6 53.2
GloVe+CBOW 600 83.0 88.8 76.4 54.8 46.3 75.5
GloVe+HLBL 400 73.7 80.1 65.5 46.4 40.0 53.8

Table 2: Results on word similarity, and analogical
tasks. Best performances bolded per task. Dimension-
ality of the meta embedding is shown next to the source
embedding names.

the CosAdd method (Mikolov et al., 2013c) shown
in (5). Specifically, we determine a fourth word d
such that the similarity between (b− a+ c) and d
is maximised.

CosAdd(a : b, c : d) = cos(b− a+ c, d) (5)

3.4 Discussion of results
Table 2 shows task performance for each source
embedding set, and for both methods on every pair
of datasets. In our experiments concatenation ob-
tains better overall performance. However, aver-
aging offers improvements over the source embed-
ding sets for semantic similarity task SL and word
analogy task GL, on the combination of CBOW
and GloVe. HLBL has a negative effect on CBOW
and GloVe, but the performance of averaging is
close to that of concatenation. An advantage of
averaging when compared against concatenation,
is that the dimensionality of the produced meta-
embedding is not increased beyond the maximum
dimension present within the source embeddings,
resulting in a meta-embedding which is easier to
process and store.

4 Conclusion

We have presented an argument for averaging as
a valid meta-embedding technique, and found ex-
perimental performance to be close to, or in some
cases better than that of concatenation, with the
additional benefit of reduced dimensionality. We
propose that when conducting meta-embedding,
both concatenation and averaging should be con-
sidered as methods of combining embedding
spaces, and their individual advantages consid-
ered.

197



References
Danushka Bollegala, Kohei Hayashi, and Ken-ichi

Kawarabayashi. 2017. Think globally, embed
locally—locally linear meta-embedding of words.
arXiv preprint arXiv:1709.06671 .

Tony Cai, Jianqing Fan, and Tiefeng Jiang. 2013.
Distributions of angles in random packing on
spheres. The Journal of Machine Learning Research
14(1):1837–1864.

Yanqing Chen, Bryan Perozzi, Rami Al-Rfou, and
Steven Skiena. 2013. The expressive power of word
embeddings. arXiv preprint arXiv:1301.3226 .

Ronan Collobert and Jason Weston. 2008. A unified
architecture for natural language processing: Deep
neural networks with multitask learning. In Pro-
ceedings of the 25th international conference on
Machine learning. ACM, pages 160–167.

Lev Finkelstein, Evgeniy Gabrilovich, Yossi Matias,
Ehud Rivlin, Zach Solan, Gadi Wolfman, and Eytan
Ruppin. 2001. Placing search in context: The con-
cept revisited. In Proceedings of the 10th interna-
tional conference on World Wide Web. ACM, pages
406–414.

Felix Hill, Roi Reichart, and Anna Korhonen. 2015.
Simlex-999: Evaluating semantic models with (gen-
uine) similarity estimation. Computational Linguis-
tics 41(4):665–695.

Eric H Huang, Richard Socher, Christopher D Man-
ning, and Andrew Y Ng. 2012. Improving word
representations via global context and multiple word
prototypes. In Proceedings of the 50th Annual Meet-
ing of the Association for Computational Linguis-
tics: Long Papers-Volume 1. Association for Com-
putational Linguistics, pages 873–882.

Omer Levy and Yoav Goldberg. 2014. Linguistic reg-
ularities in sparse and explicit word representations.
In CoNLL.

Omer Levy, Yoav Goldberg, and Ido Dagan. 2015. Im-
proving distributional similarity with lessons learned
from word embeddings. Transactions of Association
for Computational Linguistics 3:211–225.

Thang Luong, Richard Socher, and Christopher D
Manning. 2013. Better word representations with
recursive neural networks for morphology. In
CoNLL. pages 104–113.

Tomas Mikolov, Kai Chen, Greg Corrado, and Jef-
frey Dean. 2013a. Efficient estimation of word
representations in vector space. arXiv preprint
arXiv:1301.3781 .

Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Cor-
rado, and Jeff Dean. 2013b. Distributed representa-
tions of words and phrases and their compositional-
ity. In Advances in neural information processing
systems. pages 3111–3119.

Tomas Mikolov, Wen-tau Yih, and Geoffrey Zweig.
2013c. Linguistic regularities in continuous space
word representations. In Hlt-naacl. volume 13,
pages 746–751.

George A Miller and Walter G Charles. 1991. Contex-
tual correlates of semantic similarity. Language and
cognitive processes 6(1):1–28.

Andriy Mnih and Geoffrey E Hinton. 2009. A scal-
able hierarchical distributed language model. In
Advances in neural information processing systems.
pages 1081–1088.

Jeffrey Pennington, Richard Socher, and Christopher D
Manning. 2014. Glove: Global vectors for word
representation. In EMNLP. volume 14, pages 1532–
1543.

Herbert Rubenstein and John B Goodenough. 1965.
Contextual correlates of synonymy. Communica-
tions of the ACM 8(10):627–633.

Joseph Turian, Lev Ratinov, and Yoshua Bengio. 2010.
Word representations: a simple and general method
for semi-supervised learning. In Proceedings of the
48th annual meeting of the association for compu-
tational linguistics. Association for Computational
Linguistics, pages 384–394.

Wenpeng Yin and Hinrich Schütze. 2016. Learning
word meta-embeddings. In Proceedings of the 54th
Annual Meeting of the Association for Computa-
tional Linguistics (Volume 1: Long Papers). Asso-
ciation for Computational Linguistics, Berlin, Ger-
many, pages 1351–1360.

Will Y Zou, Richard Socher, Daniel M Cer, and
Christopher D Manning. 2013. Bilingual word em-
beddings for phrase-based machine translation. In
EMNLP. pages 1393–1398.

198


