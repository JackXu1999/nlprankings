



















































More Constructions, More Genres: Extending Stanford Dependencies


Proceedings of the Second International Conference on Dependency Linguistics (DepLing 2013), pages 187–196,
Prague, August 27–30, 2013. c© 2013 Charles University in Prague, Matfyzpress, Prague, Czech Republic

More constructions, more genres: Extending Stanford Dependencies
Marie-Catherine de Marneffe∗, Miriam Connor, Natalia Silveira,
Samuel R. Bowman, Timothy Dozat and Christopher D. Manning

∗Linguistics Department
The Ohio State University

Columbus, OH 43210
mcdm@ling.osu.edu

Linguistics Department
Stanford University
Stanford, CA 94305

{mkconnor,natalias,sbowman,
tdozat,manning}@stanford.edu

Abstract

The Stanford dependency scheme aims to
provide a simple and intuitive but linguis-
tically sound way of annotating the depen-
dencies between words in a sentence. In
this paper, we address two limitations the
scheme has suffered from: First, despite
providing good coverage of core gram-
matical relations, the scheme has not of-
fered explicit analyses of more difficult
syntactic constructions; second, because
the scheme was initially developed primar-
ily on newswire data, it did not focus on
constructions that are rare in newswire but
very frequent in more informal texts, such
as casual speech and current web texts.
Here, we propose dependency analyses for
several linguistically interesting construc-
tions and extend the scheme to provide
better coverage of modern web data.

1 Introduction

The Stanford dependency representation (de
Marneffe et al. 2006, de Marneffe and Manning
2008b, henceforth SD) has seen wide usage within
the Natural Language Processing (NLP) commu-
nity as a standard for English grammatical rela-
tions, and its leading ideas are being adapted for
other languages. This adaptation seems to be mo-
tivated by two principal advantages: (i) it provides
a richer, more linguistically faithful typology of
dependencies than the main alternatives and (ii) it
adopts a simple, understandable, and uniform no-
tation of dependency triples, close to traditional
grammar. This combination makes it effective
both for use by non-linguists working directly with
linguistic information in the development of natu-
ral language understanding applications and also
as a source of features for machine learning ap-
proaches. As a result, the representation has been

variously used in relation extraction, text under-
standing, and machine translation applications.

While SD provides good coverage of core gram-
matical relations, such as subject, object, internal
noun phrase relations, and adverbial and subordi-
nate clauses, the standard remains underdeveloped
and agnostic as to the treatment of many of the
more difficult—albeit rarer—constructions that
tend to dominate discussions of syntax in linguis-
tics, such as tough adjectives, free relatives, com-
parative constructions, and small clauses. These
constructions have been analyzed many times in
various frameworks for constituency representa-
tion, and some have had some limited treatment
in dependency grammar frameworks. Neverthe-
less, it is often not obvious how to analyze them
in terms of dependencies, and currently the SD
scheme does not offer explicit, principled analy-
ses of these constructions.

Further, a current practical limitation is that
the SD scheme was developed against newswire
data, namely the Wall Street Journal portion of
the Penn Treebank. It therefore gave relatively lit-
tle consideration to constructions that are absent
or rare in newswire, such as questions, impera-
tives, discourse particles, sentence fragments, el-
lipsis, and various kinds of list structures. Such
constructions are, however, abundant in modern
web texts. Emails, blogs, forum posts, and product
reviews show a greater use of informal construc-
tions, slang, and emoticons. It is important to han-
dle these new genres by providing adequate depen-
dency representations of the constructions which
appear in such important modern genres.

Our goal in this paper is to address these two
current limitations of Stanford dependencies. We
extend the scheme to handle a wider array of lin-
guistic constructions, both linguistically interest-
ing constructions and those necessary to resolve
practical problems in providing analyses for lan-
guage use in modern web data.

187



2 The Stanford dependencies

The set of grammatical relations used by SD is
principally drawn from the grammatical-relation
oriented traditions in American linguistics: Re-
lational Grammar (Perlmutter 1983), Head-driven
Phrase Structure Grammar (HPSG, Pollard and
Sag 1994), and particularly Lexical-Functional
Grammar (LFG, Bresnan 2001). However,
the actual syntactic representation adopted fol-
lows the functional dependency grammar tradition
(Tesnière 1959, Sgall et al. 1986, Mel’cuk 1988)
and other dependency grammars such as Word
Grammar (Hudson 2010) in representing a sen-
tence as a set of grammatical relations between
its words. The SD scheme deviates from its LFG
roots in trying to achieve the correct balance be-
tween linguistic fidelity and human interpretabil-
ity of the relations, particularly in the context of
relation extraction tasks. This leads it to some-
times stay closer to the descriptions of traditional
grammar (such as for indirect object) in order to
avoid making unnecessary theoretical claims that
detract from broad interpretability. The focus of
the SD scheme is on semantically useful relations.

Automatic annotation of dependencies using the
SD scheme can be obtained for English text with
a tool distributed with the Stanford Parser.1 The
tool uses a rule-based strategy to extract gram-
matical relations as defined in the SD scheme via
structural configurations in Penn Treebank-style
phrase-structure trees. The tool performs well, but
as with all automatic parsing, it is important to
maintain a distinction between the annotations it
produces and the theoretical standard of the SD
scheme: there can be a difference between the re-
lation that the scheme would assign to two words
and the relation that gets assigned by the tool.
In this paper, we address the ideal relation struc-
tures rather than discussing parser performance di-
rectly. The Stanford dependency representation
makes available several variants, suited to differ-
ent goals. One, the basic representation, is a sim-
ple dependency tree over all the words in the sen-
tence, which is useful when a close parallelism to
the source text words must be maintained, such
as when used as a representation for direct depen-
dency parsing (Kübler et al. 2009). The expanded
representation adds additional relations that can-
not be expressed by a tree structure but may be

1http://nlp.stanford.edu/software/
lexparser.shtml

useful for capturing semantic relations between
entities in the sentence. Here, we will draw such
additional dependencies as dashed arcs.

3 Data

We have started an annotation effort to construct a
gold-standard corpus of web data annotated with
this extended SD scheme.2 To provide the com-
munity with a gold-standard corpus that better
captures linguistic phenomena present in casual
text genres, we are annotating the parsed section of
the Google Web Treebank (Petrov and McDonald
2012). This corpus contains about 250,000 words
of unedited web text and covers five domains:
questions and answers, emails, newsgroups, local
business reviews and blogs. For each domain, be-
tween 2,000 and 4,000 sentences have been an-
notated with phrase-structure trees in the style of
OntoNotes 4.0 by professional annotators from the
Linguistic Data Consortium.

4 Linguistic analyses adopted for
different constructions

The SD scheme has been in use for seven years,
but still lacks principled analyses of many of the
difficult English constructions that have been a sta-
ple of the formal linguistic literature. However, we
have found in our annotation work that some of
these constructions now arise prominently in terms
of cases for which the correct analysis is unclear.
Here we try to resolve several of these interesting
corner-cases of English grammar. Some of these
cases, such as tough adjectives and free relatives,
were also discussed in recent evaluations of de-
pendency extraction systems (Rimell et al. 2009,
Bender et al. 2011) where the goal was to recover
long dependencies. The family of CoNLL depen-
dency schemes for English (Buchholz and Marsi
2006, Johansson and Nugues 2007), another com-
mon dependency representation in NLP, largely
does not provide satisfying analyses for any of the
cases presented here. Small clauses are the one ex-
ception, and the CoNLL treatment of small clauses
is similar to ours.

4.1 Tough adjectives
Tough adjectives, discussed in Bender et al.
(2011), have posed challenges to nearly every syn-

2To date, except for the BioInfer corpus of biomedical
texts (Pyysalo et al. 2007) and a small set of chosen long-
distance dependency constructions (Rimell et al. 2009), there
are no gold standard Stanford dependency annotations.

188



tactic formalism. For example, in (1a), the object
of find can be “raised” to subject position in the
main clause to form a tough adjective construc-
tion, as in (1b). One of the difficulties for genera-
tive grammar in modeling this construction is that
the object being raised can be embedded arbitrar-
ily deeply in the sentence, as in (1c).

(1) a. It was hard (for me) to find this address.

b. This address was hard (for me) to find.

c. This address was hard (for me) to work
up the motivation to try to explain how to
find.

In (1b), this address functions syntactically as the
subject of was hard, but thematically as the ob-
ject of find, and we want to represent both of these
dependencies at some level. We simply give the
surface subject (here this address) the expected
nsubj label coming off the main predicate. We
want to represent its relationship to the embed-
ded verb as well though, since the surface subject
is its thematic argument. Paralleling the existing
xsubj dependency for the relationship between a
verb and its controlling subject (which breaks the
tree dependency structure), we introduce the xobj
dependency to capture the relationship between a
verb and its logical object when it breaks the tree
dependency structure. So (1b) will have the de-
pendency representation in (2), with an additional
xobj dependency from find to address:

(2) This address was hard to find.
det

nsubj
cop

ccomp
aux

xobj

Further, there are two competing structural
analyses for the optional for NP phrase. In one,
the for NP is a PP complement of the main predi-
cate, and in the other, for is a complementizer that
takes a tense-less sentence. Chomsky (1973) ar-
gues on the basis of sentences like (3a–3b) that the
experiencer for NP must be a true PP, not part of a
complementizer.

(3) a. It is easy [PP for the rich] [SBAR for the
poor to do all the work].

b. *All the work is easy [PP for the rich]
[SBAR for the poor to do ]

When the for introduces an SBAR proposition
(which is quite rare), the whole clause can “move”
as a unit, as demonstrated in (4a), but when the for

introduces a PP experiencer, the PP can “move”
separately (4b), both supporting the hypothesis
that the experiencer is not part of an SBAR.

(4) a. [SBAR For the poor to do all the work] is
easy [PP for the rich].

b. X-server is difficult [S to set up] [PP for
everyone].3

We conclude from data like this that the PP is usu-
ally a separate constituent, and should be anno-
tated as such; (3a) and (4b) should therefore be
assigned the dependencies shown in (5a–5b).

(5) a. It is easy for the rich for the poor to do the work.

prep
ccomp

b. X-server is difficult to set up for everyone.

nsubj prep

We opt to analyze to find in (1b) and to set up
in (4b) as clausal complements (ccomp). Faithful
to the original SD scheme, we reserve the use of
the xcomp label for controlled complements in the
LFG sense of functional control (Bresnan 1982) –
where the subject of the complement is necessarily
controlled by an argument of the governing verb.
This is not the case here: the subject can be viewed
as a covert PRO, which is coreferent with the for
PP complement. We now have a complete analy-
sis for tough adjectives. The dependency relations
for the sentence in (1b) are given in (6) below.

(6) This address was hard for me to find.
det

nsubj

aux prep pobj

ccomp
xobj

aux

4.2 Free relatives

Free relatives, which are discussed in Rimell et al.
(2009), are likewise challenging because while
their surface resemblance to embedded interrog-
atives invites a transformational treatment parallel
to wh-questions, certain of their syntactic proper-
ties point to an analysis in which the wh-phrase
serves as the head rather than as a subordinate ele-
ment. To illustrate these two conflicting analyses,
we will explore the implications of each treatment
using the free relative phrases (italicized) in (7) be-
low as our chief examples.

3https://mail.gnome.org/archives/
gnome-accessibility-list/2003-May/
msg00421.html

189



(7) a. Put your bag down wherever you see a
spot.

b. Sarah will talk to whoever comes her way.

c. I’ll dress however nicely you tell me to
dress.

An initially attractive approach to analyzing
(7a–7c) is to treat the free relatives identically to
embedded wh-interrogative complements. On this
approach, wherever is an advmod of see, whoever
is the nsubj of comes, and however nicely (with
nicely being the head of the wh-phrase) is an ad-
vmod of dress, resulting in the following depen-
dency structures:

(8) a. Put your bag down wherever you see a spot
advmod

ccomp

b. Sarah will talk to whoever comes her way
nsubj

pcomp

c. I’ll dress however nicely you tell me to dress
advmod

advmod
ccomp

The above treatment is analogous to a trans-
formational analysis of free relatives in a phrase
structure formalism. In such a treatment, the wh-
phrase is generated inside the clause and moved to
the clause-initial position through A′-movement.
The Treebank II bracketing guidelines (Bies et al.
1995) take this approach, inserting a *T* node, in-
dicating the trace of A′-movement, in the tree po-
sition where the wh-word was generated and coin-
dexing it with the wh-word—see (9).

(9) Sarah will talk . . .
PP

IN

to

SBAR-NOM

WHNP-1

WP

whoever

S

NP-SUBJ

PRP

*T*-1

VP

comes her way.

Under this analysis, we must treat see as a sen-
tential complement of put, comes as a preposi-
tional complement of to, and tell as a sentential
complement of dress, as shown in (8a–8c) and (9).

However, as Bresnan and Grimshaw (1978)
point out, this transformational analysis fails

to capture certain key syntactic properties of
free relatives. In particular, the free relative
phrases in (7) do not really behave like sentential
complements—in fact, substituting other senten-
tial wh-complements for the free relative phrases
in (7) leads to ungrammatical constructions:

(10) a. *Put your bag down what table Al put his
on.

b. *Sarah will talk to which person Fred
talked to.

c. *I’ll dress what dress I wore last time.

We make better predictions if we analyze the
free relatives like those in (7a), (7b), and (7c) as
locative adverbial phrases, nominal phrases, and
adverbial phrases, respectively. Substituting these
phrase types for the free relatives in the original
examples leads to perfectly natural constructions:

(11) a. Put your bag down on the table.
b. Sarah will talk to that man over there.
c. I’ll dress very nicely.

In each example, the syntactic category as-
signed to the free relative phrase is identical to that
of the wh-phrase within the free relative: wher-
ever being locative, whoever being nominal, and
however nicely being adverbial. Based on this ob-
servation (among others), Bresnan and Grimshaw
(1978) argue for treating the wh-phrase as the head
of the free relative. In their 1978 transforma-
tion grammar analysis, they then account for the
appearance of movement with a deleted pronoun
whose trace is coindexed with the wh-phrase and
stipulate that the coindexed nodes must agree in
certain grammatical features:

(12) a. Put your bag down [LocP wherever1 [S you
see a spot [there→ /01]]].

b. Sarah will talk to [NP whoever1 [S [s/he→
/01] comes her way]].

c. I’ll dress [AdvP [however nicely]1 [S you
tell me to dress [so→ /01]]].

So rather than follow the Treebank II guide-
lines, we adopt the approach of Bresnan and
Grimshaw (1978), analyzing the wh-phrase as the
head of the free relative and treating the senten-
tial portion of the free relative phrase as a relative
clause modifier on the head. We also mark the re-
lationships inside the relative clause, between the

190



verb and the head of the wh-phrase, with addi-
tional dependencies, to preserve the semantic re-
lationship between the two entities. The grammat-
ical relations between the verb of the relative and
the head of the wh-phrase correspond to the ones
the traces would receive. Thus, we decompose the
examples in (7a–7c) as follows:

(13) Put your bag down wherever you see a spot.
advmod rcmod

advmod

(14) Sarah will talk to whoever comes her way.
rcmodpobj

nsubj

(15) I’ll dress however nicely you tell me to dress.
advmod

advmod
rcmod

advmod

4.3 Comparative constructions
The syntax of comparative constructions in En-
glish poses various challenges for linguistic the-
ory, many of which are discussed in Bresnan
(1973). We devoted special attention to canonical
(in)equality comparisons between two elements,
of the form: as1 X as2 Y and more X than Y.

4.3.1 as . . . as constructions
In constructions of the form as1 X as2 Y, X and
Y can be of a range of syntactic types, leading to
surface forms such as those exemplified below:

(16) a. Commitment is as important as a player’s
talent.

b. Get the cash to him as soon as possible.

c. I put in as much flour as the recipe called
for.

Note that there are analogous constructions with
inequality comparatives for all of these, briefly
discussed below; the analysis argued for in this
subsection will largely extend to those. X takes
the form of an AdjP, an AdvP, and an NP in (16a),
(16b) and (16c), respectively. We analyze the
as1 X as2 Y expression as modification on the X
phrase; notice that preserving only the head of the
X phrase always yields a grammatical sentence,
indicating that this head determines the syntactic
type of the whole phrase:

(17) a. Commitment is important.

b. Get the cash to him soon.

c. I put in flour.

This suggests that the head of X is the head
of the whole structure (and therefore depends on
nothing inside it) and that the as1 . . . as2 Y phrase
modifies the inner X phrase. Our analysis ex-
presses this by making as1 dependent on a head in-
side X. However, clearly as1 . . . as2 Y is a compar-
ative modifier, and it modifies a gradable property.
That property is not always denoted by the head of
X; flour, for example, does not seem to be the tar-
get of the comparison in (16c). To reflect that, our
next analytic decision is to make as1 dependent on
the adjective, adverb or quantifier that represents
the gradable property targeted by the comparison.
The relation is advmod, consistent with other types
of degree modification, such as (18).

(18) a. Commitment is crucially important.

b. Get the cash to him very soon.

c. I put in too much flour.

With that, for (16a) we have:

(19) as important
advmod

For (16c), we make as1 dependent on much, not
flour, as it is the quantity of flour that is the target
of the comparison:

(20) as much flour
advmod amod

These decisions address the question of what
the head of the entire phrase is, and how the com-
parative modifier interfaces with it. Next, we turn
to questions about the internal structure of the
comparative. It seems that as1 has a privileged sta-
tus over as2, since it is possible to drop as2 Y (21),
but not as1 (22):

(21) a. Commitment is (just) as important.

b. ?Get the cash to him (just) as soon.

c. I put in (just) as much flour.

(22) a. *Commitment is important as a player’s
talent.

b. *Get the cash to him soon as possible.

c. *I put in much flour as the recipe called
for.

For this reason, and following other authors’
syntactic analyses of the secondary term of a com-
parative as a complement (Huddleston and Pullum

191



2002), we make as2 Y dependent on as1. This still
leaves the question of how to link Y with the rest
of the phrase. It is clear that the material in as2 Y
can be clausal, as exemplified by (16c); it is also
optional, as exemplified by (21). For that reason,
we make it an advcl, dependent on as1, with as2 as
a mark. This is consistent with the Penn Treebank
annotations for these constructions. That gives us:

(23) as much flour as the recipe called for

advmod

amod
mark

prepdet nsubj

advcl

In the case when Y is an NP, to remain consis-
tent with the Penn Treebank annotations, we treat
as2 Y as a prepositional phrase. So we have:

(24) as important as a player ’s talent

advmod
prep pobj

4.3.2 more . . . than constructions
The analysis we give to expressions like more . . .
than or less . . . than, as in (25), is very similar to
the analysis of as . . . as discussed above.

(25) I put in more flour than the recipe called for.

Again, we analyze the head of the more X than
Y expression as the head of the X phrase, since
keeping the head will yield a grammatical sen-
tence, which in this case is exactly (17c). Also in
parallel with the constructions above, we note that
the relation between more . . . than Y and X has
a parallel with other types of adverbial modifiers,
as was shown in (18c). Therefore, we again label
that relation advmod. As for than Y, again we take
it to be an adverbial clause if Y is anything other
than an NP. So we will have analyses such as:

(26) more flour than the recipe called for

mark

prepnsubj

advcl

amod

When Y is an NP, we essentially adopt the anal-
ysis of Bresnan (1973), in which an -er mor-
pheme that expresses the comparative value com-
bines with much to form more; this provides an ex-
planation for why much appears in (16c), where it
combines with as, but not in (25), where it com-
bines with -er. This is relevant because the re-
sulting more in (25) is, syntactically, an adjecti-
val modifier, as is much in (16c). Also, in parallel
with our analysis of as1 X as2 Y and consistently
with the Penn Treebank analysis, we call than Y a

prepositional phrase when Y is a noun phrase. We
therefore arrive at the following analysis for the
comparative expression below:

(27) more important than a player ’s talent

advmod
prep pobj

4.4 Small clauses
In the world of phrase structure, small clauses, like
the bracketed example in (28), have two compet-
ing analyses: in the analysis correlated with the
lexicalist approach (28a), both the entity and the
predicate depend on the main verb; and in the
one correlated with the transformational approach
(28b), the entity depends on the predicate.

(28) a. We made [them leave].

X
Y

b. We made [them leave].

X
Z

There is a substantial literature on small clauses
and evidence for and against each structure
(Borsley 1991, Culicover and Jackendoff 2005,
Matthews 2007). The optimal analysis largely de-
pends on the assumptions of the theory in ques-
tion. The Penn Treebank adopts the analysis in
(28b), putting both arguments of the main verb
under an S node. Empirically, though, the small
clause as a unit fails a considerable number of con-
stituency tests, such as those in (29) (adapted from
(Culicover and Jackendoff 2005)), which show
that the small clause cannot move around in the
sentence as a unit. So in the system we have been
developing—which we aim to make as empiri-
cally motivated as possible—we choose to have
both the entity and the predicate depend on the
main verb (28a) as is also done in the CoNLL
scheme, leading to the analysis in (30). This
analysis also allows us to add an additional sub-
ject relation between the two components of the
small clause when the small clause contains a verb
(which CoNLL does not have). Adopting the other
analysis, we would lose the link between the ob-
ject and the higher verb.

(29) a. *What we made was them leave.
b. *We made without difficulty them leave.
c. *Them leave is what we made.

(30) We made them leave.
nsubj

xcomp
dobj xsubj

192



The Penn Treebank also recognizes small clause
constructions where the predicate is a nominal or
adjectival expression as in (31b) and (31c) respec-
tively. We can extend the xcomp analysis to them
by regarding the noun or adjective as also a pred-
icate with a controlled subject. This is consistent
with both the LFG analysis where the grammat-
ical function XCOMP originated (Bresnan 1982)
and the treatment of predicate nouns and adjec-
tives in copula constructions in SD (de Marneffe
and Manning 2008a).

(31) a. We made them leave.
b. We made them martyrs.
c. We made them noticeable.

For example (32a) has a parallel analysis to (32b):

(32) a. We made them martyrs.
nsubj

root

dobj
xcomp

xsubj

b. They are martyrs.

nsubj
cop

root

Assigning the xcomp label offers a consistent anal-
ysis across all uses of the small clause construction
and also emphasizes the fact that the second noun
phrase is being used non-referentially, as a predi-
cate instead of as an entity.

5 Extensions to the Stanford
dependencies

In the process of annotating the Google Web Tree-
bank, we also discovered a number of ways in
which the SD standard needs to be modified to
capture the syntax of a broader range of text gen-
res. These changes, described in the following
paragraphs, led to a new version of the extended
SD scheme with 56 relations, listed in Figure 1.

5.1 New relations
discourse Colloquial writing contains interjec-
tions, emoticons, and other discourse markers
which are not linked to their host sentences by any
existing relation. We add a discourse element rela-
tion discourse which encompasses these construc-
tions, including emoticons and all phrases headed
by words that the Penn Treebank tags INTJ: inter-
jections (oh, uh-huh, Welcome), fillers (um, ah),
and discourse markers (well, like, actually).

(33) Hello, my name is Vera.
discourse

root - root
dep - dependent

aux - auxiliary
auxpass - passive auxiliary
cop - copula

arg - argument
agent - agent
comp - complement

acomp - adjectival complement
ccomp - clausal complement with internal subject
xcomp - clausal complement with external subject
obj - object

dobj - direct object
iobj - indirect object
pobj - object of preposition

subj - subject
csubj - clausal subject

csubjpass - passive clausal subject
nsubj - nominal subject

nsubjpass - passive nominal subject
cc - coordination
conj - conjunct
expl - expletive (expletive “there”)
list - list item
mod - modifier

advmod - adverbial modifier
neg - negation modifier

amod - adjectival modifier
appos - appositional modifier
advcl - adverbial clause modifier
det - determiner
discourse - discourse element
goeswith - goes with
predet - predeterminer
preconj - preconjunct
mwe - multi-word expression modifier
mark - marker (word introducing an advcl or ccomp)
nn - noun compound modifier
npadvmod - noun phrase adverbial modifier

tmod - temporal modifier
num - numeric modifier
number - element of compound number
prep - prepositional modifier
poss - possession modifier
possessive - possessive modifier (’s)
prt - phrasal verb particle
quantmod - quantifier modifier
rcmod - relative clause modifier
vmod - verbal modifier
vocative - vocative

parataxis - parataxis
punct - punctuation
ref - referent
sdep - semantic dependent (breaking tree structure)

xsubj - (controlled) subject
xobj - (controlled) object

Figure 1: Extended Stanford dependencies.

goeswith Unedited text often contains multiple
tokens that correspond to a single standard En-
glish word, as a result of reanalysis of com-
pounds (“hand some” for “handsome”) or input
error (“othe r” for “other”). The non-head por-
tions of these broken words are tagged GW in the
treebank. We cannot expect preprocessing steps

193



(tokenization and normalization) to fix all of these
errors, so we introduce the relation goeswith to re-
connect these non-heads to their heads—usually
the initial pieces of the words.

(34) They come here with out legal permission

goeswith

list Web text often contains passages which are
meant to be interpreted as lists of comparable
items, but are parsed as single sentences. Email
signatures in particular contain these structures, in
the form of contact information. We label the con-
tact information list as in (35). For the key-value
pair relations that often occur in these contexts, we
use the appos relation.

(35) Steve Jones Phone: 555-9814 Email: jones@abc.edf
apposlist

list

appos

vmod Since the distinction between partmod
and infmod is straightforwardly reflected in the
part-of-speech of the verb, we choose to cease du-
plicating information by merging these relations
into a single one, vmod. We intend this to cover all
cases of verb-headed phrases acting as modifiers,
which are not full clauses.

(36) I don’t have anything to say.
vmod

vocative In writing that directly addresses a di-
alog participant (e.g., emails and newsgroup post-
ings), it is common to begin sentences by naming
that other participant. We introduce the vocative
relation to link these names to their host sentences.

(37) Tracy, do we have concerns here?
vocative

xobj We introduce the relation xobj to capture
the relationship between a verb and its displaced
logical object. For further explanation, see the dis-
cussion of tough adjectives in section 4.1 above.

(38) Those were hard for him to find.
xobj

5.2 Modified and deleted relations
advcl Purpose clauses (purpcl) were singled out
based on a semantic distinction, but distinctions
were not made for other types of adverbial clause
(temporal, causal, etc.). We make the scheme
more uniform by collapsing purpose clauses with
general adverbial clauses (advcl).

(39) She talked to him in order to secure the account.
advcl

amod Parenthetically marked ages have been
treated as appositives (and marked appos), but we
find that this violates the otherwise largely sound
generalization that appositives fill the same se-
mantic role as the NPs they modify, and essen-
tially serve as alternative ways to identify the enti-
ties named by those NPs. Since, for example, it is
not reasonable to infer (41) from (40), we choose
to re-classify these cases as displaced adjectival
modifiers, and label them amod.

(40) John Smith ( 33 ) was from Kansas City, MO.
amod

(41) 33 is from Kansas City, MO.

appos We abandon the abbrev relation and sub-
stitute appos: abbrev captured parenthetical ex-
pressions indicating abbreviations, but was used
rarely, and provided little information not also
captured by the more general appos.

(42) The Australian Broadcasting Corporation ( ABC )
appos

attr In wh-questions such as (43), we treated the
copular verb as the root, and the wh-word was an
attr. We are abandoning the attr relation, leading
to the following analysis which parallels that of
affirmative copular sentences like (44) where the
predicate is the root. Copular sentences are now
treated more uniformly than before.

(43) What is that?

nsubj

cop

root

(44) That is a sturgeon.

nsubj

cop

root

mark The former complm relation captured
overt complementizers like “that” in complement
clauses (ccomp). We follow the intuition from
HPSG that this relation captures approximately
the same structural relation as mark in adverbial
clauses (45), and provides no information that
mark would not also provide. We thus abandon
the complm relation, and substitute mark (46).

(45) I like to swim when it rains.
mark

advcl

(46) He says that you like to swim.
mark

ccomp

194



mwe We have found several additional construc-
tions that we believe meet the criteria to be con-
sidered multi-word expressions for the purposes
of the mwe relation, which is intended for “multi-
word idioms that behave like a single function
word.” We add the following constructions:

at least, at most, how about, how come, in case,
in order (to), of course, prior to, so as (to), so
that, what if, whether or not

This is in addition to already-recognized construc-
tions such as:

rather than, as well as, instead of, such as, be-
cause of, instead of, in addition to, all but, such
as, because of, instead of, due to

(47) Of course I’ll go!
mwe

Ultimately, the choice of what to count as a mwe
reflects a cut across the continuous cline of gram-
maticalization, and is necessarily arbitrary.

punct We do not follow Choi and Palmer (2012)
in using the relations hmod and hyph for the non-
head words of split-up hyphenated words and the
hyphens respectively. We find the usage of hy-
phens is very inconsistent, and so we prefer to ap-
ply the most appropriate general relation that holds
between the hyphenated components rather than
adopt these labels. For the hyphen, when it is used
to construct compound words (48), we treated it
as punctuation and assign the punct relation, but
when it is used in place of an en dash to indicate
a range, as in (49), we treat it as a preposition and
assign the prep relation.

(48) short - term humanitarian crisis
punct

amod

(49) French Indochina War ( 1946 – 1954 )
prep pobj

rel rel has been used in a small number of con-
structions to mark the head words of wh-phrases
introducing relative clauses. We are retiring the re-
lation: we will mark the heads of wh-phrases in ac-
cordance with their role in the relative clause (usu-
ally nsubj, dobj, pobj, or prep), and any such head
whose role cannot be identified will be marked
with the generic relation dep.

xcomp The xcomp relation is specified in
de Marneffe and Manning (2008a) to apply to any
non-finite complement clause which has its sub-
ject controlled by the subject of the next higher
verb. However, complement clauses with object
control—wherein the object of the higher verb
controls the subject of an embedded clause, as
in (50)—structurally have more in common with
subject control cases rather than with the canoni-
cal ccomp complement clause with which it would
otherwise be classified. Further, these cases are
grouped as XCOMP in LFG. In order to ensure
that this crucial notion of external control is reli-
ably captured, we expand the definition of xcomp
to include cases of both subject and object control.

(50) It allowed material previously stored to decompose.
xcomp

6 Conclusions

We extend the coverage of the SD scheme by pre-
senting principled analyses of linguistically inter-
esting constructions and by positing new relations
to capture frequent constructions in modern web
data. Our approach has been empirical: all the
construction types discussed here appear in the
Google English Web Treebank data that we are
annotating. We are currently incorporating our ex-
tensions of the SD standard into the freely avail-
able converter tool associated with the scheme. So
far, there has not been any quantitative evaluation
of the tool: there has only been some qualitative
analysis as well as a focus on some relations as re-
ported in (Rimell et al. 2009, Bender et al. 2011),
but ultimately the annotated gold standard corpus
we are creating will enable a thorough evaluation
of the converter tool.

Acknowledgment

Annotation of the English Web Treebank with
gold Stanford dependencies has been supported by
a gift from Google Inc. We thank the anonymous
reviewers, John Bauer, and Joakim Nivre for help-
ful comments on the analyses we propose.

References

Bender, Emily M., Dan Flickinger, Stephan
Oepen, and Yi Zhang. 2011. Parser evalua-
tion over local and non-local deep dependencies
in a large corpus. In Proceedings of the Con-
ference on Empirical Methods in Natural Lan-
guage Processing, pp. 397–408.

195



Bies, Ann, Mark Ferguson, Karen Katz, Robert
MacIntyre, Victoria Tredinnick, Grace Kim,
Mary Ann Marcinkiewicz, and Britta Schas-
berger, 1995. Bracketing guidelines for Tree-
bank II style Penn Treebank project.

Borsley, Robert D. 1991. Syntactic Theory: A
Unified Approach. Edward Arnold.

Bresnan, Joan. 1973. Syntax of the comparative
clause construction in English. Linguistic In-
quiry 4.

Bresnan, Joan. 1982. Control and complementa-
tion. In Joan Bresnan (ed.), The Mental Rep-
resentation of Grammatical Relations, pp. 282–
390. MIT Press.

Bresnan, Joan. 2001. Lexical-functional syntax.
Blackwell.

Bresnan, Joan, and Jane Grimshaw. 1978. The
syntax of free relatives in English. Linguistic
Inquiry 9(3):331–391.

Buchholz, Sabine, and Erwin Marsi. 2006.
CoNLL-X shared task on multilingual depen-
dency parsing. In Proceedings of the Tenth Con-
ference on Computational Natural Language
Learning, pp. 149–164.

Choi, Jinho D., and Martha Palmer. 2012. Guide-
lines for the Clear style constituent to depen-
dency conversion. Technical report, University
of Colorado Boulder, Institute of Cognitive Sci-
ence.

Chomsky, Noam. 1973. Conditions on trans-
formations. In Stephen Anderson and Paul
Kiparsky (eds.), A Festschrift for Morris Halle,
pp. 232–286. New York: Holt, Rinehart & Win-
ston.

Culicover, Peter W., and Ray Jackendoff. 2005.
Simpler syntax. Oxford University Press.

de Marneffe, Marie-Catherine, Bill MacCartney,
and Christopher D. Manning. 2006. Generating
typed dependency parses from phrase structure
parses. In Proceedings of the 5th International
Conference on Language Resources and Evalu-
ation, pp. 449–454.

de Marneffe, Marie-Catherine, and Christopher D.
Manning. 2008a. Stanford typed dependencies
manual. Technical report, Stanford University.

de Marneffe, Marie-Catherine, and Christopher D.
Manning. 2008b. The Stanford typed depen-
dencies representation. In Proceedings of the

COLING Workshop on Cross-framework and
Cross-domain Parser Evaluation, pp. 1–8.

Huddleston, Rodney, and Geoffrey K. Pullum.
2002. The Cambridge Grammar of the English
Language. Cambridge University Press.

Hudson, Richard A. 2010. An Introduction to
Word Grammar. Cambridge University Press.

Johansson, Richard, and Pierre Nugues. 2007. Ex-
tended constituent-to-dependency conversion
for English. In Proceedings of NODALIDA
2007.

Kübler, Sandra, Ryan McDonald, and Joakim
Nivre. 2009. Dependency Parsing, volume 2 of
Synthesis Lectures on Human Language Tech-
nologies. Morgan & Claypool.

Matthews, Peter H. 2007. Syntactic relations: A
critical survey. Cambridge University Press.

Mel’cuk, Igor A. 1988. Dependency syntax: The-
ory and practice. SUNY Press.

Perlmutter, David M. (ed.). 1983. Studies in
Relational Grammar, volume 1. University of
Chicago Press.

Petrov, Slav, and Ryan McDonald. 2012.
Overview of the 2012 shared task on parsing the
web. In First Workshop on Syntactic Analysis of
Non-Canonical Language.

Pollard, Carl, and Ivan A. Sag. 1994. Head-
driven Phrase Structure Grammar. University
of Chicago Press.

Pyysalo, Sampo, Filip Ginter, Katri Haverinen,
Juho Heimonen, Tapio Salakoski, and Veronika
Laippala. 2007. On the unification of syntac-
tic annotations under the Stanford dependency
scheme: A case study on BioInfer and GENIA.
In Proceedings of BioNLP 2007: Biological,
translational, and clinical language processing
(ACL07), pp. 25–32.

Rimell, Laura, Stephen Clark, and Mark Steed-
man. 2009. Unbounded dependency recov-
ery for parser evaluation. In Proceedings of the
2009 Conference on Empirical Methods in Nat-
ural Language Processing, pp. 813–821.

Sgall, Petr, Eva Hajičová, and Jarmila Panevová.
1986. The meaning of the sentence in its seman-
tic and pragmatic aspects. D. Reidel Publishing
Company.

Tesnière, Lucien. 1959. Éléments de syntaxe
structurale. Librairie C. Klincksieck.

196


