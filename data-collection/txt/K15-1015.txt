



















































Incremental Recurrent Neural Network Dependency Parser with Search-based Discriminative Training


Proceedings of the 19th Conference on Computational Language Learning, pages 142–152,
Beijing, China, July 30-31, 2015. c©2015 Association for Computational Linguistics

Incremental Recurrent Neural Network Dependency Parser
with Search-based Discriminative Training

Majid Yazdani
Computer Science Department

University of Geneva
majid.yazdani@unige.ch

James Henderson
Xerox Research Center Europe

james.henderson@xrce.xerox.com

Abstract

We propose a discriminatively trained re-
current neural network (RNN) that pre-
dicts the actions for a fast and accurate
shift-reduce dependency parser. The RNN
uses its output-dependent model struc-
ture to compute hidden vectors that en-
code the preceding partial parse, and uses
them to estimate probabilities of parser ac-
tions. Unlike a similar previous generative
model (Henderson and Titov, 2010), the
RNN is trained discriminatively to opti-
mize a fast beam search. This beam search
prunes after each shift action, so we add
a correctness probability to each shift ac-
tion and train this score to discriminate be-
tween correct and incorrect sequences of
parser actions. We also speed up pars-
ing time by caching computations for fre-
quent feature combinations, including dur-
ing training, giving us both faster training
and a form of backoff smoothing. The re-
sulting parser is over 35 times faster than
its generative counterpart with nearly the
same accuracy, producing state-of-art de-
pendency parsing results while requiring
minimal feature engineering.

1 Introduction and Motivation

There has been significant interest recently in ma-
chine learning and natural language processing
community in models that learn hidden multi-
layer representations to solve various tasks. Neu-
ral networks have been popular in this area as a
powerful and yet efficient models. For example,
feed forward neural networks were used in lan-
guage modeling (Bengio et al., 2003; Collobert
and Weston, 2008), and recurrent neural networks
(RNNs) have yielded state-of-art results in lan-
guage modeling (Mikolov et al., 2010), language

generation (Sutskever et al., 2011) and language
understanding (Yao et al., 2013).

1.1 Neural Network Parsing

Neural networks have also been popular in pars-
ing. These models can be divided into those
whose design are motivated mostly by inducing
useful vector representations (e.g. (Socher et al.,
2011; Socher et al., 2013; Collobert, 2011)), and
those whose design are motivated mostly by ef-
ficient inference and decoding (e.g. (Henderson,
2003; Henderson and Titov, 2010; Henderson et
al., 2013; Chen and Manning, 2014)).

The first group of neural network parsers are all
deep models, such as RNNs, which gives them the
power to induce vector representations for com-
plex linguistic structures without extensive feature
engineering. However, decoding in these models
can only be done accurately if they are used to re-
rank the best parse trees of another parser (Socher
et al., 2013).

The second group of parsers use a shift-reduce
parsing architecture so that they can use search
based decoding algorithms with effective pruning
strategies. The more accurate parsers also use a
RNN architecture (see Section 6), and use gener-
ative models to allow beam search. These mod-
els are accurate but are relatively slow, and accu-
racy degrades when you choose decoding settings
to optimize speed. Because they are generative,
they need to predict the words as the parse pro-
ceeds through the sentence, which requires nor-
malization over all the vocabulary of words. Also,
the beam search must maintain many candidates
in the beam in order to check how well each one
predicts future words. Recently (Chen and Man-
ning, 2014) propose a discriminative neural net-
work shift-reduce parser, which is very fast but
less accurate (see Section 6). However, this parser
uses a feed-forward neural network with a large set
of hand-coded features, making it of limited inter-

142



est for inducing vector representations of complex
linguistic structures.

1.2 Incremental Recurrent Neural Network
Architecture

In both approaches to neural network parsing,
RNN models have the advantage that they need
minimal feature engineering and therefore they
can be used with little effort for a variety of lan-
guages and applications. As with other deep neu-
ral network architectures, RNNs induce complex
features automatically by passing induced (hid-
den) features as input to other induced features
in a recursive structure. This is a particular ad-
vantage for domain adaptation, multi-task learn-
ing, transfer learning, and semi-supervised learn-
ing, where hand-crafted feature engineering is of-
ten particularly difficult. The information that is
transferred from one task to another is embedded
in the induced feature vectors in a shared latent
space, which is input to another hidden layer for
the target model (Henderson et al., 2013; Raina
et al., 2007; Collobert et al., 2011; Glorot et al.,
2011). This transferred information has proven
to be particularly useful when it comes from very
large datasets, such as web-scale text corpora, but
learning and inference on such datasets is only
practical with efficient algorithms.

In this work, we propose a fast discriminative
RNN model of shift-reduce dependency parsing.
We choose a left-to-right shift-reduce dependency
parsing architecture to benefit from efficient de-
coding. It also easily supports incremental in-
terpretation in dialogue systems, or incremental
language modeling for speech recognition. We
choose a RNN architecture to benefit from the au-
tomatic induction of informative vector represen-
tations of complex linguistic structures and the re-
sulting reduction in the required feature engineer-
ing. This hidden vector representation is trained to
encode the partial parse tree that has been built by
the preceding parse, and is used to predict the next
parser action conditioned on this history.

As our RNN architecture, we use the neural
network approximation of ISBNs (Henderson and
Titov, 2010), which we refer to as an Incremental
Neural Network (INN). INNs are a kind of RNN
where the model structure is built incrementally
as a function of the values of previous output vari-
ables. In our case, the hidden vector used to make
the current parser decision is connected to the hid-

den vector from previous decisions based on the
partial parse structure that has been built by the
previous decisions. So any information about the
unbounded parse history can potentially be passed
to the current decision through a chain of hidden
vectors that reflects locality in the parse tree, and
not just locality in the derivation sequence (Hen-
derson, 2003; Titov and Henderson, 2007b). As in
all deep neural network architectures, this chain-
ing of nonlinear vector computations gives the
model a very powerful mechanism to induce com-
plex features from combinations of features in the
history, which is difficult to replicate with hand-
coded features.

1.3 Search-based Discriminative Training

We propose a discriminative model because it al-
lows us to use lookahead instead of word predic-
tion. As mentioned above, generative word pre-
diction is costly, both to compute and because it
requires larger beams to be effective. With looka-
head, it is possible to condition on words that
are farther ahead in the string, and thereby avoid
hypothesizing parses that are incompatible with
those future words. This allows the parser to prune
much more aggressively without losing accuracy.
Discriminative learning further improves this ag-
gressive pruning, because it can optimize for the
discrete choice of whether to prune or not (Huang
et al., 2012; Zhang and Clark, 2011).

Our proposed model primarily differs from pre-
vious discriminative models of shift-reduce de-
pendency parsing in the nature of the discrimina-
tive choices that are made and the way these deci-
sions are modeled and learned. Rather than learn-
ing to make pruning decisions at each parse ac-
tion, we learn to choose between sequences of ac-
tions that occur in between two shift actions. This
way of grouping action sequences into chunks as-
sociated with each word has been used previously
for efficient pruning strategies in generative pars-
ing (Henderson, 2003), and for synchronizing syn-
tactic parsing and semantic role labeling in a joint
model (Henderson et al., 2013). We show empiri-
cally that making discriminative parsing decisions
at the scale of these chunks also provides a good
balance between grouping decisions so that more
context can be used to make accurate parsing de-
cisions and dividing decisions so that the space of
alternatives for each decision can be considered
quickly (see Figure 4 below).

143



In line with this pruning strategy, we define a
score called the correctness probability for every
shift action. This score is trained discriminatively
to indicate whether the entire parse prefix is cor-
rect or not. This gives us a score function that is
trained to optimize the pruning decisions during
search (c.f. (Daumé III and Marcu, 2005)). By
combining the scores for all the shift actions in
each candidate parse, we can also discriminate be-
tween multiple parses in a beam of parses, thereby
giving us the option of using beam search to im-
prove accuracy in cases where bounded lookahead
does not provide enough information to make a de-
terministic decision. The correctness probability
is estimated by only looking at the hidden vector at
its shift action, which encourages the hidden units
to encode any information about the parse history
that is relevant to deciding whether this is a good
or bad parse, including long distance features.

1.4 Feature Decomposition and Caching

Another popular method of previous neural net-
work models that we use and extend in this paper
is the decomposition of input feature parameters
using vector-matrix multiplication (Bengio et al.,
2003; Collobert et al., 2011; Collobert and We-
ston, 2008). As the previous work shows, this de-
composition overcomes the features sparsity com-
mon in NLP tasks and also enables us to use un-
labeled data effectively. For example, the param-
eter vector for the feature word-on-top-of-stack is
decomposed into the multiplication of a parame-
ter vector representing the word and a parameter
matrix representing top-of-stack. But sparsity is
not always a problem, since the frequency of such
features follows a power law distribution, so there
are some very frequent feature combinations. Pre-
vious work has noticed that the vector-matrix mul-
tiplication of these frequent feature combinations
takes most of the computation time during test-
ing, so they cache these computations (Bengio et
al., 2003; Devlin et al., 2014; Chen and Manning,
2014).

We note that these computations also take most
of the computation time during training, and that
the abundance of data for these feature combina-
tions removes the statistical motivation for decom-
posing them. We propose to treat the cached vec-
tors for high frequency feature combinations as
parameters in their own right, using them both dur-
ing training and during testing.

In summary, this paper makes several contri-
butions to neural network parsing by consider-
ing different scales in the parse sequence and in
the parametrization. We propose a discrimina-
tive recurrent neural network model of depen-
dency parsing that is trained to optimize an effi-
cient form of beam search that prunes based on
the sub-sequences of parser actions between two
shifts, rather than pruning after each parser ac-
tion. We cache high frequency parameter compu-
tations during both testing and training, and train
the cached vectors as separate parameters. As
shown in section 6, these improvements signifi-
cantly reduce both training and testing times while
preserving accuracy.

2 History Based Neural Network Parsing

In this section we briefly specify the action se-
quences that we model and the neural network ar-
chitecture that we use to model them.

2.1 The Parsing Model

In shift-reduce dependency parsing, at each step of
the parse, the configuration of the parser consists
of a stack S of words, the queue Q of words and
the partial labeled dependency trees constructed
by the previous history of parser actions. The
parser starts with an empty stack S and all the in-
put words in the queue Q, and terminates when it
reaches a configuration with an empty queue Q.
We use an arc-eager algorithm, which has 4 ac-
tions that all manipulate the word s on top of the
stack S and the word q on the front of the queue
Q: The decision Left-Arcr adds a dependency arc
from q to s labeled r. Word s is then popped from
the stack. The decision Right-Arcr adds an arc
from s to q labeled r. The decision Reduce pops
s from the stack. The decision Shift shifts q from
the queue to the stack. For more details we refer
the reader to (Nivre et al., 2004). In this paper we
chose the exact definition of the parse actions that
are used in (Titov and Henderson, 2007b).

At each step of the parse, the parser needs to
choose between the set of possible next actions.
To train a classifier to choose the best actions,
previous work has proposed memory-based clas-
sifiers (Nivre et al., 2004), SVMs (Nivre et al.,
2006), structured perceptron (Huang et al., 2012;
Zhang and Clark, 2011), two-layer neural net-
works (Chen and Manning, 2014), and Incremen-
tal Sigmoid Belief Networks (ISBN) (Titov and

144



Henderson, 2007b), amongst other approaches.
We take a history based approach to model these

sequences of parser actions, which decomposes
the conditional probability of the parse using the
chain rule:

P (T |S) = P (D1· · ·Dm|S)
=

∏
t

P (Dt|D1· · ·Dt−1, S)

where T is the parse tree, D1· · ·Dm is its equiva-
lent sequence of shift-reduce parser actions and S
is the input sentence. The probability of Left-Arcr
and Right-Arcr include both the probability of the
attachment decision and the chosen label r. But
unlike in (Titov and Henderson, 2007b), the prob-
ability of Shift does not include a probability pre-
dicting the next word, since all the words S are
included in the conditioning.

2.2 Estimating Action Probabilities

To estimate each P (Dt|D1· · ·Dt−1, S), we
need to handle the unbounded nature of both
D1· · ·Dt−1 and S. We can divide S into the words
that have already been shifted, which are handled
as part of our encoding of D1· · ·Dt−1, and the
words on the queue. To condition on the words
in the queue, we use a bounded lookahead:

P (T |S) ≈
∏
t

P (Dt|D1· · ·Dt−1, wta1 · · ·wtak)

where wta1 · · ·wtak is the first k words on the front
of the queue at time t. At every Shift action the
lookahead changes, moving one word onto the
stack and adding a new word from the input.

To estimate the probability of a decision at time
t conditioned on the history of actionsD1· · ·Dt−1,
we overcome the problem of conditioning on an
unbounded amount of information by using a neu-
ral network to induce hidden representations of the
parse history sequence. The relevant information
about the whole parse history at time t is encoded
in its hidden representation, denoted by the vector
ht of size d.∏
t

P (Dt|D1· · ·Dt−1, wta1 · · ·wtak) =
∏
t

P (Dt|ht)

The hidden representation at time t is induced
from hidden representations of the relevant previ-
ous states, plus pre-defined features F computed

from the previous decision and the current queue
and stack:

ht = σ(
∑
c∈C

htcW cHH +
∑
f∈F

WIH(f, :))

In which C is the set of link types for the previous
relevant hidden representations, htc is the hidden
representation of time tc<t that is relevant to ht by
the relation c, W cHH is the hidden to hidden tran-
sition weights for the link type c, and WIH is the
weights from featuresF to hidden representations.
σ is the sigmoid function and W (i, :) shows row
i of matrix W . F and C are the only hand-coded
parts of the model.

The decomposition of features has attracted a
lot of attention in NLP tasks, because it overcomes
feature sparsity. There is transfer learning from the
same word (or POS tag, Dependency label, etc.) in
different positions or to similar words. Also unsu-
pervised training of word embeddings can be used
effectively within decomposed features. The use
of unsupervised word embeddings in various nat-
ural language processing tasks has received much
attention (Bengio et al., 2003; Collobert and We-
ston, 2008; Collobert, 2011). Word embeddings
are real-valued feature vectors that are induced
from large corpora of unlabeled text data. Using
word embeddings with a large dictionary improves
domain adaptation, and in the case of a small train-
ing set can improve the performance of the model.

Given these advantages, we use feature decom-
positions to define the input-to-hidden weights
WIH .

WIH(f, :) = Wemb.(val(f), :)W
f
HH

Every row in Wemb. is an embedding for a feature
value, which may be a word, lemma, pos tag, or
dependency relation. val(f) is the index of the
value for feature type f , for example the particular
word that is at the front of the queue. MatrixW fHH
is the transition matrix from the feature value em-
beddings to the hidden vector, for the given feature
type f . For simplicity, we assume here that the
size of the embeddings and the size of the hidden
representations of the INN are the same.

In this way, the parameters of the embedding
matrix Wemb. is shared among various feature in-
put link types f , which can improve the model in
the case of sparse features. It also allows the use of
word embeddings that are available on the web to
improve coverage of sparse features, but we leave

145



this investigation to future work since it is orthog-
onal to the contributions of this paper.

Finally, the probability of each decision is nor-
malized across other alternative decisions, and
only conditioned on the hidden representation
(softmax layer):

P (Dt=d|ht) = e
htWHO(:,d)∑
d′ e

htWHO(:,d′)

whereWHO is the weight matrix from hidden rep-
resentations to the outputs.

3 Discrimination of Partial Parses

Unlike in a generative model, the above formulas
for computing the probability of a tree make inde-
pendence assumptions in that words to the right of
wtak are assumed to be independent of D

t. And
even for words in the lookahead it can be diffi-
cult to learn dependencies with the unstructured
lookahead string. The generative model first con-
structs a structure and then uses word prediction
to test how well that matches the next word. If
a discriminative model uses normalized estimates
for decisions, then once a wrong decision is made
there is no way for the estimates to express that
this decision has lead to a structure that is incom-
patible with the current or future lookahead string
(see (Lafferty et al., 2001) for more discussion).
More generally, any discriminative model that is
trained to predict individual actions has this prob-
lem. In this section we discuss how to overcome
this issue.

3.1 Discriminating Correct Parse Chunks
Due to this problem, discriminative parsers typi-
cally make irrevocable choices for each individual
action in the parse. We propose a method for train-
ing a discriminative parser which addresses this
problem in two ways. First, we train the model
to discriminate between larger sub-sequences of
actions, namely the actions between two Shift ac-
tions, which we call chunks. This allows the parser
to delay choosing between actions that occur early
in a chunk until all the structure associated with
that chunk has been built. Second, the model’s
score can be used to discriminate between two
parses long after they have diverged, making it ap-
propriate for a beam search.

We employ a search strategy where we prune
at each shift action, but in between shift actions
we consider all possible sequences of actions,
similarly to the generative parser in (Henderson,

2003). The INN model is discriminatively trained
to choose between these chunks of sequences of
actions.

The most straightforward way to model these
chunk decisions would be to use unnormalized
scores for the decisions in a chunk and sum these
scores to make the decision, as would be done for a
structured perceptron or conditional random field.
Preliminary experiments applying this approach to
our INN parser did not work as well as having lo-
cal normalization of action decisions. We hypoth-
esize that the main reason for this result is that
updating on entire chunks does not provide a suf-
ficiently focused training signal. With a locally
normalized action score, such as softmax, increas-
ing the score of the correct chunk has the effect
of decreasing the score of all the incorrect actions
at each individual action decision. This update
is an approximation to a discriminative update on
all incorrect parses that continue from an incor-
rect decision (Henderson, 2004). Another pos-
sible reason is that local normalization prevents
one action’s score from dominating the score of
the whole parse, as can happen with high fre-
quency decisions. In general, this problem can
not be solved just by using norm regularization on
weights.

The places in a parse where the generative up-
date and the discriminative update differ substan-
tially are at word predictions, where the genera-
tive model considers that all words are possible but
a discriminative model already knows what word
comes next and so does not need to predict any-
thing. We discriminatively train the INN to choose
between chunks of actions by adding a new score
at these places in the parse. After each shift ac-
tion, we introduce a correctness probability that
is trained to discriminate between cases where the
chunk of actions since the previous shift is cor-
rect and those where this chunk is incorrect. Thus,
the search strategy chooses between all possible
sequences of actions between two shifts using a
combination of the normalized scores for each ac-
tion and the correctness probability.

In addition to discriminative training at the
chunk level, the correctness probability allows us
to search using a beam of parses. If a correct de-
cision can not be disambiguated, because of the
independence assumptions with words beyond the
lookahead or because of the difficulty of inferring
from an unstructured lookahead, the correctness

146



Figure 1: INN computations for one decision

probability score will drop whenever the mistake
becomes evident. This means that we can not only
compare two partial parses that differ in the most
recent chunk, but we can also compare two partial
parses that differ in earlier chunks. This allows
us to use beam search decoding. Instead of deter-
ministically choosing a single partial parse at each
shift action, we can maintain a small beam of al-
ternatives and choose between them based on how
compatible they are with future lookahead strings
by comparing their respective correctness proba-
bilities for those future shifts.

We combine this correctness probability with
the action probabilities by simply multiplying:

P (T |S) ≈
∏
t

P (Dt|ht)P (Correct|ht)

where we train P (Correct|ht) to be the correct-
ness probability for the cases where Dt is a shift
action, and define P (Correct|ht) = 1 otherwise.
For the shift actions, P (Correct|ht) is defined us-
ing the sigmoid function:

P (Correct|ht) = σ(htWCor)

In this way, the hidden representations are trained
not only to choose the right action, but also to en-
code correctness of the partial parses. Figure 1
shows the model schematically.

3.2 Training the Parameters

We want to train the correctness probabilities to
discriminate correct parses from incorrect parses.
The correct parses can be extracted from the train-
ing treebank by converting each dependency tree
into its equivalent sequence of arc-eager shift-
reduce parser actions (Nivre et al., 2004; Titov

and Henderson, 2007b). These sequences of ac-
tions provide us with positive examples. For dis-
criminative training, we also need incorrect parses
to act as the negative examples. In particular, we
want negative examples that will allow us to opti-
mize the pruning decisions made by the parser.

To optimize the pruning decisions made by the
parsing model, we use the parsing model itself
to generate the negative examples (Collins and
Roark, 2004). Using the current parameters of the
model, we apply our search-based decoding strat-
egy to find our current approximation to the high-
est scoring complete parse, which is the output of
our current parsing model. If this parse differs
from the correct one, then we train the parameters
of all the correctness probabilities in each parse
so as to increase the score of the correct parse and
decrease the score of the incorrect output parse.
By repeatedly decreasing the score of the incorrect
best parse as the model parameters are learned,
training will efficiently decreases the score of all
incorrect parses.

As discussed above, we train the scores of in-
dividual parser actions to optimize the locally-
normalized conditional probability of the correct
action. Putting this together with the above train-
ing of the correctness probabilities P (Correct|ht),
we get the following objective function:

argmaxφ∑
T∈Tpos

∑
t∈T

logP (dt|ht) + logP (Correct|ht)

−
∑

T∈Tφneg

∑
t∈T

logP (Correct|ht)

where φ is the set of all parameters of the model
(namely WHH , Wemb., WHO, and WCor), Tpos
is the set of correct parses, and T φneg is the set
of incorrect parses which the model φ scores
higher than their corresponding correct parses.
The derivative of this objective function is the er-
ror signal that the neural network learns to mini-
mize. This error signal is illustrated schematically
in Figure 2.

We optimize the above objective using stochas-
tic gradient descent. For each parameter update,
a positive tree is chosen randomly and a negative
tree is built using the above strategy. The resulting
error signals are backpropagated through the INN
to compute the derivatives for gradient descent.

147



Figure 2: Positive and negative derivation branches and their training signals

(a) Log-Log frequency by
rank of features

(b) Cumulative fraction of
features ranked by frequency

Figure 3: Feature frequency distributions

4 Caching Frequent Features

Decomposing the parametrization of input fea-
tures using vector-matrix multiplication, as was
described in section 2, overcomes the features
sparsity common in NLP tasks and also makes it
possible to use unlabeled data effectively. But it
adds a huge amount of computation to both the
training and decoding, since for every input fea-
ture a vector-matrix multiplication is needed. This
problem is even more severe in our algorithm be-
cause at every training iteration we search for the
best incorrect parse.

The frequency of features follows a power law
distribution; there are a few very frequent features,
and a long tail of infrequent features. Previous
work has noticed that the vector-matrix multiplica-
tion of the frequent features takes most of the com-
putation time during testing, so they cache these
computations (Bengio et al., 2003; Devlin et al.,
2014; Chen and Manning, 2014). For example
in the Figure 3(b), only 2100 features are respon-
sible of 90% of the computations among ∼400k
features, so cashing these computations can have
a huge impact on speed. We note that these com-
putations also take most of the computation time
during training. First, this computation is the dom-

inant part of the forward and backward error com-
putations. Second, at every iteration we need to
decode to find the highest scoring incorrect parse.

We propose to treat the cached vectors for high
frequency features as parameters in their own
right, using them both during training and during
testing. This speeds up training because it is no
longer necessary to do the high-frequency vector-
matrix multiplications, neither to do the forward
error computations nor to do the backpropagation
through the vector-matrix multiplications. Also,
the cached vectors used in decoding do not need to
be recomputed every time the parameters change,
since the vectors are updated directly by the pa-
rameter updates.

Another possible motivation for treating the
cached vectors as parameters is that it results in
a kind of backoff smoothing; high frequency fea-
tures are given specific parameters, and for low
frequency features we back off to the decomposed
model. Results from smoothing methods for sym-
bolic statistical models indicate that it is better to
smooth low frequency features with other low fre-
quency features, and treat high frequency features
individually. In this paper we do not systemati-
cally investigate this potential advantage, leaving
this for future work.

In our experiments we cache features that make
up to 90% of the feature frequencies. This gives
us about a 20 times speed up during training and
about a 7 times speed up during testing, while per-
formance is preserved.

5 Parsing Complexity

If the length of the latent vectors is d, for a sen-
tence of length L, and beamB, the decoding com-

148



Figure 4: Histogram of number of candidate ac-
tions between shifts

plexity is O(L×M ×B × (|F|+ |C|)× d2).1 If
we choose the best partial parse tree at every shift,
then B = 1. M is the total number of actions in
the candidate chunks that are generated between
two shifts, so L ×M is the total number of can-
didate actions in a parse. For shift-reduce depen-
dency parsing, the total number of chosen actions
is necessarily linear in L, but because we are ap-
plying best-first search in between shifts, M is not
necessarily independent of L. To investigate the
impact of M on the speed of the parser, we em-
pirically measure the number of candidate actions
generated by the parser between 33368 different
shifts. The resulting distribution is plotted in Fig-
ure 4. We observe that it forms a power law distri-
bution. Most of the time the number of actions is
very small (2 or 3), with the maximum number be-
ing 40, and the average being 2.25. We conclude
from this that the value of M is not a major factor
in the parser’s speed.

Remember that |F| is the number of input fea-
ture types. Caching 90% of the input feature com-
putations allows us to reasonably neglect this term.
Because |C| is the number of hidden-to-hidden
connection types, we cannot apply caching to re-
duce this term. However, |C| is much smaller
than |F| (here |C|=3). This is why caching in-
put feature computations has such a large impact
on parser speed.

1For this analysis, we assume that the output computation
is negligable compared to the hidden representation compu-
tation, because the output computation grows with d while
the hidden computation grows with d2.

6 Experimental Results

We used syntactic dependencies from the English
section of the CoNLL 2009 shared task dataset
(Hajič et al., 2009). Standard splits of training,
development and test sets were used. We compare
our model to the generative INN model (Titov and
Henderson, 2007b), MALT parser, MST parser,
and the feed-forward neural network parser of
(Chen and Manning, 2014) (“C&M”). All these
models and our own models are trained only on the
CoNLL 2009 syntactic data; they use no external
word embeddings or other unsupervised training
on additional data. This is one reason for choos-
ing these models for comparison. In addition, the
generative model (“Generative INN, large beam”
in Table 1) was compared extensively to state-of-
art parsers on various languages and tasks in pre-
vious work (Titov and Henderson, 2007b; Titov
and Henderson, 2007a; Henderson et al., 2008).
Therefore, here our objective is not repeating an
extensive comparison to the available parsers.

Table 1 shows the labeled and unlabeled ac-
curacy of attachments for these models. The
MALT and MST parser scores come from (Sur-
deanu and Manning, 2010), which compared dif-
ferent parsing models using CoNLL 2008 shared
task dataset, which is the same as CoNLL 2009
for English syntactic parsing. The results for the
generative INN with a large beam were taken from
(Henderson and Titov, 2010), which uses an archi-
tecture with 80 hidden units. We replicate this set-
ting for the other generative INN results and our
discriminative INN results. The parser of (Chen
and Manning, 2014) was run with their architec-
ture of 200 hidden units with dropout training
(“C&M”). All parsing speeds were computed us-
ing the latest downloadable versions of the parsers,
on a single 3.4GHz CPU.

Our model with beam 1 (i.e. deterministic
choices of chunks) (“DINN, beam 1”) produces
state-of-the-art results while it is over 35 times
faster than the generative model with beam size
10. Moreover, we are able to achieve higher ac-
curacies using larger beams (“DINN, beam 10”).
The discriminative training of the correctness
probabilities to optimize search is crucial to these
levels of accuracy, as indicated by the relatively
poor performance of our model when this training
is removed (“Discriminative INN, no search train-
ing”). Previous deterministic shift-reduce parsers
(“MALTAE” and “C&M”) are around twice as fast

149



Model LAA UAA wrd/sec
MALTAE 85.96 88.64 7549
C&M 86.49 89.17 9589
MST 87.07 89.95 290
MALT-MST 87.45 90.22 NA
Generative INN,
beam 1 77.83 81.49 1122
beam 10 87.67 90.61 107
large beam 88.65 91.44 NA

Discriminative INN,
no search training 85.28 88.98 4012
DINN, beam 1 87.26 90.13 4035
DINN, beam 10 88.14 90.75 433

Table 1: Labelled and unlabelled attachment accu-
racies and speeds on the test set.

Model
German Spanish Czech

LAA UAA LAA UAA LAA UAA
C&M 82.5 86.1 81.5 85.4 58.6 70.6
MALT 80.7 83.1 82.4 86.6 67.3 77.4
MST 84.1 87.6 82.7 87.3 73.4 81.7
DINN 86.0 89.6 85.4 88.3 77.5 85.2

Table 2: Labelled and unlabelled attachment accu-
racies on the test set of CoNLL 2009.

as our beam 1 model, but at the cost of significant
reductions in accuracy.

To evaluate our RNN model’s ability to induce
informative features automatically, we trained our
deterministic model, MALT, MST and C&M on
three diverse languages from CoNLL 2009, using
the same features as used in the above experiments
on English (model “DINN, beam 1”). We did no
language-specific feature engineering for any of
these parsers. Table 2 shows that our RNN model
generalizes substantially better than all these mod-
els to new languages, demonstrating the power of
this model’s feature induction.

7 Conclusion

We propose an efficient and accurate recurrent
neural network dependency parser that uses neu-
ral network hidden representations to encode arbi-
trarily large partial parses for predicting the next
parser action. This parser uses a search strategy
that prunes to a deterministic choice at each shift
action, so we add a correctness probability to each
shift operation, and train this score to discriminate
between correct and incorrect sequences of parser
actions. All other probability estimates are trained

to optimize the conditional probability of the parse
given the sentence. We also speed up both pars-
ing and training times by only decomposing infre-
quent features, giving us both a form of backoff
smoothing and twenty times faster training.

The discriminative training for this pruning
strategy allows high accuracy to be preserved
while greatly speeding up parsing time. The re-
current neural network architecture provides pow-
erful automatic feature induction, resulting in high
accuracy on diverse languages without tuning.

Acknowledgments

The research leading to this work was funded by
the EC FP7 programme FP7/2011-14 under grant
agreement no. 287615 (PARLANCE).

References
Yoshua Bengio, Réjean Ducharme, Pascal Vincent, and

Christian Janvin. 2003. A neural probabilistic lan-
guage model. J. Mach. Learn. Res., 3:1137–1155,
March.

Danqi Chen and Christopher Manning. 2014. A fast
and accurate dependency parser using neural net-
works. In Proceedings of the 2014 Conference on
Empirical Methods in Natural Language Processing
(EMNLP), pages 740–750. Association for Compu-
tational Linguistics, October.

Michael Collins and Brian Roark. 2004. Incremen-
tal parsing with the perceptron algorithm. In Pro-
ceedings of the 42nd Annual Meeting on Association
for Computational Linguistics, ACL ’04. Associa-
tion for Computational Linguistics.

Ronan Collobert and Jason Weston. 2008. A uni-
fied architecture for natural language processing:
Deep neural networks with multitask learning. In
Proceedings of the 25th International Conference
on Machine Learning, ICML ’08, pages 160–167.
ACM.

Ronan Collobert, Jason Weston, Léon Bottou, Michael
Karlen, Koray Kavukcuoglu, and Pavel Kuksa.
2011. Natural language processing (almost) from
scratch. J. Mach. Learn. Res., 12:2493–2537,
November.

Ronan Collobert. 2011. Deep learning for efficient
discriminative parsing. In Proceedings of the Four-
teenth International Conference on Artificial Intel-
ligence and Statistics (AISTATS-11), volume 15,
pages 224–232. Journal of Machine Learning Re-
search - Workshop and Conference Proceedings.

Hal Daumé III and Daniel Marcu. 2005. Learning
as search optimization: Approximate large margin
methods for structured prediction. In Proceedings

150



of the 22nd International Conference on Machine
Learning, pages 169–176.

Jacob Devlin, Rabih Zbib, Zhongqiang Huang, Thomas
Lamar, Richard Schwartz, and John Makhoul. 2014.
Fast and robust neural network joint models for sta-
tistical machine translation. In Proceedings of the
52nd Annual Meeting of the Association for Compu-
tational Linguistics (Volume 1: Long Papers). Asso-
ciation for Computational Linguistics.

Xavier Glorot, Antoine Bordes, and Yoshua Bengio.
2011. Domain adaptation for large-scale sentiment
classification: A deep learning approach. In Pro-
ceedings of the 28th International Conference on
Machine Learning (ICML-11), ICML ’11, pages
513–520. ACM, June.

Jan Hajič, Massimiliano Ciaramita, Richard Johans-
son, Daisuke Kawahara, Maria Antònia Martı́, Lluı́s
Màrquez, Adam Meyers, Joakim Nivre, Sebastian
Padó, Jan Štěpánek, Pavel Straňák, Mihai Surdeanu,
Nianwen Xue, and Yi Zhang. 2009. The conll-2009
shared task: Syntactic and semantic dependencies
in multiple languages. In Proceedings of the Thir-
teenth Conference on Computational Natural Lan-
guage Learning: Shared Task, CoNLL ’09, pages
1–18. Association for Computational Linguistics.

James Henderson and Ivan Titov. 2010. Incremental
sigmoid belief networks for grammar learning. J.
Mach. Learn. Res., 11:3541–3570, December.

James Henderson, Paola Merlo, Gabriele Musillo, and
Ivan Titov. 2008. A latent variable model of syn-
chronous parsing for syntactic and semantic depen-
dencies. In Proceedings of the Twelfth Confer-
ence on Computational Natural Language Learning,
CoNLL ’08, pages 178–182. Association for Com-
putational Linguistics.

James Henderson, Paola Merlo, Ivan Titov, and
Gabriele Musillo. 2013. Multilingual joint parsing
of syntactic and semantic dependencies with a latent
variable model. Comput. Linguist., 39(4):949–998,
December.

James Henderson. 2003. Inducing history representa-
tions for broad coverage statistical parsing. In Pro-
ceedings of the 2003 Conference of the North Amer-
ican Chapter of the Association for Computational
Linguistics on Human Language Technology - Vol-
ume 1, NAACL ’03, pages 24–31. Association for
Computational Linguistics.

James Henderson. 2004. Discriminative training of
a neural network statistical parser. In Proceedings
of the 42nd Meeting of the Association for Compu-
tational Linguistics (ACL’04), Main Volume, pages
95–102, July.

Liang Huang, Suphan Fayong, and Yang Guo. 2012.
Structured perceptron with inexact search. In Pro-
ceedings of the 2012 Conference of the North Amer-
ican Chapter of the Association for Computational

Linguistics: Human Language Technologies, pages
142–151. Association for Computational Linguis-
tics, June.

John D. Lafferty, Andrew McCallum, and Fernando
C. N. Pereira. 2001. Conditional random fields:
Probabilistic models for segmenting and labeling se-
quence data. In Proceedings of the Eighteenth Inter-
national Conference on Machine Learning, ICML
’01, pages 282–289. Morgan Kaufmann Publishers
Inc.

Tomáš Mikolov, Martin Karafiát, Lukáš Burget, Jan
Černocký, and Sanjeev Khudanpur. 2010. Recur-
rent neural network based language model. In Pro-
ceedings of the 11th Annual Conference of the In-
ternational Speech Communication Association (IN-
TERSPEECH 2010), volume 2010, pages 1045–
1048. International Speech Communication Associ-
ation.

Joakim Nivre, Johan Hall, and Jens Nilsson. 2004.
Memory-based dependency parsing. In Hwee Tou
Ng and Ellen Riloff, editors, HLT-NAACL 2004
Workshop: Eighth Conference on Computational
Natural Language Learning (CoNLL-2004), pages
49–56. Association for Computational Linguistics,
May 6 - May 7.

Joakim Nivre, Johan Hall, Jens Nilsson, Gülşen
Eryiǧit, and Svetoslav Marinov. 2006. Labeled
pseudo-projective dependency parsing with support
vector machines. In Proceedings of the Tenth Con-
ference on Computational Natural Language Learn-
ing, CoNLL-X ’06, pages 221–225. Association for
Computational Linguistics.

Rajat Raina, Alexis Battle, Honglak Lee, Benjamin
Packer, and Andrew Y. Ng. 2007. Self-taught
learning: Transfer learning from unlabeled data. In
Proceedings of the 24th International Conference
on Machine Learning, ICML ’07, pages 759–766.
ACM.

Richard Socher, Cliff Chiung-Yu Lin, Andrew Ng, and
Chris Manning. 2011. Parsing natural scenes and
natural language with recursive neural networks. In
Proceedings of the 28th International Conference
on Machine Learning (ICML-11), ICML ’11, pages
129–136. ACM.

Richard Socher, John Bauer, Christopher D. Manning,
and Ng Andrew Y. 2013. Parsing with composi-
tional vector grammars. In Proceedings of the 51st
Annual Meeting of the Association for Computa-
tional Linguistics (Volume 1: Long Papers), pages
455–465. Association for Computational Linguis-
tics, August.

Mihai Surdeanu and Christopher D. Manning. 2010.
Ensemble models for dependency parsing: Cheap
and good? In Human Language Technologies:
The 2010 Annual Conference of the North American
Chapter of the Association for Computational Lin-
guistics, HLT ’10, pages 649–652. Association for
Computational Linguistics.

151



Ilya Sutskever, James Martens, and Geoffrey Hin-
ton. 2011. Generating text with recurrent neu-
ral networks. In Proceedings of the 28th Interna-
tional Conference on Machine Learning (ICML-11),
ICML ’11, pages 1017–1024. ACM, June.

Ivan Titov and James Henderson. 2007a. Fast and
robust multilingual dependency parsing with a gen-
erative latent variable model. In Proceedings of
the CoNLL Shared Task Session of EMNLP-CoNLL
2007, pages 947–951. Association for Computa-
tional Linguistics, June.

Ivan Titov and James Henderson. 2007b. A latent
variable model for generative dependency parsing.
In Proceedings of the 10th International Conference
on Parsing Technologies, IWPT ’07, pages 144–155.
Association for Computational Linguistics.

Kaisheng Yao, Geoffrey Zweig, Mei-Yuh Hwang,
Yangyang Shi, and Dong Yu. 2013. Recurrent neu-
ral networks for language understanding. In INTER-
SPEECH, pages 2524–2528.

Yue Zhang and Stephen Clark. 2011. Syntactic pro-
cessing using the generalized perceptron and beam
search. Comput. Linguist., 37(1):105–151, March.

152


