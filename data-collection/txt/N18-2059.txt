



















































Structure Regularized Neural Network for Entity Relation Classification for Chinese Literature Text


Proceedings of NAACL-HLT 2018, pages 365–370
New Orleans, Louisiana, June 1 - 6, 2018. c©2018 Association for Computational Linguistics

Structure Regularized Neural Network for Entity Relation Classification
for Chinese Literature Text

Ji Wen1, Xu Sun1,2, Xuancheng Ren1, Qi Su3
1MOE Key Lab of Computational Linguistics, School of EECS, Peking University

2Deep Learning Lab, Beijing Institute of Big Data Research, Peking University
3School of Foreign Languages, Peking University

{wenjics, xusun, renxc, sukia}@pku.edu.cn

Abstract

Relation classification is an important seman-
tic processing task in the field of natural lan-
guage processing. In this paper, we propose
the task of relation classification for Chinese
literature text. A new dataset of Chinese liter-
ature text is constructed to facilitate the study
in this task. We present a novel model, named
Structure Regularized Bidirectional Recurrent
Convolutional Neural Network (SR-BRCNN),
to identify the relation between entities. The
proposed model learns relation representations
along the shortest dependency path (SDP)
extracted from the structure regularized de-
pendency tree, which has the benefits of re-
ducing the complexity of the whole model.
Experimental results show that the proposed
method significantly improves the F1 score by
10.3, and outperforms the state-of-the-art ap-
proaches on Chinese literature text1.

1 Introduction

Relation classification is the task of identifying
the semantic relation holding between two nom-
inal entities in text. Recently, neural networks
are widely used in relation classification. Wang
et al. (2016) proposes a convolutional neural net-
work with two levels of attention. Zhang et al.
(2015) uses bidirectional long short-term mem-
ory networks to model the sentence with sequen-
tial information. Bunescu and Mooney (2005)
first uses SDP between two entities to capture
the predicate-argument sequences. Wang et al.
(2017) explores the idea of incorporating syntac-
tic parse tree into neural networks. Liu et al.
(2017) proposes a noise-tolerant method to deal
with wrong labels in distant-supervised relation
extraction with soft labels. In recent years, we

1The Chinese literature text corpus for relation classi-
fication, developed and used by this paper, is available
at https://github.com/lancopku/Chinese-Li
terature-NER-RE-Dataset

have seen a move towards deep learning archi-
tectures. Liu et al. (2015) develops dependency-
based neural networks. Xu et al. (2015) applies
long short term memory (LSTM) (Hochreiter and
Schmidhuber, 1997) based recurrent neural net-
works (RNNs) along with the SDP.

In this paper, we focus on relation classification
of Chinese literature text, which to our knowledge
has not been studied before, due to the challenge.
Chinese literature text tends to express intuitions
and feelings. It has a wide range of topics. Many
literature articles express feelings in a subtle and
special way, making it more difficult to recognize
entities. Chinese literature text is not organized
very logically, whether among paragraphs or sen-
tences. They tend to use various and flexible forms
of sentences to create free feelings. The sentences
are not associated with each other by evident con-
junctions. Besides, Chinese is a topic-prominent
language, the subject is usually covert and the us-
age of words is relatively flexible.

In short, sentences of Chinese literature text
contain many non-essential words, and embody
very complex and flexible structures. Existing
methods make intensive use of the syntactical in-
formation, such as part-of-speech tags, and depen-
dency relations. However, the automatically gen-
erated information is not reliable and of poor qual-
ity for Chinese literature text. It is of great chal-
lenge for the existing methods to achieve satisfy-
ing performance.

To mitigate the noisy syntactical information,
we propose to apply structure regularization to the
structures used in relation classification. Recently,
many existing systems on structured prediction fo-
cus on increasing the level of structural depen-
dencies within the model. However, the theoret-
ical and experimental study of Sun (2014a) sug-
gests that complex structures are tend to increase
the overfitting risk, and can potentially be harm-

365



我[Person]便是那时候随了父母[Person]建设国营农场的梦想,来到

西洞庭的

At that time, I[Person] came to West Dongting with my parents'[Person] dream 

of building a state farm.

她[Person]用一片青菜叶[Thing]托着几块臭豆腐款款地送回你的手中,然后

踽踽前行

She[Person] handed you several pieces of stinky tofu using a cabbage leaf[Thing],

then traveled forward.

Family

Family

Use

Use

Figure 1: Examples from the Chinese literature text corpus.

ful to the model accuracy. As pointed out by Sun
(2014a), complex structural dependencies have a
drawback of increasing the generalization risk, be-
cause more complex structures are easier to suffer
from overfitting.

In this paper, we focus on the study of applying
structure regularization to the relation classifica-
tion task of Chinese literature text. To summarize,
the contributions of this paper are as follows:

• To our knowledge, we are the first to develop
a corpus of Chinese literature text for relation
classification. The corpus contains 837 arti-
cles. It helps alleviate the dilemma of the lack
of corpus in Chinese Relation Classification.

• We develop the tree-based structure regular-
ization methods and make progress on the
task of relation classification. The method
of structure regularization is normally used
on the structure of sequences, while we find
a way to realize it on the structure of trees.
Comparing to the original model, apply-
ing structure regularization substantially im-
proves the F1 score by 10.3.

2 Chinese Literature Text Corpus

In Figure 1, we show two examples from the an-
notated corpus. We label the entities and relations
of the text on a sentence level. There are 6 kinds of
entities and 9 kinds of relations. Details of the tags
are shown in Table 1. The task aims at predicting
the labels of these relations, given the sentences as
well as the entities and their types. The corpus is
part of the work of Xu et al. (2017).

We obtain over 1,000 Chinese prose articles
from the Internet and then filter and extract 837
articles. Articles that are too short or too noisy are
not included. Due to the difficulty of tagging Chi-
nese prose text, we divide the annotation process
into three steps.

First, we attempt to annotate the raw articles
based on defined entity and relation tags. Second,
we design several generic disambiguation rules to
ensure the consistency of annotation guidelines.
For example, remove all adjective words and only
tag “entity header” when tagging entities (e.g.,
change “a girl in red cloth” to “girl”). In this stage,
we re-annotate all articles and correct all inconsis-
tency entities based on the heuristic rules. Even
though the heuristic tagging process significantly
improves dataset quality, it is too hard to handle
all inconsistency cases based on limited heuris-
tic rules. Finally, we introduce a machine auxil-
iary tagging method. The core idea is to train a
model to learn annotation guidelines on the subset
of the corpus and produce predicted tags on the
rest data. The predicted tags are used to be com-
pared with the gold tags to discovery inconsistent
entities, which largely reduce annotators’ efforts.
After all annotation steps, we also manually check
all entities and relations to ensure the correctness
of corpus.

In prior work, Chinese literature text corpus is
very rare. Many tasks cannot achieve a satisfy-
ing result on Chinese literature text compared to
other corpus. However, understanding Chinese lit-
erature text is of great importance to Chinese liter-
ature research.

366



Tag Description Example %
Located locate in 幽兰(orchid)-山谷(valley) 37.43

Part-Whole be a part of 花(flower)-仙人掌(cactus) 23.76
Family be family members 母亲(mother)-奶奶(grandmother) 10.25

General-Special be a general range and a special kind of it 鱼(fish)-鲫鱼(carp) 6.99
Social be socially related 母亲(mother)-邻里(neighbour) 6.02

Ownership be in possession of 村民(villager)-旧屋(house) 5.10
Use do something with 爷爷(grandfather)-毛笔(brush) 4.76

Create make happen or exist 男人(man)-陶器(pottery) 2.93
Near a short distance away 山(hill)-县城(town) 2.76

Table 1: The set of relation tags. The last column indicates each tag’s relative frequency in the full
annotated data.

3 Structure Regularized BRCNN

3.1 Basic BRCNN

The Bidirectional Recurrent Convolutional Neural
Network (BRCNN) model is used to learn repre-
sentations with bidirectional information along the
shortest dependency path (SDP).

Given a sentence and its dependency tree, we
build our neural network on its SDP extracted
from tree. Along the SDP, recurrent neural net-
works are applied to learn hidden representations
of words and dependency relations, respectively.
A convolution layer is applied to capture local
features from hidden representations of every two
neighbor words and the dependency relations be-
tween them. A max pooling layer thereafter gath-
ers information from local features of the SDP and
the inverse SDP. We have a softmax output layer
after pooling layer for classification in the unidi-
rectional model RCNN.

On the basis of RCNN model, we build a bidi-
rectional architecture BRCNN taking the SDP and
the inverse SDP of a sentence as input. Dur-
ing the training stage of a (K+1)-relation task,
two fine-grained softmax classifiers of RCNNs do
a (2K+1)-class classification respectively. The
pooling layers of two RCNNs are concatenated
and a coarse-grained softmax output layer is fol-
lowed to do a (K+1)-class classification. The fi-
nal (2K+1)-class distribution is the combination
of two (2K+1)-class distributions provided by fine
grained classifiers during the testing stage.

We use two bidirectional LSTMs to capture the
features of words and relations separately. After
we obtain representations of words and relations,
we concatenate them to get a representation of a
complete dependency unit. The hidden state of
a relation is denoted as rab. Words on its sides
have the hidden states denoted as ha and hb. [ha

hab hb] denotes the representation of a dependency
unit Lab. Then we utilize a convolution layer upon
the concatenation. We have

Lab = f(Wcon · [ha ⊕ h′ab ⊕ hb] + bcon) (1)

where Wcon is the weight matrix and bcon is a bias
term. We choose tanh as our activation function
and apply max pooling following the activation.

Two RCNNs pick up information along the SDP
and its reverse. A coarse-grained softmax classi-
fier is applied on the global representations

−→
G and←−

G . Two fine-grained softmax classifier are applied
to to give a more detailed prediction of 2K+1 class.

−→y = softmax(Wf ·
−→
G + bf ) (2)

←−y = softmax(Wf ·
←−
G + bf ) (3)

During training, our objective is the penalized
cross-entropy of three classifiers. Formally,

J =
2K+1∑

i=1

−→
ti log

−→yi +
2K+1∑

i=1

←−
ti log

←−yi

+

K∑

i=1

ti log yi + λ · ‖θ‖2
(4)

When decoding, the final prediction is a combina-
tion of −→y and←−y

ytest = α · −→y + (1− α) · z(←−y ) (5)

where α is the fraction of the composition of dis-
tributions. We apply a function z to transform←−y
to a corresponding forward distribution like −→y .

3.2 Structure Regularized BRCNN
The basic BRCNN model can handle the task to
some extent, but there still remains some weak-
ness, especially dealing with long sentences with

367



Models Information F1 Score

Baselines

SVM Word embeddings, NER, WordNet, HowNet, 48.9(Hendrickx et al., 2010) POS, dependency parse, Google n-gram
RNN Word embeddings 48.3

(Socher et al., 2011) + POS, NER, WordNet 49.1
CNN Word embeddings 47.6

(Zeng et al., 2014) + word position embeddings, NER, WordNet 52.4
CR-CNN Word embeddings 52.7

(dos Santos et al., 2015) + word position embeddings 54.1
SDP-LSTM Word embeddings 54.9

(Xu et al., 2015) + POS + NER + WordNet 55.3
DepNN Word embeddings, WordNet 55.2(Liu et al., 2015)
BRCNN Word embeddings 55.0

(Cai et al., 2016) + POS, NER, WordNet 55.6

Our Model SR-BRCNN Word embeddings 65.2 (+9.6)+ POS, NER, WordNet 65.9 (+10.3)

Table 2: Comparison of relation classification systems on Chinese literature text.

complicated structures. The SDP generated from a
more complicated dependency tree consists more
irrelevant words. Sun (2014b) shows both theoret-
ically and empirically that structure regularization
can effectively control overfitting risk and lead to
better performance. Sun et al. (2017a) and Sun
et al. (2017b) also show that complex structure
models are prone to the structure-based overfit-
ting. Therefore, we propose the structure regular-
ized BRCNN.

We conduct structure regularization on the de-
pendency tree of the sentences. Based on the
heuristic rules, several nodes in the dependency
tree are selected. The subtrees of these selected
nodes are cut from the whole dependency tree.
With these selected nodes as the roots, these sub-
trees form a forest. The forest will be connected
by lining the roots of the trees of the forest. Tradi-
tional SDP is extracted directly from the depen-
dency tree, while in our model, the SDP is ex-
tracted from the final forest. We call these kinds of
SDPs as SR-SDPs. We build our BRCNN model
on the SR-SDP.

3.3 Various Structure Regularization
Methods

We experiment with three kinds of regularization
rules. First, the punctuation is a natural break
point of the sentence. The resulting subtrees usu-
ally keep similar syntax to traditional dependency
trees. Another popular method to regularize the
structure is to decompose the structure randomly.
In our model, we randomly select several nodes in
the dependency tree and then cut the subtrees un-
der these nodes. Finally we decide to cut the de-
pendency tree by prepositions. Especially in Chi-

a

b c

d e

f g

(a) The dependency
tree and the SDP be-
fore flattening.

a

b c

d

e

f g

(b) The dependency tree
and the SDP after flatten-
ing.

Figure 2: An example of the proposed method. The
two words in circles are the entities, and the thick
edges form the SDP. By flattening the dependency
tree, the path becomes shorter.

nese literature text, there usually are many deco-
rations to describe the entities, and the using of
prepositional phrases is very common for that pur-
pose. So we also try to decompose the dependency
trees using prepositions.

4 Experiments

We evaluate our model on the Chinese literature
text corpus. It contains 9 distinguished types of
relations among 837 articles. The dataset contains
695 articles for training, 58 for validation, and 84
for testing.

4.1 Experiment settings

We use pre-trained word embeddings, which are
trained on Gigaword with word2vec (Mikolov
et al., 2013). Word embeddings are 200-
dimensional. The embeddings of relation are ini-
tialized randomly and are 50-dimensional. The

368



hidden layers of LSTMs to extract information
from entities and relations are the same as the em-
bedding dimension of entities and relations. We
applied L2 regularization to weights in neural net-
works and dropout to embeddings with a keep
probability 0.5. AdaDelta (Zeiler, 2012) is used
for optimization.

4.2 Experimental Results
Table 2 compares our SR-BRCNN model with
other state-of-the-art methods on the corpus of
Chinese literature text, including the basic BR-
CNN method (Cai et al., 2016). Structure regular-
ization helps improve the result substantially. The
method of structure regularization could prevent
the overfitting of poor quality SDPs.

4.3 Analysis: Effect of SR
Figure 2a and Figure 2b show an example of struc-
ture regularized SDP. The relation is between the
two circled elements. The main idea of the method
is to avoid the incorrect structure from the depen-
dency trees generated by the parser. The SDP in
Figure 2a is longer than the SR-SDP in Figure 2b.
However, the dependency tree of the example is
not completely correct. The longer the SDP is, the
more incorrect information the model learns.

The structure regularized BRCNN has shown
obvious improvements. We attribute the improve-
ments to the simplified structures that generated
by structure regularization. The internal relations
of components of a sentence are more obscure
due to the feature of Chinese literature text. By
conducting structure regularization on the depen-
dency tree, we get several subtrees with simpler
structure, and then we extract SDP from the lined
forests. In most cases, the distance between two
entities will be shortened along the new SR-SDP.
Without the redundant information along the orig-
inal SDP. The model that benefits from the inten-
sive dependencies will capture more effective in-
formation for classification.

4.4 Analysis: Effect of Different
Regularization Methods

The punctuation is a natural break point of the sen-
tence, which makes subtrees more like the tradi-
tional dependency trees in the aspect of integrity.
However, the original dependency trees cannot be
sufficiently regularized. Despite its drawbacks,
this method still shows obvious improvements on
the model and leads to further experiments.

Classifier F1 score
BRCNN 55.6
SR by punctuation 59.7
SR by random 62.4
SR by preposition 65.9

Table 3: Different structure regularization results
on Chinese literature text.

Regularizing the structure by decomposing the
structure randomly will solve the insufficient de-
composition problems. The method of structure
regularization has shown that the degree of loss of
information is not a serious problem. It gives a
slightly better result compared to cutting depen-
dency trees by punctuations.

A more elaborate method is to cut the depen-
dency tree by prepositions. In Chinese literature
text, prepositional phrases are used frequently.
Cutting by prepositions will regularize the tree
more sufficiently. Meanwhile, the subtrees un-
der the prepositional nodes are usually internally
linked.

5 Conclusions

In this paper, we present a novel model, Structure
Regularized BRCNN, to classify the relation of
two entities in a sentence. We demonstrate that
tree-based structure regularization can help im-
prove the results, while the method is normally
used in sequence-based models before. The pro-
posed structure regularization method makes the
SDP shorter and contain less noise from the un-
reliable parse trees. This leads to substantial im-
provements on the relation classification results.
The results also show how different ways of regu-
larization act in the model of BRCNN.

We also develop a corpus on Chinese literature
text focusing on the task of Relation Classifica-
tion. The new corpus is large enough for us to
train models and verify the models.

Acknowledgements

This work was supported in part by National Natu-
ral Science Foundation of China (No. 61673028),
National High Technology Research and Devel-
opment Program of China (863 Program, No.
2015AA015404), and the National Thousand
Young Talents Program. Xu Sun is the corre-
sponding author of this paper.

369



References
Razvan Bunescu and Raymond Mooney. 2005. A

shortest path dependency kernel for relation extrac-
tion. In Proceedings of Human Language Technol-
ogy Conference and Conference on Empirical Meth-
ods in Natural Language Processing, pages 724–
731.

Rui Cai, Xiaodong Zhang, and Houfeng Wang. 2016.
Bidirectional recurrent convolutional neural network
for relation classification. In Proceedings of the
54th Annual Meeting of the Association for Compu-
tational Linguistics (Volume 1: Long Papers), pages
756–765.

Iris Hendrickx, Su Nam Kim, Zornitsa Kozareva,
Preslav Nakov, Diarmuid Ó. Séaghdha, Sebastian
Padó, Marco Pennacchiotti, Lorenza Romano, and
Stan Szpakowicz. 2010. Semeval-2010 task 8:
Multi-way classification of semantic relations be-
tween pairs of nominals. In Proceedings of the
5th International Workshop on Semantic Evaluation,
pages 33–38.

Sepp Hochreiter and Jürgen Schmidhuber. 1997.
Long short-term memory. Neural computation,
9(8):1735–1780.

Tianyu Liu, Kexiang Wang, Baobao Chang, and Zhi-
fang Sui. 2017. A soft-label method for noise-
tolerant distantly supervised relation extraction. In
Proceedings of the 2017 Conference on Empirical
Methods in Natural Language Processing, pages
1790–1795.

Yang Liu, Furu Wei, Sujian Li, Heng Ji, Ming Zhou,
and Houfeng WANG. 2015. A dependency-based
neural network for relation classification. In Pro-
ceedings of the 53rd Annual Meeting of the Associ-
ation for Computational Linguistics and the 7th In-
ternational Joint Conference on Natural Language
Processing (Volume 2: Short Papers), pages 285–
290.

Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Cor-
rado, and Jeff Dean. 2013. Distributed representa-
tions of words and phrases and their compositional-
ity. In Advances in neural information processing
systems, pages 3111–3119.

Cicero dos Santos, Bing Xiang, and Bowen Zhou.
2015. Classifying relations by ranking with con-
volutional neural networks. In Proceedings of the
53rd Annual Meeting of the Association for Compu-
tational Linguistics and the 7th International Joint
Conference on Natural Language Processing (Vol-
ume 1: Long Papers), pages 626–634.

Richard Socher, Jeffrey Pennington, Eric H. Huang,
Andrew Y. Ng, and Christopher D. Manning. 2011.
Semi-supervised recursive autoencoders for predict-
ing sentiment distributions. In Proceedings of the
2011 Conference on Empirical Methods in Natural
Language Processing, pages 151–161.

Xu Sun. 2014a. Structure regularization for structured
prediction. In Advances in Neural Information Pro-
cessing Systems 27: Annual Conference on Neural
Information Processing Systems 2014, pages 2402–
2410.

Xu Sun. 2014b. Structure regularization for struc-
tured prediction: Theories and experiments. CoRR,
abs/1411.6243.

Xu Sun, Xuancheng Ren, Shuming Ma, Bingzhen Wei,
Wei Li, and Houfeng Wang. 2017a. Training simpli-
fication and model simplification for deep learning:
A minimal effort back propagation method. CoRR,
abs/1711.06528.

Xu Sun, Weiwei Sun, Shuming Ma, Xuancheng Ren,
Yi Zhang, Wenjie Li, and Houfeng Wang. 2017b.
Complex structure leads to overfitting: A structure
regularization decoding method for natural language
processing. CoRR, abs/1711.10331.

Linlin Wang, Zhu Cao, Gerard de Melo, and Zhiyuan
Liu. 2016. Relation classification via multi-level
attention CNNs. In Proceedings of the 54th An-
nual Meeting of the Association for Computational
Linguistics (Volume 1: Long Papers), pages 1298–
1307.

Yizhong Wang, Sujian Li, Jingfeng Yang, Xu Sun, and
Houfeng Wang. 2017. Tag-enhanced tree-structured
neural networks for implicit discourse relation clas-
sification. In Proceedings of the Eighth Interna-
tional Joint Conference on Natural Language Pro-
cessing (Volume 1: Long Papers), pages 496–505.

Ji Wen. 2017. Structure regularized bidirectional recur-
rent convolutional neural network for relation clas-
sification. CoRR, abs/1711.02509.

Jingjing Xu, Ji Wen, Xu Sun, and Qi Su. 2017. A
discourse-level named entity recognition and rela-
tion extraction dataset for chinese literature text.
CoRR, abs/1711.07010.

Yan Xu, Lili Mou, Ge Li, Yunchuan Chen, Hao Peng,
and Zhi Jin. 2015. Classifying relations via long
short term memory networks along shortest depen-
dency paths. In Proceedings of the 2015 Confer-
ence on Empirical Methods in Natural Language
Processing, pages 1785–1794.

Matthew D. Zeiler. 2012. ADADELTA: an adaptive
learning rate method. CoRR, abs/1212.5701.

Daojian Zeng, Kang Liu, Siwei Lai, Guangyou Zhou,
and Jun Zhao. 2014. Relation classification via con-
volutional deep neural network. In Proceedings of
COLING 2014, the 25th International Conference
on Computational Linguistics: Technical Papers,
pages 2335–2344.

Shu Zhang, Dequan Zheng, Xinchen Hu, and Ming
Yang. 2015. Bidirectional long short-term memory
networks for relation classification. In Proceedings
of the 29th Pacific Asia Conference on Language,
Information and Computation, pages 73–78.

370


