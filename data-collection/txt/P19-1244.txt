



















































Sentence-Level Evidence Embedding for Claim Verification with Hierarchical Attention Networks


Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 2561–2571
Florence, Italy, July 28 - August 2, 2019. c©2019 Association for Computational Linguistics

2561

Sentence-Level Evidence Embedding for Claim Verification with
Hierarchical Attention Networks

Jing Ma1, Wei Gao2, Shafiq Joty3,4, Kam-Fai Wong1,5
1The Chinese University of Hong Kong, Hong Kong SAR

2Victoria University of Wellington, New Zealand
3Nanyang Technological University, Singapore

4Salesforce Research Asia, Singapore
5MoE Key Laboratory of High Confidence Software Technologies, China

1{majing,kfwong}@se.cuhk.edu.hk
2wei.gao@vuw.ac.nz, 3srjoty@ntu.edu.sg

Abstract

Claim verification is generally a task of ver-
ifying the veracity of a given claim, which
is critical to many downstream applications.
It is cumbersome and inefficient for human
fact-checkers to find consistent pieces of evi-
dence, from which solid verdict could be in-
ferred against the claim. In this paper, we pro-
pose a novel end-to-end hierarchical attention
network focusing on learning to represent co-
herent evidence as well as their semantic relat-
edness with the claim. Our model consists of
three main components: 1) A coherence-based
attention layer embeds coherent evidence con-
sidering the claim and sentences from rele-
vant articles; 2) An entailment-based attention
layer attends on sentences that can semanti-
cally infer the claim on top of the first atten-
tion; and 3) An output layer predicts the ver-
dict based on the embedded evidence. Ex-
perimental results on three public benchmark
datasets show that our proposed model outper-
forms a set of state-of-the-art baselines.

1 Introduction

The increasing popularity of social media has
drastically changed how our daily news are pro-
duced, disseminated and consumed.1 Without sys-
tematic moderation, a large volume of informa-
tion based on false or unverified claims (e.g., fake
news, rumours, propagandas, etc.) can proliferate
online. Such misinformation poses unprecedented
challenges to information credibility, which tradi-
tionally relies on fact-checkers to manually assess
whether specific claims are true or not.

Despite the increased demand, the effectiveness
and efficiency of human fact-checking is handi-
capped by the volume and fast pace the noteworthy

1The latest Pew Research statistics show that 68%
American adults at least occasionally get news on social
media. http://www.pewinternet.org/2018/03/
01/social-media-use-in-2018/

claims being produced on daily basis. Therefore,
it is an urgent need to automate the process and
ease the human burden in assessing the veracity of
claims (Thorne and Vlachos, 2018).

Not surprisingly, various methods for automatic
claim verification have been proposed using ma-
chine learning. Typically, given the claims, mod-
els are learned from auxiliary relevant sources
such as news articles or social media responses
for capturing words and linguistic units that might
indicate viewpoint or language style towards the
claim (Jin et al., 2016; Rashkin et al., 2017; Popat
et al., 2017; Volkova et al., 2017; Dungs et al.,
2018). However, the factuality of a claim is inde-
pendent of people’s belief and subjective language
use, and human perception is unconsciously prone
to misinformation due to the common cognitive
biases such as naive realism (Reed et al., 2013)
and confirmation bias (Nickerson, 1998).

A recent trend is that researchers are trying to
establish more objective tasks and evidence-based
verification solutions, which focus on the use of
evidence obtained from more reliable sources,
e.g., encyclopedia articles, verified news, etc., as
an important distinguishing factor (Thorne and
Vlachos, 2018). Ferreira and Vlachos (2016) use
news headlines as evidence to predict whether it
is for, against or observing a claim. In the Fake
News Challenge2, the body text of an article is
used as evidence to detect the stances relative to
the claim made in the headline. Thorne et al.
(2018a) formulate the Fact Extraction and VER-
ification (FEVER) task which requires extract-
ing evidence from Wikipedia and synthesizing in-
formation from multiple documents to verify the
claim. Popat et al. (2018) propose DeClarE, an
evidence-aware neural attention model to aggre-
gate salient words from source news articles as the

2http://www.fakenewschallenge.org/

http://www.pewinternet.org/2018/03/01/social-media-use-in-2018/
http://www.pewinternet.org/2018/03/01/social-media-use-in-2018/
http://www.fakenewschallenge.org/


2562

c: The test of a 5G cellular network is the cause of
unexplained bird deaths occurring in a park in The
Hague, Netherlands.
Verdict: False

s1: [Contradict]: Lots of tests going on with it in the
Netherlands, but there haven’t been test done in The
Haque during the time that the mysterious starling
deaths occurred.

s2: [Contradict]: One such test did occur in an area
generally near Huijgenspark, but it took place on
28 June 2018.

s3: [Entail]: It’s not clear whether tests with 5G have
been carried out again, but so far everything points
in the direction of 5G as the most probable cause.

s4: [Neutral]: Between Friday, 19 Oct and Saturday, 3
Nov 2018, 337 dead starlings and 2 dead common
wood pigeons were found.

s5: [Entail]: The radiation created on the attempt of 5G
cellular networks are not harmful only for birds but
also for humans too.

s6: [Neutral]: 5G network developers promise faster
data rates in addition to reduce energy and financial
cost.

s7: [Neutral]: Parts of the park are blocked and dogs
are no longer allowed to be let out, the dead birds
are always cleaned up as quickly as possible.

Table 1: Sentences topically coherent (s1–s4) and not
coherent (s5–s7) with each other relative to the claim c,
where their semantic entailment relations (i.e., entail,
contradict, neural) with c are shown.

main evidence to obtain claim-specific representa-
tion based on the attention score of each token.

Inspired by the FEVER task (Thorne et al.,
2018a) and DeClarE (Popat et al., 2018), we pro-
pose our approach to claim verification by using
representation learning to embed sentence-level
evidences based on coherence modeling and natu-
ral language inference (NLI). The example in Ta-
ble 1 illustrates our general idea: given a claim
“The test of a 5G cellular network is the cause
of unexplained bird deaths occurring in a park in
The Hague, Netherlands” and its relevant articles,
we try to embed into the claim-specific represen-
tation those evidential sentences (e.g., s1–s4) that
are not only topically coherent among themselves
considering the claim, but could also semantically
infer the claim based on textual entailment rela-
tions such as entail, contradict, and neutral. It
is hypothesized that sentence-level evidence can
convey more complete and deeper semantics, thus
providing stronger NLI capacity between claim
and evidence, which would result in better claim-
specific representation for the more accurate fact-
checking decision.

In this work, we propose an end-to-end hier-
archical attention network for sentence-level ev-

idence embedding that aims to attend on impor-
tant sentences (i.e., evidence) by considering their
topical coherence and semantic inference strength.
Different from DeclarE (Popat et al., 2018), our
model can determine the verdict of a claim more
reasonably with evidential sentences embedded
into the learned claim representation. Meanwhile,
with the help of attention, crucial evidence can be
highlighted and referred for better interpretability
of the verdict. Our model is also advantageous
over pipeline methods such as Neural Semantic
Matching Network (NSMN) (Nie et al., 2019)
which topped the FEVER shared task (Thorne
et al., 2018b), because our model can be trained to
address evidence representation learning directly
rather than rank and select sentences semantically
similar to the claim. Our contributions are sum-
marized as follows:

• We propose a novel claim verification frame-
work based on hierarchical attention neural net-
works to learn sentence-level evidence embed-
dings to obtain claim-specific representation.
• We use a co-attention mechanism to model sen-

tence coherence and integrate the coherence-
and entailment-based attentions into our pro-
posed hierarchical attention framework for bet-
ter evidence embedding.
• We experimentally confirm that our method is

much more effective than several state-of-the-
art claim verification models using three public
benchmark datasets collected from snopes.com,
politifact.com and Wikipedia.

2 Related Work

The literature on fact-checking and credibility as-
sessment has been reviewed by several compre-
hensive surveys (Shu et al., 2017; Zubiaga et al.,
2018; Kumar and Shah, 2018; Sharma et al.,
2019). We only briefly review prior works closely
related to ours.

Many studies on claim verification extracted
veracity-indicative features that can reflect stances
and writing styles from relevant texts such as
news articles, microblog posts, etc. and used
the traditional supervised models to learn the pa-
rameters (Castillo et al., 2011; Qazvinian et al.,
2011; Rubin et al., 2016; Ferreira and Vla-
chos, 2016; Rashkin et al., 2017). Deep learn-
ing models such as recurrent neural networks
(RNN) (Ma et al., 2016), convolutional neural net-
works (CNN) (Wang, 2017) and recursive neural



2563

networks (Ma et al., 2018) were also exploited to
learn the feature representations.

More recently, semantic matching methods
were proposed to retrieve evidence from relatively
trustworthy sources such as checked news and
Wikipedia articles. Popat et al. (2018) attempted
to debunk false claims by learning claim repre-
sentations from relevant articles using an atten-
tion mechanism to focus on words that are closely
related to the claim. Following NLI (Bowman
et al., 2015), which is a task of classifying the re-
lationship between a pair of sentences, composed
by a premise and a hypothesis, as Entails, Con-
tradicts or Neutral, Thorne et al. (2018a) formu-
lated claim verification as a task that aims to clas-
sify claims into Supported, Refuted or Not Enough
Info (NEI). They released a large dataset contain-
ing mutated claims based on relevant Wikipedia
articles and developed a basic pipeline with docu-
ment retrieval, sentence selection, and NLI mod-
ules. Similar pipelines were developed by most
of the participating teams (Nie et al., 2019; Pa-
dia et al., 2018; Alhindi et al., 2018; Hanselowski
et al., 2018) in FEVER shared task (Thorne et al.,
2018b). Apart from the document retrieval func-
tion, our model is end-to-end and aims to learn
sentence-level evidence with a hierarchical atten-
tion framework.

Attention is in general used to attend on the
most important part of texts, and has been success-
fully applied in machine translation (Luong et al.),
question answering (Xiong et al., 2016) and pars-
ing (Dozat and Manning, 2016), and is adopted in
our model for attending on important sentences as
evidence. Our work is also related to coherence
modeling. Different from traditional coherence
studies focusing on discourse coherence among
sentences that are widely applied in text genera-
tion (Park and Kim, 2015; Kiddon et al., 2016) and
summarization (Logeswaran et al., 2018), we try
to capture evidential sentences topically coherent
not only among themselves but also with respect
to the target claim.

3 Problem Statement

We define a claim verification dataset as {C},
where each instance C = (y, c, S) is a tuple repre-
senting a given claim c which is associated with
a ground-truth label y and a set of n sentences
S = {si}ni=1 from the relevant documents of the
claim. We assume the relevant documents are re-

trieved from text collections containing variable
number of sentences, and we disregard the order
of sentences and which documents they are from.
Our task is to classify an instance into a class de-
fined by the specific dataset, such as veracity class
labels, e.g., True/False, or NLI-style class labels,
e.g., Supported/Refuted/NEI.

Our approach exploits and integrates two core
semantic relations: 1) coherence of the sentences
given the claim; 2) entailment relation between
the claim and each sentence, which are described
more specifically below.
Coherence Evaluation: According to the coher-
ence theory of truth, the truth of any (true) propo-
sition consists in its coherence with some speci-
fied set of propositions (Young, 2018). In order to
focus on the useful evidence in a set of relevant
sentences S, we propose a coherence-based atten-
tion component by cross-checking if any sentence
si ∈ S coheres well with the claim and with other
sentences in S in terms of topical consistency.
Textual Entailment: Entailment is used to mea-
sure whether a piece of evidence semantically in-
fers a given claim. We propose an entailment-
based attention component that can be pre-trained
to capture entailment relations (Dagan et al., 2010;
Bowman et al., 2015) based on sentence pairs la-
beled with NLI-specific classes: entails, contra-
dicts and neutral. This pre-trained component to-
gether with the entire claim verification frame-
work then will be trained end-to-end to attend on
the salient sentences for inferring the claim.

4 End-to-End Claim Verification Model

In this section, we introduce our end-to-end hi-
erarchical attention network for claim verifica-
tion, which consist of two attention layers, i.e.,
coherence-based attention and entailment-based
attention, for learning evidence embeddings. Fig-
ure 1 gives an overview of our framework, which
will be depicted in detail in the subsections.

4.1 Sentence Representation

Given a word sequence T = (w1 . . . wt . . . w|T |)
which could be either a claim or a sentence, each
wt ∈ Rd is d-dimensional vector which can be
initialized with pre-trained word embeddings. We
map eachwt into a fixed-sized hidden vector using
standard GRU (Cho et al., 2014). We then obtain
the sentence-level representation for a claim c and
each sentence si ∈ S using two GRU-based RNN



2564

Figure 1: Our end-to-end hierarchical attention networks for claim verification.

encoders (one for c and the other for si):

hc = h|c| = GRU(w|c|, h|c|−1, θc)

hsi = h|si| = GRU(w|si|, h|si|−1, θS)
(1)

where |.| denotes the number of words, w|c| is the
last word of c, w|si| is the last word of si, θc con-
tains the claim encoder parameters, θS contains
the sentence encoder parameters, and hc, hsi ∈
R1×l are l-dimensional vectors.

4.2 Coherence-based Evidence Attention
Our assumption is that sentences used as evidence
should be topically coherent given a claim. For
example, for the claim in Table 1, which is about
the connection between 5G test and birds’ death in
a park in Hague, the sentences s1-s4 are topically
coherent by specifically addressing the event’s de-
tail while s5-s7 are marginal as s6 and s7 diverge
from the focus and s5 is a too general statement
even though it might imply a possibility.

Our model cross-checks all the sentences to
capture the coherence among them using an atten-
tion mechanism. We consider the relation from
two perspectives: 1) global coherence measures
the consistency of each sentence regarding the en-
tire set as a whole; and 2) local coherence mea-
sures the consistency of each sentence consider-
ing its relation with another sentence. For each si,
we use a biaffine attention (Dozat and Manning,
2016), which naturally fits our problem, to get the
attention weights:

ãi = (HS ·Wc) · h>si +HS · u
>

α̃i = softmax(ãi)
(2)

where HS = [hs1 ; . . . ;hsn ] ∈ Rn×l is the ma-
trix representing all sentences, andWc ∈ Rl×l and

u ∈ R1×l contain the weights of the biaffine trans-
formation. The term HS · u> ∈ Rn×1 denotes
the global coherence where each element is a prior
probability of a sentence sj being coherent with
any sentences in S; the term (HS · Wc) · h>si ∈
Rn×1 is the local coherence where each element
hsj ·Wc · h>si represents the relative likelihood of
sj being coherent with si. Therefore, α̃i ∈ Rn×1
is a n-dimensional weight vector for si where each
element α̃ij for j ∈ [1, . . . , n] denotes the coher-
ence attention weight between si and sj .

Extension of Coherence Attention
The coherence attention in Eq. 2 ignores the claim
information. To prevent off-topic coherence which
deviates from claim’s focus, we propose to assess
each sentence’s coherence by jointly considering
the claim and all sentences, which shares a similar
intuition with the co-attention method in question-
answering (Lu et al., 2016; Xiong et al., 2016).

Unlike the question-answer co-attention focus-
ing on mutual selection of salient words in ques-
tion and documents, we focus on sentence-level at-
tention, for which we have multiple sentences but
only one claim. So, we only need a claim-guided
sentence attention. We use a gating unit to endow
the model with the capacity of deciding how much
information it should accept from the claim. The
new attention weight of si is computed by:

h̄si = g
c→si � hsi + (1− gc→si)� hc

āi = (H̄S ·Wc) · h̄>si + H̄S · u
>

ᾱi = softmax(āi)

(3)

where gc→si = σ(Wg · hsi + Ug · hc) is the gate
function with trainable parameters Wg and Ug,
H̄S = [h̄s1 ; . . . ; h̄sn ] denotes the stacked output



2565

of the gating unit, and other settings are same as
the biaffine coherence attention (see Eq. 2).

Based on the attention weights, each sentence
can be represented as the weighted sum of all sen-
tences, capturing its overall coherence:

h′si =
∑
j

αij · hsj (4)

where αij is the attention weight between si and
sj obtained from Eq. 2 (α̃i) or Eq. 3 (ᾱi).

Finally, we concatenate the coherence-based
sentence embedding h′si with the original embed-
ding hsi to obtain a richer sentence representation:

h̃si = tanh(Wco · [hsi , h′si ] + bco) (5)

where Wco and bco are parameters for transform-
ing the concatenation into a l-dimensional vector.

4.3 Entailment-based Evidence Attention

We further enhance the sentence representation
by capturing the entailment relations between
the sentences and the claim based on the NLI
method (Bowman et al., 2015) for strengthening
the semantic inference capacity of our model.

Given c and si, we represent each such pair by
integrating three matching functions between hc
and h̃si : 1) concatenation [hc, h̃si ]; 2) element-
wise product hc � h̃si ; and 3) absolute element-
wise difference |hc − h̃si |. The similar matching
scheme was commonly used to train NLI mod-
els (Conneau et al., 2017; Mou et al., 2016; Liu
et al., 2016; Chen et al., 2016). We then perform
a transformation to obtain the joint representation
hcsi as follow:

hcsi = tanh
(
We ·

[
hc, h̃si , hc � h̃si , |hc − h̃si |

])
(6)

where We are trainable weights for transforming
the long concatenation into an l-dimensional vec-
tor. We omit the bias to avoid notational clutter.

To capture entailment-based evidence, we again
apply attention over the original sentences guided
by the joint representation hcsi which is obtained
on top of the coherence attention. This yields:

bi = tanh(Ve · hcsi + be)

βi =
exp(bi)∑
i exp(bi)

hcS =
∑
i

βi · hsi

(7)

where Ve and be are parameters turning hcsi to an
entailment score bi, βi is the entailment-based at-
tention weight of si which is used to produce the
final representation hcS of an entire instance.

Note that the hierarchy of our attention structure
is conveyed by the query part hcsi , and we apply the
weight βi on the original representation hsi rather
than h′si (Eq. 4) or h̃si (Eq. 5), which is empiri-
cally better based on our trials since the latter two
may contain more redundant information due to
the sum over an entire set when computing h′si .

4.4 The Overall Model
The attention vector hcS is the high-level represen-
tation of the claim with the embedded evidence
based on the hierarchical attention method. We
use a fully connected output layer to output the
probability distribution over the veracity classes:

ŷ = softmax(Vo · hcS + bo) (8)

where Vo and bo are the weights and bias in output
layer. Note that Eq. 8 assumes that using hcS alone
can determine the veracity as true or false without
direct reference to the claim again. This may be
suitable for news data as the salient news sentences
often straightforwardly comment on the claim’s
veracity. However, some claim verification tasks
such as FEVER (Thorne et al., 2018a) are partic-
ularly defined to classify if the factual evidence
from the source like Wikipedia, which rarely re-
mark on the veracity of the mutated claim, can in-
fer the claim as being supported, refuted or NEI. In
such case, we replace hcS in Eq. 8 with the richer
representation ĥcS = [hc, h

c
S , hc � hcS , |hc − hcS |]

to facilitate the inference from the evidence to the
claim in accordance with such NLI style of the
task definition. Interestingly, such treatment does
not work for veracity classification of news claim
(see Section 5.2), which may be because the verac-
ity features of news claim have been already em-
bedded into hcS and the richer representation ĥ

c
S

involving the claim could introduce unnecessary
noise to a non-NLI type of task unlike FEVER.

To fine-tune our model, we also pre-train the
coherence- and entailment-related parameters for
avoiding the sole reliance on the potentially lim-
ited supervision from the task-specific labels.

Pre-training Coherence Model
Without ground truth for learning the coherence
model, we use a pair-wise training strategy to op-
timize a large margin objective. For each claim



2566

c, we randomly choose another “negative” claim
c′. Then we construct a tuple (s,X+, X−), where
X+ = (c, S) and X− = (c′, S′) are tuples con-
sisting of different claims and their relevant article
sentences, and s ∈ S is a sentence selected ran-
domly. Generally, (s,X+) should exhibit higher
topical coherence than (s,X−) since the former
reports the same claim c. We seek for parameters
that assign a higher score to (s,X+) than (s,X−)
by minimizing the following margin-based rank-
ing loss:

Lc = max
{

0, 1 + r(s,X−)− r(s,X+)
}

(9)

and r(, ) is the ranking function turning the
coherence-based sentence embedding to a ranking
score:

r(s,X) = tanh
(
W ′c · cohAtt(s,X) + b′c

)
(10)

where cohAtt(, ) is a shorthand of Eq. 4, and W ′c
and b′c are the weights and bias of an added ranking
output layer which is not a part of our end-to-end
model. The pre-trained model is used to initialize
all the parameters needed for computing Eq. 4.

Pre-training Entailment Model
We use the Standford Natural Language Infer-
ence (SNLI) dataset (Bowman et al., 2015) to pre-
train the parameters of entailment-based attention
model. Specifically, we train a model for Recog-
nizing Textual Entailment (RTE) as follow:

ȳ = softmax(V ′e · hRTE + b′e) (11)

where ȳ is the entailment class label, i.e., entails,
contradicts, or neutral, hRTE has the same form
as Eq. 6 while the input claim-sentence pair is re-
placed by a pair of premise and hypothesis in the
SNLI corpus (each element is encoded by a GRU
sentence encoder), and V ′e and b

′
e are the weights

and bias of the RTE output layer which is not part
of our end-to-end model. The pre-trained model is
used to initialize the parameters We in Eq. 6.

For pre-training, we minimize the square loss
between the distributions of the predicted and the
ground-truth entailment classes.

Overall Training
After pre-training, all the model parameters are
trained end-to-end by minimizing the squared er-
ror between the class probability distribution of
the prediction and that of the ground truth over

the claims. Parameters are updated through back-
propagation (Collobert et al., 2011) with Ada-
Grad (Duchi et al., 2011) for speeding up con-
vergence. The training process ends when the
model converges or the maximum epoch number
is met. We represent input words using pre-trained
GloVe Wikipedia 6B word embeddings (Penning-
ton et al., 2014). We set d to 300 for word vectors
and l to 100 for hidden units, and no parameter
depends on n which varies with different claims.

5 Experiments and Results

5.1 Datasets and Evaluation Metrics

We use three public fact-checking datasets for
evaluation: 1) Snopes and 2) PolitiFact, released
by Popat et al. (2018), containing 4,341 and
3,568 news claims, respectively, along with rele-
vant articles collected from various web sources;
3) FEVER, released by Thorne et al. (2018a),
which consists of 185,445 claims accompanied by
human-annotated relevant Wikipedia articles and
evidence-bearing sentences, and many claims in
FEVER are human altered by mutating the origi-
nal claims from Wikipedia.

Each Snopes claim was labeled as true or false,
while each PolitiFact claim was originally as-
signed one of six veracity labels: true, mostly true,
half true, mostly false, false, and pants on fire. Un-
like Popat et al. (2018) converting all the classes
into true or false, we merge mostly true, half true
and mostly false into mixed, and treat false and
pants on fire as false. Thus, we have a more practi-
cal classification on PolitiFact, i.e., true, false and
mixed. We use micro-/macro-averaged F1, class-
specific precision, recall and F-measure as evalu-
ation metrics. We hold out 10% of the claims for
tuning the hyper parameters, and conduct 5-fold
cross-validation on the rest of the claims.

On FEVER dataset, each claim, which is classi-
fied as Supported, Refuted or NEI, can be verified
with its ground-truth label and a set of human-
annotated evidential sentences extracted from its
relevant Wikipedia pages. This task is similar
as predicting the entailment relation by aggregat-
ing the sentences to infer the NLI-style label of
the target claim, instead of directly predicting the
claim’s veracity as true or false. FEVER shared
task used label accuracy, F1 score of evidential
sentence selection, and FEVER score as evalua-
tion metrics (Thorne et al., 2018b).



2567

Method
Snopes PolitiFact

True False True False Mixed
micF1 macF1 Prec. Rec. F1 Prec. Rec. F1 micF1 macF1 F1 F1 F1

CNN 0.721 0.636 0.477 0.440 0.460 0.802 0.822 0.812 0.453 0.402 0.368 0.566 0.270
LSTM 0.689 0.642 0.441 0.512 0.517 0.834 0.716 0.771 0.463 0.413 0.452 0.561 0.228
SVM 0.704 0.649 0.459 0.584 0.511 0.832 0.747 0.786 0.450 0.421 0.440 0.547 0.277
DeClarE 0.762 0.695 0.559 0.556 0.553 0.839 0.837 0.837 0.475 0.443 0.447 0.576 0.307
HAN-na 0.750 0.674 0.535 0.500 0.517 0.821 0.841 0.831 0.470 0.431 0.456 0.594 0.242
HAN-ba 0.771 0.738 0.556 0.765 0.644 0.899 0.774 0.832 0.520 0.471 0.475 0.629 0.308
HAN 0.807 0.759 0.637 0.665 0.651 0.874 0.860 0.867 0.523 0.487 0.495 0.627 0.340
HAN-nli 0.747 0.670 0.534 0.491 0.512 0.817 0.841 0.830 0.485 0.432 0.467 0.599 0.230

Table 2: Results of comparison among different models on Snopes (left) and PolitiFact (right) datasets

Method Snopes PolitiFactmicF1 macF1 micF1 macF1
HAN-na 0.750 0.674 0.470 0.431
+ ba 0.776 0.727 0.495 0.455
+ ca 0.788 0.741 0.516 0.473
+ ea 0.779 0.728 0.508 0.463
+ ba + ea 0.771 0.738 0.520 0.471
+ ca + ea 0.807 0.759 0.523 0.487

Table 3: Results of ablation test across different atten-
tions on Snopes (left) and PoliFact (right) datasets.

5.2 Experiments on Veracity-based Datasets

We compare our model and several state-of-the-art
baseline methods described below. 1) SVM: A lin-
ear SVM model for fake news detection using a set
of linguistic features (e.g., bag-of-words, ngrams,
etc.) handcrafted from relevant sentences (Thorne
and Vlachos, 2018); 2) CNN and LSTM: The
CNN-based detection model (Wang, 2017) and
LSTM-based RNN model for representation learn-
ing from word sequences (Rashkin et al., 2017),
respectively, both using only claim content without
considering external resources; 3) DeClarE: The
word-level neural attention model for Debunking
Claims with Interpretable Evidence (Popat et al.,
2018) capturing world-level evidence from rele-
vant articles; 4) HAN: Our full model based on
Hierarchical Attention Networks, where coher-
ence component uses Eq. 3; 5) HAN-ba: A variant
of HAN with biaffine attention in Eq. 2; 6) HAN-
na: Our reduced model with no attention but only
using original sentence representations; 7) HAN-
nli: A variant of HAN by replacing hcS in Eq. 8
with ĥcS for the output layer (see Section 4.4).

We implement our models and DeClarE with
Theano3, and use the original codes of other base-
lines. As DeClarE is not yet open-source, we con-
sult with its developers for our implementation.

3http://deeplearning.net/software/
theano/

Results of Comparison
As shown in Table 2, CNN and LSTM us-
ing barely content of claims without considering
external information are comparable with SVM
which uses handcrafted features based on rele-
vant article sentences. Among all the baselines,
DeClarE performs the best because it not only
learns to capture complex features effectively via
the neural model, but also strengthens the learned
features by attending on the salient words that are
important for predicting the correct label.

Our model can capture more accurate sentence-
level evidence which convey the semantics more
completely and deeply. The superiority is clear:
HAN-na which considers sentence as evidence
without using attention is already better than the
baselines except DeClarE, implying the impor-
tance of sentence-level information. HAN-ba and
HAN using attentions to embed sentence-level ev-
idence consistently outperform DeClarE in large
margin that is based on word-level attention.

HAN consistently outperforms HAN-ba on both
datasets. This suggests that the co-attention con-
sidering claim for capturing sentence coherence
is more effective to represent more accurate ev-
idence. HAN-nli, however, fails to work and is
even worse than DeClarE, which confirms our
conjecture that veracity classification on news data
differs from a NLI type of task like on FEVER (see
Section 5.3) since news reports often openly re-
mark the claim’s veracity and involving the claim
in the output layer may interfere the decision.

Ablation Study
To evaluate the impact of each component, we
perform ablation tests based on the no-attention
model HAN-na plus some component(s) which
can be one or combination of the following atten-
tions: 1) ba and 2) ca correspond to the coherence-
based biaffine attention (Eq. 2) and co-attention

http://deeplearning.net/software/theano/
http://deeplearning.net/software/theano/


2568

(a) Snopes dataset (b) Politifact dataset

Figure 2: Results of HAN and HAN- (not pre-trained)
under different sizes of training data.

(Eq. 3), respectively; 3) ea: entailment-based at-
tention (Eq. 7).

As shown in Table 3, HAN-na plus each com-
ponent alone improves the model, indicating their
effectiveness for embedding sentence-level evi-
dence. Furthermore, +ca consistently outperforms
+ba, reaffirming the advantage of co-attention; +ea
makes similar improvements over HAN-na as +ba
and +ca did, suggesting that both types of attention
are comparably helpful. Combining them hierar-
chically makes further improvements especially in
the case of +ca+ea, implying that the two attention
mechanisms are complementary.

We also examine the impact of pre-training on
HAN in comparison with its performance with-
out pre-training, namely HAN-. In Figure 2, we
observe that the pre-training does not have much
impact when we use the entire training set, but it
clearly improves the model when only using cer-
tain proportions of the training data. This indicates
that the fine-tuned coherence and entailment mod-
els are generally helpful for claim verification, es-
pecially when the sampled set is not sufficiently
large for fully training the model.

Discussion
Regarding the gap between the published per-
formance of DeClarE (Plain+Attn) (Popat et al.,
2018) which is 0.79 on the Snopes dataset and that
of our implementation of it which is 0.759, we
conjecture the reason may be that DeClarE utilized
an undisclosed strategy for balancing the training
datasets that we could not easily replicate, while
we trained all the systems in Table 2 on the orig-
inal unbalanced dataset. We leave this for fur-
ther investigation in future upon the availability of
DeClarE source codes. On the PolitiFact dataset,
since we adopt a three-way classification, it is thus
not directly comparable with the original DeClarE
performance which is based on two classes.

Claim: Comedian Bill Murray is running for president
Verdict: False

1 It turns out it’s not true and just the subject of a hoax
article by a website parodying ABC news.

2 Bill Murry is not running for president, nor has he an-
nounced that fact from his hometown.

3 Unknown Internet prankster created fake website for
NBC, ABC and Fox News running the headline “Bill
Murray is running for president”.
...

8 Murray made the announcement from his home in and
he felt the 2016 presidential election seemed like the
right time to go.

9 Paul Horner, a spokesman for the campaign, told re-
porters that he believes in Bill Murry for President.

Table 4: Examples of attended sentences ranked by the
attention weight βi that can explain the verdict.

Method Acc. Prec. Rec. F1 FEVER
Fever-base 0.521 − − − 0.326
NSMN 0.697 0.286 0.870 0.431 0.665
HAN-nli 0.642 0.340 0.484 0.400 0.464
HAN-nli* 0.720 0.447 0.536 0.488 0.571
HAN* 0.475 0.356 0.471 0.406 0.365

Table 5: Results of different claim verification models
on FEVER dataset (Dev set). The columns correspond
to the predicted label accuracy, the evidence precision,
recall, F1 score, and the FEVER score.

Case Study
Table 4 illustrates some top sentences embedded
with a claim from Snopes dataset which is cor-
rectly detected as fake. We can see that 1) the top
sentences have high topical overlap with both the
claim and each other; 2) the highly ranked sen-
tences play a major role in deciding the verdict,
as they remark on the claim’s veracity directly; 3)
the lower sentences seem less important since they
either repeat the claim or are very subjective. Pro-
viding such readable pieces of evidence to human
fact-checker for verifying the claim can be helpful.

5.3 Experiments on FEVER Dataset

We compare the following systems on the public
Dev set4 of FEVER dataset: 1) Fever-base: The
FEVER baseline (Thorne et al., 2018a) that is a
pipeline for claim verification including 3 stages:
document retrieval, sentence selection and textual
entailment. 2) NSMN: The pipeline-based system
named as UNC-NLP topping the FEVER shared
task (Thorne et al., 2018b), which was later re-
ported as using Neural Semantic Matching Net-
works (Nie et al., 2019). 3) HAN-nli: Our full

4The test set is not publicly available at the time of this
work being done.



2569

model trained using the FEVER task dataset. Note
that similar to DeClarE our model assumes that
the set of articles about each claim have been
retrieved, while the FEVER task requires users
search relevant Wikipages in the first place. Using
FEVER, our method thus is not truly end-to-end
in this setting. We utilize the document retrieval
module of NSMN (Nie et al., 2019) to obtain
the relevant Wikipages. 4) HAN-nli*: For more
fair comparison with NSMN which utilized the
ground-truth sentences in the training set to train
their sentence selector, we fine-tune the HAN-nli,
namely HAN-nli*, by optimizing the square error
loss between the entailment attention score bi (see
Eq. 7) and the -1/+1 value indicating whether si is
selected as a piece of evidence in the ground truth.
5) HAN*: The original HAN using Eq. 8 in the
output layer and fine-tuned like HAN-nli*.

Table 5 shows that HAN-nli* is much better
than the two baselines in terms of label accuracy
and evidence F1 score. There are two reasons: 1)
apart from the retrieval module, our model opti-
mizes all the parameters end-to-end, while the two
pipeline systems may result in error propagation;
and 2) our evidence embedding method consid-
ers more complex facets such as topical coher-
ence and semantic entailment, while NSMN just
focuses on similarity matching between the claim
and each sentence. HAN-nli seem already a de-
cent model given its much better performance than
Fever-base. This confirms the advantage of our ev-
idence embedding method on the FEVER task.

NSMN achieves higher FEVER score and evi-
dence recall than our method. However, the rea-
son is straightforward: FEVER score favors re-
calling the annotated evidential sentences while
one of the limitations of FEVER dataset is that
the ground-truth sentences provided by human
annotators were often incomplete (Thorne et al.,
2018a,b). Our approach is not limited by select-
ing top-k sentences and may embed into evidence
as many diverse sentences as the model requires.
Compared to NSMN which aims to recall the top
evidence sentences in FEVER’s ground truth, our
model achieves much higher Accuracy, Evidence
Precision and F1.

HAN* is ineffective, confirming that in FEVER
task the claim content is needed in the output layer
for the NLI to take effect since the evidence from
Wikipedia typically does not contain direct re-
marks on the veracity of a claim.

Discussion
The pipeline-based system NSMN demonstrates
superior evidence retrieval performance in terms
of FEVER score. We emphasize that the essential
objective of our model is not for evidence retrieval
and ranking. Instead of ranking sentences into the
top-k positions, we pay more attention on claim
verification accuracy by embedding and aggregat-
ing the useful sentences as evidence like we have
explained above. However, such discrepancy in-
spires us to investigate in the future an end-to-end
approach to jointly model evidence retrieval and
claim verification in a unified framework based on
our sentence-level attention mechanism.

Finally, thanks to one of our reviewers, we learn
about another two-stage model named TwoWin-
gOS (Yin and Roth, 2018), which achieves a com-
parable FEVER score but a little bit higher ac-
curacy than ours on FEVER task. The TwoWin-
gOS applies a two-wing optimization approach to
jointly optimizing sentence selection and veracity
classification. The reasons regarding their higher
performance might lie in that: 1) their input word
embeddings are fine-tuned based on the context of
the evidence and claim while ours are fixed dur-
ing training; and 2) the document retrieval module
of the TwoWingOS has demonstrated higher effec-
tiveness than that of the NSMN (see rate (recall)
and acc ceiling (OFEVER) in Tables 2 in (Yin and
Roth, 2018; Nie et al., 2019) for details).

6 Conclusions and Future Work

We propose a novel neural end-to-end frame-
work for claim verification by learning to embed
sentence-level evidence with a hierarchical atten-
tion mechanism. Our model strengthens the ev-
idence representations by attending on the sen-
tences that are not only topically coherent but can
also semantically infer the target claim. The re-
sults on three public benchmark datasets confirm
the advantages of our method. For the future work,
beyond what we have mentioned, we plan to ex-
amine our model on different information sources.
We will also try to incorporate relevant metadata
into it, e.g., author profile, website credibility, etc.

Acknowledgment

This work was partly supported by Hong Kong
RGC GRF (14232816, 14209416, 14204118),
NSFC (61877020) and SCSE-SUG grant
M4082038 at NTU.



2570

References
Tariq Alhindi, Savvas Petridis, and Smaranda Mure-

san. 2018. Where is your evidence: Improving fact-
checking by justification modeling. In Proceedings
of the First Workshop on Fact Extraction and VERi-
fication (FEVER), pages 85–90.

Samuel R Bowman, Gabor Angeli, Christopher Potts,
and Christopher D Manning. 2015. A large anno-
tated corpus for learning natural language inference.
In Proceedings of the 2015 Conference on Empiri-
cal Methods in Natural Language Processing, pages
632–642.

Carlos Castillo, Marcelo Mendoza, and Barbara
Poblete. 2011. Information credibility on twitter. In
Proceedings of the 20th international conference on
World wide web, pages 675–684. ACM.

Qian Chen, Xiaodan Zhu, Zhen-Hua Ling, Si Wei, and
Hui Jiang. 2016. Enhancing and combining sequen-
tial and tree LSTM for natural language inference.
CoRR, abs/1609.06038.

Kyunghyun Cho, Bart van Merriënboer, Dzmitry Bah-
danau, and Yoshua Bengio. 2014. On the properties
of neural machine translation: Encoder-decoder ap-
proaches. arXiv preprint arXiv:1409.1259.

Ronan Collobert, Jason Weston, Léon Bottou, Michael
Karlen, Koray Kavukcuoglu, and Pavel Kuksa.
2011. Natural language processing (almost) from
scratch. Journal of Machine Learning Research,
12(Aug):2493–2537.

Alexis Conneau, Douwe Kiela, Holger Schwenk, Loı̈c
Barrault, and Antoine Bordes. 2017. Supervised
learning of universal sentence representations from
natural language inference data. In Proceedings of
the 2017 Conference on Empirical Methods in Nat-
ural Language Processing, pages 670–680.

Ido Dagan, Bill Dolan, Bernardo Magnini, and Dan
Roth. 2010. Recognizing textual entailment: Ratio-
nal, evaluation and approaches. Journal of Natural
Language Engineering, 4.

Timothy Dozat and Christopher D. Manning. 2016.
Deep biaffine attention for neural dependency pars-
ing. CoRR, abs/1611.01734.

John Duchi, Elad Hazan, and Yoram Singer. 2011.
Adaptive subgradient methods for online learning
and stochastic optimization. Journal of Machine
Learning Research, 12(Jul):2121–2159.

Sebastian Dungs, Ahmet Aker, Norbert Fuhr, and
Kalina Bontcheva. 2018. Can rumour stance alone
predict veracity? In Proceedings of the 27th Inter-
national Conference on Computational Linguistics,
pages 3360–3370.

William Ferreira and Andreas Vlachos. 2016. Emer-
gent: a novel data-set for stance classification. In
Proceedings of the 2016 conference of the North

American chapter of the association for computa-
tional linguistics: Human language technologies,
pages 1163–1168.

Andreas Hanselowski, Hao Zhang, Zile Li, Daniil
Sorokin, Benjamin Schiller, Claudia Schulz, and
Iryna Gurevych. 2018. Multi-sentence textual en-
tailment for claim verification. In Proceedings of the
First Workshop on Fact Extraction and VERification
(FEVER), pages 103–108.

Zhiwei Jin, Juan Cao, Yongdong Zhang, and Jiebo Luo.
2016. News verification by exploiting conflicting
social viewpoints in microblogs. In Proceedings of
the Thirtieth AAAI Conference on Artificial Intelli-
gence, pages 2972–2978. AAAI Press.

Chloé Kiddon, Luke Zettlemoyer, and Yejin Choi.
2016. Globally coherent text generation with neural
checklist models. In Proceedings of the 2016 Con-
ference on Empirical Methods in Natural Language
Processing, pages 329–339.

Srijan Kumar and Neil Shah. 2018. False information
on web and social media: A survey. arXiv preprint
arXiv:1804.08559.

Yang Liu, Chengjie Sun, Lei Lin, and Xiaolong Wang.
2016. Learning natural language inference us-
ing bidirectional LSTM model and inner-attention.
CoRR, abs/1605.09090.

Lajanugen Logeswaran, Honglak Lee, and Dragomir
Radev. 2018. Sentence ordering and coherence
modeling using recurrent neural networks. In
Thirty-Second AAAI Conference on Artificial Intel-
ligence.

Jiasen Lu, Jianwei Yang, Dhruv Batra, and Devi
Parikh. 2016. Hierarchical question-image co-
attention for visual question answering. In Advances
In Neural Information Processing Systems, pages
289–297.

Minh-Thang Luong, Hieu Pham, and Christopher D
Manning. Effective approaches to attention-based
neural machine translation.

Jing Ma, Wei Gao, Prasenjit Mitra, Sejeong Kwon,
Bernard J Jansen, Kam-Fai Wong, and Meeyoung
Cha. 2016. Detecting rumors from microblogs with
recurrent neural networks. In Proceedings of the
Twenty-Fifth International Joint Conference on Ar-
tificial Intelligence, pages 3818–3824. AAAI Press.

Jing Ma, Wei Gao, and Kam-Fai Wong. 2018. Rumor
detection on twitter with tree-structured recursive
neural networks. In Proceedings of the 56th Annual
Meeting of the Association for Computational Lin-
guistics (Volume 1: Long Papers), volume 1, pages
1980–1989.

Lili Mou, Rui Men, Ge Li, Yan Xu, Lu Zhang, Rui
Yan, and Zhi Jin. 2016. Natural language inference
by tree-based convolution and heuristic matching.
In The 54th Annual Meeting of the Association for
Computational Linguistics, page 130.



2571

Raymond S Nickerson. 1998. Confirmation bias: A
ubiquitous phenomenon in many guises. Review of
general psychology, 2(2):175–220.

Yixin Nie, Haonan Chen, and Mohit Bansal. 2019.
Combining fact extraction and verification with neu-
ral semantic matching networks.

Ankur Padia, Francis Ferraro, and Tim Finin. 2018.
Team umbc-fever: Claim verification using semantic
lexical resources. In Proceedings of the First Work-
shop on Fact Extraction and VERification (FEVER),
pages 161–165.

Cesc C Park and Gunhee Kim. 2015. Expressing an
image stream with a sequence of natural sentences.
In Advances in neural information processing sys-
tems, pages 73–81.

Jeffrey Pennington, Richard Socher, and Christopher
Manning. 2014. Glove: Global vectors for word
representation. In Proceedings of the 2014 confer-
ence on empirical methods in natural language pro-
cessing (EMNLP), pages 1532–1543.

Kashyap Popat, Subhabrata Mukherjee, Jannik
Strötgen, and Gerhard Weikum. 2017. Where the
truth lies: Explaining the credibility of emerging
claims on the web and social media. In Proceedings
of the 26th International Conference on World Wide
Web Companion, pages 1003–1012. International
World Wide Web Conferences Steering Committee.

Kashyap Popat, Subhabrata Mukherjee, Andrew Yates,
and Gerhard Weikum. 2018. Declare: Debunking
fake news and false claims using evidence-aware
deep learning. In Proceedings of the 2018 Con-
ference on Empirical Methods in Natural Language
Processing, pages 22–32.

Vahed Qazvinian, Emily Rosengren, Dragomir R
Radev, and Qiaozhu Mei. 2011. Rumor has it: Iden-
tifying misinformation in microblogs. In Proceed-
ings of the Conference on Empirical Methods in Nat-
ural Language Processing, pages 1589–1599. Asso-
ciation for Computational Linguistics.

Hannah Rashkin, Eunsol Choi, Jin Yea Jang, Svitlana
Volkova, and Yejin Choi. 2017. Truth of varying
shades: Analyzing language in fake news and polit-
ical fact-checking. In Proceedings of the 2017 Con-
ference on Empirical Methods in Natural Language
Processing, pages 2931–2937.

Edward S Reed, Elliot Turiel, and Terrance Brown.
2013. Naive realism in everyday life: Implications
for social conflict and misunderstanding. In Values
and Knowledge, pages 113–146. Psychology Press.

Victoria Rubin, Niall Conroy, Yimin Chen, and Sarah
Cornwell. 2016. Fake news or truth? using satirical
cues to detect potentially misleading news. In Pro-
ceedings of the Second Workshop on Computational
Approaches to Deception Detection, pages 7–17.

Karishma Sharma, Feng Qian, He Jiang, Natali
Ruchansky, Ming Zhang, and Yan Liu. 2019.
Combating fake news: A survey on identifica-
tion and mitigation techniques. arXiv preprint
arXiv:1901.06437.

Kai Shu, Amy Sliva, Suhang Wang, Jiliang Tang, and
Huan Liu. 2017. Fake news detection on social me-
dia: A data mining perspective. ACM SIGKDD Ex-
plorations Newsletter, 19(1):22–36.

James Thorne and Andreas Vlachos. 2018. Automated
fact checking: Task formulations, methods and fu-
ture directions. In Proceedings of the 27th Inter-
national Conference on Computational Linguistics,
pages 3346–3359.

James Thorne, Andreas Vlachos, Christos
Christodoulopoulos, and Arpit Mittal. 2018a.
Fever: a large-scale dataset for fact extraction and
verification. In Proceedings of the 2018 Conference
of the North American Chapter of the Association
for Computational Linguistics: Human Language
Technologies, Volume 1 (Long Papers), volume 1,
pages 809–819.

James Thorne, Andreas Vlachos, Oana Cocarascu,
Christos Christodoulopoulos, and Arpit Mittal.
2018b. The fact extraction and verification (fever)
shared task. In Proceedings of the First Workshop on
Fact Extraction and VERification (FEVER), pages
1–9. Association for Computational Linguistics.

Svitlana Volkova, Kyle Shaffer, Jin Yea Jang, and
Nathan Hodas. 2017. Separating facts from fiction:
Linguistic models to classify suspicious and trusted
news posts on twitter. In Proceedings of the 55th An-
nual Meeting of the Association for Computational
Linguistics (Volume 2: Short Papers), volume 2,
pages 647–653.

William Yang Wang. 2017. ” liar, liar pants on fire”:
A new benchmark dataset for fake news detection.
In Proceedings of the 55th Annual Meeting of the
Association for Computational Linguistics (Volume
2: Short Papers), volume 2, pages 422–426.

Caiming Xiong, Victor Zhong, and Richard Socher.
2016. Dynamic coattention networks for question
answering. CoRR, abs/1611.01604.

Wenpeng Yin and Dan Roth. 2018. Twowingos: A
two-wing optimization strategy for evidential claim
verification. In Proceedings of the 2018 Conference
on Empirical Methods in Natural Language Pro-
cessing, pages 105–114.

James O. Young. 2018. The coherence theory of truth.
In Edward N. Zalta, editor, The Stanford Encyclope-
dia of Philosophy. Metaphysics Research Lab, Stan-
ford University.

Arkaitz Zubiaga, Ahmet Aker, Kalina Bontcheva,
Maria Liakata, and Rob Procter. 2018. Detection
and resolution of rumours in social media: A survey.
ACM Computing Surveys (CSUR), 51(2):32.


