



















































Incorporating External Knowledge into Machine Reading for Generative Question Answering


Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing
and the 9th International Joint Conference on Natural Language Processing, pages 2521–2530,
Hong Kong, China, November 3–7, 2019. c©2019 Association for Computational Linguistics

2521

Incorporating External Knowledge into Machine Reading for Generative
Question Answering

Bin Bi, Chen Wu, Ming Yan, Wei Wang,
Jiangnan Xia, Chenliang Li

Alibaba Group
{b.bi, wuchen.wc, ym119608, hebian.ww}@alibaba-inc.com

{jiangnan.xjn, lcl193798}@alibaba-inc.com

Abstract

Commonsense and background knowledge is
required for a QA model to answer many non-
trivial questions. Different from existing work
on knowledge-aware QA, we focus on a more
challenging task of leveraging external knowl-
edge to generate answers in natural language
for a given question with context.

In this paper, we propose a new neural
model, Knowledge-Enriched Answer Gener-
ator (KEAG), which is able to compose a
natural answer by exploiting and aggregating
evidence from all four information sources
available: question, passage, vocabulary and
knowledge. During the process of answer gen-
eration, KEAG adaptively determines when
to utilize symbolic knowledge and which fact
from the knowledge is useful. This allows the
model to exploit external knowledge that is not
explicitly stated in the given text, but that is
relevant for generating an answer. The empiri-
cal study on public benchmark of answer gen-
eration demonstrates that KEAG improves an-
swer quality over models without knowledge
and existing knowledge-aware models, con-
firming its effectiveness in leveraging knowl-
edge.

1 Introduction

Question Answering (QA) has come a long way
from answer sentence selection, relational QA
to machine reading comprehension. The next-
generation QA systems can be envisioned as the
ones which can read passages and write long
and abstractive answers to questions. Different
from extractive question answering, generative
QA based on machine reading produces an answer
in true natural language which does not have to be
a sub-span in the given passage.

Most existing models, however, answer ques-
tions based on the content of given passages as the
only information source. As a result, they may not

be able to understand certain passages or to answer
certain questions, due to the lack of commonsense
and background knowledge, such as the knowl-
edge about what concepts are expressed by the
words being read (lexical knowledge), and what
relations hold between these concepts (relational
knowledge). As a simple illustration, given the
passage:
State officials in Hawaii on Monday said they have
once again checked and confirmed that President
Barack Obama was born in Hawaii.
to answer the question: Was Barack Obama born
in the U.S.?, one must know (among other things)
that Hawaii is a state in the U.S., which is external
knowledge not present in the text corpus.

Therefore, a QA model needs to be enriched
with external knowledge properly to be able to an-
swer many nontrivial questions. Such knowledge
can be commonsense knowledge or factual back-
ground knowledge about entities and events that
is not explicitly expressed but can be found in a
knowledge base such as ConceptNet (Speer et al.,
2016), Freebase (Pellissier Tanon et al., 2016) and
domain-specific KBs collected by information ex-
traction (Fader et al., 2011; Mausam et al., 2012).
Thus, we aim to design a neural model that en-
codes pre-selected knowledge relevant to given
questions, and that learns to include the available
knowledge as an enrichment to given textual infor-
mation.

In this paper, we propose a new neural archi-
tecture, Knowledge-Enriched Answer Generator
(KEAG), specifically designed to generate natural
answers with integration of external knowledge.
KEAG is capable of leveraging symbolic knowl-
edge from a knowledge base as it generates each
word in an answer. In particular, we assume that
each word is generated from one of the four infor-
mation sources: 1. question, 2. passage, 3. vocab-
ulary and 4. knowledge. Thus, we introduce the



2522

source selector, a sentinel component in KEAG
that allows flexibility in deciding which source
to look to generate every answer word. This is
crucial, since knowledge plays a role in certain
parts of an answer, while in others text context
should override the context-independent knowl-
edge available in general KBs.

At each timestep, before generating an answer
word, KEAG determines an information source.
If the knowledge source is selected, the model ex-
tracts a set of facts that are potentially related to
the given question and context. A stochastic fact
selector with discrete latent variables then picks a
fact based on its semantic relevance to the answer
being generated. This enables KEAG to bring ex-
ternal knowledge into answer generation, and to
generate words not present in the predefined vo-
cabulary. By incorporating knowledge explicitly,
KEAG can also provide evidence about the exter-
nal knowledge used in the process of answer gen-
eration.

We introduce a new differentiable sampling-
based method to learn the KEAG model in
the presence of discrete latent variables. For
empirical evaluation, we conduct experiments
on the benchmark dataset of answer generation
MARCO (Nguyen et al., 2016). The experimental
results demonstrate that KEAG effectively lever-
ages external knowledge from knowledge bases
in generating natural answers. It achieves signif-
icant improvement over classic QA models that
disregard knowledge, resulting in higher-quality
answers.

2 Related Work
There have been several attempts at using ma-
chine reading to generate natural answers in the
QA field. Tan et al. (2018) took a generative
approach where they added a decoder on top of
their extractive model to leverage the extracted
evidence for answer synthesis. However, this
model still relies heavily on the extraction to per-
form the generation and thus needs to have start
and end labels (a span) for every QA pair. Mi-
tra (2017) proposed a seq2seq-based model that
learns alignment between a question and passage
words to produce rich question-aware passage rep-
resentation by which it directly decodes an an-
swer. Gao et al. (2019) focused on product-aware
answer generation based on large-scale unlabeled
e-commerce reviews and product attributes. Fur-
thermore, natural answer generation can be refor-

mulated as query-focused summarization which is
addressed by Nema et al. (2017).

The role of knowledge in certain types of QA
tasks has been remarked on. Mihaylov and Frank
(2018) showed improvements on a cloze-style task
by incorporating commonsense knowledge via a
context-to-commonsense attention. Zhong et al.
(2018) proposed commonsense-based pre-training
to improve answer selection. Long et al. (2017)
made use of knowledge in the form of entity de-
scriptions to predict missing entities in a given
document. There have also been a few studies
on incorporating knowledge into QA models with-
out passage reading. GenQA (Yin et al., 2016)
combines knowledge retrieval and seq2seq learn-
ing to produce fluent answers, but it only deals
with simple questions containing one single fact.
COREQA (He et al., 2017) extends it with a copy
mechanism to learn to copy words from a given
question. Moreover, Fu and Feng (2018) intro-
duced a new attention mechanism that attends
across the generated history and memory to ex-
plicitly avoid repetition, and incorporated knowl-
edge to enrich generated answers.

Some work on knowledge-enhanced natural
language (NLU) understanding can be adapted
to the question answering task. CRWE (Weis-
senborn, 2017) dynamically integrates back-
ground knowledge in a NLU model in the form
of free-text statements, and yields refined word
representations to a task-specific NLU architec-
ture that reprocesses the task inputs with these
representations. In contrast, KBLSTM (Yang and
Mitchell, 2017) leverages continuous representa-
tions of knowledge bases to enhance the learn-
ing of recurrent neural networks for machine read-
ing. Furthermore, Bauer et al. (2018) proposed
MHPGM, a QA architecture that fills in the gaps
of inference with commonsense knowledge. The
model, however, does not allow an answer word
to come directly from knowledge. We adapt these
knowledge-enhanced NLU architectures to answer
generation, as baselines for our experiments.

3 Knowledge-aware Answer Generation
Knowledge-aware answer generation is a ques-
tion answering paradigm, where a QA model
is expected to generate an abstractive an-
swer to a given question by leveraging both
the contextual passage and external knowledge.
More formally, given a knowledge base K
and two sequences of input words: question



2523

Question Passage

Attention!"
Attention !#

Question 
Encoder

Passage 
Encoder$%

" $&" $'" $%# $&# $'# $(#

)%* )&* )'* )(* )+*
Decoder

Context 
vector

Context 
vector

,%"

,&"

,'"

,("

,+"

,%#

,&#

,'#

,(#

,+#

Source 
Selector

Answer

1 2 3 4

-.=2

/ 0.1%|-.

3Next word 0.1%

!." !.# 4.* ,." ,.# ).*
Attentions Word 

embedding
Context 
vectors

Decoder 
state

/ -.|θ

-.=1

Fact 
Selection

-.=3
-.=4

Source Selector

Figure 1: An overview of the architecture of KEAG (best viewed in color). A question and a passage both go
through an extension of the sequence-to-sequence model. The outcomes are then fed into a source selector to
generate a natural answer.

q = {wq1, w
q
2, . . . , w

q
Nq
} and passage p =

{wp1, w
p
2, . . . , w

p
Np
}, the answer generation model

should produce a series of answer words r =
{wr1, wr2, . . . , wrNr}. The knowledge base K con-
tains a set of facts, each of which is repre-
sented as a triple f = (subject, relation, object)
where subject and object can be multi-word ex-
pressions and relation is a relation type, e.g.,
(bridge, UsedFor, cross water).

3.1 Knowledge-Enriched Answer Generator
To address the answer generation problem, we
propose a novel KEAG model which is able to
compose a natural answer by recurrently selecting
words at the decoding stage. Each of the words
comes from one of the four sources: question q,
passage p, global vocabulary V , and knowledgeK.
In particular, at every generation step, KEAG first
determines which of the four sources to inspect
based on the current state, and then generates a
new word from the chosen source to make up a fi-
nal answer. An overview of the neural architecture
of KEAG is depicted in Figure 1.

3.2 Sequence-to-sequence model
KEAG is built upon an extension of the sequence-
to-sequence attentional model (Bahdanau et al.,
2015; Nallapati et al., 2016; See et al., 2017). The
words of question q and passage p are fed one-by-
one into two different encoders, respectively. Each
of the two encoders, which are both bidirectional
LSTMs, produces a sequence of encoder hidden

states (Eq for question q, and Ep for passage p).
In each timestep t, the decoder, which is a unidi-
rectional LSTM, takes an answer word as input,
and outputs a decoder hidden state srt .

We calculate attention distributions aqt and a
p
t

on the question and the passage, respectively, as
in (Bahdanau et al., 2015):

aqt =softmax(g
qᵀtanh(WqEq +Uqsrt + b

q)),
(1)

apt =softmax(

gpᵀtanh(WpEp +Upsrt +V
pcq + bp)),

(2)

where gq, Wq, Uq, bq, gp, Wp, Up and bp

are learnable parameters. The attention distribu-
tions can be viewed as probability distributions
over source words, which tells the decoder where
to look to generate the next word. The coverage
mechanism is added to the attentions to avoid gen-
erating repetitive text (See et al., 2017). In Equa-
tion 2, we introduce cq, a context vector for the
question, to make the passage attention aware of
the question context. cq for the question and cp

for the passage are calculated as follows:

cqt =
∑
i

aqti · e
q
i , c

p
t =

∑
i

apti · e
p
i , (3)

where eqi and e
p
i are an encoder hidden state for

question q and passage p, respectively. The con-
text vectors (cqt and c

p
t ) together with the atten-

tion distributions (aqt and a
p
t ) and the decoder state



2524

(srt ) will be used downstream to determine the next
word in composing a final answer.

4 Source Selector

During the process of answer generation, in each
timestep, KEAG starts with running a source se-
lector to pick a word from one source of the ques-
tion, the passage, the vocabulary and the knowl-
edge. The right plate in Figure 1 illustrates how
the source selector works in one timestep during
decoding.

If the question source is selected in timestep t,
KEAG picks a word according to the attention dis-
tribution aqt ∈ RNq over question words (Equa-
tion 1), where Nq denotes the number of distinct
words in the question. Similarly, when the pas-
sage source is selected, the model picks a word
from the attention distribution apt ∈ RNp over pas-
sage words (Equation 2), where Np denotes the
number of distinct words in the passage. If the vo-
cabulary is the source selected in timestep t, the
new word comes from the conditional vocabulary
distribution Pv(w|cqt , c

p
t , s

r
t ) over all words in the

vocabulary, which is obtained by:

Pv(w|cqt , c
p
t , s

r
t ) = softmax(W

v·[cqt , c
p
t , s

r
t ]+b

v),
(4)

where cqt and c
p
t are context vectors, and s

r
t is a

decoder state. Wv and bv are learnable parame-
ters.

To determine which of the four sources a new
word wt+1 is selected from, we introduce a dis-
crete latent variable yt ∈ {1, 2, 3, 4} as an indica-
tor. When yt = 1 or 2, the word wt+1 is generated
from the distribution P (wt+1|yt) given by:

P (wt+1|yt) =

{∑
i:wi=wt+1

aqti yt = 1∑
i:wi=wt+1

apti yt = 2.
(5)

If yt = 3, KEAG picks word wt+1 according to
the vocabulary distribution Pv(w|cqt , c

p
t , s

r
t ) given

in Equation 4. Otherwise, if yt = 4, the word
wt+1 comes from the fact selector, which will be
described in the coming section.

5 Knowledge Integration

In order for KEAG to integrate external knowl-
edge, we first extract related facts from the knowl-
edge base in response to a given question, from
which we then pick the most relevant fact that can
be used for answer composition. In this section,
we present the two modules for knowledge inte-
gration: related fact extraction and fact selection.

5.1 Related Fact Extraction
Due to the size of a knowledge base and the large
amount of unnecessary information, we need an
effective way of extracting a set of candidate facts
which provide novel information while being re-
lated to a given question and passage.

For each instance (q, p), we first extract facts
with the subject or object that occurs in question q
or passage p. Scores are added to each extracted
fact according to the following rules:

• Score+4, if the subject occurs in q, and the
object occurs in p.
• Score+2, if the subject and the object both

occur in p.
• Score+1, if the subject occurs in q or p.

The scoring rules are set heuristically such that
they model relative fact importance in different
interactions. Next, we sort the fact triples in de-
scending order of their scores, and take the topNf
facts from the sorted list as the related facts for
subsequent processing.

5.2 Fact Selection
Figure 2 displays how a fact is selected from the
set of related facts for answer completion. With
the extracted knowledge, we first embed every re-
lated fact f by concatenating the embeddings of
the subject es, the relation er and the object eo.
The embeddings of subjects and objects are initial-
ized with pre-trained GloVe vectors (and average
pooling for multiple words), when the words are
present in the vocabulary. The fact embedding is
followed by a linear transformation to relate sub-
ject es to object eo with relation er:

f = We · [es, er, eo] + be. (6)

where f denotes fact representation, [·, ·] denotes
vector concatenation, and We and be are learn-
able parameters. The set of all related fact rep-
resentations F = {f1, f2, . . . , fNf } is considered
to be a short-term memory of the knowledge base
while answering questions on given passages.

To enrich KEAG with the facts collected from
the knowledge base, we propose to complete an
answer with the most relevant fact(s) whenever it
is determined to resort to knowledge during the
process of answer generation. The most relevant
fact is selected from the related fact set F based
on the dynamic generation state. In this model, we
introduce a discrete latent random variable zt ∈



2525

Embedding Layer (Glove)

Subject ObjectRelation

Fully-connected Layer

Decoder 
state

Fact !"

# !"|%, '"(

Figure 2: An overview of the fact selection module
(best viewed in color)

[1, Nf ] to explicitly indicate which fact is selected
to be put into an answer in timestep t. The model
selects a fact by sampling a zt from the discrete
distribution P (zt|F, srt ) given by:

P (zt|·) =
1

Z
·exp(gfᵀtanh(Wf fzt+Ufsrt+bf )),

(7)
where Z is the normalization term, Z =∑Nf

i=1 exp(g
fᵀtanh(Wf fi +Ufsrt + b

f )), and srt
is the hidden state from the decoder in timestep t.
gf , Wf , Uf and bf are learnable parameters.

The presence of discrete latent variables z, how-
ever, presents a challenge to training the neu-
ral KEAG model, since the backpropagation al-
gorithm, while enabling efficient computation of
parameter gradients, does not apply to the non-
differentiable layer introduced by the discrete vari-
ables. In particular, gradients cannot propagate
through discrete samples from the categorical dis-
tribution P (zt|F, srt ).

To address this problem, we create a dif-
ferentiable estimator for discrete random vari-
ables with the Gumbel-Softmax trick (Jang et al.,
2017). Specifically, we first compute the dis-
crete distribution P (zt|F, srt ) with class probabili-
ties π1, π2, . . . , πNf by Equation 7. The Gumbel-
Max trick (Gumbel, 1954) allows to draw samples
from the categorical distribution P (zt|F, srt ) by
calculating one hot(argmaxi[gi + log πi]), where
g1, g2, . . . , gNf are i.i.d. samples drawn from the
Gumbel(0, 1) distribution. For the inference of a
discrete variable zt, we approximate the Gumbel-
Max trick by the continuous softmax function (in
place of argmax) with temperature τ to generate
a sample vector ẑt:

ẑti =
exp((log(πi) + gi)/τ)∑Nf
j=1 exp((log(πj) + gj)/τ)

. (8)

When τ approaches zero, the generated sample ẑt
becomes a one-hot vector. τ is gradually annealed
over the course of training.

This new differentiable estimator allows us to
backpropagate through zt ∼ P (zt|F, srt ) for gra-
dient estimation of every single sample. The value
of zt indicates a fact selected by the decoder in
timestep t. When the next word is determined to
come from knowledge, the model appends the ob-
ject of the selected fact to the end of the answer
being generated.

6 Learning Model Parameters
To learn the parameters θ in KEAG with la-
tent source indicators y, we maximize the log-
likelihood of words in all answers. For each an-
swer, the log-likelihood of the words is given by:

logP (wr1, w
r
2, . . . , w

r
Nr |θ) =

Nr∑
t=1

logP (wrt |θ)

=

Nr∑
t=1

log

4∑
yt=1

P (wt+1|yt)P (yt|θ) (9)

≥
Nr∑
t=1

4∑
yt=1

P (yt|θ) logP (wt+1|yt) (10)

=

Nr∑
t=1

Eyt|θ[logP (wt+1|yt)], (11)

where the word likelihood at each timestep is ob-
tained by marginalizing out the latent source vari-
able yt. Unfortunately, direct optimization of
Equation 9 is intractable, so we instead learn the
objective function through optimizing its varia-
tional lower bound given in Equations 10 and 11,
obtained from Jensen’s inequality.

To estimate the expectation in Equation 11, we
use Monte Carlo sampling on the source selec-
tor variables y in the gradient computation. In
particular, the Gumbel-Softmax trick is applied to
generate discrete samples ŷ from the probability
P (yt|cqt , c

p
t , s

r
t ,x

r
t ) given by:

P (yt|·) = softmax(Wy · [cqt , c
p
t , s

r
t ,x

r
t ] + b

y),
(12)

where xrt is the embedding of the answer word in
timestep t, Wy and by are learnable parameters.
The generated samples are fed to logP (wt+1|yt)
to estimate the expectation.

7 Experiments
We perform quantitative and qualitative analysis
of KEAG through experiments. In our experi-



2526

ments, we also study the impact of the integrated
knowledge and the ablations of the KEAG model.
In addition, we illustrate how natural answers
are generated by KEAG with the aid of external
knowledge by analyzing a running example.

7.1 Dataset and Evaluation Metrics
Given our objective of generating natural an-
swers by document reading, the MARCO
dataset (Nguyen et al., 2016) released by Mi-
crosoft is the best fit for benchmarking KEAG and
other answer generation methods. We use the lat-
est MARCO V2.1 dataset and focus on the “Q&A
+ Natural Language Generation” task in the eval-
uation, the goal of which is to provide the best an-
swer available in natural language that could be
used by a smart device / digital assistant.

In the MARCO dataset, the questions are user
queries issued to the Bing search engine and the
contextual passages are from real web documents.
The data has been split into a training set (153,725
QA pairs), a dev set (12,467 QA pairs) and a test
set (101,092 questions with unpublished answers).
Since true answers are not available in the test set,
we hold out the dev set for evaluation in our ex-
periments, and test models for each question on
its associated passages by concatenating them all
together. We tune the hyper-parameters by cross-
validation on the training set.

The answers are human-generated and not nec-
essarily sub-spans of the passages, so the offi-
cial evaluation tool of MARCO uses the met-
rics BLEU-1 (Papineni et al., 2002) and ROUGE-
L (Lin, 2004). We use both metrics for our evalu-
ation to measure the quality of generated answers
against the ground truth.

For external knowledge, we use Concept-
Net (Speer et al., 2016), one of the most widely
used commonsense knowledge bases. Our KEAG
is generic and thus can also be applied to other
knowledge bases. ConceptNet is a semantic net-
work representing words and phrases as well as the
commonsense relationships between them. After
filtering out non-English entities and relation types
with few facts, we have 2,823,089 fact triples and
32 relation types for the model to consume.

7.2 Implementation Details
In KEAG, we use 300-dimensional pre-trained
Glove word embeddings (Pennington et al., 2014)
for initialization with update during training. The
dimension of hidden states is set to 256 for every

Model Rouge-L Bleu-1
BiDAF 19.42 13.03
BiDAF+Seq2Seq 34.15 29.68
S-Net 42.71 36.19
S-Net+Seq2Seq 46.83 39.74
QFS 40.58 39.96
VNET 45.93 41.02
gQA 45.75 41.10
KEAG 51.68 45.97

Table 1: Metrics of KEAG and QA models disregard-
ing knowledge on the MARCO dataset.

LSTM. The fact representation f has 500 dimen-
sions. The maximum number of related factsNf is
set to be 1000. We use a vocabulary of 50K words
(filtered by frequency). Note that the source se-
lector enables KEAG to handle out-of-vocabulary
words by generating a word from given text or
knowledge.

At both training and test stages, we truncate a
passage to 800 words, and limit the length of an
answer to 120 words. We train on a single Tesla
M40 GPU with the batch size of 16. At test time,
answers are generated using beam search with the
beam size of 4.

7.3 Model Comparisons
Table 1 compares KEAG with the following state-
of-the-art extractive/generative QA models, which
do not make use of external knowledge:

1. BiDAF (Seo et al., 2017): A multi-stage hier-
archical process that represents the context at
different levels of granularity, and using the
bi-directional attention flow mechanism for
answer extraction

2. BiDAF+Seq2Seq: A BiDAF model followed
by an additional sequence-to-sequence model
for answer generation

3. S-Net (Tan et al., 2018): An extraction-then-
synthesis framework to synthesize answers
from extracted evidences

4. S-Net+Seq2Seq: An S-Net model followed
by an additional sequence-to-sequence model
for answer generation

5. QFS (Nema et al., 2017): A model
that adapts the query-focused summarization
model to answer generation

6. VNET (Wang et al., 2018): An MRC model
that enables answer candidates from different



2527

Model Rouge-L Bleu-1
gQA w/ KBLSTM 49.33 42.81
gQA w/ CRWE 49.79 43.35
MHPGM 50.51 44.73
KEAG 51.68 45.97

Table 2: Metrics of KEAG and knowledge-enriched
QA models on the MARCO dataset.

passages to verify each other based on their
content representations

7. gQA (Mitra, 2017): A generative approach
to question answering by incorporating the
copying mechanism and the coverage vector

Table 1 shows the comparison of QA models
in Rouge-L and Bleu-1. From the table we ob-
serve that abstractive QA models (e.g., KEAG)
are consistently superior to extractive models (e.g.,
BiDAF) in answer quality. Therefore, abstractive
QA models establish a strong base architecture to
be enhanced with external knowledge, which mo-
tivates this work. Among the abstractive mod-
els, gQA can be viewed as a simplification of
KEAG, which generates answer words from pas-
sages and the vocabulary without the use of knowl-
edge. In addition, KEAG incorporates a stochas-
tic source selector while gQA does not. The re-
sult that KEAG significantly outperforms gQA
demonstrates the effectiveness of KEAG’s archi-
tecture and the benefit of knowledge integration.

Table 2 shows the metrics of KEAG in compar-
ison to those of the following state-of-the-art QA
models that are adapted to leveraging knowledge:

1. gQA w/ KBLSTM (Yang and Mitchell,
2017): KBLSTM is a neural model that lever-
ages continuous representations of knowl-
edge bases to enhance the learning of re-
current neural networks for machine reading.
We plug it into gQA to make use of external
knowledge for natural answer generation.

2. gQA w/ CRWE (Weissenborn, 2017):
CRWE is a reading architecture with dy-
namic integration of background knowledge
based on contextual refinement of word
embeddings by leveraging supplementary
knowledge. We extend gQA with the refined
word embedding for this model.

3. MHPGM (Bauer et al., 2018): A multi-hop
reasoning QA model which fills in the gaps
of inference with commonsense knowledge.

Model Syntactic Correct
gQA 3.78 3.54
gQA w/ KBLSTM 3.98 3.62
gQA w/ CRWE 3.91 3.69
MHPGM 4.10 3.81
KEAG 4.18 4.03

Table 3: Human evaluation of KEAG and state-of-the-
art answer generation models. Scores range in [1, 5].

From Table 2, it can be clearly observed that
KEAG performs best with the highest Rouge-L
and Bleu-1 scores among the knowledge-enriched
answer generation models. The major differ-
ence between KEAG and the other models is the
way of incorporating external knowledge into a
model. gQA w/ KBLSTM and gQA w/ CRWE ex-
tend gQA with the module that consumes knowl-
edge, and MHPGM incorporates knowledge with
selectively-gated attention while its decoder does
not leverage words from knowledge in answer
generation. Different from these models, KEAG
utilizes two stochastic selectors to determine when
to leverage knowledge and which fact to use.
It brings additional gains in exploiting external
knowledge to generate abstractive answers.

Since neither Rouge-L nor Bleu-1 can measure
the quality of generated answers in terms of their
correctness and accuracy, we also conduct hu-
man evaluation on Amazon Mechanical Turk. The
evaluation assesses the answer quality on gram-
maticality and correctness. We randomly select
100 questions from the dev set, and ask turkers for
ratings in a Likert scale (∈ [1, 5]) on the generated
answers.

Table 3 reports the human evaluation scores
of KEAG and state-of-the-art answer generation
models. The KEAG model surpasses all the oth-
ers in generating correct answers syntactically
and substantively. In terms of syntactic correct-
ness, KEAG and MHPGM both perform well
thanks to their architectures of composing answer
text and integrating knowledge. On the other
hand, KEAG significantly outperforms all com-
pared models in generating substantively correct
answers, which demonstrates its power in exploit-
ing external knowledge.

7.4 Ablation Studies
We conduct ablation studies to assess the individ-
ual contribution of every component in KEAG. Ta-
ble 4 reports the performance of the full KEAG



2528

Ablation Rouge-L Bleu-1
Full KEAG 51.68 45.97
7 supplementary knowledge 49.98 44.59
7 latent indicators y 47.61 42.10
7 source selector 38.33 36.75

Table 4: Ablation tests of KEAG.

model and its ablations.
We evaluate how much incorporating exter-

nal knowledge as supplementary information con-
tributes to natural answer generation by removing
the supplementary knowledge and the correspond-
ing fact selection module from KEAG’s architec-
ture. It can be seen that the knowledge component
plays an important role in generating high-quality
answers, with a drop to 49.98 on Rouge-L after the
supplementary knowledge is removed.

To study the effect of our learning method, we
further ablate the latent indicators y, which leads
to degradation to gQA except that the new model
can select answer words from the question source
while gQA cannot. Our learning method proves to
be effective with a drop of about 5% on Rouge-L
and about 6% on Bleu-1 after ablation.

Finally, for ablating the source selector, we have
a new model that generates answer words from the
vocabulary alone. It results in a significant drop to
38.33 on Rouge-L, confirming its effectiveness in
generating natural answers.

7.5 Visualization and Interpretation
The source selector allows us to visualize how ev-
ery word in an answer is generated from one of
the sources of the question, passage, vocabulary
and knowledge, which gives us insights about how
KEAG works.

Table 5 visualizes a sample QA pair from
KEAG and which source every word in the answer
is selected from (indicated by the sample value
of the source selector variable yt). As exempli-
fied in the table, the source distribution P (yt|θ)
varies over decoding timesteps. To answer the
question, at each timestep, KEAG first selects a
source based on the sample from P (yt|θ), fol-
lowed by generating an answer word from the se-
lected source. It is observed that the in the gener-
ated answer the keyword personality comes from
the knowledge source which relates psychopathy
to personality. The answer word psychopathy is
selected from the question source, which leads
to a well-formed answer with a complete sen-

Question
What’s psychopathy?
Answer with source probabilities

Question src Psychopathy is a personality
disorder.

Passage src Psychopathy is a personality
disorder.

Vocabulary src Psychopathy is a personality
disorder.

Knowledge src Psychopathy is a personality
disorder.

Answer colored by source
Psychopathy is a personality disorder.

Table 5: Visualization of a sample QA pair and the
source of individual words in the answer. The Answer
with source probabilities section displays a heatmap
on answer words selected from the question, passage,
vocabulary and knowledge, respectively. A slot with
a higher source probability is highlighted in darker
cyan. The Answer colored by source section shows
the answer in which every word is colored based on
the source it was actually selected from. Words in blue
come from the question, red from the passage, green
from the vocabulary, and orange from the knowledge.
The visualization is best viewed in color.

tence. Another keyword disorder, on the other
hand, comes from the passage source. This re-
sults from reading comprehension of the model on
the passage. To generate a final answer in good
form, KEAG picks the filler words is and a as well
as the period “.” from the vocabulary source. It
makes the generated answer semantically correct
and comprehensive.

8 Conclusion and Future Work
This paper presents a new neural model KEAG
that is designed to bring symbolic knowledge from
a knowledge base into abstractive answer genera-
tion. This architecture employs the source selec-
tor that allows for learning an appropriate tradeoff
for blending external knowledge with information
from textual context. The related fact extraction
and stochastic fact selection modules are intro-
duced to complete an answer with relevant facts.

This work opens up for deeper investigation of
answer generation models in a targeted way, al-
lowing us to investigate what knowledge sources
are required for different domains. In future work,
we will explore even tighter integration of sym-
bolic knowledge and stronger reasoning methods.



2529

References
Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Ben-

gio. 2015. Neural machine translation by jointly
learning to align and translate. In Proceedings of
the International Conference on Learning Represen-
tations.

Lisa Bauer, Yicheng Wang, and Mohit Bansal. 2018.
Commonsense for generative multi-hop question an-
swering tasks. In Proceedings of the 2018 Con-
ference on Empirical Methods in Natural Language
Processing, pages 4220–4230.

Anthony Fader, Stephen Soderland, and Oren Etzioni.
2011. Identifying relations for open information ex-
traction. In Proceedings of the Conference on Em-
pirical Methods in Natural Language Processing,
pages 1535–1545.

Yao Fu and Yansong Feng. 2018. Natural answer gen-
eration with heterogeneous memory. In Proceed-
ings of the 2018 Conference of the North Ameri-
can Chapter of the Association for Computational
Linguistics: Human Language Technologies, pages
185–195.

Shen Gao, Zhaochun Ren, Yihong Zhao, Dongyan
Zhao, Dawei Yin, and Rui Yan. 2019. Product-
aware answer generation in e-commerce question-
answering. In Proceedings of the 12th ACM Inter-
national Conference on Web Search and Data Min-
ing.

E.J. Gumbel. 1954. Statistical theory of extreme val-
ues and some practical applications: a series of lec-
tures. Applied mathematics series. U. S. Govt. Print.
Office.

Shizhu He, Cao Liu, Kang Liu, and Jun Zhao.
2017. Generating natural answers by incorporating
copying and retrieving mechanisms in sequence-to-
sequence learning. In Proceedings of the 55th An-
nual Meeting of the Association for Computational
Linguistics, pages 199–208.

Eric Jang, Shixiang Gu, and Ben Poole. 2017. Cate-
gorical reparametrization with gumbel-softmax. In
Proceedings of the International Conference on
Learning Representations.

Chin-Yew Lin. 2004. Rouge: A package for automatic
evaluation of summaries. In Proceedings of the ACL
Workshop: Text Summarization Braches Out 2004,
page 10.

Teng Long, Emmanuel Bengio, Ryan Lowe, Jackie
Chi Kit Cheung, and Doina Precup. 2017. World
knowledge for reading comprehension: Rare entity
prediction with hierarchical lstms using external de-
scriptions. In Proceedings of the 2017 Conference
on Empirical Methods in Natural Language Pro-
cessing, pages 825–834.

Mausam, Michael Schmitz, Robert Bart, Stephen
Soderland, and Oren Etzioni. 2012. Open language

learning for information extraction. In Proceedings
of the 2012 Joint Conference on Empirical Methods
in Natural Language Processing and Computational
Natural Language Learning, pages 523–534.

Todor Mihaylov and Anette Frank. 2018. Knowledge-
able reader: Enhancing cloze-style reading compre-
hension with external commonsense knowledge. In
Proceedings of the 56th Annual Meeting of the As-
sociation for Computational Linguistics (Volume 1:
Long Papers), pages 821–832.

Rajarshee Mitra. 2017. An abstractive approach to
question answering. CoRR, abs/1711.06238.

Ramesh Nallapati, Bowen Zhou, Cicero dos Santos,
Caglar Gulcehre, and Bing Xiang. 2016. Ab-
stractive text summarization using sequence-to-
sequence rnns and beyond. In Proceedings of The
20th SIGNLL Conference on Computational Natu-
ral Language Learning.

Preksha Nema, Mitesh M. Khapra, Anirban Laha, and
Balaraman Ravindran. 2017. Diversity driven at-
tention model for query-based abstractive summa-
rization. In Proceedings of the 55th Annual Meet-
ing of the Association for Computational Linguis-
tics, pages 1063–1072. Association for Computa-
tional Linguistics.

Tri Nguyen, Mir Rosenberg, Xia Song, Jianfeng Gao,
Saurabh Tiwary, Rangan Majumder, and Li Deng.
2016. MS MARCO: A human generated machine
reading comprehension dataset. In Proceedings
of the Workshop on Cognitive Computation: Inte-
grating neural and symbolic approaches 2016 co-
located with the 30th Annual Conference on Neural
Information Processing Systems (NIPS).

Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: A method for automatic eval-
uation of machine translation. In Proceedings of the
40th Annual Meeting on Association for Computa-
tional Linguistics, pages 311–318.

Thomas Pellissier Tanon, Denny Vrandečić, Sebas-
tian Schaffert, Thomas Steiner, and Lydia Pintscher.
2016. From freebase to wikidata: The great migra-
tion. In Proceedings of the 25th International Con-
ference on World Wide Web, pages 1419–1428.

Jeffrey Pennington, Richard Socher, and Christopher D
Manning. 2014. Glove: Global vectors for word
representation. In EMNLP, volume 14, pages 1532–
1543.

Abigail See, Peter J. Liu, and Christopher D. Manning.
2017. Get to the point: Summarization with pointer-
generator networks. In Proceedings of the 55th An-
nual Meeting of the Association for Computational
Linguistics.

Minjoon Seo, Aniruddha Kembhavi, Ali Farhadi, and
Hananneh Hajishirzi. 2017. Bidirectional attention
flow for machine comprehension. In Proceedings of
the International Conference on Learning Represen-
tations.



2530

Robert Speer, Joshua Chin, and Catherine Havasi.
2016. Conceptnet 5.5: An open multilingual graph
of general knowledge. In AAAI Conference on Arti-
ficial Intelligence.

Chuanqi Tan, Furu Wei, Nan Yang, Bowen Du,
Weifeng Lv, and Ming Zhou. 2018. S-net: From
answer extraction to answer synthesis for machine
reading comprehension. In AAAI.

Yizhong Wang, Kai Liu, Jing Liu, Wei He, Yajuan Lyu,
Hua Wu, Sujian Li, and Haifeng Wang. 2018. Multi-
passage machine reading comprehension with cross-
passage answer verification. In Proceedings of the
56th Annual Meeting of the Association for Compu-
tational Linguistics (Volume 1: Long Papers), pages
1918–1927, Melbourne, Australia. Association for
Computational Linguistics.

Dirk Weissenborn. 2017. Dynamic integration of back-
ground knowledge in neural NLU systems. CoRR,
abs/1706.02596.

Bishan Yang and Tom Mitchell. 2017. Leveraging
knowledge bases in LSTMs for improving machine
reading. In Proceedings of the 55th Annual Meet-
ing of the Association for Computational Linguis-
tics, pages 1436–1446.

Jun Yin, Xin Jiang, Zhengdong Lu, Lifeng Shang,
Hang Li, and Xiaoming Li. 2016. Neural gener-
ative question answering. In Proceedings of the
Workshop on Human-Computer Question Answer-
ing, pages 36–42.

Wanjun Zhong, Duyu Tang, Nan Duan, Ming Zhou,
Jiahai Wang, and Jian Yin. 2018. Improving ques-
tion answering by commonsense-based pre-training.
CoRR, abs/1809.03568.


