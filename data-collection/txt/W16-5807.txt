



















































A Neural Model for Language Identification in Code-Switched Tweets


Proceedings of the Second Workshop on Computational Approaches to Code Switching, pages 60–64,
Austin, TX, November 1, 2016. c©2016 Association for Computational Linguistics

A Neural Model for Language Identification in Code-Switched Tweets

Aaron Jaech1 George Mulcaire2 Shobhit Hathi2 Mari Ostendorf1 Noah A. Smith2
1Electrical Engineering 2Computer Science & Engineering

University of Washington, Seattle, WA 98195, USA
ajaech@uw.edu, gmulc@uw.edu, shathi@uw.edu
ostendor@uw.edu, nasmith@cs.washington.edu

Abstract

Language identification systems suffer when
working with short texts or in domains with
unconventional spelling, such as Twitter or
other social media. These challenges are ex-
plored in a shared task for Language Identi-
fication in Code-Switched Data (LICS 2016).
We apply a hierarchical neural model to this
task, learning character and contextualized
word-level representations to make word-level
language predictions. This approach performs
well on both the 2014 and 2016 versions of the
shared task.

1 Introduction

Language identification (language ID) remains a
difficult problem, particulary in social media text
where informal styles, closely related language
pairs, and code-switching are common. Progress on
language ID is needed especially since downstream
tasks, like translation or semantic parsing, depend
on it.

Continuous representations for language data,
which have produced new states of the art for lan-
guage modeling (Mikolov et al., 2010), machine
translation (Bahdanau et al., 2015), and other tasks,
can be useful for language ID. For the Language
Identification in Code-Switched Data shared task
(LICS 2016), we submitted a hierarchical character-
word model closely following Jaech et al. (2016b),
focusing on word-level language ID. Our discussion
of the model closely follows that paper.

This model, which we call C2V2L (“character
to vector to language”) is hierarchical in the sense
that it explicitly builds a continuous representation
for each word from its character sequence, captur-
ing orthographic and morphology-related patterns,

and then combines those word-level representations
to use context from the full word sequence before
making predictions for each word. The use of char-
acter representations is well motivated for code-
switching tasks, since the presence of multiple lan-
guages means that one is more likely to encounter a
previously unseen word.

Our model does not require special handling of
casing or punctuation, nor do we need to remove the
URLs, usernames, or hashtags, and it is trained end-
to-end using standard procedures.

2 Model

Our model has two main components, though
they are trained together, end-to-end. The first,
“char2vec,” applies a convolutional neural network
(CNN) to a whitespace-delimited word’s Unicode
character sequence, providing a word vector. The
second is a bidirectional LSTM recurrent neural net-
work (RNN) that maps a sequence of such word vec-
tors to a language label.

2.1 Char2vec

The first layer of char2vec embeds characters. An
embedding is learned for each Unicode code point
that appears at least twice in the training data, in-
cluding punctuation, emoji, and other symbols. If
C is the set of characters then we let the size of the
character embedding layer be d = dlog2 |C|e. (If
each dimension of the character embedding vector
holds just one bit of information then d bits should
be enough to uniquely encode each character.) The
character embedding matrix is Q ∈ Rd×|C|. Words
are given to the model as a sequence of charac-
ters. When each character in a word of length l is
replaced by its embedding vector we get a matrix
C ∈ Rd×(l+2). There are l + 2 columns in C be-

60



cause padding characters are added to the left and
right of each word.

The char2vec architecture uses two sets of fil-
ter banks. The first set is comprised of matrices
Hai ∈ Rd×3 where i ranges from 1 to n1. The
matrix C is narrowly convolved with each Hai , a
bias term ba is added and an ReLU non-linearity,
ReLU(x) = max(0, x), is applied to produce an
output T1 = ReLU(conv(C,Ha) + ba). T1 is
of size n1 × l with one row for each of the filters
and one column for each of the characters in the in-
put word. Since each Hai is a filter with a width
of three characters, the columns of T1 each hold a
representation of a character tri-gram. During train-
ing, we apply dropout on T1 to regularize the model.
The matrix T1 is then convolved with a second set
of filters Hbi ∈ Rn1×w where bi ranges from 1 to
3n2 and n2 controls the number of filters of each of
the possible widths, w = 3, 4, or 5. Another con-
volution and ReLU non-linearity is applied to get
T2 = ReLU(conv(T1,Hb) + bb). Max-pooling
across time is used to create a fix-sized vector y from
T2. The dimension of y is 3n2, corresponding to the
number of filters used.

Similar to Kim et al. (2016) who use a highway
network after the max-pooling layer, we apply a
residual network layer. The residual network uses
a matrix W ∈ R3n2×3n2 and bias vector b3 to cre-
ate the vector z = y + fR(y) where fR(y) =
ReLU(Wy + b3). The resulting vector z is used as
a word embedding vector in the word-level LSTM
portion of the model.

There are three differences between our version
of the model and the one described by Kim et al.
(2016). First, we use two layers of convolution in-
stead of just one, inspired by Ling et al. (2015a)
which uses a 2-layer LSTM for character modeling.
Second, we use the ReLU function as a nonlinear-
ity as opposed to the tanh function. ReLU has been
highly successful in computer vision in conjunction
with convolutional layers (Jarrett et al., 2009). Fi-
nally, we use a residual network layer instead of a
highway network layer after the max-pooling step,
to reduce the model size.

2.2 Sentence-level Context
The sequence of word embedding vectors is pro-
cessed by a bi-LSTM, which outputs a sequence of

Figure 1: C2V2L model architecture. The model takes the
word “esfuezo,” a misspelling of the Spanish word “esfuerzo,”

and maps it to a word vector via the two CNN layers and the

residual layer. The word vector is then combined with others

via the LSTM, and a prediction made for each word.

vectors, [v1,v2,v3 . . .vT ] where T is the number
of words in the tweet. All LSTM gates are used as
defined by Sak et al. (2014). Dropout is used as a
regularizer on the inputs to the LSTM (Pham et al.,
2014). The output vectors vi are transformed into
probability distributions over the set of languages by
applying an affine transformation followed by a soft-
max:

pi = fL(vi) =
exp(Avi + b)∑T
t=1 exp(Avt + b)

The final affine transformation can be interpreted
as a language embedding, where each language is

61



represented by a vector of the same dimensional-
ity as the LSTM outputs. The goal of the LSTM
then is (roughly) to maximize the dot product of
each word’s representation with the language em-
bedding(s) for that token. The loss is the cross-
entropy between each word’s prediction and the cor-
responding gold label for that word.

3 Implementation Details

3.1 Preprocessing

The data contains many long and repetitive charac-
ter sequences such as “hahahaha...” or “arghhhhh...”.
To deal with these, we restricted any sequence of re-
peating characters to at most five repetitions where
the repeating pattern can be from one to four charac-
ters. There are many tweets that string together large
numbers of usernames or hashtags without spaces
between them. These create extra long “words”
that cause our implementation to use more mem-
ory and do extra computation during training. We
therefore enforce the constraint that there must be
a space before any URL, username, or hashtag. To
deal with the few remaining extra-long character se-
quences, we force word breaks in non-space charac-
ter sequences every 40 bytes. This primarily affects
languages that are not space-delimited like Chinese.
We do not perform any special handling of casing
or punctuation nor do we need to remove the URLs,
usernames, or hashtags as has been done in previous
work (Zubiaga et al., 2014).

3.2 Training and Tuning

Training is done using minibatches of size 25 and a
learning rate of 0.001 using Adam (Kingma and Ba,
2015), and continued for 50,000 minibatches.

There are only four hyperparameters to tune for
each model: the number of filters in the first con-
volutional layer, the number of filters in the second
convolutional layer, the size of the word-level LSTM
vector, and the dropout rate.

To tune hyperparameters, we trained 10 models
with random parameter settings on 80% of the data
from the Spanish-English training set, and chose the
settings from the model that performed best on the
remaining 20%. We then retrained on the full train-
ing set with these settings (respectively, 59, 108, 23,
and 25%). Our final model architecture has roughly

177K parameters.

4 Experiments

Because C2V2L produces language predictions for
every word, the architecture is well suited to analysis
of code-switched text, in which different words may
belong to different languages. We used the Spanish-
English dataset from the EMNLP 2014 shared task
on Language Identification in Code-Switched Data
(Solorio et al., 2014), and the Spanish-English and
Arabic-MSA datasets from the EMNLP 2016 ver-
sion of the shared task. Each dataset is a collec-
tion of monolingual and code-switched tweets in
two main languages: English and Spanish, or Mod-
ern Standard Arabic (MSA) and Arabic dialects.

4.1 LICS 2014

The LICS 2014 dataset (Zubiaga et al., 2014) comes
from a language ID shared task that focused on
tweets that code-switch between two languages.
While several language pairs were included in the
shared task, we evaluated only on Spanish-English
in these experiments. There are approximately
110,000 labeled examples in the training data and
34,000 in the test set. The data is unbalanced, with
over twice as many examples in English (74,000)
as in Spanish (35,000). A further 30,000 are la-
beled “other” for punctuation, emoji, and unintelli-
gible words. There are also a small number of ex-
amples labeled “NE” for named entities (2.16%),
“mixed” for words that include both English and
Spanish (0.03%), and “ambiguous” for words that
could be interpreted as either language in context
(0.22%). “NE” and “other” are predicted as if they
were separate languages, but the ’ambiguous’ label
is ignored.

C2V2L performed well at this task, scoring 95.1
F1 for English (which would have achieved second
place in the 2014 shared task, out of eight entries),
94.1 for Spanish (second place), 36.2 for named en-
tities (fourth place) and 94.2 for Other (third place).1

While our code-switching results are not quite state-
of-the-art, they show that our model learns accurate
word-level predictions.

1Full results for the 2014 shared task can be found at
http://emnlp2014.org/workshops/CodeSwitch/
results.php.

62



Lang. pair (L1-L2) L1 L2 NE other
Spanish-English (ours) 0.931 0.977 0.454 0.910
Spanish-English (best) 0.931 0.977 0.537 0.994
Arabic-MSA (ours) 0.603 0.603 0.468 0.712
Arabic-MSA (best) 0.854 0.904 0.828 0.988

Table 1: F1 scores for the LICS 2016 shared task. The best
result from any system in each category is provided for com-

parison.

4.2 LICS 2016

The LICS 2016 shared task uses a similar format.
Here, the training and development sets for each lan-
guage pair correspond to the training and test sets
from LICS 2014, and new data was added to create
a new test set. However, labels were updated to cor-
rect errors and add two new categories: “fw” (for-
eign word) for examples that belong to a language
other than the main two, and “unk” for examples that
cannot be classified. We ignore “unk” and “fw.”

We submitted labels for both available language
pairs: Spanish-English and Arabic-MSA (distin-
guishing Modern Standard Arabic from Arabic di-
alects). Partial results,2 showing only the four
largest categories, are given in Table 1.

Our results for Spanish-English are competitive
with the best submitted systems. We ranked first or
tied for first in F1 for the primary categories, English
and Spanish, out of nine submitted systems. On
Arabic-MSA, we came in last among five systems.
This is likely due in part to the fact that we tuned
only on Spanish-English data and did not make any
adjustments when training the Arabic-MSA model.
A model that is tuned to the specific language pair,
and perhaps handled the ’other’ category with reg-
ular expressions in preprocessing, would likely per-
form better.

5 Related Work

Language ID has a long history both in the speech
domain (House and Neuburg, 1977) and for text
(Cavnar and Trenkle, 1994). Previous work on
the text domain mostly uses word or character n-
gram features combined with linear classifiers (Hur-
tado et al., 2014; Gamallo et al., 2014). Chang
and Lin (2014) outperformed the top results for

2Full results can be found at http://care4lang1.
seas.gwu.edu/cs2/results.html

English-Spanish and English-Nepali in the EMNLP
2014 Language Identitication in Code-Switched
Data (Solorio et al., 2014), using an RNN with skip-
gram word embeddings and character n-gram fea-
tures. Word-level language ID has also been studied
by Mandal et al. (2015) in the context of question an-
swering and by King and Abney (2013). Both used
primarily character n-gram features.

Several other studies have investigated the use of
character sequence models in language processing.
These techniques were first applied only to create
word embeddings (dos Santos and Zadrozny, 2015;
dos Santos and Guimaraes, 2015) and then later ex-
tended to have the word embeddings feed directly
into a word-level RNN. Applications include part-
of-speech (POS) tagging (Ling et al., 2015b), lan-
guage modeling (Ling et al., 2015a), dependency
parsing (Ballesteros et al., 2015), translation (Ling
et al., 2015b), and slot filling text analysis (Jaech et
al., 2016a). A more extensive discussion of related
work on language ID and character sequence models
can be found in Jaech et al. (2016b).

6 Conclusion

We present C2V2L, a hierarchical neural model for
language ID that preforms competitively on chal-
lenging word-level language ID tasks. Without fea-
ture engineering, we achieved the best performance
in two common categories and good results in two
others. Future work could include adapting C2V2L
for other sequence labeling tasks, having shown that
the current architecture already performs well.

Acknowledgments

We thank the anonymous reviewers for helpful feedback.
Part of this material is based upon work supported by
a subcontract with Raytheon BBN Technologies Corp.
under DARPA Prime Contract No. HR0011-15-C-0013.
This work was also supported by a gift to the University
of Washington by Google. Views expressed are those of
the authors and do not reflect the official policy or po-
sition of the Department of Defense of the U.S. govern-
ment.

References
Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Ben-

gio. 2015. Neural machine translation by jointly

63



learning to align and translate. In Proc. Int. Conf.
Learning Representations (ICLR).

Miguel Ballesteros, Chris Dyer, and Noah Smith. 2015.
Improved transition-based parsing by modeling char-
acters instead of words with LSTMs. In Proc.
Conf. Empirical Methods Natural Language Process.
(EMNLP).

William B. Cavnar and John M. Trenkle. 1994. N-gram-
based text categorization. In In Proc. of SDAIR-94,
3rd Annual Symposium on Document Analysis and In-
formation Retrieval, pages 161–175.

Joseph Chee Chang and Chu-Cheng Lin. 2014.
Recurrent-neural-network for language detection on
Twitter code-switching corpus. CoRR, abs/1412.4314.

Cicero dos Santos and Victor Guimaraes. 2015. Boost-
ing named entity recognition with neural character em-
beddings. In Proc. ACL Named Entities Workshop.

Cicero dos Santos and Bianca Zadrozny. 2015. Learning
character-level representations for part-of-speech tag-
ging. In Proc. Int. Conf. Machine Learning (ICML).

Pablo Gamallo, Marcos Garcia, and Susana Sotelo. 2014.
Comparing ranking-based and naive Bayes approaches
to language detection on tweets. In TweetLID@ SE-
PLN.

Arthur S House and Edward P Neuburg. 1977. Toward
automatic identification of the language of an utter-
ance. The Journal of the Acoustical Society of Amer-
ica, 62(3):708–713.

Lluís F Hurtado, Ferran Pla, and Mayte Giménez. 2014.
ELiRF-UPV en TweetLID: Identificación del idioma
en Twitter. In TweetLID@ SEPLN.

Aaron Jaech, Larry Heck, and Mari Ostendorf. 2016a.
Domain adaptation of recurrent neural networks for
natural language understanding. In Proc. Conf. Int.
Speech Communication Assoc. (INTERSPEECH).

Aaron Jaech, George Mulcaire, Shobhit Hathi, Mari Os-
tendorf, and Noah A. Smith. 2016b. Hierarchical
character-word models for language identification. In
Proc. Int. Workshop on Natural Language Processing
for Social Media (SocialNLP).

Kevin Jarrett, Koray Kavukcuoglu, Marc’Aurelio Ran-
zato, and Yann Lecun. 2009. What is the best multi-
stage architecture for object recognition? In 2009
IEEE 12th International Conference on Computer Vi-
sion. IEEE.

Yoon Kim, Yacine Jernite, David Sontag, and Alexander
Rush. 2016. Character-aware neural language mod-
els. In Proc. AAAI.

Ben King and Steven Abney. 2013. Labeling the lan-
guages of words in mixed-language documents using
weakly supervised methods. In Proc. Conf. North
American Chapter Assoc. for Computational Linguis-
tics: Human Language Technologies (NAACL-HLT).

Diederik Kingma and Jimmy Ba. 2015. Adam: A
method for stochastic optimization. In Proc. Int. Conf.
Learning Representations (ICLR).

Wang Ling, Tiago Luís, Luís Marujo, Ramón Fernandez
Astudillo, Silvio Amir, Chris Dyer, Alan W Black, and
Isabel Trancoso. 2015a. Finding function in form:
Compositional character models for open vocabulary
word representation. In Proc. Conf. Empirical Meth-
ods Natural Language Process. (EMNLP).

Wang Ling, Isabel Trancoso, Chris Dyer, and Alan Black.
2015b. Character-based neural machine translation.
arXiv preprint arXiv:1511.04586v1.

Soumik Mandal, Somnath Banerjee, Sudip Kumar
Naskar, Paolo Rosso, and Sivaji Bandyopadhyay.
2015. Adaptive voting in multiple classifier systems
for word level language identification. In the Working
Notes in Forum for Information Retrieval Evaluation
(FIRE).

Tomas Mikolov, Martin Karafiát, Lukas Burget, Jan Cer-
nockỳ, and Sanjeev Khudanpur. 2010. Recurrent neu-
ral network based language model. In Proc. Conf. Int.
Speech Communication Assoc. (INTERSPEECH).

V. Pham, T. Bluche, C. Kermorvant, and J. Louradour.
2014. Dropout improves recurrent neural networks for
handwriting recognition. In Proc. Int. Conf. on Fron-
tiers in Handwriting Recognition (ICFHR).

Hasim Sak, Andrew W Senior, and Françoise Beaufays.
2014. Long short-term memory recurrent neural net-
work architectures for large scale acoustic modeling.
In Proc. Conf. Int. Speech Communication Assoc. (IN-
TERSPEECH).

Thamar Solorio, Elizabeth Blair, Suraj Maharjan, Steven
Bethard, Mona Diab, Mahmoud Ghoneim, Abdelati
Hawwari, Fahad AlGhamdi, Julia Hirschberg, Alison
Chang, et al. 2014. Overview for the first shared task
on language identification in code-switched data. In
Proc. Int. Workshop on Computational Approaches to
Linguistic Code Switching (CALCS).

Arkaitz Zubiaga, Inaki San Vicente, Pablo Gamallo,
José Ramom Pichel Campos, Iñaki Alegría Loinaz,
Nora Aranberri, Aitzol Ezeiza, and Víctor Fresno-
Fernández. 2014. Overview of TweetLID: Tweet lan-
guage identification at SEPLN 2014. In TweetLID@
SEPLN.

64


