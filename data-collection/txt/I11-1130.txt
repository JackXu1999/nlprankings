















































Opinion Expression Mining by Exploiting Keyphrase Extraction


Proceedings of the 5th International Joint Conference on Natural Language Processing, pages 1162–1170,
Chiang Mai, Thailand, November 8 – 13, 2011. c©2011 AFNLP

Opinion Expression Mining by Exploiting Keyphrase Extraction

Gábor Berend
Department of Informatics,

University of Szeged
2. Árpád tér, H-6720, Szeged, Hungary
berendg@inf.u-szeged.hu

Abstract

In this paper, we shall introduce a system
for extracting the keyphrases for the rea-
son of authors’ opinion from product re-
views. The datasets for two fairly different
product review domains related to movies
and mobile phones were constructed semi-
automatically based on the pros and cons
entered by the authors. The system illus-
trates that the classic supervised keyphrase
extraction approach – mostly used for sci-
entific genre previously – could be adapted
for opinion-related keyphrases. Besides
adapting the original framework to this
special task through defining novel, task-
specific features, an efficient way of rep-
resenting keyphrase candidates will be
demonstrated as well. The paper also pro-
vides a comparison of the effectiveness of
the standard keyphrase extraction features
and that of the system designed for the
special task of opinion expression mining.

1 Introduction

The amount of community-generated contents on
the Web has been steadily growing and most of
the end-user contents (e.g. blogs and customer re-
views) are likely to deal with the author’s emo-
tions and opinions towards some subject. The au-
tomatic analysis of such material is useful for both
companies and consumers. Companies can eas-
ily get an overview of what people think of their
products and services and what their most impor-
tant strengths and weaknesses are while users can
have access to information from the Web before
purchasing some product.

In this paper we will introduce a system which
assigns pro and con keyphrases (free-text anno-
tation) to product reviews. When dealing with
product reviews, our definition of keyphrases is

the set of phrases that make the opinion-holder
feel negative or positive towards a given prod-
uct, i.e. they should be the reason why the au-
thor likes or dislikes the product in question (e.g.
cheap price, convenient user interface). Here, we
adapted the general keyphrase extraction proce-
dure from the scientific publications domain (Wit-
ten et al., 1999; Turney, 2003) to the extraction of
opinion-reasoning features. However, our task is
rather different since we aim at identifying the rea-
sons for opinions, instead of keyphrases that rep-
resent the content of the whole document.

The supervised keyphrase extractor to be in-
troduced here was trained on the pros and cons
assigned to the reviews by their authors on the
epinions.com site. These pros and cons
are ill-structured free-text annotations and their
length, depth and style are extremely heteroge-
neous. In order to have clean gold-standard cor-
pora, we manually revised the segmentation and
the contents of the pros and cons, and obtained sets
of tag-like keyphrases.

2 Related work

There have been many studies on opinion mining
(Turney, 2002; Pang et al., 2002; Titov and Mc-
Donald, 2008; Liu and Seneff, 2009). Our ap-
proach relates to previous work on the extraction
of reasons for opinions. Most of these papers treat
the task of mining reasons from product reviews as
one of identifying sentences that express the au-
thor’s negative or positive feelings (Hu and Liu,
2004a; Popescu and Etzioni, 2005). This paper is
clearly distinguishable from them as our goal is to
find the reasons for opinions expressed by phrases
and we aim the task of phrase extraction instead of
sentence recognition.

This work differs in important aspects even
from the frequent pattern mining-based approach
of (Hu and Liu, 2004b) since they regarded the
main task of mining opinion features with respect

1162



to a group of products, not individually at review-
level as we did. Even if an opinion feature phrase
is feasible for a given product-type, it is not nec-
essary that all of its occurrence are accompanied
with sentiments expressed towards it (e.g. The
phone comes in red and black colors, where color
could be an appropriate product feature, but not an
opinion-forming phrase).

A similar task to pro and con extraction gath-
ers the key aspects from document sets, which
has also gained interest recently (Sullivan, 2008;
Branavan et al., 2008; Liu and Seneff, 2009).
Existing aspect extraction systems first identify a
number of aspects throughout the whole review
set, then they automatically assign items from this
pre-recognized set of aspects to each unseen re-
view. Hence, they work at the corpus level and re-
strict themselves to using only a pre-defined num-
ber of aspects.

The approach presented here differs from these
studies in the sense that it looks for the reason
phrases themselves review by review, instead of
multi-labeling some aspects. These approaches
are intended for applications used by companies
who would like to obtain a general overview about
a product or would like to monitor the polarity
relating to their products in a particular commu-
nity. In contrast, we introduce here a keyphrase
extraction-based approach which works at the doc-
ument level as it extracts keyphrases from reviews
which are handled independently of each other.
This approach is more appropriate for the con-
sumers, who would like to be informed before pur-
chasing some product.

The work of Kim and Hovy (2006) lies probably
the closest to our one. They addressed the task of
extracting con and pro sentences, i.e. the sentences
on why the reviewers liked or disliked the product.
They also note that such pro and con expressions
can differ from positive and negative opinion ex-
pressions as factual sentences can also be reason
sentences (e.g. Video drains battery.). Here the
difference is that they extracted sentences, but we
targeted phrase extraction.

Most of the keyphrase extraction approaches
(Witten et al., 1999; Turney, 2003; Medelyan et
al., 2009; Kim et al., 2010) work on the scien-
tific domain and extract phrases from one docu-
ment that are the most characteristic of its content.
In these supervised approaches keyphrase extrac-
tion is regarded as a classification task, in which

certain n-grams of a specific document function
as keyphrase candidates, and the task is to clas-
sify them as proper or improper keyphrases. Here,
our task formalization of keyphrase extraction is
adapted from this line of research for opinion min-
ing and we focus on the extraction of phrases from
product reviews that also bear subjectivity and in-
duce sentiments in its author. As community gen-
erated pros and cons can provide abundant train-
ing samples and our goal is to extract the users’
own words, here we also follow this supervised
keyphrase extraction procedure.

3 Opinion Phrase Extraction Framework

Here, we employed a supervised machine learning
approach for the extraction of reason keyphrases
from a given review. Candidate terms were ex-
tracted from the text of the review and those
present in the extracted set of pros and cons were
regarded as positive examples during training and
evaluation. Maximum Entropy classifiers were
trained and the keyphrase candidates with the
highest posteriori probabilities were selected to be
keyphrases for a review of a test document in ques-
tion. In the following subsections we will describe
how keyphrase candidates and the feature space
representing them were constructed.

3.1 Candidate term generation

One key aspect in keyphrase extraction is the way
keyphrase candidates are selected and represented.
As usually the number of potentially extracted n-
grams and that of genuine keyphrases among them
show high imbalancedness, keyphrase candidates
are worth to be filtered, instead of using any suc-
cessive n-grams. For this reason we limited the
maximal length of the extracted phrases to at most
4 tokens and also required that the phrases should
begin with either a non-stopword adjective, verb
or noun and should end to either a non-stopword
noun or adjective.

As for the filtration of the candidate set, a
new step is introduced here, which omits nor-
malized phrases that had only such occurrences
which contained stopwords. This simple step
proved effective in excluding many non-proper
opinion phrases (i.e. increasing the maximal pre-
cision achievable) at the cost of discarding only
a small proportion of proper phrases (i.e. slightly
decreasing the best recall achievable).

Once we had the keyphrase candidates, they had

1163



to be brought to a normalized form. The normal-
ization of an n-gram consisted of lowercasing and
Porter-stemming each of the lemmatized forms of
its tokens, then putting these stems into alphabeti-
cal order (while omitting the stems of stopword to-
kens). With this kind of representation it was then
possible to handle two orthographically different,
but semantically equivalent phrases, such as ‘the
screen is tiny’ and ‘TINY screen’ in the same way.

Previous works on keyphrase extraction also
usually carry out this step of normalization, how-
ever, here we did it in such a manner that a map-
ping to each of the original orthographic forms of
a normalized form and its corresponding context
(i.e. the sentences containing it) was preserved at
the same time and that could be successfully uti-
lized at later processing steps.

To provide an alternative way of normaliz-
ing phrases, experiments relying on the usage of
WordNet (Fellbaum, 1998) were also conducted.
In these settings the normalized form of a single
token was determined by first searching for all its
synsets (in the case of verbs, these were such noun
synsets that were in derivative relation with the
synsets of the verb word form). Then instead of
Porter-stemming the original token, its most fre-
quent word form was stemmed, based on the es-
timated frequencies of WordNet for all the word
forms of the synsets of the original token. In
this way two – originally differently stemmed –
word forms, such as decide and decision could be
stemmed to the same root forms. Another advan-
tage of this procedure is that it is able to handle
semantic similarity to some extent.

The remaining parts of the normalization pro-
cedure were left unchanged (i.e. lowercasing and
alphabetical ordering of the normalized forms of
the individual tokens). Later, in the Results sec-
tion, the effect of this kind of normalization will
be shown.

Candidate terms were handled at the review
level instead of occurrence level. This means that
each normalized occurrence of a keyphrase candi-
date was gathered from the document and the fea-
ture values for the candidate term aggregate over
its occurrences.

3.2 Feature representation

We constructed a rich feature set to represent the
review-level keyphrase candidates. The feature
space incorporates features calculated on the ba-

sis of the normalized phrases themselves, but more
importantly, thanks to the mapping between the
normalized phrase forms and their original occur-
rences, new contextual and orthographic features
were possible to incorporate.

Features that could be generally used for any
kind of keyphrase extracting task (e.g. that makes
use of multiword expressions or character suffixes
in a special way) and ones designed especially for
the novel task of opinion phrase extraction (e.g.
that uses SentiWordNet to determine polarity) as
well as the standard features of keyphrase extrac-
tion are both introduced in the following.

Standard Features Since we assumed that the
underlying principles of extracting opinionated
phrases are quite similar to that of extracting stan-
dard (most of the time scientific) keyphrases, fea-
tures of the standard setting were applied in this
task as well. The most common ones, introduced
by KEA (Witten et al., 1999) are the Tf-idf value
and the relative position of the first occurrence
of a candidate phrase within a document. We
should note that KEA is primarily designed for
keyphrase extraction from scientific publications
and whereas the position of the first occurrence
might be indicative in research papers, product re-
views usually do not contain a summarizing “ab-
stract” at the beginning. For these reasons we
chose these features as the ones which form our
baseline system. Phrase length is also a common
feature, which was defined here as the number of
the non-stopword tokens of an opinion phrase can-
didate.

Linguistic and orthographic features Since
certain POS-codes are more frequent than others
among genuine keyphrases, features generated by
POS-codes belonging to an occurrence of a nor-
malized phrase were applied. As POS-code se-
quences seem to be more informative, instead of
simply indicating which POS-codes were assigned
to any orthographic alternation of a normalized
keyphrase candidate, it would be desirable to store
the POS-code sequences in their full length as
well. However, doing so might affect dimensional-
ity in a negative way (especially when having few
training data), i.e. the number of all the possible
POS-code sequences ranging from lengths of 1 to
4 is too much. To overcome this issue, positional
information was added to the POS-code features
derived from the tokens of an n-gram. Features

1164



of POS-codes that were assigned to a token be-
ing itself a 1-token long keyphrase candidate, at
the beginning, at the end, in between an n-gram,
got a prefix S-, B-, E- and I-, respectively. For
instance, the phrase cheap/JJ phone/NN induces
the features {B-JJ, E-NN}, whereas the 1-token-
long phrase cheap/JJ induces the feature {S-JJ}.
Finally, numeric values for a normalized candi-
date phrase were assigned based on the distribu-
tion of the different POS-related features of all the
running-text forms of a normalized phrase.

We introduced features exploiting the syntac-
tic context of a candidate with parse trees. For
an n-gram with respect to all the sentences it was
contained in a given document, this feature stored
the average and the minimal depths of those NP-
rooted trees that contained the whole n-gram in
its yield. These features are intended to express
the “noun phraseness” of the phrase.

Features generated from the character suffixes
of the individual tokens of the occurrences of a
normalized keyphrase candidate were also em-
ployed. Character suffix features also incorporated
positional information, similarly as it was done in
the case of POS features. The suffixes themselves
came from the last 2 and 3 characters of the tokens
constructing an n-gram. For instance, the features
induced by (and thus assigned with true value) for
the phrase cheap phone are {B-eap, B-ap, E-one,
E-ne}.

Opinionated phrases often bear special ortho-
graphic characteristics, e.g. in the case of so
slooow or CHEAP. Due to the fact that the original
forms of the phrases are stored in our representa-
tion, it was possible to construct two features for
this phenomenon: the first feature is responsible
for character runs (i.e. more than 2 of the same
consecutive characters), and an other is responsi-
ble for strange capitalization (i.e. the presence
of uppercase characters besides the initial one).
The S-,B-,E-,I- prefixes were applied here as well,
just like in the case of the Named Entity feature,
which represented if a token was part of NE (with
its type as well).

World knowledge-based features Features re-
lying on the outer resources of Wikipedia and Sen-
tiWordNet were also exploited during our exper-
iments. They were useful as world knowledge
could be incorporated by their means.

Multiword expressions are lexical items that
can be decomposed into single words and display

idiosyncratic features (Sag et al., 2002), in other
words, they are lexical items that contain space.

To measure the added value of MWEs in the
task of opinion phrase extraction, a set of fea-
tures was designed that indicated whether a cer-
tain phrase candidate (1) is an MWE on its own
(e.g. ease of use), (2) can be composed from more
MWEs on the list (e.g. mobile internet access),
or is just the (3) superstring of at least one MWE
from the list (e.g. send text messages). In or-
der to be able to make such decisions, a wide list
of MWEs was constructed from Wikipedia (dump
2011-01-07): all the links and formatted (i.e. bold
or italic) text were gathered that were at least two
tokens in length, started with lowercase letters and
contained only English characters or some punctu-
ation. Finally, an alignment of the elements of the
list and the contexts of the reviews of the dataset
was carried out (taking care of linguistic alterna-
tions and POS-tag matchings).

A more sophisticated surface-based feature
used external information as well on the individ-
ual tokens of a phrase. It relied on the sentiment
scores of SentiWordNet (Esuli et al., 2010), a pub-
licly available database that contains a subset of
the synsets of the Princeton Wordnet with pos-
itivity, negativity and neutrality scores assigned
to each one, depending on the use of its senti-
ment orientation (which can be regarded as the
probability of a phrase belonging to a synset be-
ing mentioned in a positive, negative or neu-
tral context). These scores were utilized for
the calculation of the sentiment orientations of
each token of a keyphrase candidate. Surface-
based SentiWordnet-calculated feature values for
a keyphrase candidate included the maximal posi-
tivity and negativity and subjectivity scores of the
individual tokens and the total sum over all the to-
kens of one phrase.

Sentence-based features were also defined
based on SentiWordNet as it was also used to
check for the presence of indicator terms within
the sentences containing a candidate phrase.
Those word forms were gathered from SentiWord-
Net, for which the sum of the average positiv-
ity and negativity sentiments scores among all its
synsets were above 0.5 (i.e. the ones that are more
likely to have some kind of polarity). Then for a
given keyphrase candidate of a given document,
a true value was assigned to the SentiWordNet-
derived indicator features that had at least one

1165



co-occurrence within the same sentence with the
keyphrase candidate in the same document.

SentiWordnet was also used to investigate the
entire sentences that contained a phrase candi-
date. This kind of feature calculated the sum of
every sentiment score in each sentence where a
given phrase candidate was present. Then the
mean and the deviation of the sum of the sen-
timent scores were calculated for each token of
the phrase-containing sentences and assigned to
the phrase candidate. The mean of the sentiment
scores of the individual sentences yielded a gen-
eral score on the sentiment orientation of the sen-
tences containing a candidate phrase, while higher
values for the deviation was intended to capture
cases when a reviewer writes both factual (i.e. uses
few opinionated words) and non-factual (i.e. uses
more emotional phrases and opinions) sentences
about a product.

Finally, Wikipedia was also used to incorpo-
rate semantic features from its category hierarchy.
(Wikipedia categories form a taxonomy, indicat-
ing which article belongs to which (sub)category).
In the case of a candidate phrase all the nomi-
nal parts of the normalized titles of Wikipedia
categories for its related Wikipedia articles were
added as separate binary features to the feature
space. The normalization of the Wikipedia cate-
gory names was similar to that of keyphrase can-
didates. For instance, given the candidate phrase
‘service quality’ the feature wiki control qual is
set to true since the Wikipedia article named Ser-
vice quality is in the category Quality control.

Document and corpus-level features Among
document-level features, the standard deviation
of the relative positions compared to the doc-
ument length was a measure to be computed.
Higher values of the deviation in the position
means that the reviewer keeps repeating some
phrase from the beginning to the end of the review,
which might indicate that this phrase is of higher
importance for them.

As verbs often contribute to the sentiment po-
larity of the noun phrases they accompany (e.g.
‘I adore its fancy screen.’ versus ‘I bought this
phone one year ago.’), a set of features was intro-
duced to deal with the indicative verbs in the con-
text of candidate phrase occurrences within their
document. For this feature to be calculated we
took those verbs as indicators that occurred at least
100 times in the whole training dataset. When cal-

culating a feature value for an opinionated-phrase
candidate, the algorithm matched all of its occur-
rences in a document against every indicator verb.
For the calculation of the feature value for a given
phrase candidate – indicator verb pair, a syntac-
tic distance value was first defined. This syntac-
tic distance was equal to the minimal height of the
subtree which contained both the keyphrase candi-
date and the indicator verb itself to the left among
all the sentences associated with a document that
contained the keyphrase candidate. The feature
value was then determined by simply taking the
reciprocal of this semantic distance. This way, the
feature value was scaled between 0 and 1. (Note
that for indicator verbs that were not present in any
of the sentences containing a phrase candidate as-
sociated with a document, the semantic distance
value was defined to be infinity, the limit value of
the reciprocal of which is 0.)

Quite general characteristics of reason-
expressing phrases can also be captured at the
corpus level. Simply using the number of times an
argument phrase aspirant was assigned to a review
as a proper phrase on the training dataset was
also taken into account as a corpus-level feature
since the same proper opinion phrases can easily
reoccur regarding products of the same type.

4 Experiments

Experiments were carried out on two fairly dif-
ferent types of product reviews, namely mobile
phones and movies. We use standard keyphrase
extraction evaluation metrics and baselines for
evaluating our pros and cons extractor system.

4.1 Datasets

In our experiments, we crawled two quite dif-
ferent domains of product reviews, i.e. mobile
phone and movie reviews from the review portal
epinions.com. For both domains, 2000 re-
views were crawled from epinions.com and
an additional of 50 and 75 reviews for measur-
ing inter-annotator agreement, respectively. This
corpus is quite noisy (similarly to other user-
generated contents); run-on sentences and im-
proper punctuation were common, as well as
grammatically incorrect sentences since reviews
were often written by non-native English speak-
ers. 1

1All the data used in our experiments are available at
http://rgai.inf.u-szeged.hu/proCon

1166



Mobiles Movies
Number of reviews 2009 1962
Avg. sentence/review 31.9 29.8
Avg. tokens/sentence 16.1 17.0
Avg. keyphrases/review 4.7 3.2
Avg. keyphrase candidates/review 130.38 135.89

Table 1: Size-related statistics of the corpora

The list of pros and cons was inconsistent too in
the sense that some reviewers used full sentences
to express their opinions, while usually a few
token-long phrases were given by others. The seg-
mentation of their elements was marked in various
ways among reviews (e.g. comma, semicolon, am-
persand or the and token) and even differed some-
times within the very same review. There were
many general or uninformative pros and cons (like
none or everything as a pro phrase) as well.

In order to have a consistent gold-standard an-
notation for training and evaluation, we manually
refined the pros and cons of the reviews in the
corpora. In the first step, the automatic prepro-
cessing of the segmentation of pros and cons was
checked by human annotators. Our automatic seg-
mentation method split the lines containing pros
and cons along the most frequent separators. This
segmentation was corrected by the annotators in
7.5% of the reviews. Then the human annotators
also marked the general pros and cons (11.1% of
the pro and con phrases) and the reviews without
any identified keyphrases were discarded.

4.2 Evaluation issues
Keyphrase extraction systems are traditionally
evaluated on the top-n ranked keyphrase candi-
dates for each document by F-score (Kim et al.,
2010), which combines the precision and recall of
the correct keyphrases’ class. Evaluation is carried
out in a strict manner as a top-ranked keyphrase
candidate is accepted if it has exactly the same
standardized form as one of the keyphrases as-
signed to the review. The ranking of the phrase
candidates was based on a probability estimation
of a candidate belonging to the positive keyphrase
class. Results reported here were obtained using
5-fold cross validation using Maximum Entropy
classifier.

As we treated the mining of pros and cons as
a supervised keyphrase extraction task, we con-
ducted measurements with KEA (Witten et al.,
1999), which is one of the most cited publicly
available automatic keyphrase extraction system.

However, we should note that due to the fact that
our phrase extraction and representation strategy
(and even the determination of true positive in-
stances to some extent) slightly differs from that
of KEA, the added values of our features should
rather be compared to our second Baseline Sys-
tem (BLWN ) which uses WordNet for candidate
phrase normalization. The baseline systems use
our framework, with the feature set of KEA, which
consists of tf-idf feature and the relative first oc-
currence of a keyphrase candidate. The only dif-
ference among the two baseline systems is that BL
does not apply the WordNet-based normalization
of phrase candidates introduced in Section 3.1.

Since we had the same findings as Branavan et
al. (2008) that authors often omit several opinion
forming aspects from their pros and cons listings
that they later include in their review, we decided
to determine the complete lists of pros and cons
manually, that is, to compose pro and con phrases
on the basis of the reviews. Due to the highly sub-
jective nature of sentiments, the determination of
sentiment-affecting pro and con phrases was car-
ried out by three linguists, who were asked to an-
notate a 25-document subset of the mobile phone
dataset. Their averaged agreements for the deter-
mination of pro phrases are 0.701 and 0.533 for
Dice’s coefficient and Jaccard index, and 0.69 and
0.526 for cons, respectively.

4.3 Results

In our experiments all the linguistic processing
of the product reviews were carried out using
Stanford CoreNLP. It uses the Maximum Entropy
POS-tagger of Toutanova and Manning (2000) and
syntactic parsing works on the basis of Klein and
Manning (2003). The ranking of the candidate
keyphrases was based on the posteriori probabili-
ties of the MALLET implementation (McCallum,
2002) of Maximum Entropy classifier (le Cessie
and van Houwelingen, 1992).

During the fully automatic evaluation, we fol-
lowed strict evaluation (see 4.2) that is commonly
utilized in scientific keyphrase extraction tasks.
Table 2 contains the results of the strict evaluation
for both domains. However, since strict evalua-
tion is more likely to suit the evaluation of scien-
tific keyphrase extraction better, i.e. semantically
equivalent but different word forms are less com-
mon at that domain, we conducted human eval-
uation on the 25-document subset of the mobile

1167



Mobiles Movies
Feature Top-5 Top-10 Top-15 Top-5 Top-10 Top-15
KEA 1.72/1.84/1.77 1.42/3.04/1.94 1.39/4.48/2.12 1.21/1.93/1.49 0.98/3.13/1.5 0.89/4.26/1.48
BL 2.6/2.8/2.73 2.6/5.5/3.54 2.6/8.2/3.93 1.6/2.5/1.95 1.5/4.9/2.34 1.6/7.4/2.58
BLWN 2.7/2.9/2.8 2.7/5.8/3.68 2.7/8.7/4.12 1.7/2.8/2.14 1.7/5.4/2.61 1.7/8.2/2.88
IV 3.1/3.4/3.25§ 2.9/6.2/3.92 2.8/9.1/4.31 2.4/3.7/2.9† 2.0/6.3/3.04§ 1.9/8.8/3.09
KF 2.6/2.8/2.71 2.7/5.9/3.73 2.7/8.7/4.11 1.7/2.7/2.09 1.7/5.4/2.59 1.7/8.2/2.87
Length 3.2/3.4/3.26 § 3.1/6.6/4.18† 2.9/9.3/4.4 2.1/3.3/2.6 2.0/6.4/3.08§ 2.0/9.1/3.22§

MWE 4.7/5.0/4.88‡ 3.8/8.0/5.11‡ 3.4/10.8/5.12‡ 2.3/3.6/2.81† 2.0/6.3/3.06† 1.9/9.1/3.18§

POS 4.6/4.9/4.71‡ 4.2/9.0/5.77‡ 3.9/12.6/5.98‡ 2.9/4.6/3.57‡ 2.8/8.7/4.18‡ 2.5/11.7/4.1‡

SWN 6.0/6.4/6.2‡ 4.9/10.4/6.65‡ 4.3/13.6/6.49‡ 3.7/6.0/4.6‡ 3.1/9.8/4.73‡ 2.8/13.1/4.59‡

StDev 3.9/4.2/4.06‡ 3.8/8.1/5.15‡ 3.5/11.2/5.33‡ 2.9/4.6/3.59‡ 2.6/8.1/3.9‡ 2.5/11.6/4.07‡

Orth. 3.2/3.4/3.28§ 3.1/6.7/4.27† 2.9/9.5/4.49 3.0/4.7/3.65‡ 2.5/7.8/3.76‡ 2.3/10.9/3.82‡

Suffix 11.5/12.2/11.83‡ 8.6/18.2/11.66‡ 6.9/22.0/10.54‡ 6.8/10.7/8.34‡ 5.2/16.4/7.91‡ 4.3/20.1/7.08‡

Syntax 3.5/3.7/3.61‡ 3.0/6.4/4.06 2.8/9.1/4.33 2.3/3.6/2.78† 2.0/6.1/2.97§ 1.9/9.1/3.2§

Wiki 11.9/12.7/12.25‡ 8.1/17.4/11.09‡ 6.3/20.1/9.63‡ 8.8/13.9/10.78‡ 6.3/19.8/9.59‡ 4.8/22.5/7.9‡

COMB 14.8/15.7/15.27‡ 10.4/22.0/14.11‡ 8.0/25.4/12.17‡ 10.0/15.8/12.22‡ 7.0/21.9/10.63‡ 5.3/24.6/8.67‡

Table 2: Performance using different features in the form of Precision/Recall/F-score obtained. IV,
KF, SWN and Orth. stands for indicator verbs, corpus-level keyphrase frequency, SentiWordNet and
orthography-driven features, respectively. Symbols §, † and ‡ in the upper index of a result indicates that
it is significantly better compared to the baseline system which uses the WordNet based candidate phrase
normalization (BLWN ) at confidence levels of 0.1, 0.05 and 0.01, based on Student’s t-test, respectively.
As it was only the KF feature which did not yield any significant improvement at all, the combined
system (COMB) incorporated all the features but KF.

phone domain. The results of the manual evalua-
tion is shown in 3.

4.4 Discussion

The fact that the highest F-scores for keyphrases
are achieved when the number of extracted phrases
is around the average number of pro and con
phrases per reviews (i.e. between 3 and 5) sug-
gests that our ordering of keyphrase candidates is
quite effective (since once we find the number of
keyphrases a document has, performance cannot
really grow anymore).

Comparing the nature of the task of extract-
ing keyphrases from scientific publications and
that of product reviews, we shall take two ob-
servations: firstly, keyphrases of scientific docu-
ments are more universal, i.e. once we have the
knowledge that the expression distributed comput-
ing was a good keyphrase for one scientific docu-
ment, we can be more confident about it being a
proper keyphrase for other documents within the
same domain as well, whereas in the case of opin-
ion phrases such as pink color can easily be men-
tioned in either opinionated and non-opinionated
contexts. Secondly, besides scientific keyphrases
being more universal, they are more deterministic
in the sense that there are fewer ways to express
good keyphrases, e.g. suppose simulated anneal-

ing is a proper keyphrase for a scientific document,
it is unlikely that an automatic system would ex-
tract imitated annealing, whereas in the case of
product review the gold standard keyphrases of-
ten differ from their mention in the text (e.g. tiny
keys and small keys).

The above mentioned examples suggest that
opinion phrase extraction is more difficult to be
performed and evaluated compared to scientific
keyphrase extraction. We should note that the best
performing system at SemEval-2010 (Kim et al.,
2010) that dealt with the much simpler task of sci-
entific keyphrase extraction achieved an F-score of
19.3 when evaluated against author keywords at
the top-15 level.

It should be also added here, that among the
keyphrases regarded as false positives in our eval-
uations, there were many near misses due to syn-
onymy, e.g. tiny keys and small keys or slow Web
and slow WAP. To overcome the synonymy issue
to some extent the WordNet-based rewriting of to-
kens was introduced, which brought improvement
in the case of the baseline systems for both do-
mains (so it was employed in the later experiments
as well). Another source of false positive clas-
sifications was due to the incompleteness of the
opinion aspect entered by the user, i.e. not all the
important aspects are necessarily listed among the

1168



Top-5 Top-10 Top-15
Prec. Recall F-score Prec. Recall F-score Prec. Recall F-score

∪ 72.8 20.63 32.14 66.8 33.54 44.66 63.47 46.88 53.92
∩ 46.4 27.81 34.77 41.6 44.92 43.2 37.07 56.68 44.82
Author 34.4 22.29 27.05 31.6 35.43 33.4 28.8 45.14 35.17

Table 3: Results of the human evaluation. ∪, ∩ and Author means when the automatic keyphrases
were matched against the union, intersection of the keyphrases of three independent annotators and the
keyphrases of the original author, respectively.

pros and cons section, as described earlier. On the
other side, many of the author-entered keyphrases
were absent in the contents of a review in their
same form: only 34,8% and 23,9% of gold stan-
dard keyphrases could be found in the texts having
the same normalized form for the mobile phone
and the movie domains, respectively, setting an
upper bound for the recall values when evaluating
based on strict matching.

To overcome all the previously mentioned
shortcoming during automatic evaluation, human
evaluation was performed and it showed that real
life application of opinion phrase extraction could
be of much higher utility than strict evaluation
would suggest. This is due to the fact that human
annotators had access to common sense knowl-
edge and during the inspection of keyphrases they
could resolve such cases that were impossible dur-
ing automatic evaluation.

All the features were effective in the sense that
expanding the baseline feature set by them sepa-
rately resulted in better results. Moreover, in the
majority of the cases improvements were of high
significance (see Table 2). The added value of
Wikipedia features (that are likely to work well
in other domains as well) should be highlighted
as well as the relatively poor effect of keyphrase
frequency feature which normally works better in
the case of standard scientific keyphrase extraction
tasks. A possible reason for keyphrase frequency
feature not being that effective in the opinion do-
main is that in the case of opinionated keyphrases,
the presence of such a phrase that was marked as
positive in one document is not necessarily marked
the same way in other documents, e.g. because
one author may write about the feature objec-
tively while the other may write his strong opin-
ions about the very same feature, using similar
wording.

5 Conclusions

In this paper, we presented a pros and cons ex-
traction system by pointing out the parallelism be-
tween the keyphrases of scientific papers – given
by their author – and the pros and cons phrases –
given by product reviewers. The WordNet-based
phrase normalization and an extended stopword-
based filtration of keyphrase candidates intro-
duced here could be of possible use for any kind
of phrase extraction tasks. Besides demonstrating
their similarity, the main differences of the two
tasks were also highlighted, and several ways to
adopt to the specialties of opinion phrase extrac-
tion have been suggested by introducing a rich
feature set, some of which could also be widely
used (e.g. Wikipedia-based ones), and others are
specifically designed to the special task of opin-
ion phrase extraction (e.g. SentiWordNet-related
ones).

Among the most important differences of opin-
ion phrase extraction from scientific keyphrase ex-
traction we should note that for product reviews
the pure occurrence of a single phrase is less de-
terministic to be a keyphrase, i.e. some emotional
context is necessary to treat them as genuine ones.
Also, the language of reviews is more special since
it tends to contain elements that are not present
in other genres of documents, such as irony and
sarcasm and offers more possibility to express
identical things in different ways. In total, our
results are competitive with those of other stan-
dard keyphrase extraction tasks even when apply-
ing strict normalized form matching evaluation.
Moreover, human evaluation showed that when se-
mantics are involved into the evaluation, results
are significantly better than it is suggested by au-
tomatic evaluations.

1169



Acknowledgments

This work was supported by the Project “TÁMOP-
4.2.1/B-09/1/KONV-2010-0005 – Creating the
Center of Excellence at the University of Szeged”,
supported by the European Union and co-financed
by the European Regional Development Fund and
by the project BELAMI financed by the National
Innovation Office of the Hungarian government.

References
S.R.K. Branavan, Harr Chen, Jacob Eisenstein, and

Regina Barzilay. 2008. Learning document-
level semantic properties from free-text annotations.
In Proceedings of ACL-08: HLT, pages 263–271,
Columbus, Ohio. ACL.

Andrea Esuli, Stefano Baccianella, and Fabrizio Sebas-
tiani. 2010. Sentiwordnet 3.0: An enhanced lexi-
cal resource for sentiment analysis and opinion min-
ing. In Proceedings of the Seventh conference on
International Language Resources and Evaluation
(LREC’10), Valletta, Malta. ELRA.

Christiane Fellbaum, editor. 1998. WordNet An Elec-
tronic Lexical Database. The MIT Press, Cam-
bridge, MA ; London.

Minqing Hu and Bing Liu. 2004a. Mining and summa-
rizing customer reviews. In Proceedings of the tenth
ACM SIGKDD international conference on Knowl-
edge discovery and data mining, KDD ’04, pages
168–177, New York, NY, USA. ACM.

Minqing Hu and Bing Liu. 2004b. Mining opinion
features in customer reviews. In Proceedings of the
19th national conference on Artifical intelligence,
AAAI’04, pages 755–760. AAAI Press.

Soo-Min Kim and Eduard Hovy. 2006. Automatic
identification of pro and con reasons in online re-
views. In Proceedings of the COLING/ACL 2006
Main Conference Poster Sessions, pages 483–490,
Sydney, Australia. ACL.

Su Nam Kim, Olena Medelyan, Min-Yen Kan, and
Timothy Baldwin. 2010. Semeval-2010 task 5: Au-
tomatic keyphrase extraction from scientific articles.
In Proceedings of the 5th International Workshop
on Semantic Evaluation, SemEval ’10, pages 21–26,
Morristown, NJ, USA. ACL.

Dan Klein and Christopher D. Manning. 2003. Ac-
curate unlexicalized parsing. In Proceedings of the
41st ACL, pages 423–430.

S. le Cessie and J.C. van Houwelingen. 1992. Ridge
estimators in logistic regression. Applied Statistics,
41(1):191–201.

Jingjing Liu and Stephanie Seneff. 2009. Review sen-
timent scoring via a parse-and-paraphrase paradigm.

In Proceedings of the 2009 Conference on Empiri-
cal Methods in Natural Language Processing, pages
161–169, Singapore. ACL.

Andrew Kachites McCallum. 2002. Mal-
let: A machine learning for language toolkit.
http://mallet.cs.umass.edu.

Olena Medelyan, Eibe Frank, and Ian H. Witten.
2009. Human-competitive tagging using automatic
keyphrase extraction. In Proceedings of the 2009
Conference on Empirical Methods in Natural Lan-
guage Processing, pages 1318–1327, Singapore.
ACL.

Bo Pang, Lillian Lee, and Shivakumar Vaithyanathan.
2002. Thumbs up?: sentiment classification us-
ing machine learning techniques. In EMNLP ’02:
Proceedings of the ACL-02 conference on Empirical
methods in natural language processing, pages 79–
86, Morristown, NJ, USA. ACL.

Ana-Maria Popescu and Oren Etzioni. 2005. Extract-
ing product features and opinions from reviews. In
Proceedings of Human Language Technology Con-
ference and Conference on Empirical Methods in
Natural Language Processing, pages 339–346, Van-
couver, British Columbia, Canada. ACL.

Ivan A. Sag, Timothy Baldwin, Francis Bond, Ann
Copestake, and Dan Flickinger. 2002. Multiword
Expressions: A Pain in the Neck for NLP. In
Proceedings of CICLing-2002, pages 1–15, Mexico
City, Mexico.

Todd Sullivan. 2008. Pro, con, and affinity tagging of
product reviews. Technical Report 224n, Stanford
CS.

Ivan Titov and Ryan McDonald. 2008. A joint model
of text and aspect ratings for sentiment summariza-
tion. In Proceedings of ACL-08: HLT, pages 308–
316, Columbus, Ohio. ACL.

Kristina Toutanova and Christopher D. Manning.
2000. Enriching the knowledge sources used in a
maximum entropy part-of-speech tagger. In Pro-
ceedings of the 2000 Joint SIGDAT conference on
Empirical methods in natural language processing
and very large corpora, EMNLP ’00, pages 63–70,
Stroudsburg, PA, USA. ACL.

Peter Turney. 2002. Thumbs up or thumbs down? se-
mantic orientation applied to unsupervised classifi-
cation of reviews. In Proceedings of the 40th An-
nual Meeting of the Association for Computational
Linguistics (ACL), pages 417–424.

Peter Turney. 2003. Coherent keyphrase extraction via
web mining. In Proceedings of IJCAI, pages 434–
439.

Ian H. Witten, Gordon W. Paynter, Eibe Frank, Carl
Gutwin, and Craig. 1999. Kea: Practical automatic
keyphrase extraction. In ACM DL, pages 254–255.

1170


